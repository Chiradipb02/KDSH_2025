Assessing the quality of information extraction
Anonymous Author(s)
Affiliation
Address
email
Abstract
Advances in large language models have notably enhanced the efficiency of infor- 1
mation extraction from unstructured and semi-structured data sources. As these 2
technologies become integral to various applications, establishing an objective 3
measure for the quality of information extraction becomes imperative. However, 4
the scarcity of labeled data presents significant challenges to this endeavor. In 5
this paper, we introduce an automatic framework to assess the quality of the in- 6
formation extraction/retrieval and its completeness. The framework focuses on 7
information extraction in the form of entity and its properties. We discuss how to 8
handle the input/output size limitations of the large language models and analyze 9
their performance when extracting the information. In particular, we introduce 10
scores to evaluate the quality of the extraction and provide an extensive discussion 11
on how to interpret them. 12
1 Introduction 13
In the domain of natural language processing (NLP), information extraction (IE) stands as a critical 14
task, transforming unstructured or semi-structured data into a structured format conducive to indexing, 15
exploration, and further analysis. The increasing amount of data across digital platforms underscores 16
the urgency for sophisticated IE techniques that can parse through volumes of information with 17
precision. An extensive survey about IE is provided by [ 1], where the authors highlight the complexity 18
of processing and analyzing text to derive meaningful information, given the heterogeneity and volume 19
of such data. 20
Large language models (LLMs) have revolutionized IE by introducing generative methods for 21
structuring knowledge from text. LLMs excel across diverse domains without extensive task-specific 22
training. A survey by [ 9] details the progress of LLMs on IE tasks. Here, the authors address specific 23
aspects of information extraction, including entity recognition, relation extraction, event detection, 24
and universal IE. They review the existing models and their efficiency on a comprehensive collection 25
of annotated benchmarks. Nonetheless, the challenge of quantitatively assessing the quality and 26
completeness of extracted information persists, particularly in the absence of labeled datasets for 27
benchmarking. Before conducting the experiments introduced in this paper, we perform IE on a vast 28
corpus of business documents utilizing LLMs. While the extraction process is beyond the scope of 29
this paper, some details about the extraction are given in Section 3. 30
To measure the quality of extraction, we propose an evaluation framework that relies on artificially 31
generated complex information which is infused into the document to test the efficiency of LLMs in 32
IE tasks. This paper introduces an iterative extraction process and a novel score, MINEA (Multiple 33
Infused Needle Extraction Accuracy), to address the critical need for objective quality assessment 34
measures. By inserting artificial information ("needles") into the data, the proposed method creates 35
a synthetic ground truth for evaluation, enabling the measurement of extraction quality in various 36
specific domains even without manually labeled data. The empirical analysis demonstrates the 37
utility of MINEA for evaluating LLM-based IE in scenarios where ground truth is unavailable. By 38
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.automating the quality assessment of information extraction, the framework could reduce the need 39
for manual review by experts, saving time and resources and thus enhance the efficiency and accuracy 40
of information extraction from large volumes of unstructured data. 41
The paper is organized as follows: Section 2 presents a related work that inspired us when developing 42
our IE quality assessment method; Section 3 sketch a way in which structured information is obtained 43
using LLMs; Section 4 deals with shortcomings arising when treating long contexts by LLMs; finally 44
Section 5 introduces the novel method to access the quality of IE and provide the reader with practical 45
tips; Sections 4 and 5 are supplemented by numerical studies. The data used in these studies are an 46
internal set of documents related to a business case in the healthcare industry. 47
2 Related work 48
A common practice in many specialized IE tasks is that well-trained experts review what was extracted 49
and provide ground truth as done in [ 5]. Such an approach is relatively reliable, however, it is manual 50
and very time-consuming. 51
In [4] they suggest summary score without reference (SUSWIR), a score to evaluate the quality of 52
text summaries without the need for human annotations. The SUSWIR score can be used for IE tasks 53
where the extracted information is viewed as a compression of original data. The score compares 54
the original text with its summary. From its nature, it is very useful when comparing the outputs 55
of extraction tasks among themselves, i.e., the best extraction/summary has the highest score value. 56
On the other hand, its ability to provide an objective absolute evaluation of a single extraction is 57
disadvantaged because the desirable output is not known. 58
Recently, an effort to eliminate the requirement for human involvement relies on LLMs. These prove 59
themselves as highly cost-effective data creators, either by labeling unlabeled data or generating data 60
given the labels, see [ 7]. Therefore they may substitute human experts providing the ground truth by 61
doing their work in an automatic way. 62
Needle In A Haystack (NIAH)1evaluation is a tool designed to evaluate the performance of LLMs in 63
retrieval across different sizes of context. Short targeted information, the ‘needle’, is inserted into a 64
large, more complex text body, the ‘haystack’. The goal is to test an LLM’s ability to find and make 65
use of this piece of information. 66
Our method builds on LLMs acting as data creators, but instead of annotating the complete data, it 67
only automatizes the process of creating the needle. I.e., given an original text, an LLM generates the 68
needle. The needle then substitutes the ground truth. 69
3 Capturing the structure 70
The form of needles depends on a form of data, on structure capturing the information and on the 71
task being solved. The needles can be short paragraphs of text, account records, graph nodes as you 72
extract information from continuous text, table, graph, respectively. The structured arrangement of 73
information is beneficial for consecutive processing and analysis. It helps to highlight relationships 74
among distinct information pieces. There are countless ways to impose a structure on unstructured 75
data in order to capture the relevant information. To demonstrate our methodology for measuring the 76
quality of information extraction, we specify a particular structure and tailor the needles to it. 77
3.1 Schema 78
To impose a structure on the data, we adopt the idea of schema markup [ 3] which is used to 79
communicate the content of a web page to the search tool. The schema markup is in the form of 80
structured data and can be viewed as a compression of the essential information. The structure 81
is defined by Schema.org2vocabulary which is a set of entity types, each associated with a set of 82
properties and hierarchically arranged. Figure 1 shows an example of structured information inspired 83
by Schema.org. It describes three entities of types ‘Insight’, ‘Person’ and ‘Organization’. Each 84
1https://github.com/gkamradt/LLMTest_NeedleInAHaystack
2https://schema.org
2type has its own set of properties, e.g., an entity of type ‘Person’ is described by ‘type’, ‘name’, 85
‘birthDate’, ‘worksFor’, and ‘jobTitle’. In other words, each entity is a set of key-value pairs, e.g., 86
‘name’ is the key and ‘AI Enthusiast’ is the value. 87
Figure 1: Toy example: structured information encapsulating three entities using schema.org.
Similarly, we extract and compress the relevant information contained in data using an LLM. 88
Schema.org presents a clear basis for the categorization of various entities contained in data. In the 89
rest of the paper, by schema we mean a predetermined set of types, such as {‘Person’, ‘Project’, 90
‘Product’, ‘Legislation’, ‘Event’, ‘OpportunityArea’, ‘Insight’, ‘Substance’, ‘Thing’, ‘BioChemEn- 91
tity’, ‘MedicalCondition’}, together with their properties. The schema is set at the beginning and 92
the information to be extracted depends on it. Therefore the schema has to be tailored to a particular 93
scope of the (proprietary) knowledge and application. If a more complex or uncommon entity needs 94
to be captured, it is natural and very easy to extend the set of core types by more detailed descriptive 95
and custom vocabulary. E.g., ‘Insight’ and ‘OpportunityArea’ are not native Schema.org types, but 96
we will use them in our study. The usage of suitably tailored schema is beneficial for specialized 97
applications since it narrows the information to the relevant core and hence potentially improves the 98
overall performance. On the other hand, the usage of schemata is not restrictive as the scope can be 99
always extended by using a broader set of types. 100
3.2 The role of LLMs 101
LLMs are rather effective in the creation of structured data, cf. [ 9]. Using dedicated prompts, we get 102
a structured text file describing entities found in the documents and matching types of predefined 103
schema. The predefined schema (types and properties) is given to an LLM within the prompt. The 104
LLM is asked to analyse the document, identify an information relevant to the mentioned types of 105
entities and populate the schema with this information. It is asked to be attentive to nested entities, 106
maintain consistency and uniqueness of extracted entities. Indeed, LLM is not prohibited from 107
extracting entities whose types do not appear in the predefined schema. It is worthy to note, that 108
LLMs are known to inherit biases present in their training data. If not carefully managed, these biases 109
could lead to unfair or inaccurate information extraction, impacting decision-making processes. 110
Besides the information extraction task, LLMs can be used to suggest suitable Schema.org types for 111
a particular document. An example together with a prompt is shown in Appendix B1. 112
34 Length aspects 113
When focusing on the quality of IE performed by an LLM, several limitations that LLM presents 114
in terms of the length of data to be extracted from must be considered. Each LLM has a maximal 115
content limit it can process, both on the input and the output. The limit on the output is typically 116
much more strict. When trying to use the maximal possible input another issue may appear – the 117
Lost in the middle phenomenon [ 8] says that the ability of LLMs to retrieve information from a long 118
context declines and that the attention focuses on the beginning and the end of the context while it 119
tends to attenuate information in the middle. 120
To demonstrate shortcomings arising from these limitations numerically we use gpt-4-1106-preview 121
model.3The model is limited by 4095 tokens on the output and by 128000 tokens on the input 122
(context window limit). The following sections present two major LLM limitations we have to 123
consider before performing IE, namely length restrictions in Section 4.1 and Lost in the middle 124
problem in Section 4.2. 125
4.1 Length restrictions 126
Long data are difficult to process because of the restrictions posed by the maximum amount of: 127
(O) output tokens: The restriction on output tokens means that there is some maximal length of 128
data from which most entities can be efficiently extracted. If the length of the text exceeds 129
this maximum, there would be no tokens for extra entities. 130
(I)input tokens: Maximal size of context window (input) prohibits the extraction of data 131
exceeding the specific token limit. 132
Another difficulty regarding the output is the tendency of LLMs to generate rather brief responses 133
which do not use the allowed maximal number of tokens. This unwillingness of models can be 134
circumvented by prompting. Even so, the limited number of output tokens is typically too low and 135
prevents effective extraction from long texts. 136
With a more sophisticated approach, the restriction (O) becomes irrelevant and only the restriction (I) 137
will apply. The issue imposed by (O) is overcome by splitting the source document into smaller pieces 138
which are extracted independently. A significant drawback is that the extracted information can be 139
easily duplicated – extracted independently from multiple text pieces. Iterating the calls to the LLM 140
with instruction to continue with already started extraction, i.e., continuing with the extraction in a 141
single thread, helps to extract more information and avoid duplication. As we insist on continuation, 142
more and more information is added and the extraction is more thorough, at least to some point – this 143
will be addressed in detail in Section 5.1. Further, a lower number of duplicates is found due to the 144
extraction history, i.e., all information extracted until present, which is kept within the thread. 145
The combination of both improvements – text splitting and iterated calls, has proven itself to perform 146
the best. We split the document into distinct text pieces which we extract sequentially. Extraction 147
from each text piece is carried out by several iterated LLM calls while taking into account the 148
extraction history from previously extracted text pieces. Once the sum of the lengths of the text 149
pieces and the extraction history exceeds the context window limit, i.e., restriction (I) applies, a new 150
independent extraction starts. A single structured output, per document or once (I) is applied, is 151
created by appending all entities identified from each text piece. 152
4.2 Lost in the middle 153
In the case of long documents, whose extraction consumes almost the whole context window, 154
LLMs are giving more inconsistent results and we can observe a presence of the Lost in the middle 155
phenomenon, see [ 8]. We extract information from several long documents from our business case 156
which are each split into 15 pieces and its processing consumes almost the whole context window. 157
We add the sixteenth piece identical to one of the fifteen that are already extracted and measure a 158
redundancy score, for details see Appendix A. Each column of Table 1 then states the redundancy of 159
the newly extracted information with the information that was already extracted from the same piece 160
of the text before. The table presents mean values per four distinct documents. We can notice that 161
3https://platform.openai.com/docs/models/overview
4for the parts ’in the middle’ the proportion of redundantly extracted entities (entities with the same 162
’name’ attribute) is higher than for those at the beginning and the end. 163
Table 1: Are we lost in the middle? After finishing the extraction of a whole document (consisting of
fifteen pieces), we re-extract the information from each of its pieces. Columns 1-15 then compare
the re-extracted information with the information that was extracted from the same piece of the text
before. The pieces in the middle of the document contain more duplicated entities then those at the
beginning and the end.
part 1 2 3 4 5 6
redundancy (key = ’name’) 0 0 0.2266 0.1150 0.1482 0.3816
7 8 9 10 11 12 13 14 15
0.3334 0.4643 0.7398 0.5152 0.6672 0.4659 0.3820 0.4473 0.4086
5 Quality of extraction 164
Once the information is extracted from data into a structured form defined by the chosen schema, 165
e.g., Figure 1, the quality of such extraction is important to evaluate. In practice, it is very rare to be 166
equipped with ground truth and its human generation requires vast expertise in the scope of data and a 167
ridiculous amount of time. Therefore we adopt methods from [ 4]. They examine semantic similarity, 168
relevance, redundancy, and bias and compound these into a single score called SUSWIR, for details 169
see Appendix A. The score and its subparts are very useful when comparing distinct extractions 170
among themselves, e.g., we can use it to find an optimal number of iterated LLM calls. Unfortunately, 171
the score does not represent an absolute way of evaluation. It does not provide a complete insight into 172
the task – some information (= entities) can be missing, misclassified or their properties not filled 173
in correctly. To come up with a robust and general solution we generalize the NIAH test, which is 174
commonly used to measure the ability of LLMs to process long documents, cf. [6]. 175
5.1 Iterated LLM calls 176
Since the first LLM extraction is typically not exhaustive, iterating the extraction process helps with 177
the completeness of extraction. To improve the quality of extraction, we ask LLM to process the 178
document again and search for other entities which were not extracted yet. A question arises: What is 179
the optimal number of iterations? It is desirable to stop when additional LLM call will return no or 180
only a few new entities. The answer however depends heavily on the text being extracted and on the 181
chosen schema. Below, we present a small comparative study regarding the contribution of iterated 182
extraction to its quality. We interpret the extracted structured data, e.g., Figure 3, as a summary of 183
the original text document. To measure the quality of the summary we adopt the scores from [ 4] (a 184
convex combination of these scores creates the overall SUSWIR metric), namely semantic similarity , 185
relevance , and redundancy avoidance . We use a modified bias avoidance score from [ 4] and add two 186
new scores, relevance spread , and incompleteness score . See Appendix A for more details. 187
Consider document which length is approximately 12k chars. Table 2 compares the content of the 188
document with extracted information created iteratively by succeeding LLM calls. Each iteration 189
enriches the extracted information, but the benefit decreases. From the third iteration, i.e., after 190
four LLM calls, the majority of scores in Table 2 are either getting worse or stagnating (the arrows 191
following the score name indicate the direction in which the score improves). It is obvious that shorter 192
and longer text will require less or more iterations to extract majority of information without reducing 193
its semantic and factually relevant meaning, respectively. Further, the risk that the LLM will suffer 194
from hallucinations increases as we observe a growth of bias. In the rest of the paper we use three 195
iterations to extract documents of approximate length 12k chars within all extractions (if not stated 196
otherwise). 197
5.2 Test the quality 198
This section introduces a robust and versatile score to objectively measure the quality of IE. Assuming 199
the structure is imposed by some schema, see Section 3.1, we would like to measure the IE quality as 200
5Table 2: Quality of extraction depends on a number of calls to LLM. The first iterated call is the most
beneficial one. From some point (bold) the scores stagnate or even deteriorate. All scores have values
between 0 and 1, the arrows indicate whether lower ( ↓) or higher ( ↑) values are desired.
# iterations 0 1 2 3 4 5
semantic similarity ↑ 0.5416 0.6316 0.6899 0.7572 0.7540 0.7685
relevance ↑ 0.3409 0.4396 0.4449 0.4746 0.4522 0.4445
relevance spread ↓ 0.3364 0.2493 0.2350 0.1445 0.1428 0.1368
redundancy avoidance (0.2) ↑0.7727 0.8670 0.8810 0.9257 0.9251 0.9307
redundancy avoidance (0.1) ↑0.4697 0.5936 0.6854 0.8002 0.7972 0.8119
redundancy avoidance 0.8182 0.9163 0.9422 0.9650 0.9699 0.9726
(0.5, key=’name’) ↑
bias avoidance ↑ 0.5614 0.5515 0.4925 0.4559 0.4447 0.4247
incompleteness ↓ 0. 0.5862 0.6735 0.4217 0.5413 0.4615
a portion of successfully extracted entities, i.e., the accuracy of name entity recognition (NER) task 201
taking into account even the context captured by entity properties. Unfortunately, such an experiment 202
is unfeasible without labeled data. As a consequence, it is unfeasible in many specialized tasks 203
because of the absence of suitable labeled data unseen by LLM models. This can be the case with 204
very recent datasets as well as proprietary datasets. To overcome this issue we use inspiration by 205
NIAH test to build up an automatic and general procedure to access the quality of IE tasks. 206
5.2.1 Needles 207
A ‘needle’ in our context represents an entity. It is created according to the chosen schema, i.e., 208
a list of types we want to extract from the document. We use an LLM to generate a short paragraph 209
introducing a new original (not appearing in the document) entity, but still relevant to the scope of the 210
document, for an example see Figure 2, and for more details on generation process see Appendix B2. 211
This artificial paragraph, the needle, is then placed into the document body at random (taking into 212
the account natural units within the text as sentences, paragraphs, etc. if applicable). Moreover, 213
the needle is accompanied with several properties, namely we assign to the needle a name, short 214
description and keywords, see Figure 2. This additional properties are assigned to the needle by the 215
LLM. 216
5.2.2 Multiple infused needle extraction accuracy 217
To measure the quality of extraction we propose a multiple infused needle extraction accuracy 218
(MINEA) score. Its computation combines the approach of NIAH evaluation and NER task. We 219
scatter several needles at random over the text document body (such that the inserted needles fill 10 220
to 30% of the enriched text) and measure how many of them were successfully extracted. Since we 221
know what exactly was inserted, we know what should be extracted. Then we can objectively measure 222
the quality of extraction on these new entities and moreover, we can compare extracted information 223
from the document with and without needles. Table 3 shows extraction accuracy – MINEA score 224
– total and per schema type – measured on a vast corpus of business documents with predefined 225
schema consisting of types ‘BioChemEntity’, ‘Event’, ‘Insight’, ‘Legislation’, ‘MedicalCondition’, 226
‘OpportunityArea’, ‘Person’, ‘Product’, ‘Project’, ‘Substance’ and ‘Thing’. 227
5.2.3 Identification of needles 228
Matching the generated needles with extracted entities imposes a challenge and mostly depends 229
on the formulation of needles. If the needles are too complex or too vague, the straightforward 230
identification changes into a serious problem. For this reason, we equip the needles with additional 231
properties which are then used to compare the needles with extracted entities and to decide whether 232
the needles were extracted successfully or not. 233
We present several alternative ways how to measure whether the extraction of a needle is successful: 234
nan entity with a name perfectly matching the needle name is found; 235
nsthe needle name is found among the extracted information; 236
6Figure 2: Toy example: two needles, highlighted by blue color, accompanied by additional informa-
tion described by ‘name’, ‘description’, and ‘keywords’.
Table 3: Quality of extraction – MINEA score – total and per schema type. Entity types are grouped
into five classes - 1. three most frequent schema.org types in the documents; 2. med-bio-chem
entities, somewhat interchangeable types; 3. best distinguishable types; 4. custom (non Schema.org)
types; 5. Schema.org types related to documents, but not stated in the chosen schema. Note: an entity
is assumed to be extracted if it is contained within the extracted information - often its type can be
misclassified (Project-Product-OpportunityArea, Substance-Thing-BioChemEntity) or sometimes it
can be mentioned indirectly (Organization is related to a Person by property ’works for’).
class entity type extraction accuracy # entities used for evaluation
Person 0.884 69
1 Project 0.702 47
Product 0.750 52
Substance 0.822 45
2 Thing 0.739 46
BioChemEntity 0.674 43
MedicalCondition 0.636 44
3 Legislation 0.942 52
Event 0.915 47
4 OpportunityArea 0.671 73
Insight 0.747 91
5 Organization 0.907 43
Place 0.767 43
overall 0.780 695
kan entity with some number of keywords perfectly matching the needle keywords is found, 237
the number is determined by the threshold parameter determining the percentage of keywords 238
to be matched; 239
7Figure 3: Toy example: extracted information from the data infused by needles from Figure 2.
llm an entity matching the needle according to LLM is found. 240
Table 4: Toy example: fulfillment of the conditions. The text enriched by two needles from Figure 2
was extracted into the form shown in Figure 3.
entity type condition for needle identification
n ns k0.5 k0.6 k0.7 llm
Event 0 1 1 0 0 1
Product 0 0 1 1 0 1
Note that other conditions can be constructed, e.g., based on the short description instead of keywords, 241
etc. Table 4 shows whether the conditions are fulfilled in the example illustrated by Figures 2 and 242
3. Namely, the condition nis not satisfied (‘AI Clan Meeting’ ̸=‘AI Meeting’, ‘Graph Index’ ̸= 243
‘GRIX’). Condition nsis satisfied only for needle representing an entity of type ‘Event’ (‘AI Clan 244
Meeting’ can be found in the extracted information). There are three keywords out of the six assigned 245
to the needle representing the entity of type ‘Event’ which match the keywords of an extracted entity, 246
hence k0.5 is, and k0.6,k0.7 are not satisfied (there is an entity within the extracted information 247
with 50% of keywords being the same as the keywords of the needle). In the case of the second 248
needle, there are four such keywords, therefore k0.5 andk0.6 are satisfied. Finally, both needles are 249
identified within the extracted information by an LLM. 250
Table 5 shows scores (ratios of successfully extracted entities) based on the above criteria in the case 251
of our business documents. The types of inserted needles are ’BioChemEntity’, ’Country’, ’Event’, 252
’Insight’, ’Legislation’, ’Person’, ’Product’, ’Project’ and ’Substance’. Matching the needle and 253
entity name usually does not perform well if the name is prone to modification (e.g., person name 254
with and without title), or if the entity is easy to be misclassified (an entity of type ‘Country’ was 255
often extracted as ‘Place’ whose name did not match the country name). Searching for a needle name 256
in all extracted information gives very accurate results if the entities are well characterized by their 257
name (compare for example types ‘Person’ and ’Legislation’ with type ’Insight’ where the name is 258
not a natural attribute). Matching the needle and entity keywords depends on the threshold parameter 259
– with a lower proportion of keywords that have to match the score value increases and the reliability 260
of the entity identification decreases. An LLM performs well the entity identification and it is an 261
important criterion in the case of more creative types such as ‘Insight’. Finally, the MINEA score for 262
each type is taken as the maximum of the scores (the values are highlighted). 263
8Table 5: The decision about the success of needle extraction can be made based on several criteria:
comparing the corresponding needle and entity properties (columns nandk0.5-k0.7 compare name
and keywords, respectively), full-text search (column nssearch for the needle name in extracted
information), comparison of needles and entities using LLM (column llm).
entity type condition for needle identification # entities used
n ns k0.5 k0.6 k0.7 llm for evaluation
Person 0.594 0.884 0.652 0.362 0.232 0.826 69
Project 0.170 0.702 0.638 0.234 0.085 0.681 47
Product 0.596 0.712 0.462 0.192 0.135 0.750 52
Country 0 0.765 0.412 0.294 0.059 0.471 17
Legislation 0.635 0.942 0.365 0.269 0.096 0.942 52
Event 0.830 0.851 0.638 0.511 0.149 0.915 47
Insight 0.176 0.187 0.714 0.418 0.088 0.747 91
BioChemEntity 0.116 0.605 0.651 0.581 0.488 0.674 43
Substance 0.289 0.578 0.822 0.644 0.222 0.800 45
5.2.4 Model comparison 264
MINEA score can be used to compare the performance of distinct LLMs, see Table 6. A corpus 265
of documents is infused by needles representing entities whose types match the schema introduced 266
in Section 5.2.2. Three OpenAI LLMs4are used to extract a relevant information under the same 267
setting (the same model parameters such as temperature, the same number of iterations, the same 268
prompting, etc.). Model gpt-3.5-turbo is outperformed by gpt-4-turbo by almost 15% and gpt-4-turbo 269
is outperformed by gpt-4o model by another 12%. Note that the achieved accuracy is lower than 270
presented in Table 3, since only one iteration instead of three was performed in order to reduce the 271
computational time. 272
Table 6: LLMs comparison using MINEA score.
model gpt-3.5-turbo gpt-4-turbo gpt-4o
MINEA 0.449198 0.593583 0.716578
Conclusions 273
In this paper, we focused on quality evaluation of information extraction (IE) performed by large 274
language models (LLMs). First, we delved into the technical limitations of LLMs complicating the 275
extraction of information from a long context. To extract reasonable information from data it is 276
needed to take into the account features such as context window limits, iterated extractions, extraction 277
history recording and Lost in the middle phenomenon. Once the extraction is performed, assessing its 278
quality is essential. However in many customized tasks, a truly objective method is missing, because 279
of the lack of labeled data fitting the scope of the application. The versatile method presented in this 280
paper overcomes the issue by adjustment of the data by insertion of an artificial information, a needle, 281
into it. The artificial information created to this purpose is application and data-specific, but the 282
method itself is applicable generally across the field of IE. By controlling the generation process of 283
the needles, we created a synthetic ground truth that enables us to absolutely measure the extraction 284
quality even when no labeled data is available. We introduced a MINEA score to measure the quality 285
of extraction. The key part is a decision rule on whether a needle was successfully extracted or not. 286
MINEA possibly combines several decision rules into one final score. Our empirical analysis of the 287
MINEA score on a specialized dataset demonstrated its utility for evaluation of LLM-based IE tasks 288
when ground truth is unavailable. 289
4https://platform.openai.com/docs/models
9References 290
[1]Kiran Adnan and Rehan Akbar. Limitations of information extraction methods and techniques for 291
heterogeneous unstructured big data. International Journal of Engineering Business Management , 292
11:1847979019890771, 2019. 293
[2]Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with 294
improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic 295
and extrinsic evaluation measures for machine translation and/or summarization , pages 65–72, 296
2005. 297
[3]Matthew Edgar. Schema and structured data markup. In Tech SEO Guide: A Reference Guide for 298
Developers and Marketers Involved in Technical SEO , pages 67–78. Springer, 2023. 299
[4]Abdullah Al Foysal and Ronald Böck. Who Needs External References?—Text Summarization 300
Evaluation Using Original Documents. AI, 4(4):970–995, 2023. 301
[5]Neil Jethani, Simon Jones, Nicholas Genes, Vincent J Major, Ian S Jaffe, Anthony B Cardillo, 302
Noah Heilenbach, Nadia Fazal Ali, Luke J Bonanni, Andrew J Clayburn, et al. Evaluating 303
ChatGPT in Information Extraction: A Case Study of Extracting Cognitive Exam Dates and 304
Scores. 2023. 305
[6]Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail 306
Burtsev. In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss. 307
arXiv preprint arXiv:2402.10790v2 , 2024. 308
[7]Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W White, and Sujay Kumar Jauhar. Making large 309
language models better data creators. arXiv preprint arXiv:2310.20111 , 2023. 310
[8]Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, 311
and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint 312
arXiv:2307.03172 , 2023. 313
[9]Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng 314
Zheng, and Enhong Chen. Large language models for generative information extraction: A 315
survey. arXiv preprint arXiv:2312.17617 , 2023. 316
Appendix A 317
To measure the quality of the summary we adopt the methods from [ 4]:semantic similarity combines 318
latent semantic similarity and cosine similarity; relevance is measured using METEOR score, see 319
[2], without chunk penalty; redundancy avoidance compares extracted entities among themselves 320
using a threshold parameter – entities with a higher cosine similarity are assumed to be redundant; 321
redundancy avoidance can be focused on a single particular property of entities (we use ’name’ as 322
this pivotal property). 323
We modify the bias avoidance score from [ 4] to be J∗(A, B ) =|A∩B|
|B|,where Arepresents the 324
entities in the original text document and we normalize by a number of entities that were extracted, 325
|B|. The score controls how much information in the structured file is not present in the original text, 326
i.e., a potential hallucination of an LLM. 327
We add two new scores: the relevance spread is the standard deviation of relevance over the text 328
pieces to which the document is split and normalized by the mean value, its higher values indicate 329
that the extraction from distinct text pieces is unbalanced; the incompleteness score just measures the 330
proportion of entities with incomplete information (at least one property value missing or unfilled), 331
e.g., the entity ‘AI Enthusiast’ in Figure 1 has an unknown ‘birthDate’. 332
Appendix B 333
Except for the IE task, LLMs are used in several subtasks within the paper, namely to determine 334
schema types appearing in the document, to create a suitable needles fitting contextually to the 335
10document and to identify whether a needle was extracted or not. In the following, we provide the 336
reader with prompts and examples of these subtasks. 337
B1 Discovering a schema 338
Figure 4 shows a prompt to obtain the Schema.org types from the attached text – Wikipedia article 339
about IE.5An LLM is asked to assign relevance to the types to distinguish the most important ones. 340
Figure 5 shows the entity types that were deduced from the text, together with their relevance and 341
reasoning for why they were chosen. The most relevant types are those directly mentioned – ‘Article’, 342
as the webpage content itself is represented as an article, ‘SoftwareApplication’, and ‘WebSite’ (all 343
with maximal relevance). The least relevant identified types are generic – ‘Thing’, as a parent type of 344
many directly mentioned types, and ‘LearningResource’, as a categorization of the article style. 345
Figure 4: Prompt to determine a possible suitable schema from a given text – Wikipedia article about
IE.
Figure 5: Schema.org types found by an LLM within Wikipedia article about IE.
5https://en.wikipedia.org/wiki/Information_extraction
11B2 Creating needles 346
A needle, i.e., a text paragraph fitting thematically to the document, but being new and unique to it, is 347
generated by an LLM using the prompt in Figure 6. The prompt specifies the type of entity that the 348
needle should represent. Multiple needles of the same type can be obtained easily within a single 349
LLM call. 350
Figure 7 shows ten needles representing the entities of type ‘Person’ generated based on a Wikipedia 351
article about IE. In the next step properties such as a name, description and keywords can be generated 352
by an LLM. 353
Figure 6: Prompt to generate needles. Given a Wikipedia article about IE, the LLM is asked to think
out 10 relevant persons.
12Figure 7: Needles generated by an LLM and representing ten entities of type ‘Person’.
. 354
13B3 Identifying needles 355
The quality of extraction is evaluated based on the proportion of successfully extracted needles. An 356
LLM can be used to decide whether the needle was extracted or not using the prompt presented in 357
Figure 8. 358
Figure 8: Prompt to identify whether the needles were extracted or not.
14NeurIPS Paper Checklist 359
1.Claims 360
Question: Do the main claims made in the abstract and introduction accurately reflect the 361
paper’s contributions and scope? 362
Answer: [Yes] 363
Justification: The abstract and introduction clearly state the development of an automatic 364
framework to assess the quality of information extraction (IE), which is the main contribution 365
of the paper. This is supported by the introduction of the MINEA score and the discussion 366
on handling input/output size limitations of large language models (LLMs). 367
Guidelines: 368
•The answer NA means that the abstract and introduction do not include the claims 369
made in the paper. 370
•The abstract and/or introduction should clearly state the claims made, including the 371
contributions made in the paper and important assumptions and limitations. A No or 372
NA answer to this question will not be perceived well by the reviewers. 373
•The claims made should match theoretical and experimental results, and reflect how 374
much the results can be expected to generalize to other settings. 375
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 376
are not attained by the paper. 377
2.Limitations 378
Question: Does the paper discuss the limitations of the work performed by the authors? 379
Answer: [Yes] 380
Justification: The paper discusses the limitations related to the complexity or vagueness of 381
the needles, dependence on the chosen schema and criteria for needle identification (Section 382
5). Further the paper focuses on limitations of LMMs in IE tasks such as input/output size 383
constraints, lost in the middle phenomenon, bias and hallucinations (Section 4). 384
Guidelines: 385
•The answer NA means that the paper has no limitation while the answer No means that 386
the paper has limitations, but those are not discussed in the paper. 387
• The authors are encouraged to create a separate "Limitations" section in their paper. 388
•The paper should point out any strong assumptions and how robust the results are to 389
violations of these assumptions (e.g., independence assumptions, noiseless settings, 390
model well-specification, asymptotic approximations only holding locally). The authors 391
should reflect on how these assumptions might be violated in practice and what the 392
implications would be. 393
•The authors should reflect on the scope of the claims made, e.g., if the approach was 394
only tested on a few datasets or with a few runs. In general, empirical results often 395
depend on implicit assumptions, which should be articulated. 396
•The authors should reflect on the factors that influence the performance of the approach. 397
For example, a facial recognition algorithm may perform poorly when image resolution 398
is low or images are taken in low lighting. Or a speech-to-text system might not be 399
used reliably to provide closed captions for online lectures because it fails to handle 400
technical jargon. 401
•The authors should discuss the computational efficiency of the proposed algorithms 402
and how they scale with dataset size. 403
•If applicable, the authors should discuss possible limitations of their approach to 404
address problems of privacy and fairness. 405
•While the authors might fear that complete honesty about limitations might be used by 406
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 407
limitations that aren’t acknowledged in the paper. The authors should use their best 408
judgment and recognize that individual actions in favor of transparency play an impor- 409
tant role in developing norms that preserve the integrity of the community. Reviewers 410
will be specifically instructed to not penalize honesty concerning limitations. 411
153.Theory Assumptions and Proofs 412
Question: For each theoretical result, does the paper provide the full set of assumptions and 413
a complete (and correct) proof? 414
Answer: [NA] 415
Justification: The paper does not include theoretical results that require formal proofs. 416
Guidelines: 417
• The answer NA means that the paper does not include theoretical results. 418
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 419
referenced. 420
•All assumptions should be clearly stated or referenced in the statement of any theorems. 421
•The proofs can either appear in the main paper or the supplemental material, but if 422
they appear in the supplemental material, the authors are encouraged to provide a short 423
proof sketch to provide intuition. 424
•Inversely, any informal proof provided in the core of the paper should be complemented 425
by formal proofs provided in appendix or supplemental material. 426
• Theorems and Lemmas that the proof relies upon should be properly referenced. 427
4.Experimental Result Reproducibility 428
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 429
perimental results of the paper to the extent that it affects the main claims and/or conclusions 430
of the paper (regardless of whether the code and data are provided or not)? 431
Answer: [Yes] 432
Justification: The paper provides detailed descriptions of the experimental setup, including 433
the use of LLMs for IE and the creation of synthetic ground truth data. This is detailed in 434
Sections 3 and 5. 435
Guidelines: 436
• The answer NA means that the paper does not include experiments. 437
•If the paper includes experiments, a No answer to this question will not be perceived 438
well by the reviewers: Making the paper reproducible is important, regardless of 439
whether the code and data are provided or not. 440
•If the contribution is a dataset and/or model, the authors should describe the steps taken 441
to make their results reproducible or verifiable. 442
•Depending on the contribution, reproducibility can be accomplished in various ways. 443
For example, if the contribution is a novel architecture, describing the architecture fully 444
might suffice, or if the contribution is a specific model and empirical evaluation, it may 445
be necessary to either make it possible for others to replicate the model with the same 446
dataset, or provide access to the model. In general. releasing code and data is often 447
one good way to accomplish this, but reproducibility can also be provided via detailed 448
instructions for how to replicate the results, access to a hosted model (e.g., in the case 449
of a large language model), releasing of a model checkpoint, or other means that are 450
appropriate to the research performed. 451
•While NeurIPS does not require releasing code, the conference does require all submis- 452
sions to provide some reasonable avenue for reproducibility, which may depend on the 453
nature of the contribution. For example 454
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 455
to reproduce that algorithm. 456
(b)If the contribution is primarily a new model architecture, the paper should describe 457
the architecture clearly and fully. 458
(c)If the contribution is a new model (e.g., a large language model), then there should 459
either be a way to access this model for reproducing the results or a way to reproduce 460
the model (e.g., with an open-source dataset or instructions for how to construct 461
the dataset). 462
(d)We recognize that reproducibility may be tricky in some cases, in which case 463
authors are welcome to describe the particular way they provide for reproducibility. 464
16In the case of closed-source models, it may be that access to the model is limited in 465
some way (e.g., to registered users), but it should be possible for other researchers 466
to have some path to reproducing or verifying the results. 467
5.Open access to data and code 468
Question: Does the paper provide open access to the data and code, with sufficient instruc- 469
tions to faithfully reproduce the main experimental results, as described in supplemental 470
material? 471
Answer: [No] 472
Justification: The paper does not provide open access to the data and code due to the 473
proprietary nature of the business documents used in the experiments. However, it provides 474
detailed instructions on how to replicate the methodology. 475
Guidelines: 476
• The answer NA means that paper does not include experiments requiring code. 477
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 478
public/guides/CodeSubmissionPolicy ) for more details. 479
•While we encourage the release of code and data, we understand that this might not be 480
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 481
including code, unless this is central to the contribution (e.g., for a new open-source 482
benchmark). 483
•The instructions should contain the exact command and environment needed to run to 484
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 485
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 486
•The authors should provide instructions on data access and preparation, including how 487
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 488
•The authors should provide scripts to reproduce all experimental results for the new 489
proposed method and baselines. If only a subset of experiments are reproducible, they 490
should state which ones are omitted from the script and why. 491
•At submission time, to preserve anonymity, the authors should release anonymized 492
versions (if applicable). 493
•Providing as much information as possible in supplemental material (appended to the 494
paper) is recommended, but including URLs to data and code is permitted. 495
6.Experimental Setting/Details 496
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 497
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 498
results? 499
Answer: [Yes] 500
Justification: The paper specifies the use of LLMs, the schema used for structuring data, and 501
the process of generating needles for evaluation. These details are provided in Sections 3, 4 502
and 5. 503
Guidelines: 504
• The answer NA means that the paper does not include experiments. 505
•The experimental setting should be presented in the core of the paper to a level of detail 506
that is necessary to appreciate the results and make sense of them. 507
•The full details can be provided either with the code, in appendix, or as supplemental 508
material. 509
7.Experiment Statistical Significance 510
Question: Does the paper report error bars suitably and correctly defined or other appropriate 511
information about the statistical significance of the experiments? 512
Answer: [No] 513
Justification: The paper does not include experiments that require statistical significance 514
testing or error bars. The experiments in Sections 4 and 5 present mean values of reasonably 515
large samples. The experiments are not repeated, each of them is carried once on a set of 516
17distinct documents containing a large amount of entities. In Section 5, a vast set of unique 517
needles (with repeating types) is used to infuse the documents. 518
Guidelines: 519
• The answer NA means that the paper does not include experiments. 520
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 521
dence intervals, or statistical significance tests, at least for the experiments that support 522
the main claims of the paper. 523
•The factors of variability that the error bars are capturing should be clearly stated (for 524
example, train/test split, initialization, random drawing of some parameter, or overall 525
run with given experimental conditions). 526
•The method for calculating the error bars should be explained (closed form formula, 527
call to a library function, bootstrap, etc.) 528
• The assumptions made should be given (e.g., Normally distributed errors). 529
•It should be clear whether the error bar is the standard deviation or the standard error 530
of the mean. 531
•It is OK to report 1-sigma error bars, but one should state it. The authors should 532
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 533
of Normality of errors is not verified. 534
•For asymmetric distributions, the authors should be careful not to show in tables or 535
figures symmetric error bars that would yield results that are out of range (e.g. negative 536
error rates). 537
•If error bars are reported in tables or plots, The authors should explain in the text how 538
they were calculated and reference the corresponding figures or tables in the text. 539
8.Experiments Compute Resources 540
Question: For each experiment, does the paper provide sufficient information on the com- 541
puter resources (type of compute workers, memory, time of execution) needed to reproduce 542
the experiments? 543
Answer: [No] 544
Justification: The paper does not provide detailed information on the compute resources used 545
for the experiments. The requirements such as time of execution are determined especially 546
by used LLMs. 547
Guidelines: 548
• The answer NA means that the paper does not include experiments. 549
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 550
or cloud provider, including relevant memory and storage. 551
•The paper should provide the amount of compute required for each of the individual 552
experimental runs as well as estimate the total compute. 553
•The paper should disclose whether the full research project required more compute 554
than the experiments reported in the paper (e.g., preliminary or failed experiments that 555
didn’t make it into the paper). 556
9.Code Of Ethics 557
Question: Does the research conducted in the paper conform, in every respect, with the 558
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 559
Answer: [Yes] 560
Justification: The research adheres to the NeurIPS Code of Ethics, ensuring that the methods 561
and data used do not violate ethical guidelines. The proprietary data used is handled with 562
confidentiality and integrity. 563
Guidelines: 564
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 565
•If the authors answer No, they should explain the special circumstances that require a 566
deviation from the Code of Ethics. 567
18•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 568
eration due to laws or regulations in their jurisdiction). 569
10.Broader Impacts 570
Question: Does the paper discuss both potential positive societal impacts and negative 571
societal impacts of the work performed? 572
Answer: [Yes] 573
Justification: The paper is primarily concerned with the technical methodology, the intro- 574
duction of the MINEA score, and the empirical analysis of the framework’s performance. 575
The potential positive impacts are mentioned in Introduction: by automating the quality 576
assessment of information extraction, the framework could reduce the need for manual 577
review by experts, saving time and resources and thus enhance the efficiency and accuracy 578
of information extraction from large volumes of unstructured data. The negative aspects of 579
using LLMs for IE tasks such as inherited bias and potential hallucinations are mentioned 580
especially in Sections 4.2 (Lost in the middle problem) and 5.1 (bias avoidance score). 581
Guidelines: 582
• The answer NA means that there is no societal impact of the work performed. 583
•If the authors answer NA or No, they should explain why their work has no societal 584
impact or why the paper does not address societal impact. 585
•Examples of negative societal impacts include potential malicious or unintended uses 586
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 587
(e.g., deployment of technologies that could make decisions that unfairly impact specific 588
groups), privacy considerations, and security considerations. 589
•The conference expects that many papers will be foundational research and not tied 590
to particular applications, let alone deployments. However, if there is a direct path to 591
any negative applications, the authors should point it out. For example, it is legitimate 592
to point out that an improvement in the quality of generative models could be used to 593
generate deepfakes for disinformation. On the other hand, it is not needed to point out 594
that a generic algorithm for optimizing neural networks could enable people to train 595
models that generate Deepfakes faster. 596
•The authors should consider possible harms that could arise when the technology is 597
being used as intended and functioning correctly, harms that could arise when the 598
technology is being used as intended but gives incorrect results, and harms following 599
from (intentional or unintentional) misuse of the technology. 600
•If there are negative societal impacts, the authors could also discuss possible mitigation 601
strategies (e.g., gated release of models, providing defenses in addition to attacks, 602
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 603
feedback over time, improving the efficiency and accessibility of ML). 604
11.Safeguards 605
Question: Does the paper describe safeguards that have been put in place for responsible 606
release of data or models that have a high risk for misuse (e.g., pretrained language models, 607
image generators, or scraped datasets)? 608
Answer: [NA] 609
Justification: The paper does not release any data or models that pose a high risk for misuse. 610
Guidelines: 611
• The answer NA means that the paper poses no such risks. 612
•Released models that have a high risk for misuse or dual-use should be released with 613
necessary safeguards to allow for controlled use of the model, for example by requiring 614
that users adhere to usage guidelines or restrictions to access the model or implementing 615
safety filters. 616
•Datasets that have been scraped from the Internet could pose safety risks. The authors 617
should describe how they avoided releasing unsafe images. 618
•We recognize that providing effective safeguards is challenging, and many papers do 619
not require this, but we encourage authors to take this into account and make a best 620
faith effort. 621
1912.Licenses for existing assets 622
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 623
the paper, properly credited and are the license and terms of use explicitly mentioned and 624
properly respected? 625
Answer: [Yes] 626
Justification: All existing models are properly referenced and credit to their creators is given. 627
These are either LLMs or metrics such as SUSWIR and METEOR (Section 5 and Appendix 628
A). 629
Guidelines: 630
• The answer NA means that the paper does not use existing assets. 631
• The authors should cite the original paper that produced the code package or dataset. 632
•The authors should state which version of the asset is used and, if possible, include a 633
URL. 634
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 635
•For scraped data from a particular source (e.g., website), the copyright and terms of 636
service of that source should be provided. 637
•If assets are released, the license, copyright information, and terms of use in the 638
package should be provided. For popular datasets, paperswithcode.com/datasets 639
has curated licenses for some datasets. Their licensing guide can help determine the 640
license of a dataset. 641
•For existing datasets that are re-packaged, both the original license and the license of 642
the derived asset (if it has changed) should be provided. 643
•If this information is not available online, the authors are encouraged to reach out to 644
the asset’s creators. 645
13.New Assets 646
Question: Are new assets introduced in the paper well documented and is the documentation 647
provided alongside the assets? 648
Answer: [NA] 649
Justification: The paper does not introduce new assets that require documentation. 650
Guidelines: 651
• The answer NA means that the paper does not release new assets. 652
•Researchers should communicate the details of the dataset/code/model as part of their 653
submissions via structured templates. This includes details about training, license, 654
limitations, etc. 655
•The paper should discuss whether and how consent was obtained from people whose 656
asset is used. 657
•At submission time, remember to anonymize your assets (if applicable). You can either 658
create an anonymized URL or include an anonymized zip file. 659
14.Crowdsourcing and Research with Human Subjects 660
Question: For crowdsourcing experiments and research with human subjects, does the paper 661
include the full text of instructions given to participants and screenshots, if applicable, as 662
well as details about compensation (if any)? 663
Answer: [NA] 664
Justification: The paper does not involve crowdsourcing or research with human subjects. 665
Guidelines: 666
•The answer NA means that the paper does not involve crowdsourcing nor research with 667
human subjects. 668
•Including this information in the supplemental material is fine, but if the main contribu- 669
tion of the paper involves human subjects, then as much detail as possible should be 670
included in the main paper. 671
20•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 672
or other labor should be paid at least the minimum wage in the country of the data 673
collector. 674
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 675
Subjects 676
Question: Does the paper describe potential risks incurred by study participants, whether 677
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 678
approvals (or an equivalent approval/review based on the requirements of your country or 679
institution) were obtained? 680
Answer: [NA] 681
Justification: The paper does not involve research with human subjects that would require 682
IRB approval. 683
Guidelines: 684
•The answer NA means that the paper does not involve crowdsourcing nor research with 685
human subjects. 686
•Depending on the country in which research is conducted, IRB approval (or equivalent) 687
may be required for any human subjects research. If you obtained IRB approval, you 688
should clearly state this in the paper. 689
•We recognize that the procedures for this may vary significantly between institutions 690
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 691
guidelines for their institution. 692
•For initial submissions, do not include any information that would break anonymity (if 693
applicable), such as the institution conducting the review. 694
21