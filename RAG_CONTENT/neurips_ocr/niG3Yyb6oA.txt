A Layer-Wise Natural Gradient Optimizer for
Training Deep Neural Networks
Xiaolei Liu∗
Ant Group
Hangzhou, China
liuxiaolei.lxl@mybank.cnShaoshuai Li∗
Ant Group
Hangzhou, China
lishaoshuai.lss@mybank.cn
Kaixin Gao†
Ocean University of China
Qingdao, China
gaokaixin06@163.comBinfeng Wang
Ant Group
Hangzhou, China
wangbinfeng.wbf@mybank.cn
Abstract
Second-order optimization algorithms, such as the Newton method and the natu-
ral gradient descent (NGD) method exhibit excellent convergence properties for
training deep neural networks, but the high computational cost limits its practical
application. In this paper, we focus on the NGD method and propose a novel layer-
wise natural gradient descent (LNGD) method to further reduce computational
costs and accelerate the training process. Specifically, based on the block diagonal
approximation of the Fisher information matrix, we first propose the layer-wise
sample method to compute each block matrix without performing a complete back-
propagation. Then, each block matrix is approximated as a Kronecker product
of two smaller matrices, one of which is a diagonal matrix, while keeping the
traces equal before and after approximation. By these two steps, we provide a new
approximation for the Fisher information matrix, which can effectively reduce the
computational cost while preserving the main information of each block matrix.
Moreover, we propose a new adaptive layer-wise learning rate to further accelerate
training. Based on these new approaches, we propose the LNGD optimizer. The
global convergence analysis of LNGD is established under some assumptions.
Experiments on image classification and machine translation tasks show that our
method is quite competitive compared to the state-of-the-art methods.
1 Introduction
With the rapid increase in the size of deep neural networks (DNNs) models in both areas of computer
vision (CV) and natural language processing (NLP), there have been remarkable attentions given to
optimizing algorithms. An effective optimizer can significantly improve the training speed of models
while ensuring high prediction performance. First-order gradient descent methods are workhorses
of training DNNs, which can be broadly divided into two categories: methods use a same learning
rate, such as stochastic gradient descent (SGD) [ 1] and its accelerations [ 2,3], and methods use
adaptive learning rate, such as AdaDelta [ 4], RMSProp [ 5], ADAM [ 6] and Adabelief [ 7]. Although
first-order gradient descent methods enjoy low computational cost and ease of implementation, they
might suffer from sensitivity to hyperparameters and slow convergence. It is challenging to reduce
the number of iterations and computational time of these methods.
∗Joint first author, these authors contributed equally to this work.
†Corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Some work has considered introducing curvature information when updating parameters of DNNs
to improve the convergence speed and overcome the above shortcomings of the first-order methods.
However, second-order optimization methods need to store and compute the inverse of curvature
matrix, which brings expensive storage and computation costs and limits the application of second-
order methods in training large-scale DNNs. Therefore, many approximate second-order methods
have been proposed for training large-scale models. For example, Keskar and Berahas [ 8] proposed a
stochastic quasi-Newton algorithm for training recurrent neural networks. Yao et al. [ 9] approximated
the Hessian matrix as a diagonal operator, which is achieved by applying Hutchinson’s method,
and proposed the AdaHessian method. Goldfarb, Ren and Bahamou [ 10] developed Kronecker-
factored block-diagonal BFGS and its limited-memory variants L-BFGS methods for training DNNs.
Generalized Gauss-Newton methods, such as the Hessian-free method [ 11] and the Krylov subspace
method [12], also have been proposed to approximate the Hessian matrix.
The natural gradient descent (NGD) method [ 13], which preconditions the gradient by the Fisher
information matrix instead of the Hessian matrix, also has shown effectiveness in training DNNs
[14,15,16,17]. NGD explores the steepest direction of the objective function when the parameter
space has a Riemannian metric structure and has a faster convergence speed. In particular, NGD
can also be seen as an approximation of the Netwon method when the objective function and the
manifold metric are compatible [ 18]. However, it is still impossible to directly compute the inverse
of the Fisher information matrix for DNNs with millions or even billions parameters. Quite a few
approximate approaches have been proposed. Under some independency assumptions, Martens
and Grosse [14] proposed the Kronecker-factored approximate curvature (KFAC) method, in which
the Fisher information matrix is approximated as a block diagonal matrix and each block matrix is
further approximated as the Kronecker product of two smaller matrices. Then, KFAC was extended
to convolutional neural networks [ 19], recurrent neural networks [ 20] and variational Bayesian neural
networks [ 21] and showed significant speedup during training. In addition, George et al. [ 22]
proposed the eigenvalue-corrected Kronecker factorization (EKFAC) method. Gao et al. [ 15,23]
proposed the trace-restricted Kronecker-factored approximate (TKFAC) method. These approaches
all focus on the Kronecker-factored approximations of the Fisher information matrix. What’s more,
some works have also considered large-scale distributed computing using NGD for training DNNs
and shows excellent experimental performance [16, 24, 25].
In this paper, our main focus is on the NGD method. Motivated by the effectiveness of diagonal
approximations and the significance of diagonal elements in the curvature matrix, we prioritize the
diagonal information and integrate it into our approximation and introduce a novel method, namely
Layer-wise Natural Gradient Descent (LNGD). Our contributions can be given as follows:
•Based on the block diagonal approximation of the Fisher information matrix, we propose
a layer-wise sample method to more efficiently compute each block matrix corresponding
to each layer. By assuming that the predictive distribution of the output after the activation
function for each layer follows a Gaussian distribution, each block matrix can be directly
computed using the inputs and the outputs separately, without having to perform a complete
back-propagation.
•For each block matrix corresponding to each layer, we further approximate it as a Kronecker
product of two smaller matrices, one of which is a diagonal matrix, while keeping the traces
equal before and after approximation. With this operation, we further reduce the cost of
computing inverse matrices while still preserving the main information of each block matrix.
•In order to further accelerate the training, we propose an adaptive layer-wise learning rate by
optimizing a quadratic model, in which parameters in the same layer share the same adaptive
learning rate. Moreover, a faster approach of computing the adaptive layer-wise learning rate
is also provided, making it speed up training while maintaining computationally efficient.
•Based on the novel approximation mentioned above of the Fisher information matrix and
the adaptive layer-wise learning rate, we propose the LNGD optimizer for training DNNs.
The global convergence analysis are also established under some assumptions.
•We perform experiments on image classification and machine translation tasks. Numerical
results show that LNGD converges faster than SGD, ADAM and KFAC, and LNGD provides
an significant improvement in computational time savings when achieves convergence.
The rest of this paper is organized as follows. Section 2 gives the notations and introduces the NGD
method. In Section 3, we propose a novel approximation of the Fisher information matrix and the
2adaptive layer-wise learning rate. Furthermore, we give the framework of LNGD and establish the
convergence analysis. Section 4 presents the results of experiments on image classification and
machine translation tasks. The conclusion is drawn in Section 5.
2 Notations and Preliminaries
In this paper, for a matrix A, we use Aijto denote its (i, j)th entry, tr(A)to denote its trace and
∥A∥Fto denote its Frobenius norm. We use ◦and⊗to denote the Hadamard and Kronecker product
of two matrices. In the following, we briefly introduce the NGD method for training DNNs. During
the training process of neural networks, the purpose is to find the vector of parameters θwhich
minimizes the loss function h(θ). If the loss function h(θ)is chosen as the the cross-entropy loss
function, h(θ)can be given as h(θ) =E[−logp(y|x,θ)], where p(y|x,θ)is the density function
of a predictive distribution Py|x(θ), andx,yare the training inputs and labels, respectively. Next,
we give the definition of natural gradient, which gives the steepest direction of the objective function
when the parameter space has a Riemannian metric structure. The natural gradient is defined as
F−1∇θh(θ), where Fis the Fisher information matrix given by
F= E
x∼q(x),y∼p(y|x,θ)[∇θlogp(y|x,θ)∇θlogp(y|x,θ)⊤]. (1)
In Eq. (1), the input xis independently sampled from a distribution Qxwith density function being
q(x)and the label yis sampled from the predictive distribution Py|x(θ). In the following pages,
we abbreviate Ex∼q(x),y∼p(y|x,θ)asEunless otherwise specified. Consider a neural network with
Llayers, for each layer l∈[L]with[L] ={1,2, . . . , L }, we denote al−1andWlas the input (the
activation from the previous layer) and the matrix of weights of this layer, respectively. What’s
more, θl=vec(Wl)andθ= (θ1, . . . ,θL)⊤= (vec(W1)⊤, . . . , vec(WL)⊤)⊤, where vec(·)
indicates vectorization of a matrix. For convenience, we denote the derivative of the loss function
with respect to θasDθ=−∇θlogp(y|x,θ). Then the Fisher information matrix can be expressed
asF=E[DθDθ⊤].
Due to the high computational and storage costs caused by the inverse operation of high-dimensional
matrices, it is impractical to directly compute F−1in the training of DNNs. The family of Kronecker-
factored approximations provides an effective approach for computing F−1of parameters in high-
dimensional space, which is usually achieved by two steps. In the first step, by assuming that the
parameters between different layers are independent, these methods approximate the entire Fisher
information matrix as a block diagonal matrix, i.e.,
F≈diag(F1,F2, . . . ,FL), (2)
where Fl=E[DθlDθ⊤
l]for any l∈[L]. By this way, the Fisher information matrix can be
approximated by Lblock matrices. This step transforms the inverse of the entire Fisher information
matrix into the inverse of a series of small block matrices. In the second step, these methods further
approximate each block matrix as the Kronecker product of some smaller factors. This approximation
can transform the inverse of each block matrix into the inverse of some smaller factors combining the
properties of the Kronecker product.
3 LNGD: A Layer-Wise Second-Order Optimizer
In this section, we first introduce the layer-wise sample approximation strategy. Then, we present the
details of adaptive layer-wise learning rate mechanism and give the specific framework of LNGD.
Finally, elaborate theoretical analysis of LNGD’s convergence is also provided.
3.1 Layer-Wise Sample Approximation
For NGD methods to train DNNs, the Fisher information matrix can be approximated by a block
diagonal one according to different layers as given by Eq. (2), this approximation can be found in
[14,15,19,22] and references therein. We call such a block diagonal approximate Fisher information
matrix the layer Fisher information matrix, which is computed based on a distribution Qxand a
predictive distribution Py|x(θ)as given in Eq. (1). To obtain the layer Fisher information matrix, we
30 2000
100
200
300
0 2000
100
200
300
0 2000
100
200
300
0 200
10
20
30
0 200
10
20
30
0 200
10
20
30Figure 1: Comparison of the exact Fisher information matrix Fand our approximation FLNGD . We
use LNGD to train MNIST on a fully-connected neural network, whose architecture is 196-20-20-20-
20-10. We show the results of the Fisher information matrix of the first layer with 20 units in top,
which is a 400×400matrix. The bottom portion displays partially enlarged parts of the top marked
with red square, which is a 40×40matrix. Within both the top and bottom sections, on the left is the
exact Fisher information matrix F, in the middle is our approximation FLNGD , and on the right is the
absolute error between them. The brightness levels correspond to the sizes of the absolute values.
need perform a complete back-propagation to sequentially compute FL,FL−1, . . . ,F1, which still
consumes much computing time.
In this subsection, we propose a layer-wise sample approximation of the Fisher information matrix, in
which each block matrix Flis computed based on the lth layer’s prediction distribution Pal|al−1(θl)
with the input al−1∈Rdlof this layer and the input al∈Rdl+1of the (l+ 1) th layer instead of
using the same predictive distribution Py|x(θ)for all layers. Specifically, for al, we assume that the
predictive distribution Pal|al−1(θl)follows Gaussian distribution, which is usual used as prior by
variational auto-encoder [ 26], soFlcan be computed by sampling from a normal distribution with
expectation being aland variance being I. Similar assumption can also be found in [ 27], in which
the normality is also supported by a central limit theorem under the independence assumption. By
this layer-wise sample approximation, we can compute the layer Fisher information matrix without
having to perform a complete back-propagation and thus improve the computational efficiency.
Next, we can give the formula of each block Flin the layer Fisher information matrix as
Fl=E[˜Dθl˜Dθ⊤
l] =E[vec(˜DWl)vec(˜DWl)⊤] =E[vec(gla⊤
l−1)vec(gla⊤
l−1)⊤]
=E[(al−1a⊤
l−1)⊗(glg⊤
l)]∈Rmldl×mldl,
where ˜Dθl=−∇θlogp(al|al−1,θl),gl=−∇sllogp(al|al−1,θl)∈Rmlwithsl=Wlal−1, and
p(al|al−1,θl)is the density function of the distribution Pal|al−1(θl).
In practice, the dimension of each block matrix Flis often still too large to directly compute its
inverse matrix. Therefore, additional approximation methods are required to handle this computational
difficulty. Suppose that the predictive distribution of alfollows Gaussian distribution with expectation
beingaland variance being I, and each element of activation output alis independent and identically
distributed random number, then each element of partial derivative glis also independent and
identically distributed. It is easy to show that Flcan be seen as a matrix with dl×dlblock matrices,
in which each block is an ml×mlmatrix and the off-diagonal elements are zero. Therefore, Flcan
be approximated as
Fl≈E[(al−1a⊤
l−1)⊗diag(glg⊤
l)]. (3)
4Combining the property that (A⊗B)−1=A−1⊗B−1for any two invertible matrices AandB,
we can significantly reduces the computational complexity. Thus, some approaches have considered
approximating the Fisher information matrix as the Kronecker product of two factors [ 14,15,19,20,
22]. Inspired by these works, we also approximate Flas the Kronecker product of two factor matrices
Φl∈Rdl×dlandΨl∈Rdl×dl. To get factor matrices ΦlandΨl, we first replace diag(glg⊤
l)in
Eq. (3) by its trace and obtain Φl. Then we compute Ψlwhile keeping that tr(Fl) =tr(Φl⊗Ψl).
Specifically, Φlis given by
Φl=E[(al−1a⊤
l−1)⊗tr(diag(glg⊤
l))] =E[(al−1a⊤
l−1)×g⊤
lgl], (4)
On the other hand, Ψlcan be computed by
Ψl=E[(a⊤
l−1al−1)×diag(glg⊤
l)]
E[(a⊤
l−1al−1)(g⊤
lgl)]. (5)
Based on Eq. (4) and Eq. (5), we can show that tr (Fl) =tr(Φl⊗Ψl).
Fig. 1 presents the visualization results of the exact Fisher information matrix F, our approximation
FLNGD , and the absolute error between them. Brighter pixels indicate higher values. From the
left column in the top row, we observe the elements in the principal diagonal exhibit quite higher
values, indicating their significance with rich information. Similarly, FLNGD can also emphasize
the importance of the diagonal elements. The error figure reveals that the errors of the diagonal
elements are small, which indicates that FLNGD provides a good approximation effect for the diagonal
elements. Furthermore, to achieve a clearer visualization, we show the results of the partially enlarged
area marked with red square in the bottom row. Here, we can observe more clearly that FLNGD
achieves a favorable approximation effect on the diagonal elements. What’s more, FLNGD can also
provide an effective approximation of the elements in the auxiliary diagonals. These visualizations
demonstrate the effectiveness of our proposed approximation in capturing the main elements of the
Fisher information matrix. Therefore, our proposed approximation FLNGD is efficient and FLNGD can
retain most of information.
3.2 Adaptive Layer-Wise Learning Rate
In this subsection, we propose an adaptive layer-wise learning rate to accelerate training DNNs. We
first consider the cases that use the same learning rate for all elements and the adaptive element-wise
learning rate. Then we present the adaptive layer-wise learning rate scheme.
Suppose that dkis the update direction of the function h:Rn→Rat the iteration point θk. We first
recall the gradient descent methods for getting the minimization of h, in which the update rule can be
given as θk+1=θk−αkdk, where αkis the learning rate, which can be chosen according to the
value of the quadratic model
h(θk−αkdk)≈h(θk)−αk⟨dk,∇θh(θk)⟩+(αk)2
2(dk)⊤∇2
θh(θk)dk.
Once the update direction is chosen, the minimizer of αkcan be given by
αk=⟨dk,∇θh(θk)⟩
(dk)⊤∇2
θh(θk)dk(6)
if(dk)⊤∇2
θh(θk)dkis nonzero. If ∇2
θh(θk)is positive definite and dk= (∇2
θh(θk))−1∇θh(θk),
thenαk= 1, which leads to the classical Netwon method. In gradient decent methods, the learning
rate is often regarded as the most important hyperparameter that highly influences model training. A
fixed learning rate may lead to slow convergence or suboptimal performance in some cases. Therefore,
many works have considered using adaptive learning rate in gradient decent methods [ 5,6,28]. In
the following, we consider giving an adaptive element-wise learning rate automatically scaled by the
direction dk. In this case, the update rule of parameters is given by θk+1=θk−αk·dk=θk−Dkαk,
where αk∈Rnis the learning rate, Dk∈Rn×nis a diagonal matrix with (Dk)ii= (dk)iand
(Dk)ij= 0when i̸=jfori, j∈[n]and “·” denotes the element-wise product. The second Taylor
expansion of h(θ−Dα)at iteration kis
h(θk−Dkαk)≈h(θk)− ⟨Dkαk,∇θh(θk)⟩+1
2(Dkαk)⊤∇2
θh(θk)Dkαk.
5Taking the derivative of hwith respect to αkand letting it equal to 0, we get
2Dk∇2
θh(θk)Dkαk−Dk∇θh(θk) =0,
which yields that
αk= (∇2
θh(θk)Dk)−1∇θh(θk) (7)
ifDkand∇2
θh(θk)are positive definite.
Note that in Eq. (7), it is impractical to compute the inverse of ∇2
θh(θk)Dkdirectly for large-
scale models due to high computational and storage costs. For second-order optimization methods
in deep learning, some methods have considered approximating the curvature matrix by a block
diagonal one according to different layers [ 10,14,15,19,24]. What’s more, some works have
observed that parameters in the same layer have gradients of similar magnitudes. Therefore, a
common learning rate can be efficiently shared by these parameters [ 29,30]. Inspired by these
works, we propose a novel adaptive layer-wise learning method as follows. Suppose that dk=
((dk
1)⊤,(dk
2)⊤, . . . , (dk
L)⊤)⊤is the update direction of a Llayers neural network at the iteration
pointθk= ((θk
1)⊤,(θk
2)⊤, . . . , (θk
L)⊤)⊤, the update rule of θkis given as θk+1=θk−˜Dk˜αk,
where
˜Dk=diag(dk
1,dk
2, . . . ,dk
L) (8)
is a block diagonal matrix and ˜αk∈RLis the learning rate. The approximate second Taylor
expansion of h(θ−˜D˜α)at iteration kis
h(θk−˜Dk˜αk)≈h(θk)− ⟨˜Dk˜αk,∇θh(θk)⟩+1
2(˜Dk˜αk)⊤Hk˜Dk˜αk, (9)
where Hk=diag(Hk
1,Hk
2, . . . ,Hk
L)andHk
l=∇2
θlh(θk)forl∈[L]and the Hessian matrix is
approximated by the block diagonal matrix Hk. Taking the derivative of hwith respect to ˜αkand
letting it equal to 0, we get (˜Dk)⊤∇θh(θk) = ( ˜Dk)⊤H⊤˜Dk˜αk, which yields that
αk=diag(Θ1,Θ2, . . . , ΘL)−1((dk
1)⊤∇θ1h(θk),(dk
2)⊤∇θ2h(θk), . . .(dk
L)⊤∇θLh(θk))⊤(10)
ifΘlis nonzero, where Θl= (dk
1)⊤Hk
1(dk
1)forl∈[L].
If a same learning rate is used for all layers, as the same way of computing the adaptive layer-wise
learning rate, we can get
α=(dk)⊤∇θh(θk)
(dk)⊤Hkdk. (11)
Theorem 1. Letg(θ)andgL(θ)be the approximate second Taylor expansions of h(θ−αd)and
h(θ−˜D˜α)as given in (9), where ˜D∈Rn×L,˜α∈RLandα∈Rare given by (8), (10) and (11)
respectively, then we have gL(θ)≤g(θ).
Proof. The proof is given in the appendix.
By Theorem 1, we know that the adaptive layer-wise learning rate may lead to a faster decline in terms
of function values. In our proposed algorithm, we choose dk= (Fk)−1∇θh(θk), where Fkis the
Fisher information matrix and can be seen as a approximation of the Hessian matrix. Then, the Fisher
information matrix is approximated by a block diagonal matrix each block matrix is approximated by
the Kronecker product of two factor matrices. In each layer, the update direction dk
lis scaled by a
layer-wise damping learning rate αk
laccording to (10), which is given by
αk
l=(dk
l)⊤∇θh(θk
l)
(dk
l)⊤Fk
ldk
l+µ, (12)
where µ > 0is a parameter. Using this adaptive layer-wise learning rate can accelerate layers
with smaller gradients. Moreover, this approach can also avoid computing the inverse matrix in
element-wise learning rate (13) and remain computationally efficient.
63.3 Algorithm Schema
To effectively apply LNGD in training DNNs, several certain techniques need to be employed. In
this section, we primarily focus on introducing the damping technique, which is a commonly used in
second-order methods. Meanwhile, a simple method can be used to compute the adaptive layer-wise
learning rate according to Eq. (12) since the cost of computing (dk
l)⊤Fk
ldk
lis relatively expensive.
Finally, we discuss the utilization of exponential moving averages to enhance the training process.
A new damping technique : Damping plays a crucial role in second-order optimization methods.
Large damping can weaken the effect of curvature matrix, while small damping may cause computa-
tional difficulty and inaccuracy since most eigenvalues of the Fisher information matrix are close
to zero and only a small number of eigenvalues take on large values. To make training stable, we
propose the following damping for the lth layer: λl= min(max( tr(Fl)/dl, ν1), ν2), where ν1and
ν2are two constants to constrain the minimum and maximum of damping, and dlis the number
of weight parameters. In our method, Flis approximated as the Kronecker product of two factors
ΦlandΨl, so we add the damping to each factors by ˆΦl=Φl+λΦ
landˆΨl=Ψl+λΨ
l, where
λΦ
l= min(max( tr(Φl)/n, ν 1), ν2)andλΨ
l= min(max( tr(Ψl)/n, ν 1), ν2).
Compute the learning rate the faster : In order to compute the adaptive layer-wise learning rate
given in Eq. (12) more quickly, we turn matrix computation into vector computation. Specifically,
(dk
l)⊤Fk
ldk
l= (dk
l)⊤E
(x,y∼p(x,y))[DθlDθ⊤
l]dk
l= E
(x,y∼p(x,y))[(dk
l)⊤DθlDθ⊤
ldk
l]
= E
(x,y∼p(x,y))[((dk
l)⊤Dθl)2]≈1
N[((dk
l)⊤Dθl)2],(13)
where Fk
lis the empirical Fisher information matrix and Nis the number of samples. The empir-
ical version of Fisher information matrix with no need for sampling from the model’s prediction
distribution, making it more computationally efficient.
Exponential moving averages : In line with previous studies, we incorporate exponential moving
averages into our approach. This involves updating the estimate by combining the previous estimate,
weighted by ϵ, with the estimate calculated from the new mini-batch, weighted by 1−ϵ. That is
ˆΦk+1
l←ϵˆΦk+1
l+ (1−ϵ)ˆΦkandˆΨk+1
l←ϵˆΨk+1
l+ (1−ϵ)ˆΨk
l. (14)
In summary, our proposed algorithm is shown in Algorithm1.
3.4 Convergence Analysis
In this subsection, we give the convergence analysis of LNGD. Following the model used in previous
works about analysing the gradient descent [ 31,32,33] and NGD [ 34,35], we consider a two-layer
neural network activated by the ReLU function with mneurons in the hidden layer as follows:
f(θ, a,x) =1√mmX
r=1arφ(θ⊤
rx),
where θ1,θ2, . . . ,θm∈Rdare the weight vectors of the first layer, x∈Rdis the input, ar∈Ris the
weight of unit rin the second layer and φ(·)is the ReLU activation function, i.e., φ(x) = max {0, x}.
Letv= [f(θ, a,xi), f(θ, a,x2), . . . , f (θ, a,xn)]⊤. In the following, we only give the result of
convergence of Algorithm 1, the specific proof, which uses some conclusions in [ 36,37,38,39], is
given in the appendix.
Theorem 2. (Convergence rate of LNGD) Under the Assumption 1 and the assumption that
rank(X) = d. If we set the number of hidden units m= Ω
n4κ8
ZX,G
ν2ε3λ4
G
, we i.i.d initialize
θr∼N(0, νI),ar∼unif[{−1,+1}]for any r∈[m], and we set the step size α≤(1−2c)
(1+c)2.
Then with probability at least 1−εover the random initialization, we have for k= 0,1,2, . . .
∥y−vk∥2
2≤(1−α)k∥y−v0∥2
2.
Proof. The proof is given in the appendix.
7Algorithm 1 LNGD
Require: learning rate α, learning rate parameter µ, damping parameter λ, damping constraints
ν1,ν2, momentum parameter τ, exponential moving average parameter ϵ, Fisher information
matrix and its inverse update intervals TFIMandTINV.
1:k←0,m←0. Initialize ˆΦlandˆΨlfor any l∈[L].
2:while convergence is not reached do
3: Select a new mini-batch
4: for all l∈[L]do
5: ifk≡0(mod TFIM)then
6: Update the factors ˆΦlandˆΨlusing Eq. (14)
7: end if
8: ifk≡0(mod TINV)then
9: Compute the inverses of ˆΦlandˆΨl
10: end if
11: Compute ∇θlh(θ)using backpropagation
12: Compute the approximated natural gradient (ˆΦ−1
l⊗ˆΨ−1
l)∇θlh(θ)
13: Compute the adaptive learning rate αlusing Eq. (12)
14: ζ← −ααl(ˆΦ−1
l⊗ˆΨ−1
l)∇θlh(θ)
15: m←τm+ζ(Update momentum)
16: θl←θl+m(Update parameters)
17: end for
18: k←k+ 1
19:end while
20:return θ
4 Experiments
In order to verify the effectiveness of the proposed optimizer, we apply the optimizer to both image
classification and machine translation tasks. We first present the optimization performance of our
optimizer by comparing with several baselines. Then, we pay attention to the contribution of different
modules of our optimizer by conducting elaborate ablation analysis, which is given in the appendix.
Unless otherwise stated, the batch size for all experiments in the following is set to 256. The
initial learning rate hyperparameters for all optimizers are tuned using a grid search with values
α∈ {1e−4,3e−4, . . . , 1,3}. The damping parameter λin KFAC[ 14] are tuned using a grid search
with values λ∈ {1e−6,1e−4,3e−4,1e−3, . . . , 1e−1,3e−1}. The minimum and maximum
of damping parameters ν1andν2in LNGD are set to 1e−5and1e−2. The moving average
parameter and the momentum correlating with KFAC and LNGD are set to 0.95 and 0.9, respectively.
Furthermore, a weight decay of 0.004 is applied in all optimizers. All experiments run on a single
A100 GPU using TensorFlow. We average the results of 5 runs and the hyper-parameter settings for
these optimizers are the best values randomly searched for many times.
4.1 CIFAR-10 Training
We first report the optimizing performance on CIFAR-10 [ 40], which is a standard task used to
benchmark optimization methods [ 6,41,42,43,44]. Following these previous works, the changes of
testing accuracy and training loss versus time as well as epoch are reported in Fig. 2, and detailed
statistics are shown in Table6. From Fig. 2, it can be observed that LNGD exhibits the most rapid
decline in training loss during the initial epochs and seconds. This suggests that LNGD is effective in
quickly reducing the training loss and reaching convergence. All optimization methods convergent at
around 200 epochs. However, it is observed that second-order optimization methods, such as KFAC
and LNGD, achieve a lower training loss compared to first-order optimization methods like SGD
and Adam. In terms of testing accuracy, as depicted in Fig. 2 (b) and (d), LNGD achieves a top-1
accuracy of 91 %at the fastest rate. It only requires 36 epochs and 189.69 seconds to achieve this
accuracy level. In comparison, as presented in Table6, SGD and ADAM require at least 100 %and
30%more epochs and time, respectively, to achieve similar accuracy. Relative to KFAC, LNGD
reduces the number of epochs and time by around 20 %and 21 %, respectively. Furthermore, as shown
in Table6, LNGD gets the highest final testing accuracy after convergence.
805 01 001 502 000.00.51.01.5Training LossE
poch SGD 
ADAM 
KFAC 
LNGD
01 002 003 004 005 006 000.00.51.01.5Training LossT
ime/Seconds SGD 
ADAM 
KFAC 
LNGD
05 01 001 502 005060708090Testing AccuracyE
poch SGD 
ADAM 
KFAC 
LNGD 
91
01 002 003 004 005 006 005060708090Testing AccuracyT
ime/Seconds SGD 
ADAM 
KFAC 
LNGD 
91Figure 2: Numerical performance on ResNet-18 with CIFAR-10.
neural network activated by the ReLU function with mneurons in the hidden layer as follows:
f(θ, a,x) =1√mm/summationdisplay
r=1arφ(θ⊤
rx),
where θ1,θ2, . . . ,θm∈Rdare the weight vectors of the first layer, x∈Rdis the input, ar∈Ris the
weight of unit rin the second layer and φ(·)is the ReLU activation function, i.e., φ(x) = max {0, x}.
Letv= [f(θ, a,xi), f(θ, a,x2), . . . , f (θ, a,xn)]⊤. In the following, we only give the result of
convergence of Algorithm 2, the specific proof is given in the appendix.
Theorem 2. (Convergence rate of LNGD) Under the Assumption 1 and the assumption that
rank(X) = d. If we set the number of hidden units m= Ω/parenleftbigg
n4κ8
ZX,G
ν2ε3λ4
G/parenrightbigg
, we i.i.d initialize
θr∼N(0, νI),ar∼unif[{−1,+1}]for any r∈[m], and we set the step size α≤(1−2c)
(1+c)2.
Then with probability at least 1−εover the random initialization, we have for k= 0,1,2, . . .
∥y−vk∥2
2≤(1−α)k∥y−v0∥2
2.
Proof. The proof is given in the appendix.
4 Experiments
In order to verify the effectiveness of the proposed optimizer, we apply the optimizer to both image
classification and machine translation tasks. We first present the optimization performance of our
optimizer by comparing with several baselines. Then, we pay attention to the contribution of different
modules of our optimizer by conducting elaborate ablation analysis, which is given in the appendix.
Unless otherwise stated, the batch size for all experiments in the following is set to 256. The
initial learning rate hyperparameters for all optimizers are tuned using a grid search with values
α∈ {1e−4,3e−4, . . . , 1,3}. The damping parameter λin KFAC[ 14] are tuned using a grid search
with values λ∈ {1e−6,1e−4,3e−4,1e−3, . . . , 1e−1,3e−1}. The minimum and maximum
of damping parameters ν1andν2in LNGD are set to 1e−5and1e−2. The moving average
parameter and the momentum correlating with KFAC and LNGD are set to 0.95 and 0.9, respectively.
Furthermore, a weight decay of 0.004 is applied in all optimizers. All experiments run on a single
A100 GPU using TensorFlow. We average the results of 5 runs and the hyper-parameter settings for
these optimizers are the best values randomly searched for many times.
4.1 CIFAR-10 Training
We first report the optimizing performance on CIFAR-10, which is a standard task used to benchmark
optimization methods [ 6,36,37,38]. Following these previous works, the changes of testing accuracy
and training loss versus time as well as epoch are reported in Fig. 2, and detailed statistics are shown
in Table.1. From Fig. 2, it can be observed that LNGD exhibits the most rapid decline in training loss
during the initial epochs and seconds. This suggests that LNGD is effective in quickly reducing the
training loss and reaching convergence. All optimization methods convergent at around 200 epochs.
However, it is observed that second-order optimization methods, such as KFAC and LNGD, achieve
a lower training loss compared to first-order optimization methods like SGD and Adam. In terms
of testing accuracy, as depicted in Fig. 2 (b) and (d), LNGD achieves a top-1 accuracy of 91 %at
8Figure 2: Numerical performance on ResNet-18 with CIFAR-10.
Table 1: Detailed statistics on CIFAR-10 when top-1 testing accuracy achieves 91%.
Epoch Total Time Time Per Epoch Acceleration Best Test Acc
SGD 79 268.67s 3.4s 29% 91.88%
ADAM 72 248.83s 3.77s 23% 92.62%
KFAC 45 241.86s 5.87s 21% 93.34%
LNGD 36 189.69s 5.08s 93.61%
05 01 001 502 000.00.51.01.5Training LossE
poch SGD 
ADAM 
KFAC 
LNGD
05 01 001 502 000.00.51.01.5Training LossE
poch SGD 
ADAM 
KFAC 
LNGD
05 01 001 502 000.00.51.01.5Training LossE
poch SGD 
ADAM 
KFAC 
LNGD
05 01 001 502 000.00.51.01.5Training LossE
poch SGD 
ADAM 
KFAC 
LNGDFigure 3: Comparison of the exact Fisher information matrix Fand our approximation FLNGD . We
use LNGD to train MNIST on a fully-connected neural network, whose architecture is 196-20-20-20-
20-10. We show the results of the Fisher information matrix of the first layer with 20 units in top,
which is a 400×400matrix. The bottom portion displays partially enlarged parts of the top marked
with red square, which is a 40×40matrix. Within both the top and bottom sections, on the left is the
exact Fisher information matrix F, in the middle is our approximation FLNGD , and on the right is the
absolute error between them. The brightness levels correspond to the sizes of the absolute values.
01 02 03 04 05 06 07 08 01.01.52.02.53.03.54.04.5Training LossE
poch SGD  
ADAM 
KFAC  
LNGD
01 00002 00003 00004 00001.01.52.02.53.03.54.04.5Training LossT
ime/Seconds SGD  
ADAM 
KFAC  
LNGD
01 02 03 04 05 06 07 08 020304050607080 
SGD 
ADAM 
KFAC 
LNGD 
75.9Testing AccuracyE
poch
01 00002 00003 00004 000020304050607080Testing AccuracyT
ime/Seconds SGD 
ADAM 
KFAC 
LNGD 
75.9
Figure 4: Numerical performance on ResNet-50 with ImageNet.
the fastest rate. It only requires 36 epochs and 189.69 seconds to achieve this accuracy level. In
comparison, as presented in Table.1, SGD and ADAM require at least 100 %and 30 %more epochs
and time, respectively, to achieve similar accuracy. Relative to KFAC, LNGD reduces the number of
epochs and time by around 20 %and 21 %, respectively. Furthermore, as shown in Table.1, LNGD
gets the highest fincal testing accuracy after convergence.
Table 1: Detailed statistics on CIFAR-10 when top-1 testing accuracy achieves 91%.
Epoch Total Time Time Per Epoch Acceleration Best Test Acc
SGD 79 268.67s 3.4s 29% 91.88%
ADAM 72 248.83s 3.77s 23% 92.62%
KFAC 45 241.86s 5.87s 21% 93.34%
LNGD 36 189.69s 5.08s 93.61%
4.2 ImageNet Training
Table 2: Detailed statistics on ImageNet when top-1 testing accuracy achieves 75.9%.
Epoch Total Time Time Per Epoch Acceleration Best Test Acc
SGD 78 11.28h 520.55s 43% 76.47%
ADAM - - - - 74.05%
KFAC 39 8.02h 739.93s 19% 76.58%
LNGD 36 6.46h 646.44s 76.73%
We extend our examination of optimizer efficacy to a larger image classification dataset, ImageNet-1K
[39]. The changes of testing accuracy and training loss versus time and epoch are reported in Fig.4
and Table.2. The results show that the LNGD optimizer is highly efficient in training large image
datasets in terms of both speed and accuracy. LNGD, which requires only 36 epochs and 6.46
hours, is much faster in achieving the top-1 testing accuracy of 75.9 %than other baselines. This is a
significant improvement over SGD, which takes 100 %more epochs and 75 %more time to reach the
same accuracy level. As for Adam, it exhibits a rapid decrease in loss during training and reaches
convergence at a fast rate. However, the best achieved testing accuracy is only 74.05 %, indicating
9
Figure 3: Numerical performance on ResNet-50 with ImageNet.
4.2 ImageNet Training
We extend our examination of optimizer efficacy to a larger image classification dataset, ImageNet-1K
[45]. The changes of testing accuracy and training loss versus time and epoch are reported in Fig.3
and Table2. The results show that the LNGD optimizer is highly efficient in training large image
datasets in terms of both speed and accuracy. LNGD, which requires only 36 epochs and 6.46
hours, is much faster in achieving the top-1 testing accuracy of 75.9 %than other baselines. This is a
significant improvement over SGD, which takes 100 %more epochs and 75 %more time to reach the
same accuracy level. As for Adam, it exhibits a rapid decrease in loss during training and reaches
convergence at a fast rate. However, the best achieved testing accuracy is only 74.05 %, indicating
that when training large-scale image tasks, a trade-off between efficiency and effectiveness needs
to be considered. Compared to KFAC, although LNGD is better for only 3 epochs, it leads to 19 %
reduction in terms of the computing time. The training loss results further support the efficiency of
LNGD, as it maintains the fastest rate of decline during the initial stages of training and ultimately
yields the lowest training loss upon convergence. Overall, the results suggest that LNGD is a highly
efficient optimizer for large-scale image classification tasks, providing faster convergence and better
accuracy than other commonly used optimizers.
4.3 Transformer Training
In this experiment, we apply LNGD to the Transformer-Big model [ 46] with 213.7M parameters.
The training datasets is WMT English-German machine translation corpus [ 46]. We use Bleu [ 47] as
the evaluation metrics, which is frequently used in machine translation tasks. The setting of learning
rate updating strategy for SGD, Adam, KFAC and LNGD are the same as in ImageNet training.
In Fig.4 and Table3, we present the comparative evaluation of the performance of LNGD against
SGD, Adam, and KFAC in terms of testing accuracy and training loss. ADAM demonstrates superior
performance over SGD, as evidenced by a more rapid decrease in training loss and a lower converged
loss value. This observation aligns with previous empirical findings that ADAM is highly effective
for transformer models. KFAC exhibits further enhancements in performance compared to Adam,
9Table 2: Detailed statistics on ImageNet when top-1 testing accuracy achieves 75.9%.
Epoch Total Time Time Per Epoch Acceleration Best Test Acc
SGD 78 11.28h 520.55s 43% 76.47%
ADAM - - - - 74.05%
KFAC 39 8.02h 739.93s 19% 76.58%
LNGD 36 6.46h 646.44s 76.73%
02 00004 00006 00008 00001 000001234Training LossS
tep SGD 
ADAM 
KFAC 
LNGD
01 00002 00003 00004 00005 00001234Training LossT
ime/Seconds SGD 
ADAM 
KFAC 
LNGD
02 00004 00006 00008 00001 0000020253035Testing BleuS
tep SGD 
ADAM 
KFAC 
LNGD 
32
01 00002 00003 00004 00005 000020253035Testing BleuT
ime/Seconds SGD 
ADAM 
KFAC 
LNGD 
32Figure 5: Numerical performance on Transformer with WMT.
that when training large-scale image tasks, a trade-off between efficiency and effectiveness needs
to be considered. Compared to KFAC, although LNGD is better for only 3 epochs, it leads to 19 %
reduction in terms of the computing time. The training loss results further support the efficiency of
LNGD, as it maintains the fastest rate of decline during the initial stages of training and ultimately
yields the lowest training loss upon convergence. Overall, the results suggest that LNGD is a highly
efficient optimizer for large-scale image classification tasks, providing faster convergence and better
accuracy than other commonly used optimizers.
4.3 Transformer Training
In this experiment, we apply LNGD to the Transformer-Big model [ 40] with 213.7M parameters.
The training datasets is WMT English-German machine translation corpus [ 40]. We use Bleu [ 41] as
the evaluation metrics, which is frequently used in machine translation tasks. The setting of learning
rate updating strategy for SGD, Adam, KFAC and LNGD are the same as in ImageNet training.
In Fig.5 and Table.3, we present the comparative evaluation of the performance of LNGD against
SGD, Adam, and KFAC in terms of testing accuracy and training loss. ADAM demonstrates superior
performance over SGD, as evidenced by a more rapid decrease in training loss and a lower converged
loss value. This observation aligns with previous empirical findings that ADAM is highly effective
for transformer models. KFAC exhibits further enhancements in performance compared to Adam,
yet it does not surpass the efficacy of LNGD. LNGD outperforms its counterparts with the swiftest
reduction in training loss and the highest convergence rates. In terms of testing accuracy, measured
by the Bleu score, LNGD achieves a top-1 Bleu score of 32 %with remarkable efficiency, which is
able to reduce the required steps by approximately 24 %and computing time by 16 %compared to
Adam. When compared to KFAC, LNGD still shows significant improvements, reducing the steps by
around 14 %and computing time by 24 %. As for SGD, it cannot reach the top-1 Bleu score of 32 %
and the best testing accuracy is only 31.8 %, which indicates that SGD is not a good choice for large
language processing tasks. In summary, the results provide strong evidences for the effectiveness of
LNGD as an optimization algorithm for transformer models and shed light for large practical NLP
tasks where time and computational resources are quite limited.
Table 3: Detailed statistics on WMT when Bleu achieves 32%.
Step Total Time Time Per 1K Acceleration Best Test Bleu
SGD - - - - 31.87%
ADAM 25K 3.39h 488.16s 16% 33.05%
KFAC 22K 3.75h 613.63s 24% 33.45%
LNGD 19K 2.85h 540s 33.55%
5 Conclusion
In summary, we propose a novel NGD optimizer named as LNGD for training DNNs, specifically
targeting the computational inefficiencies that impede the practical application of conventional natural
gradient techniques in large-scale neural networks. Our approach strategically computes Fisher
information matrices for each individual layers using sample approximation and dynamically adjusts
learning rates leveraging curvature information. This method facilitates a more refined representation
of the optimization landscape at the layer level. Besides, we provide convergence analysis of LNGD.
10
Figure 4: Numerical performance on Transformer with WMT.
yet it does not surpass the efficacy of LNGD. LNGD outperforms its counterparts with the swiftest
reduction in training loss and the highest convergence rates. In terms of testing accuracy, measured
by the Bleu score, LNGD achieves a top-1 Bleu score of 32 %with remarkable efficiency, which is
able to reduce the required steps by approximately 24 %and computing time by 16 %compared to
Adam. When compared to KFAC, LNGD still shows significant improvements, reducing the steps by
around 14 %and computing time by 24 %. As for SGD, it cannot reach the top-1 Bleu score of 32 %
and the best testing accuracy is only 31.8 %, which indicates that SGD is not a good choice for large
language processing tasks. In summary, the results provide strong evidences for the effectiveness of
LNGD as an optimization algorithm for transformer models and shed light for large practical NLP
tasks where time and computational resources are quite limited.
Table 3: Detailed statistics on WMT when Bleu achieves 32%.
Step Total Time Time Per 1K Acceleration Best Test Bleu
SGD - - - - 31.87%
ADAM 25K 3.39h 488.16s 16% 33.05%
KFAC 22K 3.75h 613.63s 24% 33.45%
LNGD 19K 2.85h 540s 33.55%
5 Conclusion
In summary, we propose a novel NGD optimizer named as LNGD for training DNNs, specifically
targeting the computational inefficiencies that impede the practical application of conventional natural
gradient techniques in large-scale neural networks. Our approach strategically computes Fisher
information matrices for each individual layers using sample approximation and dynamically adjusts
learning rates leveraging curvature information. This method facilitates a more refined representation
of the optimization landscape at the layer level. Besides, we provide convergence analysis of LNGD.
Experimental evaluations indicate its competitive performance relative to existing state-of-the-art
optimizers. This work hold significant potential for enhancing the efficiency and scalability of training
processes in deep learning frameworks.
References
[1]Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of
Mathematical Statistics , pages 400–407, 1951.
[2]Yu E Nesterov. A method for solving the convex programming problem with convergence rate
o(1
k2). InDoklady Akademii Nauk SSSR , volume 269, pages 543–547, 1983.
10[3]Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks ,
12(1):145–151, 1999.
[4]Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 ,
2012.
[5]Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning , 4(2):26–
31, 2012.
[6]Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-
tional Conference on Learning Representations , 2014.
[7]Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon
Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in
observed gradients. Advances in neural information processing systems , 33:18795–18806,
2020.
[8]Nitish Shirish Keskar and Albert S Berahas. ADAQN: An adaptive quasi-Newton algorithm for
training rnns. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases , pages 1–16, 2016.
[9]Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney.
AdaHessian: An adaptive second order optimizer for machine learning. In proceedings of the
AAAI conference on artificial intelligence , volume 35, pages 10665–10673, 2021.
[10] Donald Goldfarb, Yi Ren, and Achraf Bahamou. Practical quasi-Newton methods for training
deep neural networks. Advances in Neural Information Processing Systems , 33:2386–2396,
2020.
[11] James Martens. Deep learning via Hessian-free optimization. In International Conference on
Learning Representations , volume 27, pages 735–742, 2010.
[12] Oriol Vinyals and Daniel Povey. Krylov subspace descent for deep learning. In Artificial
Intelligence and Statistics , pages 1261–1268. PMLR, 2012.
[13] Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation , 10(2):251–
276, 1998.
[14] James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored
approximate curvature. In International conference on machine learning , pages 2408–2417,
2015.
[15] Kaixin Gao, Xiaolei Liu, Zhenghai Huang, Min Wang, Zidong Wang, Dachuan Xu, and Fan Yu.
A trace-restricted Kronecker-factored approximation to natural gradient. In Proceedings of the
AAAI Conference on Artificial Intelligence , volume 35, pages 7519–7527, 2021.
[16] Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Chuan-Sheng Foo, and Rio Yokota.
Scalable and practical natural gradient for large-scale deep learning. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 44(1):404–415, 2020.
[17] Minghan Yang, Dong Xu, Qiwen Cui, Zaiwen Wen, and Pengxiang Xu. An efficient Fisher
matrix approximation method for large-scale neural network optimization. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 45(5):5391–5403, 2022.
[18] James Martens. New insights and perspectives on the natural gradient method. Journal of
Machine Learning Research , 21:1–76, 2020.
[19] Roger Grosse and James Martens. A Kronecker-factored approximate Fisher matrix for convo-
lution layers. In International Conference on Machine Learning , pages 573–582, 2016.
[20] James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations
for recurrent neural networks. In International Conference on Learning Representations , 2018.
11[21] Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient
as variational inference. In International conference on machine learning , pages 5847–5856,
2018.
[22] Thomas George, César Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast
approximate natural gradient descent in a Kronecker factored eigenbasis. In Advances in Neural
Information Processing Systems , pages 9550–9560, 2018.
[23] Kaixin Gao, Zheng-Hai Huang, Xiaolei Liu, Min Wang, Shuangling Wang, Zidong Wang,
Dachuan Xu, and Fan Yu. Eigenvalue-corrected natural gradient based on a new approximation.
Asia-Pacific Journal of Operational Research , 40(01):2340005, 2023.
[24] Mengyun Chen, Kaixin Gao, Xiaolei Liu, Zidong Wang, Ningxi Ni, Qian Zhang, Lei Chen,
Chao Ding, Zhenghai Huang, Min Wang, et al. THOR, trace-based hardware-driven layer-
oriented natural gradient descent computation. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 35, pages 7046–7054, 2021.
[25] Lin Zhang, Shaohuai Shi, Wei Wang, and Bo Li. Scalable K-FAC training for deep neural
networks with distributed preconditioning. IEEE Transactions on Cloud Computing , 11(3):2365–
2378, 2023.
[26] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 , 2013.
[27] Krishnakumar Balasubramanian, Pinar Donmez, and Guy Lebanon. Unsupervised supervised
learning ii: Margin-based classification without labels. In Proceedings of the Fourteenth Inter-
national Conference on Artificial Intelligence and Statistics , pages 137–145. JMLR Workshop
and Conference Proceedings, 2011.
[28] Frédéric de Gournay and Alban Gossard. Adaptive scaling of the learning rate by second order
automatic differentiation. arXiv preprint arXiv:2210.14520 , 2022.
[29] Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, and Gavin Taylor. Layer-
specific adaptive learning rates for deep networks. In 2015 IEEE 14th International Conference
on Machine Learning and Applications (ICMLA) , pages 364–368. IEEE, 2015.
[30] Boris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin,
Ryan Leary, Jason Li, Huyen Nguyen, Yang Zhang, and Jonathan M Cohen. Training deep
networks with stochastic gradient normalized by layerwise adaptive second moments. 2019.
[31] Simon S. Du, Xiyu Zhai, Barnabás Póczos, and Aarti Singh. Gradient descent provably
optimizes over-parameterized neural networks. In 7th International Conference on Learning
Representations , 2019.
[32] Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. In Proceedings of the 36th International Conference
on Machine Learning , volume 97, pages 1675–1685, 2019.
[33] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Proceed-
ings of the 36th International Conference on Machine Learning , volume 97, pages 322–332,
2019.
[34] Guodong Zhang, James Martens, and Roger Grosse. Fast convergence of natural gradient
descent for over-parameterized neural networks. In Advances in Neural Information Processing
Systems , volume 32, 2019.
[35] Ryo Karakida and Kazuki Osawa. Understanding approximate Fisher information for fast
convergence of natural gradient descent in wide neural networks. In Advances in Neural
Information Processing Systems , volume 33, pages 10891–10901, 2020.
[36] Alberto Bernacchia, Máté Lengyel, and Guillaume Hennequin. Exact natural gradient in deep
linear networks and its application to the nonlinear case. In Advances in Neural Information
Processing Systems , pages 5945–5954, 2018.
12[37] Joel A Tropp. An introduction to matrix concentration inequalities. arXiv preprint
arXiv:1501.01571 , 2015.
[38] Jssai Schur. Bemerkungen zur theorie der beschränkten bilinearformen mit unendlich vielen
veränderlichen. Journal für die reine und angewandte Mathematik , 1911(140):1–28, 1911.
[39] Shuangzhe Liu and Gotz Trenkler. Hadamard, Khatri-Rao, Kronecker and other matrix products.
International Journal of Information and Systems Sciences , 4(1):160–177, 2008.
[40] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
[41] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training
imagenet in 1 hour. arXiv preprint arXiv:1706.02677 , 2017.
[42] Peng Xu, Fred Roosta, and Michael W Mahoney. Second-order optimization for non-convex ma-
chine learning: An empirical study. In Proceedings of the 2020 SIAM International Conference
on Data Mining , pages 199–207. SIAM, 2020.
[43] Yuchao Li, Shaohui Lin, Jianzhuang Liu, Qixiang Ye, Mengdi Wang, Fei Chao, Fan Yang,
Jincheng Ma, Qi Tian, and Rongrong Ji. Towards compact cnns via collaborative compression.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 6438–6447, 2021.
[44] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
[45] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009.
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics , pages 311–318, 2002.
13A Notations
In this paper, we denote [n] ={1,2, . . . , n }. For a matrix A, we use Aijto denote its (i, j)-th
entry, tr(A)to denote its trace, σmin(A)andσmax(A)to denote its smallest and largest singular
value, ∥A∥Fto denote its Frobenius norm and ∥A∥2to denote its spectral norm. If Ais positive
semi-definite, λmin(A)andλmax(A)denote its smallest and largest eigenvalue, and define κA=
λmax(A)/λmin(A), respectively. The identity matrix is denoted as I. For a vector a,∥a∥2denotes
the Euclidean norm. We use N(µ,Σ)to denote the Gaussian distribution with mean µand covariance
Σ. For two matrices
A= [a1,a2, . . . ,an]∈Rp×q,B= [b1,b2, . . . ,bn]∈Rp×q,
we use ◦and⊗to denote the Hadamard and Kronecker product, respectively. The column-wise
Khatri-Rao product ∗is defines as
A∗B= [a1⊗b1,a2⊗b2, . . . ,an⊗bn]∈Rp2×q.
Similarly, we can define the row-wise Khatri-Rao product ⋆and we have (A⋆B)⊤= (A⊤∗B⊤).
Given an event E,I{E}denotes its indicator function, i.e.,
I{E}=
1,ifEhappens,
0,otherwise.
B Comparisons and Explanations
B.1 Comparisons with Related Works
There have been some works to utilize the NGD or its approximations for training DNNs. One of
the primary computational challenges lie in the storage and computing the inverse of the Fisher
information matrix during NGD optimization. Recently, several studies have explored the adoption
of the efficient Kronecker-factored approximation to the Fisher information matrix to address this
computational challenge. The most related approaches to this work are the KFAC, EKFAC and
TKFAC[ 15]. These works and LNGD all start with a block-diagonal approximation of the Fisher
information matrix. The differences among them are the approximations of the block matrix Fl. By
approximating the expectation of the Kronecker product as the Kronecker product of expectations,
KFAC approximates Flas the Kronecker product of A=E[(al−1a⊤
l−1)]andB=E[(ˆglˆg⊤
l)]with
ˆgl=−∇sllogp(y|x). By tracking the diagonal variance in the Kronecker-factored eigenbasis,
EKFAC performs eigenvalue decomposition of the Fisher information matrix and re-scales the
eigenvalues by S∗to achieve a better approximation, where S∗is a diagonal matrix defined by
S∗
ii=E[(U⊤
B∇θh(θ)2)i], andUA,UBare eigenvectors of A,B. TKFAC approximates Flas a
Kronecker product of two factors PandQscaled by a coefficient δand keep the traces of each block
equal. In this paper, we propose the LNGD, which approximates Flas a Kronecker product of a
matrix Φland a diagonal matrix Ψl, which is computed by sampling from each layer. We summarize
the above approximations of Flin Table 4.
Table 4: Summary of some NGD optimizers
Optimizer Fl
KFAC A⊗B
EKFAC (UA⊗UB)S∗(UA⊗UB)⊤
TKFAC δP⊗Q
LNGD Φ⊗Ψ
When these methods have the same update frequency, KFAC needs to compute two factor matrices A
andB, and then invert them. However, Bcan only be computed after completely performing back-
propagation. On the other hand, EKFAC modifies KFAC by incorporating eigenvalue decomposition
to scale the eigenvalue during the inversion process. TKFAC, another variant of KFAC, maintains
the equality of traces of matrices before and after approximation. Both EKFAC and TKFAC involve
increased computational requirements compared to KFAC. Our proposed LNGD also requires
computation and inversion of two factors ΦandΨ. However, the advantage of LNGD is that the
140 5 10 15 20 25 30 350
5
10
15
20
25
30
35(a)
0 5 10 15 20 25 30 350
5
10
15
20
25
30
35 (b)
0 5 10 15 20 25 30 350
5
10
15
20
25
30
35 (c)
0 5 10 15 20 25 30 350
5
10
15
20
25
30
35
(d)
0 5 10 15 20 25 30 350
5
10
15
20
25
30
35 (e)
0 5 10 15 20 25 30 350
5
10
15
20
25
30
35 (f)
Figure 5: Comparison of the exact Fisher information matrix and the approximated Fisher information
matrix of KFAC and LNGD. On the left is the exact Fisher information matrix, in the middle is the
approximated Fisher information matrix, and on the right is the absolute error of these. The first row
shows the result of KFAC, and the second row shows the results of LNGD.
matrix Ψis diagonal, and due to the use of hierarchical sampling, Ψcan be directly inverse during
the forward propagation phase, without the need to wait for the completion of the back-propagation
process. This feature significantly reduces computational time.
In addition, THOR proposed in [ 24] also provides an optimizer using NGD for training DNNs. THOR
mainly considers reducing the computational cost of NGD through two aspects. On the one hand,
THOR gradually increases the updating frequency of the inverse matrix of Fisher information matrix
and proposes a trace-based updating rule for the Fisher information matrix of each layer. On the other
hand, THOR approximates the approximated Fisher information matrix obtained by KFAC as some
smaller matrices by splitting matrix dimensions. Our proposed LNGD first gives a layer-wise sample
method to more efficiently compute each block matrix corresponding to each layer and proposes a
novel approximate scheme of the Fisher information matrix. Furthermore, LNGD also adopts an
adaptive layer-wise learning rate to speed up training. The contributions and ideas of our proposed
LNGD are different from THOR.
B.2 Comparisons Between KFAC and LNGD
Fig. 5 shows the visualization results of KFAC and LNGD. From Fig. 5 (b) and (e), we can see that
KFAC and LNGD can all emphasize the importance of the diagonal elements in the exact Fisher
information matrix. In addition, it can also be seen clearly that KFAC still retains some elements
near the main diagonal, while LNGD does not, which also reflects that LNGD provides an efficient
approximation of Fisher information matrix with less computational cost in comparison with KFAC.
B.3 Illustration of the Gaussian Distribution Assumption
In Subsection 3.1, we assume that the predictive distribution Pal|al−1(θl)follows Gaussian distri-
bution. To illustrate the validity of the Gaussian distribution assumption, we collect the output of
two layers of the ResNet-18 network on CIFAR-10 and show the results in Fig. 6. Fig. 6 (a) and (b)
show the distributions of sample representation vectors’ values in some dimension. Since we use the
ReLU activation function, the obtained distributions are in accord with the Gaussian distribution in
the positive quadrant. Fig. 6 (c) and (d) show the distributions of values of sample representation
vectors’ Euclidean norm, from which we can see that the two distributions can also be approximated
as Gaussian distributions.
15valuescount(a)
valuescount (b)
valuescount (c)
valuescount (d)
Figure 6: Illustration of Gaussian distribution.
C Proof of Theorem 1
Proof. Since g(θ)andgL(θ)are the approximate second Taylor expansions of h(θ−αd)and
h(θ−˜D˜α), we have
gL(θ)−g(θ) =h(θ)−α⟨d,∇θh(θ)⟩+1
2α2d⊤Hd−h(θ) +LX
l=1˜αl⟨dl,∇θlh(θ)⟩
−LX
l=11
2˜α2
ld⊤
lHldl=1
2 LX
l=1(d⊤
l∇θlh(θ))2
d⊤
lHldl−(d⊤∇θh(θ))2
d⊤Hd!
≤0.
This completes the proof.
D Convergence of LNGD
In this section, we give the convergence analysis of LNGD. Following the model used in previous
works about analysing the gradient descent [ 31,32,33] and NGD [ 34,35], we consider a two-layer
neural network activated by the ReLU function with mneurons in the hidden layer as follows:
f(θ, a,x) =1√mmX
r=1arφ(θ⊤
rx),
where θ1,θ2, . . . ,θm∈Rdare the weight vectors of the first layer, x∈Rdis the input, ar∈Ris the
weight of unit rin the second layer and φ(·)is the ReLU activation function, i.e., φ(x) = max {0, x}.
For convenience, we define θ= [θ⊤
1,θ⊤
2, . . . ,θ⊤
m]⊤∈Rmd. We first initialize the parameters
randomly by
θr∼N(0, ν2I), a r∼unif[{−1,+1}],∀r∈[m],
where 0< ν≤1controls the magnitude of initialization.
Given the training dataset S={(xi, yi)}n
i=1containing (input, target) examples (xi, yi). Following
[31, 33, 34], we make the following assumption for the data.
Assumption 1. For all i,∥xi∥2
2= 1and|yi|=O(1). For any i̸=j,xi∦xj.
In this subsection, we mainly focus on the mean squared error loss (MSE) function
L(θ) =1
2nnX
i=1(f(θ, a,xi)−yi)2
=1
2nnX
i=1(1√mmX
r=1arφ(θ⊤
rx)−yi)2.
Following [ 31,33,34], we fix the weights of second layer and only optimize the weights of first layer.
Then the update rule of NGD can be written as
θk+1=θk−α(Fk)−1∇θL(θk).
16As shown in [ 18], if the network’s predictive distribution is in the exponential family, the Fisher
information matrix is equivalent to the generalized Gauss-Newton matrix, which is defined by
E(xi,yi)∈S[J⊤
iHLJi],
where HLis the Hessian matrix of the loss function L(θ)with respect to the prediction f(θ, a,xi)
andJiis the Jacobian matrix of f(θ, a,xi)with respect to the parameters θ. Under our setting that
L(θ)is the MSE loss function, the Hessian matrix HLis the identity matrix I.Jican be computed
by
Ji= 
∇θ1f(θ, a,xi)⊤, . . . ,∇θmf(θ, a,xi)⊤⊤,
where
∇θrf(θ, a,xi) =ar√mI{θ⊤
rxi≥0}xi,∀r∈[m]. (15)
LetJ= (J1,J2, . . . ,Jn)⊤∈Rn×md, then the Fisher information matrix can be written as
F=E(xi,yi)∈S[J⊤
iHLJi] =E(xi,yi)∈S[J⊤
iJi] =1
nJ⊤J.
As discussed in [ 34], when m > n , the Fisher information matrix is singular. So in this case, we use
the generalized inverse given in [36]
F†=nJ⊤(JJ⊤)−1(JJ⊤)−1J (16)
and the update rule of NGD can be written as
θk+1=θk−α
n(Fk)†(Jk)⊤(vk−y),
where y = [ y1, y2, . . . , y n]⊤and v = [ v1, v2, . . . , v n]⊤=
[f(θ, a,xi), f(θ, a,x2), . . . , f (θ, a,xn)]⊤. Consider the two-layer neural network described
in this subsection, since we fixed the weights in second layer, and the Fisher information matrix of
this model is approximated by
F≈Φ⊗Ψ.
For simplicity, we ignore the index of layer. Define
X= [x1,x2, . . . ,xn]⊤∈Rn×d
and
G= [φ′(Xθ1), φ′(Xθ2), . . . , φ′(Xθm)]∈Rn×m,
where Xis the input matrix formed by ninput vectors and Gis the pre-activation derivatives matrix
whose entry is given by
Gir=1√mI{x⊤
iθr≥0},∀i∈[n], r∈[m]. (17)
Then we have
J=∇θf(θ, a,xi) =X⋆G∈Rn×md
and
F=1
nJ⊤J=1
n(X⋆G)⊤(X⋆G)∈Rmd×md.
The formulas of ΦandΨcan be given as
Φ=X⊤(GG⊤◦I)X∈Rd×d, (18)
Ψ=1
nG⊤(XX⊤◦I)G
tr((XX⊤◦I)◦(GG⊤◦I))∈Rm×m. (19)
Under the Assumption 1, we have (XX⊤)ii= 1, i∈[d]. Therefore, Eq. (18) and Eq. (19) can be
simplified to
Φ=ZX,G,Ψ=G⊤G
nη,
where η=tr((XX⊤◦I)◦(GG⊤◦I))andZX,G=X⊤(GG⊤◦I)X. Finally, the update rule of
LNGD can be given by
θk+1=θk−α[η(Zk
X,G)−1⊗((Gk)⊤Gk)−1](Jk)⊤(vk−y).
To analyze the global convergence of LNGD, we need the following two conditions as given in [ 34].
17Condition 1. The matrix G0(G0)⊤is positive definite.
To verify this condition, we need the following two lemmas.
Lemma 1. Define K∞
i,j=Eθ∼N(0,ν2I)[I{θ⊤xi≥0,θ⊤xj≥0}], i, j∈[n], then we have the
matrix K∞is strictly positive definite.
The result of this lemma has been given and discussed in [ 34]. We define λK=λmin(K∞)≥0and
matrix Kwhose entry is given by
Kij=1
mmX
r=1I{θ⊤
rxi≥0,θ⊤
rxj≥0}= (GG⊤)ij,∀i, j∈[n].
Then we can show Condition 1 holds by the following lemma.
Lemma 2. Ifm= Ω
n
λKlogn
ε
, we have with probability at least 1−εthatλmin(K(0))≥3
4λK.
Proof. Note that K(0)can be written as the sum of random symmetric matrices
K(0) =mX
r=1K(θr),Kij(θr) =1
mI{θ⊤
rxi≥0, θ⊤
rxj≥0}.
Furthermore, we have K(θr)are positive semi-definite and ∥K(θr)∥2≤tr(K(θr))≤n
m. Thus, by
the matrix Chernoff bound [37], we can obtain
P
λmin(K(0))≤(1−1
4)λK)
≤nexp
−1
42λKm
n
.
Letε=nexp 
−1
42λKm
n
, we have m= Ω
n
λKlogn
ε
. Proof complete.
This proof is similar to the Lemma 6 in [ 34], the difference is the definition of K. For completeness
of the proof, we also give the detailed proof here. By this lemma, we have λmin(G(0)G(0)⊤) =
λmin(K(0))≥3
4λK>0, which implies that Condition 1 holds. Next, we give the other condition.
Condition 2. For all parameters θthat satisfy ∥θ−θ0∥2≤2∥y−v0∥2 √
λK/2κZX,G, there exists 0≤c <1
2
such that
∥J−J0∥2≤√
2c
4√λK
κZX,G.
To show this condition holds, we need the following lemma.
Lemma 3. [34] For all weight vectors θthat satisfy ∥θ−θ0∥2≤R, we have probability at least
1−εthat
∥J−J0∥2
2≤ ∥J−J0∥2
F≤2nR2/3
ν2/3ε2/3m1/3.
By taking R=2∥y−v0∥2 √
λK/2κZX,G, we have ∥J−J0∥2
2≤641/3n∥y−v0∥2/3
2κ2/3
ZX,G
ν2/3ε2/3m1/3λ1/3
K. Therefore, if we let
m= Ω
n3∥y−v0∥2
2κ8
ZX,G
ν2ε2λ4
K
, the Condition 2 holds. What’s more, we have probability at least 1−ε
that∥y−v0∥2
2=O n
ε
, which has been given in [ 31]. Thus we can write the condition of mas
m= Ω
n4κ8
ZX,G
ν2ε3λ4
K
.
Before giving the main result, we first give some necessary lemmas.
Lemma 4. Ifm= Ω
n4κ8
ZX,G
ν2ε3λ4
K
, for all parameters θthat satisfy ∥θ−θ0∥2≤2∥y−v0∥2 √
λG/2κZX,G,
we have probability at least 1−εthatλmin(K)≥p
λK/2.
18Proof. Combine Eq. (15), Eq. (17) and Assumption 1, we have ∥G−G(0)∥2
F≤ ∥J−J0∥2
F. Let
m= Ω
n4κ8
ZX,G
ν2ε3λ4
K
andR=2∥y−v0∥2 √
λK/2κZX,G, by Condition 2 and Lemma 3 we have
∥G−G(0)∥2
2≤ ∥G−G(0)∥2
F≤ ∥J−J0∥2
F
≤c
8λK
κ2
ZX,G≤c
8λK.
Therefore,
σmin(G)≥σmin(G(0))− ∥G−G(0)∥2
≥r
3
4p
λK−rc
8p
λK≥√
2
2p
λK,
Note that the large mis, the smaller cis. Therefore, we can choose a slight larger msatisfying this
inequality. So we have
λmin(K) =λmin(GG⊤)≥r
λK
2.
Proof complete.
Lemma 5. [38] Let AandBbe two positive define matrices, we have
λmax(A◦B)≤
max
iAii
λmax(B),
λmin(A◦B)≥
min
iAii
λmin(B).
Lemma 6. [39] Let⊗denote the Kronecker product and ∗denote the column-wise Khatri-Rao
product, we have
(A⊗B)(C∗D) =AC∗BD,
(A∗B)⊤(A∗B) =A⊤A◦B⊤B.
Now, we give the convergence analysis of LNGD.
Theorem 3. (Convergence rate of LNGD) Under the Assumption 1 and the assumption that
rank(X) = d. If we set the number of hidden units m= Ω
n4κ8
ZX,G
ν2ε3λ4
G
, we i.i.d initialize
θr∼N(0, νI),ar∼unif[{−1,+1}]for any r∈[m], and we set the step size α≤(1−2c)
(1+c)2.
Then with probability at least 1−εover the random initialization, we have for k= 0,1,2, . . .
∥y−vk∥2
2≤(1−α)k∥y−v0∥2
2.
Proof. Consider the predictive error at the (k+ 1) -th iteration, we have
∥y−vk+1∥2
2=∥y−vk+vk−vk+1∥2
2
=∥y−vk∥2
2−2(y−vk)⊤(vk+1−vk) +∥vk+1−vk∥2
2.(20)
Next, we need to estimate the bound of prediction
vk+1−vk=v(θk−α(Fk)−1(Jk)⊤(vk−y))−vk
=−Z1
ξ=0αJξ(Fk)−1(Jk)⊤(vk−y)ξ.
=−Z1
ξ=0αJk(Fk)−1(Jk)⊤(vk−y)ξ.
+Z1
ξ=0α(Jk−Jξ)(Fk)−1(Jk)⊤(vk−y)ξ.
=−αJk(Fk)−1(Jk)⊤(vk−y)| {z }
Term 1
+αZ1
ξ=0(Jk−Jξ)ξ.
(Fk)−1(Jk)⊤(vk−y)
| {z }
Term 2,
19where Jξ=∂v(θξ)
∂θξ,andθξ=ξθk+ (1−ξ)θk+1=θk−ξα(Fk)−1(Jk)⊤(vk−y).
We first analyse Term 1. We omit the index kinJ,GandFfor simplicity.
Term 1 =−αJF−1J⊤(vk−y)
=α(X⋆G)[ηZ−1
X,G⊗(G⊤G)−1](X⊤∗G⊤)(y−vk)
=α(ηXZ−1
X,GX⊤◦G(G⊤G)−1G⊤)(y−vk)
=α(ηXZ−1
X,GX⊤◦I)(y−vk),
The second equation follows the update rule of LNGD. The third equation is obtained according to
the properties of Kronecker, Hadamard and Khatri-Rao products given in Lemma 6. The last equation
uses the definition of generalized inverse as given by Eq. (16). By Lemma 5, we have
λmax(XZ−1
X,GX⊤◦I)≤max
i(XZ−1
X,GX⊤)iiλmax(I)
≤λmax(Z−1
X,G) max
i(XX⊤)ii=1
λmin(ZX,G).(21)
Therefore, we can bound Term 1 by
∥Term 1∥2=∥α(ηXZ−1
X,GX⊤◦I)(y−vk)∥2
≤αη∥XZ−1
X,GX⊤◦I∥2∥y−vk∥2≤αη
λmin(ZX,G)∥y−vk∥2.(22)
Based on the Condition 2, we have the following inequality
Z1
ξ=0(Jk−Jξ)ξ.
2≤Z1
ξ=0∥Jk−Jξ∥2ξ.≤ ∥Jk+1−Jk∥2
≤∥Jk+1−J0∥2+∥Jk−J0∥2
≤√
2c
2√λK
κZX,G≤c
κZX,Gq
λmin(GG⊤).(23)
Next, we bound Term 2. By Eq. (23), we have
∥Term 2∥2=αZ1
ξ=0(Jk−Jξ)ξ.
(Fk)−1(Jk)⊤(vk−y)
2
≤cα
κZX,Gq
λmin(GG⊤)∥(Fk)−1(Jk)⊤∥2∥y−vk∥2
=cα
κZX,Gq
λmin(GG⊤)∥(ηZ−1
X,G⊗(G⊤G)−1)(X⊤∗G⊤)∥2∥y−vk∥2
=cα
κZX,Gq
λmin(GG⊤)∥ηZ−1
X,GX⊤∗(G⊤G)−1G⊤∥2∥y−vk∥2
=cα
κZX,Gq
λmin(GG⊤)∥ηZ−1
X,GX⊤∗G⊤(GG⊤)−1(GG⊤)−1GG⊤∥2∥y−vk∥2
=cαη
κZX,Gq
λmin(GG⊤)∥Z−1
X,GX⊤∗G⊤(GG⊤)−1∥2∥y−vk∥2. (24)
Define ∆ =Z−1
X,GX⊤∗G⊤(GG⊤)−1, then we have
∥∆∥2=σmax(∆) =q
λmax(∆⊤∆)
=q
λmax(XZ−1
X,GZ−1
X,GX⊤◦(GG⊤)−1).(25)
Similar to Eq. (21), by Lemma (5) we can prove
∥∆∥2≤1
λmin(ZX,G)1p
λmin(GG⊤). (26)
20Combine Eq. (24) and Eq. (26), we have
∥Term 2∥2≤cαη
κZX,Gq
λmin(GG⊤)1
λmin(ZX,G)1p
λmin(GG⊤)∥y−vk∥2
=cαη
λmax(ZX,G)∥y−vk∥2.(27)
Combine Eq. (20), Eq. (27) and Eq. (22), we can obtain
∥y−vk+1∥2
2=∥y−vk∥2
2−2(y−vk)⊤(vk+1−vk) +∥vk+1−vk∥2
2
=∥y−vk∥2
2−2α(y−vk)⊤Jk(Fk)−1(Jk)⊤(y−vk)
+2α(y−vk)⊤Z1
ξ=0(Jk−Jξ)ξ.
(Fk)−1(Jk)⊤(y−vk)
+∥vk+1−vk∥2
2
≤
1−2αη
λmax(ZX,G)+2cαη
λmax(ZX,G)+α2η2
λ2
min(ZX,G)
+2cα2η2
λmax(ZX,G)λmin(ZX,G)+c2α2η2
λ2max(ZX,G)
∥y−vk∥2
2.
In the last second inequality, we use the fact that λmin(XZ−1
X,GX⊤◦I)≥1
λmax(ZX,G). Let
−αη
λmax(ZX,G)+2cαη
λmax(ZX,G)+α2η2
λ2
min(ZX,G)
+2cα2η2
λmax(ZX,G)λmin(ZX,G)+c2α2η2
λ2max(ZX,G)≤0,
we have
α≤(1−2c)λmax(ZX,G)
(1 +c)2η≤(1−2c)
(1 +c)2,
and
∥y−vk∥2
2≤(1−α)k∥y−v0∥2
2.
This completes the proof.
So far, we have already proved Theorem 3 under the an assumption that the parameters stay close to
the initialization point. We now verify this assumption by the following lemma.
Lemma 7. If Conditions 1 and 2 hold, then as long as λmin(GG⊤)≥1
2λK, we have
∥θk+1−θ0∥2≤2∥y−v0∥2p
λK/2κZX,G.
Proof. By the update rule of LNGD, we have
∥θk+1−θ0∥2=kX
t=0α(Ft)−1(Jt)⊤(y−vt)
2
≤αkX
t=0∥∥Z−1
X,GX⊤∗(Gt)⊤(Gt(Gt)⊤)−1∥2∥y−vt∥2
≤αkX
t=01
λmin(ZX,G)1p
λK/2∥y−vt∥2
≤αkX
t=0p
2/λK
λmin(ZX,G)
1−α
λmax(ZX,G)t/2
∥y−v0∥2
≤2∥y−v0∥2p
λK/2λmax(ZX,G)
λmin(ZX,G)=2∥y−v0∥2p
λK/2κZX,G.
This completes the proof.
21E Experiments
E.1 Setup of CIFAR-10
The training of ResNet-18 [ 44] on the CIFAR-10 [ 40] dataset serves as a fundamental experiment
in the field of image classification. In this subsection, we present a comparison of LNGD with
several established baselines including SGD with momentum (referred to as SGD), ADAM [ 6], and
KFAC. We follow the standard experimental settings and employ a commonly used data augmentation
scheme involving random crop and horizontal flip. The initial learning rate is multiplied by 0.1 every
40 epochs. The update intervals for the curvature matrix and inverse matrix correlating with KFAC
and LNGD are set to be 100. All experimental runs are conducted over a duration of 200 epochs.
E.2 Setup of ImageNet
The implementation of ResNet50 [ 44] follows the TensorFlow version which can be found in the
website3. We use the linear warmup strategy [ 41] in the first 5 epochs for SGD, ADAM and KFAC.
The update intervals for the curvature matrix and inverse matrix correlating with KFAC and LNGD
are set to be 500. For SGD and Adam, the max epoch is set to be 80, while for KFAC and LNGD,
the max epoch is set to be 50. SGD uses the cosine learning rate updating strategy and is set to be
αt= 0.001 + 0 .5∗(α0−0.001)∗(1 + cos(2 ∗0.47∗π∗t/max _epoch )), where tis the number
of epochs. For Adam, KFAC and LNGD, the learning rate uses the exponential updating strategy
αt=α0∗(1−t/max _epoch )E, where Eis decay rate ∈ {2,3,4,5,6}.α0is the initial learning
rate tuned using a grid search with values α∈ {1e−4,1e−3, . . . , 1}.
E.3 Ablation Analysis
E.3.1 Setup
In this subsection, to further elucidate the contributions of distinct components within the LNGD,
a series of ablation studies are performed. The ablation experiments aim to isolate the effects of
adaptive learning rate and sampling optimization on the LNGD’s performance. The variant denoted
as LNGD-lr corresponds to the iteration of the algorithm that employs an adaptive learning rate, but
does not incorporate sampling optimization. Conversely, LNGD-sample represents the iteration that
utilizes sampling optimization, but does not implement an adaptive learning rate. These ablation
studies are executed on the ImageNet-1K dataset, All hyperparameters are maintained consistent with
those outlined in the ImageNet training section.
E.3.2 Results
The results of the ablation experiments, as shown in Fig.7 and Table5, reveal some interesting findings.
Specifically, analyzing the training loss and testing accuracy versus epoch, we observe that LNGD-lr
achieves the fastest decrease in training loss and the most rapid initial increase in testing accuracy
within the initial few epochs. This can be attributed to the fact that LNGD-lr computes the exact Fisher
information matrix at each epoch without using any approximation sampling strategy. However, this
advantage comes at the cost of increased computational complexity, leading to a 15 %increase in the
time required to reach a top-1 testing accuracy of 75.9 %compared to LNGD, which employs both
the sampling approximation strategy and the adaptive learning rate strategy at each layer. Moreover,
LNGD-lr also takes 5 %more time compared to LNGD-sample, which only utilizes the sampling
approximation strategy. Notably, LNGD-sample exhibits the slowest decrease in training loss and
increase in testing accuracy during the initial epochs due to its approximation sampling of the Fisher
information matrix at each step. Nevertheless, when considering the time dimension, LNGD-sample
still achieves a faster speed compared with LNGD-lr in reaching a final testing accuracy of 75.9 %
due to the significant reduction in the computation of the exact Fisher information matrix. In contrast
to LNGD, LNGD-sample takes 9 %more time to reach testing accuracy of 75.9 %due to the absence
of automatic scaling learning rate. In conclusion, considering the constraints of limited computational
resources and time, LNGD demonstrates superior optimizing performance.
3https://github.com/google-deepmind/dm-haiku/tree/main/examples/imagenet
22Table 5: Detailed statistics of abalation study when top-1 testing accuracy achieves 75.9 %.
Epoch Total Time Time Per Epoch Acceleration Best Test Acc
LNGD-lr 35 7.43h 764.39s 13% 76.50%
LNGD-sample 41 7.06h 619.86s 9% 76.57%
LNGD 36 6.46h 646.44s 76.73%
01 02 03 04 05 01.01.52.02.53.03.54.04.55.01
02 03 04 05 0203040506070800
1 00002 00003 00001.01.52.02.53.03.54.04.55.01
00002 00003 000020304050607080Training LossE
poch LNGD 
LNDG_lr 
LNGD_samplea
T
est AccuracyE
poch LNGD 
LNGD_lr 
LNGD_sample 
75.9b
Training LossT
ime/Seconds LNGD 
LNGD_lr 
LNGD_samplec
T
est AccuracyT
ime/Seconds LNGD 
LNGD_lr 
LNGD_sample 
75.9d
Figure 7: The optimization performance of variants of LNGD.
E.4 Results of More Comparisons
Table 6: Detailed statistics on CIFAR-10 when top-1 testing accuracy achieves 91%.
Epoch Total Time Time Per Epoch Acceleration
SGD 79 268.67s 3.4s 29%
ADAM 72 248.83s 3.77s 23%
KFAC 45 241.86s 5.87s 21%
EKFAC 41 247.64s 6.04s 23%
TKFAC 39 239.20s 5.98s 20%
NG+ 40 204.45s 5.11s 7%
LNGD 36 189.69s 5.08s
In order to further validate the effectiveness of LNGD, we conduct additional experiments on the
CIFAR-10 dataset, in which three methods including EKFAC [ 22], TKFAC [ 15], and NG+ [ 17] are
added for comparison. The detailed statistics are presented in Table 6. From this table, we observe
that LNGD achieves a testing accuracy of 91% with the fewest epochs and the shortest total time.
Furthermore, LNGD exhibits the smallest computational time per epoch. Additionally, due to the
efficient Fisher information matrix approximation strategy adopted by NG+, it can significantly
reduce the computational time compared to EKFAC and TKFAC.
23NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Due to space constraints in the main text, we have not created a separate
"Limitations" section. However, for each theory presented, we explicitly outline any strong
assumptions and provide justification for the validity of these theories. Additionally, a
thorough comparison with related work is furnished in the appendix.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
24Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: For each theoretical result, we provide the full set of assumptions and a
complete (and correct) proof.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide every detail of the the methodology and experimental setup to
ensure the reproducibility of the main experimental results presented in the paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
255.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: Due to the involvement of proprietary code resources, the disclosure of such
materials must adhere to the company’s relevant disclosure processes. If necessary, data and
code can be made available upon request.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We specify all the training and test details necessary to understand the results.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: For each experiment, we average the results of 5 runs and the hyper-parameter
settings are the best values randomly searched for many times.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
26•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide sufficient information on the computer resources needed to repro-
duce the experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of the work performed.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
27•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: For each relevant work, whether it be code or data, we have provided appropri-
ate citations.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
28•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We present an optimization method, which does not involve the release of new
assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
29•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
30