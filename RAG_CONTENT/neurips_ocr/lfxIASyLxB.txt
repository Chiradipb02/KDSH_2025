In-Context Learning with Transformers: Softmax
Attention Adapts to Function Lipschitzness
Liam Collins∗
Chandra Family Department of ECE
The University of Texas at Austin
liamc@utexas.eduAdvait Parulekar∗
Chandra Family Department of ECE
The University of Texas at Austin
advaitp@utexas.edu
Aryan Mokhtari
Chandra Family Department of ECE
The University of Texas at Austin
mokhtari@austin.utexas.eduSujay Sanghavi
Chandra Family Department of ECE
The University of Texas at Austin
sanghavi@mail.utexas.edu
Sanjay Shakkottai
Chandra Family Department of ECE
The University of Texas at Austin
sanjay.shakkottai@utexas.edu
Abstract
A striking property of transformers is their ability to perform in-context learning
(ICL), a machine learning framework in which the learner is presented with a novel
context during inference implicitly through some data, and tasked with making a
prediction in that context. As such, that learner must adapt to the context without
additional training. We explore the role of softmax attention in an ICL setting
where each context encodes a regression task. We show that an attention unit
learns a window that it uses to implement a nearest-neighbors predictor adapted
to the landscape of the pretraining tasks. Specifically, we show that this window
widens with decreasing Lipschitzness and increasing label noise in the pretraining
tasks. We also show that on low-rank, linear problems, the attention unit learns to
project onto the appropriate subspace before inference. Further, we show that this
adaptivity relies crucially on the softmax activation and thus cannot be replicated
by the linear activation often studied in prior theoretical analyses.
1 Introduction
One of the most compelling behaviors of pretrained transformers is their ability to perform in-context
learning (ICL) [1]: determining how to solve an unseen task simply by making a forward pass
on input context tokens. Arguably the most critical innovation enabling ICL is the self-attention
mechanism [ 2], which maps each token in an input sequence to a new token using information from
all other tokens. A key design choice in this self-attention architecture is of the activation function
that controls how much “attention" a token pays to other tokens. Softmax -activated self-attention (i.e.
softmax attention) is most commonly, and successfully, used in practice [1, 3–6].
A natural approach to explain ICL adopted by the literature is to equate it with classical machine
learning algorithms, primarily variants of gradient descent (GD). Several works have shown that
∗Co-first authors, listed in alphabetical order.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Attention Window
xquery
1.0
 0.5
 0.0 0.5 1.0
Position Relative to xquery0.0000.0050.0100.0150.020Attention WeightsSoftmax
Linear
1.0
 0.5
 0.0 0.5 1.0 1.0
 0.5
 0.0 0.5 1.0
100 200 300 400
Number of Context Samples0.000.250.500.751.001.251.50ICL Error
Softmax
Linear
100 200 300 400
 100 200 300 400
Figure 1: Top Row: The black line denotes the target function over a domain (horizontal axis). The
gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of
the target function grows and the optimal softmax attention window (shaded blue) shrinks. Middle
Row: Attention weights – which determine the attention window – as a function of the relative position
from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness.
Bottom Row: ICL error versus number of context samples for the three settings. Adapting to
function Lipschitzness leads softmax attention to achieve small error . Please see Remark 2.1 and
Appendix J for further discussion and details.
when the ICL tasks are linear regressions and the activation in the attention unit is identity (referred
to as linear attention), transformers that implement preconditioned GD during ICL are global optima
of the pretraining loss, which is the population loss on ICL tasks [ 7–9]. In particular, the prediction
of such transformers with llinear attention layers equals the prediction of a regressor trained with l
preconditioned GD steps on the context examples. However, since these analyses are limited to linear
attention and tasks, they do not explain the widespread success of softmax attention at ICL.
More recent work [ 10] extends these results by showing that for general regression tasks and any
attention activation that is a kernel, ICL equates to training a kernel regressor via functional GD in
the Reproducing Kernel Hilbert Space (RKHS) induced by the activation. However, this functional
GD yields generalization guarantees only when the activation kernel is identical to a kernel that
generates the labels, which does not apply to the softmax activation, as it is not a kernel. Further, like
the aforementioned studies of the linear setting [ 7–9], this analysis only shows that pretraining leads
to learning the covariate distribution, while the activation implicitly encodes the label distribution
needed for accurate predictions. Thus, this line of work has not explained the very fundamental
question of what softmax attention learns during pretraining that enables it to perform ICL on a wide
variety of downstream tasks. Motivated by this gap in the literature, we ask the following question.
How does softmax attention learn to perform ICL?
To answer this question, we study general settings in which pretraining and evaluation ICL tasks are
regressions that share only Lipschitzness andlabel noise variance . Specifically, the rate at which their
ground-truth labels change along particular directions in the input space, and the variance in the label
noise, is similar across tasks. In such settings, we observe that softmax attention acts as a nearest
neighbors regressor with an attention window – i.e. neighborhood of points around the query that
strongly influence, or “attend to”, the prediction – that adapts to the pretraining tasks. Specifically,
our main result is as follows:
Main Claim: Softmax attention performs ICL by calibrating its attention window
to the Lipschitzness andlabel noise variance of the pretraining tasks.
While this does not contradict the line of work showing that ICL manifests via a “meta-learned"
gradient-based algorithm, we show in a general setting that a simpler mechanism can explain the
capabilities of a widely accepted model of ICL.
2Outline. We substantiate the above claim via two streams of analysis. To our knowledge, these are
the first results showing that softmax attention pretrained on ICL tasks recovers shared structure
among the tasks that facilitates ICL on downstream tasks.
(1) Attention window scale adapts to Lipschitzness and noise variance – Section 3. We prove that
the pretraining-optimal softmax attention estimator scales its attention window inversely with the task
Lipschitzness andjointly with the noise level to optimally trade-off bias and variance in its prediction
(Theorem 3.4). This requires tight upper and lower bounds on the pretraining ICL loss. While the
upper bounds (Lemma C.8) hold for all L-Lipschitz tasks, the lower bounds (Lemma C.9) are more
challenging and require considering specific classes of tasks. We consider two classes of generalized
linear models (GLMs), and obtain lower bounds via novel concentrations for particular functionals
on the distribution of the attention weights for tokens distributed on the hypersphere (Corollary G.5).
(2) Attention window directions adapt to direction-wise Lipschitzness – Section 4. We prove
that when the target function class consists of linear functions that share a common low-dimensional
structure, the optimal softmax attention weight matrix from pretraining projects the data onto this
subspace (Theorem 4.4). In other words, softmax attention learns to zero-out the zero-Lipschitzness
directions in the ambient data space, and thereby reduces the effective dimension of ICL. We prove
this via a careful symmetry-based argument to characterize a particular gradient of the ICL loss as
positive (Lemmas H.3 and H.4).
Tightness of results. Our results highlight the importance of shared Lipschitzness across training
and test, as well as the critical role of the softmax activation, to ICL. We show that softmax attention
pretrained on the setting from Section 3 in-context learns anydownstream task with similar Lipschitz-
ness to the pretraining tasks, while changing only the Lipschitzness of the evaluation tasks degrades
performance (Theorem 3.5) – implying learning Lipschitzness is both sufficient andnecessary for
generalization . Further, to emphasize the necessity of the softmax , we show that the minimum ICL
loss achievable by linear attention exceeds that achieved by pretrained softmax attention (Theorem
3.6). We verify all of these results with empirical simulations (Section 3.2 and Appendix J).
Notations. We use (upper-, lower-)case boldface for (matrices, vectors), respectively. We denote the
(identity, zero) matrix in Rd×das (Id,0d×d), respectively, the set of column-orthonormal matrices in
Rd×kasOd×k, and the (column space, 2-norm) of a matrix Bas (col(B),∥B∥), respectively. We
indicate the unit hypersphere in RdbySd−1and the uniform distribution over Sd−1asUd. We use
asymptotic notation ( O,Ω) to hide constants that depend only on the dimension d.
1.1 Additional Related Work
Numerous recent works have constructed transformers that can implement GD and other machine
learning algorithms during ICL [ 11–15], but it is unclear whether pretraining leads to such transform-
ers. [ 16] and [ 13] provide generalization bounds for ICL via tools from algorithmic stability and
uniform concentration, respectively. [ 17] investigate the pretraining statistical complexity of learning
a Bayes-optimal predictor for ICL on linear tasks with linear attention. [ 18–20] study the role of
the pretraining data distribution, rather than the learning model, in facilitating ICL. [ 21] studies
the dynamics of a softmax attention unit trained with GD on ICL tasks, but this analysis considers
only linear tasks and orthogonal inputs. The connection between ICL with softmax attention and
non-parametric regression has been noticed by other works that analyze the ICL performance of a
softmax-like kernel regressor [ 22] and aim to improve upon softmax attention [ 23–27] rather than
explain what it learns during pretraining. Please see Appendix A for further discussion of the large
body of related works studying the theory of transformers, ICL and kernel regression.
2 Preliminaries
In-Context Learning (ICL) regression tasks. We study ICL in the regression setting popularized
by [28], wherein each task is a regression problem in Rd. The context for task tconsists of a set
ofnfeature vectors paired with noisy labels {x(t)
i, f(t)(x(t)
i) +ϵ(t)
i}n
i=1, where f(t):Rd→R
generates the ground-truth labels for task tandϵ(t)
iis label noise. Given this context, the model
solves the task if it accurately predicts the label of a query x(t)
n+1. During pretraining, the model
3observes many such tasks. Then, it is evaluated on a new task with context {xi, f(∗)(x(∗)
i)+ϵ(∗)
i}n
i=1
and query x(∗)
n+1. We emphasize that the model is trained only on the pretraining tasks, not the
evaluation context. Unlike traditional supervised learning, which would involve training on the
context {xi, f(∗)(x(∗)
i) +ϵ(∗)
i}n
i=1in order to predict f(∗)(x(∗)
n+1), ICL happens entirely in a forward
pass, so there is no training using labels from f(∗). Our inquiry focuses on how ICL is facilitated by
the softmax activation in the self-attention unit, which we introduce next.
The Softmax Attention Unit. We consider a single softmax attention head HSA(·;θ) :
R(d+1)×(n+1)→R(d+1)×(n+1)parameterized by θ:=(WK,WQ,WV), where WK,WQ,WV∈
R(d+1)×(d+1)are known as key, query, and value weight matrices, respectively. Intuitively, for a se-
quence of tokens Z= [z1, . . . ,zn+1]∈z(d+1)×(n+1), the attention layer creates a “hash map" where
the key-value pairs come from key and value embeddings of the input tokens, {WKzi:WVzi}.
Each token ziis interpreted as a query WQzi, and during a pass through the attention layer,
this query is matched with the keys {WKzj}jto return an average over the associated values
{WVzj}jwith a weight determined by the quality of the match (proportional to e(WKzj)⊤(WQzi)).
Specifically, HSA(Z;θ) = [hSA(z1,Z;θ),···, hSA(zn+1,Z;θ)], where
hSA(zi,Z;θ) =Pn
j=1(WVzj)e(WKzj)⊤(WQzi)
Pn
i=1e(WKzj)⊤(WQzi)∈Rd+1. (ATTN)
With slight abuse of notation, we denote hSA(zj) =hSA(zj,Z;θ)when it is not ambiguous. To
study how this architecture enables ICL, we follow [ 28] to formalize ICL as a regression problem.
Below we define the tokenization, pretraining objective and evaluation task.
Tokenization for regression. The learning model encounters token sequences of the form
Z:=
x1 x2 . . . xn xn+1
f(x1) +ϵ1f(x2) +ϵ1. . . f (xn) +ϵn 0
∈R(d+1)×(n+1), (1)
where the ground-truth labelling function fmaps from RdtoRand belongs to some class F, each ϵi
is mean-zero noise, and the i-th input feature vector xi∈Rdis jointly embedded in the same token
with its noisy label f(xi) +ϵi∈R. We denote this token zi. The ICL task is to accurately predict
this label given the ncontext tokens {(xi, f(xi) +ϵi)}n
i=1, where fmay vary across sequences. The
prediction for the label of the (n+1)-th feature vector is the (d+1)-th element of hSA(zn+1)[10],
denoted hSA(zn+1)d+1. Ultimately, the goal is to learn weight matrices such that hSA(zn+1)d+1is
likely to approximate the (n+ 1) -th label on a random sequence Z.
Pretraining protocol. We study what softmax attention learns when its weight matrices are pre-
trained using sequences of the form of (1). These sequences are randomly generated as follows:
f∼D(F),x1, . . . ,xn+1i.i.d.∼D⊗(n+1)
x , ϵ 1, . . . , ϵ ni.i.d.∼D⊗(n+1)
ϵ (2)
where D(F)is a distribution over functions in F,Dxis a distribution over Rd, andDϵis a distribution
overRwith mean zero and variance σ2. The token embedding sequence Zis then constructed as in
(1). Given this generative model, the pretraining loss of the parameters θ= (WQ,WK,WV)is the
expected squared difference between the prediction of softmax attention and the ground-truth label of
the(n+1)-th input feature vector in each sequence, namely
¯L(θ) :=Ef,{xi}i,{ϵi}i(hSA(zn+1)d+1−f(xn+1))2. (3)
We next reparameterize the attention weights to make (3)more interpretable. For the last column of
WV, we show in Appendix B that any minimizer of (3)in the settings we consider must have the
firstdelements of this last column equal to zero. We follow [ 7,9,10] by setting the first ncolumns
ofWVto zero. As in [ 10], we fix the (d+1, d+1)-th element of WV, here as 1 for simplicity. In the
same vein, we follow [ 7,10] by setting the (d+1)-th row and column of WKandWQequal to zero.
To summarize, the reparameterized weights are:
WV=
0d×d0d×1
01×d 1
,WK=
MK0d×1
01×d 0
,WQ=
MQ0d×1
01×d 0
(4)
4where MK,MQ∈Rd×d. Now, since our goal is to reveal properties of minimizers of the pretraining
loss, rather than study the dynamics of optimizing the loss, without loss of generality we can define
M:=M⊤
KMQand re-define the pretraining loss (3) as a function of M. Doing so yields:
L(M) :=Ef,{xi}i,{ϵi}i Pn
i=1(f(xi) +ϵi)ex⊤
iMxn+1
Pn
i=1ex⊤
iMxn+1−f(xn+1)!2
. (ICL)
Interpretation of the pretraining loss. The loss (ICL) clarifies how softmax attention can be inter-
preted as a nearest neighbors regressor. When x⊤
iMxn+1is a proxy for the distance between xi
andxn+1(which we formally show in Section 3 as happening under reasonable assumptions), the
softmax attention prediction is a convex combination of the noisy labels with weights determined
by the closeness of xitoxn+1, such that the labels of points closer to xn+1have larger weight.
Moreover, the decay in weights on points further from xn+1is exponential and controlled by M,
which effectively defines a neighborhood, or attention window, of points around xn+1whose labels
have non-trivial weight. More formally, we can think of the attention window defined for a query
xn+1as the set AttnWindow (xn+1;M) :={x:x⊤Mxn+1= Ω(1) }. As we have observed in
Figure 1, our key insight is that pretrained Mscales this attention window with the Lipschitzness
of the function class. Generally speaking, larger Mentails averaging over a smaller window and
incurring less bias due to the function values of distant tokens in the estimate, while smaller Mentails
averaging over a larger window, resulting in larger bias due to distant token labels, but a smaller noise
variance. Figure 2 further depicts this tradeoff.
Connection to non-parametric estimation and the Nadaraya-Watson estimator. A nonparamet-
ric estimation technique to interpolate between known values of a function is to use a kernel estimator.
The Nadaraya-Watson (NW) estimator [29–31] is one such estimator, and interpolates the data as
fNW(xn+1) =X
iK(xn+1, xi)f(xi)P
jK(xn+1, xj)
where K(r) =e−r2/hfor some bandwidth h. In Section B.1 we show that optimizing the pretraining
loss(ICL) reduces to meta-learning the bandwidth of an NW estimator on a distribution of pretraining
tasks. However, to our knowledge, the literature has not determined the optimal bandwidth for the
kernel, as there has been no analysis of non-asymptotic lower bounds on the loss, which we need to
characterize the optimal solution. A close work to ours is [ 32], which considers regression on general
L-Lipschitz tasks, but this analysis provides only a tight upper bound on the loss.
Remark 2.1 (Extreme cases) .Consider the following two settings.
(i) Constant functions. If each of the functions the attention unit sees in pretraining is constant,
as in the Left column of Figure 1, it is best to consider an infinite attention window, that is, take
M=0d×das this results in a uniform average over all the noisy token labels.
(ii) Rapidly changing functions. If the pretraining functions change rapidly, as in the Right column
of Figure 1, attending to a distant token might only corrupt the estimate at the target. For example
suppose the input tokens are used to construct Voronoi cells on the surface of the hypersphere, and
the label for a new token in a cell is the label of the token used to construct that cell. The optimal
estimator attends only to the single nearest token since this incurs error only from label noise.
Remark 2.2 (Softmax advantage) .To further highlight the utility of the softmax, we compare with
linear attention [ 7,9,11], whose estimator can be written as hLA(x) =P
i(f(xi) +ϵi)x⊤
iMx,
up to a universal scaling due to the value embedding. This is again a weighted combination of labels,
but one that does not allow for adapting an attention window – any scaling of Mdoes not change the
relative weights placed on each label – unlike softmax attention. Please see Figure 1 (Middle Row)
for a comparison of the weights used in the different estimators.
3 Pretraining Learns Scale of Attention Window
One of our observations of the attention estimator hSAis that it computes a nearest neighbours
regression. We hypothesize that the role of pretraining is to select a neighbourhood within which to
select tokens for use in the estimator. In this section we characterize the radius of this neighborhood.
5Figure 2: From left to right , as we shrink the attention window (shaded in blue), the estimator has
lower bias (the expected value of the estimate, depicted in purple, is closer to the ground-truth label,
depicted by the white circle) but larger variance (shaded in tan).
Definition 3.1 (Lipschitzness) .A function f:X → Rhas Lipschitzness LifLis the smallest
number satisfying f(x)−f(x′)≤L∥x−x′∥for all (x,x′)∈ X2.
The general requirement for the function classes to which our results apply is that the class should
be invariant to isometries, each function should be Lipschitz, and the function value at two points
should be less correlated as those points get further. These are written formally in Assumption B.4.
To be concrete, we work with the following two function classes that satisfy these assumptions (this
is shown in Lemmas C.3 and C.7) to derive explicit bounds.
Definition 3.2 (Affine and ReLU Function Classes) .The function classes Faff
LandF+
Lare respec-
tively defined as:
Faff
L:={f:f(x) =lw⊤x+b,w∈Sd−1, b, l∈[−L, L]},
F+
L:={f:f(x) =l1(w⊤x)++l2(−w⊤x)++b,w∈Sd−1,(b, l1, l2)∈[−L, L]2}.
D(Faff
L), D(F+
L)are induced by drawing w∼ΣUdandb, l, l 1, l2i.i.d.∼Unif([−L, L])for some Σ≻
0d×d. Note that the max Lipschitzness of any function in these classes is L, and (z)+:= max( z,0).
Next, we make the following assumption, similar to [7], on the covariate distribution.
Assumption 3.3 (Covariate Distribution) .The covariate distribution satisfies Dx=Σ−1Ud.
Now we are ready to state our main theorem that characterizes minimizers of (ICL).
Theorem 3.4. Let Assumption 3.3 hold and tasks fbe drawn from (Case 1) D(Faff
L)or(Case 2)
D(F+
L). Forn= Ω(1) andΩ(n−d/2)≤σ2≤ O(nL2), any minimizer of the pretraining loss (ICL)
satisfies2M∗=wKQΣ, where for Λ:=nL2
σ2,α:=1
d+4andβ:=1
d+2:
(Case 1) Ω (Λα)≤ |wKQ| ≤ O
Λ2α
1−β
,(Case 2) Ω 
Λβ
≤ |wKQ| ≤ O 
Λ2β
.
Theorem 3.4 shows that optimizing the pretraining population loss in Equation (ICL) leads to attention
key-query parameters that scale with the Lipschitzness of the function class, as well as the noise level
and number of in-context samples. These bounds align with our observations from Figures 1 and 2
that softmax attention selects an attention window that shrinks with the function class Lipschitzness,
recalling that larger wKQresults in a smaller window. Further, the dependencies of the bounds on σ2
andnare also intuitive, since larger noise should encourage wider averaging to average out the noise,
and larger nshould encourage a smaller window since more samples makes it more likely that there
are samples very close to the query. To our knowledge, this is the first result showing that softmax
attention learns properties of the task distribution during pretraining that facilitate ICL .
Learning Lipschitzness is critical to generalization. We next give the following generalization
result for downstream tasks.
Theorem 3.5. Suppose softmax attention is first pretrained on tasks drawn from D(F+
L)and then
tested on an arbitrary L−Lipschitz task, then the loss on the new task is upper bounded as L ≤
2We further show in Appendix B that M∗=wKQΣfor scalar wKQholds for a broad family of rotationally-
invariant function classes.
60 500 1000 1500 2000
Pretraining Iterations020406080100||M||ReLU -- Isotropic
L
0
0.8
1.6
2.4
3.2
0 500 1000 1500 2000
Pretraining Iterations020406080100ReLU -- Non-Isotropic
0 500 1000 1500 2000
Pretraining Iterations020406080100Cos -- Isotropic
0 500 1000 1500 2000
Pretraining Iterations0255075100125Cos -- Non-IsotropicFigure 3: Spectral norm of Mduring pretraining with varying L. Each plot shows results for
different task and covariate distributions, with (tasks, covariates) drawn from (Left) (D(F+
L),Ud),
(Middle-Left) (D(F+
L),˜Ud),(Middle-Right) (D(Fcos
L),Ud),(Right) (D(Fcos
L),˜Ud), where ˜Udis a
non-isotropic distribution on Sd−1(see Section 3.2 for its definition).
O(L2
Λβ).Furthermore, if the new task is instead drawn from D(F+
L′), the loss is lower bounded as
L ≥Ω(L′2
Λ2β)forL′> L andL ≥Ω(Λβd/2
n)forL′< L.
Theorem 3.5 shows that pretraining on D(F+
L)yields a model that can in-context learn downstream
tasks if and only if they have similar Lipschitzness as L. Thus, learning Lipschitzness is both sufficient
and necessary for ICL. If the evaluation task Lipschitzness is much larger than that seen in pretraining,
the pretrained model will give highly biased estimates. Conversely, if the evaluation Lipschitzness is
much lower, the pretrained model will not optimally average the label noise.
Necessity of Softmax. To further emphasize the importance of the softmax in Theorem 3.4, we
next study the performance of an analogous model with the softmax removed. We consider linear
self-attention [7,9,11], which replaces the softmax activation with an identity operation. In particular,
in the in-context regression setting we study, the prediction of f(xn+1)by linear attention and the
corresponding pretraining loss are given by:
hLA(xn+1) :=nX
i=1(f(xi) +ϵi)x⊤
iMxn+1,
LLA(M) :=Ef,{xi}i,{ϵi}i(hLA(xn+1)−f(xn+1))2.
As discussed in Remark 2.1, hLA(xn+1)cannot adapt an attention window to the problem setting.
We show below that this leads it to large ICL loss when tasks are drawn from D(F+
L).
Theorem 3.6 (Lower Bound for Linear Attention) .Consider pretraining on LLAwith tasks fdrawn
fromD(F+
L)and covariates drawn from Ud. Then for all M∈Rd×d,LLA(M) = Ω( L2).
This lower bound on LLAis strictly larger than the upper bound on Lfrom Theorem 3.5, up to factors
ind, as long asσ2
n≤1, which holds in all reasonable cases. Please see Appendix F for the proof.
3.1 Proof Sketch
To highlight the key insights of our analysis, in this section we consider a modification of the softmax
attention that exhibits important properties of the original. Note that this approximation is for
illustration only; the above results use the original softmax attention – see Appendices C, D, E. For
now, consider a function class FL:={f:f(x) =Lw⊤x,w∈Sd−1}of linear functions.
(Temporary) modification of the softmax attention. Rather than averaging over every token with a
weight that decays exponentially with distance, we consider a modification which uniformly averages
all tokens within a distance specified by wKQ=∥M∥. From Lemma B.5, without loss of generality
(WLOG) we can consider M=wKQId. This means that, ignoring normalization, the weight
assigned to f(xi)by the true soft-max attention is e−wKQ∥x−xi∥2. That is, for all xisatisfying
∥x−xi∥<1/√wKQ, the assigned weights within a constant factor of each other. Meanwhile, for xi
satisfying ∥x−xi∥=√c/√wKQforc >1, the weights are e−c, decaying exponentially in c. This
motivates us to consider a “modified softmax attention" given by hMSA(x) :=P
if(xi) 1iP
j1j,where
1j:= 1{∥x−xj∥<1/√wKQ}.
7The In-context Loss. The pretraining loss in Equation ICL can be decomposed as:
L(wKQId) =Ef,{xi}i
X
j(f(xn+1)−f(xj)) 1jP
j1j
2
| {z }
=:Lsignal(wKQ)+E{xi}i,{ϵi}i X
iϵi 1iP
j1j!2
| {z }
=:Lnoise(wKQ).
We first upper and lower bound each of these terms separately, starting with Lsignal(wKQ).
Noiseless Estimator Bias. (Please see Appendix C) This term is the squared difference between an
unweighted average of the token labels within a radius of x, and the true label. Take wKQ= Ω(1) .
Then for large d, most of the points xisatisfying ∥x−xi∥ ≤ 1/√wKQlie on the boundary of the
cap, that is, ∥x−xi∥<1/√wKQ=⇒ ∥x−xi∥ ≈ 1/√wKQ.This motivates us to approximate the
set of points xisatisfying the above as coming from a uniform distribution over just the boundary of
the cap. The center of mass of a ring of radius 1/√wKQembedded on the surface of a hyper-sphere, is
O(1/wKQ)from the boundary of a sphere, so the squared bias is Θ(L2/w2
KQ).
Noise. (Please see Appendix D for details) Since the noise is independent across tokens, we can write
Lnoise(wKQ) =σ2P
j1j, which is related to the number of tokens found within a 1/√wKQradius of x.
In Lemma G.1, we derive bounds for the measure of this region. For now, we replace the sum in the
denominator with its expectation. We can bound1P
j1j= Θ 
wd
2
KQ/n
as long as wKQ≲n2/d.
Combining the Lsignal andLnoiseterms. (Please see Appendix E for details) Overall, we have L=
Lsignal+LnoisewithLsignal= Θ 
L2/wKQ
andLnoise= Θ 
wd
2
KQσ2/n
. Minimizing this sum reveals
that the optimal wKQsatisfies wKQ= Θ 
(nL2/σ2)2
d+2
.
3.2 Experiments
We next empirically verify our intuitions and results regarding learning the scale of the attention
window. In all cases we use the Adam optimizer with one task sampled per round, use the noise
distribution Dϵ=N(0, σ2), and run 10trials and plot means and standard deviations over these 10
trials. Please see Appendix J for full details as well as additional results.
Ablations over L,σandn.We verify whether the relationship between the attention window scale –
i.e.∥M∥−1– and L,σandnmatches our bounds in Theorem 3.4 for the case when tasks are drawn
fromD(F+
L)and the covariates are drawn from Ud, as well as whether these relationships generalize
to additional function classes and covariate distributions. We train on tasks drawn from D(F+
L)
andD(Fcos
L), where Fcos
L:={f:f(x) = cos 
Lw⊤x
,w∈Sd−1}andD(Fcos
L)is induced by
sampling w∼ Ud. In all cases we set d= 5, and use (L, σ, n ) = (1 ,0.01,20)if not ablating over
these parameters, and vary only one of {L, σ, n }and no other hyperparameters within each plot.
Attention window scales inversely with L.Figure 3 shows that ∥M∥increases with Lin various
settings. In Figure 3(Left, Middle-Left), tasks are drawn from D(F+
L), and in Figure 3(Middle-Right,
Right), they are drawn D(Fcos
L). In Figure 3(Left, Middle-Right), each xiis drawn from Ud, whereas
in Figure 3(Middle-Left, Right), each xiis drawn from a non-isotropic distribution ˜UdonSd−1
defined as follows. First, let Sd:=diag([1, . . . , d ])∈Rd×d, thenx∼˜Udis generated by sampling
ˆx∼ N (0d,Id), then computing x=S1/2
dˆx
∥S1/2
dˆx∥. Although larger Limplies larger ∥∇xf(x)∥on
average across f, it is not clear that it implies larger ∥∇MKL(W⊤
KWQ)∥nor∥∇MQL(W⊤
KWQ)∥,
so it is surprising that larger Limplies larger pretrained M(although it is consistent with our results).
Attention window scales with σ, inversely with n.Figure 4 shows that the dependence of ∥M∥on
σandnalso aligns with Theorem 3.4. As expected, ∥M∥increases slower during pretraining for
larger σ(shown in Figures 4(Left, Middle-Left)), since more noise encourages more averaging over a
larger window to cancel out the noise. Likewise, ∥M∥increases faster during pretraining for larger
n(shown in Figures 4(Middle-Right, Right)), since larger nincreases the likelihood that there is a
highly informative sample in a small attention window. Here always the covariate distribution is Ud.
80 500 1000 1500 2000
Pretraining Iterations0123||M||ReLU -- Varying 
0.0
0.2
0.6
1
3
10
0 500 1000 1500 2000
Pretraining Iterations0.00.51.01.52.0Cos -- Varying 
0 500 1000 1500 2000
Pretraining Iterations01234ReLU -- Varying n
n
5
10
50
100
200
500
0 500 1000 1500 2000
Pretraining Iterations012345Cos -- Varying nFigure 4: Spectral norm of Mduring pretraining on tasks drawn from D(F+
1)inLeft, Middle-Right
andD(Fcos
1)inMiddle-Left, Right .Left, Middle-Left show ablations over the noise standard
deviation σandMiddle-Right, Right show ablations over the number of context samples n.
Learning new tasks in-context. An implication of our work is that for the function classes we
consider, the softmax attention estimator does not adapt to the function class beyond its Lips-
chitzness . We have already seen in Figures 3 and 4 that the growth of ∥M∥during pretraining is
similar across different function classes with the same Lipschitzness, as long as σandnare fixed.
Here we verify the conclusion from Theorem 3.5 that for fixed nandσ, the necessary and suffi-
cient condition for downstream generalization, measured by small ICL error, is that the pretraining
and downstream tasks have similar Lipschitzness. Figure 5 supports this conclusion. Here we set
d= 5, n= 200 , σ= 0.01and draw each xii.i.d. from Ud. In Figure 5(Left, Middle-Left, Middle-
Right), we train three attention units on tasks drawn from the 1-Lipschitz affine ( D(Faff
1)), ReLU
(D(F+
1)), and cosine ( D(Fcos
1)) task distributions. Each plot shows the test ICL error on tasks drawn
from a distribution in {D(Faff
1), D(F+
1), D(Fcos
1)}. Performance is similar regardless of the pairing
of pretraining and test distributions, as the Lipschitzness is the same in all cases, demonstrating that
pretraining on tasks with appropriate Lipschitzness is sufficient for generalization .
Moreover, Figure 5(Right) shows that when the Lipschitzness of the pretraining tasks does notmatch
that of the test tasks, ICL performance degrades sharply, even when the tasks otherwise share similar
structure. Here the test task distribution is D(Fcos
1), and the pretraining task distributions are D(Faff
1),
D(Fcos
0.1), and D(Fcos
10). The only pretraining distribution that leads to downstream generalization is
D(Faff
1)since its Lipschitzness matches that of the downstream tasks, despite the fact that it is not a
distribution over cosine functions, unlike the other distributions. Thus, these results lend credence to
the idea that in addition to being sufficient, pretraining on tasks with appropriate Lipschitzness is
necessary for generalization .
4 Softmax Attention Learns Direction of Attention Window
Thus far, we have considered distributions over tasks that treat the value of the input data in all
directions within the ambient space as equally relevant to its label. However, in practice the ambient
dimension of the input data is often much larger than its information content – the labels may change
very little with many features of the data, meaning that such features are spurious. This is generally
true of embedded language tokens, whose embedding dimension is typically far larger than the
minimum dimension required to store them (logarithmic in the vocabulary size) [ 1]. Motivated by
this, we define a notion of “direction-wise Lipschitzness” of a function class to allow for analyzing
classes that may depend on some directions within the ambient input data space more than others.
Definition 4.1 (Direction-wise Lipschitzness of Function Class) .The Lipschitzness of a function
classFwith domain X ⊆Rdin the direction w∈Sd−1is defined as as the largest Lipschitz constant
of all functions in Fover the domain Xprojected onto w, that is:
Lipw(F,X) := inf
L∈R{L:f(ww⊤x)−f(ww⊤x′)≤L|w⊤x−w⊤x′| ∀(x,x′)∈ X2, f∈ F} .
Using this definition, we analyze function classes consisting of linear functions with parameters lying
in a subspace of Rd, as follows:
Definition 4.2 (Low-rank Linear Function Class) .The function class Flin
Bis defined as Flin
B:={f:
f(x) =a⊤B⊤x,a∈Rk}, and D(Flin
B)is induced by drawing a∼ Uk.
where B∈Od×kis a column-wise orthonormal matrix. Since our motivation is settings with low-
dimensional structure, we can think of k≪d. LetB⊥∈Od×(d−k)denote a matrix whose columns
90 250 500 750 1000
Pretraining Iterations102
101
100T est ICL ErrorTest on Affine, L=1
Trained on (all L=1)
Affine
ReLU
Cos
0 250 500 750 1000
Pretraining Iterations102
101
100Test on ReLU, L=1
Trained on (all L=1)
Affine
ReLU
Cos
0 250 500 750 1000
Pretraining Iterations102
101
100Test on Cos, L=1
Trained on (all L=1)
Affine
ReLU
Cos
0 250 500 750 1000
Pretraining Iterations102
101
100Test on Cos, L=1
Trained on
Affine, L=1
Cos, L=0.1
Cos, L=10Figure 5: Left, Middle-Left, Middle-Right: The test error for softmax attention as it is trained on
the distributions over 1-Lipschitz affine, ReLU, and cosine function ( D(Faff
1),D(F+
1), andD(Fcos
1),
respectively), where the error is evaluated at each pretraining iteration on 5 tasks drawn from the
distributions over the 1-Lipschitz (affine, ReLU, cosine) function classes in ( Left, Middle-Left,
Middle-Right ), respectively. Right: The test error evaluated on tasks drawn from D(Fcos
1)for three
softmax attention trained on tasks drawn from D(Faff
1),D(Fcos
0.1), and D(Fcos
10), respectively.
form an orthonormal basis for the subspace perpendicular to col(B), and note that the Lipschitzness
ofFlin
Bin the direction wisLifw∈col(B)and 0 if w∈col(B⊥). Observe that any function in
Flin
Bcan be learned by projecting the input onto the non-zero Lipschitzness directions, i.e. col(B),
then solving a k≪d-dimensional regression. To formally study whether softmax attention recovers
col(B), we assume the covariates are generated as follows.
Assumption 4.3 (Covariate Distribution) .There are fixed constants cu̸= 0and−∞< cv<∞s.t.
sampling xi∼Dxis equivalent to xi=cuBui+cvB⊥viwhere ui∼ Ukandvi∼ Ud−k.
Assumption 4.3 entails that the data is generated by latent variables uiandvithat determine label-
relevant and spurious features. This may be interpreted as a continuous analogue of dictionary
learning models studied in feature learning works [ 33,34]. We require no finite upper bound on |cv|
nor1
|cu|, so the data may be dominated by spurious features.
Theorem 4.4. LetB∈Od×kand consider the pretraining population loss (ICL) withf∼D(Flin
B).
Suppose Assumption 4.3 holds, as well as at least one of two cases: (Case 1) σ= 0, or (Case 2)
n= 2. Then among all M∈ M :={M∈Rd×d:M=M⊤,∥B⊤MB∥ ≤1
c2u}, the minimizer of
the pretraining population loss (ICL) isM∗=cBB⊤for some c∈(0,1
c2u].
Theorem 4.4 shows that softmax attention can achieve dimensionality reduction during ICL on any
downstream task that has non-zero Lipschitzness only in col(B)by removing the zero-Lipschitzness
features while pretraining on Flin
B. Removing the zero-Lipschitzness features entails that the nearest
neighbor prediction of pretrained softmax attention uses a neighborhood, i.e. attention window,
defined strictly by projections of the input onto col(B). To our knowledge, this is the first result
showing that softmax attention pretrained on ICL tasks recovers a shared low-dimensional structure
among the tasks . Please see Appendix J for empirical results verifying that softmax attention indeed
recovers low-dimensional structure, even for tasks consisting of (nonlinear) generalized linear models.
5 Conclusion
We have presented, to our knowledge, the first results showing that softmax attention learns shared
structure among pretraining tasks that facilitates downstream ICL. Moreover, we have provided
empirical evidence suggesting that our conclusions about what softmax attention learns during
pretraining generalize to function classes beyond those considered in our analysis.
Limitations and Future Work. 1. The model we use in this work is an attempt to understand a
phenomenon that emerges in LLMs, which is that the output of the model can be ‘primed’ with some
examples provided in the context that resembles few-shot learning, even though they are only trained
on next token prediction. Establishing a mathematical framework for this remains an interesting
question. 2.We consider the output of a single layer of attention. Studying the nature of the solution
when this is iterated over multiple trained layers is an interesting future prospect.
106 Acknowledgments
This work was supported in part by NSF Grants 2127697, 2019844, 2107037, and 2112471, ARO
Grant W911NF2110226, ONR Grant N00014-19-1-2566, the Machine Learning Lab (MLL) at UT
Austin, and the Wireless Networking and Communications Group (WNCG) Industrial Affiliates
Program.
References
[1]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
[2]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[3]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. Journal of Machine Learning Research , 24(240):1–
113, 2023.
[4]Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to
learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies . Association for
Computational Linguistics, 2022.
[5]Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,
John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language
models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 ,
2021.
[6]Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for
dialog applications. arXiv preprint arXiv:2201.08239 , 2022.
[7]Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to
implement preconditioned gradient descent for in-context learning, 2023.
[8]Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is
provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint
arXiv:2307.03576 , 2023.
[9]Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models
in-context. arXiv preprint arXiv:2306.09927 , 2023.
[10] Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient descent
to learn non-linear functions in context. arXiv preprint arXiv:2312.06528 , 2023.
[11] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander
Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by
gradient descent. In International Conference on Machine Learning , pages 35151–35174.
PMLR, 2023.
[12] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What
learning algorithm is in-context learning? investigations with linear models. arXiv preprint
arXiv:2211.15661 , 2022.
[13] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:
Provable in-context learning with in-context algorithm selection, 2023.
11[14] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order
optimization methods for in-context learning: A study with linear models. arXiv preprint
arXiv:2310.17086 , 2023.
[15] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dim-
itris Papailiopoulos. Looped transformers as programmable computers. arXiv preprint
arXiv:2301.13196 , 2023.
[16] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Trans-
formers as algorithms: Generalization and stability in in-context learning. In International
Conference on Machine Learning , pages 19565–19594. PMLR, 2023.
[17] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L
Bartlett. How many pretraining tasks are needed for in-context learning of linear regression?
arXiv preprint arXiv:2310.08391 , 2023.
[18] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of
in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080 , 2021.
[19] Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly
topic models: Explaining and finding good demonstrations for in-context learning. arXiv
preprint arXiv:2301.11916 , 2023.
[20] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does
in-context learning learn? bayesian model averaging, parameterization, and generalization.
arXiv preprint arXiv:2305.19420 , 2023.
[21] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. arXiv
preprint arXiv:2310.05249 , 2023.
[22] Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models
explained as kernel regression. arXiv preprint arXiv:2305.12766 , 2023.
[23] Yingyi Chen, Qinghua Tao, Francesco Tonin, and Johan AK Suykens. Primal-attention:
Self-attention through asymmetric kernel svd in primal representation. arXiv preprint
arXiv:2305.19798 , 2023.
[24] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Transformer dissection: a unified understanding of transformer’s attention via
the lens of kernel. arXiv preprint arXiv:1908.11775 , 2019.
[25] Tan Nguyen, Minh Pham, Tam Nguyen, Khai Nguyen, Stanley Osher, and Nhat Ho. Fourier-
former: Transformer meets generalized fourier integral theorem. Advances in Neural Informa-
tion Processing Systems , 35:29319–29335, 2022.
[26] Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat
Ho. Designing robust transformers using robust kernel density estimation. arXiv preprint
arXiv:2210.05794 , 2022.
[27] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression,
2023.
[28] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers
learn in-context? a case study of simple function classes. Advances in Neural Information
Processing Systems , 35:30583–30598, 2022.
[29] Kathryn A Prewitt. A distribution-free theory of nonparametric regression. laszlo gyorfi,
michael kohler, adam krzyzak, and harro walk. Journal of the American Statistical Association ,
98(464):1084–1084, 2003.
[30] Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications ,
9(1):141–142, 1964.
12[31] Geoffrey S Watson. Smooth regression analysis. Sankhy ¯a: The Indian Journal of Statistics,
Series A , pages 359–372, 1964.
[32] Samuele Tosatto, Riad Akrour, and Jan Peters. An upper bound of the bias of nadaraya-watson
kernel regression under lipschitz assumptions. Stats , 4(1):1–17, 2021.
[33] Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. In International Conference on Machine Learning , pages 11112–11122.
PMLR, 2021.
[34] Zhenmei Shi, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in
neural networks: Emergence from inputs and advantage over fixed features. arXiv preprint
arXiv:2206.01717 , 2022.
[35] Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diver-
sity and the emergence of non-bayesian in-context learning for regression. arXiv preprint
arXiv:2306.15063 , 2023.
[36] Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas
Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al.
Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858 ,
2023.
[37] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn
in-context? language models secretly perform gradient descent as meta optimizers. arXiv
preprint arXiv:2212.10559 , 2022.
[38] Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really learn
in-context by gradient descent? arXiv preprint arXiv:2310.08540 , 2023.
[39] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and
induction heads. arXiv preprint arXiv:2209.11895 , 2022.
[40] Asher Trockman and J. Zico Kolter. Mimetic initialization of self-attention layers, 2023.
[41] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: De-
mystifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint
arXiv:2310.00535 , 2023.
[42] Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind. Trans-
formers learn through gradual rank increase, 2023.
[43] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards
a mechanistic understanding. arXiv preprint arXiv:2303.04245 , 2023.
[44] Samy Jelassi, Michael E. Sander, and Yuanzhi Li. Vision transformers provably learn spatial
structure, 2022.
[45] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of
shallow vision transformers: Learning, generalization, and sample complexity. arXiv preprint
arXiv:2302.06015 , 2023.
[46] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Trans-
formers as support vector machines. arXiv preprint arXiv:2308.16898 , 2023.
[47] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token
selection in attention mechanism, 2023.
[48] Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context
reinforcement learning via supervised pretraining. arXiv preprint arXiv:2310.08566 , 2023.
13[49] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations
of transformers. arXiv preprint arXiv:2306.02896 , 2023.
[50] Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai.
How do transformers learn in-context beyond simple functions? a case study on learning with
representations. arXiv preprint arXiv:2310.10616 , 2023.
[51] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and vari-
able creation in self-attention mechanisms. In International Conference on Machine Learning ,
pages 5793–5831. PMLR, 2022.
[52] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers
learn shortcuts to automata. arXiv preprint arXiv:2210.10749 , 2022.
[53] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? a
study through the random features lens. arXiv preprint arXiv:2307.11353 , 2023.
[54] Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is turing complete. The Journal
of Machine Learning Research , 22(1):3463–3497, 2021.
[55] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077 , 2019.
[56] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of trans-
formers to recognize formal languages. arXiv preprint arXiv:2009.11264 , 2020.
[57] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of
self-attention matrices. arXiv preprint arXiv:2106.03764 , 2021.
[58] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case
study on approximating turing machines with transformers. Advances in Neural Information
Processing Systems , 35:12071–12083, 2022.
[59] Zhao Song, Guangyi Xu, and Junze Yin. The expressibility of polynomial based attention
scheme, 2023.
[60] Kevin Christian Wibisono and Yixin Wang. On the role of unstructured training data in
transformers’ in-context learning capabilities. In NeurIPS 2023 Workshop on Mathematics of
Modern Machine Learning , 2023.
[61] Yichuan Deng, Zhao Song, and Tianyi Zhou. Superiority of softmax: Unveiling the performance
edge over linear attention. arXiv preprint arXiv:2310.11685 , 2023.
[62] James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of
attention, 2020.
[63] Hyunjik Kim, George Papamakarios, and Andriy Mnih. The lipschitz constant of self-attention,
2020.
[64] Herbert Robbins. A remark on stirling’s formula. The American Mathematical Monthly ,
62(1):26–29, 1955.
[65] Brian Knaeble. Variations on the projective central limit theorem.
https://arxiv.org/pdf/0904.1048.pdf , 2015.
[66] Elliott H Lieb and Michael Loss. Analysis , volume 14. American Mathematical Soc., 2001.
[67] G.H. Hardy, J.E. Littlewood, and G. Pólya. Inequalities . Cambridge Mathematical Library.
Cambridge University Press, 1952.
14Contents
1 Introduction 1
1.1 Additional Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Preliminaries 3
3 Pretraining Learns Scale of Attention Window 5
3.1 Proof Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Softmax Attention Learns Direction of Attention Window 9
5 Conclusion 10
6 Acknowledgments 11
A Additional Related Work 16
B Preliminaries 16
B.1 Rewriting the Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C The Signal Term 21
C.1 Affine functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.2 ReLU-based functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D Bounds on Noise Variance 26
E Optimizing the Loss 27
E.1 Generalization Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
F Lower Bound for Linear Attention 31
G Bounds for gp(r) 31
G.1 Bounds on Spherical Caps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
G.2 Bounds on gp(r). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
H Attention Window Captures Appropriate Directions 36
H.1 Proof Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
H.2 Full Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
I Additional Lemmas 51
J Additional Experiments and Details 52
J.1 Low-Rank Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
15A Additional Related Work
Empirical study of ICL. Several works have studied ICL of linear tasks in the framework introduced
by [28], and demonstrated that pretrained transformers can mimic the behavior of gradient descent
[11–13,28], Newton’s method [ 14], and certain algorithm selection approaches [ 13,16]. [35]
studied the same linear setting with the goal of understanding the role of pretraining task diversity,
while [ 36] argued via experiments on general auto-regressive tasks that ICL implicitly constructs
a learning objective and optimizes it within one forward pass. Other empirical works have both
directly supported [ 37] and contradicted [ 38] the hypothesis that ICL is a gradient-based optimization
algorithm via experiments on real ICL tasks, while [ 39] empirically concluded that induction heads
with softmax attention are the key mechanism that enables ICL in transformers. Lastly, outside of the
context of ICL, [ 40] noticed that the attention parameter matrices of trained transformers are often
close to scaled identities in practice, consistent with our findings on the importance of learning a
scale to softmax attention training.
Transformer training dynamics. [21] and [ 41] studied the dynamics of softmax attention trained
with gradient descent, but assumed orthonormal input features and either linear tasks [ 21] or that
the softmax normalization is a fixed constant [41]. [42] proved that softmax attention with diagonal
weight matrices incrementally learns features during gradient-based training. Other work has shown
that trained transformers can learn topic structure [ 43], spatial structure [ 44], visual features [ 45] and
support vectors [46, 47] in specific settings disjoint from ICL.
Expressivity of transformers. Multiple works have shown that transformers with linear [ 11,36],
ReLU [ 13,14,48], and softmax [ 12,15] attention are expressive enough to implement general-
purpose machine learning algorithms during ICL, including gradient descent. A series of works have
shown the existence of transformers that recover sparse functions of the input data [ 49–52]. [53]
studied the statistical complexity the learning capabilities of attention with random weights. More
broadly, [54–59] have analyzed various aspects of the expressivity of transformers.
Other studies of softmax attention. [60] hypothesized that the role of the softmax in attention is to
facilitate a mixture-of-experts algorithm amenable to unstructured training data. [ 27] formulated a
softmax regression problem and analyzed the convergence of a stylized algorithm to solve it. [ 22]
showed that in a setting with ICL regression tasks a la [ 28], a kernel regressor akin to softmax
attention with Mequal to the inverse covariance of xconverges to the Bayes posterior for a new ICL
task – in this setting the conditional distribution of the label given the query and nlabelled context
samples – polynomially with the number of context samples, but did not study what softmax attention
learns during pretraining. [ 61] also compared softmax and linear attention, but focused on softmax’s
greater capacity to separate data from two classes. [ 62] and [ 63] investigate the Lipschitz constant of
attention rather than what attention learns.
Non-parametric regression. Our results imply that pretraining softmax attention reduces to the
problem of meta-learning the bandwidth of a Nadaraya-Watson estimator with a Gaussian kernel.
However, to our knowledge, the non-parametric regression literature has not addressed this problem.
The closest work is [ 32], which only upper bounds the noiseless loss, and only in the limit n→ ∞ ,
whereas our result characterizes the optimal bandwidth, which requires upper and lower bounds on
the noisy loss.
B Preliminaries
We first justify our claim that the first drows of the last column of WVcan be set to 0dfor any
optimal choice of parameters.
Lemma B.1. If under the function distribution, a function fis equally likely as likely as −f, then
any optimal solution to L(WV,WK,WQ)in 3 satisfies WV=
0d×d0d×1
01×d c
.
16Proof. For readability we write βi=e−wKQ∥xi−xn+1∥2P
je−wKQ∥xj−xn+1∥2Suppose WV=
0d×dv
01×dc
was optimal, then the loss can be written
L=Ef,{xi}
 X
ic(f(xi) +ϵi)βi+X
iv⊤xiβi−f(xn+1)!2
.
But because fand−fare equally likely, and because the noise is also symmetric about 0, we can
write this as
L=1
2Ef,{xi},{ϵi}
 X
ic(f(xi) +ϵi)βi+X
iv⊤xiβi−f(xn+1)!2

+1
2Ef,{xi},{ϵi}
 X
ic((−f)(xi)−ϵi)βi+X
iv⊤xiβi−(−f)(xn+1)!2

We can couple the noise {ϵi}and the data {xi}in the two summands above to write this as
E
(A+B+C)2+ (−A+B−C)2
,
where A=P
icf(xi)βi−f(x) =−(P
ic(−f)(xi)βi),B=P
iv⊤xiβi, and C=P
icϵiβi.
We can set B= 0simply by setting v=0d×1, and this has loss
L=Ef,{xi}
 X
ic(f(xi) +ϵi)βi−f(xn+1)!2

=1
2 
E
(A+C)2+ (−A−C)2
≤1
2 
E
(A+B+C)2+ (−A+B−C)2
In all of the distributions over functions we consider for pretraining, fis equally likely as −f, so
without loss of generality we set all elements of WVbesides the (d+ 1, d+ 1)-th to 0. For simplicity,
we set the (d+ 1, d+ 1) -th element to 1.
Assumption B.2 (Covariate Distribution) .For each token x, first we draw ˜xas˜x∼ Ud. Then xis
constructed as x=Σ1/2˜x.
Definition B.3 (Linear and 2-ReLU Function Classes) .The function classes Flin
LandF+
Lare
respectively defined as:
Flin
L:={fw:fw(x) =lw⊤x+b,w∈Sd−1, l∈[−L, L]}, (5)
F+
L:={fw:fw(x) =l1ReLU (w⊤x) +l2ReLU (−w⊤x) +b,w∈Sd−1}. (6)
D(Flin
L), D(F+
L)are induced by drawing w∼ N(0,Σ−1)andb, l, l 1, l2∼Unif([−L, L]). We say
that these classes are L−Lipschitz, because the maximum Lipschitz constant for any function in the
class is L.
Note that because ∥Σ−1/2xi∥= 1always, we have
2xiMxn+1
=∥Σ−1/2xi∥2+∥Σ1/2MΣ1/2Σ−1/2xn+1∥2− ∥Σ−1/2xi−Σ1/2MΣ1/2Σ−1/2xn+1∥2.
LetM′=Σ1/2MΣ1/2. This means the attention estimator can be rewritten as
hSA(x) :=X
if(xi)ex⊤
iMxn+1
P
jex⊤
jMxn+1=X
if(xi)e−∥Σ−1/2xi−M′Σ−1/2xn+1∥2
P
je−∥Σ−1/2xj−M′Σ−1/2xn+1∥2 (7)
So the attention a token xn+1places on another xiis related to the distance between
M′Σ−1/2xn+1andΣ−1/2xi. It is natural to suppose under some symmetry conditions that M′is
best chosen to be a scaled identity matrix so that the attention actually relates to a distance between
tokens. Below we discus sufficient conditions for this.
17!"!!′Figure 6: Comparison between using Mandωin Lemma B.5. Here we denote y:=yn+1. Under
the attention induced by M, the center of attention for yis actually y′, and the attention weights are
depicted by the light orange shading. Under the attention induced by ω, the center of attention for y
isyand the weights are depicted by the light blue shading. Naturally, using the blue shaded attention
should lead to a better estimate of f(y)under mild regularity conditions.
Assumption B.4. The function class Fand distribution D(F)satisfy
1.|f(x)−f(y)| ≤L∥x−y∥Σ−1∀x,y∈ X2, f∈ F
2.Ef∼D(F)[f(x)f(y)] =ρ(x⊤y)∀x,y∈ X2,for some monotonically increasing ρ.
3. For any isometry ϕpreserving the unit sphere, and f∈ F, we have f◦ϕ∈ F.
Lemma B.5. Under Assumption B.4, any minimizer of Equation ICL satisfies M∗=wKQΣ−1for
some scalar wKQ≥0.
Proof. Let{yi}={Σ−1/2xi}. Suppose Myn+1̸=cyn+1for any c >0for some yn+1. Take
cyn+1=∥Myn+1∥andy′
n+1=Myn+1
cyn+1(the projection of yonto the sphere). Consider a function
ω:Rd→Rdsatisfying ω(yn+1) =cyn+1yn+1. Note that this need not be linear. Let ϕdenote a
rotation that sends y′
n+1toyn+1.
We show that L(M)>L(ω), that is, it is favorable to notrotate yn+1. We have
L(M) =Ef,yn+1,{yi}
 
f(yn+1)−P
if(yi)e−∥yi−Myn+1∥2
P
je−∥yj−Myn+1∥2!2

=Ef,yn+1,{yi}f(yn+1)2+Ef,yn+1,{yi}
 P
if(yi)e−∥yi−Myn+1∥2
P
je−∥yj−Myn+1∥2!2

−2Ef,yn+1,{yi}"X
if(yn+1)f(yi)e−∥yi−Myn+1∥2
P
je−∥yj−Myn+1∥2#
Lets compare this with the loss of ω. For a depiction of this, please see Figure 6
L(ω) =Ef,yn+1,{yi}
 
f(yn+1)−P
if(yi)e−∥yi−ω(yn+1)∥2
P
je−∥yj−ω(yn+1)∥2!2

=Ef,yn+1,{yi}f(yn+1)2+Ef,yn+1,{yi}
 P
if(yi)e−∥yi−ω(yn+1)∥2
P
je−∥yj−ω(yn+1)∥2!2

18−2Ef,yn+1,{yi}"X
if(yn+1)f(yi)e−∥yi−ω(yn+1)∥2
P
je−∥yj−ω(yn+1)∥2#
There are three terms to compare. The first in each is identical. The second is also the same:
Ef,yn+1,{yi}
 P
if(yi)e−∥yi−Myn+1∥2
P
je−∥yj−Myn+1∥2!2

=Eyn+1Ef,{yi}
 P
if(yi)e−∥yi−Myn+1∥2
P
je−∥yj−Myn+1∥2!2

=Eyn+1Ef,{yi}
 P
if(yi)e−∥yi−cyn+1y′
n+1∥2
P
je−∥yj−cyn+1y′
n+1∥2!2

=Eyn+1Ef,{yi}
 P
if(ϕ(yi))e−∥ϕ(yi)−cyn+1yn+1∥2
P
je−∥ϕ(yj)−cyn+1yn+1∥2!2
rotational symmetry of {yi},yn+1
=Eyn+1Ef,{yi}
 P
if(yi)e−∥yi−cyn+1yn+1∥2
P
je−∥yj−cyn+1yn+1∥2!2
 rotational symmetry of {yi}
The third takes some more work. For any choice of {yi}, let
αyn+1,{yi}(y∗) =e−∥yn+1−y∗∥2
e−∥yn+1−y∗∥2+P
je−∥yn+1−yi∥2.
We see that αyn+1,{yi}(y∗)varies monotonically with y⊤
n+1y∗for all yn+1,{yi}. That is,
y⊤
∗yn+1>y′⊤
∗yn+1=⇒αyn+1,{yi}(y∗)> αyn+1,{yi}(y′
∗),
Ef,yn+1,{yi}"X
if(yn+1)f(yi)e−∥yi−Myn+1∥2
P
je−∥yj−Myn+1∥2#
=Eyn+1,{yi}"X
iEf
f(yn+1)f(yi)
e−∥yi−Myn+1∥2
P
je−∥yj−Myn+1∥2#
=Eyn+1,{yi}"X
iρ(y⊤
n+1yi)e−∥yi−Myn+1∥2
P
je−∥yj−Myn+1∥2#
=nEyn+1,y∗,{yi}i=[n−1]"
ρ(y⊤
n+1y∗)e−∥y∗−Myn+1∥2
e−∥y∗−Myn+1∥2+P
je−∥yj−Myn+1∥2#
=nEyn+1,y∗,{yi}i=[n−1]h
ρ(y⊤
n+1y∗)αMyn+1,{yi}(y∗))i
=nEyn+1,y∗,{yi}i=[n−1]h
ρ(y⊤
n+1y∗)αcyn+1y′,{yi}(y∗))i
=nEyn+1,y∗,{yi}i=[n−1]h
ρ(y⊤
n+1y∗)αcyn+1yn+1,{ϕ−1(yi)}(ϕ−1(y∗)))i
=nEyn+1,y∗,{yi}i=[n−1]h
ρ(y⊤
n+1y∗)αcyn+1yn+1,{yi}(ϕ−1(y∗)))i
Similarly, we have
Ef,yn+1,{yi}"X
if(yn+1)f(yi)e−∥yi−ω(yn+1)∥2
P
je−∥yj−ω(yn+1)∥2#
19=Eyn+1,{yi}"X
iEf
f(yn+1)f(yi)
e−∥yi−cyn+1yn+1∥2
P
je−∥yj−cyn+1yn+1∥2#
=Eyn+1,{yi}"X
iρ(y⊤
n+1yi)e−∥yi−cyn+1yn+1∥2
P
je−∥yj−cyn+1yn+1∥2#
=nEyn+1,y∗,{yi}i=[n−1]"
ρ(y⊤
n+1y∗)e−∥y∗−cyn+1yn+1∥2
e−∥y∗−cyn+1yn+1∥2+P
je−∥yj−cyn+1yn+1∥2#
=nEyn+1,y∗,{yi}i=[n−1]h
ρ(y⊤
n+1y∗)αcyn+1yn+1,{yi}(y∗))i
Critically, for a given yn+1,αy,{yi}(y∗)can be re-parameterized as
αyn+1,{yi}(y∗) =α′
{yi}(y∗−yn+1)where α′
{yi}is symmetric about 0and decreasing. Similarly,
ρ(y⊤
n+1y∗)can be re-parameterized as ρ(y⊤
n+1y∗) =ρ′(y∗−yn+1)where α′, ρ′are symmetric
decreasing rearrangement (that is, the set of points zsuch that ρ(x)> ris a ball about the origin).
From Lemma I.2 we then have
Eyn+1Ey∗,{yi}i=[n−1]h
ρ(y⊤
n+1y∗)αcyn+1yn+1,{yi}(ϕ−1(y∗))i
=Eyn+1Ey∗,{yi}i=[n−1]
ρ′(∥yn+1−y∗∥)α{yi}(∥yn+1−ϕ−1y∗∥)
<Eyn+1Ey∗,{yi}i=[n−1]
ρ′(∥yn+1−y∗∥)α{yi}(∥yn+1−y∗∥)
=Eyn+1Ey∗,{yi}i=[n−1]h
ρ(y⊤
n+1y∗)αcyn+1yn+1,{yi}(y∗)i
SoL(ω)<L(M). Let
q(cyn+1) =Ef,{yi}
 
f(yn+1)−P
if(yi)e−∥yi−cyn+1yn+1∥2
P
je−∥yj−cyn+1yn+1∥2!2
.
Observe that L(ω) =Eyn+1q(cyn+1). We might as well set ωto be such that cyn+1is the same for
allyn+1and a minimizer of q, so we have ω(yn+1) =cyn+1for all yn+1which implies ω=cId
for some c. Because the optimal M′is identity, the corresponding optimal MisΣ−1.
B.1 Rewriting the Loss
As a result of this, we can take M=wKQΣ−1and write the attention estimator as
hSA(x) =X
if(xi)e−wKQ∥Σ−1/2xi−Σ−1/2xn+1∥2
P
je−wKQ∥Σ−1/2xj−Σ−1/2xn+1∥2 (8)
This allows us to make the transformation X → Σ−1/2X. This has the effect of making both the
data covariance and the induced function class covariance equal to the identity. Essentially, WLOG
we will henceforth consider Σ=Id. Henceforth, the estimator will be taken to be
hSA(x) =X
if(xi)e−wKQ∥xi−xn+1∥2
P
je−wKQ∥xj−xn+1∥2 (9)
and the loss will be parameterized by wKQas
L(wKQ) =Ef,{xi}
 X
i(f(xi) +ϵi)e−wKQ∥xi−xn+1∥2
P
je−wKQ∥xj−xn+1∥2−f(xn+1)!2
.
Because the noise ϵiis independent of everything else, we can decompose this into two terms, a
signal term and a noise term as follows
L(wKQ) =Ef,{xi}
 X
i(f(xn+1)−f(xi))e−wKQ∥xi−xn+1∥2
P
je−wKQ∥xj−xn+1∥2!2

| {z }
Lsignal(wKQ)
20+Ef,{xi}
 X
iϵie−wKQ∥xi−xn+1∥2
P
je−wKQ∥xj−xn+1∥2−f(xn+1)!2

| {z }
Lnoise(wKQ)
We bound the first term in Appendix C and the second in Appendix D. A useful function that we
bound in Lemma G.4 and Corrolary G.5 in Appendix G is
gp(r) =nX
i=1∥xi−x∥pe−r∥x⊤
i−x2∥.
We will use this function, particularly for p= 0and1.
C The Signal Term
The purpose of this section of the Appendix is to obtain upper and lower bounds on Lsignal(wKQ).
Because we work with two different distributions over functions, and because the bounds depend on
the distributions, we will make the distribution explicit in the argument to the function
Lsignal(wKQ;D(F)) =Ef,{x} 
f(xi)−X
if(xi)e−wKQ∥xi−xn+1∥2
P
je−wKQ∥xj−xn+1∥2!2
As a reminder, we consider the following two distributions over functions. Please see section B.1 to
see why we have set the covariance of wto be identity.
Definition C.1 (Affine and 2-ReLU Function Classes) .The function classes Faff
LandF+
Lare
respectively defined as:
Faff
L:={f:f(x) =lw⊤x+b,w∈Sd−1},
F+
L:={f:f(x) =l1ReLU (w⊤x) +l2ReLU (−w⊤x) +b,w∈Sd−1}.
D(Faff
L), D(F+
L)are induced by taking w∼ Ud,b, l, l 1, l2∼Unif[−L, L].
First we have the following trivial bound on Lsignal(wKQ).
Lemma C.2. For all wKQwe have Lsignal(wKQ)≤4L2.
Proof. We have Lsignal(wKQ)≤EPf(xi)−f(xn+1)γiPγi2
for some positive {γi}. By Lipschitz-
ness, f(xi)−f(xn+1)≤L∥xi−xn+1∥ ≤2L.
C.1 Affine functions
Here we consider the affine function class Faff
L. First, we note that this class satisfies Assumption B.4.
Lemma C.3. The affine class Faff
Lin Definition 3.2 satisfies Assumption B.4.
Proof. 1. We have |f(x)−f(y)|=|w⊤(x−y)| ≤ ∥w∥∥x−y∥by Cauchy-Schwarz.
2. Because bis independent of w, we have
Ef[f(x)f(y)] =Ew
l2x⊤ww⊤y+b2
=El2Ew∥w∥2
dx⊤y+L2
3.
3.wis isotropic, so ϕ(w)is also supported by the distribution on w.
21Lemma C.4. For affine functions, the signal term is upper bounded as
Lsignal(wKQ;D(Faff
L))≤

L2O 
1
w2
KQ+wd
2−1
KQ
n+1
n!
wKQ≥d+√
d
2
4L2wKQ<d+√
d
2
Proof. In the interest of readability, we will denote xn+1asx. Consider ˜xsuch that ˜x=
P
ixie−2wKQx⊤
ix
P
je−2wKQx⊤
ix. Then our loss is given by E
l2w⊤(x−˜x)2. First, since wis indepen-
dent of x,{xi}, we have El2 
w⊤(x−˜x)2=El2ww⊤(x−˜x)(x−˜x)⊤, Now whas a uniformly
randomly chosen direction, so its covariance is a multiple of the identity. We have ETr(ww⊤) =
E∥w∥2=L2
3, soEl2ww⊤=L2
3dId. Continuing, E 
w⊤(x−˜x)2=L2
3dE∥x−˜x∥2. Take any
x′⊥x, we have
E˜x⊤x′=EX
ix⊤
ix′e−2wKQx⊤
ix
P
je−2wKQx⊤
ix
=EX
iE[x⊤
ix′|x⊤
i]e−2wKQx⊤
ix
P
je−2wKQx⊤
ix= 0 iterated expectation and symmetry
Decomposing ˜xinto an orthogonal and a parallel component, we have E∥x−˜x∥2=
E∥x−xx⊤˜x−x′x′⊤˜x∥2for some x′⊥xwith∥x′∥= 1. But
E∥x−xx⊤˜x−x′x′⊤˜x∥2
=E∥x(1−x⊤˜x)∥2+E∥x′x′⊤˜x∥2−2Ex(1−x⊤˜x)˜x⊤x′x′⊤
=E∥x(1−x⊤˜x)∥2+E∥x′x′⊤˜x∥2∵x⊤x′= 0 = ⇒2Ex(1−x⊤˜x)˜x⊤x′x′⊤= 0
(10)
Case 1: wKQ≥d+√
d
2.
Consider first the term E∥x(1−x⊤˜x)∥2=E(1−x⊤˜x)2. Here we have with probability 1−1
n
1−x⊤˜x=P(1−x⊤xi)e−wKQ∥x−xi∥2
Pe−wKQ∥x−xi∥2 =g2(wKQ)
2g0(wKQ)
≤Cbn
1
wKQd
2+1
2Cbn
1
wKQd
2≤Cb
Cb1
wKQCorollary G.5 (11)
The other term E∥x′x′⊤˜x∥2=E(x′⊤˜x)2is the component of the bias in the direction orthogonal
tox.
(x′⊤˜x)2= P
ix′⊤xie−wKQ∥xi−x∥2
P
ie−wKQ∥xi−x∥2!2
≤ P
ix′⊤xie−wKQ∥xi−x∥2
P
ie−wKQ∥xi−x∥2!2
≤ P
ix′⊤xie−wKQ∥xi−x∥2
P
ie−wKQ∥xi−x∥2!2
≤P
i 
1−(x⊤xi)2
e−2wKQ∥xi−x∥2
 P
ie−wKQ∥xi−xn∥22Popoviciu’s Variance inequality
≤P
i2 
1−x⊤xi
e−2wKQ∥xi−x∥2
 P
ie−wKQ∥xi−x∥22
22≤P
i∥xi−x∥2e−2wKQ∥xi−x∥2
 P
ie−wKQ∥xi−x∥22=g2(2wKQ)
g2
0(wKQ)
With probability 1−1
n, when wKQ≥d+√
dwe have
g2(2wKQ)
g0(wKQ)2≤cgn
1
2wKQd
2+1

cgn
1
wKQd
22≤cgwd
2−1
KQ
cg22d
2+1n(12)
Putting together Equations 11 and 12, we have with probability 1−1
n,
Lsignal(wKQ;D(FL))≤ O
L2
3d
1
wKQ+wd
2−1
KQ
n

.
The signal bias is upper bounded by 4L2always (Lemma C.2). The overall upper-bound on the
expectation is
Lsignal(wKQ;D(FL))≤ O
L2
3d
1
wKQ+wd
2−1
KQ
n+ 4

.
Case 2: wKQ<d+√
d
2. We always have L(wKQ)≤4L2from Lemma C.2.
Lemma C.5. For affine functions, the signal term is lower bounded as
Lsignal(wKQ;D(Faff
L))≥(
Ω
L2
w2
KQ
wKQ>d+√
d
2
Ω (1) wKQ<d+√
d
2.
Proof. Similar to Equation (10), for ˜x=P
ixie−2wKQx⊤
ix
P
je−2wKQx⊤
ix, we have
Lsignal(wKQ;D(Faff
L))≥L2
3dE∥x(1−x⊤˜x)∥2=L2
3dE(1−x⊤˜x)2
Now consider the term 1−x⊤˜x. We have
P(1−x⊤xi)e−wKQ∥x−xi∥2
Pe−wKQ∥x−xi∥2 ≥g2(wKQ)
2g0(wKQ)
Case 1: wKQ≥d+√
d
2. Here we have from Corollary G.5, with probability 1−1/n
P(1−x⊤xi)e−wKQ∥x−xi∥2
Pe−wKQ∥x−xi∥2 ≥Cbn
1
wKQd
2+1
2Cbn
1
wKQd
2≥Cb
2Cb1
wKQ.
With probability 1/n≤1
2the lowest we can have is Lsignal(wKQ) = 0 , so overall we have
Lsignal(wKQ)≥L2
24dCb
Cb1
wKQ2
Case 2:d+√
d
4≤wKQ≤d+√
d
2. From Corollary G.5, with probability 1−1
n
P(1−x⊤xi)e−wKQ∥x−xi∥2
Pe−wKQ∥x−xi∥2 ≥Cbn
1
wKQd
2+1
2Cbne−2wKQ≥Cb
2Cbe2wKQ
wd
2+1
KQ.
23With probability 1/n≤1
2the lowest we can have is Lsignal(wKQ;D(Faff
L)) = 0 , so overall we have
Lsignal(wKQ;D(Faff
L))≥L2
24d
Cb
Cbe2wKQ
wd
2+1
KQ
2
Case 3:d+√
d
4> w KQ. From Corollary G.5, with probability 1−1
n
P(1−x⊤xi)e−wKQ∥x−xi∥2
Pe−wKQ∥x−xi∥2 ≥Cbne−4wKQ
2Cbne−2wKQ≥Cb
2Cbe−2wKQ.
With probability 1/n≤1
2the lowest we can have is Lsignal(wKQ;D(Faff
L)) = 0 , so overall we have
Lsignal(wKQ;D(Faff
L))≥L2
24dCb
Cbe−2wKQ2
Corollary C.6. Combining the above, we have
L2O 
1
(wKQ+ 1)2!
≤ L signal(wKQ;D(Faff
L))≤L2O
1
w2
KQ+wd
2−1
KQ
n+1
n
. (13)
We can now perturb these bounds in the case ofthe ReLU-based function class F+
L.
C.2 ReLU-based functions
Consider the function class
F+
L={l1ReLU (w⊤x) +l2ReLU (−w⊤x) +b:w∈Sd−1, b, l1, l2∈[−L, L]},
where ReLU (z) := ( z)+:= max( z,0). Consider a distributions on F+
L, namely D(F+
L). Let
D(F+
L)be induced by w∼ Ud, b, l1, l2∼Unif[−L, L]. That is, a vector wis drawn uniformly on
the unit hypersphere. Then two norms are selected, l1, l2, and the overall function is given by
fw,l1,l2(x) =l1ReLU (w⊤x) +l2ReLU (−w⊤x) +b,
so that it follows one affine rule in one halfspace, and another affine rule in the opposite halfspace.
Please see section B.1 to see why we have set the covariance of wto be identity.
Lemma C.7. The class F+
Land distribution D(F+
L)defined above satisfy Assumption B.4.
Proof. 1.Each function is defined as being piece-wise L-Lipschitz, and it is continuous, so it
is also L−Lipschitz overall.
2.With probability 1−2arccos(x⊤y)
πthe points xandyare such that (w⊤x)(w⊤y)<0
(that is, they are on opposite sides of the hyperplane defining the two pieces of the ReLU).
Because the bias bis independent of the other parameters, we have as in the proof of Lemma
C.3
Ef[f(x)f(y)] =L2
3+Ew
l2
1x⊤ww⊤y(w⊤x)(w⊤y)≥0]P[(w⊤x)(w⊤y)≥0]
+Ew
l1l2x⊤ww⊤y(w⊤x)(w⊤y)<0]P[(w⊤x)(w⊤y)<0]
=L2
3+Ew
l2
1x⊤ww⊤yx⊤ww⊤y>0] 
2arccos 
x⊤y
π!
∵l1⊥l2
Letx=x
∥x∥for any vector x. Consider a re-parameterization of the pair (x,y)as
ξθ(x,y)→(x+y,x−y). Because xandyare on the unit sphere, this is a bijection as
ξ−1
θ(x,y) =1 +θ
2x+1−θ
2y,1 +θ
2x−1−θ
2y
.
24That is, for any x,y,ξ−1
x⊤y(ξx⊤y(x,y)) = (x,y). The push-forward of ξis also uniform,
that is for x,ysatisfying x⊤y=θ,ξθ(x,y)is distributed as Ud×Ud−1. For any x,y,
letξ−1
θ(x,y) = (xθ,yθ). Then we have Ef[f(xθ)f(yθ)]is a decreasing function of
θ. Finally, for θ≤θ′,L2x⊤
θww⊤yθ> L2x⊤
θ′ww⊤yθ′sox⊤
θww⊤yθ<0 =⇒
x⊤
θ′ww⊤yθ′<0. The product of two positive increasing functions is itself non-increasing.
Since we have both Ew
L2x⊤ww⊤yx⊤ww⊤y>0]and2 arccos (x⊤y)
πare increasing
functions of x⊤y, we also have
Ew
L2x⊤ww⊤yx⊤ww⊤y>0] 
2 arccos 
x⊤y
π!
is an increasing function of x⊤ysinceEw
L2x⊤ww⊤yx⊤ww⊤y>0]≥0and
2 arccos (x⊤y)
π
≥0.
3.wis distributed uniformly on the hypersphere, so ϕ(w)is also also distributed uniformly on
the hypersphere for any isometry ϕthat preserves the origin.
Lemma C.8. The signal term is upper bounded as
Lsignal(wKQ;D(F+
L))≤(
L2O
1
wKQ+1
n
wKQ≥d+√
d
2
4L2wKQ<d+√
d
2
Proof. We have
Lsignal(wKQ;D) =Ef,{xi} P
i(f(xi)−f(xn))e−wKQ∥xi−xn∥2
P
ie−wKQ∥xi−xn∥2!2
≤Ef,{xi} P
iL∥xi−xn∥e−wKQ∥xi−xn∥2
P
ie−wKQ∥xi−xn∥2!2
≤
Lg1(wKQ)
g0(wKQ)2
With probability 1−1
n, when wKQ≥d+√
d
2we have
g1(wKQ)
g0(wKQ)≤Cbn
1
wKQd+1
2
Cbn
1
wKQd
2≤Cb
Cb1
wKQ1
2
We always have Lsignal(wKQ)≤4L2from Lemma C.2. So the overall upper bound is
Lsignal(wKQ;D)≤L21
wKQ+4
n
ForwKQ≥d+√
d
2, as before, we always have Lsignal(wKQ;D)≤4L2.
Lemma C.9. The signal term is lower bounded as
Lsignal(wKQ;D(F+
L))≥ L signal(wKQ;D(Faff
L))/2
Proof. Again for readability we will write xn+1asx. For any f∈ F+
Lletfx,affdenote the
corresponding affine function that is equal to fin the halfspace containing x, that is if f(x′) =
l1ReLU (w⊤x′) +l2ReLU (−w⊤x′) +b, and WLOG w⊤x′>0, then fx,aff(x′) =l1w⊤x′+b.
Note that fx,affcomes from a wselected from the unit sphere and b, l∈[−L, L]exactly as f∼
25D(FL), so it is actually statistically indistinguishable from a sample from D(Faff
L), the distribution
over affine functions in Definition 3.2 (and the object of Lemma C.5). The error of the nonlinear
estimator can be written as
Ef,x,{xi}
 X
if(xi)γi−f(xn)!2

where γi=e−wKQ∥x−xi∥2
Σ−1
P
je−wKQ∥x−xj∥2
Σ−1Let us compare the two errors due to the two functions. Let
A={i: (x⊤
iw)(x⊤w)<0}denote the set of points on the opposite side to xof the hyperplane
defining the function.
Lsignal(wKQ;D(F+
L))
=Ef,x,{xi}
 X
if(xi)γi−f(x)!2

=Ef,x,{xi}
 X
fx,aff(xi)γi+X
i∈A(f(xi)−fx,aff(xi))γi−f(x)!2

=Ex,{xi}Ef

X
i̸∈Afx,aff(xi)γi−fx,aff(x)
2
+Ef
 X
i∈Af(xi)γi)!2

=Ex,{xi}Ef
 X
ifx,aff(xi)γi−fx,aff(x)!2
+Ef
 X
i∈Afx,aff(xi)γi!2

−2EfX
fx,aff(xi)γi−fx,aff(x) X
i∈Afx,aff(xi)γi!
+Ef
 X
i∈Af(xi)γi)!2

≥Ef,x,{xi}X
fx,aff(xi)γi−fx,aff(x)2
+Ef,x,{xi} X
i∈Afx,aff(xi)γi!2
−2vuuutEf,x,{xi}X
fx,aff(xi)γi−fx,aff(x)2
Ef,x,{xi}
 X
i∈Afx,aff(xi)γi!2

+Ef,x,{xi} X
i∈Af(xi)γi)!2
Here the third equality holds because f(xi)is independent of fx,aff(xj)ifi∈A, j̸∈A.
Letq=Ef,x,{xi} P
i∈Af(xi)γi)2=Ef,x,{xi} P
i∈Afx,aff(xi)γi)2. Then from the above we
have
Ef,x,{xi}
 X
if(xi)γi−f(x)!2
≥(Lsignal(wKQ;D(FL))(wKQ)−q)2+q2,
which has minimum at q=Lsignal(wKQ;D(FL))/2, completing the proof.
D Bounds on Noise Variance
In this section we obtain upper and lower bounds on the variance of the estimator due to label noise.
There are three relevant parameters: d, the ambient dimension of the data; wKQ, the scaling induced
26by the attention layer; and n, the number of tokens. Recall that the noise term is
Lnoise(wKQ) =Ef,{xi}
 X
iϵie−wKQ∥xi−xn+1∥2
P
je−wKQ∥xj−xn+1∥2!2

Because the ϵiare independent, this can further be simplified as
Lnoise(wKQ) =σ2E{xi}
X
ie−2wKQ∥xi−xn+1∥2
P
je−wKQ∥xj−xn+1∥22

Lemma D.1. The noise term is bounded for d+√
d≤wKQ≤
n
45√
dlogn2
das
Ω
σ2wd
2
KQ
n
≤ L noise(wKQ)≤ O
σ2
1 +wd
2
KQ
n
.
Proof. We have
Lnoise(wKQ) =σ2E
X
ie−2wKQ∥xi−xn∥2
P
je−wKQ∥xj−xn∥22
=σ2Eg0(2wKQ)
g0(wKQ)2
.
Using Lemma G.5, we have with probability at least 1−1
n
g0(2wKQ)
g0(wKQ)2≤cnn
1
w2KQd
2

cnn
1
wKQd
22≤cn
cn2wd
2
KQ
n
and similarly
g0(2wKQ)
g0(wKQ)2≥cnn
1
w2KQd
2

cnn
1
wKQd
22≤cn
cn2wd
2
KQ
n
Finally, in the worst case, we have 0≤ L noise(wKQ)≤1.
Finally, we show that the noise term is monotonic in wKQ.
Lemma D.2. Lnoise(w)>Lnoise(w′)⇐⇒ w > w′
Proof. Letai=e−w′∥xi−xn+1∥2, bi=e−(w−w′)∥xi−xn+1∥2. The result follows from Lemma I.3
because {ai}and{bi}satisfy ai> aj⇐⇒ bi> bj⇐⇒ ∥ xi−xn+1∥<∥xj−xn+1∥.
E Optimizing the Loss
For the nonlinear function class F+
L, we have the following.
Theorem E.1. Suppose the functions seen in pretraining are drawn from D(F+
L)as in Definition 3.2,
the covariates are drawn as Assumption 3.3, n= Ω
Llogn
σd
andn2
d+2= Ω(1) , then the optimal
Msatisfies
M=wKQId (14)
where wKQsatisfies
Ω 
nL21
d+2
≤wKQ≤ O 
nL22
d+2
. (15)
27bias upper bound
bias lower bound
noise
overall upper bound
overall lower boundFigure 7: Left: Rough upper and lower bounds for the bias term (shaded region), along with the
noise variance (gray). Right: Overall upper and lower bound for the in-context loss. The horizontal
dashed line establishes an upper bound for the optimal loss, while the vertical dashed lines establish
lower and upper bounds for the parameter wKQthat can attain the optimal loss.
Proof. We consider three regions in which the optimal value could potentially lie and see that only
the third region is viable.
Case 1. wKQ≤d+√
d: In this case, the signal term lower bounds the optimal loss by Lemma C.5
asΩ(1) .
Case 2. wKQ>Ω
n
logn2
d. In this case, the noise term lower bounds the optimal loss. From Lemma
D.2 we know that the noise term is non-decreasing in wKQso in the range wKQ>Ω(
n
logn2
dis
lower bounded by Lnoise(wKQ)atwKQ= Ω(
n
logn2
d, which is Ω
σ2
logn
.
Case 3. d+√
d≤wKQ≤Ω
n
logn2
dBy combining Lemmas C.8, C.9, and D.1 , we obtain the
following overall bound on the loss:
c
L2
(wKQ+ 1)2+σ2wd
2
KQ
n
≤ L(wKQ)≤c
L2
wKQ+σ2wd
2
KQ
n+σ2+L2
n

for some constants c, cthat only depend on d. In the range wKQ≥d+√
d, we have wKQ>1
andwKQ≤n, so the upper bound can be relaxed as Lnoise(wKQ)≤2c 
L2
n+σ2wd
2
KQ
n!
, which is
minimized at wKQ=
nL2
σ2d2
d+2. Here it is upper bounded by 4c
dLdσ2
n2
d+2. We note first of all
that for large enough n(as long as n= Ω
σlogn
Ld
andn2
d+2= Ω(1) ) this is lower than the lower
bounds we got in Case 1 andCase 2 , so this is indeed the region of global optimal solution. From
Lemma C.9 we have Lnoise(wKQ)≥L2
w2
KQ+σ2wd
2
KQ
n≥L2
w2
KQwhich gives
cL2
w2
KQ≤ L noise(wKQ)≤4cL2σ2d
nL22
d+2
=⇒nL2
dσ21
d+2rc
4c≤wKQ
for the upper bound, we similarly also have Lnoise(wKQ)≥L2
w2
KQ+σ2wd
2
KQ
n≥σ2wd
2
KQ
nwhich gives
4cdLdσ2
n2
d+2
≥σ2wd
2
KQ
n
28=⇒wKQ≤nL2
σ22
d+2
4c
cd2
d+22
d
Of course, for this to not be vacuous we need
nL2
σ22
d+2
4c
cd2
d+22
d
≤1
45√
dn
logn2
d
.
We will again hide constants that depend only on dand write this as
c1nL2
σ22
d+2
≤c2n
logn2
d
which is true as long as n >
Llogn
σd
.
For the affine function class Faff
L, we have the following
Theorem E.2. If the functions seen in pretraining are drawn from D(Faff
L)as in Definition 3.2, and
the noise variance σ2and Liphscitz constant Lsatisfies n≥
Llog2n
σd+2
, and n2
d≥Ω(1) , and
the covariates are drawn as Assumption 3.3, the optimal Msatisfies
M=wKQId (16)
where wKQsatisfies
Ω 
nL21
d+4
≤wKQ≤ O 
nL22(d+2)
d(d+4)
. (17)
Proof. Again we work with three cases.
Case 1. wKQ≤d+√
d. Again in this case we have a lower bound to the signal term of Ω(1) .
Case 2. wKQ≥Ω
n
logn2
d. Again we have a lower bound of Ω
σ2
logn
Case 3. d+√
d≤wKQ≤Ω
n
logn2
dCombining Lemmas C.4, C.5, D.1 is
c
L2
(wKQ+ 1)2+σ2wd
2
KQ
n
≤ L(wKQ)≤c
L2
w2
KQ+σ2wd
2
KQ
n+L2wd
2−1
KQ
n+L2+σ2
n

We will minimize the upper bound. First supposeL2
σ2≥wKQfor the wKQthat minimizes the upper
bound. Then we have
L(wKQ)≤c
L2
w2
KQ+σ2
n+ 2L2wd
2−1
KQ
n

This upper bound is minimized at wKQ=n2
d+2. However, this contradicts the constraint that
wKQ≤L2
σ2, when n2
d+2≥L2
σ2, as we assume. So we have wKQ≥L2
σ2for the minimizer. This means
the upper bound is no more than
L(wKQ)≤c
L2
w2
KQ+σ22wd
2
KQ
n+σ2+L2
n

This upper bound is minimized at wKQ=
nL2
σ2d2
d+4where it is upper bounded by
Lnoise(wKQ)≤4L2cσ2d
nL22
d+4
+L2
n≤5L2cσ2d
nL22
d+4
.
29whenever n≥L2
σ2. We see that
L(wKQ)≥c
L2
w2
KQ+σ2wd
2
KQ
n
≥cL2
w2
KQ
=⇒cL2
w2
KQ≤5L2cσ2d
nL22
d+4
=⇒nL2
σ21
d+4rc
5c1
d1
d+4
≤wKQ
for the upper bound, we similarly also have
L(wKQ)≥c
L2
w2
KQ+σ2wd
2
KQ
n
≥cσ2wd
2
KQ
n
=⇒wKQ≤nL2
σ22(d+2)
d(d+4)
5c
cd2
d+42
d
Of course, for this to not be vacuous we need
nL2
σ22(d+2)
d(d+4)
5c
cd2
d+42
d
≤1
45√
dn
logn2
d
.
We will again hide constants that depend only on dand write this as
c1nL2
σ22(d+2)
d(d+4)
≤c2n
logn2
d
which again is true as long as n= Ω
Llog2n
σd+2
E.1 Generalization Bounds
We conclude this section with a proof of the generalization error on a new L−Lipschitz task.
Theorem E.3. Suppose our attention is first pretrained on tasks drawn from D(F+
L)and then tested
on an arbitrary L−Lipschitz task, then the loss on the new task is upper bounded as L ≤ O
L2
Λβ
.
Furthermore, if the new task is instead drawn from D(F+
L′), the loss is lower bounded as L ≥
min{Ω(L′2
Λ2β),Ω(Λβd/2
n)}
Proof. We know from Theorem E.2 that Ω(Λβ)≤wKQ≤ O(Λ2β). The upper bound for L(wKQ),
which is O(L2
wKQ+wd
2
KQ
n), is a convex function for d≥2, so in any range it attains its maximum value
at the extreme points. We can check the cases to see that this is O(max{L2
Λβ+Λdβ/2
n,L2
Λ2β+Λdβ
n}) =
O(L2
Λβ+Λdβ/2
n+L2
Λ2β+Λdβ
n) =O(L2
Λβ)for large enough n.
Now consider testing on a new task from D(F+
L′). The ICL loss for Ω 
Λβ
≤wKQ≤ O 
Λ2β
is
bounded below as Ω(L′2
Λ2β)andΩ(Λβd/2
n).
The implication of this is that if L′≫L, the error scales as (L′)2rather than (L′)2d
d+2while for
L′≪L, the error is lower bounded by a constant.
30F Lower Bound for Linear Attention
In this section we prove Theorem 3.6.
Lemma F.1. Consider the function distributions D(FL)andD(F+
L)described in Definition 3.2.
We have LLA≥Ω(L2), that is, the ICL error is lower bounded as Ω(L2).
Proof. We start by decomposing the ICL loss into a bias dependent term and a cenetered term. For
f∈ FL∈ {Faff
L,F+
L}, letfdenote the centered function f−Exf. Letf′denote the flip of fabout
its expected value, so f′=Exf−f. We observe that fis independent of Exf. For linear attention,
we have, for f∼D(FL)
LLA(M) =Ef,{xi}i,{ϵi}ih
(hLA(xn+1)−f(xn+1))2i
=Ef,{xi}i,{ϵi}i
 nX
i=1 
(f(xi) +ϵi)x⊤
iMxn+1
−f(xn+1)!2

=Ef,{xi}i,{ϵi}i
 nX
i=1 
f(xi)x⊤
iMxn+1+ϵix⊤
iMxn+1+Exfx⊤
iMxn+1
−f(xn+1)!2

=Ef,{xi}i,{ϵi}i
 nX
i=1 
f(xi)x⊤
iMxn+1+ϵix⊤
iMxn+1
−f(xn+1)!2
 (18)
+Ef,x,{xi}
 X
i(Exf)x⊤
iMxn+1!2

≥Ef,{xi}i,{ϵi}i
 nX
i=1 
f(xi)x⊤
iMxn+1+ϵix⊤
iMxn+1
−f(xn+1)−Exf!2

(19)
By symmetry, this is also equal to the same expression using f′instead of f, since fandf′are
distributed identically. Besides, Exf=Exf′andϵis symmetric about the origin, so
LLA(M)≥Ef,{xi}i,{ϵi}i
 nX
i=1 
f′(xi)x⊤
iMxn+1+ϵix⊤
iMxn+1
−f′(xn+1)−Exf′!2

=Ef,{xi}i,{ϵi}i
 nX
i=1 
f′(xi)x⊤
iMxn+1−ϵix⊤
iMxn+1
−f′(xn+1)−Exf!2

=Ef,{xi}i,{ϵi}i
 
− nX
i=1 
f(xi)x⊤
iMxn+1+ϵix⊤
iMxn+1
−f(xn+1)!
−Exf!2

LetA=Pn
i=1 
f(xi)x⊤
iMxn+1+ϵix⊤
iMxn+1
−f(xn+1)andB=Exf. Then we see that
LLA(M)≥1
2E(A+B)2+1
2E(−A+B)2=EA2+EB2. Meanwhile, E(Exf)2is just the
variance of the signal term in D(Faff
L)orD(F+
L), which isL2
3. SoLLA(M)≥L2
3
G Bounds for gp(r)
The purpose of this section is to obtain upper and lower bounds on
gp(r) =nX
i=1∥xi−x∥pe−r∥x⊤
i−x∥2
31forp= 0,1/2,1. For this, we will need high probability upper and lower bounds on the number of
points in a spherical cap under a uniform distribution over the hypersphere. Consider npoints {xi}
drawn uniformly from σd−1, the uniform measure over Sd−1, thed−dimensional hypersphere. The
measure of the ϵ−spherical cap around x∈Sd−1,C(ϵ,x) ={x′:x′⊤x>1−ϵ}is denoted by
σϵ.
G.1 Bounds on Spherical Caps
1−i
r1−i+1
r
Figure 8: The surface area of the purple hemisphere is used to upper bound the surface area of C(i
r),
while the volume of the green hypersphere is used as a lower bound. Points in the orange region are
Si+1\Si, and their count is Ni+1−Ni.
Lemma G.1. The area of the spherical cap C(ϵ),σϵis bounded as
(2ϵ−ϵ2)d−1
2√
2dπ≤σϵ≤(2ϵ−ϵ2)d
2≤(2ϵ)d−1
2e−ϵd/4
Proof. We derive a lower bound as follows. We replace the surface area of a spherical cap in Sd−1
with a d−1dimensional ball of the same boundary. Let Vddenote the volume of a ddimensional
ball (that is, V3(r) =4
3πr3), and let Addenote the surface area of a ddimensional sphere (so
A3(a) = 4 πr2). It is known that
Vd(r) =πd
2
Γ(d
2+ 1)rd,andAd(r) =2πd
2
Γ(d
2)rd−1.
Then we have
σϵ≥Vd−1
(1−(1−ϵ)2)1
2
Ad(1)
=(1−(1−ϵ)2)d−1
2
2√πΓ(d
2)
Γ(d+1
2)
≥(1−(1−ϵ)2)d
2√
dπLemma G.6
=(2ϵ−ϵ2)d−1
2√
2dπ
The upper bound is similar. This time we replace the cap with the surface of a hemisphere with the
same boundary. We have
σϵ≤Ad
(1−(1−ϵ)2)1
2
2Ad(1)=(1−(1−ϵ)2)d−1
2
2≤(2ϵ−ϵ2)d−1
2
32We will also need upper and lower bounds on a discretized version of the incomplete gamma function.
Definition G.2. Denote by γ(d, α, m )the expression γ(d, α, m ) =Pm
i=1ide−αi.
We have the following
Lemma G.3. Ford >5,1≤α≤2,the incomplete Gamma function is bounded as
(
mde−αm−1/2≤γ(d, α, m )≤md+1e−αm−1/2m < d +√
d
Γ(d+1)
2αd+1≤γ(d, α, m )≤2Γ(d+1)
αd+1 m≥d+√
d
Proof. We compare with the Gamma function
Γ(d+ 1) =Z∞
0tde−tdt.
Note thatR∞
0tde−αtdt=1
αd+1R∞
0tde−tdt=1
αd+1Γ(d+ 1) . Because the function tde−αtis
uni-modal with maximum d
αed, we have from Lemma I.1
mX
i=1ide−αi+d
αed
+∞X
i=mide−αi≥Z∞
0tde−αtdt=1
αd+1Γ(d+ 1)
Now suppose m≥d+√
d
α. Then we have
∞X
i=mide−αi≤∞X
i=d+√
d
αide−αi
=∞X
i=d+√
d
α 
d+√
d
α!d
e−(d+√
d)i−d+√
d
αY
j=0
1
eα 
d+√
d
α+j+ 1
d+√
d
α+j!d

≤∞X
i=d+√
d
α 
d+√
d
α!d
e−(d+√
d)i−d+√
d
αY
j=0
1
eα 
d+√
d
α+ 1
d+√
d
α!d

≤∞X
i=d+√
d
α 
d+√
d
α!d
e−(d+√
d)
e−α√
d
d+√
di−d+√
d
α
= 
d+√
d
α!d
e−(d+√
d) 1
1−e−α√
d/(d+√
d)≤d
αed2√
d
α
the first inequality follows becaused+√
d
α≤m, the second follows because2d+1
2d≥2d+j+1
2d+j, the last
follows because
1 +√
d
dαd
≤e√
d
αand1
1−ex−x≤2xforx≤2. Over all, we have
mX
i=1ide−i+ 
2√
d
α+ 1!d
αed
≥Z∞
0tde−tdt=Γ(d+ 1)
αd+1
While for the upper bound we have
mX
i=1ide−αi−d
αed
≤Z∞
0tde−αtdt=Γ(d+ 1)
αd+1
Finally, we use Lemma G.3, specifically that d
αed≤1
αd+1√
2πd d
ed≤Γ(d+1)
αd+1to yield the
desired result.
33Form <d+√
d
α, we have from Lemma G.7 that mde−αm≥1√eide−αiso
mX
i=0ide−αi≥mde−αm−1
2
and
mX
i=0ide−αi≤md+1e−αm−1
2
G.2 Bounds on gp(r)
Lemma G.4. Suppose {xi}are drawn independently and uniformly from the unit hypersphere. For
n
logn≥45√
drd
2, n > 5, d > 2, p≤2, we have gp(r) =Pn
i=1∥xi−x∥pe−r∥x⊤
i−x∥2satisfies
(1−ep
2−2)n2p
2√
8e4πd1
rd
2+p
2
γ(d
2+p
2,2, r)≤gp(r)≤3n2
rd
2+p
2
γ(d
2+p
2,2, r)
with probability at least 1−1
2n
Proof. For0≤i≤rletNidenote the number, and Sidenote the set, of points satisfying
1−i
r≤x⊤
ix⇐⇒ ∥ xi−x∥ ≤ 2i
r1
2. Also denote by N−1the points satisfying x⊤
ix<0, and
letS−1denote this set. Note that
gp(r) =nX
i=0∥x⊤
i−x∥pe−r∥x⊤
i−x∥2
=r−1X
i=0X
j∈Si+1\Si∥x⊤
i−x∥pe−r∥x⊤
j−x∥2+X
j∈S−1∥x⊤
i−x∥pe−r∥x⊤
j−x∥2
≤r−1X
i=02(i+ 1)
rp
2
e−2i(Ni+1−Ni) + 2pe−2rN−1
Similarly,
h(r)≥r−1X
i=02i
rp
2
e−2(i+1)(Ni+1−Ni)
Note that because Ni>0,
r−1X
i=02(i+ 1)
rp
2
Ni+1e−2i≥r−1X
i=02(i+ 1)
rp
2
(Ni+1−Ni)e−2i
And similarly,
r−1X
i=02i
rp
2
Ni+1e−2i=r−1X
i=12i
rp
2iX
j=0(Nj+1−Nj)e−2i∵i= 0 = ⇒2i
r= 0
=r−1X
j=1(Nj+1−Nj)r−1X
i=j2i
rp
2
e−2i
≤r−1X
j=1(Nj+1−Nj)∞X
i=j2i
rp
2
e−2i
34≤r−1X
j=1(Nj+1−Nj)∞X
i=j2j
rp
2
e−2j

j+1
jp
2
e2
i−j
∵i < jj+ 1
ji−j
≤r−1X
j=1(Nj+1−Nj)∞X
i=j2j
rp
2
e−2j
ep
2j−2i−j
∵1 +x≤ex
≤r−1X
j=1(Nj+1−Nj)∞X
i=j2j
rp
2
e−2j
ep
2−2i−j
∵j≥1
≤r−1X
j=1(Nj+1−Nj)2j
rp
2
e−2j 1
1−ep
2−2∵p <4
and so

1−ep
2−2r−1X
i=02i
rp
2
Ni+1e−2(i+1)≤r−1X
j=0(Nj+1−Nj)2i
rp
2
e−2(i+1)
By a Chernoff bound for Binomial random variables, we have with probability 1−r
n2:
Ni=nσi
r≤nσi
r+q
6nlognσi
r≤2nσi
r∀r
and
Ni=nσi
r≥nσi
r−q
4nlognσi
r≤1
2nσi
r
Whenever
nσi
r≥16 log n∀i←1√
2πd1
rd
2
≥16 log n
n
and
N−1≤n
Over all we have with probability 1−r
n2
h(r)≤r−1X
i=02(i+ 1)
rp
2
Ni+1e−2i+ 2pN−1e−2r
≤nr−1X
i=02e−2i2(i+ 1)
rd
2+p
2
+ 2pe−2rn
= 2ne22
rd
2+p
2rX
i=1id
2+p
2e−2i+ 2pe−2rn
= 2ne22
rd
2+p
2
γ(d
2+p
2,2, r) + 2pe−2rn Definition G.2
We always have for p≤2
2ne22
rd
2+p
2
γ(d
2+p
2,2, r)≥2pe−2rn
←d
2d
2
e2r2−p≥rd+p
2
So at last, we have
gp(r)≤16n2
rd
2+p
2
γ(d
2+p
2,2, r)
35We obtain a lower bound in the same way.
h(r)≥(1−ep
2−2)r−1X
i=02i
rp
2
e−2(i+1) n
2√
2πdi
rd
2
≥(1−ep
2−2)n2p
2√
8e4πd1
rd
2+p
2r−1X
i=0e−2iid
2+p
2
≥(1−ep
2−2)n2p
2√
8e4πd1
rd
2+p
2
γ(d
2+p
2,2, r)
(1−ep
2−2)n2p
2√
8e4πd1
rd
2+p
2
γ(d
2+p
2,2, r)≤h(r)≤3n2
rd
2+p
2
γ(d
2+p
2,2, r)
with probability 1−r
n2≥1−1
2nwhenn
logn≥45√
drd
2
It will be useful to simplify this bound in regimes that we are interested in
Corollary G.5. Suppose {xi}are drawn independently and uniformly from the unit hypersphere.
Forn
logn≥45√
drd
2, n > 5,p≤2≤d, we have gp(r) =Pn
i=1∥xi−x∥pe−r∥x⊤
i−x∥2satisfies
with probability 1−1
2n 

gp(r) = Θ
n
rd+p
2
r≥d+√
d
2
gp(r) = Θ 
ne−2r
r <d+√
d
2
The following bounds are known for the Gamma function.
Lemma G.6. The Gamma function satisfies
1.√
2πd d
ed≤Γ(d+ 1)≤e√
2πd d
ed
2.Γ(x+1
2)
Γ(x+1)≥1√x+0.5
Proof. 1. Please see [64].
2. Please see [65].
Lemma G.7. The following inequality holds:

1 +1√
dd
e−√
d≥e−1
2 (20)
Proof. Take the logarithm of both sides, we have that this is equivalent to
dlog
1 +1√
d
≥√
d−1
2
A Taylor series expansion of log(1 + x)demonstrates that log
1 +1√
d
=P
i(−1)i+1 1
i√
di. For
d >1, these terms are decreasing in absolute value beyond i= 2, so we can upper bound the log
with just the first two terms: log
1 +1√
d
≥1√
d−1
2d.
H Attention Window Captures Appropriate Directions
In this section we prove Theorem 4.4, which entails showing that if the Lipschitzness of the function
class is zero in some directions, one-layer self-attention learns to ignore these directions when the
function class consists of linear functions. First, we give a brief sketch of the proof.
36H.1 Proof Sketch
We briefly sketch the proof of Theorem 4.4. WLOG we write M=BF+B⊥Gwhere F:=B⊤M
andG:=B⊤
⊥M. Lemma H.2 leverages the rotational symmetry of FBincol(B)to show that the
loss is minimized over (F,G)at(F,G) = ( cB⊤, c′B⊥)for some constants c, c′. It remains to
show that L(cBB⊤+c′B⊥B⊤
⊥)>L(cBB⊤)whenever c′is nonzero. Intuitively, if the attention
estimator incorporates the closeness of B⊤
⊥xiandB⊤
⊥xn+1into its weighting scheme via nonzero
Q, this may improperly up- or down-weight f(xi), since projections of xionto col(B⊥)do not carry
any information about the closeness of f(xi)andf(xn+1).
Using this intuition, we show that for any fixed c′and{vi}isuch that c′v⊤
ivn+1̸=v⊤
i′Qvn+1for
some i, i′, the attention estimator improperly up-weights f(x1), where 1∈arg max ic′v⊤
ivn+1
WLOG. In particular, the version of the pretraining population loss (ICL) with expectation over a,
{ui}iand{ϵi}iis reduced by reducing c′v⊤
1vn+1. The only way to ensure all {c′v⊤
ivn+1}iare
equal for all instances of {vi}iis to set c′= 0, so this c′must be optimal.
To show that reducing c′v⊤
1vn+1reduces the loss with fixed {vi}i, we define αi:=ecvc′v⊤
ivn+1for
alli∈[n]and show the loss’ partial derivative with respect to α1is positive, i.e.
∂
∂α1 
˜L(c,{αi}i) :=Ea,{ui}i,{ϵi}i"Pn
i=1(a⊤ui−a⊤un+1+ϵi)ecc2
uu⊤
iuαiPn
i=1ecc2uu⊤
iun+1αi2#!
>0.(21)
This requires a careful symmetry-based argument as the expectation over {ui}icannot be evaluated
in closed-form. To overcome this, we fix all uibutu1and one other ui′̸=un+1withαi′< α1. We
show the expectation over (u1,ui′)can be written as an integral over (y1,y2)∈Sk−1×Sk−1of
a sum of the derivatives at each of the four assignments of (u1,ui′)to(y1,y2), and show that this
sum is always positive. Intuitively, any “bad” assignment for which increasing α1reduces the loss is
outweighed by the other assignments, which favor smaller α1. For example, if y1=un+1̸=y2, and
u1=y1andui′=y2, we observe from (21) that increasing α1can reduce the loss. However, the
cumulative increase in the loss on the other three assignments due to increasing α1is always greater.
H.2 Full Proof
We now prove Theorem 4.4 in full detail.
Lemma H.1. For any u∈Sk−1andα1, . . . , α nsuch that miniαi>0, and any ca, cu∈R\ {0},
define
J(c) :=c2
ac2
uE{ui}i∈[n]"Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ec2
ucu⊤
iu+c2
ucu⊤
juαiαj
(Pn
i=1ec2ucu⊤
iuαi)2#
+σ2E{ui}i∈[n]"Pn
i=1e2c2
ucu⊤
iuα2
i
(Pn
i=1ec2ucu⊤
iuαi)2#
Then for any δ >0,0/∈arg min 0≤c≤δJ(c).
Proof. We show that there exists some arbitrarily small ϵ >0such that J(ϵ)< J(0)by showing
dJ(c)
dc
c=0<0. We have
dJ(c)
dc
= 2c4
uE{ui}i∈[n]"nX
i=1nX
i′=1nX
i′′=1(ui−u)⊤(ui′−u)(u⊤
iu−u⊤
i′′u)ec2
uc(ui+ui′+ui′′)⊤uαiαi′αi′′
(Pn
i=1ec2ucu⊤
iuαi)3#
+ 2σ2c2
uE{ui}i∈[n]"nX
i=1nX
i′=1nX
i′′=1(u⊤
iu−u⊤
i′u)ec2
uc(2ui+ui′+ui′′)⊤uα2
iαi′αi′′
(Pn
i=1ec2ucu⊤
iuαi)4#
Setting c= 0results in
dJ(c)
dc
c=0=2c2
ac4
u
(Pn
i=1αi)3nX
i=1nX
i′=1nX
i′′=1E{ui}i∈[n]
(ui−u)⊤(ui′−u)(u⊤
iu−u⊤
i′′u)αiαi′αi′′
37+2σ2c2
u
(Pn
i=1αi)4nX
i=1nX
i′=1nX
i′′=1E{ui}i∈[n]"
(u⊤
iu−u⊤
i′u)α2
iαi′αi′′#
=2c2
ac4
u
(Pn
i=1αi)3nX
i=1nX
i′=1nX
i′′=1E{ui}i∈[n]
(ui−u)⊤(ui′−u)(u⊤
iu−u⊤
i′′u)αiαi′αi′′
(22)
=2c2
ac4
u
(Pn
i=1αi)3nX
i=1nX
i′=1nX
i′′=1E{ui}i∈[n] 
u⊤
iui′+ 1
(u⊤
iu−u⊤
i′′u)αiαi′αi′′
−2c4
u
(Pn
i=1αi)3nX
i=1nX
i′=1nX
i′′=1E{ui}i∈[n]
(u⊤ui′+u⊤
iu)(u⊤
iu−u⊤
i′′u)αiαi′αi′′
=−2c2
ac4
u
(Pn
i=1αi)3
×nX
i=1nX
i′=1nX
i′′=1E{ui}i∈[n]
(u⊤ui′+u⊤
iu)(u⊤
iu−u⊤
i′′u)αiαi′αi′′
(23)
=−2c2
ac4
u
(Pn
i=1αi)3nX
i′=1αi′
× nX
i=1nX
i′′=1E{ui}i∈[n]
u⊤ui′u⊤
iuαiαi′′
−nX
i=1nX
i′′=1E{ui}i∈[n]
u⊤ui′u⊤
i′′uαiαi′′!
−2c2
ac4
u
(Pn
i=1αi)3nX
i=1nX
i′=1nX
i′′=1E{ui}i∈[n]
u⊤
iu(u⊤
iu−u⊤
i′′u)αiαi′αi′′
=−2c2
ac4
u
(Pn
i=1αi)3nX
i=1nX
i′=1nX
i′′=1E{ui}i∈[n]
u⊤
iu(u⊤
iu−u⊤
i′′u)αiαi′αi′′
(24)
=−2c2
ac4
u
(Pn
i=1αi)3nX
i=1nX
i′=1nX
i′′=1αiαi′αi′′E{ui}i∈[n]
u⊤uiu⊤
iu
+2c2
ac4
u
(Pn
i=1αi)3nX
i=1nX
i′=1nX
i′′=1αiαi′αi′′E{ui}i∈[n]
u⊤uiu⊤
i′′u
=−2c2
ac4
u
k+2c4
u
k(Pn
i=1αi)3nX
i=1nX
i′=1α2
iαi′ (25)
=−2c2
ac4
u
k
1−Pn
i=1α2
i
(Pn
i=1αi)2
<0 (26)
where (22) follows since E[ui] =0k,(23) similarly follows since odd moments of uniform random
variables on the hypersphere are zero, (24) follows by the i.i.d.-ness of the ui’s,(25) follows since
E[uiu⊤
i] =1
kIkandu⊤u= 1, and (26) follows since miniαi>0. This completes the proof.
Lemma H.2. Consider any B∈Od×kand resulting function class Flin
B. Consider the training
population loss Ldefined in (ICL) , and tasks drawn from D(Flin
B)such that Ea[aa⊤] =c2
aIkfor
some ca̸= 0 and let M:=M⊤
KMQbe optimized over the domain Mˆc:={M∈Rd×d:M=
M⊤,∥B⊤MB∥2≤ˆc
c2u}for any ˆc >0. Then any
M∗∈arg min
M∈M ˆcL(M) (27)
satisfies M∗=c∗
1BBT+c∗
2B⊥B⊤
⊥for some c∗
1:|c∗
1| ∈(0,ˆc
c2u].
Proof. Without loss of generality (WLOG), we can decompose M=BF+B⊥Gwhere F:=B⊤M
andG:=B⊤
⊥M. Recall that for each i∈[n+ 1],xi=cuBui+cvB⊥vi. Thus, for each i∈[n],
38we have
ex⊤
iMxn+1=ex⊤
iBFx n+1ex⊤
iB⊥Gxn+1
=ecuu⊤
iFxn+1ecvv⊤
iGxn+1
=ecuu⊤
iFxn+1αi (28)
where, for each i∈[n],αi:=ecvv⊤
iGxn+1. For ease of notation, denote x=xn+1.
We start by expanding the square and using the linearity of the expectation to re-write the population
loss as:
L(M)
=Ea,x,{xi}i∈[n],{ϵi}i∈[n]"Pn
i=1Pn
j=1(a⊤B⊤xi−a⊤B⊤x+ϵi)(a⊤B⊤xj−a⊤B⊤x+ϵj)ex⊤
iMx+x⊤
jMx
(Pn
i=1ex⊤
iMx)2#
=c2
uEa,x,{ui},{vi}i∈[n]"Pn
i=1Pn
j=1(a⊤ui−a⊤u)(a⊤uj−a⊤u)ecuu⊤
iFx+cuu⊤
jFxαiαj
(Pn
i=1ecuu⊤
iFxαi)2#
+σ2Eu,{ui},{αi}i∈[n],{ϵi}i∈[n]"Pn
i=1e2cuu⊤
iFxα2
i
(Pn
i=1ecuu⊤
iFxαi)2#
=Ex"
c2
ac2
uE{ui},{vi}i∈[n]"Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecuu⊤
iFx+cuu⊤
jFxαiαj
(Pn
i=1ecuu⊤
iFxαi)2#
| {z }
=:˜Lsignal(M,x)
+σ2E{ui},{vi}i∈[n]"Pn
i=1e2cuu⊤
iFxα2
i
(Pn
i=1ec2uu⊤
iFxαi)2#
| {z }
=:˜Lnoise(M,x)#
(29)
WLOG we can write Fx=R(Fx)u∥Fx∥2for some rotation matrix R(Fx)∈Ok×k. Denote
C1(Fx) :=∥Fx∥2. Then we have
˜Lsignal(M,x)
=c2
ac2
uE{ui},{vi}"Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecuC1(Fx)u⊤
iR(Fx)u+cuC1(Fx)u⊤
jR(Fx)uαiαj
(Pn
i=1ecuC1(Fx)u⊤
iR(Fx)uαi)2#
=c2
ac2
uE{ui},{vi}"Pn
i=1Pn
j=1(ui−u)⊤R(Fx)R(Fx)⊤(uj−u)ecuC1(Fx)u⊤
iR(Fx)u+cuC1(Fx)u⊤
jR(Fx)uαiαj
(Pn
i=1ecuC1(Fx)u⊤
iR(Fx)uαi)2#
(30)
=c2
ac2
uE{ui},{vi}"
1
(Pn
i=1ecuC1(Fx)u⊤
iR(Fx)uαi)2
×nX
i=1nX
j=1
(R(Fx)⊤ui−R(Fx)⊤u)⊤(R(Fx)⊤uj−R(Fx)⊤u)
×ecuC1(Fx)u⊤
iR(Fx)u+cuC1(Fx)u⊤
jR(Fx)uαiαj#
=c2
ac2
uE{ui},{vi}"Pn
i=1Pn
j=1(ui−R(Fx)⊤u)⊤(uj−R(Fx)⊤u)ecuC1(Fx)u⊤
iu+cuC1(Fx)u⊤
juαiαj
(Pn
i=1ecuC1(Fx)u⊤
iuαi)2#
(31)
39where (30) follows since R(Fx)R(Fx)⊤=Ikand(31) follows since the distribution of uiis the
same as the distribution of R(Fx)⊤uifor any rotation R(Fx)⊤. Define
g(F,u,v) :=E{ui},{vi}"Pn
i=1Pn
j=1(ui−R(Fx)⊤u)⊤(uj−R(Fx)⊤u)ecuC1(Fx)u⊤
iu+cuC1(Fx)u⊤
juαiαj
(Pn
i=1ecuC1(Fx)u⊤
iuαi)2#
for any F∈Rk×d. We have Lsignal(M) =c2
ac2
uEu,v[g(F,u,v)], and Note that if F′=cB⊤, then
RF′x=IkandC1(F′x) =cuc. Thus,
g(F,u,v)−g(C1(Fx)
cuB⊤,u,v)
=E{ui},{vi}"Pn
i=1Pn
j=1(ui−R(Fx)⊤u)⊤(uj−R(Fx)⊤u)ecuC1(Fx)u⊤
iu+cuC1(Fx)u⊤
juαiαj
(Pn
i=1ecuC1(Fx)u⊤
iuαi)2#
−E{ui},{vi}"Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecuC1(Fx)u⊤
iu+cuC1(Fx)u⊤
juαiαj
(Pn
i=1ecuC1(Fx)u⊤
iuαi)2#
=E{ui},{vi}"Pn
i=1Pn
j=1(u⊤
iu−u⊤
iR(Fx)⊤u+u⊤
ju−u⊤
jR(Fx)⊤u)ecuC1(Fx)u⊤
iu+cuC1(Fx)u⊤
juαiαj
(Pn
i=1ecuC1(Fx)u⊤
iuαi)2#
= 2E{ui},{vi}"Pn
i=1(u⊤
iu−u⊤
iR(Fx)⊤u)ecuC1(Fx)u⊤
iuαiPn
j=1ecuC1(Fx)u⊤
juαj
(Pn
i=1ecuC1(Fx)u⊤
iuαi)2#
= 2E{ui},{vi}"Pn
i=1(u⊤
iu−u⊤
iR(Fx)⊤u)ecuC1(Fx)u⊤
iuαiPn
i=1ecuC1(Fx)u⊤
iuαi#
= 2(u⊤−u⊤R(Fx))E{ui},{vi}"Pn
i=1uiecuC1(Fx)u⊤
iuαiPn
i=1ecuC1(Fx)u⊤
iuαi#
(32)
Define ˆu:=E{ui},{vi}Pn
i=1uiecuC1(Fx)u⊤
iuαi
Pn
i=1ecuC1(Fx)u⊤
iuαi
and WLOG write ui=pui+qui, where pui:=
uu⊤uiandqui:= (Ik−uu⊤)ui. Note that for any ui=pui+qui,u′
i:=pui−quioccurs with
equal probability, and flipping quidoes not change any exponent or αiin (32). Thus
ˆu=E{(pui,qui)}i∈[n],{vi}"Pn
i=1(pui+qui)ecuC1(Fx)u⊤
iuαiPn
i=1ecuC1(Fx)u⊤
iuαi#
=1
2E{(pui,qui)}i,{vi}"Pn
i=1(2pui+qui−qui)ecuC1(Fx)u⊤
iuαiPn
i=1ecuC1(Fx)u⊤
iuαi#
=E{pui}i,{vi}"Pn
i=1puiecuC1(Fx)u⊤
iuαiPn
i=1ecuC1(Fx)u⊤
iuαi#
(33)
= ˜cu
where ˜c:=E{ui},{vi}Pn
i=1u⊤
iuecuC1(Fx)u⊤
iuαi
Pn
i=1ecuC1(Fx)u⊤
iuαi
. Note that for any ui,−uioccurs with equal
probability, so
˜c=nX
i=1E{ui},{vi}"
u⊤uiecuC1(Fx)u⊤
iuαiPn
j=1ecuC1(Fx)u⊤
juαj#
=1
2nX
i=1E{ui},{vi}"
u⊤
iuecuC1(Fx)u⊤
iuαi
ecuC1(Fx)u⊤
iuαi+Pn
j=1,j̸=iecuC1(Fx)u⊤
juαj
40−u⊤
iue−cuC1(Fx)u⊤
iuαi
e−cuC1(Fx)u⊤
iuαi+Pn
j=1,j̸=iecuC1(Fx)u⊤
juαj#
=1
2nX
i=1E{ui},{vi}"
u⊤
iu 
ecuC1(Fx)u⊤
iuαi
ecuC1(Fx)u⊤
iuαi+Pn
j=1,j̸=iecuC1(Fx)u⊤
juαj
−e−cuC1(Fx)u⊤
iuαi
e−cuC1(Fx)u⊤
iuαi+Pn
j=1,j̸=iecuC1(Fx)u⊤
juαj!#
.(34)
Since αi>0and¯cu,v>0by definition, ecuC1(Fx)u⊤
iuαiis monotonically increasing in u⊤
iu. Also,
f(x):=x
x+cis monotonically increasing for x >0for all c >0. Thus we have that
u⊤
iu>0
⇐⇒ 
ecuC1(Fx)u⊤
iuαi
ecuC1(Fx)u⊤
iuαi+Pn
j=1,j̸=iecuC1(Fx)u⊤
juαj−e−cuC1(Fx)u⊤
iuαi
e−cuC1(Fx)u⊤
iuαi+Pn
j=1,j̸=iecuC1(Fx)u⊤
juαj!
>0,
(35)
and thereby ˜c >0. Therefore, arg max u′∈Sk−1(u′)⊤ˆu=u, in particular u⊤ˆu>u⊤R(Fx)⊤ˆu
whenever R(Fx)u̸=u, so (32) is strictly positive if R(Fx)u̸=u. Thus, for any u,vsuch
thatR(Fx)u̸=u,g(F,u,v)> g(C1(Fx)
cuB⊤,u,v). Also, for any u,vsuch that R(Fx)u=u,
g(F,u,v) =g(C1(Fx)
cuB⊤,u,v).
Next we need to account for ˜Lnoise(M,x). Again writing Fx=R(Fx)u∥Fx∥2andC1(Fx) =
∥Fx∥2and using the rotational invariance of ui, we obtain
Lnoise(M) =σ2Ex,{xi}i∈[n]"Pn
i=1e2cuu⊤
iFxα2
i
(Pn
i=1ecuu⊤
iFxαi)2#
=σ2Eu,v,{ui},{vi}"Pn
i=1e2cuC1(Fx)u⊤
iR(Fx)uα2
i
(Pn
i=1ecuC1(Fx)u⊤
iR(Fx)uαi)2#
=σ2Eu,v,{ui},{vi}"Pn
i=1e2cuC1(Fx)u⊤
iuα2
i
(Pn
i=1ecuC1(Fx)u⊤
iuαi)2#
(36)
where (36) follows using the rotational invariance of ui. So, returning to (29), we have
L(M) =Ex[˜Lsignal(BF+B⊥G,x) +˜Lnoise(BF+B⊥G,x)]
≥Eu,v"
c2
ac2
uE{ui},{vi}"Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecuC1(Fx)u⊤
iu+cuC1(Fx)u⊤
juαiαj
(Pn
i=1ecuC1(Fx)u⊤
iuαi)2#
+σ2E{ui},{vi}"Pn
i=1e2cuC1(Fx)u⊤
iuα2
i
(Pn
i=1ecuC1(Fx)u⊤
iuαi)2##
(37)
where (37) is strict if R(Fx)u̸=ufor some u,v, which is equivalent to saying that F/∈ {c′B⊤, c′>
0}.
Next, recall that we have defined αi:=ecvv⊤
iGx. Using a similar argument as earlier, by the rotational
invariance of the vi’s, for any fixed x, we can write αi=ecvC2(Gx)v⊤
ie1where C2(Gx):=∥Gx∥2
ande1is the first standard basis vector.
Next, for c1, c2≥0and some fixed u,v, define
H(u,v, c1, c2):=c2
ac2
uE{ui},{vi}"Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecuc1u⊤
iu+cuc1u⊤
juecvc2v⊤
ie1+cvc2v⊤
je1
(Pn
i=1ecuc1u⊤
iuecvc2v⊤
ie1)2#
41+σ2E{ui},{vi}"Pn
i=1e2cuc1u⊤
iue2cvc2v⊤
ie1
(Pn
i=1ecuc1u⊤
iuecvc2v⊤
ie1)2#
(38)
and let
(C∗
1(u,v), C∗
2(u,v))∈arg min
(c1,c2):0≤c1≤ˆc
c2u,c2≥0H(u,v, c1, c2) (39)
Since Hdoes not vary with v, we have (C∗
1(u,v), C∗
2(u,v)) = ( C∗
1(u), C∗
2(u))WLOG. In
fact,Hdoes not vary with ueither, due to the rotational invariance of the ui’s. So, we have
(C∗
1(u,v), C∗
2(u,v)) = ( C∗
1, C∗
2)WLOG, i.e. there is a single pair (C∗
1, C∗
2)that minimizes
H(u,v, c1, c2)overc1, c2for all u∈Sk−1andv∈Sd−k−1.
ThusF∗=C∗
1B⊤andG∗satisfies ∥G∗x∥=C∗
2for all x, which implies, using also that Mis
symmetric, that G∗=C∗
2B⊤
⊥.
Lemma H.3. Consider any α:= [α1, . . . , α n]such that α1= max iαiandα1>miniαi>0.
Further, let c∈(0,2]. Define
Hsignal(u,α) :=E{ui}i∈[n]"Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecu⊤
iu+cu⊤
juαiαj
(Pn
i=1ecu⊤
iuαi)2#
. (40)
Then
∂H signal(u,α)
∂α1>0.
Proof. We first compute∂H signal(u,α)
∂α1. Using the linearity of the expectation and the quotient rule we
obtain:
∂H signal(u,α)
∂α1
=E{ui}i∈[n]"
∂
∂α1Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecu⊤
iu+cu⊤
juαiαj
(Pn
i=1ecu⊤
iuαi)2#
= 2E{ui}i
(Pn
i=1ecu⊤
iuαi)2Pn
j=2(u1−u)⊤(uj−u)ecu⊤
1u+cu⊤
juαj+∥u1−u∥2
2e2cu⊤
1uα1
(Pn
i=1ecu⊤
iuαi)4

−2E{ui}i"
(Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecu⊤
iu+cu⊤
juαiαj)(Pn
i=1ecu⊤
iuαi)ecu⊤
1u
(Pn
i=1ecu⊤
iuαi)4#
= 2E{ui}i
(Pn
i=1ecu⊤
iuαi)Pn
j=1(u1−u)⊤(uj−u)ecu⊤
1u+cu⊤
juαj
(Pn
i′=1ecu⊤
i′uαi′)3

−2E{ui}i"
(Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecu⊤
iu+cu⊤
juαiαj)ecu⊤
1u
(Pn
i′=1ecu⊤
i′uαi′)3#
= 2nX
i=2nX
j=1Si,j (41)
where
Si,j:=αiαjE{ui′}i′∈[n]"
(u1−ui)⊤(uj−u)ec(u⊤
1u+u⊤
iu+u⊤
ju)
(Pn
i′=1ecu⊤
i′uαi′)3#
.
Note that terms with i= 1do not appear in (41). We analyze Si,1+Si,iand each Si,j,j /∈ {1, i}
separately, and will ultimately show that each of these terms is positive. We start with the latter case
as it is easier to handle. For j /∈ {1, i}, we have
Si,j=αiαjE{ui′}i′∈[n]"
(u1−ui)⊤(uj−u)ec(u⊤
1u+u⊤
iu+u⊤
ju)
(Pn
i′=1ecu⊤
i′uαi′)3#
42=αiαjE{ui′}i′∈[n]"
(u1−ui)⊤uu⊤(uj−u)ec(u⊤
1u+u⊤
iu+u⊤
ju)
(Pn
i′=1ecu⊤
i′uαi′)3#
+αiαjE{ui′}i′∈[n]"
u⊤
1(Ik−uu⊤)(uj−u)ec(u⊤
1u+u⊤
iu+u⊤
ju)
(Pn
i′=1ecu⊤
i′uαi′)3#
| {z }
=0
−αiαjE{ui′}i′∈[n]"
u⊤
i(Ik−uu⊤)(uj−u)ec(u⊤
1u+u⊤
iu+u⊤
ju)
(Pn
i′=1ecu⊤
i′uαi′)3#
| {z }
=0(42)
=αiαjE{ui′}i′∈[n]"
(u⊤
1u−u⊤
iu)(u⊤
ju−1)ec(u⊤
1u+u⊤
iu+u⊤
ju)
(Pn
i′=1ecu⊤
i′uαi′)3#
where the latter two terms in (42) are zero by the same argument as in (33): flipping the component
of either u1oruiperpendicular to udoes not change any of the values in any exponent, and each flip
occurs with equal probability. Next, note that if αi=α1,
E{ui′}i′∈[n]"
u⊤
1u(u⊤
ju−1)ec(u⊤
1u+u⊤
iu+u⊤
ju)
(Pn
i′=1ecu⊤
i′uαi′)3#
=E{ui′}i′∈[n]"
u⊤
iu(u⊤
ju−1)ec(u⊤
1u+u⊤
iu+u⊤
ju)
(Pn
i′=1ecu⊤
i′uαi′)3#
thusSi,j= 0. Otherwise, αi< α 1by definition of α1, and there must be some such αi, since if
not, there would be some c′∈R+such that α=c′α∗. For the case αi< α 1, we use a symmetry
argument to show that Si,j>0.
First we define additional notations. Let ¯U1,i:={ui′}i′∈[n]\{1,i}, and for any (a, b)∈[−1,1]2,
define
fa,b(¯U1,i):=(a−b)(u⊤
ju−1)ec(a+b+u⊤
ju)
(ecaα1+ecbαi+P
i′̸=1,iecu⊤
i′uαi′)3.
In particular, for any a∈[−1,1], define pa:=Pu1[u⊤
1u=a]. Since u1anduiare i.i.d., we have
Pu1,ui[u⊤
1u=a,u⊤
iu=b] =Pu1,ui[u⊤
1u=b,u⊤
iu=a] =papbfor any (a, b)∈[−1,1]2Thus,
by the law of total expectation we have
Si,j=αiαjE¯U1,iZ1
−1Z1
−1fa,b(¯U1,i)papbda db
=αiαj
2E¯U1,iZ1
−1Z1
−1(fa,b(¯U1,i) +fb,a(¯U1,i))papbda db
(43)
Next we show that for any instance of a, band¯U1,i,fa,b(¯U1,i) +fb,a(¯U1,i)is positive. We have:
fa,b(¯U1,i) +fb,a(¯U1,i)
= (a−b)(u⊤
ju−1)ec(a+b+u⊤
ju)
× 
1
(ecaα1+ecbαi+P
i′̸=1,iecu⊤
i′uαi′)3−1
(ecbα1+ecaαi+P
i′̸=1,iecu⊤
i′uαi′)3!
≥0
with equality only if a=boruj=u, since u⊤
ju≤1with equality only if uj=u, and
a > b ⇐⇒ (ecaα1+ecbαi+X
i′̸=1,iecu⊤
i′uαi′)3>(ecbα1+ecaαi+X
i′̸=1,iecu⊤
i′uαi′)3(44)
due to α1> αiandαi′>0for all i′. So we have Si,j>0.
Next we analyze Si,1+Si,i. In these cases we cannot immediately drop the components of u1and
uithat are perpendicular to u. We have:
Si,1+Si,i=αiα1E{ui′}i′∈[n]"
(u1−ui)⊤(u1−u)ec(2u⊤
1u+u⊤
iu)
(Pn
i′=1ecu⊤
i′uαi′)3#
43+α2
iE{ui′}i′∈[n]"
(u1−ui)⊤(ui−u)ec(u⊤
1u+2u⊤
iu)
(Pn
i′=1ecu⊤
i′uαi′)3#
=αiα1E{ui′}i′∈[n]"
(1−u⊤
iu1−u⊤
1u+u⊤
iu)ec(2u⊤
1u+u⊤
iu)
(Pn
i′=1ecu⊤
i′uαi′)3#
+α2
iE{ui′}i′∈[n]"
(u⊤
iu1−1−u⊤
1u+u⊤
iu)ec(u⊤
1u+2u⊤
iu)
(Pn
i′=1ecu⊤
i′uαi′)3#
=αiE{ui′}i′∈[n]"
(u⊤
iu−u⊤
1u)ec(u⊤
1u+u⊤
iu)(ecu⊤
1uα1+ecu⊤
iuαi)
(Pn
i′=1ecu⊤
i′uαi′)3#
+αiE{ui′}i′∈[n]"
(1−u⊤
iu1)ec(u⊤
1u+u⊤
iu)(ecu⊤
1uα1−ecu⊤
iuαi)
(Pn
i′=1ecu⊤
i′uαi′)3#
Now we can split u⊤
iu1into the product of the components of ui,u1in the direction uand the
product of their components in the perpendicular subspace as before. Doing so yields
Si,1+Si,i=αiE{ui′}i′∈[n]"
(u⊤
iu−u⊤
1u)ec(u⊤
1u+u⊤
iu)(ecu⊤
1uα1+ecu⊤
iuαi)
(Pn
i′=1ecu⊤
i′uαi′)3#
+αiE{ui′}i′∈[n]"
(1−u⊤
iuu⊤u1)ec(u⊤
1u+u⊤
iu)(ecu⊤
1uα1−ecu⊤
iuαi)
(Pn
i′=1ecu⊤
i′uαi′)3#
−αiE{ui′}i′∈[n]"
u⊤
i(Ik−uu⊤)u1ec(u⊤
1u+u⊤
iu)(ecu⊤
1uα1−ecu⊤
iuαi)
(Pn
i′=1ecu⊤
i′uαi′)3#
=αiE{ui′}i′∈[n]"
(u⊤
iu−u⊤
1u)ec(u⊤
1u+u⊤
iu)(ecu⊤
1uα1+ecu⊤
iuαi)
(Pn
i′=1ecu⊤
i′uαi′)3#
+αiE{ui′}i′∈[n]"
(1−u⊤
iuu⊤u1)ec(u⊤
1u+u⊤
iu)(ecu⊤
1uα1−ecu⊤
iuαi)
(Pn
i′=1ecu⊤
i′uαi′)3#
Next, define
ga,b(¯U1,i):=E{ui′}i′∈[n]"
(u⊤
iu−u⊤
1u)ec(u⊤
1u+u⊤
iu)(ecu⊤
1uα1+ecu⊤
iuαi)
(Pn
i′=1ecu⊤
i′uαi′)3#
+E{ui′}i′∈[n]"
(1−u⊤
iuu⊤u1)ec(u⊤
1u+u⊤
iu)(ecu⊤
1uα1−ecu⊤
iuαi)
(Pn
i′=1ecu⊤
i′uαi′)3#
(45)
We argue similarly as in the previous case, except that here we must include additional terms.
Si,1+Si,i
=αi
2E¯U1,iZ1
−1Z1
−1(ga,b(¯U1,i) +gb,a(¯U1,i))papbda db
=αi
2E¯U1,iZ1
−1Z1
−1Ga,b(¯U1,i)papbda db
(46)
where
Ga,b(¯U1,i):=ga,b(¯U1,i) +gb,a(¯U1,i) (47)
We show that for any (a, b)∈[−1,1]2and any ¯U1,i,Ga,b(¯U1,i)is positive, which implies that
Si,1+Si,iis positive by (46).
First, note that if b=afor any a∈[−1,1]and¯U1,i, we have
ga,a(¯U1,i) =E{ui′}i′∈[n]"
(1−a2)e3ca(α1−αi)
((α1+αi)eca+P
i′∈[n]\{1,i}ecu⊤
i′uαi′)3#
≥0 (48)
44since each term inside the expectation is nonnegative, as a2≤1andα1> αi. Note that this implies
Ga,b≥0when a=b, so WLOG we consider b̸=afor the remainder of the proof. Now we focus
on showing (61). Throughout, we will make use of the notation
da,b:=ecaα1+ecbαi+X
i′∈[n]\{1,i}ecu⊤
i′uαi′ (49)
which represents the cube root of the denominator in all terms when u⊤
1u=aandu⊤
iu=b, and
γa,b:= 1−ab+a−b.
Using this notation, we can rewrite
ga,b(¯U1,i) =ec(a+b)ecaγb,aα1−ecbγa,bαi
d3
a,b(50)
Therefore,
ga,b(¯U1,i) +gb,a(¯U1,i)
=ec(a+b)ecaγb,aα1−ecbγa,bαi
d3
a,b+ec(a+b)ecbγa,bα1−ecaγb,aαi
d3
b,a
=ec(a+b)d−3
a,bd−3
b,a
α1 
ecaγb,ad3
b,a+ecbγa,bd3
a,b
−αi 
ecaγb,ad3
a,b+ecbγa,bd3
b,a
Note that ec(a+b)d−3
a,bd−3
b,a>0, so it remains to show that the term inside the parentheses is positive.
This term can be rearranged as:
α1 
ecaγb,ad3
b,a+ecbγa,bd3
a,b
−αi 
ecaγb,ad3
a,b+ecbγa,bd3
b,a
= (α1−αi) 
ecaγb,ad3
b,a+ecbγa,bd3
a,b
+αi 
ecaγb,ad3
b,a+ecbγa,bd3
a,b−ecaγb,ad3
a,b−ecbγa,bd3
b,a
= (α1−αi) 
ecaγb,ad3
b,a+ecbγa,bd3
a,b
| {z }
=:T1+αi 
d3
b,a−d3
a,b 
ecaγb,a−ecbγa,b
| {z }
=:T2(51)
First we show that T1is positive by analyzing γa,bandγb,a. For any (a, b)∈[−1,1]2such that
a̸=b,
∂
∂b(γa,b) =∂
∂b(1−ab+a−b) =−1−a≤0 (52)
with equality holding if and only if a=−1. Ifa=−1, we have γa,b= 1 + b−1−b= 0for all
b∈[−1,1]. Otherwise, (52) shows that γa,bis strictly decreasing with b, so it is minimized over
b∈[−1,1]atb= 1. When b= 1, we have γa,b= 1−a+a−1 = 0 for all a. So, γa,b≥0with
equality holding if and only if a=−1orb= 1. Note that by symmetry, this implies γb,a≥0with
equality holding if and only if a= 1orb=−1. So, we can have both γa,b= 0andγb,a= 0if and
only if a=b=−1ora=b= 1. However, we have a̸=b, so at least one of γa,bandγb,aare
strictly positive, and T1is strictly positive (using also that α1> αi).
We next show that T2is positive. Observe that
d3
b,a−d3
a,b>0⇐⇒ b > a (53)
since α1> αi,so it remains to show
b > a ⇐⇒ ecaγb,a−ecbγa,b>0. (54)
where
ecaγb,a−ecbγa,b=eca(1−ab−a+b)−ecb(1−ab+a−b). (55)
We first show the forward direction, namely b > a =⇒ecaγb,a−ecbγa,b>0.
45Note that if b=a,ecaγb,a−ecbγa,b= 0. So, if we can show that for any fixed a,ecaγb,a−ecbγa,b
is increasing with bas long as b≥a, then we will have ecaγb,a−ecbγa,b>0forb > a . To show
ecaγb,a−ecbγa,bis increasing, we take its partial derivative with respect to b:
∂
∂b 
ecaγb,a−ecbγa,b
=eca(1−a) +ecb(1 +a+cb−ca−c+cab) (56)
We would like to show that the RHS of (56) is nonnegative. To do so, we show that its partial
derivative with respect to ais positive, so it achieves minimum value at a=−1, at which point the
value is positive. We have:
∂
∂a∂
∂b 
ecaγb,a−ecbγa,b
=eca(c−ca−1) +ecb(1−c+cb)
=q(b)−q(a) (57)
where q(x):=ecx(1 +cx−c). Note that q(x)is monotonically increasing in x∈[−1,1]; to see
this, observe that
∂
∂xq(x) =ecx(1 +cx−c)c+ecxc=ecx(2 +cx−c)c≥0 (58)
where the inequality follows since c∈(0,2]andx∈[−1,1]. Therefore, since b > a , we have
q(b)−q(a)≥0and∂
∂a ∂
∂b 
ecaγb,a−ecbγa,b
≥0from (57). As a result,∂
∂b 
ecaγb,a−ecbγa,b
achieves minimum value at a=−1. At this point, using (56) we have
∂
∂b 
ecaγb,a−ecbγa,b
= 2e−c+ecb(cb+c−c−cb)
= 2e−c
>0
This implies that the minimum value of ecaγb,a−ecbγa,boverb∈[a,1]is achieved at b=a, and we
know this value is zero, so we have that ecaγb,a−ecbγa,b>0when b−a.
To show the backward direction of (54), namely ecaγb,a−ecbγa,b>0 =⇒b > a , note that the
converse, namely a > b =⇒ecaγb,a−ecbγa,b<0, follows by the same argument as above with a
andbswapped. Therefore, we have T2>0as desired.
Lemma H.4. Consider any α:= [α1, α2]such that α1> α2>0. Further, let c∈(0,1]. Define
Hnoise(u,α) :=Eu1,u2"
e2cu⊤
1uα2
1+e2cu⊤
2uα2
2
(ecu⊤
1uα1+ecu⊤
2uα2)2#
.
Then
∂H noise(u,α)
∂α1>0
Proof. We have
Hnoise(u,α) :=Eu1,u2"
e2cu⊤
1uα2
1+e2cu⊤
2uα2
2
(ecu⊤
1uα1+ecu⊤
2uα2)2#
Since n= 2, we have
∂H noise(u,α)
∂α1=Eu1,u2"
∂
∂α1e2cu⊤
1uα2
1+e2cu⊤
2uα2
2
(ecu⊤
1uα1+ecu⊤
2uα2)2#
=Eu1,u2"
2e2cu⊤
1uα1(ecu⊤
1uα1+ecu⊤
2uα2)2
(ecu⊤
1uα1+ecu⊤
2uα2)4#
−Eu1,u2"
2(ecu⊤
1uα1+ecu⊤
2uα2)ecu⊤
1u(e2cu⊤
1uα2
1+e2cu⊤
1uα2
2)
(ecu⊤
1uα1+ecu⊤
2uα2)4#
46= 2α2Eu1,u2"
ec(u⊤
1u+u⊤
2u)(ecu⊤
1uα1−ecu⊤
2uα2)
(ecu⊤
1uα1+ecu⊤
2uα2)3#
Define N:=Eu1,u2
ec(u⊤
1u+u⊤
2u)(ecu⊤
1uα1−ecu⊤
2uα2)
(ecu⊤
1uα1+ecu⊤
2uα2)3
, and
da,b:=ecaα1+ecbα2
ha,b:=ec(a+b)ecaα1−ecbαi
d3
a,b,
Now, we have
N=Z1
−1Z1
−1ha,bpapbda db
=1
2Z1
−1Z1
−1(ha,b+hb,a)papbda db
=1
2Z1
−1Z1
−1(ha,b+hb,a)papbχ{a̸=b}da db +1
2Z1
−1Z1
−1(ha,b+hb,a)papbχ{a=b}da db
=1
2Z1
−1Z1
−1(ha,b+hb,a)papbχ{a̸=b}da db +Z1
−1ha,ap2
ada
=1
2Z1
−1Z1
−1(ha,b+hb,a)papbχ{a̸=b}da db +1
2Z1
−1ha,ap2
ada+1
2Z1
−1hb,bp2
bdb
=1
2Z1
−1Z1
−1(ha,b+hb,a)papbχ{a̸=b}da db +1
4Z1
−1Z1
−1ha,ap2
ada db
+1
4Z1
−1Z1
−1hb,bp2
bda db
=1
2Z1
−1Z1
−1(ha,b+hb,a)papbχ{a̸=b}da db +1
4Z1
−1Z1
−1(ha,ap2
a+hb,bp2
b)da db
=1
2Z1
−1Z1
−1Ha,bda db (59)
where
Ha,b:=papb(ha,b+hb,a) +p2
a
2ha,a+p2
b
2hb,b (60)
We will show that for any (a, b)∈[−1,1]2and(pa, pb)∈[0,1]2,Ha,bis positive, which implies
thatNiis positive by (59). To do this, assuming ha,ais nonnegative for any a, it is sufficient to show
˜Ha,b:=ha,b+hb,a+p
ha,ahb,b>0, (61)
since this implies ha,b+hb,a>−p
ha,ahb,band thus, from (60),
Ha,b>−papbp
ha,ahb,b+p2
a
2ha,a+p2
b
2hb,b
= 
par
ha,a
2−pbr
hb,b
2!2
≥0 (62)
Before showing (61), we need to confirm that ha,ais not negative for all a∈[−1,1]. We have
ha,a=e3ca(α1−α2)
d3a,a≥0 (63)
47since each term inside the expectation is nonnegative, as α1> α2. Note that this implies Ha,b≥0
when a=b, so WLOG we consider a > b for the remainder of the proof.
Note that
ha,ahb,b=e3c(a+b)(α1−αi)2
e3c(a+b)(α1+α2)6=(α1−α2)2
(α1+α2)6(64)
Using this, we have
˜Ha,b=ha,b+hb,a+p
ha,ahb,b
=e2ca+cbα1−e2cb+caα2
d3
a,b+e2cb+caα1−e2ca+cbα2
d3
b,a+α1−α2
(α1+α2)3
=d−3
a,bd−3
b,aec(a+b)(α1+α2)3
×
(ecaα1−ecbα2)d3
b,a(α1+α2)3+ (ecbα1−ecaα2)d3
a,b(α1+α2)3
| {z }
=:P
+e−c(a+b)d3
a,bd3
b,a(α1−α2)
| {z }
=:P
(65)
To show that ˜Ha,bis positive, we need to show that Pis positive. Without loss of generality we can
consider α1= 1andα2∈(0,1)by dividing the numerator and denominator of Hnoisebyα2
1. Thus,
for the remainder of the proof we treat α1as 1 and write α:=α2for ease of notation. Using this
notation we can expand Pas follows:
P= (eca−ecbα)d3
b,a(1 +α)3+ (ecb−ecaα)d3
a,b(1 +α)3+e−c(a+b)d3
a,bd3
b,a(1−α)
= (eca−ecbα)(ecb+ecaα)3(1 +α)3+ (ecb−ecaα)(eca+ecbα)3(1 +α)3
+e−c(a+b)(eca+ecbα)3(ecb+ecaα)3(1−α)
= (e5ca−cb+e5cb−ca) 
α3(1−α)
+ (e4ca+e4cb) 
−α−5α3+ 5α4+α6
+ (e3ca+cb+e3cb+ca) 
1 + 6 α+ 10α3−10α4−6α6−α7
+e2ca+2cb 
1 + 5 α+ 27α2+ 3α3−3α4−27α5−5α6−α7
= (1−α)× 
(e5ca−cb+e5cb−ca)α3
+ (e4ca+e4cb) 
−α−α2−6α3−α4−α5
+ (e3ca+cb+e3cb+ca) 
1 + 7 α+ 7α2+ 17α3+ 7α4+ 7α5+α6
+e2ca+2cb 
1 + 6 α+ 33α2+ 36α3+ 33α4+ 6α5+α6!
Recall that 1−α >0, so we need to show that the sum of the remaining terms is positive. These
terms can be written as a polynomial in y:=ec(a−b)as follows:
P(1−α)−1eca−5cb=y6α3
+y5 
−α−α2−6α3−α4−α5
+y4 
1 + 7 α+ 7α2+ 17α3+ 7α4+ 7α5+α6
+y3 
1 + 6 α+ 33α2+ 36α3+ 33α4+ 6α5+α6
+y2 
1 + 7 α+ 7α2+ 17α3+ 7α4+ 7α5+α6
+y 
−α−α2−6α3−α4−α5
+α3(66)
48We know that y6> y5>···>1since a > b . We also have that α <1. Using these facts we next
show that the sum of the third and smaller-order terms in the RHS of (66) is positive.
(∗) :=y3 
1 + 6 α+ 33α2+ 36α3+ 33α4+ 6α5+α6
+y2 
1 + 7 α+ 7α2+ 17α3+ 7α4+ 7α5+α6
+y 
−α−α2−6α3−α4−α5
+α3
> y 
1 + 6 α+ 33α2+ 36α3+ 33α4+ 6α5+α6
+y 
1 + 7 α+ 7α2+ 17α3+ 7α4+ 7α5+α6
+y 
−α−α2−6α3−α4−α5
+α3
> y 
2 + 12 α+ 39α2+ 47α3+ 39α4+ 12α5+ 1α6
>0
Next we show that the sum of the sixth-, fifth-, and fourth-order terms is positive. Let a6:=α3,
a5:=α+α2+ 6α3+α4+α5, anda4:= 1 + 7 α+ 7α2+ 17α3+ 7α4+ 7α5+α6,so the sum of
the sixth-, fifth-, and fourth-order terms is y6a6−y5a5+y4a4. Note that 32a6< a4since α <1,
and
a5−4a6=α+α2+ 2α3+α4+α5
=1
7.5 
7.5α+ 7.5α2+ 15α3+ 7.5α4+ 7.5α5
<1
7.5 
1 + 7 α+ 7α2+ 17α3+ 7α4+ 7α5+α6
=a4
7.5(67)
thusa5<a4
7.5+ 4a6. Also, y=ec(a−b)≤e2<7.5since c≤1. Therefore,
y6a6−y5a5+y4a4=y4 
y2a6−ya5+a4
> y4
y2a6−4ya6−ya4
7.5+a4
> y4
y2a6−4ya6+a4
1−y
7.5
|{z}
>0sincey<7.5

> y4
y2a6−4ya6+ 32a6
1−y
7.5
=y4a6
y2−62
7.5y+ 32
> y4a6 
−1
462
7.52
+ 32!
(68)
>0 (69)
where (68) follows by minimizing the terms inside the parentheses over y. Thus, we have ˜Ha,b>0,
which completes the proof.
Now we can finally prove Theorem 4.4. We prove a slightly stronger result, formally stated as follows.
Theorem H.5. Consider any B∈Od×kand the corresponding function class Flin
Bas defined in (4.2) .
Suppose tasks are drawn from D(Flin
B)and Assumption 4.3 holds. Recall the pretraining population
loss:
L(M) =Ef,{xi}i∈[n+1],{ϵi}i∈[n]
 Pn
i=1(f(xi)−f(xn+1) +ϵi)ex⊤
iMxn+1
Pn
i=1ex⊤
iMxn+1!2
.(70)
49Consider two cases:
•Case 1: σ= 0,n >1.Then define Cp:= 2.
•Case 2: σ >0,n= 2.Then define Cp:= 1.
Then in each case, among all M∈ M :={M∈Rd×d:M=M,∥B⊤MB∥2≤Cp
c2u}, any
minimizer M∗of(70) satisfies M∗=cBB⊤for some c∈(0,Cp
c2u].
Proof. From Lemma H.2, we have M∗=cpBB⊤+ ˜cmathbfB ⊥B⊤
⊥for some ˜c∈Rand some
cp∈(0,Cp
c2u], where Cp= 2in Case 1 and Cp= 1in Case 2. Suppose that ˜c̸= 0. Then it remains to
show that L(cpBB⊤+ ˜cB⊥B⊤
⊥)>L(cpBB⊤).
We start by establishing the same notations as in the proof of Lemma H.2. For each i∈[n+ 1],
xi=cuBui+cvB⊥vi. Thus, for each i∈[n], we have
ex⊤
iMxn+1=ecpx⊤
iBB⊤xn+1ec′x⊤
iB⊥B⊤
⊥xn+1
=ecpc2
uu⊤
iun+1ec2
v˜cv⊤
ivn+1
=ecpc2
uu⊤
iun+1αi (71)
where, for each i∈[n],αi:=ec2
v˜cv⊤
ivn+1. For ease of notation, denote x=xn+1,u:=un+1and
c=cpc2
u. Also, note that for any xi,f(xi) =a⊤B⊤xi=cua⊤ui, and that drawing f∼D(Flin
B)
is equivalent to drawing a∼Dafor some distribution DaoverRksuch that Ea∼Da[aaT] =c2
aIk.
Using this, we have:
L(cpBB⊤+ ˜cB⊥B⊤
⊥)
=Ea,u,{ui}i∈[n],{αi}i∈[n],{ϵi}i∈[n]
Pn
i=1(cua⊤ui−cua⊤u+ϵi)ecu⊤
iuαi2
(Pn
i=1ecu⊤
iuαi)2

=Eu,{ui}i∈[n],{αi}i∈[n]"Pn
i=1Pn
j=1Ea,{ϵi}i∈[n][(cua⊤ui−cua⊤u+ϵi)(cua⊤uj−cua⊤u+ϵj)]ecu⊤
iu+cu⊤
juαiαj
(Pn
i=1ecu⊤
iu)2#
=Eu,{ui}i∈[n],{αi}i∈[n]"
c2
ac2
uPn
i=1Pn
j=1(ui−u)⊤(uj−u)ecu⊤
iu+cu⊤
juαiαj
(Pn
i=1ecu⊤
iu)2+σ2Pn
i=1e2cu⊤
iuαiαj
(Pn
i=1ecu⊤
iu)2#
=Eu,α[H(u,α)]
where α:= [α1, . . . , α n]and
H(u,α)
:=E{ui}i∈[n]"
c2
ac2
uPn
i=1Pn
j=1(ui−u)⊤(uj−u)ecu⊤
iu+cu⊤
juαiαj
(Pn
i=1ecu⊤
iuαi)2+σ2Pn
i=1e2cu⊤
iuα2
i
(Pn
i=1ecu⊤
iuαi)2#
.
(72)
Define α∗= [1, . . . , 1]∈Rn. We proceed by showing that for any u∈Sd−1, allα∈Rn
+satisfy
(i) if α=c′α∗for some c′∈R+,thenH(u,α) =H(u,α∗)
(ii) if α̸=c′α∗for any c′∈R+,thenH(u,α)> H(u,α∗)
This implies L(cpBB⊤+ ˜cB⊥B⊤
⊥)>L(cpBB⊤), since
Pα({α=c′α∗for some c′∈R+}) = 1 ⇐⇒ ˜c= 0,
50which implies that ˜c= 0 is the unique argument that achieves the minimal value of L(cpBB⊤+
˜cB⊥B⊤
⊥)over˜c∈R(and this value is Eu[H(u,α∗)]).
Proving (i)is trivial as it can be easily checked that H(u,α) =H(u, c′α)for all u∈Sd−1,α∈Rn
+,
andc′∈R+.
Proving (ii)is more involved. Consider any α̸=c′α∗for any c′∈R+. WLOG let 1∈arg max iαi.
We show that the partial derivative of H(u,α)with respect to α1is strictly positive, which means
thatH(u,α)can be reduced by reducing α1by some ϵ >0. We can repeat this argument, repeatedly
reducing max iαiat each step and thereby reducing the loss, until we reach an α′satisfying α′=
c′α∗. Since the loss is reduced at each step, we have that H(u,α)> H(u,α∗).
To show that the partial derivative of H(u,α)with respect to α1is strictly positive, we decompose
∂H(u,α)
∂α1=∂H signal(u,α)
∂α1+∂H noise(u,α)
∂α1, where
Hsignal(u,α) :=c2
ac2
uE{ui}i∈[n]"Pn
i=1Pn
j=1(ui−u)⊤(uj−u)ecu⊤
iu+cu⊤
juαiαj
(Pn
i=1ecu⊤
iuαi)2#
Hnoise(u,α) :=σ2E{ui}i∈[n]"Pn
i=1e2cu⊤
iuα2
i
(Pn
i=1ecu⊤
iuαi)2#
By Lemma H.3, we have∂H signal(u,α)
∂α1>0. Ifσ= 0 we are done, otherwise we have n= 2 and
∂H noise(u,α)
∂α1>0by Lemma H.4. This completes the proof.
I Additional Lemmas
Lemma I.1. Consider a continuous unimodal function f. Then we have
∞X
i=0f(i)−maxf≤Z∞
0f(t)dt≤∞X
i=1f(i) + max f
Proof. LetTdenote the point that achieves the maximum of f. Then we know that f(t)≥f(⌊t⌋)for
t < T , while f(t)≥f(⌈t⌉)fort > T . This meansRi
i−1f(t)dt≤f(i)≤Ri+1
if(t)dtfort≤ ⌊T⌋
andRi
i−1f(t)dt≥f(i)≥Ri+1
if(t)dtfort≥ ⌈T⌉So
∞X
i=0f(i) =⌊T⌋X
i=0f(i) +∞X
i=⌈T⌉f(i)
≤⌊T⌋X
i=0Zi+1
if(t)dt+∞X
⌈T⌉Zi
i−1f(t)dt
≤∞X
i=0Zi+1
if(t)dt+Z⌈T⌉
⌊T⌋f(t)dt
≤Z∞
0f(t)dt+ max f
Similarly we have
∞X
i=1f(i) =⌊T⌋X
i=1f(i) +∞X
i=⌈T⌉f(i)
≤⌊T⌋X
i=1Zi
i−1f(t)dt+∞X
⌈T⌉Zi+1
if(t)dt
51≤∞X
i=1Zi
i−1f(t)dt−Z⌈T⌉
⌊T⌋f(t)dt
≤Z∞
0f(t)dt−maxf
Lemma I.2. Iffandgare nonnegative measurable real functions, then
Z
f(x)g(x)dx≤Z
f∗(x)g∗(x)dx
where f∗, g∗are the symmetric decreasing rearrangements of fandg.
Proof. Please see [66] or [67].
Lemma I.3. Suppose {ai},{bi}are sorted the same way, ai> aj⇐⇒ bi> bj. Then we have
Pa2
i
(Pai)2<Pa2
ib2
i
(Paibi)2.
Proof. Cross multiplying and expanding, we have
(X
a2
i)(X
aibi)2<(X
a2
ib2
i)(X
ai)2
⇐⇒X
i,j,kaibiajbja2
k<X
i,j,ka2
ib2
iajak
⇐⇒1
3X
i,j,kaibiajbja2
k+ajbjakbka2
i+akbkaibia2
j<1
3X
i,j,ka2
ib2
iajak+a2
jb2
jakai+a2
kb2
kaiaj
⇐⇒1
3X
i,j,ka2
ib2
iajak+a2
jb2
jakai+a2
kb2
kaiaj−(aibiajbja2
k+ajbjakbka2
i+akbkaibia2
j)>0
⇐⇒1
3X
i,j,kaiajak 
aib2
i+ajb2
j+akb2
k−aibjbk−ajbkbi−akbibj
>0
The last of which follows from the rearrangement inequality [67].
J Additional Experiments and Details
All experiments were run in Google Colab in a CPU runtime. We used a random seed of 0 in all
cases. All training was executed in PyTorch with the Adam optimizer. We tuned learning rates in
{10−3,10−2,10−1}separately for linear and softmax attention, and we initialized MKandMQby
setting each to 0.001Id, and tie the weights of MKandMQto speed up training.
Figure 1. The upper row depicts our functions, which increase in Lipschitzness from left to right.
The black curve depicts the ground truth, while the gray dots depict the noisy training samples. The
shaded region represents the attention window. The middle row depicts the attention weights for
softmax and linear attention. We remark that the softmax is able to adapt to the Lipschitzness while
linear is not. The bottom row depicts the ICL error as a function of the context length nfor Linear
and ReLU pretraining using Linear and Softmax attention. That is, at each iteration, a context is
drawn from a non-linear regression (defined below) consisting of a randomly phase shifted cosine
function. The ICL task is to predict the function value at a randomly chosen query on the unit circle.
Each point in the plot depicts the ICL error of a pretrained attention unit (using softmax (blue) or
linear (orange) activation) at the end of 15000 iterations with learning rate 10−3. We use d= 2and a
distribution D(Fν,hills). Here we define
Fν,hills={νcos (θ−b)}
and a distribution D(Fν,hills)is induced by drawing buniformly from [−π, π]. We use ν= 0,1.5,6
for the left, middle and right plots in the bottom row, respectively.
52Figure 9: Representation learning error (ρ(M,B))and test ICL error (mean squared error) during
pretraining softmax and linear attention on tasks from Left:Faff
B,Center: F2
B, and Right: Fcos
B.
Figures 3, 4, 5. In all cases, we use an exponentially decaying learning rate schedule with factor
0.999. In Figures 3 and 5 we use initial learning rate 0.1 and in Figure 4 we use an initial learning
rate 0.01. Moreover, in all cases besides those with varying nin Figure 4, we compute gradients
with respect to the ICL loss evaluated on N:=⌊√n⌋query samples per task (that is, each context
input to the attention unit has n+Nsamples, of which nare labeled, and the other Nlabels are
inferred). When nvaries in Figure 4, we use N= 1. In Figure 5 we show smoothed test ICL errors
with smoothing rate 0.01.
J.1 Low-Rank Experiments
Due to our results in Section 3 showing that softmax attention can learn an appropriate attention
window scale when pretrained on nonlinear tasks, we hypothesize that it can also learn the appropriate
directions during pretraining on nonlinear tasks. To test this, we consider tasks drawn from low-
rank versions of affine, quadratic and cosine function classes, in particular: Faff
B:={f:f(x) =
a⊤B⊤x+2,a∈Sk−1},F2
B:={f:f(x) = (a⊤B⊤x)2,a∈Sk−1}andFcos
B:={f:f(x) =
cos 
4a⊤B⊤x
,a∈Sk−1}. Each task distribution D(Faff
B), D(F2
B), D(Fcos
B)is induced by drawing
a∼ Uk. We train MKandMQwith Adam with learning rate tuned separately for softmax and linear
attention. We set d= 10 ,k= 2,n= 50 , andσ= 0.01. We draw {xi}n+1
i=1i.i.d. from a non-uniform
distribution on Sd−1for each task, and draw one task per training iteration. We draw Brandomly at
the start of each trial, and repeat each trial 5 times and plots means and standard deviations over the
5 trials. We capture the extent to which the learned M=M⊤
KMQrecovers col(B)via the metric
ρ(M,B) :=∥B⊤
⊥MB⊥∥2
σmin(B⊤MB), where σmin(A)is the minimum singular value of A. For test error, we
compute the average squared error on 500 random tasks drawn from the same distribution as the
(pre)training tasks. Please see Appendix J for more details.
We randomly generate Bon each trial by first sampling each element of ˆBi.i.d. from the standard
normal distribution, then take its QR decomposition to obtain B.To draw the covariates, we draw
a random matrix ˜J∈Rd×dby sampling each element i.i.d. from the standard normal distribution.
Then, we compute J= (˜J⊤˜J)1/2. Then we draw ˜xi∼ N(0d,Id)and set xi=J˜xi
∥J˜xi∥.
Results. Figure 9 shows that softmax attention recovers the low-rank structure when tasks are drawn
from each of the three function classes, which leads to test error improving with the quality of the
learned subspace. In contrast, linear attention does not learn any meaningful structure in these cases.
53NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Our abstract is consistent with our introduction. In the introduction, we point
to the places in the paper in which we substantiate our claims.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We provide a discussion in Section 5.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
54Justification: Assumptions are specified before all theorem statements.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide details in Sections 3.2 and J.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
55Answer: [Yes]
Justification: Please see supplementary material.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide details in Sections 3.2 and J.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Please see results in Sections 3.2 and J.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
56•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Please see Section J.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: we have read the Code of Ethics and ensured conformity.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is primarily an analysis of an already existing algorithm.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
57generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
58Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
59