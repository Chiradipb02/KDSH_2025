Unveiling the Potential of Robustness in Selecting
Conditional Average Treatment Effect Estimators
Yiyan Huang∗
The Hong Kong Polytechnic University
yiyhuang3-c@my.cityu.edu.hkCheuk Hang Leung∗
City University of Hong Kong
chleung87@cityu.edu.hk
Siyi Wang
City University of Hong Kong
siyi.wang@my.cityu.edu.hkYijun Li
City University of Hong Kong
yijunli5-c@my.cityu.edu.hk
Qi Wu†
City University of Hong Kong
qi.wu@cityu.edu.hk
Abstract
The growing demand for personalized decision-making has led to a surge of
interest in estimating the Conditional Average Treatment Effect (CATE). Various
types of CATE estimators have been developed with advancements in machine
learning and causal inference. However, selecting the desirable CATE estimator
through a conventional model validation procedure remains impractical due to the
absence of counterfactual outcomes in observational data. Existing approaches
for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face
two challenges. First, they must determine the metric form and the underlying
machine learning models for fitting nuisance parameters (e.g., outcome function,
propensity function, and plug-in learner). Second, they lack a specific focus
on selecting a robust CATE estimator. To address these challenges, this paper
introduces a Distributionally Robust Metric (DRM) for CATE estimator selection.
The proposed DRM is nuisance-free, eliminating the need to fit models for nuisance
parameters, and it effectively prioritizes the selection of a distributionally robust
CATE estimator. The experimental results validate the effectiveness of the DRM
method in selecting CATE estimators that are robust to the distribution shift incurred
by covariate shift and hidden confounders.
1 Introduction
The escalating demand for decision-making has sparked an increasing interest in Causal Inference
across various research domains, such as economics [ 23,13,43,1], statistics [ 70,49,26,41],
healthcare [ 78,27,61,9,42], and financial application [ 11,15,37,21,35,24,50]. The primary
goal in personalized decision-making is to quantify the causal effect of a specific treatment (or
policy/intervention) on the target outcome, and understanding such causal effects is closely connected
with identifying the Conditional Average Treatment Effect (CATE) . In observational studies, identify-
ing the CATE faces a significant and fundamental challenge: the absence of counterfactual knowledge.
According to Rubin Causal Model [ 64], the CATE is determined by comparing potential outcomes
∗The co-first authors.
†Corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).under different treatment assignments (i.e., treat and control) for a specific covariate. Nonetheless, in
real-world applications, we can only observe the potential outcome under the actual treatment (i.e.,
factual outcome ), while the potential outcome under the alternative treatment (i.e., counterfactual
outcome ) remains unobserved. The unavailability of the counterfactual outcome is widely recognized
as the fundamental problem in causal inference [ 33], making it difficult to accurately determine the
true value of the CATE.
The advancement of machine learning (ML) has opened up a promising opportunity to improve the
CATE estimation from observational data. Several innovative CATE estimation approaches, such
as meta-learners and causal ML models, have been proposed to tackle the fundamental challenge in
causal inference and enhance the predictive accuracy of CATE estimates (as discussed in Section 3).
Nevertheless, the emergence of various CATE estimation methods has brought forth a new question:
Given multifarious options for CATE estimators, which should be chosen? In observational data,
treatment is often non-random and propensity scores remain unknown. Conventional model validation
procedures, unfortunately, are not suitable for CATE estimator selection in this case due to the absence
of ground truth CATE labels. Therefore, exploring proper metrics for CATE estimator selection
remains an essential yet challenging research topic in causal inference.
Recent research has emphasized the significance of model selection for CATE estimators, as high-
lighted in [ 66,20,53]. These works have proposed and summarized two types of criteria for CATE
estimator selection, the plug-in metric Rplug
˜τ(ˆτ)and the pseudo-outcome metric Rpseudo
˜Y(ˆτ):
Rplug
˜τ(ˆτ) =vuut1
nnX
i=1(ˆτ(Xi)−˜τ(Xi))2,Rpseudo
˜Y(ˆτ) =vuut1
nnX
i=1(ˆτ(Xi)−˜Yi)2. (1)
One can establish a plug-in estimator ˜τor construct a pseudo-outcome estimator ˜Yusing the
validation data to select CATE estimator ˆτ. The previous studies [ 66,20,53] have shown that
these metrics offer some assistance in identifying well-performing CATE estimators. However, two
additional challenges are still encountered in these two metrics.
Challenge 1: How to determine the metric form and underlying ML models for nuisance pa-
rameters? As previously discussed, plug-in and pseudo-outcome metrics have various forms, and
both of them rely on estimating nuisance parameters ˜ηusing ML algorithms such as linear models,
tree-based models, etc. Plug-in metrics even need to fit an additional ML model for the plug-in learner
˜τ. However, selecting the suitable metric form and ML algorithms can be very difficult without
the knowledge of true data generating process. Consequently, we might go round in circles as this
challenge leads us back to the original estimator selection problem [20].
Challenge 2: These metrics are not well-targeted for selecting a robust CATE estimator. In
potential outcome framework [ 64], the factual distribution PFand the counterfactual distribution
PCFfort∈ {0,1}can be defined as follows:
PF:=P(X, Yt|T=t) =P(Yt|X, T =t)P(X|T=t);
PCF:=P(X, Yt|T= 1−t) =P(Yt|X, T = 1−t)P(X|T= 1−t).(2)
The above (2)reveals that the covariate shift P(X|T=t)̸=P(X|T= 1−t)leads to a distribution
shift between PFandPCF- and such distribution shift can be further exacerbated once the uncon-
foundedness assumption P(Yt|X, T =t) =P(Yt|X, T = 1−t)is violated. It is widely recognized
that ML models often struggle when the training and test data do not adhere to the same distribution.
Therefore, it becomes essential to select a CATE estimator learned on PFthat demonstrates robust
performance to the counterfactual distribution PCF. This need for robustness holds even greater
significance than the pursuit of an ideal “stellar” estimator because striving for the perfect estimator
can be futile in the absence of ground truth counterfactual labels.
Contributions. In this paper, we propose a Distributionally Robust Metric (DRM) for CATE
estimator selection. The main contributions are summarized as follows: (1) The proposed DRM
method is nuisance-free, eliminating the need to fit models for nuisance parameters (outcome function,
propensity function, and plug-in learner). (2) The DRM method is designed to prioritize selecting
a distributionally robust CATE estimator. (3) We provide a finite sample analysis of the proposed
2distributionally robust value ˆVt(ˆτ)fort∈ {0,1}, showing it decays to Vt(ˆτ)at a rate of n−1/2. (4)
Experimental results validate the effectiveness of the DRM method in selecting a CATE estimator
that is robust to the distribution shift incurred by covariate shift and hidden confounders.
2 Background of CATE Estimator Selection
Suppose the observational data contain ni.i.d. samples {(xi, ti, yi)}n
i=1, with the associated random
variables being {(Xi, Ti, Yi)}n
i=1. For each unit i,Xi∈ X ⊂ Rdisd-dimensional covariates and
Ti∈ {0,1}is the binary treatment. Potential outcomes for treat ( T= 1) and control ( T= 0) are
denoted by Y1, Y0∈ Y ⊂ R. The observed (factual) outcome is Y=TY1+ (1−T)Y0. The
propensity score [ 63] is defined as π(x) :=P(T= 1|X=x). The conditional mean potential
outcome surface is defined as µt(x) :=E[Yt|X=x]fort∈ {0,1}. The true CATE is defined as
τtrue(x) :=E
Y1−Y0|X=x
=µ1(x)−µ0(x).
Following the standard and necessary assumptions in potential outcome framework [ 64], we impose
Assumption 2.1 that ensure treatment effects are identifiable.
Assumption 2.1 (Consistency, Overlap, and Unconfoundedness) .Consistency: If the treatment is t,
then the observed outcome Yequals Yt. Overlap: The propensity score is bounded away from 0to1,
i.e.,0< π(x)<1,∀x∈ X. Unconfoundedness3:Yt⊥ ⊥T|X,∀t∈ {0,1}.
The goal of CATE estimator selection is to select the best CATE estimator, denoted by ˆτbest, from a
set of Jcandidate estimators {ˆτ1, . . . , ˆτJ}:
ˆτbest= arg min
ˆτ∈{ˆτ1,...,ˆτJ}Roracle(ˆτ),Roracle(ˆτ) :=vuut1
nnX
i=1(ˆτ(Xi)−τtrue(Xi))2. (3)
Here,Roracle(ˆτ)is associated with E[(ˆτ(X)−τtrue(X))2], known as the Precision of Estimating
Heterogeneous Effects (PEHE) w.r.t. ˆτ[32,68]. Note that Roracle(ˆτ)cannot be employed to evaluate
CATE estimators’ performances in real applications as we do not have access to τtrue. Previous
studies have introduced plug-in and pseudo-outcome metrics to aid in CATE estimator selection, as
shown in equation (1). Then, the CATE estimator ˆτselect is selected on validation data by
ˆτselect = arg min
ˆτ∈{ˆτ1,...,ˆτJ}Rplug
˜τ(ˆτ)orˆτselect = arg min
ˆτ∈{ˆτ1,...,ˆτJ}Rpseudo
˜Y(ˆτ). (4)
Notably, both the plug-in and pseudo-outcome metrics necessitate the fitting of nuisance parameters
˜η(e.g., ˜η= (˜µ1,˜µ0,˜π)) using off-the-shelf ML models. While some papers like [ 16] address the
selection of nuisance parameters for Averate Treatment Effect (ATE) estimators, e.g., the doubly
robust estimator [ 23,13,36], our paper focuses on the selection of CATE estimators rather than
nuisance parameters. For the plug-in metric, ˜τcan be constructed using any CATE estimator discussed
in Appendix A.1, yielding metrics such as plug-T, plug-DR, etc. For the pseudo-outcome metric,
˜Ycan be constructed using a specific formula discussed in Appendix A.2, yielding metrics such
as pseudo-DR, pseudo-R, etc. The metrics based on the influence function [ 5] and the R-learner
objective [ 55] are categorized into the pseudo-outcome metric. The categorization of plug-in and
pseudo-outcome metrics maintains consistency with [20, 53].
3 Related Work
CATE estimation. Recent advancements in ML have emerged as powerful tools for estimating
CATE from observational data, and researchers pay particular attention to meta-learners andcausal
MLmodels. Existing meta-learners mainly include traditional learners such as S-learner, T-learner,
PS-learner, and IPW-learner, as well as new learners such as X-learner [ 47], U-learner [ 25,55],
DR-learner [ 41,26], R-learner [ 55], and RA-learner [ 18]. The specific details of these meta-learners
are stated in Appendix A.1. Additionally, some studies also focus on developing innovative causal ML
3Note that in the setting C of our experiments, the unconfoundedness assumption is violated, leading to
misspecified nuisance parameters in CATE estimators and baseline selectors.
3models for CATE estimation, such as Causal BART [ 30], Causal Forest [ 70,8,58], generative models
like CEV AE [ 51] and GANITE [ 77], representation learning nets including SITE [ 76], TARNet [ 68],
Dragonnet [ 69], FlexTENet [ 19], and HTCE [ 10], disentangled learning nets like D2VD [ 44,45],
DeR-CFR [ 74], and DR-CFR [ 31], and representation balancing nets such as BNN [ 39], CFRNet
[68], DKLITE [ 79], IGNITE [ 29], BWCFR [ 6], DRRB [ 35], and DIGNet [ 38]. Recent surveys
[28, 75, 56] have also conducted a systematic review of various causal inference methods.
CATE estimator selection. Compared to the diverse range of CATE estimation methods, selecting
CATE estimators has received limited attention in existing causal inference research. Current methods
for selecting CATE estimators can be broadly classified into two main categories. The first category ,
which is also considered in this paper, involves using plug-in and pseudo-outcome methods to
evaluate CATE estimators. These methods share two common characteristics: 1) Both methods
require fitting ML models for nuisances (e.g., outcome function, propensity function, CATE function)
on a validation set and then implementing the learned ML models in either the plug-in surrogate or
the pseudo-outcome surrogate; 2) Both methods serve as surrogates for the expected error between
the CATE estimator and the true CATE, i.e., Roracle(ˆτ)in equation (3). The difference between the
two methods is that the plug-in method directly approximates the true CATE function, where only
covariate variables are involved, while the pseudo-outcome method typically constructs a specific
formula incorporating covariates, treatment, and outcome variables. For example, the pseudo-DR
proposed in [ 65] is constructed by the outcome predictors learned with representation balancing
objective [ 68,40]. Recent research [ 66,20,53] has conducted thorough empirical investigations
into exploring these two methods for selecting CATE estimators. Their findings suggest that no
single selection criterion can universally outperform others in all scenarios in the task of selecting
CATE estimators. More details of the two selection methods are stated in Appendix A.2. The second
category considers leveraging the data generating process (DGP) to generate synthetic data with
the known true CATE function, allowing the validation of CATE estimators’ performance on this
synthetic data. For example, authors in [ 2] find that placebo and structured empirical Monte Carlo
methods are helpful for estimator selection under some restrictive conditions. In addition, researchers
in [67,7,59] focus on training generative models to enforce the generated data to approximate the
distribution of the observed data. However, the DGP-based method still faces some limitations in
CATE estimator selection: 1) it only guarantees the resemblance of the generated data to the factual
distribution, without considering the counterfactual distribution; and ii) there is a potential risk of the
method favoring estimators that closely resemble the generative models [17].
4 The Distributionally Robust Metric
In this section, we introduce the Distributionally Robust Metric (DRM) for CATE estimator selection.
First, we capture the uncertainty in PEHE in a distributionally robust manner (Section 4.1). We then
establish the DRM based on the distributionally robust value of PEHE (Section 4.2).
4.1 Capturing the Uncertainty in PEHE
Proposition 4.1. The PEHE w.r.t. the CATE estimator ˆτcan be decomposed as follows:
E[(ˆτ(X)−τtrue(X))2] =E[ˆτ(X)2] + 2E[ˆτ(X)Y0] + 2E[−ˆτ(X)Y1] +ζ, (5)
where ζ=E[(µ1(X)−µ0(X))2]. The proof is deferred to Appendix B.1.
Proposition 4.1 indicates that the PEHE is equal to four terms, where E[ˆτ(X)2],E[ˆτ(X)Y0], and
E[−ˆτ(X)Y1]depend on ˆτ, while ζis a constant that is independent of ˆτ. The term E[ˆτ(X)Yt]for
t∈ {0,1}can be further decomposed as follows:
E[ˆτ(X)Yt] =E[ˆτ(X)Yt|T=t]| {z }
(a) Empirically computableP(T=t) +E[ˆτ(X)Yt|T= 1−t]| {z }
(b) Empirically uncomputableP(T= 1−t).(6)
Equation (6a) can be computed empirically since the potential outcome Ytis observable in the group
ofT=t. However, equation (6b) is empirically uncomputable due to the unavailability of Ytin the
group of T= 1−t. The unknown term E[ˆτ(X)Yt|T= 1−t]therefore determines the uncertainty
in PEHE. To capture such an uncertainty, we therefore establish distributionally robust values for
E[ˆτ(X)Y0|T= 1] andE[−ˆτ(X)Y1|T= 0] based on a Kullback-Leibler (KL) ambiguity set.
4Definition 4.2 (KL ambiguity set) .Given two distributions QandPand the ambiguity radius ϵ >0.
The KL ambiguity (uncertainty) set Bϵ(P)is defined as
Bϵ(P) :={Q:DKL(Q||P)≤ϵ},where DKL(Q||P) =Z
Xq(x) logq(x)
p(x)dx. (7)
Here, DKL(Q||P)denotes the KL divergence of some arbitrary distribution Qfrom the reference
distribution P. Now we define the distribution of (X, Y0, Y1)in the treated and controlled groups as
PT:=P(X, Y0, Y1|T= 1); PC:=P(X, Y0, Y1|T= 0). (8)
By setting an adequately large ambiguity radius in Definition 4.2, the following inequalities hold for
E[ˆτ(X)Y0|T= 1] = EPT[ˆτ(X)Y0]andE[−ˆτ(X)Y1|T= 0] = EPC[−ˆτ(X)Y1]:
E[ˆτ(X)Y0|T= 1] = EPT[ˆτ(X)Y0]≤ sup
Q∈Bϵ0(PC)EQ[ˆτ(X)Y0] =:V0(ˆτ);
E[−ˆτ(X)Y1|T= 0] = EPC[−ˆτ(X)Y1]≤ sup
Q∈Bϵ1(PT)EQ[−ˆτ(X)Y1] =:V1(ˆτ).(9)
To provide a clearer understanding, let us consider the example of EPT[ˆτ(X)Y0]. Since the term
E[ˆτ(X)Y0]is computable on its factual distribution PCbut uncomputable on its counterfactual
distribution PT, we can construct an ambiguity set centered around the distribution PCsuch that
it is large enough to contain the distribution PT. By doing so, we can capture the uncertainty of
EPT[ˆτ(X)Y0]w.r.t. ˆτ. In other words, the value of the uncomputable quantity EPT[ˆτ(X)Y0]will be
at most V0(ˆτ). Similarly, the value of the uncomputable quantity EPC[−ˆτ(X)Y1]will be at most
V1(ˆτ). Obviously, the uncertainty in PEHE will be larger if the distribution shift between factual and
counterfactual distribution is severer. Consequently, we can obtain the distributionally robust value of
PEHE in Corollary 4.3, which measures the uncertainty in PEHE.
Corollary 4.3. LetV0(ˆτ)andV1(ˆτ)be the quantities defined in equation (9),ζbe the constant
given in Proposition 4.1, u1:=P(T= 1) , and u0= 1−u1=P(T= 0) . The distributionally
robust value of PEHE w.r.t. ˆτis defined as VPEHE (ˆτ)such that
E[(ˆτ(X)−τtrue(X))2]≤ VPEHE (ˆτ)
=E[ˆτ(X)2] + 2 
u0EPC[ˆτ(X)Y0] +u1EPT[−ˆτ(X)Y1]
+ 2 
u0V1(ˆτ) +u1V0(ˆτ)
+ζ.(10)
4.2 Establishing Distributionally Robust Metric
As Corollary 4.3 provides the distributionally robust (worst-case) value of PEHE, it can naturally
measure the robustness of the CATE estimator ˆτagainst distribution shift between counterfactual
distribution and factual distribution. In this section, we will provide two steps involved in using
Corollary 4.3 to construct the DRM method for CATE estimator selection.
Step 1: Establishing computational tractability of Vt(ˆτ).The distributionally robust values
V0(ˆτ)andV1(ˆτ)in equation (10) are initially defined as supremum problems over infinite support,
presenting a substantial computational challenge. Theorem 4.4 reformulates the infeasible supremum
problems into tractable minimum problems.
Theorem 4.4. The distributionally robust values V0(ˆτ)andV1(ˆτ)in equation (9)are equivalent to
V0(ˆτ) = min
λ0>0λ0ϵ0+λ0logEPC[exp(ˆ τ(X)Y0/λ0)];
V1(ˆτ) = min
λ1>0λ1ϵ1+λ1logEPT[exp(−ˆτ(X)Y1/λ1)].(11)
The proof is deferred to Appendix B.3.
In the finite-sample scenario, V0(ˆτ)andV1(ˆτ)can be empirically approximated as follows:
ˆV0(ˆτ) = min
λ0>0λ0ϵ0+λ0log1
ncnX
i=1(1−Ti) exp(ˆ τ(Xi)Yi/λ0);
ˆV1(ˆτ) = min
λ1>0λ1ϵ1+λ1log1
ntnX
i=1Tiexp(−ˆτ(Xi)Yi/λ1).(12)
5Algorithm 1 Using DRM for CATE Estimator Selection
Input: The candidate CATE estimators {ˆτ1, . . . , ˆτJ}. The validation dataset with ni.i.d. observa-
tional samples {(Xi, Ti, Yi)}n
i=1. The number of iterations K. The initialization λ(0)
0andλ(0)
1.
The ambiguity radius ϵ0andϵ1.
1:forj= 1toJdo
2: fork= 0toK−1do
3: Compute ˆFt(λ(k)
t, ϵt; ˆτj)fort∈ {0,1}by equation (14a).
4: Compute ∂ˆFt(λ(k)
t, ϵt; ˆτj)/∂λ(k)
tfort∈ {0,1}by equation (14b).
5: λ(k+1)
t←max{λ(k)
t−ˆFt(λ(k)
t, ϵt; ˆτj)/(∂ˆFt(λ(k)
t, ϵt; ˆτj)/∂λ(k)
t),0}fort∈ {0,1}.
6: Save ˆVt(ˆτj)[k] =ˆFt(λ(k+1)
t, ϵt; ˆτj)fort∈ {0,1}.
7: Return ˆVt(ˆτj) = arg min k∈{0,...,K−1}ˆVt(ˆτj)[k]fort∈ {0,1}.
8: UseˆV0(ˆτj)andˆV1(ˆτj)to compute RDRM(ˆτj)by equation (15).
Output: ˆτselect = arg minˆτ∈{ˆτ1,...,ˆτJ}RDRM(ˆτ).
Note that in equation (12), the potential outcomes Y0andY1are replaced by the observed outcome
Ydue to the fact that (1−T)Y0= (1−T)YandTY1=TY, which aligns with the Consistency
assumption in Assumption 2.1. We then provide a finite-sample analysis of the gap between ˆVt(ˆτ)
andVt(ˆτ)in the following Theorem 4.5, which suggests the gap decays at a rate of n−1/2.
Theorem 4.5. Letut:=P(T=t)fort∈ {0,1}. Assume 0<¯λ≤λ0, λ1≤¯λandˆτ(X)Y
is bounded within the range of¯Mto¯M. Define Cexp=1{¯M≤¯M≤0}exp ¯M/¯λ−¯M/¯λ
+
1{¯M≤0,¯M≥0}exp ¯M/¯λ−¯M/¯λ
+1{0≤¯M≤¯M}exp ¯M/¯λ−¯M/¯λ
. For n≥2/u2log(2/δ)and
t∈ {0,1}, with probability 1−δ, we have
|ˆVt(ˆτ)− Vt(ˆτ)| ≤ O
s
8¯λ2log2
δ
nu2
tC2exp
+O
s
2¯λ2log(2
δ)
nu2
t
. (13)
The proof is deferred to Appendix B.4.
Step 2: Finalizing Distributionally Robust Metric for CATE estimator selection. We first define
two functions that are useful in obtaining V0(ˆτ)andV1(ˆτ):
ˆF0(λ0, ϵ0; ˆτ) =λ0ϵ0+λ0log1
ncncX
i=1eZi
λ0,ˆF1(λ1, ϵ1; ˆτ) =λ1ϵ1+λ1log1
ntntX
i=1e−Zi
λ1; (14a)
∂ˆF0
∂λ0=ϵ0+ logncX
i=1eZi
λ0
nc−Pnc
i=1ZieZi
λ0
λ0Pnc
i=1eZi
λ0,∂ˆF1
∂λ1=ϵ1+ logntX
i=1e−Zi
λ1
nt−Pnt
i=1−Zie−Zi
λ1
λ1Pnt
i=1e−Zi
λ1.(14b)
Here, Zdenotes ˆτ(X)Yfor notational simplicity. We then use the Newton-Raphson method to find
the empirical solution for ˆVt(ˆτ), exploiting the convexity of ˆFt(λt, ϵt; ˆτ)w.r.t. λt. Based on the
distributionally robust value of PEHE, i.e., VPEHE (ˆτ)in equation (10), we finally obtain the selected
estimator ˆτselect = arg minˆτ∈{ˆτ1,...,ˆτJ}RDRM(ˆτ)such that
RDRM(ˆτ) =1
nnX
i=1ˆτ(Xi)2+2
n ncX
i=1ˆτ(Xi)Yi+ntX
i=1−ˆτ(Xi)Yi+ncˆV1(ˆτ) +ntˆV0(ˆτ)!
.(15)
Algorithm 1 provides complete procedure of using the DRM method for CATE estimator selection.
Discussion on the ambiguity radius ϵ.The ambiguity radius ϵplays a critical role in real-world
applications [ 54,52,60]. However, determining an appropriate value for ϵcan be challenging as it
requires striking a balance between ensuring the bound in equation (9)holds and maintaining its
tightness. Specifically, if ϵis set too small, it fails to guarantee that the counterfactual distribution is
contained within the ambiguity set centered at factual distribution (the bound in Corollary 4.3 can
hold). On the other hand, if ϵis set too large, even though the ambiguity set can encompass more
6distributions to ensure the counterfactual distribution is contained, the bound in Corollary 4.3 can
be less tight. In general, selecting a proper ambiguity radius is an open problem in distributioanlly
robust optimization (DRO) literature [34, 54, 46, 48, 72].
In this paper, we provide a guidance for determining the ambiguity radius for our DRM method. Based
on the above discussion, an ideal radius should be ϵ∗
1=DKL(PC||PT)andϵ∗
0=DKL(PT||PC),
which ensures that the bound in Corollary 4.3 holds and is tight. However, as defined in equation
(8), both PCandPTinvolve counterfactual information, making it unattainable to directly compute
DKL(PC||PT)andDKL(PT||PC). To overcome this challenge, we demonstrate that Proposition
4.6 provides an intriguing alternative approach to acquire DKL(PC||PT)andDKL(PT||PC)when
unconfoundedness in Assumption 2.1 is satisfied.
Proposition 4.6. LetPT
X:=P(X|T= 1) andPC
X:=P(X|T= 0) denote the covariates distri-
bution in the treat and control group, respectively. Assuming that random variables (X, T, Y1, Y0)
satisfy the unconfoundedness in Assumption 2.1, we have
DKL(PC||PT) =DKL(PC
X||PT
X);DKL(PT||PC) =DKL(PT
X||PC
X). (16)
The proof is deferred to Appendix B.2.
Proposition 4.6 provides an important insight that the uncomputable term DKL(PC||PT)(or
DKL(PT||PC)) can be replaced by a computable quantity DKL(PC
X||PT
X)(orDKL(PT
X||PC
X)),
where PC
XandPT
Xare empirically observable. Consequently, the ideal ambiguity radius can be set
asϵ∗
1=DKL(PC
X||PT
X)andϵ∗
0=DKL(PT
X||PC
X). While the KL divergence can be approximated
using empirical algorithm (e.g, Nearest-Neighbor [ 73,57]), we recommend setting the ambiguity
radius larger than the empirically approximated KL divergence (see specific explanations in Appendix
C.1). This is necessary because it ensures that the ambiguity set is large enough to contain the
target distribution. It is also important to note that though the Algorithm 1 involves approximating
ϵ∗
1=DKL(PC
X||PT
X)andϵ∗
0=DKL(PT
X||PC
X), the DRM itself remains free of nuisances, as this
approach only determines the ambiguity radius but does not involve learning any nuisance function
such as the outcome function, propensity function, and plug-in learner.
5 Experiments
5.1 Experimental Setup.
Estimators & Selectors. We consider a total of 36 CATE estimators , comprising the combination
of 4 base ML models and 9 meta-learners. Specifically, the base ML models are Linear Regression
(LR), Support Vector Machine (SVM), Random Forests (RF), and Neural Net (Net). We consider
these ML models for CATE estimators because they are representative of both rigid and flexible
models, with each encoded distinct inductive biases, as highlighted by [ 19,20]. Note that for the LR
method, we employ Ridge regression for regression tasks and Logistic regression for classification
tasks. As for the remaining methods, we utilize their corresponding regressors and classifiers for
regression and classification tasks, respectively. Regarding the meta-learners, we select a set of both
traditional basic learners (S-, T-, PS-, and IPW-learners) and recently developed learners (X-, DR-,
U-, R-, and RA-learners), as detailed in Appendix A.1. We consider 14 CATE selectors , consisting
of 9 plug-in methods that rely on the above 9 learners, 3 pseudo-outcome methods (pseudo-DR, -R,
and -IF), the random selection, the factual selection (from the 6-learner pool with S-, T-), the Nearest-
Neighbor Matching [ 62], and our proposed DRM. The specific details of baseline selectors are stated
in Appendix A.2. We employ the eXtreme Gradient Boosting (XGB) [ 12] as the underlying ML
model for both plug-in and pseudo-outcome methods. We choose XGB because: i) it demonstrates
superior performance in various scenarios, ensuring a good performance of baseline selectors; ii) the
need to avoid potential congeniality bias that may arise from using the similar ML models employed
in CATE estimators [ 20]; iii) aligning with [ 5] where XGB is used for their proposed pseudo-IF
metric. The details of hyperparameters for nuisance models are stated in Section C.2 of Appendix.
Dataset. Since the ground truth of CATE is unavailable in real-world data, previous studies
commonly utilize semi-synthetic datasets to compare model performance. In line with [ 19,20], we
collect the covariates with n= 4802 data points from ACIC2016 dataset [ 22]. Then, we generate
treatment with Ti|Xi∼Bern (1/(1 + exp( −ξ(β′
TXi+ 3)))) , where Bern indicates the Bernoulli
7Table 1: Comparison of Regret for different selectors across Settings A, B, and C (Note that B
(ξ= 1) matches A ( ρ= 0.1)). Reported values (mean ±standard deviation) are computed over 100
experiments. Bold denotes the best three results among all selectors. Smaller value is better.
A (ρ= 0) A ( ρ= 0.1) A ( ρ= 0.3) B ( ξ= 0) B ( ξ= 2) C ( m= 0.1) C ( m= 0.5) C ( m= 0.9)
Plug-U 47.87 ±94.89 39.22±60.78 32.13±61.49 0.51±2.09 151.98 ±291.20 39.41±52.58 54.47±209.86 15.58±26.41
Plug-S 3.38 ±7.73 2.65±5.65 2.25±5.64 0.22±1.08 5.91±10.61 2.72±4.70 3.74±6.34 4.65±6.33
Plug-PS 3.08 ±7.27 2.65±5.65 2.24±5.64 0.22±1.08 5.91±10.61 2.72±4.70 3.55±6.11 4.65±6.33
Plug-T 59.12 ±21.87 56.38±23.02 55.35±21.36 10.48±10.72 64.96±18.03 59.87±18.73 43.28±23.64 36.28±19.33
Plug-X 8.10 ±10.57 7.67±12.26 5.77±11.26 4.76±10.81 11.55±13.94 7.78±15.02 10.76±14.62 11.80±10.82
Plug-IPW 33.37 ±28.34 35.78±27.50 35.26±27.24 4.43±7.40 58.64±23.85 38.43±31.16 24.98±22.79 20.54±19.67
Plug-DR 43.11 ±26.54 43.76±26.92 44.20±26.48 4.22±7.97 64.60±18.88 46.57±32.93 28.66±23.08 23.59±18.21
Plug-R 1.92±4.91 2.62±15.63 1.47±3.60 0.43±2.09 9.78±31.43 1.91±4.94 2.53±6.51 2.12±4.94
Plug-RA 56.60 ±24.06 57.69±19.98 54.60±22.86 6.60±9.16 64.50±17.72 55.87±19.63 40.48±24.55 33.34±19.04
Pseudo-DR 61.35 ±22.41 61.09±20.08 59.06±19.46 14.75±22.95 70.02±17.53 62.08±19.65 48.83±25.78 44.99±23.53
Pseudo-R 9.85 ±27.04 14.12±45.74 5.94±21.05 4.73±20.40 14.86±30.64 10.58±24.63 15.93±29.84 21.26±32.51
Pseudo-IF 64.54 ±15.18 62.49±16.61 62.69±16.13 26.73±23.60 65.74±16.68 60.06±21.16 54.96±20.63 38.60±22.21
Random 7214 ±22745 6511±21651 4196±17049 1135±5596 7549±22498 3768±16625 6214±19942 3445±14591
Fact 51.09 ±18.00 50.86±19.40 51.01±21.03 14.33±16.54 65.23±27.53 48.92±17.19 47.40±22.51 40.37±23.14
Matching 60.85 ±21.45 62.18±17.77 59.91±18.67 13.33±22.86 68.98±17.24 61.52±19.04 52.83±23.85 40.01±24.05
DRM 0.96±3.67 0.84±4.83 1.25±5.97 0.38±1.39 15.51±112.62 1.56±8.73 1.40±8.67 1.26±3.52
distribution. The potential outcomes are generated by a linear function with interaction terms:
Yi=dX
jβ′
jXi;j+dX
j=1dX
k=jβ′
j,kXi;jXi;k+dX
j=1dX
k=jdX
l=kβ′
j,k,lXi;jXi;kXi;l+TidX
j=1γjXi;j+ϵi.
The coefficient values are set as follows: βT, βj, βj,k, βj,k,l∼Bern (0.2),γj∼Bern (ρ), and
ϵi∼ N(0,0.1). The parameter ξin treatment assignment represents the level of selection bias, and
the parameter ρinγjrepresents the complexity of the CATE function. We adopt the above data
generating process to randomly generate 100 datasets, each with a training/validation/testing ratio of
49%/21%/30%.
Settings. In this section, we mainly investigate whether the estimator selected by DRM can
demonstrate robustness to the selection bias and unobserved confounders. In addition, as demonstrated
in [19,20], the complexity of CATE function also affects relative performance of estimators and
selectors. Given these considerations, we design the following three settings to compare the CATE
selectors. Setting A: With the unconfoundedness assumption, let ρvary in {0,0.1,0.3}with fixing
ξ= 1.Setting B: With the unconfoundedness assumption, let ξvary in {0,1,2}with fixing ρ= 0.1.
Setting C : Without unconfoundedness assumption, fix ρ= 0.1andξ= 1. Then randomly remove
⌊m·d⌋covariates such that the dimension of observed covariates is d− ⌊m·d⌋, where mdenotes
the ratio of missing covariates varying in {0.1,0.5,0.9}. All the experiments are run on Dell 3640
with Intel Xeon W-1290P 3.60GHz CPU.
Comparison criteria. The CATE estimator ˆτis believed better if it achieves a smaller difference
between Roracle(ˆτ)andRoracle(ˆτbest), where ˆτbestis the actual best estimator in equation (3). We
therefore use the following Regret criteria to compare estimators chosen by different selectors:
Regret =Roracle(ˆτselect)− Roracle(ˆτbest).
To further assess the ranking ability of each selector, we calculate the Spearman rank correlation
between the rank order determined by the oracle metric Roracle(ˆτ)and the rank order determined by
each selector. All the reported values (Mean ±Standard deviation) are computed over 100 runs.
5.2 Experimental Results
Regret comparison. The results presented in Table 1 demonstrate consistently good performance
from the DRM selector across various settings. In setting A, the DRM selector outperforms other
selectors as the CATE complexity ( ρ) varies. Additionally, Plug-R, Plug-S, and Plug-PS also perform
well in terms of the Regret criterion, which aligns with prior findings in [ 66] that the R-objective
is excellent in many cases. Note that the strong performance of Plug-S and Plug-PS may be due
to less pronounced heterogeneity in the CATE function compared to the outcome function in the
data generating process. We also compare the PEHE performance (i.e., Roracle(ˆτselect)) of different
selectors in Table 3 of Section C.3. The results indicate that Plug-R, Plug-S, and Plug-PS tend to
8Table 2: Comparison of rank correlation for different selectors across Settings A, B, and C (Note that
B (ξ= 1) matches A ( ρ= 0.1)). Bold denotes the best three results among all selectors. Reported
values (mean ±standard deviation) are computed over 100 experiments. Larger is better.
A (ρ= 0) A ( ρ= 0.1) A ( ρ= 0.3) B ( ξ= 0) B ( ξ= 2) C ( m= 0.1) C ( m= 0.5) C ( m= 0.9)
Plug-U 0.69 ±0.34 0.70±0.35 0.75±0.29 0.95±0.04 0.53±0.30 0.68±0.33 0.73±0.34 0.83±0.24
Plug-S 0.95 ±0.06 0.95±0.06 0.95±0.05 0.95±0.04 0.95±0.05 0.95±0.03 0.95±0.05 0.91±0.07
Plug-PS 0.95 ±0.06 0.95±0.06 0.95±0.05 0.95±0.04 0.95±0.05 0.95±0.03 0.95±0.05 0.91±0.07
Plug-T 0.54 ±0.18 0.54±0.18 0.54±0.16 0.89±0.07 0.57±0.16 0.51±0.16 0.58±0.21 0.59±0.21
Plug-X 0.94 ±0.05 0.94±0.04 0.94±0.04 0.93±0.05 0.93±0.05 0.93±0.04 0.92±0.06 0.85±0.13
Plug-IPW 0.72 ±0.19 0.71±0.19 0.71±0.19 0.92±0.06 0.68±0.15 0.69±0.19 0.76±0.18 0.77±0.17
Plug-DR 0.65 ±0.19 0.63±0.20 0.63±0.18 0.93±0.06 0.59±0.16 0.61±0.18 0.71±0.21 0.73±0.18
Plug-R 0.96±0.03 0.96±0.03 0.96±0.03 0.95±0.04 0.93±0.07 0.96±0.03 0.96±0.05 0.96±0.04
Plug-RA 0.55 ±0.19 0.54±0.17 0.55±0.17 0.92±0.06 0.57±0.15 0.53±0.17 0.60±0.22 0.62±0.21
Pseudo-DR 0.54 ±0.18 0.53±0.18 0.53±0.16 0.87±0.10 0.55±0.15 0.50±0.17 0.54±0.24 0.58±0.23
Pseudo-R 0.86 ±0.11 0.87±0.09 0.88±0.08 0.93±0.06 0.83±0.13 0.85±0.13 0.85±0.12 0.80±0.16
Pseudo-IF 0.52 ±0.17 0.52±0.17 0.51±0.15 0.66±0.18 0.64±0.16 0.52±0.16 0.53±0.19 0.62±0.18
Random 0.26 ±0.13 0.26±0.13 0.27±0.13 0.47±0.11 0.23±0.13 0.28±0.10 0.28±0.11 0.24±0.14
Fact 0.35 ±0.08 0.36±0.08 0.35±0.09 0.48±0.08 0.31±0.10 0.35±0.07 0.33±0.09 0.29±0.11
Matching 0.53 ±0.17 0.51±0.18 0.52±0.16 0.89±0.08 0.58±0.15 0.51±0.16 0.55±0.21 0.60±0.21
DRM 0.81 ±0.08 0.80±0.08 0.80±0.08 0.85±0.06 0.77±0.15 0.79±0.09 0.81±0.10 0.80±0.08
Figure 1: The stacked bar chart showing the distribution of the selected estimator’s rank for each
evaluation metric across rank intervals: [1-3], [4-11], [12-19], [20-27], and [28-36]. The greener (or
redder) color indicates that the selected estimator ranks higher (or lower). For example, the dark red
(orgreen ) indicates the percentage of cases (out of 100 experiments) where the selected estimator
ranks among the worst 9 estimators, specifically as ranks 28, 29, ..., or 36 (or among the best 3
estimators, specifically as ranks 1, 2, or 3).
exhibit better PEHE as the CATE complexity decreases, aligning with the findings in [ 20]. In setting
B, the DRM selector demonstrates robustness against selection bias (controlled by ξ) compared
to many baselines. However, for the case ξ= 2, DRM selects a poor estimator 1 or 2 times out
of 100 experiments, as shown in Figure 1. Although this weakens its overall performance, DRM
still outperforms many baselines in this scenario. In the scenario ξ= 0where no selection bias is
present, the factual selection criterion performs better in this specific setting. In this case, DRM
does not demonstrate a significant advantage, as there is no distribution shift caused by selection
bias. In setting C where the unconfoundedness assumption is violated, most selectors exhibit inferior
performance. In contrast, DRM demonstrates consistent outperformance across all three cases, and its
superiority becomes particularly significant as mincreases to 0.9, showcasing its robustness against
the distribution shift arising from unobserved confounders.
Ranking ability. In Table 2, the DRM method demonstrates favorable performance in ranking
estimators, surpassing certain Plug- (e.g., U, T, IPW, DR, RA) and Pseudo- (e.g., DR, IF) selectors.
9In comparison to other nuisance-free baselines (Random, Fact, and Matching), DRM achieves
significantly superior ranking ability. However, compared to Plug-S, -PS, -X, and -R, it does not
exhibit remarkable performance in ranking CATE estimators, possibly due to the fact that DRM selects
estimators based on their distributionally robust (worst-case) performance. Indeed, the definition of
ranking inherently involves the concept of expected (average) performance, which is not determined
solely by either the best or worst performance. While distributionally robust performance serves as
a suitable criterion for selecting players to participate in the Olympics, it may not be a reasonable
standard for ranking players’ average performance. Therefore, it would be intriguing to explore some
ways in future research that can enhance the ranking ability of our DRM selector.
Variance analysis. Table 1 indicates that baseline selectors tend to exhibit higher variances in
Regret performance. This is primarily due to the wide range of PEHE performances across the 36
CATE estimators. If a selector consistently selects either good or bad estimators, the variance would
not be very large. To investigate this further, we sorted all 36 estimators in ascending order based
on their Roracle(ˆτ)values, resulting in the sorted list: [Roracle(ˆτ1), . . . ,Roracle(ˆτJ)]. We then
determine the actual rank of the selected estimator within this list and visualize the distribution of
these 100 ranks using a stacked bar chart. Figure 1 shows that many baseline methods tend to select
CATE estimators from various percentile ranges, leading to high variance across the 100 selections.
Notably, the DRM selector consistently chooses higher-ranked (i.e., better performing in PEHE)
estimators, demonstrating its robustness in CATE estimator selection.
Potential improvements. There are several potential improvements based on the current experi-
mental settings. First, the existing results suggest that Plug-S performs better than Plug-T, indicating
that the complexity of CATE function is relatively simple. It would help to provide more compre-
hensive analysis if investigating how DRM compares to baselines when the CATE function is more
complex. Second, since the impact of selection bias can vary with sample size [ 3], it is important to
compare different selectors when the sample size is sufficiently large. Third, considering baselines
that are specifically designed for addressing hidden confounders could provide valuable insights for
testing different selectors under such conditions. We encourage deeper investigation of causal model
selection without assuming unconfoundedness. Finally, it would be good if future studies will apply
DRM and other selectors in Healthcare, Economics, and Business applications with real-world data,
as CATE estimator selection plays an important role in personalized decision makings.
6 Conclusion
This paper sheds lights on the potential of robustness in CATE estimator selection. We propose a
distributionally robust metric (DRM). The proposed metric is nuisance-free, eliminating the need
to fit models for nuisance parameters (outcome function, propensity function, and plug-in learner).
Additionally, it is well-targeted for selecting a robust CATE estimator. We provide a finite sample
analysis that demonstrates the gap between ˆVt(ˆτ)andVt(ˆτ)reduces at a rate of n−1/2fort∈ {0,1}.
The experimental results showcase that the CATE estimator selected by DRM demonstrate robustness
to the distribution shift incurred by covariate shift and hidden confounders.
Limitations and future work. This paper explores the potential of robustness in CATE estimator
selection. However, we must acknowledge that our DRM method is not a silver bullet, as consistent
estimation on the CATE are never attainable [ 14]. Here, we outline some challenges and suggest
future research directions. First, while Proposition 4.6 provides useful guidance for setting ambiguity
radius in the DRM algorithm, we cannot guarantee that the empirically-computed radius is optimal
due to potential bias in the algorithm’s approximation of KL-divergence. Second, as discussed
in Section 5.2, enhancing the ranking capability of DRM is a promising area for further research.
Moreover, our findings are based on KL-divergence. However, using other divergences, such as the
Wasserstein distance, to construct the ambiguity set could incorporate more diverse distributions,
despite the challenges in solving the dual formulation of the Wasserstein distributionally robust
value. Simultaneously, exploring whether alternative divergences can yield a tighter bound for the
PEHE error is also interesting [ 4]. Finally, inspired by [ 16], understanding how nuisance parameters
influence metrics like plug-DR and pseudo-DR might be helpful in CATE estimator selection. We
hope our methods and findings will spur interest in model selection for causal inference, as well as in
related fields like domain adaptation and out-of-distribution generalization.
10Acknowledgement
Qi WU acknowledges the support from The CityU-JD Digits Joint Laboratory in Financial Tech-
nology and Engineering, The Hong Kong Research Grants Council [General Research Fund
11219420/9043008], and The CityU APRC Grant 9610643. The work described in this paper
was partially supported by the InnoHK initiative, the Government of the HKSAR, and the Laboratory
for AI-Powered Financial Technologies. We finally thank all the anonymous reviewers for their
constructive suggestions.
References
[1]Alberto Abadie, Susan Athey, Guido W Imbens, and Jeffrey M Wooldridge. When should you
adjust standard errors for clustering? The Quarterly Journal of Economics , 138(1):1–35, 2023.
[2]Arun Advani, Toru Kitagawa, and Tymon Słoczy ´nski. Mostly harmless simulations? using
monte carlo studies for estimator selection. Journal of Applied Econometrics , 34(6):893–910,
2019.
[3]Ahmed Alaa and Mihaela Schaar. Limits of estimating heterogeneous treatment effects: Guide-
lines for practical algorithm design. In International Conference on Machine Learning , pages
129–138. PMLR, 2018.
[4]Ahmed Alaa and Mihaela van der Schaar. Limits of estimating heterogeneous treatment effects:
Guidelines for practical algorithm design. In Jennifer Dy and Andreas Krause, editors, Proceed-
ings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of
Machine Learning Research , pages 129–138. PMLR, 10–15 Jul 2018.
[5]Ahmed Alaa and Mihaela Van Der Schaar. Validating causal inference models via influence
functions. In International Conference on Machine Learning , pages 191–201. PMLR, 2019.
[6] Serge Assaad, Shuxi Zeng, Chenyang Tao, Shounak Datta, Nikhil Mehta, Ricardo Henao, Fan
Li, and Lawrence Carin. Counterfactual representation learning with balancing weights. In
International Conference on Artificial Intelligence and Statistics , pages 1972–1980. PMLR,
2021.
[7]Susan Athey, Guido W Imbens, Jonas Metzger, and Evan Munro. Using wasserstein generative
adversarial networks for the design of monte carlo simulations. Journal of Econometrics , 2021.
[8]SUSAN ATHEY , JULIE TIBSHIRANI, and STEFAN WAGER. Generalized random forests.
The Annals of Statistics , 47(2):1148–1178, 2019.
[9]Ioana Bica, Ahmed M Alaa, Craig Lambert, and Mihaela Van Der Schaar. From real-world
patient data to individualized treatment effects using machine learning: current and future
methods to address underlying challenges. Clinical Pharmacology & Therapeutics , 109(1):87–
100, 2021.
[10] Ioana Bica and Mihaela van der Schaar. Transfer learning on heterogeneous feature spaces for
treatment effects estimation. Advances in Neural Information Processing Systems , 35:37184–
37198, 2022.
[11] Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X Charles, D Max Chickering,
Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and
learning systems: The example of computational advertising. Journal of Machine Learning
Research , 14(11), 2013.
[12] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of
the 22nd acm sigkdd international conference on knowledge discovery and data mining , pages
785–794, 2016.
[13] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,
Whitney Newey, and James Robins. Double/debiased machine learning for treatment and
structural parameters, 2018.
11[14] Victor Chernozhukov, Mert Demirer, Esther Duflo, and Ivan Fernandez-Val. Generic machine
learning inference on heterogeneous treatment effects in randomized experiments, with an
application to immunization in india. Technical report, National Bureau of Economic Research,
2018.
[15] Zhixuan Chu, Stephen L Rathbun, and Sheng Li. Graph infomax adversarial learning for
treatment effect estimation with networked observational data. In Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery & Data Mining , pages 176–184, 2021.
[16] Yifan Cui and EJ Tchetgen Tchetgen. Selective machine learning of doubly robust functionals.
Biometrika , 111(2):517–535, 2024.
[17] Alicia Curth, David Svensson, Jim Weatherall, and Mihaela van der Schaar. Really doing great
at estimating cate? a critical look at ml benchmarking practices in treatment effect estimation.
InThirty-fifth conference on neural information processing systems datasets and benchmarks
track (round 2) , 2021.
[18] Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of heterogeneous treat-
ment effects: From theory to learning algorithms. In International Conference on Artificial
Intelligence and Statistics , pages 1810–1818. PMLR, 2021.
[19] Alicia Curth and Mihaela van der Schaar. On inductive biases for heterogeneous treatment
effect estimation. Advances in Neural Information Processing Systems , 34:15883–15894, 2021.
[20] Alicia Curth and Mihaela Van Der Schaar. In search of insights, not magic bullets: Towards
demystification of the model selection dilemma in heterogeneous treatment effect estimation.
InProceedings of the 40th International Conference on Machine Learning , volume 202 of
Proceedings of Machine Learning Research , pages 6623–6642. PMLR, 23–29 Jul 2023.
[21] Robert Donnelly, David Blei, Susan Athey, et al. Correction to: Counterfactual inference for
consumer choice across many product categories. Quantitative Marketing and Economics ,
19(3-4):409–409, 2021.
[22] Vincent Dorie, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone. Automated versus
do-it-yourself methods for causal inference: Lessons learned from a data analysis competition.
Statistical Science , 34(1):43–68, 2019.
[23] Max H Farrell. Robust inference on average treatment effects with possibly more covariates
than observations. Journal of Econometrics , 189(1):1–23, 2015.
[24] Carlos Fernández-Loría, Foster Provost, Jesse Anderton, Benjamin Carterette, and Praveen
Chandar. A comparison of methods for treatment assignment with an application to playlist
generation. Information Systems Research , 34(2):786–803, 2023.
[25] Aaron Fisher. Inverse-variance weighting for estimation of heterogeneous treatment effects. In
Forty-first International Conference on Machine Learning .
[26] Dylan J Foster and Vasilis Syrgkanis. Orthogonal statistical learning. The Annals of Statistics ,
51(3):879–908, 2023.
[27] Jared C Foster, Jeremy MG Taylor, and Stephen J Ruberg. Subgroup identification from
randomized clinical trial data. Statistics in medicine , 30(24):2867–2880, 2011.
[28] Ruocheng Guo, Lu Cheng, Jundong Li, P Richard Hahn, and Huan Liu. A survey of learning
causality with data: Problems and methods. ACM Computing Surveys (CSUR) , 53(4):1–37,
2020.
[29] Ruocheng Guo, Jundong Li, Yichuan Li, K Selçuk Candan, Adrienne Raglin, and Huan Liu.
Ignite: A minimax game toward learning individual treatment effects from networked observa-
tional data. In Proceedings of the Twenty-Ninth International Conference on International Joint
Conferences on Artificial Intelligence , pages 4534–4540, 2021.
[30] P Richard Hahn, Jared S Murray, and Carlos M Carvalho. Bayesian regression tree models for
causal inference: Regularization, confounding, and heterogeneous effects (with discussion).
Bayesian Analysis , 15(3):965–1056, 2020.
12[31] Negar Hassanpour and Russell Greiner. Learning disentangled representations for counterfactual
regression. In International Conference on Learning Representations , 2019.
[32] Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computa-
tional and Graphical Statistics , 20(1):217–240, 2011.
[33] Paul W Holland. Statistics and causal inference. Journal of the American statistical Association ,
81(396):945–960, 1986.
[34] Zhaolin Hu and L Jeff Hong. Kullback-leibler divergence constrained distributionally robust
optimization. Available at Optimization Online , 1(2):9, 2013.
[35] Yiyan Huang, Cheuk Hang Leung, Shumin Ma, Zhiri Yuan, Qi Wu, Siyi Wang, Dongdong Wang,
and Zhixiang Huang. Towards balanced representation learning for credit policy evaluation. In
International Conference on Artificial Intelligence and Statistics , pages 3677–3692. PMLR,
2023.
[36] Yiyan Huang, Cheuk Hang Leung, Qi Wu, Xing Yan, Shumin Ma, Zhiri Yuan, Dongdong Wang,
and Zhixiang Huang. Robust causal learning for the estimation of average treatment effects. In
2022 International Joint Conference on Neural Networks (IJCNN) , pages 1–9. IEEE, 2022.
[37] Yiyan Huang, Cheuk Hang Leung, Xing Yan, Qi Wu, Nanbo Peng, Dongdong Wang, and
Zhixiang Huang. The causal learning of retail delinquency. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 35, pages 204–212, 2021.
[38] Yiyan Huang, WANG Siyi, Cheuk Hang Leung, WU Qi, WANG Dongdong, and Zhixiang
Huang. Dignet: Learning decomposed patterns in representation balancing for treatment effect
estimation. Transactions on Machine Learning Research , 2024.
[39] Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual
inference. In International conference on machine learning , pages 3020–3029. PMLR, 2016.
[40] Fredrik D Johansson, Uri Shalit, Nathan Kallus, and David Sontag. Generalization bounds and
representation learning for estimation of potential outcomes and causal effects. The Journal of
Machine Learning Research , 23(1):7489–7538, 2022.
[41] Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal effects.
Electronic Journal of Statistics , 17(2):3008–3049, 2023.
[42] Newton Mwai Kinyanjui and Fredrik D Johansson. Adcb: An alzheimer’s disease simulator for
benchmarking observational estimators of causal effects. In Conference on Health, Inference,
and Learning , pages 103–118. PMLR, 2022.
[43] Toru Kitagawa and Aleksey Tetenov. Who should be treated? empirical welfare maximization
methods for treatment choice. Econometrica , 86(2):591–616, 2018.
[44] Kun Kuang, Peng Cui, Bo Li, Meng Jiang, Shiqiang Yang, and Fei Wang. Treatment effect
estimation with data-driven variable decomposition. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 31, 2017.
[45] Kun Kuang, Peng Cui, Hao Zou, Bo Li, Jianrong Tao, Fei Wu, and Shiqiang Yang. Data-driven
variable decomposition for treatment effect estimation. IEEE Transactions on Knowledge and
Data Engineering , 34(5):2120–2134, 2020.
[46] Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-
Abadeh. Wasserstein distributionally robust optimization: Theory and applications in machine
learning. In Operations research & management science in the age of analytics , pages 130–166.
Informs, 2019.
[47] Sören R Künzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating
heterogeneous treatment effects using machine learning. Proceedings of the national academy
of sciences , 116(10):4156–4165, 2019.
13[48] Daniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for
distributionally robust optimization. Advances in Neural Information Processing Systems ,
33:8847–8860, 2020.
[49] Shuangning Li and Stefan Wager. Random graph asymptotics for treatment effect estimation
under network interference. The Annals of Statistics , 50(4):2334–2358, 2022.
[50] Yijun Li, Cheuk Hang Leung, Xiangqian Sun, Chaoqun Wang, Yiyan Huang, Xing Yan,
Qi Wu, Dongdong Wang, and Zhixiang Huang. The causal impact of credit lines on spending
distributions. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38,
pages 180–187, 2024.
[51] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling.
Causal effect inference with deep latent-variable models. Advances in neural information
processing systems , 30, 2017.
[52] Shumin Ma, Cheuk Hang Leung, Qi Wu, Wei Liu, and Nanbo Peng. Understanding distributional
ambiguity via non-robust chance constraint. In Proceedings of the First ACM International
Conference on AI in Finance , pages 1–8, 2020.
[53] Divyat Mahajan, Ioannis Mitliagkas, Brady Neal, and Vasilis Syrgkanis. Empirical analysis
of model selection for heterogenous causal effect estimation. International Conference on
Learning Representations , 2024.
[54] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: performance guarantees and tractable reformulations. Mathemati-
cal Programming , 171(1-2):115–166, 2018.
[55] Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects.
Biometrika , 108(2):299–319, 2021.
[56] Ana Rita Nogueira, Andrea Pugnana, Salvatore Ruggieri, Dino Pedreschi, and João Gama.
Methods and tools for causal discovery and causal inference. Wiley interdisciplinary reviews:
data mining and knowledge discovery , 12(2):e1449, 2022.
[57] Yung-Kyun Noh, Masashi Sugiyama, Song Liu, Marthinus C Plessis, Frank Chongwoo Park,
and Daniel D Lee. Bias reduction and metric learning for nearest-neighbor estimation of
kullback-leibler divergence. In Artificial Intelligence and Statistics , pages 669–677. PMLR,
2014.
[58] Miruna Oprescu, Vasilis Syrgkanis, and Zhiwei Steven Wu. Orthogonal random forest for
causal inference. In International Conference on Machine Learning , pages 4932–4941. PMLR,
2019.
[59] Harsh Parikh, Carlos Varjao, Louise Xu, and Eric Tchetgen Tchetgen. Validating causal
inference methods. In International Conference on Machine Learning , pages 17346–17358.
PMLR, 2022.
[60] Georg Ch Pflug. Multistage stochastic decision problems: Approximation by recursive structures
and ambiguity modeling. European Journal of Operational Research , 306(3):1027–1039, 2023.
[61] Zhaozhi Qian, Yao Zhang, Ioana Bica, Angela Wood, and Mihaela van der Schaar. Synctwin:
Treatment effect estimation with longitudinal outcomes. Advances in Neural Information
Processing Systems , 34:3178–3190, 2021.
[62] Craig A Rolling and Yuhong Yang. Model selection for estimating treatment effects. Journal of
the Royal Statistical Society Series B: Statistical Methodology , 76(4):749–769, 2014.
[63] Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika , 70(1):41–55, 1983.
[64] Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions.
Journal of the American Statistical Association , 100(469):322–331, 2005.
14[65] Yuta Saito and Shota Yasui. Counterfactual cross-validation: Stable model selection procedure
for causal inference models. In International Conference on Machine Learning , pages 8398–
8407. PMLR, 2020.
[66] Alejandro Schuler, Michael Baiocchi, Robert Tibshirani, and Nigam Shah. A comparison
of methods for model selection when estimating individual treatment effects. arXiv preprint
arXiv:1804.05146 , 2018.
[67] Alejandro Schuler, Ken Jung, Robert Tibshirani, Trevor Hastie, and Nigam Shah. Synth-
validation: Selecting the best causal inference method for a given dataset. arXiv preprint
arXiv:1711.00083 , 2017.
[68] Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect:
generalization bounds and algorithms. In International conference on machine learning , pages
3076–3085. PMLR, 2017.
[69] Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of
treatment effects. Advances in neural information processing systems , 32, 2019.
[70] Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects
using random forests. Journal of the American Statistical Association , 113(523):1228–1242,
2018.
[71] Chi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight
automl library. Proceedings of Machine Learning and Systems , 3:434–447, 2021.
[72] Jie Wang, Rui Gao, and Yao Xie. Sinkhorn distributionally robust optimization. arXiv preprint
arXiv:2109.11926 , 2021.
[73] Qing Wang, Sanjeev R Kulkarni, and Sergio Verdú. A nearest-neighbor approach to estimating
divergence between continuous random vectors. In 2006 IEEE International Symposium on
Information Theory , pages 242–246. IEEE, 2006.
[74] Anpeng Wu, Junkun Yuan, Kun Kuang, Bo Li, Runze Wu, Qiang Zhu, Yueting Zhuang, and Fei
Wu. Learning decomposed representations for treatment effect estimation. IEEE Transactions
on Knowledge and Data Engineering , 35(5):4989–5001, 2022.
[75] Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, and Aidong Zhang. A survey on
causal inference. ACM Transactions on Knowledge Discovery from Data (TKDD) , 15(5):1–46,
2021.
[76] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. Representation
learning for treatment effect estimation from observational data. Advances in neural information
processing systems , 31, 2018.
[77] Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. Ganite: Estimation of individualized
treatment effects using generative adversarial nets. In International conference on learning
representations , 2018.
[78] Linying Zhang, Yixin Wang, Anna Ostropolets, Jami J Mulgrave, David M Blei, and George
Hripcsak. The medical deconfounder: assessing treatment effects with electronic health records.
InMachine Learning for Healthcare Conference , pages 490–512. PMLR, 2019.
[79] Yao Zhang, Alexis Bellot, and Mihaela Schaar. Learning overlapping representations for
the estimation of individualized treatment effects. In International Conference on Artificial
Intelligence and Statistics , pages 1005–1014. PMLR, 2020.
15Appendix
A CATE Estimation Strategies
A.1 CATE Learners
We now detail how to construct CATE learners using the observed samples {(Xi, Ti, Yi)}n
i=1. Note
that CATE learners are learned on the training set, so the sample size nhere equals the training
sample size. Denote ntby the sample size in the treat group, and ncby the sample size in the control
group such that n=nt+nc.
•S-learner: Let predictors= (X, T), response= Y. Train a model ˆµ(X, T). Then we obtain
ˆτS(X):
ˆτS(X) = ˆµ(X,1)−ˆµ(X,0).
•T-learner: Let predictors= XT(covariates in the treat), response= YT(outcome in the treat).
Train a model ˆµ1(X). Let predictors= XC(covariates in the control), response= YC(out-
come in the control). Train a model ˆµ0(X). Then we obtain ˆτT(X):
ˆτT(X) = ˆµ1(X)−ˆµ0(X).
•PS-learner: Fisrt-step: Train ˆτS(X)using the above-mentioned step in S-learner. Second-
step: Let predictors= X, response= ˆτS(X). Train a model ˆτPS(X)from the following
objective:
ˆτPS= arg min
τ1
nnX
i=1(τ(Xi)−ˆτS(Xi))2.
•IPW-learner: First-step: let predictors= X, response= T. Train a propensity score model
ˆπ(X). Construct surrogate of CATE using pseudo-outcomes with inverse propensity weight-
ing (IPW) formula: Y1,0
IPW =Y1
IPW−Y0
IPW , where Y1
IPW =TY
ˆπ(X)andY0
IPW =(1−T)Y
1−ˆπ(X).
Train a model ˆτIPW(X)from the following objective:
ˆτIPW = arg min
τ1
nnX
i=1(τ(Xi)−Y1,0
i,IPW )2.
•X-learner [ 47]: First-step: Train ˆµ1(X)andˆµ0(X)using the the above-mentioned proce-
dure in T-learner. Train a propensity score model ˆπ(X)using the the above-mentioned
procedure in IPW-learner. Second-step: Let predictors= XT, response= ˆµ1(XT)−YT, and
predictors= XC, response= ˆµ0(XC)−YC. Obtain a model ˆτX(X)by learning two separate
functions ˆτ1
X(X)andˆτ0
X(X):
ˆτX(X) = (1 −ˆπ(X))ˆτ1
X(X) + ˆπ(X)ˆτ0
X(X),
ˆτ1
X= arg min
τ1
ntntX
i=1(τ(Xi)−(Yi−ˆµ0(Xi)))2,
ˆτ0
X= arg min
τ1
ncncX
i=1(τ(Xi)−(ˆµ1(Xi)−Yi))2.
•U-learner [ 25,55]: First-step: Let predictors= X, response= Y. Train a model ˆµ(X)to
approximate the conditional mean outcome E[Y|X]. Train a propensity score model ˆπ(X)
using the the above-mentioned procedure in IPW-learner. Second-step: Compute the
outcome residual ξ=Y−ˆµ(X)and treatment residual ν=T−ˆπ(X). Train a model
ˆτU(X)from the following objective:
ˆτU= arg min
τ1
nnX
i=1(ξi
νi−τ(Xi))2.
16•DR-learner [ 41,26]: First-step: Train ˆµ1(X)andˆµ0(X)using the the above-mentioned
procedure in T-learner. Train a propensity score model ˆπ(X)using the the above-
mentioned procedure in IPW-learner. Second-step: Construct surrogate of CATE us-
ing pseudo-outcomes with doubly robust (DR) formula: Y1,0
DR=Y1
DR−Y0
DR, where
Y1
DR= ˆµ1(X) +T
ˆπ(X)(Y−ˆµ1(X))andY0
DR= ˆµ0(X) +1−T
1−ˆπ(X)(Y−ˆµ0(X)). Train a
model ˆτDR(X)from the following objective:
ˆτDR= arg min
τ1
nnX
i=1(τ(Xi)−Y1,0
i,DR)2.
•R-learner [ 55]: First-step: Let predictors= X, response= Y. Train a model ˆµ(X)to approxi-
mate the conditional mean outcome E[Y|X]. Train a propensity score model ˆπ(X)using
the the above-mentioned procedure in IPW-learner. Second-step: Compute the outcome
residual ξ=Y−ˆµ(X)and treatment residual ν=T−ˆπ(X). Train a model ˆτR(X)from
the following objective:
ˆτR= arg min
τ1
nnX
i=1(ξi−νiτ(Xi))2.
•RA-learner [ 18]: First-step: Train ˆµ1(X)andˆµ0(X)using the the above-mentioned pro-
cedure in T-learner. Second-step: Construct surrogate of CATE using pseudo-outcomes
with regression adjustment (RA) formula: YRA=T(Y−ˆµ0(X)) + (1 −T)(ˆµ1(X)−Y).
Train a model ˆτRA(X)from the following objective:
ˆτRA= arg min
τ1
nnX
i=1(τ(Xi)−Yi,RA)2.
A.2 CATE Selectors
We now detail how to construct CATE selectors using the observed samples {(Xi, Ti, Yi)}n
i=1. Note
that CATE selectors are constructed on the validation set, so the sample size nhere equals the
validation sample size.
•Plug-in selector: Obtain any CATE learners ˜τusing the observational validation data. Then
plug-in ˜τinto the following metric Rplug
˜τ(ˆτ):
Rplug
˜τ(ˆτ) =vuut1
nnX
i=1(ˆτ(Xi)−˜τ(Xi))2.
For each plug-in selector ˜τ, the selected j∗-th CATE estimator is ˆτj∗, where j∗=
arg minj∈{1,...,J}Rplug
˜τ(ˆτj).
• Pseudo-outcome selector:
1.Pseudo-DR: Utilize validation data to estimate nuisance parameters (˜µ1,˜µ0,˜π), fol-
lowing the procedure described in Section A.1. ˜YDR=˜Y1
DR−˜Y0
DR, where
˜Y1
DR= ˜µ1(X) +T
˜π(X)(Y−˜µ1(X))and˜Y0
DR= ˜µ0(X) +1−T
1−˜π(X)(Y−˜µ0(X)).
Then the pseudo-DR metric is
Rpseudo
DR (ˆτ) =vuut1
nnX
i=1(ˆτ(Xi)−˜Yi,DR)2.
For pseudo-DR selector, the selected j∗-th CATE estimator is ˆτj∗, where j∗=
arg minj∈{1,...,J}Rpseudo
DR (ˆτj).
2.Pseudo-R: Utilize validation data to estimate nuisance parameters (˜µ,˜π), following the
procedure described in Section A.1. Then the pseudo-R metric is
Rpseudo
R (ˆτ) =vuut1
nnX
i=1((Yi−˜µ(Xi))−ˆτ(Xi)(Ti−˜π(Xi)))2.
17For pseudo-R selector, the selected j∗-th CATE estimator is ˆτj∗, where j∗=
arg minj∈{1,...,J}Rpseudo
R (ˆτj).
3.Pseudo-IF [ 5]: Utilize validation data to estimate nuisance parameters (˜µ1,˜µ0,˜π),
following the procedure described in Section A.1. Let ˜τ(X) = (˜ µ1(X)−˜µ0(X)).
Then the pseudo-IF metric is
Rpseudo
IF (ˆτ) =vuut1
nnX
i=1((1−Bi)˜τ2(Xi) +BiYi(˜τ(Xi)−ˆτ(Xi))−Ai(˜τ(Xi)−ˆτ(Xi))2+ ˆτ2(Xi)),
where A i=Ti−˜π(Xi), Bi= 2Ti(Ti−˜π(Xi))C−1
i, Ci= ˜π(Xi)(1−˜π(Xi)).
For pseudo-IF selector, the selected j∗-th CATE estimator is ˆτj∗, where j∗=
arg minj∈{1,...,J}Rpseudo
IF (ˆτj).
4.Other pseudo-outcome selector: By manipulating the formula of ˜Y, it is possible to
create additional pseudo-outcome selectors, such as the pseudo-IPW selector. In our
paper, we choose pseudo-DR as the baseline because it is representative in the causal
inference literature and it often demonstrates superior performance, owing to its doubly
robust property.
B Proofs
B.1 Proof of Proposition 4.1
Proof.
E[(ˆτ(X)−τtrue(X))2]
=E[(ˆτ(X)−(µ1(X)−µ0(X)))2]
=E[(ˆτ(X)−µ1(X) +µ0(X))2]
=E[(ˆτ(X)−µ1(X))2] +E[µ0(X)2] + 2E[(ˆτ(X)−µ1(X))µ0(X)]
=E[ˆτ(X)2] +E[µ1(X)2]−2E[ˆτ(X)µ1(X)] +E[µ0(X)2] + 2E[ˆτ(X)µ0(X)]−2E[µ1(X)µ0(X)]
=E[ˆτ(X)2]−2E[ˆτ(X)(µ1(X)−Y1+Y1)] + 2E[ˆτ(X)(µ0(X)−Y0+Y0)]
+E[µ1(X)2] +E[µ0(X)2]−2E[µ1(X)µ0(X)]
=E[ˆτ(X)2]−2E[ˆτ(X)Y1]−2E[ˆτ(X)(µ1(X)−Y1)] + 2E[ˆτ(X)Y0] + 2E[ˆτ(X)(µ0(X)−Y0)]
+E[µ1(X)2] +E[µ0(X)2]−2E[µ1(X)µ0(X)]
=E[ˆτ(X)2]−2E[ˆτ(X)Y1]−2E[E[ˆτ(X)µ1(X)−ˆτ(X)Y1|X]] + 2E[ˆτ(X)Y0]
+ 2E[E[ˆτ(X)µ0(X)−ˆτ(X)Y0|X]] +E[µ1(X)2] +E[µ0(X)2]−2E[µ1(X)µ0(X)]
=E[ˆτ(X)2]−2E[ˆτ(X)Y1]−2E[ˆτ(X)µ1(X)−ˆτ(X)E[Y1|X]] + 2E[ˆτ(X)Y0]
+ 2E[ˆτ(X)µ0(X)−ˆτ(X)E[Y0|X]] +E[µ1(X)2] +E[µ0(X)2]−2E[µ1(X)µ0(X)]
=E[ˆτ(X)2]−2E[ˆτ(X)Y1]−2E[ˆτ(X)µ1(X)−ˆτ(X)µ1(X)] + 2E[ˆτ(X)Y0]
+ 2E[ˆτ(X)µ0(X)−ˆτ(X)µ0(X)] +E[µ1(X)2] +E[µ0(X)2]−2E[µ1(X)µ0(X)]
=E[ˆτ(X)2] + 2E[ˆτ(X)Y0]−2E[ˆτ(X)Y1] +E[µ1(X)2] +E[µ0(X)2]−2E[µ1(X)µ0(X)]
=E[ˆτ(X)2] + 2E[ˆτ(X)Y0]−2E[ˆτ(X)Y1] +ζ.
B.2 Proof of Proposition 4.6
The following Proposition B.1 is useful in proving Proposition 4.6.
Proposition B.1. Assuming the random variable tuple (X, T, Y1, Y0)satisfies Assumption 2.1, we
have
p(X, Y0, Y1|T= 0) = p(Y0, Y1|X)p(X|T= 0);
p(X, Y0, Y1|T= 1) = p(Y0, Y1|X)p(X|T= 1).(17)
18Proof.
p(X, Y0, Y1|T= 0)
=p(Y0, Y1|X, T = 0)p(X|T= 0)
=p(Y0, Y1|X)p(X|T= 0).(Unconfoundedness)
p(X, Y0, Y1|T= 1)
=p(Y0, Y1|X, T = 1)p(X|T= 1)
=p(Y0, Y1|X)p(X|T= 1).(Unconfoundedness)
Now we can prove Proposition 4.6.
Proof.
DKL(PC||PT)
=DKL(P(X, Y0, Y1|T= 0)||P(X, Y0, Y1|T= 1))
=Z
XZ
Y0Z
Y1p(x, y0, y1|T= 0) logp(x, y0, y1|T= 0)
p(x, y0, y1|T= 1)dy1dy0dx
=Z
XZ
Y0Z
Y1p(y0, y1|x)p(x|T= 0) logp(y0, y1|x)p(x|T= 0)
p(y0, y1|x)p(x|T= 1)dy1dy0dx (By Proposition B.1)
=Z
XZ
Y0Z
Y1p(y0, y1|x)p(x|T= 0) logp(x|T= 0)
p(x|T= 1)dy1dy0dx
=Z
XZ
Y0Z
Y1p(y0, y1|x)dy1dy0
p(x|T= 0) logp(x|T= 0)
p(x|T= 1)dx
=Z
Xp(x|T= 0) logp(x|T= 0)
p(x|T= 1)dx
=DKL(P(X|T= 0)||P(X|T= 1))
=DKL(PC
X||PT
X).
Similarly, it is easy to show DKL(PT||PC) =DKL(PT
X||PC
X)
B.3 Proof of Theorem 4.4
Lemma B.2 (Theorem 1 in [ 34]).Letfθ(X)denote the loss function of Xand it is bounded almost
surely. θ∈Θrepresents the model parameters of the function fθ(X). LetBϵ(P)be the uncertainty
ball centered at distribution Pwith ambiguity radius ϵ. Define κas the mass of the distribution Pon
its essential supremum (Proposition 2 in [ 34]). Assume fθ(X)is bounded and logκ+ϵ <0, then
we have
V:= sup
Q∈Bϵ(P)EQ[fθ(X)] = min
λ>0λϵ+λlogEP[exp( fθ(X)/λ)].
Our Theorem 4.4 follows by directly applying the above Lemma B.2.
B.4 Proof of Theorem 4.5
For notational simplicity, we denote W= (X, T, Y )∈ W andZ= ˆτ(X)Y. Assume Zis bounded
within the range¯Mand¯M. Define the following functions:
G0(λ0;W) =E[g0(λ0;W)],ˆG0(λ0;W) =1
nnX
i=1g0(λ0;Wi),
where g0(λ0;W) = (1 −T) exp ( Z/λ 0) ;
G1(λ1;W) =E[g1(λ1;W)],ˆG1(λ1;W) =1
nnX
i=1g1(λ1;Wi),
where g1(λ1;W) =Texp (−Z/λ 1).
19Then we have the following lemma that guarantees the convergence for ˆG0(λ0;W)andˆG1(λ1;W).
Lemma B.3. Assume 0<¯λ≤λ0, λ1≤¯λ, and ˆτ(X)Yis bounded within the range of¯Mto¯M.
Then with probability 1−δ, we have
If¯M≤¯M≤0 :
|ˆG0(λ0;W)−G0(λ0;W)| ≤ O
s
2 log2
δ 
exp ¯M/¯λ2
n
;
|ˆG1(λ1;W)−G1(λ1;W)| ≤ O
s
2 log2
δ(exp (−¯M/¯λ))2
n
.
If¯M≤0,¯M≥0 :
|ˆG0(λ0;W)−G0(λ0;W)| ≤ O
s
2 log2
δ 
exp ¯M/¯λ2
n
;
|ˆG1(λ1;W)−G1(λ1;W)| ≤ O
s
2 log2
δ(exp (−¯M/¯λ))2
n
.
If0≤¯M≤¯M:
|ˆG0(λ0;W)−G0(λ0;W)| ≤ O
s
2 log2
δ 
exp ¯M/¯λ2
n
;
|ˆG1(λ1;W)−G1(λ1;W)| ≤ O
s
2 log2
δ 
exp 
−¯M/¯λ2
n
.(18)
Proof. Denote h0(W1, W2, . . . , W n) =1
nPn
i=1g0(λ0;Wi). We notice that h0(W1, W2, . . . , W n)
satisfies the bounded difference inequality:
sup
W1,...,W n,W′
i∈W|h0(W1, . . . , W i,···, Wn)−h0(W1, . . . , W′
i,···, Wn)|
= sup
Wi,W′
i∈W|g0(λ0;Wi)−g0(λ0;W′
i)|
n
≤2 sup
Wi∈W|g0(λ0;Wi)|
n≤2 exp ¯M/λ 0
n.
Note that |ˆG0(λ0;W)−G0(λ0;W)|=|h0(W1, W2, . . . , W n)−E[h0(W1, W2, . . . , W n)]|. Then
using McDiarmid’s inequality, for any ϵ >0, we have
PˆG0(λ0;W)−G0(λ0;W)≥ϵ
=P(|h0(W1, W2, . . . , W n)−E[h0(W1, W2, . . . , W n)]| ≥ϵ)
≤2 exp
−2ϵ2
n(2 exp(¯M/λ 0)
n)2
= 2 exp 
−nϵ2
2 
exp ¯M/λ 02!
.
For some δ >0, we have
PˆG0(λ0;W)−G0(λ0;W)≥ϵ
≤2 exp 
−nϵ2
2 
exp ¯M/λ 02!
≤δ.
This solves ϵsuch that
ϵ≥s
2 log2
δ 
exp ¯M/λ 02
n.
20The above inequality should hold for any λ0such that 0<¯λ≤λ0≤¯λ. Therefore, we have
If¯M≥0 : ϵ≥s
2 log2
δ 
exp ¯M/¯λ2
n;
If¯M≤0 : ϵ≥s
2 log2
δ 
exp ¯M/¯λ2
n.
Similarly, denote h1(W1, W2, . . . , W n) =1
nPn
i=1g1(λ1;Wi). We note that h1(W1, W2, . . . , W n)
satisfies the bounded difference inequality:
sup
W1,...,W n,W′
i∈W|h1(W1, . . . , W i,···, Wn)−h1(W1, . . . , W′
i,···, Wn)|
= sup
Wi,W′
i∈W|g1(λ1;Wi)−g1(λ1;W′
i)|
n
≤2 sup
Wi∈W|g1(λ1;Wi)|
n≤2 exp ( −¯M/λ 1)
n.
Then using McDiarmid’s inequality, for any ϵ >0, we have
PˆG1(λ1;W)−G1(λ1;W)≥ϵ
=P(|h1(W1, W2, . . . , W n)−E[h1(W1, W2, . . . , W n)]| ≥ϵ)
≤2 exp 
−2ϵ2
n(2 exp(−¯M/λ 1)
n)2!
= 2 exp 
−nϵ2
2 (exp ( −¯M/λ 1))2!
.
For some δ >0, we have
PˆG1(λ1;W)−G1(λ1;W)≥ϵ
≤2 exp 
−nϵ2
2 (exp ( −¯M/λ 1))2!
≤δ.
This solves ϵsuch that
ϵ≥s
2 log2
δ(exp (−¯M/λ 1))2
n.
The above inequality should hold for any λ1such that 0<¯λ≤λ1≤¯λ. Therefore, we have
If¯M≥0 : ϵ≥s
2 log2
δ 
exp 
−¯M/¯λ2
n;
If¯M≤0 : ϵ≥s
2 log2
δ(exp (−¯M/¯λ))2
n.
In the following content, we will bound termslog(ˆG0(λ0;W))−log (G0(λ0;W))andlog(ˆG1(λ1;W))−log (G1(λ1;W)). Lemma B.4 is useful for bounding these two terms.
Lemma B.4. Letcbe a constant. For any x1,x2such that x1, x2≥c >0, we have
|log(x1)−log(x2)| ≤1
c|x1−x2| (19)
Proof. Without loss of generality, assume 0< c≤x1≤x2. We then have
log(x2)−log(x1) = log(x2
x1) = log(1 +x2
x1−1)≤x2
x1−1 =x2−x1
x1≤x2−x1
c.
Taking the absolute value of both the left-hand side and the right-hand side, we have
|log(x1)−log(x2)| ≤1
c|x1−x2|.
21Next, we introduce Lemma B.5 that bounds termslog(ˆG0(λ0;W))−log (G0(λ0;W))andlog(ˆG1(λ1;W))−log (G1(λ1;W)).
Lemma B.5. Letudenote the probability of treat, i.e., u=P(T= 1) . Assume that λ0, λ1∈Λ :=
[¯λ,¯λ]andˆτ(X)Yis bounded within¯Mand ¯M. Then for n≥max{2
u2log 2
δ
,2
(1−u)2log 2
δ
},
with probability 1−δ, we have
If¯M≤¯M≤0 :
log(ˆG0(λ0;W))−log (G0(λ0;W))≤2
exp(¯M/¯λ)(1−u)ˆG0(λ0;W)−G0(λ0;W);
log(ˆG1(λ1;W))−log (G1(λ1;W))≤2
exp(−¯M/¯λ)uˆG1(λ1;W)−G1(λ1;W).
If¯M≤0,¯M≥0 :
log(ˆG0(λ0;W))−log (G0(λ0;W))≤2
exp(¯M/¯λ)(1−u)ˆG0(λ0;W)−G0(λ0;W);
log(ˆG1(λ1;W))−log (G1(λ1;W))≤2
exp(−¯M/¯λ)uˆG1(λ1;W)−G1(λ1;W).
If0≤¯M≤¯M:
log(ˆG0(λ0;W))−log (G0(λ0;W))≤2
exp(¯M/¯λ)(1−u)ˆG0(λ0;W)−G0(λ0;W);
log(ˆG1(λ1;W))−log (G1(λ1;W))≤2
exp(−¯M/¯λ)uˆG1(λ1;W)−G1(λ1;W).(20)
Proof. First, we bound the termlog(ˆG0(λ0;W))−log (G0(λ0;W)).
G0(λ0;W)andˆG0(λ0;W)are greater than 0and bounded because Z= ˆτ(X)Yis bounded within
the range¯Mand¯M. Therefore, applying Lemma B.4, we have
log(ˆG0(λ0;W))−log (G0(λ0;W))≤1
cˆG0(λ0;W)−G0(λ0;W),
where c= min
inf
λ0∈Λ,W∈WˆG0(λ0;W), inf
λ0∈Λ,W∈WG0(λ0;W)
.
Moreover, for any λ0∈Λ, we have
If¯M≥0 : G0(λ0;W) =E[(1−T) exp( Z/λ 0)] =E[exp( Z/λ 0)|T= 0]P(T= 0)
≥E[exp(¯M/¯λ)|T= 0](1 −u) = exp(¯M/¯λ)(1−u);
ˆG0(λ0;W) =1
nnX
i=1(1−Ti) exp( Zi/λ0)
≥1
nnX
i=1(1−Ti) exp(¯M/¯λ) = exp(¯M/¯λ)(1−ˆu).(21)
If¯M≤0 : G0(λ0;W) =E[(1−T) exp( Z/λ 0)] =E[exp( Z/λ 0)|T= 0]P(T= 0)
≥E[exp(¯M/¯λ)|T= 0](1 −u) = exp(¯M/¯λ)(1−u);
ˆG0(λ0;W) =1
nnX
i=1(1−Ti) exp( Zi/λ0)
≥1
nnX
i=1(1−Ti) exp(¯M/¯λ) = exp(¯M/¯λ)(1−ˆu).(22)
Given ˆu=1
nPn
i=1Tiandu=E[1
nPn
i=1Ti], using Hoeffding’s inequality, we have
P 1
nnX
i=1(1−Ti)−E[1
nnX
i=1(1−Ti)]≥E[1
nPn
i=1(1−Ti)]
2!
≤2 exp
−2(1−u
2)2
n(1
n)2
≤δ.
22We can solve nby
2 exp
−n(1−u)2
2
≤δ⇒n≥2
(1−u)2log2
δ
.
This indicates that (1−ˆu)≥(1−u)/2with probability 1−δwhen n≥2
(1−u)2log 2
δ
. Combining
this with equations (21) and (22), with probability 1−δ, when n≥2
(1−u)2log 2
δ
, we have
If¯M≥0 : inf
λ0∈Λ,W∈WG0(λ0;W)≥exp(¯M/¯λ)(1−u);
inf
λ0∈Λ,W∈WˆG0(λ0;W)≥exp(¯M/¯λ)(1−ˆu)≥exp(¯M/¯λ)(1−u)/2.
If¯M≤0 : inf
λ0∈Λ,W∈WG0(λ0;W)≥exp(¯M/¯λ)(1−u);
inf
λ0∈Λ,W∈WˆG0(λ0;W)≥exp(¯M/¯λ)(1−ˆu)≥exp(¯M/¯λ)(1−u)/2.
Therefore, with probability 1−δ, when n≥2
(1−u)2log 2
δ
, we have
If¯M≥0 :
log(ˆG0(λ0;W))−log (G0(λ0;W))≤2
exp(¯M/¯λ)(1−u)ˆG0(λ0;W)−G0(λ0;W);
If¯M≤0 :
log(ˆG0(λ0;W))−log (G0(λ0;W))≤2
exp(¯M/¯λ)(1−u)ˆG0(λ0;W)−G0(λ0;W).
Next, we bound the termlog(ˆG1(λ1;W))−log (G1(λ1;W)).G1(λ1;W)andˆG1(λ1;W)are
greater than 0and bounded above. Therefore, applying Lemma B.4, we have
log(ˆG1(λ1;W))−log (G1(λ1;W))≤1
cˆG1(λ1;W)−G1(λ1;W),
where c= min
inf
λ1∈Λ,W∈WˆG1(λ1;W), inf
λ1∈Λ,W∈WG1(λ1;W)
.
Moreover, for any λ1∈Λ, we have
If¯M≥0 : G1(λ1;W) =E[Texp(−Z/λ 1)] =E[exp(−Z/λ 1)|T= 1]P(T= 1)
≥E[exp(−¯M/¯λ)|T= 1]u= exp( −¯M/¯λ)u;
ˆG1(λ1;W) =1
nnX
i=1Tiexp(−Zi/λ1)
≥1
nnX
i=1Tiexp(−¯M/¯λ) = exp( −¯M/¯λ)ˆu.(23)
If¯M≤0 : G1(λ1;W) =E[Texp(−Z/λ 1)] =E[exp(−Z/λ 1)|T= 1]P(T= 1)
≥E[exp(−¯M/¯λ)|T= 1]u= exp( −¯M/¯λ)u;
ˆG1(λ1;W) =1
nnX
i=1Tiexp(−Zi/λ1)
≥1
nnX
i=1Tiexp(−¯M/¯λ) = exp( −¯M/¯λ)ˆu.(24)
Given ˆu=1
nPn
i=1Tiandu=E[1
nPn
i=1Ti], using Hoeffding’s inequality, we have
P 1
nnX
i=1Ti−E[1
nnX
i=1Ti]≥E[1
nPn
i=1Ti]
2!
≤2 exp
−2(u
2)2
n(1
n)2
≤δ.
23We can solve nby
2 exp
−nu2
2
≤δ⇒n≥2
u2log2
δ
.
This indicates that hatu≥u/2with probability 1−δwhen n≥2
u2log 2
δ
. Combining this with
equations (23) and (24), with probability 1−δ, when n≥2
u2log 2
δ
, we have
If¯M≥0 : inf
λ1∈Λ,W∈WG1(λ1;W)≥exp(−¯M/¯λ)u;
inf
λ1∈Λ,W∈WˆG1(λ1;W)≥exp(−¯M/¯λ)ˆu≥exp(−¯M/¯λ)u/2.
If¯M≤0 : inf
λ1∈Λ,W∈WG1(λ1;W)≥exp(−¯M/¯λ)u;
inf
λ1∈Λ,W∈WˆG1(λ1;W)≥exp(−¯M/¯λ)ˆu≥exp( ¯M/¯λ)u/2.
Therefore, with probability 1−δ, when n≥2
u2log 2
δ
, we have
If¯M≥0 :
log(ˆG1(λ1;W))−log (G1(λ1;W))≤2
exp(−¯M/¯λ)uˆG1(λ1;W)−G1(λ1;W);
If¯M≤0 :
log(ˆG1(λ1;W))−log (G1(λ1;W))≤2
exp(−¯M/¯λ)uˆG1(λ1;W)−G1(λ1;W).
This completes the proof of Lemma B.5.
Additionally, the following Lemma B.6 provides the bound of |log(ˆu)−log(u)|.
Lemma B.6. Letˆu=1
nPn
i=1Tiandu=E[1
nPn
i=1Ti]. For n≥2
u2log 2
δ
, with probability
1−δ, we have
|log(ˆu)−log(u)| ≤ O
s
2 log(2
δ)
nu2
. (25)
Proof. Using Hoeffding’s inequality, we have
P(|ˆu−u| ≥ϵ) =P 1
nnX
i=1Ti−E[1
nnX
i=1Ti]≥ϵ!
≤2 exp 
−2nϵ2
,
2 exp 
−2nϵ2
≤δsolves ϵ≥s
log(2
δ)
2n.
Notably, using the results in the previous lemma, we know for n≥2
u2log 2
δ
,ˆu≥u/2. Therefore,
we have
|log(ˆu)−log(u)| ≤1
min{ˆu, u}|ˆu−u|.(By Lemma B.4)
≤2
u|ˆu−u| ≤2
uO
s
log(2
δ)
2n
=O
s
2 log(2
δ)
nu2
.
24In the following, we will bound the term |ˆV(ˆτ)−V(ˆτ)|using above lemmas. We first define functions
F0(λ0),ˆF0(λ0),F1(λ1), and ˆF1(λ1):
F0(λ0) =λ0ϵ0+λ0log(EPC[exp(ˆ τ(X)Y/λ 0)])
=λ0ϵ0+λ0log1
1−uE[(1−T) exp(ˆ τ(X)Y/λ 0)]
;
ˆF0(λ0) =λ0ϵ0+λ0log(1
ncnX
i=1(1−Ti) exp(ˆ τ(Xi)Yi/λ0))
=λ0ϵ0+λ0log 
1
n(1−ˆu)nX
i=1(1−Ti) exp(ˆ τ(Xi)Yi/λ0)!
.
F1(λ1) =λ1ϵ1+λ1log(EPT[exp(−ˆτ(X)Y/λ 1)])
=λ1ϵ1+λ1log1
uE[Texp(−ˆτ(X)Y/λ 1)]
;
ˆF1(λ1) =λ1ϵ1+λ1log(1
ntnX
i=1Tiexp(−ˆτ(Xi)Yi/λ1))
=λ1ϵ1+λ1log 
1
nˆunX
i=1Tiexp(−ˆτ(Xi)Yi/λ1)!
.
The following Lemma B.7 bounds the term |ˆF(λ)−F(λ)|.
Lemma B.7. Letu:=P(T= 1) . Assuming that 0<¯λ≤λ≤¯λandˆτ(X)Yis
bounded within the range of¯Mto¯M. Define Cexp=1{¯M≤¯M≤0}exp ¯M/¯λ−¯M/¯λ
+
1{¯M≤0,¯M≥0}exp ¯M/¯λ−¯M/¯λ
+1{0≤¯M≤¯M}exp ¯M/¯λ−¯M/¯λ
. Forn≥2/u2log(2/δ), with
probability 1−δ, we have
|ˆF0(λ0)−F0(λ0)| ≤ O
s
8λ2
0log2
δ
n(1−u)2C2exp
+O
s
2λ2
0log(2
δ)
n(1−u)2
;
|ˆF1(λ1)−F1(λ1)| ≤ O
s
8λ2
1log2
δ
nu2C2exp
+O
s
2λ2
1log(2
δ)
nu2
.(26)
Proof.
|ˆF0(λ0)−F0(λ0)|
=λ0 
log1
1−uE[(1−T) exp(ˆ τ(X)Y/λ 0)]
−log 
1
n(1−ˆu)nX
i=1(1−Ti) exp(ˆ τ(Xi)Yi/λ0)!!
=λ0log (E[(1−T) exp(ˆ τ(X)Y/λ 0)])−log 
1
nnX
i=1(1−Ti) exp(ˆ τ(Xi)Yi/λ0)!
+ log(1 −ˆu)−log(1−u)
≤λ0log (E[(1−T) exp(ˆ τ(X)Y/λ 0)])−log 
1
nnX
i=1(1−Ti) exp(ˆ τ(Xi)Yi/λ0)!+λ0|log(1−ˆu)−log(1−u)|.
25If¯M≤¯M≤0 :
|ˆF0(λ0)−F0(λ0)|
≤2λ0
exp(¯M/¯λ)(1−u)ˆG0(λ0;W)−G0(λ0;W)+λ0|log(1−ˆu)−log(1−u)|(By Lemma B.5)
≤ O
s
8λ2
0log2
δ
n(1−u)2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2λ2
0log(2
δ)
n(1−u)2
 (By Lemma B.3 and Lemma B.6)
If¯M≤0,¯M≥0 :
|ˆF0(λ0)−F0(λ0)|
≤2λ0
exp(¯M/¯λ)(1−u)ˆG0(λ0;W)−G0(λ0;W)+λ0|log(1−ˆu)−log(1−u)|(By Lemma B.5)
≤ O
s
8λ2
0log2
δ
n(1−u)2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2λ2
0log(2
δ)
n(1−u)2
 (By Lemma B.3 and Lemma B.6)
If0≤¯M≤¯M:
|ˆF0(λ0)−F0(λ0)|
≤2λ0
exp(¯M/¯λ)(1−u)ˆG0(λ0;W)−G0(λ0;W)+λ0|log(1−ˆu)−log(1−u)|(By Lemma B.5)
≤ O
s
8λ2
0log2
δ
n(1−u)2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2λ2
0log(2
δ)
n(1−u)2
 (By Lemma B.3 and Lemma B.6)
|ˆF1(λ1)−F1(λ1)|
=λ1 
log1
uE[Texp(−ˆτ(X)Y/λ 1)]
−log 
1
nˆunX
i=1Tiexp(ˆτ(Xi)Yi/λ0)!!
=λ1log (E[Texp(ˆτ(X)Y/λ 1)])−log 
1
nnX
i=1Tiexp(ˆτ(Xi)Yi/λ1)!
+ log(ˆ u)−log(u)
≤λ1log (E[Texp(ˆτ(X)Y/λ 1)])−log 
1
nnX
i=1Tiexp(ˆτ(Xi)Yi/λ1)!+λ1|log(ˆu)−log(u)|.
26If¯M≤¯M≤0 :
|ˆF1(λ1)−F1(λ1)|
≤2λ1
exp(−¯M/¯λ)uˆG0(λ0;W)−G0(λ0;W)+λ1|log(ˆu)−log(u)|(By Lemma B.5)
≤ O
s
8λ2
1log2
δ
nu2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2λ2
1log(2
δ)
nu2
 (By Lemma B.3 and Lemma B.6)
If¯M≤0,¯M≥0 :
|ˆF1(λ1)−F1(λ1)|
≤2λ1
exp(−¯M/¯λ)uˆG1(λ1;W)−G1(λ1;W)+λ1|log(ˆu)−log(u)|(By Lemma B.5)
≤ O
s
8λ2
1log2
δ
nu2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2λ2
1log(2
δ)
nu2
 (By Lemma B.3 and Lemma B.6)
If0≤¯M≤¯M:
|ˆF1(λ1)−F1(λ1)|
≤2λ1
exp(−¯M/¯λ)uˆG1(λ1;W)−G1(λ1;W)+λ1|log(ˆu)−log(u)|(By Lemma B.5)
≤ O
s
8λ2
1log2
δ
nu2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2λ2
1log(2
δ)
nu2
 (By Lemma B.3 and Lemma B.6)
Now, we can prove the result in Theorem 4.5.
Proof. Letˆλ0= arg min λˆF0(λ0),λ∗
0= arg min λ0F0(λ0),ˆλ1= arg min λˆF1(λ1)andλ∗
1=
arg min λ1F1(λ1). Then we have
V0(ˆτ)−ˆV0(ˆτ) =F0(λ∗
0)−ˆF0(ˆλ0)
=F0(λ∗
0)−ˆF0(ˆλ0) +F0(ˆλ0)−F0(ˆλ0)
=F0(ˆλ0)−ˆF0(ˆλ0) +F0(λ∗
0)−F0(ˆλ0)
≤ |F0(ˆλ0)−ˆF0(ˆλ0)|+ 0
≤sup
λ0|F0(λ0)−ˆF0(λ0)|.
ˆV0(ˆτ)− V0(ˆτ) =ˆF0(ˆλ0)−F0(λ∗
0)
=ˆF0(ˆλ0)−F0(λ∗
0) +ˆF0(λ∗
0)−ˆF0(λ∗
0)
=ˆF0(λ∗
0)−F0(λ∗
0) +ˆF0(ˆλ0)−ˆF0(λ∗
0)
≤ |ˆF0(λ∗
0)−F0(λ∗
0)|+ 0
≤sup
λ0|ˆF0(λ0)−F0(λ0)|.
V1(ˆτ)−ˆV1(ˆτ) =F1(λ∗
1)−ˆF1(ˆλ1)
=F1(λ∗
1)−ˆF1(ˆλ1) +F1(ˆλ1)−F1(ˆλ1)
=F1(ˆλ1)−ˆF1(ˆλ1) +F1(λ∗
1)−F1(ˆλ1)
≤ |F1(ˆλ1)−ˆF1(ˆλ1)|+ 0
≤sup
λ1|F1(λ1)−ˆF1(λ1)|.
27ˆV1(ˆτ)− V1(ˆτ) =ˆF1(ˆλ1)−F1(λ∗
1)
=ˆF1(ˆλ1)−F1(λ∗
1) +ˆF1(λ∗
1)−ˆF1(λ∗
1)
=ˆF1(λ∗
1)−F1(λ∗
1) +ˆF1(ˆλ1)−ˆF1(λ∗
1)
≤ |ˆF1(λ∗
1)−F1(λ∗
1)|+ 0
≤sup
λ1|ˆF1(λ1)−F1(λ1)|.
Therefore, we have
If¯M≤¯M≤0 :
|ˆV0(ˆτ)− V0(ˆτ)| ≤sup
λ|ˆF(λ)−F(λ)| ≤ O
s
8¯λ2log2
δ
n(1−u)2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2¯λ2log(2
δ)
n(1−u)2
;
|ˆV1(ˆτ)− V1(ˆτ)| ≤sup
λ|ˆF(λ)−F(λ)| ≤ O
s
8¯λ2log2
δ
nu2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2¯λ2log(2
δ)
nu2
.
If¯M≤0,¯M≥0 :
|ˆV0(ˆτ)− V0(ˆτ)| ≤sup
λ|ˆF(λ)−F(λ)| ≤ O
s
8¯λ2log2
δ
n(1−u)2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2¯λ2log(2
δ)
n(1−u)2
;
|ˆV1(ˆτ)− V1(ˆτ)| ≤sup
λ|ˆF(λ)−F(λ)| ≤ O
s
8¯λ2log2
δ
nu2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2¯λ2log(2
δ)
nu2
.
If0≤¯M≤¯M:
|ˆV0(ˆτ)− V0(ˆτ)| ≤sup
λ|ˆF(λ)−F(λ)| ≤ O
s
8¯λ2log2
δ
n(1−u)2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2¯λ2log(2
δ)
n(1−u)2
;
|ˆV1(ˆτ)− V1(ˆτ)| ≤sup
λ|ˆF(λ)−F(λ)| ≤ O
s
8¯λ2log2
δ
nu2 
exp ¯M/¯λ−¯M/¯λ2
+O
s
2¯λ2log(2
δ)
nu2
.
Finally, we have
|ˆVt(ˆτ)− Vt(ˆτ)| ≤ O
s
8¯λ2log2
δ
nu2
tC2exp
+O
s
2¯λ2log(2
δ)
nu2
t
.
Note that u1=P(T= 1) andu0=P(T= 0) .Cexp=1{¯M≤¯M≤0}exp ¯M/¯λ−¯M/¯λ
+
1{¯M≤0,¯M≥0}exp ¯M/¯λ−¯M/¯λ
+1{0≤¯M≤¯M}exp ¯M/¯λ−¯M/¯λ
.
C Additional Materials
C.1 Additional Explanations
Q1. Why DRM can select CATE estimators that are robust to the uncertainty in PEHE caused
by selection bias and unobserved confounders? In Section 3.1 and Section 4.1, we have presented
theoretical explanations for the reason why DRM can measure a CATE estimator’s robustness against
selection bias and unobserved confounding. Below we will explain it more specifically.
In causal inference, all the CATE estimators are constructed on the observational factual data.
But how reliable the CATE estimator that learned on factual data is? This question can be never
known unless we have the knowledge of the oracle PEHE. As shown in equation (6), we know
that the PEHE is equal to two ˆτ-dependent terms, E[ˆτ(X)Yt|T=t]andE[ˆτ(X)Yt|T= 1−t].
28Unfortunately, E[ˆτ(X)Yt|T= 1−t]is uncomputable empirically because we can only observe
the factual distribution PF=P(X, Yt|T=t)but not the counterfactual distribution PCF=
P(X, Yt|T= 1−t). The unobserved counterfactual distribution can be regarded as an uncertain
distribution varying around the observed and certain factual distribution PF. If we could assume a
"God’s perspective" and observe PCFdirectly, the counterfactual distribution will be certain - like a
quantum world! Such an uncertainty in PCFresults in the uncertainty in PEHE. Now we will analyze
the source of such uncertainty by analyzing the relationship between the uncertain distribution PCF
and the certain distribution PFbased on equation (2):
P(X, Yt|T= 1−t) =P(X, Yt|T=t)P(Yt|T= 1−t, X)
P(Yt|T=t, X)P(X|T= 1−t)
P(X|T=t).
From above, we find the unobservable distribution P(X, Yt|T= 1−t)is equal to the observ-
able distribution P(X, Yt|T=t)multiplied withP(Yt|T=1−t,X)
P(Yt|T=t,X)P(X|T=1−t)
P(X|T=t). In other words,
p(yt|T=1−t,x)
p(yt|T=t,x)p(x|T=1−t)
p(x|T=t)controls the discrepancy between PFandPCF. Note that if there is no
unmeasured confounders, then we have p(yt|T= 1−t, x) =p(yt|T=t, x); and if there is no
selection bias (covariate shift), then we have p(x|T= 1−t) =p(x|T=t). Now we understand
the root cause of the discrepancy between PFandPCF(or between E[τ(X)Yt|T= 1−t]and
E[τ(X)Yt|T=t]) lies at the unobserved confounders and selection bias (covariate shift). In the
DRM method, the uncertainty caused by potential unobserved confounders and selection bias in
PEHE can be further measured as the distributionally robust values ˆV1andˆV0. Then the PEHE
w.r.t. the CATE estimator ˆτwill be at most RDRM(ˆτ), as shown in equation (15). An estimator ˆτ
that attains smallest RDRM(ˆτ)by definition reflects the distributional robustness against potential
unobserved confounders and selection bias.
Q2. How to set ϵ∗when there are unobserved confounders? When unobserved confounders are
present, Proposition 3.6 can also provide guidance for setting ϵ∗. Taking ϵ∗
1=DKL(PC||PT)as an
example, we have
DKL(PC||PT)
=Z
XZ
Y0Z
Y1p(y0, y1|x, T= 0)p(x|T= 0) logp(y0, y1|x, T= 0)p(x|T= 0)
p(y0, y1|x, T= 1)p(x|T= 1)dy1dy0dx
=Z
XZ
Y0Z
Y1p(y0, y1|x, T= 0)dy1dy0
p(x|T= 0) logp(x|T= 0)
p(x|T= 1)dx
+Z
XZ
Y0Z
Y1p(y0, y1|x, T= 0)p(x|T= 0) logp(y0, y1|x, T= 0)
p(y0, y1|x, T= 1)dy1dy0dx
=DKL(P(X|T= 0)||P(X|T= 1))
+Z
XZ
Y0Z
Y1p(y0, y1|x, T= 0)p(x|T= 0) logp(y0, y1|x, T= 0)
p(y0, y1|x, T= 1)dy1dy0dx
> D KL(PC
X||PT
X)
Therefore, when unobserved confounders present, we can set ϵ∗to a larger value than the one guided
by Proposition 4.6. Simultaneously, as the empirical approximation of ϵ∗
1=DKL(PC
X||PT
X)and
ϵ∗
0=DKL(PT
X||PC
X)can be biased, we also suggest set ϵ∗to a large value than the empirically-
computed ones to ensure the ambiguity set is large enough to contain the target distribution. Therefore,
we generally set ϵ∗
1=DKL(PC
X||PT
X) + 5.2andϵ∗
0=DKL(PT
X||PC
X) + 5.2for all settings in our
experiment. Theoretically, a larger ϵ∗should guarantee the DRM-selected estimator to be more
robust, as it allows for a broader range of possible counterfactual distributions in the ambiguity set.
However, setting ϵ∗too large can result in overly conservative estimator selection (similar to the
well-known accuracy-robustness tradeoff). Therefore, how to determine a proper ambiguity radius
still remains an open challenge in both our work and distributionally robust optimization literature.
C.2 Hyperparameters
•For linear model, we use LogisticRegressionCV and RidgeCV (both are with 3-fold cross-
validation) from sklearn package to tune hyperparameters: Logistic regression: Cs∈{0.01,
0.1, 1, 10}; Ridge Regression: α∈{0.01, 0.1, 1, 10, 100}.
29•For Neural Net, we set the hidden layers as [200, 200, 200, 100, 100], each with the ReLU
activation function. The model is trained using the Adam optimizer with a learning rate of
0.001, a batch size of 64, and 300 epochs.
•For RF, XGBoost, and SVM model, we use AutoML [ 71,53] (with 3-fold cross-validation)
from flaml package to tune hyperparameters.
C.3 The Complementary Results
First, we would like to emphasize that the experimental results in this final version of the paper
differ from those in the original version. We made several revisions based on the feedback from
anonymous reviewers: 1) We add Neural Net model to the base ML models and add the U-learner
to the meta-learners, increasing the number of CATE estimators from 24 to 36; 2) We adopted
AutoML for hyperparameter tuning when training SVM, RF, and XGBoost. All code is available at
https://github.com/yiyhuang3/CATE_estimator_selection .
Below, we prsent the complementary PEHE results for 36 candidate CATE estimators, where the
candidate pool contains 4 ML models (LR, SVM, RF, and Neural Net) ×9 learners (U-, S-, T-, PS-,
IPW-, X-, DR-, R-, RA-).
Table 3: Comparison of PEHE for different selectors across Settings A, B, and C (Note that B ( ξ= 1)
matches A ( ρ= 0.1)), with base model for CATE estimator being {LR, SVM, RF, Net}. Reported
values (mean ±standard deviation) are computed over 100 experiments. Smaller is better.
A (ρ= 0) A ( ρ= 0.1) A ( ρ= 0.3) B ( ξ= 0) B ( ξ= 2) C ( m= 0.1) C ( m= 0.5) C ( m= 0.9)
Plug-U 49.59 ±95.07 41.93±61.07 36.16±61.77 2.28±2.32 155.24 ±291.78 42.45±52.65 59.51±210.54 24.37±26.51
Plug-S 5.10 ±8.29 5.36±5.84 6.29±5.76 1.99±1.41 9.18±11.57 5.76±5.46 8.78±7.55 13.45±9.53
Plug-PS 4.80 ±7.74 5.36±5.85 6.28±5.75 1.99±1.41 9.17±11.58 5.76±5.46 8.58±7.40 13.45±9.53
Plug-T 60.84 ±22.03 59.09±22.88 59.39±21.34 12.25±10.80 68.22±18.14 62.90±19.13 48.32±23.60 45.07±20.63
Plug-X 9.82 ±10.67 10.39±12.20 9.81±11.30 6.52±10.90 14.82±14.23 10.82±15.24 15.80±15.03 20.59±13.03
Plug-IPW 35.09 ±28.69 38.50±27.78 39.29±27.48 6.19±7.39 61.90±24.17 41.47±31.37 30.02±22.57 29.33±20.45
Plug-DR 44.83 ±26.77 46.47±27.02 48.23±26.56 5.98±8.09 67.87±18.94 49.61±33.34 33.69±23.05 32.39±19.28
Plug-R 3.64 ±5.01 5.33±15.72 5.51±3.75 2.19±2.31 13.04±31.51 4.95±5.38 7.56±7.88 10.91±7.92
Plug-RA 58.32 ±24.02 60.40±20.13 58.63±22.86 8.37±9.28 67.77±17.82 58.91±19.66 45.52±24.80 42.13±20.35
Pseudo-DR 63.07 ±22.54 63.80±20.22 63.10±19.41 16.51±23.05 73.29±17.48 65.12±20.14 53.87±26.16 53.79±24.91
Pseudo-R 11.57 ±27.25 16.83±45.81 9.97±21.23 6.49±20.46 18.13±30.61 13.62±24.78 20.96±30.42 30.05±32.02
Pseudo-IF 66.26 ±15.20 65.21±16.35 66.72±15.84 28.49±23.55 69.01±16.57 63.09±20.62 60.00±19.18 47.40±20.16
Random 7216 ±22745 6514±21650 4200±17048 1136±5595 7552±22498 3771±16625 6219±19942 3453±14590
Fact 52.81 ±18.01 53.58±19.42 55.05±21.10 16.09±16.50 68.50±27.69 51.96±17.45 52.44±22.51 49.16±24.47
Matching 62.57 ±21.57 64.90±17.85 63.94±18.72 15.10±22.93 72.25±17.46 64.56±19.59 57.87±24.40 48.81±25.23
DRM 2.68±4.73 3.55±5.65 5.28±6.37 2.14±1.70 18.77±112.78 4.60±9.58 6.44±9.73 10.05±7.19
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: [Yes]
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: [Yes]
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
31Justification: [Yes]
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: [Yes]
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
32Answer: [Yes]
Justification: [Yes]
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: [Yes]
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: [Yes]
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
33•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: [Yes]
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: [Yes]
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
34generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
35Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
36