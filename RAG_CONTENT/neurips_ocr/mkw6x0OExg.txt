Explanations that reveal all through the
definition of encoding
Aahlad Puli∗, Nhi Nguyen∗, Rajesh Ranganath
New York University
Abstract
Feature attributions attempt to highlight what inputs drive predictive power. Good
attributions or explanations are thus those that produce inputs that retain this pre-
dictive power; accordingly, evaluations of explanations score their quality of pre-
diction. However, evaluations produce scores better than what appears possible
from the values in the explanation for a class of explanations, called encoding ex-
planations. Probing for encoding remains a challenge because there is no general
characterization of what gives the extra predictive power. We develop a definition
of encoding that identifies this extra predictive power via conditional dependence
and show that the definition fits existing examples of encoding. This definition im-
plies, in contrast to encoding explanations, that non-encoding explanations contain
all the informative inputs used to produce the explanation, giving them a "what
you see is what you get" property, which makes them transparent and simple to
use. Next, we prove that existing scores ( ROAR ,FRESH ,EVAL -X) do not rank non-
encoding explanations above encoding ones, and develop STRIPE -Xwhich ranks
them correctly. After empirically demonstrating the theoretical insights, we use
STRIPE -Xto show that despite prompting an LLM to produce non-encoding expla-
nations for a sentiment analysis task, the LLM-generated explanations encode.
1 Introduction
Artificial intelligence can unlock information in data that was previously unknown. In medicine, for
example, using AI, researchers have shown that electrocardiograms are predictive of structural heart
conditions [1] or new-onset diabetes [2]. Good predictions often lead one to ask what in the input
is important for a prediction; this question is a driving factor behind research in interpretability
and explainability [3, 4]. One primary direction in interpretability seeks to produce explanations
that are subsets of the input that retain the predictability of the label. These types of explanations
and interpretations are called feature attributions and have been used to find factors associated with
debt defaults [5], to demonstrate that detecting COVID-19 from chest radiographs can rely on non-
physiological signals [6], and to discover a new class of antibiotics [7].
Several methods exist for producing feature attributions or explanations. While some methods com-
pute functions of model gradients [8] or look at predictability after removing features [9], other
methods attribute scores to different inputs by treating them as players in a game [4, 10] or amor-
tize their explanations by learning a single model to select subsets for each instance [11]. Choos-
ing one from the many feature attribution methods requires an evaluation. There are, however,
many approaches to evaluation itself: qualitative ones [12, 13, 14], which are limited to cases
where humans have precise knowledge about the inputs relevant to prediction, and quantitative
ones [2, 4, 15, 16, 17, 18, 19, 20], which do not require human knowledge.
Intuitively, a good evaluation method for feature attributions should assign higher scores to
explanations that select inputs that are more predictive of the label. However, evaluations that
score explanations based on the predictability of the label from the explanation face one major
challenge: encoding . Informally, an encoding explanation is one where the explanation predicts
the label beyond what seems plausible from the values of the inputs themselves. The top left panel
of Figure 1 shows an explanation that predicts the label of dog or cat depending on whether the
∗Equal contribution
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Figure 1: Overview of the paper. Explanations are produced to find inputs that are relevant to predicting
a label. However, explanations can predict the label well due to the selection being predictive of the label
beyond the explanation’s values. Such explanations are called encoding. In contrast, predicting instead from
a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations
are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing
evaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE -X.
explanation is a pixel on the right half or left half of the image respectively. Many explanation
methods fit the description of encoding [20, 21]. Further, given that many evaluations only look
at the quality of prediction, encoding can go undetected, rendering the evaluations ineffective at
picking explanations. In contrast, non-encoding explanations predict the label well only when the
values in the explanation do, making them easy to reason about.
In addressing encoding, this work makes the following contributions:
•Develops a simple statistical definition of encoding via a conditional dependence property.
• Confirms the introduced definition captures all existing ad hoc encoding instances.
• Shows that non-encoding explanations are easy to use because they retain all the predictive
inputs used to build them, meaning that predictive non-encoding explanations reveal inputs that
predict the label to their users, and thus have a "what you see is what you get" property .
• Formalizes evaluations’ sensitivity to encoding as weak detection (optimal scoring explanations
are non-encoding) and strong detection (non-encoding explanations score above encoding ones).
• Demonstrates that the evaluations ROAR [19] and FRESH [18] do not weakly detect encoding.
• Proves that EVAL -X[20] weakly detects encoding, but does not strongly detect encoding.
• Develops STRIPE -Xand proves that it strongly detects encoding .
• Uses STRIPE -Xto show that despite prompting an LLM to produce non-encoding explanations
for a sentiment analysis task, the LLM-generated explanations encode.
Figure 1 provides an overview of this paper.
2 Evaluating explanations
We focus on explanation methods where the goal is to produce subsets of the input that predict the
label [22, 23]. Explanation methods of this form, also called feature attributions, saliency methods
[4, 8, 24], or just "explanations," include thresholded rankings from Shapley values [25, 26], LIME
[24], and REAL -X[20]. With yas the label and x∈Rdas the inputs, let q(y,x)be the joint
distribution over them. An explanation method emaps the inputs xto a binary selection mask e(x)
over the inputs: e:Rd→ {0,1}d. The explanation xe(x)is a pair: the selection e(x)and the vector
of explanation’s values . For example, if x= [a, b, c ]is three-dimensional and e(x) = [0 ,1,1],
xe(x)consists of the binary mask e(x)and the values associated with the inputs that correspond to
the indices in e(x)with value 1:
xe(x)= (e(x),[b, c]).
2We keep track of the indices because the same value can lead to different predictions depending on
the index it appears at; for example, in predicting mortality from patient vital signs, a heart rate above
110can occur in healthy patients but a temperature of 110◦F is almost always fatal. Equivalently,
like in existing work [15, 16, 17, 20], one can choose xe(x)to retain the values in the explanation in
the same position and mask out those not selected: xe(x)=e(x)×x+ (1−e(x))×mask-token .
For concision, we overload the word "explanation" to mean the explanation method instead of the
random variable xe(x)when it is clear from context.
Choosing between explanation methods requires evaluation. Explanation methods seek to return
inputs that predict the label, so existing evaluations consider how well the explanation xe(x)predicts
the label y[15, 16, 17, 20]. To score explanations based on predictive power, an evaluation method
α(·)takes as arguments both the explanation e(x)and the joint distribution q(y,x):α(q, e). Without
loss of generality let higher be better.
2.1 Encoding: A disconnect between the predictiveness of explanations and the
predictiveness of their values
We give a simple example of encoding to build intuition for the disconnect between predicting the
label from the explanation and predicting the label from the explanation’s values. Imagine that the
goal is to explain which set of vital signs signal bacterial pneumonia as the diagnosis compared to
the common cold. Consider the explanation method that selects the patient’s height when the true
probability of pneumonia is high given the whole set of observables (including labs, symptoms,
and vital signs) and otherwise selects the patient’s hair color. Physiologically, height and hair color
do not indicate that the patient has pneumonia, meaning that this explanation should not be highly
predictive of the label. However, by construction, pneumonia is likely exactly when the explanation
selects height, and predicting the label from the explanation achieves the same accuracy as predicting
with the full conditional arg maxy∈{pneumonia, cold }q(y=y|x). Thus, despite the explanation
method only selecting physiologically irrelevant inputs, the explanation predicts the label well.
Encoding examples such as the one above are neither contrived nor unique. For example, Jethani
et al. [20] show that certain procedures that learn to explain, when applied to MNIST digit classifi-
cation, yield explanations that select a background, black pixel that predicts the label at an accuracy
>90%; (see Figure 1 in [20]). Other examples of encoding explanations that predict better than
what is expected from the explanation’s values exist [20, 21]. Encoding explanations should not
score optimally under a good evaluation because the explanation selects inputs that do not appear to
predict the label. However, without a general characterization of the discrepancy in predictive power
for encoding, finding explanations whose values predict well remains a challenge. The next section
develops a definition of encoding.
3 Formalizing encoding
Intuitively, encoding is a phenomenon where the information about the label in the explanation xe(x)
exceeds what is known from the explanation’s values . As the input xdetermines the explanation
xe(x), the quality of predicting the label yfrom the explanation relies on the information about the
label transmitted from xtoxe(x). There are two pathways for this transmission; we elaborate below.
Denoting the values in a subset vbyxv, compare the event this subset takes the values a, i.e.
xv=ato the event that the explanation’s selection is vand that the explanation’s values are a, i.e.,
xe(x)= (v,a).
1. Knowing that the explanation is xe(x)= (v,a)implies not only that the values in the explana-
tion are determined as xv=a, but also that the selection is determined as e(x) =v.
2. In reverse, knowing that the values of a subset of inputs are xv=aand knowing the selection
e(x) =vimplies that the explanation are xe(x)= (v,a).
Putting these two points together yields an equality between events:
{x:xe(x)= (v,a)}={x:e(x) =v} ∩ {x:xv=a}. (1)
Thus, the two pathways for information between xand the explanation xe(x)are the selection e(x)
and explanation’s values xv; see Figure 2. Existing work makes similar intuitive observations but
stops short of formalizing the additional predictive power in an explanation xe(x)[20, 21].
3y x
xe(x)e(x) xv
Figure 2: Intuition for
encoding: There are two
ways the information in
the inputs xabout the la-
belyis transmitted to the
explanation xe(x): (1)
through the values in the
explanation and (2) the
selection e(x)(in red).
When the latter happens,
the explanation is said to
beencoding .To formalize this extra predictive power, define the explanation indicator
Ev= 1[e(x) =v]. A little algebra in Appendix A.1 shows the explana-
tion indicator Evprovides the extra information:
q(y|xe(x)= (v,a)) =q(y|xv=a,Ev= 1|{z}
extra information in xe(x))̸=q(y|xv=a).
Building on this insight, we define encoding as a conditional dependence :
Definition 1 (Encoding ).The explanation e(x)is encoding if there exists
anSwhere q(xe(x)∈S)>0such that for every (v,a)∈S:
y
̸|=Ev|xv=a. (2)
An example mathematical construction of an encoding explanation is pro-
vided in Appendix B.1. The dependence in Def: Encoding means that
for encoding explanations, there is a disconnect between how well the ex-
planation xe(x)predicts the label versus only the explanation’s values xv.
This disconnect means that evaluations that score explanations based on
predictions from the explanation or their transformations [15, 16, 17, 20]
can favor explanation methods that select inputs whose values have little
relevance to predicting the label.
Beyond the disconnect in prediction, encoding explanations are undesir-
able as they conceal predictive inputs that nevertheless affect the explanation. This concealment can
lead to incorrect conclusions, such as that inputs outside the selection are irrelevant, or bewilderment
because predictive inputs outside the explanation drive changes in the selection in ways that cannot
be understood from the explanation itself. An example is provided in Appendix B.2.
Non-encoding explanations. Conversely, for a non-encoding explanation, there exists no positive
measure set of explanations xe(x), where the explanation indicator has conditional dependence given
the explanation’s values. That is, for a set Awhere q(xe(x)∈A) = 1 , then for all (v,a)∈A
y
|=Ev|xv=a
which in turn guarantees
q(y|xe(x)= (v,a)) =q(y|xv=a,Ev= 1) = q(y|xv=a).
Appendix A.2 shows this. A simple example of a non-encoding explanation is a constant explanation
that always picks the same subset of inputs, since a constant Evis independent of any variable.
The information for predicting the label in a non-encoding explanation lives in the explanation’s
values. Evaluations based on predictions from the explanation xe(x)of non-encoding explanations
will yield explanations where the input values xvpredict the label. In other words, non-encoding
explanations reveal all the informative inputs they depend on, and "what you see is what you get"
in the explanation.
3.1 Encoding explanations in the wild
Def: Encoding encompasses examples in the existing literature beyond the example in Section 2.1.
In that example, the information about ylies in the positions in the selection e(x), which motivates
the name position-based encoding ( POSI ). This section describes two other informal examples from
the literature of encoding explanations, prediction-based encoding ( PRED ) [21] and marginal encod-
ing ( MARG ) [20], and explains the intuition behind why they encode. In the appendix, we develop
formalizations of these types of encoding and show that these formulations meet Def: Encoding.
Prediction-based encoding ( PRED ).To understand how prediction-based encoding occurs, con-
sider the task of sentiment analysis from movie reviews. Assume that reviews can either be of type
"My day was terrible, but the movie was [ADJ1]." and "The movie was [ADJ2], but the day was not
great." where adjective ADJ1 can be "good" or "not great" and adjective ADJ2 can be "not great" or
"terrible". Due to common English parlance, "terrible" indicates bad sentiment more often than "not
great". Then, in the example setup above, only seeing that the fourth word is "terrible" yields bad
sentiment with higher probability than when only seeing that the phrase is "not great". However, the
fourth word does not always describe the movie. An explanation can look at "not great" describing
the movie as bad but then selects "terrible" to encode the bad sentiment. This explanation encodes
because the selected word may not describe the movie but the selection predicts the sentiment.
4cat dog
xe(x)e(x)
xe(x)e(x)
Figure 3: Left: Consider data where the color in the
left half determines whether the label "cat", "dog") is
produced from the top or bottom image on the right.
Right: AMARG encoding explanation that produces
only the top or the bottom animal image based on the
color. The animal image alone says less about the label
than knowing the animal image and the color. Knowing
the selection determines the color and thus provides ad-
ditional information about the label.Marginal encoding ( MARG ).This type of
encoding occurs when some inputs determine
which other inputs determine the label. For
example, in Figure 3, the color determines
whether the top right patch produces the label
or the bottom right patch. Inputs that control
where the label comes from are named control
flow inputs . For a real-world example, consider
the following example from Jethani et al. [20],
where the goal is to predict mortality for pa-
tients with chest pain. A lab value that checks
for heart injury and acts like a control flow input
is troponin. Abnormal troponin indicates that
cardiac issues exist and cardiac imaging would
inform mortality. Normal troponin on the other
hand can indicate that chest pain is unrelated to
cardiac health and a chest X-ray would instead inform mortality. Selecting one image or the other,
but not the control flow input, conceals information about why the image was relevant to the label.
Formalization. In Appendix B, we provide mathematical formulations of each informal example
and show that they fall under the definition of encoding in Def: Encoding: position-based encoding
(Appendix B.3), prediction-based encoding (Appendix B.4), and marginal encoding (Appendix B.5).
The key intuition behind all of these is that the explanation e(x)varies with inputs other than the
selected ones, and these additional inputs provide information about the label beyond the selected
ones. Next, we turn to detecting encoding via quantitative evaluations.
4 Detecting encoding in explanations
This section develops notions of sensitivity to encoding for evaluation methods, and uses the math-
ematical definition of encoding developed in the previous section to establish which methods detect
encoding and which do not. Hsia et al. [21] suggest that evaluation methods like EVAL -Xcan be
gamed to produce high scores for encoding explanations by optimizing the evaluation. To study this
case, we introduce the notion of weak detection. If the optimal score of an evaluation of explanations
does not permit encoding, then that evaluation is said to weakly detect encoding:
Definition 2 (Weak detection of encoding ).An evaluation α(q, e)of explanations weakly detects
encoding if the optimal explanations e∗, i.e. α(q, e∗) = max eα(q, e), are non-encoding.
Weak detection provides a recipe for finding non-encoding explanations: find the explanation that
achieves the maximum score of a weak detector. However, such a recipe would only work when op-
timizing without constraints because weak detection does not require non-encoding explanations to
have a better score than any encoding one. Requiring this leads to the definition of strong detection .
Definition 3 (Strong detection of encoding ).An evaluation α(q, e)strongly detects encoding if for
any encoding explanation eand non-encoding explanation e′,α(q, e′)> α(q, e).
Evaluations that are not weak detectors cannot be strong detectors because they score some encoding
explanation optimally.
4.1 Do existing evaluation methods detect encoding?
Here, we consider whether several techniques for evaluating explanations: ROAR [19], FRESH [18],
and EVAL -X[20] can detect encoding. We analyze these evaluations on the following distribution q
x= [x1,x2,x3]∼ B(0.5)⊗3, y=x1w.p.0.9else 1−x1ifx3= 1,
x2w.p.0.9else 1−x2ifx3= 0.(3)
Consider the explanation eencode(x) =ξ1= [1,0,0]ifx3= 1 andξ2= [0,1,0]otherwise; this
encodes because x3is used to create the explanation and x3predicts the label conditional on x1
when Eξ1= 1. This is a MARG explanation (see Section 3.1).
5ROAR and FRESH do not weakly detect encoding. ROAR evaluates explanations by predict-
ing the label from the inputs not selected by the explanation, denoted as x−e(x);ROAR scores
explanations optimally if the predictions from the remaining covariates are as random as pre-
dicting without any covariates at all. In other words, ROAR checks how informative x−e(x)is
ofyand provides the highest score when y
|=x−e(x). In contrast, FRESH evaluates explana-
tions by predicting the label from the explanation after removing all other inputs, denoted as
val(xe(x)). For example, assume we are given an input x="Visually stunning. My fa-
vorite movie ever" and an explanation e(x)that selects the words "stunning" and "favorite".
Then, the explanation is xe(x)= ([0 ,1,0,1,0,0],["stunning" ,"favorite" ]), whereas val(xe(x)) =
["stunning" ,"favorite" ,pad-token ,pad-token ,pad-token ,pad-token ], which drops the infor-
mation about where the selected words are in the input. See Appendix B.6 for a formal definition of
val(xe(x)).FRESH checks how predictive q(y|val(xe(x)))is and assigns an optimal score if the
prediction is as good as that of q(y|x). These conditions hold for eencode(x)in eq. (3):
Proposition 1. For the data generating process ( DGP) in eq. (3),ROAR and FRESH assign their
respective optimal scores to the encoding explanation eencode(x).
The proof is in Appendix B.6. The intuition is that the encoding explanation eencode(x)always se-
lects the input that informs the label given the control flow x3; removing the only conditionally
informative input means that x−eencode(x)has no information about y. In turn, ROAR scores an encod-
ing explanation x−eencode(x)optimally, meaning it does not even weakly detect encoding. In addition,
val(xe(x))provides the exact same information about the label regardless of which position it came
from. As a result, x
|=y|val(xe(x)), so FRESH scores eencode(x)optimally. Even though FRESH at-
tempts to drop the information about the selection v=e(x)during evaluation, val(xe(x))remains
a function of xe(x)= (v,a), so extra information can still be transmitted through the selection v.
Thus, ROAR and FRESH are not weak detectors of encoding.
EVAL -Xweakly detects encoding but not strongly. EVAL -X[10, 26] is an evaluation method and
is sometimes called the surrogate model score. The EVAL -Xscore with log-probabilities is
EVAL -X(q, e) :=E(v,a)∼q(xe(x))Eq(y|xe(x)=(v,a))[logq(y|xv=a)]. (4)
This score measures the expected log-likelihood of the labels given the input values chosen by the
explanation method eand is grounded in the sampling distribution q. Log-likelihoods are maximized
by matching the true distribution, this leads to EVAL -X’s weak detection:
Theorem 1. Ife(x)isEVAL -Xoptimal, then e(x)is not encoding.
Appendix A.4 gives a proof. The proof shows that at optimality, the prediction from the values of
explanation has to match the prediction from the full inputs. In turn, given the values there is no
additional information in xabout y, which means the explanation indicator Evis independent of y;
this violates Def: Encoding, which proves the non-encoding nature of EVAL -X-optimal explanations.
To test strong detection for EVAL -X, we consider explanations constrained to select one input. Such
reductive constraints appear in practice because the goal of producing an explanation is often to aid
humans who benefit from reduced complexity. Such constraints prohibit explanations from reaching
EVAL -X’s optimal score. Compare eencode (x)with a non-encoding constant explanation:
Proposition 2. Letec(x) =ξ3. Then, for the DGP in eq. (3),EVAL -X(q, e encode)>EVAL -X(q, ec).
Thus, EVAL -Xis not a strong detector. The intuition is that the first two coordinates x1,x2predict
the label when selected by eencode , while the control flow feature does not predict the label. EVAL -X
not being a strong detector means that optimizing EVAL -Xover a reductive set may yield an encoding
explanation. In this case, eencode is one of the EVAL -X-optimal reductive explanations (Lemma 6).
4.2 STRIPE -X: a strong detector of encoding
Encoding explanations induce the dependence between the label yand the identity of the selection
Ev= 1[e(x) =v]given the values in the explanation xv(Def: Encoding). This dependence can
be tested for by building on conditional independence tests [27, 28, 29]. Rather than testing, direct
quantification of dependence can be useful for when combining with other scores, which can be
done using instantaneous conditional mutual information:
ϕq(e) :=E(v,a)∼q(xe(x))I(Ev;y|xv=a) ( ENCODE -METER ). (5)
6ENCODE -METER is0only when Def: Encoding does not hold:
Proposition 3. ENCODE -METER ϕq(e) = 0 if and only if eis not encoding.
Method Weak Strong
ROAR [19] ✗ ✗
FRESH [18] ✗ ✗
EVAL -X[20] ✓ ✗
STRIPE -X ✓ ✓
Table 1: The weak and strong de-
tection properties of different evalu-
ation methods. Existing scores like
ROAR [19] and FRESH [18], are not
weak detectors, which in turn means
they are not strong detectors either.The proof is in Appendix A.5. Combining EVAL -Xwith
ENCODE -METER weighed by αyields a method we call the
strongly information-penalized evaluator ( STRIPE -X):
STRIPE -Xα(q, e) := EVAL -X(q, e)−αϕq(e). (6)
For a large enough α, the added penalty term pushes down the
scores of encoding explanations below that of all non-encoding
ones, meaning that STRIPE -Xis a strong detector of encoding:
Theorem 2. With finite H(y|x)andH(y), for any explanation
that encodes eand any that does not encode e′, there exists an
α∗such that ∀α > α∗STRIPE -Xα(q, e′)>STRIPE -Xα(q, e).
The proof is in Appendix A.5. The intuition behind the proof is
that for a large enough α, the STRIPE -Xscores for any encoding explanations will be dominated by
the information term, and thus will become smaller than any non-encoding explanation whose score
is lower bounded by the negative marginal entropy, −Hq(y). Table 1 summarizes the weak and
strong detection properties of different evaluations.
Estimating STRIPE -X.The first component of STRIPE -XisEVAL -X. Computing EVAL -X(eq. (4))
requires an estimate of the predictive distribution of the label ygivenxv,q(y|xv)[20]. Estimation
can be done in two ways. The first way makes use of a surrogate model trained to predict the label
from different random subsets using masked tokens [9, 20]. The second way to compute EVAL -X
(eq. (4)) relies on conditional generative models [30, 31]. Both hyperparameters and a combination
of the estimators can be chosen to maximize the average log-likelihood on a held-out validation set
across random input subsets.
To estimate the second part of STRIPE -X, the ENCODE -METER , first expand the mutual information
terms in ENCODE -METER ,ϕq(e), in terms of expected KL:
ϕq(e) =E(v,a)∼q(xe(x))Ey∼q(y|xv=a)KL[q(Ev|xv=a,y)∥q(Ev|xv=a)]. (7)
The outer expectation can be estimated using samples from the data and the inner expectation over
ycan be estimated using the EVAL -Xmodel q(y|xv). The distributions over Evcan be estimated
using a classifier of Evthat randomly masks the label and masks different subsets of the inputs.
Further details and a generative way to estimate STRIPE -Xare in Appendix C.1 and Appendix C.3;
full algorithms are given in Appendix D.
STRIPE -Xin practice. Using STRIPE -Xto choose between explanations is straightforward: pick the
one with the larger score. However, like other evaluations that use learned models, misestimation
can pose a problem. With large α, non-encoding explanations with misestimated ENCODE -METER
will have bad STRIPE -Xscores, while with small αsome encoding explanations can have good
scores. Across all experiments, we set α= 20 , which yielded STRIPE -Xscores for known encoding
explanations worse than known non-encoding explanations.
5 Experiments
This section consists of two parts. The first part demonstrates the weak and strong detection ca-
pabilities of the evaluations ROAR ,EVAL -X, and STRIPE -Xin a simulated setting and on an image
recognition task. To demonstrate these capabilities, we run these evaluations on instantiations of
POSI ,PRED , and MARG . Additionally, we evaluate an existing method that learns to explain under
a reductive constraint, called REAL -X[20]. The second part shows how STRIPE -Xenables discov-
ering encoding explanations in the wild, without specific knowledge of the DGP or the method that
produced the explanation. We employ STRIPE -Xto uncover encoding in explanations generated by
a large language model ( LLM) for predicting sentiments from movie reviews.
5.1 Empirically studying the detection of encoding in a simulated setting
We construct two examples with binary labels y: one discrete input xand one that is a hybrid of
continuous and discrete components. Both use one binary input in x∈ {0,1}5as a control flow
7POSI PRED MARG
e(x) =(
ξ4ifπ(x)>0.5,
ξ5else.e(x) =

arg max
M:|M|≤1π(xM)ifπ(x)>0.5,
arg max
M:|M|≤11−π(xM)else.e(x) =(
ξ1ifx3= 1,
ξ2else.
Table 2: Here, π(x) =q(y= 1|x). Different encoding explanation methods that we consider.
variable and switch the inputs that ydepends on. In both DGPs,yonly depends on x1ifx3= 1,
and only on x2ifx3= 0; this means that x4,x5are purely noise. For both DGPs,yis sampled per
the following distribution where x3determines the subset the ydepends on
q(y= 1|x) = 1[x3= 1]q(y|x1,x3) + 1[x3= 0]q(y|x2,x3). (8)
Thus, EVAL -X∗is achieved by an explanation of size 2:e(x) =ξ1+ξ3ifx3= 1elsee(x) =ξ2+ξ3.
See Appendix C.4 for details; the exact DGPs are given in eq. (36) and eq. (37).
Encoding explanations. Table 2 describes the encoding explanations we consider for this setting.
In Appendix C.4, we check that Def: Encoding holds for these explanations in the discrete DGP
by estimating the role of the unselected inputs in affecting the explanation and the role of Evin
predicting ybeyond xv; a characterization of Def: Encoding to support this check is in Lemma 1.
(a)Results: discrete DGP.
(b)Results: hybrid DGP.
Figure 4: EVAL -Xand STRIPE -Xscores of
the3encoding constructions and the non-
encoding constant explanation (e(x) =
ξ1), for both DGPs.EVAL -X, being only
a weak detector, assigns suboptimal scores
to all encoding explanations ( <), but scores
some encoding explanations above the con-
stant explanation. On the other hand,
STRIPE -X, being a strong detector, pushes
down the scores of all the encoding expla-
nations below that of the non-encoding con-
stant explanation that always selects x1.ROAR and FRESH fails to weakly detect encoding. To
empirically test the analysis about ROAR and FRESH , we
study whether the two evaluations weakly detect encod-
ing. In this study, we compare each evaluation’s score on
the all-inputs explanation, which is optimal, to the score
assigned to MARG .MARG ignores x3which is required to
produce the label yin eq. (8). ROAR log-likelihoods for
MARG and the all-inputs explanation are approximately
−H(y) =−0.69for both DGPs. In addition, the value
of the input that MARG selects alone contains all the infor-
mation about the label regardless of whether MARG selects
x1orx2. Thus, FRESH log-likelihoods for MARG and the
all-inputs explanation are both approximately −0.29for
both DGPs. This result validates that ROAR and FRESH are
not weak detectors because they do not separate the opti-
mal explanation from all encoding explanations.
EVAL -Xis a weak detector of encoding but not a strong
detector. EVAL -Xlog-likelihood scores are given in blue
in Figures 4a and 4b. EVAL -X, being a weak detec-
tor, scores the encoding constructions ( POSI ,PRED , and
MARG ) strictly lower than the log-likelihood of the opti-
mal explanation EVAL -X∗. However, the EVAL -Xscore for
theMARG explanation is −0.4, which is above the score of
−0.6achieved by a non-encoding explanation e(x) =ξ1;
thus, EVAL -Xis not a strong detector.
Strong detector STRIPE -Xprices out all the encoding
explanations. Figures 4a and 4b report STRIPE -Xscores
for the same set of explanations as above; STRIPE -Xscores
are shown in red. Strong detector STRIPE -Xscores the non-encoding explanations above the nega-
tive entropy −Hq(y) =−0.69and scores every encoding construction under that threshold.
5.2 Detecting encoding on images of dogs and cats
The goal of this section is to study the encoding detection capabilities of ROAR ,EVAL -X, and
STRIPE -Xon real data. We consider an image recognition task like the one in Figure 3 with la-
bels and images from the cats_vs_dogs dataset from the Tensorflow package [32]. We break
images of size 64×64into4patches each of size 32×32. In left-right then top-down order,
letx1,x2,x3,x4be the upper left, upper right, bottom left, and bottom right patches respectively;
x1,x3capture color, and x2,x4are the animal images. With annot(image) denoting the annota-
8tion in the cats_vs_dogs dataset the image having a dogor acat, the label is assigned as:
y= 1[x1=blue ]× 1[annot(image x2)=dog ] + 1[x1=red]× 1[annot(image x4)=dog ]
ROAR FRESH EVAL -X STRIPE -X
opt 0.69−0.23−0.27−0.31
fixed 0.59 −0.64 −0.64 −0.64
POSI 0.51 −0.69 −0.70 −5.98
PRED 0.69−0.23 −0.51 −1.40
MARG 0.69−0.23 −0.53 −1.02
Table 3: ROAR ,FRESH ,EVAL -X, and STRIPE -X
scores for the image recognition experiment. Higher
is better. ROAR and FRESH score two encoding ex-
planations PRED and MARG as high as the optimal
explanation, meaning they are not even weak de-
tectors. EVAL -Xbeing only a weak detector scores
POSI ,PRED , and MARG all worse than the optimal
explanation under both EVAL -Xbut not the non-
encoding constant explanation ( e(x) =ξ4), denoted
fixed .STRIPE -Xbeing a strong detector scores
the non-encoding explanations above the negative
marginal entropy −Hq(y) =−0.69and scores ev-
ery encoding construction under that threshold.We consider three encoding explanations ( POSI ,
PRED ,MARG ) and two non-encoding ones: 1)
optimal , which selects the color and the patch
that produces the label as dictated by the color, and
2) denoted fixed , which always outputs the bot-
tom right patch x4. Appendix C.6 gives details.
We report the scores assigned to each explana-
tion by ROAR ,EVAL -X, and STRIPE -Xin Table 3.
ROAR scores two encoding explanations PRED and
MARG as high as the optimal explanation, mean-
ing it is not even a weak detector. POSI ,PRED , and
MARG all score worse than the optimal expla-
nation under both the weak detector EVAL -Xand
the strong detector STRIPE -X. However, EVAL -
Xscores one non-encoding explanation ( fixed )
worse than two encoding ones, meaning it is not a
strong detector. Being a strong detector, STRIPE -X
scores the fixed explanation above the negative
marginal entropy −Hq(y) =−0.69and scores
every encoding construction under that threshold.
Evaluating explanations produced by REAL -X[20]. We ran REAL -Xto learn explanations for the
simulated setting and the image recognition task. In the simulated setting, REAL -Xis run to select
one input; Appendix C.4 gives details. In the image recognition task, REAL -Xis run to select one
of the four patches as an explanation; Appendix C.6 gives details. In both the simulated setting
(see Figures 4a and 4b) and the image recognition task (see Table 3), REAL -Xfails to achieve the
optimal EVAL -Xscore while achieving a STRIPE -Xscore below the threshold of negative marginal
entropy −Hq(y) =−0.69. Upon investigation, we found that REAL -Xproduced an explanation that
matched the MARG construction on at least 80% of the inputs in the simulated setting. On the image
recognition task, REAL -Xexplanation matched the MARG explanation on the whole dataset. In both
cases, STRIPE -X, being a strong detector, correctly alerts that the REAL -Xexplanation encodes.
5.3 Encoding in LLM -generated explanations
One can detect encoding in any explanation by checking if the STRIPE -Xscore falls below the
negative marginal entropy. Recent work uses LLMs to produce explanations; e.g. [33] prompt an
LLM to generate explanations for reasoning tasks which are later used to improve smaller models. If
theLLM explanation encodes, the smaller model can falsely ignore the informative inputs the larger
model’s explanation depends on and yet does not reveal. In this section, we evaluate explanations
generated by an LLM, Llama 3, for a sentiment analysis task. We consider reviews that take one of
two forms: with ADJ1 andADJ2 as adjectives, the review is
•‘My day was <ADJ1> and the movie was <ADJ2>. that is it’ or
•‘My day was <ADJ1> and the movie was <ADJ2>. oh wait, reverse the adjectives’ .
The second sentence in the review acts as a "control flow" input and determines whether ADJ1 or
ADJ2 describes the sentiment about the movie. We prompt Llama 3 (see Appendix C.8) to predict the
sentiment and select a few words from the review that were important for that sentiment; the selected
parts form the generated explanation. To discourage encoding, the prompt explicitly instructs the
LLM to select all the words that the LLM based the selection on; such an explanation, by Lemma 1,
would be non-encoding. On the 5most common selections e(x)generated by the LLM, we compute
the EVAL -Xscore and the ENCODE -METER ϕq(e). The resulting STRIPE -Xscore is −2.78, falling
short of the negative entropy −Hq(y) =−0.69, meaning the LLM encodes. We investigated why.
As an example, consider the review ‘My day was resplendent and the movie was hollow.
that is it.’ ; the LLM selects only hollow in the explanation. However, the LLM instead selects
resplendent when that is it is switched to oh wait, reverse the adjectives. Such occur-
rences are common. On >70% of the data, the LLM selects the word that describes the movie
9but does not select the second sentence in the review which controls which adjective describes the
movie; this is akin to MARG encoding. Thus, the LLM-generated explanation encodes by looking
at the control flow input in the second sentence to find the correct adjectives, but failing to select
the control flow input. Such an explanation falsely indicates that only the adjectives are relevant to
predicting the label. In contrast, a non-encoding explanation would, in addition to the adjective that
describes the movie, reveal control flow words that indicate which adjective predicts the label.
In summary, despite being instructed to include all the words that were looked at when producing
the explanation, the LLM encodes. Building non-encoding explanations with LLMs may require an
extensive search over prompts or finetuning guided by scores from STRIPE -X.
6 Discussion
When an explanation is encodes, predictions from the explanation become disconnected from pre-
dictions from the values in the explanations. Such explanations can select values with little relevance
to the label and yet score highly on the many existing predictive evaluations. We develop a simple
statistical definition of encoding. Inverting this definition shows that when non-encoding explana-
tions predict the label, users know the values of those inputs selected in the explanation predict the
label. We then show that existing evaluations are either non-detectors ( ROAR [19], FRESH [18]) or
only weak detectors ( EVAL -X[20]). Motivated by this, we introduce a new strong detector, STRIPE -
X. After empirically demonstrating the detection capabilities (or lack thereof) of said evaluations,
we use STRIPE -Xto discover encoding in LLM-generated explanations.
More related work. Other investigations into evaluating explanations focused on label leakage
[26, 34] and faithfulness [18, 35, 36, 37, 38]. Label leakage is similar to encoding in that additional
information is in the explanation, but focuses on explanations that have access to both the inputs
and the observed label; we leave extending Def: Encoding to leakage to the future. Faithfulness,
intuitively, asks that the explanation reflect the process of how a label is predicted from the inputs; a
formalization does not exist. Jacovi and Goldberg [38] note the need to define faithfulness formally.
Encoding explanations are not faithful to the process of making an explanation because predictive
inputs outside those selected by the explanation control the explanation.
Limitations and the future. Using misestimated models in evaluations (like EVAL -X) may lead to
mistakes (see Appendix B.8 for an example). The retinal fundus experiment from Jethani et al. [26]
is an example where misestimation leads to reductive explanations scoring higher than using the
full input. Misestimation can be due to poor uncertainty or due to dependence on shortcut features.
One fruitful direction is to use better uncertainty estimates, like conformal inference [39] or calibra-
tion [40], or employ robustness methods [41, 42] to ameliorate errors due to misestimation. Another
direction is use tricks like REINFORCE -style gradients to construct non-encoding explanations by
optimizing STRIPE -X. Explanations that output subsets may not always help humans interpret the
mechanism of the prediction. For example, imagine one wants to understand why a model correctly
answers the question "Who won the ski halfpipe at the X-games 3 years after her debut in 2021?"
with "Eileen Gu". A subset explanation may return "3 years after her debut in 2021" and "ski half-
pipe", but that does not help a human interpret how the model predicts. A better interpretation would
be to make the model output, "3 years after 2021 is 2024. Eileen Gu won in 2024, and debuted in
2021." Such explanations can also encode information about the prediction in the text produced as
a rationale [43]. An important direction here would be to extend the definitions of weak and strong
detectors of encoding to evaluations of free-text rationales.
Data versus Model Explanations. Even with the formal definitions of explanation methods, there
is a question about what is being explained: the data or the model. These two concepts often get
blended together in the literature [11, 20]. We clarify this point and abstract the choice away as two
different ways to produce the joint distribution q(y,x). Indata explanation , the distribution under
which a feature attribution method seeks to output a subset of inputs that predict the label should
be the population distribution of the data [23]. If, instead, the goal is model explanation , the goal
should not be to highlight inputs that predict the label well in samples of the data; rather it should
beto predict the label well in samples from the model. Formally, a model with parameters θis
a conditional distribution, pθ(y|x). To target a model explanation, a feature attribution method
would aim to output a subset of inputs that predict the label under the distribution F(x)pθ(y|x).
10Acknowledgements
This work was partly supported by the NIH/NHLBI Award R01HL148248, NSF Award 1922658
NRT-HDR:FUTURE Foundations, Translation, and Responsibility for Data Science, NSF CAREER
Award 2145542, NSF Award 2404476, ONR N00014-23-1-2634, Google DeepMind, and Apple.
The authors would like to thank Yoav Wald, the NeurIPS 2024 reviewers and the NeurIPS 2024 area
chair for helpful feedback.
References
[1] Pierre Elias, Timothy J Poterucha, Vijay Rajaram, Luca Matos Moller, Victor Rodriguez,
Shreyas Bhave, Rebecca T Hahn, Geoffrey Tison, Sean A Abreau, Joshua Barrios, et al. Deep
learning electrocardiographic analysis for detection of left-sided valvular heart disease. Jour-
nal of the American College of Cardiology , 80(6):613–626, 2022.
[2] Neil Jethani, Aahlad Puli, Hao Zhang, Leonid Garber, Lior Jankelson, Yindalon
Aphinyanaphongs, and Rajesh Ranganath. New-onset diabetes assessment using artificial
intelligence-enhanced electrocardiography. arXiv preprint arXiv:2205.02900 , 2022.
[3] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Visualising image classification
models and saliency maps. Deep Inside Convolutional Networks , 2, 2014.
[4] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Ad-
vances in neural information processing systems , 30, 2017.
[5] Kim Long Tran, Hoang Anh Le, Thanh Hien Nguyen, and Duc Trung Nguyen. Explainable
machine learning for financial distress prediction: evidence from vietnam. Data , 7(11):160,
2022.
[6] Alex J DeGrave, Joseph D Janizek, and Su-In Lee. Ai for radiographic covid-19 detection
selects shortcuts over signal. Nature Machine Intelligence , 3(7):610–619, 2021.
[7] Felix Wong, Erica J Zheng, Jacqueline A Valeri, Nina M Donghia, Melis N Anahtar, Satotaka
Omori, Alicia Li, Andres Cubillos-Ruiz, Aarti Krishnan, Wengong Jin, et al. Discovery of a
structural class of antibiotics with explainable deep learning. Nature , pages 1–9, 2023.
[8] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi
Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-
based localization. In Proceedings of the IEEE international conference on computer vision ,
pages 618–626, 2017.
[9] Ian Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for
model explanation. Journal of Machine Learning Research , 22(209):1–90, 2021.
[10] Neil Jethani, Mukund Sudarshan, Ian Connick Covert, Su-In Lee, and Rajesh Ranganath. Fast-
shap: Real-time shapley value estimation. In International Conference on Learning Represen-
tations , 2022.
[11] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. Invase: Instance-wise variable
selection using neural networks. In International Conference on Learning Representations ,
2018.
[12] Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel J Gershman, and
Finale Doshi-Velez. Human evaluation of models built for interpretability. In Proceedings of
the AAAI Conference on Human Computation and Crowdsourcing , volume 7, pages 59–67,
2019.
[13] Adriel Saporta, Xiaotong Gui, Ashwin Agrawal, Anuj Pareek, Steven QH Truong, Chanh DT
Nguyen, Van-Doan Ngo, Jayne Seekins, Francis G Blankenberg, Andrew Y Ng, et al. Bench-
marking saliency methods for chest x-ray interpretation. Nature Machine Intelligence , 4(10):
867–878, 2022.
[14] Jonathan Crabbé, Alicia Curth, Ioana Bica, and Mihaela van der Schaar. Benchmarking het-
erogeneous treatment effect models through the lens of interpretability. Advances in Neural
Information Processing Systems , 35:12295–12309, 2022.
[15] Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, and Klaus-
Robert Müller. Evaluating the visualization of what a deep neural network has learned. IEEE
transactions on neural networks and learning systems , 28(11):2660–2673, 2016.
11[16] V Petsiuk, A Das, and K Saenko. Rise: Randomized input sampling for explanation of black-
box models. arXiv preprint arXiv:1806.07421 , 2018.
[17] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. Advances
in neural information processing systems , 30, 2017.
[18] Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron C Wallace. Learning to faithfully
rationalize by construction. arXiv preprint arXiv:2005.00115 , 2020.
[19] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for in-
terpretability methods in deep neural networks. Advances in neural information processing
systems , 32, 2019.
[20] Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, and Rajesh Ranganath. Have
we learned to explain?: How interpretability methods can learn to encode predictions in their
interpretations. In International Conference on Artificial Intelligence and Statistics , pages
1459–1467. PMLR, 2021.
[21] Jennifer Hsia, Danish Pruthi, Aarti Singh, and Zachary C Lipton. Goodhart’s law applies to
nlp’s explanation benchmarks. arXiv preprint arXiv:2308.14272 , 2023.
[22] Isabelle Guyon and André Elisseeff. An introduction to variable and feature selection. Journal
of machine learning research , 3(Mar):1157–1182, 2003.
[23] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An
information-theoretic perspective on model interpretation. In International conference on ma-
chine learning , pages 883–892. PMLR, 2018.
[24] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining
the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international
conference on knowledge discovery and data mining , pages 1135–1144, 2016.
[25] Erik Štrumbelj and Igor Kononenko. Explaining prediction models and individual predictions
with feature contributions. Knowledge and information systems , 41:647–665, 2014.
[26] Neil Jethani, Adriel Saporta, and Rajesh Ranganath. Don’t be fooled: label leakage in explana-
tion methods and the importance of their quantitative evaluation. In International Conference
on Artificial Intelligence and Statistics , pages 8925–8953. PMLR, 2023.
[27] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Kernel-based condi-
tional independence test and application in causal discovery. In Proceedings of the Twenty-
Seventh Conference on Uncertainty in Artificial Intelligence , pages 804–813, 2011.
[28] Mukund Sudarshan, Wesley Tansey, and Rajesh Ranganath. Deep direct likelihood knockoffs.
Advances in neural information processing systems , 33:5036–5046, 2020.
[29] Mukund Sudarshan, Aahlad Puli, Wesley Tansey, and Rajesh Ranganath. Diet: Conditional
independence testing with marginal dependence measures of residual information. In Interna-
tional Conference on Artificial Intelligence and Statistics , pages 10343–10367. PMLR, 2023.
[30] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language mod-
els are few-shot learners. Advances in neural information processing systems , 33:1877–1901,
2020.
[31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hier-
archical text-conditional image generation with clip latents, 2022. URL https://arxiv.
org/abs/2204.06125 , 7, 2022.
[32] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Good-
fellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz
Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek
Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow:
Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.
org/ . Software available from tensorflow.org.
12[33] Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang,
Jing Qian, Baolin Peng, Yi Mao, et al. Explanations from large language models make small
reasoners better. arXiv preprint arXiv:2210.06726 , 2022.
[34] Peter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal. Leakage-adjusted simulatability:
Can models generate non-trivial explanations of their behavior in natural language? arXiv
preprint arXiv:2010.04119 , 2020.
[35] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova.
" will you find these shortcuts?" a protocol for evaluating the faithfulness of input salience
methods for text classification. arXiv preprint arXiv:2111.07367 , 2021.
[36] Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, and Julie Shah. Do feature attribution meth-
ods correctly attribute features? In Proceedings of the AAAI Conference on Artificial Intelli-
gence , volume 36, pages 9623–9633, 2022.
[37] Yiming Ju, Yuanzhe Zhang, Zhao Yang, Zhongtao Jiang, Kang Liu, and Jun Zhao. Logic traps
in evaluating attribution scores. arXiv preprint arXiv:2109.05463 , 2021.
[38] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we
define and evaluate faithfulness? arXiv preprint arXiv:2004.03685 , 2020.
[39] Jing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman.
Distribution-free predictive inference for regression. Journal of the American Statistical Asso-
ciation , 113(523):1094–1111, 2018.
[40] Mark Goldstein, Xintian Han, Aahlad Puli, Adler Perotte, and Rajesh Ranganath. X-cal: Ex-
plicit calibration for survival analysis. Advances in neural information processing systems , 33:
18296–18307, 2020.
[41] Aahlad Manas Puli, Lily H Zhang, Eric Karl Oermann, and Rajesh Ranganath. Out-of-
distribution generalization in the presence of nuisance-induced spurious correlations. ICLR
2022 , 2021.
[42] Aahlad Manas Puli, Lily Zhang, Yoav Wald, and Rajesh Ranganath. Don’t blame dataset
shift! shortcut learning due to gradients and cross entropy. Advances in Neural Information
Processing Systems , 36, 2023.
[43] Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue
Simonsen, and Isabelle Augenstein. Faithfulness tests for natural language explanations. arXiv
preprint arXiv:2305.18029 , 2023.
13A Theoretical Details
A.1 Expressing q(y|xe(x))in terms of the values and the identity of the explanation
To express q(y|xe(x)), we use the following equivalence of events from eq. (1)
{x:xe(x)= (v,a)}={x:e(x) =v} ∩ {x:xv=a}. (eq. (1))
Then, intuitively, conditioning on the event that xe(x)= (v,a)gives you the same information as
conditioning on the events e(x) =vandxv=asimultaneously. We make this formal below.
For discrete x, for any v,asuch that the probability q(xe(x)= (v,a))>0, define the LHS and
RHS of eq. (1) as Bv, Cvrespectively. Then, the conditionals q(y|xe(x)= (v,a))andq(y|Ev=
1,xv=a)exist and can be written as follows:
q(y|xe(x)= (v,a)) =q(y|x∈Bv) q(y|Ev= 1,xv=a) =q(y|x∈Cv).
These two conditionals are equal because Bv=Cv.
The same kind of result holds for general random vectors (discrete or continuous) xbut is a little
more involved because Bvmay be non-empty while q(x∈Bv) = 0 and the equality of conditional
densities/probabilities need to be written via measure theory. Assume the regular conditional proba-
bilities q(y|xe(x))andq(y,Ev|xv), q(y|Ev,xv)andq(Ev|xv)are defined almost everywhere
in their respective probability measures. Take any Sv⊆ {xv:e(x) =v}where q(xv∈Sv)>0
andq(e(x) =v)>0. Consider any measurable sets YoveryandBv(Sv) :={(v,a) :a∈Sv}
overxe(x). Now, by definition of regular conditional probability measures, joint probabilities are
obtained by taking the expectation of the conditional with respect to marginal distributions over the
conditioning set:
q(y∈Y,xe(x)∈Bv(Sv)) =Z
Bv(Sv)q(y∈Y|xe(x)= (v,a))q(dxe(x))
=Z
Svq(y∈Y|xe(x)= (v,a))q(xe(x)= (v,a))da (9)
and
q(y∈Y,Ev= 1,xv∈Sv) =Z
Svq(y∈Y,Ev= 1|xv=a)q(xv=a)da
=Z
Svq(y∈Y|Ev= 1,xv=a)q(Ev= 1,xv=a)da.(10)
Due to eq. (1), the LHS terms of the two equations above are equal and so are the probability
measures over the integrating variables in eqs. (9) and (10). Letting xvbe defined on a Borel sigma
algebra, these two integrals eqs. (9) and (10) are equal if and only if for any Borel set Sv, for almost
every a∈Sv
q(y∈Y|xe(x)= (v,a)) =q(y∈Y|Ev= 1,xv=a).
That is, in more plain terms, the conditional distributions are equal q(y|xe(x)= (v,a)) =
q(y|Ev= 1,xv=a).
A.2 With non-encoding explanations "what you see is what you get"
Def: Encoding says that an explanation e(x)is encoding if there exists an Swhere q(xe(x)∈S)>0
such that for every (v,a)∈S, :
y
̸|=Ev|xv=a. (11)
For a non-encoding explanation, Def: Encoding does not hold. Here, we derive the implications of
violating Def: Encoding. Define the set Ato contain all (v,a)where eq. (11) is violated:
A={(v,a) :y
|=Ev|xv=a}. (12)
14By definition, the complement ACis such that
∀(v,a)∈AC,y
̸|=Ev|xv=a.
Such a set cannot have positive measure when Def: Encoding is violated which means
q(xe(x)∈AC) = 0 .
In turn,
q(xe(x)∈A) = 1−q(xe(x)∈AC) = 1 .
Thus, Ais such that q(xe(x)∈A) = 1 , and by eq. (12) for all (v,a)∈A
y
|=Ev|xv=a,
which in turn guarantees
q(y|xe(x)= (v,a)) =q(y|xv=a,Ev= 1) = q(y|xv=a).
A.3 Helpful Lemmas and their proofs
A.3.1 Alternate conditions equivalent to Def: Encoding
The dependence in Def: Encoding occurs due to two reasons, understanding which sheds more
light on the definition. First, for some selection e(x) =v, the explanation’s values xvdo not
provide enough information to reveal that the explanation should select the inputs denoted by v.
In other words, the indicator of the selection is variable even after fixing the explanation’s values
themselves. Second, this indicator is predictive of the label for the data with the explanation v.
These two properties provide intuition on the definition of encoding:
Lemma 1. Def: Encoding holds for an explanation e(x)if and only if there exists a selection v
such that q(e(x) =v)>0and a set Sv⊆ {xv:e(x) =v}such that q(xv∈Sv)>0where both
of the following conditions hold for almost every a∈Sv:
Unpredictability of Explanation q(Ev= 1|xv=a)̸= 1;
Additional Information from Explanation q(y|xv=a,Ev= 1)̸=q(y|xv=a,Ev= 0).
Proof. First, Lemma 2 shows the Def: Encoding holds if only if there exists a selection vsuch that
q(e(x) =v)>0and a set Sv⊆ {xv:e(x) =v}such that q(xv∈Sv)>0where
∀a∈Sv,y
̸|=Ev|xv.
We use this alternate definition in what follows.
Given a non-measure zero set Sv, by Lemma 3, almost everywhere in Svit holds that q(Ev=
1|xv)>0.
Conditional dependence implies Unpredictability and Additional information (the only if
part). Ifq(Ev= 1|xv) = 1 almost everywhere (under q(x)), then Evis constant given xv,
and therefore independent of any variable given xv:
q(Ev= 1|xv) = 1 = ⇒y
|=Ev|xv.
Then, it follows that conditional dependence implies the unpredictability property
y
̸|=Ev|xv=⇒q(Ev= 1|xv)<1.
Second, with the result from Lemma 3, we have q(Ev= 1|xv)∈(0,1). Thus q(y|xv,Ev= 1) and
q(y|xv,Ev= 0) exist almost every where in Sv. Then, by definition of conditional dependence,
there is additional information about the label in the explanation:
y
̸|=Ev|xv=⇒q(y|xv,Ev= 1)̸=q(y|xv,Ev= 0).
This shows that Def: Encoding implies the additional information property.
15Conditional dependence implied by Unpredictability and Additional information (the if part).
Now, if q(Ev= 1|xv)∈(0,1), then the following two conditional distributions exist almost
everywhere in Sv
q(y|xv,Ev= 1) , q (y|xv,Ev= 0).
Then, by definition of dependence almost everywhere Sv:
q(y|xv,Ev= 1)̸=q(y|xv,Ev= 0) = ⇒y
̸|=Ev|xv.
Thus, the unpredictability and the additional information properties imply Def: Encoding.
Lemma 2. Def: Encoding holds for an explanation e(x)if and only if there exists a selection v
such that q(e(x) =v)>0and a set Sv⊆ {xv:e(x) =v}such that q(xv∈Sv)>0where
∀a∈Sv,y
̸|=Ev|xv=a. (13)
Proof. Def: Encoding says that the explanation e(x)is encoding if there exists an Swhere q(xe(x)∈
S)>0such that for every (v,a)∈Seq. (13) holds. This proof works by showing that Shaving a
positive measure implies the existence of vandSvas in Lemma 2 such that eq. (13) holds.
Decompose q(xe(x)∈S)by introducing an expectation over v∼q(e(x)),
q(xe(x)∈S) =Ev∼q(e(x))q(xe(x)∈S|e(x) =v).
As there are only finitely many v,
q(xe(x)∈S)>0⇐⇒ ∃ vs.t.q(e(x) =v)>0and q(xe(x)∈S|e(x) =v)>0.
The "only if" direction. Pick any vsuch that the RHS above holds and define Sv={a: (v,a)∈
S}. By definition,
Sv={xv: (v,xv)∈S} ∩ {xv:e(x) =v} ⊆ {xv:e(x) =v}.
This proves that Svhas positive measure:
q(xv∈Sv) =q(xe(x)∈S, e(x) =v) =q(xe(x)∈S|e(x) =v)∗q(e(x) =v)>0.
Finally, as a∈Sv=⇒(v,a)∈S, eq. (13) holds:
∀a∈Sv,y
̸|=Ev|xv=a.
This completes the "only if" direction.
The "if" direction. Assume that there exists vsuch that q(e(x) =v)>0andSv⊆ {xv:
e(x) =v}such that q(xv∈Sv)>0where
∀a∈Sv y
̸|=Ev|xv=a.
Define S={(v,a) :a∈Sv}. By this construction, Shas positive measure:
q(xe(x)∈S) =q((v,xv)∈S)
=q(e(x) =v)q((v,xv)∈S|e(x) =v)
=q(e(x) =v)q(xv∈Sv|e(x) =v)
=q(e(x) =v)q(xv∈Sv){asSv⊆ {xv:e(x) =v}
>0,
where the last inequality holds because by assumption
q(e(x) =v)>0 q(xv∈Sv)>0.
Finally, as (v,a)∈S=⇒a∈Sv, eq. (13) holds:
∀(v,a)∈S,y
̸|=Ev|xv=a.
This completes the "if" directions and with that the proof.
16Lemma 3. For any set Sv⊆ {xv:e(x) =v}such that q(xv∈Sv)>0, then for almost every
a∈Sv,q(Ev= 1|xv=a)>0.
Proof. Define the set Av={a:q(Ev= 1|xv=a) = 0}. Next compute the joint probability
q(xv∈Av∩Sv) =q(xv∈Av)q(xv∈Sv|xv∈Av).
Now, noting that Svis a subset of {xv:e(x) =v}, which is equivalent to {xv:Ev= 1}, thus
q(xv∈Sv|xv∈Av)
=Z
q(xv∈Sv|xv=a,xv∈Av)q(xv=a|xv∈Av)da
=Z
q(xv∈Sv|xv=a)q(xv=a|xv∈Av)da
≤Z
q(Ev= 1|xv=a)q(xv=a|xv∈Av)da
=Z
q(Ev= 1|xv=a)q(xv=a|xv∈ {a:q(Ev= 1|xv=a) = 0})da
= 0.
The probability q(xv∈Sv|xv∈Av)is non-negative, so it must be zero. Plugging this conditional
back into the joint gives q(xv∈Av∩Sv) = 0 . Then expanding yields
0 =q(xv∈Av∩Sv) =q(xv∈Av|xv∈Sv)q(xv∈Sv).
Since q(xv∈Sv)>0,q(xv∈Av|xv∈Sv)must be zero and thus, q(xv/∈Av|xv∈Sv) = 1 ,
where expanding out the definition of Avgives the desired result that q(Ev= 1|xv=a)>0for
almost a∈Sv:
1 =q(a/∈Av|a∈Sv)
=q(a/∈ {a:q(Ev= 1|xv=a) = 0} |a∈Sv)
=q(a∈ {a:q(Ev= 1|xv=a)>0} |a∈Sv).
A.3.2 Optimal value and the optimal gap under EVAL -X
Lemma 4. The EVAL -Xoptimality gap value for e(·)is an averaged KL between q(y|x)and
q(y|xv):P
v∈Vq(e(x) =v)Eq(x|e(x)=v)KL(q(y|x)∥q(y|xv)).This gap is zero, i.e. e(x)is
optimal with the score EVAL -X∗=Eq[logq(y|x)]if for all vsuch that q(e(x) =v)>0,
q(y|x) =q(y|xv)a.e. in {x:e(x) =v}.
Proof. Letpbe a generic conditional distribution and let x−vbe the values outside the explanation.
max
eEVAL -X(q, e) = max
eE(v,a)∼q(xe(x))Eq(y|xe(x)=(v,a))logq(y|xv=a)
= max
eE(v,a)∼q(xe(x))Eq(x−v|xe(x)=(v,a))Eq(y|xe(x)=(v,a),x−v)logq(y|xv=a)
= max
eE(v,a)∼q(xe(x))Eq(x|xe(x)=(v,a))Eq(y|x)logq(y|xv=a)
≤max
pE(v,a)∼q(xe(x))Eq(x|xe(x)=(v,a))Eq(y|x)[logp(y|x)]
≤max
pEq(x)Eq(y|x)[logp(y|x)]
= max
p−Eq(x)KL(q(y|x)||p(y|x)) +Eq[logq(y|x)]
=Eq[logq(y|x)].
This upper bound is achievable by an explanation that selects all inputs, so the maximum EVAL -X
denoted as EVAL -X∗=Eqlogq(y|x).
17As in the math above, the EVAL -Xscore for an explanation method can be expanded as
EVAL -Xe=E(v,a)∼q(xe(x))Eq(y|xe(x)=(v,a))logq(y|xv=a)
=E(v,a)∼q(xe(x))Eq(x|xe(x)=(v,a))Eq(y|x)logq(y|xv=a)
=Ev∼q(e(x))Ea∼q(xv|e(x)=v)Eq(x|xv=a,e(x)=v))Eq(y|x)logq(y|xv=a)
=Ev∼q(e(x))Eq(x|e(x)=v))Eq(y|x)logq(y|xv),
where in the last step, we dropped abecause it equals xvalmost surely. Similarly, the optimal score
EVAL -X∗expands to
Ev∼q(e(x))Eq(x|e(x)=v))Eq(y|x)logq(y|x).
LetVbe the set of values that explanations can take on, then taking the difference from optimality
EVAL -X∗−EVAL -Xe
=Ev∼q(e(x))Eq(x|e(x)=v))Eq(y|x)logq(y|x)
q(y|xv=x)
=Ev∼q(e(x))Eq(x|e(x)=v))KL[q(y|x)∥q(y|xv)]
=X
v∈Vq(e(x) =v)Eq(x|e(x)=v))KL[q(y|x)∥q(y|xv)].
As each KLterm is non-negative, each term in the sum being set to 0simultaneously achieves the
optimum, which happens when for all vsuch that q(e(x) =v)>0,
q(y|x) =q(y|xv)for almost every {x:e(x) =v}.
In Appendix A.4, we use the results from Lemma 1 and Lemma 4 to prove that the optimal score of
EVAL -Xcan only be achieved by non-encoding explanations.
A.4 Proof of Theorem 1
Theorem 1. Ife(x)isEVAL -Xoptimal, then e(x)is not encoding.
Proof. Note only q(e(x) =v)>0are of interest, since q(e(x) =v) = 0 implies that Ev= 0
almost surely and thus y
|=Ev|xv.
Then if e(x)achieves EVAL -X∗, then by Lemma 4, for all vsuch that q(e(x) =v)>0,
q(y|x) =q(y|xv)for almost every {x:e(x) =v}.
First, this optimality criteria can incorporate Ev= 1 on the lefthand side by first conditioning on
e(x)and then noting that the equality holds for xwhere e(x) =v.
q(y|x) =q(y|xv)for almost every {x:e(x) =v}
⇐⇒ q(y|x, e(x)) =q(y|xv)for almost every {x:e(x) =v}
⇐⇒ q(y|x, e(x) =v) =q(y|xv)for almost every {x:e(x) =v}
⇐⇒ q(y|x,Ev= 1) = q(y|xv)for almost every {x:e(x) =v}.
To understand if the optimality criterion disallows encoding, integrate the left and right-hand sides
of this optimality criterion with the respect to complement of the inputs in xv,q(xc
v|xv,Ev= 1)
yieldsZ
q(y|x,Ev= 1)q(xc
v|xv,Ev= 1)dxc
v=Z
q(y|xv)q(xc
v|xv,Ev= 1)dxc
v
for almost every {x:e(x) =v}
⇐⇒Z
q(y|xc
v,xv,Ev= 1)q(xc
v|xv,Ev= 1)dxc
v=q(y|xv)Z
q(xc
v|xv,Ev= 1)dxc
v
for almost every {x:e(x) =v}
⇐⇒ q(y|xv,Ev= 1) = q(y|xv)for almost every {x:e(x) =v}
⇐⇒ q(y|xv,Ev= 1) = q(y|xv)for almost every {xv:e(x) =v}.
18Now expanding the right-hand side gives
q(y|xv) =q(y,Ev= 1|xv) +q(y,Ev= 0|xv).
Combing the two equations gives
q(y|xv,Ev= 1) = q(y,Ev= 1|xv) +q(y,Ev= 0|xv)for almost every {xv:e(x) =v}.
(14)
We show that this equality implies that y
|=Ev|xvby splitting the analysis into cases based on
q(Ev= 1|xv) = 1 andq(Ev= 1|xv)<1. In turn, the condition in Def: Encoding is violated
and the explanation e(·)is not encoding.
Case 1: EVAL -Xoptimality holds when the explanation is predictable. The first case to con-
sider is when the event that the explanation takes the value vis determined by xvfor all samples
with the explanation v. That is, q(Ev= 1|xv) = 1 :
q(Ev= 1|xv) = 1 ⇐⇒ q(Ev= 0|xv) = 0 .
Then expanding this marginal into the joint shows that the joint q(y,Ev= 0|xv)has to be zero as
well.
q(Ev= 0|xv) =Z
q(y,Ev= 0|xv)dy= 0,
because an integral of non-negative terms being zero implies that each term itself is zero almost
surely.
Then, we can show that the determinism condition q(Ev= 1|xv) = 1 is sufficient for the optimal-
ity criterion eq. (14):
q(y|xv,Ev= 1) = q(y|xv,Ev= 1)×1
=q(y|xv,Ev= 1)q(Ev= 1|xv)
=q(y,Ev= 1|xv)
=q(y,Ev= 1|xv) +q(y,Ev= 0|xv)for almost every {xv:e(x) =v}.
This shows that the EVAL -Xoptimality criteria is satisfied when the q(Ev= 1|xv) = 1 , thus the
explanation is completely predictable from the explanation for examples with that explanation. By
Lemma 1, we have
q(Ev= 1|xv) = 1 = ⇒y
|=Ev|xv,
which violates Def: Encoding. So there is no encoding in this case.
Case 2: When the explanation is unpredictable, EVAL -Xoptimality requires that the explana-
tion provide no extra information. Now consider the alternative case, q(Ev= 1|xv)<1. Here
the explanation does not determine the explanation opening the possibility that the EVAL -X-optimal
explanation method can encode information in the explanation.
Because q(e(x) =v)>0, we have q(xv∈ {xv:e(x) =v})>0. Thus, by Lemma 3, for almost
every{xv:e(x) =v}it holds that q(Ev= 1|xv)>0.Putting this result together with alternative
case ( q(Ev= 1|xv)<1) gives: 0< q(Ev= 1|xv)<1for almost every {xv:e(x) =v}.
Now, expanding out the optimality criterion eq. (14):
q(y|xv,Ev= 1) = q(y,Ev= 1|xv)×1 +q(y,Ev= 0|xv)×1
for almost every {xv:e(x) =v}
⇐⇒ q(y|xv,Ev= 1) = q(y|Ev= 1,xv)q(Ev= 1|xv)
+q(y|Ev= 0,xv)(1−q(Ev= 1|xv))
for almost every {xv:e(x) =v}
⇐⇒ (1−q(Ev= 1|xv))q(y|xv,Ev= 1) = (1 −q(Ev= 1|xv))q(y|xv,Ev= 0)
for almost every {xv:e(x) =v}
⇐⇒ q(y|xv,Ev= 1) = q(y|xv,Ev= 0) for almost every {xv:e(x) =v}.
This equality says for all samples with the explanation v, knowing Evdoes not change the distri-
bution of the label y. By Lemma 1, this equality implies that the independence y
|=Ev|xvholds
which violates Def: Encoding.
19A.5 Proof of Proposition 3 and Theorem 2
Proposition 3. ENCODE -METER ϕq(e) = 0 if and only if eis not encoding.
Proof. First, Def: Encoding is violated if and only if there exists a set Asuch that q(xe(x)∈A) = 1
and
∀(v,a)∈A y
|=Ev|xv=a.
For the if direction, note that if ENCODE -METER ϕq(e) = 0 ,
E(v,a)∼q(xe(x))I(y;Ev|xv=a) = 0 .
To show the forward direction, the above equality means that if ϕq(e) = 0 , almost surely for every
(v,a)∼q(xe(x)), the instantaneous mutual information is 0which implies the desired conditional
independence
I(y;Ev|xv=a) = 0 = ⇒y
|=Ev|xv=a.
By definition of almost surely, there exists a set Asuch that q(xe(x)∈A) = 1 the independence
above holds; this completes the "if" direction.
To show the reverse direction, let there exist a set Asuch that q(xe(x)∈A) = 1 , for every (v,a)∈
A,
y
|=Ev|xv.
In turn, for all (v,a)∈A,
I(y;Ev|xv=a) = 0 .
Then, the fact that q(xe(x)∈A) = 1 implies that expectations with respect to q(xe(x))over the
whole support equal expectations over q(xe(x)|xe(x)∈A), which is q(xe(x))restricted to A:
E(v,a)∼q(xe(x))I(y;Ev|xv=a) =E(v,a)∼q(xe(x)|xe(x)∈A)I(y;Ev|xv=a) = 0 .
This completes the "only if" direction.
Theorem 2. With finite H(y|x)andH(y), for any explanation that encodes eand any that does
not encode e′, there exists an α∗such that ∀α > α∗STRIPE -Xα(q, e′)>STRIPE -Xα(q, e).
Proof. Recall that STRIPE -Xis
STRIPE -Xα(q, e) :=E(v,a)∼q(xe(x))Eq(y|xe(x)=(v,a))[logq(y|xv=a)]−αϕq(e),
where the ENCODE -METER
ϕq(e) :=E(v,a)∼q(xe(x))I(Ev;y|xv=a).
We first show bounds for the first term in STRIPE -Xand then derive the STRIPE -Xscores for encoding
and non-encoding explanations.
Bounds on EVAL -Xscores. We lower bound the EVAL -Xscore, which is the first term in STRIPE -
X, for non-encoding explanations.
For non-encoding explanations, almost surely over v,a∼q(xe(x))
q(y|xe(x)= (v,a)) =q(y|xv=a).
Then,
EVAL -X(q, e)−Eq(y,xv=a)logq(y)
=E(v,a)∼q(xe(x))Eq(y|xe(x)=(v,a))logq(y|xv=a)−Eq(y,xe(x))logq(y)
=E(v,a)∼q(xe(x))Eq(y|xe(x)=(v,a))[logq 
y|xe(x)= (v,a)
]−Eq(y,xe(x))logq(y)
=Eq(y,xe(x))[logq 
y|xe(x)
]−Eq(y,xe(x))logq(y)
=Eq(y,xe(x))logq 
y|xe(x)
q(y)
=I(y;xe(x)).
20The above inequality implies that
EVAL -X(q, e)−E(v,a)∼q(xe(x))Ey∼q(y|xe(x)=(v,a))logq(y) =I(y;xe(x))≥0
=⇒ EVAL -X(q, e) +Hq(y)≥0
=⇒ EVAL -X(q, e)≥ −Hq(y).
Every inequality in the derivation above becomes strict when the explanation selects inputs that are
predictive of the label because
I(y;xe(x))>0.
Thus, non-encoding explanations have EVAL -Xscores that are at least −Hq(y).
The optimal EVAL -Xscore for any explanation (see Lemma 4) equals the negative conditional en-
tropy which is upper bounded by some finite number:
Eq[logq(y|x)] =−Hq(y|x) =C.
Comparing explanations via STRIPE -X.For any encoding explanation, by Proposition 3, for
some c >0
ϕq(e)> c.
Now, consider α∗=I(y;x)
c≥0, which is finite because each term in the ratio is finite. Then, for all
α > α∗
αϕq(e)> α∗ϕq(e)>I(y;x).
Thus,
−αϕq(e)<−I(y;x).
AsEVAL -Xscores are below C=−H(y|x)for any encoding explanation,
STRIPE -Xα(q, e) = EVAL -X(q, e)−αϕq(e)
<−H(y|x)−I(y;x)
<−H(y|x)−(Hq(y)−H(y|x))
=−Hq(y).
Finally, for any non-encoding explanation, ϕq(e′) = 0 by Proposition 3, STRIPE -Xscores equal
EVAL -Xscores, which are lower bounded at −Hq(y).
Together, for every non-encoding explanation e′(x)and encoding explanation e(x), it holds that
STRIPE -Xα(q, e′)≥ −Hq(y)>STRIPE -Xα(q, e).
This proves that STRIPE -Xis a strong detector of encoding.
B Encoding examples, non-detection of ROAR ,FRESH , and non-strong
detection of EVAL -X
B.1 An illustrative DGP for Def: Encoding
WithB(0.5)being a Bernoulli distribution, consider the following example
y∼ B(0.5),z∼ B(0.5), ϵ 1, ϵ2, ϵ3∼ N (0,I),
x=[y+ϵ1, ϵ 3,0, ϵ2]ifz= 0,
[ ϵ3,y+ϵ1,1, ϵ2]ifz= 1.
For this problem, if the third coordinate x3= 0, all the information between the label and the
covariates is in the first coordinate x1, and if x3= 1, the information is between the label and the
second coordinate x2. The corresponding explanation function is e(x) = 1[x3= 0]ξ1+ 1[x3=
1]ξ2. This explanation is encoding because neither explanation’s values x1norx2determine the
explanation function because it depends on x3. Formally
q(y= 1|x1,x3= 1)̸=q(y= 1|x1,x3= 0) = ⇒y
̸|=x3|x1=⇒y
̸|=Ev|x1,
21which meets Def: Encoding. Consider an alternate non-encoding explanation function e(x) =
[ 1[x4>0], 1[x4≤0],0,0];x1,x2do not determine e(x)that depends on the noise ϵ2inx4. That
means the unpredictability property in Lemma 1 holds. However, by construction,
(y,x1,x2)
|=ϵ2=⇒(y,x1,x2)
|=Ev=⇒y
|=Ev|x1and y
|=Ev|x2.
So no additional information about the label is encoded:
y
|=Ev|xv.
The additional information property in Lemma 1 avoids such cases where the explanations keeps
additional information that is irrelevant to the label.
B.2 Encoding explanations conceal predictive inputs that affect the explanation
Consider the following DGP
x= [x1,x2,x3]∼ B(0.5)⊗3, y=x1ifx3= 1,
x2ifx3= 0.(15)
Letebe an encoding explanation that selects the first coordinate if x3= 1and the second coordinate
otherwise. We never observe x3when looking only at the explanation. Table 4 shows all possible
values of this explanation. Notice that in the third and fourth rows, the value of xe(x)changes to
match the label yexactly, even though the values of the first two coordinates that we can observe
stay constant. It is impossible to understand the perfect predictiveness of xe(x), as the encoding
explanation conceals the control flow feature x3that determines which of the first two features
should be picked to predict the label.
Table 4: Possible values of the inputs, label, and explanation for the DGP in eq. (15)
xe(x)= (v,a)
x1x2x3y v a
0 0 0 0 [0, 1, 0] 0
0 0 1 0 [1, 0, 0] 0
0 1 0 1 [0, 1, 0] 1
0 1 1 0 [1, 0, 0] 0
1 0 0 0 [0, 1, 0] 0
1 0 1 1 [1, 0, 0] 1
1 1 0 1 [0, 1, 0] 1
1 1 1 1 [1, 0, 0] 1
B.3 Position-based encoding fits Def: Encoding
Recall the perceptual task that classifying images of dogs versus classifying images of cats, and
consider the encoding explanation eposition (x)that is
eposition (x) =ξ1ifq(y=dog|x) = 1 ,
eposition (x) =ξ2ifq(y=cat|x) = 1 .
Assume that the inputs in the top leftmost pixels are always background, meaning that the values
of these inputs provide no information about the label y
|=x1,x2. Now we check if this intuitively-
defined position-encoded explanation meets the definition for encoding (Def: Encoding). To condi-
tion on xξ1,Eξ1= 0, we need q(y=dog)̸= 1. Note that
q(Eξ1= 1|xξ1) =q(y=dog|xξ1) =q(y=dog)̸= 1.
Def: Encoding holds because the indicator of which explanation was chosen Eξ1determines the
label.
q(y=dog|xξ1,Eξ1= 1) = 1 ̸= 0 = q(y=dog|xξ1,Eξ1= 0) = ⇒y
̸|=Eξ1|xξ1.
This example shows how the encoding definition Def: Encoding captures the informally described
position-based encoding from the literature.
22B.4 Prediction-based encoding fits Def: Encoding
The informal example of prediction-based encoding from Section 3.1 selects a single input that
makes the prediction from all of the input have the highest confidence when given the single input.
One way to mathematically express such a selection is as follows:
eprediction (x) =ξargmaxiq(y=1|xi)ifq(y= 1|x)>0.5,
eprediction (x) =ξargminiq(y=1|xi)ifq(y= 1|x)≤0.5.(16)
Here, we describe one set of conditions on the distribution q(y,x)for which the explanation in
eq. (16) fits the definition of encoding in Def: Encoding. Assume that there exists a non-measure-
zero set U⊆ {x:q(y= 1|x)>0.5}and an index ksuch that
x∈U=⇒q(y= 1|x)≥ρ, (17)
x∈U=⇒ ∀i q(y= 1|xξi)< q(y= 1|xξk), (18)
x̸∈U=⇒q(y= 1|x)< ρ, (19)
x̸∈U=⇒ ∃i, j q (y= 1|xξi)> q(y= 1|xξk)> q(y= 1|xξj). (20)
Further, assume that xξkalone does not determine x∈U:
0<E[ 1[x∈U]|xξk]<1. (21)
The assumptions above imply the facts below about eprediction (x):
1. By eqs. (18) and (20)
x∈U⇔eprediction (x) =ξk.
Define the explanation indicator Ev= 1[eprediction (x) =v].
2. By eqs. (18) and (20) and the definition of Ev
x∈U⇔Eξk= 1. (22)
3. By eqs. (21) and (22)
0< q(Eξk= 1|xξk) =q(x∈U|xξk)<1. (23)
4. By eq. (23), q(y= 1|xξk,Eξk= 1) andq(y= 1|xξk,Eξk= 0) are well defined. Then,
by eqs. (17) and (19), for x∈U
q(y= 1|xξk,Ek= 1)
=Eq(x|xξk,Ek=1)q(y|x)
≥Eq(x|xξk,Ek=1)ρ {asEk= 1 = ⇒x∈U}
=ρ.
q(y= 1|xξk,Ek= 0)
=Eq(x|xξk,Ek=0)q(y|x)
<Eq(x|xξk,Ek=0)ρ {asEk= 0 = ⇒x̸∈U}
=ρ.
Thus, for all elements of {xξk:x∈U}
q(y= 1|xξk,Eξk= 1) > q(y= 1|xξk,Eξk= 0). (24)
By Lemma 1, the properties in eqs. (23) and (24) imply that ∀a∈ {xξk:x∈U}
y
̸|=Eξk|xξk=a.
Finally, the set Uk={xξk:x∈U}is non-measure-zero: as 1[x∈U] = 1 = ⇒ 1[xξk∈Uk] =
1, accumulating q(x)with the restriction xξk∈Ukleads to at least as much mass as accumulating
with the stricter restriction x∈U:
q(xξk∈Uk) =Z
q(x) 1[xξk∈Uk]dx≥Z
q(x) 1[x∈U]dx=q(x∈U)>0.
Together, the last two equations implies that Def: Encoding holds for eprediction (x)from eq. (16).
23cat dog
(a)Distribution q(x,y)
xe(x) xe(x)
(b)Non-encoding explanation
xe(x)e(x)
xe(x)e(x)
(c)Marginal encoding
Figure 5: Example DGP and MARG encoding. (a)The color determines whether the label is pro-
duced from the top or bottom image. (b)An explanation that correctly reveals that the label is
generated based on both the color and, as dictated by the color, the top or the bottom image. The
label is deterministic given the value of the explanation which means the label can be predicted per-
fectly. (c)An encoding explanation would be one that produces only the top or the bottom animal
image based on the color being red of blue respectively. This returned animal image does not indi-
cate the fact that the data generating process depends on color. Now, the animal image selected by
the explanation alone is insufficient to dictate the label because the color determines which image
determines the label. The identity of the image, whether top or bottom, provides additional infor-
mation about the label beyond the values explanation, as captured in Def: Encoding.
B.5 MARG explanations are encoding
We provide an illustrative example of a MARG explanation for the DGP in Figure 5. Here, we show
how a mathematical formulation of MARG satisfied Def: Encoding.
Consider a generic DGP with a Bernoulli control flow input denoted xc: for some distinct sets U, V
that do not include cand let no combination of xc,xU,xVdetermine the rest
xc= 1 = ⇒q(y|x) =q(y|xU),
xc= 0 = ⇒q(y|x) =q(y|xV).
Further, assume that the two subsets leads to different distributions over y= 1 such that on a
non-zero measure subset SU⊆ {xU:xsuch that xc= 1}
q(y= 1|xU,xc= 1)̸=q(y= 1|xU,xc= 0). (25)
Now, consider a MARG explanation that looks at xcand outputs the corresponding sets U, V :
e(x) =Uifxc= 1 else e(x) =V.
By definition the explanation only depends on the control flow input, not by xUorxV. Next, as
EU= 1is the same event as xc= 1,e(x)is encoding because the assumption from eq. (25) implies:
q(y|xU,EU= 1)̸=q(y|xU,EU= 0).
Then, this inequality holds for all elements of the non-measure-zero set SU, by Lemma 1, MARG is
encoding.
B.6 Proof of Proposition 1
Definition 4. We denote val(xe(x))as the function that maps explanation xe(x)= (v,a)to the
values the inputs take at the selected indices, right-padded to have the same dimension as the input
x∈Rd:
val(xe(x))j=(
aj if1≤j≤Pd
i=1vi
pad-token ifPd
i=1vi< j≤d
For example, if x= [α, β, γ ]ande(x) = [0 ,1,0], then xe(x)= ([0 ,1,0],[β])and
val(xe(x)) = [β,pad-token ,pad-token ].
24Table 5: Probability table for the DGP in eq. (3). Conditional on the explanation x3, does predict
the label. For example, given knowing x1= 1, ifx3= 1 implies p(y= 1) = 0 .9but if x3= 0,
p(y= 1) = 0 .5. The probability table in Table 5 shows this.
x1x2x3p(y= 1|x)e(x) val(xe(x))
0 0 0 0.1 ξ2 [0,pad-token ,pad-token ]
0 0 1 0.1 ξ1 [0,pad-token ,pad-token ]
0 1 0 0.9 ξ2 [1,pad-token ,pad-token ]
0 1 1 0.1 ξ1 [0,pad-token ,pad-token ]
1 0 0 0.1 ξ2 [0,pad-token ,pad-token ]
1 0 1 0.9 ξ1 [1,pad-token ,pad-token ]
1 1 0 0.9 ξ2 [1,pad-token ,pad-token ]
1 1 1 0.9 ξ1 [1,pad-token ,pad-token ]
Proposition 1. For the DGP in eq. (3),ROAR and FRESH assign their respective optimal scores to
the encoding explanation eencode(x).
Proof. First, note that
x3
|=y.
See Table 5 for the probability table for why this is true.
For this proof let e(x) =eencode(x). In the example eq. (3), masking out the inputs selected by e(x)
would mean that
x3= 1 = ⇒x−e(x)= (1−e(x),[x2,x3]), x3= 0 = ⇒x−e(x)= (1−e(x),[x1,x3]).
In turn, by the construction in eq. (3)
x−e(x)
|=y|x3= 1,
x−e(x)
|=y|x3= 0.=⇒x−e(x)
|=y|x3=⇒(x−e(x),x3)
|=y=⇒x−e(x)
|=y,(26)
where the conditional independence in the second step turns into the joint independence in the third
step due to x3
|=y.
ROAR scores an explanation highly if x−e(x)predicts the label poorly. So if x−e(x)is independent of
y, then e(x)would be scored optimally. Then, due to eq. (26), ROAR scores an encoding explanation
optimally.
FRESH scores an explanation highly if the selected value val(xe(x))(as defined in Definition 4)
predicts the label well. From Table 5, we see that p(y= 1|x) = 0 .9for all cases with
val(xe(x)) = [1 ,pad-token ,pad-token ]andp(y= 1|x) = 0 .1for all cases with val(xe(x)) =
[0,pad-token ,pad-token ]. Thus, if val(xe(x)) = [1 ,pad-token ,pad-token ]then
p(y= 1|x) = 0 .9
=Ex′∼p(x′|val(xe(x))=[1,pad-token ,pad-token ])[0.9]
=Ex′∼p(x′|val(xe(x))=[1,pad-token ,pad-token ])[p(y= 1|x′)]
=p(y= 1|val(xe(x)) = [1 ,pad-token ,pad-token ]),
and if val(xe(x)) = [0 ,pad-token ,pad-token ]then
p(y= 1|x) = 0 .1
=Ex′∼p(x′|val(xe(x))=[0,pad-token ,pad-token ])[0.1]
=Ex′∼p(x′|val(xe(x))=[0,pad-token ,pad-token ])p(y= 1|x′)
=p(y= 1|val(xe(x)) = [0 ,pad-token ,pad-token ]).
Therefore, in all cases, p(y= 1|x) =p(y= 1|val(xe(x))). Thus, predicting label from the
selected value alone is as good as predicting from the whole input. As a result, FRESH scores this
explanation optimally.
25B.7 Showing that eencode is the optimal reductive explanation for eq. (3) and scores better
than a constant explanation under EVAL -X
We repeat the DGP in eq. (3) here
x= [x1,x2,x3]∼ B(0.5)⊗3,
y=x1w.p.0.9else 1−x1ifx3= 1,
x2w.p.0.9else 1−x2ifx3= 0.
Lemma 5. In the DGP in eq. (3),
q(y= 1|x1= 1) = 0 .7
q(y= 1|x2= 1) = 0 .7,
q(y= 1|x1= 0) = 0 .3,
q(y= 1|x2= 0) = 0 .3.
q(y= 1|x3) = 0 .5.
Proof. We can compute these values from Table 5.
Proposition 2. Letec(x) =ξ3. Then, for the DGP in eq. (3),EVAL -X(q, e encode)>EVAL -X(q, ec).
Proof. By Lemma 5, we have
EVAL -X(q, ec) =Eq(y,x)logq(y|x3) =Eq(y,x)log 0.5≈ −0.69.
Now, denote eencode aseefor ease of reading
EVAL -X(q, ee) =E(v,a)∼q(xe(x))Ey∼q(y|xe(x)=(v,a))[logq(y|xv=a)]
=q(x3= 1)Eq(x1)Eq(y|x1,x3=1)logq(y|x1)
+q(x3= 0)Eq(x2)Eq(y|x2,x3=0)logq(y|x2)
= 0.5∗0.5∗(0.9∗ −log 0.7 + 0 .1∗ −log 0.3)∗2
+ 0.5∗0.5∗(0.9∗ −log 0.7 + 0 .1∗ −log 0.3)∗2
≈ −0.44.
This concludes that EVAL -X(q, e encode)>EVAL -X(q, ec).
Lemma 6. In the DGP in eq. (3),eencode(x)is an EVAL -X-optimal reductive explanation and is
encoding.
Proof. First, the following properties show for the DGP because when x3= 1,yonly depends on
x1, and if x3= 0,yonly depends on x2:
x3
|=y,y
|=x2|x3= 1 ,y
|=x1|x3= 0.
These independencies imply that
q(y|x= [x1,x2,1]) = q(y|x1,x3= 1) , q(y|x= [x1,x2,0]) = q(y|x2,x3= 0).
Then, the optimal explanation function that achieves EVAL -X∗ise(x) = [1 ,0,1]ifx3= 1 and
[0,1,1]otherwise.
26Reductive explanations of size 1. If the explanation is forced to have fewer than 2inputs, the
optimal reductive explanation e(x)is only allowed to be one of ξ1, ξ2, ξ3:
max
e:|e(x)|≤1Eq(y,x)X
i∈{1,2,3}1[e(x) =ξi]q(y|xi).
Rewriting this expression to split the support of xbased on x3= 1or0:
Eq(y,x)X
i∈{1,2,3}1[e(x) =ξi] logq(y|xi)
=q(x3= 1)Eq(x2|x3=1)Eq(y,x1|x3=1)X
i∈{1,2,3}1[e(x) =ξi] logq(y|xi)
+q(x3= 0)Eq(x1|x3=0)Eq(y,x2|x3=0)X
i∈{1,2,3}1[e(x) =ξi] logq(y|xi)
= 0.5Eq(x2)Eq(y,x1|x3=1)X
i∈{1,2,3}1[e(x) =ξi] logq(y|xi)
+ 0.5Eq(x1)Eq(y,x2|x3=0)X
i∈{1,2,3}1[e(x) =ξi] logq(y|xi)
=1
2 
Eq(x2)"
Eq(y,x1|x3=1) 1[e([x1,x2,1]) = ξ1] logq(y|x1)
+Eq(y,x1|x3=1)X
i∈2,31[e([x1,x2,1]) = ξi] logq(y|xi)#
+Eq(x1)"
Eq(y,x2|x3=0) 1[e([x1,x2,0]) = ξ2] logq(y|x2)
+Eq(y,x2|x3=0)X
i∈1,31[e([x1,x2,0]) = ξi] logq(y|xi)#!
=1
2Eq(x1,x2|x3=1)
1[e([x1,x2,1]) = ξ1]Eq(y|x1,x3=1)logq(y|x1)
+X
i∈2,31[e([x1,x2,1]) = ξi]Eq(y|x1,x3=1)logq(y|xi)
(27)
+1
2Eq(x1,x2|x3=0)
1[e([x1,x2,0]) = ξ2]Eq(y|x2,x3=0)logq(y|x2)
+X
i∈1,31[e([x1,x2,0]) = ξi]Eq(y|x2,x3=0)logq(y|xi)
.
(28)
We will now focus on the three terms within each of eq. (27) and eq. (28). Due to the following
equality
q(y= 1|x1= 1,x3= 1) = q(y= 0|x1= 0,x3= 1)
=q(y= 1|x2= 1,x3= 0) = q(y= 0|x2= 0,x3= 0) = 0 .9,
the expectations in the first terms in each of eq. (27) and eq. (28) are
Eq(y|x1,x3=1)logq(y|x1) =Eq(y|x2,x3=1)logq(y|x2) = 0 .9 log 0 .7 + 0 .1 log 0 .3≈ −0.44.
Next we turn to setting i= 1term in eq. (27). Due that q(y= 1|x2= 1) = q(y= 0|x2= 0) ,
x1=x2=⇒Eq(y|x1,x3=1)logq(y|x2) = (0 .9 log 0 .7 + 0 .1 log 0 .3)≈ −0.44,
27x1̸=x2=⇒Eq(y|x1,x3=1)logq(y|x2) = (0 .9 log 0 .3 + 0 .1 log 0 .7)≈ −1.12.
The same equalities hold for the i= 2 term in eq. (28) Eq(y,|x2,x3=0)logq(y|x1). Finally,
regardless of x1,x2, thei= 3terms in both eq. (27) and eq. (28) can be expressed as follows:
Eq(y|x1,x3=1)logq(y|x2)Eq(y|x2,x3=0)logq(y|x1) = (0 .9 log 0 .5 + 0 .1 log 0 .5)≈ −0.69.
Now we can maximize the sum of eq. (27) and eq. (28), over e(x)such that |e(x)|= 1.
Notice that setting 1[e(x) =ξ1] = 1 when x3= 1 and 1[e(x) =ξ2] = 1 when x3= 0 achieves
the highest score −0.44in each of eq. (27) and eq. (28). This implies that one optimal reductive
explanation is ξ1= [1,0,0]ifx3= 1andξ2= [0,1,0]otherwise. This is an encoding explanation
as we show below. Due to Eξ1=x3,
q(y= 1|x1,Eξ1= 1) = 0 .9̸=q(y= 1|x1,Eξ1= 0) = 0 .5.
In turn, y
̸|=Eξ1|xξ1for{x:e(x) =ξ1}and Def: Encoding holds, meaning that e(x)is encoding.
B.8 An example of misestimation of EVAL -X
Consider the following example where
x= [x1,x2,x3,x4],
x2,x3∼ B(0.5)⊗2,x1,x4∼ N(0,I),y=x2⊕x3.
Assume that the misestimated EVAL -Xmodel satisfies these equalities
qmisestimated
ξ1(y= 1|x1) = 1 for all x1,
qmisestimated
ξ4(y= 0|x4) = 1 for all x4.
There exists a bad explanation that scores optimally under the misestimated EVAL -X:
e(x) =ξ1 ifx2⊕x3= 1,
ξ4 ifx2⊕x3= 0.
Then the EVAL -Xscore of this explanation under this particular misestimation is
EVAL -Xmisestimated(q, e) =Eq[logqmisestimated
e(x) (y|xe(x))]
=q(y= 1)Eq[logqmisestimated
e(x) (y|xe(x))|y= 1] + q(y= 0)Eq[logqmisestimated
e(x) (y|xe(x))|y= 0]
= 0.5·Eq[logqmisestimated
ξ1(y|x1)|y= 1] + 0 .5·Eq[logqmisestimated
ξ4(y|x4)|y= 0]
= 0.
Since yis deterministic given xso the maximum value of the EVAL -Xscore is also 0. So the
bad explanation scores optimally due to misestimation. Deterministic y|xis not necessary for
estimation error to affect explanation quality. Here, with this incorrectly estimated EVAL -X, inputs
that are pure noise, independent of everything, will be chosen.
B.9 Attention map explanations be encode.
Here, treating each of the coordinates of xas tokens, we consider a cross-attention based predictive
model of the following form: with γ(a)as softmax function over a vector a,Was a matrix, α,βas
vectors, and σas the sigmoid function, and κas the temperature, the predictive model f(·)is
f(x) =σ
X
iβi
X
jγ(κxi∗Wx)jαjxj

.
28We then show that using the highest attention score as the explanation produces an encoding expla-
nation. For this example, we consider the following DGP:
z1,z2,z3∼ B(0.5)⊗3,
z+= [z1+ 1,0,+1],
z−= [0 ,−z2−1,−1],
x=z+ifz3= 1,
z−ifz3= 0,
y∼ B(ρ)where ρ=σ(x1)ifx3= 1,
σ(−x2)ifx3=−1.(29)
The following setting of parameters in f(x)produces a function of xthat converges to ρasκ→ ∞ :
α= [1,−1,0], β= [1,1,0], W = 0 0 0
0 0 0
0 0 1!
.(30)
By definition of W
Wx=[0,0,1]ifx3= 1
[0,0,−1]ifx3=−1
=⇒x1Wx=[z1+ 1,0,0]ifx3= 1
[0,0,0]ifx3=−1=⇒γ(x1Wx)κ→∞−→[1,0,0]ifx3= 1
[0,0,0]ifx3=−1
=⇒x2Wx=[0,0,0]ifx3= 1
[0,z2+ 1,0]ifx3=−1=⇒γ(x2Wx)κ→∞−→[0,0,0]ifx3= 1
[0,1,0]ifx3=−1.
Then, β3= 0, the inner sum for i= 3 does not appear in the function f(x)Then, as α3= 0,
α1= 1, α2=−1,
X
jγ(κx1∗Wx)jαjxjκ→∞−→x1ifx3= 1
0ifx3=−1, (31)
X
jγ(κx2∗Wx)jαjxjκ→∞−→0ifx3= 1
−x2ifx3=−1. (32)
In turn, as β1=β2= 1
X
i∈1,2βi
X
jγ(κxi∗Wx)jαjxj
κ→∞−→x1ifx3= 1
−x2ifx3=−1,
f(x) =σ
X
i∈1,2βi
X
jγ(κxi∗Wx)jαjxj

κ→∞−→σ(x1)ifx3= 1
σ(−x2)ifx3=−1. (33)
So, as κ→ ∞ , the function f(x), with the parameters in eq. (30), converges to ρ(x), meaning that
this model will achieve the population log-likelihood optimum under the DGP in eq. (29).
Now, the attention map as an explanation selects x1ifx3= 1 andx2otherwise; this comes from
eq. (31) and eq. (32). This is an encoding explanation because Eξ1= 1ifx3= 1which gives
q(y|x1)̸=q(y|x1,x3= 1) = ⇒y
̸|=Eξ1|xξ1.
C Experimental details
C.1 Estimating STRIPE -X
To compute the KL term in ENCODE -METER , we estimate q(Ev|xv,y)andq(Ev|xv). To
estimate these, we train a single model — to predict Evfromxvand a new variable ℓthat can equal
29the label yor a dummy value null that is outside the support of y— in the following way:
arg max
θEx,y∼q(x,y)h
logpθ(Ev= 1[e(x) =v]|xv, ℓ=y)
+ log pθ(Ev= 1[e(x) =v]|xv, ℓ=null )i
. (34)
As log-likelihood is a proper scoring rule and q(y=null ) = 0 , the maximum above is achieved
when
pθ(Ev|xv, ℓ=y) =q(Ev|xv,y=y) pθ(Ev|xv, ℓ=null ) =q(Ev|xv).
In summary, to estimate ENCODE -METER , solve eq. (34), use its solution to estimate the KL term
from the RHS in eq. (7) for each xv,y, and then average this KLterm over samples of xvfrom the
data such that e(x) =vand samples of yfrom the EVAL -Xmodel for q(y|xv).
In practice, one does not need train a model for each v. We describe how to estimate ENCODE -
METER with a single model in Appendix C.2. We give the full STRIPE -Xestimation procedure
in Algorithm 2 in Appendix D.
C.2 Estimating the encoding cost in STRIPE -Xwith categorical predictive models
STRIPE -Xconsists of the EVAL -Xscore and a cost of encoding measured by ENCODE -METER . De-
fineVto be the set of possible explanations and let V[j]denote the jth element of V. The EVAL -X
model pγ(y|xv)is trained to predict the label yfrom subsets xvwhere vis uniformly sam-
pled from V[20]. Next is computing the ENCODE -METER ϕq(e)that is used in the encoding cost
term in STRIPE -X. For each explanation, let Fbe the categorical variable (instead of an indicator
Ev) that denotes, for each sample, which inputs were selected by the explanation e(x):F=jif
EV[j]= 1[e(x) =V[j]] = 1 . Let q(j)be the distribution over jinduced by q(e(x)). We train a
model pθ(F|xv, ℓ,v)with a modification of eq. (34) that averages over v∼q(e(x)):
arg max
θEv∼q(e(x))Ex,y∼q(x,y)X
V[j]∈V
1[e(x) =V[j]]
logpθ(F=j|xv, ℓ=y,v)+
logpθ(F=j|xv, ℓ=null,v)
.
(35)
The variable ℓtakes values in {−1,0,1}where 0and1correspond to y= 0andy= 1respectively
and−1corresponds to the null value. For a flexible enough model pθthat achieves the population
maximum of eq. (35), for any v=V[j]∈ V,
pθ(F=j|xv, ℓ=y,v) =q(Ev= 1|xv,y=y),
pθ(F=j|xv, ℓ=null,v) =q(Ev= 1|xv).
This fact indicates how one can use the model pθto estimate ENCODE -METER . First, construct the
explanation dataset De={(y,xe(x))}fromDt. Define qDeto be the uniform distribution over De.
Define E(v,a)as the uniform distribution over Ksamples of yfrom the EVAL -Xmodel:
E(v,a)=U[{ˆy}k≤K]{where ˆyk∼pγ(y|xv=a)}.
Then, estimate ENCODE -METER as follows:
ˆϕ(q, e) =E(v,a)∼qDe(xe(x))Eˆy∼E(v,a)
pθ(F=j|xv, ℓ=ˆy,v) logpθ(F=j|xv, ℓ=ˆy,v)
pθ(F=j|xv, ℓ=null,v)
+pθ(F̸=j|xv, ℓ=ˆy,v) logpθ(F̸=j|xv, ℓ=ˆy,v)
pθ(F̸=j|xv, ℓ=null,v)
.
C.3 Estimating ENCODE -METER with a generative model
When estimating the STRIPE -Xscore with procedure above for many different explanations, the
maximization in eq. (34) repeated for every explanation, which can be computationally expensive.
30Table 6: Position-based, prediction-based, and marginal explanation schemes are all encoding. For
samples in the set {x:e(x) =v}for one of the selections vthateproduces, accuracy <1and the
KLbeing non-zero means these explanations are all encoding per Lemma 1.
Encoding Acc. Ev(↑)KL(↓)
POSI 0.61 0 .88
PRED 0.51 0 .18
MARG 0.51 0 .20
This motivates a second procedure to estimate ENCODE -METER that avoids having to retrain models
for each explanation by using generative model for q(x|xv,y). Formally, with xvfixed, the
conditional mutual information term in eq. (5) can be computed as the marginal dependence between
Nsamples of yfrom q(y|xv)andq(Ev|xv,y). The model for the former is available from
EVAL -Xestimation. Simulating from the later, namely q(Ev|xv,y), is done by sampling from the
generative model x|xv,yand then computing the indicator Evas 1[e(x) =v]. Mechanically, with
an estimator of mutual information from samples ( {ai}i≤N,{bi}i≤N) denoted MI({ai},{bi})and
with samples {ai}produced conditionally on values cidenoted by a subscript of the conditioned
value{ai}ci, one can estimate ENCODE -METER as follows: sample yi
(v,a)∼y|xv=aand
xi
v,a,yi∼q(x|xv=a,y=yi)repeatedly Ntimes and compute
E(v,a)∼q(xe(x))MI 
{yi}(v,a),{ 1[e(xi) =v]}v,a,yi
.
We give the full procedure in Algorithm 1.
C.4 Experimental details from the simulated study.
The data-generating processes from the experiments. LetNbe the standard normal distribution
and let B(α)be the Bernoulli distribution with 1occurring with probability α. With ρ= 0.9, the
discrete DGP is:
x= [x1,x2,x3,x4,x5]∼ B(0.5)⊗5, y=x1w.p.ρelse 1−x1ifx3= 1,
x2w.p.ρelse 1−x2ifx3= 0.
(36)
The hybrid DGP is as follows: with γ= 5andσ(x) =1
1+exp( −x)as the sigmoid function
x= [x1,x2,x4,x5]∼ N(0.5)⊗4,x3∼ B(0.5), ρ =σ(γx1)ifx3= 1,
σ(γx2)ifx3= 0,y∼ B(ρ).(37)
Computing accuracy and KLto show that POSI ,PRED ,MARG are encoding. For each encod-
ing type, we build two decision trees from 1000 samples from eq. (36): the first decision tree learns
q(Ev|xv)and the second learns q(y|xv,Ev=b)forb∈ {0,1}. We set the maximum depth to
be6. Trees of this depth learn any function of 6binary digits; xwithEvas an additional column
amounts to 6binary digits. These decision trees are used to compute the accuracy of predicting Ev
withq(Ev|xv)and the KL between q(y|xv,Ev= 1) andq(y|xv,Ev= 0) . Within a set
{x:e(x) =v}that is all xthat have one of the possible selections v, Table 6 report the accuracy
of predicting Evwithq(Ev|xv)and the KLbetween q(y|xv,Ev= 1) andq(y|xv,Ev= 0) ,
averaged only over samples in {x:e(x) =v}.
EVAL -X.To estimate EVAL -Xfor the DGPs in eq. (36) and eq. (37), we compute conditionals
q(y= 1|xv)via Monte Carlo approximation. Due to the different coordinates of xbeing inde-
pendent, one can compute q(y= 1|xv)as a marginal expectation over the inputs except those in
v:
q(y|xv) =Eq(xcv|xv)q(y|xv,xc
v) =Eq(xcv)q(y|x).
We Monte Carlo estimate the RHS of this equation over 500resamples of xc
v. We take 5000 samples
from each DGP to estimate EVAL -Xscores with respect to q(y,x). In Appendix C.5 we also show
experiment results where we use the EVAL -Xaccuracy and AUROC as the score instead.
31(a)Results: Discrete DGP.
 (b)Results: hybrid DGP.
Figure 6: The EVAL -X-ACC and AUROC scores for the different explanations for the discrete DGP are on
the left and the scores for the hybrid DGP are in the right. In both, multiple encoding explanations ( PRED ,
MARG , and the reductive one from REAL -X) all achieve the same score as the corresponding EVAL -X∗score.
Thus, ranking metrics like accuracy and AUROC are not sensitive to encoding explanations like EVAL -Xlog-
likelihoods, and can fail to even weakly detect encoding. This stems from the fact that accuracy and AUROC
only depend on the ranking of the datapoint, and therefore are not sensitive to differences in log probabilities
that do not change ranks.
REAL -X.We solve REAL -Xfor any specified explanation size Kas follows. In the case of the
discrete DGP, for each possible value of x∈supp(q(x))(of which there are finitely many), we
make e(x)output the subset of at most size Kthat achieves that maximum averaged log-likelihood
over the samples that equal said value x=x. This produces the optimally-scoring explanation
e(x)that maps each finite value in the support of q(x)to one subset of the coordinates of x. To do
the same in the continuous DGP in eq. (37), we round xto integers and then use the same type of
optimization as in the discrete case.
STRIPE -X.In estimating ENCODE -METER , the model pθ(Ev|xv, ℓ)is a decision tree of depth
at most 5, which is then used to estimate averaged KL in the RHS of eq. (7) with a single sample
fromy|xv. The process is repeated for each vand averaged to produce the ENCODE -METER . The
simulated experiments were done on a CPU with the whole runtime around 10minutes.
C.5 Experiments with EVAL -Xaccuracy and EVAL -XAUROC
Here, instead of EVAL -Xlog-likelihoods, we use the accuracy and AUROC of the EVAL -Xmodel as
the score. We call these EVAL -X-ACC and EVAL -X-AUROC scores. These metrics only depend on
the ranking of the datapoints, and therefore are not sensitive to differences in log probabilities that
do not change ranks. Figure 6 shows that, due to this insensitivity, multiple encoding explanations
(PRED ,MARG , and the excessively reductive one) all achieve the same score as the correspond-
ing EVAL -X∗score. In summary, ranking metrics like accuracy and AUROC are not sensitive to
encoding explanations like EVAL -Xlog-likelihoods.
C.6 Classifying dogs and cats.
The POSI explanation selects the upper or the lower color patch depending on whether q(y=
1|x)>0.5or not. The PRED explanation selects the patch predicting from which best matches
the prediction from q(y= 1|x). The MARG explanation selects the top or the bottom image patch
based on the color as in the DGP in Figure 5. We consider two non-encoding explanations. The first
explanation, denoted optimal , selects exactly the features that occur in the DGP:{x1,x2}if the
color patch x1isblue and{x1,x4}otherwise. As yis determined by the explanation, meaning
y
|=Ev|xvfor all vand values xv, this explanation is non-encoding. The second one, denoted
fixed , always outputs the bottom right patch x4; this explanation is constant which violates the
first criterion in Lemma 1 meaning there is no encoding.
We also run the REAL -Xmethod from [20] to produce an explanation. REAL -Xwas run over expla-
nations that select one of the four quarter patches and exact marginalization over the selections.
The base cat and dog images were obtained from the cats_vs_dogs dataset from the Tensorflow
datasets package. To construct images like in Figure 5, the color and the two images are sampled
independently. The color being blue/red determines that the label associated with the top/bottom
image becomes the label for the constructed image. The training, validation, and test dataset consist
of8000,1000,and1000 samples respectively.
32We follow the procedure in Appendix C.2 to estimate STRIPE -X. The EVAL -Xmodel is a pre-trained
18-layer residual network. The model pθ(F|xv, ℓ,v)used in computing the ENCODE -METER term
inSTRIPE -X(eq. (6)) are 34-layer Residual neural networks. The EVAL -Xmodel is trained for 100
epochs with a batch size of 100with the Adam optimizer, with the learning rate and weight decay
parameters set to 10−3and0respectively. The pθ(F|xv, ℓ,v)model is trained for 50epochs with
a batch size of 200with the Adam optimizer, with the learning rate and weight decay parameters set
to5×10−5and1respectively. The pθmodel sees variable ℓthrough an entire extra channel where
all the pixels take the value ℓ. We used validation loss as the metric to early stop. The EVAL -Xand
STRIPE -Xscores are computed on the test dataset. The cats vs. dogs experiment were done on an
A100 GPU where the whole training and evaluation ran in less than 20 minutes.
Remark on the gap between FRESH and EVAL -Xscores for the optimal explanation. As
the optimal explanation selects features sufficient to produce the label, meaning y
|=x|xvor
y
|=x|val(xe(x)),FRESH and EVAL -Xlog-likelihoods should be the same as predicting from the
full feature set. One potential reason there is a gap between the two scores in table 3 is that the
FRESH and EVAL -Xscores are computed with ResNet18 models that solve prediction problems of
different levels of difficulty On one hand, FRESH is computed with a model trained for a single
prediction task: predict yfrom val(xe(x)). On the other hand, EVAL -Xis computed with a model
trained for a more complicated task: for a range possible v, predict yfromxv. Using large models
with appropriate regularization, such as weight decay, should mitigate the gap in scores.
C.7 LLM experiment details
We generate 10,000reviews of the following type: with ADJ1 andADJ2 as adjectives, the review is
•‘My day was <ADJ1> and the movie was <ADJ2>. that is it’ or
•‘My day was <ADJ1> and the movie was <ADJ2>. oh wait, reverse the adjectives’ .
The second sentence in the review acts as a "control flow" input and determines whether ADJ1 or
ADJ2 describes the sentiment about the movie. We prompted Llama 3 to predict the sentiment and
select words relevant to predicting the sentiment. In Appendix C.8, we give the prompts we used
to make Llama 3 produce explanations from. For this problem, the inputs xare the reviews and
Llama 3 produces explanations e(x)that select a subset of words in the review. The summaries
and explanations were generated for all 10,000samples but to estimate STRIPE -X, we only used
data from the 5most common explanations (we restricted to inputs whose explanations vhad high
q(e(x) =v)). This resulted in a dataset of size 8136 , which we split into a training, validation, and
test datasets of sizes 6102,1017,and1017 respectively.
Both the EVAL -Xmodel and the model for pθ(F|xv,v, ℓ)(see Appendix C.2) used in estimating
the ENCODE -METER term in STRIPE -Xwere finetuned GPT-2 models. For the EVAL -Xmodel, we
used the AdamW optimizer with a batch size of 100and trained for 50epochs with the learning rate
set to 5e−5, weight decay set to 0, and a Cosine learning rate scheduler with the number of cycles
set to 1. For the pθmodel used in estimating ENCODE -METER , we used the AdamW optimizer with
a batch size of 50and trained for 25epochs with the learning rate set to 5e−5, weight decay set to
0, and a Cosine learning rate scheduler with number of cycles set to 1. The pθmodel sees variable
ℓthrough the following word added to the input sequence of words: positive ifℓ=y= 1,
negative ifℓ=y= 0, and nothing if ℓ=null . We used validation loss to early stop. We
follow the procedure in Appendix C.2 to compute ENCODE -METER withpθ(F|xv,v, ℓ)on the test
data with the averaging over y|xvestimated using a 5samples per value of xv. All training and
inference for this experiment was done on an A100. The explanation step and the estimation for
both parts of STRIPE -Xtogether took under 2 hours. The LLM-generated explanations achieves an
EVAL -Xscore of −0.497and an ENCODE -METER value was 0.114.
C.8 Prompts used to predict sentiment and produce explanation
In Figure 7, we provide the prompt we used to predict the sentiment from a review and generate an
explanation for that prediction.
Figure 7: Llama 3 prompt used to predict sentiment and generate an explanation for that prediction.
33System: You are a helpful and honest assistant. Please, respond concisely and
truthfully.
You are asked to summarize movie reviews of the form "first sentence. second
sentence".
The following are examples along with the reasoning.
Consider ‘My day was moving and the movie was overblown. that is it.’
The second sentence means the second adjective ’overblown’ describes the movie.
Due to this description, the sentiment is negative.
Consider ‘My day was moving and the movie was overblown. oh wait, reverse the
adjectives.’
The second sentence means the first adjective ’moving’ describes the movie.
Due to this description, the sentiment is positive.
These are all examples.
user: What is the sentiment about the movie in this review ‘<REVIEW>‘?
Think step-by-step about this latest review. If the second sentence instructs it,
switch the adjectives and then based on the new descriptor of the movie, answer
either ’positive’ or ’negative’.
Explain why you chose those this sentiment by selecting as few words as possible
from the review. Include all the words that you looked at.
Use this helpful format: "the sentiment is <positive/negative> and the explanation
is <words, ...>. END. "
D Algorithms
Algorithm 1 describes an alternate way to estimate the ENCODE -METER component of STRIPE -X
with a conditional generative model. Algorithm 2 describes the predictive version of STRIPE -X
estimation, which we used in our experiments.
34Algorithm 1: ENCODE -METER , generative version.
Input: Training data D∼q(y,x)and test data Dt∼q(y,x), explanation function e(x),
penalty weight λ.EVAL -Xmodel pγ(y|xv). Conditional generative model
pθ(x|xv,y)and mutual information estimator that takes two sets as arguments
MI[{ci},{di}];
Result: Return estimate of STRIPE -X:
1Define J(v,a)as the set of Krandom samples of yfrom the EVAL -Xmodel:
J(v,a)={ˆyk}k≤K{where ˆyk∼pγ(y|xv=a)}
2Define L(v,a,J(v,a))as the set of Krandom samples of xfrom pθconditioned on aandˆy:
L(v,a,J(v,a))={ 1[e(ˆxk) =v]}k≤K{where ˆxk∼pθ(x|xv=a,y=ˆyk)}
3Construct the explanation dataset De={(xe(x)= (e(x),a))}from Dt.
4Define qDeto be the uniform distribution over De.
5Compute the following averaging of estimated mutual information between, J,L
ˆϕq(e) =E(v,a)∼qDe(xe(x))MIh
J(v,a),L(v,a,J(v,a))i
6Return ˆϕ(q, e)as the ENCODE -METER estimate.
35Algorithm 2: STRIPE -X, predictive version.
Input: Training data D∼q(y,x)and test data Dt∼q(y,x), explanation function e(x),
penalty weight λ. Specifications for the models pγ(y|xv)andpθ(F|xv, ℓ).
Result: Return estimate of STRIPE -X:
1Define qDto be the uniform distribution over D
2Construct the explanation dataset De={(y,xe(x))}from Dt
3Define qDeto be the uniform distribution over De.
4Estimate eval-x ()
5 Solve the following minimization problem to learn pγ(y|xv)
arg max
γEv∼qD(e(x))Ex,y∼qD(x,y)
logpγ(y|xv)
Output: pγ
6Estimate encode-meter ()
7 Construct the set of possible selections V={v:q(e(x) =v)>0}
8 Construct data of the form (x,F)where F=jifEV[j]= 1.
9 Fit the model pθ(F|xv, ℓ)via the following log-likelihood maximization:
arg max
θEv∼qD(e(x))Ex,y∼qD(x,y)X
V[j]∈V
1[e(x) =V[j]]
logpθ(F=j|xv, ℓ=y,v)
+ log pθ(F=j|xv, ℓ=null,v)
10
11 Define E(v,a)as the uniform distribution over Ksamples of yfrom the EVAL -Xmodel:
E(v,a)=U[{ˆy}k≤K]{where ˆyk∼pγ(y|xv=a)}
Output: The following nested expectation over qDeandE(·):
E(v,a)∼qDe(xe(x))Eˆy∼E(v,a)
pθ(F=j|xv, ℓ=ˆy,v) logpθ(F=j|xv, ℓ=ˆy,v)
pθ(F=j|xv, ℓ=null,v)
+pθ(F̸=j|xv, ℓ=ˆy,v) logpθ(F̸=j|xv, ℓ=ˆy,v)
pθ(F̸=j|xv, ℓ=null,v)
12Learn the EVAL -Xmodel pγ←eval-x ().
13Estimate ENCODE -METER as the ˆϕq(e)←encode-meter ().
14Return the following as the STRIPE -Xestimate:
E(v,a)∼qDe(xe(x))Ey∼qDe(y|xe(x)=(v,a))[logpγ(y=y|xv=a)]−λˆϕq(e)
36