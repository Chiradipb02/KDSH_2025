SDEs for Adaptive Methods: The Role of Noise
Anonymous Author(s)
Affiliation
Address
email
Abstract
Despite the vast empirical evidence supporting the efficacy of adaptive optimization 1
methods in deep learning, their theoretical understanding is far from complete. 2
In this work, we introduce novel SDEs for commonly used adaptive optimizers: 3
SignSGD, RMSprop(W), and Adam(W). Our SDEs offer a quantitatively accurate 4
description of these optimizers and help bring to light an intricate relationship 5
between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD 6
highlights a noteworthy and precise contrast to SGD in terms of convergence speed, 7
stationary distribution, and robustness to heavy-tail noise. We extend this analysis 8
to AdamW and RMSpropW, for which we observe that the role of noise is much 9
more complex. Crucially, we support our theoretical analysis with experimental 10
evidence by verifying our insights: this includes numerically integrating our SDEs 11
using Euler-Maruyama discretization on various neural network architectures such 12
as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the 13
behavior of the respective optimizers, especially when compared to previous SDEs 14
derived for Adam and RMSprop. We believe our approach can provide valuable 15
insights into best training practices and novel scaling rules. 16
1 Introduction 17
Adaptive optimizers lay the foundation for effectively training of modern deep learning models. 18
These methods are typically employed to optimize an objective function expressed as a sum across N 19
individual data points: minx∈Rd[f(x) :=1
NPN
i=1fi(x)],where f, fi:Rd→R, i= 1, . . . , N. 20
Due to the practical difficulties of selecting the learning rate of stochastic gradient descent, adaptive 21
methods have grown in popularity over the past decade. At a high level, these optimizers adjust the 22
learning rate for each parameter based on the historical gradients. Popular optimizers that belong to 23
this family are RMSprop (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2015), SignSGD 24
(Bernstein et al., 2018), AdamW (Loshchilov and Hutter, 2019), and many other variants. SignSGD is 25
often used for compressing gradients in distributed machine learning (Karimireddy et al., 2019a), but 26
it also has gained popularity due to its connection to RMSprop and Adam (Balles and Hennig, 2018). 27
The latter algorithms have emerged as the standard methods for training modern large language 28
models, partly because of enhancements in signal propagation (Noci et al., 2022). 29
Although adaptive methods are widely favored in practice, their theoretical foundations remain enig- 30
matic. Recent research has illuminated some of their advantages: Zhang et al. (2020b) demonstrated 31
how gradient clipping addresses heavy-tailed gradient noise, Pan and Li (2022) related the success of 32
Adam over SGD to sharpness, and Yang et al. (2024) showed that adaptive methods handle large gra- 33
dients better than SGD. At the same time, many optimization studies focus on worst-case convergence 34
rates: These rates (e.g., Défossez et al. (2022)) are valuable, yet they provide an incomplete depiction 35
of algorithm behavior, showing no quantifiable advantage over standard SGD. One particular aspect 36
still lacking clarity is the precise role of noise in the algorithm trajectory. 37
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Our investigation aims to study how gradient noise influences the dynamics of adaptive optimizers 38
and how it impacts their asymptotic behaviors in terms of expected loss and stationary distribution. In 39
particular, we want to understand which algorithms are more resilient to high (possibly heavy-tailed) 40
gradient noise levels. To do this, we rely on stochastic differential equations (SDEs) which have 41
become popular in the literature to study the behavior of optimization algorithms (Li et al., 2017; 42
Jastrzebski et al., 2018). These continuous-time models unlock powerful tools from Itô calculus, 43
enabling us to establish convergence bounds, determine stationary distributions, unveil implicit 44
regularization, and elucidate the intricate interplay between landscape and noise. Notably, SDEs 45
facilitate direct comparisons between optimizers by explicitly illustrating how each hyperparameter 46
and certain landscape features influence their dynamics (Compagnoni et al., 2024). 47
We begin by analyzing SignSGD, showing how the signal-to-noise ratio affects its dynamics and 48
elucidating the impact of noise at convergence. After analyzing the case where the gradient noise 49
exhibits infinite variance, we extend our analysis to Adam and RMSprop with decoupled weight 50
decay (Loshchilov and Hutter, 2019) – i.e. AdamW and RMSpropW: for both, we refine batch size 51
scaling rules and compare the role of noise to SignSGD. Our analysis provides some theoretical 52
grounding for the resilience of these adaptive methods to high noise levels. Importantly, we highlight 53
that Adam and RMSprop are byproducts of our analysis and that our novel SDEs are derived under 54
much weaker and more realistic assumptions than those in the literature (Malladi et al., 2022). 55
Contributions We identify our key contributions as follows: 56
1.We derive the first SDE for SignSGD under very general assumptions: We show that SignSGD 57
exhibits three different phases of the dynamics and characterize the loss behavior in these phases, 58
including the stationary distribution and asymptotic loss value. 59
2.We demonstrate that for SignSGD, noise inversely affects the convergence rate of both the loss and 60
the iterates. Differently, it has a linear impact on the asymptotic expected loss and the asymptotic 61
variance of the iterates. This is in contrast to SGD, where noise does not influence the convergence 62
speed, but it has a quadratic effect on the loss and variance of the iterates. Finally, we show 63
that, even if the noise has infinite variance, SignSGD is very resilient: its performance is only 64
marginally impacted. In the same conditions, SGD would diverge. 65
3.We derive new, improved, SDEs for AdamW and RMSpropW and use them to (1) show a novel 66
batch size scaling rule and (2) inspect the stationary distribution and stationary loss value in 67
convex quadratics. In particular, we dive into the properties of weight decay: while for vanilla 68
Adam and RMSprop the effect of noise at convergence mimics SignSGD, something different 69
happens in AdamW and RMSpropW — Due to an intricate interaction between noise, curvature, 70
and regularization, weight decay plays a crucial stabilization role at high noise levels near the 71
minimizer. 72
4.We empirically verify every theoretical insight we derive. Importantly, we integrate our SDEs 73
with Euler-Maruyama to confirm that our SDEs faithfully track their respective optimizers. We do 74
so on an MLP, a CNN, a ResNet, and a Transformer. For RMSprop and Adam, our SDEs exhibit 75
superior modeling power than the SDEs already existing in the literature. 76
2 Related work 77
SDE approximations and applications. (Li et al., 2017) introduced a formal theoretical framework 78
aimed at deriving SDEs that effectively model the inherent stochastic nature of optimizers. Ever since, 79
SDEs have found several applications in the field of machine learning, for instance in connection 80
with stochastic optimal control to select the stepsize (Li et al., 2017, 2019) and batch size (Zhao 81
et al., 2022), the derivation of convergence bounds andstationary distributions (Compagnoni et al., 82
2023, 2024), implicit regularization (Smith et al., 2021), and scaling rules (Jastrzebski et al., 2018). 83
Previous work by Malladi et al. (2022) has already made strides in deriving SDE models for RMSprop 84
and Adam, albeit under certain restrictive assumptions. They establish a scaling rule which they 85
assert remains valid throughout the entirety of the dynamics. Unfortunately, their derivation is based 86
on the approach of Jastrzebski et al. (2018) which is problematic in the general case (See Appendix 87
E for a detailed discussion). Indeed, we demonstrate that the SDEs derived in Malladi et al. (2022) 88
are only accurate around minima, indicating that their scaling rule is not globally valid. (Zhou et al., 89
2020a) also claimed to have derived a Lévy SDE for Adam. Unfortunately, the quality of their 90
SDE approximation does not come with theoretical guarantees. Additionally, their SDE has random 91
2coefficients: an approach which is theoretically sound in very limited settings (Kohatsu-Higa et al., 92
1997; Bishop and Del Moral, 2019). Zhou et al. (2024) informally presented an SDE for (only) the 93
parameters of AdamW: this is achieved under strong assumptions and various approximations, some 94
of which are hard to motivate formally. 95
Influence of noise on convergence. Several empirical papers demonstrate that adaptive algorithms 96
adjust better to the noise during training. Specifically, (Zhang et al., 2020b) noticed a consistent gap 97
in the performance of SGD and Adam on language models and connected that phenomenon with 98
heavy-tailed noise distributions. (Pascanu et al., 2013) suggests using gradient clipping to deal with 99
heavy tail noise, and consequently several follow-up works analyzed clipped SGD under heavy-tailed 100
noise (Zhang et al., 2020a; Mai and Johansson, 2021; Puchkin et al., 2024). Kunstner et al. (2024) 101
present thorough numerical experiments illustrating that a significant contributor to heavy-tailed noise 102
during language model training is class imbalance, where certain words occur much more frequently 103
than others. They demonstrate that adaptive optimization methods such as Adam and SignSGD can 104
better adapt to such class imbalances. However, the theoretical understanding of the influence of 105
noise in the context of adaptive algorithms is much more limited. The first convergence results on 106
Adam and RMSprop were derived under bounded stochastic gradients assumption (De et al., 2018; 107
Zaheer et al., 2018; Chen et al., 2019; Défossez et al., 2022). Later, this noise model was relaxed 108
to weak growth condition (Zhang et al., 2022; Wang et al., 2022) and its coordinate-wise version 109
(Hong and Lin, 2023; Wang et al., 2024) and sub-gaussian noise (Li et al., 2023a). SignSGD and 110
its momentum version Signum were originally studied as a method for compressed communication 111
(Bernstein et al., 2018) under bounded variance assumption, but with a requirement of large batches. 112
Several works provided counterexamples where SignSGD fails to converge if stochastic and full 113
gradients are not correlated enough (Karimireddy et al., 2019b; Safaryan and Richtarik, 2021). In 114
the case of AdamW, (Zhou et al., 2022, 2024) provide convergence guarantees under restrictive 115
assumptions such as bounded gradient and bounded noise. All aforementioned results only show 116
that SignSGD, Adam, and RMSprop at least do not perform worse than vanilla SGD. None of them 117
studied how noise affects the dynamics of the algorithm: In this work, we attempt to close this gap. 118
3 Formal statements & insights: the SDEs 119
This section provides the general formulations of the SDEs of SignSGD (Theorem 3.2) and AdamW 120
(Theorem 3.12). Due to the technical nature of the analysis, we refer the reader to the appendix for 121
the complete formal statements and proofs. 122
Assumptions and notation. In this section, we assume that ∇fγ(x) =∇f(x)+Z(x),E[Z(x)] = 0 123
and, unless we study the cases where the gradient variance is unbounded, we write Cov(Z(x)) = 124
Σ(x)where we omit the batch size unless relevant. To derive the stationary distribution around an 125
optimum, we will approximate the loss function with a quadratic convex function f(x) =1
2x⊤Hx 126
as commonly done in the literature (Ge et al., 2015; Levy, 2016; Jin et al., 2017; Poggio et al., 127
2017; Mandt et al., 2017; Compagnoni et al., 2023). Regarding the notation, η > 0is the step 128
size, the mini-batches {γk}are of size B≥1and modeled as i.i.d. random variables uniformly 129
distributed on {1, . . . , N }. The βparameters refer to momentum parameters, γ >0is the (decoupled) 130
L2-regularization parameter, and ϵ >0is a small scalar used for numerical stability. 131
The following definition formalizes the idea that an SDE can be a “good model” to describe an 132
optimizer. It is drawn from the field of numerical analysis of SDEs (see Mil’shtein (1986)) and it 133
quantifies the disparity between the discrete and the continuous processes. 134
Definition 3.1 (Weak Approximation) .A continuous-time stochastic process {Xt}t∈[0,T]is an order 135
αweak approximation (or α-order SDE) of a discrete stochastic process {xk}⌊T/η⌋
k=0 if for every 136
polynomial growth function g, there exists a positive constant C, independent of the stepsize η, such 137
thatmax k=0,...,⌊T/η⌋|Eg(xk)−Eg(Xkη)| ≤Cηα. 138
3.1 SignSGD SDE 139
In this section, we derive an SDE model for SignSGD, which we believe to be a novel addition to 140
the existing literature. This derivation will reveal the unique manner in which noise influences the 141
dynamics of SignSGD. First, we recall the update equation of SignSGD: 142
xk+1=xk−ηsign(∇fγk(xk)). (1)
31 10000 20000 30000 40000 50000
Iterations0.00.10.20.30.40.50.60.70.8LossLosses - DNN
SignSGD
SDE
1 8000 16000 24000 32000 40000
Iterations0.00.51.01.52.02.5LossLosses - CNN
SignSGD
SDE
1 1000 2000 3000 4000 5000
Iterations0.00.51.01.52.0LossLosses - Transformer
SignSGD
SDE
1 1000 2000 3000 4000 5000
Iterations0.51.01.52.02.5LossLosses - ResNet
SignSGD
SDEFigure 1: Comparison of SignSGD and its SDE in terms of f(x): Our SDE successfully tracks the
dynamics of SignSGD on several architectures: DNN on the Breast Cancer dataset (Left); CNN on
MNIST (Center-Left); Transformer on MNIST (Center-Right); ResNet on CIFAR-10 (Right).
The following theorem derives a formal continuous-time model for SignSGD. 143
Theorem 3.2 (Informal Statement of Theorem C.5) .Under sufficient regularity conditions, the 144
solution of the following SDE is an order 1weak approximation of the discrete update of SignSGD: 145
dXt=−(1−2P(∇fγ(Xt)<0))dt+√ηq
¯Σ(Xt)dWt, (2)
where ¯Σ(x)is the noise covariance ¯Σ(x) =E[ξγ(x)ξγ(x)⊤]andξγ(x) := sign(∇fγ(x))−1 + 146
2P(∇fγ(x)<0)the noise in the sample sign (∇fγ(x)). 147
For didactic reasons, we next present a corollary of Theorem 3.2 that provides a more interpretable 148
SDE. Figure 1 shows the empirical validation of this model for various neural network classes: All 149
details are presented in Appendix F. 150
Corollary 3.3 (Informal Statement of Corollary C.7) .Under the assumptions of Theorem 3.2, and 151
that the stochastic gradient is ∇fγ(x) =∇f(x)+Zsuch that Z∼ N(0,Σ),Σ = diag( σ2
1,···, σ2
d), 152
the following SDE provides a 1weak approximation of the discrete update of SignSGD 153
dXt=−Erf 
Σ−1
2∇f(Xt)√
2!
dt+√ηvuutId−diag 
Erf 
Σ−1
2∇f(Xt)√
2!!2
dWt, (3)
where the error function Erf (x)and the square are applied component-wise. 154
While Eq. (3)may appear intricate at first glance, it becomes apparent upon closer inspection that 155
the properties of the Erf(·)function enable a detailed exploration of the dynamics of SignSGD. In 156
particular, we demonstrate that the dynamics of SignSGD can be categorized into three distinct 157
phases. The left of Figure 2 empirically verifies this result on a convex quadratic function. 158
Lemma 3.4. Under the assumptions of Corollary 3.3 and signal-to-noise ratio Yt:=Σ−1
2∇f(Xt)√
2, 159
1.Phase 1: If|Yt|>3
2, the SDE coincides with the ODE of SignGD: 160
dXt=−sign(∇f(Xt))dt; (4)
2.Phase 2: If1<|Yt|<3
2:1161
(a)mYt+q−≤dE[Xt]
dt≤mYt+q+; 162
(b) For any a >0,P
∥Xt−E[Xt]∥2
2> a
≤η
a 
d− ∥mYt+q−∥2
2
; 163
3.Phase 3: If|Yt|<1, the SDE is 164
dXt=−r
2
πΣ−1
2∇f(Xt)dt+√ηr
Id−2
πdiag
Σ−1
2∇f(Xt)2
dWt. (5)
1Letmandq1are the slope and intercept of the line secant to the graph of Erf(x)between the points
(1,Erf(1)) and 3
2,Erf 3
2, while q2is the intercept of the line tangent to the graph of Erf(x)and slope m,
(q+)i:=q2 if∂if(x)>0
−q1if∂if(x)<0,(q−)i:=q1 if∂if(x)>0
−q2if∂if(x)<0, and ˆq:= max( q1, q2).
40.00 0.05 0.10 0.15 0.20 0.25
X10.000.020.040.060.080.100.120.14X2Trajectories
SignSGD
SDE (Full)
ODE (Phase 1)
SDE (Phase 3)
0.000.040.080.120.160.200.240.28
0 500 1000 1500 2000 2500 3000
Iterations104
103
102
101
100LossLosses
SignSGD
SDE
Bound (Phase 1)
Bound (Phase 3)
0.000 0.002 0.004
[X1]
0.00000.00050.00100.00150.00200.0025[X2]
Empirical
Theor. Pred.
0.0 2.5 5.0
Var[X1]1e5
0.00.51.01.52.02.53.0Var[X2]1e5
Empirical
Theor. Pred.
102103104105106
Iterations1010
109
108
107
106
105
104
LossLosses
SignSGD =0.1
SignSGD =0.5
SignSGD =1.5
Bound =0.1
Bound =0.5
Bound =1.5
Figure 2: Phases of SignSGD: The ODE of Phase 1 and the SDE of Phase 3 overlap with the “Full”
SDE as per Lemma 3.4 (Left); Phases of the Loss: The bounds derived in Lemma 3.5 for the loss
during Phase 1 and Phase 3 correctly track the loss evolution (Center-Left); The dynamics of the
moments of Xtpredicted in Lemma 3.7 track the empirical ones (Center-Right); If the schedulers
satisfy the condition in Lemma 3.9, the loss decays to 0as prescribed. Otherwise, the loss does not
converge to 0(Right).
Remark: The behavior of SignSGD depends on the size of the signal-to-noise ratio. In particular, the 165
SDE itself shows that in Phase 3, the inverse of the scale of the noise Σ−1
2premultiplies the gradient, 166
thus affecting the rate of descent. This is not the case for SGD where Σonly influences the diffusion 167
term.2To better understand the role of the noise, we need to study how it affects the dynamics of the 168
loss and compare it with SGD. 169
Lemma 3.5. Letfbeµ-strongly convex, Tr(∇2f(x))≤ L τ, and St:=f(Xt)−f(X∗). Then, 170
during 171
1. Phase 1, the loss will reach 0before t∗= 2q
S0
µbecause St≤1
4 √µt−2√S02; 172
2. Phase 2 with ∆ :=
m√
2σmax+ηµm2
4σ2max
:E[St]≤S0e−2µ∆t+η
2(Lτ−µdˆq2)
2µ∆ 
1−e−2µ∆t
; 173
3. Phase 3 with ∆ :=q
2
π1
σmax+η
πµ
σ2max
:E[St]≤S0e−2µ∆t+η
2Lτ
2µ∆ 
1−e−2µ∆t
. 174
In Phase 1, the signal-to-noise ratio is large, meaning that SignSGD behaves like SignGD: Consistently 175
with the analysis of SignGD in (Ma et al., 2022), this explains the fast initial convergence of the 176
optimizer as well as of RMSprop and Adam. In this phase, the loss undergoes a steady decrease 177
which ensures the emergence of Phase 2 which in turn triggers that of Phase 3 which is characterized 178
by an exponential decay to an asymptotic loss level: As a practical example, we verify the dynamics 179
of the expected loss around a minimum in the center-left of Figure 2. 180
Lemma 3.6. For SGD, the expected loss satisfies: E[St]≤S0e−2µt+η
2Lτσ2
max
2µ 
1−e−2µt
. 181
Remark: The two key observations are that: 182
1.Both in Phase 2 and Phase 3, the noise level σmaxinversely affects the exponential conver- 183
gence speed, while this trend is not observed with SGD; 184
2. The asymptotic loss of SignSGD is (almost) linear in σmaxwhile that of SGD is quadratic. 185
Additionally, we characterize the stationary distribution of SignSGD around a minimum: Empirical 186
validation is provided in the center-right of Figure 2. 187
Lemma 3.7. LetH= diag( λ1, . . . , λ d)andMt:=e−2√
2
πΣ−1
2H+η
πΣ−1
2H2
t. Then, 188
1.E[Xt] =e−√
2
πΣ−1
2HtX0t→∞→0; 189
2.Cov[Xt] =
Mt−e−2√
2
πΣ−1
2Ht
X2
0+η
2q
2
πId+η
πH−1
H−1Σ1
2(Id−Mt), 190
which as t→ ∞ converges toη
2q
2
πId+η
πH−1
H−1Σ1
2. 191
Lemma 3.8. Under the same assumptions as Lemma 3.7, the stationary distribution for SGD is: 192
E[Xt] =e−HtX0t→∞→0and Cov[Xt] =η
2H−1Σ 
Id−e−2Htt→∞→η
2H−1Σ. 193
2Ths SDE of SGD is dXt=−∇f(Xt)dt+√ηΣ1
2dWt.
5As we observed above, the noise inversely affects the convergence rate of the iterates of SignSGD 194
while it does not impact that of SGD. Additionally, while both covariance matrices essentially scale 195
inversely to the hessian, that of SignSGD scales with Σ1
2while that of SGD scales with Σ. 196
We conclude this section by presenting a condition on the step size scheduler that ensures the 197
asymptotic convergence of the expected loss to 0in Phase 3. For general schedulers, we characterize 198
precisely the speed of convergence and the factors influencing it. Empirical validation is provided in 199
the right of Figure 2 for a convex quadratic. 200
Lemma 3.9. Under the assumptions of Lemma 3.5, any step size scheduler ηtsuch that 201
Z∞
0ηsds=∞and lim
t→∞ηt= 0 = ⇒E[f(Xt)−f(X∗)]t→∞→≲Lτσmax
4µrπ
2ηtt→∞→0.(6)
Remark: Under the same conditions, SGD satisfies E[f(Xt)−f(X∗)]t→∞→≲Lτσ2
max
4µηtt→∞→0. 202
Conclusion: As noted in Bernstein et al. (2018), the signal-to-noise ratio is key in determining
the dynamics of SignSGD. Our SDEs help clarify the mechanisms underlying the dynamics of
SignSGD: we show that the effect of noise is radically different from SGD: 1) It affects the rate
of convergence of the iterates, of the covariance of the iterates, and of the expected loss; 2) The
asymptotic loss value and covariance of the iterates scale in Σ1
2while for SGD it does so in
Σ. On the one hand, low levels of noise will ensure a faster and steadier loss decrease close to
minima for SignSGD than for SGD. On the other, SGD will converge to much lower loss values.
A symmetric argument holds for high levels of noise, which suggests that SignSGD is more
resilient to high levels of noise.
203
3.1.1 Heavy-tailed noise 204
Interestingly, we can replicate the efforts above also in case the noise structure is heavy-tailed as it is 205
distributed according to a Student’s t distribution. Notably, we derive the SDE for the case where the 206
noise has infinite variance and show how little marginal effect this has on the dynamics of SignSGD. 207
Lemma 3.10. Under the assumptions of Corollary 3.3 but the noise on the gradients U∼tν(0, Id) 208
where ν∈Z+: The following SDE is a 1weak approximation of the discrete update of SignSGD 209
dXt=−2Ξ
Σ−1
2∇f(Xt)
dt+√ηr
Id−4 diag
Ξ
Σ−1
2∇f(Xt)2
dWt, (7)
where Ξ(x)is defined as Ξ(x) :=xΓ(ν+1
2)√πνΓ(ν
2)2F1
1
2,ν+1
2;3
2;−x2
ν
and 2F1(a, b;c;x)is the hyper- 210
geometric function. Above, the function Ξ(x)and the square are applied component-wise. 211
We now characterize the dynamics of SignSGD when the noise on the gradient has infinite variance. 212
Corollary 3.11. Under the assumptions of Lemma 3.10 and ν= 2, the dynamics in Phase 3 is: 213
dXt=−r
1
2Σ−1
2∇f(Xt)dt+√ηr
Id−1
2diag
Σ−1
2∇f(Xt)2
dWt. (8)
Conclusion: We observe that the dynamics of SignSGD when the noise is Gaussian (Eq. (5)) and
when the noise is heavy-tailed with unbounded variance (Eq. (8)) are very similar: By comparing
the constants in front of the drift terms Σ−1
2∇f(Xt), they are only ∼10% apart, and the diffusion
coefficients are comparable. Not only do we once more showcase the resilience of SignSGD to
high levels of noise, but in alignment with (Zhang et al., 2020b), we provide theoretical support
to the success of Adam in such a scenario where SGD would diverge.
214
All the results derived above can be extended to this setting: this is left as an exercise for the reader. 215
3.2 AdamW SDE 216
In the last subsection, we showcased how SDEs can serve as powerful tools to understand the 217
dynamics of the simplest among coordinate-wise adaptive methods: SignSGD. Here, we extend the 218
61
 0 1
X10.6
0.4
0.2
0.00.20.40.6X2Trajectories
AdamW
AdamW SDE
RMSpropW
RMSpropW SDE
036912151821
0 10000 20000 30000 40000 50000
Iterations103
102
101
100101LossLosses
AdamW
AdamW SDE
RMSpropW
RMSpropW SDE
2
 1
 0 1 2
X10.6
0.4
0.2
0.00.20.40.6X2Trajectories
AdamW
AdamW SDE
RMSpropW
RMSpropW SDE
2
1
0123456
0 5000 10000 15000 20000 25000
Iterations103
102
101
100LossLosses
AdamW
AdamW SDE
RMSpropW
RMSpropW SDEFigure 3: The first two images compare the SDEs of AdamW and RMSpropW with the respective
optimizers in terms of trajectories and f(x)for a convex quadratic function while the other two
figures provide a comparison for an embedded saddle. In all cases, we observe good agreements.
discussion to Adam with decoupled weight decay, i.e. AdamW: 219
vk+1=β2vk+ (1−β2) (∇fγk(xk))2, m k+1=β1mk+ (1−β1)∇fγk(xk),
xk+1=xk−ηˆmk+1p
ˆvk+1+ϵ−ηγxk,ˆmk=mk
1−βk
1,ˆvk=vk
1−βk
2, (9)
which, of course, covers Adam, RMSprop, and RMSpropW depending on the values of γandβ1. 220
The following result proves the SDE of AdamW which we validate in Figure 3 for two simple 221
landscapes and in Figure 4 for a Transformer and a ResNet. 222
Theorem 3.12 (Informal Statement of Theorem C.31) .Under sufficient regularity conditions, ρ1= 223
O(η−ζ)s.t.ζ∈(0,1), and ρ2=O(1), the order 1weak approximation of AdamW is: 224
dXt=−p
γ2(t)
γ1(t)P−1
t(Mt+ηρ1(∇f(Xt)−Mt))dt−γXtdt (10)
dMt=ρ1(∇f(Xt)−Mt)dt+√ηρ1Σ1/2(Xt)dWt (11)
dVt=ρ2 
(∇f(Xt))2+ diag (Σ ( Xt))−Vt
dt, (12)
where βi= 1−ηρi∼1,γi(t) = 1−e−ρit, and Pt= diag√Vt+ϵp
γ2(t)Id. 225
In contrast to Remark 4.3 of Malladi et al. (2022), which suggests that an SDE for RMSprop and 226
Adam is only viable if σ≫ ∥∇ f(x)∥andσ∼1
η, our derivation that does not need these assumptions: 227
See Remark C.25 for a deeper discussion, the implications, and the experimental comparison. 228
The following result demonstrates how the asymptotic expected loss of AdamW scales with the noise 229
level. Notably, it introduces the first scaling rule for AdamW, extending the one proposed for Adam 230
in (Malladi et al., 2022) to include weight decay scaling. It is crucial to understand that, unlike the 231
typical approach in the literature (see (Jastrzebski et al., 2018; Malladi et al., 2022)), our objective in 232
deriving these rules is not to maintain the dynamics of the optimizers or the SDE unchanged. Instead, 233
our goal is to offer a practical strategy for adjusting hyperparameters (e.g., from ηto˜η) to retain 234
certain performance metrics or optimizer properties as the batch size increases (e.g., from Bto˜B). 235
Therefore, in our upcoming analysis, we aim to derive scaling rules that preserve specific relevant 236
aspects of the dynamics, such as the convergence bound on the loss or the speed. For a more detailed 237
discussion motivating our approach, see Appendix E. 238
Lemma 3.13. Iffisµ-strongly convex and L-smooth, Lτ:=Tr(∇2f(x)), and (∇f(x))2=O(η), 239
˜η=κη,˜B=Bδ, and ˜ρi=αiρi, and ˜γ=ξγ, AdamW satisfies 240
E[f(Xt)−f(X∗)]t→∞
≤ηLτσL
2κ
2µ√
BδL +σξγ(L+µ). (13)
We derive the novel scaling rule by 1) Preserving the upper bound, which requires that κ=√
δand 241
ξ=κ; 2) Preserving the relative speed of Mt,VtandXt, which requires that ˜βi= 1−κ2(1−βi). 242
The left of Figure 5 shows the empirical verification of the predicted loss value and scaling rule on 243
a convex quadratic function.3Interestingly, and consistently with Lemma 3.13, such a value is not 244
3Table 1 in Appendix F.8 shows that our scaling rule works on DNNs: it confirms that failing to rescale the
weight decay parameter is suboptimal.
71 400 800 1200 1600 2000
Iterations0.00.51.01.52.02.53.0LossLosses - Transformer
AdamW
SDE
100101102103
Iterations0.00.51.01.52.02.5LossLosses - ResNet
AdamW
SDE
1 400 800 1200 1600 2000
Iterations0.00.51.01.52.02.53.0LossLosses - Transformer
RMSpropW
SDE
100101102103
Iterations024681012LossLosses - ResNet
RMSpropW
SDEFigure 4: The first two represent the comparison between AdamW and its SDE in terms of f(x). The
other two do the same for RMSpropW. In both cases, the first is a Transformer on MNIST and the
second a ResNet on CIFAR-10: Our SDEs match the respective optimizers.
1 4000 8000 12000 16000 20000
Iterations104
103
102
LossLosses
AdamW (=1)
AdamW R (=1)
AdamW NR (=1)
Theor. Pred. (=1)
AdamW (=4)
AdamW R (=4)
AdamW NR (=4)
Theor. Pred. (=4)
1 4000 8000 12000 16000 20000
Iterations103
102
LossLosses
RMSpropW (=1)
RMSpropW R (=1)
RMSpropW NR (=1)
Theor. Pred. (=1)
RMSpropW (=4)
RMSpropW R (=4)
RMSpropW NR (=4)
Theor. Pred. (=4)
0 10000 20000 30000 40000 50000
Iterations104
102
100102LossLosses - AdamW
1=0.999, 2=0.998
1=0.999, 2=0.996
1=0.999, 2=0.992
1=0.9, 2=0.998
1=0.9, 2=0.996
1=0.9, 2=0.992
0 20000
t107
106
105
104
Var[X1]
AdamW
RMSpropW
Theor. Pred.
0 20000
t107
106
105
104
103
Var[X2]
AdamW
RMSpropW
Theor. Pred.
Figure 5: The loss predicted in Lemma 3.13 matches the experimental results on a convex quadratic
function. AdamW is run with regularization parameter γ= 1.AdamW R (AdamW Rescaled) is run
as we apply the scaling rule with κ= 2.AdamW NR (AdamW NotRescaled) is run as we apply
the scaling rule with κ= 2on all hyperparameters but γ, which is left unchanged: Our scaling rule
holds, and failing to rescale γleads the optimizer not to preserve the asymptotic loss level. The same
happens for γ= 4(Left); The same for RMSpropW (Center-Left); For AdamW, β1andβ2influence
which basin will attract the dynamics and how fast this will converge, but not the asymptotic loss
level inside the basin (Center-Right). For both AdamW and RMSpropW, the variance at convergence
predicted in Lemma 3.14 matches the experimental results (Right).
influenced by the choice of βi: We argue that βido not impact the asymptotic level of the loss, but 245
rather drive the selection of the basin and speed at which AdamW converges to it — The center-right 246
of Figure 5 exemplifies this on a simple nonconvex landscape. 247
We conclude this section with the stationary distribution of AdamW around a minimum which we 248
empirically validate on the right of Figure 5. 249
Lemma 3.14. The stationary distribution of AdamW is 250
(E[X∞], Cov [X∞]) =
0,η
2
Id+γH−1Σ1
2−1
H−1Σ1
2
.
RMSpropW We derived the same results for RMSprop(W) and we reported them in Appendix 251
C.4: importantly, we validate the SDE in Figure 3 for two simple landscapes and in Figure 4 for a 252
Transformer and a ResNet. The results regarding the asymptotic loss level and stationary distributions 253
are validated in the center-left and right of Figure 5 for a convex quadratic function. 254
Conclusion: While for both SignSGD and Adam the asymptotic loss value and the covariance of
the iterates scale linearly with Σ1
2, we observe for AdamW this is more intricate: The interaction
between curvature, noise, and regularization implies that these two quantities are upper-bounded
inΣ1
2and increasing Σto infinity does not lead to their explosion: Weight decay plays a crucial
stabilization role at high noise levels near the minimizer — See Figure 6 for a comparison across
optimizers. Finally, we argue that βiplay a key role in selecting the basin and the convergence
speed to the asymptotic loss value rather than impacting the loss value itself.
255
4 Experiments: SDE validation 256
The point of our experiments is to validate the theoretical results derived from the SDEs. Therefore, 257
we first show that our SDEs faithfully represent the dynamics of their respective optimizers. To do 258
80 25000 50000 75000 100000
Iterations107
105
103
101
101103LossSGD
Optim. (=102)
Optim. (=101)
Optim. (=1)
Optim. (=10)
Optim. (=103)
Limit  (=102)
Limit  (=101)
Limit  (=1)
Limit  (=10)
Limit  (=103)
0 25000 50000 75000 100000
Iterations106
105
104
103
102
101
100LossSignSGD
0 25000 50000 75000 100000
Iterations107
106
105
104
103
102
101
100LossAdam
0 25000 50000 75000 100000
Iterations107
106
105
104
103
102
101
100LossAdamWFigure 6: For SGD (Left), SignSGD (Center-Left), Adam (Center-Right), and AdamW: For each
optimizer , we plot the loss value on a convex quadratic and compare its asymptotic value with the
limits predicted by our theory. As we take Σ = σ2Id, we confirm that the loss of SGD scales
quadratically in σ(Lemma 3.6), and linearly for SignSGD (Lemma 3.5) and Adam (Lemma 3.13
withγ= 0). For AdamW, the maximum asymptotic loss value is bounded in σ(Lemma 3.13 with
γ >0). In accordance with the experiments, our theory predicts that adaptive methods are more
resilient to noise.
so, we integrate the SDEs with Euler-Maruyama (Algorithm 1): This is particularly challenging and 259
expensive as one needs to calculate the full gradients of the DNNs at each iteration.4We present the 260
first set of validation experiments on a variety of architectures and datasets: An MLP on the Breast 261
Cancer dataset, a CNN and a Transformer on MNIST, and a ResNet on CIFAR-10. All details are in 262
Appendix F. 263
5 Conclusion 264
We derived the first formal SDE for SignSGD, enabling us to demonstrate its dynamics traversing 265
three discernible phases. We characterize how the signal-to-noise ratio drives the dynamics of the 266
loss in each of these phases, and we derive the asymptotic value of the loss function, as well as the 267
stationary distribution. Regarding the role of noise, we draw a straightforward comparison with 268
SGD. For SignSGD, the noise level√
Σhas an inverse linear effect on the convergence speed of the 269
loss and the iterates. However, it linearly affects the asymptotic expected loss and the asymptotic 270
variance of the iterates. In contrast, for SGD, noise does not influence the convergence speed but 271
has a quadratic impact on the loss level and variance. We also examine the scenario where the noise 272
has infinite variance and demonstrate the resilience of SignSGD, showing that its performance is 273
only marginally affected. Finally, we generalize the analysis to include AdamW and RMSpropW. 274
Specifically, we leverage our novel SDEs to derive the asymptotic value of the loss function, their 275
stationary distribution on a convex quadratic, and a novel scaling rule. The key insight is that, similarly 276
to SignSGD, the loss level and covariance matrix of the iterates of Adam and RMSprop scale linearly 277
in the noise level Σ1
2. For AdamW and RMSpropW, the complex interaction of noise, curvature, and 278
regularization implies that these two quantities are bounded in terms of Σ1
2, showing that weight 279
decay plays a crucial stabilization role at high noise levels near the minimizer. Interestingly, the 280
SDEs for Adam and RMSprop are straightforward corollary of our general results and were derived 281
under much less restrictive and more realistic assumptions than those in the literature. Finally, we 282
thoroughly validate all our theoretical results: We compare the dynamics of the various optimizers 283
with the respective SDEs and find good agreement on simple landscapes and deep neural networks. 284
For Adam and RMSprop, our SDEs track them better than those derived in (Malladi et al., 2022). 285
Future work We believe that our results can be extended to other optimizers commonly used in 286
practice such as Signum, AdaGrad, AdaMax, and Nadam. Additionally, inspired by the insights 287
from our SDE analysis, there is potential for designing new optimization algorithms that combine the 288
strengths of existing methods while mitigating their weaknesses. For example, developing hybrid 289
optimizers that adaptively switch between different strategies based on the training phase or current 290
state of the optimization process could offer superior performance. 291
4Many papers derived SDEs to model optimizers: most of them do not validate them, some do so on quadratic
functions, and Paquette et al. (2021); Compagnoni et al. (2023) do it on NNs: See Appendix A for details.
9References 292
An, J., Lu, J., and Ying, L. (2020). Stochastic modified equations for the asynchronous stochastic 293
gradient descent. Information and Inference: A Journal of the IMA , 9(4):851–873. 294
Ankirchner, S. and Perko, S. (2024). A comparison of continuous-time approximations to stochastic 295
gradient descent. Journal of Machine Learning Research , 25(13):1–55. 296
Ayadi, I. and Turinici, G. (2021). Stochastic runge-kutta methods and adaptive sgd-g2 stochastic 297
gradient descent. In 2020 25th International Conference on Pattern Recognition (ICPR) , pages 298
8220–8227. IEEE. 299
Balles, L. and Hennig, P. (2018). Dissecting adam: The sign, magnitude and variance of stochastic 300
gradients. In International Conference on Machine Learning , pages 404–413. PMLR. 301
Barakat, A. and Bianchi, P. (2021). Convergence and dynamical behavior of the adam algorithm for 302
nonconvex stochastic optimization. SIAM Journal on Optimization , 31(1):244–274. 303
Bardi, M. and Kouhkouh, H. (2022). Deep relaxation of controlled stochastic gradient descent via 304
singular perturbations. arXiv preprint arXiv:2209.05564 . 305
Bercher, A., Gonon, L., Jentzen, A., and Salimova, D. (2020). Weak error analysis for stochastic 306
gradient descent optimization algorithms. arXiv preprint arXiv:2007.02723 . 307
Bernstein, J., Wang, Y .-X., Azizzadenesheli, K., and Anandkumar, A. (2018). signSGD: Compressed 308
optimisation for non-convex problems. In Proceedings of the 35th International Conference on 309
Machine Learning . 310
Bishop, A. N. and Del Moral, P. (2019). Stability properties of systems of linear stochastic differential 311
equations with random coefficients. SIAM Journal on Control and Optimization , 57(2):1023–1042. 312
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, 313
A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. (2018). JAX: composable transformations 314
of Python+NumPy programs. 315
Chen, P., Lu, J., and Xu, L. (2022). Approximation to stochastic variance reduced gradient langevin dy- 316
namics by stochastic delay differential equations. Applied Mathematics & Optimization , 85(2):15. 317
Chen, X., Liu, S., Sun, R., and Hong, M. (2019). On the convergence of a class of adam-type 318
algorithms for non-convex optimization. In International Conference on Learning Representations . 319
Compagnoni, E. M., Biggio, L., Orvieto, A., Proske, F. N., Kersting, H., and Lucchi, A. (2023). An 320
sde for modeling sam: Theory and insights. In International Conference on Machine Learning , 321
pages 25209–25253. PMLR. 322
Compagnoni, E. M., Orvieto, A., Kersting, H., Proske, F., and Lucchi, A. (2024). Sdes for minimax 323
optimization. In International Conference on Artificial Intelligence and Statistics , pages 4834–4842. 324
PMLR. 325
Cui, Z.-X., Fan, Q., and Jia, C. (2020). Momentum methods for stochastic optimization over 326
time-varying directed networks. Signal Processing , 174:107614. 327
Dambrine, M., Dossal, C., Puig, B., and Rondepierre, A. (2024). Stochastic differential equations for 328
modeling first order optimization methods. SIAM Journal on Optimization , 34(2):1402–1426. 329
De, S., Mukherjee, A., and Ullah, E. (2018). Convergence guarantees for rmsprop and adam in 330
non-convex optimization and an empirical comparison to nesterov acceleration. arXiv preprint 331
arXiv:1807.06766 . 332
Défossez, A., Bottou, L., Bach, F., and Usunier, N. (2022). A simple convergence proof of adam and 333
adagrad. Transactions on Machine Learning Research . 334
Deng, L. (2012). The mnist database of handwritten digit images for machine learning research. 335
IEEE Signal Processing Magazine , 29(6):141–142. 336
10Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, 337
M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is 338
worth 16x16 words: Transformers for image recognition at scale. In International Conference on 339
Learning Representations . 340
Dua, D. and Graff, C. (2017). UCI machine learning repository. 341
Fontaine, X., De Bortoli, V ., and Durmus, A. (2021). Convergence rates and approximation results 342
for sgd and its continuous-time counterpart. In Conference on Learning Theory , pages 1965–2058. 343
PMLR. 344
Ge, R., Huang, F., Jin, C., and Yuan, Y . (2015). Escaping from saddle points—online stochastic 345
gradient for tensor decomposition. In Conference on Learning Theory , pages 797–842. 346
Gess, B., Kassing, S., and Konarovskyi, V . (2024). Stochastic modified flows, mean-field limits and 347
dynamics of stochastic gradient descent. Journal of Machine Learning Research , 25(30):1–27. 348
Gu, H., Guo, X., and Li, X. (2021). Adversarial training for gradient descent: Analysis through its 349
continuous-time approximation. arXiv preprint arXiv:2105.08037 . 350
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, 351
E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., 352
Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., Gérard-Marchant, P., Sheppard, K., Reddy, 353
T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. (2020). Array programming with 354
NumPy. Nature , 585(7825):357–362. 355
Higham, D. J. (2001). An algorithmic introduction to numerical simulation of stochastic differential 356
equations. SIAM review , 43(3):525–546. 357
Hong, Y . and Lin, J. (2023). High probability convergence of adam under unbounded gradients and 358
affine variance noise. arXiv preprint arXiv:2311.02000 . 359
Hu, W., Li, C. J., and Zhou, X. (2019). On the global convergence of continuous–time stochastic 360
heavy–ball method for nonconvex optimization. In 2019 IEEE International Conference on Big 361
Data (Big Data) , pages 94–104. IEEE. 362
Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y ., and Storkey, A. (2018). 363
Three factors influencing minima in sgd. ICANN 2018 . 364
Jin, C., Ge, R., Netrapalli, P., Kakade, S. M., and Jordan, M. I. (2017). How to escape saddle points 365
efficiently. In International Conference on Machine Learning , pages 1724–1732. PMLR. 366
Karatzas, I. and Shreve, S. (2014). Brownian motion and stochastic calculus , volume 113. springer. 367
Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. (2019a). Error feedback fixes signsgd and 368
other gradient compression schemes. In International Conference on Machine Learning , pages 369
3252–3261. PMLR. 370
Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. (2019b). Error feedback fixes SignSGD 371
and other gradient compression schemes. In Proceedings of the 36th International Conference on 372
Machine Learning . 373
Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In International 374
Conference on Learning Representations . 375
Kohatsu-Higa, A., León, J. A., and Nualart, D. (1997). Stochastic differential equations with random 376
coefficients. Bernoulli , pages 233–245. 377
Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images. 378
Toronto, ON, Canada . 379
Kunin, D., Sagastuy-Brena, J., Gillespie, L., Margalit, E., Tanaka, H., Ganguli, S., and Yamins, D. L. 380
(2023). The limiting dynamics of sgd: Modified loss, phase-space oscillations, and anomalous 381
diffusion. Neural Computation , 36(1):151–174. 382
11Kunstner, F., Yadav, R., Milligan, A., Schmidt, M., and Bietti, A. (2024). Heavy-tailed class 383
imbalance and why adam outperforms gradient descent on language models. arXiv preprint 384
arXiv:2402.19449 . 385
Lanconelli, A. and Lauria, C. S. (2022). A note on diffusion limits for stochastic gradient descent. 386
arXiv preprint arXiv:2210.11257 . 387
Levy, K. Y . (2016). The power of normalization: Faster evasion of saddle points. arXiv preprint 388
arXiv:1611.04831 . 389
Li, H., Rakhlin, A., and Jadbabaie, A. (2023a). Convergence of adam under relaxed assumptions. In 390
Thirty-seventh Conference on Neural Information Processing Systems . 391
Li, L. and Wang, Y . (2022). On uniform-in-time diffusion approximation for stochastic gradient 392
descent. arXiv preprint arXiv:2207.04922 . 393
Li, Q., Tai, C., and Weinan, E. (2017). Stochastic modified equations and adaptive stochastic gradient 394
algorithms. In International Conference on Machine Learning , pages 2101–2110. PMLR. 395
Li, Q., Tai, C., and Weinan, E. (2019). Stochastic modified equations and dynamics of stochastic 396
gradient algorithms i: Mathematical foundations. The Journal of Machine Learning Research , 397
20(1):1474–1520. 398
Li, Z., Malladi, S., and Arora, S. (2021). On the validity of modeling SGD with stochastic differential 399
equations (SDEs). In Beygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan, J. W., editors, 400
Advances in Neural Information Processing Systems . 401
Li, Z., Wang, Y ., and Wang, Z. (2023b). Fast equilibrium of sgd in generic situations. In The Twelfth 402
International Conference on Learning Representations . 403
Liu, T., Chen, Z., Zhou, E., and Zhao, T. (2021). A diffusion approximation theory of momentum 404
stochastic gradient descent in nonconvex optimization. Stochastic Systems . 405
Loshchilov, I. and Hutter, F. (2019). Decoupled weight decay regularization. In International 406
Conference on Learning Representations . 407
Ma, C., Wu, L., and Weinan, E. (2022). A qualitative study of the dynamic behavior for adaptive 408
gradient algorithms. In Mathematical and Scientific Machine Learning , pages 671–692. PMLR. 409
Mai, V . V . and Johansson, M. (2021). Stability and convergence of stochastic gradient clipping: 410
Beyond lipschitz continuity and smoothness. In International Conference on Machine Learning . 411
Malladi, S., Lyu, K., Panigrahi, A., and Arora, S. (2022). On the SDEs and scaling rules for adaptive 412
gradient algorithms. In Advances in Neural Information Processing Systems . 413
Mandt, S., Hoffman, M. D., and Blei, D. M. (2017). Stochastic gradient descent as approximate 414
bayesian inference. JMLR 2017 . 415
Mao, X. (2007). Stochastic differential equations and applications . Elsevier. 416
Maulen-Soto, R., Fadili, J., Attouch, H., and Ochs, P. (2024). Stochastic inertial dynamics via time 417
scaling and averaging. arXiv preprint arXiv:2403.16775 . 418
Maulén Soto, R. I. (2021). A continuous-time model of stochastic gradient descent: convergence 419
rates and complexities under lojasiewicz inequality. Universidad de Chile . 420
Milstein, G. N. (2013). Numerical integration of stochastic differential equations , volume 313. 421
Springer Science & Business Media. 422
Mil’shtein, G. (1986). Weak approximation of solutions of systems of stochastic differential equations. 423
Theory of Probability & Its Applications , 30(4):750–766. 424
Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. (2022). Signal 425
propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in 426
Neural Information Processing Systems , 35:27198–27211. 427
12Øksendal, B. (1990). When is a stochastic integral a time change of a diffusion? Journal of theoretical 428
probability , 3(2):207–226. 429
Pan, Y . and Li, Y . (2022). Toward understanding why adam converges faster than SGD for transform- 430
ers. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop) . 431
Paquette, C., Lee, K., Pedregosa, F., and Paquette, E. (2021). Sgd in the large: Average-case analysis, 432
asymptotics, and stepsize criticality. In Conference on Learning Theory , pages 3548–3626. PMLR. 433
Pascanu, R., Mikolov, T., and Bengio, Y . (2013). On the difficulty of training recurrent neural 434
networks. In International conference on machine learning . 435
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., Blondel, M., 436
Prettenhofer, P., Weiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, 437
M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of 438
Machine Learning Research , 12:2825–2830. 439
Poggio, T., Kawaguchi, K., Liao, Q., Miranda, B., Rosasco, L., Boix, X., Hidary, J., and Mhaskar, 440
H. (2017). Theory of deep learning iii: explaining the non-overfitting puzzle. arXiv preprint 441
arXiv:1801.00173 . 442
Puchkin, N., Gorbunov, E., Kutuzov, N., and Gasnikov, A. (2024). Breaking the heavy-tailed noise 443
barrier in stochastic optimization problems. In International Conference on Artificial Intelligence 444
and Statistics . 445
Safaryan, M. and Richtarik, P. (2021). Stochastic sign descent methods: New algorithms and better 446
theory. In Proceedings of the 38th International Conference on Machine Learning . 447
Smith, S. L., Dherin, B., Barrett, D. G. T., and De, S. (2021). On the origin of implicit regularization 448
in stochastic gradient descent. ArXiv , abs/2101.12176. 449
Soto, R. M., Fadili, J., and Attouch, H. (2022). An sde perspective on stochastic convex optimization. 450
arXiv preprint arXiv:2207.02750 . 451
Su, L. and Lau, V . K. (2023). Accelerated federated learning over wireless fading channels with 452
adaptive stochastic momentum. IEEE Internet of Things Journal . 453
Sun, J., Yang, Y ., Xun, G., and Zhang, A. (2023). Scheduling hyperparameters to improve generaliza- 454
tion: From centralized sgd to asynchronous sgd. ACM Transactions on Knowledge Discovery from 455
Data , 17(2):1–37. 456
Tieleman, T. and Hinton, G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average 457
of its recent magnitude. 458
Van Rossum, G. and Drake, F. L. (2009). Python 3 Reference Manual . CreateSpace, Scotts Valley, 459
CA. 460
Wang, B., Fu, J., Zhang, H., Zheng, N., and Chen, W. (2024). Closing the gap between the upper 461
bound and lower bound of adam’s iteration complexity. Advances in Neural Information Processing 462
Systems , 36. 463
Wang, B., Zhang, Y ., Zhang, H., Meng, Q., Ma, Z.-M., Liu, T.-Y ., and Chen, W. (2022). Provable 464
adaptivity in adam. arXiv preprint arXiv:2208.09900 . 465
Wang, Y . and Wu, S. (2020). Asymptotic analysis via stochastic differential equations of gradient 466
descent algorithms in statistical and computational paradigms. Journal of machine learning 467
research , 21(199):1–103. 468
Wang, Z. and Mao, Y . (2022). Two facets of sde under an information-theoretic lens: Generalization 469
of sgd via training trajectories and via terminal states. arXiv preprint arXiv:2211.10691 . 470
Yang, J., Li, X., Fatkhullin, I., and He, N. (2024). Two sides of one coin: the limits of untuned sgd 471
and the power of adaptive methods. Advances in Neural Information Processing Systems , 36. 472
13Zaheer, M., Reddi, S., Sachan, D., Kale, S., and Kumar, S. (2018). Adaptive methods for nonconvex 473
optimization. Advances in neural information processing systems , 31. 474
Zhang, J., He, T., Sra, S., and Jadbabaie, A. (2020a). Why gradient clipping accelerates training: A 475
theoretical justification for adaptivity. In International Conference on Learning Representations . 476
Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. (2020b). Why are 477
adaptive methods good for attention models? Advances in Neural Information Processing Systems . 478
Zhang, Y ., Chen, C., Shi, N., Sun, R., and Luo, Z.-Q. (2022). Adam can converge without any 479
modification on update rules. Advances in neural information processing systems . 480
Zhang, Z., Li, Y ., Luo, T., and Xu, Z.-Q. J. (2023). Stochastic modified equations and dynamics of 481
dropout algorithm. arXiv preprint arXiv:2305.15850 . 482
Zhao, J., Lucchi, A., Proske, F. N., Orvieto, A., and Kersting, H. (2022). Batch size selection by 483
stochastic optimal control. In Has it Trained Yet? NeurIPS 2022 Workshop . 484
Zhou, P., Feng, J., Ma, C., Xiong, C., Hoi, S. C. H., et al. (2020a). Towards theoretically understanding 485
why sgd generalizes better than adam in deep learning. Advances in Neural Information Processing 486
Systems , 33:21285–21296. 487
Zhou, P., Xie, X., Lin, Z., and Yan, S. (2024). Towards understanding convergence and generalization 488
of adamw. IEEE Transactions on Pattern Analysis and Machine Intelligence . 489
Zhou, P., Xie, X., and Shuicheng, Y . (2022). Win: Weight-decay-integrated nesterov accelera- 490
tion for adaptive gradient algorithms. In The Eleventh International Conference on Learning 491
Representations . 492
Zhou, X., Yuan, H., Li, C. J., and Sun, Q. (2020b). Stochastic modified equations for continuous 493
limit of stochastic admm. arXiv preprint arXiv:2003.03532 . 494
Zhu, Y . and Ying, L. (2021). A sharp convergence rate for a model equation of the asynchronous 495
stochastic gradient descent. Communications in Mathematical Sciences . 496
A Additional related works 497
In this section, we list some papers that derived or used SDEs to model optimizers. In particular, we 498
focus on the aspect of empirically verifying the validity of such SDEs in the sense that they indeed 499
track the respective optimizers. We divide these into three categories: Those that did not carry out 500
any type of validation, those that did it on simple landscapes (quadratic functions et similia), and 501
those that did small experiments or neural networks. 502
None of the following papers carried out any experimental validation of the approximating power of 503
the SDEs they derived. Many of them did not even validate the insights derived from the SDEs: (Liu 504
et al., 2021; Hu et al., 2019; Bercher et al., 2020; Zhu and Ying, 2021; Cui et al., 2020; Maulén Soto, 505
2021; Wang and Wu, 2020; Lanconelli and Lauria, 2022; Ayadi and Turinici, 2021; Soto et al., 2022; 506
Li and Wang, 2022; Wang and Mao, 2022; Bardi and Kouhkouh, 2022; Chen et al., 2022; Kunin 507
et al., 2023; Zhang et al., 2023; Sun et al., 2023; Li et al., 2023b; Gess et al., 2024; Dambrine et al., 508
2024; Maulen-Soto et al., 2024). 509
The following ones carried out validation experiments on artificial landscapes, e.g. quadratic or 510
quartic function, or easy regression tasks: (Li et al., 2017, 2019; Zhou et al., 2020b; An et al., 2020; 511
Fontaine et al., 2021; Gu et al., 2021; Su and Lau, 2023; Ankirchner and Perko, 2024). 512
The following papers carried out some experiments which include neural networks: (Paquette et al., 513
2021; Compagnoni et al., 2023). In particular, they both simulate the SDEs with a numerical integrator 514
and compare them with the respective optimizers: The first validates the SDE on a shallow MLP 515
while the second does so on a shallow and a deep MLP. Regarding (Li et al., 2021; Malladi et al., 516
2022), they do not validate their SDEs: Rather, their approach conceptually proceeds as follows: 517
1. Derive an SDE for an optimizer which we now dub “ A”; 518
142. Notice that simulating the SDE is too expensive; 519
3.Define another discrete-time algorithm called SV AG which also has the same SDE as “ A” 520
but does not numerically integrate the SDE as it does not even require access to it: It does 521
not need access neither to the drift nor to the diffusion term; 522
4. Simulate SV AG and show that it tracks “ A” successfully; 523
5. Conclude that the SDE is a good approximation for “ A”. 524
However, they never validated that the SDE is a good approximation for “ A” or for SV AG either. 525
With the same logic, they could have done the following: 526
1. Derive an SDE for “ A”; 527
2. Notice that simulating the SDE is too expensive; 528
3.Define another discrete-time algorithm called “ B” which coincides with “ A” and thus of 529
course shares the same SDE; 530
4. Simulate “ B” and show that it tracks “ A” perfectly; 531
5. Conclude that the SDE is a good approximation for “ A”. 532
In particular, the only fact they prove is that SV AG is a discrete-time optimizer that shares the same 533
SDE as “ A” because it describes a discrete trajectory that is a 1st-order approximation of the SDE of 534
“A”. Technically speaking, “ A” also does the same. One cannot conclude that the SDE derived for “ A” 535
is a good model for “ A” by simply comparing two algorithms “ A” and “ B” that share the same SDE. 536
Otherwise, simply comparing an optimizer “ A” with itself would do the trick. An SDE’s empirical 537
validation can only occur if the SDE is simulated with a numerical integrator that requires access to 538
the drift and diffusion terms (Higham, 2001; Milstein, 2013). 539
B Stochastic calculus 540
In this section, we summarize some important results in the analysis of Stochastic Differential 541
Equations Mao (2007); Øksendal (1990). The notation and the results in this section will be used 542
extensively in all proofs in this paper. We assume the reader to have some familiarity with Brownian 543
motion and with the definition of stochastic integral (Ch. 1.4 and 1.5 in Mao (2007)). 544
B.1 Itô’s Lemma 545
We start with some notation: Let (Ω,F,{Ft}t≥0,P)be a filtered probability space. We say that an
event E∈ F holds almost surely (a.s.) in this space if P(E) = 1 . We call Lp([a, b],Rd), with p >0,
the family of Rd-valued Ft-adapted processes {ft}a≤t≤bsuch that
Zb
a∥ft∥pdt≤ ∞.
Moreover, we denote by Mp([a, b],Rd), with p >0, the family of Rd-valued processes {ft}a≤t≤b 546
inL([a, b],Rd)such that EhRb
a∥ft∥pdti
≤ ∞ . We will write h∈ Lp 
R+,Rd
, with p >0, if 547
h∈ Lp 
[0, T],Rd
for every T >0. Similar definitions hold for matrix-valued functions using the 548
Frobenius norm ∥A∥:=qP
ij|Aij|2. 549
LetW={Wt}t≥0be a one-dimensional Brownian motion defined on our probability space and let 550
X={Xt}t≥0be anFt-adapted process taking values on Rd. 551
Definition B.1. Let the drift beb∈ L1 
R+,Rd
and the diffusion term be σ∈ L2 
R+,Rd×m
. 552
Xtis an Itô process if it takes the form 553
Xt=x0+Zt
0bsds+Zt
0σsdWs.
15We shall say that Xthas the stochastic differential 554
dXt=btdt+σtdWt. (14)
555
Theorem B.2 (Itô’s Lemma) .LetXtbe an Itô process with stochastic differential dXt=btdt+ 556
σtdWt. Let f(x, t)be twice continuously differentiable in xand continuously differentiable in t, 557
taking values in R. Then f(Xt, t)is again an Itô process with stochastic differential 558
d f(Xt, t) =∂tf(Xt, t))dt+⟨∇f(Xt, t), bt⟩dt+1
2Tr 
σtσ⊤
t∇2f(Xt, t)
dt+⟨∇f(Xt, t), σt⟩dWt.
(15)
B.2 Stochastic Differential Equations 559
Stochastic Differential Equations (SDEs) are equations of the form 560
dXt=b(Xt, t)dt+σ(Xt, t)dWt.
First of all, we need to define what it means for a stochastic process X={Xt}t≥0with values in Rd561
to solve an SDE. 562
Definition B.3. LetXtbe as above with deterministic initial condition X0=x0. Assume b: 563
Rd×[0, T]→Rdandσ:Rd×[0, T]→Rd×mare Borel measurable; Xtis called a solution to the 564
corresponding SDE if 565
1.Xtis continuous and Ft-adapted; 566
2.b∈ L1 
[0, T],Rd
; 567
3.σ∈ L2 
[0, T],Rd×m
; 568
4. For every t∈[0, T]
Xt=x0+Zt
0b(Xs, s)ds+Zt
0σ(Xs, s)dW(s)a.s.
Moreover, the solution Xtis said to be unique if any other solution X⋆
tis such that
P{Xt=X⋆
t,for all 0≤t≤T}= 1.
569Notice that since the solution to an SDE is an Itô process, we can use Itô’s Lemma. The following 570
theorem gives a sufficient condition on bandσfor the existence of a solution to the corresponding 571
SDE. 572
Theorem B.4. Assume that there exist two positive constants ¯KandKsuch that 573
1. (Global Lipschitz condition) for all x, y∈Rdandt∈[0, T]
max{∥b(x, t)−b(y, t)∥2,∥σ(x, t)−σ(y, t)∥2} ≤¯K∥x−y∥2;
2. (Linear growth condition) for all x∈Rdandt∈[0, T]
max{∥b(x, t)∥2,∥σ(x, t)∥2} ≤K(1 +∥x∥2).
Then, there exists a unique solution Xtto the corresponding SDE, and Xt∈ M2([0, T],Rd). 574
Numerical approximation. Often, SDEs are solved numerically. The simplest algorithm to provide 575
a sample path (ˆxk)k≥0forXt, so that Xk∆t≊ˆxkfor some small ∆tand for all k∆t≤Mis called 576
Euler-Maruyama (Algorithm 1). For more details on this integration method and its approximation 577
properties, the reader can check Mao (2007). 578
16Algorithm 1 Euler-Maruyama Integration Method for SDEs
input The drift b, the volatility σ, and the initial condition x0.
Fix a stepsize ∆t;
Initialize ˆx0=x0;
k= 0;
while k≤T
∆t
do
Sample some d-dimensional Gaussian noise Zk∼ N(0, Id);
Compute ˆxk+1= ˆxk+ ∆t b(ˆxk, k∆t) +√
∆t σ(ˆxk, k∆t)Zk;
k=k+ 1;
end while
output The approximated sample path (ˆxk)0≤k≤⌊T
∆t⌋.
C Theoretical framework - Weak Approximation 579
In this section, we introduce the theoretical framework used in the paper, together with its assumptions 580
and notations. 581
First of all, many proofs will use Taylor expansions in powers of η. For ease of notation, we introduce
the shorthand that whenever we write O(ηα), we mean that there exists a function K(x)∈Gsuch
that the error terms are bounded by K(x)ηα. For example, we write
b(x+η) =b0(x) +ηb1(x) +O 
η2
to mean: there exists K∈Gsuch that
|b(x+η)−b0(x)−ηb1(x)| ≤K(x)η2.
Additionally, we introduce the following shorthand: 582
• A multi-index is α= (α1, α2, . . . , α n)such that αj∈ {0,1,2, . . .}; 583
•|α|:=α1+α2+···+αn; 584
•α! :=α1!α2!···αn!; 585
• For x= (x1, x2, . . . , x n)∈Rn, we define xα:=xα1
1xα2
2···xαnn; 586
• For a multi-index β,∂|β|
βf(x) :=∂|β|
∂β1x1∂β2x2···∂βnxnf(x); 587
• We also denote the partial derivative with respect to xiby∂ei. 588
589
Definition C.1 (G Set) .LetGdenote the set of continuous functions Rd→Rof at most polynomial 590
growth, i.e. g∈Gif there exists positive integers ν1, ν2>0such that |g(x)| ≤ν1 
1 +|x|2ν2
, for 591
allz∈Rd. 592
The next results are inspired by Theorem 1 of Li et al. (2017) and are derived under some regularity 593
assumption on the function f. 594
Assumption C.2. Assume that the following conditions on f, fi, and their gradients are
satisfied:
•∇f,∇fisatisfy a Lipschitz condition: there exists L >0such that
|∇f(u)− ∇f(v)|+nX
i=1|∇fi(u)− ∇fi(v)| ≤L|u−v|;
•f, fiand its partial derivatives up to order 7 belong to G;
•∇f,∇fisatisfy a growth condition: there exists M > 0such that
|∇f(x)|+nX
i=1|∇fi(x)| ≤M(1 +|x|).
595
17Lemma C.3 (Lemma 1 Li et al. (2017)) .Let0< η < 1. Consider a stochastic process
Xt, t≥0satisfying the SDE
dXt=b(Xt)dt+√ησ(Xt)dWt
withX0=x∈Rdandb, σtogether with their derivatives belong to G. Define the one-step
difference ∆ =Xη−x, and indicate the i-th component of ∆with∆i. Then we have
1.E∆i=biη+1
2hPd
j=1bj∂ejbii
η2+O 
η3
∀i= 1, . . . , d ;
2.E∆i∆j=h
bibj+σσT
(ij)i
η2+O 
η3
∀i, j= 1, . . . , d ;
3.EQs
j=1∆(ij)=O 
η3
for all s≥3, ij= 1, . . . , d .
All functions above are evaluated at x.
596
Theorem C.4 (Theorem 2 and Lemma 5, Mil’shtein (1986)) .Let Assumption C.2 hold and
let us define ¯∆ =x1−xto be the increment in the discrete-time algorithm, and indicate the
i-th component of ¯∆with¯∆i. If in addition there exists K1, K2, K3, K4∈Gso that
1.E∆i−E¯∆i≤K1(x)η2,∀i= 1, . . . , d ;
2.E∆i∆j−E¯∆i¯∆j≤K2(x)η2,∀i, j= 1, . . . , d ;
3.EQs
j=1∆ij−EQs
j=1¯∆ij≤K3(x)η2,∀s≥3,∀ij∈ {1, . . . , d };
4.EQ3
j=1¯∆ij≤K4(x)η2,∀ij∈ {1, . . . , d }.
Then, there exists a constant Cso that for all k= 0,1, . . . , N we have
|Eg(Xkη)−Eg(xk)| ≤Cη.
597
C.1 Limitations 598
Modeling of discrete-time algorithms using SDEs relies on Assumption C.2. As noted by Li et al. 599
(2021), the approximation can fail when the stepsize ηis large or if certain conditions on ∇fand the 600
noise covariance matrix are not met. Although these issues can be addressed by increasing the order 601
of the weak approximation, we believe that the primary purpose of SDEs is to serve as simplification 602
tools that enhance our intuition: We would not benefit significantly from added complexity. 603
C.2 Formal derivation - SignSGD 604
In this subsection, we provide the first formal derivation of an SDE model for SignSGD. Let us 605
consider the stochastic process Xt∈Rddefined as the solution of 606
dXt=−(1−2P(∇fγ(Xt)<0))dt+√ηq
¯Σ(Xt)dWt, (16)
where 607
¯Σ(x) =E[ξγ(x)ξγ(x)⊤], (17)
andξγ(x) := sign(∇fγ(x))−1 + 2P(∇fγ(x)<0)the noise in the sample sign(∇fγ(x)). The 608
following theorem guarantees that such a process is a 1-order SDE of the discrete-time algorithm of 609
SignSGD 610
xk+1=xk−ηsign(fγk(xk)), (18)
withx0∈Rd,η∈R>0is the step size, the mini-batches {γk}are modelled as i.i.d. random variables 611
uniformly distributed on {1,···, N}, and of size B≥1. 612
18Theorem C.5 (Stochastic modified equations) .Let0< η < 1, T > 0and set N=⌊T/η⌋.
Letxk∈Rd,0≤k≤Ndenote a sequence of SignSGD iterations defined by Eq. (18).
Consider the stochastic process Xtdefined in Eq. (16) and fix some test function g∈Gand
suppose that gand its partial derivatives up to order 6 belong to G.
Then, under Assumption C.2, there exists a constant C >0independent of ηsuch that for all
k= 0,1, . . . , N , we have
|Eg(Xkη)−Eg(xk)| ≤Cη.
That is, the SDE (16) is an order 1weak approximation of the SignSGD iterations (18).
613
Lemma C.6. Under the assumptions of Theorem C.5, let 0< η < 1and consider xk, k≥0
satisfying the SignSGD iterations
xk+1=xk−ηsign(∇fγk(xk))
withx0∈Rd. From the definition the one-step difference ¯∆ =x1−x, then we have
1.E¯∆i=−(1−2P(∂ifγ<0))η∀i= 1, . . . , d ;
2.E¯∆i¯∆j= 
(1−2P(∂ifγ<0)) (1−2P(∂jfγ<0)) + ¯Σ(ij)
η2∀i, j =
1, . . . , d ;
3.EQs
j=1¯∆ij=O 
η3
∀s≥3, i j∈ {1, . . . , d }.
All the functions above are evaluated at x.
614
Proof of Lemma C.6. First of all, we have that by definition 615
E
xi
1−xi
=−ηE[sign(∂ifγ(x)<0)], (19)
which implies 616
E¯∆i=−(1−2P(∂ifγ(x)<0))η∀i= 1, . . . , d. (20)
Second, we have that by definition 617
Eh
(x1−x) (x1−x)⊤i
=Eh
(sign(∂ifγ(x)<0)−1 + 2P(∂ifγ(x)<0)) (21)
(sign(∂ifγ(x)<0)−1 + 2P(∂ifγ(x)<0))⊤i
η2, (22)
which implies that 618
E¯∆i¯∆j= (1−2P(∂ifγ<0)) (1−2P(∂jfγ<0))η2+¯Σ(ij)η2∀i, j= 1, . . . , d. (23)
Finally, by definition 619
EsY
j=1¯∆ij=O 
η3
∀s≥3, i j∈ {1, . . . , d }, (24)
which concludes our proof. 620
Proof of Theorem C.5. To prove this result, all we need to do is check the conditions in Theorem C.4. 621
As we apply Lemma C.3, we make the following choices: 622
•b(x) =−(1−2P(∇fγ(x)<0)); 623
•σ(x) =p¯Σ(x). 624
19First of all, we notice that ∀i= 1, . . . , d , it holds that 625
•E¯∆i1. Lemma C.6= −(1−2P(∂ifγ(x)<0))η; 626
•E∆i1. Lemma C.3= −(1−2P(∂ifγ(x)<0))η+O 
η2
. 627
Therefore, we have that for some K1(x)∈G, 628
E∆i−E¯∆i≤K1(x)η2,∀i= 1, . . . , d. (25)
Additionally, we notice that ∀i, j= 1, . . . , d , it holds that 629
•E¯∆i¯∆j2. Lemma C.6= (1 −2P(∂ifγ(x)<0)) (1−2P(∂jfγ(x)<0))η2+¯Σ(ij)(x)η2; 630
•E∆i∆j2. Lemma C.3= 
(1−2P(∂ifγ(x)<0)) (1−2P(∂jfγ(x)<0)) + ¯Σ(ij)(x)
η2+ 631
O 
η3
. 632
Therefore, we have that for some K2(x)∈G, 633
E∆i∆j−E¯∆i¯∆j≤K2(x)η2,∀i, j= 1, . . . , d. (26)
Additionally, we notice that ∀s≥3,∀ij∈ {1, . . . , d }, it holds that 634
•EQs
j=1¯∆ij3. Lemma C.6= O 
η3
; 635
•EQs
j=1∆ij3. Lemma C.3= O 
η3
. 636
Therefore, we have that for some K3(x)∈G, 637
EsY
j=1∆ij−EsY
j=1¯∆ij≤K3(x)η2. (27)
Additionally, for some K4(x)∈G,∀ij∈ {1, . . . , d }, 638
E3Y
j=1¯∆(ij)3. Lemma C.6
≤ K4(x)η2. (28)
To conclude, Eq. (25), Eq. (26), Eq. (27), and Eq. (28) allow us to conclude the proof. 639
Corollary C.7. Let us take the same assumptions of Theorem C.5, and that the stochastic
gradient is ∇fγ(x) =∇f(x) +Usuch that U∼ N(0,Σ)that does not depend on x. Then,
the following SDE provides a 1weak approximation of the discrete update of SignSGD
dXt=−Erf 
Σ−1
2∇f(Xt)√
2!
dt+√ηvuutId−diag 
Erf 
Σ−1
2∇f(Xt)√
2!!2
dWt,(29)
where the error function Erf(x)and the square are applied component-wise, and Σ =
diag 
σ2
1,···, σ2
d
.
640
Proof of Corollary C.7. First of all, we observe that 641
1−2P(∇fγ(x)<0) = 1 −2P
∇f(x) + Σ1
2U <0
= 1−2Φ
−Σ−1
2∇f(x)
, (30)
20where Φis the cumulative distribution function of the standardized normal distribution. Remembering 642
that 643
Φ(x) =1
2
1 +Erfx√
2
, (31)
we have that 644
1−2P(∇fγ(x)<0) = 1 −21
2 
1 +Erf 
−Σ−1
2∇f(x)√
2!!
=Erf 
Σ−1
2∇f(x)√
2!
.(32)
Similarly, one can prove that ¯Σdefined in (17) becomes 645
¯Σ =Id−diag 
Erf 
Σ−1
2∇f(Xt)√
2!!2
. (33)
646
Corollary C.8. Let us take the same assumptions of Theorem C.5, and that the stochastic
gradient is ∇fγ(x) =∇f(x) +√
ΣUsuch that U∼tν(0, Id)that does not depend on x
andνis a positive integer number. Then, the following SDE provides a 1weak approximation
of the discrete update of SignSGD
dXt=−2Ξ
Σ−1
2∇f(Xt)
dt+√ηr
Id−4 diag
Ξ
Σ−1
2∇f(Xt)2
dWt,(34)
where Ξ(x)is defined as
Ξ(x) :=xΓ ν+1
2
√πνΓ ν
22F11
2,ν+ 1
2;3
2;−x2
ν
, (35)
and 2F1(a, b;c;x)is the hypergeometric function. Above, function Ξ(x)and the square are
applied component-wise, and Σ = diag 
σ2
1,···, σ2
d
.
647
Proof of Corollary C.8. First of all, we observe that 648
1−2P(∇fγ(x)<0) = 1 −2P
∇f(x) + Σ1
2U <0
= 1−2Fν
−Σ−1
2∇f(x)
, (36)
where Fν(x)is the cumulative function of a tdistribution with νdegrees of freedom. Remembering 649
that 650
Fν(x) =1
2+ Ξ(x), (37)
we have that 651
1−2P(∇fγ(x)<0) = 1 −21
2+ Ξ(x)
=−2Ξ(x). (38)
Similarly, one can prove that ¯Σdefined in (17) becomes 652
¯Σ =Id−4 diag
Ξ
Σ−1
2∇f(Xt)2
. (39)
653
Lemma C.9. Under the assumptions of Corollary C.7 and signal-to-noise ratio Yt:=Σ−1
2∇f(Xt)√
2, 654
1.Phase 1: If|Yt|>3
2, the SDE coincides with the ODE of SignGD: 655
dXt=−sign(∇f(Xt))dt; (40)
2.Phase 2: If1<|Yt|<3
2: 656
(a)mYt+q−≤dE[Xt]
dt≤mYt+q+; 657
21(b)P
∥Xt−E[Xt]∥2
2> a
≤η
a 
d− ∥mYt+q−∥2
2
; 658
3.Phase 3: If|Yt|<1, the SDE is 659
dXt=−r
2
πΣ−1
2∇f(Xt)dt+√ηr
Id−2
πdiag
Σ−1
2∇f(Xt)2
dWt. (41)
Proof of Lemma C.9. Exploiting the regularity of the Erf function, we approximate the SDE in (29) 660
in three different regions: 661
1.Phase 1: If|x|>3
2, Erf(x)∼sign(x). Therefore, ifΣ−1
2∇f(Xt)√
2>3
2, 662
(a) Erf
Σ−1
2∇f(Xt)√
2
∼sign
Σ−1
2∇f(Xt)√
2
=sign(∇f(Xt)); 663
(b) Erf
Σ−1
2∇f(Xt)√
22
∼sign
Σ−1
2∇f(Xt)√
22
= (1, . . . , 1). 664
Therefore, 665
dXt=−Erf 
Σ−1
2∇f(Xt)√
2!
dt+√ηvuutId−diag 
Erf 
Σ−1
2∇f(Xt)√
2!!2
dWt
∼ −sign(∇f(Xt)); (42)
2.Phase 2: Letmandq1are the slope and intercept of the line secant to the graph of Erf(x) 666
between the points (1,Erf(1)) and 3
2,Erf 3
2
, while q2is the intercept of the line tangent 667
to the graph of Erf (x)and slope m. If1< x <3
2, we have that 668
mx+q1<Erf(x)< mx +q2. (43)
Analogously, if −3
2< x < −1 669
mx−q2<Erf(x)< mx −q1. (44)
Therefore, we have that if 1<Σ−1
2∇f(Xt)√
2<3
2, then 670
(a)
m√
2Σ−1
2∇f(Xt) +q−<Erf 
Σ−1
2∇f(Xt)√
2!
<m√
2Σ−1
2∇f(Xt) +q+,(45)
where 671
(q+)i:=q2 if∂if(x)>0
−q1if∂if(x)<0,(46)
and 672
(q−)i:=q1 if∂if(x)>0
−q2if∂if(x)<0,(47)
Therefore, 673
m√
2Σ−1
2∇f(Xt) +q−≤dE[Xt]
dt≤m√
2Σ−1
2∇f(Xt) +q+; (48)
(b) Similar to the above, 674
m√
2Σ−1
2∇f(Xt) +q−2
≤Erf 
Σ−1
2∇f(Xt)√
2!2
≤m√
2Σ−1
2∇f(Xt) +q+2
.
22Therefore, 675
P
∥Xt−E[Xt]∥2
2> a
≤P
∃is.t.|Xi
t−E
Xi
t
|2> a
(49)
≤X
iP
|Xi
t−E
Xi
t
|>√a
≤η
aX
i
1−Erf 
Σ−1
2
i∂if(Xt)√
2!2
 (50)
<η
a
d− ∥m√
2Σ−1
2∇f(Xt) +q−∥2
2
. (51)
3.Phase 3: If|x|<1, Erf(x)∼2√π. Therefore, ifΣ−1
2∇f(Xt)√
2<1, 676
(a) Erf
Σ−1
2∇f(Xt)√
2
∼q
2
πΣ−1
2∇f(Xt); 677
(b)
Erf
Σ−1
2∇f(Xt)√
22
∼2
π
Σ−1
2∇f(Xt)2
. 678
Therefore, 679
dXt=−Erf 
Σ−1
2∇f(Xt)√
2!
dt+√ηvuutId−diag 
Erf 
Σ−1
2∇f(Xt)√
2!!2
dWt
∼ −r
2
πΣ−1
2∇f(Xt)dt+√ηr
Id−2
πdiag
Σ−1
2∇f(Xt)2
dWt. (52)
680
Lemma C.10 (Dynamics of Expected Loss) .Letfbeµ-strongly convex, Tr(∇2f(x))≤ Lτ, and 681
St:=f(Xt)−f(X∗). Then, during 682
1. Phase 1, the dynamics will stop before t∗= 2q
S0
µbecause St≤1
4 √µt−2√S02; 683
2. Phase 2 with ∆ :=
m√
2σmax+ηµm2
4σ2max
:E[St]≤S0e−2µ∆t+η
2(Lτ−µdˆq2)
2µ∆ 
1−e−2µ∆t
; 684
3. Phase 3 with ∆ :=q
2
π1
σmax+η
πµ
σ2max
:E[St]≤S0e−2µ∆t+η
2Lτ
2µ∆ 
1−e−2µ∆t
. 685
Proof of Lemma C.10. We prove each point by leveraging the shape of the law of Xtderived in 686
Lemma C.9: 687
1.Phase 1: 688
d(f(Xt)−f(X∗)) =−∇f(Xt)sign(∇f(Xt)) =−∥∇f(Xt)∥1≤ −∥∇ f(Xt)∥2(53)
Since fisµ−PL, we have that −∥∇f(Xt)∥2
2<−2µ(f(Xt)−f(X∗)), which implies 689
that 690
f(Xt)−f(X∗)≤1
4√µt−2p
f(X0)−f(X∗)2
, (54)
meaning that the dynamics will stop before t∗= 2q
f(X0)−f(X∗)
µ; 691
2.Phase 2: By applying the Itô Lemma to f(Xt)−f(X∗)and that 692
m√
2Σ−1
2∇f(Xt) +q−<Erf 
Σ−1
2∇f(Xt)√
2!
<m√
2Σ−1
2∇f(Xt) +q+, (55)
23we have that if ˆq:= max( q1, q2), 693
d(f(Xt)−f(X∗))≤ −m√
2Σ−1
2∇f(Xt) +q−⊤
∇f(Xt)dt+O(Noise ) (56)
+η
2Tr"
∇2f(Xt) 
Id−diagm√
2Σ−1
2∇f(Xt) +q−2!#
(57)
≤ −m√
21
σmax∥∇f(Xt)∥2
2dt−ˆq∥∇f(Xt)∥1dt+ηLτ
2dt (58)
−ηµ
2∥m√
2Σ−1
2∇f(Xt) +q−∥2
2dt+O(Noise ) (59)
≤ −m√
21
σmax∥∇f(Xt)∥2
2dt−ˆq∥∇f(Xt)∥1dt+ηLτ
2dt (60)
−ηµm2
4σ2max∥∇f(Xt)∥2
2dt−ηµdˆq2
2dt−√
2mˆq
σmax∥∇f(Xt)∥1dt(61)
+O(Noise ) (62)
≤ −2µm√
2σmax+ηµm2
4σ2max
(f(Xt)−f(X∗))dt (63)
+η
2 
Lτ−µdˆq2
dt+O(Noise ), (64)
which implies that if k:= 2µ
m√
2σmax+ηµm2
4σ2max
, 694
E[f(Xt)−f(X∗)]≤(f(X0)−f(X∗)))e−kt+η 
Lτ−µdˆq2
2k 
1−e−kt
. (65)
3.Phase 3: By applying the Itô Lemma to f(Xt)−f(X∗), we have that: 695
d(f(Xt)−f(X∗)) =−r
2
π∇f(Xt)⊤Σ−1
2∇f(Xt)dt+O(Noise ) (66)
+η
2Tr
Id−2
πdiag
Σ−1
2∇f(Xt)2
∇2f(Xt)
dt (67)
≤ −r
2
π1
σmax∥∇f(Xt)∥2
2dt+O(Noise ) (68)
+η
2Tr 
∇2f(Xt)
dt−η
πµ
σ2max∥∇f(Xt)∥2
2dt (69)
≤ − r
2
π1
σmax+η
πµ
σ2max!
∥∇f(Xt)∥2
2dt (70)
+η
2Tr(∇2f(Xt))dt+O(Noise ) (71)
Since fisµ-Strongly Convex, fis also µ-PL. Therefore, we have 696
d(f(Xt)−f(X∗))≤ −2µ r
2
π1
σmax+η
πµ
σ2max!
(f(Xt)−f(X∗))dt (72)
+η
2Tr(∇2f(Xt))dt+O(Noise ). (73)
Therefore, 697
dE[f(Xt)−f(X∗)]≤ −2µ r
2
π1
σmax+η
πµ
σ2max!
(E[f(Xt)−f(X∗)])dt+η
2Lτdt,
(74)
24which implies that if k:= 2µq
2
π1
σmax+η
πµ
σ2max
, 698
E[f(Xt)−f(X∗)]≤(f(X0)−f(X∗)))e−kt+ηLτ
2k 
1−e−kt
. (75)
699
Lemma C.11. Under the assumptions of Lemma 3.5, for any step size scheduler ηtsuch that 700
Z∞
0ηsds=∞and lim
t→∞ηt= 0 = ⇒E[f(Xt)−f(X∗)]t→∞→0. (76)
Proof of Lemma C.11. For any scheduler ηkused in 701
xk+1=xk−ηηksign(fγk(xk)), (77)
the SDE of Phase 3 is 702
dXt=−r
2
πΣ−1
2∇f(Xt)ηtdt+√ηηtr
Id−2
πdiag
Σ−1
2∇f(Xt)2
dWt. (78)
Therefore, analogously to the calculations in Lemma C.10, we have that 703
E[f(Xt)−f(X∗)]≤f(X0)−f(X∗) +ηLτ
2Rt
0e2µRs
0√
2
π1
σmaxηl+η
πµ
σ2maxη2
l
dlη2
sds
e2µRt
0√
2
π1
σmaxηs+η
πµ
σ2maxη2s
ds. (79)
Therefore, using l’Hôpital’s rule we have that 704
Z∞
0ηsds=∞andlim
t→∞ηt= 0 = ⇒E[f(Xt)−f(X∗)]t→∞→0. (80)
705
Lemma C.12. LetH= diag( λ1, . . . , λ d)andMt:=e−2√
2
πΣ−1
2H+η
πΣ−1
2H2
t. Then, 706
1.E[Xt] =e−√
2
πΣ−1
2HtX0; 707
2.V ar[Xt] =
Mt−e−2√
2
πΣ−1
2Ht
X2
0+η
2q
2
πId+η
πH−1
H−1Σ1
2(Id−Mt). 708
Proof of Lemma C.12. The proof is banal: The expected value derivation leverages the martingale 709
property of the Brownian motion while that of the variance uses the Ito Isomerty. 710
Lemma C.13. LetH= diag( λ1, . . . , λ d). Then, Eh
X⊤
tHXt
2i
is equal to 711
dX
i=1λi(Xi
0)2
2e−2λi√
2
π1
σi+λiη
πσ2
i
t+η
4q
2
π1
σi+λiη
πσ2
i 
1−e−2λi√
2
π1
σi+λiη
πσ2
i
t!
. (81)
Proof of Lemma C.13. Since the matrix His diagonal, we focus on a single component. We apply 712
the Ito Lemma toλi(Xi
t)2
2: 713
dλi(Xi
t)2
2
=−2r
2
πλi
σiλi(Xi
t)2
2dt+ηλi
2dt−2λ2
iη
πσ2
iλi(Xi
t)2
2+O(Noise ), (82)
which implies that 714
Eλi(Xi
t)2
2
=λi(Xi
0)2
2e−2√
2
πλi
σi+λ2
iη
πσ2
i
t+η
4q
2
π1
σi+λiη
πσ2
i 
1−e−2√
2
πλi
σi+λ2
iη
πσ2
i
t!
.
(83)
25Therefore, 715
EX⊤
tHXt
2
=dX
i=1λi(Xi
0)2
2e−2λi√
2
π1
σi+λiη
πσ2
i
t+η
4q
2
π1
σi+λiη
πσ2
i 
1−e−2λi√
2
π1
σi+λiη
πσ2
i
t!
.
(84)
716
Lemma C.14. Under the assumptions of Corollary C.8, where ∇fγ(x) =∇f(x) +√
ΣU, we have 717
that the dynamics of SignSGD in Phase 3 is: 718
dXt=−r
1
2Σ−1
2∇f(Xt)dt+√ηr
Id−1
2diag
Σ−1
2∇f(Xt)2
dWt. (85)
Proof of lemma C.14. We apply Eq. (34) withν= 2 and linearly approximate Ξ(x)as|x|<1, 719
where 2Ξ(x)∼x√
2. 720
C.3 Formal derivation - RMSprop 721
2
 0 2
X11.00
0.75
0.50
0.25
0.000.250.500.751.00X2Trajectories
RMSprop
SDE (Ours)
SDE (Malladi et al.)
01020304050607080
0250 500 750 1000 1250 1500 1750 2000
Iterations104
103
102
101
100101102LossLosses
RMSprop
SDE (Ours)
SDE (Malladi et al.)
1.0
 0.5
 0.0 0.5 1.0
X11.0
0.5
0.00.51.0X2Trajectories
RMSprop
SDE (Ours)
SDE (Malladi et al.)
0.3
0.00.30.60.91.21.51.82.12.4
0200 400 600 800 1000 1200 1400 1600
Iterations105
104
103
102
101
100LossLosses
RMSprop
SDE (Ours)
SDE (Malladi et al.)
Figure 7: The first two subfigures on the left compare our SDE, that from Malladi et al. (2022), and
RMSprop in terms of trajectories and f(x), respectively, for a convex quadratic function. The others
subfigures do the same for an embedded saddle and one clearly observes that our derived SDE better
matches RMSprop.
In this subsection, we provide our formal derivation of an SDE model for RMSprop. Let us consider 722
the stochastic process Lt:= (Xt, Vt)∈Rd×Rddefined as the solution of 723
dXt=−P−1
t(∇f(Xt)dt+√ηΣ(Xt)1
2dWt) (86)
dVt=ρ((∇f(Xt))2+ diag(Σ( Xt))−Vt))dt, (87)
where β= 1−ηρ,ρ=O(1), and Pt:= diag ( Vt)1
2+ϵId. 724
Remark C.15 .We observe that the term in blue is the only difference w.r.t. the SDE derived in 725
(Malladi et al., 2022) (see Theorem D.2): This is extremely relevant when the gradient size is not 726
negligible. Figure 7 shows the comparison between our SDE, the one derived in (Malladi et al., 2022), 727
and RMSprop itself: It is clear that even on simple landscapes, our SDE matches the algorithm much 728
better. Importantly, one can observe that the SDE derived in (Malladi et al., 2022) is only slightly 729
worse than ours at the end of the dynamics: As we show in Lemma C.17, Theorem D.2 is a corollary 730
of Theorem C.16 when ∇f(x) =O(√η): It only describes the dynamics where the gradient is 731
vanishing. In Figure 8, we compare the two SDEs in question with RMSprop on an MLP, a CNN, a 732
ResNet, and a Transformer: Our SDE exhibits a superior description of the dynamics. 733
260250 500 750 1000 1250 1500 1750 2000
Iterations0.00.10.20.30.40.50.60.7LossLosses - DNN
RMSprop
SDE (Ours)
SDE (Malladi et al.)
0250 500 750 1000 1250 1500 1750 2000
Iterations01234LossLosses - CNN
RMSprop
SDE (Ours)
SDE (Malladi et al.)
0250 500 750 1000 1250 1500 1750 2000
Iterations101
100101LossLosses - Transformer
RMSprop
SDE (Ours)
SDE (Malladi et al.)
100101102103
Iterations0510152025LossLosses - ResNet
RMSprop
SDE (Ours)
SDE (Malladi et al.)Figure 8: We compare our SDE, that from Malladi et al. (2022), and RMSprop in terms of f(x): The
first is an MLP on the Breast Cancer dataset, the second a CNN on MNIST, the third a Transformer
on MNIST, and the last a ResNet on CIFAR-10: Ours match the algorithms better.
The following theorem guarantees that such a process is a 1-order SDE of the discrete-time algorithm 734
of RMSprop 735
xk+1=xk−η∇fγk(xk)√vk+1+ϵId(88)
vk+1=βvk+ (1−β) (∇fγk(xk))2(89)
with(x0, v0)∈Rd×Rd,η∈R>0is the step size, β= 1−ρηforρ=O(1), the mini-batches {γk} 736
are modelled as i.i.d. random variables uniformly distributed on {1,···, N}, and of size B≥1. 737
Theorem C.16 (Stochastic modified equations) .Let0< η < 1, T > 0and set N=⌊T/η⌋.
Letlk:= (xk, vk)∈Rd×Rd,0≤k≤Ndenote a sequence of RMSprop iterations defined
by Eq. (88). Consider the stochastic process Ltdefined in Eq. (86) and fix some test function
g∈Gand suppose that gand its partial derivatives up to order 6 belong to G.
Then, under Assumption C.2 and ρ=O(1)there exists a constant C >0independent of η
such that for all k= 0,1, . . . , N , we have
|Eg(Lkη)−Eg(lk)| ≤Cη.
That is, the SDE (86) is an order 1weak approximation of the RMSprop iterations (88).
738
Proof. The proof is virtually identical to that of Theorem C.5. Therefore, we only report the key 739
steps necessary to conclude the thesis. First of all, we observe that since β= 1−ηρ 740
vk+1−vk=−ηρ
vk−(∇fγk(xk))2
. (90)
Then, 741
1√vk+1=s
vk
vk+11
vk=s
vk+1+O(η)
vk+11
vk=s
1 +O(η)
vk+1r
1
vk∼r
1
vk(1 +O(η)).(91)
Therefore, we work with the following algorithm as all the approximations below only carry an 742
additional error of order O(η2), which we can ignore. Therefore, we have that 743
xk+1−xk=−η∇fγk(xk)√vk+ϵId(92)
vk−vk−1=−ηρ
vk−1− 
∇fγk−1(xk−1)2
. (93)
Therefore, if ∇fγj(xj) =∇f(xj) +Zj(xj),E[Zj(xj)] = 0 , and Cov(Zj(xj)) = Σ( xj) 744
1.E[xk+1−xk] =−ηdiag( vk+ϵId)−1
2∇f(xk); 745
2.E[vk−vk−1] =ηρh
(∇f(xk−1))2+ diag(Σ( xk))−vk−1i
. 746
27Then, we have that if Φk:=∇f(xk)√vk+ϵId−∇fγk(xk)√vk+ϵId747
1.
E[(xk+1−xk)(xk+1−xk)⊤] =E[(xk+1−xk)]E[(xk+1−xk)]⊤(94)
+η2Eh
(Φk) (Φk)⊤i
(95)
=E[(xk+1−xk)]E[(xk+1−xk)]⊤(96)
+η2(diag( vk) +ϵId)−1Σ(xk); (97)
2.E[(vk−vk−1)(vk−vk−1)⊤] =E[(vk−vk−1)]E[(vk−vk−1)]⊤+O(ρη2); 748
3.E[(xk+1−xk)(vk−vk−1)⊤] =E[(xk+1−xk)]E[(vk−vk−1)⊤] + 0 . 749
Therefore 750
dXt=−P−1
t(∇f(Xt)dt+√ηΣ(Xt)1
2dWt) (98)
dVt=ρ(((∇f(Xt))2+ diag(Σ( Xt))−Vt))dt. (99)
751
Lemma C.17. If(∇f(x))2=O(η), Theorem D.2 is a Corollary of Theorem C.16. 752
Proof. In the proof of Theorem C.16, one drops the term η(∇f(x))2as it is of order η2. 753
Corollary C.18. Under the assumptions of Theorem C.16 with Σ(x) =σ2Id,˜η=κη,˜B=Bδ, and 754
˜ρ=αρ, 755
dXt=κdiag( Vt)−1
2
−∇f(Xt)dt+1√
δrη
BσIddWt
(100)
dVt=α
κρ
(∇f(Xt))2+σ2
Bδ1−Vt
dt. (101)
Lemma C.19 (Scaling Rule at Convergence) .Under the assumptions of Corollary C.18, fisµ- 756
strongly convex, Lτ:=Tr(∇2f(x)), and (∇f(x))2=O(η), the asymptotic dynamics of the iterates 757
of RMSprop satisfies the classic scaling rule κ=√
δbecause 758
E[f(Xt)−f(X∗)]t→∞
≤ησLτ
4µ√
Bκ√
δ. (102)
By enforcing that the speed of Vtmatches that of Xt, one needs ˜ρ=κ2ρ, which implies ˜β= 759
1−κ2(1−β). 760
Proof of Lemma C.19. In order to recover the scaling of β, we enforce that the rate at which Vt 761
converges to its limit matches the speed of Xt: We need ˜ρ=κ2ρ, which recovers the classic scaling 762
˜β= 1−κ2(1−β). Additionally, since (∇f(x))2=O(η)we have that 763
dXt=κdiag( Vt)−1
2
−∇f(Xt)dt+1√
δrη
BσIddWt
(103)
dVt=κρσ2
Bδ1−Vt
dt. (104)
Therefore, Vtt→∞→σ2
Bδ1, meaning that under these conditions: 764
dXt=−√
Bδκ
σ∇f(Xt)dt+κ√ηIddWt, (105)
which satisfies the following for µ-strongly convex functions 765
dE[f(Xt)−f(X∗)]≤ −2κµ√
Bδ
σE[f(Xt)−f(X∗)]dt+κ2ηLτ
2dt, (106)
28meaning that E[f(Xt)−f(X∗)]t→∞
≤ησLτ
4µ√
Bκ√
δ. 766
Since the asymptotic the loss isη
2Lτσ
2µ√
Bκ√
δdoes not depend on κandδifκ√
δ= 1, we recover the 767
classic scaling rule. 768
Remark: Under the same conditions, SGD satisfies 769
dXt=−κ∇f(Xt)dt+κ1√
δrη
BσIddWt (107)
and therefore 770
E[f(Xt)−f(X∗)]≤(f(X0)−f(X∗))e−2µκt+η
2Lτσ2
2µBκ
δ 
1−e−2µκt
, (108)
meaning that asymptotically the loss isη
2Lτσ2
2µBκ
δwhich does not depend on κandδifκ
δ= 1. 771
Lemma C.20. Forf(x) :=x⊤Hx
2, the stationary distribution of RMSprop is (E[X∞]], Cov (X∞)) = 772
0,η
2Σ1
2H−1
. 773
Proof. As(∇f(x))2=O(η)andt→ ∞ , we have 774
dXt=−Σ−1
2HXtdt+√ηIddWt (109)
which implies that 775
Xt=e−Σ−1
2Ht
X0+√ηZt
0eΣ−1
2HsdWs
. (110)
The thesis follows from the martingale property of Brownian motion and the Itô isometry. 776
C.4 RMSpropW 777
1 400 800 1200 1600 2000
Iterations0.10.20.30.40.50.60.7LossLosses - DNN
AdamW
SDE
1 400 800 1200 1600 2000
Iterations1.01.21.41.61.82.02.22.4LossLosses - CNN
AdamW
SDE
1 400 800 1200 1600 2000
Iterations0.10.20.30.40.50.60.7LossLosses - DNN
RMSpropW
SDE
1 400 800 1200 1600 2000
Iterations012345LossLosses - CNN
RMSpropW
SDE
Figure 9: The first two represent the comparison between AdamW and its SDE in terms of f(x).
The other two do the same for RMSpropW. In both cases, the first is an MLP on the Breast Cancer
Dataset and the second a CNN on MNIST: Our SDEs match the respective optimizers.
In this subsection, we derive the SDE of RMSpropW defined as 778
xk+1=xk−η∇fγk(xk)√vk+1+ϵId−ηγxk (111)
vk+1=βvk+ (1−β) (∇fγk(xk))2(112)
with(x0, v0)∈Rd×Rd,η∈R>0is the step size, β= 1−ρηforρ=O(1),γ >0, the mini-batches 779
{γk}are modelled as i.i.d. random variables uniformly distributed on {1,···, N}, and of size B≥1. 780
Theorem C.21. Under the same assumptions as Theorem C.16, the SDE of RMSpropW is 781
dXt=−P−1
t(∇f(Xt)dt+√ηΣ(Xt)1
2dWt)−γXtdt (113)
dVt=ρ((∇f(Xt))2+ diag(Σ( Xt))−Vt))dt, (114)
where β= 1−ηρ,ρ=O(1),γ >0, and Pt:= diag ( Vt)1
2+ϵId. 782
29Proof. The proof is the same as the of Theorem C.16 and the only difference is that ηγxkis 783
approximated with γXtdt. 784
Figure 4 and Figure 9 validate this result on a variety of architectures and datasets. 785
Corollary C.22. Under the assumptions of Theorem C.21 with Σ(x) =σ2Id,˜η=κη,˜B=Bδ, and 786
˜ρ=αρ, and ˜γ=ξγ, 787
dXt=κdiag( Vt)−1
2
−∇f(Xt)dt+1√
δrη
BσIddWt
−ξγκX tdt (115)
dVt=α
κρ
(∇f(Xt))2+σ2
Bδ1−Vt
dt. (116)
Lemma C.23 (Scaling Rule at Convergence) .Under the assumptions of Corollary C.22, fisµ- 788
strongly convex and L-smooth, Lτ:=Tr(∇2f(x)), and (∇f(x))2=O(η), the asymptotic dynamics 789
of the iterates of RMSpropW satisfies the novel scaling rule if κ=√
δandξ=κbecause 790
E[f(Xt)−f(X∗)]t→∞
≤ηLτσL
2κ
2µ√
BδL +σξγ(L+µ). (117)
By enforcing that the speed of Vtmatches that of Xt, one needs ˜ρ=κ2ρ, which implies ˜β= 791
1−κ2(1−β). 792
Proof of Lemma C.23. In order to recover the scaling of β, we enforce that the rate at which Vt 793
converges to its limit matches the speed of Xt: We need ˜ρ=κ2ρ, which recovers the classic scaling 794
˜β= 1−κ2(1−β). Additionally, since (∇f(x))2=O(η)we have that 795
dXt=κdiag( Vt)−1
2
−∇f(Xt)dt+1√
δrη
BσIddWt
−κξγX tdt (118)
dVt=κρσ2
Bδ1−Vt
dt. (119)
Therefore, Vtt→∞→σ2
Bδ1, meaning that under these conditions: 796
dXt=−√
Bδκ
σ∇f(Xt)dt+κ√ηIddWt−κξγX tdt, (120)
which satisfies the following for µ-strongly convex and L-smooth functions 797
dE[f(Xt)−f(X∗)]≤κ 
2µ√
Bδ
σ+ξγ
1 +µ
L!
E[f(Xt)−f(X∗)]dt+κ2ηLτ
2dt, (121)
meaning that E[f(Xt)−f(X∗)]t→∞
≤ηLτσL
2κ
2µ√
BδL+σξγ(L+µ). 798
Since the asymptotic the lossηLτσL
2κ
2µ√
BδL+σξγ(L+µ)does not depend on κandδandξifκ=ξ= 799
√
δ, we recover the novel scaling rule. 800
Lemma C.24. For f(x) :=x⊤Hx
2, the stationary distribution of RMSpropW is 801
(E[X∞]], Cov (X∞)) =
0,η
2(HΣ−1
2+γId)−1
. 802
Proof. As(∇f(x))2=O(η)andt→ ∞ , we have 803
dXt=−Σ−1
2HXtdt+√ηIddWt−γXtdt (122)
which implies that 804
Xt=e−(Σ−1
2H+γId)t
X0+√ηZt
0e(Σ−1
2H+γId)sdWs
. (123)
The thesis follows from the martingale property of Brownian motion and the Itô isometry. 805
301
 0 1
X11.0
0.5
0.00.51.0X2Trajectories
Adam
SDE (Ours)
SDE (Malladi et al.)
03691215182124
0 10000 20000 30000 40000 50000
Iterations105
104
103
102
101
100101LossLosses
Adam
SDE (Ours)
SDE (Malladi et al.)
1
 0 1
X10.6
0.4
0.2
0.00.20.40.6X2Trajectories
Adam
SDE (Ours)
SDE (Malladi et al.)
0.4
0.00.40.81.21.62.02.42.8
0 5000 10000 15000 20000 25000
Iterations104
103
102
101
100LossLosses
Adam
SDE (Ours)
SDE (Malladi et al.)Figure 10: The first two on the left compare our SDE, that from Malladi et al. (2022), and Adam in
terms of trajectories and f(x), respectively, for a convex quadratic function. The others do the same
for an embedded saddle: Ours clearly matches Adam better.
C.5 Formal derivation - Adam 806
In this subsection, we provide our formal derivation of an SDE model for Adam. Let us consider the 807
stochastic process Lt:= (Xt, Mt, Vt)∈Rd×Rd×Rddefined as the solution of 808
dXt=−p
γ2(t)
γ1(t)P−1
t(Mt+ηρ1(∇f(Xt)−Mt))dt (124)
dMt=ρ1(∇f(Xt)−Mt)dt+√ηρ1Σ1/2(Xt)dWt (125)
dVt=ρ2 
(∇f(Xt))2+ diag (Σ ( Xt))−Vt
dt, (126)
where βi= 1−ηρi,γi(t) = 1 −e−ρit,ρ1=O(η−ζ)s.t.ζ∈(0,1),ρ2=O(1), and Pt= 809
diag√Vt+ϵp
γ2(t)Id. 810
Remark C.25 .The terms in purple and in blue are the two differences w.r.t. that of (Malladi et al., 811
2022) which is reported in Theorem D.5. The first appears because we assume realistic values of β1 812
while the second appears because we allow the gradient size to be non-negligible. For two simple 813
landscapes, Figure 10 compares our SDE and that of Malladi et al. (2022) with Adam: In both 814
cases, the first part of the dynamics is perfectly represented only by our SDE. While the discrepancy 815
between the SDE of (Malladi et al., 2022) and Adam is asymptotically negligible in the convex 816
setting, we observe that in the nonconvex case, it converges to a different local minimum than ours 817
and of Adam. Finally, Theorem D.5 is a corollary of ours when (∇f(x))2=O(η)andρ1=O(1): 818
It only describes the dynamics where the gradient to noise ratio is vanishing and only for unrealistic 819
values of β1= 1−ηρ1. In Figure 11, we compare the dynamics of our SDE, that of Malladi et al. 820
(2022), and Adam on an MLP, a CNN, a ResNet, and a Transformer. One can clearly see that our 821
SDE more accurately captures the dynamics. Details on these experiments are available in Appendix 822
F. 823
The following theorem guarantees that such a process is a 1-order SDE of the discrete-time algorithm 824
of Adam 825
vk+1=β2vk+ (1−β2) (∇fγk(xk))2(127)
mk+1=β1mk+ (1−β1)∇fγk(xk) (128)
ˆmk=mk 
1−βk
1−1(129)
ˆvk=vk 
1−βk
2−1(130)
xk+1=xk−ηˆmk+1p
ˆvk+1+ϵId, (131)
with(x0, m0, v0)∈Rd×Rd×Rd,η∈R>0is the step size, βi= 1−ρiηforρ1=O(η−ζ)s.t. 826
ζ∈(0,1),ρ2=O(1), the mini-batches {γk}are modelled as i.i.d. random variables uniformly 827
distributed on {1,···, N}, and of size B≥1. 828
310250 500 750 1000 1250 1500 1750 2000
Iterations0.10.20.30.40.50.60.7LossLosses - DNN
Adam
SDE (Ours)
SDE (Malladi et al.)
0250 500 750 1000 1250 1500 1750 2000
Iterations0123456LossLosses - CNN
Adam
SDE (Ours)
SDE (Malladi et al.)
0250 500 750 1000 1250 1500 1750 2000
Iterations010203040LossLosses - Transformer
Adam
SDE (Ours)
SDE (Malladi et al.)
100101102103
Iterations100101102LossLosses - ResNet
Adam
SDE (Ours)
SDE (Malladi et al.)Figure 11: We compare our SDE, that from Malladi et al. (2022), and Adam in terms of f(x): The
first is an MLP on the Breast Cancer dataset, the second a CNN on MNIST, the third a Transformer
on MNIST, and the last a ResNet on CIFAR-10: Ours match the algorithms better.
Theorem C.26 (Stochastic modified equations) .Let0< η < 1, T > 0and set N=⌊T/η⌋.
Letlk:= (xk, mk, vk)∈Rd×Rd×Rd,0≤k≤Ndenote a sequence of Adam iterations
defined by Eq. (127) . Consider the stochastic process Ltdefined in Eq. (124) and fix some
test function g∈Gand suppose that gand its partial derivatives up to order 6 belong to G.
Then, under Assumption C.2 ρ1=O(η−ζ)s.t.ζ∈(0,1), while ρ2=O(1), there exists a
constant C >0independent of ηsuch that for all k= 0,1, . . . , N , we have
|Eg(Lkη)−Eg(lk)| ≤Cη.
That is, the SDE (124) is an order 1weak approximation of the Adam iterations (127) .
829
Proof. The proof is virtually identical to that of Theorem C.5. Therefore, we only report the key 830
steps necessary to conclude the thesis. First of all, we observe that since β1= 1−ηρ1 831
vk+1−vk=−ηρ1
vk−(∇fγk(xk))2
. (132)
Then, 832
1√vk+1=s
vk
vk+11
vk=s
vk+1+O(η)
vk+11
vk=s
1 +O(η)
vk+1r
1
vk∼r
1
vk(1 +O(η)).(133)
Therefore, we work with the following algorithm as all approximations only carry an additional error 833
of order O(η2), which we can ignore. Therefore, we have that 834
vk−vk−1=−ηρ2
vk−1− 
∇fγk−1(xk−1)2
(134)
mk+1−mk=−ηρ1(mk− ∇fγk(xk)) (135)
ˆmk=mk 
1−βk
1−1(136)
ˆvk=vk 
1−βk
1−1(137)
xk+1−xk=−η√vk+ϵIdp
1−(1−ηρ2)k
1−(1−ηρ1)k+1(mk+ηρ1(∇fγk(xk)−mk)). (138)
Therefore, if ∇fγj(xj) =∇f(xj) +Zj(xj)andE[Zj(xj)] = 0 , and Cov(Zj(xj)) = Σ( xj), we 835
have that 836
1.E[vk−vk−1] =ηρ2h
(∇f(xk−1))2+ diag(Σ( xk))−vk−1i
; 837
2.E[mk+1−mk] =ηρ1[∇f(xk)−mk]; 838
3.E[xk+1−xk] =−η√vk+ϵId√
1−(1−ηρ2)k
1−(1−ηρ1)k+1(mk+ηρ1(∇f(xk)−mk)). 839
Then, we have 840
321.E[(xk+1−xk)(xk+1−xk)⊤] =E[(xk+1−xk)]E[(xk+1−xk)]⊤+O(η4ρ2
1); 841
2.E[(xk+1−xk)(mk−mk−1)⊤] =E[(xk+1−xk)]E[(mk−mk−1)]⊤+ 0; 842
3.E[(xk+1−xk)(vk−vk−1)⊤] =E[(xk+1−xk)]E[(vk−vk−1)]⊤+ 0; 843
4.E[(vk−vk−1)(vk−vk−1)⊤] =E[(vk−vk−1)]E[(vk−vk−1)]⊤+O(η2ρ2
2); 844
5.E[(mk−mk−1)(mk−mk−1)⊤] =E[(mk−mk−1)]E[(mk−mk−1)]⊤+η2ρ2
1Σ(xk−1); 845
6.E[(vk−vk−1)(mk−mk−1)⊤] =E[(vk−vk−1)]E[(mk−mk−1)]⊤+O(η2ρ1ρ2). 846
Since in real-world applications, ρ1=O(η−ζ)s.t.ζ∈(0,1), while ρ2=O(1), we have 847
dXt=−p
γ2(t)
γ1(t)P−1
t(Mt+ηρ1(∇f(Xt)−Mt))dt (139)
dMt=ρ1(∇f(Xt)−Mt)dt+√ηρ1Σ1/2(Xt)dWt (140)
dVt=ρ2 
(∇f(Xt))2+ diag (Σ ( Xt))−Vt
dt. (141)
where βi= 1−ηρi,γi(t) = 1−e−ρit, and Pt= diag√Vt+ϵp
γ2(t)Id. 848
Corollary C.27. Under the assumptions of Theorem C.26 with Σ(x) =σ2Id,˜η=κη,˜B=Bδ, 849
˜ρ1=α1ρ1, and ˜ρ2=α2ρ2 850
dXt=−κp
γ2(t)
γ1(t)P−1
t(Mt+ηα1ρ1(∇f(Xt)−Mt))dt (142)
dMt=α1ρ1
κ(∇f(Xt)−Mt)dt+√ηα1ρ1
κσ√
BδIddWt (143)
dVt=α2ρ2
κ
(∇f(Xt))2+σ2
BδId−Vt
dt. (144)
Lemma C.28. Under the assumptions of Corollary C.27, fisµ-strongly convex, Lτ:=Tr(∇2f(x)), 851
and(∇f(x))2=O(η), the asymptotic dynamics of the iterates of Adam satisfies the classic scaling 852
ruleκ=√
δbecause E[f(Xt)]t→∞
≤ησLτ
4√
Bκ√
δ. To enforce that the speed of MtandVtmatch that of 853
Xt, one needs ˜ρi=κ2ρi, which implies ˜βi= 1−κ2(1−βi). 854
Proof. First of all, we need to ensure that the relative speeds of Xt,Mt, and Vtmatch. Therefore, 855
we select αi=κ2, which recovers the scaling rules for ˜βi= 1−κ2(1−βi). Then, recalling that 856
(∇f(x))2=O(η), we have that as t→ ∞ ,Vt→σ2
Bδ, and Mt→ ∇f(Xt)with high probability. 857
Therefore, 858
dXt=−κ√
Bδ
σ∇f(Xt)dt (145)
dMt=κ√ηρ1σ√
BδdWt (146)
dVt= 0. (147)
Therefore, if H(Xt, Vt) :=f(Xt) +LτδB
ρ2σ2∥Mt∥2
2
2andξ∈(0,1)we have that by Itô’s lemma, 859
33dH(Xt, Vt) =−(∇f(Xt))⊤ 
κ√
Bδ
σ∇f(Xt)!
dt+LτδB
ρ2σ2Mt
κ√ηρ1σ√
BδdWt (148)
+1
2LτδB
ρ2σ2
κ2ηρ2σ2
Bδdt (149)
=− 
κ√
Bδ
σ!
∥∇f(Xt)∥2
2dt+Noise +κ2ηλ
2dt (150)
=− 
κ√
Bδ
σ!
 
ξ∥∇f(Xt)∥2
2+ (1−ξ)∥∇f(Xt)∥2
2
dt+Noise +κ2ηλ
2dt(151)
≤ −2κµ√
Bδ
σξ
f(Xt) +1−ξ
µξ∥∇f(Xt)∥2
2
2
dt+Noise +κ2ηλ
2dt. (152)
Let us now select ξsuch that1−ξ
µξ=LτδB
ρ2σ2, this means that ξ=σ2ρ2
σ2ρ2+µLτσB∈(0,1)and1
ξ= 860
1 +µLτδB
ρ2σ2. Since Mt→ ∇f(Xt), we have that 861
dH(Xt, Vt)≤ −2κµ√
Bδ
σξH(Xt, Vt)dt+κ2ηλ
2dt+Noise . (153)
Therefore, 862
E[f(Xt)]
ξ=
1 +µLτδB
ρ2σ2
E[f(Xt)]≤E[H(Xt, Vt)]t→∞
≤1
ξησLτ
4µ√
Bκ√
δ, (154)
which implies that 863
E[f(Xt)]t→∞
≤ησLτ
4µ√
Bκ√
δ. (155)
Analogously, 864
E[f(Xt)−f(X∗)]t→∞
≤ησLτ
4µ√
Bκ√
δ. (156)
which gives the square root scaling rule. 865
Lemma C.29. Under the assumptions of Corollary C.27, f(x) =x⊤Hx
2s.t.H= diag( λ1,···, λd) 866
and(∇f(x))2=O(η), the dynamics of Adam implies that f(Xt)→ησd
4√
Bκ√
δ. 867
Proof. Recalling that (∇f(x))2=O(η), we have that as t→ ∞ ,Vt→σ2
Bδ, and Mt→λXtwith 868
high probability. Therefore, in the one-dimensional case 869
dXt=−κ√
Bδ
σλXtdt (157)
dMt=κ√ηρ1σ√
BδdWt (158)
dVt= 0. (159)
Therefore, if H(Xt, Vt) :=λX2
t
2+λδB
ρ2σ2M2
t
2,5we have that by Itô’s lemma, 870
5Inspired by (Barakat and Bianchi, 2021)
34dH(Xt, Vt) =−(λXt) 
κ√
Bδ
σλXt!
dt+λδB
ρ2σ2Mt
κ√ηρ1σ√
BδdWt (160)
+1
2λδB
ρ2σ2
κ2ηρ2σ2
Bδdt (161)
=−2κλ√
Bδ
σf(Xt)dt+κ2ηρ2σ2
2BδλδB
ρ2σ2dt+Noise . (162)
=−2κλ√
Bδ
σf(Xt)dt+κ2ηλ
2dt+Noise . (163)
Once again, since Mt→λXt, we have that 871
H(Xt, Vt) =λX2
t
2+λδB
ρ2σ2M2
t
2→λX2
t
2+λλδB
ρ2σ2λX2
t
2=
1 +λλδB
ρ2σ2λX2
t
2=:Kf(Xt).
(164)
Therefore, 872
KdE[f(Xt)] =−2κλ√
Bδ
σE[f(Xt)]dt+κ2ηλ
2dt, (165)
which implies that E[f(Xt)]→ησ
4√
Bκ√
δ, which also gives the square root scaling rule. The general- 873
ization to ddimension is analogous and one needs to sum across all the dimensions. 874
Lemma C.30. Letf(x) :=x⊤Hx
2where H= diag( λ1, . . . , λ d). The stationary distribution of 875
Adam is (E[X∞]], Cov (X∞)) =
0,η
2Σ1
2H−1
. 876
Proof. The expected value follows immediately from the fact that 877
dXt=−Σ−1
2Xtdt (166)
For the covariance, we focus on the one-dimensional case. We define H(Xt, Vt) :=X2
t
2+λ2
2σ2ρ2M2
t
2. 878
With the same arguments as Lemma C.29, we have 879
d(Xt)2=−λ
σX2
tdt+η
2dt+Noise , (167)
which implies that 880
E[X2
t]t→0→η
2σ
λ. (168)
The thesis follows by applying the same logic to multiple dimensions. 881
C.6 AdamW 882
In this subsection, we derive the SDE of AdamW defined as defined as 883
vk+1=β2vk+ (1−β2) (∇fγk(xk))2(169)
mk+1=β1mk+ (1−β1)∇fγk(xk) (170)
ˆmk=mk 
1−βk
1−1(171)
ˆvk=vk 
1−βk
2−1(172)
xk+1=xk−ηˆmk+1p
ˆvk+1+ϵId−ηγxk (173)
with(x0, m0, v0)∈Rd×Rd×Rd,η∈R>0is the step size, βi= 1−ρiηforρ1=O(η−ζ) 884
s.t.ζ∈(0,1),ρ2=O(1),γ >0, the mini-batches {γk}are modelled as i.i.d. random variables 885
uniformly distributed on {1,···, N}, and of size B≥1. 886
35Theorem C.31. Under the same assumptions as Theorem C.26, the SDE of AdamW is 887
dXt=−p
γ2(t)
γ1(t)P−1
t(Mt+ηρ1(∇f(Xt)−Mt))dt−γXtdt (174)
dMt=ρ1(∇f(Xt)−Mt)dt+√ηρ1Σ1/2(Xt)dWt (175)
dVt=ρ2 
(∇f(Xt))2+ diag (Σ ( Xt))−Vt
dt. (176)
where βi= 1−ηρi,γ >0,γi(t) = 1−e−ρit, and Pt= diag√Vt+ϵp
γ2(t)Id. 888
Proof. The proof is the same as the of Theorem C.26 and the only difference is that ηγxkis 889
approximated with γXtdt. 890
Figure 4 and Figure 9 validate this result on a variety of architectures and datasets. 891
Corollary C.32. Under the assumptions of Theorem C.31 with Σ(x) =σ2Id,˜η=κη,˜B=Bδ, 892
˜ρ1=α1ρ1,˜γ:ξγ, and ˜ρ2=α2ρ2 893
dXt=−κp
γ2(t)
γ1(t)P−1
t(Mt+ηα1ρ1(∇f(Xt)−Mt))dt−κξγX tdt (177)
dMt=α1ρ1
κ(∇f(Xt)−Mt)dt+√ηα1ρ1
κσ√
BδIddWt (178)
dVt=α2ρ2
κ
(∇f(Xt))2+σ2
BδId−Vt
dt. (179)
Lemma C.33 (Scaling Rule at Convergence) .Under the assumptions of Corollary C.32, fisµ- 894
strongly convex and L-smooth, Lτ:=Tr(∇2f(x)), and (∇f(x))2=O(η), the asymptotic dynamics 895
of the iterates of AdamW satisfies the novel scaling rule if κ=√
δandξ=κbecause 896
E[f(Xt)−f(X∗)]t→∞
≤ηLτσL
2κ
2µ√
BδL +σξγ(L+µ)(180)
By enforcing that the speed of Vtmatches that of Xt, one needs ˜ρ=κ2ρ, which implies ˜βi= 897
1−κ2(1−βi). 898
Proof. The proof is the same as Lemma C.28 where we also use L-smoothness as in Lemma C.23. 899
Lemma C.34. Forf(x) :=x⊤Hx
2, the stationary distribution of AdamW is (E[X∞]], Cov (X∞)) = 900
0,η
2(HΣ−1
2+γId)−1
. 901
Proof. The proof is the same as Lemma C.30. 902
D SDEs from the literature 903
Theorem D.1 (Original Malladi’s Statement) .Letσ0:=ση, ϵ 0:=ϵη, and c2:=1−β
η2. Define the 904
state of the SDE as Lt= (Xt, ut)and the dynamics as 905
dXt=−P−1
t
∇f(Xt)dt+σ0Σ1/2(Xt)dWt
(181)
dut=c2(diag (Σ ( Xt))−ut)dt (182)
where Pt:=σ0diag ( ut)1/2+ϵ0Id. 906
Theorem D.2 (Informal Statement of Theorem C.2 Malladi et al. (2022)) .Under sufficient regularity 907
conditions and ∇f(x) =O(√η), the following SDE is an order 1weak approximation of RMSprop: 908
dXt=−P−1
t(∇f(Xt)dt+√ηΣ(Xt)1
2dWt) (183)
dVt=ρ(diag(Σ( Xt))−Vt))dt, (184)
where β= 1−ηρ,ρ=O(1), and Pt:= diag ( Vt)1
2+ϵId. 909
36Lemma D.3. Theorem D.1 and Theorem D.2 are equivalent. 910
Proof. It follows applying time rescaling t:=ηξand observing that Wt=Wηξ=√ηWξ. 911
Theorem D.4 (Original Malladi’s Statement) .Letc1:= (1−β1)/η2, c2:= (1−β2)/η2and define 912
σ0, ϵ0in Theorem D.1. Let γ1(t) := 1 −exp (−c1t)andγ2(t) := 1 −exp (−c2t). Define the state 913
of the SDE as Lt= (Xt, mt, ut)and the dynamics as 914
dXt=−p
γ2(t)
γ1(t)P−1
tmtdt (185)
dmt=c1(∇f(Xt)−mt)dt+σ0c1Σ1/2(Xt)dWt, (186)
dut=c2(diag (Σ ( Xt))−ut)dt, (187)
where Pt:=σ0diag ( ut)1/2+ϵ0p
γ2(t)Id. 915
Theorem D.5 (Informal Statement of Theorem D.2 Malladi et al. (2022)) .Under sufficient regularity 916
conditions and ∇f(x) =O(√η), the following SDE is an order 1weak approximation of Adam: 917
dXt=−p
γ2(t)
γ1(t)P−1
tMtdt (188)
dMt=ρ1(∇f(Xt)−Mt)dt+√ηρ1Σ1/2(Xt)dWt (189)
dVt=ρ2(diag (Σ ( Xt))−Vt)dt. (190)
where βi= 1−ηρi,γi(t) = 1−e−ρit,ρi=O(1), and Pt= diag√Vt+ϵp
γ2(t)Id. 918
Lemma D.6. Theorem D.4 and Theorem D.5 are equivalent. 919
Proof. It follows applying time rescaling t:=ηξand observing that Wt=Wηξ=√ηWξ. 920
E SDE cannot be derived nor used naively 921
In this section, we provide a gentle introduction to the meaning of deriving an SDE model for an 922
optimizer and discuss how SDEs have been used to derive scaling rules. To aid the intuition of the 923
reader, we informally derive an SDE for SGD with learning rate η, mini-batches γBof size B, and 924
starting point x0=x, which we dub SGD(η,B). The iterates are given by: 925
xk+1=xk−η∇fγB
k(xk) (191)
which for Uk:=√η(∇f(xk)− ∇fγB
k(xk)),we rewrite as 926
xk−η∇f(xk) +√ηUk, (192)
whereE[Uk] = 0 andCov(Uk) =η
BΣ(xk) =η
B1
nPB
i=0(∇f(xk)−∇fi(xk))(∇f(x)−∇fi(xk))⊤. 927
If we now consider the SDE 928
dXt=−∇f(Xt)dt+rη
BΣ(Xt)1
2dWt, (193)
its Euler-Maruyama discretization with pace ∆t=ηandZk∼ N(0, Id)is 929
Xk+1=Xk−η∇f(Xk) +√ηrη
BΣ(Xt)1
2Zk. (194)
Since the Eq. (191) and Eq. (194) share the first two moments, it is reasonable that by identifying 930
t=kη, the SDE in Eq. (193) is a good model to describe the iterates of SGD in Eq. (191). 931
Informally, we need a “good model”, which is an SDE that is close to the real optimizer. This is 932
formalized in the following definition which comes from the field of numerical analysis of SDEs (see 933
Mil’shtein (1986)) and bounds the disparity between the the discrete and the continuous process. 934
Definition E.1 (Weak Approximation) .A continuous-time stochastic process {Xt}t∈[0,T]is an order 935
αweak approximation (or α-order SDE) of a discrete stochastic process {xk}⌊T/η⌋
k=0 if for every 936
polynomial growth function g, there exists a positive constant C, independent of the stepsize η, such 937
thatmax k=0,...,⌊T/η⌋|Eg(xk)−Eg(Xkη)| ≤Cηα. 938
37To see if an SDE satisfies such a definition, one has to check that for ¯∆ =x1−xand∆ =Xη−x, 939
1.E∆i−E¯∆i=O(η2),∀i= 1, . . . , d ; 940
2.E∆i∆j−E¯∆i¯∆j=O(η2),∀i, j= 1, . . . , d . 941
Example: Let us prove that the SDE in Eq. (193) is a valid approximation of SGD(η,B): The first 942
condition is easily verified. Coming to the second condition we have that 943
1.E∆i∆j=η2∂if(x)∂jf(x) +η2
BΣ(x); 944
2.E¯∆i¯∆j=η2∂if(x)∂jf(x) +η2
BΣ(x) +O(η3); 945
whose difference is of order η3and thus satisfies the condition. However, we observe that if the 946
scale of the noise is too small w.r.t η, i.e.Σ(x) =O(ηα)forα≥0, then the simplest SDE model 947
describing SGD(η,B)is the ODE dXt=−∇f(Xt)dtas in that case 948
1.E∆i∆j=η2∂if(x)∂jf(x) +O(η2+α); 949
2.E¯∆i¯∆j=η2∂if(x)∂jf(x) +O(η2), 950
whose difference is also of order η2. Much differently, if Σ(x) =O(η−α)forα >0, the simplest 951
model is the SDE in Eq. (193) . We highlight that simplest does not mean best: The SDE is more 952
accurate than the ODE even in a regime with low noise, but this observation serves as a provocation. 953
One has to pay attention when deriving SDEs: Some models are more realistic than others. 954
Let us dig deeper into this thought as we derive twoSDEs for SGD with learning rate ˜η:=κηand 955
batch size ˜B:=δBforκ >1andδ >1, which we dub SGD(˜η,˜B). The first is derived considering 956
that the learning rate is ˜ηand carries an error of order O(˜η)w.r.t. SGD(˜η,˜B)957
dXt=−∇f(Xt)dt+r
˜η
˜BΣ(Xt)1
2dWt=−∇f(Xt)dt+rηκ
BδΣ(Xt)1
2dWt. (195)
The second one instead is derived considering ηas the learning rate and κas a constant “scheduler”. 958
Consistently with (Li et al., 2017), the SDE which carries an error of order O(η)w.r.t SGD(˜η,˜B)is 959
dXt=−κ∇f(Xt)dt+κrη
BδΣ(Xt)1
2dWt. (196)
While they both are valid models, there are three reasons why one should prefer the latter: 960
1. It fully reflects the fact that a larger learning rate results in a faster and noisier dynamics 961
2. It has intrinsically less error than the other; 962
3.It is consistent with the optimizer in that there is no combination of κandδthat can ever 963
leave the dynamics unchanged. 964
E.1 Deriving scaling rules 965
Jastrzebski et al. (2018) observed that only the ratio between ηandBmatters in determining the 966
dynamics of Eq. (194) . Therefore, they argue that for κ=δthe SDE for SGD(κη,δB )coincides with 967
that of SGD(η,B)and that this implies that the path properties of the optimizers are the same. On the 968
contrary, the path of SGD(η,B)strongly depends on the hyperparameters: The speed and volatility of 969
the dynamics are driven by η, and no choice of Bcan undo this. We remind the reader that the goal of 970
these rules is not to keep the dynamics of the optimizers unaltered, but rather to give a practical way 971
to change a hyperparameter, e.g. η, and have a principled way to adjust the others, e.g. B, such that 972
the performance of the optimizer is preserved. Therefore, we propose deriving scaling rules as we 973
preserve certain relevant quantities of the dynamics such as the convergence bound on the expected 974
loss or the speed. To show this quantitative, we use this rationale to derive the scaling rule of SGD as 975
we aim at preserving the asymptotic loss level. 976
Lemma E.2. Iffis aµstrongly convex function, Lτ≤Tr(∇2f(x))andΣ(x) =σ2Id, then: 977
381. Under the dynamics of Eq. (193) we have: 978
E[f(Xt)−f(X∗)]≤(f(X0)−f(X∗))e−2µt+η
2Lτσ2
2µB 
1−e−2µt
; (197)
2. Under the dynamics of Eq. (195) we have: 979
E[f(Xt)−f(X∗)]≤(f(X0)−f(X∗))e−2µt+η
2Lτσ2
2µBκ
δ 
1−e−2µt
; (198)
3. Under the dynamics of Eq. (196) we have: 980
E[f(Xt)−f(X∗)]≤(f(X0)−f(X∗))e−2µκt+η
2Lτσ2
2µBκ
δ 
1−e−2µκt
. (199)
The first bound implies that the asymptotic limit of the expected loss for SGD(η,B)isη
2Lτσ2
2µB. The 981
last two bounds predict that the asymptotic loss level for SGD(˜η,˜B)isη
2Lτσ2
2µBκ
δ. Since the objective 982
of the scaling rule is to find κandδsuch that SGD(˜η,˜B)achieves the same loss level as SGD(η,B), 983
we recover the linear scaling rule setting κ=δ. However, only the last bound can correctly capture 984
the fact that the dynamics of SGD(˜η,˜B)isκtimes faster than that of SGD(η,B). 985
We conclude the discussion with a simple sample of how deriving a scaling rule from the SDE itself 986
inevitably leads to the wrong conclusion. We define the following algorithm which is inspired by 987
AdamW and which we dub SGDW: 988
xk+1=xk−η∇fγk(xk)−ηγxk. (200)
Lemma E.3. The SDE of SGDW is 989
dXt=−∇f(Xt)dt+rη
BΣ(Xt)1
2dWt−γXtdt. (201)
Therefore, one would naively deduce that to keep the SDE unchanged, one can simply use the linear 990
scaling rule of SGD and leave γunaltered. However, one can easily derive the upper bound on the 991
expected loss for a convex quadratic function and observe that to preserve that, it is imperative to 992
scale γbyκas well. 993
We thus conclude that: 994
1. Eq. (196) is a better model for SGD(˜η,˜B)as it represents the dynamics more accurately; 995
2. Maintaining the shape of the SDE does not preserve the path properties of the optimizer; 996
3.Deriving a scaling rule uniquely from the SDE might lead to the wrong conclusions in the 997
general case. 998
Remark E.4.We highlight that Theorem 5.3 of Malladi et al. (2022) claimed to have formally derived 999
one for RMSprop: In line with (Jastrzebski et al., 2018), they argue that if they were to find a scaling 1000
rule that would leave their SDE unchanged, this would imply that even the dynamics of the iterates of 1001
RMSprop itself would be unchanged. First, we remind the reader that an SDE is formally defined 1002
as an equation that drives the dynamics plus aninitial condition (See (Karatzas and Shreve, 2014), 1003
Section 5). While their scaling rule does leave the equation unchanged , italters the initial condition , 1004
thus changing the SDE itself: This invalidates their claim and proof. Second, contrary to their claim, 1005
the rule is only valid near convergence as their SDE is only valid there. Third, Lemma E.2 offers a 1006
shred of concrete evidence that keeping the SDE unchanged does not imply that the path properties 1007
of the optimizers are preserved. Fourth, Lemma E.3 is a piece of concrete evidence that deriving 1008
scaling rules directly and naively from the SDE might lead to the wrong conclusions. 1009
F Experiments 1010
In this section, we provide the modeling choices and instructions to replicate our experiments. All 1011
experiments we run on one NVIDIA GeForce RTX 3090 GPU. The code is implemented in Python 3 1012
(Van Rossum and Drake, 2009) mainly using Numpy (Harris et al., 2020), scikit-learn (Pedregosa 1013
et al., 2011), and JAX (Bradbury et al., 2018). 1014
39F.1 SignSGD: SDE validation (Figure 1) 1015
In this subsection, we describe the experiments we run to produce Figure 1: The loss dynamics of 1016
SignSGD and that of our SDE match on average. 1017
DNN on Breast Cancer Dataset (Dua and Graff, 2017) This paragraph refers to the leftof Figure 1018
1. The DNN has 10dense layers with 20neurons each activated with a ReLu. We minimize the binary 1019
cross-entropy loss. We run SignSGD for 50000 epochs as we calculate the full gradient and inject it 1020
with Gaussian noise Z∼ N(0, σ2Id)where σ= 1. The learning rate is η= 0.001. Similarly, we 1021
integrate the SignSGD SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are 1022
averaged over 3runs and the shaded areas are the average ±the standard deviation. 1023
CNN on MNIST (Deng, 2012) This paragraph refers to the center-left of Figure 1. The CNN 1024
has a (3,3,32)convolutional layer with stride 1, followed by a ReLu activation, a (2,2)max pool 1025
layer with stride (2,2), a(3,3,32)convolutional layer with stride 1, a ReLu activation, a (2,2)max 1026
pool layer with stride (2,2). Then the activations are flattened and passed through a dense layer that 1027
compresses them into 128dimensions, a final ReLu activation, and a final dense layer into the output 1028
dimension 10. The output finally goes through a softmax as we minimize the cross-entropy loss. We 1029
run SignSGD for 40000 epochs as we calculate the full gradient and inject it with Gaussian noise 1030
Z∼ N(0, σ2Id)where σ= 1. The learning rate is η= 0.001. Similarly, we integrate the SignSGD 1031
SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged over 3run 1032
and the shaded areas are the average ±the standard deviation. 1033
Transformer on MNIST This paragraph refers to the center-right of Figure 1. The Architecture is 1034
a scaled-down version of (Dosovitskiy et al., 2021), where the hyperparameters are patch size =28, 1035
out features =10,width =48,depth =3,num heads =6, and dim ffn =192. We minimize the cross-entropy 1036
loss as we run SignSGD for 5000 epochs as we calculate the full gradient and inject it with Gaussian 1037
noise Z∼ N(0, σ2Id)where σ= 1. The learning rate is η= 0.001. Similarly, we integrate the 1038
SignSGD SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged 1039
over3runs and the shaded areas are the average ±the standard deviation. 1040
ResNet on CIFAR-10 (Krizhevsky et al., 2009) This paragraph refers to the right of Figure 1. 1041
The ResNet has a (3,3,128) convolutional layer with stride 1, followed by a ReLu activation, a 1042
second (3,3,64)convolutional layer with stride 1, followed by a residual connection from the first 1043
convolutional layer, then a (2,2)max pool layer with stride (2,2). Then the activations are flattened 1044
and passed through a dense layer that compresses them into 128dimensions, a final ReLu activation, 1045
and a final dense layer into the output dimension 10. The output finally goes through a softmax as we 1046
minimize the cross-entropy loss. We run SignSGD for 5000 epochs as we calculate the full gradient 1047
and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 1. The learning rate is η= 0.001. 1048
Similarly, we integrate the SignSGD SDE (Eq. (7)) with Euler-Maruyama (Algorithm 1) with ∆t=η. 1049
Results are averaged over 3runs and the shaded areas are the average ±the standard deviation. 1050
F.2 SignSGD: insights validation (Figure 2) 1051
In this subsection, we describe the experiments we run to produce Figure 2: We successfully validate 1052
them all. 1053
Phases: Lemma 3.4 and Lemma 3.5 In this paragraph, we describe how we validated the existence 1054
of the phases of SignSGD as predicted in Lemma 3.4 and Lemma 3.5. To produce the leftof Figure 1055
2), we simulated the full SDE (Eq. (16)) and the one describing Phase 3 (Eq. (5)). The optimized 1056
function is f(x) =x⊤Hx
2forH= diag(1 ,2),x0drawn (and fixed for all runs) from a normal 1057
distribution N(0,0.01),η= 0.001, and Σ = σ2Idwhere σ= 0.1. We integrate the SDEs with 1058
Euler-Maruyama (Algorithm 1) with ∆t=ηand for 3000 iterations. Results are averaged over 500 1059
runs and the shaded areas are the average ±the standard deviation. Clearly, the two SDEs share the 1060
same dynamics. 1061
To produce the center-left of Figure 2, we repeat the above as x0drawn (and fixed for all runs) from 1062
a normal distribution N(0,1). Then, we plot the average loss values together with the theoretical 1063
prediction of Phase 1 and Phase 3: They perfectly overlap. 1064
40Stationary distribution: Lemma 3.7 In this paragraph, we describe how we validated the conver- 1065
gence behavior predicted in Lemma 3.7. To produce the center-right of Figure 2), we run SignSGD on 1066
f(x) =x⊤Hx
2forH= diag(1 ,2),x0= (0.001,0.001),η= 0.001andΣ =σ2Idwhere σ= 0.1. 1067
We run this for 5000 times and report the evolution of the moments. Then, we add lines representing 1068
the theoretical predictions derived in Lemma 3.7: They match. 1069
Schedulers: Lemma 3.9 In this paragraph, we describe how we validated the convergence behavior 1070
predicted in Lemma 3.9. To produce the right of Figure 2, we run SignSGD on f(x) =x⊤Hx
2for 1071
H= diag(1 ,2),x0= (0.01,0.01),η= 0.01andΣ =σ2Idwhere σ= 0.1. We used the scheduler 1072
ηγ
t=1
(t+1)γforγ∈ {0.1,0.5,1.5}. For the first two choices of γ,ηγ
tsatisfies our sufficient 1073
condition for the convergence of SignSGD: In the figure, we observe that indeed SignSGD converges 1074
to 0 with the same speed as the one predicted in the Lemma. For γ= 1.5, we observe that SignSGD 1075
does not converge following the theoretical curve because it does not satisfy our sufficient condition. 1076
Results are averaged over 500runs. 1077
F.3 RMSprop: SDE validation (Figure 7 and Figure 8) 1078
In this subsection, we describe the experiments we run to produce Figure 7 and Figure 8: The 1079
dynamics of our SDE matches that of RMSprop better than the SDE derived in (Malladi et al., 2022). 1080
Quadratic convex function This paragraph refers to the leftandcenter-left of Figure 7. We 1081
optimize the function f(x) =x⊤Hx
2where H= diag(10 ,2). We run RMSprop for 2000 epochs as 1082
we calculate the full gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 0.1. The 1083
learning rate is η= 0.01,β= 0.99. Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of 1084
Malladi (Eq. (183) ) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged over 1085
500runs and the shaded areas are the average ±the standard deviation: Our SDE matches RMSprop 1086
much better. 1087
Embedded saddle This paragraph refers to the center-right andright of Figure 7. We optimize the 1088
function f(x) =x⊤Hx
2+1
4λP2
i=1x4
i−ξ
3P2
i=1x3
iwhere H= diag( −1,2),λ= 1, and ξ= 0.1. 1089
We run RMSprop for 1600 epochs as we calculate the full gradient and inject it with Gaussian noise 1090
Z∼ N(0, σ2Id)where σ= 0.01. The learning rate is η= 0.01,β= 0.99. Similarly, we integrate 1091
our RMSprop SDE (Eq. (86)) and that of Malladi (Eq. (183) ) with Euler-Maruyama (Algorithm 1) 1092
with∆t=η. Results are averaged over 500runs and the shaded areas are the average ±the standard 1093
deviation: Our SDE matches RMSprop much better. 1094
DNN on Breast Cancer Dataset This paragraph refers to the leftof Figure 8. The architecture and 1095
loss are the same as used above for SignSGD. We run RMSprop for 2000 epochs as we calculate the 1096
full gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 10−2. The learning rate 1097
isη= 10−4,β= 0.9995 . Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of Malladi 1098
(Eq. (183) ) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged over 3runs and 1099
the shaded areas are the average ±the standard deviation: Our SDE matches RMSprop much better. 1100
CNN on MNIST This paragraph refers to the center-left of Figure 8. The architecture and loss 1101
are the same as used above for SignSGD. We run RMSprop for 2000 epochs as we calculate the full 1102
gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 10−2. The learning rate is 1103
η= 10−3,β= 0.995. Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of Malladi (Eq. 1104
(183) ) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged over 3run and the 1105
shaded areas are the average ±the standard deviation: Our SDE matches RMSprop much better. 1106
Transformer on MNIST This paragraph refers to the center-right of Figure 8. The architecture 1107
and loss are the same as used above for SignSGD. We run RMSprop for 2000 epochs as we calculate 1108
the full gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 10−2. The learning 1109
rate is η= 10−3,β= 0.995. Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of 1110
Malladi (Eq. (183) ) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged over 1111
3runs and the shaded areas are the average ±the standard deviation: Our SDE matches RMSprop 1112
much better. 1113
41ResNet on CIFAR-10 This paragraph refers to the right of Figure 8. The architecture and loss 1114
are the same as used above for SignSGD. We run RMSprop for 500epochs as we calculate the full 1115
gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 10−4. The learning rate is 1116
η= 10−4,β= 0.9999 . Similarly, we integrate our RMSprop SDE (Eq. (86)) and that of Malladi 1117
(Eq. (183) ) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged over 3runs and 1118
the shaded areas are the average ±the standard deviation: Our SDE matches RMSprop much better. 1119
F.4 Adam: SDE validation (Figure 10 and Figure 11) 1120
In this subsection, we describe the experiments we run to produce Figure 11 and Figure 10: The 1121
dynamics of our SDE matches that of Adam better than that derived in (Malladi et al., 2022). 1122
Quadratic convex function This paragraph refers to the leftandcenter-left of Figure 10. We 1123
optimize the function f(x) =x⊤Hx
2where H= diag(10 ,2). We run Adam for 50000 epochs as we 1124
calculate the full gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 0.01. The 1125
learning rate is η= 0.001,β1= 0.9, and β2= 0.999. Similarly, we integrate our Adam SDE (Eq. 1126
(124) ) and that of Malladi (Eq. (188) ) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results 1127
are averaged over 500runs and the shaded areas are the average ±the standard deviation: Our SDE 1128
matches Adam much better. 1129
Embedded saddle This paragraph refers to the center-right andright of Figure 10. We optimize the 1130
function f(x) =x⊤Hx
2+1
4λP2
i=1x4
i−ξ
3P2
i=1x3
iwhere H= diag( −1,2),λ= 1, and ξ= 0.1. 1131
We run Adam as we calculate the full gradient and inject it with Gaussian noise Z∼ N(0, σ2Id) 1132
where σ= 0.1. The learning rate is η= 0.001,β1= 0.9, and β2= 0.999. Similarly, we integrate 1133
our Adam SDE (Eq. (124) ) and that of Malladi (Eq. (188) ) with Euler-Maruyama (Algorithm 1) with 1134
∆t=η. Results are averaged over 500runs and the shaded areas are the average ±the standard 1135
deviation: Our SDE matches Adam much better. 1136
DNN on Breast Cancer Dataset This paragraph refers to the leftof Figure 11. The architecture 1137
and loss are the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the 1138
full gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 10−2. The learning rate 1139
isη= 10−4,β1= 0.99, and β2= 0.999. Similarly, we integrate our Adam SDE (Eq. (124) ) and 1140
that of Malladi (Eq. (188)) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged 1141
over3runs and the shaded areas are the average ±the standard deviation: Our SDE matches Adam 1142
much better. 1143
CNN on MNIST This paragraph refers to the center-left of Figure 11. The architecture and loss are 1144
the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the full gradient 1145
and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 10−2. The learning rate is η= 10−2, 1146
β1= 0.9, andβ2= 0.99. Similarly, we integrate our Adam SDE (Eq. (124) ) and that of Malladi (Eq. 1147
(188) ) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged over 3runs and the 1148
shaded areas are the average ±the standard deviation: Our SDE matches Adam much better. 1149
Transformer on MNIST This paragraph refers to the center-right of Figure 11. The architecture 1150
and loss are the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the 1151
full gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 10−2. The learning rate 1152
isη= 10−2,β1= 0.9, and β2= 0.99. Similarly, we integrate our Adam SDE (Eq. (124) ) and that 1153
of Malladi (Eq. (188) ) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged over 1154
3runs and the shaded areas are the average ±the standard deviation: Our SDE matches Adam much 1155
better. 1156
ResNet on CIFAR-10 This paragraph refers to the right of Figure 11. The architecture and loss are 1157
the same as used above for SignSGD. We run Adam for 2000 epochs as we calculate the full gradient 1158
and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 10−5. The learning rate is η= 10−5, 1159
β1= 0.99, andβ2= 0.9999 . Similarly, we integrate our Adam SDE (Eq. (124) ) and that of Malladi 1160
(Eq. (188) ) with Euler-Maruyama (Algorithm 1) with ∆t=η. Results are averaged over 3runs and 1161
the shaded areas are the average ±the standard deviation: Our SDE matches Adam much better. 1162
42F.5 RMSpropW & AdamW: SDE validation (Figure 3, Figure 4) 1163
The settings are exactly the same as those for RMSprop and Adam. The regularization parameter 1164
used is always γ= 0.01. We observe that our SDEs match the respective algorithm with a good 1165
agreement. 1166
F.6 RMSpropW & AdamW: insights validation (Figure 5) 1167
In this subsection, we describe the experiments we run to produce Figure 5: The theoretically 1168
predicted asymptotic loss value and moments of RMSpropW and AdamW match those empirically 1169
found. 1170
Asymptotic loss & scaling rule of AdamW This paragraph refers to the leftof Figure 5. We 1171
optimize the function f(x) =x⊤Hx
2where H= diag(1 ,3). We run AdamW for 20000 epochs as 1172
we calculate the full gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 1. The 1173
learning rate is η= 0.001,β1= 0.9, and β2= 0.999. Experiments are run for both γ= 1 and 1174
γ= 4. The rescaled versions of the algorithms AdamW R follow the novel scaling rule with κ= 2. 1175
AdamW NR follows the scaling rule but not for γwhich is left unchanged. We plot the evolution of 1176
the loss values with the theoretical predictions of Lemma C.28: Results are averaged over 500runs. 1177
Asymptotic loss & scaling rule of RMSpropW This paragraph refers to the center-left of Figure 1178
5: The only difference with the previous paragraph is that we use RMSpropW with β= 0.999. 1179
AdamW: the role of the βsThis paragraph refers to the center-right of Figure 5. We optimize 1180
the function f(x) =x⊤Hx
2+1
4λP2
i=1x4
i−ξ
3P2
i=1x3
iwhere H= diag( −1,2),λ= 1, and 1181
ξ= 0.1. We run AdamW as we calculate the full gradient and inject it with Gaussian noise 1182
Z∼ N(0, σ2Id)where σ= 0.1. The learning rate is η= 0.001,γ= 0.1,β1∈ {0.99,0.999}, 1183
andβ2∈ {0.992,0.996,0.998}: Clearly, three combinations go into a minimum and three go into 1184
the other. For each minimum, the three optimizers converge to the same asymptotic loss value 1185
independently on the values of β1andβ2. We argue that β1, andβ2select the basin and the speed of 1186
convergence, not the asymptotic loss value: This is consistent with Lemma 3.13. 1187
Stationary distribution This paragraph refers to the right of Figure 5. We optimize the function 1188
f(x) =x⊤Hx
2where H= diag(1 ,3). We run Adam for 20000 epochs as we calculate the full 1189
gradient and inject it with Gaussian noise Z∼ N(0, σ2Id)where σ= 0.01. The learning rate is 1190
η= 0.001,γ= 4,β= 0.999,β1= 0.9, and β2= 0.999. We plot the evolution of the average 1191
variances with the theoretical predictions of Lemma C.24 and Lemma 3.14: Results are averaged 1192
over100runs. 1193
F.7 Effect of noise - validation (Figure 6) 1194
In this subsection, we describe the experiments run to produce Figure 6: All bounds on the asymptotic 1195
expected loss value for SGD, SignSGD, Adam, and AdamW are perfectly verified. 1196
We optimize the loss f(x) =x⊤Hx
2where H= diag(1 ,1)as we run each optimizer for 100000 1197
iterations with η= 0.01. We repeat this procedure five times, one for each σ∈ {0.01,0.1,1,10,100}. 1198
As we train, we inject noise on the gradient as distributed as N(0, σ2Id). We plot the average loss 1199
together with the respective limits predicted by our Lemmas. For each optimizer and each σ, the 1200
average asymptotic loss matches the predicted limit. Therefore, we verify that the loss of SGD scales 1201
quadratically in σ, that of Adam and SignSGD scales linearly, and that of AdamW is limited in σ. 1202
F.8 Increasing weight decay with the batch size 1203
The analysis of Malladi et al. (2022) suggests that, when scaling batch size Bby a factor κone has 1204
to scale up (↑)the learning rate ηby a factor√κand scale down (↓)β2to the value 1−κ(1−β2). 1205
Our SDE analysis confirms similar rules (Lemma 3.13) but additionally suggests scaling up the 1206
decoupled weight decay parameter γby a factor√κ. We test this in two settings: VGG11 and 1207
ResNet34 (convolutional networks) on CIFAR-10 classification. We select a base batch size of 256, 1208
43and run AdamW with η= 0.001,β2= 0.99, andγ= 0.1. We consider scaling the batch by a factor 1209
4: In Table 1, we show the effect of updating each hyperparameter with the proposed rule and we 1210
denote by a “ ·” the model parameters of the base run with B= 256 . We train for 150epochs the 1211
model with B= 256 , and 150×4the model with B= 4×256. Experiments are repeated 3times. 1212
We find that, while improvements are marginal, they are consistent with our theoretical results. 1213
B ηβ2λ VGG11 (Test Acc ↑) ResNet 34 (Test Acc ↑)
···· 90.581±0.295 94.396±0.126
↑··· 90.502±0.093 94.296±0.220
↑↑·· 90.767±0.119 94.507±0.148
↑↑↓· 90.703±0.271 94.590±0.188
↑↑↓↑ 90.966±0.252 94.639±0.192
Table 1: Scaling with the batch size: Effect of adapting AdamW hyperparameters.
1214
44NeurIPS Paper Checklist 1215
1.Claims 1216
Question: Do the main claims made in the abstract and introduction accurately reflect the 1217
paper’s contributions and scope? 1218
Answer: [Yes] 1219
Justification: The abstract is a high-level description of what we achieve. The results are 1220
clearly presented in Section 3 and validated in the figures. Details are in the appendix. 1221
Guidelines: 1222
•The answer NA means that the abstract and introduction do not include the claims 1223
made in the paper. 1224
•The abstract and/or introduction should clearly state the claims made, including the 1225
contributions made in the paper and important assumptions and limitations. A No or 1226
NA answer to this question will not be perceived well by the reviewers. 1227
•The claims made should match theoretical and experimental results, and reflect how 1228
much the results can be expected to generalize to other settings. 1229
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 1230
are not attained by the paper. 1231
2.Limitations 1232
Question: Does the paper discuss the limitations of the work performed by the authors? 1233
Answer: [Yes] 1234
Justification: See Section C.1. 1235
Guidelines: 1236
•The answer NA means that the paper has no limitation while the answer No means that 1237
the paper has limitations, but those are not discussed in the paper. 1238
• The authors are encouraged to create a separate ”Limitations” section in their paper. 1239
•The paper should point out any strong assumptions and how robust the results are to 1240
violations of these assumptions (e.g., independence assumptions, noiseless settings, 1241
model well-specification, asymptotic approximations only holding locally). The authors 1242
should reflect on how these assumptions might be violated in practice and what the 1243
implications would be. 1244
•The authors should reflect on the scope of the claims made, e.g., if the approach was 1245
only tested on a few datasets or with a few runs. In general, empirical results often 1246
depend on implicit assumptions, which should be articulated. 1247
•The authors should reflect on the factors that influence the performance of the approach. 1248
For example, a facial recognition algorithm may perform poorly when image resolution 1249
is low or images are taken in low lighting. Or a speech-to-text system might not be 1250
used reliably to provide closed captions for online lectures because it fails to handle 1251
technical jargon. 1252
•The authors should discuss the computational efficiency of the proposed algorithms 1253
and how they scale with dataset size. 1254
•If applicable, the authors should discuss possible limitations of their approach to 1255
address problems of privacy and fairness. 1256
•While the authors might fear that complete honesty about limitations might be used by 1257
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 1258
limitations that aren’t acknowledged in the paper. The authors should use their best 1259
judgment and recognize that individual actions in favor of transparency play an impor- 1260
tant role in developing norms that preserve the integrity of the community. Reviewers 1261
will be specifically instructed to not penalize honesty concerning limitations. 1262
3.Theory Assumptions and Proofs 1263
Question: For each theoretical result, does the paper provide the full set of assumptions and 1264
a complete (and correct) proof? 1265
Answer: [Yes] 1266
45Justification: In the main paper, Theorems, Lemmas, and Corollaries state the assumptions 1267
and theses. Sometimes, these are simplified for the sake of clarity: Complete and formal 1268
statements including proofs are in the Appendices. 1269
Guidelines: 1270
• The answer NA means that the paper does not include theoretical results. 1271
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 1272
referenced. 1273
•All assumptions should be clearly stated or referenced in the statement of any theorems. 1274
•The proofs can either appear in the main paper or the supplemental material, but if 1275
they appear in the supplemental material, the authors are encouraged to provide a short 1276
proof sketch to provide intuition. 1277
•Inversely, any informal proof provided in the core of the paper should be complemented 1278
by formal proofs provided in appendix or supplemental material. 1279
• Theorems and Lemmas that the proof relies upon should be properly referenced. 1280
4.Experimental Result Reproducibility 1281
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 1282
perimental results of the paper to the extent that it affects the main claims and/or conclusions 1283
of the paper (regardless of whether the code and data are provided or not)? 1284
Answer: [Yes] 1285
Justification: We provide all the hyperparameters necessary to replicate our experiments. 1286
Datasets are all publicly available: Breast Cancer, MNIST, and CIFAR-10. 1287
Guidelines: 1288
• The answer NA means that the paper does not include experiments. 1289
•If the paper includes experiments, a No answer to this question will not be perceived 1290
well by the reviewers: Making the paper reproducible is important, regardless of 1291
whether the code and data are provided or not. 1292
•If the contribution is a dataset and/or model, the authors should describe the steps taken 1293
to make their results reproducible or verifiable. 1294
•Depending on the contribution, reproducibility can be accomplished in various ways. 1295
For example, if the contribution is a novel architecture, describing the architecture fully 1296
might suffice, or if the contribution is a specific model and empirical evaluation, it may 1297
be necessary to either make it possible for others to replicate the model with the same 1298
dataset, or provide access to the model. In general. releasing code and data is often 1299
one good way to accomplish this, but reproducibility can also be provided via detailed 1300
instructions for how to replicate the results, access to a hosted model (e.g., in the case 1301
of a large language model), releasing of a model checkpoint, or other means that are 1302
appropriate to the research performed. 1303
•While NeurIPS does not require releasing code, the conference does require all submis- 1304
sions to provide some reasonable avenue for reproducibility, which may depend on the 1305
nature of the contribution. For example 1306
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 1307
to reproduce that algorithm. 1308
(b)If the contribution is primarily a new model architecture, the paper should describe 1309
the architecture clearly and fully. 1310
(c)If the contribution is a new model (e.g., a large language model), then there should 1311
either be a way to access this model for reproducing the results or a way to reproduce 1312
the model (e.g., with an open-source dataset or instructions for how to construct 1313
the dataset). 1314
(d)We recognize that reproducibility may be tricky in some cases, in which case 1315
authors are welcome to describe the particular way they provide for reproducibility. 1316
In the case of closed-source models, it may be that access to the model is limited in 1317
some way (e.g., to registered users), but it should be possible for other researchers 1318
to have some path to reproducing or verifying the results. 1319
5.Open access to data and code 1320
46Question: Does the paper provide open access to the data and code, with sufficient instruc- 1321
tions to faithfully reproduce the main experimental results, as described in supplemental 1322
material? 1323
Answer: [Yes] 1324
Justification: Most of the codes have been released in the supplementary material. The 1325
missing ones are simply the implementations of the numerical integration of the SDEs, 1326
which consist of applying Euler-Maruyama: All code will be released in an appropriate 1327
GitHub repository upon publication. 1328
Guidelines: 1329
• The answer NA means that paper does not include experiments requiring code. 1330
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 1331
public/guides/CodeSubmissionPolicy ) for more details. 1332
•While we encourage the release of code and data, we understand that this might not be 1333
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 1334
including code, unless this is central to the contribution (e.g., for a new open-source 1335
benchmark). 1336
•The instructions should contain the exact command and environment needed to run to 1337
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 1338
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 1339
•The authors should provide instructions on data access and preparation, including how 1340
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 1341
•The authors should provide scripts to reproduce all experimental results for the new 1342
proposed method and baselines. If only a subset of experiments are reproducible, they 1343
should state which ones are omitted from the script and why. 1344
•At submission time, to preserve anonymity, the authors should release anonymized 1345
versions (if applicable). 1346
•Providing as much information as possible in supplemental material (appended to the 1347
paper) is recommended, but including URLs to data and code is permitted. 1348
6.Experimental Setting/Details 1349
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 1350
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 1351
results? 1352
Answer: [Yes] 1353
Justification: We describe all the experimental settings in Section F. 1354
Guidelines: 1355
• The answer NA means that the paper does not include experiments. 1356
•The experimental setting should be presented in the core of the paper to a level of detail 1357
that is necessary to appreciate the results and make sense of them. 1358
•The full details can be provided either with the code, in appendix, or as supplemental 1359
material. 1360
7.Experiment Statistical Significance 1361
Question: Does the paper report error bars suitably and correctly defined or other appropriate 1362
information about the statistical significance of the experiments? 1363
Answer: [Yes] 1364
Justification: Our figures report error bars when relevant. 1365
Guidelines: 1366
• The answer NA means that the paper does not include experiments. 1367
•The authors should answer ”Yes” if the results are accompanied by error bars, confi- 1368
dence intervals, or statistical significance tests, at least for the experiments that support 1369
the main claims of the paper. 1370
47•The factors of variability that the error bars are capturing should be clearly stated (for 1371
example, train/test split, initialization, random drawing of some parameter, or overall 1372
run with given experimental conditions). 1373
•The method for calculating the error bars should be explained (closed form formula, 1374
call to a library function, bootstrap, etc.) 1375
• The assumptions made should be given (e.g., Normally distributed errors). 1376
•It should be clear whether the error bar is the standard deviation or the standard error 1377
of the mean. 1378
•It is OK to report 1-sigma error bars, but one should state it. The authors should 1379
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 1380
of Normality of errors is not verified. 1381
•For asymmetric distributions, the authors should be careful not to show in tables or 1382
figures symmetric error bars that would yield results that are out of range (e.g. negative 1383
error rates). 1384
•If error bars are reported in tables or plots, The authors should explain in the text how 1385
they were calculated and reference the corresponding figures or tables in the text. 1386
8.Experiments Compute Resources 1387
Question: For each experiment, does the paper provide sufficient information on the com- 1388
puter resources (type of compute workers, memory, time of execution) needed to reproduce 1389
the experiments? 1390
Answer: [Yes] 1391
Justification: As we state in Section F, we run our experiments on an NVIDIA GeForce 1392
RTX 3090. 1393
Guidelines: 1394
• The answer NA means that the paper does not include experiments. 1395
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 1396
or cloud provider, including relevant memory and storage. 1397
•The paper should provide the amount of compute required for each of the individual 1398
experimental runs as well as estimate the total compute. 1399
•The paper should disclose whether the full research project required more compute 1400
than the experiments reported in the paper (e.g., preliminary or failed experiments that 1401
didn’t make it into the paper). 1402
9.Code Of Ethics 1403
Question: Does the research conducted in the paper conform, in every respect, with the 1404
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 1405
Answer: [Yes] 1406
Justification: All we do is derive some convergence bounds and similar results. 1407
Guidelines: 1408
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 1409
•If the authors answer No, they should explain the special circumstances that require a 1410
deviation from the Code of Ethics. 1411
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 1412
eration due to laws or regulations in their jurisdiction). 1413
10.Broader Impacts 1414
Question: Does the paper discuss both potential positive societal impacts and negative 1415
societal impacts of the work performed? 1416
Answer: [Yes] 1417
Justification: It can have a positive impact as it helps understand adaptive optimizers better. 1418
Possibly, it might help reduce the cost of fine-tuning thanks to our novel scaling law. 1419
Guidelines: 1420
• The answer NA means that there is no societal impact of the work performed. 1421
48•If the authors answer NA or No, they should explain why their work has no societal 1422
impact or why the paper does not address societal impact. 1423
•Examples of negative societal impacts include potential malicious or unintended uses 1424
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 1425
(e.g., deployment of technologies that could make decisions that unfairly impact specific 1426
groups), privacy considerations, and security considerations. 1427
•The conference expects that many papers will be foundational research and not tied 1428
to particular applications, let alone deployments. However, if there is a direct path to 1429
any negative applications, the authors should point it out. For example, it is legitimate 1430
to point out that an improvement in the quality of generative models could be used to 1431
generate deepfakes for disinformation. On the other hand, it is not needed to point out 1432
that a generic algorithm for optimizing neural networks could enable people to train 1433
models that generate Deepfakes faster. 1434
•The authors should consider possible harms that could arise when the technology is 1435
being used as intended and functioning correctly, harms that could arise when the 1436
technology is being used as intended but gives incorrect results, and harms following 1437
from (intentional or unintentional) misuse of the technology. 1438
•If there are negative societal impacts, the authors could also discuss possible mitigation 1439
strategies (e.g., gated release of models, providing defenses in addition to attacks, 1440
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 1441
feedback over time, improving the efficiency and accessibility of ML). 1442
11.Safeguards 1443
Question: Does the paper describe safeguards that have been put in place for responsible 1444
release of data or models that have a high risk for misuse (e.g., pretrained language models, 1445
image generators, or scraped datasets)? 1446
Answer: [NA] 1447
Justification: The paper poses no such risks. 1448
Guidelines: 1449
• The answer NA means that the paper poses no such risks. 1450
•Released models that have a high risk for misuse or dual-use should be released with 1451
necessary safeguards to allow for controlled use of the model, for example by requiring 1452
that users adhere to usage guidelines or restrictions to access the model or implementing 1453
safety filters. 1454
•Datasets that have been scraped from the Internet could pose safety risks. The authors 1455
should describe how they avoided releasing unsafe images. 1456
•We recognize that providing effective safeguards is challenging, and many papers do 1457
not require this, but we encourage authors to take this into account and make a best 1458
faith effort. 1459
12.Licenses for existing assets 1460
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 1461
the paper, properly credited and are the license and terms of use explicitly mentioned and 1462
properly respected? 1463
Answer: [Yes] 1464
Justification: We cite the used datasets. The rest is all our code and we cite the most relevant 1465
libraries used. 1466
Guidelines: 1467
• The answer NA means that the paper does not use existing assets. 1468
• The authors should cite the original paper that produced the code package or dataset. 1469
•The authors should state which version of the asset is used and, if possible, include a 1470
URL. 1471
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1472
•For scraped data from a particular source (e.g., website), the copyright and terms of 1473
service of that source should be provided. 1474
49•If assets are released, the license, copyright information, and terms of use in the 1475
package should be provided. For popular datasets, paperswithcode.com/datasets 1476
has curated licenses for some datasets. Their licensing guide can help determine the 1477
license of a dataset. 1478
•For existing datasets that are re-packaged, both the original license and the license of 1479
the derived asset (if it has changed) should be provided. 1480
•If this information is not available online, the authors are encouraged to reach out to 1481
the asset’s creators. 1482
13.New Assets 1483
Question: Are new assets introduced in the paper well documented and is the documentation 1484
provided alongside the assets? 1485
Answer: [NA] 1486
Justification: The paper does not release new assets 1487
Guidelines: 1488
• The answer NA means that the paper does not release new assets. 1489
•Researchers should communicate the details of the dataset/code/model as part of their 1490
submissions via structured templates. This includes details about training, license, 1491
limitations, etc. 1492
•The paper should discuss whether and how consent was obtained from people whose 1493
asset is used. 1494
•At submission time, remember to anonymize your assets (if applicable). You can either 1495
create an anonymized URL or include an anonymized zip file. 1496
14.Crowdsourcing and Research with Human Subjects 1497
Question: For crowdsourcing experiments and research with human subjects, does the paper 1498
include the full text of instructions given to participants and screenshots, if applicable, as 1499
well as details about compensation (if any)? 1500
Answer: [NA] 1501
Justification: The paper does not involve crowdsourcing nor research with human subjects. 1502
Guidelines: 1503
•The answer NA means that the paper does not involve crowdsourcing nor research with 1504
human subjects. 1505
•Including this information in the supplemental material is fine, but if the main contribu- 1506
tion of the paper involves human subjects, then as much detail as possible should be 1507
included in the main paper. 1508
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1509
or other labor should be paid at least the minimum wage in the country of the data 1510
collector. 1511
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1512
Subjects 1513
Question: Does the paper describe potential risks incurred by study participants, whether 1514
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1515
approvals (or an equivalent approval/review based on the requirements of your country or 1516
institution) were obtained? 1517
Answer: [NA] 1518
Justification: The paper does not involve crowdsourcing nor research with human subjects 1519
Guidelines: 1520
•The answer NA means that the paper does not involve crowdsourcing nor research with 1521
human subjects. 1522
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1523
may be required for any human subjects research. If you obtained IRB approval, you 1524
should clearly state this in the paper. 1525
50•We recognize that the procedures for this may vary significantly between institutions 1526
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1527
guidelines for their institution. 1528
•For initial submissions, do not include any information that would break anonymity (if 1529
applicable), such as the institution conducting the review. 1530
51