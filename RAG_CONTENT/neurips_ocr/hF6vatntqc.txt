Transformers are Minimax Optimal
Nonparametric In-Context Learners
Juno Kim1,2∗Tai Nakamaki1Taiji Suzuki1,2
1University of Tokyo2Center for Advanced Intelligence Project, RIKEN
∗junokim@g.ecc.u-tokyo.ac.jp
Abstract
In-context learning (ICL) of large language models has proven to be a surprisingly
effective method of learning a new task from only a few demonstrative exam-
ples. In this paper, we study the efficacy of ICL from the viewpoint of statistical
learning theory. We develop approximation and generalization error bounds for
a transformer composed of a deep neural network and one linear attention layer,
pretrained on nonparametric regression tasks sampled from general function spaces
including the Besov space and piecewise γ-smooth class. We show that sufficiently
trained transformers can achieve – and even improve upon – the minimax optimal
estimation risk in context by encoding the most relevant basis representations
during pretraining. Our analysis extends to high-dimensional or sequential data
and distinguishes the pretraining andin-context generalization gaps. Furthermore,
we establish information-theoretic lower bounds for meta-learners w.r.t. both the
number of tasks and in-context examples. These findings shed light on the roles of
task diversity and representation learning for ICL.
1 Introduction
Large language models (LLMs) have demonstrated remarkable capabilities in understanding and
generating natural language data. In particular, the phenomenon of in-context learning (ICL) has
recently garnered widespread attention. ICL refers to the ability of pretrained LLMs to perform a new
task by being provided with a few examples within the context of a prompt, without any parameter
updates or fine-tuning. It has been empirically observed that few-shot prompting is especially effective
in large-scale models (Brown et al., 2020) and requires only a couple of examples to consistently
achieve high performance (García et al., 2023). In contrast, Raventos et al. (2023) demonstrate that
sufficient pretraining task diversity is required for the emergence of ICL. However, we still lack a
comprehensive understanding of the statistical foundations of ICL and few-shot prompting.
A vigorous line of research has been directed towards understanding ICL of single-layer linear
attention models pretrained on the query prediction loss of linear regression tasks (Garg et al., 2022;
Akyürek et al., 2023; Zhang et al., 2023; Ahn et al., 2023; Mahankali et al., 2023; Wu et al., 2024).
It has been shown that the global minimizer of the L2pretraining loss implements one step of GD
on a least-squares linear regression objective (Mahankali et al., 2023) and is nearly Bayes optimal
(Wu et al., 2024). Moreover, risk bounds with respect to the context length (Zhang et al., 2023) and
number of tasks (Wu et al., 2024) have been obtained.
Other works have examined ICL of more complex multi-layer transformers. Bai et al. (2023); von
Oswald et al. (2023) give specific transformer constructions which simulate GD in context, however
it is unclear how such meta-algorithms may be learned. Another approach is to study learning with
representations , where tasks consist of a fixed nonlinear feature map composed with a varying linear
function. Guo et al. (2023) empirically found that trained transformers exhibit a separation where
lower layers transform the input and upper layers perform linear ICL. Recently, Kim and Suzuki
38th Conference on Neural Information Processing Systems (NeurIPS 2024).(2024) analyzed a model consisting of a shallow neural network followed by a linear attention layer
and proved that the MLP component learns to encode the true features during pretraining. However,
they assumed the infinite task and sample size limit and did not study generalization capabilities.
Our contributions. In this paper, we analyze the optimality of ICL from the perspective of statistical
learning theory. Our object of study is a transformer consisting of a deep neural network with N-
dimensional output followed by one linear attention layer. The model is pretrained on ninput-output
samples from Tnonparametric regression tasks, generated from a suitably decaying distribution on a
general function space. Compared to previous works, we take a crucial step towards understanding
practical multi-layer transformers by incorporating the representation learning capabilities of the
DNN module. From a more abstract perspective, this work can also be situated as a nonlinear
extension of meta-learning. Our contributions are highlighted below.
•We develop a general framework for upper bounding the in-context estimation error of the
empirical risk minimizer in terms of the approximation error of the neural network and separate
in-context andpretraining generalization gaps depending on n, T, respectively.
•In the Besov space setting, we show that ICL achieves nearly minimax optimal risk n−2α
2α+d
when Tis sufficiently large. Since LLMs are pretrained on vast amounts of data in practice, T
can be taken to be nearly infinite, justifying the emergence of ICL at large scales. We extend the
optimality guarantees to nearly dimension-free rates in the anisotropic Besov space and also to
learning sequential data with deep transformers in the piecewise γ-smooth function class.
•We show that ICL can improve upon the a priori optimal rate when the task class basis resides
in a coarser Besov space by learning to encode informative basis representations, emphasizing
the importance of pretraining on diverse tasks.
•We also derive information-theoretic lower bounds for the minimax risk in both n, T, rigorously
confirming that ICL is jointly optimal when Tis large (Besov space setting), while any meta-
learning method is jointly suboptimal when Tis small (coarser space setting). This separation
aligns with empirical observations of a task diversity threshold (Raventos et al., 2023).
The paper is structured as follows. In Section 2, the regression tasks and transformer model are
defined in an abstract setting. In Section 3, we present the general framework for estimating the ICL
approximation and generalization error. In Section 4, we specialize to the Besov-type and piecewise
γ-smooth class settings and show that transformers can achieve or exceed the minimax optimal rate
in context. In Section 5, we derive minimax lower bounds. All proofs are deferred to the appendix;
moreover, we provide numerical experiments validating our results in Appendix E.
1.1 Other Related Works
Meta-learning. The theoretical setting of ICL is closely related to meta-learning , where the goal
is to infer a shared representation ψ◦with samples from a set of transformed tasks β⊤
iψ◦. When
ψ◦is linear, fast rates have been established by Tripuraneni et al. (2020); Du et al. (2021), while
the nonlinear case has been studied by Meunier et al. (2023) where ψ◦is a feature projection into
a reproducing kernel Hilbert space. Our results can be viewed as extending this body of work to
function spaces of generalized smoothness with a specific deep transformer architecture.
Optimal rates for DNNs. Our analysis extends established optimality results for classes of DNNs
in ordinary supervised regression settings to ICL. Suzuki (2019) has shown that deep feedforward
networks with the ReLU activation can efficiently approximate functions in the Besov space and thus
achieve the minimax optimal rate. This has been extended to the anisotropic Besov space (Suzuki and
Nitanda, 2021), convolutional neural networks for infinite-dimensional input (Okumoto and Suzuki,
2022), and transformers for sequence-to-sequence functions (Takakura and Suzuki, 2023). We remark
that a work in progress (Imaizumi, 2024) also studies the sample complexity of ICL of transformers
in a Sobolev space setting.
Pretraining dynamics for ICL. While how ICL arises from optimization is not fully understood,
there are encouraging developments in this direction. Zhang et al. (2023) has shown for one layer of
linear attention that running GD on the population risk always converges to the global optimum. This
was extended to incorporate a linear output layer by Zhang et al. (2024), and to softmax attention
2by Huang et al. (2023); Li et al. (2024); Chen et al. (2024). Kim and Suzuki (2024) considered
a compound transformer equivalent to ours with a shallow MLP component and proved that the
loss landscape becomes benign in the mean-field limit, deriving convergence guarantees for the
corresponding gradient dynamics. These analyses indicate that the attention mechanism, while highly
nonconvex, may possess structures favorable for gradient-based optimization.
2 Problem Setup
2.1 Nonparametric Regression
In this paper, we analyze the ability of a transformer to solve nonparametric regression problems in
context when pretrained on examples from a family of regression tasks, which we describe below.
LetX ⊆Rdbe the input space ( dis allowed to be infinite), PXa probability distribution on X,
and(ψ◦
j)∞
j=1a fixed countable subset of L2(PX). A regression function F◦
β:X →Ris randomly
generated for each task by sampling the sequence of coefficients β∈R∞from a distribution Pβon
B(R∞); the class of tasks F◦is defined as
F◦=n
F◦
β=P∞
j=1βjψ◦
jβ∈suppPβo
,
endowed with the induced distribution. Each task prompt contains nexample input-response pairs
{(xk, yk)}n
k=1. The covariates xkare i.i.d. drawn from PXand the responses are generated as
yk=F◦
β(xk) +ξk,1≤k≤n, (1)
where the noise ξkis assumed to be i.i.d. with mean zero and |ξk| ≤σalmost surely.1In addition,
we independently generate a query token ˜xand corresponding output ˜yin the same manner.
We proceed to state our assumptions for the regression model. Informally, we suppose a relaxed
version of sparsity and orthonormality of ψ◦
jand suitable decay rates for the basis expansion. These
will be subsequently verified for specific function spaces of interest with their natural decay rate.
Assumption 1 (relaxed sparsity and orthonormality of basis functions) .ForN∈N, there exist inte-
gers¯N < ¯N≲Nwith ¯N−¯N+1 = Nsuch that ψ◦
¯N,···, ψ◦
¯Nare independent and ψ◦
1,···, ψ◦
¯N−1
are all contained in the linear span of ψ◦
¯N,···, ψ◦
¯N. Moreover, there exist r, C1, C2, C∞>0such
thatΣΨ,N:= 
Ex∼PX[ψ◦
j(x)ψ◦
k(x)]¯N
j,k=¯Nsatisfies C1IN⪯ΣΨ,N⪯C2INand
P¯N
j=¯N(ψ◦
j)21/2
L∞(PX)≤C∞Nr. (2)
Denoting the ¯N-basis approximation of F◦
βasF◦
β,¯N:=P¯N
j=1βjψ◦
j, by Assumption 1 there exist
‘aggregated’ coefficients ¯β
¯N,···,¯β¯Nuniquely determined by βsuch that F◦
β,¯N=P¯N
j=¯N¯βjψ◦
j. We
define two types of coefficient covariance matrices
Σβ,¯N:= (Eβ[βjβk])¯N
j,k=1∈R¯N×¯Nand Σ¯β,N:= 
Eβ[¯βj¯βk]¯N
j,k=¯N∈RN×N.
Assumption 2 (decay of β).Fors, B > 0it holds that ∥F◦
β∥L∞(PX)≤Bfor all F◦
β∈ F◦and
∥F◦
β−F◦
β,N∥2
L2(PX)≲N−2s(3)
uniformly over β∈suppPβ. Furthermore, Tr(Σ ¯β,N)is bounded for all Nand
0≺Σβ,¯N≾diagh
(j−2s−1(logj)−2)¯N
j=1i
. (4)
Remark 2.1. In the simple case where (ψ◦
j)∞
j=1is a basis for L2(PX), we may set¯N= 1,¯N=N
so that the dependency condition of Assumption 1 is trivially satisfied, moreover, Σ¯β,N= Σ β,N
and boundedness of Tr(Σ ¯β,N)automatically follows from (4). However, the assumptions in the
stated form also allow for hierarchical bases with dependencies such as wavelet systems. We
also note that (3)and(4)entail basically the same rate but are not equivalent: the uniform bound
|βj|2≲j−2s−1(logj)−2along with Assumption 1 implies (3). The (logj)−2term can be replaced
with any g(j)such thatP∞
j=1j−1g(j)is convergent.
1This implies Varξk≤σ2. We require boundedness since the values ykform part of the prompt input, and
we wish to utilize sup-norm covering number estimates for the attention map; this technicality can be removed
with more careful analysis. However, for the information-theoretic lower bound we assume Gaussian noise.
32.2 In-Context Learning
We now describe our transformer model, which takes ncontext pairs X= (x1,···, xn)∈Rd×n,
y= (y1,···, yn)⊤∈Rnand a query token ˜xas input and returns a prediction for the corresponding
output. The covariates are first passed through a nonlinear representation or feature mapping ϕ:
X →RN, which we assume belongs to a sufficiently powerful class of estimators FN. Specifically:
Assumption 3 (expressivity of FN).∥ϕ(x)∥2≤B′
Nfor some B′
N>0for all x∈ X,ϕ∈ F N.
Moreover for some δN>0, there exist ϕ∗
¯N,···, ϕ∗
¯N∈ FNsatisfying
max
¯N≤j≤¯N∥ψ◦
j−ϕ∗
j∥L∞(PX)≤δN.
By choosing FNandδNto satisfy the above assumption, we will be able to utilize established
approximation and generalization guarantees for families of deep neural networks in Section 4.
The extracted representations ϕ(X) = (ϕ(x1),···, ϕ(xn))are then mapped to a scalar output via a
linear attention layer parametrized by a matrix Γ∈ SNforSN⊂RN×N,
ˇfΘ(X,y,˜x) :=1
nnX
k=1ykϕ(xk)⊤Γ⊤ϕ(˜x) =Γϕ(X)y
n, ϕ(˜x)
,where Θ = (Γ , ϕ)∈ SN×FN.
Finally, the output is constrained to lie on [−¯B,¯B]by applying clip ¯B(u) := max {min{u,¯B},−¯B},
yielding fΘ(X,y,˜x) := clip ¯B(ˇfΘ(X,y,˜x)). We set SN={Γ∈RN×N|0⪯Γ⪯C3IN}for
some C3>0and fix ¯B=Bfor simplicity.
The above setup is a restricted reparametrization of linear attention widely used in theoretical analyses
(see e.g. Zhang et al., 2023; Wu et al., 2024, for more details), where the values only refer to yand
the query and key matrices are consolidated into one matrix Γ. The form is equivalent to one step
of GD with matrix step size and has been shown to be optimal for a single layer of linear attention
for linear regression tasks (Ahn et al., 2023; Mahankali et al., 2023). The placement of the attention
layer after the DNN module ϕis justified by the observation that lower layers of trained transformers
act as data representations on top of which upper layers perform ICL (Guo et al., 2023).
During pretraining, the model is presented with Tprompts {(X(t),y(t),˜x(t))}T
t=1where the tasks
F◦
β(t)∈ F◦,β(t)∼ P βand tokens X(t)= (x(t)
1,···, x(t)
n),y(t)= (y(t)
1,···, y(t)
n)⊤,˜x(t)and˜y(t)
are independently generated as described in Section 2.1, and is trained to minimize the empirical risk
bΘ = arg min
Θ∈SN×FNbR(Θ),bR(Θ) =1
TTX
t=1
˜y(t)−fΘ(X(t),y(t),˜x(t))2
.
Our goal is to verify the efficiency of ICL as a learning algorithm and show that learning the optimal
bΘallows the transformer to solve new random regression problems y=F◦
β(x) +ξforF◦
β∈ F◦in
context. To this end, we evaluate the convergence of the mean-squared risk or estimation error,
¯R(bΘ) := E(X(t),y(t),˜x(t),˜y(t))T
t=1[R(bΘ)], R (Θ) := EX,y,˜x,β
(F◦
β(˜x)−fΘ(X,y,˜x))2
.
Note that we do not study whether the transformer always converges to bΘ; the training dynamics of a
DNN is already a very difficult problem. For the attention layer, see the discussion in Section 1.1.
3 Risk Bounds for In-Context Learning
In this section, we outline our framework for analyzing the in-context estimation error ¯R(bΘ). Some
additional definitions are in order. The ϵ-covering number N(C, ρ, ϵ)of a metric space Cequipped
with a metric ρforϵ >0is defined as the minimal number of balls in ρwith radius ϵneeded to
coverC(van der Vaart and Wellner, 1996). The ϵ-covering entropy ormetric entropy is given as
V(F, ρ, ϵ) := log N(F, ρ, ϵ). The ϵ-packing number M(ϵ,C, ρ)is given as the maximal cardinality
of aϵ-separated set {c1, . . . , c M} ⊆ C such that ρ(ci, cj)≥δfor all i̸=j. The transformer model
class is defined as TN:={fΘ|Θ∈ SN× F N}.
To bound the overall risk, we first decompose into the approximation and generalization gaps.
4Theorem 3.1 (Schmidt-Hieber (2020), Lemma 4, adapted) .There exists a universal constant Csuch
that for any ϵ >0such that V(TN,∥·∥L∞, ϵ)≥1,
¯R(bΘ)≤2 inf
Θ∈SN×FNR(Θ) + CB2+σ2
TV(TN,∥·∥L∞, ϵ) + (B+σ)ϵ
.
Proof. The convergence rate of the empirical risk minimizer is established for a fixed regression
problem y=f◦(z) +ξin Schmidt-Hieber (2020) when ξis Gaussian; we modify the proof to
incorporate bounded noise in Appendix B.1. The ICL setup can be reduced to the ordinary case as
follows. We consider the entire batch (β,X, ξ1:n,˜x)including the hidden coefficient βas a single
datum zwith output ˜y. The true function is given as f◦(z) =F◦
β(˜x)and the model class is taken to
beTNimplicitly concatenated with the generative process (β,X, ξ1:n,˜x)7→(X,y,˜x). Then R(Θ),
V(TN,∥·∥L∞, ϵ)andTagree with the ordinary L2risk, model class entropy and sample size.
Here, the second term is the pretraining generalization error dependent on the number of tasks T; the
in-context generalization error dependent on the prompt length nmanifests as part of the first term.
This separation allows us to compare the relative difficulty of the two types of learning.
Bounding approximation error. In order to bound the first term, we analyze the risk of the choice
Θ∗= (Γ∗, ϕ∗) :=
ΣΨ,N+1
nΣ−1
¯β,N−1
, ϕ∗
¯N:¯N
where ϕ∗is given as in Assumption 3 for a suitable δNto be determined. The definition of Γ∗
approximately generalizes the global optimum Γ = 
(1 +1
n)Λ +1
ntr(Λ)Id−1for the Gaussian
linear regression setup where x∼ N(0,Λ)(Zhang et al., 2023). Since ΣΨ,N⪰C1INwe have
Γ∗⪯C−1
1INand hence we may assume Γ∗∈ SNby replacing C3withC3∨C−1
1if necessary.
Proposition 3.2. Under Assumptions 1-3, it holds that
inf
Θ∈SN×FNR(Θ)≤R(Θ∗)≲N2r
nlogN+N4r
n2log2N+N
n+N−2s+N2δ4
N+N2r+1δ2
N.
The proof is presented throughout Appendix A. The overall scheme is to approximate F◦
β(˜x)by its
truncation F◦
β,¯N(˜x)and the finite basis ψ◦
¯N:¯Nbyϕ∗
¯N:¯N, which incurs errors N−2sand the terms
pertaining to δN, respectively. The first three terms arise from the concentration of the ntoken
representations ψ◦
¯N:¯N(xk). All hidden constants are at most polynomial in problem parameters.
Bounding generalization error. To estimate the metric entropy of TN, we first reduce to the
metric entropy of the representation class FN. Here, ∥·∥L∞refers to the essential supremum over
the support of PXand also over all Ncomponents for FN. The proof is given in Appendix B.2.
Lemma 3.3. Under Assumptions 1-3, there exists D > 0such that for all ϵsufficiently small,
V(TN,∥·∥L∞, ϵ)≲N2logB′2
N
ϵ+V
FN,∥·∥L∞,ϵ
DB′
N√
N
.
4 Minimax Optimality of In-Context Learning
4.1 Besov Space and DNNs
We now apply our theory to study the sample complexity of ICL when FNconsists of (clipped, see
(6)) deep neural networks. These can be also seen as simplified transformers with attention layers
and skip connections removed. To be precise, we define the set of DNNs with depth L, width W,
sparsity S, norm bound Mand ReLU activation η(x) =x∨0(applied element-wise) as
FDNN(L, W, S, M ) =
(W(L)η+b(L))◦···◦ (W(1)x+b(1))W(1)∈RW×d,W(ℓ)∈RW×W,
W(L)∈RW, b(ℓ)∈RW, b(L)∈R,LX
ℓ=1∥W(ℓ)∥0+∥b(ℓ)∥0≤S,max
1≤ℓ≤L∥W(ℓ)∥∞∨∥b(ℓ)∥∞≤M
.
5TheBesov space is a very general class of functions including the Hölder and Sobolev spaces which
captures spatial inhomogeneity in smoothness, and provides a natural setting in which to study the
expressive power of deep neural networks (Suzuki, 2019). Here, we fix X= [0,1]dfor simplicity.
Definition 4.1 (Besov space) .For2≤p≤ ∞ ,0< q≤ ∞ , fractional smoothness α > 0and
r=⌊α⌋+ 1, therth modulus of f∈Lp(X)is defined using the difference operator ∆r
h, h∈Rdas
wr,p(f, t) := sup∥h∥2≤t∥∆r
h(f)∥p,∆r
h(f)(x) = 1 {x,x+rh∈X}Pr
j=0 r
j
(−1)r−jf(x+jh).
Also, the Besov (quasi-)norm is given as ∥·∥Bαp,q=∥·∥Lp+|·|Bαp,qwhere
|f|Bαp,q:=( R∞
0t−qαwr,p(f, t)qdt
t1/qq <∞
supt>0t−αwr,p(f, t) q=∞
and the Besov space is defined as Bα
p,q(X) ={f∈Lp(X)| ∥f∥Bαp,q<∞}. We write U(Bα
p,q(X))
for the unit ball in (Bα
p,q(X),∥·∥Bαp,q).
We have that the Hölder space Cα(X) =Bα
∞,∞(X)for order α >0, α /∈Nand the Sobolev space
Wm
2(X) =Bm
2,2(X)form∈Nas well as the embeddings Bm
p,1(X),→Wm
p(X),→Bm
p,∞(X); if
α > d/p ,Bα
p,q(X)compactly embeds into the space of continuous functions on X. See Triebel
(1983); Giné and Nickl (2015) for more details. The difficulty of learning a regression function in the
Besov is quantified by the minimax risk; the following rate is classical.
Proposition 4.2 (Donoho and Johnstone (1998)) .The minimax risk for an estimator bfnwithni.i.d.
samples Dn={(xi, yi)}n
i=1overU(Bα
p,q(X))satisfies
infbfn:Dn→Rsupf◦∈U(Bαp,q(X))EDn[∥f◦−bfn∥2
L2(X)]≍n−2α
2α+d.
A natural basis system for Bα
p,q(X)is formed by the B-splines , which can be seen as a type of wavelet
decomposition or multiresolution analysis (DeVore and Popov, 1988). As B-splines are piecewise
polynomials, they can be efficiently approximated by DNNs with at most log depth (Suzuki, 2019).
Definition 4.3 (B-spline wavelet basis) .The tensor product B-spline of order m∈Nsatisfying
m > α + 1−1/p, at resolution k∈Zd
≥0and location ℓ∈Id
k=Qd
i=1[−m: 2ki]is
ωd
k,ℓ(x) =dY
i=1ιm(2kixi−ℓi),where ιm(x) = (ι∗ι∗ ··· ∗ ι|{z}
m+1)(x), ι(x) = 1 {x∈[0,1]}.
When k1=···=kd, we abuse notation and write ωd
k,ℓfork∈Z≥0in place of ωd
(k,···,k),ℓ.
4.2 Estimation Error Analysis
To apply our framework, we set the task class as the unit ball F◦=U(Bα
p,q(X))and take as basis
{ψ◦
j|j∈N}={2kd/2ωd
k,ℓ|k∈Z≥0, ℓ∈Id
k}the set of all B-spline wavelets ordered primarily
by increasing kand scaled to counteract the dilation in x. Abusing notation, we also write βk,ℓ
to denote the coefficient in βcorresponding to 2kd/2ωd
k,ℓ. The set of B-splines at each resolution
are independent, while those of lower resolution can always be decomposed into a linear sum of
B-splines of higher resolution satisfying certain decay rates, which we prove in Proposition C.10.
From this setup, in Appendix C.1.1, we verify Assumptions 1 and 2withr= 1/2, s=α/d under:
Assumption 4. F◦=U(Bα
p,q(X)),α > d/p andPXhas positive Lebesgue density ρXbounded
above and below on X. Also, all coefficients βk,ℓare independent and
Eβ[βk,ℓ] = 0,0<Eβ[β2
k,ℓ]≲2−k(2α+d)k−2,∀k≥0, ℓ∈Id
k. (5)
We can check that we have not given ourselves an easier learning problem with (5): the assumed
variance decay rate is tight (up to the logarithmic factor k−2) in the sense that any f∈U(Bα
p,q(X))
can indeed be expanded into a sum of wavelets with the same coefficient decay when averaged over
ℓ∈Id
k. See Lemma C.1 and the following discussion. We also obtain the following in-context
approximation and entropy bounds in Appendix C.1.2.
6Lemma 4.4. For any δN>0, Assumption 3 is satisfied by taking
FN={ΠB′
N◦ϕ|ϕ= (ϕj)N
j=1, ϕj∈ F DNN(L, W, S, M )} (6)
where ΠB′
Nis the projection in RNto the centered ball of radius B′
N=O(√
N)and each ϕjis a
ReLU network such that L=O(logN+ log δ−1
N)andW, S, M =O(1). Also, the metric entropy of
FNis bounded as V(FN,∥·∥L∞, ϵ)≲NlogN
δNϵ.
Hence Assumptions 1-3 all follow from Assumption 4, and we conclude in Appendix C.1.3:
Theorem 4.5 (minimax optimality of ICL in Besov space) .Under Assumption 4, if n≳NlogN,
¯R(bΘ)≲N−2α
d DNN
approximation
error!
+NlogN
n in-context
generalization
error!
+N2logN
T pretraining
generalization
error!
.
Hence if T≳n2α+2d
2α+dandN≍nd
2α+d, in-context learning achieves the minimax optimal rate
n−2α
2α+dup to a log factor.
The first term arises from the N-term truncation and oracle approximation error of the DNN module,
and is equal to the N-term optimal error (D ˜ung, 2011a). The second and third term each correspond
to the in-context and pretraining generalization gap. With regard to N, we see that n=eΩ(N)
is enough to learn the basis expansion in context, while T=eΩ(N2)is necessary to learn the
attention layer. However if T/N =o(n), the third term dominates and the overall complexity scales
suboptimally as T−α
α+d, illustrating the importance of sufficient pretraining. This also aligns with
the task diversity threshold observed by Raventos et al. (2023). Since the amount of training data for
LLMs is practically infinite in practice, our result justifies the effectiveness of ICL at large scales
with only a small number of in-context samples.
A limitation of ICL. In the regime 1≤p <2, the approximation error is strictly worse without
anadaptive representation scheme and the resulting rate is suboptimal (see Remark C.3). While
DNNs can adapt to task smoothness in supervised settings (Suzuki, 2019), ICL and any other meta-
learning methods are fundamentally constrained to non-adaptive representations since they cannot
update at inference time, and hence are bounded below by the best linear approximation rate or
Kolmogorov width, which is strictly worse than the minimax optimal rate when p <2. Indeed, for
anyN-dimensional subspace LN⊂Bα
p,q(X)it holds that (Vybíral, 2008)
infLNsupf◦∈U(Bαp,q(X))infℓn∈LN∥f◦−ℓn∥L2(PX)≳N−α/d+(1/p−1/2)+.
Remark 4.6. TheN2logNterm in the pretraining generalization gap is due to the covering bound of
the attention matrix Γ, while the entropy of the DNN class is only NlogN. Hence the task diversity
requirement may be lessened to the latter by considering low-rank structure or approximation of
attention heads (Bhojanapalli et al., 2020; Chen et al., 2021).
4.3 Avoiding the Curse of Dimensionality
The above rate inevitably suffers from the curse of dimensionality as dappears in the exponent of
the optimal rate. We also consider the anisotropic Besov space (Nikol’skii, 1975), a generalization
allowing for different degrees of smoothness (α1,···, αd)in each coordinate. Then the optimal
rate is nearly dimension-free in the sense that the rate only depends on dthrough the quantity
eα:= (P
iα−1
i)−1, and becomes independent of dimension if only a few directions are important i.e.
have small αi. Rigorous definitions, statements and proofs are provided in Appendix C.2.
Extending Theorem 4.5, we show that ICL again attains near-optimal estimation error in the
anisotropic Besov space, circumventing the curse of dimensionality and theoretically establishing the
efficiacy of in-context learning in high-dimensional settings.
Theorem 4.7 (informal version of Theorem C.7) .For the anisotropic Besov space of smoothness
(α1,···, αd), assume variance decay (4)withs=eα >1/p. IfT≳nNandN≍n1
2eα+1, in-context
learning achieves the minimax optimal rate n−2eα
2eα+1up to a log factor.
74.4 Learning a Coarser Basis
Thus far, we have demonstrated the importance of sufficient pretraining to achieve optimal risk;
as another application of our framework, we illustrate how pretraining can actively mitigate the
complexity of in-context learning. Consider the case where (ψ◦
j)∞
j=1is no longer the B-spline basis
ofBα
p,q(X)but instead is chosen from some wider function space, say the unit ball of Bτ
p,q(X)for a
smaller smoothness τ < α . Without knowledge of the basis, the sample complexity of learning any
regression function F◦
βis a priori lower bounded by the minimax rate n−2τ
2τ+dby Proposition 4.2.
For ICL, this difficulty manifests as an increase in the metric entropy of the class FNwhich must be
powerful enough to approximate ψ◦
1:N(Corollary C.12), giving rise to the modified risk bound:
Corollary 4.8 (ICL for coarser basis) .Suppose α > τ > d/p , the basis (ψ◦
j)∞
j=1⊂U(Bτ
p,q(X))
and Assumptions 1, 2 hold with r= 1/2, s=α/d. Then if n≳NlogN,
¯R(bΘ)≲N−2α
d+NlogN
n+N1+α
τ+d
τlog3N
T.
Hence if T≳n1+d
2α+dα+d
τandN≍nd
2α+d, the risk converges as n−2α
2α+dlogn.
The pretraining generalization gap is now dominated by the higher complexity N1+α
τ+d
τlog3Nof
the DNN class and strictly worse compared to N2logNfor Theorem 4.5. The required number of
tasks also suffers and the exponent is no longer2α+2d
2α+d∈(1,2)but scales as O(d). Nevertheless,
observe that the burden of complexity is entirely carried by T; with sufficient pretraining, the third
term can be made arbitrarily small and the ICL risk again attains n−2α
2α+d. Hence ICL improves
upon the a priori lower bound n−2τ
2τ+dat inference time by encoding information on the coarser basis
during pretraining. We remark that the result is also readily adapted to the anisotropic setting.
4.5 Sequential Input and Transformers
We now consider a more complex setting where the inputs x∈[0,1]d×∞are bidirectional sequences
of tokens (e.g. entire documents) and ϕis itself a transformer network.2In this infinite-dimensional
setting, transformers can still circumvent the curse of dimensionality and in fact achieve near-optimal
sample complexity due to their parameter sharing and feature extraction capabilities (Takakura and
Suzuki, 2023). Our goal in this section is to extend this guarantee to ICL of trained transformers.
For sequential data, it is natural to suppose the smoothness w.r.t. each coordinate can vary depending
on the input. For example, the position of important tokens in a sentence will change if irrelevant
strings are inserted. To this end, we adopt the piecewise γ-smooth function class introduced by
Takakura and Suzuki (2023), which allows for arbitrary bounded permutations among input tokens;
see Appendix D.1 for definitions. Also borrowing from their setup, we consider multi-head sliding
window self-attention layers with window size U, embedding dimension D, number of heads Hwith
key, query, value matrices K(h), Q(h)∈RD×d, V(h)∈Rd×dand norm bound Mdefined as3
FAttn(U, D, H, M ) =
g:Rd×∞→Rd×∞max
1≤h≤H∥K(h)∥∞∨ ∥Q(h)∥∞∨ ∥V(h)∥∞≤M,
g(x)i=xi+HX
h=1V(h)xi−U:i+USoftmax
(K(h)xi−U:i+U)⊤Q(h)xi
.
We also consider a linear embedding layer Enc(x) =Ex+P,E∈RD×dwith absolute positional
encoding P∈RDof bounded norm. Then the class of depth Jtransformers is defined as
FTF(J, U, D, H, L, W, S, M ) :=
fJ◦gJ◦ ··· ◦ f1◦g1◦Enc| ∥E∥ ≤M,
fi∈ F DNN(L, W, S, M ), gi∈ F Attn(U, D, H, M )	
.
Our result, proved in Appendix D.2, reads:
2We clarify that this is notequivalent to a multi-layer transformer setting where ϕis the rest of the transformer.
Instead, ϕoperates on individual tokens xiseparately, which may now themselves be sequences of unbounded
dimension. The extracted per-token features are cross-referenced only at the final attention layer fΘ.
3Here the ith column and (j, i)th component of x∈Rd×∞fori∈Z, j∈[d]are denoted by xiandxij,
respectively. These are not to be confused with sample indexes (1) as those will not be used in this section.
8Theorem 4.9 (informal version of Theorem D.1) .Suppose F◦consists of functions on [0,1]d×∞
of bounded piecewise γ-smooth and L∞-norm with smoothness α∈Rd×∞
>0, and let γbe mixed
or anisotropic smoothness with α†= max i,jαijor(P
i,jα−1
ij)−1, respectively. Under suitable
regularity and decay assumptions, by taking FNto be a class of clipped transformers it holds that
¯R(bΘ)≲N−2α†+NlogN
n+N2∨(1+1/α†)polylog( N)
T.
Hence if T≳nN1∨1/α†andN≍n1
2α†+1, ICL achieves the rate n−2α†
2α†+1polylog( n).
This matches the optimal rate in finite dimensions independently of the (possibly infinite) length of
the input or context window. The dynamical feature extraction ability of attention layers in the FTF
class is essential in dealing with input-dependent smoothness, further justifying the efficiacy of ICL
of sequential data.
5 Minimax Lower Bounds
In this section, we provide lower bounds for the minimax rate in both n, T by extending the theory of
Yang and Barron (1999), which can be leveraged to yield results stronger than optimality in merely n.
The bound is purely information-theoretic and hence applies to not just ICL but any meta-learning
scheme for the regression problem of Section 2.1 from the data Dn,T={(X(t),y(t))}T+1
t=1, where
the index T+ 1corresponds to the test task.
For this section we assume that the noise (1)is i.i.d. Gaussian, ξk∼ N(0, σ2), instead of bounded;
while the exact shape of the noise distribution is not important, having restricted support may convey
additional information and affect the minimax rate. We also suppose for simplicity that the support
ofPβis included in B:={β∈R∞| |βj|2≲j−2s−1(logj)−2, j∈N}and that the aggregated
coefficients ¯βjfor¯N≤j≤¯Nsatisfy Eβ[¯β2
j]≤σ2
βfor some σβdependent on N. The proof of the
following statement is given in Appendix F.1.
Proposition 5.1. Forεn,1, εn,2, δn>0, letQ1andQ2be the εn,1- and εn,2-covering numbers
ofFNandBrespectively, and Mbe the δn-packing number of F◦. Suppose that the following
conditions are satisfied:
1
2σ2
n(T+ 1)σ2
βε2
n,1+C2nε2
n,2
≤logQ1+ log Q2≤1
8logM, 4 log 2 ≤logM. (7)
Then the minimax rate is lower bounded as
infbf:Dn,T→Rsupf◦∈F◦EDn,T[∥bf−f◦∥2
L2(PX)]≥1
4δ2
n.
Finally, Proposition 5.1 is applied to obtain concrete lower bounds for the settings studied in Section
4 throughout Appendices F.2-F.4.
Corollary 5.2 (minimax lower bound) .The minimax rates in the previous regression settings are
lower bounded as follows.
(i)Besov space (Section 4.2): infbfsupf◦EDn,T[∥bf−f◦∥2]≳n−2α
2α+d,
(ii)Coarser basis (Section 4.4): infbfsupf◦EDn,T[∥bf−f◦∥2]≳n−2α
2α+d+ (nT)−2τ
2τ+d,
(iii) Sequential input (Section 4.5): infbfsupf◦EDn,T[∥bf−f◦∥2]≳n−2α†
2α†+1.
These results match the upper bounds for (i), (iii) and show that ICL is provably jointly optimal in
n, T in the ‘large T’ regime. Moreover, we can check for the coarser basis setting that insufficient
pretraining T=O(1)indeed leads to the worse complexity n−2τ
2τ+d, while the faster rate n−2α
2α+dis
retrieved when T≳n(α−τ)d
(2α+d)τ. This aligns with the discussion in Section 4.4, showing that ICL is
provably suboptimal in the ‘small T’ regime.
Remark 5.3. The obtained upper and lower bounds in the coarser basis setting are not tight as T
varies, hence it remains to be shown whether there exists a meta-learning algorithm that attains the
lower bound (ii). The task diversity threshold for optimal learning suggested by the bounds are also
different ( n1+d(α+d)
(2α+d)v.s.n(α−τ)d
(2α+d)τ); it would be interesting for future work to resolve this gap.
96 Conclusion
In this paper, we performed a learning-theoretic analysis of ICL of a transformer consisting of a DNN
and a linear attention layer pretrained on nonparametric regression tasks. We developed a general
framework for bounding the in-context estimation error of the empirical risk minimizer in terms of
both the number of tasks and samples, and proved that ICL can achieve nearly minimax optimal rates
in the Besov space, anisotropic Besov space and γ-smooth class. We also demonstrated that ICL can
improve upon the a priori optimal rate by learning informative representations during pretraining.
We supplemented our analyses with corresponding minimax lower bounds jointly in n, T and also
performed numerical experiments validating our findings. Our work opens up interesting approaches
of adapting classical learning theory to study emergent phenomena of foundation models.
Limitations. Our transformer model is limited to a single layer of linear self-attention and does not
consider more complex in-context learning behavior which may arise in transformers with multiple
attention layers. Moreover, the obtained upper and lower bounds are not tight in certain regimes,
suggesting future research directions for meta-learning.
Acknowledgments
JK was partially supported by JST CREST (JPMJCR2015). TS was partially supported by JSPS
KAKENHI (24K02905, 20H00576) and JST CREST (JPMJCR2115). We would like to express our
gratitude to Masaaki Imaizumi for valuable and insightful discussions on the topic in relation to his
work in progress (Imaizumi, 2024).
References
K. Ahn, X. Cheng, H. Daneshmand, and S. Sra. Transformers learn to implement preconditioned
gradient descent for in-context learning. arXiv preprint arXiv:2306.00297 , 2023.
E. Akyürek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-
context learning? Investigations with linear models. In International Conference on Learning
Representations , 2023.
Y . Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: provable in-
context learning with in-context algorithm selection. In ICML Workshop on Efficient Systems for
Foundation Models , 2023.
S. Bhojanapalli, C. Yun, A. S. Rawat, S. J. Reddi, and S. Kumar. Low-rank bottleneck in multi-head
attention models. In International Conference on Machine Learning , 2020.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot
learners. In Advances in Neural Information Processing Systems , 2020.
B. Chen, T. Dao, E. Winsor, Z. Song, A. Rudra, and C. Ré. Scatterbrain: unifying sparse and low-rank
attention approximation. In Advances in Neural Information Processing Systems , 2021.
S. Chen, H. Sheen, T. Wang, and Z. Yang. Training dynamics of multi-head softmax attention for
in-context learning: emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442 ,
2024.
R. A. DeVore and V . A. Popov. Interpolation of Besov spaces. Transactions of the American
Mathematical Society , 305(1):397–414, 1988.
D. L. Donoho and I. M. Johnstone. Minimax estimation via wavelet shrinkage. The Annals of
Statistics , 26(3):879–921, 1998.
S. Du, W. Hu, S. Kakade, J. Lee, and Q. Lei. Few-shot learning via learning the representation,
provably. In International Conference on Learning Representations , 2021.
10D. D ˜ung. Optimal adaptive sampling recovery. Advances in Computational Mathematics , 34:1–41,
2011a.
D. D ˜ung. B-spline quasi-interpolant representations and sampling recovery of functions with mixed
smoothness. Journal of Complexity , 27(6):541–567, 2011b.
X. García, Y . Bansal, C. Cherry, G. F. Foster, M. Krikun, F. Feng, M. Johnson, and O. Firat.
The unreasonable effectiveness of few-shot learning for machine translation. In International
Conference on Machine Learning , 2023.
S. Garg, D. Tsipras, P. Liang, and G. Valiant. What can Transformers learn in-context? A case study
of simple function classes. In Advances in Neural Information Processing Systems , 2022.
E. Giné and R. Nickl. Mathematical foundations of infinite-dimensional statistical models . Cambridge
Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015.
T. Guo, W. Hu, S. Mei, H. Wang, C. Xiong, S. Savarese, and Y . Bai. How do Transformers learn
in-context beyond simple functions? A case study on learning with representations. arXiv preprint
arXiv:2310.10616 , 2023.
S. Hayakawa and T. Suzuki. On the minimax optimality and superiority of deep neural network
learning over sparse parameter spaces. Neural Networks , 123:343–361, 2020.
Y . Huang, Y . Cheng, and Y . Liang. In-context convergence of Transformers. arXiv preprint
arXiv:2310.05249 , 2023.
M. Imaizumi. Statistical analysis on in-context learning, 2024. Personal communication.
J. Kim and T. Suzuki. Transformers learn nonlinear features in context: nonconvex mean-field
dynamics on the attention landscape. In International Conference on Machine Learning , 2024.
H. Li, M. Wang, S. Lu, X. Cui, and P.-Y . Chen. Training nonlinear Transformers for efficient in-context
learning: a theoretical learning and generalization analysis. arXiv preprint arXiv:2402.15607 ,
2024.
A. Mahankali, T. B. Hashimoto, and T. Ma. One step of gradient descent is provably the optimal
in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576 , 2023.
D. Meunier, Z. Li, A. Gretton, and S. Kpotufe. Nonlinear meta-learning can guarantee faster rates.
arXiv preprint arXiv:2307.10870 , 2023.
S. M. Nikol’skii. Approximation of functions of several variables and imbedding theorems , volume
205 of Grundlehren der mathematischen Wissenschaften . Springer Berlin, 1975.
Y . Nishimura and T. Suzuki. Minimax optimality of convolutional neural networks for infinite dimen-
sional input-output problems and separation from kernel methods. In The Twelfth International
Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=
EW8ZExRZkJ .
S. Okumoto and T. Suzuki. Learnability of convolutional neural networks for infinite dimensional
input via mixed and anisotropic smoothness. In International Conference on Learning Representa-
tions , 2022.
A. Raventos, M. Paul, F. Chen, and S. Ganguli. Pretraining task diversity and the emergence of
non-Bayesian in-context learning for regression. In Advances in Neural Information Processing
Systems , 2023.
J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation
function. The Annals of Statistics , 48(4), 2020.
T. Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:
optimal rate and curse of dimensionality. In International Conference on Learning Representations ,
2019.
11T. Suzuki and A. Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness
in anisotropic Besov space. In Advances in Neural Information Processing Systems , 2021.
S. J. Szarek. Nets of Grassmann manifold and orthogonal group. Proceedings of Banach Space
Workshop , pages 169–186, 1981.
S. Takakura and T. Suzuki. Approximation and estimation ability of Transformers for sequence-
to-sequence functions with infinite dimensional input. In International Conference on Machine
Learning , 2023.
H. Triebel. Theory of function spaces . Monographs in mathematics. Birkhäuser Verlag, 1983.
H. Triebel. Entropy numbers in function spaces with mixed integrability. Revista Matematica
Complutense , 24:169–188, 2011.
N. Tripuraneni, C. Jin, and M. Jordan. Provable meta-learning of linear representations. In Interna-
tional Conference on Machine Learning , 2020.
J. A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine
Learning , 8(1–2):1–230, May 2015. ISSN 1935-8237.
A. W. van der Vaart and J. A. Wellner. Weak convergence and empirical processes: with applications
to statistics . Springer, New York, 1996.
J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and
M. Vladymyrov. Transformers learn in-context by gradient descent. In International Conference
on Machine Learning , 2023.
J. Vybíral. Function spaces with dominating mixed smoothness , volume 30 of Lectures in Mathematics .
European Mathematical Society, 2006.
J. Vybíral. Widths of embeddings in function spaces. Journal of Complexity , 24(4):545–570, 2008.
J. Wu, D. Zou, Z. Chen, V . Braverman, Q. Gu, and P. L. Bartlett. How many pretraining tasks are
needed for in-context learning of linear regression? In International Conference on Learning
Representations , 2024.
Y . Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. The
Annals of Statistics , 27(5):1564–1599, 1999.
D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks , 94:
103–114, 2016.
R. Zhang, S. Frei, and P. L. Bartlett. Trained Transformers learn linear models in-context. arXiv
preprint arXiv:2306.09927 , 2023.
R. Zhang, J. Wu, and P. L. Bartlett. In-context learning of a linear Transformer block: benefits of the
MLP component and one-step GD initialization. arXiv preprint arXiv:2402.14951 , 2024.
12Table of Contents
1 Introduction 1
1.1 Other Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2 Problem Setup 3
2.1 Nonparametric Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 In-Context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 Risk Bounds for In-Context Learning 4
4 Minimax Optimality of In-Context Learning 5
4.1 Besov Space and DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.2 Estimation Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4.3 Avoiding the Curse of Dimensionality . . . . . . . . . . . . . . . . . . . . . . . . 7
4.4 Learning a Coarser Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.5 Sequential Input and Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 Minimax Lower Bounds 9
6 Conclusion 10
A Proof of Proposition 3.2 15
A.1 Decomposing Approximation Error . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.2 Bounding Term (8) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Bounding Term (9) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.4 Bounding Terms (10)-(12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.5 Proof of Lemma A.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B Proofs on Metric Entropy 22
B.1 Modified Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2 Proof of Lemma 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 Proof of Lemma B.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C Details on Besov-type Spaces 24
C.1 Besov Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.1.1 Verification of Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.1.2 Proof of Lemma 4.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
C.1.3 Proof of Theorem 4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2 Anisotropic Besov Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2.1 Definitions and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2.2 Proof of Theorem C.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.3 Wavelet Refinement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
C.4 Proof of Corollary 4.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
D Details on Sequential Input 33
D.1 Definitions and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
13D.2 Proof of Theorem D.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
E Numerical Experiments 36
F Proofs of Minimax Lower Bounds 37
F.1 Proof of Proposition 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
F.2 Lower Bound in Besov Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
F.3 Lower Bound with Coarser Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
F.4 Lower Bound in Piecewise γ-smooth Class . . . . . . . . . . . . . . . . . . . . . 39
14Appendix
A Proof of Proposition 3.2
A.1 Decomposing Approximation Error
Recall that
Θ∗= (Γ∗, ϕ∗) = 
ΣΨ,N+1
nΣ−1
¯β,N−1
, ϕ∗
¯N:¯N!
.
We introduce some additional notation in the following fashion. For brevity, we write ¯N:∞instead
of(¯N+ 1) : ∞as an exception.
Φ∗= (ϕ∗
¯N:¯N(x1),···, ϕ∗
¯N:¯N(xn))∈RN×n, ξ = (ξ1, . . . , ξ n)⊤∈Rn,
Ψ◦= (ψ◦(x1), . . . , ψ◦(xn))∈R∞×n,Ψ◦
¯N:¯N= (ψ◦
¯N:¯N(x1),···, ψ◦
¯N:¯N(xn))∈RN×n,
Ψ◦
¯N:∞= (ψ◦
¯N:∞(x1),···, ψ◦
¯N:∞(xn))∈R∞×n.
Since clipping ˇfΘdoes not make its difference with F◦
β∈[−B, B]larger, it holds that
R(Θ∗) =Eh 
F◦
β(˜x)−fΘ∗(X,y,˜x)2i
≤Eh 
F◦
β(˜x)−ˇfΘ∗(X,y,˜x)2i
≤2E
F◦
β(˜x)−F◦
β,¯N(˜x)2
+ 2E
F◦
β,¯N(˜x)−ˇfΘ∗(X,y,˜x)2
≲N−2s+E"
F◦
β,¯N(˜x)−ϕ∗(˜x)⊤Γ∗Φ∗y
n2#
due to Assumption 2 and ¯N≍N. Expanding the attention output as
ϕ∗(˜x)⊤Γ∗Φ∗y
n= (ϕ∗(˜x)−ψ◦
¯N:¯N(˜x) +ψ◦
¯N:¯N(˜x))⊤Γ∗(Φ∗−Ψ◦
¯N:¯N+ Ψ◦
¯N:¯N)y
n
= (ϕ∗(˜x)−ψ◦
¯N:¯N(˜x))⊤Γ∗(Φ∗−Ψ◦
¯N:¯N)y
n
+ (ϕ∗(˜x)−ψ◦
¯N:¯N(˜x))⊤Γ∗Ψ◦
¯N:¯Ny
n
+ψ◦
¯N:¯N(˜x)⊤Γ∗(Φ∗−Ψ◦
¯N:¯N)y
n
+ψ◦
¯N:¯N(˜x)⊤Γ∗Ψ◦
¯N:¯Ny
n,
and the final term further as
ψ◦
¯N:¯N(˜x)⊤Γ∗Ψ◦
¯N:¯Ny
n=ψ◦
¯N:¯N(˜x)⊤Γ∗Ψ◦
¯N:¯N(Ψ◦⊤β+ξ)
n
=ψ◦
¯N:¯N(˜x)⊤Γ∗Ψ◦
¯N:¯NΨ◦⊤
¯N:¯N¯β
¯N:¯N
n+ψ◦
1:N(˜x)⊤Γ∗Ψ◦
¯N:¯N(Ψ◦⊤
¯N:∞β¯N:∞+ξ)
n,
we obtain that
E"
F◦
β,¯N(˜x)−ϕ∗(˜x)⊤Γ∗Φ∗y
n2#
≲E
 
F◦
β,¯N(˜x)−ψ◦
¯N:¯N(˜x)⊤Γ∗Ψ◦
¯N:¯NΨ◦⊤
¯N:¯N¯β
¯N:¯N
n!2
 (8)
15+E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗Ψ◦
¯N:¯N(Ψ◦⊤
¯N:∞β¯N:∞+ξ)
n!2
 (9)
+E
 
(ϕ∗(˜x)−ψ◦
¯N:¯N(˜x))⊤Γ∗(Φ∗−Ψ◦
¯N:¯N)y
n!2
 (10)
+E
 
(ϕ∗(˜x)−ψ◦
¯N:¯N(˜x))⊤Γ∗Ψ◦
¯N:¯Ny
n!2
 (11)
+E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗(Φ∗−Ψ◦
¯N:¯N)y
n!2
. (12)
We proceed to control each term separately, from which the statement of Proposition 3.2 will follow.
A.2 Bounding Term (8)
Since F◦
β,¯N(˜x) =ψ◦
1:¯N(˜x)⊤β1:¯N=ψ◦
¯N:¯N(˜x)⊤¯β
¯N:¯N, we can introduce a (Γ∗)−1factor to bound
E
 
F◦
β,¯N(˜x)−ψ◦
¯N:¯N(˜x)⊤Γ∗Ψ◦
¯N:¯NΨ◦⊤
¯N:¯N¯β
¯N:¯N
n!2

=E
 
F◦
β,¯N(˜x)−ψ◦
¯N:¯N(˜x)⊤Γ∗ Ψ◦
¯N:¯NΨ◦⊤
¯N:¯N
n−(Γ∗)−1+ (Γ∗)−1!
¯β
¯N:¯N!2

=E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗ Ψ◦
¯N:¯NΨ◦⊤
¯N:¯N
n−ΣΨ,N−1
nΣ−1
¯β,N!
¯β
¯N:¯N!2

≤2E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗ Ψ◦
¯N:¯NΨ◦⊤
¯N:¯N
n−ΣΨ,N!
¯β
¯N:¯N!2
 (13)
+ 2E"
ψ◦
¯N:¯N(˜x)⊤Γ∗Σ−1
¯β,N
n¯β
¯N:¯N2#
. (14)
Denote the operator norm (with respect to L2norm of vectors) by ∥·∥op. For (13), noting that ΣΨ,N
is positive definite,
E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗ Ψ◦
¯N:¯NΨ◦⊤
¯N:¯N
n−ΣΨ,N!
¯β
¯N:¯N!2

=E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗Σ1/2
Ψ,N 
Σ−1/2
Ψ,NΨ◦
¯N:¯NΨ◦⊤
¯N:¯N
nΣ−1/2
Ψ,N−IN!
Σ1/2
Ψ,N¯β
¯N:¯N!2

=EX,β"
¯β⊤
¯N:¯NΣ1/2
Ψ,N 
Σ−1/2
Ψ,NΨ◦
¯N:¯NΨ◦⊤
¯N:¯N
nΣ−1/2
Ψ,N−IN!
Σ1/2
Ψ,NΓ∗E˜xh
ψ◦
¯N:¯N(˜x)ψ◦
¯N:¯N(˜x)⊤i
×Γ∗Σ1/2
Ψ,N 
Σ−1/2
Ψ,NΨ◦
¯N:¯NΨ◦⊤
¯N:¯N
nΣ−1/2
Ψ,N−IN!
Σ1/2
Ψ,N¯β
¯N:¯N#
=EX"
Tr" 
Σ−1/2
Ψ,NΨ◦
¯N:¯NΨ◦⊤
¯N:¯N
nΣ−1/2
Ψ,N−IN!
Σ1/2
Ψ,NEβh
¯β
¯N:¯N¯β⊤
¯N:¯Ni
Σ1/2
Ψ,N
× 
Σ−1/2
Ψ,NΨ◦
¯N:¯NΨ◦⊤
¯N:¯N
nΣ−1/2
Ψ,N−IN!
Σ1/2
Ψ,NΓ∗ΣΨ,NΓ∗Σ1/2
Ψ,N##
16≲EX
Tr" 
Σ−1/2
Ψ,NΨ◦
¯N:¯NΨ◦⊤
¯N:¯N
nΣ−1/2
Ψ,N−IN!2
Σ1/2
Ψ,NΣ¯β,NΣ1/2
Ψ,N#

due to the independence of X,˜xandβ. For the last inequality, we have used the fact that both the
Σ1/2
Ψ,NΓ∗ΣΨ,NΓ∗Σ1/2
Ψ,Nterm and the matrix multiplied to it are positive semi-definite, and the former
is bounded above as ≾INby Assumption 1.
Furthermore, we utilize the following result proved in Appendix A.5:
Lemma A.1. EX
Σ−1/2
Ψ,NΨ◦
¯N:¯NΨ◦⊤
¯N:¯N
nΣ−1/2
Ψ,N−IN2
op
≲N2r
nlogN+N4r
n2log2N.
It follows that (13) is bounded as
E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗ Ψ◦
¯N:¯NΨ◦⊤
¯N:¯N
n−ΣΨ,N!
¯β
¯N:¯N!2

≲EX
Σ−1/2
Ψ,NΨ◦
¯N:¯NΨ◦⊤
¯N:¯N
nΣ−1/2
Ψ,N−IN2
op
Trh
Σ1/2
Ψ,NΣ¯β,NΣ1/2
Ψ,Ni
≲N2r
nlogN+N4r
n2log2N
∥ΣΨ,N∥opTr(Σ ¯β,N)
≲N2r
nlogN+N4r
n2log2N
since Tr(Σ ¯β,N)is bounded by Assumption 2.
Moreover for (14), we compute
E"
ψ◦
¯N:¯N(˜x)⊤Γ∗Σ−1
¯β,N
n¯β
¯N:¯N2#
=E"
¯β⊤
¯N:¯NΣ−1
¯β,N
nΓ∗ψ◦
¯N:¯N(˜x)ψ◦
¯N:¯N(˜x)⊤Γ∗Σ−1
¯β,N
n¯β
¯N:¯N#
=E"
Tr"Σ−1
¯β,N
nΓ∗ψ◦
¯N:¯N(˜x)ψ◦
¯N:¯N(˜x)⊤Γ∗Σ−1
¯β,N
n¯β
¯N:¯N¯β⊤
¯N:¯N##
= Tr"Σ−1
¯β,N
nΓ∗E˜xh
ψ◦
¯N:¯N(˜x)ψ◦
¯N:¯N(˜x)⊤i
Γ∗Σ−1
¯β,N
nEβh
¯β
¯N:¯N¯β⊤
¯N:¯Ni#
= Tr"Σ−1
¯β,N
nΓ∗ΣΨ,NΓ∗
|{z}
positive definiteΣ−1
¯β,N
nΣ¯β,N#
≤1
nTr"
ΣΨ,N+1
nΣ−1
¯β,N
Γ∗ΣΨ,NΓ∗#
=1
nTr"
ΣΨ,N
ΣΨ,N+1
nΣ−1
¯β,N−1#
≤N
n.
A.3 Bounding Term (9)
Since the sequence of covariates x1,···, xnand noise ξ1,···, ξnare each i.i.d.,
E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗Ψ◦
¯N:¯N(Ψ◦⊤
¯N:∞β¯N:∞+ξ)
n!2

17=E
 nX
i=1ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(xi)(β⊤
¯N:∞ψ◦
¯N:∞(xi) +ξi)
n!2

=1
n2E
 nX
i=1ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(xi)β⊤
¯N:∞ψ◦
¯N:∞(xi) +nX
i=1ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(xi)ξi!2

≤1
nE
ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x)β⊤
¯N:∞ψ◦
¯N:∞(x)2
(15)
+n−1
nEh
ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x)β⊤
¯N:∞ψ◦
¯N:∞(x)ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x′)β⊤
¯N:∞ψ◦
¯N:∞(x′)i
(16)
+σ2
nE
ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x)2
(17)
for independent samples x, x′,˜x∼ PX. We now bound the three terms separately below.
For (15), we have that
1
nE
ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x)β⊤
¯N:∞ψ◦
¯N:∞(x)2
=1
nEh
ψ◦
¯N:¯N(x)⊤Γ∗ψ◦
¯N:¯N(˜x)ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x)ψ◦
¯N:∞(x)⊤β¯N:∞β⊤
¯N:∞ψ◦
¯N:∞(x)i
=1
nExh
ψ◦
¯N:¯N(x)⊤Γ∗ΣΨ,NΓ∗ψ◦
¯N:¯N(x)ψ◦
¯N:∞(x)⊤Eβ
β¯N:∞β⊤
¯N:∞
ψ◦
¯N:∞(x)i
≲1
nExh
∥ψ◦
¯N:¯N(x)∥2ψ◦
¯N:∞(x)⊤Eβ
β¯N:∞β⊤
¯N:∞
ψ◦
¯N:∞(x)i
≲1
nsup
x∈suppPX∥ψ◦
¯N:¯N(x)∥2·lim
M→∞Tr 
Eβ
β¯N:Mβ⊤
¯N:M
Ex
ψ◦
¯N:M(x)ψ◦
¯N:M(x)⊤
≲1
nsup
x∈suppPX∥ψ◦
¯N:¯N(x)∥2·lim
M→∞Tr 
Eβ
β¯N:Mβ⊤
¯N:M
since ΣΨ,M⪯C2INasM→ ∞ by Assumption 2. As supx∥ψ◦
¯N:¯N(x)∥2≲N−2rby(2)and the
diagonal of Eβ[β¯N:Mβ⊤
¯N:M]decays faster than ¯N−2sM−1(logM)−2when M > ¯Ndue to (4), it
follows that
1
nsup
x∈suppPX∥ψ◦
¯N:¯N(x)∥2·lim
M→∞Tr 
Eβ
β¯N:Mβ⊤
¯N:M
≲1
nN2rN−2s.
Similarly, for (16), we have that
n−1
nEh
ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x)β⊤
¯N:∞ψ◦
¯N:∞(x)ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x′)β⊤
¯N:∞ψ◦
¯N:∞(x′)i
≤E
ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x)β⊤
¯N:∞ψ◦
¯N:∞(x)⊤
ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x′)β⊤
¯N:∞ψ◦
¯N:∞(x′)
=Ex,x′,βh
ψ◦
¯N:∞(x)⊤β¯N:∞ψ◦
¯N:¯N(x)⊤Γ∗ΣΨ,NΓ∗ψ◦
¯N:¯N(x′)β⊤
¯N:∞ψ◦
¯N:∞(x′)i
=Eβ
Exh
ψ◦
¯N:¯N(x)β⊤
¯N:∞ψ◦
¯N:∞(x)i⊤
Γ∗ΣΨ,NΓ∗Exh
ψ◦
¯N:¯N(x)β⊤
¯N:∞ψ◦
¯N:∞(x)i
≲EβExh
ψ◦
¯N:¯N(x)β⊤
¯N:∞ψ◦
¯N:∞(x)i2
≲Eβ
ψ◦
¯N:∞(x)⊤β¯N:∞β⊤
¯N:∞ψ◦
¯N:∞(x)
≲N−2s.
And for (17), we have that
σ2
nE
ψ◦
¯N:¯N(˜x)⊤Γ∗ψ◦
¯N:¯N(x)2
18=σ2
nTrh
Γ∗E˜xh
ψ◦
¯N:¯N(˜x)ψ◦
¯N:¯N(˜x)⊤i
Γ∗Exh
ψ◦
¯N:¯N(x)ψ◦
¯N:¯N(x)⊤ii
=σ2
nTr
Γ∗ΣΨ,NΓ∗
|{z}
positive definiteΣΨ,N
≤σ2
nTr
Γ∗ΣΨ,NΓ∗
ΣΨ,N+1
nΣ−1
¯β,N
=σ2
nTr [Γ∗ΣΨ,N]≤σ2N
n.
Therefore, we obtain the following bound:
E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗Ψ◦
¯N:¯N(Ψ◦⊤
¯N:∞β¯N:∞+ξ)
n!2
≲N2rN−2s
n+N−2s+σ2N
n.
A.4 Bounding Terms (10)-(12)
For (10), we use the Cauchy-Schwarz inequality and Assumption 3 to bound
E
 
(ϕ∗(˜x)−ψ◦
¯N:¯N(˜x))⊤Γ∗(Φ∗−Ψ◦
¯N:¯N)y
n!2

≤1
nnX
i=1E
(ϕ∗(˜x)−ψ◦
¯N:¯N(˜x))⊤Γ∗(ϕ∗(xi)−ψ◦
¯N:¯N(xi))yi2
≤1
nnX
i=1Eh
∥ϕ∗(˜x)−ψ◦
¯N:¯N(˜x)∥2∥Γ∗(ϕ∗(xi)−ψ◦
¯N:¯N(xi))∥2y2
ii
≤ ∥Γ∗∥2
op
sup
x∈suppPX∥ϕ∗(x)−ψ◦
¯N:¯N(x)∥22
E
y2
≤ ∥Γ∗∥2
op
Nmax
¯N≤j≤¯N∥ϕ∗
j−ψ◦
j∥2
L∞(PX)2
E
y2
≲N2δ4
N(B2+σ2).
Similarly for (11) and (12), we have
E
 
(ϕ∗(˜x)−ψ◦
¯N:¯N(˜x))⊤Γ∗Ψ◦
¯N:¯Ny
n!2

≤1
nnX
i=1E
(ϕ∗(˜x)−ψ◦
¯N:¯N(˜x))⊤Γ∗ψ◦
¯N:¯N(xi)yi2
≤1
nnX
i=1Eh
∥ϕ∗(˜x)−ψ◦
¯N:¯N(˜x)∥2∥Γ∗ψ◦
¯N:¯N(xi)∥2y2
ii
≤ sup
x∈suppPX∥ϕ∗(x)−ψ◦
¯N:¯N(x)∥2∥Γ∗∥2
op sup
x∈suppPX∥ψ◦
¯N:¯N(x)∥2E
y2
≲Nδ2
N·N2r(B2+σ2) =N2r+1δ2
N(B2+σ2)
and
E
 
ψ◦
¯N:¯N(˜x)⊤Γ∗(Φ∗−Ψ◦
¯N:¯N)y
n!2

≤1
nnX
i=1E
ψ◦
¯N:¯N(˜x)⊤Γ∗(ϕ∗(xi)−ψ◦
¯N:¯N(xi))yi2
19≤1
nnX
i=1Eh
∥ψ◦
¯N:¯N(˜x)∥2∥Γ∗(ϕ∗(xi)−ψ◦
¯N:¯N(xi))∥2y2
ii
≤ sup
x∈suppPX∥ψ◦
¯N:¯N(x)∥2∥Γ∗∥2
op sup
x∈suppPX∥(ϕ∗(x)−ψ◦
¯N:¯N(x))∥2E
y2
≲N2r+1δ2
N(B2+σ2).
This concludes the proof of Proposition 3.2.
A.5 Proof of Lemma A.1
We will make use of the following concentration bound and its corollary, proved at the end of this
subsection.
Theorem A.2 (matrix Bernstein inequality) .LetS1,···,Sn∈RN×Nbe independent random
matrices such that E[Si] = 0 and∥Si∥op≤Lalmost surely for all i. Define Z=Pn
i=1Snand the
matrix variance statistic v(Z)as
v(Z) = max
∥E[ZZ⊤]∥op,∥E[Z⊤Z]∥op	
= maxnPn
i=1E[SiS⊤
i]
op,Pn
i=1E[S⊤
iSi]
opo
.
Then it holds for all t >0that
Pr (∥Z∥op≥t)≤2Nexp
−t2
2v(Z) +2
3Lt
.
Proof. See Tropp (2015), Theorem 6.1.1.
Corollary A.3. For matrices S1,···,Snsatisfying the conditions of Theorem A.2, it holds that
E1
n2∥Z∥2
op
≤4v(Z)
n2(1 + log 2 N) +16L2
9n2(2 + 2 log 2 N+ (log 2 N)2).
We will apply Corollary A.3 to the matrices
Si:= Σ−1/2
Ψ,Nψ◦
¯N:¯N(xi)ψ◦
¯N:¯N(xi)⊤Σ−1/2
Ψ,N−IN.
It is straightforward to verify that
E[Si] = Σ−1/2
Ψ,NΣΨ,NΣ−1/2
Ψ,N−IN= 0
and
∥Si∥op≲∥ψ◦
¯N:¯N(xi)ψ◦
¯N:¯N(xi)⊤∥op+ 1 =¯NX
j=¯Nψ◦
j(xi)2+ 1≲N2r
almost surely by Assumption 1.
Next, we evaluate the matrix variance statistic. Since each Siis symmetric,
v(Z) =nX
i=1E[SiS⊤
i]
op
=nX
i=1
EXh
Σ−1/2
Ψ,Nψ◦
¯N:¯N(xi)ψ◦
¯N:¯N(xi)⊤Σ−1
Ψ,Nψ◦
¯N:¯N(xi)ψ◦
¯N:¯N(xi)⊤Σ−1/2
Ψ,Ni
−2EXh
Σ−1/2
Ψ,Nψ◦
¯N:¯N(xi)ψ◦
¯N:¯N(xi)⊤Σ−1/2
Ψ,Ni
+IN
op
=nX
i=1EXh
Σ−1/2
Ψ,Nψ◦
¯N:¯N(xi)ψ◦
¯N:¯N(xi)⊤Σ−1
Ψ,Nψ◦
¯N:¯N(xi)ψ◦
¯N:¯N(xi)⊤Σ−1/2
Ψ,Ni
−nIN
op
≤n+nExh
Σ−1/2
Ψ,Nψ◦
¯N:¯N(x)ψ◦
¯N:¯N(x)⊤Σ−1
Ψ,Nψ◦
¯N:¯N(x)ψ◦
¯N:¯N(x)⊤Σ−1/2
Ψ,Ni
op
20for a single sample x∼ PX. The second term is further bounded asExh
Σ−1/2
Ψ,Nψ◦
¯N:¯N(x)ψ◦
¯N:¯N(x)⊤Σ−1
Ψ,Nψ◦
¯N:¯N(x)ψ◦
¯N:¯N(x)⊤Σ−1/2
Ψ,Ni
op
≤Exh
Σ−1/2
Ψ,Nψ◦
¯N:¯N(x)ψ◦
¯N:¯N(x)⊤Σ−1/2
Ψ,Ni
op·ψ◦⊤
¯N:¯NΣ−1
Ψ,Nψ◦
¯N:¯N
L∞(PX)
≲∥IN∥op·¯NX
j=¯N(ψ◦
j)2
L∞(PX)
≲N2r,
again by Assumption 1.
Hence we may apply Corollary A.3 with v(Z)≲nN2r,L≲N2rto conclude:
EX
Σ−1/2
Ψ,NΨ◦
¯N:¯NΨ◦⊤
¯N:¯N
nΣ−1/2
Ψ,N−IN2
op
=E"1
nnX
i=1Si2
op#
≲N2r
nlogN+N4r
n2log2N.
Proof of Corollary A.3. From Theorem A.2 and with the change of variables λ=t2/n2, we have
Pr1
n2∥Z∥2
op≥λ
≤2Nexp
−n2λ
2v(Z) +2
3Ln√
λ
.
Since the probability is also bounded above by 1, it follows that
Pr1
n2∥Z∥2
op≥λ
≤1∧2Nexp
−n2λ
2(2v(Z)∨2
3Ln√
λ)
≤1∧2Nexp
−n2λ
4v(Z)
+ 1∧2Nexp
−3n√
λ
4L
,
and the expectation can be controlled as
E1
n2∥Z∥2
op
=Z∞
0Pr1
n2∥Z∥2
op≥λ
dλ
≤Z∞
01∧2Nexp
−n2λ
4v(Z)
dλ+Z∞
01∧2Nexp
−3n√
λ
4L
dλ.
For the first integral, we truncate at λ1:=4v(Z)
n2log 2Nso that
Z∞
01∧2Nexp
−n2λ
4v(Z)
dλ=λ1+Z∞
λ12Nexp
−n2λ
4v(Z)
dλ
=4v(Z)
n2log 2N+4v(Z)
n2.
For the second integral, we truncate at λ2:= 4L
3nlog 2N2so that
Z∞
01∧2Nexp
−3n√
λ
4L
dλ
=λ2+Z∞
λ22Nexp
−3n√
λ
4L
dλ
=λ2−16LN
3n√
λ+4L
3n
exp
−3n√
λ
4L∞
λ=λ2
=4L
3nlog 2N2
+8L
3n4L
3nlog 2N+4L
3n
.
Adding up, we conclude that
E1
n2∥Z∥2
op
≤4v(Z)
n2(1 + log 2 N) +16L2
9n2(2 + 2 log 2 N+ (log 2 N)2)
as was to be shown.
21B Proofs on Metric Entropy
B.1 Modified Proof of Theorem 3.1
For a full proof of the original statement, we refer the reader to Section B.1 of Hayakawa and Suzuki
(2020), which corrects some technical flaws in the original proof. Here we only outline the necessary
modification to incorporate the bounded noise setting.
Denote an ϵ-cover of the model space by f1,···, fM. The only step which relies on the normality of
noise ξ1:nis a concentration result for the random variables
εj:=Pn
i=1ξi(fj(xi)−f◦(xi))

(Pn
i=1(fj(xi)−f◦(xi))21/2,1≤j≤M,
where it is shown via the normality of εjthat
E
max
1≤j≤Mε2
j
≤4σ2(logM+ 1).
We will instead rely on Hoeffding’s inequality. By writing εj=w⊤
jξ1:n=Pn
i=1wj,iξiwhere
wj,i=fj(xi)−f◦(xi)

(Pn
i=1(fj(xi)−f◦(xi))21/2,
since|wj,iξi| ≤σ|wj,i|a.s. it follows that
Pr (|εj| ≥u)≤2 exp
−2u2
Pn
i=1(2σ|wj,i|)2
= 2 exp
−u2
2σ2
.
for all u >0. Then the squared-exponential moment of each εjis bounded as
E
exp(tε2
j)
= 1 +Z∞
1Pr 
exp(tε2
j)≥λ
dλ
≤1 +Z∞
12 exp
−logλ
2σ2t
dλ
≤1 + 2Z∞
1λ−1
2σ2tdλ≤3
by setting t=1
4σ2. Hence via Jensen’s inequality we obtain
exp
tE
max
1≤j≤Mε2
j
≤E
max
1≤j≤Mexp(tε2
j)
≤MX
j=1E
exp(tε2
j)
≤3M
and thus
E
max
1≤j≤Mε2
j
≤4σ2log 3M,
retrieving the original result up to a constant.
B.2 Proof of Lemma 3.3
Let us take two functions fΘ1, fΘ2∈ TNforΘi= (Γ i, ϕi),i= 1,2separated as
∥Γ1−Γ2∥op≤δ1,max
1≤j≤N∥ϕ1,j−ϕ2,j∥L∞(PX)≤δ2.
Then it holds that
|fΘ1(X,y,˜x)−fΘ2(X,y,˜x)|
≤ |ˇfΘ1(X,y,˜x)−ˇfΘ2(X,y,˜x)|
=Γ1ϕ1(X)y
n, ϕ1(˜x)
−Γ2ϕ2(X)y
n, ϕ2(˜x)
22=1
nϕ1(˜x)⊤Γ1ϕ1(X)y−ϕ2(˜x)⊤Γ1ϕ1(X)y+ϕ2(˜x)⊤Γ1ϕ1(X)y
−ϕ2(˜x)⊤Γ2ϕ1(X)y+ϕ2(˜x)⊤Γ2ϕ1(X)y−ϕ2(˜x)⊤Γ2ϕ2(X)y
≤1
n∥ϕ1(˜x)−ϕ2(˜x)∥∥Γ1∥op∥ϕ1(X)y∥+1
n∥ϕ2(˜x)∥∥Γ1−Γ2∥op∥ϕ1(X)y∥
+1
n∥ϕ2(˜x)∥∥Γ2∥op∥(ϕ1(X)−ϕ2(X))y∥
≤√
Nδ2C2
n+B′
Nδ1
nnX
i=1∥ϕ1(xi)∥|yi|+B′
NC2
nnX
i=1∥ϕ1(xi)−ϕ2(xi)∥|yi|
≤(√
Nδ2C2+B′
Nδ1)B′
N(B+σ) +B′
NC2√
Nδ2(B+σ)
=B′2
N(B+σ)δ1+ 2B′
N(B+σ)C2√
Nδ2.
Hence to construct an ϵ-cover GTofTN, it suffices to exhibit a δ1-cover GSofSNand a δ2-cover
GFofFNfor
δ1=ϵ
2B′2
N(B+σ), δ 2=ϵ
4B′
N(B+σ)C2√
N
and set GT=GS×GF. For the metric entropy, this implies
V(TN,∥·∥L∞, ϵ)≤ V(SN,∥·∥op, δ1) +V(FN,∥·∥L∞, δ2).
We further bound the metric entropy of SNwith the following result, proved in Appendix B.3.
Lemma B.1. Forδ≤1
2it holds that V(SN,∥·∥op, δ)≲N2log1
δ.
Substituting into the above, we conclude that
V(TN,∥·∥L∞, ϵ)≲N2logB′2
N
ϵ+V 
FN,∥·∥L∞,ϵ
4B′
N(B+σ)C2√
N!
.
The choice of δ2is not important as long as the metric entropy of FNis at most polynomial in δ2.
B.3 Proof of Lemma B.1
LetΓ1,Γ2∈ SNand consider their diagonalizations
Γi=UiΛiU⊤
i,Λi= diag( λi,1,···, λi,N), i= 0,1,
where Ui∈ O N, the orthogonal group in dimension N, and 0≤λi,j≤C3for each 1≤j≤N.
Assuming
∥U1−U2∥op≤δ
4C3,|λ1,j−λ2,j| ≤δ
2∀j≤N,
it follows that
∥Γ1−Γ2∥op
=∥U1Λ1U⊤
1−U1Λ1U⊤
2+U1Λ1U⊤
2−U2Λ1U⊤
2+U2Λ1U⊤
2−U2Λ2U⊤
2∥op
≤2∥Λ1∥L∞∥U1−U2∥op+∥Λ1−Λ2∥L∞
≤2C3·δ
4C3+δ
2=δ.
Moreover, the covering number of ONin operator norm is given by the following result.
Theorem B.2 (Szarek (1981), Proposition 6) .There exist universal constants c1, c2>0such that
for all N∈Nandδ∈(0,2],
c1
δN(N−1)
2≤ N(ON,∥·∥op, δ)≤c2
δN(N−1)
2.
23Hence we obtain that
V(SN,∥·∥op, δ)≤ V
ON,∥·∥op,δ
4C3
+V
[0, C3]N,∥·∥L∞,δ
2
≤N(N−1)
2logc2
δ+Nlog2C3
δ
≲N2log1
δ.
Finally, we remark that if elements of the domain SNare not constrained to be symmetric, we can
alternatively consider the singular value decomposition and separately bound entropy of the two
rotation components, giving the same result up to constants.
C Details on Besov-type Spaces
C.1 Besov Space
C.1.1 Verification of Assumptions
We first give some background on wavelet decomposition. The decay rate s=α/d is intrinsic to
the Besov space as shown by the following result, which allows us to translate between functions
f∈Bα
p,q(X)and their B-spline coefficient sequences.
Lemma C.1 (DeVore and Popov (1988), Corollary 5.3) .Ifα > d/p andm > α + 1−1/p, a
function f∈Lp(X)is inBα
p,q(X)if and only if fcan be represented as
f=∞X
k=0X
ℓ∈Id
k˜βk,ℓωd
k,ℓ
such that the coefficients satisfy
∥˜β∥bαp,q:="∞X
k=0"
2k(α−d/p)X
ℓ∈Id
k|˜βk,ℓ|p1/p#q#1/q
<∞,
with appropriate modifications if p=∞orq=∞. Moreover, the two norms ∥˜β∥bαp,qand∥f∥Bαp,q
are equivalent.
In particular, this implies that for any f∈U(Bα
p,q(X))thep-norm average of ˜βk,ℓforℓ∈Id
kat
resolution kis bounded as
 
1
|Id
k|X
ℓ∈Id
k|˜βk,ℓ|p!1/p
≲(2−kd)1/p·2k(d/p−α)∥f∥Bαp,q≤2−kα,
and the coefficients βk,ℓ= 2−kd/2˜βk,ℓw.r.t. the scaled basis (2kd/2ωd
k,ℓ)k,ℓsatisfy
 
1
|Id
k|X
ℓ∈Id
k|βk,ℓ|p!1/p
≲(2kd)−α/d−1/2. (18)
Thus it is natural in a probabilistic sense to assume E[βp
k,ℓ]1/p≲(2kd)−α/d−1/2. This will be
the case if for instance we sample (βk,ℓ)ℓ∈Id
kuniformly from the p-norm ball (18). This matches
Assumption 4 up to the logarithmic factor and hence the rate is nearly tight in variance, even though
(18) only applies to the average over locations rather than each coefficient explicitly. See also the
discussion in Lemma 2 of Suzuki (2019).
24Assumption 1. We take mto be even for simplicity. The wavelet system (ωd
K,ℓ)ℓ∈Id
Kat each
resolution Kis linearly independent; for any g∈L2(X)that can be expressed as
g=X
ℓ∈Id
KβK,ℓ2Kd/2ωd
K,ℓ
we have the quasi-norm equivalence (D ˜ung, 2011b, 2.15)
∥g∥2≍2−Kd/2X
ℓ∈Id
K2Kdβ2
K,ℓ1/2
=∥(βK,ℓ)ℓ∈Id
K∥2
which implies that the covariance matrix Ex∼Unif([0 ,1]d)[ψ◦
¯N:¯N(x)ψ◦
¯N:¯N(x)⊤]is bounded above and
below. Since we assume PXhas Lebesgue density bounded above and below, it follows that ΣΨ,Nis
uniformly bounded above and below for all K≥0.
In contrast, any B-spline at a lower resolution k < K can be exactly expressed as a linear combination
of elements of (ωd
K,ℓ)ℓ∈Id
Kby repeatedly applying the following relation.
Lemma C.2 (refinement equation) .For even mandr= (r1,···, rd)⊤,1= (1,···,1)⊤∈Rdit
holds that
ωd
k,ℓ=mX
r1,···,rd=02(−m+1)ddY
i=1m
ri
·ωd
k+1,2ℓ+r−m
21. (19)
Proof. The relation for one-dimensional wavelets is given in equation (2.21) of D ˜ung (2011b),
ιm(x) = 2−m+1mX
r=0m
r
ιm
2x−r+m
2
,
from which it follows that
ωd
k,ℓ(x) =dY
i=1ιm(2kxi−ℓi)
=mX
r1,···,rd=02(−m+1)ddY
i=1m
ri
ιm
2k+1xi−2ℓi−ri+m
2
=mX
r1,···,rd=02(−m+1)ddY
i=1m
ri
·ωd
k+1,2ℓ+r−m
21(x)
as was to be shown.
Therefore we select all B-splines at a fixed resolution Kto approximate the target tasks,
N=Id
K= (m+ 1 + 2K)d≍2Kd
and
¯N=K−1X
k=0Id
k+ 1≍N, ¯N=KX
k=0Id
k≍N.
It is straightforward to see that 0≤ωd
k,ℓ(x)≤1for all x∈ X and moreover the B-splines (extended
to all ℓ∈Zd) form a partition of unity of Rdat all resolutions:
X
ℓ∈Zdωd
k,ℓ(x)≡1,∀x∈Rd,∀k≥0.
Then for all x∈ X we have the bound
¯NX
j=¯Nψ◦
j(x)2=X
ℓ∈Id
K2Kdωd
K,ℓ(x)2≤2KdX
ℓ∈Zdωd
K,ℓ(x) = 2Kd≲N,
and hence (2) holds with r= 1/2.
25Assumption 2. For any β∈suppPβwe have that
∥F◦
β∥L∞(PX)≤∞X
k=0X
ℓ∈Id
kβk,ℓ·2kd/2ωd
k,ℓ
L∞(PX)
≤∞X
k=02kd/2max
ℓ∈Id
k|βk,ℓ| ·X
ℓ∈Id
kωd
k,ℓ
L∞(X)
≤∞X
k=02kd(1/2+1/p) 
1
|Id
k|X
ℓ∈Id
k|βk,ℓ|p!1/pX
ℓ∈Id
kωd
k,ℓ
L∞(X)
≲∞X
k=02kd(1/2+1/p)·(2kd)−α/d−1/2∥F◦
β∥Bαp,q
≲(1−2d/p−α)−1=:B.
Furthermore, the convergence rate of the truncated approximation F◦
β,¯Nis determined by the decay
rate of βin Lemma C.1 as follows (it does not matter whether we bound F◦
β,¯NorF◦
β,Nsince ¯N≍N).
We consider a truncation of all resolutions lower than Kso that ¯N, N≍2Kd. Then it holds that
∥F◦
β−F◦
β,¯N∥2
L2(PX)=Z
X∞X
j=¯N+1βjψ◦
j(x)2
PX(dx) =∞X
j,k=¯N+1βjβkEx[ψ◦
j(x)ψ◦
k(x)]
≤lim
M→∞C2∥β¯N:M∥2=C2∞X
k=KX
ℓ∈Id
kβ2
k,ℓ
≲∞X
k=K|Id
k|1−2/p X
ℓ∈Id
k|βk,ℓ|p!2/p
≲∞X
k=K2kd·(2kd)−2α/d−1
=2−2αK
1−2−2α≍N−2α/d,
where for the last two inequalities we have used the inequality ∥z∥2≤D1/2−1/p∥z∥pforz∈RD
andp≥2in conjunction with (18). Thus our choice of s=α/d is justified. Under this choice, (5)
directly implies (4) as
Eβ[β2
K,ℓ]≲2−Kd(2s+1)K−2≍¯N−2s−1(log¯N)−2
holds for the basis elements at each resolution K, that is for those numbered between¯Nand¯N.
Remark C.3. When 1≤p <2, the truncation up to ¯Ndoes not suffice to achieve the N−2α/d
approximation rate, and basis elements must be judiciously selected from a wider resolution range.
More concretely, a size Nsubset of all wavelets up to resolution K′=⌈K(1 + ν−1)⌉where
ν=pα/2d−1/2>0must be used (Suzuki and Nitanda, 2021, Lemma 2). Hence the exponent is a
factor of 1 +ν−1worse w.r.t. N′≍2K′d, leading to the inevitable suboptimal rate.
To show boundedness of Tr(Σ ¯β,N), we analyze the composition of the aggregated coefficients ¯β
using the following result.
Corollary C.4. For any 0≤k < k′there exists constants γk,k′,ℓ,ℓ′≥0forℓ∈Id
k,ℓ′∈Id
k′such
that X
ℓ∈Id
kβk,ℓ2kd/2ωd
k,ℓ=X
ℓ′∈Id
k′¯βk′,ℓ′2k′d/2ωd
k′,ℓ′,¯βk′,ℓ′=X
ℓ∈Id
kγk,k′,ℓ,ℓ′βk,ℓ
holds for all (βk,ℓ)ℓ∈Id
k. Moreover, it holds that
X
ℓ∈Id
kγk,k′,ℓ,ℓ′≤2(k−k′)d/2,X
ℓ′∈Id
k′γk,k′,ℓ,ℓ′≤2(k′−k)d/2.
26The statement follows directly from the more general Proposition C.10, stated and proved in Appendix
C.3 below, by restricting to wavelets with uniform resolution across dimensions. Using Corollary
C.4, we can refine each lower resolution component of F◦
βto resolution K:
F◦
β,¯N=¯NX
j=1βjψ◦
j=KX
k=0X
ℓ∈Id
kβk,ℓ2kd/2ωd
k,ℓ=KX
k=0X
ℓ′∈Id
KX
ℓ∈Id
kγk,K,ℓ,ℓ′βk,ℓ2Kd/2ωd
K,ℓ′.
Thus each aggregated coefficient, indexed here by ℓ∈Id
K, can be expressed as
¯βK,ℓ=KX
k=0X
ℓ∈Id
kγk,K,ℓ,ℓ′βk,ℓ.
Hence it follows that
Eβ[¯β2
K,ℓ] =KX
k=0X
ℓ∈Id
kγ2
k,K,ℓ,ℓ′Eβ[β2
k,ℓ]≲KX
k=0 X
ℓ∈Id
kγk,K,ℓ,ℓ′!2
2−k(2α+d)k−2
≤KX
k=02(k−K)d·2−k(2α+d)k−2
≲2−Kd·KX
k=02−2kαk−2≍N−1,
from which we conclude that Tr(Σ ¯β,N) =P
ℓ∈Id
KEβ[¯β2
K,ℓ]≲N·N−1is uniformly bounded.
Finally for the verification of Assumption 3, see Appendix C.1.2.
C.1.2 Proof of Lemma 4.4
We use the following result to approximate each wavelet ωd
K,ℓat resolution Kwith the class (6). The
proof, in turn, relies on the construction by Yarotsky (2016) of DNNs which efficiently approximates
the multiplication operation.
Lemma C.5 (Suzuki (2019), Lemma 1) .For all δ > 0, there exists a ReLU neural network
˜ω∈ F DNN(L, W, S, M )with
L= 3 + 2l
log2
3d∨m(1 +dm−1/2(2e)m+1)δ−1
+ 5m
⌈log2(d∨m)⌉,
W=W0= 6dm(m+ 2) + 2 d, S =LW2, M = 2(m+ 1)m
satisfying supp ˜ ω⊆[0, m+ 1]dand∥ωd
0,0−˜ω∥L∞(X)≤δ.
Here, δis also dependent on N.
Now consider Nidentical copies of ˜ωin parallel, where each module is preceded by the scaling
(xi)d
i=17→(2Kxi−ℓi)d
i=1forℓ∈Id
Kand whose output is scaled by 2Kd/2. In particular, these
operations can be implemented by K≲logNconsecutive additional layers with norm bounded by a
constant. Hence each module ϕ∗
¯N,···, ϕ∗
¯Napproximates the basis 2Kd/2ωd
K,ℓwith2Kd/2δ≲√
Nδ
accuracy, and substituting δN=√
Nδgives that
∥ψ◦
j−ϕ∗
j∥L∞(PX)≤δN,¯N≤j≤¯N,
withL≲logδ−1+ log N≲logδ−1
N+ log N. Note that the sparsity Sis only multiplied by a factor
ofNsince different modules do not share any connections. Moreover the target basis has 2-norm
bounded as
∥ψ◦
¯N:¯N(x)∥2≲ X
ℓ∈Id
K2KdωK,ℓ(x)2!1/2
≤ 
2KdX
j∈ZdωK,ℓ(x)!1/2
≍√
N,
where we have again used the sparsity of ωd
k,ℓat each resolution. Hence we may clip the magnitude
of the vector output ϕbyB′
Nand the approximation guarantee remains unchanged.
To bound the covering number of FN, we directly apply the following result.
27Lemma C.6 (Suzuki (2019), Lemma 3) .The covering number of FDNNis bounded as
N(FDNN(L, W, S, M ),∥·∥L∞, ϵ)≤L(M∨1)L−1(W+ 1)2L
ϵS
.
Since clipping the magnitude of the outputs does not increase the covering number, we conclude:
V(FN,∥·∥L∞, ϵ)≤N· V(FDNN(L, W, S, M ),∥·∥L∞, ϵ)
≤SNlogL+SLN logM+ 2SLN log(W+ 1) + SNlog1
ϵ
≲NlogN
δN+Nlog1
ϵ.
C.1.3 Proof of Theorem 4.5
By Lemma 3.3 and Lemma 4.4, the metric entropy of TNis bounded as
V(TN,∥·∥L∞, ϵ)≲N2logN
ϵ+NlogN2
δNϵ.
Combining with Theorem 3.1 and Proposition 3.2 with r= 1/2ands=α/d gives
¯R(bΘ)≲N
nlogN+N2
n2log2N+N−2α/d+N2δ2
N+1
T
N2logN
ϵ+NlogN2
δNϵ
+ϵ.
Substituting δN≍N−1−α/dandϵ≍N−2α/dyields the desired bound.
C.2 Anisotropic Besov Space
C.2.1 Definitions and Results
For1≤p, q≤ ∞ , directional smoothness α= (α1,···, αd)∈Rd
>0andr= max i≤d⌊αi⌋+ 1, we
define ∥·∥Bαp,q=∥·∥Lp+|·|Bαp,qwhere
|f|Bαp,q:=( P∞
k=0
2kwr,p(f,(2−k/α1,···,2−k/αd))q1/qq <∞
supk≥02kwr,p(f,(2−k/α1,···,2−k/αd)) q=∞.
The anisotropic Besov space is defined as
Bα
p,q(X) ={f∈Lp(X)| ∥f∥Bαp,q<∞}.
The definition reduces to the usual Besov space if α1=···=αd; see Vybíral (2006); Triebel (2011)
for details. We also write α= max iαi,α= min iαiand the harmonic mean smoothness as
eα:=dX
i=1α−1
i−1
.
For the anisotropic Besov space, we need to redefine the wavelet basis so that the sensitivity to
resolution k∈Z≥0differs for each component depending on α. Define the quantities
∥k∥α/α:=dX
i=1⌊kα/αi⌋, Id,α
k:=dY
i=1{−m,−m+ 1,···,2⌊kα/αi⌋} ⊂Zd.
We then set for each k≥0andℓ∈Id,α
k
ωd,α
k,ℓ(x) :=ωd
(⌊kα/α1⌋,···,⌊kα/αd⌋),ℓ(x) =dY
i=1ιm(2⌊kα/αi⌋xi−ℓi),
28and take the scaled basis
{ψ◦
j|j∈N}={2∥k∥α/α/2ωd,α
k,ℓ|k∈Z≥0, ℓ∈Id,α
k}
with the natural hierarchy induced by k.
The minimax optimal rate for the anisotropic Besov space is equal to n−2eα
2eα+1(Suzuki and Nitanda,
2021, Theorem 4). Our result for in-context learning is as follows.
Theorem C.7 (minimax optimality of ICL in anisotropic Besov space) .Letα∈Rd
>0witheα >1/p
andF◦=U(Bα
p,q(X)). Suppose that PXhas positive Lebesgue density ρXbounded above and
below on X. Also suppose that all coefficients are independent and
Eβ[βk,ℓ] = 0,Eβ[β2
k,ℓ]≲2−kα(2+1/eα)k−2,∀k≥0, ℓ∈Id,α
k. (20)
Then for n≳NlogNwe have
¯R(bΘ)≲N−2eα+NlogN
n+N2logN
T.
Hence if T≳nNandN≍n1
2eα+1, in-context learning achieves the rate n−2eα
2eα+1lognwhich is
minimax optimal up to a polylog factor.
C.2.2 Proof of Theorem C.7
The overall approach is similar to Appendix C.1. The decay rate of functions in the anisotropic Besov
space is characterized by the following result which extends Lemma C.1.
Lemma C.8 (Suzuki and Nitanda (2021), Lemma 2) .Ifeα >1/pandm > α+ 1−1/p, a function
f∈Lp(X)is inMBα
p,q(X)if and only if fcan be represented as
f=X
k∈Zd
≥0X
ℓ∈Id,α
k˜βk,ℓωd,α
k,ℓ(x)
such that the coefficients satisfy
∥˜β∥bαp,q:="∞X
k=0"
2kα−∥k∥α/α/pX
ℓ∈Id,α
k|˜βk,ℓ|p1/p#q#1/q
≲∥f∥Bαp,q.
Moreover, the two norms ∥˜β∥bαp,qand∥f∥Bαp,qare equivalent.
We again select all B-splines (ωd,α
K,ℓ)ℓ∈Id
Kat each resolution Kto approximate the target functions. By
repeatedly applying the refinement equation for one-dimensional wavelets as many times as needed
to each dimension separately, we may express any B-spline at a lower resolution k < K as a linear
combination of (ωd,α
K,ℓ)ℓ∈Id
Ksimilarly to Lemma C.2. See Proposition C.10 for details. We thus have
N=|Id,α
K|=dY
i=1(m+ 1 + 2⌊Kα/αi⌋)≍2∥K∥α/α.
Since∥k∥α/α=kα/eα+Ok(1)always holds, it also follows that
¯N=KX
k=0|Id,α
k|+ 1≍KX
k=02∥k∥α/α≍KX
k=0(2α/eα)k≍2Kα/eα≍N
and similarly¯N≍N. Therefore,
¯NX
j=¯Nψ◦
j(x)2≤2∥K∥α/αX
ℓ∈Id,α
kωd,α
K,ℓ(x)2≤2∥K∥α/α≍N
29and the scaled coefficients decay in average as
 
1
|Id,α
k|X
ℓ∈Id,α
k|βk,ℓ|p!1/p
≲ dY
i=12⌊kα/αi⌋!−1/p
2−∥k∥α/α/2 X
ℓ∈Id,α
k|˜βk,ℓ|p!1/p
≲2−∥k∥α/α/2−kα∥f∥Bα
p,q
≍N−(eα+1/2)∥f∥Bαp,q.
For Assumption 2, we can check that
∥F◦
β∥L∞(PX)≤∞X
k=0X
ℓ∈Id,α
kβk,ℓ·2∥k∥α/α/2ωd,α
k,ℓ
L∞(PX)
≤∞X
k=02(1/2+1/p)∥k∥α/α 
1
|Id,α
k|X
ℓ∈Id,α
k|βk,ℓ|p!1/p
≲∞X
k=02(1/2+1/p)∥k∥α/α·2−∥k∥α/α/2−kα
≲
1−2α/eα(1/p−eα)−1
=:B
and for a resolution cutoff K > 0,¯N≍2∥K∥α/αthe truncation error satisfies
∥F◦
β−F◦
β,¯N∥2
L2(PX)≲∞X
k=K+1X
ℓ∈Id,α
kβ2
k,ℓ≲∞X
k=K+1|Id,α
k|1−2/p X
ℓ∈Id,α
k|βk,ℓ|p!2/p
≲∞X
k=K+12∥k∥α/α·2−∥k∥α/α−2kα≍2−2Kα≍N−2eα.
Thus we may set r= 1/2, s=eαand take the variance decay rate (20) as
Eβ[β2
k,ℓ]≲2−∥k∥α/α(2eα+1)k−2≍2−kα(2+1/eα))k−2.
For boundedness of Tr(Σ ¯β,N), we use the following result which is also obtained from Proposition
C.10 by considering resolution vectors (⌊kα/α1⌋,···,⌊kα/αd⌋)and(⌊k′α/α1⌋,···,⌊k′α/αd⌋).
Corollary C.9. For any 0≤k < k′there exists constants γk,k′,ℓ,ℓ′≥0forℓ∈Id,α
k,ℓ′∈Id,α
k′such
thatX
ℓ∈Id,α
kβk,ℓ2∥k∥α/α/2ωd,α
k,ℓ=X
ℓ′∈Id,α
k′¯βk′,ℓ′2∥k′∥α/α/2ωd,α
k′,ℓ′,¯βk′,ℓ′=X
ℓ∈Id,α
kγk,k′,ℓ,ℓ′βk,ℓ
holds for all (βk,ℓ)ℓ∈Id,α
k. Moreover, it holds that
X
ℓ∈Id,α
kγk,k′,ℓ,ℓ′≤2(∥k∥α/α−∥k′∥α/α)/2,X
ℓ′∈Id,α
k′γk,k′,ℓ,ℓ′≤2(∥k′∥α/α−∥k∥α/α)/2.
We apply Corollary C.9 to refine all components of F◦
β,¯Nto resolution K:
F◦
β,¯N=KX
k=0X
ℓ∈Id,α
kβk,ℓ2∥k∥α/α/2ωd,α
k,ℓ=KX
k=0X
ℓ′∈Id,α
KX
ℓ∈Id,α
kγk,K,ℓ,ℓ′βk,ℓ2∥K∥α/α/2ωd,α
K,ℓ′.
Hence it follows that
Eβ[¯β2
K,ℓ] =KX
k=0X
ℓ∈Id,α
kγ2
k,K,ℓ,ℓ′Eβ[β2
k,ℓ]≲KX
k=0 X
ℓ∈Id,α
kγk,K,ℓ,ℓ′!2
2−kα(2+1/eα)k−2
30≤KX
k=02∥k∥α/α−∥K∥α/α·2−kα(2+1/eα)k−2
≲2−∥K∥α/α·KX
k=02−2kαk−2≍N−1,
and we again conclude that Tr(Σ ¯β,N)is uniformly bounded.
The rest of the proof proceeds similarly to the ordinary Besov space.
C.3 Wavelet Refinement
In this subsection, we present and prove an auxiliary result concerning the refinement of B-spline
wavelets and the recurrence relations satisfied by their coefficient sequences.
Proposition C.10. For any k, k′∈Zd
≥0such that k′−k∈Zd
≥0there exists constants γk,k′,ℓ,ℓ′≥0
forℓ∈Id
k,ℓ′∈Id
k′such that
X
ℓ∈Id
kβk,ℓ2∥k∥1/2ωd
k,ℓ=X
ℓ′∈Id
k′¯βk′,ℓ′2∥k′∥1/2ωd
k′,ℓ′,¯βk′,ℓ′=X
ℓ∈Id
kγk,k′,ℓ,ℓ′βk,ℓ (21)
holds for all (βk,ℓ)ℓ∈Id
k. Moreover, it holds that
X
ℓ∈Id
kγk,k′,ℓ,ℓ′≤2−∥k′−k∥1/2,X
ℓ′∈Id
k′γk,k′,ℓ,ℓ′≤2∥k′−k∥1/2.
Proof. We proceed by induction on ∥k′−k∥1. When k′=k+ejfor some 1≤j≤d, we can refine
eachωd
k,ℓusing equation (2.21) of D ˜ung (2011b) as
ωd
k,ℓ(x) =dY
i=1ιm(2kixi−ℓi)
= 2−m+1Y
i̸=jιm(2kixi−ℓi)mX
r=0m
r
ιm
2kj+1xj−2ℓj−r+m
2
= 2−m+1mX
r=0m
r
ωd
k+ej,ℓ+(ℓj+r−m
2)ej(x).
Since ℓ+ (ℓj+r−m
2)ejmatches a given location vector ℓ′∈Id
k+ejif and only if ℓi=ℓ′
i(i̸=j)
andℓ′
j= 2ℓj+r−m
2, comparing coefficients in (21) yields
γk,k+ej,ℓ,ℓ′= 2−m+1/21{ℓi=ℓ′
i(i̸=j)}m
ℓ′
j−2ℓj+m
2
.
Here,1Adenotes the indicator function for condition A. It follows that γk,k+ej,ℓ,ℓ′≥0and
X
ℓ∈Id
kγk,k+ej,ℓ,ℓ′≤X
ℓj∈Z2−m+1/2m
ℓ′
j−2ℓj+m
2
≤2−1/2,
X
ℓ′∈Id
k+ejγk,k+ej,ℓ,ℓ′≤X
ℓ′
j∈Z2−m+1/2m
ℓ′
j−2ℓj+m
2
≤21/2,
by considering parities.
Now suppose the claim holds for a fixed difference ∥k′−k∥1. Applying the above derivation to
further refine resolution k′tok′′=k′+ejfor arbitrary jgives
X
ℓ∈Id
kβk,ℓ2∥k∥1/2ωd
k,ℓ=X
ℓ′∈Id
k′¯βk′,ℓ′2∥k′∥1/2ωd
k′,ℓ′=X
ℓ′′∈Id
k′+1¯¯βk′+1,ℓ′′2(∥k′∥1+1)/2ωd
k′+ej,ℓ′′
31where
¯¯βk′+ej,ℓ′′=X
ℓ′∈Id
k′2−m+1/21{ℓ′
i=ℓ′′
i(i̸=j)}m
ℓ′′
j−2ℓ′
j+m
2
¯βk′,ℓ′
=X
ℓ∈Id
kX
ℓ′∈Id
k′2−m+1/21{ℓ′
i=ℓ′′
i(i̸=j)}m
ℓ′′
j−2ℓ′
j+m
2
γk,k′,ℓ,ℓ′βk,ℓ.
Hence we obtain the recurrence relation
γk,k′+ej,ℓ,ℓ′′=X
ℓ′∈Id
k′2−m+1/21{ℓ′
i=ℓ′′
i(i̸=j)}m
ℓ′′
j−2ℓ′
j+m
2
γk,k′,ℓ,ℓ′,
from which we verify that γk,k′+ej,ℓ,ℓ′′≥0and
X
ℓ∈Id
kγk,k′+ej,ℓ,ℓ′′=X
ℓ′∈Id
k′2−m+1/21{ℓ′
i=ℓ′′
i(i̸=j)}m
ℓ′′
j−2ℓ′
j+m
2X
ℓ∈Id
kγk,k′,ℓ,ℓ′
≤2−m+1/2−∥k′−k∥1/2X
ℓ′
j∈Zm
ℓ′′
j−2ℓ′
j+m
2
= 2−(∥k′−k∥1+1)/2,
and furthermore
X
ℓ′′∈Id
k′+ejγk,k′+ej,ℓ,ℓ′′=X
ℓ′∈Id
k′X
ℓ′′∈Id
k′+ej2−m+1/21{ℓ′
i=ℓ′′
i(i̸=j)}m
ℓ′′
j−2ℓ′
j+m
2
γk,k′,ℓ,ℓ′
≤2−m+1/2X
ℓ′∈Id
k′X
ℓ′′
j∈Zm
ℓ′′
j−2ℓ′
j+m
2
γk,k′,ℓ,ℓ′
≤21/2X
ℓ′∈Id
k′γk,k′,ℓ,ℓ′≤2(∥k′−k∥1+1)/2.
This concludes the proof.
C.4 Proof of Corollary 4.8
In order to approximate arbitrary ψ◦
j∈U(Bτ
p,q(X)), we need the following construction instead of
Lemma C.5. Note that N′corresponds to the number of B-splines used to approximate the target
function and can be freely chosen to match the desired error, which however affects the covering
number of FN.
Lemma C.11 (Suzuki (2019), Proposition 1) .Setm∈N,m > τ + 2−1/pandν= (pτ−d)/2d.
For all N′∈Nsufficiently large and ϵ=N′−τ/d(logN′)−1, for any f◦∈U(Bτ
p,q(X))there exists
a ReLU network ˜f∈ F DNN(L, W, S, M )with
L= 3 + 2l
log2
3d∨m(1 +dm−1/2(2e)m+1)ϵ−1
+ 5m
⌈log2(d∨m)⌉,
W=N′W0, S = ((L−1)W2
0+ 1)N′, M =O(N′1/ν+1/d)
satisfying ∥f◦−˜f∥L∞(X)≤N′−τ/d.
Also note that from Assumption 1 it follows that ∥ψj∥L∞(PX)≤C∞N1/2. Setting N′≍δ−d/τ
N
and applying the covering number bound in Lemma C.6, after some algebra we obtain the following
counterpart to Lemma 4.4.
Corollary C.12. For any δN>0, Assumption 3 is satisfied by taking
FN={ΠB′
N◦ϕ|ϕ= (ϕj)N
j=1, ϕj∈ F DNN(L, W, S, M )}
where B′
N=C∞N1/2and
L=O(logδ−1
N), W =O(δ−d/τ
N), S =O(δ−d/τ
N logδ−1
N),logM=O(logδN).
32Also, the metric entropy of FNis bounded as
V(FN,∥·∥L∞, ϵ)≲Nδ−d/τ
N log1
δN
log1
ϵ+ log21
δN
.
Then by combining with Lemma 3.3 and Proposition 3.2 via Theorem 3.1, it follows that
V(TN,∥·∥L∞, ϵ)≲N2logN
ϵ+Nδ−d/τ
N log1
δN
logN
ϵ+ log21
δN
.
and
¯R(bΘ)≲N
nlogN+N2
n2log2N+N−2α/d+N2δ2
N
+N2
TlogN
ϵ+N
Tδ−d/τ
N log1
δN
logN
ϵ+ log21
δN
+ϵ.
Substituting δN≍N−1−α/dandϵ≍N−2α/dconcludes the desired bound.
D Details on Sequential Input
D.1 Definitions and Results
γ-smooth class. We first define the γ-smooth function class introduced by Okumoto and Suzuki
(2022). Let r∈Zd×∞
0 ands∈¯Nd×∞
0 , where ¯N=N∪{0}and the subscript 0indicates restriction
to the subset of elements with a finite number of nonzero components. Consider the orthonormal
basis (ψr)rofL2([0,1]d×∞)given as
ψr(x) =Y
i∈ZdY
j=1ψrij(xij), ψ rij(xij) =

√
2 cos(2 πrijxij)rij<0
1 rij= 0√
2 sin(2 πrijxij)rij>0.
The frequency scomponent δs(f)off∈L2([0,1]d×∞)is defined as
δs(f) :=X
⌊2sij−1⌋≤|rij|<2sij⟨f, ψr⟩ψr.
For a monotonically nondecreasing function γ:¯Nd×∞
0→Randp≥2, q≥1, theγ-smooth norm
and function class are defined as
∥f∥Fγ
p,q(PX):= X
s∈¯Nd×∞
02qγ(s)∥δs(f)∥q
p,PX!1/q
and
Fγ
p,q(PX) :=
f∈L2([0,1]d×∞)| ∥f∥Fγ
p,q(PX)<∞	
.
Theγ-smooth class over finite-dimensional input space [0,1]d×mis similarly defined.
In particular, we consider two specific cases of γfor the component-wise smoothness parameter
α∈Rd×∞
>0, for which we also define the corresponding degrees of smoothness α†∈R>0. Denote by
(˜αj)∞
j=1all components of αsorted by ascending magnitude.
• Mixed smoothness: γ(s) =⟨α, s⟩,α†= ˜α1= max i,jαij.
• Anisotropic smoothness: γ(s) = max i,jαijsij,α†= (P
i,jα−1
ij)−1.
Furthermore, the weak lη-norm of αis defined as ∥α∥wlη:= supjjη˜α−1
jforη >0.
33Piecewise γ-smooth class. The piecewise γ-smooth class is an extension of the γ-smooth class
allowing for arbitrary bounded permutations of the tokens of an input (Takakura and Suzuki, 2023).
For a threshold V∈Nand an index set Λ, let{Ωλ}λ∈Λbe a disjoint partition of suppPXand
{πλ}λ∈Λa set of bijections from [2V+ 1] to[−V:V]. Further define the permutation operator
Π : supp PX→Rd×(2V+1)as
Π(x) = (xπλ(1),···, xπλ(2V+1)),ifx∈Ωλ.
Then the piecewise γ-smooth function class is defined as
Pγ
p,q(PX) :=
g=f◦Π|f∈ Fγ
p,q(PX),∥g∥Pγ
p,q(PX)<∞	
,
where
∥g∥Pγ
p,q(PX):= X
s∈¯Nd×[−V:V]
02qγ(s)∥δs(f)◦Π∥q
p,PX!1/q
.
Next, we state the set of assumptions inherited from Takakura and Suzuki (2023). In particular, the
importance function makes precise a notion of relative importance between tokens which is preserved
by permutations.
Assumption 5 (smoothness and importance function) .1< q≤2and:
1.The smoothness parameter αsatisfies ∥α∥wlη≤1andαij= Ω(|i|η)for some η >1. For
mixed smoothness, we also require ˜α1<˜α2.
2.There exists a shift-equivariant map µ: supp PX→R∞such that µ0∈U(Fγ
∞,q),
∥µ0∥ ≤1andΩλ={x∈suppPX|µ(x)πλ(1)>···> µ(x)πλ(2V+1)}for all λ∈Λ.µ
is moreover well-separated, that is µ(x)πλ(v)−µ(x)πλ(v+1)≥Cµv−ϱforCµ, ϱ > 0.
We focus on parameter ranges 1< q≤2andη >1strictly for simplicity of presentation, but the
cases q= 1, q > 2andη >0can be handled with some more analysis. Note that η >1ensures
α†>0for anisotropic smoothness.
Additionally, the assumption pertaining to our ICL setup is stated as follows.
Assumption 6. Forr∈Zd×∞
0 the coefficients βrcorresponding to ψrare independent and satisfy
fors∈¯Nd×∞
0 such that the frequency component δs(f)contains the element ψr,
Eβ[βr] = 0,Eβ[β2
r]≲2−(2+1/α†)γ(s)γ(s)−2. (22)
AlsoΣΨ,N≍INholds, for example PXis bounded above and below with respect to the product
measure λd×∞onB([0,1]d×∞)of the uniform measure λonB([0,1]).
We then obtain the following result for ICL with transformers:
Theorem D.1 (minimax optimality of ICL for sequential input) .LetF◦={f∈U(Pγ
p,q(PX))|
∥f∥L∞(PX)≤B}for some B > 0where γcorresponds to mixed or anisotropic smoothness.
Suppose Assumptions 5 and 6 hold. Then for n≳NlogNwe have
¯R(bΘ)≲N−2α†+NlogN
n+N2∨(1+1/α†)polylog( N)
T.
Hence if T≳nN1∨1/α†andN≍n1
2α†+1, ICL achieves the rate n−2α†
2α†+1polylog( n).
D.2 Proof of Theorem D.1
Since the system (ψr)ris orthonormal, we may take¯N= 1,¯N=Nfollowing Remark 2.1. We
mainly utilize the following approximation and covering number bounds.
Theorem D.2 (Takakura and Suzuki (2023), Theorem 4.5) .For a function F◦∈U(Pγ
p,q(PX)),
∥F◦∥L∞(PX)≤Band any K > 0, there exists a transformer bF∈ F TF(J, U, D, H, L, W, S, M )
such that
∥bF0−F◦∥L2(PX)≲2−K,
34where
J≲K1/η,logU≲logK∨logV, D ≲K2(1+ϱ)/ηlogV, H ≲(logK)1/η,
L≲K2, W≲2K/α†K1/η, S≲2K/α†K2+2/η,logM≲K∨log log V.
Theorem D.3 (Takakura and Suzuki (2023), Theorem 5.3) .Forϵ >0andB≥1it holds that
logN(FTF(J, U, D, H, L, W, S, M ),∥·∥L∞, ϵ)≲J3L(S+HD2) logDHLWM
ϵ
.
To analyze the decay rate in the γ-smooth class, we approximate a function f∈L2([0,1]d×∞)by
the partial sum of its frequency components up to ‘resolution’ K, measured via the γfunction:
RK(f) :=X
γ(s)<Kδs(f).
The basis functions ψrare thus ordered primarily ordered by increasing γ(s).
Lemma D.4 (Okumoto and Suzuki (2022), Lemma 17) .For1≤q≤2it holds that
∥f−RK(f)∥L2(PX)≲2−K∥f∥Fγ
p,q(PX).
Note that if γ(s)< K thensij< K/a ij≲K/|i|ηfor all i, jfor both types of smoothness and so
∥s∥0≲dK1/η. In addition, the number of basis functions ψrused in the sum for δs(f)is exactly
2∥s∥1=Q
i,j2sij. Theorem D.3 of Takakura and Suzuki (2023) shows that the number of basis
elements used in the sum RK(f)satisfies
N≍X
γ(s)<K2∥s∥1≲2K/α†
for both mixed and anisotropic smoothness. Hence the N-term approximation error decays as N−α†
so that the choice s=α†leading to the assumed variance bound N−2α†−1≍2−(2+1/α†)Kin(22)
is justified. Moreover for large K,
X
γ(s)<KX
⌊2sij−1⌋≤|rij|<2sij∥ψr∥2
L∞(PX)≤X
γ(s)<K2∥s∥1(√
2∥s∥0)2≲2K/α†+O(K1/η)
since∥r∥0=∥s∥0, so that (2)of Assumption 1 is satisfied with r= 1/2. The second part of
Assumption 1 holds since (ψr)r∈Zd×∞
0is orthonormal w.r.t. λd×∞. Furthermore, the discussion thus
far immediately extends to the piecewise γ-smooth class for any partition {Ωλ}λ∈Λby composing
with the permutation operator Π.
We proceed to use Theorem D.2 to approximate each basis function ψr◦Πup to resolution K.
Moreover, we can see from the proof of Lemma 17 of Okumoto and Suzuki (2022) that we do not
need to account for the sup-norm scaling of ψrand thus it suffices to find the parameter K′∈Nsuch
that the approximation error 2−K′≍δN. Hence combining Theorems D.2 and D.3, we conclude that
V(FTF(J, U, D, H, L, W, S, M ),∥·∥L∞, ϵ)≲K′3/ηK′2·2K′/α†K′2+2/η·K′log1
ϵ
≲1
δN1/α†
polylog
N,1
δN
log1
ϵ
is sufficient to satisfy Assumption 3. Therefore, we can now apply our framework with B′
N≍Nto
obtain the bound
¯R(bΘ)≲N
nlogN+N2
n2log2N+N−2α†+N2δ2
N
+N2
TlogN
ϵ+1
Tδ−1/α†
N polylog
N,1
δN
log1
ϵ+ϵ.
Substituting δN≍N−1−α†andϵ≍N−2α†concludes the theorem.
35Figure 1: Architecture of the compared models. Each model contains two MLP components, all
attention layers are single-head and LayerNorm is not included. (a),(b) implement the simplified
reparametrization for attention, while all layers in (c) utilize the full embeddings. The input dimension
is 8 and all hidden layer and DNN output widths are 32. The query prediction is read off the last entry
of the output at the query position.
Figure 2: Training and test curves for the ICL pretraining objective. We use the Adam optimizer with
a learning rate of 0.02 for all layers. For the task class we take α= 1,p=q=∞,T=n= 512
and generate samples from random combinations of order 2 wavelets.
E Numerical Experiments
In this section, we connect our theoretical contributions to practical transformers by conducting
experiments verifying our results as well as justifying the simplified model setup and the empirical risk
minimization assumption. We implement and compare the following toy models: (a) the simplified
architecture studied in our paper; (b) the same model with linear attention replaced by softmax; and
(c) a full transformer with 2 stacked encoder layers. The number of feedforward layers, widths of
hidden layers, learning rate, etc. are set to equal for a fair comparison, see Figure 1 for details.
Figure 2 shows training (solid) and test loss curves (dashed) during pretraining. All 3 architectures
exhibit similar behavior and converge to near zero training loss, justifying the use of our simplified
model and also supporting the assumption that the empirical risk is minimized. Moreover, Figure
3 shows the converged losses over a wide range of N, n, T values. We verify that increasing N, n
Figure 3: Training and test losses of the three models after 50 epochs while varying (a) DNN width
N; (b) number of in-context samples n; (c) number of tasks T. For (a), the widths of all hidden layers
also vary with N. We take the median over 5 runs for robustness.
36leads to decreasing train and test error, corresponding to the approximation error of Theorem 3.1. We
also observe that increasing Ttends to improve the pretraining generalization gap up to a threshold,
confirming our theoretical analysis of task diversity. Again, this behavior is consistent across the
3 architectures. We note that in the overparametrized regime when the number of total parameters
≳nT, the trained model is likely not the empirical risk minimizer, which may also contribute to the
large error for large Nor small n, T.
F Proofs of Minimax Lower Bounds
F.1 Proof of Proposition 5.1
In this section, we develop our framework for obtaining minimax lower bounds in the ICL setup by
adapting the information-theoretic approach of Yang and Barron (1999).
Let{(ψ(j), β(j)
T+1)}M
j=1be aδn-packing of the class F◦with respect to the L2(X)-norm such that
∥β(j)⊤
T+1ψ(j)−β(j′)⊤
T+1ψ(j′)∥2
L2(X)≥δ2
n,1≤j < j′≤M,
where Mis the corresponding packing number. Then we have the following proposition as an
application of Fano’s inequality (Yang and Barron, 1999).
Proposition F.1. LetΘbe a random variable uniformly distributed over {(ψ(j), β(j)
T+1)}M
j=1. Then,
it holds that
inf
bfn:Dn,T→Rsup
f◦∈F◦EDn,T[∥bf−f◦∥2
L2(PX)]≥δ2
n
2
1−EX[IX(1:T+1)(Θ,y(1:T+1))] + log 2
logM
,
where IX(1:T+1)(Θ,y(1:T+1))is the mutual information between Θ,y(1:T+1)for given X(1:T+1).
The mutual information IX(1:T+1)(Θ,y(1:T+1))is formulated more concretely as
X
θ∈supp Θw(θ)Z
p(y(1:T+1)|θ,X(1:T+1)) logp(y(1:T+1)|θ,X(1:T+1))
pw(y(1:T+1)|X(1:T+1))
dy(1:T+1),
where p(y|θ,X)is the probability density of yconditioned on θ,Xandpwis the marginal distri-
bution of y(1:T+1)where w(·)≡1
Mis the probability mass function of Θ(i.e.,pw(·|X(1:T+1)) =P
θ∈supp Θw(θ)p(·|θ,X(1:T+1))). We let Py(t)|θ(andPy(1:t)|θ) be the distribution of y(t)condi-
tioned on θ,X(t)(andX(1:t)) respectively, and let
¯Py(1:T+1)=1
MMX
j=1Py(1:T+1)|θ(j)
be the marginal distribution of y(1:T+1)conditioned on X(1:T+1).
Next, we define the set {˜ψ(j)}Q1
j=1to be a εn,1-covering of FNw.r.t. the norm d(ψ, ψ′) :=p
Ex[∥ψ(x)−ψ′(x)∥2]withεn,1-covering number Q1, and{˜β(j)}Q2
j=1to be a εn,2-covering of
Bw.r.t. the L2norm with εn,2-covering number Q2. By taking all combinations of (˜ψ(j),˜β(j′))
for1≤j≤Q1and1≤j′≤Q2, we obtain the covering {˜θ(j)}Q
j=1with respect to the quantity
ε2
n=σ2
βε2
n,1+C2ε2
n,2where Q=Q1Q2and each ˜θ(j)is given by ˜θ(j)= (˜ψ(j1),˜β(j2))for some
indices j1andj2.
Then as in the discussion of Yang and Barron (1999), the mutual information is bounded by
IX(1:T+1)(Θ,y(1:T+1)) =1
MMX
j=1D(Py(1:T+1)|θ(j)∥¯Py(1:T+1))
≤1
MMX
j=1D(Py(1:T+1)|θ(j)∥˜Py(1:T+1)),
37where D(·∥·)is the Kullback-Leibler divergence and ˜Py(1:T+1)=1
QPQ
j=1Py(1:T+1)|˜θ(j)because
¯Py(1:T+1)minimizes the right hand side. If we let
κ(j) := arg min
1≤k≤QD(Py(1:T+1)|θ(j)∥Py(1:T+1)|˜θ(k)),
then each summand of the right-hand side is further bounded by
logQ+D(Py(1:T+1)|θ(j)∥Py(1:T+1)|˜θκ(j)).
Moreover, for θ= (ψ, β(T+1))it holds that
p(y(1:T+1)|θ,X(1:T+1))
=TY
t=1p(y(t)|ψ,X(t))·p(y(T+1)|ψ, β(T+1),X(T+1))
=TY
t=1Z
p(y(t)|ψ, β(t),X(t))pβ(β(t))dβ(t)·p(y(T+1)|ψ, β(T+1),X(T+1)).
Then the KL-divergence can be bounded as
D(Py(1:T+1)|θ(j)∥Py(1:T+1)|˜θκ(j))
=TX
t=1D
Py(t)|ψ(j)∥Py(t)|˜ψ(κ(j))
+D
Py(T+1)|ψ(j),β(j)
T+1∥Py(T+1)|˜ψ(κ(j)),˜β(κ(j))
T+1
≤TX
t=1Z
D
Py(t)|ψ(j),β(t)∥Py(t)|˜ψ(κ(j)),β(t)
pβ(β(t))dβ(t)
+D
Py(T+1)|ψ(j),β(j)
T+1∥Py(T+1)|˜ψ(κ(j)),˜β(κ(j))
T+1
,
where the joint convexity of KL-divergence was used for the last inequality. Since the observation
noise is assumed to be normally distributed, the integrand KL-divergence can be bounded as
D
Py(t)|ψ(j),β∥Py(t)|˜ψκ(j),β
=nX
i=11
2σ2
β⊤ψ(j)(x(t)
i)−β⊤˜ψ(κ(j))(x(t)
i)2
.
Hence, its expectation with respect to β,X(t)becomes
EX(t),βh
D
Py(t)|ψ(j),β∥Py(t)|˜ψκ(j),βi
=nσ2
β
2σ2∥ψ(j)−˜ψκ(j)∥2
L2(PX)≤nσ2
β
2σ2ε2
n,1,
In the same manner, we have that
D
Py(T+1)|ψ(j),β(j)
T+1∥Py(T+1)|˜ψκ(j),˜β(κ(j))
T+1
=1
2σ2nX
i=1
β(j)⊤
T+1ψ(j)(x(t)
i)−˜β(κ(j))⊤
T+1˜ψ(κ(j))(x(t)
i)2
≤nX
i=11
2σ2
β(j)
T+1−˜β(κ(j))
T+1⊤˜ψ(κ(j))(x(t)
i)2
+nX
i=11
2σ2h
β(j)⊤
T+1(ψ(j)(x(t)
i)−˜ψ(κ(j))(x(t)
i))i2
.
The expectation of the right-hand side with respect to X(T+1), β(j)
T+1is bounded as
EX(T+1),β(j)
T+1h
D
Py(T+1)|ψ(j),β(j)
T+1∥Py(t)|˜ψκ(j),˜β(κ(j))
T+1i
≤C2n
2σ2∥β(j)
T+1−˜β(κ(j))
T+1∥2+n
2σ2σ2
β∥ψ(j)−˜ψ(κ(j))∥2
L2(PX)
≤C2n
2σ2ε2
n,2+n
2σ2σ2
βε2
n,1=n
2σ2ε2
n.
Therefore, the expected mutual information can be bounded as
EX[IX(1:T+1)(Θ,y(1:T+1))]≤logQ1+ log Q2+nT
2σ2σ2
βε2
n,1+n
2σ2ε2
n.
Applying Proposition F.1 together with (7) concludes the proof.
38F.2 Lower Bound in Besov Space
Here, we derive the minimax lower bound when F◦=U(Bα
p,q(X)). Recall that in this setting
s=α/d. We fix a resolution Kand then consider the set of B-splines ωd
K,ℓ, ℓ∈Id
Kof cardinality
N′≍2Kd. Considering the basis pairs (ωd
K,1, ωd
K,2), . . . , (ωd
K,N′−1, ωd
K,N′), we can determine
which one is employed to construct the basis ψ(j). The Varshamov-Gilbert bound yields that for
Ω ={0,1}N′/2, we can construct a subset Ω′={w1, . . . , w2N′/16} ⊂Ωsuch that |Ω|= 2N′/16
andw̸=w′∈Ω′has a Hamming distance not less than N′/16. Using this Ω′, we set N=N′/2
andM= 2N′/16and define (ψ(j))M
j=1asψ(j)
i=ωd
K,2i−1ifwj,i= 0andψ(j)
i=ωd
K,2iifwj,i= 1.
We use the same B-spline bases with resolution more than Kforψ(j)
i(i≥N)across all j.
By the construction of (ψ(j)), if we set β(1)= (σβ, . . . , σ β,0,0, . . .), then
∥β(1)⊤ψ(j)−β(1)⊤ψ(j′)∥2
L2(PX)≥σ2
βN/8.
Hence, for δ2
n≤σ2
βN/8≲1, theδn-packing number is not less than 2N/8. Moreover, the logarithmic
δn-packing number of {β⊤ψ(j)|β∈ B} for a fixed jisΘ(min {δ−1/s
n, Nlog(1/δn)})by the
standard argument.
Hence taking δn=N−s, we obtain logM≳Nand the upper bound of the covering numbers
logQ1+ log Q2≲Nforσ2
βε2
n,1≤δ2
nandε2
n,2=Cδ2
nwhere Cis a constant. Then, by choosing C
appropriately and εn,1≲N−1−s(so that logQ1≲N), as long as
nTσ2
βε2
n,1+nδ2
n≲logQ1+ log Q2≲N
is satisfied, the minimax rate is lower bounded by δ2
n. Taking N≍n1
2s+1, we obtain the lower bound
δ2
n≳n−2s
2s+1. (23)
F.3 Lower Bound with Coarser Basis
We consider a generalized setting where X=Rd×Rd×··· × Rd
| {z }
(N+1) timesand take ψ(j)
i∈U(Bτ
p,q(Rd))
and assume that β1∈[−1,1]andβj∈[−σβ, σβ]where σ2
β=˜Θ(N−2s−1). Since the logarithmic
˜ε1-covering and packing numbers of U(Bτ
p,q(Rd))areΘ(˜ε−d/τ
1), those for the basis functions on
j= 2, . . . , N + 1become Θ(N˜ε−d/τ
1), and those for BareΘ(Nlog(1 +Nσ2
β
ε2
n,2)). Therefore, by
taking ε2
n,1=N˜ε2
1we see that
nT(ε2
n,1+σ2
βε2
n,1) +nε2
n,2≲ε−d/τ
n,1+Nεn,1√
N−d/τ
+Nlog 
1 +Nσ2
β
ε2
n,2!
should be satisfied. Moreover, by taking ε(2τ+d)/τ
n,1 ≍1/nT andε2
n,2≍Nlog(1 + N−2s/ε2
n,2)/n
we can balance both sides. In particular, we may set
ε2
n,1≍(nT)−2τ
2τ+d, ε2
n,2≍N
n∧N−2s.
Taking the balance with respect to Nto maximize ε2
n,2, we have N≍nd
2α+dandε2
n,1≍(nT)−2τ
2τ+d.
Therefore, the minimax rate is lower bounded as
δ2
n≃(1 +σ2
β)ε2
n,1+ε2
n,2≃n−2α
2α+d+ (nT)−2τ
2τ+d. (24)
F.4 Lower Bound in Piecewise γ-smooth Class
Suppose that we utilize the basis functions up to resolution K. Then, by the argument by Nishimura
and Suzuki (2024), the number of basis functions ψrin the K-th resolution is N′≍2K/a†. Moreover,
theδn-packing number of the γ-smooth class is also lower bounded by
logM≥N′polylog( δn, N′). (25)
39Here, by noticing the approximation error bound in Appendix D.2, we take N′≍δ−1/a†
n where the
basis functions are chosen from the Kth resolution. As in the case of the Besov space, we construct
(ψ(j))M′
j=1where M′= 2N′/16andψ(j)(x)∈RNforN=N′/2and∥ψ(j)−ψ(j′)∥2
L2(PX)≥N/8
forj̸=j′. Following the same argument as in the Besov case, we need to take εn,1andεn,2as
nTσ2
βε2
n,1+nε2
n,2≲δ−1/a†
n
up to logarithmic factors. This is satisfied by taking ε2
n,2=Cδ2
n≍n−2a†
2a†+1with a constant Cand
balancing Nso that σ2
βε2
n,1= (nT)−2a†
2a†+1σ2
2a†+1
β≍T−1n−2a†
2a†+1. Combining this evaluation and
(25) yields that the minimax lower bound is given by
δ2
n≳n−2a†
2a†+1. (26)
40NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
•[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist" ,
•Keep the checklist subsection headings, questions/answers and guidelines below.
•Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: See the Our Contributions paragraph for details.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See the Limitations paragraph.
41Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: See Assumptions 1, 2, 3, 4, 5, 6. All proofs were provided in the Appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: See Appendix E, Figure 1.
Guidelines:
• The answer NA means that the paper does not include experiments.
42•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: All experiments are toy simulations and data is i.i.d. random.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
43•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See Appendix E, Figure 2.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The median over 5 runs is reported in Figure 3.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: All experiments are toy simulations.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
44•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research is theoretical and raises no ethical concerns.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The research is theoretical and raises no concerns on societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
45•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
46Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
47