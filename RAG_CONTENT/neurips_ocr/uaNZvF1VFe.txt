Efficient Sign-Based Optimization: Accelerating
Convergence via Variance Reduction
Wei Jiang1, Sifan Yang1,2, Wenhao Yang1,2, Lijun Zhang1,3,2,∗
1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
2School of Artificial Intelligence, Nanjing University, Nanjing, China
3Pazhou Laboratory (Huangpu), Guangzhou, China
{jiangw, yangsf, yangwh, zhanglj}@lamda.nju.edu.cn
Abstract
Sign stochastic gradient descent (signSGD) is a communication-efficient method
that transmits only the sign of stochastic gradients for parameter updating. Exist-
ing literature has demonstrated that signSGD can achieve a convergence rate of
O(d1/2T−1/4), where drepresents the dimension and Tis the iteration number. In
this paper, we improve this convergence rate to O(d1/2T−1/3)by introducing the
Sign-based Stochastic Variance Reduction (SSVR) method, which employs vari-
ance reduction estimators to track gradients and leverages their signs to update. For
finite-sum problems, our method can be further enhanced to achieve a convergence
rate of O(m1/4d1/2T−1/2), where mdenotes the number of component functions.
Furthermore, we investigate the heterogeneous majority vote in distributed set-
tings and introduce two novel algorithms that attain improved convergence rates
ofO(d1/2T−1/2+dn−1/2)andO(d1/4T−1/4)respectively, outperforming the
previous results of O(dT−1/4+dn−1/2)andO(d3/8T−1/8), where nrepresents
the number of nodes. Numerical experiments across different tasks validate the
effectiveness of our proposed methods.
1 Introduction
This paper investigates the stochastic optimization problem
min
x∈Rdf(x), (1)
where f:Rd7→Ris a smooth and non-convex function. We assume that only noisy estimations of
the gradient ∇f(x)can be accessed, represented as ∇f(x;ξ), where ξis a random sample drawn
from a stochastic oracle such that E[∇f(x;ξ)] =∇f(x).
The most well-known method for problem (1) is stochastic gradient descent (SGD), which performs
xt+1=xt−η∇f(xt;ξt)for each iteration, where ξtis the sample used in the t-th iteration, and
ηis the learning rate. It has been proved that the SGD method can obtain a convergence rate of
O(T−1/4)[Ghadimi and Lan, 2013], where Tis the iteration number. Recently, sign stochastic
gradient descent (signSGD) method [Seide et al., 2014, Bernstein et al., 2018] has become popular in
the machine learning community, which uses the sign of the stochastic gradient to update, i.e.,
xt+1=xt−ηSign(∇f(xt;ξt)).
This method can largely reduce the communication overhead in distributed environments, and prior
research [Bernstein et al., 2018, 2019] has established that signSGD can achieve a convergence rate
∗Lijun Zhang is the corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).ofO(d1/2T−1/4)measured in terms of the l1-norm. Since the O(T−1/4)rate is already optimal for
SGD methods when measured in the l2-norm [Arjevani et al., 2023] , we can not further improve the
dependence on Tfor signSGD method, considering that ∥x∥2≤ ∥x∥1for any x.2However, it is also
known that variance reduction techniques can further enhance the convergence rate to O(T−1/3),
under a slightly stronger assumption of average smoothness [Fang et al., 2018, Wang et al., 2019,
Cutkosky and Orabona, 2019]. This leads to a natural question: Can the convergence of sign-based
methods be further improved by employing variance reduction techniques along with the average
smoothness assumption? We respond affirmatively by introducing the Sign-based Stochastic Variance
Reduction (SSVR) method. By integrating variance reduction technique [Cutkosky and Orabona,
2019] with sign operations, we achieve an improved convergence rate of O(d1/2T−1/3)measured in
thel1-norm, matching the optimal rates in terms of Tfor stochastic variance reduction methods [Fang
et al., 2018, Li et al., 2021, Arjevani et al., 2023].
Furthermore, we investigate a special case of problem (1), in which the objective function exhibits a
finite-sum structure:
min
x∈Rdf(x) =1
mmX
i=1fi(x), (2)
where each fi(·)is smooth and non-convex. This problem has been extensively studied in stochastic
optimization [Zhang et al., 2013, Defazio et al., 2014, Fang et al., 2018], but is less explored with
sign-based methods. Previous literature proposes signSVRG [Chzhen and Schechtman, 2023] method
to deal with the finite-sum problem, which achieves a convergence rate of O(m1/2d1/2T−1/2).
However, its dependence on mis sub-optimal, failing to match the O(m1/4T−1/2)lower bound [Fang
et al., 2018, Li et al., 2021] for problem (2). To address this gap, we propose the SSVR-FS algorithm,
which periodically computes the exact gradient [Zhang et al., 2013, Johnson and Zhang, 2013]
and incorporates it into the variance reduction estimator. In this way, we can achieve an improved
convergence rate of O(m1/4d1/2T−1/2)for finite-sum problems.
Finally, sign-based methods are especially favorable in distributed settings, where the parameter
server aggregates gradient signs from each worker through majority vote [Bernstein et al., 2018],
allowing 1-bit compression of communication in both directions. Existing literature [Bernstein et al.,
2018, 2019] has proved that signSGD can obtain a convergence rate of O(d1/2T−1/4)for majority
vote in homogeneous settings, where the data across nodes is uniformly distributed or identical. For
the more challenging heterogeneous setting, in which data distribution can vary significantly across
nodes, existing methods can only achieve convergence rates of O(dT−1/4+dn−1/2)[Sun et al.,
2023] and O(d3/8T−1/8)3[Jin et al., 2023], where ndenotes the number of nodes. Note that the first
rate indicates that the gradient does not converge to zero as Tapproaches infinity, and the second
one suffers from a high sample complexity. To address these limitations, we first introduce our basic
SSVR-MV method, which employs variance reduction estimators to track gradients and replaces the
sign operation in each worker as a stochastic unbiased sign operation. This practice ensures 1-bit
compression and unbiased estimation at the same time, and the newly proposed method can obtain an
improved convergence rate of O(d1/2T−1/2+dn−1/2). By further substituting the sign operation in
the parameter server with another stochastic unbiased sign operation, our method can further achieve
a convergence rate of O(d1/4T−1/4), which converges to zero as Tincreases.
In summary, compared with existing methods, this paper makes the following contributions:
•For stochastic non-convex functions, we develop a sign-based variance reduction algorithm
to achieve an improved convergence rate of O(d1/2T−1/3), surpassing the O(d1/2T−1/4)
rate for signSGD methods.
•For non-convex finite-sum optimization, we further improve the our proposed method
to obtain an enhanced convergence rate of O(m1/4d1/2T−1/2), which is better than the
O(m1/2d1/2T−1/2)convergence rate for SignSVRG method.
2Note that most lower bounds are measured in the l2-norm, and there are no existing lower bounds for the
l1-norm to the best of our knowledge. However, since ∥x∥1≈√
d∥x∥2for dense vector x, the additional
O(√
d)term is acceptable when considering the l1-norm. This is also the case for our methods (measured in the
l1-norm) and the lower bounds for variance reduction methods (for l2-norm).
3The original convergence rate is measured under the squared l2-norm, and we convert it to the rate under the
l2-norm criterion for a fair comparison.
2Table 1: Summary of results for sign-based algorithms. Here, stochastic indicates problem (1),
finite-sum represents problem (2), Nis the number of stochastic gradient calls, and mis the number
of component functions. Note that some rates are measured under squared l1- orl2-norm, and we
convert them to l1- orl2-norm for a fair comparison.
Method Setting Measure Convergence rate
signSGD [Bernstein et al., 2018] stochastic l1-norm O
d1/2
N1/4
EF-signSGD [Karimireddy et al., 2019] stochastic l2-norm O
1
N1/4
signSGD-SIM [Sun et al., 2023] stochastic l1-norm O
d
N1/4
SignSVRG [Chzhen and Schechtman, 2023] finite-sum l1-norm O
d1/2m1/2
N1/2
SignRVR/SignRVM [Qin et al., 2023] finite-sum l1-norm O
d1/2m1/2
N1/2
Theorem 1 stochastic l1-norm O
d1/2
N1/3
Theorem 2 finite-sum l1-norm O
d1/2m1/4
N1/2
Table 2: Summary of results for sign-based algorithms under the majority vote setting, where nis the
number of workers. Some rates are measured under squared l1- orl2-norm, and we convert them to
l1- orl2-norm for a fair comparison.
Method Heterogeneous Measure Convergence rate
signSGD [Bernstein et al., 2018] ✗ l1-norm O
d1/2
T1/4
Signum [Bernstein et al., 2019] ✗ l1-norm O
d1/2
T1/4
SSDM [Safaryan and Richtarik, 2021] ✓ l2-norm O
d1/2
T1/4
Sto-signSGD [Jin et al., 2023] ✓ l2-norm O(d3/8
T1/8)
MV-sto-signSGD-SIM [Sun et al., 2023] ✓ l1-norm O
d
T1/4+d
n1/2
Theorem 3 ✓ l1-norm O
d1/2
T1/2+d
n1/2
Theorem 4 ✓ l2-norm O
d1/4
T1/4
•We also investigate sign-based variance reduction methods with heterogeneous majority
vote in distributed settings. The proposed algorithms can obtain the convergence rates of
O(d1/2T−1/2+dn−1/2)andO(d1/4T−1/4), which outperform the previous results of
O(dT−1/4+dn−1/2)andO(d3/8T−1/8), respectively.
We compare our results with existing methods in Table 1 and Table 2, and validate the effectiveness
of our method via numerical experiments in Section 5.
2 Related work
This section provides an overview of the existing literature on signSGD methods and stochastic
variance reduction techniques.
2.1 SignSGD and its variants
The idea of only transmitting the sign information of the stochastic gradient traces back to the 1-bit
SGD algorithm, introduced by Seide et al. [2014]. Despite the biased nature of the sign operation,
Bernstein et al. [2018] demonstrated that signSGD achieves a convergence rate of O(d1/2T−1/4)by
using large batch sizes in each iteration. Despite the theoretical assurance, Karimireddy et al. [2019]
highlighted that signSGD may not converge to the optimal solutions for convex functions and could
suffer from poor generalization without large batches. To address these issues, they proposed the
3EF-signSGD method, which integrates error feedback into signSGD to correct errors introduced by
the sign operation. Instead of requiring unbiased stochastic gradients in previous literature, Safaryan
and Richtarik [2021] assumed that the signs of the stochastic gradient are the same as those of true
gradient with a probability greater than 1/2. Under this assumption, they demonstrated that signSGD
can obtain a similar convergence rate but does not require large batches anymore. Recently, Sun et al.
[2023] proposed the signSGD-SIM method, which incorporates the momentum into the signSGD,
achieving a convergence rate of O(dT−1/4)with constant batch sizes and an improved convergence
ofO(d3/2T−2/7)with second-order smoothness.
To deal with the finite-sum problems, Chzhen and Schechtman [2023] developed SignSVRG algo-
rithm, which combines SVRG [Johnson and Zhang, 2013] method with signSGD and achieves a
convergence rate of O(d1/2m1/2T−1/2), where mis the number of component functions. More
recently, Qin et al. [2023] further investigate signSGD with random reshuffling, achieving a con-
vergence rate of O 
m−1/2T−1/2log(mT) +∥σ∥1
, where σis the variance bound of stochastic
gradients. By leveraging variance-reduced gradients and momentum updates, they further propose
the SignRVR and SignRVM methods, both achieving the convergence rate of O 
d1/2m1/2T−1/2
.
In distributed settings, sign-based methods with majority vote are also widely investigated. Bernstein
et al. [2018, 2019] first indicated that signSGD and its momentum variant Signum can enable 1-bit
compression of worker-server communication, obtaining the O(d1/2T−1/4)convergence rates in the
homogeneous environment. For the more challenging heterogeneous settings, SSDM method [Sa-
faryan and Richtarik, 2021] attains the same O(d1/2T−1/4)convergence rate, but the information
sent back to the server is not a sign information anymore. To remedy this issue, Sto-signSGD
algorithm [Jin et al., 2023] is proposed, equipped with a convergence rate of O(d3/4T−1/4)measured
in squared l2-norm. More recently, Sun et al. [2023] introduced the MV-signSGD-SIM algorithm
and demonstrated a convergence rate of O(dT−1/4+dn−1/2), which could be further enhanced to
O(d3/2T−2/7+dn−1/2)under second-order smoothness conditions, where ndenotes the number
of nodes in the distributed system.
2.2 Stochastic variance reduction methods
Stochastic variance reduction methods have gained significant attention in the optimization com-
munity in recent years. Among the pioneering approaches, the stochastic average gradient (SAG)
method [Roux et al., 2012] and Stochastic Dual Coordinate Ascent (SDCA) algorithm [Shalev-
Shwartz and Zhang, 2013] utilize a memory of previous gradients to ensure variance reduction,
achieving linear convergence for strongly convex functions. To circumvent the need for storing
gradients, the stochastic variance reduced gradient (SVRG) [Zhang et al., 2013, Johnson and Zhang,
2013] recalculates the full gradient periodically to enhance the accuracy of gradient estimators,
maintaining linear convergence for strongly convex functions. Inspired by SAG and SVRG, Defazio
et al. [2014] introduced the SAGA algorithm, which not only provides superior convergence rates
but also supports proximal regularization. Subsequently, the stochastic recursive gradient algorithm
(SARAH) [Nguyen et al., 2017] employs a simple recursive approach to update gradient estimators,
ensuring better convergence for smooth convex functions.
For non-convex optimization, inspired by the SVRG algorithm, many methods [Reddi et al., 2016,
Lei et al., 2017, Zhou et al., 2020] employ variance reduction to design their algorithms and provide
the corresponding convergence guarantees. More recent well-known advancements include the SPI-
DER [Fang et al., 2018] and SpiderBoost [Wang et al., 2019] methods, which improved the O(T−1/4)
convergence rate of traditional SGD to O(T−1/3)under the average smoothness assumption. The
convergence rate can be further improved to O(m1/4T−1/2)for problems with a finite-sum structure,
where mrepresents the number of component functions. However, these methods typically require a
huge batch size to ensure convergence. To avoid this limitation, the stochastic recursive momentum
(STORM) method [Cutkosky and Orabona, 2019] introduces a momentum-based updating and an
adaptive learning rate based on the stochastic gradients, achieving a convergence rate of ˜O(T−1/3)
without necessitating large batches. More recently, variance reduction techniques are widely em-
ployed in more complex problems to improve the existing convergence rates, such as compositional
optimization [Wang et al., 2017a,b, Yuan et al., 2019, Jiang et al., 2022a, 2023], multi-level optimiza-
tion [Chen et al., 2021, Zhang and Xiao, 2021, Jiang et al., 2022b, 2024b], adaptive algorithms [Kavis
et al., 2022, Jiang et al., 2024a], and distributionally robust optimization [Yu et al., 2024].
4Algorithm 1 SSVR
1:Input: time step T, initial point x1
2:fortime step t= 1toTdo
3: Draw a batch of samples {ξ1
t,···, ξB1
t}
4: Compute vt=1
B1PB1
k=1∇f(xt;ξk
t) + (1 −β)
vt−1−1
B1PB1
k=1∇f(xt−1;ξk
t)
5: Update the decision variable: xt+1=xt−ηSign ( vt)
6:end for
7:Select τuniformly at random from {1, . . . , T }
8:Return xτ
3 The proposed methods
In this section, we present the proposed methods for the expectation case, i.e., problem (1), and the
finite-sum structure, i.e., problem (2), respectively, along with corresponding theoretical guarantees.
3.1 Sign-based stochastic variance reduction
In this subsection, we introduce our Sign-based Stochastic Variance Reduction (SSVR) method for
problem (1). One crucial step in stochastic optimization is to track the gradient of the objective
function. Here, we use a variance reduction gradient estimator vtto evaluate the overall gradient
∇f(xt). In the first iteration ( t= 1), the estimator is defined as v1=1
B0PB0
k=1∇f(x1;ξk
1), where
B0is the batch size used in the first iteration. For subsequent iterations ( t≥2),vtis updated in the
style of STORM [Cutkosky and Orabona, 2019], i.e.,
vt=1
B1B1X
k=1∇f(xt;ξk
t) + (1 −β) 
vt−1−1
B1B1X
k=1∇f(xt−1;ξk
t)!
,
where βrepresents the momentum parameter and B1is the batch size. This method ensures that the
expectation of the estimation error E[∥vt− ∇f(xt)∥2]would be reduced gradually. After obtaining
the gradient estimator vt, we update the decision variable using the sign of vt:
xt+1=xt−ηSign ( vt).
The whole algorithm is outlined in Algorithm 1. Next, we introduce the following assumptions for
our SSVR method, which are standard and commonly adopted in the analysis of variance reduction
methods and stochastic non-convex optimization [Fang et al., 2018, Wang et al., 2019, Cutkosky and
Orabona, 2019, Li et al., 2021].
Assumption 1 (Average smoothness)
Eξh
∥∇f(x;ξ)− ∇f(y;ξ)∥2i
≤L2∥x−y∥2.
Assumption 2 (Bounded variance)
Eξh
∥∇f(x;ξ)− ∇f(x)∥2i
≤σ2.
With the above assumptions, we can obtain the theoretical guarantee for our method as stated below.
Theorem 1 Under Assumptions 1 and 2, by setting β=O(1
T2/3),η=O(1
d1/2T2/3),B0=O(T1/3),
andB1=O(1), our SSVR method ensures:
E[∥∇f(xτ)∥1]≤ Od1/2
T1/3
.
Remark: This convergence rate surpasses the O(d1/2T−1/4)rate achieved by previous sign-based
methods [Bernstein et al., 2018, 2019], and it also outperforms the O(d3/2T−2/7)convergence
rate under the second-order smoothness condition [Sun et al., 2023]. Specifically, to ensure that
E[∥∇f(xτ)∥1]≤ϵ, our method requires a sample complexity of O(d3/2ϵ−3), which is much better
than the O(d2ϵ−4)andO(d21/4ϵ−7/2)complexities of previous approaches.
5Algorithm 2 SSVR for Finite-Sum (SSVR-FS)
1:Input: time step T, initial point x1
2:fortime step t= 1toTdo
3: iftmod I== 0 then
4: Setτ=tand compute ∇f(xτ) =1
mPm
i=1∇fi(xτ)
5: end if
6: Sample itrandomly from {1,2,···, m}
7: Compute gradient estimator vtaccording to equation (3)
8: Update the decision variable: xt+1=xt−ηSign(vt)
9:end for
10:Select φuniformly at random from {1, . . . , T }
11:Return xφ
3.2 Sign-based stochastic variance reduction for finite-sum structure
We now extend our SSVR method to deal with the finite-sum structure in problem (2). In this context,
we introduce the following assumption for each component function, which is standard and widely
adopted in existing literature [Fang et al., 2018, Wang et al., 2019, Li et al., 2021].
Assumption 3 (Smoothness) For each i∈ {1,2,···, m}, the gradient functions satisfy:
∥∇fi(x)− ∇fi(y)∥ ≤L∥x−y∥.
To handle the finite-sum problems, we retain the core structure of our SSVR method while incorporat-
ing elements from the SVRG [Zhang et al., 2013, Johnson and Zhang, 2013] approach. Specifically,
we compute a full batch gradient at the first step and every Iiteration, i.e.,
∇f(xτ) =1
mmX
i=1∇fi(xτ).
For other iterations, we randomly select an index itfrom the set {1,2,···, m}and construct a
variance reduction gradient estimator vtas follows:
vt=∇fit(xt) + (1 −β)(vt−1− ∇fit(xt−1))| {z }
STORM estimator−β(∇fit(xτ)− ∇f(xτ))| {z }
error correction. (3)
The first two terms of vtalign with the STORM estimator, and the last term measures the difference of
past gradients between the selected component function ∇fit(xτ)and the overall objective ∇f(xτ).
Note that the STORM estimator employs the component gradient ∇fit(xt)to track the overall
gradient ∇f(xt), which leads to an estimation error due to the gap between the component function
and the overall objective. This gap can be effectively mitigated by the error correction term we
introduced in equation (3). With such a design, we can obtain a better gradient estimation of the
overall gradient, and ensure that the estimation error E[∥∇f(xt)−vt∥2]can be reduced gradually.
After computing vt, we utilize its sign information to update the decision variable. The detailed
procedure is outlined in Algorithm 2. Next, we present the theoretical convergence for this method.
Theorem 2 Under Assumption 3, by setting β=O(1
m),I=m, and η=O(1
m1/4d1/2T1/2), our
algorithm ensures:
E[∥∇F(xφ)∥1]≤ Om1/4d1/2
T1/2
.
Remark: To ensure E[∥∇F(xφ)∥1]≤ϵ, the sample complexity is O(m+d√m
ϵ2), which improves
over the O(dm
ϵ2)complexity of the previous SignSVRG method [Chzhen and Schechtman, 2023].
4 Sign-based stochastic variance reduction with majority vote
Sign-based methods are advantageous in distributed settings for their low communication overhead, as
they can only transmit sign information between nodes via majority vote. This section explores sign-
based stochastic methods with majority vote, a typical example of distributed learning extensively
6Algorithm 3 SSVR-MV
1:Input: time step T, initial point x1
2:fortime step t= 1toTdo
3: Onnode j∈ {1,2,···, n}:
4: Draw sample ξj
tand compute vj
t=∇fj(xt;ξj
t) + (1 −β)
vj
t−1− ∇fj(xt−1;ξj
t)
5: Option 1: Send SR(vj
t)to the parameter server, where R= 4G
6: Option 2: Send SG(ˆvj
t)to the parameter server, where ˆvj
t= Π G[vj
t]
7: Onparameter server:
8: Option 1: Send vt= Sign
1
nPn
j=1SR
vj
t
to all nodes
9: Option 2: Send vt= S1
1
nPn
j=1SG
ˆvj
t
to all nodes
10: Onnode j∈ {1,2,···, n}:
11: Update the decision variable xt+1=xt−ηvt
12:end for
13:Select τuniformly at random from {1, . . . , T }
14:Return xτ
studied in previous sign-based algorithms [Bernstein et al., 2018, 2019, Safaryan and Richtarik, 2021,
Sun et al., 2023]. To begin with, we investigate the following distributed learning task:
min
x∈Rdf(x):=1
nnX
j=1fj(x), f j(x) =Eξj∼Dj
fj(x;ξj)
, (4)
where Djrepresents the data distribution for node j, and fj(x)is the corresponding loss function.
Some previous studies [Bernstein et al., 2018, 2019] investigate the homogeneous setting, which
assumes the data across each node is uniformly distributed or identical, ensuring that E[fi(x)] =f(x).
In contrast, this paper considers the more challenging heterogeneous setting [Jin et al., 2023, Sun
et al., 2023], where data distributions can vary significantly across nodes.
For sign-based methods in distributed settings, each node jcomputes a gradient estimator vj
tand
transmits its sign, i.e., Sign( vj
t), to the parameter server. Note that the server can not directly send
the aggregate informationPn
j=1Sign(vj
t)back to each node, since it loses binary characteristic after
summation. A natural solution is to apply another sign operation to update the decision variable as:
xt+1=xt−ηSign
1
nnX
j=1Sign( vj
t)
.
This process is called majority vote [Bernstein et al., 2018], as each worker votes on the sign of
the gradient, with the server tallying these votes and broadcasting the decision back to the nodes.
However, the sign operation introduces bias in the estimation, and employing it twice can significantly
amplify this bias, particularly in a heterogeneous environment. Previous analysis [Chen et al., 2020]
indicates that signSGD fails to converge in the heterogeneous setting. To deal with this problem, we
introduce an unbiased sign operation SR(·), which is defined below.
Definition 1 For any vector vwith∥v∥∞≤R, define the function mapping SR(v)as:
[SR(v)]k=

+1,with probability1
2+[v]k
2R,
−1,with probability1
2−[v]k
2R.(5)
Remark: This operation provides an unbiased estimation of v/R, such that E[SR(v)] =v/R. It
is worth noting that the function mapping is valid when ∥v∥∞≤R, since the probability should
always fall within [0,1]. For this purpose, we need to further assume that the gradient is bounded.
7Utilizing this unbiased sign operation, we can update the decision variable as:
xt+1=xt−ηSign
1
nnX
j=1SR(vj
t)
.
After applying SR(·), the output is a sign information, which can be transported between nodes
efficiently. The complete algorithm, named SSVR with majority vote (SSVR-MV), is described in
Algorithm 3 (with Option 1 ). Note that in Step 4, we set vj
1=∇f(x1;ξj
1)when t= 1. Next, we
present the convergence guarantee for the proposed algorithm with the following assumption.
Assumption 4 For each node j, the stochastic gradient is bounded by Gin the infinity norm, such
that∥∇fj(x;ξ)∥∞≤G.
Theorem 3 Under Assumptions 1, 2 and 4, by setting β=1
2andη=O(1
T1/2d1/2), our SSVR-MV
method (with Option 1) ensures:
E[∥∇f(xτ)∥1]≤ Od1/2
T1/2+d
n1/2
.
Remark: Our rate is better than the previous result of O(dT−1/4+dn−1/2), and also outperforms
the rate of O(d3/2T−2/7+dn−1/2)under the second-order smoothness [Sun et al., 2023].
Although the above convergence rate is superior to previous results, we have to note that the gradient
does not converge to zero even as T→ ∞ . To address this issue, we propose replacing another sign
operation with the S1(·)mapping, as defined in equation (5) with R= 1. Additionally, in our prior
analysis, we ensured that each vj
tis bounded by assuming the stochastic gradient is bounded and
using a constant β. Here, we instead suppose that the true gradient is bounded, as detailed below.
Assumption 4′For each node j, the gradient is bounded such that ∥∇fj(x)∥ ≤G.
Remark: This assumption is weaker than the one used by Sun et al. [2023], which assumes all
stochastic gradients are bounded, i.e., ∥∇fj(x;ξ)∥ ≤G.
To ensure each gradient estimator is bounded, we employ a projection operation ˆvj
t= Π G[vj
t],
where ΠGdenotes the projection onto a ball of radius G. This allows us to utilize an unbiased sign
mapping SG(ˆvj
t)before transmission to the parameter server. The revised algorithm is presented
in Algorithm 3 (with Option 2 ), and the modifications lie in Steps 6 and 9. We now present the
convergence guarantee for this modified approach below.
Theorem 4 Under Assumptions 1, 2 and 4′, by setting β=O(1
T1/2)andη=O(1
d1/2T1/2), our
SSVR-MV method (with Option 2) ensures:
E[∥∇f(xτ)∥]≤ Od1/4
T1/4
.
Remark: This rate converges to zero as T→ ∞ , and offers a significant improvement over the
previous results of O(d3/8T−1/8)[Jin et al., 2023]. Our result is also better than the O(d1/2T−1/4)
convergence rate obtained by Safaryan and Richtarik [2021], whose algorithm requires transmittingPn
j=1sign(vj
t)back to all nodes, which is actually not sign information anymore.
5 Experiments
In this section, we assess the performance of the proposed methods through numerical experiments.
We first evaluate the SSVR and SSVR-FS algorithms within the centralized setting, and then assess
the performance of SSVR-MV method in the distributed learning environment. All experiments are
conducted on NVIDIA 3090 GPUs.
80 50 100 150 200
Epoch0.000.250.500.751.00Training loss
0 50 100 150 200
Epoch246Gradient norm
0 50 100 150 200
Epoch8688909294Testing accuracy
signSGD signSGD-SIM SignSVRG SSVR SSVR-FSFigure 1: Results for CIFAR-10 dataset in the centralized environment.
0 50 100 150 200
Epoch01234Training loss
0 50 100 150 200
Epoch505560657075Testing accuracy
(a) Majority vote with 4 nodes
0 50 100 150 200
Epoch01234Training loss
0 50 100 150 200
Epoch505560657075Testing accuracy (b) Majority vote with 8 nodes
signSGD Signum SSDM Sto-signSGD MV-signSGD-SIM SSVR-MV
Figure 2: Results for CIFAR-100 dataset in the distributed environment.
5.1 Evaluation of SSVR and SSVR-FS methods in the centralized environment
To begin with, we conduct numerical experiments on multi-class image classification tasks to validate
the effectiveness of our proposed methods. Concretely, we train a ResNet18 model [He et al.,
2016] on the CIFAR-10 dataset [Krizhevsky, 2009]. We compare the performance of our SSVR
and SSVR-FS methods against signSGD [Bernstein et al., 2018], signSGD-SIM [Sun et al., 2023],
and SignSVRG [Chzhen and Schechtman, 2023]. For hyper-parameter tuning, we either follow the
recommendations from the original papers or employ a grid search to determine the best settings.
Specifically, the momentum parameter βis searched from the set {0.1,0.5,0.9,0.99}, and the
learning rate is fine-tuned within the range of {1e−5,1e−4,1e−3,1e−2,1e−1}.
Results. The training loss, gradient norm, and testing accuracy are presented in Figure 1, with curves
averaged over five runs. We observe that all methods exhibit a rapid decrease in training losses, with
our methods showing a more pronounced reduction in the gradient norm. In terms of testing accuracy,
our SSVR algorithm outperforms other sign-based methods, and our SSVR-FS method achieves
superior accuracy in the final epochs.
5.2 Evaluation of SSVR-MV method in the distributed learning
Subsequently, we conduct experiments to evaluate the effectiveness of the SSVR-MV method in the
distributed environment. Specifically, we train a ResNet50 model [He et al., 2016] on the CIFAR-
100 dataset [Krizhevsky, 2009] with 4 and 8 nodes respectively. We compare the performance of
our method against signSGD (with majority vote) [Bernstein et al., 2018], Signum (with majority
vote) [Bernstein et al., 2019], SSDM [Safaryan and Richtarik, 2021], Sto-signSGD [Jin et al., 2023],
and MV-signSGD-SIM [Sun et al., 2023]. The hyper-parameter tuning follows the same methodology
as in the centralized environment experiment.
Results. We plot the training loss and testing accuracy in Figure 2, with all curves averaged over five
runs. The results indicate that the training loss of our SSVR-MV algorithm decreases rapidly, and our
method obtains higher testing accuracy compared to other methods, both in experiments with 4 nodes
and 8 nodes.
96 Conclusion
In this paper, we explore sign-based stochastic variance reduction (SSVR) methods, which use only
the sign information of variance-reduced estimators to update decision variables. The proposed
method achieves an improved convergence rate of O(d1/2T−1/3), surpassing the O(d1/2T−1/4)
convergence rate of signSGD methods. When applied to finite-sum problems, this rate can be further
enhanced to O(m1/4d1/2T−1/2), which is also better than the O(m1/2d1/2T−1/2)convergence
rate of SignSVRG. Finally, we investigate the SSVR method in distributed settings and devise
novel algorithms to attain convergence rates of O(d1/2T−1/2+dn−1/2)andO(d1/4T−1/4), which
improve upon the previous results of O(dT−1/4+dn−1/2)andO(d3/8T−1/8)respectively.
Acknowledgements
This work was partially supported by NSFC (62122037), the Collaborative Innovation Center of Novel
Software Technology and Industrialization, and the Postgraduate Research & Practice Innovation
Program of Jiangsu Province (No. KYCX24_0231).
References
Y . Arjevani, Y . Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. E. Woodworth. Lower bounds
for non-convex stochastic optimization. Mathematical Programming , 199(1-2):165–214, 2023.
J. Bernstein, Y .-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signSGD: Compressed optimisa-
tion for non-convex problems. In Proceedings of the 35th International Conference on Machine
Learning , pages 560–569, 2018.
J. Bernstein, J. Zhao, K. Azizzadenesheli, and A. Anandkumar. signSGD with majority vote is com-
munication efficient and fault tolerant. In International Conference on Learning Representations ,
2019.
T. Chen, Y . Sun, and W. Yin. Solving stochastic compositional optimization is nearly as easy as
solving stochastic optimization. IEEE Transactions on Signal Processing , 69:4937–4948, 2021.
X. Chen, T. Chen, H. Sun, S. Z. Wu, and M. Hong. Distributed training with heterogeneous data:
Bridging median- and mean-based algorithms. In Advances in Neural Information Processing
Systems 33 , pages 21616–21626, 2020.
Z. Chen, Y . Zhou, Y . Liang, and Z. Lu. Generalized-smooth nonconvex optimization is as efficient as
smooth nonconvex optimization. In Proceedings of the 40th International Conference on Machine
Learning , pages 5396–5427, 2023.
E. Chzhen and S. Schechtman. SignSVRG: fixing SignSGD via variance reduction. ArXiv e-prints ,
arXiv:2305.13187, 2023.
A. Cutkosky and F. Orabona. Momentum-based variance reduction in non-convex SGD. In Advances
in Neural Information Processing Systems 32 , pages 15210–15219, 2019.
A. Defazio, F. R. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support
for non-strongly convex composite objectives. In Advances in Neural Information Processing
Systems 27 , pages 1646–1654, 2014.
C. Fang, C. J. Li, Z. Lin, and T. Zhang. SPIDER: Near-optimal non-convex optimization via stochastic
path-integrated differential estimator. In Advances in Neural Information Processing Systems 31 ,
pages 689–699, 2018.
M. Faw, I. Tziotis, C. Caramanis, A. Mokhtari, S. Shakkottai, and R. Ward. The power of adaptivity
in SGD: Self-tuning step sizes with unbounded gradients and affine variance. In Proceedings of
Thirty Fifth Conference on Learning Theory , volume 178, pages 313–355, 2022.
S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic program-
ming. SIAM Journal on Optimization , 23(4):2341–2368, 2013.
10K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016.
W. Jiang, G. Li, Y . Wang, L. Zhang, and T. Yang. Multi-block-single-probe variance reduced estimator
for coupled compositional optimization. In Advances in Neural Information Processing Systems
35, pages 32499–32511, 2022a.
W. Jiang, B. Wang, Y . Wang, L. Zhang, and T. Yang. Optimal algorithms for stochastic multi-level
compositional optimization. In Proceedings of the 39th International Conference on Machine
Learning , pages 10195–10216, 2022b.
W. Jiang, J. Qin, L. Wu, C. Chen, T. Yang, and L. Zhang. Learning unnormalized statistical models
via compositional optimization. In Proceedings of the 40th International Conference on Machine
Learning , pages 15105–15124, 2023.
W. Jiang, S. Yang, Y . Wang, and L. Zhang. Adaptive variance reduction for stochastic optimization
under weaker assumptions. In Advances in Neural Information Processing Systems 37 , 2024a.
W. Jiang, S. Yang, W. Yang, Y . Wang, Y . Wan, and L. Zhang. Projection-free variance reduction
methods for stochastic constrained multi-level compositional optimization. In Proceedings of the
41st International Conference on Machine Learning , pages 21962–21987, 2024b.
R. Jin, Y . Huang, X. He, H. Dai, and T. Wu. Stochastic-Sign SGD for federated learning with
theoretical guarantees. ArXiv e-prints , arXiv:2002.10940, 2023.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction.
InAdvances in Neural Information Processing Systems 26 , pages 315–323, 2013.
S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi. Error feedback fixes SignSGD and other
gradient compression schemes. In Proceedings of the 36th International Conference on Machine
Learning , pages 3252–3261, 2019.
A. Kavis, S. Skoulakis, K. Antonakopoulos, L. T. Dadi, and V . Cevher. Adaptive stochastic variance
reduction for non-convex finite-sum minimization. In Advances in Neural Information Processing
Systems 35 , pages 23524–23538, 2022.
A. Krizhevsky. Learning multiple layers of features from tiny images. Masters Thesis, Deptartment
of Computer Science, University of Toronto , 2009.
L. Lei, C. Ju, J. Chen, and M. I. Jordan. Non-convex finite-sum optimization via scsg methods. In
Advances in Neural Information Processing Systems , volume 30, pages 2345–2355, 2017.
Z. Li, H. Bao, X. Zhang, and P. Richtarik. Page: A simple and optimal probabilistic gradient estimator
for nonconvex optimization. In Proceedings of the 38th International Conference on Machine
Learning , pages 6286–6295, 2021.
L. M. Nguyen, J. Liu, K. Scheinberg, and M. Takac. SARAH: A novel method for machine learning
problems using stochastic recursive gradient. In Proceedings of the 34th International Conference
on Machine Learning , pages 2613–2621, 2017.
Z. Qin, Z. Liu, and P. Xu. Convergence of sign-based random reshuffling algorithms for nonconvex
optimization. ArXiv e-prints , arXiv:2310.15976, 2023.
S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola. Stochastic variance reduction for nonconvex
optimization. In Proceedings of The 33rd International Conference on Machine Learning , pages
314–323, 2016.
N. L. Roux, M. Schmidt, and F. R. Bach. A stochastic gradient method with an exponential
convergence rate for finite training sets. In Advances in Neural Information Processing Systems 25 ,
pages 2672–2680, 2012.
M. Safaryan and P. Richtarik. Stochastic sign descent methods: New algorithms and better theory. In
Proceedings of the 38th International Conference on Machine Learning , pages 9224–9234, 2021.
11F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient descent and its application
to data-parallel distributed training of speech DNNs. In Conference of the International Speech
Communication Association , pages 1058–1062, 2014.
S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss
minimization. Journal of Machine Learning Research , 14(16):567–599, 2013.
T. Sun, Q. Wang, D. Li, and B. Wang. Momentum ensures convergence of SIGNSGD under weaker
assumptions. In Proceedings of the 40th International Conference on Machine Learning , pages
33077–33099, 2023.
M. Wang, E. X. Fang, and H. Liu. Stochastic compositional gradient descent: algorithms for
minimizing compositions of expected-value functions. Mathematical Programming , 161(1-2):
419–449, 2017a.
M. Wang, J. Liu, and E. X. Fang. Accelerating stochastic composition optimization. Journal of
Machine Learning Research , 18:105:1–105:23, 2017b.
Z. Wang, K. Ji, Y . Zhou, Y . Liang, and V . Tarokh. SpiderBoost and momentum: Faster variance
reduction algorithms. In Advances in Neural Information Processing Systems 32 , pages 2406–2416,
2019.
D. Yu, Y . Cai, W. Jiang, and L. Zhang. Efficient algorithms for empirical group distributionally
robust optimization and beyond. In Proceedings of the 41st International Conference on Machine
Learning , pages 57384–57414, 2024.
H. Yuan, X. Lian, C. J. Li, J. Liu, and W. Hu. Efficient smooth non-convex stochastic composi-
tional optimization via stochastic recursive gradient descent. In Advances in Neural Information
Processing Systems 32 , pages 14905–14916, 2019.
J. Zhang and L. Xiao. Multilevel composite stochastic optimization via nested variance reduction.
SIAM Journal on Optimization , 31(2):1131–1157, 2021.
L. Zhang, M. Mahdavi, and R. Jin. Linear convergence with condition number independent access of
full gradients. In Advances in Neural Information Processing Systems 26 , pages 980–988, 2013.
D. Zhou, P. Xu, and Q. Gu. Stochastic nested variance reduction for nonconvex optimization. Journal
of Machine Learning Research , 21(103):1–63, 2020.
12A Proof of Theorem 1
Firstly, note that Assumption 1 indicates that the objective function f(x)is also L-smooth [Li et al.,
2021]. Given this property, we have that
f(xt+1)
≤f(xt) +⟨∇f(xt),xt+1−xt⟩+L
2∥xt+1−xt∥2
≤f(xt) +⟨∇f(xt),−ηSign( vt)⟩+η2L
2∥Sign( vt)∥2
≤f(xt) +η⟨∇f(xt),Sign(∇f(xt))−Sign( vt)⟩ −η⟨∇f(xt),Sign(∇f(xt))⟩+η2Ld
2
=f(xt) +η⟨∇f(xt),Sign(∇f(xt))−Sign( vt)⟩ −η∥∇f(xt)∥1+η2Ld
2
≤f(xt) + 2η√
d∥∇f(xt)−vt∥ −η∥∇f(xt)∥1+η2Ld
2,(6)
where the last inequality is due to the fact that
⟨∇f(xt),Sign(∇f(xt))−Sign( vt)⟩
=dX
i=1⟨[∇f(xt)]i,Sign([∇f(xt)]i)−Sign([ vt]i)⟩
≤dX
i=12|[∇f(xt)]i| ·I(Sign([ ∇f(xt)]i)̸= Sign([ vt]i))
≤dX
i=12|[∇f(xt)]i−[vt]i| ·I(Sign([ ∇f(xt)]i)̸= Sign([ vt]i))
≤dX
i=12|[∇f(xt)]i−[vt]i|
=2∥∇f(xt)−vt∥1
≤2√
d∥∇f(xt)−vt∥.(7)
Summing up and rearranging the equation (6), we derive:
E"
1
TTX
t=1∥∇f(xt)∥1#
≤f(x1)−f(xT+1)
ηT+ 2√
d·E"
1
TTX
t=1∥∇f(xt)−vt∥#
+ηLd
2
≤∆f
ηT+ 2√
d·vuutE"
1
TTX
t=1∥∇f(xt)−vt∥2#
+ηLd
2(8)
where we define ∆f=f(x1)−f∗, and the second inequality is due to Jensen’s Inequality.
13Next, we can bound the term Eh
1
TPT
t=1∥∇f(xt)−vt∥2i
as follows.
Eh
∥∇f(xt+1)−vt+1∥2i
=E
(1−β)vt+1
B1B1X
k=1∇f(xt+1;ξk
t+1)−(1−β)1
B1B1X
k=1∇f(xt;ξk
t+1)− ∇f(xt+1)2

=E"(1−β)(vt− ∇f(xt)) +β1
B1B1X
k=1 
∇f(xt;ξk
t+1)− ∇f(xt)
+ 
∇f(xt)− ∇f(xt+1) +1
B1B1X
k=1∇f(xt+1;ξk
t+1)−1
B1B1X
k=1∇f(xt;ξk
t+1)!2

≤(1−β)2Eh
∥vt− ∇f(xt)∥2i
+ 2β2E
1
B1B1X
k=1∇f(xt;ξk
t+1)− ∇f(xt)2

+ 2E
1
B1B1X
k=1 
∇f(xt+1;ξk
t+1)− ∇f(xt;ξk
t+1)2

≤(1−β)E
∥vt− ∇f(xt)∥2
+2β2σ2
B1+2L2∥xt+1−xt∥2
B1
≤(1−β)E
∥vt− ∇f(xt)∥2
+2β2σ2
B1+2L2η2d
B1,
where the first inequality is due to the fact Eh
β1
B1PB1
k=1 
∇f(xt;ξk
t+1
− ∇f(xt)
+∇f(xt)
−∇f(xt+1) +1
B1PB1
k=1 
∇f(xt+1;ξk
t+1)− ∇f(xt;ξk
t+1)i
= 0, and (a+b)2≤2a2+ 2b2.
Summing up and noticing that we use a batch size of B0in the first iteration, we can ensure
E"
1
TTX
t=1∥vt− ∇f(xt)∥2#
≤Eh
∥v1− ∇f(x1)∥2i
βT+2σ2β
B1+2L2η2d
βB1
≤σ2
B0βT+2σ2β
B1+2L2η2d
βB1(9)
Incorporating the above into equation (8) and setting that β=O 
T−2/3
,η=O 
d−1/2T−2/3
,
B0=O 
T1/3
,B1=O(1), we observe:
E"
1
TTX
t=1∥∇f(xt)∥1#
≤∆f
ηT+ 2√
d·vuutE"
1
TTX
t=1∥∇f(xt)−vt∥2#
+ηLd
2
≤∆f
ηT+ 2√
d·s
σ2
B0βT+2σ2β
B1+2L2η2d
βB1+ηLd
2
=O(∆f+σ+L)d1/2
T1/3
=Od1/2
T1/3
,
which finishes the proof of Theorem 1.
B Proof of Theorem 2
To improve the convergence rate for finite-sum structures, we can reuse the results of equation (8), but
bound the term Eh
1
TPT
t=1∥∇f(xt)−vt∥2i
differently. Since vt= (1−β)vt−1+βht+ (1−
14β) (∇fit(xt)− ∇fit(xt−1)), where ht=∇fit(xt)− ∇fit(xτ) +∇f(xτ), we have:
Eh
∥∇f(xt+1)−vt+1∥2i
=Eh(1−β)vt+βht+1+ (1−β)(∇fit+1(xt+1)− ∇fit+1(xt))− ∇f(xt+1)2i
=E[∥(1−β)(vt− ∇f(xt)) +β(ht+1− ∇f(xt+1))
+(1−β) 
∇f(xt)− ∇f(xt+1) +∇fit+1(xt+1)− ∇fit+1(xt)
∥2
≤(1−β)2E
∥vt− ∇f(xt)∥2
+ 2β2E
∥ht+1− ∇f(xt+1)∥2
+ 2(1−β)2E
∥∇f(xt)− ∇f(xt+1) +∇fit+1(xt+1)− ∇fit+1(xt)∥2
≤(1−β)2E
∥vt− ∇f(xt)∥2
+ 2β2E
∥ht+1− ∇f(xt+1)∥2
+ 2(1−β)2E
∥∇fit+1(xt+1)− ∇fit+1(xt)∥2
≤(1−β)E
∥vt− ∇f(xt)∥2
+ 2β2E
∥ht+1− ∇f(xt+1)∥2
+ 2L2E
∥xt+1−xt∥2
≤(1−β)E
∥vt− ∇f(xt)∥2
+ 2β2L2E
∥xt+1−xτ∥2
+ 2L2E
∥xt+1−xt∥2
,
where the last inequality is due to the fact that:
E
∥ht+1− ∇f(xt+1)∥2
=E
∥∇fit+1(xt+1)− ∇fit+1(xτ) +∇f(xτ)− ∇f(xt+1)∥2
≤E
∥∇fit+1(xt+1)− ∇fit+1(xτ)∥2
≤L2E
∥xt+1−xτ∥2
.
By rearranging and summing up, we establish:
E"
1
TTX
t=1∥∇f(xt)−vt∥2#
≤E
∥v1− ∇f(x1)∥2
βT+ 2βL2E"
1
TTX
t=1∥xt+1−xτ∥2#
+2L2
βE"
1
TTX
t=1∥xt+1−xt∥2#
≤2βI2L2E"
1
TTX
t=1∥xt+1−xt∥2#
+2L2
βE"
1
TTX
t=1∥xt+1−xt∥2#
≤2L2
βI2+1
β
η2d,
where we use full batch in the first iteration, and the second inequality is due to the fact that
1
TTX
t=1∥xt+1−xτ∥2=1
TTX
t=1tX
i=τ(xi+1−xi)2
≤1
TTX
t=1ItX
i=τ∥xi+1−xi∥2
≤I2
TTX
t=1∥xt+1−xt∥2.
Incorporate the above into equation (8) and setting I=m, β =O 1
m
, and η=O 1
m1/4d1/2T1/2
,
we refine the bound as:
E"
1
TTX
t=1∥∇f(xt)∥1#
≤∆f
ηT+ 2√
d·vuutE"
1
TTX
t=1∥∇f(xt)−vt∥2#
+ηLd
2
≤∆f
ηT+ 2√
d·s
2L2
βI2+1
β
η2d+ηLd
2
=O(∆f+L)m1/4d1/2
T1/2
=Om1/4d1/2
T1/2
.
15C Proof of Theorem 3
By setting β=1
2, we can ensure thatvj
t
∞≤R= 4G, since
vj
t
∞=(1−β)vj
t−1+∇f(xt;ξj
t)−(1−β)f(xt−1;ξj
t)
∞
≤(1−β)vj
t−1
∞+ (2−β)G
≤(1−β)t−1vj
1
∞+ (2−β)GtX
s=1(1−β)t−s
≤G+(2−β)G
β≤4G=R.
Since the overall objective function f(x)isL-smooth, we have the following:
f(xt+1)≤f(xt) +⟨∇f(xt),xt+1−xt⟩+L
2∥xt+1−xt∥2
≤f(xt)−η*
∇f(xt),Sign
1
nnX
j=1SR(vj
t)
+
+η2Ld
2
−η⟨∇f(xt),Sign(∇f(xt))⟩+η2Ld
2
=f(xt) +η*
∇f(xt),Sign(∇f(xt))−Sign
1
nnX
j=1SR(vj
t)
+
−η∥∇f(xt)∥1+η2Ld
2
≤f(xt) + 2ηR√
d∇f(xt)
R−1
nnX
j=1SR(vj
t)−η∥∇f(xt)∥1+η2Ld
2,
(10)
where the last inequality is because of
*
∇f(xt),Sign(∇f(xt))−Sign
1
nnX
j=1SR(vj
t)
+
=dX
i=1*
[∇f(xt)]i,Sign([∇f(xt)]i)−Sign

1
nnX
j=1SR(vj
t)

i
+
≤dX
i=12R|[∇f(xt)]i/R| ·I
Sign([∇f(xt)]i)̸= Sign

1
nnX
j=1SR(vj
t)

i


≤dX
i=12R[∇f(xt)]i
R−
1
nnX
j=1SR(vj
t)

i·I
Sign([∇f(xt)]i)̸= Sign

1
nnX
j=1SR(vj
t)

i


≤dX
i=12R[∇f(xt)]i
R−
1
nnX
j=1SR(vj
t)

i
=2R∇f(xt)
R−1
nnX
j=1SR(vj
t)
1≤2R√
d∇f(xt)
R−1
nnX
j=1SR(vj
t).
(11)
16Rearranging and taking the expectation over equation (10), we have:
E[f(xt+1)−f(xt)]
≤2ηR√
dE
∇f(xt)
R−1
nnX
j=1SR(vj
t)
−ηE[∥∇f(xt)∥1] +η2Ld
2
≤2ηR√
dE
∇f(xt)
R−1
nRnX
j=1vj
t
+ 2ηR√
dE
1
nnX
j=1 
SR(vj
t)−vj
t
R!

−ηE[∥∇f(xt)∥1] +η2Ld
2
≤2η√
dE
∇f(xt)−1
nnX
j=1vj
t
+ 2ηR√
dvuuuutE
1
nnX
j=1 
SR(vj
t)−vj
t
R!2

−ηE[∥∇f(xt)∥1] +η2Ld
2
≤2η√
dE
∇f(xt)−1
nnX
j=1vj
t
+ 2ηR√
dvuuut1
n2nX
j=1E
 
SR(vj
t)−vj
t
R!2

−ηE[∥∇f(xt)∥1] +η2Ld
2
≤2η√
dE
∇f(xt)−1
nnX
j=1vj
t
+ 2ηR√
dvuut1
n2nX
j=1ESR(vj
t)2
−ηE[∥∇f(xt)∥1] +η2Ld
2
≤2η√
dE
∇f(xt)−1
nnX
j=1vj
t
+2ηdR√n−ηE[∥∇f(xt)∥1] +η2Ld
2,(12)
where the third inequality is due to the fact that (E[X])2≤E
X2
, and the forth inequality is
because of Eh
S
vj
ti
=vj
t
R, as well as the Soperation in each node is independent.
Rearranging the terms and summing up, we have:
1
TTX
i=1E[∥∇f(xt)∥1]≤∆f
ηT+ 2√
dE
1
TTX
i=1∇f(xt)−1
nnX
j=1vj
t
+2dR√n+ηLd
2
≤∆f
ηT+ 2√
dvuuuutE
1
TTX
i=1∇f(xt)−1
nnX
j=1vj
t2
+2dR√n+ηLd
2,
where the last inequality is due to Jensen’s inequality.
For each worker j, we have the following according to the definition of vj
t:
vj
t+1− ∇fj(xt+1) = (1 −β)
vj
t− ∇fj(xt)
+β
∇fj(xt+1;ξj
t+1)− ∇fj(xt+1)
+ (1−β)
∇fj(xt+1;ξj
t+1)− ∇fj(xt;ξj
t+1) +∇fj(xt)− ∇fj(xt+1)
.
17Summing over {n}and noting that ∇f(x) =1
nPn
j=1∇fj(x), we can obtain:
1
nnX
j=1vj
t+1− ∇f(xt+1) =1
nnX
j=1
vj
t+1− ∇fj(xt+1)
=(1−β)1
nnX
j=1
vj
t− ∇fj(xt)
+β1
nnX
j=1
∇fj(xt+1;ξj
t+1)− ∇fj(xt+1)
+ (1−β)1
nnX
j=1
∇fj(xt+1;ξj
t+1)− ∇fj(xt;ξj
t+1) +∇fj(xt)− ∇fj(xt+1)
.
Then we have
E
1
nnX
j=1vj
t+1− ∇f(xt+1)2

≤(1−β)2E
1
nnX
j=1
vj
t− ∇fj(xt)2
+ 2β21
n2nX
j=1E∇fj(xt+1;ξj
t+1)− ∇fj(xt+1)2
+ 2(1−β)21
n2nX
j=1E∇fj(xt+1;ξj
t+1)− ∇fj(xt;ξj
t+1)2
≤(1−β)E
1
nnX
j=1
vj
t− ∇fj(xt)2
+2β2σ2
n+2L2
n∥xt+1−xt∥2
≤(1−β)E
1
nnX
j=1vj
t− ∇f(xt)2
+2β2σ2
n+2L2η2d
n.
By summing up and rearranging, we observe
E
1
TTX
t=11
nnX
j=1vj
t− ∇f(xt)2
≤E1
nPn
j=1vj
1− ∇f(x1)2
βT+2σ2β
n+2L2η2d
nβ
≤σ2
nβT+2σ2β
n+2L2η2d
nβ.(13)
Finally, by setting β=1
2andη=O 
T−1/2d−1/2
, we ensure that
1
TTX
i=1∥∇f(xt)∥1≤∆f
ηT+2dR√n+ηLd
2+ 2√
dvuuuutE
1
TTX
i=1∇f(xt)−1
nnX
j=1vj
t2

≤∆f
ηT+2dR√n+ηLd
2+ 2√
ds
σ2
nβT+2σ2β
n+2L2η2d
nβ
=Od1/2
T1/2+d
n1/2
.
18D Proof of Theorem 4
Due to the fact that the overall objective function f(x)isL-smooth, we have the following:
f(xt+1)≤f(xt) +⟨∇f(xt),xt+1−xt⟩+L
2∥xt+1−xt∥2
≤f(xt)−η*
∇f(xt),S1
1
nnX
j=1SG(ˆvj
t)
+
+η2Ld
2
=f(xt) +η*
∇f(xt),∇f(xt)−S1
1
nnX
j=1SG(ˆvj
t)
+
−η∥∇f(xt)∥2+η2Ld
2.
Taking expectations leads to:
E[f(xt+1)−f(xt)]
≤ηE
*
∇f(xt),1
G∇f(xt)−S1
1
nnX
j=1SG(ˆvj
t)
+
−η
GEh
∥∇f(xt)∥2i
+η2Ld
2
≤ηE
*
∇f(xt),1
G∇f(xt)−1
nnX
j=1SG(ˆvj
t)+
−η
GEh
∥∇f(xt)∥2i
+η2Ld
2
≤ηE
*
∇f(xt),1
G∇f(xt)−1
nGnX
j=1ˆvj
t+
−η
GEh
∥∇f(xt)∥2i
+η2Ld
2
≤ηE
1
2G∥∇f(xt)∥2+1
2G∇f(xt)−1
nnX
j=1ˆvj
t2
−η
GEh
∥∇f(xt)∥2i
+η2Ld
2
≤η
2GE
∇f(xt)−1
nnX
j=1ˆvj
t2
−η
2GEh
∥∇f(xt)∥2i
+η2Ld
2(14)
Rearranging the terms and summing up:
1
TTX
i=1E∇f(xt)∥2
≤2∆fG
ηT+E
1
TTX
i=1∇f(xt)−1
nnX
j=1ˆvj
t2
+ηLdG
≤2∆fG
ηT+E
1
nnX
j=11
TTX
i=1∇fj(xt)−ˆvj
t2
+ηLdG
≤2∆fG
ηT+E
1
nnX
j=11
TTX
i=1∇fj(xt)−ΠGh
vj
ti2
+ηLdG
≤2∆fG
ηT+E
1
nnX
j=11
TTX
i=1∇fj(xt)−vj
t2
+ηLdG.
where the last inequality is due to the non-expansive property of the projection operation.
For each worker j, according to the definition of vj
t, we have:
vj
t+1− ∇fj(xt+1) = (1 −β)
vj
t− ∇fj(xt)
+β
∇fj(xt+1;ξj
t+1)− ∇fj(xt+1)
+ (1−β)
∇fj(xt+1;ξj
t+1)− ∇fj(xt;ξj
t+1) +∇fj(xt)− ∇fj(xt+1)
.
19Then we have
Evj
t+1− ∇fj(xt+1)2
≤(1−β)2Evj
t− ∇fj(xt)2
+ 2β2E
∇fj(xt+1;ξj
t+1)− ∇fj(xt+1)2
+ 2(1−β)2E
∇fj(xt+1;ξj
t+1)− ∇fj(xt;ξj
t+1)2
≤(1−β)Evj
t− ∇fj(xt)2
+ 2β2σ2+ 2L2∥xt+1−xt∥2
≤(1−β)Evj
t− ∇fj(xt)2
+ 2β2σ2+ 2L2η2d.
As a result, we know that
E
1
nnX
j=11
TTX
t=1vj
t− ∇fj(xt)2
≤σ2
βT+ 2σ2β+2L2η2d
β.
Finally, combining the above and setting that β=O 1
T1/2
,η=O 1
d1/2T1/2
, we obtain the final
bound:
E"
1
TTX
i=1∥∇f(xt)∥#
≤vuutE"
1
TTX
i=1∥∇f(xt)∥2#
≤s
2∆fG
ηT+ηLdG +σ2
βT+ 2σ2β+2L2η2d
β
≤ O r
d1/2(∆f+L) +σ2+L2
T1/2!
=Od1/4
T1/4
.
E Results under weaker assumptions
In this section, we demonstrate that our proposed methods can maintain similar convergence rates
under less stringent assumptions — expected α-symmetric generalized-smoothness [Chen et al.,
2023] and affine variance [Faw et al., 2022]. We first detail these relaxed assumptions below.
Assumption 1′(Expected α-symmetric generalized-smoothness)
Eξ
∥∇f(x;ξ)− ∇f(y;ξ)∥2
≤ ∥x−y∥2Eξ"
L0+L1max
θ∈[0,1]∥∇f(xθ;ξ)∥α2#
,
where xθ:=θx+ (1−θ)y, and 0≤α≤1.
Assumption 2′(Affine variance)
Eξ
∥∇f(x;ξ)− ∇f(x)∥2
≤Γ2∥∇f(x)∥2+ Λ2.
Remark: Assumption 1′can be reduced to standard average smoothness (Assumption 1) when
L1= 0. Note that α-symmetric generalized-smooth functions not only include asymmetric and
Hessian-based generalized-smooth functions, but also contain high-order polynomials and exponential
functions [Chen et al., 2023]. Moreover, affine variance is also weaker than Assumption 2 and can be
reduced to it when Γ = 0 .
We then demonstrate that these relaxed conditions are sufficient for our algorithms to achieve the
same convergence rate.
20Theorem 5 Under Assumptions 1′and 2′, by setting that β=O(d1/3
T2/3),η=O(1
d1/6T2/3),B0=
O(1),B1=O(d), and denoting Nas the samples used, our SSVR method guarantees:
E[∥∇f(xτ)∥1]≤ Od1/2
N1/3
.
Furthermore, we introduce the following relaxed assumption for the finite-sum problem.
Assumption 3′(Generalized smoothness) For each i∈ {1,2,···, m}, we have
∥∇fi(x)− ∇fi(y)∥ ≤ ∥ x−y∥
L0+L1max
θ∈[0,1]∥∇f(xθ)∥α
,
where xθ:=θx+ (1−θ)y, and 0≤α≤1.
This assumption is weaker than the standard Assumption 3. We validate that our SSVR-FS algorithm
can still achieve similar convergence under this relaxed condition.
Theorem 6 Under Assumption 3′, by setting η=O(min{1
m1/4d1/2T1/2,1
md}),β=O(1
m), and
I=m, our SSVR-FS algorithm ensures:
E[∥∇F(xφ)∥1]≤ Om1/4d1/2
T1/2+md
T
.
Remark: When the iteration number Tis large, the dominant term becomes O(m1/4d1/2T−1/2),
which aligns with the results in Theorem 2.
E.1 Proof of Theorem 5
We first present some useful tools for analysis. According to Proposition 4 in Chen et al. [2023],
Assumption 1′leads to the following lemmas.
Lemma 1 Forα∈(0,1), we have:
f(xt+1)≤f(xt) +⟨∇f(xt),xt+1−xt⟩
+1
2∥xt+1−xt∥2 
K0+K1∥∇f(xt)∥α+ 2K2∥xt+1−xt∥α
1−α
,
where K0:=L0 
2α2
1−α+ 1
,K1:=L1·2α2
1−α·3α,K2:=L1
1−α
1·2α2
1−α·3α(1−α)α
1−α.
Forα= 1, we also have:
f(xt+1)≤f(xt) +⟨∇f(xt),xt+1−xt⟩
+1
2∥xt+1−xt∥2 
L0+L1∥∇f(xt)∥
exp 
L1∥xt+1−xt∥
,
Similarly, according to the Proposition 4 in Chen et al. [2023], we have the following guarantees.
Lemma 2 Forα∈(0,1), we have:
Eξ∥∇f(x;ξ)− ∇f(y;ξ)∥2≤ ∥x−y∥2 
K0+K1Eξ∥∇f(y;ξ)∥α+K2∥x−y∥α
1−α2.
where K0= 22−α
1−αL0,K1= 22−α
1−αL1,K2= (5L1)1
1−α.
Forα= 1, we also have:
Eξ∥∇f(x;ξ)− ∇f(y;ξ)∥2≤2∥x−y∥2(L2
0+ 2L2
1Eξ∥∇f(y;ξ)∥2) exp(12 L2
1∥x−y∥2).
21Then, we can begin our proof. For α∈(0,1), according to Lemma 1, by setting η≤d−1
2, we have:
f(xt+1)
≤f(xt) +⟨∇f(xt),xt+1−xt⟩
+1
2∥xt+1−xt∥2 
K0+K1∥∇f(xt)∥α+ 2K2∥xt+1−xt∥α
1−α
≤f(xt) +⟨∇f(xt),−ηSign( vt)⟩
+1
2∥xt+1−xt∥2(K0+K1(1 +∥∇f(xt)∥) + 2K2)
≤f(xt) +η⟨∇f(xt),Sign(∇f(xt))−Sign( vt)⟩ −η⟨∇f(xt),Sign(∇f(xt))⟩
+1
2∥xt+1−xt∥2(K0+K1(1 +∥∇f(xt)∥) + 2K2)
=f(xt) +η⟨∇f(xt),Sign(∇f(xt))−Sign( vt)⟩ −η∥∇f(xt)∥1
+1
2∥xt+1−xt∥2((K0+K1+ 2K2) +K1∥∇f(xt)∥)
≤f(xt) + 2η√
d∥∇f(xt)−vt∥ −η∥∇f(xt)∥1+η2d
2((K0+K1+ 2K2) +K1∥∇f(xt)∥),
where the second inequality is due to the fact that α <1and∥xt+1−xt∥2≤η2d≤1.
Rearranging and summing up, we then have
E"
1
TTX
t=1∥∇f(xt)∥1#
≤∆f
ηT+ 2√
d·E"
1
TTX
t=1∥∇f(xt)−vt∥#
+ηd(K0+K1+ 2K2)
2+ηdK 1
2TE"TX
t=1∥∇f(xt)∥#
.
By setting η≤min{1√
d,1
dK1}, we can get
E"
1
TTX
t=1∥∇f(xt)∥1#
≤2∆f
ηT+ 4√
d·E"
1
TTX
t=1∥∇f(xt)−vt∥#
+ηd(K0+K1+ 2K2).
(15)
Forα= 1, according to Lemma 1 and setting η≤1
L1√
d, we have
f(xt+1)
≤f(xt) +⟨∇f(xt),xt+1−xt⟩+1
2∥xt+1−xt∥2 
L0+L1∥∇f(xt)∥
exp 
L1∥xt+1−xt∥
≤f(xt) +⟨∇f(xt),xt+1−xt⟩+exp (1)
2η2d(L0+L1∥∇f(xt)∥)
≤f(xt) + 2η√
d∥∇f(xt)−vt∥ −η∥∇f(xt)∥1+3η2d
2(L0+L1∥∇f(xt)∥),
where the second inequality is due to L1∥xt+1−xt∥ ≤1, and others follow the previous proof.
Rearranging and summing up, we then have
E"
1
TTX
t=1∥∇f(xt)∥1#
≤∆f
ηT+ 2√
d·E"
1
TTX
t=1∥∇f(xt)−vt∥#
+3ηdL0
2+3ηdL1
2TE"TX
t=1∥∇f(xt)∥#
.
By setting η≤1
3dL1, we can get
E"
1
TTX
t=1∥∇f(xt)∥1#
≤2∆f
ηT+ 4√
d·E"
1
TTX
t=1∥∇f(xt)−vt∥#
+ 3ηdL0. (16)
22Then we begin to bound the term Eh
1
TPT
t=1∥∇f(xt)−vt∥i
. According to the definition of vt,
we have:
vt− ∇f(xt) =(1−β) (vt−1− ∇f(xt−1)) +β 
1
B1B1X
k=1∇f(xt;ξk
t)− ∇f(xt)!
+ (1−β) 
1
B1B1X
k=1 
∇f(xt;ξk
t)− ∇f(xt−1;ξk
t)
+∇f(xt−1)− ∇f(xt)!
.
Denote that Gt=
1
B1PB1
k=1 
∇f(xt;ξk
t)− ∇f(xt−1;ξk
t)
+∇f(xt−1)− ∇f(xt)
and∆t=
1
B1PB1
k=1 
∇f(xt;ξk
t)− ∇f(xt)
. By summing up, we have
vt− ∇f(xt)
=(1−β)(vt−1− ∇f(xt−1)) +β∆t+ (1−β)Gt
=···
=(1−β)t−1(v1− ∇f(x1)) +βtX
s=1(1−β)t−s∆s+ (1−β)tX
s=1(1−β)t−sGs.
Thus, we can know that
E[∥vt− ∇f(xt)∥]≤(1−β)t−1E[∥v1− ∇f(x1)∥]
+βE"tX
s=1(1−β)t−s∆s#
+ (1−β)E"tX
s=1(1−β)t−sGs#
.
Then we give the following two important lemmas, and their proofs can be found in Appendix E.1.1
and Appendix E.1.2, respectively.
Lemma 3
E"tX
s=1(1−β)t−s∆s#
≤E
vuutt−rX
s=1(1−β)t−s∆s2
+1
B1rX
s=1(1−β)2s−2Λ2

+tX
s=t+1−rΓ√B1(1−β)t−sE[∥∇f(xs)∥]
Lemma 4
E"tX
s=1(1−β)t−sGs#
≤E
vuutt−rX
s=1(1−β)t−sGs2
+1
B1rX
s=12(1−β)2s−2η2L2
3d

+√
2dηL4√B1tX
s=t+1−r(1−β)t−sE[∥∇f(xs)∥]
23Using these lemmas and setting r=t, we then have
E"tX
s=1(1−β)t−s∆s#
≤vuutΛ2
B1tX
s=1(1−β)2s−2+tX
s=1Γ√B1(1−β)t−sE[∥∇f(xs)∥]
≤Λ√B1β+tX
s=1Γ√B1(1−β)t−sE[∥∇f(xs)∥]
E"tX
s=1(1−β)t−sGs#
≤vuut2η2L2
3d
B1tX
s=1(1−β)2s−2+√
2dηL4√B1tX
s=1(1−β)t−sE[∥∇f(xs)∥]
≤√
2dηL3√B1β+√
2dηL4√B1tX
s=1(1−β)t−sE[∥∇f(xs)∥].
Combining above inequalities and setting β=η√
d, we derive
E"
1
TTX
t=1∥∇f(xt)−vt∥#
≤1
TTX
t=1(1−β)t−1E[∥v1− ∇f(x1)∥] +β1
TTX
t=1E"tX
s=1(1−β)t−s∆s#
+1
TTX
t=1E"tX
s=1(1−β)t−sGs#
≤σ√B01
TTX
t=1(1−β)t−1+Λ√β√B1+√
2dηL3√B1β+ 
βΓ√B1+√
2dηL4√B1!
1
TTX
t=1tX
s=1(1−β)t−sE[∥∇f(xs)∥]
≤σ
βT√
B0+Λ√β√B1+√
2dηL3√B1β+ 
βΓ√B1+√
2dηL4√B1! TX
i=1(1−β)i!
1
TTX
t=1E[∥∇f(xt)∥]
≤σ
βT√
B0+Λ√β√B1+√
2dηL3√B1β+ 
Γ√B1+√
2dηL4√B1β!
1
TTX
t=1E[∥∇f(xt)∥]
≤σ
ηT√B0d+(Λ +√
2L3)d1/4η1/2
√B1+ 
Γ√B1+√
2L4√B1!
1
TTX
t=1E[∥∇f(xt)∥].
Forα∈(0,1), by setting that
B0= 1,
B1≥max{256Γ2,512L2
4}d,
β=d1/3
T2/3,
η=1
d1/6T2/3,
and suppose that iteration number
T≥ O(d2),
24then we can guarantee
E"
1
TTX
t=1∥∇f(xt)∥1#
≤2∆f
ηT+ηd(K0+K1+ 2K2) +4σ
ηT√B0+4(Λ +√
2L3)d3/4η1/2
√B1
+ 
4√
dΓ√B1+4√
2dL4√B1!
1
TTX
t=1E[∥∇f(xt)∥]
≤Od1/6
T1/3
+1
2E"
1
TTX
t=1∥∇f(xt)∥#
≤Od1/6
T1/3
+1
2E"
1
TTX
t=1∥∇f(xt)∥1#
,
which indicates that
E"
1
TTX
t=1∥∇f(xt)∥1#
≤ Od1/6
T1/3
.
Note that the batch size for each iteration is B1=O(d), by assuming that N=B1∗T, we know
that the convergence concerning Nis
Od1/6
T1/3
=Od1/2
(B1T)1/3
=Od1/2
N1/3
.
Similar results can be easily obtained for α= 1, i.e., we can also guarantee the following for α= 1:
E"
1
TTX
t=1∥∇f(xt)∥1#
≤ Od1/6
N1/3
.
E.1.1 Proof of Lemma 3
We prove this lemma by mathematical induction.
1) When r= 0, we have the following:
E"tX
s=1(1−β)t−s∆s#
=E
vuuttX
s=1(1−β)t−s∆s2
,
which satisfies the above lemma.
252) Then, suppose the lemma holds for r=k. Forr=k+ 1, we have
E"tX
s=1(1−β)t−s∆s#
≤E
vuutt−kX
s=1(1−β)t−s∆s2
+1
B1kX
s=1(1−β)2s−2Λ2
+tX
s=t+1−kΓ√B1(1−β)t−sE[∥∇f(xs)∥]
=E
Eξt−k
vuutt−k−1X
s=1(1−β)t−s∆s+ (1−β)k∆t−k2
+1
B1kX
s=1(1−β)2s−2Λ2


+tX
s=t+1−kΓ√B1(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutEξt−kt−k−1X
s=1(1−β)t−s∆s+ (1−β)k∆t−k2
+1
B1kX
s=1(1−β)2s−2Λ2

+tX
s=t+1−kΓ√B1(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutt−k−1X
s=1(1−β)t−s∆s2
+Eξt−kh
(1−β)2k∥∆t−k∥2i
+1
B1kX
s=1(1−β)2s−2Λ2

+tX
s=t+1−kΓ√B1(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutt−k−1X
s=1(1−β)t−s∆s2
+(1−β)2k
B1
Λ2+ Γ2∥∇f(xt−k)∥2
+1
B1kX
s=1(1−β)2s−2Λ2

+tX
s=t+1−kΓ√B1(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutt−k−1X
s=1(1−β)t−s∆s2
+1
B1k+1X
s=1(1−β)2s−2Λ2
+(1−β)kΓ√B1E[∥∇f(xt−k)∥]
+tX
s=t+1−kΓ√B1(1−β)t−sE[∥∇f(xs)∥]
=E
vuutt−k−1X
s=1(1−β)t−s∆s2
+1
B1k+1X
s=1(1−β)2s−2Λ2
+tX
s=t−kΓ√B1(1−β)t−sE[∥∇f(xs)∥],
where the second inequality is due to the Jensen Inequality.
E.1.2 Proof of Lemma 4
We prove this lemma by mathematical induction.
1) When r= 0, we can easily prove EhPt
s=1(1−β)t−sGsi
=E"rPt
s=1(1−β)t−sGs2#
262) Suppose the inequality holds for r=k. Then, for r=k+ 1, we derive
E"tX
s=1(1−β)t−sGs#
≤E
vuutt−kX
s=1(1−β)t−sGs2
+1
B1kX
s=12(1−β)2s−2η2L2
3d

+√
2dηL4√B1tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
Eξt−k
vuutt−kX
s=1(1−β)t−sGs2
+1
B1kX
s=12(1−β)2s−2η2L2
3d


+√
2dηL4√B1tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
Eξt−k
vuutt−k−1X
s=1(1−β)t−sGs+ (1−β)kGt−k2
+1
B1kX
s=12(1−β)2s−2η2L2
3d


+√
2dηL4√B1tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutEξt−kt−k−1X
s=1(1−β)t−sGs+ (1−β)kGt−k2
+1
B1kX
s=12(1−β)2s−2η2L2
3d

+√
2dηL4√B1tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
vuutt−k−1X
s=1(1−β)t−sGs2
+ (1−β)2kEξt−kh
∥Gt−k∥2i
+1
B1kX
s=12(1−β)2s−2η2L2
3d

+√
2dηL4√B1tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutt−k−1X
s=1(1−β)t−sGs2
+2
B1(1−β)2kη2L2
3d+1
B1kX
s=12(1−β)2s−2η2L2
3d

+√
2dηL4(1−β)k
√B1E[∥∇f(xt−k)∥] +√
2dηL4√B1tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
vuutt−k−1X
s=1(1−β)t−sGs2
+1
B1k+1X
s=12(1−β)2s−2η2L2
3d

+√
2dηL4√B1tX
s=t−k(1−β)t−sE[∥∇f(xs)∥],
27where the third inequality is due to the following, for simplify we denote ξt−k=ξ1
t−k, ..., ξB1
t−k:
Eξt−kh
∥Gt−k∥2i
=Eξt−k
1
B2
1B1X
j=1∇f(xt−k;ξj
t−k)− ∇f(xt−k−1;ξj
t−k) +∇f(xt−k−1)− ∇f(xt−k)2

=Eξt−k
1
B2
1B1X
j=1∇f(xt−k;ξj
t−k)− ∇f(xt−k−1;ξj
t−k)2
+1
B1h
∥∇f(xt−k−1)− ∇f(xt−k)∥2i
−2Eξt−k
1
B2
1B1X
j=1D
∇f(xt−k;ξj
t−k)− ∇f(xt−k−1;ξj
t−k),∇f(xt−k)− ∇f(xt−k−1)E

=Eξt−k
1
B2
1B1X
j=1∇f(xt−k;ξj
t−k)− ∇f(xt−k−1;ξj
t−k)2
−1
B1h
∥∇f(xt−k−1)− ∇f(xt−k)∥2i
≤Eξt−k
1
B2
1B1X
j=1∇f(xt−k;ξj
t−k)− ∇f(xt−k−1;ξj
t−k)2

Forα∈(0,1), denoting L2
3= 
K0+K1+K22+K2
1Λ2, and L2
4=K2
1(1 + Γ2), we have:
Eξt−k
1
B2
1B1X
j=1∇f(xt−k;ξj
t−k)− ∇f(xt−k−1;ξj
t−k)2

≤1
B2
1B1X
j=1∥xt−k−xt−k−1∥2
K0+K1Eξt−kh∇f(xt−k;ξj
t−k)αi
+K2∥xt−k−xt−k−1∥α
1−α2
≤η2d
B2
1B1X
j=1
K0+K1+K2+K1Eξt−kh∇f(xt−k;ξj
t−k)i2
≤2η2d
B1 
K0+K1+K22+2η2d
B2
1B1X
j=1K2
1
Eξt−kh∇f(xt−k;ξj
t−k)i2
≤2η2d
B1 
K0+K1+K22+2η2d
B1K2
1 
(1 + Γ2)∥∇f(xt−k)∥+ Λ2
≤2η2dL2
3
B1+2η2dL2
4
B1∥∇f(xt−k)∥2,
where the second inequality holds by setting η≤d−1/2such that ∥xt−k−xt−k−1∥ ≤η√
d≤1.
Forα= 1, denoting L2
3= 3 
L2
0+ 2L2
1Λ2
, and L2
4= 6L2
1(1 + Γ2), we have:
Eξt−k
1
B2
1B1X
j=1∇f(xt−k;ξj
t−k)− ∇f(xt−k−1;ξj
t−k)2

≤2
B2
1B1X
j=1∥xt−k−xt−k−1∥2
L2
0+ 2L2
1Eξt−k∇f(xt−k;ξj
t−k)2
exp
12L2
1∥xt−k−xt−k−1∥2
≤6η2d
B2
1B1X
j=1
L2
0+ 2L2
1Eξt−k∇f(xt−k;ξj
t−k)2
≤6η2d
B1 
L2
0+ 2L2
1 
(1 + Γ2)∥∇f(xt−k)∥2+ Λ2
≤2η2dL2
3
B1+2η2dL2
4
B1∥∇f(xt−k)∥2,
where the second inequality holds by setting η≤1√
12L2
1d, such that ∥xt−k−xt−k−1∥2≤1
12L2
1.
28E.2 Proof of Theorem 6
Similar to Lemma 1 and 2, we can have the following lemma for generalized individual smoothness.
Lemma 5 Forα∈(0,1), generalized individual smoothness (Assumption 3′) leads to
fi(xt+1)≤fi(xt) +⟨∇fi(xt),xt+1−xt⟩
+1
2∥xt+1−xt∥2 
K0+K1∥∇f(xt)∥α+ 2K2∥xt+1−xt∥α
1−α
,
as well as
∥∇fi(xt+1)− ∇fi(xt)∥ ≤ ∥ xt+1−xt∥ 
K0+K1∥∇f(xt)∥α+K2∥xt+1−xt∥α
1−α
.
where K0:=L0 
2α2
1−α+ 1
,K1:=L1·2α2
1−α·3α,K2:=L1
1−α
1·2α2
1−α·3α(1−α)α
1−α.
Lemma 6 Forα= 1, generalized individual smoothness (Assumption 3′) leads to
fi(xt+1)≤fi(xt) +⟨∇fi(xt),xt+1−xt⟩
+1
2∥xt+1−xt∥2 
L0+L1∥∇f(xt)∥
exp 
L1∥xt+1−xt∥
,
as well as
∥∇fi(xt+1)− ∇fi(xt)∥ ≤ ∥ xt+1−xt∥ 
L0+L1∥∇f(xt)∥
exp 
L1∥xt+1−xt∥
.
Then, we can begin our proof. For α∈(0,1), according to Lemma 5, by setting η≤d−1
2, we have:
f(xt+1)−f(xt)
≤1
mmX
i=1fi(xt+1)−1
mmX
i=1fi(xt)
≤1
mmX
i=1⟨∇fi(xt),xt+1−xt⟩+1
2∥xt+1−xt∥2 
K0+K1∥∇f(xt)∥α+ 2K2∥xt+1−xt∥α
1−α
≤ ⟨∇ f(xt),−ηSign( vt)⟩+1
2∥xt+1−xt∥2(K0+K1(1 +∥∇f(xt)∥) + 2K2)
≤η⟨∇f(xt),Sign(∇f(xt))−Sign( vt)⟩ −η⟨∇f(xt),Sign(∇f(xt))⟩
+1
2∥xt+1−xt∥2(K0+K1(1 +∥∇f(xt)∥) + 2K2)
=η⟨∇f(xt),Sign(∇f(xt))−Sign( vt)⟩ −η∥∇f(xt)∥1
+1
2∥xt+1−xt∥2((K0+K1+ 2K2) +K1∥∇f(xt)∥)
≤2η√
d∥∇f(xt)−vt∥ −η∥∇f(xt)∥1+η2d
2((K0+K1+ 2K2) +K1∥∇f(xt)∥),
where the second inequality is due to the fact that α <1and∥xt+1−xt∥2≤η2d≤1.
Rearranging and summing up, we then have
E"
1
TTX
t=1∥∇f(xt)∥1#
≤∆f
ηT+ 2√
d·E"
1
TTX
t=1∥∇f(xt)−vt∥#
+ηd(K0+K1+ 2K2)
2+ηdK 1
2TE"TX
t=1∥∇f(xt)∥#
.
By setting η≤min{1√
d,1
dK1}, we can get
E"
1
TTX
t=1∥∇f(xt)∥1#
≤2∆f
ηT+ 4√
d·E"
1
TTX
t=1∥∇f(xt)−vt∥#
+ηd(K0+K1+ 2K2).
(17)
29Forα= 1, by setting η≤1
3dL1, we can apply the very similar analysis and obtain
E"
1
TTX
t=1∥∇f(xt)∥1#
≤2∆f
ηT+ 4√
d·E"
1
TTX
t=1∥∇f(xt)−vt∥#
+ 3ηdL0. (18)
Then we bound the term Eh
1
TPT
t=1∥∇f(xt)−vt∥i
. According to the definition of vt, we have:
vt− ∇f(xt) =(1−β) (vt−1− ∇f(xt−1)) +β(ht− ∇f(xt))
+ (1−β) 
∇f(xt;ξk
t)− ∇f(xt−1;ξk
t) +∇f(xt−1)− ∇f(xt)
Denote that Gt= 
∇f(xt;ξk
t)− ∇f(xt−1;ξk
t) +∇f(xt−1)− ∇f(xt)
and∆t=ht− ∇f(xt).
By summing up, we have
vt− ∇f(xt)
=(1−β)(vt−1− ∇f(xt−1)) +β∆t+ (1−β)Gt
=(1−β)t−1(v1− ∇f(x1)) +βtX
s=1(1−β)t−s∆s+ (1−β)tX
s=1(1−β)t−sGs.
Thus, we can know that
E[∥vt− ∇f(xt)∥]
≤(1−β)t−1E[∥v1− ∇f(x1)∥] +βE"tX
s=1(1−β)t−s∆s#
+ (1−β)E"tX
s=1(1−β)t−sGs#
≤βE"tX
s=1(1−β)t−s∆s#
+ (1−β)E"tX
s=1(1−β)t−sGs#
,
where the first term vanishes since we use full batch in the first iteration.
Then we give the following two important lemmas, and their proofs can be found in Appendix E.2.1
and Appendix E.2.2, respectively.
Lemma 7
E"tX
s=1(1−β)t−s∆s#
≤E
vuutt−rX
s=1(1−β)t−s∆s2
+rX
s=12(1−β)2s−2η2I2L2
5d

+√
2dηIL 6tX
s=t+1−r(1−β)t−sE[∥∇f(xs)∥]
Lemma 8
E"tX
s=1(1−β)t−sGs#
≤E
vuutt−rX
s=1(1−β)t−sGs2
+rX
s=12(1−β)2s−2η2L2
7d

+√
2dηL8tX
s=t+1−r(1−β)t−sE[∥∇f(xs)∥]
30Using these lemmas and setting r=t, we then have
E"tX
s=1(1−β)t−s∆s#
≤vuut2η2I2L2
5dtX
s=1(1−β)2s−2+√
2dηIL 6tX
s=1(1−β)t−sE[∥∇f(xs)∥]
≤√
2dηIL 5√β+√
2dηIL 6tX
s=1(1−β)t−sE[∥∇f(xs)∥],
as well as
E"tX
s=1(1−β)t−sGs#
≤vuut2η2L2
7dtX
s=1(1−β)2s−2+√
2dηL8tX
s=1(1−β)t−sE[∥∇f(xs)∥]
≤√
2dηL7√β+√
2dηL8tX
s=1(1−β)t−sE[∥∇f(xs)∥].
Combining above inequalities and setting β= 1/mandI=m, we derive
E"
1
TTX
t=1∥∇f(xt)−vt∥#
≤β1
TTX
t=1E"tX
s=1(1−β)t−s∆s#
+1
TTX
t=1E"tX
s=1(1−β)t−sGs#
≤p
2dβηIL 5+√
2dηL7√β+√
2dηβIL 6+√
2dηL81
TTX
t=1tX
s=1(1−β)t−sE[∥∇f(xs)∥]
≤√
2dmη(L5+L7) +√
2dη(L6+L8) TX
i=1(1−β)i!
1
TTX
t=1E[∥∇f(xt)∥]
≤√
2dmη(L5+L7) +√
2dηm(L6+L8)1
TTX
t=1E[∥∇f(xt)∥].
Forα∈(0,1), by setting η= minn
1
m1/4d1/2T1/2,1
8√
2md(L6+L8+1)o
, we can guarantee
E"
1
TTX
t=1∥∇f(xt)∥1#
≤2∆f
ηT+ηd(K0+K1+ 2K2) + 4d√
2mη(L5+L7) + 4d√
2ηm(L6+L8)1
TTX
t=1E[∥∇f(xt)∥]
≤Om1/4d1/2
T1/2+md
T
+1
2E"
1
TTX
t=1∥∇f(xt)∥1#
,
which indicates that Eh
1
TPT
t=1∥∇f(xt)∥1i
≤ O
m1/4d1/2
T1/2+md
T
. Similar results can be easily
obtained for α= 1.
31E.2.1 Proof of Lemma 7
We prove this lemma by mathematical induction.
1) When r= 0, we have the following:
E"tX
s=1(1−β)t−s∆s#
=E
vuuttX
s=1(1−β)t−s∆s2
,
which satisfies the above lemma.
2) Then, suppose the lemma holds for r=k. Forr=k+ 1, we have
E"tX
s=1(1−β)t−s∆s#
≤E
vuutt−kX
s=1(1−β)t−s∆s2
+kX
s=12(1−β)2s−2η2I2L2
5d

+√
2dηIL 6tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
Eit−k
vuutt−k−1X
s=1(1−β)t−s∆s+ (1−β)k∆t−k2
+kX
s=12(1−β)2s−2η2I2L2
5d


+√
2dηIL 6tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutEit−kt−k−1X
s=1(1−β)t−s∆s+ (1−β)k∆t−k2
+kX
s=12(1−β)2s−2η2I2L2
5d

+√
2dηIL 6tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
vuutt−k−1X
s=1(1−β)t−s∆s2
+ (1−β)2kEit−kh
∥∆t−k∥2i
+kX
s=12(1−β)2s−2η2I2L2
5d

+√
2dηIL 6tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutt−k−1X
s=1(1−β)t−s∆s2
+ 2(1−β)2kη2I2L2
5d+kX
s=12(1−β)2s−2η2I2L2
5d

+√
2dηIL 6(1−β)kE[∥∇f(xt−k)∥] +√
2dηIL 6tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
vuutt−k−1X
s=1(1−β)t−s∆s2
+k+1X
s=12(1−β)2s−2η2I2L2
5d

+√
2dηIL 6tX
s=t−k(1−β)t−sE[∥∇f(xs)∥],
32where the third inequality is due to the following:
Eit−kh
∥∆t−k∥2i
=Eit−kh∇fit−k(xt−k)− ∇fit−k(xτ) +∇f(xτ)− ∇f(xt−k)2i
≤Eit−kh∇fit−k(xt−k)− ∇fit−k(xτ)2i
Forα∈(0,1), denoting that
L2
5= 
K0+K1+K22,
L2
6=K2
1,
we have:
Eit−kh∇fit−k(xt−k)− ∇fit−k(xτ)2i
≤ ∥xt−k−xτ∥2 
K0+K1∥∇f(xt−k)∥α+K2∥xt−k−xτ∥α
1−α2
≤η2I2d(K0+K1+K2+K1∥∇f(xt−k)∥)2
≤2η2I2d(K0+K1+K2)2+ 2η2I2dK2
1∥∇f(xt−k)∥2
≤2η2dI2L2
5+ 2η2I2dL2
6∥∇f(xt−k)∥2,
where the second inequality holds by setting
η≤I−1d−1/2
such that
∥xt−k−xτ∥ ≤ηI√
d≤1.
Forα= 1, denoting that
L2
5= 9L2
0
L2
6= 9L2
1,
we have:
Eit−kh∇fit−k(xt−k)− ∇fit−k(xτ)2i
≤2∥xt−k−xτ∥2
L2
0+L2
1∥∇f(xt−k)∥2
exp
L2
1∥xt−k−xτ∥22
≤18η2dI2
L2
0+L2
1∥∇f(xt−k)∥2
≤2η2I2dL2
5+ 2η2I2dL2
6∥∇f(xt−k)∥2,
where the second inequality holds by setting
η≤1p
L2
1I2d,
such that we have
∥xt−k−xt−k−1∥2≤η2I2d≤1
L2
1.
E.2.2 Proof of Lemma 8
We prove this lemma by mathematical induction.
1) When r= 0, we can easily prove that
E"tX
s=1(1−β)t−sGs#
=E
vuuttX
s=1(1−β)t−sGs2
.
332) Suppose the inequality holds for r=k. Then, for r=k+ 1, we derive
E"tX
s=1(1−β)t−sGs#
≤E
vuutt−kX
s=1(1−β)t−sGs2
+kX
s=12(1−β)2s−2η2L2
7d

+√
2dηL8tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
≤E
Eit−k
vuutt−kX
s=1(1−β)t−sGs2
+kX
s=12(1−β)2s−2η2L2
7d


+√
2dηL8tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
Eit−k
vuutt−k−1X
s=1(1−β)t−sGs+ (1−β)kGt−k2
+kX
s=12(1−β)2s−2η2L2
7d


+√
2dηL8tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutEit−kt−k−1X
s=1(1−β)t−sGs+ (1−β)kGt−k2
+kX
s=12(1−β)2s−2η2L2
7d

+√
2dηL8tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
vuutt−k−1X
s=1(1−β)t−sGs2
+ (1−β)2kEit−kh
∥Gt−k∥2i
+kX
s=12(1−β)2s−2η2L2
7d

+√
2dηL8tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
≤E
vuutt−k−1X
s=1(1−β)t−sGs2
+ 2(1−β)2kη2L2
7d+kX
s=12(1−β)2s−2η2L2
7d

+√
2dηL8(1−β)kE[∥∇f(xt−k)∥] +√
2dηL8tX
s=t+1−k(1−β)t−sE[∥∇f(xs)∥]
=E
vuutt−k−1X
s=1(1−β)t−sGs2
+k+1X
s=12(1−β)2s−2η2L2
7d

+√
2dηL8tX
s=t−k(1−β)t−sE[∥∇f(xs)∥],
34where the third inequality is due to the following:
Eit−kh
∥Gt−k∥2i
=Eit−kh∇fit−k(xt−k)− ∇fit−k(xt−k−1) +∇f(xt−k−1)− ∇f(xt−k)2i
=Eit−kh∇fit−k(xt−k)− ∇fit−k(xt−k−1)2i
+h
∥∇f(xt−k−1)− ∇f(xt−k)∥2i
−2Eit−k
∇fit−k(xt−k)− ∇fit−k(xt−k−1),∇f(xt−k)− ∇f(xt−k−1)
=Eit−kh∇fit−k(xt−k)− ∇fit−k(xt−k−1)2i
−h
∥∇f(xt−k−1)− ∇f(xt−k)∥2i
≤Eit−kh∇fit−k(xt−k)− ∇fit−k(xt−k−1)2i
Forα∈(0,1), denoting L2
7= 
K0+K1+K22, and L2
8=K2
1, we have:
Eit−kh∇fit−k(xt−k)− ∇fit−k(xt−k−1)2i
≤ ∥xt−k−xt−k−1∥2 
K0+K1∥∇f(xt−k)∥α+K2∥xt−k−xt−k−1∥α
1−α2
≤η2d(K0+K1+K2+K1∥∇f(xt−k)∥)2
≤2η2d(K0+K1+K2)2+ 2η2dK2
1∥∇f(xt−k)∥2
≤2η2dL2
7+ 2η2dL2
8∥∇f(xt−k)∥2,
where the second inequality holds by setting η≤d−1/2such that ∥xt−k−xt−k−1∥ ≤η√
d≤1.
Forα= 1, denoting L2
7= 9L2
0andL2
4= 9L2
1, we have:
Eit−kh∇fit−k(xt−k)− ∇fit−k(xt−k−1)2i
≤2∥xt−k−xt−k−1∥2
L2
0+L2
1∥∇f(xt−k)∥2
exp
L2
1∥xt−k−xt−k−1∥22
≤18η2d
L2
0+ 2L2
1∥∇f(xt−k)∥2
≤2η2dL2
7+ 2η2dL2
8∥∇f(xt−k)∥2,
where the second inequality holds by setting η≤1√
L2
1d, such that we have ∥xt−k−xt−k−1∥2≤
η2d≤1
L2
1.
F Additional experiments
In this section, we present additional experiments on the CIFAR-10 dataset to validate whether
the proposed SSVR method is sensitive to hyper-parameters such as learning rate η, momentum
parameter β, and batch size. Specifically, we fix the value of βas 0.5 and try different learning rates
from the set {5e−3,1e−3,5e−4,1e−4,5e−5}. Then, we fix the learning rate as 1e−3and
enumerate βfrom the set {0.3,0.5,0.7,0.9,0.99}. The results are reported in Figure 3 and Figure 4,
respectively, which indicate that our method is insensitive to the choice of hyper-parameters within
a certain range. Finally, we also try different batch sizes from the set {64,128,256,512}, and the
results are shown in Figure 5. It can be seen that our algorithm does not necessitate large batches for
convergence and is not sensitive to variations in batch sizes.
350 50 100 150 200
Epoch0.00.51.0Training loss
0 50 100 150 200
Epoch7075808590Testing accuracy
η= 0.005 η= 0.001 η= 0.0005 η= 0.0001 η= 0.00005Figure 3: Results for CIFAR-10 dataset with different learning rates.
0 50 100 150 200
Epoch0.000.250.500.751.00Training loss
0 50 100 150 200
Epoch7075808590Testing accuracy
β= 0.3 β= 0.5 β= 0.7 β= 0.9 β= 0.99
Figure 4: Results for CIFAR-10 dataset with different β.
0 50 100 150 200
Epoch0.000.250.500.751.00Training loss
0 50 100 150 200
Epoch7075808590Testing accuracy
BS=64 BS=128 BS=256 BS=512
Figure 5: Results for CIFAR-10 dataset with different batch sizes.
36NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The claims presented in the abstract and introduction accurately represent the
contributions and scope of the paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The theoretical results demonstrated in the paper rely on specific assumptions,
which have been clearly stated in the main text.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
37Answer: [Yes]
Justification: The paper provides assumptions and proofs for each theoretical result.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The paper discloses the information necessary to reproduce the main experi-
mental results.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
38Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: Due to privacy concerns and ongoing research, we do not include the code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The paper describes the training and testing details.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The paper reports error bars.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
39• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We have provided the relevant information.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conforms with the NeurIPS Code of
Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This is primarily a theoretical paper with no potential negative social impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
40•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The creators or original owners of assets used in the paper are properly credited
and the license and terms of use explicitly are properly respected.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
41•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
42