Pretrained Transformer Efficiently Learns
Low-Dimensional Target Functions In-Context
Kazusato Oko1,3,Yujin Song2,3,Taiji Suzuki2,3,Denny Wu4,5
1University of California, Berkeley,2University of Tokyo,3RIKEN AIP
4New York University,5Flatiron Institute
oko@berkeley.edu ,song-yujin139@g.ecc.u-tokyo.ac.jp ,
taiji@mist.i.u-tokyo.ac.jp ,dennywu@nyu.edu
Abstract
Transformers can efficiently learn in-context from example demonstrations. Most
existing theoretical analyses studied the in-context learning (ICL) ability of trans-
formers for linear function classes, where it is typically shown that the minimizer
of the pretraining loss implements one gradient descent step on the least squares
objective. However, this simplified linear setting arguably does not demonstrate the
statistical efficiency of ICL, since the pretrained transformer does not outperform
directly solving linear regression on the test prompt. In this paper, we study ICL
of a nonlinear function class via transformer with nonlinear MLP layer: given a
class of single-index target functions f∗(x) =σ∗(⟨x,β⟩), where the index features
β∈Rdare drawn from a r-dimensional subspace, we show that a nonlinear trans-
former optimized by gradient descent (with a pretraining sample complexity that
depends on the information exponent of the link functions σ∗) learns f∗in-context
with a prompt length that only depends on the dimension of the distribution of
target functions r; in contrast, any algorithm that directly learns f∗on test prompt
yields a statistical complexity that scales with the ambient dimension d. Our result
highlights the adaptivity of the pretrained transformer to low-dimensional struc-
tures of the function class, which enables sample-efficient ICL that outperforms
estimators that only have access to the in-context data.
1 Introduction
Pretrained transformers [ VSP+17] possess the remarkable ability of in-context learning (ICL)
[BMR+20], whereby the model constructs a predictor from a prompt sequence consisting of pairs of
labeled examples without updating any parameters. A common explanation is that the trained trans-
former can implement a learning algorithm, such as gradient descent on the in-context examples, in
its forward pass [ HvMM+19,DSD+22,VONR+23]. Such explanation has been empirically studied
in synthetic tasks such as linear regression [ GTLV22 ,ASA+22], which has motivated theoretical
analyses of the statistical and computational complexity of ICL in learning simple function classes.
Many recent theoretical works on ICL focus on learning the function class of linear models using
linear transformers trained via empirical risk minimization. In this setting, it can be shown that
minima of the pretraining loss implements one (preconditioned) gradient descent step on the least
squares objective computed on the test prompt [ ZFB23 ,ACDS23 ,MHM23 ]. This implies that the
forward pass of the pretrained transformer can learn linear targets with n≳din-context examples,
hence matching the sample complexity of linear regression on the test prompt. Subsequent works
also studied how the distribution and “diversity” of pretrained tasks affect the ICL solution in similar
problem settings [WZC+23, RPCG23, ZWB24, LLZV+24].
38th Conference on Neural Information Processing Systems (NeurIPS 2024).The motivation of our work is the observation that the simple setting of learning linear models with
linear transformers does not fully capture the statistical efficiency and adaptivity of ICL. Specifically,
•A linear transformer has limited expressivity and hence only implements simple operations in
the forward pass, such as one gradient step for least squares regression. This limits the class of
algorithms that can be executed in-context, and consequently, the pretrained transformer cannot
outperform directly solving linear regression on the test prompt. We therefore ask the question:
With the aid of MLP layer, can a pretrained transformer learn a nonlinear function class in-context,
and outperform baseline algorithms that only have access to the test prompt?
•A key feature of ICL is the adaptivity to structure of the learning problem. For example, [ GTLV22 ]
empirically showed that transformers can match the performance of either ridge regression or
LASSO, depending on parameter sparsity of the target class; [ RPCG23 ] observed that transformers
transitions from a weighted-sum estimator to ridge regression as the number of pretraining tasks
increases. Such adaptivity enables the pretrained model to outperform algorithms that only have
access to the test prompt, which cannot take into account the “prior” distribution of target functions.
Hence a natural question to ask is
Can a pretrained transformer adapt to certain structures of the target function class,
and how does such adaptivity contribute to the statistical efficiency of ICL?
1.1 Our Contributions
1.1.1 Function Class: Gaussian Single-index Models
To address the above questions, we study the in-context learning of arguably the simplest nonlinear
extension of linear regression where a nonlinearity is applied to the response. Specifically, we consider
the following class of single-index target functions, where the t-th pretraining task is constructed as
xt
1,xt
2, . . . ,xt
N,xti.i.d.∼ N (0,Id), yt
i=σt
∗(⟨xt
i,βt⟩) +ςt
i, (1.1)
where σt
∗:R→Ris the unknown link function, and βt∈Rdis the index feature vector which is
drawn from some fixed r-dimensional subspace for some r≪d. The task is to learn this input-output
relation by reading the context (x1, y1, . . . ,xN, yN)and predict the output corresponding to the
query x(See Section 2 for details). This problem setting is based on the following considerations.
•Nonlinearity of function class. Due to the nonlinear link function, single-index targets cannot
be learned by linear transformers. For this class of functions, the statistical efficiency of simple
algorithms has been extensively studied: given a link function with degree Pand information
exponent Q(defined as the index of the smallest non-zero coefficient in the Hermite expansion),
we know that kernel methods require at least n≳dPsamples [ GMMM21 ,DWY21 ], whereas two-
layer neural network trained by gradient descent can achieve better sample complexity n≳dΘ(Q)
[BAGJ21 ,BBSS22 ]. Hence these algorithms, if directly applied to the test prompt, require a
context length polynomial in the ambient dimensionality d, which arguably deviates from practical
settings of ICL where the pretrained transformer learns from a few in-context examples which may
come from high dimensional data space. These algorithms serve as a baseline for comparing the
statistical efficiency of ICL.
•Two types of low-dimensional structures. Prior works on gradient-based feature learning high-
lights the adaptivity of neural network to low-dimensional functions , i.e., single-index model
σ∗(⟨xi,β⟩)that depends on one direction (index features) βinRd[AAM22 ,BES+22,BMZ23 ].
In such setting, the complexity of gradient descent is dominated by the search for direction
β∈Rd, the difficulty of which can be characterized by various computational lower bounds
[DLS22, AAM23, DPVLB24].
Importantly, our pretraining problem setup introduces another notion of low dimensionality : the
“distribution” of target functions is low-dimensional, since the index features for each task βtare
drawn from a rank- rsubspace. This low-dimensionality of function class cannot be exploited
by any algorithms that directly estimate the target function from test prompt, but as we will see,
transformers can adapt to this additional structure via gradient-based pretraining, which reduces
the search problem (for the index features β) tor-dimensional in the in-context phase. Therefore,
when r≪d, we expect pretrained transformers to outperform baseline algorithms on the in-context
examples (kernel methods, neural network, etc.).
2101102
in-context sample size N*102
101
100prediction risk
d=16
d=32
kernel
two-layer NN
pretrained TFFigure 1: In-context generalization error (with
standard deviation) of kernel ridge regression,
neural network + gradient descent, and pre-
trained transformer. The target function is a
polynomial single-index model. We fix r= 8
and vary d= 16,32.Empirical observations. We pretrain a GPT-2 model
[RWC+19] (with the same configurations as the in-
context linear regression setting in [ GTLV22 ]) to learn
the Gaussian single-index task (1.1) with degree-3 link
function, and compare its in-context sample complexity
against baseline algorithms (see Section 4 for details).
In Figure 1 we observe that the pretrained transformer
achieves low prediction risk using fewer in-context ex-
amples than two baseline algorithms: kernel ridge re-
gression, and neural network trained by gradient descent.
Moreover, we observe that unlike the baseline methods,
Transformer achieves an in-context sample complexity
(almost) independent of the ambient dimensionality d.
The goal of this work is to rigorously establish a d-
independent in-context sample complexity in an ideal-
ized theoretical setting for a shallow transformer opti-
mized via gradient-based pretraining.
1.1.2 Main Result: Learning Single-index Models In-Context
We characterize the sample complexity of learning (1.1) in-context, using a nonlinear transformer
optimized by gradient descent. Each single-index task is specified by an unknown index feature
vector β∈Rddrawn from some r-dimensional subspace, and a link function σ∗with degree Pand
information exponent Q≤P; we allow the degree and information exponent to vary across tasks,
to model the scenario where the difficult of pretraining tasks may differ. We show that pretraining
of the nonlinear MLP layer can extract the low-dimensional structure of the function class, and the
attention layer efficiently approximates the nonlinear link function. Our main theorem upper bounds
the in-context generalization error of the pretrained transformer.
Theorem (Informal) .Letf: (x1, y1, . . . ,xN, yN,x)7→ybe a transformer with nonlinear MLP
layer pretrained with gradient descent (Algorithm 1) on the single-index regression task (1.1) . With
probability at least 0.99, the model fachieves in-context prediction risk E|f(x)−f∗(x)|−τ=od(1),
where τis the noise level, if the number of pretraining tasks T, the number of training examples N,
the test prompt length N∗, and the network width msatisfy (we omit polylogarithmic factors)
T≳dΘ(Q), NT ≫T≳dΘ(Q)
| {z }
pretraining, N∗≳rΘ(P)
|{z}
inference, m≳rΘ(P)
|{z}
approximation,
where Q, P are the information exponent and the highest degree of link functions, respectively.
To the best of our knowledge, this is the first end-to-end optimization and statistical guarantee for
in-context regression of this nonlinear function class. We make the following remarks.
•The required sample size for pretraining T, N scale with the ambient dimensionality d. In particular,
the number of pretraining tasks Tis parallel to the complexity of learning a single-index model
with information exponent Qusing a two-layer neural network. On the other hand, the sample
complexity for the in-context phase only depends on the dimensionality of the function class r≪d.
•Note that any estimator that only has access to the in-context examples requires n≳dsamples
to learn the single-index model, as suggested by the information theoretic lower bound (e.g., see
[MM18 ,BKM+19]). Therefore, when r≪d, we see a separation between pretrained transformer
and algorithms that directly learn from the test prompt, such as linear/kernel regression and neural
network + gradient descent. This highlights the adaptivity (via pretraining) of transformers to
low-dimensional structures of the target function class.
•Our analysis of pretraining reveals the following mechanism analogous to [ GHM+23]: the nonlinear
MLP layer extract useful features and adapt to the low-dimensionality of the function class, whereas
the attention layer performs in-context function approximation on top of the learned features.
1.2 Related Works
Theory of in-context learning. Many recent works studied the in-context learning ability of
transformers trained by gradient descent. [ ZFB23 ,ACDS23 ,MHM23 ,WZC+23,ZWB24 ] studied
3the training of linear transformer models to learn linear target functions in-context by implementing
one gradient descent step in the forward pass. Similar optimization results have been established
for looped linear transformers [ GSR+24], transformers with SoftMax attention [ HCL23 ,NDL24 ,
CSWY24a ,CSWY24b ] or nonlinear MLP layer [ KS24 ,LWL+24]. Our problem setting resembles
[KS24 ], where a nonlinear MLP block is used to extract features, followed by a linear attention
layer; the main difference is that we establish end-to-end guarantees on the optimization and sample
complexity for learning a concrete nonlinear function class, whereas [ KS24 ] focused on convergence
of optimization. [ CCS23 ] showed that transformers can learn nonlinear functions in-context via a
functional gradient mechanism, but no statistical and optimization guarantees were given. If we do
not take gradient-based optimization into account, the function class that can be implemented in-
context by transformers has been characterized in many prior works [ BCW+23,GHM+23,ZZYW23 ,
JLLVR24 ,SGS+24,KNS24 ]. These results typically aim to encode specific algorithms (LASSO,
gradient descent, etc.) in the forward pass or directly analyze the Bayes-optimal estimator.
Gradient-based learning of low-dimensional functions. The complexity of learning low-
dimensional functions with neural network has been extensively studied in the feature learning
theory literature. Typical target functions include single-index models [ BAGJ21 ,BES+22,BBSS22 ,
MHPG+23,DNGL23 ,BES+23] and multi-index models [ DLS22 ,AAM22 ,AAM23 ,BBPV23 ].
While a shallow neural network can efficiently approximate such low-dimensional functions, the effi-
ciency of gradient-based training is governed by properties of the nonlinearity σ∗. In the single-index
setting, prior works established a sufficient sample size n≳dΘ(K), where K∈Nis the infor-
mation exponent for algorithms utilizing correlational information [ BAGJ21 ,BBSS22 ,DNGL23 ,
MHWSE23 ], or the generative exponent for algorithms that employ suitable label transformations
[DPVLB24 ,LOSW24 ,ADK+24,JMS24 ]. Moreover, n≍dsamples are information theoretically
necessary without additional structural assumptions; this entails that estimators that only access the
test prompt inevitably pay a sample size that scales with the ambient dimensionality. As we will see,
pretrained transformers can avoid this “curse of dimensionality” by exploiting the low-dimensionality
of the task distribution. Low-dimensional structure of the function class similar to our setting has
been assumed to study the efficiency of transfer learning [ DLS22 ] and multi-task learning [ CHS+23].
2 Problem Setting
Notations. ∥ · ∥ denotes the ℓ2norm for vectors and the ℓ2→ℓ2operator norm for matrices. For a
vector w, we use wa:bfora≤bto denote the vector [wa, wa+1, . . . , w b]⊤.1Ndenotes the all-one
vector of size N. The indicator function of Ais denoted by IA. LetNbe a nonnegative integer; then
[N]denotes the set {n∈Z|1≤n≤N}. For a nonnegative integer i, thei-th Hermite polynomial
is defined as Hei(z) = (−1)iez2
2di
dzie−z2
2. For a set S,Unif( S)denotes the uniform distribution
overS. We denote the unit sphere {x∈Rd| ∥x∥= 1}bySd−1.˜O(·),˜Ω(·)represent O(·)andΩ(·)
notations where polylogarithmic terms are hidden. We write a≲bwhen there exists a constant c
such that a≤cb. If both a≲bandb≲aholds, we write a≍b.
2.1 Data Generating Process
2.1.1 In-context learning
We first introduce the basic setting of ICL [ BMR+20] of simple function classes as investigated
in [GTLV22 ,ASA+22]. In each task, the learner is given a sequence of inputs and outputs
(x1, y1, . . . ,xN, yN,x)referred to as prompt , where xi,x∈Rdandyi∈R. The labeled ex-
amples X= (x1···xN)∈Rd×N,y= (y1··· yN)⊤∈RNare called context , andxis
thequery . Given input distribution x1, . . . ,xN,xi.i.d.∼ D x, the output yiis expressed as
yi=f∗(xi) +ςi, i∈[N],
where f∗is the true function describing the input-output relation and ςii.i.d.∼ D ςis i.i.d. label noise.
Note that f∗also varies across tasks — we assume f∗is drawn i.i.d. from some true distribution Df∗.
In the pretraining phase, we optimize the model parameters given training data from Tdistinct tasks
{(xt
1, yt
1, . . . ,xt
M, yt
M,xt, yt)}T
t=1, which is composed of prompts {(xt
1, yt
1, . . . ,xt
M, yt
M,xt)}T
t=1
and responses {yt}T
t=1for queries {xt}T
t=1, where yt=ft
∗(xt) +ςtandςti.i.d.∼ D ς.
4We say a model learns these functional relations in-context , when the model can predict the output
f∗(x)corresponding to query xby solely examining the context (X,y), without updating model
parameters for each task. Given the pretrained model f(X,y,x;θ)with parameter θwhich predicts
the label of query xfrom context (X,y), we define the expected ICL risk as
RN∗(f):=E[|f(X1:N∗,y1:N∗,x;θ)−y|], (2.1)
where y=f∗(x) +ςand the expectation is taken over the in-context data: x1, . . . ,xN∗,x∼
Dx, f∗∼ D f∗, ς1, . . . , ς N∗, ς∼ D ς. Note that we take the expectation with respect to contexts
(X1:N∗,y1:N∗)∈Rd×N∗×RN∗of length N∗, in order to examine the behavior of ICL at a specific
context length.
2.1.2 Gaussian single-index models
We consider the situation where the true input-output relation is expressed by single-index models ,
i.e., functions that only depend on the direction of the index vector βin the input space. An
efficient learning algorithm should adapt to this feature and identify the relevant subspace from high-
dimensional observations; hence this problem setting has been extensively studied in the deep learning
theory literature [ BL19 ,BES+22,BBSS22 ,MHPG+23,MZD+23] to demonstrate the adaptivity of
gradient-based feature learning.
Assumption 1. Letτ≥0be the noise level. The prompt (x1, y1, . . . ,xN, yN,x)is generated as
x1,x2, . . . ,xN,xi.i.d.∼ D x=N(0,Id), y i=f∗(xi) +ςi, f∗(xi) =σ∗(⟨xi,β⟩),
where ς1, . . . , ς Ni.i.d.∼Unif({−τ, τ}), and the distribution Df∗of the true function f∗is specified as:
1.Index Features. LetSbe an r≤d-dimensional linear subspace of Rd. We draw βuniformly
from the unit sphere S(S)inS, i.e., from S(S):={β|β∈ S,∥β∥= 1}.
2.Link Function. σ∗(z) =PP
i=Qci
i!Hei(z), where 2≤Q≤P. We draw the Hermite coefficients
{ci}P
i=Qfrom any distribution satisfying
E[c2
Q] = Θ d,r(1)̸= 0,PX
i=Qc2
i≤R2
c(a.s.)and(cQ, . . . , c P)̸= (0, . . . , 0) (a.s.). (2.2)
Throughout the paper, we assume that P≪d, randr≪d; specifically, we take P= Θ d,r(1)and
r≲d1/2. Note that the condition r≪dentails that the class of target functions is low-dimensional ,
and as we will see, such structure can be adapted by the transformer via pretraining.
Remark 1. We make the following remarks on the assumption of single-index function class.
•For each task, the target is a single-index model with different index features drawn from some
rank- rsubspace, and different link function with degree at most Pand information exponent
(defined as the index of the lowest degree non-zero coefficient in the Hermite expansion of the link
function, i.e, min{i|ci̸= 0}in this case; see [ BAGJ21 ,DH18 ]) at least Q. This heterogeneity
reflects the situation where the difficulty of learning the input-output relation varies across tasks.
Note that we allow for different distributions of the Hermite coefficients {ci}: for example, we
may set (cQ, . . . , c P)∼Unifn
(cQ, . . . c P)|PP
i=Qc2
i
i!= 1o
(manifold of coefficients satisfying
Ex[f∗(x)] = 1 ),Unif({0,1}P−Q+1\(0, . . . , 0)), orUnif({(1, . . . , 0), . . . , (0, . . . , 1)}).
•The condition Q≥2(i.e., the Hermite expansion of σ∗does not contain constant and linear
terms) ensures that the gradient update detects the entire r-dimensional subspace instead of the
trivial rank-1 component. For generic polynomial σ∗, this assumption can be satisfied by a simple
preprocessing step that subtracts the low-degree components, as done in [DLS22].
2.2 Student Model: Transformer with Nonlinear MLP Layer
We consider a transformer composed of a single-layer self-attention module preceded by an embed-
ding module using a nonlinear multi-layer perceptron (MLP). Let E∈Rde×dNbe an embedding
5matrix constructed from prompt (x1, y1, . . . ,xN, yN,x). A single-layer SoftMax self-attention
module [VSP+17] is given as
fAttn(E;WP,WV,WK,WQ) =E+WPWVE·softmax(WKE)⊤WQE
ρ
,(2.3)
where ρis the temperature, and WK,WQ∈Rdk×de,WV∈Rdv×deandWP∈Rde×dvare the
key, query, value, and projection matrix, respectively. Following prior theoretical works [ ZFB23 ,
ACDS23 ,MHM23 ,KNS24 ], we remove the SoftMax and instead analyze the linear attention
withρ=N; it has been argued that such simplification can reproduce phenomena in practical
transformer training [ ACS+23]. We further simplify the original self-attention module (2.3) by
merging WPWVasWPV∈Rde×deand(WK)⊤WQasWKQ∈Rde×de, and consider the
following parameterization also introduced in [ZFB23, ACDS23, WZC+23],
WPV=
∗ ∗
01×(de−1)v
,WKQ=
K ∗
01×(de−1)∗
, (2.4)
where v∈RandK∈R(de−1)×(de−1). Then, the simplified attention module is written as
˜fAttn(E;WPV,WKQ) =E+WPVE
E⊤WKQE
N
, and we take the right-bottom entry as the
prediction of ycorresponding to query x.
Prior analyses of linear transformers [ ZFB23 ,ACDS23 ,MHM23 ] defined the embedding matrix E
simply as the input-output pairs; however, the combination of linear embedding and liner attention is
not sufficient to learn nonlinear single-index models. Instead, we set de=m+ 1, dN=N+ 1for
m∈Nand construct Eusing an MLP layer:
E=
σ(w⊤
1x1+b1)··· σ(w⊤
1xN+b1)σ(w⊤
1x+b1)
............
σ(w⊤
mx1+bm)··· σ(w⊤
mxN+bm)σ(w⊤
mx+bm)
y1 ··· yN 0
∈R(m+1)×(N+1),(2.5)
where w1, . . . ,wm∈Rdandb1, . . . , b m∈Rare trainable parameters and σ:R→Ris a nonlinear
activation function; we use σ(z) = ReLU( z) = max {z,0}. This is to say, we define the embedding
as the “hidden representation” σ(w⊤x+b)of a width- mtwo-layer neural network. For concise
notation we write W= (w1, . . . ,wm)⊤,b= (b1, . . . , b m)⊤.
Remark 2. We make the following remarks on the considered architecture.
•The MLP embedding layer before attention has been adopted in recent works [ GHM+23,KS24 ,
KNS24 ]. This setting can be interpreted as an idealized version of the mechanism that lower layers
of the transformer construct useful representation, on top of which upper attention layers implement
the in-context learning algorithm.
•Our architecture is also inspired by recent theoretical analyses of gradient-based feature learning,
where it is shown that gradient descent on the MLP layer yields adaptivity to features of the target
function and hence improved statistical efficiency [AAM22, DLS22, BES+22].
Combining the attention ˜fAttn and MLP embedding (2.5) , we can express the model prediction yas
f(X,y,x;W,Γ,b) =*
Γσ(WX +b1⊤
N)y
N,
σ(w⊤
1x+b1)
...
σ(w⊤
mx+bm)
+
, (2.6)
where Γ=vK⊤andσ(WX +b1⊤
N)denotes the m×Nmatrix whose (i, j)-th entry is σ(w⊤
ixj+
bi); see Appendix E for full derivation. We refer to Γas the attention matrix .
2.3 Pretraining: Empirical Risk Minimization via Gradient Descent
We pretrain the transformer (2.6) by the gradient-based learning algorithm specified in Algorithm 1,
which is inspired by the layer-wise training procedure studied in the neural network theory literature
[AAM22 ,DLS22 ,BES+22]. In particular, we update the trainable parameters in a sequential manner.
6Algorithm 1: Gradient-based training of transformer with MLP layer
Input : Learning rate η1, weight decay λ1, λ2, prompt length N1, N2, number of tasks
T1, T2, attention matrix initialization scale γ.
1Initialize w(0)
j∼Unif(Sd−1) (j∈[m]);b(0)
j∼Unif([−1,1]) (j∈[m]);
Γ(0)
j,j∼Unif({±γ}) (j∈[m])andΓ(0)
i,j= 0 ( i̸=j∈[m]).
2Stage I: Gradient descent for MLP layer
3 Draw data {(xt
1, yt
1, . . . ,xt
N1, yt
N1,xt, yt)}T1
t=1with prompt length N1.
4w(1)
j←w(0)
j−η1h
∇w(0)
j1
T1PT1
t=1(yt−f(Xt,yt,xt;W(0),Γ(0),b(0)))2+λ1w(0)
ji
.
5Initialize bj∼Unif([−logd,logd]).
6Stage II: Empirical risk minimization for attention layer
7 Draw data {(xt
1, yt
1, . . . ,xt
N2, yt
N2,xt, yt)}T1+T2
t=T1+1with prompt length N2.
8Γ∗←argminΓ1
T2PT1+T2
t=T1+1(yt−f(Xt,yt,xt;W(1),Γ,b))2+λ2
2∥Γ∥2
F.
Output : Prediction function x→f(X,y,x;W(1),Γ∗,b).
•In Stage I we optimize the parameters of the MLP (embedding) layer, which is a non-convex
problem due to the nonlinear activation function. To circumvent the nonlinear training dynamics,
we follow the recipe in recent theoretical analyses of gradient-based feature learning [ DLS22 ,
BES+22,BEG+22]; specifically, we zoom into the early phase of optimization by taking one
gradient step on on the regularized empirical risk. As we will see, the first gradient step already
provides a reliable estimate of the target subspace S, which enables the subsequent attention layer
to implement a sample-efficient in-context learning algorithm.
•In Stage II we train the attention layer, which is a convex problem and the global minimizer can
be efficiently found. We show that the optimized attention matrix Γperforms regression on the
polynomial basis (defined by the MLP embedding), which can be seen as an in-context counterpart
to the second-layer training (to learn the polynomial link) in [DLS22, AAM23, OSSW24].
3 Transformer Learns Single-index Models In-Context
3.1 Main Theorem
Our main theorem characterizes the pretraining and in-context sample complexity of the transformer
with MLP layer (2.6) optimized by layer-wise gradient-based pretraining outlined in Algorithm 1.
Theorem 1. Given Assumption 1 and r≲d1
2. We pretrain the transformer (2.6) using Algorithm 1
withm=˜Ω(rP), T1=˜Ω(dQ+1rQ),N1T1=˜Ω(d2Q+1r),γ≍1
m3
2r1
2dQ,η1≍m3
2rd2Q−1
2·
(logd)−Cηfor constant Cη. Then, for appropriately chosen regularization parameters λ1, λ2>0,
with probability at least 0.99 over the data distribution and random initialization, the ICL prediction
risk(2.1) with test prompt length N∗for the output fof Algorithm 1 can be upper bounded as
RN∗(f)−τ=˜O s
r3P
m+r4P1
T2+1
N2+1
N∗!
.
Theorem 1 suggests that to achieve low in-context prediction risk, it is sufficient to set T1, N1=
˜Ω(dΘ(Q)), and m, T 2, N2, N∗=˜Ω(rΘ(P)). Observe that the pretraining complexity T1, N1scales
with the ambient dimensionality d, but the in-context sample complexity N∗only scales with the
dimensionality of the function class r≪d. This illustrates the in-context efficiency of pretrained
transformers and aligns with our observations in the GPT-2 experiment reported in Figure 1.
Remark 3. We make the following remarks.
•The sample complexity highlights different roles of the two sources of low dimensionality in our
setting. The low dimensionality of single-index f∗entails that the pretraining cost scales as
N≳dΘ(Q), which is consistent with prior analyses on gradient-based feature learning [ BAGJ21 ,
DNGL23 ]. On the other hand, the low dimensionality of function class (i.e., βlies in r-dimensional
7subspace) leads to an in-context sample complexity that scales as N∗≳rΘ(P), which, roughly
speaking, is the rate achieved by polynomial regression or kernel models on r-dimensional data.
•The multiplicative scaling between N1andT1in the sample complexity suggests that one can
tradeoff between the two quantities, that is, pretraining on more diverse tasks (larger T1) can
reduce the required pretraining context length N1.
•Similar low-dimensional function class has been considered in the setting of transfer or multi-task
learning with two-layer neural networks [ DLS22 ,CHS+23], where the first-layer weights identify
the span of all target functions, and the second-layer approximates the nonlinearity. However,
the crucial difference is that we do not update the network parameters based on the in-context
examples; instead, the single-index learning is implemented by the forward pass of the transformer.
Comparison against baseline methods. Below we summarize the statistical complexity of
commonly-used estimators that only have access to N∗in-context examples, but not the pretraining
data. Note that the in-context sample complexity all depends on the ambient dimensionality d≫r.
•Kernel models. Recall that our target function is a degree- Ppolynomial, and hence kernel ridge
regression requires N∗≳dPin-context examples [GMMM21, DWY21].
•CSQ learners. The correlational statistical query (CSQ) lower bound suggests that an algorithm
making use of correlational information requires N∗≳dΘ(Q)samples to learn a single-index
model with information exponent Q[DLS22 ,AAM22 ]. This sample complexity can be achieved
by online SGD training of shallow neural network [BAGJ21, DNGL23].
•Information theoretic limit. Since the single-index model (1.1) contains dunknown parameters,
we can infer an information theoretic lower bound of N∗≳dsamples to estimate this function. This
complexity can be achieved (up to polylog factors) by tailored SQ algorithms [ CM20 ,DPVLB24 ]
or modified gradient-base training of neural network [LOSW24, ADK+24, JMS24].
3.2 Proof Sketch of Main Theorem
We provide a sketch of derivation for Theorem 1. The essential mechanism is outlined as follows:
after training the MLP embedding via one gradient descent step, the MLP parameters {w(1)
j}align
with the common subspace of the target functions S. Subsequently, the (linear) attention module
estimates the input-output relation f∗(which varies across tasks) on this r-dimensional subspace. We
explain these two ingredients in the ensuing sections.
3.2.1 Training the MLP Layer
We first show that the first gradient descent step on Wresults in significant alignment with the
target subspace S, using a proof strategy pioneered in [ DLS22 ]. Note that under sufficiently small
initialization and appropriately chosen weight decay, we have
w(1)
j≃η1·2
T1PT1
t=1yt∇w(0)
jf(Xt,yt,xt;W(0),Γ(0),b(0)).
The key observation is that this correlation between yand the gradient of model output contains
information of S. We establish the concentration of this empirical gradient around the expected
(population) one, which determines the required rates of T1andN1. For the population gradient,
we make use of the Hermite expansion of σbj(z) =σ(z+bj)and show that its leading term is
proportional to
w(0)
1:r
0d−r
, which is contained in the target subspace S; see Appendix B for details.
3.2.2 Attention Matrix with Good Approximation Property
Next we construct the attention matrix ¯Γwhich satisfies the following approximation property:
Proposition 2 (Informal) .There exists ¯Γsuch that with high probability,
f(Xt,yt,xt;W(1),¯Γ,b)−yt−τ=˜Oq
r3P/m+r2P/N2
for all t∈ {T1+ 1, . . . , T 2}. Moreover, we have ∥¯Γ∥F=˜O 
r2P/m
.
8We provide an intuitive explanation of this construction. Recall that the target function can be written
in its Hermite expansion f∗(x) =PP
i=Qci
i!Hei(⟨x,β⟩). We build an orthonormal basis for f∗as
follows. Let {β1, . . . ,βr}be an orthonormal basis of S. Then, any f∗can be expressed as a linear
combination of functions in H={Qr
j=1Hepj(⟨βj,·⟩)|Q≤p1+···+pr≤P, p1≥0, . . . , p r≥
0}, where Ex∼N(0,Id)[h(x)h′(x)] =Ih=h′holds for h, h′∈ H. We write H={h1, . . . , h BP}with
BP=|H|= Θ( rP), and observe that a two-layer neural network can be constructed to approximate
eachhn. Specifically, there exist vectors a1, . . . ,aBP∈Rmsuch thatPm
j=1an
jσ 
⟨w(1)
j,x⟩+bj
≃
hn(x)for each n∈[BP].
Consequently, we can build the desired attention matrix using coefficients a1, . . . ,aBP. LetA=  
a1···aBP
∈Rm×BPand¯Γ=AA⊤, the attention module can recover the true function as
D
1
N2¯Γσ(W(1)Xt+b1⊤
N2)yt, σ(W(1)x+b)E
=PBP
n=1
1
N2PN2
i=1Pm
j=1an
jσ 
w(1)
j,xi
+bj
yiPm
j=1an
jσ 
w(1)
j,x
+bj
(a)≃PBP
n=1
1
N2PN2
i=1hn(xi)yi
hn(x)(b)≃PBP
n=1E[hn(x)f∗(x)]hn(x) =f∗(x). (3.1)
Roughly speaking, the self-attention architecture computes the correlation between the target function
and basis element hito estimate the corresponding coefficient in this basis decomposition. We
evaluate (a) the approximation error of two-layer neural network in Appendix C.1, and (b) the
discrepancy between the empirical and true correlations in Appendix C.2.
Note that the approximation errors and the norm of ¯Γat this stage scale only with rup to poly-
logarithmic terms; this is because W(1)already identifies the low-dimensional target subspace S.
Consequently, the in-context sample size N∗only scales with the target dimensionality r≪d.
3.2.3 Generalization Error Analysis
Finally, we transfer the learning guarantee from the constructed ¯Γto the (regularized) empirical
risk minimization solution Γ∗. By the equivalence between optimization with L2regularization and
norm-constrained optimization, there exists λ2such that ∥Γ∗∥F≤ ∥¯Γ∥Fand the empirical ICL loss
byΓ∗is no larger than that of ¯Γ. Hence, we can bound the generalization error by Γ∗using a standard
Rademacher complexity bound for norm-constrained transformers provided in Appendix D.1. One
caveat here is that the context length N∗at test time may differ from training time; hence we establish
a context length-free generalization bound, which is discussed in Appendix D.2.
4 Synthetic Experiments
4.1 Experimental Setting
We pretrain a GPT-2 model [ RWC+19] to learn the Gaussian single-index function class (1.1) .
Specifically, we consider the 12-layer architecture (with 22.3M parameters) used in [ GTLV22 ]
for in-context linear regression. The pretraining data is generated from random single-index
models: for each task t, the context {(xt
i, yt
i)}N
i=1is generated as xt
ii.i.d.∼ N (0,Id)and
yt
i=PP
i=Qct
i
i!Hei(⟨xt
i,βt⟩), where (ct
Q, . . . ct
P)i.i.d.∼Unifn
(cQ, . . . c P)|PP
i=Qc2
i
i!= 1o
and
βti.i.d.∼Unif
β|β= [β1, . . . , β r,0, . . . , 0]⊤|,∥β∥= 1	
. See Appendix F for further details.
4.2 Empirical Findings
Ambient dimension-free sample complexity. In Figure 2 we examine how the in-context sample
complexity of the GPT-2 model depends on the ambient dimensionality dand the function class
dimensionality r. For each problem setting the model is pretrained for 100,000steps using the data
of degree P= 4and information exponent Q= 2(see Appendix F for details). In Figure 2(a) we
observe that for fixed r= 8, varying the ambient dimensionality d= 16,32,64leads to negligible
change in the model performance for the in-context phase. In contrast, Figure 2(b) illustrates that for
fixed d, the required sample size N∗scales with the dimensionality of the function class r= 2,4,8.
This confirms our theoretical finding that transformers can adapt to low-dimensional structure of the
distribution of target functions via gradient-based pretraining.
90 50 100 150 200 250
in-context sample size N*0.00.20.40.60.81.01.21.4prediction riskd=16,r=8
d=32,r=8
d=64,r=8(a) ICL risk – varying d, fixed r.
0 50 100 150 200 250
in-context sample size N*0.00.20.40.60.81.01.2prediction riskd=64,r=2
d=64,r=4
d=64,r=8 (b) ICL risk – fixed d, varying r.
Figure 2: In-context sample complexity of GPT-2 model pretrained on Gaussian single-index function (see
Section 4.1 for details) of degree-4 polynomial. Observe that (a)the ICL risk curve overlaps for different
ambient dimensions dbut the same target (subspace) dimensionality r, and (b)the required sample size N∗
becomes larger as rincreases.
Superiority over baseline algorithms. In Figure 1 we compare the in-context sample complexity
of the GPT-2 model pretrained by data of Q= 3 andP= 2 against two baseline algorithms
that directly learn f∗on the test prompt: (i)kernel ridge regression with the Gaussian RBF ker-
nelk(x,x′) = exp 
−∥x−x′∥2/σ2
, and (ii)two-layer neural network with ReLU activation
fNN(x) =1
mPm
i=1aiσ(⟨x,wi⟩)trained by the Adam optimizer [ KB15 ]. We observe that for
r= 8, d= 16 ,32, the pretrained transformer outperforms both KRR and two-layer NN; more-
over, the performance of these two baseline algorithms deteriorates significantly as the ambient
dimensionality dbecomes larger.
5 Conclusion and Future Direction
We study the complexity of in-context learning for the Gaussian single-index models using a pre-
trained transformer with nonlinear MLP layer. We provide an end-to-end analysis of gradient-based
pretraining and establish a generalization error bound that takes into account the number of pre-
training tasks, the number of pretraining and in-context examples, and the network width. Our
analysis suggests that when the distribution of target functions exhibits low-dimensional structure,
transformers can identify and adapt to such structure during pretraining, whereas any algorithm that
only has access to the test prompt necessarily requires a larger sample complexity.
We outline a few limitations and possible future directions.
•The in-context sample complexity we derived rΘ(P)corresponds to that of polynomial regression
or kernel methods in r-dimensional space. This rate is natural as discussed in Section 3.2.2, where
the linear self-attention module extracts the coefficients with respect to fixed basis functions (of size
rΘ(P)). An interesting question is whether transformers can implement a more efficient in-context
algorithm that matches the complexity of gradient-based feature learning in rdimensions. This can
be achieved if the pretrained model learns features in-context.
• Our pretraining complexity is based on one GD step analysis similar to [DLS22, BES+22] which
makes use of the correlational information; hence the information exponent of the link functions
plays an important role. We conjecture that the sample complexity can be improved if we modify
the pretraining procedure or training objective, as done in [ DTA+24,LOSW24 ,ADK+24,JMS24 ].
Acknowledgement
KO was partially supported by JST, ACT-X Grant Number JPMJAX23C4. TS was partially supported
by JSPS KAKENHI (24K02905) and JST CREST (JPMJCR2015). This research is unrelated to
DW’s work at xAI.
10References
[AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-
staircase property: a necessary and nearly sufficient condition for sgd learning of
sparse functions on two-layer neural networks. In Conference on Learning Theory ,
pages 4782–4887. PMLR, 2022.
[AAM23] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. SGD learning on
neural networks: leap complexity and saddle-to-saddle dynamics. In Conference on
Learning Theory , pages 2552–2623. PMLR, 2023.
[ACDS23] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn
to implement preconditioned gradient descent for in-context learning. Advances in
Neural Information Processing Systems , 36, 2023.
[ACS+23]Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit
Sra. Linear attention is (maybe) all you need (to understand transformer optimization).
arXiv preprint arXiv:2310.01082 , 2023.
[ADK+24]Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan.
Repetita iuvant: Data repetition allows sgd to learn high-dimensional multi-index
functions. arXiv preprint arXiv:2405.15459 , 2024.
[ASA+22]Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What
learning algorithm is in-context learning? investigations with linear models. arXiv
preprint arXiv:2211.15661 , 2022.
[BAGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient
descent on non-convex losses from high-dimensional inference. The Journal of
Machine Learning Research , 22(1):4788–4838, 2021.
[BBPV23] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning Gaussian multi-
index models with gradient flow. arXiv preprint arXiv:2310.19793 , 2023.
[BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index
models with shallow neural networks. Advances in Neural Information Processing
Systems , 35, 2022.
[BCW+23]Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statis-
ticians: Provable in-context learning with in-context algorithm selection. Advances in
Neural Information Processing Systems , 36, 2023.
[BEG+22]Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril
Zhang. Hidden progress in deep learning: Sgd learns parities near the computational
limit. Advances in Neural Information Processing Systems , 35, 2022.
[BES+22]Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang.
High-dimensional asymptotics of feature learning: How one gradient step improves
the representation. Advances in Neural Information Processing Systems , 35, 2022.
[BES+23]Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Learning
in the presence of low-dimensional structure: A spiked random matrix perspective.
Advances in Neural Information Processing Systems , 36, 2023.
[BKM+19]Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová.
Optimal errors and phase transitions in high-dimensional generalized linear models.
Proceedings of the National Academy of Sciences , 116(12):5451–5460, 2019.
[BL19] Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order
approximation of wide neural networks. arXiv preprint arXiv:1910.01619 , 2019.
[BMR+20]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. Advances in Neural Information Processing
Systems , 33, 2020.
11[BMZ23] Raphaël Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in
two-layers neural networks. arXiv preprint arXiv:2303.00055 , 2023.
[CCS23] Xiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient
descent to learn non-linear functions in context. arXiv preprint arXiv:2312.06528 ,
2023.
[CHS+23]Liam Collins, Hamed Hassani, Mahdi Soltanolkotabi, Aryan Mokhtari, and Sanjay
Shakkottai. Provable multi-task representation learning by two-layer relu neural
networks. arXiv preprint arXiv:2307.06887 , 2023.
[CM20] Sitan Chen and Raghu Meka. Learning polynomials in few relevant dimensions. In
Conference on Learning Theory , pages 1161–1227. PMLR, 2020.
[CSWY24a] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of
multi-head softmax attention for in-context learning: Emergence, convergence, and
optimality. arXiv preprint arXiv:2402.19442 , 2024.
[CSWY24b] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Unveiling induction
heads: Provable training dynamics and feature learning in transformers. arXiv preprint
arXiv:2409.10559 , 2024.
[DH18] Rishabh Dudeja and Daniel Hsu. Learning single-index models in gaussian space. In
Conference On Learning Theory , pages 1887–1930. PMLR, 2018.
[DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn
representations with gradient descent. In Conference on Learning Theory , pages
5413–5452. PMLR, 2022.
[DNGL23] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D. Lee. Smoothing the landscape
boosts the signal for SGD: Optimal sample complexity for learning single index
models. Advances in Neural Information Processing Systems , 36, 2023.
[DPVLB24] Alex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna. The com-
putational complexity of learning gaussian single-index models. arXiv preprint
arXiv:2403.05529 , 2024.
[DSD+22]Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei.
Why can gpt learn in-context? language models implicitly perform gradient descent as
meta-optimizers. arXiv preprint arXiv:2212.10559 , 2022.
[DTA+24]Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborová, and
Florent Krzakala. The benefits of reusing batches for gradient descent in two-layer
networks: Breaking the curse of information and leap exponents. arXiv preprint
arXiv:2402.03220 , 2024.
[DWY21] Konstantin Donhauser, Mingqi Wu, and Fanny Yang. How rotational invariance
of common kernels prevents generalization in high dimensions. In International
Conference on Machine Learning , pages 2804–2814. PMLR, 2021.
[GHM+23]Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and
Yu Bai. How do transformers learn in-context beyond simple functions? a case study
on learning with representations. arXiv preprint arXiv:2310.10616 , 2023.
[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Lin-
earized two-layers neural networks in high dimension. The Annals of Statistics ,
49(2):1029–1054, 2021.
[GSR+24]Khashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv
Kumar. Can looped transformers learn to implement multi-step gradient descent for
in-context learning? In International Conference on Machine Learning , 2024.
[GSS21] Friedrich Götze, Holger Sambale, and Arthur Sinulis. Concentration inequalities for
polynomials in α-sub-exponential random variables. Electronic Journal of Probability ,
26:1 – 22, 2021.
12[GTLV22] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can
transformers learn in-context? a case study of simple function classes. Advances in
Neural Information Processing Systems , 35, 2022.
[HCL23] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers.
arXiv preprint arXiv:2310.05249 , 2023.
[HvMM+19]Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott
Garrabrant. Risks from learned optimization in advanced machine learning systems.
arXiv preprint arXiv:1906.01820 , 2019.
[JLLVR24] Hong Jun Jeon, Jason D Lee, Qi Lei, and Benjamin Van Roy. An information-theoretic
analysis of in-context learning. arXiv preprint arXiv:2401.15530 , 2024.
[JMS24] Nirmit Joshi, Theodor Misiakiewicz, and Nathan Srebro. On the complexity of learning
sparse functions with statistical and gradient queries. arXiv preprint arXiv:2407.05622 ,
2024.
[KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In
International Conference on Learning Representations , 2015.
[KNS24] Juno Kim, Tai Nakamaki, and Taiji Suzuki. Transformers are minimax optimal
nonparametric in-context learners. arXiv preprint arXiv:2408.12186 , 2024.
[KS24] Juno Kim and Taiji Suzuki. Transformers learn nonlinear features in context: Noncon-
vex mean-field dynamics on the attention landscape. arXiv preprint arXiv:2402.01258 ,
2024.
[LLZV+24]Yue M Lu, Mary I Letey, Jacob A Zavatone-Veth, Anindita Maiti, and Cengiz Pehle-
van. Asymptotic theory of in-context learning by linear attention. arXiv preprint
arXiv:2405.11751 , 2024.
[LOSW24] Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns
low-dimensional polynomials with sgd near the information-theoretic limit. arXiv
preprint arXiv:2406.01581 , 2024.
[LWL+24]Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. How do
nonlinear transformers learn and generalize in in-context learning? In International
Conference on Machine Learning , 2024.
[Mau16] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In
Algorithmic Learning Theory: 27th International Conference, ALT 2016, Bari, Italy,
October 19-21, 2016, Proceedings 27 , pages 3–17. Springer, 2016.
[MHM23] Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient
descent is provably the optimal in-context learner with one layer of linear self-attention.
arXiv preprint arXiv:2307.03576 , 2023.
[MHPG+23]Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and
Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations
with SGD. In International Conference on Learning Representations , 2023.
[MHWSE23] Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, and Murat A Erdogdu. Gradient-
based feature learning under structured data. Advances in Neural Information Process-
ing Systems , 36, 2023.
[MM18] Marco Mondelli and Andrea Montanari. Fundamental limits of weak recovery with
applications to phase retrieval. In Conference On Learning Theory , pages 1445–1450.
PMLR, 2018.
[MZD+23]Arvind Mahankali, Haochen Zhang, Kefan Dong, Margalit Glasgow, and Tengyu Ma.
Beyond ntk with vanilla gradient descent: A mean-field analysis of neural networks
with polynomial width, samples, and time. Advances in Neural Information Processing
Systems , 36, 2023.
13[NDL24] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal
structure with gradient descent. arXiv preprint arXiv:2402.14735 , 2024.
[OSSW24] Kazusato Oko, Yujin Song, Taiji Suzuki, and Denny Wu. Learning sum of diverse
features: computational hardness and efficient gradient-based training for ridge combi-
nations. arXiv preprint arXiv:2406.11828 , 2024.
[RPCG23] Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diver-
sity and the emergence of non-bayesian in-context learning for regression. Advances
in Neural Information Processing Systems , 36, 2023.
[RWC+19]Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
Language models are unsupervised multitask learners. 2019.
[SGS+24]Michael E Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, and Gabriel Peyré.
How do transformers perform in-context autoregressive learning? arXiv preprint
arXiv:2402.05787 , 2024.
[Ver18] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications
in Data Science . Cambridge Series in Statistical and Probabilistic Mathematics.
Cambridge University Press, 2018.
[VONR+23]Johannes V on Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexan-
der Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn
in-context by gradient descent. In International Conference on Machine Learning ,
pages 35151–35174. PMLR, 2023.
[VSP+17]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,
U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems , volume 30. Curran
Associates, Inc., 2017.
[Wai19] Martin J Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint .
Cambridge University Press, 2019.
[WZC+23]Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and
Peter L Bartlett. How many pretraining tasks are needed for in-context learning of
linear regression? arXiv preprint arXiv:2310.08391 , 2023.
[ZFB23] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear
models in-context. arXiv preprint arXiv:2306.09927 , 2023.
[ZWB24] Ruiqi Zhang, Jingfeng Wu, and Peter L Bartlett. In-context learning of a linear
transformer block: Benefits of the mlp component and one-step gd initialization. arXiv
preprint arXiv:2402.14951 , 2024.
[ZZYW23] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how
does in-context learning learn? bayesian model averaging, parameterization, and
generalization. arXiv preprint arXiv:2305.19420 , 2023.
14Table of Contents
1 Introduction 1
1.1 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Problem Setting 4
2.1 Data Generating Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Student Model: Transformer with Nonlinear MLP Layer . . . . . . . . . . . . . . 5
2.3 Pretraining: Empirical Risk Minimization via Gradient Descent . . . . . . . . . . . 6
3 Transformer Learns Single-index Models In-Context 7
3.1 Main Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.2 Proof Sketch of Main Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Synthetic Experiments 9
4.1 Experimental Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2 Empirical Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5 Conclusion and Future Direction 10
A Preliminaries 16
A.1 Definition of High Probability Event . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Tensor Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Hermite Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.4 Other Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B Proofs for the MLP Layer 19
B.1 Calculation of Population Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.2 Concentration of Empirical Gradient . . . . . . . . . . . . . . . . . . . . . . . . . 24
C Construction of Attention Matrix 29
C.1 Approximation Error of Two-layer Neural Network . . . . . . . . . . . . . . . . . 29
C.2 Concentration of Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.3 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
D Generalization Error Analysis and Proof of Theorem 1 37
D.1 Rademacher Complexity Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.2 Prompt Length-free Generalization Bound . . . . . . . . . . . . . . . . . . . . . . 40
D.3 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
E Derivation of Simplified Self-attention Module 42
F Details of Experiments 43
F.1 Detailed Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
F.2 Experiment on Our Architecture and Algorithm 1 . . . . . . . . . . . . . . . . . . 44
15A Preliminaries
We consider the high-dimensional setting, i.e., our result holds for all d≥Dwhere Dis a constant
which does not depend on dandr. Throughout the proofs, we take S={(x1, . . . , x r,0, . . . , 0)⊤|
x1, . . . , x r∈R}andβ∼Unif(S(S)) = Unif( {(β1, . . . , β r,0, . . . , 0)⊤|β2
1+···+β2
r= 1}).
Note that this assumption is without loss of generality by the rotational invariance of the Gaussian
distribution.
A.1 Definition of High Probability Event
Definition 3. We say that an event Aoccurs with high probability when there exists a sufficiently
large constant C∗which does not depend on d, rand
1−Pr[A]≤O(d−C∗)
holds.
We implicitly assume that we can redefine the constant C∗to be sufficiently large as needed. The
lemma below is a basic example and will be used in the proofs:
Lemma 4. Letx∼ N(0,1). Then, |x|≲√logdholds with high probability.
Proof. From the tail bound of Gaussian, P(|x| ≥t)≤2 exp ( −t2/2)holds. Hence we get
P(|x| ≥√2C∗logd)≤O(d−C∗).
Note that C∗can be redefined by changing the hidden constant in |x|≲√logd.
IfA1, . . . , A Moccurs with high probability where M=O(poly( d)), then A1∩ ··· ∩ AMalso
occurs with high probability (by redefining C∗). In particular, throughout this paper we assume that
m, N 1, N2, T1, T2=O(poly( d)), which allows us to take such unions.
A.2 Tensor Notations
In this paper, a k-tensor is a multidimensional array which has kindices: for example, matrices are
2-tensors. Let Abe ak-tensor. Ai1,...,ikdenotes (i1, . . . , i k)-th entry of A. LetAbe ak-tensor and
Bbe al-tensor where k≥l.A(B)denotes a k−ltensor whose (i1, . . . , i k−l)-th entry is
A(B)i1,...,ik−l=X
j1,...,jlAi1,...,ik−l,j1,...,jlBj1,...,jl,
and is defined only when sizes are compatible. If k=l, we sometimes write A(B)asA◦B
or⟨A,B⟩. Letv∈Rdbe a vector and kbe a positive integer. Then, v⊗k∈Rd×···× ddenotes a
k-tensor whose (i1, . . . , i k)-th entry is vi1···vik.
Letf(x) :Rd→Rbe ad-variable differentiable function. A k-tensor ∇kf(x)is defined as
 
∇kf(x)
i1,...,ik=∂
∂xi1···∂
∂xikf(x).
The following properties can be verified easily.
Lemma 5. For tensors AandB,∥A(B)∥2
F≤ ∥A∥2
F∥B∥2
Fholds.
Lemma 6. For a vector v,∥v⊗k∥2
F= (∥v∥2
2)kholds.
A.3 Hermite Polynomials
We frequently use (probablists’) Hermite polynomials, which is defined as
Hei(z) = (−1)iez2
2di
dzie−z2
2,
where iis a nonnegative integer. We often make use of the orthogonality property:
Ez∼N(0,1)[Hei(z)Hej(z)] = i!δi,j. The Hermite expansion for σ:R→Ris defined as
σ(z) =P
i≥0ai
i!Hei(z)where ai=Ez∼N(0,1)[σ(z)Hei(z)]. Similarly, the multivariate Hermite
16expansion for f:Rd→Ris defined as f(z) =P
i1≥0,...,id≥0ai1,...,id
(i1)!···(id)!Hei1(z1)···Heid(zd),
where ai1,...,id=Ez1,...,z d∼N(0,1)[f(z)Hei1(z1)···Heid(zd)]. The coefficient ai1,...,idcan also be
obtained by ai1,...,id=Ez1,...,z d∼N(0,1)
∂i1
∂zi1
1···∂id
∂zid
df(z)
.
Consider f∗in our problem setting (Assumption 1). We can bound E[f∗(x)2]andE[f∗(x)4]as
follows.
Lemma 7. Under Assumption 1, Ex,f∗[f∗(x)2] = Θ d,r(1)andEx,f∗[f∗(x)4] =Od,r(1)holds.
Proof. From (2.2) and P= Θ d,r(1),
Ex∼N(0,Id),f∗∼Df∗[f∗(x)2] =Ez∼N(0,1),{ci}

PX
i=Qci
i!Hei(z)
2

=E
PX
i=Qc2
i
i!

= Θ d,r(1).
We can bound Ex[f∗(x)4] =Ez∼N(0,1),{ci}PP
i=Qci
i!Hei(z)4
naively as follows: let ζ4
P=
max Q≤i≤PEz∼N(0,1)[Hei(z)4].ζPis anO(1)quantity depending only on PandQ. Then,
Ez∼N(0,1),{ci}

PX
i=Qci
i!Hei(z)
4

≤X
Q≤i,j,k,l≤PE[|cicjckcl|]
i!j!k!l!E[|Hei(z)Hej(z)Hek(z)Hel(z)|]
≤X
Q≤i,j,k,l≤PR4
cEz[Hei(z)4]1/4Ez[Hej(z)4]1/4Ez[Hek(z)4]1/4Ez[Hel(z)4]1/4
≤P4R4
cζ4
P.
The lemma below is useful to find a basis of the set of single-index functions.
Lemma 8. Suppose β∈S(S). Then,
Hep(⟨x,β⟩) =p1+···+pr=pX
p1≥0,...,p r≥0p!
p1!···pr!·βp1
1···βpr
r·Hep1(x1)···Hepr(xr)
holds.
Proof. Note that Ex1,...,x r∼N(0,1)
∂i1
∂xi1
1···∂ir
∂xirrHep(⟨x,β⟩)
is nonzero only when
i1+···+ir=p, because∂i1
∂xi1
1···∂ir
∂xirrHep(⟨x,β⟩) = p(p−1)···(p−(i1+···+ir) +
1)βi1
1···βirrHep−(i1+···+ir)(⟨x,β⟩)andEx1,...,x r∼N(0,1)[Heq(⟨x,β⟩)] =Ez∼N(0,1)[Heq(z)] = 0
ifq >0. When i1+···+ir=p, thenEx1,...,x r∼N(0,1)
∂i1
∂xi1
1···∂ir
∂xirrHep(⟨x,β⟩)
=p!βi1
1···βirr
holds. Thus, from the multivariate Hermite expansion we obtain the claim.
Corollary 9. Letf∗(x) =PP
i=Qci
i!Hei(⟨x,β⟩). Then, Ex[∇kf∗(x)] =ckβ⊗kifQ≤k≤Pand
otherwise it is the zero tensor.
17A.4 Other Auxiliary Lemmas
Lemmas on random vectors. Letχ(d)be the distribution ofp
z2
1+···+z2
dwhere zii.i.d.∼
N(0,1). Note that the Gaussian vector v∼ N (0, Id)can be decomposed as v=zwwhere
z∼χ(d)andw∼Unif(Sd−1), because the norm and the direction of the Gaussian vector are
independent of each other.
Lemma 10 (Example 2.11 in [Wai19]) .Forz∼χ(d)and0< t < 1,
P[|z2−d| ≥dt]≤2 exp(−dt2/8)
holds.
Lemma 11. LetC >0. Forw∼Unif(Sd−1),∥w1:r∥2≤4Cr
dlogdholds with probability at least
1−2 exp(−d/32)−2rd−C(then with high probability).
Proof. Letz2∼χ2(d)be independent from w. We observe that the distribution of z2∥w1:r∥2is the
χ2(r)-distribution. First, from Lemma 10, with probability at least 1−2 exp(−d/32),z2≥d/2
holds. Moreover, we can say that (z′)2∼χ2(r)satisfies (z′)2≤2Crlogdwith high probability
for sufficiently large constant: let us decompose (z′)2=v2
1+···+v2
rwhere vi∼ N(0,1); from
Lemma 4, v2
i≤2Clogdholds with probability at least 1−2d−C, and thus (z′)2≤2Crlogdholds
with probability at least 1−2rd−C. Taking the union bound yields the result.
Lemma 12. Fork≥1andr≤d,
Ew∼Sd−1[∥w1:r∥2k] =Od,rr
dk
holds.
Proof. Since∥w1:r∥2is a polynomial of w, Lemma 24 in [ DLS22 ] yields Ew∼Sd−1[∥w1:r∥2k]≲
Ew∼Sd−1[∥w1:r∥4]k/2. Here, Ew∼Unif( Sd−1)[∥w1:r∥4] =Evi∼N(0,1)[(v2
1+···+v2
r)2]
Ez∼χ4(d)[z]=3r+r(r−1)
d(d+2)=
Θ(r2/d2)yields the result.
Lemma 13 (Sub Gaussian vector) .Letxbe ad-dimensional random vector and suppose that ⟨u,x⟩
is zero-mean σ2-sub Gaussian for any u∈Rdsuch that ∥u∥= 1. Then, ∥x∥=˜O(σ√
d)holds with
high probabillity.
Lemma 14. LetXbe aσ2- sub Gaussian random variable and Ybe a bounded random variable
satisfying |Y| ≤Ralmost surely. Then, XY−E[XY]isO(σ2R2)-sub Gaussian.
Proof. XY isσ2R2-Gaussian by examining the tail probability. Hence, XY−E[XY]is
O(σ2R2)-sub Gaussian from Lemma 2.6.8 in [Ver18].
Polynomial concentration. We frequently use the following bounds on an output data y(which is
a polynomial of x).
Lemma 15 (Theorem 1.5 in [ GSS21 ]).Letx∼ N(0,In)andfbe a degree- D n-variable polyno-
mial function. Then, for all t≥0it holds that
P(|f(x)−Ex[f(x)]| ≥t)≤2 exp 
−1
CDM2min
1≤k≤Dt
∥Ex[∇kf(x)]∥F2/k!
,
where Mis an absolute constant and CDis a constant depending only on D(if∥Ex[∇kf(x)]∥F= 0
then we ignore the term with k).
Lemma 16. Letx∼ N(0, Id)andy=f∗(x) +ς=PP
i=Qci
i!Hei(⟨x,β⟩) +ςwhere ciandβare
drawn according to Assumption 1, and ς∼Unif{±τ}. Then, for any fixed {ci}andβon the support
of their distributions, it holds that
P(|y| ≥t+τ)≤2 exp 
−1
CPM2min
2≤k≤Pt
Rc2/k!
,
where Mis an absolute constant and CPis a constant depending only on P.
18Proof. Note that from Corollary 9, Ex[f∗(x)] = 0 ,Ex[∇f(x)] =0andEx[∇if∗(x)] =ci(β)⊗i;
by (2.2), Lemma 6 and ∥β∥= 1,∥Ex[∇if∗(x)]∥F≤Rcholds. Lemma 15 yields the result.
Corollary 17. Fix any {ci}andβon the support of their distributions. Then, |f∗(x)|,|y|≲
(logd)P/2holds with high probability over the distribution of xandς.
B Proofs for the MLP Layer
This section analyzes the behavior of MLP weight wafter one gradient descent step (line 4 in
Algorithm 1), as sketched in Section 3.2.1.
We set λ1=η−1
1. From line 4 of Algorithm 1, for each j∈[m],
w(1)
j= 2η11
T1T1X
t=1yt∇w(0)
jf(Xt,yt,xt;W(0),Γ(0),b(0))
−2η11
T1T1X
t=1f(Xt,yt,xt;W(0),Γ(0),b(0))∇w(0)
jf(Xt,yt,xt;W(0),Γ(0),b(0))
= 2η1Γ(0)
j,jT1X
t=11
T1ytσbj(w(0)
j⊤xt)1
N1N1X
i=1yt
iσ′
bj(w(0)
j⊤xt
i)xt
i
+T1X
t=11
T1ytσ′
bj(w(0)
j⊤xt)xt1
N1N1X
i=1yt
iσbj(w(0)
j⊤xt
i)
(B.1)
−2η11
T1T1X
t=1f(Xt,yt,xt;W(0),Γ(0),b(0))∇w(0)
jf(Xt,yt,xt;W(0),Γ(0),b(0)).
holds where σbj(z):=σ(z+bj). Here, xt
iandyt
iis the input and output of i-th context at t-th task,
respectively. First we show that the final term is sufficiently small, if we set γ=|Γ(0)
j,j|sufficiently
small.
Lemma 18. With high probability over the distribution of {(Xt,yt,xt, yt)}T1
t=1and{w(0)
j}m
j=1,
2η11
T1T1X
t=1f(Xt,yt,xt;W(0),Γ(0),b(0))∇w(0)
jf(Xt,yt,xt;W(0),Γ(0),b(0))
=˜O
η1γ2m√
d
.
Proof. Recall that
f(Xt,yt,xt;W(0),Γ(0),b(0)) =mX
j=1Γ(0)
j,j 
1
N1N1X
i=1yt
iσbj(w(0)
j⊤xt
i)!
σbj(w(0)
j⊤xt),
∇w(0)
jf(Xt,yt,xt;W(0),Γ(0),b(0)) = Γ(0)
j,j 
1
N1N1X
i=1yt
iσ′
bj(w(0)
j⊤xt
i)xt
i!
σbj(w(0)
j⊤xt)
+ Γ(0)
j,j 
1
N1N1X
i=1yt
iσbj(w(0)
j⊤xt
i)!
σ′
bj(w(0)
j⊤xt)xt.
Fixi, t, j and consider the inner product between w(0)
j∼Unif(Sd−1)andxt
i∼ N(0, Id). From
the rotational invariance, without loss of generality, we can assume that w(0)
j= [1,0, . . . , 0]⊤and
xt
i∼ N(0, Id). Therefore,D
w(0)
j,xt
iE
∼ N(0,1). From Lemma 4,D
w(0)
j,xt
iE2
≲logdholds with
high probability. Moreover, from Lemma 10 and Corollary 17, we know that |yt
i|,|yt|≲(logd)P/2
19and∥xt
i∥,∥xt∥≲√
dholds. We can take the union bound with respect to all the i, t, j , retaining the
high probability bound (see Appendix A). Therefore, noting that |bj| ≤1, we obtain
|f(Xt,yt,xt;W(0),Γ(0),b(0))|≲mγ(logd)P/2+1
and
∥∇w(0)
jf(Xt,yt,xt;W(0),Γ(0),b(0))∥≲γ(logd)P/2+1/2√
d.
Thus we obtain the assertion.
From now on we focus on analyzing (B.1) , which expresses the correlation between the output label
yand the gradient of model output ∇wf. Let
gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)
:=T1X
i=11
T1ytσb(w⊤xt)1
N1N1X
i=1yt
iσ′
b(w⊤xt
i)xt
i
+T1X
i=11
T1ytσ′
b(w⊤xt)xt1
N1N1X
i=1yt
iσb(w⊤xt
i), (B.2)
andg(w, b) =E[gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)], where the expectation is taken with respect
to the data {(Xt,yt,xt, yt)}T1
t=1. Note that
w(1)
j= 2η1Γ(0)
j,jgT1,N1(w(0)
j, b(0)
j,{(Xt,yt,xt, yt)}T1
t=1) +˜O
η1γ2m√
d
from Lemma 18.
In Appendix B.1, we analyze the explicit form of g(w, b)via the Hermite expansion: we can check
that its main term aligns with the low-dimensional subspace S. In Appendix B.2, we show the
concentration of gT1,N1around g.
B.1 Calculation of Population Gradient
B.1.1 Hermite Expansion
First, note that
g(w, b) =E
yσb(w⊤x)yiσ′
b(w⊤xi)xi
+E
yσ′
b(w⊤x)xyiσb(w⊤xi)
=Ef∗
E{xi},x,{ςi},ς
(f∗(x) +ς)σb(w⊤x)(f∗(xi) +ςi)σ′
b(w⊤xi)xi
+Ef∗
E{xi},x,{ςi},ς
(f∗(x) +ς)σ′
b(w⊤x)x(f∗(xi) +ςi)σb(w⊤xi)
=Ef∗
Ex,ς
(f∗(x) +ς)σb(w⊤x)
E{xi},{ςi}
(f∗(xi) +ςi)σ′
b(w⊤xi)xi
+Ef∗
Ex,ς
(f∗(x) +ς)σ′
b(w⊤x)x
E{xi},{ςi}
(f∗(xi) +ςi)σb(w⊤xi)
= 2Ef∗[Ex[f∗(x)σ′
b(w⊤x)x]Ex[f∗(x)σb(w⊤x)]].
HereEf∗denotes the expectation with respect to the distribution of the true function f∗(i.e., the
distribution of {ci}P
i=Qandβ) specified in Assumption 1. Now let σb(z) =P
i≥0ai(b)
i!Hei(z)be
the Hermite expansion of student activation σb(z) = ReLU( z+b).
Lemma 19. It holds that
Ex[f∗(x)σ′
b(w⊤x)x]
=P−1X
i=Q−1ai+1(b)Ex[∇i+1f∗(x)](w⊗i)
i!+wPX
i=Qai+2(b)Ex[∇if∗(x)](w⊗i)
i!, (B.3)
Ex[f∗(x)σb(w⊤x)] =PX
i=Qai(b)Ex[∇if∗(x)](w⊗i)
i!.
20Proof. Consider a function g(w⊤x) =P
i≥0˜ai
i!Hei(w⊤x). Here Ex[f∗(x)g(w⊤x)]can be
calculated as
Ex[f∗(x)g(w⊤x)] =X
i≥0˜ai
i!Ex[f∗(x)Hei(w⊤x)]
=X
i≥0i1+···+id=iX
i1≥0,...,id≥0˜aiwi1
1···wid
d
i1!···id!Ex[f∗(x)Hei1(x1)···Heid(xd)] (∵Lemma 8)
=X
i≥0i1+···+id=iX
i1≥0,...,id≥0˜aiwi1
1···wid
d
i1!···id!Ex∂i1
∂xi1
1···∂id
∂xidrf∗(x)
=X
i≥0˜aiEx[∇if∗(x)](w⊗i)
i!.
Where the final line is obtained noting that the tensor ∇if∗(x)hasi!
i1!···id!entries whose value is
∂i1
∂xi1
1···∂id
∂xidrf∗(x).
Note that Ex[∇if∗(x)] =Exh
∇iPP
j=Qcj
j!Hej(⟨x,β⟩)i
is a nonzero tensor only when Q≤i≤P,
from Lemma 8. We can also show (B.3) as
Ex[f∗(x)σ′
b(w⊤x)x]
=Ex[∇f∗(x)σ′
b(w⊤x)] +Ex[f∗(x)σ′′
b(w⊤x)]w(∵Stein’s lemma )
=X
i≥0ai+1(b)Ex[∇i+1f∗(x)](w⊗i)
i!+wX
i≥0ai+2(b)Ex[∇if∗(x)](w⊗i)
i!
=P−1X
i=Q−1ai+1(b)Ex[∇i+1f∗(x)](w⊗i)
i!+wPX
i=Qai+2(b)Ex[∇if∗(x)](w⊗i)
i!
Using Lemma 19, g(w, b)can be expanded as
g(w, b) =Ef∗
2P−1X
i=Q−1ai+1(b)Ex[∇i+1f∗(x)](w⊗i)
i!
+wPX
i=Qai+2(b)Ex[∇if∗(x)](w⊗i)
i! PX
i=Qai(b)Ex[∇if∗(x)](w⊗i)
i!
=:Ef∗2aQ(b)
(Q−1)!Ex[∇Qf∗(x)](w⊗(Q−1))aQ(b)Ex[∇Qf∗(x)](w⊗Q)
Q!
+s(w, b)
=2aQ(b)2
Q!(Q−1)!Ef∗[Ex[∇Qf∗(x)](w⊗(Q−1))·Ex[∇Qf∗(x)](w⊗Q)] +s(w, b),(B.4)
where
s(w, b)
:=2Ef∗ P−1X
i=Q−1ai+1(b)Ex[∇i+1f∗(x)](w⊗i)
i!+wPX
i=Qai+2(b)Ex[∇if∗(x)](w⊗i)
i!
·PX
i=Qai(b)Ex[∇if∗(x)](w⊗i)
i!
−aQ(b)2
(Q−1)!Ex[∇Qf∗(x)](w⊗(Q−1))Ex[∇Qf∗(x)](w⊗Q)
Q!
expresses terms in the asymptotic expansion which are not the leading term.
Note that, in this explicit formulation, ai(b) =Ez∼N(0,1)[σb(z)Hei(z)]appears. It will be helpful to
show that this coefficient is well-controlled.
21Lemma 20. There are constants CHandCLdepending only on QandPsatisfying the following
condition. Let σb(z) =σ(z+b)andb∼Unif([−1,1]). Then,
Ez∼N(0,1)[σb(z)Hei(z)]≤CH(2≤i≤P+ 2) andEz∼N(0,1)[σb(z)HeQ(z)]≥CL(B.5)
holds with probability 1/2. Especially,Ez∼N(0,1)[σb(z)Hei(z)]≤CH(2≤i≤P+ 2) holds with
probability 1.
Proof. From Lemma 15 in [ BES+23], we know that for i≥2,Ez∼N(0,1)[σb(z)Hei(z)]=
e−b2/2
√
2π|Hei−2(b)|holds. First, as b∼Unif([−1,1]), by setting sufficiently large CH,|Hei(b)| ≤
CHholds for all 0≤i≤P. Therefore, asEz∼N(0,1)[σb(z)Hei(z)]≤ |Hei−2(b)|,Ez∼N(0,1)[σb(z)Hei(z)]≤CHholds for all 2≤i≤P+ 2with probability one.
From continuity we know that there exists CLsuch thatEz∼N(0,1)[σb(z)HeQ(z)]≥CLwith
probability 1/2.
In the following, we first calculate the main term and then upper bound the residual terms.
B.1.2 Calculation of the Main Term
Let us calculate the main term in (B.4) explicitly.
Lemma 21.
2aQ(b)2
Q!(Q−1)!Ef∗[Ex[∇Qf∗(x)](w⊗(Q−1))·Ex[∇Qf∗(x)](w⊗Q)]
=2aQ(b)2EcQ[c2
Q]
Q!(Q−1)!(2Q−1)!!
Ez∼χr[z2Q]∥w1:r∥2Q−2
w1:r
0d−r
holds.
Proof. From Corollary 9, it holds that Ex[∇Qf∗(x)] =cQ(β)⊗Q. Then, we obtain
Ef∗[Ex[∇Qf∗(x)](w⊗(Q−1))·Ex[∇Qf∗(x)](w⊗Q)]
=EcQ[c2
Q]Eβ[(β⊗Qw⊗(Q−1))·(β⊗Q◦w⊗Q)]
=EcQ[c2
Q]Eβ[β⊗2Q](w⊗(2Q−1)).
Next, let us show
Eβ[β⊗2Q](w⊗(2Q−1)) =(2Q−1)!!
Ez∼χr[z2Q]∥w1:r∥2Q−2
w1:r
0d−r
.
First, note that by letting β′∼ N(0,Σβ)where
Σβ=Ir0r
0⊤
rOd−r
,
andz∼χrindependent from β,β′∼βzholds (recall that we assumed that β∼
Unif({(β1, . . . , β r,0, . . . , 0)⊤|β2
1+···+β2
r= 1})– see Appendix A). Then,
Eβ[β⊗2Q] =1
Ez∼χr[z2Q]Ev′
1:r∼N(0,Ir),v′
r+1:d=0[v′⊗2Q].
It suffices to show that Ev∼N(0,Ir)[v⊗2Q](z⊗(2Q−1)) = (2 Q−1)!!z∥z∥2Q−2forz∈Rr. As an ex-
ample, we show that the first entry of Ev∼N(0,Ir)[v⊗2Q](z⊗(2Q−1))equals to (2Q−1)!!z1∥z∥2Q−2:
the other components can be calculated similarly. First, we expand (Ev∼N(0,Ir)[v⊗2Q](z⊗(2Q−1)))1
as
(Ev∼N(0,Ir)[v⊗2Q](z⊗(2Q−1)))1=X
i1,...,i 2Q−1∈[r]E[v1vi1···vi2Q−1]zi1···zi2Q−1.
Note that E[v1vi1···vi2Q−1]̸= 0if and only if the degree of v1vi1···vi2Q−1with respect to vjis
even, for all j∈[r]. Then, we obtain
(Ev∼N(0,Ir)[v⊗2Q](z⊗(2Q−1)))1
22=q1+···+qr=QX
q1≥1,q2,...,qr≥0cq1,...,qrE[v2q1
1v2q2
2···v2qr
r]z2q1−1
1 z2q2
2···z2qr
r
=z1q1+···+qr=QX
q1≥1,q2,...,qr≥0cq1,...,qrE[v2q1
1v2q2
2···v2qr
r]z2q1−2
1 z2q2
2···z2qr
r,
where cq1,...,qr=(2Q−1)!
(2q1−1)!(2q2)!···(2qr)!is the number of ways to assign (2q1−1)“1”s, (2q2)“2”s, ...,
among i1toi2Q−1.
Note that Ev∼N(0,1)[v2q] = (2 q−1)!!. Therefore, we obtain
(Ev∼N(0,Ir)[v⊗2Q](z⊗(2Q−1)))1
=z1q1+···+qr=QX
q1≥1,q2,...,qr≥0(2Q−1)!
(2q1−1)!(2q2)!···(2qr)!(2q1−1)!!···(2qr−1)!!z2q1−2
1 z2q2
2···z2qr
r
=z1q1+···+qr=QX
q1≥1,q2,...,qr≥0(Q−1)!
(q1−1)!(q2)!···(qr)!(2Q−1)!!(z2
1)q1−1(z2
2)q2···(z2
r)qr
=(2Q−1)!!z1(z2
1+···+z2
r)Q−1(∵multinomial theorem )
which concludes the assertion.
By Lemma 21, we observe that the main term of the expected one-step gradient aligns to the r-
dimensional subspace S. In other words, the MLP layer captures the information of Svia pretraining,
which will be useful in the later stage.
B.1.3 Bounding Residual Terms
First, similarly to the Lemma 21, we can obtain the explicit formulation of the entire g(w, b).
Lemma 22.
g(w, b)
=X
Q−1≤i≤P−1,
Q≤j≤P,
i+jis odd2ai+1(b)aj(b)E[ci+1cj]
i!j!(i+j)!!
Ez∼χr[zi+j+1]∥w1:r∥i+j−1
w1:r
0d−r
+wX
Q≤i≤P,
Q≤j≤P,
i+jis even2ai+2(b)aj(b)E[cicj]
i!j!(i+j−1)!!
Ez∼χr[zi+j]∥w1:r∥i+j(B.6)
holds.
Proof. Note that
Eβ[β⊗n](w⊗(n−1)) =

(n−1)!!
Ez∼χr[zn]∥w1:r∥n−2w1:r
0d−r
(nis even )
0 (nis odd )
and
Eβ[β⊗n](w⊗n) =((n−1)!!
Ez∼χr[zn]∥w1:r∥n(nis even )
0 (nis odd ),
which can be obtained similarly to the proof of Lemma 21.
Let us upper bound the non-leading terms; we need to show that the moment of first rcomponents in
residual term s(w, b)are sufficiently small.
23Lemma 23. Letj= 4kwhere k∈[P]. then,
Ew∼Sd−1[∥s(w, b)1:r∥j]1/j=˜Od,rrr
d2Q+1
(B.7)
holds uniformly over all b∈[−1,1].
Proof. Note that ∥s(w, b)1:r∥2is a polynomial in w; from Lemma 24 in [ DLS22 ], it suffices to
show the case k= 1.
s(w, b)1:r
=X
Q−1≤i≤P−1,
Q≤j≤P,
i+jis odd,
i,j̸=(Q−1,Q)2ai+1(b)aj(b)E[ci+1cj]
i!j!(i+j)!!
Ez∼χr[zi+j+1]∥w1:r∥i+j−1w1:r
+X
Q≤i≤P,
Q≤j≤P,
i+jis even2ai+2(b)aj(b)E[cicj]
i!j!(i+j−1)!!
Ez∼χr[zi+j]∥w1:r∥i+jw1:r.
By Minkowski’s inequality E[∥x+y∥4]1/4≤E[∥x∥4]1/4+E[∥y∥4]1/4, it suffices to show that
(B.7) holds for each term. Note that E[ci+1cj]≤R2
candE[cicj]≤R2
cfrom Assumption 1,
|ai(b)| ≤CHfrom Lemma 20, and Ew[∥w1:r∥4k] =O((r/d)2k)from Lemma 12. Moreover, it is
known that Ez∼χr[z2l] = Θ( rl). Putting these things together yields the assertion.
It is also needed in later stages to give a high probability upper bound on the ∥g(w, b)∥;
Lemma 24.
sup
b∈[−1,1]∥g(w, b)∥=˜Od,r r
1
rd2Q−1!
holds with high probability over w∼Unif(Sd−1).
Proof. This can be shown similarly to Lemma 23 – we use Lemma 11 instead of Lemma 12 to
obtain a high probability bound (note that this bound includes not only s(w, b)but also the main
term, and it is for g(w, b), instead of g(w, b)1:r).
B.2 Concentration of Empirical Gradient
Now we control the deviation of gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)fromg(w, b).
Lemma 25. Under Assumption 1,
sup
f∗,b∈[−1,1]∥Ex[f∗(x)σ′
b(w⊤x)x]∥≲rr
dlogdQ−1
, (B.8)
sup
f∗,b∈[−1,1]∥Ex[f∗(x)σ′
b(w⊤x)x]1:r∥≲rr
dlogdQ−1
, (B.9)
sup
f∗,b∈[−1,1]|Ex[f∗(x)σb(w⊤x)]|≲rr
dlogdQ
(B.10)
holds with high probability over w∼Unif(Sd−1), respectively. Here supf∗denotes the supremum
over the support of f∗whose distribution is specified in Assumption 1.
Proof. We bound the leading termaQ(b)Ex[∇Qf∗(x)](w⊗(Q−1))
(Q−1)!in(B.3) of Lemma 19 to show (B.8) .
From Lemmas 5 and 6, we have
aQ(b)Ex[∇Qf∗(x)](w⊗(Q−1))
(Q−1)!2
=aQ(b)cQ
(Q−1)!(β)⊗Q(w⊗(Q−1))2
24=aQ(b)cQ
(Q−1)!(β)⊗Q(w⊗(Q−1)
1:r )2
≤aQ(b)cQ
(Q−1)!2
∥β∥2Q∥w1:r∥2(Q−1)
=aQ(b)cQ
(Q−1)!2
∥w1:r∥2(Q−1).
Moreover, Lemma 20 tells us that |aQ(b)| ≤ CHalways holds when b∈[−1,1], and
Assumption 1 ensures that |cQ| ≤ Rc. Therefore, from Lemma 11, we can show
thataQ(b)Ex[∇Qf∗(x)](w⊗(Q−1))
(Q−1)!2
≤
CHRc
(Q−1)!2 4Cr
dlogdQ−1with probability at least
1−2 exp(−d/32)−2rd−C. (B.9) and (B.10) can be obtained similarly.
The following lemma is parallel to Lemma 19 in [DLS22].
Lemma 26. Fix any t∈[T1],b∈[−1,1]andw∈Sd−1satisfying the conditions (B.8) to(B.10) .
Then, with high probability over the distribution of (Xt,yt,xt, yt), it holds that
1
N1N1X
i=1yt
iσ′
b(w⊤xt
i)xt
i−Ex[ft
∗(x)σ′
b(w⊤x)x]≤˜O r
d
N1!
,
 
1
N1N1X
i=1yt
iσ′
b(w⊤xt
i)xt
i−Ex[ft
∗(x)σ′
b(w⊤x)x]!
1:r≤˜Orr
N1
,
1
N1N1X
i=1yt
iσb(w⊤xt
i)−Ex[ft
∗(x)σb(w⊤x)]≤˜Or
1
N1
.
Here the right-hand sides do not depend on t.
Proof. We only present the proof of the first inequality, as other bounds can be attained similarly.
From Corollary 17, |ft
∗(x)| ≤Rwhere R≲(logd)P/2holds with high probability for all t∈[T1].
Conditioned on this,
1
N1N1X
i=1ft
∗(xt
i)σ′
b(w⊤xt
i)xt
i−Ex[ft
∗(x)σ′
b(w⊤x)x]
≤1
N1N1X
i=1ft
∗(xt
i)Ift∗(xt
i)≤Rσ′
b(w⊤xt
i)xt
i−Ex[ft
∗(x)Ift∗(x)≤Rσ′
b(w⊤x)x]
+Ex[ft
∗(x)Ift∗(x)>Rσ′
b(w⊤x)x]
Note that ft
∗(xt
i)Ift∗(xt
i)≤Rσ′
b(w⊤xt
i)⟨xt
i,u⟩ −Ex[ft
∗(x)Ift∗(x)≤Rσ′
b(w⊤x)⟨x,u⟩]isR2-sub Gaus-
sian for any ∥u∥= 1from Lemma 14. Hence the first term can be upper bounded by ˜O
Rq
d
N1
with high probability from Lemma 13. For the second term, we observe that
Ex[ft
∗(x)Ift∗(x)>Rσ′
b(w⊤x)x]≤Ex[∥ft
∗(x)Ift∗(x)>Rσ′
b(w⊤x)x∥]
≤Ex[ft
∗(x)2]1/2Ex[(σ′
b(w⊤x)∥x∥)2]1/2Ex[(Ift∗(x)>R)4]1/4
≤Ex[ft
∗(x)2]1/2Ex[∥x∥2]1/2Ex[(Ift∗(x)>R)]1/4
≤O(√
d·d−C/4)
for sufficiently large C; hence this term can be ignored. Finally, ςiσ′
b(w⊤xt
i)⟨xt
i,u⟩isτ2-sub Gaus-
sian for any ∥u∥= 1and then ∥1
N1PN1
i=1ςσ′
b(w⊤xt
i)xt
i∥=˜O(p
d/N 1)with high probability.
By similar procedure, we can obtain a bound of ∥gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)−g(w, b)∥.
25Lemma 27. Fix any b∈[−1,1]andw∈Sd−1satisfying the conditions (B.8) -(B.10) . Then, with
high probability over the distribution of {(Xt,yt,xt, yt)}T1
t=1,
∥gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)−g(w, b)∥=˜O
r
rQ−1
dQ−1r
d
T1+s
d2
N1T1
,
∥(gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)−g(w, b))1:r∥=˜O
r
rQ−1
dQ−1rr
T1+s
r2
N1T1
,
holds.
Proof. In this proof, the term “with high probability” indicates the probability with respect to the
distribution of {(Xt,yt,xt, yt)}T1
t=1. Let
z1(w, b,{(Xt,yt,xt, yt)}T1
t=1) =T1X
t=11
T1ytσb(w⊤xt)1
N1N1X
i=1yt
iσ′
b(w⊤xt
i)xt
i
and
z2(w, b,{(Xt,yt,xt, yt)}T1
t=1) =T1X
t=11
T1ytσ′
b(w⊤xt)xt1
N1N1X
i=1yt
iσb(w⊤xt
i).
Note that gT1,N1=z1+z2holds.
Let us consider z1(w, b,{(Xt,yt,xt, yt)}T1
t=1). Since we assume (B.8) -(B.10) hold, by taking union
bound for Lemma 26 with respect to t∈[T1], with high probability ∥1
N1PN1
i=1yt
iσ′
b(w⊤xt
i)xt
i∥ ≤
R=˜Oq
d
N1+q
rQ−1
dQ−1
for each t. Moreover, from Corollary 17, |yt| ≤R′=˜O(1)for each t.
Here, we notice that the procedure in the proof of Lemma 26 can be applied, yielding
∥z1(w, b,{(Xt,yt,xt, yt)}T1
t=1)−E[z1(w, b,{(Xt,yt,xt, yt)}T1
t=1)]∥
=˜O
RR′p
d/T1
=˜O  r
rQ−1
dQ−1+r
d
N1!r
d
T1!
with high probability. Similarly we can obtain
•∥z2−E[z2]∥=˜Oq
rQ
dQ+q
1
N1q
d
T1
,
•∥(z1−E[z1])1:r∥=˜Oq
rQ−1
dQ−1+q
r
N1q
r
T1
, and
•∥(z2−E[z2])1:r∥=˜Oq
rQ
dQ+q
1
N1q
r
T1
,
which concludes the proof.
We need to obtain a bound uniformly over b∈[−1,1]. We first introduce the following definition.
Definition 28. Fix any w∈Sd−1andX=ST1
t=1{xt
1, . . . ,xt
N1,xt} ⊂Rd. Then, define a finite
disjoint partition B(w, X) ={B1, . . . , B N(w,X)}of[−1,1], i.e.,∪iBi= [−1,1]andBi∩Bj=
∅(i̸=j)as follows: bandb′belong to the same Biif and only if sign(⟨w,x⟩+b) = sign( ⟨w,x⟩+b′)
for all x∈X.
It is clear that |B(w, X)|≲|X| ≤(N1+ 1)T1holds because it suffices to divide [−1,1]at the point
satisfying ⟨w,x⟩+b= 0for each x∈X.
Lemma 29. Fix any w∈Sd−1. Then, with high probability over the distribution of
{(Xt,yt,xt, yt)}T1
t=1, the following holds:
26•g(w, b)isL1-Lipschitz continuous with respect to bin the interval [−1,1].
•LetX=ST1
t=1{xt
1, . . . ,xt
N1,xt}. Then, gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)isL2-Lipschitz con-
tinuous with respect to bin each interval Bi∈ B(w, X).
Here L1=O(1)andL2=˜O(√
d)do not depend on w.
Proof. By the explicit form of g(w, b)(B.6) , we notice that each term is made by taking the product
between the term as aj(b)ak(b)and a vector independent of bwhose norm is O(1). Thus, it suffices to
show that aj(b)ak(b)isO(1)-Lipschitz. This follows from the fact aj=±e−b2/2
√
2πHej−2(b)(see the
proof of Lemma 20) and Hej−2(b)e−b2/2isO(1)-Lipschitz continuous on the bounded set [−1,1].
Next, let us see gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1). Recall that
gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)
:=T1X
i=11
T1ytσb(w⊤xt)1
N1N1X
i=1yt
iσ′
b(w⊤xt
i)xt
i+T1X
i=11
T1ytσ′
b(w⊤xt)xt1
N1N1X
i=1yt
iσb(w⊤xt
i).
From Lemma 10 and Corollary 17, we know that |yt
i|,|yt|≲(logd)P/2and∥xt
i∥,∥xt∥≲√
dholds
for all i, twith high probability. Assuming this and noting that σis1-Lipschitz, in each interval in
B(w, X),gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)isL2=˜O(√
d)-Lipschitz.
Corollary 30. With high probability over the distribution of wand{(Xt,yt,xt, yt)}T1
t=1,
sup
b∈[−1,1]∥gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)−g(w, b)∥=˜O
r
rQ−1
dQ−1r
d
T1+s
d2
N1T1
,
sup
b∈[−1,1]∥(gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)−g(w, b))1:r∥=˜O
r
rQ−1
dQ−1rr
T1+s
r2
N1T1

(B.11)
holds.
Proof. LetX=ST1
t=1{xt
1, . . . ,xt
N1,xt}and consider B(w, X) ={B1, . . . , B N(w,X)}. Then, let
Bbe the union of ˜Oq
rQ−1
dQ−1q
r
T1+q
r2
N1T1
/√
d
-coverings of Bi∈ B(w, X). Since |B|is
polynomial in d, Lemma 27 holds with high probability uniformly over b∈B.
Moreover, from the construction and Lemma 29, for b∈[−1,1]\B, there exists π(b)∈Bsuch that
|∥gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)−g(w, b)∥
− ∥gT1,N1(w, π(b),{(Xt,yt,xt, yt)}T1
t=1)−g(w, π(b))∥|
≤˜O
r
rQ−1
dQ−1rr
T1+s
r2
N1T1

holds.
Lemma 31. With high probability over the distribution of {(Xt,yt,xt, yt)}T1
t=1,
•It holds that
sup
b∈[−1,1]∥g(w, b)−gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥
=˜O
r
rQ−1
dQ−1r
d
T1+s
d2
N1T1
 (B.12)
with high probability over w∼Unif(Sd−1).
27•It holds that
sup
b∈[−1,1]Ew∼Unif( Sd−1)[∥g(w, b)−gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥j]1/j
=˜O
r
rQ−1
dQ−1rr
T1+s
r2
N1T1
 (B.13)
forj∈[4P].
Proof. If an event Aoccurs with probability at least 1−d−Cover the simultaneous distribution of
(w,{(Xt,yt,xt, yt)}T1
t=1), then we can say that
•with probability at least 1−dC/2over{(Xt,yt,xt, yt)}T1
t=1,Pw{A| {(Xt,yt,xt, yt)}T1
t=1)} ≥
1−d−C/2holds.
(B.12) follows from this remark and Corollary 30.
For(B.13) , with high probability over the distribution of {(Xt,yt,xt, yt)}T1
t=1,Pw{(B.11) |
{(Xt,yt,xt, yt)}T1
t=1} ≥1−d−˜Cis satisfied for some ˜C. Also, from Lemma 10 and Corollary 17,
with high probability over the distribution of {(Xt,yt,xt, yt)}T1
t=1, we have ∥xt
i∥,∥xt∥=O(√
d)
and|yt|=˜O(1). From the definition of gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)(B.2) and expan-
sion of g(w, b)(B.6) ,supw∥g(w, b)−gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥ ≤supw∥g(w, b)∥+
supw∥gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥is upper bounded by ˜O(d). Therefore, with high proba-
bility
sup
b∈[−1,1]Ew∼Unif( Sd−1)[∥g(w, b)−gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥j]1/j
≤Ew∼Unif( Sd−1)
 
sup
b∈[−1,1]∥g(w, b)−gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥!j
1/j
=˜O


1−d−˜C
r
rQ−1
dQ−1rr
T1+s
r2
N1T1
j
+
d−˜C
dj
1/j
.
By setting sufficiently large ˜C, we arrive at the claim.
Summary. We combine the obtained results and show that ∥gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥
and the moment of the residual terms are bounded with high probability.
Corollary 32. With high probability over the distribution of {(Xt,yt,xt, yt)}T1
t=1,
(I)gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1) = m(w, b) +r(w, b,{(Xt,yt,xt, yt)}T1
t=1)holds
where
m(w, b):=2aQ(b)2EcQ[c2
Q]
Q!(Q−1)!(2Q−1)!!
Ez∼χr[z2Q]∥w1:r∥2Q−2
w1:r
0d−r
,
and
sup
b∈[−1,1]Ew∼Unif( Sd−1)
r(w, b,{(Xt,yt,xt, yt)}T1
t=1)
1:rj1/j
=˜Od,r
r
rQ−1
dQ−1rr
T1+s
r2
N1T1+rr
d2Q+1
.
28(II)Ifη1γ≤√
rd2Q−1/C1(logd)C2for sufficiently large C1andC2,T1≳rQdQ+1, andN1T1≳
rd2Q+1, then
sup
b∈[−1,1]∥2η1γgT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥ ≤1
holds with high probability over w∼Unif(Sd−1). Moreover,
sup
b∈[−1,1]D
2η1γgT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1,z)E≤logd
holds with high probability over w∼Unif(Sd−1)and the training data for the second stage
{(Xt,yt,xt, yt)}T2
t=T1+1, where z∈ST2
t=T1+1{xt
1, . . . ,xt
N2,xt}.
Proof. Combining Lemmas 23 and 31 yields (I). For (II), by the condition for T1andN1, we obtain
sup
b∈[−1,1]∥gT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥ ≤˜Od,r
r
rQ−1
dQ−1r
d
T1+s
d2
N1T1+r
1
rd2Q−1

=˜Od,r r
1
rd2Q−1!
from Lemmas 24 and 31. Then, as η1γ≲√
rd2Q−1/C1(logd)C2,
supb∈[−1,1]∥2η1γgT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1)∥ ≤1is achieved.
By an argument similar to the one in the proof of Lemma 18, we obtain that with high probability,D
2η1γgT1,N1(w, b,{(Xt,yt,xt, yt)}T1
t=1,z)E≲logdforz∈ST2
t=T1+1{xt
1, . . . ,xt
N2,xt}. To
obtain the supremum bound over b∈[−1,1], it suffices to utilize Lemma 29 again.
In the following sections, we assume that these two events (I) and (II) occur.
C Construction of Attention Matrix
We construct an attention matrix ¯Γwith a good approximation property: see Section 3.2.2 for the
proof outline.
In this section, a set of basis functions His defined as
H=

x7→rY
j=11√j!Hepj(xj)Q≤p1+···+pr≤P, p1≥0, . . . , p r≥0

.
We let BP:=|H|and introduce a numbering H={h1, . . . , h BP}. Note that
BP=PX
i=Qr+i−1
i
= Θ( rP).
C.1 Approximation Error of Two-layer Neural Network
As sketched in Section 3.2.2, we need to show that there is a set of two-layer neural networks which
can approximate basis functions well. The goal of this section is to show the following proposition.
Proposition 33. Assume T1=˜Ω(dQ+1rQ)andN1T1=˜Ω(d2Q+1r). Let{w(1)
j}jbe neurons
obtained by line 4 of Algorithm 1 with γ≍1
m3
2r1
2dQ,η1≍m3
2rd2Q−1
2·(logd)−Cηfor constant Cη.
Assume bis re-initialized in Line 5 of Algorithm 1. Then, with high probability over the distribution
of training data and the initialization of model parameters, there exist vectors a1, . . . ,aBP∈Rm
such that for each n∈[BP],
sup
i∈[N2],T1+1≤t≤T2
mX
j=1an
jσ(w(1)
j⊤xt
i+bj)−hn(xt
i)
2
=˜OrP
m+1
N2
,
29sup
T1+1≤t≤T2
mX
j=1an
jσ(w(1)
j⊤xt+bj)−hn(xt)
2
=˜OrP
m+1
N2
holds. Moreover, supn∈[BP]∥an∥2=˜O
rP
m
.
This proposition states that there exists a two-layer neural networkPm
j=1an
jσ
⟨w(1)
j,·⟩+bj
which
can approximate the basis function hn, and its second-layer magnitude ∥an∥2only scales with r.
Note that the condition of T1andN1T1comes from Corollary 32, which implies that the noise terms
of the one-step gradient is well controlled.
C.1.1 Alignment to Sand Efficient Approximation
From now on, we suppose that gT1,N1(w, b)satisfies the conditions in Corollary 32 (to simplify the
notation, we drop the dependency on {(Xt,yt,xt, yt)}T1
t=1). An important previous result [ DLS22 ]
is that, if gT1,N1(w, b)aligns with the r-dimensional subspace S, then it can be used to approximate
polynomials on Sefficiently.
Lemma 34 (Lemma 21 in [ DLS22 ]).There exists a set of symmetric r-dimensional tensors
{Tn
k}0≤k≤P,n∈[BP]such that
hn(x) =X
0≤k≤P
Tn
k,x⊗k
1:r
and∥Tn
k∥F≲rP−k
4.
Definition 35 (f(r, d)-alignment) .We call that gT1,N1(w, b)hasf(r, d)-alignment with S, if the
following holds:
•For any 0≤k≤P-symmetric r-dimensional tensor T,
Ew∼Unif( Sd−1)[
T,gT1,N1(w, b)⊗k
1:r2]≳f(r, d)k∥T∥2
F,
and the hidden constant in “ ≳” needs to be uniformly lower bounded.
Lemma 36 (Corollary 4 in [ DLS22 ], adapted) .Suppose that gT1,N1(w, b)hasf(r, d)-alignment
withS. LetTbe a0≤k≤P-symmetric r-dimensional tensor. Then, there exists ψT(w, b)such
that
Ew∼Unif( Sd−1)[ψT(w, b)⟨gT1,N1(w, b),x⟩k] =
T,x⊗k
1:r
,
Ew∼Unif( Sd−1)[ψT(w, b)2]≲f(r, d)−k∥T∥2
F,
|ψT(w, b)|≲f(r, d)−k∥T∥F∥gT1,N1(w, b)∥k.
Intuitively speaking, f(r, d)-alignment ensures that the MLP gradient gT1,N1(w, b)has suffi-
ciently large component on S. If so, the lemma states that, an infinite-width neural network
Ew[ψT(w, b)⟨gT1,N1(w, b),·⟩k], whose hidden layer weight is gT1,N1(w, b)and the activation is
z7→zk, can express the functions
T,x⊗k
1:r
with small output layer ψT(w, b).
Let us show that our gT1,N1(w, b)has sufficient alignment if bsatisfies (B.5).
Lemma 37 (Tensor expectation lower bound) .LetTbe ak≤P-symmetric r-dimensional tensor
and¯w=w1:r∥w1:r∥2Q−2. Then
Ew∼Unif( Sd−1)[T(¯w⊗k)2]≳rQi
d(Q+1)iE[∥T(¯w⊗k−i)∥2
F]
holds for all 0≤i≤k. Here T(¯w⊗0):=T.
Proof. Letu∼ N (0, Id),z∼χ(d)and¯u=u1:r∥u1:r∥2Q−2. We can decompose ¯uas¯u=
z2Q−1¯w. Therefore
E[T(¯u⊗k)2] =E[z(4Q−2)k]E[T(¯w⊗k)2]
30holds. On the other hand, let x∼Sr−1andz′∼χ(r). Then we can decompose ¯uas¯u= (z′)2Q−1x.
Thus
E[T(¯u⊗k)2] =E[(z′)(4Q−2)k]E[T(x⊗k)2]
holds. It implies that
E[T(¯w⊗k)2] =E[(z′)(4Q−2)k]
E[z(4Q−2)k]E[T(x⊗k)2].
Similarly,
E[∥T(¯w⊗k−i)∥2
F] =E[(z′)(4Q−2)(k−i)]
E[z(4Q−2)(k−i)]E[∥T(x⊗(k−i))∥2
F]
is satisfied. Now from Corollary 13 in [DLS22],
E[∥T(x⊗(k−i))∥2
F]≲riE[T(x⊗k)2]
holds. Therefore we obtain
E[T(¯w⊗k)2] =E[(z′)(4Q−2)k]
E[z(4Q−2)k]E[T(x⊗k)2]
≳r−iE[(z′)(4Q−2)k]
E[z(4Q−2)k]E[∥T(x⊗(k−i))∥2
F]
=r−iE[(z′)(4Q−2)k]
E[z(4Q−2)k]E[z(4Q−2)(k−i)]
E[(z′)(4Q−2)(k−i)]E[∥T(¯w⊗k−i)∥2
F]
=r(2Q−2)i
d(2Q−1)iE[∥T(¯w⊗k−i)∥2
F].
Corollary 38. LetTbe a k≤P-symmetric r-dimensional tensor and let m(w, b)1:r=
2aQ(b)2
Q!(Q−1)!EcQ[c2
Q](2Q−1)!!
Ez∼χr[z2Q]∥w1:r∥2Q−2w1:r. Moreover, assume that (B.5) holds. Then,
Ew∼Unif( Sd−1)[
T,m(w, b)⊗k
1:r2]≳1
d(2Q−1)ir2iE[∥T(m(w, b)⊗k−i
1:r)∥2
F]
holds for all 0≤i≤k.
Proof. This is obtained by calibrating the coefficients from the previous lemma.
Lemma 39. LetTbe a0≤k≤P-symmetric r-dimensional tensor. Assume that (B.5) holds.
Furthermore, suppose r≲d1
2,T1=˜Ω(dQ+1rQ)andN1T1=˜Ω(d2Q+1r). Then,
Ew∼Unif( Sd−1)[
T,gT1,N1(w, b)⊗k
1:r2]≳1
d(2Q−1)kr2k∥T∥2
F
holds for all 0≤k≤P. Therefore, gT1,N1(w, b)hasd−2Q+1r−2-alignment with S.
Proof. Decompose gT1,N1(w, b) =m(w, b) +r(w, b)as defined in Corollary 32. From the proof
of Lemma 11 in [DLS22], we can show that
Ew∼Unif( Sd−1)[
T,gT1,N1(w, b)⊗k
1:r2]
≥Ew∼Unif( Sd−1)1
2
T,m(w, b)⊗k
1:r2
−Ew∼Unif( Sd−1)[δ(w, b)2] (C.1)
for some δ(w, b)satisfying
Ew∼Unif( Sd−1)[δ(w, b)2]
≲kX
i=1Ew∼Unif( Sd−1)[∥T(m(w, b)⊗k−i
1:r)∥2
F]q
Ew∼Unif( Sd−1)[∥r(w, b)1:r∥4i].
31Here, from Corollaries 32 and 38,
Ew∼Unif( Sd−1)[δ(w, b)2]
≲kX
i=1d(2Q−1)ir2iEw∼Unif( Sd−1)[
T,m(w, b)⊗k
1:r2]˜Od,r rQ−1
dQ−1r
T1+r2
N1T1+r
d2Q+1i!
holds. Now, from the condition of N1andT1, and r≲√
d, we know
d(2Q−1)r2rQ−1
dQ−1r
T1+r2
N1T1+r
d2Q+1
≲1.
Therefore we can achieve Ew∼Unif( Sd−1)[δ(w, b)2]≤1
4Ew∼Unif( Sd−1)[
T,m(w, b)⊗k
1:r2]. Apply-
ing this and Corollary 38 (i=k)to (C.1) yields the result.
C.1.2 Approximation Power of Infinite-width Neural Network
No we can show that some “infinite-width” neural network can approximate the functions h∈ H.
Lemma 40 (Lemma 9 in [ DLS22 ], adapted) .Leta∼Unif({−1,1})andb∼Unif([−logd,logd]).
Then, for any k≥0there exists vk(a, b)such that for |x| ≤logd,
Ea,b[vk(a, b)σ(ax+b)] =xk,sup
a,b|vk(a, b)|=˜Od(1).
Lemma 41 (Lemma 12 and Corollary 5 in [ DLS22 ], adapted) .There exists ϕn(a,w, b1, b2)such
that if
fϕn(x):=Ea,w,b1,b2[ϕn(a,w, b1, b2)σ(2η1agT1,N1(w, b1) +b2)]
where
a∼Unif{±γ},w∼Unif(Sd−1), b1∼Unif([−1,1]), b2∼Unif([−logd,logd]),
it holds that
sup
i∈[N2],T1+1≤t≤T2 
fϕn(xt
i)−hn(xt
i)2≲1
N2,
sup
T1+1≤t≤T2 
fϕn(xt)−hn(xt)2≲1
N2,
Ea,w,b1,b2[ϕn(a,w, b1, b2)2] =˜O(rP),
sup
a,w,b1,b2|ϕn(a,w, b1, b2)|=˜O(rP)
for each n∈[BP].
Proof. We first construct ϕTn
k(a,w, b1, b2)for basis tensors defined in Lemma 34, such that if
fϕTn
k(x):=Ea,w,b1,b2[ϕTn
k(a,w, b1, b2)σ(⟨2η1agT1,N1(w, b1),x⟩+b2)],
it holds that
sup
i∈[N2],T1+1≤t≤T2
fϕTn
k(xt
i)−
Tn
k,(xt
i)⊗k
1:r2
≲1
N2, (C.2)
sup
T1+1≤t≤T2
fϕTn
k(xt)−
Tn
k,(xt)⊗k
1:r2
≲1
N2. (C.3)
LetϕTn
k(a,w, b1, b2) =2vk(a,b2)ψTn
k(w,b1)
(2η1)kγkIA(w)IB(b1)where vkandψTn
kare constructed in
Lemma 36 and the events A(w)andB(b1)are defined as
A(w):=
sup
b1∈[−1,1]2η1γ|
gT1,N1(w, b1),xt
i
|,sup
b1∈[−1,1]2η1γ|
gT1,N1(w, b1),xt
| ≤logd
32fori∈[N2], T1+ 1≤t≤T2
∩(
sup
b1∈[−1,1]∥2η1γgT1,N1(w, b1)∥ ≤1)
B(b1):=Ez∼N(0,1)[σb1(z)Hei(z)]≤CH(2≤i≤P+ 2)	
∩Ez∼N(0,1)[σb1(z)HeQ(z)]≥CL	
.
Then,
fϕTn
k(x)
=Ea,w,b1,b22vk(a, b2)ψTn
k(w, b1)
(2η1)kγkIA(w)IB(b1)σ(⟨2η1agT1,N1(w, b1),x⟩+b2)
=Eb1
2·IB(b1)EwψTn
k(w, b1)
(2η1)kγkIA(w)Ea′,b2[vk(a, b2)σ(⟨2η1a′γgT1,N1(w, b1),x⟩+b2)]
,
where a′∼Unif({±1}). Here, if x∈ST2
t=T1+1{xt
1, . . . ,xt
N2,xt},
EwψTn
k(w, b1)
(2η1)kγkIA(w)Ea′∼Unif({±1}),b2[vk(a, b2)σ(⟨2η1a′γgT1,N1(w, b1),x⟩+b2)]
=Ewh
ψTn
k(w, b1)⟨gT1,N1(w, b1),x⟩ki
+Ewh
ψTn
k(w, b1)(1−IA(w))⟨gT1,N1(w, b1),x⟩ki
.
Note that the absolute value of second term above can be upper bounded by
Ew
(1−IA(w))1/2Ewh
(ψTn
k(w, b1)⟨gT1,N1(w, b1),x⟩k)2i1/2
. Recall that A(w)occurs
with high probability from Corollary 32, and Ewh
(ψTn
k(w, b1)⟨gT1,N1(w, b1),x⟩k)2i1/2
is
polynomial in d. Hence we can set the second term to be O(1/√N2).
Moreover, as P[B(b1)] = 1 /2from Lemma 20, we have
Eb1h
2·IB(b1)Ewh
ψTn
k(w, b1)⟨gT1,N1(w, b1),x⟩kii
=Eb1
2·
Tn
k,x⊗k
+Eb1
2(1−IB(b1))
Tn
k,x⊗k
=
Tn
k,x⊗k
.
Therefore, (C.2) and (C.3) holds. Noting that η1γ≍√
rd2Q−1/C1(logd)C2, We obtain
Ea,w,b1,b2[ϕTn
k(a,w, b1, b2)2]
≤4
(2η1)2kγ2kEa,b2[vk(a, b2)2]Ew,b1[IB(b1)ψTn
k(w, b1)2]
≲4
(2η1)2kγ2kEa,b2[vk(a, b2)2](d(2Q−1)r2)k∥Tn
k∥2
F
=˜O
rkrP−k
2
and
sup
a,w,b1,b2|ϕTn
k(a,w, b1, b2)|
≤2
(2η1)kγk|vk(a, b2)|IA(w)IB(b1)|ψTn
k(w, b1)|
≲2
(2η1)2kγ2k(d(2Q−1)r2)k∥Tn
k∥F∥(2η1γ)gT1,N1(w, b1)∥kIA(w)IB(b1)
≲˜O
rkrP−k
4
from Lemmas 36 and 39. To show the original claim, it suffices to set ϕn(a,w, b1, b2) =PP
k=0ϕTn
k(a,w, b1, b2), which yields the upper bounds on the magnitude of ϕnbecausePP
k=0rkrP−k
2≲rPandPP
k=0rkrP−k
4≲rP.
33C.1.3 Proof of Proposition 33
We discretize Lemma 41 to establish Proposition 33.
Proof. [Proof of Proposition 33] First, in the same discretization manner as the proof of Lemma 13
in [DLS22], we can show that there exists a1, . . . ,aBPsuch that for each n∈[BP],
sup
i∈[N2],T1+1≤t≤T2
mX
j=1an
jσD
2η1Γ(0)
j,jgT1,N1(w(0)
j, b(0)
j),xt
iE
+bj
−hn(xt
i)
2
=˜OrP
m+1
N2
,
sup
T1+1≤t≤T2
mX
j=1an
jσD
2η1Γ(0)
j,jgT1,N1(w(0)
j, b(0)
j),xtE
+bj
−hn(xt)
2
=˜OrP
m+1
N2
andsupn∈[BP]∥an∥2=˜O
rP
m
with high probability ( b(0)
j,bjdenotes the biases at the
first and second initialization, respectively). Recall that w(1)
j= 2η1Γ(0)
j,jgT1,N1(w(0)
j, b(0)
j)−
2η11
T1PT1
t=1f(Xt,yt,xt;W(0),Γ(0),b(0))∇w(0)
jf(Xt,yt,xt;W(0),Γ(0),b(0)). It remains to
bound the effect of the second term. By Lemma 18, we know that the norm of second term is
˜O
η1γ2m√
d
. Similar to the argument in the proof of Lemma 18, the inner product between the
second term and xt,xt
i(i∈[N2], T1+ 1≤t≤T2)is also ˜O
η1γ2m√
d
, with high probability.
From the Cauchy-Schwarz inequality and the Lipschitz continuity of σ,
mX
j=1an
jσD
2η1Γ(0)
j,jgT1,N1(w(0)
j, b(0)
j),xE
+bj
−mX
j=1an
jσD
w(1)
j,xE
+bj
=˜O r
rP
mη1γ2m√
d√m!
=˜O r
rP
m!
.
We obtain the assertion.
C.2 Concentration of Correlation
Next, we evaluate the discrepancy between the empirical correlation1
N2PN2
j=1yjh(xj)and its
expectation for each h∈ H, which decides the approximation error in (b) of (3.1).
Recall that h∈ H can be written in the form as
h(x) =rY
i=11√pi!Hepi(xi) (Q≤p1+···+pr≤P, p1≥0, . . . , p r≥0).
Also note that
1
N2N2X
j=1yjh(xj)−Ex,ς[yh(x)]
=1
N2N2X
j=1ςjh(xj) +1
N2N2X
j=1σ∗(⟨xj,β⟩)h(xj)−Ex[σ∗(⟨x,β⟩)h(x)],
where σ∗(z) =PP
i=Qci
i!Hei(z). First we give an upper bound for |1
N2PN2
j=1σ∗(⟨xj,β⟩)h(xj)−
Ex[σ∗(⟨x,β⟩)h(x)]|.
34Lemma 42. LetD≤2Pbe the degree of σ∗(⟨x,β⟩)h(x), when we regard it as an r-variable
polynomial with respect to (x1, . . . , x r). For all t >0there exist absolute constants MandCD
depending only on Dsuch that
P
1
N2N2X
j=1σ∗(⟨xj,β⟩)h(xj)−Ex[σ∗(⟨x,β⟩)h(x)]≥t

≤2 exp 
−1
CDM2min
1≤k≤Dt√N2
∥E[∇kx(σ∗(⟨x,β⟩)h(x))]∥F2/k!
holds, where CDandMare constants.
Proof. LetF(x1, . . . ,xN) =1
N2PN2
j=1σ∗(⟨xj,β⟩)h(xj). It is a degree- Dpolynomial for
x1,1, . . . , x 1,r, . . . , x N2,r, which are standard Gaussian variables. Also,
∥E[∇kF(x1, . . . ,xN)]∥F=p
N2−1∥E[∇k(σ∗(⟨x,β⟩)h(x))]∥F
is satisfied. Then, applying Lemma 15 to Fyields the desired result.
Then, our aim is to bound ∥E[∇k(σ∗(⟨x,β⟩)h(x))]∥F.
Lemma 43. Leth(x) =Qr
i=11√piHepi(xi)and˜P=p1+···+pr. For Q+˜P≤k≤P+˜P,
∥E[∇k(σ∗(⟨x,β⟩)h(x))]∥2
F≤R2
c¯Ck
holds where ¯Ckis a constant depending only on k. For other k,∥E[∇k(σ∗(⟨x,β⟩)h(x))]∥F= 0.
Proof. First,
∥E[∇k(σ∗(⟨x,β⟩)h(x))]∥2
F=PX
i=Qci
i!E[∇k(Hei(⟨x,β⟩)h(x))]2
F
holds. To calculate E[∇k(Hei(⟨x,β⟩)h(x))], note that from Lemma 8,
Hei(⟨x,β⟩)h(x)
=
q1+···+qr=iX
qj≥0i!
q1!···qr!βq1
1···βqr
rHeq1(x1)···Heqr(xr)
Hep1(x1)√p1!···Hepr(xr)√pr!.(C.4)
Also, from the discussion in Appendix A.3, Exh
∂s1
∂xs1
1···∂sr
∂zsrrHei(⟨x,β⟩)h(x)i
is equal to the coeffi-
cient of (C.4) with respect to1
s1!···sr!Hes1(x1)···Hesr(xr). Therefore, E[∇k(Hei(⟨x,β⟩)h(x))]̸=
Oif and only if i+˜P=k. Then,PP
i=Qci
i!E[∇k(Hei(⟨x,β⟩)h(x))]2
F=
ck−˜P
(k−˜P)!E[∇k(Hek−˜P(⟨x,β⟩)h(x))]2
F.
Here, we can observe that the (j1, . . . , j k)-th entry of the tensorck−˜P
(k−˜P)!E[∇k(Hek−˜P(⟨x,β⟩)h(x))]
is equal to
ck−˜Ps1!···sr!
(s1−p1)!···(sr−pr)!·βs1−p1
1···βsr−pr
r1√p1!···√pr!Is1−p1≥0,...,s r−pr≥0
where si=Pk
k′=1Ijk′=i(i∈[r]). There arek!
s1!···sr!index sets (j1, . . . , j k)having the same entry,
and thus
ck−˜P
(k−˜P)!E[∇k(Hek−˜P(⟨x,β⟩)h(x))]2
F
=X
s1+···+sr=kc2
k−˜P1
p1!···pr!k!
s1!···sr!s1!···sr!
(s1−p1)!···(sr−pr)!2
35·(βs1−p1
1···βsr−pr
r )2Is1−p1≥0,...,s r−pr≥0
=X
q1+···+qr=k−˜P,qj≥0c2
k−˜P1
p1!···pr!k!
(q1+p1)!···(qr+pr)!(q1+p1)!···(qr+pr)!
q1!···qr!2
·(βq1
1···βqr
r)2
=X
q1+···+qr=k−˜P,qj≥0c2
k−˜P(k−˜P)!
q1!···qr!k!
(k−˜P)!(q1+p1)!···(qr+pr)!
p1!···pr!q1!···qr!
·(βq1
1···βqr
r)2. (C.5)
We can verify that there exists a constant ¯Ckdepending only on Psuch that
k!
(k−˜P)!
(q1+p1)!···(qr+pr)!
p1!···pr!q1!···qr!
≤¯Ckholds uniformly; for example, one can naively obtain
k!
(k−˜P)!(q1+p1)!···(qr+pr)!
p1!···pr!q1!···qr!
≤k!(q1+p1)!···(qr+pr)!≤k!kk.
Here we used (q1+p1) +···+ (qr+pr) =k.
Moreover,P
q1+···+qr=k−˜P,qi≥0(k−˜P)!
q1!···qr!(βq1
1···βqrr)2=∥β⊗k−˜P∥2
F= 1 holds. Hence (C.5) is
upper bounded by c2
k−˜P¯Ck≤R2
c¯Ck.
Corollary 44. With high probability,
1
N2N2X
j=1yjh(xj)−Ex[σ∗(⟨x,β⟩)h(x)]=˜O1√N2
holds.
Proof. Plugging the results above and t= Θ(1√N2(logd)P)into Lemma 42 yields
1
N2N2X
j=1σ∗(⟨xj,β⟩)h(xj)−Ex[σ∗(⟨x,β⟩)h(x)]≲1√N2(logd)P
with high probability. Moreover, we can easily verify1
N2PN2
j=1ςjh(xj)=˜O
1√N2
by the
method used in the proof of Lemma 26.
C.3 Proof of Proposition 2
We prove the following Proposition 2 using the preparations above.
Proposition 2. LetXt= (xt
1, . . . ,xt
N2)andyt= (yt
1, . . . , yt
N2)⊤where T1+ 1≤t≤T1+T2.
With high probability over the data distribution and random initialization of the parameters, there
exists ¯Γsuch that
f(Xt,yt,xt;W(1),¯Γ,b)−yt−τ=˜O
s
r3P
m+r2P
N2

holds for all t∈ {T1+ 1, . . . , T 2}, where W(1)is obtained by line 4 of Algorithm 1 and bis
re-initialized in Line 5 of Algorithm 1, where the conditions on η1, λ1, m, γ, N 1, T1are identical to
that in Theorem 1. Moreover, ∥¯Γ∥F=˜O 
r2P/m
is satisfied.
Proof. It suffices to show the proposition for each T1+ 1≤t≤T1+T2, and then take the union
bound. We drop the superscript tfor simplicity.
Note that, from Lemma 8 and Ex[h(x)h′(x)] =Ih=h′forh, h′∈ H , we can expand f∗(x)as
f∗(x) =PBP
n=1Ex∼N(0,Id)[f∗(x)hn(x)]hn(x). Now let A=
a1···aBP
∈Rm×BP, where
36{an}BP
n=1is constructed in Proposition 33. Then, by setting ¯Γ=AA⊤,
*
AA⊤σ(W(1)X+b1⊤
N)y
N2,
σ(w(1)
1⊤x+b1)
...
σ(w(1)
m⊤x+bm)
+
−y
=BPX
n=1(hn(x) +ϵn(x))
E[f∗hn] +
1
N2N2X
j=1yjhn(xj)−E[f∗hn]
+1
N2N2X
j=1yjϵn(xj)

−f∗(x)−ς
=BPX
n=1(hn(x) +ϵn(x))
E[f∗hn] +
1
N2N2X
j=1yjhn(xj)−E[f∗hn]
+1
N2N2X
j=1yjϵn(xj)

−BPX
n=1hn(x)E[f∗hn]−ς
holds, where ϵn(x):=Pm
j=1an
jσ(w(1)
j⊤x+bj)−hn(x)satisfits |ϵn(x)|=˜Op
rP/m+ 1/N2
from Proposition 33.
To bound the error, we note that the following holds for each n:
•Ashn(x)is a polynomial whose degree is at most P, andEx[∇khn(x)] = O(1),|hn(x)|≲
(logd)P/2holds with high probability from Lemma 15.
• From lemma 8, |E[f∗(x)hn(x)]|=O(1)holds.
• From Corollary 17, |yj|≲(logd)P/2holds with high probability.
Hence, for each n∈[BP], we obtain
 
hn(x)|{z}
˜O(1)+ ϵn(x)|{z}
˜O(q
rP
m+1
N2)! 
E[f∗hn]|{z}
O(1)+
1
N2N2X
j=1yjhn(xj)−E[f∗hn]

| {z }
˜O(q
1
N2)+1
N2N2X
j=1yjϵn(xj)
| {z }
˜O(q
rP
m+1
N2)!
−hn(x)E[f∗hn]
=˜O
s
rP
m+1
N2+rP
m+1
N2
.
Combining this with BP= Θ( rP)andm=˜Ω(rP), we arrive at the upper bound.
Finally for ∥¯Γ∥F, we have
∥¯Γ∥F≤BPX
n=1∥an(an)⊤∥F=BPX
n=1∥an∥2=˜O 
r2P/m
.
D Generalization Error Analysis and Proof of Theorem 1
Note that after one-step gradient descent, ∥w(1)
j∥=˜Od,r(1)is ensured with high probability from
Lemma 18 and Corollary 32.
37D.1 Rademacher Complexity Bound
LetW=W(1)and suppose b1, . . . , b mare biases obtained in line 5 of Algorithm 1. Let
FN,G=
(X,y,x)7→Γσ(W(1)X+b1⊤
N)y
N,
σ((w(1)
1)⊤x+b1)
...
σ((w(1)
m)⊤x+bm)
∥Γ∥F≤G
be the set of transformers where the norm of the attention matrix is constrained, and let
Rad T(FN,G) =E{Xt},{yt},{xt},{ϵt}"
sup
f∈FN,G1
TTX
t=1ϵtf(Xt,yt,xt)#
be its Rademacher complexity, where ϵt∼Unif({±1}). Here contexts X∈Rd×Nandy∈RNare
of length N. We can evaluate the Rademacher complexity as follows.
Proposition 45.
Rad T(FN,G) =˜Od,rGm√
T
holds, when |bj|=˜Od,r(1)and∥w(1)
j∥=˜Od,r(1)for all j∈[m].
Proof. First, it holds that
Rad T(FN,G)
=E{Xt},{yt},{xt},{ϵt}"
sup
f∈FN,G1
TTX
t=1ϵtf(Xt,yt,xt)#
=E
sup
∥Γ∥F≤G1
TTX
t=1ϵt*
Γσ(W(1)Xt+b1⊤
N)yt
N,
σ((w(1)
1)⊤xt+b1)
...
σ((w(1)
m)⊤xt+bm)
+

(a)
≤E
1
Tsup
∥Γ∥F≤G∥Γ∥FTX
t=1ϵtσ(W(1)Xt+b1⊤
N)yt
N
σ((w(1)
1)⊤xt+b1)
...
σ((w(1)
m)⊤xt+bm)
⊤
F

=G
TE
TX
t=1ϵtσ(W(1)Xt+b1⊤
N)yt
N
σ((w(1)
1)⊤xt+b1)
...
σ((w(1)
m)⊤xt+bm)
⊤
F

≤G
TvuuuuuutE
TX
t=1ϵtσ(W(1)Xt+b1⊤
N)yt
N
σ((w(1)
1)⊤xt+b1)
...
σ((w(1)
m)⊤xt+bm)
⊤2
F

(b)=G
TvuuuuuutTEX,y,x
σ(W(1)X+b1⊤
N)y
N
σ((w(1)
1)⊤x+b1)
...
σ((w(1)
m)⊤x+bm)
⊤2
F

=G√
TvuuuutmX
k,l=1EX,y,x

1
NNX
j=1yjσ((w(1)
k)⊤xj+bk)σ((w(1)
l)⊤x+bl)
2

38(c)
≤G√
TvuutmX
k,l=1Ex∼N(0,Id),x′∼N(0,Id),f∗,ςh
(f∗(x) +ς)2σ((w(1)
k)⊤x+bk)2σ((w(1)
l)⊤x′+bl)2i
(d)
≤G√
TvuutmX
k,l=1Ex,f∗,ςh
(2f∗(x)2+ 2ς2)(2((w(1)
k)⊤x)2+ 2b2
k)i
Ex′h
(2((w(1)
l)⊤x′)2+ 2b2
l)i
,
where the annotated equality/inequalities are obtained by (a) Cauchy–Schwarz inequality, (b)
Eϵt∼Unif{±1}[∥P
tϵtAt∥2
F] =P
t∥At∥2
F, (c) convexity of f(z) =z2, and (d) (σ(α+β))2≤
(α+β)2≤2α2+ 2β2.
Let us further evaluate the last line: from the assumption and Lemma 7, we know that ∥wk∥,|bk|and
E[f∗(x)2]are all ˜O(1): in particular, we can easily obtain that Ex∼N(0,Id)h
((w(1)
k)⊤x)2i
=˜Od,r(1).
Consequently, the last line isG√
TrPm
k,l=1Ex∼N(0,Id),f∗h
(2f∗(x)2)(2((w(1)
k)⊤x)2)i
+˜Od,r(1),
and remains to evaluate Ex∼N(0,Id),f∗h
f∗(x)2((w(1)
k)⊤x)2i
. On this term, we obtain
Ex,f∗h
f∗(x)2((w(1)
k)⊤x)2i
≤Ex,f∗
f∗(x)41/2Exh
((w(1)
k)⊤x)4i1/2
=˜O(1) (∵Lemma 7).
Hence we can conclude Rad T(FN,G) =O
Gm√
T
.
Corollary 46. Let
˜FN,G=
(X,y,x, y)7→Γσ(W(1)X+b1⊤
N)y
N,
σ((w(1)
1)⊤x+b1)
...
σ((w(1)
m)⊤x+bm)
∥Γ∥F≤G
∪
(X,y,x, y)7→y
.
Then, Rad T(˜FN,G) = ˜Od,r
Gm√
T
holds when |bj|=˜Od,r(1)and∥w(1)
j∥=˜Od,r(1)for all
j∈[m].
Proof. Note that
E{Xt},{yt},{xt},{yt},{ϵt}"
sup
f∈˜FN,G1
TTX
t=1ϵtf(Xt,yt,xt, yt)#
≤E{Xt},{yt},{xt},{ϵt}"
sup
f∈FN,G1
TTX
t=1ϵtf(Xt,yt,xt)#
+E{yt},{ϵt}"1
TTX
t=1ϵtyt#
.
This follows from supf∈FN,G1
TPT
t=1ϵtf(Xt,yt,xt)≥0, asFN,Gcontains zero function corre-
sponding to Γ=O. The second term in the last line can be bounded as
E{yt},{ϵt}"1
TTX
t=1ϵtyt#
≤vuuutE{yt},{ϵt}
 
1
TTX
t=1ϵtyt!2

=1√
TE[(yt)2] =O1√
T
by Lemma 7.
39D.2 Prompt Length-free Generalization Bound
The ICL risk for prompt length Nwas defined as
RN(f) =EX,y,x,y[|f(X1:N,y1:N,x;W,Γ,b)−y|],
where the length of X1:Nandy1:Nis fixed to N. In this section, we show that the ICL risk
“converges” when the context length is sufficiently large.
Proposition 47. Assume that ∥wj∥=˜Od,r(1)and|bj|=˜Od,r(1)for each j∈[m]. Then,
|RN(f)− R M(f)|=˜O
∥Γ∥Fmp
1/N+ 1/M
holds.
Proof. Note that
|RN(f)− R M(f)|
≤E[|f(X1:N,y1:N,x;W,Γ,b)−f(X1:M,y1:M,x;W,Γ,b)|]
=E
*
Γσ(WX 1:N+b1⊤
N)y1:N
N−σ(WX 1:M+b1⊤
M)y1:M
M
,
σ(w⊤
1x+b1)
...
σ(w⊤
mx+bm)
+

(a)
≤∥Γ∥FE
σ(WX 1:N+b1⊤
N)y1:N
N−σ(WX 1:M+b1⊤
M)y1:M
M
σ(w⊤
1x+b1)
...
σ(w⊤
mx+bm)


≤∥Γ∥FE"σ(WX 1:N+b1⊤
N)y1:N
N−σ(WX 1:M+b1⊤
M)y1:M
M2#1/2
·E

σ(w⊤
1x+b1)
...
σ(w⊤
mx+bm)
2
1/2
,
where (a) holds because a⊤Γb=Γ◦(ab⊤)≤ ∥Γ∥F∥ab⊤∥F=∥Γ∥F∥a∥∥b∥.
Let us bound E
σ(WX 1:N+b1⊤
N)y1:N
N−σ(WX 1:M+b1⊤
M)y1:M
M2
, which is to
Pm
j=1Ef∗
EX,y
1
NPN
i=1σ(w⊤
jxi+bj)yi−1
MPM
i=1σ(w⊤
jxi+bj)yi2
. For simplic-
ity, we drop the subscript jand obtain
EX,y
 
1
NNX
i=1σ(w⊤xi+b)yi−1
MMX
i=1σ(w⊤xi+b)yi!2

≤2E
 
1
NNX
i=1σ(w⊤xi+b)yi−E
σ(w⊤x+b)y!2

+2E
 
1
MMX
i=1σ(w⊤xi+b)yi−E
σ(w⊤x+b)y!2

=2
NV 
σ(w⊤x+b)y
+2
MV 
σ(w⊤x+b)y
≤2
N+2
M
E
σ(w⊤x+b)2(f∗(x) +ς)2
(a)
≤2
N+2
M
E
(2(w⊤x)2+ 2b2)(2f∗(x)2+ 2ς2)
.
40Here (a) is obtained in the same vein as the derivation of (d) in the proof of Proposition 45. Moreover,
in the same way as that proof, we can show that E
(2(w⊤x)2+ 2b2)(2f∗(x)2+ 2ς2)
=˜Od,r(1).
Consequently we get E
σ(WX 1:N+b1⊤
N)y1:N
N−σ(WX 1:M+b1⊤
M)y1:M
M2
= 2
N+2
M˜Od,r(m).
Next, we observe that
E

σ(w⊤
1x+b1)
...
σ(w⊤
mx+bm)
2
≤2mX
j=1E[(w⊤
jx)2+b2
j] =˜Od,r(m).
Putting all things together, we arrive at
|RN(f)− R M(f)|=˜O
∥Γ∥Fmp
1/N+ 1/M
.
D.3 Proof of Theorem 1
Finally we are ready to prove our main theorem.
Proof. [Proof of Theorem 1] Let ¯Γbe the attention matrix constructed in Proposition 2 and let Γ∗be
the minimizer of the ridge regression problem (line 8 in Algorithm 1). By the equivalence between
optimization with L2regularization and norm-constrained optimization, there exists λ2such that
∥Γ∗∥F≤ ∥¯Γ∥F=˜O 
r2P/m
,
 
1
T2T1+T2X
t=T1+1|yt−f(Xt,yt,xt;W(1),Γ∗,b)|!2
≤1
T2T1+T2X
t=T1+1(yt−f(Xt,yt,xt;W(1),Γ∗,b))2
≤1
T2T1+T2X
t=T1+1(yt−f(Xt,yt,xt;W(1),¯Γ,b))2.
Then, from Proposition 2,
1
T2T1+T2X
t=T1+1|yt−f(Xt,yt,xt;W(1),Γ∗,b)| −τ=˜O
s
r3P
m+r2P
N2

holds.
We evaluate RN2(f)−τwhere f=f(Xt,yt,xt;W(1),Γ∗,b)using the Rademacher complexity.
RN2(f)−τ
=1
T2T1+T2X
t=T1+1|yt−f(Xt,yt,xt;W(1),Γ∗,b)|
+ 
RN2(f)−1
T2T1+T2X
t=T1+1|yt−f(Xt,yt,xt;W(1),Γ∗,b)|!
−τ
≤˜O
s
r3P
m+r2P
N2

+ sup
f∈˜FN,∥¯Γ∥F 
RN2(f)−1
T2T1+T2X
t=T1+1|yt−f(Xt,yt,xt, yt)|!
41holds, where ˜Fis defined in Corollary 46. We can evaluate the expectation value of the second term
of the last line as, using ϵti.i.d.∼Unif{±1},
E{Xt},{yt},{xt},{yt}
sup
f∈˜FN,∥¯Γ∥F 
RN2(f)−1
T2T1+T2X
t=T1+1|yt−f(Xt,yt,xt, yt)|!

(a)
≤2E{Xt},{yt},{xt},{yt},{ϵt}
sup
f∈˜FN,∥¯Γ∥F1
T2T1+T2X
t=T1+1ϵt|yt−f(Xt,yt,xt, yt)|

(b)
≲Rad T2(˜FN2,∥¯Γ∥F) +Ey,ϵ"
1
T2T1+T2X
t=T1+1ϵtyt#
≤Rad T2(˜FN2,∥¯Γ∥F) +Ey,ϵ
 
1
T2T1+T2X
t=T1+1ϵtyt!2
1/2
≤Rad T2(˜FN2,∥¯Γ∥F) +1√T2E
y21/2(c)=˜O
s
r4P
T2
.
Here, (a) holds from the standard symmetrization argument (see eq. (4.17) in [Wai19] for example),
(b) is derived using eq. (1) in [ Mau16 ], noting that (x, y)⊤7→ |x−y|is√
2-Lipschitz continuous,
and (c) follows from the bound of ∥¯Γ∥Fin Proposition 2, together with Lemma 7 and Proposition 45.
Note that
sup
f∈˜FN,∥¯Γ∥F 
RN2(f)−1
T2T1+T2X
t=T1+1|yt−f(Xt,yt,xt, yt)|!
(D.1)
is nonnegative because f(X,y,x, y) =yis included in ˜FN,∥¯Γ∥F. Hence from Markov’s inequality,
(D.1) is ˜Oq
r4P
T2
with probability at least 0.995. Hence we obtain
RN2(f)−τ=˜O
s
r3P
m+r2P
N2+s
r4P
T2
.
Now we have done the upper bound for RN2(f). ForRN∗(f), we can use Proposition 47.
Note that all the desired events occur with high probability, except for the one described above,
which occurs with probability 0.995. This completes the proof of Theorem 1.
E Derivation of Simplified Self-attention Module
We derive equation (2.6) , following the same line of argument as [ ZFB23 ]. As we assumed that
WPVWKQare in the form as (2.4), the output of the attention layer is written as
˜fAttn(E;WPV,WKQ) =E+
∗ ∗
01×mv
E·E⊤
K ∗
01×m∗
E
N. (E.1)
Note that we adopt the right-bottom entry
˜fAttn(E;WPV,WKQ)
m+1,N+1as a prediction for
output of the query. By (E.1). Hence we have

˜fAttn(E;WPV,WKQ)
m+1,N+1
42=[01×mv]E·E⊤
K ∗
01×m∗
σ(w⊤
1x+b1)
...
σ(w⊤
mx+bm)
0

N
=v[y1··· yN0]·E⊤
K
σ(w⊤
1x+b1)
...
σ(w⊤
mx+bm)

0

N
=v[y1··· yN]·σ(WX +b1⊤
N)⊤K
σ(w⊤
1x+b1)
...
σ(w⊤
mx+bm)

N
=*
vK⊤σ(WX +b1⊤
N)y
N,
σ(w⊤
1x+b1)
...
σ(w⊤
mx+bm)
+
.
Setting Γ=vK⊤yields equation (2.6).
F Details of Experiments
F.1 Detailed Experimental Settings
For the experiments in Section 4, test loss was averaged over 128 independent tasks, with 2048 and
128 independent queries generated for each task in the experiment of Figure 1 and 2, respectively.
During the validation of the experiment of Figure 1, the coefficients {ci}in the single-index model
were fixed to be (c2, c3) = (√
2·2!/2,√
2·3!/2)to reduce the variance in the baseline methods.
GPT-2 model. The experiments in Section 4 were based on the architecture used in [ GTLV22 ]1,
whose backbone is the GPT-2 model [ RWC+19]. Given the prompt {(xi, yi)}N+1
i=1, the embedding
E∈Rd×(2N+2)was constructed as E= [x1,˜y1, . . . ,xN,˜yN]where ˜yi= [yi,0, . . . , 0]⊤. This
embedding was mapped to ˜E∈R256×(2N+2)through the read-in layer. The transformed embedding
˜Ewas then passed through the 12-layer GPT-2 backbone with 8 heads, with dropout disabled.
Finally, the output was converted into a 2N+ 2-dimensional vector through the read-out layer, where
the(2k−1)-th entry (k∈[N+ 1]) corresponds to the prediction ˆyk(xt
1, yt
1. . . ,xt
k−1, yt
k−1,xt
k)
foryk, that is, the answer for the query xt
kreading the context of length k−1. We used the
Adam optimizer [ KB15 ] with a learning rate of 0.0001 . The training loss at each step was set as
1
BPB
t=1PN+1
k=1 
yt
k−ˆyk(xt
1, yt
1. . . ,xt
k−1, yt
k−1,xt
k)2where B= 8is the minibatch size.
Baseline algorithms. Baseline algorithms used in the comparison experiment are as follows:
•Two-layer neural network f(x) =Pm
j=1ajσ(w⊤
jx)where σ= ReLU andm= 256 . The
initialization scale followed the mean-field regime, with aji.i.d.∼Unif({±1/m})andwji.i.d.∼
Unif(Sd−1). The architecture was trained using the Adam optimizer, with a learning rate of 0.1
for the first layer and 0.1/mfor the second layer, along with a weight dacay rate of 0.0001 . Early
stopping was applied to prevent overfitting.
•Kernel ridge regression using the Gaussian RBF kernel. The ridge regression parameter λwas set
toλ= 0.01.
Again, note that these baseline algorithms were trained from scratch for each validation task, whereas
the parameters of the pretrained GPT-2 model remained unchanged across all validation tasks.
1https://github.com/dtsip/in-context-learning , MIT License.
43F.2 Experiment on Our Architecture and Algorithm 1
102
in-context sample size N*101
100prediction risk
kernel
two-layer NN
pretrained TF
Figure 3: In-context generalization error of
kernel ridge regression, neural network + one-
step gradient descent, and Algorithm 1 on our
Transformer (2.6).In addition to the GPT-2 experiments, we also con-
ducted comparison between our Algorithm 1 on our
simplified architecture and baseline algorithms. Data
is generated as yt
i= He 2(⟨xt
i,βt⟩), where βti.i.d.∼
Unif{β∈Rd|β= [β1:r,0]⊤,∥β∥= 1}with
d= 32 andr= 2. We pretrained our transformer ar-
chitecture (2.6) withm= 8,000, using Algorithm 1
with T1= 105, N1= 104, η1= 105(recall that
η1≍m3
2rd2Q−1
2·(logd)−Cηshould be large) and
T2= 3,000, N2= 512 , λ2= 0.1. We used mini-batch
SGD to find an approximate optimizer Γ∗of the ridge
regression. For validation, we altered the context length
from 24.5to29and compared its test error with one-
step GD (training the first layer via one-step gradient
descent, and then conducting ridge regression on the sec-
ond layer) on a two-layer NN of width 8,000and kernel
ridge regression with RBF kernel working directly on
the test prompt. In Figure 3 we see that our simplified transformer also outperforms neural network
and kernel method, especially in short context regime.
44NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: They are presented accurately.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We mentioned the limitation of our result and the future challenge in Section 5.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
45Justification: We presented full-proofs for our results in the appendix, and sketched the
proof in Section 3.2.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: See Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
46Answer: [No]
Justification: This is a theory paper where the toy experiments are not central to our
contribution.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer:[Yes]
Justification: See Section 4 and Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: See Section 4 and Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
47•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: We only have toy experiments and it is not a central part of this paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Yes, and we checked the code.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Our results are purely theoretical working on a small simplified model, thus
there seems no societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
48•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We don’t have any models or datasets to release.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We included the information in Appendix F.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
49•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We don’t release any assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our paper does not include such subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our paper does not include such subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
50