Learning to Mitigate Externalities: the Coase Theorem
with Hindsight Rationality
Antoine Scheid1Aymeric Capitaine1
Etienne Boursier2Eric Moulines1Michael I. Jordan3,4Alain Durmus1
1Centre de Mathématiques Appliquées – CNRS – École polytechnique – Palaiseau, 91120, France
2INRIA Saclay, Université Paris Saclay, LMO - Orsay, 91400, France
3University of California, Berkeley
4Inria, Ecole Normale Supérieure, PSL Research University - Paris, 75, France
Abstract
In economic theory, the concept of externality refers to any indirect effect resulting
from an interaction between players that affects the social welfare. Most of the
models within which externality has been studied assume that agents have perfect
knowledge of their environment and preferences. This is a major hindrance to
the practical implementation of many proposed solutions. To address this issue,
we consider a two-player bandit setting where the actions of one of the players
affect the other player and we extend the Coase theorem [Coase, 2013]. This result
shows that the optimal approach for maximizing the social welfare in the presence
of externality is to establish property rights, i.e., enable transfers and bargaining
between the players. Our work removes the classical assumption that bargainers
possess perfect knowledge of the underlying game. We first demonstrate that in the
absence of property rights, the social welfare breaks down. We then design a policy
for the players which allows them to learn a bargaining strategy which maximizes
the total welfare, recovering the Coase theorem under uncertainty.
1 Introduction
The concept of externality is used in economics to capture phenomena that impact the global welfare
stemming from economic interactions without any compensation [Buchanan and Stubblebine, 2006,
Shah et al., 2018]. Externality is generally considered as market failure since they result in a loss
of collective welfare. Given its practical importance [Dahlman, 1979, Greenfield et al., 2009],
mechanisms that characterize and mitigate externalities are central to modern economic thinking.
A first approach to tackle the adverse effects of externalities in modern economics was based on
quotas and taxation [see, e.g., Pigou, 2017, and the references therein]. However, Coase’s theorem
[Coase, 2013] shows that in the presence of well-defined property rights and low transaction costs,
parties affected by externalities can privately negotiate efficient solutions, and recover a welfare
efficient allocation through transfers and bargaining.
Throughout the paper, we use the following simple example to illustrate our results.
Example 1. Consider two firms 1 (upstream) and 2 (downstream) respectively producing quantities
q1⩾0andq2⩾0of a good sold at a fixed price p >0. They incur strictly increasing and convex
costs, captured by the differentiable cost functions c1:q17→c1(q1)andc2:q27→c2(q2), satisfying
38th Conference on Neural Information Processing Systems (NeurIPS 2024).c1(0) = 0 andc2(0) = 0 . We assume that firm 1exerts on firm 2 a constant externality α >0per
unit produced. In other words, their profit functions (or utilities) are given by
π1(q1) =pq1−c1(q1)and π2(q1, q2) =pq2−c2(q2)−αq1.
One simple concrete illustration consists in an upstream firm emitting pollutants that reduce the
downstream firm’s production. If the upstream firm owns the property rights, it may receive a
payment from the downstream one to reduce its output and thereby pollution. On the other hand, if
the downstream firm owns the property right, it may require compensation from the upstream firm to
allow its operation.
The Coase theorem demonstrates that in both cases with appropriate property rights, the resulting
levels of production would be welfare efficient. The theorem is typically explained in textbooks under
the assumption that the players have perfect knowledge of their own utility or profit function, as well
as that of others. However, this assumption is unlikely to hold in real-world scenarios, where players
have to learn about their own preferences and those of their competitors.
This example is, of course, a simplification of real world scenarios. For example, Abildtrup et al.
[2012] consider a more complex setting to model the interaction between farmers and waterworks
in Denmark where the farmers have the property rights whereas the waterworks can pay to reduce
pollution. In particular, they demonstrate the failure of the theorem, attributing it to the breakdown of
the main assumptions: no transaction costs, maximizing behaviors and perfect information, with an
important focus on strategic behaviors and the asymmetry of information. We restore the latter in our
work and provide foundations for the theorem to hold in more realistic scenarios.
A key question is whether the Coase theorem holds when players learn their preferences over time.
We investigate this question within the framework of a multi-armed bandit learning. We build upon
recent works that extend the classical bandit setting to economics in which there are two players
interacting via principal-agent protocols [Dogan et al., 2023b,a, Scheid et al., 2024]. This allows us
to capture, for example, a version of the two-firm problem where firms are uncertain regarding both
their profit functions and the degree of externality on other firms. We represent production decisions
in this problem as arms which can be played by the firms at any round over time, with the goal of
finding decisions that maximize their rewards. More precisely, we assume that the reward of the
upstream firm only depends on its own action, while the reward of the downstream firm depends on
both its action and the upstream firm’s action. This dependency on both actions allows us to capture
externalities.
The property rights in Example 1, or more generally over a bandit instance, amount to giving a
firm the possibility of engaging in monetary transfers that influence the arms that are played over
time. The owner of the bandit instance will face a problem of bandit learning with transfers with an
upstream player who is also learning his preferences.
To account for the efficiency of a policy in this setup, we extend the classical static notion of welfare
efficiency to the online setting. We say that a policy is Welfare efficient if the social welfare regret is
sub-linear. Proving the Coase theorem within our setup therefore boils down to show that if the bandit
owner (who is without loss of generality the upstream player in this study1) runs a no-regret bandit
algorithm to learn and exploit his preferences, the downstream player can then choose an optimal
transfer scheme leading to a sub-linear total regret.
Our contributions are as follows:
•We show that when an upstream agent exerts externality on a downstream agent, in the
absence of property right, the social welfare breaks down. Put differently, no joint policy of
the agents can be welfare efficient.
•We then introduce property rights and show how it affects the game. In this case, bargaining
and transfers are available to the players. We propose a policy for the downstream player
that leads to welfare efficiency when the upstream player follows any black-box no-regret
policy under mild assumption. This solution addresses the breakdown issue at equilibrium.
Put together, we show an online version of the Coase theorem .
1The fact that efficiency is restored whoever is given the property rights is known as the invariance property
of the Coase theorem.
22 Setup and Inefficiency of Externality
2.1 Bandit game
We consider a sequential bandit game in which two players (downstream and upstream) simulta-
neously play actions in a bandit instance for a horizon T∈N⋆. The action set for both players is
A={1, . . . , K }, K∈N⋆.
The reward distributions of the agents differ. Given a family of distributions {γa:a∈ A} indexed
byA, the upstream player’s rewards are provided by an i.i.d. family of random variables
{(Za(t))t∈[T]:a∈ A} ,where Za(t)∼γafor any t∈[T]anda∈ A.
To model the externality exerted by the upstream on the downstream player, we assume that the latter
has a reward that depends both on her action and on that of the upstream player. Formally, this is
modeled through a family of distributions {νa,b:a, b∈ A} double-indexed by Aand an i.i.d. family
of random variables
{(Xa,b(t))t∈[T]:a, b∈ A} ,where Xa,b(t)∼νa,b
is the reward received by the downstream player at time tif she pulls the arm band the upstream
player pulls the arm a.
Players. We assume that players are risk-neutral expected-utility maximizers, and we define their
expected utilities for any (a, b)∈ A × A as
vup(a) =Z
z γa(dz)∈Rand vdown(a, b) =Z
x νa,b(dx)∈R.
The distributions (γa)a∈Aand(νa,b)(a,b)∈A2are unknown to both the downstream and the upstream
players and they aim to learn the distributions with best mean rewards by sequentially observing
samples from {(Za(t))t∈[T]:a∈ A} and{(Xa,b(t))t∈[T]:a, b∈ A} .
Moreover, we suppose that players are rational in hindsight; that is, they minimize their regret.
Formally, the upstream player aims to minimize his regret defined as
Rup
n(T,Πup
n) =Tµ⋆,up−E"TX
t=1vup(At)#
,where µ⋆,up= max
a∈Avup(a), (1)
while the downstream player seeks to minimize her external regret defined as
Rdown
n(T,Πup
n,Πdown
n) =E"TX
t=1max
b∈Avdown(At, b)−vdown(At, Bt)#
, (2)
where the players’ actions (At)t∈[T],(Bt)t∈[T]as well as their policies Πup
n,Πdown
n are defined below.
Note that the utility of the downstream player also depends on the actions taken by the upstream
player, which represents the externality exerted by the upstream player on the downstream player,
hence the strategic dimension of our setting. We first consider a game where no property right is
defined, so each player is free to pick his preferred arm irrespectively of the other player’s choice.
This will result in a breakdown of the total utility.
Policies without property rights. Consider first the upstream player. Based on a policy Πup
n(for
example a no-regret bandit algorithm such as the Upper Confidence Bounds algorithm ( UCB)
[Auer, 2002] or the ε-greedy algorithm [Robbins, 1952, Langford and Zhang, 2007]), we define
his history (Hup,n
t)t∈[T]by induction. We set Hup,n
0=∅and supposing that Hup,n
tis defined for
t∈[T], then
Hup,n
t+1=Hup,n
t∪
At+1, Vt+1, ZAt+1(t+ 1)	
,
where (Vs)s∈N⋆is a family of independent uniform random variables in [0,1], allowing for random-
ization in the policy, and At+1is provided by Πup
n, following Πup
n: (Vt+1,Hup,n
t)7→At+1.
Second, consider the downstream player and an algorithm Πdown
n (specifically a no-regret bandit
algorithm). We define her history (Hdown ,n
t )t∈[T]by induction. We set Hdown ,n
0 =∅and supposing
thatHdown ,n
t is defined for t∈[T], then
Hdown ,n
t+1 =Hdown ,n
t ∪
At+1, Bt+1, Ut+1XAt+1,Bt+1(t+ 1)	
,
3where (Us)s∈N⋆is a family of independent uniform random variables in [0,1]allowing for random-
ization in the policy and Bt+1is provided by Πdown
n , following Πdown
n: (Ut+1,Hdown ,n
t )7→Bt+1.
Welfare efficiency. We now introduce the notion of Welfare efficiency for our setup. The global
utility, or social welfare , of the players at round tis defined as vup(At) +vdown(At, Bt). We define
thesocially optimal action (asw, bsw)∈ A × A of the game as
(asw, bsw)∈argmaxa,b∈Avup(a) +vdown(a, b), (3)
as well as the global regret (orsocial welfare regret ) associated with policies Πup
nandΠdown
n as
Rsw(T,Πup
n,Πdown
n) =T 
vup(asw) +vdown(asw, bsw)
−TX
t=1E
vup(At) +vdown(At, Bt)
.
(4)
Then, the joint policies Πup
nandΠdown
n for the players are said to be Welfare efficient if
lim
T→+∞Rsw(T,Πup
n,Πdown
n)/T= 0.
Intuitively, this condition implies that the frequency of the socially optimal action (asw, bsw)tends
to 1 as Tgoes to infinity. In this sense, it mimics the usual, static Welfare efficiency criterion. As
we will see, (Πup
n,Πdown
n)is typically not Welfare efficient when there is a disalignment in the game
between the players’ individual interests based on their rationality and the social welfare.
2.2 Inefficiency without property rights
We first present a result that captures the adverse consequence of externality on social welfare. The
upstream player does not take into account the indirect cost incurred by the downstream player when
he chooses his action. This drives the social welfare away from its optimal level. We illustrate this
fact within our simple bilateral externality example.
Example 1 (continuing from p. 1) .We show that the competitive outcome, where each firm maximizes
its profit independently, is not welfare efficient in the presence of externality. Define the social welfare
as the function
W: (q1, q2)7→π1(q1) +π2(q1, q2) =p(q1+q2)−(c1(q1) +c2(q2))−αq1. (5)
By definition, the welfare efficient outcome (q⋆
1, q⋆
2)∈R2
+satisfies W(q⋆
1, q⋆
2)⩾W(q1, q2)for any
(q1, q2)∈R2
+. Since Wis differentiable and strictly concave, (q⋆
1, q⋆
2)is uniquely defined by the
condition ∇W(q⋆
1, q⋆
2) = 0 , that is
c′
1(q1)−α=pand c′
2(q2) =p . (6)
Note that at the welfare efficient optimum, firm 1 does not equalize marginal cost with marginal profit,
but produces less to account for the negative effect of externality on firm 2. We now characterize the
competitive outcome (q′
1, q′
2)∈R2
+. Since π1andπ2are differentiable and strictly concave, (q′
1, q′
2)
satisfies
c′
1(q′
1) =pand c′
2(q′
2) =p . (7)
For the competitive outcome to be welfare efficient, we require, by Equation (6)and Equation (7),
c′
1(q′
1)−α=c′
1(q′
1),that is α= 0.
This proves that whenever there are externalities, no competitive outcome is efficient.
We now show that in our model, when there is no property right and under mild assumptions,
no achievable policy is welfare efficient whenever there is a misalignment between the players’
interests and the social welfare. The upstream player’s policy Πup
nis said to be no-regret if
limT→+∞Rup
n(T,Πup
n)/T= 0, where Rup
nis defined in (1).
Theorem 2. Suppose that argmaxa∈Avup(a)is the singleton {au
⋆}and that
vup(asw) +vdown(asw, bsw)−vup(au
⋆) +vdown(au
⋆, b)>0, (8)
for any b∈ A. In the absence of property rights and when the upstream player runs any no-regret
policy Πup
n, we have Rsw(T,Πup
n,Πdown
n) = Ω( T). Therefore, Rsw(T,Πup
n,Πdown
n) = Ω( T)and
(Πup
n,Πdown
n)is not welfare efficient.
4Condition (8)in Theorem 2 represents the unalignment between the upstream player’s preference
and the optimal choice from a social welfare point of view. Note that the upstream and downstream
players can both have an o(T)external regret, while the social welfare regret still grows linearly
withTbecause of the unfavorable interactions between their policies.
3 Online Property Game with Bargaining Players
3.1 Online Property Game
We now consider the same repeated game in the form of a property game where one of the players
possesses the bandit instance ( upstream player ). As in the original setup of Coase [2013], the other
player ( downstream player ) will provide the bandit owner with transfers to incentivize him to choose
some specific action and influence the outcome of the game in her favor.
We show in Appendix B that our method applies similarly when property rights are given to the
upstream player rather than the downstream player. Hence, there is no loss of generality in considering
the aforementioned framework. In this sense, we recover the invariance property of the Coasean
bargaining [Mas-Colell et al., 1995].
Example 1 (continuing from p. 1) .We now illustrate how Coasean bargaining re-instaures efficiency.
Suppose without loss of generality that property rights are such that firm 2 can pay τ∈R+to firm 1
for it to operate at a level ˜q1. Profits become
¯π2: (q1, q2, τ,˜q1)7→π2(q1, q2)− 1{q1=˜q1}τand ¯π1: (q1, τ,˜q1)7→π(q1) + 1{q1=˜q1}τ.
Consider the competitive outcome (q1, q2, τ,˜q1)∈R4
+which satisfies
q1=q1(τ,˜q1)∈arg max
q′
1⩾0¯π1(q′
1, τ,˜q1)and
¯π2(q1(τ,˜q1), q2, τ,˜q1) = max
q′
2,τ′,˜q′
1¯π2(q1(τ′,˜q′
1), q′
2, τ′,˜q′
1).
The condition on q1accounts for the rationality of the firm 1and the fact that its choice depends on
the payment (τ,˜q1). Obviously, the optimal solution is reached for ˜q1=q1andτ= max q′π1(q′)−
π1(˜q1). Plugging this back in the expression of ¯π2then yields
(q1, q2) = argmaxq′
1,q′
2π1(q′
1) +π2(q′
2) = argmaxq′
1,q′
2W(q′
1, q′
2),
so the competitive outcome (q1, q2)is welfare efficient.
The transfers at each step can be interpreted as a contract between two players [see, e.g., Bolton and
Dewatripont, 2004, Salanié, 2005, for general contract theory] and providing the right amount of
incentives relates to adjusting a contract in an online setting [see Dütting et al., 2019, Guruganesh
et al., 2021, Zhu et al., 2022, Fallah and Jordan, 2023, Guruganesh et al., 2024, Ananthakrishnan
et al., 2024, for learning-based perspectives about contracts].
Similarly to Example 1, we modify the players’ policies to now account for the transfer τ(t)that the
downstream player offers at round tto the upstream player if he picks action ˜at. The downstream
player’s policy at round tdoes not only output an arm Btbut now a triple (˜at, τ(t), Bt), where Bt
is the arm that she should play and ˜atis the arm on which a transfer τ(t)is offered to the upstream
player. On the upstream player’s side, the policy still outputs an arm Atto play but also takes as an
input the incentive (˜at, τ(t)). In addition, the instantaneous utility of the upstream player becomes
ZAt(t) +1˜at(At)τ(t), whereas the downstream player receives XAt,Bt(t)−1˜at(At)τ(t).
Policies with property rights. Based on policies Πup
pfor the upstream player and Πdown
p for the
downstream player, we define their histories (Hup,p
t)t∈[T]and(Hdown ,p
t )t∈[T]by induction. We set
Hup,p
0=∅,Hdown ,p
0 =∅and supposing that Hup,p
t,Hdown ,p
t are defined for t∈[T], then
Hup,p
t+1=Hup,p
t∪
˜at+1, τ(t+ 1), At+1, Vt+1, ZAt+1(t+ 1)	
and
Hdown ,p
t+1 =Hdown ,p
t ∪
˜at+1, τ(t+ 1), At+1, Bt+1, Ut+1, XAt+1,Bt+1(t+ 1)	
,
5where (Vs)s∈N⋆,(Us)s∈N⋆are two families of independent uniform random variables in
[0,1]allowing for randomization in the policies, and the remaining quantities are given by
Πdown
p: (Ut+1,Hdown ,p
t )7→(˜at+1, τ(t+1), Bt+1)andΠup
p: (˜at+1, τ(t+1), Vt+1,Hup,p
t)7→At+1.
Players’ goal. Given a transfer τfrom the downstream to the upstream player on arm ˜a, actions aand
brespectively are chosen by the upstream and the downstream player, the upstream player’s expected
utility reads vup(a) +1˜a(a)τwhile the downstream player’s expected utility is vdown(a, b)−1˜a(a)τ.
This defines the upstream player’s expected regret for a horizon Tas
Rup
p(T,Πup
p,Πdown
p) =E"TX
t=1max
a∈A{vup(a) +1˜at(a)τ(t)} −(vup(At) +1˜at(At)τ(t))#
.(9)
Based on the upstream player’s utility, the downstream player aims on a single round at proposing an
optimal transfer τopton an arm aopt∈ A as well as picking an arm bopt∈ A which solves
maximize (a, b, τ )7→vdown(a, b)−τ
such that τ∈R+, b∈ A, a∈argmaxa′∈A{vup(a′) +1a(a′)τ}.(10)
Her regret for any horizon Tis defined as
Rdown
p(T,Πup
p,Πdown
p) =Tµ⋆,down−E"TX
t=1vdown(At, Bt)−1˜at(At)τ(t)#
, (11)
where we define µ⋆,down=vdown(aopt, bopt)−τoptas the optimal utility she can aim for. We can
see that the downstream player’s influence is exerted through her action choice Btas well as through
transfers which enable her to influence the upstream player’s actions. Hence, the notion of external
regret is obsolete here. The game has now the form of a repeated Stackelberg game [V on Stackelberg,
2010].
Lemma 1. Recall that µ⋆,downis the downstream player’s optimal reward as defined as a solution
of(10). We have µ⋆,down= max a,b∈A{vdown(a, b) +vup(a)} −max a′∈A{vup(a′)}, as well as
(aopt, bopt) = (asw, bsw)andµ⋆,up+µ⋆,down=vup(asw)+vdown(asw, bsw) = max a,b∈A{vup(a)+
vdown(a, b)}, where µ⋆,upis defined in Equation (1). Moreover, for any integer T∈N⋆, and policies
Πup
p,Πdown
p , we have that
Rsw(T,Πup
p,Πdown
p)⩽Rup
p(T,Πup
p,Πdown
p) +Rdown
p(T,Πup
p,Πdown
p).
This lemma has an interesting economic interpretation: if both players individually seek for their
own interest within this online property game, they will together converge towards the optimal global
utility. Individual rationality moves the outcome of the game towards the optimal social welfare. The
transfers allow the players to align their goals and share the global reward, in line with the Coase
theorem. Consequently, if both players run no-regret policies Πup
pandΠdown
p, the social welfare
regret will also be in o(T). The rest of the paper shows that such no-regret policies exist. To this end,
we introduce the following assumptions.
Without loss of generality, we assume that the upstream player’s utility is rescaled and shifted, which
corresponds to the following assumption on the reward distribution (γa)a∈AinR.
H1. For any a∈ A, we have vup(a)∈[0,1].
We now make a high probability bound assumption on the upstream player’s regret.2.
H2.There exist C, ζ > 0, κ∈[0,1)such that for any s, t∈[T]withs+t⩽T, any{τa}a∈[K]∈RK
+
and any policy Πdown
p that offers almost surely a transfer (˜al, τ(l)) = (˜ al, τ˜al)for any l∈ {s+
1, . . . , s +t}, the batched regret of the upstream player following Πup
psatisfies, with probability at
least1−t−ζ,
s+tX
l=s+1max
a∈A{vup(a) +1˜al(a)τ˜al} −(vup(Al) +1˜al(Al)τ˜al)⩽Ctκ.
2A similar assumption is made in the work of Donahue et al. [2024] but with a stronger instantaneous regret
bound which does not encompass the UCB’s regret bound.
6The constraint on the downstream player’s algorithm Πdown
p enforces constant incentives associated
with any arm a∈ A withtin the batch, while the incentivized actions (˜al)l∈{s+1,...,s+t}may change.
Proposition 2 in Appendix C shows that an adaptation of UCBtaking account the incentives satisfies
H2 with C = 8p
Klog(KT3),κ= 1/2andζ= 2. Note that usual bandit algorithms such as AAE,
ETCorEXP-IX also satisfy the assumption [see, e.g., Donahue et al., 2024, Lattimore and Szepesvári,
2020].
3.2 Downstream player’s procedure
We fix the policy Πup
pwhich can be any algorithm satisfying H2 for the upstream player and introduce
the algorithm BELGIC (Bandits and Externalities for a Learning Game with Incentivized Coase)
which provides a policy achieving sub-linear regret for the downstream player. It can be seen as an
online bargaining strategy to mitigate externalities. Simply put, BELGIC unfolds in two steps. First
note that for any action a∈ A, the optimal (lowest) transfer to offer to the upstream player to make
him choose ais
τ⋆
a= max
a′∈Avup(a′)−vup(a), (12)
as detailed in Appendix A. Therefore, a batched binary search procedure (Algorithm 2) first allows
the downstream player to estimate the optimal transfers τ⋆
1, . . . , τ⋆
Kwith a good precision level of
1/Tβ, where β >0. More precisely, the downstream player offers a constant incentive (˜a, τ˜a)for a
batch of time steps of length ˜T=⌈Tα⌉. The observation of T̸=
˜a, the number of steps from the batch
for which the upstream player does not pick ˜aallows her to estimate whether τ˜ais above or below τ⋆
˜a
and adjust it, following Lemma 2 in Appendix A under the condition that α, β satisfy
β/α < (1−κ). (13)
The procedure needs to be run for K⌈Tα⌉⌈log2Tβ⌉rounds since we have to make ⌈log2Tβ⌉batches
of binary search of length ⌈Tα⌉on each of the Karms [see Scheid et al., 2024]. This corresponds to
the first phase of BELGIC as described in Algorithm 2. At the end of this stage, the estimated transfers
(ˆτa)a∈Asatisfy the bound in Proposition 1. These are then used to feed the subroutine Bandit-Alg .
Proposition 1. UnderH1 andH2, after the first phase of BELGIC which consists in K⌈Tα⌉⌈logTβ⌉
steps of binary search grouped in ⌈log2Tβ⌉batches per arm a∈ A, we have that
P
for any a∈ A,ˆτa−4/Tβ−CT(κ−1)/2⩽τ⋆
a⩽ˆτa
⩾1−K⌈log2Tβ⌉/Tαζ.
The additional term 1/Tβ+ CTκ−1inˆτaensures that if H2 holds, the upstream player necessarily
plays the incentivized action ˜atat round twith high probability. Lemmas 2 and 6 from Appendix C
show how the binary search batches in BELGIC allow us to estimate τ⋆
adepending on ˜T−T̸=
a, the
number of times that arm ahas been pulled by the upstream player during the batch.
Then, any bandit subroutine Bandit-Alg , such as UCBorε-greedy , for instance, can be run in a
black-box fashion on the shifted bandit instance, where the rewards are shifted by the upper estimated
transfers (ˆτa)a∈A. The downstream player computes a shifted history ˜Hdown ,p
t such that for any
t⩽K⌈Tα⌉⌈log2Tβ⌉,˜Hdown ,p
t =∅and for any t > K⌈Tα⌉⌈log2Tβ⌉
˜Hdown ,p
t =(
{˜at, Bt, τ(t), Ut, X˜at,Bt(t)−ˆτ˜at} ∪˜Hdown ,p
t−1 if˜at=At
˜Hdown ,p
t−1 otherwise ,(14)
which serves to feed Bandit-Alg , following
Bandit-Alg : (Ut,˜Hdown ,p
t−1)7→(˜at, Bt)∈ A × A . (15)
For any family of constant incentives {τa}a∈A∈RK
+, we define RBandit-Alg (T, ν,{τa}a∈A)as the
regret for the downstream player’s subroutine Bandit-Alg on the bandit instance with shifted means
overTrounds, following
RBandit-Alg (T, ν,{τa}a∈A) =Tmax
a,b∈A2E
vdown
a,b(1)−τa
−E"TX
t=1vdown(˜at, Bt)−τ˜at#
.
Note that here, Bandit-Alg aims to maximize the shifted reward (vdown(a, b)−τa)(a,b)∈A2.
7Algorithm 1 BELGIC
1:Input: Set of actions A= [K], time horizon T, subroutine Πup
p, upstream player’s regret
constants C, κ, parameters αandβ.
2:Compute ˜Hdown ,p
s =∅for any s⩽K⌈log2Tβ⌉⌈Tα⌉.
3:fora∈ A do
4: # See Algorithm 2
5: τa,τa=Binary Search (a,⌈log2Tβ⌉,⌈Tα⌉,0,1)
6:end for
7:For any action a∈ A,ˆτa=τa+ 1/Tβ+ CT(κ−1)/2.
8:fort=K⌈Tα⌉⌈log2Tβ⌉+ 1, . . . , T do
9: Get recommended actions by Bandit-Alg on the A × A bandit instance, (˜at, Bt) =
Bandit-Alg (Ut,˜Hdown ,p
t−1).
10: Offer a transfer ˆτ˜aton action ˜at, nothing for any other action a′∈ A and play action Bt.
11: Observe At= Πup
p(˜at+1, τ(t+ 1), Vt,Hup,p
t−1), X˜at,Bt(t)
12: ifAt= ˜atthen update history ˜Hdown ,p
t .
13: end if
14: Update upstream player’s history Hup,p
t.
15:end for
Algorithm 2 Binary Search Subroutine
1:Input: action a, N T,˜T, τa,τa.
2:ford= 0, . . . , N T−1do
3: Compute τmid
a= (τa(d) +τa(d))/2,T̸=
a= 0.
4: fort=d˜T+ 1, . . . , d ˜T+˜Tdo
5: Propose transfer τmid
a(d)on arm aand nothing for any other action a′∈ A.
6: At= Πup
p(t, τmid(a), a, V t,Hup,p
t−1)
7: ifAt̸=athen :T̸=+ = 1
8: end if
9: Update upstream player’s history Hup,p
t.
10: end for
11: ifC˜Tκ+β/α< T̸=
a<˜T−C˜Tκ+β/αthen Return τa(d),τa(d).
12: else if T̸=
a⩽˜T−C˜Tκ+β/αthenτa(d) =τmid
a(d) + 1/Tβand update history ˜Hdown ,p
t .
13: elseτa(d) =τmid
a(d)−1/Tβand update history ˜Hdown ,p
t .
14: end if
15:end for
Theorem 3. Assume that H1 andH2 hold. Then BELGIC , run with α, β satisfying (13) and any
bandit subroutine Bandit-Alg , has an overall regret Rdown
p such that
Rdown
p(T,Πup
p,BELGIC )⩽2(3 + 2C + ¯ v−v) log2(T)(2T1−αζ+T(κ+1)/2+⌈Tα⌉) + 4T1−β
+RBandit-Alg (T, ν,{ˆτa}a∈A)
where, for ease of notation
¯v= max
a,b∈A×A{vdown(a, b)}and v= min
a,b∈A×A{vdown(a, b)}.
Knowledge of Candκ.An upper bound on Candκis sufficient to compute the hyperparameters in
BELGIC . Theorem 3 shows that the bigger Candκare, the worse is the downstream player’s regret,
hence the interest of knowing them more precisely.
Corollary 1. Assume that the upstream player’s distribution (γa)a∈Ais such that H1 holds. In
addition, suppose that the distributions (γa)a∈Aand(νa,b)a,b∈A×A are1-sub-Gaussian and that the
upstream player plays Πup
p=Algorithm 3(a slight modification of UCBto take into account the
incentives). Then the downstream player’s regret when she runs BELGIC with parameters α= 3/4
andβ= 1/4(which satisfy (13)) and subroutine Bandit-Alg =UCBsatisfies the following upper
8bound3
Rdown
p(T,UCB,BELGIC )⩽(10 + 4 K+ 32p
Klog2(KT3) + ¯v−v) log2(T)(3 + 2 T3/4)
+ 3K2(¯v−v).
The upper bound on the social welfare regret in Lemma 1 together with Corollary 1 shows that when
the upstream player runs Πup
p=UCBand the downstream player runs BELGIC , the social welfare
regret then satisfies Rsw(T,Πup
p,BELGIC ) =O(Klog(T)T(κ+1)/2).
In other words, if the downstream player runs BELGIC which produces a policy Πn
down, for any
upstream policy Πup
p,(Πn
down,Πup
p)is welfare efficient.
Influence of the upstream performance. It is interesting to note that in the downstream player’s
regret bound, the upstream player’s regret bound in O(Tκ)plays a significant role: the downstream
player never learns faster than the upstream player. The latter’s performance determines the social
welfare convergence rate towards the social optimum. We can observe that the players’ bounded
rationality [Selten, 1990, Jones, 1999] and personal interest make the game converge towards the
optimal social welfare equilibrium—even though they are both learning here.
Figure 1: Empirical frequencies of the upstream player’s actions when property rights are not defined
(left) and when they are defined (right).
Experiments. We conclude this section with experiments showing the empirical convergence of our
algorithm to a social optimum. In the simulation, we consider two firms, with firm 1 being upstream
and firm 2 being downstream. Their profit functions are respectively given by
π1:7→max
q1−2q1
102
+ 2q1
10,0
,
and
π2(q1, q2)7→max(
−16q1
10−6
102
+ 8
q1−6
102
+1
50q2,0)
.
Thus, firm 1’s and firm 2’s profit functions depends quadratically on q1with an firm 1optimum at
q′
1= 5and a social optimum at q⋆
1= 8. Note that in the expression of π2,q2has very little influence
as compared to q1- which allows to plot profits for only one value of q2.
We discretize the setup, consider a bandit instance (horizon T= 5.106,10arms, average over 10
rounds) and we assume that UCBis used as a subroutine. In the first setting, there are no property
rights and each firm runs UCBon their side. Second, property rights are defined and firm 2 runs
BELGIC as its policy. The plots in Figure 1 display the empirical frequencies and show empirically
the effectiveness of BELGIC to mitigate externalities.
4 Related work
Our work addresses the impact of externalities and is therefore related to taxation theory [see, e.g.,
Mirrlees et al., 2011, Salanie, 2011, and the references therein], a prominent solution for this issue, as
3Note that it is Kand not√
Khere, since the action space is of cardinality K2for the downstream player.
9exemplified by the Pigouvian tax [see Pigou, 2017]. Taxation is a fundamental aspect of all developed
economies, with 30% to50% of national income derived from taxes. The topic has been fruitful
for various scenarios, including the carbon tax [Carattini et al., 2018, Metcalf and Weisbach, 2009],
alcohol markets [Griffith et al., 2019], or business taxation [Boadway and Bruce, 1984]. Taxation can
also be studied through an operations research lens, where it is used to enhance system efficiency or
manage specific games [Roughgarden, 2010, Caragiannis et al., 2010, Bilò and Vinci, 2019]. Recent
work by Cui et al. [2024] explores online mechanisms to maximize efficiency in congestion games.
Mechanism designs [Myerson, 1989, Nisan and Ronen, 1999, Laffont and Martimort, 2009] allow to
design games that have specific desired outcomes. Deploying these mechanisms in their classical
ecnomical form assume that players’ utility functions are known a priori, which is often unrealistic.
There is a major need to blend mechanism design with machine learning.
However, our approach differs, since, drawing inspiration from Coase’s theory, we implement an
online version of his theorem [Coase, 2013, Cooter, 1982], incorporating uncertainty to tackle
the breakdown of social welfare in an online setting. We use the bandit setup [see Lattimore and
Szepesvári, 2020, Slivkins et al., 2019] as a general and convenient way to model the game introduced
by Coase. However, our work differs from considering a single agent playing a bandit game. Instead,
we focus on the more general problem of multi-players bandits, a field receiving a growing attention
from the community [see e.g., Boursier and Perchet, 2019, 2022, Sankararaman et al., 2019].
Our approach is inspired by the principal-agent model introduced by Dogan et al. [2023b], which
was further extended by Dogan et al. [2023a], Scheid et al. [2024]. However, unlike the models
proposed in the work of Dogan et al. [2023b,a], we do not specify a particular bandit algorithm for the
upstream player and instead, we allow him to use any no-regret algorithm satisfying H2 in a black-box
fashion. Conversely, the model of Scheid et al. [2024] assumes that the upstream player is always
fully informed and best-responding, whereas we assume that he is also learning. Chen et al. [2023b]
leverages a similar model to study information acquisition by a principal through an agent’s actions.
However, in their model, the agent is also almighty and knows exactly the costs associated with each
action. Designing incentives in an unknown environment is related to auction theory incorporating
uncertainty, as it is explored in the work of Feng et al. [see 2018], Li et al. [see 2023]. Similar issues
have been explored in the Reinforcement Learning framework within a leader-follower game [see
Chen et al., 2023a, with quantal responses by the follower] or in a principal-agent game with incentive
design as done by Ben-Porat et al. [2023]. Donahue et al. [2024] also study a two-players repeated
Stackelberg game on a bandit instance but instead of allowing for transfers, their main focus concerns
the achievability of a Stackelberg equilibrium through iterations of bandit policies: the same kind of
goal also appears in Collina et al. [2023]. Such principal-agent setups are of some interest to model
various real-world situations such as the design of fundings for hospitals [Wang et al., 2024] or have
been studied with multiple agents through the lens of auction design in dynamic setups [Bergemann
and Said, 2010, Chen et al., 2023c], or to account for fairness [Fallah et al., 2024].
In our game, the downstream player needs to learn the optimal transfers/incentives to offer to the
upstream player. This is related to the Incentivized Exploration literature [Mansour et al., 2016,
Simchowitz and Slivkins, 2023, Esmaeili et al., 2023], which is often cast in terms of a benevolent
planner who aims to optimize the global welfare of agents via plausible recommendations. A related
model is Bayesian Persuasion [Kamenica and Gentzkow, 2011], where a sender influences a receiver’s
action through sending a signal. This model has begun to be studied in learning settings [see, e.g.,
Castiglioni et al., 2020, Bernasconi et al., 2022, Wu et al., 2022b,a].
5 Conclusion
This paper studies a model of externalities in a two-players sequential game where both players learn
their optimal actions. We first show that when the players act independently, then a misalignment
between the players’ interests and the social welfare leads to a breakdown of the global utility. We
then introduce interactions through transfers, which restores a social welfare optimum, representing
the online version of the Coase theorem . To that purpose, we propose a policy for the downstream
player which allows her to estimate the optimal transfers as well as choosing the best actions. The
mathematical difficulty comes from the learning aspect on both sides. Since our work is coined in a
learning framework for mechanism design, several directions for research are open, as for instance
extensions to the multi-agent setting, which raises many questions.
10Acknowledgements
Funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are
however those of the author(s) only and do not necessarily reflect those of the European Union or
the European Research Council Executive Agency. Neither the European Union nor the granting
authority can be held responsible for them.
References
Jens Abildtrup, Frank Jensen, and Alex Dubgaard. Does the coase theorem hold in real markets?
an application to the negotiations between waterworks and farmers in denmark. Journal of
environmental management , 93(1):169–176, 2012.
Nivasini Ananthakrishnan, Stephen Bates, Michael Jordan, and Nika Haghtalab. Delegating data
collection in decentralized machine learning. In International Conference on Artificial Intelligence
and Statistics , pages 478–486. PMLR, 2024.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research , 3(Nov):397–422, 2002.
Omer Ben-Porat, Yishay Mansour, Michal Moshkovitz, and Boaz Taitler. Principal-agent reward
shaping in mdps. arXiv preprint arXiv:2401.00298 , 2023.
Dirk Bergemann and Maher Said. Dynamic auctions: A survey. Wiley Encyclopedia of Operations
Research and Management Science , 2010.
Martino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and Francesco Trovò.
Sequential information design: Learning to persuade in the dark. Advances in Neural Information
Processing Systems , 35:15917–15928, 2022.
Vittorio Bilò and Cosimo Vinci. Dynamic taxes for polynomial congestion games. ACM Transactions
on Economics and Computation (TEAC) , 7(3):1–36, 2019.
Robin Boadway and Neil Bruce. A general proposition on the design of a neutral business tax.
Journal of Public Economics , 24(2):231–239, 1984.
Patrick Bolton and Mathias Dewatripont. Contract theory . MIT press, 2004.
Etienne Boursier and Vianney Perchet. Sic-mmab: Synchronisation involves communication in
multiplayer multi-armed bandits. Advances in Neural Information Processing Systems , 32, 2019.
Etienne Boursier and Vianney Perchet. A survey on multi-player bandits. arXiv preprint
arXiv:2211.16275 , 2022.
Sébastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends ®in Machine Learning , 5(1):1–122, 2012.
James M Buchanan and Wm Craig Stubblebine. Externality. In Inframarginal Contributions to
Development Economics , pages 55–73. World Scientific, 2006.
Ioannis Caragiannis, Christos Kaklamanis, and Panagiotis Kanellopoulos. Taxes for linear atomic
congestion games. ACM Transactions on Algorithms (TALG) , 7(1):1–31, 2010.
Stefano Carattini, Maria Carvalho, and Sam Fankhauser. Overcoming public resistance to carbon
taxes. Wiley Interdisciplinary Reviews: Climate Change , 9(5):e531, 2018.
Matteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online bayesian persuasion.
Advances in Neural Information Processing Systems , 33:16188–16198, 2020.
Siyu Chen, Mengdi Wang, and Zhuoran Yang. Actions speak what you want: Provably sample-
efficient reinforcement learning of the quantal stackelberg equilibrium from strategic feedbacks.
arXiv preprint arXiv:2307.14085 , 2023a.
11Siyu Chen, Jibang Wu, Yifan Wu, and Zhuoran Yang. Learning to incentivize information acquisition:
Proper scoring rules meet principal-agent model. In International Conference on Machine Learning ,
pages 5194–5218. PMLR, 2023b.
Yurong Chen, Qian Wang, Zhijian Duan, Haoran Sun, Zhaohua Chen, Xiang Yan, and Xiaotie Deng.
Coordinated dynamic bidding in repeated second-price auctions with budgets. In International
Conference on Machine Learning , pages 5052–5086. PMLR, 2023c.
Ronald Coase. The problem of social cost. Journal of Law and Economics , 56(4):837 – 877, 2013.
URL https://EconPapers.repec.org/RePEc:ucp:jlawec:doi:10.1086/674872 .
Natalie Collina, Eshwar Ram Arunachaleswaran, and Michael Kearns. Efficient stackelberg strategies
for finitely repeated games. In Proceedings of the 2023 International Conference on Autonomous
Agents and Multiagent Systems , pages 643–651, 2023.
Robert Cooter. The cost of coase. The Journal of Legal Studies , 11(1):1–33, 1982.
Qiwen Cui, Maryam Fazel, and Simon S Du. Learning optimal tax design in nonatomic congestion
games. arXiv preprint arXiv:2402.07437 , 2024.
Carl J Dahlman. The problem of externality. The journal of law and economics , 22(1):141–162,
1979.
Ilgin Dogan, Zuo-Jun Max Shen, and Anil Aswani. Estimating and incentivizing imperfect-knowledge
agents with hidden rewards. arXiv preprint arXiv:2308.06717 , 2023a.
Ilgin Dogan, Zuo-Jun Max Shen, and Anil Aswani. Repeated principal-agent games with unobserved
agent rewards and perfect-knowledge agents. arXiv preprint arXiv:2304.07407 , 2023b.
Kate Donahue, Nicole Immorlica, Meena Jagadeesan, Brendan Lucier, and Aleksandrs Slivkins.
Impact of decentralized learning on player utilities in stackelberg games. arXiv preprint
arXiv:2403.00188 , 2024.
Paul Dütting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In
Proceedings of the 2019 ACM Conference on Economics and Computation , pages 369–387, 2019.
Seyed A Esmaeili, Suho Shin, and Aleksandrs Slivkins. Robust and performance incentivizing
algorithms for multi-armed bandits with strategic agents. arXiv preprint arXiv:2312.07929 , 2023.
Alireza Fallah and Michael I Jordan. Contract design with safety inspections. arXiv preprint
arXiv:2311.02537 , 2023.
Alireza Fallah, Michael I Jordan, and Annie Ulichney. Fair allocation in dynamic mechanism design.
arXiv preprint arXiv:2406.00147 , 2024.
Zhe Feng, Chara Podimata, and Vasilis Syrgkanis. Learning to bid without knowing your value. In
Proceedings of the 2018 ACM Conference on Economics and Computation , pages 505–522, 2018.
Thomas K Greenfield, Yu Ye, William Kerr, Jason Bond, Jürgen Rehm, and Norman Giesbrecht.
Externalities from alcohol consumption in the 2005 us national alcohol survey: implications for
policy. International journal of environmental research and public health , 6(12):3205–3224, 2009.
Rachel Griffith, Martin O’Connell, and Kate Smith. Tax design in the alcohol market. Journal of
public economics , 172:20–35, 2019.
Guru Guruganesh, Jon Schneider, and Joshua R Wang. Contracts under moral hazard and adverse
selection. In Proceedings of the 22nd ACM Conference on Economics and Computation , pages
563–582, 2021.
Guru Guruganesh, Yoav Kolumbus, Jon Schneider, Inbal Talgam-Cohen, Emmanouil-Vasileios
Vlatakis-Gkaragkounis, Joshua R Wang, and S Matthew Weinberg. Contracting with a learning
agent. arXiv preprint arXiv:2401.16198 , 2024.
Bryan D Jones. Bounded rationality. Annual review of political science , 2(1):297–321, 1999.
12Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review , 101(6):
2590–2615, 2011.
Jean-Jacques Laffont and David Martimort. The theory of incentives: the principal-agent model. In
The theory of incentives . Princeton university press, 2009.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. Advances in neural information processing systems , 20, 2007.
Tor Lattimore and Csaba Szepesvári. Bandit algorithms . Cambridge University Press, 2020.
Ningyuan Li, Yunxuan Ma, Yang Zhao, Zhijian Duan, Yurong Chen, Zhilin Zhang, Jian Xu, Bo Zheng,
and Xiaotie Deng. Learning-based ad auction design with externalities: the framework and a
matching-based approach. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining , pages 1291–1302, 2023.
Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. Bayesian explo-
ration: Incentivizing exploration in bayesian games. arXiv preprint arXiv:1602.07570 , 2016.
Andreu Mas-Colell, Michael D. Whinston, and Jerry R. Green. Microeconomic Theory . Oxford
University Press, New York, 1995.
Gillbert E Metcalf and David Weisbach. The design of a carbon tax. Harv. Envtl. L. Rev. , 33:499,
2009.
James Mirrlees et al. Tax by design: The Mirrlees review . OUP Oxford, 2011.
Roger B Myerson. Mechanism design . Springer, 1989.
Noam Nisan and Amir Ronen. Algorithmic mechanism design. In Proceedings of the thirty-first
annual ACM symposium on Theory of computing , pages 129–140, 1999.
Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems.
The Annals of Statistics , 44:660–681, 2016.
Arthur Pigou. The economics of welfare . Routledge, 2017.
Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American
Mathematical Society , page 527–535, 1952.
Tim Roughgarden. Algorithmic game theory. Communications of the ACM , 53(7):78–86, 2010.
Bernard Salanié. The economics of contracts: a primer . MIT press, 2005.
Bernard Salanie. The economics of taxation . MIT press, 2011.
Abishek Sankararaman, Ayalvadi Ganesh, and Sanjay Shakkottai. Social learning in multi agent multi
armed bandits. Proceedings of the ACM on Measurement and Analysis of Computing Systems , 3
(3):1–35, 2019.
Antoine Scheid, Daniil Tiapkin, Etienne Boursier, Aymeric Capitaine, El Mahdi El Mhamdi, Éric
Moulines, Michael I Jordan, and Alain Durmus. Incentivized learning in principal-agent bandit
games. ICML , 2024.
Reinhard Selten. Bounded rationality. Journal of Institutional and Theoretical Economics
(JITE)/Zeitschrift für die gesamte Staatswissenschaft , 146(4):649–658, 1990.
Virag Shah, Jose Blanchet, and Ramesh Johari. Bandit learning with positive externalities. Advances
in Neural Information Processing Systems , 31, 2018.
Max Simchowitz and Aleksandrs Slivkins. Exploration and incentives in reinforcement learning.
Operations Research , 2023.
Aleksandrs Slivkins et al. Introduction to multi-armed bandits. Foundations and Trends ®in Machine
Learning , 12(1-2):1–286, 2019.
13Heinrich V on Stackelberg. Market structure and equilibrium . Springer Science & Business Media,
2010.
Serena Wang, Stephen Bates, P Aronow, and Michael Jordan. On counterfactual metrics for social
welfare: Incentives, ranking, and information asymmetry. In International Conference on Artificial
Intelligence and Statistics , pages 1522–1530. PMLR, 2024.
Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng
Xu. Markov persuasion processes and reinforcement learning. In ACM Conference on Economics
and Computation , 2022a.
Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng
Xu. Sequential information design: Markov persuasion process and its efficient reinforcement
learning. arXiv preprint arXiv:2202.10678 , 2022b.
Banghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I Jordan. The
sample complexity of online contract design. arXiv preprint arXiv:2211.05732 , 2022.
14A Algorithmic Subroutine for the Binary Search
Optimal Transfer. For any given round t⩾1, action a∈ A andε >0, the downstream player
can incentivize any best-responding upstream player to choose aby offering a transfer, τ⋆,ε
a∈R+,
defined as:
τ⋆,ε
a= max
a′∈Avup(a′)−vup(a) +ε .
With this transfer, it holds that for any a′∈ A, a′̸=a, we have vup(a′)< vup(a) +τ⋆,ε
a, ensuring
the upstream player’s action At=a, since action ayields a superior reward. Consequently,
τ⋆
a= lim
ε→0τ⋆,ε
a= max
a′∈Avup(a′)−vup(a)
represents the infimal transfer necessary to make arm athe best upstream player’s choice.
First step of BELGIC : estimation of the optimal transfers. Suppose that we consider an arm a∈ A
and that the downstream player offers an incentive τato the upstream player if he picks this arm. We
consider this procedure with a constant incentive τafor a batch of time steps of length ˜T=⌈Tα⌉
due to the fact that the upstream player is learning [see Perchet et al., 2016, for batched bandits in
the usual multi-armed setting]. Lemma 2 shows that for BELGIC to accurately estimate τ⋆
awith high
probability, it must hold
C˜Tκ+β/α<˜T/2,which is equivalent to CTκα+β−α<1/2, (16)
This is why we impose the condition (13) onαandβ, namely β/α < 1−κ, which ensures that
κ(α−1) +β <0and therefore limT→+∞Tκ(α−1)+β= 0. More precisely, we define for a∈[K]
Λa= (a−1)⌈log2Tβ⌉˜T ,
which is the step after which starts the binary search procedure on arm a. For any d∈
{1, . . . ,⌈log2Tβ⌉}on arm a, we define
ka,d= Λa+ (d−1)˜T (17)
as the step after which starts the d-th batch iteration on arm a, and we consider
T̸=
a,d=Card{t∈ {kd,a+ 1, . . . , k d,a+˜T}such that At̸=a}, (18)
where (At)t∈{1,...,K⌈log2Tβ⌉˜T}is given by Algorithm 2. Lemma 2 shows that for any a∈ A, d∈
{1, . . . ,⌈log2Tβ⌉}, with high probability
ifT̸=
d,a<˜T−C˜Tκ+β/αandT̸=
d,a>C˜Tκ+β/α,then|τ⋆
a−ˆτd,a|⩽1/Tβ, (19)
where ˆτa,dis the current estimate of τ⋆
aoffered for iteration ka,d. In case (18) does not hold, it means
that the upstream player has misplayed and has chosen most of the steps a suboptimal action, leading
to an instantaneous regret for him larger than the bound given in H2. This is why (19) holds with
high probability. To sum up, the first phase of BELGIC consists in ⌈log2Tβ⌉batches of binary search
on each arm a∈ A to obtain a precision level 1/Tβon the optimal transfer τ⋆
a.
During this phase in Algorithm 2, we define τa(d)∈R+as the upper estimate and τa(d)∈R+as
the lower estimate of τ⋆
aafterd∈ {1, . . . ,⌈log2Tβ⌉}rounds of binary search on arm a. For any
t∈[T]anda∈ A, we define τmid
a(t) = ( τa(t) +τa(t))/2.τa(d), τa(d), τmid
a(d)are updated at
the end of the d-th binary search batch of length ˜Ton arm a. We define NT=⌈log2Tβ⌉as the
number of binary search steps per arm.
After this first binary search phase, the downstream player computes estimates of the optimal transfers
τ⋆
a
(ˆτa)a∈A= (τa(⌈log2Tβ⌉) + 1/Tβ+ CT(κ−1)/2)a∈A,
and offers these transfers (τ⋆
a)a∈Ato make the upstream player play any action ˜a∈ A she wants.
Second Step. After the first phase during which the optimal incentives are estimated by the down-
stream player through (ˆτa)a∈A, she runs in the second phase the subroutine Bandit-Alg on theA×A
bandit instance driven by her action Btand the upstream player’s one At. More precisely, any bandit
subroutine Bandit-Alg , such as UCBorε-greedy , for instance, can be run in a black-box fashion
15on the shifted bandit instance, where rewards are shifted by the upper estimated transfers (ˆτa)a∈A.
The downstream player computes a shifted history ˜Hdown ,p
t =∅for any t⩽K⌈Tα⌉⌈log2Tβ⌉and
for any t > K⌈Tα⌉⌈log2Tβ⌉
˜Hdown ,p
t =(
{˜at, Bt, τ(t), Ut, X˜at,Bt(t)−ˆτ˜at} ∪˜Hdown ,p
t−1 if˜at=At
˜Hdown ,p
t−1 otherwise .
which serves to feed Bandit-Alg , following Bandit-Alg : (Ut,˜Hdown ,p
t−1)7→(˜at, Bt)∈ A × A .
Note that here, Bandit-Alg aims to maximize the shifted reward (νdown(a, b)−ˆτa)(a,b)∈A2.
Based on this decision by Bandit-Alg ,Πdown
p offers the incentive ˆτ˜atassociated with action ˜atand
plays action Bt. Lemma 5 ensures that ˜atis the upstream player’s best choice. Therefore, H2 ensures
that the upstream player will not deviate from the downstream player’s recommendation with high
probability.
B Invariance when the property rights are given to the downstream player
Our focus in the paper was the case where the upstream player possesses the bandit instance and
receives monetary payments. We argue here that the symmetric situation, i.e. when the property
rights are given to the downstream player, can be analysed in the exact same way.
Assume that the the downstream player owns the bandit instance. This implies that (i) they can
prescribe what arm the upstream player has to play at each round, and (ii) the upstream player may
perform a monetary transfer to influence the arm they are allowed to pull.
Consider the same bandit setup as before. Formally, the downstream player’s action is (At, Bt)∈
A × A where Atis the arm that the upstream player is prescribed to pull (he cannot deviate since the
downstream player has the property rights), while Btis the arm played by the downstream player. On
the other hand, the upstream player’s policy outputs at each round the action (˜at, τ(t))∈ A × R+,
where ˜atis the arm they choose to incentivize and τ(t)is the amount of transfer. It means that the
downstream player receives a transfer τ(t)if she prescribes action At= ˜at. As a consequence, the
instantaneous utility of the upstream player is ZAt−1˜at(At)τ(t), while the downstream player
receives XAt,Bt+1˜at(At)τ(t). In that case, the upstream player may perform a binary search on
each arm ¯a∈ A to identify the optimal incentive τ⋆
¯a, by considering Card{t∈[T]such that ˜at= ¯a}
during batches designed for the binary search and then play on the shifted bandit instance as we
explained. This situation and the upstream player’s strategy are now equivalent to the one presented
before.
C Proofs and Technical Results
Recall that we defined the shifted history ˜Hdown ,p
t that will serve to feed Πdown
p at time tas˜Hdown ,p
t =
(˜as, τ(s), As, Vs, ZAs(s))s⩽t.
Theorem 4. Suppose that argmaxa∈Avup(a)is the singleton {au
⋆}and that
vup(asw) +vdown(asw, bsw)−vup(au
⋆) +vdown(au
⋆, b)>0,
for any b∈ A . In the absence of property rights and when the upstream player runs any no-
regret policy Πup
n, we have Rsw(T,Πup
n,Πdown
n)⩾T∆sw−Rup
n(T,Πup
n)∆sw/∆up, where ∆up=
mina′∈A\{ au⋆}vup(au
⋆)−vup(a′)and∆sw=vup(asw) +vdown(asw, bsw)−max b∈A(vup(au
⋆) +
vdown(au
⋆, b)). Therefore, Rsw(T,Πup
n,Πdown
n) = Ω( T)and(Πup
n,Πdown
n)is not welfare efficient.
Proof of Theorem 4. Since argmaxa∈Avup(a)is the singleton {aup
⋆}, we define ∆up=
mina′∈A\{ au⋆}vup(au
⋆)−vup(a′)as the upstream player reward gap and ∆sw=vup(asw) +
vdown(asw, bsw)−max b∈A(vup(au
⋆) +vdown(au
⋆, b))as the social welfare reward gap if the up-
stream player plays his most preferred action.
Denote Nup
⋆(T)the number of pulls of the upstream player up to time Ton the arm au
⋆. By definition
of∆up, we have that for any step t∈[T]such that At̸=aup
⋆,max a∈A{vup(a)} −vup(At) =
16vup(aup
⋆)−vup(At)⩾mina′∈A\{ au⋆}vup(au
⋆)−vup(a′) = ∆up. There are T−Nup
⋆(T)such steps,
which leads to
Rup
n(T,Πup
n,Πdown
p)⩾E"TX
t=1max
a∈A{vup(a)} −vup(At)#
⩾(T−E[Nu
⋆(T)])∆up,
and we obtain
E[Nu
⋆(T)]⩾T−Rup
n(T,Πup
n)/∆up.
Moreover, for any t∈[T]such that At=aup
⋆and any Bt∈ A,vup(asw) +vdown(asw, bsw)−
(vup(At) +vdown(At, Bt)) =vup(asw) +vdown(asw, bsw)−(vup(aup
⋆) +vdown(aup, Bt))⩾∆sw
by definition, which leads to
Rsw(T,Πup
n,Πdown
n)⩾E[Nu
⋆(T)]∆sw,
and we obtain
Rsw(T,Πup
n,Πdown
n)⩾T∆sw−Rup
n(T,Πup
n)∆sw/∆up.
Since limT→+∞Rup
n(T,Πup
n)/T= 0, we have limT→+∞Rsw(T,Πup
n,Πdown
n)/T= ∆sw>0,
hence the result.
The proof of Theorem 2 is an immediate consequence of Theorem 4.
Lemma 1. Recall that µ⋆,downis the downstream player’s optimal reward as defined as a solution
of(10). We have µ⋆,down= max a,b∈A{vdown(a, b) +vup(a)} −max a′∈A{vup(a′)}, as well as
(aopt, bopt) = (asw, bsw)andµ⋆,up+µ⋆,down=vup(asw)+vdown(asw, bsw) = max a,b∈A{vup(a)+
vdown(a, b)}, where µ⋆,upis defined in Equation (1). Moreover, for any integer T∈N⋆, and policies
Πup
p,Πdown
p , we have that
Rsw(T,Πup
p,Πdown
p)⩽Rup
p(T,Πup
p,Πdown
p) +Rdown
p(T,Πup
p,Πdown
p).
Proof of Lemma 1. Recall that µ⋆,downis defined as µ⋆,down= supa,b∈A2,τ∈R+{vdown(a, b)−
τ},such that a∈argmaxa′∈A{vup(a′) +1a(a′)τ}andaup
⋆= argmaxa′∈Avup(a). Note that we
can write
µ⋆,down= max {supa,b∈A2,τ∈R+1˜A(a, τ)(vdown(a, b)−τ),max b∈Avdown(aup
⋆, b)},
where ˜A={(a, τ):vup(a) +τ⩾max a′vup(a′) +1a(a′)τ}which is the set of pairs (a, τ)∈
A ×R+such that the constraint binds. However, we also have by definition that vup(aup
⋆) + 0⩾
max a′∈Avup(a′) +1aup
⋆(a′)·0and hence, (aup
⋆,0)∈˜A. Therefore, since vdown(aup
⋆, b)⩾0for
anyb∈ A, we can write
µ⋆,down= sup
(a,τ)∈˜A,b∈A{vdown(a, b)−τ}.
First note that if (a, τ)∈˜A, then for any a′∈ A, τ∈R+,vup(a) +τ⩾vup(a′) +1a(a′)τ, which
gives
vup(a)−vup(a′)⩾(1a(a′)−1)τ .
However, either a=a′and hence vup(a)−vup(a′) = 0 , either a̸=a′and hence 1a(a′) = 0 .
Therefore, we have that
vup(a)−vup(a′)⩾−τ ,
which implies by definition of the optimal incentives that τ⩾vup(a′)−vup(a)for any a′∈ A, and
hence τ⋆
a⩽τfor any (a, τ)∈˜A.
In addition, (a, τ⋆
a)∈˜Aby definition. Consequently,
µ⋆,down= max
a,b∈A2{vdown(a, b)−τ⋆
a}= max
a,b∈A{vdown(a, b)−max
a′∈A{vup(a′)}+vup(a)},
17hence the first part of the result. Since µ⋆,upis defined as µ⋆,up= max a∈Avup(a), we have that
µ⋆,down+µ⋆,up= max
a,b∈A{vdown(a, b)−max
a′∈A{vup(a′)}+vup(a)}+ max
a∈Avup(a)
= max
a,b∈A{vdown(a, b) +vup(a)}
=vup(asw) +vdown(asw, bsw).
Now summing Rdown
p andRup
pas defined in (9) and (11), we obtain
Rup
p(T,Πup
p,Πdown
p) +Rdown
p(T,Πup
p,Πdown
p)
=E"TX
t=1max
a∈A{vup(a) +1˜at(a)τ(t)} −(vup(At) +1˜at(At)τ(t))#
+Tmax
a,b∈A2{vdown(a, b)−max
a′∈A{vup(a′)}+vup(a)} −E"TX
t=1vdown(At, Bt)−1˜at(At)τ(t)#
⩾Tmax
a∈A{vup(a)} −E"TX
t=1vup(At) +1˜at(At)τ(t)#
−E"TX
t=1vdown(At, Bt)−1˜at(At)τ(t)#
+Tmax
a,b∈A2{vdown(a, b) +vup(a)} −Tmax
a′∈A{vup(a′)}
=Tmax
a,b∈A2{vup(a) +vdown(a, b)} −E"TX
t=1vup(At) +vdown(At, Bt)#
=Rsw(T,Πup
p,Πdown
p),
hence the result.
For a downstream player’s policy Πdown
p, we define ˜Rup
pas the upstream player’s regret without
expectation, following
˜Rup
p({s+ 1, . . . , s +t},Πup
p,Πdown
p) =s+tX
l=s+1max
a∈A{vup(a) +1˜al(a)τ(l)} −(vup(Al) +1˜al(Al)τ(l)),
where (˜al, τ(l))l∈[T]are the incentives output by Πdown
p and(Al)l∈[T]are the output of Πup
p. Recall
the assumption that we use on the upstream player’s regret for a policy Πup
p. We show here that it is
satisfied by typical no-regret bandit algorithms.
H2.There exist C, ζ > 0, κ∈[0,1)such that for any s, t∈[T]withs+t⩽T, any{τa}a∈[K]∈RK
+
and any policy Πdown
p that offers almost surely a transfer (˜al, τ(l)) = (˜ al, τ˜al)for any l∈ {s+
1, . . . , s +t}, the batched regret of the upstream player following Πup
psatisfies, with probability at
least1−t−ζ,
s+tX
l=s+1max
a∈A{vup(a) +1˜al(a)τ˜al} −(vup(Al) +1˜al(Al)τ˜al)⩽Ctκ.
We present the upstream player’s UCBsubroutine.
Proposition 2. Lets, t∈[T]such that s+t⩽T. Suppose that there exists a family (τa)a∈A
of constant incentives associated with each arm a∈ A such that we have in Algorithm 3:
(˜al, τ(l))s+l∈{s+1,...,s+t}= (˜al, τ˜al)l∈{s+1,...,s+t}as an output of Πdown
p. Suppose that the dis-
tributions γaare1-sub-Gaussian. Then with probability at least 1−T−2, the regret of the version of
UCBgiven in Algorithm 3 run by the upstream player satisfies
˜Rup
p({s+ 1, . . . , s +t},UCB,Πdown
p)⩽8p
log(KT3)√
tK .
Note that the major difference between this assumption and the regret bounds that we generally
consider in multi-armed bandit problems is that we consider the regret without expectation here.
18Algorithm 3 Upstream player’s UCB
1:Input: Set of arms K, horizon T.
2:Initialize: For any arm a∈[K], setˆµa= 0, Ta= 0.
3:for1⩽t⩽K:do
4: Pull arm At=t
5: Update ˆµAt=XAt(t), TAt(t) = 1
6:end for
7:fort⩾K+ 1do
8: Observe the incentive (˜at, τ(t)).
9: Pull arm At∈argmaxa∈[K]n
ˆµa(t−1) + 2q
log(KT3)
Ta(t−1)+1˜at(a)τ(t)o
10: Update TAt(t) =TAt(t−1) + 1 ,ˆµAt(t) =1
TAt(t)(TAt(t−1)ˆµAt(t−1) +XAt(t))
11:end for
Proof of Proposition 2. The proof is adapted from the proof of Bubeck et al. [2012, Theorem 2.1].
Lets, t∈[T]such that s+t⩽T. For any integer l∈ {s+ 1, . . . , s +t}, we write nl(a) =
Card{l′∈[l]such that Al′=a}for the number of pulls of arm aandˆµa(l)for the empirical mean
utility of the arm a∈ A estimated on the batch {1, . . . , l}:ˆµa(l) =nl(a)−1Pl
k=11a(Ak)Xa(k).
Since the rewards on the incentivized bandit instance are 1-sub-Gaussian, a Hoeffding bound gives
that for any δ∈(0,1),a∈ A,l∈ {s+ 1, . . . , s +t}, and any family of arms (al′)l′∈{1,...,l}such
that Card {j:aj=a}=k, we have that
P
|ˆµa(l)−vup(a)|⩾2p
log(2/δ)/k(Al′)l′∈{1,...,s+l}= (al′)l′∈{1,...,s+l}
⩽δ .
Therefore, for any δ∈(0,1),a∈ A,l∈ {s+ 1, . . . , s +t}, we have the following bound
P
|ˆµa(l)−vup(a)|⩾2p
log(2/δ)/nl(a)
=P
l[
k=1[
(al′)l′s.t.
Card{l′∈{1,...,l}:al′=a}=k{|ˆµa(l)−vup(a)|⩾2p
log(2/δ)/k}

⩽lX
k=1P
[
(al′)l′s.t.
Card{l′∈{1,...,l}:al′=a}=k{|ˆµa(s+l)−vup(a)|⩾2p
log(2/δ)/k}

⩽lX
k=1X
(al′)l′s.t.
Card{l′∈{1,...,l}:al′=a}=kP
|ˆµa(s+l)−vup(a)|⩾2p
log(2/δ)/kAl=al
P(Al=al)
⩽Tδ ,
and with an union bound, we obtain that
P
∃l∈[t], a∈ A such that |ˆµa(l)−vup(a)|⩾2p
log(2/δ)/nl(a)
⩽T2Kδ ,
Considering the probability of the opposite event, we have that
P
for any l∈[t], a∈ A,|ˆµa(l)−vup(a)|⩽2p
log(2T2K/δ)/nl(a)
⩾1−δ , (20)
where we rescaled δasδ/T2K.
For the remaining of the proof, we take δ=T−2and define a⋆
l= argmaxa∈A{vup(a) +1˜al(a)τ}.
We now assume that the event {for any l∈ {s+ 1, . . . , s +t}, a∈ A,|ˆµa(l)−vup(a)|⩽
2p
log(2T2K/δ)/nl(a)}holds. If at some step l∈ {s+ 1, . . . , s +t}, action Alis chosen in
Algorithm 3, it means that
ˆµAl(l) + 2p
log(tK/δ )/nl(Al) +1˜al(Al)τ˜al⩾ˆµa⋆
l(l) + 2q
log(tK/δ )/nl(a⋆
l) +1˜al(a⋆
l)τ˜al,
19with regards to the choice of actions in UCBbased on the upper confidence bound. We now decompose
the whole regret on the batch {s+ 1, . . . , s +t}defined as ˜Rup
p({s+ 1, . . . , s +t},UCB,Πdown
p).
(20) ensures that with probability at least 1−1/T2
˜Rup
p({s+ 1, . . . , s +t},UCB,Πdown
p) =s+tX
l=s+1vup(a⋆
l) +1˜al(a⋆
l)τ˜al−(vup(Al) +1˜al(Al)τ˜al)
⩽s+tX
l=s+1ˆµa⋆
l(l) + 2q
log(Kt/δ )/nl(a⋆
l)−vup(Al) +1˜al(a⋆
l)τ˜al−1˜al(Al)τ˜al
⩽s+tX
l=s+1ˆµAl(l) + 2p
log(Kt/δ )/nl(Al)−vup(Al)
⩽s+tX
l=s+1ˆµAl(l) + 2p
log(Kt/δ )/nl(Al)−(ˆµAl(l)−2p
log(Kt/δ )/nl(Al))
⩽4p
log(Kt/δ )s+tX
l=s+1p
1/nl(Al),
and we have
s+tX
l=s+1p
1/nl(Al)⩽KX
i=1s+tX
l=s+1p
1Al(i)/nl(i)
⩽KX
i=1ns+t(i)X
j=ns+1(i)1/p
j⩽2KX
i=1p
ns+t(i)−ns(i), (21)
where the last step holds because for any integers s, t∈N⋆, we have that
s+tX
l=s+11√
l=s+tX
l=s+11√
lZl
x=l−1dx⩽s+tX
l=s+1Zl
x=l−1dx√x=Zs+t
x=sdx√x= 2(√
s+t−√s).
Using Cauchy-Schwarz inequality we obtain from (21)
1/Ks+tX
l=s+1p
1/nl(Al)⩽2vuut1/KKX
i=1ns+t(i)−ns(i) = 2p
t/K ,
which givesPs+t
l=s+1p
1/nl(Al)⩽2√
tK.
Finally plugging all the terms together, since δ= 1/T2, we obtain that with probability at least
1−1/T2
˜Rup
p({s+ 1, . . . , s +t},UCB)⩽8p
log(KT3)√
tK .
Lemma 2. Assume H2 holds and consider some arm a∈ A such that we run the d-th batch
of binary search on awithd∈ {1, . . . ,⌈log2Tβ⌉}: for any t∈ {ka,d+ 1, . . . , k a,d+˜T}, we
have (˜at, τ(t)) = ( a, τa)withτa=τmid
a(d)and˜T=⌈Tα⌉. Recall that we defined in (18):
T̸=
a,d=Card{t∈ {ka,d+ 1, . . . , k a,d+˜T}such that At̸=a}. Let β∈(0,1)be such that
β < α (1−κ). Given that the event {˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩽C˜Tκ}holds,
we have that
•IfT̸=
a,d<˜T−C˜Tκ+β/α, then τ⋆
a< τa+ 1/Tβ.
•IfT̸=
a,d>C˜Tκ+β/α, then τ⋆
a> τa−1/Tβ.
Consequently, with probability at least 1−2T−αζ, ifC˜Tκ+β/α< T̸=
a,d<˜T−C˜Tκ+β/α, then
|τ⋆
a−τa|⩽1/Tβ.
20Proof of Lemma 2. The whole proof is done conditionally on the event
{˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩽C˜Tκ}.
Note that it holds with probability at least 1−˜T−ζ⩾1−T−αζsince we suppose that Πup
psatisfies
H2.
Suppose that we have τa⩾τ⋆
a+ 1/Tβ. By definition of the optimal incentives, we obtain, using by
assumption τa⩾τ⋆
a+ 1/Tβ
1a(a)τa+vup(a)⩾τ⋆
a+vup(a) + 1/Tβ
= max
a′∈Avup(a′)−vup(a) +vup(a) + 1/Tβ
= max
a′vup(a′) + 1/Tβ,
which ensures that ais the optimal arm for the upstream player during the batch {ka,d+ 1, . . . , k a,d+
˜T}that we consider. In that case, since ais the best arm, by definition of the upstream player’s
utility, the reward gap E[vup(a) +τa−(vup(At) +1a(At)τa)]for the upstream player at any step
t∈ {ka,d+ 1, . . . , k a,d+˜T}is at least vup(a) +τa−max a′∈Avup(a′), and we obtain
C˜Tκ⩾˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩾T̸=
a,d(τa+vup(a)−max
a′∈A{vup(a′)}),
byH2,T̸=
a,dbeing the number of steps for which a suboptimal arm has been chosen. Therefore, by
definition of the optimal incentives, we obtain that
C˜Tκ⩾T̸=
a,d(τa−τ⋆
a)⩾T̸=
a,d/Tβ⩾T̸=
a,d˜T−β/α,
which gives: T̸=
a,d⩽C˜Tκ+β/α. Therefore, if we take the contrapositive, we obtain that during the
sequence {ka,d+ 1, . . . , k a,d+˜T}, ifT̸=
a,d>C˜Tκ+β/α, then with probability at least 1−T−αζ,
τa< τ⋆
a+ 1/Tβ, or equivalently τ⋆
a> τa−1/Tβ.
Now suppose that τa⩽τ⋆
a−1/Tβ. By definition of the optimal incentives, we obtain
1a(a)τa+vup(a)⩽τ⋆
a+vup(a)
⩽max
a′∈Avup(a′)−vup(a) +vup(a)−1/Tβ
= max
a′vup(a′)−1/Tβ,
which ensures that ais a suboptimal arm for the upstream player during this batch of time steps.
Therefore, arm awhich has a reward gap bigger than 1/Tβsince max a′∈A{vup(a) +1a(a′)} −
(vup(a) +τa)⩾1/Tβand arm ahas been picked ˜T−T̸=
a,dtimes. Consequently, we have that
C˜Tκ⩾˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)
⩾(˜T−T̸=
a,d)(max
a′∈Avup(a′)−(τa+vup(a))
| {z }
⩾1/Tβ)
⩾(˜T−T̸=
a,d)˜T−β/α,
which gives T̸=
a,d⩾˜T−C˜Tκ+/β/α. Therefore, if we take the contrapositive, we obtain that if
T̸=
a,d<˜T−C˜Tκ+/β/α, then with probability at least 1−T−αζ,τa> τ⋆
a−1/Tβ, or equivalently
τ⋆
a< τa+ 1/Tβ.
For the second part of the proof, suppose that: C˜Tκ+β/α< T̸=
a,d<˜Tκ+β/α−C˜Tκ+β/α.
From the above result, we have that τ⋆
a< τa+ 1/Tβandτ⋆
a> τa−1/Tβwith probability at least
1−2T−αζ. Plugging these inequalities in the absolute value |τ⋆
a−τa|concludes the proof.
Lemma 3. Assume that we run Algorithm 2 and consider some binary search batch iteration
d∈ ⌈logTβ⌉run on arm a∈ A. Then 0⩽τa(d)⩽τmid
a(d)⩽τa(d)⩽1.
21Proof of Lemma 3. The proof proceeds by induction. Considering some action a∈ A, we have for
the initialisation before any binary search is run: τa(0) = 0 ,τa(0) = 1 and therefore τmid
a(0)∈
[τa(0),τa(0)]. We now consider that a number dof binary search batches has been run on a. Suppose
that we run an additional binary search batch on action a. We have
τmid
a(d+ 1) =τa(d) +τa(d)
2which gives τmid
a(d+ 1)∈[τa(d),τa(d)]. (22)
After this iteration of binary search, we either have τa(d+1) = τmid
a(d+1)+1 /Tβandτa(d+1) =
τa(d)orτa(d+ 1) = τmid
a(d)andτa(d+ 1) = τmid
a(d+ 1)−1/Tβ. Therefore, we still have
0⩽τa(d+ 1)⩽τmid
a(d+ 1)⩽τa(d+ 1)⩽1, hence the result for any d∈ ⌈log2Tβ⌉by
induction.
Lemma 4. Consider some arm a∈ A and suppose that we have run D∈N⋆batches of binary
search of length ˜Tona. Then, we have that
P
\
d∈[D]{˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩽C˜Tκ}
⩾1−D/Tαζ.
Proof of Lemma 4. First observe that for any batch d∈ {1, . . . , D }of binary search run on arm a
during steps {ka,d+ 1, . . . , k a,d+˜T}, we have by H2 that
P
˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩾C˜Tκ
⩽1/˜Tζ,
and applying a union bound over the Dbatches, we have that
P
[
d∈[D]{˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩾C˜Tκ}

⩽DX
j=1P
{˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩾C˜Tκ}
⩽D/˜Tζ,
which gives that
P
\
d∈[D]{˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩽C˜Tκ}
⩾1−D/Tαζ,
hence the result.
Lemma 5. Suppose that the upstream player runs a subroutine Πup
psatisfying H2. Consider some ac-
tiona∈ A and the D-th binary search batch of length ˜Trun on arm awithD∈ {1, . . . ,⌈log2Tβ⌉}.
Then\
d∈[D]{˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩽C˜Tκ} ⊆ { τ⋆
a∈[τa(D),τa(D)]},
and the probability of these events is at least 1− ⌈log2Tβ⌉/Tαζ. We also have that
|τa(D)−τa(D)|⩽1/2D+ 2/Tβholds almost surely .
Proof of Lemma 5. Suppose that the conditions of the lemma hold and consider some arm a.
We show by induction on the number of binary search batches Dthat have been run on athatT
d∈[D]{˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩽C˜Tκ} ⊆ { τ⋆
a∈[τa(D),τa(D)]}. If it is
true, Lemma 4 completes this first part of the proof.
The initialisation holds since τa(0) = 0 ,τa(0) = 1 andτ⋆
a= max a′∈Avup(a′)−vup(a)∈[0,1]
with probability 1- since max a′∈Avup(a′)∈[0,1]andvup(a)∈[0,1].
22We suppose that the property is true for some integer D <⌈log2Tβ⌉and that we have run one more
binary search on arm a. We have
τmid
a(D+ 1) =τa(D) +τa(D)
2,
τmid
a(D+ 1) being the incentive offered to the upstream player if he chooses action aduring
theD-th batch {ka,D+ 1, . . . , k a,D+˜T}. After this batch, if T̸=
a,D<˜T−C˜Tκ+β/α,BELGIC
updates τa(D+ 1) = τmid
a(D+ 1) + 1 /Tβ,τa(D+ 1) = τa(D)and Lemma 2 ensures that
τa(D+ 1)< τ⋆
a<τa(D+ 1) given{˜Rup
p({ka,D+1+ 1, . . . , k a,D+1+˜T},Πup
p,Πdown
p)⩽C˜Tκ}.
Thus the induction holds.
Otherwise, if T̸=
a,D>C˜Tκ+β/α,BELGIC updates τa(D+ 1) = τmid
a(D+ 1)−1/Tβ,τa(D+
1) = τa(D)and Lemma 2 ensures that τa(D+ 1) < τ⋆
a<τa(D+ 1) given{˜Rup
p({ka,D+1+
1, . . . , k a,D+1+˜T},Πup
p,Πdown
p)⩽C˜Tκ}. The induction still holds.
Consequently, we have that for any number Dof binary search batches run on arm a,τ⋆
a∈
[τa(D),τa(D)]with probability 1−D/Tαζ
For the second part of the proof, we define u(D) =τa(D)−τa(D)⩾0as the length of the interval
containing τ⋆
awith probability at least 1−D/Tαζ. We have u(0) = 1 . Suppose that after Diterations
of binary search batches, the next batch of binary search {ka,D+1+ 1, . . . , k a,D+1+˜T}outputs
T̸=
a,D+1<˜T−C˜Tκ+β/α. Then, the update of Algorithm 2 gives
uD+1=τa(D+ 1)−τa(D+ 1)
=τmid
a(D+ 1) + 1 /Tβ−τa(D)
=τa(D) +τa(D)
2−τa(D) + 1/Tβ
=τa(D)−τa(D)
2+ 1/Tβ
=uD/2 + 1 /Tβ.
On the other hand, if T̸=
a,D+1>C˜Tκ+β/α, the update gives
uD+1=τa(D+ 1)−τa(D+ 1)
=τa(Da
t)−(τmid
a(D+ 1)−1/Tβ)
=τa(D)−τa(D) +τa(D)
2+ 1/Tβ
=τa(D)−τa(D)
2+ 1/Tβ
=uD/2 + 1 /Tβ.
We can see that (uD)D⩾0is an arithmetico-geometric sequence defined by uD+1=uD/2 + 1 /Tβ
with an initial term u0= 1. Writing r= 1/Tβ/(1−1/2) = 2 /Tβ, we obtain that
|τa(D)−τa(D)|=uD= 1/2D(1−r) +r= 1/2D(1−2/Tβ) + 2/Tβ⩽1/2D+ 2/Tβ,
for any D∈ {1, . . . ,⌈log2Tβ⌉}, hence the result.
Lemma 6. Suppose that the upstream player runs a policy Πup
psatisfying H2. Considering some
action a∈ A , we have that after the binary search batch D=⌈log2Tβ⌉:P(τa(D)⩽τ⋆
a⩽
τa(D)⩽τa+ 3/Tβ)⩾1−D/Tαζ.
Proof of Lemma 6. We suppose that the eventT
d∈[⌈log2Tβ⌉]{˜Rup
p({ka,d+ 1, . . . , k a,d+
˜T},Πup
p,Πdown
p)⩽C˜Tκ}holds. Lemma 4 ensures that this event holds with probability at least
1− ⌈log2Tβ⌉/Tαζ.
23After D=⌈βlog2T⌉batches of binary search on arm a, we have by Lemma 5 that
|τa(D)−τa(D)|⩽1/2D+ 2/Tβ⩽1/2βlog2T+ 2/Tβ= 3/Tβ.
Lemma 5 guarantees that τ⋆
a∈[τa(D);τa(D)]with probability at least 1−D/Tαζ, and we obtain
τa(D)⩽τ⋆
a⩽τa(D)⩽τa(D) + 3/Tβwith the same probability.
Proposition 1. UnderH1 andH2, after the first phase of BELGIC which consists in K⌈Tα⌉⌈logTβ⌉
steps of binary search grouped in ⌈log2Tβ⌉batches per arm a∈ A, we have that
P
for any a∈ A,ˆτa−4/Tβ−CT(κ−1)/2⩽τ⋆
a⩽ˆτa
⩾1−K⌈log2Tβ⌉/Tαζ.
Proof of Proposition 1. We consider a number of binary search batches D=⌈logTβ⌉and we define
the event GasG=
for any a∈ A, τa(D)⩽τ⋆
a⩽τa(D)⩽τa(D) + 3/Tβ	
. We have that
P(G) =P \
a∈A
τa(D)⩽τ⋆
a⩽τa(D)⩽τa+ 3/Tβ	!
= 1−P [
a∈A
τa(D)⩽τ⋆
a⩽τa(t)⩽τa(D) + 3/Tβ	c!
⩾1−X
a∈AP
τa(D)⩽τ⋆
a⩽τa(D)⩽τa(D) + 3/Tβ	c
,
where the last inequality holds with an union bound. Lemma 6 with D=⌈log2Tβ⌉ensures that we
have
P
τa(D)⩽τ⋆
a⩽τa(D)⩽τa(D) + 3/Tβ	c
⩽D/Tαζ,
and since Card {A}=K, we obtain
P(G)⩾1−K⌈log2Tβ⌉/Tαζ.
Since the estimated incentives are defined as ˆτa=τa(⌈log2Tβ⌉) + 1 /Tβ+ CT(κ−1)/2, we can
conclude
P(for any a∈ A,ˆτa−4/Tβ−CT(κ−1)/2⩽τ⋆
a⩽ˆτa)⩾1−K⌈log2Tβ⌉/Tαζ,
since whenever τa(⌈log2Tβ⌉)⩽τ⋆
a⩽τa(⌈log2Tβ⌉)⩽τa(⌈log2Tβ⌉) + 1/Tβ, we also have by
definition: ˆτa(⌈log2Tβ⌉)−4/Tβ−CT(κ−1)/2⩽τa(⌈log2Tβ⌉)⩽τ⋆
a⩽ˆτa(⌈log2Tβ⌉).
Theorem 3. Assume that H1 andH2 hold. Then BELGIC , run with α, β satisfying (13) and any
bandit subroutine Bandit-Alg , has an overall regret Rdown
p such that
Rdown
p(T,Πup
p,BELGIC )⩽2(3 + 2C + ¯ v−v) log2(T)(2T1−αζ+T(κ+1)/2+⌈Tα⌉) + 4T1−β
+RBandit-Alg (T, ν,{ˆτa}a∈A)
where, for ease of notation
¯v= max
a,b∈A×A{vdown(a, b)}and v= min
a,b∈A×A{vdown(a, b)}.
Proof of Theorem 3. Suppose that the conditions of Theorem 3 are satisfied. By definition,
ΛK+1+ 1∈[T]is the step at which starts the run of the subroutine Bandit-Alg , since
ΛK+1=K⌈Tα⌉⌈βlogT⌉. All the binary searche batches have length ˜T=⌈Tα⌉. For any
a∈ A, d∈ {1, . . . ,⌈log2Tβ⌉}, we define the event
Ba,d=n
˜Rup
p({ka,d+ 1, . . . , k a,d+˜T},Πup
p,Πdown
p)⩽C˜Tκo
,
as well as
E=\
a∈[K]
d∈[⌈βlogT⌉]Ba,d\n
˜Rup
p({ΛK+1+ 1, . . . , T },Πup
p)⩽C(T−ΛK+1)κo
,
24and by H2 and Lemma 4, with an union bound, we have that
P(E) = 1−P
[
a∈[K]
d=∈[⌈logTβ⌉]Bc
a,d[n
˜Rup
p({ΛK+1+ 1, . . . , T },Πup
p,Πdown
p)⩽C(T−ΛK+1)κo

⩾1−X
a∈[K]
d∈[⌈logTβ⌉]P(Bc
a,d) +P(˜Rup
p({ΛK+1+ 1, . . . , T },Πup
p)⩽C(T−ΛK+1)κ)
⩾1−K⌈βlogT⌉˜T−ζ−(T−ΛK+1)−ζ
⩾1−K⌈βlogT⌉T−αζ−T−ζ,
and we now decompose
Rdown
p(T,Πup
p,Πdown
p) =Eh
1(E)˜Rdown
p(T,Πup
p,Πdown
p)i
+h
1(Ec)˜Rdown
p(T,Πup
p,Πdown
p)i
,
(23)
where ˜Rdown
p is defined as
˜Rdown
p(T,Πup
p,Πdown
p) =Tµ⋆,down−TX
t=1(vdown(At, Bt)−1˜at(At)τ(t)).
By definition, µ⋆,down⩾mina,b∈A×A vdown(a, b)⩾vand since Lemma 1 allows to write µ⋆,down=
max a,b∈A×A{vdown(a, b) +vup(a)} −max a′∈A{vup(a′)}, we have that
v⩽µ⋆,down⩽1 + ¯v ,
and note that since κ <1, for any a∈ A,ˆτa⩽2 + C T(κ−1)/2⩽2 + C . Consequently, Tv⩽
˜Rdown
p(T,Πup
p,Πdown
p)⩽(3 + C + ¯ v−v)Talmost surely. Therefore
Eh
1(Ec)˜Rdown
p(T,Πup
p,Πdown
p)i
⩽(3 + C + ¯ v−v)(K⌈logTβ⌉T1−αζ+T1−ζ).(24)
We consider the second term in (23). We decompose it between the steps of binary search
{1, . . . , K ⌈Tα⌉⌈log2Tβ⌉}during which we run the Binary Search Subroutine and the fol-
lowing ones when we run Bandit-Alg , which gives
Eh
1(E)˜Rdown
p(T,Πup
p,Πdown
p)i
(25)
⩽E
1(E)K⌈Tα⌉⌈log2Tβ⌉X
t=1µ⋆,down−(vdown(At, Bt)−1˜at(At)τ(t))
| {z }
(A)

+E
1(E)TX
t=K⌈Tα⌉⌈log2Tβ⌉+1µ⋆,down−(vdown(At, Bt)−1˜at(At)τ(t))
| {z }
(B)
.
Similarly to (24), we use the bound on µ⋆,downto bound (A), which gives
E[1(E)(A)]⩽E
1(E)ΛK+1X
t=11 + ¯v−(v−2−C)
⩽K⌈Tα⌉(1 + log2T)(3 + C + ¯ v−v).
(26)
25After the binary search, at each step t∈ {ΛK+1+ 1, . . . , T },Bandit-Alg recommends (˜at, Bt)∈
A × A following (15) andBELGIC offers an incentive (˜at,ˆτ˜at)withˆτa=τa(⌈log2Tβ⌉) + 1/Tβ+
CT(κ−1)/2.
Lemma 5 ensures that E ⊆ { τ⋆
a∈[τa(⌈log2Tβ⌉),τa(⌈log2Tβ⌉)]for any a∈ A}T{˜Rup
p({ΛK+1+
1, . . . , T },Πup
p)⩽C(T−ΛK+1)κ}. Therefore, if Eholds, for any a∈ A, we have that
vup(a) + ˆτa=vup(a) +τa+ 1/Tβ+ CT(κ−1)/2
> vup(a) +τ⋆
a+ CT(κ−1)/2
=vup(a) + max
a′′∈Avup(a′′)−vup(a) + C T(κ−1)/2,
and therefore, for any a′∈ A such that a′̸=a, we have that
vup(a) + ˆτa> vup(a′) + C T(κ−1)/2. (27)
This shows that at any steps t∈ {ΛK+1+ 1, . . . , T }after the binary search, we have on the event E
that
˜at= argmaxa′∈A{vup(a′) +1˜at(a′)ˆτ˜at}, (28)
and the reward gap at step tfor any a̸= ˜atis defined as
max
a′∈A{vup(a′) +1˜at(a′)ˆτ˜at} −(vup(a) +1˜at(a)ˆτ˜at) =vup(˜at) + ˆτa−vup(a). (29)
Following (27), the reward gap from (29) satisfies
max
a′∈A{vup(a′) +1˜at(a′)ˆτ˜at} −(vup(a) +1˜at(a)ˆτ˜at)⩾CT(κ−1)/2.
We now define two sets
IT={t∈ {K⌈Tα⌉⌈log2T⌉+ 1, . . . , T }such that ˜at=At},
JT={t∈ {K⌈Tα⌉⌈log2T⌉+ 1, . . . , T }such that ˜at̸=At},
which satisfy IT∪JT={K⌈Tα⌉⌈log2T⌉+ 1, . . . , T }almost surely. As shown in (28),IT
corresponds to all the steps during which the upstream player picked the best arm and for any t∈IT
vup(At) +1˜at(At)τ(t)⩾max
a∈A{vup(a) +1˜at(a)τ(t)},
while by (29), for any t∈JT, we have that
max
a∈A{vup(a) +1˜al(a)ˆτ˜al} −(vup(Al) +1˜al(Al)ˆτ˜al)⩾CT(κ−1)/2. (30)
H2 ensures that if Eholds, then Rup
p({ΛK+1+ 1, . . . , T },Πup
p,BELGIC )⩽CTκ, and this condition
together with (30) gives that
Card{JT}CT(κ−1)/2⩽Rup
p({ΛK+1+ 1, . . . , T },Πup
p,BELGIC )⩽CTκ,
26and consequently Card {JT}⩽T(κ+1)/2. We now bound (B)as follows
E[1(E)(B)] =E
1(E)TX
t=ΛK+1+1µ⋆,down− 
vdown(At, Bt)−ˆτ˜at

=E"
1(E)X
t∈ITmax
a,b∈A×A{vdown(a, b)−τ⋆
a} − 
vdown(˜at, Bt)−ˆτ˜at#
+E
1(E)X
t∈JTµ⋆,down− 
vdown(At, Bt)−ˆτ˜at
| {z }
⩽3+C+¯ v−v

⩽E"
1(E)X
t∈ITmax
a,b∈A×A
vdown(a, b)−ˆτa−(vdown(˜at, Bt)−ˆτ˜at)	
+ max
a′∈A{ˆτa′−τ⋆
a′}#
+ (3 + C + ¯ v−v)E[1(E)Card{JT}]
=E"
1(E)X
t∈ITmax
a,b∈A×A
vdown(a, b)−ˆτa	
−(vdown(˜at, Bt)−ˆτ˜at)#
+E"
1(E)X
t∈ITmax
a′∈A{ˆτa′−τ⋆
a′}#
+ (3 + C + ¯ v−v)T(κ+1)/2
⩽RBandit-Alg (Card{IT}, ν,{ˆτa}a∈A) +E
1(E)Card{IT}max
a′∈A{ˆτa′−τ⋆
a′}
+ (3 + C + ¯ v−v)T(κ+1)/2,
where the first step holds by Lemma 1. Using Lemma 5, as well as the definition of E, we obtain
E
1(E)Card{IT}max
a′∈A{ˆτa′−τ⋆
a′}
⩽Eh
1(E)Card{E}(4/Tβ+ CT(κ−1)/2)i
⩽E[1(E)T](4/Tβ+ CT(κ−1)/2)
⩽4T1−β+ CT(κ+1)/2,
which finally gives
E[1(E)(B)]⩽RBandit-Alg (T, ν,{ˆτa}a∈A) + 4T1−β+ (3 + 2C + ¯ v−v)T(1+κ)/2, (31)
and plugging together (26) and (31) in the decomposition (25) gives the following bound
Eh
1(E)˜Rdown
p(T,Πup
p,BELGIC ))i
⩽RBandit-Alg (T, ν,{ˆτa}a∈A) + 4T1−β+ (3 + 2C + ¯ v−v)T(1+κ)/2
+ (3 + C + ¯ v−v)K⌈Tα⌉(1 + log2T),
and summing the bounds on the events EandEcfinally gives
Rdown
p(T,Πup
p,BELGIC )⩽RBandit-Alg (T, ν,{ˆτa}a∈A) + 4T1−β+ (3 + 2C + ¯ v−v)T(1+κ)/2
+ (2 + C + ¯ v−v)K⌈Tα⌉(1 + log2T)
+ (3 + C + ¯ v−v)(K(1 + log2T)T1−αζ+T1−ζ)
⩽RBandit-Alg (T, ν,{ˆτa}a∈A) + 4T1−β+ (3 + 2C + ¯ v−v)T(1+κ)/2
+ (3 + ¯C+ ¯v−v)((1 + log2T)(⌈Tα⌉+T1−αζ) +T1−ζ)
⩽(3 + 2C + ¯ v−v)(T1−ζ+T(κ+1)/2+ (1 + log2T)(⌈Tα⌉+T1−αζ))
+RBandit-Alg (T, ν,{ˆτa}a∈A) + 4T1−β
⩽2(3 + 2C + ¯ v−v) log2(T)(2T1−αζ+T(κ+1)/2+⌈Tα⌉) + 4T1−β
+RBandit-Alg (T, ν,{ˆτa}a∈A).
27Corollary 1. Assume that the upstream player’s distribution (γa)a∈Ais such that H1 holds. In
addition, suppose that the distributions (γa)a∈Aand(νa,b)a,b∈A×A are1-sub-Gaussian and that the
upstream player plays Πup
p=Algorithm 3(a slight modification of UCBto take into account the
incentives). Then the downstream player’s regret when she runs BELGIC with parameters α= 3/4
andβ= 1/4(which satisfy (13)) and subroutine Bandit-Alg =UCBsatisfies the following upper
bound4
Rdown
p(T,UCB,BELGIC )⩽(10 + 4 K+ 32p
Klog2(KT3) + ¯v−v) log2(T)(3 + 2 T3/4)
+ 3K2(¯v−v).
Proof of Corollary 1. First note that Πup
p=Algorithm 3satisfies H2 with constants κ= 1/2,
ζ= 2,C = 8p
Klog(KT3), following Proposition 2. Note that β/α = 1/3<1/2 = 1 −κ,
therefore Equation (13) is satisfied. Plugging these terms in the bound from Theorem 3 with
α= 3/4, β= 1/4gives
Rdown
p(T,UCB,BELGIC )⩽2(3 + 16p
Klog2(KT3) + ¯v−v) log2(T)(2T1−3/2+T3/4+⌈T3/4⌉)
+ 4T1/4+ 8p
K2log2(T)T1/2+ 3K2(¯v−v)
where we use the bound for RBandit-Alg (T, ν,{ˆτa}a∈A)with Bandit-Alg =UCBrun on any bandit
instance with K2arms, 1-subgaussian rewards and reward gaps of at most max a,b∈A×A vdown(a, b)−
mina,b∈A×A vdown(a, b) = ¯v−v, following [Lattimore and Szepesvári, 2020, Theorem 7.2]. There-
fore, we have that
Rdown
p(T,UCB,BELGIC )⩽(6 + 32p
Klog2(KT3) + ¯v−v) log2(T)(2 + 1 + 2 T3/4)
+ 4T1/4+ 3K2(¯v−v) + 8Kp
log2TT1/2
⩽(10 + 32p
Klog2(KT3) + ¯v−v) log2(T)(3 + 2 T3/4)
+ 3K2(¯v−v) + 8Kp
log2(T)T1/2,
which finally gives
Rdown
p(T,UCB,BELGIC )⩽(10 + 4 K+ 32p
Klog2(KT3) + ¯v−v) log2(T)(3 + 2 T3/4)
+ 3K2(¯v−v),
hence the result.
4Note that it is Kand not√
Khere, since the action space is of cardinality K2for the downstream player.
28NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
•[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist" ,
•Keep the checklist subsection headings, questions/answers and guidelines below.
•Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract mostly presents the issue that we tackle in the paper, namely
presenting an online version of the Coase theorem . This contribution is the most important
part of our paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
29Justification: Our work presents a model to explore an online version of a theory from
economics. Thereby, it implicitly has limitations due to the fact that a choice was made in
the model.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Our work presents a lot of theorems supported by assumptions (see H2,
Theorems 2 and 3...). All of Appendix C is here to provide a theoretical support to these
results.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
30Justification: Since our work is mostly a theoretic contribution, we do not present experi-
ments. Therefore, our paper is not concerned by this issue.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: Since our work is mostly a theoretic contribution, we do not present exper-
iments. Therefore, our paper is not concerned by this issue (same answer as for item
5).
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
31•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: Since our work is mostly a theoretic contribution, we do not present exper-
iments. Therefore, our paper is not concerned by this issue (same answer as for item
5).
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: Since our work is mostly a theoretic contribution, we do not present exper-
iments. Therefore, our paper is not concerned by this issue (same answer as for item
5).
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g., negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
32Answer: [NA]
Justification: Since our work is mostly a theoretic contribution, we do not present exper-
iments. Therefore, our paper is not concerned by this issue (same answer as for item
5).
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: As explained in the Related Works, an issue with externalities is the harm
caused to all the parties involved in the considered setting. Here, we try to provide a setup
so the players can negociate and achieve a social welfare optimal equilibrium. Therefore,
we are confident in the fact that we follow the NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We consider an issue arising in several real-world situations and provide an
algorithmic solution to it. We realize that this theoretical contribution still needs some work
to be implemented but we hope that it will contribute to positive social impacts in the future.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
33•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [Yes]
Justification: Our work is a theoretical contribution. Thereby, it does not propose a release
of data or any kind of trained model. We mix learning and theories from economics, which
does not involve yet risks for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: We do not use any assets requiring such conditions here.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We do not present such kind of new assets.
Guidelines:
34• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: We do not incorporate any experiment involving human subjects. Therefore,
we are not concerned by this item.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: We do not incorporate any experiment involving human subjects, hence our
answer.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
35