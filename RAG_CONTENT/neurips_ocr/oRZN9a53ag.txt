Score matching through the roof: linear, nonlinear,
and latent variables causal discovery
Anonymous Author(s)
Affiliation
Address
email
Abstract
Causal discovery from observational data holds great promise, but existing methods 1
rely on strong assumptions about the underlying causal structure, often requiring 2
full observability of all relevant variables. We tackle these challenges by leveraging 3
the score function ∇logp(X)of observed variables for causal discovery and 4
propose the following contributions. First, we generalize the existing results of 5
identifiability with the score to additive noise models with minimal requirements 6
on the causal mechanisms. Second, we establish conditions for inferring causal 7
relations from the score even in the presence of hidden variables; this result is 8
two-faced: we demonstrate the score’s potential as an alternative to conditional 9
independence tests to infer the equivalence class of causal graphs with hidden 10
variables, and we provide the necessary conditions for identifying direct causes in 11
latent variable models. Building on these insights, we propose a flexible algorithm 12
for causal discovery across linear, nonlinear, and latent variable models, which we 13
empirically validate. 14
1 Introduction 15
The inference of causal effects from observations holds the potential for great impact arguably in any 16
domain of science, where it is crucial to be able to answer interventional and counterfactual queries 17
from observational data [ 1,2,3]. Existing causal discovery methods can be categorized based on 18
the information they can extract from the data [ 4], and the assumptions they rely on. Traditional 19
causal discovery methods (e.g. PC, GES [ 5,6]) are general in their applicability but limited to the 20
inference of an equivalence class. Additional assumptions on the structural equations generating 21
effects from the cause are, in fact, imposed to ensure the identifiability of a causal order [ 7,8,9,10]. 22
As a consequence, existing methods for causal discovery require specialized and often untestable 23
assumptions, preventing their application to real-world scenarios. 24
Further, the majority of existing approaches are hindered by the assumption that all relevant causes 25
of the measured data are observed, which is necessary to interpret associations in the data as causal 26
relationships. Despite the convenience of this hypothesis, it is often not met in practice, and the solu- 27
tions relaxing this requirement face substantial limitations. The FCI algorithm [ 11] can only return an 28
equivalence class from the data. Appealing to additional restrictions ensures the identifiability of some 29
direct causal effects in the presence of latent variables: RCD [ 12] relies on the linear non-Gaussian 30
additive noise model, whereas CAM-UV [ 13] requires nonlinear additive mechanisms. Nevertheless, 31
the strict conditions on the structural equations hold back their applicability to more general settings. 32
Our paper tackles these challenges and can be put in the context of a recent line of work that 33
derives a connection between the score function ∇logp(X)and the causal graph underlying the 34
data-generating process [ 14,15,16,17,18,19]. The use of the score for causal discovery is 35
practically appealing, as it yields advantages in terms of scalability to high dimensional graphs [ 16] 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.and guarantees of finite sample complexity bounds [ 20]. Instead of imposing assumptions that ensure 37
strong, though often impractical, theoretical guarantees, we organically demonstrate different levels of 38
identifiability based on the strength of the modeling hypotheses, always relying on the score function 39
to encode all the causal information in the data. Starting from results of Spantini et al. [21] and Lin 40
[22], we show how constraints on the Jacobian of the score ∇2logp(X)can be used as an alternative 41
to conditional independence testing to identify the Markov equivalence class of causal models with 42
hidden variables. Further, we prove that the score function identifies the causal direction of additive 43
noise models, with minimal assumptions on the causal mechanisms. This extends the previous findings 44
of Montagna et al. [17], limited by the assumption of nonlinearity of the causal effects, and Ghoshal 45
and Honorio [14], limited to linear mechanisms. On these results, we build the main contributions 46
of our work, enabling the identification of direct causal effects in hidden variables models. 47
Our main contributions are as follows: (i)We present the necessary conditions for the identifiability 48
of direct causal effects and the presence of hidden variables with the score in the case of latent 49
variables models. (ii)We propose AdaScore (Adaptive Score-based causal discovery), a flexible 50
algorithm for causal discovery based on score matching estimation of ∇logp(X)[23]. Based on the 51
user’s belief about the plausibility of several modeling assumptions on the data, AdaScore can output 52
a Markov equivalence class, a directed acyclic graph, or a mixed graph, accounting for the presence 53
of unobserved variables. To the best of our knowledge, the broad class of causal models handled by 54
our method is unmatched by other approaches in the literature. 55
2 Model definition and related works 56
In this section, we introduce the formalism of structural causal models (SCMs), separately for the the 57
cases with and without hidden variables. 58
2.1 Causal model with observed variables 59
LetXbe a set of random variables in Rdefined according to the set of structural equations 60
Xi:=fi(XPAG
i, Ni),∀i= 1, . . . , k. (1)
Ni∈Rare mutually independent random variables with strictly positive density, known as noise 61
or error terms. The function fiis the causal mechanism mapping the set of direct causes XPAG
i62
ofXiand the noise term Ni, toXi’s value. A structural causal model (SCM) is defined as the 63
tuple (X, N,F,PN), where F= (fi)k
i=1is the set of causal mechanisms, and PNis the joint 64
distribution relative to the density pNover the noise terms N∈Rk. We define the causal graph G 65
as a directed acyclic graph (DAG) with nodes X={X1, . . . , X k}, and the set of edges defined as 66
{Xj→Xi:Xj∈XPAG
i}, such that PAG
iare the indices of the parent nodes of Xiin the graph 67
G. (In the remainder of the paper, we adopt the following notation: given a set of random variables 68
Y={Y1, . . . , Y n}and a set of indices Z⊂N, then YZ={Yi|i∈Z, Yi∈Y}.) 69
Under this model, the probability density of Xsatisfies the Markov factorization (e.g. Peters et al. 70
[1] Proposition 6.31): 71
p(x) =kY
i=1p(xi|xPAG
i), (2)
where we adopt the convention of lowercase letters referring to realized random variables, and use p
to denote the density of different random objects, when the distinction is clear from the argument.
This factorization is equivalent to the global Markov condition (e.g. Peters et al. [1]Proposition 6.22)
that demands that for all {Xi, Xj} ∈X, X Z⊆X\ {Xi, Xj}, then
Xi
|=d
GXj|XZ=⇒Xi
|=Xj|XZ,
where (·
|=· |·)denotes probabilistic conditional independence of Xi, Xjgiven XZ, and (·
|=d
G· |·) 72
is the notation for d-separation , a criterion of conditional independence defined on the graph G 73
(Definition 5 of the appendix). As it is commonly done, we assume that the reverse direction 74
Xi
|=Xj|XZ=⇒Xi
|=d
GXj|XZhold, and we say that the density pisfaithful to the graph G 75
[2,24] (hence the faithfulness assumption ). Together with the global Markov condition, faithfulness 76
implies an equivalence between the probabilistic and graphical notions of conditional independence: 77
Xi
|=Xj|XZ⇐⇒Xi
|=d
GXj|XZ. (3)
2In general, several DAGs may entail the same set of d-separations: graphs sharing such common 78
structure form a Markov equivalence class (see Definition 6 in the appendix). 79
The above model assumes that there aren’t any unobserved causes of variables in X, other than the 80
noise terms in N. As we are interested in distributions with potential hidden variables, we will now 81
generalize our model to represent data-generating processes that may involve latent causes. 82
Definitions on graphs. As graphs play a central role in our work, Appendix A.1 provides a 83
detailed overview of the fundamental notation and definitions that we rely on in the remainder of 84
the paper. For the next section, we advise the reader to be comfortable with the notions of ancestors 85
(Definition 2) and inducing paths (Definition 3) in DAGs. 86
Closely related works. Several methods for the causal discovery of fully observable models using 87
the score have been recently proposed. Ghoshal and Honorio [14] demonstrates the identifiability of 88
the linear non-Gaussian model from the score, and it is complemented by Rolland et al. [15], which 89
shows the connection between score matching estimation of ∇logp(X)and the inference of causal 90
graphs underlying nonlinear additive noise models with Gaussian noise terms, also allowing for 91
sample complexity bounds [ 20]. Montagna et al. [17] provides identifiability results in the nonlinear 92
setting, without posing any restriction on the distribution of the noise terms. Montagna et al. [16] 93
is the first to show that the Jacobian of the score provides information equivalent to conditional 94
independence testing in the context of causal discovery, limited to the case of additive noise models. 95
All of these studies make specialized assumptions to find theoretical guarantees of identifiability, 96
whereas our paper provides a unifying view of causal discovery with the score function, which 97
generalizes and expands the existing results. 98
2.2 Causal model with unobserved variables 99
Under the model (1), we consider the case where the set of variables Xis partitioned into the disjoint 100
subsets of observed random variables V={V1, . . . , V d}andunobserved (orlatent ) random variables 101
U={U1, . . . , U p}. We assume that the following set of structural equations is satisfied: 102
Vi:=fi(VPAG
i, Ui, Ni),∀i= 1, . . . , d, (4)
where Uistands for the set of unobserved parents of Vi, and VPAG
i={Vk|k∈PAG
i, Vk∈V}are 103
the observed direct causes of Vi. Some of the causal relations and the conditional independencies 104
implied by the set of equations (4)can be summarized in a graph obtained as a marginalization of the 105
DAGGonto the observable nodes V. 106
Definition 1 (Marginal graph, Zhang [25]).LetX=V˙∪UandGbe a DAG over X. The following 107
construction gives the marginal graphMG
V, with nodes Vand edges found as follows: 108
•pair of nodes Vi, Vjare adjacent in the graph MG
Vif and only if there is an inducing path 109
between them relative to UinG; 110
•for each pair of adjacent nodes Vi, VjinMG
V, orient the edge as Vi→VjifViis an ancestor 111
ofVjinG, else orient it as Vi↔Vj. 112
We define the map G 7→ MG
Vas the marginalization of the DAG GontoV, the observable nodes. 113
The graph resulting from the above construction is a maximal ancestral graph (MAG, Definition 4), 114
hence we will often refer to it as the marginal MAG ofG. Intuitively, a directed edge denotes the 115
presence of an ancestorship relation, whereas bidirected edges represent dependencies that can not be 116
removed by conditioning on any of the variables in the graph. 117
In the case of DAGs, d-separation encodes the probabilistic conditional independence relations 118
between the variables of Xin the graph G, as explicit by Equation (3). Such notion of graphical sepa- 119
ration has a natural generalization to maximal ancestral graphs, known as m-separation (Definition 5 120
of the appendix). Zhang [25] shows that m-separation andd-separation are in fact equivalent (see 121
Lemma 1 of the appendix), such that given VZ⊂Vand{Vi, Vj} ⊂V, the following holds: 122
Vi
|=d
GVj|VZ\ {Vi, Vj} ⇐⇒ Vi
|=m
MG
VVj|VZ\ {Vi, Vj}, (5)
3where (·
|=m
MG
V· | ·)denotes m-separation relative to the graph MG
V. Just like with DAGs, MAGs 123
that imply the same set of conditional independencies define an equivalence class. Usually, the 124
common structure of these graphs is represented by partial ancestral graphs (PAGs, Definition 7 of 125
the appendix). We use PMG
Vto denote the PAG relative to MG
V. 126
Problem definition. In this work, our goal is to provide theoretical guarantees for the
identifiability of the Markov equivalence class of the marginal graph MG
Vand its direct causal
effects with the score, where variables Viare defined according to Equation (4).
127
Without further assumptions on the data-generating process, we can identify the graph MG
Vonly up 128
to its partial ancestral graph, as discussed in the next section. 129
Closely related works. Causal discovery with latent variables have been first studied in the context 130
ofconstraint-based approaches with the FCI algorithm [ 11], which shows the identifiability of the 131
equivalence class of a marginalized graph via conditional independence testing. The RCD and 132
CAM-UV [ 12,13] approaches instead demonstrate the inferrability of directed causal edges via 133
regression and residuals independence testing. Both methods rely on strong assumptions on the 134
causal mechanisms: their theoretical guarantees apply to models where the effects are generated by a 135
linear (RCD) or nonlinear (CAM-UV) additive contribution of each cause. Our work demonstrates 136
that using the score function for causal discovery unifies and generalizes these results, presenting 137
an alternative to conditional independence testing for constraint-based methods, and being agnostic 138
about the class of causal mechanisms of the observed variables, under the weaker requirement of 139
additivity of the noise terms. 140
3 Theory for a score-based test of separation 141
In this section, we show that for V⊆Xgenerated according to Equation (4)the Hessian matrix of 142
logp(V)identifies the equivalence class of the marginal MAG MG
V. It has already been proven that 143
cross-partial derivatives of the log-likelihood are informative about a set of conditional independence 144
relationships between random variables: Spantini et al. [21] (Lemma 4.1) shows that, given VZ⊆X 145
such that {Vi, Vj} ⊆VZ, then 146
∂2
∂Vi∂Vjlogp(VZ) = 0 ⇐⇒ Vi
|=Vj|VZ\ {Vi, Vj}. (6)
Equation (3)resulting from faithfulness and the directed global Markov property immediately 147
implies that this expression can be used as a test of conditional independence to identify the Markov 148
equivalence class of the graph MG
V, as commonly done in constraint-based causal discovery (for 149
reference, see e.g. Section 3 in Glymour et al. [4]). This result generalizes Lemma 1 of Montagna et al. 150
[16], where it is used to define constraints to infer edges in the causal structure without latent variables. 151
Proposition 1 (Adapted1from [ 21]).LetVbe a set of random variables with strictly positive density
generated according to model (4). For each set VZ⊆Vof nodes in MG
Vsuch that {Vi, Vj} ⊆VZ,
the following holds for each supported value vZ:
∂2
∂Vi∂Vjlogp(vZ) = 0 ⇐⇒ Vi
|=m
MG
VVj|VZ\ {Vi, Vj}.
The result of Proposition 1 presents an alternative to conditional independence testing in constraint- 152
based approaches to causal discovery, showing that the equivalence class of the graph MG
Vcan be 153
identified using the cross partial derivatives of the log-likelihood as a test of conditional independence 154
between variables, much in the spirit of the Fast Causal Inference algorithm [ 11]. Identifying the 155
1In their Lemma 4.1 Spantini et al. [21] provides the connection between vanishing cross-partial derivatives
of the log-likelihood and conditional independence of random variables. Note that this result does not depend on
the assumption of a generative model, thus holding beyond the set of structural equations (4). Our result adapts
their finding to the case when observations are generated according to a fully observable causal model.
4Markov equivalence class is the most we can hope to achieve without further hypotheses. As we will 156
see in the next section, the score function can also help leverage additional restrictive assumptions on 157
the causal mechanisms of Equation (4) to identify direct causal effects. 158
4 A theory of identifiability from the score 159
In this section, we show that, under additional assumptions on the data-generating process, we can 160
identify the direct causal relations that are not influenced by unobserved variables, as well as the 161
presence of unobserved active paths (Definition 5) between nodes in the marginalized graph MG
V. 162
As a preliminary step before diving into causal discovery with latent variables, we show how the 163
properties of the score function identify edges in directed acyclic graphs, that is in the absence of 164
latent variables (when U=∅andG=MG
V). The goal of the next section is two-sided: first, it 165
introduces the fundamental ideas connecting the score function to causal discovery that also apply to 166
hidden variable models, second, it extends the existing theory of causal discovery with score matching 167
to additive noise models with both linear and nonlinear mechanisms. 168
4.1 Warm up: identifiability without latent confounders 169
In this section, we summarise and extend the theoretical findings presented in Montagna et al. [17], 170
where the authors show how to derive constraints on the score function that identify the causal order of 171
the DAG Gwhere all the variables in the set Xare observed. Define the structural relations of (1)as: 172
Xi:=hi(XPAG
i) +Ni, i= 1, . . . , k, (7)
with three times continuously differentiable mechanisms hi, noise terms centered at zero, and strictly 173
positive density pX. Given the Markov factorization of Equation (2), the components of the score 174
function ∇logp(x)are: 175
∂Xilogp(x) =∂Xilogp(xi|xPAG
i) +X
j∈CHG
i∂Xilogp(xj|xPAG
j)
=∂Nilogp(ni)−X
j∈CHG
i∂Xihj(xPAG
j)∂Njlogp(nj),(8)
where CHG
idenotes the set of children of node Xi. We observe that if a node Xsis asink, i.e. a 176
node satisfying CHG
s=∅, then the summation over the children vanishes, implying that: 177
∂Xslogp(x) =∂Nslogp(ns). (9)
The key point is that the score component of a sink node is a function of its structural equation noise 178
term, such that one could learn a consistent estimator of ∂XslogpXfrom a set of observations of the 179
noise term Ns. Given that, in general, one has access to Xsamples rather than observations of the 180
noise random variables, authors in Montagna et al. [17] show that Nsof a sink node can be consistently 181
estimated from i.i.d. realizations of X. For each node X1, . . . , X k, we define the quantity: 182
Ri:=Xi−E[Xi|X\Xi], (10)
where X\Xiare the random variables in the set X\ {Xi}.E[Xi|X\Xi]is the optimal least squares 183
predictor of Xifrom all the remaining nodes in the graph, and Riis the regression residual. For 184
a sink node Xs, the residual satisfies: 185
Rs=Ns, (11)
which can be seen by rewriting E[Xs|X\Xs] = hs(XPAG
s) +E[Ns|XDEG
s, XNDG
s] = 186
hs(XPAG
s) +E[Ns], where XDEG
sandXNDG
sdenotes the descendants and non-descendants of Xs, 187
respectively. Equations (9)and(11) together imply that the score ∂Nslogp(Ns)is a function of Rs, 188
such that it is possible to find a consistent approximator of the score of a sink from observations of Rs. 189
Proposition 2 (Generalization of Lemma 1 in Montagna et al. [17]).LetXbe a set of random 190
variables, generated by a restricted additive noise model (Definition 9) with structural equations (7), 191
and let Xj∈X. Consider rjin the support of Rj. Then: 192
Xjis a sink ⇐⇒Eh 
E
∂Xjlogp(X)|Rj=rj
−∂Xjlogp(X)2i
= 0. (12)
5Our result generalizes Lemma 1 in Montagna et al. [17], as they assume Xgenerated by an 193
identifiable additive noise model with nonlinear mechanisms. Instead, we remove the nonlinearity 194
assumption and make the weaker hypothesis of a restricted additive noise model, which is provably 195
identifiable [9], in the formal sense defined in the appendix (Definition 8). This result doesn’t come 196
as a surprise, given the previous findings of Ghoshal and Honorio [14] showing that the score infers 197
linear non-Gaussian additive noise models: Proposition 2 provides a unifying and general theory 198
for the identifiability of models with potentially mixed linear and nonlinear mechanisms. 199
Based on these insights, Montagna et al. [17] propose the NoGAM algorithm to exploit the con- 200
dition in (12) for identifying the causal order of the graph: being E[∂Xilogp(X)|Ri]the opti- 201
mal least squares estimator of the score of node Xifrom Ri, a sink node is characterized as the 202
argminiE[E[∂Xilogp(X)|Ri]−∂Xilogp(X)]2, where in practice the residuals Ri, the score 203
components and the least squares estimators are replaced by their empirical counterparts. After a 204
sink node is identified, it is removed from the graph and assigned a position in the order, and the 205
procedure is iteratively repeated up to the source nodes. Being the score estimated by score matching 206
techniques [23], we usually make reference to score matching-based causal discovery. 207
In the next section, we show how we can generalize these results to identify direct causal effects 208
between a pair of variables in the marginal MAG MG
Vwhen U̸=∅ 209
4.2 Identifiability in the presence of latent confounders 210
We now introduce the last of our main theoretical results, that is: given a pair of nodes Vi,Vjthat 211
are adjacent in the graph MG
VwithU̸=∅, we can use the score function to identify the presence 212
of a direct causal effect between ViandVj, or that of an active path that is influenced by unobserved 213
variables. Given that the causal model of Equation (4)ensures identifiability only up to the equivalence 214
class, we need additional restrictive assumptions. In particular, we enforce an additive noise model 215
with respect to both the observed and unobserved noise variables. This corresponds to an additive 216
noise model on the observed variables with the noise terms recentered by the latent causal effects. 217
Assumption 1 (SCM assumptions) .The set of structural equations of the observable variables 218
specified in (4)is now defined as: 219
Vi:=fi(VPAG
i) +gi(Ui) +Ni,∀i= 1, . . . , d, (13)
assuming the mechanisms fito be of class C3(R|VPAG
i|), and mutually independent noise terms with 220
strictly positive density function. The Ni’s are assumed to be non-Gaussian when fiis linear in some 221
of its arguments. 222
Crucially, our hypothesis is weaker than those required by two state-of-the-art approaches, CAM-UV 223
[13] and RCD [ 12]: CAM-UV assumes a Causal Additive Model (CAM) with structural equations 224
with nonlinear mechanisms in the form Vi:=P
k∈PAG
ifik(Vk) +P
Ui
kgik(Ui
k) +Ni, and RCD 225
requires an additive noise model with linear effects of both the latent and observed causes. Thus, 226
our model encompasses and extends the nonlinear and linear settings of CAM-UV and RCD, such 227
that the theory developed in the remainder of the section is valid for a broader class of causal models. 228
Our first step is rewriting the structural relations in (13) as: 229
Vi:=fi(VPAG
i) +˜Ni,
˜Ni:=gi(Ui) +Ni,∀i= 1, . . . , d,(14)
which provides an additive noise model in the form of (7). Next, we define the following regression 230
residuals for any node Vkin the graph MG
V: 231
Rk(VZ):=Vk−E[Vk|VZ\{k}], (15)
where VZ\{k}denotes the set of random variables VZ\ {Vk}. 232
Given these definitions, we are ready to show how directed edges, and the presence of unobserved 233
variables can be identified from the score of linear and nonlinear additive noise models. 234
64.2.1 Identifiability of directed edges 235
Consider Vi, Vjadjacent nodes in the PAG PMG
V: we want to investigate when a direct causal 236
effect Vi∈VPAG
jcan be identified from the score. We make the following observations: for 237
VZ=VPAG
j∪ {Vj}andVPAG
j
|=G
dUj, by Equation (15) it follows 238
Rj(VZ) =˜Nj−E[˜Nj], (16)
where we use VPAG
j
|=G
dUjto write E[˜Nj|VZ\{j}] =E[˜Nj]. Moreover, we note that Vjis a sink node 239
relative to MG
VZ, the marginalization of GontoVZ. In analogy to the case without latent variables, we 240
can show that ∂Vjlogp(VZ)is a function of ˜Nj, the error term in the additive noise model of Equation 241
(14), such that the score of Vjcan be consistently predicted from observations of the residual Rj(VZ). 242
Proposition 3. Let X be generated by a restricted additive noise model with structural equations (7), 243
and causal graph G. Consider Vi, Vjadjacent in MG
V, marginalization of G. Further, assume that 244
the score component ∂Vjlogp(VZ)is not constant for uncountable values of VZ. 245
(i) Let VZ=VPAG
j∪ {Vi, Vj}, and rj∈Rin the support of Rj(VZ). Then: 246
VPAG
j
|=d
GUj∧Vi∈VPAG
j⇐⇒E[∂Vjlogp(VZ)−E[∂Vjlogp(VZ)|Rj(VZ) =rj]]2= 0.
(ii) Let VZ⊆V, such that {Vi, Vj} ⊆VZ. Then: 247
VPAG
j̸|=d
GUj∨Vi̸∈VPAG
j⇐⇒E[∂Vjlogp(VZ)−E[∂Vjlogp(VZ)|Rj(VZ) =rj]]2̸= 0.
Intuitively, the proposition has two essential implications. Part (i)provides the condition for the 248
identifiability of the potential direct causal effect between a pair Vi, Vj, that is, when the association 249
between Vjand its observed parents is not influenced by active paths that involve latent variables. 250
This condition is necessary: given an active path such that VPAG
j̸|=d
GUj, the score could not identify 251
a direct causal effect Vi→Vj, which is the content of the second part of the proposition. 252
We have established theoretical guarantees of identifiability for linear and nonlinear additive noise 253
models, even in the presence of hidden variables: we find that the score function is a means for the 254
identifiability of all direct parental relations that are not influenced by unobserved variables; all the 255
remaining arrowheads of the edges in the graph MG
Vare identified no better than in the equivalence 256
class. Based on these insights, we propose AdaScore, a score matching-based algorithm for the 257
inference of Markov equivalence classes, direct causal effects, and the presence of latent variables. 258
4.3 A score-based algorithm for causal discovery 259
Building on our theory, we propose AdaScore, a generalization of NoGAM to linear and nonlinear 260
additive noise models with latent variables. The main strength of our approach is its adaptivity 261
with respect to structural assumptions: based on the user’s belief about the plausibility of several 262
modeling assumptions on the data, AdaScore can output an equivalence class (using the condition 263
of Proposition 1 instead of conditional independence testing in an FCI-like algorithm), a directed 264
acyclic graph (as in NoGAM), or a mixed graph, accounting for the presence of unobserved variables. 265
We now describe the version of our algorithm whose output is a mixed graph, where we rely on score 266
matching estimation of the score and its Jacobian (Appendix C.2). At an intuitive level, we find 267
unoriented edges using Proposition 1, i.e. checking for dependencies in the form of non-zero entries 268
in the Jacobian of the score via hypothesis testing on the mean, and find the edges’ directions via the 269
condition of Proposition 3, i.e. by estimating residuals of each node Xiand checking whether they can 270
correctly predict the i-th score entry (the vanishing mean squared errors are verified by hypothesis test 271
of zero mean). It would be tempting to simply find the skeleton (i.e. the graphical representation of 272
the constraints of an equivalence class) first via the well-known adjacency search of the FCI algorithm 273
and then iterate through all neighborhoods of all nodes to orient edges using Proposition 3. This 274
would be prohibitively expensive, as finding the skeleton is well-known to have super-exponential 275
computational complexity [ 11]. Instead, we propose an alternative solution: exploiting the fact that 276
some nodes may not be influenced by latent variables, we first use Proposition 2 to find sink nodes 277
7that are not affected by latents (using hypothesis testing to find vanishing mean squared error in the 278
score predictions from the residuals), in the spirit of the NoGAM algorithm. If there is such a sink, 279
we search all its adjacent nodes via Proposition 1 (plus an optional pruning step for better accuracy, 280
Appendix C.2), and orient the inferred edges towards the sink. Else, if no sink can be found, we pick 281
a node in the graph and find its neighbors by Proposition 1, orienting its edges using the condition in 282
Proposition 3 (score estimation by residuals under latent effects). This way, we get an algorithm that 283
is polynomial in the best case (Appendix C.3). Details on AdaScore are provided in Appendix C, 284
while a pseudo-code summary is provided in the Algorithm 1 box. 285
Algorithm 1 Simplified pseudo-code of AdaScore
while nodes remain do
ifProposition 3 finds a sink with all parents observed then
add edges from adjacent nodes to sink
else
pick some remaining node Vi∈V
prune neighbourhood of Viusing Proposition 1
orient edges adjacent to Viusing Proposition 3
ifVihas outgoing directed edge to some Vj∈Vthen
continue with Vj
else
remove Viform remaining nodes
prune remaining bidirected edges using Proposition 1
5 Experiments 286
We use the causally2Python library [ 26] to generate synthetic data with known ground truths, 287
created as Erdös-Rényi sparse and dense graphs, respectively with probability of edge between pair 288
of nodes equals 0.3and0.5. We sample the data according to linear and nonlinear mechanisms with 289
additive noise, where the nonlinear functions are parametrized by a neural network with random 290
weights, a common approach in the literature [ 18,26,27,28,29]. Noise terms are sampled from a 291
uniform distribution in the [−2,2]range. Hidden causal effects are obtained by randomly picking 292
two nodes and dropping the corresponding column from the data matrix. See Appendix D.1 for 293
further details on the data generation. As metric, we consider the structural Hamming distance (SHD) 294
[30,31], a simple count of the number of incorrect edges, where missing and wrongly directed 295
edges count as one error. We fix the level of the hypothesis tests of AdaScore to 0.05, which is a 296
common choice in the absence of prior knowledge. We compare AdaScore to NoGAM, CAM-UV , 297
RCD, and DirectLiNGAM, whose assumptions are detailed in Table 1. In the main manuscript, we 298
comment on the results on datasets of 1000 observations from dense graphs, with and without latent 299
variables. Additional experiments including those on sparse networks are presented in Appendix E. 300
Our synthetic data are standardized by their empirical variance to remove shortcuts in the data [ 18,32]. 301
Discussion. Our experimental results on models without latent variables of Figure 1a show that when 302
causal relations are linear, AdaScore can recover the causal graph with accuracy that is comparable 303
with all the other benchmarks, with the exception of DirectLiNGAM. On nonlinear data AdaScore 304
presents better performance than CAM-UV , RCD, and DirectLiNGAM while being comparable 305
to NoGAM in accuracy. This is in line with our expectations: in the absence of finite sample 306
errors and in the fully observable setting, NoGAM and AdaScore are indeed the same algorithms. 307
When inferring under latent causal effects, Figure 1b, our method performs comparably to CAM- 308
UV and RCD on graphs up to seven nodes while slightly degrading on nine nodes. Additionally, 309
AdaScore outperforms NoGAM in this setting, as we would expect according to our theory. Overall, 310
we observe that our method is robust to a variety of structural assumptions, with accuracy that is 311
often comparable and sometimes better than competitors (as in nonlinear observable settings). We 312
remark that although AdaScore does not clearly outperform the other baselines, its broad theoretical 313
guarantees of identifiability are not matched by any available method in the literature; this makes it 314
an appealing option for inference in realistic scenarios that are hard to investigate with synthetic data, 315
where the structural assumptions of the causal model underlying the observations are unknown. 316
2https://causally.readthedocs.io/en/latest/
8adascore camuv nogam rcd lingam
3 5 7 9
number of nodes05101520253035shd
linear
3 5 7 9
number of nodes05101520253035shd
nonlinear(a) Fully observable model
3 5 7 9
number of nodes051015202530shd
linear
3 5 7 9
number of nodes05101520253035shd
nonlinear
(b) Latent variables model
Figure 1: Empirical results on dense graphs with different numbers of nodes, on fully observable (no hidden
variables) and latent variable models. We report the SHD accuracy (the lower, the better). We note that
DirectLiNGAM is surprisingly robust to different structural assumptions, and AdaScore is generally comparable
or better (as in nonlinear observable data) than the other benchmarks.
Table 1: Experiments causal discovery algorithms. The content of the cells denotes whether the method supports
(✓) or not ( ✗) the condition specified in the corresponding row.
CAM-UV RCD NoGAM DirectLiNGAM AdaScore
Linear additive noise model ✗ ✓ ✗ ✓ ✓
Nonlinear additive noise model ✗ ✗ ✓ ✗ ✓
Nonlinear CAM ✓ ✗ ✓ ✗ ✓
Latent variables effects ✓ ✓ ✗ ✗ ✓
Output Mixed Mixed DAG DAG Mixed
6 Conclusion 317
The existing literature on causal discovery shows a connection between score matching and structure 318
learning in the context of nonlinear ANMs: in this paper, (i) we formalize and extend these results 319
to linear SCMs, and (ii) we show that the score retains information on the causal structure even in the 320
presence of unobserved variables. Additionally, while previous works posit the accent on finding the 321
causal order through the score, we study its potential to identify the Markov equivalence class with a 322
constraint-based strategy that does not explicitly require tests of conditional independence, as well as 323
to identify direct causal effects. Our theoretical insights result in AdaScore: unlike existing approaches 324
for the estimation of causal directions, our algorithm provides theoretical guarantees for a broad class 325
of identifiable models, namely linear and nonlinear, with additive noise, in the presence of latent 326
variables. Even though AdaScore does not clearly outperform the existing baselines on our synthetic 327
benchmark, its adaptivity to different structural hypotheses is a step towards causal discovery that is 328
less reliant on prior assumptions, which are often untestable and thus hindering reliable inference in 329
real-world problems. While we do not touch on the task of causal representation learning [ 33], where 330
causal variables are learned from data, we believe this is a promising research direction in relation 331
to our work due to the specific interplay between score-matching estimation and generative models. 332
9References 333
[1]Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: founda- 334
tions and learning algorithms . The MIT Press, 2017. 335
[2] Judea Pearl. Causality . Cambridge university press, 2009. 336
[3]Peter Spirtes. Introduction to causal inference. Journal of Machine Learning Research , 11(54): 337
1643–1662, 2010. URL http://jmlr.org/papers/v11/spirtes10a.html . 338
[4]Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on 339
graphical models. Frontiers in Genetics , 10, 2019. ISSN 1664-8021. doi: 10.3389/fgene.2019. 340
00524. URL https://www.frontiersin.org/articles/10.3389/fgene.2019.00524 . 341
[5]P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search . MIT press, 2nd 342
edition, 2000. 343
[6]David Maxwell Chickering. Optimal structure identification with greedy search. J. Mach. Learn. 344
Res., 3(null):507–554, mar 2003. ISSN 1532-4435. doi: 10.1162/153244303321897717. URL 345
https://doi.org/10.1162/153244303321897717 . 346
[7]Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, and Antti Kerminen. A linear non-gaussian 347
acyclic model for causal discovery. J. Mach. Learn. Res. , 7:2003–2030, dec 2006. ISSN 348
1532-4435. 349
[8]Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Non- 350
linear causal discovery with additive noise models. In D. Koller, D. Schuurmans, Y . Bengio, 351
and L. Bottou, editors, Advances in Neural Information Processing Systems , volume 21. Cur- 352
ran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2008/file/ 353
f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf . 354
[9]Jonas Peters, Joris M. Mooij, Dominik Janzing, and Bernhard Schölkopf. Causal discovery 355
with continuous additive noise models. J. Mach. Learn. Res. , 15(1):2009–2053, jan 2014. ISSN 356
1532-4435. 357
[10] Kun Zhang and Aapo Hyvärinen. On the identifiability of the post-nonlinear causal model. In 358
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence , UAI ’09, 359
page 647–655, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958. 360
[11] Peter Spirtes. An anytime algorithm for causal inference. In Thomas S. Richardson and Tommi S. 361
Jaakkola, editors, Proceedings of the Eighth International Workshop on Artificial Intelligence 362
and Statistics , volume R3 of Proceedings of Machine Learning Research , pages 278–285. 363
PMLR, 04–07 Jan 2001. URL https://proceedings.mlr.press/r3/spirtes01a.html . 364
Reissued by PMLR on 31 March 2021. 365
[12] Takashi Nicholas Maeda and Shohei Shimizu. Rcd: Repetitive causal discovery of linear 366
non-gaussian acyclic models with latent confounders. In Silvia Chiappa and Roberto Calandra, 367
editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and 368
Statistics , volume 108 of Proceedings of Machine Learning Research , pages 735–745. PMLR, 369
26–28 Aug 2020. URL https://proceedings.mlr.press/v108/maeda20a.html . 370
[13] Takashi Nicholas Maeda and Shohei Shimizu. Causal additive models with unobserved variables. 371
InUncertainty in Artificial Intelligence , pages 97–106. PMLR, 2021. 372
[14] Asish Ghoshal and Jean Honorio. Learning linear structural equation models in polynomial 373
time and sample complexity. In Amos Storkey and Fernando Perez-Cruz, editors, Proceedings 374
of the Twenty-First International Conference on Artificial Intelligence and Statistics , volume 84 375
ofProceedings of Machine Learning Research , pages 1466–1475. PMLR, 09–11 Apr 2018. 376
URL https://proceedings.mlr.press/v84/ghoshal18a.html . 377
[15] Paul Rolland, V olkan Cevher, Matthäus Kleindessner, Chris Russell, Dominik Janzing, Bernhard 378
Schölkopf, and Francesco Locatello. Score matching enables causal discovery of nonlinear 379
additive noise models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, 380
Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on 381
10Machine Learning , volume 162 of Proceedings of Machine Learning Research , pages 18741– 382
18753. PMLR, 17–23 Jul 2022. 383
[16] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello. 384
Scalable causal discovery with score matching. In 2nd Conference on Causal Learning and 385
Reasoning , 2023. URL https://openreview.net/forum?id=6VvoDjLBPQV . 386
[17] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Kun Zhang, and Francesco Locatello. 387
Causal discovery with score matching on additive models with arbitrary noise. In 2nd Conference 388
on Causal Learning and Reasoning , 2023. URL https://openreview.net/forum?id= 389
rVO0Bx90deu . 390
[18] Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, and Francesco Locatello. Shortcuts 391
for causal discovery of nonlinear models by score matching, 2023. 392
[19] Pedro Sanchez, Xiao Liu, Alison Q O’Neil, and Sotirios A. Tsaftaris. Diffusion models for 393
causal discovery via topological ordering. In The Eleventh International Conference on Learning 394
Representations , 2023. URL https://openreview.net/forum?id=Idusfje4-Wq . 395
[20] Zhenyu Zhu, Francesco Locatello, and V olkan Cevher. Sample complexity bounds for score- 396
matching: Causal discovery and generative modeling. Advances in Neural Information Process- 397
ing Systems , 36, 2024. 398
[21] Alessio Spantini, Daniele Bigoni, and Youssef Marzouk. Inference via low-dimensional 399
couplings, 2018. 400
[22] Juan Lin. Factorizing multivariate function classes. In M. Jordan, M. Kearns, and 401
S. Solla, editors, Advances in Neural Information Processing Systems , volume 10. MIT 402
Press, 1997. URL https://proceedings.neurips.cc/paper_files/paper/1997/ 403
file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf . 404
[23] Aapo Hyvärinen. Estimation of non-normalized statistical models by score matching. J. Mach. 405
Learn. Res. , 6:695–709, 2005. URL https://api.semanticscholar.org/CorpusID: 406
1152227 . 407
[24] Caroline Uhler, G. Raskutti, Peter Bühlmann, and B. Yu. Geometry of the faithfulness assump- 408
tion in causal inference. The Annals of Statistics , 41, 07 2012. doi: 10.1214/12-AOS1080. 409
[25] Jiji Zhang. Causal reasoning with ancestral graphs. Journal of Machine Learning Research , 9 410
(7), 2008. 411
[26] Francesco Montagna, Atalanti Mastakouri, Elias Eulig, Nicoletta Noceti, Lorenzo Rosasco, 412
Dominik Janzing, Bryon Aragam, and Francesco Locatello. Assumption violations 413
in causal discovery and the robustness of score matching. In A. Oh, T. Neumann, 414
A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural 415
Information Processing Systems , volume 36, pages 47339–47378. Curran Associates, 416
Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 417
93ed74938a54a73b5e4c52bbaf42ca8e-Paper-Conference.pdf . 418
[27] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without 419
acyclicity constraints. In International Conference on Learning Representations , 2022. URL 420
https://openreview.net/forum?id=eYciPrLuUhG . 421
[28] Nan Rosemary Ke, Silvia Chiappa, Jane X Wang, Jorg Bornschein, Anirudh Goyal, Melanie Rey, 422
Theophane Weber, Matthew Botvinick, Michael Curtis Mozer, and Danilo Jimenez Rezende. 423
Learning to induce causal structure. In International Conference on Learning Representations , 424
2023. URL https://openreview.net/forum?id=hp_RwhKDJ5 . 425
[29] Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and 426
Alexandre Drouin. Differentiable causal discovery from interventional data. In Proceedings of 427
the 34th International Conference on Neural Information Processing Systems , NIPS ’20, Red 428
Hook, NY , USA, 2020. Curran Associates Inc. ISBN 9781713829546. 429
11[30] Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing 430
bayesian network structure learning algorithm. Machine learning , 65:31–78, 2006. 431
[31] Sofia Triantafillou and Ioannis Tsamardinos. Score-based vs constraint-based causal learning in 432
the presence of confounders. In Cfa@ uai , pages 59–67, 2016. 433
[32] Alexander G. Reisach, Christof Seiler, and Sebastian Weichwald. Beware of the simulated dag! 434
causal discovery benchmarks may be easy to game. In Neural Information Processing Systems , 435
2021. URL https://api.semanticscholar.org/CorpusID:239998404 . 436
[33] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Ke, Nal Kalchbrenner, Anirudh 437
Goyal, and Y . Bengio. Toward causal representation learning. Proceedings of the IEEE , PP: 438
1–23, 02 2021. doi: 10.1109/JPROC.2021.3058954. 439
[34] Peter Spirtes and Thomas Richardson. A polynomial time algorithm for determining dag 440
equivalence in the presence of latent variables and selection bias. In Proceedings of the 6th 441
International Workshop on Artificial Intelligence and Statistics , pages 489–500. Citeseer, 1996. 442
[35] Yingzhen Li and Richard E Turner. Gradient estimators for implicit models. arXiv preprint 443
arXiv:1705.07107 , 2017. 444
[36] Peter Bühlmann, Jonas Peters, and Jan Ernest. CAM: Causal additive models, high-dimensional 445
order search and penalized regression. The Annals of Statistics , 42(6), dec 2014. URL 446
https://doi.org/10.1214%2F14-aos1260 . 447
12A Useful results 448
In this section, we provide a collection of results and definitions relevant to the theory of this paper. 449
A.1 Definitions over graphs 450
LetX=X1, . . . , X da set of random variables. A graph G= (X, E)consists of finitely many nodes 451
or vertices Xand edges E. We now provide additional definitions, separately for directed acyclic 452
and mixed graphs. 453
Directed acyclic graph. In adirected graph , nodes can be connected by a directed edge (→), and 454
between each pair of nodes there is at most one directed edge. We say that X1is aparent ofXjif 455
Xi→Xj∈E, in which case we also say that Xjis achild ofXi. Two nodes are adjacent if they 456
are connected by an edge. Three nodes are called a v-structure if one node is a child of the other 457
two, e.g. as Xi→Xk←Xjis a collider. A path inGis a sequence of at least two distinct vertices 458
Xi1, . . . , X imsuch that there is an edge between XikandXik+1. IfXik→Xik+1for every node 459
in the path, we speak of a directed path , and call Xikanancestor ofXik+1,Xik+1adescendant of 460
Xik. Given the set DEG
iof descendants of a node Xi, we define the set of non-descendants ofXias 461
NDG
i=X\(DEG
i∪{Xi}). A node without parents is called a source node . A node without children 462
is called a sink node . Adirected acyclic graph is a directed graph with no cycles. 463
Mixed graph. In amixed graph nodes can be connected by a directed edge (→) or a bidirected 464
edge (↔), and between each pair of nodes there is at most one directed edge. Two vertices are said 465
to be adjacent in a graph if there is an edge (of any kind) between them. The definitions of parent , 466
child ,ancestor ,descendant ,path provided for directed acyclic graph also apply in the case of mixed 467
graphs. Additionally, Xiis a spouse of Xj(and vice-versa) if Xi↔Xj∈E. An almost directed 468
cycle occurs when Xi↔Xj∈EandXiis an ancestor of XjinG. 469
For ease of reference from the main text, we separately provide the definition of inducing paths and 470
ancestors in directed acyclic graphs. 471
Definition 2 (Ancestor) .Consider a DAG Gwith set of nodes X, and Xi, Xjelements of X. We 472
say that Xiis an ancestor ofXjif there is a directed path from XitoXjin the graph, as in 473
Xi→. . .→Xj. 474
Definition 3 (Inducing path) .Consider a DAG Gwith set of nodes X, andY, Z disjoint subsets such 475
thatX=Y˙∪Z. We say that there is an inducing path relative to Zbetween the nodes Yi, Yjif every 476
node on the path that is not in Z∪ {Yi, Yj}is a collider on the path (i.e. for each Yk∈Yon the path 477
the sequence Yi. . .→Yk←. . . Y jappears) and every collider on the path is an ancestor of YiorYj. 478
One natural way to encode inducing paths and ancestral relationships between variables is represented 479
by maximal ancestral graphs. 480
Definition 4 (MAG) .Amaximal ancestral graph (MAG) is a mixed graph such that: 481
1. there are no directed cycles and no almost directed cycles; 482
2. there are no inducing paths between two non-adjacent nodes. 483
Next, we define conditional independence in the context of graphs. 484
Definition 5 (m-separation) .LetMbe a mixed graph with nodes X. A path πinMbetween Xi, Xj 485
elements of Xisactive w.r.t. Z⊆X\ {Xi, Xj}if: 486
1. every non-collider on πis not in Z 487
2. every collider on πis an ancestors of a node in Z. 488
XiandXjare said to be m-separated byZif there is no active path between XiandXjrelative to Z. 489
Two disjoint sets of variables WandYarem-separated byZif every variable in Wis m-separated 490
from every variable in YbyZ. 491
If m-separation is applied to DAGs, it is called d-separation . 492
13The set of directed acyclic graphs that satisfy the same set of conditional independencies form an 493
equivalence class, known as the Markov equivalence class . 494
Definition 6 (Markov equivalence class of a DAG) .LetGbe a DAG with nodes X. We denote with 495
[G]theMarkov equivalence class ofG. A DAG ˜Gwith nodes Xis in[G]if the following conditions 496
are satisfied for each pair Xi, Xjof distinct nodes in X: 497
• there is an edge between Xi,XjinGif and only if there is an edge between Xi,Xjin˜G; 498
• letZ⊆X\ {Xi, Xj}. Then Xi
|=d
GXj|Z⇐⇒ Xi
|=d
˜GXj|Z; 499
•letπbe a path between XiandXj.Xkis a collider in the path πinGif and only if it is a 500
collider in the path πin˜G. 501
In summary, graphs in the same equivalence class share the edges up to direction, the set of d- 502
separations, and the set of colliders. 503
Just as for DAGs, there may be several MAGs that imply the same conditional independence 504
statements. Denote the Markov-equivalence class of a MAG Mwith[M]: this is represented by a 505
partial mixed graph, the class of graphs that can contain four kinds of edges: →,↔,◦− −◦ and◦→, 506
and hence three kinds of end marks for edges: arrowhead ( >), tail ( −) and circle ( ◦). 507
Definition 7 (PAG, Definition 3 of Zhang [25]).Let[M]be the Markov equivalence class of an 508
arbitrary MAG M. The partial ancestral graph (PAG) for [M],PM, is a partial mixed graph such 509
that: 510
•PMhas the same adjacencies as M(and any member of [M]) does; 511
• A mark of arrowhead is in PMif and only if it is shared by all MAGs in [M]; and 512
• A mark of tail is in PMif and only if it is shared by all MAGs in [M]. 513
Intuitively, a PAG represents an equivalence class of MAGs by displaying all common edge marks 514
shared by all members of the class and displaying circles for those marks that are not in common. 515
A.2 Equivalence between m-separation and d-separation 516
In this section, we provide a proof for equation (5), stating the equivalence between m-separation and 517
d-separation in a formal sense. 518
Lemma 1 (Adapted from Zhang [25]).LetGbe a DAG with nodes X=V∪U, with VandU
disjoint sets, and MG
Vthe marginalization of GontoV. For any {Vi, Vj} ∈VandVZ⊆V\{Vi, Vj},
the following equivalence holds:
Vi
|=d
GVj|VZ⇐⇒ Vi
|=m
MG
VVj|VZ.
Proof. The implication Vi
|=d
GVj|VZ=⇒Vi
|=m
MG
VVj|VZis a direct consequence of Lemma 18 519
from Spirtes and Richardson [34], where we set S=∅, since we do not consider selection bias. The 520
implication Vi
|=d
GVj|VZ⇐=Vi
|=m
MG
VVj|VZfollows from Lemma 17 by Spirtes and Richardson 521
[34], again with S=∅. Note, that in their terminology “d-separation in MAGs” is what we call 522
m-separation. 523
A.3 Additive noise model identifiability 524
We study the identifiability of the additive noise model, reporting results from Peters et al. [9]. We 525
start with a formal definition of identifiability in the context of causal discovery. 526
Definition 8 (Identifiable causal model) .Let(X, N,F, pN)be an SCM with underlying graph Gand 527
pXjoint density function of the variables of X. We say that the model is identifiable from observa- 528
tional data if the distribution pXcan not be generated by a structural causal model with graph ˜G ̸=G. 529
First, we consider the case of models of two random variables 530
X2:=f(X1) +N, X 1
|=N. (17)
14Condition 1 (Condition 19 of Peters et al. [9]).Consider an additive noise model with structural 531
equations (17). The triple (f, pX1, pN)does not solve the following differential equation for all pairs 532
x1, x2withf′(x2)ν′′(x2−f(x1))̸= 0: 533
ξ′′′=ξ′′f′′
f′−ν′′′f′
ν′′
+ν′′′ν′f′′f′
ν′′−ν′(f′′)2
f′−2ν′′f′′f′+ν′f′′′, (18)
Here, ξ:= log pX1,ν:= log pN, the logarithms of the strictly positive densities. The arguments 534
x2−f(x1),x1, and x1ofν,ξandfrespectively, have been removed to improve readability. 535
Next, we show that a structural causal model satisfying Condition 1 is identifiable, as in Definition 8 536
Theorem 1 (Theorem 20 of Peters et al. [9]).LetpX1,X2the joint distribution of a pair of random 537
variables generated according to the model of equation (17) that satisfies Condition 1, with graph G. 538
Then,Gis identifiable from the joint distribution. 539
Finally, we show an important fact, holding for identifiable bivariate models, which is that the score 540
∂
∂X1logp(x1, x2)is nonlinear in x1. 541
Lemma 2 (Sufficient variability of the score) .LetpX1,X2the joint distribution of a pair of random
variables generated according to a structural causal model that satisfies Condition 1, with graph G.
Then:
∂
∂X1(ξ′(x1)−f′(x1)ν′(x2−f(x1)))̸= 0,
for all pairs (x1, x2). 542
Proof. By contradiction, assume that there exists (x1, x2)such that∂
∂X1(ξ′(x1)−f′(x1)ν′(x2−
f(x1))) = 0 . Then:
∂
∂X1
∂2
∂X2
1π(x1, x2)
∂2
∂X1∂X2π(x1, x2)
= 0,
where π(x1, x2) = log p(x1, x2). By explicitly computing all the partial derivatives of the above 543
equation, we obtain that equation 18 is satisfied, which violates Condition 1. 544
These results guaranteeing the identifiability of the bivariate additive noise model can be generalized 545
to the multivariable case, with a set of random variables X={X1, . . . , X k}that satisfy: 546
Xi:=fi(XPAG
i) +Ni, i= 1, . . . , k, (19)
where Gis the resulting causal graph directed and acyclic. The intuition is that, rather than studying 547
the multivariate model as a whole, we need to ensure that Condition 1 is satisfied for each pair of 548
nodes, adding restrictions on their marginal conditional distribution. 549
Definition 9 (Definition 27 of Peters et al. [9]).Consider an additive noise model with structural
equations (19). We call this SCM a restricted additive noise model if for all Xj∈X,Xi∈XPAG
j,
and all sets XS⊆X,S⊂N, with XPAG
j\ {Xi} ⊆XS⊆XG
NDj\ {Xi, Xj}, there is a value xS
withp(xS)>0, such that the triplet
(fj(xPAG
j\{i},·), pXi|XS=xS, pNj)
satisfies Condition 1. Here, fj(xPAG
j\{i},·)denotes the mechanism function xi7→fj(xPAG
j). 550
Additionally, we require the noise variables to have positive densities and the functions fjto be 551
continuous and three times continuously differentiable. 552
Then, for a restricted additive noise model, we can identify the graph from the distribution. 553
Theorem 2 (Theorem 28 of Peters et al. [9]).LetXbe generated by a restricted additive noise 554
model with graph G, and assume that the causal mechanisms fjare not constant in any of the input 555
arguments, i.e. for Xi∈XPAG
j, there exist xi̸=x′
isuch that fj(xPAG
j\{i}, xi)̸=fj(xPAG
j\{i}, x′
i). 556
Then,Gis identifiable. 557
15A.4 Other auxiliary results 558
We state several results that hold for a pair of random variables that are not connected by an active path 559
that includes unobserved variables (active paths are introduced in Definition 5). For the remainder of 560
the section, let V, U be a pair of disjoint sets of random variables, X=V∪Ugenerated according 561
to the structural causal model defined by the set of equations (1),Gthe associated causal graph, and 562
MG
Vthe marginalization onto V. 563
The first statement provides under which condition the unobserved parents of two variables in the 564
marginal MAG are mutually independent random vectors. 565
Lemma 3. LetVj∈V, and Z⊂Nsuch that VZ=VPAG
j∪ {Vj}. Assume VPAG
j
|=d
GUj. Then 566
Uj
|=d
GUZkfor each index Zk̸=j. 567
Proof. The assumption VPAG
j
|=d
GUjimplies that there is no active path in Gbetween nodes in VPAG
j568
and nodes in Uj. Given that for each Zk∈Z,Zk̸=Z, nodes in UZkare direct causes of at least 569
one node in VPAG
j, any active path between nodes in UZkand nodes in Ujwould also be an active 570
path between VPAG
jandUj, which is a contradiction. Hence Uj
|=d
GUZk. 571
The previous lemmas allow proving the following result, which will be fundamental to demonstrate 572
the theory of Proposition 3. 573
Lemma 4. LetVj∈V, and Z⊂Nsuch that VZ=VPAG
j∪ {Vj}. Assume VPAG
j
|=d
GUj. W.l.o.g.,
let the j-th element of VZbeVZj=Vj. Denote as UZthe set of unobserved parents of nodes in VZ,
andUZ\{j}the unobserved parents of nodes in VZ\{j}:=VZ\Vj. Then, the following holds for
eachvZ, uZvalues:
logp(vZ) = log p(vj|vPAG
j) + log Q(vZ),
where
Q(vZ) =X
uZ\{j}p(uZ\{j})|Z|Y
k̸=jp(vZk|vZ1, . . . , v Zk−1, uZk).
Proof. By the law of total probability and the chain rule, we can write p(vZ)as: 574
p(vZ) =X
up(vZ|u)p(u)
=X
up(u)p(vZj|u, vZ\{j})p(vZ\{j}|u).(20)
By Lemma 3, UZj
|=UZk,k̸=j, where UZkdenotes unobserved parents of the node VZk. Then, 575
we can factorize p(u) =p 
uZj
p 
uZ\{j}
. Plugging the factorization in equation (20) we find 576
p(vZ) =X
up 
uZj
p
uZ\{j}
p(vZj|u, vZ\{j})p(vZ\{j}|u)
=X
up 
uZj
p
uZ\{j}
p(vZj|uZj, vPAG
Zj)p(vZ\{j}|u),
where the latter equation comes from the global Markov property on the graph G. Further, by assump- 577
tion of VPAG
j
|=d
GUj, we know that UZj
|=VZk,k̸=j, such that p(vZ\{j}|u) =p(vZ\{j}|uZ\{j}). 578
Then: 579
p(vZ) =X
up 
uZj
p
uZ\{j}
p(vZj|uZj, vPAG
Zj)p(vZ\{j}|uZ\{j})
=X
uZjp 
uZj
p(vZj|uZj, vPAG
Zj)X
uZ\{j}p
uZ\{j}
p(vZ\{j}|uZ\{j})
=p(vZj|vPAG
Zj)X
uZ\{j}p
uZ\{j}
p(vZ\{j}|uZ\{j}),
which proves the claim. 580
16Intuitively, Lemma 4 shows that given a node Vjwithout children and bidirected edges in a marginal- 581
ized graph MG
VZ, the kernel of node Vjin the Markov factorization of p(vZ)is equal to the kernel of 582
the same node in the Markov factorization of p(x)of equation (2), relative to the graph without latent 583
confounders G. 584
B Proofs of theoretical results 585
B.1 Proof of Proposition 1 586
Proof of Proposition 1. Observe that
∂2
∂Vi∂Vjlogp(vZ) = 0 ⇐⇒ Vi
|=d
GVj|VZ\ {Vi, Vj} ⇐⇒ Vi
|=m
MG
VVj|VZ\ {Vi, Vj},
where the first equivalence holds by a combination of the faithfulness assumption with the global 587
Markov property, as explicit in equation (3), and the second due to Lemma 1. Then, the claim is 588
proven. 589
B.2 Proof of Proposition 2 590
Proof. The forward direction is immediate from equation (9)andRj=Nj, when Xjis a sink
(equation (11)). Thus, we focus on the backward direction. Given
Eh 
E
∂Xjlogp(X)|Rj=rj
−∂Xjlogp(X)2i
= 0,
we want to show that Xjhas no children, which we prove by contradiction. 591
Let us introduce a function q:R→Rsuch that:
E
∂Xjlogp(X)|Rj=rj
=q(rj),
andsj:R|X|→R,
sj(x) =∂Xjlogp(x).
The mean squared error equal to zero implies that sj(X)is a constant, once Rjis observed. Formally,
under the assumption of p(x)>0for each x∈Rk, this implies that
p(sj(x)̸=q(Rj)|Rj=rj) = 0 ,∀x∈Rk.
By contradiction, we assume that Xjis not a leaf, and want to show that sj(X)is not constant in X,
given Rjfixed. Let Xisuch that Xj∈XPAG
i. Being the structural causal model identifiable, there
is no model with distribution pXwhose graph has a backward edge Xi→Xj: thus, the Markov
factorization of equation (2) is unique and implies:
∂Xjlogp(X) =∂Njlogp(Nj)−X
k∈CHG
j∂Xjhk(XPAk)∂Nklogp(Nk).
We note that, by definition of residual in equation (10), Rj=rjfixes the following distance:
Rj=Nj−E[Nj|X\Xj].
Hence, conditioning on Rjdoesn’t restrict the support of X: given Rj=rj, for any x\Xj(value
of the vector of elements in X\ {Xj}),∃njwithp(nj>0)(by the hypothesis of strictly positive
densities of the noise terms) that satisfies
rj=nj−E[Nj|x\Xj].
Next, we condition on all the parents of Xi, except for Xj, to reduce our problem to the simpler 592
bivariate case. Let S⊂NandXS⊆Xsuch that XPAG
i\ {Xj} ⊆XS⊆XNDG
i\ {Xi, Xj}, 593
and consider xSsuch that p(xS>0). Let XPAG
i=xPAG
ihold under XS=xS. We define 594
Xj|xs:=Xj|(XS=xS), and similarly X|xs:=X|(XS=xS). Being the SCM a restricted 595
17additive noise model, by Definition 9, the triplet (gi, pXj|xs, pNi)satisfies Condition 1, where 596
gi(xj) =hi(xPAG
i\{Xj}, xj). Consider Xi=xi, and the pair of values (xj, x∗
j)such that xj̸=x∗
j597
and 598
ν′′
Ni(xi−gi(xj))g′
i(xj)̸= 0,
ν′′
Ni(xi−gi(x∗
j))g′
i(x∗
j)̸= 0,
where we resort to the usual notation νNi:= log pNi. By Lemma 2, (xi, xj)and(xi, x∗
j)satisfy: 599
∂Xj(ξ′(xj)−ν′
Ni(xi−gi(xj))g′
i(xj))̸= 0,
∂Xj(ξ′(x∗
j)−ν′
Ni(xi−gi(x∗
j))g′
i(x∗
j))̸= 0,
where ξ:= log pXj|xs. Thus, we can fix xjandx∗
j(which are arbitrarily chosen) such that 600
∂Xj(ξ′(xj)−ν′
Ni(xi−gi(xj))g′
i(xj))−∂Xj(ξ′(x∗
j)−ν′
Ni(xi−gi(x∗
j))g′
i(x∗
j))̸= 0. (21)
Fixing X|xS,xj=xandX|xS,x∗
j=x∗, where the two values differ only in their j- thcomponent, we
find the following difference:
sj(x)−sj(x∗) =∂Xj(ξ′(xj)−ν′
Ni(xi−gi(xj))g′
i(xj))−∂Xj(ξ′(x∗
j)−ν′
Ni(xi−gi(x∗
j))g′
i(x∗
j)),
which is different from 0by equation (21). This contradicts the fact that the score sjis constant once 601
Rjis fixed, which proves our claim. 602
B.3 Proof of Proposition 3 603
In this proof, we use several ideas from the demonstration of Proposition 2. We demonstrate the 604
forward and the backward parts of the two statements separately. 605
Proof of part (i), forward direction. Given VZ=VPAG
j∪ {Vi, Vj}andrj∈Rin the image of Rj, 606
we want to show: 607
VPAG
j
|=d
GUj∧Vi∈VPAG
j=⇒E[∂Vjlogp(VZ)−E[∂Vjlogp(VZ)|Rj(VZ) =rj]]2= 0.
By Lemma 4, the score of Vjis 608
∂Vjlogp(VZ) =∂Vjlogp(Vj|VPAG
j) +∂VjlogQ(VZ)
= log p(˜Nj),
for some Qmap acting on VZ. The latter equality holds because all variables in VZare non-
descendants of Vj, such that ∂VjQ(VZ) = 0 . Further, by equation (16) we know that
Rj(VZ) =˜Nj+c,
where c=−E[˜Nj]is a constant. It follows that the least square estimator of the score of Vjfrom
Rj(VZ)satisfies the following equation:
E[∂Vjlogp(VZ)|Rj(VZ)] =E[∂Vjlogp(˜Nj)|˜Nj] =∂Vjlogp(˜Nj),
where the first equality holds because E[·|˜Nj] =E[·|˜Nj+c]. Then, we find
E[∂Vjlogp(VZ)−E[∂Vjlogp(VZ)|Rj(VZ) =rj]]2=E[∂Vjlogp(˜Nj)−∂Vjlogp(˜Nj)]2= 0,
which is exactly our claim. 609
Proof of part (i), backward direction. Given VZ=VPAG
j∪{Vi, Vj},rj∈Rin the image of Rj, and 610
E[∂Vjlogp(VZ)−E[∂Vjlogp(VZ)|Rj(VZ) =rj]]2= 0, (22)
we want to show that VPAG
j
|=d
GUj∧Vi∈VPAG
j, meaning that there is a direct causal effect that 611
is not biased by unobserved variables. We provide the proof by contradiction, in analogy to the 612
demonstration of the backward direction of Proposition 2. 613
18Let us introduce sj:R|VZ|→R,
sj(vZ) =∂Vjlogp(VZ).
The mean squared error equal to zero implies that sj(VZ)is constant in VZ, once Rjis observed.
By contradiction, we assume that VPAG
j̸|=d
GUj∨Vi̸∈VPAG
j, and want to show that sj(VZ)is not
constant in VZ, given Rjfixed. In this regard, we make the following observation: by definition of
residual in equation (15), Ri(VZ) =rifixes the following distance:
Rj(VZ) =˜Nj−E[˜Nj|VZ\{j}].
Hence, conditioning on Rj(VZ)doesn’t restrict the support of VZ: given Rj(VZ) =rj,∃˜njwith
p(˜nj)>0(by assumption of strictly positive densities pNjandpX), that satisfies
rj= ˜nj−E[˜Nj|vZ\{j}],
for all vZ\{j}. Hence, the random variable VZ|Rj(VZ) =rjhas strictly positive density on all points 614
vZwhere pVZ(vZ)>0. Now, consider vZandv∗
Z, taken from the set of uncountable values such that 615
the score sjfunction is not a constant, meaning that sj(vZ)̸=sj(v∗
Z), where VZis sampled given 616
Rj(VZ) =rj. Given that different vZandv∗
Zare selected from an uncountable subset of the support, 617
we conclude that the score sj|(Rj(VZ) =rj) =∂Vjlogp(VZ|Rj(VZ) =rj)is not a constant for at 618
least an uncountable set of points, which contradicts equation (22). 619
Proof of part (ii), forward direction. Given that Viis connected to Vjin the marginal MAG and that 620
VPAG
j̸
|=d
GUj∨Vi̸∈VPAG
j, we want to show that for each VZ⊆Vwith{Vi, Vj} ⊆VZ, the 621
following holds: 622
E[∂Vjlogp(VZ)−E[∂Vjlogp(VZ)|Rj(VZ) =rj]]2̸= 0. (23)
Let us introduce h:R→Rsuch that:
E[∂Vjlogp(VZ)|Rj(VZ) =rj] =h(rj),
and further define:
sj(VZ) =∂Vjlogp(VZ).
Having the mean squared error in equation (23) equals zero implies that sj(VZ)is a constant, once
Rj(VZ)is observed. Thus, the goal of the proof is to show that there are values of VZsuch that the
score is not a constant once Rjis fixed. By definition of residual in equation (15),Rj(VZ) =rjfixes
the following distance:
Rj(VZ) =˜Nj−E[˜Nj|VZ\{j}].
Hence, conditioning on Rj(VZ)doesn’t restrict the support of VZ: given Rj(VZ) =rj,∃˜njwith
p(˜nj)>0(by assumption of positive density of the noise Njon the support R), that satisfies
rj= ˜nj−E[˜Nj|vZ\{j}],
for all vZ\{j}. Hence, the random variable VZ|Rj(VZ) =rjhas strictly positive density on all points 623
vZwhere pVZ(vZ)>0. Now, consider vZandv∗
Z, taken from the set of uncountable values such that 624
the score sjfunction is not a constant, meaning that sj(vZ)̸=sj(v∗
Z), where VZis sampled given 625
Rj(VZ) =rj. Given that different vZandv∗
Zare selected from an uncountable subset of the support, 626
we conclude that the score sj|(Rj(VZ) =rj) =∂Vjlogp(VZ|Rj(VZ) =rj)is not a constant for at 627
least an uncountable set of points, such that the claim follows. 628
Proof of part (ii), backward direction. Given that E[∂Vjlogp(VZ)−E[∂Vjlogp(VZ)|Rj(VZ) =
rj]]2̸= 0 for all VZ⊆Vsuch that {Vi, Vj} ∈VZ, and given ViandVjadjacent in the marginal
MAG, we want to show that
VPAG
j̸|=d
GUj∨Vi̸∈VPAG
j.
The prove comes easily by contradiction: say that VPAG
j
|=d
GUj∧Vi∈VPAG
j. Then, by the forward 629
direction of part (i)of Proposition 3, we know that VZ=VPAG
j∪ {Vj}satisfies E[∂Vjlogp(VZ)− 630
E[∂Vjlogp(VZ)|Rj(VZ) =rj]]2= 0, leading to a contradiction. 631
19C Algorithm 632
C.1 Detailed description of our algorithm 633
In Proposition 1 we have seen that score matching can detect m-separations and therefore the skeleton 634
of the PAG describing the data. If one is willing to make the assumptions required for Proposition 3 635
it could be desirable to use this to orient edges, since the interpretation of PAG edges might be 636
cumbersome for people not familiar with ancestral models. Therefore, one could simply find the 637
skeleton of the PAG using the fast adjacency search [ 5] and then orient the edges by applying 638
Proposition 3 on every subset of the neighbourhood of every node. This would yield a very costly 639
algorithm. But if we make the assumptions required to orient edges with Proposition 3 we can do a 640
bit better. In Algorithm 2 we present an algorithm that still has the same worst case runtime but runs 641
polynomially in the best case. The main intuition is that we iteratively remove irrelevant nodes in the 642
spirit of the original SCORE algorithm [ 15]. To this end, we first check if the is any unconfounded 643
sink if we consider the set of all remaining variables. If there is one, we can orient its parents and 644
ignore it afterwards. If there is no such set, we need to fall back to the procedure proposed above, i.e. 645
we need to check the condition of Proposition 3 on all subsets of the neighbourhood of a node, until 646
we find no node with a direct outgoing edge. In Proposition 4 we show that this way we do not fail 647
orient edge or fail to remove any adjacency. In the following discussion, we will use the notation 648
δi(XZ):=E[∂Vjlogp(VZ)−E[∂Vjlogp(VZ)|Rj(VZ) =rj]]2,
for the second residual from Proposition 3 and also 649
δi,j(XZ):=∂2
∂Vi∂Vjlogp(vZ)
for the cross-partial derivative, where Xi, Xj∈VandZ⊆V. 650
Proposition 4 (Correctness of algorithm) .LetX=V˙∪Ube generated by the SCM in Equation (4) 651
with non-constant scores for uncountably many values. Let GXbe the causal DAG of XandGVbe 652
the marginal MAG of GX. Then Algorithm 2 outputs a directed edge from Xi∈VtoXj∈Viff 653
there is a direct edge in GXbetween them and no unobserved backdoor path w.r.t. U. Further, the 654
output of Algorithm 2 has the same skeleton as GV. 655
Proof. We proof the statement by induction over the steps of the algorithm. Let Sbe the set of 656
remaining nodes in an arbitrary step of the algorithm. Our induction hypothesis is that for Xi, Xj∈S 657
andXk∈Biwe have 658
1.Xiis an unconfounded sink w.r.t. to some set S′⊆SiffXiis an unconfounded sink w.r.t. 659
some S′′⊆V 660
2. if there is no S′⊆V\ {Xi, Xj}such that Xi
|=Xj|S′thenXj∈Bi 661
Clearly, this holds in the initial step as S=V. 662
Suppose we find δi(XS) = 0 forXi∈S. IfXihas at least one adjacent node in MG
V, by 663
Proposition 3, we know that Xidoes not have any children and is also not connected to any other 664
node in Svia a hidden mediator or unobserved confounder. This means, all nodes that are not 665
separable from Ximust be direct parents of Xi, which are by our induction hypothesis 2) the nodes 666
inBi. Since Xidoes not have children, it also suffices to check Xi
|=Xj|S\ {Xi, Xj}forXj∈Bi 667
(instead of conditioning on all subsets of Bi). So we can already add these direct edges to the output. 668
If, on the other hand, Xihas no adjacent nodes in MG
V, we have Xi
|=Xj|S\{Xi, Xj}forXj∈Bi, 669
so in both cases we add the correct set of parents. Since Xiis not an ancestor of any of the nodes in 670
S\ {Xi},Xicannot be a hidden mediator or hidden confounder between nodes in S\ {Xi}and 671
conditioning on Xicannot block an open path. Thus, the induction hypothesis still holds in the next 672
step. 673
Suppose now there is no unconfounded sink and we explore Xi. By our induction hypothesis 2), Bi 674
contains the parents of Xiand by Proposition 3 it suffices to only look at subsets of Bito orient direct 675
edges. And also due to the induction hypothesis 2) Bicontains all nodes that are not separable from 676
Xi. So by adding bidirected edges to all nodes in Bican only add too many edges but not miss some. 677
20Algorithm 2 AdaScore Algorithm
procedure ADASCORE (p, X 1, . . . , X d)
S← {X1, . . . , X d} ▷Remaining nodes
E← { } ▷Edges
forXi∈Sdo
Bi← {X1, . . . , X d} ▷Neighbourhoods
while S̸=∅do ▷While nodes remain
if∃Xi∈S:δi(XS) = 0 then ▷If there is an unconfounded sink
S←S\ {Xi}
E←E∪ {Xj→Xi:δi,j(XS)̸= 0} ▷Add edges like DAS
else
forXi∈Sdo
forXj∈Bido ▷Prune neighbourhoods
ifδi,j(XS) = 0 then
Bi←Bi\ {Xj}
Bj←Bj\ {Xi}
forXj∈Bido ▷Orient edges in Bi
mi= min S′⊆Biδi(XS′∪{Xi})
mj= min S′⊆Bjδj(XS′∪{Xj}))
ifmi= 0∧mj̸= 0then
E←E∪ {Xj→Xi}
else if mi̸= 0∧mj= 0then
E←E∪ {Xi→Xj}
else
E←E∪ {Xi↔Xj}
if∃Xj∈Bi: (Xi→Xj)∈Ethen
continue with Xj
else ▷ Xihas no unconfounded outgoing edge
S←S\ {Xi} ▷Remove Xi
break
forXi↔Xj∈Edo ▷Prune bidirected edges
ifminS′⊆Adj(Xi)δi,j(XS′∪{Xi}) = 0∨minS′⊆Adj(Xj)δi,j(XS′∪{Xi}) = 0 then
E←E\ {Xi↔Xj}
return E
Now it remains to show that the induction hypothesis holds if we set StoS\ {Xi}. For 1) we need 678
to show that Xicannot be a hidden mediator or hidden confounder w.r.t. S\ {Xi}(since ignoring 679
Xiwon’t change whether there is a direct edge or not). Suppose Xiis on a unobserved causal path 680
Xk→ ··· → Um→XlwithXk, Xl∈S\ {Xi}andUm∈X\(S\ {Xi}). This path must have 681
been a unobserved causal path before, unless Xi=Um. But then there is a direct edge Xi→Xl. 682
We would not remove XifromSif this edge was unconfounded, so there must a hidden confounder 683
between XiandXl. But in this case, Proposition 3 wouldn’t allow us to direct the edge anyway, since 684
VPAl̸|=d
GUl. Suppose there is confounding path Xk← ··· → Um→XlwithXk, Xl∈S\ {Xi} 685
andUm∈X\(S\ {Xi}). IfXi̸=Umthe path was already been a confounding path without Xi 686
being unobserved. So again, there must be a confounder between XiandXl, as otherwise we would 687
not remove Xi. And analogously to before, we could not have oriented the edge even with Xi∈S 688
since VPAl̸|=d
GUl. For 2) we only have to see that we just remove nodes from Biif we found an 689
independence. 690
For|S|<2, the algorithm enters the final pruning stage. From the discussion above it is clear, 691
that we already have the correct result, up to potentially too many bidirected edges. In the final 692
step we certainly remove all these edges Xi↔Xj, as we check m-separation for all subsets of the 693
neighbourhoods Adj(Xi)andAdj(Xj), which are supersets of the true neighbourhoods. 694
695
21C.2 Finite sample version of AdaScore 696
All theoretical results in the paper have assumed that we know the density of our data. Obviously, in 697
practise we have to deal with a finite sample instead. Especially, in Proposition 1 and Proposition 3 698
we derived criteria that compare random variables with zero. Clearly, this condition is never met in 699
practise. Therefore, we need find ways to reasonably set thresholds for these random quantities. 700
First note, that we use the Stein gradient estimator [ 35] to estimate the score function. This means 701
especially that for a node Viwe get a vector 702

(∂
∂Vilogp(v))l
l=1,...,m, (24)
i.e. an estimate of the score for every one of the msamples. Analogously, we get a m×d×dtensor 703
for the estimates of∂2
∂Vi∂Vjlogp(v). 704
In Proposition 1 we showed that 705
∂2
∂Vi∂Vjlogp(vZ) = 0 ⇐⇒ Xi
|=m
MG
VVj|VZ\ {Vi, Vj}.
In the finite sample version, we use a one sample t-test on the vector of estimated cross-partial 706
derivatives with the null-hypothesis that the means is zero. Due to the central limit theorem, the 707
sample mean follows approximately a Gaussian distribution, regardless of the true distribution of the 708
observations. 709
For Proposition 3 we need to do some additional steps. Recall, that the relevant quantity in Propo- 710
sition 3 is the mean squared error of a regression, which is always positive. Therefore, a test for 711
mean zero is highly likely to reject in any case. We decided to employ a two-sample test in a similar 712
(but different) manner as Montagna et al. [17]. As test, we used the Mann-Whitney U-test. Note, 713
that Algorithm 2 employs Proposition 3 in two different ways: first, to decide whether there is an 714
unconfounded sink and second, to orient edges in case there is no unconfounded sink. We pick a 715
different sample as second sample of the Mann-Whitney U-test. 716
Analogously to before, this is a vector with mentries, one for every sample. 717
Note, that in the case where we want to check if there is an unconfounded sink, we do not make any 718
mistake by rejecting too few hypotheses, i.e. if we miss some unconfounded sinks (instead, we only 719
lose efficiency, as we do the costly iteration over all possible sets of parents). Therefore, for this test 720
we chose a a second sample that yields a “conservative” test result. 721
As candidate sink for set S⊆V, we pick the node Xi= min imean( δi(XS)). In fact, we want to 722
know whether the mean of δiis significantly lower than allother means. But we empirically observed 723
that choosing the concatenated δs of all nodes as second sample makes the test reject with very high 724
probability, which would lead our algorithm to falsely assume the existence of an unconfoudned sink. 725
Instead, we then pick as second “reference node” Xj= min j̸=imean( δj(XZ)). We then do the two 726
sample test between δi(XZ)andδj(XZ). The intuition is that the test will reject the hypothesis of 727
identical means, if Xiis an unconfounded sink but Xjis not. 728
In the case where we use Proposition 3 to orient edges, we only need to decide whether an not 729
previsouly directed edge Xi−Xjneeds to be oriented one way, the other way, or not at all. Instead, 730
here the issue lies in the fact that we need to iterate over possible sets of parents of the nodes. Let 731
Bibe the set of nodes that have not been m-separated from Xiby any test so far. We pick the 732
subset Zi= min Z′⊆Bimean( δZ′
i), i.e. the set with the lowest mean error. We then conduct the test 733
withδi(XZi)andδj(XZj). If there is a directed edge between them, one of the residuals will be 734
significantly lower than the other. 735
Just like Montagna et al. [17] we use a cross-validation scheme to generate the residuals, in order to 736
prevent overfitting. We split the dataset into several equally sized, disjoint subsamples. For every 737
residual we fit the regression on all subsamples that don’t contain the respective target. 738
Also, just like in the NoGAM algorithm Montagna et al. [17] we add a pruning step for the directed 739
edges to the end. The idea is to use a feature selection method to remove insignificant edges. Just like 740
Montagna et al. [17], we use the CAM-based pruning step proposed by Bühlmann et al. [36], which 741
fits a generalised additive regression model from the parents to a child and test whether one of the 742
22additive components is significantly non-zero. All parents for which the test rejects this hypothesis 743
are removed. 744
C.3 Complexity 745
Proposition 5. Complexity Let nbe the number of samples and dthe number of observable nodes. 746
Algorithm 2 runs in 747
Ω 
(d2−d)·(r(n, d) +s(n, d))
and O 
d2·2d(r(n, d) +s(n, d))
,
where r(n, d)is the time required to solve a regression problem and s(n, d)is the time for calculating 748
the score. With e.g. kernel-ridge regression and the Stein-estimator, both run in O(n3). 749
Proof. Algorithm 2 runs its main loop dtimes. It first checks for the existence of an unconfounded 750
sink, which involves solving 2dregression problems (including cross-validation prediction) and 751
calculating the score, adding up to (2d2−d)regressions and dscore evaluations. In the worst case, 752
we detect no unconfounded sink and iterate through all subsets of the neighbourhood of a node 753
(which is in the worst case of size d−1) and for all other nodes in the neighbourhood we solve 2d 754
regression problems and evaluate the score. For each subset we calculate two regression functions, 755
the score and calculate the entries in the Hessian of the log-density, i.e. d·2dregressions, d·2d−1756
scores and additionally 2d−1Hessians. If we are unlucky, this node has a directed outgoing edge 757
and we continue with this node (with the same size of nodes). This can happen d−1times. So we 758
get(d2−d)·2dregressions and (d2−d)·2d−1scores and Hessians. In the final pruning step we 759
calculate for every bidirected edge (of which there can be (d2−d)/2) a Hessian for all subsets of the 760
neighbourhoods, which can again be 2d−1subsets. Using the pruning procedure from CAM for the 761
directed edges we also spend at most O(nd3)steps. 762
In the best case, we always find an unconfounded sink. Then our algorithm reduces to NoGAM. 763
764
D Experimental details 765
In this section, we present the details of our experiments in terms of synthetic data generation and 766
algorithms hyperparameters. 767
D.1 Synthetic data generation 768
In this work, we rely on synthetic data to benchmark AdaScore’s finite samples performance. For 769
each dataset, we first sample the ground truth graph and then generate the observations according to 770
the causal graph. 771
Erdös-Renyi graphs. The ground truth graphs are generated according to the Erdös-Renyi model. 772
It allows specifying the number of nodes and the probability of connecting each pair of nodes). In ER 773
graphs, a pair of nodes has the same probability of being connected. 774
Nonlinear causal mechanisms. Nonlinear causal mechanisms are parametrized by a neural network 775
with random weights. We create a fully connected neural network with one hidden layer with 10 776
units, Parametric ReLU activation function, followed by one normalizing layer before the final fully 777
connected layer. The weights of the neural network are sampled from a standard Gaussian distribution. 778
This strategy for synthetic data generation is commonly adopted in the literature [ 26,18,28,29,27]. 779
Linear causal mechanisms. For the linear mechanisms, we define a simple linear regression model 780
predicting the effects from their causes and noise terms, weighted by randomly sampled coefficients. 781
Coefficients are generated as samples from a Uniform distribution supported in the range [−3,−0.5]∪ 782
[0.5,3]. We avoid too small coefficients to avoid close to unfaithful datasets Uhler et al. [24]. 783
23Noise terms distribution. The noise terms are sampled from a Uniform distribution supported 784
between −2and2. 785
Finally, we remark that we standardize the data by their empirical data. This is known to remove 786
shortcuts that allow finding a correct causal order sorting variables by their marginal variance, as in 787
varsortability , described in Reisach et al. [32], or sorting variables by the magnitude of their score 788
|∂Xilogp(X)|, a phenomenon known as scoresortability analyzed by Montagna et al. [18]. 789
D.2 AdaScore hyperparameters 790
For AdaScore, we set the αlevel for the required hypothesis testing at 0.05. For the CAM-pruning 791
step, the level is instead set at 0.001, the default value of the dodidscover Python implementation of 792
the method, and commonly found in all papers using CAM-pruning for edge selection [ 15,16,17,36]. 793
For the remaining parameters. The regression hyperparameters for the estimation of the residuals are 794
found via cross-validation during inference: tuning is done minimizing the generalization error on 795
the estimated residuals, without using the performance on the causal graph ground truth. Finally, for 796
the score matching estimation, the regularization coefficients are set to 0.001. 797
D.3 Computer resources 798
All experiments have been run on an AWS EC2 instance of type p3.2xlarge . These machines 799
contain Intel Xeon E5-2686-v4 processors with 2.3 GHz and 8 virtual cores as well as 61 GB RAM. 800
All experiments can be run within a day. 801
E Additional Experiments 802
In this section, we provide additional experimental results. All synthetic data has been generated as 803
described in Appendix D.1. 804
E.1 Non-additive mechanisms 805
In Figure 1 we have demonstrated the performance of our proposed method on data generated by 806
linear SCMs and non-linear SCMs with additive noise. But Proposition 1 also holds for anyfaithful 807
distribution generated by an acyclic model. Thus, we employed as mechanism a neural network-based 808
approach similar to the non-linear mechanism described in Appendix D. Instead of adding the noise 809
term, we feed it as additional input into the neural network. Results in this setting are reported in 810
Figure 2. As neither AdaScore nor any of the baseline algorithms has theoretical guarantees for the 811
orientation of edges in this scenario, we report the F1-score (popular in classification problems) w.r.t. 812
to the existence of an edge, regardless of orientation. Our experiments show that AdaScore can, in 813
general, correctly recover the graph’s skeleton in all the scenarios, with an F1score median between 814
1and∼0.75, respectively for small and large numbers of nodes. 815
E.2 Sparse graphs 816
In this section, we present the experiments on sparse Erdös-Renyi graphs where each pair of nodes 817
is connected by an edge with probability 0.3. The results are illustrated in Figure 3. For sparse 818
graphs, recovery results are similar to the dense case, with AdaScore generally providing comparable 819
performance to the other methods. 820
E.3 Increasing number of samples 821
In the following series of plots we demonstrate the scaling behaviour of our method w.r.t. to the 822
number of samples. Figure 5 shows results with edge probability 0.5 and Figure 4 with 0.3. All 823
graphs contain seven observable nodes. As before we observe that AdaScore performs comparably to 824
other methods. E.g. in Figures 4a and 5b we can see that the median error AdaScore improves with 825
additional samples and in all plots we see that no other algorithm seems to gain an advantage over 826
AdaScore with increasing sample size. 827
24adascore camuv nogam rcd lingam
3 5 7 9
number of nodes0.00.20.40.60.81.0skeleton_f1
sparse
3 5 7 9
number of nodes0.00.20.40.60.81.0skeleton_f1
dense(a) Fully observable model
3 5 7 9
number of nodes0.00.20.40.60.81.0skeleton_f1
sparse
3 5 7 9
number of nodes0.00.20.40.60.81.0skeleton_f1
dense
(b) Latent variables model
Figure 2: Empirical results for non-additive causal mechanisms on sparse graphs with different numbers of
nodes, on fully observable (no hidden variables) and latent variable models. We report the F1score w.r.t. the
existence of edges (the higher, the better).
adascore camuv nogam rcd lingam
3 5 7 9
number of nodes0246810121416shd
linear
3 5 7 9
number of nodes0510152025shd
nonlinear
(a) Fully observable model
3 5 7 9
number of nodes0510152025shd
linear
3 5 7 9
number of nodes0510152025shd
nonlinear
(b) Latent variables model
Figure 3: Empirical results on sparse graphs with different numbers of nodes, on fully observable (no hidden
variables) and latent variable models. We report the SHD accuracy (the lower, the better).
25adascore camuv nogam rcd lingam
500 1000 1500 2000
number of samples02468101214shd
linear
500 1000 1500 2000
number of samples0.02.55.07.510.012.515.017.520.0shd
nonlinear(a) Fully observable model
500 1000 1500 2000
number of samples0.02.55.07.510.012.515.017.520.0shd
linear
500 1000 1500 2000
number of samples0.02.55.07.510.012.515.017.520.0shd
nonlinear
(b) Latent variables model
Figure 4: Empirical results on sparse graphs with different numbers of samples and seven nodes, on fully
observable (no hidden variables) and latent variable models. We report the SHD accuracy (the lower, the better).
adascore camuv nogam rcd lingam
500 1000 1500 2000
number of samples0.02.55.07.510.012.515.017.520.0shdnonlinear
500 1000 1500 2000
number of samples0.02.55.07.510.012.515.017.5shdlinear
(a) Fully observable model
500 1000 1500 2000
number of samples0.02.55.07.510.012.515.017.520.0shdnonlinear
500 1000 1500 2000
number of samples0.02.55.07.510.012.515.017.520.0shd
linear
(b) Latent variables model
Figure 5: Empirical results on dense graphs with different numbers of samples and seven nodes, on fully
observable (no hidden variables) and latent variable models. We report the SHD accuracy (the lower, the better).
26E.4 Limitations 828
In this section, we remark the limitations of our empirical study. It is well known that causal discovery 829
lacks meaningful, multivariate benchmark datasets with known ground truth. For this reason, it is 830
common to rely on synthetically generated datasets. We believe that results on synthetic graphs should 831
be taken with care, as there is no strong reason to believe that they should mirror the benchmarked 832
algorithms’ behaviors in real-world settings, where often there is no prior knowledge about the 833
structural causal model underlying available observations. 834
27NeurIPS Paper Checklist 835
The checklist is designed to encourage best practices for responsible machine learning research, 836
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove 837
the checklist: The papers not including the checklist will be desk rejected. The checklist should 838
follow the references and precede the (optional) supplemental material. The checklist does NOT 839
count towards the page limit. 840
Please read the checklist guidelines carefully for information on how to answer these questions. For 841
each question in the checklist: 842
• You should answer [Yes] , [No] , or [NA] . 843
•[NA] means either that the question is Not Applicable for that particular paper or the 844
relevant information is Not Available. 845
• Please provide a short (1–2 sentence) justification right after your answer (even for NA). 846
The checklist answers are an integral part of your paper submission. They are visible to the 847
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it 848
(after eventual revisions) with the final version of your paper, and its final version will be published 849
with the paper. 850
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. 851
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a 852
proper justification is given (e.g., "error bars are not reported because it would be too computationally 853
expensive" or "we were unable to find the license for the dataset we used"). In general, answering 854
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we 855
acknowledge that the true answer is often more nuanced, so please just use your best judgment and 856
write a justification to elaborate. All supporting evidence can appear either in the main paper or the 857
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification 858
please point to the section(s) where related material for the question can be found. 859
1.Claims 860
Question: Do the main claims made in the abstract and introduction accurately reflect the 861
paper’s contributions and scope? 862
Answer: [Yes] 863
Justification: In the abstract, we claim that we connect the properties of the score function 864
to causal structure learning. In the paper, particularly sections 3 and 4, we present the 865
theoretical results supporting our claim. Further, in the abstract we mention that based on 866
our theory we propose an algorithm for causal discovery from score matching estimation, 867
algorithm that we define in Section 4.3 and we empirically validate in Section 5 and 868
Appendix E. 869
Guidelines: 870
•The answer NA means that the abstract and introduction do not include the claims 871
made in the paper. 872
•The abstract and/or introduction should clearly state the claims made, including the 873
contributions made in the paper and important assumptions and limitations. A No or 874
NA answer to this question will not be perceived well by the reviewers. 875
•The claims made should match theoretical and experimental results, and reflect how 876
much the results can be expected to generalize to other settings. 877
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 878
are not attained by the paper. 879
2.Limitations 880
Question: Does the paper discuss the limitations of the work performed by the authors? 881
Answer: [Yes] 882
Justification: The main limitation of our work is on the experimental side: our experiments 883
are limited to synthetic data, which are not an ideal probing ground. Additionally, our 884
28method does not provide performance that clearly improves on the existing literature. These 885
limitations of our work are discussed in the discussion of the experiments in Section 5, as 886
well as in the "Limitations" appendix section E.4. Concerning the assumptions required by 887
our method, we thoroughly discuss them in the theoretical sections of the paper, where we 888
define the results that are later used for the definition of the AdaScore. Finally, computational 889
complexity is discussed in Appendix C.3. 890
Guidelines: 891
•The answer NA means that the paper has no limitation while the answer No means that 892
the paper has limitations, but those are not discussed in the paper. 893
• The authors are encouraged to create a separate "Limitations" section in their paper. 894
•The paper should point out any strong assumptions and how robust the results are to 895
violations of these assumptions (e.g., independence assumptions, noiseless settings, 896
model well-specification, asymptotic approximations only holding locally). The authors 897
should reflect on how these assumptions might be violated in practice and what the 898
implications would be. 899
•The authors should reflect on the scope of the claims made, e.g., if the approach was 900
only tested on a few datasets or with a few runs. In general, empirical results often 901
depend on implicit assumptions, which should be articulated. 902
•The authors should reflect on the factors that influence the performance of the approach. 903
For example, a facial recognition algorithm may perform poorly when image resolution 904
is low or images are taken in low lighting. Or a speech-to-text system might not be 905
used reliably to provide closed captions for online lectures because it fails to handle 906
technical jargon. 907
•The authors should discuss the computational efficiency of the proposed algorithms 908
and how they scale with dataset size. 909
•If applicable, the authors should discuss possible limitations of their approach to 910
address problems of privacy and fairness. 911
•While the authors might fear that complete honesty about limitations might be used by 912
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 913
limitations that aren’t acknowledged in the paper. The authors should use their best 914
judgment and recognize that individual actions in favor of transparency play an impor- 915
tant role in developing norms that preserve the integrity of the community. Reviewers 916
will be specifically instructed to not penalize honesty concerning limitations. 917
3.Theory Assumptions and Proofs 918
Question: For each theoretical result, does the paper provide the full set of assumptions and 919
a complete (and correct) proof? 920
Answer: [Yes] 921
Justification: All our theoretical results make explicit the assumptions for which they are 922
valid. Plus, we in section Appendix B we provide the proofs of our theoretical results. 923
Guidelines: 924
• The answer NA means that the paper does not include theoretical results. 925
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 926
referenced. 927
•All assumptions should be clearly stated or referenced in the statement of any theorems. 928
•The proofs can either appear in the main paper or the supplemental material, but if 929
they appear in the supplemental material, the authors are encouraged to provide a short 930
proof sketch to provide intuition. 931
•Inversely, any informal proof provided in the core of the paper should be complemented 932
by formal proofs provided in appendix or supplemental material. 933
• Theorems and Lemmas that the proof relies upon should be properly referenced. 934
4.Experimental Result Reproducibility 935
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 936
perimental results of the paper to the extent that it affects the main claims and/or conclusions 937
of the paper (regardless of whether the code and data are provided or not)? 938
29Answer: [Yes] 939
Justification: In Appendix D, we provide all the details to reproduce the data generation of 940
our experiments and the hyperparameters used in AdaScore for our experimental runs. 941
Guidelines: 942
• The answer NA means that the paper does not include experiments. 943
•If the paper includes experiments, a No answer to this question will not be perceived 944
well by the reviewers: Making the paper reproducible is important, regardless of 945
whether the code and data are provided or not. 946
•If the contribution is a dataset and/or model, the authors should describe the steps taken 947
to make their results reproducible or verifiable. 948
•Depending on the contribution, reproducibility can be accomplished in various ways. 949
For example, if the contribution is a novel architecture, describing the architecture fully 950
might suffice, or if the contribution is a specific model and empirical evaluation, it may 951
be necessary to either make it possible for others to replicate the model with the same 952
dataset, or provide access to the model. In general. releasing code and data is often 953
one good way to accomplish this, but reproducibility can also be provided via detailed 954
instructions for how to replicate the results, access to a hosted model (e.g., in the case 955
of a large language model), releasing of a model checkpoint, or other means that are 956
appropriate to the research performed. 957
•While NeurIPS does not require releasing code, the conference does require all submis- 958
sions to provide some reasonable avenue for reproducibility, which may depend on the 959
nature of the contribution. For example 960
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 961
to reproduce that algorithm. 962
(b)If the contribution is primarily a new model architecture, the paper should describe 963
the architecture clearly and fully. 964
(c)If the contribution is a new model (e.g., a large language model), then there should 965
either be a way to access this model for reproducing the results or a way to reproduce 966
the model (e.g., with an open-source dataset or instructions for how to construct 967
the dataset). 968
(d)We recognize that reproducibility may be tricky in some cases, in which case 969
authors are welcome to describe the particular way they provide for reproducibility. 970
In the case of closed-source models, it may be that access to the model is limited in 971
some way (e.g., to registered users), but it should be possible for other researchers 972
to have some path to reproducing or verifying the results. 973
5.Open access to data and code 974
Question: Does the paper provide open access to the data and code, with sufficient instruc- 975
tions to faithfully reproduce the main experimental results, as described in supplemental 976
material? 977
Answer: [Yes] 978
Justification: We provide the code for the experiments and the data generation in a zip file. 979
Further, we describe all the details for reproducing our experimental results in Appendix D. 980
Guidelines: 981
• The answer NA means that paper does not include experiments requiring code. 982
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 983
public/guides/CodeSubmissionPolicy ) for more details. 984
•While we encourage the release of code and data, we understand that this might not be 985
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 986
including code, unless this is central to the contribution (e.g., for a new open-source 987
benchmark). 988
•The instructions should contain the exact command and environment needed to run to 989
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 990
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 991
•The authors should provide instructions on data access and preparation, including how 992
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 993
30•The authors should provide scripts to reproduce all experimental results for the new 994
proposed method and baselines. If only a subset of experiments are reproducible, they 995
should state which ones are omitted from the script and why. 996
•At submission time, to preserve anonymity, the authors should release anonymized 997
versions (if applicable). 998
•Providing as much information as possible in supplemental material (appended to the 999
paper) is recommended, but including URLs to data and code is permitted. 1000
6.Experimental Setting/Details 1001
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 1002
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 1003
results? 1004
Answer: [Yes] 1005
Justification: In our experiments section 5, we present all the necessary details on the 1006
data generation procedure and a description of the empirical results that are necessary for 1007
understanding our findings. Additionally, a comprehensive overview of our experimental 1008
design is presented in Appendix D. 1009
Guidelines: 1010
• The answer NA means that the paper does not include experiments. 1011
•The experimental setting should be presented in the core of the paper to a level of detail 1012
that is necessary to appreciate the results and make sense of them. 1013
•The full details can be provided either with the code, in appendix, or as supplemental 1014
material. 1015
7.Experiment Statistical Significance 1016
Question: Does the paper report error bars suitably and correctly defined or other appropriate 1017
information about the statistical significance of the experiments? 1018
Answer: [Yes] 1019
Justification: We report all our experimental results in the form of boxplots. 1020
Guidelines: 1021
• The answer NA means that the paper does not include experiments. 1022
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 1023
dence intervals, or statistical significance tests, at least for the experiments that support 1024
the main claims of the paper. 1025
•The factors of variability that the error bars are capturing should be clearly stated (for 1026
example, train/test split, initialization, random drawing of some parameter, or overall 1027
run with given experimental conditions). 1028
•The method for calculating the error bars should be explained (closed form formula, 1029
call to a library function, bootstrap, etc.) 1030
• The assumptions made should be given (e.g., Normally distributed errors). 1031
•It should be clear whether the error bar is the standard deviation or the standard error 1032
of the mean. 1033
•It is OK to report 1-sigma error bars, but one should state it. The authors should 1034
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 1035
of Normality of errors is not verified. 1036
•For asymmetric distributions, the authors should be careful not to show in tables or 1037
figures symmetric error bars that would yield results that are out of range (e.g. negative 1038
error rates). 1039
•If error bars are reported in tables or plots, The authors should explain in the text how 1040
they were calculated and reference the corresponding figures or tables in the text. 1041
8.Experiments Compute Resources 1042
Question: For each experiment, does the paper provide sufficient information on the com- 1043
puter resources (type of compute workers, memory, time of execution) needed to reproduce 1044
the experiments? 1045
31Answer: [Yes] 1046
Justification: Details on the computer resources required for the experiments can be found 1047
in Appendix D.3. 1048
Guidelines: 1049
• The answer NA means that the paper does not include experiments. 1050
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 1051
or cloud provider, including relevant memory and storage. 1052
•The paper should provide the amount of compute required for each of the individual 1053
experimental runs as well as estimate the total compute. 1054
•The paper should disclose whether the full research project required more compute 1055
than the experiments reported in the paper (e.g., preliminary or failed experiments that 1056
didn’t make it into the paper). 1057
9.Code Of Ethics 1058
Question: Does the research conducted in the paper conform, in every respect, with the 1059
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 1060
Answer: [Yes] 1061
Justification: We do not believe any of the concerns in the Code of Ethics apply to our work. 1062
Guidelines: 1063
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 1064
•If the authors answer No, they should explain the special circumstances that require a 1065
deviation from the Code of Ethics. 1066
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 1067
eration due to laws or regulations in their jurisdiction). 1068
10.Broader Impacts 1069
Question: Does the paper discuss both potential positive societal impacts and negative 1070
societal impacts of the work performed? 1071
Answer: [NA] 1072
Justification: In this work, we present a novel causal discovery method from observational 1073
data. We believe there are no specific negative societal impacts, while positive impacts are 1074
those generally recognized to causal discovery, as discussed in the Introduction section 1. 1075
Guidelines: 1076
• The answer NA means that there is no societal impact of the work performed. 1077
•If the authors answer NA or No, they should explain why their work has no societal 1078
impact or why the paper does not address societal impact. 1079
•Examples of negative societal impacts include potential malicious or unintended uses 1080
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 1081
(e.g., deployment of technologies that could make decisions that unfairly impact specific 1082
groups), privacy considerations, and security considerations. 1083
•The conference expects that many papers will be foundational research and not tied 1084
to particular applications, let alone deployments. However, if there is a direct path to 1085
any negative applications, the authors should point it out. For example, it is legitimate 1086
to point out that an improvement in the quality of generative models could be used to 1087
generate deepfakes for disinformation. On the other hand, it is not needed to point out 1088
that a generic algorithm for optimizing neural networks could enable people to train 1089
models that generate Deepfakes faster. 1090
•The authors should consider possible harms that could arise when the technology is 1091
being used as intended and functioning correctly, harms that could arise when the 1092
technology is being used as intended but gives incorrect results, and harms following 1093
from (intentional or unintentional) misuse of the technology. 1094
•If there are negative societal impacts, the authors could also discuss possible mitigation 1095
strategies (e.g., gated release of models, providing defenses in addition to attacks, 1096
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 1097
feedback over time, improving the efficiency and accessibility of ML). 1098
3211.Safeguards 1099
Question: Does the paper describe safeguards that have been put in place for responsible 1100
release of data or models that have a high risk for misuse (e.g., pretrained language models, 1101
image generators, or scraped datasets)? 1102
Answer: [NA] 1103
Justification: We release a causal discovery model from observational data which does not 1104
poses such risks. 1105
Guidelines: 1106
• The answer NA means that the paper poses no such risks. 1107
•Released models that have a high risk for misuse or dual-use should be released with 1108
necessary safeguards to allow for controlled use of the model, for example by requiring 1109
that users adhere to usage guidelines or restrictions to access the model or implementing 1110
safety filters. 1111
•Datasets that have been scraped from the Internet could pose safety risks. The authors 1112
should describe how they avoided releasing unsafe images. 1113
•We recognize that providing effective safeguards is challenging, and many papers do 1114
not require this, but we encourage authors to take this into account and make a best 1115
faith effort. 1116
12.Licenses for existing assets 1117
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 1118
the paper, properly credited and are the license and terms of use explicitly mentioned and 1119
properly respected? 1120
Answer: [Yes] 1121
Justification: We use our proprietary assets, as well as public assets available under MIT 1122
license, which we correctly cite in our work. 1123
Guidelines: 1124
• The answer NA means that the paper does not use existing assets. 1125
• The authors should cite the original paper that produced the code package or dataset. 1126
•The authors should state which version of the asset is used and, if possible, include a 1127
URL. 1128
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 1129
•For scraped data from a particular source (e.g., website), the copyright and terms of 1130
service of that source should be provided. 1131
•If assets are released, the license, copyright information, and terms of use in the 1132
package should be provided. For popular datasets, paperswithcode.com/datasets 1133
has curated licenses for some datasets. Their licensing guide can help determine the 1134
license of a dataset. 1135
•For existing datasets that are re-packaged, both the original license and the license of 1136
the derived asset (if it has changed) should be provided. 1137
•If this information is not available online, the authors are encouraged to reach out to 1138
the asset’s creators. 1139
13.New Assets 1140
Question: Are new assets introduced in the paper well documented and is the documentation 1141
provided alongside the assets? 1142
Answer: [Yes] 1143
Justification: We release the code for AdaScore, submitted in the form of a zip file containing 1144
the necessary documentation for usage. Moreover, an extensive description of the data 1145
generation is provided in the paper, as well as a description of the method itself. 1146
Guidelines: 1147
• The answer NA means that the paper does not release new assets. 1148
33•Researchers should communicate the details of the dataset/code/model as part of their 1149
submissions via structured templates. This includes details about training, license, 1150
limitations, etc. 1151
•The paper should discuss whether and how consent was obtained from people whose 1152
asset is used. 1153
•At submission time, remember to anonymize your assets (if applicable). You can either 1154
create an anonymized URL or include an anonymized zip file. 1155
14.Crowdsourcing and Research with Human Subjects 1156
Question: For crowdsourcing experiments and research with human subjects, does the paper 1157
include the full text of instructions given to participants and screenshots, if applicable, as 1158
well as details about compensation (if any)? 1159
Answer: [NA] 1160
Justification: We do not work with human subjects or crowdsourcing. 1161
Guidelines: 1162
•The answer NA means that the paper does not involve crowdsourcing nor research with 1163
human subjects. 1164
•Including this information in the supplemental material is fine, but if the main contribu- 1165
tion of the paper involves human subjects, then as much detail as possible should be 1166
included in the main paper. 1167
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 1168
or other labor should be paid at least the minimum wage in the country of the data 1169
collector. 1170
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 1171
Subjects 1172
Question: Does the paper describe potential risks incurred by study participants, whether 1173
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 1174
approvals (or an equivalent approval/review based on the requirements of your country or 1175
institution) were obtained? 1176
Answer: [NA] 1177
Justification: We do not work with human subjects or crowdsourcing. 1178
Guidelines: 1179
•The answer NA means that the paper does not involve crowdsourcing nor research with 1180
human subjects. 1181
•Depending on the country in which research is conducted, IRB approval (or equivalent) 1182
may be required for any human subjects research. If you obtained IRB approval, you 1183
should clearly state this in the paper. 1184
•We recognize that the procedures for this may vary significantly between institutions 1185
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 1186
guidelines for their institution. 1187
•For initial submissions, do not include any information that would break anonymity (if 1188
applicable), such as the institution conducting the review. 1189
34