ExID: Offline RL with Intuitive Expert Insights in
Limited-Data Settings
Anonymous Author(s)
Affiliation
Address
email
Abstract
With the ability to learn from static datasets, Offline Reinforcement Learning (RL) 1
emerges as a compelling avenue for real-world applications. However, state-of-the- 2
art offline RL algorithms perform sub-optimally when confronted with limited data 3
confined to specific regions within the state space. The performance degradation 4
is attributed to the inability of offline RL algorithms to learn appropriate actions 5
for rare or unseen observations. This paper proposes a novel domain knowledge- 6
based regularization technique and adaptively refines the initial domain knowledge 7
to considerably boost performance in limited data with partially omitted states. 8
The key insight is that the regularization term mitigates erroneous actions for 9
sparse samples and unobserved states covered by domain knowledge. Empirical 10
evaluations on standard discrete environment datasets demonstrate a substantial 11
average performance increase compared to ensemble of domain knowledge and 12
existing offline RL algorithms operating on limited data. 13
1 Introduction 14
Offline RL [ 9,1], also referred to as batch RL, is a learning approach that focuses on extracting 15
knowledge solely from static datasets. This class of algorithms has a wider range of applications being 16
particularly appealing to real-world data sets from business [ 46], healthcare [ 25], and robotics [ 35]. 17
However, offline RL poses unique challenges, including over-fitting and the need for generalization 18
to data not present in the dataset. To surpass the behavior policy, offline RL algorithms need to 19
query Q values of actions not in the dataset, causing extrapolation errors [ 21]. Most offline RL 20
algorithms address this problem by enforcing constraints that ensure that the learned policy does not 21
deviate too far away from the data set’s state action distribution [ 13,11] or is conservative towards 22
Out-of-Distribution (OOD) actions [ 21,20]. However, such approaches are designed on coherent 23
batches [13], which do not account for OOD states. 24
In many domains, such as business and healthcare, available data is scarce and often confined to expert 25
behaviors within a limited state space. For example, a sales recommendation system, where historic 26
data may not contain details about many active users and operator gives coupon of higher value to 27
attract sales. Learning on such limited data sets can curtail the generalization capabilities of state-of- 28
the-art (SOTA) offline RL algorithms, resulting in sub-optimal performance [ 23]. We illustrate this 29
limitation via Fig 1. In Fig 1a) the state action space of a simple Mountain Car environment [ 27] is 30
plotted for an expert dataset [ 32] and a partial dataset with first 10% samples from the entire dataset. 31
Fig 1b) shows the average reward obtained over these data sets and the average difference between 32
the Q value of action taken by the under-performing Conservative Q Learning (CQL) [ 21] agent and 33
the action in the full expert dataset for unseen states. It can be observed that the performance of the 34
offline RL agent considerably drops. This is attributed to the critic overestimating the Q value of 35
non-optimal actions for states that do not occur in the dataset while training. 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Figure 1: a) Full expert, Mountain Car dataset, and reduced dataset with first 10% samples showing
distribution of state (position, velocity) and action b) CQL agent converging to a sub-optimal policy
for reduced dataset exhibiting high Q values for actions different from actions in the expert dataset
for unseen states.
In numerous real-world applications, expert insights regarding the general behavior of a policy are 37
often accessible [ 33].For example, sales operators often distribute lower discount coupons to active 38
users to maximize profit . While these insights may not be optimal, they serve as valuable guidelines 39
for understanding the overall behavior of the policy. A rich literature in knowledge distillation [ 18] 40
has shown that teacher networks trained on domain knowledge can transfer knowledge to another 41
network unaware of it. This work aims to leverage a teacher network mimicking simple decision 42
tree-based domain knowledge to help offline RL generalize in limited data settings. 43
The paper makes the following novel contributions: 44
•We introduce an algorithm dubbed ExID , leveraging intuitive human obtainable expert 45
insights. The domain expertise is incorporated into a teacher policy, which improves offline 46
RL in limited-data settings through regularization. 47
•The teacher based on expected performance improvement of the offline policy during 48
training, improving the teacher network beyond initial heuristics. 49
•We demonstrate the effectiveness of our methodology on real sales promotion dataset , 50
several discrete OpenAI gym and Minigrid environments with standard offline RL data sets 51
and show that ExID significantly exceeds the performance when faced with limited data. 52
2 Related Work 53
This work improves offline RL learning on batches sampled from static datasets using domain 54
expertise. One of the major concerns in offline RL is the erroneous extrapolation of OOD actions 55
[13]. Two techniques have been studied in the literature to prevent such errors. 1) Constraining the 56
policy to be close to the behavior policy 2) Penalizing overly optimistic Q values [ 24]. We discuss a 57
few relevant algorithms following these principles. In Batch-Constrained deep Q-learning (BCQ) 58
[13] candidate actions sampled from an adversarial generative model are considered, aiming to 59
balance proximity to the batch while enhancing action diversity. Algorithms like Random Ensemble 60
Mixture Model (REM) [ 2], Ensemble-Diversified Actor-Critic (EDAC) [ 3] and Uncertainty Weighted 61
Actor-Critic (UWAC) [ 42] penalize the Q value according to uncertainty by either using Q ensemble 62
networks or directly weighting the loss with uncertainty. CQL [ 21] enforces regularization on Q- 63
functions by incorporating a term that reduces Q-values for OOD actions while increasing Q-values 64
for actions within the expected distribution. However, these algorithms do not handle OOD actions 65
for states not in the static dataset and can have errors induced by changes in transition probability. 66
Integration of domain knowledge in offline RL, though an important avenue, has not yet been 67
2extensively explored. Domain knowledge incorporation has improved online RL with tight regret 68
bounds [ 33,4]. In offline RL, bootstrapping via blending heuristics computed using Monte-Carlo 69
returns with rewards has shown to outperform SOTA algorithms by 9% [ 15]. Recent works improve 70
offline RL by incorporating a safety expert [ 40] and preference query [ 44], contrary to our work 71
which improves imperfect domain knowledge. The closest to our work is Domain Knowledge guided 72
Q learning (DKQ) [ 46] where domain knowledge is represented in terms of action importance and 73
the Q value is weighted according to importance. However, obtaining action importance in practical 74
scenarios is nontrivial. 75
3 Preliminaries 76
A DRL setting is represented by a Markov Decision Process (MDP) formalized as (S, A, T, r, ρ 0, γ). 77
Here, Sdenotes the state space, Asignifies the action space, T(s′|s, a)represents the transition prob- 78
ability distribution, r:S×A→Ris the reward function, ρ0represents the initial state distribution, 79
andγ∈(0,1]is the discount factor. The primary objective of any DRL algorithm is to identify an 80
optimal policy π(a|s)that maximizes Est,at[P∞
t=0γtr(st, at)]where, s0∼d0(.), at∼π(.|st),and 81
s′∼T(.|st, at). Deep Q networks (DQNs) [ 26] learn this objective by minimizing the Bellman resid- 82
ual(Qθ(s, a)−BπθQθ(s, a))2where BπθQθ(s, a) =Es′∼T[r(s, a) +γEa′∼πθ(.|s′)[Qθ′(s′, a′)]]. 83
The policy πθchooses actions that maximize the Q value max a′∈AQθ(s′, a′). However, in offline 84
RL where transitions are sampled from a pre-collected dataset B, the chosen action a′may exhibit a 85
bias towards OOD actions with inaccurately high Q-values. To handle the erroneous propagation 86
from OOD actions, CQL [ 22] learns conservative Q values by penalizing OOD actions. The CQL 87
loss for discrete action space is given by 88
Lcql(θ) = min
QαEs∼B[logX
aexp(Qθ(s, a))−
Ea∼B|s[Qθ(s, a)]] +1
2Es,a,s′∼B[Qθ−Qθ′]2(1)
Eq. 1 encourages the policy to be close to the actions seen in the dataset. However, CQL works on the 89
assumption of coherent batches, i.e., if (s, a, s′)∈ B, then s′∈ B. There is no provision for handling 90
OOD actions for s /∈ B, which can lead to policy failure when data is limited. In the next sections, we 91
present ExID, a domain knowledge-based approach to improve performance in data-scarce scenarios. 92
4 Problem Setting and Methodology 93
In our problem setting, the RL agent learns the policy on a limited dataset with rare and unseen 94
demonstrations. We define the characteristics of this dataset as follows: 95
Definition 4.1. A reduced buffer Bris a proper subset of the full dataset Bi.e.,Br⊂ B satisfying 96
the following conditions: 97
• Some states in Bare not present in Br, i.e.,∃s′∈ B ∧ ∀ (s, a, s′) : (s, a, s′)/∈ Br 98
•The number of samples N(s, a, s′)for some transitions in Bare less in Bri.e,∃(s, a, s′)∈ 99
B:N(s, a, s′)Br< N(s, a, s′)B 100
We observe, performing Q−Learning by sampling from a limited buffer Brmay not converge 101
to an optimal policy for the MDP MBrepresenting the full buffer. This can be shown as a special 102
case of (Theorem 1,[ 13]) aspB(s′|s, a)̸=pBr(s′|s, a)and no Q updates for (s, a)/∈ Brleading to 103
sub-optimal policy. Please refer to the App. B for analysis and example. 104
We also assume a set of common sense rules in the form of domain knowledge is available. Domain 105
knowledge Dis defined as hierarchical decision nodes capturing S→Aas represented by Eq. 2. 106
Each decision node Tηiis represented by a constraint ϕηiand Boolean indicator µηifunction selects 107
the branch to be traversed based on ϕηi. 108
Action =
aηi ifleaf
µηiTηi↙(s) + (1 −µηi)Tηi↘(s) o/w
µηi(s) =
1ifs|=ϕηi
0 o/w(2)
3Figure 2: Overview of the proposed methodology (a) Training a teacher policy network with domain
knowledge and synthetic data (b) Updating the offline RL critic network with teacher network
We assume that Dgives heuristically reasonable actions for s|=DandSD∩SBr̸=∅where SD, SBr109
are the state coverage of DandBr. 110
Training Teacher: An overview of our methodology is depicted in Fig 2. We first construct a 111
trainable actor network πω
tparameterized by ωfromD, Fig 2 step 1. For training πω
tsynthetic 112
data ˆSis generated by sampling states from a uniform random distribution over state boundaries 113
B(s),ˆS=U(B(S)). Note that this does not represent the true state distribution and may have state 114
combinations that will never occur. We train πω
tusing behavior cloning where state ˆs∼ˆSis checked 115
with root decision node in Eq. 2. A random action is chosen if ˆsdoes not satisfy decision node Tη0116
or leaf action is absent. If ˆssatisfies a Tηi,Tηiis traversed and action aηiis returned from the leaf 117
node. This is illustrated in Fig 2 (a). We term the pre-trained actor network πω
tas the teacher policy. 118
Regularizing Critic: We now introduce Algo 1 (App C) to train an offline RL agent on Br. Algo 1 119
takesBrand pretrained πω
tas input. The algorithm uses two hyper-parameters, warm start parameter 120
kand mixing parameter λ. A critic network Qθ
swith Monte-Carlo (MC) dropout and target network 121
Qθ′
sare initialized. ExID is divided into two phases. In the first phase, we aim to warm start the critic 122
network Qθ
swith actions from πω
tas shown in Fig 2b( i). However, this must be done selectively 123
as the teacher’s policy is random around the states that do not satisfy domain knowledge. In each 124
iteration, we first check the states sampled from a mini-batch of BrwithD. For the states which 125
satisfy Dwe compute the teacher action πω
t(s)and critic’s action argmaxa(Qθ
s(s, a))and collect it 126
in lists at, as, Algo 1 lines 4-10. Our main objective is to keep actions chosen by the critic network 127
fors|=Dclose to the teacher’s policy. To achieve this, we introduce a regularization term: 128
Lr(θ) = Es∼Br∧s|=D|{z}
states matching domain rule[Qθ
s(s, as)−Qθ
s(s, at)]2
| {z }
Q regularizer(3)
Eq 3 incentivizes the critic to increase Q values for actions from πω
tand decreases Q values for other 129
actions when argmaxa(Qθ
s(s, a))̸=πω
t(s)for states that satisfy domain knowledge. Note that Eq 3 130
will only be 0 when argmaxa(Qθ
s(s, a)) =πω
t(s)fors|=D. It is also set to 0 for s̸|=D. However, 131
since πω
tmimicking heuristic rules is sub-optimal, it is also important to incorporate learning from 132
the data. The final loss is a combination of Eq. 1 and Eq. 3 with a mixing parameter λ∈[0,1]: 133
4L(θ) =Lcql(θ) +λEs∼Br∧s|=D[Qθ
s(s, as)−Qθ
s(s, at)]2(4)
The choice of λand the warm start parameter kdepends on the quality of D. In the case of perfect 134
domain knowledge, λwould be set to 1, and setting λto 0 would lead to the vanilla CQL loss. Mixing 135
both the losses allows the critic to learn both from the data in Brand knowledge in D. 136
Updating Teacher: Given a reasonable warm start, the critic is expected to give higher Q values 137
for optimal actions for s∈ D ∩ B ras it learns from data. We aim to leverage this knowledge 138
to enhance the initial teacher policy πω
ttrained on heuristic domain knowledge. For s∼ B and 139
s|=D, we calculate the average Q values over critic actions and teacher actions and check which 140
one is higher in Algo 1 line 11 which refers to Cond. 6. For brevity Es∼Br∧s|=Dis written as E. 141
IfE(Qθ
s(s, as))>E(Qθ
s(s, at))it denotes the critic expects a better return on an average over its 142
own policy than the teacher’s policy. Hence, we can use the critic’s policy to update πω
t, making 143
it better over D. However, only checking the critic’s value can be erroneous as the critic can have 144
high values for OOD actions. We check the average uncertainty of the predicted Q values to prevent 145
the teacher from getting updated by OOD actions. Uncertainty has been shown to be a good metric 146
for OOD action detection by [ 42,3]. A well-established methodology to capture uncertainty is 147
predictive variance, which takes inspiration from Bayesian formulation for the critic function and 148
aims to maximize p(θ|X, Y) =p(Y|X, θ)p(θ)/p(Y|X)where X= (s, a)andYrepresents the true 149
Q value of the states. However, p(Y|X)is generally intractable and is approximated using Monte 150
Carlo (MC) dropout, which involves including dropout before every layer of the critic network and 151
using it during inference [14]. Following [42], we measure the uncertainty of prediction using Eq 5. 152
V arT[Q(s, a)]≈1
TTX
t=1[Q(s, a)−¯Q(s, a)]2(5)
Eq 5 estimates the variance of Q value Q(s, a)for an action ausing Tforward passes on the Qθ
s(s, a) 153
with dropout where ¯Q(s, a)represents the predictive mean. We check the average uncertainty of 154
the Q value for action chosen by the critic and teacher policy over the states that match domain 155
knowledge in a batch. The teacher network is updated using the critic’s action only when the policy 156
expects a higher average Q return on its action and the average uncertainty of taking this action is 157
lower than the teacher action. E(V arTQθ
s(sr, as))<E(V arTQθ
s(sr, at))indicates the actions were 158
learned from the expert data in the buffer and are not OOD samples. The condition is summarized in 159
cond. 6: 160
E(Qθ
s(sr, as))>E(Qθ
s(sr, at))∧
E(V arTQθ
s(sr, as))<E(V arTQθ
s(sr, at)) (6)
We update the teacher with cross-entropy described in Eq 7: 161
L(ω) =−X
s|=D(πω
t(s)log(πs(s))) (7)
where, πs(s, a) =eQ(s,a)P
a′Q(s,a′). When the critic’s policy is better than the teacher’s policy, Lr(θ)is 162
set to 0 Algo 1 Lines 11 to 13. Finally, the critic network is updated using calculated loss L(θ)Algo 163
1 Lines 17-18. We theoretically analyse the implications of using ExID in propositions 4.2 and 4.3. 164
Proposition 4.2. Denote ˆπas the policy learned by ExID, πuas any offline RL policy learned on Br 165
and optimal Q function as Q∗and V function as V∗. Then it holds that 166
η(ˆπ)−η(πu)≥Es∼O|πu[V∗(s)−Q∗(s, πu(s))]−¯ρˆπα
Where α=Es∼O[V∗(s)−Q∗(s,ˆπ(s))],¯ρπ(s) = [1
|Sˆπ|(1−γ),1
1−γ](|Sˆπ|is the number of different 167
states observed by ˆπ) andO /∈ Br. Here αdenotes the quality of regularized action for s /∈ Br. Hence, 168
updating πω
tis important as high divergence of action from the optimal can lead to performance 169
degradation. In offline RL, the extrapolation error for non optimal action is usually high for states not 170
observed in dataset (as illustrated in 1b), regularization can lead performance improvement when πω
t171
is reasonable. Furthermore, in ExID coarse actions from πω
tare updated driving them closer to the 172
optimal actions, improving the performance lower bound. Additionally πω
tincreases |Sˆπ|making 173
¯ρπ≪1in practice further improving the performance lower bound. Proof is deferred to App. A . 174
5Proposition 4.3. ExID reduces generalization error if Q∗(s, πω
t(s))> Q∗(s, πu(s))fors∈ D ∩B r. 175
Proof is deferred to App. A . In the next section, we discuss our empirical evaluations. 176
5 Empirical Evaluations 177
We investigate the following through our empirical evaluations: 1. Does ExID perform better than 178
combining Dand offline RL algos on different environments with datasets exhibiting rare and OOD 179
states Sec 5.2? 2. Does ExID generalize to OOD states covered by DSec 5.4? 3. What is the effect of 180
varying k,λand updating πω
tSec 5.5? 4. How does performance vary with the quality of DSec 5.6? 181
5.1 Experimental Setting 182
We evaluate our methodology on open-AI gym [ 5], MiniGrid [ 6] and real sales promotion (SP) [ 30] 183
offline data sets. All our data sets are generated using standard methodologies defined in [ 32,31] 184
except SP which is generated by human operators. All experiments have been conducted on a 185
Ubuntu 22.04.2 LTS system with 1 NVIDIA K80 GPU, 4 CPUs, and 61GiB RAM. App. F notes the 186
hyperparameter values and network architectures. 187
Dataset: We experiment on three types of data sets. Expert Data-set [10,16,22] generated using 188
an optimal policy without any exploration with high trajectory quality but low state action coverage. 189
Replay Data-set [2,13] generated from a policy while training it online, exhibiting a mixture of 190
multiple behavioral policies with high trajectory quality and state action coverage. Noisy Data-set 191
[12,13,22,16] generated using an optimal policy that also selects random actions with ϵgreedy 192
strategy where ϵ= 0.2having low trajectory quality and high state action coverage. Additionally we 193
also experiment on human generated dataset for sales promotion task. 194
Baselines: We do comparative studies on 10 baselines for OpenAI gym datasets. The first baseline 195
simply checks the conditions of Dand applies corresponding actions in execution. The performance 196
of this baseline shows that Dis imperfect and does not achieve the optimal reward. CQL SE is 197
from [ 40] where the expert is replaced by D. The other baselines are an ensemble of Dand eight 198
algorithms popular in the Offline RL literature for discrete environments. These algorithms include 199
Behavior Cloning (BC) [ 29], Behaviour Value Estimation (BVE) [ 16], Quantile Regression DQN 200
(QRDQN) [ 7], REM, MCE, BCQ, CQL and Critic Regularized Regression Q-Learning (CRR) [ 41]. 201
For a fair comparison, we use actions from domain knowledge for states not in the buffer and actions 202
from the trained policy for other states to obtain the final reward. Hence, each algorithm is renamed 203
with the suffix D in Table 5.1. 204
Limiting Data: To create limited-data settings for benchmark datasets, we first extract a small 205
percentage of samples from the full dataset and remove some of the samples based on state conditions. 206
This is done to ensure the reduced buffer satisfies the conditions defined in Def 4.1. We describe 207
the specific conditions of removal in the next section. Further insights and the state visualizations 208
for selected reduced datasets are in App H. Note : no data reduction has been performed on SP 209
dataset to demonstrate a real dataset exhibits characteristics of reduced buffer . 210
5.2 Performance across Different Datasets 211
Our results for OpenAI gym environments are summarised in Table 5.1 and Minigrid in Table 3 (App 212
D). We observe the performance of offline RL algorithms degrades substantially when part of the data 213
is not seen and trajectory ratios change. For these cases with only 10% partial data, ExID surpasses 214
the performance by at least 27% in the presence of reasonable domain knowledge. The proposed 215
method performs strongest on the replay dataset where the contribution of Lr(θ)is significant due 216
to state coverage, and the teacher learns from high-quality trajectories. Environment details are 217
described in the App. D. All domain knowledge trees are shown in the App. D Fig 10. We describe 218
limiting data conditions and domain knowledge specific to the environment as follows: 219
Mountain Car Environment: [27] We use simple, intuitive domain knowledge in this environment 220
shown in the App. D Fig 10 (c), which represents taking a left action when the car is at the bottom of 221
the valley with low velocity to gain momentum; otherwise, taking the right action to drive the car up. 222
Fig 6 (c) shows the state action pairs this rule generates on states sampled from a uniform random 223
distribution over the state boundaries. It can be observed that the states of Dcover part of the missing 224
6Table 1: Average reward [ ↑] obtained during online evaluation over 3 seeds on openAI gym envs
ENV
DATADATA
TYPEDQRDQN
DREM
DBVE
DCRR
DMCE
DBC
DBCQ
DCQL
DCQL
SECQL
(FULL)EXID
(OURS )
MOUNTAIN
CAREXPERT
-159.9
±
52.28-168.2
±
33.71-147.7
±
21.54-175.36
±
25.16-157.2
±
39.09-152
±
37.41-181.38
±
28.60-172.9
±
27.5-167.49
±
12.3-161.33
±
18.57-128.63
±
10.94-125.5
±
2.60
REPLAY-137.14
±
39.27-136.26
±
40.15-152.0
±
35.06-137.23
±
42.79-139.91
±
40.01-137.26
±
43.04-136.29
±
36.15-140.38
±
33.58-150.67
±
16.68-135.4
±
3.74-105.79
±
11.38
NOISY-141.61
±
33.04-134.99
±
32.60-173.95
±
39.60-178.99
±
23.58-168.69
±
38.78-140.0
±
28.5-144.52
±
43.04-179.8
±
29.99-126.96
±
17.84-107.06
±
12.73-109.9
±
13.45
CART
POLEEXPERT
57.0
±
5.3533.23
±
3.1741.31
±
8.7616.16
±
9.4115.24
±
5.6216.1
±
4.4225.76
±
74.39165.36
±
15.01121.8
±
14.0155.78
±
26.47364.1
±
22.15307.18
±
137.72
REPLAY149.09
±
14.05180.70
±
62.7911.1
±
2.1311.24
±
2.719.16
±
0.25144.43
±
2.41144.76
±
6.04131.97
±
23.23113.37
±
5.88250.02
±
55.02340.26
±
30.58
NOISY161
±
6.4015.33
±
0.5811.53
±
3.7713.68
±
7.4910.66
±
2.0468.4
±
14.6763.53
±
14.0892.6
±
22.0592.6
±
22.0593.72
±
37.79228.61
±
38.64
LUNAR
LANDEREXPERT
52.48
±
26.515.14
±
25.10-184.84
±
26.45-681.67
±
34.868.79
±
25.3819.71
±
10.5238.40
±
23.21-45.99
±
30.4765.43
±
71.3753.22
±
78.85167.74
±
29.4161.34
±
17.10
REPLAY-444.20
±
12.20-556.81
±
21.39-572
±
27.93-131.21
±
31.97-115.23
±
18.16136.63
±
12.40111.47
±
54.6761.83
±
45.5787.70
±
18.20187.72
±
25.62156.03
±
56.67
NOISY-4.81
±
97.2821.41
±
14.7128.65
±
12.26-158.27
±
7.71-50.47
±
15.7898.62
±
28.01101.59
±
30.835.01
±
128.6340.35
±
65.72111
±
52.32163.57
±
49.24
Figure 3: Performance of (a) CQL and (b) EXID on all datasets for Mountain Car during online
evaluation (c) Evaluation curves for the sales promotion dataset
data in Fig 1 (a). For limiting datasets, we remove states with position >-0.8. The performance of 225
CQLD and ExID are shown in Fig 3 (a),(b) where ExID surpasses CQLD for all three datasets. 226
Cart-pole Environment: For this environment, we use domain knowledge from [ 33], which aims to 227
move in the direction opposite to the lean of the pole, keeping the cart close enough to the center. If 228
the cart is close to an edge, the domain knowledge attempts to account for the cart’s velocity and 229
recenter the cart. The full tree is given in the App. D Fig 10 (a). We remove states with cart velocity 230
>-1.5 to create the reduced buffer. 231
Lunar-Lander Environment: We borrow the decision nodes from [ 34] and get actions from a 232
sub-optimal policy trained online with an average reward of 52.48. The full set of decision nodes is 233
shown in the App. D Fig 10 (b). Dfocuses on keeping the lander balanced when the lander is above 234
ground. When the lander is near the surface, Dfocuses on keeping the y velocity lower. To create the 235
reduced datasets, we remove data of lander angle <-0.04. 236
Mini-Grid Environments: For our experiments, we choose two environments: Random Dynamic 237
Obstacles 6X6 and LavaGapS 7X7. We use intuitive domain knowledge which avoids crashing into 238
obstacles in front, left, or right of agent ref. App. D Fig 10 (d), (e). We remove states with obstacles 239
on the right for creating limited data settings. Due to limitation of space we report the results of the 240
best-performing algorithms on the replay dataset in Table 3 (App D). 241
5.3 Case study on real human generated Sales Promotion (SP) dataset 242
SP dataset and environment [ 30] simulates a real-world sales promotion platform. The number of 243
coupons and the discount the user received will affect his behavior. A higher discount will promote 244
7the sales, but the cost will also increase. The goal for the platform operator is to maximize the 245
total profit. The horizon of the dataset is 50 days for the training and 30 days for the test. Domain 246
knowledge ([ 30], App A] : Active users can be given more coupons with lower discount to maximize 247
profit. We model this as order number >60∧Avgfee>0.8 =⇒[5,0.95]where action 1 is number 248
of coupons range [0,5] and action 2 is coupon value (discount value = (1-coupon value)) range 249
[0.6,0.95]. The dataset exhibits the properties in Def 4.1 as first 50 days of sales does not contain 250
many active users as reported in the coverage column of Tab 2 depicting scarcity. The domain rule is 251
imperfect as coupon value and number depend on multiple factors such as user purchase history and 252
behavior. As illustrated in the table 2 and Fig 3 (c) the intuitive domain rule enhances performance 253
by 10.49% in the real dataset. 254
Table 2: Results on human generated Sales Promotion dataset
Dataset D coverage D CQL + D CQLSE EXID Performance
gain
Sales
Promotion654.68
±20.0620.32% 722.06 ±71.40 727.03 ±
49.56802.91
±41.6910.49%
5.4 Generalization to OOD states and contribution of Lr(θ) 255
Figure 4: Q value difference between CQL and EXID for expert and policy action on states not
present in the buffer for a) expert b) noisy in log scale c) contribution of Lr(θ)
In Fig 4 (a), (b), we plot Qθ
s(s, aexpert )−Qθ
s(s, aθ)for CQL and EXID policies for different datasets 256
of Mountain-Car environments. Action aexpert is obtained from the full expert dataset where position 257
>−0.8. We observe that the Q value for actions of CQL policy diverges from the expert policy 258
actions with high values for the states not in the reduced buffer, whereas ExID stays close to the 259
expert actions for the unseen states. This empirically shows generalization to OOD states not in the 260
dataset but covered by domain knowledge. In Fig 4 (d), we plot the contribution by Lr(θ)during the 261
training and observe the contribution is higher for replay data sets with more state coverage. 262
5.5 Performance on varying λ,k, and ablation of πω
t263
We study the effect of varying λon the algorithm for the given domain knowledge. We empirically 264
observe setting a high or a low λcan yield sub-optimal performance, and λ= 0.5generally gives 265
good performance. In Fig 5 (a), we show this effect for LunarLander. Plots for other environments 266
are in the App. G Fig 11. For kwe observe setting the warm start parameter to 0 yields a sub-optimal 267
policy, as the critic may update πω
twithout completely learning from it. The starting performance 268
increases with an increase in kas shown in Fig 5 (b) for LunarLander. k= 30 works best according 269
to empirical evaluations. Plots for other environments are in the App. G Fig 12. We show two 270
ablations for Cart-pole in Fig 5 (c) with no teacher update after the warm start and no inclusion of 271
Lr(θ)after the warm start. The warm start in this environment is set to 30 episodes. Fig 5 c) shows 272
without teacher updated, the sub-optimal teacher drags down the performance of the policy beyond 273
the warm start, exhibiting the necessity of πω
tupdate. Also, the student converges to a sub-optimal 274
policy if no Lr(θ)is included beyond the warm start. 275
8Figure 5: (a) Effect of different λon the performance of ExID on Lunar Lander (b) Effect of different
kon the performance of EXID on Lunar Lander (c) Performance of EXID with teacher update, no
teacher update, and just warm start on Cart-pole.
Figure 6: (a) Dwith different average rewards (b) Performance effect on Lunar-lander (c) State
distribution generated for training the teacher network for mountain-car
5.6 Effect of varying Dquality 276
We show the effect of choosing policies as Dwith different average rewards for Lunar-Lander expert 277
data in Fig 6 (a) and (b). Rule 1 is optimal and has almost the same effect as Rule 3, which is the D 278
used in our experiments exhibiting that updating a sub-optimal Dcan lead to equivalent performance 279
as optimal D. Using a rule with high uncertainty, as Rule 2, induces high uncertainty in the learned 280
policy but performs slightly better than the baseline. Rule 4, which has a lower average reward, also 281
causes gains on average performance with slower convergence. Finally, Rule 5, with very bad actions, 282
affects policy performance adversely and leads to a performance lower than baseline CQL. 283
6 Conclusion and Limitation 284
In this paper, we study the effect of limited and partial data on offline RL and observe that the 285
performance of SOTA offline RL algorithms is sub-optimal in such settings. The paper proposes a 286
methodology to handle offline RL’s performance degradation using domain insights. We incorporate 287
a regularization loss in the CQL training using a teacher policy and refine the initial teacher policy 288
while training. We show that incorporating reasonable domain knowledge in offline RL enhances 289
performance, achieving a performance close to full data. However, this method is limited by the 290
quality of the domain knowledge and the overlap between domain knowledge states and reduced 291
buffer data. The study is also limited to discrete domains. In the future, the authors would like to 292
improve on capturing domain knowledge into the policy network without dependence on data and 293
extending the methodology to algorithms that handle continuous action space. 294
7 Broader Impact 295
During the trial-and-error training phase, RL agents may exhibit irrational behavior, which can be 296
risky and costly in real-world scenarios. As a more practical alternative to online RL, offline RL 297
9utilizes pre-existing collected data to eliminate the need for real-time interactions during training. 298
However, a drawback of offline RL is its dependence on the quality and quantity of historical data, 299
which, when sub-optimal, could adversely affect overall performance. Therefore, through this work, 300
we use domain knowledge to suppress erroneous actions when available data is limited. However, this 301
inclusion may facilitate harmful behavior in the presence of biased domain knowledge. Therefore, 302
we advocate the use of well-regulated domain knowledge obtained from experts. Beyond this, we do 303
not foresee any ethical impact on our work. 304
References 305
[1]A survey on offline reinforcement learning: Taxonomy, review, and open problems. IEEE Transactions on 306
Neural Networks and Learning Systems , 2023. ISSN 21622388. doi: 10.1109/TNNLS.2023.3250269. 307
[2]Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline 308
reinforcement learning. In International Conference on Machine Learning , pages 104–114. PMLR, 2020. 309
[3]Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement 310
learning with diversified q-ensemble. Advances in neural information processing systems , 34:7436–7447, 311
2021. 312
[4]Peter L. Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement learning in 313
weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial 314
Intelligence , UAI ’09, page 35–42, Arlington, Virginia, USA, 2009. AUAI Press. ISBN 9780974903958. 315
[5]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and 316
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540 , 2016. 317
[6]Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, 318
Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable 319
reinforcement learning environments for goal-oriented tasks. CoRR , abs/2306.13831, 2023. 320
[7]Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos. Distributional reinforcement learning 321
with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 322
2018. 323
[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- 324
tional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. 325
[9]Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal 326
of Machine Learning Research , 6, 2005. 327
[10] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep 328
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219 , 2020. 329
[11] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advances 330
in neural information processing systems , 34:20132–20145, 2021. 331
[12] Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep 332
reinforcement learning algorithms. arXiv preprint arXiv:1910.01708 , 2019. 333
[13] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without explo- 334
ration. In International conference on machine learning , pages 2052–2062. PMLR, 2019. 335
[14] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural 336
networks. Advances in neural information processing systems , 29, 2016. 337
[15] Sinong Geng, Aldo Pacchiano, Andrey Kolobov, and Ching-An Cheng. Improving offline rl by blending 338
heuristics. arXiv preprint arXiv:2306.00321 , 2023. 339
[16] Caglar Gulcehre, Sergio Gómez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad 340
Zolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and Nando de Freitas. Regularized behavior 341
value estimation. arXiv preprint arXiv:2103.09575 , 2021. 342
[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv 343
preprint arXiv:1503.02531 , 2015. 344
[18] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks 345
with logic rules. arXiv preprint arXiv:1603.06318 , 2016. 346
10[19] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In 347
Proceedings of the Nineteenth International Conference on Machine Learning , pages 267–274, 2002. 348
[20] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with 349
fisher divergence critic regularization. In International Conference on Machine Learning , pages 5774–5783. 350
PMLR, 2021. 351
[21] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning 352
via bootstrapping error reduction. Advances in Neural Information Processing Systems , 32, 2019. 353
[22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline 354
reinforcement learning. Advances in Neural Information Processing Systems , 33:1179–1191, 2020. 355
[23] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, 356
review, and perspectives on open problems. ArXiv , abs/2005.01643, 2020. URL https://api. 357
semanticscholar.org/CorpusID:218486979 . 358
[24] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, 359
review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020. 360
[25] Siqi Liu, Kay Choong See, Kee Yuan Ngiam, Leo Anthony Celi, Xingzhi Sun, and Mengling Feng. 361
Reinforcement learning for clinical decision support in critical care: comprehensive review. Journal of 362
medical Internet research , 22(7):e18477, 2020. 363
[26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, 364
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through 365
deep reinforcement learning. nature , 518(7540):529–533, 2015. 366
[27] Andrew William Moore. Efficient memory-based learning for robot control. Technical report, University 367
of Cambridge, Computer Laboratory, 1990. 368
[28] Susan A Murphy. A generalization error for q-learning. 2005. 369
[29] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural 370
computation , 3(1):88–97, 1991. 371
[30] Rong-Jun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan Zhang, and Yang Yu. 372
Neorl: A near real-world benchmark for offline reinforcement learning. Advances in Neural Information 373
Processing Systems , 35:24753–24765, 2022. 374
[31] Kajetan Schweighofer, Markus Hofmarcher, Marius-Constantin Dinu, Philipp Renz, Angela Bitto-Nemling, 375
Vihang Prakash Patil, and Sepp Hochreiter. Understanding the effects of dataset characteristics on offline 376
reinforcement learning. In Deep RL Workshop NeurIPS 2021 , 2021. URL https://openreview.net/ 377
forum?id=A4EWtf-TO3Y . 378
[32] Kajetan Schweighofer, Marius-constantin Dinu, Andreas Radler, Markus Hofmarcher, Vihang Prakash 379
Patil, Angela Bitto-Nemling, Hamid Eghbal-zadeh, and Sepp Hochreiter. A dataset perspective on offline 380
reinforcement learning. In Conference on Lifelong Learning Agents , pages 470–517. PMLR, 2022. 381
[33] Andrew Silva and Matthew Gombolay. Encoding human domain knowledge to warm start reinforcement 382
learning. In Proceedings of the AAAI conference on artificial intelligence , volume 35, pages 5042–5050, 383
2021. 384
[34] Andrew Silva, Matthew Gombolay, Taylor Killian, Ivan Jimenez, and Sung-Hyun Son. Optimization 385
methods for interpretable differentiable decision trees applied to reinforcement learning. In International 386
conference on artificial intelligence and statistics , pages 1855–1865. PMLR, 2020. 387
[35] Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4rl: Surprisingly simple self-supervision for offline 388
reinforcement learning in robotics. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, 389
Proceedings of the 5th Conference on Robot Learning , volume 164 of Proceedings of Machine Learning 390
Research , pages 907–917. PMLR, 08–11 Nov 2022. 391
[36] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple 392
semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757 , 2020. 393
[37] Jiaxi Tang and Ke Wang. Ranking distillation: Learning compact ranking models with high performance 394
for recommender system. In Proceedings of the 24th ACM SIGKDD international conference on knowledge 395
discovery & data mining , pages 2289–2298, 2018. 396
11[38] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling task-specific 397
knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136 , 2019. 398
[39] Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent 399
reinforcement learning with knowledge distillation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, 400
K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 401
226–237. Curran Associates, Inc., 2022. 402
[40] Richa Verma, Durgesh Kalwar, Harshad Khadilkar, and Balaraman Ravindran. Guiding offline reinforce- 403
ment learning using a safety expert. In Proceedings of the 7th Joint International Conference on Data 404
Science & Management of Data (11th ACM IKDD CODS and 29th COMAD) , pages 82–90, 2024. 405
[41] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, 406
Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. 407
Advances in Neural Information Processing Systems , 33:7768–7778, 2020. 408
[42] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M. Susskind, Jian Zhang, Ruslan Salakhutdinov, and 409
Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In International Confer- 410
ence on Machine Learning , 2021. URL https://api.semanticscholar.org/CorpusID:234763307 . 411
[43] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves 412
imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern 413
recognition , pages 10687–10698, 2020. 414
[44] Qisen Yang, Shenzhi Wang, Matthieu Gaetan Lin, Shiji Song, and Gao Huang. Boosting offline reinforce- 415
ment learning with action preference query. In International Conference on Machine Learning , pages 416
39509–39523. PMLR, 2023. 417
[45] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label 418
smoothing regularization. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition 419
(CVPR) , pages 3902–3910, 2020. doi: 10.1109/CVPR42600.2020.00396. 420
[46] Xiaoxuan Zhang and S Zhang Y Yu. Domain knowledge guided offline q learning. In Second Offline 421
Reinforcement Learning Workshop at Neurips , volume 2021, 2021. 422
[47] Ying Zheng, Haoyu Chen, Qingyang Duan, Lixiang Lin, Yiyang Shao, Wei Wang, Xin Wang, and 423
Yuedong Xu. Leveraging domain knowledge for robust deep reinforcement learning in networking. 424
InIEEE INFOCOM 2021 - IEEE Conference on Computer Communications , pages 1–10, 2021. doi: 425
10.1109/INFOCOM42981.2021.9488863. 426
12A Theoretical Analysis 427
Notations 428
For any deterministic policy πthe performance return is formulated as η(π) =Eτ∼π[P∞
t=0γtr(st, at)] 429
For any policy π,ρπis the (unormalized) discounted visitation frequency given by ρπ(s) =P∞
t=0γtP(st=s) 430
where s0∼ρ0(s0)and the trajectory (s0, s1, . . .)is sampled from the policy πandρπ(s)∈[0,1
1−γ]. 431
¯ρπ(s) =sup{ρπ(s), s∈S} ∈[1
|Sπ|(1−γ),1
(1−γ)] 432
We denote the regularized policy learned by ExID on Brasˆπand the unregularized policy as πu. 433
Lemmas 434
We introduce the following Lemma required for our theoretical analysis. 435
Lemma A.1. ([44]) Given two policies π1andπ2 436
η(π1)−η(π2) =Z
s∈Sρπ1(s)(Q∗(s, π1(s)−V∗(s))ds−Z
s∈Sρπ2(s)(Q∗(s, π2(s)−V∗(s))ds
Proof. Please refer to Lemma A.1 Eq 17 in [44] 437
Proposition A.2. (4.2) Denote ˆπas the policy learned by ExID, πuas any offline RL policy learned on Brand 438
optimal Q function as Q∗and V function as V∗. Then it holds that 439
η(ˆπ)−η(πu)≥Es∼O|πu[V∗(s)−Q∗(s, πu(s))]−¯ρˆπα
Proof. According to [19] performance improvement between two policies if given by 440
η(π1) =η(π2) +Eτ∼π1"∞X
t=0γtQπ2(st, at)−Vπ2(st)#
(8)
Replacing π1byˆπandπ2byπuand by following Lemma A.1 441
η(ˆπ)−η(πu) =Z
s∈Sρˆπ(s)(Q∗(s,ˆπ(s))−V∗(s))ds−Z
s∈Sρπu(s)(Q∗(s, πu(s))−V∗(s))ds (9)
=Z
s∈Sρπu(s)(V∗(s)−Q∗(s, πu(s)))ds−Z
s∈Sρˆπ(s)(V∗(s)−Q∗(s,ˆπ(s)))ds (10)
Dividing the state space into in dataset domain states (I) and OOD states (O). The 442
(11)
Z
s∈Iρπu(s)(V∗(s)−Q∗(s, πu(s)))ds−Z
s∈Iρˆπ(s)(V∗(s)−Q∗(s,ˆπ(s)))ds
| {z }
a+
Z
s∈Oρπu(s)(V∗(s)−Q∗(s, πu(s)))ds−Z
s∈Oρˆπ(s)(V∗(s)−Q∗(s,ˆπ(s)))ds
| {z }
b(12)
Since the regularization loss facilitates visitation to OOD states via knowledge distillation we assume
ρˆπ=ρπu−∆ifors∈iandρˆπ=ρπu+ ∆ ofors∈owhere ∆i∈[0, ρπu(s)]and∆o∈[0,1
1−γ−ρπu(s)] 443
a=Z
s∈Iρπu(s)(V∗(s)−Q∗(s, πu(s)))ds−Z
s∈I(ρπu−∆i)(s)(V∗(s)−Q∗(s,ˆπ(s)))ds (13)
=Z
s∈Iρπu(s)(Q∗(s,ˆπ(s))−Q∗(s, πu(s)))ds+Z
s∈I∆i(s)(V∗(s)−Q∗(s,ˆπ(s)))ds (14)
13Under assumption in distribution action can be learned from the dataset due to conservatism of offline RL
(Q∗(s,ˆπ(s))−Q∗(s, πu(s)))≈0,a≥0 444
b=Z
s∈Oρπu(s)(V∗(s)−Q∗(s, πu(s)))ds−Z
s∈O(ρπu+ ∆ o)(s)(V∗(s)−Q∗(s,ˆπ(s)))ds (15)
≥Z
s∈Oρπu(s)(V∗(s)−Q∗(s, πu(s)))ds−Z
s∈Oρˆπ(s)(V∗(s)−Q∗(s,ˆπ(s)))ds (16)
≥Es∼O|πu[V∗(s)−Q∗(s, πu(s))]−Es∼O|ˆπ[V∗(s)−Q∗(s,ˆπ(s))] (17)
Further loosening the lower bound 445
=Es∼O|πu[V∗(s)−Q∗(s, πu(s))]−¯ρˆπZ
s∈Oρˆπ
¯ρˆπ(V∗(s)−Q∗(s,ˆπ(s)))ds (18)
≥Es∼O|πu[V∗(s)−Q∗(s, πu(s))]−¯ρˆπZ
s∈O(V∗(s)−Q∗(s,ˆπ(s)))ds (19)
Combining Eq 14, 17 and 19, and denoting α=Es∼O[V∗(s)−Q∗(s,ˆπ(s))] 446
η(ˆπ)−η(πu)≥Es∼O|πu[V∗(s)−Q∗(s, πu(s))]−¯ρˆπα (20)
Hence, Proposition 4.2 follows Q.E.D 447
448
Proposition A.3. (4.3) Algo 1 reduces generalization error if Q∗(s, πω
t(s))> Q∗(s, π(s))fors∈ D ∩ B r, 449
where πis vanilla offline RL policy learnt on Br. 450
Proof. Generalization error for any policy πas defined by [28] can be written as: 451
Gπ=V∗(s0)−Vπ(s0) =−Eτ∼π[TX
t=0γtQ∗(st, π(st))−V∗(st)] (21)
Here,Eτ∼πrepresents sampling trajectories with policy π. Since the state space is continuous, we can represent
the expectation as an integral over the state space 452
=−TX
t=0γtZ
s∈SP(st=s|π)(Q∗(st, π(st))−V∗(st))ds (22)
=−Z
s∈STX
t=0γtP(st=s|π)(Q∗(st, π(st))−V∗(st))ds (23)
Analysing with respect to s∈ D ∩ B rwe can break the integration into two parts 453
=−"Z
s∈S/DTX
t=0γtP(st=s|π)(Q∗(st, π(st))−V∗(st))ds+Z
s∈DTX
t=0γtP(st=s|π)(Q∗(st, π(st))−V∗(st))#
(24)
=−"
f(s|π) +Z
s∈DTX
t=0γtP(st=s|π)(Q∗(st, π(st))−V∗(st))#
(25)
For a policy ˆπlearnt in Algo 1 the action for st=s∈ D is regularized to be close to πω
twhich either follows
domain knowledge or expert demonstrations. Hence, it is reasonable to assume Q∗(st, πω
t(st))> Q∗(st, π(st)).
It follows 454
Z
s∈DTX
t=0γtP(st=s|ˆπ)(Q∗(st,ˆπ(st))−V∗(st))<Z
s∈DTX
t=0γtP(st=s|π)(Q∗(st, π(st))−V∗(st))
(26)
14Note for s /∈ D,f(s|ˆπ)≈f(s|π).This is because the regularization term assigns max Q value to a different
action for s∈ D butmax a(Q(s, a))remains same 455
∴−"
f(s|ˆπ) +Z
s∈DTX
t=0γtP(st=s|ˆπ)(Q∗(st,ˆπ(st))−V∗(st))#
<−"
f(s|π) +Z
s∈DTX
t=0γtP(st=s|π)(Q∗(st, π(st))−V∗(st))#
(27)
Hence, Gˆπ< G πProposition 2 follows Q.E.D 456
457
15B Missing Examples 458
Performing Q−Learning by sampling from a reduced batch Brmay not converge to an optimal policy for the 459
MDP MBrepresenting the full buffer. 460
Example (Theorem 1,[ 13]) defines MDP MBofBfrom same state action space of the original MDP Mwith 461
transition probabilities pB(s′|s, a) =N(s,a,s′)P
˜sN(s,a,˜s)where N(s, a, s′)is the number of times (s, a, s′)occurs in B 462
and an terminal state sinit. It states pB(sinit|s, a) = 1 whenP
˜sN(s, a,˜s) = 0 . This happens when transitions 463
of some s′of(s, a, s′)are missing from the buffer, which may occur in BrwhenBr⊂ B.r(sinit, s, a)is 464
initialized to Q(s, a). We assume that a policy learned on reduced dataset Brconverges to optimal value function 465
and disprove it using the following counterexample: 466
Figure 7: Example MDP, sampled buffer MDP and reduced buffer with Q tables
Figure 8: We hypothesize the suboptimal perfor-
mance of offline RL for limited data can be ad-
dressed via domain knowledge via action regular-
ization and knowledge distillation.We take a simple MDP illustrated in Fig 7 with 3 467
states and 2 actions (0,1). The reward of each ac- 468
tion is marked along the transition. The sampled 469
MDP is constructed the following samples (1,0,2)- 470
2,(1,1,2)-3, (2,0,3)-3, and (2,1,3)-2 and the reduced 471
buffer MDP with samples (1,0,2)-2 and (1,1,2)-1. 472
The probabilities are marked along the transition. 473
It is easy to see that the policy learned under the 474
reduced MDP converges to a nonoptimal policy af- 475
ter one step of the Q table update with Q(s, a) = 476
r(s, a) +p(s′|s, a)∗max a′(Q(s′, a′)). This hap- 477
pens because of transition probability shift on reduc- 478
ing samples pB(s′|s, a)̸=pBr(s′|s, a)and no Q 479
updates for (s, a)/∈ Br. 480
Our methodology addresses these issues as follows: 481
•Fors∈D∩ Brbetter actions are enforced 482
through regularization using πω
teven when 483
the transition probabilities are low for op- 484
timal transitions. 485
•Incorporating regularization distills the 486
teacher’s knowledge in the critic-enhancing 487
generalization. 488
A visualization is shown in Fig 8. 489
C Algorithm 490
The pseudo code of the algorithm is described in Algo 1. 491
16Algorithm 1 Pseudo code for EXID
1:Input: Reduced buffer Br, Initial teacher network πω
t, Training steps N, Warm-up steps k, Soft
update τ, hyperparameters: λ, α
2:Initialize Critic with MC dropout and Target Critic Qθ
s, Qθ′
s
3:forn←1toNdo
4: Sample mini-batch bof transitions (s, a, r, s′)∼ Brat= [], as= [], sr= []
5: fors∈bdo
6: ifs|=Dandπω
t(s)̸=argmax a(Qθ
s(s, a))then
7: at.append (πω
t(s))
8: as.append (argmaxa(Qθ
s(s, a)))
9: sr.append (s)
10: end if
11: end for
12: ifn > k∧Cond. 6 then
13: Update πω
t(s)using Eq 7
14: Lr(θ) = 0
15: else
16: Calculate Lr(θ)using Eq 3
17: end if
18: Calculate L(θ)using Eq 4
19: Update Qθ
swithL(θ)and softy update Qθ′
sandτ
20:end for
D Environments and Domain Knowledge Trees 492
Figure 9: Graphical visualizations of environments used in the experiments. These environments are
a)MountainCar-v0 b)CartPole-v1 c)LunarLander-v2 d)MiniGrid-LavaGapS7-v0 e)MiniGrid-
Dynamic-Obstacles-Random-6x6-v0
The graphical visualization of each environment is depicted in Fig 9. The choice of environment in this paper 493
depended on two factors: a) Pre-existing standard methods of generating offline RL datasets. b) Possibility of 494
creating intuitive decision tree-based domain knowledge. All datasets have been created via [ 31]. We explain the 495
environments in detail as follows: 496
17Mountain-car Environment: This environment Fig 9 a) has two state variables, position and velocity, and three 497
discrete actions: left push, right push, and no action [ 27]. The goal is to drive a car up a valley to reach the flag. 498
This environment is challenging for offline RL because of sparse rewards, which are only obtained on reaching 499
the flag. 500
Cart-pole Environment The environment Fig 9 b) has 4 states and 2 actions representing left force and right 501
force. The objective is to balance a pole on a moving cart. 502
Lunar-Lander Environment: The task is to land a lunar rover between two flags Fig 9 c) by observing 8 states 503
and applying one of 4 actions. 504
Minigrid Environments: Mini-grid [ 6] is an environment suite containing 2D grid-worlds with goal oriented 505
tasks. As explained in the main text, we experiment using MiniGrid-LavaGapS7-v0 and MiniGrid-Dynamic- 506
Obstacles-Random-6x6-v0 from this environment suite is shown in Fig 9 d) and e). In MiniGrid-LavaGapS7-v0, 507
the agent has to avoid Lava and pass through the gap to reach the goal. Dynamic obstacles are similar; however, 508
the agent can start at a random position and has to avoid dynamically moving balls to reach the goal. The 509
environment has image observation with 3 channels (OBJECT_ID, COLOR_ID, STATE). Following [ 31] 510
experiments, we flatten the image to an array of 98 observations and restrict action space to three actions: Turn 511
left, Turn Right, and Move forward. The results of minigrid environment are reported in Table 3. Since this 512
environment uses a semantic map from image observation, we collect states from a fixed policy with random 513
actions to generate the teacher’s state distribution. CQL on the full dataset achieves the average reward of 514
0.92±0.1for DynamicObstacles and 0.53±0.01for LavaGapS. 515
The domain knowledge trees for all the environments are shown in Fig 10. The cart pole domain knowledge 516
tree Fig 10 a) is taken from [ 33] (Fig 7). The Lunar Lander decision nodes Fig 10 b) have been taken from [ 34] 517
(Fig4). For the mini-grid environments, we construct intuitive decision trees shown in Fig 10 d) and Fig 10 e). 518
Positions 52, 40, and 68 represent positions front, right, and left of the agent. Value 0.2 represents a wall, 0.9 519
represents Lava, and 0.6 represents a ball. We check positions 52, 40, and 68 for these obstacles and choose the 520
recommended actions as domain knowledge. 521
Figure 10: Domain knowledge trees for a) CartPole-v1 b)LunarLander-v2 c)MountainCar-v0 d)
MiniGrid-LavaGapS7-v0 e)MiniGrid-Dynamic-Obstacles-Random-6x6-v0 environments
E Related Work: Knowledge Distillation 522
Knowledge distillation is a well-embraced technique of incorporating additional information in neural networks 523
and has been applied to various fields like computer vision [ 43,36], natural language processing [ 8,38], and 524
recommendation systems [ 37]. [17] introduced the concept of distilling knowledge from a complex, pre-trained 525
model (teacher) into a smaller model (student). In recent years, researchers have explored the integration 526
of rule-based regularization techniques within the context of knowledge distillation. Rule regularization 527
introduces additional constraints based on predefined rules, guiding the learning process of the student model 528
18Table 3: Average reward [ ↑] obtained during online evaluation over 3 seeds on Minigrid environments
ENVIRONMENT D BC
DBCQ
DCQL
DEXID
MINIGRID
DYNAMIC
RANDOM 6X60.50
±
0.080.59
±
0.070.24
±
0.220.14
±
0.10.79
±
0.07
MINIGRID
LAVAGAPS
7X70.27
±
0.090.29
±
0.110.26
±
0.10.28
±
0.120.46
±
0.13
[18,45]. These techniques have shown to reduce overfitting and enhance generalization [ 38]. Knowledge 529
distillation is also prevalent in the field of RL [ 47] and offline RL [ 39]. Contrary to prevalent teacher-student 530
knowledge distillation techniques, our work does not enforce parameter sharing among the networks. Through 531
experiments, we demonstrate that a simple regularization loss and expected performance-based updates can 532
improve generalization to unobserved states covered by domain knowledge. There are also no constraints on 533
keeping the same network structure for the teacher, paving ways for capturing the domain knowledge into more 534
structured networks such as Differentiable Decision Trees (DDTs). 535
F Network Architecture and Hyper-parameters 536
We follow the network architecture and hyper-parameters proposed by [ 31] for all our networks, including the 537
baseline networks. The teacher BC network πt
ωand Critic network Qθ
s(s, a)consists of 3 linear layers, each 538
having a hidden size of 256 neurons. The number of input and output neurons depends on the environment’s state 539
and action size. All layers except the last are SELU activation functions; the final layer uses linear activation. 540
πt
ωuses a softmax activation function in the last layer for producing action probabilities. A learning rate of 541
0.0001 with batch size 32 and α= 0.1is used for all environments. MC dropout probability of 0.5 and 542
number of stochastic passes T=10 have been used for the critic network. The uncertainty check is performed 543
every 15 episodes after the warm start to avoid computational overhead. The hyper-parameters specific to our 544
algorithm for OpenAI gym are reported in Table F. The hyper-parameters specific to our algorithm for Minigrid 545
environments are reported in Table 5. 546
Table 4: Hyperparameters for openAI gym environments
HYPERPARAM MOUNTAIN CAR CARTPOLE LUNAR -
LANDER
DATA TYPE EXPERT REPLAY NOISY EXPERT REPLAY NOISY EXPERT REPLAY NOISY
λ 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5
k 30 30 30 30 30 30 30 30 30
πt
ωLR 1e51e51e51e21e21e21e41e41e4
TRAINING
STEPS42000 36000 36000 30000 17000 17000 18000 18000 18000
Table 5: Hyper-parameters for Mini-grid environments for replay dataset
Environment DynamicObstRandom6x6-
v0LavaGapS7v0
λ 0.1 0.1
k 30 30
πt
ωlr 1e41e4
training steps 5000 10000
19Figure 11: Effect of λon the performance of ExID for different environments expert datasets.
G Effect of kandλand Evaluation Plots 547
We empirically evaluate the effect of λIn Fig 11 and kin Fig 12. We believe these parameters depend on the 548
quality of D. For the given Din the environments we empirically observe, λ= 0.5generally performs well, 549
except for Minigrid environments where λ= 0.1works better. Increasing the warm start parameter kgenerally 550
increases the initial performance of the policy, allowing it to learn from the teacher. Meanwhile, no warm start 551
adversely affects policy performance as the critic may erroneously update the teacher. From empirical evaluation, 552
we observe that k= 30 gives a reasonable start to the policy. All the evaluation plots are shown in Fig 13, where 553
it can be observed that ExID performs better than baseline CQL. 554
Figure 12: Effect of kon the performance of ExID for different environments expert datasets.
H Data reduction design and data distribution visualization of reduced 555
dataset 556
In this section, we discuss the intuition behind our data-limiting choices. We also visually represent selected 557
reduced datasets for the OpenAI gym environments. 558
20Figure 13: Evaluation plots of CQL and EXID algorithms for Cartpole, Lunar-Lander, and Minigrid
environments using different data types and seeds reported in the main paper Table 5.1.
Figure 14: (a) The effect of data reduction and removal on baseline CQL visualized on Mountain Car
Environment (b) Performance of ExID on removing different parts of the data based on nodes of Fig
10 (c) from Mountain Car expert dataset
Reducing transitions from the dataset: For all datasets, 10% of the data samples were extracted from the full 559
dataset. This experimental design choice is based on the observation shown in Fig 14 (a). Performance degrades 560
on reducing samples to 0.1% of the dataset and reduces further on reducing samples to 0.05% of the dataset. 561
However, this drop is not substantial. The performance also reduces on removing part of the dataset from the 562
full dataset with states >−0.8. However, the worst performance is observed when both samples are reduced 563
and data is omitted, attributing to accumulated errors from probability ratio shift contributing to an increase in 564
generalization error. Our methodology aims to address this gap in performance. 565
Removing part of the state space: Due to the simplicity of the Mountain-Car environment, we analyze the 566
Mountain-Car expert dataset to show the effect of removing data matching state conditions of the different nodes 567
in the decision tree in Fig 10 (c). The performance for each condition is summarised in Table 6. The most 568
informative node in the tree is position >−0.5; removing states matching this condition causes a performance 569
drop in the algorithm as the domain knowledge regularization does not contribute significant information to the 570
policy. Similarly, removing data with velocity <0.01causes a performance drop. However, both performances 571
are higher than the baseline CQL trained on reduced data. Based on this observation, we choose state removal 572
conditions that preserve states matching part of the information in the tree such that the regularization term 573
contributes substantially to the policy. Fig 15 shows the data distribution plot of 10% samples extracted from 574
mountain car replay and noisy data with states >−0.8removed. Fig 16 shows visualizations for 10% samples 575
21extracted from expert data with velocity >−1.5removed. Fig 17 shows visualizations for 10% samples 576
extracted from expert data with lander angle <−0.04removed. 577
Table 6: Performance of ExID on removing different parts of the data based on nodes of Fig 10 (c)
from Mountain Car expert dataset
Position >-0.5 Position <-0.5 Velocity >0.01 Velocity <0.01
-121.89 ±7.69 -151 ±13.6 -128.48 ±11.84 -147.80 ±5.01
Figure 15: Data distribution of reduced dataset compared to the full dataset for mountain replay and
noisy data
Figure 16: Data distribution of reduced cart pole expert dataset compared to the full dataset
22Figure 17: Data distribution of reduced LunarLander expert dataset compared to the full dataset
23NeurIPS Paper Checklist 578
1.Claims 579
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s 580
contributions and scope? 581
Answer: [Yes] 582
Justification: The claims made in the paper have been experimented on different settings for validity 583
and generalization. Please refer to sec 5.2. 584
Guidelines: 585
•The answer NA means that the abstract and introduction do not include the claims made in the 586
paper. 587
•The abstract and/or introduction should clearly state the claims made, including the contributions 588
made in the paper and important assumptions and limitations. A No or NA answer to this 589
question will not be perceived well by the reviewers. 590
•The claims made should match theoretical and experimental results, and reflect how much the 591
results can be expected to generalize to other settings. 592
•It is fine to include aspirational goals as motivation as long as it is clear that these goals are not 593
attained by the paper. 594
2.Limitations 595
Question: Does the paper discuss the limitations of the work performed by the authors? 596
Answer: [Yes] 597
Justification: The paper acknowledges the dependency on reasonable domain knowledge and coverage 598
please refer to sec 6 599
Guidelines: 600
•The answer NA means that the paper has no limitation while the answer No means that the paper 601
has limitations, but those are not discussed in the paper. 602
• The authors are encouraged to create a separate "Limitations" section in their paper. 603
•The paper should point out any strong assumptions and how robust the results are to violations of 604
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, 605
asymptotic approximations only holding locally). The authors should reflect on how these 606
assumptions might be violated in practice and what the implications would be. 607
•The authors should reflect on the scope of the claims made, e.g., if the approach was only tested 608
on a few datasets or with a few runs. In general, empirical results often depend on implicit 609
assumptions, which should be articulated. 610
•The authors should reflect on the factors that influence the performance of the approach. For 611
example, a facial recognition algorithm may perform poorly when image resolution is low or 612
images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide 613
closed captions for online lectures because it fails to handle technical jargon. 614
•The authors should discuss the computational efficiency of the proposed algorithms and how 615
they scale with dataset size. 616
•If applicable, the authors should discuss possible limitations of their approach to address problems 617
of privacy and fairness. 618
•While the authors might fear that complete honesty about limitations might be used by reviewers 619
as grounds for rejection, a worse outcome might be that reviewers discover limitations that 620
aren’t acknowledged in the paper. The authors should use their best judgment and recognize 621
that individual actions in favor of transparency play an important role in developing norms that 622
preserve the integrity of the community. Reviewers will be specifically instructed to not penalize 623
honesty concerning limitations. 624
3.Theory Assumptions and Proofs 625
Question: For each theoretical result, does the paper provide the full set of assumptions and a complete 626
(and correct) proof? 627
Answer: [Yes] 628
Justification: Please refer to App A in the supplement material for theoretical analysis and proofs. 629
Guidelines: 630
• The answer NA means that the paper does not include theoretical results. 631
• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. 632
24• All assumptions should be clearly stated or referenced in the statement of any theorems. 633
•The proofs can either appear in the main paper or the supplemental material, but if they appear in 634
the supplemental material, the authors are encouraged to provide a short proof sketch to provide 635
intuition. 636
•Inversely, any informal proof provided in the core of the paper should be complemented by 637
formal proofs provided in appendix or supplemental material. 638
• Theorems and Lemmas that the proof relies upon should be properly referenced. 639
4.Experimental Result Reproducibility 640
Question: Does the paper fully disclose all the information needed to reproduce the main experimental 641
results of the paper to the extent that it affects the main claims and/or conclusions of the paper 642
(regardless of whether the code and data are provided or not)? 643
Answer: [Yes] 644
Justification: Yes all hyper-parameters and experimental setting have been clearly listed in the paper. 645
Please refer to App F and sec 5.1. 646
Guidelines: 647
• The answer NA means that the paper does not include experiments. 648
•If the paper includes experiments, a No answer to this question will not be perceived well by the 649
reviewers: Making the paper reproducible is important, regardless of whether the code and data 650
are provided or not. 651
•If the contribution is a dataset and/or model, the authors should describe the steps taken to make 652
their results reproducible or verifiable. 653
•Depending on the contribution, reproducibility can be accomplished in various ways. For 654
example, if the contribution is a novel architecture, describing the architecture fully might suffice, 655
or if the contribution is a specific model and empirical evaluation, it may be necessary to either 656
make it possible for others to replicate the model with the same dataset, or provide access to 657
the model. In general. releasing code and data is often one good way to accomplish this, but 658
reproducibility can also be provided via detailed instructions for how to replicate the results, 659
access to a hosted model (e.g., in the case of a large language model), releasing of a model 660
checkpoint, or other means that are appropriate to the research performed. 661
•While NeurIPS does not require releasing code, the conference does require all submissions 662
to provide some reasonable avenue for reproducibility, which may depend on the nature of the 663
contribution. For example 664
(a)If the contribution is primarily a new algorithm, the paper should make it clear how to 665
reproduce that algorithm. 666
(b)If the contribution is primarily a new model architecture, the paper should describe the 667
architecture clearly and fully. 668
(c)If the contribution is a new model (e.g., a large language model), then there should either be 669
a way to access this model for reproducing the results or a way to reproduce the model (e.g., 670
with an open-source dataset or instructions for how to construct the dataset). 671
(d)We recognize that reproducibility may be tricky in some cases, in which case authors are 672
welcome to describe the particular way they provide for reproducibility. In the case of 673
closed-source models, it may be that access to the model is limited in some way (e.g., 674
to registered users), but it should be possible for other researchers to have some path to 675
reproducing or verifying the results. 676
5.Open access to data and code 677
Question: Does the paper provide open access to the data and code, with sufficient instructions to 678
faithfully reproduce the main experimental results, as described in supplemental material? 679
Answer: [Yes] 680
Justification: The code is provided with the submission in a zip file with Readme for instructions. 681
Guidelines: 682
• The answer NA means that paper does not include experiments requiring code. 683
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/ 684
guides/CodeSubmissionPolicy ) for more details. 685
•While we encourage the release of code and data, we understand that this might not be possible, 686
so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless 687
this is central to the contribution (e.g., for a new open-source benchmark). 688
•The instructions should contain the exact command and environment needed to run to reproduce 689
the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/ 690
guides/CodeSubmissionPolicy ) for more details. 691
25•The authors should provide instructions on data access and preparation, including how to access 692
the raw data, preprocessed data, intermediate data, and generated data, etc. 693
•The authors should provide scripts to reproduce all experimental results for the new proposed 694
method and baselines. If only a subset of experiments are reproducible, they should state which 695
ones are omitted from the script and why. 696
•At submission time, to preserve anonymity, the authors should release anonymized versions (if 697
applicable). 698
•Providing as much information as possible in supplemental material (appended to the paper) is 699
recommended, but including URLs to data and code is permitted. 700
6.Experimental Setting/Details 701
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, 702
how they were chosen, type of optimizer, etc.) necessary to understand the results? 703
Answer: [Yes] 704
Justification: The paper uses open source code to create the dataset and lists the modifications in 705
details in the main paper and supplement material. Please refer to App F and sec 5.1. 706
Guidelines: 707
• The answer NA means that the paper does not include experiments. 708
•The experimental setting should be presented in the core of the paper to a level of detail that is 709
necessary to appreciate the results and make sense of them. 710
• The full details can be provided either with the code, in appendix, or as supplemental material. 711
7.Experiment Statistical Significance 712
Question: Does the paper report error bars suitably and correctly defined or other appropriate informa- 713
tion about the statistical significance of the experiments? 714
Answer: [Yes] 715
Justification: All experiments have been run on 3 random seeds and the error bounds have been 716
reported in Table 5.1, Table 2 and Table 3. 717
Guidelines: 718
• The answer NA means that the paper does not include experiments. 719
•The authors should answer "Yes" if the results are accompanied by error bars, confidence 720
intervals, or statistical significance tests, at least for the experiments that support the main claims 721
of the paper. 722
•The factors of variability that the error bars are capturing should be clearly stated (for example, 723
train/test split, initialization, random drawing of some parameter, or overall run with given 724
experimental conditions). 725
•The method for calculating the error bars should be explained (closed form formula, call to a 726
library function, bootstrap, etc.) 727
• The assumptions made should be given (e.g., Normally distributed errors). 728
•It should be clear whether the error bar is the standard deviation or the standard error of the 729
mean. 730
•It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 731
a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is 732
not verified. 733
•For asymmetric distributions, the authors should be careful not to show in tables or figures 734
symmetric error bars that would yield results that are out of range (e.g. negative error rates). 735
•If error bars are reported in tables or plots, The authors should explain in the text how they were 736
calculated and reference the corresponding figures or tables in the text. 737
8.Experiments Compute Resources 738
Question: For each experiment, does the paper provide sufficient information on the computer 739
resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? 740
Answer: [Yes] 741
Justification: Please refer to the Experimental setup section in the main paper sec 5.1. 742
Guidelines: 743
• The answer NA means that the paper does not include experiments. 744
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud 745
provider, including relevant memory and storage. 746
26•The paper should provide the amount of compute required for each of the individual experimental 747
runs as well as estimate the total compute. 748
•The paper should disclose whether the full research project required more compute than the 749
experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into 750
the paper). 751
9.Code Of Ethics 752
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code 753
of Ethics https://neurips.cc/public/EthicsGuidelines ? 754
Answer: [Yes] 755
Justification: The authors have reviewed the code of ethics and the paper adheres to it. 756
Guidelines: 757
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 758
•If the authors answer No, they should explain the special circumstances that require a deviation 759
from the Code of Ethics. 760
•The authors should make sure to preserve anonymity (e.g., if there is a special consideration due 761
to laws or regulations in their jurisdiction). 762
10.Broader Impacts 763
Question: Does the paper discuss both potential positive societal impacts and negative societal impacts 764
of the work performed? 765
Answer: [Yes] 766
Justification: Please refer to the section broader impacts 7. 767
Guidelines: 768
• The answer NA means that there is no societal impact of the work performed. 769
•If the authors answer NA or No, they should explain why their work has no societal impact or 770
why the paper does not address societal impact. 771
•Examples of negative societal impacts include potential malicious or unintended uses (e.g., 772
disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy- 773
ment of technologies that could make decisions that unfairly impact specific groups), privacy 774
considerations, and security considerations. 775
•The conference expects that many papers will be foundational research and not tied to particular 776
applications, let alone deployments. However, if there is a direct path to any negative applications, 777
the authors should point it out. For example, it is legitimate to point out that an improvement in 778
the quality of generative models could be used to generate deepfakes for disinformation. On the 779
other hand, it is not needed to point out that a generic algorithm for optimizing neural networks 780
could enable people to train models that generate Deepfakes faster. 781
•The authors should consider possible harms that could arise when the technology is being used 782
as intended and functioning correctly, harms that could arise when the technology is being used 783
as intended but gives incorrect results, and harms following from (intentional or unintentional) 784
misuse of the technology. 785
•If there are negative societal impacts, the authors could also discuss possible mitigation strategies 786
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitor- 787
ing misuse, mechanisms to monitor how a system learns from feedback over time, improving the 788
efficiency and accessibility of ML). 789
11.Safeguards 790
Question: Does the paper describe safeguards that have been put in place for responsible release of 791
data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or 792
scraped datasets)? 793
Answer: [NA] 794
Justification: The algorithm proposed in this paper does not not pose any such risk of misuse. 795
Guidelines: 796
• The answer NA means that the paper poses no such risks. 797
•Released models that have a high risk for misuse or dual-use should be released with necessary 798
safeguards to allow for controlled use of the model, for example by requiring that users adhere to 799
usage guidelines or restrictions to access the model or implementing safety filters. 800
•Datasets that have been scraped from the Internet could pose safety risks. The authors should 801
describe how they avoided releasing unsafe images. 802
27•We recognize that providing effective safeguards is challenging, and many papers do not require 803
this, but we encourage authors to take this into account and make a best faith effort. 804
12.Licenses for existing assets 805
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, 806
properly credited and are the license and terms of use explicitly mentioned and properly respected? 807
Answer: [Yes] 808
Justification: All codes and datasets used in this paper are under MIT licence and the original owners 809
have been cited. 810
Guidelines: 811
• The answer NA means that the paper does not use existing assets. 812
• The authors should cite the original paper that produced the code package or dataset. 813
• The authors should state which version of the asset is used and, if possible, include a URL. 814
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 815
•For scraped data from a particular source (e.g., website), the copyright and terms of service of 816
that source should be provided. 817
•If assets are released, the license, copyright information, and terms of use in the package should 818
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for 819
some datasets. Their licensing guide can help determine the license of a dataset. 820
•For existing datasets that are re-packaged, both the original license and the license of the derived 821
asset (if it has changed) should be provided. 822
•If this information is not available online, the authors are encouraged to reach out to the asset’s 823
creators. 824
13.New Assets 825
Question: Are new assets introduced in the paper well documented and is the documentation provided 826
alongside the assets? 827
Answer: [NA] 828
Justification: No new assets have been introduced in this paper. 829
Guidelines: 830
• The answer NA means that the paper does not release new assets. 831
•Researchers should communicate the details of the dataset/code/model as part of their sub- 832
missions via structured templates. This includes details about training, license, limitations, 833
etc. 834
•The paper should discuss whether and how consent was obtained from people whose asset is 835
used. 836
•At submission time, remember to anonymize your assets (if applicable). You can either create an 837
anonymized URL or include an anonymized zip file. 838
14.Crowdsourcing and Research with Human Subjects 839
Question: For crowdsourcing experiments and research with human subjects, does the paper include 840
the full text of instructions given to participants and screenshots, if applicable, as well as details about 841
compensation (if any)? 842
Answer: [NA] 843
Justification: The paper did not require any crowdsourcing or human subject for experimentation. 844
Guidelines: 845
•The answer NA means that the paper does not involve crowdsourcing nor research with human 846
subjects. 847
•Including this information in the supplemental material is fine, but if the main contribution of the 848
paper involves human subjects, then as much detail as possible should be included in the main 849
paper. 850
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other 851
labor should be paid at least the minimum wage in the country of the data collector. 852
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 853
Question: Does the paper describe potential risks incurred by study participants, whether such 854
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an 855
equivalent approval/review based on the requirements of your country or institution) were obtained? 856
28Answer: [NA] 857
Justification: The paper did not require any crowdsourcing or human subject for experimentation. 858
Guidelines: 859
•The answer NA means that the paper does not involve crowdsourcing nor research with human 860
subjects. 861
•Depending on the country in which research is conducted, IRB approval (or equivalent) may be 862
required for any human subjects research. If you obtained IRB approval, you should clearly state 863
this in the paper. 864
•We recognize that the procedures for this may vary significantly between institutions and 865
locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for 866
their institution. 867
• For initial submissions, do not include any information that would break anonymity (if applica- 868
ble), such as the institution conducting the review. 869
29