Local Anti-Concentration Class: Logarithmic Regret
for Greedy Linear Contextual Bandit
Seok-Jin Kim
Columbia University
New York, NY , USA
seok-jin.kim@columbia.eduMin-hwan Oh
Seoul National Univeristy
Seoul, South Korea
minoh@snu.ac.kr
Abstract
We study the performance guarantees of exploration-free greedy algorithms for the
linear contextual bandit problem. We introduce a novel condition, named the Local
Anti-Concentration (LAC) condition, which enables a greedy bandit algorithm
to achieve provable efficiency. We show that the LAC condition is satisfied by a
broad class of distributions, including Gaussian, exponential, uniform, Cauchy, and
Student’s tdistributions, along with other exponential family distributions and their
truncated variants. This significantly expands the class of distributions under which
greedy algorithms can perform efficiently. Under our proposed LAC condition, we
prove that the cumulative expected regret of the greedy algorithm for the linear
contextual bandit is bounded by O(poly log T). Our results establish the widest
range of distributions known to date that allow a sublinear regret bound for greedy
algorithms, further achieving a sharp poly-logarithmic regret.
1 Introduction
In the contextual multi-armed bandit problem [ 2,6,24,25], an agent uses revealed contextual
information in each round to decide which arm to pull, receiving a reward corresponding to the
pulled arm. The stochastic version of this problem observes rewards as random samples, with their
expectation tied to the arm’s contextual information. The agent’s goal is to design a sequential
arm-pulling strategy to maximize cumulative rewards, necessitating a balance between exploration
and exploitation. Linear contextual bandits, where expected reward is modeled as a linear function
of contextual information, serve as the fundamental framework for contextual bandits [ 1,10,26].
Various exploration strategies, including upper confidence bound (UCB) [ 1,11], Thompson sampling
(TS) [ 3,4], and ϵ-greedy [ 19] are widely used and studied in the theoretical analysis for linear
contextual bandits. However, exploration can often be challenging in practice, possibly leading to
over-exploration and performance deterioration. Some domains may find exploration infeasible or
even unethical, and it may appear unfair in applications such as healthcare and clinical domains.
Furthermore, exploration strategies tend to add complexity for algorithm designers and decision-
making systems.
A greedy policy, i.e., pure exploitation without exploration, selects arms greedily based on current
problem parameter estimates. While a greedy policy’s effectiveness cannot be guaranteed in general
since it may fail to find optimality in the worst case, the possibility of its favorable performances in
certain scenarios has been of interest both practically and theoretically. Therefore, understanding when
a greedy policy can perform effectively, i.e., when exploration is not needed, is a fundamental research
question. Recently, a simple greedy policy has been proved to achieve near-optimal regret bounds for
linear contextual bandit problems under some stochastic conditions of contexts [ 8,20,30,33,34].
Such efficient learning is possible if the greedy policy can benefit from suitable diversity in the contexts
(or the features of arms) — so that even with exploration-free action selection, parameter estimation is
38th Conference on Neural Information Processing Systems (NeurIPS 2024).effectively possible. However, distributions known in the existing literature to allow efficient greedy
algorithms are mostly limited only to Gaussian and uniform distributions [ 8,20,30,33]. Hence, the
following research questions arise.
Is it possible for a wider range of distributions to allow efficient learning for greedy algorithms?
If so, how can we characterize such distributions?
We answer the above questions affirmatively by proposing a general distributional condition that
allows for a broad range of distributions to achieve provable efficiency of greedy linear bandit algo-
rithms. In this work, we present a new Local Anti-Concentration (LAC) condition for distributions
that encompasses a wider range of context distributions compared to the previous findings. We
demonstrate that the class of distributions that satisfy LAC, which we denote as LAC class , include
Gaussian, exponential, Cauchy, Student’s t, and uniform distributions, as well as other exponential
family distributions and their truncated variants. This study provides the first evidence that greedy
algorithms can perform efficiently beyond Gaussian and uniform distributions. Our findings signifi-
cantly expand the class of admissible distributions that are suitable for greedy algorithms for linear
contextual bandits.
Our proposed LAC condition not only broadens the class of permissible distributions for greedy bandit
algorithms but also facilitates a sharper regret guarantee, achieving a poly-logarithmic O(poly log T)
regret for greedy algorithms. Our regret analysis constitutes a distinct improvement over the previously
known results for greedy linear contextual bandit algorithms. The existing results are primarily
categorized into two folds: (i) Gaussian-distributed contexts could only yield O(√
T)regret for greedy
algorithms for single-parameter linear contextual bandits [ 20,30,33]; (ii) Context diversity (e.g.,
Assumption 3 in [ 8]) alone was previously regarded as not sufficient to derive a poly-logarithmic regret
but additionally assuming a margin condition (e.g., Assumption 2 in [ 8]) can achieve O(poly log T)
regret.1In either case, there are limited prior results about context diversity beyond Gaussian and
uniform distributions. As for the margin condition, to the best of our knowledge, no prior work has
rigorously examined the scaling of the margin constant, rather than simply treating it as a universal
constant. To this end, we establish that Gaussian and uniform distributions as well as all of the
common distributions that satisfy the LAC condition (see Table 1) induce O(poly log T)regret
without having to additionally assume a margin condition.
The key difference between the analysis of greedy algorithms and that of exploration-based algorithms,
such as UCB and TS, for linear contextual bandits lies in the estimation bounds. While UCB [ 1]
and TS [ 4] analyses involve bounding the weighted estimation error of the parameter using self-
normalized martingales, the analysis of greedy algorithms relies on the ℓ2estimation bound in all
directions. Ensuring this estimation consistency is more challenging, especially when actions are
chosen adaptively, resulting in non-i.i.d. data.
In this work, we prove that for a broad class of context distributions,√
t-consistency of the estimator
can be guaranteed, enabling poly-logarithmic regret for greedy algorithms. Our newly proposed
class of context distributions represents the largest known class from which√
t-consistency of
the estimator can be derived, even with adaptively chosen (non-i.i.d.) contexts. To establish this
consistency, we derive two key technical results. First, we show that the minimum eigenvalue of the
Gram matrix increases sufficiently under the LAC condition. Additionally, we demonstrate that under
this condition, the suboptimality gap can be bounded probabilistically—a result derived from our
analysis rather than assumed explicitly.
1.1 Contributions
The main contributions of our paper are summarized as follows:
•We propose a novel condition, called Local Anti-Concentration (LAC) condition, for a
greedy linear contextual bandit algorithm to achieve provable efficiency. The newly proposed
1It is important to note that the problem setting of Bastani et al. [8](multiple parameters with shared context)
differs from our setting, which involves a single parameter with separate contexts, as is predominantly studied
in the linear contextual bandit literature [ 1,4,20,30,33]. While Bastani et al. [8]demonstrated the diversity
condition and the existence of a margin for Gaussian, uniform, and Gibbs distributions in the two-armed case
within multi-parameter settings, we show in Appendix K that the Gibbs distribution does not satisfy the diversity
condition when extended to cases with more than two arms.
2Table 1: Comparisons of Greedy Linear Contextual Bandit Studies
Results
Paper Context Distribution Regret Bound Problem Setting
Kannan et al. [20] Gaussian eO(√
T) Single parameter
Sivakumar et al. [33] Gaussian eO(√
T) Single parameter
Raghavan et al. [30] Gaussian eO(T1
3)†Single parameter
Bastani et al. [8]‡ GaussianO(poly log T)Multiple parametersUniform
This workGaussian
O(poly log T)Single parameterUniform
Laplace
Truncated exponential
Truncated Student’s t
Truncated Cauchy
PDFf∝exp(−π)with
polynomially growing π
†The regret bound of Raghavan et al. [30] is shown in the Bayesian regret, which is a weaker notion
of regret than the frequentist regret that we consider in our work.
‡ The problem setting of Bastani et al. [8] is a multi-parameter linear contextual bandit, where a
context vector is shared across the arms, but each arm has a separate parameter. Bastani et al. [8]
show that Gaussian and uniform distributions satisfy their covariate diversity condition
(Assumption 3 in [8]) and the margin condition. For the two-armed case ( K= 2), they also show
that Gibbs distribution satisfies those conditions. However, we show in Appendix K that Gibbs
distribution fails to satisfy the conditions for the general multi-armed case with K≥3.
LAC condition is satisfied by a wide rage of common distributions, including Gaussian,
exponential, Cauchy, Student’s t, and uniform distributions, and many common distributions,
as well as their truncated variants. This significantly expands the class of admissible
distributions that are suitable for greedy algorithms and is, to our best knowledge, by far the
largest class of distributions that induces efficient learning for greedy algorithms.
•Under our proposed LAC condition, we prove that the cumulative expected regret for the
greedy algorithm is bounded by O(poly log T)(Theorem 1), the sharpest known bound for
greedy algorithms in linear contextual bandits with a single parameter.
•By leveraging the proposed condition, we can guarantee both (i) the growth of the minimum
eigenvalue of the Gram matrix and (ii) a probabilistically bounded suboptimality gap. These
two steps are key technical components for analyzing greedy bandit algorithms and were
explicitly assumed in existing literature [ 8] to achieve poly-logarithmic regret. Notably,
we do not assume these steps; instead, we prove that distributions satisfying the LAC
condition inherently induce these two technical results (Theorems 2 and 3), which may be
of independent interest.
•Various context distributions have been empirically shown to allow favorable performances
of greey algorithms (see Appendix M). However, the distributions previously known in the
literature that enable efficient greedy algorithms were primarily limited to Gaussian and
uniform distributions. Our theoretical results offer a significant step toward bridging this gap
between theory and practice, providing insights into why greedy algorithms can be effective
under a wide range of distributions.
1.2 Related Work
Linear bandits and generalized linear bandits have been widely studied [ 1,2,4,6,10,11,18,23,27,
32]. Upper confidence bound (UCB) algorithms for the linear contextual bandit have been proposed
and analyzed for their regret performance [ 1,6,10,11,32]. Thompson sampling [ 35] algorithms for
linear contextual bandits have also been widely studied, with results demonstrating their effectiveness
3both theoretically and empirically [ 3,4,9]. While UCB [ 1] and Thompson sampling [ 3,4] analyses
rely on bounding the weighted estimation error of the parameter using self-normalized martingales,
the analysis of greedy bandit algorithms depends on the ℓ2estimation bound in all directions. Ensuring
this estimation consistency is more challenging, especially in adaptive action settings where data are
not i.i.d.
Recent studies [ 8,20,30,33] have shown that a greedy algorithm can achieve near-optimal regret
performance for linear contextual bandit problems under stochastic contexts by providing sufficient
conditions under which the greedy algorithm can be efficient. These conditions typically focus on the
diversity of the context distribution, ensuring that the greedy policy benefits from sufficient context
diversity for effective parameter estimation even without exploratory actions.
However, the existing literature has mainly limited itself to Gaussian [ 8,20,30,33] and uniform [ 8]
distributions, leaving open questions about broader applicability. Specifically, it is unclear if other
distributions could also support efficient greedy algorithms and what fundamental characteristics
these distributions should have to enable consistent parameter estimation without exploration. Our
work addresses this gap by identifying broader conditions under which diverse distributions can
effectively support greedy algorithms in linear contextual bandits.
2 Preliminaries
2.1 Notations
We use ∥x∥pto denote the ℓp-norm of vector x∈Rd. For a positive definite matrix A∈Rd×d,
we define ∥x∥A=√
x⊤Ax. We use λmin(A)to denote the minimum eigenvalue of the positive
definite matrix A. We denote Dd
R:= [−R, R]dandBd
R:={x∈Rd:∥x∥2≤R}. Ifdis
clear, we just write Bd
R:=BRandDd
R:=DR. We define [n]for a set [n] :={1,2, . . . , n }. We
writeSd−1for a d-dimensional unit sphere. We set ∥X∥ψ1= supp≥1{p−1E1/p|X|p},∥X∥ψ2=
supp≥1{p−1
2E1/p|X|p}for a random variable X. IfXis ad-dimensional random vector, then we
write∥X∥ψ2= sup∥u∥2=1∥⟨u, X⟩∥ψ2,∥X∥ψ1= sup∥u∥2=1∥⟨u, X⟩∥ψ1. We use the notation O()
or≲to hide constants, and eO()to hide constants and logarithmic terms. We use the notation a≍b
when a≲bandb≲a. We use c, c1, c2. . .for absolute constant, which may differ from line by line .
2.2 Linear Contextual Bandits with Stochastic Contexts
We consider the linear contextual bandit problem with Karms ( K≥2), where in each round
t= 1,2, . . . , T , the set of context vectors X(t) ={Xi(t)∈Rd, i∈[K]}is drawn from some
unknown distribution PX(t). Each arm’s feature Xi(t)∈ X(t)fori∈[K]need not be independent of
each other and can possibly be correlated. The agent then pulls an arm a(t)∈[K]. Each context vector
Xi(t)fori∈[K]is associated with stochastic reward Yi(t)∈Rwith mean Xi(t)⊤θ⋆where θ⋆∈Rd
is a fixed, unknown parameter. For simplicty, we assume ∥θ⋆∥2≤1. After pulling arm a(t), the agent
receives a stochastic reward Ya(t)(t)as a bandit feedback: Ya(t)(t) =Xa(t)(t)⊤θ⋆+ηa(t)(t), where
ηa(t)(t)∈Ris a zero mean noise. We assume that there is an increasing sequence of sigma fields
{Ht}such that each ηat(t)isHt-measurable with E[ηat(t)|Ht−1] = 0 . In our problem, Htis the
sigma field generated by random variables of the arms chosen {a(1), ..., a (t)}, their context vectors
{Xa(1)(1), ..., X a(t)(t)}, and the corresponding rewards {Ya(1)(1), ..., Y a(t)(t)}. Also, ηa(t)(t)is
assumed to be conditionally σ-sub-Gaussian, i.e., for all λ∈R,E[eληa(t)(t)| Ht−1]≤exp 
λ2σ2/2
forσ≥0. Observing context vector X(t), leta∗(t)denote the optimal arm in round t, that is,
a∗(t) = arg max i∈[K]Xi(t)⊤θ⋆. Then the instantaneous expected regret ( reg(t)) and cumulative
expected regret ( Reg(T)) are defined respectively as
Reg(T) :=TX
t=1reg(t) :=TX
t=1E
Xa⋆(t)(t)⊤θ⋆−Xa(t)(t)⊤θ⋆
which are respectively the instantaneous and cumulative differences between the optimal expected
reward and the expected reward of the pulled arms. The expectation is taken with respect to the
stochasticity of history, containing randomness of contexts. The goal of the agent is to minimize the
cumulative expected regret.
42.3 LinGreedy : Exploration-Free Algorithm for Linear Contextual Bandits
In this work, we focus on identifying sufficient conditions that enable exploration-free greedy
algorithms to efficiently learn the optimal policy. Specifically, we analyze a greedy algorithm for
linear contextual bandits, which we refer to as LinGreedy (Algorithm 1). The LinGreedy algorithm
selects arms greedily based on the OLS estimator, without any exploratory actions.
Algorithm 1 LinGreedy : Greedy Linear Contextual Bandit
Initialize Σ(0) = 0 ·Id, b(0) = 0,θ0∈Rd.
fort∈[T]do
while λmin 
Σ(t−1)
= 0do
Choose a(t) = arg max i∈[K]Xi(t)⊤θ0and observe reward Ya(t).
Update b(t) =b(t−1) +Xa(t)(t)Ya(t)andΣ(t) = Σ( t−1) +Xa(t)(t)Xa(t)(t)⊤.
end while
Choose a(t) = arg max i∈[K]Xi(t)⊤ˆθt−1and observe reward Ya(t).
Update b(t) =b(t−1) +Xa(t)(t)Ya(t)andΣ(t) = Σ( t−1) +Xa(t)(t)Xa(t)(t)⊤.
Update ˆθt= Σ(t)−1b(t).
end for
Description of Algorithm 1. The algorithm performs a greedy action in each round based on
estimated rewards. In the initial rounds, when the Gram matrix Σ(t)is not yet invertible, parameter
estimation is deferred, and the algorithm selects actions based on an initial parameter θ0. Once the
Gram matrix becomes invertible—which can be shown with high probability after sufficient time, the
algorithm computes an OLS estimator and performs a greedy action based on the estimated parameter
in each subsequent round. This algorithm is exploration-free. In the following sections, we present a
novel and more general condition that enables efficient learning for greedy algorithms.
3 Local Anti-Concentration Class
In this section, we introduce a new sufficient condition for efficient greedy contextual bandits. This
condition is general and encompasses a wide range of common distributions, including Gaussian,
exponential, uniform, Cauchy, and Student’s tdistributions, as well as their truncated variants. To
the best of our knowledge, this is the most extensive class of distributions considered in the greedy
contextual bandit literature [ 8,20,28,30,31,33], which has primarily focused on Gaussian, uniform
distributions, and their truncated variants.
Our proposed condition centers on the rate of the log density of stochastic contexts, a concept we
term Local Anti-Concentration (LAC). We now formally introduce the novel LAC class.
Definition 1 (Local Anti-Concentration (LAC)) A density function fXof a random variable
X∈Rnis said to satisfy the Local Anti-Concentration (LAC) condition with a non-decreasing
polynomial Lif
∥∇logfX(x)∥∞≤ L(∥x∥∞)
for all x∈Rn. We refer to Las the LAC function of X. We denote the class of distributions that
satisfy this LAC condition as the Local Anti-Concentration class.
3.1 Intuition of LAC Condition
The LAC condition implies that a density is not overly concentrated at any given point, leading to a
gradual decay in density across all directions—hence the term local anti-concentration. A geometric
interpretation of the LAC condition and a rigorous definition of this decay rate are provided in
Appendix D. Section 3.2 demonstrates that the LAC condition applies to a broad range of common
distributions. To the best of our knowledge, very few distributions have been previously shown to
support efficient performance guarantees for greedy algorithms. However, we prove that the LAC
condition holds for a wide range of distributions, including a variety of exponential families. Note
thatLcan be a constant when contexts have bounded support (see Appendix C). In the following
sections, we further explore the characteristics of the LAC condition.
53.2 Generality of LAC Condition
We show that the LAC condition is applicable to various distributions, significantly expanding the
class of admissible distributions for greedy linear contextual bandits. The LAC condition is satisfied
when the exponential component in the exponential family has a polynomial scale. Included in
the LAC class are distributions such as Gaussian, exponential, uniform, Student’s t, and Cauchy
distributions, along with their truncated variants.
The following proposition demonstrates that the LAC function does not directly depend on the
dimension of X. We further discuss in Appendix C that the LAC condition is more closely related
to the correlation structure of Xrather than its dimensionality. Therefore, we suggest that the LAC
condition provides a suitable framework for comparing regret when both the number of arms and the
dimension are large.
Proposition 1 Suppose the random variable X= (X1, X2), where X1∈Rn1andX2∈Rn2,
consists of two independent components. If X1andX2satisfy the LAC condition with functions L1(·)
andL2(·), respectively, then Xsatisfies the LAC condition with L(x) = max( L1(x),L2(x)).
This holds because, when we take the logarithm of the density, the independent coordinates decompose
as the sum of each density. Upon taking the gradient and evaluating the ℓ∞norm, the expression
decomposes perfectly. Using this proposition, the LAC condition remains robust across dimensions if
the coordinates are independent, making it dimension-free in such cases. Furthermore, this condition
is very accessible because it can be readily computed for a given density function. For many
well-known exponential families, the exponential component of the density often scales polynomially.
Examples of Distributions with LAC Condition
We present a few examples of known distributions satisfying the LAC condition. We provide rigorous
proofs for the examples in Appendix C.
•Gaussian distribution: For a Gaussian random variable X= (x1, . . . , x n)∼N(µ,Σ), if
Σis diagonal, it satisfies the LAC condition with L(x) =4
λmin(Σ)(∥x∥∞+∥µ∥∞). For the
general (non-diagonal) case of Σ, see Appendix C.
•Exponential distribution: The exponential distribution’s density fX(x) =1
λexp(−λx)
satisfies the LAC condition with a constant function L(x) =λ.
•Uniform distribution: The uniform distribution has constant density and satisfies the LAC
condition with a constant function L(x) = 1 .
•Student’s t-distribution: The1-dimensional Student’s t-distribution has density fX(x) =
Γ(ν+1
2)√νπΓ ν
2
· 
1 +x2
ν−(ν+1)/2and satisfies the LAC condition with L(x) =cνfor some
νdependent constant cν>0.
•Laplace distribution: The Laplace distribution has density f(x) =1
2bexp 
−|x−µ|
b
and
satisfies the LAC condition with L(x) =cfor some constant c >0.
If each coordinate’s density independently adheres to one of the aforementioned distributions,
according to Proposition 1, they all share the same LAC function irrespective of the dimension.
Consider the density f(x)withf(x)∝exp(−V(x))for some differentiable function V(x). If
∇V(x)has polynomial growth, i.e., is bounded by a polynomial, then the density f(x)meets the
LAC condition. This holds because f(x) =Cexp(−V(x)),∇logf(x) =∇log exp( −V(x)) =
−∇V(x). If∇V(x)exhibits polynomial-scale growth, then the supremum norm confirms that the
LAC condition is satisfied.
This observation makes the LAC condition easily verifiable for exponential family distributions
with density forms fX(x|θ) =h(x) exp[ η(θ)·T(x)−A(θ)], where the exponential part T(x)has
polynomial growth. In many exponential family cases, T(·)indeed exhibits polynomial growth.
Proofs and further details can be found in Appendix C.
4 Statistical Challenges of Greedy Linear Contextual Bandits
In this section, we outline the key statistical challenges in analyzing greedy algorithms for linear
contextual bandits: (i) ensuring the diversity of the adapted Gram matrix (Section 4.1) and (ii)
6bounding the suboptimality gap to achieve logarithmic regret (Section 4.2). For ease of exposition, we
use the vectorized context expression X(t) = (X⊤
1(t), . . . X⊤
K(t))∈RdKofX(t), which combines
context vectors Xi(t)fori∈[K]. We define Xij(t)as the j-th coordinate of the context Xi(t).
4.1 Diversity of Adapted Gram Matrix
The first key challenge lies in ensuring sufficient ℓ2-concentration of the estimator. This requires
sufficient eigenvalue growth of the Gram matrix, constructed from the policy-selected contexts. For
the OLS estimator used in Algorithm 1, if the minimum eigenvalue of the adapted Gram matrix
λmin(Σ(t))increases linearly with the number of rounds t, we can obtain the high probability ℓ2
error bound ∥ˆθt−θ⋆∥2with a convergence rate of O(1/√
t)using martingale concentration [ 13,29].
In fact, the growth of λmin(Σ(t))is a necessary condition to obtain√
t-consistency of the estimator.
However, estimating the covariance of the selected contexts Xa(t)(t)is relatively challenging, as its
distribution differs significantly from the overall distribution (before selection) of X(t). Some studies
have investigated the statistical properties of selected contexts [ 20,28]; however, the known results
are limited to specific distributions, such as arm-independent Gaussian and uniform distributions.
Therefore, it remains an open question whether the growth of λmin(Σ(t))can be ensured for a broader
class of distributions and what characteristics such distributions would need to satisfy.
4.2 Bounding Suboptimality Gap: Road to Logarithmic Regret
The next challenge that we face particularly in order to achieve logarithmic regret is to bound the
suboptimality gap. We first denote the suboptimality gap as the difference between the optimal
expected reward and the second highest expected reward:
∆(X(t)) := Xa⋆(t)(t)⊤θ⋆−max
i̸=a⋆(t)Xi(t)⊤θ⋆,
which is determined by the true parameter θ⋆. We aim to bound this suboptimality gap probabilisti-
cally, as described precisely in Challenge 2 of Section 4.3.
When this challenge is resolved, along with the growth of the minimum eigenvalue of the adapted
Gram matrix discussed in Section 4.1, we can achieve logarithmic expected regret using analysis
techniques for linear contextual bandit with stochastic contexts [ 7,19]. A high-level description of
the role of the margin constant in the regret bound is provided in Appendix D.1, with a rigorous
analysis in Appendix J.
4.3 Formal Statements of Two Key Challenges
As mentioned above, we encounter two primary challenges: ensuring the diversity of the chosen
contexts (i.e., the growth of the minimum eigenvalue of the Gram matrix of the selected contexts)
and bounding the suboptimality gap. Importantly, we do not assume these conditions to hold a priori;
rather, we will demonstrate that they are satisfied in the stochastic context under the LAC condition.
In this section, we formally define these challenges to be addressed.
Before delving into the formal statements for each of the two challenges, we first define the concept
of the diversity constant, which depends on the minimum eigenvalue of the adapted Gram matrix.
Definition 2 (Diversity Constant) For a linear contextual bandit with contexts X(t)and history
Ht−1, the diversity constant λ⋆(t)is defined as the value satisfying
E[Xa(t)(t)Xa(t)(t)⊤| Ht−1]⪰λ⋆(t)Id, (1)
for all t >0, where a(t)denotes the arm selected by the algorithm in round t.
Then, the first challenge is to ensure a positive diversity constant λ⋆(t)>0, which involves sufficient
eigenvalue growth of the adapted Gram matrix. We explore this further in Appendix F.1.
Challenge 1 (Positive Diversity Constant) Our goal is to ensure λ⋆(t)>0.
7Achieving a positive diversity constant is challenging, as it requires analyzing the behavior of a
context selected by the greedy policy in a specific direction rather than relying on the overall context
distribution. In Section 5.3.1, we demonstrate that the minimum eigenvalue of the Gram matrix grows
sufficiently, thereby ensuring a positive diversity constant.
We now formally state our second challenge of bounding the suboptimality gap.
Challenge 2 (Probabilistic Suboptimality Gap) We aim to bound the constant C∆(t), which holds
under the given history Ht−1and for any ε >0,
P
∆ 
X(t)
≤ε
≤εC∆(t) +1√
T. (2)
We also refer to this constant C∆(t)as the margin constant.
Note that Eq. (2)is a relaxed version of the margin condition presented in [ 5,7,8,19]. The
aforementioned literature explicitly assumes this condition to hold. However, we instead show that
the suboptimality gap can be bounded without directly assuming it (Section 5.3.2). Rigorously, C∆(t)
depends on T, as it is a function of T. However, we emphasize that our algorithm does not require
prior knowledge of T; this dependency is needed only for the analysis.
5 Regret Analysis
We present the main results of our paper. We prove that the regret of the greedy algorithm (Algo-
rithm 1) for linear contextual bandits can be bounded at a logarithmic scale in the time horizon T,
provided that the context distribution satisfies the LAC condition with a polynomial function L.
Assumption 1 (Independently distributed contexts) The context sets X(1), . . . ,X(T)are inde-
pendently distributed across time.
Discussion of Assumption 1. To the best of our knowledge, all analyses of greedy linear contextual
bandits assume the independence and identical distribution (i.i.d.) of context sets [ 8,20,28,33]. In
Assumption 1, we only require context sets to be independent; they may be non-identically distributed.
Additionally, much of the literature on linear contextual bandits that investigates√
t-consistency
of estimators also assumes independence of context sets [ 21,22]. Note that under Assumption 1,
context vectors within the same round are permitted to be dependent.
5.1 Considerations for Context Boundedness
We first provide detailed considerations on context boundedness. In the linear contextual bandit
setting, ℓ2boundedness is commonly assumed. However, for light-tailed distributions (such as
Gaussian or exponential), the ℓ2norm is unbounded. In such cases, a general approach in the
statistical literature is to assume bounded ψ1orψ2norms [ 14,17,37,38]. Therefore, we divide our
analysis into cases of bounded and unbounded contexts.
Bounded Contexts vs. Unbounded Contexts. In linear contextual bandit studies, boundedness of
theℓ2norm of contexts is commonly assumed. In this paper, for bounded contexts, we consider both
truncated contexts (e.g., truncated Gaussian, truncated Cauchy distributions) and naturally bounded
contexts (e.g., uniform distribution). For unbounded contexts, we assume a bounded ψ1norm, which
is a standard assumption for handling light-tailed distributions (e.g., as in [ 14,17], which assume
bounded ψ2norms).
Assumption 2 (Boundedness) For unbounded contexts, we assume ∥Xi(t)∥ψ1≤xmax. For
bounded contexts, we assume ∥Xi(t)∥2≤xmaxfor all i∈[K], t∈[T].
Discussion of Assumption 2 The bounded context assumption is widely used in the literature [ 1,7,
8,21,22,28]. For unbounded contexts, our assumption of ψ1boundedness is notably weaker than
the sub-Gaussianity (or ψ2) assumption commonly used in statistical regression literature to handle
random design covariates [ 14,39]. Ifℓ2boundedness holds, it automatically implies boundedness of
theψ1norm. However, as the analysis differs slightly depending on whether the support is restricted
to a bounded ball or is unbounded, we address these cases separately.
85.2 Regret Bound of LinGreedy for LAC Distribution
We first introduce our main result, the regret bound of the greedy algorithm under the LAC condition.
Theorem 1 (Regret bound of LinGreedy )Suppose X(t)satisfies the LAC condition with the poly-
nomial function Land also satisfies Assumptions 1 and 2 for all t. Then, the cumulative expected
regret of LinGreedy (Algorithm 1) is bounded by
Reg(T)≤ O(poly log T),
whereOconcerns only the dependency on T. Considering the dependency on dandK, for unbounded
contexts, we have
Reg(T)≤eO(d2.5).
For bounded contexts, refer to Appendix H for explicit results, as we consider several cases.
Discussion of Theorem 1. Theorem 1 states that if the contexts are drawn from a distribution in the
LAC class, the regret scales as O(poly log T). While our primary objective is not solely to achieve
the sharpest regret bounds, attaining poly-logarithmic regret is highly favorable. Our main goal is to
demonstrate that a large class of context distributions satisfies the LAC condition. When they do, a
simple greedy algorithm can suffice or even outperform exploration-based algorithms (see numerical
experiments in Section 6). The worst-case dependence on dandKfor bounded contexts is detailed
in Appendix H, where dependencies remain at most polynomial in dandK. As these dependencies
vary across distributions, refer to Appendix H for precise information. Proofs for unbounded contexts
are provided in Appendix G, with a proof sketch in Appendix D. Proofs for bounded contexts are
included in Appendix I.
Theorem 1 is the first result to expand the class of admissible distributions for greedy bandit algo-
rithms beyond Gaussian and uniform distributions. Our result demonstrates, for the first time, that
distributions in the LAC class inherently exhibit margin behavior, achieving sharp poly-logarithmic
regret without requiring an additional margin assumption. This finding is of independent interest
beyond the analysis of greedy bandit algorithms.
5.3 Proof Sketch of Theorem 1
We first present our key results for addressing each of Challenges 1 and 2 stated in Section 4.3.
5.3.1 Ensuring the Positive Diversity Constant
In this section, we present our key result for estimating lower bounds on the diversity constant for
densities that satisfy the LAC condition, therefore addressing Challenge 1. Theorem 2 is the analysis
under the case of unbounded contexts, where contexts have full support. A similar result is presented
in Appendix H for contexts with bounded or truncated support.
Theorem 2 (Diversity constant for unbounded contexts) If unbounded contexts X(t)has the LAC
condition with L(x) :=A1+A2xαand satisfies Assumption 2 for all t,
λ⋆(t)≥c11
d·1
(A1+A2(R1+ 2)α)2
holds for R1:=c2xmax(logd+ log K+ 2) . Here, c1, c2are absolute constants.
Discussion of Theorem 2. Theorem 2 implies that λ⋆(t)≥Ω(1
d), hence ensuring the growth of the
minimum eigenvalue of the adapted Gram matrix. In Appendix D.1, we discuss how1
λ⋆(t)factors into
the regret bounds. Note that αis generally small in many distributions. For example, for Gaussian
distributions, α= 1, and for exponential distributions, α= 0.
5.3.2 Bounding Suboptimality Gap
Next, we present our result addressing Challenge 2 by computing the suboptimality gap constant
C∆(t), which satisfies the inequality in Eq. (2)for every ε >0. By combining this condition with the
estimates for λ⋆(t), we can obtain an O(poly(log T))regret bound in terms of Tby applying the
analysis techniques of linear contextual bandits with stochastic contexts [7, 8, 19] to our setting.
9Theorem 3 (Suboptimality gap for unbounded contexts) In the same setup as Theorem 2,
C∆(t)≤c3√
d(A1+A23αRα
2)1
∥θ⋆∥2
holds for R2=c4xmax(1 + log K+ log d+1
2logT) + 1 and absolute constants c3, c4>0.
Discussion of Theorem 3. Note that the suboptimality gap constant C∆(t)is multiplied linearly in
regret bounds [5, 7, 19]. For bounded contexts, we present a similar result in the Appendix H.
5.3.3 Proof Intuitions
We provide a high-level proof overview of Theorem 1 in Appendix D. Note that once Challenges 1
and 2 are satisfied, achieving logarithmic regret becomes straightforward, as detailed in Appendix J.
The remaining task is to address these two challenges, with a particular focus on bounding the
constants λ⋆(t)andC∆(t). A key implication of the LAC condition is that the density decays slowly
at every point. Another useful property is that the LAC condition is preserved under conditioning,
meaning that X(t)| {X(t)∈A}also satisfies the LAC condition with the same function for any set
A⊂RK×d. This can be verified using the fact that logfX(t)|{X(t)∈A}(x) = log fX(t)(x)−logP[A],
where P[A]is a constant (see Appendix C.3 for details).
The main challenge of analyzing the statistical concentration in greedy linear contextual bandits lies
in the fact that the distribution of selected contexts Xa(t)(t)differs significantly from the distribution
of the overall (pre-selected) contexts X(t). The preservation of LAC under conditioning ensures that
LAC still holds when conditioning on the event of selecting arm i, enabling our analysis.
The full proofs for unbounded contexts are provided in Appendix G. For results on bounded contexts,
see Appendix H (and their proofs in Appendix I).
5.4√
t-Consistency of Estimator
In addition to achieving logarithmic regret, an independently valuable result is obtained: the ℓ2-
consistency of the estimator ˆθt. This is a property that even typical sublinear-regret algorithms,
such as UCB and TS, do not generally guarantee. Under the same setup as Theorem 1, we achieve
∥ˆθt−θ⋆∥2≤eO
d√
t
with high probability (see Corollaries 6 and 7). This additional result may
also facilitate analysis of sample complexity, such as PAC bounds.
6 Experiments
To validate our theoretical findings numerically, we conducted experiments using various context
distributions: Gaussian, Laplace, uniform, and truncated Cauchy distributions. We compared the
performance of LinGreedy with the LinUCB and LinTS algorithms. The results showed that
LinGreedy exhibited significantly superior regret performance compared to the other exploration-
based algorithms, achieving a logarithmic scale of regret. Detailed experimental results are provided
in Appendix M.
0 200 400 600 800 1000
Round (t)050100150200250300350400Cumulative Regret
d=20, K=20, Uniform
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)0100200300400Cumulative Regret
d=20, K=100, Laplace
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)050100150200250300Cumulative Regret
d=20, K=20, Truncated Cauchy
LinUCB
LinTS
LinGreedy
Figure 1: The cumulative regret plots of the numerical experiments. The full results are available in
Appendix M.
10Acknowledgements
This work was supported by the National Research Foundation of Korea(NRF) grant funded by the
Korea government(MSIT) (No. 2022R1C1C1006859, 2022R1A4A1030579, and RS-2023-00222663)
and by AI-Bio Research Grant through Seoul National University. The author thanks H.Choi for for
helpful comments.
References
[1]Abbasi-Yadkori, Y ., Pál, D., and Szepesvári, C. (2011). Improved algorithms for linear stochastic
bandits. Advances in neural information processing systems , 24.
[2]Abe, N. and Long, P. M. (1999). Associative reinforcement learning using linear probabilistic
concepts. In International Conference on Machine Learning , pages 3–11.
[3]Abeille, M. and Lazaric, A. (2017). Linear thompson sampling revisited. In Artificial Intelligence
and Statistics , pages 176–184. PMLR.
[4]Agrawal, S. and Goyal, N. (2013). Thompson sampling for contextual bandits with linear payoffs.
InInternational conference on machine learning , pages 127–135. PMLR.
[5]Ariu, K., Abe, K., and Proutière, A. (2022). Thresholded lasso bandit. In International Conference
on Machine Learning , pages 878–928. PMLR.
[6]Auer, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. Journal of
Machine Learning Research , 3(Nov):397–422.
[7]Bastani, H. and Bayati, M. (2020). Online decision making with high-dimensional covariates.
Operations Research , 68(1):276–294.
[8]Bastani, H., Bayati, M., and Khosravi, K. (2021). Mostly exploration-free algorithms for
contextual bandits. Management Science , 67(3):1329–1349.
[9]Chapelle, O. and Li, L. (2011). An empirical evaluation of thompson sampling. In Advances in
neural information processing systems , pages 2249–2257.
[10] Chu, W., Li, L., Reyzin, L., and Schapire, R. E. (2011). Contextual bandits with linear payoff
functions. Journal of Machine Learning Research .
[11] Dani, V ., Hayes, T. P., and Kakade, S. M. (2008). Stochastic linear optimization under bandit
feedback. In Proceedings of the 21st Annual Conference on Learning Theory , page 355–366.
[12] De Haan, L., Ferreira, A., and Ferreira, A. (2006). Extreme value theory: an introduction ,
volume 21. Springer.
[13] de la Pena, V . H., Klass, M. J., and Leung Lai, T. (2004). Self-normalized processes: exponential
inequalities, moment bounds and iterated logarithm laws.
[14] DUAN, Y . and WANG, K. (2023). Adaptive and robust multi-task learning. The Annals of
Statistics , 51(5):2015–2039.
[15] Durrett, R. (2019). Probability: theory and examples , volume 49. Cambridge university press.
[16] Evans, L. C. (2022). Partial differential equations , volume 19. American Mathematical Society.
[17] Fan, J., Wang, D., Wang, K., and Zhu, Z. (2019). Distributed estimation of principal eigenspaces.
Annals of statistics , 47(6):3009.
[18] Filippi, S., Cappe, O., Garivier, A., and Szepesvári, C. (2010). Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems , pages 586–594.
[19] Goldenshluger, A. and Zeevi, A. (2013). A linear response bandit problem. Stochastic Systems ,
3(1):230–261.
[20] Kannan, S., Morgenstern, J. H., Roth, A., Waggoner, B., and Wu, Z. S. (2018). A smoothed
analysis of the greedy algorithm for the linear contextual bandit problem. Advances in neural
information processing systems , 31.
[21] Kim, G.-S. and Paik, M. C. (2019). Doubly-robust lasso bandit. In Advances in Neural
Information Processing Systems , pages 5869–5879.
[22] Kim, W., Kim, G.-s., and Paik, M. C. (2021). Doubly robust thompson sampling for linear
payoffs. In Advances in neural information processing systems .
11[23] Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Ghavamzadeh, M., and Boutilier, C. (2020).
Randomized exploration in generalized linear bandits. In International Conference on Artificial
Intelligence and Statistics , pages 2066–2076.
[24] Langford, J. and Zhang, T. (2007). The epoch-greedy algorithm for contextual multi-armed
bandits. Advances in neural information processing systems , 20(1):96–1.
[25] Lattimore, T. and Szepesvári, C. (2019). Bandit Algorithms . Cambridge University Press
(preprint).
[26] Li, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web , pages 661–670. ACM.
[27] Li, L., Lu, Y ., and Zhou, D. (2017). Provably optimal algorithms for generalized linear
contextual bandits. In International Conference on Machine Learning , pages 2071–2080.
[28] Oh, M.-H., Iyengar, G., and Zeevi, A. (2021). Sparsity-agnostic lasso bandit. In Meila, M.
and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning ,
volume 139 of Proceedings of Machine Learning Research , pages 8271–8280. PMLR.
[29] Peña, V . H., Lai, T. L., and Shao, Q.-M. (2009). Self-normalized processes: Limit theory and
Statistical Applications . Springer.
[30] Raghavan, M., Slivkins, A., Vaughan, J. W., and Wu, Z. S. (2023). Greedy algorithm almost
dominates in smoothed contextual bandits. SIAM Journal on Computing , 52(2):487–524.
[31] Raghavan, M., Slivkins, A., Wortman, J. V ., and Wu, Z. S. (2018). The externalities of
exploration and how data diversity helps exploitation. In Conference on Learning Theory , pages
1724–1738. PMLR.
[32] Rusmevichientong, P. and Tsitsiklis, J. N. (2010). Linearly parameterized bandits. Mathematics
of Operations Research , 35(2):395–411.
[33] Sivakumar, V ., Wu, S., and Banerjee, A. (2020). Structured linear contextual bandits: A sharp
and geometric smoothed analysis. In International Conference on Machine Learning , pages
9026–9035. PMLR.
[34] Sivakumar, V ., Zuo, S., and Banerjee, A. (2022). Smoothed adversarial linear contextual bandits
with knapsacks. In International Conference on Machine Learning , pages 20253–20277. PMLR.
[35] Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in
view of the evidence of two samples. Biometrika , 25(3/4):285–294.
[36] Tropp, J. A. (2011). User-friendly tail bounds for matrix martingales. Technical report,
CALIFORNIA INST OF TECH PASADENA.
[37] Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data
science , volume 47. Cambridge university press.
[38] Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint , volume 48.
Cambridge University Press.
[39] Wang, K. (2023). Pseudo-labeling for kernel ridge regression under covariate shift. arXiv
preprint arXiv:2302.10160 .
12Appendix
Contents
1 Introduction 1
1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Preliminaries 4
2.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Linear Contextual Bandits with Stochastic Contexts . . . . . . . . . . . . . . . . . 4
2.3 LinGreedy : Exploration-Free Algorithm for Linear Contextual Bandits . . . . . . 5
3 Local Anti-Concentration Class 5
3.1 Intuition of LAC Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Generality of LAC Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4 Statistical Challenges of Greedy Linear Contextual Bandits 6
4.1 Diversity of Adapted Gram Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 Bounding Suboptimality Gap: Road to Logarithmic Regret . . . . . . . . . . . . . 7
4.3 Formal Statements of Two Key Challenges . . . . . . . . . . . . . . . . . . . . . . 7
5 Regret Analysis 8
5.1 Considerations for Context Boundedness . . . . . . . . . . . . . . . . . . . . . . . 8
5.2 Regret Bound of LinGreedy for LAC Distribution . . . . . . . . . . . . . . . . . . 9
5.3 Proof Sketch of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5.3.1 Ensuring the Positive Diversity Constant . . . . . . . . . . . . . . . . . . 9
5.3.2 Bounding Suboptimality Gap . . . . . . . . . . . . . . . . . . . . . . . . 9
5.3.3 Proof Intuitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5.4√
t-Consistency of Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
6 Experiments 10
A Additional Notations 16
B∥θ⋆∥2Dependency of Suboptimality Gap 16
C More Details of LAC: Distributions, Conditioning, & Truncation 16
C.1 LAC of Various Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
C.2 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
C.3 LAC and Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
C.4 LAC and Truncation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.5 LAC and Decay Rate of Density . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
13D High-Level Proof of Theorem 1 (Unbounded Contexts) 20
D.1 Regret Analysis: Overcoming Two Challenges . . . . . . . . . . . . . . . . . . . . 20
D.2 Tackling Two Challenges and Fixed History Arguments . . . . . . . . . . . . . . . 21
D.3 Starting with Truncation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D.4 Event Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D.5 Conditional Contexts are still in LAC . . . . . . . . . . . . . . . . . . . . . . . . 23
D.6 Remaining Goal: Bounding Decay Rate of Projected Contexts . . . . . . . . . . . 23
E Sections, Section Densities and Decay Rate 24
E.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
E.2 Sections and Section Densities . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
E.3 Decay Rate of Section Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
F Fixed History Results of Two Challenges for Unbounded Contexts 26
F.1 Details for Diversity Constant (Challenge 1) . . . . . . . . . . . . . . . . . . . . . 26
F.2 The Setup of Fixed History Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 27
F.3 Fixed History Results: Diversity Constant . . . . . . . . . . . . . . . . . . . . . . 27
F.4 Fixed History Results: Suboptimality Gap . . . . . . . . . . . . . . . . . . . . . . 30
G Proofs of Results for Unbounded Contexts 32
G.1 Constructing Truncation Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
G.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
G.2.1 Constructing Truncation Sets . . . . . . . . . . . . . . . . . . . . . . . . . 34
G.2.2 Properties of Truncated Contexts W . . . . . . . . . . . . . . . . . . . . 36
G.2.3 Applying Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
G.3 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
G.3.1 Constructing Truncation Sets . . . . . . . . . . . . . . . . . . . . . . . . . 36
G.3.2 Truncation with High-probability Region . . . . . . . . . . . . . . . . . . 37
G.3.3 Applying Proposition 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
G.4 Proof of Theorem 1: Unbounded Contexts Case . . . . . . . . . . . . . . . . . . . 38
H Results for Bounded Contexts 38
H.1 Two Cases of Bounded Contexts . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
H.2 Regret Bounds for Bounded Contexts . . . . . . . . . . . . . . . . . . . . . . . . 38
H.3 Concentration Parameters for Bounded Contexts . . . . . . . . . . . . . . . . . . . 39
H.4 Results for Two Challenges: Bounded Contexts . . . . . . . . . . . . . . . . . . . 41
I Proofs of Results for Bounded Contexts 42
I.1 Fixed History Arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
I.2 Sections of the Ball . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
I.3 Linear Section Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
14I.4 One-side Decay Rate of Linear Section Maps . . . . . . . . . . . . . . . . . . . . 44
I.5 Linear Section Maps between Sliced Balls, SR(v, y). . . . . . . . . . . . . . . . 45
I.6 Linear Section Maps for Double Slicde balls, SR(θ, b, v, y ). . . . . . . . . . . . . 46
I.6.1 Case v⊥θ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
I.6.2 Caseπ
2−τ0≤∠(v, θ)≤π
2. . . . . . . . . . . . . . . . . . . . . . . . . 47
I.6.3 Case 0≤∠(v, θ)≤π
2−τ0. . . . . . . . . . . . . . . . . . . . . . . . . 47
I.7 Proof of Proposition 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
I.8 Proof of Proposition 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
I.9 Proof of Proposition 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
I.10 Proof of Proposition 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
I.11 Proof of Corollary 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
I.12 Proof of Corollary 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
I.13 Proof of Corollary 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
J Analysis After Challenges 1 and 2 are Satisfied 51
J.1 Proof of Proposition 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
J.2 Concentration of Sub-exponential Contexts . . . . . . . . . . . . . . . . . . . . . 53
J.3 Proof of Proposition 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
J.3.1 Gram Matrix Concentration . . . . . . . . . . . . . . . . . . . . . . . . . 53
J.3.2 Good Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
J.3.3 Bounding Regret by the Peeling Technique . . . . . . . . . . . . . . . . . 55
K Discussion on Discrete-Supported Contexts 56
L Dicussions, Limitations and Further Ideas 57
M Numerical Experiments 57
N Technical Lemmas 59
N.1 Concentration Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
15A Additional Notations
We define additional notation. For v∈Rd, define Pv:={x|x⊤v= 0}. For S⊂Rn, we define
∥S∥∞:= sup {∥x∥∞, x∈S}. For the intervals I⊂Randv∈Rd, define Iv:={x|x⊤v∈I}.
For instance, [−1,1]v:={x| −1≤x⊤v≤1}. We define πV(·)as the projection to the subspace V.
Also, in the appendix, we write BR=Bd
R={x∈Rd| ∥x∥2≤R}with slight abuse of notations.
For random variable X, we define supp( X)as the support of X. Recall that we define the expected
regret in round tasreg(t)and we define unexpected regret as reg′(t). Also for Ω =S
i∈IEifor
disjoint events Ei, we use following notations
E[E[X|Ei]] :=X
i∈IP[Ei]E[X|Ei].
For event A, we define P[X;A],E[X;A]asP[X∩A]andE[X∩A]by following Durrett [15]’s
notation.
B∥θ⋆∥2Dependency of Suboptimality Gap
In the whole proof, we first bound the margin constant C∆by assuming ∥θ⋆∥2= 1. If we get C∆for
∥θ⋆∥2= 1, then for general case, if ∥θ⋆∥2=κ, observe that
P[Xa⋆(t)(t)⊤θ⋆−max
j̸=a⋆(t)Xj(t)⊤θ⋆≤ε] =P[Xa⋆(t)(t)⊤θ⋆/κ−max
j̸=a⋆(t)Xj(t)⊤θ⋆/κ≤ε/κ]
(3)
=C∆
κε+1√
T(4)
holds. So from now on, we assume ∥θ⋆∥2= 1and we adjust it later. Also, we highlight that we only
need to bound C∆it for ε <1
2, since if ε >1
2, it holds with C∆= 2.
C More Details of LAC: Distributions, Conditioning, & Truncation
In this section we provide the omitted details of LAC and its properties.
C.1 LAC of Various Distributions
We first provide more details on the LAC of various distributions, presented in Section 3.2.
Gaussian. For density f(x) = Cexp(−(x−µ)⊤V(x−µ)), x∈Rn, where V=1
2Σ−1,
∇logf= 2V⊤(x−µ). First, when Vis diagonal, after taking log and gradient, we get
∥∇logf(x)∥∞≤2λmax(V)∥x−µ∥∞≤2λmax(V)(∥x∥∞+∥µ∥∞).
Since V=1
2Σ−1, we get the wanted results. For general V, we can see that
∥∇logf(x)∥∞≤2(max
i∥Vi∥1)∥x−µ∥∞
where Viisi-th row of V.
LAC of bounded support contexts. If the support of the contexts is bounded in the compact set,
∇logfis bounded when it is continuous. Therefore, in this case, LAC can be a constant function.
Also, if ∥x∥2≤R, we have ∥∇logf(x)∥∞≤ L(R)by monotonicity of L(·).
LAC and context correlations. We claim that LAC L(·)function of random vector x=
(x1, . . . x n)related to the correlation structure of xi, xj, rather then dimension nitself. Consider
f(x)∝exp(−V(x)). Then,
∇logf(x) =−∇V(x)
16and
∂ilogf(x) =−∂iV(x)
holds. Since Lis defined in the sense of supremum norm, to investigate LAC function, the maximum
value: max i∥∂iV(x)∥∞is important. We claim that it is a dimension-free property, and the
correlation structure is more important rather than dimension nitself. For instance, if x= (x1, . . . x n)
is coordinate-wise independent, we have V(x) =V1(x1) +V2(x2) +···+Vn(xn). Hence
∥∇V(x)∥∞= max
i∥V′
i(xi)∥∞
and we can see it is dimension-free value.
LAC with shifted mean distribution. Let the mean zero contexts X’s density as f(x)with LAC
fuction L(·). Then, the shifted contexts with mean µhas density g(x) =f(x+µ)and see that
∥∇logg(x)∥∞=∥∇logf(x+µ)∥∞
≤ L(∥x+µ∥∞)
≤ L(∥x∥∞+∥µ∥∞).
Hence the density g(x)has LAC with function L′(x) =L(x+∥µ|∞)and when ∥µ∥∞=O(1), it
has the same rate. Thus, LAC does not require that contexts be mean zero, and LAC can be defined
for distributions with a general mean.
C.2 Proof of Proposition 1
IfX= (X⊤
1, X⊤
2)andX1, X2are independent, the fX, density of Xcan be decomposed as
fX((x1, x2)) =fX1(x1)fX2(x2).
Then,∇logfX((x1, x2)) =∇(logfX1(x1)+log fX2(x2)) = (∇x1logfX1(x1),∇x2logfX2(x2))
holds, and when taking the supremum norm, we get
∥∇logfX((x1, x2))∥∞= max 
∥∇x1logfX1(x1)∥∞,∥∇x2logfX2(x2)∥∞
≤max 
L1(∥x1∥∞),L2(∥x2∥∞)
≤max(L1(∥(x1, x2)∥∞),L2(∥(x1, x2)∥∞)).
By the definition of the LAC function, we finally obtain the desired result: fhas LAC with the
function L(x) = max( L1(x),L2(x))forx∈R.
C.3 LAC and Conditioning
First, we observe that the LAC condition is maintained under conditioning. Let the density of
X= (X⊤
1, . . . X⊤
K)∈RdKbef(·). For event E:={X∈A}forA⊂RdK, the conditional density
ofXgiven event Eis formulated as f|E(x) =f(x)
P[E]∈Rdforx∈RdKand the following holds:
∇logf|E(x) =∇logf(x)− ∇P[E] =∇logf(x).
It means X|Eis also LAC with the same function, hence LAC is robust with conditioning.
Especially, if event Ehas a form of E={Xi∈DandXj=xjforj̸=i}for some D⊂Rd, we
define the density of Xi|Easfi,E(x) =f(x1,...xi−1,x,xi+1,...xK)
P[E]forx∈Rd. Then,
∇xlogfi,E(x) =∇xlogf((x1, . . . x i−1, x, x i+1, . . . x K))− ∇P[E]
=∇xlogf((x1, . . . x i−1, x, x i+1, . . . x K))
holds. We have
∥∇xlogfi,E(x)∥∞≤ L(∥(x1, . . . x i−1, x, x i+1, . . . x K)∥∞)
which tell us the conditional density also enjoys LAC property with L(·).
We summarize above observations in the following lemma, using L(·)is non-decreasing function.
Lemma 1 (LAC of conditional contexts) For random vector X= (X⊤
1, . . . X⊤
K)∈RdK, let it’s
density has LAC with function L(·). Then for event E:={X∈A}forA⊂RdKwith∥A∥∞≤R,
the conditional random vector X|Ehas LAC with constant function L(R). Especially, if event
Ehas a form of E={Xi∈DandXj=xjforj̸=i}for some D⊂Rdwith∥D∥∞≤R,
conditional random vector Xi|Ehas LAC with L(R).
17C.4 LAC and Truncation
We introduce the property that can compute the LAC of truncated contexts. Truncating the contexts
Xi(t)to ad-dimensional ball BR⊂Rdfor every i∈[K]still satisfies the LAC with a constant
function, L(R).
Lemma 2 (LAC with truncation) Suppose the density of contexts X∈RdKsatisfies the LAC
condition with the function L(·). Consider the case we truncate Xinto the region (BR)Kand define
truncated contexts as X. Then Xsatisfy the LAC condition with constant function L(R).
Proof The density of truncated contexts is calculated as fX(x) =fX(x)
P[(BR)K]. Then, when taking the
log and gradient, we get ∇logfX(x) =∇logfX(x)holds. Since ∥x∥2≤Rforx∈(BR)K, and
Lis defined by non-decreasing function, we get ∇logfX(x)≤ L(R).
C.5 LAC and Decay Rate of Density
Next, we rigorously define the decay rate of a density.
Definition 3 (Decay rate) For a density f(x), x∈Rd, and for a set A⊂Rd, we say it has decay
rateM > 0when
f(x1)
f(x2)≥exp(−M|x1−x2|).
for all x1, x2∈A.
We next present lemma that the density with a bounded LAC function Lhas a bounded decay rate in
every direction. This property is especially used to control the projected contexts in the whole paper.
Lemma 3 (Decay rate and LAC) Suppose a density fis defined in a domain D⊂Rdand satisfies
the LAC condition with constant function L ∈R. Then the decay rate of finDis bounded by√
dL.
Proof By using Cauchy-Schwarz inequality, we can bound the directional derivative for any v∈Sd−1
as
∂vf(x)
f(x)=v⊤∇f(x)
f(x)≤ ∥v∥2∥∇f(x)
f(x)∥2=∥∇f(x)
f(x)∥2≤√
d∥∇f(x)
f(x)∥∞≤√
dL.
Next, by applying Gronwall inequality Lemma 22, we get
f(x+hv)
f(x)≥exp(−√
dLh)
for any x, x+hv∈D. Since h, vis arbitrary, we get the wanted result.
For univariate function, we define one-side decay rate, which is weaker quantity then decay rate.
Definition 4 (One-side decay rate) For univariate function g, we say it has one-side decay rate M
in set Awhen for all y, y′∈A, y < y′
g(y′)
g(y)≥exp(−M(y′−y)).
Using these observations, we can see that the LAC condition gives the upper bound of the decay
rate of density. For the one-dimensional case, for instance, for density with f(x)∝exp(−Mx)has
the decay rate M. Then we can bound the lower bound of variance as1
M2scale and the maximum
density is bounded by M. Since Challenge 1 is related to the variance lower bound and Challenge 2
is related to the maximum density (of suboptimality gap), we aim to investigate the decay rate of
contexts density.
We first present our key lemma, which gives relation between one-side decay rate and variance lower
bound. In the following part, we present the rigorous relations between LAC, (one-side) decay rate,
variance lower bound and maximum density.
18Lemma 4 (One-side decay rate and variance lower bound) Consider the density g(·)of random
variable Y∈Rwith support I= [a, b]withb >1
Mfor some M > 0. Suppose g(·)satisfies
g(y′)
g(y)≥exp(−M|y−y′|)for all y, y′∈I∩[−1
M,1
M],y < y′. Then we get
E[Y2]≥c1
M2
for some absolute constant c >0.
Proof Using Lemma 5, we get the density gis bounded by 3Min the interval [−1
2M,1
2M]∩I. By
applying Lemma 6, we get the wanted result. If I∩[−1
M,1
M] =∅orI∩[−1
2M,1
2M] =∅, it means
that|Y| ≥1
2Malmost surely and we get the result directly.
Next, we present another key lemma that gives relation between one-side decay rate and maximum
density.
Lemma 5 (One-side decay rate leads maximum density) Suppose a real-valued random variable
Yhas density gand for y0∈R,[y0, y0+1
2M]⊂supp( Y)for some M > 0. Ifgsatisfies
g(y0+h)
g(y0)≥exp(−Mh)holds for any 0≤h <1
2M, then g(y0)≤3M.
Proof Since the integral of the density is 1, we get
1 =Z
y∈supp( g)g(y)dx
≥g(y0)Zy0+1
2M
y0g(y)
g(y0)dx
≥g(y0)Zy0+1
2M
y0exp(−M(y−y0))dx
≥g(y0)1
3M
therefore
g(y0)≤3M
holds.
Lemma 6 (Maximum density leads variance lower bound) LetYbe an univariate random vari-
able and set A= supp( Y). For an interval I= [−1
3M,1
3M], if density of YinI∩Ais bounded by
M, then we have
E[Y2]≥c1
M2.
Proof First, since the density is bounded by M,
P[Y∈A∩I]≤2
3M×M=2
3.
Therefore, with a probability greater than1
3,Y∈A∩ {|x|>1
3M}. Therefore,
E[Y2]≥1
3(1
3M)2=1
27M2.
19D High-Level Proof of Theorem 1 (Unbounded Contexts)
In this section, we briefly outline the high-level proof of Theorem 1 for unbounded contexts. This
overview provides a summary and high-level sketch of the proofs, with full details presented in
Appendices F and G.
We begin by explaining how overcoming the two primary challenges (discussed in Section 4) lead
to a logarithmic regret bound in the proof of Theorem 1. Next, we provide a sketch of the proofs
for the two key theorems: Theorem 2 for lower-bounding the diversity constant and Theorem 3 for
upper-bounding Suboptimality gap.
For bounded contexts, the result statements are given in Appendix H, and the full proofs are in
Appendix I. Since the proof for bounded contexts follows a similar approach to that of unbounded
contexts, we start with a proof sketch for the unbounded case.
D.1 Regret Analysis: Overcoming Two Challenges
In this section, we briefly present our proof sketch for the regret bounds. We describe how we can
achieve logarithmic regret addressing the two challenges: Challenges 1 and 2.
Addressing Challenge 1, we can obtain the O(1√
t)rateℓ2bound of the parameter ˆθtandθ⋆by
using the combination of the self-normalized concentration [ 1] and properties that Σt⪰1
4λ⋆tholds
with high probability (by using Corollary 9). The details are in Appendix J. Hence, by tackling
Challenge 1, ∥ˆθt−θ⋆∥2≤c√dlogT√λ⋆tholds with high probability. The following part describes how
we can get logarithmic regret addressing Challenge 2. First, simply assume the contexts are bounded,
such as ∥Xi(t)∥2≤xmax. Later, in our main proof, we also modify it to the relaxed condition
∥Xi(t)∥ψ1≤xmax.
Challenge 1:√
t-rate ℓ2concentration. Ensuring the first Challenge 1 can lead the ℓ2statistical
resolution of the estimator. Hence, we can get
|X⊤
a(t)(ˆθt−1−θ⋆)| ≤cxmax√dlogTp
λ⋆×(t−1),|X⊤
a⋆(t)(ˆθt−1−θ⋆)| ≤cxmax√dlogTp
λ⋆×(t−1)
holds with high probability. Details are in Appendix J.1.
However, this resolution in insufficient for the logarithmic regret, it can only makes O(√
T)regret
bound.
Challenge 2: Towards logarithmic regret. Furthermore, addressing Challenge 2 (margin condi-
tion), we can get logarithmic expected regret upper bound. When the greedy policy select a(t), it
means that
Xa(t)(t)⊤ˆθt−1≥X⊤
a⋆(t)ˆθt−1
and by the definition of the optimal arm,
Xa(t)(t)⊤θ⋆≤X⊤
a⋆(t)θ⋆
holds. Ensuring Challenge 1, we get
reg′(t) :=Xa⋆(t)(t)⊤θ⋆−Xa(t)(t)⊤θ⋆≤2cxmax√dlogTp
λ⋆×(t−1).
Next, we define the event Eas the event of X(t)with regret occurring as reg′(t)>0. Under the
event E, the suboptimality gap of X(t)satisfies
∆(X(t))≤reg′(t)≤2cxmax√dlogTp
λ⋆×(t−1)
and by overcoming Challenge 2, we get
PX(t)[reg′(t)>0]≤C∆×2cxmax√dlogTp
λ⋆×(t−1)+1√
T≤3cxmaxC∆√dlogTp
λ⋆×(t−1)
20By combining two things, we can bound the expected regret as
EX(t)[reg′(t)]≤3cxmaxC∆√dlogTp
λ⋆×(t−1)×2cxmax√dlogTp
λ⋆×(t−1)
= 6c2x2
maxC∆dlogT
λ⋆×(t−1).
This observation enable us to get logarithmic regret bound. Using this argument, our only remaining
goal is bounding two constants in Challenge 1 and 2. We summarize our above observation in the
following Lemma.
Lemma 7 Assume that ∥Xi(t)∥2≤Rfor some R > 0for all i∈[K]. Also assume that the
estimator ∥ˆθt−1−θ⋆∥ ≤A1√t−1holds for some constant A >0. Then the (unexpected) regret in
round tis bounded by reg′(t)≤2AR1√t−1. Furthermore, under the margin condition (Challenge 2),
we get EX(t)[reg′(t)]≤6R2A2C∆1
t−1to hold.
Using the above observation, we can get a logarithmic regret bound when ∥Xi(t)∥2is bounded. For
bounded ∥Xi(t)∥ψ1case, we use the peeling technique. Given the history Ht−1,ˆθt−1−θ⋆is a fixed
vector, and using the results from Appendix J.2, we can bound |Xi(t)⊤v|, v∈ {ˆθt−1−θ⋆, θ⋆}with
high probability. Hence we can do the similar arguments, and details are presented in Appendix J.
D.2 Tackling Two Challenges and Fixed History Arguments
Now, we summarize our proof strategy to prove diversity (Challenge 1) and suboptimality gap
(Challenge 2). First, we consider the problem with fixed history setup, which do analysis under
the given history Ht−1. We set history-conditioned contexts as X:=X(t)| Ht−1andX=
(X⊤
1, . . . X⊤
K), Xi∈Rd. We describe more about this history fixing in the next Appendix F. Under
the given event Ht−1,ˆθt−1is a deterministic value and no longer random. Thus, the policy a(t),
conditioned on Ht−1, is a greedy policy with respect to ˆθt−1, making it deterministic as well. To
address any ˆθt−1, we propose an analysis that applies to any θ∈Rd. In Appendix F.1, we argue
that it suffices to bound the variance of selected contexts of greedy policy w.r.t. any fixed θ. Since
we fix θ, which is the corresponding value of ˆθt−1under the given history Ht−1, we define the
policy-selected arm a= arg max X⊤
iθ. To address Challenge 1, our first goal is to find the lower
bound of E[XaX⊤
a]. Next, recall that we defined ∆(X) :=X⊤
a⋆θ⋆−max i̸=a⋆X⊤
iθ⋆and we aim to
bound this suboptimality gap. Next, we introduce our two important goals to achieve two challenges.
Goal 1. For any θ, v∈Sd−1and greedy policy w.r.t. θ, we aim find the lower bound of
E[vXaX⊤
av].
Goal 2. For fixed θ⋆, our aim is to find the upper bound of C∆that satisfies
P[∆(X)≤ε]≤C∆ε+1√
T.
It is straightforward that when we achieve Goal 1, then Challenge 1 can be achieved easily since
Goal 1 is preparing for all greedy policy with θ.
D.3 Starting with Truncation
Before we tackle two above goals, we start by truncating our contexts Xwith high-probability
regions. Since Xihas bounded ψ1norm as xmax, roughly speaking, there exists set D⊂Rdwith
∥D∥∞=eO(1)and
P[X∈DK]≈1.
We can view Xas a mixture of X| {X∈DK}andX| {X∈(DK)c}and with high probability,
Xis sampled from the distribution X| {X∈DK}. In this section (Appendix D), since we are
21providing proof sketch, so we just regard Xis sampled from the distribution with bounded support
DKwhere ∥D∥∞=eO(1). At Appendix C.3, we observed that X| {X∈DK}also has LAC
and since ∥D∥∞=eO(1), it has constant LAC with L(∥D∥∞) =L(eO(1)) = eO(1)sinceLis
polynomial.
So only for this section, we assume that Xhas bounded support DKandit has bounded constant
LAC function L=eO(1).For rigorous proof and justification, we provide them throughout
Appendix F and Appendix G.
D.4 Event Decompositions
Next, we start to bound the constants in two challenges. Before that, we present event deompositions
for our analysis.
Definition 5 (Event decomposition 1) We define the event {a=i}asΩiandΩi({xj}j̸=i) :=
{a=i} ∩ {Xj=xjfor all j̸=i}.
Pick and v∈Sd−1. We can decompose the variance term as
E[v⊤XaX⊤
av] =E[E[v⊤XaX⊤
av|Ωi]]
=E[E[v⊤XiX⊤
iv|Ωi]]
=E[E[v⊤XiX⊤
iv|Ωi({xj}j̸=i)]].
For notations E[E[· | ·]], please refer Appendix A. From now on, we aim to bound
E[v⊤XiX⊤
iv|Ωi({xj}j̸=i)] (5)
for any i,{xj}j̸=i. Hence we investigate the property of the conditional density
Xi|Ωi({xj}j̸=i)
and especially we are interested at density of projected conditional contexts
X⊤
iv|Ωi({xj}j̸=i). (6)
We define new event decompositions for bounding suboptimality gap. Under the fixed history, set
a⋆:= arg max i∈[K]X⊤
iθ⋆.
Definition 6 (Event decomposition 2) We define the event {a⋆=i}asΩ⋆
iandΩ⋆
i({xj}j̸=i) :=
{a⋆=i} ∩ {Xj=xjfor all j̸=i}.
Next, we aim to bound C∆, which is another key constant in the Challenge 2. Define the optimal arm
a⋆= arg max X⊤
iθ⋆and suboptimal arm a′= argmaxi̸=a⋆X⊤
iθ⋆. By the definition, for any ε >0,
we have to calculate
P[∆(X)≤ε] =P[X⊤
a⋆θ⋆−X⊤
a′θ⋆≤ε]
=E[P[X⊤
a⋆θ⋆−X⊤
a′θ⋆≤ε|Ω⋆
i]]
=E[P[X⊤
a⋆θ⋆−X⊤
a′θ⋆≤ε|Ω⋆
i({xj}j̸=i)]]
=E[P[X⊤
iθ⋆−max
j̸=ix⊤
jθ⋆≤ε|Ω⋆
i({xj}j̸=i)]]
then we aim to bound
P[max
j̸=ix⊤
jθ⋆≤X⊤
iθ⋆≤max
j̸=ix⊤
jθ⋆+ε|Ω⋆
i({xj}j̸=i)]
and hence we investigate the properties of the conditional density
X⊤
iθ⋆|Ω⋆
i({xj}j̸=i). (7)
It is enough to bound the maximum density of X⊤
iθ⋆|Ω⋆
i{xj}j̸=isince if it is bounded by U, we
can see that
P[max
j̸=ix⊤
jθ⋆≤X⊤
iθ⋆≤max
j̸=ix⊤
jθ⋆+ε|Ω⋆
i({xj}j̸=i)]≤Uε
holds and we can set C∆=U.
22D.5 Conditional Contexts are still in LAC
In the previous Appendix C.3 and Lemma 1, we saw that LAC is preserved for conditioning. Our
conditioned contexts for interest, Xi|Ωi({xj})andXi|Ω⋆
i({xj})is conditioning with events such
asXi| {Xj=xjfor all j̸=i, X⊤
iθ≥max j̸=ix⊤
jθ}, which meets condition of Lemma 1. Then
we can apply Lemma 1 and we get that these two conditional density
Xi|Ωi({xj}), X i|Ω⋆
i({xj})
alsohave LAC with bounded constant function L.
D.6 Remaining Goal: Bounding Decay Rate of Projected Contexts
Lemma 4 tell us that bounded one-side decay rate of density guarantees the lower bound of the
variance. Lemma 5 tell us the bounded one-side decay rate of density guarantee the maximum density.
In summary, we need to find the lower bound of equation (6)and the maximum density of (7). We
present Lemma 4 and 5, which tell us that if we can bound the one-side decay rate, then we can
bound variance of (6)and maximum density of (7). By the property that LAC is preserved under
conditioning, we can bound the decay rate of Xi|Ωi({xj}j̸=i)andXi|Ω⋆
i({xj}j̸=i)by√
dL, by
combining Lemma 3 and LAC property of conditional contexts. However, our interests are projected
contexts, defined as
X⊤
iv|Ωi({xj}j̸=i)
and
X⊤
iθ⋆|Ω⋆
i({xj}j̸=i)
which are projected contexts with some direction vandθ⋆. In the remaining part, we aim to bound
theone-side decay rate of these projected random variables densities to apply Lemma 4 and 5.
To summary, we proceed with the following steps:
1. Event decomposition by conditioning.
2. We know Xi|Ωi({xj}j̸=i),Xi|Ω⋆
i({xj})has bounded decay rate√
dL.
3.We aim to bound decay rate of projeced density X⊤
iv|Ωi({xj}j̸=i)andX⊤
iθ⋆|
Ω⋆
i({xj}j̸=i).
4. Bounding two key constants in two Challenges using Lemma 4 and 5.
Support of Conditional Densities. We first observe the support of the conditional density Xi|
Ωi({xj}j̸=i). By the definition of Ωi, the arm ishould be optimal under the greedy policy θin this
event. Therefore, its support is restricted to {x∈D|x⊤θ≥max j̸=ix⊤
jθ}.
Next, we consider the density of projected contexts, X⊤
iv|Ωi({xj}j̸=i). Since the support of
Xi|Ωi({xj}j̸=i)is of the form {x∈D|x⊤θ≥max j̸=ix⊤
jθ}, our projected density is an
integrated form:
P[X⊤
iv=y|Ωi({xj}j̸=i)] =Z
{x∈D|x⊤θ≥maxj̸=ix⊤
jθ}∩{x|x⊤v=y}P[Xi=x|Ωi({xj}j̸=i)]dx.
Geometrically, this represents the total density at the intersection of the hyperplanes {x|x⊤v=y}
and the set {x∈D|x⊤θ≥max j̸=ix⊤
jθ}. We refer to these as section densities, as they represent
the total density of sections sliced by the hyperplane {x|x⊤v=y}. Further discussion is provided
in Appendix E.
Conclusion. Later in Appendix F and Appendix G, since Xi|Ωi({xj}j̸=ihas bounded decay
rate√
dL=eO(√
d), we prove that the projected densities also have bounded one-side decay rate of
eO(√
d). Lastly, by applying Lemma 4 and can prove the quantity
E[v⊤XiX⊤
iv|Ωi({xj}j̸=i)]≥c1
d
for some c=eO(1). Also, using Lemma 5, we can prove that the density of (7)has bounded density√
dLand we can prove that C∆=eO(√
d).
23E Sections, Section Densities and Decay Rate
In this section, we define sections and section densities and investigate their properties. Previously in
Appendix D, we discussed decomposition by conditioning, and our interest became the properties
of conditioned contexts, the form of Xi|Ωi({xj}j̸=i)andXi|Ω⋆
i({xj}j̸=i). Our first goal is to
bound the variance of projected contexts, such as X⊤
iv|Ωi({xj}j̸=i)for all v∈Rd. To do that, we
investigate the projected density of X⊤
iv=y|Ωi({xj}j̸=i). To apply Lemma 4 and Lemma 5, we
only need to bound the one-side decay rate of that projected density. We saw that its density is the
total density of intersections with {x|x⊤θ≥max j̸=ix⊤
jθ}and hyperplane {x|x⊤v=y}. We
can view it as sections sliced by hyperplanes, and we provide some theory related to sections and
section densities.
E.1 Motivation
In the proof strategy (Appendix D), we aims to bound the one-side decay rate and maximum density
of projected conditional density, X⊤
iv|Ωi({xj}j̸=i)andX⊤
iθ⋆|Ω⋆
i({xj}j̸=i). Recall that we
define one-side decay rate of univariate density g(Definition 4) at Appendix D as a constant M
satisfying
g(y′)
g(y)≥exp(−M(y′−y))
for all y < y′.
Recall that density of X⊤
iv|Ωi({xj}j̸=i)andX⊤
iθ⋆|Ω⋆
i({xj}j̸=i)are density of sections. If
the support of Xi|Ωi({xj}j̸=i)isA, then the density of X⊤
iv|Ωi({xj}j̸=i)becomes the total
density in the sections A∩ {x|x⊤v=y}. This argument also works for the Xi|Ω⋆
i({xj}j̸=i)
andX⊤
iθ⋆|Ω⋆
i({xj}j̸=i). We already discussed that LAC is maintained under the conditioning,
especially the conditioning to event Ωi({xj}j̸=i)andΩ⋆
i({xj}j̸=i)in the Lemma 1.
E.2 Sections and Section Densities
Our remaining goal is to bound the one-side decay rate of section density. To build some general
theory, we first define sections and section densities explicitly.
Definition 7 (Sections) For the set A⊂Rdandv∈Sd−1we define sections as
Sec(A, v, y ) :=A∩ {x|x⊤v=y}
fory∈R.
This means that section is the intersection of region Aand hyperplane {x∈Rd|x⊤v=y}.
Definition 8 (Section density) We define the section density of the region A. Let fbe the density
defined in the region A⊂Rd. Then we define g(y)is the section density of Sec(A, v, y )as
g(y) =Z
x∈Sec(A,v,y )f(x)dx.
Remark 1 If the density of X∈Rdisf, the section density corresponds to the density of X⊤vfor
v∈Sd−1.
Next, we define the areas with special section structures: equal sections andexpanding sections . First,
we define equal sections, the area whose section with direction vis equal.
Definition 9 (Equal sections) We define the set Ahave equal sections with v∈Sd−1when
Sec(A, v, y )is the congruence of shapes for y >0.
Next, we define expanding sections, where the sections of Awith direction vexpand when yincreases.
24Definition 10 (Expanding sections) We define the sections of Awith direction vare expanding
sections when
Sec(A, v, y ) +hv⊂Sec(A, v, y +h)
for every h >0. Technically, equal sections are included in expanding sections.
<latexit sha1_base64="PPAag2Q5fZEmfC+YbHHxbqO/4DU=">AAAGTXicjVRNbxMxEHULoaV8tXCjF4sIqYhVla0qQOqlqK3EjVJokyobRV7Hm1jxl2xvm2i7B34NV/glnPkh3BDCm2xI1ykSljaezHvjGT+PJlaMGtto/FhavnW7dmdl9e7avfsPHj5a33h8ZmSqMTnFkkndipEhjApyaqllpKU0QTxmpBkPDwq8eUG0oVJ8smNFOhz1BU0oRta5uuubkVREIyu1QJxkHwnOt94GF8H45eBFd73e2G5MFlw0wtKog3IddzdqT6OexCknwmKGjGmHDWU7GdKWYkbytSg1RCE8RH3SdmaR0nSyyS1y+Nx5ejCR2n3Cwon3ekSGuDFjHjsmR3ZgfKxw3oS1U5u86WRUqNQSgaeJkpRBK2EhCexRTbBlY1g5USAb0zi/KUvg9qJGE8xKqrAmlQeQCCeEk5YEMGES2QD2NVIDikcBHEpL3MbRkGDCmLNSZqmWlwGkoufkS6g2LsKkMUaqeCqvENZXhqRObNkjC5DU1A541T1wN9WaJFUvo8qkHtOFWimZyT053GVIErh3vCAL58xu5p9EpCa8YFe1jb2U/7rmaNoaFZ/SUnrZFdJmSJVzCnKJJedI9LLoKM+i4rXiODvKPexkjp342OHhHDz0QdeeDt2L3PPZrPgDoz3ocQqBxhUWRIzdxGzNMiVZy890PsfOF6qYY4mPNedY08dGc2zkY3yOcR9DscnbYSeLGEnsVT2MNO0P7JXHMqkSUvNrzOkvrIewjCi3bhZRkdixn8deyv8+YccPVpIVqlfm2cS3WKbyeRPflFc2bSan/Zi9n/XlNQxL0aNT+OCvWWG4LnUj2Gl6XBprbpqG/uxcNM52tsNX27sfduv72+VcXQWb4BnYAiF4DfbBO3AMTgEGn8EX8BV8q32v/az9qv2eUpeXypgnoLLurPwB1e1Mzg==</latexit>Sec(A, v, y+h)<latexit sha1_base64="JYNRW61qwCdmKGn6DV8AJUjYidQ=">AAAGS3icjVTNbhMxEHYLgVL+WjgBF4sIqUirKFtVgNRLUVuJG6XQJlU2iryON7HiP9neNtF2xdNwhSfhAXgObogD3mRDuk6RsLTxZL5vPOPPo4kVo8Y2mz9WVm/crN26vXZn/e69+w8ebmw+OjUy1ZicYMmkbsfIEEYFObHUMtJWmiAeM9KKR/sF3jon2lApPtmJIl2OBoImFCPrXL2NJ5FURCMrtUCcZB8JzrfeBufB5GVvo95sNKcLLhthadRBuY56m7WnUV/ilBNhMUPGdMKmst0MaUsxI/l6lBqiEB6hAek4s0houtn0Djl84Tx9mEjtPmHh1Hs1IkPcmAmPHZMjOzQ+VjivwzqpTd50MypUaonAs0RJyqCVsBAE9qkm2LIJrJwokI1pnF+XJXB7UaMJ5iVVWNPKA0iEE8IJSwKYMIlsAAcaqSHF4wCOpCVu42hEMGHMWSmzVMuLAFLRd/IlVBsXYdIYI1U8lFcIGyhDUie27JMlSGpqh7zqHrqbak2SqpdRZVKP6UKtlMzknhzuMiQJ3Duek6Vz5jfzTyJSE16wq9rGXsp/XXM8a42KT2kpvewKaTOiyjkFucCScyT6WXSYZ1HxWnGcHeYedrzAjn3s4GABHviga0+H7kbu+WxW/IHRLvQ4hUCTCgsixq5jtueZkqztZzpbYGdLVSywxMdaC6zlY+MFNvYxvsC4j6HY5J2wm0WMJPayHkaaDob20mOZVAmp+RXm7BfWQ1hGlFsvi6hI7MTPYy/kf5+w7QcryQrVK9Ns6lsuU/m8qW/GK5s2k7N+zN7P+/IKhqXo0xm8/9esMFyXugHsND0qjXU3TUN/di4bp9uN8FVj58NOfa9RztU18Aw8B1sgBK/BHngHjsAJwOAz+AK+gm+177WftV+13zPq6koZ8xhU1q3aHxr5TCc=</latexit>Sec(A, v, y)<latexit sha1_base64="a0kgBgysldnQ3V+24yP/rmuxMZk=">AAAGMXicjVTLbhMxFHULgVIebWEHG4sIicUoylQVIHVT1FZiR1vRJlUmqjyTO4kVv2R72kTTfAFb+BK+pjvElp/Ak0xIxykSlhLfnHOu78M3jhWjxjabNyur9+7XHjxce7T++MnTZxubW8/PjMx0AqeJZFK3Y2KAUQGnlloGbaWB8JhBKx7uF3zrErShUnyxYwVdTvqCpjQh1kHHHy82681Gc7rwshGWRh2V6+hiq/Yy6skk4yBswogxnbCpbDcn2tKEwWQ9ygwokgxJHzrOFISD6ebTTCf4jUN6OJXafYTFU/S2R064MWMeOyUndmB8rgDv4jqZTT90cypUZkEks0BpxrCVuCgb96iGxLIxrpwoiI1pPLkrSuD2IkcTzFOqqKaZBxiEa4QmFgKcMklsgPuaqAFNRgEeSgtu42QICTDmrIxZquVVgKnoufalVBvnYbI4Iaq4Di8R1lcGMtds2YMlSmpqB7wKD1ylWkNaRRlVJvOUztVKyczEa4crBtLA3eMlLJ0zr8w/CaQGXqirvY29kP8qczQbjQqmtJRedEW0GVLlQAFXieSciF4eHU7yqLitOM4PJx53suBOfO7gYEEe+KQbT8fuRu76bF78wNEu9jRFg8YVFSaM3aVszyOleduPdL7gzpeyWHCpz7UWXMvnRgtu5HN8wXGfI7GZdMJuHjFI7XU9jDTtD+y1pzKZElLzW8rZN66HuPQot4s8oiK1Yz+OvZL/fcK276wkK7ouVfGnk7oY2HyKLaepfN0Um+nKoc3lbB7zz/O5vMUlUvTojN7/a1YUbkrdM+t6elQa6+41Df23c9k4226E7xo7xzv1vUb5rq6hV+g1eotC9B7toU/oCJ2iBAH6ir6h77UftZvaz9qvmXR1pfR5gSqr9vsPGnNC8Q==</latexit>A
<latexit sha1_base64="S3MY0AHnz1mHFgJwKbfKKmJhj1o=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHHlxcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AT8FDLA==</latexit>v
Figure 2: Illustration of expanding section’s example. The section with direction vis expanding when
yincreases: ytoy+h.
E.3 Decay Rate of Section Density
Recall that we define one-side decay rate of univariate density g(Definition 4) at Appendix D as a
constant Msatisfying
g(y′)
g(y)≥exp(−M(y′−y))
for all y < y′. Then, how to bound the (one-side) decay rate of the section density? We present a
technique to bound the (one-side) decay rate of section densities defined in equal and expanding
sections.
Lemma 8 (One-side decay rate: equal and expanding sections) ForA⊂Rd, v∈Rdandy∈R,
a density fhas support Aand has a bounded decay rate M. Assume that the sections of Awith the
direction vare expanding sections. We define the section density of Sec(A, v, y )asg(y)then we
have for any y1< y2:
g(y1)
g(y2)≥exp(−M|y1−y2|).
Proof Since the section is expanding with the direction v, we can observe for any h >0,
g(y+h)
g(y)=R
Sec(A,v,y +h)f(x)dx
R
Sec(A,v,y )f(x)dx
≥R
Sec(A,v,y )f(x) exp(−Mh)dx
hR
Sec(A,v,y )f(x)dx
≥exp(−Mh).
The second inequality holds since the section is expanding and using Lemma 3. Since equal sections
are also expanding sections, we end the proof.
25Lemma 9 (Maximum density: equal and expanding sections) The density f(·)has support A⊂
Rdand has a bounded decay rate M. Furthermore, sections with direction vare expanding sections
and define the section density of Sec(A, v, y )asg(y). If the support of g(·)contains an interval [a, b],
theng(y)≤3Mfory∈[a, b−1
2M].
Proof It can be obtained directly by applying result of the previous Lemma 8 and Lemma 5.
F Fixed History Results of Two Challenges for Unbounded Contexts
This section aim to provide key propositions to prove Theorem 2 and Theorem 3. Our two challenges
aim to bound
E[Xa(t)(t)Xa(t)(t)]
and
P[∆(X(t))≤ε]. (8)
We define X(t) = (X1(t)⊤, . . . , X K(t)⊤)∈RdK. Under the given event Ht−1,ˆθt−1is a determin-
istic value and no longer random.
F.1 Details for Diversity Constant (Challenge 1)
In the definition of the diversity constant, we recall that the expection is taken with respect to X(t)
and history Ht−1. In round t, for any fixed history Ht−1, we perform the greedy policy with the
estimator ˆθt−1. Hence, when the entire set of contexts X(t) = (X1(t)⊤, . . . , X K(t)⊤)is revealed,
a(t)is determined immediately given the history Ht−1. Then the exact statement is:
“In round t, for any given history Ht−1,
EX(t)[Xa(t)(t)Xa(t)(t)⊤]⪰λ⋆Id
holds with some λ⋆>0.”
In the proof, we proved a stronger statement:
"For any greedy policy with any θ∈Rdand selected arm a(t)with that policy,
EX(t)
Xa(t)(t)Xa(t)(t)⊤
⪰λ⋆Id
holds with some λ⋆>0."
We prove a stronger quantity (second argument), for any given parameter θ∈Rd, define aθ(X(t)) :=
argmaxi∈[K]Xi(t)⊤θand define λ⋆(t) := min ∥θ∥2=1λmin
Et[Xaθ(X(t))X⊤
aθ(X(t))]
. Since ˆθt−1
can be arbitrary value, we prepare for all greedy policy w.r.t. θ∈Sd−1. Then the new defined λ⋆(t)
satisfies equation (1) and we discuss this in Appendix F.
Thus, the policy a(t), conditioned on Ht−1, is a greedy policy with respect to ˆθt−1, making it
deterministic as well. To address any ˆθt−1, we propose an analysis that applies to any θ∈Rd, aiming
to bound the variance of the selected contexts. In Appendix F.1, we argue that it suffices to bound
this variance for any fixed θ.
min
∥θ∥2=1λmin
E[Xaθ(X(t))(t)X⊤
aθ(X(t))(t)]
Note that aθ(X(t))is defined as arg max i∈[K]Xi(t)⊤θ. This quantity is equal to
min
∥θ∥2=1,∥v∥2=1Et
|v⊤Xaθ(X(t))(t)|2
and we prove this lower bound for any θandv. In summary, our aim is to prove that for any history
Ht−1andθ, v, we aim to bound
E
|v⊤Xaθ(X(t))(t)|2
. (9)
To bound the margin constant, our aim is to prepare for all θ⋆, since the true model parameter θ⋆also
can be arbitrary. For simplicity, when contexts are clear, we wrote aθ(X) =a(when we fix θ).
26F.2 The Setup of Fixed History Analysis
We aim to find a class of density Fsuch that if X(t)∈ F, then equation (9)is lower bounded for any
θ, v. Similarly, we aim to find the class of density G, ifX(t)∈ G, then equation (8)can be upper
bounded for any θ⋆∈Rd.
1.We first prove that for fixed θ, v, for densities with LAC and its support satisfies some
geometric conditons, we can bound equation (9).
2.We also prove that for fixed θ⋆, for densities with LAC and its support satisfies some
geometric conditions, we can bound equation (8).
3.Next, we prove that the densities with LAC and bounded ψ1norm, for any give θ, vwe can
truncate it with high probability region to the densities contained in class of 1.
4.Lastly, we prove that for the densities with LAC and bounded ψ1norm, for any given θ⋆we
can truncate it with high probability region to the densities contained in class of 2.
In this section, we provide the results of 1 and 2. And in the next Appendix G, we proceed to 3 and 4.
Now, we build the theory with the assumptions that θ, vandHt−1are fixed. We assume that random
vectors Z= (Z⊤
1. . . Z⊤
K), Zi∈Rdarise from some distribution with LAC L(·). We fix arbitrary
θ∈Rdand define random index variable
a= argmin
iZ⊤
iθ.
We also fix v∈Sd−1and first aim to bound E[v⊤ZaZ⊤
av]. Our goal is to find the lower bound of
E[v⊤ZaZ⊤
av]for any v∈Sd−1and from now on, we fix the arbitrary v∈Sd−1. Then, we aim to
bound
E[v⊤ZaZ⊤
av]
for fixed θandv. The next Proposition 2 presents when support of Zsatisfies some geometric
condition, we can bound the E[v⊤ZaZ⊤
av]effectively.
We next define C∆as the margin constant of the Zwith the parameter θ⋆which satisfies
P[∆(Z)≤ε]≤C∆ε+1√
T.
where ∆(Z)is a suboptimality gap, defined similarly in Section 4. The next Proposition 3 presents
when support of Zsatisfies some geometric condition, we can bound the margin constant of Z
effectively.
F.3 Fixed History Results: Diversity Constant
Recall that we define [−1,1]v:={x| −1≤x⊤v≤1}.
Proposition 2 (Fixed history diversity constant lower bound) The random vector Z =
(Z⊤
1. . . Z⊤
K),Zi∈Rdsatisfies the following conditions:
1.Zhas LAC with L(·)and∥supp( Z)∥∞≤R.
2.For all i∈[K],supp( Zi)is identical for some subset A⊂Rd. It means supp( Z) =AK
for some A⊂Rd.
3. The support A∩[−1,1]vhas equal sections with direction v.
Then we have the following with some absolute constant c >0:
E[v⊤ZaZ⊤
av]≥c
dL(R)2.
Remark 2 For unbounded contexts X(t)in the original bandit problem, we truncate to some region
C1with positive probability, and we make the truncated contexts satisfy the condition of the above
proposition.
27Proof of Proposition 2 Since supp( Z)≤R, it has bounded decay rate√
dL(R)by Lemma 3. By
the previous decomposition, we can see
Ωi=[
{zj}j̸=iΩi({zj}j̸=i)
holds. By applying tower property, we get
E[v⊤ZaZ⊤
av] =E
E[ZaZ⊤
a|Ωi({zj}j̸=i)]
=E
E[ZiZ⊤
i|Ωi({zj}j̸=i)]
and we are enough to bound the term
E[v⊤ZiZ⊤
iv|Ωi({zj}j̸=i)]. (10)
for every Ωi({zj}j̸=i). Recall that we defined [−1,1]v:={x|x⊤v∈[−1,1]}. Then, we can
decompose it again as
E[v⊤ZiZ⊤
iv|Ωi({zj}j̸=i)]
=E
v⊤ZiZ⊤
iv|Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}
P
Zi∈[−1,1]v|Ωi({zj}j̸=i)
+E
v⊤ZiZ⊤
iv|Ωi({zj}j̸=i)∩ {Zi∈([−1,1]v)c}
P
Zi∈([−1,1]v)c|Ωi({zj}j̸=i)
≥E
v⊤ZiZ⊤
iv|Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}].
The last inequality holds since E
v⊤ZiZ⊤
iv|Ωi({zj}j̸=i)∩ {Zi∈([−1,1]v)c}
≥1and
E
v⊤ZiZ⊤
iv|Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}
≤1.
From now on, we focus on the conditional density of Zi|Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}and its
projected density v⊤Zi|Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}. For simplicity, we set the conditional
density of Zi|Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}asf1(·)and the projected density of v⊤Zi|
Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}asg1(·). Using Lemma 1, the density f1(·)has LAC with constant
function L(R)and it has bounded decay rate√
dL(R)by Lemma 3.
[1] Investigating the support of f1andg1.First, we examine the support of f1. Since arm i
is optimal with estimator θ, for all zin the support of f1should satisfy z⊤θ≥max j̸=iz⊤
jθ. Also,
because −1≤z⊤v≤1holds, then it is the intersection of these two areas. Lets define
A1:= supp( Zi|Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}) ={z∈A|z⊤θ≥max
j̸=iz⊤
jθ,−1≤z⊤v≤1}.
Recall that A:= supp( Zi)is designed to have equal sections with direction v. Hence A∩[−1,1]v
has equal section with v. Therefore, we can see that support of Z⊤
iv|Ωi({zj}j̸=i)∩{Zi∈[−1,1]v}
is interval by our design of A.
[2] Bounding one-side decay rate of section density in [−1,1].We aim to apply Lemma 4 to
bound the variance E
v⊤ZiZ⊤
iv|Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}]. It told us that it is enough to
bound the one-side decay rate of the section density, g1(·)in the interval [−1
2,1
2].
Claim 1 Then one of Sec(A1, v, y)orSec(A1,−v, y)are expanding sections when yincreases for
y∈[−1,1].
Proof Choose one of v,−vthat satisfies ⟨·, θ⟩ ≥0. We want to use the result of Lemma 10. Since
[−1,1]vhas equal sections with direction v, by rotating the axis, we can make this satisfy the
condition of Lemma 10. Then the result of the Lemma 10 tells that at least one direction of v,−v
makes an expanding section. Please see Figure 5 for intuitions.
28<latexit sha1_base64="kVTjUxKHngXexWcdVUPlQCrLtyI=">AAAGTnicjVRNbxMxEHULgVK+WrgBB4sIqUirKltVgNRLUVuJG6XQplU2irzObGLFa1u2t0203Qu/hiv8Eq78EW4IvPkgXadIWNp4Mu+NZ/w8mlhxZmyj8WNp+cbN2q3bK3dW7967/+Dh2vqjEyMzTeGYSi71aUwMcCbg2DLL4VRpIGnMoRkP9kq8eQ7aMCk+2ZGCdkp6giWMEutcnbVnkVSgiZVakBTyj0CLjbedMMDnwehlZ63e2GyMF140wqlRR9N12FmvPYm6kmYpCEs5MaYVNpRt50RbRjkUq1FmQBE6ID1oObPMadr5+BoFfuE8XZxI7T5h8dh7NSInqTGjNHbMlNi+8bHSeR3Wymzypp0zoTILgk4SJRnHVuJSE9xlGqjlI1w5URAbs7i4Lkvg9rJGE8xKqrDGlQcYhBPCaQsBTrgkNsA9TVSf0WGAB9KC21IyAAqcOyvjlml5EWAmuk6+hGnjIkwWU6LKt/IK4T1lIHNiyy4sQFIz20+r7r67qdaQVL2cKZN5TBdqpeSm8ORwl4EkcO94DgvnzG7mnwRSQ1qyq9rGXsp/XXM4aY2KT2kpveyKaDNgyjkFXFCZpkR08+igyKPyteI4Pyg87GiOHfnY/v4c3PdB154O3Ync89m8/IOjHexxSoFGFRYmnF/HPJ1lSvJTP9PZHDtbqGKOJT7WnGNNHxvOsaGPpXMs9TESm6IVtvOIQ2Iv62GkWa9vLz2WyZSQOr3CnPzieoinEdOtk0dMJHbk57EX8r9P2PKDleSl6pWBNvYtlql83tg34U2bNpeTfszfz/ryCkal6LIJvPfXrDBcl7oZ7DQ9nBqrbpqG/uxcNE62NsNXm9sftuu729O5uoKeoudoA4XoNdpF79AhOkYUfUZf0Ff0rfa99rP2q/Z7Ql1emsY8RpV1a+UPsodM+w==</latexit>Sec(A1,v ,y)<latexit sha1_base64="g/CHMjg9IH2oAnn/dZKjAJTI5Oc=">AAAGM3icjVTLbhMxFHULA6U82sIONhYREotRlakqQOqmqK3EjlJokyoTRR7Hk1jxS7anTTTNJ7CFL+FjEDvEln/Ak0yYjlMkLCW+Oedc34dvnChGjW02v6+s3rod3Lm7dm/9/oOHjzY2tx6fGZlpTE6xZFK3E2QIo4KcWmoZaStNEE8YaSWjg4JvXRBtqBSf7ESRLkcDQVOKkXXQx7e9qLfZaG43ZwsuG1FpNEC5jntbwdO4L3HGibCYIWM6UVPZbo60pZiR6XqcGaIQHqEB6ThTIE5MN5/lOoUvHNKHqdTuIyycodc9csSNmfDEKTmyQ+NzBXgT18ls+qabU6EySwSeB0ozBq2EReGwTzXBlk1g7USBbEKT6U1RQrcXOZpwkVJNNcs8hES4RmhkSQhTJpEN4UAjNaR4HMKRtMRtHI0IJow5K2OWankZQir6rn0p1cZ5mCzBSBUX4iXCBsqQzDVb9skSJTW1Q16Hh65SrUlaRxlVJvOUztVKyczUa4crhqShu8cLsnTOojL/JCI14YW63tvEC/mvMsfz0ahhSkvpRVdImxFVDhTkEkvOkejn8dE0j4vbSpL8aOpxJxV34nOHhxV56JNuPB27F7vrs3nxA8Z70NMUDZrUVBAxdpOyvYiU5m0/0nnFnS9lUXGpz7UqruVz44ob+xyvOO5zKDHTTtTNY0ZSe9WIYk0HQ3vlqUymhNT8mnL+DRsRLD3KrZfHVKR24sexl/K/T9jxnZVkRdelKv50UhcDm8+w5TSVr5thc105tLmcz2P+fjGX1zgsRZ/O6YO/Zk3hptQ9tK6nx6Wx7l7TyH87l42zne3o1fbuh93G/m75rq6BZ+A5eAki8Brsg3fgGJwCDAbgM/gCvgbfgh/Bz+DXXLq6Uvo8AbUV/P4DsvtDmw==</latexit>A1<latexit sha1_base64="S3MY0AHnz1mHFgJwKbfKKmJhj1o=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHHlxcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AT8FDLA==</latexit>v
<latexit sha1_base64="CEshLfCf6/+AUiAmdWXSb32QrMQ=">AAAGNnicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXYUSrapMpElWdyJzHxS7anTTTNP7CFL+FX2LBDbPkEPJkJ6ThFwlLim3PO9X34xrFi1NhW6/vK6q3bjTt31+6t33/w8NHjjc0np0ZmOoGTRDKpOzExwKiAE0stg47SQHjMoB2P9gu+fQHaUCk+2omCHicDQVOaEOug08gOwZLzjWZrqzVbeNkIK6OJqnV0vtl4FvVlknEQNmHEmG7YUraXE21pwmC6HmUGFElGZABdZwrCwfTyWbpT/NIhfZxK7T7C4hl63SMn3JgJj52SEzs0PleAN3HdzKZvezkVKrMgkjJQmjFsJS5qx32qIbFsgmsnCmJjGk9vihK4vcjRBPOUaqpZ5gEG4RqhiYUAp0wSG+CBJmpIk3GAR9KC2zgZQQKMOStjlmp5GWAq+q59KdXGeZgsTogq7sRLhA2Ugcw1W/ZhiZKa2iGvw0NXqdaQ1lFGlck8pXO1UjIz9drhioE0cPd4AUvnzCvzTwKpgRfqem9jL+S/yhyXo1HDlJbSi66INiOqHCjgMpGcE9HPo8NpHhW3Fcf54dTjjhfcsc8dHCzIA5904+nY3chdn82LHzjaxZ6maNCkpsKEsZuUnXmkNO/4kc4W3NlSFgsu9bn2gmv73HjBjX2OLzjucyQ2027YyyMGqb1qhpGmg6G98lQmU0Jqfk1ZfuNmiCuPajvPIypSO/Hj2Ev53yds+85KsqLrUhV/OqmLgc1n2HKaytfNsFJXDW0uy3nM38/n8hqXSNGnJb3/16wp3JS6t9b19Kgy1t1rGvpv57Jxur0Vvt7a+bDT3Nup3tU19By9QK9QiN6gPfQOHaETlKBP6DP6gr42vjV+NH42fpXS1ZXK5ymqrcbvP6dIRVo=</latexit>ω<latexit sha1_base64="XYV5ZI9l/JI2AkcWPcTsVUex7nQ=">AAAGMnicjVTLbhMxFHULA6W8WtjBxiJCYjFUmaoCpG4qtZXYUSrapMpElcfxJFb8ku1pE03zB2zhS/gZ2CG2fASezITpOEXCUuKbc871ffjGiWLU2Hb7+8rqrdvBnbtr99bvP3j46PHG5pNTIzONyQmWTOpuggxhVJATSy0jXaUJ4gkjnWS8X/CdC6INleKTnSrS52goaEoxsg46fn1xvtFqb7XnCy4bUWW0QLWOzjeDZ/FA4owTYTFDxvSitrL9HGlLMSOz9TgzRCE8RkPSc6ZAnJh+Pk91Bl86ZABTqd1HWDhHr3vkiBsz5YlTcmRHxucK8Caul9n0XT+nQmWWCFwGSjMGrYRF3XBANcGWTWHjRIFsQpPZTVFCtxc5mnCRUkM1zzyERLhGaGRJCFMmkQ3hUCM1ongSwrG0xG0cjQkmjDkrY5ZqeRlCKgaufSnVxnmYLMFIFffhJcKGypDMNVsOyBIlNbUj3oRHrlKtSdpEGVUm85TO1UrJzMxrhyuGpKG7xwuydM6iMv8kIjXhhbrZ28QL+a8yJ+VoNDClpfSiK6TNmCoHCnKJJedIDPL4cJbHxW0lSX4487jjmjv2uYODmjzwSTeejt2N3fXZvPgB413oaYoGTRsqiBi7SdldRErzrh/prObOlrKoudTnOjXX8blJzU18jtcc9zmUmFkv6ucxI6m9akWxpsORvfJUJlNCan5NWX7DVgQrj2o7z2MqUjv149hL+d8nbPvOSrKi61IVfzqpi4HN59hymsrXzbFSVw1tLst5zD8s5vIah6UY0JLe/2s2FG5K3TvrenpUGevuNY38t3PZON3eit5s7Xzcae3tVO/qGngOXoBXIAJvwR54D47ACcAgBZ/BF/A1+Bb8CH4Gv0rp6krl8xQ0VvD7D34zQ2M=</latexit>→vFigure 3: Illustration of A1and expanding sections of vor−v.A1is the area above the green line.
In this case, sections with direction +vis expanding! If a cylindrical set is cut by some hyperplane
(which is A1),at least one direction makes expanding sections.
The result of Claim 1 tell us that we can apply Lemma 8 and finally we can bound the one-side decay
rate of section density g1(·). Without loss of generality, Sec(A1, v, y)is expanding sections whan y
increases. Then support of Z⊤
iv|Ωi({zj}j̸=i)∩ {Zi∈[−1,1]v}is interval of form [c,1]with some
−1< c < 1.
Claim 2 One-side decay rate of the densities P[v⊤Zi=y|Ωi({zj}j̸=i)∩[−1,1]v]is bounded by√
dL(R)iny∈[−1,1].
Proof To bound the one-side decay rate, we aim to use Lemma 8. By the following Claim 1, without
loss of generality, we can set sections with direction vasSec(A, v, y )is expanding section. (Since
variance of v⊤Xis the same as (−v)⊤X, either case is fine) Then we can apply Lemma 8 and get
the wanted result directly.
[3] Bounding desired variance. We can see that the support of g1(·)is form of interval [c,1]for
some c <1. Hence, by using the result of the previous Lemma 4, we conclude the variance
E[v⊤ZiZ⊤
iv|Ωi({zj}j̸=i)∩Zi∈[−1,1]v]≥c
dL(R)2
for some absolute constant c >0.
We prove the Lemma 10 which is used in the proof.
Lemma 10 Consider the (cylindrical shape) set S=U×IforU⊂Rd−1and the interval I⊂R.
Forθ∈Rdwithθ⊤en≥0and any b∈R, define the set S′:=S∩ {x|x⊤θ≥b}. Then S′is an
expanding sections with direction en.
Proof To prove that they are expanding sections, we need to show that for any h > 0,x0+
hen∈Sec(S′, en, y+h)for any x0∈Sec(S′, en, y). By the definition of sections, it is clear that
x0+hen∈ {x∈Rd|x⊤en=y+h}. Next, we need to show that x0+hen∈ {x|x⊤θ≥b}.
Since e⊤
nθ≥0
(x0+hen)⊤θ≥x⊤
0θ≥b
holds.
29<latexit sha1_base64="MuOAM2lQOiz/IuMIEoO0FIDQu7k=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0RbaBGWiyjO5k1jxS7anTTSdL2ALX8LXdIfY8hN4kgnpOEXCUuKbc871ffjGsWLU2FbremX1zt3GvftrD9YfPnr85OnG5rMzIzOdwGkimdSdmBhgVMCppZZBR2kgPGbQjkf7Jd++AG2oFJ/tREGPk4GgKU2IddDxp/ONZmurNV142Qgro4mqdXS+2XgR9WWScRA2YcSYbthStpcTbWnCoFiPMgOKJCMygK4zBeFgevk00wK/dkgfp1K7j7B4it70yAk3ZsJjp+TEDo3PleBtXDez6fteToXKLIhkFijNGLYSl2XjPtWQWDbBtRMFsTGNi9uiBG4vczTBPKWaapp5gEG4RmhiIcApk8QGeKCJGtJkHOCRtOA2TkaQAGPOypilWl4GmIq+a19KtXEeJosTosrr8BJhA2Ugc82WfViipKZ2yOvw0FWqNaR1lFFlMk/pXK2UzBReO1wxkAbuHi9g6Zx5Zf5JIDXwUl3vbeyF/FeZ49lo1DClpfSiK6LNiCoHCrhMJOdE9PPosMij8rbiOD8sPO5kwZ343MHBgjzwSTeejt2N3PXZvPyBo13sacoGTWoqTBi7TdmZR0rzjh/py4L7spTFgkt9rr3g2j43XnBjn+MLjvsciU3RDXt5xCC1V80w0nQwtFeeymRKSM1vKGffuBniyqPazvOIitRO/Dj2Uv73Cdu+s5Ks7LpU5Z9O6nJg8ym2nKbydVNspquGNpezecw/zufyBpdI0aczev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8u7VzvNPc26ne1TX0Er1Cb1CI3qE99AEdoVOUIEBf0Tf0vfGjcd342fg1k66uVD7PUW01fv8BhKtDCQ==</latexit>S
<latexit sha1_base64="OYuXwyv4KB9yhTrRGuxEDKUvzts=">AAAGMnicjVTLbhMxFHWBQCmvFnawsYgQLEZVpqoAqZtKbSV2lEKbVJmo8jiexIpfsj1toun8AVv4En4GdogtH4EnM2E6TpGwlPjmnHN9H75xrBg1ttP5vnLj5q3W7Turd9fu3X/w8NH6xuMTI1ONyTGWTOpejAxhVJBjSy0jPaUJ4jEj3XiyV/Ddc6INleKTnSky4GgkaEIxsg46+vjybL3d2ezMF1w2wspog2odnm20nkZDiVNOhMUMGdMPO8oOMqQtxYzka1FqiEJ4gkak70yBODGDbJ5qDl84ZAgTqd1HWDhHr3pkiBsz47FTcmTHxucK8Dqun9rk7SCjQqWWCFwGSlIGrYRF3XBINcGWzWDjRIFsTOP8uiiB24scTbBIqaGaZx5AIlwjNLIkgAmTyAZwpJEaUzwN4ERa4jaOJgQTxpyVMku1vAggFUPXvoRq4zxMGmOkivvwEmEjZUjqmi2HZImSmtoxb8JjV6nWJGmijCqTekrnaqVkJvfa4YohSeDu8ZwsnbOozD+JSE14oW72NvZC/qvMaTkaDUxpKb3oCmkzocqBglxgyTkSwyw6yLOouK04zg5yjzuquSOf29+vyX2fdOPp2J3IXZ/Nih8w2oGepmjQrKGCiLHrlL1FpCTr+ZFOa+50KYuaS3yuW3Ndn5vW3NTneM1xn0OxyfvhIIsYSexlO4w0HY3tpacyqRJS8yvK8hu2Q1h5VNtZFlGR2Jkfx17I/z5hy3dWkhVdl6r400ldDGw2x5bTVL5ujpW6amgzWc5j9n4xl1c4LMWQlvTeX7OhcFPq3lnX08PKWHOvaei/ncvGydZm+Hpz+8N2e3e7eldXwTPwHLwCIXgDdsE7cAiOAQYJ+Ay+gK+tb60frZ+tX6X0xkrl8wQ0Vuv3H5B1Qzo=</latexit>S→<latexit sha1_base64="CEshLfCf6/+AUiAmdWXSb32QrMQ=">AAAGNnicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXYUSrapMpElWdyJzHxS7anTTTNP7CFL+FX2LBDbPkEPJkJ6ThFwlLim3PO9X34xrFi1NhW6/vK6q3bjTt31+6t33/w8NHjjc0np0ZmOoGTRDKpOzExwKiAE0stg47SQHjMoB2P9gu+fQHaUCk+2omCHicDQVOaEOug08gOwZLzjWZrqzVbeNkIK6OJqnV0vtl4FvVlknEQNmHEmG7YUraXE21pwmC6HmUGFElGZABdZwrCwfTyWbpT/NIhfZxK7T7C4hl63SMn3JgJj52SEzs0PleAN3HdzKZvezkVKrMgkjJQmjFsJS5qx32qIbFsgmsnCmJjGk9vihK4vcjRBPOUaqpZ5gEG4RqhiYUAp0wSG+CBJmpIk3GAR9KC2zgZQQKMOStjlmp5GWAq+q59KdXGeZgsTogq7sRLhA2Ugcw1W/ZhiZKa2iGvw0NXqdaQ1lFGlck8pXO1UjIz9drhioE0cPd4AUvnzCvzTwKpgRfqem9jL+S/yhyXo1HDlJbSi66INiOqHCjgMpGcE9HPo8NpHhW3Fcf54dTjjhfcsc8dHCzIA5904+nY3chdn82LHzjaxZ6maNCkpsKEsZuUnXmkNO/4kc4W3NlSFgsu9bn2gmv73HjBjX2OLzjucyQ2027YyyMGqb1qhpGmg6G98lQmU0Jqfk1ZfuNmiCuPajvPIypSO/Hj2Ev53yds+85KsqLrUhV/OqmLgc1n2HKaytfNsFJXDW0uy3nM38/n8hqXSNGnJb3/16wp3JS6t9b19Kgy1t1rGvpv57Jxur0Vvt7a+bDT3Nup3tU19By9QK9QiN6gPfQOHaETlKBP6DP6gr42vjV+NH42fpXS1ZXK5ymqrcbvP6dIRVo=</latexit>ω
<latexit sha1_base64="ZgO1MnUdpZ3oGBh9xw/d21bKZSU=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZVgRVvRJigTVZ7JncSKX7I9baLpfAFb+BK+pjvElp/Ak0xIxykSlhLfnHOu78M3jhWjxrZa1yurd+427t1fe7D+8NHjJ083Np+dGZnpBE4TyaTuxMQAowJOLbUMOkoD4TGDdjzaL/n2BWhDpfhsJwp6nAwETWlCrIOOP55vNFtbrenCy0ZYGU1UraPzzcaLqC+TjIOwCSPGdMOWsr2caEsTBsV6lBlQJBmRAXSdKQgH08unmRb4tUP6OJXafYTFU/SmR064MRMeOyUndmh8rgRv47qZTd/3cipUZkEks0BpxrCVuCwb96mGxLIJrp0oiI1pXNwWJXB7maMJ5inVVNPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIywFT0XftSqo3zMFmcEFVeh5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZKbw2uGKgTRw93gBS+fMK/NPAqmBl+p6b2Mv5L/KHM9Go4YpLaUXXRFtRlQ5UMBlIjknop9Hh0UelbcVx/lh4XEnC+7E5w4OFuSBT7rxdOxu5K7P5uUPHO1iT1M2aFJTYcLYbcrOPFKad/xIXxbcl6UsFlzqc+0F1/a58YIb+xxfcNznSGyKbtjLIwapvWqGkaaDob3yVCZTQmp+Qzn7xs0QVx7Vdp5HVKR24sexl/K/T9j2nZVkZdelKv90UpcDm0+x5TSVr5tiM101tLmczWP+aT6XN7hEij6d0ft/zZrCTal7Zl1Pjypj3b2mof92Lhtn21vh262d453m3k71rq6hl+gVeoNC9A7toQ/oCJ2iBAH6ir6h740fjevGz8avmXR1pfJ5jmqr8fsPSqlC/w==</latexit>I
<latexit sha1_base64="Wq4lPd5D1s1fY8A9yGZE7IoFU0I=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmSoCpG4qtZXY0VakSZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwJBPScYqEpcQ355zr+/CNY8WosY3Gzdr6vfu1Bw83Hm0+fvL02fOt7RdnRmY6gVYimdSdmBhgVEDLUsugozQQHjNox6ODgm9fgjZUii92oqDHyUDQlCbEOuikdbFVb+w0ZguvGmFp1FG5ji+2a6+ivkwyDsImjBjTDRvK9nKiLU0YTDejzIAiyYgMoOtMQTiYXj7LdIrfOqSPU6ndR1g8Q2975IQbM+GxU3Jih8bnCvAurpvZ9GMvp0JlFkQyD5RmDFuJi7Jxn2pILJvgyomC2JjG07uiBG4vcjTBIqWKapZ5gEG4RmhiIcApk8QGeKCJGtJkHOCRtOA2TkaQAGPOypilWl4FmIq+a19KtXEeJosToorr8BJhA2Ugc82WfVihpKZ2yKvw0FWqNaRVlFFlMk/pXK2UzEy9drhiIA3cPV7CyjmLyvyTQGrghbra29gL+a8yx/PRqGBKS+lFV0SbEVUOFHCVSM6J6OfR0TSPituK4/xo6nGnS+7U5w4Pl+ShT7rxdOxe5K7P5sUPHO1hT1M0aFJRYcLYXcrOIlKad/xI50vufCWLJZf6XHvJtX1uvOTGPseXHPc5EptpN+zlEYPUXtfDSNPB0F57KpMpITW/pZx/43qIS49yu8gjKlI78ePYK/nfJ+z6zkqyoutSFX86qYuBzWfYaprK182wua4c2lzO5zH/vJjLW1wiRZ/O6YO/ZkXhptQ9s66nx6Wx6V7T0H87V42z3Z3w/U7zpFnfb5bv6gZ6jd6gdyhEH9A++oSOUQslCNBX9A19r/2o3dR+1n7Npetrpc9LVFm1338AkEVDCw==</latexit>U<latexit sha1_base64="RySjdC5DkrowgBQb44qxyoGLg0Y=">AAAGM3icjVTLbhMxFHULgVJeLexgYxEhsRhVmSoCpG4qtZXYUQptUmWiyOPcSax4bMv2tImm+QS28CV8DGKH2PIPeJIJ6ThFwlLim3PO9X34xrHizNhG4/va+q3btTt3N+5t3n/w8NHjre0nZ0ZmmsIplVzqdkwMcCbg1DLLoa00kDTm0IpHBwXfugBtmBSf7ERBNyUDwRJGiXXQR+iJ3la9sdOYLbxqhKVRR+U67m3XnkV9SbMUhKWcGNMJG8p2c6Itoxymm1FmQBE6IgPoOFOQFEw3n+U6xS8d0seJ1O4jLJ6h1z1ykhozSWOnTIkdGp8rwJu4TmaTt92cCZVZEHQeKMk4thIXheM+00Atn+DKiYLYmMXTm6IEbi9yNMEipYpqlnmAQbhGaGIhwAmXxAZ4oIkaMjoO8EhacFtKRkCBc2dl3DItLwPMRN+1L2HaOA+TxZSo4kK8RPhAGchcs2UfViipmR2mVXjoKtUakirKmTKZp3SuVkpupl47XDGQBO4eL2DlnEVl/kkgNaSFutrb2Av5rzLH89GoYEpL6UVXRJsRUw4UcEllmhLRz6OjaR4VtxXH+dHU406W3InPHR4uyUOfdOPp2L3IXZ/Nix842sOepmjQpKLChPOblO1FpCRv+5HOl9z5ShZLLvG51pJr+dx4yY19Ll1yqc+R2Ew7YTePOCT2qh5Gmg2G9spTmUwJqdNryvk3roe49Ci3Xh4xkdiJH8deyv8+Ydd3VpIXXZeq+NNJXQxsPsNW01S+bobNdeXQ5nI+j/n7xVxe46gUfTanD/6aFYWbUvfQup4el8ame01D/+1cNc52d8LXO80Pzfp+s3xXN9Bz9AK9QiF6g/bRO3SMThFFA/QZfUFfa99qP2o/a7/m0vW10ucpqqza7z/mDkP8</latexit>enFigure 4: Illustration of Sand expanding sections with direction en. Blue lines are sections
Sec(S′, en, y). If cylindrical shape set is sliced by some hyperplane, at least one direction makes an
expanding sections.
F.4 Fixed History Results: Suboptimality Gap
Next, we provide a fixed history analysis to bound margin constant of Challenge 2.
Before we start, we define the cylindrical set, which is used in characterizing the support of densities.
Definition 11 (Cylindrical region) We define A⊂Rdas a cylindrical region with direction v∈
Sd−1and length Hwhen A={B+tv| −H≤t≤H}for some subset of the hyperplane
B⊂ {x|x⊤v= 0}. We write this set as cyl(B, v, H ).
<latexit sha1_base64="S3MY0AHnz1mHFgJwKbfKKmJhj1o=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHHlxcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AT8FDLA==</latexit>v<latexit sha1_base64="vuBQhbLi3xKK26k2IMHUxypayyI=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG6K2krsaCvaBGWiyjO5k1jxS7anTTSdL2ALX8LXdIfY8hN4kgnpOEXCUuKbc871ffjGsWLU2FbremX1zt3GvftrD9YfPnr85OnG5rMzIzOdwGkimdSdmBhgVMCppZZBR2kgPGbQjkf7Jd++AG2oFJ/tREGPk4GgKU2IddDxh/ONZmurNV142Qgro4mqdXS+2XgR9WWScRA2YcSYbthStpcTbWnCoFiPMgOKJCMygK4zBeFgevk00wK/dkgfp1K7j7B4it70yAk3ZsJjp+TEDo3PleBtXDez6fteToXKLIhkFijNGLYSl2XjPtWQWDbBtRMFsTGNi9uiBG4vczTBPKWaapp5gEG4RmhiIcApk8QGeKCJGtJkHOCRtOA2TkaQAGPOypilWl4GmIq+a19KtXEeJosTosrr8BJhA2Ugc82WfViipKZ2yOvw0FWqNaR1lFFlMk/pXK2UzBReO1wxkAbuHi9g6Zx5Zf5JIDXwUl3vbeyF/FeZ49lo1DClpfSiK6LNiCoHCrhMJOdE9PPosMij8rbiOD8sPO5kwZ343MHBgjzwSTeejt2N3PXZvPyBo13sacoGTWoqTBi7TdmZR0rzjh/py4L7spTFgkt9rr3g2j43XnBjn+MLjvsciU3RDXt5xCC1V80w0nQwtFeeymRKSM1vKGffuBniyqPazvOIitRO/Dj2Uv73Cdu+s5Ks7LpU5Z9O6nJg8ym2nKbydVNspquGNpezecw/zefyBpdI0aczev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8u7VzvNPc26ne1TX0Er1Cb1CI3qE99BEdoVOUIEBf0Tf0vfGjcd342fg1k66uVD7PUW01fv8BHEFC9w==</latexit>A
<latexit sha1_base64="WCqN4NyGcU/VS7v36JpDIDbPFR0=">AAAGMnicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZW6o1S0SZWJKo/jSaz4JdvTJprOH7CFL+FnYIfY8hF4MhOm4xQJS4lvzjnX9+Ebx4pRYzud7yurd+627t1fe7D+8NHjJ083Np+dGZlqTE6xZFL3YmQIo4KcWmoZ6SlNEI8Z6caT/YLvXhJtqBSf7EyRAUcjQROKkXXQyfbRxUa7s9WZL7hshJXRBtU6vthsvYiGEqecCIsZMqYfdpQdZEhbihnJ16PUEIXwBI1I35kCcWIG2TzVHL52yBAmUruPsHCO3vTIEDdmxmOn5MiOjc8V4G1cP7XJ+0FGhUotEbgMlKQMWgmLuuGQaoItm8HGiQLZmMb5bVECtxc5mmCRUkM1zzyARLhGaGRJABMmkQ3gSCM1pngawIm0xG0cTQgmjDkrZZZqeRVAKoaufQnVxnmYNMZIFffhJcJGypDUNVsOyRIlNbVj3oTHrlKtSdJEGVUm9ZTO1UrJTO61wxVDksDd4yVZOmdRmX8SkZrwQt3sbeyF/FeZ03I0GpjSUnrRFdJmQpUDBbnCknMkhll0mGdRcVtxnB3mHndScyc+d3BQkwc+6cbTsbuRuz6bFT9gtAs9TdGgWUMFEWO3KXuLSEnW8yOd19z5UhY1l/hct+a6PjetuanP8ZrjPodik/fDQRYxktjrdhhpOhrba09lUiWk5jeU5Tdsh7DyqLaLLKIisTM/jr2S/33Ctu+sJCu6LlXxp5O6GNhsji2nqXzdHCt11dBmspzH7MNiLm9wWIohLen9v2ZD4abUvbOup8eVse5e09B/O5eNs+2t8O3Wzsed9t5O9a6ugZfgFXgDQvAO7IEjcAxOAQYJ+Ay+gK+tb60frZ+tX6V0daXyeQ4aq/X7D5BUQzo=</latexit>2H
<latexit sha1_base64="Ojr5zyNGx5NMkm0PRKvZu2pmzIY=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4q2krsaCvaBGWiyjO5k1jxS7anTTSdL2ALX8LXdIfY8hN4kgnpOEXCUuKbc871ffjGsWLU2FbremX1zt3GvftrD9YfPnr85OnG5rMzIzOdwGkimdSdmBhgVMCppZZBR2kgPGbQjkf7Jd++AG2oFJ/tREGPk4GgKU2IddDxh/ONZmurNV142Qgro4mqdXS+2XgR9WWScRA2YcSYbthStpcTbWnCoFiPMgOKJCMygK4zBeFgevk00wK/dkgfp1K7j7B4it70yAk3ZsJjp+TEDo3PleBtXDez6fteToXKLIhkFijNGLYSl2XjPtWQWDbBtRMFsTGNi9uiBG4vczTBPKWaapp5gEG4RmhiIcApk8QGeKCJGtJkHOCRtOA2TkaQAGPOypilWl4GmIq+a19KtXEeJosTosrr8BJhA2Ugc82WfViipKZ2yOvw0FWqNaR1lFFlMk/pXK2UzBReO1wxkAbuHi9g6Zx5Zf5JIDXwUl3vbeyF/FeZ49lo1DClpfSiK6LNiCoHCrhMJOdE9PPosMij8rbiOD8sPO5kwZ343MHBgjzwSTeejt2N3PXZvPyBo13sacoGTWoqTBi7TdmZR0rzjh/py4L7spTFgkt9rr3g2j43XnBjn+MLjvsciU3RDXt5xCC1V80w0nQwtFeeymRKSM1vKGffuBniyqPazvOIitRO/Dj2Uv73Cdu+s5Ks7LpU5Z9O6nJg8ym2nKbydVNspquGNpezecw/zefyBpdI0aczev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8u7VzvNPc26ne1TX0Er1Cb1CI3qE99BEdoVOUIEBf0Tf0vfGjcd342fg1k66uVD7PUW01fv8BIg5C+A==</latexit>B
Figure 5: Illustration of cylindrical region A.
Next, we prove that if the support of each context Ziis a cylindrical set, we can bound the margin
constant. We state our key results for the bounding margin constant in the fixed history setup.
Proposition 3 (Fixed history margin constant bound) Assume that the random vector Z=
(Z⊤
1. . . Z⊤
K),Zi∈Rdsatisfies LAC with function L(·)and∥supp( Z)∥∞≤R. Additionally,
it satisfies following conditions:
1.For all i∈[K],supp( Zi)⊂Rdis identical for some set A⊂Rd. It means supp( Z) =AK.
302.For some H > 0,Zi’s support supp( Zi) =A= cyl( B, θ⋆, H+ 1) for all i∈[K]and
P[Z∈(cyl(B, θ⋆, H))K]≥1−δ.
Then we have
P[∆(Z)≤ε]≤3√
dL(R)ε+δ.
Remark 3 For the original bandit problem with contexts X(t), we truncate to some high-probability
region C2and set the truncated contexts to satisfy the conditions of the above proposition. We provide
the analysis in the next Appendix G.
<latexit sha1_base64="gWDd6YAnwizqQLtvJdFc+5rSyxY=">AAAGOHicjVTLbhMxFHWBQCmvFnawsYiQEBpVmaoCpG4qtZXYUSrapMqEyuPcSax4bMv2tImm+Qm28CX8CTt2iC1fgCczIR2nSFhKfHPOub4P3zhWnBnban1fuXHzVuP2ndW7a/fuP3j4aH3j8YmRmaZwTCWXuhMTA5wJOLbMcugoDSSNObTj0V7Bt89BGybFRztR0EvJQLCEUWId1InsECz59OpsvdnabM0WXjbCymiiah2ebTSeRn1JsxSEpZwY0w1byvZyoi2jHKZrUWZAEToiA+g6U5AUTC+fJTzFLxzSx4nU7iMsnqFXPXKSGjNJY6dMiR0anyvA67huZpO3vZwJlVkQtAyUZBxbiYvqcZ9poJZPcO1EQWzM4ul1UQK3FzmaYJ5STTXLPMAgXCM0sRDghEtiAzzQRA0ZHQd4JC24LSUjoMC5szJumZYXAWai79qXMG2ch8liSlRxK14ifKAMZK7Zsg9LlNTMDtM6PHSVag1JHeVMmcxTOlcrJTdTrx2uGEgCd4/nsHTOvDL/JJAa0kJd723shfxXmeNyNGqY0lJ60RXRZsSUAwVcUJmmRPTz6GCaR8VtxXF+MPW4owV35HP7+wty3yfdeDp2J3LXZ/PiB452sKcpGjSpqTDh/DplZx4pyTt+pNMFd7qUxYJLfK694No+N15wY59LF1zqcyQ2027YyyMOib1shpFmg6G99FQmU0Lq9Iqy/MbNEFce1XaWR0wkduLHsRfyv0/Y8p2V5EXXpSr+dFIXA5vPsOU0la+bYaWuGtpclvOYv5/P5RWOStFnJb3316wp3JS619b19LAy1txrGvpv57JxsrUZvt7c/rDd3N2q3tVV9Aw9Ry9RiN6gXfQOHaJjRBFHn9EX9LXxrfGj8bPxq5TeWKl8nqDaavz+AxOERfQ=</latexit>✓⇤<latexit sha1_base64="WCqN4NyGcU/VS7v36JpDIDbPFR0=">AAAGMnicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZW6o1S0SZWJKo/jSaz4JdvTJprOH7CFL+FnYIfY8hF4MhOm4xQJS4lvzjnX9+Ebx4pRYzud7yurd+627t1fe7D+8NHjJ083Np+dGZlqTE6xZFL3YmQIo4KcWmoZ6SlNEI8Z6caT/YLvXhJtqBSf7EyRAUcjQROKkXXQyfbRxUa7s9WZL7hshJXRBtU6vthsvYiGEqecCIsZMqYfdpQdZEhbihnJ16PUEIXwBI1I35kCcWIG2TzVHL52yBAmUruPsHCO3vTIEDdmxmOn5MiOjc8V4G1cP7XJ+0FGhUotEbgMlKQMWgmLuuGQaoItm8HGiQLZmMb5bVECtxc5mmCRUkM1zzyARLhGaGRJABMmkQ3gSCM1pngawIm0xG0cTQgmjDkrZZZqeRVAKoaufQnVxnmYNMZIFffhJcJGypDUNVsOyRIlNbVj3oTHrlKtSdJEGVUm9ZTO1UrJTO61wxVDksDd4yVZOmdRmX8SkZrwQt3sbeyF/FeZ03I0GpjSUnrRFdJmQpUDBbnCknMkhll0mGdRcVtxnB3mHndScyc+d3BQkwc+6cbTsbuRuz6bFT9gtAs9TdGgWUMFEWO3KXuLSEnW8yOd19z5UhY1l/hct+a6PjetuanP8ZrjPodik/fDQRYxktjrdhhpOhrba09lUiWk5jeU5Tdsh7DyqLaLLKIisTM/jr2S/33Ctu+sJCu6LlXxp5O6GNhsji2nqXzdHCt11dBmspzH7MNiLm9wWIohLen9v2ZD4abUvbOup8eVse5e09B/O5eNs+2t8O3Wzsed9t5O9a6ugZfgFXgDQvAO7IEjcAxOAQYJ+Ay+gK+tb60frZ+tX6V0daXyeQ4aq/X7D5BUQzo=</latexit>2H
<latexit sha1_base64="DUYzAxfrNGgpQdgmRgW/+ssHM14=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtF0voAtfAlf0x1iy0/gSSak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tl3z7ErShUnyxEwU9TgaCpjQh1kHH4cVGs7XVmi68bISV0UTVOrrYbLyK+jLJOAibMGJMN2wp28uJtjRhUKxHmQFFkhEZQNeZgnAwvXyaaYHfOqSPU6ndR1g8RW975IQbM+GxU3Jih8bnSvAurpvZ9GMvp0JlFkQyC5RmDFuJy7Jxn2pILJvg2omC2JjGxV1RAreXOZpgnlJNNc08wCBcIzSxEOCUSWIDPNBEDWkyDvBIWnAbJyNIgDFnZcxSLa8CTEXftS+l2jgPk8UJUeV1eImwgTKQuWbLPixRUlM75HV46CrVGtI6yqgymad0rlZKZgqvHa4YSAN3j5ewdM68Mv8kkBp4qa73NvZC/qvM8Ww0apjSUnrRFdFmRJUDBVwlknMi+nl0WORReVtxnB8WHney4E587uBgQR74pBtPx+5G7vpsXv7A0S72NGWDJjUVJozdpezMI6V5x490vuDOl7JYcKnPtRdc2+fGC27sc3zBcZ8jsSm6YS+PGKT2uhlGmg6G9tpTmUwJqfkt5ewbN0NceVTbRR5RkdqJH8deyf8+Ydt3VpKVXZeq/NNJXQ5sPsWW01S+borNdNXQ5nI2j/nn+Vze4hIp+nRG7/81awo3pe6ZdT09qox195qG/tu5bJxtb4Xvt3aOd5p7O9W7uoZeozfoHQrRB7SHPqEjdIoSBOgr+oa+N340bho/G79m0tWVyuclqq3G7z+/YkLn</latexit>1<latexit sha1_base64="DUYzAxfrNGgpQdgmRgW/+ssHM14=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtF0voAtfAlf0x1iy0/gSSak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tl3z7ErShUnyxEwU9TgaCpjQh1kHH4cVGs7XVmi68bISV0UTVOrrYbLyK+jLJOAibMGJMN2wp28uJtjRhUKxHmQFFkhEZQNeZgnAwvXyaaYHfOqSPU6ndR1g8RW975IQbM+GxU3Jih8bnSvAurpvZ9GMvp0JlFkQyC5RmDFuJy7Jxn2pILJvg2omC2JjGxV1RAreXOZpgnlJNNc08wCBcIzSxEOCUSWIDPNBEDWkyDvBIWnAbJyNIgDFnZcxSLa8CTEXftS+l2jgPk8UJUeV1eImwgTKQuWbLPixRUlM75HV46CrVGtI6yqgymad0rlZKZgqvHa4YSAN3j5ewdM68Mv8kkBp4qa73NvZC/qvM8Ww0apjSUnrRFdFmRJUDBVwlknMi+nl0WORReVtxnB8WHney4E587uBgQR74pBtPx+5G7vpsXv7A0S72NGWDJjUVJozdpezMI6V5x490vuDOl7JYcKnPtRdc2+fGC27sc3zBcZ8jsSm6YS+PGKT2uhlGmg6G9tpTmUwJqfkt5ewbN0NceVTbRR5RkdqJH8deyf8+Ydt3VpKVXZeq/NNJXQ5sPsWW01S+borNdNXQ5nI2j/nn+Vze4hIp+nRG7/81awo3pe6ZdT09qox195qG/tu5bJxtb4Xvt3aOd5p7O9W7uoZeozfoHQrRB7SHPqEjdIoSBOgr+oa+N340bho/G79m0tWVyuclqq3G7z+/YkLn</latexit>1
Figure 6: Illustration of supp( Zi). It has equal section with θ⋆and∥Zi∥∞≤R.
Proof of Proposition 3 Define Z⊤
iθ⋆=Uifor all i∈[K]. With slight abusing of notations, we
define optimal arm a⋆= arg max iZ⊤
iθ⋆and suboptimal arm a′= arg max i̸=a⋆Z⊤
iθ⋆for the proof.
[1] Decomposition by conditioning. We can bound the margin probability by conditioning, as
P[Ua⋆−Ua′≤ε]
=E[1(Ua⋆−Ua′≤ε)]
=E[1(Ua⋆−Ua′≤ε)|Ua′≤H]P[Ua′≤H] +E[1(Ua⋆−Ua′≤ε)|Ua′> H]P[Ua′> H]
≤E[1(Ua⋆−Ua′≤ε)|Ua′≤H]P[Ua′≤H] +δ(By the second condition of the proposition)
≤E[1(Ua⋆−Ua′≤ε)|Ua′≤H] +δ
≤E
E[1(Ua⋆−Ua′≤ε)| {Ua′≤H} ∩Ω⋆
i({zj}j̸=i)]
+δ
≤E
E[1(0≤Ui−max
j̸=iz⊤
jθ⋆≤ε)| {max
j̸=iz⊤
jθ⋆≤H} ∩Ω⋆
i({zj}j̸=i)]
+δ
=E
E[1(max
j̸=iz⊤
jθ⋆≤Z⊤
iθ⋆≤ε+ max
j̸=iz⊤
jθ⋆)| {max
j̸=iz⊤
jθ⋆≤H} ∩Ω⋆
i({zj}j̸=i)]
+δ
We set the density of Zi| {max j̸=iz⊤
jθ⋆≤H} ∩Ω⋆
i({zj}j̸=i)andZ⊤
iθ⋆| {max j̸=iz⊤
jθ⋆≤
H} ∩Ω⋆
i({zj}j̸=i)asf2andg2.
[2] Support of f2(·),g2(·)and their geometry. We first examine the support of the conditional
density f2, which is a density of Zi=z| {max j̸=iz⊤
jθ⋆≤H} ∩Ω⋆
i({zj}j̸=i). Under the given
Ω⋆
i({zj}j̸=i), we have z⊤θ⋆≥max j̸=iz⊤
jθ⋆by the definition of Ω⋆
i({zj}j̸=i), since arm ishould
be the optimal arm. When we write max j̸=iz⊤
jθ⋆=bwithb≤H, the support of f2becomes
{z∈A|z⊤θ⋆≥b}.
Recall that A:= supp( Zi)and it has equal section with direction θ⋆. Using Lemma 1, the density
f2(·)has LAC with constant function L(R)and it has bounded decay rate√
dL(R)by Lemma 3.
31[3] Bounding one-side decay rate. Next we aim to bound the one-side decay rate of
Z⊤
iθ⋆=y| {max j̸=iz⊤
jθ⋆≤H} ∩Ω⋆
i({zj}j̸=i). Define the (conditional) density of
Z⊤
iθ⋆=y| {max j̸=iz⊤
jθ⋆≤H} ∩Ω⋆
i({zj})asg2(y). Then, its support is restricted to the
region {y|b≤y≤H+ 1}.
Claim 3 The one-side decay rate of the projected density Z⊤
iθ⋆=y| {max j̸=iz⊤
jθ⋆≤H} ∩
Ω⋆
i({zj})is bounded by√
dL(R)in the interval [b, b+1
2].
Proof First, we observe that the support of Zi| {max j̸=iz⊤
jθ⋆≤H}∩Ω⋆
i({zj})is form of interval
[b, H+ 1]. This holds from the definition of Ω⋆
i({zj}and our design of A. Also this support has
equal sections with direction θ⋆. This is straightforward from the condition that Ais a cylindrical set
with the direction θ⋆and intersection with {z∈Rd|z⊤θ⋆≥b}also makes equal section with θ⋆.
Since b≤H, we can apply Lemma 8 we get wanted result.
3) Bounding maximum density by applying Corollary 5. Using the previous Lemma 5 and we
can conclude that the maximum density is bounded by 3√
dL(R)in the interval [b, b+1
2]. Finally,
we get the wanted result:
E[1(max
j̸=iz⊤
jθ⋆≤Zi≤max
j̸=iz⊤
jθ⋆+ε| {max
j̸=iz⊤
jθ⋆≤H} ∩Ωi({zj})]≤3√
dL(R)ε
for any ε <1
2.
G Proofs of Results for Unbounded Contexts
In this section, we prove our main theorems stated in Section 4.3. First, we present ways to get
truncation set, which used to prove Theorems 2 and 3. After that we prove two Theorems 2 and 3.
G.1 Constructing Truncation Sets
This section presents operations to construct truncation sets, which will be used in the proof of our
main results. In the proof, we first truncate our contexts X∈RdKto a truncation set C=QK
i=1D,
where D⊂Rd, and work with the truncated contexts. If the supremum norm of the set is bounded,
by combining it with LAC, we can bound the decay rate of the truncated contexts.
First, we define the directional completion of v. For a set Aand a vector v, we define the following
process: the filling of Ain the direction of v. This process expands Aso that its cross-section remains
the same when cut by a hyperplane orthogonal to v. This procedure ensures that all sections of A
along the direction vare equal. Recall that πS(A)denotes the projection of Aonto the subspace S.
IfSis the subspace spanned by a vector v, we write πv(A), which is a subset of a straight line.
Definition 12 (Set completion) ForA⊂Rdandv∈Sd−1, We define C[A, v] := π⟨v⟩⊥(A) +
vπv(A)
After the completion, the section sliced by the normal hyperplane of vis the same.
32<latexit sha1_base64="vuBQhbLi3xKK26k2IMHUxypayyI=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG6K2krsaCvaBGWiyjO5k1jxS7anTTSdL2ALX8LXdIfY8hN4kgnpOEXCUuKbc871ffjGsWLU2FbremX1zt3GvftrD9YfPnr85OnG5rMzIzOdwGkimdSdmBhgVMCppZZBR2kgPGbQjkf7Jd++AG2oFJ/tREGPk4GgKU2IddDxh/ONZmurNV142Qgro4mqdXS+2XgR9WWScRA2YcSYbthStpcTbWnCoFiPMgOKJCMygK4zBeFgevk00wK/dkgfp1K7j7B4it70yAk3ZsJjp+TEDo3PleBtXDez6fteToXKLIhkFijNGLYSl2XjPtWQWDbBtRMFsTGNi9uiBG4vczTBPKWaapp5gEG4RmhiIcApk8QGeKCJGtJkHOCRtOA2TkaQAGPOypilWl4GmIq+a19KtXEeJosTosrr8BJhA2Ugc82WfViipKZ2yOvw0FWqNaR1lFFlMk/pXK2UzBReO1wxkAbuHi9g6Zx5Zf5JIDXwUl3vbeyF/FeZ49lo1DClpfSiK6LNiCoHCrhMJOdE9PPosMij8rbiOD8sPO5kwZ343MHBgjzwSTeejt2N3PXZvPyBo13sacoGTWoqTBi7TdmZR0rzjh/py4L7spTFgkt9rr3g2j43XnBjn+MLjvsciU3RDXt5xCC1V80w0nQwtFeeymRKSM1vKGffuBniyqPazvOIitRO/Dj2Uv73Cdu+s5Ks7LpU5Z9O6nJg8ym2nKbydVNspquGNpezecw/zefyBpdI0aczev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8u7VzvNPc26ne1TX0Er1Cb1CI3qE99BEdoVOUIEBf0Tf0vfGjcd342fg1k66uVD7PUW01fv8BHEFC9w==</latexit>A
<latexit sha1_base64="S3MY0AHnz1mHFgJwKbfKKmJhj1o=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHHlxcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AT8FDLA==</latexit>v
<latexit sha1_base64="HnLrOK3cCzSDWc1HWGo/nu1H5c0=">AAAGQnicjZTNbhMxEMfdQqCUrxRucLGIkDisqmxVAVIvRW0lboSKNqmyUeR1vIkVr23Z3jSRu2/CFZ6El+AVuCGuHPAmG9J1ioSlxJP5/e0ZjyeOJaPaNJvfNzZv3a7dubt1b/v+g4ePHtd3npxrkSlMzrBgQnVipAmjnJwZahjpSEVQGjPSjsdHBW9PiNJU8E9mJkkvRUNOE4qRca5+vR6lyIwwYvYo774LJr1+vdHcbc4HXDfC0miAcrT6O7Vn0UDgLCXcYIa07oZNaXoWKUMxI/l2lGkiER6jIek6k6OU6J6dp57Dl84zgIlQ7sMNnHuvr7Ao1XqWxk5ZJKp9VjhvYt3MJG97lnKZGcLxIlCSMWgELOoAB1QRbNgMVnbkyMQ0zm+KEri5yFEHy5QqqnnmASTcFUIhQwKYMIFMAIcKyRHF0wCOhSFuStGYYMKYszJmqBKXAaR84MqXUKXdCp3FGMnifrxE2FBqkrliiwFZQ0JRM0qr7pE7qVIkqXoZlTrzlG6pEYLp3CuHOwxJAnePE7K2z/Jk/k5EKJIW6mptYy/kv445XbRGxSeVEF50iZQeU+mcnFxikaaID2x0ktt5R8exPck9drpipz47Pl7BYx+69nT0IHLXZ2zxA0YH0NMUBZpVVBAxdpOys4yU2I4f6WLFLtayWLHEZ+0Va/tsumJTn6UrlvoMxTrvhj0bMZKYq0YYKTocmStPpTPJhUqvKRffsBHCckU59W1EeWJmfhxzKf57hz1/sRSsqLqQxZ9OqKJh7dy3nqb0dXPfQlc2rRWLfrQfln15jWHBB3SBj/6aFYXrUvfuupq2SmPbvaah/3auG+d7u+Hr3f2P+43D/fJd3QLPwQvwCoTgDTgE70ELnAEMJuAz+AK+1r7VftR+1n4tpJsb5ZqnoDJqv/8AT05JiQ==</latexit>C[A, v]Figure 7: Illustration of C(A, v)process. This is the operation of filling the area Ain the vdirection.
Then, the whole sections with direction vbecome equal .
Lemma 11 Suppose a set A∈Rdand unit vector vsatisfies πv(A)is an interval with length ℓ.
Then we have
∥C[A, v]∥∞≤ ∥A∥∞+ℓ.
Proof For all p∈ C[A, v], there exists q∈Asuch that p=q+hvforh < ℓ . Then
∥p∥∞≤ ∥q∥∞+∥hv∥∞≤ ∥A∥∞+ℓ
holds.
Next, we define partial completion, which makes the section with direction vin the area of C(A, v)∩
[−1,1]vequal.
Definition 13 (Partial completion) For a set A⊂Rdand a unit vector v∈Rd, we define the
completing operator as
P(A, v) :=C[A∩ {x| |x·v| ≤1}, v]∪A
Recall that πv(·)is a projection to the direction v.
<latexit sha1_base64="cyNy8a0boOK0+jHlRXxI5wXpX+w=">AAAGMnicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG6K2krsKBVtUmWiyuN4Eit+yfa0iabzB2zhS/gZ2CG2fASezITpOEXCUuKbc871ffjGsWLU2E7n+8rqrdutO3fX7q3ff/Dw0eONzSenRqYakxMsmdS9GBnCqCAnllpGekoTxGNGuvFkv+C7F0QbKsUnO1NkwNFI0IRiZB10/G79fKPd2erMF1w2wspog2odnW+2nkVDiVNOhMUMGdMPO8oOMqQtxYzk61FqiEJ4gkak70yBODGDbJ5qDl86ZAgTqd1HWDhHr3tkiBsz47FTcmTHxucK8Caun9rk7SCjQqWWCFwGSlIGrYRF3XBINcGWzWDjRIFsTOP8piiB24scTbBIqaGaZx5AIlwjNLIkgAmTyAZwpJEaUzwN4ERa4jaOJgQTxpyVMku1vAwgFUPXvoRq4zxMGmOkivvwEmEjZUjqmi2HZImSmtoxb8JjV6nWJGmijCqTekrnaqVkJvfa4YohSeDu8YIsnbOozD+JSE14oW72NvZC/qvMaTkaDUxpKb3oCmkzocqBglxiyTkSwyw6zLOouK04zg5zjzuuuWOfOzioyQOfdOPp2N3IXZ/Nih8w2oWepmjQrKGCiLGblL1FpCTr+ZHOau5sKYuaS3yuW3Ndn5vW3NTneM1xn0OxyfvhIIsYSexVO4w0HY3tlacyqRJS82vK8hu2Q1h5VNt5FlGR2Jkfx17K/z5h23dWkhVdl6r400ldDGw2x5bTVL5ujpW6amgzWc5j9mExl9c4LMWQlvT+X7OhcFPq3lnX06PKKF7T0H87l43T7a3w9dbOx5323k71rq6B5+AFeAVC8AbsgffgCJwADBLwGXwBX1vfWj9aP1u/SunqSuXzFDRW6/cff7FDCw==</latexit>A
<latexit sha1_base64="23h/u6l/XcdnkFGvVZ6SuC5DmaI=">AAAGMnicjVTLbhMxFHWBQCmvFnawsYiQWIyqTFUBUjeV2krsKBVtUnWiyuPcSaz4JdvTJprmD9jCl/AzsENs+Qg8yYR0nCJhKfHNOef6PnzjVHNmXav1feXW7TuNu/dW7689ePjo8ZP1jacnVuWGwjFVXJlOSixwJuHYMcehow0QkXJop8O9km9fgLFMyU9urKErSF+yjFHiPHR0sXa+3mxttqYLLxtxZTRRtQ7PNxrPk56iuQDpKCfWnsUt7boFMY5RDpO1JLegCR2SPpx5UxIBtltMU53gVx7p4UwZ/5EOT9HrHgUR1o5F6pWCuIENuRK8iTvLXfauWzCpcweSzgJlOcdO4bJu3GMGqONjXDtREpeydHJTlMjvZY42mqdUU00zjzBI3whDHEQ444q4CPcN0QNGRxEeKgd+E2QIFDj3Vs4dM+oywkz2fPsyZqz3sHlKiS7vI0iE97WF3Ddb9WCJUoa5gajDA1+pMZDVUc60zQOld3VKcTsJ2uGLgSzy93gBS+fMKwtPAmVAlOp6b9Mg5L/KHM1Go4Zpo1QQXRNjh0x7UMIlVUIQ2SuSg0mRlLeVpsXBJOCOFtxRyO3vL8j9kPTj6dmdxF+fK8ofONnBgaZs0LimwoTzm5SdeaSs6ISRThfc6VIWCy4LufaCa4fcaMGNQk4sOBFyJLWTs7hbJBwyd9WME8P6A3cVqGyupTLimnL2jZsxrjyq7bxImMzcOIzjLtV/n7AVOmvFy64rXf7plCkHtphiy2nqUDfFZrpqaAs1m8fiw3wur3FUyR6b0Xt/zZrCT6l/Z31PDyujfE3j8O1cNk62NuM3m9sft5u729W7uopeoJfoNYrRW7SL3qNDdIwoytBn9AV9bXxr/Gj8bPyaSW+tVD7PUG01fv8Bs2ZDQA==</latexit>v<latexit sha1_base64="23h/u6l/XcdnkFGvVZ6SuC5DmaI=">AAAGMnicjVTLbhMxFHWBQCmvFnawsYiQWIyqTFUBUjeV2krsKBVtUnWiyuPcSaz4JdvTJprmD9jCl/AzsENs+Qg8yYR0nCJhKfHNOef6PnzjVHNmXav1feXW7TuNu/dW7689ePjo8ZP1jacnVuWGwjFVXJlOSixwJuHYMcehow0QkXJop8O9km9fgLFMyU9urKErSF+yjFHiPHR0sXa+3mxttqYLLxtxZTRRtQ7PNxrPk56iuQDpKCfWnsUt7boFMY5RDpO1JLegCR2SPpx5UxIBtltMU53gVx7p4UwZ/5EOT9HrHgUR1o5F6pWCuIENuRK8iTvLXfauWzCpcweSzgJlOcdO4bJu3GMGqONjXDtREpeydHJTlMjvZY42mqdUU00zjzBI3whDHEQ444q4CPcN0QNGRxEeKgd+E2QIFDj3Vs4dM+oywkz2fPsyZqz3sHlKiS7vI0iE97WF3Ddb9WCJUoa5gajDA1+pMZDVUc60zQOld3VKcTsJ2uGLgSzy93gBS+fMKwtPAmVAlOp6b9Mg5L/KHM1Go4Zpo1QQXRNjh0x7UMIlVUIQ2SuSg0mRlLeVpsXBJOCOFtxRyO3vL8j9kPTj6dmdxF+fK8ofONnBgaZs0LimwoTzm5SdeaSs6ISRThfc6VIWCy4LufaCa4fcaMGNQk4sOBFyJLWTs7hbJBwyd9WME8P6A3cVqGyupTLimnL2jZsxrjyq7bxImMzcOIzjLtV/n7AVOmvFy64rXf7plCkHtphiy2nqUDfFZrpqaAs1m8fiw3wur3FUyR6b0Xt/zZrCT6l/Z31PDyujfE3j8O1cNk62NuM3m9sft5u729W7uopeoJfoNYrRW7SL3qNDdIwoytBn9AV9bXxr/Gj8bPyaSW+tVD7PUG01fv8Bs2ZDQA==</latexit>v
<latexit sha1_base64="f+AVD9neVTU6EUCCiEtalYV/hUc=">AAAGQnicjZTNbhMxEMddIFDKVwo3uFhESEVaVdmqAqReCm0lboSKNqmyUeV1vIkVr23Z3jSRu2/CFZ6El+AVuCGuHPAmG9J1ioSlxJP5/e0ZjyeOJaPaNJvf127cvFW7fWf97sa9+w8ePqpvPj7VIlOYnGDBhOrESBNGOTkx1DDSkYqgNGakHY8OCt4eE6Wp4J/MVJJeigacJhQj41zn9XqUIjPEiNlWvvU2GL88rzea283ZgKtGWBoNUI7W+WbtadQXOEsJN5ghrbthU5qeRcpQzEi+EWWaSIRHaEC6zuQoJbpnZ6nn8IXz9GEilPtwA2feqyssSrWeprFTFolqnxXO61g3M8mbnqVcZoZwPA+UZAwaAYs6wD5VBBs2hZUdOTIxjfProgRuLnLUwSKlimqWeQAJd4VQyJAAJkwgE8CBQnJI8SSAI2GIm1I0Ipgw5qyMGarERQAp77vyJVRpt0JnMUayuB8vETaQmmSu2KJPVpBQ1AzTqnvoTqoUSapeRqXOPKVbaoRgOvfK4Q5DksDd45is7LM4mb8TEYqkhbpa29gL+a9jTuatUfFJJYQXXSKlR1Q6JycXWKQp4n0bHeV21tFxbI9yjx0v2bHPDg+X8NCHrj0d3Yvc9Rlb/IDRHvQ0RYGmFRVEjF2n7CwiJbbjRzpbsrOVLJYs8Vl7ydo+myzZxGfpkqU+Q7HOu2HPRowk5rIRRooOhubSU+lMcqHSK8r5N2yEsFxRTuc2ojwxUz+OuRD/vcOOv1gKVlRdyOJPJ1TRsHbmW01T+rqZb64rm9aKeT/aD4u+vMKw4H06xwd/zYrCdal7d11NW6Wx4V7T0H87V43Tne3w1fbux93G/rvyXV0Hz8BzsAVC8Brsg/egBU4ABmPwGXwBX2vfaj9qP2u/5tIba+WaJ6Ayar//AEjWST0=</latexit>P(A, v)
Figure 8: Illustration of P(A, v).
P(A, v)has same section with the direction vwith the area [−1,1]v. Equivalently,
P(A, v)∩ {x|x⊤v=u1} ≡ P (A, v)∩ {x|x⊤v=u2}
for every u1̸=u2∈[−1,1]. The partial completion operator has a bounded sup-norm, and the
following lemma shows the result.
33Lemma 12 (Sup norm bound of P(A, v))For any A⊂Rdandv∈Sd−1,
∥P(A, v)∥∞≤ ∥A∥∞+ 2
holds.
Proof For any y∈ P(A, v), there is y′∈Asuch that y=y′+tvfor|t| ≤2. Then, it is
straightforward by taking the sup norm.
G.2 Proof of Theorem 2
Our goal is to prove
E[Xaθ(X(t))(t)Xaθ(X(t))(t)⊤]≥λ⋆(t)>0
for any tandθ. Then, we fix arbitrary history Ht−1and fix any θ∈Sd. For simplicity, define
(X⊤
1. . . X⊤
K)as the conditioned random variable of (X1(t)⊤. . . X K(t)⊤)| Ht−1. The condition of
Theorem 2 means that X= (X⊤
1. . . X⊤
K)satisfies boundedness Assumption 2 and LAC condition.
Under conditioning the history Ht−1, we aim to apply the Proposition 2 for any θandv. We also set
a= arg max X⊤
iθand for any fixed v∈Rd,∥v∥= 1, our goal is to calculate the lower bound of
E[v⊤XaXav⊤].
We can view it as the fixed history random variable and we aim to use the arguments developed in
Section F.
To use Proposition 2, we first truncate our contexts (X⊤
1, . . . X⊤
K)into some region Cv
1and then
apply Proposition 2 to the truncated contexts. To meet the geometric conditions of Proposition 2, we
do the truncation.
G.2.1 Constructing Truncation Sets
We first define the truncation set rigorously.
1. Define R1=c0xmax(2 + log dK).
2. Define D:= [−R1, R1]d.
3. Define Dv:=P[D, v].
4. Set truncation set Cv
1:= (Dv)K.
5. Then ∥Cv
1∥∞≤R1+ 2holds. (by Lemma 12)
34<latexit sha1_base64="ihXkrSXpKNbYX0TMl/lKbS2nloU=">AAAGMXicjVTLbhMxFHWBQCmvFnawsYiQWIyqTFUBUjeV2krs+hBtUmWiyuPcSax4bMv2tImm+QK28CV8TXeILT+BJzMhHadIWEp8c865vg/fOFacGdtq3azcu/+g8fDR6uO1J0+fPX+xvvHyzMhMUzilkkvdiYkBzgScWmY5dJQGksYc2vFor+Dbl6ANk+KLnSjopWQgWMIosQ46PrxYb7Y2W7OFl42wMpqoWkcXG43XUV/SLAVhKSfGdMOWsr2caMsoh+lalBlQhI7IALrOFCQF08tnmU7xO4f0cSK1+wiLZ+htj5ykxkzS2ClTYofG5wrwLq6b2eRTL2dCZRYELQMlGcdW4qJs3GcaqOUTXDtREBuzeHpXlMDtRY4mmKdUU80yDzAI1whNLAQ44ZLYAA80UUNGxwEeSQtuS8kIKHDurIxbpuVVgJnou/YlTBvnYbKYElVch5cIHygDmWu27MMSJTWzw7QOD12lWkNSRzlTJvOUztVKyc3Ua4crBpLA3eMlLJ0zr8w/CaSGtFDXext7If9V5rgcjRqmtJRedEW0GTHlQAFXVKYpEf08OpjmUXFbcZwfTD3uZMGd+Nz+/oLc90k3no7didz12bz4gaMd7GmKBk1qKkw4v0vZmUdK8o4f6XzBnS9lseASn2svuLbPjRfc2OfSBZf6HInNtBv28ohDYq+bYaTZYGivPZXJlJA6vaUsv3EzxJVHtV3kEROJnfhx7JX87xO2fGcledF1qYo/ndTFwOYzbDlN5etmWKmrhjaX5Tzmh/O5vMVRKfqspPf+mjWFm1L3zLqeHlXGmntNQ//tXDbOtjbDD5vbx9vN3e3qXV1Fb9Bb9B6F6CPaRZ/RETpFFAH6ir6h740fjZvGz8avUnpvpfJ5hWqr8fsPbXdDBQ==</latexit>O<latexit sha1_base64="2PNKgnMZ6DeQepcQNkvhiuGw+W0=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhFmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNNE0X8AWvoSv6Q6x5SfwJBPScYqEpcQ355zr+/CNY8Wosc3mzdr6vfu1Bw83Hm0+fvL02fOt7RfnRmY6gbNEMqnbMTHAqIAzSy2DttJAeMygFQ8PCr41Am2oFF/sREGXk76gKU2IddDJ6HKr3mw0ZwuvGmFp1FG5ji+3a6+inkwyDsImjBjTCZvKdnOiLU0YTDejzIAiyZD0oeNMQTiYbj7LdIrfOqSHU6ndR1g8Q2975IQbM+GxU3JiB8bnCvAurpPZ9GM3p0JlFkQyD5RmDFuJi7Jxj2pILJvgyomC2JjG07uiBG4vcjTBIqWKapZ5gEG4RmhiIcApk8QGuK+JGtBkHOChtOA2ToaQAGPOypilWl4FmIqea19KtXEeJosToorr8BJhfWUgc82WPVihpKZ2wKvwwFWqNaRVlFFlMk/pXK2UzEy9drhiIA3cPY5g5ZxFZf5JIDXwQl3tbeyF/FeZ4/loVDClpfSiK6LNkCoHCrhKJOdE9PLoaJpHxW3FcX409bjTJXfqc4eHS/LQJ914OnYvctdn8+IHjvawpykaNKmoMGHsLmV7ESnN236kiyV3sZLFkkt9rrXkWj43XnJjn+NLjvscic20E3bziEFqr+thpGl/YK89lcmUkJrfUs6/cT3EpUe5XeYRFamd+HHslfzvE3Z8ZyVZ0XWpij+d1MXA5jNsNU3l62bYXFcObS7n85h/XszlLS6Rokfn9MFfs6JwU+qeWdfT49LYdK9p6L+dq8b5TiN839g92a3vN8p3dQO9Rm/QOxSiD2gffULH6AwlCNBX9A19r/2o3dR+1n7Npetrpc9LVFm1338ATfNDJg==</latexit>v
<latexit sha1_base64="Kk3Or+yljz+bjdi7xspXy9JPyyA=">AAAGM3icjVTLbhMxFHULA6U82sIONhYREotRlakqQOqmUlOJHaXQJlUmVB7Hk1jxS7anTTSdT2ALX8LHIHaILf+AJ5mQjlMkLCW+Oedc34dvnChGjW02v6+s3rod3Lm7dm/9/oOHjzY2tx6fGplpTE6wZFJ3EmQIo4KcWGoZ6ShNEE8YaSejg5JvXxBtqBQf7USRHkcDQVOKkXXQh9ani/PNRnO7OV1w2YgqowGqdXS+FTyN+xJnnAiLGTKmGzWV7eVIW4oZKdbjzBCF8AgNSNeZAnFievk01wK+cEgfplK7j7Bwil73yBE3ZsITp+TIDo3PleBNXDez6ZteToXKLBF4FijNGLQSloXDPtUEWzaBtRMFsglNipuihG4vczThPKWaapp5CIlwjdDIkhCmTCIbwoFGakjxOIQjaYnbOBoRTBhzVsYs1fIyhFT0XftSqo3zMFmCkSovxEuEDZQhmWu27JMlSmpqh7wOD12lWpO0jjKqTOYpnauVkpnCa4crhqShu8cLsnTOvDL/JCI14aW63tvEC/mvMsez0ahhSkvpRVdImxFVDhTkEkvOkejn8WGRx+VtJUl+WHjc8YI79rlWa0G2fNKNp2P3Ynd9Ni9/wHgPepqyQZOaCiLGblJ25pHSvONHOltwZ0tZLLjU59oLru1z4wU39jm+4LjPocQU3aiXx4yk9qoRxZoOhvbKU5lMCan5NeXsGzYiWHlU23keU5HaiR/HXsr/PmHHd1aSlV2XqvzTSV0ObD7FltNUvm6KzXTV0OZyNo/5u/lcXuOwFH06ow/+mjWFm1L30LqeHlXGuntNI//tXDZOd7ajV9u773cb+7vVu7oGnoHn4CWIwGuwD96CI3ACMBiAz+AL+Bp8C34EP4NfM+nqSuXzBNRW8PsPTvlD4g==</latexit>Dv
<latexit sha1_base64="ziHR1p5GzyRZVvjNDGMdye2C5bo=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmSoCpG4qNZXY0Va0SZWJKo9zJ7HisS3b0yaa5gvYwpfwNd0htvwEnmRCOk6RsJT45pxzfR++caw4M7bRuFlbv3e/9uDhxqPNx0+ePnu+tf3izMhMUzilkkvdiYkBzgScWmY5dJQGksYc2vHooODbl6ANk+KLnSjopWQgWMIosQ46bl1s1Rs7jdnCq0ZYGnVUrqOL7dqrqC9ploKwlBNjumFD2V5OtGWUw3QzygwoQkdkAF1nCpKC6eWzTKf4rUP6OJHafYTFM/S2R05SYyZp7JQpsUPjcwV4F9fNbPKxlzOhMguCzgMlGcdW4qJs3GcaqOUTXDlREBuzeHpXlMDtRY4mWKRUUc0yDzAI1whNLAQ44ZLYAA80UUNGxwEeSQtuS8kIKHDurIxbpuVVgJnou/YlTBvnYbKYElVch5cIHygDmWu27MMKJTWzw7QKD12lWkNSRTlTJvOUztVKyc3Ua4crBpLA3eMlrJyzqMw/CaSGtFBXext7If9V5ng+GhVMaSm96IpoM2LKgQKuqExTIvp5dDjNo+K24jg/nHrcyZI78blWa0m2fNKNp2P3Ind9Ni9+4GgPe5qiQZOKChPO71J2FpGSvONHOl9y5ytZLLnE59pLru1z4yU39rl0yaU+R2Iz7Ya9POKQ2Ot6GGk2GNprT2UyJaRObynn37ge4tKj3C7yiInETvw49kr+9wm7vrOSvOi6VMWfTupiYPMZtpqm8nUzbK4rhzaX83nMPy/m8hZHpeizOX3w16wo3JS6Z9b19Kg0Nt1rGvpv56pxtrsTvt9pHjfr+83yXd1Ar9Eb9A6F6APaR5/QETpFFAH6ir6h77UftZvaz9qvuXR9rfR5iSqr9vsPLahC+g==</latexit>D<latexit sha1_base64="CEshLfCf6/+AUiAmdWXSb32QrMQ=">AAAGNnicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXYUSrapMpElWdyJzHxS7anTTTNP7CFL+FX2LBDbPkEPJkJ6ThFwlLim3PO9X34xrFi1NhW6/vK6q3bjTt31+6t33/w8NHjjc0np0ZmOoGTRDKpOzExwKiAE0stg47SQHjMoB2P9gu+fQHaUCk+2omCHicDQVOaEOug08gOwZLzjWZrqzVbeNkIK6OJqnV0vtl4FvVlknEQNmHEmG7YUraXE21pwmC6HmUGFElGZABdZwrCwfTyWbpT/NIhfZxK7T7C4hl63SMn3JgJj52SEzs0PleAN3HdzKZvezkVKrMgkjJQmjFsJS5qx32qIbFsgmsnCmJjGk9vihK4vcjRBPOUaqpZ5gEG4RqhiYUAp0wSG+CBJmpIk3GAR9KC2zgZQQKMOStjlmp5GWAq+q59KdXGeZgsTogq7sRLhA2Ugcw1W/ZhiZKa2iGvw0NXqdaQ1lFGlck8pXO1UjIz9drhioE0cPd4AUvnzCvzTwKpgRfqem9jL+S/yhyXo1HDlJbSi66INiOqHCjgMpGcE9HPo8NpHhW3Fcf54dTjjhfcsc8dHCzIA5904+nY3chdn82LHzjaxZ6maNCkpsKEsZuUnXmkNO/4kc4W3NlSFgsu9bn2gmv73HjBjX2OLzjucyQ2027YyyMGqb1qhpGmg6G98lQmU0Jqfk1ZfuNmiCuPajvPIypSO/Hj2Ev53yds+85KsqLrUhV/OqmLgc1n2HKaytfNsFJXDW0uy3nM38/n8hqXSNGnJb3/16wp3JS6t9b19Kgy1t1rGvpv57Jxur0Vvt7a+bDT3Nup3tU19By9QK9QiN6gPfQOHaETlKBP6DP6gr42vjV+NH42fpXS1ZXK5ymqrcbvP6dIRVo=</latexit>ω<latexit sha1_base64="ygGISCpsp8s1Av8EAc6d0DS0oBE=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHH2xcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AxS9C6A==</latexit>2Figure 9: Illustration of the set Dv. The red rectangle is DandDvis union with green boundary
rectangle and D. We force to make equal sections with direction vin[−1,1]v. Hence we can apply
the previous results from Proposition 2!
Claim 4 For any Xwith Assumption 2, P[X∈Cv
1]≥1
2.
Proof First, we prove P[X∈DK]≥1
2. If we prove that, since Cv
1⊃DK, we get the result. Using
equation 15, we get
P[X∈(DK)c]≤KX
i=1P[Xi∈Dc]
≤KX
i=1dX
j=1P[|Xij|> R 1]
≤dK×1
2dK=1
2.
In the second inequality, we use the Assumption 2 and sub-exponential context concentration results
from Appendix J.2.
Next, we define the truncated contexts W=X| {X∈Cv
1}:= (W⊤
1, . . . W⊤
K).
Claim 5 The truncated contexts W= (W⊤
1. . . W⊤
K)andWasatisfies
E[v⊤XaXav]≥1
2E[v⊤Wa(Wa)⊤v].
Note that ais defined as a= arg max i∈[K]X⊤
iθ.
Proof First, observe that
E[v⊤XaX⊤
av]≥E[v⊤XaX⊤
av∩ {X=W}]
+E[v⊤XaX⊤
av∩ {X̸=W}]
≥E[v⊤XaX⊤
av∩ {X=W}]
≥E[v⊤WaW⊤
av| {X=W}]P[X=W]
≥1
2E[v⊤WaW⊤
av]
35and then we only need to bound the diversity of the truncated contexts, W= (W⊤
1, . . . W⊤
K).
G.2.2 Properties of Truncated Contexts W
We need to check two conditions: The sections of supp( Wi)with direction vin[−1,1]vshould be
the same and have a bounded decay rate√
dL(R1+ 2) . Since Xsatisfies LAC with functions Land
∥C1∥∞≤R1+ 2by the Lemma 12, it has a bounded decay rate of√
dL(R1+ 2) by Lemma 3.
Claim 6 The truncated contexts Whave LAC with L(·)and∥supp( W)∥∞≤R1+ 2.
Proof By the observation of Appendix C.3 for the relation of the conditioning and LAC, it
is clear that Whas LAC with function L. Also, by the construction of C1and Lemma 11,
∥supp( W)∥∞≤R1+ 2is true.
Claim 7 For every i∈[K],supp( Wi)is identical for all i∈[K]andsupp( Wi)∩[−1,1]vhas
equal sections with direction v.
Proof Since we set Dvas the partial completion with direction v, it is straightforward.
G.2.3 Applying Proposition 2
We already checked that our truncated contexts Wfulfill the condition of the Proposition 2. Hence,
we apply Proposition 2 and we get the bound,
E[WaW⊤
a]⪰c
d(A1+A2(R1+ 2)α)2Id.
hence we get
E[XaX⊤
a]⪰c
d(A1+A2(R1+ 2)α)2Id
for some absolute constant c >0.
G.3 Proof of Theorem 3
We aim to use the result of Proposition 3. We first consider the case ∥θ⋆∥2= 1, and for general case,
using the same argument from Appendix F.4 and (3), we can adjust it. We first truncate our contexts
X= (X⊤
1. . . X⊤
K)to some region C2=DK,D⊂Rd, with slight abusing of notations. We define
the truncated contexts as W= (W⊤
1, . . . W⊤
K). Note that the support of WiisD. To satisfy the
conditions of Proposition 3, we want to make the sections with direction θ⋆of the support of Withe
same.
G.3.1 Constructing Truncation Sets
We construct the truncation set C2as follows. Firstly, set R3=c0xmax(1 + log dK+1
2logT)and
R2=R3+ 1.
• Set D1:= ([−R2, R2])d⊂Rd.
• Set D2:={x∈Rd| −R2≤x⊤θ⋆≤R2} ⊂Rd
• Set D3:=D1∩D2.
• Set D:=C[D3, θ⋆].
• The final truncation set is constructed by C2=DK.
36Claim 8 The supnorm of the truncation set satisfies ∥supp( W)∥∞=∥D∥∞≤3R2and the W
has bounded LAC with L(3R2).
Proof The LAC of Whas the same function as the original, L(·)by Lemma 1. By Lemma 11, we
easily see that ∥supp( W)∥∞≤3R2since πθ⋆(D2)is an interval with length 2R2. Since L(·)is an
increasing function (by definition) and ∥supp( W)∥∞≤3R2,Whas a bounded LAC with L(3R2).
Claim 9 The region C2has equal sections with direction θ⋆andP[X∈C2]≥1−1√
T.
Proof By the moment Assumption 2, P[|Xij| ≤R2]≥1−1
2dK√
Tholds for all i, jand therefore
P[X∈DK
1]≥1−1
2√
Tholds. Next, by Assumption 2, we get P[X∈DK
2]≥1−1
2√
T. Hence
P[X∈DK
3]≥1−1√
TandD⊃D3, we get P[X∈C2]≥1−1√
T.
G.3.2 Truncation with High-probability Region
We truncate Xinto the high probability confidence region C2and define the truncated contexts as
W= (W⊤
1. . . W⊤
K)and play with W= (W⊤
1, . . . W⊤
K)from now on. The following calculation
shows that it is enough to bound the suboptimality gap of W.
P[∆(X)≤ε] =Z
RKdI∆(x)≤εf(x)dx
=Z
C2I∆(x)≤εf(x)dx+Z
Cc
2I∆(x)≤εf(x)dx
≤Z
C2I∆(x)≤εf(x)dx+P[X∈Cc
2]
=P[X∈C2]Z
C2I∆(x)≤εf(x)
P[X∈C2]dx+1√
T
=P[X∈C2]P[∆(W)≤ε] +1√
T
≤P[∆(W)≤ε] +1√
T. (11)
Therefore, if we find the constant C′
∆satisfying
P[∆(W)≤ε]≤C′
∆ε, (12)
we finally can bound C∆≤C′
∆.
Lemma 13 The truncated contexts W=X| {X∈C2}meet the condition of Proposition 3 with
H=R2+ 1andδ=1√
T.
Proof LetD= cyl( B, θ⋆, R2)for some set B⊂Pθ⋆and define D′= cyl( B, θ⋆, R2−1). We can
show that P[X∈D′]≥1−1√
Twith exactly same argument from Claim 9. Hence we can calculate
P[W∈D′]≥P[X∈D′]/P[X∈D]
≥1−1√
T
1= 1−1√
T
which means that it satisfies the condition of Proposition 3 with δ=1√
T.
37G.3.3 Applying Proposition 3
Finally, we aim to apply Proposition 3, which bounds the margin constant.
Then, by Proposition 3, we can get the equation (12) holds for
C′
∆= 3√
dL(R2) = 3√
d(A1+A2(3R2)α).
Then, finally, we get
C∆≤3√
d(A1+A2(3R2)α) =eO(√
d).
G.4 Proof of Theorem 1: Unbounded Contexts Case
We combine the previous Proposition 9 and our result of Theorem 2 and 3. Then we get
Reg(T)≤cC∆dx2
max1
λ⋆(logT)4
≤cC∆d2.5x2
max(A1+A2(R1+ 2)α)2(A1+A22αRα
2)(log T)4
≤eO(d2.5)
holds.
H Results for Bounded Contexts
Now, we present our main result for bounded contexts.
H.1 Two Cases of Bounded Contexts
In the linear contextual bandit setting, ℓ2boundedness is widely used. However, for light-tailed
distributions (such as Gaussian or exponential), the ℓ2norm is unbounded. Therefore, we divide
our analysis into two cases: unbounded and bounded contexts. While our previous focus was on
unbounded contexts, here we summarize and present the results for bounded contexts.
We classify the analysis into two cases: the first is for truncated contexts, and the second is for
naturally bounded contexts, which include cases where the uniform distribution on the ball BRor a
distribution with bounded density on the ball.
1) Truncated contexts. Truncated contexts refer to cases where the context distribution is a trun-
cated version of the original distribution. For example, truncated Gaussian and truncated exponential
distributions fall into this category.
2) Naturally bounded contexts. Naturally bounded contexts include uniform distributions on a
ball or distributions with bounded density defined on the ball.
H.2 Regret Bounds for Bounded Contexts
Now, we present our result for the regret bound. The first case is when we receive the truncated
contexts, generated by truncating the unbounded contexts to (BR)Kwhere BRisd-dimensional ball
with radius R. For the ℓ2-bounded case, Rmay depend on the dimension dwhen we choose a large
R, so we do not hide the Rterm in our main regret bound result. Corollary 1 and 2 gives the regret
bound for truncated contexts. Corollary 3 gives the regret bound for naturally bounded contexts.
Corollary 1 (Regret bound: truncated contexts) LetX(t)be unbounded contexts with the same
condition as Theorem 1. For R′>0withP[X(t)∈(BR′)K] =p >0, we receive truncated contexts
X(t)|(BR′+r)Kfor some r >0each round t≥1. In this case, for R=R′+r, the regret of
Algorithm 1 is bounded by
Reg(T)≤eO(d2.5R2L(R)21
p).
when r≍R′.
38Corollary 2 (Regret bound: high-prob truncated contexts) LetX(t)be unbounded contexts with
the same condition as Theorem 1. We receive truncated contexts X(t)|(BL)KforL≳√
dxmax(1 +
logd+ log K)each round t. In this case, regret of Algorithm 1 is bounded by
Reg(T)≤eO(d2.5).
Discussion. For Corollary 1, when we receive truncated light-tail distributions (e.g. Gaussian,
exponential, Laplace), this theorem states that they enjoy a logarithmic regret bound. It has a
poly(log T)regret bound for Tand it has a R(truncation radius) dependency. Corollary 2 states that
when we choose a sufficiently large truncation radius, it has a radius-free regret bound and it has the
same scale with the unbounded case, Theorem 1. Proofs for the above corollaries are presented in
Appendix I.
Next, we introduce the regret bound results for naturally bounded contexts. Here, new parameters
c⋆andp⋆are involved, along with related new Condition 1. This condition will be discussed in
the following Appendix H.3, where we clarify that both uniform distributions and bounded density
distributions satisfy them. We will provide detailed explanations accordingly and also state the range
ofc⋆, p⋆for several distributions containing uniform distribution.
Corollary 3 (Regret bound: naturally bounded contexts) Let naturally bounded contexts X(t)∈
(BR)Ksatisfy LAC condition with function L(·)and Condition 1 holds with concentration parameters
c⋆, p⋆each round t. Under the Assumption 1 and 2, the regret of Algorithm 1 is bounded by
Reg(T)≤eO(Kd2.5R2L(R)3 1
p⋆(1−c⋆)2).
Discussion. Corollary 3 works for uniform distribution and any distribution with upper bounded
density. We present the related result in Appendix H.3. It also contains truncation of heavy-tail
distributions. For Corollary 3, the polynomial dependency of Kis due to boundedness, and it matches
the asymptotic result of the uniform distribution studied in [ 12] when d= 1. Combining with
Lemma 15, for uniform distribution or bounded density distribution in the ball, we have a regret
bound Reg(T)≤eO(Kd+5
d+1d2.5). Proofs for the above corollaries are presented in Appendix I.
Comparison with known results. For truncated contexts, to the best of our knowledge, there
are no known results for greedy bandits. For naturally bounded contexts, Oh et al. [28] studied the
case of a uniform distribution on the sphere, where each context Xifor arm iis independent across
arms and only considered with the case K≤d. That work considers the minimum eigenvalue of
the context covariance matrix as a constant; however, for a uniform distribution, it scales as ≍1
d.
Taking this into account, their regret bound is eO(d3√
T)when K≤d. When K≤d, our result has
boundeO(d2.5+d+5
d+1)for uniform distribution. Moreover, for the multi-parameter shared context setup,
Bastani and Bayati [7], Bastani et al. [8]studied the uniform context case, and their worst case regret
bound is also eO(K4d4), considering that the minimum eigenvalue of the context covariance scales as
1
d. In conclusion, our result is the sharpest among known previous results.
H.3 Concentration Parameters for Bounded Contexts
We introduce a new condition that defines two concentration parameters, which measure sufficient
concentration for bounded contexts. We will show that truncated contexts satisfy this condition, as do
naturally bounded contexts with bounded density. This condition does not need to be assumed for
truncated contexts, as it is automatically satisfied.
Condition 1 (Concentration parameters for bounded contexts) For the random vectors (con-
texts) X= (X⊤
1. . . X⊤
K)∈RdKwhere Xi∈BR, there exist 0< c⋆<1and0< p ⋆<1
such that for every ηwith∥η∥2= 1,P[max i∈[K]X⊤
iη≤c⋆R]≥p⋆. We call p⋆, c⋆concentration
parameters.
Discussion. Two parameters, c⋆andp⋆, must exist if Xis random. In the next part, we discuss
1−c⋆,p⋆≍1for truncated contexts that truncated into the positive probability region. Additionally,
we will show that the uniform distribution within the ball, as well as any distribution within the ball
with bounded density, also satisfies this condition, and we explicitly calculate the range of these two
parameters. For our regret bound, we have a dependency onp⋆
(1−c⋆)2.
39Example: truncated contexts. The next lemma states that the truncated contexts satisfy Con-
dition 1 has parameters 1−c⋆, p⋆≍1. If contexts are generated by truncating from the original
distribution and the truncated region is a positive probability region, then the parameters in Condi-
tion 1 c⋆, p⋆are well-defined and do not harm the regret bound when we choose a sufficiently large
truncation set.
Lemma 14 (Concentration parameters for truncated contexts) Suppose the unbounded contexts
X∈RdKsatisfy
P[X∈(BR′)K] =p >0,
for some R′>0. Then if we truncate each XitoBR′+rfor any r >0, i∈[K], the truncated
contexts X= (X⊤
1, . . .X⊤
K)satisfy Condition 1 with p⋆=p, c⋆=R′
R′+r.
Proof Recall that we truncate Xto(BR′+r)K. IfP[X∈(BR′)K] =p >0and truncate to BR′+r,
P[max
i∈[K]X⊤η≤R′] =P[{maxX⊤
iη≤R′} ∩ {X∈(BR′+r)K}]
P[X∈(BR′+r)K]
≥P[X∈(BR′)K]
P[X∈(BR′+r)K]
≥P[X∈(BR′)K]
=p.
Therefore, we can set p⋆=pandc⋆=R′
R′+r.
Example: Bounded Density Distributions The below lemma tells us that a uniform distribution
or a bounded density distribution defined in the ball BRsatisfies Condition 1 and presents the range
of two parameters. Without loss of generality, we prove for distribution defined in unit ball, B1.
Lemma 15 Consider a random vector Xi∈B1, i∈[K]with each Xi’s density being upper
bounded withcu
ωdHere, ωdis a volume of d-dimensional unit ball B1, recalling that the density of the
uniform distribution in the ball B1has density1
ωd. When X1=···=XK(strongly correlated), then
Condition 1 holds with
p⋆=1
2,1−c⋆≳1.
When X1, . . . X Kare independent (not correlated), we have
p⋆=1
2,1−c⋆≥cK−2
d+1.
Proof We use the result of the below Claim 10.
Correlated Case. For the case X1=X2=. . . X K, the1
2-quantile of X⊤
iηsayc⋆satisfies
2(1−c⋆)≥(1
2cuωd
ωd−1)2
d+1
by the Claim 10. Since (ωd
ωd−1)2
d+1≍1, we get
1−c⋆≳1.
Independent Case. For the case X⊤
1, . . . X⊤
Kare independent, for c⋆with
2(1−c⋆)≥1−c2
⋆= (1
Kcuωd
ωd−1)2
d+1≍K−2
d+1
satisfies P[X⊤
iη≤c⋆R]≤1−1
Kfor all i∈[K]. Due to independence, we get p⋆= (1−1
K)K≤1
2.
40Claim 10 Consider random vector X∈Rddefined in B1with density upper bounded bycu
ωd. For
anyη∈Sd−1, let1−pquantile of X⊤ηasα. We have
2(1−α)≥1−α2≥(p
cuωd
ωd−1)2
d+1.
Proof
We prove that αwith1−α2= (p
cuωd
ωd−1)2
d+1satisfies
P[X⊤η≥α]≤p.
Since the density of X⊤η=rsatisfies
P[X⊤η=r]≤ωd−1(1−r2)d−1
2cu
ωd
hence
P[X⊤η≥α]≤Z1
αωd−1(1−r2)d−1
2cu1
ωd
≤cuωd−1
ωd(1−α)(1−α2)d−1
2
≤cuωd−1
ωd(1−α2)d+1
2
≤p
H.4 Results for Two Challenges: Bounded Contexts
Now, we present results related to two challenges for bounded contexts. Proof are presented in
Appendix I.
Proposition 4 (Diversity constant: naturally bounded contexts) Suppose X(t)is the random vec-
tor supported in (BR)K, its density satisfies the LAC condition with constant function L(R)and has
Condition 1 with p⋆, c⋆. Then
λ⋆(t)≥cp⋆
d1
(L(R) +1
R(1−c⋆))2
for the absolute constant c >0.
Proposition 5 (Diversity constant: high-prob truncated contexts) Suppose X(t)∈Rd×Ksat-
isfies LAC with L(·)and Assumption 2. Then we truncate X(t)to(BL)Kfor some L≥
c√
dxmax(1 + log(dKx max
λ⋆))and define this truncated contexts as X(t) = (X1(t). . .XK(t)). Then,
it has diversity constant (Challenge 1) with1
2λ⋆(t), where λ⋆(t)is a diversity constant of X(t).
Proposition 6 (Suboptimality gap: truncated contexts) LetX(t)be a truncated random variable
ofX(t)into(BR)KwithP[X∈(BR)K]≥1−δfor some δ >0. LetC∆(t)is a margin constant of
the contexts before truncation. Then margin constant of truncated contexts X(t), sayC∆(t)satisfies
C∆(t)≤1
1−δC∆(t).
Proposition 7 (Suboptimality gap: naturally bounded contexts) For naturally bounded contexts
X(t)with LAC function L(·), the margin constant is bounded as
C∆(t)≤cK√
dL(R)
for some absolute constant c >0.
41Discussion of Proposition 7. It has a Klinear bound, which can worsen the regret bound for the
number of arms Kcompared to linUCB and linTS. We emphasize that this Kdependence occurs for
the logarithmic regret bound, which is still advantageous for other algorithms with O(√
T)regret
bound when T≫K. Also, if every Xi(t)follows a uniform distribution independently, this gap
should have a dependence on Kusing extreme value theory De Haan et al. [12]. So, our results meet
the lower bound for uniform distribution, hence it cannot be improved.
I Proofs of Results for Bounded Contexts
We provide proofs for Appendix H. For simplicity, we assume R≥1. Using observation of
Appendix C.1, any density defined in BRwith LAC L(·)has bounded decay rate√
dL(R)by
combining Lemma 3.
I.1 Fixed History Arguments
We fix again the time step tand history Ht−1and derive the fixed history results for the diversity
and suboptimality gap. We use the same fixed history arguments in the Appendix F. We set history-
conditioned contexts as X:=X(t)| Ht−1andX= (X⊤
1, . . . X⊤
K), Xi∈Rd. To address any
greedy policy with respect to ˆθt−1, we propose an analysis that applies to any greedy policy with
arbitrary θ∈Rd. In Appendix F.1, we already argued that it suffices to bound this variance for
any fixed θ. Since we fix θ, which is the corresponding value of ˆθt−1under the given history Ht−1,
we define the policy-selected arm a= arg max X⊤
iθ. Same as previous definitions, we define the
event Ωi:={a=i}andΩi({xj}j̸=i) :={a=i} ∩ {Xj=xjfor all j̸=i}. Simiarly, define
Ω⋆
i:={a⋆=i}andΩ⋆
i({xj}j̸=i) :={a⋆=i} ∩ {Xj=xjfor all j̸=i}.
Event decomposition and conditional density. We first define b= max i̸=aX⊤
iθ. Similar to the
unbounded contexts, we decompose our diversity as
E[v⊤XaX⊤
av] =P[b≤c⋆R]E[v⊤XaX⊤
av|b≤c⋆R] +P[b > c ⋆R]E[v⊤XaX⊤
av|b > c ⋆R]
≥p∗E[v⊤XaX⊤
av|b≤c⋆R]
≥p∗E
E[v⊤XaX⊤
av| {b≤c⋆R} ∩Ωi({xj}j̸=i)]
Therefore our next interest are projected contexts, defined as
X⊤
iv|Ωi({xj}j̸=i)∩ {b≤c⋆R}.
We first investigate the support of conditonal random vector,
Xi|Ωi({xj}j̸=i)∩ {b≤c⋆R}
whose density is the integration of fwithin the section {x∈BR|x⊤θ≥b}∩{x∈BR|x⊤v=y}.
Hence, the density of X⊤
iv|Ωi({xj})∩ {b≤c⋆R}is a section density of {x∈BR|x⊤θ≥b}
with direction v.
I.2 Sections of the Ball
Next, we aim to bound the one-side decay rate of the section density in the ball. Unlike the previous
sections, the sections corresponding to Ωi({xj}j̸=i)does not makes no longer expanding sections
due to the boundary of BR. To deal with sections for bounded contexts, we define several sections of
the ball.
Definition 14 (Sliced ball) We define slide ball SR(v, y)as
SR(v, y) :={x∈BR|x⊤v=y}.
Also define double sliced ball
SR(θ, b, v, y ) :={x∈BR|x⊤v=y, x⊤θ≥b}.
42<latexit sha1_base64="PeMgCxkv/6YLANh8LNF6+EPP03k=">AAAGSXicjVTLbhMxFHVbUkp5pbBAgo1FhFSkUZWpKkDqplJbiR2l0CZVJoo8jiex4pdsT5toOl/DFr6EL+Az2CFWeJIJ6ThFwlLim3PO9X34xrFi1Nhm88fK6tqd2vrdjXub9x88fPS4vvXk3MhUY3KGJZO6HSNDGBXkzFLLSFtpgnjMSCseHRZ865JoQ6X4bCeKdDkaCJpQjKyDevVnEUd2GMfZp7x3uh3ZIbEoiF/DXr3R3GlOF1w2wtJogHKd9LZqz6O+xCknwmKGjOmETWW7GdKWYkbyzSg1RCE8QgPScaZAnJhuNq0gh68c0oeJ1O4jLJyiNz0yxI2Z8Ngpi3yNzxXgbVwntcm7bkaFSi0ReBYoSRm0EhbtgH2qCbZsAisnCmRjGue3RQncXuRognlKFdU08wAS4RqhkSUBTJhENoADjdSQ4nEAR9ISt3E0Ipgw5qyUWarlVQCp6Lv2JVQb52HSGCNVXJOXCBsoQ1LXbNknS5TU1A55FR66SrUmSRVlVJnUUzpXKyUzudcOVwxJAnePl2TpnHll/klEasILdbW3sRfyX2WOZ6NRwZSW0ouukDYjqhwoyBWWnCPRz6LjPJsP9nHucacL7tTnjo4W5JFPuvF07H7krs9mxQ8Y7UNPUzRoUlFBxNhtyvY8UpK1/UgXC+5iKYsFl/hca8G1fG684MY+xxcc9zkUm7wTdrOIkcReN8JI08HQXnsqkyohNb+hnH3DRghLj3LrZREViZ34ceyV/O8Tdn1nJVnRdamKP53UxcBmU2w5TeXrpthMVw5tJmfzmH2Yz+UNDkvRpzP68K9ZUbgpdc+v6+lJaWy61zT0385l43x3J3yzs/dxr3GwV76rG+AFeAm2QQjeggPwHpyAM4BBDr6Ar+Bb7XvtZ+1X7fdMurpS+jwFlbW+9gc3Vkr7</latexit>SR(✓,b)<latexit sha1_base64="rSLI8BhovbOjcknHJoZpkRZCrsQ=">AAAGTXicjVTLbhMxFHULaUt5tIUd3VhESEUaVZmqAqRuKrWV2FEKbVJlosjjeBIrfsn2tImms+Br2MKXsOZD2CGEJ5mQjlMkLCW+Oedc34dvHCtGjW00fiwt37tfW1lde7D+8NHjJxubW08vjEw1JudYMqlbMTKEUUHOLbWMtJQmiMeMNOPhUcE3r4g2VIpPdqxIh6O+oAnFyDqou7kdcWQHcZx9zLtnO5EdEIuCOLgKxq9gd7Pe2G1MFlw0wtKog3Kddrdqz6OexCknwmKGjGmHDWU7GdKWYkby9Sg1RCE8RH3SdqZAnJhONqkihy8d0oOJ1O4jLJygtz0yxI0Z89gpi5yNzxXgXVw7tcnbTkaFSi0ReBooSRm0EhYtgT2qCbZsDCsnCmRjGud3RQncXuRogllKFdUk8wAS4RqhkSUBTJhENoB9jdSA4lEAh9ISt3E0JJgw5qyUWarldQCp6Ln2JVQb52HSGCNVXJWXCOsrQ1LXbNkjC5TU1A54FR64SrUmSRVlVJnUUzpXKyUzudcOVwxJAnePV2ThnFll/klEasILdbW3sRfyX2WOpqNRwZSW0ouukDZDqhwoyDWWnCPRy6KTPJsN90nucWdz7sznjo/n5LFPuvF07EHkrs9mxQ8YHUBPUzRoXFFBxNhdytYsUpK1/EiXc+5yIYs5l/hcc841fW4050Y+x+cc9zkUm7wddrKIkcTe1MNI0/7A3ngqkyohNb+lnH7DeghLj3LrZhEViR37cey1/O8T9nxnJVnRdamKP53UxcBmE2wxTeXrJthUVw5tJqfzmL2fzeUtDkvRo1P66K9ZUbgpdU+w6+lpaay71zT0385F42JvN3y9u/9hv364X76ra2AbvAA7IARvwCF4B07BOcDgM/gCvoJvte+1n7Vftd9T6fJS6fMMVNbK6h9qd0xq</latexit>SR(✓,b ,v ,y)
<latexit sha1_base64="CEshLfCf6/+AUiAmdWXSb32QrMQ=">AAAGNnicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXYUSrapMpElWdyJzHxS7anTTTNP7CFL+FX2LBDbPkEPJkJ6ThFwlLim3PO9X34xrFi1NhW6/vK6q3bjTt31+6t33/w8NHjjc0np0ZmOoGTRDKpOzExwKiAE0stg47SQHjMoB2P9gu+fQHaUCk+2omCHicDQVOaEOug08gOwZLzjWZrqzVbeNkIK6OJqnV0vtl4FvVlknEQNmHEmG7YUraXE21pwmC6HmUGFElGZABdZwrCwfTyWbpT/NIhfZxK7T7C4hl63SMn3JgJj52SEzs0PleAN3HdzKZvezkVKrMgkjJQmjFsJS5qx32qIbFsgmsnCmJjGk9vihK4vcjRBPOUaqpZ5gEG4RqhiYUAp0wSG+CBJmpIk3GAR9KC2zgZQQKMOStjlmp5GWAq+q59KdXGeZgsTogq7sRLhA2Ugcw1W/ZhiZKa2iGvw0NXqdaQ1lFGlck8pXO1UjIz9drhioE0cPd4AUvnzCvzTwKpgRfqem9jL+S/yhyXo1HDlJbSi66INiOqHCjgMpGcE9HPo8NpHhW3Fcf54dTjjhfcsc8dHCzIA5904+nY3chdn82LHzjaxZ6maNCkpsKEsZuUnXmkNO/4kc4W3NlSFgsu9bn2gmv73HjBjX2OLzjucyQ2027YyyMGqb1qhpGmg6G98lQmU0Jqfk1ZfuNmiCuPajvPIypSO/Hj2Ev53yds+85KsqLrUhV/OqmLgc1n2HKaytfNsFJXDW0uy3nM38/n8hqXSNGnJb3/16wp3JS6t9b19Kgy1t1rGvpv57Jxur0Vvt7a+bDT3Nup3tU19By9QK9QiN6gPfQOHaETlKBP6DP6gr42vjV+NH42fpXS1ZXK5ymqrcbvP6dIRVo=</latexit>ω
<latexit sha1_base64="S3MY0AHnz1mHFgJwKbfKKmJhj1o=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHHlxcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AT8FDLA==</latexit>vFigure 10: Illustrations for various sections in the ball BR. The red line is SR(θ, b, v, y )and the green
line is SR(θ, b).
For bounded contexts, we need to bound the one-side decay rate of section density, where the sections
are sliced balls. We aim to get the lower bound of
E[|v⊤Xi|2|Ωj({xj}j̸=i)∩ {b≤c⋆R}] (13)
for any Ωi({xj}j̸=i), b. Observe that the support of v⊤Xi=y|Ωj({xj}j̸=i)∩ {b≤c⋆R}is the
section of a double sliced ball, SR(θ,max j̸=ix⊤
jθ=b, v, y ). Here we aim to bound the one-side
decay rate of theses section densities to apply Lemma 4.
I.3 Linear Section Maps
To deal with varying sections, we define linear section maps , and using linear section maps we can
bound the one-side decay rate of the section densities. We first define a projection map, a projection
to the center of similarity.
Definition 15 (Projection map) We define the projection map between A⊂RdandP∈Rdas a
mapΦ :A→P, which is an affine point projection with the center of similarity at P. Furthermore,
we call Pas the projection point.
<latexit sha1_base64="ikPtnOm0pQtMYae2pZqbIFMweH4=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG6K2krsaCvaBGWiyjO5k1jxS7anTTSdL2ALX8LXdIfY8hN4kgnpOEXCUuKbc871ffjGsWLU2FbremX1zt3GvftrD9YfPnr85OnG5rMzIzOdwGkimdSdmBhgVMCppZZBR2kgPGbQjkf7Jd++AG2oFJ/tREGPk4GgKU2IddDxh/ONZmurNV142Qgro4mqdXS+2XgR9WWScRA2YcSYbthStpcTbWnCoFiPMgOKJCMygK4zBeFgevk00wK/dkgfp1K7j7B4it70yAk3ZsJjp+TEDo3PleBtXDez6fteToXKLIhkFijNGLYSl2XjPtWQWDbBtRMFsTGNi9uiBG4vczTBPKWaapp5gEG4RmhiIcApk8QGeKCJGtJkHOCRtOA2TkaQAGPOypilWl4GmIq+a19KtXEeJosTosrr8BJhA2Ugc82WfViipKZ2yOvw0FWqNaR1lFFlMk/pXK2UzBReO1wxkAbuHi9g6Zx5Zf5JIDXwUl3vbeyF/FeZ49lo1DClpfSiK6LNiCoHCrhMJOdE9PPosMij8rbiOD8sPO5kwZ343MHBgjzwSTeejt2N3PXZvPyBo13sacoGTWoqTBi7TdmZR0rzjh/py4L7spTFgkt9rr3g2j43XnBjn+MLjvsciU3RDXt5xCC1V80w0nQwtFeeymRKSM1vKGffuBniyqPazvOIitRO/Dj2Uv73Cdu+s5Ks7LpU5Z9O6nJg8ym2nKbydVNspquGNpezecw/zefyBpdI0aczev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8u7VzvNPc267e1TX0Er1Cb1CI3qE99BEdoVOUIEBf0Tf0vfGjcd342fg1k66uVD7PUW01fv8BG6dC9Q==</latexit>A
<latexit sha1_base64="ytcdlUPZy9Q+EbZ82W+Y1qm8EyM=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmagCpG4qtZXYkVa0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwJBPScYqEpcQ355zr+/CNY8WosY3Gzdr6vfu1Bw83Hm0+fvL02fOt7RdnRmY6gdNEMqk7MTHAqIBTSy2DjtJAeMygHY8OCr59CdpQKb7YiYIeJwNBU5oQ66Dj1sVWvbHTmC28aoSlUUflal1s115FfZlkHIRNGDGmGzaU7eVEW5owmG5GmQFFkhEZQNeZgnAwvXyW6RS/dUgfp1K7j7B4ht72yAk3ZsJjp+TEDo3PFeBdXDez6cdeToXKLIhkHijNGLYSF2XjPtWQWDbBlRMFsTGNp3dFCdxe5GiCRUoV1SzzAINwjdDEQoBTJokN8EATNaTJOMAjacFtnIwgAcaclTFLtbwKMBV9176UauM8TBYnRBXX4SXCBspA5pot+7BCSU3tkFfhoatUa0irKKPKZJ7SuVopmZl67XDFQBq4e7yElXMWlfkngdTAC3W1t7EX8l9ljuejUcGUltKLrog2I6ocKOAqkZwT0c+jo2keFbcVx/nR1ONOltyJzx0eLslDn3Tj6di9yF2fzYsfONrDnqZo0KSiwoSxu5SdRaQ07/iRzpfc+UoWSy71ufaSa/vceMmNfY4vOe5zJDbTbtjLIwapva6HkaaDob32VCZTQmp+Szn/xvUQlx7ldpFHVKR24sexV/K/T2j6zkqyoutSFX86qYuBzWfYaprK182wua4c2lzO5zH/vJjLW1wiRZ/O6YO/ZkXhptQ9s66nrdLYdK9p6L+dq8ZZcyd8v7N7vFvfb5bv6gZ6jd6gdyhEH9A++oRa6BQlCNBX9A19r/2o3dR+1n7Npetrpc9LVFm1338AcqpDBA==</latexit>P
Figure 11: Illustration of projection map.
Remark 4 The projection map means that homothety toward some point P.
43Definition 16 (Linear section maps) We define the linear section maps between sections. For two
sections of A,Sec(A, v, y )andSec(A, v, y +h), we define linear section map Φh
yas
Φh
y(·) : Sec( A, v, y )→Sec(A, v, y +h)
which satisfies Φh
y(Sec( A, v, y ))⊂Sec(A, v, y +h)andΦh
yis a part of some projection map with
center P.
<latexit sha1_base64="ENyO9HGAqFNCTtUlnw5+Wrkq1xA=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmSoCpG4qtZXYkVa0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwJBPScYqEpcQ355zr+/CNY8WosY3Gzdr6vfu1Bw83Hm0+fvL02fOt7RdnRmY6gdNEMqk7MTHAqIBTSy2DjtJAeMygHY8OCr59CdpQKb7YiYIeJwNBU5oQ66Dj1sVWvbHTmC28aoSlUUflal1s115FfZlkHIRNGDGmGzaU7eVEW5owmG5GmQFFkhEZQNeZgnAwvXyW6RS/dUgfp1K7j7B4ht72yAk3ZsJjp+TEDo3PFeBdXDez6cdeToXKLIhkHijNGLYSF2XjPtWQWDbBlRMFsTGNp3dFCdxe5GiCRUoV1SzzAINwjdDEQoBTJokN8EATNaTJOMAjacFtnIwgAcaclTFLtbwKMBV9176UauM8TBYnRBXX4SXCBspA5pot+7BCSU3tkFfhoatUa0irKKPKZJ7SuVopmZl67XDFQBq4e7yElXMWlfkngdTAC3W1t7EX8l9ljuejUcGUltKLrog2I6ocKOAqkZwT0c+jo2keFbcVx/nR1ONOltyJzx0eLslDn3Tj6di9yF2fzYsfONrDnqZo0KSiwoSxu5SdRaQ07/iRzpfc+UoWSy71ufaSa/vceMmNfY4vOe5zJDbTbtjLIwapva6HkaaDob32VCZTQmp+Szn/xvUQlx7ldpFHVKR24sexV/K/T9j1nZVkRdelKv50UhcDm8+w1TSVr5thc105tLmcz2P+eTGXt7hEij6d0wd/zYrCTal7Zl1PW6Wx6V7T0H87V42z3Z3w/U7zuFnfb5bv6gZ6jd6gdyhEH9A++oRa6BQlCNBX9A19r/2o3dR+1n7Npetrpc9LVFm1338Ac0RDBg==</latexit>P<latexit sha1_base64="72v0O59ZLRATJbguATx5qaKRcEI=">AAAGS3icjVTNbhMxEHYLgVL+WjgBF4sIqUirKltVgNRLUVuJG6XQplU2irzObGLFa1u2t020XfE0XOFJeACegxvigDc/pOsUCUsbT+b7xjP+PJpYcWZso/FjafnGzdqt2yt3Vu/eu//g4dr6oxMjM03hmEou9WlMDHAm4Ngyy+FUaSBpzKEZD/ZKvHkO2jApPtmRgnZKeoIljBLrXJ21J5FUoImVWpAU8o9Ai423wXkwetlZqzc2G+OFF41watTRdB121mtPo66kWQrCUk6MaYUNZds50ZZRDsVqlBlQhA5ID1rOLBOadj6+Q4FfOE8XJ1K7T1g89l6NyElqzCiNHTMltm98rHReh7Uym7xp50yozIKgk0RJxrGVuBQEd5kGavkIV04UxMYsLq7LEri9rNEEs5IqrHHlAQbhhHDCQoATLokNcE8T1Wd0GOCBtOC2lAyAAufOyrhlWl4EmImuky9h2rgIk8WUqPKhvEJ4TxnInNiyCwuQ1Mz206q7726qNSRVL2fKZB7ThVopuSk8OdxlIAncO57Dwjmzm/kngdSQluyqtrGX8l/XHE5ao+JTWkovuyLaDJhyTgEXVKYpEd08OijyqHytOM4PCg87mmNHPra/Pwf3fdC1p0N3Ivd8Ni//4GgHe5xSoFGFhQnn1zFPZ5mS/NTPdDbHzhaqmGOJjzXnWNPHhnNs6GPpHEt9jMSmaIXtPOKQ2Mt6GGnW69tLj2UyJaROrzAnv7ge4mnEdOvkEROJHfl57IX87xO2/GAleal6ZZqNfYtlKp839k1406bN5aQf8/ezvryCUSm6bALv/TUrDNelbgA7TQ+nxqqbpqE/OxeNk63N8NXm9oft+u72dK6uoGfoOdpAIXqNdtE7dIiOEUWf0Rf0FX2rfa/9rP2q/Z5Ql5emMY9RZd2q/QEcx0wt</latexit>Sec(A, v, y)<latexit sha1_base64="2rMB2t/By1kdey7aq4vJaR+Gjvk=">AAAGTXicjVRNbxMxEHULoaV8tXCjF4sIqYhVla0qQOqlqK3EjVJokyobRV7Hm1jxl2xvm2i7B34NV/glnPkh3BDCm2xI1ykSljaezHvjGT+PJlaMGtto/FhavnW7dmdl9e7avfsPHj5a33h8ZmSqMTnFkkndipEhjApyaqllpKU0QTxmpBkPDwq8eUG0oVJ8smNFOhz1BU0oRta5uuubkVREIyu1QJxkHwnOt94GF8H45eBFd73e2G5MFlw0wtKog3IddzdqT6OexCknwmKGjGmHDWU7GdKWYkbytSg1RCE8RH3SdmaR0nSyyS1y+Nx5ejCR2n3Cwon3ekSGuDFjHjsmR3ZgfKxw3oS1U5u86WRUqNQSgaeJkpRBK2EhCexRTbBlY1g5USAb0zi/KUvg9qJGE8xKqrAmlQeQCCeEk5YEMGES2QD2NVIDikcBHEpL3MbRkGDCmLNSZqmWlwGkoufkS6g2LsKkMUaqeCqvENZXhqRObNkjC5DU1A541T1wN9WaJFUvo8qkHtOFWimZyT053GVIErh3vCAL58xu5p9EpCa8YFe1jb2U/7rmaNoaFZ/SUnrZFdJmSJVzCnKJJedI9LLoKM+i4rXiODvKPexkjp342OHhHDz0QdeeDt2L3PPZrPgDoz3ocQqBxhUWRIzdxGzNMiVZy890PsfOF6qYY4mPNedY08dGc2zkY3yOcR9DscnbYSeLGEnsVT2MNO0P7JXHMqkSUvNrzOkvrIewjCi3bhZRkdixn8deyv8+YccPVpIVqlfm2cS3WKbyeRPflFc2bSan/Zi9n/XlNQxL0aNT+OCvWWG4LnUj2Gl6XBprbpqG/uxcNM52tsNX27sfduv7u+VcXQWb4BnYAiF4DfbBO3AMTgEGn8EX8BV8q32v/az9qv2eUpeXypgnoLLurPwB17tM1A==</latexit>Sec(A, v, y+h)
<latexit sha1_base64="S3MY0AHnz1mHFgJwKbfKKmJhj1o=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHHlxcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AT8FDLA==</latexit>v<latexit sha1_base64="LVbIEHnJhzxxltzIMYIZygjSvzg=">AAAGOHicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXYESrapMqEyON4Mlb8ku1pE03zE2zhS/gTduwQW74ATzJhOk6RsJT45pxzfR++cawYNbbV+r62fut2487djXub9x88fPR4a/vJmZGZxuQUSyZ1N0aGMCrIqaWWka7SBPGYkU48Piz4zgXRhkrx0U4V6XM0EjShGFkHdaN2SgfTT+lgq9naac0XXDXC0miCcrUH241n0VDijBNhMUPG9MKWsv0caUsxI7PNKDNEITxGI9JzpkCcmH4+T3gGXzpkCBOp3UdYOEeve+SIGzPlsVNyZFPjcwV4E9fLbPK2n1OhMksEXgRKMgathEX1cEg1wZZNYe1EgWxM49lNUQK3FzmaYJlSTTXPPIBEuEZoZEkAEyaRDeBII5VSPAngWFriNo7GBBPGnJUxS7W8DCAVQ9e+hGrjPEwWY6SKW/ESYSNlSOaaLYdkhZKa2pTX4dRVqjVJ6iijymSe0rlaKZmZee1wxZAkcPd4QVbOWVbmn0SkJrxQ13sbeyH/VeZkMRo1TGkpvegKaTOmyoGCXGLJORLDPDqe5VFxW3GcH8887qTiTnzu6Kgij3zSjadj9yN3fTYvfsBoH3qaokHTmgoixm5SdpeRkrzrRzqvuPOVLCou8blOxXV8blJxE5/jFcd9DsVm1gv7ecRIYq+aYaTpKLVXnspkSkjNrykX37AZwtKj3AZ5REVip34ceyn/+4Rd31lJVnRdquJPJ3UxsPkcW01T+bo5ttCVQ5vLxTzm75dzeY3DUgzpgj78a9YUbkrda+t62i6NTfeahv7buWqc7e6Er3f2Puw1D/bKd3UDPAcvwCsQgjfgALwDbXAKMGDgM/gCvja+NX40fjZ+LaTra6XPU1Bbjd9/ANK0Rhc=</latexit> hy<latexit sha1_base64="y3a65FhMT85Q1aQrMX0KCoi+Q+k=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4q2krd0Va0SZWJKs/kTmLFL9meJtE0X8AWvoSv6Q6x5SfwJBPScYqEpcQ355zr+/CNY8WosY3Gzcrqvfu1Bw/XHq0/fvL02fONzRfnRmY6gbNEMqlbMTHAqIAzSy2DltJAeMygGQ/2C755BdpQKb7YsYIOJz1BU5oQ66CTo8uNemOrMV142QhLo47KdXy5WXsVdWWScRA2YcSYdthQtpMTbWnCYLIeZQYUSQakB21nCsLBdPJpphP81iFdnErtPsLiKXrbIyfcmDGPnZIT2zc+V4B3ce3Mph87ORUqsyCSWaA0Y9hKXJSNu1RDYtkYV04UxMY0ntwVJXB7kaMJ5ilVVNPMAwzCNUITCwFOmSQ2wD1NVJ8mowAPpAW3cTKABBhzVsYs1XIYYCq6rn0p1cZ5mCxOiCquw0uE9ZSBzDVbdmGJkpraPq/CfVep1pBWUUaVyTylc7VSMjPx2uGKgTRw93gFS+fMK/NPAqmBF+pqb2Mv5L/KHM1Go4IpLaUXXRFtBlQ5UMAwkZwT0c2jw0keFbcVx/nhxONOF9ypzx0cLMgDn3Tj6djdyF2fzYsfONrFnqZo0LiiwoSxu5SteaQ0b/mRLhbcxVIWCy71ueaCa/rcaMGNfI4vOO5zJDaTdtjJIwapva6Hkaa9vr32VCZTQmp+Szn7xvUQlx7ldplHVKR27MexQ/nfJ2z7zkqyoutSFX86qYuBzafYcprK102xma4c2lzO5jH/PJ/LW1wiRZfO6P2/ZkXhptQ9s66nx6Wx7l7T0H87l43z7a3w/dbOyU5971P5rq6h1+gNeodC9AHtoSN0jM5QggB9Rd/Q99qP2k3tZ+3XTLq6Uvq8RJVV+/0HSRJDDA==</latexit>H
Figure 12: Illustration of linear section maps. Projection map of Sec(A, v, y )toPinduce linear
section map Sec(A, v, y )toSec(A, v, y +h). Also we use Has the length between Sec(A, v, y )and
P.
I.4 One-side Decay Rate of Linear Section Maps
Our subgoal is to bound the one-side decay rate of the section density, and we use linear section
maps to bound them. Previously, we deal with expanding or equal sections, hence section maps
directly gives the lower bound of density. However, for general linear section maps, they are no
longer expanding sections due to boundaries. Assume the density fis defined in BRand it has a
decay rate M. Iffhas LAC with function L(·), we already studied in Lemma 3 that it has decay rate
withM=√
dL(R). We can set L(R)≥10.
Slope of linear section maps. We first bound the maximum length between Φh
y(x)andx. It is
related to the slope of section maps, and we define s >0such that
∥Φh
y(x)−x∥2≤sh
for any x∈Sec(A, v, y ). We call sas a slope of linear section map Φh
y(·).
Volume element. We define |det(∇Φh
y(x))|:=uy(h). This value is the same for any x∈
Sec(A, v, y ), since we use the linear section maps. Using that, we can calculate the lower bound of
g(y+h)as the following:
g(y+h) =Z
Sec(A,v,y +h)f(x)dx
≥Z
Sec(A,v,y )f(Φh
y(x))|det(∇Φh
y(x))|dx
≥Z
Sec(A,v,y )f(x) exp(−Msh)uy(h)dx
= exp( −Msh)uy(h)g(y).
44The third inequality is by using the Gronwall inequality, Lemma 22, using the decay rate is bounded
byMand the length between xandΦh
y(x)is bounded by sh. This formula tell us that to bound the
one-side decay rate of section density, there are two quantities: slope sand volume element uy(h).
A way to bound uy(h).Lets define Has the intersection of two section Sec(A, v, y )andP. The
volume element can be calculated as
uy(h) = (H−h
H)d
holds sinceH−h
His the ratio of similarity. Then,
g(y+h)
g(y)≥exp(−Msh−(−dlog(H−h
H)))
When h≤H
2, we have log(H−h
H)≥1−2h
H, we have
g(y+h)
g(y)≥exp(−Msh−2d
Hh)
This implies that when we can find the linear section map Φh
y, we can bound the one-side decay rate
of the section density by Ms+2d
H.
I.5 Linear Section Maps between Sliced Balls, SR(v, y)
<latexit sha1_base64="ENyO9HGAqFNCTtUlnw5+Wrkq1xA=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmSoCpG4qtZXYkVa0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwJBPScYqEpcQ355zr+/CNY8WosY3Gzdr6vfu1Bw83Hm0+fvL02fOt7RdnRmY6gdNEMqk7MTHAqIBTSy2DjtJAeMygHY8OCr59CdpQKb7YiYIeJwNBU5oQ66Dj1sVWvbHTmC28aoSlUUflal1s115FfZlkHIRNGDGmGzaU7eVEW5owmG5GmQFFkhEZQNeZgnAwvXyW6RS/dUgfp1K7j7B4ht72yAk3ZsJjp+TEDo3PFeBdXDez6cdeToXKLIhkHijNGLYSF2XjPtWQWDbBlRMFsTGNp3dFCdxe5GiCRUoV1SzzAINwjdDEQoBTJokN8EATNaTJOMAjacFtnIwgAcaclTFLtbwKMBV9176UauM8TBYnRBXX4SXCBspA5pot+7BCSU3tkFfhoatUa0irKKPKZJ7SuVopmZl67XDFQBq4e7yElXMWlfkngdTAC3W1t7EX8l9ljuejUcGUltKLrog2I6ocKOAqkZwT0c+jo2keFbcVx/nR1ONOltyJzx0eLslDn3Tj6di9yF2fzYsfONrDnqZo0KSiwoSxu5SdRaQ07/iRzpfc+UoWSy71ufaSa/vceMmNfY4vOe5zJDbTbtjLIwapva6HkaaDob32VCZTQmp+Szn/xvUQlx7ldpFHVKR24sexV/K/T9j1nZVkRdelKv50UhcDm8+w1TSVr5thc105tLmcz2P+eTGXt7hEij6d0wd/zYrCTal7Zl1PW6Wx6V7T0H87V42z3Z3w/U7zuFnfb5bv6gZ6jd6gdyhEH9A++oRa6BQlCNBX9A19r/2o3dR+1n7Npetrpc9LVFm1338Ac0RDBg==</latexit>P<latexit sha1_base64="fO+7KXz/sJzf71fSk9ZP7AVOXFM=">AAAGSXicjVTLbhMxFHVbUkp5tbBAgo1FhFSkUZSpKkDqplJbiR2l0CZVJoo8jiex4pdsT5toOl/DFr6EL+Az2CFWeJIJ6ThFwlLim3PO9X34xrFi1Nhm88fK6tqd2vrdjXub9x88fPR4a/vJuZGpxuQMSyZ1O0aGMCrImaWWkbbSBPGYkVY8Oiz41iXRhkrx2U4U6XI0EDShGFkH9baeRRzZYRxnn/Le6U5kh8SiAE5e97bqzUZzuuCyEZZGHZTrpLddex71JU45ERYzZEwnbCrbzZC2FDOSb0apIQrhERqQjjMF4sR0s2kFOXzlkD5MpHYfYeEUvemRIW7MhMdOWeRrfK4Ab+M6qU3edTMqVGqJwLNAScqglbBoB+xTTbBlE1g5USAb0zi/LUrg9iJHE8xTqqimmQeQCNcIjSwJYMIksgEcaKSGFI8DOJKWuI2jEcGEMWelzFItrwJIRd+1L6HaOA+Txhip4pq8RNhAGZK6Zss+WaKkpnbIq/DQVao1Saooo8qkntK5WimZyb12uGJIErh7vCRL58wr808iUhNeqKu9jb2Q/ypzPBuNCqa0lF50hbQZUeVAQa6w5ByJfhYd59l8sI9zjztdcKc+d3S0II980o2nY/cjd302K37AaB96mqJBk4oKIsZuU7bnkZKs7Ue6WHAXS1ksuMTnWguu5XPjBTf2Ob7guM+h2OSdsJtFjCT2uh5Gmg6G9tpTmVQJqfkN5ewb1kNYepRbL4uoSOzEj2Ov5H+fsOs7K8mKrktV/OmkLgY2m2LLaSpfN8VmunJoMzmbx+zDfC5vcFiKPp3Rh3/NisJNqXt+XU9PSmPTvaah/3YuG+e7jfBNY+/jXv2gUb6rG+AFeAl2QAjeggPwHpyAM4BBDr6Ar+Bb7XvtZ+1X7fdMurpS+jwFlbW+9ge6v0sM</latexit>SR(✓,y)<latexit sha1_base64="rPGIf1lMfJTPTC0vUiYoYJHLKco=">AAAGSnicjVTLbhMxFHVLU0p5tbBBsLGIkIoYVZmqAqRuKrWV2FEKbVJlosjjeBIrfsn2tImmw9ewhS/hB/gNdogNnmRCOk6RsJT45pxzfR++cawYNbbR+LG0fGultnp77c763Xv3Hzzc2Hx0ZmSqMTnFkkndipEhjApyaqllpKU0QTxmpBkPDwq+eUG0oVJ8smNFOhz1BU0oRtZB3Y0nEUd2EMfZx7x7shXZAbEoGL8avOxu1BvbjcmCi0ZYGnVQruPuZu1p1JM45URYzJAx7bChbCdD2lLMSL4epYYohIeoT9rOFIgT08kmJeTwhUN6MJHafYSFE/S6R4a4MWMeO2WRsPG5AryJa6c2edvJqFCpJQJPAyUpg1bCoh+wRzXBlo1h5USBbEzj/KYogduLHE0wS6mimmQeQCJcIzSyJIAJk8gGsK+RGlA8CuBQWuI2joYEE8aclTJLtbwMIBU9176EauM8TBpjpIp78hJhfWVI6pote2SBkpraAa/CA1ep1iSpoowqk3pK52qlZCb32uGKIUng7vGCLJwzq8w/iUhNeKGu9jb2Qv6rzNF0NCqY0lJ60RXSZkiVAwW5xJJzJHpZdJRns8k+yj3uZM6d+Nzh4Zw89Ek3no7di9z12az4AaM96GmKBo0rKogYu0nZmkVKspYf6XzOnS9kMecSn2vOuabPjebcyOf4nOM+h2KTt8NOFjGS2Kt6GGnaH9grT2VSJaTm15TTb1gPYelRbt0soiKxYz+OvZT/fcKO76wkK7ouVfGnk7oY2GyCLaapfN0Em+rKoc3kdB6z97O5vMZhKXp0Sh/8NSsKN6Xu/XU9PS6Ndfeahv7buWic7WyHr7d3P+zW93fLd3UNPAPPwRYIwRuwD96BY3AKMPgMvoCv4Fvte+1n7Vft91S6vFT6PAaVtbryB4pFS48=</latexit>SR(ω,y+h)
<latexit sha1_base64="JkPAMd7ifD4qIRlTRsW1XutTiCY=">AAAGMXicjVTLbhMxFHWBQCmvFnawsYiQWIyqTFUBUjeV2krd0Va0SZWJKo9zJ7HisS3b0yaa5gvYwpfwNd0htvwEnsyEdJwiYSnxzTnn+j5841hxZmyrdbNy7/6DxsNHq4/Xnjx99vzF+sbLMyMzTeGUSi51JyYGOBNwapnl0FEaSBpzaMejvYJvX4I2TIovdqKgl5KBYAmjxDro+PBivdnabM0WXjbCymiiah1dbDReR31JsxSEpZwY0w1byvZyoi2jHKZrUWZAEToiA+g6U5AUTC+fZTrF7xzSx4nU7iMsnqG3PXKSGjNJY6dMiR0anyvAu7huZpNPvZwJlVkQtAyUZBxbiYuycZ9poJZPcO1EQWzM4uldUQK3FzmaYJ5STTXLPMAgXCM0sRDghEtiAzzQRA0ZHQd4JC24LSUjoMC5szJumZZXAWai79qXMG2ch8liSlRxHV4ifKAMZK7Zsg9LlNTMDtM6PHSVag1JHeVMmcxTOlcrJTdTrx2uGEgCd4+XsHTOvDL/JJAa0kJd723shfxXmeNyNGqY0lJ60RXRZsSUAwVcUZmmRPTz6GCaR8VtxXF+MPW4kwV34nP7+wty3yfdeDp2J3LXZ/PiB452sKcpGjSpqTDh/C5lZx4pyTt+pPMFd76UxYJLfK694No+N15wY59LF1zqcyQ2027YyyMOib1uhpFmg6G99lQmU0Lq9Jay/MbNEFce1XaRR0wkduLHsVfyv0/Y8p2V5EXXpSr+dFIXA5vPsOU0la+bYaWuGtpclvOYf57P5S2OStFnJb3316wp3JS6Z9b19Kgy1txrGvpv57JxtrUZftjcPt5u7m5X7+oqeoPeovcoRB/RLjpER+gUUQToK/qGvjd+NG4aPxu/Sum9lcrnFaqtxu8/RNxC/g==</latexit>H
Figure 13: Illustration of the linear section map between two sections of the ball. The linear section
map is constructed by the center P.
Lemma 16 (One-side decay rate of SR(v, y))The projected density g(y)of sections SR(v, y)sat-
isfies
g(y′)
g(y)≥exp(−3√
dL(R)(y′−y))
for any −1√
d< y < y′<1√
d.
Proof SetPas the similarity center of SR(v, y)andSR(v, y+h). We aim to apply arguments of
Appendix I.4. Using this P, we can make section maps for Φh
y. First, slope of Φh
yis bounded by
s≤R√
R2−(y+h)2≤2for|y| ≤1√
d. The volume element of Φh
ycan be interpreted as the ratio of
45similarity. Let Hbe the distance of PandSR(v, y). Note that H≥R2−(y+h)2
y+hand hence H≥2h
holds. By applying result in Appendix I.4, it’s one-side decay rate in [−1√
d,1√
d]is bounded by
2√
dL(R) +2d
H≤2√
dL(R) +2d(y+h)
R2−(y+h)2
≤3√
dL(R).
holds for all −1√
d≤y < y +h≤1√
dsinceL(R)≥10.
I.6 Linear Section Maps for Double Slicde balls, SR(θ, b, v, y )
Goal. We investigate the one-sided decay rate of section densities with sections SR(θ, b, v, y )for
fixed θ, v, b . For density f(x)defined in BRwith LAC function L(·), we define section density of
SR(θ, b, v, y )asg(y). We set M= 4√
dL(R) +16√
d
R2(1−c⋆). We prove that the one-side decay rate at
y∈[−1
M,1
M]is bounded by M. Also we define some small angle τ0:= sin−1(1
MR).
I.6.1 Case v⊥θ.
For this case, the sections SR(θ, b, v, y )are no longer expanding section when ⟨θ, v⟩is close toπ
2. In
that case, we can construct a linear section map, and the following figure illustrates the procedure.
In each section SR(θ, b, v, y )andSR(θ, b, v, y +h), assume the highest coordinate with respect to
direction θasQandQh. Assume the directed half- line starting from connecting Qand connecting
Qhand the hyperplane {x|x⊤θ=b}meets at some point P. Then we can set the intersection point
Pas the center of the projection map and we can construct linear section map using P. Please see
the figure 14 for the intuitions.
<latexit sha1_base64="CEshLfCf6/+AUiAmdWXSb32QrMQ=">AAAGNnicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXYUSrapMpElWdyJzHxS7anTTTNP7CFL+FX2LBDbPkEPJkJ6ThFwlLim3PO9X34xrFi1NhW6/vK6q3bjTt31+6t33/w8NHjjc0np0ZmOoGTRDKpOzExwKiAE0stg47SQHjMoB2P9gu+fQHaUCk+2omCHicDQVOaEOug08gOwZLzjWZrqzVbeNkIK6OJqnV0vtl4FvVlknEQNmHEmG7YUraXE21pwmC6HmUGFElGZABdZwrCwfTyWbpT/NIhfZxK7T7C4hl63SMn3JgJj52SEzs0PleAN3HdzKZvezkVKrMgkjJQmjFsJS5qx32qIbFsgmsnCmJjGk9vihK4vcjRBPOUaqpZ5gEG4RqhiYUAp0wSG+CBJmpIk3GAR9KC2zgZQQKMOStjlmp5GWAq+q59KdXGeZgsTogq7sRLhA2Ugcw1W/ZhiZKa2iGvw0NXqdaQ1lFGlck8pXO1UjIz9drhioE0cPd4AUvnzCvzTwKpgRfqem9jL+S/yhyXo1HDlJbSi66INiOqHCjgMpGcE9HPo8NpHhW3Fcf54dTjjhfcsc8dHCzIA5904+nY3chdn82LHzjaxZ6maNCkpsKEsZuUnXmkNO/4kc4W3NlSFgsu9bn2gmv73HjBjX2OLzjucyQ2027YyyMGqb1qhpGmg6G98lQmU0Jqfk1ZfuNmiCuPajvPIypSO/Hj2Ev53yds+85KsqLrUhV/OqmLgc1n2HKaytfNsFJXDW0uy3nM38/n8hqXSNGnJb3/16wp3JS6t9b19Kgy1t1rGvpv57Jxur0Vvt7a+bDT3Nup3tU19By9QK9QiN6gPfQOHaETlKBP6DP6gr42vjV+NH42fpXS1ZXK5ymqrcbvP6dIRVo=</latexit>✓
<latexit sha1_base64="S3MY0AHnz1mHFgJwKbfKKmJhj1o=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHHlxcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AT8FDLA==</latexit>v<latexit sha1_base64="M3EAGo6G6mKQkuyzAbMYHOY7ZrE=">AAAGTnicjVTLbhMxFHULKaW8UtgBC4sIqUijKlNVgNRNpbYSO0qhTapMFHkmnsSKX7I9bSJ3NnwNW/gStvwIOwSeZEI6TpGwlPjmnHN9H75xLCnRptn8sbJ663Zt7c763Y179x88fFTffHymRaYSfJoIKlQ7RhpTwvGpIYbitlQYsZjiVjw6KPjWBVaaCP7JTCTuMjTgJCUJMg7q1Z9HDJlhHNuPec+e5FuRGWKDgji4CCavevVGc7s5XXDZCEujAcp13NusPY36IskY5iahSOtO2JSma5EyJKE434gyjSVKRmiAO87kiGHdtdMycvjSIX2YCuU+3MApet3DIqb1hMVOWSStfa4Ab+I6mUnfdi3hMjOYJ7NAaUahEbDoCewThRNDJ7ByIkcmJnF+U5TA7UWOOpinVFFNMw8g5q4RChkcwJQKZAI4UEgOSTIO4EgY7DaGRjjBlDoro4YocRlAwvuufSlR2nnoLE6QLO7KS4QOpMaZa7bo4yVKKGKGrAoPXaVK4bSKUiJ15imdqxGC6txrhysGp4G7xwu8dM68Mv8kLBRmhbra29gL+a8yx7PRqGBSCeFFl0jpEZEO5PgyEYwh3rfRUW7n032Ue9zJgjvxucPDBXnok248HbsXuesztvgBoz3oaYoGTSoqiCi9SdmeR0pt2490vuDOl7JYcKnPtRZcy+fGC27sc2zBMZ9Dsc47YddGFKfmqhFGigyG5spT6Uxyodg15ewbNkJYepRbz0aEp2bixzGX4r9P2PGdpaBF14Us/nRCFQNrp9hymtLXTbGZrhxaK2bzaN/P5/IalwjeJzP64K9ZUbgpdW+w6+lxaWy41zT0385l42xnO3y9vftht7G/W76r6+AZeAG2QAjegH3wDhyDU5CAz+AL+Aq+1b7XftZ+1X7PpKsrpc8TUFlr638Ah5xNTA==</latexit>SR(✓,b ,v ,y)
<latexit sha1_base64="fQ6W2MLwSwHr+xfE94tNpwUMJuM=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhFmaoCpG4qtZXYkVa0SZWJKo9zJ7HisS3b0yaa5gvYwpfwNd0htvwEnmRCOk6RsJT45pxzfR++caw4M7bZvFlbv3e/9uDhxqPNx0+ePnu+tf3izMhMUzilkkvdiYkBzgScWmY5dJQGksYc2vHooODbl6ANk+KLnSjopWQgWMIosQ46bl1s1ZuN5mzhVSMsjToqV+tiu/Yq6kuapSAs5cSYbthUtpcTbRnlMN2MMgOK0BEZQNeZgqRgevks0yl+65A+TqR2H2HxDL3tkZPUmEkaO2VK7ND4XAHexXUzm3zs5UyozIKg80BJxrGVuCgb95kGavkEV04UxMYsnt4VJXB7kaMJFilVVLPMAwzCNUITCwFOuCQ2wANN1JDRcYBH0oLbUjICCpw7K+OWaXkVYCb6rn0J08Z5mCymRBXX4SXCB8pA5pot+7BCSc3sMK3CQ1ep1pBUUc6UyTylc7VScjP12uGKgSRw93gJK+csKvNPAqkhLdTV3sZeyH+VOZ6PRgVTWkovuiLajJhyoIArKtOUiH4eHU3zqLitOM6Pph53suROfO7wcEke+qQbT8fuRe76bF78wNEe9jRFgyYVFSac36XsLCIlecePdL7kzleyWHKJz7WXXNvnxktu7HPpkkt9jsRm2g17ecQhsdf1MNJsMLTXnspkSkid3lLOv3E9xKVHuV3kEROJnfhx7JX87xN2fGcledF1qYo/ndTFwOYzbDVN5etm2FxXDm0u5/OYf17M5S2OStFnc/rgr1lRuCl1z6zraas0Nt1rGvpv56pxttMI3zd2j3fr+43yXd1Ar9Eb9A6F6APaR59QC50iigB9Rd/Q99qP2k3tZ+3XXLq+Vvq8RJVV+/0HcXZDAA==</latexit>P
<latexit sha1_base64="y3a65FhMT85Q1aQrMX0KCoi+Q+k=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4q2krd0Va0SZWJKs/kTmLFL9meJtE0X8AWvoSv6Q6x5SfwJBPScYqEpcQ355zr+/CNY8WosY3Gzcrqvfu1Bw/XHq0/fvL02fONzRfnRmY6gbNEMqlbMTHAqIAzSy2DltJAeMygGQ/2C755BdpQKb7YsYIOJz1BU5oQ66CTo8uNemOrMV142QhLo47KdXy5WXsVdWWScRA2YcSYdthQtpMTbWnCYLIeZQYUSQakB21nCsLBdPJpphP81iFdnErtPsLiKXrbIyfcmDGPnZIT2zc+V4B3ce3Mph87ORUqsyCSWaA0Y9hKXJSNu1RDYtkYV04UxMY0ntwVJXB7kaMJ5ilVVNPMAwzCNUITCwFOmSQ2wD1NVJ8mowAPpAW3cTKABBhzVsYs1XIYYCq6rn0p1cZ5mCxOiCquw0uE9ZSBzDVbdmGJkpraPq/CfVep1pBWUUaVyTylc7VSMjPx2uGKgTRw93gFS+fMK/NPAqmBF+pqb2Mv5L/KHM1Go4IpLaUXXRFtBlQ5UMAwkZwT0c2jw0keFbcVx/nhxONOF9ypzx0cLMgDn3Tj6djdyF2fzYsfONrFnqZo0LiiwoSxu5SteaQ0b/mRLhbcxVIWCy71ueaCa/rcaMGNfI4vOO5zJDaTdtjJIwapva6Hkaa9vr32VCZTQmp+Szn7xvUQlx7ldplHVKR27MexQ/nfJ2z7zkqyoutSFX86qYuBzafYcprK102xma4c2lzO5jH/PJ/LW1wiRZfO6P2/ZkXhptQ9s66nx6Wx7l7T0H87l43z7a3w/dbOyU5971P5rq6h1+gNeodC9AHtoSN0jM5QggB9Rd/Q99qP2k3tZ+3XTLq6Uvq8RJVV+/0HSRJDDA==</latexit>H
<latexit sha1_base64="bI9+YFpUWBSFPUGDLynobTyqzQo=">AAAGMnicjVTLbhMxFHWBQCmvFnawsYgQLEZVpqoAqZtKbaXuKBVtUmWiyuN4Eit+yfa0iabzB2zhS/gZ2CG2fASezITpOEXCUuKbc871ffjGsWLU2E7n+8qt23dad++t3l978PDR4yfrG09PjUw1JidYMql7MTKEUUFOLLWM9JQmiMeMdOPJXsF3L4g2VIpPdqbIgKORoAnFyDro+PD1+Xq7s9mZL7hshJXRBtU6Ot9oPY+GEqecCIsZMqYfdpQdZEhbihnJ16LUEIXwBI1I35kCcWIG2TzVHL5yyBAmUruPsHCOXvfIEDdmxmOn5MiOjc8V4E1cP7XJ+0FGhUotEbgMlKQMWgmLuuGQaoItm8HGiQLZmMb5TVECtxc5mmCRUkM1zzyARLhGaGRJABMmkQ3gSCM1pngawIm0xG0cTQgmjDkrZZZqeRlAKoaufQnVxnmYNMZIFffhJcJGypDUNVsOyRIlNbVj3oTHrlKtSdJEGVUm9ZTO1UrJTO61wxVDksDd4wVZOmdRmX8SkZrwQt3sbeyF/FeZ03I0GpjSUnrRFdJmQpUDBbnEknMkhll0kGdRcVtxnB3kHndcc8c+t79fk/s+6cbTsTuRuz6bFT9gtAM9TdGgWUMFEWM3KXuLSEnW8yOd1dzZUhY1l/hct+a6PjetuanP8ZrjPodik/fDQRYxktirdhhpOhrbK09lUiWk5teU5Tdsh7DyqLbzLKIisTM/jr2U/33Clu+sJCu6LlXxp5O6GNhsji2nqXzdHCt11dBmspzH7MNiLq9xWIohLem9v2ZD4abUvbOup0eVseZe09B/O5eN063N8O3m9sft9u529a6ughfgJXgDQvAO7IJDcAROAAYJ+Ay+gK+tb60frZ+tX6X01krl8ww0Vuv3H1CbQy8=</latexit>H→<latexit sha1_base64="1MXdkDenjdstwrCirlKxpu7CZ0c=">AAAGTnicjVTLbhMxFHULKaU82sIOWFhESEWMqkxVAVI3ldpK7CiFNqkyUeRxPIkVv2R72kTT2fA1bOFL2PIj7BB4kgnpOEXCUuKbc871ffjGsWLU2Ebjx9Lyrdu1lTurd9fu3X/wcH1j89GZkanG5BRLJnUrRoYwKsippZaRltIE8ZiRZjw8KPjmBdGGSvHJjhXpcNQXNKEYWQd1N55FHNlBHGcf8+7JVmQHxKIgDi6C8avBy+5GvbHdmCy4aISlUQflOu5u1p5EPYlTToTFDBnTDhvKdjKkLcWM5GtRaohCeIj6pO1MgTgxnWxSRg5fOKQHE6ndR1g4Qa97ZIgbM+axUxZJG58rwJu4dmqTt52MCpVaIvA0UJIyaCUsegJ7VBNs2RhWThTIxjTOb4oSuL3I0QSzlCqqSeYBJMI1QiNLApgwiWwA+xqpAcWjAA6lJW7jaEgwYcxZKbNUy8sAUtFz7UuoNs7DpDFGqrgrLxHWV4akrtmyRxYoqakd8Co8cJVqTZIqyqgyqad0rlZKZnKvHa4YkgTuHi/IwjmzyvyTiNSEF+pqb2Mv5L/KHE1Ho4IpLaUXXSFthlQ5UJBLLDlHopdFR3k2m+6j3ONO5tyJzx0ezslDn3Tj6di9yF2fzYofMNqDnqZo0Liigoixm5StWaQka/mRzufc+UIWcy7xueaca/rcaM6NfI7POe5zKDZ5O+xkESOJvaqHkab9gb3yVCZVQmp+TTn9hvUQlh7l1s0iKhI79uPYS/nfJ+z4zkqyoutSFX86qYuBzSbYYprK102wqa4c2kxO5zF7P5vLaxyWoken9MFfs6JwU+reYNfT49JYc69p6L+di8bZznb4env3w259f7d8V1fBU/AcbIEQvAH74B04BqcAg8/gC/gKvtW+137WftV+T6XLS6XPY1BZK6t/ADkFTOc=</latexit>SR(ω,b ,v ,y+h)
<latexit sha1_base64="iukJQpsqc+9bro7UhbXtwVt/KqY=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtF0voAtfAlf0x1iy0/gSSak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tl3z7ErShUnyxEwU9TgaCpjQh1kHH8cVGs7XVmi68bISV0UTVOrrYbLyK+jLJOAibMGJMN2wp28uJtjRhUKxHmQFFkhEZQNeZgnAwvXyaaYHfOqSPU6ndR1g8RW975IQbM+GxU3Jih8bnSvAurpvZ9GMvp0JlFkQyC5RmDFuJy7Jxn2pILJvg2omC2JjGxV1RAreXOZpgnlJNNc08wCBcIzSxEOCUSWIDPNBEDWkyDvBIWnAbJyNIgDFnZcxSLa8CTEXftS+l2jgPk8UJUeV1eImwgTKQuWbLPixRUlM75HV46CrVGtI6yqgymad0rlZKZgqvHa4YSAN3j5ewdM68Mv8kkBp4qa73NvZC/qvM8Ww0apjSUnrRFdFmRJUDBVwlknMi+nl0WORReVtxnB8WHney4E587uBgQR74pBtPx+5G7vpsXv7A0S72NGWDJjUVJozdpezMI6V5x490vuDOl7JYcKnPtRdc2+fGC27sc3zBcZ8jsSm6YS+PGKT2uhlGmg6G9tpTmUwJqfkt5ewbN0NceVTbRR5RkdqJH8deyf8+Ydt3VpKVXZeq/NNJXQ5sPsWW01S+borNdNXQ5nI2j/nn+Vze4hIp+nRG7/81awo3pe6ZdT09qox195qG/tu5bJxtb4Xvt3aOd5p7O9W7uoZeozfoHQrRB7SHPqEjdIoSBOgr+oa+N340bho/G79m0tWVyuclqq3G7z/brkMY</latexit>b<latexit sha1_base64="BCEljZhOGma7yPVpJLpN2nH2kjI=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHHxxcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AeRFDBw==</latexit>Q<latexit sha1_base64="nkbHUqd5iV9A6veLqu8Cum1/F6E=">AAAGM3icjVTLbhMxFHULgVIebWEHG4sIicWoylQVIHVTqa3EjrbQJlUmijweT2LFL9meNtE0n8AWvoSPQewQW/4BT2ZCOk6RsJT45pxzfR++cawYNbbV+r6yeudu4979tQfrDx89frKxufX03MhMY3KGJZO6EyNDGBXkzFLLSEdpgnjMSDseHRR8+5JoQ6X4ZCeK9DgaCJpSjKyDPp70h/3NZmu7NVtw2QgrowmqddzfajyPEokzToTFDBnTDVvK9nKkLcWMTNejzBCF8AgNSNeZAnFievks1yl85ZAEplK7j7Bwht70yBE3ZsJjp+TIDo3PFeBtXDez6bteToXKLBG4DJRmDFoJi8JhQjXBlk1g7USBbEzj6W1RArcXOZpgnlJNNcs8gES4RmhkSQBTJpEN4EAjNaR4HMCRtMRtHI0IJow5K2OWankVQCoS176UauM8TBZjpIoL8RJhA2VI5potE7JESU3tkNfhoatUa5LWUUaVyTylc7VSMjP12uGKIWng7vGSLJ0zr8w/iUhNeKGu9zb2Qv6rzHE5GjVMaSm96AppM6LKgYJcYck5EkkeHU3zqLitOM6Pph53uuBOfe7wcEEe+qQbT8fuRe76bF78gNEe9DRFgyY1FUSM3abszCOlecePdLHgLpayWHCpz7UXXNvnxgtu7HN8wXGfQ7GZdsNeHjGS2utmGGk6GNprT2UyJaTmN5TlN2yGsPKotn4eUZHaiR/HXsn/PmHHd1aSFV2XqvjTSV0MbD7DltNUvm6GlbpqaHNZzmP+YT6XNzgsRUJL+uCvWVO4KXUPrevpcWWsu9c09N/OZeN8Zzt8s717stvc363e1TXwArwEr0EI3oJ98B4cgzOAwQB8Bl/A18a3xo/Gz8avUrq6Uvk8A7XV+P0HTxRD4g==</latexit>Qh
Figure 14: Illustration of section maps between SR(θ, b, v, y )andSR(θ, b, v, y +h). We connect two
points QandQhand let Pbe the intersection of {x|x⊤θ=b}. And then, using P, we can set the
section maps between SR(θ, b, v, y )andSR(θ, b, v, y +h).
Lemma 17 (One-side decay rate of doubly sliced ball) Ifv⊥θ, suppose we construct linear sec-
tion map constructed by the sliced balls between SR(θ, b, v, y )andSR(θ, b, v, y +h)forb≤αRin
the way we described before. By setting M= 2√
dL(R) +8√
d
R2(1−α), the one-side decay rate of the
section density g(·)in the interval −1
M< y < y +h <1
Mis bounded by
g(y+h)
g(y)≥exp(−Mh).
46Proof We aim to estimate sandHto apply arguments in Appendix I.4. To bound s, we can easily
see that s≤R√
R2−(y+h2)≤2since it is related to the slope of the tangent line at y+h.
To bound H, with some elementary calculation, we can see that
H=H′×p
R2−y2−c⋆Rp
R2−y2
for
H′≥R2−(y+h)2
y+h.
H′is define in the figure 14. Then we can see that
d
H≤dy+h
R2−(y+h)2p
R2−y2
p
R2−y2−αR
≤2yd
R2−y22
1−α
for any y, y+h∈[−1
M,1
M]. Then we can get
2√
dL(R) +2d
H≲2√
dL(R) +8yd
R2(1−α)≤M
The remaining part is proved by the argument from Appendix I.4.
I.6.2 Caseπ
2−τ0≤∠(v, θ)≤π
2
In this case, we define θ′:=θ−v(v⊤θ)
∥θ−v(v⊤θ)∥2and we aim to define section maps between SR(θ, b, v, y )
using vandθ′. For new sections, SR(θ′, b, v, y ), we can make section maps as we described in
Lemma 17. We can see that these section maps also become the section maps between SR(θ, b, v, y ).
By using Lemma 17 for αwith1−α=1−c⋆
2andθ′, v, we have
g(y+h)
g(y)≥exp(−Mh)
holds for M= 2√
dL(R) +16
R2(1−c⋆)for any −1
M< y < y +h <1
M.
I.6.3 Case 0≤∠(v, θ)≤π
2−τ0
In this case, the natural section map between SR(v, y)andSR(v, y+h)for any −1
M< y < y +h <
1
Mcan be a expanding section map for y, y+h∈[−1
M,1
M]. We already prove that the one-side
decay rate of section density is bounded by
g(y+h)
g(y)≥exp(−Mh)
using Lemma 16.
47<latexit sha1_base64="CEshLfCf6/+AUiAmdWXSb32QrMQ=">AAAGNnicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXYUSrapMpElWdyJzHxS7anTTTNP7CFL+FX2LBDbPkEPJkJ6ThFwlLim3PO9X34xrFi1NhW6/vK6q3bjTt31+6t33/w8NHjjc0np0ZmOoGTRDKpOzExwKiAE0stg47SQHjMoB2P9gu+fQHaUCk+2omCHicDQVOaEOug08gOwZLzjWZrqzVbeNkIK6OJqnV0vtl4FvVlknEQNmHEmG7YUraXE21pwmC6HmUGFElGZABdZwrCwfTyWbpT/NIhfZxK7T7C4hl63SMn3JgJj52SEzs0PleAN3HdzKZvezkVKrMgkjJQmjFsJS5qx32qIbFsgmsnCmJjGk9vihK4vcjRBPOUaqpZ5gEG4RqhiYUAp0wSG+CBJmpIk3GAR9KC2zgZQQKMOStjlmp5GWAq+q59KdXGeZgsTogq7sRLhA2Ugcw1W/ZhiZKa2iGvw0NXqdaQ1lFGlck8pXO1UjIz9drhioE0cPd4AUvnzCvzTwKpgRfqem9jL+S/yhyXo1HDlJbSi66INiOqHCjgMpGcE9HPo8NpHhW3Fcf54dTjjhfcsc8dHCzIA5904+nY3chdn82LHzjaxZ6maNCkpsKEsZuUnXmkNO/4kc4W3NlSFgsu9bn2gmv73HjBjX2OLzjucyQ2027YyyMGqb1qhpGmg6G98lQmU0Jqfk1ZfuNmiCuPajvPIypSO/Hj2Ev53yds+85KsqLrUhV/OqmLgc1n2HKaytfNsFJXDW0uy3nM38/n8hqXSNGnJb3/16wp3JS6t9b19Kgy1t1rGvpv57Jxur0Vvt7a+bDT3Nup3tU19By9QK9QiN6gPfQOHaETlKBP6DP6gr42vjV+NH42fpXS1ZXK5ymqrcbvP6dIRVo=</latexit>✓<latexit sha1_base64="S3MY0AHnz1mHFgJwKbfKKmJhj1o=">AAAGMXicjVTLbhMxFHULgVJeLexgYxEhsRhVmaoCpG4qtZXY0Va0SZWJKs/kTmLFL9meNtE0X8AWvoSv6Q6x5SfwZCak4xQJS4lvzjnX9+Ebx4pRY1utm5XVe/cbDx6uPVp//OTps+cbmy/OjMx0AqeJZFJ3YmKAUQGnlloGHaWB8JhBOx7tF3z7ErShUnyxEwU9TgaCpjQh1kHHlxcbzdZWa7bwshFWRhNV6+his/Eq6ssk4yBswogx3bClbC8n2tKEwXQ9ygwokozIALrOFISD6eWzTKf4rUP6OJXafYTFM/S2R064MRMeOyUndmh8rgDv4rqZTT/2cipUZkEkZaA0Y9hKXJSN+1RDYtkE104UxMY0nt4VJXB7kaMJ5inVVLPMAwzCNUITCwFOmSQ2wANN1JAm4wCPpAW3cTKCBBhzVsYs1fIqwFT0XftSqo3zMFmcEFVch5cIGygDmWu27MMSJTW1Q16Hh65SrSGto4wqk3lK52qlZGbqtcMVA2ng7vESls6ZV+afBFIDL9T13sZeyH+VOS5Ho4YpLaUXXRFtRlQ5UMBVIjknop9Hh9M8Km4rjvPDqcedLLgTnzs4WJAHPunG07G7kbs+mxc/cLSLPU3RoElNhQljdyk780hp3vEjnS+486UsFlzqc+0F1/a58YIb+xxfcNznSGym3bCXRwxSe90MI00HQ3vtqUymhNT8lrL8xs0QVx7VdpFHVKR24sexV/K/T9j2nZVkRdelKv50UhcDm8+w5TSVr5thpa4a2lyW85h/ns/lLS6Rok9Lev+vWVO4KXXPrOvpUWWsu9c09N/OZeNseyt8v7VzvNPc26ne1TX0Gr1B71CIPqA99AkdoVOUIEBf0Tf0vfGjcdP42fhVSldXKp+XqLYav/8AT8FDLA==</latexit>v
<latexit sha1_base64="XzJ8PTcNcOiOdlOyo7Tq7DPxkqQ=">AAAGS3icjVTdbtMwFPYGhTH+NrgCbiwqJC6iqZkmQNrNpG0Sd4yJrZ2aUjnpSWvVsY3tbK2yiKfhFp6EB+A5uENc4LQpWdwhYan16fd9x+fHpw4lo9q0Wj9WVm/cbNy6vXZn/e69+w8ebmw+OtUiVRGcRIIJ1QmJBkY5nBhqGHSkApKEDNrheL/g2+egNBX8g5lK6CVkyGlMI2Is1N94EmSdj4EREgdmBIbgYAifcIiDvL/RbG21ZgsvG35pNFG5jvqbjafBQERpAtxEjGjd9VvS9DKiDI0Y5OtBqkGSaEyG0LUmJwnoXjarIccvLDLAsVD2ww2eoVc9MpJoPU1Cq0yIGWmXK8DruG5q4je9jHKZGuDRPFCcMmwELhqCB1RBZNgU107kxIQ0zK+L4tm9yFF7i5RqqlnmHgZuG6GIAQ/HTBDj4aEickSjiYfHwoDdEjKGCBizVsoMVeLCw5QPbPtiqrT10GkYEVlclJMIG0oNqW22GMASJRQ1o6QOj2ylSkFcRxmVOnWU1tUIwXTutMMWA7Fn7/Ecls5ZVOaeBEJBUqjrvQ2dkP8qczIfjRomlRBOdEmUHlNpQQ4XkUgSwgdZcJhnQXFbYZgd5g53XHHHLndwUJEHLmnH07K7gb0+kxU/cLCLHU3RoGlNhQlj1yk7i0hx1nEjnVXc2VIWFRe7XLvi2i43qbiJyyUVl7gcCXXe9XtZwCA2l00/UHQ4MpeOSqeSC5VcUc6/cdPHpUe59bOA8thM3TjmQvz3CduusxSs6LqQxZ9OqGJgsxm2nKZ0dTNsriuHNhPzeczeLebyChcJPqBzev+vWVPYKbUPsO3pUWms29fUd9/OZeN0e8t/tbXzfqe5t1O+q2voGXqOXiIfvUZ76C06QicoQp/RF/QVfWt8b/xs/Gr8nktXV0qfx6i2bjX+ADJjS9o=</latexit>{X>✓ b}
<latexit sha1_base64="1MXdkDenjdstwrCirlKxpu7CZ0c=">AAAGTnicjVTLbhMxFHULKaU82sIOWFhESEWMqkxVAVI3ldpK7CiFNqkyUeRxPIkVv2R72kTT2fA1bOFL2PIj7BB4kgnpOEXCUuKbc871ffjGsWLU2Ebjx9Lyrdu1lTurd9fu3X/wcH1j89GZkanG5BRLJnUrRoYwKsippZaRltIE8ZiRZjw8KPjmBdGGSvHJjhXpcNQXNKEYWQd1N55FHNlBHGcf8+7JVmQHxKIgDi6C8avBy+5GvbHdmCy4aISlUQflOu5u1p5EPYlTToTFDBnTDhvKdjKkLcWM5GtRaohCeIj6pO1MgTgxnWxSRg5fOKQHE6ndR1g4Qa97ZIgbM+axUxZJG58rwJu4dmqTt52MCpVaIvA0UJIyaCUsegJ7VBNs2RhWThTIxjTOb4oSuL3I0QSzlCqqSeYBJMI1QiNLApgwiWwA+xqpAcWjAA6lJW7jaEgwYcxZKbNUy8sAUtFz7UuoNs7DpDFGqrgrLxHWV4akrtmyRxYoqakd8Co8cJVqTZIqyqgyqad0rlZKZnKvHa4YkgTuHi/IwjmzyvyTiNSEF+pqb2Mv5L/KHE1Ho4IpLaUXXSFthlQ5UJBLLDlHopdFR3k2m+6j3ONO5tyJzx0ezslDn3Tj6di9yF2fzYofMNqDnqZo0Liigoixm5StWaQka/mRzufc+UIWcy7xueaca/rcaM6NfI7POe5zKDZ5O+xkESOJvaqHkab9gb3yVCZVQmp+TTn9hvUQlh7l1s0iKhI79uPYS/nfJ+z4zkqyoutSFX86qYuBzSbYYprK102wqa4c2kxO5zF7P5vLaxyWoken9MFfs6JwU+reYNfT49JYc69p6L+di8bZznb4env3w259f7d8V1fBU/AcbIEQvAH74B04BqcAg8/gC/gKvtW+137WftV+T6XLS6XPY1BZK6t/ADkFTOc=</latexit>SR(ω,b ,v ,y+h)
<latexit sha1_base64="M3EAGo6G6mKQkuyzAbMYHOY7ZrE=">AAAGTnicjVTLbhMxFHULKaW8UtgBC4sIqUijKlNVgNRNpbYSO0qhTapMFHkmnsSKX7I9bSJ3NnwNW/gStvwIOwSeZEI6TpGwlPjmnHN9H75xLCnRptn8sbJ663Zt7c763Y179x88fFTffHymRaYSfJoIKlQ7RhpTwvGpIYbitlQYsZjiVjw6KPjWBVaaCP7JTCTuMjTgJCUJMg7q1Z9HDJlhHNuPec+e5FuRGWKDgji4CCavevVGc7s5XXDZCEujAcp13NusPY36IskY5iahSOtO2JSma5EyJKE434gyjSVKRmiAO87kiGHdtdMycvjSIX2YCuU+3MApet3DIqb1hMVOWSStfa4Ab+I6mUnfdi3hMjOYJ7NAaUahEbDoCewThRNDJ7ByIkcmJnF+U5TA7UWOOpinVFFNMw8g5q4RChkcwJQKZAI4UEgOSTIO4EgY7DaGRjjBlDoro4YocRlAwvuufSlR2nnoLE6QLO7KS4QOpMaZa7bo4yVKKGKGrAoPXaVK4bSKUiJ15imdqxGC6txrhysGp4G7xwu8dM68Mv8kLBRmhbra29gL+a8yx7PRqGBSCeFFl0jpEZEO5PgyEYwh3rfRUW7n032Ue9zJgjvxucPDBXnok248HbsXuesztvgBoz3oaYoGTSoqiCi9SdmeR0pt2490vuDOl7JYcKnPtRZcy+fGC27sc2zBMZ9Dsc47YddGFKfmqhFGigyG5spT6Uxyodg15ewbNkJYepRbz0aEp2bixzGX4r9P2PGdpaBF14Us/nRCFQNrp9hymtLXTbGZrhxaK2bzaN/P5/IalwjeJzP64K9ZUbgpdW+w6+lxaWy41zT0385l42xnO3y9vftht7G/W76r6+AZeAG2QAjegH3wDhyDU5CAz+AL+Aq+1b7XftZ+1X7PpKsrpc8TUFlr638Ah5xNTA==</latexit>SR(✓,b ,v ,y)<latexit sha1_base64="EpsE2PvJzHbRCnuKszLFSFahqMk=">AAAGQ3icjZTNbhMxEMfdQqCUrxZucLGIkIq0qrJVBUi9VGorcaMU2qTKRpHX8SZW/CXb2yTa7qNwhSfhIXgGbogrEt5kQ7JOkbCUeDK/vz3j8cSxYtTYRuP72vqt27U7dzfubd5/8PDR463tJxdGphqTcyyZ1K0YGcKoIOeWWkZaShPEY0aa8fCo4M0rog2V4pOdKNLhqC9oQjGyztXd2o44soM4zj7m3bOdq2DyqrtVb+w2pgOuGmFp1EE5TrvbtWdRT+KUE2ExQ8a0w4aynQxpSzEj+WaUGqIQHqI+aTtTIE5MJ5vmnsOXztODidTuIyycepdXZIgbM+GxUxaZGp8VzptYO7XJ205GhUotEXgWKEkZtBIWhYA9qgm2bAIrOwpkYxrnN0UJ3FzkaIJ5ShXVNPMAEuEKoZElAUyYRDaAfY3UgOJxAIfSEjdxNCSYMOaslFmq5SiAVPRc+RKqjVth0hgjVVyQlwjrK0NSV2zZIytIamoHvOoeuJNqTZKql1FlUk/pllopmcm9crjDkCRw93hFVvaZn8zfiUhNeKGu1jb2Qv7rmONZa1R8SkvpRVdImyFVzinICEvOkehl0UmezVv6JPfY2YKd+ez4eAGPfeja09GDyF2fzYofMDqAnqYo0KSigoixm5SteaQka/mRLhfsciWLBUt81lywps/GCzb2GV8w7jMUm7wddrKIkcRe18NI0/7AXnsqkyohNV9Szr5hPYTlinLqZhEViZ34cexI/vcOe/5iJVlRdamKP53URcNmU99qmsrXTX0zXdm0mZz1Y/Z+3pdLDEvRozN89NesKFyXuofX1fS0NDbdaxr6b+eqcbG3G77e3f+wXz/cL9/VDfAcvAA7IARvwCF4B07BOcBgBD6DL+Br7VvtR+1n7ddMur5WrnkKKqP2+w9bPkm5</latexit>SR(v, y)
<latexit sha1_base64="cNg0g6A7WwTvq2EOcsBeNa1p8GI=">AAAGRXicjZTNbhMxEMfdQqCUr5Te4GIRIRWxqrJVBUi9VGorcaMU2qTKRpHX8SZW/CXb2yba7rNwhSfhGXgIbogreJMN6TpFwlLiyfz+9ozHE8eKUWObze8rq7du1+7cXbu3fv/Bw0eP6xtPzoxMNSanWDKp2zEyhFFBTi21jLSVJojHjLTi0UHBWxdEGyrFJztRpMvRQNCEYmSdq1ffjDiywzjOPua9k62LYPJq+LJXbzS3m9MBl42wNBqgHMe9jdrTqC9xyomwmCFjOmFT2W6GtKWYkXw9Sg1RCI/QgHScKRAnpptNs8/hC+fpw0Rq9xEWTr3XV2SIGzPhsVMWuRqfFc6bWCe1ydtuRoVKLRF4FihJGbQSFqWAfaoJtmwCKzsKZGMa5zdFCdxc5GiCeUoV1TTzABLhCqGRJQFMmEQ2gAON1JDicQBH0hI3cTQimDDmrJRZquVlAKnou/IlVBu3wqQxRqq4Ii8RNlCGpK7Ysk+WkNTUDnnVPXQn1ZokVS+jyqSe0i21UjKTe+VwhyFJ4O7xgiztMz+ZvxORmvBCXa1t7IX81zHHs9ao+JSW0ouukDYjqpxTkEssOUein0VHeTZv6qPcYycLduKzw8MFPPSha09H9yJ3fTYrfsBoD3qaokCTigoixm5StueRkqztRzpfsPOlLBYs8VlrwVo+Gy/Y2Gd8wbjPUGzyTtjNIkYSe9UII00HQ3vlqUyqhNT8mnL2DRshLFeUUy+LqEjsxI9jL+V/77DjL1aSFVWXqvjTSV00bDb1LaepfN3UN9OVTZvJWT9m7+d9eY1hKfp0hg/+mhWF61L39LqaHpfGuntNQ//tXDbOdrbD19u7H3Yb+7vlu7oGnoHnYAuE4A3YB+/AMTgFGEzAZ/AFfK19q/2o/az9mklXV8o1m6Ayar//AA/bSmA=</latexit>SR(v, y+h)Figure 15: For the case θandvhas not-too-large angle, we first define linear section map between
S(θ, y)andS(θ, y+h). This section map can also be the linear section map between S(θ, b, v, y )
andS(θ, b, v, y +h). Using this map, we can bound the one-side decay rate of section densities of
S(θ, b, v, y )by Lemma 16.
I.7 Proof of Proposition 4
We first fix θandvwith the same argument as in the proof of Theorem 2. For fixed history contexts
X= (X⊤
1, . . . X⊤
K)and fixed θandv. Define bas the second largest value of (X⊤
1θ, . . . X⊤
Kθ). By
the Condition 1, we first bound
E[v⊤XaX⊤
av] =P[b≤c⋆R]E[v⊤XaX⊤
av|b≤c⋆R] +P[b > c ⋆R]E[v⊤XaX⊤
av|b > c ⋆R]
≥p⋆E[v⊤XaX⊤
av|b≤c⋆R]
≥p⋆E
E[v⊤XaX⊤
av| {b≤c⋆R} ∩Ωi({xj}j̸=i)]
≥p⋆E
E[v⊤XiX⊤
iv| {b≤c⋆R} ∩Ωi({xj}j̸=i)]
Then, we aim to bound
E[v⊤XiX⊤
iv| {b≤c⋆R} ∩Ωi({xj}j̸=i)]
and we further observe the density of X⊤
iv=y|Ωi({xj}j̸=i)∩ {b≤c⋆R}. It is a section density
of sections SR(θ, b, v, y ).
Case 1π
2−τ0≤∠(θ, v)≤π
2:By applying results from Appendix I.6 and Lemma 4, we have
E[v⊤XiX⊤
iv| {b≤c⋆R} ∩Ωi(xj)]≥c1
M2.
Case 2 0≤∠(θ, v)≤π
2−τ:By applying results from Appendix I.6 and Lemma 4, we have
E[v⊤XiX⊤
iv| {b≤c⋆R} ∩Ωi(xj)]≥c1
M2
Case 3 ∠(θ, v)≥π
2:When we play with −vinstead of v, it falls down to Case 1 or 2.
48I.8 Proof of Proposition 5
Proof Using similar argument, we do the fixed history arguments. Simply write X= (X1, . . . X K) =
X(t)| Ht−1and we first fix θand consider the greedy policy awith estimator θ. We define λ⋆as
a diversity constant of X. Also, we define Ln:=c0√
dxmax(1 + log dK+nlogγ)and set event
Bn:={Xi∈B(0, Ln)for all i∈[K]}. We determine γ > 0later. By Lemma 19, we have
P[Bn]≥1−1
γn. We apply the peeling technique to bound the truncated contexts.
E[XaX⊤
a] =E[XaX⊤
a;B1] +∞X
n=1E[XaX⊤
a;Bn+1\Bn]
By setting α=c0√
dxmax, β= 1 + log dK, observe that
∞X
n=1E[XaX⊤
a;Bn+1\Bn]⪯∞X
n=1L2
n+11
γnId
⪯∞X
n=1(α(β+nlogγ))21
γnId
⪯∞X
n=12α2(β2+n2log2γ)1
γnId.
Ifγ≥3α2β21
λ⋆, we have
∞X
n=1E[XaX⊤
a;Bn+1\Bn]⪯1
2λ⋆Id.
Using Theorem 2, we get
λ⋆Id≤E[XaX⊤
a;B1] +1
2λ⋆Id
and hence
E[XaX⊤
a;B1]≥1
2λ⋆.
Finally, we get
E[XaX⊤
a|B1]≥1
2λ⋆.
I.9 Proof of Proposition 6
Define ∆(X)as the suboptimality gap of Xand denote its density as fX(x), x∈(Rd)K. By the
direct expectation, we get
P[∆(X)≤ε] =Z
∆(X)≤εfX(x)dx
=Z
{∆(X)≤ε}∩DfX(x)
P[X∈D]dx
≤Z
∆(X)≤εfX(x)
P[X∈D]dx
≤1
1−δP[∆(X)≤ε]
holds.
49I.10 Proof of Proposition 7
By conditioning Ht−1, we define fixed history contexts X= (X⊤
1, . . . X⊤
K)similar to previous
proofs in Appendix G. We decompose the probability of suboptimality gap as
P[∆(X)≤ε] =KX
i=1P[{∆(X)≤ε} ∩Ω⋆
i]
=KX
i=1E
P[{∆(X)≤ε} ∩Ω⋆
i| {Xj=xj}j̸=i]
=KX
i=1E
P[{max
j̸=ix⊤
jθ⋆≤X⊤
iθ⋆≤max
j̸=ix⊤
jθ⋆+ε} | {Xj=xj}j̸=i]
the last equality holds by the definition of Ω⋆
i. Next we aim to bound
P[{max
j̸=ix⊤
jθ⋆≤X⊤
iθ⋆≤max
j̸=ix⊤
jθ⋆+ε} | {Xj=xj}j̸=i]
and for the same argument, we only need to bound the maximum density of
P[X⊤
iθ⋆=y| {Xj=xj}j̸=i].
and it is enough to bound its one-side decay rate by Lemma 5. Since the conditional density of
Xi| {Xj=xj}j̸=i,
sayf3(·), has same LAC with constant function L(R), hence it has bounded decay rate by√
dL(R)
by Lemma 3. Also the density P[X⊤
iθ⋆=y| {Xj=xj}j̸=i]is a section density of f3ofSR(θ⋆, y)
fory∈[−R, R]. Then we can observe that at least one of the directions of θ⋆or−θ⋆, the sections
SR(θ⋆, y)are expanding section in BR. Then by applying Lemma 8, we can bound the one-side
decay rate of section density X⊤
iθ⋆| {Xj(t) =xj}j̸=iby√
dL(R)and finally we can bound the
maximum density by 3√
dLRby using Lemma 5. Therefore, by the decomposition we did before,
we finally get
P[∆(X(t))≤ε]≤X
i∈[K]3√
dL(R)
≤3K√
dL(R).
I.11 Proof of Corollary 1
We have λ⋆(t)≥cp
d1
(L(R)+1
R(1−c⋆))2with1−c⋆=R′
r≍1by using Proposition 4 and Lemma 14.
Hence we get λ⋆(t)≥cp
dL(R)2:=λ⋆. Also, using Proposition 6, we have the margin constant of
truncated contexts ¯C∆is bounded by1
1−pC∆, where C∆is defined in Theorem 3. We can see that
Reg(T)≤cσ2dR2C∆1
λ⋆(log(T))2
≤eO(d2.5R2L(R)21
p)
holds.
I.12 Proof of Corollary 2
IfL≥√
dc0xmax(3 + log dK), we can use the same proof of the Theorem 2. For the diversity
constant of truncated contexts, it is lower bounded by1
2λ⋆(t)where λ⋆(t)is define in Theorem 2.
Also, using Proposition 6, we have the margin constant of truncated contexts ¯C∆is bounded by
501
1−pC∆, where C∆is defined in Theorem 3. Combining these observations, we finally can apply
Proposition 9 since truncated contexts are also has bounded ψ1-norm by xmax, and get the wanted
result.
I.13 Proof of Corollary 3
We can prove it directly by combining the results of Proposition 4 and Proposition 7. This can be
obtained directly by combining the results and Proposition 8.
J Analysis After Challenges 1 and 2 are Satisfied
We now present the results and proofs to obtain an exact regret bound after the two challenges are
addressed. Recall that we define the (unexpected) regret as reg′(t) :=Xa⋆(t)(t)⊤θ⋆−Xa(t)(t)⊤θ⋆
and expected regret as reg(t) =EHt−1,X(t)[reg′(t)]. We can achieve logarithmic regret bounds if
the contexts meet Challenges 1 and 2.
We first present our regret analysis for bounded contexts.
Proposition 8 (Regret bound for bounded contexts) For bounded contexts as ∥Xi(t)∥2≤R, sup-
pose that E[Xa(t)Xa(t)| Ht−1]⪰λ⋆IandC∆(X(t))≤C∆for all t∈[T]andHt−1. Under the
Assumption 1, expected regret of Algorithm 1 is bounded by
Reg(T)≤cσ2R2dC∆1
λ⋆(logT)2=eO(R2dC∆
λ⋆).
Next, we present the regret bound result for unbounded contexts, under the satisfaction of two
challenges.
Proposition 9 (Expected regret analysis for unbounded contexts) For unbounded contexts as
∥Xi(t)∥ψ1≤xmax, suppose that E[Xa(t)Xa(t)| Ht−1]⪰λ⋆IandC∆(X(t))≤C∆for all t∈[T]
andHt−1. Under the Assumption 1, expected regret of Algorithm 1 is bounded by
Reg(T)≤cσ2x2
maxdC∆1
λ⋆(logT)4=eO(x2
maxdC∆
λ⋆).
J.1 Proof of Proposition 8
Firstly, set T0:=c0
λ⋆R(3 log T+ log d). Before starting the proof, we define the good events which
satisfy the sufficient concentration of the estimator ˆθt.
Definition 17 From now on, in this section, we define the event by Et.
Et:={λmin(Σ(t))≥λ⋆
4t}
Which is the event that the Gram matrix has a sufficient growth of the minimum eigenvalue. The event
Etoccurs with high probability according to Corollary 9.
Corollary 4 ForT0=1
c1λ⋆R(3 log T+ log d), the following holds.
P[T\
τ=T0Et]≥1
2T
Proof Using the Corollary 9, with probability 1−1
2T2, event Etholds. By the union bound, we get
the result.
We define the eventTt
τ=T0Et:=Et.
51Definition 18 (Self-normalized bound of OLS estimator) Next, we define the event Ftas
Ft:=n
∥ˆθt−θ⋆∥Σ(t)≤2σp
dlog (T(1 +tR2)) + 1o
which is the event that satisfies the self-normalized concentration of the estimator.
Lemma 18 (Concentration of OLS Estimator) For any t >4
λ⋆, under the Et,
P[Ft]≥1−1
2T2
holds.
Proof Under the event Et, we have Σ(t)⪰1
4λ⋆tId, and the concentration of the OLS estimator
satisfies
∥ˆθt−θ⋆∥Σ(t)= (tX
τ=1Xa(τ)(τ)ητ)(Σ(t))−1(tX
τ=1Xa(τ)(τ)ητ)
≤(tX
τ=1Xa(τ)(τ)ητ)(1
2Σ(t) +Id)−1(tX
τ=1Xa(τ)(τ)ητ)
≤2 (tX
τ=1Xa(τ)(τ)ητ)(Σ(t) +Id)−1(tX
τ=1Xa(τ)(τ)ητ)
| {z }
I.
To bound I, we use Lemma 24 from Abbasi-Yadkori et al. [1] and get the wanted result.
Next, we introduce an important lemma that is used to obtain the concentration of minimum eigenvalue
of the Gram matrix. We define the eventTt
τ=T0Et:=EtandTt
τ=T0Fτ:=Ft. We further define
the event Gt:=Et∩Ft.
Corollary 5 (Good events) For the event Gtdefined above, for any T0≤t≤T,
P[Gt]≥1−1
T
holds.
Proof Straightforward from previous observations.
Then under the event Gt, we have the following corollary.
Corollary 6 ( ℓ2concentration: bounded contexts) For any t≥T0, under the event Gt, we have
∥ˆθt−θ⋆∥2≤cσ√dlogT√λ⋆t(14)
for some absolute constant c >0.
Proof of the Proposition. Now we are ready to prove the Proposition 8. First, for time t≥T0,
the history Ht−1is contained in Gt−1with probability 1−1
T. The expectation of the reg′(t)is
calculated with randomness of the whole history Ht−1and the distribution of contexts X(t), and we
can observe the following for γ(d) := 4 σp
dlog(T+T2R2):
E[reg′(t)] =EHt−1
EX(t)[reg′(t)| Ht−1]
=EX(t)
reg′(t)|Gt−1
P[Gt−1] +RP[Gc
t−1]
≤6C∆γ(d)2R2
(t−1)λ⋆+RP[Gc
t−1](by Lemma 7)
≤6C∆γ(d)2R2
(t−1)λ⋆+R
T.
By summing up the inequalities until T, we can obtain the wanted result.
52J.2 Concentration of Sub-exponential Contexts
Next we aim to provide regret analysis for unbounded contexts. To do that, we first provide some
known facts for sub-exponential vectors. Under the Assumption 2, Xi(t)hasψ1norm with xmax.
Using property from Wainwright [38], for any v∈Sd−1, there exists absolute constant c0>0such
that
P[|Xi(t)⊤v|> c0xmax(1 +u)]≤exp(−u) (15)
holds for all u >0. If we set v=ei= (0,0, . . .1, . . .0)then we get the concentrations for each
coordinates, as
P[|Xij(t)|> c0xmax(1 +u)]≤exp(−u).
First, we investigate concentration of ℓ2norm of the contexts.
Lemma 19 (High probability ℓ2bound of the contexts) Suppose the contexts X(t)satisfies As-
sumption 2, then
max
i∈[K]∥Xi(t)∥2≤c0√
dxmax(1 + log( dK1
δ))
holds with probability greater than 1−δ.
Proof For any v∈Sd−1, we know that
P[|Xi(t)⊤v| ≥c0xmax(1 +u)]≤exp(−u)
holds by results from Appendix J.2. Then, we get the following directly
P[exists i∈[K]such that ∥Xi(t)∥2>√
d1
c0xmax(1 + log( dK1
δ))]
≤KX
i=1dX
j=1P[|Xij(t)|>1
c0xmax(1 + log( dK1
δ))]
≤dK×δ
dK=δ.
J.3 Proof of Proposition 9
Next we prove Proposition 9. It consists of 3 steps. First, we investigate the concentration of gram
matrix for unbounded contexts, and next we define several high-probability good events. Lastly, we
bound the regret bound using the peeling technique.
J.3.1 Gram Matrix Concentration
For bounded contexts, we can apply Lemma 23 and Corollary 9 to ensure the linear growth of the
Gram matrix. However, for unbounded contexts, we cannot apply Lemma 23 directly since they
require the ℓ2boundedness. We apply the same technique used in [ 20], they also deal with Gaussian
contexts which are not bounded. They use a truncation technique to guarantee the growth of the
Gram matrix: Interpret as the mixture of truncated contexts and large ℓ2norm contexts. We apply
similar arguments. In this section, we set our truncation radius L=c√
dxmax(1 + 3 log(dKT
λ⋆))and
define T1:=2
c1L
λ⋆(2 log T+ log d).
Lemma 20 For any t≥T1, the following holds with probability 1−2
T2:
Σ(t)⪰1
8λ⋆t.
53Proof We prove it using the similar argument used in Kannan et al. [20] to bound the Gram matrix.
We view contexts Xi(t)given the history Ht−1as a mixture of Xi(t)|BLandXi(t)|Bc
L. Consider
there is an invisible coin toss csfor every time s∈[t], and if cs= 1 the contexts are drawn in
X(t)|(BL)Kand if cs= 0, it is drawn from X(t)|(Bc
L)K. By our choice of high-probability
region radius L, for any cs,P[cs= 1]≥1−1
T2holds. Define Σ(t)as the Gram matrix of the
contexts sampled from the truncated distribution for all 1≤s≤t. For truncated contexts, the
Proposition 5 tell us that it has diversity constant with1
2λ⋆under our choice of L. Then, the following
holds by our independence assumption and Lemma 5:
P[λmin(Σ(t))>1
8λ⋆t]≤P[ci= 1for some i∈[t]] +P[λmin(Σ(t))>1
8λ⋆t]
≤1
T3×T+dexp(−c1λ⋆t
2L).
Hence if we choose t≥T1, we get the wanted result.
Recall that we define L=c√
dxmax(1 + 3 log( dKT1
λ⋆)).
J.3.2 Good Events
Concentration 1. For any t≥1, with probability 1−1
T2,∥Xi(t)∥2≤Lholds for all i∈[K].
Then, under that event,
det(Σ( t))≤(1 +TL2
d)d
and
log det(Σ( t))≤dlog(1 +TL2
d)
holds.
Concentration 2. For any 1≤t≤T, with probability 1−1
T2, using the Lemma 24, we can get
vuut(tX
τ=1Xa(τ)(τ)ητ)(Σ(t) +Id)−1(tX
τ=1Xa(τ)(τ)ητ)≤σq
2 log( T2det(Σ( t))1/2)
Concentration 3. In the previous section, we prove that for any t≥T1,
Σ(t)⪰1
8λ⋆t
holds with probability 1−2
T2.
Combining these three concentrations, we get the following result: with probability 1−4
T2for any
t≥T1,
∥ˆθt−θ⋆∥Σ(t)=vuut(tX
τ=1Xa(τ)(τ)ητ)Σ(t)−1(tX
τ=1Xa(τ)(τ)ητ)
≤vuut2(tX
τ=1Xa(τ)(τ)ητ)(Σ(t) +Id)−1(tX
τ=1Xa(τ)(τ)ητ)
≤σq
2 log( T2det(Σ( t))1/2)
≤2σq
log(det(Σ( t))1/2) + log T
≤2σr
d
2log(1 + TL2) + log T
≤cσp
dlogT.
Second inequality holds since under the Concentration 3, Σ(t)⪰1
2(Σ(t) +Id). Finally, we obtain
the following concentration results. ℓ2.
54Corollary 7 ( ℓ2concentration: unbounded contexts) For any t≥T1, we have
∥ˆθt−θ⋆∥2≤cσ1√λ⋆tp
dlogT (16)
for some absolute constant cwith probability 1−4
T2.
We define event Gtas the above inequality (16) holds and set Gt=ST
t=T1Gt.
Finally, we present a key analysis to bound the regret. Remark that we set T1:=1
c1Llogd
λ⋆(1+2 log T).
Corollary 8 (Good events) For the event Gtdefined above, for any T1≤t≤T,
P[Gt]≥1−4
T
holds.
Proof Straightforward by previous arguments.
J.3.3 Bounding Regret by the Peeling Technique
Lemma 21 Under the good event Gt−1, and if ∥Xi(t)∥ψ1≤xmax, then
EX(t)[reg′(t)]≤cdx2
maxC∆
(t−1)λ⋆(logT)3
holds for some absolute constant c >0.
Proof Under any history contained in Gt−1, by using (15), with probability 1−1
T2we have
max
i∈[K]|Xi(t)⊤(ˆθt−1−θ⋆)| ≤cσxmax√dlogTp
(t−1)λ⋆(1 + log KT) :=e
holds for some absolute constant c >0. We define this event as Kt. The the (unexpected) regret is
bounded by
reg′(t)≤2e.
We now bound the expected regret as
E[reg′(t);Gt−1]
=E[reg′(t);Gt−1∩Kt] +E[reg′(t);Gt−1∩Kc
t]
≤2eP[reg′(t)>0;Gt−1∩Kt] +E[reg′(t);Gt−1∩Kc
t]
≤2eP[∆(X(t))≤2e] +E[reg′(t);Gt−1∩Kc
t]
≤6C∆e2+E[reg′(t);Gt−1∩Kc
t].
In the last inequality, we use definition of Challenge 2 and Lemma 7.
Peeling technique for tail events. Next we bound the regret E[reg′(t)|Gt−1∩Kc
t]. In the event
Gt−1∩Kc
t, we do the peeling technique to bound the expected regret. Let Ln=c0√
dxmax(1 +
logdK+nlogγ)forn≥1where we determine γ >0later. Then, by results from Appendix J.2,
we get
P[max
i∈[K]|Xi(t)⊤θ⋆| ≥Ln]≤X
i∈[K]P[|Xi(t)⊤θ⋆| ≥Ln]
≤1
γn.
55We define the event Vnas{max i∈[K]|Xi(t)⊤θ⋆| ∈ [Ln, Ln+1]}. Then P[Vn]≤1
γnand
E[reg′(t);Vn]≤2Ln+1. Setα=c0√
dxmax, β= 1 + log dK. The regret can be decomposed as
E[reg′(t);Gt−1∩Kc
t]≤E[reg′(t);Gt−1∩Kc
t∩V1] +∞X
n=1E[reg′(t);Kc
t∩(Vn+1\Vn)]
≤2L1P[Gt−1∩Kc
t] +∞X
n=1E[reg′(t);Kc
t∩(Vn+1\Vn)]
≤2L11
T+ 2∞X
n=1Ln+11
γn
≤2L11
T+ 2∞X
n=1α(β+nlogγ)1
γn
≤3L11
T
holds when γ >(αβ)3+T2. Now we set γ= (αβ)3+T2and we have
E[reg′(t);Gt−1]≤cC∆(σxmax√dlogTp
(t−1)λ⋆logKT)2
≤cσ2C∆dx2
max1
λ⋆(t−1)(logT)3.
Main proof of the Proposition 9 We apply the result of Proposition 21. We get
E[reg′(t)]≤cC∆dx2
max1
λ⋆(t−1)(logT)3+E[reg′(t);Gc
t−1]
≤cC∆dx2
max1
λ⋆(t−1)(logT)3+E[reg′(t);Gc
t−1].
Next we observe the E[reg′(t);Gc
t−1]and we can also bound this using the peeling technique. We
define the same LnandVnin the previous Proposition 21
E[reg′(t);Gc
t−1]≤E[reg′(t);V1∩Gc
t−1] +∞X
n=1E[reg′(t); (Vn+1\Vn)∩Gc
t−1]
≤2L1P[V1∩Gc
t−1] +∞X
n=1E[reg′(t); (Vn+1\Vn)∩Gc
t−1]
≤2L14
T+ 2∞X
n=1Ln+11
γn
≤cL1
T.
for some absolute constant c >0. Then we get
E[reg′(t)]≤cC∆dx2
max1
λ⋆(t−1)(logT)3
and by summing up, we get the desired result directly. For t≤T1, just using peeling technique, we
can see that E[reg(t)]≤cL1hence Reg(T1)≤cL1T1≤cσ2C∆dx2
max1
λ⋆(logT)4.
K Discussion on Discrete-Supported Contexts
In this paper, we considered only context distributions with differentiable densities.
56Single-Parameter Linear Contextual Bandits. To the best of our knowledge, no study has
addressed the greedy algorithm for linear contextual bandits with discrete-supported stochastic
contexts.
Multiple-Parameter Linear Contextual Bandits. For multi-parameter linear contextual bandits,
Bastani et al. [8]proved that the Gibbs distribution, which has discrete support, satisfies the mar-
gin condition and their diversity condition, achieving logarithmic regret for the greedy algorithm.
However, their proof applies only to the two-arm case.
We assert that, for the K-armed multi-parameter linear contextual bandit with K≥3, Gibbs
distribution can fail under the greedy policy. For example, the two-dimensional Gibbs distribution
has support points (1,1),(1,−1),(−1,1),(−1,−1). However, if three parameters are given as
β⋆
1= (1,1), β⋆
2= (1,−1), β⋆
3= (−1,1), the diversity assumption of Bastani et al. [8] is violated.
We further claim that for multiple-parameter linear contextual bandits, the effectiveness of discrete-
supported contextual bandits with an arbitrary number of arms Khas not yet been thoroughly
studied.
L Dicussions, Limitations and Further Ideas
•Since our LAC class primarily includes differentiable densities, examining the performance
of the greedy bandit with discrete valued contexts would be a valuable future direction. For
discrete valued contexts, the only existing result by Bastani et al. [8]establishes performance
for a Gibbs distribution in a 2-arm (shared-context) bandit, but this generally fails when
K≥32and also it differs somewhat from our setup. In our setup, the linear contextual
bandit, no results are currently known for discrete contexts. Given that Bastani et al. [8]
studied a shared contexts setup, our work represents the largest class of distributions for
which the greedy bandit shows efficient performance in the linear contextual bandit problem.
•To get the concentration of minimum eigenvalue, the boundedness of contexts (random
variables) is required. However, for heavy tail contexts, the upper bound of the contexts’
norm can be large, so it leads to poor concentration. Dealing with not-truncated heavy tail
contexts can be another interesting problem.
M Numerical Experiments
We conducted numerical experiments to evaluate the performance of the greedy algorithm and
compare it with existing bandit algorithms, LinUCB from Abbasi-Yadkori et al. [1]and LinTS
from Agrawal and Goyal [4]. We conducted experiments for three cases with varying parameters:
d= 20, K= 20, T≤1000 ,d= 100 , K= 20, T≤1000 , and d= 20, K= 100 , T≤1000 , and
five different distributions of contexts: Uniform in a ball, truncated Student’s t, Laplace, Gaussian,
and exponential. The experiments were repeated 10 times for each case, and the deviation was also
displayed on the graph.
We note that the results were obtained as√
dtimes larger than the actual size because of the absence
of dimensional correction. For the uniform distribution, we used Unif(B(0,√
d)). The Laplace
contexts were generated by independently sampling each component of a Laplace distribution with
parameters µ= 0, b= 1. The Gaussian context was created so that each element of the feature
vectors was drawn from a multivariate Gaussian distribution with a covariance matrix VwithVi,i= 1
andVi,j= 0.7for any i̸=j. The truncated Cauchy contexts were generated by independently
sampling each component from a truncated Cauchy with loc 0, scale 1and truncation range [−5,5].
In most cases, the greedy algorithm produced the best results. Our theory predicted a polynomial
scale dependency on dimension for the regret of the greedy algorithm, and the experimental results
confirmed good performance even with an increased dimension. This discrepancy is due to the fact
that we considered the worst case. The experimental results for the three cases are listed.3
2Further details on discrete contexts are in Appendix K.
3We used jupyter notebook to run the experiments.
570 200 400 600 800 1000
Round (t)050100150200Cumulative Regret
d=20, K=20, Guassian
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)050100150200250300350400Cumulative Regret
d=20, K=20, Uniform
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)050100150200250300350Cumulative Regret
d=20, K=20, Laplace
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)050100150200250300Cumulative Regret
d=20, K=20, Truncated Cauchy
LinUCB
LinTS
LinGreedyFigure 16: Results For d= 20, K= 20
0 200 400 600 800 1000
Round (t)050100150200250Cumulative Regret
d=20, K=100, Guassian
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)0100200300400500Cumulative Regret
d=20, K=100, Uniform
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)0100200300400Cumulative Regret
d=20, K=100, Laplace
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)0100200300400Cumulative Regret
d=20, K=100, Truncated Cauchy
LinUCB
LinTS
LinGreedy
Figure 17: Results For d= 20, K= 100
580 200 400 600 800 1000
Round (t)0100200300400500Cumulative Regret
d=100, K=20, Guassian
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)0200400600800Cumulative Regret
d=100, K=20, Uniform
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)020040060080010001200Cumulative Regret
d=100, K=20, Laplace
LinUCB
LinTS
LinGreedy
0 200 400 600 800 1000
Round (t)02004006008001000Cumulative Regret
d=100, K=20, Truncated Cauchy
LinUCB
LinTS
LinGreedyFigure 18: Results For d= 100 , K= 20
N Technical Lemmas
Lemma 22 (Gronwall Inequality) Forg(y)∈Rsatisfiesg′
g(y)≥ −Min[y, y+h], then
g(y+h)
g(y)≥exp(−Mh).
Proof See classic PDE books like Evans [16].
N.1 Concentration Inequalities
Lemma 23 (Matrix Chernoff : Adapted Sequence from [36]) Consider a finite adapted sequence
{Xk}with filtration {Ft}t≥0of positive-semi definite matrices with dimension d, and suppose that
λmax(Xk)≤R almost surely.
Define the finite series
Y:=X
kXkand W:=X
kEk−1Xk
For all µ≥0
P{λmin(Y)≤(1−δ)µand λmin(W)≥µ} ≤d·e−δ
(1−δ)1−δµ/R
forδ∈[0,1)
Corollary 9 (Eigenvalue Growth of Adaptive Gram Matrix) If∥Xi∥2≤ xmax and
λmin(E[XiX⊤
i| Ht−1])≥λ0, then with probability 1−dexp(−c1λ0t
xmax)
λmin(tX
i=1XiX⊤
i)≥λ0
4t
holds for some absolute constant c1.
59Proof Putδ=3
4, µ=λ0t, R=xmaxat the Lemma 23.
Lemma 24 (Theorem 1 from [1]) Let{Ft}∞
t=0be a filtration. Let {ηt}∞
t=1be a real-valued stochas-
tic process such that ηtisFt-measurable and ηtis conditionally R-sub-Gaussian for some R≥0
i.e.
∀λ∈RE
eληt|Ft−1
≤expλ2R2
2
Let{Xt}∞
t=1be anRd-valued stochastic process such that XtisFt−1-measurable. Assume that Vis
ad×dpositive definite matrix. For any t≥0, define
Vt=V+tX
s=1XsX⊤
sSt=tX
s=1ηsXs.
Then, for any δ >0, with probability at least 1−δ, for all t≥0,
∥St∥2
V−1
t≤2R2log 
det 
Vt1/2det(V)−1/2
δ!
Lemma 25 (Lemma 10 from [1]) Suppose X1, X2, . . . , X t∈Rdand for any 1≤s≤t,∥Xs∥2≤
L. LetVt=λI+Pt
s=1XsX⊤
sfor some λ >0. Then,
det 
Vt
≤ 
λ+tL2/dd.
60NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Yes, we think our abstract and introduction well reflect the whole paper’s
contributions.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We wrote it in Appendix L.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
61Justification: Yes, we state whole assumptions and provide rigorous proofs.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We fully provide informations about experiments in Appendix M.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
62Answer: [No]
Justification: Our experiments are done with synthetic data. But we provide whole data
generation details and algorithm details.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Yes, we provide whole details in Appendix M.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Yes, we provide whole details in Appendix M.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
63•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Yes, we provide whole details in Appendix M.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Yes, we checked.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Yes, we include them in Introduction and Appendix L.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
64•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Not relevant.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: Not relevant.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
65•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Not relevant.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Not relevant.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Not relevant.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
66