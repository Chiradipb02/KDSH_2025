AUCSeg: AUC-oriented Pixel-level Long-tail
Semantic Segmentation
Boyu Han1,2Qianqian Xu1,3∗Zhiyong Yang2Shilong Bao2
Peisong Wen1,2Yangbangyan Jiang2Qingming Huang2,1,4∗
1Key Lab. of Intelligent Information Processing, Institute of Computing Technology, CAS
2School of Computer Science and Tech., University of Chinese Academy of Sciences
3Peng Cheng Laboratory
4Key Laboratory of Big Data Mining and Knowledge Management, CAS
{hanboyu23z,xuqianqian,wenpeisong20z}@ict.ac.cn,
{yangzhiyong21,baoshilong,jiangyangbangyan,qmhuang}@ucas.ac.cn
Abstract
The Area Under the ROC Curve (AUC) is a well-known metric for evaluating
instance-level long-tail learning problems. In the past two decades, many AUC
optimization methods have been proposed to improve model performance under
long-tail distributions. In this paper, we explore AUC optimization methods in the
context of pixel-level long-tail semantic segmentation, a much more complicated
scenario. This task introduces two major challenges for AUC optimization tech-
niques. On one hand, AUC optimization in a pixel-level task involves complex
coupling across loss terms, with structured inner-image and pairwise inter-image
dependencies, complicating theoretical analysis. On the other hand, we find that
mini-batch estimation of AUC loss in this case requires a larger batch size, re-
sulting in an unaffordable space complexity. To address these issues, we develop
a pixel-level AUC loss function and conduct a dependency-graph-based theoret-
ical analysis of the algorithm’s generalization ability. Additionally, we design a
Tail-Classes Memory Bank (T-Memory Bank) to manage the significant memory
demand. Finally, comprehensive experiments across various benchmarks con-
firm the effectiveness of our proposed AUCSeg method. The code is available at
https://github.com/boyuh/AUCSeg.
1 Introduction
Semantic segmentation aims to categorize each pixel within an image into a specific class, which
is a fundamental task in image processing and computer vision [ 33,51,75]. Over the past decades,
substantial efforts [ 58,23,43,96] have advanced the field of semantic segmentation. The mainstream
paradigm is to develop innovative network architectures that encode more discriminative features
for dense pixel-level classifications. Typical backbones include CNN-based [ 58,14,80] and newly
emerging Transformer-based methods [ 108,83,65,15,32], which have achieved the state-of-the-art
(SOTA) performance. Beyond this direction, researchers [ 24,66,48,41] have recently realized
thePixel-level Long-tail issue in Semantic Segmentation (PLSS) , as shown at the top of Figure 1.
Similar to the flaws of traditional long-tail problems, the major classes will dominate the model
learning process, causing the model to overlook the segmentation of minority classes in an image.
Several remedies have been proposed to alleviate this [ 49,61,10,56,104,92,78]. For example,
[77] introduces a category-wise variation technique inversely proportional to distribution to achieve
balanced segmentation; [ 66] introduces a sequence-based generative adversarial network for imbal-
anced medical image segmentation, and [ 41] develops a re-weighting scheme for semi-supervised
segmentation.
∗Corresponding authors.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Currently, mainstream studies fall into two camps. One is to develop carefully designed backbones
for long-tail distributions but leave the effect of loss functions unconsidered. The other is to conduct
empirical studies on the loss functions without exploring their theoretical impact on the generalization
performance. A question then arises naturally:
Can we find a theoretically grounded loss function for PLSS on top of SOTA backbones?
Figure 1: Statistic information of pixel number for
each class in the Cityscapes training set and the
performance of previous methods (DeepLabV3+,
HRNet and STDC) compared to our method (AUC-
Seg). Our method aims to improve overall perfor-
mance, particularly for tail classes. The dashed
lines represent mIoU values for each class, while
the solid lines represent the average mIoU for the
head, middle, and tail classes.This paper provides an affirmative answer
from the AUC perspective and proposes a
novel framework called AUC-oriented Pixel-
level Long-tail Semantic Segmentation (AUC-
Seg). Specifically, AUC indicates the likelihood
that a positive sample scores higher than a neg-
ative one, which has been proven to be insen-
sitive to data distribution [ 86,101]. Applying
AUC to instance-level long-tail classifications
has shown promising progress in the machine
learning community [ 86,100,88,68]. Moti-
vated by its success, this paper starts an early
trial to study AUC optimization for PLSS. The
primary concern is to study its effectiveness for
PLSS from a theoretical perspective. The key
challenge is that the standard techniques for gen-
eralization analysis [ 62,8,21] require the loss
function to be expressed as a sum of indepen-
dent terms. Unfortunately, the proposed loss
function does not satisfy this assumption due
to the dual effect of structured inner-image de-
pendency and pairwise inter-image dependency.
This complicated structure poses a big challenge
to understanding its generalization behavior. To
address this, we decompose the loss function
into inner-image and inter-image terms. On top
of this reformulation, we deploy the dependency
graph [ 103] to decouple the interdependency. Finally, we reach a bound of eO
τp
log (τNk)/N
,
where τbehaves like an indicator for imbalance degree, and kdenotes the number of pixels in each
image. This suggests optimizing AUC loss could ensure a promising performance under PLSS.
Back to the practical AUC learning process, we realize that the stochastic gradient optimization
(SGD) for structured pixel-level tasks imposes a greater computational burden compared to instance-
level long-tail problems. Specifically, the SGD algorithm of AUC requires at least one sample
from each class in each mini-batch [99,85]. In light of this, the primary choice is to adopt the
so-called stratified sampling on all images [ 70,60,86] for mini-batch generation (See Equation (7)).
Unfortunately, as shown in Figure 3(a) and (b), this is hard to implement under PLSS because
pixel-level labels are densely coupled in each image . Meanwhile, as shown in Proposition 1, we
also argue that directly using random sampling to include all classes would require an extremely large
batch size. This leads to unaffordable GPU memory demands for optimization, as described in the
experiments in Appendix G.6.
To alleviate this, a novel Tail-class Memory Bank (T-Memory Bank) is carefully designed. The main
idea is to identify those missing pixel-level classes in each randomly generated mini-batch and then
complete these absences using stored historical class information from the T-Memory Bank. This
enables efficient optimization of AUCSeg with a light memory usage, enhancing the scalability of
our proposed method, as shown in Figure 1. Finally, comprehensive empirical studies consistently
speak to the efficacy of our proposed AUCSeg.
Our main contributions are summarized as follows:
•This paper starts the first attempt to explore the potential of AUC optimization in pixel-level
long-tail problems.
2•We theoretically demonstrate the generalization performance of AUCSeg in semantic segmentation.
To our knowledge, this area remains underexplored in the machine-learning community.
•We introduce a Tail-class Memory Bank to reduce the optimization burden for pixel-level AUC
learning.
2 Related Work
2.1 Semantic Segmentation
Semantic segmentation is a subtask of computer vision, which has seen significant development since
the inception of FCN [ 58]. The most common framework for semantic segmentation networks is the
encoder-decoder. For the encoder, researchers typically use general models such as ResNet [ 37] and
ResNeXt [ 84]. As the segmentation tasks become more challenging, some specialized networks have
emerged, such as HRNet [ 75], ICNet [ 105], and multimodal networks [ 76,42]. For the decoder, a
series of studies focus on strengthening edge features [ 23,107], capturing global context [ 43,31,45],
and enhancing the receptive field [ 96,67,12,13]. Recently, the transformer has shown immense
potential, surpassing previous methods. A series of methods related to Vision Transformer [ 108,
83,65,15,74] are proposed. SegNeXt [ 32], which is the current state-of-the-art (SOTA) method,
possesses the same powerful feature extraction capabilities as the Vision Transformer and the same
low computational requirements as CNN. Apart from improving the network, some research [ 48,56,
24,11,10,66,39] is directed toward addressing the issue of class imbalance in semantic segmentation.
However, the effectiveness of these methods is not significant. In this paper, we aim to improve the
performance of long-tailed semantic segmentation from an AUC optimization perspective.
2.2 AUC Optimization
The development of AUC Optimization can be divided into two periods: the machine learning
era and the deep learning era. As a pioneering study, [ 20] ushers in the era of AUC in machine
learning. It studies the necessity of AUC research, which points out that AUC maximization
and error rate minimization are inconsistent. After that, AUC gains significant attention in linear
fields such as Logistic Regression [ 38] and SVM [ 46,47]. Then researchers begin to explore the
online [106, 28] and stochastic [94, 63] optimization extensions of the AUC maximization problem.
Research from the perspectives of generalization analysis [ 2,73,17] and consistency analysis [ 1,29]
provides theoretical support for AUC optimization algorithms. [ 57] is the first to extend AUC
optimization to deep neural networks, ushering in the era of AUC in deep learning. Meanwhile,
a series of AUC variants [ 87,86,68,88,69,90] emerge, gradually enriching AUC optimization
algorithms. Furthermore, in practice, AUC optimization demonstrates its effectiveness in various
class-imbalanced tasks, such as recommendation systems [ 5,6,4,7], disease prediction [ 79,30],
domain adaptation [89], and adversarial training [40, 91].
Despite significant progress, existing studies of AUC optimization mainly pay attention to the instance-
level imbalanced classification tasks. This paper starts an early trial to introduce AUC optimization
to semantic segmentation. However, due to the high complexity of pixel-level multi-class AUC
optimization, such a goal cannot be attained by simply using the current techniques in the AUC
community.
3 Preliminaries
In this section, we briefly introduce the semantic segmentation task and the AUC optimization
problem.
3.1 Semantic Segmentation Training Framework
LetD={(Xi,Yi)n
i=1|Xi∈RH×W×3,Yi∈RH×W×K}be the training dataset, where Hand
Wrepresent the height and width of the images, and Kdenotes the total number of classes. Let
fθbe a semantic segmentation model ( θis the model parameters), which commonly follows an
encoder-decoder backbone [ 75,105,108,83,32]. Let ˆYi=fθ(Xi)∈RH×W×Kbe the dense
3pixel-level prediction, i.e.,
ˆYi=fθ(Xi) =fd
θ(fe
θ(Xi)), (1)
where the encoder fe
θextracts features from the image Xi, and then the decoder fd
θpredicts each
pixel based on extracted features and outputs a dense segmentation map with the same size as Yi.
Furthermore, let Yi
u,vandˆYi
u,vrepresent the ground truth and prediction of the (u, v)-th pixel of the
i-th image, respectively. To train the model fθ, most current studies [ 14,80,83,65] usually adopt the
cross-entropy (CE) loss:
ℓce:=1
nnX
i=1H−1X
u=0W−1X
v=0"
−KX
c=1Yic
u,vlog(ˆYic
u,v)#
, (2)
where Yic
u,vandˆYic
u,vare the one-hot encoding of ground truth and the prediction of pixel (Xi
u,v,Yi
u,v)
in class c, respectively.
3.2 AUC Optimization
Area under the Receiver Operating Characteristic Curve (AUC) is a well-known ranking performance
metric for binary classification task, which measures the probability that a positive instance has a
higher score than a negative one [35]:
AUC (fθ) =P 
fθ(X+)> fθ(X−)|y+= 1, y−= 0
, (3)
where (X+, y+)and(X−, y−)represent positive and negative samples, respectively. When AUC→
1, it indicates that the classifier can perfectly separate positive and negative samples.
According to [ 87,86,88], given finite datasets, maximizing AUC (hθ)is usually realized by
maximizing its unbiased empirical estimation:
ˆAUC (hθ) = 1−1
n+n−n+X
i=1n−X
j=1ℓ(hθ(X+)−hθ(X−)), (4)
where ℓis a differentiable surrogate loss [ 86] measuring the ranking error between two samples, n+
andn−denote the number of positive and negative samples, respectively.
Moreover, we can directly optimize the following problem for AUC maximization:
min
θ1
n+n−n+X
i=1n−X
j=1ℓ(hθ(X+)−hθ(X−)). (5)
Note that, AUC has achieved significant progress in long-tailed classification [ 100,88,68]. Due to
the limitations of space, we refer interested readers to the literature [ 86,99] for more introductions to
AUC. However, most existing studies merely focus on the instance-level or image-level problems.
Inspired by its distribution-insensitive property [ 26], this paper starts an early trial to introduce AUC
to PLSS.
4 AUC-Oriented Semantic Segmentation
In this section, we introduce our proposed AUCSeg method for semantic segmentation. A brief
overview is provided in Figure 2. AUCSeg is a generic optimization method that can be directly
applied to any SOTA backbone for semantic segmentation. Specifically, AUCSeg includes two
crucial components: (1) AUC optimization where a theoretically grounded loss function is explored
for PLSS and (2) Tail-class Memory Bank , an effective augmentation scheme to ensure efficient
optimization of the proposed AUC loss. In what follows, we will go into more detail about them. For
clarity, we include a table of symbol definitions in Appendix A.
4Figure 2: An overview of AUCSeg.
4.1 Pixel-level AUC Optimization
Semantic segmentation is a multi-class classification task. Therefore, to apply AUC, we follow
a popular multi-class AUC manner, i.e., the One vs. One (ovo) strategy [ 64,34,86], which is
an average of binary AUC score introduced in Section 3.2. Specifically, on top of the notation of
Section 3.1, we further denote Dp={(Xi
u,v,Yi
u,v)|i∈[1, n], u∈[0, H−1], v∈[0, W−1]}as the
set of all pixels; the j-th element ( j∈[1, n×(H−1)×(W−1)]) inDpis abbreviated as (Xp
j,Yp
j)
for convenience. Given the model prediction fθ= (f(1)
θ, . . . , f(K)
θ),∀c∈[K],f(c)
θ∈[0,1], where
f(c)
θserves as a continuous score function supporting class c,AUCovo
segcalculates the average of
binary AUC scores for every class pair:
AUCovo
seg=1
K(K−1)KX
c=1X
c̸=c′AUC cc′(fθ), (6)
AUC cc′(fθ) =P(f(c)
θ(Xp
m)> f(c)
θ(Xp
n)|Yp
m=c,Yp
n=c′).
To this end, as introduced in Section 3.2, the goal is to minimize the following unbiased empirical
risk:
ℓauc:=KX
c=1X
c′̸=cX
Xp
m∈NcX
Xp
n∈Nc′1
|Nc||Nc′|ℓc,c′,m,n
sq , (7)
where we adopt the widely used square loss ℓsq(x) = (1 −x)2as the surrogate loss [ 29];ℓc,c′,m,n
sq :=
ℓsq(f(c)
θ(Xp
m)−f(c)
θ(Xp
n));Nc={Xp
k|Yp
k=c}represents the set of pixels with label cin the set
Dp, and|Nc|denotes the size of the set.
4.2 Generalization Bound
In this section, we explore the theoretical guarantees of the AUC loss function in semantic segmenta-
tion tasks and demonstrate that AUCSeg can generalize well to unseen data.
A key challenge is that standard techniques for generalization analysis [ 62,8,21] require the loss
function to be expressed as a sum of independent terms. Unfortunately, the proposed loss function does
5not satisfy this assumption because there are two layers of interdependency among the loss terms .
On one hand, semantic segmentation can be considered a structured prediction problem [ 16], where
couplings between output substructures within a given image create the first layer of interdependency.
On the other hand, the AUC loss creates a pairwise coupling between positive and negative pixels, so
any pixel pairs sharing the same positive/negative instance are interdependent, resulting in the second
layer of interdependency.
We present our main result in the following theorem and the proof is deferred to Appendix B.
Theorem 1 (Generalization Bound for AUCSeg) .LetEDh
ˆLD(f)i
be the population risk of ˆLD(f).
Assume F ⊆ { f:X →RH×W×K}, where HandWrepresent the height and width of the image,
andKrepresents the number of categories, ˆL(i)is the risk over i-th sample, and is µ-Lipschitz with
respect to the l∞norm, (i.e. ∥ˆL(x)−ˆL(y)∥∞≤µ· ∥x−y∥∞). There exists three constants A >0,
B > 0andC > 0, the following generalization bound holds with probability at least 1−δover a
random draw of i.i.d training data (at the image-level):
ˆLD(f)−EDh
ˆLD(f)i≤8
N+ηinner+ηinter√
Np
Alog (2 BµτNk +C)
+ 3 r
1
2N+Kr
1−1
N!s
log4K(K−1)
δ
,
where
ηinner=48µτlnN
N, η inter= 2√
2τ, τ = 
max
c∈[K]n(c)
max
n(c)
mean!2
,
n(c)
max= max Xn(X(c)),n(c)
mean =PN
i=1n(X(c)
i),N=|D|,k=H×WandX(c)represents the
pixel of class cin image X.
Remark 1. We achieve a bound of eO
τp
log (τNk)/N
, indicating reliable generalization with a
large training set. Here, τrepresents the degree of pixel-level imbalance. More interestingly, even
though we have kclassifiers for every single image, the generalization bound only has an algorithm
dependent on k, suggesting that pixel-level prediction doesn’t hurt generalization too much.
4.3 Tail-class Memory Bank
Motivation. Although we have examined the effectiveness of AUC for PLSS from the theoretical
point of view, there is a practical challenge when conducting AUC optimization for semantic
segmentation, as discussed in Section 1. Specifically, the stochastic AUC optimization, as defined in
Equation (7), requires at least one sample from each class in a mini-batch. In instance-level AUC
optimization , recent studies [ 87,86] often use a stratified sampling technique that generates batches
consistent with the original class distribution, as shown in Figure 3(a). Such a strategy will work well
when each image belongs to a unique category (say, ‘Banana’, ‘Apple’, or ‘Lemon’) in traditional
classifications. Yet it cannot apply to pixel-level cases because each sample involves multiple and
coupled labels, making it hard to split them for stratified sampling, as illustrated in Figure 3(b).
Meanwhile, we also provide a bound (Proposition 1) to show that simply adopting random sampling
will suffer from an overlarge batch size B, making an unaffordable GPU memory burden.
Proposition 1. Consider a dataset Dthat includes images with Kdifferent pixel categories. Let
pirepresent the probability of observing a pixel with label iin a given image. Randomly select B
images from Das training data, where
B= Ω
log(δ/K)
log(1−min
ipi)
.
Then with probability at least 1−δ, for any c∈[K], there exists Xin the training data that contains
pixels of label c.
Remark 2. The proof is deferred to Appendix C. Proposition 1 suggests that the value of Bis
inversely proportional to minipi. Note that piwill be smaller as the long-tail degree becomes more
6Figure 3: Instance-level and pixel-level task sampling.
severe, leading to a larger B. For example, in terms of the Cityscapes dataset with K= 19 classes,
assuming δ= 0.01andminipi= 1% ,Bshould be at least 759to guarantee that each class of pixels
appears at least once with a high probability. This results in a significant strain on GPU memory.
To address this, considering that the tail-class samples generally have less opportunity to be included
in a mini-batch and are often more crucial for final performance, we thus develop a novel Tail-
class Memory Bank (T-Memory Bank) to efficiently optimize Equation (7) and manage GPU usage
effectively. As depicted in Figure 3(c), the high-level ideas of the T-Memory Bank are as follows: 1)
identify missing tail classes of all images involved in a mini-batch and 2) randomly replace some
pixels in the image with missing classes based on stored historical class information in T-Memory
Bank. In this sense, we can obtain an approximated batch-version of Equation (7), i.e.,
˜ℓauc:=KX
c=1X
c′̸=c
ncnc′̸=0X
Xp
m∈Nc∪Tc
Xp
n∈Nc′∪Tc′1
|Nc||Nc′|˜ℓc,c′,m,n
sq (8)
where NcandTcrepresent the set of pixels with label cin the original image and those pixels stored
in the T-Memory Bank, respectively; ˜ℓc,c′,m,n
sq :=ℓsq(f(c)
θ(˜Xp
m)−f(c)
θ(˜Xp
n));˜Xprepresents the
sample after replacing some pixels with tail classes pixels from the T-Memory Bank.
Detailed Components. As shown in Figure 2, T-Memory Bank comprises three main parts: (1)
Memory Branch stores a set with SM(the Memory Size) images for each tail class. We define the
set asM={Mc1, . . . ,Mcnt}, where Ct={ci}nt
i=1denotes the labels of tail classes, and ntis the
total number of selected tail classes; (2) Retrieve Branch selects pixels from the Memory Branch to
supplement the missing tail classes and (3) Store Branch updates the Memory Branch whenever a
new image arrives. Algorithm 1 summarizes a short version of AUCSeg equipped with T-Memory
Bank. Please refer to the detailed version in Appendix D. Note that we introduce CE loss as a
regularization term for our proposed AUCSeg, which is widely used in the AUC community [ 99] to
pursue robust feature learning. Experiments demonstrate that the performance is insensitive to the
regularization weight λ, as shown in Figure 5d.
At the start of training, the Memory Branch is empty. In this case, we only calculate the loss
function ℓfor the classes present in the mini-batch, while the Retrieve Branch will not take any action.
Meanwhile, the Store Branch will continuously append pixel data of tail classes to the Memory
Branch. As the Memory Branch reaches its maximum capacity SM, we adopt a random replacement
strategy to update the Store Branch (Lines 5to6in Algorithm 1 or Lines 6to11in Algorithm 2).
As the training process progresses, if the Memory Branch is not empty, the Retrieve Branch kicks in
to count the missing classes in each image of the mini-batch, denoted as Cmiss. It then calculates
the number of classes needed to be added for optimization, nsample =⌈|Cmiss| ×RS⌉. Here, we
introduce a tunable sample ratio RSto strike a trade-off between the original and missing tail-class
semantic information. Finally, it uniformly retrieves the corresponding pixels of nsample missing
classes from the Memory Branch, resizes them by the resize ratio RR, and randomly selects positions
to overwrite (Lines 7to8in Algorithm 1 or Lines 13to18in Algorithm 2).
7Algorithm 1: AUCSeg Algorithm (Short Version)
Input: Training data D, number of tail classes nt, labels of tail classes Ct={ci}nt
i=1, Memory
Branch M={Mc1, . . .Mcnt}, memory size SM, sample ratio RS, resize ratio RR,
max iteration Tmax, batch size Nb
Output: model parameters θ
1foriter= 1toTmax do
2DB={(Xi,Yi)}Nb
i=1←SampleBatch (D, Nb);
3Cmiss⊆ Ct←MissingTailClasses (DB);
4Cmiss=Ct− Cmiss;
5▷forcminCmiss do
6 Extract pixels of class cmfromDBand save them to Mcm←RandomReplace (SM);
7▷fori= 1to⌈|Cmiss| ×RS⌉do
8 Randomly choose cmfromCmiss and sample pixels from Mcmto paste into DB←
SizeScale (SR);
9▷Calculate ˆYi=fθ(Xi)andℓ=˜ℓauc+λℓce;
10 Backpropagation updates θ.
Discussions. We recognize that Memory Bank [ 82,36] has achieved great success in deep
learning. However, our T-Memory Bank behaves differently compared to earlier studies. The
key difference is that the Memory Bank and T-Memory Bank are designed for different
tasks. The goal of the previous Memory Bank is to facilitate the traditional classifications by
storing instance-level or image-level features , while our T-Memory Bank specifically stores the
original pixels for each object. This strategy is particularly beneficial for our PLSS task. Addi-
tionally, the T-Memory Bank enables AUCSeg without substantially increasing GPU workload as
well as the number of samples per mini-batch by selectively replacing non-essential pixels. Our
experiments, detailed in Appendix G.6, include a comparison of GPU overhead. For more discussion
on the T-Memory Bank, please refer to Appendix E.
5 Experiments
In this section, we describe some details of the experiments and present our results. Due to space
limitations, please refer to Appendix F, Appendix G and Appendix H for an extended version.
5.1 Experimental Setups
The experiment includes three benchmark datasets: Cityscapes [ 19], ADE20K [ 109], and COCO-
Stuff 164K [ 9]. We use SegNeXt [ 32] as the backbone for our model and the mean of Intersection
over Union (mIoU) as the evaluation metric. We compare our method with 13recent advancements
and6long-tail approaches in semantic segmentation. All the long-tail methods also use SegNeXt
as the backbone. To ensure fairness, we re-implement the listed methods using their publicly shared
code and test them on the same hardware. Detailed introductions are deferred to Appendix F.
5.2 Overall Performance
Table 1 shows the quantitative performance comparisons. We draw the following conclusions: First,
most current algorithms perform poorly in long-tail scenarios. Specifically, performance drops
sharply from head to tail classes. For instance, the performance gap for PointRend and OCRNet
on the Cityscapes reaches up to 40%. Second, models using long-tail approaches generally achieve
better results than those that do not. However, they still fail to produce satisfactory outcomes. One
possible reason is that these long-tail approaches focus on reweighting, giving too much attention to
the tail classes and leading to overfitting. Additionally, our proposed AUCSeg method surpasses all
competitors in most metrics. This success is due to the appealing properties of AUC. Our method
consistently outperforms the runner-up by +1.21%,+0.75%, and+0.38% in tail classes mIoU across
the datasets. Overall mIoU also improves by +1.05%,+0.27%, and +0.31%. In some Head/Middle
8Table 1: Quantitative results on Cityscapes ,ADE20K andCOCO-Stuff 164K val set in terms of
mIoU (%). The champion and the runner-up are highlighted in bold and underline .
MethodADE20K Cityscapes COCO-Stuff 164K
Overall Head Middle Tail Overall Head Middle Tail Overall Head Middle Tail
DeepLabV3+ [14] 31.95 75.88 51.96 26.01 66.53 90.11 57.16 54.36 29.11 51.11 32.93 24.82
EncNet [102] 32.12 75.34 51.60 26.32 71.34 91.62 60.76 63.03 27.31 49.89 30.41 23.09
FastFCN [80] 29.78 74.20 49.44 23.86 63.97 90.37 52.43 51.22 28.37 50.60 32.52 23.96
EMANet [55] 32.83 75.77 50.03 27.36 70.93 91.69 60.61 61.97 28.48 49.73 29.97 24.85
DANet [27] 33.83 74.62 51.01 28.52 65.77 89.66 55.30 54.26 26.83 49.60 31.14 22.29
HRNet [71] 31.83 75.35 49.98 26.19 73.40 91.98 65.79 64.00 28.65 48.00 30.74 25.16
OCRNet [97] 29.64 74.00 49.40 23.72 66.95 90.24 63.18 50.21 28.67 51.04 32.41 24.33
DNLNet [93] 33.24 75.90 51.16 27.69 70.68 91.98 59.90 61.66 30.23 50.71 33.05 26.41
PointRend [50] 17.77 67.18 37.60 11.46 60.67 89.79 53.92 41.49 11.17 21.17 13.64 9.04
BiSeNetV2 [95] 10.26 60.38 28.72 4.10 73.04 92.00 63.52 64.93 10.30 34.96 12.71 5.92
ISANet [98] 29.53 74.34 48.77 23.64 70.63 91.67 61.50 60.43 26.37 48.87 30.78 21.86
STDC [25] 30.17 73.36 48.02 24.58 76.30 92.58 65.09 71.94 29.83 51.74 33.40 25.61
SegNeXt [32] 47.45 80.54 60.35 43.28 82.41 94.08 72.46 80.92 42.42 57.05 41.71 40.33
VS [49] 24.72 75.30 48.02 17.86 55.40 92.16 52.52 26.36 24.27 47.80 30.38 19.19
LA [61] 31.16 77.07 53.43 24.77 62.75 92.98 64.79 35.09 28.56 49.67 33.16 24.21
LDAM [10] 33.11 74.06 51.26 27.65 65.95 92.72 69.27 40.17 42.39 56.85 41.59 40.34
Focal Loss [56] 47.68 80.54 59.04 43.73 82.44 93.90 72.79 80.89 41.98 56.87 41.51 39.79
DisAlign [104] 48.15 80.33 59.14 44.31 81.94 93.61 72.12 80.36 42.10 55.20 41.24 40.28
BLV [77] 46.76 79.96 58.96 42.67 81.81 93.84 71.83 80.05 42.17 56.83 41.52 40.06
AUCSeg (Ours) 49.20 80.59 59.45 45.52 82.71 93.91 72.72 81.67 42.73 56.95 41.93 40.72
metrics, AUCSeg does not achieve the best performance. Even in these cases, AUCSeg still secures the
runner-up status. We analyze the performance trade-off between head and tail classes in Appendix H.
These experimental results underscore the effectiveness of our proposed method. We further present
the results for each tail class in Appendix G.1, and analyze the reasons for the varying performance
improvements of tail classes across the three datasets in Appendix G.2.
Figure 4 displays the qualitative results on the Cityscapes validation set. Benefiting from our proposed
AUC and T-Memory Bank techniques, AUCSeg segments objects in tail classes more accurately. It
correctly distinguishes between bicycles and motorcycles and successfully identifies distant traffic
lights, which other methods overlook. More qualitative results can be found in Appendix G.3.
5.3 Backbone Extension
Table 2: Results of AUCSeg using different back-
bones in terms of mIoU (%).
Backbone AUCSeg Overall Tail
DeepLabV3+✕ 31.95 26.01
✓ 36.13 31.10
EMANet✕ 32.83 27.36
✓ 36.32 31.39
OCRNet✕ 29.64 23.72
✓ 34.82 29.75
ISANet✕ 29.53 23.64
✓ 35.07 30.13In Section 5.1, we select the current SOTA Seg-
NeXt as the backbone. Nevertheless, AUC-
Seg can also adapt to other backbones, con-
sistently delivering effective results. Table 2
presents the experimental results of AUCSeg
when using DeepLabV3+, EMANet, OCRNet,
and ISANet as backbones. These results re-
veal significant improvements in both overall
mIoU and tail classes mIoU with AUCSeg. No-
tably, on ISANet, the increases are 5.54% and
6.49%. Moreover, AUCSeg enhances perfor-
mance across various model sizes and different
pixel-level long-tail problems, as detailed in Ap-
pendix G.4 and Appendix G.4. This demon-
strates the superiority of our proposed AUC-
Seg for long-tailed semantic segmentation.
5.4 Ablation studies
We perform several ablation studies to test the effectiveness of different modules and hyperparameters.
All experiments are conducted on the ADE20K validation set.
9(a) Input
 (b) Ground Truth
 (c) DeepLabV3+
 (d) SegNeXt
 (e) AUCSeg (Ours)
Figure 4: Qualitative results on the Cityscapes val set. Red rectangles highlight and magnify the
image details in the lower left corner.
OverallOverallOverallOverallOverallOverall
TailTailTailTailTailTail
BestBestBestBestBestBest
404244464850
13581020
SMmIoU (%)
(a) Memory Size ( SM).
404244464850
0.010.040.050.070.10.15
RSmIoU (%) (b) Sample Ratio ( RS).
404244464850
0.30.40.50.60.71
RRmIoU (%) (c) Resize Ratio ( RR).
404244464850
1/61/51/41/31/21
λmIoU (%) (d) The weight λ.
Figure 5: Ablation Study on Hyper-Parameters.
Table 3: Ablation study on the effectiveness of
AUC Optimization and T-Memory Bank (TMB) in
terms of mIoU (%).
Model AUC TMB Overall Tail
SegNeXt 47.45 43.28
SegNeXt+AUC ✓ 48.46 44.70
SegNeXt+TMB ✓ 47.86 43.86
AUCSeg ✓ ✓ 49.20 45.52The Effectiveness of AUC Optimization and
T-Memory Bank. Table 3 details our step-by-
step ablation study on the AUC and T-Memory
Bank components of AUCSeg. Compared to
the baseline SegNeXt, AUC enhances perfor-
mance by +1.01% and+1.42% in overall and
tail classes. The T-Memory Bank further ad-
dresses the imbalance issue, yielding improve-
ments of +1.75% overall and +2.24% in tail
classes. Our results also show that employing
T-Memory Bank without AUC yields no signif-
icant improvements, underscoring the necessity of AUC Loss. More ablation experiments on the
effectiveness of AUC optimization and TMB are deferred to Appendix G.6, G.7, G.8, and G.9.
Ablation Study on Hyper-Parameters. Figure 5a ablates the maximum number of images stored
per class in the Memory Branch, referred to as Memory Size ( SM). For the ADE20K dataset, optimal
performance occurs when SM= 5, and performance shows little sensitivity to changes in SM.
Figure 5b ablates the Sample Ratio ( RS), the fraction of classes sampled from the Memory Branch
relative to the total number of missing tail classes. Figure 5c ablates the Resize Ratio ( RR), the
scaling factor for the sampled pixels. For ADE20K, the best result is obtained when RS= 0.05and
RR= 0.4. A potential reason for their small value is that the original image is overwritten when RS
andRRare too large, resulting in poor training performance. Figure 5d ablates the weight λforℓce
andℓauc, with λ=1
4providing slightly better results. The influence of λon performance is minimal.
Detailed results from this hyper-parameter ablation study are available in Appendix G.10 and G.11.
6 Conclusion
This paper explores AUC optimization in the context of PLSS tasks. To begin with, we theoretically
study the generalization performance of AUC-oriented PLSS by overcoming the two-layer coupling
issue across the loss terms of AUCSeg therein. The corresponding results show that applying AUC
optimization to PLSS could also enjoy a promising performance. Subsequently, we propose a novel T-
Memory Bank to reduce the significant memory demand for the mini-batch optimization of AUCSeg.
Finally, comprehensive experiments suggest the effectiveness of our proposed AUCSeg.
10Acknowledgments
This work was supported in part by the National Key R&D Program of China under Grant
2018AAA0102000, in part by National Natural Science Foundation of China: 62236008, U21B2038,
U23B2051, 61931008, 62122075, 92370102, 62406305, 62471013 and 62476068, in part by Youth
Innovation Promotion Association CAS, in part by the Strategic Priority Research Program of the
Chinese Academy of Sciences, Grant No. XDB0680000, in part by the Innovation Funding of ICT,
CAS under Grant No.E000000, in part by the Postdoctoral Fellowship Program of CPSF under Grant
GZB20240729 and GZB20230732, and in part by the China Postdoctoral Science Foundation under
Grant No.2023M743441.
References
[1]Shivani Agarwal. Surrogate regret bounds for bipartite ranking via strongly proper losses. The Journal of
Machine Learning Research , 15(1):1653–1674, 2014.
[2]Shivani Agarwal, Thore Graepel, Ralf Herbrich, Sariel Har-Peled, Dan Roth, and Michael I Jordan.
Generalization bounds for the area under the roc curve. Journal of Machine Learning Research , 6(4),
2005.
[3]Massih-Reza Amini and Nicolas Usunier. Learning with partially labeled and interdependent data .
Springer, 2015.
[4]Shilong Bao, Qianqian Xu, Ke Ma, Zhiyong Yang, Xiaochun Cao, and Qingming Huang. Collaborative
preference embedding against sparse labels. In ACMMM , pages 2079–2087, 2019.
[5]Shilong Bao, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, and Qingming Huang. Rethinking collaborative
metric learning: Toward an efficient alternative without negative sampling. IEEE TPAMI , 45(1):1017–
1035, 2022.
[6]Shilong Bao, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang. The minority
matters: A diversity-promoting collaborative metric learning algorithm. In NeurIPS , pages 2451–2464,
2022.
[7]Shilong Bao, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang. Improved
diversity-promoting collaborative metric learning for recommendation. IEEE TPAMI , 2024.
[8]Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. JMLR , 3(Nov):463–482, 2002.
[9]Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In
CVPR , pages 1209–1218, 2018.
[10] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets
with label-distribution-aware margin loss. In NeurIPS , 2019.
[11] Robin Chan, Matthias Rottmann, Fabian Hüger, Peter Schlicht, and Hanno Gottschalk. Application of
decision rules for handling class imbalance in semantic segmentation. arXiv preprint arXiv:1901.08394 ,
2019.
[12] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
IEEE TPAMI , 40(4):834–848, 2017.
[13] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolu-
tion for semantic image segmentation. arXiv preprint arXiv:1706.05587 , 2017.
[14] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-
decoder with atrous separable convolution for semantic image segmentation. In ECCV , pages 801–818,
2018.
[15] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-
attention mask transformer for universal image segmentation. In CVPR , pages 1290–1299, 2022.
[16] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A general framework for consistent structured
prediction with implicit loss embeddings. JMLR , 21(98):1–67, 2020.
11[17] Stéphan Clémençon, Gábor Lugosi, and Nicolas Vayatis. Ranking and empirical minimization of
u-statistics. The Annals of Statistics , 2008.
[18] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and
benchmark. https://github.com/open-mmlab/mmsegmentation , 2020.
[19] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Be-
nenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene
understanding. In CVPR , pages 3213–3223, 2016.
[20] Corinna Cortes and Mehryar Mohri. Auc optimization vs. error rate minimization. In NeurIPS , 2003.
[21] Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the American
mathematical society , 39(1):1–49, 2002.
[22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR , pages 248–255, 2009.
[23] Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Thalmann, and Gang Wang. Boundary-aware
feature propagation for scene segmentation. In ICCV , pages 6819–6829, 2019.
[24] Rongsheng Dong, Xiaoquan Pan, and Fengying Li. Denseu-net-based semantic segmentation of small
objects in urban remote sensing images. IEEE Access , 7:65347–65356, 2019.
[25] Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, and Xiaolin Wei.
Rethinking bisenet for real-time semantic segmentation. In CVPR , pages 9716–9725, 2021.
[26] Tom Fawcett. An introduction to roc analysis. Pattern recognition letters , 27(8):861–874, 2006.
[27] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention
network for scene segmentation. In CVPR , pages 3146–3154, 2019.
[28] Wei Gao, Rong Jin, Shenghuo Zhu, and Zhi-Hua Zhou. One-pass auc optimization. In ICML , pages
906–914, 2013.
[29] Wei Gao and Zhi-Hua Zhou. On the consistency of auc pairwise optimization. arXiv preprint
arXiv:1208.0645 , 2012.
[30] Damian Gola, Jeannette Erdmann, Bertram Müller-Myhsok, Heribert Schunkert, and Inke R König.
Polygenic risk scores outperform machine learning methods in predicting coronary artery disease status.
Genetic epidemiology , 44(2):125–138, 2020.
[31] Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, and Shi-Min Hu. Beyond self-attention: External
attention using two linear layers for visual tasks. IEEE TPAMI , 45(5):5436–5447, 2022.
[32] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu. Segnext:
Rethinking convolutional attention design for semantic segmentation. In NeurIPS , pages 1140–1156,
2022.
[33] Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-
Hai Zhang, Ralph R Martin, Ming-Ming Cheng, and Shi-Min Hu. Attention mechanisms in computer
vision: A survey. Computational visual media , 8(3):331–368, 2022.
[34] David J Hand and Robert J Till. A simple generalisation of the area under the roc curve for multiple class
classification problems. Machine learning , 45:171–186, 2001.
[35] James A Hanley and Barbara J McNeil. The meaning and use of the area under a receiver operating
characteristic (roc) curve. Radiology , 143(1):29–36, 1982.
[36] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
visual representation learning. In CVPR , pages 9729–9738, 2020.
[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
InCVPR , pages 770–778, 2016.
[38] Alan Herschtal and Bhavani Raskutti. Optimising area under the roc curve using gradient descent. In
ICML , pages 385–392, 2004.
[39] Md Sazzad Hossain, John M Betts, and Andrew P Paplinski. Dual focal loss to address class imbalance
in semantic segmentation. Neurocomputing , 462:69–87, 2021.
12[40] Wenzheng Hou, Qianqian Xu, Zhiyong Yang, Shilong Bao, Yuan He, and Qingming Huang. Adauc:
End-to-end adversarial auc optimization against long-tail problems. In ICML , pages 8903–8925, 2022.
[41] Hanzhe Hu, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui, and Liwei Wang. Semi-supervised semantic
segmentation via adaptive equalization learning. In NeurIPS , pages 22106–22118, 2021.
[42] Cong Hua, Qianqian Xu, Shilong Bao, Zhiyong Yang, and Qingming Huang. Reconboost: Boosting can
achieve modality reconcilement. In ICML , pages 19573–19597, 2024.
[43] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:
Criss-cross attention for semantic segmentation. In ICCV , pages 603–612, 2019.
[44] Svante Janson. Large deviations for sums of partly dependent random variables. Random Structures &
Algorithms , 24:234–248, 2004.
[45] Zhenchao Jin, Xiaowei Hu, Lingting Zhu, Luchuan Song, Li Yuan, and Lequan Yu. Idrnet: Intervention-
driven relation network for semantic segmentation. In NeurIPS , 2024.
[46] Thorsten Joachims. A support vector method for multivariate performance measures. In ICML , pages
377–384, 2005.
[47] Thorsten Joachims. Training linear svms in linear time. In SIGKDD , pages 217–226, 2006.
[48] Michael Kampffmeyer, Arnt-Borre Salberg, and Robert Jenssen. Semantic segmentation of small objects
and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks.
InCVPR workshops , pages 1–9, 2016.
[49] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label-
imbalanced and group-sensitive classification under overparameterization. NeurIPS , pages 18970–18983,
2021.
[50] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick. Pointrend: Image segmentation as
rendering. In CVPR , pages 9799–9808, 2020.
[51] Fahad Lateef and Yassine Ruichek. Survey on semantic segmentation using deep learning techniques.
Neurocomputing , 338:321–348, 2019.
[52] Antoine Ledent, Yunwen Lei, and Marius Kloft. Improved generalisation bounds for deep learning
through l∞covering numbers. arXiv preprint arXiv:1905.12430 , 2019.
[53] Feiran Li, Qianqian Xu, Shilong Bao, Zhiyong Yang, Runmin Cong, Xiaochun Cao, and Qingming
Huang. Size-invariance matters: Rethinking metrics and losses for imbalanced multi-object salient object
detection. In ICML , pages 28989–29021, 2024.
[54] Shaojie Li and Yong Liu. Towards sharper generalization bounds for structured prediction. In NeurIPS ,
pages 26844–26857, 2021.
[55] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and Hong Liu. Expectation-
maximization attention networks for semantic segmentation. In ICCV , pages 9167–9176, 2019.
[56] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object
detection. In ICCV , pages 2980–2988, 2017.
[57] Mingrui Liu, Zhuoning Yuan, Yiming Ying, and Tianbao Yang. Stochastic auc maximization with deep
neural networks. In ICLR , 2020.
[58] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmen-
tation. In CVPR , pages 3431–3440, 2015.
[59] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. In ICLR , 2018.
[60] Xiangrui Meng. Scalable simple random sampling and stratified sampling. In ICML , pages 531–539,
2013.
[61] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and
Sanjiv Kumar. Long-tail learning via logit adjustment. In ICLR , 2020.
[62] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning . MIT
press, 2018.
13[63] Michael Natole, Yiming Ying, and Siwei Lyu. Stochastic proximal algorithms for auc maximization. In
ICML , pages 3710–3719, 2018.
[64] Foster Provost and Pedro Domingos. Tree induction for probability-based ranking. Machine learning ,
52:199–215, 2003.
[65] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In
ICCV , pages 12179–12188, 2021.
[66] Mina Rezaei, Haojin Yang, and Christoph Meinel. Recurrent generative adversarial network for learning
imbalanced medical image semantic segmentation. Multimedia Tools and Applications , 79(21-22):15329–
15348, 2020.
[67] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In Medical Image Computing and Computer-Assisted Intervention , pages 234–241,
2015.
[68] Huiyang Shao, Qianqian Xu, Zhiyong Yang, Shilong Bao, and Qingming Huang. Asymptotically
unbiased instance-wise regularized partial auc optimization: Theory and algorithm. In NeurIPS , pages
38667–38679, 2022.
[69] Huiyang Shao, Qianqian Xu, Zhiyong Yang, Peisong Wen, Gao Peifeng, and Qingming Huang. Weighted
roc curve in cost space: Extending auc to cost-sensitive learning. In NeurIPS , pages 17357–17368, 2024.
[70] Ravindra Singh, Naurang Singh Mangat, Ravindra Singh, and Naurang Singh Mangat. Stratified sampling.
Elements of survey sampling , pages 102–144, 1996.
[71] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for
human pose estimation. In CVPR , pages 5693–5703, 2019.
[72] Nicolas Usunier, Massih R Amini, and Patrick Gallinari. Generalization error bounds for classifiers
trained with interdependent data. In NeurIPS , 2005.
[73] Nicolas Usunier, Massih-Reza Amini, and Patrick Gallinari. A data-dependent generalisation error bound
for the auc. In ICML Workshop , 2005.
[74] Haonan Wang, Qixiang Zhang, Yi Li, and Xiaomeng Li. Allspark: Reborn labeled features from unlabeled
in transformer for semi-supervised semantic segmentation. In CVPR , 2024.
[75] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu,
Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition.
IEEE TPAMI , 43(10):3349–3364, 2020.
[76] Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, and Yunhe Wang. Multimodal
token fusion for vision transformers. In CVPR , pages 12186–12195, 2022.
[77] Yuchao Wang, Jingjing Fei, Haochen Wang, Wei Li, Tianpeng Bao, Liwei Wu, Rui Zhao, and Yujun Shen.
Balancing logit variation for long-tailed semantic segmentation. In CVPR , pages 19561–19573, 2023.
[78] Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang. A unified
generalization analysis of re-weighting and logit-adjustment for imbalanced learning. In NeurIPS , pages
48417–48430, 2023.
[79] Andrew Westcott, Dante PI Capaldi, David G McCormack, Aaron D Ward, Aaron Fenster, and Grace
Parraga. Chronic obstructive pulmonary disease: thoracic ct texture analysis and machine learning to
predict pulmonary ventilation. Radiology , 293(3):676–684, 2019.
[80] Huikai Wu, Junge Zhang, Kaiqi Huang, Kongming Liang, and Yizhou Yu. Fastfcn: Rethinking dilated
convolution in the backbone for semantic segmentation. arXiv preprint arXiv:1903.11816 , 2019.
[81] Yu-Huan Wu, Yun Liu, Le Zhang, Ming-Ming Cheng, and Bo Ren. Edn: Salient object detection via
extremely-downsampled network. IEEE Tip , 31:3125–3136, 2022.
[82] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-
parametric instance discrimination. In CVPR , pages 3733–3742, 2018.
[83] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:
Simple and efficient design for semantic segmentation with transformers. In NeurIPS , pages 12077–12090,
2021.
14[84] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-
tions for deep neural networks. In CVPR , pages 1492–1500, 2017.
[85] Tianbao Yang and Yiming Ying. Auc maximization in the era of big data and ai: A survey. ACM
computing surveys , 55(8):1–37, 2022.
[86] Zhiyong Yang, Qianqian Xu, Shilong Bao, Xiaochun Cao, and Qingming Huang. Learning with multiclass
auc: Theory and algorithms. IEEE TPAMI , 44(11):7747–7763, 2021.
[87] Zhiyong Yang, Qianqian Xu, Shilong Bao, Yuan He, Xiaochun Cao, and Qingming Huang. When all
we need is a piece of the pie: A generic framework for optimizing two-way partial auc. In ICML , pages
11820–11829, 2021.
[88] Zhiyong Yang, Qianqian Xu, Shilong Bao, Yuan He, Xiaochun Cao, and Qingming Huang. Optimizing
two-way partial auc with an end-to-end framework. IEEE TPAMI , 2022.
[89] Zhiyong Yang, Qianqian Xu, Shilong Bao, Peisong Wen, Yuan He, Xiaochun Cao, and Qingming Huang.
Auc-oriented domain adaptation: From theory to algorithm. IEEE TPAMI , 2023.
[90] Zhiyong Yang, Qianqian Xu, Xiaochun Cao, and Qingming Huang. Task-feature collaborative learning
with application to personalized attribute prediction. IEEE TPAMI , 43(11):4094–4110, 2020.
[91] Zhiyong Yang, Qianqian Xu, Wenzheng Hou, Shilong Bao, Yuan He, Xiaochun Cao, and Qingming
Huang. Revisiting auc-oriented adversarial training with loss-agnostic perturbations. IEEE TPAMI , 2023.
[92] Zhiyong Yang, Qianqian Xu, Zitai Wang, Sicong Li, Boyu Han, Shilong Bao, Xiaochun Cao, and Qing-
ming Huang. Harnessing hierarchical label distribution variations in test agnostic long-tail recognition.
InICML , pages 56624–56664, 2024.
[93] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled
non-local neural networks. In ECCV , pages 191–207, 2020.
[94] Yiming Ying, Longyin Wen, and Siwei Lyu. Stochastic online auc maximization. In NeurIPS , 2016.
[95] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, and Nong Sang. Bisenet v2:
Bilateral network with guided aggregation for real-time semantic segmentation. IJCV , 129:3051–3068,
2021.
[96] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint
arXiv:1511.07122 , 2015.
[97] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmenta-
tion. In ECCV , pages 173–190, 2020.
[98] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Ocnet: Object
context for semantic segmentation. IJCV , 129(8):1–24, 2021.
[99] Zhuoning Yuan, Zhishuai Guo, Nitesh Chawla, and Tianbao Yang. Compositional training for end-to-end
deep auc maximization. In ICLR , 2021.
[100] Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Large-scale robust deep auc maximization: A
new surrogate loss and empirical studies on medical image classification. In ICCV , pages 3040–3049,
2021.
[101] Zhuoning Yuan, Dixian Zhu, Zi-Hao Qiu, Gang Li, Xuanhui Wang, and Tianbao Yang. Libauc: A deep
learning library for x-risk optimization. In KDD , pages 5487–5499, 2023.
[102] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit
Agrawal. Context encoding for semantic segmentation. In CVPR , pages 7151–7160, 2018.
[103] Rui-Ray Zhang and Massih-Reza Amini. Generalization bounds for learning under graph-dependence: A
survey. Machine Learning , pages 1–31, 2024.
[104] Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and Jian Sun. Distribution alignment: A unified
framework for long-tail visual recognition. In CVPR , pages 2361–2370, 2021.
[105] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic
segmentation on high-resolution images. In ECCV , pages 405–420, 2018.
15[106] Peilin ZHAO, Steven CH HOI, Rong JIN, and Tianbo YANG. Online auc maximization. In ICML , pages
233–240, 2011.
[107] Mingmin Zhen, Jinglu Wang, Lei Zhou, Shiwei Li, Tianwei Shen, Jiaxiang Shang, Tian Fang, and Long
Quan. Joint semantic segmentation and boundary detection using iterative pyramid contexts. In CVPR ,
pages 13666–13675, 2020.
[108] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng
Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence
perspective with transformers. In CVPR , pages 6881–6890, 2021.
[109] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing
through ade20k dataset. In CVPR , pages 633–641, 2017.
[110] Ding-Xuan Zhou. The covering number in learning theory. Journal of Complexity , 18:739–767, 2002.
16Contents
A Symbol Definitions 18
B Generalization Bounds and Its Proofs 18
B.1 Preliminary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.2 Key Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.3 Proof of the Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C Proof for Propositions of Tail-class Memory Bank 26
D Details of T-Memory Bank Algorithm 27
E More Discussions about T-Memory Bank 27
E.1 Discussion on the Improved Version of Stratified Sampling . . . . . . . . . . . . . 27
E.2 Discussion on Why the T-Memory Bank Works . . . . . . . . . . . . . . . . . . . 28
E.3 Discussion on AUC and Contrastive Learning from the Perspective of the Loss Function 29
F Additional Experimental Settings 30
F.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
F.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
F.3 Competitors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
G Additional Experimental Results 33
G.1 Per-tail-class Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
G.2 Performance Differences Across Different Datasets . . . . . . . . . . . . . . . . . 33
G.3 More Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
G.4 Backbone Extension of Different Model Sizes . . . . . . . . . . . . . . . . . . . . 35
G.5 Backbone Extension of Different Pixel-level Long-tail Problems . . . . . . . . . . 35
G.6 Spatial Resource Consumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
G.7 Results of Different AUC Surrogate Losses and Calculation Methods . . . . . . . . 36
G.8 Results of the Comparison Between PMB and TMB . . . . . . . . . . . . . . . . . 36
G.9 Results of Different Memory Bank Update Strategies . . . . . . . . . . . . . . . . 37
G.10 Detailed Results of the Ablation Study on Hyper-Parameters . . . . . . . . . . . . 38
G.11 Results of the Ablation Study on the Impact of Batch Size . . . . . . . . . . . . . 38
H More Discussions About AUCSeg 39
I Broader Impact 39
17A Symbol Definitions
In this section, Table 4 includes a summary of key notations and descriptions in this work.
Table 4: A summary of key notations and descriptions in this work.
Notations Descriptions
D Training dataset.
H/W The height/width of the images in dataset D.
N The number of image samples in dataset D.
K The total number of classes in dataset D.
(Xi,Yi) Thei-th sample and its ground-truth in dataset D, where i∈[1, n],Xi∈RH×W×3,Yi∈RH×W×K.
fθ The semantic segmentation model, where θis its parameter.
fe
θ/fd
θ The encoder/decoder in the semantic segmentation model fθ.
ˆYiThe prediction of model fθ, where ˆYi=fθ(Xi)∈RH×W×K.
(Xi
u,v,Yi
u,v/ˆYi
u,v)The sample and its ground-truth/prediction of the (u, v)-th pixel of the i-th image.
Yic
u,v/ˆYic
u,v The ground-truth one-hot encoding/prediction of pixel (Xi
u,v,Yi
u,v)in class c.
DpThe set of all pixels in dataset D.
(Xp
j,Yp
j) Thej-th element in Dp, where j∈[1, n×(H−1)×(W−1)].
f(c)
θThe continuous score function supporting class c.
Nc The set of pixels with label cin the set Dp, where Nc={Xp
k|Yp
k=c}.
|A| The number of elements in set A.
µ The Lipschitz constant.
Ω(·) The lower bound.
Nc/Tc The set of pixels with label cin the original image/those pixels stored in the T-Memory Bank.
˜XpThe sample after replacing some pixels with tail classes pixels from the T-Memory Bank.
nt The number of tail classes.
Ct The labels of tail classes, where Ct={ci}nt
i=1.
M The Memory Branch, where M={Mc1, . . .Mcnt}.
SM The memory size.
RS The sample ratio.
RR The resize ratio.
Tmax The max iteration.
Nb The batch size.
B Generalization Bounds and Its Proofs
B.1 Preliminary Lemmas
Lemma 1 (Jensen’s Inequality) .IfXis a random variable and φis a convex function, then
φ(E[X])≤E[φ(X)]. (9)
Assumption 1. Assume that ℓisµ-Lipschitz continuous, that is
|ℓ(t)−ℓ(s)| ≤µ|t−s|. (10)
Assumption 1 is a pretty mild assumption. The square loss ℓsq(x) = (1 −x)2satisfies Assumption
1.
Lemma 2. The empirical Rademacher complexity of function gwith respect to the predictor fis
defined as:
ˆRF(g) =Eσ[sup
f∈F1
NNX
i=1σig(f(i))]. (11)
whereF ⊆ { f:X →RK}is a family of predictors, and Nrefers to the size of the dataset, and σis
are independent uniform random variables taking values in {−1,+1}. The random variables σiare
called Rademacher variables.
18Lemma 3. LetE[g]and ˆE[g]represent the expected risk and empirical risk, and F ⊆
f:X →RK	
. Then with probability at least 1−δover the draw of an i.i.d. sample S of
sizen, the generalization bound holds:
sup
f∈F
E[g(f)]−ˆE[g(f)]
≤2ˆRF(g) + 3s
log2
δ
2n. (12)
Definition 1 (Fractional Independent Vertex Cover, and Fractional Chromatic Number [ 103]).Let a
graph be G= (V, E).
(1) A fractional vertex cover ofGis a family {(Fj, ωj)}jof pairs (Fj, ωj), where Fj⊆V(G),
ωj∈(0,1], andP
j:v∈Fjωj= 1,∀v∈V(G).
(2) An independent set ofGis a set of vertices in Gwith no two adjacent. Let I(G)denote the set
of independent sets of G.
(3) A fractional vertex cover is a fractional independent vertex cover {(Ij, ωj)}jofGif∀j,
Ij∈ I(G).
(4) A fractional coloring of a graph Gis a mapping g:I(G)→(0,1]such thatP
I∈I(G):v∈Ig(I)≥
1,∀v∈V(G). The fractional chromatic number χf(G)is the minimum of the valueP
I∈I(G)g(I)
over fractional colorings of G.
Notably, the minimum ofP
jωjover all fractional independent vertex covers {(Ij, ωj)}jofGis the
fractional chromatic number χf(G).
Definition 2 (Dependency Graph [ 44]).An (undirected) graph G= (V, E)is called a dependency
graph associated with a random vector (or random variables) x= (x1, . . . , x m)if
(1)V(G) = [m].
(2) For all disjoint vertex sets I, J⊆[m], ifI, Jare not adjacent in G, then random variables {xi}i∈I
and{xj}j∈Jare independent.
A useful result is Janson’s decomposition property [ 44], which combines the concept of dependency
graphs with fractional independent vertex covers. The property states that if interdependent random
variables (xi)i∈[m]is associated with a dependency graph Gwith a fractional independent vertex
cover (Ij, ωj)j∈[J], then, the sum of the interdependent variables, can be equivalently decomposed
into a weighted sum of sums of independent variables:
mX
i=1xi=mX
i=1JX
j=1ωj1i∈Ijxi=JX
j=1ωjX
i∈Ijxi. (13)
B.2 Key Lemmas
Lemma 4. LetDrepresents the training set, and +/−respectively denote the categories c/c′. The
function f(+)
i,jrepresents the score function for the (i, j)-th pixel in category c. For a set A,|A|
denotes the number of elements in the set. We have:
EDh
ℓc,c′
auci
=ED
X
xp
m∈NcX
xp
n∈Nc′1
|Nc||Nc′|ℓc,c′,m,n
sqc, c′

=E
X1∼Dh
˜ℓinner
+,−i
+E
X1,X2∼Dh
˜ℓinter
+,−i
.(14)
19where,
˜ℓinner
+,−=|D|n 
X+
1
n 
X−
1
|N+||N−|X
(i1,j1)∈X+
1
(i2,j2)∈X−
1ℓsq
˜f(+)
i1,j1,˜f(+)
i2,j2,X,Y
|D|n 
X+
1
n 
X−
1,
˜ℓinter
+,−=|D|(|D| − 1)n 
X+
1
n 
X−
2
|N+||N−|X
(i1,j1)∈X+
1
(i2,j2)∈X−
2ℓsq
˜f(+)
i1,j1,˜f(+)
i2,j2,X,Y
|D|(|D| − 1)n 
X+
1
n 
X−
2,
andn(X+)/n(X−)represents the number of positive/negative samples in image X.
Definition 3 (Covering Number [ 110,54]).LetFbe class of real-valued fucntions, defined over a
spaceZandS:={(X1,Y1), . . . , (Xn,Yn)} ∈ Znof cardinality n. For any ϵ >0, the empirical
ℓ∞-norm covering number N(F,|| · ||∞, S, ϵ)w.r.tSis defined as the minimal number mof a
collection of vectors v1, . . . ,vm∈Rnsuch that ( vj
iis the i-th component of the vector vj)
sup
f∈Fmin
j=1,...,mmax
i=1,...,nf(Xi,Yi)−vj
i≤ϵ. (15)
In this case, we call
v1, . . . ,vm	
an(ϵ, ℓ∞)-cover of Fw.r.tS.
Lemma 5. Let P(X) represents the pixels in image X,F =nn
f(+)
i,jo
: (i, j)∈P(X),(X,Y)∈So
andℓ◦ F=n
ℓ◦n
f(+)
i,jo
:n
f(+)
i,jo
∈ Fo
. According to
Definition 3, we have:
N(ℓ◦ F,|| · ||∞, S, ϵ)≤ N (F,|| · ||∞, S, ϵ/ 2µρmax), (16)
where ρmax= max
X|D|n(X+)n(X−)
|N+||N−|.
Proof. For any g=ℓ◦f, we can find ˜g=ℓ◦˜fthat satisfies the following conditions:
||g−˜g||∞,S= max
(X,Y)∈S|g(X,Y)−˜g(X,Y)|
≤max
(X,Y)∈S|D|n(X+)n(X−)
|N+||N−|X
(i1,j1)∈X+
(i2,j2)∈X−1
n(X+)n(X−)h
ℓsq−˜ℓsqi
≤max
(X,Y)∈S|D|n(X+)n(X−)
|N+||N−|X
(i1,j1)∈X+
(i2,j2)∈X−1
n(X+)n(X−)ℓsq−˜ℓsq,(17)
where
ℓsq−˜ℓsq=ℓsq
f(+)
i1,j1, f(+)
i2,j2,X,Y
−ℓsq
˜f(+)
i1,j1,˜f(+)
i2,j2,X,Y
.
According to Assumption 1, we have:
ℓsq
f(+)
i1,j1, f(+)
i2,j2,X,Y
−ℓsq
˜f(+)
i1,j1,˜f(+)
i2,j2,X,Y
≤µ
f(+)
i1,j1(X,Y)−f(+)
i2,j2(X,Y)
−
˜f(+)
i1,j1(X,Y)−˜f(+)
i2,j2(X,Y)
=µ
f(+)
i1,j1(X,Y)−˜f(+)
i1,j1(X,Y)
+
˜f(+)
i2,j2(X,Y)−f(+)
i2,j2(X,Y)
≤µhf(+)
i1,j1(X,Y)−˜f(+)
i1,j1(X,Y)+˜f(+)
i2,j2(X,Y)−f(+)
i2,j2(X,Y)i
=µhf(+)
i1,j1(X,Y)−˜f(+)
i1,j1(X,Y)+f(+)
i2,j2(X,Y)−˜f(+)
i2,j2(X,Y)i
.(18)
20Denote ρ(x)by|D|n(X+)n(X−)
|N+||N−|. Therefore,
||g−˜g||∞,S≤2µmax
(X,Y)∈Smax
(i,j)∈Xρ(x)f(+)
i,j(X,Y)−˜f(+)
i,j(X,Y)
≤2µρmax max
(X,Y)∈Smax
(i,j)∈Xf(+)
i,j(X,Y)−˜f(+)
i,j(X,Y)
= 2µρmax||f−˜f||∞,S,(19)
where ρmax≜max
Xρ(X).
Define aϵ
2µρmax-covering of the class Fwith|| · ||∞norm:
{C1, . . . ,CN},
with
N=N(F,|| · ||∞, S, ϵ/ 2µρmax). (20)
There exists a fCk =nn
fCk
i,jo
: (i, j)∈P(X)o
, such that for any f =nn
f(+)
i,jo
: (i, j)∈P(X)o
∈ Ck∩ F:
max
(i,j)∈P(X)|f(+)
i,j−fCk
i,j| ≤ϵ
2µρmax, (21)
which implies that
max
(i,j)∈P(X)|gf−gfCk| ≤2µρmax·ϵ
2µρmax=ϵ, (22)
where gf=ℓ◦fandgfCk=ℓ◦fCk.
Denote
Cg,i=
gf: max
(i,j)∈P(X)|gf−gfCk| ≤ϵ
, (23)
then{Cg,1, . . . ,Cg,N}realizes an ϵ-covering of ℓ◦f. Hence, the minimum size of the ϵ-covering is
at most N. Mathematically, we then have:
N(ℓ◦ F,|| · ||∞, S, ϵ)≤ N (F,|| · ||∞, S, ϵ/ 2µρmax). (24)
This completed the proof.
Lemma 6 ([52]).LetFbe a real-valued function class taking values in [0,1], and assume that
0∈ F. LetSbe a finite sample of size n. For any 2≤p≤ ∞ , we have the following relationship
between the Rademacher complexity ˆR(F)and the covering number N(F,|| · || p, S, ϵ).
ˆR(F)≤inf
α>0
4α+12√nZ1
αq
logN(F,|| · || p, S, ϵ)dϵ
. (25)
Lemma 7 ([72,3]).Given a sample ˜D={(˜xi,˜yi)}m
i=1where ˜xi∈˜X,˜yi∈˜Yand˜Dis associated
with a dependency graph G, where χf(G)is its fractional chromatic number, and a loss function
L:˜X × ˜Y × ˜F → [0, M], where ˜F={˜f:˜X → R}. Then, for any δ∈(0,1), the following
generalization bound holds with probability at least 1−δ:
∀˜f∈eF, R(˜f)≤bReD(˜f) + 2bR∗
eD(L◦eF) + 3Ms
χf(G)
2mlog2
δ
, (26)
wherebR∗
eD(L◦eF)is the empirical fractional Rademacher complexity of the loss space.
Lemma 8. Given a sample ˜D={(˜xi,˜yi)}2m
i=1where ˜xi∈˜X,˜yi∈˜Yand ˜Dis associated
with a dependency graph G, where χf(G)is its fractional chromatic number, and each fractional
independent vertex cover contains two independent samples, one positive and one negative. Under
these conditions, χf(G)satisfies:
χf(G) = 2 (2 m−1) (27)
21Proof. The calculation of the fractional chromatic number can be transformed into finding how many
groups can be formed where each group contains mordered pairs of positive and negative samples.
In each group, each sample appears only once, and there are no duplicate ordered pairs of positive
and negative samples across all groups.
For example, in a dataset where 2m= 4, there exist 6groups:
Group 1: (1,2),(3,4) Group 2: (2,1),(4,3)
Group 3: (1,3),(2,4) Group 4: (3,1),(4,2)
Group 5: (1,4),(2,3) Group 6: (4,1),(3,2)
Therefore, χf(G) = 6 .
For2msamples, we can extract A2
2mordered pairs of positive and negative samples. Since these
samples have an equal status in the dataset, they appear the same number of times among these A2
2m
pairs.
After selecting the first group from these A2
2mpairs, A2
2m−mpairs of positive and negative samples
remain. The frequency of each sample appearing in these remaining pairs remains equal.
We continue to select the second group, and so on, until the last group. The frequency of each sample
in the remaining pairs still remains equal.
Thus, we can findA2
2m
mgroups, i.e.,χf(G) = 2(2 m−1).
This completed the proof.
B.3 Proof of the Main Result
Restate of Theorem 1 (Generalization Bound for AUCSeg) .LetEDh
ˆLD(f)i
be the population
risk of ˆLD(f). Assume F ⊆ { f:X → RH×W×K}, where HandWrepresent the height and
width of the image, and Krepresents the number of categories, ˆL(i)is the risk over i-th sample, and
isµ-Lipschitz with respect to the l∞norm, ( i.e.∥ˆL(x)−ˆL(y)∥∞≤µ· ∥x−y∥∞). There exists
three constants A >0,B > 0andC > 0, the following generalization bound holds with probability
at least 1−δover a random draw of i.i.d training data (at the image-level):
ˆLD(f)−EDh
ˆLD(f)i≤8
N+ηinner+ηinter√
Np
Alog (2 BµτNk +C)
+ 3 r
1
2N+Kr
1−1
N!s
log4K(K−1)
δ
,
where
ηinner=48µτlnN
N, η inter= 2√
2τ,
τ= 
max
c∈[K]n(c)
max
n(c)
mean!2
,
n(c)
max= max Xn(X(c)),n(c)
mean =PN
i=1n(X(c)
i),N=|D|,k=H×WandX(c)represents the
pixel of class cin image X.
Proof. First, we find that the calculation of pair-wise AUC requires both positive and negative
samples. These two samples can come from the same image or from two different images. Therefore,
22we transform the original problem into two sub-problems:
ˆLD(f)−EDh
ˆLD(f)i
= sup
f∈F
1
K(K−1)X
c,c′ℓc,c′
auc−1
K(K−1)X
c,c′EDh
ℓc,c′
auci

(Lem. 1)
≤1
K(K−1)X
c,c′"
sup
f∈F
ℓc,c′
auc−EDh
ℓc,c′
auci#
(Lem. 4)=1
K(K−1)X
c,c′
sup
f∈F
ℓinner
+,−−EDh
˜ℓinner
+,−i
| {z }
Part 1+ sup
f∈F
ℓinter
+,−−EDh
˜ℓinter
+,−i
| {z }
Part 2
,(28)
where +/−respectively denote the categories c/c′.
For Part 1, we use the complexity measure technique of the covering number.
Assuming logN(F,|| · ||∞, S, ϵ)≤A
ϵ2logB
ϵ· |D| · k+C
, where k=H×Wis the pixel count
in an image. A,BandCrepresent constants. Based on Lemma 5, we have:
logN(ℓ◦ F,|| · ||∞, S, ϵ)≤logN(F,|| · ||∞, S, ϵ/ 2µρmax)
≤4Aµ2ρ2
max
ϵ2log2Bµρ max
ϵ· |D| · k+C
,(29)
ρmax= max
X|D|n(X+)n(X−)
|N+||N−|
=1
|D|·n+
max·n−
max
n+
mean·n−
mean
≤1
|D|·τ
where:
n+
max= max
X∈Xn(X+),
n−
max= max
X∈Xn(X−),
n+
mean =|D|X
i=1n(X+
i),
n−
mean =|D|X
i=1n(X−
i),
andτ=
max
c∈[K]n(c)
max
n(c)
mean2
.
23Denoted by a:= 4Aµ2ρ2
max,b:= 2Bµρ max|D|kandc:=C. Based on Lemma 6 and Lemma A.3
in [54], we have:
ˆR(ℓ◦ F)≤inf
α>0 
4α+12p
|D|Z1
αp
logN(ℓ◦ F,|| · ||∞, S, ϵ)dϵ!
≤inf
α>0 
4α+12p
|D|Z1
αp
alog (b/ϵ+c)
ϵdϵ!
≤4
|D|+12p
|D|Z1
1/|D|p
alog (b|D|+c)
ϵdϵ
=4
|D|+12 ln|D|p
|D|p
alog (b|D|+c).(30)
Substituting this result into Lemma 3, with probability at least 1−δ, we have
sup
f∈F
ℓinner
+,−−EDh
˜ℓinner
+,−i
≤8
|D|+48µτln|D|
|D|1.5p
Alog (2 Bµτ|D|k+C) + 3vuutlog
4K(K−1)
δ
2|D|.(31)
ForPart 2 , we use the complexity measure technique of the fractional chromatic number and covering
number.
According to Lemma 7, calculating the generalization bound only requires knowing the chromatic
complexity. Based on Equation (13), we have
ˆR(ℓ◦ F) =1
|D|(|D| − 1)Eσ
X
j∈[J]wj
sup
f∈FX
(i1,j1)∈Ij
(i2,j2)∈Ijσ1,2
i,jρ(x)ℓsq
˜f(+)
i1,j1,˜f(+)
i2,j2,X,Y

,
(32)
where ρ(x) =|D|(|D|− 1)n(X+)n(X−)
|N+||N−|, and ωjdenotes the weight assigned to the subset in the
fractional vertex cover.
Similar to Part 1, using the covering number we can get
Rj= sup
f∈FX
(i1,j1)∈Ij
(i2,j2)∈Ijσ1,2
i,jρ(x)ℓsq
˜f(+)
i1,j1,˜f(+)
i2,j2,X,Y
≲√cj·mj,
(33)
where we define cjasln2|D| ·A·µ2·ρ2
max·log
2B·µ·ρmax· |D|2·k+C
ascj, and define
mjas|Ij|.
24Assume that the number of images in the dataset is even, i.e.,|D| ≡0 (mod 2) . We have:
ˆR(ℓ◦ F) =1
|D|(|D| − 1)X
j∈[J]wjRj
≲1
|D|(|D| − 1)X
j∈[J]wj√cj·mj
=χf(G)
|D|(|D| − 1)X
j∈[J]wj
χf(G)√cjmj
≤p
χf(G)
|D|(|D| − 1)sX
j∈[J]wjmjcj
=p
χf(G)p
|D|(|D| − 1)vuutP
j∈[J]wjmjcj
|D|(|D| − 1)
(∗)
≲1p
|D|/2vuutP
j∈[J]wjcj
2(|D| − 1),(34)
where (∗)follows the Lemma 8.
The second-order inequality is based on the fact that:
Rj≲√cj·mj, (35)
define ρj
maxas the largest ρin the j-th cluster, and that:
√cj=ρj
max·q
Alog(2Bµρ max|D|2k+C)
≤|D|(|D| − 1)n+
maxn−
max
|N+||N−|q
Alog(2Bµρ max|D|2k+C)
≈n+
max·n−
max
n+
mean·n−
meanq
Alog(2Bµρ max|D|2k+C)
≤τp
Alog(2Bµτ|D|k+C)(36)
where:
n+
max= max
X∈Xn(X+),
n−
max= max
X∈Xn(X−),
n+
mean =|D|X
i=1n(X+
i),
n−
mean =|D|X
i=1n(X−
i).
andτ=
max
c∈[K]n(c)
max
n(c)
mean2
.
Combining Equation (34) and Equation (36), we obtain:
ˆR(ℓ◦ F)≤1p
|D|/2·τp
Alog(2Bµτ|D|k+C). (37)
25Based on Lemma 7 and Lemma 8, it comes:
sup
f∈F
ℓinter
+,−−EDh
˜ℓinter
+,−i
≤2ˆR(ℓ◦ F) + 3Ms
χf(G)
2mlog4K(K−1)
δ
≤2√
2p
|D|·τp
Alog (2 Bµτ|D|k+C) + 3Ks
|D| − 1
|D|log4K(K−1)
δ
.(38)
Therefore, by combining Equation (28),Equation (31) and Equation (38), we can obtain:
ˆLD(f)−EDh
ˆLD(f)i≤8
N+ηinner+ηinter√
Np
Alog (2 BµτNk +C)
+ 3 r
1
2N+Kr
1−1
N!s
log4K(K−1)
δ
,(39)
where ηinner=48µτlnN
N,ηinter= 2√
2τ,N=|D|andk=H×W.
This completed the proof.
C Proof for Propositions of Tail-class Memory Bank
Restate of Proposition 1. Consider a dataset Dthat includes images with Kdifferent pixel categories.
Letpirepresent the probability of observing a pixel with label iin a given image. Randomly select
Bimages from Das training data, where
B= Ω
log(δ/K)
log(1−min
ipi)
.
Then with probability at least 1−δ, for any c∈[K], there exists Xin the training data that contains
pixels of label c.
Proof. Define event Ajas the extraction of Nimages where the pixels of class jappear at least once.
Letpm= min {p1, p2, . . . , p K}, where m∈[K].
The probability that each category appears at least once when randomly selecting Nimages:
P K\
i=1Ai!
= 1−P K[
i=1Ai!
≥1−KX
i=1P 
Ai
= 1−KX
i=1(1−pi)B
≥1−K(1−pm)B(40)
When PKT
i=1Ai
≥1−δ,
δ≥K(1−pm)B(41)
Therefore,
B= Ωlog(δ/K)
log(1−pm)
= Ω
log(δ/K)
log(1−min
ipi)
, (42)
This completed the proof.
26D Details of T-Memory Bank Algorithm
In this section, Algorithm 2 provides a full version of Algorithm 1.
Algorithm 2: AUCSeg Algorithm (Full Version)
Input: Training data D, number of tail classes nt, labels of tail classes Ct={ci}nt
i=1, Memory
Branch M={Mc1, . . .Mcnt}, memory size SM, sample ratio RS, resize ratio RR,
max iteration Tmax, batch size Nb
Output: model parameters θ
1foriter= 1toTmax do
2DB={(Xi,Yi)}Nb
i=1←SampleBatch (D, Nb);
3Cmiss⊆ Ct←MissingTailClasses (DB);
4Cmiss=Ct− Cmiss;
5▷Store Branch
6 forcminCmiss do
7 Divide a picture containing only the cm-th class pixels from DBand name it P;
8 if|Mcm|< SMthen
9 Add the divided image PtoMcm;
10 else
11 Randomly replace an image in Mcmwith the divided image P;
12▷Retrieve Branch
13 nsample =⌈|Cmiss| ×RS⌉;
14 fori= 1tonsample do
15 Randomly choose cmfromCmiss;
16 Remove cminCmiss;
17 if|Mcm| ̸= 0then
18 Sample from Mcm, scale according to RR, paste randomly into DB;
19▷Semantic Segmentation
20 ˆYi←fθ(Xi);
21 Calculate ℓ=˜ℓauc+λℓcewith Equation (2) and Equation (8);
22 Backpropagation updates θ.
E More Discussions about T-Memory Bank
E.1 Discussion on the Improved Version of Stratified Sampling
In this section, we introduce the definition of the improved version of stratified sampling and explain
why it is not applicable to the PLSS task.
The definition of the improved version of stratified sampling. The improved version of stratified
sampling starts by grouping images according to their categories. Due to the multi-label nature,
different categories may contain the same images. After that, stratified sampling is applied to each
category, making sure that even if an image is sampled more than once (as both head and tail), each
mini-batch still includes at least one sample from every category.
Reasons for inapplicability to PLSS task. Conventional stratified sampling can hardly cover all the
involved classes with a small batch size. To ensure coverage, one has to employ a much larger batch
size, which results in a significantly higher computational burden. Although the improved version of
stratified sampling can cover all classes, images from tail classes may appear repeatedly, leading to
overfitting and consequently a degradation in performance. In contrast, our Tail-class Memory Bank
only involves pasting a portion of one image onto another, effectively functioning as an implicit data
augmentation. This approach mitigates the sampling problem without compromising generalization
ability. The following empirical results support our assertion.
27First, we counted the number of images containing pixels from each class in the Cityscapes, ADE20K,
and COCO-Stuff 164K datasets. The results are shown in Table 5, Table 6, and Table 7.
Table 5: The number of images containing pixels from each class in the Cityscapes . Class ID
represents the class number in the original dataset.
Class ID 5 0 2 8 13 1 7 10 11 6 9 18 4 12 3 17 14 15 16
Num 2949 2934 2934 2891 2832 2811 2808 2686 2343 1658 1654 1646 1296 1023 970 513 359 274 142
Table 6: The number of images containing pixels from each class in the ADE20K . Class ID represents
the class number in the original dataset.
Class ID 1 4 3 5 6 2 13 9 16 18 7 23 15 20 21 37 12 11 44
Num 11588 9314 8240 6674 6579 6042 5069 4687 4266 3995 3990 3295 3276 3258 3161 3083 3061 2851 2646
Class ID 83 10 19 88 8 14 17 40 42 25 33 67 136 28 24 126 48 31 29
Num 2508 2421 2148 1987 1825 1791 1690 1447 1437 1404 1385 1308 1281 1229 1191 1191 1181 1172 1132
Class ID 68 135 94 99 58 54 39 43 65 35 149 22 34 139 26 70 27 113 90
Num 1112 1020 992 965 930 880 803 799 792 781 773 702 698 671 667 658 650 622 618
Class ID 86 60 53 103 143 45 87 72 116 137 32 148 82 30 50 111 138 96 84
Num 583 564 561 556 556 549 532 531 530 528 521 504 492 479 468 465 452 451 440
Class ID 150 124 41 38 51 140 66 36 73 46 101 128 109 64 71 76 61 125 47
Num 421 417 411 404 402 397 395 378 369 367 354 347 340 335 330 324 320 319 310
Class ID 98 77 49 133 117 63 134 69 122 75 62 81 130 142 144 119 145 132 57
Num 307 304 287 284 282 275 268 266 266 265 261 247 246 228 217 213 206 201 198
Class ID 95 93 147 56 78 74 59 120 85 91 52 146 100 121 102 131 105 127 141
Num 181 178 178 172 170 144 139 136 135 133 130 126 117 116 108 108 99 97 92
Class ID 55 92 114 108 118 89 79 107 110 80 115 123 106 104 129 112 97
Num 84 83 80 77 73 71 68 66 66 65 59 58 57 52 52 50 41
Table 7: The number of images containing pixels from each class in the COCO-Stuff 164K . Class
ID represents the class number in the original dataset.
Class ID 0 157 145 160 93 84 112 120 161 128 111 153 137 169 155 56 2 60 118
Num 63965 36466 31808 31481 27657 23021 22575 22526 19095 18311 17882 16282 15402 14209 13052 12757 12238 11834 11772
Class ID 101 131 90 99 94 85 130 127 100 41 103 39 86 45 26 109 165 105 143
Num 11303 11137 10546 10163 9886 9849 9522 9521 9475 8910 8893 8261 7176 6782 6744 6672 6642 6618 6598
Class ID 116 106 114 7 13 24 164 73 133 159 147 97 170 123 89 142 71 95 144
Num 6549 6324 6252 6122 5568 5464 5290 5268 5251 5246 5114 5101 5053 4887 4858 4688 4674 4589 4589
Class ID 74 62 139 58 57 16 9 80 43 25 5 32 15 88 59 67 121 6 75
Num 4575 4550 4490 4450 4420 4153 4138 4135 3996 3959 3950 3879 3848 3787 3680 3677 3622 3587 3567
Class ID 3 63 37 36 138 38 61 107 1 44 14 42 117 92 30 8 152 65 4
Num 3500 3498 3485 3476 3397 3384 3353 3259 3241 3217 3200 3173 3169 3129 3082 3023 3016 3007 2982
Class ID 17 53 87 98 69 82 55 135 140 149 108 113 81 35 156 23 115 34 40
Num 2931 2925 2911 2909 2877 2813 2741 2720 2703 2667 2659 2613 2598 2585 2558 2544 2498 2494 2478
Class ID 166 27 28 72 162 136 168 33 29 20 46 110 66 77 134 48 163 158 132
Num 2453 2401 2387 2360 2357 2313 2297 2260 2162 2139 2130 2112 2100 2087 2068 2064 2020 2016 2009
Class ID 146 129 19 22 150 64 11 50 10 83 31 49 68 18 51 47 154 54 125
Num 1998 1986 1962 1916 1828 1822 1732 1725 1711 1676 1652 1597 1536 1522 1509 1487 1486 1411 1405
Class ID 151 126 104 52 96 102 21 76 79 148 12 124 119 141 91 122 70 78 167
Num 1385 1362 1259 1134 1004 1002 959 919 846 749 703 659 559 477 351 256 217 188 121
The results indicate that images containing tail class pixels are very limited. Specifically, in the
ADE20K dataset, there are only 41images in the training set of 20210 images that contain pixels
from the tail class with ID 97. As a result, tail class images are repeatedly sampled when using
stratified sampling, leading to overfitting on such repeated images.
Next, we trained on the ADE20K dataset using the improved version of the stratified sampling
method. The results show that, compared to using the Tail-class Memory Bank, the performance on
tail classes dropped by over 3%due to heavy sample repetition in the batch. However, our T-Memory
Bank, with its random pasting technique, diversifies the backgrounds of the tail classes, enabling the
model to better learn the features of these tail classes.
E.2 Discussion on Why the T-Memory Bank Works
In this section, we discuss why the primary function of the T-Memory Bank is not to enhance the
diversity of tail samples.
As shown in Table 3, using a memory bank does indeed increase the diversity of tail classes as an
implicit form of augmentation (comparing the rows for SegNeXt and SegNeXt+TMB in the table).
However, we cannot rely solely on the bank to fully address the long-tail issue, as the bank’s capacity
28is always limited. This is why we also need to consider the problem from the perspective of the loss
function. We find that the AUC loss focuses only on the ranking loss between positive and negative
samples and is not sensitive to data distribution, fundamentally avoiding the risk of underfitting
caused by insufficient training samples. We believe that the use of the T-Memory Bank is intended
to both facilitate the effectiveness of the AUC loss and enhance the diversity of tail samples.
E.3 Discussion on AUC and Contrastive Learning from the Perspective of the Loss Function
In this section, we compare AUC and contrastive learning from the perspective of loss functions.
Theorem 2 (Comparison between AUC Loss and Contrastive Loss) .Minimizing the weighted
contrastive loss approximately corresponds to minimizing an upper bound of the logistic AUC loss:
X
iwi"
−logef(xi)
ef(xi)+P
j̸=iwjef(xj)#
≥X
iX
j̸=i1
ninj
−log1
1 +ef(xj)−f(xi)
,
where, wj=1/njP
k̸=i1/nkandwi=P
k̸=i1/nk
ni.
Proof. For the AUC loss under the logistic surrogate loss function:
ℓlogistic
auc =ℓlogistic 
f(x+)−f(x−)
=X
iX
j̸=i1
ninj
−log1
1 +ef(xj)−f(xi)
=X
iX
j̸=iwiwj
−log1
1 +ef(xj)−f(xi)
=X
iwiX
j̸=iwjh
−log(ef(xi)) + log( ef(xi)+ef(xj))i
=X
iwi
−log(ef(xi)) +X
j̸=iwjlog(ef(xi)+ef(xj))

≤X
iwi
−log(ef(xi)) + log
X
j̸=iwj(ef(xi)+ef(xj))


=X
iwi
−log(ef(xi)) + log
ef(xi)+X
j̸=iwjef(xj)


=X
iwi"
−logef(xi)
ef(xi)+P
j̸=iwjef(xj)#
where, wj=1/ninjP
k̸=i1/nink=1/njP
k̸=i1/nkandwi=P
k̸=i1/nk
ni.
This completed the proof.
The theorem indicates that minimizing a weighted version of contrastive loss can implicitly optimize
the ovo logistic AUC loss. This paper adopts a more general form of AUC loss, in which various
surrogate loss functions are explored.
29F Additional Experimental Settings
In this section, we make a supplementation to Section 5.1.
F.1 Datasets
We use three datasets in our experiments: Cityscapes, ADE20K, and COCO-Stuff 164K.
Cityscapes [19] is a dataset of urban road traffic scenes, with each image sized at 1024×2048 pixels.
It consists of 5000 images with pixel-level annotations across 19classes. The training, validation,
and testing set numbers are 2975 ,500, and 1525 , respectively.
ADE20K [109] is a benchmark for scene parsing with 150class labels. It includes over 25000 images,
with20210 ,2000 , and 3352 images used for training, validation, and testing, respectively.
COCO-Stuff 164K [9] is a large-scale dataset with 164Kimages. It is finely annotated across 171
classes.
We split the three datasets into head class, middle class, and tail class based on the proportion of
pixels for each class in the training set. Table 8, Table 9, and Table 10 provide the details of these
partitions.
F.2 Implementation Details
Network Architecture. We perform all experiments using mmsegmentation [ 18] on an NVIDIA
3090 GPU. For our model, we use SegNeXt [ 32] as the backbone and pretrain all encoders on the
ImageNet-1K [22] dataset.
Data Augmentation. For Cityscapes, we resize the images to 1024×2048 , randomly crop them
to1024×1024 , and then apply random horizontal flips. For ADE20K and COCO-Stuff 164K, the
resizing is set to 512×2048 , random cropping is done at 512×512, and random horizontal flips are
applied as well.
Training Strategy. We use Adam with Weight Decay (AdamW) [59] optimizer with an initial learning
rate of 6e-5and a weight decay of 0.01. We adopt the ‘poly’ learning rate policy, where the initial
learning rate is multiplied by 1−iter
max _iter. Moreover, a ‘linear’ warmup strategy is employed at
the beginning of training, allowing the learning rate to increase from 1e-6to the initial learning
rate within 1500 iterations. The batch size is set to 2for the Cityscapes dataset and 4for all the
other datasets. The total number of iterations is 160000 on Cityscapes and ADE20K and 80000 on
COCO-Stuff 164K.
Evaluation Metrics. Following the setup outlined by SegNeXt [ 32], we conduct experiments using
themean of Intersection over Union (mIoU) as the evaluation metric on the validation set.
F.3 Competitors
Here we give a more detailed summary of the competitors mentioned in the experiments.
We compared our method with 13recent advancements and 6long-tail approaches in semantic
segmentation. The recent advancements include DeepLabV3+, EncNet, FastFCN, EMANet, DANet,
HRNet, OCRNet, DNLNet, PointRend, BiSeNetV2, ISANet, STDC, and SegNeXt. The long-tail
methods are VS, LA, LDAM, Focal Loss, DisAlign, and BLV , all based on SegNeXt. To ensure
fairness, we re-implement the listed methods using their publicly shared code and test them on
the same hardware.
For the semantic segmentation methods:
DeepLabV3+ [14] combines the advantages of the spatial pyramid pooling module and the encoder-
decoder structure. It explores the Xception model and applies depthwise separable convolution to
both the atrous spatial pyramid pooling and decoder modules, resulting in a faster and more robust
encoder-decoder network.
EncNet [102] enhances semantic segmentation by utilizing a context encoding module that captures
global contextual information to aid in the accurate segmentation of complex scenes.
30Table 8: Cityscapes Dataset Partition Status. The first column represents the head class, the second
column represents the middle class, and the third column represents the tail class.
Label Pixel Ratio Label Pixel Ratio Label Pixel Ratio
road 46.27% fence 0.85% truck 0.24%
building 20.62% person 0.83% bicycle 0.24%
vegetation 13.40% terrain 0.61% bus 0.24%
car 6.62% pole 0.60% train 0.21%
sidewalk 4.42% wall 0.57% traffic light 0.11%
sky 3.63% traffic sign 0.40% rider 0.08%
motorcycle 0.06%
Table 9: ADE20K Dataset Partition Status. The first column represents the head class, the second
column represents the middle class, and the third to sixth columns represent the tail class.
Label Pixel Ratio Label Pixel Ratio Label Pixel Ratio Label Pixel Ratio Label Pixel Ratio Label Pixel Ratio
wall 16.93% grass 1.95% sea 0.59% runway 0.17% streetlight 0.08% bag 0.05%
building 11.56% cabinet 1.95% mirror 0.57% stairway 0.17% airplane 0.08% step 0.05%
sky 9.52% sidewalk 1.80% seat 0.49% river 0.17% dirt 0.08% bicycle 0.04%
floor 6.66% person 1.71% rug 0.49% screen 0.17% television 0.08% food 0.04%
tree 5.21% earth 1.61% field 0.48% bridge 0.16% apparel 0.07% trade 0.04%
ceiling 4.86% door 1.27% armchair 0.48% bookcase 0.16% land 0.07% dishwasher 0.04%
road 4.29% table 1.19% fence 0.35% flower 0.16% bannister 0.07% tank 0.04%
bed 2.47% mountain 1.17% desk 0.34% coffee 0.15% pole 0.07% pot 0.04%
windowpane 2.15% curtain 1.12% wardrobe 0.32% toilet 0.15% bottle 0.07% sculpture 0.04%
chair 1.11% rock 0.32% hill 0.14% stage 0.07% hood 0.04%
plant 1.09% lamp 0.28% book 0.14% ottoman 0.07% vase 0.04%
car 1.07% bathtub 0.26% blind 0.14% escalator 0.07% lake 0.04%
water 0.79% railing 0.25% bench 0.14% van 0.07% screen 0.04%
painting 0.73% base 0.25% palm 0.13% poster 0.07% microwave 0.04%
sofa 0.71% cushion 0.25% countertop 0.13% buffet 0.06% sconce 0.04%
shelf 0.67% box 0.23% kitchen 0.13% ship 0.06% animal 0.04%
house 0.65% column 0.23% stove 0.13% plaything 0.06% tray 0.04%
signboard 0.22% swivel 0.11% barrel 0.06% blanket 0.04%
chest 0.21% computer 0.11% conveyer 0.06% traffic 0.04%
counter 0.20% boat 0.10% fountain 0.06% pier 0.04%
grandstand 0.20% arcade 0.10% swimming 0.06% shower 0.03%
sink 0.20% hovel 0.09% stool 0.06% crt 0.03%
sand 0.20% bus 0.09% canopy 0.06% fan 0.03%
fireplace 0.19% bar 0.09% ball 0.05% ashcan 0.03%
refrigerator 0.19% towel 0.09% waterfall 0.05% bulletin 0.03%
skyscraper 0.19% truck 0.09% washer 0.05% plate 0.03%
path 0.19% light 0.09% oven 0.05% monitor 0.03%
case 0.18% tower 0.08% minibike 0.05% radiator 0.03%
pool 0.18% awning 0.08% basket 0.05% clock 0.02%
stairs 0.18% chandelier 0.08% tent 0.05% glass 0.02%
pillow 0.18% booth 0.08% cradle 0.05% flag 0.02%
Table 10: COCO-Stuff 164K Dataset Partition Status. The first column represents the head class, the
second column represents the middle class, and the third to sixth columns represent the tail class.
Label Pixel Ratio Label Pixel Ratio Label Pixel Ratio Label Pixel Ratio Label Pixel Ratio Label Pixel Ratio
person 9.01% fence 0.94% motorcycle 0.49% shelf 0.28% floor-stone 0.16% carrot 0.07%
sky-other 6.24% ceiling-other 0.93% elephant 0.49% leaves 0.28% bird 0.16% parking meter 0.07%
tree 5.78% wall-tile 0.90% curtain 0.49% gravel 0.28% bottle 0.16% traffic light 0.07%
wall-concrete 4.44% furniture-other 0.89% carpet 0.48% wall-panel 0.27% bicycle 0.16% kite 0.07%
grass 4.23% metal 0.88% cage 0.45% cow 0.27% roof 0.15% napkin 0.07%
dining table 3.11% plant-other 0.80% water-other 0.44% boat 0.27% stone 0.15% skateboard 0.06%
building-other 2.98% cabinet 0.79% dog 0.44% skyscraper 0.26% keyboard 0.14% tennis racket 0.06%
road 2.43% train 0.77% house 0.44% wood 0.26% light 0.14% pillow 0.06%
clouds 2.39% bus 0.77% paper 0.42% cup 0.25% orange 0.14% solid-other 0.06%
sea 2.35% pizza 0.74% refrigerator 0.38% potted plant 0.23% clock 0.13% remote 0.05%
pavement 2.24% ground-other 0.73% plastic 0.38% banner 0.23% fruit 0.13% salad 0.05%
wall-other 2.19% floor-other 0.70% clothes 0.37% hill 0.23% hot dog 0.13% knife 0.04%
snow 2.04% door-stuff 0.67% cake 0.37% cardboard 0.23% bridge 0.12% snowboard 0.04%
playingfield 1.81% floor-tile 0.66% oven 0.37% platform 0.23% surfboard 0.12% scissors 0.04%
dirt 1.43% wall-wood 0.64% laptop 0.37% banana 0.23% blanket 0.12% frisbee 0.03%
table 1.17% chair 0.64% horse 0.36% wall-stone 0.23% fire hydrant 0.11% skis 0.03%
bed 1.03% truck 0.63% bench 0.36% branch 0.22% stop sign 0.11% ceiling-tile 0.03%
window-other 1.02% car 0.61% mirror-stuff 0.36% vegetable 0.21% stairs 0.11% tie 0.03%
sand 1.02% bowl 0.61% tv 0.34% flower 0.20% apple 0.11% fork 0.03%
bush 0.60% airplane 0.34% sheep 0.19% handbag 0.10% mat 0.03%
floor-wood 0.60% giraffe 0.33% straw 0.19% cloth 0.10% spoon 0.02%
cat 0.60% zebra 0.33% bear 0.19% floor-marble 0.10% mouse 0.02%
couch 0.58% teddy bear 0.32% net 0.19% cell phone 0.10% baseball glove 0.02%
textile-other 0.57% toilet 0.32% broccoli 0.18% cupboard 0.10% moss 0.02%
wall-brick 0.55% counter 0.32% donut 0.18% microwave 0.09% toothbrush 0.02%
river 0.55% rock 0.32% sink 0.18% backpack 0.09% baseball bat 0.01%
fog 0.55% suitcase 0.32% book 0.18% wine glass 0.09% sports ball 0.01%
mountain 0.51% desk-stuff 0.31% window-blind 0.18% tent 0.09% waterdrops 0.01%
food-other 0.50% sandwich 0.30% structural-other 0.17% mud 0.08% toaster 0.01%
umbrella 0.30% vase 0.17% railing 0.08% hair drier 0.01%
railroad 0.29% rug 0.17% towel 0.08%
31FastFCN [80] employs a novel joint upsampling module called Joint Pyramid Upsampling, which
transforms the task of extracting high-resolution feature maps into a joint upsampling challenge.
EMANet [55] enhances semantic segmentation by utilizing an EM-based attention mechanism that
iteratively refines feature representations for more accurate segmentation.
DANet [27] improves scene segmentation by integrating both spatial and channel-wise attention
mechanisms to capture rich contextual relationships across features.
HRNet [71] maintains high-resolution representations through the network and progressively adds
lower-resolution subnetworks to enhance the learning of spatial hierarchies, significantly improving
semantic segmentation.
OCRNet [97] enhances semantic segmentation by leveraging object-contextual representations,
which aggregates contextual information around each pixel to improve segmentation accuracy.
DNLNet [93] improves performance on tasks like image classification by disentangling the traditional
non-local operations into two separate streams for capturing spatial and channel dependencies
separately.
PointRend [50] introduces a novel rendering-style algorithm that selectively refines segmentation
predictions at adaptively sampled points, enhancing detail accuracy in image segmentation tasks.
BiSeNetV2 [95] utilizes an efficient architecture with inverted residuals and linear bottlenecks,
enabling high-performance mobile vision applications with significantly reduced computational cost.
ISANet [98] improves semantic segmentation by using a novel interlaced sparse self-attention mech-
anism that efficiently captures long-range dependencies with fewer parameters and computational
overhead.
STDC [25] addresses real-time semantic segmentation by proposing a novel and efficient structure
that removes structural redundancy, reduces dimensions of feature maps gradually, and uses their
aggregation for image representation.
SegNeXt [32] rethinks convolutional attention design for semantic segmentation by introducing an
advanced network architecture, enhancing the model’s ability to focus on relevant features for more
accurate segmentation.
For the long-tail methods:
VS[49] proposes to leverage both multiplicative and additive logit adjustments to address label
imbalance problems.
LA[61] advances the conventional softmax cross-entropy by ensuring Fisher consistency in minimiz-
ing the balanced error.
LDAM [10] improves the performance of tail classes by encouraging larger margins for tail classes.
Focal Loss [56] is a modified cross-entropy loss designed to address class imbalance by focusing
more on hard-to-classify examples, reducing the relative loss for well-classified instances and thus
boosting performance on imbalanced datasets.
DisAlign [104] introduces a unified framework for long-tail visual recognition by aligning feature
distributions across different classes, using a novel distribution alignment technique that adjusts
class-specific thresholds to mitigate the bias towards head classes and enhance recognition of tail
classes.
BLV [77] addresses long-tailed semantic segmentation by dynamically adjusting the learning rates
for the logits of different classes based on their frequency, effectively reducing the performance gap
between head and tail classes.
32G Additional Experimental Results
G.1 Per-tail-class Results
In Figure 6, we present the results for each tail class. We select 7classes with the fewest training
samples from the Cityscapes dataset as tail classes: truck, bicycle, bus, train, traffic light, rider,
and motorcycle. Our method not only achieves the highest overall mIoU but also shows significant
improvements in these tail classes. Specifically, it outperforms the current SOTA method by more
than1%in several tail classes.
Overall Tail Truck Bicycle Bus Train Traffic Light Rider Motorcycle30405060708090100
Classes NamemIoU (%)DeepLabV3+ EMANet STDC SegNeXt AUCSeg (Ours)
Figure 6: Per-tail-class results on Cityscapes val set. The tail class names are listed from left to right
according to the ascending number of training samples in the dataset, with ‘motorcycles’ containing
the fewest.
G.2 Performance Differences Across Different Datasets
The performance gain depends on the degree of imbalance of the underlying dataset. To see this, we
show the pairwise mean imbalance ratio rm(average the imbalance ratio of each class pair).
rm=1
|Ch||Ch|X
a∈ChX
b∈Cha
b
(43)
where Chrepresents the set containing the pixel counts of each head class, and Chdenotes the set
containing the pixel counts of each non-head class. The larger the rmvalue, the more imbalanced the
dataset is.
In Table 11, we compare rmfor ADE20K, Cityscapes, and COCO-Stuff 164K, along with the tail
classes performance improvements of AUCSeg compared to the runner-up method.
Table 11: The comparison of imbalance ratio and tail classes performance improvements on ADE20K,
Cityscapes, and COCO-Stuff 164K.
Dataset ADE20K Cityscapes COCO-Stuff 164K
rm 90.43 80.39 38.17
Tail Classes Improvement 1.21% 0.75% 0.38%
The results suggest that the larger the imbalance degree the larger the improvements of our method.
ADE20K has the largest degree imbalance, therefore gaining the most significant improvement.
33G.3 More Qualitative Results
Here we present more qualitative results on the Cityscapes, ADE20K, and COCO-Stuff 164K
validation sets.
Input
 Ground Truth
 DeepLabV3+
 SegNeXt
 AUCSeg (Ours)
Input
 Ground Truth
 DeepLabV3+
 SegNeXt
 AUCSeg (Ours)
Input
 Ground Truth
 DeepLabV3+
 SegNeXt
 AUCSeg (Ours)
Figure 7: More qualitative results on the Cityscapes ,ADE20K andCOCO-Stuff 164K val set. Red
rectangles highlight and magnify the details of the image.
34G.4 Backbone Extension of Different Model Sizes
In this paper, we use the large version of SegNeXt because of its outstanding performance. We also
provide the results for the tiny, small, and base versions of SegNeXt, as shown in Table 12. The
experiments indicate that AUCSeg achieves better performance under any model size.
Table 12: Results of AUCSeg on different model sizes of SegNeXt in terms of mIoU (%).
Backbone AUCSeg Overall Tail
Tiny✕ 38.73 33.96
✓ 39.00 34.52
Small✕ 43.25 38.90
✓ 43.29 39.18
Base✕ 45.45 41.33
✓ 46.37 42.49
Large✕ 47.45 43.28
✓ 49.20 45.52
G.5 Backbone Extension of Different Pixel-level Long-tail Problems
Apart from semantic segmentation, salient object detection is also a pixel-level task. In salient object
detection, the salient objects often exhibit a long-tailed distribution. We apply AUCSeg to this task,
using the latest SOTA method SI-SOD-EDN [ 81,53] as the backbone. The results are shown in
Table 13.
Table 13: Experimental results of AUCSeg in the salient object detection.
DatasetECSSD HKU-IS PASCAL-S
MAE↓ Fβ
m↑ Em↑ MAE↓ Fβ
m↑ Em↑ MAE↓ Fβ
m↑ Em↑
SI-SOD-EDN 0.0358 0.9084 0.9375 0.0287 0.8986 0.9442 0.0644 0.826 0.8859
+AUCSeg 0.0349 0.9087 0.9377 0.0278 0.8992 0.9455 0.0629 0.8281 0.8875
Our AUCSeg achieves improvements across three commonly used evaluation metrics on three
datasets, demonstrating that our method is highly versatile and extensible.
G.6 Spatial Resource Consumption
In this section, we thoroughly explore the spatial resource consumption of the T-Memory Bank.
Table 14 details the ablation experiments on the spatial resource use of the T-Memory Bank. We
set the memory size SMto5and conduct experiments on the ADE20K dataset. It is no longer
necessary for samples of all classes to appear in the mini-batch, but rather only a minimum of 5tail
class samples are needed. The results demonstrate that using AUC alone requires a batch size and
graphics memory 5.5 times greater than the baseline to satisfy computational demands, which is a
significant expense. However, the T-Memory Bank substantially reduces this cost, enabling more
efficient training without an intolerant increase in graphics memory.
Moreover, we explore the effect of memory size SMon spatial resource consumption. The findings
in Figure 8 show that the graphics memory occupation increases slightly as SMrises. However,
a smaller memory size generally suffices for effective performance, indicating that AUCSeg can
achieve significant improvements with a manageable graphics memory burden. As shown in Figure 5a,
when SM= 5, significant performance improvements can be achieved with lower spatial resource
consumption.
35Table 14: Space resource consumption required
for training properly. TMB is the abbreviation of
T-Memory Bank.
AUC TMB Batch Size Graphic Memory
✕ ✕ 4 13.29G
✓ ✕ 22 72.90G
✓ ✓ 4 15.45G
SM=1SM=3SM=5SM=8SM=10SM=200GB
2GB
4GB
6GB
8GB
10GB12GB14GB16GB18GB Figure 8: The effect of memory size on
spatial resource consumption.
G.7 Results of Different AUC Surrogate Losses and Calculation Methods
AUCSeg can adopt various surrogate losses. In the previous section, we use the square loss. In
Table 15, we explore two other popular surrogate losses (hinge loss and exponential loss) for AUCSeg.
Additionally, we include results for two AUC loss calculation methods (one-vs-one and one-vs-all)
applied to AUCSeg using the square loss. The results are presented in Table 16.
Table 15: Results of different AUC surrogate losses in terms of mIoU (%).
Dataset AUC Surrogate Loss Overall Tail
ADE20K- 47.45 43.28
Hinge 48.59(+1.14) 44.76(+1.48)
Exp 48.86(+1.41) 45.07(+1.79)
Square 49.2(+1.75) 45.52(+2.24)
Cityscapes- 82.41 80.92
Hinge 82.64(+0.23) 81.35(+0.43)
Exp 82.45(+0.04) 81.55(+0.63)
Square 82.71(+0.30) 81.67(+0.75)
COCO-Stuff 164K- 42.42 40.33
Hinge 42.52(+0.10) 40.49(+0.16)
Exp 42.52(+0.10) 40.53(+0.20)
Square 42.73(+0.31) 40.72(+0.39)
Table 16: Results of different AUC calculation methods in terms of mIoU (%).
Dataset AUC Calculation Method Overall Tail
ADE20Kova 48.46 44.58
ovo 49.2 45.52
Cityscapesova 82.31 80.79
ovo 82.71 81.67
COCO-Stuff 164Kova 42.25 40.17
ovo 42.73 40.72
The results indicate that AUCSeg shows improved performance with any of the surrogate functions.
Among them, using square loss and the ovo calculation method delivers the best overall performance.
G.8 Results of the Comparison Between PMB and TMB
There are two differences between the Pixel-level Memory Bank (PMB) and our Tail-class Memory
Bank (TMB). First , the PMB stores pixels from all classes, whereas TMB only stores pixels from
tail classes. Second , in TMB, the storing and retrieving processes are conducted on an entire object
(we ensure that the pasted pixel forms a meaningful object). However, the PMB typically focuses on
a fixed number of pixels without structural information (regardless of whether these pixels can form a
complete image).
36Why do we only store tail class pixels instead of all pixels?
Table 17 shows the average number of pixels from head and tail classes per image in the ADE20K,
Cityscapes, and COCO-Stuff 164K datasets.
Table 17: The average number of pixels from head and tail classes per image.
Dataset ADE20K Cityscapes COCO-Stuff 164K
Head 46685 294290 60157
Tail 18977 31128 22526
It can be observed that the number of head class pixels in each image is 2.46to9.45times greater
than that of the tail classes, meaning that storing head class pixels would require significantly more
memory.
Table 18 compares the performance differences between storing all and tail class pixels. It shows
that the PMB, which incurs additional memory costs, performs almost the same as the TMB, and
even shows a noticeable decline in the Cityscapes dataset. This is because head classes appear in
almost every image (for example, in urban road datasets, it is hard to find an image without head class
pixels like ‘road’ or ‘sky’), so they do not need additional supplementation. Even if some images
require supplementation of head classes, their larger pixel counts might cause them to overwrite the
original tail class pixels when pasted, leading to a decline in performance. Thus, we only store tail
class pixels.
Table 18: The performance differences between PMB and TMB in terms of mIoU (%).
Dataset ADE20K Cityscapes COCO-Stuff 164K
PMB 49.09 82.07 42.66
TMB 49.2 82.71 42.73
Why it is not feasible to focus on a fixed number of pixels?
We conduct tests on the ADE20K dataset by supplementing a fixed number of tail class pixels
(10000 /20000 /30000 /40000 ) in each image and find that compared to AUCSeg, the performance
differences are −3.00%/−1.93%/−0.86%/−0.73%. This is because supplementing a fixed number
of pixels can result in incomplete images, such as only adding the front wheel of a bicycle, therefore
loss of the structural information. The model is then unable to learn complete and accurate features.
Therefore, in TMB, storing and retrieving are conducted on all pixels of an entire image.
G.9 Results of Different Memory Bank Update Strategies
In the previous section, we use the random replacement strategy to update the T-Memory Bank. We
experiment with three other selection methods on the ADE20K dataset:
•First-In-First-Out (FIFO) replacement : Prioritizes replacing the images that were first stored in
the Tail-class Memory Bank.
•Last-In-First-Out (LIFO) replacement : Prioritizes replacing the images that were last stored in
the Tail-class Memory Bank.
•Priority Used (PU) replacement : Prioritizes replacing images that have previously been selected
by the retrieval branch.
The results are shown in table 19.
FIFO and PU both show better performance overall and on tail classes compared to random sampling.
However, LIFO, by updating only the most recently added images in the T-Memory Bank, causes
the earlier images to remain unchanged. This leads to overfitting and, consequently, a decline in
performance.
37Table 19: Results of different memory bank update Strategies on ADE20K in terms of mIoU (%).
Overall Head Middle Tail
Random 49.2 80.59 59.45 45.52
FIFO 49.35 80.51 58.71 45.8
LIFO 49.05 80.35 58.76 45.45
PU 49.21 80.24 58.73 45.65
While these complex strategies can improve performance, the gains are relatively limited. On the
other hand, the random replacement method is easy to implement. Exploring more complex and
effective replacement methods could be a promising direction for future work.
G.10 Detailed Results of the Ablation Study on Hyper-Parameters
The detailed results of the ablation study on hyper-parameters are shown in Table 20, Table 21,
Table 22, and Table 23.
Table 20: Ablation study on Memory Size
(SM) in terms of mIoU (%).
SM Overall Tail
1 48.46 44.68
3 49.09 45.40
5 49.20 45.52
8 48.81 45.06
10 48.80 44.99
20 48.63 44.96Table 21: Ablation study on Sample Ratio
(RS) in terms of mIoU (%).
RS Overall Tail
0.01 48.91 45.13
0.04 48.70 44.97
0.05 49.20 45.52
0.07 48.47 44.75
0.1 48.34 44.53
0.15 47.27 43.30
Table 22: Ablation study on Resize Ratio
(RR) in terms of mIoU (%).
RR Overall Tail
0.3 49.07 45.42
0.4 49.20 45.52
0.5 48.89 45.23
0.6 48.01 44.11
0.7 47.47 43.55
1 44.41 40.26Table 23: Ablation study on the weight λfor
ℓaucandℓcein terms of mIoU (%).
λ Overall Tail
1/6 48.88 45.20
1/5 49.07 45.41
1/4 49.20 45.52
1/3 48.85 45.07
1/2 48.76 44.92
1 48.53 44.74
Explanation of the reasons why performance improvement decreases as memory size increases.
This is a trade-off between diversity and learnability. When the memory size is too large, the
probability of any single sample being effectively learned decreases. So the model fails to focus
on important examples, and thus fails to capture their features, ultimately leading to underfitting.
Conversely, if the memory size is too small, the diversity of samples is limited, which leads to
overfitting. Hence, there is no free lunch for increasing the bank.
Therefore, we pursue a reasonable memory size. As shown in Table 20, we believe that a memory
size of 5is suitable in most cases. As the memory size increases/decreases, the performance slightly
declines due to model overfitting/underfitting.
G.11 Results of the Ablation Study on the Impact of Batch Size
We conduct ablation experiments on the ADE20K dataset to evaluate the impact of batch size. The
results are shown in Table 24:
The performance improves as the batch size increases. Moreover, our AUCSeg is consistently
effective across different batch sizes.
38Table 24: Results of the ablation study on the impact of batch size in terms of mIoU (%).
Batch Size Overall Tail
1 34.73 29.66
+AUCSeg 40.14 35.74
2 45.5 41.34
+AUCSeg 46.86 42.93
4 47.45 43.28
+AUCSeg 49.2 45.52
8 49.35 45.46
+AUCSeg 49.36 45.53
16 50.07 46.32
+AUCSeg 50.96 47.03
H More Discussions About AUCSeg
Training and inference efficiency. During training, since AUCSeg ( 1.62±0.2sper iteration) adopts
pairwise loss, it will inevitably suffer from extra complexity compared to the standard CE-based
SegNeXt ( 0.76±0.15sper iteration). Besides, during inference, since AUCSeg does not modify
the model’s backbone, all algorithms with the same backbone achieve similar inference efficiency
(60±5msper image). Overall, AUCSeg could perform well with acceptable efficiency.
Performance trade-off between head and tail classes. We recognize that AUCSeg might slightly
impair the performance of head classes due to an increased focus on tail classes. However, this
often results in substantial improvements for tail classes, a trade-off that is generally beneficial since
tail classes are typically more critical. For instance, on the Cityscapes dataset, AUCSeg exhibits a
marginal decrease of 0.17% in head classes but gains 0.75% in tail classes compared to the runner-up,
SegNeXt. Furthermore, we note that performance decreases in head classes mainly arise from
misclassification at the blurry edges of objects. In contrast, gains in tail classes often stem from
either the successful detection of smaller objects or the more complete detection of such objects. To
summarize, on the Cityscapes datasets, detecting a new tail object like ‘Traffic Lights’ is far more
significant than precisely detecting the edge pixels of a head class like ‘sky’. Thus, we consider this
trade-off highly beneficial.
I Broader Impact
We propose a general semantic segmentation method to deal with the potential bias toward long-tail
objects. For fairness-sensitive scenarios, it might be helpful to improve fairness for long-tail groups.
39NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: We briefly summarize it in the abstract and detail the paper’s contributions and
scope in the introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss this issue in Appendix H.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
40Justification: We provide complete proofs for each theoretical result in Appendix B and
Appendix C.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We describe our algorithm in Section 4 and fully disclose all the informa-
tion needed to reproduce the main experimental results of the paper in Section 5.1 and
Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
41Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide a link to the data and code in the abstract.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/pu
blic/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run
to reproduce the results. See the NeurIPS code and data submission guidelines (
https://nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide the completed experimental setting in Appendix F.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We report this issue in Section 5.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
42• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide sufficient information on the computer resources in Appendix F.2.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conforms in every respect with the
NeurIPS Code of Ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discuss this issue in Appendix I.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
43•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not release data or models that have a high risk for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We cite the original paper that produced the code package and dataset.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
44•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We provide the code with documentation in the link within the abstract.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing and research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing and research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
45