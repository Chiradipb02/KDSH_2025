S-SOS: Stochastic Sum-Of-Squares for Parametric
Polynomial Optimization
Richard L. Zhu
Department of Computational and Applied Mathematics
University of Chicago
Chicago, IL 60637
richardzhu@uchicago.edu
Mathias Oster∗
Institute of Geometry and Practical Mathematics
RWTH Aachen
Aachen, Germany
oster@igpm.rwth-aachen.de
Yuehaw Khoo
Department of Statistics
University of Chicago
Chicago, IL 60637
yuehaw.khoo@uchicago.edu
Abstract
Global polynomial optimization is an important tool across applied mathematics,
with many applications in operations research, engineering, and the physical sci-
ences. In various settings, the polynomials depend on external parameters that
may be random. We discuss a stochastic sum-of-squares (S-SOS) algorithm based
on the sum-of-squares hierarchy that constructs a series of semidefinite programs
to jointly find strict lower bounds on the global minimum and extract candidates
for parameterized global minimizers. We prove quantitative convergence of the
hierarchy as the degree increases and use it to solve unconstrained and constrained
polynomial optimization problems parameterized by random variables. By em-
ploying n-body priors from condensed matter physics to induce sparsity, we can
use S-SOS to produce solutions and uncertainty intervals for sensor network lo-
calization problems containing up to 40 variables and semidefinite matrix sizes
surpassing 800×800.
1 Introduction
Many effective nonlinear and nonconvex optimization techniques use local information to identify
local minima. But it is often the case that we want to find global optima. Sum-of-squares (SOS)
optimization is a powerful and general technique in this setting.
The core idea is as follows: suppose we are given polynomials g1, . . . , g m, fwhere each function
is onRn→Rand we seek to determine the minimum value of fon the closed set S:S={x∈
Rn|gi(x)≥0∀i= 1, . . . , m }. Our optimization problem is then to find infx∈Rn{f(x)|x∈ S} .
∗Corresponding Author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).An equivalent formulation is to find the largest constant c∈R(i.e. the tightest lower bound) that
can be subtracted from fsuch that f−c≥0over the set S. This reduction converts a polynomial
optimization problem over a semialgebraic set to the problem of checking polynomial non-negativity.
This problem is NP-hard in general [ 1], therefore one instead resorts to checking if f−cis a
sum-of-squares (SOS) function, e.g. in the unconstrained setting where S=Rnone seeks to find
some polynomials hk:Rn→Rsuch that f−c=P
kh2
k. If such a decomposition can be found,
then we have an easily-checkable certification that f−c≥0, as all sum-of-squares are non-negative.
Since the converse is not true (not all non-negative functions are sum-of-squares), this is a relaxation
of the original non-negativity problem.
Notably, if we restrict the hkto have maximum degree d, the search for a degree- 2dSOS decomposi-
tion of a function can be automated as a semidefinite program (SDP) [2, 3, 4]. Solving this SDP for
varying degrees dgenerates the well-known Lasserre (SOS) hierarchy. A given degree dcorresponds
to a particular level of the hierarchy. Solving this SDP produces a lower bound cdwhich has been
proven to converge to the true global minimum c∗= inf xf(x)asdincreases, with finite convergence
(cd=c∗at finite d) for functions with second-order local optimality conditions [ 5,6] and asymptotic
convergence with milder assumptions thanks to representation theorems for positive polynomials
from real algebraic geometry [ 7,8,9]. Further work has elucidated both theoretical implications
[7,3,10,11,12,13] and useful applications of SOS to disparate fields [ 14,15,16,5,6,17,18] (see
further discussion in Appendix A.2).
Motivated by the sum-of-squares certification for a lower bound con a function f(x), we generalize
to the case where the function to be minimized has additional parameters, i.e. f(x, ω)where
xare variables and ωare parameters drawn from some probability distribution ω∼ν(ω). We
seek a function c(ω)that is the tightest lower bound to f(x, ω)everywhere: f(x, ω)≥c(ω)with
c(ω)→infxf(x, ω). This setting was originally presented in [ 19] as a “Joint and Marginal” approach
to parametric polynomial optimization. With the view that ω∼ν(ω)and seeking to parameterize
the minimizers x∗(ω) =argminxf(x, ω), we are reminded of some of the prior work in polynomial
chaos, where a system of stochastic variables is expanded into a deterministic function of those
stochastic variables [20, 21].
Contributions and outline. Our primary contributions are a quantitative convergence proof for the
Stochastic Sum-of-Squares (S-SOS) hierarchy of semidefinite programs (SDPs), a formulation of a
new hierarchy (the cluster basis hierarchy) that uses the structure of a problem to sparsify the SDP,
and numerical results on its application to the sensor network localization problem.
In Section 2, we review the S-SOS hierarchy of SDPs [ 19] and its primal and dual formulations
(Section 2.1). We then detail how different hierarchies can be constructed (Section 2.2.1). Finally, in
Section 2.3 (complete proof in Appendix A.5.2) we specialize to compact X×Ωand outline the
proof for a theorem on quantitative convergence (the gap between the optimal values of the degree- 2s
S-SOS SDP and the “tightest lower-bounding” optimization problem goes →0ass→ ∞ ) of the
S-SOS hierarchy for trigonometric polynomials on [0,1]n×[0,1]dfollowing the kernel formalism of
[22, 6, 23].
In Section 3 we review the hierarchy’s applications in parametric polynomial minimization and
uncertainty quantification, focusing on several variants of sensor network localization on X×Ω =
[−1,1]n×[−1,1]d. We present numerical results for the accuracy of the extracted solutions that
result from S-SOS, comparing to other approaches to parametric polynomial optimization, including
a simple Monte Carlo-based method.
2 Stochastic Sum-of-squares (S-SOS)
2.0.1 Notation
LetP(S)be the space of polynomials on S, where S∈ {X,Ω}.X⊆RnandΩ⊆Rd, respectively,
where XandΩare (not-necessarily compact) subsets of their respective ambient spaces Rnand
Rd. A polynomial in P(X)can be written as p(x) =P
α∈Zn
≥0cαxα∈ P(X)(substituting n→
d, x→ω, X→Ωfor a polynomial in P(Ω)). Let x:= (x1, . . . , x n), ω:= (ω1, . . . , ω d),αbe
a multi-index (size given by context), and cαbe the polynomial coefficients. Let Ps(S)for some
s∈Z≥0, S∈ {X,Ω}denote the subspace of P(S)consisting of polynomials of degree ≤s, i.e.
polynomials where the multi-indices of the monomial terms satisfy ||α||1≤s.PSOS(X×Ω)refers
2to the space of polynomials on X×Ωthat can be expressible as a sum-of-squares in xandωjointly,
andPs
SOS(X×Ω)be the same space restricted to polynomials of degree ≤s. Additionally, W≽0
for a matrix Wdenotes that Wis symmetric positive semidefinite (PSD). Finally, P(Ω)denotes the
set of Lebesgue probability measures on Ω. For more details, see Appendix A.1.
2.1 Formulation of S-SOS hierarchy
We present two formulations of the S-SOS hierarchy that are dual to each other in the sense of
Fenchel duality [ 24,25]. The primal problem seeks to find the tightest lower-bounding function and
the dual problem seeks to find a minimizing probability distribution. Note that the “tightest lower
bound” approach is dual to the “minimizing distribution” approach, otherwise known as a “joint and
marginal” moment-based approach originally detailed in [19].
2.1.1 Primal S-SOS: The tightest lower-bounding function
Consider a polynomial f(x, ω) :Rn+d→Rwithx∈X⊆Rn, ω∈Ω⊆Rdequipped with a
probability measure ν(ω). We interpret xas our optimization variables and ωas noise parameters,
and seek a lower-bounding function c∗(ω)such that f(x, ω)≥c∗(ω)for all x, ω. In particular, we
want the tightest lower bound c∗(ω) = inf x∈Xf(x, ω). Note that even when f(x, ω)is polynomial,
the tightest lower bound c∗(ω)can be non-polynomial. A simple example is the function f(x, ω) =
(x−ω)2+ (ωx)2, which has c∗(ω) = inf xf(x, ω) =ω4/(1 +ω2)(Appendix A.6.1).
For us to select the “best” lower-bounding function, we want to maximize the expectation of the
lower-bounding function c(ω)under ω∼ν(ω)while requiring f(x, ω)−c(ω)≥0, giving us the
following optimization problem over L1-integrable lower-bounding functions:
p∗= sup
c∈L1(Ω)Z
c(ω)dν(ω) (1)
s.t.f(x, ω)−c(ω)≥0
Even if we restricted c(ω)to be polynomial so that the residual f(x, ω)−c(ω)is also polynomial,
we would still have a challenging nonconvex optimization problem over non-negative polynomials.
In SOS optimization, we take a relaxation and require the residual to be SOS: f(x, ω)−c(ω)∈
PSOS(X×Ω). Doing the SOS relaxation of the non-negative Equation (1) and restricting c(ω), i.e.
f(x, ω)−c(ω)to polynomials of degree ≤2sgives us Equation (2), which we call the primal S-SOS
degree- 2sSDP:
p∗
2s= sup
c∈P2s(Ω),W≽0Z
c(ω)dν(ω) (2)
s.t.f(x, ω)−c(ω) =ms(x, ω)TWm s(x, ω)
where ms(x, ω)is a basis function X×Ω→Ra(n,d,s )containing monomial terms of degree
≤swritten as a column vector, and W∈Ra(n,d,s )×a(n,d,s )a symmetric PSD matrix. Here,
a(n, d, s )represents the dimension of the basis function, which depends on the degree sand on
the dimensions n, d. For this formulation to find the best degree- 2sapproximation to the lower-
bounding function, we require g(x, ω) =ms(x, ω)TWm s(x, ω)to span P2s(X×Ω). Selecting all
combinations of standard monomial terms of degree ≤ssuffices and results in a basis function with
sizea(n, d, s ) = n+d+s
s
.
2.1.2 Dual S-SOS: A minimizing distribution
The formal dual to Equation (1) (proof of duality in Appendix A.5.1) seeks to find a “minimizing
distribution” µ(x, ω), i.e. a probability distribution that places weight on the minimizers of f(x, ω)
subject to the constraint that the marginal µX(ω)matches ν(ω):
d∗= inf
µ∈P(X×Ω)Z
f(x, ω)dµ(x, ω) (3)
s.t.Z
Xdµ(x, ω) =µX(ω) =ν(ω)
3where we have written P(X×Ω)as the space of joint probability distributions on X×ΩandµX(ω)
is the marginal of µ(x, ω)with respect to ω, obtained via disintegration.
For the primal, we considered polynomials of degree ≤2s. We do the same here. The formal
dual becomes a tractable SDP, where the objective turns into moment-minimization and the con-
straints become moment-matching. Following [ 3,15], letM∈Ra(n,d,s )×a(n,d,s )be the symmetric
PSD moment matrix with entries defined as Mi,j=R
X×Ωm(i)
s(x, ω)m(j)
s(x, ω)dµ(x, ω)where
m(i)
s(x, ω)is the i-th element of the basis function ms. Let y∈Rb(n,d,s )be the moment vec-
tor of independent moments that completely specifies M, e.g. in the case that we use all stan-
dard monomials of degree ≤sand have a(n, d, s ) = n+d+s
s
, then b(n, d, s ) = n+d+2s
2s
. We
write M(y)as the moment matrix that is formed from these independent moments. We have
yα(i,j)=R
X×Ωm(i)
s(x, ω)m(j)
s(x, ω)dµ(x, ω)where the multi-index α(i, j)∈Zn+d
≥0corresponds
to the sum of the multi-indices corresponding to the i-th entry and the j-th entry of ms(x, ω).
We write f(x, ω)in terms of the monomials f(x, ω) =P
||α||1≤2sfα[x, ω]α, where [x, ω]is the
concatenation of the n+dvariables from x, ω andα∈Zn+d
≥0is a multi-index. Note that every
monomial [x, ω]αhas a corresponding moment yα:R
[x, ω]αdµ(x, ω) =yα. We then observe that
the integral in the objective reduces to a dot product between the coefficients of fand the moment
vector: Z
f(x, ω)dµ(x, ω) =ZX
αfα[x, ω]αdµ(x, ω) =X
αfαyα
After converting the distribution-matching constraint µX(ω) =ν(ω)in(3)into equality constraints
on the moments of ωup to degree 2s, we obtain the following dual S-SOS degree- 2sSDP:
d∗
2s= inf
y∈Rb(n,d,s )X
||α||1≤2sfαyα (4)
s.t.M(y)≽0
yα=mα∀(α, m α)∈ M ν
We write Mνas the set of (α, m α)representing the moment-matching constraints on ωαup to
degree- 2s, i.e. we want to setR
X×Ωωαdµ(x, ω) =R
Ωωαdν(ω) = mαfor all multi-indices
α∈Zd
≥0with||α||1≤2s. There are d+2s
2s
multi-indices α∈Zn+d
≥0,||α||1≤2swhere only the d
entries associated with ωare non-zero, and therefore the number of moment-matching constraints is
|Mν|= d+2s
2s
. Note that the moment matrix M(y)∈Ra(n,d,s )×a(n,d,s )is a symmetric PSD matrix
and is the dual variable to the primal W. Observe also that we require the moments of ν(ω)of degree
up to 2sto be bounded. (4)is often a more convenient form than (2), especially when working with
additional equality or inequality constraints, as we will see in Section 3. For concrete examples of the
primal and dual SDPs with explicit constraints, see Appendix A.3.
2.2 Variations
In this section, we detail two ways of building a hierarchy, one based on the maximum degree of
monomial terms in the basis function (Lasserre) and a novel one based on the maximum number
of interactions occurring in the terms of the basis function (cluster basis). To define any SOS
hierarchy, we first select a monomial basis. Some examples include the standard monomial basis
x1, . . . , x n, trigonometric/Fourier 1-periodic monomial basis sinx1,cosx1, . . . , sinxn,cosxn), or
others. Using this basis, we write down a basis function m(x)which comprises some combinations
of monomials. Squared linear combinations of the basis functions then span a SOS space of functions:
H:{(P
ihimi(x))2}.
2.2.1 Standard Lasserre hierarchy
In the Lasserre hierarchy, the basis function ms(x)is composed of all combinations of monomials
up to degree s∈Z>0and a given level of the hierarchy is set by the maximum degree s. The basis
function consists of terms xαwithαa multi-index and ||α||1≤s. The degree- 2sSOS function
space parameterized by this basis function is that spanned by ms(x)TWm s(x)for PSD W, i.e. the
4functions that can result from squaring any linear combination of degree- spolynomials that can be
generated from our basis ms(x). As we increase the degree s, our basis function gets larger and our
S-SOS SDP objective values converge to the optimal value of the “tightest lower-bounding” problem
Equation (1) [19].
2.2.2 Cluster basis hierarchy
In this section, we propose a cluster basis hierarchy, wherein we utilize possible spatial organization
of the problem to sparsify the problem and reduce the size of the SDP that must be solved [ 26,27].
The cluster basis is a physically motivated prior often used in statistical and condensed matter physics,
where we assume that our degrees of freedom can be arrayed in space, with locally close variables
interacting strongly (kept in the model) and globally separated variables interacting weakly (ignored).
Moreover, one may also keep only the terms with interactions between a small number of degrees of
freedom, such as considering only pairwise or triplet interactions between particles.
In the cluster basis hierarchy, a given level of the hierarchy is specified by a 2-tuple (b, t), the desired
body order band the maximum degree of a single variable t. Body order denotes the maximum
number of interacting variables in a given monomial term, e.g. xa
ixb
jxc
kwould have body order 3 and
total degree a+b+c. The basis function mb,tconsists of terms xαwithαa multi-index, ||α||0≤b
(at most binteracting variables can occur in a single term), and ||α||∞≤t(each variable can have up
to degree t. The maximum degree of the basis function mb,tis then s=bt. If we are to compare
mb,tfrom the cluster basis hierarchy with msfrom the Lasserre hierarchy, we find that even when
bt=swe still have strictly fewer terms, e.g. in the case where b= 2, t= 2, s= 4 we have ms
containing terms of the form x4
ibutmb,tonly has degree-4 terms of the x2
ix2
j.
To expand on this, consider that in the standard Lasserre hierarchy we have ms(x)containing all
monomials of degree ≤sinnvariables, or n+s
s
terms in total. In the proposed cluster basis
hierarchy, mb,t(x)has
bX
k=0n
k
tk
terms. This expression results from the need to sum over body orders k, considering that there are  n
k
ways to choose kvariables and that each selected variable has kpossible degrees so there are
tkways to assign degrees ≤tto these kvariables. For fixed b, t,s=bt, and n≫b, twe note that
ms(x)hasO(nbt)terms while mb,t(x)hasO(nb)terms so the size reduction factor in using the
cluster basis asymptotically goes like nb(t−1). As the number of variables nin the problem grow,
we have asymptotic dominance in using the cluster basis to reduce the size of the SDP that must
be solved. As bt→ ∞ we might expect asymptotic convergence of the SDP hierarchy just like the
standard Lasserre hierarchy, however, a full proof of convergence is out of scope for this paper. For
further details, see discussion in Appendix A.7.4.
2.3 Convergence of S-SOS
As we increase the degree s(either sin the Lasserre hierarchy or b, tin the cluster basis hierarchy)
we would expect the SDP objective values p∗
2s(Equation (2)) to converge to the optimal value p∗and
the lower bounding function c∗
2s(ω)to converge to the tightest lower bound c∗(ω) = inf xf(x, ω). In
this paper we refer to p∗
2s→p∗andd∗
2s→d∗interchangeably as strong duality occurs in practice
despite being difficult to formally verify (Appendix A.4). This convergence is a common feature of
SOS hierarchies.
In this section we show that using polynomial c∗
2s(ω)to approximate c∗(ω)still allows for asymptotic
convergence in L1ass→ ∞ .
2.3.1 Overview of result
We specialize to the particular case of 1-periodic trigonometric polynomials f(x, ω), c(ω)on compact
X= [0,1]nand compact Ω⊂Rdand prove asymptotic convergence of the degree- 2sS-SOS
hierarchy as s→ ∞ . Though seemingly restrictive, this choice allows us to leverage Fourier
convergence results on a compact domain. For generic f(x, ω)where we are only interested in its
behavior in some compact set (nearly all practical problems involve a restriction of domain), we may
5rescale the compact set to this 1-periodic compact domain and apply the results found here. To use
other families of polynomials, we note that a substitution argument justifies the focus on trigonometric
polynomials as any convergence result achieved for the trigonometric polynomial hierarchy lead
directly to a matching result on regular polynomials (2.2 in [ 28]). Previously, rates of 1/s2were
attained for the standard Lasserre hierarchy (i.e. without external parameters ω) on compact domains
with mild assumptions [ 29,28,9]. In our work, a slower convergence rate of lns/sis achieved due
to the introduction of external parameters, requiring us to approximate the lower-bounding function
c∗(ω)and thereby limiting the obtained convergence rate.
2.3.2 lns/sconvergence using a polynomial approximation to c∗(ω)
We would like to bound the gap between the optimal lower bound c∗(ω) = inf x∈Xf(x, ω)and the
lower bound c∗
2s(ω)resulting from solving the degree- 2sprimal S-SOS SDP, i.e.
0≤c∗(ω)−c∗
2s(ω)≤ε(f, s)∀ω∈Ω. (5)
To that end, we need to understand the regularity of c∗. Without further assumptions, we may assume
c∗to be Lipschitz continuous, per Proposition 2.1.
With Equation (5) we may then integrate
0≤Z
Ωinf
xf(x, ω)−c∗
2s(ω)dν(ω)≤ |Ω|ε(f, s)
where we control εin terms of the degree s. If we can drive ϵ→0ass→ ∞ then we are done.
Proposition 2.1 (Theorem 2.1 in [ 30]).Letg:X×Y→Rbe polynomial. Then y7→infx∈Xg(x, y)
is Lipschitz continuous.
Theorem 2.1 (Asymptotic convergence of S-SOS) .Letf: [0,1]n×Ω→Rbe a trigonometric
polynomial of degree 2r,c∗(ω) = inf xf(x, ω)the optimal lower bound as a function of ω, and ν
any probability measure on compact Ω⊂Rd. Letsrefer to the degree of the basis in both x, ω terms
and the degree of the lower-bounding polynomial c(ω), i.e.ms([x, ω]) :Rn+d→Ra(n,d,s )is the
full basis function of terms [x, ω]αwith||α||1≤sandc(ω)only has terms ωαwith||α||1≤s.
Letp∗
2sbe the solution to the following S-SOS SDP (c.f. Equation (2)) with ms(x, ω)a spanning
basis of trigonometric monomials with degree ≤s:
p∗
2s= sup
c∈P2s(Ω),W≽0Z
c(ω)dν(ω)
s.t.f(x, ω)−c(ω) =ms(x, ω)TWm s(x, ω)
Then there is a constant C > 0depending only on ||f−¯f||F,||c∗−¯c∗||, r,Ω, n, d such that the
following holds:Z
Ω[c∗(ω)−c∗
2s(ω)] dν(ω)≤Clns
s
where ¯fdenotes the average value of the function fover [0,1]n, i.e. ¯f=R
[0,1]nf(x)dxand
||f(x)||F=P
ˆx|ˆf(ˆx)|denotes the norm of the Fourier coefficients. Thus we have asymptotic
convergence of the S-SOS SDP hierarchy to the optimal value p∗of Equation (1)as we send s→ ∞ .
Proof. The following is an outline of the proof. For complete details, including the full theorem and
proof, please see Appendix A.5.2.
The key argument used follows that of [ 22,28,23,31,32]. Take the lower-bounding minimizer of
Equation (1) c∗(ω)(not necessarily polynomial) and approximate it with a trigonometric polynomial
c∗
a(ω)of degree s. We then pass the non-negative component f−c∗
athrough an integral operator T
that is built out of sums of squares of trigonometric polynomials (related to the Christoffel-Darboux
and Jackson kernels) to obtain a strictly-positive SOS component:
Th(x, ω) =Z
X×Ω|qx(x−¯x)|2|qω(ω−¯ω)|2h(¯x,¯ω)d¯xd¯ω.
6qx(x), qω(ω)can be chosen to be “kernel functions” of bounded degree ≤sso that the output SOS
function is of degree 2s. We can show the approximation error ||c∗−c∗
a||is small, that the operator
Texists and is close to the identity, and the deformation resulting from the SOS projection is small
for sufficiently large degree s(degree of the approximating c∗
a(ω)and in the SOS kernel). We may
conclude that the true strictly-positive part may be well-approximated by the SOS hierarchy and find
asymptotic convergence along with a convergence rate in the degree sof the hierarchy.
3 Numerical experiments
We present two numerical studies of S-SOS demonstrating its use in applications. The first study
(Section 3.1) numerically tests how the optimal values of the SDP Equation (2) p∗
2sconverge to p∗of
the original primal Equation (1) as we increase the degree. The second study (Section 3.2) evaluates
the performance of S-SOS for solution extraction and uncertainty quantification in various sensor
network localization problems.
3.1 Simple quadratic SOS function
As a simple illustration of S-SOS, we test it on the SOS function
f(x, ω) = (x−ω)2+ (ωx)2(6)
withx∈R, ω∈R. The lower bound c∗(ω) = inf xf(x, ω)can be computed analytically as
c∗(ω) =ω4/(1 + ω2). Assuming ω∼Uniform (−1,1), we get that the objective value for the
“tightest lower-bounding” primal problem Equation (1) is p∗=R1
−1ω4
2(1+ω2)dω=π
4−2
3≈0.1187 .
For further details, see Appendix A.6.
We are interested in studying the quantitative convergence of the S-SOS hierarchy numerically. The
idea is to solve the primal (dual) degree- 2sSDP to find the tightest polynomial lower bound (the
minimizing probability distribution) for varying degrees s. Assgets larger, the basis function ms(x)
gets larger and the objective value of the SDP Equation (2) p∗
2sshould converge to the theoretical
optimal value p∗.
In Figure 1 we see very good agreement between p∗andp∗
2swith exponential convergence as s
increases. This is much faster than the rate we found in Section 2.3.2, but agrees with the exponential
convergence results from [ 6] achieved with local optimality assumptions. Due to the simplicity of (6),
it is not surprising that we see much faster convergence. In fact, for most typical functions, we might
expect convergence much faster than the worst-case rate. The tapering-off of the convergence rate is
likely attributed to the numerical tolerance used in our solver (CVXPY/MOSEK), as we observed
that increasing the tolerance shifts the best-achieved gap higher.
3.2 Sensor network localization
Sensor network localization (SNL) is a common testbed for global optimization and SDP solvers
due to the high sensitivity and ill-conditioning of the problem. In SNL, one seeks to recover the
positions of Nsensors X∈RN×ℓpositioned in Rℓgiven a set of noisy observations of pairwise
distances dij=||xi−xj||between the sensors [ 15,33]. To have a unique global minimum and
remove symmetries, sensor-anchor distance observations are often added, where several sensors are
anchored at known locations in the space. This can improve the conditioning of the problem, making
it “easier” in some sense.
3.2.1 Definitions
We define a SNL problem instance withX∈[−1,1]N×ℓas the ground-truth positions for S=
{1,2, . . . , N }sensors, A∈[−1,1]K×ℓas the ground-truth positions for A={1,2, . . . , K }anchors,
Dss(r) ={dij=||xi−xj||:i, j∈ S anddij≤r}as the set of observed sensor-sensor distances
andDsa(r) ={dik=||xi−ak||:i∈ S, k∈ A anddik≤r}as the set of observed sensor-anchor
distances, both of which depend on some sensing radius r.
7Figure 1: Comparison between the objective value p∗
2sfrom solving the degree- 2sS-SOS SDP
and the objective value p∗resulting from the best-possible lower bound c∗(ω)for noise drawn as
ω∼Uniform (−1,1).p∗=R
c∗(ω)dν(ω) =π
4−2
3≈0.1187 is plotted as the line in black and the
p∗
2svalues are shown as blue dots (left) with the gap between the values p∗−p∗
2s(right).
Writing xi, ak∈[−1,1]ℓas the unknown positions of the i-th sensor and the k-th anchor, we can
write the potential function to be minimized as a polynomial:
f(x, ω;X, A, r ) =X
dij∈Dss(r)(||xi−xj||2
2−dij(ω)2)2
| {z }
sensor-sensor interactions+X
dik∈Dsa(r)(||xi−ak||2
2−dik(ω)2)2
| {z }
sensor-anchor interactions(7)
The observed sensor-sensor and sensor-anchor distances dij(ω), dik(ω)can be perturbed arbitrarily,
but in this paper we focus on linear uniform noise, i.e. for a subset of observed distances we have
dij,k(ω) =d∗
ij+ϵωkwithωk∼Uniform (−1,1). Other noise types may be explored, including
those including outliers, which may be a better fit for robust methods (Appendix A.7.2).
Equation (7) contains soft penalty terms for sensor-sensor terms and sensor-anchor terms. We can see
that this is a degree-4 polynomial in the standard monomial basis elements, and a global minimum of
this function is achieved at f(X,0d;X, A, r ) = 0 (where the distances have not been perturbed by
any noise). In general for non-zero ω(measuring distances under noise perturbations) we expect the
function minimum to be >0, as there may not exist a configuration of sensors ˆXthat is consistent
with the observed noisy distances.
We can also support equality constraints in our solution, in particular hard equality constraints on the
positions of certain sensors relative to known anchors. This corresponds to removing all sensor-anchor
soft penalty terms from the function and instead selecting NH< N sensors at random to exactly fix
in known positions via equality constraints in the SDP. The SDP is still large but the effective number
of variable sensors has been reduced to N′=N−NH.
A given SNL problem type is specified by a spatial dimension ℓ,Nsensors, Kanchors, a sensing
radius r∈(0,2√
ℓ), a noise type (linear), and anchor type (soft penalty or hard equality). Once these
are specified, we generate a random problem instance by sampling X∼Uniform (−1,1)n, A∼
Uniform (−1,1)d. The potential f(x, ω)for a given instance is formed (either with sensor-anchor
terms or not, with terms kept based on some sensing radius r, and noise variables appropriately
added).
The number of anchors is chosen to be as few as possible so as to still enable exact localization,
i.e.K=ℓ+ 1anchors for a SNL problem in ℓspatial dimensions. The SDPs are formulated with
the help of SymPy [ 34] and solved using CVXPY [ 35,36] and Mosek [ 37] on a server with two
Intel Xeon 6130 Gold processors (32 physical cores total) and 256GB of RAM. For an expanded
discussion and further details, see Appendix A.7.
83.2.2 Evaluation metrics
The accuracy of the recovered solution is of primary interest, i.e. our primary evaluation metric should
be the distance between our extracted sensor positions xand the ground-truth sensor positions X,
i.e.dist(x, X). Because the S-SOS hierarchy recovers estimates of the sensor positions E[xi]along
with uncertainty estimates Var[xi], we would like to measure the distance between our ground-truth
positions Xto our estimated distribution p(x) =N(E[x],Var[x]). The Mahalanobis distance δM
(Equation (8)) is a modified distance metric that accounts for the uncertainty [ 38]. We use this as our
primary metric for sensor recovery accuracy.
δM(X,N(µ,Σ)) :=q
(X−µ)TΣ−1(X−µ) (8)
As our baseline method, for each problem instance we apply a basic Monte Carlo method detailed in
Algorithm 1 (Appendix A.7.3) where we sample ω∼ν(ω), use a local optimization solver to find
x∗(ω) = inf xf(x, ω), and use this to estimate Eω∼ν[x],Varω∼ν[x]. Note that though this non-SOS
method achieves some estimate of the dual SDP objectiveR
f(x, ω)dµ(x, ω), it is not guaranteed to
be a lower bound.
3.2.3 Results
Recovery accuracy. In Table 1 we see a comparison of the S-SOS method and the MCPO baseline.
Each row corresponds to one SNL problem type, i.e. we fix the physical dimension ℓ, the number of
anchors K=ℓ+ 1, and select the sensing radius rand the noise scale ϵ. We then generate L= 20
random instances of each problem type, corresponding to a random realization of the ground-truth
sensor and anchor configurations X∈[−1,1]N×ℓ, A∈[−1,1]K×ℓ, producing a f(x, ω)that we
then solve the SDP for (in the case of S-SOS) or do pointwise optimizations for (in the case of
MCPO). Each method outputs estimates for the sensor positions and uncertainty around it as a
N(E[x],Cov[x]), which we then compute δMfor (see Equation (8)), treating each dimension as
independent of each other (i.e. Xas a flat vector). Each instance solve gives us one observation of
δMor each method, and we report the median and the ±1σ34%values over the L= 20 instances we
generate.
4 Discussion
In this paper, we discuss the stochastic sum-of-squares (S-SOS) method to solve global polynomial
optimization in the presence of noise, prove two asymptotic convergence results for polynomial fand
compact Ω, and demonstrate its application to parametric polynomial minimization and uncertainty
quantification along with a new cluster basis hierarchy that enables S-SOS to scale to larger problems.
In our experiments, we specialized to sensor network localization and low-dimensional uniform
random noise with small n, d. However, it is relatively straightforward to extend this method to
support other noise types (such as Gaussian random variates without compact support, which we do
in Appendix A.6.4) and support higher-dimensional noise with d≫1.
Scaling this method to larger problems n≫1is an open problem for all SOS-type methods. In
this paper, we take the approach of sparsification, by making the cluster basis assumption to build
up a block-sparse W. We anticipate that methods that leverage sparsity or other structure in fwill
be promising avenues of research, as well as approximate solving methods that avoid the explicit
materialization of the matrices W, M . For example, we assume that the ground-truth polynomial
possesses the block-sparse structure because our SDP explicitly requires the polynomial f(x, ω)
to exactly decompose into some lower-bounding c(ω)and SOS fSOS(x, ω). Relaxing this exact-
decomposition assumption and generalizing beyond polynomial f(x, ω), c(ω)may require novel
approaches and would be an exciting area for future work.
9Table 1: Comparison of S-SOS and MCPO solution extraction accuracy. We present the Maha-
lanobis distance δM(Equation (8)) of the the true sensor positions X∗to the extracted distribution
N(E[x],Var[x])over solutions recovered from S-SOS for varying SNL problem types. ℓis the spatial
dimension, ris the sensing radius used to cutoff terms in the potential f(x, ω),ϵis the noise scale,
NHis the number of hard equality constraints used (sensors fixed at known locations), NCis the
number of clusters used (see Appendix A.7.4), and Nis the number of sensors used. Each SNL
problem instance has K=ℓ+ 1anchors used in the potential (if NH= 0). The MCPO values
are estimated with T= 50 Monte Carlo iterates. Each entry is ˆµ±ˆσwhere ˆµis the median and
robust standard-deviation ( σ34%) estimated over 20 runs of the same problem type with varying
random initializations of the sensor positions. The entries with the lowest median δMare bolded.
We also compare the number of elements in the full basis af, the cluster basis ac, and the reduction
multiple when using the cluster basis af/ac. When passing to the cluster basis, af/acis how much
the semidefinite matrix shrinks by.
Parameters Basis comparison M-distance ( δM)
ℓ r ϵ N HNCN a f acaf/ac S-SOS MCPO
1 0.5 0.3 0 1 10 78 78 1x 0.94±0.22 2.61±3.86
1 1.0 0.3 0 1 10 78 78 1x 0.29±0.16 1.10±0.58
1 1.5 0.3 0 1 10 78 78 1x 0.11±0.11 0.86±0.52
1 1.5 0.3 2 1 10 78 78 1x 0.24±0.37 1.06±1.28
1 1.5 0.3 4 1 10 78 78 1x 0.10±0.03 0.61±0.41
1 1.5 0.3 6 1 10 78 78 1x 0.06±0.04 0.48±0.32
1 1.5 0.3 8 1 10 78 78 1x 0.04±0.02 0.31±0.17
2 1.5 0.1 0 9 9 406 163 2.5x 2.86±0.94 1562.39±596.29
2 1.5 0.1 0 9 15 820 317 2.6x 3.25±1.19 1848.65±650.45
References
[1]Michael R. Garey and David S. Johnson. Computers and intractability: a guide to the theory of
NP-completeness . A series of books in the mathematical sciences. Freeman, New York [u.a],
27. print edition, 2009.
[2]Yurii Nesterov. Squared Functional Systems and Optimization Problems. In Hans Frenk, Kees
Roos, Tamás Terlaky, and Shuzhong Zhang, editors, High Performance Optimization , Applied
Optimization, pages 405–440. Springer US, Boston, MA, 2000.
[3]Jean B. Lasserre. Global Optimization with Polynomials and the Problem of Moments. SIAM
Journal on Optimization , 11(3):796–817, January 2001. Publisher: Society for Industrial and
Applied Mathematics.
[4]Monique Laurent. Sums of Squares, Moment Matrices and Optimization Over Polynomials. In
Mihai Putinar and Seth Sullivant, editors, Emerging Applications of Algebraic Geometry , The
IMA V olumes in Mathematics and its Applications, pages 157–270. Springer, New York, NY ,
2009.
[5]Jiawang Nie. Optimality conditions and finite convergence of Lasserre’s hierarchy. Mathematical
Programming , 146(1-2):97–121, August 2014.
[6]Francis Bach and Alessandro Rudi. Exponential convergence of sum-of-squares hierarchies for
trigonometric polynomials, April 2023. arXiv:2211.04889 [math].
[7]Mihai Putinar. Positive Polynomials on Compact Semi-algebraic Sets. Indiana University Math-
ematics Journal , 42(3):969–984, 1993. Publisher: Indiana University Mathematics Department.
[8]Konrad Schmüdgen. The Moment Problem , volume 277 of Graduate Texts in Mathematics .
Springer International Publishing, Cham, 2017.
10[9]Lucas Slot and Monique Laurent. Sum-of-squares hierarchies for binary polynomial opti-
mization. Mathematical Programming , 197(2):621–660, January 2023. Publisher: Springer
Verlag.
[10] Jean Lasserre. The Moment-SOS hierarchy, August 2018. arXiv:1808.03446 [math].
[11] Jean-Bernard Lasserre. The Moment-SOS hierarchy: Applications and related topics. To appear
in Acta Numerica (2024) , September 2023.
[12] Lorenzo Baldi and Bernard Mourrain. On the effective Putinar’s Positivstellensatz and moment
approximation. Mathematical Programming , 200(1):71–103, June 2023.
[13] Lorenzo Baldi and Lucas Slot. Degree Bounds for Putinar’s Positivstellensatz on the Hypercube.
SIAM Journal on Applied Algebra and Geometry , 8(1):1–25, March 2024.
[14] P.A. Parrilo. Structured Semidefinite Programs and Semialgebraic Geometry Methods in
Robustness and Optimization . PhD thesis, California Institute of Technology, Pasadena, CA,
2000.
[15] Jiawang Nie. Sum of squares method for sensor network localization. Computational Optimiza-
tion and Applications , 43(2):151–179, June 2009.
[16] Etienne de Klerk. The complexity of optimizing over a simplex, hypercube or sphere: a short
survey. Central European Journal of Operations Research , 16(2):111–125, June 2008.
[17] Amir Ali Ahmadi and Anirudha Majumdar. DSOS and SDSOS Optimization: More Tractable
Alternatives to Sum of Squares and Semidefinite Optimization. SIAM Journal on Applied
Algebra and Geometry , 3(2):193–230, January 2019.
[18] Dávid Papp and Sercan Yildiz. Sum-of-Squares Optimization without Semidefinite Program-
ming. SIAM Journal on Optimization , 29(1):822–851, January 2019.
[19] Jean B. Lasserre. A “Joint+Marginal” Approach to Parametric Polynomial Optimization. SIAM
Journal on Optimization , 20(4):1995–2022, January 2010. Publisher: Society for Industrial and
Applied Mathematics.
[20] Bruno Sudret. Global sensitivity analysis using polynomial chaos expansion. Reliability
Engineering & System Safety , 93:964–979, July 2008.
[21] Habib N. Najm. Uncertainty Quantification and Polynomial Chaos Techniques in Compu-
tational Fluid Dynamics. Annual Review of Fluid Mechanics , 41(1):35–52, 2009. _eprint:
https://doi.org/10.1146/annurev.fluid.010908.165248.
[22] Kun Fang and Hamza Fawzi. The sum-of-squares hierarchy on the sphere, and applications in
quantum information theory. Mathematical Programming , 190(1-2):331–360, November 2021.
arXiv:1908.05155 [quant-ph].
[23] Lucas Slot. Sum-of-Squares Hierarchies for Polynomial Optimization and the Christoffel–
Darboux Kernel. SIAM Journal on Optimization , 32(4):2612–2635, December 2022.
[24] Ralph Tyrell Rockafellar. Convex Analysis . Princeton Landmarks in Mathematics and Physics.
Princeton University Press, Princeton, NJ, 2015.
[25] Stephen Boyd and Lieven Vandenberghe. Convex Optimization . Cambridge University Press, 1
edition, March 2004.
[26] Lieven Vandenberghe. Chordal Graphs and Sparse Semidefinite Optimization, 2017.
[27] Yian Chen, Yuehaw Khoo, and Lek-Heng Lim. Convex Relaxation for Fokker-Planck, June
2023. arXiv:2306.03292 [cs, math].
[28] Francis Bach. Sums-of-squares for dummies: a view from the Fourier domain – Machine
Learning Research Blog, November 2022.
11[29] Markus Schweighofer. On the complexity of Schmüdgen’s Positivstellensatz. Journal of
Complexity , 20(4):529–543, August 2004.
[30] Frank H. Clarke. Generalized gradients and applications. Transactions of the American
Mathematical Society , 205:247–262, 1975.
[31] Corbinian Schlosser, Matteo Tacchi, and Alexey Lazarev. Convergence rates for the moment-
SoS hierarchy, February 2024. arXiv:2402.00436 [math].
[32] Milan Korda, Victor Magron, and Rodolfo Ríos-Zertuche. Convergence rates for sums-of-
squares hierarchies with correlative sparsity. Mathematical Programming , March 2024.
[33] Anthony Man-Cho So and Yinyu Ye. Theory of semidefinite programming for Sensor Network
Localization. Mathematical Programming , 109(2-3):367–384, January 2007.
[34] Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ond ˇrejˇCertík, Sergey B. Kirpichev,
Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rath-
nayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta,
Shivam Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Št ˇepán
Rouˇcka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony Sco-
patz. Sympy: symbolic computing in python. PeerJ Computer Science , 3:e103, January
2017.
[35] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for
convex optimization. Journal of Machine Learning Research , 17(83):1–5, 2016.
[36] Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system
for convex optimization problems. Journal of Control and Decision , 5(1):42–60, 2018.
[37] MOSEK ApS. The MOSEK optimization toolbox for Python manual. Version 10.0. , 2023.
[38] PC Mahalanobis. On the generalized distance in statistics. In Proceedings National Institute of
Science of India , volume 49, pages 234–256, 1936. Issue: 2.
[39] Ryan O’Donnell. SOS is not obviously automatizable, even approximately. R. O, 2016.
[40] Matthew B. Hastings. Perturbation Theory and the Sum of Squares, June 2022.
arXiv:2205.12325 [cond-mat, physics:hep-th, physics:quant-ph].
[41] M. B. Hastings. Field Theory and The Sum-of-Squares for Quantum Systems, February 2023.
arXiv:2302.14006 [quant-ph].
[42] Samuel B. Hopkins. Statistical Inference and the Sum of Squares Method . PhD Thesis, Cornell
University, August 2018.
[43] Samuel B. Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In
Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing , pages
1021–1034, Los Angeles CA USA, June 2018. ACM.
[44] Saeid Sedighi, Kumar Vijay Mishra, M. R. Bhavani Shankar, and Bjorn Ottersten. Localization
With One-Bit Passive Radars in Narrowband Internet-of-Things Using Multivariate Polynomial
Optimization. IEEE Transactions on Signal Processing , 69:2525–2540, January 2021. ADS
Bibcode: 2021ITSP...69.2525S.
[45] Dimitris Bertsimas, Dan Andrei Iancu, and Pablo A. Parrilo. A Hierarchy of Near-Optimal
Policies for Multistage Adaptive Optimization. IEEE Transactions on Automatic Control ,
56(12):2809–2824, December 2011.
[46] Luigi Ambrosio, Nicola Gigli, and Savare. Gradient Flows in Metric Spaces and in the Space
of Probability Measures . Birkhäuser, second edition, 2005.
[47] Ivar Ekeland and Roger Temam. Convex analysis and variational problems, 1976.
[48] D. Jackson. The Theory of Approximation . Colloquium Publications. American Mathematical
Society, 1930.
12[49] Nathan Krislock and Henry Wolkowicz. Explicit Sensor Network Localization using Semidefi-
nite Representations and Facial Reductions, January 2010. arXiv:1002.0013 [math].
[50] Brian D. O. Anderson, Peter N. Belhumeur, Tolga Eren, David K. Goldenberg, A. Stephen
Morse, Walter Whiteley, and Y . Richard Yang. Graphical properties of easily localizable sensor
networks. Wireless Networks , 15(2):177–191, February 2009.
[51] Brian D. O. Anderson, Iman Shames, Guoqiang Mao, and Bari¸ s Fidan. Formal Theory of Noisy
Sensor Network Localization. SIAM Journal on Discrete Mathematics , 24(2):684–698, January
2010.
13A Appendix / supplemental material
A.1 Notation
LetP(X)andP(Ω)denote the spaces of polynomials on X⊆RnandΩ⊆Rd, respectively,
where XandΩare (not-necessarily compact) subsets of their respective ambient spaces RnandRd.
Specifically, all polynomials of the forms below belong to their respective spaces:
p(x) =X
α∈Z≥0cαxα∈ P(X), p(ω) =X
α∈Z≥0cαωα∈ P(Ω)
where x= (x1, . . . , x n), ω= (ω1, . . . , ω d),αis a multi-index for the respective spaces, and cαare
the polynomial coefficients.
LetPd(S)for some S∈ {X,Ω}denote the subspace of P(S)consisting of polynomials of degree
≤d, i.e. polynomials where the multi-indices of the monomial terms satisfy ||α||1≤d.PSOS(X×Ω)
refers to the space of polynomials on X×Ωthat can be expressible as a sum-of-squares in xand
ωjointly. Additionally, W≽0for a matrix Wdenotes that Wis symmetric positive semidefinite
(PSD). Finally, P(Ω)denotes the set of Lebesgue probability measures on Ω.
A.2 Related work
A.2.1 Sum-of-squares theory and practice
The theoretical justification underlying the SDP relaxations in global optimization we use here derive
from the Positivstellensätz (positivity certificate) of [ 7], a representation theorem guaranteeing that
strictly positive polynomials on certain sets admit sum-of-squares representations. Following this,
[3,10,11] developed the Moment-SOS hierarchy, describing a hierarchy of primal-dual SDPs (each
having fixed degree) of increasing size that provides a monotonic non-decreasing sequence of lower
bounds.
There is rich theory underlying the SOS hierarchy combining disparate results from algebraic
geometry [ 14,10,11], semidefinite programming [ 15,18], and complexity theory [ 16,39]. The
hierarchy exhibits finite convergence in particular cases where convexity and a strict local minimum
are guaranteed [ 5], otherwise converging asymptotically [ 6]. In practice, the hierarchy often does
even better than these guarantees, converging exactly at c∗
sfor some small s.
The SOS hierarchy has found numerous applications in wide-ranging fields, including: reproducing
certain results of perturbation theory and providing useful lower-bound certifications in quantum field
theory and quantum chemistry [ 40,41], providing better provable guarantees in high-dimensional sta-
tistical problems [ 42,43], useful applications in the theory and practice of sensor network localization
[15, 44] and in robust and stochastic optimization [45].
Due to the SDP relaxation, the SOS hierarchy is quite powerful. This flexibility comes at a cost,
primarily in the form of computational complexity. The SDP prominently features a PSD matrix
W∈Ra(n,d,s )×a(n,d,s )witha(n, d, s )scaling as n+d+s
s
forndimensions and maximum degree
s. Without exploiting the structure of the polynomial, such as locality (coupled terms) or sparsity,
solving the SDP using a standard interior point method becomes prohibitively expensive for moderate
values of sorn. Work attempting to improve the scalability of the core ideas underlying the SOS
hierarchy and the SDP method include [17, 18].
A.2.2 Stochastic sum-of-squares and parametric polynomial optimization
The S-SOS hierarchy we present in this paper as a solution to parametric polynomial optimization was
presented originally by [ 19] as a “Joint + Marginal” approach. That work provides the same hierarchy
of semidefinite relaxations where the sequence of optimal solutions converges to the moment vector
of a probability measure encoding all information about the globally-optimal solutions x∗(ω) =
argminxf(x, ω)and provides a proof that the dual problem (our primal) obtains a polynomial
approximation to the optimal value function that converges almost-uniformly to c∗(ω).
14A.2.3 Uncertainty quantification and polynomial chaos
Once a physical system or optimization problem is characterized, sensitivity analysis and uncertainty
quantification seek to quantify how randomness or uncertainty in the inputs can affect the response.
In our work, we have the parametric problem of minimizing a function f(x, ω)overxwhere ω
parameterizes the function and is drawn from some noise distribution ν(ω).
If only function evaluations f(x, ω)are allowed and no other information is known, Monte Carlo is
often applied, where one draws ωk∼ν(ω)and solves many realizations of infxfk(x) =f(x, ωk)
to approximately solve the following stochastic program:
f∗= inf
xEω∼ν[f(x, ω)]
Standard Monte Carlo methods are ill-suited for integrating high-dimensional functions, so this
method is computationally challenging in its own right. In addition, we have no guarantees on our
result except that as we take the number of Monte Carlo iterates T→ ∞ we converge to some
unbiased estimate of Eω∼ν[f(x, ω)].
Our approach to quantifying the uncertainty in optimal function value resulting from uncertainty
in parameters ωis to find a deterministic lower-bounding c∗(ω)which guarantees f(x, ω)≥c∗(ω)
no matter the realization of noise. This is reminiscent of the polynomial chaos expansion literature,
wherein a system of some stochastic variables is expanded into a deterministic function of those
stochastic variables, usually in some orthogonal polynomial basis [20, 21].
A.3 An example
Example A.1. Letf(x, ω)be some polynomial of degree ≤2swritten in the standard monomial
basis, i.e.
f(x, ω) =X
||α||1≤2sfαxα
=X
||α||1≤2sf(α1,...,α n+d)nY
i=1xαi
idY
i=1ωαn+i
i
Letms(x, ω)∈Ra(n,d,s )be the basis vector representing the full set of monomials in x, ω of degree
≤switha(n, d, s ) = n+d+s
s
.
For all α∈Zn+d
≥0with||α||1≤2sandαk= 0 for all k∈ {1, . . . , n }(i.e. monomial terms
containing only ω1, . . . , ω d) we must have:
Z
X×Ωωαdµ(x, ω)−Z
Ωωαdν(ω) = 0
Explicitly, for µto be a valid probability distribution we must have:
Z
X×Ωdµ(x, ω)−1 =M0,0−1 =y(1,0,...)−1 = 0
Suppose Ω = [−1,1], ω∼Uniform (−1,1)so that d= 1,ν(ω) = 1 /2. We require:
Z
X×Ωωαdµ(x, ω) =Z
[−1,1]ωαdν(ω) =

1α= 0
0α= 1
1
3α= 2
0α= 3
1
5α= 4
15A.4 Strong duality
To guarantee strong duality theoretically, we need a strictly feasible point in the interior (Slater’s
condition). For us, this is a consequence of Putinar’s Positivstellensatz, if f(x, ω)admits a decom-
position as f(x, ω) =c(ω) +g(x, ω)where g(x, ω)>0(i.e. is strictly positive), we have strong
duality, i.e. p∗=d∗andp∗
2s=d∗
2s[3,8]. However, it is difficult to verify the conditions analytically.
In practice, strong duality is observed in most cases, so in this paper we refer to solving the primal and
dual interchangeably, as p∗
2s=d∗
2sin all cases we encounter where a SDP solver returns a feasible
point.
A.5 Proofs
A.5.1 Primal-dual relationship of S-SOS
Regular SOS Global polynomial optimization can be framed as the following lower-bound maxi-
mization problem where we need to check global non-negativity:
sup
c∈Rc (9)
s.t.f(x)−c≥0∀x
When we take the SOS relaxation of the non-negativity constraint in the primal, we now arrive at the
SOS primal problem, where we require f(x)−cto be SOS which guarantees non-negativity but is a
stronger condition than necessary:
sup
c∈Rc (10)
s.t.f(x)−c∈ PSOS(X).
The dual to Equation (9) is the following moment-minimization problem:
inf
µ∈P(X)Z
f(x)dµ(x) (11)
withZ
dµ(x) = 1 .
Taking some spanning basis ms(x) :Rn→Ra(n,s)of monomials up to degree s, we have the
moment matrix M∈Ra(n,s)×a(n,s):
Mi,j=Z
mi(x)mj(x)dµ(x) =yα
where we introduce a moment vector ywhose elements correspond to the unique moments of the
matrix M. Then we may write the degree- 2smoment-minimization problem, which is now in a
solvable numerical form:
inf
yX
αfαyα (12)
with M(y)1,1= 1
M(y)≽0
where we write M(y)as the matrix formed by placing the moments from yinto their appropriate
places and we set the first element of ms(x)to be 1, hence M1,1=R
dν(x) = 1 is simply the
normalization constraint. For further reading, see [15, 3].
Stochastic SOS Now let us lift this problem into the stochastic setting with parameters ωsampled
from a given distribution ν, i.e. replacing x→(x, ω). We need to make some choice for the objective.
The expectation of the lower bound under ν(ω)is a reasonable choice, i.e.
Z
Ωc(ω)dν(ω)
16but we could also make other choices, such as ones that encourage more robust lower bounds. In this
paper however, we formulate the primal S-SOS as below (same as Equation (1)):
p∗= sup
c∈L1(Ω)Z
c(ω)dν(ω) (13)
s.t.f(x, ω)−c(ω)≥0
Note that if the ansatz space for the function c(ω)is general enough, the maximization of the curve c
is equivalent to a pointwise maximization, i.e. we recover the best approximation for almost all ω.
Then the dual problem has a very similar form to the non-stochastic case.
Theorem A.2. The dual to Equation (13) is the following moment minimization where µ(x, ω)is a
probability measure on X×Ω:
inf
µ∈P(X×Ω)Z
f(x, ω)dµ(x, ω)
withZ
X×Ωωαdµ(x, ω) =Z
Ωωαdν(ω)for all α∈Nd.
Remark A.3.Notice, that the conditionR
X×Ωωαdµ(x, ω) =R
Ωωαdν(ω)implies that the first
marginal of µis the noise distribution ν. Letµωdenote the disintegration of µwith respect to ν, [46].
Then the moment matching condition is equivalent to µω(X) = 1 for almost all ωandµbeing a
Young measure w.r.t. ν. The idea is that µω(x)is a minimizing density for every single configuration
ofω.
Proof. We use P≥0(X×Ω)to denote the space of non-negative polynomials on X×Ω. Given
measure νonΩand polynomial function p:X×Ω→Rconsider
sup
γ∈L1(Ω,ν)
q∈P≥0(X×Ω).Z
Ωγ(ω)dν(ω)
s.tp(x, ω)−γ(ω) =q(x, ω)
This is equivalent to
− inf
γ∈L1(Ω,µ)
q∈P≥0(X×Ω)f(γ, q) +g(γ, q)
with
f(γ, q) =−Z
Ωγ(ω)dν(ω)
and
g(γ, q) =−χ{f−γ−q=0}=0iff−γ−q= 0
−∞ else,
i.e.gis the characteristic function enforcing non-negativity.
Denote by h∗the Legendre dual, i.e.
h∗(y) = sup
x⟨x, y⟩ −h(x).
Then by Rockafellar duality, [ 47,24], and noting that signed Borel measures Bare the dual to
continuous functions, the dual problem reads
sup
Γ∈L∞(Ω,µ),µ∈B−f∗(Γ, µ)−g∗(−(Γ, ν))
and we would have
sup
Γ∈L∞(Ω,µ),µ∈B−f∗(Γ, µ)−g∗(−(Γ, µ)) =− inf
γ∈L1(Ω,µ)
q∈P≥0(X×Ω)f(γ, q) +g(γ, q).
17The Legendre duals of fandgcan be explicitly calculated as
f∗(Γ, µ) =0ifΓ =−1andµ≤0
∞else
and
g∗(Γ, µ) =

Z
Ω×Xf(x, ω)dµ(ω, x)iff−γ∈ P≥0(X×Ω)andΓ(ω) =µω(X)
∞ else
since
f∗(Γ, µ) = sup
γ,qZ
Ωγ(ω)Γ(ω)dν(ω) +Z
Ω×Xq(x, ω)dµ(x, ω)−f(γ, q)
= sup
γ,qZ
Ωγ(ω)(Γ(ω) + 1)d ν(ω) +Z
Ω×Xq(x, ω)dµ(x, ω)
=0ifΓ =−1andµ≤0
∞ else
and
g∗(Γ, µ) = sup
γ,qZ
Ωγ(ω)Γ(ω)dν(ω) +Z
Ω×Xq(x, ω)dµ(ω, x) +χ{f−γ−q=0}
=

sup
γZ
Ωγ(ω)Γ(ω)dν(ω) +Z
Ω×X(f(x, ω)−γ(ω))dµ(ω, x) iff−γ=∈ P≥0(X×Ω)
∞ else
=

sup
γZ
Ωγ(ω)(Γ(ω)−µω(X))dν(ω) +Z
Ω×X(f(x, ω)dµ(ω, x) iff−γ∈ P≥0(X×Ω)
∞ else
=

Z
Ω×X(f(x, ω)dµ(ω, x) iff−γ∈ P≥0(X×Ω)andΓ(ω) =µω(X)
∞ else
Altogether, we get
−f∗(Γ, µ)−g∗(−Γ,−µ) =

Z
Ω×Xf(x, ω)dµ(ω, x) ifµω(X) = 1
∞ else.
A.5.2 Convergence of S-SOS hierarchy
c∗(ω)is at best Lipschitz continuous By Proposition 2.1, we argue that c∗(ω) = inf xf(x, ω)is
Lipschitz continuous. One cannot expect much more as the following example shows:
Example A.4. Consider g:R×R2→Rdefined by
g(x, p, q ) = (x2+px+q)2.
Then we have for every (p, q)∈R2that
inf
x∈Rg(x, p, q ) =(
0 ifp2
4≥q
(p2
4−q)2else.
Therefore, (p, q)7→infx∈Rg(x, p, q )is once differentiable but not twice.
18Lemma on approximating polynomials
Lemma A.5. LetΩbe a compact subset of Rnandg: Ω→Rbe Lipschitz continuous. Then there
exists a trigonometric polynomial gsof degree sand a constant C >0depending only on Ωandn
such that
g≥gs
and
∥g−gs∥L1(Ω)≤1 + ln( s)
sC∥g∥H1(Ω).
Proof. By Jackson’s inequality for Lipschitz functions [ 48] we have the existence of a trigonometric
polynomial g′of degree swith
∥g−g′∥L1(Ω)≤C′
s∥g∥H1(Ω)
as well as
∥g−g′∥L∞(Ω)≤Lgln(s)
s.
Then we define gs=g′− ∥g−g′∥L∞(Ω)and hence g≥gs. Furthermore,
∥g−gs∥L1(Ω)≤(C′+|Ω|lns)
sLg.
Writing C(Ω) = max {C′,|Ω|}we have the desired form where |Ω|is the volume of Ω.
Convergence at lns/srate using integral operator methodology
Theorem A.1 (Asymptotic convergence of S-SOS) .Letf: [0,1]n×Ω→Rbe a trigonometric
polynomial of degree 2r,c∗(ω) = inf xf(x, ω)the optimal lower bound as a function of ω, and ν
any probability measure on compact Ω⊂Rd. Lets= (sx, sω, sc), referring separately to the degree
of the basis in xterms, the degree of the basis in ωterms, and the degree of the lower-bounding
polynomial c(ω).
Letc∗
2s(ω)be the lower bounding function obtained from the primal S-SOS SDP with ms(x, ω)a
spanning basis of trigonometric monomials with degree ≤sxinxterms and of degree ≤sωinω
terms:
p∗
2s= sup
c∈P2sc(Ω),W≽0Z
c(ω)dν(ω)
s.t.f(x, ω)−c(ω) =ms(x, ω)TWm s(x, ω)
Then there is a constant C >0depending only on Ω, d,andnsuch that for all sω, sx≥max{3r,3sc}
the following holds:Z
Ω[c∗(ω)−c∗
2s(ω)] dν(ω)≤ |Ω|ϵ(f, s)
ε(f, s)≤∥f−¯f∥F"
1−
1−6r2
s2ω−d
1−6r2
s2x−n#
+∥c∗−¯c∗∥F"
1−
1−6r2
s2ω−d#
+C(1 + ln(2 sc))
2sc.
where ¯fdenotes the average value of the function fover [0,1]n, i.e. ¯f=R
[0,1]nf(x)dxand
||f(x)||F=P
ˆx|ˆf(ˆx)|denotes the norm of the Fourier coefficients.
ϵ(f, s)bounds the expected error, giving us asymptotic convergence as s= min( sx, sω, sc)→ ∞ .
Note the first two terms give a O(1
s2)convergence rate. However, the overall error will be dominated
by the degree of c(ω)(from the third term) hence our convergence rate is O(lns
s).
19Proof of Theorem A.1. LetΩ⊂Rdbe compact and f:Rn×Ω→Rbe a 1-periodic trigonometric
polynomial (t.p.) of degree ≤2r. We then make Ωisomorphic to [0,1]dand hereafter consider
Ω = [0 ,1]dandf: [0,1]n×[0,1]d→R. Letε >0andb=ε
2. Let the best lower bound be
c∗(ω) = inf
x∈Xf(x, ω).
Proof outline. We split the error into two parts. First, we use the fact that there is a lower-bounding
t.p.c∗
aof degree scsuch that
∥c∗−c∗
a∥ ≤C1 + ln sc
sc
and
c∗≥c∗
a.
This will provide us with a degree- sct.p. approximation to the lower bounding function, which in
general is only known to be Lipschitz continuous.
Next, we show, that for any b >0there is a degree- 2sSOS t.p. fSOS(x, ω)such that
fSOS=f−(c∗
a−b).
We write s= (sx, sω)where sx, sωdenotes the respective max degrees in the variables x, ω. Once we
have constructed this, we can compute f−fSOS=c∗
a−εand since we know that fSOS≥0everywhere
andc∗
a−εis some degree- sct.p. we have found a degree- sclower-bounding t.p. The construction of
this SOS t.p. adds another error term. If we can drive ε→0as¯s= min( sx, sω, sc)→ ∞ then we
are done.
Proof continued. To that end, let c∗
a: Ω→Rbe the best degree- sctrigonometric approximation of
c∗with respect to L1such that
c∗≥c∗
a.
By [30], we know that c∗is locally Lipschitz continuous with Lipschitz constant Lc∗and hence, by
Lemma A.5 we get that there is C(Ω)>0such that
∥c∗−c∗
a∥L1Ω)≤C(Ω)1 + ln sc
scLc∗.
Next we introduce c∗
2s(ω)which is some degree- 2st.p. After an application of the triangle inequality
and Cauchy-Schwarz on the integrated error termR
Ω|c∗−c∗
2s|dωwe have
Z
Ωinf
x∈Xf(x, ω)−c∗
2s(ω)dω≤Z
Ω|c∗
a(ω)−c∗
2s(ω)|dω+|Ω|∥c∗−c∗
a∥L2(Ω)
Z
Ωinf
x∈Xf(x, ω)−c∗
2s(ω)dω≤Z
Ω|c∗
a(ω)−c∗
2s(ω)|dω
| {z }
gap between some SDP solution c∗
2s(ω)and t.p. c∗
a(ω)+C(Ω)1 + ln sc
scLc∗
| {z }
approx. error of L-contin. fn.
Now we want to show that for any ε >0we can construct a degree- 2sSOS trigonometric polynomial
fSOS(x, ω)such that
fSOS=f−c∗
a+b.
withb=ε/2ands= (sx, sω)> r. We can then set f−fSOS=c∗
a−b=c∗
2sas the degree- 2s
lower-bounding function. If we can drive b=ε/2→0ass, sc→ ∞ we are done, as by construction
|c∗
a−c∗
2s|=b.
Observe that by assumption f−c∗
a+bis a t.p. in (x, ω)where fis degree- 2randc∗
ais degree
sc≥2r. Denote by (f−fa
∗+b)ωits coefficients w.r.t the ωbasis. Note that the coefficients are
functions in x. Following the integral operator proof methodology in [ 6], define the integral operator
Tto be
Th(x, ω) =Z
X×Ω|qω(ω−¯ω)|2|qx(x−¯x)|2h(¯x,¯ω)d¯xd¯ω,
where qωis a trigonometric polynomial in ωof degree ≤sωandqxis a trigonometric polynomial
inxof degree ≤sx. The intuition is that this integral operator explicitly builds a SOS function of
degrees (sx, sω)out of any non-negative function hby hitting it against the kernels qx, qω.
20We want to find a positive function h:X×Ω→Rsuch that
Th=f−c∗
a+b.
In frequency space, the Fourier transform turns a convolution into pointwise multiplication so we
have:
cTh(ˆx,ˆω) = ˆqω∗ˆqω(ˆω)·ˆqx∗ˆqx(ˆx)·ˆh(ˆx,ˆω).
In the Fourier domain it is easy to write down the coefficients of ˆh:
ˆh(ˆx,ˆω) =

0 if∥ˆx,ˆω∥∞>max{2r,2sc}
ˆf(ˆx,ˆω)−ˆc∗
a(ˆω)1ˆx=0+b1ˆx=01ˆω=0
ˆqω∗ˆqω(ˆω)·ˆqx∗ˆqx(ˆx)otherwise .
Computing Th−hgives:
f(x, ω)−c∗
a(ω) +b−h(x, ω)
=X
ˆω,ˆxˆf(ˆx,ˆω)
1−1
ˆqω∗ˆqω(ˆω)·ˆqx∗ˆqx(ˆx)
exp(2 iπˆωTω) exp(2 iπˆxTx)
+X
ˆω(b1ˆω=0−c∗
a)
1−1
ˆqω∗ˆqω(ˆω)
exp(2 iπˆωTω)
and thus after requiring ˆqω∗ˆqω(0) = ˆ qx∗ˆqx(0) = 1 we have:
max
x,ω|f(x, ω)−c∗
a(ω) +b−h(x, ω)|
≤∥f−¯f∥Fmax
ˆω̸=0max
ˆx̸=01−1
ˆqω∗ˆqω(ˆω)·ˆqx∗ˆqx(ˆx)
+ max
ˆω̸=0∥c∗
a−¯c∗
a∥F1−1
ˆqω∗ˆqω(ˆω).
As a reminder, because c∗≥c∗
aeverywhere we have f−ca≥f−c∗≥0orf−c∗
a+b >0, since
b=ε/2>0. Since Th=f−c∗
a+b >0and it is a SOS, we need to guarantee h >0.
Ifmax x,ω|f(x, ω)−fa
∗(ω) +b−h(x, ω)| ≤bthen
max
x,ω|Th−h|< b.
Since Th≥bandb >0we have
h=Th+h−Th≥Th− ∥h−Th∥∞≥b−b≥0
and hence h >0if we ensure max x,ω|Th−h| ≤b.
Now let us show that
max
x,ω|f(x, ω)−c∗
a(ω) +b−h(x, ω)| ≤b
can be ensured if s= (sx, sω)is large enough.
Using the same kernel and bounds as in [ 6], we choose for z∈ {x, ω}the triangular kernel such that
ˆqz(ˆz) =
1−6r2
z2d
+dY
i=1
1−|ˆzi|
sx,ω
+.
Note that (x)+= max( x,0). Then we have
max
x|f(x, ω)−c∗
a(ω) +b−h(x, ω)|
≤∥f−¯f∥Fmax
ˆω,ˆx1−1
ˆqω∗ˆqω(ˆω)·ˆqx∗ˆqx(ˆx)+∥c∗
a−¯c∗
a∥Fmax
ˆω1−1
ˆqω∗ˆqω(ˆω)
≤∥f−¯f∥F1−
1−6r2
s2ω−d
1−6r2
s2x−n+∥c∗
a−¯c∗
a∥F1−
1−62
s2ω−d
21Therefore, by choosing sωandsxlarge enough such that
∥f−¯f∥F1−
1−6r2
s2ω−d
1−6r2
s2x−n+∥c∗
a−¯c∗
a∥F1−
1−62
s2ω−d≤b=ε
2
we have
h≥0
and thus This SOS. By design we have
c∗
a−c∗
2s≤b
and thus Z
Ω|c∗
a−c∗
2s|dω≤ε
2.
Recalling
Z
Ωinf
x∈Xf(x, ω)−c∗
2s(ω)dω≤Z
Ω|c∗
a(ω)−c∗
2s(ω)|dω
| {z }
gap between some SDP solution c∗
2s(ω)and t.p. c∗
a(ω)+C(Ω)1 + ln sc
scLc∗
| {z }
approx. error of L-contin. fn.
we can additionally choose sclarge enough to guarantee
C(Ω)1 + ln sc
scLc∗≤ε
2
and then we are done.
Setting sx, sω, sc=sand sending s→ ∞ we have asymptotic behavior of the final error expression:
Z
Ωinf
x∈Xf(x, ω)−c∗
2s(ω)dω≤C11
s2+C21
s+C3lns
s=Olns
s
with the constants C1, C2, C3depending on r, n, d, ∥f−¯f∥F,∥ca−¯c∗
a∥F,ΩandLc∗.
Convergence at 1/srate using piecewise-constant approximation to c∗(ω)Prior work [ 6]
achieves 1/s2convergence for the regular SOS hierarchy without further assumptions. In our
work, we could only achieve lns/sfor S-SOS due to the need to first approximate the tightest
lower-bounding function c∗(ω)with a polynomial approximation, which converges at a slower rate.
To accelerate the convergence rate, we want to control the regularity of c∗(ω). We can achieve 1/s
by approximating the c∗(ω)pointwise instead of using a smooth parameterized polynomial. By
constructing a domain decomposition of Ωand finding a SOS approximation in xfor each domain, we
can stitch these together to build a piecewise-constant approximation to the lower-bounding function
c∗. This lets us leverage the 1/s2rate for the standard SOS hierarchy, however note that we lose
the guarantee that the lower-bounding function c∗
s(ω)is a true lower bound everywhere. Another
disadvantage of this approach is that the computational complexity scales exponentially in das we
are required to solve for the SOS lower bound with degree spointwise at O(sd
p)points.
ForΩ = [0 ,1]d⊂Rdwe achieve the following:
Proposition A.6. LetΩ = [0 ,1]dandf: [0,1]n→Rbe a trigonometric polynomial of degree 2r.
Let{ωi}be grid points where spis the number of equally-spaced points along each dimension for a
total of (sp)dpoints in Ω. Denote by c∗
s(ωi)the best SOS approximation of degree sofx7→f(x, ωi)
at each grid point ωiand define
c∗
s(ω) =sd
pX
i=1c∗
s(ωi)1ω∈B∞(ωi,ϵ)
where B∞(ωi, ϵ)denotes the L∞ball of radius ϵcentered at ωiwithϵ= (2sp)−1so that the balls
are non-overlapping. Then we have for some constant C′depending only on max ωi||f(ωi,·)−
¯f(ωi,·)||F, r, n, d, s p:
Z
Ω[c∗(ω)−c∗
s(ω)]dω≤max
ωi∥f(ωi,·)−¯f(ωi,·)∥F"
1−
1−6r2
s2−n#
+C√
d
sp≤C′
s.
22Proof of Proposition A.6. Letc∗
a(ω) : [0,1]d→Rbe the best piecewise-constant approximation of
the true lower-bounding function c∗(ω) = inf xf(x, ω)on equidistant grid-points, i.e. spevenly-
spaced points in each dimension for a total of sd
ppoints.
Now also define a piecewise-constant approximation using the best pointwise SOS lower bounds:
c∗
s(ω) =sd
pX
i=1c∗
s(ωi)1ω∈B∞(ωi,ϵ)
where c∗
s(ωi)is the best lower bound (resulting from regular SOS) of degree sofx7→f(x, ωi)and
B∞(ωi, ϵ)denotes the L∞ball of radius ϵcentered at ωiwithϵ= (2sp)−1so that the balls are
non-overlapping.
By [6] we have that c∗
a(ωi)−c∗
s(ωi)can be bounded by
max
ωi∥f(ωi,·)−¯f(ωi,·)∥F 
1−
1−6r2
s2−n!
.
Then we have:
Z
Ω[c∗(ω)−c∗
s(ω)]dω≤sd
pX
i=1|c∗
a(ωi)−c∗
s(ωi)|(2ϵ)d+∥c∗−c∗
a∥L1(Ω).
Using the same bound we get for the first term from the proof of Theorem A.1, we can reduce the first
term to a O(1/s2)dependence and we use the theorem on the L1convergence of piecewise-constant
approximation to 1-periodic trigonometric polynomials from [48] for the second:
Z
Ω[c∗(ω)s−c∗
s(ω)]dω≤max
ωi∥f(·, ωi)−¯f(·, ωi)∥F 
1−
1−6r2
s2−n!
|Ω|+C√
d
sp
It is straightforward to extend this to arbitrary compact Ω⊂Rd, as one can construct a smooth
homeomorphism between Ωand[0,1]dand therefore we can achieve the same convergence rates on
general compact Ωas we can for [0,1]d.
A.6 S-SOS for a simple quadratic potential
We provide a simple application of S-SOS to a simple quadratic potential that admits a closed-form
solution so as to demonstrate its usage and limitations.
A.6.1 Analytic solution for the lower bounding function c∗(ω)withω∼Uniform (−1,1)
Letx∈Randω∼Uniform (−1,1). Suppose that we have
f(x, ω) = (x−ω)2+ (ωx)2
In this case we may explicitly evaluate the exact minimum function c∗(ω) = inf xf(x;ω). Note that
f(x;ω) =x2−2ωx+ω2+ω2x2
Explicitly evaluating the zeros of the first derivative we have
∂xf(x;ω) = 2 x∗−2ω+ 2ω2x∗= 0
x∗(1 +ω2) =ω
x∗=ω
1 +ω2
and, thus,
c∗(ω) = inf
xf(x;ω) =ω4
1 +ω2.
Note that despite f(x, ω)being a simple degree-2 SOS polynomial, the tightest lower-bound c∗(ω) =
infxf(x, ω)is explicitly not polynomial. However, it is algebraic, as it is defined implicitly as the
root of the polynomial equation
c∗(ω)(1 + ω2)−ω4= 0
23Figure 2: Lower bound functions for basis function degree d= 2,4(left) and the optimality gap to
the true lower bound c∗(ω)−c∗
2s(ω)(right)
A.6.2 Degree- 2sS-SOS to find a polynomial lower-bounding function c∗
2s(ω)
Observe that the tightest lower-bounding function c∗(ω)is not polynomial even in this simple setting.
However, we can relax the problem to trying to find c2s∈ P2s(Ω)to obtain a weaker bound with
infxf(x, ω) =c∗(ω)≥c2s(ω).
We now proceed with formulating and solving the degree- 2sprimal S-SOS SDP (Equation (2)). We
assume that c2s(ω)is parameterized by a polynomial of degree ≤2sinω. Observe that this class of
functions is not large enough to contain the true function c∗(ω).
We choose s∈ {2,4}and use the standard monomial basis in x, ω, we have the feature maps
m2(x, ω) :R2→R6andm4(x, ω) :R2→R15, since there are n+s
s
unique monomials of up to
degree- sinnvariables. These assumptions together enable us to explicitly write a SOS SDP in terms
of coefficient matching. Note that we must assume some noise distribution ν(ω). For this section,
we present results assuming ω∼Uniform (−1,1). We solve the resulting SDP in CVXPY using
Legendre quadrature with k= 5zeroes on [−1,1]to evaluate the objectiveR
c(ω)dν(ω). In fact, k
sample points suffice to exactly integrate polynomials of degree ≤2k−1.
We solve the SDP for two different levels of the hierarchy, s= 2ands= 4(producing lower-bound
polynomials of degree 4and8respectively), and plot the lower bound functions c2s(ω)vs the true
lower bound c∗(ω) =ω4/(1 +ω2)as well as the optimality gap to the true lower bound in Fig.2.
A.6.3 Convergence of lower bound as degree sincreases
To solve the S-SOS SDP in practice, we must choose a maximum degree 2sfor the SOS function
m2(x, ω)TWm 2(x, ω)and the lower-bounding function c(ω), which are both restricted to be poly-
nomials. Indeed, a larger snot only increases the dimension of our basis function ms(x, ω)but also
the complexity of the resulting SDP. We would expect that d∗
2s→d∗ass→ ∞ , i.e. the optimal
value of the degree- 2sS-SOS SDP (Equation (4)) converges to that of the “minimizing distribution”
optimization problem (Equation (3)).
In particular, note that in the standard SOS hierarchy we typically find finite convergence (exact
agreement at some degree 2s∗<∞). However, in S-SOS, we thus far have only a guarantee of
asymptotic convergence, as each finite-degree S-SOS SDP solves for a polynomial approximation to
the optimal lower bound c∗(ω) = inf x∈Xf(x, ω). In Figure 1, we illustrate the primal S-SOS SDP
objective values
p∗
2s= sup
c∈P2s(Ω)Z
c(ω)dν(ω)with f(x, ω)−c(ω)∈ P2s
SOS(X×Ω)
24Figure 3: Different lower-bounding functions for degree-4 S-SOS done on the simple quadratic
potential f(x, ω) = (x−ω)2+ (ωx)2. The true lower-bounding function c∗(ω)is plotted in black.
for a given level of the hierarchy (a chosen degree sfor the basis ms(x, ω)) and their convergence
towards the optimal objective valueZ
c∗(ω)dν(ω) =π
4−2
3≈0.1187
for the simple quadratic potential, assuming ν(ω) =1
2withω∼Uniform (−1,1). We note that in
the log-linear plot (right) we have a “hinge”-type curve, with a linear decay (in logspace) and then
flattening completely. This suggests perhaps that in realistic scenarios the degree needed to achieve
a close approximation is very low, lower than suggested by our bounds. The flattening that occurs
here is likely due to the numerical tolerance used in our solver (CVXPY/MOSEK), as increasing the
tolerance also increases the asymptotic gap and decreases the degree at which the gap flattens out.
A.6.4 Effect of different noise distributions
In the previous two sections, we assumed that ω∼Uniform (−1,1). This enabled us to solve the
primal exactly using Legendre quadrature of polynomials. Note that in Figure 1 we see that the
lower-bounding c∗
2(ω), c∗
4(ω)forω∼Uniform (−1,1)is a smooth polynomial that has curvature (i.e.
sign matching that of the true minimum). This is actually not guaranteed, as we will see shortly.
In Figure 3, we present the lower-bounding functions c∗
4(ω)achieved by degree-4 S-SOS by solving
the dual for ω∼Normal (0, σ2)for varying widths σ. We can see that for small σ≪1, the
primal solution only cares about the lower-bound accuracy within a small region of ω= 0, and the
lower-bounding curve fails to “generalize” effectively outside the region of consideration.
A.7 S-SOS for sensor network localization
A.7.1 SDP formulation
Recall the form of f(x, ω):
f(x, ω;X, A, r ) =X
dij∈Dss(r)(||xi−xj||2
2−dij(ω)2)2
| {z }
sensor-sensor interactions+X
dik∈Dsa(r)(||xi−ak||2
2−dik(ω)2)2
| {z }
sensor-anchor interactions
25Note that the function f(x, ω)is exactly a degree-4 SOS polynomial, so it suffices to choose the
degree-2 monomial basis containing a= Nℓ+d+2
2
elements as m2(x, ω) :RNℓ+d→Ra. That is,
we have Nsensor positions in ℓspatial dimensions and dparameters for a total of Nℓ+dvariables.
Let the moment matrix be M∈Ra×awith elements defined as
Mi,j:=Z
m(i)
2(x, ω)m(j)
2(x, ω)dµ(x, ω)
fori, j∈ {1, . . . , a }, which fully specifies the minimizing distribution µ(x, ω)as in Equation (4).
Our SDP is then of the form
d∗
4= inf
yX
αfαyα
s.t.M(y)≽0
yα=mα∀(α, m α)∈ M ν
yα=y∗
α∀(α, y∗
α)∈ H
where yα=mαcorresponds to the moment-matching constraints of Equation (4) and yα=y∗
α
correspond to any possible hard equality constraints required to set the exact position (and uncertainty)
of a sensor E[xi] =x∗
i,E[x2
i]−E[xi]2= 0for all ω.Mνrepresents the d+2s
2s
moment-matching
constraints necessary for all moments w.r.t. ωandHrepresents the 2ℓnconstraints needed to set the
exact positions of nknown sensor positions in Rℓ(i.e. 1 constraint per sensor and dimension, 2 each
for mean and variance).
A.7.2 Noise types
In this paper we focus on the linear uniform noise case, as it is a more accurate reflection of
measurement noise in true SNL problems. Special robust estimation approaches may be needed to
properly handle the outlier noise case.
•Linear uniform noise : for a subset of edges we write dij,k(ω) = d∗
ij+ϵωk,ωk∼
Uniform (−1,1), and ϵ≥0some noise scale we set. The same random variate ωkmay
perturb any number of edges. Otherwise the observed distances are the true distances.
•Outlier uniform noise : for a subset of edges we ignore any information in the actual
measurement dij,k=ωk,ωk∼Uniform (0,2√
ℓ)where ℓis the physical dimension of the
problem, i.e. xi∈Rℓ.
A.7.3 Algorithms: S-SOS and MCPO
Here we explicitly formulate MCPO and S-SOS as algorithms. Let X=Rn,Ω =Rdand use the
standard monomial basis. We write z= [x1, . . . , x n, ω1, . . . , ω d]. Our objective is to approximate
c∗(ω) = inf xf(x, ω)for all ω, with a view towards maximizingR
c∗(ω)dν(ω)forωsampled from
some probability density ν(ω).
MCPO (Algorithm 1) simply samples ωtand finds a set of tuples (x∗(ωt), ωt)where the optimal
minimizer (x∗(ωt, ωt)is computed using a local optimization scheme (we use BFGS).
S-SOS (Algorithm 2) via solving the dual (Equation (4)) is also detailed below.
A.7.4 Cluster basis hierarchy
Recall from Section 2.2.2 that we defined the cluster basis hierarchy using body order band maximum
degree per variable t. In this section, we review the additional modifications needed to scale S-SOS
for SNL.
In SNL, f(x, ω)is by design a degree s= 4polynomial in z= [x, ω], with interactions of body order
b= 2(due to the (xi, xj)interactions) and maximum individual variable degree t= 4. Written this
way, we want to only consider monomial terms [x, ω]αwith||α||1≤s,||α||∞≤4, and||α||0≤2.
To sparsify our problem, we start with some k-clustering ( kclusters, mutually-exclusive) of the
sensor set C={C1, . . . , C k}. This clustering can be considered as leveraging some kind of “coarse“
26Algorithm 1 Monte Carlo Point Optimization (MCPO)
1:Input: Function f(x;ω), sampler for distribution ν(ω), number of samples T
2:Output: Approximate integral ˆI, empirical distribution pD(x), empirical mean µ, empirical
covariance Σ
3:fort= 1toTdo
4: Sample ωt∼ν(ω)
5: Find minimizer xt= min xf(x;ωt)using BFGS
6:end for
7:Estimate integral ˆI≈1
TPT
t=1f(xt;ωt)
8:Construct empirical distribution
pD(x) =1
TTX
t=1δ(x−xt)
9:Calculate empirical mean ˆµ=1
TPT
t=1xtand covariance ˆΣ =1
T−1PT
t=1(xt−ˆµ)(xt−ˆµ)T.
Algorithm 2 Stochastic Sum-of-squares (S-SOS), Dual formulation
1:Input: Maximum basis function degree s∈Z>0, complete basis function ms(x, ω) :Rn+d→
R(n+d+s
s), function f(x;ω) :Rn+d→Rrepresented as a dictionary mapping multi-index
α∈Zn+d
≥0→coefficient fα, probability density function for ν(ω)with known momentsR
ωαdν(ω)<∞ ∀ || α||1≤2s, any hard equality constraints where we want to set xk=x∗
kfor
some k∈ K.
2:Leti1, i2, i4be the lexicographically-ordered arrays
Z(n+d+1)×(n+d)
≥0 ,Z(n+d+s
s)×(n+d)
≥0 ,Z(n+d+2s
2s)×(n+d)
≥0
which correspond to the arrays of multi-indices for all degree-1, degree- s, and degree- 2smono-
mials in the variables z.
3:Create M∈R(n+d+s
s)×(n+d+s
s)as a matrix of variables to be estimated.
4:Create y∈R(n+d+2s
2s)as a vector of variables to be estimated, corresponding to the vector of
independent moments that fully specifies M.
5:AddM≽0constraint.
6:foriin length( i2)do
7: forjin length( i2)do ▷Require Mto be formed from the elements of y.
8: Compute αij=i2[i] +i2[j]as the multi-index corresponding to the sum of the multi-
indices i2[i], i2[j].
9: Add constraint Mi,j=yαij.
10: end for
11:end for
12:foreach row αini4do ▷Require yαmoments to equal the known moments of ωα.
13: ifPn
i=1αi= 0then
14: Add constraint yα=R
zαdν(ω) =R
ωα[−d:]dν(ω).
15: end if
16:end for
17:forkinKdo ▷Handle any hard equality constraints in our variables x.
18: Form multi-index α1∈Zn+d
≥0where the entry for xkis set to 1 and everything else is zero.
19: Form multi-index α2∈Zn+d
≥0where the entry for x2
kis set to 1 and everything else is zero.
20: Add constraint yα1=x∗
k. ▷E[xk] =x∗
k.
21: Add constraint yα2= (x∗
k)2. ▷Var[xk] =E[x2
k]−E[xk]2= 0.
22:end for
23:Form the objective to be minimized: F=R
f(x, ω)dµ(x, ω) =P
α∈i4fαyα.
24:Solve SDP where we compute infFsubject to above constraints.
25:Output: If the problem is feasible (i.e. there exists a degree- 2sdecomposition of fintofSOSand
c∗
2s(ω)), return moment matrix M∈R(n+d+s
s)×(n+d+s
s), dual objective value d∗
2s. Otherwise,
terminate and return failed/infeasible SDP solve.
27information about which sensors are close to each other. For example, just looking at the polynomial
f(x, ω)enables us to see which sensors (i, j)must be interacting.
Assume that there is some a priori clustering given to us. We denote x(i)as the subset of the variables
restricted to the cluster Ci, i.e.x(i)={xj:j∈Ci}. Moreover, let G= (V, E)be a graph where
the vertices V={1, . . . , k }correspond to the kclusters and the edges E={(i, j) :i, j∈V}
correspond to known cluster-cluster interactions.
The SOS part of the function f(x)may then be approximated as the sum of dense intra-cluster
interactions and sparse inter-cluster interactions, where the cluster-cluster interactions are given
exactly by edges in the graph G:
ms(x)TWm s(x)≈X
i∈Vms(x(i))TW(i)ms(x(i)) +X
(i,j)∈Ems(x(i))TW(i,j)ms(x(j))
where W(k)are symmetric PSD matrices and W(i,j)are rectangular matrices where we require
W(i,j)= (W(j,i))T.ms(x)forx∈Rnhere behaves as before and denotes the basis function
generated by all n+s
s
combinations of monomials with degree ≤s. Notice that this is a strict
reduction from the standard Lasserre hierarchy at the same degree s, since in general the standard
basis ms(x)on the full variable set will contain terms that mix variables from two different clusters
that may not have an edge connecting them.
Efficiency gains in the SDP solve occur when we constrain certain of the off-diagonal W(i,j)blocks
to be zero, i.e. the graph Gis sparse in cluster-cluster interactions. As we can see from the block
decomposition written above, this resembles block sparsity on the matrix W. We may interpret the
above scheme as having a hierarchical structure out to depth 2, where we have dense interactions at
the lowest level and sparse interactions aggregating them. In full generality, the resulting hierarchical
sparsity in Wmay be interpreted as generating a chordal W, which is known to admit certain
speed-ups in SDP solvers [26].
When attempting to solve an SNL problem in the cluster basis instead of the full basis, we need to
throw away terms in the potential f(x, ω)that correspond to cross-terms that are “ignored” by the
particular cluster basis we chose. The resulting polynomial ¯f(x, ω)has fewer terms and produces a
cluster basis SDP that is easier to solve, but generally less accurate due to the sparser connectivity.
In particular, for the rows in Table 1 that have NC>1, we do a NC-means clustering of the
ground-truth sensor positions and use those sensor labels to create our partitioning of the sensors.
We connect every cluster using plus-one ci, ci+1(including the wrap-around one) connections, so
that the cluster-cluster connectivity graph has NCedges. We then use this information to throw out
observed distances from the set Dssand from the full basis function m2(x, ω). See our code for
complete details.
A.7.5 Hard equality constraints
The sensor-anchor terms in Equation (7) are added to make the problem easier, because by adding
them now each sensor no longer needs to rely only on a local neighborhood of sensors to localize
itself, but can also use its position relative to some known anchor. When we remove them entirely, we
need to incorporate hard equality constraints between certain sensors and known “anchor” positions.
This fixes certain known sensors but lets every other sensor be unrooted, defined only relative to other
sensors (and potentially an anchor if it is within the sensing radius).
To deal with the equality constraints where we set the exact position of a sensor xi=x∗
i, we solve
the dual Equation (4) and implement them as equality constraints on the moment matrix, i.e. for the
basis element m2(x, ω)i=xiwe may set E[xi]−x∗
i=M0,i−x∗
i= 0. Note that we also need to
setVar(xi) = 0 so for m2(x, ω)j=x2
iwe add the equality constraint Var(xi) =E[x2
i]−E[xi]2=
M0,j−M2
0,i= 0.
A.7.6 Solution extraction
Once the dual SDP has been solved, we extract the moment matrix Mand can easily recover the
point and uncertainty estimates for the sensor positions E[x],Var[x]by inspecting the appropriate
entries M0,icorresponding to m2(x, ω)i=xiandM0,jcorresponding to m2(x, ω)j=x2
i.
28Figure 4: Comparison of the performance of MCPO and S-SOS (degree-4) for sensor recovery
accuracy in 1D SNL with varying number of samples Tused in the estimate of empirical ˆµ,ˆΣ.
M-distance is δM, our metric for sensor recovery accuracy per Equation (8). The problem type here is
aN= 5sensor, ℓ= 1spatial dimension, |Ω|= 2noise variables, ϵ= 0.1noise scale, r= 3sensing
radius problem. The full basis is used here for the S-SOS SDP.
A.7.7 Impact of using MCPO with varying numbers of samples T
In Figure 4 we can see how δMvaries as we scale the number of samples Tused in the MCPO estimate
of the empirical mean/covariance of the recovered solutions. In this particular example, the runtime
of the S-SOS estimate was 0.3 seconds, comparing to 30 seconds for the T= 300 MCPO point.
Despite taking 100x longer, the MCPO solution recovery still dramatically underperforms S-SOS
inδM. This reflects the poor performance of local optimization methods vs. a global optimization
method (when it is available).
A.7.8 Scalability
The largest 2D SNL experiment we could run had N= 15 sensors, NC= 9 clusters, and d= 9
noise parameters. This generated Nℓ+d= 39 variables and 820basis elements in the naive
m2(x, ω)construction, which was reduced to 317after our application of the cluster basis, giving us
W, M ∈R317×317. A single solve in CVXPY (MOSEK) took 30 minutes on our workstation (2x
Intel Xeon 6130 Gold and 256GB of RAM). We attempted a run with N= 20 sensors and NC= 9
clusters and d= 9noise parameters, but the process failed due to OOM constraints. Thus, we report
the largest experiment that succeeded.
A.8 Limitations
As discussed in the main text, our work provides both a theoretical convergence rate as well as
numerical understanding of the stochastic sum-of-squares (S-SOS). Notably, our convergence rate is
bottlenecked on the approximation quality of c∗(ω), only achieving lns/swhere regular SOS results
achieve 1/s2. In practice, we see much faster convergence, which may be a clue that other more
restrictive assumptions or more powerful methods may enable one to find better convergence results.
In addition, our numerical work focuses on the sensor network localization problem. Due to the time
complexity of SDPs, the maximum number of sensors we could solve in any one instance is limited
toN= 15 . This seems much smaller than the state-of-the-art, i.e. [ 49] where the authors propose an
algebraic reduction of the noiseless SNL problem so that they can analytically obtain the range of
the PSD matrix. This simplifies the SDP dramatically, however in the noisy setting (where observed
sensor-sensor distances are perturbed with observation noise) this approach is unusable, requiring
significant modification. his reduction is used to solve SNL problems of 10k-100k sensors, but they
develop a highly specialized algorithm that does not use any SDP solvers.
29Note that noiseless SNL simplifies the problem. In the noiseless setting, localizing even a small
number of sensors near an anchor will propagate the correctly localized positions to sensors nearby.
As such, if one can find small groups of sensors that can be well-oriented with respect to each other,
one can pursue localization of these groups in parallel and then orient them globally at the very end.
In the noisy setting, any error in localization can easily propagate to the whole instance, dramatically
reducing the size of problems that can be effectively solved [50, 51].
Thus, we observe that uncertainty makes the SNL problem considerably more difficult than the
noiseless setting. S-SOS handles this naturally while also solving the problem for its global optimum.
We get a solution for the sensor positions for every possible configuration of noise via the probability
distribution µ(x, ω). This additional complexity places further limits on the size of the problems that
can be solved with S-SOS.
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction (Section 1) summarize the contributions which
are principally an elucidation of a stochastic SOS hierarchy (an optimization procedure),
a proof (with listed assumptions) giving a quantitative rate of convergence, and numerical
experiments. The full proof, numerical plots and tables, as well as code necessary to replicate
our results are all provided.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See the Discussion section (Section 4), where we highlight the non-scalability
of all algorithms (including ours) that require optimization over the SDP cone, and dis-
cuss potential solutions, some of which we rectify in this paper by using sparsification
assumptions such as a “cluster basis”. Despite a quantitative convergence proof with a
clear rate, we also highlight the assumptions made, including the fact that we restrict to
1-periodic trigonometric polynomials over [0,1]n×ΩwithΩcompact (Section 2.3.2 and
Appendix A.5.2).
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
31•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: A full proof of our core result is provided in the supplementary material
(Appendix A.5.2), along with an outline of the core ideas in the main body (Section 2.3.2).
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All information regarding experiments is disclosed, with core details in the
main draft and full experimental details in the appendix. In addition, we provide code in our
submission (and will publicly release it upon paper decision) to reproduce all of the figures,
tables, and other results we report.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
32(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: Our code has been provided with our submission and a cleaned-up version
will be released publicly once the preprint is public.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All necessary details to replicate results are summarized in the core body and
explicated in full in supplementary material (Appendix A.7).
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
33Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Where necessary, we report confidence intervals on our solution recovery
metrics for SNL which are computed over multiple runs. In particular, we report robust
mean and 1-sigma robust std estimates and details can be found at Section 3.2.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Specific details regarding the compute resources used are detailed in Section 3,
as well a section on scalability in Appendix A.7.8.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: All authors have reviewed the NeurIPS Code of Ethics and certify that the
research presented conforms with all aspects of it.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
34•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: As foundational research on algorithms and optimization, we do not see
substantial societal impact positive or negative tied to possible applications of this research.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our contributions are primarily algorithmic and do not have a high risk for
misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
35Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All open-source software packages such as scientific computing libraries and
optimization libraries were cited.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No new assets were used, all data necessary for experiments was generated
from scratch.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Human subjects were not involved.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
3615.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Human subjects were not involved.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
37