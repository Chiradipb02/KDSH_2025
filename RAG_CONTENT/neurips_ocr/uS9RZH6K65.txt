DENOISER : Rethinking the Robustness
for Open-Vocabulary Action Recognition
Anonymous Author(s)
Affiliation
Address
email
Abstract
As one of the fundamental video tasks in computer vision, Open-V ocabulary Action 1
Recognition (OV AR) has recently gained increasing attention, with the develop- 2
ment of vision-language pre-trainings. To enable open-vocabulary generalization, 3
existing methods formulate vanilla OV AR to evaluate the embedding similarity 4
between visual samples and text descriptions. However, one crucial issue is com- 5
pletely ignored: the text descriptions given by users may be noisy, e.g., misspellings 6
and typos, limiting the real-world practicality. To fill the research gap, this paper 7
analyzes the noise rate/type in text descriptions by full statistics of manual spelling; 8
then reveals the poor robustness of existing methods; and finally rethinks to study 9
a practical task: noisy OV AR. One novel DENOISER framework, covering two 10
parts: generation and discrimination, is further proposed for solution. Concretely, 11
the generative part denoises noisy text descriptions via a decoding process, i.e., 12
proposes text candidates, then utilizes inter-modal and intra-modal information to 13
vote for the best. At the discriminative part, we use vanilla OV AR models to assign 14
visual samples to text descriptions, injecting more semantics. For optimization, we 15
alternately iterate between generative-discriminative parts for progressive refine- 16
ments. The denoised text descriptions help OV AR models classify visual samples 17
more accurately; in return, assigned visual samples help better denoising. We carry 18
out extensive experiments to show our superior robustness, and thorough ablations 19
to dissect the effectiveness of each component. 20
1 Introduction 21
Action recognition is one of the fundamental tasks in computer vision that involves classifying videos 22
into meaningful semantics. Despite huge progress that has been made, existing researches focus more 23
on closed-set scenarios, where action classes remain constant during training and inference. Such 24
scenarios are an oversimplification of real life, and thus limiting their practical application. Recently, 25
another line of research considers one more challenging scenario, namely open-vocabulary action 26
recognition (OV AR), and receives increasing attention. 27
OV AR allows users to give free texts to describe action classes, and the model needs to match novel 28
(unseen) text descriptions to videos with similar semantics. To tackle OV AR task, Vision-Language 29
Alignment (VLA) paradigm [ 41,14,57] provides one preliminary but popular idea, i.e., measuring 30
the embedding similarity between text descriptions and video embeddings. Following this paradigm, 31
recent works focus on minor improvements, e.g., better align vision-language modalities [ 16,49,62]. 32
Although promising, these works all maintain one unrealistic assumption in real-world scenarios, i.e., 33
the given text descriptions are absolutely clean/accurate. The concrete form is that they evaluate open- 34
vocabulary performance by re-partitioning closed-set datasets in which text descriptions of classes are 35
fully human-checked. But in fact, under real-world OV AR, novel text descriptions provided by users 36
are sometimes noisy. Character misspellings (typos, missing, tense error) are inevitable [ 43,25] in 37
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.Clean Text Descriptionsacting in play
adjusting glasses
…
yarn spinningExisting Models for
Open -Vocabulary Action Recognition
acting in play yarn spinning√×
Noisy Text Descriptionsavtingin play
adjusting hlasses
…
yarn spinning
Figure 1: Left: For open-vocabulary action recognition (OV AR), existing researches neglect an
essential aspect: the text descriptions provided by users may be noisy ( e.g., misspelling and typos),
resulting in potential classification errors and limiting the real-world practicality. Right: Rethinking
the robustness for popular OV AR methods [ 49,62]. On various datasets, they exhibit high sensitivity
to text noises. Besides, as the noise level increases, the performance degrades significantly.
thousands of descriptions, since users often don’t double-check, as well as differences in user habits 38
and diversity of scenarios (Fig. 1 Left). 39
We are hence motivated to fill the research gap of noisy text descriptions in OV AR. We analyze the 40
noise rate/type in real-world corpora [ 26,45,3]. We also make comprehensive simulations of text 41
noises, following NLP literature [ 42,47]. Fig. 1 Right empirically evaluates noise hazards for existing 42
OV AR methods [ 16,49,62]. One can find that just a small amount of noise lowers recognition 43
accuracy by a large margin, implying quite poor robustness. 44
To spur the community to deal with the noisy OV AR task, being necessary and practical, this paper 45
bravely faces the challenges. One vanilla idea is using a separate language model ( e.g., GPT [ 1]) to 46
correct noisy class descriptions, and then adapt the off-the-shelf vision-language paradigm [ 41,14,57]. 47
However, there exist two nettlesome issues. 1) Textual Ambiguity . One text description is usually a few 48
compact words, with vague semantics, e.g., for the noisy text “boird”, there could be multiple cleaned 49
candidates in terms of spelling, such as “bird” and “board”. This short text lacks context, making 50
phrase correction difficult for uni-modal language models. 2) Cascaded Errors . Text correction and 51
action recognition are independently completed, without sharing knowledge. The noisy output of 52
text correction is cascaded to the input of action recognition, resulting in continuous propagation of 53
errors. To address these issues, we design one multi-modal robust framework: DENOISER . 54
Our first insight is to treat denoising of text descriptions as one generative task: given noisy text 55
descriptions, decode the clean ones, by considering text-vision information to help denoising. Specif- 56
ically, it consists of three components: text proposals, inter-modal weighting, and intra-modal 57
weighting. We first propose potential text candidates based on spelling similarity to limit the decoding 58
space. Then, two types of weighting are combined to decide the best candidate, that is, inter-modal 59
weighting uses assigned visual samples to vote; while intra-modal weighting relies solely on text 60
information. Our other insight is employing existing OV AR models as off-the-shelf tools to assign 61
visual samples at discriminative step. Such tools have been proven to handle clean OV AR tasks well, 62
also making our framework easier to adapt to previous models. For full usage of information in 63
the same semantics, we then assign detail-rich visual samples to clarify the semantic ambiguity of 64
compact text descriptions. To further avoid cascaded errors, we propose a solution of alternating 65
iterations, to connect generative anddiscriminative steps. By progressive refinement, denoised text 66
descriptions help OV AR models to match visual samples more accurately; assigned visual samples 67
help better denoising. Under multiple iterations, denoising results and OV AR are both better. 68
Our main contributions are summarized as follows: 69
•We pioneer to explore noisy text descriptions for open-vocabulary action recognition (OV AR): first 70
fully analyze the noise rate/type in text descriptions by extensive statistics in real-world corpora; then 71
evaluate the robustness for existing methods; finally rethink to study one practical task: noisy OV AR. 72
•We propose a novel DENOISER framework to tackle the noisy OV AR task, by alternately optimizing 73
generative-discriminative steps. The generative step leverages knowledge of vision-text alignment to 74
denoises noisy text descriptions, in the form of progressive decoding; while the discriminative step 75
assigns visual samples to text descriptions for open-vocabulary action recognition. 76
2•We carry out extensive experiments to show the superior robustness of DENOISER against noisy 77
text descriptions, under various noises and datasets. Great performance improvements are achieved 78
over existing competitors. Thorough ablations are studied to show effectiveness of every design. 79
2 Related Work 80
Vision-Language-Audio Pre-training (VLP) aims to jointly optimize multi-modal embeddings with 81
large-scale web data, e.g., CLIP [ 41], ALIGN [ 14], Florence [ 57], FILIP [ 55], VideoCLIP [ 52], and 82
LiT [ 58]. In architectures, VLP uses independent encoders for vision, text, and audio, followed by 83
cross-modal fusion. For optimization, contrastive learning [ 5,61] and cross-modal matching [ 7,29] 84
are mainstream, covering self supervision [ 32,34], weak supervision [ 28,8] and partial supervi- 85
sion [ 19,33]. VLP benefits various applications: image-text retrieval [ 6,18], video understand- 86
ing [23, 20, 22, 21], action recognition [16, 60], visual grounding [32, 56, 31], AIGC [4, 36]. 87
Open-Vocabulary Concept Learning aims to understand vision, where conceptual semantics are 88
described by free/arbitrary text descriptions. It is characterized by using vision-language pre-trainings 89
to match text descriptions and visual samples in semantic space. Its typical evaluation metric is 90
the downstream zero-shot performance, i.e., classify unseen classes [ 49,62,17,38,54,48,37]. To 91
achieve the evaluation, most methods re-partition closed-set datasets.[ 49] Although there is some 92
plausibility, such re-partition implicitly makes an unrealistic assumption: text descriptions of unseen 93
classes are human-checked, and thus absolutely clean, limiting real-world application. We pioneer 94
taking noises from text descriptions (misspellings and typos) into consideration. By adding real-world 95
noise for the above methods, we reveal their poor robustness, and design DENOISER for solution. 96
Robustness of Language Models is extensively studied by adversarial attack-defense techniques [ 50, 97
59]. When text inputs are facing noises, defense methods correct the outputs, dividing into: detection- 98
purification [ 63,39], as well as adversarial training [ 53,9,35,30,51]. The former methods detect 99
and correct the corrupted part of a text phrase. The latter trains a model on adversarial samples to 100
increase its direct noise-against ability. Overall, all these methods employ solely textual information 101
for robustness in pure NLP tasks. We differ from them by considering robustness in the context of 102
multi-modal scenarios and by employing multi-modal information to better assist text denoising. 103
3 Method 104
We explore noisy text descriptions for open-vocabulary action recognition. In Sec 3.1, we introduce 105
noisy open-vocabulary setting; in Sec 3.2, we detail our DENOISER framework, covering generative 106
-discriminative sub-parts; in Sec 3.3, we report the accompanying optimization strategy. 107
3.1 Preliminary & Rethinking 108
Open-Vocabulary Action Recognition (OV AR). For a video dataset V= (vj∈RT×H×W×3)N
j, 109
OV AR aims to train one model ΦOVAR that matches target videos with arbitrary text description T. 110
Ytrain= Φ OVAR (Vtrain,Ttrain)∈RCbase,Ytest= Φ OVAR (Vtest,Ttest)∈RCnovel, (1)
whereYrefers to the matching label between VandT. During training, (video, text, matching label) 111
triplets from the base semantic-classes are provided; while during testing, the model is evaluated 112
on the novel semantic-classes. Note that, the semantic-classes between training ( Cbase) and testing 113
(Cnovel) are disjoint, i.e.,Cbase∩Cnovel=∅. 114
Vision-Language Alignment (VLA). To enable open-vocabulary capability, recent OV AR stud- 115
ies [16,49,62,40] embrace vision-language pre-trainings (VLPs), for their notable ability in cross- 116
modal alignment. Specifically, OV AR could be achieved by measuring the embedding similarity 117
between text descriptions Tand video samples V, which is formally formulated as: 118
Y=σ(Fv∗ Ft),Fv= Φ pool(Φvis(V))∈RN×D,Ft= Φ txt(T)∈RC×D. (2)
where σrefers to the softmax activation, Φpoolis the spatio-temporal pooling, ΦvisandΦtxtare 119
visual and textual encoders of VLPs, Dis the embedding dimension. 120
Noisy Text Descriptions in OV AR. Although great progress has been made, the VLA paradigm 121
suffers from an unrealistic assumption, i.e., that text descriptions are absolutely clean/accurate, 122
3Generative Step Discriminative Step
Better Texts,
Better Results
0.7
0.2
 0.1
0.2
0.6
0.2weight distance
1
22Text Candidates
adjoininghlassesadjusting hlasses
assisting hlasses
→
Visual
EncoderTextual
Encoder
Visual
EncoderTextual
Encoderavtingin pla t
abseiling
air drumming
adjisting hlasses𝒯𝑖−1: 
Decoded Texts
at 𝑖−1Step
Visual
Samples
:
avtingin pla t
abseiling
air drumming
adjisting hlasses𝒯𝑖−1: Decoded
Texts at 𝑖−1Step
𝒯𝑖: Decoded
Texts at 𝑖Stepacting in pla t
abseiling
air drumming
adjusting hlassesEmbedding SpaceInter -modal
Weighting
Intra -modal
Weighting
Embedding Space
Using
Update
Using
Text embeddings
of text candidatesVisual 
EmbeddingsText Embeddings
of 𝒯𝑖Text Embeddings
of 𝒯𝑖−1
Fusion & V ote for 
Text CandidatesVisual
Samples
:
Use the Best,
to Update |Figure 2: Framework Overview .DENOISER is composed of one generative partΨgeneand one
discriminative partΨdisc.Ψgeneviews denoising text descriptions as a decoding process Ti−1→ T i.
We first propose text candidates Φprop forTi−1based on spelling similarity; then choose the best
candidate by inter-modal weighting Φinter and intra-modal weighting Φintra.Φinter uses vision-text
information, while Φintra relies solely on texts. Ψdiscassigns text semantics to visual samples, then
only visual samples with the same semantics can vote for text candidates. We optimize alternatively
between generative anddiscriminative steps to tackle noisy OV AR.
limiting the practicality in reality. Actually, the diversity of users and scenarios can easily cause 123
text descriptions given to be somewhat noisy, especially for unseen semantic-classes, due to their 124
enormous degree of freedom. Formally, for one text description with nwords, the clean/noisy 125
versions T/T′are: 126
T′= (t′
1,···, t′
n) = Ψ noise(T;p),T= (t1,···, tn). (3)
where tiis the i-th word of T.Ψnoise refers to noise contamination in reality, e.g.,inserting ,substitut- 127
inganddeleting characters with probability p, following [ 42,47]. Since these three atomic operations 128
defined in Levenshtein edit distance Dare of distance 1, noise rate pcan also be deduced by: 129
p=D(T,T′)
max( length of T,length of T′)(4)
As a result, the noisy OV AR task can be formulated as: given VandT′, the model is expected to 130
maximize the accuracy of action recognition, and even recovering T′toT. 131
Robustness of Existing Methods. Fig. 1 evaluates for typical OV AR studies [ 49,62], across three 132
public datasets. In terms of Top-1 classification accuracy, existing methods are rather sensitive to 133
noise and show one trend: the larger the noise, the more significant the performance degradation 134
(please see quantitative experiments in Tab. 2). Such poor robustness to the noisy OV AR task, proves 135
excessive idealization of existing studies and also motivates us to fill the research gap. 136
3.2 DENOISER : One Robust OV AR Framework 137
Motivation. Given the complexity of noisy OV AR, we here divide it into two sub-steps: denoising of 138
text descriptions, and then vanilla OV AR. The former is viewed as one generative decoding form, by 139
considering both vision-text information for progressive denoising. While the latter is in one natural 140
discriminative form, by assigning text descriptions to video samples. For the joint optimization of 141
these two sub-steps, we iterate alternately between generative anddiscriminative forms. As a result, 142
ourDENOISER framework progressively tackles the noisy OV AR task. 143
4Framework. As shown in Fig. 2, our DENOISER framework covers two components: generative 144
sub-step Ψgeneanddiscriminative sub-step Ψdisc. For Ψgene, we iteratively refine text descriptions 145
by one decoding process, that is, (T0,T1,···,Tn), where nis the index of decoding steps. Upon 146
finishing step i, we will have Ti= (t1,···,ti, t′
i+1,···, t′
n), where trefers to the decoded version 147
oft, meaning that the i-th word of text descriptions is decoded at step i. We start with T0=T′, and 148
finish at Tnto ensure that all words are denoised. While for Ψdisc, we find it identical to vanilla OV AR 149
task and thus leveraging the VLA pipeline [16, 49] for help, which is off-the-shelf and well-studied. 150
Formally, our DENOISER framework tackles noisy OV AR as follows: 151
Ti= Ψ gene(Ti−1,Yi−1,V),Yi−1= Ψ disc(Ti−1,V) = Φ OVAR (Ti−1,V). (5)
At the discriminative step, we calculate the matching label Yi−1to make coarse semantic classification 152
of visual samples, i.e., assign Ti−1toV. At the generative step, we first propose Ktext candidates 153
Φprop(Ti−1)forTibase on Ti−1to limit the decoding space. Then, to vote for the best candidate, we 154
design two novel modules, namely inter-modal weighting Φinter and intra-modal weighting Φintra. 155
Here, Φinter uses vision information V, while Φintra relies on text information Ti−1. 156
We alternate between the generative anddiscriminative steps to optimize the decoding result step by 157
step. Please find in Algorithm 1 for comprehensive details. 158
3.3 Optimization for the DENOISER Framework 159
Discriminative Step consists in calculating cross-modal matching labels Yusing Ψdisc. Intuitively, 160
visual samples Vcwhose labels Yare assigned to semantic-class c,i.e.argmax Y=c, are those who 161
could help decode Tc,imost efficiently. On the contrary, visual samples from other semantic-classes 162
may have few connections with the current class and thus provide no meaningful aid. Here, we find 163
this process is identical to vanilla OV AR, and hence employs ΦOVAR asΨdisc. We theoretically 164
prove in the Appendix that, Vcis the best set of visual samples to choose from. With Vcdefined and 165
argmax Y=c,Ψgenedecodes text descriptions Tc,ifor each semantic-class c: 166
Ψgene(Tc,i−1,Y,V) = Ψ gene(Tc,i−1,Vc) = argmax
Tc,ip(Tc,i|Tc,i−1,Vc). (6)
Recall tc,iis the i-th word to be decoded, and Tc,i−1is from last decoding, with the first i−1 167
words decoded. As we decode word-by-word, choosing the best Tc,iis exactly choosing the best tc,i, 168
i.e.argmaxTc,ip(Tc,i|Tc,i−1,Vc) = argmaxtc,ip(tc,i|Tc,i−1,Vc), as we do in generative step. 169
Generative Step here consists in, for each semantic-class c, choosing the best tc,ithat maximizes 170
p(tc,i|Tc,i−1,Vc). With p(Tc,i−1,Vc)andp(Vc)same for all possible tc,i, we make detailed deriva- 171
tions in the Appendix to show that: 172
p(tc,i|Tc,i−1,Vc)∝p(tc,i,Tc,i−1,Vc)∝Y
vj∈Vcp(tc,i|vj)p(Tc,i−1|tc,i, vj). (7)
Here, the error model p(Tc,i−1|tc,i, vj)evaluates how tc,imay be misspelled as t′
c,i, since the i-th 173
word in Tc,i−1is still noisy and not decoded. Knowing that errors in text descriptions are independent 174
of visual samples, it reduces to uni-modal p(Tc,i−1|tc,i). As the error that one may make given the 175
correct text is harder to model while the reverse is much easier, we let p(Tc,i−1|tc,i)∝p(tc,i|Tc,i−1). 176
Please refer to detailed derivations in the Appendix. As a result, our final objective is: 177
p(tc,i|Tc,i−1)Y
vj∈Vcp(tc,i|vj) = Φ intraY
vj∈VcΦinter. (8)
Text Proposals consists in proposing Kcandidates {tk
i}kfortiwith the lowest Levenshtein Edit 178
Distance D(·, t′
i)(a metric of spelling similarity). By replacing original noisy word t′
iinTk
i−1with 179
{tk
i}k, they form Φprop(Ti−1) =Tk
i= (t1,···,ti−1, tk
i, t′
i+1,···, t′
n), theKcandidates for Ti. 180
The benefit of text proposals is to reduce computing complexity. Since text embeddings are quantized 181
in the semantic space, the search is limited to proposed candidates, rather than in the entire space. 182
Inter-modal Weighting Φinter=p(tc,i|vj), vj∈ Vcrelies on vision samples from semantic-class c 183
to determine the best tc,ifor the next iteration. Concretely, we model the probability of being chosen 184
5Algorithm 1 DENOISER : Robust Open-V ocabulary Action Recognition
Require: noisy text descriptions T′, visual samples V, iteration number n, temperature λ, candidate
number K, edit distance D, open-vocabulary model ΦOVAR
T0← T′
fori= 1,2,···, ndo
forc= 1,2,···, Cdo ▷Text Proposals
t′
c,iis the i-th word of Tc,i−1, which is noisy and not yet decoded
Select from corpus, Kcandidates {tk
c,i}kwith the smallest Dwitht′
c,i
Replace t′
c,iwith{tk
c,i}k, forming {Tk
c,i}k
end for
forj= 1,2,···,|V|do ▷Discriminative Step
c←argmax
cmax
kexp(S(vj,Tk
c,i))P
k′exp(S(vj,Tk′
c,i))
Assign vjto class c,vj∈ Vc
end for
forc= 1,2,···, Cdo ▷Generative Step
Φk
intra←exp(−D(tk
c,i,t′
c,i)/λ)P
k′exp(−D(tk′
c,i,t′
c,i)/λ)▷Intra-Modal Weighting
Φk
inter←Q
vj∈Vcexp(S(vj,Tk
c,i))P
k′exp(S(vj,Tk′
c,i))▷Inter-Modal Weighting
Tc,i← Tk
c,i,k= argmaxkΦk
intra×Φk
inter
end for
end for
for each proposed candidate to be: 185
P(tc,i=tk
c,i|vj) =P(Tc,i=Tk
c,i|vj) =exp(S(vj,Tk
c,i))P
k′exp(S(vj,Tk′
c,i)), vj∈ Vc. (9)
whereS(·,·)is the cosine similarity between video-text embeddings, both encoded by ΦOVAR . The 186
intuition is that the more unanimously visual samples agree on candidate Tk
c,i, the more likely it is the 187
text descriptions corresponding to semantic-class c. Besides, by letting visual samples vote on Tk
c,i188
instead of tk
c,i, we take into consideration not only the current word tc,ibut also context implicitly. 189
Intra-modal Weighting Φintra=p(tc,i|Tc,i−1)relies solely on text information to decide the best tc,i 190
for next iteration. Although Φintra may be solved by uni-modal spell-checkers [ 15] or large language 191
models [ 1], we here design a simple model by considering only spelling similarity (ignore contexts), 192
to save computing costs. That is, choose tc,idepending solely on t′
c,iinstead of on entire Tc,i−1: 193
P(tc,i=tk
c,i|Tc,i−1) =P(tc,i=tk
c,i|t′
c,i) =exp(−D(tk
c,i, t′
c,i)/λ)P
k′exp(−D(tk′
c,i, t′
c,i)/λ). (10)
The intuition is that, the more similar a word candidate tk
c,iis, compared to the noisy word t′
c,i, the 194
more likely it is the corresponding denoised word. Here, we introduce one temperature parameter λto 195
balance Φintra andΦinter. A larger λindicates that different edit distance gives similar probabilities, 196
meaning that we rely more on visual samples for decision, and vice versa. 197
4 Experiments 198
Typical Models for Vanilla OV AR . To illustrate the generalizability of our framework, we leverage 199
two typical models from the VLA pipeline as ΦOVAR , that is, ActionCLIP [49] and XCLIP [62]. 200
These two models adopt hand-crafted prompts and visual-conditioned prompt tuning, respectively. 201
Under both models, we choose ViT-B/16-32F as the network backbones, for simplicity. 202
Datasets .HMDB51 [26] contains 7k videos covering 51action categories. UCF101 [45] contains 203
13k videos spanning 101action categories. Kinetics700 [3] (K700) is simply an extension of K400, 204
with around 650k video clips sourced from YouTube. To partition these datasets for open-vocabulary 205
action recognition, this paper follows the standard consensus [49, 62], for the sake of fairness. 206
6Figure 3: Statistics for Noises in Reality .
Text noises may be classified into 4 types: in-
serting, substituting, swapping, and deleting
characters.[ 2] In terms of edit distance, based on
TOEFL-Spell dataset[ 10], most of the text noises
have an edit distance = 1 compared to the clean
version. Nevertheless, the distribution tends to
be positively skewed towards larger noise.
Insertion
14%
Substitution
37%Deletion
33%Swap
16%NOISE TYPE
=1
83%
=2
13%
=3
3%
>3
1%EDIT DISTANCE
Table 1: Comparisons between Various Competitors. Using ActionCLIP [ 49] asΦOVAR while
evaluating on UCF101, we compare with statistical text spell-checkers (PySpellChecker [ 15]), neural
based ones (Bert from NeuSpell) [ 13], and GPT 3.5 [ 1]. Our method remarkably outperforms others
in terms of Top-1 classification accuracy, and semantic similarity of recovered text descriptions.
Noise Type Noise Rate Competitors Top-1 Acc Label Acc Semantic Similarity
– 0% Upper Bound 66.3 100 100
Real ∼5.52%GPT 3.5 [1] 61.2±1.4 74.7±1.9 97.1±0.4
Bert (NeuSpell) [13] 56.0±1.1 64.7±2.0 94.5±0.4
PySpellChecker [15] 59.9±1.2 79.6±1.6 96.7±0.3
Ours 61.5±0.782.3±1.6 97.2±0.3
Simulated5%GPT 3.5 [1] 59.7±1.2 47.6±3.1 95.9±0.4
Bert (NeuSpell) [13] 56.6±0.5 66.2±2.3 94.6±0.4
PySpellChecker [15] 60.9±1.1 82.5±2.9 97.1±0.4
Ours 63.8±0.786.4±2.3 97.7±0.2
10%GPT 3.5 [1] 58.5±1.3 51.6±2.3 95.8±0.3
Bert (NeuSpell) [13] 51.0±0.5 50.4±3.6 91.6±0.6
PySpellChecker [15] 55.7±1.1 69.3±1.5 94.8±0.3
Ours 61.2±0.875.9±1.9 96.4±0.3
Metric. We use three metrics for full evaluations from multiple perspectives. Top-1 Acc refers to 207
the top-1 classification accuracy of noisy open-vocabulary action recognition. Label Acc counts the 208
percentage of denoised text descriptions that match exactly with ground truth. Semantic Similarity 209
calculates the cosine similarity of embeddings, between denoised and clean text descriptions. Label 210
Acc and Semantic Similarity measure how well noisy text descriptions are recovered. 211
Implementations. We set the proposal number K= 10 . Intra-modal weighting and inter-modal 212
weighting are both used to determine the best candidate. Temperature λfollows a linear schedule 213
from 0.01 to 1. We use the same corpus as in PySpellChecker, which contains 70317 English words, 214
for text proposals. For typical OV AR methods [ 49,62], we choose the ViT-B/16-32F checkpoint 215
pretrained on K400 [ 24] to evaluate their zero-shot robustness on HMDB51 [ 27], UCF101 [ 46] and 216
K700 [ 44]. Since K700 and K400 have overlapped categories, we exclude them when evaluating on 217
K700. For UCF101, we use the separated lowercase text label. All ablation studies are conducted on 218
UCF101 under 20% noise. For statistical significance, We do each simulation 10 times and report the 219
mean and confidence interval of 95%. All experiments are done using a single RTX 3090. 220
4.1 Statistics on Noise Type/Rate for Text Descriptions 221
Real Noise. We adopt two large-scale corpora [ 11,10] of misspellings to analyze noise type in text 222
descriptions. As shown in Fig. 3, the conclusion is similar to the NLP community [ 42,47],i.e., three 223
atomic types of noise are inserting, substituting, and deleting text characters. More complicated noise 224
patterns, e.g.swaping, can be constructed by mixing atomic noise types. Then, following previous 225
literature, we quantify noise rate through Levenshtein Edit Distance, a generally accepted metric, 226
to calculate the occurrence number of atomic noise types. Specifically, GitHub Typo Corpus [ 11] 227
contains over 350k edits of typos from GitHub. The average noise rate (per sentence) is 3.3%. 228
Nevertheless, the distribution is highly positively skewed (skewness = 2.9). For the worst 5% cases, 229
the noise rate (per sentence) is larger than 9.4%. TOEFL-Spell Corpus [ 10] samples essays written 230
by candidates from various language backgrounds in TOEFL®iBT test. There are, on average, 6.9 231
spelling mistakes per essay. For misspelled words, the noise rate (per word) is on average 16.0%. 232
7Table 2: Comparison Across Datasets and Models . On three standard datasets, facing multiple
noise types (real or simulated), and under various noise rates, our DENOISER consistently improves
the performance for noisy OV AR, regardless of underlying OV AR methods ΦOVAR .
Dataset Noise Type Noise RateΦOVAR : Typical Models for Vanilla OV AR task
ActionCLIP [49] XCLIP [62]
w/o Ours w Ours w/o Ours w Ours
UCF101Upper Bound 66.3 68.6
Real ∼5.52% 54.0±2.3 61.5±0.7 53.8±2.7 63.4±0.9
Simulated5% 54.9±1.8 63.2±0.7 55.6±2.2 64.2±1.4
10% 47.3±1.4 61.2±1.2 46.4±1.3 62.9±2.3
HMDB51Upper Bound 46.2 45.0
Real ∼6.71% 37.6±1.6 40.0±1.4 35.3±1.5 38.4±1.4
Simulated5% 39.4±1.4 41.3±1.4 37.5±1.8 39.7±1.0
10% 35.2±2.3 39.6±1.4 31.8±2.2 37.3±1.5
K700Upper Bound 40.2 49.3
Real ∼5.47% 30.8±0.51 35.9±0.4 35.6±0.6 43.5±0.7
Simulated5% 31.5±0.5 36.8±0.3 36.7±0.9 44.1±0.6
10% 25.4±0.8 35.3±0.5 27.5±0.7 41.8±0.9
Noise Scenarios. In the "Simulated" noise type, we mix three atomic noises: insertion, substitution, 233
and deletion. Concretely, for each character, we perturb it with probability p. For each perturbation, 234
it will be insertion, substitution, and deletion with equal probability. To further ensure real-world 235
generalizability, we ask GPT3.5 to give examples of perturbation according to real-world scenarios. 236
We mix them into simulated noises. Noise rate pof the "Real" noise type is estimated with Eq. (3). 237
4.2 Comparison with State-of-the-art Methods 238
Comparison to Competitors. Tab. 1 compares from three axes: Top-1 Acc of ΦOVAR after correction, 239
Label Acc and Semantic Similarity. PySpellChecker is a uni-modal statistical model that corrects 240
each word by edit distance and appearance frequency. Bert (NeuSpell) [13] employs a uni-modal 241
Bert-based model to translate noisy text descriptions into clean ones. We also ask GPT 3.5 to denoise 242
text descriptions using the prompt “The following words may contain spelling errors by deleting, 243
inserting, and substituting letters. You are a corrector of spelling errors. Give only the answer 244
without explication. What is the correct spelling of the action of <noisy text description>?”. Our 245
method outperforms all competitors by large margins, which is impressive because our method is 246
unsupervised without prior knowledge other than those contained in the OV AR model. Note that the 247
output of GPT 3.5 tends to be unstable depending on prompts, which requires manual cleaning to 248
remove irrelevant parts contained in the output, thus impeding real-world usage. 249
Comparisons Across Datasets/Models. Tab. 2 compares Top-1 Acc to further reveal our solution is 250
scalable/generalizable. Under various noise rates, our model is robust to achieve huge improvements. 251
In terms of scalability across models, our method is not only applicable to hand-crafted prompts as in 252
ActionCLIP but also to learnable visual-conditioned prompts as in XCLIP. Furthermore, we notice 253
that, whenever XCLIP outperforms ActionCLIP, our method also yields a better result. A better 254
visual encoder and well-tuned prompt may significantly increase our performance, showing that our 255
method’s upper limit could become higher, as the community continues to train better OV AR models. 256
4.3 Ablation Study 257
Inter-modal Weighting Φinter & Intra-modal Weighting Φintra.Tab. 3 shows that, both Φinter 258
andΦintra contribute to denoising text descriptions and to improving the robustness of underlying 259
ΦOVAR . In terms of Top-1 Acc and Semantic Similarity, Φinter performs better than Φintra, since 260
Φinter uses visual information as one direct optimization guideline to improve video recognition. 261
While Φintra performs better in terms of Label Acc, which focuses more on spelling correctness. 262
Besides, Φinter andΦintra turn out to be complementary: visual information helps to understand 263
noisy text descriptions; while textual information prevents the model from being misled by visual 264
samples. We achieve the best performance when combining these two weightings. 265
8Table 3: Ablations for Inter-modal Weighting ΦInter, Intra-modal Weighting ΦInter, Schedule of
Temperature λ.ΦInter alone outperforms ΦIntra. Both contribute to correcting class texts, and give
the best results when combined. Linear schedule of balancing factor λoutperforms the constant one,
meaning that it helps to rely more on ΦIntra at first, and then gradually switch to ΦInter.
ΦInter ΦIntra Schedule λ Top-1 Acc Label Acc Semantic Similarity
A1 ✓ / 48.1±2.2 38.2±2.5 88.9±0.4
A2 ✓ / 52.9±1.4 34.1±2.4 89.1±0.6
A3 ✓ ✓ Constant 54.5±2.5 54.9±4.5 92.4±0.8
A4 ✓ ✓ Linear 55.2±1.5 55.1±3.0 92.9±0.6
Figure 4: We evaluate on UCF101 by using ActionCLIP as ΦOVAR .Left: Ablation Study on Noise
Type . “Mixed” means that all types of text noises: “Substitute”, “Insert”, “Delete” take place with
equal probability. Our DENOISER shows good resilience, especially against noises of inserting or
substituting. Right: Ablation Study on Proposal Number K. AsKincreases, Top-1 Acc increases
and converges gradually towards the upper bound, but it also brings heavier computing costs.
Temperature Schedule λbalances intra-modal weighting and inter-modal weighting. One larger λ 266
indicates more reliance on inter-modal weighting. “Linear” means that λaugments from 0.01 to 1 267
linearly. Tab. 3 reports that it is beneficial to rely more on intra-modal at the beginning of decoding, 268
and then gradually turn to inter-modal for more help. This indicates that, when text noises are high, 269
Φintra offers more help; when text noises are slight, Φinter could help more. 270
Noise Type. Fig. 4 Left reports our robustness under various noise types/rates. “Mixed” means that 271
three noise types: “Substitute”, “Insert”, “Delete” are equally possible to appear. Our method shows 272
remarkable resilience when texts are perturbed by inserting or substituting characters. Performance 273
degradation is observed when texts are perturbed by deleting characters. It is reasonable, as deleting 274
characters causes huge information loss, making the model difficult to recover clean text descriptions. 275
Number of Candidates K.Fig. 4 Right shows as Kincreases, inter-modal weighting can reveal 276
its full power, hence improving performance. Otherwise, if a good candidate is excluded from the 277
proposal stage due to a small K, it can be selected by neither of the inter- or intra-modal weighting, 278
thus decreasing performance. Moreover, the performance tends towards one plateau, showing a 279
decreasing marginal contribution of more proposals to performance. Since a larger Kmeans more 280
computing costs for text encoding, we select K= 10 by default to make reasonable trade-offs. 281
5 Conclusion 282
This paper investigates how noises in class-text descriptions negatively interference OV AR; and 283
one novel framework DENOISER is proposed for solutions. By incorporating visual information 284
during denoising, we clarify the ambiguity induced by short and context-lacking text descriptions; by 285
iteratively refining the denoised output through one generative-discriminative process, we mitigate 286
cascaded errors which may propagate from spell-checking models to outputs of OV AR model. We 287
conduct extensive experiments to demonstrate the generalizability of DENOISER across multiple 288
models and datasets, and also show our superiority over uni-modal spell-checking solutions. 289
Limitations. 1) We focus more on spelling noises; while in the real world, text noises can be more 290
complex, involving semantic ambiguity. Equipping DENOISER with large language models may 291
be a feasible solution. 2) Using more text candidates or visual samples brings better results for 292
DENOISER , but also costs more. There is a trade-off between performance and computational cost. 293
9References 294
[1] Gpt-3.5 turbo, https://platform.openai.com/docs/models/gpt-3-5-turbo/ 295
[2]Al-Oudat, A.: Spelling errors in english writing committed by english-major students at bau. 296
Journal of Literature, Languages and Linguistics 32(2) (2017) 297
[3]Carreira, J., Noland, E., Hillier, C., Zisserman, A.: A short note on the kinetics-700 human 298
action dataset. arXiv preprint arXiv:1907.06987 (2019) 299
[4]Chen, M., Chen, X., Zhai, Z., Ju, C., Hong, X., Lan, J., Xiao, S.: Wear-any-way: Manipulable 300
virtual try-on via sparse correspondence alignment. arXiv preprint arXiv:2403.12965 (2024) 301
[5]Chen, X., Chen, S., Yao, J., Zheng, H., Zhang, Y ., Tsang, I.W.: Learning on attribute-missing 302
graphs. IEEE transactions on pattern analysis and machine intelligence (2020) 303
[6]Chen, X., Cheng, Z., Yao, J., Ju, C., Huang, W., Lan, J., Zeng, X., Xiao, S.: Enhancing 304
cross-domain click-through rate prediction via explicit feature augmentation. arXiv preprint 305
arXiv:2312.00078 (2023) 306
[7]Cheng, F., Wang, X., Lei, J., Crandall, D., Bansal, M., Bertasius, G.: Vindlu: A recipe for 307
effective video-and-language pretraining. In: Proceedings of the IEEE Conference on Computer 308
Vision and Pattern Recognition (2023) 309
[8]Cheng, Z., Xiao, S., Zhai, Z., Zeng, X., Huang, W.: Mixer: Image to multi-modal retrieval 310
learning for industrial application. arXiv preprint arXiv:2305.03972 (2023) 311
[9]Dinan, E., Humeau, S., Chintagunta, B., Weston, J.: Build it break it fix it for dialogue safety: 312
Robustness from adversarial human attack. arXiv preprint arXiv:1908.06083 (2019) 313
[10] Flor, M., Fried, M., Rozovskaya, A.: A benchmark corpus of english misspellings and a 314
minimally-supervised model for spelling correction. In: Proceedings of the Fourteenth Workshop 315
on Innovative Use of NLP for Building Educational Applications. pp. 76–86 (2019) 316
[11] Hagiwara, M., Mita, M.: Github typo corpus: A large-scale multilingual dataset of misspellings 317
and grammatical errors. arXiv preprint arXiv:1911.12893 (2019) 318
[12] Hu, X., Zhang, K., Xia, L., Chen, A., Luo, J., Sun, Y ., Wang, K., Qiao, N., Zeng, X., Sun, 319
M., et al.: Reclip: Refine contrastive language image pre-training with source free domain 320
adaptation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer 321
Vision. pp. 2994–3003 (2024) 322
[13] Jayanthi, S.M., Pruthi, D., Neubig, G.: Neuspell: A neural spelling correction toolkit. arXiv 323
preprint arXiv:2010.11085 (2020) 324
[14] Jia, C., Yang, Y ., Xia, Y ., Chen, Y .T., Parekh, Z., Pham, H., Le, Q.V ., Sung, Y ., Li, Z., Duerig, 325
T.: Scaling up visual and vision-language representation learning with noisy text supervision. 326
In: Proceedings of the International Conference on Machine Learning (2021) 327
[15] Jiang, Y .G., Liu, J., Zamir, A.R., Toderici, G., Laptev, I., Shah, M., Sukthankar, R.: 328
pyspellchecker: Action recognition with a large number of classes, https://github.com/ 329
barrust/pyspellchecker/ 330
[16] Ju, C., Han, T., Zheng, K., Zhang, Y ., Xie, W.: Prompting visual-language models for efficient 331
video understanding. In: Proceedings of the European Conference on Computer Vision. Springer 332
(2022) 333
[17] Ju, C., Li, Z., Zhao, P., Zhang, Y ., Zhang, X., Tian, Q., Wang, Y ., Xie, W.: Multi-modal 334
prompting for low-shot temporal action localization. arXiv preprint arXiv:2303.11732 (2023) 335
[18] Ju, C., Wang, H., Li, Z., Chen, X., Zhai, Z., Huang, W., Xiao, S.: Turbo: Informativity-driven 336
acceleration plug-in for vision-language models. arXiv preprint arXiv:2312.07408 (2023) 337
[19] Ju, C., Wang, H., Liu, J., Ma, C., Zhang, Y ., Zhao, P., Chang, J., Tian, Q.: Constraint and union 338
for partially-supervised temporal sentence grounding. arXiv preprint arXiv:2302.09850 (2023) 339
[20] Ju, C., Zhao, P., Chen, S., Zhang, Y ., Wang, Y ., Tian, Q.: Divide and conquer for single-frame 340
temporal action localization. In: Proceedings of the International Conference on Computer 341
Vision (2021) 342
[21] Ju, C., Zhao, P., Chen, S., Zhang, Y ., Zhang, X., Wang, Y ., Tian, Q.: Adaptive mutual supervision 343
for weakly-supervised temporal action localization. IEEE Transactions on Multimedia (2022) 344
10[22] Ju, C., Zhao, P., Zhang, Y ., Wang, Y ., Tian, Q.: Point-level temporal action localization: Bridging 345
fully-supervised proposals to weakly-supervised losses. arXiv preprint arXiv:2012.08236 (2020) 346
[23] Ju, C., Zheng, K., Liu, J., Zhao, P., Zhang, Y ., Chang, J., Tian, Q., Wang, Y .: Distilling vision- 347
language pre-training to collaborate with weakly-supervised temporal action localization. In: 348
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2023) 349
[24] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., 350
Green, T., Back, T., Natsev, P., et al.: The kinetics human action video dataset. arXiv preprint 351
arXiv:1705.06950 (2017) 352
[25] Keller, Y ., Mackensen, J., Eger, S.: Bert-defense: A probabilistic model based on bert to combat 353
cognitively inspired orthographic adversarial attacks. arXiv preprint arXiv:2106.01452 (2021) 354
[26] Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: HMDB: A large video database 355
for human motion recognition. In: Proceedings of the International Conference on Computer 356
Vision (2011) 357
[27] Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: HMDB: a large video database for 358
human motion recognition. In: Proceedings of the International Conference on Computer Vision 359
(ICCV) (2011) 360
[28] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with 361
frozen image encoders and large language models. In: International conference on machine 362
learning. PMLR (2023) 363
[29] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified 364
vision-language understanding and generation. In: International conference on machine learning. 365
pp. 12888–12900. PMLR (2022) 366
[30] Liu, H., Zhang, Y ., Wang, Y ., Lin, Z., Chen, Y .: Joint character-level word embedding and 367
adversarial stability training to defend adversarial text. In: Proceedings of the AAAI Conference 368
on Artificial Intelligence (2020) 369
[31] Liu, J., Ju, C., Ma, C., Wang, Y ., Wang, Y ., Zhang, Y .: Audio-aware query-enhanced transformer 370
for audio-visual segmentation. arXiv preprint arXiv:2307.13236 (2023) 371
[32] Liu, J., Ju, C., Xie, W., Zhang, Y .: Exploiting transformation invariance and equivariance 372
for self-supervised sound localisation. In: Proceedings of ACM International Conference on 373
Multimedia (2022) 374
[33] Liu, J., Liu, Y ., Zhang, F., Ju, C., Zhang, Y ., Wang, Y .: Audio-visual segmentation via unlabeled 375
frame exploitation. arXiv preprint arXiv:2403.11074 (2024) 376
[34] Liu, J., Wang, Y ., Ju, C., Ma, C., Zhang, Y ., Xie, W.: Annotation-free audio-visual segmentation. 377
In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision 378
(2024) 379
[35] Liu, K., Liu, X., Yang, A., Liu, J., Su, J., Li, S., She, Q.: A robust adversarial training approach 380
to machine reading comprehension. In: Proceedings of the AAAI Conference on Artificial 381
Intelligence (2020) 382
[36] Ma, C., Yang, Y ., Ju, C., Zhang, F., Liu, J., Wang, Y ., Zhang, Y ., Wang, Y .: Diffusionseg: 383
Adapting diffusion towards unsupervised object discovery. arXiv preprint arXiv:2303.09813 384
(2023) 385
[37] Ma, C., Yang, Y ., Ju, C., Zhang, F., Zhang, Y ., Wang, Y .: Open-vocabulary semantic segmen- 386
tation via attribute decomposition-aggregation. Advances in Neural Information Processing 387
Systems (2024) 388
[38] Nag, S., Zhu, X., Song, Y .Z., Xiang, T.: Zero-shot temporal action detection via vision-language 389
prompting. In: Proceedings of the European Conference on Computer Vision. Springer (2022) 390
[39] Pruthi, D., Dhingra, B., Lipton, Z.C.: Combating adversarial misspellings with robust word 391
recognition. arXiv preprint arXiv:1905.11268 (2019) 392
[40] Qian, R., Li, Y ., Xu, Z., Yang, M.H., Belongie, S., Cui, Y .: Multimodal open-vocabulary video 393
classification via pre-trained vision and language models. arXiv preprint arXiv:2207.07646 394
(2022) 395
11[41] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, 396
A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language 397
supervision. In: Proceedings of the International Conference on Machine Learning. PMLR 398
(2021) 399
[42] Rychalska, B., Basaj, D., Gosiewska, A., Biecek, P.: Models in the wild: On corruption robust- 400
ness of neural nlp systems. In: Neural Information Processing: 26th International Conference, 401
ICONIP 2019, Sydney, NSW, Australia, December 12–15, 2019, Proceedings, Part III 26. 402
Springer (2019) 403
[43] Sakaguchi, K., Duh, K., Post, M., Van Durme, B.: Robsut wrod reocginiton via semi-character 404
recurrent neural network. In: Proceedings of the AAAI Conference on Artificial Intelligence 405
(2017) 406
[44] Smaira, L., Carreira, J., Noland, E., Clancy, E., Wu, A., Zisserman, A.: A short note on the 407
kinetics-700-2020 human action dataset. arXiv preprint arXiv:2010.10864 (2020) 408
[45] Soomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 101 human actions classes from 409
videos in the wild. arXiv preprint arXiv:1212.0402 (2012) 410
[46] Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes from 411
videos in the wild. arXiv preprint arXiv:1212.0402 (2012) 412
[47] Sun, S., Gu, J., Gong, S.: Benchmarking robustness of text-image composed retrieval. arXiv 413
preprint arXiv:2311.14837 (2023) 414
[48] Wang, H., Yan, C., Wang, S., Jiang, X., Tang, X., Hu, Y ., Xie, W., Gavves, E.: Towards 415
open-vocabulary video instance segmentation. In: Proceedings of the International Conference 416
on Computer Vision (2023) 417
[49] Wang, M., Xing, J., Liu, Y .: Actionclip: A new paradigm for video action recognition. arXiv 418
preprint arXiv:2109.08472 (2021) 419
[50] Wang, W., Wang, R., Wang, L., Wang, Z., Ye, A.: Towards a robust deep neural network in 420
texts: A survey. arXiv preprint arXiv:1902.07285 (2019) 421
[51] Wang, Z., Wang, H.: Defense of word-level adversarial attacks via random substitution encoding. 422
In: Knowledge Science, Engineering and Management: 13th International Conference, KSEM 423
2020, Hangzhou, China, August 28–30, 2020, Proceedings, Part II 13. Springer (2020) 424
[52] Xu, H., Ghosh, G., Huang, P.Y ., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., 425
Feichtenhofer, C.: Videoclip: Contrastive pre-training for zero-shot video-text understanding. 426
arXiv preprint arXiv:2109.14084 (2021) 427
[53] Xu, J., Zhao, L., Yan, H., Zeng, Q., Liang, Y ., Sun, X.: Lexicalat: Lexical-based adversarial re- 428
inforcement training for robust sentiment classification. In: Proceedings of the 2019 conference 429
on empirical methods in natural language processing and the 9th international joint conference 430
on natural language processing (EMNLP-IJCNLP). pp. 5518–5527 (2019) 431
[54] Yang, Y ., Ma, C., Ju, C., Zhang, Y ., Wang, Y .: Multi-modal prototypes for open-set semantic 432
segmentation. arXiv preprint arXiv:2307.02003 (2023) 433
[55] Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang, X., Xu, C.: 434
Filip: Fine-grained interactive language-image pre-training. In: Proceedings of the International 435
Conference on Learning Representations (2022) 436
[56] Ye, Z., Ju, C., Ma, C., Zhang, X.: Unsupervised domain adaption via similarity-based prototypes 437
for cross-modality segmentation. In: Domain Adaptation and Representation Transfer, and 438
Affordable Healthcare and AI for Resource Diverse Global Health: Third MICCAI Workshop, 439
DART 2021, and First MICCAI Workshop, FAIR 2021, Held in Conjunction with MICCAI 440
2021, Strasbourg, France, September 27 and October 1, 2021, Proceedings 3 (2021) 441
[57] Yuan, L., Chen, D., Chen, Y .L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., 442
et al.: Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 443
(2021) 444
[58] Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., Beyer, L.: Lit: 445
Zero-shot transfer with locked-image text tuning. In: Proceedings of the IEEE Conference on 446
Computer Vision and Pattern Recognition (2022) 447
12[59] Zhang, W.E., Sheng, Q.Z., Alhazmi, A., Li, C.: Adversarial attacks on deep-learning models 448
in natural language processing: A survey. ACM Transactions on Intelligent Systems and 449
Technology (TIST) (2020) 450
[60] Zhao, P., Xie, L., Ju, C., Zhang, Y ., Wang, Y ., Tian, Q.: Bottom-up temporal action localization 451
with mutual regularization. In: Proceedings of the European Conference on Computer Vision 452
(2020) 453
[61] Zheng, H., Chen, X., Yao, J., Yang, H., Li, C., Zhang, Y ., Zhang, H., Tsang, I., Zhou, J., Zhou, 454
M.: Contrastive attraction and contrastive repulsion for representation learning. arXiv preprint 455
arXiv:2105.03746 (2021) 456
[62] Zhou, J., Dong, L., Gan, Z., Wang, L., Wei, F.: Non-contrastive learning meets language- 457
image pre-training. In: Proceedings of the IEEE Conference on Computer Vision and Pattern 458
Recognition (2023) 459
[63] Zhou, Y ., Jiang, J.Y ., Chang, K.W., Wang, W.: Learning to discriminate perturbations for 460
blocking adversarial attacks in text classification. arXiv preprint arXiv:1909.03084 (2019) 461
13A Theoretical Analysis 462
A.1 Decoding Objective 463
At each step i, the decoding objective to find argmaxtip(ti|Ti−1,V). Note that, p(Ti−1,V)is same 464
for all possible ti. As a result, our objective is written as: 465
argmax
tip(ti|Ti−1,V) = argmax
tip(ti|Ti−1,V)p(Ti−1,V) (11)
= argmax
tip(ti,Ti−1,V) (12)
= argmax
tilogp(ti,Ti−1,V) (13)
A.2 Discriminative Step 466
At the discriminative step, we choose the best set of Vthat helps decode tc,ifor each semantic-class 467
c. To understand why Vc, the set of visual samples vjwhose labels Yjare assigned to semantic-class 468
care those who help decode most efficiently, we first introduce a hidden discrete random variable 469
zj∼Qjfor each vj, indicating the index of class assignment. zj=cmeans that argmax Yj=c. 470
Knowing that all visual samples are independent and using Jensen inequality: 471
logp(ti,Ti−1,V) =X
jlogp(ti,Ti−1, vj) (14)
=X
jlogX
zjp(ti,Ti−1, vj, zj) (15)
=X
jlogX
zjQj(zj)p(ti,Ti−1, vj, zj)
Qj(zj)(16)
≥X
jX
zjQj(zj) logp(ti,Ti−1, vj, zj)
Qj(zj)(17)
Equality is attained at Qj(zj)∝p(ti,Ti−1, vj, zj). SinceP
zjQj(zj) = 1 , to maximize the lower 472
bound, we have: 473
Qj(zj) =p(ti,Ti−1, vj, zj)P
zjp(ti,Ti−1, vj, zj)(18)
=p(ti,Ti−1, vj, zj)
p(ti,Ti−1, vj)(19)
=p(zj|ti,Ti−1, vj) (20)
=p(zj|Ti, vj) (21)
Given class texts and visual samples, the best estimation is: 474
P(zj=c|Ti, vj) =

1c= argmax
cmax
kexp(S(vj,Tk
c,i))P
k′exp(S(vj,Tk′
c,i))
0otherwise(22)
Note that, Qjis well defined because: 475
lim
Qj(zj)→0+Qj(zj) logp(ti,Ti−1, vj, zj)
Qj(zj)= 0 (23)
14With Qjdefined in this way, we find the discriminative step to be identical to how ΦOVAR assigns 476
labels. We have Qj(c) = 1 only for {j|vj∈ Vc}: 477
logp(ti,Ti−1,V)≥X
jX
zjQj(zj) logp(ti,Ti−1, vj, zj)
Qj(zj)(24)
=X
cX
j,vj∈VcX
zjQj(zj) logp(ti,Ti−1, vj, zj)
Qj(zj)(25)
=X
cX
j,vj∈Vclogp(ti,Ti−1, vj, zj=c) (26)
=X
clogp(tc,i,Tc,i−1,Vc) (27)
(28)
A.3 Generative Step 478
We optimize tc,ifor each semantic-class: 479
argmax
tc,ilogp(tc,i,Tc,i−1,Vc) = argmax
tc,ip(tc,i,Tc,i−1,Vc) (29)
= argmax
tc,iY
vj∈Vcp(tc,i,Tc,i−1, vj) (30)
= argmax
tc,iY
vj∈Vcp(Tc,i−1|tc,i, vj)p(tc,i|vj)p(vj) (31)
= argmax
tc,iY
vj∈Vcp(Tc,i−1|tc,i, vj)p(tc,i|vj) (32)
Noting that p(Tc,i−1)is the same for any possible tc,i: 480
argmax
tc,ip(Tc,i−1|tc,i, vj) = argmax
tc,ip(Tc,i−1|tc,i) (33)
= argmax
tc,ip(tc,i|Tc,i−1)p(Tc,i−1)
p(tc,i)(34)
= argmax
tc,ip(tc,i|Tc,i−1)
p(tc,i)(35)
It is possible to optimize with prior p(tc,i)by considering that the more a word is frequent, the less it 481
is likely to be misspelled in real-world scenarios. In this paper, for simplicity, we assume the tc,ito 482
be uniform: 483
argmax
tc,ip(Tc,i−1|tc,i, vj) = argmax
tc,ip(tc,i|Tc,i−1) (36)
B Additional Experiments 484
B.1 DENOISER vs. Adversarial Training 485
Fig. 5 studies how adversarial training might mitigate the noise in text descriptions. We first train 486
ActionCLIP ViT-B/32-8F from scratch on K400 by randomly injecting noise in its text labels, then 487
test the model’s zero-shot performance on UCF101 under different noise rate scenarios. We find that 488
adversarial training, though promising under closed-set scenarios in previous studies, is relatively 489
ineffective under open-vocabulary settings. Specifically, training with more noise lowers significantly 490
the model’s performance under low noise rate. Additionally, its added value is limited under heavy 491
noise rate. These phenomena are probably related to the domain gap between datasets. By training 492
on noisy text descriptions, the model tends to overfit the noise pattern, jeopardizing its zero-shot 493
performance. We conclude that noisy text descriptions are better solved in testing time rather than 494
during training stage. Our DENOISER framework shows a significant advantage over the adversarial 495
training. 496
15Figure 5: Comparison to Adversarial Training. Adversarial training is not efficient, especially in
low-noise scenarios, even leading to a lower performance compared to the original model. It also
falls behind our method by a significant margin.
Figure 6: Ablation Study on the Number of Visual Samples . When fewer visual samples are used
inΦinter, our method shows a drop in performance. The bigger the noise rate, the larger the drop,
showing that Φinter plays a role of increasing importance when the noise is larger.
B.2 Ablation Study on the Number of Visual Samples 497
Fig. 6 ablates on the number of visual samples in Φinter. Our method shows a drop in performance 498
when fewer visual samples are used in Φinter. The performance tends to converge towards that 499
when solely Φintra is used. We hypothesize that fewer visual samples make Φinter harder to extract 500
added value to Φintra. With the noise rate increasing, we find an increasingly large drop in perfor- 501
mance, which shows conversely that Φinter is more important under large noise scenarios as textual 502
information becomes more ambiguous and less informative. 503
B.3 Qualitative Results 504
Fig. 7 visualizes the embedding of (visual samples, text descriptions) from three semantic-classes: 505
bird (green), ship (yellow), truck (blue) in CIFAR-10 using T-SNE. The first principal component of 506
textual embedding is removed following ReCLIP[ 12] to prevent them from clustering at the same 507
place. The Left shows that classification accuracy is low when text descriptions are noisy. Almost 508
all visual samples are recognized as “bird”. The Middle shows the embeddings of proposed text 509
candidates. Some of them remain at the same place, because they move perpendicular to this 2D space 510
in the real semantic space. We assign the best set of visual samples for each semantic-class to help 511
denoise, e.g., the blue dots are used to vote on the two candidates “trump” (red) and “truck” (purple) 512
of “trumk”. The Right shows that the denoised text descriptions improve the OV AR performance. 513
Tab. 4 quantifies some good/bad cases. We find GPT 3.5 is better at understanding semantics of 514
noisy text descriptions, e.g., “wal4ingm with a dog” →“dogwalking”. However, its output is highly 515
16Table 4: Cases of Denoised Text Descriptions for GPT 3.5 and DENOISER . The output from
GPT 3.5 [ 1] tends to be unstable, and sometimes it’s a relatively high-level understanding of noisy
text descriptions. Our DENOISER ensures a relatively faithful output in terms of spelling but could
be slightly mistaken when two words are similar in terms of both semantics and spelling.
Ground Truth Noisy Text Descriptions GPT 3.5 [1] Ours
Good Casewalking with a dog wal4ingm with a dog dogwalking walking with a dog
baby crawling babty crawling baby crying baby crawling
cutting in kitchen cutting i aitnchen cutting cutting in kitchen
Bad Case juggling balls juggling ball juggling juggling ball
Figure 7: Denoising Visualization. Left: result with noisy text descriptions (crosses w black border).
Middle: text candidates (crosses w/o black border), the visual samples (in dots) that are used to vote
for candidates. Right: denoised class texts (crosses w black border) help for better classification.
affected by input prompts, and thus tends to be unstable: important text parts are sometimes omitted 516
or misinterpreted, e.g., “babty crawling” →“baby crying”. Such unstable outputs require manual 517
cleaning, limiting its applications in reality. Our DENOISER remains faithful in terms of spelling, 518
e.g., “wal4ingm with a dog” →“walking with a dog” instead of “dogwalking”. While it may be 519
mistaken when two words are similar in semantics and spelling (rare cases), e.g., “ball” and “balls”. 520
C On the efficiency of DENOISER 521
Our model requires a trade-off between computational cost and performance. As shown in Fig. 4 522
and Fig. 6, the performance of our DENOISER increases as the number of proposals Kand the 523
percentage of the visual samples used. Since the theoretical complexity of DENOISER increases 524
linearly with Kand the percentage of visual samples used, while the marginal contribution of a larger 525
Kor percentage is decreasing, a trade-off between computational cost and performance is necessary. 526
DENOISER requires only simple operations for each iteration. After having extracted the embedding 527
of visual samples, DENOISER only requires recomputing the text embedding and doing a dot product 528
with visual embeddings, which is extremely fast. Compared to other approaches that intend to align 529
noisy text-image pairs or to train spell-checking models, DENOISER that denoises at evaluation time 530
is extremely time-saving. 531
17NeurIPS Paper Checklist 532
1.Claims 533
Question: Do the main claims made in the abstract and introduction accurately reflect the 534
paper’s contributions and scope? 535
Answer: [Yes] 536
Justification: The main claims made in the abstract and introduction accurately reflect the 537
paper’s contributions and scope. 538
Guidelines: 539
•The answer NA means that the abstract and introduction do not include the claims 540
made in the paper. 541
•The abstract and/or introduction should clearly state the claims made, including the 542
contributions made in the paper and important assumptions and limitations. A No or 543
NA answer to this question will not be perceived well by the reviewers. 544
•The claims made should match theoretical and experimental results, and reflect how 545
much the results can be expected to generalize to other settings. 546
•It is fine to include aspirational goals as motivation as long as it is clear that these goals 547
are not attained by the paper. 548
2.Limitations 549
Question: Does the paper discuss the limitations of the work performed by the authors? 550
Answer: [Yes] 551
Justification: We discuss the limitation of our method at the end of the paper, and in the 552
appendix. 553
Guidelines: 554
•The answer NA means that the paper has no limitation while the answer No means that 555
the paper has limitations, but those are not discussed in the paper. 556
• The authors are encouraged to create a separate "Limitations" section in their paper. 557
•The paper should point out any strong assumptions and how robust the results are to 558
violations of these assumptions (e.g., independence assumptions, noiseless settings, 559
model well-specification, asymptotic approximations only holding locally). The authors 560
should reflect on how these assumptions might be violated in practice and what the 561
implications would be. 562
•The authors should reflect on the scope of the claims made, e.g., if the approach was 563
only tested on a few datasets or with a few runs. In general, empirical results often 564
depend on implicit assumptions, which should be articulated. 565
•The authors should reflect on the factors that influence the performance of the approach. 566
For example, a facial recognition algorithm may perform poorly when image resolution 567
is low or images are taken in low lighting. Or a speech-to-text system might not be 568
used reliably to provide closed captions for online lectures because it fails to handle 569
technical jargon. 570
•The authors should discuss the computational efficiency of the proposed algorithms 571
and how they scale with dataset size. 572
•If applicable, the authors should discuss possible limitations of their approach to 573
address problems of privacy and fairness. 574
•While the authors might fear that complete honesty about limitations might be used by 575
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 576
limitations that aren’t acknowledged in the paper. The authors should use their best 577
judgment and recognize that individual actions in favor of transparency play an impor- 578
tant role in developing norms that preserve the integrity of the community. Reviewers 579
will be specifically instructed to not penalize honesty concerning limitations. 580
3.Theory Assumptions and Proofs 581
Question: For each theoretical result, does the paper provide the full set of assumptions and 582
a complete (and correct) proof? 583
18Answer: [Yes] 584
Justification: We provide detailed derivation in the appendix. 585
Guidelines: 586
• The answer NA means that the paper does not include theoretical results. 587
•All the theorems, formulas, and proofs in the paper should be numbered and cross- 588
referenced. 589
•All assumptions should be clearly stated or referenced in the statement of any theorems. 590
•The proofs can either appear in the main paper or the supplemental material, but if 591
they appear in the supplemental material, the authors are encouraged to provide a short 592
proof sketch to provide intuition. 593
•Inversely, any informal proof provided in the core of the paper should be complemented 594
by formal proofs provided in appendix or supplemental material. 595
• Theorems and Lemmas that the proof relies upon should be properly referenced. 596
4.Experimental Result Reproducibility 597
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 598
perimental results of the paper to the extent that it affects the main claims and/or conclusions 599
of the paper (regardless of whether the code and data are provided or not)? 600
Answer: [Yes] 601
Justification: We detail the proposed algorithm and the setting of experiments. Additionally, 602
we provide source code. 603
Guidelines: 604
• The answer NA means that the paper does not include experiments. 605
•If the paper includes experiments, a No answer to this question will not be perceived 606
well by the reviewers: Making the paper reproducible is important, regardless of 607
whether the code and data are provided or not. 608
•If the contribution is a dataset and/or model, the authors should describe the steps taken 609
to make their results reproducible or verifiable. 610
•Depending on the contribution, reproducibility can be accomplished in various ways. 611
For example, if the contribution is a novel architecture, describing the architecture fully 612
might suffice, or if the contribution is a specific model and empirical evaluation, it may 613
be necessary to either make it possible for others to replicate the model with the same 614
dataset, or provide access to the model. In general. releasing code and data is often 615
one good way to accomplish this, but reproducibility can also be provided via detailed 616
instructions for how to replicate the results, access to a hosted model (e.g., in the case 617
of a large language model), releasing of a model checkpoint, or other means that are 618
appropriate to the research performed. 619
•While NeurIPS does not require releasing code, the conference does require all submis- 620
sions to provide some reasonable avenue for reproducibility, which may depend on the 621
nature of the contribution. For example 622
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 623
to reproduce that algorithm. 624
(b)If the contribution is primarily a new model architecture, the paper should describe 625
the architecture clearly and fully. 626
(c)If the contribution is a new model (e.g., a large language model), then there should 627
either be a way to access this model for reproducing the results or a way to reproduce 628
the model (e.g., with an open-source dataset or instructions for how to construct 629
the dataset). 630
(d)We recognize that reproducibility may be tricky in some cases, in which case 631
authors are welcome to describe the particular way they provide for reproducibility. 632
In the case of closed-source models, it may be that access to the model is limited in 633
some way (e.g., to registered users), but it should be possible for other researchers 634
to have some path to reproducing or verifying the results. 635
5.Open access to data and code 636
19Question: Does the paper provide open access to the data and code, with sufficient instruc- 637
tions to faithfully reproduce the main experimental results, as described in supplemental 638
material? 639
Answer: [Yes] 640
Justification: We provide source code. Datasets are publicly accessible. 641
Guidelines: 642
• The answer NA means that paper does not include experiments requiring code. 643
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 644
public/guides/CodeSubmissionPolicy ) for more details. 645
•While we encourage the release of code and data, we understand that this might not be 646
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not 647
including code, unless this is central to the contribution (e.g., for a new open-source 648
benchmark). 649
•The instructions should contain the exact command and environment needed to run to 650
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 651
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 652
•The authors should provide instructions on data access and preparation, including how 653
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 654
•The authors should provide scripts to reproduce all experimental results for the new 655
proposed method and baselines. If only a subset of experiments are reproducible, they 656
should state which ones are omitted from the script and why. 657
•At submission time, to preserve anonymity, the authors should release anonymized 658
versions (if applicable). 659
•Providing as much information as possible in supplemental material (appended to the 660
paper) is recommended, but including URLs to data and code is permitted. 661
6.Experimental Setting/Details 662
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 663
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 664
results? 665
Answer: [Yes] 666
Justification: We specify all settings of experiments in the experiments section. 667
Guidelines: 668
• The answer NA means that the paper does not include experiments. 669
•The experimental setting should be presented in the core of the paper to a level of detail 670
that is necessary to appreciate the results and make sense of them. 671
•The full details can be provided either with the code, in appendix, or as supplemental 672
material. 673
7.Experiment Statistical Significance 674
Question: Does the paper report error bars suitably and correctly defined or other appropriate 675
information about the statistical significance of the experiments? 676
Answer: [Yes] 677
Justification: We report confidence intervals. 678
Guidelines: 679
• The answer NA means that the paper does not include experiments. 680
•The authors should answer "Yes" if the results are accompanied by error bars, confi- 681
dence intervals, or statistical significance tests, at least for the experiments that support 682
the main claims of the paper. 683
•The factors of variability that the error bars are capturing should be clearly stated (for 684
example, train/test split, initialization, random drawing of some parameter, or overall 685
run with given experimental conditions). 686
•The method for calculating the error bars should be explained (closed form formula, 687
call to a library function, bootstrap, etc.) 688
20• The assumptions made should be given (e.g., Normally distributed errors). 689
•It should be clear whether the error bar is the standard deviation or the standard error 690
of the mean. 691
•It is OK to report 1-sigma error bars, but one should state it. The authors should 692
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 693
of Normality of errors is not verified. 694
•For asymmetric distributions, the authors should be careful not to show in tables or 695
figures symmetric error bars that would yield results that are out of range (e.g. negative 696
error rates). 697
•If error bars are reported in tables or plots, The authors should explain in the text how 698
they were calculated and reference the corresponding figures or tables in the text. 699
8.Experiments Compute Resources 700
Question: For each experiment, does the paper provide sufficient information on the com- 701
puter resources (type of compute workers, memory, time of execution) needed to reproduce 702
the experiments? 703
Answer: [Yes] 704
Justification: We report information of computer resources. 705
Guidelines: 706
• The answer NA means that the paper does not include experiments. 707
•The paper should indicate the type of compute workers CPU or GPU, internal cluster, 708
or cloud provider, including relevant memory and storage. 709
•The paper should provide the amount of compute required for each of the individual 710
experimental runs as well as estimate the total compute. 711
•The paper should disclose whether the full research project required more compute 712
than the experiments reported in the paper (e.g., preliminary or failed experiments that 713
didn’t make it into the paper). 714
9.Code Of Ethics 715
Question: Does the research conducted in the paper conform, in every respect, with the 716
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 717
Answer: [Yes] 718
Justification: We conduct in the paper conform, in every respect, with the NeurIPS Code of 719
Ethics. 720
Guidelines: 721
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 722
•If the authors answer No, they should explain the special circumstances that require a 723
deviation from the Code of Ethics. 724
•The authors should make sure to preserve anonymity (e.g., if there is a special consid- 725
eration due to laws or regulations in their jurisdiction). 726
10.Broader Impacts 727
Question: Does the paper discuss both potential positive societal impacts and negative 728
societal impacts of the work performed? 729
Answer: [Yes] 730
Justification: Our model helps users better leverage the existing Open-V ocabulary models in 731
a more robust way. 732
Guidelines: 733
• The answer NA means that there is no societal impact of the work performed. 734
•If the authors answer NA or No, they should explain why their work has no societal 735
impact or why the paper does not address societal impact. 736
•Examples of negative societal impacts include potential malicious or unintended uses 737
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 738
(e.g., deployment of technologies that could make decisions that unfairly impact specific 739
groups), privacy considerations, and security considerations. 740
21•The conference expects that many papers will be foundational research and not tied 741
to particular applications, let alone deployments. However, if there is a direct path to 742
any negative applications, the authors should point it out. For example, it is legitimate 743
to point out that an improvement in the quality of generative models could be used to 744
generate deepfakes for disinformation. On the other hand, it is not needed to point out 745
that a generic algorithm for optimizing neural networks could enable people to train 746
models that generate Deepfakes faster. 747
•The authors should consider possible harms that could arise when the technology is 748
being used as intended and functioning correctly, harms that could arise when the 749
technology is being used as intended but gives incorrect results, and harms following 750
from (intentional or unintentional) misuse of the technology. 751
•If there are negative societal impacts, the authors could also discuss possible mitigation 752
strategies (e.g., gated release of models, providing defenses in addition to attacks, 753
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 754
feedback over time, improving the efficiency and accessibility of ML). 755
11.Safeguards 756
Question: Does the paper describe safeguards that have been put in place for responsible 757
release of data or models that have a high risk for misuse (e.g., pretrained language models, 758
image generators, or scraped datasets)? 759
Answer: [NA] 760
Justification: The paper poses no such risks. 761
Guidelines: 762
• The answer NA means that the paper poses no such risks. 763
•Released models that have a high risk for misuse or dual-use should be released with 764
necessary safeguards to allow for controlled use of the model, for example by requiring 765
that users adhere to usage guidelines or restrictions to access the model or implementing 766
safety filters. 767
•Datasets that have been scraped from the Internet could pose safety risks. The authors 768
should describe how they avoided releasing unsafe images. 769
•We recognize that providing effective safeguards is challenging, and many papers do 770
not require this, but we encourage authors to take this into account and make a best 771
faith effort. 772
12.Licenses for existing assets 773
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 774
the paper, properly credited and are the license and terms of use explicitly mentioned and 775
properly respected? 776
Answer: [Yes] 777
Justification: All the assets are properly cited. License and terms of use are properly 778
respected. 779
Guidelines: 780
• The answer NA means that the paper does not use existing assets. 781
• The authors should cite the original paper that produced the code package or dataset. 782
•The authors should state which version of the asset is used and, if possible, include a 783
URL. 784
• The name of the license (e.g., CC-BY 4.0) should be included for each asset. 785
•For scraped data from a particular source (e.g., website), the copyright and terms of 786
service of that source should be provided. 787
•If assets are released, the license, copyright information, and terms of use in the 788
package should be provided. For popular datasets, paperswithcode.com/datasets 789
has curated licenses for some datasets. Their licensing guide can help determine the 790
license of a dataset. 791
•For existing datasets that are re-packaged, both the original license and the license of 792
the derived asset (if it has changed) should be provided. 793
22•If this information is not available online, the authors are encouraged to reach out to 794
the asset’s creators. 795
13.New Assets 796
Question: Are new assets introduced in the paper well documented and is the documentation 797
provided alongside the assets? 798
Answer: [Yes] 799
Justification: We provided well-documented source code. 800
Guidelines: 801
• The answer NA means that the paper does not release new assets. 802
•Researchers should communicate the details of the dataset/code/model as part of their 803
submissions via structured templates. This includes details about training, license, 804
limitations, etc. 805
•The paper should discuss whether and how consent was obtained from people whose 806
asset is used. 807
•At submission time, remember to anonymize your assets (if applicable). You can either 808
create an anonymized URL or include an anonymized zip file. 809
14.Crowdsourcing and Research with Human Subjects 810
Question: For crowdsourcing experiments and research with human subjects, does the paper 811
include the full text of instructions given to participants and screenshots, if applicable, as 812
well as details about compensation (if any)? 813
Answer: [NA] 814
Justification: The paper does not involve crowdsourcing nor research with human subjects. 815
Guidelines: 816
•The answer NA means that the paper does not involve crowdsourcing nor research with 817
human subjects. 818
•Including this information in the supplemental material is fine, but if the main contribu- 819
tion of the paper involves human subjects, then as much detail as possible should be 820
included in the main paper. 821
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 822
or other labor should be paid at least the minimum wage in the country of the data 823
collector. 824
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 825
Subjects 826
Question: Does the paper describe potential risks incurred by study participants, whether 827
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 828
approvals (or an equivalent approval/review based on the requirements of your country or 829
institution) were obtained? 830
Answer: [NA] 831
Justification: The paper does not involve crowdsourcing nor research with human subjects. 832
Guidelines: 833
•The answer NA means that the paper does not involve crowdsourcing nor research with 834
human subjects. 835
•Depending on the country in which research is conducted, IRB approval (or equivalent) 836
may be required for any human subjects research. If you obtained IRB approval, you 837
should clearly state this in the paper. 838
•We recognize that the procedures for this may vary significantly between institutions 839
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 840
guidelines for their institution. 841
•For initial submissions, do not include any information that would break anonymity (if 842
applicable), such as the institution conducting the review. 843
23