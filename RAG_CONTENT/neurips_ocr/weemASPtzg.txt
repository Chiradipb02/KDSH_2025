Linear Causal Representation Learning from
Unknown Multi-node Interventions
Burak Varıcı∗
Carnegie Mellon UniversityEmre Acartürk
Rensselaer Polytechnic Institute
Karthikeyan Shanmugam
Google DeepMindAli Tajer
Rensselaer Polytechnic Institute
Abstract
Despite the multifaceted recent advances in interventional causal representation
learning (CRL), they primarily focus on the stylized assumption of single-node
interventions. This assumption is not valid in a wide range of applications, and
generally, the subset of nodes intervened in an interventional environment is fully
unknown . This paper focuses on interventional CRL under unknown multi-node
(UMN) interventional environments and establishes the first identifiability results
forgeneral latent causal models (parametric or nonparametric) under stochastic in-
terventions (soft or hard) and linear transformation from the latent to observed space.
Specifically, it is established that given sufficiently diverse interventional environ-
ments, (i) identifiability up to ancestors is possible using only softinterventions,
and (ii) perfect identifiability is possible using hard interventions. Remarkably,
these guarantees match the best-known results for more restrictive single-node
interventions. Furthermore, CRL algorithms are also provided that achieve the iden-
tifiability guarantees. A central step in designing these algorithms is establishing the
relationships between UMN interventional CRL and score functions associated with
the statistical models of different interventional environments. Establishing these
relationships also serves as constructive proof of the identifiability guarantees.
1 Introduction
Causal representation learning (CRL) is a major leap in causal inference, moving away from the
conventional objective of discovering causal relationships among a set of variables and learning the
variables themselves. By combining the strengths of causal inference and machine learning, CRL
specifies data representations that facilitate reasoning and planning [ 1]. CRL is motivated by the
premise that in a wide range of applications, a lower-dimensional latent set of variables with causal
interactions generates the usually high-dimensional observed data. Therefore, CRL’s objective is to
use the observed data and learn the latent causal generative factors, which include the causal latent
variables and their causal relationships.
CRL objectives. Formally, consider a set of latent causal random variables Z∈Rnand a directed
acyclic graph (DAG) Gthat encodes the causal relationships among Z. The latent variables are
transformed by an unknown function gto generate the observed random variables X∈Rd, where
X≜g(Z). CRL aims to use Xto recover the latent causal variables Zand the causal structure G.
Two central questions of CRL pertain to identifiability , which refers to determining the conditions
under which ZandGcan be recovered, and achievability , which refers to designing CRL algorithms
that can achieve the foreseen identifiability guarantees. Identifiability has been demonstrated to
∗Work was done while BV was a Ph.D. student at Rensselaer Polytechnic Institute.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).be inherently under-constrained [ 2], prompting the development of diverse methodologies that
incorporate inductive biases to enable identifiability. Interventional CRL is one direction with
significant recent advances in which interventions on latent causal variables are used to create
statistical diversity in the observed data [1, 3–6].
Unknown multi-node interventions. Despite covering many aspects of interventional CRL, such as
parametric versus nonparametric causal models, parametric versus nonparametric transformations,
and intervention types, the majority of the existing studies assume that the interventions are single-
node, i.e., exactly one latent variable is intervened in each environment [ 3–10]. This assumption,
however, is restrictive in some of the application domains of CRL such as biology and robotics in
which generally the subset of nodes intervened in an intervention environment can be fully unknown .
For instance, biological perturbations in genomics are imperfect interventions with off-target effects
on other genes [ 11,12], or interventions on robotics applications are likely to affect multiple causal
variables [ 13]. Hence, realizing the promises of CRL critically hinges on dispensing with the
assumption of single-node interventions.
In this paper, we address the open problem of using unknown multi-node (UMN) stochastic interven-
tions to recover the latent causal variables Zand their causal graph G, wherein each environment an
unknown subset of nodes are intervened. We consider a general latent causal model (parametric or
nonparametric) and focus on the linear transformations as an important class of parametric transfor-
mation models. We establish identifiability results and design algorithms to achieve them under both
soft and hard interventions. For this purpose, we delineate connections between UMN interventions
and the properties of score functions, i.e., the gradients of the logarithm of density functions. This
score-based framework is the UMN counterpart of the single-node framework proposed in [ 5,7],
albeit with significant technical differences. Our contributions are summarized below.
•We show that under sufficiently diverse interventional environments, UMN stochastic hard inter-
ventions suffice to guarantee perfect identifiability of the latent causal graph and the latent variables
(up to permutations and element-wise scaling).
•We show that under sufficiently diverse interventional environments, UMN soft interventions
guarantee identifiability up to ancestors – transitive closure of the latent DAG is recovered, and
latent variables are recovered up to a linear function of their ancestors. Remarkably, these
guarantees match the best possible results in the literature of single-node interventions.
•We design score-based CRL algorithms for implementing CRL with UMN interventions with prov-
able guarantees. These guarantees also serve as constructive proof steps of the identifiability results.
Challenges of UMN interventions. There are two broad challenges specific to addressing the UMN
intervention setting that render it substantially distinct from the single-node (SN) intervention setting.
First, in SN interventions, since the learner knows exactly one node is intervened in each environment,
it can readily identify the intervention targets up to a permutation. In contrast, in UMN interventions,
the learner does not know how many nodes are intervened in each environment. Therefore, the
nature of resolving the uncertainty about the intervention targets becomes fundamentally different.
An immediate impact of this is that it becomes more challenging to properly capitalize on the
statistical diversity embedded in the interventional data. Secondly, in SN interventions, only one
causal mechanism changes across the environments. Such sparse variations of the causal mechanisms
are a core property leveraged by various existing CRL approaches, e.g., contrastive learning [ 8],
and score-based framework [ 6,7]. On the contrary, UMN interventions allow for many concurrent
causal mechanism changes, which renders leveraging sparsity patterns in mechanism variations futile.
Finally, since intervention targets are unknown, our central algorithmic idea is to properly aggregate
the UMN interventional environments to create new distinct environments under which the inherent
statistical diversity is more accessible.
1.1 Related literature
Single-node interventional CRL. The majority of the studies on interventional CRL focus on SN
interventions [ 3–10,14], which can be categorized based on their assumptions on the latent causal
model, transformation, and intervention model. Based on this taxonomy, it has been shown that SN
hard interventions suffice for identifiability with general latent causal models and linear transforma-
tions (one intervention per node) [ 7], with linear Gaussian latent models and general transformations
(one intervention per node) [ 8], and with general latent models and general transformations (two
interventions per node) [ 6,9]. For the less restrictive SN soft interventions, identifiability up to
2Table 1: Comparison of the results to existing work in multi-node interventional CRL. We note that all studies
assume linear transformation.
Work Latent model Int. type Main assumption on interventions Identifiability (ID)
[14] Linear Soft |pa(i)|independent int. mechanisms ID up to surrounding
[16] General do strongly separated interventions perfect ID
Theorem 1 General Hard lin. indep. interv. (Assumption 1) perfect ID
Theorem 2 General Soft lin. indep. interv. (Assumption 1) ID up to ancestors
ancestors is shown for general latent models and linear transformations [ 7] and linear Gaussian
latent models and general transformations [ 8]. Furthermore, under additional assumptions such as
sufficiently nonlinear latent models, the latent DAG is shown to be perfectly identifiable [ 7,10,14]. In
a related study, [ 15] focuses on learning the latent DAG (but without learning latent causal variables)
using SN hard interventions without parametric assumptions on the model.
Multi-node interventional CRL. The studies on MN intervention settings are sparser than the SN
intervention settings. Table 1 summarizes the results closely related to the scope of this paper along
with the identifiability results established in this paper. In summary, the existing studies either provide
partial identifiability or focus on non-stochastic dointerventions. The study in [ 14] focuses on
linear non-Gaussian latent models and linear transformations and uses soft interventions to establish
identifiability up to surrounding variables by using multiple interventional mechanisms for each node.
In a different study, [ 16] uses strongly separated multi-node dointerventions and provides perfect
identifiability results for general latent models and linear transformations. We also note the partially
related study in [ 17] that applies soft interventions on a subset of nodes and aims to disentangle the
non-intervened variables from the intervened ones. Distinct from all these studies, we address the
open problem of perfect identifiability under UMN stochastic interventions.
Other approaches to CRL. We note that there exist other interesting settings that address CRL
without interventions. Some examples include using multi-view data [ 18–21], leveraging temporal
sequences [ 22,23], building on nonlinear independent component analysis (ICA) principles to
identify polynomial latent causal models [ 24], and imposing sparsity constraints to obtain partial
disentanglement [ 25,26], and grouping of observational variables [ 27]. We refer to [ 28] for a detailed
literature review on various CRL problems.
2 CRL setting and preliminaries
Notations. Vectors are represented by lowercase bold letters, and element iof vector ais denoted
byai. Matrices are represented by uppercase bold letters, and we denote row iand column j
of matrix AbyAiand by A:,j, respectively, and Ai,jdenotes the entry at row iand column j.
We use null({A1, . . . ,Ar})to denote the nullspace of the matrix consisting of the row vectors
{A1, . . . ,Ar}. Forn∈N, we define [n]≜{1, . . . , n }. The row permutation matrix associated with
any permutation πof[n]is denoted by Pπ. We denote the indicator function by 1. We use im(f)
to denote the image of a function fanddim(V)to denote the dimension of a subspace V. Random
variables and their realizations are presented by upper and lower case letters, respectively.
2.1 Latent causal model
Consider a latent causal space consisting of ncausal random variables Z≜[Z1, . . . , Z n]⊤. An
unknown linear transformation G∈Rd×nmaps Zto the observed random variables denoted by
X≜[X1, . . . , X d]⊤according to:
X=G·Z , (1)
where d≥nandGis full rank. The probability density functions (pdfs) of XandZare denoted by
pXandpZ, respectively. We assume that pZhas full support on Rn. Subsequently, pXis supported
onX≜im(G). The causal relationships among latent variables Zare represented by a DAG Gin
which the i-th node corresponds to Zi. Hence, pZfactorizes according to:
pZ(z) =nY
i=1pi(zi|zpa(i)), (2)
3where pa(i)denotes the set of parents of node iinG. The conditional pdfs {pi(zi|zpa(i)) :i∈[n]}
are assumed to be continuously differentiable with respect to all zvariables. We use ch(i),an(i),
andde(i)to denote the children, ancestors, and descendants of node i, respectively. We say that
a permutation (π1, . . . , π n)of[n]is a valid causal order if the membership πi∈an(πj)indicates
thati < j . Without loss of generality, we assume that (1, . . . , n )is a valid causal order. We will
specialize some of our results for the latent causal models with additive noise1specified by
Zi=fi(Zpa(i)) +Ni, (3)
where functions {fi:i∈[n]}capture the causal dependence of node ion its parents and the terms
{Ni:i∈[n]}represent the exogenous noise variables.
2.2 Unknown multi-node intervention models
In addition to the observational environment, we have MUMN interventional environments denoted
by{Em:m∈[M]}. We assume that the set of nodes intervened in each environment is unknown , and
denote the set intervened in environment EmbyIm⊆[n]. Accordingly, we define the intervention
signature matrix Dint∈ {0,1}n×Mto compactly represent the intervention targets under various
environments as
[Dint]i,m=1{i∈Im},∀i∈[n],∀m∈[M]. (4)
Them-th column of Dintlists the indices of the nodes intervened in environment Em, which we
refer to as the intervention vector of environment Em. Ensuring identifiability inevitably imposes
restrictions on the structure of Dint. For instance, if the i-th row of Dintis a zero vector, it means
that node iis not intervened in any environment, then the perfect identifiability is not possible [ 4]2.
Therefore, to avoid such impossibility cases, we impose the mild condition that Dinthas sufficiently
diverse columns, formalized next.
Assumption 1. Intervention signature matrix Dintdefined in (4)is full row rank, i.e., it contains n
linearly independent intervention vectors.
In this paper, we consider UMN stochastic interventions and address identifiability results under both
hard interventions as well as soft interventions as the most general form of intervention.
Soft interventions. A soft intervention on node ialters the observational causal mechanism pi(zi|
zpa(i))to an interventional causal mechanism qi(zi|zpa(i)). Such a change occurs in node iin all
the environments Emthat contain node i, i.e., i∈Im. Subsequently, the pdf of the latent variables in
environment Em, denoted by pm
Z, factorizes according to:
pm
Z(z)≜Y
i∈Imqi(zi|zpa(i))Y
i̸∈Impi(zi|zpa(i)),∀m∈[M]. (5)
Hard interventions. Under a hard intervention on node i, the functional dependence of node i
on its parents is removed, and the observational causal mechanism pi(zi|zpa(i))is changed to an
interventional causal mechanism qi(zi), independent of parents of node i.
To distinguish the observational and interventional data, we denote the latent and observed random
variables in environment EmbyZmandXm, respectively. We note that interventions do not affect
the transformation G. Hence, in Emwe have Xm=G·Zmfor all m∈[M].
Score functions. The score function of a pdf is defined as the gradient of its logarithm. We denote
the score functions associated with the distributions of ZmandXmby
sm
Z(z)≜∇logpm
Z(z),and sm
X(x)≜∇logpm
X(x),∀m∈[M]. (6)
Note that, using the factorization in (5), sm
Zdecomposes as
sm
Z(z) =X
i∈Im∇logqi(zi|zpa(i)) +X
i̸∈Im∇logpi(zi|zpa(i)). (7)
We denote the difference in score functions between interventional and observational environments by
∆sm
Z(z)≜sm
Z(z)−sZ(z)and ∆sm
X(x)≜sm
X(x)−sX(x),∀m∈[M]. (8)
1Perfect identifiability results in the closely related literature are given for additive noise models [ 7,4,8,16].
2[4, Proposition 5] shows that if the non-intervened node ihas at least one parent, then perfect identifiability
is not possible.
42.3 Identifiability criteria
In CRL, we use observed variables Xto recover the true latent variables Zand the latent causal
graphG. We denote a generic estimator of Zgiven XbyˆZ(X) :Rd→Rn. We also consider a
generic estimate of Gdenoted by ˆG. To assess the fidelity of the estimates ˆZ(X)andˆGwith respect
to the ground truth ZandG, we provide the following well-known identifiability measures.
Definition 1 (Identifiability) .For CRL under linear transformations, we define:
1.Perfect identifiability: ˆGandGare isomorphic, and the estimator ˆZ(X)satisfies that
ˆZ(X) =Pπ·Cs·Z , ∀Z∈Rn, (9)
where Cs∈Rn×nis aconstant diagonal matrix with nonzero diagonal entries and Pπis a row
permutation matrix.
2.Identifiability up to ancestors: ˆGand transitive closure of G, denoted by Gtc, are isomorphic,
and the estimator ˆZ(X)satisfies that
ˆZ(X) =Pπ·Can·Z , ∀Z∈Rn, (10)
where Can∈Rn×nis aconstant matrix with nonzero diagonal entries that satisfies [Can]i,j= 0
for all j /∈ {an(i)∪ {i}}, andPπis a row permutation matrix.
In the algorithm we will design, estimating ˆZ(X)andˆGare facilitated by estimating the inverse of
the transformation G, that is Moore-Penrose inverse G†≜[G⊤·G]−1·G⊤, which we refer to as
thetrue encoder . To formalize the process of estimating the true encoder, we define Has the set of
candidate encoders specified by H≜{H∈Rn×d:rank(H) =nandH⊤·H·X=X ,∀X∈ X} .
Corresponding to any pair of observation Xand valid encoder H∈ H, we define ˆZ(X;H)as an
auxiliary estimate of Zgenerated as ˆZ(X;H)≜H·X= (H·G)·Z.
3 Identifiability under UMN interventions
In this section, we present the main identifiability and achievability results for CRL with UMN
interventions and interpret them in the context of the recent results in the literature. We start by
specifying the regularity conditions on the statistical models, which are needed to ensure sufficient
statistical diversity and establish identifiability results for hard and soft UMN interventions. The
constructive proofs of the results are based on CRL algorithms, the details of which are presented in
Section 4. Complete proofs are deferred to Appendix A.
We note that the UMN setting subsumes SN interventions. Similarly to all the existing identifiability
results from SN interventions, it is necessary to have sufficient statistical diversity created by the
intervention models.3These conditions can be generally presented in the form of regularity conditions
on the probability distributions. Specifically, a commonly adopted regularity condition (or its
variations) in the SN intervention setting is that for every possible pair (i, j)where i∈[n], j∈pa(i),
the following term cannot be a constant function in z,
∂
∂zj
logpi(zi|zpa(i))
qi(zi|zpa(i)) ∂
∂zilogpi(zi|zpa(i))
qi(zi|zpa(i))−1
. (11)
We present a counterpart of these conditions for UMN interventions, which involves one additional
term to account for the effect of intervening on multiple nodes simultaneously.
Definition 2 (Intervention regularity) .We say that an interventions are regular if for every possible
triplet (i, j, c)where i∈[n], j∈pa(i)andc∈Q, the following ratio cannot be a constant function
inz
∂
∂zj
logpi(zi|zpa(i))
qi(zi|zpa(i))+c·logpj(zj|zpa(j))
qj(zj|zpa(j)) ∂
∂zilogpi(zi|zpa(i))
qi(zi|zpa(i))−1
. (12)
3Some examples include Assumption 1 in [ 6], generic SN interventions in [ 4], no pure shift interventions
condition in [8], and the genericity condition in [9].
5Essentially,∂
∂zjlogpi(zi|zpa(i))
qi(zi|zpa(i))captures the effect of intervening on node ion the score associated
with node j. In our method, we will use combinations of score differences of multi-node environments.
This regularity condition ensures that the effect of a multi-node intervention is not the same on the
scores associated with different nodes. Given these properties, we establish perfect identifiability for
CRL with linear transformations using UMN stochastic hard interventions.
Theorem 1 (Identifiability under UMN hard interventions) .Under Assumption 1 and a latent model
with additive noise,
1. perfect latent recovery is possible using regular UMN hard interventions; and
2.if the latent causal model satisfies adjacency-faithfulness, then perfect latent DAG recovery is
possible using regular UMN hard interventions.
Theorem 1 is the first perfect identifiability result using UMN stochastic hard interventions. In
contrast, [ 16] establishes perfect latent recovery using highly more stringent do-interventions. Fur-
thermore, Theorem 1 establishes the first perfect latent DAG recovery result (under any type of
multi-node interventions) for nonparametric latent models. We note that the capability of handling
nonparametric latent models stems from leveraging the score functions. Similar properties are demon-
strated by the prior work on score-based CRL for SN interventions [ 7]. It is noteworthy that we use a
total of n+ 1environments whereas the study in [ 16] requires 2⌈log2n⌉dointerventions of strongly
separating sets. However, we show that identifiability is impossible using strongly separating sets of
UMN stochastic hard interventions (see Appendix A.6).
Next, we consider UMN softinterventions. Since soft interventions retain the ancestral dependence
of the intervened node, in general, the identifiability guarantees for soft interventions are weaker than
those of hard interventions. Next, we establish that UMN soft interventions guarantee identifiability
up to ancestors for the general causal latent models and linear transformations.
Theorem 2 (Identifiability under UMN soft interventions) .Under Assumption 1, identifiability up to
ancestors is possible using regular UMN soft interventions.
Identifiability up to ancestors has recently shown to be possible using SN soft interventions on general
latent models [ 7]. Theorem 2 establishes the same identifiability guarantees without the restrictive
assumption of SN interventions. Furthermore, Theorem 2 is significantly different from existing
results for UMN soft interventions. Specifically, the study in [ 14] focuses on linear non-Gaussian
latent models and requires |pa(i)|+ 1distinct mechanisms for each node i. In contrast, Theorem 2
does not make parametric assumptions on latent variables and works with sufficiently diverse
interventions described by Assumption 1 instead of requiring multiple interventional mechanisms for
the same node.
4 UMN interventional CRL algorithm
In this section, we design the Unknown Multi-node Interventional (UMNI)-CRL algorithm that
achieves identifiability guarantees presented in Section 3. This algorithm falls in the category of
score-based frameworks for CRL [ 5,7] and incorporates novel components to this framework that
facilitate UMN interventions with provable guarantees. Our score-based approach uses the structural
properties of score functions and their variations across different interventional environments to
find reliable estimates for the true encoder G†. The critical step involved is a process that can
aggregate the score differences under the available interventional environments, which have entirely
unknown intervention targets, and reconstruct the score differences for any desired hypothetical set of
intervention targets. In particular, we establish that such desired score differences can be computed
by aggregating the score differences available under the given UMN interventions. The proposed
UMNI-CRL algorithm consists of four stages for implementing CRL. The properties of these stages
also serve as the steps of constructive proof for identifiability results. We present the key algorithmic
stages and their properties in the remainder of this section and defer their proofs to Appendix A.
Stage 1: Basis score differences. In the first stage, we compute score differences for each interven-
tional environment and construct the basis score difference functions that are linearly independent.
The purpose of these functions is to subsequently use them and reconstruct the score differences under
any arbitrary hypothetical interventional environment. To this end, we use the following relationship
between score functions of XandZ.
6Algorithm 1 U nknown Multi-node Interventional (UMNI)-CRL
1:Input: Samples of Xfrom environment E0and interventional environments {Em:m∈[M]}
2:Stage 1: Choose basis score differences and construct ∆SXusing (17)
3:Stage 2: Identifiability up to a causal order
4:fort∈(1, . . . , n )do
5: forw∈ W do ▷Wis specified in (18)
6: V ← projnull({H∗
i:i∈[t−1]})im(∆SX·w)
7: ifdim(V) = 1 then
8: pickv∈im(∆SX·w)\span({H∗
i:i∈[t−1]})
9: H∗
t←v/∥v∥2and [W]:,t←w
10: break
11:Stage 2 outputs: H∗andW
12:Stage 3: Identifiability up ancestors
13:Initialize ˆGwith empty graph over nodes [n]
14:fort∈(n−1, . . . , 1)do
15: forj∈(t+ 1, . . . , n )do
16: ifj∈ˆch(t)then
17: continue
18: if_parent ←True
19: Mt,j←[j−1]\ {ˆch(t)∪ {t}}
20: for(α, β)∈ {− (∥W:,j∥1, . . . ,∥W:,j∥1)} ×[∥W:,t∥1]do
21: w∗←α[W]:,t+β[W]:,j
22: V ← projnull({H∗
i:i∈ M t,j})im(∆SX·w∗)
23: ifdim(V) = 1 then
24: pickv∈im(∆SX·w∗)\span({H∗
i:i∈ M t,j})
25: H∗
j←v/∥v∥2and [W]:,j←w∗
26: Setif_parent ←False and break
27: ifif_parent is True then
28: Addt→jandt→utoˆGfor all u∈ˆde(j) ▷edges to identified descendants
29:Stage 3 outputs: ˆG,H∗andW
30:ifthe interventions are hard then
31: Stage 4: Unmixing for hard interventions
32: fort∈(2, . . . , n )do ▷refine rows of H∗sequentially
33: ˆZ←ˆZ(X;H∗)
34: uobs← − Cov( ˆZt,ˆZˆ an(t))·[Cov( ˆZˆ an(t))]−1
35: form∈ {i:Wi,t̸= 0}do ▷searching for a suitable environment
36: ˆZm←ˆZm(X;H∗)
37: um← − Cov( ˆZm
t,ˆZm
ˆ an(t))·[Cov( ˆZm
ˆ an(t))]−1
38: if(um·ˆZm
ˆ an(t)+ˆZm
t)⊥ ⊥ˆZm
ˆ an(t)andum̸=uobsthen
39: H∗
t←H∗
t+um·H∗
ˆ an(t)▷removing the effect of the ancestors on Zt
40: break
41: ˆZ←ˆZ(X;H∗) ▷use recovered Zin obs. env. for graph recovery
42: fort∈(1, . . . , n )do
43: forj∈ˆch(t)do
44: ifˆZt⊥ ⊥ˆZj| {ˆZi:i∈ˆ pa(j)\ {t}}then
45: Remove t→jfrom ˆG ▷removing the edges from the nonparent ancestors
46:Return ˆGandˆZ
7Lemma 1 ([7, Corollary 2]) .Latent and observational score functions are related via sX(x) =
[G†]⊤·sZ(z), where x=G·z.
Using Lemma 1 for the scores and score differences defined in (7) and (8), respectively, we have
∆sm
X(x) = [G†]⊤·∆sm
Z(z)(7)= [G†]⊤·X
i∈Im∇logpi(zi|zpa(i))
qi(zi|zpa(i)). (13)
We compactly represent the summands in the right-hand side of (13) by defining the matrix-valued
function Λ:Rn→Rn×nwith the entries
[Λ(z)]j,i≜∂
∂zjlogpi(zi|zpa(i))
qi(zi|zpa(i)),∀i, j∈[n], (14)
based on which (13) can be restated as
∆sm
X(x) = [G†]⊤·Λ(z)·[Dint]:,m. (15)
We note that [Λ(z)]i,jis constantly zero for i /∈ {pa(j)∪ {j}}, and j-th column of Λis a function
of the variables in {zk:k∈pa(j)∪ {j}}which implies that the columns of Λare linearly
independent. Throughout the rest of the paper, we omit the arguments of the functions ∆sm
XandΛ
when the dependence is clear from the context. Note that Dinthasnlinearly independent columns
(Assumption 1). Denote the indices of the independent columns by {b1, . . . , b n}and define the basis
intervention matrix D∈Rn×nusing these columns as
[D]i,m≜[Dint]i,bm=1{i∈Ibm},∀i, m∈[n]. (16)
Subsequently, it can be readily verified that the score difference functions {∆sm
X:m∈ {b1, . . . , b n}}
are also linearly independent by leveraging (15) and linearly independent columns of Λ. Hence, these
score difference functions are sufficient to reconstruct the remaining unavailable score difference
functions. As such, {∆sm
X:m∈ {b1, . . . , b n}}serve as basis score difference functions. We stack
these basis score differences, where each is a d-dimensional vector, to construct the matrix-valued
function ∆SX:X →Rd×n, which is used as the basis score difference matrix in the subsequent
stages.
∆SX≜
∆sb1
X, . . . , ∆sbn
X
= [G†]⊤·Λ·D. (17)
Finally, we note that ∆SXis directly estimated from samples of Xvia learning {∆sbm
X:m∈[n]}.
Since Λencodes the score differences in latent space, it cannot be estimated directly. Furthermore,
Dis unknown, and (17) is given to emphasize the relationship between observed and latent score
differences.
Stage 2: Identifiability up to an unknown causal order. We design a process that aggregates score
differences of the UMN interventions and obtains a partial identifiability guarantee (identifiability up
to an unknown causal order) as an intermediate step toward more accurate identifiability. Specifically,
we linearly aggregate the columns of ∆SXsuch that those aggregate scores facilitate identifiability up
to an unknown causal order. Such mixing of the columns is facilitated by computing ∆SX·W, where
the mixing matrix W∈Rn×nshould be learned. Given the decomposition of ∆SXin(17), if we
learnWsuch that D·Wis upper triangular up to a row permutation, then we can subsequently learn
an intermediate estimate H∗using the image of ∆SX. This ensures identifiability up to an unknown
causal order since rows of H∗will be equal to combinations of rows of G†up to a causal order.
We design an iterative process to sequentially learn the columns of W. Specifically, at each iteration
of Stage 2, we learn an integer-valued vector wsuch that the projection of im(∆SX·w)onto the
nullspace of the partially recovered encoder estimate becomes a one-dimensional subspace. To see
why this procedure works, note that the function ∆SX·wis essentially a combination of SN latent
score differences via (17), and the SN score difference ∇logpi(zi|zpa(i))− ∇logqi(zi|zpa(i))
is a one-dimensional subspace if and only if the intervened node ihas no parents. By taking the
projection of the im(∆SX·w)onto the nullspace of the partially learned encoder while searching
for a desired w, we ensure that the final encoder estimate H∗of this stage will be full-rank. Finally,
we use κto denote maximum determinant of a matrix in {0,1}(n−1)×(n−1), and show that the set
W≜{−κ, . . . , +κ}n(18)
is guaranteed to contain such wvectors. The following result summarizes the guarantees of this
procedure.
8Lemma 2. Under Assumption 1 and intervention regularity, outputs of Stage 2 of Algorithm 1 satisfy:
1.D·Whas nonzero diagonal entries and is upper triangular up to a row permutation.
2.[H∗]t∈span({[G†]πj:j∈[t]})for all t∈[n]and{[H∗]t:t∈[n]}are linearly independent.
Proof: See Appendix A.1. □
Stage 3: Identifiability up to ancestors. Next, we refine the outcome of Stage 2 to ensure identifia-
bility up to ancestors by updating the columns of Wsuch that the entries of D·Wcan be nonzero
only for the coordinates that correspond to ancestor-descendant node pairs. For this purpose, we
design Stage 3 of UMNI-CRL that iteratively updates the columns of W. The key idea is that the
edges in the transitive closure graph Gtccan be determined by investigating the subspaces’ dimensions
similarly to Stage 2. Leveraging this property, for all node pairs that do not constitute an edge in Gtc,
we aggregate the corresponding columns of Wsuch that the corresponding entry of D·Wwill be
zero. In Theorem 3, we show that the outputs of this stage achieve identifiability up to ancestors, that
isˆGis isomorphic to GtcandˆZiis a linear function of {Zπj:j∈an(i)∪ {i}}for all i∈[n].
Theorem 3. Under Assumption 1 and regular UMN soft interventions, outputs of Stage 3 of Algo-
rithm 1 have the following properties.
1. The estimate ˆZ(X;H∗)satisfies identifiability up to ancestors.
2.ˆGandGtcare related through a graph isomorphism.
Proof: See Appendix A.2. □
Stage 4: Perfect identifiability via hard interventions. In the case of hard interventions, we
apply an unmixing procedure to further refine our estimates and achieve perfect identifiability. This
stage consists of two steps. The first step relies on the property that the intervened node becomes
independent of its non-descendants and updates rows of the encoder estimate sequentially. In
the second step, we leverage the knowledge of ancestral relationships and use a small number of
conditional independence tests to refine the graph estimate from transitive closure Gtcto the true
latent DAG G. The following theorem summarizes the guarantees achieved by Algorithm 1.
Theorem 4. Under Assumption 1 and regular UMN hard interventions for an additive noise model,
outputs of Stage 4 of Algorithm 1 have the following properties.
1. Estimate ˆZ(X;H∗)satisfies perfect latent variable recovery.
2. IfpZis adjacency-faithful to G, then ˆGandGare related through a graph isomorphism.
Proof: See Appendix A.3. □
Finally, we note that the computational cost of UMNI-CRL is dominated by the cardinality of the
search space for aggregating the score differences, e.g., in the worst-case, Stage 2 has O((2κ)n)
complexity. Therefore, the structure of the UMN interventions determines the complexity via its
determinant. We elaborate on the computational complexity of the algorithm and the range of κin
Appendix A.8.
5 Simulations
We empirically assess the performance of the UMNI-CRL algorithm for recovering the latent DAG G
and latent variables Z. Implementation details and additional results are provided in Appendix B4.
Data generation. To generate G, we use Erd ˝os-Rényi model with density 0.5andn∈ {4,5,6,7,8}
nodes. For the causal models, we adopt linear structural equation models (SEMs) with Gaussian
noise. The nonzero edge weights of the linear SEMs are sampled from Unif(±[0.5,1.5]), and the
noise terms are zero-mean Gaussian variables with variances σ2
isampled from Unif([0 .5,1.5]). For
a soft intervention on node i, the edge weight vector of node iis reduced by a factor of 1/2, and for a
hard intervention, the edge weights are set to zero. The variance of the noise term is reduced to σ2
i/4
in both intervention types. We consider target dimensions d∈ {10,50}, generate 100 latent graphs
for each (n, d)pair, and generate ns= 105samples of Zfrom each environment. Transformation
4The codebase for the experiments can be found at https://github.com/acarturk-e/umni-crl .
9Table 2: UMNI-CRL for a linear causal model with UMN interventions (mean ±standard error)
UMN Soft UMN Hard
n d SHD(Gtc,ˆG) MCC ℓsoft SHD(G,ˆG) MCC ℓhard
4 10 0.91±0.12 0 .95±0.01 0 .08±0.01 0.75±0.11 0 .98±0.02 0 .13±0.02
5 10 1.67±0.20 0 .93±0.01 0 .09±0.01 1.65±0.11 0 .97±0.02 0 .13±0.02
6 10 3.19±0.26 0 .92±0.01 0 .12±0.01 3.12±0.25 0 .96±0.02 0 .12±0.02
7 10 5.44±0.34 0 .90±0.01 0 .15±0.01 5.36±0.35 0 .93±0.03 0 .15±0.03
8 10 7.63±0.41 0 .89±0.01 0 .16±0.01 9.70±0.52 0 .87±0.03 0 .20±0.03
4 50 0.77±0.12 0 .96±0.01 0 .06±0.01 0.66±0.10 0 .98±0.01 0 .13±0.02
5 50 1.93±0.20 0 .93±0.01 0 .10±0.01 1.80±0.19 0 .98±0.02 0 .13±0.01
6 50 3.39±0.27 0 .92±0.01 0 .13±0.01 3.05±0.25 0 .95±0.03 0 .13±0.01
7 50 4.62±0.30 0 .91±0.01 0 .13±0.01 6.12±0.34 0 .91±0.02 0 .16±0.01
8 50 8.26±0.49 0 .90±0.01 0 .14±0.01 9.01±0.53 0 .88±0.03 0 .28±0.02
G∈Rd×nis randomly sampled under full-rank constraint, and observed variables are generated as
X=G·Z. Finally, for each graph realization, intervention matrix Dis chosen randomly among
column permutations of full-rank {0,1}n×nmatrices, which satisfies Assumption 1.
Score functions. The algorithm uses score differences between environment pairs. Since we use
a linear Gaussian model, Xis also multivariate Gaussian, and its score function can be estimated
bysX(x) =−ˆΘ·xin which ˆΘis the sample estimate of the precision matrix of X. We note that
the design of UMNI-CRL is agnostic to the choice of the estimator and can adopt any reliable score
estimator for nonparametric distributions [29, 30].
Graph recovery. To assess the graph recovery, we report the structural Hamming distance (SHD)
between the true and estimated DAGs. Recall that UMN hard and soft interventions ensure different
levels of identifiability guarantees. Hence, we report the SHD between (i) transitive closure Gtcand
ˆGfor soft interventions, and (ii) true DAG GandˆGfor hard interventions. Table 2 shows that latent
graph recovery performance remains consistent for both soft and hard interventions when observed
variables dimension dincreases from 10to50, which conforms to our expectations due to theoretical
results. Note that the expected number of edges is n(n−1)/4since we set the density of random
graphs to 0.5. Hence, the increasing SHD is also unsurprising when the latent dimension nincreases
from 4to8, and the performance remains reasonable at n= 8. Finally, we note that n= 8is the
largest latent graph size considered among the closely related SN intervention studies [4, 8, 7, 9].
Latent variable recovery. The estimates are given by ˆZ(X;H∗) = (H∗·G)·Z. Hence, we
scrutinize the effective mixing matrix (H∗·G)and report the ratio of its incorrect mixing entries to
the number of zeros in constant matrices CsandCanaccording to Definition 1, denoted by
ℓhard≜P
j̸=i1([H∗·G]i,j̸= 0)
n2−n, and ℓsoft≜P
j /∈an(i)1([H∗·G]i,j̸= 0)
n2−P
i|an(i)|.(19)
We also report the mean correlation coefficient (MCC) [ 31], which measures linear correlations
between the estimated and ground truth latent variables and is commonly used in related work.
Table 2 shows that the UMNI-CRL algorithm achieves strong MCC performance (over 0.90) in all
cases. Furthermore, the ratio of incorrect mixing entries remains less than 0.20for both soft and hard
interventions This demonstrates a strong performance of the UMNI-CRL algorithm at recovering
latent variables for as many as n= 8latent variables even for observed variables dimension d= 50 .
6 Discussion
In this paper, we established novel identifiability results using unknown multi-node (UMN) in-
terventions for CRL under linear transformations. Specifically, we designed the provably correct
UMNI-CRL algorithm, leveraging the structural properties of score functions across different en-
vironments. To facilitate identifiability, we introduced a sufficient condition for the set of UMN
interventions, abstracted as having nsufficiently diverse interventional environments. Investigating
the necessary conditions for UMN interventions to enable identifiability remains an open problem.
The main limitation is the assumption of linear transformations. Given existing results for general
transformations using two SN interventions per node [ 6,9], a promising direction for future work is
extending our results to general transformations using UMN interventions with multiple interventional
mechanisms per node.
10Acknowledgements and disclosure of funding
This work was supported by IBM through the IBM-Rensselaer Future of Computing Research
Collaboration.
References
[1]Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the
IEEE , 109(5):612–634, May 2021.
[2]Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In Proc. International Conference on Machine Learning , Long
Beach, CA, June 2019.
[3]Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal repre-
sentation learning. In Proc. International Conference on Machine Learning , Honolulu, Hawaii,
July 2023.
[4]Chandler Squires, Anna Seigal, Salil S. Bhate, and Caroline Uhler. Linear causal disentangle-
ment via interventions. In Proc. International Conference on Machine Learning , Honolulu,
Hawaii, July 2023.
[5]Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score-
based causal representation learning with interventions. arXiv:2301.08230 , 2023.
[6]Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, and Ali Tajer. General identifiability
and achievability for causal representation learning. In Proc. International Conference on
Artificial Intelligence and Statistics , Valencia, Spain, May 2024.
[7]Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score-
based causal representation learning: Linear and general transformations. arXiv:2402.00849 ,
2024.
[8]Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Schölkopf,
and Pradeep Ravikumar. Learning linear causal representations from interventions under general
nonlinear mixing. In Proc. Advances in Neural Information Processing Systems , New Orleans,
LA, December 2023.
[9]Julius von Kügelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Keki ´c, Elias
Bareinboim, David M Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal rep-
resentations from unknown interventions. In Proc. Advances in Neural Information Processing
Systems , New Orleans, LA, December 2023.
[10] Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shan-
mugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft
interventions. In Proc. Advances in Neural Information Processing Systems , New Orleans, LA,
December 2023.
[11] Chandler Squires, Yuhao Wang, and Caroline Uhler. Permutation-based causal structure learning
with unknown intervention targets. In Proc. Conference in Uncertainty in Artificial Intelligence ,
August 2020.
[12] Alejandro Tejada-Lapuerta, Paul Bertin, Stefan Bauer, Hananeh Aliee, Yoshua Bengio, and
Fabian J Theis. Causal machine learning for single-cell genomics. arXiv:2310.14935 , 2023.
[13] Tabitha Edith Lee, Shivam Vats, Siddharth Girdhar, and Oliver Kroemer. Scale: Causal learning
and discovery of robot manipulation skills using simulation. In CoRL 2023 Workshop on
Learning Effective Abstractions for Planning , Atlanta, GA, November 2023.
[14] Jikai Jin and Vasilis Syrgkanis. Learning causal representations from general environments:
Identifiability and intrinsic ambiguity. arXiv:2311.12267 , 2023.
11[15] Yibo Jiang and Bryon Aragam. Learning nonparametric latent causal graphs with unknown
interventions. In Proc. Advances in Neural Information Processing Systems , New Orleans, LA,
December 2023.
[16] Simon Bing, Urmi Ninad, Jonas Wahl, and Jakob Runge. Identifying linearly-mixed causal
representations from multi-node interventions. In Proc. Causal Learning and Reasoning , Los
Angeles, CA, April 2024.
[17] Kartik Ahuja, Amin Mansouri, and Yixin Wang. Multi-domain causal representation learning
via weak distributional invariances. In Proc. International Conference on Artificial Intelligence
and Statistics , Valencia, Spain, May 2024.
[18] Julius von Kügelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Schölkopf,
Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations
provably isolates content from style. In Proc. Advances in Neural Information Processing
Systems , virtual, December 2021.
[19] Johann Brehmer, Pim De Haan, Phillip Lippe, and Taco S Cohen. Weakly supervised causal
representation learning. In Proc. Advances in Neural Information Processing Systems , New
Orleans, LA, December 2022.
[20] Dingling Yao, Danru Xu, Sebastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg
Martius, Julius von Kügelgen, and Francesco Locatello. Multi-view causal representation learn-
ing with partial observability. In Proc. International Conference on Learning Representations ,
Vienna, Austria, May 2024.
[21] Nils Sturma, Chandler Squires, Mathias Drton, and Caroline Uhler. Unpaired multi-domain
causal representation learning. In Proc. Advances in Neural Information Processing Systems ,
New Orleans, LA, December 2023.
[22] Phillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M Asano, Taco Cohen, and Efstratios Gavves.
Causal representation learning for instantaneous and temporal effects in interactive systems. In
Proc. International Conference on Learning Representations , Kigali, Rwanda, May 2023.
[23] Phillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M Asano, Taco Cohen, and Efstratios Gavves.
BISCUIT: Causal representation learning from binary interactions. In Proc. Uncertainty in
Artificial Intelligence , Pittsburgh, PA, August 2023.
[24] Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel,
Kun Zhang, and Javen Qinfeng Shi. Identifiable latent polynomial causal models through the
lens of change. In Proc. International Conference on Learning Representations , Vienna, Austria,
May 2024.
[25] Sébastien Lachapelle, Pau Rodríguez López, Yash Sharma, Katie Everett, Rémi Le Priol, Alexan-
dre Lacoste, and Simon Lacoste-Julien. Nonparametric partial disentanglement via mechanism
sparsity: Sparse actions, interventions and sparse temporal dependencies. arXiv:2401.04890 ,
2024.
[26] Kun Zhang, Shaoan Xie, Ignavier Ng, and Yujia Zheng. Causal representation learning from
multiple distributions: A general setting. In Proc. International Conference on Machine
Learning , Vienna, Austria, July 2024.
[27] Hiroshi Morioka and Aapo Hyvärinen. Causal representation learning made identifiable by
grouping of observational variables. In Proc. International Conference on Machine Learning ,
Vienna, Austria, July 2024.
[28] Aneesh Komanduri, Xintao Wu, Yongkai Wu, and Feng Chen. From identifiable causal repre-
sentations to controllable counterfactual generation: A survey on causal generative modeling.
Transactions on Machine Learning Research , May 2024.
[29] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable
approach to density and score estimation. In Proc. Uncertainty in Artificial Intelligence , virtual,
August 2020.
12[30] Yuhao Zhou, Jiaxin Shi, and Jun Zhu. Nonparametric score estimators. In Proc. International
Conference on Machine Learning , virtual, July 2020.
[31] Ilyes Khemakhem, Ricardo Monti, Diederik Kingma, and Aapo Hyvärinen. Ice-beem: Iden-
tifiable conditional energy-based deep models based on nonlinear ICA. In Proc. Advances in
Neural Information Processing Systems , virtual, December 2020.
[32] Joel Brenner and Larry Cummings. The hadamard maximum determinant problem. The
American Mathematical Monthly , 79(6):626–630, 1972.
[33] Igor Araujo, József Balogh, and Yuzhou Wang. Maximum determinant and permanent of sparse
0-1 matrices. Linear Algebra and its Applications , 645:194–228, 2022.
13Linear Causal Representation Learning from Unknown
Multi-node Interventions
Appendices
Table of Contents
A Proofs 14
A.1 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.2 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.4 Proofs of Theorem 1 and Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . 22
A.5 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.6 Insufficiency of strongly separating sets . . . . . . . . . . . . . . . . . . . . . . 23
A.7 Analysis of interventional regularity . . . . . . . . . . . . . . . . . . . . . . . . 25
A.8 Computational complexity of UMNI-CRL algorithm . . . . . . . . . . . . . . . 26
B Additional simulations 27
B.1 Simulations with nonlinear latent causal models and sensitivity to noisy scores . 27
A Proofs
We start this section by introducing the additional notations used in the subsequent proofs.
Additional notations. We use {ei:i∈[n]}to denote n-dimensional standard unit basis vectors.
For a set S ∈[n], we use ASto denote the submatrix of Aconstructed by the rows {Ai:i∈ S} .
For a valid encoder H∈ H , we denote the score functions associated with the pdfs of ˆZ(X;H)
under environments E0andEmbysˆZ(·;H)andsm
ˆZ(·;H)for all m∈[M]. In addition to the graph
notations introduced in Section 2.1, we define
pa(i)≜pa(i)∪ {i},ch(i)≜ch(i)∪ {i},and an(i)≜an(i)∪ {i}. (20)
Next, we provide an auxiliary result that will be used throughout the proofs of the main results.
Lemma 3. For every pair (i, j)where i∈[n]andj∈pa(i), the following ratio function cannot be
a constant in z

Λ(z)]j,j
Λ(z)
i,i=∂
∂zjlogpj(zj|zpa(j))
qj(zj|zpa(j))
·∂
∂zilogpi(zi|zpa(i))
qi(zi|zpa(i))−1
. (21)
Furthermore, the columns of Λare linearly independent vector-valued functions.
Proof: See Appendix A.5. □
We also restate the interventional regularity in terms of the Λfunction defined in (14), which will
help clarity in the subsequent proofs.
14Definition 2 (Intervention regularity) We say that an intervention on node iisregular if for every
possible triplet (i, j, c)where i∈[n], j∈pa(i)andc∈Q, the following ratio cannot be a constant
function in z
∂
∂zj
logpi(zi|zpa(i))
qi(zi|zpa(i))+c·logpj(zj|zpa(j))
qj(zj|zpa(j))∂
∂zilogpi(zi|zpa(i))
qi(zi|zpa(i))−1
. (22)
By definition of Λ, this means that for (i, j, c)where i∈[n], j∈pa(i)andc∈Q, the following
ratio cannot be a constant function in z

Λ(z)
j,i+c·
Λ(z)
j,j
Λ(z)
i,i. (23)
A.1 Proof of Lemma 2
First, we show that the search space in Stage 1 of Algorithm 1, i.e., the set W, has a certain property
that will be needed for the rest of the proof. Recall the definition in (18)
W≜{−κ, . . . , +κ}n, (24)
in which κdenotes the maximum possible determinant of a matrix in {0,1}(n−1)×(n−1). We will
show that for all i∈[n], there exists w∈ W such that 1([D·w]̸= 0) = ei. Let adj(D)be the
cofactor matrix of D, which is the matrix formed by the entries
[adj(D)]i,j≜det([D]−i,−j), (25)
where [D]−i,−jis the first minor of Dwithi-th row and j-th columns are removed. Then, the inverse
ofDis given by
D−1=1
det(D)·[adj(D)]⊤. (26)
By multiplying both sides from left by Dand rearranging we obtain
D·[adj(D)]⊤= det( D)·In×n, (27)
where all matrices are integer-valued and In×ndenotes the n-dimensional identity matrix. The
identity in (27) implies that
D·w∗= det( D)·ei,where w∗= [adj( D)]⊤
:,i. (28)
Since entries of adj(D)are upper bounded by κ, we have w∗∈ W and we conclude that
∀i∃w∈ W such that 1 
D·w̸= 0
=ei. (29)
Next, we prove the lemma statements by induction as follows.
Base case. At the base case t= 1, we will show that 1([D·W]:,1̸= 0) = eiandH∗
1∈span([ G†]i)
for some root node i. Consider a vector w∈ W and denote c=D·w. Then, using (17),
∆SX·w= [G†]⊤·Λ·D·w= [G†]⊤·Λ·c. (30)
In the base case, we investigate the dimension of im(∆SX·w). Then, using (30), we have
dim 
im(∆SX·w)
= dim 
im([G†]⊤·Λ·c)
= dim 
im(Λ·c)
. (31)
(29) implies that there exists w∗∈ W that makes 1(c̸= 0) = ei. Subsequently, if iis a root node,
only nonzero entry of the column Λ:,iisΛi,iand we have
dim 
im(∆SX·w)
= dim 
im([G†]⊤·Λ:,i)
= dim 
im([G†]i·Λi,i)
= 1. (32)
Hence, by searching wwithin W, Algorithm 1 is guaranteed to find a vector wsuch that dim(im( Λ·
c)) = 1 . Next, we show that if dim(im( Λ·c)) = 1 , then 1(c̸= 0) = eifor a root node i. We prove
this as follows. Consider the set A={i:ci̸= 0}and let ibe the youngest node in A, i.e., ihas no
15descendants within A. Using the fact that Λℓ,k= 0for all ℓ /∈pa(k)andck= 0for all k∈de(i),
we have
Λℓ·c
Λi·c=P
k∈ch(ℓ)ck·Λℓ,kP
k∈ch(i)ck·Λi,k=P
k∈ch(ℓ)ck·Λℓ,k
ci·Λi,i,∀ℓ∈[n]. (33)
Ifchas multiple nonzero entries, let cℓdenote the second youngest node in A. Ifj /∈pa(i), then for
ℓ=j, (33) reduces to
Λj·c
Λi·c=cj·Λj,j
ci·Λi,i, (34)
which is not constant due to Lemma 3. If j∈pa(i), then (33) reduces to
Λj·c
Λi·c=cj·Λj,j+ci·Λj,i
ci·Λi,i, (35)
which is not constant due to interventional regularity. Hence, in either case, there exist multiple
vectors in im(Λ·c)with distinct directions. Therefore, dim(im( Λ·c)) = 1 implies that chas only
one nonzero entry ci̸= 0. Finally, if iis not a root node, for j∈pa(i), (33) reduces to
Λj·c
Λi·c=Λj,i
Λi,i, (36)
which is not constant due to interventional regularity. Therefore, dim(im( Λ·c)) = 1 implies that
1(c̸= 0) = 1([D·W]:,1̸= 0) = eifor a root node i. Next, using (30), we have
∆SX·w=ci·Λi,i·[G†]i. (37)
Hence, denoting the root node ibyπ1, setting H∗
1=vfor any v∈im(∆SX·w)ensures that
[D·W]:,1=eπ1andH∗
1∈span([ G†]π1)which completes the base step of the induction.
Induction hypothesis. For the induction step, assume that for all t∈[k], we have linearly
independent vectors H∗
t∈span({[G†]πj:j∈pa(t)})and
[D·W]πj,t=0, j > t
̸= 0, j=t,∀t∈[k],∀j≥t , (38)
where {π1, . . . , π k}constitutes the first knodes of a causal order π. We will show that if
dim(V) = 1 where V= projnull({H∗
i:i∈[k]})im(∆SX·w), (39)
thenw∈ W satisfies
[D·w]πj=0, j > k + 1
̸= 0, j=k+ 1, (40)
for the causal order π. First, define Mk+1≜[n]\ {π1, . . . , π k}. Note that, by the induction premise,
null({H∗
i:i∈[k]}) = null({[G†]πi:i∈[k]}). (41)
Then, for any w∈ W andc=D·w, we have
V= projnull({[G†]πi:i∈[k]})([G†]⊤·im(Λ·c)). (42)
Subsequently, we have
dim(V) = dim
im 
[G†]⊤
Mk+1·ΛMk+1·c
= dim 
im(ΛMk+1·c)
. (43)
Note that, if node i∈ M k+1has no ancestors in Mk+1, using the vector w∈ W which makes
1(c̸= 0) = eiensures that dim(im( ΛMk+1·)c) = dim(im( Λi,i·ei)) = 1 . Hence, by searching w
within W, Algorithm 1 is guaranteed to find a vector wsuch that dim(im( ΛMk+1·c)) = 1 . Next,
consider the set A={i∈ M k+1:ci̸= 0}. Since π1, . . . , π kare first k-nodes of a causal order π,
we have
Λi,πj= 0,∀i∈ M k+1,∀j∈[k]. (44)
16Then, if dim(V) = 1 ,Ais not empty since otherwise (ΛMk+1·c)is equal to zero vector and
dim(V) = 0 . Letibe youngest node in A, i.e,ihas no descendants within A. Similarly to (33), we
have
Λℓ·c
Λi·c=P
k∈ch(ℓ)ck·Λℓ,k
ci·Λi,i,∀ℓ∈ M k+1. (45)
IfAhas multiple elements, let jbe the second youngest node in A. Ifj /∈pa(i), then for ℓ=j,(45)
reduces toΛj·c
Λi·c=cj·Λj,j
ci·Λi,i, (46)
which is not constant due to Lemma 3. If j∈pa(i), then for ℓ=j, (45) reduces to
Λj·c
Λi·c=cj·Λj,j+ci·Λj,i
ci·Λi,i, (47)
which is not constant due to interventional regularity. Hence, in either case, there exist vectors with
different directions in im(ΛMk+1·c). Therefore, dim(im( ΛMk+1·c)) = 1 implies that Ahas
only one element, i.e., there is only one node i∈[n]\ {π1, . . . , π k}such that ci̸= 0, and also
pa(i)⊆ {π1, . . . , π k}. Hence, denoting πk+1=i,[D·W]πjis nonzero for j=k+ 1and zero for
allj > k + 1. Subsequently, by setting H∗
k+1=vfor any v∈im(∆SX·w)\span({H∗
i:i∈[k]})
we have
H∗
k+1∈span({[G†]πi:i∈[k+ 1]}). (48)
Finally, note that the contribution of [G†]iinH∗
k+1is nonzero since ci̸= 0. Hence, H∗
k+1is linearly
independent of {H∗
1, . . . ,H∗
k}, which completes the proof of the induction hypothesis. Therefore,
(38) holds for all t∈[n], which implies that Pπ·D·Wis upper triangular and has nonzero diagonal
entries, and concludes the proof of the lemma.
A.2 Proof of Theorem 3
We will prove the theorem by proving the following equivalent statements: The output Wof Stage 3
of Algorithm 1 satisfies
[D·W]πt,j=0, π t/∈an(πj)
1, π t=πj, (49)
and
t∈ˆ pa(j)⇐⇒ πt∈an(πj). (50)
for all t, j∈[n]. The second statement of the theorem, graph recovery up to a transitive closure, is
equivalent to (50) by definition. Next, we show that (49) implies the first statement of the theorem,
that is identifiability of latent variables up to mixing with ancestors. For this purpose, using (17), for
anyw∈Rn, we have
∆SX·w= [G†]⊤·Λ·D·w. (51)
Then, for w= [W]:,j, using the fact that Λπt,πk= 0for all πt/∈pa(πk), the coefficient of [G†]πt
on the right-hand side becomesX
πk∈[n]Λπt,πk·[D·W]πk,j=X
πk∈ch(πt)Λπt,πk·[D·W]πk,j. (52)
Ifπt/∈an(πj), then πk∈ch(πt)implies that πk/∈an(πj). Then, using (49), the sum in (52), i.e.,
the coefficient of [G†]πtin function (∆SX·w), becomes zero. Furthermore, since [D·W]πt,t̸= 0,
the coefficient of [G†]πtin(∆SX·w)is nonzero. Therefore, (49) ensures, by choosing H∗
t∈
im(∆SX·[W]:,t)for all t∈[n], we have
H∗
t∈span 
{[G†]πj:j∈an(t)}
(53)
and{H∗
1, . . . ,H∗
n}are linearly independent. This implies that we have
H∗=Pπ·Can·G†, (54)
where Pπis the row permutation matrix of π, andCan∈Rn×nis a constant matrix with nonzero
diagonal entries that satisfies [Can]i,j= 0for all j /∈an(i). Subsequently,
ˆZ(X;H∗) =H∗·X=Pπ·Can·G†·G·Z=Pπ·Can·Z , (55)
which is the definition of identifiability up to ancestors for latent variables, specified in Definition 1.
In the rest of the proof, we will prove (49) by induction.
17Base case. At the base case, we have t=n−1andj=n. We will show that, at the end of step
(t, j) = (n−1, n),[D·W]πn−1,n̸= 0implies that πn−1∈pa(πn). By Lemma 2, we know that
[D·W]πn−1,n−1̸= 0,[D·W]πn,n̸= 0,and [D·W]πn,n−1= 0. (56)
Consider some integers α∈ {− nκ, . . . , nκ }andβ∈ {1, . . . , nκ }. Letw∗=α[W]:,n−1+β[W]:,n
andc∗=D·w∗. Since β̸= 0, (56) implies that c∗
n̸= 0. Using Lemma 2, we have
null({H∗
i:i∈[n−2]}) = null({[G†]πi:i∈[n−2]}). (57)
Then, we have
V= projnull({H∗
i:i∈[n−2]})im(∆SX·w∗) (58)
= projnull({[G†]πi:i∈[n−2]})([G†]⊤·im(Λ·c∗)). (59)
Subsequently,
dim(V) = dim 
[G†]πn−1
[G†]πn⊤
·im
Λπn−1
Λπn
·c∗!
(60)
= dim
im
Λπn−1
Λπn
·c∗
(61)
= dim
imc∗
πn−1·Λπn−1,πn−1+c∗
πn·Λπn−1,πn
c∗
πn·Λπn,πn
. (62)
Note that in the last step, we have used the fact that πis a causal order and j /∈pa(i)implies that
Λi,j= 0. Then, if πn−1∈pa(πn), interventional regularity ensures that the ratio of the two entries
in(62) is not a constant function which implies that dim(V) = 2 . Furthermore, if πn−1/∈pa(πn)
butc∗
πn−1̸= 0, then (62) reduces to
dim(V) = dim
imc∗
πn−1·Λπn−1,πn−1
c∗
πn·Λπn,πn
= 2, (63)
due to Lemma 3. Therefore, if w∗satisfies
dim(projnull({H∗
i:i∈[n−2]})im(∆SX·w∗)) = 1 , (64)
by setting [W]:,n=w∗, we guarantee that [D·W]πn−1,n= 0ifπn−1/∈pa(πn). Note that, for this
case, setting either (α, β) = (−[D·W]n−1,n,[D·W]n−1,n−1)or(α, β) = ([D·W]n−1,n,−[D·
W]n−1,n−1)achieves
c∗
πn−1= [D·w∗]πn−1=α·[D·W]πn−1,n−1+β·[D·W]πn−1,n= 0. (65)
Note that the entries of [D·W]are bounded as
[D·W]i,j=X
k∈[n]Di,k·Wk,j≤X
k∈[n]|Wk,j|=∥W:,j∥1,∀i, j∈[n]. (66)
Hence, Algorithm 1 is guaranteed to find c∗
πn−1= 0by searching over
(α, β)∈ {−∥ W:,n−1∥1, . . . ,∥W:,n−1∥1} × { 1, . . . ,∥W:,n∥1}. (67)
Therefore, if dim(V)is never found to be 1for any (α, β), it means that πn−1∈pa(πn), and the
algorithm adds the edge (n−1)→n, which concludes the proof of the base case.
Induction hypothesis. Next, assume that for all t∈ {k+ 1, . . . , n }andj∈ {t+ 1, . . . , n },
[D·W]πt,j=0πt/∈an(πj)
1πt=πjand πt∈an(πj)⇐⇒ t∈ˆ pa(j). (68)
We will prove that (68) holds for t=kandj∈ {t+ 1, . . . , n }. We prove this by induction as well.
18Base case for the inner induction. At the base case, we have t=kandj=k+ 1. Since πis a
causal order, we have
πk∈an(πk+1)⇐⇒ πk∈pa(πk+1). (69)
Also, ˆch(k)is empty at this iteration of the algorithm. Then, Mk,k+1= [k−1]. Consider
some integers α∈ {− nκ, . . . , nκ }andβ∈ {1, . . . , nκ }. Letw∗=α[W]:,k+β[W]:,k+1and
c∗=D·w∗. Using Lemma 2, we have
null 
{H∗
i:i∈[k−1]}
=null 
{[G†]πi:i∈[k−1]}
. (70)
Then, we have
V= projnull({H∗
i:i∈[k−1]})im(∆SX·w∗) (71)
= projnull({[G†]πi:i∈[k−1]}) 
[G†]⊤·im(Λ·c∗)
. (72)
Using (68), c∗
πi= 0fori > k + 1. Subsequently,
dim(V) = dim 
im 
[G†]πk
[G†]πk+1⊤
·
Λπk
Λπk+1
·c∗!!
= dim
im
Λπk
Λπk+1
·c∗
.(73)
Also, recall that Λπk,πj= 0for all j < k . Then,
dim(V) = dim
im
Λπk
Λπk+1
·c∗
= dim
imc∗
πk·Λπk,πk+c∗
πk+1·Λπk,πk+1
c∗
πk+1·Λπk+1,πk+1
.
(74)
Using (68) again, we have c∗
πk+1̸= 0. Then, if πk∈pa(πk+1), interventional regularity ensures
that the ratio of the two entries in (74) is not a constant function which implies that dim(V) = 2 .
Furthermore, if πk/∈pa(πk+1)butc∗
πk̸= 0, based on Lemma 3, (74) reduces to
dim(V) = dim
imc∗
πk·Λπk,πk
c∗
πk+1·Λπk+1,πk+1
= 2. (75)
Therefore, if we have
dim(projnull({H∗
i:i∈[k−1]})im(∆SX·w∗)) = 1 , (76)
by setting [W]:,k+1=w∗, we guarantee that [D·W]πk,k+1= 0 ifπk/∈pa(πk+1). Note that,
similarly to the base case of outer induction, Algorithm 1 is guaranteed to find c∗
πn−1= 0by searching
over(α, β)∈ {−∥ W:,k∥1, . . . ,∥W:,k∥1} × { 1, . . . ,∥W:,k+1∥1}. Therefore, if dim(V)is never
found to be 1for any (α, β), it means πk∈pa(πk+1), and the algorithm adds the edge t→j, which
concludes the proof of the base case for the inner induction.
Induction hypothesis for the inner induction. Next, assume that for all j∈ {k+ 1, . . . , u },
[D·W]πk,j=0πk/∈an(πj)
1πk=πjand πk∈an(πj)⇐⇒ k∈ˆ pa(j). (77)
We will prove that (77) holds for j=u+1as well. We work with Mk,u+1= [u−1]\{ˆch(k)∪{k}}.
Consider some w∗=α[W]:,k+β[W]:,u+1in which β̸= 0 and let c∗=D·w∗. Due to
the assumption in (77), ancestors of any node in Mk,u+1are contained in Mk,u+1. Then, using
Lemma 2 again, we have
null({H∗
i:i∈ M k,u+1}) = null({[G†]πi:i∈ M k,u+1}). (78)
Then, we have
V= projnull({H∗
i:i∈Mk,u+1})im(∆SX·w∗) (79)
= projnull({[G†]πi:i∈Mk,u+1})([G†]⊤·im(Λ·c∗)), (80)
and
dim(V) = dim 
im([Λ][n]\Mk,u+1·c∗)
≥dim
im
Λπk
Λπu+1
·c∗
. (81)
19We will investigate dim(V)through the ratio
Λπk·c∗
Λπu+1·c∗. (82)
Note that, using (77) and the fact that πis a causal order, we know that
cπi= 0,∀i∈ {u+ 2, . . . , n } ∪ {{ k+ 1, . . . , u } \ˆ an(u+ 1)}, (83)
and Λπj,πi= 0,∀πj/∈pa(πi). (84)
Then, using (83) and (84), we have
Λπu+1·c∗=X
πi∈ch(πu+1)c∗
πi·Λπu+1,πi=c∗
πu+1·Λπu+1,πu+1, (85)
Λπk·c∗=c∗
πk·Λπk,πk+c∗
πu+1·Λπk,πu+1+X
i:πi∈ch(πk)andi∈ˆ an(u+1)c∗
πi·Λπk,πi.(86)
First, note that if there exists ℓsuch that πk∈an(πℓ)andπℓ∈an(πu+1), then by the assumptions in
(68) and(77), we already have that k∈ˆ pa(ℓ)andℓ∈ˆ pa(u+ 1) which implies that k∈ˆ pa(u+ 1) .
Then, we only need to consider the case where there does not exist such ℓ. In this case, the summation
in (86) is zero and we have
Λπk·c∗
Λπu+1·c∗=c∗
πk·Λπk,πk+c∗
πu+1·Λπk,πu+1
c∗πu+1·Λπu+1,πu+1. (87)
Next, if πk∈pa(πu+1), interventional regularity ensures that this ratio is not a constant function
which implies that dim(V)≥2. Furthermore, if πk/∈an(πu+1)butc∗
πk̸= 0, then (87) reduces to
Λπk·c∗
Λπu+1·c∗=c∗
πk·Λπk,πk
c∗πu+1·Λπu+1,πu+1, (88)
which is not constant due to Lemma 3 and subsequently dim(V)≥2. Therefore, dim(V) = 1
implies that πk/∈an(πu+1)andc∗
πk= [D·W]πk,u+1= 0. Finally, similarly to the previous
cases, Algorithm 1 is guaranteed to find such (α, β)that makes c∗
πk= 0by searching over (α, β)∈
{−∥W:,k∥1, . . . ,∥W:,k∥1} × { 1, . . . ,∥W:,u+1∥1}, e.g., (α, β) = (−[D·W]k,u+1,[D·W]k,k).
Then, the proof of the inner induction step, and subsequently, the outer induction step is concluded,
and (49) holds true. Consequently, the proof of the theorem is completed.
A.3 Proof of Theorem 4
We start with a short synopsis of the proof. Stage 4 of Algorithm 1 consists of two steps. The first
step resolves the mixing with ancestors in recovered latent variables and the second step refines the
estimated graph to the edges from non-parent ancestors to children nodes. The proof of the first step
uses similar ideas to that of [ 7, Lemma 10]. Specifically, we use zero-covariance as a surrogate for
independence and search for unmixing vectors to eliminate the mixing with ancestors. Since we do
not have SN interventions unlike the setting in [ 7], we use additional proof techniques to identify
an environment in which a certain node is intervened. In the graph recovery stage, we leverage the
knowledge of ancestral relationships and use a small number of conditional independence tests to
remove the edges from non-parent ancestors to the children nodes.
A.3.1 Recovery of the latent variables
First, by Theorem 3, for the output H∗of Stage 3 of Algorithm 1 we have
[H∗·G] =Pπ·L, (89)
for some lower triangular matrix L∈Rn×nsuch that Lhas non-zero diagonal entries and Li,j= 0
for all j /∈an(i), and πis a causal order. We will show that the output of Stage 4 satisfies that
1 
[H∗·G]t
=e⊤
πt,∀t∈[n], (90)
which will imply
ˆZ(X;H∗) =H∗·X=H∗·G·Z=Pπ·Cs·Z , (91)
20where Pπis the row permutation matrix of πandCsis a constant diagonal matrix with nonzero
diagonal entries. Note that this is the definition of perfect identifiability for latent variables, specified
in Definition 1. We prove that Stage 4 output H∗satisfies (90) as follows.
We start by noting that since π1is a root node in Gas shown in Lemma 2, (89) implies
[H∗·G]1=Lπ1,π1·e⊤
π1and 1 
[H∗·G]1
=e⊤
π1, (92)
and (90) is satisfied for t= 1. Next, assume that (90) is satisfied for all t∈ {1, . . . , k −1}, i.e.,
1 
[H∗·G]t
=e⊤
πt,∀t∈[k−1]. (93)
Consider k-th iteration of the algorithm in which we update H∗
kwhere {H∗
i:i∈[k−1]}already
satisfy (90). We will prove that (90) will be satisfied in four steps. Consider any environment Emand
use shorthand ˆZmforˆZm(X;H∗).
Step 1: Cov(u·ˆZm
ˆ an(k)+ˆZm
k,ˆZm
ˆ an(k)) =0has a unique solution for u.For any i∈[k−1],(93)
implies
ˆZm
i= [H∗·G]·Zm=ci·Zm
πi, (94)
for some nonzero constants {c1, . . . ,ck−1}. Specifically, by defining
ˆ an(k) ={γ1, . . . , γ r} (95)
in which the nodes are topologically ordered, then
ˆZm
ˆ an(k)=ˆZm
γ1, . . . , ˆZm
γr
= [cγ1·Zm
πγ1, . . . ,cγr·Zm
πγr]. (96)
Note that Cov( ˆZm
ˆ an(k))is invertible since the causal relationships among the entries in ˆZm
ˆ an(k)are not
deterministic. Then, we have
Cov(u·ˆZm
ˆ an(k)+ˆZm
k,ˆZm
ˆ an(k)) = 0 ⇐⇒ u=−Cov( ˆZm
k,ˆZm
ˆ an(k))·[Cov( ˆZm
ˆ an(k))]−1.(97)
For the next step, using the additive noise model specified in (3), under hard interventions we have
Zm
πk=fm
πk(Zpa(πk)) +Nm
πk,where (fm
πk, Nm
πk)≜(fπk, Nπk), π k/∈Im
(0,¯Nπk), π k∈Im, (98)
where ¯Nπkdenotes the exogenous noise term under intervention. Let us use fandNas shorthands
forfm
πkandNm
πk.
Step 2: Cov(u·ˆZm
ˆ an(k)+ˆZm
k,ˆZm
ˆ an(k)) = 0 andu·ˆZm
ˆ an(k)⊥ ⊥ˆZm
ˆ an(k)implies that fm
πkcannot be
nonlinear. Define random variable U≜(u·ˆZm
ˆ an(k)+ˆZm
k). Suppose that Cov(U,ˆZm
ˆ an(k)) = 0 and
U⊥ ⊥ˆZm
ˆ an(k). Recall that using (89),ˆZm
kis a linear combination of the variables {Zm
πi:i∈an(k)}
andZm
πk, i.e.,
ˆZm
k=c′·ˆZm
ˆ an(k)+c′
0·Zm
πk, (99)
where c′∈R|ˆ an(k)|andc′
0is a non-zero scalar. Then, Ucan be restated as
U=u·ˆZm
ˆ an(k)+ˆZm
k= (u+c′)·ˆZm
ˆ an(k)+c′
0·(f(Zm
pa(k)) +N). (100)
Note that U⊥ ⊥ˆZm
ˆ an(k)and(96) together imply that Uis independent of any function of the variables
in{Zm
πi:i∈an(k)}. Hence,
(u+c′)·ˆZm
ˆ an(k)+c′
0·f(Zpa(k))⊥ ⊥(u+c′)·ˆZm
ˆ an(k)+c′
0(f(Zpa(k)) +N), (101)
where the right-hand size is Uand the left-hand side is a function of {Zm
πi:i∈an(k)}. Also, since
Nis the exogenous noise variable associated with Zπk, we have
(u+c′)·ˆZm
ˆ an(k)+c′
0·f(Zpa(k))⊥ ⊥c′
0·N . (102)
(101) and(102) imply that (u+c′)·ˆZm
ˆ an(k)+c′
0·f(Zpa(k))is a constant function of ˆZm
ˆ an(k). Since
(u+c′)·ˆZm
ˆ an(k)is a linear function (or constant zero) and c′
0̸= 0,fcannot be a nonlinear function
which concludes the proof of this step. Next, we consider two possible cases, fis zero, i.e., πk∈Im
case, and fis a linear function.
21Step 3: πk∈Imandf= 0.In this case, we have Zm
πk=N. Note that for u=−c′,(100)
becomes
U=c′
0·N=c′
0·Zm
πk, (103)
which implies
(H∗
k+u·H∗
ˆ an(k))·G·Zm=ˆZm
k+u·ˆZm
ˆ an(k)=U=c′
0·Zm
πk(104)
and 1 
(H∗
k+u·H∗
ˆ an(k))·G
=e⊤
πk. (105)
Also note that, any uvector that satisfies U⊥ ⊥ˆZm
ˆ an(k)also satisfies Cov(U,ˆZm
ˆ an(k)) =0. In Step 1,
we have shown that Cov(U,ˆZm
ˆ an(k)) =0has a unique solution. Therefore, U⊥ ⊥ˆZm
ˆ an(k)also has a
unique solution, which we have found in (97). Hence, if πk∈Im, then Algorithm 1 updates H∗
k
correctly.
Step 4: πk/∈Imandfis linear. Only remaining case to check is πk/∈Imandfis a linear
function. Let
f(Zpa(πk))≜c′′·ˆZm
ˆ an(k), (106)
in which c′′∈R|ˆ an(k)|has nonzero entries only at the coordinates corresponding to pa(πk). Then,
for(u+c′)·ˆZm
ˆ an(k)+c′
0·f(Zpa(k))to be a constant function, we need to have (u+c′+c′
0·c′′) =0.
Note that, for c′′̸=0case, i.e., f̸= 0andπk/∈Im, we have already found uobs=−c′−c′
0·c′′
using the observational environment. Since we are searching for u=−c′, which is required to
achieve scaling consistency, we compare the solution umat environment Emtouobsand if they are
distinct, we update H∗
k.
To sum up, Stage 4 updates H∗
kcorrectly by identifying an environment Emin which Zπkis intervened
and eliminating the effect of H∗
ˆ an(k). Finally, we note that such an environment is guaranteed to exist
among {Em:Wm,k̸= 0}. To see this, recall that [D·W]πk,k̸= 0due to (49), proven in Theorem 3.
This implies that there exists msuch that Dπk,m= 1andWm,k̸= 0, andDπk,m= 1implies that
πk∈Im. This completes the proof of (90), and subsequently the proof of the perfect latent recovery.
A.3.2 Recovery of the latent graph
After the first step of Stage 4, we have
ˆZt=ct·Zπt,∀t∈[n], (107)
where {ct:t∈[n]}are nonzero constants. Then,
ˆZt⊥ ⊥ˆZj| {ˆZi:i∈ S} ⇐⇒ Zπt⊥ ⊥Zπj| {Zπi:i∈ S} . (108)
Consider node t∈[n]and node j∈ˆch(t). Ifπt∈pa(πj), given the adjacency-faithfulness assump-
tion, (108) implies that ˆZtandˆZjcannot be made conditionally independent for any conditioning set.
On the other hand, note that for any set Sthat contains all the nodes in pa(πj)and does not contain a
node in de(πj)satisfies
Zπt⊥ ⊥Zπj| {Zπi:i∈ S} , (109)
and subsequently,
ˆZt⊥ ⊥ˆZj| {ˆZi:i∈ S} . (110)
Finally, if πt/∈pa(πj), then ˆ pa(j)\{t}contains all the nodes in pa(πj)and does not contain a node
inde(πj). Hence, the second stage of Step 4 of Algorithm 1 successfully eliminates all spurious
edges between tandj∈ˆch(t).
A.4 Proofs of Theorem 1 and Theorem 2
Under Assumption 1 and interventional regularity, Lemma 2 and Theorem 3 show that using UMN
soft interventions, outputs of Algorithm 1 satisfy identifiability up to ancestors. Hence, identifiability
up to ancestors is possible using UMN soft interventions.
Furthermore, note that Lemma 2 and Theorem 3 are valid for both soft and hard interventions.
Then, Theorem 4 shows that using UMN hard interventions, Algorithm 1 outputs satisfy perfect
identifiability. Hence, perfect identifiability is possible using UMN hard interventions.
22A.5 Proof of Lemma 3
We start by showing that [Λ(z)]i,icannot be a constant function in z. This is shown in the proof of [ 5,
Lemma 7]. For our paper to be self-contained, we repeat the proof steps in [ 5] as follows. For i∈[n]
define
h(zi, zpa(i))≜pi(zi|zpa(i))
qi(zi|zpa(i)). (111)
We prove by contradiction that h(zi, zpa(i))varies with zi. Assume the contrary, i.e., let
h(zi, zpa(i)) =h(zpa(i)). By rearranging (111) we have
pi(zi|zpa(i)) =h(zpa(i))·qi(zi|zpa(i)). (112)
Fix a realization of zpa(i)=z∗
pa(i), and integrate both sides of (112) with respect to zi. Since both pi
andqiare pdfs, we have
1 =Z
Rpi(zi|z∗
pa(i))dzi=Z
Rh(z∗
pa(i))·qi(zi|z∗
pa(i))dzi (113)
=h(z∗
pa(i))Z
Rqi(zi|z∗
pa(i))dzi (114)
=h(z∗
pa(i)). (115)
This identity implies that pi(zi|z∗
pa(i)) =qi(zi|z∗
pa(i))for any arbitrary realization z∗
pa(i), which
contradicts with the premise that observational and interventional causal mechanisms are distinct.
Consequently,
[Λ(z)]i,i=∂
∂zilogpi(zi|zpa(i))
qi(zi|zpa(i))(116)
is not a constant in z. Note that for a fixed realization of zj=z∗
j,zpa(i)=z∗
pa(i), andzpa(j)=z∗
pa(j),
[Λ(z)]j,jbecomes constant whereas [Λ(z)]i,ivaries with zi. Hence, their ratio is not a constant in z.
Finally, note that Λ(z)is an upper-triangular matrix since (1, . . . , n )is a valid causal order and for
alli∈[n],
∇logpi(zi|zpa(i))
qi(zi|zpa(i))(117)
is a function of only {zk:k∈pa(i)∪ {i}}. Together with the fact that diagonal entries of Λ(z)are
not constantly zero, the columns (and rows) of Λare linearly independent vector-valued functions.
A.6 Insufficiency of strongly separating sets
Background. Recently, [ 16] has shown the identifiability of latent representations under a linear
transformation using dointerventions. Specifically, they have shown that a strongly separating set of
multi-node interventions are sufficient for identifiability. It is well-known that strongly separating sets
can be constructed using 2⌈log2n⌉elements. Hence, identifiability can be achieved using 2⌈logn⌉
dointerventions. In this section, we show that a similar result is impossible when using stochastic
hard interventions.
Lemma 4 (Impossibility) .A strongly separated set of 2⌈log2n⌉stochastic hard interventions are
not guaranteed to be sufficient for perfect identifiability. In fact, they are not even sufficient for
identifiability up to ancestors.
Proof: We prove the claim for n= 2nodes. The smallest strongly separating set for two nodes is
{{1},{2}}. We will consider two distinct models of latent variables and latent graphs that are not
distinguishable using interventional data of I1={1}andI2={2}(without observational data).
The key idea is that after the linear transformation in the first model is fixed, we can design the linear
transformation in the second model such that observed variables in both models will have the same
distributions. We construct a pair of indistinguishable models as follows.
23First model. LetGconsists of the edge 1→2. Consider a linear Gaussian latent model with the
edge weight of Z1onZ2is set to 1, and consider identity mapping, i.e.,
G: 1→2,G=
1 0
0 1
, Z1=
N∗
1
N∗
1+N2
, Z2=
N1
N∗
2
, (118)
N1∼ N(0, V1), N∗
1∼ N(0, V∗
1), N 2∼ N(0, V2), N∗
2∼ N(0, V∗
2). (119)
where V1, V∗
1, V2, V∗
2are nonzero variances of the exogenous noise terms such that ¯V1̸=¯V∗
1to
ensure that interventional and observational mechanisms of node 1are distinct. Then, the observed
variables Xin two environments are given by
X1=G·Z1∼ N
0,
V∗
1 V∗
1
V∗
1V∗
1+V2
, (120)
and X2=G·Z2∼ N
0,
V10
0V∗
2
. (121)
Second model. Let¯Gbe the empty graph. Consider a linear Gaussian latent model under a
non-identity mapping parameterized by
¯G:empty , ¯G=
a b
c d
,¯Z1=¯N∗
1¯N2
, Z2=¯N1¯N∗
2
, (122)
¯N1∼ N(0,¯V1),¯N∗
1∼ N(0,¯V∗
1),¯N2∼ N(0,¯V2),¯N∗
2∼ N(0,¯V∗
2). (123)
where ¯V1,¯V∗
1,¯V2,¯V∗
2are nonzero variances of the exogenous noise terms such that ¯V1̸=¯V∗
1and
¯V2̸=¯V∗
2to ensure that interventional mechanisms are distinct from the observational ones. Then,
the observed variables ¯Xin two environments are given by
¯X1=¯G·¯Z1∼ N
0,
a2¯V∗
1+b2¯V2ac¯V∗
1+bd¯V2
ac¯V∗
1+bd¯V2c2¯V∗
1+d2¯V2
, (124)
and ¯X2=¯G·¯Z2∼ N
0,
a2¯V1+b2¯V∗
2ac¯V1+bd¯V∗
2
ac¯V1+bd¯V∗
2c2¯V1+d2¯V∗
2
. (125)
Non-identifiability. We will show a nontrivial construction that ensures X1=¯X1andX2=¯X2,
which implies the non-identifiability from intervention set {{1},{2}}. First, using (120) ,(121) ,
(124), and (125), we write all requirements for X1=¯X1andX2=¯X2to hold:
V∗
1=a2¯V∗
1+b2¯V2, (126)
V∗
1=ac¯V∗
1+bd¯V2, (127)
V2= (c2−ac)¯V∗
1+ (d2−bd)¯V2, (128)
V1=a2¯V1+b2¯V∗
2, (129)
V∗
2=c2¯V1+d2¯V∗
2, (130)
0 =ac¯V1+bd¯V∗
2. (131)
Now, let ¯V1,¯V∗
1,¯V2,¯V∗
2take any values such that ¯V1¯V2̸=¯V∗
1¯V∗
2. We want to show that there exist
{a, b, c, d }and{V1, V∗
1, V2, V∗
2}values that satisfy all the six equations. Note that, the values of
V1, V2, and V∗
2are not constrained by multiple equations or additional conditions. Hence, for any
given{a, b, c, d }and{¯V1,¯V∗
1,¯V2,¯V∗
2}, we can readily set V1, V2, andV∗
2to satisfy (129) ,(128) , and
(130) , respectively. Therefore, we only need to ensure that we can choose {a, b, c, d }such that the
identities in (126) ,(127) , and (131) are satisfied. Next, after substituting d= 1, and rearranging, we
only need to choose {a, b, c}that satisfy
ac¯V1+b¯V∗
2= 0, (132)
(a2−ac)¯V∗
1+ (b2−b)¯V2= 0. (133)
Substituting ac=−b¯V∗
2
¯V1into (133), we require
b2·¯V2−b(¯V2−¯V∗
2¯V∗
1
¯V1) +a2¯V∗
1= 0. (134)
24(134) has a real solution for bif and only if
a2≤¯V2
4¯V∗
1
1−¯V∗
1¯V∗
2
¯V1¯V22
. (135)
This implies that, if ¯V1¯V2̸=¯V∗
1¯V∗
2, we can choose aandbthat satisfies (134) , and cis determined
by(132) . This means that, for any given {¯V1,¯V∗
1,¯V2,¯V∗
2}such that ¯V1¯V2̸=¯V∗
1¯V∗
2, there exists
{a, b, c, d }and{V1, V∗
1, V2, V∗
2}that ensures X1=¯X1andX2=¯X2. Hence, interventions on the
strongly separating set {{1},{2}}are not sufficient to distinguish the first and second models, which
completes the proof of non-identifiability. □
A.7 Analysis of interventional regularity
Lemma 5. Consider additive noise models under hard interventions. Interventional regularity
is satisfied if the post-intervention score function of Ni, denoted by ¯r, is analytic and one of the
following is true.
1.∂fi(zpa(i))
∂zjis not constant and there do not exist constants α1̸= 1,α2, α3∈Rsuch that
¯r(y) =α1·¯r(y+α2) +α3,∀y∈R. (136)
2.∂fi(zpa(i))
∂zjis constant and noise term Niremains unaltered after the intervention.
Proof: The additive noise model for nodes iandjare given by
Zi=fi(Zpa(i)) +Ni,and Zj=fj(Zpa(j)) +Nj. (137)
When nodes iandjare intervened, respectively, ZiandZjare generated according to
Zi=¯Niand Zj=¯Nj, (138)
in which ¯Niand¯Njcorrespond to exogenous noise terms for nodes iandjunder intervention. Then,
denoting the pdfs of Ni,¯Ni, Nj,¯Njbyhi,¯hi, hj,¯hj, respectively, we have
pi(zi|zpa(i)) =hi(zi−fi(zpa(i))), (139)
qi(zi) =¯hi(zi), (140)
pj(zj|zpa(j)) =hj(zj−fj(zpa(j))), (141)
qj(zj) =¯hj(zj). (142)
Then, by denoting the score functions associated with hi,¯hi, hj,¯hjbyri,¯ri, rj,¯rj, respectively, we
have
∂
∂zilogpi(zi|zpa(i))
qi(zi)=ri(ni)−¯ri(ni+fi(zpa(i))), (143)
∂
∂zjlogpi(zi|zpa(i))
qi(zi)=−ri(ni)·∂fi(zpa(i))
∂zj, (144)
∂
∂zjlogpj(zj|zpa(j))
qj(zj)=rj(nj)−¯rj(nj+fj(zpa(j))). (145)
Next, assume the contrary and let the ratio in (12) be a constant α∈R. Then, substituting (143) ,
(144), and (145) into (12) and rearranging the terms, we have

α+∂fi(zpa(i))
∂zj
·ri(ni)−α·¯ri(ni+fi(zpa(i))) =c· 
rj(nj)−¯rj(nj+fj(zpa(j)))
.(146)
25Case 1:∂fi(zpa(i))
∂zjis not constant. Consider two distinct realizations of Zpa(i)∪pa(j)such that
∂
∂zjfi(zpa(i))takes values of β1andβ2where β1̸=β2. Then, (146) implies
(α+β1)·ri(ni)−α·¯ri(ni+γ1) =c·u1, (147)
(α+β2)·ri(ni)−α·¯ri(ni+γ2) =c·u2, (148)
for some constants γ1, γ2, u1, u2for all ni∈R, and β1̸=β2. By rearranging the terms, we get rid
ofri(ni)terms and obtain
α·((α+β2)·¯ri(ni+γ1)−(α+β1)·¯ri(ni+γ2)) =c·((α+β1)·u2−(α+β2)·u1).
(149)
Since β1̸=β2, (149) implies that
¯ri(y) =α1·¯ri(y+α2) +α3, (150)
where α1, α2, α3are constants and α1̸= 1. Therefore, if there does not exist such constants, then the
ratio in (12) cannot be a constant.
Case 2:∂fi(zpa(i))
∂zj=βfor some nonzero constant β.In this case, (146) becomes
(α+β)·ri(ni)−α·¯ri(ni+fi(zpa(i))) =c· 
rj(nj)−¯rj(nj+fj(zpa(j)))
. (151)
Note that the right-hand side of (151) does not contain ni. Also note that since fi(zpa(i))is continuous,
there exists an open interval Θ⊆Rsuch that fi(zpa(i))can take all values θ∈Θ. Then, by taking
the derivative of both sides with respect to niand varying fi(zpa(i))inΘ, we find that ¯r′
i(ni+θ)is
constant for all θ∈Θ. Since ¯riis analytic, this means that ¯ri(y) =α1·y+α2for some constants
α1, α2for all y∈R. Then, since the noise term is invariant under intervention, we have ri= ¯ri.
Substituting this into (151), we obtain
(α+β)·(αni+α2)−αα1·(ni+fi(zpa(i))) =c· 
rj(nj)−¯rj(nj+fj(zpa(j)))
.(152)
Since β̸= 0, the left-hand side is a function of niwhereas the right-hand side is not, which is invalid.
Hence, the ratio in (12) cannot be constant in this case. □
A.8 Computational complexity of UMNI-CRL algorithm
UMNI-CRL (Algorithm 1) consists of four stages that we elaborate on as follows.
Stage 1: The score difference estimation is only performed once before the subsequent main algo-
rithm steps. Since our results and the algorithm do not rely on a specific score difference
estimation technique, studying the computational complexity of this step is out of scope.
Stage 2: In this step, we check the dimension of V, a projection of a subspace (see line 6 of
Algorithm 1) at most n×(2κ+1)ntimes. Here, κdenotes the maximum possible determinant
of a matrix in {0,1}(n−1)×(n−1). In the proof of Lemma 2 in Appendix A.1, we discuss
why this choice facilitates the identifiability guarantees.
Stage 3: In this step, we check the dimension of V(see line 23 of Algorithm 1) at most ∥W:,j∥1×
∥W:,t∥1times for every (t, j)in Stage 3. Note that Wis updated within the steps of Stage 3.
Therefore, the exact computational complexity would be a function of the graph structure
and the outputs of Stage 2.
Stage 4: Unmixing procedure for hard interventions essentially operates as a post-processing step
that does not pose additional computational challenges. For instance, the total number of
conditional independence tests in Stage 4 is O(n2).
Bounds on κ.In Section 4, we defined κas the maximum determinant of a binary matrix
{0,1}(n−1)×(n−1)and noted that the computational complexity of UMNI-CRL algorithm depends
onnandκas seen in Stage 2 above. In general, using the well-known bound for the determinant of
{0,1}matrices [32], we have
κ≤ ⌊2(n/4)n⌋.
adj(D)
i,j≤nn/2
2n−1= 2n
4n
. (153)
2630 35 40 45 50 55 60
Signal to noise ratio (dB)1234SHD
n=5, d=20
soft (w.r.t. ancestors)
hard (wrt true DAG)(a) Graph recovery
30 35 40 45 50 55 60
Signal to noise ratio (dB)0.000.050.100.150.200.250.30Incorrect mixing ratio
n=5, d=20
soft (w.r.t ancestors)
hard (w.r.t true DAG) (b) Latent variable recovery
Figure 1: Sensitivity analysis of UMNI-CRL algorithm for quadratic latent causal models. The results
are for n= 5latent nodes and d= 20 observed variables, 104samples, and for average of 100 runs.
(a):SHD(Gtc,ˆG)versus SNR (soft) and SHD(G,ˆG)versus SNR (hard). (b):Incorrect mixing ratio
ℓsoftversus SNR (soft) and ℓhard versus SNR (hard).
However, for specific cases, we have much smaller upper bounds for κ. For instance, for the
choices of nbeing 2,3,4,5,6,7,κis known to be upper bounded by 1,1,2,3,5,9, respectively [ 32].
Furthermore, [ 33] shows that the determinant of a matrix in {0,1}n×nwithn+knonzero entries is
bounded by 2k/3. Hence, if Dhasn+knonzero entries, then
κ≤ ⌊2k/3⌋. (154)
This implies that κcan be quite small for sparse UMN interventions. For instance, if we take the
exhaustive set of SN interventions and only add two additional intervened nodes in total, then we
haveκ= 1.
Empirical tricks. Finally, we note that various empirical tricks can be used to reduce the algorithm’s
computational complexity. For instance, after every update to W, we can divide the columns of W
by the greatest common divisor of its entries. Furthermore, even though κcan grow quickly as n
becomes larger, in our experiments with up to n= 8nodes, we observe that setting κ= 2usually
suffices for a good performance. Hence, we set κ= 2in the simulations in Section 5.
B Additional simulations
Details of evaluation metrics. For the graph recovery via hard interventions, we use conditional
independence tests in Stage 4 of Algorithm 1. Since we adopt a linear Gaussian SEM latent model,
we use a partial correlation test and set the significance level to α= 0.05. For the latent variable
recovery error metrics ℓhardandℓsoftdefined in (19), we first pass the entries of H∗·Gthrough a
threshold of 0.1, then compute the incorrect mixing metrics.
B.1 Simulations with nonlinear latent causal models and sensitivity to noisy scores
In Section 5, we have adopted linear Gaussian SEMs as latent causal models for which the score
estimation can be done via estimating the precision matrices of the observed variables. In this section,
we perform additional simulations to study nonlinear latent causal models to investigate the relation
between the algorithm’s performance and the accuracy of the score function estimates. To this end, we
adopt a quadratic latent causal model under varying amounts of noise in score functions. Specifically,
we follow the experimental setup in [ 7] and adopt a quadratic latent causal model with additive noise
as follows
Zi=q
Z⊤
pa(i)·Ai·Zpa(i)+Ni, (155)
where {Ai:i∈[n]}are positive-definite matrices, and the noise terms are zero-mean Gaussian
variables with variances σ2
isampled randomly from Unif([0 .5,1.5]). For an intervention on node i,
Ziis set to Ni/2. This causal model admits a closed-form score function (see [ 7, Appendix E.2] for
27details), which enables us to obtain a score oracle. In our MN intervention setup, we use this score
oracle and introduce varying levels of artificial noise according to
ˆsX(x;σ2) =sX(x)· 
1 + Ξ
,where Ξ∼ N(0, σ2·Id×d) (156)
to test the behavior of our algorithm under different noise regimes σ∈[10−3,10−1.5]. Figure 1
demonstrates the results for the latent graph recovery and latent variable recovery.
28NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: Sections 3 and 4 establish the results claimed in the abstract and introduction.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The limitations of the work, e.g., linear transformations, are clarified through-
out the paper and discussed in Section 6.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
29Justification: The assumptions for the theoretical results are clearly stated in the theorem
statements, and the proofs of the results are provided in Appendix A.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The algorithm details are summarized in Algorithm 1, and described in
Section 4. The experimental procedure and details of parameterization are described in
Section 5 and Appendix B. The codebase for the experiments can be found at https:
//github.com/acarturk-e/umni-crl .
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
305.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The codebase for the experiments can be found at https://github.com/
acarturk-e/umni-crl .
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The experimental procedure is described in Section 5, and detailed parameteri-
zation is provided in Appendix B.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The experiments in Table 2 are reported for an average of 100 runs.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
31•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Experiments are run on a single commercial CPU.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have reviewed the NeurIPS Code of Ethics and confirm that the
research in this paper conforms with the code of ethics.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper is mostly theoretical and does not pose potential negative societal
impacts.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
32•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper is mostly theoretical and does not pose such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Portions of the publicly available code of [ 7], available under Apache 2.0
license, are adopted in the code of our experiments.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
33•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The codebase for the experiments can be found at https://github.com/
acarturk-e/umni-crl , and is released under Apache 2.0 license.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
34•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
35