The Last Iterate Advantage:
Empirical Auditing and Principled
Heuristic Analysis of Differentially Private SGD
Anonymous Author(s)
AfÔ¨Åliation
Address
email
Abstract
We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient 1
descent (DP-SGD) in the setting where only the last iterate is released and the in- 2
termediate iterates remain hidden. Namely, our heuristic assumes a linear structure 3
for the model. 4
We show experimentally that our heuristic is predictive of the outcome of privacy 5
auditing applied to various training procedures. Thus it can be used prior to training 6
as a rough estimate of the Ô¨Ånal privacy leakage. We also probe the limitations of 7
our heuristic by providing some artiÔ¨Åcial counterexamples where it underestimates 8
the privacy leakage. 9
The standard composition-based privacy analysis of DP-SGD effectively assumes 10
that the adversary has access to all intermediate iterates, which is often unrealistic. 11
However, this analysis remains the state of the art in practice. While our heuristic 12
does not replace a rigorous privacy analysis, it illustrates the large gap between 13
the best theoretical upper bounds and the privacy auditing lower bounds and sets a 14
target for further work to improve the theoretical privacy analyses. 15
1 Introduction 16
Differential privacy (DP) [DMNS06] deÔ¨Ånes a measure of how much private information from the 17
training data leaks through the output of an algorithm. The standard differentially private algorithm 18
for deep learning is DP-SGD [BST14; ACGMMTZ16], which differs from ordinary stochastic 19
gradient descent in two ways: the gradient of each example is clipped to bound its norm and then 20
Gaussian noise is added at each iteration. 21
The standard privacy analysis of DP-SGD is based on composition [BST14; ACGMMTZ16; Mir17; 22
Ste22; KJH20]. In particular, it applies to the setting where the privacy adversary has access to 23
all intermediate iterates of the training procedure. In this setting, the analysis is known to be tight 24
[NSTPC21; NHSBTJCT23]. However, in practice, potential adversaries rarely have access to the 25
intermediate iterates of the training procedure, rather they only have access to the Ô¨Ånal model. Access 26
to the Ô¨Ånal model can either be through queries to an API or via the raw model weights. The key 27
question motivating our work is the following. 28
Is it possible to obtain sharper privacy guarantees for DP-SGD when the adversary 29
only has access to the Ô¨Ånal model, rather than all intermediate iterates? 30
1.1 Background & Related Work 31
The question above has been studied from two angles: Theoretical upper bounds, and privacy auditing 32
lower bounds. Our goal is to shed light on this question from a third angle via principled heuristics. 33
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.A handful of theoretical analyses [FMTT18; CYS21; YS22; AT22; BSA24] have shown that asymp- 34
totically the privacy guarantee of the last iterate of DP-SGD can be far better than the standard 35
composition-based analysis that applies to releasing all iterates. In particular, as the number of 36
iterations increases, these analyses give a privacy guarantee that converges to a constant (depending 37
on the loss function and the scale of the noise), whereas the standard composition-based analysis 38
would give a privacy guarantee that increases forever. Unfortunately, these theoretical analyses 39
are only applicable under strong assumptions on the loss function, such as (strong) convexity and 40
smoothness. We lack an understanding of how well they reÔ¨Çect the ‚Äúreal‚Äù privacy leakage. 41
Privacy auditing [JUO20; DWWZK18; BGDCTV18; SNJ23; TTSSJC22; ZBWTSRPNK22] com- 42
plements theoretical analysis by giving empirical lower bounds on the privacy leakage. Privacy 43
auditing works by performing a membership inference attack [SSSS17; HSRDTMPSNC08; SOJH09; 44
DSSUV15]. That is, it constructs neighbouring inputs and demonstrates that the corresponding 45
output distributions can be distinguished well enough to imply a lower bound on the differential 46
privacy parameters. In practice, the theoretical privacy analysis may give uncomfortably large values 47
for the privacy leakage (e.g., ">10); in this case, privacy auditing may be used as evidence that 48
the ‚Äúreal‚Äù privacy leakage is lower. There are settings where the theoretical analysis is matched by 49
auditing, such as when all intermediate results are released [NSTPC21; NHSBTJCT23]. However, 50
despite signiÔ¨Åcant work on privacy auditing and membership inference [CCNSTT22; BTRKMW24; 51
WBKBGGG23; LF20; SDSOJ19; ZLS23], a large gap remains between the theoretical upper bounds 52
and the auditing lower bounds [AKOOMS23; NHSBTJCT23] when only the Ô¨Ånal parameters are 53
released. This observed gap is the starting point for our work. 54
1.2 Our Contributions 55
We propose a heuristic privacy analysis of DP-SGD in the setting where only the Ô¨Ånal iterate is 56
released. Our experiments demonstrate that this heuristic analysis consistently provides an upper 57
bound on the privacy leakage measured by privacy auditing tools in realistic deep learning settings. 58
Our heuristic analysis corresponds to a worst-case theoretical analysis under the assumption that the 59
loss functions are linear. This case is simple enough to allow for an exact privacy analysis whose 60
parameters are can be computed numerically (Theorem 1). Our consideration of linear losses is built 61
on the observation that current auditing techniques achieve the highest "values when the gradients 62
of the canaries ‚Äì that is, the examples that are included or excluded to test the privacy leakage ‚Äì are 63
Ô¨Åxed and independent from the gradients of the other examples. This is deÔ¨Ånitely the case for linear 64
losses; the linear assumption thus allows us to capture the setting where current attacks are most 65
effective. Linear loss functions are also known to be the worst case for the non-subsampled (i.e., full 66
batch) case; see Appendix B. Assuming linearity is unnatural from an optimization perspective, as 67
there is no minimizer. But, from a privacy perspective, we show that it captures the state of the art. 68
We also probe the limitations of our heuristic and give some artiÔ¨Åcial counterexamples where it 69
underestimates empirical privacy leakage. One class of counterexamples exploits the presence of a 70
regularizer. Roughly, the regularizer partially zeros out the noise that is added for privacy. However, 71
the regularizer also partially zeros out the signal of the canary gradient. These two effects are almost 72
balanced, which makes the counterexample very delicate. In a second class of counterexamples, the 73
data is carefully engineered so that the Ô¨Ånal iterate effectively encodes the entire trajectory, in which 74
case there is no difference between releasing the last iterate and all iterates. 75
Implications: Heuristics cannot replace rigorous theoretical analyses. However, our heuristic can 76
serve as a target for future improvements to both privacy auditing as well as theoretical analysis. For 77
privacy auditing, matching or exceeding our heuristic is a more reachable goal than matching the 78
theoretical upper bounds, although our experimental results show that even this would require new 79
attacks. When theoretical analyses fail to match our heuristic, we should identify why there is a gap, 80
which builds intuition and could point towards further improvements. 81
Given that privacy auditing is computationally intensive and difÔ¨Åcult to perform correctly [AZT24], 82
we believe that our heuristic can also be valuable in practice. In particular, our heuristic can be used 83
prior to training (e.g., during hyperparameter selection) to predict the outcome of privacy auditing 84
when applied to the Ô¨Ånal model. (This is a similar use case to scaling laws.) 85
22 Linearized Heuristic Privacy Analysis 86
Algorithm 1 Noisy Clipped Stochastic Gradient De-
scent (DP-SGD) [BST14; ACGMMTZ16]
function DP-SGD (x2Xn,T2N,q2[0;1],2
(0;1),2(0;1),`:RdX! R,r:Rd!R)
Initialize model m02Rd.
fort= 1Tdo
Sample minibatch Bt[n]including each ele-
ment independently with probability q.
Compute gradients of the loss rmt 1`(mt 1;xi)
for alli2Btand of the regularizer
rmt 1r(mt 1).
Clip loss gradients: clip 
rmt 1`(mt 1;xi)
:=
rmt 1`(mt 1;xi)
maxf1;krmt 1`(mt 1;xi)k2g.
Sample noise t N (0;2Id).
Update
mt=mt 1 P
i2Btclip 
rmt 1`(mt 1;xi)
+rmt 1r(mt 1)+t
:
end for
iflast_iterate_only then
return mT
else if intermediate_iterates then
return m0;m1;;mT 1;mT
end if
end functionTheorem 1 presents our heuristic differen- 87
tial privacy analysis of DP-SGD (which we 88
present in Algorithm 1 for completeness; 89
note that we include a regularizer rwhose 90
gradient is notclipped, because it does not 91
depend on the private data x). We con- 92
sider Poisson subsampled minibatches and 93
add/remove neighbours, as is standard in 94
the differential privacy literature. 95
Our analysis takes the form of a conditional 96
privacy guarantee. Namely, under the as- 97
sumption that the loss and regularizer are 98
linear, we obtain a fully rigorous differen- 99
tial privacy guarantee. The heuristic is to 100
apply this guarantee to loss functions that 101
are not linear (such as those that arise in 102
deep learning applications). Our thesis is 103
that, in most cases, the conclusion of the 104
theorem is still a good approximation, even 105
when the assumption does not hold. 106
Recall that a function `:Rd!Ris linear 107
if there exist 2Rdand2Rsuch that 108
`(m) =h;mi+for all m. 109
Theorem 1 (Privacy of DP-SGD for linear 110
losses) .Letx;T;q;;;`;r be as in Algo- 111
rithm 1. Assume rand`(;x), for every 112
x2X, are linear. 113
Letting 114
P:=Binomial (T;q) +N(0;2T);Q:=N(0;2T); (1)
DP-SGD with last_iterate_only satisÔ¨Åes (";)-differential privacy with "0arbitrary and 115
=T;q;(") := maxfHe"(P;Q);He"(Q;P)g: (2)
Here,He"denotes thee"-hockey-stick-divergence He"(P;Q) := supSP(S) e"Q(S). 116
Equation 1 gives us a value of the privacy failure probability parameter . But it is more natural to 117
work with the privacy loss bound parameter ", which can be computed by inverting the formula: 118
"T;q;() := minf"0 :T;q;(")g: (3)
BothT;q;(")and"T;q;()can be computed using existing open-source DP accounting libraries 119
[Goo20]. We also provide a self-contained & efÔ¨Åcient method for computing them in Appendix A. 120
The proof of Theorem 1 is deferred to Appendix A, but we sketch the main ideas: Under the linearity 121
assumption, the output of DP-SGD is just a sum of the gradients and noises. We can reduce to 122
dimensiond= 1, since the only relevant direction is that of the gradient of the canary1(which is 123
constant). We can also ignore the gradients of the other examples. Thus, by rescaling, the worst case 124
pair of output distributions can be represented as in Equation 1. Namely, Q=PT
t=1tis simply the 125
noiset N (0;2)summed over Titerations; this corresponds to the case where the canary is 126
excluded. When the canary is included, it is sampled with probability qin each iteration and thus 127
the total number of times it is sampled over Titerations is Binomial (T;q). ThusPis the sum of the 128
contributions of the canary and the noise. Finally the deÔ¨Ånition of differential privacy lets us compute 129
"andfrom this pair of distributions. Tightness follows from the fact that there exists a loss function 130
and pair of inputs such that the corresponding outputs of DP-SGD matches the pair PandQ. 131
1The canary refers to the individual datapoint that is added or removed between neighbouring datasets. This
terminology is used in the privacy auditing/attacks literature inspired on the expression ‚Äúcanary in a coalmine.‚Äù
3100101102103
T=1/q=25/2
100101102103 at =1e06
Heuristic
Standard
Full Batch(a) KeepTpandT2constant.
100101102103104
T=1/q0246810 at =1e06
Heuristic
Standard
Full Batch (b) Keep Standard DP constant.
100101102103104
T (with q=0.01 and =0.5)
02468101214 at =1e06
Heuristic
Standard
Full Batch (c) Keeppandconstant.
Figure 1: Comparison of our heuristic to baselines in various parameter regimes. Horizontal axis is
number of iterations Tand vertical axis is "such that we have (";10 6)-DP.
2.1 Baselines 132
In addition to privacy auditing, we compare our heuristic to two different baselines in Figure 1. 133
The Ô¨Årst is the standard, composition-based analysis. We use the open-source library from Google 134
[Goo20], which computes a tight DP guarantee for DP-SGD with intermediate_iterates . Be- 135
cause DP-SGD with intermediate_iterates gives the adversary more information than with 136
last_iterate_only , this will always give at least as large an estimate for "as our heuristic. 137
We also consider approximating DP-SGD by full batch DP-GD. That is, set q= 1and rescale the 138
learning rate and noise multiplier to keep the expected step and privacy noise variance constant: 139
DP-SGD (x;T;q;;;`;r )|{z}
batch sizenq;T iterations,TqepochsDP-SGD (x;T;1;q;=q;`;r )| {z }
batch sizen;T iterations,Tepochs: (4)
The latter algorithm is full batch DP-GD since at each step it includes each data point in the batch 140
with probability 1. Since full batch DP-GD does not rely on privacy ampliÔ¨Åcation by subsampling, 141
it is much easier to analyze its privacy guarantees. Interestingly, there is no difference between 142
full batch DP-GD with last_iterate_only and with intermediate_iterates ; see Appendix B. 143
Full batch DP-GD generally has better privacy guarantees than the corresponding minibatch DP-SGD 144
and so this baseline usually (but not always) gives smaller values for the privacy leakage "than our 145
heuristic. In practice, full batch DP-GD is too computationally expensive to run. But we can use it as 146
an idealized comparison point for the privacy analysis. 147
103104
Number of steps012345678Œµ at Œ¥=1e‚àí05
Heuristic (œÉ=1.1,q=0.01)
Standard (œÉ=1.1,q=0.01) 
Empirical
(a)q= 0:01
103
Number of steps02468Œµ at Œ¥=1e‚àí05
Heuristic (œÉ=2.1,q=0.08)
Standard (œÉ=2.1,q=0.08) 
Empirical (b)q= 0:08
Figure 2: Black-box gradient space attacks fail to achieve tight auditing when other data points are
sampled from the data distribution. Heuristic and standard bounds diverge from empirical results,
indicating the attack‚Äôs ineffectiveness. This contrasts with previous work which tightly auditing with
access to intermediate updates.
4100101102103
Number of steps012345678Œµ at Œ¥=1e‚àí05
Heuristic
Standard
Empirical(a)q= 0:01
100101102103
Number of steps012345678Œµ at Œ¥=1e‚àí05
Heuristic
Standard
Empirical (b)q= 0:1
Figure 3: For gradient space attacks with adversarial datasets, the empirical epsilon ( ") closely tracks
the Ô¨Ånal epsilon except for at small step counts, where distinguishing is more challenging. This is
evident at both subsampling probability values we study ( q= 0:01andq= 0:1).
101102103
Number of steps012345678Œµ at Œ¥=1e‚àí05
Heuristic 
Standard 
Empirical
(a) All zero gradient inputs
101102103
Number of steps012345678Œµ at Œ¥=1e‚àí05
Heuristic 
Standard 
Empirical (b) CIFAR10 Dataset
Figure 4: Input space attacks show promising results with both natural and blank image settings,
although blank images have higher attack success. These input space attacks achieve tighter results
than gradient space attacks in the natural data setting, in contrast to Ô¨Åndings from prior work.
3 Empirical Evaluation via Privacy Auditing 148
Setup: We follow the construction of Nasr, Song, Thakurta, Papernot, and Carlini [NSTPC21] where 149
we have 3 entities, adversarial crafter, model trainer, and distinguisher. In this paper, we assume 150
the distinguisher only has access the Ô¨Ånal iteration of the model parameters. We use the CIFAR10 151
dataset [Ale09] with a WideResNet model [ZK16] unless otherwise speciÔ¨Åed; in particular, we follow 152
the training setup of De, Berrada, Hayes, Smith, and Balle [DBHSB22], where we train and audit 153
a model with 79% test accuracy and, using the standard analysis, ("= 8;= 10 5)-DP. For each 154
experiment we trained 512 CIFAR10 models with and without the canary (1024 total). To compute 155
the empirical lower bounds we use the PLD approach with Clopper-Pearson conÔ¨Ådence intervals 156
used by Nasr, Hayes, Steinke, Balle, Tram√®r, Jagielski, Carlini, and Terzis [NHSBTJCT23]. Here we 157
assume the adversary knows the sampling rate and the number of iterations and is only estimating the 158
noise multiplier used in DP-SGD, from which the reported privacy parameters ( "and) are derived. 159
53.1 Experimental Results 160
We implement state-of-the-art attacks from prior work [NSTPC21; NHSBTJCT23]. These attacks 161
heavily rely on the intermediate steps and, as a result, do not achieve tight results. In the next 162
section, we design speciÔ¨Åc attacks for our heuristic privacy analysis approach to further understand 163
its limitations and potential vulnerabilities. We used Google Cloud A2-megagpu-16g machines with 164
16 Nvidia A100 40GB GPUs. Overall, we use roughly 33,000 GPU hours for our experiments. 165
Gradient Space Attack: The most powerful attacks in prior work are gradient space attacks where 166
the adversary injects a malicious gradient directly into the training process, rather than an example; 167
prior work has shown that this attack can produce tight lower bounds, independent of the dataset 168
and model used for training [NHSBTJCT23]. However, these previous attacks require access to all 169
intermediate training steps to achieve tight results. Here, we use canary gradients in two settings: one 170
where the other data points are non-adversarial and sampled from the real training data, and another 171
where the other data points are designed to have very small gradients ( 0). This last setting was 172
shown by [NSTPC21] to result in tighter auditing. In all attacks, we assume the distinguisher has 173
access to all adversarial gradient vectors. For malicious gradients, we use Dirac gradient canaries, 174
where gradient vectors consist of zeros in all but a single index. In both cases, the distinguishing test 175
measures the dot product of the Ô¨Ånal model checkpoint and the gradient canary. 176
Figure 2 summarizes the results for the non-adversarial data setting, with other examples sampled 177
from the true training data. In this experiment, we Ô¨Åx noise magnitude and subsampling probability, 178
and run for various numbers of training steps. While prior work has shown tight auditing in this 179
setting, we Ô¨Ånd an adversary without access to intermediate updates obtains much weaker attacks. 180
Indeed, auditing with this strong attack results even in much lower values than the heuristic outputs. 181
Our other setting assumes the other data points are maliciously chosen. We construct an adversarial 182
‚Äúdataset‚Äù of m+ 1gradients,mof which are zero, and one gradient is constant (with norm equal to 183
the clipping norm), applying gradients directly rather than using any examples. As this experiment 184
does not require computing gradients, it is very cheap to run more trials, so we run this procedure 185
N= 100;000times with the gradient canary, and Ntimes without it, and compute an empirical 186
estimate for "with these values. We plot the results of this experiment in Figure 3 together with 187
the"output by the theoretical analysis and the heuristic, Ô¨Åxing the subsampling probability and 188
varying the number of update steps. We adjust the noise parameter to ensure the standard theoretical 189
analysis produces a Ô¨Åxed "bound. The empirical measured "is close to the heuristic "except for 190
when training with very small step counts: we expect this looseness to be the result of statistical 191
effects, as lower step counts have higher relative variance at a Ô¨Åxed number of trials. 192
Input Space Attack: In practice, adversaries typically cannot insert malicious gradients freely in 193
training steps. Therefore, we also study cases where the adversary is limited to inserting malicious 194
inputs into the training set. Label Ô¨Çip attacks are one of the most successful approaches used to audit 195
DP machine learning models in prior work [NHSBTJCT23; SNJ23]. For input space attacks, we use 196
the loss of the malicious input as a distinguisher. Similar to our gradient space attacks, we consider 197
two settings for input space attacks: one where other data points are correctly sampled from the 198
dataset, and another where the other data points are blank images. 199
Figure 4 summarizes the results for this setting. Comparing to Figure 2, input space attacks achieve 200
tighter results than gradient space attacks. This Ô¨Ånding is in stark contrast to prior work. The reason 201
is that input space attacks do not rely on intermediate iterates, so they transfer well to our setting. 202
In all the cases discussed so far, the empirical results for both gradient and input attacks fall below 203
the heuristic analysis and do not violate the upper bounds based on the underlying assumptions. This 204
suggests that the heuristic might serve as a good indicator for assessing potential vulnerabilities. 205
However, in the next section, we delve into speciÔ¨Åc attack scenarios that exploit the assumptions used 206
in the heuristic analysis to create edge cases where the heuristic bounds are indeed violated. 207
4 Counterexamples 208
We now test the limits of our heuristic by constructing some artiÔ¨Åcial counterexamples. That is, we 209
construct inputs to DP-SGD with last_iterate_only such that the true privacy loss exceeds the 210
bound given by our heuristic. While we do not expect the contrived structures of these examples to 211
6manifest in realistic learning settings, they highlight the difÔ¨Åculties of formalizing settings where the 212
heuristic gives a provable upper bound on the privacy loss. 213
4.1 Warmup: Zeroing Out The Model Weights 214
We begin by noting the counterintuitive fact that our heuristic "T;q;()isnotalways monotone in 215
the number of steps Twhen the other parameters ;q; are kept constant. This is shown in Figure 1c. 216
More steps means there is both more noise and more signal from the gradients; these effects partially 217
cancel out, but the net effect can be non-monotone. 218
We can use a regularizer r(m) =kmk2
2=2so thatrmr(m) =m. This regularizer zeros out the 219
model from the previous step, i.e., the update of DP-SGD becomes 220
mt=mt 1  X
i2Btclip 
rmt 1`(mt 1;xi)
+rmt 1r(mt 1) +t!
(5)
=X
i2Btclip 
rmt 1`(mt 1;xi)
+t: (6)
This means that the last iterate mTis effectively the result of only a single iteration of DP-SGD. In 221
particular, it will have a privacy guarantee corresponding to one iteration. Combining this regularizer 222
with a linear loss and a setting of the parameters T;q;; such that the privacy loss is non-monotone 223
‚Äì i.e.,"T;q;()<"1;q;()‚Äì yields a counterexample. 224
In light of this counterexample, in the next subsection, we benchmark our counterexample against 225
sweeping over smaller values of T. I.e., we consider maxtT"t;q;()instead of simply "T;q;(). 226
4.2 Linear Loss +Quadratic Regularizer 227
Consider running DP-SGD in one dimension (i.e., d= 1) with a linear loss `(m;x) =mxfor 228
the canary and a quadratic regularizer r(m) =1
2m2, where2[0;1]andx2[ 1;1]and we 229
use learning rate = 1. With sampling probability q, afterTiterations the privacy guarantee 230
is equivalent to distinguishing Q:=N(0;b2)andP:=N(P
i2[T](1 )i 1Bernoulli (q);b2), 231
whereb2:=2P
i2[T](1 )2(i 1). When= 0, this retrieves linear losses. When = 1, this 232
corresponds to distinguishing N(0;b2)andN(Bernoulli (q);b2)or, equivalently, to distinguishing 233
linear losses after T= 1iteration. If we maximize our heuristic over the number of iterations T, 234
then our heuristic is tight for the extremes 2f0;1g. 235
A natural question is whether the worst-case privacy guarantee on this quadratic is always given by 236
2f0;1g. Perhaps surprisingly, the answer is no: we found that for T= 3;q= 0:1;= 1;= 0, 237
DP-SGD is (2:222;10 6)-DP. For= 1 instead DP-SGD is (2:182;10 6)-DP. However, for 238
= 0:5instead the quadratic loss does not satisfy (";10 6)-DP for"<2:274. 239
However, this violation is small, which suggests our heuristic is still a reasonable for this class of 240
examples. To validate this, we consider a set of values for the tuple (T;q; ). For each setting 241
ofT;q; , we compute maxtT"t;q;()at= 10 6. We then compute "for the linear loss 242
with quadratic regularizer example with = 1=2in the same setting. Since the support of the 243
random variableP
i2[T](1 )i 1Bernoulli (q)has size 2Tfor= 1=2, computing exact "for 244
even moderate Tis computationally intensive. Instead, let Xbe the random variable equal to 245P
i2[T](1 )i 1Bernoulli (q), except we round up values in the support which are less than :0005 246
up to:0005 , and then round each value in the support up to the nearest integer power of 1:05. We then 247
compute an exact "for distingushingN(0;b2)vsN(X;b2). By Lemma 4.5 of Choquette-Choo, 248
Ganesh, Steinke, and Thakurta [CCGST24], we know that distinguishing N(0;b2)vs.N(P
i2[T](1  249
)i 1Bernoulli (q);b2)is no harder than distingushing N(0;b2)vsN(X;b2), and since we increase 250
the values in the support by no more than 1.05 multiplicatively, we expect that our rounding does not 251
increase"by more than 1.05 multiplicatively. 252
In Figure 5, we plot the ratio of "at= 10 6for distingushing between N(0;b2)andN(X;b2) 253
divided by the maximum over i2[T]of"at= 10 6for distinguishing between N(0;i2) 254
andN(Binomial (i;q);i2). We sweep over Tandq, and for each qIn Figure 5a (resp. Figure 255
5b) we setsuch that distinguishing N(0;2)fromN(Bernoulli (q);2)satisÔ¨Åes (1;10 6)-DP 256
7(a) One iteration of DP-SGD satisÔ¨Åes (1;10 6)-
DP.
(b) One iteration of DP-SGD satisÔ¨Åes (2;10 6)-
DP.
Figure 5: Ratio of upper bound on "for quadratic loss with = 0:5divided by maximum "ofi
iterations on a linear loss. In Figure 5a (resp. Figure 5b), for each choice of q,is set so 1 iteration
of DP-SGD satisÔ¨Åes (1;10 6)-DP (resp (2;10 6)-DP).
(resp. (2;10 6)-DP). In the majority of settings, the linear loss heuristic provides a larger "than 257
the quadratic with = 1=2, and even when the quadratic provides a larger ", the violation is small 258
(3%). This is evidence that our heuristic is still a good approximation for many convex losses. 259
4.3 Pathological Example 260
If we allow the regularizer rto be arbitrary ‚Äì in particular, not even requiring continuity ‚Äì then the 261
gradient can also be arbitrary. This Ô¨Çexibility allows us to construct a counterexample such that the 262
standard composition-based analysis of DP-SGD with intermediate_iterates is close to tight. 263
SpeciÔ¨Åcally, choose the regularizer so that the update m0=m rmr(m)does the following: 264
m0
1= 0 and, fori2[d 1],m0
i+1=vmi. Herev >1is a large constant. We chose the loss 265
so that, for our canary x1, we haverm`(m;x1) = (1;0;0;;0)and, for all other examples xi 266
(i2f2;3;;ng), we haverm`(m;xi) =0. Then the last iterate is 267
mT= (AT+T;1;vAT 1+vT 1;1+T;2;v2AT 2+v2T 2;1+vT 1;2+T;3;);(7)
whereAt Bernoulli (p)indicates whether or not the canary was sampled in the t-th iteration and 268
t;idenotes thei-th coordinate of the noise tadded in the t-th step. Essentially, the last iterate mT 269
contains the history of all the iterates in its coordinates. Namely, the i-th coordinate of mTgives a 270
scaled noisy approximation to AT i: 271
v1 imT;i=AT i+i 1X
j=0vj+1 iT j;i jN
AT i;21 v 2i
1 v 2
: (8)
Asv!1 , the variance converges to 2. In other words, if vis large, from the Ô¨Ånal iterate, we can 272
obtainN(Ai;2)for alli. This makes the standard composition-based analysis of DP-SGD tight. 273
4.4 Malicious Dataset Attack 274
The examples above rely on the regularizer having large unclipped gradients. We now construct a 275
counterexample without a regularizer, instead using other examples to amplify the canary signal. 276
Our heuristic assumes the adversary does not have access to the intermediate iterations and that the 277
model is linear. However, we can design a nonlinear model and speciÔ¨Åc training data to directly 278
challenge this assumption. The attack strategy is to use the model‚Äôs parameters as a sort of noisy 279
storage, saving all iterations within them. Then with access only to the Ô¨Ånal model, an adversary 280
8Table 1: Previous works showed that large batch sizes achieve high performing models [DBHSB22].
Using our heuristic analysis it is possible to achieve similar performance for smaller batch sizes.
Batch size Heuristic "Standard"Accuracy Empirical "
4096 6.34 8 79.5 % 1.7
512 7.0 12 79.1 % 1.8
256 6.7 14 79.4 % 1.6
can still examine the parameters, extract the intermediate steps, and break the assumption. Our 281
construction introduces a data point that changes its gradient based on the number of past iterations, 282
making it easy to identify if the point was present a given iteration of training. The rest of the 283
data points are maliciously selected to ensure the noise added during training doesn‚Äôt impact the 284
information stored in the model‚Äôs parameters. We defer the full details of the attack to Appendix C. 285
Figure 6 summarizes the results. As illustrated in the Ô¨Ågure, this attack achieves a auditing lower 286
bound matching the standard DP-SGD analysis even in the last_iterate_only setting. As a result, 287
the attack exceeds our heuristic. However, this is a highly artiÔ¨Åcial example and it is unlikely to 288
reÔ¨Çect real-world scenarios. 289
100101102103
Number of steps012345678Œµ at Œ¥=1e‚àí05
Heuristic
Standard
Empirical
(a)q= 0:01
100101102103
Number of steps012345678Œµ at Œ¥=1e‚àí05
Heuristic
Standard
Empirical (b)q= 0:1
Figure 6: In this adversarial example, the attack encodes all training steps within the Ô¨Ånal model
parameters, thereby violating the speciÔ¨Åc assumptions used to justify our heuristic analysis.
5 Discussion & Conclusion 290
Both theoretical analysis and privacy auditing are valuable for understanding privacy leakage in 291
machine learning, but each has limitations. Theoretical analysis is inherently conservative, while 292
auditing procedures evaluate only speciÔ¨Åc attacks, and may thus underrepresent the privacy leakage. 293
Our work introduces a novel heuristic analysis for DP-SGD that focuses on the privacy implications 294
of releasing only the Ô¨Ånal model iterate. This approach is based in the empirical observation that 295
linear loss functions accurately model the effectiveness of state of the art membership inference 296
attacks. Our heuristic offers a practical and computationally efÔ¨Åcient way to estimate privacy leakage 297
to complement privacy auditing and the standard composition-based analysis of DP-SGD. As shown 298
in Table 1, we trained a series of CIFAR10 models with varying batch sizes that all achieved the 299
similar level of heuristic epsilon, albeit with different standard epsilon values. Remarkably, these 300
models exhibited similar performance and similar empirical epsilon values. 301
We also acknowledge the limitations of our heuristic by identifying speciÔ¨Åc counterexamples where 302
the heuristic underestimates the true privacy leakage. 303
9References 304
[ACGMMTZ16] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, 305
and L. Zhang. ‚ÄúDeep learning with differential privacy‚Äù. In: Proceedings of 306
the 2016 ACM SIGSAC conference on computer and communications security . 307
2016, pp. 308‚Äì318. URL:https://arxiv.org/abs/1607.00133 (cit. on 308
pp. 1, 3). 309
[AKOOMS23] G. Andrew, P. Kairouz, S. Oh, A. Oprea, H. B. McMahan, and V . Suriyakumar. 310
‚ÄúOne-shot Empirical Privacy Estimation for Federated Learning‚Äù. In: arXiv 311
preprint arXiv:2302.03098 (2023). URL:https://arxiv.org/abs/2302. 312
03098 (cit. on p. 2). 313
[Ale09] K. Alex. ‚ÄúLearning multiple layers of features from tiny images‚Äù. In: https: 314
// www. cs. toronto. edu/ kriz/ learning-features-2009-TR. pdf 315
(2009) (cit. on pp. 5, 20). 316
[AT22] J. Altschuler and K. Talwar. ‚ÄúPrivacy of noisy stochastic gradient descent: 317
More iterations without more privacy loss‚Äù. In: Advances in Neural Informa- 318
tion Processing Systems 35 (2022), pp. 3788‚Äì3800. URL:https://arxiv. 319
org/abs/2205.13710 (cit. on p. 2). 320
[AZT24] M. Aerni, J. Zhang, and F. Tram√®r. ‚ÄúEvaluations of Machine Learning Privacy 321
Defenses are Misleading‚Äù. In: arXiv preprint arXiv:2404.17399 (2024). URL: 322
https://arxiv.org/abs/2404.17399 (cit. on p. 2). 323
[BGDCTV18] B. Bichsel, T. Gehr, D. Drachsler-Cohen, P. Tsankov, and M. Vechev. ‚ÄúDp- 324
Ô¨Ånder: Finding differential privacy violations by sampling and optimization‚Äù. 325
In:Proceedings of the 2018 ACM SIGSAC Conference on Computer and 326
Communications Security . 2018, pp. 508‚Äì524 (cit. on p. 2). 327
[BSA24] J. Bok, W. Su, and J. M. Altschuler. ‚ÄúShifted Interpolation for Differential 328
Privacy‚Äù. In: arXiv preprint arXiv:2403.00278 (2024). URL:https://arxiv. 329
org/abs/2403.00278 (cit. on p. 2). 330
[BST14] R. Bassily, A. Smith, and A. Thakurta. ‚ÄúPrivate empirical risk minimization: 331
EfÔ¨Åcient algorithms and tight error bounds‚Äù. In: 2014 IEEE 55th annual 332
symposium on foundations of computer science . IEEE. 2014, pp. 464‚Äì473. 333
URL:https://arxiv.org/abs/1405.7085 (cit. on pp. 1, 3). 334
[BTRKMW24] M. Bertran, S. Tang, A. Roth, M. Kearns, J. H. Morgenstern, and S. Z. Wu. 335
‚ÄúScalable membership inference attacks via quantile regression‚Äù. In: Advances 336
in Neural Information Processing Systems 36 (2024). URL:https://arxiv. 337
org/abs/2307.03694 (cit. on p. 2). 338
[CCGST24] C. A. Choquette-Choo, A. Ganesh, T. Steinke, and A. Thakurta. Privacy 339
AmpliÔ¨Åcation for Matrix Mechanisms . 2024. arXiv: 2310.15526 [cs.LG] 340
(cit. on p. 7). 341
[CCNSTT22] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer. ‚ÄúMembership 342
inference attacks from Ô¨Årst principles‚Äù. In: 2022 IEEE Symposium on Security 343
and Privacy (SP) . IEEE. 2022, pp. 1897‚Äì1914. URL:https://arxiv.org/ 344
abs/2112.03570 (cit. on p. 2). 345
[CYS21] R. Chourasia, J. Ye, and R. Shokri. ‚ÄúDifferential privacy dynamics of langevin 346
diffusion and noisy gradient descent‚Äù. In: Advances in Neural Information 347
Processing Systems 34 (2021), pp. 14771‚Äì14781. URL:https://arxiv. 348
org/abs/2102.05855 (cit. on p. 2). 349
[DBHSB22] S. De, L. Berrada, J. Hayes, S. L. Smith, and B. Balle. ‚ÄúUnlocking high- 350
accuracy differentially private image classiÔ¨Åcation through scale‚Äù. In: arXiv 351
preprint arXiv:2204.13650 (2022) (cit. on pp. 5, 9). 352
[DMNS06] C. Dwork, F. McSherry, K. Nissim, and A. Smith. ‚ÄúCalibrating noise to 353
sensitivity in private data analysis‚Äù. In: Theory of Cryptography: Third Theory 354
of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. 355
Proceedings 3 . Springer. 2006, pp. 265‚Äì284. URL:https://www.iacr. 356
org/archive/tcc2006/38760266/38760266.pdf (cit. on p. 1). 357
[DRS19] J. Dong, A. Roth, and W. J. Su. ‚ÄúGaussian differential privacy‚Äù. In: arXiv 358
preprint arXiv:1905.02383 (2019) (cit. on p. 13). 359
10[DSSUV15] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan. ‚ÄúRobust traceability 360
from trace amounts‚Äù. In: 2015 IEEE 56th Annual Symposium on Foundations 361
of Computer Science . IEEE. 2015, pp. 650‚Äì669 (cit. on p. 2). 362
[DWWZK18] Z. Ding, Y . Wang, G. Wang, D. Zhang, and D. Kifer. ‚ÄúDetecting violations of 363
differential privacy‚Äù. In: Proceedings of the 2018 ACM SIGSAC Conference 364
on Computer and Communications Security . 2018, pp. 475‚Äì489 (cit. on p. 2). 365
[FMTT18] V . Feldman, I. Mironov, K. Talwar, and A. Thakurta. ‚ÄúPrivacy ampliÔ¨Åcation 366
by iteration‚Äù. In: 2018 IEEE 59th Annual Symposium on Foundations of 367
Computer Science (FOCS) . IEEE. 2018, pp. 521‚Äì532. URL:https://arxiv. 368
org/abs/1808.06651 (cit. on p. 2). 369
[Goo20] Google. Differential Privacy Accounting .https://github.com/google/ 370
differential- privacy/tree/main/python/dp_accounting . 2020 371
(cit. on pp. 3, 4). 372
[HSRDTMPSNC08] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, 373
J. V . Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. ‚ÄúResolving 374
individuals contributing trace amounts of DNA to highly complex mixtures 375
using high-density SNP genotyping microarrays‚Äù. In: PLoS genetics 4.8 376
(2008), e1000167 (cit. on p. 2). 377
[JUO20] M. Jagielski, J. Ullman, and A. Oprea. ‚ÄúAuditing differentially private ma- 378
chine learning: How private is private sgd?‚Äù In: Advances in Neural Informa- 379
tion Processing Systems 33 (2020), pp. 22205‚Äì22216 (cit. on p. 2). 380
[KJH20] A. Koskela, J. J√§lk√∂, and A. Honkela. ‚ÄúComputing tight differential privacy 381
guarantees using fft‚Äù. In: International Conference on ArtiÔ¨Åcial Intelligence 382
and Statistics . PMLR. 2020, pp. 2560‚Äì2569. URL:https://arxiv.org/ 383
abs/1906.03049 (cit. on p. 1). 384
[LF20] K. Leino and M. Fredrikson. ‚ÄúStolen memories: Leveraging model memoriza- 385
tion for calibratedfWhite-Boxgmembership inference‚Äù. In: 29th USENIX se- 386
curity symposium (USENIX Security 20) . 2020, pp. 1605‚Äì1622. URL:https: 387
//arxiv.org/abs/1906.11798 (cit. on p. 2). 388
[Mir17] I. Mironov. ‚ÄúR√©nyi differential privacy‚Äù. In: 2017 IEEE 30th computer secu- 389
rity foundations symposium (CSF) . IEEE. 2017, pp. 263‚Äì275. URL:https: 390
//arxiv.org/abs/1702.07476 (cit. on p. 1). 391
[NHSBTJCT23] M. Nasr, J. Hayes, T. Steinke, B. Balle, F. Tram√®r, M. Jagielski, N. Carlini, 392
and A. Terzis. ‚ÄúTight Auditing of Differentially Private Machine Learning‚Äù. 393
In:arXiv preprint arXiv:2302.07956 (2023). URL:https://arxiv.org/ 394
abs/2302.07956 (cit. on pp. 1, 2, 5, 6). 395
[NSTPC21] M. Nasr, S. Song, A. Thakurta, N. Papernot, and N. Carlini. ‚ÄúAdversary 396
instantiation: Lower bounds for differentially private machine learning‚Äù. In: 397
2021 IEEE Symposium on security and privacy (SP) . IEEE. 2021, pp. 866‚Äì 398
882. URL:https://arxiv.org/abs/2101.04535 (cit. on pp. 1, 2, 5, 6). 399
[SDSOJ19] A. Sablayrolles, M. Douze, C. Schmid, Y . Ollivier, and H. J√©gou. ‚ÄúWhite- 400
box vs black-box: Bayes optimal strategies for membership inference‚Äù. In: 401
International Conference on Machine Learning . PMLR. 2019, pp. 5558‚Äì5567. 402
URL:https://arxiv.org/abs/1908.11229 (cit. on p. 2). 403
[SNJ23] T. Steinke, M. Nasr, and M. Jagielski. ‚ÄúPrivacy auditing with one (1) training 404
run‚Äù. In: Advances in Neural Information Processing Systems 36 (2023). URL: 405
https://arxiv.org/abs/2305.08846 (cit. on pp. 2, 6). 406
[SOJH09] S. Sankararaman, G. Obozinski, M. I. Jordan, and E. Halperin. ‚ÄúGenomic 407
privacy and limits of individual detection in a pool‚Äù. In: Nature genetics 41.9 408
(2009), pp. 965‚Äì967 (cit. on p. 2). 409
[SSSS17] R. Shokri, M. Stronati, C. Song, and V . Shmatikov. ‚ÄúMembership inference 410
attacks against machine learning models‚Äù. In: 2017 IEEE symposium on 411
security and privacy (SP) . IEEE. 2017, pp. 3‚Äì18 (cit. on p. 2). 412
[Ste22] T. Steinke. ‚ÄúComposition of Differential Privacy & Privacy AmpliÔ¨Åcation 413
by Subsampling‚Äù. In: arXiv preprint arXiv:2210.00597 (2022). URL:https: 414
//arxiv.org/abs/2210.00597 (cit. on p. 1). 415
11[TTSSJC22] F. Tramer, A. Terzis, T. Steinke, S. Song, M. Jagielski, and N. Carlini. ‚ÄúDebug- 416
ging differential privacy: A case study for privacy auditing‚Äù. In: arXiv preprint 417
arXiv:2202.12219 (2022). URL:https://arxiv.org/abs/2202.12219 418
(cit. on p. 2). 419
[WBKBGGG23] Y . Wen, A. Bansal, H. Kazemi, E. Borgnia, M. Goldblum, J. Geiping, and 420
T. Goldstein. ‚ÄúCanary in a coalmine: Better membership inference with en- 421
sembled adversarial queries‚Äù. In: ICLR . 2023. URL:https://arxiv.org/ 422
abs/2210.10750 (cit. on p. 2). 423
[YS22] J. Ye and R. Shokri. ‚ÄúDifferentially private learning needs hidden state (or 424
much faster convergence)‚Äù. In: Advances in Neural Information Processing 425
Systems 35 (2022), pp. 703‚Äì715. URL:https://arxiv.org/abs/2203. 426
05363 (cit. on p. 2). 427
[ZBWTSRPNK22] S. Zanella-B√©guelin, L. Wutschitz, S. Tople, A. Salem, V . R√ºhle, A. Paverd, 428
M. Naseri, and B. K√∂pf. ‚ÄúBayesian estimation of differential privacy‚Äù. In: 429
arXiv preprint arXiv:2206.05199 (2022) (cit. on p. 2). 430
[ZK16] S. Zagoruyko and N. Komodakis. ‚ÄúWide residual networks‚Äù. In: arXiv preprint 431
arXiv:1605.07146 (2016) (cit. on pp. 5, 20). 432
[ZLS23] S. Zarifzadeh, P. C. -J. M. Liu, and R. Shokri. ‚ÄúLow-Cost High-Power Mem- 433
bership Inference by Boosting Relativity‚Äù. In: (2023). URL:https://arxiv. 434
org/abs/2312.03262 (cit. on p. 2). 435
A Proof of Theorem 1 436
Proof. Letxibe the canary, let Dbe the dataset with the canary and D0be the dataset without the 437
canary. Since `andrare linear, wlog we can assume r= 0andrmt 1`(mt 1;xi) =vifor some 438
set of vectorsfvig, such thatkvik21. We can also assume wlog kvik= 1since, ifkvik<1, 439
the Ô¨Ånal privacy guarantee we show only improves. 440
We have the following recursion for mt: 441
mt=mt 1  X
i2Btvi+t!
; ti:i:d N (0;2Id):
Unrolling the recursion: 442
mt=m0 2
4X
t2[T]X
i2Btvi+3
5; N(0;T2Id):
By the post-processing property of DP, we can assume that in addition to the Ô¨Ånal model mT, we 443
release m0andfBtnfxiggt2[T], that is we release all examples that were sampled in each batch 444
except for the canary. The following fis a bijection, computable by an adversary using the released 445
information: 446
f(mT) := 2
4mT m0
 X
t2[T]X
i2Btnfxigvi:3
5
Sincefis a bijection, distinguishing mTsampled using DandD0is equivalent to distinguishing 447
f(mT)instead. Now we have f(mT) =N(0;T2Id)forD0, andf(mT) =N(0;T2Id) +kvi, 448
kBinomial (T;q). For any vector uorthogonal to vi, by isotropy of the Gaussian distribution the 449
distribution ofhf(mT);uiis the same for both DandD0and independent of hf(mT);vii, hence 450
distinguishing f(mT)givenDandD0is the same as distingushing hf(mT);viigivenDandD0. 451
Finally, the distribution of hf(mT);viiis exactlyPforDand exactlyQforD0. By post-processing, 452
this gives the theorem. 453
12We can also see that the function T;q; is tight (i.e., even if we do not release Btnfxig), by 454
considering the 1-dimensional setting, where vi= 0fori6=iandvi= 1;= 1;m0= 0. Then, 455
the distribution of mTgivenDis exactlyP, and givenD0is exactlyQ. 456
A.1 Computing from" 457
Here, we give an efÔ¨Åciently computable expression for the function T;q;("). UsingP;Q as in 458
Theorem 1, let f(y)be the privacy loss for the output y: 459
f(y) = logP(y)
Q(y)
= log TX
k=0T
k
qk(1 q)n kexp( (y k)2=2T2)
exp( y2=2T2)!
= log TX
k=0T
k
qk(1 q)kexp2ky k2
2T2!
:
Then for any ", using the fact that S=fy:f(y)"gmaximizesP(S) e"Q(S), we have: 460
He"(P;Q) =P(fy:f(y)"g) e"Q(fy:f(y)"g)
=P(fy:yf 1(")g) e"Q(fy:yf 1(")g)
=TX
k=0T
k
qk(1 q)kPr[N(k;T2)f 1(")] e"Pr[N(0;T2)f 1(")]:
Similarly,S=fy:f(y) "gmaximizesQ(S) e"P(S)so we have: 461
He"(Q;P) =Q(fy:f(y) "g) e"P(fy:f(y) "g)
=Q(fy:yf 1( ")g) e"P(fy:yf 1( ")g)
= Pr[N(0;T2)f 1( ")] e"TX
k=0T
k
qk(1 q)kPr[N(k;T2)f 1( ")]:
These expressions can be evaluated efÔ¨Åciently. Since fis monotone, it can be inverted via binary 462
search. We can also use binary search to evaluate "as a function of . 463
B Linear Worst Case for Full Batch Setting 464
It turns out that in the full-batch setting, the worst-case analyses of DP-GD with 465
intermediate_iterates and with last_iterate_only are the same. This phenomenon arises 466
because there is no subsampling (because q= 1 in Algorithm 1) and thus the algorithm is ‚Äújust‚Äù 467
the Gaussian mechanism. Intuitively, DP-GD with intermediate_iterates corresponds to T 468
calls to the Gaussian mechanism with noise multiplier , while DP-GD with last_iterate_only 469
corresponds to one call to the Gaussian mechanism with noise multiplier =p
T; these are equivalent 470
by the properties of the Gaussian distribution. 471
We can formalize this using the language of Gaussian DP [DRS19]: DP-GD (Algorithm 1 with 472
q= 1) satisÔ¨Åesp
T=-GDP. (Each iteration satisÔ¨Åes 1=-GDP and adaptive composition implies 473
the overall guarantee.) This means that the privacy loss is exactly dominated by that of the Gaussian 474
mechanism with noise multiplier =p
T. Linear losses give an example such that DP-GD with 475
last_iterate_only has exactly this privacy loss, since the Ô¨Ånal iterate reveals the sum of all the 476
noisy gradient estimates. The worst-case privacy of DP-GD with intermediate_iterates is no 477
worse than that of DP-GD with last_iterate_only . The reverse is also true (by postprocessing). 478
In more detail: For Titerations of (full-batch) DP-GD on a linear losses, if the losses are (wlog) 479
1-Lipschitz and we add noise N(0;2
n2I)to the gradient in every round, distinguishing the last 480
13iterate of DP-SGD on adjacent databases is equivalent to distinguishing N(0;T2)andN(T;T2). 481
This can be seen as a special case of Theorem 1 for p= 1, so we do not a give a detailed argument 482
here. 483
If instead we are given every iteration mt, for any1-Lipschitz loss, distinguishing the joint distribu- 484
tions of mtgivenmt 1on adjacent databases is equivalent to distinguishing N(0;2)andN(1;2). 485
In turn, distinguishing the distribution of all iterates on adjacent databases is equivalent to distin- 486
guishingN(0T;2IT)andN(1T;2IT), where 0Tand1Tare the all-zeros and all-ones vectors in 487
RT. Because the Gaussian distribution is isotropic, distinguishing N(0T;2IT)andN(1T;2IT)is 488
equivalent to distinguishing hx;1Tiwhere xN(0T;2IT)andhx;1Tiwhere xN(1T;2IT). 489
These distributions are N(0;T2)andN(T;T2), the exact pair of distributions we reduced to for 490
last-iterate analysis of linear losses. 491
C Malicious Dataset Attack Details 492
Algorithms 2, 3, and 4 summarizes the construction for the attack. The attack assume the model 493
parameters have dimension equal to the number of iterations. It also assumes each data point can 494
reference which iteration of training is currently happening (this can be implemented by having 495
a single model parameter which increments in each step, independently of the training examples, 496
without impacting the privacy of the training process). Then we build our two datasets Dand 497
D0=D[fxgso that all points in dataset D(‚Äúrepeaters‚Äù) run Algorithm 3 to compute gradients 498
and the canary point in D0runs Algorithm 2 to compute its gradient. Our attack relies heavily on 499
DP-SGD‚Äôs lack of assumptions on the data distribution and any speciÔ¨Åc properties of the model or 500
gradients. Algorithm 2, which generates the canary data point, is straightforward. Its goal is to store 501
in the model parameters whether it was present in iteration iby outputting a gradient that changes 502
only thei-th index of the model parameters by 1 (assuming a clipping threshold of 1). 503
All other data points, the ‚Äúrepeaters‚Äù, are present in both datasets ( DandD0), and have three tasks: 504
‚Ä¢Cancel out any noise added to the model parameters at an index larger than the current 505
iteration. At iteration i, their gradients for parameters from index ionward will be the same 506
as the current value of the model parameter, scaled by the batch size and the learning rate to 507
ensure this parameter value will be 0 after the update. 508
‚Ä¢Evaluate whether the canary point was present in the previous iteration by comparing 509
the model parameter at index i 1with a threshold, and rewrite the value of that model 510
parameter to a large value if the canary was present. 511
‚Ä¢Ensure that all previous decisions are not overwritten by noise by continuing to rewrite them 512
with a large value based on their previous value. 513
To achieve all of these goals simultaneously, we require that the batch size is large enough that the 514
repeaters‚Äô updates are not clipped. 515
Finally Algorithm 4 runs DP-SGD, with repeater points computing gradients with Algorithm 3 and 516
the canary point, sampled with probability p, computing its gradient using Algorithm 2. In our 517
experiments we run Algorithm 4 100,000 times. And to evaluate if the model parameters was from 518
datasetDorD0we run a hypothesis test on the values of the model parameters. All constants are 519
chosen to ensure all objectives of the repeaters are satisÔ¨Åed. 520
Algorithm 2 Canary data point
1:function ADV(x,i)
2: Initialize aas a zero vector of the same dimension as x
3: Setai 1 .Set thei-th component to 1
4: return a
5:end function
14Algorithm 3 Additional data points
Require: model parameters x, iteration number i, batch sizeN, learning rate , previous history
thresholdtpast, last iteration threshold tlast, history ampliÔ¨Åcation value BIG_V AL
1:function REPEATERS (x,i,N,,tpast,tlast, BIG_V AL)
2: h x0:i .Parameter ‚Äúhistory‚Äù up to iteration i, not inclusive
3: f xi:end .Future and current parameters, starting from iteration i
4: f  f=(N) .Remove noise from last iteration
5: base_history  x0:i=(N) .By default, zero out entire history
6: iflength (h)>1then
7: h0:i 1 BIG_V AL=(N)(2 1[h0:i 1tpast] 1).If an old iteration is large
enough, it was a canary iteration, so amplify it
8: end if
9: iflength (h)>0then
10: hi 1 BIG_V AL=(N)(2 1[hitlast] 1).If the last iteration is large enough, it
was a canary iteration, so amplify it
11: end if
12: h h+base_history .Don‚Äôt zero out canary iterations
13: a concatenate (h;f)
14: return a
15:end function
Algorithm 4 Encoding Attacking
Require: add-diff , whether to add the canary, batch size N, sampling rate p, learning rate ( ),
iteration count/parameter count D
1:function RUN_DPSGD (add-diff)
2:C 1
3: Initialize model m 0of dimension D
4: fori= 0toDdo
5: Generate a uniform random value q2[0;1]
6: r repeaters (m;i)
7: Compute norm c jjrjj
8: ifc>0then
9: Normalize r r=max(c;C)
10: end if
11: Adjusted vector z rN
12: Verify condition on mi
13: ifpqand add-diff then
14: r adv(m;i)
15: Normalize and update z
16: end if
17: Apply Gaussian noise to z
18: Update model m m z
19: end for
20: return m
21:end function
15NeurIPS Paper Checklist 521
1.Claims 522
Question: Do the main claims made in the abstract and introduction accurately reÔ¨Çect the 523
paper‚Äôs contributions and scope? 524
Answer: [Yes] 525
JustiÔ¨Åcation: The abstract and introduction state what we do and then the following sections 526
and the appendix provide details. 527
Guidelines: 528
‚Ä¢The answer NA means that the abstract and introduction do not include the claims 529
made in the paper. 530
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the 531
contributions made in the paper and important assumptions and limitations. A No or 532
NA answer to this question will not be perceived well by the reviewers. 533
‚Ä¢The claims made should match theoretical and experimental results, and reÔ¨Çect how 534
much the results can be expected to generalize to other settings. 535
‚Ä¢It is Ô¨Åne to include aspirational goals as motivation as long as it is clear that these goals 536
are not attained by the paper. 537
2.Limitations 538
Question: Does the paper discuss the limitations of the work performed by the authors? 539
Answer: [Yes] 540
JustiÔ¨Åcation: Section 4 discusses the limitations. 541
Guidelines: 542
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that 543
the paper has limitations, but those are not discussed in the paper. 544
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper. 545
‚Ä¢The paper should point out any strong assumptions and how robust the results are to 546
violations of these assumptions (e.g., independence assumptions, noiseless settings, 547
model well-speciÔ¨Åcation, asymptotic approximations only holding locally). The authors 548
should reÔ¨Çect on how these assumptions might be violated in practice and what the 549
implications would be. 550
‚Ä¢The authors should reÔ¨Çect on the scope of the claims made, e.g., if the approach was 551
only tested on a few datasets or with a few runs. In general, empirical results often 552
depend on implicit assumptions, which should be articulated. 553
‚Ä¢The authors should reÔ¨Çect on the factors that inÔ¨Çuence the performance of the approach. 554
For example, a facial recognition algorithm may perform poorly when image resolution 555
is low or images are taken in low lighting. Or a speech-to-text system might not be 556
used reliably to provide closed captions for online lectures because it fails to handle 557
technical jargon. 558
‚Ä¢The authors should discuss the computational efÔ¨Åciency of the proposed algorithms 559
and how they scale with dataset size. 560
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to 561
address problems of privacy and fairness. 562
‚Ä¢While the authors might fear that complete honesty about limitations might be used by 563
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 564
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best 565
judgment and recognize that individual actions in favor of transparency play an impor- 566
tant role in developing norms that preserve the integrity of the community. Reviewers 567
will be speciÔ¨Åcally instructed to not penalize honesty concerning limitations. 568
3.Theory Assumptions and Proofs 569
Question: For each theoretical result, does the paper provide the full set of assumptions and 570
a complete (and correct) proof? 571
Answer: [Yes] 572
16JustiÔ¨Åcation: Theorem 1 is proved in Appendix A. 573
Guidelines: 574
‚Ä¢ The answer NA means that the paper does not include theoretical results. 575
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross- 576
referenced. 577
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems. 578
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if 579
they appear in the supplemental material, the authors are encouraged to provide a short 580
proof sketch to provide intuition. 581
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented 582
by formal proofs provided in appendix or supplemental material. 583
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced. 584
4.Experimental Result Reproducibility 585
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 586
perimental results of the paper to the extent that it affects the main claims and/or conclusions 587
of the paper (regardless of whether the code and data are provided or not)? 588
Answer: [Yes] 589
JustiÔ¨Åcation: The setup is described for each experiment we conduct and we reference prior 590
work that these build on. 591
Guidelines: 592
‚Ä¢ The answer NA means that the paper does not include experiments. 593
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived 594
well by the reviewers: Making the paper reproducible is important, regardless of 595
whether the code and data are provided or not. 596
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken 597
to make their results reproducible or veriÔ¨Åable. 598
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways. 599
For example, if the contribution is a novel architecture, describing the architecture fully 600
might sufÔ¨Åce, or if the contribution is a speciÔ¨Åc model and empirical evaluation, it may 601
be necessary to either make it possible for others to replicate the model with the same 602
dataset, or provide access to the model. In general. releasing code and data is often 603
one good way to accomplish this, but reproducibility can also be provided via detailed 604
instructions for how to replicate the results, access to a hosted model (e.g., in the case 605
of a large language model), releasing of a model checkpoint, or other means that are 606
appropriate to the research performed. 607
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis- 608
sions to provide some reasonable avenue for reproducibility, which may depend on the 609
nature of the contribution. For example 610
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 611
to reproduce that algorithm. 612
(b)If the contribution is primarily a new model architecture, the paper should describe 613
the architecture clearly and fully. 614
(c)If the contribution is a new model (e.g., a large language model), then there should 615
either be a way to access this model for reproducing the results or a way to reproduce 616
the model (e.g., with an open-source dataset or instructions for how to construct 617
the dataset). 618
(d)We recognize that reproducibility may be tricky in some cases, in which case 619
authors are welcome to describe the particular way they provide for reproducibility. 620
In the case of closed-source models, it may be that access to the model is limited in 621
some way (e.g., to registered users), but it should be possible for other researchers 622
to have some path to reproducing or verifying the results. 623
5.Open access to data and code 624
Question: Does the paper provide open access to the data and code, with sufÔ¨Åcient instruc- 625
tions to faithfully reproduce the main experimental results, as described in supplemental 626
material? 627
17Answer: [No] 628
JustiÔ¨Åcation: We intend to release the code eventually, but we are not able to do so at the 629
moment; we refrain from providing a detailed reason, as this could violate anonymity. 630
Guidelines: 631
‚Ä¢ The answer NA means that paper does not include experiments requiring code. 632
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 633
public/guides/CodeSubmissionPolicy ) for more details. 634
‚Ä¢While we encourage the release of code and data, we understand that this might not be 635
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not 636
including code, unless this is central to the contribution (e.g., for a new open-source 637
benchmark). 638
‚Ä¢The instructions should contain the exact command and environment needed to run to 639
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 640
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 641
‚Ä¢The authors should provide instructions on data access and preparation, including how 642
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 643
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new 644
proposed method and baselines. If only a subset of experiments are reproducible, they 645
should state which ones are omitted from the script and why. 646
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized 647
versions (if applicable). 648
‚Ä¢Providing as much information as possible in supplemental material (appended to the 649
paper) is recommended, but including URLs to data and code is permitted. 650
6.Experimental Setting/Details 651
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 652
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 653
results? 654
Answer: [Yes] 655
JustiÔ¨Åcation: The setup is described for each experiment we conduct and we reference 656
prior work that these build on. We use the standard CIFAR10 dataset for deep learning 657
experiments. 658
Guidelines: 659
‚Ä¢ The answer NA means that the paper does not include experiments. 660
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail 661
that is necessary to appreciate the results and make sense of them. 662
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental 663
material. 664
7.Experiment Statistical SigniÔ¨Åcance 665
Question: Does the paper report error bars suitably and correctly deÔ¨Åned or other appropriate 666
information about the statistical signiÔ¨Åcance of the experiments? 667
Answer: [Yes] 668
JustiÔ¨Åcation: The auditing results present a lower bound which can be viewed as a one-sided 669
conÔ¨Ådence interval. For the other results the numbers are computed non-statistically (i.e. 670
by numerically evaluating a formula); the only potential error here is due to numerical 671
precision. 672
Guidelines: 673
‚Ä¢ The answer NA means that the paper does not include experiments. 674
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, conÔ¨Å- 675
dence intervals, or statistical signiÔ¨Åcance tests, at least for the experiments that support 676
the main claims of the paper. 677
‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for 678
example, train/test split, initialization, random drawing of some parameter, or overall 679
run with given experimental conditions). 680
18‚Ä¢The method for calculating the error bars should be explained (closed form formula, 681
call to a library function, bootstrap, etc.) 682
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors). 683
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error 684
of the mean. 685
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should 686
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 687
of Normality of errors is not veriÔ¨Åed. 688
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or 689
Ô¨Ågures symmetric error bars that would yield results that are out of range (e.g. negative 690
error rates). 691
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how 692
they were calculated and reference the corresponding Ô¨Ågures or tables in the text. 693
8.Experiments Compute Resources 694
Question: For each experiment, does the paper provide sufÔ¨Åcient information on the com- 695
puter resources (type of compute workers, memory, time of execution) needed to reproduce 696
the experiments? 697
Answer: [Yes] 698
JustiÔ¨Åcation: We used A2-megagpu-16g machines from Google cloud which have 16 Nvidia 699
A100 40GB GPUs to run the experiments in this paper. Overall we used around 33,000 700
hours of GPU to run all of the experiments in the paper. 701
Guidelines: 702
‚Ä¢ The answer NA means that the paper does not include experiments. 703
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster, 704
or cloud provider, including relevant memory and storage. 705
‚Ä¢The paper should provide the amount of compute required for each of the individual 706
experimental runs as well as estimate the total compute. 707
‚Ä¢The paper should disclose whether the full research project required more compute 708
than the experiments reported in the paper (e.g., preliminary or failed experiments that 709
didn‚Äôt make it into the paper). 710
9.Code Of Ethics 711
Question: Does the research conducted in the paper conform, in every respect, with the 712
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 713
Answer: [Yes] 714
JustiÔ¨Åcation: No human subjects or sensitive data were used. 715
Guidelines: 716
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 717
‚Ä¢If the authors answer No, they should explain the special circumstances that require a 718
deviation from the Code of Ethics. 719
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid- 720
eration due to laws or regulations in their jurisdiction). 721
10.Broader Impacts 722
Question: Does the paper discuss both potential positive societal impacts and negative 723
societal impacts of the work performed? 724
Answer: [NA] 725
JustiÔ¨Åcation: This work is primarily theoretical. While it is possible that downstream uses 726
of our work could be societally impactful, the precise consequences are difÔ¨Åcult to foresee. 727
The considerations are similar to any other paper on private machine learning. 728
Guidelines: 729
‚Ä¢ The answer NA means that there is no societal impact of the work performed. 730
19‚Ä¢If the authors answer NA or No, they should explain why their work has no societal 731
impact or why the paper does not address societal impact. 732
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses 733
(e.g., disinformation, generating fake proÔ¨Åles, surveillance), fairness considerations 734
(e.g., deployment of technologies that could make decisions that unfairly impact speciÔ¨Åc 735
groups), privacy considerations, and security considerations. 736
‚Ä¢The conference expects that many papers will be foundational research and not tied 737
to particular applications, let alone deployments. However, if there is a direct path to 738
any negative applications, the authors should point it out. For example, it is legitimate 739
to point out that an improvement in the quality of generative models could be used to 740
generate deepfakes for disinformation. On the other hand, it is not needed to point out 741
that a generic algorithm for optimizing neural networks could enable people to train 742
models that generate Deepfakes faster. 743
‚Ä¢The authors should consider possible harms that could arise when the technology is 744
being used as intended and functioning correctly, harms that could arise when the 745
technology is being used as intended but gives incorrect results, and harms following 746
from (intentional or unintentional) misuse of the technology. 747
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation 748
strategies (e.g., gated release of models, providing defenses in addition to attacks, 749
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 750
feedback over time, improving the efÔ¨Åciency and accessibility of ML). 751
11.Safeguards 752
Question: Does the paper describe safeguards that have been put in place for responsible 753
release of data or models that have a high risk for misuse (e.g., pretrained language models, 754
image generators, or scraped datasets)? 755
Answer: [NA] 756
JustiÔ¨Åcation: Our paper uses standard datasets (CIFAR10) and standard models (WideRes- 757
Net). 758
Guidelines: 759
‚Ä¢ The answer NA means that the paper poses no such risks. 760
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with 761
necessary safeguards to allow for controlled use of the model, for example by requiring 762
that users adhere to usage guidelines or restrictions to access the model or implementing 763
safety Ô¨Ålters. 764
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors 765
should describe how they avoided releasing unsafe images. 766
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do 767
not require this, but we encourage authors to take this into account and make a best 768
faith effort. 769
12.Licenses for existing assets 770
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 771
the paper, properly credited and are the license and terms of use explicitly mentioned and 772
properly respected? 773
Answer: [Yes] 774
JustiÔ¨Åcation: We use CIFAR10 [Ale09] and a WideResNet [ZK16]. 775
Guidelines: 776
‚Ä¢ The answer NA means that the paper does not use existing assets. 777
‚Ä¢ The authors should cite the original paper that produced the code package or dataset. 778
‚Ä¢The authors should state which version of the asset is used and, if possible, include a 779
URL. 780
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset. 781
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of 782
service of that source should be provided. 783
20‚Ä¢If assets are released, the license, copyright information, and terms of use in the 784
package should be provided. For popular datasets, paperswithcode.com/datasets 785
has curated licenses for some datasets. Their licensing guide can help determine the 786
license of a dataset. 787
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of 788
the derived asset (if it has changed) should be provided. 789
‚Ä¢If this information is not available online, the authors are encouraged to reach out to 790
the asset‚Äôs creators. 791
13.New Assets 792
Question: Are new assets introduced in the paper well documented and is the documentation 793
provided alongside the assets? 794
Answer: [NA] 795
JustiÔ¨Åcation: Our main contribution is a heuristic privacy analysis. This is fully described in 796
the paper and can be computed using existing open-source libraries. 797
Guidelines: 798
‚Ä¢ The answer NA means that the paper does not release new assets. 799
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their 800
submissions via structured templates. This includes details about training, license, 801
limitations, etc. 802
‚Ä¢The paper should discuss whether and how consent was obtained from people whose 803
asset is used. 804
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either 805
create an anonymized URL or include an anonymized zip Ô¨Åle. 806
14.Crowdsourcing and Research with Human Subjects 807
Question: For crowdsourcing experiments and research with human subjects, does the paper 808
include the full text of instructions given to participants and screenshots, if applicable, as 809
well as details about compensation (if any)? 810
Answer: [NA] 811
JustiÔ¨Åcation: The paper does not involve crowdsourcing nor research with human subjects. 812
Guidelines: 813
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 814
human subjects. 815
‚Ä¢Including this information in the supplemental material is Ô¨Åne, but if the main contribu- 816
tion of the paper involves human subjects, then as much detail as possible should be 817
included in the main paper. 818
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 819
or other labor should be paid at least the minimum wage in the country of the data 820
collector. 821
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 822
Subjects 823
Question: Does the paper describe potential risks incurred by study participants, whether 824
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 825
approvals (or an equivalent approval/review based on the requirements of your country or 826
institution) were obtained? 827
Answer: [NA] 828
JustiÔ¨Åcation: The paper does not involve crowdsourcing nor research with human subjects. 829
Guidelines: 830
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 831
human subjects. 832
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent) 833
may be required for any human subjects research. If you obtained IRB approval, you 834
should clearly state this in the paper. 835
21‚Ä¢We recognize that the procedures for this may vary signiÔ¨Åcantly between institutions 836
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 837
guidelines for their institution. 838
‚Ä¢For initial submissions, do not include any information that would break anonymity (if 839
applicable), such as the institution conducting the review. 840
22