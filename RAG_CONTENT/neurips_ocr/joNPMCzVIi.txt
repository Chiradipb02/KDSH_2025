Improved Bayes Regret Bounds for Multi-Task
Hierarchical Bayesian Bandit Algorithms
Jiechao Guan1Hui Xiong1,2,
1AI Thrust , The Hong Kong University of Science and Technology (Guangzhou), China
2Department of Computer Science and Engineering, HKUST, China
{jiechaoguan, xionghui}@hkust-gz.edu.cn
Abstract
Hierarchical Bayesian bandit refers to the multi-task bandit problem in which
bandit tasks are assumed to be drawn from the same distribution. In this work,
we provide improved Bayes regret bounds for hierarchical Bayesian bandit algo-
rithms in the multi-task linear bandit and semi-bandit settings. For the multi-task
linear bandit, we ﬁrst analyze the preexisting hierarchical Thompson sampling
(HierTS) algorithm, and improve its gap-independent Bayes regret bound from
O(mp
nlognlog (mn))toO(mpnlogn)in the case of inﬁnite action set, with
mbeing the number of tasks and nthe number of iterations per task. In the case
of ﬁnite action set, we propose a novel hierarchical Bayesian bandit algorithm,
named hierarchical BayesUCB (HierBayesUCB), that achieves the logarithmic but
gap-dependent regret bound O(mlog (mn) logn)under mild assumptions. All
of the above regret bounds hold in many variants of hierarchical Bayesian linear
bandit problem, including when the tasks are solved sequentially or concurrently.
Furthermore, we extend the aforementioned HierTS and HierBayesUCB algorithms
to the multi-task combinatorial semi-bandit setting. Concretely, our combinatorial
HierTS algorithm attains comparable Bayes regret bound O(mpnlogn)with
respect to the latest one. Moreover, our combinatorial HierBayesUCB yields a
sharper Bayes regret bound O(mlog (mn) logn). Experiments are conducted to
validate the soundness of our theoretical results for multi-task bandit algorithms.
1 Introduction
A stochastic bandit [ 26,6,27] is a sequential decision-making problem where at each round, an agent
has to choose an action, and receives a stochastic reward without knowing its expected value. The gap
between the cumulative reward of optimal actions in hindsight and the cumulative reward of agent
is deﬁned as regret . The goal is to minimize regret, through a combination of exploring different
actions and exploiting those with high rewards in the past. Typical applications of bandit algorithms
include news article recommendation [ 28], computational advertisement [ 20], and dynamic pricing
[24]. For example, in news article recommendation, the agent must choose a news article for a user.
The actions in this bandit setting are articles and the reward could be an indicator of a click from user.
When the agent has to solve multiple bandit tasks, many machine learning researchers resort to
multi-task learning/meta-learning paradigm [ 8,34] to beneﬁt task adaptation. The existing works
focused on the multi-task bandit problem can be categorized into three main groups: (1)The ﬁrst
group attempts to learn a low-dimensional representation shared by different bandit tasks, to derive a
sharper cumulative regret bound than that derived by learning each task independently [ 19,10].(2)
The second group leverages the similarity of contexts (e.g. the feature of actions) in bandit tasks
to improve agent’s ability to predict rewards in a new task [ 14,36].(3)The third group chooses to
Corresponding Author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).maintain a meta-distribution over the hyper-parameters of within-task bandit algorithms (like Tsallis-
INF [ 23], OFUL [ 9], and Thompson sampling [ 25,7,17]), and draws informative hyper-parameters
from the meta-distribution for efﬁcient regret minimization. Our work falls into the third group and
formulates the problem of learning similar bandit tasks in a hierarchical Bayesian bandit model [ 17].
Speciﬁcally, in hierarchical Bayesian bandit setting, each bandit task is characterized by a task pa-
rameter. Different bandit task parameters are assumed to be independently and identically distributed
according to the same distribution. At each round, the learning agent interacts with one or several
bandit tasks, which correspond to the sequential and concurrent bandit settings respectively. Many ex-
isting works considered hierarchical Bayesian bandit problem, and proposed Thompson sampling [ 33]
type algorithms to solve it [ 25,7,36]. The latest work [ 17] proposed hierarchical Thompson sampling
(HierTS) algorithm and developed a gap-independent Bayes regret bound O(mp
nlognlog (mn))
in the Gaussian linear bandit setting, where mis the number of bandit tasks and nthe number of
iterations per task. However, it is still unclear for us whether we can derive sharper regret bounds or
how to extend hierarchical Bayesian bandit algorithms to the more general multi-task bandit setting.
In this work, we attempt to tackle the above two issues, by providing improved Bayes regret bounds
for hierarchical Bayesian bandit algorithms in the multi-task Gaussian linear bandit and semi-bandit
setting. Firstly, in the linear bandit setting, we improve the multi-task Bayes regret bound of HierTS
toO(mpnlogn)in the case of inﬁnite action set, strengthening the latest bound in [ 17, Thm 3] by a
factor ofO(p
log (mn)). In the case of ﬁnite action set, we propose a novel hierarchical Bayesian
bandit algorithm, named hierarchical BayesUCB (HierBayesUCB), that achieves the logarithmic but
gap-dependent regret bound O(mlog (mn) logn)under mild assumptions. All of the above regret
bounds for linear bandit hold in both the sequential and concurrent setting. Secondly, we extend the
aforementioned HierTS and HierBayesUCB algorithms to the multi-task Gaussian combinatorial
semi-bandit setting. Concretely, our combinatorial HierTS algorithm attains comparable Bayes regret
boundO(mpnlogn)with respect to the latest one in [ 7, Thm 6]. Moreover, our combinatorial
HierBayesUCB yields a sharper but gap-dependent regret bound O(mlog (mn) logn). Extensive
experiments in the Gaussian linear bandit setting are conducted to support our theoretical results.
Overall, our theoretical contributions are four-fold: (1)In the case of inﬁnite action set, we provide
a tighter Bayes regret bound O(mpnlogn)for HierTS. This bound improves the latest result by
a factor ofO(p
log (mn)).(2)In the case of ﬁnite action set, we propose a novel HierBayesUCB
algorithm, and provide gap-dependent logarithmic Bayes regret bound O(mlog (mn) logn)for it.
(3)We generalize the above regret bounds for linear bandit from sequential setting to the more
challenging concurrent setting. (4)We extend both HierTS and HierBayesUCB algorithms to the
more general multi-task combinatorial semi-bandit setting and derive improved Bayes regret bounds.
2 Related Work
Frequentist Regret Bounds for Stochastic Linear Bandit . In the frequentist stochastic bandit
setting, we do not assume the bandit task parameter is sampled from a ﬁxed distribution. The
frequentist regret is thus for any ﬁxed task parameter, without taking expectation over the distribution
of task parameter. (1)In the case of ﬁnite action set: [5] for the ﬁrst time investigated the stochastic
linear bandit problem and proposed an algorithm with a frequentist regret of O(p
dnlog3=2n), where
dis the dimension of action space and nis the number of rounds. [ 29] developed a new algorithm
and improved the regret bound to O(pdnlogn). [12] showed that the lower frequentist regret bound
in the ﬁnite action set is 
(p
dn).(2)In the case of inﬁnite action set: Both [ 13] and [ 30] proposed
algorithms that achieve O(dpnlog3=2n)regret. The regret bound was further improved in [ 1,15] to
O(dpnlogn), by designing novel linear bandit algorithms or utilizing advanced martingale methods.
Bayes Regret Bounds for Bayesian Linear Bandit . In the Bayesian stochastic bandit setting, the
Bayes regret is the expected cumulative regret whose expectation is taken over the draw of task
parameter from a distribution. It is not difﬁcult to see that the frequentist regret upper bound implies
a Bayes regret upper bound, because the former holds for any task parameter. (1)When the action set
is inﬁnite: [ 30] showed that in the Gaussian linear bandit setting, the Bayes regret of any Bayesian
bandit algorithm is lower bounded by 
(pn). [31] for the ﬁrst time gave the Bayes regret bound
ofO(dpnlogn)for both Thompson sampling (TS) and BayesUCB [ 22] algorithms. Recently,
[21] provided an improved Bayes regret O(dpnlogn)for TS algorithm with a concise proof. (2)
When the action set is ﬁnite: [ 32] derived a tight regret bound of O(p
dn)for TS algorithm with
2sub-Gaussian reward noise, via a novel information-theoretic approach. Recently, [ 3] developed a
logarithmic Bayes regret bound O(d2log2n)for BayesUCB algorithm in the Gaussian bandit setting.
Frequentist Regret Bounds for Multi-Task Linear Bandit Problems . Under the representation
learning paradigm, the frequentist regret bounds in [ 19,10] for multi-task linear bandits scales as
O(mp
nk), wherekis the dimension of low-dimensional representation. The expected frequentist
regret upper bound for multi-task adversarial linear bandit in [ 23] isO(mp
nlog (1 +nV)), withV
being the similarity among multiple adversarial bandit tasks. Nevertheless, we should mention that all
of these frequentist regret bounds for multi-task linear bandit problem are not tighter than 
(mpn).
Bayes Regret Bounds for Multi-Task Bayesian Linear Bandit/Semi-Bandit . The most related
works to ours are [ 25,7,17], which provided hierarchical-type Thompson sampling algorithms for
multi-task bandit and derived Bayes regret bounds in the Gaussian reward setting. We list these
Bayes regret bounds in Table 1 for direct comparisons. Among them, the latest work [ 17] proposed
the HierTS algorithm and obtained its regret bound of O(mp
nlognlog (mn)), [7] derived the ﬁrst
Bayes regret bound for multi-task hierarchical Bayesian semi-bandit algorithm. In this work, we
provide for HierTS improved Bayes regret bound of O(mpnlogn)in Theorem 5.1. We also propose
a novel HierBayesUCB algorithm that achieves logarithmic regret bound O(mlognlog (nm)). We
ﬁnally extend HierTS and HierBayesUCB to the semi-bandit setting and derive improved theoretical
results. Other works utilized action features or structure information to derive Bayes regret bounds
for multi-task bandit [ 36,37], e.g. the Bayes regret bound in [ 36] isO(mpnlogn+mlog2(mn)).
Hierarchical Bayesian Bandit Algorithms . Hierarchical Bayesian bandit algorithm was ﬁrst pro-
posed by [ 25] to solve multi-task bandit problems. More hierarchical-type Thompson sampling
algorithms based on multi-task/meta learning frameworks were developed with improved theoretical
guarantees and empirical performance [ 7,17,36,37]. There also existed other works investigating
hierarchical Bayesian bandit algorithms within the single-task bandit setting. For example, [ 16] ex-
tended the two-level hierarchical Bayesian bandit framework to the deeper multiple-level hierarchial
Bayesian bandit framework. [ 2] generalized the single-effect-parameter HierTS algorithm (i.e. the
action parameter is centered at a single latent variable) to the mixed-effect bandit framework where
each action is associated with a parameter that depends upon one or multiple effect parameters.
3 Problem Setting
For any positive integer n, denote [n] =f1;2;:::;ngfor brevity. For any square matrix M2Rdd,
denote1(M),d(M)as its maximum and minimum eigenvalues respectively, denote (M) =
1(M)=d(M)as its condition number. The action set AB(0;B)Rdis assumed to be compact
for some positive constant B > 0, whereB(0;B)is the closed ball centered at the origin. We use h;i
to denote the inner-product between vectors, use w(a)orwato denote the a-th element of vector w.
Single-Task Bandit . A stochastic bandit problem is characterized by an unknown parameter 
with an action set A. Each action a2A under the bandit instance is associated with a reward
distribution P(ja;). The reward mean of action aunderis denoted as r(a;) =EYP(ja;)[Y],
and the optimal action under is denoted as A= arg maxa2Ar(a;). In the stochastic linear
bandit setting, the mean reward of action a2A isr(a;) =a>. In Bayesian bandit problem, we
further assume that the task parameter is independently and identically distributed (i.i.d.) according
to a task parameter distribution P(j), which is characterized by an unknown hyper-parameter .
Single-Task Semi-Bandit . In the semi-bandit setting, the action set A= [K]is a set of ﬁnite items.
A=fAA :jAjLgis a family of subsets of Awith up toLitems, where LK.w2RKis
a weight vector. The weight of a set A2Ais deﬁned asP
a2Aw(a). We assume that the weights w
are drawn i.i.d. from a distribution, and the mean weight is denoted as w=E[w]. Following previous
work [ 38], we focus on the coherent case [ 39] which assumes that the agent knows a feature matrix
2RKd, such that w= , whereis the task parameter drawn from P(j). The reward of a
subsetA2Aunder the bandit instance is deﬁned as r(A;) =P
a2A()(a) =P
a2Aha;i,
where ais the transpose of the a-th row of matrix . We further assume that kakB,8a2A .
Hierarchical Bayesian Multi-Task Bandit/Semi-Bandit . In this setting, the agent interacts with
mtasks sequentially or concurrently. First, sample the hyper-parameter from a hyper-prior Q.
Then, for each task s2[m], sample the task parameter s;independently from distribution P(j).
The learning process can be detailed as follows. At round t1, the agent interacts with a set
3of tasksSt[m], takes a series of actions At= (As;t)s2St, and receives a series of rewards
Yt= (Ys;t)s2St. In the bandit setting, Ys;tP(jAs;t;s;)is a stochastic reward obtained by taking
actionAs;tin tasks2St; in the semi-bandit setting, Ys;t=f^ws;t(a)ga2As;tis a series of stochastic
rewards, where ^ws;t=ws+s;t,ws= s;, ands;tis aK-dimensional random noise. The full
hierarchical Bayesian bandit/semi-bandit model in the m-task learning setting is exhibited as follow:
(1)Q;(2)s;jP(j);8s2[m];(3)Ys;tjAs;t;s;P(jAs;t;s;);8t1;s2St:
Therefore, the goal of the agent in hierarchical Bayesian multi-task bandit/semi-bandit setting is to
interact with mtasks efﬁciently and minimize the following cumulative multi-task Bayes regret :
BR(m;n) =EX
t1X
s2Str(As;;s;) r(As;t;s;)
; (1)
whereAs;= arg maxa2Ar(a;s;)is the optimal action for task s2[m]in the bandit setting,
andAs;2arg maxA2Ar(A;s;)is the optimal subset for task s2[m]in the semi-bandit setting.
The expectation is taken over , all task parameters (s;)s2[m], all actions (At)t1, all stochastic
rewards (Yt)t1. We further assume that the action set Ais the same across different tasks for ease of
exposition, and assume that the learning agent interacts with any task s2[m]for at mostnrounds for
convenient comparison with exiting regret upper bounds for multi-task bandit/semi-bandit problem.
4 Algorithm
DenoteHs;t=((As;`;Ys;`))`<t;s2Stas the history of all interactions of agent with task s2[m], and
Ht=(Hs;t)s2[m]as the whole interaction history up to round t. We next introduce the speciﬁc form of
Hierarchical Thompson Sampling (HierTS) and Hierarchical BayesUCB (HierBayesUCB) algorithms
in the multi-task Bayesian linear bandit and semi-bandit settings, and instantiate these two algorithms
to the multi-task Gaussian linear bandit (Algorithm 1) and semi-bandit (Algorithm 2) problems.
4.1 Hierarchical Thompson Sampling and Hierarchical BayesUCB
At roundt, hierarchical Bayesian bandit algorithm samples a hyper-parameter tfrom the hyper-
posteriorQtdeﬁned asQt() =P(=jHt), and then interacts with tasks St[m]. Next, we
give details of bandit algorithms, and details of semi-bandit algorithms are deferred to Section 5.4.
Hierarchical Thompson Sampling . For any task s2St, HierTS samples task parameter s;t
from the distribution Ps;t(jt),P(s;=j=t;Hs;t)and takes the action As;t=
arg maxa2Aa>s;t, where Ps;t(jt)is only conditioned on Hs;tdue to the independence be-
tween task parameter s;and other task histories. This process clearly samples bandit instance
s;tfrom the true posterior P(s;=jHt), which is equivalent to the form:R
P(s;=;=
jHt)d=R
Ps;t(j)Qt()d, where Ps;t(j)/Ls;t()P(j)is the posterior probability,
Ls;t()=Q
(a;y)2Hs;tP(yja;)is the likelihood function, P(j)is the prior probability by Bayes rule.
Hierarchical BayesUCB . For any task s2Stin roundt, HierBayesUCB computes the upper
conﬁdence bound Ut;s;a=a>^s;t+q
2 log1
kak^s;tfor anya2A, where ^s;tand^s;tare the
expectation and covariance of the distribution (i.e. P(s;=jHt)) ofs;conditioned on the history
Ht, and then takes action with the highest upper conﬁdence bound : As;t arg maxa2AUt;s;a.
4.2 Multi-Task Gaussian Linear Bandit and Semi-Bandit
The hierarchical Gaussian environment is generated as follow. In the multi-task linear ban-
dit setting: (1) N (q;q),(2)s;j N (;0);8s2[m],(3)Ys;tjAs;t;s;
N(A>
s;ts;;2);8t1;s2St; In the semi-bandit setting, the only difference lies in step (3)
whereYs;t;ajAs;t;s;N(ha;s;i;2)for anya2As;t. Here,q;;s;ared-dimensional
vectors; q;02Rddare positive semi-deﬁnite covariance matrices. In the above two settings, the
reward noise can be regarded as N(0;2). In the following theoretical analysis sections, we assume
that all ofq;q;0andare known by the agent to guarantee an analytically tractable posterior.
Concretely, using some basic algebraic computations in hierarchical Gaussian model (e.g. see [ 25,
Appendix D]), we can obtain the closed-form hyper-posterior in round tasQt() =N(; t;t),
where the expectation tand the covariance matrix tofQt()have the following explicit forms:
t=t 
 1
qq+X
s2[m](0+G 1
s;t) 1G 1
s;tBs;t
;  1
t=  1
q+X
s2[m](0+G 1
s;t) 1:(2)
4Algorithm 1 Hierarchical Bayesian Algorithms
for Multi-Task Linear Bandit Setting
1:Input: Hyper-prior Q
2:InitializeQ1 Q
3:fort= 1;2;::: do
4: Sample hyper-parameter tQt
5: Observe tasksSt[m]
6: fors2Stdo
7: Option I (HierTS) :
Compute Ps;t(jt)/Ls;t()P(jt)
Sample task parameter s;tPs;t(jt)
Take actionAs;t arg maxa2Aa>s;t
Option II (HierBayesUCB) :
SetUt;s;a=a>^s;t+q
2 log1
kak^s;t,
for anya2A
Take actionAs;t arg maxa2AUt;s;a
8: Observe reward Ys;t
9: end for
10: UpdateQt+1
11:end forAlgorithm 2 Hierarchical Bayesian Algorithms
for Multi-Task Combinatorial Semi-Bandit Setting
1:Input: Hyper-prior Q, features 2RKd
2:InitializeQ1 Q
3:fort= 1;2;::: do
4: Sample hyper-parameter tQt
5: Observe tasksSt[m]
6: fors2Stdo
7: Option I (HierTS):
Compute Ps;t(jt)/Ls;t()P(jt)
Sample task parameter s;tPs;t(jt)
ComputeAs;t=ORACLE(A;A;s;t)
Option II (HierBayesUCB):
ComputeUt;s(A) =P
a2A(a>^s;t+q
2 log1
kak^s;t), for allA2A
ComputeAs;t= arg max A2AUt;s(A)
8: ChoooseAs;tand observef^ws;t(a)ga2As;t
9: end for
10: UpdateQt+1
11:end for
Here, in the bandit setting Gs;t= 2P
`<t1fs2S`gAs;`A>
s;`andBs;t= 2P
`<t1fs2
S`gAs;`Ys;`; in the semi-bandit setting Gs;t= 2P
`<t1fs2S`g(P
a2As;`a>
a)andBs;t=
 2P
`<t1fs2S`g(P
a2As;`a^ws;t(a)). After the hyper-parameter tis sampled from Qt(),
we sample task parameter s;tN(; ~s;t;~s;t)for tasks, where ~s;t=~s;t( 1
0t+Bs;t)is the
posterior mean, ~ 1
s;t=  1
0+Gs;tthe posterior covariance matrix. Such posterior of a linear model
is obtained with a Gaussian prior N(t;0)and Gaussian observations (Ys;`)`<t;s2S`by Bayes rule.
On the other hand, we also need to handle P(s;=jHt). It is not difﬁcult to see that, in the
multi-task Gaussian linear bandit/semi-bandit setting, s;jHtis Gaussian and denoted as P(s;=
jHt)=N(; ^s;t;^s;t). According to Lemma B.1, ^s;tand^s;thave the following explicit forms:
^s;t=~s;t( 1
0t+Bs;t);^s;t=~s;t+~s;t 1
0t 1
0~s;t: (3)
5 Bayes Regret Bounds
In this section, we provide improved regret bounds of hierarchical Bayesian bandit algorithms for
multi-task Gaussian linear bandit/semi-bandit problem. Concretely, we provide improved analysis
for HierTS in the sequential linear bandit setting (Sections 5.1), propose a novel HierBayesUCB
bandit algorithm with logarithmic regret guarantee (Section 5.2), develop regret bounds for these
two algorithms in the concurrent linear bandit setting (Section 5.3), and ﬁnally extend these two
algorithms to the semi-bandit setting (Section 5.4) with improved regret bounds. In the proof for
our theoretical results, the most important step is to give an upper bound on the so-called posterior
varianceVm;n, which in the multi-task linear bandit setting is deﬁned and upper bounded as follow:
Vm;n,EhX
t1X
s2StkAs;tk2
^s;ti
O 
mdlog (n
d) +dlog (m
d)
: (4)
Although the above bound on Vm;nachieves the same order (w.r.t. m;n andd) as that in the latest
bound of [ 17, Sect B], our bound has a smaller multiplicative factor (see more details in Table 4). In the
multi-task semi-bandit setting, the posterior variance is Vm;n,EP
t1P
s2StP
a2As;tkak2
^s;t
and can be bounded in a similar way. To ﬁnish the whole proof, our strategy consists of two main
steps: (1)The ﬁrst step is to transform the multi-task Bayes regret BR(m;n)into an intermediate
regret upper bound that involves the posterior variance Vm;nas the dominant term. (2)The second
step is to boundVm;nwith Eq. (4). Combining the results in steps (1)and(2)yields Bayes regret
bound for multi-task hierarchical Bayesian bandit/semi-bandit algorithms. Detailed comparisons
between our regret bounds and others in the bandit setting are shown in Table 1. Next, we deﬁne
c1=2+B21(0),c2=2+B21(0)+B21(q)(0)to be used through the whole Section 5.
5Table 1: Different Bayes regret bounds for multi-task d-dimensional linear (or K-armed) bandit
problem in the sequential setting. mis the number of tasks, nthe number of iterations per task, Ais
the action set. Bayes Regret Bound =Bound I +Bound II +Negligible Terms , where Bound I is
the regret bound for solving mtasks, Bound II the regret bound for learning hyper-parameter .
Bayes Regret Bound jAj Bound I Bound II
[25, Theorem 3] Finite O 
mpKnlogn
O 
n2Kp
mlog (n) log (K)
[7, Theorem 5] FiniteO 
mp
dn(logn) log (n2jAj)
O p
dmn(logm) log (njAj)
[17, Theorem 3] InﬁniteO 
mdp
nlog (n
d) log (mn)
O 
dp
mnlog (m) log (mn)
Our Theorem 5.1 Inﬁnite O 
mdp
nlog (n
d)
O 
dp
mnlog (m
d)
Our Theorem 5.2 Finite O 
mdlog (n
d) log (mn)
O 
dlog (m
d) log (mn)
5.1 Improved Regret Bound for HierTS in the Sequential Bandit Setting
In the sequential bandit setting, jStj= 1. Then, conditioned on Ht, it is not difﬁcult to see that in
Bayes regret, each term E[>
s;As; >
s;As;tjHt] =E
(s; ^s;t)>As;Ht
, and we use a novel
Cauchy-Schwartz type inequality from [ 21, Prop 2] to bound E
(s; ^s;t)>As;Ht
, leading
toBR(m;n)EhP
t1P
s2Stq
dE 
(s; ^s;t)>As;t2Hti
. Expand the expression in the
right hand side of the above inequality, we then have BR(m;n)EP
t;s2Stq
dA>
s;t^s;tAs;tp
dmnVm;n, reducing the Bayes regret bound to the posterior variance bound problem. Recalling
Eq. (4) achieves our ﬁrst improved Bayes regret upper bound in the sequential linear bandit setting.
Theorem 5.1 (Near-Optimal Sequential Regret) Let jStj= 1for any round t. Then in the multi-task
Gaussian linear bandit setting, the Bayes regret upper bound of HierTS is as follow:
BR(m;n)dp
2mns
mc1log (1 +n
d) +c2log (1 +mTr(q 1
0)
d):
Our explanations for the above sequential regret bound are three-fold: (1)The term
mdp
nc1log (1 +n=d)represents the regret bound for solving mbandit tasks, whose parame-
terss;are drawn i.i.d. from the prior distribution N(;0). Under this assumption, no task
provides information for any other task, and hence this bound is linear in m. Similar observation was
also pointed out by [ 25,7,17].(2)The termdq
mnc 2log 
1+mTr(q 1
0)=d
represents the regret
bound for learning the hyper-parameter . Such bound is sublinear in mand is not a dominant
term whenmis large. (3)For a largem, the averaged Bayes regret bound across mtasks is of
BR(m;n)=m=O(dpnlogn), and strengthens the latest averaged bound O(dpnlogn)in [17,
Thm 3] by a factorplogn. Besides, since the lower Bayes regret bound for any Bayesian bandit
algorithm is 
(dpn)[30], our task-averaged Bayes regret bound is within O(plogn)of optimality
and hence is called ‘Near-Optimal’ sequential regret bound. We further make a detailed comparison
between our regret bound in Theorem 5.1 and the regret bound [ 17, Thm 3] in the following remark.
Remark 5.1 (Improvements of Our Theorem 5.1 over the Latest One) Our sequential regret bound
has two improvements over the latest one in [ 17, Thm 3, shown in Table 1]: (1)We remove the
additionalp
log (mn)factor in both the regret bound for solving mbandit tasks and the regret bound
for learning the hyper-parameter .(2)In the regret bound for learning hyper-parameter , [17]
has a multiplicative factor 2(0), whereas our multiplicative factor is (0). Such improvement is
achieved by using technical matrix analysis proposed in Lemma C.1. and explained in Remark A.1.
5.2 Logarithmic Regret Bound for HierBayesUCB in the Sequential Bandit Setting
In this section, we attempt to provide further improved Bayes regret bounds for hierarchical bandit
algorithms in the sequential bandit setting. Because the task averaged Bayes regret bound in
Theorem 5.1 is near optimal, it is not easy to derive improved Bayes regret bounds under the same
assumptions. Therefore, we further assume that the action set Ais ﬁnite, and propose a novel
6hierarchical Bayesian bandit algorithm, named Hierarchical BayesUCB (HierBayesUCB), for multi-
task linear bandit problem. The pseudo-code of our proposed algorithm is shown in Algorithm 1.
Next, we introduce some necessary notations. Let s;t=>
s;(As; As;t),s;min =
mina2AnfAs;g 
>
s;As; >
s;a
,min= mins2[m]s;min. For any  > 0, let 
min=
maxf;ming. Deﬁne the event Es;t=f8a2A :ja>(s; ^s;t)jq
2 log1
kak^s;tg. Then,
analogous to [ 3], we decompose the Bayes regret BR(m;n) =EP
t1P
s2Sts;tinto three terms:
EP
t1;s2Sts;t
1fs;t;Es;tg+1fs;t< ;Es;tg+1fEs;tg
. We can bound the last
two terms trivially with mn[+ 2 
maxt;sjs;tj
jAj]. For the ﬁrst term, we use the fact that
As;tjHti.i.d.As;jHt, as well as the Upper Conﬁdence Bound (UCB) technique to reduce it to an
intermediate upper bound P
t1;s2StkAs;tk2
^s;tlog1

=mins;tjs;tj. Combining the upper bound
overVm;nin Eq. (4), HierBayesUCB can achieve the following logarithmic Bayes regret bound in
the sequential bandit setting (the logarithmic bound can be extended to the concurrent setting).
Theorem 5.2 (Logarithmic Sequential Regret of HierBayesUCB) Let jStj= 1for any round t, and
the action setAis ﬁnite withjAj<1. Then in the multi-task Gaussian linear bandit setting, for any
2(0;1),>0, the Bayes regretBR(m;n)of HierBayesUCB is upper bounded by
mnh
+4B1
2
1(0+q) 
d1
2+kqk^ 1
s;1
jAji
+E[16dlog1


min]h
mc1log (1+n
d)+c2log (1+mTr(q 1
0)
d)i
:
We give more explanations for the above sequential regret in terms of the following ﬁve aspects:
(1)If let= 1=(mn),= 1=(mn)andmin>>  , the above sequential regret bound is of
O 
log (mn)(mdlog (n
d) +dlog (m
d))
. The term O(mdlog (mn) log (n
d))represents the regret
bound for solving mbandit tasks and is linear in m. Such bound is sharper than the corresponding
boundO(mdp
nlog (n
d))in our Theorem 5.1 by a multiplicative factor O(p
log (n=d)=nlog (mn)),
which is less than 1especially when mn.(2)The termO(dlog (mn) log (m
d))represents
the regret bound for learning the hyper-parameter , and its contribution to the Bayes regret
bound can be negligible. Besides, this bound is sharper than the bound O(dp
mnlog (m=d))
in our Theorem 5.1. (3)The averaged Bayes regret bound across mtasks can be regarded as
BR(m;n)=m=O(dlog (mn) logn), which is logarithmic in n. Therefore, we call our regret bound
as ‘Logarithmic’ sequential regret bound. Moreover, if there exists a ﬁxed positive integer i<<n ,
such thatmni, then our task-averaged Bayes regret BR(m;n)=m=O(dE[1

min] log2n)matches
the latest single-task Bayes regret bound in [ 3, Thm 5] and is remarkably similar to the frequentist
regretO(d 1
minlog2n)in [1, Thm 5] . (4)We can obtain sharper bounds by setting ;as different
values. For example, by setting = 1=n, our regret bound becomes O([mn +m] +logn

minmlogn),
which is of order O(mlog2n)if we set= 1=(mn)and the gap min>>  is large. (5)We
also need to point out that, the Bayes regret bound in Theorem 5.2 scales with E[1

min]. If the gap
min1=(mn), then 
min= 1=(mn)and this may cause a large Bayes regret upper bound.
5.3 Improved Regret Bounds of HierTS and HierBayesUCB in the Concurrent Bandit Setting
In the concurrent bandit setting, there exists a positive integer Lm, such that 1jStjL. The
concurrent bandit setting is thus more challenging than the sequential bandit setting, because the
agent in the concurrent setting needs to interact with multiple bandit tasks in parallel at each round
t1, and the hyper-posterior Qtwill not be updated until the end of round t. Therefore, we need to
make an additional assumption on the action space Aas follow to facilitate our theoretical analysis.
Assumption 5.1 There exist actions faigd
i=1A, a constant>0, such thatd(Pd
i=1aia>
i).
This assumption is also used in previous works [ 7,17] for hierarchical Bayesian linear bandit. It
indicates thatPd
i=1aia>
iis a positive deﬁnite matrix, and does not weaken the generality of our
theoretical results. Actually, if Rdis not spanned by actions in A, we can projectAinto a subspace
where the assumption holds. We also need to modify the HierTS algorithm to let the agent take the
basic actionsfaigd
i=1for the ﬁrstdinteractions in any task s2[m]. This modiﬁcation guarantees that
the agent explores all directions within the task. Such exploration is very similar to the initialization
method in UCB type K-arm bandit algorithms [ 6,4], which choose to pull each arm in the ﬁrst K
rounds. Deﬁne c3= 1 +B2 2(0)
1(0) +2=
that will be used throughout the concurrent
7Table 2: Different Bayes regret bounds for multi-task semi-bandit problem. Bayes Regret Bound
=Bound I +Bound II +Negligible Terms .mis the number of tasks, nthe number of iterations per
task,Kthe size of action set, Lthe number of pulled actions at each round ( 1LK).Bound I
is the regret bound for solving mtasks, Bound II the regret bound for learning hyper-parameter .
Bayes Regret Bound A Bound I Bound II
[7, Theorem 6] [K]O 
mp
nKL lognlog (nK)
O p
mnKL logmlog (nK)
Our Theorem 5.4 [K]O 
mp
nLlog (nL) log (nK)
O 
L3
2p
mnlogmlog (nK)
Our Theorem 5.5 [K]O 
mLlog (nL) log (mnK )
O 
L3log (m) log (mnK )
setting. Then, analogous to the proof for Theorem 5.1, we boundp
mnVm;nwith a more reﬁned
analysis, achieving the following improved Bayes regret bound for HierTS in the concurrent setting.
Theorem 5.3 Under Assumption 5.1, let 1jStjLfor any round t1. Then in the multi-task
Gaussian linear bandit setting, the Bayes regret BR(m;n)of HierTS is upper bounded by
2Bmdq
1(0+ q)(p
d+kqk^ 1
s;1)+dpmns
2mc1log (1+n
d)+2c2c3log (1+mTr(q 1
0)
d):
The concurrent regret bound in Theorem 5.3 achieves almost the same order (w.r.t. m;n;d ) as the
sequential regret bound in Theorem 5.1, but differs in two aspects: (1)The bound for learning mi.i.d.
bandit tasks has an additional term Bmdp
1(0+ q)(p
d+kqk^ 1
s;1). This is due to the fact
that we take the basic actions faigd
i=1ﬁrst for each task s2[m]in the modiﬁed HierTS algorithm.
(2)The bound for learning the hyper-parameter has an additional multiplicative factor c3. This is
the price for deriving regret bounds in the concurrent setting. Nevertheless, when compared with the
latest concurrent regret bound in [ 17, Thm 4] for HierTS, our concurrent regret bound in Theorem 5.3
removes thep
log (mn)factor in both the regret bound for learning mbandit tasks and the regret
bound for learning hyper-parameter . Detailed comparisons between different concurrent regret
bounds for multi-task linear bandit setting are listed in Table 3. Furthermore, utilizing the proof
strategy to demonstrate the logarithmic sequential regret for HierBayesUCB in our Theorem 5.2, we
can analogously develop a logarithmic concurrent regret upper bound for HierBayesUCB algorithm,
which is deferred to our Theorem C.2 in Appendix C due to the limited space of the main paper.
5.4 Improved Regret Bounds for HierTS and HierBayesUCB in the Semi-Bandit Setting
In this section, we extend the HierTS and HierBayesUCB algorithms to the multi-task Gaussian com-
binatorial semi-bandit setting. The pseudo-code of them is shown in Algorithm 2. Algorithm 2 is very
similar to Algorithm 1 (i.e. the multi-task linear bandit algorithms), except that the combinatorial Hi-
erTS in Algorithm 2 uses the approximation/randomized algorithm ORACLE to solve combinatorial
problemA2arg maxA2AP
a2Aw(a)and denotes the solution as A= ORACLE(A;A;w).
We adopt the ORACLE operator as in the seminal works [ 11,38] to guarantee the efﬁciency of com-
binatorial HierTS semi-bandit algorithm. In this section, we only consider the sequential semi-bandit
setting (i.e.jStj= 1) for ease of presentation, and our results can be extended to the concurrent
semi-bandit setting. Then, deﬁne c4=2+B2L1(0) +B21(q)(0), we ﬁrst derive the
Bayes regret upper bound for combinatorial HierTS algorithm in the sequential semi-bandit setting.
Theorem 5.4 LetjStj= 1 for anyt1. Letcq
2 ln nKB 1(0)p
2
, then in the multi-task
Gaussian semi-bandit setting, the Bayes regret upper bound of combinatorial HierTS is:
BR(m;n)m+cp
mnLs
2c1mlog (1 +nL
d) + 2c4Ldlog(1 +mTr( 1
0q)
d):
Detailed comparisons between different Bayes regret bounds for multi-task semi-bandit problem
are listed in Table 2. We can see that, in our Theorem 5.4, both the regret bound O(mpnlogn)for
learningmtasks and the regret bound O(pmnlogmlogn)for learning hyper-parameter can
achieve the same order (w.r.t. mandn) when compared with the latest bound in [ 7, Thm 6]. Besides,
our Bayes regret bound is logarithmic in the number Kof items, whereas the Bayes regret bound in
8[7, Thm 6] is sublinear in K. Therefore, our regret bound becomes sharper when the size of action
set is very large, e.g. K>>L . Next, we derive a gap-dependent logarithmic multi-task Bayes regret
bound for our proposed combinatorial HierBayesUCB algorithm in the sequential semi-bandit setting.
Theorem 5.5 LetjStj= 1for anyt1. Then for any >0;2(0;1), in the multi-task Gaussian
semi-bandit setting, the Bayes regret BR(m;n)of combinatorial HierBayesUCB is bounded by
mn
+4LBK1
2
1(0+q)(d1
2+kqk^ 1
s;1)
+E8Llog1


minh
2c1mlog (1+nL
d)+2c4Ldlog(1+mTr( 1
0q)
d)i
:
In Theorem 5.5, if we set = 1=(mnK ),= 1=(mn), and min>> , then the regret bound
O 
mlognlog (mn)
for learning mtasks is logarithmic in n. Such bound is sharper than the latest
oneO(mpnlogn)in [7, Thm 6] for multi-task semi-bandit. The regret bound O(logmlog (mn))
for learning hyper-parameter is also sharper than that of O(pmnlogmlogn)in [7, Thm 6].
Besides, since =1=(mnK ), the whole Bayes regret bound is also logarithmic in the number Kof
items. Nevertheless, we should point out that our bounds hold for the multi-task semi-bandits with
linear generalization, but [ 7] focuses on the multi-task K-arm semi-bandits without feature matrix .
5.5 Technical Novelties for Deriving Improved Regret Bounds
In this section, we summarize our technical novelties in terms of the following three aspects:
(1)For the improved regret bound for HierTS in Theorem 5.1: our proof has three novelties: (i)We
apply a novel Cauchy-Schwartz type inequality in Lemma A.2 to bound E
(s; ^s;t)>As;Ht
q
dE 
(s; ^s;t)>As;t2Ht
, leading to a sharper bound withoutp
log (mn)factor:
BR(m;n)EX
t;s2Stq
dA>
s;t^s;tAs;tp
dmnVm;nO(mp
nlogn):
(ii)We use a more technical positive semi-deﬁnite matrix decomposition analysis (i.e. our Lemma A.1)
to reduce the multiplicative factor 2(0)to(0).(iii)Deﬁne a new matrix ~Xs;tsuch that the
denominator in the regret is 2+B21(0), not just2, avoiding the case that the variance serves
alone as the denominator. Such technical novelties are also listed explicitly in Table 4.
(2)For the improved regret bound for HierBayesUCB in Theorem 5.2 in the sequential bandit setting:
our novelty lies in decomposing the Bayes regret BR(m;n) =EP
t1P
s2Sts;tinto three terms:
EX
t1X
s2Sts;t=EX
t1;s2Sts;t
1fs;t;Es;tg+1fs;t<;Es;tg+1fEs;tg
;
and bounding the ﬁrst term with a new method as well as the property of BayesUCB algorithm as
Es;t1fs;t;Es;tg=E2
s;t
s;t1fs;t;Es;tgEC2
t;s;As;t

min;
resulting in the ﬁnal improved gap-dependent regret bound for HierBayesUCB as follows
 X
t1;s2StkAs;tk2
^s;tlog1

=
minO 
mlog (n) log1
=1=mn= = = = = = =O 
mlog (n) log (mn)
:
(3)For the improved regret bounds for HierTS and HierBayesUCB in the concurrent setting and in
the semi-bandit setting: besides the aforementioned technical novelties in (1)and(2), the additional
technical novelty lies in leveraging more reﬁned analysis (e.g. using Woodbury matrix identity) to
bound the gap between matrices  1
t+1and 1
t(more details is shown in Lemma C.1 and Eq. (6)).
6 Experiments
In this section, we conduct experiments in the linear bandit setting to verify our theoretical results.
Speciﬁcally, we show the inﬂuence of hyper-parameters (e.g. m;n;L ) to the multi-task Bayes regret
of HierTS and HierBayesUCB, to validate the consistency between their regret bounds and practical
performance. Besides, we compare the performance between our algorithms and other baselines, to
show the effectiveness of hierarchical Bayesian bandit algorithms in the multi-task bandit setting.
90 2 4 6 8 10
Number of Tasks m050100150200Regret
Linear Bandit ( d= 4,σq= 1.0,L=1)
n=100
n=200
n=300
n=400(a) Regrets w.r.t. different m
2 4 6 8 10
Number of Concurrent Tasks L0100200300400Regret
Linear Bandit ( m= 10,σq= 0.5)
d=2
d=4
d=8 (b) Regrets w.r.t. different L
0 100 200 300 400
Roundt02505007501000Regret
Linear Bandit ( d= 4,m= 10,L=5)
σq=4
σq=3
σq=2
σq=1
σq=0.1 (c) Regrets w.r.t. different q
0 100 200 300 400
Roundt0200400Regret
Linear Bandit ( d= 4,m= 10,L=5)
σ0=0.5
σ0=0.4
σ0=0.3
σ0=0.2
σ0=0.1
(d) Regrets w.r.t. different 0
0 100 200 300 400
Roundt05001000Regret
Linear Bandit ( d= 4,m= 10,L=5)
σ=10
σ=7
σ=4
σ=1
σ=0.1 (e) Regrets w.r.t. different 
0 100 200 300 400
Roundt0100200300Regret
Linear Bandit ( d= 4,σq= 1.0)
OracleTS
TS
HierTS
HierBayesUCB (f) Regrets of different algorithms
Figure 1: Regrets of HierTS algorithm with respect to (w.r.t.) different hyper-parameters.
Experimental Setting . We follow the same experimental setting as that in [ 7,17]. Concretely, we
conduct linear bandit experiments with Gaussian reward. The synthetic problem is deﬁned as follows.
In most experiments, we set the number of total tasks as m= 10 , the dimension of action space as
d= 4, the number of concurrent tasks as L= 5, the number of rounds as n= 200m=L . We focus
on the ﬁnite action space with jAj= 10 , and each action is sampled uniformly from [ 0:5;0:5]d. In
hierarchical Bayesian model, we set the hyper-prior as zero-mean isotropic Gaussian distribution
N(q;q) =N(0;q), where q=2
qId; and set the task variance 0=2
0Id. Unless otherwise
stated, we set q= 1,0= 0:1,2= 0:5for each task in most experiments. We exhibit the regret
performance of HierTS algorithm with respect to ﬁve hyper-parameters m;L;q;0;in Figure 1
(a)-(e) respectively. The regret performance of HierBayesUCB is shown in Figure 2 of Appendix F.
Besides, we compare HierTS/HierBayesUCB with other two TS type algorithms that do not learn the
hyper-parameter in a hierarchical Bayesian model. The ﬁrst baseline is the vanilla TS algorithm
that samples task parameter s;from the marginal prior N(q;q+  0). The second baseline is an
idealized TS algorithm that knows exactly and uses the true prior N(;0). We call the second
baseline as OracleTS, since this TS algorithm accesses more information of than HierTS and
vanilla TS algorithm. We show the regret performance of these four bandit algorithms in Figure 1 (f).
Experimental Results . From Figure 1, we can observe that: (1)In plot (a), the multi-task regret
becomes larger with the increase of mandn, which is consistent with our regret upper bound in
Theorems 5.1. (2)In plot (b), the regret increases with a higher dimension d. The number Lof the
concurrent tasks seems do not have a large impact on regret. (3)In plots (c)-(e), the regret decreases
with a smaller variance (e.g. q,0and) in hierarchical Bayesian model, validating the provable
beneﬁts of variance-reduction in regret minimization, which is revealed in our multi-task Bayes regret
upper bounds. (4)The task-averaged regret of HierTS is tighter than that of single-task TS algorithm,
empirically demonstrating the advantages of multi-task Bayesian bandit optimization paradigm over
single-task bandit learning. (5)Our proposed HierBayesUCB achieves lower regret than HierTS.
7 Conclusions
This paper provides improved Bayes regret bounds for hierarchical Bayesian bandit algorithms in the
multi-task Gaussian linear bandit and semi-bandit setting. For linear bandit problem: in the case of
inﬁnite action set, we strengthen the preexisting regret bound O(mp
nlognlog (mn))of HierTS
toO(mpnlogn)by a factor of O(p
log (mn)); in the case of ﬁnite action set, we propose a novel
HierBayesUCB algorithm that achieves logarithmic regret bound O(mlog (mn) logn)under mild
conditions. Our regret bounds in the bandit setting hold when the agent solves tasks sequentially or
concurrently. Then, we extend the above HierTS and HierBayesUCB algorithms to the multi-task
semi-bandit setting and derive improved regret bounds. The synthetic experiments further support our
theoretical results. Our future work aims to extend our bounds to the sub-exponential bandit setting.
10Acknowledgments and Disclosure of Funding
Jiechao sincerely appreciates the ﬁnancial support from the People’s Government of Guangzhou
Municipality for his postdoctoral project. We thank all reviewers for their constructive suggestions
to improve the quality of this paper. This work was supported in part by the National Key R&D
Program of China (Grant No.2023YFF0725001), in part by the National Natural Science Foundation
of China (Grant No.92370204), in part by the guangdong Basic and Applied Basic Research Foun-
dation(Grant No.2023B1515120057), in part by Guangzhou-HKUST(GZ) Joint Funding Program
(Grant No.2023A03J0008), Education Bureau of Guangzhou Municipality.
References
[1]Y . Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear stochastic bandits.
InNeurIPS , pages 2312–2320, 2011.
[2]I. Aouali, B. Kveton, and S. Katariya. Mixed-effect thompson sampling. In AISTATS , pages
2087–2115, 2023.
[3]A. Atsidakou, B. Kveton, S. Katariya, C. Caramanis, and sujay sanghavi. Finite-time logarithmic
bayes regret upper bounds. In NeurIPS , 2023.
[4]J. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In
Conference on Learning Theory (COLT) , 2009.
[5]P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research (JMLR) , 3:397–422, 2002.
[6]P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine Learning , 47(2-3):235–256, 2002.
[7]S. Basu, B. Kveton, M. Zaheer, and C. Szepesvári. No regrets for learning the prior in bandits.
InNeurIPS , pages 28029–28041, 2021.
[8] R. Caruana. Multitask learning. Machine Learning , 28(1):41–75, 1997.
[9]L. Cella, A. Lazaric, and M. Pontil. Meta-learning with stochastic linear bandits. In ICML ,
pages 1360–1370, 2020.
[10] L. Cella, K. Lounici, G. Pacreau, and M. Pontil. Multi-task representation learning with
stochastic linear bandits. In AISTATS , pages 4822–4847, 2023.
[11] W. Chen, Y . Wang, and Y . Yuan. Combinatorial multi-armed bandit: General framework and
applications. In ICML , pages 151–159, 2013.
[12] W. Chu, L. Li, L. Reyzin, and R. E. Schapire. Contextual bandits with linear payoff functions.
InAISTATS , pages 208–214, 2011.
[13] V . Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback.
InConference on Learning Theory (COLT) , pages 355–366, 2008.
[14] A. A. Deshmukh, Ü. Dogan, and C. Scott. Multi-task learning for contextual bandits. In
NeurIPS , pages 4848–4856, 2017.
[15] H. Flynn, D. Reeb, M. Kandemir, and J. Peters. Improved algorithms for stochastic linear
bandits using tail bounds for martingale mixtures. In NeurIPS , 2023.
[16] J. Hong, B. Kveton, S. Katariya, M. Zaheer, and M. Ghavamzadeh. Deep hierarchy in bandits.
InICML , pages 8833–8851, 2022.
[17] J. Hong, B. Kveton, M. Zaheer, and M. Ghavamzadeh. Hierarchical bayesian bandits. In
AISTATS , pages 7724–7741, 2022.
[18] R. A. Horn and C. R. Johnson. Matrix Analysis . Cambridge University Press, 2012.
11[19] J. Hu, X. Chen, C. Jin, L. Li, and L. Wang. Near-optimal representation learning for linear
bandits and linear RL. In ICML , pages 4349–4358, 2021.
[20] S. Kale, L. Reyzin, and R. E. Schapire. Non-stochastic bandit slate problems. In NeurIPS ,
pages 1054–1062, 2010.
[21] C. Kalkanli and A. Özgür. An improved regret bound for thompson sampling in the gaussian
linear bandit setting. In IEEE International Symposium on Information Theory (ISIT) , pages
2783–2788, 2020.
[22] E. Kaufmann, O. Cappe, and A. Garivier. On bayesian upper conﬁdence bounds for bandit
problems. In AISTATS , pages 592–600, 2012.
[23] M. Khodak, I. Osadchiy, K. Harris, M. Balcan, K. Y . Levy, R. Meir, and Z. S. Wu. Meta-learning
adversarial bandit algorithms. In NeurIPS , 2023.
[24] R. D. Kleinberg and F. T. Leighton. The value of knowing a demand curve: Bounds on regret
for online posted-price auctions. In Symposium on Foundations of Computer Science (FOCS) ,
pages 594–605, 2003.
[25] B. Kveton, M. Konobeev, M. Zaheer, C. Hsu, M. Mladenov, C. Boutilier, and C. Szepesvári.
Meta-thompson sampling. In ICML , pages 5884–5893, 2021.
[26] T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in
Applied Mathematics , 6:4—-22, 1985.
[27] T. Lattimore and C. Szepesvári. Bandit Algorithms . Cambridge University Press, 2020.
[28] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized
news article recommendation. In International Conference on World Wide Web (WWW) , pages
661–670, 2010.
[29] Y . Li, Y . Wang, and Y . Zhou. Nearly minimax-optimal regret for linearly parameterized bandits.
InConference on Learning Theory (COLT) , pages 2173–2174, 2019.
[30] P. Rusmevichientong and J. N. Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research , 35(2):395–411, 2010.
[31] D. Russo and B. V . Roy. Learning to optimize via posterior sampling. Mathematics of Operations
Research , 39(4):1221–1243, 2014.
[32] D. Russo and B. V . Roy. An information-theoretic analysis of thompson sampling. Journal of
Machine Learning Research (JMLR) , 17:68:1–68:30, 2016.
[33] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika , 25:285–294, 1933.
[34] S. Thrun and L. Pratt. Learning to Learn . Kluwer Academic Publishers, 1998.
[35] M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint . Cambridge
University Press, 2019.
[36] R. Wan, L. Ge, and R. Song. Metadata-based multi-task bandits with bayesian hierarchical
models. In NeurIPS , pages 29655–29668, 2021.
[37] R. Wan, L. Ge, and R. Song. Towards scalable and robust structured bandits: A meta-learning
framework. In AISTATS , pages 1144–1173, 2023.
[38] Z. Wen, B. Kveton, and A. Ashkan. Efﬁcient learning in large-scale combinatorial semi-bandits.
InICML , pages 1113–1122, 2015.
[39] Z. Wen and B. V . Roy. Efﬁcient exploration and value function generalization in deterministic
systems. In NeurIPS , pages 3021–3029, 2013.
12APPENDIX
A Proofs for Regret Bound of HierTS in the Sequential Bandit Setting
We ﬁrst give the following proposition to bound the posterior variance EP
t1P
s2StkAs;tk2
^s;tin
the sequential setting. We choose to give the worst-case upper bound onP
t1P
s2StkAs;tk2
^s;t.
Proposition A.1 Letc1=2+B21(0),c2=2+B21(0) +B21(q)(0), then
X
t1X
s2StkAs;tk2
^s;t2mdc 1log 
1 +n
d
+ 2dc2log (1 +mTr 
 1
0q
d):
Proof. Note thatkAs;tk2
^s;t=A>
s;t ~s;t+~s;t 1
0t 1
0~s;t
As;t, then we bound
P
t1P
s2StkAs;tk2
~s;tandP
t1P
s2StA>
s;t~s;t 1
0t 1
0~s;tAs;trespectively.
(1) BoundingP
t1P
s2StkAs;tk2
~s;t. Note that ~s;t= ( 1
0+Gs;t) 10, then we have
A>
s;t~s;tAs;tB21(0)<B21(0) +2. Accordingly, we deﬁne a new matrix ~Xs;t,( 1
0+
1
B21(0)+2P
`<t1fs2StgAs;`A>
s;`) 1, and notice that ~Xs;t~s;t= ( 1
0+1
2P
`<t1fs2
StgAs;`A>
s;`) 1, and that ~Xs;1=  0. Then recall c1=2+B21(0), we have
X
t1X
s2StA>
s;t~s;tAs;t
=c1X
t1X
s2StA>
s;t~s;tAs;t
2+B21(0)
2c1X
t1X
s2Stlog 
1 +A>
s;t~s;tAs;t
2+B21(0)
=2c1X
t1log 
1 +A>
St;t~St;tASt;t
2+B21(0)
=2c1X
t1mX
s=11fSt=sglog 
1 +A>
s;t~s;tAs;t
2+B21(0)
2c1X
t1mX
s=11fSt=sglog 
1 +A>
s;t~Xs;tAs;t
2+B21(0)
=2c1mX
s=1X
t11fSt=sglog det 
I+~X1
2
s;tAs;tA>
s;t~X1
2
s;t
2+B21(0)
=2c1mX
s=1X
t11fSt=sg
log det ~X 1
s;t+As;tA>
s;t
2+B21(0)
 log det ~X 1
s;t
=2c1mX
s=1
log det ~X 1
s;mn +1
 log det ~X 1
s;1
=2c1mX
s=1log det 
I+1
2+B21(0)X
tmn1fs2Stg1
2
0As;tA>
s;t1
2
0
2dc1mX
s=1logTr 
I+1
2+B21(0)P
tmn1fs2Stg1
2
0As;tA>
s;t1
2
0
d
13=2dc1mX
s=1log 
1 +P
tmn1fs2StgA>
s;t0As;t
d(2+B21(0))
2mdc 1log 
1 +n
d
;
where the ﬁrst inequality holds because the basic inequality x2 log (1 +x),8x2[0;1]; the third
inequality holds due to the mean-value inequality (Qd
i=1i)1
dPd
i=1i
d, for anyi0; the last
inequality holds due to the fact that the agent interacts with each task s2[m]at mostntimes.
Before bounding the remainingP
t1P
s2StA>
s;t~s;t 1
0t 1
0~s;tAs;t, we introduce the follow-
ing lemma.
Lemma A.1 If the square matrices A>0;B0, then1 
(I+AB)(I+BA) 1
1(A)
d(A).
Proof. According to Theorem 7.6.1 in Page 485 of [ 18], there exists a non-singular matrix S, such that
A=SS>, andB=S >S 1, in which 0is a diagonal matrix. Then we have AB=SS 1,
BA=S >S>. Therefore, applying Weyl’s inequality we have
d 
(I+AB)(I+BA)
=d 
(I+SS 1)(I+S >S>)
=d 
S(I+ )S 1S >(I+ )S>
=d 
S>S(I+ )S 1S >(I+ )
d 
S>S
d 
(I+ )S 1S >(I+ )
d 
S>S
d 
S 1S >
d 
(I+ )2
d 
S>S
=1 
S1S>
=d(A)=1(A):
Remark A.1 (Smaller multiplicative factor than the latest one in [17]) The improvement lies in
our sharper upper bound on 1( 1
0~s;t~s;t 1
0), and detailed explanations are two-fold:
(1)Previous work [17, Appendix B] directly used Weyl’s inequality to upper bound
1( 1
0~s;t~s;t 1
0)2
1( 1
0)2
1(~s;t)2
1( 1
0)2
1(0) =2(0):
(2)Instead of directly using Weyl’s inequality, we ﬁrst propose Lemma A.1 which uses positive
semi-deﬁnite matrix diagonalization technique to bound
1 
(I+AB)(I+BA) 1
1(A)
d(A):
Then we apply Lemma A.1 to upper bound
1( 1
0~s;t~s;t 1
0) =1( 1
0~s;t~s;t 1
0)1 
(I+  0~s;t)(I+~s;t0) 1
(0);
resulting in a smaller multiplicative factor than that in [17]. 
(2) BoundingP
t1P
s2StA>
s;t~s;t 1
0t 1
0~s;tAs;t. First recall that
t=t 
 1
qq+X
s2[m]Bs;t Gs;t( 1
0+Gs;t) 1Bs;t
=t 
 1
qq+X
s2[m](0+G 1
s;t) 1G 1
s;tBs;t
;
 1
t= 1
q+X
s2[m]Gs;t Gs;t( 1
0+Gs;t) 1Gs;t=  1
q+X
s2[m](0+G 1
s;t) 1:
Therefore tq. Then applying Lemma A.1 and Weyl’s inequality, we have
A>
s;t~s;t 1
0t 1
0~s;tAs;tB21(~s;t 1
0t 1
0~s;t)B21( 1
0~s;t~s;t 1
0)1(t)
=B21 
(I+  0~s;t)(I+~s;t0) 1
1(t)B21(q)1(0)
d(0)B21(q)1(0)
d(0)+B21(0) +2:
14Meanwhile, we estimate the gap between matrix  1
t+1and matrix  1
tas follow
 1
t+1  1
t= ( 0+ (Gs;t+ 2As;tA>
s;t) 1) 1 (0+G 1
s;t) 1
=  1
0  1
0(~ 1
s;t+ 2As;tA>
s;t) 1 1
0 ( 1
0  1
0~s;t 1
0)
=  1
0~s;t (~ 1
s;t+ 2As;tA>
s;t) 1
 1
0
=  1
0~1
2
s;t
I (I+ 2~1
2
s;tAs;tA>
s;t~1
2
s;t) 1~1
2
s;t 1
0
=  1
0~1
2
s;t
I (I  2~1
2
s;tAs;tA>
s;t~1
2
s;t
1 + 2A>
s;t~s;tAs;t)~1
2
s;t 1
0
= 1
0~s;tAs;tA>
s;t~s;t 1
0
2+A>
s;t~s;tAs;t
 1
0~s;tAs;tA>
s;t~s;t 1
0
2+B21(0) +B21(q)1(0)=d(0); (5)
where the second equality holds due to the Woodbury matrix identity, and the ﬁfth equality
holds due to the Sherman-Morrison formula. Then analogous to the proof for (1) BoundingP
t1P
s2StkAs;tk2
~s;t, recallc2=
2+B21(0) +B21(q)(0)
we have
X
t1X
s2StA>
s;t~s;t 1
0t 1
0~s;tAs;t
2c2X
t1X
s2Stlog 
1 +A>
s;t~s;t 1
0t 1
0~s;tAs;t
2+B21(0) +B21(q)(0)
=2c2X
t1X
s2Stlog det 
I+1
2
t 1
0~s;tAs;tA>
s;t~s;t 1
01
2
t
2+B21(0) +B21(q)(0)
=2c2X
t1X
s2St
log det  1
t+ 1
0~s;tAs;tA>
s;t~s;t 1
0
2+B21(0) +B21(q)(0)
 log det  1
t
2c2X
t1X
s2St
log det  1
t+1
 log det  1
t
2c2
log det  1
mn+1
 log det  1
1
=2c2log det 
I+X
s2[m]1
2q(0+G 1
s;mn +1) 11
2q
2dc2logTr 
I
+ Tr P
s2[m]1
2q(0+G 1
s;mn +1) 11
2q
d
2dc2log (1 +mTr 
 1
0q
d);
where the second inequality holds due to Eq. (5). Combining (1)and(2)ﬁnishes the whole proof. 
Remark A.2 Actually, we can replace the term Tr( 1
0q)in the above regret bound with
O(n1(q)), at the cost of a slightly larger regret upper bound, by bounding (0+G 1
s;mn +1) 1
Gs;mn +1in the last but one step in the above (2), instead of bounding (0+G 1
s;mn +1) 1 1
0.
Speciﬁcally, we have the following estimation:
logTr(I+P
s2[m]1
2q(0+G 1
s;mn +1) 11
2q)
d
logTr(I+P
s2[m]1
2qGs;mn +11
2q)
d
15= log
1 +Tr( 2P
s2[m]P
`<mn +11[s2S`]A>
s;`qAs;`)
d
log
1 + 2mn 1(q)
d
;
which isO(log (mn)), slightly larger than the regret bound of O(log (m))in our Proposition A.1.
Then we can begin proving our ﬁrst Bayes regret bound for HierTS in the multi-task Gaussian linear
bandit setting. We ﬁrst give a lemma as follow, which is useful to prove our multi-task Bayes regret
bound in the sequential setting.
Lemma A.2 (Proposition 2 in [ 21]) LetX1andX2be arbitrary i.i.d. Rmvalued random variables
andf1;f2measurable maps such that f1;f2:Rm!RdwithEkf1(X1)k2
2,Ekf2(X1)k2
2<1,
thenjE[f1(X1)>f2(X1)]jp
dE[(f1(X1)>f2(X2))2].
Theorem A.1 (Theorem 5.1 in the main text). Let jStj= 1 for all rounds t1. Then in the
multi-task Gaussian linear bandit setting, the Bayes regret upper bound of HierTS is as follow:
BR(m;n)p
mnds
2mdc 1log (1 +n
d) + 2dc2log (1 +mTr(q 1
0)
d)
Proof. Recall thatHt= (Hs;`)`<t;s2S`is the history up to round t, then
BR(m;n) =EhX
t1X
s2StE[>
s;As; >
s;As;tjHt]i
=EhX
t1X
s2StE
>
s;As; E[s;jHt]>E[As;tjHt]Hti
=EhX
t1X
s2StE
(s; ^s;t)>As;Hti
EhX
t1X
s2Stq
dE 
(s; ^s;t)>As;t2Hti
=EhX
t1X
s2Stq
dE
A>
s;t(s; ^s;t)(s; ^s;t)>As;tHti
=EhX
t1X
s2Stq
dE
A>
s;t^s;tAs;ti
p
mnds
EX
t1X
s2StkAs;tk2
^s;t;
where both the second and the ﬁfth equality hold due to the independence between As;tands;
conditioned on Ht; the ﬁrst inequality holds by applying Lemma A.2 with functions f1(y1;y2) =y1,
f2(y1;y2) =y2for anyy1;y22Rd, and the random variable X1= (s; ^s;t;As;)jHt,X2(with
the second element as As;t) is the i.i.d. copy of X1; the second inequality holds due to Jensen’s
inequality. Plugging the upper bound over EP
t1P
s2StkAs;tk2
^s;tin Proposition A.1 into the
above result obtains the Bayes regret bound for HierTS. 
B Proofs for Regret Bound of HierBayesUCB in the Sequential Bandit
Setting
Lemma B.1 LetjN (;0)andH= (xt;Yt)n
t=1benobservations generated as Ytj
;xtN(x>
t;2). LetP(jH) =N(; ;), andG= 2Pn
t=1xtx>
t. Then
E[jH] = ( 1
0+G) 1( 1
0+B);cov[jH] =( 1
0+G) 1+ ( 1
0+G) 1 1
0 1
0( 1
0+G) 1:
16Proof. By deﬁnition, we have cov[j;H] = ( 1
0+G) 1,E[j;H] = cov[j;H]( 1
0+
B)whereB= 2Pn
t=1xtYt. Hence cov[j;H]does not depend on . Then we have
E[jH] =E[E[j;H]jH] = cov[j;H]( 1
0E[jH] +B) = ( 1
0+G) 1( 1
0+B):
On the other hand, because cov[j;H]does not depend on ,E[cov[j;H]jH] = cov[j
;H]. In addition, since Bis a constant conditioned on H, then according to [ 17, Lemma 2], we
have the following result:
cov[E[j;H]jH] = cov[cov[ j;H] 1
0jH] = ( 1
0+G) 1 1
0 1
0( 1
0+G) 1:
Theorem B.1 (Theorem 5.2 in the main text). Suppose the action set Ais ﬁnite withjAj<1. Let
jStj= 1for all rounds t1. Then in the multi-task Gaussian linear bandit setting, the Bayes regret
upper bound of Hierarchical BayesUCB is as follow:
BR(m;n)mn + 4Bq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1
mnjAj
+E8 log1


minn
2mdc 1log 
1 +n
d
+ 2dc2log (1 +mTr 
 1
0q
d)o
:
In Theorem 5.2 in the main text, we replacer
d+q
8dln1
in the right hand side of the above
inequality withp
dfor ease of exposition.
Proof. Deﬁne s;t=>
s;As; >
s;As;t, the eventEs;t=f8a2A :ja>(s; ^s;t)jq
2 log1
kak^s;tg, andCt;s;a=q
2 log1
kak^s;t. Then we can rewrite the multi-task Bayes regret
BR(m;n)as the following equivalent form:
EX
t1X
s2Sts;t
=X
t1X
s2StE
s;t1fs;t;Es;tg
+X
t1X
s2StE
s;t1fs;t<;Es;tg
+X
t1X
s2StE
s;t1fEs;tg
:
Then we will bound the three terms in the RHS of the above equality respectively.
(1) BoundingP
t1P
s2StE
s;t1fs;t;Es;tg
.Recall that Ut;s;a =a>^s;t+q
2 log1
kak^s;t, then we have
X
t1X
s2StE
s;t1fs;t;Es;tg
=X
t1X
s2StE
E 
>
s;As; >
s;As;t2
s;t1fs;t;Es;tgHt
X
t1X
s2StE
E 
>
s;As; Ut;s;As;+Ut;s;As;t >
s;As;t2
s;t1fs;t;Es;tgHt
X
t1X
s2StE
E 
Ut;s;As;t >
s;As;t2
s;t1fs;t;Es;tgHt
X
t1X
s2StE4C2
t;s;As;t

min
=X
t1X
s2StE(8 log1
)kAs;tk2
^s;t

min
E8 log1


minn
2mdc 1log 
1 +n
d
+ 2dc2log (1 +mTr 
 1
0q
d)o
:
where the ﬁrst inequality holds due to the fact that Ut;s;As;tUt;s;As;in the BayesUCB algorithm;
the second inequality holds because when event Es;toccurs,>
s;As;Ut;s;As;the third inequality
17holds due to the deﬁnition of Ut;s;a; and the last inequality due to the result in Proposition A.1.
(2) BoundingP
t1P
s2StE
s;t1fs;t< ;E s;tg
. We trivially haveP
t1P
s2StE
s;t1fs;t<;Es;tg
mn .
(3) BoundingP
t1P
s2StE
s;t1fEs;tg
.
First we give an upper bound of s;t=>
s;(As; As;t). Using Schwartz’s inequality, we have
>
s;(As; As;t)ks;k^ 1
s;1kAs; As;tk^s;1
2Bq
1(^s;1)ks;k^ 1
s;12Bq
1(^s;1)
ks; qk^ 1
s;1+kqk^ 1
s;1
:
Besides, we also have s; q=s; + qN (0;0+ q) =N(0;^s;1), then
E
ks; qk^ 1
s;1
q
Ek^ 1
2
s;1(s; q)k2
2=p
d. According to [ 35, Exp 2.11], we have with
probability 1 ,ks; qk^ 1
s;1r
d+q
8dln1
. Therefore, with probability 1 over the
draw offs;gs2[m], we have
X
t1X
s2StE
s;t1fEs;tg
=X
t1X
s2StE
E
s;t1fEs;tgHt
2Bq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1X
t1X
s2StE
1fEs;tgHt
=2Bq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1X
t1X
s2StP Es;tHt
4Bq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1
mnjAj;
where the last inequality holds because P Es;tHt
2. Combining (1),(2)and(3), we achieve
the ﬁnal Bayes regret bound for any 2(0;1),>0,2(0;1):
BR(m;n)mn + 4Bq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1
mnjAj
+E8 log1


minn
2mdc 1log 
1 +n
d
+ 2dc2log (1 +mTr 
 1
0q
d)o
:
C Proofs for Regret Bounds of HierTS and HierBayesUCB in the
Concurrent Bandit Setting
LetCt=fs2St:d(Gs;t)=2gbe the set of sufﬁciently-explored tasks at round t. We ﬁrst
give the following proposition to bound the posterior variance EP
t1P
s2St1fs2CtgkAs;tk2
^s;t
in the concurrent setting. Analogous to the proof for Proposition A.1, we choose to give the worst-case
upper bound onP
t1P
s2St1fs2CtgkAs;tk2
^s;tas follow.
Proposition C.1 Letc1=2+B21(0),c2=2+B21(0) +B21(q)(0),c3= 1 +
B2 2(0)
1(0) +2=
, then we have
X
t1X
s2St1fs2CtgkAs;tk2
^s;t2mdc 1log (1 +n
d) + 2dc2c3log (1 +mTr 
 1
0q
d):
18Proof. Note that we have
X
t1X
s2St1fs2CtgkAs;tk2
^s;t=X
t1X
s2St1fs2CtgA>
s;t ~s;t+~s;t 1
0t 1
0~s;t
As;t:
On eventfs2Ctg, the modiﬁed HierTS samples from the posterior and actually behaves the same as
the original HierTS algorithm in Algorithm 1. Then we bound the two terms in the right hand side of
the above equality respectively .
(1) BoundingP
t1P
s2St1fs2CtgA>
s;t~s;tAs;t
Similar to the proof for Theorem A.1, we have A>
s;t~s;tAs;tB21(0) +2and ~Xs;t,
( 1
0+1
B21(0)+2P
`<t1fs2StgAs;`A>
s;`) 1~s;t. Then we can analogously obtain
X
t1X
s2St1fs2CtgA>
s;t~s;tAs;t
2
2+B21(0)X
t1X
s2St1fs2Ctglog 
1 +A>
s;t~s;tAs;t
2+B21(0)
=2
2+B21(0)X
t1X
s2St1fs2Ctg
log det ~X 1
s;t+As;tA>
s;t
2+B21(0)
 log det ~X 1
s;t
2
2+B21(0)X
t1mX
s=11fs2Ctg
log det ~X 1
s;t+As;tA>
s;t
2+B21(0)
 log det ~X 1
s;t
2
2+B21(0)mX
s=1X
t11fs2Stg
log det ~X 1
s;t+As;tA>
s;t
2+B21(0)
 log det ~X 1
s;t
=2
2+B21(0)mX
s=1
log det ~X 1
s;mn +1
 log det ~X 1
s;1
2d
2+B21(0)mX
s=1log 
1 +P
t11fs2StgA>
s;t0As;t
d(2+B21(0))
2md
2+B21(0)
log 
1 +n
d
= 2mdc 1log 
1 +n
d
;
where the second inequality holds due to the fact that, if square matrix AB0, then det (A)
det (B), andjStjm; the last inequality holds because the agent interact with each task at most n
times.
(2) BoundingP
t1P
s2St1fs2CtgA>
s;t ~s;t 1
0t 1
0~s;t
As;t
Analysis . The real difference of the proof for the concurrent regret from the sequential regret lies in
boundingP
t1P
s2St1fs2CtgA>
s;t ~s;t 1
0t 1
0~s;t
As;t, becausejStj1and the result in
Eq. (5) (which only holds for the case jStj= 1) does not hold. To tackle this difference, we reduce the
concurrent setting to the sequential setting. Let St=fItjgjStj
j=1and deﬁneSt;1:i=fItjgi
j=1. Then at
roundt, lets=Itiand deﬁne  1
s;t, 1
q+P
z2St;1:i 1(0+G 1
z;t+1) 1+P
z2[m]nSt;1:i 1(0+
G 1
z;t) 1, we estimate the gap between  1
s;tand 1
tas follow:
 1
s;t  1
t=X
z2St;1:i 1h
(0+G 1
z;t+1) 1 (0+G 1
z;t)i
=X
z2St;1:i 1h
(0+ (Gz;t+Az;tA>
z;t
2) 1) 1 (0+G 1
z;t)i
=X
z2St;1:i 1 1
0~s;tAz;tA>
z;t
2+A>
z;t~z;tAz;t~s;t 1
0:
Thus we can bound 1( 1
s;t  1
t)and tackle  1
s;tinstead of  1
tto reduce the concurrent setting
to the sequential setting. We ﬁrst give a useful lemma as follow.
19Lemma C.1 For any ﬁxed t1andi2[L], supposed(Gs;t)=2and lets=It;i. Then
1( 1
s;tt)1 +B21(0)
2d(0)
1(0) +2

:
Proof. Applying Weyl’s inequality, we have
1 
( 1
s;t  1
t)t+I
=1 1
2
t( 1
s;t  1
t)1
2
t+I
1 +1( 1
s;t  1
t)1(t) = 1 +1( 1
s;t  1
t)
d( 1
t)
We ﬁrst lower bound d( 1
t). According to Weyl’s inequality, we have
d( 1
t)d( 1
q) +X
z2[m]d 
(0+G 1
z;t) 1
d( 1
q) +X
z2[m]1
1(0) +1(G 1
z;t)
=d( 1
q) +X
z2[m]1
1(0) +1
d(Gz;t)d( 1
q) +i 1
1(0) +2=;
where the last inequality holds because the tasks St;1:i 1have been sufﬁciently explored. On the
other hand, using our Lemma A.1 we can bound 1( 1
s;t  1
t)as follow
1( 1
s;t  1
t)X
z2St;1:i 11(Az;tA>
z;t
2+A>
z;t~z;tAz;t)1( 1
0~s;t~s;t 1
0)(i 1)B2
21(0)
d(0):
Combining the above results, we have
1( 1
s;tt)1 +(i 1)B2
21(0)
d(0)
d( 1q) +i 1
1(0)+2=1 +B2
21(0)
d(0)
1(0) +2

:
Then, recall c3= 1 +B2 2(0)
1(0) +2=
, we can boundP
t1P
s2St1fs2
CtgA>
s;t ~s;t 1
0t 1
0~s;t
As;tas follows:
X
t1X
s2St1fs2CtgA>
s;t ~s;t 1
0t 1
0~s;t
As;t
=X
t1X
s2St1fs2CtgA>
s;t~s;t 1
01
2
s;t  1
2
s;tt 1
2
s;t1
2
s;t 1
0~s;tAs;t
X
t1X
s2St1fs2Ctg1  1
2
s;tt 1
2
s;t
A>
s;t~s;t 1
01
2
s;t1
2
s;t 1
0~s;tAs;t
n
1 +B2
21(0)
d(0)
1(0) +2
o
EX
t1X
s2St1fs2CtgA>
s;t~s;t 1
0s;t 1
0~s;tAs;t
2c3c2
log det  1
mn+1
 log det  1
1
2dc3c2log (1 +mTr 
 1
0q
d);
where the second inequality holds due to Lemma C.1, the third and the fourth inequality hold in the
same way as that in the proof of Bounding (2) in Proposition A.1. Combining the results in (1)and
(2)ﬁnishes the whole proof. 
Remark C.1 In the last step of proof for Lemma C.1, we bound
1( 1
s;tt)1 +(i 1)B2
21(0)
d(0)
d( 1q) +i 1
1(0)+2=1 +(i 1)B2
21(0)
d(0)
i 1
1(0)+2=:
Thus our upper bound is independent of the number Lof the concurrent tasks. Actually, 8i2[L]:
1( 1
s;tt)1 +B2
21(0)
d(0)
d( 1
q)
(i 1)+1
1(0)+2=1 +B2
21(0)
d(0)
d( 1
q)
L+1
1(0)+2=;
20the sharper bound 1 +B2
21(0)
d(0)
d( 1q)
L+1
1(0)+2=isL-dependent. Ifd( 1
q)
L<<1
1(0)+2=, the
inﬂuence ofLto the regret may be large; otherwise, the inﬂuence of Lto the regret may be negligible.
Next, we prove the concurrent regret bound for HierTS and HierBayesUCB.
Theorem C.1 (Theorem 5.3 in the main text). Let jStjLmfor all rounds t1. Then in the
multi-task Gaussian linear bandit setting, the Bayes regret bound of HierTS is as follow:
BR(m;n)2Bmdq
1(0+ q)(p
d+kqk^ 1
s;1) +mdr
2nc1log (1 +n
d)
+p
mnds
2dc2c3log (1 +mTr 
 1
0q
d):
Proof. Recall thatCt=fs2St:d(Gs;t)=2gis the set of sufﬁciently-explored tasks at round
t. Then, due to the modiﬁcation of HierTS algorithm, we decompose Bayes regret BR(m;n)into
two terms and bound them respectively:
BR(m;n) =EX
t1X
s2St1fs =2Ctg>
s;(As; As;t) +EX
t1X
s2St1fs2Ctg>
s;(As; As;t)
(1) Bounding EP
t1P
s2St1fs =2Ctg>
s;(As; As;t). Similar to the proof for Theorem B.1
(3), we have
>
s;(As; As;t)ks;k^ 1
s;1kAs; As;tk^s;12cq
1(^s;1)
ks; qk^ 1
s;1+kqk^ 1
s;1
;
andE
ks; qk^ 1
s;1
q
Ek^ 1
2
s;1(s; q)k2
2=p
d. Recalling the independence between s;
and actionsAs;tyields
EX
t1X
s2St1fs =2Ctg>
s;(As; As;t)
2Bq
1(0+ q)EX
t1X
s2St1fs =2CtgE
ks; qk^ 1
s;1+kqk^ 1
s;1
2Bq
1(0+ q)(p
d+kqk^ 1
s;1)md;
The last inequality holds because in the modiﬁed HierTS, event fs =2Ctgoccurs at most dtimes for
any tasks2[m].
(2) Bounding EP
t1P
s2St1fs2CtgkAs;tk2
^s;t. It sufﬁces to apply the upper bound in Proposi-
tion C.1.
Combining the upper bounds in steps (1)and(2)obtains the ﬁnal Bayes regret bound for HierTS in
the concurrent setting. 
Theorem C.2 (Logarithmic Regret Bound for HierBayesUCB in the Concurrent Bandit Setting).
Suppose the action set Ais ﬁnite withjAj<1. LetjStjLmfor all rounds t1. Then in the
multi-task Gaussian linear bandit setting, the Bayes regret bound of HierTS is as follow:
BR(m;n)mn + 4Bq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1
mnjAj
+ 2Bq
1(0+ q) p
d+kqk^ 1
s;1
md+E8 log1


minn
2mdc 1log (1 +n
d) + 2dc3c2log (1 +mTr 
 1
0q
d)o
:
Proof. Similar to the proof of Theorem C.1, we decompose the Bayes regret as BR(m;n) =
EP
t1P
s2St1fs =2Ctg>
s;(As; As;t) +EP
t1P
s2St1fs2Ctg>
s;(As; As;t). Then
21Table 3: Different Bayes regret bounds for multi-task d-dimensional linear bandit problem in the
concurrent setting. mis the number of tasks, nis the number of iterations per task, Ais the action
set.Bayes Regret Bound =Bound I +Bound II +Negligible Terms , where Bound I is the regret
bound for solving mtasks, Bound II the regret bound for learning hyper-parameter .
Bayes Regret Bound jAj Bound I Bound II
[17, Theorem 4] InﬁniteO 
mdp
nlog (n
d) log (mn)
O 
dp
mnlog (m) log (mn)
Our Theorem 5.3 Inﬁnite O 
mdp
nlog (n
d)
O 
dp
mnlog (m
d)
Our Theorem C.2 FiniteO 
mdlog (n
d) log (mn)
O 
dlog (m
d) log (mn)
we bound the ﬁrst term with the proof for Theorem C.1 (1), bound the second term with the proof for
our Theorem B.1. Then with probability 1 over the draw offs;gs2[m],
BR(m;n)2Bq
1(0+ q)(p
d+kqk^ 1
s;1)md+E[8 log1


min]X
t1X
s2StE
1fs2CtgkAs;tk2
^s;t
+mn + 4Bq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1
mnjAj:
Plugging the upper bound onP
t1P
s2StE
1fs2CtgkAs;tk2
^s;t
in Proposition C.1 into the right
hand side of the above inequality ﬁnishes the whole proof. 
D Proofs for Regret Bounds of HierTS and HierBayesUCB in the
Semi-Bandit Setting
We also choose to give the worst-case upper bound onP
t1P
s2StP
a2As;t>
a^s;taas follow.
Proposition D.1 Letc1=2+B21(0),c4=2+B2L1(0) +B21(q)(0), then
X
t1X
s2StX
a2As;t>
a^s;ta2c1mlog (1 +nL
d) + 2c4Ldlog(1 +mTr( 1
0q)
d):
Proof. Recall that Gs;t= 2P
`<t1fs2 S`g(P
a2As;ta>
a),Bs;t= 2P
`<t1fs2
S`g(P
a2As;taws(a)),~ 1
s;t=  1
0+Gs;t, 1
t=  1
q+P
s2[m](0+G 1
s;t) 1. Then, analogous
to the proof for Proposition A.1, we introduce the matrix ~Xs;t, 
 1
0+1
B21(0)+2P
`<t1fs2
S`g(P
a2As;ta>
a) 1. We next boundP
t1P
s2StP
a2As;t>
a~s;taandP
t1P
s2StP
a2As;t>
a~s;t 1
0t 1
0~s;ta.
(1) BoundingP
t1P
s2StP
a2As;t>
a~s;ta.
X
t1X
s2StX
a2As;t>
a~s;ta
=[2+B21(0)]X
t1mX
s=11fSt=sgX
a2As;t>
a~s;ta
2+B21(0)
2[2+B21(0)]X
t1mX
s=11fSt=sgX
a2As;tlog (1 +>
a~s;ta
2+B21(0))
2[2+B21(0)]X
t1mX
s=11fSt=sgX
a2As;tlog det(I+~1
2
s;ta>
a~1
2
s;t
2+B21(0))
22=2[2+B21(0)]mX
s=1X
t1X
a2As;t1fSt=sg
log det( ~X 1
s;t+a>
a
2+B21(0)) log det( ~X 1
s;t)
=2[2+B21(0)]mX
s=1
log det( ~X 1
s;mn +1) log det( ~X 1
s;1)
=2[2+B21(0)]mX
s=1log det 
I+1
2+B21(0)X
tmn1fSt=sgX
a2As;t^1
2
s;ta>
a^1
2
s;t
=2[2+B21(0)]mX
s=1logTr 
I+1
2+B21(0)P
tmn1fSt=sgP
a2As;t^1
2
s;ta>
a^1
2
s;t
d
=2[2+B21(0)]mX
s=1log 
1 +P
tmn1fSt=sgP
a2As;t>
a^s;ta
d(2+B21(0))
2[2+B21(0)]mlog (1 +nL
d) = 2c1mlog (1 +nL
d):
(2) BoundingP
t1P
s2StP
a2As;t>
a~s;t 1
0t 1
0~s;ta.
8t1;s2St,8a2As;t, we have >
a~s;t 1
0t 1
0~s;taB21(q)(0) +LB21(0) +
2. Meanwhile, deﬁne the matrix M,
~1
2
s;ta1;~1
2
s;ta2;:::; ~1
2
s;tajAs;tj
2RdjAs;tj, we have
P
a2As;t~1
2
s;ta>
a~1
2
s;t=MM>. Using the Wely’s inequality, we further have
1(I+ 2M>M)1(I) +1( 2MM>)1 + 2X
a2As;t1(~1
2
s;ta>
a~1
2
s;t) = 1 + 2X
a2As;t>
a~s;ta:
Then we can estimate the gap between matrix  1
t+1and 1
tas follow:
 1
t+1  1
t
= 
0+ (Gs;t+ 2X
a2As;ta>
a) 1 1  
0+G 1
s;t 1
= 1
0  1
0( 1
0+Gs;t+ 2X
a2As;ta>
a) 1 1
0 [ 1
0  1
0( 1
0+Gs;t) 1 1
0]
= 1
0[~s;t (~ 1
s;t+ 2X
a2As;ta>
a) 1] 1
0
= 1
0~1
2
s;t
I (I+ 2X
a2As;t~1
2
s;ta>
a~1
2
s;t) 1~1
2
s;t 1
0
= 1
0~1
2
s;t
I (I+ 2MM>) 1~1
2
s;t 1
0
= 1
0~1
2
s;t
 2M(I+ 2M>M) 1M>~1
2
s;t 1
0
 1
0~1
2
s;t
 2Md 
(I+ 2M>M) 1
M>~1
2
s;t 1
0
= 1
0~1
2
s;t
 2 1
1(I+ 2M>M)MM>~1
2
s;t 1
0
 1
0~1
2
s;t
 2 1
1 + 2P
a2As;t>a~s;taMM>~1
2
s;t 1
0
= 1
0~s;t P
a2As;ta>
a~s;t 1
0
2+P
a2As;t>a~s;ta
 1
0~s;t P
a2As;ta>
a~s;t 1
0
2+B2L1(0) +B21(q)(0); (6)
23where the second and the sixth equality hold due to the Woodbury matrix identity. The proof for Eq. (6)
is similar to the proof for Eq. (5), but requires more reﬁned analysis (i.e. the ﬁrst inequality in Eq. (6))
to estimate the lower bound of  1
t+1  1
t. Then recall c4=2+B2L1(0) +B21(q)(0)
for brevity, we can boundP
t1P
s2StP
a2As;t>
a~s;t 1
0t 1
0~s;taas follow:
X
t1X
s2StX
a2As;t>
a~s;t 1
0t 1
0~s;ta
2c4X
t1X
s2StX
a2As;tlog (1 +>
a~s;t 1
0t 1
0~s;ta
2+B2L1(0) +B21(q)(0))
=2c4X
t1X
s2StX
a2As;tlog det(I+1
2
t 1
0~s;ta>
a~s;t 1
01
2
t
2+B2L1(0) +B21(q)(0))
=2c4X
t1X
s2StX
a2As;t
log det(  1
t+ 1
0~s;ta>
a~s;t 1
0
2+B2L1(0) +B21(q)(0)) log det(  1
t)
2c4LX
t1X
s2St
log det(  1
t+1) log det(  1
t)
=2c4L
log det(  1
mn+1) log det(  1
1)
=2c4L
log det(I+X
s2[m]1
2q(0+G 1
s;mn +1) 11
2q)
2c4Ld
logTr(I+P
s2[m]1
2q(0+G 1
s;mn +1) 11
2q)
d
2c4Ldlog(1 +mTr( 1
0q)
d);
where the second inequality holds due to Eq. (6). 
Lemma D.1 If a Gaussian random variable X N (;2), then E[X1fX0g] =
1 
G( 
)
+p
2expf 2
22g. If further0, thenE[X1fX0g] =p
2expf 2
22g.
Theorem D.1 (Theorem 5.4 in the main text, Regret Bound of HierTS in the Semi-Bandit Setting).
LetjStj= 1 for anyt1. Letcq
2 ln nKB 1(0)p
2
,c1=2+B21(0),c4=2+
B2L1(0)+B21(q)(0), then in the multi-task Gaussian semi-bandit setting, the Bayes regret
bound of combinatorial HierTS is:
BR(m;n)m+cp
mnLs
2c1mlog (1 +nL
d) + 2c4Ldlog(1 +mTr( 1
0q)
d):
Proof. Note that ws= s;, then deﬁne g(A;) =P
a2Aha;ifor brevity, we have the following
result:
BR(m;n) =EX
t1X
s2StX
a2As;ws(a) X
a2As;tws(a)
=EX
t1X
s2StX
a2As;ha;s;i X
a2As;tha;s;i
=EX
t1X
s2St
g(As;;s;) g(As;t;s;)
:
Deﬁne upper conﬁdence bound Ut;s(A) =P
a2A
ha;^s;ti+cq
>a^s;ta
, wherecis a constant
to be speciﬁed. Notice that As;jHti.i.d.As;tjHtandUt;s()is a deterministic function, thus
24E[Ut;s(As;)jHt] =E[Ut;s(As;t)jHt]. Then we can decompose Bayes regret BR(m;n)as follow:
BR(m;n) =EX
t1X
s2StE
g(As;;s;) Ut;s(As;) +Ut;s(As;t) g(As;t;s;)jHt
=EX
t1X
s2St
g(As;;s;) Ut;s(As;)
+EX
t1X
s2St
Ut;s(As;t) g(As;t;s;)
:
(1) Bounding EP
t1P
s2St
g(As;;s;) Ut;s(As;)
.
For anyt1,s2St,a2A, deﬁne random variable Xt;s;a=ha;s; ^s;ti cq
>a^s;ta,
then we have Xt;s;ajHtN( cq
>a^s;ta;>
a^s;ta)sinceE[s; ^s;tjHt] =0. Then
EX
t1X
s2St
g(As;;s;) Us;t(As;)
=EX
t1X
s2StX
a2As;Xt;s;a
EX
t1X
s2StX
a2As;Xt;s;a1fXt;s;a0g
EX
t1X
s2StX
a2[K]Xt;s;a1fXt;s;a0g
=EX
t1X
s2StX
a2[K]E
Xt;s;a1fXt;s;a0gjHt
EX
t1X
s2StX
a2[K]q
>a^s;tap
2expf c2
2g
EX
t1X
s2StX
a2[K]B1(^s;t)p
2expf c2
2g
nmKB1(0)p
2expf c2
2g:
If letnmKB1(0)p
2expf c2
2gm, thencq
2 ln nKB 1(0)p
2
.
(2) Bounding EP
t1P
s2St
Ut;s(As;t) g(As;t;s;)
.
EX
t1X
s2St
Ut;s(As;t) g(As;t;s;)
=EX
t1X
s2StX
a2As;tha;^s;t s;i+cq
>a^s;ta
=EX
t1X
s2StX
a2[K]E
1a2As;tjHt
E
ha;^s;t s;ijHt] +cEX
t1X
s2StX
a2As;tq
>a^s;ta
=cEX
t1X
s2StX
a2As;tq
>a^s;ta
cp
mnLs
EX
t1X
s2StX
a2As;t>a^s;ta;
where the second equality holds because of the mutual independence between As;tjHtands;jHt,
andE[^s;t s;jHt] =0; the last inequality holds due to the Jensen inequality. Then applying
25Proposition D.1 to bound EP
t1P
s2StP
a2[As;t]>
a^s;ta, we can obtain
EX
t1X
s2St
Ut;s(As;t) g(As;t;s;)
cp
mnLs
2c1mlog (1 +nL
d) + 2c4Ldlog(1 +mTr( 1
0q)
d):
Combining the above results ﬁnishes the whole proof. 
Theorem D.2 (Theorem 5.5 in the main text, Regret Bound of HierBayesUCB in the Semi-Bandit
Setting). LetjStj= 1 for all rounds t1. Letc1=2+B21(0),c4=2+B2L1(0) +
B21(q)(0), Then for any >0;2(0;1),2(0;1), in the multi-task Gaussian semi-bandit
setting, the Bayes regret upper bound of combinatorial HierBayesUCB is as follow:
BR(m;n)E8Llog1


min
2c1mlog (1 +nL
d) + 2c4Ldlog(1 +mTr( 1
0q)
d)
+mn
+ 4LBq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1
mnK:
In Theorem 5.5 in the main text, we replacer
d+q
8dln1
in the right hand side of the above
inequality withp
dfor ease of exposition.
Proof. Deﬁne the event Es;t=f8a2A:j>
a(s; ^s;t)jq
2 log1
kak^s;tg, and the upper
conﬁdence bound Ut;s(A) =P
a2Aha;^s;ti+q
2 log1
kak^s;t. Let s;t=g(As;;s;) 
g(As;t;s;), then we decompose the Bayes regret into three parts as follow:
EX
t1X
s2Sts;t
=X
t1X
s2StE
s;t1fs;t;Es;tg
+X
t1X
s2StE
s;t1fs;t<;Es;tg
+X
t1X
s2StE
s;t1fEs;tg
:
(1) BoundingP
t1P
s2StE
s;t1fs;t;Es;tg
.
X
t1X
s2StE
s;t1fs;t;Es;tg
=X
t1X
s2StE 
g(As;;s;) g(As;t;s;)2
s;t1fs;t;Es;tg
X
t1X
s2StE 
g(As;;s;) Ut;s(As;) +Ut;s(As;t) g(As;t;s;)2
s;t1fs;t;Es;tg
X
t1X
s2StE 
Ut;s(As;t) g(As;t;s;)2
s;t1fs;t;Es;tg
=X
t1X
s2StE P
a2As;tha;^s;t s;i+q
2 log1
kak^s;t2
s;t1fs;t;Es;tg
X
t1X
s2StE P
a2As;t2q
2 log1
kak^s;t2
s;t1fs;t;Es;tg
X
t1X
s2StE P
a2As;t8 log1
 P
a2As;tkak2
^s;t
s;t1fs;t;Es;tg
26E8Llog1


minX
t1X
s2StX
a2As;tkak2
^s;t
;
where the ﬁrst and the second inequality hold due to the deﬁnition of the upper conﬁdence bound
Ut;s(As;t), the fourth inequality holds due to the Cauchy-Schwartz inequality. Utilizing the upper
bound onP
t1P
s2StP
a2As;tkak2
^s;tin Proposition D.1 completes the proof for the ﬁrst part.
(2) BoundingP
t1P
s2StE
s;t1fs;t<;Es;tg
.
We trivially haveP
t1P
s2StE
s;t1fs;t<;Es;tg
mn .
(3) BoundingP
t1P
s2StE
s;t1fEs;tg
.
Note thats; qN(0;^s;1), andE
ks; qk^ 1
s;1
q
Ek^ 1
2
s;1(s; q)k2
2=p
d. Then
according to [ 35, Exp 2.11], we have with probability 1 ,ks; qk^ 1
s;1r
d+q
8dln1

Therefore, with probability 1 over the draw offs;gs2[m], we have
s;t=g(As;;s;) g(As;t;s;)
=X
a2As;ha;s;i X
a2As;tha;s;i
X
a2As;kakks;k+X
a2As;tkakks;k
2LBks;k
1 
2LBq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1
;
where the ﬁrst inequality holds due to the Schwartz inequality. Then we have with probability 1 :
X
t1X
s2StE
s;t1fEs;tg
=X
t1X
s2StE
E
s;t1fEs;tgHt
2LBq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1X
t1X
s2StE
1fEs;tgHt
= 2LBq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1X
t1X
s2StP Es;tHt
4LBq
1(0+ q) s
d+r
8dln1
+kqk^ 1
s;1
mnK:
Combining the results in (1) (2) and(3)ﬁnishes the whole proof. 
E Technical Overview and Limitations of this Work
In this section, we explain our technical novelties for deriving near-optimal sequential regret bound
for HierTS and logarithmic sequential regret bound for HierBayesUCB as follow, when compared
with the latest bound in [17] (More detailed explanations can be found in Table 4):
(1) The Technical Overview for Deriving Near-Optimal Regret Bound in Theorem 5.1 . The
biggest novelty lies in bounding each term E
(s; ^s;t)>As;Ht
in Bayes regretBR(m;n).
Existing work [ 17] chose Cauchy-Schwartz inequality to directly bound (s; ^s;t)>As;
ks; ^s;tk^ 1
s;tkAs;k^s;t, used UCB technique to bound ks; ^s;tk^ 1
s;t(which caused an
additional multiplicative factor log1
), leveraged the fact that As;jHti.i.d.As;tjHtto transform
27P
t;skAs;k^s;tintoVm;n, and obtained an intermediate regret upper boundp
mnVm;nlog (1=).
Instead of using UCB technique, our Theorem 5.1 applies a novel Cauchy-Schwartz type inequality
(i.e. Lemma A.2) to bound E(s; ^s;t)>As;q
dE 
(s; ^s;t)>As;t2p
dkAs;tk^s;t,
and ﬁnally achieves the regret boundp
mnVm;n, removing thep
log (1=)factor. Besides, when
bounding the posterior variance Vm;n, we use a different matrix analysis to prevent variance terms
(e.g.2;1(0)) solely appearing in the denominator of regret bound. Moreover, we employ a
matrix decomposition technique (in our Lemma A.1) to reduce the multiplicative factor 2(0)in
[17, Theorems 3-4] to (0)in our bounds (see more details in Table 4).
(2) The Technical Overview for Deriving Logarithmic Regret Bound in Theorem 5.2 . To obtain
sharper sequential regret bound than the near-optimal regret bound in Theorem 5.1, our Theorem 5.2
chooses the Bayes regret decomposition strategy shown above Theorem 5.2, uses UCB technique
to bound the ﬁrst term in the regret decomposition asP
t;sEs;t1fs;t;Es;tgVm;nlog1
,
and ﬁnally combines the upper bound on posterior variance Vm;nin Eq. (4) to achieve a logarithmic
Bayes regret upper bound of (log1
)mdlogn
d.
Nevertheless, we also need to point out the limitations of our Bayes regret bounds:
The Limitations of the Multi-Task Bayes Regret Bounds . Honestly speaking, our regret bounds
have two main limitations, because they are: (i)Not advantageous when compared with single-task
regret bound. This is because our Bayes regret bounds (e.g. O(mpnlogn)in Theorem 5.1) for
hierarchical Bayesian bandit problem are almost the same as the summation of regret bounds of
learningmBayesian bandit task independently. This is also the limitation of existing bounds in
this ﬁeld (see [ 25,7,17]).(ii)Unable to shed more light on the advantages of multi-task bandit
optimization. The existing regret bound O(mp
nk)for multi-task representation which demonstrated
that multi-task regret bound can be smaller for learning a low-dimensional representation (i.e.
k<<d ) than the regret bound of O(mp
nd)for learning each task independently, and the existing
regret bound O(mp
nlog (1 +nV))for multi-task adversarial linear bandit which proved that the
regret bound decreases with more similarity (i.e. smaller V) among bandit tasks. Our hierarchical
Bayesian bandit model has assumed that different bandit instances are sampled the same meta-
distribution, and hence fails to reveal the inﬂuence of task similarity to the multi-task Bayes regret.
Remark E.1 (The Underlying Causes for the Limitation of Multi-Task Bayes Regret Bound.)
The underlying causes for the shortcoming of the multi-task Bayes regret bound is that the upper bound
on the posterior variance Vm;n=EP
t1P
s2StkAs;tk2
^s;tmay be not tight enough. Detailed
explanations lie in the following three aspects:
(1)Recall that in the proof for our Theorem 5.1, we can upper bound the multi-task Bayes regret as
BR(m;n)p
mndp
Vm;n:Then in Proposition A.1 we use a purely algebraic technique to bound
the posterior-variance Vm;nO(mlogn), resulting in the ﬁnal Bayes regret bound of
p
mndp
Vm;n=O(p
mndp
mlogn) =O(mp
nlogn);
which is almost the same as the summation of the regret bounds for learning mbandit tasks indepen-
dently. Therefore, if we can upper bound the posterior-variance Vm;nwith a bound that is sublinear
with respect to mand logarithmic w.r.t. n(e.g. a bound of O(pmlogn)), then the ﬁnal Bayes regret
bound will be much sharper. The upper bounds on the posterior-variance Vm;nin existing works (e.g.
see [ 25,7,17] in our Table 1 in the main text) are also obtained via purely algebraic techniques and
are not sharp either (or even worse).
(2) In the proof for Proposition A.1, we only give the worst-case upper bound onP
t1P
s2StkAs;tk2
^s;tvia purely algebraic technique (thus leading to a worst-case upper bound
on the posterior variance EP
t1P
s2StkAs;tk2
^s;t). Such worst-case upper bound is obtained
via purely algebraic techniques, ignoring the expectation over the randomness of As;tand^s;t.
Therefore, we may achieve sharper regret bound by considering the expectation over the randomness
in the posterior variance.
(3)To derive a sharper and meaningful upper bound on the posterior-variance Vm;n =
EP
t1P
s2StkAs;tk2
^s;t, we need to consider other bounding technique like concentration in-
equality, or more technical matrix analysis, to achieve an upper bound that is sublinear w.r.t. the
28numbermof tasks and sublinear w.r.t. the number nof iterations per task. Only in this way can we
obtain a multi-task regret bound o(mn)that is sublinear w.r.t. mand sublinear w.r.t. n.
(4)On the other hand, we also consider ﬁnding the lower bound of posterior-variance Vm;nto
show that our upper bound on Vm;nis tight, or ﬁnding the lower bound of multi-task Bayes regret
BR(m;n)to show that our multi-task Bayes regret upper bound could not be improved. This serves
as one of our ongoing research directions.
F Additional Experiments and Computer Resources
0 2 4 6 8 10
Number of Tasks m050100150Regret
Linear Bandit ( d= 4,σq= 1.0,L=1)
n=100
n=200
n=300
n=400
(a) Regrets w.r.t. different m
2 4 6 8 10
Number of Concurrent Tasks L050100150Regret
Linear Bandit ( m= 10,σq= 1.0)
d=8
d=4
d=2 (b) Regrets w.r.t. different L
0 100 200 300 400
Roundt050100150Regret
Linear Bandit ( d= 4,m= 10,L=5)
σq=5
σq=4
σq=3σq=2
σq=1 (c) Regrets w.r.t. different q
0 100 200 300 400
Roundt0100200300Regret
Linear Bandit ( d= 4,m= 10,L=5)
σ0=0.5
σ0=0.4
σ0=0.3
σ0=0.2
σ0=0.1
(d) Regrets w.r.t. different 0
0 100 200 300 400
Roundt02505007501000Regret
Linear Bandit ( d= 4,m= 10,L=5)
σ=10
σ=7
σ=4
σ=1
σ=0.1 (e) Regrets w.r.t. different 
0 100 200 300 400
Roundt0100200300400Regret
Linear Bandit ( d= 4,σq= 0.5)
OracleTS
TS
HierTS
HierBayesUCB (f) Regrets of different algorithms
Figure 2: Regrets of HierBayesUCB algorithm with respect to (w.r.t.) different hyper-parameters.
Experimental Results . From Figure 2, we have the similar observations as that in Figure 1: (1)In
plot (a), the multi-task regret of HierBayesUCB becomes larger with the increase of mandn, which
is consistent with our regret upper bound in Theorems 5.2. (2)In plot (b), the regret increases with
a higher dimension d, and increases with a larger number Lof the concurrent tasks. (3)In plots
(c)-(e), the regret decreases with a smaller variance (e.g. q,0and) in hierarchical Bayesian
model, validating the provable beneﬁts of variance-reduction in Bayes regret minimization. (4)
The task-averaged regret of our proposed HierBayesUCB is smaller than that of HierTS, and such
improvement becomes larger with the increase of q(when compared with q= 1:0in Figure 1 (f)).
Computer Resources . Our implementations are based on Python. We run all bandit algorithms on
a platform with 8 NVIDIA RTX 6000 GPUs and 2 AMD EPYC 7543 Processors. Each GPU has
48G memory, and each CPU has 64 cores. The CUDA version is 12.1, the Python version 3.7.16,
the matplotlib version 3.5.3, and the tensorﬂow version 1.15. The source code for reproducing all
experimental results of HierTS and HierBayesUCB is provided in the supplementary material.
29Table 4: The technical novelties for deriving our improved sequential regret bound when compared with the latest regret bound in [17, Thm 3]. mis the number of
bandit tasks, nis the number of iterations per task, and dis the dimension of action a2A.
Regret Bound [17, Thm 3] Existing Problems Improvement Motivations Our Theorem 5.1 Our Improvements
Bound Idmq
nlog (mn) log (1 +n1(0)
d2)
r
1(0)
log (1+1(0)
2)(1)There exists an
additional factorp
log (mn).(1)Use a Shwartz-type
inequality in Lemma A.2,
instead of UCB strategy,
to bound per-task regret
to avoid additional term
log1
(where=nm).
(2)Deﬁne a new matrix
~Xs;ts.t. the denominator
in the regret is
2+B21(0), not only
2. Avoid the case that
the variance serves alone
as the denominator.
(3)Give a more technical
analysis in Lemma A.1
to improve2
1(0)
2
d(0)to
1(0)
d(0)dmq
nlog 
1 +n
d
q 
2+B21(0)(1)Our regret bounds
remove thep
log (mn)
factor.
(2)To minimize our
bound, it sufﬁces
to decrease the
variances2;1(0),
1(q).
(3)Our regret bounds
also show that
we should decrease
the condition number
1(0)
d(0)
of the variance
matrix 0
to minimize the
Bayes regret.Bound IIdq
mnlog (mn) log (1 +m1(q)
d(0))
vuut2
1(0)1(q) 
1+1(0)
2
2
d(0) log 
1+2
1(0)1(q)
2
d(0)2(1)There also exists an
additional factorp
log (mn).
(2)There exists a paradox in
this bound, i.e. variance 2
exists in both the denominator
and numerator. Then
whether we should increase
or decrease2
to minimize the regret bound?dq
mnlog 
1 +mTr(q 1
0)
d
q
2+B21(0) +B21(0)1(q)
d(0)
30NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?
Answer: [Yes]
Justiﬁcation: see Section 5.
Guidelines:
The answer NA means that the abstract and introduction do not include the claims
made in the paper.
The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
The claims made should match theoretical and experimental results, and reﬂect how
much the results can be expected to generalize to other settings.
It is ﬁne to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: see Section E.
Guidelines:
The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
The authors are encouraged to create a separate "Limitations" section in their paper.
The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-speciﬁcation, asymptotic approximations only holding locally). The authors
should reﬂect on how these assumptions might be violated in practice and what the
implications would be.
The authors should reﬂect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
The authors should reﬂect on the factors that inﬂuence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
The authors should discuss the computational efﬁciency of the proposed algorithms
and how they scale with dataset size.
If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciﬁcally instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
31Justiﬁcation: see assumptions in Section 5, and see proof sketch in Section E.
Guidelines:
The answer NA means that the paper does not include theoretical results.
All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
All assumptions should be clearly stated or referenced in the statement of any theorems.
The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justiﬁcation: see implementation details in Section 6 and Section F, and see the source code
in the supplementary material.
Guidelines:
The answer NA means that the paper does not include experiments.
If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or veriﬁable.
Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
32Answer: [Yes]
Justiﬁcation: see the source code in our supplementary material.
Guidelines:
The answer NA means that paper does not include experiments requiring code.
Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justiﬁcation: see the implementation details in Section 6.
Guidelines:
The answer NA means that the paper does not include experiments.
The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Signiﬁcance
Question: Does the paper report error bars suitably and correctly deﬁned or other appropriate
information about the statistical signiﬁcance of the experiments?
Answer: [Yes]
Justiﬁcation: see our Figures 1-2.
Guidelines:
The answer NA means that the paper does not include experiments.
The authors should answer "Yes" if the results are accompanied by error bars, conﬁ-
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
the main claims of the paper.
The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
The assumptions made should be given (e.g., Normally distributed errors).
It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
33It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not veriﬁed.
For asymmetric distributions, the authors should be careful not to show in tables or
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding ﬁgures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justiﬁcation: see details in Section F.
Guidelines:
The answer NA means that the paper does not include experiments.
The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justiﬁcation: this paper is a purely theoretical paper and has no negative social impact. Be-
sides, we release the source code to implement our proposed algorithms in the supplementary
material.
Guidelines:
The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justiﬁcation: there is no societal impact of the work performed.
Guidelines:
The answer NA means that there is no societal impact of the work performed.
If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact speciﬁc
groups), privacy considerations, and security considerations.
34The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efﬁciency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justiﬁcation: the paper poses no such risks.
Guidelines:
The answer NA means that the paper poses no such risks.
Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety ﬁlters.
Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justiﬁcation: the paper does not use existing assets.
Guidelines:
The answer NA means that the paper does not use existing assets.
The authors should cite the original paper that produced the code package or dataset.
The authors should state which version of the asset is used and, if possible, include a
URL.
The name of the license (e.g., CC-BY 4.0) should be included for each asset.
For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
35If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justiﬁcation: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
The answer NA means that the paper does not release new assets.
Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
The paper should discuss whether and how consent was obtained from people whose
asset is used.
At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip ﬁle.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justiﬁcation: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
Including this information in the supplemental material is ﬁne, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justiﬁcation: the paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
We recognize that the procedures for this may vary signiﬁcantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
36