On Convergence of Adam for Stochastic Optimization
under Relaxed Assumptions
Yusu Hong
Center for Data Science
and School of Mathematical Sciences
Zhejiang University
yusuhong@zju.edu.cnJunhong Lin∗
Center for Data Science
Zhejiang University
junhong@zju.edu.cn
Abstract
In this paper, we study Adam in non-convex smooth scenarios with potential un-
bounded gradients and affine variance noise. We consider a general noise model
which governs affine variance noise, bounded noise, and sub-Gaussian noise. We
show that Adam with a specific hyper-parameter setup can find a stationary point
with a ˜O(1/√
T)rate in high probability under this general noise model where T
denotes total number iterations, matching the lower rate of stochastic first-order
algorithms up to logarithm factors. Moreover, we show that under the same setup,
Adam without corrective terms and RMSProp can find a stationary point with a
˜O(1/T+σ0/√
T)rate which is adaptive to the noise level σ0. We also provide a
probabilistic convergence result for Adam under a generalized smooth condition
which allows unbounded smoothness parameters and has been illustrated empir-
ically to capture the smooth property of many practical objective functions more
accurately.
1 Introduction
Since its introduction by [34], the Stochastic Gradient Descent (SGD): xt+1=xt−ηtgthas
achieved significant success in solving the unconstrained stochastic optimization problems:
min
x∈Rdf(x),where f(x) =Eξ[fξ(x,ξ)], (1)
where ξis a random variable, gtis the stochastic gradients and ηtis the step-size. From then on,
numerous literature focused on the convergence behavior of SGD in various scenarios. Several
studies focused on the non-convex smooth scenario where the stochastic gradient g(x)is unbiased
with affine variance noise, i.e., for some constants σ0, σ1≥0and all x∈Rd,
E[∥g(x)− ∇f(x)∥2]≤σ2
0+σ2
1∥∇f(x)∥2. (2)
Under the noise assumption (2), [3] provided an almost-sure convergence bound for SGD. [4] proved
that SGD could reach a stationary point with a O(1/√
T)rate when step-sizes are tuned by problem-
parameters such as the smooth parameter L. The theoretical result also revealed that the analysis of
SGD under (2) is not essentially different from the bounded noise case [17].
In the popular field of deep learning, a range of variants based on SGD, known as adaptive gradient
methods have emerged. These methods employ the past gradients to adaptively tune their step-
sizes and are preferred to SGD for minimizing various objective functions due to their efficiency.
Among these methods, Adam [23] has been one of the most effective methods empirically. Generally
∗The corresponding author is Junhong Lin.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Table 1: Comparison for existing Adam analyses with ours.
FCT Grad. Noise Smooth β1, β2 ϵ Conv. Rate Conv. Type
[49] ✗ Bounded Bounded L 1−β2≤cϵ2poly(1
ϵ)1
T+σ2E
[7] ✗ Bounded Bounded L β 1,t< β1, β2= 1−1
T-1√
TE
[58] ✗ Bounded - L β 2= 1−c
Tpoly(log1
ϵ)1√
TE
[35] ✗ - Finite Sum Affine L T (β1, β2)→01- - E
[10] ✗ Bounded Bounded L β 1< β2, β2= 1−1
Tpoly(log1
ϵ)1√
TE
[18] ✗ Bounded Affine L β 1= 1−c√
Tpoly(1
ϵ)1√
TE
[53] ✓ - Finite Sum Affine L β 1<√β2, β2= 1−c
T[3]-1√
TE
[42] ✗ - Finite Sum Affine (L0,L1) β1<√β2 - - E
[25] ✓ - Sub-Gaussian (L0,L1) β1= 1−c√
T1√ϵ1√
Tw.h.p.
[40] ✗ - Coordinate-wise Affine L β 1=b√β2, β2= 1−c
Tpoly(log1
ϵ)1√
TE
[20] ✓ - Coordinate-wise Affine L β 1< β2, β2= 1−1
Tpoly(log1
ϵ)1√
Tw.h.p.
Thm. 3.1 ✓ - Affine L β 1< β2, β2= 1−c
Tpoly(log1
ϵ)1√
Tw.h.p.
Thm. 4.1 ✓ - Affine (L0,L1) β1< β2, β2= 1−c
Tpoly(log1
ϵ)1√
Tw.h.p.
1[35] requires T(β1, β2) =Oβ1
βn
21−β1
1−βn
1+ 1
→0, which seems could only achieve when β1= 0 .
2Though not explicitly stated, the results in (Zhang et al., 2022) could imply convergence to the stationary point when with some calculations.
3“FCT" refers to “full corrective terms". The “Conv. rate" column presents the convergence rate omitting logarithm factors.
speaking, Adam absorbs some key ideas from previous adaptive methods such as AdaGrad [12,
37] and RMSProp [38] while adding more unique structures. It combines the exponential moving
average mechanism from RMSProp and meanwhile adds the heavy-ball style momentum [30] and
two unique corrective terms. This unique structure leads to a huge success for Adam in practical
applications but at the same time brings more challenges to the theoretical analysis.
Considering the significance of affine variance noise and Adam in both theoretical and empirical
fields, it’s natural to question whether Adam can find a stationary point at a rate comparable to SGD
under the same smooth condition and (2). Earlier researches [14, 41, 2] have shown that AdaGrad-
Norm, a scalar version of AdaGrad, can find a stationary point at the same rate as SGD, not tuning
step-sizes based on problem-parameters. Moreover, they addressed an essential challenge brought
by the correlation of adaptive step-sizes and noise from (2) which does not appear in SGD’s cases.
However, since AdaGrad-Norm applies a cumulative step-sizes mechanism which is rather different
from the exponential moving average step-sizes in Adam, the analysis for AdaGrad-Norm could not
be trivially extended to Adam. Furthermore, the coordinate-wise step-size architecture of Adam,
rather than the unified step-size for all coordinates in AdaGrad-Norm, brings more challenge when
considering (2). In affine variance noise landscape, existing literature could only ensure the Adam’s
convergence with random-reshuffling scheme under certain parameter restrictions [53, 42], or de-
duce the convergence at the expense of requiring bounded gradient assumption and using problem-
parameters to tune the step-sizes [18], both of which ignored the corrective terms. Some other works
proved convergence to a stationary point by altering the original Adam algorithm such as removing
certain corrective terms and modifying (2) to a stronger coordinate-wise variant [40, 20].
To the best of our knowledge, existing research has not yet fully confirmed the convergence of
Adam under affine variance noise. To address this gap, we conduct an in-depth analysis and
prove that Adam with the right parameter can find a stationary point in high probability. We as-
sume a milder noise model (detailed in Assumption (A3)), covering almost surely affine variance
noise, the bounded noise, and sub-Gaussian noise. We show that the convergence rate can reach at
O
poly(logT)/√
T
matching the lower rate in [1] up to logarithm factors. Our proof employs
the descent lemma over the introduced proxy iterative sequence and adopts techniques related to the
new proxy step-sizes and error decomposition. Based on this, we are able to handle the correla-
tion between stochastic gradients and adaptive step-sizes and transform the first-order term from the
descent lemma into the gradient norm.
Finally, we apply the analysis to the (L0, Lq)-smooth condition [52]. Several researchers have found
empirical evidence of objective functions satisfying (L0, Lq)-smoothness but out of L-smoothness
range, especially in large-scale language models [50, 39, 11, 8]. Theoretical analysis of adaptive
methods under this relaxed condition is more complicated and needs further nontrivial proof tech-
niques. Also, prior knowledge of problem-parameters to tune step-sizes is needed, as indicated by
the counter-examples from [41] for the AdaGrad. Existing works [13, 41] obtained a convergence
bound for AdaGrad-Norm with (2), and [25] considered Adam with sub-Gaussian noise. In this
2Algorithm 1 Adam
Input: Horizon T,x1∈Rd,β1, β2∈[0,1),m0=v0=0d,η, ϵ > 0,ϵ=ϵ1d
fors= 1,···, Tdo
Draw a new sample zsand generate gs=g(xs,zs);
ms=β1ms−1+ (1−β1)gs;
vs=β2vs−1+ (1−β2)g2
s;
ηs=ηp
1−βs
2/(1−βs
1),ϵs=ϵp
1−βs
2;
xs+1=xs−ηs·ms/ √vs+ϵs
;
end for
paper, we provide a probabilistic convergence result for Adam with the affine variance noise and the
generalized smoothness condition.
We also refer readers to see the main contributions of our works and comparisons with the existing
works in Table 1.
Notations We use [T]to denote the set {1,2,···, T}for any positive integer T,∥·∥,∥·∥1and∥·∥∞
to denote l2-norm, l1-norm and l∞-norm respectively. a∼ O(b)anda≤ O(b)denote a=C1band
a≤C2bfor some positive universal constants C1, C2, and a≤˜O(b)denotes a≤ O(b)poly(logb).
a≲bdenotes a≤ O(b).For any vector x∈Rd,x2and√xdenote coordinate-wise square and
square root respectively. xidenotes the i-th coordinate of x. For any two vectors x,y∈Rd, we use
x⊙yandx/yto denote the coordinate-wise product and quotient respectively. 0dand1drepresent
zero and one d-dimensional vectors respectively.
2 Problem set up and algorithm
We consider unconstrained stochastic optimization (1) over Rdwithl2-norm. The objective function
f:Rd→Ris differentiable. Given x∈Rd, we assume a gradient oracle that returns a random
vector g(x,z)∈Rddependent by the random sample z. The true gradient of fatxis denoted by
∇f(x)∈Rd.
Assumptions. We make the following assumptions throughout the paper.
• (A1) Bounded below: There exists f∗>−∞ such that f(x)≥f∗,∀x∈Rd;
• (A2) Unbiased estimator: The gradient oracle provides an unbiased estimator of ∇f(x), i.e.,
Ez[g(x,z)] =∇f(x),∀x∈Rd;
• (A3) Generalized affine variance noise: The gradient oracle satisfies that there are some con-
stants σ0, σ1>0, p∈[0,4),Ezh
exp
∥g(x,z)−∇f(x)∥2
σ2
0+σ2
1∥∇f(x)∥pi
≤exp(1) ,∀x∈Rd.
The first two assumptions are standard in the stochastic optimization. The third assumption provides
a mild noise model that covers the almost surely bounded noise and sub-Gaussian noise. Moreover,
it’s more general than almost surely affine variance noise as follows
∥g(x,z)− ∇f(x)∥2≤σ2
0+σ2
1∥∇f(x)∥2, a.s., (3)
and enlarge the range of pto[0,4). Assumption ( A3) with p= 2 and (3) are also utilized in
[2] to establish high probability results for AdaGrad-Norm. It represents a stronger condition than
the expected version of (2) that is commonly employed for deriving the expected convergence of
algorithms. However, almost surely assumption enables the derivation of stronger high-probability
convergence guarantees for algorithms, while still ensuring expected convergence.
The affine noise variance assumption is important for machine learning applications with feature
noise (including missing features) [15, 22], in robust linear regression [46], and generally whenever
the model parameters are multiplicatively perturbed by noise (e.g., a multilayer network, where
noise from a previous layer multiplies the parameters in subsequent layers). We refer interested
readers to see e.g., [3, 46, 4, 14, 41, 2] for more discussions about the affine variance noise.
3Adam. For the stochastic optimization problem, we study Algorithm 1, which is an equivalent
form of Adam [23] with the two corrective terms for msandvsincluded into ηsfor notation
simplicity.
The iterative relationship in Algorithm 1 can be also written as for any s∈[T],
xs+1=xs−ηs(1−β1)·gs√vs+ϵs+β1·ηs(√vs−1+ϵs−1)
ηs−1(√vs+ϵs)⊙(xs−xs−1), (4)
where we let x0=x1andη0=η. (4) plays a key role in the convergence analysis, showing that
Adam incorporates a heavy-ball style momentum and dynamically adjusts its momentum through β1
andβ2, along with adaptive step-sizes. This inspires us to learn from some classical analysis meth-
ods for algorithms with momentum and provides some new estimations to fit in with the adaptive
property.
3 Convergence of Adam with smooth objective functions
In this section, we assume that the objective function fisL-smooth satisfying that for any x,y∈
Rd,
∥∇f(y)− ∇f(x)∥ ≤L∥y−x∥. (5)
We then show that Adam has the following high probability results.
Theorem 3.1. LetT≥1and{xs}s∈[T]be the sequence generated by Algorithm 1. If Assumptions
(A1)-(A3) hold, and the hyper-parameters satisfy that
0≤β1< β2<1, β 2= 1−c/T, η =C0p
1−β2, ϵ=ϵ0p
1−β2, (6)
for some constants c, C0>0andϵ0>0, then for any given δ∈(0,1/2), it holds that with
probability at least 1−2δ,
1
TTX
s=1∥∇f(xs)∥2≤ O(
G2 r
σ2
0+σ2
1Gp+G2
T+ϵ0
T!
logT
δ)
,
where G2is defined by the following order with respect to T, ϵ0, δ:2
G2∼ O
log3
2max{2,4
4−p}T
ϵ0δ
. (7)
Theorem 3.1 provides the nearly optimal convergence rate O
poly(logT)/√
T
to find a stationary
point when setting the parameter probably: β2= 1− O(1/T). It’s worth noting that the setting
requires β2to be closed enough to 1when Tis sufficiently large, which roughly aligns with the
typical setting in [23, 58, 10, 40]. For a more detailed comparison of our results to existing works,
including assumptions, convergence rate, and dependency, we refer readers to Table 1.
Adam without corrective terms and RMSProp. We also consider a simplified version of Adam
that drops two corrective terms as shown in Algorithm 2 in Appendix. Algorithm 2 with β1= 0
can be directly reduced to RMSProp [38]. More importantly, the following result shows that the
convergence rate of Algorithm 2 is adaptive to the noise level.
Theorem 3.2 (informal version of Theorem C.2) .LetT≥1and{xs}s∈[T]be generated by Algo-
rithm 2 covering RMSProp. Following the assumptions and setup in Theorem 3.1, with probability
at least 1−2δ,PT
s=1∥∇f(xs)∥2/T≲˜O(1/T+σ0/√
T).
4 Convergence of Adam with generalized smooth objective functions
In this section, we study the convergence behavior of Adam in the generalized smooth case. We first
provide some necessary introduction to the generalized smooth condition.
2The detailed expression of G2could be found in (53) from Appendix.
44.1 Generalized smoothness
We consider the following (L0, Lq)-smoothness condition: there exist constants q∈[0,2)and
L0, Lq>0, satisfying that for any x,y∈Rdwith∥x−y∥ ≤1/Lq,
∥∇f(y)− ∇f(x)∥ ≤(L0+Lq∥∇f(x)∥q)∥x−y∥. (8)
The generalized smooth condition was originally put forward by [52] for any twice differentiable
function fsatisfying that
∥∇2f(x)∥ ≤L0+L1∥∇f(x)∥. (9)
It has been proved that a lot of objective functions in experimental areas satisfy (9) but out of L-
smoothness range, especially in training large language models, see e.g., Figure 1 in [52] and [8].
To better understand the theoretical significance of the generalized smoothness, [50] provided an
alternative form in (8) with q= 1, only requiring fto be differentiable. They showed that (8) is
sufficient to elucidate the convergence of gradient-clipping algorithms.
There are three key reasons for opting for (8). Firstly, considering our access is limited to first-order
stochastic gradients, it’s logical to only assume that fis differentiable. Second, as pointed out by
Lemma A.2 in [50] and Proposition 1 in [13], (8) and (9) are equivalent up to constant factors when
fis twice differentiable considering q= 1. Thus, (8) covers a broader range of functions than (9).
Finally, it’s easy to verify that (8) is strictly weaker than L-smoothness. A concrete example is that
the simple function f(x) =x4, x∈Rdoes not satisfy any global L-smoothness but (8). Moreover,
the expanded range of qto[0,2)is necessary as all univariate rational functions P(x)/Q(x), where
P, Q are polynomials and double exponential functions a(bx)witha, b > 1are(L0, Lq)-smooth
with1< q < 2(see [25, Proposition 3.4]). We refer interested readers to see [52, 50, 13, 25] for
more discussions of concrete examples of generalized smoothness.
4.2 Convergence result
We then provide the high probability convergence result of Adam with (L0, Lq)-smoothness condi-
tion as follows.
Theorem 4.1. LetT≥1andδ∈(0,1/2). Suppose that {xs}s∈[T]is a sequence generated by
Algorithm 1, fis(L0, Lq)-smooth satisfying (8), Assumptions (A1)-(A3) hold, and the parameters
satisfy
0≤β1< β2<1, β 2= 1−c/T, ϵ =ϵ0p
1−β2, η =˜C0p
1−β2,
˜C0≤min(
E0,E0
H,E0
L,s
β2(1−β1)2(1−β1/β2)
4L2qd)
, (10)
where c, ϵ0, E0,˜C0>0are constants, ˆHis controlled by O
log
T
ϵ0δ
3, andH,H,Lare defined
as
H:=L0/Lq+
4LqˆHq
+
4LqˆHq
2−q+
4L0ˆHq
2+ 4LqˆH+
4LqˆH1
2−q+q
4L0ˆH,
H:=s
2(σ2
0+σ2
1Hp+H2) logeT
δ
,L:=L0+Lq
Hq+H+L0
Lqq
. (11)
Then it holds that with probability at least 1−2δ,
1
TTX
s=1∥∇f(xs)∥2≤ O(ˆH
˜C0 r
σ2
0+σ2
1Hp+H2
T+ϵ0
T!
logT
δ)
. (12)
Note that in the above theorem, the order of logTinˆHand the final convergence bound is bet-
ter than the one in Theorem 3.1 under the same noise assumption. This better dependency comes
from the expense of using problem parameters to tune step-size ˜C0. Since ˆHis logarithm order
3The specific definition of ˆHcan be found in (115) from Appendix.
5ofT,H,H,Lare both polynomial logarithm order of Tand the final convergence rate in (12) is
O(poly(logT)/√
T)order. Note that ˜C0≤ O(1/poly(logT))from (10) when T≫d. Hence,
when Tis large enough, a possible optimal setting is that η=c1/(√
Tpoly(logT))for some con-
stantc1>0, which roughly matches the typical setting as mentioned before.
Similarly, we also obtain a convergence bound that is adaptive to the noise level for Adam without
corrective terms and RMSProp under generalized smoothness.
Theorem 4.2 (informal version of Theorem E.1) .LetT≥1and{xs}s∈[T]be generated by Algo-
rithm 2 covering RMSProp. Following the assumptions and the same order of hyper-parameters in
Theorem 4.1, with probability at least 1−2δ,PT
s=1∥∇f(xs)∥2/T≲˜O(1/T+σ0/√
T).
5 Related works
There is a large amount of works on stochastic approximations (or online learning algorithms) and
adaptive variants, e.g., [5, 36, 48, 29, 12, 4, 6, 27, 55] and the references therein. In this section, we
will discuss the most related works and make a comparison with our main results.
5.1 Convergence with affine variance noise and its variants
We mainly list previous literature considering (2) over non-convex smooth scenario. [3] provided an
asymptotic convergence result for SGD with (2). In terms of non-asymptotic results, [4] proved the
convergence of SGD, illustrating that the analysis was non-essentially different from the bounded
noise case from [17].
In the adaptive methods field, [14] studied convergence of AdaGrad-Norm with (2), pointing out that
the analysis is more challenging than the bounded noise and bounded gradient case in [44]. They
provided a convergence rate of ˜O(1/√
T)without knowledge of problem parameters, and further
improved the bound adapting to the noise level: when σ1∼ O(1/√
T),
1
TTX
t=1E∥∇f(xt)∥2≤˜Oσ0√
T+1
T
. (13)
(13) matches exactly with SGD’s case [4], showing a fast rate of ˜O(1/T)when σ0is sufficiently
low. Later, [41] proposed a deep analysis framework obtaining (13) with a tighter dependency to T
and not requiring any restriction over σ1. They further obtained the same rate for AdaGrad under a
stronger coordinate-wise version of (2): for all i∈[d],
Ez|g(x,z)i− ∇f(x)i|2≤σ2
0+σ2
1|∇f(x)i|2. (14)
[2] obtained a probabilistic convergence rate for AdaGrad-Norm with (3) using a novel induction
argument to estimate the function value gap without any requirement over σ1as well.
In the analysis of Adam, a line of works [35, 53, 42] considered Adam for finite-sum objective
functions under different regimes while possibly incorporating natural random shuffling technique.
They could ensure that this variant converged to a bounded region where
min
t∈[T]E
min{∥∇f(xt)∥,∥∇f(xt)∥2}
≲logT√
T+C1σ0 (15)
under the affine growth condition which is equivalent to (2). Though not explicitly concluded, when
setting β2= 1− O(1/T), [53]’s work can also ensure a convergence rate of order ˜O(1/√
T)under
certain settings. Besides, both [21] and [18] provided convergence bounds allowing for large heavy-
ball momentum parameter that aligns more closely with practical settings. However, they relied on
the assumption for step-sizes where Cl≤ ∥1√vt+ϵt∥∞≤Cu,∀t∈[T]. [40] and [20] used distinct
methods to derive convergence bounds in expectation and high probability respectively, without
relying on bounded gradients. Both studies achieved a convergence rate of the form in (13) for
Adam ignoring the corrective terms. [20] further achieved a ˜O(1/√
T)rate for Adam. However, the
two works only studied coordinate-wise affine variance noise.
In this paper, we derive a stronger high probability convergence rate for Adam with original cor-
rective terms, relying on an almost surely noise assumption. The noise model is general enough to
6cover bounded noise, sub-Gaussian noise, and (coordinate-wise) affine variance noise. Although we
consider a stronger almost surely assumption, our probabilistic convergence result is also stronger
than the expected convergence.
5.2 Convergence with generalized smoothness
The generalized smooth condition was first proposed for twice differentiable functions by [52] (see
(9)) to explain the acceleration mechanism of gradient-clipping. This assumption was extensively
confirmed in experiments of large-scale language models [52]. Later, [50] further relaxed it to a
more general form in (8) allowing for first-order differentiable functions. Subsequently, a series of
works [31, 54, 33] studied different algorithms’ convergence under this condition.
In the field of adaptive methods, [13] provided a convergence bound for AdaGrad-Norm assuming
(2) and (8) with q= 1, albeit requiring σ1<1. Based on the same conditions, [41] improved the
convergence rate to the form in (13) without restriction on σ1. [42] explored how Adam without cor-
rective terms behaves under generalized smoothness with q= 1and (2). However, they could only
assert convergence to a bounded region as shown in (15). [8] showed that an Adam-type algorithm
converges to a stationary point under a stronger coordinate-wise generalized smooth condition. Re-
cently, [25] provided a novel framework to derive high probability convergence bound for Adam
under the generalized smooth and sub-Gaussian noise case.
In this paper, we consider a more general noise setup and investigate Adam’s convergence under the
generalized smooth landscape. We prove that Adam is powerful enough to find a stationary point
with properly tuned step-sizes even under these relaxed assumptions. Moreover, the convergence
rate is not harmed by the relaxation of noise and smoothness, matching the optimal O(1/√
T)rate
up to logarithm factors.
5.3 Convergence of Adam
Adam was first proposed by [23] with empirical studies and theoretical results on online convex
learning. The original proof of convergence in [23] was later shown by [32] to contain gaps. [32]
and the subsequent work [43] also showed that for a range of momentum parameters chosen indepen-
dently with the problem instance, Adam does not necessarily converge even for convex objectives.
Many works have focused on its convergence behavior in non-convex smooth fields. A series of
works studied Adam ignoring corrective terms, all requiring a uniform bound for gradients’ norm.
Among these works, [49] demonstrated that Adam can converge within a specific region if step-sizes
and decay parameters are determined properly by the smooth parameter. [9] proposed a convergence
result to a stationary point and required all stochastic gradients must keep the same sign. To circum-
vent this requirement, [58] introduced a convergence bound only requiring hyper-parameters to sat-
isfy specific conditions. [10] conducted a simple proof and further improved the dependency on the
heavy-ball momentum parameter. Recently, [56] introduced Nesterov-like acceleration into Adam
and AdamW [28] indicating their superiority in convergence over the non-accelerated versions. For
Adam-related works under (2) or generalized smoothness, we refer readers to Sections 5.1 and 5.2.
We also want to highlight that a series of works [24, 45, 51] investigated the geometry of Adam from
anl∞-norm perspective. [24] and [45] studied the geometry of Adam by regarding it as a variant of
SignSGD and [51] showed that full-batch Adam converges towards a linear classifier that achieves
the maximum l∞-margin when the training data are linearly separable.
6 Proof sketch under the smooth case
In this section, we provide a proof sketch of Theorem 3.1 with some insights and proof novelty. Our
proof borrows some ideas from [44, 10, 14, 2, 40, 20]. The detailed proof is in Appendix B.
Preliminary. To start with, we let the stochastic gradient gs= (gs,i)i, the true gradient ∇f(xs) =
¯gs= (¯gs,i)iandξs= (ξs,i)i=gs−¯gs. We also let ϵs=ϵp
1−βs
2and thus ϵs=ϵs1d. For any
positive integer Tandδ∈(0,1), we define MT=p
log (e T/δ). We denote the adaptive part of
the step-size as
bs:=√vs+ϵs=p
β2vs−1+ (1−β2)g2s+ϵs. (16)
7We define two auxiliary sequences {ps}s≥1and{ys}s≥1,
p1=0d,y1=x1,ps=β1
1−β1(xs−xs−1),ys=ps+xs,∀s≥2. (17)
We follow from [16, 47] which was used to prove the convergence of SGD with momentum and
later applied to handle many variants of momentum-based algorithms. Recalling the iteration of xs
in (4), we reveal that yssatisfies
ys+1=ys−ηs·gs
bs+β1
1−β1ηsbs−1
ηs−1bs−1d
⊙(xs−xs−1). (18)
In addition, given T≥1, we define, ∀s∈[T],
Gs= max
j∈[s]∥¯gj∥,GT(s) =MTq
2σ2
0+ 2σ2
1Gp
s+ 2G2s,GT=MTq
2σ2
0+ 2σ2
1Gp+ 2G2,(19)
where Gis as in Theorem 3.1. Both GsandGT(s)will serve as upper bounds for gradients’ norm
before time s. We will verify their importance in the later argument.
Starting from the descent lemma. We fix the horizon Tand start from the standard descent
lemma of L-smoothness. Then, for any given t∈[T], combining with (18) and summing over
s∈[t],
f(yt+1)≤f(x1) +tX
s=1−ηs
∇f(ys),gs
bs
| {z }
A+β1
1−β1tX
s=1⟨∆s⊙(xs−xs−1),∇f(ys)⟩
| {z }
B
+L
2tX
s=1ηs·gs
bs−β1
1−β1(∆s⊙(xs−xs−1))2
| {z }
C, (20)
where we let ∆s=ηsbs−1
ηs−1bs−1dand use y1=x1from (17). In what follows, we will estimate A,
B, and Crespectively.
Probabilistic estimations. To proceed with the analysis, we next introduce two probabilistic es-
timations showing that the norm of the noises and a related summation of martingale difference
sequence could be well controlled with high probability. We show that with probability at least
1−2δ, the following two inequalities hold simultaneously for all t∈[T]:
∥ξt∥2≤M2
T 
σ2
0+σ2
1∥¯gt∥p
,and (21)
−tX
s=1ηs
¯gs,ξs
as
≤GT(t)
4GTtX
s=1ηs¯gs√as2
+D1GT, (22)
where D1is a constant defined in Lemma B.7 and aswill be introduced later. In what follows, we
always assume that (21) and (22) hold for all t∈[T]and carry out our subsequent analysis with
some deterministic estimations.
Estimating A. We first decompose Aas
A=tX
s=1−ηs
¯gs,gs
bs
| {z }
A.1+tX
s=1ηs
¯gs− ∇f(ys),gs
bs
| {z }
A.2.
Due to the correlation of the stochastic gradient gsand the step-size ηs/bs, the estimating of A.1is
challenging, as also noted in the analysis for other adaptive gradient methods, e.g., [44, 10, 14, 2, 40,
20]. To break this correlation, the so-called proxy step-size technique is introduced and variants of
proxy step-size have been introduced in the related literature. However, to our best knowledge, none
8of these proxy step-sizes could be used in our analysis for Adam considering potential unbounded
gradients under the noise model in Assumption (A3). In this paper, we construct a proxy step-size
ηs/as, withasrelying on GT(s)in (19), defined as for any s∈[T],
as=q
β2vs−1+ (1−β2) (GT(s)1d)2+ϵs. (23)
With the so-called proxy step-size technique over ηs/asandξs=gs−¯gs, we decompose A.1as
A.1=−tX
s=1ηs¯gs√as2
−tX
s=1ηs
¯gs,ξs
as
| {z }
A.1.1+tX
s=1ηs
¯gs,1
as−1
bs
gs
| {z }
A.1.2.
In the above decomposition, the first term serves as a descent term. A.1.1 is now a summation of
a martingale difference sequence which could be estimated by (22). A.1.2 is regarded as an error
term when introducing as. However, due to the delicate construction of as, the definition of local
gradients’ bound GT(t), and using some basic inequalities, we show that
A.1.2≤1
4tX
s=1ηs¯gs√as2
+ηGT(t)√1−β2
1−β1tX
s=1gs
bs2
.
The first RHS term can be eliminated with the descent term while the summation of the last term
can be bounded by
tX
s=1gs
bs2
∨tX
s=1ms
bs2
∨tX
s=1ms
bs+12
∨tX
s=1ˆms
bs≲d
1−β2logT
βT
2
, (24)
due to the step-size’s adaptivity, the iterative relationship of the algorithm, the smoothness of the
objective function, as well as (21). Here, ˆms=ms
1−βs
1.
Estimating B and C. The key to estimate Bis to decompose Bas
B=β1
1−β1tX
s=1⟨∆s⊙(xs−xs−1),¯gs⟩
| {z }
B.1+β1
1−β1tX
s=1⟨∆s⊙(xs−xs−1),∇f(ys)−¯gs⟩
| {z }
B.2.
To estimate B.1, we use the updated rule and further write ∆s⊙(xs−xs−1)as
ηs
bs−ηs
as
⊙ms−1+ηs
as−ηs
bs−1
⊙ms−1+ (ηs−ηs−1)ms−1
bs−1,
and upper bound the three related inner products. Using some basic inequalities, the smoothness,
(24), and some delicate computations, one can estimate the three related inner products, B.2andC,
and thus get that
B+C≤1
4tX
s=1ηs¯gs√as2
+ (b1GT(t) +b2) logT
βT
2
,
where b1andb2are positive constants determined by β1, β2, d, L, η .
Bounding gradients through induction. The last challenge comes from the potential unbounded
gradients’ norm. Plugging the above estimations into (20), we obtain that
f(yt+1)≤f(x1) +GT(t)
4GT−1
2tX
s=1ηs¯gs√as2
+c1GT+ (c2GT(t) +c3) logT
βT
2
,(25)
where c1, c2, c3are constants determined by β1, β2, d, L, η . Then, we will first show that G1≤G
and suppose that for some t∈[T],
Gs≤G,∀s∈[t]thus GT(s)≤ GT,∀s∈[t]. (26)
9It’s then clear to reveal from (25) and the induction assumption that f(yt+1)is restricted by the
first-order of GT. Moreover, f(yt+1)−f∗could be served as the upper bound of ∥¯gt+1∥2since
∥¯gt+1∥2≤ ∥∇ f(yt+1)∥2+∥¯gt+1− ∇f(yt+1)∥2≤2L(f(yt+1)−f∗) +∥¯gt+1− ∇f(yt+1)∥2,
(27)
where we use a standard result ∥∇f(x)∥2≤2L(f(x)−f∗)in smooth-based optimization. We also
use the smoothness to control ∥¯gt+1− ∇f(yt+1)∥2and combine with (26) and (27) to derive that
∥¯gt+1∥2≤˜d1+˜d2(σ1Gp/2+G),
where ˜d1,˜d2are constants that are also determined by hyper-parameters and restricted by O(logT−
Tlogβ2)with respect to T. Then, using Young’s inequality,
∥¯gt+1∥2≤G2
2+˜d1+4−p
4·pp
4−p
˜d24
4−p+
˜d22
.
Thus, combining with a proper construction G2(detailed in (53)), we could prove that
G2= 2˜d1+4−p
2·pp
4−p
˜d24
4−p+ 2
˜d22
,
which leads to ∥¯gt+1∥2≤G2. Combining with the induction argument, we deduce that ∥¯gt∥2≤
G2,∀t∈[T+ 1].
Final estimation. Following the induction step for upper bounding the gradients’ norm, we also
prove the following result in high probability:
LTX
s=1ηs
∥as∥∞∥¯gs∥2≤LTX
s=1ηs¯gs√as2
≤G2.
We could rely on GTto prove that ∥as∥∞≤ GTp
1−βs
2+ϵs,∀s∈[T], and then combine with ηs
in Algorithm 1 to further deduce the desired guarantee forPT
s=1∥¯gs∥2/T.
7 Conclusion
In this paper, we investigate the convergence of the Adam optimization algorithm on non-convex
smooth problems under certain relaxed conditions. We begin by considering a mild noise assump-
tion that encompasses several noise types, particularly the almost surely affine variance noise.
Under this noise condition, we demonstrate that Adam can find a stationary point at a rate of
O(poly(logT)/√
T)with high probability. Within our framework, we introduce a novel proxy step-
size to manage the entanglement of stochastic gradients and adaptive step-sizes, and we employ a
new decomposition method to estimate the errors introduced by the proxy step-size, the momentum,
and the corrective terms in Adam.
We also extend our analysis to the convergence of Adam when the objective function is generalized
smooth. This relaxed assumption is empirically validated to be more realistic in practical applica-
tions. Our results indicate that, with appropriate hyper-parameter tuning, Adam can find a stationary
point at the same order of convergence rate as in the smooth case.
Limitations. Our study has several limitations that warrant further exploration. First, it would be
advantageous to provide experimental results to validate the hyper-parameter settings in our results.
Second, the convergence bound is not strictly tight compared to the lower bound, leaving a gap
involving logarithmic factors, which may be improved in future work.
Acknowledgement
This work was supported in part by the NSFC under grant number 12471096, and the National Key
Research and Development Program of China under grant number 2021YFA1003500.
10References
[1] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Wood-
worth. Lower bounds for non-convex stochastic optimization. Mathematical Programming ,
199(1-2):165–214, 2023.
[2] Amit Attia and Tomer Koren. SGD with AdaGrad stepsizes: full adaptivity with high prob-
ability to unknown parameters, unbounded gradients and affine variance. In International
Conference on Machine Learning , 2023.
[3] Dimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with
errors. SIAM Journal on Optimization , 10(3):627–642, 2000.
[4] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale ma-
chine learning. SIAM Review , 60(2):223–311, 2018.
[5] Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-
line learning algorithms. IEEE Transactions on Information Theory , 50(9):2050–2057, 2004.
[6] Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu. Closing
the generalization gap of adaptive gradient methods in training deep neural networks. arXiv
preprint arXiv:1806.06763 , 2018.
[7] Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of
Adam-type algorithms for non-convex optimization. In International Conference on Learning
Representations , 2019.
[8] Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Ro-
bustness to unbounded smoothness of generalized signSGD. In Advances in Neural Informa-
tion Processing Systems , 2022.
[9] Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for RMSProp and
Adam in non-convex optimization and an empirical comparison to Nesterov acceleration. arXiv
preprint arXiv:1807.06766 , 2018.
[10] Alexandre Défossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence
proof of Adam and Adagrad. Transactions on Machine Learning Research , 2022.
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,
2018.
[12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research , 12(7):2121–2159, 2011.
[13] Matthew Faw, Litu Rout, Constantine Caramanis, and Sanjay Shakkottai. Beyond uniform
smoothness: a stopped analysis of adaptive SGD. In Conference on Learning Theory , 2023.
[14] Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay Shakkottai,
and Rachel Ward. The power of adaptivity in SGD: self-tuning step sizes with unbounded
gradients and affine variance. In Conference on Learning Theory , 2022.
[15] Wayne A Fuller. Measurement error models . John Wiley & Sons, 2009.
[16] Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence
of the heavy-ball method for convex optimization. In European Control Conference , 2015.
[17] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization , 23(4):2341–2368, 2013.
[18] Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence anal-
ysis for algorithms of the Adam family. In Annual Workshop on Optimization for Machine
Learning , 2021.
11[19] Yusu Hong and Junhong Lin. Revisiting convergence of adagrad with relaxed assumptions. In
The 40th Conference on Uncertainty in Artificial Intelligence .
[20] Yusu Hong and Junhong Lin. High probability convergence of Adam under unbounded gradi-
ents and affine variance noise. arXiv preprint arXiv:2311.02000 , 2023.
[21] Feihu Huang, Junyi Li, and Heng Huang. Super-Adam: faster and universal framework of
adaptive gradients. In Advances in Neural Information Processing Systems , 2021.
[22] Fereshte Khani and Percy Liang. Feature noise induces loss discrepancy across groups. In
International Conference on Machine Learning , pages 5209–5219. PMLR, 2020.
[23] Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In Interna-
tional Conference on Learning Representations , 2015.
[24] Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise is
not the main factor behind the gap between SGD and Adam on Transformers, but sign descent
might be. In The Eleventh International Conference on Learning Representations , 2023.
[25] Haochuan Li, Ali Jadbabaie, and Alexander Rakhlin. Convergence of Adam under relaxed
assumptions. In Advances in Neural Information Processing Systems , 2023.
[26] Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive SGD with momen-
tum. In Workshop on International Conference on Machine Learning , 2020.
[27] Mingrui Liu, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das, and Tianbao
Yang. Towards better understanding of adaptive gradient algorithms in generative adversarial
nets. arXiv preprint arXiv:1912.11940 , 2019.
[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International
Conference on Learning Representations , 2019.
[29] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochas-
tic approximation approach to stochastic programming. SIAM Journal on Optimization ,
19(4):1574–1609, 2009.
[30] Boris T Polyak. Some methods of speeding up the convergence of iteration methods. Ussr
Computational Mathematics and Mathematical Physics , 4(5):1–17, 1964.
[31] Jiang Qian, Yuren Wu, Bojin Zhuang, Shaojun Wang, and Jing Xiao. Understanding gradient
clipping in incremental gradient methods. In International Conference on Artificial Intelli-
gence and Statistics , 2021.
[32] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond.
InInternational Conference on Learning Representations , 2018.
[33] Amirhossein Reisizadeh, Haochuan Li, Subhro Das, and Ali Jadbabaie. Variance-reduced
clipping for non-convex optimization. arXiv preprint arXiv:2303.00883 , 2023.
[34] Herbert Robbins and Sutton Monro. A stochastic approximation method. Annals of Mathe-
matical Statistics , pages 400–407, 1951.
[35] Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun. RMSProp converges with proper
hyper-parameter. In International Conference on Learning Representations , 2020.
[36] Steve Smale and Yuan Yao. Online learning algorithms. Foundations of Computational Math-
ematics , 6:145–170, 2006.
[37] Matthew Streeter and H Brendan McMahan. Less regret via online conditioning. arXiv preprint
arXiv:1002.4862 , 2010.
[38] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSProp: Divide the gradient by a run-
ning average of its recent magnitude. COURSERA: Neural Networks for Machine Learning ,
2012.
12[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems , 2017.
[40] Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen. Closing the gap
between the upper bound and lower bound of Adam’s iteration complexity. In Advances in
Neural Information Processing Systems , 2023.
[41] Bohan Wang, Huishuai Zhang, Zhiming Ma, and Wei Chen. Convergence of AdaGrad for
non-convex objectives: simple proofs and relaxed assumptions. In Conference on Learning
Theory , 2023.
[42] Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Zhi-Ming Ma, Tie-Yan Liu, and Wei
Chen. Provable adaptivity in Adam. arXiv preprint arXiv:2208.09900 , 2022.
[43] Ruiqi Wang and Diego Klabjan. Divergence results and convergence of a variance reduced
version of adam. arXiv preprint arXiv:2210.05607 , 2022.
[44] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: sharp convergence over
nonconvex landscapes. Journal of Machine Learning Research , 21(1):9047–9076, 2020.
[45] Shuo Xie and Zhiyuan Li. Implicit bias of adamw: l∞-norm constrained optimization. In
International Conference on Machine Learning , 2024.
[46] Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and Lasso. In Advances
in Neural Information Processing Systems , 2008.
[47] Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi Yang. A unified analysis of stochastic
momentum methods for deep learning. In Proceedings of the Twenty-Seventh International
Joint Conference on Artificial Intelligence , 2018.
[48] Yiming Ying and D-X Zhou. Online regularized classification algorithms. IEEE Transactions
on Information Theory , 52(11):4775–4788, 2006.
[49] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive
methods for nonconvex optimization. In Advances in Neural Information Processing Systems ,
2018.
[50] Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algo-
rithms for non-convex optimization. In Advances in Neural Information Processing Systems ,
2020.
[51] Chenyang Zhang, Difan Zou, and Yuan Cao. The implicit bias of Adam on separable data.
InHigh-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning ,
2024.
[52] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: a theoretical justification for adaptivity. In International Conference on Learning
Representations , 2020.
[53] Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can con-
verge without any modification on update rules. In Advances in Neural Information Processing
Systems , 2022.
[54] Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochas-
tic normalized gradient descent. Science China Information Sciences , 64:1–13, 2021.
[55] Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu. On the
convergence of adaptive gradient methods for nonconvex optimization. In Annual Workshop
on Optimization for Machine Learning , 2020.
[56] Pan Zhou, Xingyu Xie, and Shuicheng Yan. Win: weight-decay-integrated Nesterov acceler-
ation for adaptive gradient algorithms. In International Conference on Learning Representa-
tions , 2023.
13[57] Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of
adam in learning neural networks with proper regularization. arXiv preprint arXiv:2108.11371 ,
2021.
[58] Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for
convergences of Adam and RMSProp. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 2019.
14The appendix is organized as follows. The next section presents some necessary technical lemmas,
some of which have appeared in the previous literature. In Appendix B and Appendix D, the detailed
proofs for Theorem 3.1 and Theorem 4.1 are presented respectively. In Appendix C and Appendix E,
we provide detailed results and proofs for Theorem 3.2 and Theorem 4.2, respectively. Finally,
Appendix F and Appendix G provide all the omitted proofs in previous sections.
A Complementary lemmas
We first provide some necessary technical lemmas as follows.
Lemma A.1. Suppose that {αs}s≥1is a non-negative sequence. Given β2∈(0,1]andε >0, we
define θs=Ps
j=1βs−j
2αj. Then, for any t≥1,
tX
s=1αj
ε+θj≤log
1 +θt
ε
−tlogβ2.
Proof. See the proof of Lemma 5.2 in [10].
Lemma A.2. Suppose that {αs}s≥1is a real number sequence. Given 0≤β1< β2≤1andε >0,
we define ζs=Ps
j=1βs−j
1αj,γs=1
1−βs
1Ps
j=1βs−j
1αjandθs=Ps
j=1βs−j
2α2
j, then
tX
s=1ζ2
s
ε+θs≤1
(1−β1)(1−β1/β2)
log
1 +θt
ε
−tlogβ2
,∀t≥1,
tX
s=1γ2
s
ε+θs≤1
(1−β1)2(1−β1/β2)
log
1 +θt
ε
−tlogβ2
,∀t≥1.
Proof. The proof for the first inequality can be found in the proof of Lemma A.2 [10]. For the
second result, let ˆM=Ps
j=1βs−j
1. Then using Jensen’s inequality, we have

sX
j=1βs−j
1αj
2
=
ˆMsX
j=1βs−j
1
ˆMαj
2
≤ˆM2sX
j=1βs−j
1
ˆMα2
j=ˆMsX
j=1βs−j
1α2
j. (28)
Hence, we further have
γ2
s
ε+θs≤ˆM
(1−βs
1)2sX
j=1βs−j
1α2
j
ε+θs=1
(1−β1)(1−βs
1)sX
j=1βs−j
1α2
j
ε+θs.
Recalling the definition of θs, we have ε+θs≥ε+βs−j
2θj≥βs−j
2(ε+θj). Hence, combining
with1−β1≤1−βs
1,
γ2
s
ε+θs≤1
(1−β1)(1−βs
1)sX
j=1β1
β2s−jα2
j
ε+θj≤1
(1−β1)2sX
j=1β1
β2s−jα2
j
ε+θj.
Summing up both sides over s∈[t], and noting that β1< β2,
tX
s=1γ2
s
ε+θs≤1
(1−β1)2tX
s=1sX
j=1β1
β2s−jα2
j
ε+θj≤1
(1−β1)2(1−β1/β2)tX
j=1α2
j
ε+θj.
Finally applying Lemma A.1, we obtain the desired result.
Then, we introduce a standard concentration inequality for the martingale difference sequence that
is useful for achieving the high probability bounds, see [26] for a proof.
15Lemma A.3. Suppose {Zs}s∈[T]is a martingale difference sequence with respect to ζ1,···, ζT.
Assume that for each s∈[T],σsis a random variable only dependent by ζ1,···, ζs−1and satisfies
that
E
exp(Z2
s/σ2
s)|ζ1,···, ζs−1
≤e,
then for any λ >0, and for any δ∈(0,1), it holds that
P TX
s=1Zs>1
λlog1
δ
+3
4λTX
s=1σ2
s!
≤δ.
B Proof of Theorem 3.1
The detailed proof of Theorem 3.1 corresponds to the proof sketch in Section 6.
B.1 Preliminary
To start with, we introduce the following two notations,
ˆms=ms
1−βs
1,ˆvs=vs
1−βs
2, (29)
which include two corrective terms for msandvs. It is easy to see that ηssatisfies
ηs=ηp
1−βs
2
1−βs
1≤η
1−βs
1≤η
1−β1. (30)
We follow all the notations in Section 6, which we present here for the convenience of reading,
MT=s
logeT
δ
, G s= max
j∈[s]∥¯gj∥,
GT(s) =MTq
2σ2
0+ 2σ2
1Gp
s+ 2G2s,GT=MTq
2σ2
0+ 2σ2
1Gp+ 2G2,
bs=p
β2vs−1+ (1−β2)g2s+ϵs,
as=q
β2vs−1+ (1−β2) (GT(s)1d)2+ϵs. (31)
The following lemmas provide some estimations for the algorithm-dependent terms, which play
vital roles in the proof of Theorem 3.1. The detailed proofs could be found in Appendix F.1.
Lemma B.1. Letηs,bsbe given in Algorithm 1 and (16), then
ηsbs−1
ηs−1bs−1d
∞≤Σmax:=1√β2,∀s≥2.
The following lemma could be found similarly in [57, Lemma A.2].
Lemma B.2. Letms,bsbe given in Algorithm 1 and (16) with0≤β1< β 2<1, respectively.
Then,
ms
bs
∞≤s
(1−β1)(1−βs
1)
(1−β2)(1−β1/β2),∀s≥1.
Consequently, if fisL-smooth and we set η=C0√1−β2for some constant C0>0, then
∥¯gs∥ ≤ ∥ ¯g1∥+LC0ss
d
1−β1/β2,∀s≥1.
The following lemma is necessary for deriving (24) in the proof sketch.
16Lemma B.3. Letgs,msbe given in Algorithm 1 and ˆms,bsbe defined in (29) and(16). If0≤
β1< β2<1andFi(t) = 1 +1
ϵ2Pt
s=1g2
s,i, then for any t≥1,
tX
s=1gs
bs2
≤1
1−β2dX
i=1logFi(t)
βt
2
,
tX
s=1ms
bs2
≤1−β1
(1−β2)(1−β1/β2)dX
i=1logFi(t)
βt
2
,
tX
s=1ms
bs+12
≤1−β1
β2(1−β2)(1−β1/β2)dX
i=1logFi(t)
βt
2
,
tX
s=1ˆms
bs≤1
(1−β2)(1−β1/β2)dX
i=1logFi(t)
βt
2
.
The following lemmas are based on the smooth condition.
Lemma B.4. Suppose that fisL-smooth and Assumption (A1) holds, then for any x∈Rd,
∥∇f(x)∥2≤2L(f(x)−f∗).
Lemma B.5. Letxsbe given in Algorithm 1 and ysbe defined in (17). IffisL-smooth, η=
C0√1−β2and0≤β1< β2<1, then
∥∇f(xs)∥ ≤ ∥∇ f(ys)∥+M, M :=LC0√
d
(1−β1)p
1−β1/β2,∀s≥1.
B.2 Start point and decomposition
Specifically, we fix the horizon Tand start from the descent lemma of L-smoothness,
f(ys+1)≤f(ys) +⟨∇f(ys),ys+1−ys⟩+L
2∥ys+1−ys∥2,∀s∈[T]. (32)
For any given t∈[T], combining with (18) and (32) and then summing over s∈[t], we obtain the
same inequality in (20),
f(yt+1)≤f(x1) +tX
s=1−ηs
∇f(ys),gs
bs
| {z }
A+β1
1−β1tX
s=1⟨∆s⊙(xs−xs−1),∇f(ys)⟩
| {z }
B
+L
2tX
s=1ηs·gs
bs−β1
1−β1(∆s⊙(xs−xs−1))2
| {z }
C, (33)
where we use ∆sin (20) and y1=x1. We then further make a decomposition by introducing ¯gs
intoAandB
A=tX
s=1−ηs
¯gs,gs
bs
| {z }
A.1+tX
s=1ηs
¯gs− ∇f(ys),gs
bs
| {z }
A.2, (34)
and
B=β1
1−β1tX
s=1⟨∆s⊙(xs−xs−1),¯gs⟩
| {z }
B.1+β1
1−β1tX
s=1⟨∆s⊙(xs−xs−1),∇f(ys)−¯gs⟩
| {z }
B.2.
(35)
17B.3 Probabilistic estimations
We will provide two probabilistic inequalities with the detailed proofs given in Appendix F.2. The
first one establishes an upper bound for the noise norm, which we have already informally presented
in (21).
Lemma B.6. Given T≥1, suppose that for any s∈[T],ξs=gs−¯gssatisfies Assumption (A3).
Then for any given δ∈(0,1), it holds that with probability at least 1−δ,
∥ξs∥2≤M2
T 
σ2
0+σ2
1∥¯gs∥p
,∀s∈[T]. (36)
We next provide a probabilistic upper bound as shown in (22) for a summation of the inner product,
where we rely on the property of the martingale difference sequence and the proxy step-size asin
(23).
Lemma B.7. Given T≥1andδ∈(0,1). If Assumptions (A2) and (A3) hold, then for any λ >0,
with probability at least 1−δ,
−tX
s=1ηs
¯gs,ξs
as
≤3ληGT(t)
4(1−β1)√1−β2tX
s=1ηs¯gs√as2
+1
λlogT
δ
,∀t∈[T].(37)
As a consequence, when setting λ= (1−β1)√1−β2/(3ηGT), it holds that with probability at
least1−δ,
−tX
s=1ηs
¯gs,ξs
as
≤GT(t)
4GTtX
s=1ηs¯gs√as2
+D1GT,∀t∈[T], (38)
where D1=3η
(1−β1)√1−β2log T
δ
.
B.4 Deterministic estimations
In this section, we shall assume that (36) or/and (38) hold whenever the related estimation is needed.
Then we obtain the following key lemmas with the detailed proofs given in Appendix F.3.
Lemma B.8. Given T≥1. If(36) holds, then we have
max
j∈[s]∥ξj∥ ≤ G T(s),max
j∈[s]∥gj∥ ≤ G T(s),max
j∈[s]∥vj∥∞≤(GT(s))2,∀s∈[T].
Lemma B.9. Given T≥1. Ifbs= (bs,i)iandas= (as,i)ifollow the definitions in (16) and(23)
respectively, and (36) holds, then for all s∈[T], i∈[d],
1
as,i−1
bs,i≤GT(s)√1−β2
as,ibs,iand1
as,i−1
bs−1,i≤(GT(s) +ϵ)√1−β2
as,ibs−1,i.
Lemma B.10. Given T≥1. Under the conditions in Lemma B.3 and Lemma B.5, if (36) holds,
then the following inequality holds,
Fi(t)≤ F(T),∀t∈[T], i∈[d],
where ˆM=M(1−β1)andMfollows the definition in Lemma B.5, F(T)is define by
F(T) := 1 +2M2
T
ϵ2
σ2
0T+σ2
1T
∥¯g1∥+TˆMp
+T
∥¯g1∥+TˆM2
. (39)
We move to estimate all the related terms in Appendix B.2. First, the estimation for A.1relies on
both the two probabilistic estimations in Appendix B.3.
Lemma B.11. Given T≥1, suppose that (36) and(38) hold. Then for all t∈[T],
A.1≤GT(t)
4G−3
4tX
s=1ηs¯gs√as2
+D1GT+D2GT(t)tX
s=1gs
bs2
, (40)
where D1is given as in Lemma B.7 and D2=η√1−β2
1−β1.
18We also obtain the following lemma to estimate the adaptive momentum part B.1.
Lemma B.12. Given T≥1, if(36) holds, then for all t∈[T],
B.1≤1
4tX
s=1ηs¯gs√as2
+ (D3GT(t) +D4)tX
s=1 ms−1
bs2
+ms−1
bs−12!
+D5Gt,(41)
where
D3=2η√1−β2
(1−β1)3, D 4=ϵD3, D 5=2η√
dp
(1−β1)3(1−β2)(1−β1/β2). (42)
Proposition B.13. Given T≥1. IffisL-smooth, then the following inequality holds,
f(yt+1)≤f(x1) +A.1+B.1+D6t−1X
s=1ˆms
bs2
+D7tX
s=1gs
bs2
,∀t∈[T],
where Σmaxis as in Lemma B.1 and
D6=Lη2(1 + 4Σ2
max)
2(1−β1)2, D 7=3Lη2
2(1−β1)2. (43)
Proof. Recalling the decomposition in Appendix B.2. We first estimate A.2. Using the smoothness
offand (17), we have
∥∇f(ys)−¯gs∥ ≤L∥ys−xs∥=Lβ1
1−β1∥xs−xs−1∥. (44)
Hence, applying Young’s inequality, (44) and (30),
ηs
¯gs− ∇f(ys),gs
bs
≤ηs∥¯gs− ∇f(ys)∥ ·gs
bs
≤1
2L∥¯gs− ∇f(ys)∥2+Lη2
s
2gs
bs2
≤Lβ2
1
2(1−β1)2∥xs−xs−1∥2+Lη2
2(1−β1)2gs
bs2
.(45)
Recalling the updated rule in Algorithm 1 and applying (29) as well as (30),
∥xs−xs−1∥2=η2
s−1ms−1
bs−12
≤η2ˆms−1
bs−12
. (46)
Therefore, applying (45), (46) and β1∈[0,1), and then summing over s∈[t]
A.2≤Lη2
2(1−β1)2tX
s=1ˆms−1
bs−12
+Lη2
2(1−β1)2tX
s=1gs
bs2
. (47)
Applying Cauchy-Schwarz inequality, Lemma B.1, and combining with (44), (46), Σmax≥1, and
β1∈[0,1)
B.2≤β1
1−β1tX
s=1∥∆s∥∞∥xs−xs−1∥∥∇f(ys)−¯gs∥
≤Lβ2
1Σmax
(1−β1)2tX
s=1∥xs−xs−1∥2≤LΣ2
maxη2
(1−β1)2tX
s=1ˆms−1
bs−12
, (48)
Finally, applying the basic inequality, Lemma B.1 and (46),
C≤LtX
s=1η2
sgs
bs2
+Lβ2
1
(1−β1)2tX
s=1∥∆s∥2
∞∥xs−xs−1∥2
≤Lη2
(1−β1)2tX
s=1gs
bs2
+Lη2Σ2
max
(1−β1)2tX
s=1ˆms−1
bs−12
. (49)
Recalling the decomposition in (34) and (35), then plugging (47), (48) and (49) into (33), we obtain
the desired result.
19B.5 Bounding gradients
Based on all the results in Appendix B.3 and Appendix B.4, we are now ready to provide a global
upper bound for gradients’ norm along the optimization trajectory.
Proposition B.14. Under the same conditions in Theorem 3.1, for any given δ∈(0,1/2), it holds
that with probability at least 1−2δ,
∥¯gt∥2≤G2
t≤G2,∥gt∥2≤(GT(t))2≤ G2
T,∀t∈[T+ 1], (50)
and
∥¯gt+1∥2≤G2−LtX
s=1ηs¯gs√as2
,∀t∈[T], (51)
where G2is as in Theorem 3.1 and Gt, G,GTare given by (19).
Proof. Applying Lemma B.6 and Lemma B.7, we know that (36) or (38) hold with probability at
least 1−δ. With these two inequalities, we could deduce the desired inequalities (50) and (51).
Therefore, (50) and (51) hold with probability at least 1−2δ. We first plug (40) and (41) into the
result in Proposition B.13, which leads to that for all t∈[T],
f(yt+1)≤f(x1) +GT(t)
4GT−1
2tX
s=1ηs¯gs√as2
+D1GT+ (D2GT(t) +D7)tX
s=1gs
bs2
+ (D3GT(t) +D4)tX
s=1 ms−1
bs2
+ms−1
bs−12!
+D5Gt+D6t−1X
s=1ˆms
bs2
.(52)
Next, we will introduce the induction argument based on (52). We first provide the specific definition
ofG2as follows which is a constant determined by the horizon Tand other hyper-parameters but
not relying on t,4
G2:= 8L(f(x1)−f∗) +48MTLC0σ0
1−β1logT
δ
+16MTLC0σ0d
1−β1logF(T)
βT
2
+ 83LC0+ 8(MTσ0+ϵ0)
β2LC0d
(1−β1)2(1−β1/β2)logF(T)
βT
2
+4−p
2·pp
4−p72MTLσ1C0d
β2(1−β1)2(1−β1/β2)logT+F(T)
δβT
2 4
4−p
+ 3218MTLC0d
β2(1−β1)2(1−β1/β2)logT+F(T)
δβT
22
+4L2C2
0d
(1−β1)2(1−β1/β2). (53)
The induction then begins by noting that G2
1=∥¯g1∥2≤2L(f(x1)−f∗)≤G2from Lemma B.4
and (53). Then we assume that for some t∈[T],
Gs≤G,∀s∈[t]consequently GT(s)≤ GT,∀s∈[t]. (54)
Using this induction assumption over (52) and subtracting with f∗on both sides,
f(yt+1)−f∗≤f(x1)−f∗−1
4tX
s=1ηs¯gs√as2
+D1GT+ (D2GT+D7)tX
s=1gs
bs2
+ (D3GT+D4)tX
s=1 ms−1
bs2
+ms−1
bs−12!
+D5G+D6t−1X
s=1ˆms
bs2
.(55)
Further, we combine with Lemma B.3 and Lemma B.10 to estimate the four summations defined in
Lemma B.3, and then use G≤ GT≤2MT 
σ0+σ1Gp/2+G
to control the RHS of (55),
f(yt+1)−f∗≤ −1
4tX
s=1ηs¯gs√as2
+˜D1+˜D2+˜D3H(G), (56)
4We further deduce (7) in Theorem 3.1 based on (53).
20where H(G) =σ1Gp/2+Gand˜D1,˜D2,˜D3are defined as
˜D1=f(x1)−f∗+ 2MTσ0D1,
˜D2=2MTσ0D2+D7
1−β2+4 (MTσ0D3+D4) (1−β1)
β2(1−β2)(1−β1/β2)+D6
(1−β2)(1−β1/β2)
dlogF(T)
βT
2
,
˜D3= 2MT
D1+D2d
1−β2+2D3(1−β1)d
β2(1−β2)(1−β1/β2)
logF(T)
βT
2
+D5.
Applying Lemma B.5 and Lemma B.4,
∥¯gt+1∥2≤2∥∇f(yt+1)∥2+ 2M2≤4L(f(yt+1)−f∗) + 2M2. (57)
Then combining (56) with (57),
∥¯gt+1∥2≤ −LtX
s=1ηs¯gs√as2
+ 4L(˜D1+˜D2) + 4L˜D3H(G) + 2M2.
Applying two Young’s inequalities where ab≤a2
2+b2
2andabp
2≤4−p
4·a4
4−p+p
4·b2,∀a, b≥0,
∥¯gt+1∥2≤G2
4+G2
4−LtX
s=1ηs¯gs√as2
+ 4L(˜D1+˜D2)
+ 16L2˜D2
3+4−p
4·pp
4−p
4Lσ1˜D34
4−p+ 2M2. (58)
Recalling the definitions of Di, i∈[7]in (38), (40), (42), and (43). With a simple calculation relying
onη=C0√1−β2, ϵ≤ϵ0,Σmax≤1/√β2and0≤β1< β2<1, we could deduce that G2given
in (53) satisfies
G2= 8L(˜D1+˜D2) + 32 L2˜D2
3+4−p
2·pp
4−p
4Lσ1˜D34
4−p+ 4M2. (59)
Based on (58) and (59), we then deduce that ∥¯gt+1∥2≤G2. Further combining with Gt+1in (19)
and the induction assumption in (54),
Gt+1≤max{∥¯gt+1∥, Gt} ≤G.
Hence, the induction is complete and we obtain the desired result in (50). Furthermore, as a conse-
quence of (58), we also prove that (51) holds.
B.6 Proof of the main result
Now we are ready to prove the main convergence result.
Proof of Theorem 3.1. We set t=Tin (51) to obtain that with probability at least 1−2δ,
LTX
s=1ηs
∥as∥∞∥¯gs∥2≤LTX
s=1ηs¯gs√as2
≤G2− ∥¯gT+1∥2≤G2. (60)
Then, in what follows, we will assume that both (50) and (60) hold. Based on these two inequalities,
we could derive the final convergence bound. Since (50) and (60) hold with probability at least
1−2δ, the final convergence bound also holds with probability at least 1−2δ. Applying asin (23)
and (50), we have
∥as∥∞= max
i∈[d]q
β2vs−1,i+ (1−β2)(GT(s))2+ϵs
≤max
i∈[d]vuuut(1−β2)
s−1X
j=1βs−j
2g2
j,i+ (GT(s))2
+ϵs
≤vuut(1−β2)sX
j=1βs−j
2G2
T+ϵs=GTp
1−βs
2+ϵs,∀s∈[T]. (61)
21Then combining with the setting ηsandϵsin (6), we have for any s∈[T],
ηs
∥as∥∞≥C0p
(1−βs
2)(1−β2)
GTp
1−βs
2+ϵ0p
(1−βs
2)(1−β2)·1
1−βs
1≥C0√1−β2
GT+ϵ0√1−β2.
We therefore combine with (60) to obtain that with probability at least 1−2δ,
1
TTX
s=1∥¯gs∥2≤G2
TLC 0 p
2σ2
0+ 2σ2
1Gp+ 2G2
√1−β2+ϵ0!s
logeT
δ
. (62)
Since β2∈(0,1), we have
−logβ2= log1
β2
≤1−β2
β2=c
Tβ2,
where we apply log(1/a)≤(1−a)/a,∀a∈(0,1). With both sides multiplying T, we obtain that
log 
1/βT
2
≤c/β2. Then, we further have that when β2= 1−c/T,
logT
βT
2
≤logT+c
β2. (63)
Since 0≤β1< β2<1, there exists some constants ε1, ε2>0such that
1
β2≤1
ε1,1
1−β1/β2≤1
ε2. (64)
Therefore combining (63), (64) and (53), we could verify that G2∼ O(poly(log T))with respect
toT. Finally, using the convergence result in (62), we obtain the desired result.
C Convergence of Adam without corrective terms and RMSProp under
smoothness
In this section, we will prove the convergence bounds in Theorem 3.2 for Adam without corrective
terms and RMSProp, which are adaptive to the noise level σ0. We first present the algorithm.
Algorithm 2 Adam without corrective terms
Input: Horizon T,x1∈Rd,β1, β2∈[0,1),m0=v0=0d,η, ϵ > 0,ϵ=ϵ1d
fors= 1,···, Tdo
Draw a new sample zsand generate gs=g(xs,zs);
ms=β1ms−1+ (1−β1)gs;
vs=β2vs−1+ (1−β2)g2
s;
xs+1=xs−η·ms/ √vs+ϵ
;
end for
Remark C.1.Setting β1= 0, Algorithm 2 reduces to RMSProp.
Then, we will state the formal version for Theorem 3.2.
Theorem C.2 (Adam without corrective terms/RMSProp) .LetT≥1and{xs}s∈[T]be the se-
quence generated by Algorithm 2. If Assumptions (A1)-(A3) hold, and the hyper-parameters are
given in (6), then for any given δ∈(0,1/2), it holds that with probability at least 1−2δ,
1
TTX
s=1∥∇f(xs)∥2≤˜O ˜G4(1 +σ2
1) +˜G2(σ0+σ1˜Gp/2+˜G+ϵ0)
T+˜G2σ0√
T!
, (65)
where ˜G2is defined by the following order with respect to T, ϵ0, δ:5
˜G2∼ O
 
log3/2(T/(ϵ0δ))
1−β1!max{2,4
4−p}
. (66)
5The detailed expression of ˜G2could be found in (88).
22Remark C.3.(1). Setting β1= 0, Theorem C.2 then shows that RMSProp with the hyper-parameter
setup in (6) can also find a stationary point with the convergence rate given in (65).
(2). The convergence rate in Theorem C.2 is of order ˜O
1/T+σ0/√
T
, which can be accelerated
to˜O(1/T)rate when σ0is sufficiently low. This form matches the ones for SGD [4] and AdaGrad
[2, 19], as well as the lower bound for first-order gradient method [1] on non-convex smooth opti-
mization with affine variance noise up to logarithm factors.
C.1 Proof details
To start with, we follow the same notations of MT, Gs,GT(s)in (31) and define
˜bs= (˜bs,i)i=p
β2vs−1+ (1−β2)g2s+ϵ,
˜as= (˜as,i)i=q
β2vs−1+ (1−β2) (GT(s)1d)2+ϵ. (67)
We also follow the definition of ysin (17) and obtain from Algorithm 2 that
ys+1=ys−η·gs
˜bs+β1
1−β1 ˜bs−1
˜bs−1d!
⊙(xs−xs−1). (68)
Then, we have the following claims.
Proposition C.4. Setting ηs=ηandη=C0√1−β2, the results in Lemma B.1, Lemma B.2,
Lemma B.3, Lemma B.5, Lemma B.6, Lemma B.8 and Lemma B.10 remain unchanged.
Proof. Using ηs=ηand following the proof for the above lemmas, it’s easy to verify that these
lemmas still hold.
The result in Lemma B.9 can be improved to the following one.
Lemma C.5. Given T≥1. If˜bs= (˜bs,i)iand˜as= (˜as,i)iare defined in (67), and (36) holds,
then for all s∈[T], i∈[d],1
˜as,i−1
˜bs,i≤GT(s)√1−β2
˜as,i˜bs,iand1
˜as,i−1
˜bs−1,i≤GT(s)√1−β2
˜as,i˜bs−1,i.
Based on these results, we are able to show that the gradient norm is bounded along the training
process generated by Algorithm 2.
Proposition C.6. Under the same conditions in Theorem C.2, for any given δ∈(0,1/2), it holds
that with probability at least 1−2δ,
∥¯gt∥2≤˜G2,∥gt∥2≤ GT(t)≤˜G2
T,∀t∈[T+ 1], (69)
and
∥¯gt+1∥2≤˜G2−LηtX
s=1¯gs√˜as2
,∀t∈[T], (70)
where ˜G2,˜GTare as in (88).
Proof. Let˜∆s=˜bs−1
˜bs−1d. We also start from the descent lemma and and use (68),
f(yt+1)≤f(x1) +tX
s=1−η
∇f(ys),gs
˜bs
| {z }
˜A+β1
1−β1tX
s=1D
˜∆s⊙(xs−xs−1),∇f(ys)E
| {z }
˜B
+L
2tX
s=1η·gs
˜bs−β1
1−β1(˜∆s⊙(xs−xs−1))2
| {z }
˜C, (71)
The following estimation is based on the probability event in Lemma B.6.
23Bounding ˜A.We first have
˜A=−ηtX
s=1
¯gs,gs
˜bs
| {z }
˜A.1+ηtX
s=1
¯gs− ∇f(ys),gs
˜bs
| {z }
˜A.2, (72)
Introducing ˜asdefined in (67) into ˜A.1
˜A.1=−ηtX
s=1¯gs√˜as2
−ηtX
s=1
¯gs,ξs
˜as
| {z }
˜A.1.1+ηtX
s=1
¯gs,1
˜as−1
˜bs
gs
| {z }
˜A.1.2. (73)
Following the proof for Lemma B.7 with ηs=η, we obtain that with probability at least 1−δ,
˜A.1.1≤3λη2GT(t)
4√1−β2tX
s=1¯gs√˜as2
+1
λlogT
δ
,∀t∈[T]. (74)
Setting λ=√1−β2/(3η˜GT), we obtain that with probability at least 1−δ,
˜A.1.1≤ηGT(t)
4˜GTtX
s=1¯gs√˜as2
+3η˜GT√1−β2logT
δ
,∀t∈[T]. (75)
Using Lemma C.5 and following the similar deduction for bounding (A.1.2) in Lemma B.11, we
have
˜A.1.2≤η
4tX
s=1¯gs√˜as2
+ηp
1−β2GT(t)tX
s=1gs
˜bs2
. (76)
Plugging (75) and (76) into (73), we obtain that
˜A.1≤GT(t)
4˜GT−3
4tX
s=1η¯gs√˜as2
+3η˜GT√1−β2logT
δ
+ηp
1−β2GT(t)tX
s=1gs
˜bs2
.
(77)
The estimation for ˜A.2is similar to (45) and (47). Using (44) and Young’s inequality,
η
¯gs− ∇f(ys),gs
˜bs
≤1
2L∥¯gs− ∇f(ys)∥2+Lη2
2gs
˜bs2
≤L
2(1−β1)2∥xs−xs−1∥2+Lη2
2gs
˜bs2
.
Thereby,
˜A.2≤Lη2
2(1−β1)2tX
s=1ms−1
˜bs−12
+Lη2
2tX
s=1gs
˜bs2
. (78)
Bounding ˜B,˜C.Following the same decomposition in (35) with ∆sreplaced by ˜∆s,
˜B=β1
1−β1tX
s=1D
˜∆s⊙(xs−xs−1),¯gsE
| {z }
˜B.1+β1
1−β1tX
s=1D
˜∆s⊙(xs−xs−1),∇f(ys)−¯gsE
| {z }
˜B.2.
(79)
We define ˜Σ :=β1
1−β1D
˜∆s⊙(xs−xs−1),¯gsE
. Since ηs=η, we have the following decomposi-
tion
˜Σ≤ηβ1
1−β1·1
˜bs−1
˜as
⊙ms−1,¯gs
| {z }
˜Σ1+ηβ1
1−β1·1
˜as−1
˜bs−1
⊙ms−1,¯gs
| {z }
˜Σ2.(80)
24The additional Σ3in (159) vanishes. Then, using Lemma C.5 and Young’s inequality
˜Σ1≤dX
i=1β1
1−β1·ηGT(s)√1−β2
˜as,i˜bs,i· |¯gs,ims−1,i|
≤dX
i=1η
8·¯g2
s,i
˜as,i+2ηβ2
1(1−β2)
(1−β1)2dX
i=1(GT(s))2
˜as,i·m2
s−1,i
˜b2
s,i
≤η
8¯gs√˜as2
+2ηGT(t)√1−β2
(1−β1)2ms−1
˜bs2
. (81)
Similarly, using Lemma C.5 and Young’s inequality
˜Σ2≤η
8¯gs√˜as2
+2ηGT(t)√1−β2
(1−β1)2ms−1
˜bs−12
. (82)
Then, summing up (81) and (82) over s∈[t]and combining with (80),
˜B.1≤η
4tX
s=1¯gs√˜as2
+2ηGT(t)√1−β2
(1−β1)2tX
s=1 ms−1
˜bs2
+ms−1
˜bs−12!
. (83)
Following the similar deduction in (48) and using Lemma B.1, we have
˜B.2≤LΣ2
maxη2
(1−β1)2tX
s=1ms−1
˜bs−12
≤Lη2
β2(1−β1)2tX
s=1ms−1
˜bs−12
. (84)
Using Young’s inequality and Lemma B.1,
˜C≤Lη2tX
s=1gs
˜bs2
+Lβ2
1
(1−β1)2tX
s=1˜∆s2
∞∥xs−xs−1∥2
≤Lη2tX
s=1gs
˜bs2
+Lη2Σ2
max
(1−β1)2tX
s=1ms−1
˜bs−12
≤Lη2tX
s=1gs
˜bs2
+Lη2
β2(1−β1)2tX
s=1ms−1
˜bs−12
. (85)
Putting together. Noting that the above results are established based on probability events in
Lemma B.6 and (75). Hence, plugging (77), (78), (83), (84) and (85) into (71) and using β2<1, it
holds that with probability at least 1−2δ, for all t∈[T],
f(yt+1)≤f(x1) +ηGT(t)
4˜GT−1
2tX
s=1¯gs√˜as2
+3η˜GT√1−β2logT
δ
+2η√1−β2GT(t)
(1−β1)2+5Lη2
2β2(1−β1)2tX
s=1ms−1
˜bs−12
+2η√1−β2GT(t)
(1−β1)2tX
s=1ms−1
˜bs2
+
ηp
1−β2GT(t) +3Lη2
2tX
s=1gs
˜bs2
.
Then, combining with Lemma B.3 and Lemma B.10 and using η=C0√1−β2, we obtain that with
probability at least 1−2δ, for all t∈[T],
f(yt+1)≤f(x1) +ηGT(t)
4˜GT−1
2tX
s=1¯gs√˜as2
+E1˜GT+E2GT(t) +E3. (86)
where the coefficients are defined as
E1= 3C0logT
δ
, E2=d
C0+4C0
β2(1−β1)(1−β1/β2)
logF(T)
βT
2
,
E3=d5LC2
0
2β2(1−β1)(1−β1/β2)+3LC2
0
2(1−β1/β2)
logF(T)
βT
2
. (87)
25An induction argument. Noting that (86) holds with probability at least 1−2δfor all t∈[T],
then we will provide an induction to prove the desired result based on (86), which also holds with
probability at least 1−2δ. We first provide the detailed expression of ˜G2and˜GTas follows:
˜G2= 8L(f(x1)−f∗) + 128 L2M2
T(E1+E2)2+(4−p)
2pp
4−p(8Lσ1MT(E1+E3))4
4−p
+ 16LMT(E1+E2)σ0+ 8LE3+ 4M2,
˜GT=MTq
2σ2
0+ 2σ2
1˜Gp+ 2˜G2, (88)
where E1, E2, E3are defined in (87) and Mis as in Lemma B.5. The induction then begins by
noting that G2
1=∥¯g1∥2≤2L(f(x1)−f∗)≤˜G2from Lemma B.4. Then, we assume that for
some t∈[T],
Gs≤˜G,∀s∈[t]consequently GT(s)≤˜GT,∀s∈[t].
Using this induction assumption over (86) and ˜GT≤2MT
σ0+σ1˜Gp/2+˜G
,
f(yt+1)≤f(x1)−η
4tX
s=1¯gs√˜as2
+ 2MT(E1+E2)
σ0+σ1˜Gp/2+˜G
+E3. (89)
Following the same result in (57), we have
∥¯gt+1∥2≤4L(f(yt+1)−f∗) + 2M2. (90)
Combining with (89) and (90),
∥¯gt+1∥2≤4L(f(x1)−f∗)−LηtX
s=1¯gs√˜as2
+ 8MTL(E1+E2)
σ0+σ1˜Gp/2+˜G
+ 4LE3+ 2M2. (91)
Using two Young’s inequalities where ab≤a2
2+b2
2andabp
2≤4−p
4·a4
4−p+p
4·b2,∀a, b≥0,
∥¯gt+1∥2≤˜G2
2+ 4L(f(x1)−f∗)−LηtX
s=1¯gs√˜as2
+ 64L2M2
T(E1+E2)2
+(4−p)
4pp
4−p(8Lσ1MT(E1+E3))4
4−p+ 8LMT(E1+E2)σ0+ 4LE3+ 2M2
≤˜G2, (92)
where the last inequality comes from the definition of ˜Gin (88). Hence, the induction is complete
and we obtain the desired result in (69). Furthermore, as a consequence of (92), we also prove that
(70) holds.
C.2 Proof of the main result
Now we are ready to prove the main convergence result.
Proof of Theorem C.2. In what follows, we will assume that Proposition C.6 holds. Hence, the final
convergence bound also holds with probability at least 1−2δ. We set t=Tin (70) to obtain that
with probability at least 1−2δ,
LηTX
s=1∥¯gs∥2
∥˜as∥∞≤LηTX
s=1¯gs√˜as2
≤˜G2− ∥¯gT+1∥2≤˜G2. (93)
Using Lemma B.6, we derive that ∀s∈[T],
∥gs∥2≤2∥ξs∥2+ 2∥¯gs∥2≤2M2
Tσ2
0+ 2M2
T(1 +σ2
1)∥¯gs∥2. (94)
26Applying ˜asin (67), Proposition C.6, (94) and η=C0√1−β2, ϵ=ϵ0√1−β2, we have for any
s∈[T],
∥˜as∥∞
η=1
η
max
i∈[d]q
β2vs−1,i+ (1−β2)(GT(s))2+ϵ
≤1
η
vuut(1−β2)TX
j=1βs−j
2∥gj∥2+p
1−β2˜GT+ϵ

≤MT
C0vuut2(1 + σ2
1)TX
j=1∥¯gj∥2+MTσ0√
2T+˜GT+ϵ0
C0. (95)
We therefore combine with (93) to obtain that
TX
s=1∥¯gs∥2≤˜G2
LC0
MTvuut2(1 + σ2
1)TX
j=1∥¯gj∥2+MTσ0√
2T+˜GT+ϵ0

≤1
2TX
s=1∥¯gs∥2+˜G4M2
T(1 +σ2
1)
L2C2
0+˜G2
LC0
MTσ0√
2T+˜GT+ϵ0
,
which leads to
1
TTX
s=1∥¯gs∥2≤1
T 
2˜G4M2
T(1 +σ2
1)
L2C2
0+2˜G2(˜GT+ϵ0)
LC0!
+1√
T·2√
2˜G2MTσ0
LC0. (96)
Following the result in (63) and (64), we show that when β2= 1−c/T,˜G2,˜GT∼ O(poly(log T))
with respect to T. Finally, using the convergence result in (96), we obtain the desired result.
D Proof of Theorem 4.1
In this section, we shall follow all the notations defined in Section 6. Further, we will add two
non-decreasing sequences {L(x)
s}s≥1and{L(y)
s}s≥1as follows
L(x)
s=L0+LqGq
s,L(y)
s=L0+Lq(Gs+Gq
s+L0/Lq)q,∀s≥1. (97)
D.1 Preliminary
We first mention that Lemma B.1, Lemma B.2, and Lemma B.3 in Appendix B.1 remain unchanged
since they are independent of the smooth condition. Then the first essential challenge is that we
need to properly tune ηto restrict the distance between xs+1andxs,ys+1andyswithin 1/Lqfor
alls≥1. The following two lemmas then ensure this point. The detailed proofs could be found in
Appendix G.
Lemma D.1. Letxs,ysbe defined in Algorithm 1 and (17). If0≤β1< β 2<1, then for any
s≥1,
max{∥xs+1−xs∥,∥ys−xs∥,∥ys+1−ys∥} ≤ ηs
4d
β2(1−β1)2(1−β2)(1−β1/β2).(98)
As a consequence, when
η≤1
LqF, F :=s
4d
β2(1−β1)2(1−β2)(1−β1/β2), (99)
then for any s≥1, all the three gaps in (98) are smaller than 1/Lq.
27Lemma D.2. Letη≤1/(LqF)where Fis as in Lemma D.1. If fis(L0, Lq)-smooth, then for any
s≥1,
∥∇f(ys)∥ ≤L0/Lq+∥∇f(xs)∥q+∥∇f(xs)∥,
∥∇f(xs)∥ ≤L0/Lq+∥∇f(ys)∥q+∥∇f(ys)∥.
As a consequence, for any s≥1,
∥∇f(ys)− ∇f(xs)∥ ≤ L(x)
s∥ys−xs∥,∥∇f(ys+1)− ∇f(ys)∥ ≤ L(y)
s∥ys+1−ys∥,(100)
f(ys+1)−f(ys)− ⟨∇ f(ys),ys+1−ys⟩ ≤L(y)
s
2∥ys+1−ys∥. (101)
In the generalized smooth case, Lemma B.4 does not hold. In contrast, we provide a generalized
smooth version of [50, Lemma A.5], which establishes a different relationship between the gradi-
ent’s norm and the function value gap. Noting that when q= 1, Lemma D.3 reduces to [50, Lemma
A.5].
Lemma D.3. Suppose that fis(L0, Lq)-smooth and Assumption (A1) holds. Then for any x∈Rd,
∥∇f(x)∥ ≤maxn
4Lq(f(x)−f∗),[4Lq(f(x)−f∗)]1
2−q,p
4L0(f(x)−f∗)o
.
D.2 Probabilistic estimations
The probabilistic inequalities in (36) and (37) remain unchanged since they do not rely on any
smooth-related conditions. However, we shall rely on a different setting of λin (37) as follows.
Lemma D.4. Given T≥1andδ∈(0,1). Under the same conditions of Lemma B.7, if we set
λ= (1−β1)√1−β2/(3ηH)where His as in (11), then with probability at least 1−δ,
tX
s=1−ηs
¯gs,ξs
as
≤GT(t)
4HtX
s=1ηs¯gs√as2
+D1H,∀t∈[T], (102)
where D1is given in Lemma B.7.
The bounds of the four summations in Lemma B.3 also remain unchanged. However, the upper
bound for Fi(t)should be revised by the following lemma. The detailed proof could be found in
Appendix G.
Lemma D.5. Given T≥1. Under the conditions and notations of Lemma B.3, if fis(L0, Lq)-
smooth, η=˜C0√1−β2,(36) and(99) hold, then the following inequalities hold,
Fi(t)≤ J(t),∀t∈[T], i∈[d], (103)
where J(t)is defined as
J(t) := 1 +2M2
T
ϵ2
σ2
0t+σ2
1t
∥¯g1∥+t˜Mtp
+t
∥¯g1∥+t˜Mt2
, (104)
and˜Mt:=˜C0L(x)
tq
d
1−β1/β2.
It’s worth noting that J(t)is still random relying on the random variable L(x)
t.
D.3 Deterministic estimations
Note that (41) in Appendix B.4 remains unchanged since it’s independent from any smooth-related
condition. In terms of A.1, the only difference is using Hto replace Gin (40) as we choose a
different λin (37), leading to
A.1≤GT(t)
4H−3
4tX
s=1ηs¯gs√as2
+D1H+D2GT(t)tX
s=1gs
bs2
. (105)
We also establish the following proposition which is a generalized smooth version of Proposition
B.13.
28Proposition D.6. Given T≥1. Iffis(L0, Lq)-smooth and (99) holds, then
f(yt+1)≤f(x1) +A.1+B.1+t−1X
s=1D6(s)ˆms
bs2
+tX
s=1D7(s)gs
bs2
,∀t∈[T],(106)
where Σmaxis as in Lemma B.1 and D6(s), D7(s)are defined as,6
D6(s) =L(y)
sη2(1 + 4Σ2
max)
2(1−β1)2, D 7(s) =3L(y)
sη2
2(1−β1)2.
Proof. The proof follows some same parts in proving Proposition B.14. We start from the descent
lemma (101) in Lemma D.2 and sum over s∈[t]to obtain that
f(yt+1)≤f(x1) +tX
s=1⟨∇f(ys),ys+1−ys⟩+tX
s=1L(y)
s
2∥ys+1−ys∥2
=f(x1) +A+B+tX
s=1L(y)
s
2ηs·gs
bs−β1
1−β1ηsbs−1
ηs−1bs−1
Σs⊙(xs−xs−1)2
| {z }
C’,
(107)
where AandBfollow the same definitions in (33). We also follow the decompositions in (34) and
(35). We could also rely on the same analysis for the smooth case in (47) but the smooth parameter
is replaced by L(x)
s. Hence, we obtain that
A.2≤tX
s=1L(x)
sη2
2(1−β1)2ˆms−1
bs−12
+tX
s=1L(x)
sη2
2(1−β1)2gs
bs2
. (108)
Similarly,
B.2≤tX
s=1Σ2
maxL(x)
sη2
(1−β1)2ˆms−1
bs−12
, (109)
Noting that C’differs from CwithLreplaced by L(y)
s. Hence, relying on a similar analysis in (49),
we obtain that
C’≤tX
s=1L(y)
sη2
(1−β1)2gs
bs2
+tX
s=1Σ2
maxL(y)
sη2
(1−β1)2ˆms−1
bs−12
. (110)
Combining (107) with (108), (109) and (110), and noting that L(x)
s≤ L(y)
sfrom (97), we thereby
obtain the desired result.
D.4 Bounding gradients
Based on the unchanged parts in Appendix B.3 and Appendix B.4 and the new estimations in (105)
and (106), we are now ready to provide the uniform gradients’ bound in the following proposition.
Proposition D.7. Under the same conditions in Theorem 4.1, for any given δ∈(0,1/2), it holds
that with probability at least 1−2δ,
∥¯gt∥ ≤H,GT(t)≤ H,L(x)
t≤ L(y)
t≤ L,∀t∈[T+ 1], (111)
and
f(yt+1)−f∗≤ −1
4tX
s=1ηs¯gs√as2
+ˆH,∀t∈[T], (112)
where H,H,Lare given in (11) andˆHis given in (115) .
6The notations are different from D6andD7defined in (43).
29Proof. Based on the two inequalities (36) and (102), we could deduce the final results in (111) and
(112). Since (36) and (102) hold with probability at least 1−2δ, we thereby deduce the desired
result holding with probability at least 1−2δ. To start with, we shall verify that (99) always holds.
Recalling ηin (10) and Fin Lemma D.1,
ηF=˜C0p
1−β2F≤s
β2(1−β1)2(1−β2)(1−β1/β2)
4L2qd·F≤1
Lq.
Hence, we make sure that the distance requirement in (8) always holds according to Lemma D.1.
Second, plugging (105) and (41) into the result in (106),
f(yt+1)≤f(x1) +GT(t)
4H−1
2tX
s=1ηs¯gs√as2
+D1H+D2GT(t)tX
s=1gs
bs2
+tX
s=1D7(s)gs
bs2
+ (D3GT(t) +D4)tX
s=1 ms−1
bs2
+ms−1
bs−12!
+D5Gt+t−1X
s=1D6(s)ˆms
bs2
. (113)
We still rely on an induction argument to deduce the result. First, we provide the detail expressions
ofˆH, H as follows which is determined by hyper-parameters β1, β2and constants E0, d, T, δ,MT,
ˆH:=f(x1)−f∗+3E0MT
1−β1logT
δ
+E0MTd
1−β1log ˜J(T)
βT
2!
+4E0(MT+ϵ)d
β2(1−β1)2(1−β1/β2)log ˜J(T)
βT
2!
+2E0dp
(1−β1)3(1−β1/β2)
+3E2
0d
2(1−β1)2log ˜J(T)
βT
2!
+5E2
0d
2β2(1−β1)2(1−β1/β2)log ˜J(T)
βT
2!
, (114)
H:=L0/Lq+
4LqˆHq
+
4LqˆHq
2−q+
4L0ˆHq
2+ 4LqˆH+
4LqˆH1
2−q+q
4L0ˆH.
(115)
where E0>0is a constant and ˜J(T)is a polynomial of Tgiven as
˜J(T) := 1 +2M2
T
ϵ2
σ2
0T+σ2
1T
∥¯g1∥+T˜Mp
+T
∥¯g1∥+T˜M2
, (116)
and ˜M:=E0q
d
1−β1/β2. The induction then begins by noting that from Lemma D.3 and Hin
(115),
G1=∥¯g1∥ ≤4Lq(f(x1)−f∗) + (4 Lq(f(x1)−f∗))1
2−q+p
4L0(f(x1)−f∗)≤H.
Suppose that for some t∈[T],
Gs≤H,∀s∈[t]. (117)
Consequently, recalling GT(s)in (19), L(x)
s,L(y)
sin (97) and H,Lin (11),
GT(s)≤ H,L(x)
s≤ L(y)
s≤ L,∀s∈[t]. (118)
We thus apply (118) to (113),
f(yt+1)≤f(x1)−1
4tX
s=1ηs¯gs√as2
+D1H+D2HtX
s=1gs
bs2
+tX
s=1D7(s)gs
bs2
+ (D3H+D4)tX
s=1 ms−1
bs2
+ms−1
bs−12!
+D5H+t−1X
s=1D6(s)ˆms
bs2
.
(119)
30Further recalling the setting of ˜C0in (10), with a simple calculation it holds that,
˜C0H≤˜C0H ≤E0,˜C0L ≤E0,˜C2
0L ≤E2
0,˜C0ϵ0≤E0ϵ0. (120)
Therefore, combining with (118), (120) and ˜Mtin (104), we could use the deterministic polynomial
˜J(t)to further control J(t)in (104),
˜Mt≤˜C0Ls
d
1−β1/β2≤E0s
d
1−β1/β2=˜M,J(t)≤˜J(t)≤˜J(T),
logFi(t)
βt
2
≤logJ(t)
βt
2
≤log ˜J(T)
βT
2!
,∀t≤T, i∈[d].
Then, we could use ˜J(T)to control the four summations in Lemma B.3 which emerge in (119). In
addition, we rely on η=˜C0√1−β2and the induction assumptions of (117) and (118) to further
upper bound the RHS of (119), leading to
f(yt+1)−f∗≤f(x1)−f∗−1
4tX
s=1ηs¯gs√as2
+3˜C0H
1−β1logT
δ
+˜C0Hd
1−β1log ˜J(T)
βT
2!
+4˜C0(H+ϵ0)d
β2(1−β1)2(1−β1/β2)log ˜J(T)
βT
2!
+2˜C0Hdp
(1−β1)3(1−β1/β2)
+3˜C2
0Ld
2(1−β1)2log ˜J(T)
βT
2!
+5˜C2
0Ld
2β2(1−β1)2(1−β1/β2)log ˜J(T)
βT
2!
.
(121)
Then, combining with (120) and the definition of ˆHin (114), we obtain that
∆t+1:=f(yt+1)−f∗≤ −1
4tX
s=1ηs¯gs√as2
+ˆH≤ˆH. (122)
Then, further using Lemma D.2, Lemma D.3 and Hin (115),
∥¯gt+1∥ ≤L0/Lq+∥∇f(yt+1)∥q+∥∇f(yt+1)∥
≤L0/Lq+ (4Lq∆t+1)q+ (4Lq∆t+1)q
2−q
+ (4L0∆t+1)q
2+ 4Lq∆t+1+ (4Lq∆t+1)1
2−q+p
4L0∆t+1≤H.
We then deduce that Gt+1= max {Gt,∥¯gt+1∥} ≤ H. The induction is then complete and we obtain
the desired result in (111). Finally, as an intermediate result of the proof, we obtain that (112) holds
as well.
D.5 Proof of the main result
Proof of Theorem 4.1. The proof for the final convergence rate follows a similar idea and some same
estimations in the proof of Theorem 3.1. Setting t=Tin (112), it holds that with probability at
least1−2δ,
1
4tX
s=1ηs
∥as∥∞· ∥¯gs∥2≤1
4tX
s=1ηs¯gs√as2
≤ˆH. (123)
Then, in what follows, we would assume that (111) and (123) always hold. Relying on the two
inequalities, we thereby deduce the final convergence result. Furthermore, since (111) and (123)
hold with probability at least 1−2δ, the final convergence result also holds with probability at least
1−2δ. Using (111) and following the same analysis in (61),
∥as∥∞≤max
i∈[d]vuuut(1−β2)
s−1X
j=1βs−j
2g2
j,i+ (GT(j))2
+ϵs≤(H+ϵ)p
1−βs
2,∀s∈[T].
31Combining with the parameter setting in (10),
ηs
∥as∥∞≥ηp
1−βs
2
(1−βs
1)∥as∥∞≥˜C0√1−β2
H+ϵ0√1−β2.
We then combine with (123) and Hin (11) to obtain that with probability at least 1−2δ,
1
TTX
s=1∥¯gs∥2≤4ˆH
T˜C0 p
2(σ2
0+σ2
1Hp+H2)√1−β2+ϵ0!s
logeT
δ
.
Finally, following the same deduction in (63) and (64), we could derive that ˆH∼ O
log2
T
ϵ0δ
from (114) and the desired results in Theorem 4.1.
E Convergence of Adam without corrective terms and RMSProp under
generalized smoothness
In this section, we will provide the detailed result and proof for the convergence of Algorithm 2
under generalized smoothness. First, we will provide the formal version of Theorem 4.2.
Theorem E.1. LetT≥1andδ∈(0,1/2). Suppose that {xs}s∈[T]is a sequence generated by
Algorithm 2, fis(L0, Lq)-smooth satisfying (8), Assumptions (A1)-(A3) hold, and
0≤β1< β2<1, β 2= 1−c/T, ϵ =ϵ0p
1−β2, η =˜C0p
1−β2,
˜C0≤min(
E0,E0
˜H,E0
˜L,s
β2(1−β1)2(1−β1/β2)
4L2qd)
, (124)
where c, ϵ0, E0,˜C0>0are constants, ¯His controlled by O
log
T
ϵ0δ
7, and ˜H,˜H,˜Lare defined
as
˜H:=L0/Lq+ 
4Lq¯Hq+ 
4Lq¯Hq
2−q+ 
4L0¯Hq
2+ 4Lq¯H+ 
4Lq¯H1
2−q+p
4L0¯H,
˜H:=s
2(σ2
0+σ2
1˜Hp+˜H2) logeT
δ
,˜L:=L0+Lq
˜Hq+˜H+L0
Lqq
. (125)
Then, it holds that with probability at least 1−2δ,
1
TTX
s=1∥∇f(xs)∥2≤˜O

¯H2(1 +σ2
1) +¯H
˜H+ϵ0
T+¯Hσ0√
T

. (126)
Remark E.2.(1). The convergence rate in Theorem C.2 is of order ˜O
1/T+σ0/√
T
, which can
be accelerated to ˜O(1/T)rate when σ0is sufficiently low.
(2). Setting β1= 0, Theorem E.1 then shows that RMSProp with the hyper-parameter setup in (124)
can also find a stationary point with the convergence rate of ˜O
1/T+σ0/√
T
order.
E.1 Proof detail
To start with, we follow all the notations defined in Appendix C and Appendix D, particularly
Gs,GT(s)in (31) and L(x)
s,L(y)
sin (97). Then, we have the following claims.
Proposition E.3. Setting ηs=ηandη=˜C0√1−β2, the results in Lemma D.1, Lemma D.2 and
Lemma D.3 remain unchanged.
Proof. Using ηs=ηand following the proof for the above lemmas, it’s easy to verify that these
lemmas still hold.
7The specific definition of ¯Hcan be found in (138).
32Based on these results, we are able to show that the gradient norm is bounded along the training
process generated by Algorithm 2 under generalized smoothness.
Proposition E.4. Under the same conditions in Theorem C.2, for any given δ∈(0,1/2), it holds
that with probability at least 1−2δ,
∥¯gt∥ ≤˜H,GT(t)≤˜H,L(x)
t≤ L(y)
t≤˜L,∀t∈[T+ 1], (127)
and
∆t+1≤ −η
4tX
s=1¯gs√˜as2
+¯H,∀t∈[T], (128)
Proof. To start with, we shall verify that (99) always holds. Recalling ηin (124) and Fin Lemma
D.1,
ηF=˜C0p
1−β2F≤s
β2(1−β1)2(1−β2)(1−β1/β2)
4L2qd·F≤1
Lq.
Hence, we make sure that the distance requirement in (8) always holds according to Lemma D.1.
Then, using the descent lemma in (101) and (68), it leads to for any t≥1,
f(yt+1)≤f(x1) +tX
s=1−η
∇f(ys),gs
˜bs
| {z }
˜A+β1
1−β1tX
s=1D
˜∆s⊙(xs−xs−1),∇f(ys)E
| {z }
˜B
+L(y)
s
2tX
s=1η·gs
˜bs−β1
1−β1(˜∆s⊙(xs−xs−1))2
| {z }
˜C′, (129)
The following estimation is based on the probability event in Lemma B.6.
Bounding ˜A.We first have the same decomposition as in (72). Then, setting λ=√1−β2/(3η˜H)
in (74), we obtain that with probability at least 1−δ,
˜A.1.1≤ηGT(t)
4˜HtX
s=1¯gs√˜as2
+3η˜H√1−β2logT
δ
,∀t∈[T]. (130)
Using Lemma C.5 and following the similar deduction for bounding (˜A.1.2)in (76), we have
˜A.1.2≤η
4tX
s=1¯gs√˜as2
+ηp
1−β2GT(t)tX
s=1gs
˜bs2
. (131)
Combining with (130) and (131), we obtain that
˜A.1≤GT(t)
4˜H−3
4tX
s=1η¯gs√˜as2
+3η˜H√1−β2logT
δ
+ηp
1−β2GT(t)tX
s=1gs
˜bs2
.
(132)
The estimation for ˜A.2is similar to (108) and (78). With Lreplaced by L(x)
sin (78),
˜A.2≤tX
s=1L(x)
sη2
2(1−β1)2ms−1
˜bs−12
+tX
s=1L(x)
sη2
2gs
˜bs2
. (133)
33Bounding ˜B,˜C′.Following the same decomposition in (79) and (80), we first note that the esti-
mation for ˜B.1in (83) remains unchangedL
˜B.1≤η
4tX
s=1¯gs√˜as2
+2ηGT(t)√1−β2
(1−β1)2tX
s=1 ms−1
˜bs2
+ms−1
˜bs−12!
. (134)
Following the similar deduction in (84) and using Lemma B.1, we can replace LwithL(x)
sto obtain
that
˜B.2≤tX
s=1Σ2
maxL(x)
sη2
(1−β1)2ˆms−1
bs−12
≤tX
s=1L(x)
sη2
β2(1−β1)2ˆms−1
˜bs−12
. (135)
Following the similar deduction in (110), we indeed replace LwithL(y)
sin (85)
˜C′≤η2tX
s=1L(y)
sgs
˜bs2
+η2
β2(1−β1)2tX
s=1L(y)
sms−1
˜bs−12
. (136)
Putting together. Noting that the above results are established based on probability events in
Lemma B.6 and (130). Hence, plugging (132), (133), (134), (135) and (136) into (129) and using
β2<1andL(x)
s≤ L(y)
sfrom (97), it holds that with probability at least 1−2δ, for all t∈[T],
f(yt+1)≤f(x1) +ηGT(t)
4˜H−1
2tX
s=1¯gs√˜as2
+3η˜H√1−β2logT
δ
+tX
s=1 
2η√1−β2GT(t)
(1−β1)2+5L(y)
sη2
2β2(1−β1)2!ms−1
˜bs−12
+2η√1−β2GT(t)
(1−β1)2tX
s=1ms−1
˜bs2
+tX
s=1 
ηp
1−β2GT(t) +3L(y)
sη2
2!gs
˜bs2
.
(137)
An induction argument. We first provide the detailed expression of ¯H:
¯H:=f(x1)−f∗+ 3E0logT
δ
+2E0
(1−β1)(1−β1/β2)+5E2
0
2β2(1−β1)(1−β1/β2)
dlogI(T)
βT
2
+2E0d
β2(1−β1)(1−β1/β2)logI(T)
βT
2
+
E0+3E2
0
2d
1−β1/β2logI(T)
βT
2
I(t) = 1 +2M2
T
ϵ2
σ2
0t+σ2
1t
∥¯g1∥+t˜M′p
+t
∥¯g1∥+t˜M′2
,˜M′=E0s
d
1−β1/β2.
(138)
The induction then begins by noting that from Lemma D.3 and ˜Hin (125),
G1=∥¯g1∥ ≤4Lq(f(x1)−f∗) + (4 Lq(f(x1)−f∗))1
2−q+p
4L0(f(x1)−f∗)≤˜H.
Suppose that for some t∈[T],
Gs≤˜H,∀s∈[t]. (139)
Consequently, recalling GT(s)in (19), L(x)
s,L(y)
sin (97) and H,Lin (11),
GT(s)≤˜H,L(x)
s≤ L(y)
s≤˜L,∀s∈[t]. (140)
Further recalling the setting of ˜C0in (10), with a simple calculation it holds that,
˜C0H≤˜C0˜H ≤E0,˜C0˜L ≤E0,˜C2
0˜L ≤E2
0,˜C0ϵ0≤E0ϵ0. (141)
34Using (140), (141), J(t),˜M′
tdefined in Lemma D.5 and I(t),˜M′defined in (138),
˜Mt≤˜M′,logFi(t)
βt
2
≤logJ(t)
βt
2
≤log ˜I(T)
βT
2!
,∀t≤T, i∈[d]
Using Lemma B.3, Lemma B.10 and Lemma D.5, we have
tX
s=1ms−1
˜bs−12
≤d(1−β1)
(1−β2)(1−β1/β2)logI(T)
βT
2
,
tX
s=1ms−1
˜bs2
≤d(1−β1)
β2(1−β2)(1−β1/β2)logI(T)
βT
2
,
tX
s=1gs
˜bs2
≤d
(1−β2)(1−β1/β2)logI(T)
βT
2
. (142)
Then, plugging (140) and (142) into (137), and using η=˜C0√1−β2and (141),
∆t+1=f(yt+1)−f∗≤∆1−η
4tX
s=1¯gs√˜as2
+ 3E0logT
δ
+2E0
(1−β1)(1−β1/β2)+5E2
0
2β2(1−β1)(1−β1/β2)
dlogI(T)
βT
2
+2E0d
β2(1−β1)(1−β1/β2)logI(T)
βT
2
+
E0+3E2
0
2d
1−β1/β2logI(T)
βT
2
≤¯H,
(143)
where we use ¯Hdefined in (138). Further using Lemma D.2, Lemma D.3 and ˜Hin (125),
∥¯gt+1∥ ≤L0/Lq+ (4Lq∆t+1)q+ (4Lq∆t+1)q
2−q
+ (4L0∆t+1)q
2+ 4Lq∆t+1+ (4Lq∆t+1)1
2−q+p
4L0∆t+1≤˜H.
We then deduce that Gt+1= max {Gt,∥¯gt+1∥} ≤ ˜H. The induction is then complete and we obtain
the desired result in (127). Finally, as an intermediate result of the proof, we obtain that (143) holds
as well.
E.2 Proof of the main result
Now we are ready to prove the main convergence result.
Proof of Theorem C.2. In what follows, we will assume that Proposition E.4 holds. Hence, the final
convergence bound also holds with probability at least 1−2δ. We set t=Tin (127),
η
4TX
s=1∥¯gs∥2
∥˜as∥∞≤η
4TX
s=1¯gs√˜as2
≤¯H−∆t+1≤¯H. (144)
Following the similar deduction in (95), with η=˜C0√1−β2, ϵ=ϵ0√1−β2, we have for any
s∈[T],
∥˜as∥∞
η≤MT
C0vuut2(1 + σ2
1)TX
j=1∥¯gj∥2+MTσ0√
2T+˜H+ϵ0
C0. (145)
We therefore combine with (144) and (145) to obtain that
TX
s=1∥¯gs∥2≤4¯H
˜C0
MTvuut2(1 + σ2
1)TX
j=1∥¯gj∥2+MTσ0√
2T+˜H+ϵ0

≤1
2TX
s=1∥¯gs∥2+8¯H2MT(1 +σ2
1)
˜C2
0+4¯H
˜C0
MTσ0√
2T+˜H+ϵ0
, (146)
35which leads to
1
TTX
s=1∥¯gs∥2≤1
˜C0T
16¯H2MT(1 +σ2
1) + 8 ¯H(˜H+ϵ0)
+8√
2¯HMTσ0
˜C0√
T. (147)
Following the result in (63) and (64), we show that when β2= 1−c/T,¯H,˜H ∼ O (poly(log T))
with respect to T. Finally, using the convergence result in (147), we obtain the desired result.
F Omitted proof in Appendix B
F.1 Omitted proof in Appendix B.1
Proof of Lemma B.1. We fix arbitrary i∈[d]and have the following two cases. Whenηsbs−1,i
ηs−1bs,i<1,
we haveηsbs−1,i
ηs−1bs,i−1= 1−ηsbs−1,i
ηs−1bs,i<1.
Whenηsbs−1,i
ηs−1bs,i≥1, letr=βs−1
2. Since 0<1−βs−1
1<1−βs
1,∀s≥2, then we have
ηs
ηs−1=s
1−βs
2
1−βs−1
2·1−βs−1
1
1−βs
1≤s
1 +βs−1
2(1−β2)
1−βs−1
2=r
1 + (1 −β2)·r
1−r.
Since h(r) =r/(1−r)is increasing as rgrows and rtakes the maximum value when s= 2. Hence,
it holds that
ηs
ηs−1≤s
1 + (1 −β2)·β2
1−β2=p
1 +β2. (148)
Then, since ϵs−1≤ϵs, we further have
bs−1,i
bs,i=ϵs−1+√vs−1,i
ϵs+q
β2vs−1,i+ (1−β2)g2
s,i≤ϵs+√vs−1,i
ϵs+p
β2vs−1,i≤1√β2. (149)
Combining with (148) and (149), we have
ηsbs−1,i
ηs−1bs,i−1=ηsbs−1,i
ηs−1bs,i−1≤s
1 +β2
β2−1.
Combining the two cases and noting that the bound holds for any i∈[d], we then obtain the desired
result.
Proof of Lemma B.2. Denoting ˜M=Ps−1
j=1βs−1−j
1 and applying (28) with ˆMandαjreplaced by
˜Mandgj,irespectively,

s−1X
j=1βs−1−j
1 gj,i
2
≤˜M·s−1X
j=1βs−1−j
1 g2
j,i. (150)
Hence, combining with the definition of bs,iin (16), we further have for any i∈[d]ands≥2,
ms−1,i
bs−1,i≤ms−1,i√vs−1,i=vuuut(1−β1)2Ps−1
j=1βs−1−j
1 gj,i2
(1−β2)Ps−1
j=1βs−1−j
2 g2
j,i
≤1−β1√1−β2vuut˜M·Ps−1
j=1βs−1−j
1 g2
j,iPs−1
j=1βs−1−j
2 g2
j,i≤1−β1√1−β2vuut˜M·s−1X
j=1β1
β2s−1−j
=1−β1√1−β2s
1−βs−1
1
1−β1·1−(β1/β2)s−1
1−β1/β2≤s
(1−β1)(1−βs−1
1)
(1−β2)(1−β1/β2),
36where the last inequality applies β1< β2. We thus prove the first result. To prove the second result,
from the smoothness of f,
∥¯gs∥ ≤ ∥ ¯gs−1∥+∥¯gs−¯gs−1∥ ≤ ∥ ¯gs−1∥+L∥xs−xs−1∥. (151)
Combining with (30) and η=C0√1−β2,
∥xs−xs−1∥∞≤ηs−1ms−1
bs−1
∞≤ηs
1
(1−β2)(1−β1/β2)=C0s
1
1−β1/β2. (152)
Using∥xs−xs−1∥ ≤√
d∥xs−xs−1∥∞and (151),
∥¯gs∥ ≤ ∥ ¯gs−1∥+LC0s
d
1−β1/β2≤ ∥¯g1∥+LC0ss
d
1−β1/β2.
Proof of Lemma B.3. Recalling the updated rule and the definition of bs,iin (16), using ϵ2
s=ϵ2(1−
βs
2)≥ϵ2(1−β2),
b2
s,i≥v2
s,i+ϵ2
s≥(1−β2)
sX
j=1βs−j
2g2
j,i+ϵ2
,and ms,i= (1−β1)sX
j=1βs−j
1gj,i.(153)
Proof for the first summation. Using (153), for any i∈[d],
tX
s=1g2
s,i
b2
s,i≤1
1−β2tX
s=1g2
s,i
ϵ2+Ps
j=1βs−j
2g2
j,i.
Applying Lemma A.1 and recalling the definition of Fi(t),
tX
s=1g2
s,i
b2
s,i≤1
1−β2"
log 
1 +1
ϵ2tX
s=1βt−s
2g2
s,i!
−tlogβ2#
≤1
1−β2logFi(t)
βt
2
.
Summing over i∈[d], we obtain the first desired result.
Proof for the second summation. Following from (153),
tX
s=1m2
s,i
b2
s,i≤(1−β1)2
1−β2·tX
s=1Ps
j=1βs−j
1gj,i2
ϵ2+Ps
j=1βs−j
2g2
j,i.
Applying Lemma A.2 and β2≤1,
tX
s=1m2
s,i
b2
s,i≤(1−β1)2
1−β2·1
(1−β1)(1−β1/β2)"
log 
1 +1
ϵ2tX
s=1βt−s
2g2
s,i!
−tlogβ2#
=1−β1
(1−β2)(1−β1/β2)logFi(t)
βt
2
.
Summing over i∈[d], we obtain the second desired result.
Proof for the third summation. Following from (153),
tX
s=1m2
s,i
b2
s+1,i≤tX
s=1h
(1−β1)Ps
j=1βs−j
1gj,ii2
ϵ2(1−β2) + (1 −β2)Ps+1
j=1βs+1−j
2 g2
j,i
≤tX
s=1(1−β1)2Ps
j=1βs−j
1gj,i2
ϵ2(1−β2) + (1 −β2)β2Ps
j=1βs−j
2g2
j,i.
37Applying Lemma A.2, and using β2≤1,
tX
s=1m2
s,i
b2
s+1,i≤(1−β1)2
(1−β2)β2·tX
s=1Ps
j=1βs−j
1gj,i2
ϵ2
β2+Ps
j=1βs−j
2g2
j,i
≤(1−β1)2
(1−β2)β2·1
(1−β1)(1−β1/β2)"
log 
1 +β2
ϵ2tX
s=1βt−s
2g2
s,i!
−tlogβ2#
≤1−β1
β2(1−β2)(1−β1/β2)logFi(t)
βt
2
.
Summing over i∈[d], we obtain the third desired result.
Proof for the fourth summation. Following the definition of ˆms,ifrom (29), and combining with
(153),
tX
s=1ˆm2
s,i
b2
s,i≤(1−β1)2
1−β2·tX
s=1
1
1−βs
1Ps
j=1βs−j
1gj,i2
ϵ2+Ps
j=1βs−j
2g2
j,i.
Applying Lemma A.2 and using β2≤1,
tX
s=1ˆm2
s,i
b2
s,i≤(1−β1)2
1−β2·1
(1−β1)2(1−β1/β2)"
log 
1 +1
ϵ2tX
s=1βt−s
2g2
s,i!
−tlogβ2#
≤1
(1−β2)(1−β1/β2)logFi(t)
βt
2
.
Summing over i∈[d], we obtain the fourth desired result.
Proof of Lemma B.4. Letˆx=x−1
L∇f(x). Then using the descent lemma of smoothness,
f(ˆx)≤f(x) +⟨∇f(x),ˆx−x⟩+L
2∥ˆx−x∥2≤f(x)−1
2L∥∇f(x)∥2.
Re-arranging the order, and noting that f(ˆx)≥f∗,
∥∇f(x)∥2≤2L(f(x)−f(ˆx))≤2L(f(x)−f∗).
Proof of Lemma B.5. Applying the norm inequality and the smoothness of f,
∥∇f(xs)∥ ≤ ∥∇ f(ys)∥+∥∇f(xs)− ∇f(ys)∥ ≤ ∥∇ f(ys)∥+L∥ys−xs∥.
Combining with the definition of ysin (17) and (152), and using β1∈[0,1), we obtain the desired
result that
∥∇f(xs)∥ ≤ ∥∇ f(ys)∥+Lβ1
1−β1∥xs−xs−1∥ ≤ ∥∇ f(ys)∥+LC0√
d
(1−β1)p
1−β1/β2.
F.2 Omitted proof in Appendix B.3
Proof of Lemma B.6. Let us denote γs=∥ξs∥2
σ2
0+σ2
1∥¯gs∥p,∀s∈[T]. Then from Assumption (A3), we
first have Ezs[exp ( γs)]≤exp(1) . Taking full expectation,
E[exp( γs)]≤exp(1) .
38By Markov’s inequality, for any A∈R,
P
max
s∈[T]γs≥A
=P
exp
max
s∈[T]γs
≥exp(A)
≤exp(−A)E
exp
max
s∈[T]γs
≤exp(−A)E"TX
s=1exp (γs)#
≤exp(−A)Texp(1) ,
which leads to that with probability at least 1−δ,
∥ξs∥2≤logeT
δ 
σ2
0+σ2
1∥¯gs∥p
,∀s∈[T].
Proof of Lemma B.7. Recalling the definitions of asin (23) and ϵsin Algorithm 1, we have for any
s∈[T], i∈[d],
1
as,i≤1
GT(s)√1−β2+ϵp
1−βs
2≤1
(GT(s) +ϵ)√1−β2
≤1
GT(s)√1−β2≤1p
σ2
0+σ2
1∥¯gs∥p√1−β2. (154)
Then given any i∈[d], we set
Xs=−ηs
¯gs,ξs
as
, ωs=ηs¯gs
asq
σ2
0+σ2
1∥¯gs∥p,∀s∈[T].
Noting that ¯gs,asandηsare random variables dependent by z1,···,zs−1andξsis only dependent
onzs. We then verify that Xsis a martingale difference sequence since
E[Xs|z1,···,zs−1] =Ezs
−ηs
¯gs,ξs
as
=−ηs
¯gs,Ezs[ξs]
as
= 0.
Noting that ωsis a random variable only dependent by z1,···,zs−1and applying Assumption (A3)
and Cauchy-Schwarz inequality, we have
E
expX2
s
ω2s
|z1,···,zs−1
≤E
expξ2
s
σ2
0+σ2
1∥¯gs∥p
|z1,···,zs−1
≤Ezs
exp∥ξs∥2
σ2
0+σ2
1∥¯gs∥p
≤exp(1) ,∀s∈[T].
Applying Lemma A.3 and (154), we have that for any λ >0, with probability at least 1−δ,
tX
s=1Xs≤3λ
4tX
s=1ω2
s+1
λlog1
δ
≤3λ
4√1−β2tX
s=1η2
s¯gs
as2
(σ2
0+σ2
1∥¯gs∥p) +1
λlog1
δ
≤3λ
4√1−β2tX
s=1η2
s¯gs√as2q
σ2
0+σ2
1∥¯gs∥p+1
λlog1
δ
. (155)
Note that for any t∈[T], (155) holds with probability at least 1−δ. Then for any fixed λ >0, we
could re-scale δto obtain that with probability at least 1−δ, for all t∈[T],
tX
s=1Xs≤3λ
4√1−β2tX
s=1η2
s¯gs√as2q
σ2
0+σ2
1∥¯gs∥p+1
λlogT
δ
.
Usingp
σ2
0+σ2
1∥¯gs∥p≤ GT(t), s≤tfrom (19), together with (30), we have that with probability
at least 1−δ,
−tX
s=1ηs
¯gs,ξs
as
≤3ληGT(t)
4(1−β1)√1−β2tX
s=1ηs¯gs√as2
+1
λlogT
δ
,∀t∈[T].
Finally setting λ= (1−β1)√1−β2/(3ηGT), we then have the desired result in (38).
39F.3 Omitted proof of Appendix B.4
Proof of Lemma B.8. First directly applying (36) and Gsin (19), for any j∈[s],
∥ξj∥ ≤MTq
σ2
0+σ2
1∥¯gj∥p≤MTq
σ2
0+σ2
1Gp
j≤MTq
σ2
0+σ2
1Gp
s≤ GT(s).
Applying the basic inequality, (36) and MT≥1, for any j∈[s],
∥gj∥2≤2∥¯gj∥2+ 2∥ξj∥2≤2M2
T 
σ2
0+σ2
1∥¯gj∥p+∥¯gj∥2
≤(GT(s))2.
Finally, we would use an induction argument to prove the last result. Given any i∈[d], noting that
v1,i= (1−β2)g2
1,i≤(GT(s))2. Suppose that for some s′∈[s],vj,i≤(GT(s))2,∀j∈[s′],
vs′+1,i=β2vs′,i+ (1−β2)g2
s′,i≤β2(GT(s))2+ (1−β2)(GT(s))2≤(GT(s))2.
We then obtain that vj,i≤(GT(s))2,∀j∈[s]. Noting that the above inequality holds for all i∈[d],
we therefore obtain the desired result.
Proof of Lemma B.9. Recalling the definition of bs,iin (16) and letting as,i=p˜vs,i+ϵsin (23),
1
as,i−1
bs,i=√vs,i−p˜vs,i
as,ibs,i=1−β2
as,ibs,ig2
s,i−(GT(s))2
√vs,i+p˜vs,i
≤1−β2
as,ibs,i·(GT(s))2
√vs,i+p
β2vs−1,i+ (1−β2)(GT(s))2≤GT(s)√1−β2
as,ibs,i,
where we apply g2
s,i≤ ∥gs∥2≤(GT(s))2from Lemma B.8 in the first inequality since (36) holds.
The second result also follows from the same analysis. We first combine with ϵs=ϵp
1−βs
2to
obtain that
|ϵs−ϵs−1| ≤ϵp
1−βs
2−q
1−βs−1
2
≤ϵq
βs−1
2(1−β2)≤ϵp
1−β2, (156)
where we apply√a−√
b≤√
a−b,∀0≤b≤a. Applying the definition of bs−1,iandas,i,
1
bs−1,i−1
as,i=p˜vs,i−√vs−1,i+ (ϵs−ϵs−1)
bs−1,ias,i
≤1
bs−1,ias,i(1−β2)(GT(s))2−vs−1,i
p˜vs,i+√vs−1,i+|ϵs−ϵs−1|
bs−1,ias,i
≤1
bs−1,ias,i·(1−β2)(GT(s))2
p˜vs,i+√vs−1,i+ϵ√1−β2
bs−1,ias,i≤(GT(s) +ϵ)√1−β2
bs−1,ias,i.
where the second inequality applies vs−1,i≤(GT(s))2in Lemma B.8 and the last inequality comes
from√1−β2GT(s)≤˜vs,i.
Proof of Lemma B.10. Applying the basic inequality and (36), for all t∈[T], i∈[d],
tX
s=1g2
s,i≤tX
s=1∥gs∥2≤2tX
s=1 
∥¯gs∥2+∥ξs∥2
≤2M2
T 
σ2
0t+σ2
1tX
s=1∥¯gs∥p+tX
s=1∥¯gs∥2!
.
(157)
Combining with Lemma B.2, we have
tX
s=1∥¯gs∥p≤tX
s=1 
∥¯g1∥+LC0√
dsp
1−β1/β2!p
≤t· 
∥¯g1∥+LC0√
dtp
1−β1/β2!p
tX
s=1∥¯gs∥2≤t· 
∥¯g1∥+LC0√
dtp
1−β1/β2!2
.
Further applying the definition of Fi(t)in Lemma B.3, it leads to Fi(t)≤ F(t),∀i∈[d]. Finally,
sinceF(t)is increasing with t, we obtain the desired result.
40Proof of Lemma B.11. First, we have the following decomposition,
A.1=−tX
s=1ηs¯gs√as2
−tX
s=1ηs
¯gs,ξs
as
| {z }
A.1.1+tX
s=1ηs
¯gs,1
as−1
bs
gs
| {z }
A.1.2. (158)
Since (36) holds, we could apply Cauchy-Schwarz inequality, Lemma B.9, and GT(s)≤
GT(t),∀s≤tfrom (19) to obtain that for all t∈[T],
A.1.2≤dX
i=1tX
s=1ηs1
as,i−1
bs,i· |¯gs,igs,i| ≤dX
i=1tX
s=1ηs·GT(s)√1−β2
as,ibs,i· |¯gs,igs,i|
≤1
4dX
i=1tX
s=1ηs¯g2
s,i
as,i+ (1−β2)dX
i=1tX
s=1(GT(s))2
as,i·ηsg2
s,i
b2
s,i
(30),(154)
≤1
4tX
s=1ηs¯gs√as2
+ηGT(t)√1−β2
1−β1tX
s=1gs
bs2
.
Finally, combining with (38) for estimating A.1.1 , we deduce the desired result in (40).
Proof of Lemma B.12. Let us denote Σ :=β1
1−β1⟨∆s⊙(xs−xs−1),¯gs⟩where ∆sis defined in
(20). We have
Σ≤β1
1−β1·
∆s⊙ηs−1ms−1
bs−1,¯gs=β1
1−β1·ηs
bs−ηs−1
bs−1
⊙ms−1,¯gs
≤β1
1−β1·ηs
bs−ηs
as
⊙ms−1,¯gs
| {z }
Σ1+β1
1−β1·ηs
as−ηs
bs−1
⊙ms−1,¯gs
| {z }
Σ2
+β1
1−β1·(ηs−1−ηs)ms−1
bs−1,¯gs
| {z }
Σ3. (159)
Since (36) holds, we could apply Lemma B.9 and Young’s inequality and then use (154), (30),
β1∈[0,1)andGT(s)≤ GT(t)≤ GT(t) +ϵ,∀s≤t,
Σ1≤dX
i=1β1
1−β1·GT(s)ηs√1−β2
as,ibs,i· |¯gs,ims−1,i|
≤dX
i=1ηs
8·¯g2
s,i
as,i+2ηsβ2
1(1−β2)
(1−β1)2dX
i=1(GT(s))2
as,i·m2
s−1,i
b2
s,i
≤ηs
8¯gs√as2
+2(GT(t) +ϵ)η√1−β2
(1−β1)3ms−1
bs2
. (160)
Using the similar analysis for Σ1, we also have
Σ2≤dX
i=1ηsβ1
1−β1√1−β2
as,ibs−1,i·(GT(s) +ϵ)· |¯gs,i·ms−1,i|
≤ηs
8¯gs√as2
+2 (GT(t) +ϵ)η√1−β2
(1−β1)3ms−1
bs−12
. (161)
41Then we move to bound the summation of Σ3overs∈ {2,···, t}sincem0= 0. Recalling ηsin
(30), we have the following decomposition,
Σ3≤ηβ1p
1−βs
2
1−β11
1−βs−1
1−1
1−βs
1
¯gs,ms−1
bs−1
| {z }
Σ3.1
+ηβ1
(1−β1)(1−βs−1
1)q
1−βs−1
2−p
1−βs
2
¯gs,ms−1
bs−1
| {z }
Σ3.2. (162)
Noting that ∥¯gs∥ ≤ Gs≤Gt,∀s≤t. Then further applying Cauchy-Schwarz inequality and
Lemma B.2,
p
1−βs
2
¯gs,ms−1
bs−1≤p
1−βs
2∥¯gs∥ms−1
bs−1≤√
dGts
(1−β1)(1−βs−1
1)
(1−β2)(1−β1/β2).
Hence, summing Σ3.1up over s∈[t], applying β1∈(0,1)and noting that Σ3.1vanishes when
s= 1,
tX
s=1Σ3.1≤√
dηG t
1−β1·s
1−β1
(1−β2)(1−β1/β2)tX
s=21
1−βs−1
1−1
1−βs
1
≤√
dηG tp
(1−β1)3(1−β2)(1−β1/β2). (163)
Similarly, using ∥¯gs∥ ≤Gs≤Gt,∀s≤tand1−βs−1
1≥1−β1,
1
1−βs−1
1
¯gs,ms−1
bs−1≤1
1−βs−1
1∥¯gs∥ms−1
bs−1≤√
dGts
1
(1−β2)(1−β1/β2).
Hence, summing Σ3.2up over s∈[t]and still applying β1∈[0,1),
tX
s=1Σ3.2≤√
dηG t
1−β1·s
1
(1−β2)(1−β1/β2)tX
s=2p
1−βs
2−q
1−βs−1
2
≤√
dηG t
(1−β1)p
(1−β2)(1−β1/β2)≤√
dηG tp
(1−β1)3(1−β2)(1−β1/β2). (164)
Combining with (162), (163) and (164), we obtain an upper bound forPt
s=1Σ3. Summing (159),
(160) and (161) up over s∈[t], and combining with the estimation forPt
s=1Σ3, we obtain the
desired inequality in (41).
G Omitted proof in Appendix D
Proof of Lemma D.1. Recalling in (152), we have already shown that
∥xs+1−xs∥ ≤√
d∥xs+1−xs∥∞≤ηs
d
(1−β2)(1−β1/β2),∀s≥1. (165)
Applying the definition of ysin (17), an intermediate result in (165) and β1∈[0,1),8
∥ys−xs∥=β1
1−β1∥xs−xs−1∥ ≤η
1−β1s
d
(1−β2)(1−β1/β2),∀s≥1. (166)
8The inequality still holds for s= 1sincex1=y1.
42Recalling the iteration of ysin (18) and then using Young’s inequality
∥ys+1−ys∥2≤2η2
sgs
bs2
|{z}
(∗)+2β2
1
(1−β1)2ηsbs−1
ηs−1bs−12
∞∥xs−xs−1∥2
| {z }
(∗∗).
Noting that gs,i/bs,i≤1/√1−β2from (16), we then combine with (30) to have
(∗)≤2η2
s·d
1−β2≤2η2d
(1−β1)2(1−β2).
Applying Lemma B.1 where Σ2
max≤1/β2and (165),
(∗∗)≤2η2β2
1Σ2
maxd
(1−β1)2(1−β2)(1−β1/β2)≤2η2d
β2(1−β1)2(1−β2)(1−β1/β2).
Summing up two estimations and using 0≤β1< β2<1, we finally have
∥ys+1−ys∥ ≤ηs
4d
β2(1−β1)2(1−β2)(1−β1/β2). (167)
Combining with (165), (166) and (167), and using 0≤β1< β 2<1, we then deduce a uniform
bound for all the three gaps.
Proof of Lemma D.2. Under the same conditions in Lemma D.1, we have
∥ys−xs∥ ≤1
Lq,∥ys+1−ys∥ ≤1
Lq.
Then, using the generalized smoothness in (8),
∥∇f(ys)∥ ≤ ∥∇ f(xs)∥+∥∇f(ys)− ∇f(xs)∥
≤ ∥∇ f(xs)∥+ (L0+Lq∥∇f(xs)∥q)∥ys−xs∥
≤ ∥∇ f(xs)∥+∥∇f(xs)∥q+L0/Lq.
We could use a similar argument to deduce the bound for ∥∇f(xs)∥. Further, combining with L(x)
s
andL(y)
sin (97), we could bound the generalized smooth parameters as
L0+Lq∥∇f(xs)∥q≤L0+LqGq
s=L(x)
s,
L0+Lq∥∇f(ys)∥q≤L0+Lq(∥∇f(xs)∥+∥∇f(xs)∥q+L0/Lq)q=L(y)
s. (168)
We could then deduce the first two inequalities in (100). Finally, (101) could be deduced by using
the same argument in the proof of [50, Lemma A.3].
Proof of Lemma D.3. Given any x∈Rd, we let
τ=1
L0+Lqmax{∥∇f(x)∥q,∥∇f(x)∥},ˆx=x−τ∇f(x).
From the definition of τ, we could easily verify that ∥ˆx−x∥=τ∥∇f(x)∥ ≤1/Lq. Since fis
(L0, Lq)-smooth, we could thereby use the descent lemma in [50, Lemma A.3] such that
f(ˆx)≤f(x) +⟨∇f(x),ˆx−x⟩+L0+Lq∥∇f(x)∥q
2∥ˆx−x∥2
=f(x)−τ∥∇f(x)∥2+(L0+Lq∥∇f(x)∥q)τ2
2∥∇f(x)∥2≤f(x)−τ
2∥∇f(x)∥2.
Since f(ˆx)≥f∗, when ∥∇f(x)∥= 0, the desired result is trivial. Let us suppose ∥∇f(x)∥>0.
43Case 1 ∥∇f(x)∥q>∥∇f(x)∥
τ
2∥∇f(x)∥2=∥∇f(x)∥2−q
2L0/∥∇f(x)∥q+ 2Lq≤f(x)−f(ˆx)≤f(x)−f∗.
When∥∇f(x)∥q< L 0/Lq, it leads to
∥∇f(x)∥2
4L0=∥∇f(x)∥2−q
4L0/∥∇f(x)∥q≤∥∇f(x)∥2−q
2L0/∥∇f(x)∥q+ 2Lq≤f(x)−f∗.
When∥∇f(x)∥q≥L0/Lq, it leads to
∥∇f(x)∥2−q
4Lq≤∥∇f(x)∥2−q
2L0/∥∇f(x)∥q+ 2Lq≤f(x)−f∗.
We then deduce that
∥∇f(x)∥ ≤maxn
[4Lq(f(x)−f∗)]1
2−q,p
4L0(f(x)−f∗)o
. (169)
Case 2 ∥∇f(x)∥q≤ ∥∇ f(x)∥We could rely on the similar analysis to obtain that9
∥∇f(x)∥ ≤maxn
4Lq(f(x)−f∗),p
4L0(f(x)−f∗)o
. (170)
Combining (169) and (170), we then deduce the desired result.
Proof of Lemma D.5. Recalling (152), we then obtained that when η=˜C0√1−β2,
∥xs−xs−1∥ ≤√
d∥xs−xs−1∥∞≤˜C0s
d
1−β1/β2. (171)
Noting that when (99) holds, we have
∥¯gs∥ ≤ ∥ ¯gs−1∥+∥¯gs−¯gs−1∥ ≤ ∥ ¯gs−1∥+ (L0+Lq∥¯gs−1∥q)∥xs−xs−1∥
≤ ∥¯gs−1∥+˜C0L(x)
s−1s
d
1−β1/β2≤ ∥¯g1∥+˜C0s
d
1−β1/β2s−1X
j=1L(x)
j.
UsingL(x)
j≤ L(x)
t,∀j≤t, we have
tX
s=1∥¯gs∥p≤tX
s=1 
∥¯g1∥+˜C0s
d
1−β1/β2(s−1)L(x)
t!p
≤t 
∥¯g1∥+t˜C0L(x)
ts
d
1−β1/β2!p
.
Similarly, we also have
tX
s=1∥¯gs∥2≤t 
∥¯g1∥+t˜C0L(x)
ts
d
1−β1/β2!2
.
Further combining with Fi(t)in Lemma B.3 and J(t)in (104),
Fi(t)≤1 +1
ϵ2tX
s=1∥gs∥2≤ J(t),∀t∈[T], i∈[d].
9We refer readers to see [52, Lemma A.5] for a detailed proof under this case.
44NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not re-
move the checklist: The papers not including the checklist will be desk rejected. The checklist
should follow the references and follow the (optional) supplemental material. The checklist does
NOT count towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
• [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evalu-
ation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No]
" provided a proper justification is given (e.g., "error bars are not reported because it would be too
computationally expensive" or "we were unable to find the license for the dataset we used"). In
general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased
in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your
best judgment and write a justification to elaborate. All supporting evidence can appear either in the
main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question,
in the justification please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
•Delete this instruction block, but keep the section heading “NeurIPS paper checklist" ,
•Keep the checklist subsection headings, questions/answers and guidelines below.
•Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: [NA]
45Guidelines:
• The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The au-
thors should reflect on how these assumptions might be violated in practice and what
the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
46• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
• While we encourage the release of code and data, we understand that this might not
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
47• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
48• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
cific groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper poses no such risks.
49• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
50Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
51