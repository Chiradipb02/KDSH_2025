Fast Last-Iterate Convergence of Learning in Games
Requires Forgetful Algorithms
Yang Cai
Yale
yang.cai@yale.eduGabriele Farina
MIT
gfarina@mit.eduJulien Grand-Clément
HEC Paris
grand-clement@hec.fr
Christian Kroer
Columbia
ck2945@columbia.eduChung-Wei Lee
USC
leechung@usc.eduHaipeng Luo
USC
haipengl@usc.edu
Weiqiang Zheng
Yale
weiqiang.zheng@yale.edu
Abstract
Self-play via online learning is one of the premier ways to solve large-scale two-
player zero-sum games, both in theory and practice. Particularly popular algo-
rithms include optimistic multiplicative weights update (OMWU) and optimistic
gradient-descent-ascent (OGDA). While both algorithms enjoy O(1/T)ergodic
convergence to Nash equilibrium in two-player zero-sum games, OMWU offers
several advantages including logarithmic dependence on the size of the payoff
matrix and ˜O(1/T)convergence to coarse correlated equilibria even in general-
sum games. However, in terms of last-iterate convergence in two-player zero-sum
games, an increasingly popular topic in this area, OGDA guarantees that the duality
gap shrinks at a rate of (1/√
T), while the best existing last-iterate convergence for
OMWU depends on some game-dependent constant that could be arbitrarily large.
This begs the question: is this potentially slow last-iterate convergence an inherent
disadvantage of OMWU, or is the current analysis too loose? Somewhat surpris-
ingly, we show that the former is true. More generally, we prove that a broad class
of algorithms that do not forget the past quickly all suffer the same issue: for any
arbitrarily small δ >0, there exists a 2×2matrix game such that the algorithm ad-
mits a constant duality gap even after 1/δrounds. This class of algorithms includes
OMWU and other standard optimistic follow-the-regularized-leader algorithms.
1 Introduction
Self-play via online learning is one of the premier ways to solve large-scale two-player zero-sum
games. Major examples include super-human AIs for Go, Poker [Brown and Sandholm, 2018], and
human-level AI for Stratego [Perolat et al., 2022] and alignment of large language models [Munos
et al., 2023]. In particular, Optimistic Multiplicative Weights Update (OMWU) and Optimistic
Gradient Descent-Ascent (OGDA) are two of the most well-known online learning algorithms. When
applied to learning a two-player zero-sum game via self-play for Trounds, the average iterates of
both algorithms are known to be an O(1/T)-approximate Nash equilibrium [Rakhlin and Sridharan,
2013, Syrgkanis et al., 2015], while other algorithms, such as vanilla Multiplicative Weights Update
38th Conference on Neural Information Processing Systems (NeurIPS 2024).(MWU) and vanilla Gradient Descent-Ascent (GDA), have a slower ergodic convergence rate of
O(1/√
T).
For multiple practical reasons, there is growing interest in studying the last-iterate convergence of
these learning dynamics [Daskalakis and Panageas, 2019, Golowich et al., 2020b, Wei et al., 2021,
Lee et al., 2021]. In this regard, existing results seemingly exhibit a gap between OGDA and OMWU
— the duality gap of the last iterate of OGDA is known to decrease at a rate of O(1/√
T)[Cai
et al., 2022, Gorbunov et al., 2022], with no dependence on constants beyond the dimension and
the smoothness of the players’ utility functions of the game.1In contrast, the existing convergence
rate for OMWU depends on some game-dependent constant that could be arbitrarily large, even
after fixing the dimension and the smoothness constant of the game [Wei et al., 2021].2Given the
fundamental role of OMWU in online learning and its other advantages over OGDA (such as its
logarithmic dependence on the number of actions), it is natural to ask the following question:
Is the potentially slow last-iterate convergence an inherent disadvantage of OMWU? (*)
Main Results. In this work, we show that the answer to this question is yes, contrary to a common
belief that better analysis and better last-iterate convergence results similar to those of OGDA are
possible for OMWU. More specifically, we show the following.
Theorem (Informal) .For OMWU with constant step size, there is no function fsuch that the
corresponding learning dynamics {(xt, yt)}t≥1in two-player zero-sum games [0,1]d1×d2has a
last-iterate convergence rate of f(d1, d2, T).3More specifically, no function fcan satisfy
1.DualityGap( xT, yT)≤f(d1, d2, T)for all matrices [0,1]d1×d2andT≥1.
2.limT→∞f(d1, d2, T)→0.
Our findings show that, despite the significantly superior regret properties of OMWU compared
to OGDA, its last-iterate convergence properties are remarkably worse. In turn, this counters
the viewpoint that “Follow-the-Regularized-Leader (FTRL) is better than Online Mirror Descent
(OMD)” [van Erven, 2021]: crucially, while OMWU is an instance of (optimistic) FTRL, OGDA is
an instance of optimistic OMD that cannot be expressed in the FTRL formalism.
We further show that similar negative results extend to several other standard online learning algo-
rithms, including a close variant of OGDA. More concretely, our main results are as follows.
•We identify a broad family of Optimistic FTRL (OFTRL) algorithms that do not forget about the
past quickly. We prove that, for any sufficiently small δ >0, there exists a 2×2two-player zero-sum
game such that, even after 1/δiterations, the duality gap of the iterate output by these algorithms
is still a constant (Theorem 1). This excludes the possibility of showing a game-independent
last-iterate convergence rate similar to that of OGDA.
•We prove that many standard online learning algorithms, such as OFTRL with the entropy regu-
larizer (equivalently, OMWU), the Tsallis entropy family of regularizers, the log regularizer, and
the squared Euclidean norm regularizer, all fall into this family of non-forgetful algorithms and
thus all suffer from the same slow convergence. Also note that Optimistic OMD (OOMD), another
well-known family of algorithms, is equivalent to OFTRL when given a Legendre regularizer.
Therefore, OOMD with the entropy, Tsallis entropy, and log regularizer also suffer the same issue.4
•Finally, we also generalize our negative results from 2×2games to 2n×2ngames for any positive
integer n, strengthening our message that forgetfulness is generally needed in order to achieve fast
last-iterate convergence.
1In finite two-player zero-sum games, the dependence is polynomial in the number of actions and the largest
absolute value in the payoff matrix.
2We note that there are also linear-rate last-iterate results for OGDA when we allow dependence on such
constants; see [Wei et al., 2021].
3Under the same condition, OGDA has a last-iterate convergence rate ofpoly(d1,d2)√
T.
4We focus on optimistic variants of these algorithms since it is well-known that their vanilla version does not
converge in the last iterate at all, see e.g. [Mertikopoulos et al., 2018, Daskalakis and Panageas, 2018, Bailey and
Piliouras, 2018, Cheung and Piliouras, 2019].
20.6 0.8 1.0
xt[1]0.00.20.40.60.81.0yt[1]Entropy (OMWU)
101103
Iteration10−210−1Equilibrium gap
1
0.6 0.8 1.0
xt[1]0.00.20.40.60.81.0Log regularizer
101103
Iteration10−210−1
1
0.6 0.8 1.0
xt[1]0.00.20.40.60.81.0Sq. Euclidean norm
101103
Iteration10−210−1
1
0.6 0.8 1.0
xt[1]0.00.20.40.60.81.0OGDA
101103
Iteration10−410−310−210−1
1Figure 1: Comparison of the dynamics produced by three variants of OFTRL with different regu-
larizers (negative entropy, logarithmic regularizer, and squared Euclidean norm) and OGDA in the
same game Aδdefined in (2) for δ:= 10−2. The bottom row shows the duality gap achieved by the
last iterates. The OFTRL variants exhibit poor performance due to their lack of forgetfulness , while
OGDA converges quickly to the Nash equilibrium. Since the regularizers in the first two plots are
Legendre, the dynamics are equivalent to the ones produced by optimistic OMD with the respective
Bregman divergences. In the plot for OMWU we observe that xt[1]can get extremely close to the
boundary ( e.g., in the range 1−e−50< xt[1]<1). To correctly simulate the dynamics, we used
1000 digits of precision. The red star, blue dot, and green square illustrate the key times T1,T2,T3
defined in our analysis in Section 3.
100101102103104
Iteration10−210−1Equilibrium gapδ= 0.05
100101102103104
Iterationδ= 0.01
100101102103104
Iterationδ= 0.005
1
Figure 2: Performance of OMWU on the game Aδdefined in eq. (2) for three choices of δ. In all
plots, the learning rate was set to η= 0.1. As predicted by our analysis, the length of the “flat region”
between iteration T1(red star) and T2(blue dot) scales inversely proportionally with δ.
Main Ideas. Intuitively, we trace the poor last-iterate convergence properties of OFTRL to its lack
of forgetfulness . The high-level idea of our hard 2×2game instance, parametrized by δ >0, is as
follows. First, it has a unique Nash equilibrium at which one player is O(δ)close to the boundary of
the simplex. We refer to the first row of plots in Figure 1, where the equilibrium is noted by a blue dot
(note that we can plot only x[1], y[1]for each player, since x[2] = 1 −x[1]andy[2] = 1 −y[2]). As
can be seen, the iterates of OGDA and all three OFTRL variants initially have a two-phase structure.
In the first phase, they converge to the lower-right area denoted by a red star in Figure 1. Then, from
there all algorithms start moving towards the equilibrium. In particular, y[1]increases. However,
once they enter the vicinity of the equilibrium, the behavior depends on the algorithms. For OGDA,
the dynamics start spiraling closer and closer to the equilibrium. On the other hand, for the OFTRL
algorithm, the xplayer has built up a lot of “memory" of x[1]being better than x[2], and for this
reason, x[1]will stay very close to 1for a long time. During the time when x[1]is close to 1,y[1]
keeps increasing since the yplayer receives gradients that indicates y[1]is better than y[2]. As a
3result, the dynamics cannot “stop” near the equilibrium but start to move away from the equilibrium.
The dynamics reach a point (denoted by a green square) whose duality gap is a constant and enter a
new cycle where they move out towards the starting point of the learning process. This cycle repeats
in smaller and smaller semi-ellipses that slowly converge to equilibrium. Note that the semi-ellipses
correspond to the seesaw pattern in the equilibrium gap (second row of plots). OFTRL overshoots the
equilibrium as it has built up a lot of "memory" of x[1]being better than x[2]along the phase from
the red star to the blue circle, and it requires many iterations to "forget" this fact. We show that as
we make δ, the parameter defining the nearness to the boundary, smaller and smaller, it takes longer
and longer for these semi-ellipses to get close to the equilibrium along the entire path, as illustrated
in Figure 2.
Our results are related to numerical observations made in the literature on solving large-scale extensive-
form games. There, algorithms based on the regret-matching+(RM+) algorithm [Tammelin et al.,
2015], combined with the counterfactual regret minimization [Zinkevich et al., 2007], perform by
far the best in practice. In contrast, the classical regret matching algorithm [Hart and Mas-Colell,
2000] performs much worse, in spite of similar regret guarantees. It was later discovered that RM+
corresponds to OGD, while RM corresponds to FTRL [Farina et al., 2021, Flaspohler et al., 2021]. It
was hypothesized that RM builds up too much negative regret at times, and thus is slow to adapt to
changes in the learning dynamics related to the strategy of the other player. These numerical results
and the hypothesis are consistent with our theoretical findings: FTRL (and thus RM) is not able to
“forget,” whereas OGD and OGDA can forget and thereby quickly adapt to changes in which actions
should be played.
1.1 Related Work
The literature on last-iterate convergence of online learning methods in games is vast. In this section,
we will cover key contributions focusing on the case of interest for this paper: discrete-time dynamics
for two-player zero-sum normal-form games.
Convergence of OGDA. Average-iterate convergence of OGDA has been studied for minimax opti-
mization problems in both the unconstrained [Mokhtari et al., 2020] and constrained settings [Hsieh
et al., 2019]. Last-iterate convergence of OGDA in unconstrained saddle-point problems has been
shown in [Daskalakis et al., 2018, Golowich et al., 2020a]. In the (constrained) game setting, Wei et al.
[2021], Anagnostides et al. [2022] showed best-iterate convergence to the set of Nash equilibria in
any two-player zero-sum game with payoff matrix Aat a rate of O(poly(d1, d2,max i,j|Ai,j|)/√
T)
using constant learning rate, where d1andd2are the number of actions of the players. A stronger
result was shown by Cai et al. [2022], who showed that the same rate applies to the lastiterate.
Convergence of OMWU. Optimistic multiplicative weights update (also known as optimistic hedge) is
often regarded as the premier algorithm for learning in games. Unlike OGDA, it guarantees sublinear
regret with a logarithmic dependence on the number of actions, and it is known to guarantee only
polylogarithmic regret per player when used in self play even for general-sum games [Daskalakis et al.,
2021]. It can be applied with similar strong properties beyond normal-form games in several important
combinatorial settings [Takimoto and Warmuth, 2003, Koolen et al., 2010, Farina et al., 2022]. The
work by Daskalakis and Panageas [2019] established asymptotic last-iterate convergence for OMWU
in games using a small learning rate under the assumption of a unique Nash equilibrium. Similar
asymptotic results without the unique equilibrium assumption were also given by Mertikopoulos et al.
[2019], Hsieh et al. [2021]. Wei et al. [2021] were the first to provide nonasymptotic learning rates for
OMWU. Specifically, they showed a linear rate of convergence in games with a unique equilibrium,
albeit with a dependence on a condition number-like quantity that could be arbitrarily large given
fixed d1,d2, and max i,j|Ai,j|.This result was later extended by Lee et al. [2021] to extensive-form
games. Unlike OGDA, no last-iterate convergence result for OMWU with a polynomial dependence
on only the natural parameters of the game ( i.e.,d1,d2, and max i,j|Ai,j|) is known. As we show
in this paper, perhaps surprisingly, this is no coincidence: in general, OMWU does not exhibit a
last-iterate convergence rate that solely depends on these parameters, whether polynomial or not.
FTRL vs. OMD. While the last-iterate convergence of instantiations of Optimistic Online Mirror
Descent has been observed before, the properties of Follow-the-Regularized-Leader dynamics remain
mostly elusive. The present paper partly explains this vacuum: all standard instantiations of opti-
mistic FTRL cannot hope to converge in iterates with only a polynomial dependence on the natural
parameters of the game, unlike optimistic OMD. Complications in obtaining last-iterate convergence
4results for continuous-time FTRL instantiations were already reported by Vlatakis-Gkaragkounis
et al. [2020], who showed the necessity of strict Nash equilibria.
Exploiting a no-regret learner. The forgetfulness property that we identify is closely related to the
concept of mean-based learning algorithms from Braverman et al. [2018]. Intuitively, mean-based
algorithms are ones such that if the mean reward for action ais significantly greater than the mean
reward for action b, then the algorithm selects bwith negligible probability. They show that MWU is
mean-based, along with Follow-the-Perturbed-Leader and the Exp3 bandit algorithm. Braverman
et al. [2018] shows that "mean-based" algorithms are exploitable when learning to bid in first-price
auctions, whereas Kumar et al. [2024] shows that OGD does not suffer from this exploitability issue.
2 Preliminaries and Problem Setup
We consider the standard setting of no-regret learning in a zero-sum game A∈[0,1]d1×d2. In
each iteration t≥1, thex-player chooses xt∈ X := ∆d1while the y-player chooses yt∈ Y:=
∆d2. Then the x-player receives loss vector ℓt
x=Aytwhile the y-player receives loss vector
ℓt
y=−A⊤xt. The goal is to find or approximate a Nash equilibrium (x∗, y∗)to the game such
thatx∗∈argminx∈Xmax y∈Yx⊤Ayandy∗∈argmaxy∈Yminx∈Xx⊤Ay. The approximation
error of a strategy pair (x, y)is measured by its duality gap, defined as DualityGap( x, y) =
max y′∈Yx⊤Ay′−minx′∈Xx′⊤Ay, which is always non-negative.
Popular no-regret algorithms for solving the game include the Optimistic Follow-the-Regularized-
Leader (OFTRL) algorithm and the Optimistic Online Mirror Descent (OOMD) algorithm, both
defined in terms of a certain regularizer R: ∆d→R(for some general dimension d). The
corresponding Bregman divergence of RisDR(x, x′) =R(x)−R(x′)−⟨∇R(x′), x−x′⟩, and the
regularizer is 1-strongly convex if DR(x, x′)≥1
2∥x−x′∥2
2for all x, x′∈∆d.
Optimistic Online Mirror Descent (OOMD) Starting from an initial point (x1, y1) = (bx1,by1),
the OOMD algorithm with regularizer Rand steps size η >0updates in each iteration t≥2,
bxt= argmin
x∈X{η
x, ℓt−1
x
+DR(x,bxt−1)}, xt= argmin
x∈X{η
x, ℓt−1
x
+DR(x,bxt)},
byt= argmin
y∈Y{η
y, ℓt−1
y
+DR(y,byt−1)}, yt= argmin
y∈Y{η
y, ℓt−1
y
+DR(y,byt)}.(OOMD)
In particular, we call OOMD with a squared Euclidean norm regularizer, that is, R(x) =1
2Pd
i=1x[i]2
optimistic gradient-descent-ascent (OGDA). When Ris the negative entropy, that is, R(x) =Pd
i=1x[i] logx[i], we call the resulting OOMD algorithm optimistic multiplicative weights update
(OMWU). OGDA and OMWU have been extensively studied in the literature regarding their last-
iterate convergence properties in zero-sum games. Specifically, both OMWU and OGDA guarantee
that(xt, yt)approaches to a Nash equilibrium as t→ ∞ .
Optimistic Follow-the-Regularized-Leader (OFTRL) Define the cumulative loss vectors Lt
x:=Pt
k=1ℓk
xandLt
y:=Pt
k=1ℓk
y. The update rule of OFTRL with regularizer Ris for each t≥1,
xt= argmin
x∈X
x, Lt−1
x+ℓt−1
x
+1
ηR(x)
,
yt= argmin
y∈Y
y, Lt−1
y+ℓt−1
y
+1
ηR(y)
.(OFTRL)
Throughout the paper, we consider the following regularizers:
•Negative entropy ( R(x) =Pd
i=1x[i] logx[i]): the resulting OFTRL algorithm coincides with
OMWU defined by the OOMD framework previously.
•Squared Euclidean norm ( R(x) =1
2Pd
i=1x[i]2): note that the resulting algorithm is different
from OGDA since the squared Euclidean norm is not a Legendre regularizer. As we will show, the
two algorithms behave very differently in terms of last-iterate convergence.
• Log barrier ( R(x) =Pd
i=1−log(x[i])): we also call it the log regularizer.
5• Negative Tsallis entropy regularizers ( R(x) =1−Pd
i=1(x[i])β
1−βparameterized by β∈(0,1)).
The 2-dimension case We denote x∈ R2asx= [x[1], x[2]]⊤. Ford1= 2, finding xtof OFTRL
reduces to the following 1-dimensional optimization problem:
xt[1] = argmin
x∈[0,1]
x·(Lt−1
x[1] + ℓt−1
x[1]−Lt−1
x[2]−ℓt−1
x[2]) +1
ηR(x)
, xt[2] = 1 −xt[1],
where we slightly abuse the notation and denote R(x) =R([x,1−x])forx∈[0,1]. We introduce
two notations (the case for the y-player is similar): let et
x=ℓt
x[1]−ℓt
x[2]be the difference between
the losses of the two actions, and Et
x=Pt
k=1ek
xbe the cumulative difference between the losses
of the two actions. For OFTRL, it is clear that the update of xtonly depends on the differences
Et−1
x, et−1
x, the step size η, and the regularizer R. For this reason, we define Fη,R:R→[0,1]as
follows:
Fη,R(e) := argmin
x∈[0,1]
x·e+1
ηR(x)
. (1)
We assume the function Fη,Ris well-defined, i.e., the above optimization problem admits a unique
solution in [0,1]. This is a condition easily satisfied, for example, when the regularizer Ris strongly
convex. Then the OFTRL algorithm can be written as
xt[1] = Fη,R(Et−1
x+et−1
x), xt[2] = 1 −xt[1].
The following lemma shows that the function Fη,Ris non-increasing (we defer missing proofs in the
section to Appendix A).
Lemma 1 (Monotonicity of Fη,R).The function Fη,R(·) :R→[0,1]defined in (1)is non-increasing.
We present some blanket assumptions on the regularizer, which are satisfied by all the regularizers
introduced before.
Assumption 1. We assume that the regularizer Rsatisfies the following properties: the function
Fη,R:R→[0,1]defined in (1)is,
1.Unbiased: Fη,R(0) =1
2.
2.Rational: limE→−∞ Fη,R(E) = 1 andlimE→+∞Fη,R(E) = 0 .
3.Lipschitz continuous: There exists L≥0such that F1,RisL-Lipschitz.
Item 1 in Assumption 1 shows that the initial strategy is the uniform distribution over the two actions,
which is standard in practice. The rational assumption (item 2 in Assumption 1) is natural since
otherwise, the algorithm could not even converge to a pure Nash equilibrium. The Lipschitzness (item
3 in Assumption 1) is implied when the regularizer is strongly convex over [0,1]2(see Lemma 4),
and it further implies Lipschitzness of Fη,Rfor any ηas shown in the following proposition.
Proposition 1. The function Fη,Rsatisfies Fη,R(E/η) =F1,R(E). IfF1,RisL-Lipschitz, then Fη,R
isηL-Lipschitz for any η >0.
3 Slow Convergence of OFTRL: A Hard Game Instance
We give negative results on the last-iterate convergence properties of OFTRL by studying its behavior
on a surprisingly simple 2×2two-player zero-sum games. The game’s loss matrix Aδis parameterized
byδ∈(0,1
2)and is defined as follows:
Aδ:="1
2+δ1
2
0 1#
. (2)
3.1 Basic Properties
We summarize some useful properties of Aδin the following proposition (missing proofs of this
section can be found in Appendix B).
6Proposition 2. The matrix game Aδsatisfies:
1.Aδhas a unique Nash equilibrium x∗= [1
1+δ,δ
1+δ]andy∗= [1
2(1+δ),1+2δ
2(1+δ)].
2.For a strategy pair (xt, yt), the loss vectors (i.e., gradients) for the two players are respec-
tively:
ℓt
x=Aδyt=1
2+δyt[1]
1−yt[1]
ℓt
y=−A⊤
δxt=−
(1
2+δ)xt[1]
1−1
2xt[1]
. (3)
Moreover,
et
x=ℓt
x[1]−ℓt
x[2] =−1
2+ (1 + δ)yt[1]∈[−1
2,1
2+δ]
et
y=ℓt
y[1]−ℓt
y[2] = 1 −(1 +δ)xt[1]∈[−δ,1].
In particular, we notice that et
y≥ −δ. It implies that if the cumulative differences between the losses
of the two actions Et
yis large, then it takes Ω(1
δ)iterations to make Et
ysmall (close to 0). This
has important implications for non-forgetful algorithms like OFTRL that look at the whole history
of losses. Since OFTRL chooses the strategy ytbased on Et
y, it could be trapped in a bad action
for a long time even if the current gradients suggest that the other action is better. This is the key
observation for our main negative results on the slow last-iterate convergence rates of OFTRL.
The following lemma shows that in a particular region of (x, y), the duality gap is a constant.
Lemma 2. Letδ, ϵ∈(0,1
2). For any x, y∈∆2such that x[1]≥1
1+δandy[1]≥1
2+ϵ, the duality
gap of (x, y)for game Aδ(defined in (2)) satisfies DualityGap( x, y)≥ϵ.
3.2 Slow Last-Iterate Convergence
We further require the following assumption on the regularizer R(and thus the function F1,R).
Assumption 2. LetLbe the Lipschitness constant of F1,Rin Assumption 1. Denote constant
c1=1
2−F1,R(1
20L). There exist universal constants δ′, c2>0andc3∈(0,1
2]such that for any
0< δ≤δ′,
1. For any Ethat satisfies F1,R(E)≥1
1+δ, we have F1,R(−c2
1
30Lδ+E)≥1+c3
1+c3+δ
2. For any Ethat satisfies F1,R(E)≥1
2(1+δ), we have F1,R(−c3c2
1
120L+δ
4L+E)≥1
2+c2.
Although Assumption 2 is technical, the idea is simple. Item 1 in Assumption 2 states that if a loss
difference E <0already makes F1,R(E)≥1
1+δ, then the loss difference E′=E−Ω(1
δ)is able to
make F1,R(E′)greater than F1,R(E)by a margin of Ω(δ). Item 2 in Assumption 2 states that if a
loss difference Ealready makes F1,R(E)≥1
2(1+δ)≈1
2, then the loss difference E′=E−Ω(1)
is able to make F1,R(E′)greater than1
2by a constant margin c2. In Appendix C, we verify that
Assumption 2 holds for the negative entropy, squared Euclidean norm, the log barrier, and the negative
Tsallis entropy regularizers.
Now we present the main result of the section showing that even after Ω(1/δ)iterations, the duality
gap of the iterate output by OFTRL is still a constant.
Theorem 1. Assume the regularizer Rsatisfies Assumption 1 and Assumption 2. For any δ∈(0,ˆδ),
where ˆδis a constant depending only on the constants c1andδ′defined in Assumption 2, the
OFTRL dynamics on Aδ(defined in (2)) with any step size η≤1
4Lsatisfies the following: there
exists an iteration t≥c1
3ηLδwith a duality gap of at least c2, a strictly positive constant defined in
Assumption 2.
Proof Sketch: We decompose the analysis into three stages as illustrated in Figure 3. We describe
the three stages and the high-level ideas of our proof below and defer the full proof to Appendix B.2.
7yt[1]
xt[1] 01
1 +δ1 3
41
2(1 + δ)
1
2−c1
2
1
2−c11
2+c21
z∗
Stage I Stage II
Ω(1
ηδ)iterationsStage III
t=Tst=Th
t=T1t=T2
Figure 3: Pictorial depiction of the three stages incurred by the OFTRL dynamics in the game Aδ
defined in (2). The point z∗denotes the unique Nash equilibrium. The times T1andT2are shown for
concrete instantiations of OFTRL in Figure 1 by a red star and a blue dot, respectively. The times Ts
andThare defined in the proof of Theorem 1 in Appendix B.2.
•Stage I [1, T1−1]:Recall that x1[1] = y1[1] =1
2by Assumption 1. We show that xt[1]increases
and denote T1the first iteration that xT1[1]≥1
1+δ. During the time [1, T1−1], since xt[1]is
always smaller than1
1+δ, we know from Proposition 2 action 1has larger loss than action 2for
they-player, i.e., et
y=ℓt
y[1]−ℓt
y[2]≥0. Thus yt[1]decreases during stage I and we show that
yT1[1]≤1
2−c1withc1defined in Assumption 2.
•Stage II [T1, T2]:Recall that yT1[1]≤1
2−c1. We define T2> T 1as the first iteration where
yT2[1]≥1
2(1+δ)>1
2−c1. We remark that for yt[1]to increase, the loss vector must satisfy et
y<0.
However, the game matrix Aδguarantees that et
y≥ −δno matter what the x-player’s strategy is
(Proposition 2). Thus by the ηL-Lipschitzness of Fη,R(Proposition 1), the per-iteration increase in
yt[1]is at most ηLδ. Therefore, we know T2−T1= Ω(c1
ηLδ). As a result, et
x<0during [T1, T2]
and the cumulative loss for the x-player decreases to ET2x≤ET1x−Ω(1
ηLδ). Recall xT1[1]≥1
1+δ.
Thus xT2[1]> xT1[1]is much closer to 1.
•Stage III [T2, T3]:Recall that yT2[1]≥1
2(1+δ). Moreover, yt[1]could keep increasing if xt[1]≥
1
1+δsince that implies et
y≤0. Now the question is how long would the x-player stay close to the
boundary, i.e, xt[1]≥1
1+δ. Since OFTRL-type algorithms are not forgetful, this happens only
when Et
x≥ET1x(recall xT1[1]≥1
1+δ). But we have at the end of stage II, ET2x≤ET1x−Ω(1
ηLδ).
Since the per-iteration loss is bounded by 1, it requires at least Ω(1
ηLδ)iterations to cancel the
cumulative loss of Ω(1
ηLδ). Define T3=T2+ Ω(1
ηLδ). During [T2, T3], the y-player always
receives loss such that et
y≤0and we prove that in the end yT3[1]≥1
2+c2for some constant c2.
•Conclusion: Finally, we get one iteration T3≥Ω(1
ηLδ)withxT3[1]≥1
1+δandyT3[1]≥1
2+c2.
Using Lemma 2, the duality gap of (xT3, yT3)is at least c2>0.
Theorem 1 immediately implies the following (proof deferred to Appendix B.3).
Theorem 2. For optimistic FTRL with any regularizer satisfying Assumption 1 and Assumption 2
and constant steps size η≤1
4L(Lis defined in Assumption 1), there is no function fsuch that
the corresponding learning dynamics {(xt, yt)}t≥1in two-player zero-sum games [0,1]d1×d2has a
last-iterate convergence rate of f(d1, d2, T). More specifically, no function fcan satisfy
1.DualityGap( xT, yT)≤f(d1, d2, T)for all [0,1]d1×d2and for all T≥1.
82.limT→∞f(d1, d2, T)→0.
Theorem 1 and Theorem 2 provide impossibility results for getting a last-iterate convergence rate
for OFTRL that solely depends on the bounded parameters, even in two-player zero-sum games.
Moreover, they show the necessity of forgetfulness for fast last-iterate convergence in games since
OGDA has a last-iterate convergence rate of O(poly( d1,d2)√
T)[Cai et al., 2022, Gorbunov et al., 2022].
4 Extension to Higher Dimensions
In this section, we extend our negative results from 2×2matrix games to games with higher
dimensions. We start by showing an equivalence result for a single player (say, the first player). We
assume that a decision maker is using OFTRL with a 1-strongly convex (w.r.t. the ℓ2norm) and
separable regularizer R(x) =R1(x1) +R2(x2)to choose decisions. At a given time time t, they see
a loss ℓt∈[0,1]2.
Now consider the following 2n-dimensional decision problem: The player uses OFTRL using the
regularizer ˆR(ˆx) =Pn
i=1R1(ˆxi) +P2n
i=n+1R2(ˆxi),i.e., they use R1on the first half of actions,
andR2on the second half. This is again a 1-strongly convex regularizer (w.r.t. the ℓ2norm). Suppose
the decision maker sees the rescaled and duplicated version of the losses ℓ1, . . . , ℓTfrom the 2-
dimensional case: ˆℓt
i=1
nαℓt
1ifi≤n, and ˆℓt
i=1
nαℓt
2ifi > n . The parameter αwill be chosen later
based on the regularizer.
Now we wish to show that by choosing αin the right way, we get that the decisions for the
2-dimensional and 2n-dimensional OFTRL algorithms are equivalent. Let x1, . . . , xTbe the 2-
dimensional OFTRL decisions, and let ˆx1, . . . , ˆxTbe the 2n-dimensional OFTRL decisions. Then,
we want to show thatPn
i=1ˆxt
i=xt[1]andP2n
i=n+1ˆxt
i=xt[2]for all t.
Lemma 3. Let the losses ˆℓ1, . . . , ˆℓTsatisfy the duplication procedure given in the preceding para-
graph. Then for any time t, we have ˆxt
1=···= ˆxt
nandˆxt
n+1=···= ˆxt
2n.
Proof. Suppose not and let ˆxtbe the corresponding solution. Then the optimal solution is such that
ˆxt
i̸= ˆxt
kfor some i, kboth less than n, or both greater than n. But then, by symmetry, we have that
there is more than one optimal solution to the OFTRL optimization problem at time t: the objective
is exactly the same if we create a new solution where we swap the values of ˆxt
iandˆxt
k. This is a
contradiction due to strong convexity.
From lemma 3, we have that the OFTRL decision problem in 2ndimensions can equivalently be
written as a 2-dimensional decision problem: Since the first nentries must be the same, we can
simply optimize over that one shared value, say xt[1], which we use for all nentries, and similarly
we use xt[2]for the second half of the entries. Let Dupl : ∆2→∆2nbe a function that maps the
two-dimensional solution into the corresponding duplicated 2n-dimensional solution. The equivalent
2-dimensional problem is then:
ˆxt= Dupl"
argmin
x∈1
n·∆2(
n
nα*
x,t−1X
τ=1ℓτ+ℓt−1+
+n
ηR1(x[1]) +n
ηR2(x[2]))#
= Dupl"
1
n·argmin
x∈∆2(
n
nα*
1
nx,t−1X
τ=1ℓτ+ℓt−1+
+n
ηR(x/n))#
= Dupl"
1
n·argmin
x∈∆2(*
x,t−1X
τ=1ℓτ+ℓt−1+
+nα+1
ηR(x/n))#
.
The next theorem shows that we can choose αfor different regularizers and construct 2n×2nloss
matrices whose learning dynamics are equivalent to the learning dynamics in 2×2games given in
the preceding sections. We defer the proof to Appendix D.
Theorem 3. For any loss matrix A∈[0,1]2×2, there exists a loss matrix ˆA∈[0, n−α]2n×2nsuch
that for the Euclidean ( α= 1), entropy ( α= 0), Tsallis ( β∈(0,1)andα=−1 +β), and log
(α=−1) regularizers, the resulting OFTRL learning dynamics are equivalent in the two games.
9Combining Theorem 1 and Theorem 3, we have the following:
Corollary 1. In the same setup as Theorem 3, under Assumption 1 and Assumption 2, there exists a
game matrix ˆAδ∈[0, n−α]2n×2nsuch that the OFTRL learning dynamics with any step size η≤1
4L
satisfies the following: there exists an iteration t≥c1
3ηLδwith a duality gap at least c2n−α.
Since α= 0for the entropy regularizer, the same results hold more generally for games where one
player has more actions than the other. In particular, we can create a 2n×2mgame such that the
resulting dynamics are equivalent to those in a 2×2game. This does not work for the Euclidean and
log regularizers because the rescaling factors would be different for the row and column players.
5 Conclusion and Discussions
In this paper, we study last-iterate convergence rates of OFTRL algorithms with various popular
regularizers, including the popular OMWU algorithm. Our main results show that even in simple
2×2two-player zero-sum games parametrized by δ >0, the lack of forgetfulness of OFTRL leads
to the duality gap remaining constant even after 1/δiterations (Theorem 1). As a corollary, we show
that the last-iterate convergence rate of OFTRL must depend on a problem-dependent constant that
can be arbitrarily bad (Theorem 2). This highlights a stark contrast with OOMD algorithms: while
OGDA with constant step size achieves a O(1√
T)last-iterate convergence rate, such a guarantee is
impossible for OMWU or more generally OFTRL. We now discuss several interesting questions
regarding the convergence guarantees of learning in games and leave them as future directions.
Best-Iterate Convergence Rates While we focus on the last-iterate ( i.e.,DualityGap( xT, yT)),
the weaker notion of best-iterate ( i.e.,mint∈[T]DualityGap( xt, yt)) is also of both practical and
theoretical interest. By definition, we know the best-iterate convergence rate is at least as good as the
last-iterate convergence rate and could be much faster. This raises the following question:
What is the best-iterate convergence rate of OMWU/OFTRL?
To our knowledge, there are no concrete results on the best-iterate convergence rates of OMWU
or other OFTRL algorithms. It is thus interesting to extend our negative results to the best-iterate
convergence rates or develop fast best-iterate convergence rates of OMWU/OFTRL.
Dynamic Step Sizes Our negative results hold for OFTRL with fixed step sizes. We conjecture
that the slow last-iterate convergence of OFTRL persists even with dynamic step sizes. In particular,
we believe our counterexamples still work for OFTRL with decreasing step sizes. This is because
decreasing the step size makes the players move even slower, and they may be trapped in the wrong
direction for a longer time due to the lack of forgetfulness. In Appendix E, we include numerical
results for OMWU with adaptive stepsize akin to Adagrad [Duchi et al., 2011], which supports our
intuition. We observe the same cycling behavior as for fixed step size. While the cycle is smaller than
that of fixed step sizes, the dynamics take more steps to finish each cycle. Investigating the effect of
dynamic step sizes on last-iterate convergence rates is an interesting future direction.
Slow Convergence due to Lack of Forgetfulness Our work shows that various OFTRL-type
algorithms do not have fast last-iterate convergence rates for learning in games. Our proof and hard
game instance build on the intuition that these algorithms lack forgetfulness: they do not forget
the past quickly. This intuition is also utilized in [Panageas et al., 2023]. In particular, they give
and×dpotential game where the last-iterate convergence rate of the Fictitious Play algorithm,
which is equivalent to the Follow-the-Leader (FTL) algorithm, suffers exponential dependence in the
dimension d. One natural future direction is to formalize the intuition of non-forgetfulness further
and give a general condition for algorithms under which they suffer slow last-iterate convergence. It
is also interesting to show other lower-bound results for learning in games.
10Acknowledgements
We thank the anonymous reviewers for their constructive comments on improving the paper. Yang
Cai was supported by the NSF Awards CCF-1942583 (CAREER) and CCF-2342642. Christian Kroer
was supported by the Office of Naval Research awards N00014-22-1-2530 and N00014-23-1-2374,
and the National Science Foundation awards IIS-2147361 and IIS-2238960. Julien Grand-Clément
was supported by Hi! Paris and Agence Nationale de la Recherche (Grant 11-LABX-0047). Haipeng
Luo was supported by NSF award IIS-1943607. Weiqiang Zheng was supported by the NSF Awards
CCF-1942583 (CAREER), CCF-2342642, and a Research Fellowship from the Center for Algorithms,
Data, and Market Design at Yale (CADMY).
References
Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate
convergence beyond zero-sum games. In International Conference on Machine Learning , pages
536–581. PMLR, 2022.
James P Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In
Proceedings of the 2018 ACM Conference on Economics and Computation , pages 321–338, 2018.
Mark Braverman, Jieming Mao, Jon Schneider, and Matt Weinberg. Selling to a no-regret buyer. In
Proceedings of the 2018 ACM Conference on Economics and Computation , pages 523–538, 2018.
Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats
top professionals. Science , 359(6374):418–424, 2018.
Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Finite-time last-iterate convergence for learning
in multi-player games. In Advances in Neural Information Processing Systems (NeurIPS) , 2022.
Yun Kuen Cheung and Georgios Piliouras. V ortices instead of equilibria in minmax optimization:
Chaos and butterfly effects of online learning in zero-sum games. In Conference on Learning
Theory , pages 807–834. PMLR, 2019.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. Advances in neural information processing systems (NeurIPS) , 2018.
Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and con-
strained min-max optimization. In 10th Innovations in Theoretical Computer Science Conference
(ITCS) , 2019.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. In International Conference on Learning Representations (ICLR) , 2018.
Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning
in general games. Advances in Neural Information Processing Systems (NeurIPS) , 2021.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research , 12(7), 2011.
Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Faster game solving via predictive blackwell
approachability: Connecting regret matching and mirror descent. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 35, pages 5363–5371, 2021.
Gabriele Farina, Chung-Wei Lee, Haipeng Luo, and Christian Kroer. Kernelized multiplicative
weights for 0/1-polyhedral games: Bridging the gap between learning in extensive-form and
normal-form games. In International Conference on Machine Learning (ICML) , pages 6337–6357,
2022.
Genevieve E Flaspohler, Francesco Orabona, Judah Cohen, Soukayna Mouatadid, Miruna Oprescu,
Paulo Orenstein, and Lester Mackey. Online learning with optimism and delay. In International
Conference on Machine Learning , pages 3363–3373. PMLR, 2021.
11Noah Golowich, Sarath Pattathil, and Constantinos Daskalakis. Tight last-iterate convergence rates
for no-regret learning in multi-player games. Advances in neural information processing systems
(NeurIPS) , 2020a.
Noah Golowich, Sarath Pattathil, Constantinos Daskalakis, and Asuman Ozdaglar. Last iterate is
slower than averaged iterate in smooth convex-concave saddle point problems. In Conference on
Learning Theory (COLT) , 2020b.
Eduard Gorbunov, Adrien Taylor, and Gauthier Gidel. Last-iterate convergence of optimistic gradient
method for monotone variational inequalities. In Advances in Neural Information Processing
Systems , 2022.
Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica , 68(5):1127–1150, 2000.
Yu-Guan Hsieh, Franck Iutzeler, Jérôme Malick, and Panayotis Mertikopoulos. On the convergence
of single-call stochastic extra-gradient methods. Advances in Neural Information Processing
Systems , 32, 2019.
Yu-Guan Hsieh, Kimon Antonakopoulos, and Panayotis Mertikopoulos. Adaptive learning in
continuous games: Optimal regret bounds and convergence to nash equilibrium. In Conference on
Learning Theory , pages 2388–2422. PMLR, 2021.
Wouter M Koolen, Manfred K Warmuth, Jyrki Kivinen, et al. Hedging structured concepts. In COLT ,
pages 93–105. Citeseer, 2010.
Rachitesh Kumar, Jon Schneider, and Balasubramanian Sivan. Strategically-robust learning al-
gorithms for bidding in first-price auctions. In Proceedings of the 2024 ACM Conference on
Economics and Computation , 2024.
Chung-Wei Lee, Christian Kroer, and Haipeng Luo. Last-iterate convergence in extensive-form
games. Advances in Neural Information Processing Systems , 34:14293–14305, 2021.
Haipeng Luo. Lecture note 2, Introduction to Online Learning. 2022. URL https://haipeng-luo.
net/courses/CSCI659/2022_fall/lectures/lecture2.pdf .
Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial
regularized learning. In Proceedings of the twenty-ninth annual ACM-SIAM symposium on discrete
algorithms , pages 2703–2717. SIAM, 2018.
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar,
and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra
(gradient) mile. In International Conference on Learning Representations (ICLR) , 2019.
Aryan Mokhtari, Asuman E Ozdaglar, and Sarath Pattathil. Convergence rate of O(1/k)for optimistic
gradient and extragradient methods in smooth convex-concave saddle point problems. SIAM
Journal on Optimization , 30(4):3230–3251, 2020.
Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland,
Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash
learning from human feedback. arXiv preprint arXiv:2312.00886 , 2023.
Ioannis Panageas, Nikolas Patris, Stratis Skoulakis, and V olkan Cevher. Exponential lower bounds for
fictitious play in potential games. In Thirty-seventh Conference on Neural Information Processing
Systems , 2023. URL https://openreview.net/forum?id=tkenkPYkxj .
Julien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer,
Paul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego
with model-free multiagent reinforcement learning. Science , 378(6623):990–996, 2022.
Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences.
Advances in Neural Information Processing Systems , 2013.
12Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of
regularized learning in games. Advances in Neural Information Processing Systems (NeurIPS) ,
2015.
Eiji Takimoto and Manfred K Warmuth. Path kernels and multiplicative updates. The Journal of
Machine Learning Research , 4:773–818, 2003.
Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit
texas hold’em. In Twenty-fourth international joint conference on artificial intelligence , 2015.
Tim van Erven. Why FTRL is better than online mirror descent. https://www.timvanerven.nl/
blog/ftrl-vs-omd/ , 2021. Accessed: 2024-05-22.
Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, Thanasis Lianeas, Panayotis Mer-
tikopoulos, and Georgios Piliouras. No-regret learning and mixed nash equilibria: They do not
mix. Advances in Neural Information Processing Systems , 33:1380–1391, 2020.
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence
in constrained saddle-point optimization. In International Conference on Learning Representations
(ICLR) , 2021.
Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. Advances in neural information processing systems , 20,
2007.
Contents
1 Introduction 1
1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Preliminaries and Problem Setup 5
3 Slow Convergence of OFTRL: A Hard Game Instance 6
3.1 Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Slow Last-Iterate Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Extension to Higher Dimensions 9
5 Conclusion and Discussions 10
A Missing Proofs in Section 2 14
A.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B Missing Proofs in Section 3 14
B.1 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B.2 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B.3 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
C Verifying Assumption 2 for Different Regularizers 18
C.1 Negative Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.2 Squared Euclidean Norm Regularizer . . . . . . . . . . . . . . . . . . . . . . . . 19
13C.3 Log Barrier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.4 Negative Tsallis Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
D Proof of Theorem 3 22
E Numerical Experiments with Adaptive Stepsizes 22
A Missing Proofs in Section 2
A.1 Proof of Lemma 1
Proof. Lete1< e2. Denote x1=Fη,R(e1)andx2=Fη,R(e2). By definition, we have
e2(x2−x1)≤1
η(R(x1)−R(x2))≤e1(x2−x1).
Since e1< e2, we have x2≤x1.
A.2 Proof of Proposition 1
Proof. By definition,
Fη,RE
η
= argmin
x∈[0,1]
x·E
η+1
ηR(x)
= argmin
x∈[0,1]{x·E+R(x)}=F1,R(E).
The second claim on the Lipschitzness follows directly.
B Missing Proofs in Section 3
B.1 Proof of Lemma 2
Proof. We have
DualityGap( x, y) = max
˜y∈∆2x⊤Aδ˜y−min
˜x∈∆2˜x⊤Aδy
= max
i∈{1,2}(A⊤
δx)[i]−min
i∈{1,2}(Ayδ)[i]
= (1
2+δ)x[1]−(1−y[1]) (x[1]≥1
1+δ, ϵ > 0)
≥1
21 + 2 δ
1 +δ−1
2+ϵ
≥ϵ.
B.2 Proof of Theorem 1
Proof. Recall that c1=1
2−F1,R(1
20L)defined in Assumption 2. We fix any δ <
min{1
15,c1
6,c2
1
300, δ′}. Since δ < δ′, Assumption 2 holds. We will prove that there exists an it-
eration t≥c1
3ηLδwith duality gap c2.
Proof Plan: We decompose the analysis into three stages as shown in Figure 3. Below, we describe
the three stages and the high-level ideas in our proof.
•Stage I: Recall that x1[1] = y1[1] =1
2. In Stage I, we show that xt[1]will increase and denote
T1≥1the first iteration where xt[1]≥1
1+δ. The existence of T1can be proved by contradiction
(Claim 1). Since before the end of Stage I, xt[1]<1
1+δ, the loss vector for the y-player satisfies
et
y=ℓt
y[1]−ℓt
y[2]≥0meaning action 1is worse than action 2. We will prove that finally
yT1[1]≤1
2−c1.
14•Stage II: Now we have that yT1[1]≤1
2−c1, we denote T2> T 1the first iteration where
yT2[1]≥1
2(1+δ)>1
2−c1. We remark that in order to increase yt[1], the loss vector must satisfy
et
y<0. However, the game matrix Aδguarantees that et
y≥ −δno matter what the x-player is
playing. Thus by the ηL-Lipschitzness of Fη,R(Lemma 4), the increase in yt[1]is at most ηLδ.
Therefore, we know T2−T1= Ω(c1
ηLδ). But during [T1, T2], for the x-player, we have et
x<0
which implies its cumulative loss ET2x≤ET1x−Ω(1
ηLδ). In other words, xt[1]is very close to 1
and the cumulative loss for action 1 is much smaller than that of action 2.
•Stage III: Now we have yT2[1]≥1
2(1+δ)and that yt[1]could keep increasing if xt[1]≥1
1+δsince
then the loss satisfies et
y≤0. Now the question is how long would the x-player stay close to the
boundary, i.e, xt[1]≥1
1+δ. Since OFTRL-type algorithms are not forgetful, this happens only
when Et
x≥ET1x(recall xT1[1]≥1
1+δ). But we have at the end of stage II, ET2x≤ET1x−Ω(1
ηLδ).
Since et
xis bounded by a constant, we know xt[1]≥1
1+δeven after Ω(1
ηLδ)iterations. Define
T3=T2+ Ω(1
ηLδ). During [T2, T3], they-player always receives loss such that et
y≤0and we
prove that yT3[1]≥1
2+c2for some constant c2.
•Conclusion Finally we get one iteration T3≥Ω(1
ηLδ)withxT3[1]≥1
1+δandyT3[1]≥1
2+c2,
Using Lemma 2, the duality gap of (xT3, yT3)is at least c2.
Stage I: We know x1[1] = y1[1] =1
2. We define (i) Ts>1to be the smallest iteration such that
xTs[1]≥3
4and (ii) T1> Tsto be the smallest iteration such that xT1[1]≥1
1+δ. Both TsandT1
must exist, and the reason will become clear in the following analysis. We postpone the proof of this
fact in Claim 1 at the end of this paragraph.
Notice from Proposition 2, the difference et
xis lower bounded: et
x≥ −1
2for any t. Thus Et−1
x+
et−1
x≥ −t
2for any t≥1. Since xTs[1]≥3
4>1
2, we know that ETs−1
x +eTs−1
x <0. AsFη,Ris
ηL-Lipschitz,
1
4≤xTs[1]−x1[1]≤ηL·ETs−1
x +eTs−1
x≤LηT s
2.
This implies
Ts≥1
2ηL.
Since xt[1]<3
4for all 1≤t≤Ts−1, we know that et
y=ℓt
y[1]−ℓt
y[2] = 1 −(1 +δ)xt[1]>
1−3δ
4≥1
5(asδ≤1
15) for all 1≤t≤Ts−1. Moreover, for all 1≤t≤T1−1, we know that et
y≥0
asxt[1]≤1
1+δ. Since the difference et
yis at least 1/5for all t≤Ts−1and remains non-negative
for all t∈[Ts, T1−1], we can conclude that for all Ts≤t≤T1
yt[1] = Fη,R(Et−1
y+et−1
y)≤Fη,R(Et−1
y),
and moreover
Fη,R(Et−1
y)≤Fη,RTs−1
5
≤Fη,R1
20Lη
(Ts−1≥1
2ηL−1≥1
4Lη)
=1
2−c1.
This completes the proof of Stage I, where xT1[1]≥1
1+δandyT1[1]≤1
2−c1. Before we proceed to
the next stage, we prove the existence of TsandT1.
Claim 1. TsandT1exist.
Proof. It suffices to prove that T1exists as it implies the existence of Ts. Assume for the sake of
contradiction that T1does not exist, i.e.,xt[1]<1
1+δfor all t≥1. By the same analysis as for Stage
I, we get yt[1]≤1
2−c1for all t≥1
2ηL. This implies et
x=−1
2+ (1 + δ)yt[1]≤δ
2−c1≤ −c1
2for
15allt≥1
2ηL. Then Et
x+et
x→ −∞ ast→+∞. As a consequence, xt[1] = Fη,R(Et−1
x+et−1
x)→1
ast→+∞by item 2 in Assumption 1. But this contradicts with the assumption that xt[1]<1
1+δ
for all t≥1. This completes the proof.
Stage II We define
T:=c1
2Lηδ
∈c1
3Lηδ,c1
2Lηδ
, (4)
where the lower bound on Tholds sincec1
6Lηδ≥c1
6δ≥1. We note that T= Ω(1
δ)since ηL≤1
4.
In Stage I, we have proved that yT1[1]≤1
2−c1. Define Th=T1+T. We claim that for all
t∈[T1, Th−1],yt[1]≤1
2−c1
2. To prove the claim, we first notice that −δ≤et
y≤1for all t≥1.
Then by the monotonicity and the ηL-Lipschitzness of Fη,R(Lemma 1 and Lemma 4), we get for all
t∈[T1, Th−1],
yt[1]≤Fη,R(ET1−1
y) +ηLmax
ET1−1
y−Et−1
y−et−1
y,0	
≤1
2−c1+ηL·(t−T1+ 1)δ
≤1
2−c1+ηLTδ
≤1
2−c1
2,
where, in the second-to-last inequality, we use t−T1+ 1≤T≤c1
2ηLδby Equation (4).
Now we denote T2≥Ththe smallest iteration when yT2[1]≥1
2(1+δ). The existence of T2will
become clear in the following analysis, and we postpone the proof to Claim 2 at the end of the
discussion. Then for all t∈[Ts, T2−1], we have yt[1]≤1
2(1+δ), which implies et
x≤0. Moreover,
for all t∈[Ts, T1+T−1], since yt[1]≤1
2−c1
2, we have
et
x=ℓt
x[1]−ℓt
x[2]
=−1
2+ (1 + δ)yt[1]
≤−1 + (1 + δ)(1−c1)
2
≤δ−c1
2
≤ −c1
4. (δ≤c1
2)
Then for any T1+T≤t≤T2, we have
xt[1] = Fη,R(Et−1
x+et−1
x)
≥Fη,R(ET1+T−1
x ) (et−1
x≤0for all t∈[T1+T, T2])
≥Fη,R
−c1T
4+ET1−1
x
≥Fη,R
−c1T
5+ET1−1
x +eT1−1
x
,
where in the last inequality, we use the fact thatc1T
20≥c2
1
60ηLδ≥1.
Claim 2. T2exists.
Proof. Assume for the sake of contradiction that T2does not exist, i.e.,yt[1]<1
2(1+δ)for all
t≥T1(since we know yt[1]≤1
2−c1
2for all t∈[T1, T1+T−1]). Then by the analysis of
Stage II and Equation (5), we have xt[1]≥4
4+δfor all t≥T1. This implies et
y≤ −3δ
5for all
t≥T1. As a result, we have Et−1
y+et−1
y→ −∞ ast→ ∞ . By item 2 in Assumption 1, we
getyt[1] = Fη,R(Et−1
y+et−1
y)≥1
2ast→ ∞ . But this contradicts with the assumption that
yt[1]<1
2(1+δ)for all t≥T1. This completes the proof.
16Stage III Recall that we have argued in State I that Fη,R(ET1−1
x +eT1−1
x) =F1,R(η(ET1−1
x +
eT1−1
x)) =xT1[1]≥1
1+δ. By item 1 in Assumption 2, we have that
Fη,R
−c1T
10+ET1−1
x +eT1−1
x)
≥Fη,R
−c2
1
30Lηδ+ET1−1
x +eT1−1
x)
=F1,R
−c2
1
30Lδ+η(ET1−1
x +eT1−1
x))
≥1 +c3
1 +c3+δ, (5)
where the first inequality follows from the definition of Tand the monotonicity of Fη,R(Lemma 1).
Now denote T3=T2+⌊c1T
10⌋ −2. For any T2≤t≤T3, we know that
xt[1] = Fη,R(Et−1
x+et−1
x)
=Fη,R(ET2−1
x +eT2−1
x +t−1X
k=T2ek
x+et−1
x−eT2−1
x)
≥Fη,R(−c1T
5+ET1−1
x +eT1−1
x +t−1X
k=T2ek
x+et−1
x−eT2−1
x)
≥Fη,R(−c1T
5+ET1−1
x +eT1−1
x +c1T
10−2 + 2)
≥Fη,R(−c1T
10+ET1−1
x +eT1−1
x))
≥1 +c3
1 +c3+δ. (by (5))
Note that 1+c3+δ≤2. This implies et
y= 1−(1+δ)xt[1] =−c3δ
1+c3+δ≤ −c3δ
2for all T2≤t≤T3.
Moreover, we know that et
y≥ −δfor any t. Then
yT3[1] = Fη,R(ET3−1
y +eT3−1
y))
≥Fη,R(ET2−1
y +eT2−1
y +T3−1X
k=T2ek
y+eT3−1
y−eT2−1
y))
≥Fη,R(ET2−1
y +eT2−1
y−c3δ(T3−T2)
2+δ)
≥Fη,R(ET2−1
y +eT2−1
y−c3δc1T
40+δ) (T3−T2=⌊c1T
10⌋ −2≥c1T
20)
≥Fη,R(ET2−1
y +eT2−1
y−c3c2
1
120ηL+δ) (T≥c1
3ηLδ)
=F1,R(η(ET2−1
y +eT2−1
y)−c3c2
1
120L+ηδ)
≥F1,R(η(ET2−1
y +eT2−1
y)−c3c2
1
120L+δ
4L). (η≤1
4L)
Recall that F1,R(η(ET2−1
y +eT2−1
y)) = Fη,R(ET2−1
y +eT2−1
y) =yT2[1]≥1
2(1+δ). By item 2 in
Assumption 2, we have F1,R(η(ET2−1
y +eT2−1
y)−c2
1
120L+δ
4L)≥1
2+c2for some absolute constant
c2>0. Thus, we have yT3[1]≥1
2+c2. Recall that xT3[1]≥1+c3
1+c3+δ≥1
1+δ. Then by Lemma 2
we can conclude that the duality gap of (xT3, yT3)is at least c2>0. This completes the proof as
T3≥T2≥T≥c1
3ηLδ.
B.3 Proof of Theorem 2
Proof. Assume for the sake of contradiction that there is a function that satisfies both conditions.
Then for any A∈[0,1]2×2, we have the OFTRL learning dynamics over Asatisfies
171.DualityGap( xT, yT)≤f(2,2, T)for all T.
2.limT→∞f(2,2, T)→0
Since limT→∞f(2,2, T)→0, we know there exists T0>0such that for any t≥T0,
DualityGap( xt, yt)≤f(2,2, t)< c2. Now let δ≤min{ˆδ,c1
3ηLT 0}. Then by Theorem 1, we
know there exists an iteration t≥c1
3ηLδ≥T0such that DualityGap( xt, yt)≥c2. This completes
the proof.
C Verifying Assumption 2 for Different Regularizers
Lemma 4. If the regularizer Ris1-strongly convex, then F1,Ris1
2-Lipschitz.
Proof. Notice that R(x) +R(1−x)is2-strongly convex. Thus by standard analysis (see e.g., Luo
[2022, Lemma 4]) we know F1,Ris1
2-Lipschitz.
By Lemma 4, we can choose L=1
2for any 1-strongly convex regularizer in Assumption 1.
C.1 Negative Entropy
Lemma 5 (Assumption 2 holds for the entropy regularizer) .Consider the negative entropy regularizer
Rdefined as R(x) =xlogx+ (1−x) log(1 −x). Then F1,RisL=1
2-Lipschitz. We have c1and
Assumption 2 holds with δ′=c2
1
480L,c2=F1,R(−c2
1
480L)−1
2, and c3=1
2.
Proof. It is easy to verify that F1,R(x)has a closed-form representation
F1,R(E) =1
1 + exp( E).
Thus L=1
2andc1=1
2−F1,R(1
20L)is a universal constant. We also choose c3=1
2.
IfF1,R(E)≥1
1+δ≥1
1+δ, then we have E≤ −log(1/δ). We note that
exp
−c2
1
30Lδ
≤1
1 +c3⇒1
1 + exp ( −c2
1
30Lδ−log(1/δ))≥1 +c3
1 +c3+δ.
Thus δ≤δ1=c2
1
30Llog(1+ c3))=c2
1
30 log(3
2))Lsuffices for item 1 in Assumption 2.
IfF1,R(E)≥1
2(1+δ)=1
1+1+2 δ, we have E≤log(1 + 2 δ). Note that since log(1 + 2 y)≤2yfor
y >0, we have
δ≤c3c2
1
480L⇒ −c3c2
1
120L+ log(1 + 2 δ)<−c3c2
1
240L
⇒F1,R
−c3c2
1
120L+E
> F1,R
−c3c2
1
240L
.
Thus item 2 in Assumption 2 holds for any δ≤δ2=c3c2
1
480L=c2
1
960Landc2=F1,R(−c3c2
1
240L)−1
2=
F1,R(−c2
1
480L)−1
2.
Combining the above, we know Assumption 2 holds for the negative entropy regularizer with
δ′=c2
1
960Landc2=F1,R(−c2
1
480L)−1
2.
18C.2 Squared Euclidean Norm Regularizer
Lemma 6 (Assumption 2 holds for the Euclidean regularizer) .Consider the Euclidean regularizer
Rdefined as R(x) =1
2(x2+ (1−x)2). We have L=1
2andc1=1
20. We also have Assumption 2
holds with δ′=c2
1
480L,c2=c2
1
960L, and c3=1
2.
Proof. It is easy to verify that F1,R(x)has a closed-form representation
F1,R(x) =

1 ifx≤ −1
1−x
2ifx∈(−1,1)
0 ifx≥1
Thus F1,RisL-Lipschitz with L=1
2. Moreover, c1=1
2−F1,R(1
20L) =1
20. We choose c3=1
2.
Fix any Esuch that F1,R(E)≥1
1+δ. We have E≤ −1−δ
1+δ<0. We note that for any δ≤c2
1
30L=c2
1
15,
F1,R
−c2
1
30Lδ+E
≥F1,R(−1) = 1 .
Thus δ≤δ1=c2
1
30Lsuffices for item 1 in Assumption 2.
Fix any Esuch that F1,R(E)≥1
2(1+δ)=1
2(1+δ). We have E≤δ
1+δ≤δ. The for any δ≤c3c2
1
240L,
we have
F1,R
−c3c2
1
120L+E
≥F1,R
−c3c2
1
240L
=1
2+c3c2
1
480L
Thus item 2 in Assumption 2 holds for any δ≤δ2=c2
1
480Landc2=c2
1
960L.
Combining the above, we know Assumption 2 holds for the negative entropy regularizer with
δ′= min {δ1, δ2}=c2
1
480Landc2=c2
1
960L.
C.3 Log Barrier
Lemma 7 (Assumption 2 holds for the log barrier) .Consider the log barrier regularizer Rdefined
asR(x) =−log(x)−log(1−x). Then Assumption 2 holds with the following choices of constants:
1.c1=q
1
4+ 400 L2−20L >0.
2.c3=c2
1
60L.
3.c2=q
1
4+ (c3c2
1
240L)2−c3c2
1
240L>0.
4.δ′=c3c2
1
2160L.
Proof. By setting the gradient of x·E+R(x)to0, we get a closed-form expression of F1,R:
F1,R(E) =

1
2+1
E−q
1
4+1
E2ifE >0
1
2ifE= 0
1
2+1
E+q
1
4+1
E2ifE <0.
Forx∈(0,1), theF1,Rfunction admits an inverse function defined as
F−1
1,R(x) =2x−1
x2−x.
19Thus we know E0:=F−1
1,R(1
1+δ) =−1−δ2
δsatisfies F1,R(E0) =1
1+δ. Moreover, we can calculate
F−1
1,R1 +c3
1 +c3+δ
=−(1 +c3)2−δ2
(1 +c3)δ
=−1 +c3
δ+δ
1 +c3
=E0−c3
δ−c3δ
1 +c3.
Thus we can choose c3=c2
1
60Lso that
E0−c2
1
30Lδ
=E0−c3
δ−c3
δ
≤E0−c3
δ−c3δ
1 +c3. (since δ <1/2andc3>0)
Thus we have F1,R(E0−c2
1
30Lδ)≥F1,R(E0−c3
δ−c3δ
1+c3)≥1+c3
1+c3+δ.
We calculate E1:=F−1
1,R(1
2(1+δ)) =4(δ+δ2)
1+2δ≤8δ. Then we can choose δ≤δ′:=c3c2
1
2160L. Then we
have
F1,R(−c3c2
1
120L+δ
4L+E1)
≥F1,R(−c3c2
1
120L+ 9δ)
≥F1,R(−c3c2
1
240L)
=1
2+c2,
where c2=q
1
4+ (c3c2
1
240L)2−c3c2
1
240L>0by the closed-form expression of F1,R.
C.4 Negative Tsallis Entropy
Forx∈[0,1], the negative Tsallis entropy is a family of regularizers parameterized by β∈(0,1):
R(x) =1−xβ
1−β. (6)
The corresponding F1,Ris defined as
F1,R(E) = argmin
x∈(0,1)
x·E+1−xβ
1−β+1−(1−x)β
1−β
.
Forx∈(0,1), we note that F1,Rhas an inverse function
F−1
1,R(x) =β
1−β 
xβ−1−(1−x)β−1
.
Lemma 8 (Assumption 2 holds for Tsallis entropy) .Consider Tsallis entropy parameterized by
β∈(0,1). Then L=1
2βand Assumption 2 holds with the following choices of constants:
1.c1=1
2−F1,R(1
20L)>0.
2.c3=1
2.
3.c2=F1,R(−c3c2
1
240L)−1
2>0.
204.δ′= min {(c2
1(1−β)
120Lβc1−β
3)1
β,c3c2
1
120,1−β
8β·c3c2
1
480L}.
Proof. We choose c3=1
2. We have c1=1
2−F1,R(1
20L)is a constant.
We note that
E0:=F−1
1,R(1
1 +δ) =β
1−β 
(1 +δ)1−β−1 +δ
δ1−β!
satisfies F1,R(E0) =1
1+δ. Similarly, we calculate
E1:=F−1
1,R(1 +c3
1 +c3+δ)
=β
1−β 1 +c3+δ
1 +c31−β
−1 +c3+δ
δ1−β!
≥β
1−β 
(1 +δ)1−β−2−1 +c3+δ
δ1−β!
≥β
1−β 
(1 +δ)1−β−1 +δ
δ1−β
−c3
δ1−β
−2!
=E0−β
1−βc3
δ1−β
+ 2
where in the first inequality we use the fact that (1 +δ)1−β≤2since δ≤1; the second inequality
we use the inequality (x+y)1−β≤x1−β+y1−β. We note that
δ≤δ1:= 
c2
1(1−β)
120Lβc1−β
3!1
β
⇒ −c2
1
30Lδ≤ −β
1−βc3
δ1−β
+ 2
. (7)
Thus for any δ≤δ1, we have for any Esuch that F1,R(E)≥1
1+δ,
−c2
1
30Lδ+E≤ −c2
1
30Lδ+E0≤E0−β
1−βc3
δ1−β
+ 2
≤E1.
The above implies F1,R(−c2
1
30Lδ+E)≥1+c3
1+c3+δand the first item in Assumption 2 is satisfied.
We define E2
E2:=F−1
1,R1
2(1 + δ)
=β
1−β 
(2 + 2 δ)1−β−2 + 2 δ
1 + 2 δ1−β!
=β
1−β(2 + 2 δ)1−β· 
1−1
1 + 2 δ1−β!
≤4β
1−β· 
1−
1−2δ
1 + 2 δ1−β!
≤4β
1−β·2δ
1 +δ
=8βδ
(1−β)(1 + δ)
where in the first inequality we use (2 + 2 δ)1−β≤4since 0≤δ≤1andβ∈(0,1); in the second
inequality we use the basic inequality (1−x)r≤1−xforr, x∈(0,1). We define
δ2:= min {c3c2
1
120,1−β
8β·c3c2
1
480L}
21Then for any δ≤δ2andEsuch that F1,R[E]≥1
2(1+δ), we have
−c3c2
1
120L+δ
4L+E≤ −c3c2
1
120L+δ
4L+E2
≤ −c3c2
1
120L+c3c2
1
480L+8βδ
(1−β)(1 + δ)
≤ −c3c2
1
120L+c3c2
1
480L+ +c3c2
1
480L
=−c3c2
1
240L.
Thus we know F1,R(−c3c2
1
120L+δ
4L+E)≥F1,R(−c3c2
1
240L)and item 2 in Assumption 2 is satisfied by
c2=F1,R(−c3c2
1
240L)−1
2>0.
Combining the above, we can choose ˆδ= min {δ1, δ2}so that both items in Assumption 2 hold for
δ≤ˆδ.
D Proof of Theorem 3
Recall the equivalent 2-dimensional problem:
ˆxt= Dupl"
argmin
x∈1
n·∆2(
n
nα*
x,t−1X
τ=1ℓτ+ℓt−1+
+n
ηR1(x[1]) +n
ηR2(x[2]))#
= Dupl"
1
n·argmin
x∈∆2(
n
nα*
1
nx,t−1X
τ=1ℓτ+ℓt−1+
+n
ηR(x/n))#
= Dupl"
1
n·argmin
x∈∆2(*
x,t−1X
τ=1ℓτ+ℓt−1+
+nα+1
ηR(x/n))#
.
Euclidean regularizer : this regularizer is homogeneous of degree two. Choosing α= 1, the inner
minimization problem is exactly the same as the one solved by OFTRL in two dimensions.
Entropy regularizer : we set α= 0to get equivalence:
nR(x/n) =2X
i=1x[i] log(x[i]/n) =2X
i=1x[i] logx[i]−2X
i=1x[i] logn=2X
i=1x[i] logx[i]−logn.
Now we have equivalence because the last term is a constant that does not affect the argmin .
Log regularizer: we set α=−1to get equivalence, using similar logic as for entropy:
R(x/n) =2X
i=1−log(x[i]/n) = 2 log n+2X
i=1−logx[i].
Tsallis entropy regularizer: we set α=−1 +βto get equivalence, using similar logic as for entropy:
nβR(x/n) =nβ·1−P2
i=1(x[i]
n)β
1−β=nβ−1
1−β+1−P2
i=1x[i]β
1−β.
E Numerical Experiments with Adaptive Stepsizes
In this section we present our numerical results when OFTRL and OOMD are instantiated with
adaptive stepsize [Duchi et al., 2011]: ηt= 1/q
ϵ+Pt−1
k=1∥ℓk∥2
kwith some constant ϵ >0. We
present our numerical experiments in Figure 4, where we choose ϵ= 0.1.
220.6 0.8 1.0
xt[1]0.00.20.40.60.81.0yt[1]Entropy (OMWU)
102105
Iteration10−310−210−1Equilibrium gap
1
0.6 0.8 1.0
xt[1]0.00.20.40.60.81.0Log regularizer
102105
Iteration10−310−210−1
1
0.6 0.8 1.0
xt[1]0.00.20.40.60.81.0Sq. Euclidean norm
102105
Iteration10−310−210−1
1
0.6 0.8 1.0
xt[1]0.00.20.40.60.81.0OGDA
101103
Iteration10−410−310−210−1
1Figure 4: Comparison of the dynamics produced by three variants of OFTRL with different regulariz-
ers (negative entropy, logarithmic regularizer, and squared Euclidean norm) and OGDA in the same
game Aδdefined in (2)forδ:= 10−2and adaptive step size with ϵ= 0.1. The bottom row shows
the duality gap achieved by the iterates.
NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The claims made in the abstract and introduction match both the theoretical
and experimental results of the paper.
Guidelines:
•The answer NA means that the abstract and introduction do not include the claims
made in the paper.
•The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
•The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
•It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The extension to the general m×ngame does not work for the Euclidean and
log regularizers because the rescaling factors would be different for the row and column
players.
Guidelines:
•The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
23•The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
•The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
•The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
•The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
•If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
•While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: See Section 2, Section 3 and the appendix.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
•All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
•All assumptions should be clearly stated or referenced in the statement of any theorems.
•The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
•Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: See Figure 1 and Python codes in the supplementary materials.
Guidelines:
• The answer NA means that the paper does not include experiments.
•If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
•If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
24•Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
•While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: See Python code in the supplementary materials.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
•Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
•While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
•The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
•The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
•The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
•At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
•Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
25Answer: [Yes]
Justification: See Python code in the supplementary materials.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
•The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: There is no randomness in our experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
•The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
•The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
•It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
•It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
•For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
•If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Our simulations were done within a few hours on an average consumer laptop.
Guidelines:
• The answer NA means that the paper does not include experiments.
•The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
•The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
•The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9.Code Of Ethics
26Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The authors have reviewed the NeurIPS Code of Ethics. The research con-
ducted in this paper conforms with it in every respect.
Guidelines:
•The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
•If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
•The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper is mostly theoretical.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
•If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
•Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
•The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
•The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
•If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
•Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
27•Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
•We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: This paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
•The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
•For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
•If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
•For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
•If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: This paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
•Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
•The paper should discuss whether and how consent was obtained from people whose
asset is used.
•At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
28•Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
•According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
•The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
•Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
•We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
•For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
29