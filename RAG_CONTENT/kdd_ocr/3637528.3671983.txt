Influence Maximization via Graph Neural Bandits
Yuting Feng
Department of Electrical and Computer
Engineering (ECE)
National University of Singapore
Singapore
yt.f@nus.edu.sgVincent Y. F. Tan
Department of Mathematics,
Department of ECE,
National University of Singapore
Singapore
vtan@nus.edu.sgBogdan Cautis
University of Paris-Saclay,
CNRS LISN
Gif-sur-Yvette, France
bogdan.cautis@universite-paris-saclay.fr
ABSTRACT
We consider a ubiquitous scenario in the study of Influence Max-
imization (IM), in which there is limited knowledge about the
topology of the diffusion network. We set the IM problem in a
multi-round diffusion campaign, aiming to maximize the number of
distinct users that are influenced. Leveraging the capability of ban-
dit algorithms to effectively balance the objectives of exploration
and exploitation, as well as the expressivity of neural networks,
our study explores the application of neural bandit algorithms to
the IM problem. We propose the framework IM-GNB (Influence
Maximization with Graph Neural Bandits), where we provide an es-
timate of the users’ probabilities of being influenced by influencers
(also known as diffusion seeds). This initial estimate forms the basis
for constructing both an exploitation graph and an exploration
one. Subsequently, IM-GNB handles the exploration-exploitation
tradeoff, by selecting seed nodes in real-time using Graph Con-
volutional Networks (GCN), in which the pre-estimated graphs
are employed to refine the influencers’ estimated rewards in each
contextual setting. Through extensive experiments on two large
real-world datasets, we demonstrate the effectiveness of IM-GNB
compared with other baseline methods, significantly improving the
spread outcome of such diffusion campaigns, when the underlying
network is unknown.
CCS CONCEPTS
•Information systems →Social recommendation; Social ad-
vertising; •Human-centered computing →Social media; So-
cial recommendation ;•Networks→Social media networks .
KEYWORDS
Information diffusion, influence, influencer marketing, contextual
bandits, graph neural networks.
ACM Reference Format:
Yuting Feng, Vincent Y. F. Tan, and Bogdan Cautis. 2024. Influence Maxi-
mization via Graph Neural Bandits. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671983
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.36719831 INTRODUCTION
Motivated by the rise of “influencer marketing” in social media
advertising, a class of algorithmic problems termed Influence Max-
imization (IM) has emerged, starting with the pioneering work
of [10,18]. These algorithms aim to identify the most influential
nodes within a diffusion network for initiating the spread of spe-
cific information, thereby maximizing its reach. In many ways, this
research directly mirrors the increasingly prevalent and successful
marketing strategy of targeting key individuals (influencers).
The objective of IM is typically formulated by maximizing the
expected spread under a stochastic diffusion model, which charac-
terizes the information dissemination process. The work of [ 18] laid
the foundations for the IM literature, by introducing two prominent
models: Linear Threshold (LT) and Independent Cascade (IC). These
models, widely adopted in subsequent research, represent diffusion
networks as probabilistic graphs, where the edges are weighted by
probabilities of information transmission.
Selecting the seed nodes maximizing the expected spread is NP-
hard under common diffusion models [ 18]. Despite the development
of approximate algorithms, exploiting the monotonicity and sub-
modularity of the spread, scaling IM algorithms to large networks
remains challenging. Acquiring meaningful influence probabilities
is equally challenging, as learning them from past information cas-
cades (e.g., as in [ 11,13]) can be data-intensive and thus impractical.
Moreover, the applicability of such models is limited in scenarios
where historical cascades are not available.
In the face of these challenges, since even the most efficient IM
algorithms such as [ 16,30] rely on assumptions and parameters
that often fail to capture the complex reality of how information
spreads online, a change in research direction has been followed
recently. It consists of approaches that neither rely on pre-defined
diffusion models nor require upfront knowledge of the diffusion
network. Instead, these online methods, such as [ 17,20,33],learn to
spread on the fly. More precisely, they involve a sequential learning
agent that actively gathers information through a multi-round influ-
ence campaign. In each round, the agent selects so-called seed nodes,
observes the resulting information spread, and uses this feedback
to make better choices in subsequent rounds, with the campaign’s
total reward being the objective that is to be optimized. Such a
learning framework leads naturally to a policy that balances explor-
ingunknown aspects (i.e., the diffusion dynamics) with exploiting
known and successful choices (i.e., the high-performing spread seed
nodes), using multi-armed bandits [21].
We consider in this paper such an online IM scenario with limited
network information. Specifically, the diffusion graph is largely
unknown, except for a set of predefined influencers, representing
the potential seeds for information dissemination at each round
 
771
KDD ’24, August 25–29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
of a multi-round diffusion campaign. Additionally, we incorporate
contextual features of both influencers and the information being
diffused. Regarding the latter, the rationale is that within a campaign
aiming to maximize the reach of a specific message, its framing
and presentation can significantly impact its spread. For instance,
a political campaign may use various formats like news articles,
opinion pieces, data visualizations, or multimedia content, each
leading to distinct diffusion patterns.
We leverage such contextual information through the formal
framework of Contextual Multi-Armed Bandits (CMABs) [ 21]. Fur-
thermore, recognizing that significant correlations between the fea-
tures of the basic (to-be-influenced) users may exist, albeit unknown
to the agent, and that by implication their activation probabilities
may be correlated, we enhance the learning framework with mech-
anisms by which each activation can provide useful information
about neighbouring users in the network as well, allowing to refine
the agent’s predictions. We achieve this by adapting to our IM prob-
lem setting the Graph Neural Bandits (GNB) framework of [ 26] (a
bandit algorithm for recommender systems). Correlation graphs
are constructed based on the similarity of users to be influenced
by the same influencer, and GNBs are then employed to handle the
challenges associated with graph-based bandit algorithm. In doing
so, our work is the first to leverage the implicit relationships that
may exist between basic users in the unknown diffusion medium.
In essence, we dynamically model these relationships based on
the observed campaign feedback, and we use them as input for a
graph neural network (GNN)-based learning algorithm guiding the
seed selection process at each round, optimizing choices under the
exploit-explore paradigm.
Overview of our IM scenario. As usual in IM scenarios, we run
campaigns under budget constraints (limited seedings and rounds),
with the goal to maximize the number of distinct users activated,
starting from known influencers. The learning agent chooses seeds
sequentially, i.e., at each round, with potential re-seeding, and feed-
back consists solely of the activated nodes after each round, without
additional details on the triggering causes. The feedback is used
to refine estimates of influencer potential, guiding future seeding
choices. Aligning with the overall objective, each round’s reward
is the number of newly activated users, and the campaign aims to
maximize the cumulative reward across rounds. In this scenario,
we mimic real-world influencer marketing, where access is limited
to a few influencers, feedback is restricted to user actions (like pur-
chases or subscriptions), and the goal is to reach as many unique
users as possible.
Our contributions. We detail our contributions in the following:
•By introducing the IM-GNB framework, we connect GNBs and
the IM problem. This integration is non-trivial due to the inherent
challenges of learning from graph-structured data and making
sequential decisions under uncertain environments in the context
of diffusion campaigns.
•We tackle the challenge of balancing exploration and exploitation
in dynamic influence propagation by incorporating contextual
bandits into the IM-GNB framework. This enables us to effec-
tively explore the potential rewards while exploiting available
information, resulting in enhanced influence spread.
•We construct user-user correlation graphs for exploitation and
exploration purposes, capturing intricate interactions amongusers and influencers in each round of the diffusion campaign.
This graph-based approach is scalable to various network set-
tings, even without prior knowledge of the network’s topology
structure.
•We develop a novel algorithm that optimally selects seed nodes
in real-time, with contextual bandits integrated with GNNs to
refine the reward estimates in each contextual setting. Through
extensive experiments, we show that our algorithm outperforms
baseline methods, highlighting the utility of GNBs as a prin-
cipled approach to optimize influence campaigns in uncertain
environments.
2 RELATED WORK
Influence Maximization (IM) addresses the challenge of identify-
ing a set of seeds (influencers) within a social network to maximize
information spread. Researchers first explored this problem in [ 10].
Later, [ 18] provided a clear formulation of the problem, including
how influence spreads through stochastic models like Independent
Cascade (IC) and Linear Threshold (LT). They also described the
important properties of the spread objective, its approximation guar-
antees and hardness results. Since then, such stochastic models have
become widely adopted in the literature, and most works focused
on finding approximate solutions that can be computed efficiently.
A key breakthrough was the concept of reverse influence sampling,
introduced in [ 6] and made practical in [ 23,30,31]. Diffusion model-
based IM approaches rely on diffusion graphs where the edges are
labeled by weights (spread probabilities). In empirical evaluations,
these weights may be data-based [ 14,15] (computed from diffusion
cascades), degree based, or simply assumed random. Some recent
studies [ 12,24] employ representation learning to infer influence
probabilities from ground-truth diffusion cascades, a resource that
may not be readily available in many application scenarios. (See
the recent survey [22] for a review of the IM literature.)
Bandits for Influence Maximization By virtue of their versatil-
ity and sequential nature, bandit algorithms are apt to be used in
IM problems, especially in uncertain diffusion environments with
which a learning agent may interact repeatedly [ 17,29,34,37]. A
multi-round, sequential setting allows to spread information and
gather feedback, striking a balance between influencing / activating
nodes in each round and learning influence parameters for uncer-
tain or unexplored network facets. This strategy closely mirrors
real-world influencer marketing scenarios, in which campaigns
often unfold over time. [ 34] is one of the earliest works that map
an IM problem formulation to a combinatorial multi-armed ban-
dit (CMAB) paradigm, where diffusions are assumed to follow the
IC model. IMLinUCB [ 35] learns the optimal influencers dynam-
ically, while repeatedly interacting with a network under the IC
assumption as well. Vaswani et al. [ 33] introduce a diffusion model-
agnostic framework, based on a pairwise-influence semi-bandit
feedback model and the LinUCB-based algorithm, addressing sce-
narios involving new marketers that exploit existing networks.
Since the aforementioned approaches leverage a given diffusion
graph topology, the inherent difficulty of obtaining such data limits
their practical interest.
Operating in highly uncertain diffusion scenarios that (i) make
no assumption on the diffusion model and (ii) lack knowledge of
the diffusion topology and historical activations (cascades), [ 20]
 
772Influence Maximization via Graph Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
proposes FAT-GT-UCB, where a Good–Turing estimator is used
to capture the utility (called remaining potential) of an influencer,
throughout the multiple rounds of a diffusion campaign. They also
consider a fatigue effect for influencers, since these may be repeat-
edly chosen in the sequential rounds. GLM-GT-UCB [ 17] considers
the same setting as [ 20], while exploiting contextual information
(e.g., features pertaining to influencers or the information being
conveyed). Our work shares a similar setting, where the network
topology is unknown and no assumptions are made about the diffu-
sion model. In a multi-round diffusion campaign, we select at each
round diffusion seeds, without factoring in influencer fatigue.
Bandits with deep learning Early works [ 1,9,28] in the con-
textual bandit literature focused on linear models, assuming the
expected reward at each round is linear in the feature vector. This
assumption, however, often fails to hold in practice, prompting ex-
ploration into nonlinear or nonparametric contextual bandits [ 7,32].
However, these more complex models impose restrictive assump-
tions on the reward function, such as Lipschitz continuity [ 7], or a re-
ward function from a reproducing kernel Hilbert space (RKHS) [ 32].
To overcome these limitations, several recent studies [ 27,38,41]
leverage the expressivity of deep neural networks (DNNs) to in-
corporate nonlinear models, which require less domain knowledge.
The works of [27, 38] employ DNNs for effective context transfor-
mation with a linear exploration policy, showing notable empirical
success despite the absence of regret guarantees. The work of [ 41]
introduces NeuralUCB, a provably efficient neural contextual bandit
algorithm using DNN-based random feature mappings to construct
the UCB, with a near-optimal regret guarantee. The construct of
the UCB is based on the past gradient of the exploitation function.
The work of [ 40] assigns a normal distribution as the distribution
of the reward of each arm, similar to the deviation computed on the
gradient of the estimation function. Similar to some other studies,
EE-Net [ 2] has an exploitation network to estimate rewards for
each arm. It additionally builds an exploration network to predict
the potential gain for each arm, relative to the current estimate,
where the inputs of the exploration network are the previous gra-
dients of the exploitation function. The work of Qi et al. [26] em-
ploys contextual neural bandits in recommender systems, to build
a graph neural bandit framework where each arm is induced with
an exploitation graph and an exploration one, with the weights of
edges representing users’ correlations regarding the exploitation
and exploration performed. The effectiveness of [ 26] in the recom-
mendation setting serves as our initial motivation for leveraging its
neural bandits framework in our IM problem. Given the similarities
in predicting user preferences (user-item in recommender systems
or user-influencer susceptibility in IM), we exploit a graph neural
contextual bandit algorithm to maximize the influence spread in
multi-round diffusion campaigns.
3 PROBLEM FORMULATION
We formulate the Influence Maximization (IM) problem with a
discrete-time diffusion model [ 18], adopting a combinatorial multi-
armed bandit paradigm to estimate the influence spread.
IM Problem Within the context of information scenarios charac-
terized by stochastic or epidemic information diffusion phenomena,
particularly on social media, the information spread is initiated byseed users (influencers) and amplified through sharing and retweet-
ing via user interactions. For a campaign of information spread
consisting of 𝑇rounds (trials), we select the influencers at each
round to maximize the overall information spread.
We are given a known base set of influencers 𝐾={𝑘𝑖}𝑛
𝑖=1as
seeds, a budget of 𝑇rounds (trials). At each round 𝑡∈{1,2...,𝑇},
the environment provides us with the message 𝐶𝑡to diffuse, and
there are𝐿∈{1,2,...,𝑛}seeds to be activated initially. With 𝐼𝑡
(which has cardinality |𝐼𝑡|=𝐿) the set of activated seeds, 𝑆(𝐼𝑡,𝐶𝑡)
is the round’s spread (all activated users) starting from the chosen
seed set𝐼𝑡. Our objective is to maximize the cumulative and distinct
spread of the 𝑇rounds, i.e., find
argmax
𝐼𝑡⊆𝐾,|𝐼𝑡|=𝐿,∀1≤𝑡≤𝑇EØ
1≤𝑡≤𝑇𝑆(𝐼𝑡,𝐶𝑡)
. (1)
Adaptation to the bandit setting To adapt the IM problem to a
contextual bandit setting, the set of influencers 𝐾can be considered
the set of arms to be pulled in 𝑇rounds. At each round 𝑡, with the
provided message 𝐶𝑡as the context, the set of arms 𝐼𝑡={𝑘𝑖}𝐿
𝑖=1
is chosen. For each chosen arm 𝑘𝑖,𝐴𝑘𝑖is the set of basic users
activated or influenced by seed (arm) 𝑘𝑖. For each basic user 𝑢,
let𝑐𝑡𝑢denote the total number of times it has been influenced or
activated until round 𝑡. With the set of activated users (influence
spread) as the node semi-bandit feedback, the reward is the number
of new activations [17] as
𝑅𝑡=∑︁
𝑢∈Ð
𝑘𝑖∈𝐼𝑡𝐴𝑘𝑖1{𝑐𝑡
𝑢>0}−𝑅𝑡−1;𝑅0=0, (2)
Note that distinct activations are used for the cumulative reward,
i.e., a given user will be counted only once in the total reward, even
if it has been influenced several times.
Modeling with graph bandits We are mainly motivated by appli-
cation scenarios in social media (e.g., information campaigns for
elections, online advertising, public awareness campaigns, crisis
information diffusion, etc.), where users may exhibit similar prefer-
ences and influence susceptibility for certain diffusion topics (e.g.,
sharing the same political views) initiated by certain influencers
(arms), while they may react differently and be more susceptible to
other influencers for other topics (e.g., entertainment or sports).
Thus, instead of representing the social graph uniformly, in
the bandit setting, we allow each arm 𝑘𝑖at each round 𝑡to in-
duce a distinct graph𝐺𝑖,𝑡(U,𝐸,𝑊𝑖,𝑡)to represent user connectivity.
With 𝒌𝑖the𝑑1-dimensional feature vector of arm 𝑘𝑖and𝑪𝑡the𝑑2-
dimensional context vector, the expected reward1at each round
𝑡∈[𝑇]brought by arm 𝑘𝑖is defined as
𝑟𝑖,𝑡=𝑓(𝒌𝑖,𝑪𝑡,𝐺𝑖,𝑡). (3)
In𝐺𝑖,𝑡, each user 𝑢∈U ={1,2,...,𝑚}corresponds to a node,
𝐸is the set of edges connecting users, and 𝑊𝑖,𝑡={𝑤𝑖,𝑡(𝑢,𝑢′):
𝑢,𝑢′∈U} is the set of weights corresponding to each edge 𝑒∈𝐸.
Modeling real applications, we assume that the weights of the edges
connecting nodes in 𝐺𝑖,𝑡represent users’ similarity w.r.t. the same
influencer (arm 𝑘𝑖), i.e., the probability to be similarly influenced
1Note that this expected reward 𝑟𝑖,𝑡is assessing the distinct activations by arm 𝑘𝑖at
round𝑡, in alignment with the reward brought by each arm in 𝐼𝑡, as defined in Eq. (2).
 
773KDD ’24, August 25–29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
by arm𝑘𝑖in round𝑡, which is defined as
𝑤𝑖,𝑡(𝑢,𝑢′)=Φ(1)
E
𝑝𝑡
𝑖,𝑢|𝒌𝑖,𝑪𝑡
,E
𝑝𝑡
𝑖,𝑢′|𝒌𝑖,𝑪𝑡
, (4)
where𝑝𝑡
𝑖,𝑢=ℎ𝑢(𝒌𝑖,𝑪𝑡)∈[ 0,1]theexpected diffusion probabil-
itybetween influencer (arm 𝑘𝑖) and user𝑢under the context 𝑪𝑡,
andΦ(1):R×R→Rmaps the expected diffusion probability of
users w.r.t. influencer 𝑘𝑖to the weights among users in 𝐺𝑖,𝑡.
However, the similarity graph 𝐺𝑖,𝑡and the function ℎ𝑢are un-
known in our problem setting. Thus we propose an estimate graph
𝐺(1)
𝑖,𝑡=(U,𝐸,𝑊(1)
𝑖,𝑡)to approximate 𝐺𝑖,𝑡by exploiting the current
observations. This is known as the exploitation graph. We also con-
sider a pre-defined hypothesis function ℎ(1)
𝑢(𝒌𝑖,𝑪𝑡)to approximate
the expected diffusion probability 𝑝𝑡
𝑖,𝑢, so as to estimate 𝑊𝑖,𝑡with
𝑊(1)
𝑖,𝑡in the graph 𝐺(1)
𝑖,𝑡. With the pre-estimated graph 𝐺(1)
𝑖,𝑡, the
estimate reward of arm 𝑘𝑖across all users is then expressed as
ˆ𝑟𝑖,𝑡=𝑓(1) 𝒌𝑖,𝑪𝑡,𝐺(1)
𝑖,𝑡. (5)
To quantify the estimation gap (uncertainty of estimation) between
𝐺(1)
𝑖,𝑡and𝐺𝑖,𝑡(or to measure the potential gain on the estimated
diffusion probability for each user-influencer pair), we also propose
anexploration graph, denoted by 𝐺(2)
𝑖,𝑡=(U,𝐸,𝑊(2)
𝑖,𝑡), where analo-
gously the weights among users 𝑤(2)
𝑖,𝑡(𝑢,𝑢′)∈𝑊(2)
𝑖,𝑡indicate users’
correlations w.r.t. potential gains, expressed as
𝑤(2)
𝑖,𝑡(𝑢,𝑢′)
=Φ(2) ℎ𝑢(𝒌𝑖,𝑪𝑡)−ℎ(1)
𝑢(𝒌𝑖,𝑪𝑡),ℎ𝑢′(𝒌𝑖,𝑪𝑡)−ℎ(1)
𝑢′(𝒌𝑖,𝑪𝑡)(6)
for some function Φ(2):R×R→Rwhich is similar to Φ(1).
With the exploration graph 𝐺(2)
𝑖,𝑡, the potential gain of arm 𝑘𝑖
across all the users is defined as ˆ𝑏𝑖,𝑡=𝑓(2)(𝒌𝑖,𝑪𝑡,𝐺(2)
𝑖,𝑡). At each
round𝑡, the arm set 𝐼𝑡is selected as arg max𝐼𝑡⊂𝐾:|𝐼𝑡|=𝐿(ˆ𝑟𝑖,𝑡+ˆ𝑏𝑖,𝑡).
This maximizes the overall influence spread in the campaign.
The details of the constructions of the exploitation and explo-
ration graphs are given in Sec. 4 below.
4 PROPOSED FRAMEWORK
Many recent works [ 17,35] on the IM problem that exploit bandits
for the exploration-exploitation trade-off assume that the reward is
a linear or generalized linear function of arm vectors. Considering
the high complexity and dynamicity of social network-related data,
we use the representation power of neural networks to firstly, learn
users’ connectivity to build exploitation and exploration graphs and
secondly, learn the underlying reward function and the potential
gains on the estimated reward. The overall framework of our model
is illustrated in Fig. 1.
4.1 Estimating the User Graphs
In this section, we first provide a strategy to estimate the users’
correlations to be influenced by the same arm, forming the basis
for the exploration-exploitation strategy in Sec. 4.2.
4.1.1 User exploitation graph. We bridge the users in the social
graph with diffusion probabilities between influencers and users.
The intuition is that given the same message to be diffused (context
𝐶𝑡), users who exhibit high correlations in this graph are more likely
to be influenced by the same influencer. As the context changes, aninfluencer may not exert the same influence on users. Thus, at each
round𝑡and for each influencer (arm) 𝑘𝑖, we induce an exploitation
graph𝐺(1)
𝑖,𝑡to represent the users’ correlations.
In the exploitation graph 𝐺(1)
𝑖,𝑡, the weights among users are
referred to as users’ correlations w.r.t. the diffusion probability from
arm𝑘𝑖(hence likelihood to influenced by the same influencer 𝑘𝑖).
For each user 𝑢∈U, we use a neural network as the pre-defined
hypothesis function ℎ(1)
𝑢=ℎ(1)
𝑢(𝒌𝑖,𝑪𝑡;[P(1)
𝑢]𝑡−1)to learn these
probabilities ([P(1)
𝑢]𝑡−1are the updated parameters of the network
from round 𝑡−1). The weights in 𝐺(1)
𝑖,𝑡are
𝑤(1)
𝑖,𝑡(𝑢,𝑢′)=Φ(1) ℎ(1)
𝑢(𝒌𝑖,𝑪𝑡),ℎ(1)
𝑢′(𝒌𝑖,𝑪𝑡), (7)
where Φ(1)is the same function in Eq. (4). For each user 𝑢,ℎ(1)
𝑢will
be trained by gradient descent (GD) with the given context and the
chosen arm as input and the reward as label. The loss is defined as
L(1)
𝑢=
ℎ(1)
𝑢 𝒌𝑖,𝑪𝑡;P(1)
𝑢−𝑑𝑡
𝑢2
, (8)
where𝑑𝑡𝑢=1if𝑐𝑡𝑢>0and𝑐𝑡−1𝑢=0, else𝑑𝑡𝑢=0. Recall that we use
𝑐𝑡𝑢to denote the total number of times user 𝑢has been activated
(influenced) up to and including round 𝑡, and we only count the
newly activated nodes at each round.
4.1.2 User exploration graph. Recent works on neural bandits [ 2–
4,40,41] take advantage of the representation power of neural
networks to learn the uncertainty of estimation (potential gain).
These works use the past gradient to incorporate the feature of arms
and the learned discriminative information of estimation function
(ℎ(1)
𝑢(𝒌𝑖,𝑪𝑡)in our work).
Qi et al. [ 26] applied this paradigm in collaborative filtering
for user-item pair prediction in online recommendation scenarios
and demonstrated its effectiveness. Since the IM problem shares
similarity with predicting user preferences towards items (in our
case susceptibility to influencers), especially when the connections
(correlations) among users are reinforced by social ties, we apply
the past gradient to quantify the “exploration bonus” [21].
For a user𝑢∈U , we use a neural network ℎ(2)
𝑢to learn the
uncertainty of the estimated diffusion probability between arm
𝑘𝑖and user𝑢, i.e.,E[𝑝𝑖,𝑡|𝑢,𝒌𝑖,𝑪𝑡]−ℎ(1)
𝑢(𝒌𝑖,𝑪𝑡), similar to Eq. (6).
As in [ 2], we apply ℎ(2)
𝑢directly on the previous gradient of ℎ(1)
𝑢.
Analogously, the exploration graph 𝐺(2)
𝑖,𝑡=(U,𝐸,𝑊(2)
𝑖,𝑡)is con-
structed with 𝑊(2)
𝑖,𝑡=
𝑤(2)
𝑖,𝑡(𝑢,𝑢′):𝑢,𝑢′∈U	
, and𝑤(2)
𝑖,𝑡(𝑢,𝑢′)is
the exploration correlation among users, defined as
𝑤(2)
𝑖,𝑡(𝑢,𝑢′)=Φ(2)
ℎ(2)
𝑢 ∇ℎ(1)
𝑢,ℎ(2)
𝑢′ ∇ℎ(1)
𝑢′
. (9)
The previous gradient ∇ℎ(1)
𝑢(𝒌𝑖,𝑪𝑡)=∇Pℎ(1)
𝑢(𝒌𝑖,𝑪𝑡;[P(1)
𝑢]𝑡−1)
is the network gradient at round 𝑡−1, with[P(1)
𝑢]𝑡−1the last up-
dated parameters of ℎ(1)
𝑢. In addition, Φ(2)is the function defined
in Eq. (6)andℎ(2)
𝑢will be trained with GD, where the previous
gradient ofℎ(1)
𝑢is computed based on the input samples, and the
residual diffusion probability (potential gain on the estimated diffu-
sion probability) is the label, with the loss given as
L(2)
𝑢=
ℎ(2)
𝑢 ∇ℎ(1)
𝑢(𝒌𝑖,𝑪𝑡)− 𝑑𝑡
𝑢−ℎ(1)
𝑢(𝒌𝑖,𝑪𝑡)2
. (10)
 
774Influence Maximization via Graph Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
ℎ𝑢1ሺ1ሻ 𝑤13ሺ1ሻ 𝑝̂𝑖𝑢1  
𝑤23ሺ1ሻ 𝑤12ሺ1ሻ 
𝑟̂𝑖,𝑡ൌඨ෍൫𝑝̂𝑖,𝑢𝑡൯2
𝑖∈𝒰 GCN FC  Network Exploitation  graph
...Input Output
𝑤13ሺ2ሻ   
𝑤23ሺ2ሻ 𝑤12ሺ2ሻ 
GCN FC  Network Exploration  graph...Input Output
𝑏෠𝑖,𝑡ൌඨ෍൫𝑧̂𝑖,𝑢𝑡൯2
𝑖∈𝒰 
ℎ𝑢1ሺ2ሻ ℎ𝑢2ሺ2ሻ 
ℎ𝑢3ሺ2ሻ 
ℎ𝑢2ሺ1ሻ 
ℎ𝑢3ሺ1ሻ 
∇ℎ𝑢ሺ1ሻ ∇𝑓ሺ1ሻ ℎ𝑢ሺ1ሻ൫𝒌𝒊,𝒕,𝑪𝒕൯
ℎ𝑢ሺ2ሻ൫∇ℎ𝑢ሺ1ሻ൯𝐺𝑖,𝑡ሺ1ሻ 
𝐺𝑖,𝑡ሺ2ሻ 𝑓ሺ1ሻ 
𝑓ሺ2ሻ 𝑝̂𝑖𝑢2 𝑝̂𝑖𝑢𝑚 
𝑧̂𝑖𝑢1 𝑧̂𝑖𝑢2 𝑧̂𝑖𝑢𝑚 
Potential  gainReward Exploitation
Graph estimation Reward and potential  gain estimation
𝑤𝑢1,𝑢2ሺ2ሻൌΦሺ2ሻ൫hu1ሺ2ሻ,hu2ሺ2ሻ൯ Feature vector 
& context vector
Exploration𝑤𝑢1,𝑢2ሺ1ሻൌΦሺ1ሻ൫hu1ሺ1ሻ,hu2ሺ1ሻ൯ 
argmax
𝐼𝑡⊂𝐾,|𝐼𝑡|ൌ𝐿ሺ𝑟̂𝑖,𝑡൅𝑏෠𝑖,𝑡ሻ Arm selection
Figure 1: The framework of IM-GNB. For each arm, we initially take the arm feature vector and the current context vector
(𝒌𝑖,𝑪𝑡)as inputs to estimate the diffusion probability for each user-arm pair with ℎ(1)
𝑢. Subsequently, we assess the potential
gain on the diffusion probability with the past gradient of ℎ(1)
𝑢, yielding both exploitation and exploration graphs. With the
pre-estimated graphs, we refine the estimate of the diffusion probability for each user-arm pair with 𝑓(1)and𝑓(2). The aggregate
reward of the arm across all users is derived from the sum of all the refined individual diffusion probabilities. The potential
gain is measured similarly. Finally, we select the arm with the highest sum of estimated reward and its potential gain.
Regarding the network structure of ℎ(1)andℎ(2), since there
are no data characteristics requiring specific models such as RNNs
for sequential dependencies or CNNs for visual content, we simply
employ an𝐽-layer fully connected (FC) neural network at this stage
for initial graph estimation.
To summarise, we use ℎ(1)
𝑢, denoting user 𝑢, to obtain the es-
timated diffusion probability from influencer (arm) 𝑘𝑖to𝑢(the
estimation function is built for each user individually, i.e., there are
𝑚estimation functions ℎ(1)
𝑢in total), and the exploitation graph
𝐺(1)
𝑖,𝑡for arm𝑘𝑖is built such that the basic users are correlated
based on estimated diffusion probabilities. We also apply ℎ(2)
𝑢to
represent the uncertainty of the estimated diffusion probability,
and the exploration graph 𝐺(2)
𝑖,𝑡corresponding to arm 𝑘𝑖is built
such that the users are correlated based on the potential gains. The
graph estimation process is given in Lines 13–17 of Algorithm 1.
4.2 Exploitation-Exploration with GNNs
With the user correlation graphs 𝐺(1)
𝑖,𝑡and𝐺(2)
𝑖,𝑡forexploitation
andexploration respectively, we now have a refined estimate of the
diffusion probabilities between influencers and users, as well as the
expected total spread (newly activated users), i.e., the reward.
4.2.1 GNN for exploitation. In round𝑡, for each arm 𝑘𝑖, with the
pre-estimated exploitation graph 𝐺(1)
𝑖,𝑡for arm𝑘𝑖as input, we use
a GNN model 𝑓(1)(𝒌𝑖,𝑪𝑡,𝐺(1)
𝑖,𝑡;P(1))to estimate the reward de-
scribed in Eq. (3), with P(1)representing the parameters of 𝑓(1).We define for each arm a symmetric adjacency matrix 𝐴(1)
𝑖,𝑡∈
R𝑚×𝑚from the exploitation graph 𝐺(1)
𝑖,𝑡, with each element in the
matrix corresponding to the correlations weights 𝑤𝑢,𝑢′between
user𝑢and user𝑢′in𝐺(1)
𝑖,𝑡, and the normalized adjacency matrix [ 19]
being𝑆(1)
𝑖,𝑡=𝐷−1
2𝐴(1)
𝑖,𝑡𝐷−1
2, with𝐷the degree matrix. We concate-
nate the arm feature vector 𝒌𝑖with the context vector 𝑪𝑡to build
the feature matrix 𝑿𝑖,𝑡=diag([𝒌𝑖,𝑪𝑡],[𝒌𝑖,𝑪𝑡],...,[𝒌𝑖,𝑪𝑡]) ∈
R𝑚×𝑚(𝑑1+𝑑2).
We adopt a simplified Graph Convolutional Network (GCN)
model [ 36] to learn the aggregated representation of the exploita-
tion graph. With 𝑆(1)
𝑖,𝑡and𝑿(1)
𝑖,𝑡as inputs, the feature representation
matrix is expressed as
𝐻G=𝜎 𝑆(1)
𝑖,𝑡𝛾𝑿(1)
𝑖,𝑡;P(1)
G
, (11)
where𝜎is the activation function, P(1)
G∈R𝑚(𝑑1+𝑑2)×𝑝is the
trainable weight matrix in the GCN model, and 𝛾is the number of
hops the information propagating over the user graph, indicating
that after𝑘layers a node obtains the feature information from all
nodes found 𝛾hops away in the graph. In the GCN model, 𝑿(1)
𝑖,𝑡is
applied to the corresponding weight matrix P(1)
Gso thatP(1)
Gis
partitioned for each user 𝑢∈U to get the𝑝-dimensional arm-user
diffusion representation, corresponding to each row of 𝐻G∈R𝑚×𝑝.
To further refine the 𝑝-dimensional arm-user pair representation
in𝐻G, we add an 𝐽-layer FC neural network to the GCN model,
and for𝑙∈{1,2,...,𝐽−1}the representation for each layer is
𝐻𝑙=𝜎 𝐻𝑙−1·P(1)
𝑙,and𝐻0=𝐻G, (12)
 
775KDD ’24, August 25–29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
with𝐻𝑙∈R𝑚×𝑝, andP(1)
𝑙being the trainable parameters in each
layer in the FC network. For the last layer, we have
ˆ𝑃𝑖,𝑡=𝐻𝐽−1·P(1)
𝐽, (13)
where theP(1)
𝐽are the parameters in the last layer, and ˆ𝑃𝑖,𝑡∈R𝑚is
the𝑚-dimensional vector with each element the refined estimated
diffusion probability ˆ𝑝𝑡
𝑖,𝑢∈Rbetween arm 𝑘𝑖and user𝑢∈U.
With the refined estimated diffusion probability between arm
𝑘𝑖and all the users, the estimated reward for arm 𝑘𝑖across all the
users is computed as the norm of the output layer:
ˆ𝑟𝑖,𝑡=∥ˆ𝑃𝑖,𝑡∥=√︄∑︁
𝑢∈U ˆ𝑝𝑡
𝑖,𝑢2. (14)
The exploitation network will be trained with GD based on the
influence spread from arm 𝑘𝑖, where the predicted outputs are the
diffusion probabilities across all users, with the label (reward)
𝑟𝑖,𝑡=∑︁
𝑢∈𝐴𝑘𝑖𝑑𝑡
𝑢. (15)
Recall that𝐴𝑘𝑖is the set of users activated or influenced by arm 𝑘𝑖
at round𝑡, and𝑑𝑡𝑢is defined as the distinct activations in Eq. (8).
For refined learning on each user-arm diffusion pair, we calculate
the quadratic loss w.r.t. each user individually, as
L(1)=∑︁
𝑢∈𝐴𝑘𝑖 ˆ𝑝𝑡
𝑖,𝑢−𝑑𝑡
𝑢2. (16)
4.2.2 GNN for exploration. Similar to the user graph pre-estimation
described in Sec. 4.1, we follow the exploration-exploitation strat-
egy by applying a gradient-based exploration function w.r.t. the
exploitation function; also see [2, 26, 40, 41] for similar strategies.
In round𝑡, for each arm 𝑘𝑖, with the induced exploration graph
𝐺(2)
𝑖,𝑡where the pre-estimated weights in Sec. 4.1.2 represent the ex-
ploration correlations among users, we apply another GNN model
𝑓(2)(∇𝑓(1),𝐺(2)
𝑖,𝑡;P(2))to evaluate the potential gain (the gap be-
tween expected reward and estimated reward) for arm 𝑘𝑖, where
∇𝑓(1)=∇P𝑓(1)(𝒌𝑖,𝑪𝑡;[P(1)]𝑡−1), andP(1)andP(2)are the
parameters of 𝑓(1)and𝑓(2)respectively.
We adopt the same network architecture as in the exploitation
network to learn the representation matrix for the exploration graph
with a𝑘-hop simplified GCN, and to predict the potential gain with
an𝐽-layer FC neural network. The architecture of 𝑓(2)can be
also implemented via Eqs. (11)–(13), with the input feature vector
𝑿(2)
𝑖,𝑡∈R𝑚×𝑚𝑞and trainable matrix P(2)
G∈R𝑚𝑞×𝑝in the GCN
model, where 𝑞is the dimension of input gradient. In the GCN of
𝑓(2), the input gradient matrix 𝑿(2)
𝑖,𝑡is similarly applied to partition
the weight matrixP(2)
G, so that each user-arm pair is represented
by a𝑝-dimensional vector for the purpose of exploration.
In the output layer we obtain an 𝑚-dimensional vector ˆ𝑍𝑖,𝑡,
where each element represents the estimated potential gain ˆ𝑧𝑡
𝑖,𝑢∈
R,𝑢∈U (with|U|=𝑚) for each user-arm pair. With the esti-
mated potential gains from the output layer, the overall estimated
potential gain for arm 𝑘𝑖is obtained as the norm of output ˆ𝑍𝑖,𝑡, i.e.,
ˆ𝑏𝑖,𝑡=∥ˆ𝑍𝑖,𝑡∥=√︄∑︁
𝑢∈U ˆ𝑧𝑡
𝑖,𝑢2. (17)
When training 𝑓(2)with GD, the quadratic loss is computed
between the estimated potential gain and the residual gain (the gapbetween the reward in Eq. (15) and the estimated reward), as
L(2)=∑︁
𝑢∈𝐴𝑘𝑖
ˆ𝑧𝑡
𝑖,𝑢− 𝑑𝑡
𝑢−ˆ𝑝𝑡
𝑖,𝑢2
. (18)
The computations of the reward and potential gain are summa-
rized in Lines 5–7 in Algorithm 1.
Algorithm 1: IM-GNB
Input: Influencer set 𝐾, number of selections 𝐿per round
Output: Arm recommendation for each time step 𝑡
1Initialization of all the trainable parameters
2for𝑡=1,2,3,...,𝑇 do
3 Receive from environment the context 𝑪𝑡
4 for𝑘𝑖∈𝐾do
5 construct two user graphs 𝐺(1)
𝑖,𝑡and𝐺(2)
𝑖,𝑡from
Procedure Estimating graphs for arm 𝑘𝑖
6 Compute estimate of reward
ˆ𝑟𝑖,𝑡=𝑓(1)(𝒌𝑖,𝑪𝑡,𝐺(1)
𝑖,𝑡;[P(1)]𝑡−1)
7 and, potential gain
ˆ𝑏𝑖,𝑡=𝑓(2)(∇[𝑓(1)]𝑖,𝑡,𝐺(2)
𝑖,𝑡;[P(2)]𝑡−1)
8 choose arm set 𝐼𝑡=arg max𝐼𝑡⊂𝐾,|𝐼𝑡|=𝐿(ˆ𝑟𝑖,𝑡+ˆ𝑏𝑖,𝑡)and
observe the true reward (spread) 𝑅𝑡in Eq. (2), which
represents the newly activated users.
9 for𝑢∈Udo
10 train the user networks ℎ(1)
𝑢(·;P(1)
𝑢),ℎ(2)
𝑢(·;P(2)
𝑢)
11 for𝑘∈Kdo
12 train the GNN models 𝑓(1)(·;P(1)),𝑓(2)(·;P(2))
13Procedure, Estimating graphs for arm( 𝑘𝑖,𝑡)
14 foreach user pair(𝑢,𝑢′)∈U×U do
15 Foredge weight 𝑤(1)
𝑖,𝑡(𝑢,𝑢′)∈𝑊(1)
𝑖,𝑡,update
𝑤(1)
𝑖,𝑡(𝑢,𝑢′)=Φ(1)(ℎ(1)
𝑢(𝑘𝑖,𝑡),ℎ(1)
𝑢′(𝑘𝑖,𝑡))
16 Foredge weight 𝑤(1)
𝑖,𝑡(𝑢,𝑢′)∈𝑊(1)
𝑖,𝑡,update
𝑤(2)
𝑖,𝑡(𝑢,𝑢′)=Φ(2)(ℎ(2)
𝑢(∇ℎ(1)
𝑢),ℎ(2)
𝑢′(∇ℎ(1)
𝑢′))
17 return𝐺(1)
𝑖,𝑡and𝐺(2)
𝑖,𝑡
4.2.3 IM-GNB arm selection. We summarize the IM-GNB frame-
work in Algorithm 1. For an information diffusion campaign with
𝑇rounds, we select 𝐿influencers (arms) from a known influencers
base𝐾at each round 𝑡to diffuse the given message 𝑪𝑡. At each
round𝑡for each arm 𝑘𝑖, we firstly construct the two user graphs
i.e., the exploitation graph and the exploration graph via a procedure
(Lines 13–17) of pre-estimation on graph weights, which capture
users’ correlations in terms of exploitation and exploration respec-
tively. With the derived graphs, we compute the overall expected
reward ˆ𝑟𝑖,𝑡and potential gain ˆ𝑏𝑖,𝑡for each arm in Eqs. (14)and(17),
as the norms of the output vectors from 𝑓(1)and𝑓(2). Next, we
select the arm set based on the maximum of the sum of reward
estimation and potential gain ˆ𝑟𝑖,𝑡+ˆ𝑏𝑖,𝑡(Line 8). Finally, for each user
𝑢∈U, we train the user’s neural networks from pre-estimation,
and we train for each arm 𝑘𝑖∈𝐾the GNN models (Lines 9–12).
We observe from the above that at each round 𝑡,ℎ(1)
𝑢will take as
input the feature vector of a certain arm 𝑘𝑖, along with the context
 
776Influence Maximization via Graph Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
vector 𝑪𝑡to provide an initial estimate on the diffusion probability
for user𝑢being influenced by arm 𝑘𝑖. Subsequently, the gradient
ofℎ(1)
𝑢is employed as input to estimate the potential gain in diffu-
sion probability. Parameters in ℎ(1)
𝑢andℎ(2)
𝑢undergo continuous
training and updating at each round to refine the approximation
functions for user 𝑢, predicting the probability of being influenced
by any arm within various contexts. Similarly, for the reward esti-
mation for each arm with 𝑓(1)and𝑓(2),𝑓(1)takes as input the arm
feature vector and context vector, as well as the pre-estimated graph
𝐺(1)
𝑖,𝑡to refine the initial estimation on the diffusion probability, al-
lowing to estimate the reward across all users, and the gradient of
𝑓(1)serves as the input of exploration function 𝑓(2). Both𝑓(1)and
𝑓(2)undergo continuous training to refine the reward estimation
function (exploitation) and potential gain (exploration). This itera-
tive process ensures that 𝑓(1)and𝑓(2)adapt effectively to diverse
contexts and user correlation graphs.
4.2.4 Complexity Analysis. Recall from the previous notation con-
ventions that we have |𝐾|=𝑛arms,𝑚users, and the dimensions
of the feature vectors and context information are 𝑑1and𝑑2respec-
tively. For simplicity, we use 𝑑𝑔to denote the dimension of all the
input gradients, and we assume that the same structure is used for
all the FC neural networks in our model. In particular, each neural
network has 𝐽layers and each layer has 𝑛neurons.
For the pre-estimation of user exploitation and exploration graphs,
at each round, the complexity of the pre-defined hypothesis func-
tionsℎ(1)
𝑢andℎ(2)
𝑢(FC neural networks) is 𝑂 |𝐾|𝑚𝐽(𝑑1+𝑑2)𝑛for
exploitation and 𝑂(|𝐾|𝑚𝐽𝑑𝑔𝑛)for exploration.
For the refined estimation procedure where we use GCNs, as we
predict correlations among all users, the graphs can be seen as com-
plete. Assuming that the exploration / exploitation GCNs share the
same NN structure, the time and space complexities for exploitation
are𝑂 |𝐾|𝐽(𝑚2(𝑑1+𝑑2)+𝑚(𝑑1+𝑑2)2)and𝑂 |𝐾|𝐽(𝑚2+(𝑑1+𝑑2)2+
𝑚(𝑑1+𝑑2))respectively, and for exploration 𝑂 |𝐾|𝐽(𝑚2𝑑𝑔+𝑚𝑑2𝑔)
and𝑂 |𝐾|𝐽(𝑚2+𝑑2𝑔+𝑚𝑑𝑔).
From the above discussion, we can observe that the number of
users and the dimension of the input gradient are the most critical
parameters in determining the time complexity. We consider the
following methods to reduce the computational complexity.
User clustering. In applications with billions of users on social
media, it is impractical and excessively costly to predict diffusion
probabilities and correlations at the granularity of individual users.
In response to this challenge, we can leverage the posting activity
(e.g., retweeting history) of users to construct a topic distribution
vector for each user. We can then cluster users into a specific num-
ber of groups, with each user group representing a macro-node
in a smaller social graph. Notably, the theoretical underpinnings
outlined earlier remain applicable to these clustered user groups,
and user𝑢becomes𝑢𝑐𝑖,𝑖=1,2,...,𝑚′and∪𝑚′
𝑖=1𝑢𝑐𝑖=U, with𝑚′
denoting the number of clustered groups. Despite this adjustment,
we continue to refer to the user group 𝑢𝑐𝑖as user𝑢for simplicity.
The introduction of clustering can significantly reduce compu-
tational cost, transforming the space complexity of the adjacency
matrix in the GCN from 𝑚×𝑚to a more computationally efficient
scale. Experiments are carried out on the number of clusteringgroups in Sec. 5 to show the impact of the number of clusters on
the performance of the model.
Input gradients. In Sec. 4.1.2 and Sec. 4.2.2, we saw that the in-
put dimensions for the previous gradients can pose computational
challenges due to their potentially large values. This is particu-
larly relevant in Sec. 4.2.2, where the input gradient dimension is
𝑚(𝑑1+𝑑2)𝑝+(𝐽−1)𝑝2+𝑝. To address this issue, and inspired by
approaches commonly employed in CNN-related works, we use the
average pooling technique to effectively reduce the input dimension
and improve efficiency.
5 EXPERIMENTS
In this section, we evaluate our model IM-GNB on datasets from
Twitter and Sina Weibo. We compare it with baselines also designed
for multi-round diffusion campaigns, and we analyze the compar-
ison results in the end. For reproducibility, the IM-GNB code is
available at https://github.com/goldenretriever-5423/IM_GNB.
Datasets Twitter and Weibo are two of the largest social media
platforms. We collected the Twitter dataset through its API. In our
context analysis for Twitter, we apply 𝐾-means clustering on the
public vocabulary glove-twitter-200 [25], available from the Gen-
sim word embedding open-source library2. The resulting clusters
provide centroids that serve as representative themes within the
dataset. Subsequently, we represent them as a distribution across
these centroids (10 in our experiments) to encode tweets. Each word
in a tweet is assigned to its nearest centroid, resulting in the overall
distribution. The feature vector of the influencer is the normalized
aggregation of all its historical tweets. The Weibo dataset [ 39] is
a publicly available one built for information diffusion studies. In
this dataset, each post is encoded with a distribution over the 100
topics [ 39] using latent Dirichlet allocation [ 5]. Similar to the Twit-
ter dataset, the feature vector of the influencer is the normalized
aggregate of the topic distribution of all historical tweets.
To simulate campaigns on social media, we assume that the
marketer has access to only a few of the most important influencers
to diffuse the message in the campaigns. Hence, we fix the size of
the influencer set 𝐾by selecting the users with the highest number
of reposts in our Twitter and Weibo logs, and we keep all the
tweets related to them. The statistics of the datasets before and after
filtering are given in Tables 1 and 2. In each campaign, we randomly
chose the contexts (tweets), referred to as topic distributions, for
each round from the pool of available contexts within the dataset.
Table 1: Description of original datasets.
#users #original tweets # retweets
Twitter 11.6M 242M 341.8M
Weibo 1.8M 300K 23.8M
Table 2: Description of filtered datasets.
#users #original tweets # retweets
Twitter 31.6k 19k 1M
Weibo 54.4k 6K 1M
Baselines We compare IM-GNB to a set of bandit models designed
for the IM problem under the same multi-round campaigns scenario,
2https://pypi.org/project/gensim/
 
777KDD ’24, August 25–29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
where the underlying network is unknown and no assumptions
about the diffusion models are made. LogNorm-LinUCB [ 17] and
GLM-GT-UCB [ 17] are the recent, state-of-the-art approaches to
maximize information diffusion during such IM campaigns. LogNorm-
LinUCB directly adapts the LinUCB algorithm by using logarith-
mic normalization and contextual information to make sequential
selections of spread seeds, while GLM-GT-UCB employs a gener-
alized linear model and the Good–Turing estimator to determine
the remaining potential of influencers. We also compare with FAT-
GT-UCB [ 20], a context-free model which has the particularity to
consider the fatigue, i.e., an influencers’ diminishing tendency to
activate basic users as they are re-seeded throughout the campaign.
We also generalize several state-of-the-art neural bandit methods–
NeuralUCB [ 40],NeuralTS [ 41], and LinUCB [ 8] to our multi-round
IM campaign. Finally, we also implement a reference model that
randomly chooses the influencer(s) at each round, as in [17].
Experimental Setting In our experiments, to reduce the compu-
tation cost, we first cluster all the users into 50groups. For the
pre-estimation of the graph weights, we use a 3-layer FC neural
network as the hypothesis class for both ℎ(1)
𝑢andℎ(2)
𝑢, to estimate
the diffusion probability and potential gain. The functions Φ(1)and
Φ(2)that map the users’ correlations to the weights in the graphs
are radial basis functions (RBFs), with their bandwidths set to 5. For
the GCN model, we explore the use of 3 hops (i.e., 𝛾=3) to capture
multi-level relationships within the user graphs, with a 3-layer FC
neural network connected at the end. The pooling step sizes to
reduce dimensions [ 26] for the input gradients in 𝑓(2)are set to
1,000and10,000respectively for the Twitter and Weibo datasets.
Empirical Results For a diffusion campaign, at each round, the
environment first provides the context, an algorithm then selects
the round’s influencer(s), and finally, a tweet is sampled for the
specific pair of influencer(s) and context from the dataset. The new
activations are determined by discounting the users previously
encountered from the set of users associated with the sampled
tweet. All our empirical results are averaged over 100 independent
runs (the means and standard deviations are reported), and the
diffusion budget is set to 500 rounds.
Comparison with baselines: We conducted comparisons with various
baselines on the Twitter and Weibo datasets, varying the number
of chosen seeds ( 𝐿) per round within {1,2,..., 5}. The results are
shown in Fig. 2 and Fig. 3 respectively. From the two figures, we can
observe that, across both datasets, our model IM-GNB generally
outperforms the baselines. Notably, on the Twitter dataset, IM-GNB
exhibits a significantly increased advantage over the baselines, as
the number of seeds increases up to 𝐿=3. However, this advantage
diminishes as 𝐿continues to grow, as the probability of selecting
the correct arms increases for all models. In fact, for the extreme
scenario where 𝐿=|𝐾|=10, this results in the same performances
across all models due to the selection of the entire base set of seeds.
Similarly, for the Weibo dataset, IM-GNB demonstrates its largest
advantage at 𝐿=4. These results validate our motivation to leverage
the expressivity of both neural networks and bandit algorithms in
IM campaigns, enabling us to effectively capture dynamic user-user
and user-influencer interactions using GNBs.
It is worth noting that, in the Weibo dataset, particularly when
𝐿is small (e.g., 𝐿=1,2), the performance in the initial roundsis surpassed by certain baseline methods, notably GLM-GT-UCB,
which exhibit more rapid learning capabilities. We attribute this
phenomenon to the nature of IM-GNB as a data-driven model, typ-
ical of modern deep learning-based approaches. The efficiency of
IM-GNB improves rapidly with the accumulation of data (i.e., as
more rounds pass by), indicating potential slower convergence ini-
tially, but yielding better results as the number of rounds increases.
Additionally, the Weibo dataset is a publicly available dataset that
consists of artificially extracted data from diffusion cascades, while
the Twitter dataset, albeit sparser than Weibo, offers insights closer
to real-world IM scenarios.
In both datasets, Lognorm-LinUCB generally outperforms the
other baselines. While there are instances where GLM-GT-UCB
briefly outperforms Lognorm-LinUCB in the initial stages, the lat-
ter demonstrates stable performance with smaller error bars. This
underscores the robustness of the log-normal assumption on the
reward distribution. NeuralUCB and NeuralTS, as scalarizations of
the general neural bandit model, exhibit comparable performances
across both datasets. Notably, their effectiveness lags behind mod-
els tailored for multi-round diffusion campaigns when 𝐿is small.
However, when 𝐿increases, the models are empowered with more
data, showing marked performance improvements.
Hyperparameter Analysis
Number of clusters: We conduct experiments on the number of
clusters in the Twitter dataset with 𝐿=2, and the results on the
last round (final accumulated spread) are shown in Fig. 4 of the
appendix. We observe from Fig. 4 that the campaign performance
improves as the number of clusters increases from 2to150at the
beginning. However, beyond 200clusters, performance begins to
decline. This decline can be likely attributed to insufficient data
within each cluster for effective learning with the constraints of
a limited budget on the number of rounds. Additionally, compu-
tational costs escalate exponentially as the cluster size goes up.
Through the analysis, a cluster size of 20to50seems to strike the
right balance between performance and computational efficiency.
This observation not only validates the initial rationale for cluster-
ing users, but also underscores the significance of computational
efficiency in optimizing social campaigns under budget constraints.
Boosted Exploration Scores: Bandit algorithms aim to strike a delicate
balance between exploiting known information to maximize short-
term gains and exploring unknown options to improve long-term
performance. In this spirit, we also consider in the experiments
a variant of exploration, in which we boost the exploration score
of unchosen arms having zero reward outcomes, to increase the
likelihood of exploring alternative arms. We run experiments on
the Twitter dataset with 𝐿=2comparing the use of such artificially
boosted exploration against its absence. The results are presented
in Fig. 5 in the appendix, and confirm the effectiveness of this
approach; this further supports the importance of exploration to
uncover valuable insights and optimize online decision-making.
6 CONCLUSION
In summary, our IM-GNB framework seamlessly leverages the ex-
pressivity of GNBs to tackle the challenges of multi-round IM in
uncertain environments. Our novel approach tackles key issues in
learning from graph-structured data and makes sequential decisions
 
778Influence Maximization via Graph Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 2: Comparison of IM-GNB with baselines on the Twitter dataset.
Figure 3: Comparison of IM-GNB with baselines on the Weibo dataset.
in uncertain environments. By incorporating contextual bandits,
we obtain initial estimates of diffusion probabilities to construct
exploitation and exploration graphs. Subsequently, these estimates
are refined with GCNs, to enhance the influence spread. The frame-
work’s scalability, even without prior knowledge of the network
topology, makes it a valuable and versatile tool for optimizing dif-
fusion campaigns.Acknowledgements. This work is funded by the Singapore Ministry
of Education AcRF Tier 2 (A-8000423-00-00). This research is part
of the programme DesCartes and is supported by the National
Research Foundation, Prime Minister’s Office, Singapore under
its Campus for Research Excellence and Technological Enterprise
(CREATE) programme. The authors thank Fengzhuo Zhang and
Junwen Yang (both NUS) for valuable discussions.
 
779KDD ’24, August 25–29, 2024, Barcelona, Spain Yuting Feng, Vincent Y. F. Tan, & Bogdan Cautis
REFERENCES
[1]Naoki Abe, Alan W Biermann, and Philip M Long. 2003. Reinforcement learning
with immediate rewards and linear hypotheses. Algorithmica 37 (2003), 263–293.
[2]Yikun an, Yuchen Yan, Arindam Banerjee, and Jingrui He. 2022. EE-Net:
Exploitation-Exploration Neural Networks in Contextual Bandits. In Proceedings
of International Conference on Learning Representations.
[3]Yikun Ban and Jingrui He. 2021. Local clustering in contextual multi-armed
bandits. In Proceedings of the Web Conference 2021. 2335–2346.
[4]Yikun Ban, Jingrui He, and Curtiss B Cook. 2021. Multi-facet contextual bandits:
A neural network perspective. In Proceedings of the 27th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining. 35–45.
[5]David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet alloca-
tion. Journal of Machine Learning Research 3, Jan (2003), 993–1022.
[6]Christian Borgs, Michael Brautbar, Jennifer Chayes, and Brendan Lucier. 2014.
Maximizing social influence in nearly optimal time. In Proceedings of the Twenty-
Fifth Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM, 946–957.
[7]Sébastien Bubeck, Rémi Munos, Gilles Stoltz, and Csaba Szepesvári. 2011. X-
Armed Bandits. Journal of Machine Learning Research 12, 5 (2011).
[8]Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. 2011. Contextual bandits
with linear payoff functions. In Proc. of the Fourteenth International Conference
on Artificial Intelligence and Statistics. 208–214.
[9]Varsha Dani, Thomas P Hayes, and Sham M Kakade. 2008. Stochastic linear
optimization under bandit feedback. In Proceedings of the Conference on Learning
Theory. 355–366.
[10] Pedro M. Domingos and Matthew Richardson. 2001. Mining the network value
of customers. In Proceedings of the seventh ACM SIGKDD international conference
on Knowledge discovery and data mining, San Francisco, CA, USA, August 26-29,
2001. 57–66.
[11] Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha. 2013. Scalable
influence estimation in continuous-time diffusion networks. In Conference on
Neural Information Processing Systems. 3147–3155.
[12] Shanshan Feng, Gao Cong, Arijit Khan, Xiucheng Li, Yong Liu, and Yeow Meng
Chee. 2018. Inf2vec: Latent representation model for social influence embedding.
In2018 IEEE 34th International Conference on Data Engineering (ICDE). IEEE,
941–952.
[13] Manuel Gomez-Rodriguez and Bernhard Schölkopf. 2012. Influence Maximization
in Continuous Time Diffusion Networks. In Proceedings of the International
Conference on Machine Learning.
[14] Amit Goyal, Francesco Bonchi, and Laks VS Lakshmanan. 2010. Learning influ-
ence probabilities in social networks. In Proceedings of the Third ACM International
Conference on Web search and Data Mining. 241–250.
[15] Amit Goyal, Francesco Bonchi, and Laks VS Lakshmanan. 2011. A Data-Based
Approach to Social Influence Maximization. In Proc. VLDB Endow. 73–84.
[16] Keke Huang, Sibo Wang, Glenn S. Bevilacqua, Xiaokui Xiao, and Laks V. S.
Lakshmanan. 2017. Revisiting the Stop-and-Stare Algorithms for Influence Maxi-
mization. Proc. VLDB Endow. 10, 9 (2017), 913–924.
[17] Alexandra Iacob, Bogdan Cautis, and Silviu Maniu. 2022. Contextual bandits for
advertising campaigns: A diffusion-model independent approach. In Proceedings
of the 2022 SIAM International Conference on Data Mining (SDM). SIAM, 513–521.
[18] David Kempe, Jon Kleinberg, and Éva Tardos. 2003. Maximizing the spread of
influence through a social network. In Proceedings of the Ninth ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 137–146.
[19] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. International Conference on Learning Representations
(2016).
[20] Paul Lagrée, Olivier Cappé, Bogdan Cautis, and Silviu Maniu. 2018. Algorithms
for online influencer marketing. ACM Transactions on Knowledge Discovery from
Data (TKDD) 13, 1 (2018), 1–30.[21] Tor Lattimore and Csaba Szepesvári. 2020. Bandit Algorithms. Cambridge Uni-
versity Press.
[22] Yandi Li, Haobo Gao, Yunxuan Gao, Jianxiong Guo, and Weili Wu. 2023. A Survey
on Influence Maximization: From an ML-Based Combinatorial Optimization. ACM
Trans. Knowl. Discov. Data 17, 9, Article 133 (Jul 2023), 50 pages.
[23] Hung T Nguyen, My T Thai, and Thang N Dinh. 2016. Stop-and-stare: Optimal
sampling algorithms for viral marketing in billion-scale networks. In Proceedings
of the 2016 international conference on management of data. 695–710.
[24] George Panagopoulos, Fragkiskos D Malliaros, and Michalis Vazirgiannis. 2020.
Multi-task learning for influence estimation and maximization. IEEE Transactions
on Knowledge and Data Engineering 34, 9 (2020), 4398–4409.
[25] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proc. of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP). 1532–1543.
[26] Yunzhe Qi, Yikun Ban, and Jingrui He. 2023. Graph Neural Bandits. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
1920–1931.
[27] Carlos Riquelme, George Tucker, and Jasper Snoek. 2018. Deep Bayesian bandits
showdown. In International Conference on Learning Representations, Vol. 9.
[28] Paat Rusmevichientong and John N Tsitsiklis. 2010. Linearly parameterized
bandits. Mathematics of Operations Research 35, 2 (2010), 395–411.
[29] Lichao Sun, Weiran Huang, Philip S Yu, and Wei Chen. 2018. Multi-round
influence maximization. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 2249–2258.
[30] Youze Tang, Yanchen Shi, and Xiaokui Xiao. 2015. Influence Maximization
in Near-Linear Time: A Martingale Approach. In Proceedings of the 2015 ACM
SIGMOD International Conference on Management of Data. 1539–1554.
[31] Youze Tang, Xiaokui Xiao, and Yanchen Shi. 2014. Influence maximization: Near-
optimal time complexity meets practical efficiency. In Proceedings of the 2014
ACM SIGMOD International Conference on Management of Data. 75–86.
[32] Michal Valko, Nathaniel Korda, Rémi Munos, Ilias Flaounas, and Nelo Cristian-
ini. 2013. Finite-time analysis of kernelised contextual bandits. In Proc. of the
Uncertainty in Artificial Intelligence (UAI).
[33] Sharan Vaswani, Branislav Kveton, Zheng Wen, Mohammad Ghavamzadeh, Laks
V. S. Lakshmanan, and Mark Schmidt. 2017. Model-Independent Online Learn-
ing for Influence Maximization. In Proc. of the 34th International Conference on
Machine Learning. 3530–3539.
[34] Sharan Vaswani, Laks Lakshmanan, and Mark Schmidt. 2015. Influence maxi-
mization with bandits. arXiv preprint arXiv:1503.00024 (2015).
[35] Zheng Wen, Branislav Kveton, Michal Valko, and Sharan Vaswani. 2017. Online
influence maximization under independent cascade model with semi-bandit
feedback. Advances in Neural Information Processing Systems 30 (2017).
[36] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In Proc. of the
International Conference on Machine Learning. PMLR, 6861–6871.
[37] Qingyun Wu, Zhige Li, Huazheng Wang, Wei Chen, and Hongning Wang. 2019.
Factorization bandits for online influence maximization. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
636–646.
[38] Tom Zahavy and Shie Mannor. 2020. Deep neural linear bandits: Overcoming
catastrophic forgetting through likelihood matching. In International Conference
on Learning Representations.
[39] Jing Zhang, Biao Liu, Jie Tang, Ting Chen, and Juanzi Li. 2013. Social Influence
Locality for Modeling Retweeting Behaviors. In Proc. of the International Joint
Conference on Artificial Intelligence (IJCAI).
[40] Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. 2021. Neural
Thompson sampling. International Conference on Learning Representation (2021).
[41] Dongruo Zhou, Lihong Li, and Quanquan Gu. 2020. Neural contextual bandits
with UCB-based exploration. In Proc. of the International Conference on Machine
Learning. 11492–11502.
 
780Influence Maximization via Graph Neural Bandits KDD ’24, August 25–29, 2024, Barcelona, Spain
A SUPPLEMENTARY COMPLEXITY
EXPERIMENTS
We provide in Table 3 a comparison on running time (in hours) w.r.t.
the number of clustering groups 𝑚′when𝐿=2. We can observe
that a finer granularity (more groups) may not result in better per-
formance (as shown in Fig. 4), while cost goes up exponentially. We
chose 50 groups that represents a good tradeoff between accuracy
and complexity in our experiments.
Table 3: Results for running time.
𝑚′2
5 10 20 50 100 150 200 250
running
time 5.35 7.02 9.71 12.35 41.43 125.49 225.67 391.54 735.35
B ANALYSIS ON THE NUMBER OF
CLUSTERING GROUPS AND FOR
ARTIFICIAL EXPLORATION
Here, we present the remaining figures (Figs. 4 and 5) that were
mentioned in the main text but omitted due to space considerations.
They pertain to the analysis of the number of clustering groups
and for artificially boosted exploration.
Fig. 4 shows that the campaign performance improves as the
number of clusters increases from 2to150at the beginning. How-
ever, beyond 200clusters, performance begins to decline. This de-
cline can be likely attributed to insufficient data within each cluster
for effective learning under the constraints of a limited rounds
budget.
Fig. 5 confirms the effectiveness of artificially augmenting / boost-
ing the exploration score of the unchosen arms with zero reward
outcomes, in order to increase the likelihood of exploring alterna-
tive arms.
2 5 10 20 50 100 150 200 250
#clusters3000031000320003300034000cumulative spreadT witter - L=2 - round=500Figure 4: Analysis on the number of clustering groups.
0 100 200 300 400 500
rounds050001000015000200002500030000cumulative spreadT witter - L=2
explore
non-artifical
artificial
Figure 5: Analysis on artificially boosted exploration.
 
781