Investigating Out-of-Distribution Generalization of GNNs:
An Architecture Perspective
Kai Guo‚àó
School of Artificial Intelligence,
Jilin University
Changchun, Jilin, China
guokai20@mails.jlu.edu.cnHongzhi Wen
Department of Computer Science and
Engineering,
Michigan State University
East Lansing, Michigan, USA
wenhongz@msu.eduWei Jin
Department of Computer Science,
Emory University
Atlanta, Georgia, USA
wei.jin@emory.edu
Yaming Guo
School of Artificial Intelligence,
Jilin University
Changchun, Jilin, China
guoym21@mails.jlu.edu.cnJiliang Tang
Department of Computer Science and
Engineering,
Michigan State University
East Lansing, Michigan, USA
tangjili@msu.eduYi Chang‚Ä†
School of Artificial Intelligence,
Jilin University
Changchun, Jilin, China
yichang@jlu.edu.cn
Abstract
Graph neural networks (GNNs) have exhibited remarkable perfor-
mance under the assumption that test data comes from the same
distribution of training data. However, in real-world scenarios, this
assumption may not always be valid. Consequently, there is a grow-
ing focus on exploring the Out-of-Distribution (OOD) problem in
the context of graphs. Most existing efforts have primarily concen-
trated on improving graph OOD generalization from two model-
agnostic perspectives: data-driven methods and strategy-based
learning. However, there has been limited attention dedicated to
investigating the impact of well-known GNN model architec-
tures on graph OOD generalization, which is orthogonal to ex-
isting research. In this work, we provide the first comprehensive
investigation of OOD generalization on graphs from an architecture
perspective, by examining the common building blocks of modern
GNNs. Through extensive experiments, we reveal that both the
graph self-attention mechanism and the decoupled architecture
contribute positively to graph OOD generalization. In contrast, we
observe that the linear classification layer tends to compromise
graph OOD generalization capability. Furthermore, we provide
in-depth theoretical insights and discussions to underpin these dis-
coveries. These insights have empowered us to develop a novel
GNN backbone model, DGat, designed to harness the robust prop-
erties of both graph self-attention mechanism and the decoupled
architecture. Extensive experimental results demonstrate the effec-
tiveness of our model under graph OOD, exhibiting substantial and
‚àóWork done while author was visiting Michigan State University.
‚Ä†Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671792consistent enhancements across various training strategies. Our
codes are available at https://github.com/KaiGuo20/DGAT.
CCS Concepts
‚Ä¢Computing methodologies ‚ÜíNeural networks.
Keywords
out-of-distribution generalization, graph neural networks, self-
attention, decoupled architecture
ACM Reference Format:
Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, and Yi Chang.
2024. Investigating Out-of-Distribution Generalization of GNNs: An Ar-
chitecture Perspective. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671792
1 Introduction
Graph Neural Networks (GNNs) have emerged as a powerful tool
for representation learning on various graph-structured data, such
as social networks [ 20,49,50], citation networks [ 5,13,16,26,53],
biological networks [ 38,42], epidemiological networks [ 27] and
product graphs [ 6,8,40]. The representations learned by GNNs
can tremendously facilitate diverse downstream tasks, including
node classification [ 28,30,35], graph classification [ 18,33,41], and
link prediction [ 54,56]. In many of these tasks, it is conventionally
assumed that training and test datasets are drawn from an identical
distribution. Nonetheless, this assumption is often contravened in
practical scenarios [ 44,45]. For instance, for paper classification on
citation graphs, models may be trained on papers from a specific
timeframe but later be required to make predictions for more recent
publications [ 45]. Such discrepancies between training and test data
distributions outline the out-of-distribution (OOD) challenge.
Recent studies on GNNs have pointed out potential vulnerabili-
ties when these models face distributional shifts [ 3,22,23,44]. To
counteract this, existing techniques aim to enhance graph OOD
 
932
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, and Yi Chang
generalization for node classification tasks majorly from two per-
spectives: data-based and learning-strategy-based methods. Data-
based methods focus on manipulating the input graph data to boost
OOD generalization. Examples of such strategies include graph
data augmentation [ 11,51] and graph transformation [ 15]. On the
other hand, learning-strategy-based methods emphasize modifying
training approaches by introducing specialized optimization objec-
tives and constraints. Notable methods encompass graph invariant
learning [ 45,47] and regularization techniques [ 55]. When inte-
grated with existing GNN backbone models [ 10,16,37,43], these
methods can enhance their OOD generalization capabilities.
While these techniques have made strides in addressing graph
OOD challenges, they primarily focus on external techniques for
improving OOD generalization. Moreover, there is a phenomenon
where different GNN models exhibit varying performance in graph
OOD scenarios[ 15].Hence, there remains a significant gap
in our understanding of the inherent OOD generalization
capabilities of different GNN backbone architectures. This
lack of insight raises critical questions: Which GNN architecture
is best suited when dealing with OOD scenarios? Are some models
naturally more robust than others? There is a pressing need to delve
deeper into these architectures, comprehensively assess their innate
capabilities, and provide clearer guidelines for their deployment in
OOD situations.
Research Questions. To bridge the gap, this paper presents the
first systematic examination on OOD generalization abilities of
popular GNN architectures, specifically targeting the node classifi-
cation task. Our analysis is framed around three key questions:
Q1:How do distinct GNN architectures perform when exposed to
OOD data?
Q2:If there are performance differences as indicated in Q1, can we
identify specific designs within GNN architectures responsible
for these variations? What could be the underlying causes?
Q3:Informed by the insights from the previous questions, can we
develop new GNN designs that enhance OOD generalization?
Contributions. By addressing the above questions, our contribu-
tions are summarized as follows:
A1:We rigorously assess a set of common building modules of
GNNs, including attention mechanism, decoupled architecture
and linear classification layer. Our empirical investigation re-
veals that both the attention mechanism and the decoupled
architecture contribute positively to OOD generalization. Con-
versely, we observe that the linear classification layer tends to
impair the OOD generalization capability.
A2:For a deeper understanding, we delve into the reasons why
certain building modules enhance OOD generalization and pro-
vide corresponding analysis. We demonstrate that the graph
self-attention mechanism in graphs adhering to the informa-
tion bottleneck principle is beneficial for OOD generalization.
A3: Based on our findings, we introduce a novel GNN design, De-
coupled Graph Attention Network (DGat), which combines
the attention mechanism with the decoupled architecture, en-
abling dynamical adjustments of the propagation weights and
separating propagation from feature transformation.Our major contribution lies in the systematic investigation of
the impact of GNN architectural modules on OOD scenar-
ios. Remarkably, our study is orthogonal to existing research
efforts of model-agnostic solutions. Indeed, our findings can
complement existing external strategies. DGat achieves superior
performance against other GNN backbone models when trained
using various OOD algorithms.
2 Preliminaries
2.1 Graph OOD Generalization Problem
The aim of our study is to investigate the out-of-distribution (OOD)
generalization problem in graph domain from an underexplored
perspective, the GNN backbone models. As a preliminary, we first
introduce the graph OOD problem, and then discuss representative
backbones for graph OOD generalization.
The OOD problem originates in the distribution shifts between
training and test data. In a supervised learning setting, such distri-
bution shifts can be defined as two types: i.e., covariate shift and
concept shift [11].
In the realm of graph OOD, the input ùëãis specified as a graph
G=(V,E), withùëÅnodesùë£ùëñ‚ààV, edges ùë£ùëñ,ùë£ùëó‚ààE, an adjacency
matrix A‚ààRùëÅ√óùëÅ. Therefore, the covariate variable consists of an
input pair of(X,A), where X‚ààRùëÅ√óùëënow denotes the node feature
matrix. Consequently, graph OOD problems involve distribution
shifts with both XandA, which is more intricate than the general
OOD problem. Graph distribution shifts can also be defined as
two types: i.e., covariate shift and concept shift. Covariate shift
refers to the distribution shift in input variables between training
and test data. Formally, ùëÉtr(X,A)‚â†ùëÉte(X,A). On the other hand,
concept shift depicts the shift in conditional distribution ùëÉ(ùëå|X,A)
between training and test data, i.e., ùëÉtr(ùëå|X,A)‚â†ùëÉte(ùëå|X,A).
The aim of graph OOD generalization is to bolster model perfor-
mance in OOD scenarios. To solve an OOD problem, we deploy a
GNN backbone model, symbolized as a mapping function ùëìùúÉ(X,A)
with learnable parameter ùúÉ. The graph OOD generalization problem
can be articulated as:
min
ùúÉmax
ùëí‚ààEE(X,A,ùëå)‚àºùëù(X,A,ùëå|ùëí=ùëí)[L(ùëìùúÉ(X,A),ùëå)|ùëí] (1)
whereL(¬∑) represents a loss function, and ùëídenotes the environ-
ment. In the graph machine learning research community, much
attention has been dedicated [ 29,45] to refining loss function L,
optimizingùúÉ, or augmenting on XandA. However, our approach
diverges from these methods. Instead, we focus on examining the
impact of the design choices in the backbone model ùëì. To the best
of our knowledge, our study offers the first systematic analysis of
GNN backbone architectures‚Äô effects in OOD contexts.
2.2 Graph Neural Network Architectures
Next, we briefly compare the architectures of classic GNN models
that are investigated in our analysis in Section 3.
GCN [16]. The Graph Convolutional Network (GCN) stands
as the most representative model, and is chosen as the standard
baseline model in our study. A graph convolutional layer comprises
a pair of aggregation and transformation operators. At the ùëô-th
layer, the mathematical representation of the graph convolutional
 
933Investigating Out-of-Distribution Generalization of GNNs:
An Architecture Perspective KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
layer is as follows:
Z(ùëô)=ùúé(ÀÜAZ(ùëô‚àí1)W(ùëô)), (2)
where Z(ùëô)denotes the node representation of ùëô‚àíùë°‚Ñélayer. W(ùëô)
represents the linear transformation matrix. ùúé(¬∑)is the nonlinear
activation function. Note that ÀÜAis the normalized adjacency matrix,
calculated from ÀÜA=ÀúD‚àí1
2ÀúAÀúD‚àí1
2, where ÀúA=A+Iis the adjacency
matrix with added self-loops, ÀúDùëñùëñ=√ç
ùëóÀúAùëñùëóis the degree matrix.
When applying the GCN model, there are two prevalent imple-
mentations. The first [ 11] adds a linear prediction layer after the
final graph convolutional layer (noted as layer ùêø), projecting the
dimension of Z(ùêø)to that of the label ùëå. The second [ 16], noted as
GCN‚Äì, omits this prediction layer, adjusting dimensions directly
in the last convolutional layer by altering the dimension of W(ùêø).
While the distinction between these methods might be overlooked,
it can influence OOD performance. We have compared both imple-
mentations in Section 3, and our findings and analysis are detailed
in Section 3.4.
GAT [37]. In contrast to the fixed aggregation in GCN, Graph
Attention Networks (GAT) employ a self-attention mechanism to
assign distinct weights to neighboring nodes during aggregation.
The attention mechanism can be expressed as:
ùõºùëñùëó=exp
ùúé
aùëá
WZùëñ‚à•WZùëó
√ç
ùëò‚ààNùëñexp ùúé aùëá[WZùëñ‚à•WZùëò], (3)
where ais a learnable weight vector, Zùëñis the representation vector
of nodeùëñ, andNùëñis the neighborhood of node ùëñin the graph. In
light of its connection with GCN, GAT can be studied as an object
to investigate the attention mechanism under the controlled
variable of GCN. We conduct the experiments in Section 3 and our
findings are presented in Section 3.2.
SGC [43]. The Simplifying Graph Convolutional Network (SGC)
is a decoupled GNN model, where ‚Äúdecoupled" refers to a GNN
model that separates the neural network transformation operators
from the propagation (a.k.a, aggregation) operators [ 7]. Formally,
SGC can be defined as:
Z=ÀÜAùêæXW, (4)
where ÀÜAùêæcan be seen as a composition of ùêæpropagation layers, and
Wis a simple transformation layer. SGC can serve as the experimen-
tal group to study the influence of the decoupled architecture on
GCN performance, while a ùêælayer GCN acts as the control group.
The results of the control experiment are presented in Section 3.3.
APPNP [10]. APPNP is another decoupled model, which starts
with transformation and subsequently proceeds to propagation. It
can be expressed as:
Z(0)=H=ùëìùúÉ(X),Z(ùëò+1)=(1‚àíùõΩ)ÀÜAZ(ùëò)+ùõΩH,
Z(ùêæ)=softmax
(1‚àíùõΩ)ÀÜAZ(ùêæ‚àí1)+ùõΩH
,(5)
whereùëìùúÉrepresents a composition of multiple transformation layers
(i.e., an MLP), His the node representation of MLP, and ùõΩspeci-
fies the teleport probability of personalized PageRank propagation.
Only whenùõΩis set to 0, the propagation is equivalent to GCN and
SGC. Another noticeable distinction between APPNP and SGC is
the order of propagation and transformation layers. Therefore, inSection 3, we have two settings for APPNP, with and without con-
trollingùõΩ=0. These settings aim to distinguish between the effects
ofdecoupled architecture and teleport in propagation. Detailed
analyses are provided in Section 3.3.
3 Investigating OOD Generalization of GNN
Architectures
Graph out-of-distribution (OOD) generalization has primarily been
addressed through learning-strategy-based and data-based meth-
ods. While these model-agnostic approaches provide flexibility, the
potential influence of backbone GNN architectures on OOD general-
ization remains less explored. To delve deeper into this, we initiated
a systematic analysis of various GNN architectures in OOD sce-
narios. When designing the study, we note that GNN architectures
comprise multiple optional components, such as attention mech-
anisms and decoupled propagation. To evaluate their individual
impacts, we adopted the most classic GCN model as our baseline and
embarked on a series of ablation studies. Our ablation studies focus
on three primary modifications: (1) substituting graph convolution
with an attention mechanism, (2) decoupling feature transforma-
tion and feature propagation, and (3) removing the GCN‚Äôs linear
prediction layer. By controlling external variables, we were able to
distinguish the contributions of these individual factors. The follow-
ing sections provide detailed settings of our experiments and the
consequential findings regarding these architectural components.
3.1 Experimental Setup
Evaluation Metric. Our study aims to assess the impact of various
backbone architectures on graph OOD generalization. In previous
literature, researchers primarily use OOD test performance as their
main metric. Yet, models display performance variations both in
in-distribution (IID) and OOD settings. This suggests that OOD
test performance alone is insufficient for a holistic evaluation of
OOD generalization. To address this, we adopt the IID/OOD gen-
eralization gap metric from the NLP and CV domains [ 14,48] for
graph OOD analysis. The IID/OOD generalization gap depicts the
difference between IID and OOD performance, offering a measure
of models‚Äô sensitivity and robustness to distribution shifts. It is
defined as: GAP =IIDtest‚àíOOD test, where IIDtestandOOD test
are the test performance on IID and OOD test datasets, respectively.
To rigorously determine the component‚Äôs influence on graph
OOD generalization, we performed a paired T-Test between GCN
baseline and other models. Evaluating across 10 runs for each model
on every dataset, a p-value <0.05indicates a significant difference,
with the t-value highlighting the superior model.
Dataset. We utilized the GOOD benchmark [ 11] for evaluating
graph OOD generalization on node classification task. This bench-
mark offers a unique capability to simultaneously measure both IID
and OOD performance, which is a feature not present in prevalent
datasets from other studies, e.g., EERM [ 45]. Such simultaneous
measurement is vital as calculating the GAP depends on both perfor-
mance. The GOOD benchmark comprises citation networks, gamer
networks, university webpage networks, and synthetic datasets,
and delineates shifts as either covariate or concept shifts. From
this collection, we selected 11 datasets, excluding CBAS due to its
 
934KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, and Yi Chang
limited node size and feature dimension, and WebKB-university-
covariate as all models exhibited high variance.
Implementation of GNN models. First, we follow the imple-
mentation of GOOD benchmark and use GCN [ 16] as our baseline
model, which concludes with a linear prediction layer. In the GOOD
paper [ 11], models and hyperparameters are selected based on an
OOD validation set. In contrast, our study emphasizes the inherent
robustness of backbone models to unanticipated distribution shifts
common in real-world scenarios. Consequently, we determine opti-
mal hyperparameters using the IID validation set (Appendix A.4).
Next, to establish a comparative analysis framework, we imple-
ment other GNN models: GCN‚Äì, GAT [ 37], SGC [ 43], and APPNP [ 10],
as delineated in Section 2.2. Each implementation ensures consis-
tency by maintaining all factors constant, except for the specific
design under consideration. For instance, while deploying SGC, we
align its hidden dimensions and propagation layers with those of
the GCN baseline, even adjusting the transformation layer size to
make it slightly different from the original SGC (i.e., the linear trans-
formation in Eqn. 4 is replaced by an MLP). This adjustment isolates
the ‚Äúdecoupled architecture" as the sole variation between SGC and
GCN. By comparing these models with GCN, we discern how their
distinct architectures influence graph OOD generalization.
In the following, we examine the impact of common GNN build-
ing blocks, i.e., attention mechanism, coupled/decoupled ar-
chitecture, and linear prediction layer, respectively.
3.2 Impact of Attention Mechanism
In Table 1, we assess the impact of attention mechanism by compar-
ing OOD performance of GAT against GCN. GAT surpasses GCN
on 10 out of 11 datasets, and shows a lower GAP value on 9 out
of 11 datasets. Particularly, on the Arxiv-time-degree dataset, GAT
improves OOD performance by 5.1%relative to GCN and decreases
the GAP by 36.4%,underscoring the advantages of attention
mechanism for graph OOD generalization.
To statistically validate these observations, we further applied
T-Tests to the OOD results of both models on each dataset. In Ta-
ble 1, red numbers denote that GAT significantly outperforms GCN,
while blue signifies the opposite. The data reveals GAT‚Äôs signifi-
cant advantage on 7 datasets, further emphasizing the potency of
attention in graph OOD generalization. The detailed T-Test results
are reported in Figure 4.(a) in Appendix A.1.
Theoretical Insights. Next, we present a theoretical analysis that
delves into the success of GAT, elucidating why graph attention
yields advantages for OOD generalization. Our analysis comprises
two key components: (1) We establish a compelling link between
the graph attention mechanism and the fundamental concept of
information bottleneck; and (2) We discuss that optimizing the
information bottleneck can notably enhance OOD generalization
capabilities. At the start of our analysis, we introduce the concept
of the information bottleneck (IB).
We denote the variables of input node features as ùëã, and the
variables of the output representations as ùëç. Thus, the mapping
function of GNN ùëìùúÉ(¬∑)can be expressed as ùëì(ùëç|ùëã). Consider a
distribution ùëã‚àºGaussian(ùëã‚Ä≤,ùúñ)withùëãas the noisy input variable,
ùëã‚Ä≤as the clean target variable, and ùúñas the variance for the Gauss-
ian noise. Following Kirsch et al . [17] , the information bottleneckprinciple involves minimizing the mutual information between
the inputùëãand its latent representation ùëçwhile still accurately
predictingùëã‚Ä≤fromùëç, and can be formulated as:
ùëì‚àó
IB(ùëç|ùëã)=arg minùëì(ùëç|ùëã)ùêº(ùëã,ùëç)‚àíùêº ùëç,ùëã‚Ä≤(6)
whereùêº(¬∑,¬∑)stands for the mutual information. With the aforemen-
tioned notations and concepts, we now introduce our proposition.
Proposition 1. Given a node ùëñwith its feature vector ùë•ùëñand its
neighborhoodN(ùëñ), the following aggregation scheme for obtaining
its hidden representation zùëñ,
zùëñ=‚àëÔ∏Å
ùëó‚ààN(ùëñ)ùúÇùëñexp([Wùêæxùëñ]‚ä§WùëÑxùëó)√ç
ùëó‚ààN(ùëñ)exp([Wùêæxùëñ]‚ä§WùëÑxùëó)xùëó, (7)
withùúÇùëñ,WùëÑ,Wùêæbeing the learnable parameters, can be understood
as the iterative process to optimize the objective in Eq. (6).
The detailed proof of the proposition mentioned above is avail-
able in Appendix A.2. Proposition 1 unveils an intriguing relation-
ship between the aggregation scheme in Eq. (7)and the information
bottleneck principle in Eq. (6): it demonstrates that the information
bottleneck principle can be approached by adaptively aggregat-
ing similar node features into a learnable representation vector. It
is worth noting that the aggregation scheme employed in Graph
Attention Networks (GAT) (Eq. (3)) can be viewed as a specific
instance of Eq. (7)under certain conditions: (1) GAT sets ùúÇùëñto a
constant value; and (2) GAT computes attention using a weight
matrix multiplication on the concatenated node pair vector. Given
the insights provided by the proposition, it is reasonable to infer
that graph self-attention mechanism updates the node embeddings
following an IB principle.
Furthermore, we highlight that the information bottleneck prin-
ciple plays a pivotal role in enhancing the OOD generalization of
neural networks. Notably, Yang et al . [46] suggests that the infor-
mation bottleneck can help GNNs simultaneously discard spurious
features and learn invariant features, thereby achieving OOD gen-
eralization. Li et al . [21] and Ahuja et al . [1]have also demonstrated
the effectiveness of the information bottleneck for OOD generaliza-
tion. Consequently, we postulate that the reason the graph attention
mechanism contributes to the OOD generalization of GNNs is in-
tricately tied to its connection with the information bottleneck
principle.
3.3 Impact of Coupled/Decoupled Structure
In order to compare coupled and decoupled structures, we evaluate
the performance of various decoupled GNNs, presented in Table 2.
For both OOD tests and GAP values, SGC surpasses GCN on merely
5 out of 11 datasets. In contrast, when the order of propagation
and transformation is reversed, APPNP ( ùõΩ=0) exceeds GCN‚Äôs
performance on 9 out of 11 datasets and demonstrates a lower GAP
value on 9 out of 11 datasets. This reveals the significance of the
transformation-propagation order in OOD generalization, suggest-
ing a preference for transformation prior to propagation in
graph OOD contexts. The T-Test results are presented in the same
color scheme as in Section 3.2, which further confirms our initial
observation.
 
935Investigating Out-of-Distribution Generalization of GNNs:
An Architecture Perspective KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Table 1: OOD and GAP performances for investigating the impact of self-attention. All numerical results are averages across 10
random runs. Redcolor indicates the statistically significant improvement(i.e., ùëÉùë£ùëéùëôùë¢ùëí <0.05andùëáùë£ùëéùëôùë¢ùëí >0) over the GCN. Blue
color indicates the statistically significant worse(i.e., ùëÉùë£ùëéùëôùë¢ùëí <0.05andùëáùë£ùëéùëôùë¢ùëí <0) over the GCN. The best performance in each
dataset is highlighted in bold. OOD indicates the OOD performance on OOD test data.
G-Cora-
Word G-Cora-Degree G-Arxiv-Time G-Arxiv-Degree G-Twitch-Language G-WebKB-University
OOD‚ÜëGAP‚ÜìOOD‚ÜëGAP‚ÜìOOD‚ÜëGAP‚ÜìOOD‚ÜëGAP‚ÜìOOD‚Üë GAP‚Üì OOD‚Üë GAP‚Üì
Co
variateGCN 65.85 5.62 56.05
18.26 70.38
2.90 59.05
19.01 52.32
22.13 -
-
GA
T66.23 5.77 56.12 17.99 70.95 2.08 59.32 18.78 52.83 21.17 - -
ConceptGCN 65.44
3.05 62.48
7.05 62.50
13.42 60.13
16.36 45.12
39.25 26.42
39.41
GA
T65.86 2.21 63.85 4.83 64.96 10.89 63.07 11.99 44.14 40.81 29.27 35.23
Table 2: OOD and GAP performances for investigating the impact of decoupled architecture. All numerical results are averages
across 10 random runs. Redcolor indicates the statistically significant improvement(i.e., ùëÉùë£ùëéùëôùë¢ùëí <0.05andùëáùë£ùëéùëôùë¢ùëí >0) over the
GCN. Blue color indicates the statistically significant worse(i.e., ùëÉùë£ùëéùëôùë¢ùëí <0.05andùëáùë£ùëéùëôùë¢ùëí <0) over the GCN. The best performance
in each dataset is highlighted in bold.
G-Cora-
Word G-Cora-Degree G-Arxiv-Time G-Arxiv-Degree G-Twitch-Language G-WebKB-University
OOD‚ÜëGAP‚ÜìOOD‚ÜëGAP‚ÜìOOD‚ÜëGAP‚ÜìOOD‚ÜëGAP‚ÜìOOD‚Üë GAP‚Üì OOD‚Üë GAP‚Üì
co
variateGCN 65.85 5.62 56.05
18.26 70.38
2.90 59.05
19.01 52.32
22.13 -
-
SGC 66.19 5.61 55.43
18.46 70.54 2.27 59.66
18.10 54.03
20.40 -
-
APPNP(ùõΩ=0) 66.66 5.16 56.14 17.83 70.69 2.84 59.33 18.67 51.66 22.18 - -
APPNP 67.48 6.37 58.33 17.16 69.22 3.38 55.40 22.10 56.47 16.75 - -
conceptGCN
65.44 3.05 62.48
7.05 62.50
13.42 60.13
16.36 45.12
39.25 26.42
39.41
SGC 65.21 3.16 62.41
6.77 63.88
11.94 56.66
20.26 44.22
40.52 25.78
40.38
APPNP(ùõΩ=0) 66.21 2.81 64.09 4.85 65.10 10.94 65.44 9.86 44.10 39.87 29.17 35.83
APPNP 66.46 4.24 64.81 5.55 63.46 11.51 64.28 9.51 46.81 35.99 30.28 35.22
Figure 1: Comparision of OOD and GAP between GCN and GCN‚Äì for investagating the impact of linear classifier. GCN‚Äì means
GCN without linear classifier. D1, D2, D3, D4, D5, D6 represent G-Cora-Word, G-Cora-Degree, G-Arxiv-Time, G-Arxiv-Degree,
G-Twitch-Language and G-WebKB-University respectively.
Such observation can be potentially explained from the theory
that decoupled graph neural networks are equivalent to label prop-
agation, proposed by Dong et al . [7]. From a label propagation
perspective, the models propagate known labels across the graph
to generate pseudo-labels for unlabeled nodes. These pseudo-labels
then optimize the model predictor. The augmentation with pseudo-
labels may curb overfitting and the architecture‚Äôs training approach
can adaptively assign structure-aware weights to these pseudo-
labels [ 7]. This might account for the enhanced OOD generalization
performance seen in the decoupled architecture.
3.4 Impact of Linear Prediction Layer
Lastly, we evaluate the impact of the linear prediction layer on
a GCN. As shown in Figure 1, GCN‚Äì surpasses GCN on 8 out of
11 OOD datasets and achieves a lower GAP value on 8 out of 11
datasets. This indicates that removing the last linear predictionlayer can enhance graph OOD performance. The T-Test results
are reported in Figure 4.(b) in Appendix A.1.
The performance dip of the linear prediction layer might be at-
tributed to two factors. First, introducing an extra linear prediction
layer might lead to surplus parameters and higher model complex-
ity, amplifying the overfitting risk on IID. Second, the propagation
process is non-parametric and has a lower risk of overfitting. It
may be advantageous in both IID and OOD contexts. In contrast,
the additional linear prediction layer is fit to the training data‚Äôs
label distribution, potentially hindering performance when faced
with OOD distribution shifts. By omitting this layer, we amplify the
graph‚Äôs intrinsic structure, leading to improved OOD generalization.
Additionally, Tang and Liu [36] argue that the Lipschitz continuity
constant of APPNP can be smaller than that of GCN, suggesting
that APPNP may achieve lower generalization error than GCN.
 
936KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, and Yi Chang
This also provides a favorable explanation for the ability of decou-
pled architectures to enhance out-of-distribution generalization
capabilities.
4 New GNN Design for Enhanced OOD
Generalization
In the previous section, we provided both empirical results and
theoretical analysis that show the beneficial roles of the attention
mechanism and the decoupled architecture in enhancing the OOD
generalization of GNNs. On the other hand, we noted that the linear
prediction layer detracts from OOD generalization. Motivated by
these observations, we propose to merge the attention mech-
anism with the decoupled architecture, opting to omit the
linear prediction layer. Specifically, we calculate attention scores
from transformed features and employ these scores throughout each
propagation layer. In the following, we delve into the details of our
proposed model, Decoupled Graph Attention Network (DGat).
4.1 A New GNN Design
Our proposed DGat model integrates the principles of decoupled
architecture and the attention mechanism, both of which have
demonstrated positive contributions to OOD performance in previ-
ous sections. To adopt a decoupled architecture, we separate the
linear transformation and propagation operations and conduct the
linear transformation prior to propagation. Furthermore, we inject
the attention mechanism into the propagation operations. As a
result, our model includes three components: linear transforma-
tion, attention score computation, and adaptive propagation. The
framework is illustrated by Figure 2.
Linear Transformation. OurDGat model decouples the GNN
into transformation and propagation. This design can enhance
OOD performance as discussed in Section 3.3. Therefore, DGat
first applies two linear transformation layers to the input data.
Z(init)=ùúé(ùëæ(init)X+ùíÉ(init)) ‚àà RùëÅ√óùëë
Z(0)=ùëØ=ùëæ(0)Z(init)+ùíÉ(0)‚ààRùëÅ√óùëê(8)
whereùëëis the hidden dimension and ùëêindicates the number of
classes. The second linear layer maps features to the label space.
These pseudo-labels help mitigate overfitting and dynamically as-
sign structure-aware weights to them.
Attention Score Computation. Our experimental findings in
Section 3.2 highlight the advantages of the attention mechanism
for handling graph OOD, supported by the theoretical evidence
that the graph self-attention mechanism aligns with the informa-
tion bottleneck principle. To incorporate the attention mechanism
into our model, we calculate attention scores based on the node
representation Z(init), formulated as:
ùõºùëñùëó=exp
LeakyReLU
aùëáh
WZ(init)
ùëñ‚à•WZ(init)
ùëói
√ç
ùëò‚ààNùëñexp
LeakyReLU
aùëáh
WZ(init)
ùëñ‚à•WZ(init)
ùëòi,(9)
whereùõºùëñùëórepresents the attention score. We use ùë∑to denote the
attention matrix, where ùë∑ùëñùëó=ùõºùëñùëó. We compute ùõºùëñùëóusing Z(init)
instead of Z(0), because Z(0)usually has a low dimensionality
which can hamper the identification of important nodes.Adaptive Propagation. The last step in DGat is propagation.
Instead of a traditional fixed propagation, DGat achieves adaptive
propagation by combining attention score matrices and adjacency
matrices. We defineÀÜÀúA:=(1‚àíùõæ)ùë∑+ùõæÀÜùë®. The adaptive propagation
function is expressed as follows:
Z(ùëò+1)=(1‚àíùõΩ)ÀÜÀúAZ(ùëò)+ùõΩH,
Z(ùêæ)=softmax
(1‚àíùõΩ)ÀÜÀúAZ(ùêæ‚àí1)+ùõΩH
,(10)
whereùõΩis a hyperparameter to control the trade-off of the initial
connection.
Note that we do not employ a linear layer at the end of the model
architecture as it can negatively impact the OOD generalization
(Section 3.4). As a consequence, our model combines both the de-
coupled architecture and attention mechanism, creating an elegant
fusion of the strengths of the two preceding components.
Complexity analysis. In the following, we demonstrate that the
computational complexity of our model is comparable to that of
the GCN. We define the adjacency matrix as A‚ààRùëÅ√óùëÅ, the input
matrix as X‚ààRùëÅ√óùëë, and the transformation matrix as W‚ààRùëë√óùëë.
The operations within a GCN layer result in the following time
complexity: ùëÇ ùëÅùëë2+ùëÅ2ùëë. Considering that each multiplication
with Ais a sparse multiplication, the complexity can be rewritten
asùëÇ ùêø|ùê∏|ùëë2+ùêøùëÅ2ùëë, where|ùê∏|represents the number of edges
andùêørepresents the number of layers. Similarly, APPNP has the
same computational complexity as GCN.
For self-attention, we need to compute Q=XWùëû,K=XWùëò,
andV=XWùë£, each requiring O(ùëÅùëë2)time. Unlike in the GCN
case, we also need to compute QK‚ä§to obtain the attention score
ùú∂, which takes O(ùëÅ2ùëë)time. Finally, computing ùú∂Valso takes
O(ùëÅ2ùëë)time. These computations result in a total time complexity
ofO(ùëÅ2ùëë+ùëÅùëë2). All of these operations are computed at each layer,
leading to the final time complexity of O ùêøùëÅ2ùëë+ùêøùëÅùëë2. Therefore,
the computational complexity of our model is ùëÇ(ùêø|ùê∏|ùëë2+ùêøùëÅ2ùëë),
which is comparable to that of GCN.
Advantages. Despite its simplicity, our model stands out by offer-
ing several compelling advantages:
(a)Simple yet robust. DGat is grounded in the findings of our
prior experimental study and theoretical analysis outlined in Sec-
tion 3. It enjoys the strengths of essential components that positively
contribute to OOD generalization.
(b)Favorable computational efficiency. The efficiency analysis
above reveals that the computational complexity of DGat is com-
parable to that of traditional GNNs, yet it demonstrates superior
OOD generalization.
(c)Compatible with diverse training strategies. DGat is orthog-
onal to external OOD techniques and can achieve further OOD
generalization from OOD training strategies. In the following sec-
tions, we will empirically verify that this model can function as a
powerful backbone for various popular OOD algorithms.
4.2 Experiment
To assess the OOD generalization capabilities of the proposed DGat,
we conduct experiments under various training strategies on node
classification tasks. Through experiments, we aimed to answer
the following questions: Q1: Can DGat outperform existing GNN
 
937Investigating Out-of-Distribution Generalization of GNNs:
An Architecture Perspective KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
ùëßùëñùëñùëõùëñùë°
‚Ä¶ùë•ùëñAttention Score 
Computation: ùú∂ùíäùíã
ùú∂ùíäùíã ùú∂ùíäùíã ùú∂ùíäùíãùú∂ùíäùíã
ùëßùëñ0
Linear
TransformationAdaptive
PropagationAdaptive
PropagationAdaptive
Propagation‚Ä¶Linear
Transformation
Figure 2: An illustration of our proposed model DGat. In this decoupled architecture, we calculate attention scores from
transformed features and employ these scores throughout each propagation layer.
Table 3: OOD and GAP performances under ERM setting on datasets from GOOD. All results are averages over 10 random runs.
G-Cora-
Word G-Cora-Degree G-Arxiv-Time G-Arxiv-Degree G-Twitch-Language G-WebKB-University
OOD‚ÜëGAP‚ÜìOOD‚ÜëGAP‚ÜìOOD‚ÜëGAP‚ÜìOOD‚ÜëGAP‚ÜìOOD‚Üë GAP‚Üì OOD‚Üë GAP‚Üì
Co
variateGCN‚Äì 66.56 5.47 58.09
16.22 70.82
2.52 58.95
19.15 51.86
22.55 -
-
SGC 66.23
5.51 55.86
17.98 70.27
1.87 59.36
17.96 53.19
16.26 -
-
APPNP 67.62
6.28 58.87
16.66 69.2
2.43 56.25
20.74 56.85
16.67 -
-
GAT 65.84
5.62 58.25
15.66 70.65
2.38 58.45
19.78 53.3
20.63 - -
GraphSA
GE 65.59
7.67 56.54
19.24 69.36
3.00 57.59
19.67 55.88
17.75 -
-
GPRGNN 67.59
6.44 59.46
15.75 67.74
2.66 56.68
19.02 56.37
16.05 -
-
DGa
t 67.67 6.09 59.68 15.02 71.33 1.65 60.12 17.56 57.37 15.31 - -
ConceptGCN‚Äì 66.70
2.31 64.72
4.27 63.27
12.78 62.46
13.47 44.72
39.65 27.80
34.54
SGC 66.28 1.96 62.58
7.12 63.33
11.61 55.06
21.28 47.43
35.31 29.63
33.37
APPNP 67.31
4.12 66.3
4.83 63.64
10.76 63.92
8.83 48.1
34.08 26.88
44.45
GAT 66.32
2.05 64.41 4.10 65.02
10.86 64.75
9.00 43.95
40.89 28.35 32.65
GraphSA
GE 65.42
5.46 65.06
5.43 62.85
11.77 60.92
12.96 45.51
39.93 34.50 40.67
GPRGNN 66.95
3.59 65.97
4.96 61.89
10.96 63.05
8.06 49.07
33.08 27.06
40.27
DGa
t 67.50 3.57 66.36 4.45 65.11 10.17 65.86 7.73 45.10 40.20 33.57 37.92
Table 4: OOD performances under ERM and EERM on datasets from EERM paper. All results are averages over 5 random runs.
Dataset
Method GCN‚Äì SGC APPNP GAT GraphSAGE GPRGNN DGat
Amz-P
hoto ERM 93.79¬±0.97 93.83¬±2.30 94.44¬±0.29 96.30¬±0.79 95.09¬±0.60 91.87¬±0.65 96.56¬±0.85
Cora
ERM 91.59¬±1.44 92.17¬±2.38 95.16¬±1.06 94.81¬±1.28 99.67¬±0.14 93.00¬±2.17 99.68¬±0.06
Elliptic
ERM 50.90¬±1.51 49.19¬±1.89 62.17¬±1.78 65.36¬±2.70 56.12¬±4.47 64.59¬±3.52 73.09¬±2.14
OGB-
Arxiv ERM 38.59 ¬±1.35 41.44¬±1.49 44.84¬±1.43 40.63¬±1.57 39.56¬±1.66 44.38¬±0.59 45.95¬±0.65
T
witch-E ERM 59.89¬±0.50 59.61¬±0.68 61.05¬±0.89 58.53¬±1.00 62.06¬±0.09 59.72¬±0.40 62.14¬±0.23
Amz-P
hoto EERM 94.05¬±0.40 92.21¬±1.10 92.47¬±1.04 95.57¬±1.32 95.57¬±0.13 90.78¬±0.52 92.54¬±0.77
Cora
EERM 87.21¬±0.53 79.15¬±6.55 94.21¬±0.38 85.00¬±0.96 98.77¬±0.14 88.82¬±3.10 98.85¬±0.26
Elliptic
EERM 53.96¬±0.65 45.37¬±0.82 58.80¬±0.67 58.14¬±4.71 58.20¬±3.55 67.27¬±0.98 68.74¬±1.12
OGB-
Arxiv EERM OOM OOM OOM OOM OOM OOM OOM
T
witch-E EERM 59.85¬±0.85 54.48¬±3.07 62.28¬±0.14 59.84¬±0.71 62.11¬±0.12 61.57¬±0.12 62.52¬±0.09
architectures on OOD test data? Q2: Is DGat a better backbone
model in different OOD generalization methods?
4.2.1 OOD performance of DGat. To answer Q1, we evaluate the
performance of our proposed DGat on ERM setting.
Baselines. We evaluate the performance of our DGat by comparing
it with several state-of-the-art models, including GCN, APPNP, GAT,
SGC, GPRGNN [4], and GraphSAGE [12].
Datasets. In this experiment, we selected 11 datasets from the
GOOD benchmark, the same as introduced in Section 3. In addi-
tion, we conducted experiments on 5 new datasets that are usedin EERM [ 45] paper. These datasets exhibit diverse distribution
shifts: Cora and Amz-Photo involve synthetic spurious features;
Twitch-E exhibits cross-domain transfer with distinct domains for
each graph; Elliptic and OGB-Arxiv represent temporal evolution
datasets, showcasing dynamic changes and temporal distribution
shifts. The details of these datasets are shown in Appendix A.3
Implementation Details. In Section 3.4, we observed that us-
ing linear transformation as the final prediction layer degrades
the OOD performance. Consequently, we replace the linear pre-
diction layer with an individual graph convolutional layer for all
 
938KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, and Yi Chang
the models. Similar to our prior experiments, we train and select
model hyperparameters on IID distribution and test the model on
OOD distribution. The number of layers is chosen from {2,3}. The
hidden size is chosen from {100,200,300}. We tune the following
hyper-parameters: ùõæ‚àà{0,0.2,0.5},ùõΩ‚àà{0,0.1,0.2,0.5}. The details
of parameters are shown in Appendix A.4.
Experimental Results. The results on GOOD datasets are reported
in Table 3. From this table, we find that DGat outperforms base-
lines on 9/11 datasets for the OOD test. Meanwhile, DGat exhibits
a lower GAP value compared to baselines on 6 out of 11 datasets.
For example, DGat delivers an improvement of 1.7% over baselines
for OOD test while achieving a decline of 4.1% over baselines for
GAP on GOODArxiv-degree-concept, which indicates the effective-
ness of our model for Graph OOD generalization. The results on
datasets from EERM paper under ERM setting are reported in Ta-
ble 4. Remarkably, our proposed DGat outperforms baseline GNN
backbones on all of these datasets.
To further validate the contribution of each component in DGat,
we conduct additional ablation study. The results confirm the effec-
tiveness of our proposed method and are shown in Appendix A.5.
4.2.2 DGat Performance as a Backbone. In order to answer the
Q2, we conduct experiments to evaluate our model and baselines
across various strategies proposed for OOD. Specifically, we choose
GCN‚Äì and APPNP that perform well under ERM as the backbones.
Representative OOD algorithms such as IRM [ 2], VREx [ 19], Group-
DRO [ 29], Graph-Mixup [ 39] and EERM [ 45] are considered as
baseline methods. Among them, Graph-Mixup and EERM are graph-
specific methods. It‚Äôs worth noting that within the GOOD frame-
work, Graph-Mixup is equipped with GCN incorporating a linear
classifier, and Graph-Mixup is more suitable for this framework in
APPNP, as it is better suited for enhancement at the hidden dimen-
sion level rather than the class dimension. Hence, for Graph-Mixup,
we have added a linear classifier on top of the models.
The experimental results illustrating the performances related
to OOD and GAP across various training strategies (i.e, IRM, VRex,
GroupDRO, Graph-Mixup) on datasets from GOOD are shown in
Figure 3. The results of OOD performance under EERM setting on
datasets from EERM are reported in Table 4. We have the following
observations.
(a) First, compared to the baselines, our DGat model con-
sistently demonstrates better OOD generalization when com-
bined with external OOD algorithms. We find that DGat out-
performs baselines on 9/11 datasets under IRM training strategy.
For example, DGat delivers an improvement of 3.6% over baselines
for OOD test on G-Cora-Degree-Covariate under IRM. Meanwhile,
DGat demonstrates superior OOD performance in comparison to
baselines across 9 out of 11 datasets under Graph-Mixup training
setting. The results in Table 4 shows that our model also achieves
better performance on most datasets under EERM training setting.
(b) Second, OOD training algorithms do not always improve
the OOD performance of backbone models. For example, on G-
Cora-Word and G-Cora-Degree datasets, all backbone models suffer
from OOD performance degradation when trained with EERM
algorithm. This observation is consistent with the results in GOOD
paper [ 11] and highlights the limitation of existing OOD algorithms.(c) Meanwhile, we indeed notice a significant improvement
in certain cases. For example, on G-Twitch-language-concept,
DGat equipped with VREx achieves an improvement of 7.0% over
ERM. This confirms that combining with an effective training al-
gorithm can further enhance the OOD generalization of our DGat
backbone model.
5 Related work
5.1 OOD generalization
In the real world, when the distribution of training data differs
from that of testing data, denoted as ùëÉùë°ùëü(ùëã,ùëå)‚â†ùëÉùë°ùëí(ùëã,ùëå), this
distribution shift is referred to as an OOD problem. Common types
of distribution shifts including covariate shift, concept shift, and
prior shift [ 24]. To effectively achieve better OOD generalization,
several methods have been proposed [ 2,9,19,29,34]. For instance,
Invariant Risk Minimization (IRM) [ 2] is a representative method
designed to identify invariant features. This approach ensures that
the optimal classifier performs consistently across all environments.
GroupDRO [ 29] enhances the model‚Äôs out-of-distribution (OOD)
generalization capabilities by focusing on optimizing for the worst-
case scenario across a set of predefined groups and introducing a
strong regularization. VREx [ 19] considers there exists variation
among different training domains, and this variation can be extrap-
olated to test domains. Based on this assumption, the method aims
to make the risks across different training domains as consistent as
possible, reducing the model‚Äôs reliance on spurious features.
5.2 OOD generalization on graphs
In graph-structured data, the OOD problem exists as well, but the
research on graph OOD is currently in its early stages. The covari-
ate shift and concept shift also exit in the graph domain. Unlike
the general OOD problem, in graph-based OOD, shifts can occur
not only in features but may also occur implicitly in the graph
structure. Some efforts have been proposed to solve the graph OOD
problem in node classification tasks from two perspectives: data-
based methods and learning-strategy-based methods. Data-based
methods focus on manipulating the input graph data to boost OOD
generalization [ 15,24,39]. For example, GTrans [ 15] provides a
data-centric view to solve the graph OOD problem and propose to
transform the graph data at test time to enhance the ability of graph
OOD generalization. On the other hand, learning-strategy-based
methods emphasize modifying training approaches by introducing
specialized optimization objectives and constraints [ 25,45,47]. For
example, EERM [ 45] seeks to leverage the invariant associations
between features and labels across diverse distributions, thereby
achieving demonstrably satisfactory OOD generalization in the face
of distribution shifts. However, current methods predominantly em-
phasize external techniques to enhance OOD generalization, yet
they do not provide insights into the inherent performance of the
underlying backbone models themselves. Therefore, in this work,
we investigate the impact of the GNN architecture on graph OOD.
6 Conclusion
GNNs tend to yield suboptimal performance on out-of-distribution
(OOD) data. While prior efforts have predominantly focused on
enhancing graph OOD generalization through data-driven and
 
939Investigating Out-of-Distribution Generalization of GNNs:
An Architecture Perspective KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Figure 3: Comparision of OOD performance between DGat and APPNP equipped with various OOD algorithms.
strategy-based methods, relatively little attention has been devoted
to assessing the influence of GNN backbone architectures on OOD
generalization. To bridge this gap, we undertake the first com-
prehensive examination of the OOD generalization capabilities of
well-known GNN architectures. Our investigation unveils that both
the attention mechanism and the decoupled architecture positively
impact OOD generalization. Conversely, we observe that the lin-
ear classification layer tends to compromise OOD generalization
ability. To deepen our insights, we provide theoretical analysis
and discussions. Building upon our findings, we introduce a novel
GNN design, denoted as DGat, which combines the self-attention
mechanism and the decoupled architecture. Our comprehensive
experiments across a variety of training strategies show that the
GNN backbone architecture is indeed important, and that combin-
ing useful architectural components can lead to a superior GNN
backbone architecture for OOD generalization.
7 Acknowledgments
Kai Guo, Yaming Guo and Yi Chang are supported by the National
Key R&D Program of China under Grant No.2023YFF0905400 and
the National Natural Science Foundation of China (No.U2341229).
References
[1]Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet,
Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. 2021. Invariance principle
meets information bottleneck for out-of-distribution generalization. Advances in
Neural Information Processing Systems 34 (2021), 3438‚Äì3450.
[2]Martin Arjovsky, L√©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019.
Invariant risk minimization. arXiv preprint arXiv:1907.02893 (2019).
[3]Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui
Xie, Tongliang Liu, Bo Han, and James Cheng. 2022. Learning causally invariant
representations for out-of-distribution generalization on graphs. Advances in
Neural Information Processing Systems 35 (2022), 22131‚Äì22148.
[4]Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2021. Adaptive Universal
Generalized PageRank Graph Neural Network. In International Conference on
Learning Representations. https://openreview.net/forum?id=n6jl7fLxrP
[5]Mauro Conti, Roberto Di Pietro, Luigi V. Mancini, and Alessandro Mei. 2009. (old)
Distributed data source verification in wireless sensor networks. Inf. Fusion 10, 4
(2009), 342‚Äì353. https://doi.org/10.1016/j.inffus.2009.01.002
[6]Leonardo Cotta, Carlos HC Teixeira, Ananthram Swami, and Bruno Ribeiro. 2020.
Unsupervised joint k-node graph representations with compositional energy-
based models. Advances in Neural Information Processing Systems 33 (2020),
17536‚Äì17547.
[7]Hande Dong, Jiawei Chen, Fuli Feng, Xiangnan He, Shuxian Bi, Zhaolin Ding,
and Peng Cui. 2021. On the equivalence of decoupled graph convolution network
and label propagation. In Proceedings of the Web Conference 2021. 3651‚Äì3662.
[8]Wenqi Fan, Xiaorui Liu, Wei Jin, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2022.
Graph trend filtering networks for recommendation. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 112‚Äì121.
[9]Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, Fran√ßois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The journal of machine learning
research 17, 1 (2016), 2096‚Äì2030.[10] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan G√ºnnemann. 2018.
Predict then propagate: Graph neural networks meet personalized pagerank.
arXiv preprint arXiv:1810.05997 (2018).
[11] Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. 2022. Good: A graph out-of-
distribution benchmark. Advances in Neural Information Processing Systems 35
(2022), 2059‚Äì2073.
[12] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[13] Haoyu Han, Xiaorui Liu, Haitao Mao, MohamadAli Torkamani, Feng Shi, Victor
Lee, and Jiliang Tang. 2023. Alternately optimized graph neural networks. In
International Conference on Machine Learning. PMLR, 12411‚Äì12429.
[14] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan,
and Dawn Song. 2020. Pretrained Transformers Improve Out-of-Distribution
Robustness. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics. 2744‚Äì2751.
[15] Wei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, and Neil Shah. 2023.
Empowering Graph Representation Learning with Test-Time Graph Transfor-
mation. In The Eleventh International Conference on Learning Representations.
https://openreview.net/forum?id=Lnxl5pr018
[16] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[17] Andreas Kirsch, Clare Lyle, and Yarin Gal. 2020. Unpacking information bottle-
necks: Surrogate objectives for deep learning. (2020).
[18] Lecheng Kong, Yixin Chen, and Muhan Zhang. 2022. Geodesic graph neural net-
work for efficient graph representation learning. Advances in Neural Information
Processing Systems 35 (2022), 5896‚Äì5909.
[19] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. 2021. Out-of-
distribution generalization via risk extrapolation (rex). In International Conference
on Machine Learning. PMLR, 5815‚Äì5826.
[20] Chetan Kumar, Riazat Ryan, and Ming Shao. 2020. Adversary for social good:
Protecting familial privacy through joint adversarial attacks. In Proceedings of
the AAAI conference on artificial intelligence, Vol. 34. 11304‚Äì11311.
[21] Bo Li, Yifei Shen, Yezhen Wang, Wenzhen Zhu, Dongsheng Li, Kurt Keutzer, and
Han Zhao. 2022. Invariant information bottleneck for domain generalization. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 7399‚Äì7407.
[22] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2022. Ood-gnn: Out-of-
distribution generalized graph neural network. IEEE Transactions on Knowledge
and Data Engineering (2022).
[23] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2022. Out-of-distribution
generalization on graphs: A survey. arXiv preprint arXiv:2202.07987 (2022).
[24] Xiner Li, Shurui Gui, Youzhi Luo, and Shuiwang Ji. 2023. Graph Structure and
Feature Extrapolation for Out-of-Distribution Generalization. arXiv preprint
arXiv:2306.08076 (2023).
[25] Yang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, Tat-Seng Chua, and Qing He.
2023. FLOOD: A Flexible Invariant Learning Framework for Out-of-Distribution
Generalization on Graphs. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 1548‚Äì1558.
[26] Zhining Liu, Ruizhong Qiu, Zhichen Zeng, Hyunsik Yoo, David Zhou, Zhe Xu,
Yada Zhu, Kommy Weldemariam, Jingrui He, and Hanghang Tong. 2024. Class-
Imbalanced Graph Learning without Class Rebalancing. In Forty-first International
Conference on Machine Learning.
[27] Zewen Liu, Guancheng Wan, B Aditya Prakash, Max SY Lau, and Wei Jin. 2024.
A Review of Graph Neural Networks in Epidemic Modeling. Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)
(2024).
[28] Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and
Dongmei Zhang. 2021. Source free unsupervised graph domain adaptation. arXiv
preprint arXiv:2112.00955 (2021).
[29] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. 2020.
Distributionally robust neural networks for group shifts: On the importance of
regularization for worst-case generalization. ICLR (2020).
 
940KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, and Yi Chang
[30] Zixing Song, Yifei Zhang, and Irwin King. 2022. Towards an optimal asymmetric
graph structure for robust semi-supervised node classification. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
1656‚Äì1665.
[31] Susanne Still, William Bialek, and L√©on Bottou. 2003. Geometric clustering using
the information bottleneck method. Advances in neural information processing
systems 16 (2003).
[32] DJ Strouse and David J Schwab. 2019. The information bottleneck and geometric
clustering. Neural computation 31, 3 (2019), 596‚Äì612.
[33] Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, and Tat-Seng Chua.
2022. Causal attention for interpretable and generalizable graph classification.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 1696‚Äì1705.
[34] Baochen Sun and Kate Saenko. 2016. Deep coral: Correlation alignment for
deep domain adaptation. In Computer Vision‚ÄìECCV 2016 Workshops: Amsterdam,
The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14. Springer,
443‚Äì450.
[35] Zeyu Sun, Wenjie Zhang, Lili Mou, Qihao Zhu, Yingfei Xiong, and Lu Zhang. 2022.
Generalized equivariance and preferential labeling for gnn node classification. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 8395‚Äì8403.
[36] Huayi Tang and Yong Liu. 2023. Towards understanding generalization of graph
neural networks. In International Conference on Machine Learning. PMLR, 33674‚Äì
33719.
[37] Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[38] Hao Wang, Jiaxin Yang, and Jianrong Wang. 2021. Leverage large-scale biological
networks to decipher the genetic basis of human diseases using machine learning.
Artificial Neural Networks (2021), 229‚Äì248.
[39] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. 2021. Mixup
for node and graph classification. In Proceedings of the Web Conference 2021.
3663‚Äì3674.
[40] Yu Wang, Yuying Zhao, Yi Zhang, and Tyler Derr. 2023. Collaboration-Aware
Graph Convolutional Network for Recommender Systems. In Proceedings of the
ACM Web Conference 2023.
[41] Dana Warmsley, Alex Waagen, Jiejun Xu, Zhining Liu, and Hanghang Tong. 2022.
A survey of explainable graph neural networks for cyber malware analysis. In
2022 IEEE International Conference on Big Data (Big Data). IEEE, 2932‚Äì2939.
[42] Hongzhi Wen, Jiayuan Ding, Wei Jin, Yiqi Wang, Yuying Xie, and Jiliang Tang.
2022. Graph neural networks for multimodal single-cell data integration. In
Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data
mining. 4153‚Äì4163.
[43] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In International
conference on machine learning. PMLR, 6861‚Äì6871.
[44] Qitian Wu, Yiting Chen, Chenxiao Yang, and Junchi Yan. 2023. Energy-
based out-of-distribution detection for graph neural networks. arXiv preprint
arXiv:2302.02914 (2023).
[45] Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. 2022. Handling Distri-
bution Shifts on Graphs: An Invariance Perspective. In International Conference
on Learning Representations (ICLR).
[46] Ling Yang, Jiayi Zheng, Heyuan Wang, Zhongyi Liu, Zhilin Huang, Shenda Hong,
Wentao Zhang, and Bin Cui. 2023. Individual and Structural Graph Informa-
tion Bottlenecks for Out-of-Distribution Generalization. IEEE Transactions on
Knowledge and Data Engineering (2023).
[47] Junchi Yu, Jian Liang, and Ran He. 2023. Mind the Label Shift of Augmentation-
based Graph OOD Generalization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 11620‚Äì11630.
[48] Chongzhi Zhang, Mingyuan Zhang, Shanghang Zhang, Daisheng Jin, Qiang
Zhou, Zhongang Cai, Haiyu Zhao, Xianglong Liu, and Ziwei Liu. 2022. Delving
deep into the generalization of vision transformers under distribution shifts. In
Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition .
7277‚Äì7286.
[49] Yanfu Zhang, Hongchang Gao, Jian Pei, and Heng Huang. 2022. Robust Self-
Supervised Structural Graph Neural Network for Social Network Prediction. In
Proceedings of the ACM Web Conference 2022. 1352‚Äì1361.
[50] Yanfu Zhang, Shangqian Gao, Jian Pei, and Heng Huang. 2022. Improving social
network embedding via new second-order continuous graph neural networks. In
Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data
mining. 2515‚Äì2523.
[51] Tong Zhao, Wei Jin, Yozen Liu, Yingheng Wang, Gang Liu, Stephan G√ºnnemann,
Neil Shah, and Meng Jiang. 2022. Graph data augmentation for graph machine
learning: A survey. arXiv preprint arXiv:2202.08871 (2022).
[52] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Animashree Anandkumar,
Jiashi Feng, and Jose M Alvarez. 2022. Understanding the robustness in vision
transformers. In International Conference on Machine Learning. PMLR, 27378‚Äì
27394.[53] Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi,
and Xia Hu. 2021. Dirichlet energy constrained learning for deep graph neural
networks. Advances in Neural Information Processing Systems 34 (2021), 21834‚Äì
21846.
[54] Yangze Zhou, Gitta Kutyniok, and Bruno Ribeiro. 2022. OOD link prediction gen-
eralization capabilities of message-passing GNNs in larger test graphs. Advances
in Neural Information Processing Systems 35 (2022), 20257‚Äì20272.
[55] Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. 2021. Shift-robust
gnns: Overcoming the limitations of localized graph training data. Advances in
Neural Information Processing Systems 34 (2021), 27965‚Äì27977.
[56] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. 2021.
Neural bellman-ford networks: A general graph neural network framework for
link prediction. Advances in Neural Information Processing Systems 34 (2021),
29476‚Äì29490.
A Appendices
A.1 Results of T-Test
To statistically validate these observations, we further applied T-
Tests to the OOD results of both models on each dataset. The results
are shown in Figure 4 and Figure 5.
A.2 Proof of Proposition 1
Proposition 1. Given a node ùëñwith its feature vector ùë•ùëñand its
neighborhoodN(ùëñ), the following aggregation scheme for obtaining
its hidden representation zùëñ,
zùëñ=‚àëÔ∏Å
ùëó‚ààN(ùëñ)ùúÇùëñexp([Wùêæxùëñ]‚ä§WùëÑxùëó)√ç
ùëó‚ààN(ùëñ)exp([Wùêæxùëñ]‚ä§WùëÑxùëó)xùëó,
withùúÇùëñ,WùëÑ,Wùêæbeing the learnable parameters, can be understood
as an iterative process to optimize the objective in Eq. (6).
Proof. Given a distribution ùëã‚àºGaussian(ùëã‚Ä≤,ùúñ)withùëãas the
observed input variable and ùëã‚Ä≤as the clean target variable. Follow-
ing [ 17], the information bottleneck principle involves minimizing
the mutual information between the input ùëãand its latent rep-
resentation ùëçwhile still accurately predicting ùëã‚Ä≤fromùëç. In the
context of deep learning, the information bottleneck principle can
be formulated as the following optimization objective:
ùëì‚àó
IB(ùëç|ùëã)=arg min
ùëì(ùëç|ùëã)ùêº(ùëã,ùëç)‚àíùêº ùëç,ùëã‚Ä≤(11)
As demonstrated by Still et al . [31] , we can utilize the information
bottleneck to solve the clustering problems, in which nodes will
be clustered into clusters with indices ùëê. For simplicity, we take
the 1-hop graph of node ùë¢to illustrate where the node indices are
1,2,...,|N(ùë¢)|. Following Strouse and Schwab [32], we assume
thatùëù(ùëñ)=1
ùëõwithùëõ=|N(ùë¢)|andùëù(x|ùëñ)‚àùexp[‚àí1
2ùúñ2||x‚àíxùëñ||2]
with the introduction of a smoothing parameter ùúñ.
We denoteùëùùë°as the probability distribution after the ùë°-th itera-
tion, and the iterative equation is given by [52]:
ùëùùë°(ùëê|ùëñ)=logùëùùë°‚àí1(ùëê)
ùëç(ùëñ)exp[‚àíùê∑KL[ùëù(x|ùëñ)|ùëùùë°‚àí1(x|ùëê)]] (12)
ùëùùë°(ùëê)=ùëõ(ùëê)
ùë°
ùëõ(13)
ùëùùë°(x|ùëê)=1
ùëõ(ùëê)
ùë°‚àëÔ∏Å
ùëñ‚ààùëÜ(ùëê)
ùë°ùëù(x|ùëñ), (14)
 
941Investigating Out-of-Distribution Generalization of GNNs:
An Architecture Perspective KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
whereùëç(ùëñ)ensures normalization, ùëÜ(ùëê)
ùë°represents the set of node
indices in cluster ùëê, andùëõ(ùëê)
ùë°=|ùëÜ(ùëê)
ùë°|is the number of nodes as-
signed to cluster ùëê. Then, we can approximate ùëùùë°‚àí1(x|ùëê)using a
Gaussian distribution ùëûùë°‚àí1(x|ùëê)‚àºGaussian(ùúá(ùëê)
ùë°‚àí1,Œ£(ùëê)
ùë°‚àí1). When the
value ofùúñw.r.t.ùëù(x|ùëñ)is sufficiently small, we have:
ùê∑KL[ùëù(x|ùëñ)|ùëûùë°‚àí1(x|ùëê)]‚àù[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]‚ä§[Œ£(ùëê)
ùë°‚àí1]‚àí1[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]
+log det Œ£(ùëê)
ùë°‚àí1+ùêµ,(15)
whereùêµrepresents terms that are independent of the assignment of
data points to clusters and are consequently irrelevant to the objec-
tive. By substituting Eq. (15)back into Eq. (12), we can reformulate
the cluster update as follows:
ùëùùë°(ùëê|ùëñ)=logùëùùë°‚àí1(ùëê)
ùëç(ùëñ)exph
‚àí[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]‚ä§[Œ£(ùëê)
ùë°‚àí1]‚àí1[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]i
detŒ£(ùëê)
ùë°‚àí1
=logùëùùë°‚àí1(ùëê)
detŒ£(ùëê)
ùë°‚àí1exph
‚àí[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]‚ä§[Œ£(ùëê)
ùë°‚àí1]‚àí1[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]i
√ç
ùëêexph
‚àí[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]‚ä§[Œ£(ùëê)
ùë°‚àí1]‚àí1[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]i.
(16)
To minimize the KL-divergence between ùëùùë°‚àí1(x|ùëê)andùëûùë°‚àí1(x|ùëê),
ùúá(ùë°)
ùëêwill be updated as:
ùúá(ùë°)
ùëê=1
ùëõùëõ‚àëÔ∏Å
ùëñ=1ùëùùë°(ùëê|ùëñ)xùëñ
=1
ùëõùëõ‚àëÔ∏Å
ùëñ=1logùëùùë°‚àí1(ùëê)
detŒ£(ùëê)
ùë°‚àí1exph
‚àí[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]‚ä§[Œ£(ùëê)
ùë°‚àí1]‚àí1[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]i
√ç
ùëêexph
‚àí[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]‚ä§[Œ£(ùëê)
ùë°‚àí1]‚àí1[ùúá(ùëê)
ùë°‚àí1‚àíxùëñ]ixùëñ
=ùëõ‚àëÔ∏Å
ùëñ=1logùëùùë°‚àí1(ùëê)
ùëõdetŒ£ùë°‚àí1exph
2¬∑[ùúá(ùëê)
ùë°‚àí1]‚ä§Œ£‚àí1
ùë°‚àí1xùëñi
√ç
ùëêexph
2¬∑[ùúá(ùëê)
ùë°‚àí1]‚ä§Œ£‚àí1
ùë°‚àí1xùëñixùëñ,
(17)
where the last equation follows from the assumption that Œ£ùëê
ùë°‚àí1is
shared among all clusters and ùúáùëêare normalized w.r.t. Œ£‚àí1
ùë°‚àí1. Let
zùëê=ùúá(ùë°)
ùëê,ùúÇùëê=logùëùùë°‚àí1(ùëê)
ùëõdetŒ£ùë°‚àí1,2¬∑ùúá(ùëê)
ùë°‚àí1=Wùêæxùëê,WùëÑ=Œ£‚àí1
ùë°‚àí1and
rewrite the subscripts appropriately to obtain:
zùëñ=‚àëÔ∏Å
ùëó‚ààN(ùëñ)ùúÇùëñexp([Wùêæxùëñ]‚ä§WùëÑxùëó)√ç
ùëó‚ààN(ùëñ)exp([Wùêæxùëñ]‚ä§WùëÑxùëó)xùëó,
whereùúÇindicates attention correction weighting parameters, WùëÑ
andWùêæare transformation parameter about input features. ‚ñ°
This indicates that the graph self-attention mechanism follows
the information bottleneck principle. Specifically, ùúá(ùë°)
ùëêrefers to the
data distribution learned by the information bottleneck, and zùëêis
learned by the self-attention mechanism.
A.3 Dataset Details
We leverage the datasets from GOOD [ 11] and EERM datasets to
evaluate our model. The statistics of EERM datasets are summarized
in Table 5. These datasets exhibit various types of distribution
shifts: Cora and Amz-Photo undergo artificial transformation shifts,Table 5: Statistic information for datasets from EERM
Dataset #Nodes #Edges #Classes
Cora 2,703 5,278 10
Amz-Photo 7,650 119,081 10
Twitch-E 1,912‚àí9,498 31,299‚àí153,138 2
Elliptic 203,769 2,34,355 2
OGB-Arxiv 169,343 1,166,243 40
Twitch-E involves Cross-Domain transfers, and Elliptic and OGB-
Arxiv display temporal evolution.
A.4 Hyperparameter Selection
The fine-tuned parameters for each dataset in Section 3 are deter-
mined as follows:
‚Ä¢GOODCora-degree-covariate: lr=1e-3, dropout=0.5,
hidden=200, model_layer=2
‚Ä¢GOODCora-degree-concept: lr=1e-3, dropout=0.5,
hidden=200, model_layer=2
‚Ä¢GOODCora-word-covariate: lr=1e-3, dropout=0.5,
hidden=300, model_layer=2
‚Ä¢GOODCora-word-concept: lr=1e-3, dropout=0.5,
hidden=300, model_layer=1
‚Ä¢GOODArxiv-degree-covariate: lr=1e-3, dropout=0.2,
hidden=300, model_layer=3
‚Ä¢GOODArxiv-degree-concept: lr=1e-3, dropout=0.2,
hidden=300, model_layer=3
‚Ä¢GOODArxiv-time-covariate: lr=1e-3, dropout=0.2,
hidden=300, model_layer=3
‚Ä¢GOODArxiv-time-concept: lr=1e-3, dropout=0.2,
hidden=300, model_layer=3
‚Ä¢GOODTwitch-language-covariate: lr=1e-3, dropout=0.5,
hidden=200, model_layer=2
‚Ä¢GOODTwitch-language-concept: lr=1e-3, dropout=0.5,
hidden=300, model_layer=3
‚Ä¢GOODWebKB-university-concept: lr=5e-3, dropout=0.5,
hidden=300, model_layer=1
The other experiment involves comparing our proposed model,
DGat, with various baseline models. We fine-tune the parameters
within the following search space: layers (2, 3), dropout (0, 0.1,
0.2, 0.5), hidden (100, 200, 300), ùõæ(0, 0.2, 0.5), ùõΩ(0, 0.1, 0.2, 0.5),
learning rate (1e-3, 5e-2, 5e-3), heads (2, 4). Taking the GOODCora-
degree-Concept dataset as an example, the parameters used for
GCN are: layers: 2, learning rate: 5e-3, dropout: 0.2, hidden: 300;
the parameters used for GAT are: layers: 2, learning rate: 5e-3,
dropout: 0.2, hidden: 300, heads: 2; the parameters used for APPNP
are: layers: 2, learning rate: 5e-3, dropout: 0.2, hidden: 300, ùõΩ: 0.2;
the parameters used for DGat are: layers: 2, learning rate: 5e-3,
dropout: 0.2, hidden: 300, heads: 2, ùõæ: 0.5,ùõΩ: 0.2.
A.5 Ablation Study
We conducted the ablation study to analyze the impact of each
component in DGat. From Table 6, we find that each component
contributes positively to the model.
 
942KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, and Yi Chang
(a)
 (b)
Figure 4:ùëÉùë£ùëéùëôùë¢ùëí andùëáùë£ùëéùëôùë¢ùëí of GAT and GCN‚Äì
(a)
 (b)
Figure 5:ùëÉùë£ùëéùëôùë¢ùëí andùëáùë£ùëéùëôùë¢ùëí of SGC and APPNP( ùõΩ=0)
Table 6: Ablation study of DGat on representative datasets on ERM setting. All numerical results are averages across 10 random
runs.
GOOD-Cora-D-Covariate elliptic OGB-Arxiv
DGat 59.68¬±0.46 73 .09¬±2.14 45.51¬±0.67
DGat w/o self-attention 59.19¬±0.58 69 .58¬±2.08 45.29¬±0.83
DGat w/o decouple 58.25¬±0.58 70 .26¬±1.46 40.44¬±1.36
DGat w/o remove linear classifier 57.52¬±1.04 67 .94¬±2.63 44.79¬±0.63
 
943