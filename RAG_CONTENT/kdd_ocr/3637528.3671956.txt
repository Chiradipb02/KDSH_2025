Cross-Context Backdoor Attacks against Graph Prompt Learning
Xiaoting Lyu
School of Computer Science and
Technology
Beijing Jiaotong University
Beijing, China
xiaoting.lyu@bjtu.edu.cnYufei Han
Inria, Univ. Rennes, IRISA
Rennes, France
yufei.han@inria.frWei Wang∗
Beijing Jiaotong University
Xi’an Jiaotong University
Beijing, China
wangwei1@bjtu.edu.cn
Hangwei Qian
CFAR, A*STAR
Singapore
qian_hangwei@cfar.a-star.edu.sgIvor Tsang
CFAR, A*STAR
Singapore
ivor_tsang@cfar.a-star.edu.sgXiangliang Zhang
University of Notre Dame
Notre Dame, USA
xzhang33@nd.edu
ABSTRACT
Graph Prompt Learning (GPL) bridges significant disparities be-
tween pretraining and downstream applications to alleviate the
knowledge transfer bottleneck in real-world graph learning. While
GPL offers superior effectiveness in graph knowledge transfer and
computational efficiency, the security risks posed by backdoor poi-
soning effects embedded in pretrained models remain largely un-
explored. Our study provides a comprehensive analysis of GPL’s
vulnerability to backdoor attacks. We introduce CrossBA, the first
cross-context backdoor attack against GPL, which manipulates
only the pretraining phase without requiring knowledge of down-
stream applications. Our investigation reveals both theoretically
and empirically that tuning trigger graphs, combined with prompt
transformations, can seamlessly transfer the backdoor threat from
pretrained encoders to downstream applications. Through exten-
sive experiments involving 3 representative GPL methods across 5
distinct cross-context scenarios and 5 benchmark datasets of node
and graph classification tasks, we demonstrate that CrossBA con-
sistently achieves high attack success rates while preserving the
functionality of downstream applications over clean input. We also
explore potential countermeasures against CrossBA and conclude
that current defenses are insufficient to mitigate CrossBA. Our study
highlights the persistent backdoor threats to GPL systems, raising
trustworthiness concerns in the practices of GPL techniques.
CCS CONCEPTS
•Computing methodologies →Machine learning.
KEYWORDS
Backdoor Attacks; Graph Prompt Learning; Cross-context Learning
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671956ACM Reference Format:
Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, and Xian-
gliang Zhang. 2024. Cross-Context Backdoor Attacks against Graph Prompt
Learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671956
1 INTRODUCTION
Real-world graph learning tasks pose challenges in generalization
and knowledge transfer when deploying pretrained graph neural
networks (GNNs) to downstream applications divergent from the
pretraining stage. For instance, a GNN pretrained on social net-
works may be utilized in recommendation systems, while encoders
designed for link prediction might be repurposed for node or graph
classification tasks. The substantial differences between pretraining
and downstream applications, including variations in problem do-
mains, semantic space, and learning objectives [ 12,20,21], present
obstacles for transferring the graph knowledge in pretrained GNN
models to diverse downstream applications. In response, Graph
Prompt Learning (GPL) [ 12,20–22] has emerged as a promising so-
lution for such graph learning tasks requiring the generalization of
graph knowledge across various application contexts (abbreviated
as cross-context graph learning). Inspired by prompt learning in
Large Language Models (LLMs) [ 11,15], GPL involves training GNN
encoders initially on unannotated pretext graph data and then tailor-
ing prompts for downstream applications to guide these encoders.
This approach effectively bridges the gap between pretraining and
downstream tasks without altering the GNN’s parameters, thereby
avoiding resource-intensive data annotation and model retraining
while facilitating robust generalization of pretrained GNN encoders
across diverse downstream applications.
While GPL facilitates knowledge transfer across diverse graph
learning tasks in cross-context scenarios, it also exposes down-
stream applications to the risk of inheriting backdoors embedded in
pretrained models. Attackers can implant backdoors into pretrained
GNN encoders, leading to downstream models built on these en-
coders inheriting the backdoor poisoning effects and misclassifying
backdoored inputs to attacker-desired target labels. Such backdoor
vulnerabilities [ 2,3,6,14,29,35] have been identified in prompt
learning in Natural Language Processing (NLP), which involves us-
ing rare words as triggers and associating them with specific target
classes or output embeddings. However, adapting these NLP-based
 
2094
KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoting Lyu et al.
attacks to graph learning encounters challenges due to fundamental
differences in data structure and learning paradigms.
Prior research [ 17,25–27,34] has revealed vulnerabilities of
GNNs to backdoor attacks, manipulating labeled graph data during
supervised training to induce misclassification. However, these at-
tacks are not applicable to GPL scenarios. Unlike traditional GNN
tasks, GPL constructs GNN encoders using unlabeled data during
unsupervised pretraining, limiting attackers’ access to labeled train-
ing data for injecting backdoor noise. Recent work [ 31] targeting
graph contrastive learning (GCL) associates triggers with target
class embeddings to cause misclassification. However, this method
requires prior knowledge about downstream applications, render-
ing it infeasible in cross-context GPL scenarios where attackers
only control the pretraining process without information about
downstream applications. Additionally, existing research fails to
investigate the generalization and transferability of backdoor at-
tacks across different cross-context GPL scenarios. In summary,
organizing successful backdoor attacks against cross-context GPL
systems remains largely unexplored.
Presented Work. We study the feasibility of transferable back-
door attacks against various GPL methods across diverse cross-
context scenarios. Building on prior research [ 4,7,12,20–22,32],
we categorize cross-context learning into 5 scenarios: cross-task,
cross-domain, cross-dataset, cross-class, and cross-distribution, as
detailed in Section 3, based on the disparities between pretraining
and downstream graph data. These scenarios offer a comprehensive
assessment of the inherent backdoor threats to GPL.
Realizing such attacks in the aforementioned cross-context GPL
scenarios poses unique challenges compared to conventional graph
backdoor attacks. First, attackers in cross-context GPL scenarios
can only access and control the unlabeled graph data collected dur-
ing pretraining. They lack awareness of downstream applications
where the model may be deployed, rendering traditional backdoor
attacks unfeasible. Second, the divergences between pretraining and
downstream applications in semantic spaces, structural patterns,
and learning objectives challenge the Independent and Identically
Distributed (IID) assumption fundamental to machine learning mod-
els, hindering the generalization of the backdoor poisoning effects
embedded in the pretrained model. Third, successful backdoor at-
tacks should ensure that the backdoored model remains functional
on clean input in downstream applications. Additionally, the attack
should retain its effectiveness even when countermeasure mecha-
nisms are deployed by downstream users.
To this end, we propose CrossBA, the first cross-context backdoor
attack method against GPL, which addresses the aforementioned
challenges from the following perspectives.
First of all, CrossBA formulates the backdoor attack as a fea-
ture collision-oriented optimization problem during the pretraining
stage. CrossBA is designed to simultaneously associate backdoored
graphs with the embedding of the trigger graph and ensure that
these embeddings are distinct from those of clean graphs. In this
way, any backdoored graph will be mapped to the target embedding,
causing downstream applications to misclassify it as the target class
determined by the target embedding. Furthermore, our theoretical
analysis in Section 5.4 unveils the intrinsic vulnerability of GPL sys-
tems to backdoor poisoning effects embedded in pretrained GNN
models. While prompt learning facilitates knowledge transfer todownstream applications, it inadvertently amplifies the transfer-
ability of backdoors. Additionally, both our theoretical analysis
and empirical observations demonstrate that optimizing the trigger
graph to align the loss landscape of the backdoor and main learn-
ing task can further enhance the transferability of backdoors in
cross-context GPL scenarios. Finally, to enhance the stealthiness
of the attack, CrossBA optimizes the trigger graph to align the loss
landscape of the backdoor task with that of the main task, while
also reinforcing the closeness between the embeddings of the back-
door and backdoor-free GNN encoders on the same clean input
graphs, thereby minimizing utility loss in downstream applications.
Moreover, CrossBA constrains the node features of the trigger graph
to closely resemble those of clean nodes, aiding in evading potential
countermeasures deployed by downstream users.
We evaluate CrossBA against 3 representative GPL methods,
ProG [ 20], ProG-Meta [ 20] and GraphPrompt [ 12], using 5 real-
world graph datasets for both graph and node classification tasks.
Our evaluation covers 5 various cross-context graph learning sce-
narios, including cross-distribution, cross-class, cross-dataset, cross-
domain, and cross-task, as detailed in Section 3. Despite challenges,
CrossBA consistently achieves attack success rates exceeding 0.85
across various downstream applications in all 5 cross-context sce-
narios, while maintaining high utility with less than a 0.06 drop
in classification accuracy compared to backdoor-free counterparts.
Comparisons with GCBA [ 31], a relevant backdoor attack method
for GCL, reveal CrossBA’s superior performance across diverse
cross-context scenarios. For instance, against the GAT model trained
by ProG on CiteSeer, CrossBA outperforms GCBA by at least 68% in
both accuracy over clean data and attack success rate. Additionally,
we discuss potential countermeasures against CrossBA and con-
clude that the current defenses in GPL are insufficient to mitigate
CrossBA, yet effective against GCBA.
Our contributions focus on answering the 3 research questions:
RQ1: How does an attacker organize successful cross-context back-
door attacks against GPL systems?
RQ2: Do different GPL methods exhibit equal susceptibility to cross-
context backdoor attacks? Is there a shared underlying cause for
the vulnerability of various GPL methods to such attacks?
RQ3: Does the threat of backdoor attacks persist across different
cross-context GPL scenarios?
To address RQ1, we begin by providing an overview of relevant
literature and foundational concepts of the GPL framework in Sec-
tions 2 and 3. We then delve into the threat model of cross-context
backdoor attacks and the design of CrossBA in Sections 4 and 5.
ForRQ2, we theoretically analyze the feasibility of CrossBA within
the GPL framework in Section 5.4, revealing the inherent backdoor
vulnerability in GPL. To bolster the findings for RQ1 andRQ2, and
to address RQ3, we conduct a comprehensive empirical evaluation
in Section 6 to demonstrate the persistent threat posed by CrossBA
across diverse cross-context scenarios and against different GPL
methods. Section 7 concludes the entire paper.
2 RELATED WORKS
Graph Prompt Learning. Prompt learning, initially successful in
NLP [ 9], has been extended to graph data, tailoring prompts for
downstream tasks to guide the pretrained model to perform effec-
tively without altering its parameters. Prompts in graph learning
 
2095Cross-Context Backdoor Attacks against Graph Prompt Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
manifest in two forms: prompt as tokens and prompt as graphs
[8,10,12,13,19–21]. Two representative methods of these GPL
methods are GraphPrompt [ 12] and ProG [ 20]. Both unify pre-
training and downstream tasks into a common template but dif-
fer in prompts. ProG uses learnable, graph-structured variables as
prompts attached to the input graph, while GraphPrompt embeds
prompt tokens as learnable vectors into the hidden layers of the
GNN model, enhancing the Readout operation.
Backdoor Attacks. Backdoor attacks on GNNs have gained at-
tention recently [ 17,25–27,34]. In these attacks, the backdoored
GNN model predicts an attacker-chosen label for any testing input
embedded with triggers. Notably, [ 34] employs the Erdos-Rényi
(ER) model to generate subgraphs as triggers, [ 25] introduces an
adaptive trigger generator enhancing attack effectiveness, [ 26] de-
signs triggers that preserve the labels of backdoored samples, and
[27] proposes unnoticeable graph backdoor attacks to bypass de-
fense mechanisms. However, in cross-context GPL scenarios, tradi-
tional graph backdoor attacks are inapplicable, as the attacker only
controls the pretraining process with unlabeled data. GCBA [ 31],
targeting GCL, manipulates the victim GNN encoder to associate a
trigger with the target class’s embedding. Yet, GCBA necessitates
knowledge of the target class in downstream tasks, impractical in
cross-context GPL scenarios. Our study explores the feasibility of
backdoor attacks in cross-context GPL scenarios, where attackers
can only use unlabeled data to pretrain GNN encoders. Importantly,
attackers cannot access or interfere with downstream applications.
3 PRELIMINARIES
We focus on the workflow of cross-context graph prompt learning
[12,20]. Attackers pretrain the GNN encoder using self-supervised
learning on unlabeled graph data. Downstream users then learn the
prompts with few-shot training samples based on the pretrained
encoder. Relevant concepts and definitions are introduced below.
GNN encoder. GNNs have become a predominant approach for
learning graph embeddings. Typically, GNNs utilize a neighborhood
aggregation strategy, wherein the encoder iteratively updates a
node’s embedding by aggregating embeddings from its neighbors
through message passing. Formally, at the 𝑘-th layer, the embedding
of node𝑣𝑖is given by:
ℎ𝑘
𝑣𝑖=AGGREGATE
ℎ𝑘−1
𝑣𝑖,{𝑣𝑗∈N(𝑣𝑖)∪𝑣𝑖},𝜃(𝑘)
(1)
whereN(𝑣𝑖)is the set of first-order neighbors of node 𝑣𝑖in the
graph𝐺, and the AGGREGATE function combines neighborhood
node embeddings to update the node embedding. The final node
embedding of 𝑣𝑖is denoted as ℎ𝑣𝑖=𝐸𝜃(𝐺,𝑣𝑖), where𝜃denotes the
parameters of the encoder 𝐸. The graph embedding 𝐸𝜃(𝐺)is then
obtained through a Readout function that aggregates node embed-
dings from the entire graph. The objective functions of pretraining
include various self-supervised tasks such as GraphCL [ 30] and link
prediction [ 12], which enable the model to capture rich structural
and feature-based graph patterns. Our study focuses on inductive
learning, where the GNN encoder’s input is the inductive graph of
a given node, including the node itself and its k-hop neighbors.
Graph prompt learning. This study is involved in two sophisti-
cated GPL methods: ProG [ 20] and GraphPrompt [ 12]. Both ProG
and GraphPrompt unify pretraining and downstream tasks intograph-level tasks, introducing learnable prompts to guide these
tasks. In ProG, the prompt graph is denoted as 𝐺𝑝𝑟𝑜=(𝑃,𝑆), where
𝑃={𝑣𝑝
1,𝑣𝑝
2,...,𝑣𝑝
|𝑃|}represents the set of |𝑃|nodes, each character-
ized by a token vector p𝑖∈R1×𝑑. The set𝑆={(𝑣𝑝
𝑖,𝑣𝑝
𝑗)|𝑣𝑝
𝑖,𝑣𝑝
𝑗∈𝑃}
defines the prompt graph’s topology structure. A prompted graph is
obtained by inserting the prompt graph 𝐺𝑝𝑟𝑜into the input graph 𝐺.
The parameters for both the prompt graph and the answering func-
tion of downstream tasks are optimized through few-shot learning.
ProG-Meta enhances ProG by incorporating meta-learning tech-
niques. GraphPrompt introduces prompts within the hidden layers
of the GNN model to assist the graph pooling operation. Given
the node set 𝑉={𝑣1,𝑣2,...,𝑣𝑁}in𝐺with each node’s embedding
ℎ𝑣𝑖, and a learnable prompt vector pjfor the downstream task 𝑗,
the prompt-assisted readout operation for graph 𝐺is defined as
a reweighed readout function, Readout({pj⊙ℎ𝑣𝑖|𝑣𝑖∈𝑉}), where
⊙represents element-wise multiplication. The prompt vector pj
updates its parameters by gradient descent to minimize the graph
similarity loss.
Cross-context few-shot learning. Cross-context few-shot learning
is designed to facilitate rapid adaptation of models to new tasks in
diverse application contexts using limited labeled examples. The
inherent data heterogeneity in this paradigm results in disparities in
both semantic space and data distribution between pretraining and
downstream contexts. We summarize the cross-context scenarios
studied in previous works [ 4,7,12,20–22] and delineate five cross-
context scenarios to cover different levels of such disparities:
•Cross-task: This setting reflects the divergence in the goal of graph
learning tasks. The pretraining may be conducted for classifica-
tion of an entire graph, such as ENZYMES, while the downstream
task involves classification of nodes, like CiteSeer.
•Cross-domain: The pretraining and downstream data originate
from distinct domains but share the same task type. For instance,
pretraining could be conducted on commercial product networks
(e.g., Amazon), while downstream tasks involve academic citation
networks (e.g., Cora), both focusing on node classification tasks.
•Cross-dataset: This setting involves different datasets within the
same domain. For instance, the pretraining is on dataset like
Cora, while the downstream task involves a different dataset like
CiteSeer. Both are academic citation networks but different.
•Cross-class: Both the pretraining and downstream datasets stem
from the same data source, such as Cora. However, they focus
on different classes, i.e., pretraining and downstream tasks have
different class distributions. .
•Cross-distribution: In this scenario, both the pretraining and down-
stream datasets are sourced from the same data origin, sharing
identical features and label spaces. However, they present dis-
tinctly different data distributions.
4 THREAT MODELS
We consider a realistic backdoor attack against cross-context GPL
scenarios where a malicious service provider aims to compromise
the security of online pretrained GNN model repositories.
Attacker’s goal. Attackers aim to mislead downstream models built
upon the attacker-crafted GNN encoder to classify backdoored in-
puts as the target class. Simultaneously, the downstream model
 
2096KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoting Lyu et al.
GNNClean Representation
Trigger RepresentationBackdoor RepresentationDissimilar
Similarsim sim
Node Feature
Affinity+
Optimize the Features of Trigger nodes(a) Optimization for Trigger Graph
sim(b) Optimization for GNN Encoder
Backdoor
GNNClean
GNN
Clean RepresentationSimilar
Similar+Clean Representation
Optimize GNN modelBackdoor Representation
Trigger Representation
Figure 1: The attack workflow of CrossBA.
should behave normally for clean inputs. Specifically, in cross-
context scenarios, attackers build the backdoored GNN encoder at
the pretraining phase, which memorizes the association between
backdoored inputs and the attacker-desired output embedding. This
backdoored GNN encoder is then adapted for downstream appli-
cations using the GPL methods. To ensure the transferability of
backdoors in cross-context GPL, the attack should be organized
with the following properties: 1) Context-agnostic : The backdoor
attack should remain consistently effective across various down-
stream contexts. 2) Prompt-agnostic : The attack should be adaptable
to different designs of graph prompt learning methods used by
downstream users. 3) Stealthy : The backdoored GNN model should
contribute close classification accuracy to backdoor-free models
on clean data. Furthermore, the attack remains effective with the
defense mechanisms deployed by downstream users.
Attacker’s capability. We assume that the attacker possesses com-
plete control over the pretraining process. The attacker can inject
backdoor triggers into the unlabeled pretraining data and access
the training methods for the backdoored GNN encoder. However,
the attacker is incapable of accessing or manipulating the labeled
data and the GPL training process utilized by downstream users.
Attacker’s knowledge. The attacker has full knowledge of the
pretraining phase, including the dataset, the architecture of the
pretrained GNN encoder, and the training method. On the contrary,
the attacker lacks any information regarding the GPL training
process conducted by downstream users, including specifics about
the downstream datasets utilized and the GPL methods applied.
5 CROSS-CONTEXT BACKDOOR ATTACK
5.1 Overview of CrossBA
Figure 1 provides an overview of CrossBA. In our attack scenario,
attackers train the backdoored GNN encoder using unlabeled graph
data and self-supervised methods like graph contrastive learning.
CrossBA aims to inject embedding collisions between backdoored
graphs and the trigger graph into the backdoored GNN encoder,
while maintaining a distinct embedding for the backdoored graph
compared to its clean counterpart. This backdoor poisoning effect
causes the backdoored graph to be associated with a target em-
bedding distant from the clean input, leading to misclassification
in downstream models. Additionally, CrossBA also optimizes the
standard contrastive learning objective over clean graphs to ensure
robust classification performance.Formally, the trigger graph serves as the backdoor signal is Δ𝐺=
(𝑉Δ,𝐸Δ), with𝑉Δ={𝑣1
Δ,...,𝑣𝐶
Δ}representing the set of 𝐶trigger
nodes. Each trigger node has a feature vector x𝑖
Δ∈R1×𝑑, matching
the input graph’s node feature dimension. 𝐸Δ={(𝑣𝑖
Δ,𝑣𝑗
Δ)|𝑣𝑖
Δ,𝑣𝑗
Δ∈
𝑉Δ}defines the links of the trigger graph. To integrate the trigger
graph Δ𝐺into the input graph 𝐺for generating the backdoored
graph, the attacker randomly selects a node in 𝐺as the anchor node
𝑣𝑎𝑡𝑡, linking it to a specific node 𝑣𝑖
Δin the trigger graph. We limit the
trigger graph to a 3-node and fully connected graph, significantly
smaller than graphs in both pretraining and downstream datasets.
5.2 Attack Objective of CrossBA
We define the objective function of the CrossBA attack in Eq.2,
which jointly optimizes the backdoored GNN encoder parameters
𝜃𝑏and the node features of the trigger graph Δ𝐺.
𝜃∗
𝑏,Δ∗
𝐺=arg min
𝜃𝑏,Δ𝐺Lbdk+Lclr+𝛼L𝑐
sim+𝛽L𝑎
sim,
whereLbdk=−1
|𝐷|Í
𝐺𝑖∈𝐷sim
ˆ𝐸𝜃𝑏(𝐺𝑖⊕Δ𝐺),ˆ𝐸𝜃𝑏(Δ𝐺)
+𝜆1
|𝐷|Í
𝐺𝑖∈𝐷sim
ˆ𝐸𝜃𝑏(𝐺𝑖⊕Δ𝐺),ˆ𝐸𝜃𝑏(𝐺𝑖)
,
Lclr=−1
|𝐷|Í
𝐺𝑖∈𝐷logexp(sim(𝐻𝐺𝑖,𝐻𝐺+
𝑖)/𝜏)
exp(sim(𝐻𝐺𝑖,𝐻𝐺+
𝑖)/𝜏)+Í
𝑘≠𝑖exp(sim(𝐻𝐺𝑖,𝐻𝐺−
𝑘)/𝜏),
L𝑐
sim=−1
|𝐷|Í
𝐺𝑖∈𝐷sim
ˆ𝐸𝜃𝑏(𝐺𝑖),𝐸𝜃𝑐(𝐺𝑖)
,
L𝑎
sim=−1
|𝐷|Í
𝐺𝑖∈𝐷Í
𝑣𝑗
Δ∈𝑉Δsim
feat(𝑣𝑗
Δ),feat(𝑣𝑎𝑡𝑡)
.
(2)
where ˆ𝐸𝜃𝑏represents the backdoored GNN encoder, with 𝜃𝑏denot-
ing its parameters. 𝐺𝑖is a clean graph in the pretraining dataset
𝐷.𝐺+
𝑖is an augmented graph of 𝐺𝑖obtained by randomly adding
or removing links, while 𝐺−
𝑘is a graph different from 𝐺𝑖in𝐷. The
operation⊕attaches the trigger graph Δ𝐺into the anchor node 𝑣𝑎𝑡𝑡
of the clean graph 𝐺.feat(𝑣𝑖)denotes the feature vector of a node
𝑣𝑖.𝐸𝜃𝑐is the backdoor-free GNN encoder trained with clean graph
data.𝐻𝐺𝑖=ˆ𝐸𝜃𝑏(𝐺𝑖)denotes the embedding of the clean graph 𝐺𝑖.
sim(𝑢,𝑣)is the similarity between the embeddings 𝑢and𝑣.𝜏is
the temperature parameter. The parameters 𝜆,𝛼, and𝛽balance the
weight of the loss terms.
The main learning loss Lclrdefines a contrastive learning objec-
tive of the main task as in [ 30]. The goal is to train a GNN encoder
capable of producing distinctive embeddings for clean graph data.
Specifically, we maximize the similarity between the embeddings
 
2097Cross-Context Backdoor Attacks against Graph Prompt Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
of the graph 𝐺𝑖and its augmented counterpart 𝐺+
𝑖. Simultaneously,
we maximize the dissimilarity between the embedding of any other
graph𝐺−
𝑘and that of𝐺𝑖.
The backdoor learning loss Lbdk defines the backdoor task’s
objective. To address the attacker’s lack of knowledge about down-
stream datasets, CrossBA utilizes the embedding of the trigger graph
as the target, and induces the backdoor mapping into the GNN en-
coder by colliding the embeddings of backdoored graphs with that
of the trigger graph. By jointly tuning the backdoored GNN encoder
and the trigger graph, we ensure that the embeddings of backdoored
graphs resemble the target embedding while being different from
clean graphs’ embeddings. Consequently, when applied to down-
stream applications, backdoored graphs will be misclassified to the
target class associated with the target embedding.
Compared to manually specifying a static backdoor mapping
(with a fixed trigger graph and target embedding), our design offers
several advantages. First, by jointly tuning the target embedding
and trigger graph, we align the loss landscapes of the main task and
the backdoor task within the fixed GNN encoder. This minimizes
the backdoor learning loss while preserving the utility of the back-
doored GNN encoder on clean graph data. Second, fine-tuning the
trigger graph, as supported by our theoretical analysis in Section
5.4, reduces the disparity in backdoor task performance between
pretraining and downstream applications, thereby enhancing back-
door transferability. Lastly, we parameterize the tuning of the target
embedding by optimizing the trigger graph, ensuring it remains
within the embedding space of the graphs in 𝐷. Directly optimizing
the target embedding as an independent variable may lead to ex-
treme values outside the span of graphs in 𝐷, making them prone
to detection by downstream anomaly detection methods.
The embedding alignment loss L𝑐
simis designed to ensure that
backdoors do not impair the GNN encoder’s ability to generate
discriminative embeddings for clean graph data. The attacker can
build a clean GNN encoder 𝐸𝜃𝑐using clean pretraining data as a
reference model. By aligning the output embeddings of 𝐸𝜃𝑐with
those of ˆ𝐸𝜃𝑏on the same clean inputs, the backdoored GNN encoder
ˆ𝐸𝜃𝑏can perform similarly to the clean encoder 𝐸𝜃𝑐on clean data.
The node feature affinity loss L𝑎
simis designed to enhance the
stealthiness of CrossBA to evade anomaly detection-based sanitary
checks over node features. Since adjacent nodes in a graph typically
share similar features, downstream users can check the node feature
consistency to identify abnormal nodes with significantly deviated
feature values from their neighbors [ 5,31]. To circumvent such
defenses, we optimize the node features of the trigger graph by
minimizingL𝑎
sim, enforcing feature consistency between the nodes
in the trigger graph and the anchor nodes in the clean input graph.
5.3 Alternating Optimization for CrossBA
Algorithm 1 in Appendix A outlines the procedural flow of the
CrossBA attack. Initially, attackers employ self-supervised methods,
such as GraphCL [ 30], to train a clean GNN encoder 𝐸𝜃𝑐by mini-
mizingLclr. The trigger injection is then conducted by optimizing
the attack objective function in Eq. 2 in alternating order. During
each attack round, attackers first freeze the GNN encoder ˆ𝐸𝜃𝑏and
optimize the node features of the trigger graph Δ𝐺. Subsequently,attackers optimize the backdoored GNN encoder ˆ𝐸𝜃𝑏based on the
optimized trigger graph Δ∗
𝐺and the clean GNN encoder 𝐸𝜃𝑐.
Tuning trigger graph. After injecting the trigger graph Δ𝐺into
the input graph 𝐺, the attacker optimizes the node features of Δ𝐺,
potentially optimizing the target embedding, by minimizing the
backdoor learning loss Lbdkand the node feature affinity loss L𝑎
sim
with respect to the fixed GNN encoder ˆ𝐸𝜃𝑏. For simplicity, the
update of the trigger node features in one step is given by:
Δ𝑡
𝐺=Δ𝑡−1
𝐺−𝛾𝑡∇Δ𝑡−1
𝐺(Lbdk+𝛽L𝑎
sim) (3)
Tuning backdoored GNN encoder. Upon completing trigger opti-
mization, the attacker connects the optimized trigger graph Δ𝐺to
the anchor node in 𝐺, recreating backdoored graphs. The optimiza-
tion aims to maximize the similarity between the embeddings of
backdoored graphs and the trigger graph, as well as the similarity of
the clean graph’s embeddings between ˆ𝐸𝜃𝑏and𝐸𝜃𝑐. For simplicity,
the GNN encoder parameters are updated accordingly:
𝜃𝑡
𝑏=𝜃𝑡−1
𝑏−𝛾𝑔∇𝜃𝑡−1
𝑏(−1
|𝐷|Í
𝐺𝑖∈𝐷sim
ˆ𝐸𝜃𝑏(𝐺𝑖⊕Δ𝑡
𝐺),ˆ𝐸𝜃𝑏(Δ𝐺)
−𝛼1
|𝐷|Í
𝐺𝑖∈𝐷sim
ˆ𝐸𝜃𝑏(𝐺𝑖),𝐸𝜃𝑐(𝐺𝑖)
)(4)
5.4 Attack Feasibility of CrossBA
In this section, we explore the feasibility of the proposed CrossBA
attack against cross-context GPL. To simplify the analysis, we adopt
the prompt graph setting from [ 20], where the prompt graph 𝐺𝑝𝑟𝑜
is attached to the input graph 𝐺by the operator⊗. We define 𝐷𝑠
and𝐷𝑡as the distributions for the pretraining and downstream
graph datasets, respectively. Downstream users create the prompted
graph by𝐺𝑡⊗𝐺𝑝𝑟𝑜. The encoder outputs the embedding of the
prompted graph as ˆℎ(𝐺𝑡⊗𝐺𝑝𝑟𝑜)=ˆ𝐸𝜃𝑏(𝐺𝑡⊗𝐺𝑝𝑟𝑜).
Theorem 1. Assuming a 𝐿ˆ𝐸-Lipschitz continuous GNN encoder
ˆ𝐸𝜃𝑏with𝑘𝐺-node input graphs, and the node feature matrix 𝑋of
a graph𝐺=(𝑉,𝐸)has a bounded Frobenius norm, i.e., |𝑋|𝑓𝑟𝑜≤
𝜇. Upon freezing the GNN encoder ˆ𝐸𝜃𝑏, the backdoor learning loss
Lbdkin Eq.2 is upper-bounded by the main learning loss Lclrat the
pretraining stage, as given in Eq. 5:
L𝑠
bdk≤L𝑠
clr+2√︃
𝑑(Δ+
𝐺,𝐺𝑠⊕Δ𝐺)+𝐶
𝑑(Δ+
𝐺,𝐺𝑠⊕Δ𝐺)=2√𝑘𝐺(𝑘𝐺−1)𝐿ˆ𝐸𝜇−𝑛Í
𝑖=1𝑚Í
𝑗=1𝑠(ˆℎ(Δ+
𝐺,𝑖),ˆℎ(𝐺𝑠,𝑗⊕Δ𝐺))
𝑛𝑚
(5)
Similarly, the main learning loss in the downstream context can be
upper bounded by the main learning loss in the pretraining context:
L𝑡
clr≤L𝑠
clr+2√︁𝑑(𝐺𝑠,𝐺𝑡⊗𝐺𝑝𝑟𝑜)+𝐶0
𝑑(𝐺𝑠,𝐺𝑡⊗𝐺𝑝𝑟𝑜)=2√𝑘𝐺(𝑘𝐺−1)𝐿ˆ𝐸𝜇−𝑛Í
𝑖=1𝑚Í
𝑗=1𝑠(ˆℎ(𝐺𝑠,𝑖),ˆℎ(𝐺𝑡,𝑗⊗𝐺𝑝𝑟𝑜))
𝑛𝑚
(6)
Combing Eq.5 and Eq.6, the backdoor learning loss in the downstream
context can be upper bounded as in Eq.7:
L𝑡
bdk≤L𝑠
clr+(8√𝑘𝐺(𝑘𝐺−1)𝐿ˆ𝐸𝜇−4𝑛Í
𝑖=1𝑚Í
𝑗=1𝑠(ˆℎ(Δ+
𝐺,𝑖),ˆℎ(𝐺𝑠,𝑗⊕Δ𝐺))
𝑛𝑚)1/2
+(8√𝑘𝐺(𝑘𝐺−1)𝐿ˆ𝐸𝜇−4𝑛Í
𝑖=1𝑚Í
𝑗=1𝑠(ˆℎ(𝐺𝑠,𝑖⊕Δ𝐺),ˆℎ((𝐺𝑡,𝑗⊕Δ𝐺)⊗𝐺𝑝𝑟𝑜))
𝑛𝑚)1/2
+𝐶1
(7)
 
2098KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoting Lyu et al.
where Δ+
𝐺are augmented trigger graphs with randomly added or
removed links from Δ𝐺.𝐺𝑠and𝐺𝑡are the graphs sampled from the
pretraining and downstream data distribution 𝐷𝑠and𝐷𝑡, respectively.
The RKHS kernel function 𝑠(∗,∗)measures the similarity between two
graph embeddings. 𝐶,𝐶0, and𝐶1are constants for the GNN encoder’s
complexity and the optimal learning loss of an ideal encoder.
Observation 1. Transferability of graph prompt learning v.s.
transferability of backdoor attacks.
Proposition 1. Given a backdoored GNN encoder ˆ𝐸𝜃𝑏, there
exists a prompt graph 𝐺𝑝𝑟𝑜that maximizes the similarity values
𝑛Í
𝑖=1𝑚Í
𝑗=1𝑠(ˆℎ(𝐺𝑠,𝑖),ˆℎ(𝐺𝑡,𝑗⊗𝐺𝑝𝑟𝑜))
𝑛𝑚between the graphs from the pretrain-
ing dataset and the prompted graphs from the downstream dataset.
By combining Proposition 1, Eq.6, and Eq.7, we discover that em-
ploying prompts in the cross-context GPL presents both advantages
and disadvantages. On one hand, augmenting downstream graph
data from𝐷𝑡with the prompt 𝐺𝑝𝑟𝑜enhances the adaptability of
pretrained GNN encoders to downstream tasks by alleviating distri-
butional disparities between 𝐷𝑡and𝐷𝑠within the GNN encoder’s
embedding space. This results in a well-trained GNN encoder capa-
ble of generating discriminative embeddings for new tasks. How-
ever, on the other hand, as indicated by Eq.7, incorporating the
prompt graph into the backdoored graphs from 𝐷𝑡inadvertently
facilitates backdoor attack transferability by reducing the upper
bound of the backdoor learning loss in downstream tasks.
Observation 2. Tuning the trigger graph enhances the trans-
ferability of backdoor poisoning effects while concurrently
preserving the utility of backdoored GNN models.
Proposition 2. Given a backdoored GNN encoder ˆ𝐸𝜃𝑏, there
exists a trigger graph Δ𝐺that maximizes the similarity measure
1
𝑛𝑚𝑛Í
𝑖=1𝑚Í
𝑗=1𝑠(ˆℎ(Δ+
𝐺,𝑖),ˆℎ(𝐺𝑠,𝑗⊕Δ𝐺))between the augmented variants
of the trigger graph and the backdoored graphs in pretraining.
InCrossBA, we propose optimizing the trigger graph Δ𝐺with
a fixed GNN encoder during the pretraining stage to minimize
the backdoor learning loss. This process, as indicated by Eq.5 and
Proposition 2, improves the alignment between the embeddings of
Δ+
𝐺and𝐺𝑠⊕Δ𝐺, narrowing the disparity between the main learn-
ing loss and the backdoor learning loss on pretraining data. This
ensures that given a well-trained GNN encoder, the trigger tuning
module reduces the backdoor learning loss without compromising
the performance of the main task. Furthermore, as shown in Eq.7,
tuning the trigger graph during pretraining, along with the prompt
graph, further facilitates backdoor attack transferability by lower-
ing the upper bound of the backdoor learning loss in downstream
tasks, leading to misclassification with backdoored input.
The theoretical investigation elucidates the feasibility of deliver-
ing cross-context backdoor attacks following the design of CrossBA,
providing a response to RQ1. Moreover, Observation 1, which ad-
dresses RQ2, reveals the dual nature of knowledge transfer within
GPL’s prompt learning. While prompt learning enhances down-
stream models with the pretrained model’s expertise, it also poses
the risk of backdoor transfer. Our findings underscore substantial
concerns regarding the trustworthiness of GPL methods. Proofs ofTheorem 1 and Propositions 1 and 2 are provided in Appendix of
the arvix version of our paper1.
6 EXPERIMENTAL EVALUATION
6.1 Experimental Settings
Datasets and the Backbone GNN Models. We utilize 5 bench-
mark datasets for evaluation: CiteSeer [ 28], Cora [ 28], Amazon-
Computers [ 16], Amazon-Photo [ 16], and ENZYMES [ 1]. CiteSeer,
Cora, Amazon-Computers, and Amazon-Photo are utilized for node
classification tasks, while ENZYMES is employed for graph classifi-
cation tasks. Furthermore, following the setting in [ 20], we define
graph classification tasks using the node classification datasets. In
GPL systems, we employ two advanced GNN models: Graph Atten-
tion Network (GAT) [ 23] and Graph Transformer (GT) [ 18]. Details
about datasets and GNN models can be found in Appendix B.
GPL Methods. We evaluate the attack performance against main-
stream GPL methods applicable to both node and graph classifica-
tion tasks, categorized into two types [ 21]:Prompt as Graphs and
Prompt as Tokens. For the former branch, we choose ProG [ 20] and
ProG-Meta [ 20], formulating prompts as subgraph patterns. For the
latter, we target GraphPrompt [ 12], which considers prompts as
tokens added to the Readout module of GNNs.
Baseline Attacks. GCBA [ 31] emerges as the most relevant back-
door attack for our study, considering the threat model. While not
explicitly tailored for GPL, GCBA aims to inject backdoor poisoning
noise into a GNN encoder trained via GCL. In GCBA, the attacker
gathers graph data of the target class in downstream applications
and utilizes the GCBA-crafting method to inject the backdoor into
the GNN encoder. However, GCBA does not directly apply to cross-
context GPL scenarios as it necessitates access to downstream data.
We introduce two variants of GCBA adapted to our threat model:
GCBA_R and GCBA_M. In both variants, the attacker initially clus-
ters the embeddings of clean data collected during the pretrain-
ing stage, utilizing the backdoor-free GNN encoder. Subsequently,
GCBA_R randomly selects the embedding at the center of a cluster
as the target embedding, while GCBA_M chooses the cluster center
embedding farthest from other clusters as the target embedding.
Further details can be found in Appendix C.
Evaluation Metrics. We employ 3 metrics to evaluate attack ef-
fectiveness: (1) Attack Success Rate (ASR) [ 31], representing the
accuracy with which a backdoored downstream model classifies
backdoored inputs to the target class designated by the target em-
bedding, (2) Accuracy of the Main Task (ACC), measuring the clas-
sification accuracy of backdoored downstream models over clean
graph data, and (3) Accuracy Drop (AD) [ 31], denoting the differ-
ence in ACC between backdoor-free and backdoored downstream
models. A lower AD indicates less utility loss of backdoored GNN
encoders. A successful backdoor attack should ensure high ACC
(low AD) and high ASR values simultaneously.
Implementations. For the details about the implementations, in-
cluding the cross-context scenarios, the GPL methods, and all the
attacks, please refer to Appendix D. We provide codes in the link2.
1https://arxiv.org/abs/2405.17984
2https://github.com/xtLyu/CrossBA
 
2099Cross-Context Backdoor Attacks against Graph Prompt Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 1: ACC, ASR, and AD in cross-distribution scenarios.
No
de Classification Graph
Classification
GPL
Model AttackCiteSe
er Cora Computers P
hoto CiteSe
er-Graph P
hoto-Graph
A
CC(AD) ASR A
CC(AD) ASR A
CC(AD) ASR A
CC(AD) ASR A
CC(AD) ASR A
CC(AD) ASR
Pr
oGGATGCBA_R 0.24
(+0.60) 0.51 0.33(
+0.32) 0.76 0.43(
+0.29) 0.18 0.61(
+0.18) 0.15 0.17(
+0.54) 0.00 0.66(
+0.23) 0.24
GCBA_M 0.24
(+0.60) 0.62 0.18(
+0.47) 0.00 0.40(
+0.32) 0.44 0.65(
+0.14) 0.18 0.17(
+0.54) 0.00 0.69(
+0.20) 0.15
Cr
ossBA 0.83(
+0.01) 0.90 0.64 (
+0.01) 1.00 0.70(
+0.02) 1.00 0.79(
-0.00) 0.94 0.76(
-0.05) 1.00 0.89(
-0.00) 0.91
GTGCBA_R 0.35(
+0.47) 0.96 0.35(
+0.38) 0.69 0.47(
+0.27) 1.00 0.57(
+0.22) 0.85 0.37(
+0.42) 0.32 0.58(
+0.33) 0.89
GCBA_M 0.35(
+0.47) 0.96 0.45(
+0.28) 0.23 0.41(
+0.33) 0.75 0.60(
+0.19) 0.30 0.33(
+0.46) 0.21 0.57(
+0.34) 0.56
Cr
ossBA 0.82(
-0.00) 1.00 0.73(
-0.00) 0.99 0.72(
+0.02) 1.00 0.79(
-0.00) 1.00 0.80(
-0.01) 1.00 0.92(
-0.01) 0.99
Graph
Pr
omptGATGCBA_R 0.69(
+0.07) 0.05 0.47(
+0.11) 0.14 0.49(
+0.11) 0.14 0.59(
+0.07) 0.00 0.72(
-0.00) 0.04 0.58(
+0.20) 0.00
GCBA_M 0.78(
-0.02) 0.01 0.43(
+0.15) 0.07 0.43(
+0.17) 0.13 0.56(
+0.10) 0.15 0.67(
+0.05) 0.13 0.51(
+0.27) 0.28
Cr
ossBA 0.80(
-0.04) 1.00 0.67(
-0.09) 1.00 0.62(
-0.02) 1.00 0.68(
-0.02) 0.99 0.72(
-0.00) 0.99 0.74(
+0.04) 1.00
GTGCBA_R 0.69(
+0.09) 0.16 0.56(
+0.15) 0.19 0.50(
+0.15) 0.15 0.62(
+0.13) 0.13 0.66(
+0.09) 0.10 0.55(
+0.29) 0.25
GCBA_M 0.64(
+0.14) 0.04 0.53(
+0.18) 0.25 0.47(
+0.18) 0.10 0.59(
+0.16) 0.12 0.74(
+0.01) 0.11 0.57(
+0.27) 0.14
Cr
ossBA 0.79(
-0.01) 1.00 0.70(
+0.01) 0.99 0.67(
-0.02) 1.00 0.74(
+0.01) 1.00 0.77(
-0.02) 1.00 0.84(
-0.00) 1.00
Pr
oG
MetaGATGCBA_R 0.85(
-0.17) 0.56 0.50(
+0.38) 0.00 0.57(
+0.25) 0.00 0.75(
+0.24) 0.88 0.50(
+0.38) 0.00 0.86(
+0.14) 0.94
GCBA_M 0.50(
+0.18) 0.00 0.50(
+0.38) 0.00 0.57(
+0.25) 0.00 0.50(
+0.49) 0.00 0.50(
+0.38) 0.00 0.98(
+0.02) 0.00
Cr
ossBA 0.84(
-0.16) 1.00 0.89(
-0.01) 1.00 0.79(
+0.03) 1.00 0.99(
-0.00) 1.00 0.85(
+0.03) 1.00 1.00(
-0.00) 1.00
GTGCBA_R 0.93(
-0.02) 0.90 0.90(
+0.01) 0.31 0.57(
+0.29) 0.00 0.97(
+0.02) 0.71 0.92(
+0.03) 0.05 0.50(
+0.50) 0.00
GCBA_M 0.50(
+0.41) 0.00 0.88(
+0.03) 0.05 0.57(
+0.29) 0.00 0.99(
-0.00) 0.10 0.87(
+0.08) 1.00 1.00(
-0.00) 0.98
Cr
ossBA 0.93(
-0.02) 1.00 0.90(
+0.01) 1.00 0.85(
+0.01) 1.00 0.99(
-0.00) 1.00 0.94(
+0.01) 1.00 1.00(
-0.00) 1.00
Table 2: ACC, ASR, and AD in cross-class scenarios.
No
de Classification Graph
Classification
GPL
Model AttackCiteSe
er Cora Computers P
hoto CiteSe
er-Graph P
hoto-Graph
A
CC(AD) ASR A
CC(AD) ASR A
CC(AD) ASR A
CC(AD) ASR A
CC(AD) ASR A
CC(AD) ASR
Pr
oGGATGCBA_R 0.25(
+0.68) 0.00 0.47(
+0.31) 1.00 0.55(
+0.28) 1.00 0.73(
+0.06) 0.21 0.26(
+0.62) 0.00 0.80(
+0.12) 0.84
GCBA_M 0.25(
+0.68) 0.00 0.49(
+0.29) 1.00 0.56(
+0.27) 0.95 0.68(
+0.11) 0.36 0.47(
+0.41) 1.00 0.79(
+0.13) 1.00
Cr
ossBA 0.93(
-0.00) 1.00 0.78(
-0.00) 1.00 0.83(
-0.00) 1.00 0.79(
-0.00) 1.00 0.87(
+0.01) 1.00 0.93(
-0.01) 1.00
GTGCBA_R 0.49(
+0.44) 0.29 0.61(
+0.27) 0.99 0.60(
+0.22) 0.72 0.62(
+0.13) 1.00 0.49(
+0.39) 0.46 0.76(
+0.10) 0.01
GCBA_M 0.49(
+0.44) 0.34 0.52(
+0.36) 1.00 0.70(
+0.12) 1.00 0.69(
+0.06) 0.71 0.66(
+0.22) 0.26 0.80(
+0.06) 0.80
Cr
ossBA 0.93(
-0.00) 1.00 0.88(
-0.00) 1.00 0.82(
-0.00) 1.00 0.76(
-0.01) 1.00 0.88(
-0.00) 1.00 0.88(
-0.02) 1.00
Graph
Pr
omptGATGCBA_R 0.92(
-0.04) 0.05 0.86(
-0.01) 0.07 0.69(
+0.06) 0.10 0.74(
-0.04) 0.01 0.87(
-0.03) 0.06 0.61(
+0.17) 0.28
GCBA_M 0.89(
-0.01) 0.02 0.81(
+0.04) 0.03 0.70(
+0.05) 0.06 0.60(
+0.10) 0.00 0.84(
-0.00) 0.06 0.67(
+0.11) 0.27
Cr
ossBA 0.92(
-0.04) 1.00 0.87(
-0.02) 1.00 0.77(
-0.02) 1.00 0.72(
-0.02) 0.94 0.83(
+0.01) 1.00 0.79(
-0.01) 1.00
GTGCBA_R 0.87(
+0.06) 0.03 0.76(
+0.11) 0.03 0.72(
+0.08) 0.08 0.66(
+0.12) 0.03 0.88(
-0.01) 0.05 0.59(
+0.21) 0.24
GCBA_M 0.88(
+0.05) 0.05 0.83(
+0.04) 0.00 0.70(
+0.10) 0.05 0.66(
+0.12) 0.15 0.87(
-0.00) 0.04 0.66(
+0.14) 0.15
Cr
ossBA 0.93(
-0.00) 1.00 0.88(
-0.01) 1.00 0.80(
-0.00) 1.00 0.79(
-0.01) 1.00 0.89(
-0.02) 1.00 0.87(
-0.07) 1.00
Pr
oG
MetaGATGCBA_R 0.50(
+0.30) 0.00 0.50(
+0.14) 0.00 0.57(
+0.24) 0.00 0.99(
-0.00) 1.00 0.50(
+0.42) 0.00 1.00(
-0.00) 1.00
GCBA_M 0.50(
+0.30) 0.00 0.50(
+0.14) 0.00 0.57(
+0.24) 0.00 0.76(
+0.23) 1.00 0.50(
+0.42) 0.00 0.88(
+0.12) 1.00
Cr
ossBA 0.75(
+0.05) 1.00 0.80(
-0.16) 1.00 0.80(
+0.01) 1.00 0.99(
-0.00) 1.00 0.94(
-0.02) 1.00 1.00(
-0.00) 1.00
GTGCBA_R 0.50(
+0.45) 0.00 0.52(
+0.35) 0.00 0.57(
-0.28) 0.00 0.99(
+0.01) 1.00 0.50(
+0.44) 0.00 0.87(
+0.13) 0.38
GCBA_M 0.77(
+0.18) 1.00 0.60(
+0.27) 1.00 0.57(
-0.28) 0.00 0.99(
+0.01) 1.00 0.58(
+0.36) 0.02 0.94(
+0.06) 1.00
Cr
ossBA 0.95(
-0.00) 1.00 0.84(
+0.03) 1.00 0.85(
-0.00) 1.00 1.00(
-0.00) 1.00 0.95(
-0.01) 1.00 1.00(
-0.00) 1.00
6.2 Experimental Results
(1) Attack Performance in Cross-Context GPL Scenarios. We
evaluate the feasibility of backdoor attacks on node and graph clas-
sification tasks across 5 cross-context scenarios using 3 mainstream
GPL methods, addressing 3 research questions. Tables 1–3 and 6–4
present the ACC and ASR of all attack methods in cross-distribution,
cross-class, cross-domain, cross-dataset, and cross-task scenarios,
respectively. Due to space limitations and consistent trends across
different scenarios, we report results for cross-distribution, cross-
class, and cross-domain scenarios here and defer those for cross-
dataset and cross-task scenarios to Appendix E. The highest ASRand ACC values, and the lowest AD values achieved among all the
attacks, including our CrossBA, are highlighted in bold.
Superior attack performance of CrossBA across different
cross-context scenarios (RQ1 and RQ3). The results reveal that
CrossBA consistently outperforms the baseline methods across 5
cross-context GPL scenarios, achieving the highest ASR values
while maintaining comparable ACC levels to those of backdoor-
free models. Specifically, CrossBA achieves ASR values exceeding
0.87 in all scenarios, with only a negligible difference in ACC com-
pared to the backdoor-free models, at most 0.06 lower. In contrast,
the baseline attacks fail to achieve comparable ASR to CrossBA
 
2100KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoting Lyu et al.
Table 3: ACC, ASR, and AD in cross-domain scenarios. Photo is used as the pretraining dataset.
Node Classification Graph Classification
GPL Model AttackCiteSeer Cora CiteSeer-Graph Cora-Graph
ACC(AD) ASR ACC(AD) ASR ACC(AD) ASR ACC(AD) ASR
ProGGATGCBA_R 0.24(+0.50) 1.00 0.18(+0.40) 1.00 0.31(+0.41) 0.80 0.14(+0.74) 0.00
GCBA_M 0.20(+0.54) 0.00 0.32(+0.26) 1.00 0.20(+0.52) 0.88 0.14(+0.74) 0.00
CrossBA 0.76(-0.02) 1.00 0.73(-0.15) 1.00 0.72(-0.00) 1.00 0.85(+0.03) 1.00
GTGCBA_R 0.40(+0.43) 1.00 0.34(+0.39) 0.00 0.33(+0.47) 1.00 0.14(+0.76) 0.00
GCBA_M 0.42(+0.41) 1.00 0.33(+0.40) 1.00 0.44(+0.36) 1.00 0.40(+0.50) 1.00
CrossBA 0.83(-0.00) 1.00 0.73(-0.00) 1.00 0.80(-0.00) 1.00 0.90(-0.00) 1.00
Graph
PromptGATGCBA_R 0.75(-0.03) 0.85 0.68(-0.00) 0.24 0.59(+0.15) 0.00 0.78(+0.05) 0.60
GCBA_M 0.77(-0.05) 0.71 0.58(+0.10) 0.00 0.63(+0.11) 0.03 0.64(+0.19) 0.23
CrossBA 0.76(-0.04) 1.00 0.70(-0.02) 1.00 0.74(-0.00) 1.00 0.81(+0.02) 1.00
GTGCBA_R 0.64(+0.15) 0.19 0.57(+0.14) 0.02 0.68(+0.08) 0.00 0.74(+0.10) 0.04
GCBA_M 0.70(+0.09) 0.00 0.53(+0.18) 0.00 0.58(+0.18) 0.00 0.76(+0.08) 0.00
CrossBA 0.79(-0.00) 0.99 0.72(-0.01) 0.99 0.77(-0.01) 1.00 0.82(+0.02) 1.00
ProG
MetaGATGCBA_R 0.50(+0.34) 0.89 0.50(+0.39) 0.03 0.50(+0.38) 0.00 0.50(+0.26) 0.00
GCBA_M 0.50(+0.34) 0.00 0.50(+0.39) 0.00 0.50(+0.38) 1.00 0.50(+0.26) 0.00
CrossBA 0.80(+0.04) 1.00 0.88(+0.01) 1.00 0.90(-0.02) 1.00 0.87(-0.11) 1.00
GTGCBA_R 0.91(+0.02) 1.00 0.83(+0.07) 0.52 0.84(+0.08) 1.00 0.96(+0.01) 1.00
GCBA_M 0.50(+0.43) 0.00 0.50(+0.40) 1.00 0.50(+0.42) 0.00 0.50(+0.47) 0.00
CrossBA 0.93(-0.00) 1.00 0.90(-0.00) 1.00 0.94(-0.02) 1.00 0.95(+0.02) 1.00
Figure 2: ASR of backdoor attacks against PruneG in 5 cross-context scenarios. "-N" denotes the node classification task, while
"-G" represents the graph classification task.
Figure 3: Ablation study of CrossBA against 2 GPL methods with Prune on CiteSeer across 5 cross-context scenarios. "Node"
represents the node classification task, and "Graph" denotes the graph classification task.
across various scenarios while maintaining ACC simultaneously.
For example, the 2 baseline methods never achieve ASR above 0.32
against the GT model trained by GraphPrompt in all 5 scenarios, and
their ASR values against GAT models trained by all 3 GPL methods
on CiteSeer are below 0.48 in cross-dataset scenarios. Additionally,
when the baselines achieve comparable ASR to CrossBA, they suffer
a significant drop in ACC. For instance, in cross-class scenarios
against ProG, the baseline attacks achieve an ASR of about 0.99 on
Cora, similar to CrossBA, but with an ACC at least 0.27 lower. Simi-
larly, in cross-distribution scenarios against the GT model trainedby ProG, the baseline attacks achieve an ASR of about 0.96, close
toCrossBA, but with ACC values almost half of CrossBA’s.
Summary. CrossBA effectively generalizes the backdoor poison-
ing effects to diverse cross-context scenarios, outperforming two
baseline methods, while maintaining the utility of GNN encoders for
downstream tasks. This observation confirms the theoretical prin-
ciples outlined in Section 5.4, highlighting the efficacy of CrossBA
in enhancing backdoor transferability in various cross-context sce-
narios while preserving model utility in downstream tasks.
 
2101Cross-Context Backdoor Attacks against Graph Prompt Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 4: ACC, ASR, and AD in cross-task scenarios. ENZYMES
is the pretraining dataset.
GPL
Model AttackCiteSe
er Cora
A
CC(AD) ASR A
CC(AD) ASR
Pr
oGGATGCBA_R 0.50(
+0.26) 0.23 0.24(
+0.42) 0.00
GCBA_M 0.61(
+0.15) 0.10 0.24(
+0.42) 0.00
Cr
ossBA 0.75(
+0.01) 1.00 0.72(
-0.06) 1.00
GTGCBA_R 0.55(
+0.15) 0.85 0.44(
+0.20) 0.80
GCBA_M 0.57(
+0.13) 0.77 0.44(
+0.20) 0.93
Cr
ossBA 0.82(
-0.12) 0.87 0.64(
-0.00) 1.00
Graph
Pr
omptGATGCBA_R 0.70(
+0.04) 0.03 0.26(
+0.38) 0.41
GCBA_M 0.71(
+0.03) 0.00 0.34(
+0.30) 0.13
Cr
ossBA 0.75(
-0.01) 1.00 0.65(
-0.01) 0.97
GTGCBA_R 0.69(
+0.06) 0.05 0.56(
-0.03) 0.24
GCBA_M 0.77(
-0.02) 0.04 0.50(
+0.03) 0.26
Cr
ossBA 0.75(
-0.00) 1.00 0.53(
-0.00) 1.00
Pr
oG
MetaGATGCBA_R 0.50(
+0.14) 0.00 0.50(
+0.38) 0.00
GCBA_M 0.76(
-0.12) 1.00 0.50(
+0.38) 0.00
Cr
ossBA 0.90(
-0.26) 1.00 0.88(
-0.00) 0.87
GTGCBA_R 0.91(
+0.03) 0.61 0.50(
+0.41) 0.00
GCBA_M 0.89(
+0.05) 0.44 0.51(
+0.40) 1.00
Cr
ossBA 0.93(
+0.01) 1.00 0.91(
-0.00) 1.00
Consistent attack performance of CrossBA against differ-
ent GPL methods (RQ2). CrossBA demonstrates robust attack
effectiveness across various GPL methods, achieving high ASR
while preserving the usability of backdoored models in downstream
tasks. Specifically, against all 3 GPL methods, CrossBA consistently
achieves ASR values above 0.90 in cross-class, cross-distribution,
cross-dataset, and cross-domain scenarios. In cross-task scenarios,
CrossBA achieves ASRs exceeding 0.87. In contrast, baseline attacks
fail to deliver consistent attack performance against different GPL
methods. For instance, in cross-class scenarios, both baseline meth-
ods show ASR values below 0.05 against GAT models trained by all
three GPL methods on CiteSeer, indicating a complete failure of the
attack. The baseline methods achieve ASR values over 0.99 against
ProG on Cora but only reach ASRs of 0.07 for GraphPrompt.
Summary. The results reveal a common vulnerability of CrossBA
to diverse GPL techniques. This aligns with the theoretical foun-
dations presented in Section 5.4, highlighting the intrinsic risks
within the GPL framework that the prompt learning module of GPL
facilitates the transfer of backdoors to downstream applications.
(2) Potential Countermeasures. We evaluate the resilience of
CrossBA against potential countermeasures of downstream users.
Given the absence of specific defenses tailored for cross-context
GPL, we adapt PruneG [24], originally designed to mitigate adver-
sarial attacks on GNNs. Other defense methods, such as RandSample
[34] and GNNGuard [33], are not suitable for cross-context GPL
scenarios. This is because both RandSample and GNNGuard en-
tail training GNN encoders, which is impractical in cross-context
GPL. Therefore, we exclusively employ PruneG in our evaluation.
PruneG is a preprocessing technique that eliminates edges between
nodes with dissimilar features, removing components with fewer
connected nodes. Figure 2 illustrates the effectiveness of different
attack methods against PruneG across 5 cross-context scenarios.
CrossBA consistently outperforms the baselines, achieving highASR values exceeding 0.70 against PruneG. In cross-domain sce-
narios, CrossBA achieves ASRs above 0.90, while the ASRs of the
baselines remain below 0.50.
(3) Ablation Study. We conduct ablation studies on CrossBA to eval-
uate the significance of its components. Three variants of CrossBA
are considered: (1) CrossBA without trigger optimization, using
a fixed trigger graph and target embedding; (2) CrossBA without
embedding alignment; (3) CrossBA without node feature affinity.
The attack performance of CrossBA and its variants against 2 GPL
methods with PruneG across 5 cross-context scenarios is presented
in Figure 3. The results show that removing any component signifi-
cantly reduces ASRs. The ASR value of CrossBA remains the highest
one compared to the variants. These findings affirm the necessity
of integrating trigger graph optimization, embedding alignment-
based regularization, and node feature affinity constraint together
to achieve successful cross-context backdoor attacks against GPL.
(4) Impact of Prompt Tokens. Figure 4 in Appendix F illustrates
how the number of prompt tokens affects the attack performance of
CrossBA against ProG across 5 cross-context scenarios. The results
unveil that the effectiveness of CrossBA remains robust, exhibiting
little impact from variations in the number of prompt tokens.
(5) Impact of Trigger Nodes. Figure 5 in Appendix G illustrates
the impact of the number of trigger nodes on CrossBA’s attack
performance against ProG and GraphPrompt across five differ-
ent scenarios. The results demonstrate that CrossBA consistently
achieves ASR values exceeding 0.80, regardless of the number of
trigger nodes, highlighting the attack’s robustness and efficiency.
7 CONCLUSION AND FUTURE WORK
In this study, we conduct theoretical and empirical investigations
to assess the feasibility of backdoor attacks in cross-context graph
prompt learning. Our findings reveal that optimizing trigger graphs,
coupled with prompt transformations, significantly enhances back-
door transferability. We introduce CrossBA, the first cross-context
backdoor attack tailored for GPL, and evaluate its performance
across five cross-context scenarios encompassing node and graph
classification tasks. Additionally, we explore potential defenses
against such attacks. Our results demonstrate that CrossBA seam-
lessly embeds backdoors into various downstream models across
diverse cross-context GPL scenarios without compromising main
task performance, even under defense deployment. In the future,
we aim to investigate the transferability of backdoors within textual
attribute graphs.
8 ACKNOWLEDGMENT
This research is supported by the National Key Research and Devel-
opment Program of China(No.2022YFB3102100), the Beijing Natural
Science Foundation(No.L221014), the Systematic Major Project of
China State Railway Group Corporation Limited(No.P2023W002),
the French National Research Agency with the reference ANR-23-
IAS4-0001 (CKRISP), and the National Research Foundation, Singa-
pore and Infocomm Media Development Authority under its Trust
Tech Funding Initiative. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the au-
thor(s) and do not reflect the views of National Research Foundation,
Singapore and Infocomm Media Development Authority.
 
2102KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoting Lyu et al.
REFERENCES
[1]Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan,
Alex J Smola, and Hans-Peter Kriegel. 2005. Protein function prediction via graph
kernels. Bioinformatics 21, suppl_1 (2005), i47–i56.
[2]Xiangrui Cai, haidong xu, Sihan Xu, Ying Zhang, and Xiaojie Yuan. 2022. Bad-
Prompt: Backdoor Attacks on Continuous Prompts. In NeurIPS 2022.
[3]Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei
Li, and Chun Fan. 2022. BadPre: Task-agnostic Backdoor Attacks to Pre-trained
NLP Foundation Models. In ICLR 2022. OpenReview.net.
[4]Mouxiang Chen, Zemin Liu, Chenghao Liu, Jundong Li, Qiheng Mao, and Jianling
Sun. 2023. ULTRA-DP: Unifying Graph Pre-training with Multi-task Graph Dual
Prompt. CoRR abs/2310.14845 (2023).
[5]Enyan Dai, Minhua Lin, Xiang Zhang, and Suhang Wang. 2023. Unnoticeable
backdoor attacks on graph neural networks. In WWW 2023.
[6]Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, and Gongshen Liu. 2023. UOR: Uni-
versal Backdoor Attacks on Pre-trained Language Models. CoRR abs/2305.09574
(2023).
[7]Taoran Fang, Yunchao Zhang, Yang Yang, and Chunping Wang. 2022. Prompt
Tuning for Graph Neural Networks. CoRR abs/2209.15240 (2022).
[8]Taoran Fang, Yunchao Mercer Zhang, Yang Yang, Chunping Wang, and Lei CHEN.
2023. Universal Prompt Tuning for Graph Neural Networks. In NeurIPS 2023.
[9]Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language
Models Better Few-shot Learners. In ACL 2021.
[10] Qingqing Ge, Zeyuan Zhao, Yiding Liu, Anfeng Cheng, Xiang Li, Shuaiqiang
Wang, and Dawei Yin. 2023. Enhancing Graph Neural Networks with Structure-
Based Prompt. CoRR abs/2310.17394 (2023). arXiv:2310.17394
[11] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
Graham Neubig. 2023. Pre-train, Prompt, and Predict: A Systematic Survey of
Prompting Methods in Natural Language Processing. ACM Comput. Surv. 55, 9
(2023), 195:1–195:35. https://doi.org/10.1145/3560815
[12] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. 2023. GraphPrompt:
Unifying Pre-Training and Downstream Tasks for Graph Neural Networks. In
WWW 2023.
[13] Yihong Ma, Ning Yan, Jiayu Li, Masood S. Mortazavi, and Nitesh V. Chawla. 2023.
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous
Graph Neural Networks. CoRR abs/2310.15318 (2023).
[14] Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, and Shiqing Ma. 2023. NOTABLE:
Transferable Backdoor Attacks Against Prompt-based NLP Models. In ACL 2023.
Association for Computational Linguistics, 15551–15565.
[15] Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. 2020.
Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real
Tasks. In AAAI 2020.
[16] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).
[17] Yu Sheng, Rong Chen, Guanyu Cai, and Li Kuang. 2021. Backdoor Attack of
Graph Neural Networks Based on Subgraph Trigger. In Collaborative Computing:
Networking, Applications and Worksharing - 17th EAI International Conference,
CollaborateCom 2021 (Lecture Notes of the Institute for Computer Sciences, Social
Informatics and Telecommunications Engineering, Vol. 407). Springer, 276–296.
[18] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu
Sun. 2020. Masked label prediction: Unified message passing model for semi-
supervised classification. arXiv preprint arXiv:2009.03509 (2020).
[19] Reza Shirkavand and Heng Huang. 2023. Deep Prompt Tuning for Graph Trans-
formers. CoRR abs/2309.10131 (2023). arXiv:2309.10131
[20] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One:
Multi-Task Prompting for Graph Neural Networks. In KDD 2023.
[21] Xiangguo Sun, Jiawen Zhang, Xixi Wu, Hong Cheng, Yun Xiong, and Jia Li. 2023.
Graph Prompt Learning: A Comprehensive Survey and Beyond. arXiv:2311.16534
(2023). arXiv:2311.16534
[22] Zhen Tan, Ruocheng Guo, Kaize Ding, and Huan Liu. 2023. Virtual Node Tuning
for Few-shot Node Classification. In KDD 2023. ACM, 2177–2188.
[23] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, Yoshua Bengio, et al .2017. Graph attention networks. stat1050, 20 (2017),
10–48550.
[24] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming
Zhu. 2019. Adversarial examples on graph data: Deep insights into attack and
defense. arXiv preprint arXiv:1903.01610 (2019).
[25] Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang. 2021. Graph Backdoor. In
USENIX Security 2021. USENIX Association, 1523–1540.
[26] Jing Xu and Stjepan Picek. 2022. Poster: Clean-label Backdoor Attack on Graph
Neural Networks. In CCS 2022.
[27] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong,
and Xue Lin. 2019. Topology Attack and Defense for Graph Neural Networks:
An Optimization Perspective. In IJCAI 2019. ijcai.org, 3961–3967.
[28] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016. Revisiting semi-
supervised learning with graph embeddings. In International conference on ma-
chine learning. PMLR, 40–48.[29] Hongwei Yao, Jian Lou, and Zhan Qin. 2023. PoisonPrompt: Backdoor Attack on
Prompt-based Large Language Models. arXiv:2310.12439 [cs.CL]
[30] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. In NeurIPS
2020.
[31] Hangfan Zhang, Jinghui Chen, Lu Lin, Jinyuan Jia, and Dinghao Wu. 2023. Graph
Contrastive Backdoor Attacks. In ICML 2023.
[32] Qiannan Zhang, Shichao Pei, Qiang Yang, Chuxu Zhang, Nitesh V. Chawla, and
Xiangliang Zhang. 2023. Cross-Domain Few-Shot Graph Classification with a
Reinforced Task Coordinator. In AAAI 2023.
[33] Xiang Zhang and Marinka Zitnik. 2020. GNNGUARD: defending graph neural
networks against adversarial attacks. In NeurIPS 2020.
[34] Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. 2021. Back-
door Attacks to Graph Neural Networks. In SACMAT 2021. ACM, 15–26.
[35] Shuai Zhao and Jinming Wen. 2023. Prompt as Triggers for Backdoor Attack:
Examining the Vulnerability in Language Models. https://synthical.com/article/
7b1c5bc5-4335-4a54-b52b-1289fae245d3. arXiv:2305.01219 [cs.AI]
A OPTIMIZATION ALGORITHM
The pseudocode for the proposed CrossBA attack method is shown
in Algorithm 1.
Algorithm 1 CrossBA
Input: The pretraining dataset 𝐷={𝐺1,𝐺2,···,𝐺𝑁}, the regularization
weights𝛼,𝛽, and𝜆, the learning rate of the trigger graph 𝛾𝑡, the learning
rate of the GNN encoder 𝛾𝑔, the number of attack rounds 𝑇.
Output: The optimized trigger graph Δ𝐺and the backdoor poisoned GNN
encoder model ˆ𝐸𝜃𝑏.
1:Initialize Δ0
𝐺and𝜃0𝑐.
2:Train the clean GNN encoder 𝐸𝜃𝑐based on𝐷by minimizingLclr.
3:𝜃0
𝑏←𝜃𝑐,𝑡←1.
4:while𝑡≤𝑇do
5: For each graph 𝐺in𝐷, connect the trigger graph Δ𝑡−1
𝐺to the anchor
node to form a backdoored graph 𝐺′.
6: Obtain Δ𝑡
𝐺using Eq.3.
7: Update the trigger graph embedded in the backdoored graphs with
the optimized trigger graph Δ𝑡
𝐺.
8: Obtain𝜃𝑡
𝑏using Eq.4.
9:end while
B DATASETS AND GNN MODELS
CiteSeer [ 28] comprises 3,312 scientific publications categorized
into 6 classes, connected by 4,732 citation links. Cora [ 28] consists
of 2,708 scientific publications, each assigned to one of 7 categories
within a citation network with 5,429 links. Amazon-Computers
and Amazon-Photo [ 16] are subgraphs of the Amazon co-purchase
graph, where nodes represent products, and edges indicate frequent
co-purchases. Amazon-Computers has 13,752 nodes across 10 cat-
egories and 491,722 edges, while Amazon-Photo comprises 7,650
nodes from 8 categories and 238,162 edges. The ENZYMES dataset
[1] contains 600 enzymes from the BRENDA enzyme database, clas-
sified into 6 EC enzyme categories. For graph classification datasets
derived from node classification datasets, we follow the methodol-
ogy proposed in [ 20], involving edge and subgraph sampling from
the original data. Detailed statistics are presented in Table 5, where
the last column indicates the type of downstream task for each
dataset: "N" for node classification and "G" for graph classification.
GAT [ 23] and GT [ 18] are two advanced GNN models. GAT
employs neighborhood aggregation for node embedding learning
and distinguishes itself by assigning varied weights to neighboring
 
2103Cross-Context Backdoor Attacks against Graph Prompt Learning KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 5: Statistics of datasets.
Datasets No
des Edges Featur
es Lab
els T
asks
CiteSe
er 3,327 9,104 3,703 6 N
Cora 2,708 5,429 1,433 7 N
Amazon-Computers 13,752 491,722 767 10 N
Amazon-P
hoto 7,650 491,722 767 8 N
ENZYMES 32.63(
Avg.) 62.14(
Avg.) 18 6 G
nodes, thereby modifying their influence in the aggregation process.
GT integrates the processing capabilities for graph-structured data
with the self-attention mechanisms of Transformer networks, effec-
tively capturing complex relationships and feature dependencies
between nodes in a graph.
C BASELINE ATTACKS
GCBA [ 31] is the most relevant backdoor attack method to our
study, given the threat model definition. Unlike CrossBA, which
targets the GPL framework, GCBA aims to inject backdoor poi-
soning noise into a GNN encoder trained using Graph Contrastive
Learning (GCL). The attacker in GCBA can collect the graph data of
the target class in the downstream applications. The attack is then
formulated to maximize the similarity between the embeddings of
backdoored graph data and those of clean graph data belonging to
the target class in the downstream task, as well as the similarity
between embeddings of clean data from both the backdoored and
clean GNN encoders. However, GCBA is not directly applicable to
cross-context GPL scenarios as it relies on access to downstream
application data. To facilitate a fair comparison, we introduce two
variants of GCBA, namely GCBA_R and GCBA_M. In both vari-
ants, the attacker first clusters the embeddings of clean graph data
collected during pretraining using the backdoor-free GNN encoder.
Then, GCBA_R randomly selects an embedding centered around
a cluster in the embedding space as the target embedding, while
GCBA_M selects the cluster center furthest away from other class
centers as the target embedding. Subsequently, both variants follow
the same workflow as GCBA to execute the attack.
D IMPLEMENTATION DETAILS
We implement CrossBA using PyTorch and execute it on an NVIDIA
3090 GPU. For both GAT and GT, we adopt a two-layer graph neural
network structure with a hidden dimension set to 100. Following
the methodology of ProG [ 20], we utilize Singular Value Decomposi-
tion (SVD) to reduce the initial feature dimension of the data to 100.
The number of prompt nodes used in ProG and ProG-Meta is set
to 15. The self-supervised learning method employed by ProG and
ProG-Meta is GraphCL [ 30]. Consistent with the original paper’s
settings, we employ the Adam optimizer for ProG and ProG-Meta.
On most evaluated datasets, the learning rate for GT is set to 0.001,
and for GAT, it is set to 0.0001. Similarly, following the settings of
GraphPrompt [ 12], we use the AdamW optimizer with a unified
learning rate of 0.01 for all test instances. The self-supervised learn-
ing method used by GraphPrompt is the link prediction method [ 12].
Additionally, for downstream tasks, we employ 200-shot learning
for ProG and ProG-Meta, and 100-shot learning for GraphPrompt.
In this study, we investigate five distinct cross-context scenarios.
To address cross-distribution, cross-dataset, and cross-domain sce-
narios, we adopt the methodology of ProG [ 20], which leveragesclustering methods to divide the entire graph into 200 distinct sub-
graphs for creating the pretraining dataset. For downstream tasks,
we follow ProG’s approach to construct induced graph datasets tai-
lored for both node and graph classification tasks. In cross-class sce-
narios, we initially divide the label space into pretraining classes and
downstream classes. Subsequently, we partition the entire graph
into two disjoint subgraphs according to these classes, which are
then used to form induced graph datasets for pretraining and down-
stream tasks, respectively. For the cross-task scenario, we utilize
either the real graph classification dataset ENZYMES or the gen-
erated induced graph dataset for graph classification tasks as the
pretraining dataset. The induced graph dataset for node classifica-
tion tasks is employed as our downstream dataset.
Regarding the backdoor attack setting, for all the attack meth-
ods, the trigger graph is designed to consist of only three nodes,
significantly fewer than the number of nodes in the input graph.
During pretraining, the attacker randomly selects a node in the
input graph as the anchor node. We set 𝛼=0.5for both the baseline
attack methods and CrossBA. Additionally, for CrossBA, we set 𝜆
and𝛽to 0.05 across the majority of datasets. The Adam optimizer
is used for optimizing the trigger graph and the backdoored GNN
encoder. Specifically, for CrossBA, we set 𝛾𝑡to 0.01 and𝛾𝑔to 0.0001.
For GCBA_R and GCBA_M, following the setting in [ 31], we set𝛾𝑡
to 0.0015 and 𝛾𝑔to 0.001.
E ATTACK RESULTS IN CROSS-DATASET
SCENARIO
Table 6 highlights the attack performance of various backdoor
attacks in cross-dataset scenarios, where Cora is used as the pre-
training dataset for CiteSeer and CiteSeer-Graph, while Computers
serves as the pretraining dataset for Photo and Photo-Graph. The
results underscore the superior attack capabilities of CrossBA across
diverse downstream applications.
F IMPACT OF PROMPT TOKENS.
Figure 4 illustrates the impact of the number of prompt tokens on
the attack performance of CrossBA against ProG in both node and
graph classification tasks on CiteSeer across five cross-context sce-
narios. In cross-dataset scenarios, Cora is employed for pretraining,
while Photo serves as the pretraining dataset for cross-domain set-
tings. In cross-task scenarios, the generated induced graph dataset
for the graph classification task is utilized for pretraining.
The results reveal that the effectiveness of CrossBA remains
robust, exhibiting little impact from variations in the number of
prompt tokens. Across all five cross-context scenarios, CrossBA
consistently maintains an ASR above 0.90, even with an increased
number of prompt tokens. Importantly, this stability is observed
regardless of the presence of defense mechanisms, emphasizing the
resilience of CrossBA to variations in prompt token numbers.
G IMPACT OF TRIGGER NODES.
Figure 5 illustrates the impact of varying the number of trigger
nodes on the attack performance of CrossBA against ProG and
GraphPrompt in both node and graph classification tasks on Cite-
Seer across five cross-context scenarios. In cross-dataset scenarios,
 
2104KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoting Lyu et al.
Table 6: ACC, ASR, and AD in cross-dataset scenarios. Cora is used as the pretraining dataset for CiteSeer and CiteSeer-Graph,
and Computers is used for Photo and Photo-Graph.
Node Classification Graph Classification
GPL Model AttackCiteSeer Photo CiteSeer-Graph Photo-Graph
ACC(AD) ASR ACC(AD) ASR ACC(AD) ASR ACC(AD) ASR
ProGGATGCBA_R 0.24(+0.57) 0.48 0.44(+0.34) 0.59 0.17(+0.61) 0.00 0.59(+0.30) 0.28
GCBA_M 0.19(+0.62) 0.00 0.46(+0.32) 0.28 0.17(+0.61) 0.06 0.44(+0.45) 0.68
CrossBA 0.83(-0.02) 1.00 0.78(-0.00) 1.00 0.79(-0.01) 1.00 0.89(-0.00) 0.97
GTGCBA_R 0.48(+0.34) 0.80 0.55(+0.26) 0.95 0.44(+0.35) 0.42 0.67(+0.27) 0.99
GCBA_M 0.48(+0.34) 0.90 0.52(+0.29) 0.16 0.44(+0.35) 0.75 0.58(+0.36) 0.62
CrossBA 0.82(-0.00) 1.00 0.81(-0.00) 1.00 0.79(-0.00) 1.00 0.93(+0.01) 1.00
Graph
PromptGATGCBA_R 0.72(-0.00) 0.09 0.60(+0.12) 0.12 0.67(+0.06) 0.10 0.71(+0.10) 0.10
GCBA_M 0.59(+0.13) 0.12 0.64(+0.08) 0.03 0.64(+0.09) 0.03 0.65(+0.16) 0.16
CrossBA 0.67(+0.05) 0.97 0.70(+0.02) 1.00 0.67(+0.06) 0.93 0.82(-0.01) 1.00
GTGCBA_R 0.66(+0.13) 0.06 0.59(+0.13) 0.13 0.59(+0.18) 0.04 0.68(+0.16) 0.09
GCBA_M 0.55(+0.24) 0.01 0.58(+0.14) 0.12 0.66(+0.11) 0.09 0.66(+0.18) 0.32
CrossBA 0.80(-0.01) 1.00 0.71(+0.01) 0.99 0.77(-0.00) 1.00 0.85(+0.01) 0.93
ProG
MetaGATGCBA_R 0.50(+0.17) 0.00 0.83(+0.14) 1.00 0.50(+0.29) 0.00 1.00(-0.00) 0.96
GCBA_M 0.50(+0.17) 0.00 0.50(+0.47) 0.00 0.50(+0.29) 0.00 0.75(+0.25) 1.00
CrossBA 0.92(-0.25) 1.00 0.97(-0.00) 1.00 0.89(-0.10) 1.00 1.00(-0.00) 1.00
GTGCBA_R 0.94(-0.00) 1.00 0.89(+0.10) 0.91 0.50(+0.44) 0.00 1.00(-0.00) 0.01
GCBA_M 0.71(+0.23) 0.29 0.55(+0.44) 0.00 0.68(+0.26) 1.00 0.95(+0.05) 0.96
CrossBA 0.94(-0.00) 1.00 0.98(+0.01) 1.00 0.92(+0.02) 1.00 1.00(-0.00) 1.00
Figure 4: Impact of the number of prompt tokens on attack performance of CrossBA against ProG based on CiteSeer in 5
cross-context scenarios. "Node" represents the node classification task, and "Graph" denotes the graph classification task.
Figure 5: Impact of the number of trigger nodes on performance of CrossBA against ProG and GraphPrompt based on CiteSeer
in 5 cross-context scenarios. "Node" represents the node classification task, and "Graph" denotes the graph classification task.
the pretraining is conducted using Cora, while Photo serves as the
pretraining dataset for cross-domain settings. In cross-task scenar-
ios, the generated induced graph dataset for the graph classification
task is utilized for pretraining. The results highlight that CrossBAconsistently maintains stable attack performance, achieving an ASR
above 0.80 across all the test cases, regardless of the specific number
of trigger nodes employed.
 
2105