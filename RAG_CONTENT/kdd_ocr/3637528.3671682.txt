Self-Supervised Learning for Graph Dataset Condensation
Yuxiang Wang∗
School of Computer Science,
Wuhan University
Wuhan, China
nai.yxwang@whu.edu.cnXiao Yan∗
Centre for Perceptual and Interactive
Intelligence (CPII)
Hong Kong, China
yanxiaosunny@gmail.comShiyu Jin
School of Computer Science,
Wuhan University
Wuhan, China
syjin@whu.edu.cn
Hao Huang
School of Computer Science,
Wuhan University
Wuhan, China
haohuang@whu.edu.cnQuanqing Xu
OceanBase, Ant Group
Hangzhou, China
xuquanqing.xqq@oceanbase.comQingchen Zhang
School of Computer Science and
Technology, Hainan University
Haikou, China
zhangqingchen@hainanu.edu.cn
Bo Du
School of Computer Science,
Wuhan University
Wuhan, China
dubo@whu.edu.cnJiawei Jiang†
School of Computer Science,
Wuhan University
Wuhan, China
jiawei.jiang@whu.edu.cn
ABSTRACT
Graph dataset condensation (GDC) reduces a dataset with many
graphs into a smaller dataset with fewer graphs while maintaining
model training accuracy. GDC saves the storage cost and hence
accelerates training. Although several GDC methods have been
proposed, they are all supervised and require massive labels for
the graphs, while graph labels can be scarce in many practical sce-
narios. To fill this gap, we propose a self-supervised graph dataset
condensation method called SGDC, which does not require label
information. Our initial design starts with the classical bilevel op-
timization paradigm for dataset condensation and incorporates
contrastive learning techniques. But such a solution yields poor
accuracy due to the biased gradient estimation caused by data
augmentation. To solve this problem, we introduce representation
matching, which conducts training by aligning the representations
produced by the condensed graphs with the target representations
generated by a pre-trained SSL model. This design eliminates the
need for data augmentation and avoids biased gradient. We fur-
ther propose a graph attention kernel, which not only improves
accuracy but also reduces running time when combined with self-
supervised kernel ridge regression (KRR). To simplify SGDC and
make it more robust, we adopt a adjacency matrix reusing approach,
which reuses the topology of the original graphs for the condensed
graphs instead of repeatedly learning topology during training.
∗Both authors contributed equally to this research.
†Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671682Our evaluations on seven graph datasets find that SGDC improves
model accuracy by up to 9.7% compared with 5 state-of-the-art
baselines, even if they use label information. Moreover, SGDC is
significantly more efficient than the baselines. Our code is available
at https://github.com/wyx11112/SGDC.
CCS CONCEPTS
•Mathematics of computing →Graph algorithms; •Com-
puting methodologies →Neural networks; •Information
systems→Data mining.
KEYWORDS
graph learning, dataset condensation, graph neural networks
ACM Reference Format:
Yuxiang Wang, Xiao Yan, Shiyu Jin, Hao Huang, Quanqing Xu, Qingchen
Zhang, Bo Du, and Jiawei Jiang. 2024. Self-Supervised Learning for Graph
Dataset Condensation. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3637528.3671682
1 INTRODUCTION
Datasets of graphs are common in domains such as social net-
works [ 16,28,34], molecular detection [ 24,33], and code retrieval [ 17,
19], where the dataset contains many graphs, each modeling the
relation between some entities. There are many tasks for these
datasets, such as graph classification [ 17,34], graph clustering [ 27]
and property prediction [ 9]. In this paper, we mainly focus on
graph classification, which predicts a label for each graph and has
many applications including fraud detection [ 38], recommended
system [ 31], and chemical analysis [ 24,33]. For instance, in the HIV
molecule dataset [ 13], each graph represents a molecule, where
the nodes are atoms and edges are chemical bonds; graph classifi-
cation predicts whether a molecule inhibits HIV virus replication.
3289
KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxiang Wang, Xiao Yan, Shiyu Jin, Hao Huang, Quanqing Xu, Qingchen Zhang, Bo Du, and Jiawei Jiang
GNN(!𝜃(𝒮))GNN(!𝜃(𝒮))Gradient Matchingupper-levellower-level(a) Supervised GDCupdate !𝜃(𝒮)𝒯𝒮update 𝒮GNN(𝜃)ACC(b) Self-supervised GDC via Contrastive Learning GNN(!𝜃(𝒮))GNN(!𝜃(𝒮))Gradient Matchingupper-level
ACClower-levelupdate!𝜃(𝒮)Aug.𝒯𝒮update 𝒮GNN(𝜃)𝒯!",𝒯#"𝒮!",𝒮#"𝒯: original graph; 𝒮: synthetic graph; Aug.: augmentation;!𝜃(𝒮): parameters of GNN trained on 𝒮 
Figure 1: Supervised and naive self-supervised graph dataset
condensation solutions.
However, training accurate classification models can be expen-
sive because large datasets often contain many (e.g., millions) of
graphs. The efficiency challenge is exacerbated by hyperparameter
search [ 8,41,46], which experiments with many configurations
(e.g., network layers, output dimension, and learning rate) to im-
prove model accuracy, and each configuration requires to train a
model from scratch on the large-scale dataset.
Graph dataset condensation (GDC) aims to construct a small-scale
synthetic graph such that a GNN trained on it can achieve compa-
rable performance to the one trained on the original dataset [ 10,
15,18,43,44]. In particular, given a dataset Twith𝑁graphs, GDC
compresses it into a dataset Swith𝑀synthetic graphs, and training
time becomes much shorter by making 𝑀significantly smaller than
𝑁. There are already several methods for GDC, e.g., GCond [ 15],
DosCond [ 14], and MSGC [ 7], and they generally adopt the struc-
ture in Figure 1(a). The GDC is placed in a bilevel optimization
problem [ 4,44]: the lower-level optimization is to fit the synthetic
graph with a training objective, while the upper-level optimization
aims to find an appropriate synthetic graph. Specifically, the lower-
level optimization trains a graph neural network (GNN, parame-
terized by ˆ𝜽(S)) on the synthetic dataset S, and the upper-level
optimization applies ˆ𝜽(S)on the original dataset Tand updatesS
by encouraging the original and synthetic graphs to have similar
gradients for the GNN model (i.e., gradient matching).
Existing GDC methods can effectively reduce dataset size while
maintaining model performance. However, a critical limitation
is that they are all supervised and require labels for the original
dataset. In practice, usually a significant potion of graphs in the
datasetTdo not have labels. Thus, SSL is proposed to train models
on datasets free of labels and can achieve decent accuracy perfor-
mance [ 11,26,40]. As supervised GDC methods cannot handle
unlabeled graphs and benefit from SSL techniques, we ask the fol-
lowing research question:
Is it possible to design a self-supervised method for graph
dataset condensation?
To answer the research question, we propose an initial design in
Figure 1(b), named self-supervised GDC via contrastive learning
(SGDC-CL). It follows existing methods to compress the original
dataset in a bilevel optimization, except that the supervised objec-
tive is replaced by an SSL objective. In particular, both lower-level
and upper-level optimizations conduct data argumentation by ran-
domly perpetuating the graphs and compute the contrastive loss;
the lower-level optimization trains an SSL model on the synthetic
graphs, and the upper-level optimization applies the model trainedby the lower-level to the original dataset for gradient matching.
However, such a self-supervised GDC method suffers from low
model accuracy andpoor training efficiency :❶The SSL objective
introduces random variables for data augmentation, which leads
to biased gradient for gradient matching and thus low model accu-
racy;❷Solving the bilevel optimization problem requires a huge
computational cost, which results in poor training efficiency.
By tackling the problems above, we design an effective self-
supervised graph dataset condensation method dubbed SGDC. First,
we use a novel representation matching scheme to replace the con-
trastive learning objective that causes gradient bias. The represen-
tation matching involves computing target representations for both
original and synthetic graphs using a pre-trained SSL model, and
updating the synthetic graphs by minimizing the mean square error
(MSE) between their representations and the original graphs. In a
nutshell, we try to compress the original training set by matching
representations instead of gradients, which eliminates the depen-
dency on gradients. Second, to speed up the condensation process,
we design a graph attention neural tangent kernel (ANTK) and
combine it with kernel ridge regression (KRR), which simplifies
the bilevel optimization into a single-level optimization. ANTK
estimates attention scores for node pairs in a graph, facilitating ef-
fective neighbor feature aggregation. Furthermore, SGDC employs
anadjacency matrix reusing design, eliminating the need to com-
pute and update adjacency matrices for synthetic graphs during
training. Instead, the original graph topology is reused for the syn-
thetic graphs, simplifying the condensation process and enhancing
training stability by decoupling topology and node feature updates.
Extensive experiments show that SGDC yields the best accuracy
on 6 of the 7 graph datasets compared with 5 state-of-the-art base-
lines. Notably, SGDC outperforms the best baseline by up to 9.7%
despite not utilizing label information. Moreover, SGDC runs faster
than the baselines due to its efficiency optimizations, and we also
show that SGDC can accelerate model training on original dataset
by 2.3× with a case study.
To summarize, we make the following contributions:
•We observe that existing graph dataset condensation (GDC) meth-
ods are all supervised and propose to study self-supervised GDC,
which does not require label information.
•We propose SGDC method for self-supervised GDC, which adopts
representation matching to tackle the biased gradient issue. More-
over, techniques including KRR, ANTK and adjacency matrix
reusing are designed to speed up and stabilize the graph conden-
sation process.
•We conduct extensive experiments to evaluate SGDC and com-
pare with state-of-the-art baselines, demonstrating that SGDC en-
joys high model accuracy and training efficiency.
2 RELATED WORK
General dataset condensation. Compressing large-scale dataset
into smaller one to speed up model training has been a concern
of researchers. Dataset condensation is first proposed in computer
vision to compress image datasets. DM [ 43] synthesizes condensed
images by matching feature distributions of the synthetic and orig-
inal training images in sampled embedding spaces. Cazenavette
3290Self-Supervised Learning for Graph Dataset Condensation KDD ’24, August 25–29, 2024, Barcelona, Spain
et al. [ 3] propose to optimize the condensed images to guide net-
works to a similar state as those trained on real images across many
training steps. DSA [ 42] enables effective use of data augmenta-
tion to synthesize more informative synthetic images. However,
these methods are all used to compress image dataset, and research
related to graph dataset condensation has not been well explored.
Node-level dataset condensation. Node-level GDC methods re-
duce the number of nodes by matching the training gradient of
the model on the original dataset. Traditional methods reduce the
size of the nodes through heuristic approaches. For instance, Herd-
ing [ 32] modifies the model’s parameters so that they gravitate
towards areas in the parameter space that yield favorable outcomes.
K-Center [ 6,25] looks at the feature space of the data and seeks to
cover as much as possible with a smaller set. The earliest node-level
GDC method, GCond [ 15], simultaneously condenses node features
and structures by minimizing the gradient difference between the
original dataset and synthetic graph. Unlike matching gradient,
DM [ 43] constructs a condensed dataset by aligning the feature
distribution of synthetic and original training samples within sev-
eral sampled embedding spaces. Zheng et al. [ 45] implicitly encode
topology structure information into the node features in the syn-
thetic graph-free data. Nevertheless, these methods primarily focus
on reducing the number of nodes and, consequently, may not effi-
ciently produce valuable synthetic graphs for graph classification
when dealing with graph-level datasets.
Graph-level dataset condensation. Unlike node-level approaches,
graph-level dataset condensation methods aim to compress the
number of graphs. DosCond[ 14] adopts one-step gradient match-
ing to reduce condensation time. Taking it a step further, KiDD[ 36]
converts the bilevel optimization into a single-level optimization
problem by learning a closed-form solution through kernel ridge
regression. As opposed to emulating gradient flows on the original
training set, MIRAGE [10] compresses the computation data itself
by decomposing the input graph into a multi-set of computation
trees. Different from these state-of-the-art methods, our method
not only compresses the dataset in a self-supervised manner but
also implicitly encodes structure information into the synthetic
graph through the attention neural tangent kernel.
3 INITIAL DESIGN: SGDC-CL
Notions. Before detailing our approach, we first introduce the
main notations. Consider a original dataset T=(𝐺1,···,𝐺𝑁),
each graph inTis defined by the node feature matrix Xand the
adjacency matrix A. We randomly sample 𝑀(𝑀<<𝑁) graphs from
Tas the initialized synthetic graphs S=(𝐺′
1,···,𝐺′
𝑀). Similarly,
we denote the node feature matrix and the adjacency matrix for
each graph inSasX′andA′, respectively. Our goal is to optimize
S(i.e., optimize X′andA′) in a self-supervised manner so that the
performance of a GNN model 𝑓trained onSis comparable to the
one trained onT.
Initial design. A naive approach is to replace the supervised ob-
jective with a self-supervised one in bilevel optimization. Here, we
choose contrastive learning (CL) [ 11,39,40] as the self-supervised
objective, which is widely used in self-supervised methods and
achieves state-of-the-art performance. Thus, our initial design is a
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000017/uni00000018/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000036/uni0000002a/uni00000027/uni00000026/uni00000010/uni00000024/uni00000057/uni00000057 
/uni00000015 /uni00000015/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000056/uni0000005c/uni00000051/uni00000057/uni0000004b/uni00000048/uni00000057/uni0000004c/uni00000046/uni00000003/uni0000004a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000036/uni0000002a/uni00000027/uni00000026/uni00000010/uni00000024/uni00000057/uni00000057 /uni00000036/uni0000002a/uni00000027/uni00000026/uni00000010/uni00000026/uni0000002f /uni00000036/uni0000002a/uni00000027/uni00000026/uni00000010/uni00000035/uni00000030  /uni00000036/uni0000002a/uni00000027/uni00000026/uni00000010/uni00000035/uni00000030 /uni00000036/uni0000002a/uni00000027/uni00000026/uni00000010/uni00000026/uni0000002f Figure 2: The results of SGDC-Att, SGDC-CL and SGDC-RM
for accuracy performance and running time on NCI1.
Self-supervised Graph Condensation via Contrastive Learning (SGDC-
CL). Briefly, SGDC-CL first runs data augmentation on both the
original graph and the synthetic graph, and then uses bilevel opti-
mization to alternatively optimize the GNN model and the synthetic
graph. In the following, we elaborate the details of SGDC-CL:
•Data augmentation. As shown in Figure 1 (b), we apply data
augmentation on both the original graphs Tand the synthetic
graphsSin stage 1. Specifically, we apply two data augmentation
methods: randomly removing edges or shuffling node features.
In this way, we generate two corrupted views, (T′
1,T′
2)and(S′
1,
S′
2), and maximize the mutual information between these views
by optimizing a contrastive learning loss LSSL. Here we choose
InfoNCE [40] as the contrastive learning loss.
•Upper-level optimization. The goal of this module is to update
Swhile fixing the GNN model, so that the gradients on Scan
mimic those onT. To achieve this, in stage 2, we update the
synthetic graphs via minimizing a gradient matching loss LGM,
which is the MSE between the gradients trained on SandT.
The lower-level and upper-level optimizations are iteratively
executed until convergence.
•Lower-level optimization. We optimizeLSSLof a given GNN
model𝑓𝜽parameterized by 𝜽onSin stage 3. Then, this lower-
level optimization module passes the optimal model parameters
ˆ𝜽(S)to the upper-level optimization.
Overall, the objective of SGDC-CL can be formulated as follows:
min
SLGMh
∇ˆ𝜽(S)LSSL
𝑓ˆ𝜽(S) T′
1,T′
2
,∇ˆ𝜽(S)LSSL
𝑓ˆ𝜽(S) S′
1,S′
2i
𝑠.𝑡. ˆ𝜽(S)=arg min
𝜽LSSL 𝑓𝜽(S′
1,S′
2) (1)
Furthermore, we calculate the synthetic adjacency matrix based
on the similarity between nodes in each condensation step:
A′
𝑖𝑗=Sigmoid(x′
𝑖x′
𝑗), (2)
where x′
𝑖andx′
𝑗denote the𝑖-th and𝑗-th row vectors of the node
feature matrix X′, respectively.
Limitation of SGDC-CL. SGDC-CL updates the model parameters
by stochastic gradient descent w.r.t. Eq. 1. However, we observe
that the performance of SGDC-CL is not satisfactory in Figure 2.
Regarding model accuracy, we hypothesize that the randomness
introduced by data augmentation may lead to biased gradient esti-
mation, thereby destabilizing the model and impacting performance.
3291KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxiang Wang, Xiao Yan, Shiyu Jin, Hao Huang, Quanqing Xu, Qingchen Zhang, Bo Du, and Jiawei Jiang
𝒯𝒮GNN(#𝜃(𝒮))………..………..target rep. Y𝒯 original graph rep.ℒ"##$%GNN(𝜃)……..target rep. Y𝒮 …….….synthetic graph rep.ℒ'()$%update!𝜃(𝒮)pre-trained model 𝑔
update 𝒮upper-levelpre-trained model 𝑔lower-level(a) Self-supervised GDC via Representation Matching()𝒯𝒮pre-trained model 𝑔kernel function 𝐊………..target rep. Y𝒯 ……..target rep. Y𝒮 K𝒯𝒮: kernel matrix for 𝒯	and 𝒮 K*𝒮: kernel matrix for 𝑆	and 𝒮 predicted rep.××SimilarKRRupdate 𝒮
(b) Self-supervised GDC
𝒯: original dataset𝒮: synthetic graphrep.: representation
Figure 3: The overview of SGDC-RM and SGDC. The red dashed line represents backpropagation.
For model efficiency, the iterative bilevel training and frequent com-
putation of dense adjacency matrix prolong the running time.
Theoretical analysis. To verify our guess of biased gradient esti-
mation brought by data augmentation, we conduct a formal analysis.
We defineLSSL(𝑓(𝜽,S))=E𝜉∼D
𝑓𝜉(𝜽,S)
, where𝜉∼D is a ran-
dom variable refer to data augmentation, and Ddenotes random
distribution. 𝑓𝜉is the SSL encoder with sampled data augmentation
𝜉. In contrastive learning, we perform multiple data augmentation
operations on the input graph, so we rewrite the formulation as
ˆLSSL(𝑓(𝜽,S))=1
𝑟Í𝑟
𝑖=1
𝑓𝜉𝑖(𝜽,S)
.
Theorem 1. The derivative𝜕ˆLSSL(𝑓(𝜽,S);T)
𝜕Sis biased in back-
propagation, i.e., E𝜉
𝜕ˆLSSL(𝑓(𝜽,S);T)
𝜕S
≠𝜕LSSL(𝑓(𝜽,S);T)
𝜕S.
Proof. Let(𝑖,𝑗)∈{ 1,...,𝑚}×{ 1,...,𝑑}, where𝑚and𝑑denote
the number and dimension of the synthetic graphs, respectively.
By the chain rule,
𝜕LSSL(𝑓(𝜽,S);T)
𝜕(S)𝑖𝑗= 
𝜕LSSL(𝑓(𝜽,S);T)
𝜕𝜽𝜽=ˆ𝜽(S)!
𝜕ˆ𝜽(S)
𝜕(S)𝑖𝑗.
For convenience, we define 𝑣𝑘=𝜕LSSL(𝑓(𝜽,S);T)
𝜕𝜽𝑘𝜽=ˆ𝜽(S)and𝛼𝑘=
𝜕ˆ𝜽(S)𝑘
𝜕(S)𝑖 𝑗. Taking the expectation for both sides of the above equation,
and express the right-hand side of the equation in summation form:
𝜕LSSL(𝑓(𝜽,S);T)
𝜕(S)𝑖𝑗=𝑑𝜽∑︁
𝑘=1E𝜁[𝑣𝑘]E𝜁[𝛼𝑘] (3)
Different from ordinary differential equations, stochastic differen-
tial equations require an additional item of covariance to evaluate
the correlation between random variables, expressed as:
E𝜁"
𝜕ˆLSSL(𝑓(𝜽,S);T)
𝜕(S)𝑖𝑗#
=𝑑𝜽∑︁
𝑘=1E𝜁[𝑣𝑘]E𝜁[𝛼𝑘]+𝑑𝜽∑︁
𝑘=1Cov𝜁[𝑣𝑘,𝛼𝑘].
A comparison with Eq. 3 illustrates that the two are not equivalent.4 ENHANCED DESIGN: SGDC
The initial design adopts contrastive learning and gradient match-
ing for graph condensation in a self-supervised manner, but suf-
fers from the randomness introduced by data augmentation. Since
SGDC-CL heavily depends on the robustness of data augmentation
and gradient estimation, we ask can we design a self-supervised
graph condensation method without relying on the gradients? Moti-
vated as such, we take a different angle and try to match the graph
representations instead of gradients, yielding our enhanced design.
4.1 Representation Matching
To avoid biased gradient matching caused by data augmentation,
we introduce a pre-trained SSL model 𝑔with parameters 𝜽∗to gen-
erate target representations, where 𝜽∗=arg min 𝜽LSSL𝑔(𝜽,T).
Adopting the bilevel optimization architecture, we match the repre-
sentations of the trained GNN model and pre-trained model on the
synthetic graph in the lower-level optimization, and then match
those on the original dataset. Consequently, the instability of the
model during the condensation process is eliminated. Following,
we present the details of SGDC-RM (Self-supervised Graph Con-
densation via Representation Matching):
•Upper-level optimization. As shown in Figure 3 (a), we use
the pre-trained model 𝑔to generate target representations YT=
𝑔𝜽∗(T) in stage 1. And then we update synthetic graph Sby
matching YTwith the graph representations generated by the
GNN model 𝑓ˆ𝜽(S)in stage 2. The upper-level objective Lupper
is defined as the MSE between YTand𝑓ˆ𝜽S(T). Because ˆ𝜽Sis
related toS, according to the chain rule we can update Sby
iteratively optimizing Lupper , i.e.,𝜕Lupper
𝜕S=𝜕Lupper
𝜕ˆ𝜽(S)𝜕ˆ𝜽(S)
𝜕S.
•Lower-level optimization. Similarly, we generate the target
representations YS=𝑔𝜽∗(S)for synthetic graph. In stage 3, we
optimize lower-level objective Llower by using MSE to maximize
the similarity between target representation YSand the synthetic
graph representation generated by the GNN model 𝑓𝜽. Then we
iteratively optimize Llower to find the optimal parameters ˆ𝜽(S),
which are passed to the upper-level optimization.
3292Self-Supervised Learning for Graph Dataset Condensation KDD ’24, August 25–29, 2024, Barcelona, Spain
The overall objective of SGDC-RM is formally as follows:
 
minS1
2YT−𝑓ˆ𝜽(S)(T)2
𝐹upper-level
ˆ𝜽(S)=arg min 𝜽1
2∥YS−𝑓𝜽(S)∥2
𝐹lower-level. (4)
The intuition behind the representation matching is that we aim
to ensure the representation space of the model 𝑓ˆ𝜽(S)trained on
Sis similar to that of the pre-trained SSL model 𝑔trained onT.
However, as shown in Figure 2, the accuracy performance and
training efficiency of SGDC-RM are still unsatisfactory. This is
because using MSE cannot completely fit the target representation
with graph representation generated by GNN model, which will
inevitably cause information loss and thus a decrease in accuracy.
Moreover, the method is still a bilevel optimization architecture,
which requires huge calculations to optimize.
4.2 Self-Supervised Kernel Ridge Regression
The costly overhead of SGDC-RM stems from the bilevel optimiza-
tion and iterative GNN training. To this end, we ask can we simplify
the bilevel optimization framework? In this section, we propose a
self-supervised kernel ridge regression approach.
Kernel ridge regression. To answer above question, the current
literature demonstrates that, if a closed-form solution exists for
the lower-level optimization, it can be integrated into the upper-
level optimization [ 22,36]. In this way, bilevel optimization can be
simplified to a single-level optimization, thus reducing the over-
all computation complexity. Kernel Ridge Regression (KRR) is a
possible method that provides a closed-form solution. Traditional
ridge regression solves regression problems by minimizing the
MSE between the predicted and target data. KRR extends ridge
regression by adding a kernel function to map the input data into a
higher-dimensional feature space. This transformation allows KRR
to capture complex non-linear relationships between the input and
target data. Consequently, we use KRR combined with graph kernel
to convert Eq. 4 into the following formula:
min
SLKRR=1
2YT−KTS(KSS+𝜆I)−1YS2
𝐹, (5)
where𝜆is the hyper-parameter of KRR and Iis the identity ma-
trix.YTandYSdenote the labels of the original dataset and the
synthetic graph, respectively. KTSis the graph kernel matrix that
measures the similarity between TandS, and KSSis similarly
defined. As Eq. 5 shows, we can predict the original labels by cal-
culating these two graph kernel matrices. The predicted labels are
then evaluated against the actual labels using MSE as the objective.
Self-supervised KRR. Eq. 5 requires a large amount of ground-
truth labels. Inspired by our proposed representation matching, we
further propose a Self-supervised KRR (SKRR) method, that replaces
the graph labelsYTandYSwith the target representations. As
illustrated in Figure 3 (b), we first calculate two kernel matrices
KTSandKSS(i.e., stage 1), and then generate YT=𝑔𝜽∗(T)and
YS=𝑔𝜽∗(S)using pre-trained model 𝑔𝜽∗(i.e., stage 2). Afterwards,
we compute the predicted representation for the original graph
by utilizing YS,KTS, and KSS. The SKRR objective is computed
using this predicted representation and the target representation
(i.e., stage 3). Finally, we replace the counterparts in Eq. 5 with YTandYS, yielding the objective for SKRR:
min
SLSGDC =1
2YT−KTS(KSS+𝜆I)−1YS2
𝐹, (6)
The underlying intuition of SKRR is to update the synthetic graph
in a way that the predicted representation closely resembles the
target representation.
4.3 Attention-based Graph Kernel for KRR
Graph neural tangent kernel. There is still an unresolved issue:
how to choose an appropriate graph kernel for KRR ? Graph kernels
have been widely used in graph classification tasks [ 21,37]. How-
ever, classical graph kernels, such as the random walk kernel [ 30]
and the Laplacian kernel [ 1], cannot match the performance of
GNN methods in terms of expressive power and accuracy [ 5,36].
Fortunately, the recent emergence of Graph Neural Tangent Ker-
nel (GNTK) [ 5] has overcome the shortcomings of previous graph
kernels. It constructs a powerful tangent space at the initialization
of the GNN, not only capturing the similarity between graphs like
traditional kernels, but also incorporating the strong expressive
power of GNN. Therefore, we chose GNTK as the graph kernel for
KRR.
Formally, GNTK corresponds to an infinitely-wide multi-layer
GNN trained by gradient descent. Specifically, given two graphs 𝐺=
(𝑉,𝐸)and𝐺′=(𝑉′,𝐸′)with𝑛and𝑛′nodes, the covariance matrix
between these two graphs can be denoted as Σ(0)⟨𝐺,𝐺′⟩∈R𝑛×𝑛′.
We calculate each element inh
Σ(0)⟨𝐺,𝐺′⟩i
𝑢𝑢′by the dot product
x⊤𝑢x𝑢′, where x𝑢andx𝑢′denote the node features of 𝑢∈𝑉and
𝑢′∈𝑉′. As a result, for each GNN layer 𝑙∈{1,...,𝐿}which has𝑅
activation layers, GNTK computes the kernel matrixh
Θ(𝑙)
(𝑟)⟨𝐺,𝐺′⟩i
for each𝑟∈[𝑅]:
h
Θ(𝑙)
(𝑟)
𝐺,𝐺′i
𝑢𝑢′=h
Θ(𝑙)
(𝑟−1)
𝐺,𝐺′i
𝑢𝑢′h
¤Σ(𝑙)
(𝑟)
𝐺,𝐺′i
𝑢𝑢′
+h
Σ(𝑙)
(𝑟)
𝐺,𝐺′i
𝑢𝑢′,(7)
where¤Σ(𝑙)
(𝑟)is the derivative of the covariance matrix corresponding
to the𝑙-th layer through the 𝑟-th activation layers.
Attention neural tangent kernel. GNTK assumes that all neigh-
borhood nodes are of equal importance to the central node, which
clearly contradicts the topological relationships in the real world.
Recently, many empirical studies have utilized attention mecha-
nisms, assisting message-passing GNNs in extracting graph struc-
ture information [ 29,35]. Motivated as such, we propose an atten-
tion neural tangent kernel, termed ANTK. Specifically, we denote the
covariance matrix bΣ⟨𝐺,𝐺′⟩and the kernel matrix bΘ⟨𝐺,𝐺′⟩akin to
Eq. 7. Subsequently, we employ a multi-head attention mechanism
to compute the attention matrix, denoted as Γ⟨𝐺,𝐺′⟩, representing
the importance of each neighboring node relative to the central
node. We use the following formula to calculate the ANTK value:
h
bΘ(𝑙)
(𝑟)
𝐺,𝐺′i
𝑢𝑢′=h
Γ(𝑙)
𝐺,𝐺′i
𝑢𝑢′h
bΘ(𝑙)
(𝑟−1)
𝐺,𝐺′i
𝑢𝑢′
+2h
bΣ(𝑙)
(𝑟)
𝐺,𝐺′i
𝑢𝑢′,
h
bΣ(𝑙)
(𝑟)
𝐺,𝐺′i
𝑢𝑢′=h
Γ(𝑙)
𝐺,𝐺′i
𝑢𝑢′h
bΣ(𝑙)
(𝑟−1)
𝐺,𝐺′i
𝑢𝑢′,(8)
3293KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxiang Wang, Xiao Yan, Shiyu Jin, Hao Huang, Quanqing Xu, Qingchen Zhang, Bo Du, and Jiawei Jiang
Table 1: Computation costs of our methods and baselines. 𝑁is the number of nodes in each original graph, 𝑛is the number of
nodes in each synthetic graph, 𝑀is the number of original graphs, 𝑚is the number of synthetic graphs, 𝑑is the dimension of
the representation, 𝐿is the number of GNN layer, 𝑟is the rank of the synthetic graphs, and 𝐾is the number of clusters.
Method Herding K-Center DosCond KiDD SGDC SGDC-Att
Complexity 𝑂(𝐾𝑀𝑁2)𝑂(𝐾𝑀𝑁2)𝑂(𝐿𝑑(𝑚𝑛2+𝑚𝑛𝑑+𝑀𝑁2+𝑚𝑛2))𝑂(𝑟𝐿𝑀𝑁2𝑚𝑛)𝑂(𝐿𝑀𝑁𝑚𝑛+𝐿𝑚2𝑛2)𝑂(𝐿𝑀𝑁𝑚𝑛+𝐿𝑚2𝑛2+𝑛2)
Γ⟨𝐺,𝐺′⟩is achieved through a matrix multiplication between the
column vector of bΣ⟨𝐺,𝐺⟩’s diagonal and the row vector of bΣ⟨𝐺′,𝐺′⟩
diagonal. Compared to GNTK, ANTK evaluates the importance of
each neighbor node to the central node, so that can discover more
effective structure information. For detailed derivation and proof
of Eq. 8, please refer to [12].
Finally, we readout all nodes in 𝑉and𝑉′to compute the GNTK
and ANTK between 𝐺and𝐺′with Eq. 7 and Eq. 8, respectively:
 
𝐾GNTK(𝐺,𝐺′)=Í
𝑢∈V,𝑢′∈V′h
Θ(𝐿)
(𝑅)⟨𝐺,𝐺′⟩i
𝑢𝑢′
𝐾ANTK(𝐺,𝐺′)=Í
𝑢∈V,𝑢′∈V′h
bΘ(𝐿)
(𝑅)⟨𝐺,𝐺′⟩i
𝑢𝑢′. (9)
As a result, we can calculate KTSandKSSin Eq. 6 as follow:
KTS=𝑁∑︁
𝑖=1𝑀∑︁
𝑗=1h
𝐾(𝐺𝑖,𝐺′
𝑗)i
,KSS=𝑁∑︁
𝑖=1𝑀∑︁
𝑗=1h
𝐾(𝐺′
𝑖,𝐺′
𝑗)i
.(10)
where𝐺𝑖∈T and𝐺′
𝑖,𝐺′
𝑗∈S. The kernel function 𝐾can be either
𝐾GNTK or𝐾ANTK . We call the SGDC approach using GNTK and
ANTK as SGDC andSGDC-Att, respectively.
4.4 Adjacency Matrix Reusing
Existing GDC methods carefully design a graph structure learner
that computes the parameterized adjacency matrix of the synthetic
graph based on node features [ 5,10,14,36]. To this end, we propose
anadjacency matrix reusing method for reducing the computation of
adjacency matrix during graph condensation. Specifically, we main-
tain the original adjacency matrix when condensing the dataset,
rather than iteratively calculating a new adjacency matrix in each
condensation step as in Eq. 2.
Our intuition is that, focusing on the condensation of graph fea-
tures alone can often yield comparable or even better performance
than compressing both graph features and structure [ 45]. This is be-
cause optimizing graph features and structures simultaneously may
result in model instability during the optimization process [ 15,45].
However, our proposed method can implicitly learn structure infor-
mation during graph condensation. Firstly, we minimize the MSE
between the representations of synthetic graph and those generated
by the pre-trained model, which can implicitly capture structure
information. Secondly, as we will elaborate in Section 4.3, the pro-
posed ANTK considers different importance of neighboring nodes
and enhances the ability of learning graph structures. Therefore,
although we do not iteratively compute the adjacency matrix in
each condensation step, we implicitly embed the graph structures
in the synthetic graph during the condensation process.
Time complexity. We present a comprehensive complexity analy-
sis in Table 1. Herding [ 32] and K-Center [ 6] use𝑂(𝐾𝑀𝑁2)time to
compute the distance between the nodes and the clusters. DosCondTable 2: Dataset statistics.
Dataset # Graphs # Nodes # Edges # Features # Classes
NCI1 4110 29.9 32.3 37 2
NCI109 4127 29.7 32.1 38 2
PROTEINS 1113 39.1 72.8 4 2
DD 1178 284.3 715.7 89 2
ogbg-molhiv 41127 25.5 54.9 9 2
ogbg-molbbbp 2039 24.1 51.9 9 2
ogbg-molbace 1513 34.1 73.7 9 2
[14] uses bilevel optimization. The lower-level optimization costs
𝑂(𝐿𝑚(𝑛2𝑑+𝑛𝑑2))to evaluate the synthetic graph. The upper-level
optimization matches the gradient between the original dataset and
the synthetic graphs, which costs 𝑂(𝐿𝑀𝑁2𝑑+𝐿𝑚𝑛2𝑑). KiDD [ 36]
takes𝑂(𝑟𝐿𝑀𝑁2𝑚𝑛)time to compute the random walk kernel.
OurSGDC mainly involves two kernel matrix computations. Specif-
ically, the kernel matrix 𝐾TSconsumes𝑂(𝐿𝑀𝑁𝑚𝑛)to calculate
the similarity between the original graphs and the synthetic graphs,
and the complexity of 𝐾SSis𝑂(𝐿𝑚2𝑛2)to calculate the similar-
ity between the two synthetic graphs. SGDC-Att costs additional
𝑂(𝑛2)time to calculate the attention kernel.
In this paper, the number of synthetic graphs is significantly
smaller than the original graphs (i.e., 𝑚<<𝑀), and the number of
nodes in each synthetic graph is the same as each original graph
(i.e.,𝑁=𝑛). The complexity of SGDC and DosCond [ 14] can be sim-
plified to𝑂(𝑀𝑁2), while the KiDD [ 36] is𝑂(𝑀𝑁3). This suggests
that SGDC is as efficient as the fastest state-of-the-art methods.
5 EXPERIMENTAL EVALUATION
We conduct extensive experiments to evaluate our proposed GDC
methods and answer the following research questions:
•RQ1. How effective are SGDC andSGDC-Att compared with the
state-of-the-art baseline methods?
•RQ2. How efficient is our proposed method?
•RQ3. CanSGDC perform well for practical applications?
•RQ4. How effective is the design of adjacency matrix reusing?
•RQ5. What is the impact of pre-trained models for SGDC?
5.1 Experiment Settings
Datasets. We conduct experiments on 7 publicly accessible graph
datasets, including NCI1, NCI109, DD, and PROTEINS from TU-
Dataset [ 20], along with ogbg-molhiv, ogbg-molbbbp, and ogbg-
molbace from the OGB [ 13]. These datasets are widely used for the
graph classification task. Following [ 36], we randomly divide the
3294Self-Supervised Learning for Graph Dataset Condensation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Graph classification performance comparison (mean±std). The best and runner-up results are marked in bold and
underlined, respectively. Whole indicates the results of training the pre-trained SSL models on the original datasets.
Dataset N_Syn (Ratio)Other Graph Size Reduction Baselines Supervised GDC Self-supervised GDCWhole GainRandom Herding K-Center DosCond KiDD SGDC SGDC-Att
NCI1
(ACC)2 (0.06%) 57.4±3.0 59.2±3.0 59.2±3.0 57.1±0.9 58.6±0.7 63.8±0.3 64.5±0.5
77.99.0%
20 (0.61%) 59.9±2.0 62.8±0.9 59.1±0.8 60.8±0.9 61.7±1.3 65.1±0.6 65.9±0.2 4.9%
100 (3.04%) 60.5±2.1 62.5±2.0 59.5±0.5 62.7±0.8 64.2±0.6 66.2±0.2 65.9±0.7 3.1%
NCI109
(ACC)2 (0.06%) 54.3±2.3 51.7±0.9 51.7±0.9 54.9±2.3 51.7±1.6 59.6±0.5 58.2±0.1
75.58.6%
20 (0.61%) 61.9±1.6 63.6±0.3 52.9±1.7 61.4±1.5 49.8±1.7 64.3±0.3 64.6±0.1 1.6%
100 (3.03%) 64.0±1.4 64.7±1.2 55.0±2.1 62.9±1.6 69.0±1.0 69.6±0.5 69.2±0.1 0.9%
PROTEINS
(ACC)2 (0.22%) 57.8±1.8 67.6±1.7 67.6±1.7 63.4±1.9 68.7±1.9 70.5±0.4 70.9±0.2
74.43.2%
20 (2.25%) 67.2±0.7 68.3±1.0 71.4±3.3 71.7±0.4 73.8±1.1 71.1±0.3 72.6±0.1 -1.6%
100 (11.24%) 69.6±4.0 70.1±1.0 72.9±2.6 73.2±0.8 75.0±1.9 72.2±0.4 72.7±0.2 -3.1%
DD
(ACC)2 (0.21%) 61.3±8.5 60.7±8.4 61.0±3.2 63.0±0.7 66.4±2.1 69.1±0.2 72.3±0.1
76.98.9%
20 (2.12%) 66.8±2.1 67.4±0.7 66.2±2.4 68.1±1.8 69.2±1.7 70.1±0.4 70.7±0.2 1.1%
100 (10.62%) 71.4±2.2 71.6±1.9 72.3±1.0 70.9±1.0 72.7±1.0 73.4±0.2 73.8±0.1 1.5%
ogbg-molhiv
(ROC-AUC)2 (<0.01%) 0.555±0.036 0.633±0.025 0.633±0.025 0.612±0.025 0.637±0.033 0.689±0.012 0.699±0.014
0.7569.7%
20 (0.06%) 0.579±0.012 0.621±0.009 0.630±0.013 0.647±0.031 0.677±0.013 0.721±0.025 0.723±0.018 6.8%
100 (0.30%) 0.623±0.013 0.616±0.014 0.628±0.014 0.620±0.018 0.707±0.006 0.732±0.014 0.734±0.022 3.8%
ogbg-molbbbp
(ROC-AUC)2 (0.12%) 0.579±0.025 0.628±0.012 0.628±0.012 0.584±0.030 0.620±0.012 0.638±0.005 0.647±0.009
0.6933.2%
20 (1.23%) 0.556±0.003 0.625±0.002 0.596±0.016 0.621±0.013 0.638±0.037 0.653±0.015 0.648±0.012 2.4%
100 (6.13%) 0.610±0.007 0.630±0.013 0.595±0.018 0.628±0.012 0.653±0.016 0.658±0.027 0.667±0.011 2.1%
ogbg-molbace
(ROC-AUC)2 (0.17%) 0.638±0.009 0.546±0.038 0.546±0.038 0.667±0.021 0.693±0.016 0.716±0.021 0.722±0.004
0.7944.2%
20 (1.65%) 0.649±0.017 0.561±0.041 0.658±0.016 0.694±0.018 0.735±0.020 0.748±0.017 0.750±0.008 2.0%
100 (8.26%) 0.655±0.020 0.703±0.012 0.662±0.013 0.710±0.006 0.736±0.007 0.753±0.008 0.743±0.006 2.3%
graphs into 80%/10%/10% splits for training/validation/testing, re-
spectively. Each dataset size varies from several thousand to tens of
thousands of graphs to maintain diversity. Detailed dataset statistics
are provided in Table 2.
Baselines. We compared our methods with several state-of-the-
art baseline methods. These methods can be categorized into the
following four categories:
•Graph size reduction methods. (a) Random: it randomly sam-
ples graphs from the original training set. (b) Herding [ 32]: it
chooses the samples closest to the cluster center. (c) K-Center [ 6]:
it selects center samples that minimize the greatest distance be-
tween a sample and its nearest center generated by K-Means.
•Supervised GDC methods. (a) DosCond [ 14]: it condenses the
whole training set into smaller synthetic graphs in one gradient
matching step. (b) KiDD [ 36]: it reduces computational cost by
turning bilevel optimization into single-level optimization based
on GNTK kernel ridge regression.
•Self-supervised GDC methods. OurSGDC andSGDC-Att.
•SSL methods. We select GraphCL [ 40], InfoGraph [ 26], MV-
GRL [11], and JOAO [39] as pre-trained SSL models.
Evaluation protocol. We select GIN [ 34] as the GNN model for all
the evaluated methods, and report the graph classification results
derived from training conducted on small-scale synthetic graphs.
We report the average and standard deviation of the results from 10
independent training trials. Following previous methods [ 36], weuse the Accuracy (ACC) to evaluate NCI1, NCI109, DD and PRO-
TEINS, and assess ogbg-molhiv, ogbg-molbbbp, and ogbg-molbace
according to official recommended ROC-AUC.
Parameter settings. We adopt GraphCL [ 40] as the pre-training
SSL model that comprises 3 GIN [ 34] layers, delivering a graph
representation output of dimension 64 (expanded to 128 for the OGB
datasets). In the process of graph condensation, the Adam optimizer
is employed and the learning rate is set to 0.001. The dimension
of the synthetic graph aligns with the output dimension of the
aforementioned pre-training model. For GNTK and ANTK, the
number of GNN layers and activation layers are 3 and 1 respectively.
Moreover, during the testing phase, for NCI1, NCI109, DD, and
PROTEINS, we train an MLP to predict graph labels, with a learning
rate of 0.001 and a span of 100 epochs. For ogbg-molhiv, ogbg-
molbbbp, and ogbg-molbace, we employ Logistic Regression as our
graph classifier from the scikit-learn library [23].
5.2 Main Results
Classification performance (RQ1). To verify the efficacy of the
proposed models, we evaluate the classification performance of
GIN trained on synthetic graphs. In the paper, we use GIN [ 34] as
the default GNN model because it yields the best accuracy among
most existing works [ 2,11,35]. All results are reported in Table
3.ACC results are depicted in percentage (%). N_Syn represents
synthetic graph quantities. Ratio indicates the condensation ratio.
3295KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxiang Wang, Xiao Yan, Shiyu Jin, Hao Huang, Quanqing Xu, Qingchen Zhang, Bo Du, and Jiawei Jiang
/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017 /uni00000018/uni00000019/uni00000018/uni0000001c/uni00000019/uni00000015/uni00000019/uni00000018/uni00000019/uni0000001b/uni0000001a/uni00000014/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000000b/uni00000008/uni0000000c
/uni00000015/uni00000014/uni00000015/uni00000011/uni0000001b/uni0000001c/uni0000001b/uni00000011/uni0000001a/uni00000015/uni00000014/uni00000011/uni00000015/uni00000015/uni00000016/uni00000011/uni0000001a
/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017 /uni00000018/uni00000019/uni00000018/uni0000001c/uni00000019/uni00000015/uni00000019/uni00000018/uni00000019/uni0000001b/uni0000001a/uni00000014
/uni00000017/uni00000016/uni0000001c/uni00000011/uni00000019/uni00000014/uni00000015/uni0000001c/uni00000011/uni0000001c/uni00000015/uni00000014/uni00000011/uni0000001b/uni00000015/uni00000017/uni00000011/uni0000001c/uni00000027/uni00000052/uni00000056/uni00000026/uni00000052/uni00000051/uni00000047 /uni0000002e/uni0000004c/uni00000027/uni00000027 /uni00000036/uni0000002aDC /uni00000036/uni0000002aDC/uni00000010/uni00000024/uni00000057/uni00000057
/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017 /uni00000018/uni00000019/uni00000018/uni0000001c/uni00000019/uni00000015/uni00000019/uni00000018/uni00000019/uni0000001b/uni0000001a/uni00000014
/uni00000014/uni00000014/uni00000015/uni00000016/uni00000011/uni00000019/uni00000015/uni00000013/uni00000016/uni00000011/uni00000018/uni00000016/uni0000001b/uni00000011/uni00000015
/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017
/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c/uni00000017/uni0000001b/uni00000018/uni00000015/uni00000018/uni00000019/uni00000019/uni00000013/uni00000019/uni00000017/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000000b/uni00000008/uni0000000c/uni00000015/uni00000016/uni0000001b/uni00000011/uni00000016
/uni0000001b/uni00000019/uni00000011/uni0000001a/uni00000014/uni0000001a/uni00000011/uni0000001b
/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017
/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c/uni00000017/uni0000001b/uni00000018/uni00000015/uni00000018/uni00000019/uni00000019/uni00000013/uni00000019/uni00000017
/uni00000017/uni00000015/uni0000001a/uni00000011/uni00000016
/uni00000014/uni00000015/uni0000001824.4
/uni00000014/uni00000013/uni00000014/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni00000014/uni00000013/uni00000017
/uni00000037/uni0000004c/uni00000050/uni00000048/uni0000000b/uni00000056/uni0000000c/uni00000019/uni00000015/uni00000019/uni00000018/uni00000019/uni0000001b/uni0000001a/uni00000014
/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000011/uni0000001790.6/uni00000016/uni00000013/uni00000018/uni00000011/uni00000018/uni0000001b/uni00000014/uni00000011/uni0000001c/uni00000015/uni00000014/uni00000011/uni0000001a
/uni00000014/uni0000001a/uni00000011/uni0000001443.5N_Syn=2 N_Syn=2 0 N_Syn= 100
N_Syn=2 N_Syn=2 0
N_Syn= 100
Figure 4: Running time vs. Accuracy performance. The spe-
cific numbers of running time are marked next to the points.
Whole denotes the classification results of GraphCL trained on the
original dataset. We have the following observations:
Firstly, the proposed SGDC andSGDC-Att significantly outper-
form baseline approaches on 6 out of the 7 datasets. Compared to
the runner-up baseline method KiDD, our method improves the
performance by 9.7%, 6.8%, and 3.8% across 3 different synthetic
graph sizes on ogbg-molhiv. Notably, with only 2 synthetic graphs,
we achieve 93.4% of the performance attained when trained on
the original ogbg-molbbbp. This demonstrates the ability of our
methods to effectively condense the original dataset with relatively
small information loss.
Secondly, we observe a correlation between the number of syn-
thetic graphs and classification performance — as the former num-
ber increases, so does the latter performance, if not all, only drops
slightly. Aggregating more synthetic graphs means that more in-
formation can be included for training, thereby the performance
can be naturally improved.
Finally, the classification performance on PROTEINS is relatively
low. This is because PROTEINS has the smallest node feature dimen-
sion among the datasets (i.e., 4), which makes graph classification
easy. As such, the GDC methods have small accuracy gaps. This
is evidenced by the fact that simple methods not tailored for GDC
(i.e., Random, Herding, and K-Center) also achieve high accuracy
on PROTEINS while their accuracy is much lower on the other
datasets (compared to methods tailored for GDC, e.g., KiDD).
Efficiency study (RQ2). We evaluate the efficiency of SGDC and
SGDC-Att by comparing them with DosCond and KiDD on NCI1
and NCI109. For fairness, we set the dimensions of the synthetic
graphs generated by all the methods to 64, the number of GIN
layers to 3, and each layer includes 1 activation layer. The results of
running time versus accuracy performance for 3 different numbers
of synthetic graphs are summarized in Figure 4. The three plots
above represent the results on NCI1, while the three plots below
depict the results on NCI09. We have the following observations:
Firstly, SGDC andSGDC-Att are more efficient than KiDD and
DosCond. Taking SGDC as an example, it is on average 5.3 ×and
19.9×faster than KiDD and DosCond on NCI1, respectively, and
101102103104105
/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000052/uni0000004a/uni00000045/uni0000004a/uni00000010/uni00000003/uni00000003
/uni00000050/uni00000052/uni0000004f/uni0000004b/uni0000004c/uni00000059/uni00000052/uni0000004a/uni00000045/uni0000004a/uni00000010/uni00000003/uni00000003
/uni00000050/uni00000052/uni0000004f/uni00000045/uni00000045/uni00000045/uni00000053/uni00000052/uni0000004a/uni00000045/uni0000004a/uni00000010/uni00000003/uni00000003
/uni00000050/uni00000052/uni0000004f/uni00000045/uni00000044/uni00000046/uni00000048
/uni00000019/uni00000017/uni00000014/uni00000016/uni00000011/uni00000017/uni00000015/uni00000017/uni00000015/uni00000015/uni00000011/uni0000001b
/uni00000014/uni00000016/uni00000017/uni00000018/uni00000011/uni0000001b/uni00000014/uni00000015/uni0000001b/uni00000019/uni00000011/uni0000001b
/uni00000015/uni0000001a/uni00000019/uni00000014/uni00000011/uni00000015
   /uni00000015/uni00000015/uni0000001b/uni00000013  
   /uni00000015/uni00000014/uni00000019/uni0000001a/uni00000011/uni00000016/uni00000014/uni00000016/uni00000015/uni0000001c/uni00000011/uni00000018/uni00000014/uni00000015/uni0000001a/uni00000014/uni00000011/uni00000019/uni00000046/uni00000052/uni00000051/uni00000047/uni00000048/uni00000051/uni00000056/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051  
/uni00000052/uni0000004a/uni00000045/uni0000004a/uni00000010/uni00000003
/uni00000050/uni00000052/uni0000004f/uni0000004b/uni0000004c/uni00000059/uni00000052/uni0000004a/uni00000045/uni0000004a/uni00000010/uni00000003
/uni00000050/uni00000052/uni0000004f/uni00000045/uni00000045/uni00000045/uni00000053/uni00000052/uni0000004a/uni00000045/uni0000004a/uni00000010/uni00000003
/uni00000050/uni00000052/uni0000004f/uni00000045/uni00000044/uni00000046/uni000000480.600.650.700.750.800.85 /uni00000035/uni00000032/uni00000026/uni00000010/uni00000024/uni00000038/uni00000026/uni00000013/uni00000011/uni0000001a/uni00000016/uni00000014
/uni00000013/uni00000011/uni00000019/uni00000018/uni00000015/uni00000013/uni00000011/uni0000001a/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000014/uni0000001a
/uni00000013/uni00000011/uni0000001a/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001b/uni00000015/uni00000018
 /uni00000013/uni00000011/uni0000001a/uni0000001a/uni0000001a0/uni00000011/uni0000001a/uni0000001b/uni0000001c/uni0000002c/uni00000051/uni00000049/uni00000052/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b /uni00000036/uni0000002a/uni00000027/uni00000026 /uni00000036/uni0000002a/uni00000027/uni00000026/uni00000010/uni00000024/uni00000057/uni00000057
0/uni00000011/uni0000001a/uni0000001b/uni0000001c/uni0000004b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni00000056/uni00000048/uni00000044/uni00000055/uni00000046/uni0000004b /uni00000053/uni00000055/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000050/uni00000052/uni00000047/uni00000048/uni0000004f  /uni00000047/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057 /uni00000003Figure 5: The comparison of hyperparameter search runtime
and ROC-AUC for SGDC, SGDC-Att and InfoGraph.
correspondingly 4.9 ×and 17.8×faster on NCI109. SGDC andSGDC-
Att achieve better accuracy performance than KiDD and DosCond
in less time. DosCond requires intensive computations for gradient
matching in bilevel optimization. KiDD reduces time costs by trans-
forming bilevel optimization into single-level optimization via KRR,
however, it is supervised and still needs to additionally compute
the adjacency matrix of synthetic graph. In contrast, SGDC benefits
from the self-supervised KRR, and is efficient with adjacency matrix
reusing when condensing synthetic graph.
Secondly, SGDC-Att needs slightly longer running time than
SGDC. This is because ANTK requires an extra 𝑂(𝑁2)processing
time to compute the attention matrix, where 𝑁is the number of
nodes. Moreover, the running time naturally increases with an
increment in the number of synthetic graphs across all methods.
5.3 Auxiliary Experiments
Practical application (RQ3). To demonstrate the practical value
ofSGDC andSGDC-Att, we choose hyperparameter search as an
example application and investigate the performance of our meth-
ods and InfoGraph [ 26]. We consider a set 𝐻of 27 hyperparameter
groups, consisting of 3 hyperparameters: 𝑙𝑎𝑦𝑒𝑟∈{2,3,4}(the num-
ber of GNN layers), 𝑑𝑖𝑚∈{32,64,128}(the dimension of output
representation), and the augmentation strategy from {drop nodes,
permute edge, mask node }. Each combination from 𝐻is tested, and
we report the total running time and the best ROC-AUC scores
for our method and InfoGraph on ogbg-molhiv, ogbg-molbbbp and
ogbg-molbace in Figure 5. Following prior works [ 13], the original
dataset is divided into training and testing sets at a ratio of 8:2.
For each hyperparameter group in 𝐻, we train InfoGraph on the
original training set for 200 epochs, which is sufficient to achieve
decent performance for graph classification. Then, we evaluate
the model performance on the testing set. Our methods comprise
three components: the pre-trained model, dataset condensation,
and hyperparameter search. We train the pre-trained model (i.e.
InfoGraph) with 10 epochs to generate the target representation.
After the dataset condensation, we conduct hyperparameter search
on the synthetic graph for 10 epochs and then assess the classi-
fication performance on the testing set. We repeat this process
independently 27 times and report the running time and the best
ROC-AUC results for InfoGraph and our methods, respectively.
We observe that our methods outperform InfoGraph in terms
of speed and model quality. For instance, SGDC is 3.3×faster
3296Self-Supervised Learning for Graph Dataset Condensation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: The ablation study results for NCI1, DD and ogbg-
molbbbp. The best results are in bold.
MethodNCI1
(ACC)DD
(ACC)ogbg-molbbbp
(ROC-AUC)
SGDC-CL
(Initial design)59.8±0.7 66.5±0.8 0.597±0.017
SGDC-RM
(+RM)60.1±0.5 67.3±0.5 0.611±0.010
SGDC
(+KRR, AMR)63.8±0.3 69.1±0.2 0.638±0.004
SGDC-Att
(+ANTK)64.5±0.4 72.3±0.1 0.647±0.009
Table 5: The impact of different pre-trained models on the
classification performance of synthetic graph.
Methodogbg-molhiv ogbg-molbbbp ogbg-molbace
synthetic whole synthetic whole synthetic whole
InfoGraph 0.777 0.731 0.817 0.652 0.777 0.777
MVGRL 0.778 0.735 0.808 0.64 0.782 0.791
JOAO 0.78 0.763 0.819 0.687 0.779 0.777
than InfoGraph, and it also yields a 3% increase in ROC-AUC. This
improvement is attributed to the utilization of small-scale synthetic
graphs, the continuous optimization of node features during the
condensation process, and the design of adjacency matrix reusing.
Ablation study (RQ4). We conduct the ablation study by adding
the 3 key designs of SGDC one by one and report the experiment
results in Table 4. Specifically, SGDC-CL is the initial design based
on gradient matching; SGDC-RM uses representation matching to
solve the biased gradient problem of SGDC-CL; SGDC adopts kernel
ridge regression (KRR) to simplify SGDC-RM from a bilevel opti-
mization problem to a single-level optimization problem, and uses
adjacency matrix reusing (AMR) to avoid repeatedly optimizing the
adjacency matrix in each condensation step. SGDC-Att integrates
the attention-based graph kernel matrix (i.e., ANTK). The results
on all three datasets suggest that all the 3 key designs are effective
in improving accuracy.
We find that the performance of the two variants drops signif-
icantly. This is because optimizing graph features and structure
simultaneously leads to model instability, and the graph structure
learner also introduces new noise. In contrast, our method elimi-
nates the need to repeatedly optimize the adjacency matrix in each
condensation step. Instead, we learn structural information in the
target representation implicitly during graph condensation. The
performance of SGDC-Att is the best on two of the three datasets,
indicating that ANTK assists the model extract more effective struc-
ture information through the attention mechanism.
The impact of pre-trained models (RQ5). We use the pre-trained
SSL model to generate target representations. This necessitates an
study of the impact that pre-trained models may have on the classifi-
cation performance of synthetic graphs. To facilitate this evaluation,we choose three state-of-the-art graph contrastive learning algo-
rithms as our pre-trained SSL models. Subsequently, we input the
synthetic graph into the contrastive learning pipeline. Specifically,
we employ data augmentation on the synthetic graph to generate
multiple corrupted views and calculate the contrastive loss between
these views. We evaluate the synthetic graphs on the ogbg-molhiv,
ogbg-molbbbp, and ogbg-molbace datasets. The term Whole indi-
cates the classification results of pre-trained models trained on the
original training set. The results are shown in Table 5.
During the process of dataset condensation, our objective is to
minimize the MSE between the representations and the synthetic
graph. This leads to an inevitable loss of information, which hin-
ders the synthetic graph’s performance from surpassing that of the
pre-trained model (as shown in Table 3). However, when we inte-
grate the synthetic graph into a real contrastive learning pipeline,
we observe significantly improved performance of models trained
on the synthetic graph compared to those trained on the original
training set (except MVGRL on ogbg-molbace). This observation
verifies the effective condensation of graph features and structural
information in the synthetic graphs generated by our method.
6 CONCLUSION
This paper introduces SGDC, a method that effectively condenses
large-scale graph datasets into smaller synthetic graphs without
requiring ground-truth labels. Our method involves minimizing
the difference between synthetic data and the target representa-
tion derived from a pre-trained self-supervised learning model. To
optimize SGDC, we construct a kernel ridge regression (KRR) prob-
lem and propose a graph attention neural tangent kernel for KRR.
Experimental results demonstrate the efficiency and superior per-
formance improvements of SGDC over state-of-the-art baselines.
7 ACKNOWLEDGMENT
This work was sponsored by National Science and Technology
Major Project (No. 2022ZD0116315), Key R&D Program of Hubei
Province (No. 2023BAB077), and the Fundamental Research Funds
for the Central Universities (No. 2042023kf0219). This work was
supported by Ant Group through CCF-Ant Research Fund (CCF-
AFSG RF20220001).
REFERENCES
[1]Mikhail Belkin and Partha Niyogi. 2003. Laplacian eigenmaps for dimensionality
reduction and data representation. Neural Computation 15, 6 (2003), 1373–1396.
[2]Chen Cai, Dingkang Wang, and Yusu Wang. 2021. Graph coarsening with neural
networks. arXiv preprint arXiv:2102.01350 (2021).
[3]George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and
Jun-Yan Zhu. 2022. Dataset distillation by matching training trajectories. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .
4750–4759.
[4]Justin Domke. 2012. Generic methods for optimization-based modeling. In Artifi-
cial Intelligence and Statistics. PMLR, 318–326.
[5]Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong
Wang, and Keyulu Xu. 2019. Graph neural tangent kernel: Fusing graph neural
networks with graph kernels. Advances in Neural Information Processing Systems
32 (2019).
[6]Reza Zanjirani Farahani and Masoud Hekmatfar. 2009. Facility location: concepts,
models, algorithms and case studies. Springer Science & Business Media.
[7]Jian Gao and Jianshe Wu. 2023. Multiple sparse graphs condensation. Knowledge-
Based Systems 278 (2023), 110904.
[8]Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. 2021. Graph neural
architecture search. In International Joint Conference on Artificial Intelligence.
International Joint Conference on Artificial Intelligence.
3297KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxiang Wang, Xiao Yan, Shiyu Jin, Hao Huang, Quanqing Xu, Qingchen Zhang, Bo Du, and Jiawei Jiang
[9]Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang,
and Nitesh V Chawla. 2021. Few-shot graph learning for molecular property
prediction. In Proceedings of the Web Conference 2021. 2559–2567.
[10] Mridul Gupta, Sahil Manchanda, Sayan Ranu, and Hariprasad Kodamana. 2023.
Mirage: model-agnostic graph distillation for graph classification. arXiv preprint
arXiv:2310.09486 (2023).
[11] Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive multi-view rep-
resentation learning on graphs. In International Conference on Machine Learning.
PMLR, 4116–4126.
[12] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. 2020. In-
finite attention: NNGP and NTK for deep attention networks. In International
Conference on Machine Learning. PMLR, 4376–4386.
[13] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: datasets for
machine learning on graphs. Advances in Neural Information Processing Systems
33 (2020), 22118–22133.
[14] Wei Jin, Xianfeng Tang, Haoming Jiang, Zheng Li, Danqing Zhang, Jiliang Tang,
and Bing Yin. 2022. Condensing graphs via one-step gradient matching. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 720–730.
[15] Wei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil
Shah. 2021. Graph condensation for graph neural networks. arXiv preprint
arXiv:2110.07580 (2021).
[16] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[17] Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei Ma, Fangli Xu,
Alex X Liu, Chunming Wu, and Shouling Ji. 2021. Deep graph matching and
searching for semantic code retrieval. ACM Transactions on Knowledge Discovery
from Data 15, 5 (2021), 1–21.
[18] Mengyang Liu, Shanchuan Li, Xinshi Chen, and Le Song. 2022. Graph conden-
sation via receptive field distribution matching. arXiv preprint arXiv:2206.13697
(2022).
[19] Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and Yang Liu. 2020. Retrieval-
augmented generation for code summarization via hybrid GNN. arXiv preprint
arXiv:2006.05405 (2020).
[20] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel,
and Marion Neumann. 2020. TUDataset: a collection of benchmark datasets for
learning with graphs. arXiv preprint arXiv:2007.08663 (2020).
[21] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric
Lenssen, Martin Grohe, and Kristian Kersting. 2016. Faster kernel ridge regression
using sketching. In International Conference on Machine Learning. 1178–1186.
[22] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. 2020. Dataset meta-learning
from kernel ridge-regression. arXiv preprint arXiv:2011.00050 (2020).
[23] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[24] Manon Réau, Nicolas Renaud, Li C Xue, and Alexandre MJJ Bonvin. 2023.
DeepRank-GNN: a graph neural network framework to learn patterns in protein–
protein interfaces. Bioinformatics 39, 1 (2023), btac759.
[25] Ozan Sener and Silvio Savarese. 2017. Active learning for convolutional neural
networks: A core-set approach. arXiv preprint arXiv:1708.00489 (2017).
[26] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2019. Infograph:
unsupervised and semi-supervised graph-level representation learning via mutual
information maximization. arXiv preprint arXiv:1908.01000 (2019).
[27] Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. 2014. Learning deep
representations for graph clustering. In Proceedings of the AAAI Conference onArtificial Intelligence, Vol. 28.
[28] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, Yoshua Bengio, et al .2017. Graph attention networks. Stat1050, 20 (2017),
10–48550.
[29] Petar Velivckovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2018. Graph attention networks. In International Confer-
ence on Learning Representations.
[30] SVN Vishwanathan, Nicol N Schraudolph, and Risi Kondor. 2010. Graph kernels.
Journal of Machine Learning Research 11 (2010), 1201–1242.
[31] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu.
2019. Heterogeneous graph attention network. In The World Wide Web Conference.
2022–2032.
[32] Max Welling. 2009. Herding dynamical weights to learn. In Proceedings of the
26th Annual International Conference on Machine Learning. 1121–1128.
[33] Hongzhi Wen, Jiayuan Ding, Wei Jin, Yiqi Wang, Yuying Xie, and Jiliang Tang.
2022. Graph neural networks for multimodal single-cell data integration. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 4153–4163.
[34] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. Graph iso-
morphism networks. In International Conference on Machine Learning. PMLR,
5645–5654.
[36] Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, Hao
Yang, and Hanghang Tong. 2023. Kernel ridge regression-based graph dataset
distillation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 2850–2861.
[37] Pinar Yanardag and SVN Vishwanathan. 2015. Deep graph kernels. In Proceedings
of the 21st ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining. ACM, 1365–1374.
[38] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
recommender systems. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. 974–983.
[39] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. 2021. Graph
contrastive learning automated. In International Conference on Machine Learning.
PMLR, 12121–12132.
[40] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in
Neural Information Processing Systems 33 (2020), 5812–5823.
[41] Chris Zhang, Mengye Ren, and Raquel Urtasun. 2018. Graph hypernetworks for
neural architecture search. arXiv preprint arXiv:1810.05749 (2018).
[42] Bo Zhao and Hakan Bilen. 2021. Dataset condensation with differentiable siamese
augmentation. In International Conference on Machine Learning. PMLR, 12674–
12685.
[43] Bo Zhao and Hakan Bilen. 2023. Dataset condensation with distribution matching.
InProceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision. 6514–6523.
[44] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. Dataset condensation
with gradient matching. arXiv preprint arXiv:2006.05929 (2020).
[45] Xin Zheng, Miao Zhang, Chunyang Chen, Quoc Viet Hung Nguyen, Xingquan
Zhu, and Shirui Pan. 2023. Structure-free graph condensation: from large-scale
graphs to condensed graph-free data. arXiv preprint arXiv:2306.02664 (2023).
[46] Kaixiong Zhou, Xiao Huang, Qingquan Song, Rui Chen, and Xia Hu. 2022. Auto-
gnn: neural architecture search of graph neural networks. Frontiers in Big Data 5
(2022), 1029307.
3298