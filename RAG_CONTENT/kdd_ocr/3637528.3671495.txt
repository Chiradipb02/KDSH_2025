KiL 2024: 4th International Workshop on Knowledge-infused
Learning
(Towards Consistent, Reliable, Explainable, and Safe LLMs)
Manas Gaur
University of Maryland Baltimore
County
Baltimore, Maryland, United States
manas@umbc.eduEfthymia Tsamoura
Samsung AI Research
Cambridge, United Kingdom
efi.tsamoura@samsung.comEdward Raff
Booz Allen Hamilton
Baltimore, Maryland, United States
Raff_Edward@bah.com
Nikhita Vedula
Amazon Inc.
Seattle, Washington, United States
veduln@amazon.comSrinivasan Parthasarathy
Ohio State University
Columbus, Ohio, United States
srini@cse.ohio-state.edu
ABSTRACT
The Knowledge-infused Learning Workshop is a recurring event
in ACM’s KDD Conference that gathers the research community
on knowledge graphs and knowledge-enabled learning, grounded
neurosymbolic AI, explainable and safe AI, and applications in high-
stakes decision-making problems. This year, the workshop aligned
with Biden’s vision of Responsible AI Development [1].
KEYWORDS
Neurosymbolic AI, Knowledge-infused Learning, Large Language
Models, Trustworthy AI, Safe AI, Evaluation Metrics, Social Good
ACM Reference Format:
Manas Gaur, Efthymia Tsamoura, Edward Raff, Nikhita Vedula, and Srini-
vasan Parthasarathy. 2024. KiL 2024: 4th International Workshop on Knowledge-
infused Learning (Towards Consistent, Reliable, Explainable, and Safe LLMs) .
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 2 pages. https://doi.org/10.1145/3637528.3671495
1 MOTIVATION AND OBJECTIVES
The advent of ChatGPT on November 30, 2022, marked a pivotal
moment in AI, garnering both attention and criticism. This mile-
stone catalyzed the development of other models like Google BARD,
Google GEMINI, and Meta LLAMA, collectively called Large Lan-
guage Models (LLMs) [ 5]. Despite their remarkable capabilities,
these models have faced significant challenges, notably providing
confidently asserted but factually inaccurate information, known
ashallucinations [10]. Hallucinations undermine their reliability
and trustworthiness, compounded by their propensity to deliver
inconsistent answers and irrelevant explanations.
Addressing these issues is crucial for enhancing the applicability
of LLMs. Researchers from the Natural Language Processing (NLP),
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671495the Artificial Intelligence (AI), and the database communities are
actively developing diverse approaches to mitigate these pitfalls.
Notable among these is the Retrieval Augmentation Generation, ex-
emplified by Facebook AI’s Semantic Search, which aims to improve
text generation accuracy [ 4]. However, the widespread adoption of
such solutions remains a formidable challenge. This workshop aims
to accelerate efforts at the intersection of Symbolic Knowledge and
Statistical Knowledge in LLMs [9].
2 WORKSHOP FOCUS
The workshop will address key topics to improve the performance
and trustworthiness of LLMs. The primary focus is enhancing LLMs’
consistency, reliability, explainability, and safety, developing new
methods and metrics for consistent outputs, and adapting safety cer-
tification frameworks for sensitive fields like health, cybersecurity,
and law [ 2]. We will explore general-purpose and domain-specific
knowledge to enhance LLMs’ decision-making, including proce-
dural and declarative types. This involves comparing human and
machine information processing, focusing on abstraction, linguis-
tic structures, and generalization. Innovative interpretability and
transparency methods will be discussed, especially in healthcare,
to ensure comprehensive contextual understanding [6].
Additionally, neuroscience and computer vision methodologies
will also be applied to assess LLMs and foundational models. The
workshop will explore using LLMs and generative AI for health-
care and well-being, developing open-source tools for analysis and
visualization, and creating robust safety metrics to protect against
adversarial attacks. Reliable metrics for hallucination [ 8] and real-
world deployment experiences will be shared for valuable insights.
Our theme, neurosymbolic AI and knowledge-infused learning,
involves using retrieval-augmented LLMs in health, legal, and crisis
domains, advancing knowledge-infused reinforcement learning,
and developing knowledge-injected foundational models. Automat-
ing rule learning and inference in various domains aims to enhance
LLMs’ decision-making capabilities. Promoting user-explainable
machine learning ensures transparency and understandability. En-
suring safety in conversational systems and addressing bias within
deep learning fosters fair AI systems [ 3]. Enhancing user controlla-
bility through rules, constraints, and guidelines empowers the safe
and effective use of LLMs.
6712
KDD ’24, August 25–29, 2024, Barcelona, Spain Manas Gaur, Efthymia Tsamoura, Edward Raff, Nikhita Vedula, & Srinivasan Parthasarathy
By addressing diverse and interconnected topics, the workshop
aims to develop technically advanced, reliable, safe LLMs that align
with the needs and values of application domains. This holistic
approach ensures that the advancements in LLMs will not only push
the boundaries of what is possible but also foster trust and utility
across different sectors, ultimately contributing to the broader goal
of integrating AI responsibly and effectively into society.
3 RELEVANCE AND TARGET AUDIENCE
The concept of “Knowledge-Infused Learning” (KiL) explores inte-
grating diverse forms of knowledge into data, methods, and evalua-
tions. KiL aims to introduce a new vertical within the KDD (Knowl-
edge Discovery and Data Mining) conference, where researchers
and practitioners can showcase research that leverages knowledge
graphs, guidelines, and traditional databases to enhance the func-
tionality of current state-of-the-art AI methods. This approach will
also facilitate the development of innovative assessment techniques
to evaluate the fairness and robustness of cutting-edge algorithms.
ACM SIGKDD, a leading conference on data mining and knowl-
edge discovery, has consistently been at the forefront of advance-
ments in Responsible AI. In 2016, it introduced the LIME (Local
Interpretable Model-agnostic Explanation) algorithm, which has
become a cornerstone for model explanations [ 7]. The conference
continues to provide practical solutions to complex challenges in
various domains, including healthcare, planning, crisis, and others.
We anticipate that the workshop will attract a diverse group of
attendees from the ACM SIGKDD community, particularly those in-
terested in human-centered computing, interaction and reasoning,
statistical relational mining and learning, intelligent agent systems,
semantic social network analysis, deep graph learning, and recom-
mendation systems. This workshop will be a pivotal platform for
discussing and advancing the integration of knowledge into AI,
fostering innovation, and addressing real-world challenges.
This workshop is tailored for a diverse and dynamic audience,
including:
•Researchers and Academics: Experts in machine learning, AI
safety, consistency, reliability, and trustworthiness will find
valuable insights and collaboration opportunities.
•Postgraduate Research Students and Early Career Researchers:
Individuals eager to contribute to neurosymbolic AI and
Knowledge-infused Learning for LLMs will benefit signifi-
cantly from the discussions and knowledge exchange.
•Practitioners and Professionals: Those involved in developing,
deploying, and certifying AI systems, especially within sec-
tors like healthcare, cybersecurity, finance, legal, and cyber-
physical systems, will gain practical knowledge and innova-
tive strategies.
•Regulatory Authorities and Policymakers: Officials seeking to
understand the current challenges in AI, particularly with
LLMs and associated technologies, and explore potential
solutions through neurosymbolic AI and Knowledge-infused
Learning.
The workshop aims to foster a collaborative environment where
experts from various fields can share their experiences, discuss
innovative solutions, and drive forward the development of reli-
able and robust AI systems. This inclusive approach ensures thatthe workshop addresses the needs and interests of a broad spec-
trum of stakeholders, promoting interdisciplinary collaboration
and knowledge sharing.
4 WORKSHOP FORMAT AND SCHEDULE
KiL 2024 is a half-day workshop consisting of the following agenda:
(a) 2 Keynote Talks (b) Invited Talks (c) Regular paper presentation
sessions and (d) Poster Sessions
5 PROGRAMME COMMITTEE MEMBERS
Reviewing: The review process is single-round and double-blind.
Accepted papers will be presented during the workshop through
oral presentations or poster sessions. They will be indexed in CEUR
workshop proceedings and listed on the workshop website (non-
archival).
Program Committee (PC): We are very grateful to each of our PC
members for their hard work and time in reviewing the submitted
papers and providing helpful reviews to authors. The PC members
are Ankur Padia, Manas Paldhe, Jiajun Xu, Mukuntha Narayanan
S, Ramana Malladi, Anish Dubey, Aritran Piplai, Aman Chaddha,
Ali Mohammadi, Dibyendu Roy Chowdhury, Aamir Hamid, Akash
Vartak, Anantaa Kotal, and Vidhushini Srinivasan.
Web and Publicity: We are very thankful to Deepa Tilwani (USC)
and Jinendra Malekar (USC) for their time and effort in publicizing
the workshop and maintaining its website.
ACKNOWLEDGEMENT
Haltia.ai generously supports the workshop. The opinions, conclu-
sions, or recommendations expressed are those of the authors and
do not necessarily reflect the views of Haltia.ai.
REFERENCES
[1]Joseph R Biden. 2023. Executive order on the safe, secure, and trustworthy
development and use of artificial intelligence. (2023).
[2]Manas Gaur and Amit Sheth. 2024. Building trustworthy NeuroSymbolic AI
Systems: Consistency, reliability, explainability, and safety. AI Magazine (2024).
[3]Priyanshul Govil, Vamshi Krishna Bonagiri, Manas Gaur, Ponnurangam Ku-
maraguru, and Sanorita Dey. 2024. COBIAS: Contextual Reliability in Bias
Assessment. arXiv preprint arXiv:2402.14889 (2024).
[4]Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
et al.2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.
Advances in Neural Information Processing Systems 33 (2020), 9459–9474.
[5]Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard
Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large Language Models: A
Survey.
[6]Hoifung Poon, Tristan Naumann, Sheng Zhang, and Javier González Hernández.
2023. Precision Health in the Age of Large Language Models. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
5825–5826.
[7]Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. " Why should i
trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and data mining.
1135–1144.
[8]Deepa Tilwani, Yash Saxena, Ali Mohammadi, Edward Raff, Amit Sheth, Srini-
vasan Parthasarathy, and Manas Gaur. 2024. REASONS: A benchmark for RE-
trieval and Automated citationS Of scieNtific Sentences using Public and Propri-
etary LLMs. arXiv preprint arXiv:2405.02228 (2024).
[9]Efthymia Tsamoura, Timothy Hospedales, and Loizos Michael. 2021. Neural-
symbolic integration: A compositional perspective. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 35. 5051–5060.
[10] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei
Bi, Freda Shi, and Shuming Shi. 2023. Siren’s Song in the AI Ocean: A Survey on
Hallucination in Large Language Models.
6713