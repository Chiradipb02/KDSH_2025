F
AST: An Optimization Framework for Fast Additive
Segmentation in Transparent ML
Brian Liu
Massachusetts Institute of Technology
Cambridge, Massachusetts, USA
briliu@mit.eduRahul Mazumder
Massachusetts Institute of Technology
Cambridge, Massachusetts, USA
rahulmaz@mit.edu
ABSTRACT
We present FAST, an optimization framework for fast additive seg-
mentation. FAST segments piecewise constant shape functions for
each feature in a dataset to produce transparent additive models.
Theframeworkleveragesanoveloptimizationproceduretofitthese
models ∼2ordersofmagnitudefasterthanexistingstate-of-the-art
methods, such as explainable boosting machines [20]. We also de-
velop new feature selection algorithms in the FAST framework to
fit parsimonious models that perform well. Through experiments
and case studies, we show that FAST improves the computational
efficiency and interpretability of additive models.
CCS CONCEPTS
•Computing methodologies →Machine learning .
KEYWORDS
Interpretable Machine Learning; Additive Models; Sparsity
ACM Reference Format:
Brian Liu and Rahul Mazumder. 2024. FAST: An Optimization Framework
for Fast Additive Segmentation in Transparent ML. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671996
1 INTRODUCTION
Additive models are popular in machine learning for balancing a
high degree of explainability with good predictive performance
[2,3].Thesemodels,whenfitonadatasetwith 𝑝features,takethe
form∑𝑝
𝑗=1𝑠𝑗(𝑥𝑗). Each additive component 𝑠𝑗is the shape func-
tionoffeature 𝑥𝑗,andsincethecontributionofeachfeaturecanbe
readily observed from its shape function, additive models are said
tobeinherentlytransparent.Onesuchadditivemodel,explainable
boosting machines (EBMs), combines this inherent transparency
with the powerful predictive performance of tree ensembles [20].
EBMs use single-feature decision trees, fit via a cyclic boosting
heuristic, to construct shape functions. As such, the shape func-
tions built are piecewise constant, a departure from classical and
popular smooth components such as those based on polynomials
orsplines[7].Usingpiecewiseconstantshapefunctions,EBMscan
capture discontinuities in the underlying data, patterns that are
This
work is licensed under a Creative Commons Attribu-
tion International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671996unobserved by smooth additive models and which often have real-
world significance [2, 14]. EBMs have also been shown to match
the predictive performance of black box methods in various ap-
plications while preserving model transparency [3]. Due to these
advantages, EBMs are rapidly becoming ubiquitous in high-stakes
applications of ML, such as criminal justice [3] and healthcare [2],
where model explainability is critical.
Inspired by the success of EBMs, and stemming from a reinter-
pretation of the method, we propose an alternative, FAST. FAST
is a formal optimization-based procedure to fit piecewise constant
additive models (PCAMs). Both methods construct piecewise con-
stant shape functions, but FAST does so by minimizing a regu-
larized optimization objective while EBMs use a cyclic boosting
heuristic.
Moreover, the main goal of FAST is to address the limitations
of EBMs that result from this cyclic boosting heuristic. Starting
from the null model, EBMs are fit by cycling round-robin over the
features and building single-feature decision trees on the boosted
residuals, which are dampened by a learning rate. To ensure that
theorderingofthefeaturesisirrelevant,thislearningratemustbe
kept small. As a result, many cyclic boosting iterations and trees
are required to fit an EBM that performs well. This increases the
complexity and computational cost of the algorithm and conse-
quently, EBMs struggle to scale for larger datasets. As a motivat-
ing example, consider the UK Black Smoke dataset (9 million rows
and 14 columns) used by [30] to test the computational feasibility
of splines. It takes the InterpretML package [20] nearly 4 hours
to fit an EBM using the default hyperparameters, which are opti-
mized for computation time. FAST, on the other hand, leverages a
specialized greedy optimization algorithm to fit a PCAM that per-
formsthesameintermsofaccuracyinunder 1minute .Thecyclic
heuristic used to fit EBMs also produces feature-dense models by
design. This may harm interpretability since an EBM fit on a high
dimensional dataset ( 𝑝>50features) will contain too many shape
functions for a practitioner to explain. FAST introduces two novel
feature selection algorithms to remedy this, and these new meth-
odsoutperformexistingfeature-sparsePCAMalgorithmsbyupto
a30%reduction in test error. We summarize the contributions of
our paper below.
Main Contributions
•We introduce FAST, an efficient optimization framework to fit
PCAMs that supports feature sparsity.
 
1863
KDD’24, August 25–29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
•FASTusesanovelproceduretoimprovecomputationalefficiency.
To solve optimization problems in FAST, we apply a computa-
tionally cheap greedy block selection rule to an implicit refor-
mulation of our original problem in order to guide a block coor-
dinatedescentalgorithm.ThisprocedurecanfitPCAMs2orders
of magnitude faster than existing SOTA methods.
•We introduce 2 new feature selection algorithms to build sparse
PCAMs,aniterativealgorithmthatreliesonourgreedyblockse-
lection rule and a group ℓ0-regularized optimization algorithm.
•We investigate how correlated features impact feature selection
andshapefunctionsinPCAMsanddiscussimplicationsformodel
trustworthiness.
WefirstdiscusstheadvantagesofPCAMsoversmoothadditive
models and overview existing algorithms to build PCAMs. Follow-
ingthesepreliminaries,weintroducetheFASToptimizationframe-
work (§2) and present its novelties: the greedy optimization proce-
dure used to accelerate computation (§3) and the feature selection
algorithms used to support feature sparsity (§4).
An open-source implementation of FAST along with an online
supplementcontainingproofsandexperimentaldetailscanbefound
in thisproject repository1.
1.1 Why PCAMs?
Comparedtosmoothadditivemodelssuchassplines,PCAMshave
the advantage that they are able to capture discontinuities in the
shape functions. These discontinuities can reveal interesting in-
sights about the underlying data. Consider the example shown in
Figure 1. The scatterplot shows the daily number of car accidents
in New York City over a 12-year period and there is a large jump
discontinuity in early 2020 due to the COVID-19 pandemic [23].
This discontinuity is captured by the shape function from a PCAM
(in blue) but is interpolated and obscured by the smoothing spline
(in orange).
2012 2014 2016 2018 2020 2022 2024
Date200400600800Daily Number of Car AccidentsNew York City Car Accidents
Smooth
PCAM
Figur
e 1: PCAM shape
functionscanbeusedto
uncover discontinuities
in the underlying data.
ThediscontinuitiesobservedinPCAMshapefunctionshavebeen
used to uncover hidden patterns in mortality risk models for ICU
patients[14]andpatientswithpneumonia[2].Thesepatternswould
have been difficult to detect with smooth additive models or black-
box methods. PCAMs also have the advantage that fitted piece-
wiseconstantshapefunctionscanberepresentedbyasetofbreak-
points.Asaresult,fittedPCAMsarestraightforwardtoproduction-
izeandcanbehard-codedintoanylanguagewithconditionalstate-
ments (e.g. SQL). Finally, PCAM predictions only require lookups
and addition so PCAMs are extremely fast at inference [20].
1https://github
.com/brianliu12437/FASTopt1.2 Existing PCAM Algorithms:
Asmentionedearlier,EBMsusesingle-featuredecisiontrees,fitvia
a cyclic boosting heuristic, to build PCAMs [18]. EBMs are inter-
pretable and perform well, but are slow to train and feature-dense
by design [20]. Besides EBMs, various methods have been used
to construct PCAMs. Additive isotonic models use isotonic regres-
sion with backfitting to build PCAMs with monotonic shape func-
tions [1]. Spline-based frameworks can also fit PCAMs using zero-
degreesplines[27].Morerecently,thefusedLASSOhasbeenused
tofitPCAMsviaADMM[4]orcyclicblockcoordinatedescent[25].
The latter approach is better known as the fused LASSO additive
model (FLAM) and is considered a SOTA algorithm for building
PCAMs. As such, we primarily compare FAST against FLAMs and
EBMs for fitting feature-dense PCAMs.
2 FAST OPTIMIZATION FRAMEWORK
WeintroduceFASTandoutlinetheoptimizationalgorithmusedto
solve problems in our framework. More importantly, we motivate
why our greedy optimization procedure (§3) improves efficiency.
Given data matrix 𝑋∈R𝑛×𝑝and target vector 𝑦∈R𝑛, our goal
is to fit additive model∑𝑝
𝑗=1𝑠𝑗(𝑥𝑗), where each shape function 𝑠𝑗
is piecewise constant. To accomplish this, we introduce a decision
variable for each entry in 𝑋. These decision variables are grouped
into decision vectors 𝛽𝑗∈R𝑛for𝑗∈ [𝑝], where each decision
vector𝛽𝑗representstheblockofdecisionvariablesthatcorrespond
to feature𝑥𝑗. The decision variables in 𝛽𝑗are ordered with respect
to the sorted values of 𝑥𝑗and the sum of decision vectors gives
the prediction of our model. Wefit this prediction to 𝑦and recover
shape functions 𝑠𝑗from the fitted decision vectors 𝛽∗
𝑗.
2.1 Optimization Problem
Let𝛽denote the set of decision vectors {𝛽1...𝛽 𝑝}. FAST mini-
mizes the objective 𝐿(𝑦,𝛽) +𝑆(𝛽)to fit PCAMs, where 𝐿is a loss
functionthatcapturesdatafidelityand 𝑆isasegmentationpenalty
that encourages piecewise constant segmentation in the fitted de-
cision vectors. The optimization problem can be written as:
min
𝛽1,...,𝛽 𝑝1
2∥𝑦−𝑝∑
𝑗=1𝑄⊺
𝑗𝛽𝑗∥2
2+𝜆𝑓𝑝∑
𝑗=1∥𝐷
𝛽𝑗∥1.(1)
Thefirsttermintheobjectiveisquadraticloss,where 𝑄𝑗∈ {0,1}𝑛×𝑛
isthesquaresortingmatrixforfeature 𝑥𝑗.Inotherwords, 𝑄𝑗𝑥𝑗re-
turnstheelementsof 𝑥𝑗sortedinascendingorderand 𝑄⊺
𝑗(𝑄𝑗𝑥𝑗)=
𝑥𝑗. Since each decision vector 𝛽𝑗is ordered with respect to the
sorted values of 𝑥𝑗,∑𝑝
𝑗=1𝑄⊺
𝑗𝛽𝑗gives the prediction of our model.
The second term in the objective is the fused LASSO segmentation
penalty, where 𝜆𝑓is the parameter that controls the number of
piecewiseconstantsegmentsintheshapefunctions.Highervalues
of𝜆𝑓result in less flexible shape functions with fewer segments.
Matrix𝐷∈ {− 1,0,1}(𝑛−1)×𝑛isthedifferencingmatrix,where 𝐷𝛽𝑗
returns a vector of the successive differences of 𝛽𝑗.
Problem1fitsfeature-densePCAMs.Anoptionalgroupsparsity
constraint can applied over the blocks 𝛽𝑗to select features and we
discuss this further in §4.
 
1864FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD ’24, August 25–29, 2024, Barcelona, Spain.
2.2 Optimization Algorithm
Problem 1 is convex and separable over blocks 𝛽𝑗; we develop a
block coordinate descent (BCD) algorithm to solve this problem to
optimality. Our algorithm has two components: block selections
and block updates, and starting with all blocks 𝛽𝑗=0we alternate
between the two until convergence.
Block Updates: It is critical to note that block updates in FAST
areexpensive.Foraselectedblock 𝑘,let𝛿={1...𝑝}\𝑘anddefine
residual vector 𝑟=𝑦−∑
𝑗∈𝛿𝑄⊺
𝑗𝛽𝑗. Each block update solves:
min
𝛽𝑘1
2∥𝑟−𝑄⊺
𝑘𝛽𝑘∥2
2+𝜆𝑓∥𝐷
𝛽𝑘∥1, (2)
whichisequivalenttoafusedLASSOsignalapproximation(FLSA)
problem on 𝑄𝑘𝑟. These FLSA problems are solved using dynamic
programming [11] which is computationally expensive.
Block Selections: Since block updates are expensive, improv-
ingtheefficiencyofourBCDalgorithmreliesonreducingthenum-
berofblockupdatesthatweconduct.Todoso,wetrytoselectthe
block that makes the most progress towards the optimal solution
in each BCD iteration. Other selection rules, such as cyclic or ran-
domizedselection[22],bottleneckBCDwithunnecessaryupdates.
We also must select blocks cheaply since block selection would
be ineffective if the cost of selecting the best block to update is
similar to the cost of updating all blocks. One novelty in FAST is
that we develop a greedy optimization procedure to select blocks
extremely efficiently. We present this procedure below.
3 GREEDY OPTIMIZATION PROCEDURE
Our greedy optimization procedure hinges on the fact that we can
transform Problem 1 into an equivalent LASSO problem with 𝑛
rows and (𝑛−1)𝑝variables. While many LASSO algorithms exist
[6], it is infeasible to solve this problem directly since there are
too many variables when 𝑛is large and the variables are heavily
correlated by design [26]. Rather, we use this LASSO reformulated
problem to guide block selection when we apply BCD to Problem
1.
Importantly, we exploit the structure of the design matrix in
our LASSO reformulation to derive an extremely efficient block
selection rule. In fact, our block selection rule only requires an im-
plicit LASSO reformulation of the original problem (Problem 1),
where the design matrix is not explicitly constructed. This is cru-
cialsinceconstructingthedesignmatrixrequiresaspacecomplex-
ity of𝑂(𝑛2𝑝), which is infeasible for large data. For example, the
designmatrixfortheUKBlackSmokeproblem(9millionrowsand
14columns)mentionedintheintroductionwouldtakeover 109TB
of memory if explicitly constructed.
3.1 Implicit LASSO Reformulation
We define a new set of decision vectors 𝜃𝑗∈R𝑛−1for𝑗∈ [𝑝],
where each vector 𝜃𝑗contains the successive differences of vector
𝛽𝑗.Let𝐴∈ {0,1}𝑛×(𝑛−1)beapaddedlowertriangularmatrixwith
zeros in the first row. We first reformulate Problem 1 as:
min
𝜃1,...,𝜃 𝑝1
2∥𝑦−𝑝∑
𝑗=1𝑄⊺
𝑗𝐴
𝜃𝑗∥2
2+𝜆𝑓𝑝∑
𝑗=1∥𝜃𝑗∥1.(3)Let𝜃∈R(𝑛−1)𝑝represent the decision vectors {𝜃1...𝜃 𝑝}ver-
tically stacked. Let 𝐴′∈ {0,1}𝑛𝑝×(𝑛−1)𝑝be the matrix formed
by stacking 𝐴submatrices 𝑝times along the main diagonal. Let
𝑄⊺∈ {0,1}𝑛𝑝×𝑛𝑝be the matrix formed by stacking {𝑄⊺
1...𝑄⊺
𝑝}
along the main diagonal. Finally, let 𝑀∈ {0,1}𝑛×𝑛𝑝be the matrix
formedbystacking 𝑝identitymatricesofdimension 𝑛×𝑛horizon-
tally. We show a visualization of these matrices in the appendix
(suppl. A). Problem 3 and Problem 1 are equivalent to:
min
𝜃1
2∥𝑦−𝑀
𝑄⊺𝐴′𝜃∥2
2+𝜆𝑓∥𝜃∥1, (4)
whichisablock-separableLASSOproblemwithdesignmatrix 𝑀𝑄⊺𝐴′∈
R𝑛×(𝑛−1)𝑝. We show in the next section that we do not need to
construct this matrix for our greedy selection rule.
3.2 Block Selection Rule (BGS rule)
Since Problem 4 is an equivalent LASSO reformulation of Problem
1,weusethisreformulationtoselectwhichblockstoupdatewhen
performing BCD on Problem 1. For each BCD iteration, we apply
a block Gauss Southwell (BGS) greedy selection rule to Problem 4
to select the next block to update. BGS selection has been shown
in theory and in practice to make more progress per iteration than
cyclic or random selection [5, 21], however, on many problems,
BGS selection is prohibitively expensive [22]. One critical aspect
of our procedure is that we exploit problem structure to develop a
BGS steepest direction (BGS-s) rule that is cheap to compute.
Let𝑓(𝜃)=1
2∥𝑦−𝑀
𝑄⊺𝐴′𝜃∥2
2. For BGS-s selection, we first com-
pute vector 𝑑∈R𝑛𝑝which stores the magnitude of the most neg-
ative directional derivative for each coordinate. This vector is de-
fined coordinate-wise by
𝑑𝑖={
|𝑆𝜆𝑓(∇𝑖𝑓(𝜃)| if𝜃𝑖=0
|∇𝑖𝑓(𝜃) +sign(𝜃𝑖)𝜆𝑓|if𝜃𝑖≠0,(5)
where𝑆𝜆𝑓is the soft-thresholding operator. Let 𝑑𝑘∈R𝑛represent
theelementsinvector 𝑑associatedwithblock 𝑘.Weselectthebest
block𝑘∗to update via:
𝑘∗=arg max
𝑘∈ [𝑝]∥𝑑𝑘∥2
2.(6)
Equations 5 and 6 form our BGS selection rule, which is com-
putationally bottlenecked by the cost of computing the full gradi-
ent∇𝑓(𝜃). The LASSO design matrix 𝑀𝑄⊺𝐴′is also only used to
computethisgradient.Below,weshowhowtoefficientlycompute
gradient ∇𝑓(𝜃)without forming the LASSO design matrix.
FastGradientProcedure: Wehavethat ∇𝑓(𝜃)=−𝐴′⊺𝑄𝑀⊺𝑟′,
where𝑟′=𝑦−𝑀𝑄⊺𝐴′𝜃.Sinceouralgorithmiszero-initialized,we
can store𝑟′and update the residual vector at each BCD iteration
to avoid multiplying the design matrix with 𝜃. Matrix𝑀⊺consists
of𝑝identity matrices stacked vertically which makes the gradient
expressionblock-separable.Forafixedblock 𝑘∈ [𝑝],wehavethat
∇𝑘𝑓(𝜃)=−𝐴⊺𝑄𝑘𝑟′, where𝑄𝑘is the sorting matrix for feature 𝑥𝑗.
The matrix𝐴⊺is a padded upper triangular matrix, so computing
thegradientforblock 𝑘simplyinvolvesordering 𝑟′withrespectto
the sorted values of 𝑥𝑗and taking a rolling sum down the ordered
vector, which is extremely efficient. Computing the full gradient
can be embarrassingly parallelized across blocks.
 
1865KDD’24, August 25–29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
With this procedure, our BGS-s selection rule is efficient, par-
allelizable, and can be computed without constructing the LASSO
design matrix. Below, we formalize our greedy block coordinate
descent (GBCD) algorithm and analyze its convergence properties.
3.3 BGS-GBCD Algorithm
To solve Problem 1, we use the following GBCD algorithm. Start
with𝛽𝑗=0for all blocks 𝑗∈ [𝑝]and repeat until convergence: ap-
ply our BGS selection rule to Problem 4 (LASSO reformulation) to
selectablocktoupdateandsolveProblem2(originalblockupdate
problem) with dynamic programming to update the block. This re-
turns a sequence of solutions 𝛽𝑡that correspond to a sequence of
decreasing objective values.
3.3.1 Convergence Analysis. Thesequenceofsolutions 𝛽𝑡returned
by BGS-GBCD converges to the minimizer for Problems 1 and 4.
More generally, we show that BGS-GBCD converges to optimality
when applied to block-separable LASSO problems. We prove the
next proposition in the appendix (suppl. B.1).
PRoposition 1. Given composite problem
min
𝜃𝐹(𝜃)=𝑓(𝜃) +𝜆∥𝜃∥1,
where𝑓is convex and coordinate-wise L-smooth and 𝜃is both block
and coordinate separable, every limit point of BGS-GBCD coincides
with a minimizer for 𝐹(𝜃). Any sequence of solutions 𝜃𝑡generated
by BGS-GBCD converges to a limit/minimum point.
Weproveforthefirsttimethatgreedyblockcoordinatedescent
using block Gauss-Southwell-s selection converges to the mini-
mum point when applied to ℓ1-composite problems. We also show
that under certain conditions, BGS-GBCD updates make provably
good progress towards the minimum. Proposition 2 states a prop-
erty that we exploit in §4 when developing feature selection algo-
rithms.
PRoposition 2. If block𝜃𝑡
𝑘=0is selected via the BGS rule, the
progress after one GBCD update is bound by:
𝐹(𝜃𝑡+1) −𝐹(𝜃𝑡) ≤ min
𝛾∈R𝑛∇𝑓(𝜃𝑡)⊺𝛾+
L
2snorm (𝛾)
+𝜆∥𝜃𝑡+𝛾∥1−𝜆∥𝜃𝑡∥1,
where snorm( 𝛾) sums theℓ2-norm of each block in 𝛾.
Each block in our optimization problem corresponds to a fea-
ture;𝛽𝑗gives the contribution of feature 𝑥𝑗to the additive model.
Proposition 2 states that when the BGS rule is used to select a fea-
ture (block) to enter the support, the corresponding block update
makes substantial progress towards the minimum. The proof for
this proposition is also in the appendix (suppl. B.2).
3.4 Discussion
In most LASSO problems, greedy selection offers little advantage
over cyclic selection since the computational cost of selecting the
blockwiththesteepestdirectionalderivativesissimilartothecost
of updating all of the blocks [31]. Greedy BCD is effective in FAST,
however,sinceblockselections,whichinvolveembarrassinglypar-
allel summations are much cheaper than block updates, which re-
quire expensive dynamic programming calls.We observe that BGS-GBCD greatly reduces the number of dy-
namic programming block updates (Problem 2) required to solve
Problem 1, compared to cyclic block selection. For example, in Fig-
0 1 2 3 4
# Dynamic Programming Block Updates (log10)0:10:20:30:40:5Training ErrorCyclic vs. Greedy Block Selection
Cyclic Block Selection
Greedy Block Selection
Figur
e2:Greedyselec-
tion reduces the num-
ber of dynamic pro-
gramming block up-
dates by 2 orders of
magnitude.
ure 2, we use greedy and cyclic BCD to fit FAST on the Elevators
dataset [29]. The horizontal axis shows the number of dynamic
programming block updates (log10) and the vertical axis shows
training loss. We observe that BGS-GBCD requires nearly 100×
fewerupdatestoconverge.Thiscorrespondstosubstantialcompu-
tational speedups, which we show in our experiments in §5.1.
3.5 Binning
FAST can also incorporate binning, a popular heuristic used by
EBMs [20] and LightGBMs [13], to reduce computation time for
a nominal cost in model expressiveness. FAST performs binning
using a novel equivalent optimization formulation while existing
methods,suchasEBMs,pre-processthedata.Givenasetofbinsfor
each feature 𝑥𝑗, we add the constraints for all entries (𝑖1,𝑖2) ∈ [𝑛]
that if entries (𝑥𝑗)𝑖1and(𝑥𝑗)𝑖2fall in the same bin, then (𝛽𝑗)𝑖1=
(𝛽𝑗)𝑖2.Weshowintheappendix(suppl.C)thatwecanreformulate
these constraints into a weighted smooth loss function in the ob-
jective and efficiently solve this unconstrained problem with BGS-
GBCD. Binning directly reduces the number of decision variables
in FAST by a factor of # bins over # rows and combining BGS-
GBCD with binning further reduces computation time.
4 FEATURE-SPARSE FAST
OurFASTframeworkisquiteflexible;herewediscussanextension
of the framework to explicitly account for variable selection. We
add this group sparsity constraint to Problem 1:∑𝑝
𝑗=1/x31(𝛽𝑗≠0) ≤
𝐾, where𝐾is the maximum number of features to select. Problem
1withthisconstraintisNP-hardanddifficulttosolvetooptimality
due to the large number of variables; we have a variable for each
entry of𝑋. As such, we develop two approximate algorithms to
find good solutions. These algorithms have different strengths in
terms of solution quality and runtime, but both algorithms rely
on the BGS rule presented in §3.2 and the fact that BGS selection
makesprovablygoodprogresswhenselectingfeaturestoenterthe
support (Prop. 2).
4.1 Approximate Greedy Iterative Selection
For Approximate Greedy Iterative Selection (AGIS), we partition
theblocksintothesupport 𝑆={𝑗∈ [𝑝] |𝛽𝑗≠0}andcomplement
𝑆𝑐andstartwithallblocksequalto 0.WeusetheBGSruletoselect
 
1866FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD ’24, August 25–29, 2024, Barcelona, Spain.
the best block 𝑘∈𝑆𝑐to update and we perform a block update by
solving Problem 2 to add 𝑘into𝑆. If|𝑆|>1, we iterative through
the blocks in 𝑆and conduct block updates until convergences. We
repeat this procedure, interlacing BGS selection with sweeps on
the support 𝑆until the condition |𝑆|=𝐾is reached. AGIS returns
asequenceofPCAMswitheveryfeaturesparsitylevelfrom 1...𝐾.
To improve solution quality across all sparsity levels we apply this
local search heuristic.
4.1.1 BGS Local Search: After each sweep of 𝑆converges, use the
BGS rule to select the best block to update in 𝑆𝑐and denote that
block𝛽∗
𝑗. This is the block that we will swap into the support. To
find the best block to swap out of the support, iterate over 𝛽𝑗∈𝑆.
For each block, set 𝛽𝑗=0and conduct a block update on 𝛽∗
𝑗, and
selecttheblockin 𝑆thatwhenswappedimprovestheobjectivethe
most. After this swap, conduct another sweep over 𝑆until conver-
gence to obtain the final solution. We present our full AGIS algo-
rithm, with local search, in Algorithm 1.
Algorithm
1:AGIS
Input:𝐾,𝜆𝑓,𝐷,𝑄𝑗∀𝑗∈
[𝑝]
1Initialize𝛽𝑗=0∀𝑗∈ [𝑝],𝑆=∅,𝑆𝑎𝑙𝑙=∅
2repeat
3Use
BGS rule to select 𝑘∈𝑆𝑐.
4Update block 𝑘(Problem 2).
5𝑆=𝑆∪𝛽𝑘, 𝑆𝑐=𝑆𝑐\𝛽𝑘
6repeat
7 Sw
eep through 𝑆and update blocks (Problem 2).
8until converged
9BGS local search.
10𝑆𝑎𝑙𝑙=𝑆𝑎𝑙𝑙∪𝑆
11until |𝑆|=𝐾
Output: Sequence of models 𝑆𝑎𝑙𝑙
4.2
Groupℓ0-FAST
InadditiontoAGIS,wecanuseagroup ℓ0-sparsitypenaltytoselect
features in FAST. This approach often obtains better solutions at
the cost of increased computation time, which we discuss in §5.2.
We use this Lagrangian formulation:
min
𝛽1
2∥𝑦−𝑝∑
𝑗=1𝑄⊺
𝑗𝛽𝑗∥2
2+
𝜆𝑓𝑝∑
𝑗=1∥𝐷
𝛽𝑗∥1+𝜆𝑠𝑝∑
𝑗=1/x31(𝛽𝑗≠0),(7)
where𝜆𝑠isthesparsityhyperparameter.Thegroupsparsitypenalty
is block-separable over 𝛽𝑗so we can apply BCD methods to find
good solutions to this problem. Given fixed block 𝑘and residual
vector𝑟, we can write each block update problem as:
min
𝛽𝑘1
2∥𝑄𝑘𝑟−𝛽𝑘∥2
2+𝜆𝑓∥𝐷
𝛽𝑘∥1+𝜆𝑠 /x31(𝛽𝑘≠0).This problem can be solved by first setting 𝜆𝑠=0and solving the
FLSA for𝛽∗
𝑘. We then check the thresholding condition:
1
2∥𝑟∥2
2−1
2∥𝑄𝑘𝑟−𝛽∗
𝑘∥2
2−𝜆𝑓∥𝐷
𝛽∗
𝑘∥1≤𝜆𝑠
and set𝛽∗
𝑘=0if the condition is satisfied. We show the derivation
for this in the appendix (suppl. D).
Sincethegroupsparsitypenaltyisnotcontinuous,itisnotclear
if BGS-GBCD can be extended here. To find high-quality solutions
to Problem 7, we use cyclic block coordinate descent and apply
our BGS local search heuristic (§4.1.1) when CBCD converges. We
interlace CBCD sweeps with local search steps until the objective
no longer improves.
4.3 Discussion
We show an example of the impact of local search on solution
quality and discuss the strengths and weaknesses of both group
ℓ0-FAST and AGIS.
2 4 6 8 100:20:30:40:50:6Test MSEAGIS no LS
AGIS w/ LS
2 4 6 8 10Group`0no LS
Group`0w/ LS
Number of FeaturesEect of Local Search
Figur
e 3: BGS local search improves the solution quality for
both of our feature-sparse PCAM algorithms.
4.3.1 Local Search Performance. We observe empirically that our
BGSlocalsearchheuristicimprovestheout-of-sampleperformance
of both feature selection algorithms. For example, in Figure 3, we
usegroupℓ0-FASTandAGIStobuildfeature-sparsePCAMsonthe
Elevatorsdataset(16500rowsand16columns)[29].Wevary 𝐾,the
sparsitybudgetinthemodelfrom1to10,andcomparethetestper-
formance of the model measured via MSE. For both methods, the
local search heuristic improves performance.
4.3.2 Group ℓ0-FAST vs. AGIS. In our experiments in §5.2, we ob-
serve that group ℓ0-FAST generally outperforms AGIS at building
sparse PCAMs. AGIS, however, is computationally faster since the
algorithm can leverage greedy block selection. Fitting group ℓ0-
FASTrequiresCBCDupdatesduetothenon-convexityofthegroup
sparsity penalty. In addition, AGIS is easier to use since the algo-
rithm by design outputs a sequence of PCAMs with with every
support size from 1to𝐾. The sparsity hyperparameter 𝜆𝑠in group
ℓ0-FASTmustbetunedandthealgorithmmayskipcertainsupport
sizes due to non-convexity [9].
5 EXPERIMENTS
We evaluate the computation time of FAST against existing algo-
rithms and assess how well the framework performs at building
feature-sparse PCAMs.
 
1867KDD’24, August 25–29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
5.1 Computation Time Experiment
We compare the computation time of FAST against existing SOTA
algorithms for building feature-dense PCAMs: EBMs and FLAMs.
5.1.1 Experimental Procedure. On 10 large regression benchmark
datasets from OpenML [29], we use FAST, EBM, and FLAM to fit
PCAMs.Forthecompetingmethods,weusetheInterpretMLpack-
age[20]tofitEBMsinPythonandtheFLAMpackageinR[24].We
use the default hyperparameters for InterpretML EBMs, which are
optimized for fast runtime. For FLAM, we match the fusion hy-
perparameter with the value used in FAST. The test errors of the
models fit using the 3 methods, under these configurations, are
comparable (as intended). We conduct this experiment on a M2
MacbookProwith10coresandmatchthenumberofcoresusedin
the methods that support multiprocessing (FAST and EBMs). Ad-
ditional details can be found in the appendix (suppl. E).
Dataset
/ Method F
AST EBM FLAM
Black
Smoke +
(9214951, 41)329.6s
(1.2)
0.6315h
49m 31s
0.63__
Black
Smoke
(9214951, 14)43s
(2.8)
0.623h
57m 9s
0.61__
P
hysiochemical
(5023496, 9)33.4s
(0.3)
0.5043m
31s (31.9)
0.50__
A
uto Horsepower
(900000, 17)1.63s
(0.01)
0.6385s
(3.9)
0.64__
Ailer
ons BNG
(669994, 38)2.57s
(0.08)
0.5785s
(2.0)
0.56__
Slice
Localization
(35845, 351)6.7s
(0.05)
0.8258.7s
(2.2)
0.8114m
50s (30.4)
0.81
Sup
erconduct
(21263, 79)0.45s
(0.01)
0.887.6s
(0.05)
0.8913.0s
(0.03)
0.87
Scm1d
(8828,
280)0.7s
(0.01)
0.927.9s
(0.47)
0.91190s
(5.1)
0.90
Rf2
(8212,
448)1.34s
(.01)
0.9878.5s
(4.3)
0.98180s
(3.0)
0.97
Isolet
(7017,
613)2.50s
(0.02)
0.728.67s
(0.3)
0.70360s
(8.0)
0.68
T
able 1: Timing experiment results. FAST achieves 2 orders
of magnitude speedups for large problems.
5.1.2 Results. Table 1 shows the results of our experiment. The
leftmost column shows dataset names and dimensions: (𝑛,𝑝). In
each cell in the other columns, the top entry shows the computa-
tion time of the method averaged over runs along with the stan-
dard deviation. The bottom entry shows the test 𝑅2of the model.
Thetop5rowsofthistableshowtimingresultsonlarge 𝑛datasets
with more than 500000rows. On these datasets, we are unable to
apply FLAM due to problem scale so we compare FAST against
EBMs. We observe that FAST fits PCAMs up to 2 orders of magni-
tude faster than EBMs. For example on an augmented version of
the UK Black Smoke dataset, with 9 million rows and 41 columns,
it takes over 15 hours to fit an EBM. FAST on the other hand can
fit a PCAM that performs the same in around 5 minutes.
The bottom 5 rows of Table 1 show results on large 𝑝datasets
with more than 50 columns, but less than 50,000 rows. We observeherethatFASTfitsPCAMs2ordersofmagnitudefasterthanFLAM
and around 1 order of magnitude faster than EBMs. For example
on the Slice Localization dataset, with over 300 columns, it takes
nearly 15 minutes to fit a FLAM. FAST can fit a PCAM that per-
forms the same in under 10 seconds.
In all, we find that FAST substantially outperforms EBMs and
FLAMs in terms of computation time across various large prob-
lems.
0 10 20 30 400:40:5Test MSE1
2
3
4
5Black Smoke n107
0:0 0:5 1:0 1:50:50:61
2
3
45Auto Horsepower n106
Training Time (seconds)
Figur
e 4: FAST can fit low optimization tolerance models
that perform well extremely quickly.
5.1.3 Low Optimization Tolerance Models. As an aside, we note
that we can leverage our greedy BCD algorithm to fit FAST with
low optimization tolerances, in order to quickly produce a PCAM
thatstillperformswellout-of-sample.InFigure4,weshowthetest
error of FAST (vertical axes) plotted against the training time in
seconds(horizontalaxes)fortheUKBlackSmokeandAutoHorse-
power [29] datasets. We vary the training time of FAST by early-
stopping the optimization algorithm after a fixed number of itera-
tions, the first 5 GBCD iterations are plotted in red. In both exam-
ples,thefirst5iterationsgreatlyreducethe testerrorofthemodel.
For the UK Black Smoke dataset, FAST can fit a low optimization
tolerance model that performs well in less than 10 seconds.
5.2 Feature Selection Experiment
Here we evaluate how well FAST performs at building feature-
sparse PCAMs.
5.2.1 Experimental Setup. We repeat this procedure on 20 regres-
sion datasets from OpenML and use a 10-fold CV on each dataset.
The full list of datasets can be found in the appendix (suppl. E). On
the training folds, we use group ℓ0-FAST and AGIS to fit feature-
sparse PCAMs by varying the sparsity budget 𝐾∈ {2,4,6,8,10}.
We evaluate the MSE of each sparse model on the test fold.
We compare the performance of these models against the fol-
lowing SOTA algorithms to construct feature-sparse PCAMs.
•FLAM-GL (2016): In FLAM group LASSO [25], we fit a FLAM
withagroupLASSOpenaltyoverthefeatures.Wetunethespar-
sity hyperparameter such that at most 𝐾features are selected.
•EBM-RS(2019): InEBMrankandselect[20],wefirstfitanEBM
on the training data and rank the features by importance scores;
the contribution of each feature averaged over the training ob-
servations. We select the top 𝐾features and refit an EBM. This
method is computationally expensive since it fits two PCAMs.
 
1868FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD ’24, August 25–29, 2024, Barcelona, Spain.
Comp
eting Alg. /
Sparsity2 4 6 810
FLAM-GL84.1%
81.5%102.0%
81.6%77.9%
62.3%50.8%
48.6%45.1%
41.4%
EBM-RS48.9%
45.1%24.3%
18.3%29.7%
22.7%23.7%
22.0%22.8%
19.8%
Contr
olBurn28.4%
27.7%57.5%
47.5%71.3%
60.1%68.1%
65.4%70.4%
65.6%
FastSparseGAM131.5%
126.5%85.6%
73.4%72.4%
60.7%50.9%
48.6%48.2%
44.2%
SAM103.5%
102.4%64.0%
51.7%52.8%
40.4%30.0%
28.4%23.5%
20.5%
LASSO89.6%
89.5%71.9%
65.8%56.1%
49.1%46.0%
44.1%41.2%
38.2%
T
able 2: Average % decrease in test error between feature-
sparse FAST and our competing algorithms across sparsity
budgets(distributionsshowninFigure5).Positivevaluesin-
dicate that feature-sparse FAST outperforms the competing
algorithm. In each cell, the top value shows group ℓ0-FAST
and the bottom value shows AGIS.
•ControlBurn (2021): ControlBurn [15] is a flexible framework
for building feature-sparse nonlinear models. The feature selec-
tion algorithm in the framework first constructs a specialized
tree ensemble that is diverse, where each tree in the ensemble
uses a different subset of features. Then, the weighted LASSO is
used to select feature-sparse subsets of trees that perform well.
We refit the final model, in this case, an EBM, on the 𝐾selected
features. ControlBurn with an EBM has been used to construct
high-performing,feature-sparsePCAMsforheartfailurepredic-
tion in clinical machine learning [28].
•FastSparseGAM (2022): FastSparseGAM [16] is a package for
sparseregressionbuiltontopoftheL0Learnframework[9].The
packagecanbeadaptedtoconstruct extremely sparsePCAMsby
one-hotencodingthefeaturesandselectingasmallsubsetofthe
resulting components [17].
We also compare feature-sparse FASTagainst two traditional algo-
rithmsthatproducenon-piecewiseconstantadditivemodels,Sparse
Additive Models (SAM), which uses the group LASSO to sparsify
splines, and the linear LASSO. Additional details on our experi-
mental procedure can be found in the appendix (suppl. E).
5.2.2 Results. For each run of our experiment, we compute the
percent decrease in test MSE between feature-sparse FAST and
each competing algorithm, given by:
% decrease MSE =MSE Competing Alg. −MSE FAST
MSE
FAST,
for each sparsity budget. A positive percent decrease in test error
indicates that feature-sparse FAST performs better than the com-
peting algorithm for that sparsity budget.
In Table 2 we report the average percent decrease in test error
betweengroup ℓ0-FASTandthecompetingalgorithms(top values)
andAGISandthecompetingalgorithms( bottom values)acrossall
sparsity budgets. These averages are taken across all datasets and
folds in our experiment. In Figure 5, we show the full distributions
of our results. In each plot, the horizontal axis shows the sparsity
budget and the vertical axis shows the percent decrease in test er-
rorbetweenfeature-sparseFASTandthecompetingalgorithm;theleft plot shows group ℓ0-FAST and the right plot shows AGIS. The
grouped violin plots show the distribution of the results for each
sparsity budget and the averages of each distribution are marked
by horizontal lines, which correspond to the averages in Table 2.
From Figure 5, we see that group ℓ0-FAST and AGIS largely out-
perform all of our competing algorithms. The distributions of the
percent decrease in test error between these two methods and our
competing algorithms are nearly entirely positive across all spar-
sity budgets. We also observe that group ℓ0-FAST consistently per-
formsslightlybetter thanAGIS.In Table2,theaveragepercentde-
crease in test error for group ℓ0- FAST (top value) is always higher
than that for AGIS (bottom value). However, as discussed in §4.3.2,
AGISalsohasseveraladvantagesintermsofspeedandeaseofuse.
Forsparsitybudget 𝐾=2,groupℓ0-FASTattainsa 28%decrease
in test error compared to the best competing algorithm, Control-
Burn.Interestingly,theperformanceofControlBurndegradesas 𝐾
increases. This is because the framework selects features indepen-
dently of the final EBM that is refitted [15]. While ControlBurn is
usefulforselectingafewimportantfeatures,ourresultsshowthat
the framework fails at building sparse PCAMs for larger values of
𝐾.For𝐾∈ {4,6,8,10},groupℓ0-FASTattains upto a 30%decrease
in test error compared to the best competing algorithm, EBM-RS.
Inall,weobservethatgroup ℓ0-FASTconsistentlyoutperformsthe
best competing algorithm across all sparsities.
Inaddition,feature-sparseFASTsubstantiallyoutperformsFLAM-
GL and FastSparseGAM, by over a 100%decrease in test error for
some sparsities. FLAM-GL is affected by over-shrinkage from the
groupLASSO,whichisespeciallypronouncedsincetheFLAMfor-
mulation uses a large number of variables. The ℓ0-based penalties
and constraints in feature-sparse FAST are shrinkage-free and ro-
bust to this effect. FastSparseGAM indirectly selects features by
sparsifyingpiecewisesegmentsintheadditivemodel[17].Feature-
sparse FAST, on the other hand, directly accounts for feature spar-
sity in the optimization framework and outperforms this compet-
ing method. We also note that feature-sparse FAST substantially
outperformsourcompetingalgorithmsthatdonotfitPCAMs:SAM,
which also uses the group LASSO, and the linear LASSO.
Finally, we observe that many distributions in Figure 5 have
heavy positive tails, notably for the EBM-RS, FLAM-GL, and Con-
trolBurncompetingalgorithms.Theseheavytailstypicallycontain
theresultsfromdatasetswithcorrelatedfeatures;weshowcorrela-
tion matrices and the distribution of errors in the appendix (suppl.
F). In §6.1, we show through a semi-synthetic experiment that cor-
related features degrade the performance of EBM-RS, FLAM-GL,
andControlBurn.Group ℓ0-FASTandAGIS,ontheotherhand,can
effectively build sparse PCAMs regardless of feature correlations.
6 PCAMS AND FEATURE CORRELATIONS
We conclude by investigating how correlated features impact the
interpretability of PCAMs.
6.1 Correlated Feature Selection
We present here a semi-synthetic example to investigate how cor-
related features affect our feature selection experiment (§5.2). We
startwiththeHousesdataset[29]andbuildfeature-sparsePCAMs
 
1869KDD’24, August 25–29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
2 4 6 8 100%100%200%300%400%500%600%Group`0-FAST % Decrease in Test MSE
2 4 6 8 10AGIS % Decrease in Test MSE
FLAM-GL
EBM-RS
ControlBurn
FastSparseGAM
SAM
LASSO
Sparsity
Figur
e 5: Distribution of results from our feature selection experiment (averages shown in Table 2). The distributions are
mostly entirely positive, which indicates that feature-sparse FAST outperforms our competing algorithms.
usingFAST,EBM-RS,FLAM-GL,andControlBurnbyvaryingspar-
sity budget 𝐾. The top left plot in Figure 6 shows the test perfor-
mance of these sparse models.
0:40:60:8Original Data
AGIS
Group`0
EBM-RS
FLAM-GL
ControlBurn
3 Correlated Features Added
2 4 6 80:40:60:86 Correlated Features Added
2 4 6 89 Correlated Features Added
SparsityTest MSE
Figur
e 6: Group ℓ0-FAST and AGIS perform well even after
adding correlated features.
We then add 3, 6, and 9 correlated features to the data. As the
other plots in Figure 6 show, the performances of EBM-RS, Con-
trolBurn, and FLAM-GL degrade significantly with added corre-
lations but the performances of group ℓ0-FAST and AGIS remain
unaffected.
EBMfeatureimportancescorescapturethecontributionofeach
feature to the prediction of the model, averaged over all training
observations. Given a pair of highly correlated features, the cyclic
round-robin algorithm used to fit EBMs will split the contribution
of the features evenly between the pair. As such, the average fea-
ture importance score/ranking of a group of correlated featureswill be suppressed, which degrades the performance of EBM-RS.
This effect is analogous to the so-called correlation bias observed
in random forest feature rankings by the ControlBurn paper [15].
ControlBurn attempts to address correlation bias by using the
weighted LASSO to select features and we indeed observe in Fig-
ure 6 that the algorithm is more robust than EBM-RS to added
correlations. However, the LASSO penalty used in ControlBurn
still imparts shrinkage which biases sparse selection in the pres-
enceofmulticollinearity[8,10,19].Additionally,thegroupLASSO
penalty in FLAM-GL is known to suffer from over-shrinkage and
performsevenworseatselectingsparsesubsetsofcorrelatedgroups
[10]. The penalties and constraints used to select features in FAST
are shrinkage-free and, as a result, our algorithms are unaffected
by the added correlated features.
6.2 Identifying Discontinuities
One attractive property of PCAMs is their ability to capture dis-
continuouspatternsintheunderlyingdata.Here,wepresentacase
studytodemonstratehowcorrelatedfeaturescandegradetheabil-
ity of EBMs to identify discontinuities.
WeusetheHouses[29]datasettobuildPCAMstopredicthouse
pricesusingdemographicfeatures.Unsurprisingly,thereisanearly
linearrelationshipbetweenthemedianincomeofadistrictandthe
price of homes in that district. We add the following artificial dis-
continuitytothedata:foralldistrictswithamedianincomeabove
$40,000 a year, we drop the price of homes by $20,000.
We then fit feature-dense PCAMs using FAST and EBMs. Both
methods capture the discontinuity in the shape function for me-
dian income, as shown in the left two plots in Figure 7. Next, we
add 10 synthetic features that are correlated with median income
and refit.
The right two plots in Figure 7 show the new shape functions.
Theredlinesshowmedianincomeandthegreylinesshowthesyn-
theticcorrelatedfeatures.WeobservethatforFAST(topright),the
shape function of median income is preserved and that the discon-
tinuous pattern can still be easily identified. The shape functions
 
1870FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD ’24, August 25–29, 2024, Barcelona, Spain.
-1-0.50FAST original
Median Income
Correlated FeaturesFAST w/ correlations
0 4 8-1-0.50EBM original
0 4 8EBM w/ correlations
Feature ValueContribution
Figur
e 7: Correlated features can mask discontinuities in
PCAM shape functions.
of the synthetic correlated features are reduced in magnitude and
severalareimplicitlyregularizedtozero.TheEBMshapefunctions
(bottom right) tell a different story. Due to the cyclic algorithm
used to fit EBMs, the contribution of median income is evenly dis-
tributed among the noisy correlated features. All the shape func-
tionsinthisgrouparecompressedandthediscontinuityisdifficult
to detect.
Ifadatasetcontainsagroupofcorrelatedfeatures,andonlyone
feature in that group contains an interesting discontinuity, FAST
maybemorelikelytocapturethispatterncomparedtoEBMs.Also,
consider the case where a dataset contains a sensitive attribute
with a discontinuous pattern. A potential adversarial attack would
be to add features correlated with this sensitive attribute to mask
thisdiscontinuouspatternfromEBMs.Duetogreedymodelfitting,
FAST again may be more robust to this attack.
We emphasize that both FAST and EBMs produce transparent
PCAMs,however,theinterpretationsofthemodelschangedepend-
ing on whether the PCAMs were fit greedily or cyclically. Model
transparency does not guarantee trustworthiness and practition-
ers should still interpret transparent models cautiously.
7 CONCLUSION
FAST is an optimization-based framework that leverages a novel
greedyoptimizationproceduretofitPCAMsupto2ordersofmag-
nitude faster than SOTA methods. The framework also introduces
two feature selection algorithms that significantly outperform ex-
isting methods at building sparse PCAMs. Using FAST, we investi-
gatehowcorrelatedfeaturesimpacttheinterpretabilityofPCAMs
in terms of selecting important features and interpreting shape
functions. These phenomena should be considered when evaluat-
ing the trustworthiness of additive models.
ACKNOWLEDGMENTS This research is funded in part by a
grant from the Office of Naval Research (ONR-N00014-21-1-2841).
REFERENCES
[1] Peter Bacchetti. Additive isotonic models. Journal of the American Statistical
Association, 84(405):289–294, 1989.[2] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie
Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hos-
pital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international
conference on knowledge discovery and data mining, pages 1721–1730, 2015.
[3] Chun-Hao Chang, Sarah Tan, Ben Lengerich, Anna Goldenberg, and Rich Caru-
ana. How interpretable and trustworthy are gams? In Proceedings of the 27th
ACM SIGKDD Conference on Knowledge Discovery & Data Mining , pages 95–105,
2021.
[4] Eric Chu, Arezou Keshavarz, and Stephen Boyd. A distributed algorithm for
fitting generalized additive models. Optimization and Engineering , 14(2):213–
224, 2013.
[5] InderjitDhillon,PradeepRavikumar,andAmbujTewari.Nearestneighborbased
greedy coordinate descent. Advances in Neural Information Processing Systems ,
24, 2011.
[6] Jerome Friedman, Trevor Hastie, Holger Höfling, and Robert Tibshirani. Path-
wise coordinate optimization. 2007.
[7] Trevor Hastie and Robert Tibshirani. Generalized additive models for medical
research. Statistical methods in medical research , 4(3):187–196, 1995.
[8] Trevor Hastie, Robert Tibshirani, and R Tibshirani. Best subset, forward step-
wise or lasso. Analysis and recommendations based on extensive comparisons:
Statistical Science, 2020.
[9] Hussein Hazimeh and Rahul Mazumder. Fast best subset selection: Coordinate
descent and local combinatorial optimization algorithms. Operations Research ,
68(5):1517–1537, 2020.
[10] Jian Huang, Patrick Breheny, and Shuangge Ma. A selective review of group
selection in high-dimensional models. Statistical science: a review journal of the
Institute of Mathematical Statistics, 27(4), 2012.
[11] Nicholas A Johnson. A dynamic programming algorithm for the fused lasso
and l 0-segmentation. Journal of Computational and Graphical Statistics , 22(2):
246–260, 2013.
[12] Sai Praneeth Karimireddy, Anastasia Koloskova, Sebastian U Stich, and Martin
Jaggi. Efficient greedy coordinate descent for composite problems. In The 22nd
International Conference on Artificial Intelligence and Statistics, pages 2887–2896.
PMLR, 2019.
[13] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qi-
weiYe,andTie-YanLiu. Lightgbm:Ahighlyefficientgradientboostingdecision
tree. Advances in neural information processing systems, 30, 2017.
[14] Benjamin J Lengerich, Rich Caruana, Mark E Nunnally, and Manolis Kellis.
Death by round numbers: Glass-box machine learning uncovers biases in medi-
cal practice. medRxiv, pages 2022–04, 2022.
[15] Brian Liu, Miaolan Xie, and Madeleine Udell. Controlburn: Feature selection by
sparse forests. In Proceedings of the 27th ACM SIGKDD conference on knowledge
discovery & data mining, pages 1045–1054, 2021.
[16] Jiachang Liu. fastsparsegams, 2023. URL https://pypi.org/project/
fastsparsegams/.
[17] Jiachang Liu, Chudi Zhong, Margo Seltzer, and Cynthia Rudin. Fast sparse clas-
sification for generalized linear and additive models. Proceedings of machine
learning research , 151:9304, 2022.
[18] Yin Lou, Rich Caruana, and Johannes Gehrke. Intelligible models for classifi-
cation and regression. In Proceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 150–158, 2012.
[19] RahulMazumder. Discussionof“bestsubset,forwardstepwiseorlasso?analysis
and recommendations based on extensive comparisons”. Statistical Science , 35
(4), 2020.
[20] Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. Interpretml:
A unified framework for machine learning interpretability. arXiv preprint
arXiv:1909.09223, 2019.
[21] Julie Nutini, Mark Schmidt, Issam Laradji, Michael Friedlander, and Hoyt
Koepke. Coordinatedescentconvergesfasterwiththegauss-southwellrulethan
random selection. In International Conference on Machine Learning , pages 1632–
1641. PMLR, 2015.
[22] Julie Nutini, Issam Laradji, and Mark Schmidt. Let’s make block coordinate de-
scent converge faster: faster greedy rules, message-passing, active-set complex-
ity, and superlinear convergence. Journal of Machine Learning Research, 23(131):
1–74, 2022.
[23] NYPD. Motor vehicle collisions - crashes: Nyc open data, Sep 2023.
[24] Ashley Petersen. flam: Fits Piecewise Constant Models with Data-Adaptive Knots ,
2018. URL https://CRAN.R-project.org/package=flam. R package version 3.2.
[25] Ashley Petersen, Daniela Witten, and Noah Simon. Fused lasso additive model.
Journal of Computational and Graphical Statistics, 25(4):1005–1025, 2016.
[26] Junyang Qian and Jinzhu Jia. On stepwise pattern recovery of the fused lasso.
Computational Statistics & Data Analysis , 94:221–237, 2016.
[27] Charles J Stone and Cha-Yong Koo. Additive splines in statistics. Proceedings of
the American Statistical Association Original pagination is p, 45:48, 1985.
[28] Mike Van Ness, Tomas Bosschieter, Natasha Din, Andrew Ambrosy, Alexander
Sandhu, and Madeleine Udell. Interpretable survival analysis for heart failure
risk prediction. In Machine Learning for Health (ML4H), pages 574–593. PMLR,
2023.
 
1871KDD’24, August 25–29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
[29] Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml:
networked science in machine learning. ACM SIGKDD Explorations Newsletter,
15(2):49–60, 2014.
[30] Simon N Wood, Zheyuan Li, Gavin Shaddick, and Nicole H Augustin. General-
ized additive models for gigadata: modeling the uk black smoke network daily
data. Journal of the American Statistical Association, 112(519):1199–1210, 2017.
[31] Tong Tong Wu and Kenneth Lange. Coordinate descent algorithms for lasso
penalized regression. 2008.
APPENDIX
We show proofs and derivations below, a full online supplement
containing experiment details and visualizations can be found at:
https://github.com/brianliu12437/FASTopt.
PROOF OF PROPOSITION 1 (B.1)
We start with the following preliminaries. We have,
min
𝜃𝐹(𝜃)=𝑓(𝜃) +𝜆∥|𝜃∥1,(1)
where𝜃is both coordinate separable and block separable, with
specified blocks and 𝑓(𝜃)is coordinate-wise L-smooth. Recall we
define direction vector 𝑑(same dimensions of 𝜃) elementwise by
𝑑𝑗={
|𝑆𝜆(∇𝑗𝑓(𝜃)| if𝜃𝑗=0
|∇𝑗𝑓(𝜃) +sign(𝜃𝑗)𝜆|if𝜃𝑗≠0,(2)
and we select the block to update whose corresponding elements
of𝑑have the largest ℓ2-norm.
We also have this formula for the generalized directional deriv-
ative for𝐹(𝜃)with respect to arbitrary direction vector 𝑣[31].
𝜕𝑣𝐹(𝜃)=∑
𝑗∇𝑗𝑓(𝜃)𝑣𝑗+∑
𝑗{
𝜆sign(𝜃𝑗)𝑣𝑗if𝜃𝑗≠0
𝜆|𝑣𝑗| if𝜃𝑗=0(3)
Part1a: Wefirstwanttoshowthatanylimitpointofasequence
of solutions 𝜃𝑡generated via BGS-GBCD coincides with a mini-
mum point of 𝐹(𝜃). We have that 𝜃𝑡reaches a limit point when
𝑑=0.
First we show that limit points of 𝜃𝑡correspond to minimums
of𝐹(𝜃). Let𝜃∗be a limit point of sequence 𝜃𝑡. Consider the 𝑗𝑡ℎ
element of𝜃∗.
Case 1:𝜃∗
𝑗=0Since𝜃∗is stationary we have that 𝑑𝑗=0, and
that|𝑆𝜆𝑓(∇𝑗𝑓(𝜃)|=0.
This implies |∇𝑗𝑓(𝜃)| ≤𝜆. Consider the 𝑗𝑡ℎcomponent contri-
bution to the generalized directional derivative at 𝜃∗for arbitrary
direction𝑣. We have that
(𝜕𝑣𝐹(𝜃∗))𝑗=𝑓(𝜃)𝑣𝑗+𝜆|𝑣𝑗| ≥0 (4)
Case 2:𝜃∗
𝑗≠0We have that ∇𝑗𝑓(𝜃)=−sign (𝜃𝑗)𝜆, and that
(𝜕𝑣𝐹(𝜃∗))𝑗=∇𝑗𝑓(𝜃)𝑣𝑗+𝜆sign(𝜃𝑗)𝑣𝑗=0 (5)
We compute 𝜕𝑣𝐹(𝜃∗)by summing up the 𝑗𝑡ℎcomponents for
both cases. Since 𝑣is arbitrary we have that
𝜕𝑣𝐹(𝜃∗) ≥0∀𝑣,
and therefore 𝜃∗is the minimum for 𝐹(𝜃).
Part1b: Wenextshowthatminimumpointsof 𝐹(𝜃)correspond
to stationary points. This is easy to verify from the LASSO subgra-
dient optimality conditions. Let 𝜃∗now correspond to the mini-
mum of𝐹(𝜃). Consider the 𝑗𝑡ℎelement of𝜃∗.Case 1:𝜃∗
𝑗=0
WehavefromthesubgradientoptimalityconditionsoftheLASSO
that|∇𝑗𝑓(𝜃∗)| ≤𝜆. Therefore𝑑𝑗=|𝑆𝜆(∇𝑗𝑓(𝜃∗)|=0.
Case 2:𝜃∗
𝑗≠=0
WehavefromthesubgradientoptimalityconditionsoftheLASSO
that∇𝑗𝑓(𝜃) +𝜆sign(𝜃𝑗)=0. Therefore 𝑑𝑗=0.
Therefore combining parts 1a and 1b, we see that stationary
points of our BGS-GBCD algorithm and minimum points of 𝐹(𝜃)
coincide.
Part 2:Next, we want to show that any sequence of solutions
generatedbyBGS-GBCDconvergestoastationary/minimumpoint.
We first discuss some preliminaries. We rely on this key prop-
erty, since𝐹(𝜃)is both block andcoordinate separable, a block
update is equivalent to updating all of the coordinates in a block.
For this convergence analysis, we adopt the notation of good
and bad coordinate updates introduced by [12]. In a good coordi-
nate update, the updated coordinate does not cross the origin. For-
mally, given coordinate 𝑐𝑗>0and updated coordinate 𝑐+
𝑗we have
that𝑐𝑗𝑐+
𝑗>0; all updates on a zero coordinate 𝑐𝑗=0are good up-
dates. Bad coordinate updates, on the other hand, cross the origin,
i.e. given𝑐𝑗>0we have that 𝑐𝑗𝑐+
𝑗≤0.
After updating a block update, we can partition the updated co-
ordinatesintheblockintogoodandbadupdate.Wealsoadoptthe
post-processing step used in [12]. For bad coordinate updates, we
set𝑐+
𝑗=0. This ensures that each coordinate will never have two
consecutive bad updates, since updates on a zero coordinates are
always good.
[12] shows these properties for coordinate updates on 𝐹(𝜃).
The progress made by a good update on coordinate 𝑗is bound
by:
𝐹(𝜃+
𝑗) −𝐹(𝜃𝑗) ≤ −1
2L(𝑑𝑗)2, (6)
and
the progress made by a bad update on coordinate 𝑗after post-
processing is bound by
𝐹(𝜃+
𝑗) −𝐹(𝜃𝑗) ≤0. (7)
Again since 𝐹(𝜃)is both block and coordinate separable, the
progress made by a block update is equivalent to the sum of the
progressmadebyeachcoordinateintheblock,andthesameholds
for the bounds.
Wenowstartourproof.Considersomearbitrarysequenceofso-
lutions𝜃𝑡generatedbyBGS-GBCD,andassumethatthissequence
does not converge to to a stationary/minimum point.
Since𝜃𝑡does not converge to a stationary point, we have that
direction vector 𝑑is nonzero for each update. Therefore, in each
block update, we have at least one index 𝑗in the block s.t. 𝑑𝑗≠
0.The decrease in objective value for a block update is at least as
large as the decrease in objective value for a coordinate update of
a coordinate in the block.
If the update on coordinate 𝑗where𝑑𝑗≠0is a good coordinate
update, we have this bound on the decrease in objective value for
the corresponding blockupdate,
𝐹(𝜃𝑡+1) −𝐹(𝜃𝑡) ≤ −1
2L(𝑑𝑗)2wher
e𝑑𝑗>0. (8)
If the update on coordinate 𝑗is a bad update, then we have that:
 
1872FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML KDD ’24, August 25–29, 2024, Barcelona, Spain.
𝐹(𝜃𝑡+1) −𝐹(𝜃𝑡) ≤0. (9)
Due to post-processing, we can not have consecutive bad up-
dates on the same coordinate. Since we have a finite number of
coordinates, there can not exist a infinite contiguous subsequence
of BGS-GBCD block updates where the only contribution to the
decrease in objective values come from bad coordinate updates in
the block (9).
Therefore, consider our sequence of solutions 𝜃𝑡generated by
BGS-GBCD updates. We have that all contiguous subsequences of
BGS-GBCD updates must contain updates where the decrease in
objective value is bound by (8). As such, one of two things can oc-
cur. Either the objective value continues to decrease by −1
2L(𝑑𝑗)2
wher
e𝑑𝑗>0, and since 𝐹(𝜃)is continuous and bounded from be-
lowby 0,thesequenceconvergestotheminimum(whichcoincides
to stationary point for 𝜃𝑡). Or, we can have that 𝐹(𝜃𝑡+1) −𝐹(𝜃𝑡)
converge to zero for a contiguous subsequence, if 𝑑𝑗converges to
zero for all coordinates, but this would mean that 𝜃𝑡converges
to a stationary point. Either way, we reach a contradiction on the
assumption that 𝜃𝑡does not converge to a stationary/minimum
point, which completes our proof.
PROOF OF PROPOSITION 2 (B.2)
Wederiveheretheboundontheprogressmade(decreaseinobjec-
tive value) for a BGS-GBCD update on 𝜃𝑡
𝑘=0, where each entry
in the block is zero.
We discuss the following preliminaries. Consider vector 𝜃; the
entries of𝜃are separable into 𝑝prespecified blocks and 𝜃𝑘cor-
responds to the sub-vector of entries of 𝜃in block𝑘. We define
𝑠𝑛𝑜𝑟𝑚 (𝑥)as the sum of the ℓ2-norms of each block in 𝑥, i.e.
𝑠𝑛𝑜𝑟𝑚 (𝑥)=𝑝∑
𝑘=1∥𝑥∥2. (10)
We have that the dual of this norm is equal to:
𝑠𝑛𝑜𝑟𝑚 (𝑧)∗=𝑚𝑎𝑥(∥𝑧1∥,∥𝑧2∥,..., ∥𝑧𝑝∥). (11)
We want to show:
𝐹(𝜃𝑡+1) −𝐹(𝜃𝑡) ≤ min
𝛾∈R𝑛∇𝑓(𝜃𝑡)⊺𝛾
+L
2𝑠
𝑛𝑜𝑟𝑚 (𝛾) +𝜆∥𝜃𝑡+𝛾∥1−𝜆∥𝜃𝑡∥1,(12)
We have that:
(𝜃𝑡+1−𝜃𝑡)⊺∇𝑓(𝜃𝑡) +L
2∥𝜃𝑡+1−𝜃𝑡∥2
2+𝜆∥𝜃𝑡+1∥1−𝜆∥𝜃𝑡∥1
Let𝐵𝑘b
e the indicies in 𝜃that correspond to block 𝜃𝑘. If block
𝜃𝑡
𝑘=0was selected via the BGS rule and updated to obtain 𝜃𝑡+1,
we have that:
𝐹(𝜃𝑡+1) −𝐹(𝜃𝑡)
≤∑
𝑗∈𝐵𝑘[
(𝜃𝑡+1
𝑗−𝜃𝑡
𝑗)∇𝑓(𝜃𝑡
𝑗) +L
2(𝜃𝑡+1
𝑗−𝜃𝑡
𝑗)2+𝜆(
|𝜃𝑡+1
𝑗| − |𝜃𝑡
𝑗|)]
And since𝜃𝑡
𝑘=0, we follow the steps from [12] (Lemma 9) to
obtain:
𝐹(𝜃𝑡+1) −𝐹(𝜃𝑡) ≤ −1
2L∑
𝑗∈𝐵𝑘𝑑2
𝑗=−1
2L∥𝑑𝑘∥2
2(13)No
w, consider this expression:
∇𝑓(𝜃𝑡)⊺𝛾+L
2𝑠
𝑛𝑜𝑟𝑚 (𝛾) +𝜆∥𝜃𝑡+𝛾∥1−𝜆∥𝜃𝑡∥1,
where𝛾∈R𝑛. We follow the algebra in [12] (Lemma 8)2to obtain:
∇𝑓(𝜃𝑡)⊺𝛾+L
2𝑠
𝑛𝑜𝑟𝑚 (𝛾)+𝜆∥𝜃𝑡+𝛾∥1−𝜆∥𝜃𝑡∥1≥𝑑⊺𝛾+L
2𝑠
𝑛𝑜𝑟𝑚 (𝛾)
We minimize both sides of the inequality with respect to 𝛾to
obtain.
min 𝛾[
∇𝑓(𝜃𝑡)⊺𝛾+L
2𝑠
𝑛𝑜𝑟𝑚 (𝛾) +𝜆∥𝜃𝑡+𝛾∥1−𝜆∥𝜃𝑡∥1]
≥min 𝛾[
𝑑⊺𝛾+L
2𝑠
𝑛𝑜𝑟𝑚 (𝛾)]
Using convex conjugates we have that
min 𝛾[
𝑑⊺𝛾+L
2𝑠
𝑛𝑜𝑟𝑚 (𝛾)]
=−1
2L(𝑠
𝑛𝑜𝑟𝑚 (𝑑)∗)2
=−1
2L(𝑚
𝑎𝑥(∥𝑑1∥2...∥𝑑𝑝∥2))2
Since𝜃𝑡
𝑘is selected using BGS we have that:
𝑚𝑎𝑥(∥𝑑1∥2...∥𝑑𝑝∥2)=∥𝑑𝑘∥2
and that:
min 𝛾[
∇𝑓(𝜃𝑡)⊺𝛾+L
2𝑠
𝑛𝑜𝑟𝑚 (𝛾)+𝜆∥𝜃𝑡+𝛾∥1−𝜆∥𝜃𝑡∥1]
≥ −1
2L∥𝑑𝑘∥2
2.
(14)
Combining
(13) and (14) yields (12) completing our proof.
BINNING FORMULATION (C)
Given a prespecified set of bins for each feature 𝑥𝑗, we can formal-
izebinningintotheFASTframeworkbyaddingtheconstraintthat
all value of𝛽𝑗within each bin are fit the same value.
ConstRaint 1. Assume each feature 𝑥𝑗, 𝑗∈ [𝑝]has𝑚prespec-
ified sorted bins, 𝑏1...𝑏 𝑚. For each𝑏∈𝑏1...𝑏 𝑚and all pairs of
indicies {𝑖1, 𝑖2} ∈𝑏, we add the constraint that (𝛽𝑗)𝑖1=(𝛽𝑗)𝑖2.
UnconstrainedReformulation: Wereformulatetheseconstraints
into the following unconstrained problem by defining a new set of
decision vectors 𝛽′
𝑗∈R𝑚for𝑗∈ [𝑝]to represent the value fit for
each bin. We define mapping matrices 𝑃𝑗∈ {0,1}𝑛×𝑚such that
(𝑃𝑗)(𝑖,𝑏)=1if(𝑥𝑗)𝑖is in bin𝑏. The unconstrained problem can be
expressed by:
min
𝛽′1
2∥𝑦−𝑝∑
𝑗=1𝑄⊺
𝑗𝑃𝑗𝛽′
𝑗∥2
2+𝜆𝑓𝑝∑
𝑗=1∥𝐷′𝛽′
𝑗∥1,(15)
wher
e𝐷′is the (𝑚−1) ×𝑚differencing matrix. This problem
is equivalent to Problem (1) in the main text with Constraint (1)
added. Problem 15 is block separable and we can apply our BGS-
GBCD algorithm to solve the problem to optimality. We first dis-
cuss block updates below.
2The
only difference between the above expression and the one presented in Lemma
8 of [12] is the choice of the norm on 𝛾. The algebraic steps do not involve the norm
term.
 
1873KDD’24, August 25–29, 2024, Barcelona, Spain. Brian Liu & Rahul Mazumder
Block Update: Given a fixed block 𝑘and residual vector 𝑟=
𝑦−∑
𝑗∈𝛿𝑄⊺
𝑗𝑃𝑗𝛽′
𝑗, we derive that each block update problem can
be expressed by:
min
𝛽′
𝑘1
2∥𝑊𝑗¯𝑟−𝑊𝑗𝛽′
𝑘∥2
2+𝜆𝑓∥𝐷′𝛽′
𝑗∥1,(16)
wher
e¯𝑟∈R𝑚isavectorofthebinmeansof 𝑟,sortedwithrespect
to𝑥𝑗and binned w.r.t to the prespecified bins for 𝑥𝑗and𝑊𝑗∈
R𝑚×𝑚isadiagonalmatrixwiththesquarerootofthecardinalities
of each bin along the main diagonal,√
|𝑏1|.
..√
|𝑏𝑚|.
Each
block update minimizes:
1
2∥𝑄𝑘𝑟−𝑃𝑘𝛽′
𝑘∥2
2+𝜆𝑓∥𝐷′𝛽′
𝑘∥1,
with
respect to𝛽′
𝑘.
Considerthesmoothlossfunction;wecanrewritethelossfunc-
tions by summing over the loss for each bin in 𝑥𝑗.
1
2𝑚∑
ℎ=1∑
𝑖∈𝑏ℎ(𝑟𝑖−
(𝛽′
𝑘)ℎ)2
Expanding the polynomial, we get that
1
2𝑚∑
ℎ=1∑
𝑖∈𝑏ℎ(𝑟2
𝑖−2(𝛽′
𝑘)ℎ𝑟𝑖+
(𝛽′
𝑘)2
ℎ)
=1
2𝑚∑
ℎ=1∑
𝑖∈𝑏ℎ(𝑟2
𝑖−2(𝛽′
𝑘)ℎ𝑟𝑖+
(𝛽′
𝑘)2
ℎ)
=1
2𝑚∑
ℎ=1(∑
𝑖∈𝑏ℎ𝑟2
𝑖−2(𝛽′
𝑘)ℎ∑
𝑖∈𝑏ℎ𝑟𝑖+
|𝑏ℎ|(𝛽′
𝑘)2
ℎ)
We complete the square to get:
=1
2𝑚∑
ℎ=1(
|𝑏ℎ|(
(𝛽′
𝑘)ℎ−1
|𝑏ℎ|∑
𝑖∈𝑏ℎ𝑟𝑖)2
+
K)
where Kdoes not depend of (𝛽′
𝑘)ℎ.
We have1
|𝑏ℎ|∑
𝑖∈𝑏ℎ𝑟𝑖=¯𝑟ℎ,
i.e
., the mean of 𝑟for binℎ.
Combining this with the expression above, we have:
=1
2𝑚∑
ℎ=1(√
|𝑏ℎ|
(𝛽′
𝑘)ℎ−√
|𝑏ℎ|¯𝑟ℎ)2
+
K.
This is equivalent to Problem 16, since Kdoes not depend on
𝛽′
𝑘. We can restrict our binning procedure so that all bins have the
same cardinality:
|𝑏1|=|𝑏2|=...|𝑏𝑚|.
In this case, the block update is equivalent to:
min
𝛽′
𝑘1
2∥¯𝑟−𝛽′
𝑘∥2
2+𝜆𝑓
|𝑏1|∥𝐷′𝛽′
𝑗∥1, (17)
This
1D-FLSA can be solved using dynamic programming to com-
plete a block update.Block Selection: Next, we derive our BGS block selection rule.
Assumethateachfeatureisbinnedintoequally-sizedbins 𝑏1...𝑏 𝑚.
We start with our unconstrained formulation in Problem 15.
min
𝛽′1
2∥𝑦−𝑝∑
𝑗=1𝑄⊺
𝑗𝑃𝑗𝛽′
𝑗∥2
2+𝜆𝑓𝑝∑
𝑗=1∥𝐷′𝛽′
𝑗∥1,
A
gain, let𝜃′
𝑗∈R𝑚−1be contain the successive differences of
𝛽′
𝑗. We again use our implicit reformulation matrices to rexpress
Problem 15 into this form:
min
𝜃′1
2∥𝑦−𝑀
𝑄⊺𝑃𝐴𝜃′∥2
2+𝜆𝑓∥𝜃′∥1,
where:
𝑃=𝑃1 0 0 0 0
0𝑃2 0 0 0
...............
0 0 0 𝑃𝑝−1 0
0 0 0 0 𝑃𝑝.
We can apply our BGS rule on this reformulated problem. We
have that the gradient of the smooth loss function here is equiva-
lent too:
−𝐴⊺𝑃⊺𝑄𝑀⊺𝑟′,
where r’ is the residual. This can be computed quickly for each
block j by sorting 𝑟′with respect to 𝑥𝑗computing bin sums and
taking a rolling sum down the bin sums.
GROUPℓ0-FAST THRESHOLDING (D)
Here, we derive the thresholding procedure for the block updates
in groupℓ0-FAST. Recall the optimization objective is given by:
min
𝛽1
2∥𝑦−𝑝∑
𝑗=1𝑄⊺
𝑗𝛽𝑗∥2
2+𝜆𝑓𝑝∑
𝑗=1∥𝐷
𝛽𝑗∥1+𝜆𝑠𝑝∑
𝑗=1/x31(𝛽𝑗≠0).
(18)
Given fixed block 𝑘and residual vector 𝑟=𝑦−∑
𝑗∈𝛿𝑄⊺
𝑗𝛽𝑗, we
write each block update problem as:
min
𝛽𝑘1
2∥𝑄𝑘𝑟−𝛽𝑘∥2
2+𝜆𝑓∥𝐷
𝛽𝑘∥1+𝜆𝑠 /x31(𝛽𝑘≠0).(19)
Let𝛽∗
𝑘be the optimal solution to:
min
𝛽𝑘1
2∥𝑄𝑘𝑟−𝛽𝑘∥2
2+𝜆𝑓∥𝐷
𝛽𝑘∥1.
Consider the following two cases.
Case 1:𝛽𝑘=0The minimum objective value of Problem (19) is
equal to
1
2∥𝑄𝑘𝑟∥2
2=1
2∥𝑟∥2
2, (20)
since𝑄𝑘is
the sorting matrix.
Case 2:𝛽𝑘≠0The minimum objective value of Problem (19) is
equal to:
1
2∥𝑄𝑘𝑟−𝛽∗
𝑘∥2
2+𝜆𝑓∥𝐷
𝛽∗
𝑘∥1+𝜆𝑠 (21)
Combining (20) and (21) we get
1
2∥𝑟∥2
2−1
2∥𝑄𝑘𝑟−𝛽∗
𝑘∥2
2−𝜆𝑓∥𝐷
𝛽∗
𝑘∥1≤𝜆𝑠,
which is the desired thresholding inequality.
 
1874