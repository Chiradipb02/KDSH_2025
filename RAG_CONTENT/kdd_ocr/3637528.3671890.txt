Attacking Graph Neural Networks with Bit Flips:
Weisfeiler and Leman Go Indifferent
Lorenz Kummer
Doctoral School Computer Science,
Faculty of Computer Science,
University of Vienna
Vienna, Austria
lorenz.kummer@univie.ac.atSamir Moustafa
Doctoral School Computer Science,
Faculty of Computer Science,
University of Vienna
Vienna, Austria
samir.moustafa@univie.ac.atSebastian Schrittwieser
Christian Doppler Laboratory for
Assurance and Transparency in
Software Protection,
Faculty of Computer Science,
University of Vienna
Vienna, Austria
sebastian.schrittwieser@univie.ac.at
Wilfried Gansterer
Faculty of Computer Science,
University of Vienna
Vienna, Austria
wilfried.gansterer@univie.ac.atNils Kriege
Research Network Data Science,
Faculty of Computer Science,
University of Vienna
Vienna, Austria
nils.kriege@univie.ac.at
ABSTRACT
Prior attacks on graph neural networks have focused on graph
poisoning and evasion, neglecting the network’s weights and bi-
ases. For convolutional neural networks, however, the risk arising
from bit flip attacks is well recognized. We show that the direct
application of a traditional bit flip attack to graph neural networks
is of limited effectivity. Hence, we discuss the Injectivity Bit Flip
Attack, the first bit flip attack designed specifically for graph neural
networks. Our attack targets the learnable neighborhood aggre-
gation functions in quantized message passing neural networks,
degrading their ability to distinguish graph structures and impair-
ing the expressivity of the Weisfeiler-Leman test. We find that
exploiting mathematical properties specific to certain graph neu-
ral networks significantly increases their vulnerability to bit flip
attacks. The Injectivity Bit Flip Attack can degrade the maximal
expressive Graph Isomorphism Networks trained on graph prop-
erty prediction datasets to random output by flipping only a small
fraction of the network’s bits, demonstrating its higher destructive
power compared to traditional bit flip attacks transferred from con-
volutional neural networks. Our attack is transparent, motivated by
theoretical insights and confirmed by extensive empirical results.
CCS CONCEPTS
•Security and privacy →Security in hardware; Malware and
its mitigation; •Computing methodologies →Neural networks ;
•Mathematics of computing →Graph algorithms.
KEYWORDS
Bit Flip Attacks, Graph Neural Networks
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671890ACM Reference Format:
Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried Gansterer,
and Nils Kriege. 2024. Attacking Graph Neural Networks with Bit Flips: We-
isfeiler and Leman Go Indifferent. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671890
1 INTRODUCTION
Graph neural networks (GNNs) are a powerful machine learning
technique for handling structured data represented as graphs with
nodes and edges. These methods are highly versatile, extending
the applicability of deep learning to new domains such as financial
and social network analysis, medical data analysis, and chem- and
bioinformatics [ 8,16,39,56,64,65]. With the increasing adoption
of GNNs, there is a pressing need to investigate their potential secu-
rity vulnerabilities. Traditional adversarial attacks on GNNs have
focused on manipulating input graph data [ 63] through poisoning
attacks, which result in the learning of a faulty model [ 40,63], or
on evasion attacks, which use adversarial examples to degrade in-
ference. These attacks can be targeted [ 78] or untargeted [ 40,79]
and involve modifications of node features, edges, or the injection
of new nodes [ 55,63]. Targeted attacks degrade the model’s perfor-
mance on a subset of instances, while untargeted attacks degrade
the model’s overall performance [ 75]. A classification of existing
graph poisoning and evasion attacks and defense mechanisms as
well as a repository with representative algorithms can be found in
the comprehensive reviews by Jin et al. [28] and Dai et al. [9].
Previous research has shown that convolutional neural networks
(CNNs) prominent in the computer vision domain are highly sus-
ceptible to Bit Flip Attacks (BFAs) [ 21,47]. These works typically
focus on CNNs to which quantization is applied, e.g., [ 47,48], a
common technique to improve efficiency. Quantized CNNs are nat-
urally more perturbation resistant compared to their unquantized
(i.e., floating-point) counterparts [ 21,49], and thus their degrada-
tion poses a more challenging problem. For example, quantization
 
1428
KDD ’24, August 25–29, 2024, Barcelona, Spain Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried Gansterer, & Nils Kriege
is mentioned by Hong et al . [21] as a viable protection mechanism
against random BFAs. The perturbation resistance of quantized
CNNs largely arises from their numerical representation. Specifi-
cally, using integer representations makes it difficult for a single bit
flip to cause a significant increase in absolute value. For instance,
in an 8-bit integer quantized format, the maximum increase a pa-
rameter can undergo due to a single bit flip is limited to 128. A
flip in the exponent of an IEEE754 32-bit floating-point represen-
tation, however, can increase the parameter by up to 3.4·1038,
causing extreme neuron activation overriding the rest of the ac-
tivations [ 21,37]. Efficient implementation is likewise crucial for
practical applications of GNNs, making it necessary to investigate
the interaction between robustness and efficiency. The practical
relevance of quantizing GNNs has been recognized for applications
such as inference on IoT and edge devices [ 72], reducing energy
consumption for green deep learning [ 67], and in distributed GNN
applications [ 53]. Recent efforts have been made to further advance
quantization techniques for GNNs [ 3,13,76] as well as technical
realizations for their deployment [ 58,76] and further potential
applications exist, e.g., [6, 10, 11].
Prior research on the security of GNNs has mostly overlooked
the potential threat of BFAs [ 28,40,63,66] which directly manipu-
late a target model at inference time, and previous work on BFAs
has not yet explored attacks on quantized GNNs [ 47]. Despite the
known robustness of quantized CNNs to BFAs [ 21,49], which po-
tentially transfers to GNNs as it is mostly rooted in the numerical
representation itself, the only investigations of GNN resilience to
BFAs study bit flips in floating-point representation [ 27,62]. Fur-
thermore, while general BFAs may be transferable to GNNs, they
do not consider the unique mathematical properties of GNNs, al-
though it has been observed that adapting BFAs to the specific
properties of a target network can increase harm [ 59] and that
BFAs can be far from optimal on non-convolutional models [ 20].
We address this research gap by exploring the effects of malicious
perturbations of the trainable parameters of quantized GNNs and
their impact on prediction quality. Specifically, we target the ex-
pressivity of GNNs, i.e., their ability to distinguish non-isomorphic
graphs or node neighborhoods. The most expressive GNNs based on
standard message passing, including the widely-used Graph Isomor-
phism Networks (GINs) [ 68], have the same discriminative power
as the 1-Weisfeiler-Leman test (1-WL) for suitably parameterized
neighborhood aggregation functions [ 43,44,68]. Our attack tar-
gets the quantized parameters of these neighborhood aggregation
functions to degrade the network’s ability to differentiate between
non-isomorphic structures.
1.1 Related work
The security issue of BFAs has been recognized for quantized
CNNs, which are used in critical applications like medical image
segmentation [ 2,74] and diagnoses [ 18,50]. In contrast, the ro-
bustness of GNNs used in safety-critical domains, like medical
diagnoses [ 16,33,39], electronic health record modeling [ 38,56],
or drug development [ 8,34,65], against BFAs has not been suffi-
ciently studied. Qian et al . [47] and Khare et al . [29] distinguish
between targeted and untargeted BFAs similar to the distinctionmade between targeted and untargeted poisoning and evasion tech-
niques. The high volume of related work on BFAs for quantized
CNNs [ 29,47] based on the seminal work by Rakin et al . [48] and
associated BFA defense mechanisms, e.g., [ 29,32,36] published
recently, underscores the need for research in the direction of both
BFA and defense mechanisms for quantized GNNs. As a represen-
tative for these traditional BFA methods we focus on Progressive
Bit Flip Attack [ 48] as most other BFA variants are based on it. We
subsequently refer to this specific BFA as PBFA and use the term
BFA for the broader class of bit flip attacks only. Existing research
on the resilience of GNNs to bit flips focuses exclusively on floating-
point representation: Jiao et al . [27] study random bit faults and
Wu et al . [62] consider a variant of PBFA modified to flip bits in the
exponent of a weight’s floating-point representation.
1.2 Contribution
In a motivating case study, we explore quantized GNNs’ vulnerabil-
ity to PBFA, finding that PBFA does not notably surpass random
bit flips in tasks demanding graph structural discrimination. To
demonstrate that degrading such GNNs is nonetheless possible
in the identified scenario, we introduce the Injectivity Bit Flip At-
tack (IBFA), a novel attack targeting the discriminative power of
neighborhood aggregation functions used by GNNs. Specifically,
we investigate the GIN architecture with 1-WL expressivity, where
this function is injective for suitable parameters [ 68], and which
is integrated in popular frameworks [ 14] and widely used in prac-
tice, e.g., [ 6,17,61,70]. IBFA, unlike existing BFAs for CNNs, has a
unique bit-search algorithm optimization goal and input data selec-
tion strategy. It differs from graph poisoning and evasion attacks
as it leaves input data unmodified. We establish a theoretical basis
for IBFA, and support it by empirical evidence of its efficacy on
real-world datasets. IBFA outperforms baselines in degrading pre-
diction quality and requires fewer bit flips to reduce GIN’s output
to randomness, making it indifferent to graph structures.
2 PRELIMINARIES
IBFA targets quantized neighborhood aggregation-based GNNs,
exploiting their expressivity linked to the 1-WL graph isomorphism
test. We begin by summarizing these concepts and introduce our
notation and terminology along the way.
2.1 Graph theory
Agraph𝐺is a pair (𝑉,𝐸)of a finite set of nodes𝑉andedges𝐸⊆
{{𝑢,𝑣}⊆𝑉}. The set of nodes and edges of 𝐺is denoted by 𝑉(𝐺)
and𝐸(𝐺), respectively. The neighborhood of𝑣in𝑉(𝐺)is𝑁(𝑣) =
{𝑢∈𝑉(𝐺)|{𝑣,𝑢}∈𝐸(𝐺)}. If there exists a bijection 𝜑:𝑉(𝐺)→
𝑉(𝐻)with{𝑢,𝑣}in𝐸(𝐺)if and only if{𝜑(𝑢),𝜑(𝑣)}in𝐸(𝐻)for all
𝑢,𝑣in𝑉(𝐺), we call the two graphs 𝐺and𝐻isomorphic and write
𝐺≃𝐻. For two graphs with roots 𝑟∈𝑉(𝐺)and𝑟′∈𝑉(𝐻), the
bijection must additionally satisfy 𝜑(𝑟) =𝑟′. The equivalence classes
induced by≃are referred to as isomorphism types.
A function 𝑉(𝐺)→Σis called a node coloring. Then, a node
colored orlabeled graph (𝐺,𝑙)is a graph𝐺endowed with a node
coloring𝑙. We call𝑙(𝑣)alabel orcolor of𝑣∈𝑉(𝐺). We denote a
multiset by{{...}}.
 
1429Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Leman Go Indifferent KDD ’24, August 25–29, 2024, Barcelona, Spain
2.2 The Weisfeiler-Leman algorithm
Let(𝐺,𝑙)denote a labelled graph. In every iteration 𝑡>0, a node
coloring𝑐(𝑡)
𝑙:𝑉(𝐺)→Σis computed, which depends on the coloring
𝑐(𝑡−1)
𝑙of the previous iteration. At the beginning, the coloring is
initialized as 𝑐(0)
𝑙=𝑙. In subsequent iterations 𝑡>0, the coloring is
updated according to
𝑐(𝑡)
𝑙(𝑣) =Hash
𝑐(𝑡−1)
𝑙(𝑣),{{𝑐(𝑡−1)
𝑙(𝑢)|𝑢∈𝑁(𝑣)}}
, (1)
where Hash is an injective mapping of the above pair to a unique
value in Σ, that has not been used in previous iterations. The Hash
function can, for example, be realized by assigning new consecutive
integer values to pairs when they occur for the first time [ 54].
Let𝐶(𝑡)
𝑙(𝐺) ={{𝑐(𝑡)
𝑙(𝑣)|𝑣∈𝑉(𝐺)}}be the multiset of colors a
graph exhibits in iteration 𝑡. The iterative coloring terminates if
|𝐶(𝑡−1)
𝑙(𝐺)|=|𝐶(𝑡)
𝑙(𝐺)|, i.e., the number of colors does not change
between two iterations. For testing whether two graphs 𝐺and𝐻
are isomorphic, the above algorithm is run in parallel on both 𝐺
and𝐻. If𝐶(𝑡)
𝑙(𝐺)̸=𝐶(𝑡)
𝑙(𝐻)for any𝑡≥0, then𝐺and𝐻are not
isomorphic. The label assigned to a node 𝑣in the𝑡th iteration of
the 1-WL test can be understood as a tree representation of the
𝑡-hop neighborhood of 𝑣, in the sense that each 𝑐(𝑡)
𝑙(𝑣)corresponds
to an isomorphism type of such trees of height 𝑡, see Definition 3.1
and [12, 26, 52] for details.
2.3 Graph neural networks
Contemporary GNNs employ a neighborhood aggregation or mes-
sage passing approach, in which the representation of a node is
iteratively updated through the aggregation of representations of
its neighboring nodes. Upon completion of 𝑘iterations of aggre-
gation, the representation of a node encapsulates the structural
information within its 𝑘-hop neighborhood [ 68]. The𝑘th layer of a
GNN computes the node features h(𝑘)
𝑣formally defined by
a(𝑘)
𝑣=Aggregate(𝑘)
{{h(𝑘−1)
𝑢|𝑢∈𝑁(𝑣)}}
,
h(𝑘)
𝑣=Combine(𝑘)
h(𝑘−1)
𝑣,a(𝑘)
𝑣
.(2)
Initially, h(0)
𝑣are the graph’s node features. For graph level readout,
individual node embeddings are aggregated into a single represen-
tation h𝐺of the entire graph
h𝐺=ReadOut
{{h(𝑘)
𝑣|𝑣∈𝐺}}
. (3)
The choice of Aggregate(𝑘),Combine(𝑘)andReadOut in GNNs
is critical, and several variants have been proposed [68].
2.4 Graph isomorphism network
GIN has the same discriminative power as the 1-WL test in distin-
guishing non-isomorphic graphs [ 68]. A large body of work is de-
voted to GNNs exceeding this expressivity [ 43]. However, neighbor-
hood aggregation is widely used in practice and 1-WL is sufficient
to distinguish most graphs in common benchmark datasets [ 41,77].
As established by Xu et al . [68] , a neighborhood aggregation GNN
with a sufficient number of layers can reach the same discriminativepower as the 1-WL test if both the Aggregate andCombine func-
tions in each layer’s update rule as well as its graph level ReadOut
are injective. GIN achieves this with the update rule
h(𝑘)
𝑣= MLP(𝑘) 
(1 +𝜖(𝑘))·h(𝑘−1)
𝑣 +∑︁
𝑢∈𝑁(𝑣)h(𝑘−1)
𝑢!
(4)
integrating a multi layer perceptron (MLP) into Combine(𝑘), realiz-
ing a universal function approximator on multisets [ 22,23,73]. If
input features are one-hot encodings, an MLP is not needed before
summation in the first layer, since summation is injective in this
case. Graph level readout in GIN is realized via first summing each
layer’s embeddings followed by concatenation (denoted by ∥) of the
summed vectors extracted from GINs’ layers. The resulting graph
level embedding h𝐺can then be used as input for, e.g., an MLP for
subsequent graph level classification tasks.
h𝐺=𝑛
𝑘=0 
∑︁
𝑣∈𝑉(𝐺)h(𝑘)
𝑣!
(5)
2.5 Quantization
Quantization involves either a reduction in the precision of the
weights, biases, and activations within a neural network or the
use of a more efficient representation, resulting in a decrease in
model size and memory utilization [ 31]. In accordance with the
typical set-up chosen in the related work on BFA, see Section 1.1, we
apply scale quantization to map FLOAT32 tensors to the INT8 range.
Such a quantization function Qand its associated dequantization
functionQ−1are
Q(W(𝑙)) =W(𝑙)
𝑞= clip(⌊W(𝑙)/𝑠⌉, 𝑎,𝑏 ),
Q−1(W(𝑙)
𝑞) =bW(𝑙)=W(𝑙)
𝑞×𝑠.(6)
Here, the variable 𝑠denotes the scaling parameter, clip(𝑥,𝑎,𝑏 ) =
min(max(𝑥,𝑎),𝑏)with𝑎and𝑏the minimum and maximum thresh-
olds (also known as the quantization range), ⌊...⌉denotes nearest
integer rounding, W(𝑙)is the weight of a layer 𝑙to be quantized,
W(𝑙)
𝑞its quantized counterpart and c...indicates a perturbation, i.e.,
rounding errors in the case of Equation (6). Similar to other works
on BFA that require quantized target networks, e.g., [ 48], we address
the issue of non-differentiable rounding and clipping functions in
Qby employing Straight Through Estimation (STE) [5].
2.6 Progressive bit flip attack
PBFA [ 48] uses a quantized trained CNN Φand employs progres-
sive bit search (PBS) to identify which bit flips will damage the
CNN most. PBS begins with a single forward and backward pass,
conducting error backpropagation without weight updates on a
randomly selected batch Xof training data with a target vector t. It
then selects the weights linked to the top- 𝑘largest binary encoded
gradients as potential candidates for flipping the associated bits.
These candidate bits are iteratively tested (flipped) across all 𝐿lay-
ers to find the bit that maximizes the difference between the loss
Lof the perturbed and the loss of the unperturbed CNN, whereby
the same loss function is used that was minimized during training,
 
1430KDD ’24, August 25–29, 2024, Barcelona, Spain Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried Gansterer, & Nils Kriege
g
ua b u
hd e u
n
vi j v
ol v
Depth 0Depth 1Depth 2
Figure 1: Example of two non-isomorphic unfolding trees
𝑇(2)(𝑢)̸≃𝑇(2)(𝑣)of height 2 associated with the nodes 𝑢and𝑣.
A function solving a WL-discrimination task for 𝑘= 2must
be able to discriminate 𝑢and𝑣based on the structure of their
unfolding trees.
e.g., (binary) cross entropy (CE) for (binary) classification.
max
{bW(𝑙)
𝑞}L
Φ(X;{bW(𝑙)
𝑞}𝐿
𝑙=1),t
−L
Φ(X;{W(𝑙)
𝑞}𝐿
𝑙=1),t
(7)
The source of the perturbation c...in Equation (7)are adversarial
bit flips. If a single bit flip does not improve the optimization goal,
PBS is executed again, considering combinations of two or more
bit flips. This process continues until the desired level of network
degradation is achieved or a predefined number of iterations is
reached. Further details on PBFA/PBS can be found in the appendix.
3 INJECTIVITY BIT FLIP ATTACK
In principle, PBFA and potentially most other BFA variants based
on it can be directly ported to GNNs. However, Hector et al . [20]
demonstrate that the high susceptibility of CNNs to BFA is closely
tied to weight sharing in convolutional filters. Further, Hector et al .
[20] show that MLPs are inherently more robust to PBFA than
CNNs due to their different gradient distributions. The absence of
convolutional filters in GNNs, as well as MLPs being an integral
component of certain GNNs such as GIN, motivates the develop-
ment of a specialized attack for GNNs.
In our preliminary case study (see appendix) we examine PBFA’s
effectiveness on various GNN architectures. The case study’s results
indicate that quantized GINs trained on tasks requiring discrimina-
tion of graphs based on their structural properties and thus high
structural expressivity as found in, e.g., drug development, display
a remarkable resilience to PBFA, which in some instances hardly
outperformed random bit flips even after more than 2·103bit flips.
Considering contemporary literature [ 60,71] postulates an upper
limit of 24 to 50 precise bit flips to be achievable by an attacker
within a span of several hours, the execution of such a substantial
number of bit flips appears impractical.
Based on these observations, IBFA focuses on degrading GIN
trained on tasks requiring high structural expressivity. The discrim-
inative power of GIN is derived from its MLPs’ ability to represent
injective functions on sets (Section 2.4). Consequently, we design
our attack assuming that in tasks where learning such a discrimi-
native function is crucial, attacking injectivity will lead to a higher
degradation than performing PBFA.
3.1 Expressivity via injective set functions
A GNN based on message passing computing a function 𝐹(𝑘)as out-
put of the𝑘th layer is maximal expressive if 𝐹(𝑘)(𝑢) =𝐹(𝑘)(𝑣)⇐⇒𝑐(𝑘)
𝑙(𝑢) =𝑐(𝑘)
𝑙(𝑣). This is achieved when each layers’ Combine and
Aggregate functions are both injective, such that their combina-
tion is injective as well.
We develop the theory behind a bit flip attack exploiting this
property. We formally define the concept of an unfolding tree also
known as computational tree in the context of GNNs [ 12,26], see
Figure 1 for an example.
Definition 3.1 (Unfolding Tree [ 12]).The unfolding tree 𝑇(𝑘)(𝑣)of
height𝑘of a vertex𝑣is defined recursively as
𝑇(𝑘)(𝑣) =(
Tree(𝑙(𝑣)) if𝑘= 0,
Tree(𝑙(𝑣),𝑇(𝑘−1)(𝑁(𝑣))) if𝑘>0,
where Tree (𝑙(𝑣))is a tree containing a single node with label 𝑙(𝑣)
andTree (𝑙(𝑣),𝑇(𝑘−1)(𝑁(𝑣)))is a tree with a root node labeled 𝑙(𝑣)
having the roots of the trees 𝑇(𝑘−1)(𝑁(𝑣)) ={𝑇(𝑘−1)(𝑤)|𝑤∈𝑁(𝑣)}
as children.
Unfolding trees are a convenient tool to study the expressivity
of GNNs as they are closely related to the 1-WL colors.
Lemma 3.2 ( [ 12]).Let𝑘≥0and𝑢,𝑣nodes, then 𝑐(𝑘)
𝑙(𝑢) =
𝑐(𝑘)
𝑙(𝑣)⇐⇒𝑇(𝑘)(𝑢)≃𝑇(𝑘)(𝑣).
Xu et al . [68] show that GIN can distinguish nodes with different
WL colors. The result is obtained by arguing that the MLP in Equa-
tion(4)is a universal function approximator [ 23] allowing to learn
arbitrary permutation invariant functions [ 73]. This includes, in
particular, injective functions. The complexity of GNNs regarding
depth, width, and numerical precision required for this, however,
remains poorly understood and is a focus of recent research [1].
We investigate the functions of a GIN layer and their contribution
to the expressivity of the final output, guiding the formulation of
effective attacks. This requires determining whether our focus is
on the expressivity of the general function on nodes or graphs in
inductive learning or distinguishing the elements of a predefined
subset in transductive settings. First, we consider the general case,
where a finite-depth GNN processes all possible finite graphs and
then discuss its implication for a concrete graph dataset. For the
simplicity, we limit the discussion to unlabeled graphs.
Let𝑓(𝑖):M(R𝑑𝑖−1)→R𝑑𝑖be the learnable function of the 𝑖th
layer of a GNN, where M(𝑈)are all possible pairs (𝐴,A)with
𝐴∈𝑈andAa countable multisets of elements from 𝑈. We assume
that𝑓(𝑖)is invariant regarding the order of elements in the multiset
A. Then the output of the network for node 𝑣is obtained by the
recursive function
𝐹(𝑘)(𝑣) =𝑓(𝑘)
𝐹(𝑘−1)(𝑣),{{𝐹(𝑘−1)(𝑤)|𝑤∈𝑁(𝑣)}}
(8)
with𝐹(0)uniform initial node features. Clearly, if all 𝑓(𝑖)are injec-
tive, WL expressivity is reached as argued by Xu et al . [68] . The
following proposition (proof in appendix) makes explicit that it
suffices that all 𝑓(𝑖)are injective with respect to the elements of
their domain that represent (combinations of) unfolding trees of
height𝑖−1.
Proposition 3.3. Consider two arbitrary nodes 𝑢and𝑣in an
unlabeled graph. Let J0be a uniform node feature and J𝑖={𝑓(𝑖)(𝑥)|
 
1431Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Leman Go Indifferent KDD ’24, August 25–29, 2024, Barcelona, Spain
4𝑎
5𝑑5𝑐
9
𝑗5𝑒
5𝑓4𝑏7ℎ6𝑔8𝑖
(a) Graph with final node coloring/embedding
𝑎𝑏𝑐𝑑𝑒𝑓𝑔ℎ𝑖𝑗
𝐹(0)0 0 0 0 0 0 0 0 0 0
𝐹(1)1 1 2 2 2 2 2 2 3 4
ˆ𝐹(1)2 2 2 2 2 2 2 2 2 3
𝐹(2)=ˆ𝐹(2)4 4 5 5 5 5 6 7 8 9
(
b) Node colorings/embeddings at different layers
Figure 2: Exemplary results of the 2-layer GNNs 𝐹(2)and ˆ𝐹(2)
using𝑓(𝑖)and ˆ𝑓(𝑖), respectively, for 𝑖∈{1,2}. Nodes having
the same embedding are shown in the same color and are
labeled with the same integer. Although ˆ𝑓(1)is non-injective
and ˆ𝐹(1)is coarser than 𝐹(1), we have𝐹(2)=ˆ𝐹(2). The final
output corresponds to the WL coloring.
𝑥∈M (J𝑖−1)}the image under 𝑓(𝑖)for𝑖>0. Then
∀𝑖≤𝑘:∀𝑥,𝑦∈M (J𝑖−1):𝑓(𝑖)(𝑥) =𝑓(𝑖)(𝑦) =⇒𝑥=𝑦 (9)
implies
𝑐(𝑘)
𝑙(𝑢) =𝑐(𝑘)
𝑙(𝑣)⇐⇒𝐹(𝑘)(𝑢) =𝐹(𝑘)(𝑣). (10)
This result extends to graphs with discrete labels and continu-
ous attributes. The inputs, requiring distinct outputs from a GIN
layer to achieve WL expressivity, indicate weak points for poten-
tial attacks. These inputs are in 1-to-1 correspondence with the
unfolding trees. As 𝑖increases, the number of unfolding trees and
the discriminative complexity for GIN’s MLPs grows. However,
Proposition 3.3 provides only a sufficient condition for WL expres-
sivity. Specifically, in a transductive setting on a concrete dataset,
the variety of unfolding trees is limited by the dataset’s node count.
Additionally, even for a concrete dataset, the function 𝑓(𝑖)at layer𝑖
might be non-injective while 𝐹(𝑖+1)remains maximally expressive,
as illustrated in Figure 2b. This motivates the need for a targeted
attack on injectivity to effectively degrade expressivity, which we
develop below. Further, these considerations lead to the following
exemplary classification tasks.
3.2 Weisfeiler-Leman Discrimination Tasks
To develop an effective attack, we first need to provide a formulation
of Prop. 3.3 that is more suitable to practical machine learning
applications. Thus, we first introduce the following exemplary node
level classification task, which we then extend to the graph level
classification tasks for our experimental evaluation.
Definition 3.4 (WL-discrimination task). Let𝐺be a graph with
labels𝑙. The WL-discrimination task for𝑘inNis to learn a function
𝐹(𝑘)such that𝐹(𝑘)(𝑢) =𝐹(𝑘)(𝑣)⇐⇒𝑐(𝑘)
𝑙(𝑢) =𝑐(𝑘)
𝑙(𝑣)for all𝑢,𝑣∈
𝑉(𝐺).The definition, which in its above form is concerned solely with
node classification based on specific structural features, can easily
be extended to classifying graphs based on their structure.
To define a graph level WL discrimination task, we first extend
Equation (8)with a readout function 𝑓𝐺:A→ R𝑑𝐺, mapping a
countable multiset Aof elements from 𝑈to a real valued vector
representation of 𝐺:
𝐹(𝑘)
𝐺(𝐺) =𝑓𝐺({{𝐹(𝑘)(𝑣)|𝑣∈𝑉(𝐺)}})
Obviously, a GNN computing such a function is maximally expres-
sive if𝐹(𝑘)
𝐺(𝐺𝑖) =𝐹(𝑘)
𝐺(𝐺𝑗)⇐⇒𝐶(𝑘)
𝑙(𝐺𝑖) =𝐶(𝑘)
𝑙(𝐺𝑗)for all𝐺𝑖,𝐺𝑗. It
further follows directly from 𝐶(𝑘)
𝑙(𝐺) ={{𝑐(𝑘)
𝑙(𝑣)|𝑣∈𝑉(𝐺)}}that if
the𝑘th layer’s message passing computation is maximally expres-
sive,𝑐(𝑘)
𝑙(𝑢) =𝑐(𝑘)
𝑙(𝑣)⇐⇒𝐹(𝑘)(𝑢) =𝐹(𝑘)(𝑣), and𝑓𝐺’s injectivity is a
sufficient condition for 𝐹(𝑘)
𝐺(𝐺𝑖) =𝐹(𝑘)
𝐺(𝐺𝑗)⇐⇒𝐶(𝑘)
𝑙(𝐺𝑖) =𝐶(𝑘)
𝑙(𝐺𝑗)
to hold (Proposition 3.5, proof in appendix), which is consistent
with results by Xu et al. [68].
Proposition 3.5. Consider two arbitrary graphs 𝐺𝑖,𝐺𝑗, withA=
{{𝐹(𝑘)(𝑣)|𝑣∈𝑉(𝐺𝑖)}},B={{𝐹(𝑘)(𝑣)|𝑣∈𝑉(𝐺𝑗)}}and𝑐(𝑘)
𝑙(𝑢) =
𝑐(𝑘)
𝑙(𝑣)⇐⇒𝐹(𝑘)(𝑢) =𝐹(𝑘)(𝑣). Then
𝑓𝐺(A) =𝑓𝐺(B) =⇒A =B (11)
implies
𝐹(𝑘)
𝐺(𝐺𝑖) =𝐹(𝑘)
𝐺(𝐺𝑗)⇐⇒𝐶(𝑘)
𝑙(𝐺𝑖) =𝐶(𝑘)
𝑙(𝐺𝑗) (12)
Note that analogous to Proposition 3.3, the injectivity of 𝑓𝐺with
respect to the elements of its domain is only sufficient and not
necessary for WL expressivity. In particular, the sets AandB
have a specific structure as they represent the unfolding trees of
graphs. Hence, non-injective realization of 𝑓𝐺might still suffice to
distinguish graphs according to Equation (12).
Definition 3.6 (GLWL-discrimination task). Let𝐷be a set of
node labeled graphs and let 𝐺𝑖,𝐺𝑗∈𝐷. The Graph Level WL-
discrimination task for𝑘inNis to learn a function 𝐹(𝑘)
𝐺such
that𝐹(𝑘)
𝐺(𝐺𝑖) =𝐹(𝑘)
𝐺(𝐺𝑗)if and only if 𝐶(𝑘)
𝑙(𝐺𝑖) =𝐶(𝑘)
𝑙(𝐺𝑗)for all
𝐺𝑖,𝐺𝑗∈𝐷after𝑘iterations of the WL algorithm.
According to Propositions 3.3 and 3.5, the injectivity of 𝑓(𝑖)and
𝑓𝐺is sufficient for achieving WL-level expressivity. Therefore, if a
GNN has successfully learned a function 𝐹(𝑘)
𝐺for a GLWL discrimi-
nation task, any BFA targeting the expressivity of such a GNN will
likely compromise the injectivity of either 𝑓𝐺or𝑓(𝑖).
Real-world graph datasets, however, rarely satisfy the strict con-
ditions outlined in Definition 3.4 and 3.6 as classification targets
used for supervised training typically do not perfectly align with
the node’s WL colors or the isomorphism types of the dataset. Con-
sequently, we formulate a relaxation of Definition 3.6 based on the
Jaccard distance for multisets.
Definition 3.7 (Multiset Jaccard distance [ 46]).Let𝐴,𝐵 be mul-
tisets in universe 𝑈and𝑚be the multiplicity function of multi-
sets. The multiset Jaccard distance is then defined as 𝐽𝑚(𝐴,𝐵) =
 
1432KDD ’24, August 25–29, 2024, Barcelona, Spain Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried Gansterer, & Nils Kriege
1−P
𝑥∈𝑈min(𝑚𝐴(𝑥),𝑚𝐵(𝑥))P
𝑥∈𝑈max(𝑚𝐴(𝑥),𝑚𝐵(𝑥)), where𝑚𝐴(𝑥)and𝑚𝐵(𝑥)represent the
multiplicity of element 𝑥in multisets 𝐴and𝐵, respectively.
Definition 3.8 ( 𝜀-GLWL-discrimination task). Let𝐷=S
𝑐∈𝐶𝐷𝑐
be a set of node labeled graphs with class labels 𝐶, where𝐷𝑐are
the graphs of class 𝑐∈𝐶. The𝜀-GLWL-discrimination task for𝑘in
Nis to learn a function 𝐹(𝑘)
𝐺such that for all 𝑐∈𝐶and all𝐺𝑖,𝐺𝑗
in𝐷𝑐={𝐺1,𝐺2,...,𝐺𝑛}it holds that 𝐹(𝑘)
𝐺(𝐺𝑖) =𝐹(𝑘)
𝐺(𝐺𝑗)if and
only if𝑆𝑐≤𝜀with𝑆𝑐=1
𝛿𝑐P𝑛−1
𝑖=1P𝑛
𝑗=𝑖+1𝐽𝑚(𝐶(𝑘)
𝑙(𝐺𝑖),𝐶(𝑘)
𝑙(𝐺𝑗))for
some𝜀∈R,0≤𝜀<1after𝑘iterations of the WL algorithm and
𝛿𝑐= |𝐷𝑐|
2denoting the number of unique pairs in 𝐷𝑐.
Definition 3.8 captures a broader trend regarding structural dis-
crimination in the learning task, as it permits some graphs with
the same classification target to exhibit a degree (specified by 𝜀) of
structural dissimilarity. Likewise, it permits graphs with different
targets to show a degree of structural similarity. Nonetheless, any
𝜀<1still requires 𝐹(𝑘)
𝐺to learn to distinguish certain substructures,
for which injectivity of 𝑓(𝑘)as well as𝑓𝐺with respect to an accord-
ingly constrained domain remains a sufficient condition. Note that
Definition 3.8 provides per-class formulation. We use a per-dataset
extension1
|𝐶|P
𝑐∈𝐶𝑆𝑐≤𝜀in Figure 5 for ease of display.
3.3 Targeting injectivity
It would not suffice to consider the injectivity of a single layer’s
Combine andAggregate functions for an attack, as expressivity
could be restored at deeper layers (Section 3.1, Figure 2b). Thus, to
target the injectivity sufficent for ( 𝜀-GL)WL-discrimination tasks
as per Definition 3.4- 3.8 while considering the entire model, we
reformulate the target of PBFA from a maximization Equation (7)
to a minimization problem
min
{bW(𝑙)
𝑞}L
Φ(X𝑎;{bW(𝑙)
𝑞}𝐿
𝑙=1),Φ(X𝑏;{bW(𝑙)
𝑞}𝐿
𝑙=1)
.(13)
That is, instead of increasing, e.g., the original classification loss
of the model Φvia PBS, we use PBS to minimize the difference
between the outputs of the network computed on two different
inputs X𝑎, and X𝑏w.r.t. the functionLthat measures the difference
between the network’s outputs. This connects with our theoretical
framework via ( 𝜀-GL)WL-discrimination tasks, in which classifica-
tion targets (tend to) correlate with graph structural (dis)similarity.
Thus, inducing bit flips that exploit this tendency will potentially
impair the injective mappings facilitated by the GNNs’ Aggregate
andCombine functions. Naturally, such an attack strategy neces-
saites careful selection of loss function and input data samples,
which we discuss in detail below. This approach further allows us
to perform IBFA on unlabeled data.
3.3.1 Choosing the loss function. In a binary graph classification
task, the network’s outputs y𝑎= Φ(X𝑎;{bW(𝑙)
𝑞}𝐿
𝑙=1)as well as y𝑏=
Φ(X𝑏;{bW(𝑙)
𝑞}𝐿
𝑙=1)both are𝑛×1vectors representing the probability
mass functions (PMF) of 𝑛Bernoulli distributed discrete random
variables. For such distributed output vectors, any differentiable
𝑝-norm-based loss function would suffice to converge predictions
in the sense of Equation (13)and we choose L1 for Lfor simplicity.
In non-binary graph classification (i.e., multiclass-classification) or
Figure 3: IBFA1/2’s integration of PBS and input data selec-
tion strategies for 𝑘attack iterations. In the first attack itera-
tion, input data selection of IBFA1 and IBFA2 are identical.
multitask binary classification, however, outputs Y𝑎andY𝑏are not
𝑛×1vectors but instead 𝑛×𝑚matrices where 𝑛is the number
of samples and 𝑚the number of classes/tasks. That is, for each
of the𝑛samples, each column in Y𝑎and Y𝑏represents a PMF
over𝑚classes. Thus, simply using a 𝑝-norm-based loss function
as L1 forLin(13)would fail to capture differences in individual
class probabilities contained in Y𝑎and Y𝑏due to the reduction
operation required by L1 (e.g., mean or sum over 𝑚). We solve
this by, instead of L1, employing the discrete pointwise Kullback-
Leibler-Divergence [ 30] (KL) asL, i.e., the KL between the output’s
𝑛probability distributions of each pair of samples (data points) in
Y𝑎andY𝑏, which, in the context of (13), allows IBFA to find bits
converging the PMF of Y𝑎best on Y𝑏.
3.3.2 Choosing input samples. The proper selection of X𝑎andX𝑏
is crucial as selecting inputs that have identical outputs (e.g., two
batches that contain different samples of the same classes in the
same order) before the attack will not yield any degradation as
Equation (13)would already be optimal. Thus we chose inputs X𝑎
andX𝑏to be maximally different from one another w.r.t. to the
unperturbed network’s outputs by
arg max
{X𝑎,X𝑏}L
Φ(X𝑎;{W(𝑙)
𝑞}𝐿
𝑙=1),Φ(X𝑏;{W(𝑙)
𝑞}𝐿
𝑙=1)
. (14)
This search mechanism can be executed before the attack and the
found X𝑎,X𝑏reused for all attack iterations, a variant of IBFA to
which we refer as IBFA1. However, after each attack iteration, the
solution of (14)might change, making it promising to recompute
X𝑎,X𝑏on the perturbed model before every subsequent attack
iteration. We refer to the IBFA employing the latter data selection
strategy as IBFA2. While IBFA2 may lead to slightly faster and
more consistent degradation for a set amount of bit flips, its time
complexity of Θ(𝑘𝑛2)for𝑘attack runs and 𝑛samples makes it
less suitable for large datasets. An overview of IBFA1 and IBFA2 is
provided in Figure 3.
3.4 Assumptions and threat model
Our work builds on several fundamental assumptions which are in
line with previous work on BFA-based attacks on various types of
neural networks. Following previous literature on BFAs for CNNs,
we assume our target network is INT8 quantized [ 36,48,49,71], as
such configured networks are naturally noise resistant [49].
Furthermore, we adopt the usual premise that the attacker has
the capability to exactly flip the bits chosen by the bit-search al-
gorithm through mechanisms such as RowHammer [ 45], variants
 
1433Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Leman Go Indifferent KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 4: Pre- (clean) and post-attack test quality metrics AP, AUROC or ACC for different BFA variants on a 5-layer GIN
trained on six ogbg-mol and two TUDataset datasets, number of bit flips, averages of 10 runs.
Figure 5: Jaccard distances 𝐽𝑚(𝐶(𝑘)
𝑙(𝐺𝑖),𝐶(𝑘)
𝑙(𝐺𝑗)),𝑖̸=𝑗, averaged over all graphs 𝐺𝑖,𝐺𝑗and tasks (classes) in each randomly drawn
sample of 3200 graphs per dataset, indicating 𝜀-GLWL-discrimination tasks. The dotted lines distinguish molecular and social
datasets for various numbers of WL-iterations ( 𝑘) from 1 to 7 and suggest possible choices for 𝜀.
thereof [ 35,71] or others [ 7,24]. The feasibility of BFAs inducing
exact bit flips via a RowHammer variant was shown by, e.g., [ 60].
We thus do not discuss the detailed technical specialities of realizing
the flips of the identified vulnerable bits in hardware.
Moreover, we assume a gray-box attack model in which the
attacker’s goal is to crush a trained and deployed quantized GNN
via BFA. It represents a relaxation of some of the restrictions found
in a black-box model by allowing the attacker to have certain prior
knowledge about the training data and the general structure of the
model. This is again a typical assumption also made in previous
work [ 36]. An attacker could easily obtain required information
in many typical application scenarios, for instance, if the target
model is a publicly available pre-trained model or the attacker has
access to an identical instance of a device on which model inference
is performed. Even in the absence of such a priori knowledge on
the model, parameters and input data might be acquired through
methods such as side-channel attacks [4, 69].
4 EXPERIMENTS
Our experimental framework is designed to investigate the fol-
lowing key research questions, focusing on real-world molecu-
lar property prediction datasets, a task common in drug develop-
ment [51, 65], as well as social network classification.
RQ1 Is IBFA more destructive than other BFAs?
RQ2 Does the destructiveness of IBFA depend on whether a task
requires high structural expressivity (i.e., is an 𝜀-GLWL-
discrimination task)?RQ3 Does IBFA’s required number of bit flips fall within the ac-
cepted realistic budget of 24 to 50, as suggested in related
work [60, 71]?
RQ4 How does IBFA perform in scenarios with limited data avail-
able for selection?
We assess IBFA relative to PBFA, which we consider the most
relevant baseline, as most other, more specialized (e.g., targeted)
BFAs designed to degrade CNNs have been derived from PBFA. We
measured the degradation in the quality metrics proposed by Open
Graph Benchmark (OGB) [25] or TUDataset [42], respectively, for
each of the datasets and followed the recommended variant of a
5-layers GIN with a virtual node. For thoroughness, we include a
detailed ablation study concerning loss functions and layer prefer-
ences, which can be found in the appendix. This study also covers
experiments on GNNs less expressive than GIN and IBFA’s rela-
tion to certain BFA defenses. To ensure reproducibility, we provide
details on quantized models, measured metrics and attack configu-
ration and a code repository1.
4.1 Quantized models
We obtained INT8 quantized models by training on each dataset’s
training split using STE, see Section 2.5. We used the Adam op-
timizer with a learning rate of 10−3and trained the models for
30 epochs. Although more complex models and quantization tech-
niques might achieve higher prediction quality, our focus was not
on improving prediction quality beyond the state-of-the-art, but on
demonstrating GIN’s vulnerability to IBFA. Some of the datasets
we used present highly challenging learning tasks, and our results
1https://github.com/lorenz0890/ibfakdd2024
 
1434KDD ’24, August 25–29, 2024, Barcelona, Spain Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried Gansterer, & Nils Kriege
Figure 6: Pre- (clean) and post-attack test quality metrics AP, AUROC or ACC for IBFA with different data selection strategies
(random, IBFA selection from 1% random subsets and full training splits) on a 5-layer GIN trained on 6 ogbg-mol and 2 TUDataset
datasets, averages of 10 runs. IBFA1 in top row, IBFA2 in bottom row, number of bit flips as reported in Figure 4.
for quantized training of GIN are comparable to those by OGB [ 25]
forFLOAT32 training.
4.2 Datasets
Six benchmark datasets are chosen (as in, e.g., [ 15,57]) from graph
classification tasks from OGB based on MoleculeNet [ 64] for evalua-
tion as well as COLLAB andGITHUB_STARGAZERS from TUDataset [ 42].
The goal in each OGB dataset is to predict properties based on molec-
ular graph structures, such that the datasets are consistent with the
underlying assumptions of IBFA described in Section 3. All OGB
datasets are split using a scaffold-based strategy, which seeks to sep-
arate structurally different molecules into different subsets [ 25,64].
COLLAB is derived from scientific collaboration networks, whereby
every graph represents the ego-network of a scientist, and the task
is to predict their area of research. GITHUB_STARGAZERS contains
graphs of social networks of GitHub users, and the task is to predict
whether they starred popular machine learning or web development
repositories. COLLAB andGITHUB_STARGAZERS are split randomly
(80/10/10 for train/test/validation).
Not all targets in the OGB datasets apply to each molecule (miss-
ing targets are indicated by NaNs) and we consider only existing
targets in our experiments. Area under the receiver operating char-
acteristic curve (AUROC), average precision (AP) or accuracy (ACC)
are used to measure the models’ performance as recommended
by Hu et al. [25] and Morris et al. [42], respectively.
4.3 Attack configuration
The attacks in each of the experiments on a GIN trained on a dataset
were executed with the number of attack runs (see Section 2.6) ini-
tially set to 5and repeated with the number of attacks incremented
until the first attack type reached (nearly) random output. The other
attacks in this experiment were then set to that same number of
attack runs to ensure fair comparison. Note that PBFA, IBFA1 and
IBFA2 can flip more than a single bit during one attack run (see
Section 2.6), such that the final number of actual bit flips can vary
across experiments. For the single task binary classification datasets,
ogbg-molhiv ,ogbg-bace andGITHUB_STARGAZERS , IBFA1/2 were
used with𝐿1loss, for multitask binary classification ogbg-tox21 ,
ogbg-toxcast ,ogbg-molmuv ,ogbg-pcba and multiclass classifica-
tion ( COLLAB ), IBFA1/2 were used with KL loss. For PBFA, binary CE
(BCE) loss was used throughout the binary classification datasets
Figure 7: Pre- (clean) and post-attack test quality metrics
AP, AUROC or ACC for IBFA with different loss functions
(L1 vs. KL loss, IBFA selection from 1% random subset) on a
5-layer GIN trained on 4 ogbg-mol datasets TUDataset COLLAB ,
number of bit flips, averages of 10 runs. IBFA1 in top row,
IBFA2 in bottom row.
and CE loss was used for COLLAB . Input samples for all evaluated
BFA variants were taken from the training splits.
4.4 Results
Both IBFA variants surpass random bit flips (RBFA) and PBFA in
terms of test quality metric degradation for a given number of bit
flips in most examined cases (Figure 4) (RQ1). A fine granular visu-
alization of the progression of quality metric degradation induced
by IBFA1/2, PBFA and RBFA is given in Figure. 8. IBFA is capable of
forcing the evaluated GINs to produce (almost) random output
(AUROC≤0.5, AP≤0.11(ogbg-molpcba ), AP≤0.06(ogbg-
molmuv ), ACC≤0.33(COLLAB ) or≤0.5(GITHUB_STARGAZERS ))
by flipping less than 33 bits on average. This is realizable given
the aforementioned upper limit of 24 to 50 precise bit flips an at-
tacker can be expected to achieve [ 60,71] (RQ3). IBFA2 causes
slightly more quality metric degradation on ogbg-molhiv ,COL-
LABandGITHUB_STARGAZERS than IBFA1 but is surpassed by or
on par with IBFA1 in all other cases. IBFA2 on ogbg-molbace and
IBFA1 on COLLAB were slightly weaker than PBFA. However, on
ogbg-molbace PBFA requires 33% more bit flips to achieve results
compareable to IBFA1. On GITHUB_STARGAZERS , PBFA and IBFA
both degraded GIN equally. GINs trained on ogbg-molmuv ,ogbg-
molhiv , and ogbg-moltox21 were barely affected by PBFA for the
examined number of bit flips and GIN trained on ogbg-moltoxcast
 
1435Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Leman Go Indifferent KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 8: Probability of selecting a layer for a bit flip in a 5-
layer GIN trained on 3 small datasets, averaged over 10 runs
for PBFA or IBFA1 (top row), progression of degradation of
for different BFA variants (bottom row).
appeared to be entirely impervious to PBFA. Our quantized GNNs
resist RBFA, ruling out our observations are stochastic.
These results are consistent with our definition of 𝜀-GLWL-
discrimination tasks as Figure 5 indicates that, for suiteable choices
of𝜀, the examined molecular graph property prediction datasets
from OGB could be distinguished from the social network classifica-
tion datasets from TUDatset. That is, the OGB datasets demonstrate
a stronger connection between class membership and structural
graph similarity and thus, GINs trained on them are more vulnera-
ble to IBFA. Together with our main results (Figure 4), which show
that PBFA either causes less degradation in the GINs trained on
the OGB datasets than IBFA or requires more flips to do so, we
interpret these findings as empirical validation of our theoretical
prediction that IBFA’s destructiveness surpasses that of PBFA in
tasks demanding high structural expressivity (RQ2). The proposed
data selection strategies for IBFA1/2 provide an improvement over
random data selection (Figure 6) and IBFA1/2 remain highly de-
structive (typically more destructive than PBFA), even when limited
to a significantly constrained sample (1% random subsets of the
datasets’ training splits) for selecting input data points (RQ4).
While our setup did not permit the weaker BFA in an experiment
to continue flipping bits until the network was fully degraded, our
preliminary case study (appendix) revealed PBFA required 953 bit
flips to fully degrade GIN on ogbg-molhiv and failed to degrade
GIN on ogbg-moltoxcast even after 2662 bit flips. For comparison,
IBFA1/2 could fully degrade GIN on both ogbg-molhiv andogbg-
moltoxcast using two orders of magnitude fewer bit flips (Figure 4).
Ablation experiments, layer preferences, node classification. Fig-
ure 7 illustrates the results if L1 loss is used instead of KL loss in
the multi-task binary classification setting. As can be seen from
Figure 7, IBFA1/2 both fail to outperform PBFA on the multi-task
binary classification datasets if L1 loss is used instead of KL loss,
which is in line with our analytical results in Section 3.3.1. As
in Figure 6, 1% subset sampling was used for IBFA to accelerate
the experiments. Experiments on obg-molbace ,ogbg-molhiv and
GITHUB_STARGAZERS are not included in this experiment as we al-
ready used L1 loss in our original experiments with these datasets.
We recorded the probabilities associated with an attack’s selec-
tion of a specific layer within the evaluated 5-layer GIN (Figure 8).
RBFA was configured to exhibit a random uniform distribution of
bit flips across layers and is therefore omitted. PBFA and IBFA1
exhibit a distinct and characteristic layer selection. PBFA typically
Figure 9: Pre- (clean) and post-attack test quality metric ACC
for different BFA variants on a 5-layer GIN trained on four
TUDataset node level datasets, number of bit flips, averages
of 10 runs/folds.
confines bit flips to only 2 out of the 5 layers and, in line with Hec-
tor et al . [20] ’s findings for CNNs, displays a preference for the
input layer, while IBFA1/2 targets at least 4 layers across the entire
model, with the majority of flips occurring in the learnable aggre-
gation functions of the network (MLP1-4). The variations in layer
selection observed in IBFA1 support our hypotheses: a) introducing
non-injectivity into a single layer alone is insufficient, necessitating
an attack on the overall expressivity of GIN, and b) IBFA1 effectively
targets the learnable neighborhood aggregation functions.
Beyond graph classification, we experimented with two het-
erophilious [ 19] node classification datasets, Texas andWiscon-
sinfrom TUDataset, using the same GIN architecture as in our
graph-level experiments, sans readout. Results, averaged over 10
predefined folds (Figure 9), show that IBFA1/2 heavily degrade per-
formance while PBFA is nearly indistinguishable from RBFA. We
also tested our method on homophilious [ 19] node classification
datasets, Cora andPubMed , reporting 10-run averages. Here, we
observed a different trend: while IBFA1 remained effective, PBFA
degraded accuracy more than RBFA, particularly on Cora . These
results suggest that PBFA’s reliance on pseudo labels in its loss
function makes it less effective when these labels are poor, whereas
IBFA’s effectiveness remains robust regardless of label quality.
5 CONCLUSION
We introduce a novel concept for bit flip attacks on quantized GNNs
which exploits specific mathematical properties of GNNs related
to graph learning tasks requiring graph structural discrimination.
Upon our theory, we design the novel Injective Bit Flip Attack IBFA
and illustrate its ability to render GIN indifferent to graph structures.
IBFA compromises its predictive quality significantly more than
the most relevant BFA ported from CNNs and than random bit
flips on twelve different datasets, covering binary and multiclass
classification of molecular, social and other graphs or nodes.
ACKNOWLEDGEMENTS
This work was supported by the Vienna Science and Technology
Fund (WWTF) [10.47379/VRG19009], the Austrian Federal Ministry
of Labour and Economy, the National Foundation for Research,
Technology and Development and the Christian Doppler Research
Association. We thank Tabea Reichmann for useful discussions.
 
1436KDD ’24, August 25–29, 2024, Barcelona, Spain Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried Gansterer, & Nils Kriege
REFERENCES
[1]Anders Aamand, Justin Chen, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld,
Nicholas Schiefer, Sandeep Silwal, and Tal Wagner. 2022. Exponentially Improv-
ing the Complexity of Simulating the Weisfeiler-Lehman Test with Graph Neural
Networks. In Advances in Neural Information Processing Systems 35. 27333–27346.
[2]Mohammad-Hossein Askari-Hemmat, Sina Honari, Lucas Rouhier, Christian S.
Perone, Julien Cohen-Adad, Yvon Savaria, and Jean-Pierre David. 2019. U-net
fixed-point quantization for medical image segmentation. In Large-Scale Annota-
tion of Biomedical Data and Expert Label Synthesis (LABELS) and Hardware Aware
Learning for Medical Imaging and Computer Assisted Intervention (HAL-MICCAI),
International Workshops. 115–124.
[3]Mehdi Bahri, Gaétan Bahl, and Stefanos Zafeiriou. 2021. Binary graph neural
networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 9492–9501.
[4]Lejla Batina, Shivam Bhasin, Dirmanto Jap, and Stjepan Picek. 2018. CSI neural
network: Using side-channels to recover your artificial neural network informa-
tion. CoRR abs/2204.07697 (2018).
[5]Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. 2013. Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation.
CoRR abs/1308.3432 (2013).
[6]Blaž Bertalanič and Carolina Fortuna. 2023. Graph Isomorphism Networks for
Wireless Link Layer Anomaly Classification. In 2023 IEEE Wireless Communica-
tions and Networking Conference (WCNC). 1–6.
[7]Jakub Breier, Xiaolu Hou, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang Liu.
2018. Practical Fault Attack on Deep Neural Networks. In Proceedings of the 2018
ACM SIGSAC Conference on Computer and Communications Security. 2204–2206.
[8]Mark Cheung and José MF Moura. 2020. Graph neural networks for covid-19
drug discovery. In 2020 IEEE International Conference on Big Data. 5646–5648.
[9]Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Junjie Xu, Zhimeng Guo, Hui Liu,
Jiliang Tang, and Suhang Wang. 2022. A comprehensive survey on trustworthy
graph neural networks: Privacy, robustness, fairness, and explainability. CoRR
abs/2204.08570 (2022).
[10] Austin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Todd Hester,
Luis Perez, Marc Nunkesser, Seongjae Lee, Xueying Guo, Brett Wiltshire, et al .
2021. Eta prediction with graph neural networks in google maps. In Proceedings of
the 30th ACM International Conference on Information & Knowledge Management.
3767–3776.
[11] Guimin Dong, Mingyue Tang, Zhiyuan Wang, Jiechao Gao, Sikun Guo, Lihua Cai,
Robert Gutierrez, Bradford Campbel, Laura E Barnes, and Mehdi Boukhechba.
2023. Graph neural networks in IoT: a survey. ACM Transactions on Sensor
Networks 19, 2 (2023), 1–50.
[12] Giuseppe Alessio D’Inverno, Monica Bianchini, Maria Lucia Sampoli, and Franco
Scarselli. 2021. A unifying point of view on expressive power of GNNs. CoRR
abs/2106.08992 (2021).
[13] Boyuan Feng, Yuke Wang, Xu Li, Shu Yang, Xueqiao Peng, and Yufei Ding. 2020.
SGQuant: Squeezing the Last Bit on Graph Neural Networks with Specialized
Quantization. In 2020 IEEE 32nd international conference on tools with artificial
intelligence (ICTAI). 1044–1052.
[14] Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning with
PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and
Manifolds.
[15] Han Gao, Xu Han, Jiaoyang Huang, Jian-Xun Wang, and Liping Liu. 2022.
PatchGT: Transformer over Non-trainable Clusters for Learning Graph Rep-
resentations. In Learning on Graphs Conference. 1–27.
[16] Jianliang Gao, Tengfei Lyu, Fan Xiong, Jianxin Wang, Weimao Ke, and Zhao Li.
2022. Predicting the Survival of Cancer Patients With Multimodal Graph Neural
Network. IEEE/ACM Transactions on Computational Biology and Bioinformatics
19, 2 (2022), 699–709.
[17] Yun Gao, Hirokazu Hasegawa, Yukiko Yamaguchi, and Hajime Shimada. 2022.
Malware Detection by Control-Flow Graph Level Representation Learning With
Graph Isomorphism Network. IEEE Access 10 (2022), 111830–111841.
[18] Mukhammed Garifulla, Juncheol Shin, Chanho Kim, Won Hwa Kim, Hye Jung
Kim, Jaeil Kim, and Seokin Hong. 2021. A case study of quantizing convolutional
neural networks for fast disease diagnosis on portable medical devices. Sensors
22, 1 (2021), 219.
[19] Jhony H Giraldo, Konstantinos Skianis, Thierry Bouwmans, and Fragkiskos D.
Malliaros. 2023. On the trade-off between over-smoothing and over-squashing
in deep graph neural networks. In Proceedings of the 32nd ACM International
Conference on Information and Knowledge Management. 566–576.
[20] Kevin Hector, Pierre-Alain Moëllic, Mathieu Dumont, and Jean-Max Dutertre.
2022. A Closer Look at Evaluating the Bit-Flip Attack Against Deep Neural
Networks. In 2022 IEEE 28th International Symposium on On-Line Testing and
Robust System Design. 1–5.
[21] Sanghyun Hong, Pietro Frigo, Yiğitcan Kaya, Cristiano Giuffrida, and Tudor
Dumitra s,. 2019. Terminal brain damage: Exposing the graceless degradation
in deep neural networks under hardware fault attacks. In 28th USENIX Security
Symposium (USENIX Security 19). 497–514.[22] Kurt Hornik. 1991. Approximation capabilities of multilayer feedforward net-
works. Neural networks 4, 2 (1991), 251–257.
[23] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer feed-
forward networks are universal approximators. Neural networks 2, 5 (1989),
359–366.
[24] Xiaolu Hou, Jakub Breier, Dirmanto Jap, Lei Ma, Shivam Bhasin, and Yang Liu.
2020. Security evaluation of deep neural network resistance against laser fault in-
jection. In 2020 IEEE International Symposium on the Physical and Failure Analysis
of Integrated Circuits. 1–6.
[25] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets
for machine learning on graphs. In Advances in neural information processing
systems 33. 22118–22133.
[26] Stefanie Jegelka. 2022. Theory of graph neural networks: Representation and
learning. In Proceedings of the International Congress of Mathematicians, Vol. 7.
5450–5476.
[27] Xun Jiao, Ruixuan Wang, Fred Lin, Daniel Moore, and Sriram Sankar. 2022.
PyGFI: Analyzing and Enhancing Robustness of Graph Neural Networks Against
Hardware Errors. CoRR abs/2212.03475 (2022).
[28] Wei Jin, Yaxing Li, Han Xu, Yiqi Wang, Shuiwang Ji, Charu Aggarwal, and
Jiliang Tang. 2021. Adversarial attacks and defenses on graphs. ACM SIGKDD
Explorations Newsletter 22, 2 (2021), 19–34.
[29] Yash Khare, Kumud Lakara, Maruthi S. Inukonda, Sparsh Mittal, Mahesh Chandra,
and Arvind Kaushik. 2022. Design and Analysis of Novel Bit-flip Attacks and
Defense Strategies for DNNs. In 2022 IEEE Conference on Dependable and Secure
Computing. 1–8.
[30] Solomon Kullback and Richard Leibler. 1951. On information and sufficiency.
The annals of mathematical statistics 22 (1951), 79–86.
[31] Lorenz Kummer, Kevin Sidak, Tabea Reichmann, and Wilfried Gansterer. 2023.
Adaptive Precision Training (AdaPT): A dynamic quantized training approach for
DNNs. In Proceedings of the 2023 SIAM International Conference on Data Mining.
559–567.
[32] Jingtao Li, Adnan Siraj Rakin, Zhezhi He, Deliang Fan, and Chaitali Chakrabarti.
2021. RADAR: Run-time Adversarial Weight Attack Detection and Accuracy
Recovery. In 2021 Design, Automation and Test in Europe Conference and Exhibition.
790–795.
[33] Yang Li, Buyue Qian, Xianli Zhang, and Hui Liu. 2020. Graph neural network-
based diagnosis prediction. Big Data 8, 5 (2020), 379–390.
[34] Xuan Lin, Zhe Quan, Zhi-Jie Wang, Tengfei Ma, and Xiangxiang Zeng. 2020.
KGNN: Knowledge Graph Neural Network for Drug-Drug Interaction Prediction.
InProceedings of the Twenty-Ninth International Joint Conference on Artificial
Intelligence, IJCAI-20, Vol. 380. 2739–2745.
[35] Moritz Lipp, Michael Schwarz, Lukas Raab, Lukas Lamster, Misiker Tadesse Aga,
Clémentine Maurice, and Daniel Gruss. 2020. Nethammer: Inducing rowhammer
faults through network requests. In 2020 IEEE European Symposium on Security
and Privacy Workshops. 710–719.
[36] Qi Liu, Jieming Yin, Wujie Wen, Chengmo Yang, and Shi Sha. 2023. NeuroPots:
Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks. In 32nd
USENIX Security Symposium (USENIX Security 23). 6347–6364.
[37] Yannan Liu, Lingxiao Wei, Bo Luo, and Qiang Xu. 2017. Fault injection attack on
deep neural network. In 2017 IEEE/ACM International Conference on Computer-
Aided Design (ICCAD). 131–138.
[38] Zheng Liu, Xiaohan Li, Hao Peng, Lifang He, and S Yu Philip. 2020. Heteroge-
neous similarity graph neural network on electronic health records. In 2020 IEEE
International Conference on Big Data. 1196–1205.
[39] Haohui Lu and Shahadat Uddin. 2021. A weighted patient network-based frame-
work for predicting chronic diseases using graph neural networks. Scientific
reports 11, 1 (2021), 22607.
[40] Jiaqi Ma, Shuangrui Ding, and Qiaozhu Mei. 2020. Towards More Practical Ad-
versarial Attacks on Graph Neural Networks. In Advances in Neural Information
Processing Systems 33. 4756–4766.
[41] Christopher Morris, Matthias Fey, and Nils Kriege. 2021. The Power of the
Weisfeiler-Leman Algorithm for Machine Learning with Graphs. In Proceedings
of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21.
4543–4550.
[42] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel,
and Marion Neumann. 2020. TUDataset: A collection of benchmark datasets for
learning with graphs. In ICML 2020 Workshop on Graph Representation Learning
and Beyond (GRL+ 2020).
[43] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M. Kriege,
Martin Grohe, Matthias Fey, and Karsten Borgwardt. 2023. Weisfeiler and Leman
go Machine Learning: The Story so far. Journal of Machine Learning Research 24,
333 (2023), 1–59.
[44] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric
Lenssen, Gaurav Rattan, and Martin Grohe. 2019. Weisfeiler and leman go neural:
Higher-order graph neural networks. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 33. 4602–4609.
 
1437Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Leman Go Indifferent KDD ’24, August 25–29, 2024, Barcelona, Spain
[45] Onur Mutlu and Jeremie S. Kim. 2019. Rowhammer: A retrospective. IEEE
Transactions on Computer-Aided Design of Integrated Circuits and Systems 39, 8
(2019), 1555–1571.
[46] Javier Parapar and Álvaro Barreiro. 2008. Winnowing-Based Text Clustering. In
Proceedings of the 17th ACM Conference on Information and Knowledge Manage-
ment. 1353–1354.
[47] Cheng Qian, Ming Zhang, Yuanping Nie, Shuaibing Lu, and Huayang Cao. 2023. A
Survey of Bit-Flip Attacks on Deep Neural Network and Corresponding Defense
Methods. Electronics 12, 4 (2023), 853.
[48] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. 2019. Bit-Flip Attack: Crushing
Neural Network With Progressive Bit Search. In Proceedings of the IEEE/CVF
International Conference on Computer Vision and Pattern Recognition. 1211–1220.
[49] Adnan Siraj Rakin, Zhezhi He, Jingtao Li, Fan Yao, Chaitali Chakrabarti, and
Deliang Fan. 2022. T-BFA: Targeted Bit-Flip Adversarial Weight Attack. IEEE
Transactions on Pattern Analysis and Machine Intelligence 44, 11 (2022), 7928–7939.
[50] Henrique De Melo Ribeiro, Ahran Arnold, James P. Howard, Matthew J. Shun-
Shin, Ying Zhang, Darrel P. Francis, Phang B. Lim, Zachary Whinnett, and Mas-
soud Zolgharni. 2022. ECG-based real-time arrhythmia monitoring using quan-
tized deep neural networks: A feasibility study. Computers in Biology and Medicine
143 (2022), 105249.
[51] Ryan A Rossi, Di Jin, Sungchul Kim, Nesreen K. Ahmed, Danai Koutra, and
John Boaz Lee. 2020. On proximity and structural role-based embeddings in
networks: Misconceptions, techniques, and applications. ACM Transactions on
Knowledge Discovery from Data 14, 5 (2020), 1–37.
[52] Till Hendrik Schulz, Tamás Horváth, Pascal Welke, and Stefan Wrobel. 2022. A
Generalized Weisfeiler-Lehman Graph Kernel. Machine Learning 111, 7 (2022),
2601–2629.
[53] Yingxia Shao, Hongzheng Li, Xizhi Gu, Hongbo Yin, Yawen Li, Xupeng Miao,
Wentao Zhang, Bin Cui, and Lei Chen. 2024. Distributed graph neural network
training: A survey. Comput. Surveys 56, 8 (2024), 1–39.
[54] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn,
and Karsten M Borgwardt. 2011. Weisfeiler-lehman graph kernels. Journal of
Machine Learning Research 12, 9 (2011).
[55] Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar.
2020. Adversarial Attacks on Graph Neural Networks via Node Injections: A Hier-
archical Reinforcement Learning Approach. In Proceedings of The Web Conference
2020. 673–683.
[56] Zhenchao Sun, Hongzhi Yin, Hongxu Chen, Tong Chen, Lizhen Cui, and Fan
Yang. 2021. Disease Prediction via Graph Neural Networks. IEEE Journal of
Biomedical and Health Informatics 25, 3 (2021), 818–826.
[57] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. 2021. Adversarial graph
augmentation to improve graph contrastive learning. In Advances in Neural
Information Processing Systems 34. 15920–15933.
[58] Shyam A. Tailor, Javier Fernandez-Marques, and Nicholas D. Lane. 2021. Degree-
Quant: Quantization-Aware Training for Graph Neural Networks. In 9th Interna-
tional Conference on Learning Representations.
[59] Valerio Venceslai, Alberto Marchisio, Ihsen Alouani, Maurizio Martina, and
Muhammad Shafique. 2020. Neuroattack: Undermining spiking neural networks
security through externally triggered bit-flips. In 2020 International Joint Confer-
ence on Neural Networks. 1–8.
[60] Jialai Wang, Ziyuan Zhang, Meiqi Wang, Han Qiu, Tianwei Zhang, Qi Li, Zong-
peng Li, Tao Wei, and Chao Zhang. 2023. Aegis: Mitigating Targeted Bit-flip
Attacks against Deep Neural Networks. In 32nd USENIX Security Symposium
(USENIX Security 23). 2329–2346.
[61] Zhiqiong Wang, Zican Lin, Shuo Li, Yibo Wang, Weiying Zhong, Xinlei Wang,
and Junchang Xin. 2023. Dynamic Multi-Task Graph Isomorphism Network for
Classification of Alzheimer’s Disease. Applied Sciences 13, 14 (2023), 8433.
[62] Bang Wu, Xingliang Yuan, Shuo Wang, Qi Li, Minhui Xue, and Shirui Pan. 2024.
Securing Graph Neural Networks in MLaaS: A Comprehensive Realisation of
Query-based Integrity Verification. In 2024 IEEE Symposium on Security and
Privacy (SP). 110–110.
[63] Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao, and Le Song. 2022. Graph Neural
Networks: Foundations, Frontiers, and Applications.
[64] Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Ge-
niesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. 2018. MoleculeNet: a
benchmark for molecular machine learning. Chemical science 9, 2 (2018), 513–530.
[65] Jiacheng Xiong, Zhaoping Xiong, Kaixian Chen, Hualiang Jiang, and Mingyue
Zheng. 2021. Graph neural networks for automated de novo drug design. Drug
Discovery Today 26, 6 (2021), 1382–1393.
[66] Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K
Jain. 2020. Adversarial attacks and defenses in images, graphs and text: A review.
International Journal of Automation and Computing 17 (2020), 151–178.
[67] Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. 2021. A survey
on green deep learning. CoRR abs/2111.05193 (2021).
[68] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Pow-
erful are Graph Neural Networks?. In 7th International Conference on Learning
Representations.[69] Mengjia Yan, Christopher W Fletcher, and Josep Torrellas. 2020. Cache telepathy:
Leveraging shared resource attacks to learn {DNN}architectures. In 29th USENIX
Security Symposium (USENIX Security 20). 2003–2020.
[70] Sihong Yang, Dezhi Jin, Jun Liu, and Ye He. 2022. Identification of Young High-
Functioning Autism Individuals Based on Functional Connectome Using Graph
Isomorphism Network: A Pilot Study. Brain Sciences 12, 7 (2022), 883.
[71] Fan Yao, Adnan Siraj Rakin, and Deliang Fan. 2020. DeepHammer: Depleting the
intelligence of deep neural networks through targeted chain of bit flips. In 29th
USENIX Security Symposium (USENIX Security 20). 1463–1480.
[72] Jiangchao Yao, Shengyu Zhang, Yang Yao, Feng Wang, Jianxin Ma, Jianwei Zhang,
Yunfei Chu, Luo Ji, Kunyang Jia, Tao Shen, et al .2022. Edge-cloud polarization and
collaboration: A comprehensive survey for ai. IEEE Transactions on Knowledge
and Data Engineering 35, 7 (2022), 6866–6886.
[73] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabás Póczos, Ruslan
Salakhutdinov, and Alexander J. Smola. 2017. Deep Sets. In Advances in Neural
Information Processing Systems 30. 3391–3401.
[74] Rongzhao Zhang and Albert C. S. Chung. 2021. MedQ: Lossless ultra-low-bit
neural network quantization for medical image segmentation. Medical Image
Analysis 73 (2021), 102200.
[75] Sixiao Zhang, Hongxu Chen, Xiangguo Sun, Yicong Li, and Guandong Xu. 2022.
Unsupervised graph poisoning attack via contrastive loss back-propagation. In
Proceedings of the ACM Web Conference 2022. 1322–1330.
[76] Zeyu Zhu, Fanrong Li, Zitao Mo, Qinghao Hu, Gang Li, Zejian Liu, Xiaoyao Liang,
and Jian Cheng. 2023. 𝐴2Q: Aggregation-Aware Quantization for Graph Neural
Networks. In 11th International Conference on Learning Representations.
[77] Markus Zopf. 2022. 1-WL Expressiveness Is (Almost) All You Need. In Interna-
tional Joint Conference on Neural Networks, IJCNN. 1–8.
[78] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. 2018. Adversarial At-
tacks on Neural Networks for Graph Data. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2847–2856.
[79] Daniel Zügner and Stephan Günnemann. 2019. Adversarial Attacks on Graph
Neural Networks via Meta Learning. In 7th International Conference on Learning
Representations.
A PROOF OF PROPOSITION 3.3
Proof. We first prove by induction that the statement Equa-
tion (9)implies that there is a 1-to-1 correspondence between J𝑖
and the isomorphism types of unfolding trees of height 𝑖, denoted by
T𝑖, for all𝑖∈{0,...,𝑘}. In the base case 𝑖= 0, there is a single unfold-
ing tree inT0consisting of a single node. The uniform initialization
J0satisfies the requirement. Assume that 𝜑is a bijection between
J𝑖andT𝑖, then the statement (9)together with the permutation-
invariance guarantees that 𝑓(𝑖+1)(𝐴,A) =𝑓(𝑖+1)(𝐵,B)if and only
if𝐴=𝐵andA=B. Hence,{{𝜑(𝑎)|𝑎∈A}} ={{𝜑(𝑏)|𝑏∈B}} ,
which uniquely determines an unfolding tree in T𝑖+1according to
Definition 3.1. Vice versa, unfolding trees with different subtrees
lead to distinguishable multisets. The result follows by Lemma 3.2
and the 1-to-1 correspondence shown above at layer 𝑘. □
B PROOF OF PROPOSITION 3.5
Proof. Assume𝐹(𝑘)
𝐺(𝐺𝑖) =𝐹(𝑘)
𝐺(𝐺𝑗)but𝐶(𝑘)
𝑙(𝐺𝑖)̸=𝐶(𝑘)
𝑙(𝐺𝑗).
Then𝑓𝐺({{𝐹(𝑘)(𝑣)|𝑣∈𝑉(𝐺𝑖)}}) =𝑓𝐺({{𝐹(𝑘)(𝑣)|𝑣∈𝑉(𝐺𝑗)}})but
{{𝑐(𝑘)
𝑙(𝑣)|𝑣∈𝑉(𝐺𝑖)}}̸={{𝑐(𝑘)
𝑙(𝑣)|𝑣∈𝑉(𝐺𝑗)}}. Because per construc-
tion𝑐(𝑘)
𝑙(𝑢) =𝑐(𝑘)
𝑙(𝑣)⇐⇒𝐹(𝑘)(𝑢) =𝐹(𝑘)(𝑣)it follows thatA̸=B,
contradicting Equation (11). □
C PROGRESSIVE BIT FLIP ATTACK
The PBFA on CNN weights is an attack that can crush a CNN by
maliciously flipping minimal numbers of bits within its weight stor-
age memory (i.e., DRAM). It was first introduced as an untargeted
attack [ 48]. PBFA operates on integer quantized CNNs and seeks
 
1438KDD ’24, August 25–29, 2024, Barcelona, Spain Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried Gansterer, & Nils Kriege
Table 1: Preliminary case study illustrating the general vulnerability of GNNs to PBFA – pre- and post-attack mean of 10 runs
of top-1 test accuracy (community) or AUROC (structure) of INT8 quantized representative GNN architecture (GCN [ 63] with
3 layers, GAT [ 63] with 2 layers, GIN [ 68] with 5 layers) and dataset combinations (GCN on Cora, GAT on CiteSeer , GIN on
ogbg-mol ) baseline without BFAs; after PBFA [ 48] adapted to GNNs; after random bit flips (RBFA); total bit count of all model
parameters (attack surface) in millions.
COMMUNITY STRUCTURAL
Attack Dataset Pre Post Flips Bits Dataset Pre Post Flips Bits
RBFA Cora-GCN 0.77 0.74 63 1.6M ogbg-molhiv-GIN 0.71 0.53 953 15.1M
PBFA Cora-GCN 0.77 0.12 9 1.6M ogbg-molhiv-GIN 0.71 0.50 953 15.1M
RBFA CiteSeer-GAT 0.58 0.48 63 3.8M ogbg-moltoxcast-GIN 0.58 0.58 2662 16.6M
PBFA CiteSeer-GAT 0.58 0.14 10 3.8M ogbg-moltoxcast-GIN 0.58 0.57 2662 16.6M
to optimize Equation (15).
max
{bW(𝑙)
𝑞}L
Φ(X;{cW(𝑙)
𝑞}𝐿
𝑙=1),t
−L
Φ(X;{W(𝑙)
𝑞}𝐿
𝑙=1),t
s.t.𝐿∑︁
𝑙=1D(bW(𝑙)
𝑞,W(𝑙)
𝑞)∈ {0,1,...,𝑁𝑏}(15)
where Xandtare input batch and target vector, Lis a loss function,
𝑓is a neural network, 𝐿is the number of layers and bW(𝑙)
𝑞,W(𝑙)
𝑞are
the perturbed and unperturbed integer quantized weights (stored
in two’s complement) of layer 𝑙. In the original work by [ 48], the
functionLused is the same loss originally used during network
training.D(bW(𝑙)
𝑞,W(𝑙)
𝑞)represents the Hamming distance between
clean- and perturbed-binary weight tensor, and 𝑁𝑏represents the
maximum Hamming distance allowed through the CNN.
The attack is executed by flipping the bits along its gradient
ascending direction w.r.t. the loss of CNN. That is, using the 𝑁𝑞-bits
binary representation b= [𝑏𝑁𝑞−1,...,𝑏 0]of weights𝑤∈W𝑞,𝑙,
first the gradients of bw.r.t. to inference loss Lare computed
∇bL"
𝜕L
𝜕𝑏𝑁𝑞−1,...,𝜕L
𝜕𝑏0#
(16)
and then the perturbed bits are computed via m=b⊕(sign(∇bL)/2+
0.5)andbb=b⊕m, where⊕denotes the bitwise xoroperator.
To improve efficiency over iterating through each bit of the entire
CNN, the authors employ a method called progressive bit search
(PBS). As noted earlier, we refer to this BFA variant employing
PBS as Progressive BFA or PBFA. In PBS, at each iteration of the
attack (to which we synonymously refer as attack run ), in a first
step for each layer 𝑙∈[0,𝐿], the𝑛𝑏most vulnerable bits in cW(𝑙)
𝑞
are identified through gradient ranking (in-layer search). That is,
regarding input batch Xand target vector t, inference and backprop-
agation are performed successively to calculate the gradients of
bits w.r.t. the inference loss and the bits are ranked by the absolute
values of their gradients 𝜕L/𝜕𝑏. In a second step, after the most
vulnerable bit per layer is identified, the gradients are ranked across
all layers s.t. the most vulnerable bit in the entire CNN is found
(cross-layer search) and flipped. Should a PBS iteration not yield an
attack solution, which can be the case if no single bit flip improves
the optimization goal given in Equation (15), PBS is executed again
and evaluates increasing combinations of 2or more bit flips.D A MOTIVATING CASE STUDY
Our preliminary case study summarized in Tab. 1 indicates a sig-
nificant vulnerability of GNNs used in community-based tasks on
graphs with strong homophily [ 51] to malicious BFAs such as PBFA,
since it suggests a quantized GNN can be degraded so severely by
an extremely small number of bit flips—relative to the network’s at-
tack surface—that it produces basically random output. In our case
study, a GNN’s output on the community-based tasks is random if
its test accuracy drops below 14.3% (= 1/7) on Cora ’s 7-class node
classification task or below 16.7% (= 1/6) on CiteSeer ’s 6-class node
classification task. Our case study shows that this is consistently the
case due to the PBFA adapted from [ 48] for all community-based
architecture-dataset combinations examined. The number of bit
flips required for completely degrading a GNN in a community-
based task is remarkably small: tab. 1 shows that on average, the
adapted PBFA flipped only 0.0004%of the total number of bits of the
quantized GNNs’ parameters. Regarding random bit flips (RBFA),
the results of our case study are consistent with results obtained
for full-precision GNNs [ 27] in that they demonstrate a relatively
strong resilience of GNNs against such random perturbations.
On structural tasks on graphs with weak/low homophily as is
typical in molecular, chemical, and protein networks [ 51] which are
common in, e.g., drug development, PBFA is much less effective and
degrades the network comparable to random bit flips. On the struc-
tural tasks in Table 1, a GNN’s output is random if its test AUROC
drops to 0.5. We found that on the ogbg-moltoxcast dataset, PBFA
could not significantly degrade the network even after 2662 flips
and that on ogbg-molhiv , 0,0063% of the total number of bits of the
quantized GNNs’ parameters had to be flipped by PBFA before the
GNN’s output was degraded to random output, which constitutes
a 15.75 times increase compared to the community-based tasks.
This increased resilience of GNNs trained on structural tasks com-
pared to community-based tasks cannot be explained entirely by
the higher number of GNN parameters found in the evaluated tasks
requiring high structural expressivity, which mostly stems from
the MLPs employed in GIN: The increase in required flips for PBFA
to entirely degrade the network on the structural task is up to 2
orders of magnitudes larger compared to the community-based
task, while the increase in the attack surface is at most 1 order of
magnitude larger. Based on these observations, our work focuses
on such structural tasks which are typically solved by GIN.
 
1439