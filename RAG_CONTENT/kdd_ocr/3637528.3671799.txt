Mitigating Negative Transfer in Cross-Domain Recommendation
via Knowledge Transferability Enhancement
Zijian Song
School of CS, Peking University
Beijing, China
2201111590@stu.pku.edu.cnWenhan Zhang
School of CS, Peking University
Beijing, China
pku_zwh@pku.edu.cnLifang Deng
Lazada Group
Beijing, China
wanmei.dlf@alibaba-inc.com
Jiandong Zhang
Lazada Group
Beijing, China
chensong.zjd@alibaba-inc.comZhihua Wu
Lazada Group
Beijing, China
zhihua.wzh@lazada.comKaigui Bian
School of CS, Peking University
Beijing, China
bkg@pku.edu.cn
Bin Cui
School of CS, Peking University
Beijing, China
bin.cui@pku.edu.cn
ABSTRACT
Cross-Domain Recommendation (CDR) is a promising technique to
alleviate data sparsity by transferring knowledge across domains.
However, the negative transfer issue in the presence of numer-
ous domains has received limited attention. Most existing meth-
ods transfer all information from source domains to the target
domain without distinction. This introduces harmful noise and ir-
relevant features, resulting in suboptimal performance. Although
some methods decompose user features into domain-specific and
domain-shared components, they fail to consider other causes of
negative transfer. Worse still, we argue that simple feature decompo-
sition is insufficient for multi-domain scenarios. To bridge this gap,
we propose TrineCDR, the TRIple-level kNowledge transferability
Enhanced model for multi-target CDR. Unlike previous methods,
TrineCDR captures single domain and targeted cross-domain em-
beddings to serve multi-domain recommendation. For the latter,
we identify three fundamental causes of negative transfer, ranging
from micro to macro perspectives, and correspondingly enhance
knowledge transferability at three different levels: the feature level,
the interaction level, and the domain level. Through these efforts,
TrineCDR effectively filters out noise and irrelevant information
from source domains, leading to more comprehensive and accurate
representations in the target domain. We extensively evaluate the
Zijian Song, Wenhan Zhang, Kaigui Bian, and Bin Cui are affiliated with School of
CS, AI Innovation Center, National Engineering Laboratory for Big Data Analysis
and Applications, Peking University. Lifang Deng, Jiandong Zhang, Zhihua Wu are
affiliated with Lazada Group. Kaigui Bian is also affiliated with State Key Laboratory
of Multimedia Information Processing, Peking University.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671799proposed model on real-world datasets, sampled from Amazon and
Douban, under both dual-target and multi-target scenarios. The
experimental results demonstrate the superiority of TrineCDR over
state-of-the-art cross-domain recommendation methods.
CCS CONCEPTS
•Information systems →Recommender systems; •Comput-
ing methodologies →Transfer learning .
KEYWORDS
Cross-Domain Recommendation, Multi-Target CDR, Negative Trans-
fer, Contrastive Learning, Graph Neural Network
ACM Reference Format:
Zijian Song, Wenhan Zhang, Lifang Deng, Jiandong Zhang, Zhihua Wu,
Kaigui Bian, and Bin Cui. 2024. Mitigating Negative Transfer in Cross-
Domain Recommendation via Knowledge Transferability Enhancement . In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671799
1 INTRODUCTION
To address data sparsity and cold-start problem in recommendation
systems, researchers have proposed various cross-domain recom-
mendation (CDR) methods that adapt information from rich source
domains to the sparse target domain [ 37,44]. However, in real-
world scenarios, all domains may experience sparsity. Therefore, it
is desirable to improve the performance of all domains simultane-
ously by leveraging knowledge from each other. In response to this
need, numerous dual-target and multi-target CDR methods have
been proposed in recent years.
Unfortunately, the effectiveness of these methods is not always
guaranteed. In dual-target or multi-target CDR scenarios, the model
performance in some target domains may even decline as auxil-
iary data from other domains, especially sparse domains, is in-
troduced [ 44]. This phenomenon is known as negative transfer
 
2745
KDD ’24, August 25–29, 2024, Barcelona, Spain Zijian Song, et al.
(NT), where the transferred knowledge negatively affects the rec-
ommendation in the target domain. This results in significant model
performance degradation, which has been observed not only in our
experiments, but also in numerous prior studies [ 13,33,41]. Previ-
ous researches in transfer learning have attributed NT to various
causes [ 3,23,27]. Empirically, there are two primary approaches
to address NT: maximizing the transfer of useful information and
minimizing the transfer of harmful information. This necessitates
a focus on knowledge transferability. Inspired by these facts, we
examine every stage of knowledge transfer in CDR, and attribute
NT to the following reasons, ranging from micro to macro levels
(i.e., feature, instance, domain):
•Irrelevant Features. Some features may be useful in specific
domains but irrelevant in others. An effective algorithm should
possess the ability to select relevant features based on the target
domain’s requirement.
•Poor Source Domain Data Quality. The quality of transferred
knowledge is determined by the quality of source domain data,
i.e., the quality of interactions. This is particularly crucial when
dealing with sparse source domains, as the transferred knowledge
is prone to be severely influenced by noise interactions.
•Large Domain Divergence. The data distribution shift between
the target and source domains is the root of NT [ 38]. Knowledge
from less relevant source domains is more likely to cause NT.
Table 1: Compared to TrineCDR, existing methods did not
comprehensively address negative transfer from all aspects.
Metho
dDe
epAPF GA-DTCDR PPGN BiTGCF RL-ISN DisenCDR
Featur
e ✓
✓
Instance ✓
Domain ✓
Metho
dGA
-MTCDR-P HeroGRAPH CAT-ART MSDCR ReCDR TrineCDR
Featur
e ✓
✓ ✓ ✓ ✓
Instance ✓
Domain ✓
✓ ✓
As illustrated in Table 1, existing methods either neglect NT
or fail to comprehensively consider its causes, which results in
suboptimal performance. For instance, BiTGCF [ 17] introduce cross-
connections between two domains for deep dual transfer; PPGN [ 40]
constructs a heterogeneous graph including interactions from all
domains to conduct graph propagation. These methods transfer
information across domains without distinction between domain-
specific and domain-shared factors. Consequently, irrelevant factors
can be easily introduced into the target domain, leading to a reduc-
tion in model performance. Many methods like DeepAPF [ 35], Hero-
GRAPH [ 5], MSDCR [ 41], and ReCDR [ 33] capture domain-specific
and domain-shared representations, while only the domain-shared
parts are transferred across domains. However, in a multi-domain
scenario, the domain-shared feature selection criteria that can be
shared across all domains is too stringent. For example, in Ama-
zon dataset, the feature genre of domain CDs_and_Vinyl is useful
for capturing user preferences in domain Digital_Music, but maybe
irrelevant in domain Grocery_and_Gourmet_Food. Such features can-
not be shared across all domains, and thus cannot be encoded intodomain-shared embeddings. Consequently, they cannot be lever-
aged when Digital_Music is the target domain. Moreover, these
methods do not address NT from more perspectives other than
feature level. RL-ISN [ 8] attempts to filter out noise interactions
using reinforcement learning but faces challenges in its application
to multi-domain scenarios. GA-DTCDR [ 43] and CAT-ART [ 13]
examine the domain transferability. They learn the weights for
combining overlapping users’ representations from different do-
mains. However, the aggregation is shallow, and they still do not
investigate NT from all aspects.
Previous methods are inadequate to handle a large number of
domains. Researchers are facing two primary challenges: 1) the
intricate resemblance and exclusion relationships among multiple
domains render conventional feature decomposition ineffective; 2)
various sources of NT make it hard to prevent. In this paper, we
propose TrineCDR, a novel TRIple-level kNowledge transferability
Enhanced model for multi-target CDR. First, we decompose em-
beddings into single-domain and targeted cross-domain parts. The
former captures domain-specific information. The latter captures
only relevant information according to the target domain’s require-
ment. In this way, we ingeniously bypass the modeling of complex
relationships among numerous domains, and obtain more com-
prehensive features. Second, when calculating the targeted cross-
domain embeddings, TrineCDR rigorously examines the knowledge
transferability at every stage of knowledge transfer. From micro to
macro levels, the triple-level Knowledge Transferability Enhance-
ment (KTE) modules jointly prevent NT from occurring:
•Feature-Level Selection. We design a selection module to select
and weight candidate features based on the target domain. In this
way, we distill the most informative features for the subsequent
procedures.
•Interaction-Level Filtering and Enhancement. we apply
graph attention to filter out noise interactions and improve the
quality of source domain data. Additionally, we design a con-
trastive self-supervised task to enhance the robustness of em-
beddings against noise interactions. To be specific, we randomly
discard interactions at a rate of 𝜌, and align the features before
and after this edge dropout.
•Domain-Level Weighted Integration. We design a novel at-
tention module to estimate the user preference transferability
between domains. This allows us to conduct a weighted integra-
tion based on the transferability, blocking source domains with
large divergence from the target domain.
Generally, the three levels of NT are intertwined, so we employ
heterogeneous graph to separate them into three stages and op-
timize them individually. In conclusion, the contributions of this
study can be summarized as follows:
•Unlike previous CDR methods, we propose to capture single
domain and targeted cross-domain representations. This better
feature decomposition acquires more comprehensive and accu-
rate representations for the target domain.
•We identify three root causes of negative transfer in CDR: irrel-
evant features, noise interactions, and domain divergence. To
the best of our knowledge, we are the first to introduce the NT
mitigation theory from transfer learning field into CDR problem.
 
2746Mitigating Negative Transfer in Cross-Domain Recommendation via Knowledge Transferability Enhancement KDD ’24, August 25–29, 2024, Barcelona, Spain
•We design KTE modules at three different levels. The Feature-level
KTE (FKTE) module selects relevant features. The Interaction-level
KTE (IKTE) module filters interactions and improve the embed-
ding robustness against noise. The Domain-level KTE (DKTE)
module estimates domain transferability. These designs effec-
tively alleviate NT and significantly improve recommendation
performance.
•We conduct extensive experiments on various real-world datasets,
considering both dual-target and multi-target scenarios. The
experimental results demonstrate the superiority of TrineCDR
over state-of-the-art baselines. Additionally, our study illustrates
the universality of NT in CDR.
2 PROBLEM DEFINITION
Assuming there exist 𝑆domainsD𝑠(1≤𝑠≤𝑆)in the dataset.
Each item𝑖exclusively belongs to only one domain D𝑠. The set
of items in domain D𝑠is denoted asI𝑠. Each𝑖is associated with
raw features 𝒇𝑖∈R𝑑𝑠.𝑑𝑠is the dimensionality of raw features
of items inD𝑠, which may differ across domains. The domains
share the same pool of users, even though some users may not have
interactions in some domains. Each user 𝑢∈U is associated with
raw features 𝒇𝑢∈R𝑑𝑈.𝑑𝑈is the dimensionality of raw features
of users. For simplicity, in this paper, we use one-hot raw features,
but any auxiliary features can be easily introduced. Let N𝑢,𝑠=n
𝑖(𝑠)
𝑢,1,𝑖(𝑠)
𝑢,2,···,𝑖(𝑠)
𝑢,𝑛𝑢,𝑠o
represent user 𝑢’s𝑛𝑢,𝑠interacted items in
domainD𝑠. Then the objective is to find a function 𝑭𝑠for each
domainD𝑠to predict the probability ˆ𝑟𝑢,𝑖of user𝑢interacting with
item𝑖∈I𝑠, given all historical interactions.
3 METHODOLOGY
3.1 Model Overview
Figure 1 presents the overall architecture of TrineCDR. We propose
a novel framework to conduct more effective feature decomposition,
i.e. capturing single domain and targeted cross-domain embeddings
simultaneously. After that, we combine these two embeddings to
obtain final user/item embeddings.
•Single Domain Embedding. We calculate domain-specific fea-
tures only for the target domain D𝑑. Specifically, We apply graph
attention (GAT) on the domain subgraph that contains only the
target domain interactions.
•Targeted Cross-Domain Embedding. We construct a hetero-
geneous graph using interactions from all domains. Then we
transfer knowledge with triple-level KTE modules. In the embed-
ding layer, FKTE helps select relevant features. These features
are then processed through multiple graph propagation layers.
Each layer has an IKTE to filter noisy interactions and a DKTE
to evaluate domain transferability for weighted integration.
•Model Training. We employed the multi-task learning para-
digm, taking turns treating each domain as the target domain
and training the entire model with its data. Alongside the BPR
loss, we design an contrastive loss in the IKTE module to bolster
feature robustness against noise interactions and further mitigate
negative transfer.3.2 Graph Construction
Following the widely adopted graph construction [ 17,24,30], with
interaction data from all domains, we can construct a heterogeneous
global graphG=⟨V,E⟩. The node setV=U∪I 1∪I2∪···∪I𝑆
includes all users and items. The edge set E=E1∪E2∪···∪E𝑆
comprises interactions from all domains, where E𝑠denotes inter-
actions from domain D𝑠. Let 𝑹denote the user-item interaction
matrix. For an entry 𝑟𝑢𝑖=1, which indicated that user 𝑢has in-
teracted with item 𝑖, we establish an interaction edge between the
corresponding user node 𝑢and item node 𝑖.
Depending on the domain to which the edges belong, Gcan be
decomposed into 𝑆domain subgraphs G𝑠=⟨V𝑠,E𝑠⟩(1≤𝑠≤𝑆),
whereV𝑠=U∪I𝑠.
3.3 Single Domain Embedding
To capture domain-specific embeddings, and to alleviate the adverse
effects of data imbalance across domains, we first calculate the
single domain embeddings on the target domain subgraph D𝑑. We
start by mapping the raw features 𝒇𝑢and𝒇𝑖to latent embeddings:
𝒉(0)
𝑢,𝑑=𝑬𝑈,𝑑𝒇𝑢,𝒉(0)
𝑖,𝑑=𝑬𝐼,𝑑𝒇𝑖;where 𝑬𝑈,𝑑and𝑬𝐼,𝑑represents the
learnable mapping matrix for domain D𝑑.
To capture high-order relationships within the target domain,
we feed these embeddings through GAT layers, denoted by:
n
𝒉(𝑙)
𝑢,𝑑,𝒉(𝑙)
𝑖,𝑑o
=GAT
𝒉(𝑙−1)
𝑢,𝑑,𝒉(𝑙−1)
𝑖,𝑑;G𝑑
,(1≤𝑙≤𝐿); (1)
where𝐿is the number of graph propagation layers.
In each GAT layer, we first assign 𝒉(𝑙−1)
𝑢,𝑑and𝒉(𝑙−1)
𝑖,𝑑to corre-
sponding nodes, and conduct graph propagation on G𝑑. Taking a
user node𝑢as an example, the detailed process is as follows:
𝒉(𝑙)
𝑢,𝑑=∑︁
𝑖∈N𝑢,𝑑𝛼𝑢,𝑖𝒉(𝑙−1)
𝑖,𝑑,
𝛼𝑢,𝑖=exp
𝜙
𝒉(𝑙−1)
𝑢,𝑑,𝒉(𝑙−1)
𝑖,𝑑
Í
𝑗∈N𝑢,𝑑exp
𝜙
𝒉(𝑙−1)
𝑢,𝑑,𝒉(𝑙−1)
𝑗,𝑑,
𝜙 𝒉𝑢,𝑑,𝒉𝑖,𝑑=LeakyReLU
𝒂𝑇
𝑾𝒉𝑢,𝑑,𝑾𝒉𝑖,𝑑
;(2)
where 𝒂and𝑾are trainable parameters of this layer, and N𝑢,𝑑is
the set of𝑢’s interacted items in domain D𝑑.
The final single domain embeddings are obtained by concatena-
tion:
˜𝒆𝑢,𝑑=h
𝒉(0)
𝑢,𝑑,···,𝒉(𝐿)
𝑢,𝑑i
,˜𝒆𝑖,𝑑=h
𝒉(0)
𝑖,𝑑,···,𝒉(𝐿)
𝑖,𝑑i
. (3)
3.4 Feature-Level Knowledge Transferability
Enhancement (FKTE)
Starting from this subsection, we present the process of targeted
cross-domain embedding. The FKTE module is inspired by the
intuition that different target domains should focus on different
features. To mitigate the NT caused by irrelevant features, we select
and weight candidate features.
First, we map the raw features to cross-domain features, which
can be transferred to other domains: 𝒛𝑢=𝑬𝑈𝒇𝑢,𝒛𝑖=𝑬𝐼𝒇𝑖;where
𝑬𝑈and𝑬𝐼are learnable matrices. However, it is important to note
that not all of these candidate features are relevant in the target
 
2747KDD ’24, August 25–29, 2024, Barcelona, Spain Zijian Song, et al.
𝑙𝑙=3
𝑙𝑙=2
𝑙𝑙=1
𝑙𝑙=3
𝑙𝑙=2
𝑙𝑙=1Single Domain Embedding
Interaction-level Knowledge 
Transferability Enhancement (IKTE)
ℎ𝑢𝑢,𝑑𝑑𝑙𝑙
ℎ𝑖𝑖,𝑑𝑑𝑙𝑙
𝑐𝑐𝑢𝑢,𝑠𝑠𝑙𝑙
ℎ𝑖𝑖𝑙𝑙
Domain -Specific 
Embedding
Feature-level Knowledge
Transferability Enhancement 
(FKTE)
Cross-Domain EmbeddingFCSelection 
Module
of 𝒟𝒟𝑑𝑑Soft ThresholdAttention 
Score
ReLU
Candidate 
FeaturesFCTargeted Cross -Domain Embedding
𝒢𝒢1𝒢𝒢2𝒢𝒢3
𝒢𝒢∗𝑐𝑐𝑢𝑢,𝑑𝑑𝑙𝑙
Edge 
Dropout
ℒ𝑠𝑠𝑠𝑠𝑙𝑙
Domain -level Knowledge 
Transferability Enhancement 
(DKTE)
FC FC
Q K VMatMul
𝑐𝑐𝑢𝑢,𝑑𝑑𝑙𝑙𝑐𝑐𝑢𝑢,𝑠𝑠𝑙𝑙MatMul
ScaleSoftmaxℎ𝑢𝑢𝑙𝑙
𝑐𝑐𝑢𝑢,𝑠𝑠𝑙𝑙Prediction ̂𝑟𝑟𝑢𝑢,𝑖𝑖=𝐻𝐻𝑢𝑢⋅𝐻𝐻𝑖𝑖
Embedding Fusion
̃𝑒𝑒𝑢𝑢,𝑑𝑑ℎ𝑢𝑢,𝑑𝑑𝑙𝑙
̃𝑒𝑒𝑢𝑢ℎ𝑢𝑢𝑙𝑙
𝐻𝐻𝑖𝑖
ℎ𝑖𝑖,𝑑𝑑𝑙𝑙ℎ𝑖𝑖𝑙𝑙̃𝑒𝑒𝑖𝑖,𝑑𝑑 ̃𝑒𝑒𝑖𝑖𝐻𝐻𝑢𝑢interactions
user
item ∈source domain 𝒟𝒟 2item∈source domain 𝒟𝒟 1
item ∈target domain 𝒟𝒟3
̃𝑒𝑒𝑢𝑢,𝑑𝑑
1. Graph Construction 3. Graph Propagation 4. Prediction 2. Embedding LayerDomain Subgraph
Ground Truth 𝑟𝑟𝑢𝑢,𝑖𝑖
ℒ𝑏𝑏𝑏𝑏𝑏𝑏
Figure 1: Overall architecture of TrineCDR. After graph construction, we separately capture single domain embeddings and
targeted cross-domain embeddings. The former captures user/item representations within the target domain. The latter
leverages data from all domains, incorporating the triple-level knowledge transferability enhancement (KTE) modules to
mitigate negative transfer. Note that the target domain is denoted as D𝑑. While training, we take turns treating each domain as
the target domain.
domain. To identify and prioritize the most relevant features, we
are inspired by SENet [ 12] as it has proven to be effective in weight-
ing feature channels. We design a selection module to adaptively
generate attention score for each feature dimension. Considering
a node𝑣, we calculate the targeted cross-domain features 𝒉(0)
𝑣as
follows:
𝒉(0)
𝑣=𝜏(MLP(𝒛𝑣;𝑑))⊙𝒛𝑣,
MLP(𝒛𝑣;𝑑)=𝑴𝑑1ReLU(𝑴𝑑2𝒛𝑣+𝒃𝑑2)+𝒃𝑑1,
𝜏(𝒙)=sgn(𝒙)⊙max(0,abs(𝒙)−𝜃);(4)
where⊙denotes Hadamard product, 𝑴𝑑1,𝑴𝑑2,𝒃𝑑1,𝒃𝑑2are
trainable parameters. 𝜏is the soft threshold widely used to filter out
redundant information with hyper-parameter 𝜃. It sets dimensions
with very small absolute values to 0, mitigating the effects of noise.
3.5 Interaction-Level Knowledge
Transferability Enhancement (IKTE)
During the graph propagation of targeted cross-domain embedding,
multiple layers are involved, each containing an IKTE and a DKTE
module. The 𝑙-th layer’s IKTE takes the output of the (𝑙−1)-th
layer’s DKTE as input.
The IKTE module is designed to mitigate negative transfer at the
instance level. We first identify and filter out noise interactions that
may severely harm sparse source data by estimating the likelihood
of interactions based on node similarities. Specifically, we employ
graph attention on the domain subgraphs to measure the similari-
ties between connected nodes. In this process, lower weights are
assigned to poorly matched nodes, while higher weights are as-
signed to confident interactions. After weighting interactions, weaggregate intra-domain information from neighbors:
n
𝒄(𝑙)
𝑢,𝑠,𝒉(𝑙)
𝑖o
=GAT
𝒉(𝑙−1)
𝑢,𝒉(𝑙−1)
𝑖;G𝑠
; (5)
where 𝒄(𝑙)
𝑢,𝑠is the user domain preference in domain D𝑠. It represents
the user’s interests in that domain, and will be utilized to generate
user embedding 𝒉(𝑙)
𝑢in the subsequent DKTE module.
To further reduce the impact of noise interactions, we con-
duct contrastive learning to improve representation robustness.
We randomly remove edges on the target domain subgraph at a
dropout rate 𝜌. By bringing the embeddings before and after the
edge dropout closer together in the feature space, we extract more
general representations which are robust against noise interac-
tions. Intuitively, through this contrastive learning, adding a small
amount of noise interactions to the training data will not have much
negative impact on the final representations, because the represen-
tations before and after the random absence of a few interactions
are similar.
Considering a mini-batch T𝑑from the target domain D𝑑, the
contrastive loss is as follows:
L𝑠𝑠𝑙=1
|T𝑑|∑︁
𝑢∈T𝑑−lnexp(cos(𝒄𝑢,∗𝒄𝑢)/𝑡)Í
𝑣∈T𝑑exp(cos(𝒄𝑢,𝒄𝑣)/𝑡)
−lnexp(cos(𝒄𝑢,∗𝒄𝑢)/𝑡)Í
𝑣∈T𝑑exp(cos(∗𝒄𝑢,∗𝒄𝑣)/𝑡);(6)
where𝑡is the temperature hyper-parameter, cosis cosine distance.
∗𝒄𝑢denotes∗𝒄(𝑙)
𝑢,𝑑, the user domain preference recalculated after
the edge dropout. 𝒄𝑢denotes 𝒄(𝑙)
𝑢,𝑑.
 
2748Mitigating Negative Transfer in Cross-Domain Recommendation via Knowledge Transferability Enhancement KDD ’24, August 25–29, 2024, Barcelona, Spain
3.6 Domain-Level Knowledge Transferability
Enhancement (DKTE)
We claim that domain transferability varies among users. Therefore,
we design the DKTE module to aggregate users’ domain preference
in a personalized manner. In existing transfer learning literature,
an effective approach to mitigate negative transfer is selecting
and weighting domains [ 38]. The Maximum Mean Discrepancy
(MMD) paradigm is commonly used to assess domain similarity
and assign appropriate weights accordingly. To be specific, MMD [ 7]
calculates the mean of the kernel function for samples within each
domain, treating it as the domain representation. Then, it measures
domain similarity based on the Euclidean distance between domain
representations.
Inspired by this, we innovatively regard the user domain prefer-
ence 𝒄(𝑙)
𝑢,𝑠as the domain representation. This is motivated by the fact
that, similar to MMD, the user domain preference can be seen as the
weighted sum of samples’ representations. Furthermore, we design
a novel attention module to effectively aggregate these domain
representations. Specifically, we incorporate the user domain pref-
erence in the target domain 𝒄(𝑙)
𝑢,𝑑with the single domain embedding
˜𝒆𝑢,𝑑to obtain the query vector 𝒒(𝑙)
𝑢, because the single domain em-
bedding provides reliable target domain knowledge without threat
of NT. The source domain preference 𝒄(𝑙)
𝑢,𝑠is then utilized to obtain
both key vectors 𝒌(𝑙)
𝑢,𝑠and value vectors 𝒗(𝑙)
𝑢,𝑠. The detailed process
is described as follows:
𝒉(𝑙)
𝑢=softmax©­
«𝒒(𝑙)
𝑢𝑇𝑲(𝑙)
𝑢√︁
dim(𝒒)ª®
¬𝑽(𝑙)
𝑢,
𝒒(𝑙)
𝑢=𝑾𝑸(𝑙)
𝒄(𝑙)
𝑢,𝑑⊕˜𝒆𝑢,𝑑
,
𝑲(𝑙)
𝑢=𝑾𝑲(𝑙)𝑽(𝑙)
𝑢,
𝑽(𝑙)
𝑢=h
𝒄(𝑙)
𝑢,1,𝒄(𝑙)
𝑢,2,···,𝒄(𝑙)
𝑢,𝑆i
;(7)
where dim(𝒒)denotes the dimensionality of query vector 𝒒(𝑙)
𝑢, and
𝑾𝑸(𝑙),𝑾𝑲(𝑙)are learnable attention parameters in the 𝑙-th layer.
⊕can represent various fusion operations. In this paper, we employ
vector addition for simplicity.
3.7 Objective and Model Optimization
After multiple layers of global graph propagation, the final tar-
geted cross-domain embeddings can be obtained by concatenating
outputs from all layers:
˜𝒆𝑢=h
𝒉(0)
𝑢,···,𝒉(𝐿)
𝑢i
,˜𝒆𝑖=h
𝒉(0)
𝑖,···,𝒉(𝐿)
𝑖i
. (8)
Now we have two embeddings containing target domain infor-
mation and cross-domain information respectively for each node. In
the embedding fusion layer, we combine these embeddings simply
by concatenation:
𝑯𝑢=˜𝒆𝑢,˜𝒆𝑢,𝑑
,𝑯𝑖=˜𝒆𝑖,˜𝒆𝑖,𝑑
. (9)
Finally, in the prediction layer, we rank the recommended items
using Bayesian Personalized Ranking (BPR). The predicted rating
ˆ𝑟𝑢,𝑖for user𝑢on item𝑖is computed using the inner product 𝑯𝑇𝑢𝑯𝑖.We employ the BPR loss for model optimization. The final loss
function for the target domain D𝑑is defined as follows:
L𝑑(Θ)=1
|T𝑑|∑︁
⟨𝑢,𝑖+,𝑖−⟩∈T𝑑−ln𝜎 ˆ𝑟𝑢,𝑖+−ˆ𝑟𝑢,𝑖−+L𝑠𝑠𝑙+𝜆𝑑∥Θ∥2;
(10)
where𝜎is sigmoid, andT𝑑={⟨𝑢,𝑖+,𝑖−⟩}is a training mini-batch.
During training, we sample several negative items for each pos-
itive interaction.⟨𝑢,𝑖+⟩is a positive interaction, and ⟨𝑢,𝑖−⟩is a
non-existing interaction. Essentially, user 𝑢expresses a preference
for item𝑖+over item𝑖−. To control model complexity, a regularizer
𝜆𝑑∥Θ∥2is applied. The coefficient 𝜆𝑑varies across domains due to
differences in data density resulting in varying optimal regulariza-
tion strengths to prevent overfitting.
To serve as a multi-target CDR model, we adopt a multi-task
framework where the model is trained alternately using a data
batch from each domain. The pseudocode of TrineCDR’s training
algorithm is presented in Algorithm 1 for better understanding.
Algorithm 1: The Training Algorithm of TrineCDR
Data: global graphG, domain subgraphs G𝑠, ground-truth
labels𝑟𝑢𝑖
Result: well trained model for all domains
1while not converge do
2 ford=1 to S do
//D𝑑as the target domain
3 Sample a batchT𝑑={(𝑢,𝑖+,𝑖−)}from domainD𝑑;
// Single-Domain Embeddings
4 Calculate ˜𝒆𝑢,𝑑,˜𝒆𝑖,𝑑via Eq. (1)-(3);
// Targeted Cross-Domain Embeddings
5 FKTE calculates 𝒉(0)via Eq. (4);
6 forl=1 to L do
7 IKTE calculates item representation 𝒉(𝑙)
𝑖and
user embedding component 𝒄(𝑙)
𝑢,𝑠via Eq. (5);
8 DKTE calculates 𝒉(𝑙)
𝑢via Eq. (7);
// Contrastive Learning
9 Get∗G𝑑via Edge Dropout on G𝑑;
10 Calculate∗𝒄(𝑙)
𝑢,𝑑on∗G(D𝑑);
11 CalculateL𝑠𝑠𝑙via Eq. (6);
12 end
13 Calculate ˜𝒆𝑢,˜𝒆𝑖via Eq. (8);
// Backpropagation
14 Calculate final embedding 𝐻via Eq. (9);
15 Calculate loss via Eq. (10);
16 Gradient calculation and parameter update;
17 end
18end
4 EXPERIMENTS
4.1 Experimental Settings
4.1.1 Datasets. We conduct extensive and comprehensive exper-
iments under both dual-target and multi-target scenarios drawn
 
2749KDD ’24, August 25–29, 2024, Barcelona, Spain Zijian Song, et al.
from real-world datasets Amazon [ 19] and Douban [ 42]. The dataset
descriptions are given below, and the detailed statistics of sampled
datasets are shown in Table 2.
•Amazon 5-core is a dense subset from Amazon website in which
each user and item has at least 5 related records. It has dozens of
domains, so we sample 5-domain and 7-domain dataset from it to
further investigate negative transfer as the number of domains
increases. While sampling domains from Amazon 5-core, the
varying degrees of domain correlations are taken into account.
These datasets contain several domain groups, where each group
contains highly related domains, but domains from different
groups are less related. For example, dataset 6 has 4 groups:
(CD,Music) ,(Food) ,(Phone,Electronics) , and(Clothing,Luxury) .
This conforms to the data distribution in real-world scenarios.
Only user ID, item ID, domain ID, and ratings are used.
•Douban is crawled from Douban website and only users are
overlapped in three domains. Only user ID, item ID, domain ID,
and ratings are used.
We sample two datasets for dual-target CDR. One includes do-
mains CDs_and_Vinyl andDigital_Music from Amazon. The other
includes book andmovie from Douban. We extract only overlapping
users (i.e. users have interactions in every domain), because most
dual-target CDR methods are based on the assumption that users
fully overlap.
We also sample two datasets for multi-target CDR. One includes
CD,Music, and an irrelevant domain Grocery_and_Gourmet_Food
from Amazon to evaluate the model performance in the presence
of less related domains. One includes book, movie, and music from
Douban. Since the assumption of user fully overlapping is hard to
satisfy in practice, especially in multi-domain scenarios, we do not
extract overlapping users.
Furthermore, we sample datasets with 5and7domains from
Amazon to further investigate negative transfer as the number of
domains increases. For the same reason, we do not extract overlap-
ping users, either.
To ensure the timeliness of captured user preferences, we ex-
tract entries with relatively recent timestamps. Users and items
with less than 5 interactions are excluded. This data filtering is
widely adopted [ 5,33]. Interactions rated from 1to5are included
as positive samples, because we acknowledge the existence of noise
samples that may not accurately reflect users’ preferences. Similar
to the leave-one-out principle applied in previous studies [ 33,41],
we use the last one interaction of each user for validation and test,
while the others are used for training. Specifically, we randomly
select half of users, allocating their last interaction to the validation
set, and the other half of users to the test set. For efficiency, We
sample only one negative item for each positive interaction in each
epoch.
4.1.2 Evaluation Metrics. We utilize Hit Ratio (HR) and Normalized
Cumulative Gain (NDCG) as the evaluation metrics, as they are
widely adopted by existing studies [ 33,40,41]. Specifically, we rank
a positive item among 99randomly sampled negative items. HR@K
measures whether the positive item is in the top- 𝐾items. NDCG@K
further considers the position of the hit, assigning higher score to
higher rank. In this paper, we set 𝐾to5.Table 2: Statistics of Sampled Datasets.
Dataset Domain #
Users # Items # Ratings Density
1CD 2566
10758 35789 0.130%
Music 2566
4064 17999 0.173%
2b
ook 1919
6777 67304 0.037%
mo
vie 1919
34893 157965 0.236%
3CD 8123
5535 64685 0.145%
Music 5550
4268 49452 0.209%
Fo
od 10916
4509 75350 0.153%
4b
ook 2026
6777 67398 0.035%
mo
vie 2508
34893 207163 0.237%
music 1611
5567 51874 0.040%
5CD 2408
1537 14477 0.391%
Music 2107
1610 15613 0.460%
Fo
od 18543
7342 127111 0.093%
P
hone 6407
1734 23581 0.212%
Ele
ctronics 11462
3548 50164 0.123%
6CD 2080
1265 11624 0.442%
Music 1876
1399 13428 0.512%
Fo
od 14635
5306 90079 0.116%
P
hone 4875
1142 14953 0.269%
Ele
ctronics 9914
2821 38683 0.138%
Clothing 15141
3349 96042 0.189%
Luxur
y 1231
443 9256 1.697%
4.1.3 Baseline Methods. We compare TrineCDR with various base-
line models, including single-domain methods, dual-target CDR
method, and multi-domain CDR methods.
Single Domain Methods
•BPRMF [21] factorizes feedback matrix into product of user and
item latent embedding matrices.
•NeuMF [10] combines the results from linear matrix factorization
and non-linear MLP to improve recommendation performance.
•LightGCN [9] is a GNN-based single domain recommendation
method. It is a extension of NGCF [ 25], devising a linear propa-
gating encoder to learn representations.
Dual-target CDR Methods
•DeepAPF [35] initializes user embeddings into shared and site-
specific parts, and fuses them with attention mechanism. We
extend it for multi-target scenarios.
•GA-DTCDR [43] uses Node2Vec to generate embeddings in
each domain and employs element-wise attention mechanism to
combine these representations of overlapping users.
•BiTGCF [17] is a GNN-based extension of CoNet. It constructs
a user-item bipartite graph in each domain and employs GCN
layers for deep dual knowledge transfer.
•DisenCDR [4] proposes two mutual-information-based disen-
tanglement regularizers to disentangle the domain-shared and
domain-specific information.
Multi-target CDR Methods
•GA-MTCDR-P [45] is the multi-target extension of GA-DTCDR.
It learns attention scores between each pair of domains.
 
2750Mitigating Negative Transfer in Cross-Domain Recommendation via Knowledge Transferability Enhancement KDD ’24, August 25–29, 2024, Barcelona, Spain
•HeroGRAPH [5] learns local embeddings within each domain,
and constructs a heterogeneous graph using interactions from
all domains to capture domain-shared embeddings.
•ReCDR [33] is a extension of HeroGRAPH. On the global graph,
it introduces additional edges based on the similarity of nodes.
Then it leverages a hierarchical attention mechanism to capture
domain-shared embeddings.
•CAT-ART [13] initializes domain-specific representations with
BPRMF and transfers them with attention mechanism. It uses a
contrastive autoencoder for domain-invariant embeddings.
4.1.4 Hyper-parameter Settings. All hyper-parameters were tuned
on the validation sets. We set the embedding dimensionality of all
models to 64 for fairness. The graph propagation consists of 2 layers,
and the dimensionality of all hidden layers is 64. Soft threshold
𝜃=0.1. Learning rate 𝑙𝑟∈{5𝑒−3,1𝑒−3,5𝑒−4}. Regularization
coefficient𝜆∈{0.1,0.05,0.01}. Temperature 𝑡∈{0.1,0.05,0.01}.
Dropout rate 𝜌=0.1. We use Adam optimizer with default settings,
early stopping when the average HR@5 and NDCG@5 scores on the
validation set no longer increase for 30 epochs. We employ different
batch sizes for each domain to ensure an equal batch number per
epoch for every domain.
4.2 Performance Comparison
We compare the performance of TrineCDR with baselines on real-
world datasets Amazon andDouban. The overall results are shown
in Table 3 for dual-target CDR and Table 5 for multi-target CDR.
Some dual-target methods either necessitate full user overlapping
or cannot be adapted to multi-domain scenarios, rendering them
unsuitable for participation in multi-domain experiments. As shown
in the experimental results, TrineCDR significantly outperforms
baselines under all circumstances.
Table 3: Overall performance (in HR@5 and NDCG@5) of
TrineCDR and baselines on 2-domain datasets. ↓denotes
negative transfer compared with single domain methods.
Boldface and underline denotes the best and the second-best
results. ★denotes significance level 𝑝-value <0.05of compar-
ing TrineCDR over the best baselines.
DomainAmazon Douban
CD
Music b
ook movie
Metric HR
NDCG HR NDCG HR
NDCG HR NDCG
BPRMF 0.322
0.245 0.464 0.393 0.258
0.180 0.492 0.350
NeuMF 0.285
0.209 0.440 0.353 0.280
0.187 0.516 0.360
LightGCN 0.350
0.257 0.494 0.399 0.273
0.188 0.512 0.360
De
epAPF 0.292↓0.211↓0.443↓0.355↓0.257↓0.177↓0.517
0.371
GA-DTCDR 0.308↓0.204↓0.420↓0.305↓0.247↓0.177↓0.484↓0.344↓
BiT
GCF 0.362
0.264 0.509 0.400 0.262↓0.169↓0.483↓0.329↓
DisenCDR 0.365 0.268 0.512 0.405 0.288
0.186↓0.527 0.352
Her
oGRAPH 0.345↓0.260
0.506 0.424 0.271↓0.191
0.518 0.368
CAT-ART * 0.357
0.264 0.512 0.423 0.278↓0.194
0.519 0.366
ReCDR 0.361
0.271 0.505
0.415 0.296 0.202 0.532 0.379
Ours 0.374★0.287★0.523★0.437★0.302★0.218★0.550★0.397★
It is noteworthy that most CDR baselines suffer from negative
transfer comparing to the best single domain method. It is notonly observed in our experiments, but also raised by many exist-
ing studies [ 33,41]. However, prior studies do not attribute this
to the models’ inadequate handling of specific NT causes. Deep-
APF, DisenCDR, and HeroGRAPH employ feature decomposition.
GA-DTCDR and CAT-ART employ weighted domain aggregation.
However, they all do not address NT comprehensively. BiTGCF is
one of the top-performing models on the first dataset, but transfer-
ring all information indiscriminately results in severe NT on the
sparser dataset. ReCDR enhances knowledge transfer by adding
extra edges, but does not have a mechanism for knowledge filtering
when calculating global embeddings.
By contrast, TrineCDR mitigates NT by employing the triple-
level KTE modules to capture comprehensive features and filter
irrelevant information. It also conducts a novel feature decomposi-
tion for more comprehensive representations. The results demon-
strate that existing methods are insufficient in handling NT, and
underscore the superiority of our proposed model.
4.3 Ablation Study
We conduct ablation experiments on the multi-domain scenario
Book-Movie-Music sampled from Douban dataset to check the effec-
tiveness of core components of TrineCDR. As shown in Table 4, We
compare the original model with six variants: TrineCDR-w/o-Feat
has no FKTE modules; TrineCDR-w/o-Int replaces IKTE modules
with mean-pooling GCN layers; TrineCDR-w/o-Dom replaces
DKTE modules with mean-pooling; TrineCDR-w/o-CL does not
use contrastive loss; TrineCDR-Cross uses only targeted cross-
domain embeddings; TrineCDR-Single uses only single domain
embeddings.
Table 4: Ablation Study. The first part replaces the knowledge
transferability enhancement modules. The second part uses
only a part of embeddings.★denotes significance level 𝑝<
0.05comparing TrineCDR over the variant.
Domain Bo
ok Movie Music
Metric HR
NDCG HR NDCG HR NDCG
T
rineCDR-w/o-Feat 0.280 0.197 0.571★0.402★0.299★0.210★
T
rineCDR-w/o-Int 0.269★0.185★0.528★↓0.377★↓0.272★↓0.182★↓
T
rineCDR-w/o-Dom 0.265★0.188★0.530★↓0.376★↓0.263★↓0.174★↓
T
rineCDR-w/o-CL 0.276★0.191★0.577★0.398★0.285★0.195★
T
rineCDR-Cross 0.260★↓0.183★↓0.512★↓0.363★↓0.261★↓0.181★↓
T
rineCDR-Single 0.255★↓0.179★↓0.541★↓0.386★↓0.263★↓0.184★
T
rineCDR 0.281
0.197 0.596 0.430 0.307 0.215
TrineCDR significantly outperforms TrineCDR-w/o-Int, TrineCDR-
w/o-Dom and TrineCDR-w/o-CL in all domains, with average im-
provements of 11.48%, 11.22%, and 5.71%. Moreover, the variants
suffer from NT. These results indicate that both the IKTE and DKTE
modules play crucial roles in mitigating NT.
Figure 2 shows the average transferability scores 𝒒𝑇𝒌/√︁
dim(𝒒)
between source and target domains, which are generated by the
DKTE module. There is an evident pattern that high scores are
assigned to related domains (e.g. book→movie ), while low scores
are assigned to irrelevant domains (e.g. Grocery_and_Gourmet_Food
→Digital_Music ).
 
2751KDD ’24, August 25–29, 2024, Barcelona, Spain Zijian Song, et al.
Table 5: Overall performance of TrineCDR and baseline methods on multi-domain datasets. †denotes the multi-target adaptation.
∗denotes the adaptation when users non-fully overlap.
DomainAmazon Douban
CD
Music Food b
ook movie music
Metric HR
NDCG HR NDCG HR NDCG HR
NDCG HR NDCG HR NDCG
BPRMF 0.466
0.365 0.528 0.449 0.498 0.426 0.253
0.173 0.553 0.391 0.278 0.183
NeuMF 0.457
0.358 0.535 0.450 0.493 0.418 0.261
0.175 0.556 0.394 0.252 0.169
LightGCN 0.487
0.390 0.567 0.469 0.512 0.418 0.265
0.185 0.560 0.394 0.276 0.185
De
epAPF† 0.445↓ 0.343↓ 0.523↓0.431↓0.501↓ 0.425↓ 0.262↓ 0.177↓ 0.525↓ 0.379↓ 0.237↓ 0.165↓
Her
oGRAPH 0.497
0.381 0.548↓0.464↓ 0.514 0.439 0.272 0.186
0.548↓ 0.397 0.255↓ 0.161↓
GA-MTCDR-P 0.455↓ 0.320↓ 0.467↓0.333↓0.481↓ 0.375↓ 0.243↓ 0.160↓ 0.547↓ 0.385↓ 0.237↓ 0.149↓
CA
T-ART * 0.510 0.409 0.566↓ 0.475
0.522 0.441 0.260↓ 0.184↓ 0.562
0.402 0.278
0.187
ReCDR 0.507
0.408 0.572 0.481 0.519
0.446 0.267
0.187 0.567 0.400
0.288 0.193
Ours 0.533★0.423★0.579★0.485
0.542★0.461★0.281★0.197★0.596★0.430★0.307★0.215★
/0 /1 /0 /2 /3 /4 /3 /0 /5 /i255 /1 /0 /7 /1 /8 /9 /10 /11/12 /13 /14 /15 /14 /13 /12 /16 /10 /17 /18 /19 /20 /15 /21
/16 /22 /13 /14 /7 /3 /3 /3 /23 /7 /3 /24/25/26/27/3 /28 /29 /10 /30 /31 /20 /18 /23 /32 /29 /3 /9 /10 /11/12 /13 /14 /15 /14 /13 /12 /16 /10 /17 /18 /19 /20 /15 /21 /33 /3 /33
Figure 2: Average transferability scores on the test sets. The
visualization results reveal the pattern of higher scores for
more relevant domains.
Additionally, Figure 3 shows the relationships between the sim-
ilarity scores 𝜙(𝒉𝑢,𝒉𝑖)generated by the IKTE module and the
ground-truth ratings. Despite the absence of any rating-related
information in the training data, a clear correlation is observed
between higher scores and higher ratings. Furthermore, interac-
tions with rating 1receive slightly more attention as they express
strongly negative preferences.
/0 /1 /0 /2 /3 /4 /3 /0 /5 /i255 /1 /2 /7 /0 /8 /9 /10 /11 /12 /13 /14 /15 /14 /13 /12 /16 /17 /18 /19 /20 /15 /21
/22 /9 /13 /14 /7 /3 /3 /3 /23 /7 /3 /24/25/26/27/3 /28 /29 /17 /16 /30 /20 /11 /23 /31 /29 /3 /9 /10 /11 /12 /13 /14 /15 /14 /13 /12 /16 /17 /18 /19 /20 /15 /21 /32 /3 /32
Figure 3: Similarity score distributions of rated interactions.
Δis the mean value. There is an obvious pattern that infor-
mative ratings are tend to be associated with higher scores.Regarding the FKTE module, TrineCDR outperforms TrineCDR-
w/o-Feat significantly in domains movie andmusic with average
improvement of 3.91%, while their performance is similar in the
domain book. Figure 4 shows the average feature weights outputted
by the FKTE module. Different domains exhibit distinct selectivity
patterns: the domain music has more zero weights to filter out these
features; the domain movie assigns more large weights (greater than
1) to emphasize corresponding features; the weights in the domain
book are relatively smooth, resulting in less noticeable effects from
the FKTE module.
2023/5/19 20:25 180_fw .svg
file:///C:/ 博一春季 /_RobustCDR/source/180_fw .svg 1/1
Figure 4: Average feature weights of Book-Movie-Music sce-
nario. Each domain exhibits a distinct selective pattern.
The other two experiments highlight the advantage of proposed
novel feature decomposition framework. TrineCDR achieves av-
erage improvements of 12.51%and11.10%over TrineCDR-Cross
and TrineCDR-Single. These results indicate that this novel feature
decomposition enables the model to obtain more comprehensive
and accurate representations, effectively addressing NT.
4.4 Negative Transfer
To further showcase TrineCDR’s excellence in mitigating NT and
reveal the universality of NT, we conduct additional experiments
using datasets containing 5 and 7 domains, as presented in Table 6
and Table 7, respectively. As we increase the number of domains
from 3 to 5 and then to 7, CAT-ART exhibits NT in a growing
number of domains. Even in domains where NT originally did
not occur, such as Digital_Music, as more domains are introduced,
negative transfer also occurs. Similarly, ReCDR, which does not
initially exhibit negative transfer in 3-domain datasets, encountered
increasingly severe negative transfer issues.
In contrast, TrineCDR consistently maintains significantly supe-
rior performance compared to single-domain methods and other
 
2752Mitigating Negative Transfer in Cross-Domain Recommendation via Knowledge Transferability Enhancement KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 6: Performance comparison in the 5-domain scenario.
Domain CD
Music Food Phone Electronics
Metric HR
NDCG HR NDCG HR NDCG HR NDCG HR NDCG
BPRMF 0.407
0.325 0.504 0.429 0.502 0.428 0.393 0.311 0.332
0.270
NeuMF 0.378
0.303 0.487 0.407 0.490 0.399 0.355 0.288 0.374 0.285
LightGCN 0.421
0.312 0.517 0.438 0.508 0.428 0.389 0.307 0.381 0.289
CA
T-ART 0.398↓ 0.325
0.514↓ 0.440 0.509
0.431 0.393 0.303↓ 0.326↓ 0.268↓
ReCDR 0.427 0.335 0.523 0.437
0.513 0.434 0.386↓ 0.307↓ 0.349↓ 0.283↓
Ours 0.436★0.347★0.519 0.445
0.533★0.451★0.402★0.321★0.391★0.298★
Table 7: Performance comparison in the 7-domain scenario.
Domain CD
Music Food Phone Electronic Clothing Luxury
Metric HR
NDCG HR NDCG HR NDCG HR NDCG HR NDCG HR NDCG HR NDCG
BPRMF 0.406
0.324 0.499 0.433 0.501 0.431 0.406 0.319 0.308 0.253 0.794 0.760 0.371 0.345
NeuMF 0.399
0.294 0.470 0.398 0.497 0.413 0.421 0.330 0.354
0.270 0.772 0.726 0.403 0.358
LightGCN 0.419
0.310 0.521 0.440 0.506
0.434 0.404 0.321 0.355 0.263
0.798 0.760 0.418 0.372
CA
T-ART 0.403↓ 0.317↓ 0.496↓ 0.435↓ 0.509
0.436 0.412↓ 0.327↓ 0.328↓ 0.266↓ 0.794↓ 0.761 0.388↓ 0.348↓
ReCDR 0.420 0.325 0.507↓ 0.432↓ 0.532 0.457 0.418↓ 0.327↓ 0.347↓ 0.282 0.800 0.767 0.398↓ 0.349↓
Ours 0.438★0.339★0.547★0.465★0.551★0.469★0.428★0.343★0.366★0.301★0.813★0.771
0.446★0.398★
sota CDR methods. This compelling comparison unequivocally val-
idates the effectiveness of TrineCDR in mitigating NT, particularly
in scenarios involving a higher number of domains.
5 RELATED WORK
5.1 Cross-Domain Recommendation
Previous methods often model the preferences of overlapping users
with bi-directional transfer learning [ 11,14,42]. Recently, graph-
based CDR methods using GNN have been proposed to capture
higher-order information propagation on the user-item bipartite
graph [ 31], such as PPGN [ 40] and BiTGCF [ 17]. They transfer all
information across domains without considering transferability.
Some methods like DeepAPF [ 35] and ReCDR [ 33] consider user’s
domain-shared/specific preference. Some methods like RL-ISN [ 8]
attempt to filter noisy interactions using reinforcement learning.
Limited methods have considered domain transferability. For ex-
ample, GA-DTCDR [ 43] and CAT-ART [ 13]. However, they all do
not consider negative transfer from other aspects.
5.2 Negative Transfer Mitigation
Negative transfer mitigation can be achieved by enhancing data
transferability at various levels [ 38]. Feature decomposition is a
effective method. Instance weighting are also used to mitigate NT.
Some studies use active learning to select source samples or query
for unlabeled target samples [ 20,28]. MCTML [ 34] assign weights
for each sample cluster by iterative optimization. Also, using a
weighted aggregation of source domains may yield better perfor-
mance. Previous studies estimate domain similarity using various
indicators, such as MMD [ 7,26], KL-divergence [ 2,6], and corre-
lation coefficient [ 16,39]. The weights can also be determined by
optimization. For example, Ahmed et al. [ 1] learn the weights tominimize the model’s loss, while Richard et al. [ 22] estimate the
similarity by adversarial training.
5.3 Contrastive Learning for Recommendation
Contrastive learning is one of the most commonly used self-supervised
learning technique [ 15]. CL4SRec [ 32] uses item masking, item crop-
ping, and item reordering to augment user interaction sequences.
Considering data sparsity, CoSeRec [ 18] substitutes items or insert
related items into short sequences. In graph-based recommenda-
tion, SGL [ 29] applies node dropout, edge dropout, and random
walk on user-item bipartite graph, while EGLN [ 36] adds edges into
the graph to simulate users’ unobserved positive preferences.
6 CONCLUSION
In this paper, we focus on the negative transfer issue in multi-target
cross-domain recommendation and propose the TrineCDR model.
We claim that conventional feature decomposition is insufficient,
and propose a novel framework capturing single-domain and tar-
geted cross-domain embeddings. To our knowledge, we are the first
to introduce NT mitigation theory from transfer learning to CDR.
We consider every stage of knowledge transfer to identify three
fundamental causes of NT. Then we devise three KTE modules to
overcome these challenges. These designs effectively filter out ir-
relevant information, leading to more comprehensive and accurate
embeddings. The experimental results demonstrate the superiority
of TrineCDR, as it outperforms state-of-the-art baselines under all
circumstances, showcasing its ability to mitigate NT.
7 ACKNOWLEDGEMENT
This work is partially sponsored by National Key R&D Program of
China under Grant 2022YFB3104200, NSFC (62032003), and research
grant No. SH-2024JK29.
 
2753KDD ’24, August 25–29, 2024, Barcelona, Spain Zijian Song, et al.
REFERENCES
[1]Sk Miraj Ahmed, Dripta S Raychaudhuri, Sujoy Paul, Samet Oymak, and Amit K
Roy-Chowdhury. 2021. Unsupervised multi-source domain adaptation without
access to source data. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. 10103–10112.
[2]Ahmed M Azab, Lyudmila Mihaylova, Kai Keng Ang, and Mahnaz Arvaneh. 2019.
Weighted transfer learning for improving motor imagery-based brain–computer
interface. IEEE Transactions on Neural Systems and Rehabilitation Engineering 27,
7 (2019), 1352–1359.
[3]Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
Jennifer Wortman Vaughan. 2010. A theory of learning from different domains.
Machine learning 79 (2010), 151–175.
[4]Jiangxia Cao, Xixun Lin, Xin Cong, Jing Ya, Tingwen Liu, and Bin Wang. 2022.
Disencdr: Learning disentangled representations for cross-domain recommenda-
tion. In Proceedings of the 45th International ACM SIGIR Conference on Research
and Development in Information Retrieval. 267–277.
[5]Qiang Cui, Tao Wei, Yafeng Zhang, and Qing Zhang. 2020. HeroGRAPH: A Het-
erogeneous Graph Framework for Multi-Target Cross-Domain Recommendation..
InORSUM@ RecSys.
[6]Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. 2012. Geodesic flow
kernel for unsupervised domain adaptation. In 2012 IEEE conference on computer
vision and pattern recognition. IEEE, 2066–2073.
[7]Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and
Alexander Smola. 2012. A kernel two-sample test. The Journal of Machine
Learning Research 13, 1 (2012), 723–773.
[8]Lei Guo, Jinyu Zhang, Tong Chen, Xinhua Wang, and Hongzhi Yin. 2022. Rein-
forcement learning-enhanced shared-account cross-domain sequential recom-
mendation. IEEE Transactions on Knowledge and Data Engineering (2022).
[9]Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for
recommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval. 639–648.
[10] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international
conference on world wide web. 173–182.
[11] Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. Conet: Collaborative cross
networks for cross-domain recommendation. In Proceedings of the 27th ACM
international conference on information and knowledge management. 667–676.
[12] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition. 7132–7141.
[13] Chenglin Li, Yuanzhen Xie, Chenyun Yu, Bo Hu, Zang Li, Guoqiang Shu, Xiaohu
Qie, and Di Niu. 2023. One for All, All for One: Learning and Transferring User
Embeddings for Cross-Domain Recommendation. In Proceedings of the Sixteenth
ACM International Conference on Web Search and Data Mining. 366–374.
[14] Pan Li and Alexander Tuzhilin. 2020. Ddtcdr: Deep dual transfer cross domain
recommendation. In Proceedings of the 13th International Conference on Web
Search and Data Mining. 331–339.
[15] Zhi-Yuan Li, Man-Sheng Chen, Yuefang Gao, and Chang-Dong Wang. 2023. Signal
contrastive enhanced graph collaborative filtering for recommendation. Data
Science and Engineering 8, 3 (2023), 318–328.
[16] Yuan-Pin Lin and Tzyy-Ping Jung. 2017. Improving EEG-based emotion classifi-
cation using conditional transfer learning. Frontiers in human neuroscience 11
(2017), 334.
[17] Meng Liu, Jianjun Li, Guohui Li, and Peng Pan. 2020. Cross domain recom-
mendation via bi-directional transfer graph collaborative filtering networks. In
Proceedings of the 29th ACM international conference on information & knowledge
management. 885–894.
[18] Zhiwei Liu, Yongjun Chen, Jia Li, Philip S Yu, Julian McAuley, and Caiming
Xiong. 2021. Contrastive self-supervised sequential recommendation with robust
augmentation. arXiv preprint arXiv:2108.06479 (2021).
[19] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations
using distantly-labeled reviews and fine-grained aspects. In Proceedings of the
2019 conference on empirical methods in natural language processing and the 9th
international joint conference on natural language processing (EMNLP-IJCNLP).
188–197.
[20] Zhihao Peng, Wei Zhang, Na Han, Xiaozhao Fang, Peipei Kang, and Luyao Teng.
2019. Active transfer learning. IEEE Transactions on Circuits and Systems for
Video Technology 30, 4 (2019), 1022–1036.
[21] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint
arXiv:1205.2618 (2012).
[22] Guillaume Richard, Antoine de Mathelin, Georges Hébrail, Mathilde Mougeot,
and Nicolas Vayatis. 2021. Unsupervised multi-source domain adaptation for
regression. In Machine Learning and Knowledge Discovery in Databases: European
Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings,
Part I. Springer, 395–411.[23] Michael T Rosenstein, Zvika Marx, Leslie Pack Kaelbling, and Thomas G Diet-
terich. 2005. To transfer or not to transfer. In NIPS 2005 workshop on transfer
learning, Vol. 898.
[24] Pengyang Shao, Le Wu, Lei Chen, Kun Zhang, and Meng Wang. 2022. FairCF:
Fairness-aware collaborative filtering. Science China Information Sciences 65, 12
(2022), 222102.
[25] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural graph collaborative filtering. In Proceedings of the 42nd international ACM
SIGIR conference on Research and development in Information Retrieval. 165–174.
[26] Zirui Wang and Jaime Carbonell. 2019. Towards more reliable transfer learning.
InMachine Learning and Knowledge Discovery in Databases: European Conference,
ECML PKDD 2018, Dublin, Ireland, September 10–14, 2018, Proceedings, Part II 18.
Springer, 794–810.
[27] Zirui Wang, Zihang Dai, Barnabás Póczos, and Jaime Carbonell. 2019. Character-
izing and avoiding negative transfer. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. 11293–11302.
[28] Dongrui Wu, Brent Lance, and Vernon Lawhern. 2014. Transfer learning and
active transfer learning for reducing calibration data in single-trial classification
of visually-evoked potentials. In 2014 IEEE International Conference on Systems,
Man, and Cybernetics (SMC). IEEE, 2801–2807.
[29] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and
Xing Xie. 2021. Self-supervised graph learning for recommendation. In Proceed-
ings of the 44th international ACM SIGIR conference on research and development
in information retrieval. 726–735.
[30] Shitao Xiao, Yingxia Shao, Yawen Li, Hongzhi Yin, Yanyan Shen, and Bin Cui.
2022. LECF: recommendation via learnable edge collaborative filtering. Science
China Information Sciences 65, 1 (2022), 112101.
[31] Shuo Xiao, Dongqing Zhu, Chaogang Tang, and Zhenzhen Huang. 2023. Com-
bining Graph Contrastive Embedding and Multi-head Cross-Attention Transfer
for Cross-Domain Recommendation. Data Science and Engineering 8, 3 (2023),
247–262.
[32] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin
Ding, and Bin Cui. 2022. Contrastive learning for sequential recommendation. In
2022 IEEE 38th international conference on data engineering (ICDE). IEEE, 1259–
1273.
[33] Kun Xu, Yuanzhen Xie, Liang Chen, and Zibin Zheng. 2021. Expanding re-
lationship for cross domain recommendation. In Proceedings of the 30th ACM
International Conference on Information & Knowledge Management. 2251–2260.
[34] Yonghui Xu, Han Yu, Yuguang Yan, Yang Liu, et al .2020. Multi-component trans-
fer metric learning for handling unrelated source domain samples. Knowledge-
Based Systems 203 (2020), 106132.
[35] Huan Yan, Xiangning Chen, Chen Gao, Yong Li, and Depeng Jin. 2019. Deepapf:
Deep attentive probabilistic factorization for multi-site video recommendation.
TC2, 130 (2019), 17–883.
[36] Yonghui Yang, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2021. En-
hanced graph learning for collaborative filtering via mutual information max-
imization. In Proceedings of the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval. 71–80.
[37] Tianzi Zang, Yanmin Zhu, Haobing Liu, Ruohan Zhang, and Jiadi Yu. 2022. A
survey on cross-domain recommendation: taxonomies, methods, and future
directions. ACM Transactions on Information Systems 41, 2 (2022), 1–39.
[38] Wen Zhang, Lingfei Deng, Lei Zhang, and Dongrui Wu. 2022. A survey on
negative transfer. IEEE/CAA Journal of Automatica Sinica (2022).
[39] Wen Zhang and Dongrui Wu. 2020. Manifold embedded knowledge transfer for
brain-computer interfaces. IEEE Transactions on Neural Systems and Rehabilitation
Engineering 28, 5 (2020), 1117–1127.
[40] Cheng Zhao, Chenliang Li, and Cong Fu. 2019. Cross-domain recommendation
via preference propagation graphnet. In Proceedings of the 28th ACM international
conference on information and knowledge management. 2165–2168.
[41] Xiaoyun Zhao, Ning Yang, and Philip S Yu. 2022. Multi-sparse-domain collabora-
tive recommendation via enhanced comprehensive aspect preference learning.
InProceedings of the Fifteenth ACM International Conference on Web Search and
Data Mining. 1452–1460.
[42] Feng Zhu, Chaochao Chen, Yan Wang, Guanfeng Liu, and Xiaolin Zheng. 2019.
Dtcdr: A framework for dual-target cross-domain recommendation. In Proceed-
ings of the 28th ACM International Conference on Information and Knowledge
Management. 1533–1542.
[43] Feng Zhu, Yan Wang, Chaochao Chen, Guanfeng Liu, and Xiaolin Zheng. 2020.
A Graphical and Attentional Framework for Dual-Target Cross-Domain Recom-
mendation.. In IJCAI. 3001–3008.
[44] Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu.
2021. Cross-domain recommendation: challenges, progress, and prospects. arXiv
preprint arXiv:2103.01696 (2021).
[45] Feng Zhu, Yan Wang, Jun Zhou, Chaochao Chen, Longfei Li, and Guanfeng Liu.
2021. A unified framework for cross-domain and cross-system recommendations.
IEEE Transactions on Knowledge and Data Engineering (2021).
 
2754