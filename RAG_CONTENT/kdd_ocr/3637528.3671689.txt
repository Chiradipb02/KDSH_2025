Tensorized Unaligned Multi-view Clustering with Multi-scale
Representation Learning
Jintian Ji
Key Laboratory of Big Data &
Artificial Intelligence in
Transportation (Beijing Jiaotong
University), Ministry of Education
School of Computer Science and
Technology, Beijing Jiaotong
University
Beijing, China
22120385@bjtu.edu.cnSonghe Feng
Key Laboratory of Big Data &
Artificial Intelligence in
Transportation (Beijing Jiaotong
University), Ministry of Education
School of Computer Science and
Technology, Beijing Jiaotong
University
Beijing, China
shfeng@bjtu.edu.cnYidong Li‚àó
Key Laboratory of Big Data &
Artificial Intelligence in
Transportation (Beijing Jiaotong
University), Ministry of Education
School of Computer Science and
Technology, Beijing Jiaotong
University
Beijing, China
ydli@bjtu.edu.cn
ABSTRACT
The Unaligned Multi-view Clustering (UMC) problem is currently
receiving widespread attention, focusing on clustering unaligned
multi-view data generated in real-world applications. Although
some algorithms have emerged to address this issue, there still exist
the following drawbacks: 1) The fully unknown correspondence of
samples across views can significantly limit the exploration of con-
sistent clustering structure. 2) The fixed representation space makes
it difficult to mine the comprehensive information in the original
data. 3) Unbiased tensor rank approximation is desired to capture
the high-order correlation among different views. To address these
issues, we proposed a novel UMC framework termed Tensorized
Unaligned Multi-view Clustering with Multi-scale Representation
Learning (TUMCR). Specifically, TUMCR designs a multi-scale rep-
resentation learning and alignment framework, which constructs
multi-scale representation spaces to comprehensively explore the
unknown correspondence across views. Then, a tensorial multi-
scale fusion module is proposed to fuse multi-scale representations
and explore the high-order correlation hidden in different views,
which utilizes the Enhanced Tensor Rank (ETR) to learn the low-
rank structure. Furthermore, TUMCR is solved by an efficient algo-
rithm with good convergence. Extensive experiments on different
types of datasets demonstrate the effectiveness and superiority of
our TUMCR compared with state-of-the-art methods. Our code is
publicly available at: https://github.com/jijintian/TUMCR.
CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíMachine learning approaches;
‚Ä¢Information systems ‚ÜíClustering.
‚àóCorresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671689KEYWORDS
Unaligned Multi-view Clustering; Low-rank Tensor Regularization;
Multi-scale Representations.
ACM Reference Format:
Jintian Ji, Songhe Feng, and Yidong Li. 2024. Tensorized Unaligned Multi-
view Clustering with Multi-scale Representation Learning . In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671689
1 INTRODUCTION
Clustering is a basic task in unsupervised learning and data min-
ing, which aims to partition samples into certain categories. Many
algorithms such as ùëò-means [ 25] and spectral clustering [ 31] have
been able to achieve satisfactory performances. However, these
methods are limited to single-view data. With the development of
data science, multi-view data is rapidly becoming a mainstream
focus in data analysis, which portrays the samples from different
perspectives. For example, human fingerprints can be recorded by
different sensors, and sculptures can be photographed from mul-
tiple views. So, multi-view clustering [ 3] aims to utilize the rich
information hidden in different views to address the shortcomings
of traditional clustering and improve clustering performance.
Many multi-view clustering methods have emerged, such as
kernel-based multi-view clustering algorithms [ 17,20,35], graph-
based multi-view clustering algorithms [ 8,16,41,44], and subspace-
based multi-view clustering algorithms[ 4,7,47], etc. These algo-
rithms can achieve satisfactory performance in some areas. How-
ever, they are carried out under ideal multi-view data (e.g., Fig. 1.
(a)). In real-world applications, due to the instability of the data
acquisition and transmission process, multi-view data often face
different degrees of problems, such as the missing problem and
the unaligned problem. The missing problem is also called the In-
complete Multi-view Clustering problem (IMC) [ 15,21,22,42,48],
which has attracted a lot of attention and brought many innova-
tive solutions. In this paper, we focus on the unaligned problem,
which is also called the Unaligned Multi-view Clustering (UMC)
problem. In most of the current studies, the UMC problem is usually
set up in two different ways and thus divided into two types of
 
1246
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
(a)
 (b)
 (c)
Figure 1: The motivation of this paper. In the figure, different
colors denote different samples and different shapes indicate
different categories, the dotted lines represent the desired
correspondence. (a) Traditional multi-view data. (b) Partially
view-aligned problem: only a portion of data is with the
known correspondence; (c) Fully unaligned problem: The
correspondence of all samples between views is unknown
due to data collection and transmission errors.
problems. The first type is called the partially view-aligned prob-
lem [ 9,37,39,45](e.g., Fig. 1. (b)), which assumes that a portion
of the samples in the multi-view data have known inter-view cor-
respondence and the rest are unknown. These problems typically
utilize known correspondence as supervised signals to predict the
correspondence of unobserved samples. For example, Zhang et al.
[45] utilize the existing inter-view constraints to obtain the mutual
information among views. Huang et al. [9] take advantage of the
aligned data to learn a neural network to establish the category-
level correspondence of the unaligned data. Yang et al. [39] propose
a noise-robust contrastive loss to ensure that aligned samples in
the representation space are pulled together while the unaligned
samples are pushed away from each other.
The second type is termed the fully unaligned problem [ 14,36,
40] (e.g., Fig. 1. (c)), which is more challenging to solve as compared
with the previous type. In this setting, the inter-view correspon-
dence of all samples is unknown. In order to solve such problems,
the hidden information in the multi-view data is usually exploited
to establish connections between views. For instance, Yu et al. [40]
explore the graph-structure consistency for all views based on the
non-negative matrix factorization framework. Lin et al. [14] use
a view-specific coupling matrix to establish the inter-view corre-
spondence in the self-representation space and adopt the low-rank
tensor learning to explore the high-order correlation among views.
Wen et al. [36] propose a novel parameter-free graph clustering
framework to refine cross-view correspondence. Although someprogress has been made by these algorithms, there are still two
main issues that need to be addressed. The first issue is that all these
methods explore the inter-view correspondence under a fixed latent
dimension, which affects the expressive ability of the model and the
establishment of intrinsic correspondence between views. The sec-
ond issue lies in devising an effective low-rank tensor constraint to
capture the high-order correlation among views in existing view fu-
sion frameworks since the Tensor Nuclear Norm (TNN) constraint
has trouble in suppressing noisy information in the representation
tensor.
Being aware of the above issues, we proposed a novel framework
to tackle the fully unaligned problem, which is termed Tensorized
Unaligned Multi-view Clustering with Multi-scale Representation
Learning (TUMCR) (Fig. 2). Specifically, TUMCR obtains multi-
scale representation matrices under the Matrix Factorization (MF)
framework, and the cross-view mapping matrices can be well estab-
lished in multi-scale spaces. Then we propose a tensorial multi-scale
fusion module to integrate representation matrices in multi-scale
spaces and explore high-order correlation among views. Specifically,
the Enhanced Tensor Rank is adopted for the low-rank structural
constraint, which leads to more compact clustering representations.
We summarize the main contributions as follows:
(1)Different from most of the existing methods, TUMCR ex-
plores the inter-view correspondence in multi-scale spaces,
which can adequately integrate the information of unaligned
multi-view data from different views, thus improving the
accuracy of the alignment and clustering.
(2)To integrate multi-scale representations, TUMCR proposes
a tensorial multi-scale fusion module, which fuses the in-
formation of different scales by assigning adaptive weights.
Then a low-rank tensor learning framework with Enhanced
Tensor Rank is adopted to explore the high-order correlation
among views.
(3)We develop an alternative optimization algorithm with good
convergence. To validate the effectiveness, we conduct ex-
tensive experiments on various datasets. The results demon-
strate the efficiency and excellent performance of TUMCR.
The remainder of this paper is organized as follows. Section 2
presents notations and preliminaries. Section 3 details our proposed
method and framework. In Section 4, we present optimization,
complexity analysis, convergence analysis, and complexity analysis.
The experimental results and analysis are shown in Section 5. At
last, Section 6 concludes this paper.
2 NOTATIONS AND PRELIMINARIES
2.1 Notations
In this paper, bold-lowercase ùíôand lowercase letters ùë•represent
a vector and a scalar, respectively. Bold-uppercase Xdenotes the
matrix. Iùëõis theùëõ√óùëõidentity matrix. Calligraphy letter denotes the
tensorX‚ààRùëõ1√óùëõ2√óùëõ3.Xùëòis theùëò-th frontal slice ofX.Xùëìmeans
the Fast Fourier Transformation (FFT) along the third dimension
of tensorX. And some tensor-related operations[ 13] are described
as follows. We first define two three-order tensors A‚ààRùëõ1√óùëõ2√óùëõ3
andB‚ààRùëõ2√óùëõ4√óùëõ3.
‚Ä¢Transposition of tensor: Aùëá‚ààRùëõ2√óùëõ1√óùëõ3.
 
1247Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Figure 2: The framework of our proposed TUMCR. The method consists primarily of two parts: the multi-scale alignment
module and the tensorial multi-scale fusion model. The multi-scale alignment module learns the inter-view correspondence
in the multi-scale spaces. The tensorial multi-scale fusion model integrates the information captured in different scales and
adopts a low-rank tensor learning framework to enhance the consistency among different views.
‚Ä¢Cyclic expansion of the tensor ùëêùëñùëüùëê(A)‚àà Rùëõ1ùëõ3√óùëõ2ùëõ3:
ùëêùëñùëüùëê(A)=Ô£ÆÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞A1Aùëõ3¬∑¬∑¬∑ A2
A2A1¬∑¬∑¬∑ A3
............
Aùëõ3Aùëõ3‚àí1¬∑¬∑¬∑ A1Ô£πÔ£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª. (1)
‚Ä¢Tensor unfolding and folding:
ùë¢ùëõùëìùëúùëôùëë(A)=
A1,A2,¬∑¬∑¬∑,Aùëõ3ùëá‚ààRùëõ1ùëõ3√óùëõ2.
A=ùëìùëúùëôùëë(ùë¢ùëõùëìùëúùëôùëë(A)).(2)
‚Ä¢t-productA‚àóB‚àà Rùëõ1√óùëõ4√óùëõ3:
A‚àóB =ùëìùëúùëôùëë(ùëêùëñùëüùëê(A)¬∑ùë¢ùëõùëìùëúùëôùëë(B)). (3)
‚Ä¢Orthogonal tensor: The tensor Ais orthogonal ifAùëá‚àóA=
A‚àóAùëá=I.
Based on the above tensor operations, the definition of the tensor-
Singular Value Decomposition (t-SVD) and Tensor Nuclear Norm
(TNN) are defined as follows:
Definition 1. (t-SVD)[ 12] Given tensorA‚ààRùëõ1√óùëõ2√óùëõ3, then the
t-SVD ofAis:
A=U‚àóS‚àóVùëá(4)
whereU‚ààRùëõ1√óùëõ1√óùëõ3andV‚ààRùëõ2√óùëõ2√óùëõ3are orthogonal tensors,
S‚ààRùëõ1√óùëõ2√óùëõ3is aùëì-diagonal tensor.
Definition 2. (TNN)[ 23,24,49] Given a tensorA‚ààRùëõ1√óùëõ2√óùëõ3,
then the tensor nuclear norm is defined as:
‚à•A‚à•‚äõ=1
ùëõ3ùëõ3‚àëÔ∏Å
ùëò=1‚Ñé‚àëÔ∏Å
ùëñ=1Sùëò
ùëì(ùëñ,ùëñ) (5)where‚Ñé=min(ùëõ1,ùëõ2)andSùëìis from the t-SVD of A=U‚àóS‚àóVùëá
in Fourier domain.
Multi-view dataset with ùëõsamples and ùëöviews is denoted as
{Xùë£}ùëö
ùë£=1, where Xùë£‚ààRùëõ√óùëëùë£,ùëëùë£is the feature dimension of ùë£-th
view.ùëêis the cluster number of this dataset.
2.2 Multi-view Matrix Factorization Clustering
The Matrix Factorization (MF) technique [ 2,5,26] is an effective
method for clustering, especially for high-dimensional datasets. In
single-view clustering problem, it aims to decompose the original
feature Xùë†ùëñùëõùëîùëôùëí‚ààRùëõ√óùëëinto two parts, denoted as representation
matrix Zùë†ùëñùëõùëîùëôùëí‚ààRùëõ√óùëêand base matrix Hùë†ùëñùëõùëîùëôùëí‚ààRùëê√óùëë.
min
{Zùë†ùëñùëõùëîùëôùëí,Hùë†ùëñùëõùëîùëôùëí}L(Xùë†ùëñùëõùëîùëôùëí,Zùë†ùëñùëõùëîùëôùëí Hùë†ùëñùëõùëîùëôùëí), (6)
whereL(¬∑) is the loss function that is usually measured by the
Frobenius norm [ 18]. To improve the distinguishability of the repre-
sentation matrix Zùë†ùëñùëõùëîùëôùëí , Non-negative Matrix Factorization (NMF)
[1,5] regularizes Zùë†ùëñùëõùëîùëôùëí andHùë†ùëñùëõùëîùëôùëí to be non-negative, [ 6,32]
further impose orthogonality constraints on the base matrix or
representation matrix.
Extending Eq. (6) to the multi-view clustering problem, we can
obtain the general formula of MF-based multi-view clustering meth-
ods:
min
{Zùë£,Hùë£}ùëö
ùë£=1L({Xùë£,Zùë£Hùë£}ùëö
ùë£=1)+ùúÜR({Zùë£}ùëö
ùë£=1,{Hùë£}ùëö
ùë£=1)(7)
whereùúÜis a trade-off parameter, {Hùë£}ùëö
ùë£=1represents a group of
base matrices,R(¬∑) denotes some constraint terms on {Zùë£}ùëö
ùë£=1and
{Hùë£}ùëö
ùë£=1. For example, [ 32] imposes orthogonality constraints to
improve the discriminability of representation matrices. The graph
 
1248KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
Laplacian regularization is adopted in [ 46] to preserve the geomet-
ric structure. After learning the representation matrix Zùë£of each
view, the consensus representation matrix Zùë¢ùëõùëñùëìùëñùëíùëë is obtained by
fusion strategies (i.e., linear weighted fusion method [ 34] or spectral
rotation fusion method [ 43]), which is then fed into ùëò-means or
spectral clustering to acquire the final clustering results.
3 PROPOSE METHOD
3.1 Multi-scale Representation Learning and
Alignment
Existing unaligned multi-view clustering methods usually first map
the original features of various views into a unified space with a
group of base matrices, then explore the inter-view correspondence
in the fixed space. Thus the alignment is extremely reliant on the
representation matrices of the mapping space. However, in multi-
view data, data matrices from multiple sources usually belong to
specific latent dimensions, which means that just one feature space
alone makes it difficult to accurately characterize all views. Most
works ignore this shortcoming and simply equate it to the number
of clusters to avoid selecting the dimensions. Others tackle the
problem by introducing a hyperparameter, which increases the
computational burden during the optimization stage. To this end,
our proposed TUMCR tries to map the original features of different
views into various spaces and explore the cross-view mapping
matrices with these multi-scale representations. Specifically, we
chooseùëügroups of basis matrices {Hùë£ùëû}ùë£=1,...,ùëö
ùëû=1,...,ùëüto map the original
feature into the ùëüspaces with dimensions {ùëë1,¬∑¬∑¬∑,ùëëùëü}, so we can
obtain the multi-scale representations by solving the following
problem:
min
{Zùë£ùëû,Hùë£ùëû}ùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Xùë£‚àíZùë£
ùëûHùë£
ùëû2
ùêπ
ùë†.ùë°.‚àÄùë£,ùëû,Zùë£
ùëûùëáZùë£
ùëû=Iùëëùëû,(8)
where Zùë£ùëû‚ààRùëõ√óùëëùëûdenotes the ùë£-th view representation matrix
inùëû-th space. Then we need to establish the cross-view mapping
matrices. Since the complexity of searching for mapping matrices
for any two views is extremely high, a common operation is to
assign a central view to align the others. To avoid introducing
redundant complexity, an arbitrary view is randomly selected as
the central view in our proposed TUMCR, denoted as Zùëêùëúùëüùëíùëû. So we
only need to build ùëöcross-view mapping matrices {Pùë£}ùëö
ùë£=1, when
ùë£=ùëêùëúùëüùëí,Pùëêùëúùëüùëí=Iùëõ. Then the multi-scale alignment module can
be formalized as:
min
{Pùë£}ùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Zùëêùëúùëüùëí
ùëû‚àíPùë£Zùë£
ùëû2
ùêπ
ùë†.ùë°.‚àÄùë£,Pùë£‚àà{0,1}ùëõ√óùëõ,Pùë£1=1,Pùë£ùëá1=1,(9)
The problem in Eq. (9) is a classical NP-hard problem, so we adopt
the spectral relaxation [ 30] to simplify the problem, and taking the
multi-scale representation construction into account, the objectivefunction becomes:
min
{Pùë£,Zùë£ùëû,Hùë£ùëû}ùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Xùë£‚àíZùë£
ùëûHùë£
ùëû2
ùêπ+ùúÉùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Zùëêùëúùëüùëí
ùëû‚àíPùë£Zùë£
ùëû2
ùêπ
ùë†.ùë°.‚àÄùë£,ùëû,Zùë£
ùëûùëáZùë£
ùëû=Iùëëùëû,Pùë£Pùë£ùëá=Iùëõ,
(10)
whereùúÉis a hyperparameter.
3.2 Tensorial Multi-scale Fusion Module
The multi-scale representation matrices {Zùë£ùëû}ùë£=1,...,ùëö
ùëû=1,...,ùëüand the cross-
view mapping matrix Pùë£can be obtained by solving the Eq. (10),
but the representation matrices in different scales cannot be fused
directly, so we first map them into a ùëê-dimension space by a group
of matrices{Wùëû}ùëü
ùëû=1. Since the information characterized by dif-
ferent scales has varying importance, we further introduce scale
weights ùú∑‚ààRùëü√ó1to automatically balance the contributions of
different scales and improve the discriminative properties of the
fused representations {Gùë£}ùëö
ùë£=1‚ààRùëõ√óùëê,
min
{Gùë£,Wùëû}‚àíùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1ùú∑ùëûùëáùëü(Gùë£Wùëû(Pùë£Zùë£
ùëû)ùëá)
ùë†.ùë°.‚àÄùë£,ùëû,WùëûWùëûùëá=Iùëê,Gùë£ùëáGùë£=Iùëê,
ùëü‚àëÔ∏Å
ùëû=1ùú∑2
ùëû=1,ùú∑ùëû‚â•0.(11)
To capture the high-order correlation among views, the low-
rank tensor framework is usually adopted. In traditional tensor-
based works, the approximation of tensor rank often employs the
commonly used Tensor Nuclear Norm (TNN), which results in
under-penalizing the smaller singular values and over-penalizing
the larger singular values. Note that the large (small) singular values
usually represent the main structural information (the noise) of
the tensor, thus TNN often derives noise residuals that disrupt the
clustering structure. Being aware of this, we adopt a noise-robust
Enhanced Tensor Rank (ETR) (Definition 3) [ 10] to approximate the
true rank of representation tensor G=Œ¶(G1,¬∑¬∑¬∑,Gùëö)‚ààRùëõ√óùëê√óùëö,
where Œ¶denotes the merging operation.
Definition 3. [10] Given a tensorG‚ààRùëõ1√óùëõ2√óùëõ3, then the En-
hanced Tensor Rank (ETR) is defined as:
‚à•G‚à•ETR=1
ùëõ3ùëõ3‚àëÔ∏Å
ùëò=1Gùëò
ùëìETR
=1
ùëõ3ùëõ3‚àëÔ∏Å
ùëò=1‚Ñé‚àëÔ∏Å
ùëñ=1¬©¬≠
¬´ùëíùõø2Sùëò
ùëì(ùëñ,ùëñ)
ùõø+Sùëò
ùëì(ùëñ,ùëñ)¬™¬Æ
¬¨,(12)
where 0<ùõø‚â§1,‚Ñé=min(ùëõ1,ùëõ2)andSùëìis obtained by t-SVD of
Gùëì=UùëìSùëìVùëá
ùëìin Fourier domain.Gùëò
ùëìmeans theùëò-th frontal slice
of the tensorZùëì.
By simultaneously considering Eq. (10), Eq. (11) and Eq. (12), the
final objective function of our TUMCR is formulated as:
 
1249Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
min
{Gùë£,Pùë£,Zùë£ùëû,Hùë£ùëû,Wùëû}‚à•G‚à•ETR‚àíùõºùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1ùú∑ùëûùëáùëü(Gùë£Wùëû(Pùë£Zùë£
ùëû)ùëá)
+ùõæùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Xùë£‚àíZùë£
ùëûHùë£
ùëû2
ùêπ+ùúÉùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Zùëêùëúùëüùëí
ùëû‚àíPùë£Zùë£
ùëû2
ùêπ
ùë†.ùë°.‚àÄùë£,ùëû,Zùë£
ùëûùëáZùë£
ùëû=Iùëëùëû,Wùë£
ùëûWùë£
ùëûùëá=Iùëê,Pùë£Pùë£ùëá=Iùëõ,
G=Œ¶(G1,¬∑¬∑¬∑,Gùëö),Gùë£ùëáGùë£=Iùëê,ùëü‚àëÔ∏Å
ùëû=1ùú∑2
ùëû=1,ùú∑ùëû‚â•0(13)
whereùõº,ùõæ,ùúÉare the trade-off parameters.
Once the optimal representation matrices {Gùë£}ùëö
ùë£=1are obtained,
the unified affinity matrix can be fused as SG=1
ùëö√çùëö
ùë£=1Gùë£Gùë£ùëá‚àà
Rùëõ√óùëõ. Inspired by [ 11], the spectral embedding F‚ààRùëõ√óùëêcan be
obtained by directly imposing singular value decomposition (SVD)
onÀÜG=1‚àöùëö[G1,¬∑¬∑¬∑,Gùëö]‚ààRùëõ√óùëöùëêaccording to Theorem 1, which
enjoys a linear time complexity O(ùëõùëö2ùëê2).
Theorem 1. The left singular vectors of ÀÜG=1‚àöùëö[G1,¬∑¬∑¬∑,Gùëö]‚àà
Rùëõ√óùëöùëêis the same as the eigenvectors of S=ÀÜGÀÜGùëá.
Proof The singular value decomposition (SVD) of ÀÜGis denoted
asUŒ£Vùëá, then
S=ÀÜGÀÜGùëá=(UŒ£Vùëá)(UŒ£Vùëá)ùëá=UŒ£2Uùëá. (14)
Thereby, the left singular vectors of ÀÜGare the same as eigenvectors
ofS. ‚ñ°
4 OPTIMIZATION
To solve problem (13), we first introduce the auxiliary tensor vari-
ableJ, and obtain the following augmented Lagrangian function,
L({Zùë£
ùëû}ùë£=1,...,ùëö
ùëû=1,...,ùëü,{Gùë£}ùëö
ùë£=1,{Hùë£
ùëû}ùë£=1,...,ùëö
ùëû=1,...,ùëü,J,Pùë£,I)
=‚à•J‚à•ùê∏ùëáùëÖ‚àíùõºùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1ùú∑ùëûùëáùëü(Gùë£Wùëû(Pùë£Zùë£
ùëû)ùëá)
+ùõæùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Xùë£‚àíZùë£
ùëûHùë£
ùëû2
ùêπ+ùúÉùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Zùëêùëúùëüùëí
ùëû‚àíPùë£Zùë£
ùëû2
ùêπ
+‚ü®I,G‚àíJ‚ü©+ùúå
2‚à•G‚àíJ‚à•2
ùêπ,(15)
whereIis Lagrange multiplier and ùúåis penalty parameter. Then,
we solve the variables in Eq. (15) through the following five sub-
problems.
4.1 Update Zùë£
ùëû
When fixing other variables, the problem with Zùë£ùëûis formulated as
arg min
Zùë£ùëûùëáZùë£ùëû=Iùëëùëû‚àíùõºùú∑ùëûùëáùëü(Gùë£Wùëû(Pùë£Zùë£
ùëû)ùëá)+ùõæXùë£‚àíZùë£
ùëûHùë£
ùëû2
ùêπ
+ùúÉZùëêùëúùëüùëí
ùëû‚àíPùë£Zùë£
ùëû2
ùêπ
‚áíZùë£‚àó
ùëû=arg max
Zùë£ùëûùëáZùë£ùëû=Iùëëùëûùëáùëü(Zùë£
ùëûùëáMùëç),(16)where Mùëç=ùõºùõΩùëûPùë£ùëáGùë£Wùëû+2ùõæXùë£Hùë£ùëûùëá+2ùúÉPùë£ùëáZùëêùëúùëüùëíùëû. The optimal
solution of Zùë£ùëûisUùëçVùëçùëá, where UùëçandVùëçare the left and right
singular matrix of Mùëç.
4.2 Update Gùë£
Fixing the other variables leads to the following problem for Gùë£,
Gùë£‚àó=arg max
Gùë£ùëáGùë£=Iùëêùëáùëü(Gùë£ùëáMùê∫),(17)
where Mùê∫=√çùëü
ùëû=1ùõºùú∑ùëûPùë£Zùë£ùëûWùëûùëá‚àíIùë£‚àíJùë£. The optimal solution
ofGùë£isUùê∫Vùê∫ùëá, where Uùê∫andVùê∫are the left and right singular
matrix of Mùê∫.
4.3 Update Pùë£
Fixing the other variables, we obtain the problem for Pùë£,
Pùë£‚àó=arg max
Pùë£ùëáPùë£=Iùëõùëáùëü(Pùë£ùëáMùëÉ),(18)
where MùëÉ=√çùëü
ùëû=1(ùõºùú∑ùëûGùë£WùëûZùë£ùëûùëá+2ùúÉZùëêùëûZùë£ùëûùëá)The optimal solu-
tion of Pùë£isUùëÉVùëÉùëá, where UùëÉandVùëÉare the left and right singular
matrix of MùëÉ.
4.4 UpdateJ
When other variables are fixed, the subproblem for Gis formulated
as,
arg min
J1
ùúå‚à•J‚à•ùê∏ùëáùëÖ+1
2J‚àí(G+I
ùúå)2
ùêπ. (19)
We refer to this problem as the Enhanced Tensorial Rank Mini-
mization problem (ETRM) [ 10], which can be solved with the fol-
lowing theorem.
Theorem 2. [10] SupposeA ‚àà Rùëõ1√óùëõ2√óùëõ3with t-SVDA=
U‚àóS‚àóVùëáandùõΩ>0. The Enhanced Tensorial Rank Minimization
problem (ETRM) can be described as follows,
arg min
GùõΩ‚à•G‚à•ùê∏ùëáùëÖ+1
2‚à•G‚àíA‚à•2
ùêπ. (20)
Then, optimal solution G‚àóis obtained as,
G‚àó=U‚àóùëñùëìùëìùë°(ùëÉùëüùëúùë•ùëì,ùõΩ(Sùëì),[],3)‚àóVùëá, (21)
whereùëñùëìùëìùë°(ùëÉùëüùëúùë•ùëì,ùõΩ(Sùëì),[],3)‚ààRùëõ1√óùëõ2√óùëõ3is a f-diagonal tensor,
andùëÉùëüùëúùë•ùëì,ùõΩ(Sùëò
ùëì(ùëñ,ùëñ))satisfies the following equation,
ùëÉùëüùëúùë•ùëì,ùõΩ(Sùëò
ùëì(ùëñ,ùëñ))=arg min
ùë•‚â•01
2(ùë•‚àíSùëò
ùëì(ùëñ,ùëñ))2+ùõΩùëì(ùë•), (22)
whereùëì(ùë•)=ùëíùõø2ùë•
ùõø+ùë•.
The proof of Theorem 2 is given in [ 10]. Then we adopt the
difference of convex (DC) programming [ 29] to obtain the closed-
form solution,
ùúèùëñùë°ùëíùëü+1=
Sùëò
ùëì(ùëñ,ùëñ)‚àíùúïùëì(ùúèùëñùë°ùëíùëü)
ùúå
+, (23)
whereùúè=ùëÉùëüùëúùë•ùëì,ùõΩ(Sùëò
ùëì(ùëñ,ùëñ)),ùëì(ùë•)=ùëíùõø2ùë•
ùë•+ùõøandùëñùë°ùëíùëüis the number
of iterations.
 
1250KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
4.5 Update Hùë£
ùëû
Fixing the other variables, Hùë£ùëûcan be updated by,
Hùë£‚àó
ùëû=arg min‚à•Xùë£‚àíZùë£
ùëûHùë£
ùëû‚à•2
ùêπ. (24)
Differentiating the objective function respecting Hùë£ùëûand setting
the derivative to zero, it is obtained that Hùë£ùëûis updated by
Hùë£‚àó
ùëû=Zùë£
ùëûùëáXùë£. (25)
4.6 Update Wùëû
Fixing the other variables, Wùëûcan be updated by,
W‚àó
ùëû=arg max
WùëûWùëûùëá=Iùëêùëáùëü(WùëûùëáMùëä),(26)
where Mùëä=√çùëö
ùë£=1Gùë£ùëáPùë£Zùë£ùëû. The optimal solution of WùëûisUùëäVùëäùëá,
where UùëäandVùëäare the left and right singular matrix of Mùëä.
4.7 Update ùú∑ùëû
With other variables fixed in Eq. (15), ùú∑ùëûcan be updated by the
following formula
maxùëü‚àëÔ∏Å
ùëû=1ùú∑ùëûùúÇùëû, ùë†.ùë°.ùëü‚àëÔ∏Å
ùëû=1ùú∑2
ùëû=1,ùú∑ùëû‚â•0, (27)
whereùúÇùëû=√çùëö
ùë£=1ùëáùëü(Gùë£WùëûZùë£ùëûùëáPùë£). The optimal solution is
ùú∑ùëû=ùúÇùëû‚àöÔ∏É√çùëü
ùëû=1ùúÇ2ùëû(28)
At last, the Lagrange multipliers and penalty parameters are
updated as follows,
I=I+ùúå(G‚àíJ)
ùúå=ùúèùúåùúå, ùúå=min(ùúå,ùúåmax)(29)
whereùúèùúå>1is used to accelerate convergence. The complete
procedure is summarized in Algorithm 1.
Algorithm 1 Optimization Algorithm of TUMCR
Input: Multi-view data{X1,...,Xùëö}, cluster number ùëê, trade-off
parametersùõæ,ùõºandùúÉ.
Initialize: SetHùë£ùëû,Zùë£ùëû,Gùë£,Wùëûto zero matrix ,J=I=0,ùúá=
10‚àí5,ùúå=10‚àí4,ùúèùúå=2,ùúåùëöùëéùë•=1010,ùúñ=10‚àí7.
1:while not converge do
2: Update Zùë£ùëûby Eq. (16);
3: Update Gùë£by Eq. (17);
4: Update Pùë£by Eq. (18);
5: UpdateJby Eq. (19);
6: Update Hùë£ùëûby Eq. (25);
7: Update Wùëûby Eq. (26);
8: Update ùú∑ùëûby Eq. (28);
9: UpdateùúåandIby Eq.(29);
10: Check the convergence conditions:
‚à•Z‚àíG‚à•‚àû<ùúñ
11:end while
12:Output the clustering results via performing ùëò-means clustering
onF.4.8 Convergence Analysis
The convergence of Algorithm 1 is theoretically guaranteed by the
following Theorem 3, the proof of this theorem is similar to the
convergence analysis in the work [38].
Theorem 3. Let{Yùëò=(G(ùëò),I(ùëò),J(ùëò))}‚àû
ùëò=1be the sequence
generated by Algorithm 1, then the sequence {Yùëò}‚àû
ùëò=1satisfies the
following two principles:
1).{Yùëò}‚àû
ùëò=1is bounded.
2).Any accumulation point of {Yùëò}‚àû
ùëò=1is a KKT point of Eq. (15).
4.9 Time Complexity Analysis
The mean computational complexity of TUMCR focuses on updat-
ing the variables (i.e., Zùë£ùëû,Hùë£ùëû,Pùë£,Gùë£,Wùëû,J) in problem (15), which
takeO(ùëõùëë2ùëû),O(ùëõùëëùë£ùëëùëû),O(ùëõ3),O(ùëõùëê2),O(ùëëùëûùëê2)andO(ùëõùëöùëêùëôùëúùëî(ùëõ)+
ùëõùëö2ùëê), respectively. So, the time complexity of TUMCR is O(ùëõ3+
ùëõùëöùëêùëôùëúùëî(ùëõ)+ùëõùëëùë£ùëëùëû).
Table 1: Details of the used datasets.
Dataset Sample Cluster View Type
Yale 165 15 3 Image
MSRCv1 210 7 4 Image
NGs 500 5 3 Text
BBCSport 544 5 2 Text
Caltech101-7 1474 7 6 Object
100leaves 1600 100 3 Image
Caltech101-20 2386 20 6 Object
BDGP 2500 5 2 Genome
CCV 6773 20 3 Video
5 EXPERIMENT
5.1 Experimental Settings
Datasets: Nine challenging datasets are adopted to validate our
TUMCR, including Yale1,MSRCv1, NGs2,BBCSport3,Caltech101-
7,100leaves4,Caltech101-205,BDGP6andCCV7. More details
can be found in Table 1. Then we randomly arrange samples in all
views excluding the central view to construct the fully unaligned
multi-view datasets.
Baselines: To validate the superiority of TUMCR, nine state-
of-the-art multi-view clustering methods are selected for com-
parison, including, AWP (2018) [ 27],GMC(2019) [ 33],EOMSC-
CA(2022) [ 19],SMVSC (2021) [ 28],AWMVC (2023)[ 32],T-UMC
(2022) [ 14] and three variations of UPMGC (i.e.,CoMSC+UPMGC,
LSR+UPMGC andGMC+UPMGC) (2023) [36], respectively.
Evaluation Metrics: We employ three commonly used met-
rics to measure the clustering quality, including accuracy (ACC),
normalized mutual information (NMI), and purity (PUR). For all
metrics, the larger values indicate better performances.
1http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html
2https://lig-membres.imag.fr/grimal/data.html
3http://mlg.ucd.ie/datasets/segment.html
4https://archive.ics.uci.edu/ml/datasets/One-hundred+plant+species+leaves+data+set.
5http://www.vision.caltech.edu/Image_Datasets/Caltech101/
6https://www.fruitfly.org/
7http://www.ee.columbia.edu/ln/dvmm/CCV/
 
1251Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Table 2: Results of our proposed method and other compared methods on nine datasets. NAN means the program has a ‚ÄúNAN‚Äù
or ‚ÄúINF‚Äù problem.
Dataset
Metric AWP GMC EOMSC-CA SMVSC AWMVC T-UMC CoMSC+UPMGC LSR+UPMGC GMC+UPMGC Ours
Y
aleACC 0.3636 0.3152 0.3394 0.2909 0.5455 0.6509 0.6381 0.6728 0.5042 0.7152
NMI
0.3903 0.3402 0.3490 0.3486 0.5409 0.6858 0.6694 0.6982 0.5374 0.7887
P
UR 0.3818 0.3333 0.3515 0.3091 0.5455 0.6521 0.6474 0.6913 0.5088 0.7333
MSRCv1A
CC 0.2571 0.1905 0.3762 0.3476 0.3048 0.5200 0.7022 0.4075
0.3659 0.8714
NMI 0.0808 0.0478 0.2223 0.1612 0.1551 0.4392 0.6126 0.3322
0.2497 0.7722
PUR 0.2810 0.1952 0.3762 0.3524 0.3238 0.5295 0.7135 0.4726
0.3898 0.8714
BBCSp
ortACC 0.4853 0.4099 0.3548 0.4265 0.4504 0.954 0.9206
0.2658 0.5242 0.9669
NMI 0.1633 0.0824 0.0585 0.1003 0.2483 0.8600 0.7913
0.0183 0.3780 0.8904
PUR 0.4982 0.4154 0.4228 0.4522 0.5147 0.9540 0.9229
0.3655 0.6007 0.9669
NGsA
CC 0.4180 0.2320 0.2800 0.3140 0.3200 0.7544 0.8686 0.3682
0.3423 0.8820
NMI 0.1293 0.0270 0.0215 0.0345 0.0967 0.5631 0.6790 0.1203
0.1193 0.7097
PUR 0.4360 0.2340 0.2860 0.3140 0.3240 0.7544 0.8686 0.3761
0.3439 0.8820
Calte
ch101-7ACC 0.4389 0.5421 0.5434 0.3467 0.3426 0.3847 0.5951 0.3807
0.3337 0.6927
NMI 0.0378 0.0077 0.0075 0.0171 0.2143 0.3243 0.5586 0.2619
0.2212 0.1764
PUR 0.5461 0.5434 0.5448 0.5421 0.7001 0.7757 0.8472 0.7630
0.6671 0.6974
100leav
esACC 0.3913 0.1606 0.1313 0.1594 0.1875 0.6202 0.6307 0.1636
0.1839 0.7631
NMI 0.5813 0.2671 0.3268 0.4322 0.4781 0.7999 0.8200 0.4685
0.4843 0.8988
PUR 0.4031 0.1713 0.1475 0.1688 0.1981 0.6467 0.6653 0.1733
0.1947 0.7919
Calte
ch101-20ACC 0.1220 0.3336 0.3550 0.1572 0.2578 0.3449 0.3726 0.2644
0.2399 0.5448
NMI 0.0525 0.0182 0.0616 0.0411 0.2476 0.3782 0.5297 0.3171
0.2395 0.6969
PUR 0.3391 0.3399 0.3822 0.3378 0.4904 0.6169 0.7036 0.5490
0.4742 0.8407
BDGPA
CC 0.3624 0.1444 0.3216 0.4352 0.3996 NAN 0.5853 NAN
0.3542 0.9140
NMI 0.1002 0.1933 0.0923 0.1748 0.1870 NAN 0.3742 NAN
0.1214 0.7835
PUR 0.3624 0.7012 0.3268 0.4412 0.4040 NAN 0.6089 NAN
0.3804 0.9140
CCVA
CC 0.1000 0.1059 0.1078 0.2252 0.1084
NAN 0.2086 0.2011 0.1620 0.4310
NMI 0.0224 0.0085 0.0338 0.1618 0.0477 NAN 0.1839 0.1699
0.1227 0.5249
PUR 0.1255 0.1078 0.1236 0.2541 0.1410
NAN 0.2386 0.2268 0.1905 0.4859
Parameter Setting: For our TUMCR, five parameters need to be
tuned,ùõº,ùõæ, andùúÉare trade-off parameters to balance the importance
of different terms in the objective function, which take the range of
{10‚àí3,¬∑¬∑¬∑,1},{10‚àí3,¬∑¬∑¬∑,103}and{10‚àí3,¬∑¬∑¬∑,101}, respectively.
The parameter ùõøin ETR has a search range of {0.001,0.01,0.1,1}.
The dimensions of scale space are selected from {2ùëê,3ùëê,4ùëê}. For all
datasets, we use the first view as the central view. For the baselines,
we follow the parameter settings in the corresponding literature
and report the best results. All methods use the same random seed
to obtain stable results. All the experiments are implemented on
a computer with a 2.50GHz i7-11700 CPU and 64GB RAM, Matlab
R2021a.
5.2 Clustering Performance
Table 2 displays the clustering performance of all methods on nine
challenging datasets. The best and second best results in the table
are denoted by bold value and underline value. From Table 2, we
can conclude the following interesting observations.
1) From a global perspective, our TUMCR can achieve the best
or comparable clustering performance on all nine datasets com-
pared with the other nine comparison methods. Furthermore, the
improvement of clustering performance on some datasets is also
remarkable. For example, on the MSRCv1 dataset, TUMCR out-
performs the second-best CoMSC+UPMGC methods by 16.92%,
15.96%, and 15.79%, in terms of the three metrics respectively. For
the BDGP dataset, TUMCR gains improvements around 38.39%,
36.74%, and 31.15%, respectively. These results validate its cluster-
ing ability when processing the fully unaligned data. Meanwhile,
TUMCR achieves satisfactory clustering performance for variousdata domains (e.g., text data, object data, video data, etc.), which
further shows the clustering robustness under different scenarios.
2) Not surprisingly, the multi-view clustering baselines based on
the ideal dataset (i.e., AWP, GMC, EOMSC-CA, SMVSC, AWMVC)
obtain worse clustering results than the unaligned multi-view clus-
tering methods. This is because complementarity and consensus
among views are difficult to exploit in the fully unaligned case,
and the unaligned sample information may misrepresent these
multi-view clustering baselines, producing inferior results.
3) The clustering result obtained by TUMCR is much better than
that obtained by the unaligned multi-view clustering (i.e., T-UMC,
CoMSC+UPMGC, LSR+UPMGC, GMC+UPMGC), all of which are
proposed for fully unaligned problem. From the results, we can
infer that it is better to conduct the establishment of cross-view
correspondence on the multi-scale space than on a fixed space,
the multi-scale representations can provide a more complete pic-
ture of different views, which allows the model to better capture
complementary and consistent information among views.
Visualization: To demonstrate the advantages of TUMCR in
unaligned multi-view clustering more intuitively, we provide visual-
ization graphs on the BBCSport dataset, including the ideal original
data structure of the 2-th view, the unaligned data structure of the
2-th view, the data structure of aligned representation G2, and the
unified graphs learned by the TUMCR. As seen from Fig. 3, with the
data unalignment, the real data structure in the 2-th view is severely
damaged, which makes it difficult for traditional multi-view cluster-
ing methods to mine the consistency and complementarity among
views. Then, from Fig. 3.(c), it can be noticed that the important
diagonal block structure in the original data has been recovered,
which indicates that TUMCR can effectively recover the unaligned
 
1252KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
50 100 150 200 250 300 350 400 450 50050
100
150
200
250
300
350
400
450
500
0.10.20.30.40.50.60.70.8
(a) Ideal structure
50 100 150 200 250 300 350 400 450 50050
100
150
200
250
300
350
400
450
500
0.10.20.30.40.50.60.70.8 (b) Unaligned structure
50 100 150 200 250 300 350 400 450 50050
100
150
200
250
300
350
400
450
500
0.0050.010.0150.020.025 (c) Aligned representation
50 100 150 200 250 300 350 400 450 50050
100
150
200
250
300
350
400
450
500
0.0020.0040.0060.0080.010.0120.0140.0160.0180.020.022 (d) Unified graph
Figure 3: Visualization on 2-th view of BBCSport dataset. (a) The ideal original data structure. (b) The unaligned data structure.
(c) The data structure of aligned representation. (d) The unified graph of TUMCR.
structure. We present the clustering graphs learned by the TUMCR,
as can be seen from the Fig. 3.(d), it has a clearer diagonal block
structure as compared to previous structures, whereas a clearer
diagonal block structure implies stronger interclass variance and
intraclass compactness. These visualization results validate that our
approach is not only effective in recovering the unaligned structure
but also in mining the intrinsic similarity between multiple views.
5.3 Model Analysis
Scale Analysis: In our TUMCR, the number of the scale spaces
ùëü={1,2,3}, and the corresponding dimensions of the scale spaces
are{2ùëê,3ùëê,4ùëê}. In order to verify the effectiveness of different scales
on clustering performance, we show the clustering results on nine
datasets using different numbers of scales, as shown in Table 3.
we can see that TUMRC achieves the best clustering results when
ùëü=2,3in most cases, which means that the perception of the
full picture of multi-view data is favored by increasing the scale
space, providing a good understanding of the consensus and com-
plementary information among views. Note that for the 100leaves
dataset, the clustering performance deteriorates with increasing ùëü.
This phenomenon is probably caused by the fact that a single-scale
space characterizes all views well, and the multi-scale spaces may
introduce more noise to perturb the original structure.
Ablation Studies: To evaluate the effectiveness of the ETR
in TUMCR, we derive a variant, i.e.,TUMCR-TNN. TUMCR-TNN
replaces the Enhanced Tensor Rank (ETR) by tensor nuclear norm
(TNN), and the loss functions are as follows:
min
{Gùë£,Pùë£,Zùë£ùëû,Eùëû,Hùë£ùëû,Wùëû}‚à•G‚à•TNN‚àíùõºùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1ùú∑ùëûùëáùëü(Gùë£Wùëû(Pùë£Zùë£
ùëû)ùëá)
+ùõæùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Xùë£‚àíZùë£
ùëûHùë£
ùëû2
ùêπ+ùúÉùëö‚àëÔ∏Å
ùë£=1ùëü‚àëÔ∏Å
ùëû=1Zùëêùëúùëüùëí
ùëû‚àíPùë£Zùë£
ùëû2
ùêπ
ùë†.ùë°.‚àÄùë£,ùëû,Zùë£
ùëûùëáZùë£
ùëû=Iùëëùëû,Wùë£
ùëûWùë£
ùëûùëá=Iùëê,G=Œ¶(G1,¬∑¬∑¬∑,Gùëö).
Gùë£ùëáGùë£=Iùëê,Pùë£Pùë£ùëá=Iùëõ,ùëü‚àëÔ∏Å
ùëû=1ùú∑2
ùëû=1,ùú∑ùëû‚â•0
(30)
Fig. 5.(a) shows the ACC result of TUMCR and its variant on
nine datasets (i.e., Yale, MSRCv1, BBCSport, NGs, Caltech101-7,
100leaves, Caltech101-20, BDGP and CCV). We can observe thatTUMCR is superior to TUMCR-TNN on all datasets, which demon-
strates the effectiveness of ETR. With the benefit of ETR, we can
learn a representation tensor with a satisfactory clustering struc-
ture, which greatly facilitates subsequent clustering performance.
Parameters Analysis: Our TUMCR has four parameters ùõº,ùõæ,ùúÉ,
TUMCR TUMCR-TNN00.20.40.60.81Clustering Performanceyale
MSRCv1
WebKB
NGs
BBCSport
Caltech101-7
100leaves
Caltech101-20
BDGP
CCV
(a)
Figure 4: The comparison of TUMCR and TUMCR-TNN on
nine datasets.
andùõøthat need to be tuned. Here, we investigate the sensitiv-
ity of TUMCR on these parameters. We first define the search
range set{10‚àí3,¬∑¬∑¬∑,1}forùõºandùõø,{10‚àí3,¬∑¬∑¬∑,103}forùõæand
{10‚àí3,¬∑¬∑¬∑,101}forùúÉ. Then, we show the clustering performance
(ACC) of the Caltech101-7 dataset in Fig. 5. We can observe that ùõæ,
ùúÉandùõºcan reach a more stable performance than ùõø. Specifically,
our method is sensitive to ùõøbecauseùõøneeds to adjust the strength
of the penalty for singular values in the ETR, which is essential for
learning the low-rank structure of the tensor.
Convergence Analysis: The convergence of our TUMCR is
guaranteed by the Theorem 3. Fig. 6 shows the values of stop criteria
in each iteration on eight datasets, the stop criteria used here is
Match Error (ME): ME=‚à•J‚àíG‚à•‚àû. We can observe that the
values of ME rapidly tend to 0 within 20 steps and remain stable,
which indicates the excellent convergence property of our TUMCR.
 
1253Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
Table 3: Scale analysis on nine datasets.
ùëüYale MSRCv1 BBCSport NGs Caltech101-7 100leaves Caltech101-20 BDGP CCV
ACC(%)1 38.20 60.00 28.10 53.80 54.30 76.30 39.70 91.80 41.40
271.50 60.00 96.70 71.20 64.90 66.30 54.50 91.40 43.10
3 30.30 87.10 56.10 88.20 69.30 29.70 49.50 91.20 39.80
(a)ùõæ&ùõºvs ACC
 (b)ùõæ&ùõºvs ACC
 (c)ùõæ&ùõºvs ACC
10-310-210-110010-310-210-100.20.40.60.81Clustering Performance
ACC (d)ùõøvs ACC
Figure 5: The parameters analysis of our proposed TUMCR on the Caltech101-7 dataset.
0 5 10 15 20 25 30
Iterations00.511.522.5Stop CriteriaMatch Error
(a) Yale
0 5 10 15 20 25
Iterations00.10.20.30.40.50.60.70.80.91Stop CriteriaMatch Error (b) MSRCv1
0 5 10 15 20 25
Iterations00.20.40.60.811.21.4Stop CriteriaMatch Error (c) NGs
0 5 10 15 20 25 30
Iterations00.20.40.60.811.2Stop CriteriaMatch Error (d) BBCSport
0 5 10 15 20 25 30
Iterations00.511.522.533.544.55Stop CriteriaMatch Error
(e) 100leaves
0 5 10 15 20 25
Iterations00.511.522.5Stop CriteriaMatch Error (f) Caltech101-20
0 5 10 15 20 25
Iterations00.10.20.30.40.50.60.70.80.91Stop CriteriaMatch Error (g) BDGP
0 5 10 15 20 25 30
Iterations00.511.522.53Stop CriteriaMatch Error (h) CCV
Figure 6: Convergence Analysis: The stop criteria variation curves on eight datasets.
6 CONCLUSION
In this paper, we propose a novel unaligned multi-view clustering
method, termed tensorized unaligned multi-view clustering with
multi-scale representation learning (TUMCR). Distinct from exist-
ing unaligned multi-view clustering methods, TUMCR explores
the cross-view mapping matrices in multi-scale spaces, which are
constructed by the matrix factorization framework. The multi-scale
representations can characterize the full picture of multi-view data,
thus improving the accuracy of view alignment and clustering. To
fuse the information in different scales and views, TUMCR designs
a tensorial multi-scale fusion module, which employs an adaptive
weighting approach to integrate different scales and adopts thelow-rank tensor framework with Enhanced Tensor Rank to ex-
plore the high-order correlation among views. Furthermore, an
efficient optimization method is designed to address the objective
function. Extensive experiments are performed on nine challenging
datasets with different sizes. Compared with several state-of-the-art
methods, our proposed algorithm achieves superior performance
consistently.
ACKNOWLEDGMENTS
This work was supported by the National Natural Science Founda-
tion of China under Grant U2268203.
 
1254KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Jintian Ji, Songhe Feng, and Yidong Li
REFERENCES
[1]Michael W Berry, Murray Browne, Amy N Langville, V Paul Pauca, and Robert J
Plemmons. 2007. Algorithms and applications for approximate nonnegative
matrix factorization. Computational Statistics & Data Analysis 52, 1 (2007), 155‚Äì
173.
[2]Thomas Blumensath. 2014. Sparse matrix decompositions for clustering. In 2014
22nd European Signal Processing Conference (EUSIPCO). IEEE, 1163‚Äì1167.
[3]Guoqing Chao, Shiliang Sun, and Jinbo Bi. 2021. A survey on multiview clustering.
IEEE Transactions on Artificial Intelligence 2, 2 (2021), 146‚Äì168.
[4]Man-Sheng Chen, Chang-Dong Wang, Dong Huang, Jian-Huang Lai, and Philip S
Yu. 2022. Efficient orthogonal multi-view subspace clustering. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
127‚Äì135.
[5]Chris Ding, Xiaofeng He, and Horst D Simon. 2005. On the equivalence of
nonnegative matrix factorization and spectral clustering. In Proceedings of the
2005 SIAM International Conference on Data Mining. SIAM, 606‚Äì610.
[6]Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal nonnegative
matrix t-factorizations for clustering. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 126‚Äì135.
[7]Jicong Fan. 2021. Large-scale subspace clustering via k-factorization. In Proceed-
ings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining .
342‚Äì352.
[8]Shudong Huang, Ivor W Tsang, Zenglin Xu, and Jiancheng Lv. 2021. Measur-
ing diversity in graph learning: A unified framework for structured multi-view
clustering. IEEE Transactions on Knowledge and Data Engineering 34, 12 (2021),
5869‚Äì5883.
[9]Zhenyu Huang, Peng Hu, Joey Tianyi Zhou, Jiancheng Lv, and Xi Peng. 2020.
Partially view-aligned clustering. Advances in Neural Information Processing
Systems 33 (2020), 2892‚Äì2902.
[10] Jintian Ji and Songhe Feng. 2023. Anchor Structure Regularization Induced Multi-
view Subspace Clustering via Enhanced Tensor Rank Minimization. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 19343‚Äì19352.
[11] Zhao Kang, Wangtao Zhou, Zhitong Zhao, Junming Shao, Meng Han, and Zenglin
Xu. 2020. Large-scale Multi-view Subspace Clustering in Linear Time. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, Vol. 34. 4412‚Äì4419.
[12] Misha E Kilmer, Karen Braman, Ning Hao, and Randy C Hoover. 2013. Third-order
tensors as operators on matrices: A theoretical and computational framework
with applications in imaging. SIAM J. Matrix Anal. Appl. 34, 1 (2013), 148‚Äì172.
[13] Misha E Kilmer and Carla D Martin. 2011. Factorization strategies for third-order
tensors. Linear Algebra Appl. 435, 3 (2011), 641‚Äì658.
[14] Jia-Qi Lin, Man-Sheng Chen, Chang-Dong Wang, and Haizhang Zhang. 2024.
A tensor approach for uncoupled multiview clustering. IEEE Transactions on
Cybernetics 54, 2 (2024), 1236‚Äì1249.
[15] Yijie Lin, Yuanbiao Gou, Zitao Liu, Boyun Li, Jiancheng Lv, and Xi Peng. 2021.
Completer: Incomplete multi-view clustering via contrastive prediction. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
11174‚Äì11183.
[16] Yawen Ling, Jianpeng Chen, Yazhou Ren, Xiaorong Pu, Jie Xu, Xiaofeng Zhu,
and Lifang He. 2023. Dual label-guided graph refinement for multi-view graph
clustering. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37.
8791‚Äì8798.
[17] Jiyuan Liu, Xinwang Liu, Jian Xiong, Qing Liao, Sihang Zhou, Siwei Wang, and
Yuexiang Yang. 2020. Optimal neighborhood multiple kernel clustering with
adaptive local kernels. IEEE Transactions on Knowledge and Data Engineering 34,
6 (2020), 2872‚Äì2885.
[18] Jiyuan Liu, Xinwang Liu, Yuexiang Yang, Li Liu, Siqi Wang, Weixuan Liang, and
Jiangyong Shi. 2021. One-pass multi-view clustering for large-scale data. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 12344‚Äì
12353.
[19] Suyuan Liu, Siwei Wang, Pei Zhang, Kai Xu, Xinwang Liu, Changwang Zhang,
and Feng Gao. 2022. Efficient one-pass multi-view subspace clustering with
consensus anchors. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 36. 7576‚Äì7584.
[20] Xinwang Liu, Li Liu, Qing Liao, Siwei Wang, Yi Zhang, Wenxuan Tu, Chang
Tang, Jiyuan Liu, and En Zhu. 2021. One pass late fusion multi-view clustering.
InInternational Conference on Machine Learning. PMLR, 6850‚Äì6859.
[21] Xinwang Liu, Xinzhong Zhu, Miaomiao Li, Chang Tang, En Zhu, Jianping Yin,
and Wen Gao. 2019. Efficient and effective incomplete multi-view clustering. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 4392‚Äì4399.
[22] Xinwang Liu, Xinzhong Zhu, Miaomiao Li, Lei Wang, Chang Tang, Jianping
Yin, Dinggang Shen, Huaimin Wang, and Wen Gao. 2019. Late Fusion Incom-
plete Multi-View Clustering. IEEE Transactions on Pattern Analysis and Machine
Intelligence 41, 10 (2019), 2410‚Äì2423.
[23] Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, and Shuicheng
Yan. 2016. Tensor robust principal component analysis: exact recovery of cor-
rupted low-rank tensors via convex optimization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 5249‚Äì5257.[24] Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, and Shuicheng Yan.
2019. Tensor robust principal component analysis with a new tensor nuclear
norm. IEEE Transactions on Pattern Analysis and Machine Intelligence 42, 4 (2019),
925‚Äì938.
[25] J. Macqueen. 1967. Some methods for classification and analysis of multivariate
observations. Proc. Symp. Math. Statist. and Probability, 5th 1 (1967).
[26] Feiping Nie, Shenfei Pei, Rong Wang, and Xuelong Li. 2020. Fast clustering with
co-clustering via discrete non-negative matrix factorization for image identifica-
tion. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP). IEEE, 2073‚Äì2077.
[27] Feiping Nie, Lai Tian, and Xuelong Li. 2018. Multiview clustering via adap-
tively weighted procrustes. In Proceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining. 2022‚Äì2030.
[28] Mengjing Sun, Pei Zhang, Siwei Wang, Sihang Zhou, Wenxuan Tu, Xinwang Liu,
En Zhu, and Changjian Wang. 2021. Scalable multi-view subspace clustering
with unified anchors. In Proceedings of the 29th ACM International Conference on
Multimedia. 3528‚Äì3536.
[29] Pham Dinh Tao and LT Hoai An. 1997. Convex analysis approach to DC pro-
gramming: theory, algorithms and applications. Acta Mathematica Vietnamica
22, 1 (1997), 289‚Äì355.
[30] Shinji Umeyama. 1988. An eigendecomposition approach to weighted graph
matching problems. IEEE Transactions on Pattern Analysis and Machine Intelligence
10, 5 (1988), 695‚Äì703.
[31] Ulrike Von Luxburg. 2007. A tutorial on spectral clustering. Statistics and
Computing 17, 4 (2007), 395‚Äì416.
[32] Xinhang Wan, Xinwang Liu, Jiyuan Liu, Siwei Wang, Yi Wen, Weixuan Liang,
En Zhu, Zhe Liu, and Lu Zhou. 2023. Auto-weighted multi-view clustering for
large-scale data. arXiv preprint arXiv:2303.01983 (2023).
[33] Hao Wang, Yan Yang, and Bing Liu. 2019. GMC: Graph-based multi-view cluster-
ing.IEEE Transactions on Knowledge and Data Engineering 32, 6 (2019), 1116‚Äì1129.
[34] Jing Wang, Feng Tian, Hongchuan Yu, Chang Hong Liu, Kun Zhan, and Xiao
Wang. 2017. Diverse non-negative matrix factorization for multiview data repre-
sentation. IEEE Transactions on Cybernetics 48, 9 (2017), 2620‚Äì2632.
[35] Qiang Wang, Yong Dou, Xinwang Liu, Fei Xia, Qi Lv, and Ke Yang. 2018. Local
kernel alignment based multi-view clustering using extreme learning machine.
Neurocomputing 275 (2018), 1099‚Äì1111.
[36] Yi Wen, Siwei Wang, Qing Liao, Weixuan Liang, Ke Liang, Xinhang Wan, and
Xinwang Liu. 2023. Unpaired multi-view graph clustering with cross-view struc-
ture matching. IEEE Transactions on Neural Networks and Learning Systems (2023),
1‚Äì15.
[37] Like Xin, Wanqi Yang, Lei Wang, and Ming Yang. 2023. Selective Contrastive
Learning for Unpaired Multi-View Clustering. IEEE Transactions on Neural
Networks and Learning Systems (2023), 1‚Äì15.
[38] Haizhou Yang, Quanxue Gao, Wei Xia, Ming Yang, and Xinbo Gao. 2022. Mul-
tiview spectral clustering with bipartite graph. IEEE Transactions on Image
Processing 31 (2022), 3591‚Äì3605.
[39] Mouxing Yang, Yunfan Li, Zhenyu Huang, Zitao Liu, Peng Hu, and Xi Peng. 2021.
Partially view-aligned representation learning with noise-robust contrastive
loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 1134‚Äì1143.
[40] Hong Yu, Jia Tang, Guoyin Wang, and Xinbo Gao. 2021. A novel multi-view clus-
tering method for unknown mapping relationships between cross-view samples.
InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining. 2075‚Äì2083.
[41] Kun Zhan, Changqing Zhang, Junpeng Guan, and Junsheng Wang. 2017. Graph
learning for multiview clustering. IEEE Transactions on Cybernetics 48, 10 (2017),
2887‚Äì2895.
[42] Chao Zhang, Huaxiong Li, Wei Lv, Zizheng Huang, Yang Gao, and Chunlin
Chen. 2023. Enhanced tensor low-rank and sparse representation recovery
for incomplete multi-view clustering. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 37. 11174‚Äì11182.
[43] Chen Zhang, Siwei Wang, Jiyuan Liu, Sihang Zhou, Pei Zhang, Xinwang Liu,
En Zhu, and Changwang Zhang. 2021. Multi-view clustering via deep matrix
factorization and partition alignment. In Proceedings of the 29th ACM International
Conference on Multimedia. 4156‚Äì4164.
[44] Pei Zhang, Siwei Wang, Liang Li, Changwang Zhang, Xinwang Liu, En Zhu,
Zhe Liu, Lu Zhou, and Lei Luo. 2023. Let the data choose: flexible and diverse
anchor graph fusion for scalable multi-view clustering. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 11262‚Äì11269.
[45] Xianchao Zhang, Linlin Zong, Xinyue Liu, and Hong Yu. 2015. Constrained
NMF-based multi-view clustering on unmapped data. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 29. 3174‚Äî-3180.
[46] Handong Zhao, Zhengming Ding, and Yun Fu. 2017. Multi-view clustering via
deep matrix factorization. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 31. 2921‚Äì2927.
[47] Shuping Zhao, Jie Wen, Lunke Fei, and Bob Zhang. 2023. Tensorized incomplete
multi-view clustering with intrinsic graph completion. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 11327‚Äì11335.
 
1255Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
[48] Peng Zhou, Liang Du, Xinwang Liu, Zhaolong Ling, Xia Ji, Xuejun Li, and Yi-
Dong Shen. 2023. Partial Clustering Ensemble. IEEE Transactions on Knowledge
and Data Engineering (2023), 1‚Äì14.[49] Pan Zhou, Canyi Lu, Jiashi Feng, Zhouchen Lin, and Shuicheng Yan. 2019. Tensor
low-rank representation for data recovery and clustering. IEEE Transactions on
Pattern Analysis and Machine Intelligence 43, 5 (2019), 1718‚Äì1732.
 
1256