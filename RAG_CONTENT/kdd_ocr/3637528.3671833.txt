Budgeted Multi-Armed Bandits with
Asymmetric Confidence Intervals
Marco Heyden
marco.heyden@kit.edu
Karlsruhe Institute of Technology
Karlsruhe, GermanyVadim Arzamasov
vadim.arzamasov@kit.edu
Karlsruhe Institute of Technology
Karlsruhe, Germany
Edouard Fouché
edouard.fouche@kit.edu
Karlsruhe Institute of Technology
Karlsruhe, GermanyKlemens Böhm
klemens.boehm@kit.edu
Karlsruhe Institute of Technology
Karlsruhe, Germany
ABSTRACT
We study the stochastic Budgeted Multi-Armed Bandit (MAB) prob-
lem, where a player chooses from 𝐾arms with unknown expected
rewards and costs. The goal is to maximize the total reward un-
der a budget constraint. A player thus seeks to choose the arm
with the highest reward-cost ratio as often as possible. Current ap-
proaches for this problem have several issues, which we illustrate.
To overcome them, we propose a new upper confidence bound
(UCB) sampling policy, 𝜔-UCB, that uses asymmetric confidence
intervals. These intervals scale with the distance between the sam-
ple mean and the bounds of a random variable, yielding a more
accurate and tight estimation of the reward-cost ratio compared to
our competitors. We show that our approach has sublinear instance-
dependent regret in general and logarithmic regret for parameter
𝜌≥1, and that it outperforms existing policies consistently in
synthetic and real settings.
CCS CONCEPTS
•Computing methodologies →Machine learning algorithms ;
Online learning settings ;•Information systems →Decision sup-
port systems.
KEYWORDS
Budgeted multi-armed bandits, Multi-armed bandits, Decision mak-
ing under uncertainty, Online decision making
ACM Reference Format:
Marco Heyden, Vadim Arzamasov, Edouard Fouché, and Klemens Böhm.
2024. Budgeted Multi-Armed Bandits with Asymmetric Confidence In-
tervals. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671833
1 INTRODUCTION
In the stochastic Multi-Armed Bandit (MAB) problem, a player re-
peatedly plays one of 𝐾arms and receives a corresponding random
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671833reward. The goal is to maximize the cumulative reward by play-
ing the arm with the highest expected reward as often as possible.
The expected rewards are initially unknown, so the player must
balance trying arms to learn their expected rewards (exploration)
versus using the current information to play arms with known high
expected rewards (exploitation).
In the stochastic Budgeted MAB problem [ 16], a player must
consider not only the potential rewards but also the associated ran-
dom costs for each arm. The player chooses arms until the available
budget is exhausted. Budgeted MABs model real-world situations
such as the selection of a cloud service provider [ 1], energy-efficient
task selection for battery-powered embedded devices [ 17], bid opti-
mization [6], or optimizing advertising on social media.
Example 1 (Social media advertising). Consider a retail company
that wants to advertise products on a social network platform.
The retail company provides to the platform an advertisement
campaign consisting of multiple ads, as well as an advertisement
budget. Each time a user interacts with an ad (an arm), the platform
charges the retail company (a cost). Within the given budget, the
retailer wants to find the ads which maximize the likelihood of a
subsequent purchase (a reward). Both the reward and the cost are
random variables since they depend on the actions of users and the
competition from other advertisers. A Budgeted MAB algorithm
can help find the most promising ads in real time.
A variety of policies has been proposed to address the Budgeted
MAB problem. Section 3 provides a summary. Some policies model
the problem as Bandits with Knapsacks, which are able to take the
budget limit into account [ 4,17]. Others extend ideas from tradi-
tional multi-armed bandit algorithms (in which the costs of arms
are assumed to be constant), including Thompson Sampling [ 23]
and Upper Confidence Bound (UCB) sampling [ 24]. Several studies
indicate that the latter – and in particular UCB-sampling – perform
well in practice [18, 19, 22, 24, 25].
UCB-sampling policies continuously update an upper bound of
the ratio of the expected rewards and costs of each arm, and play
the arm with the highest upper bound. We distinguish between
three types: Some policies [ 8,18,19,24] compute the bound from
the ratio of the sample average reward and average cost plus some
uncertainty-related term (cf. Eq. (1), left). We call this type “united”
(u). Other policies [ 4,24,25] divide the reward’s upper confidence
bound by the cost’s lower confidence bound (LCB) (cf. Eq. (1), right).
 
1073
KDD ’24, August 25–29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard Fouché, and Klemens Böhm
51015% UCB viol.
ω-UCB (c, ours)m-UCB (c)Budget-UCB (h)c-UCB (h) i-UCB (u) UCB-SC (u) UCB-B2 (u)100101102UCB/expect.Samples
100 1000 10000 100000
Figure 1: Issues of existing work
We refer to this type as “composite” (c). There also are “hybrid” (h)
policies [22, 24] that combine the united and composite types.
UCB𝑢=average reward
average cost+uncertainty
UCB𝑐=average reward+uncertainty
average cost−uncertainty(1)
However, most of the current policies have at least one of the
following issues: (i1) Over-optimism: The policy often computes
UCB that are too tight. (i2) Over-pessimism: The policy often
computes UCB that are too loose. (i3) Invalid values: Negative or
undefined UCB occur if the cost’s lower confidence bound in Eq. (1)
becomes negative or zero. The latter can happen, for instance, when
computing the lower confidence bound of an arm’s expected cost
with Hoeffding’s inequality [22, 24].
To illustrate (i1) and (i2), we randomly parameterized 10 000
Bernoulli reward and cost distributions and sampled from them.
We used these samples to compute 99% confidence intervals of
the reward-cost ratio using several state-of-the-art UCB-sampling
policies. The upper plot of Figure 1 shows the share of cases when
the expected reward-cost ratio exceeds (i.e., violates) its UCB. Values
above 1% indicate overly tight bounds. The lower plot shows the
UCB of the reward-cost ratio divided by its expectation, with higher
values indicating looser bounds. Existing united policies (u) tend
to suffer from issue (i1), hybrid approaches suffer from either of
both issues, and issue (i2) mainly affects composite approaches.
An exception is UCB-B2 [ 8], who’s UCB is both tight and reliable
(although not as tight as that of 𝜔-UCB).
To address (i1) and (i2), some approaches provide a hyperparam-
eter that allows to adjust the confidence interval manually [ 24].
However, setting such a hyperparameter is difficult since it depends
on the unknown mean and variance of the reward and cost distribu-
tions. To address (i3), current approaches set the UCB of the ratio
to infinity [ 24] or the cost LCB to a small positive value [ 22]. Theseheuristic solutions largely ignore the information already acquired
about the cost distribution and tend to cause either (i1) or (i2).
Contributions. (1) We derive asymmetric confidence intervals for
bounded random variables. These intervals have the same range
as the random variable and scale with the distance between the
sample mean and the boundaries. Our formula generalizes Wilson’s
score interval for binomial proportions [ 21] to arbitrary bounded
random variables. (2) We introduce a policy called 𝜔-UCB, which
leverages these confidence intervals to address issues (i1)–(i3). We
also propose an extension of 𝜔-UCB, called 𝜔∗-UCB, that uses
the observed variances of the arms’ rewards and cost to further
tighten the UCB. (3) We prove that 𝜔-UCB has sublinear regret in
general and logarithmic regret for parameter 𝜌≥1. (4) We conduct
experiments on typical settings found in the literature and real-
world social network advertising data to compare the performance
of𝜔-UCB and𝜔∗-UCB against state-of-the-art policies. Our results
demonstrate that both policies have substantially lower regret than
the competitors for both small and large budgets. (5) We share the
code of our experiments.1
2 PROBLEM DEFINITION
We focus on a stochastic setting with 𝐾arms. Each arm 𝑘has
continuous or discrete reward and cost distributions with unknown
expected values 𝜇𝑟
𝑘∈[0,1)and𝜇𝑐
𝑘∈(0,1], respectively. Assume
without loss of generality that arm 𝑘=1has the highest ratio 𝜇𝑟
𝑘/𝜇𝑐
𝑘
among all arms. At time 𝑡a player chooses an arm 𝑘𝑡∈{1,...,𝐾}
and observes the reward 𝑟𝑡∈[0,1]and the cost 𝑐𝑡∈[0,1]. We
assume that the arms are independent and that rewards and costs
observed at different time steps are independent and identically
distributed (iid). This is consistent with previous work [ 18,19,22–
24]. We do not make any assumptions about the correlation between
rewards and costs of the same arm. The game ends after 𝑇𝐵plays
that exhaust the available budget 𝐵.
Let1𝑘(𝑘𝑡)be the indicator function: 1𝑘(𝑘𝑡)=1iff𝑘𝑡=𝑘,
else1𝑘(𝑘𝑡)=0. The number of plays, and the sample average of
rewards and costs of arm 𝑘at time𝑇are:
𝑛𝑘(𝑇)=𝑇∑︁
𝑡=11𝑘(𝑘𝑡)
ˆ𝜇𝑟
𝑘(𝑇)=1
𝑛𝑘(𝑇)𝑇∑︁
𝑡=11𝑘(𝑘𝑡)𝑟𝑡
ˆ𝜇𝑐
𝑘(𝑇)=1
𝑛𝑘(𝑇)𝑇∑︁
𝑡=11𝑘(𝑘𝑡)𝑐𝑡
The goal of the player is to minimize the pseudo-regret compared
to the cumulative reward 𝑅∗of an optimal policy, given by 𝑅∗−
EÍ𝑇𝐵
𝑡=1𝑟𝑡. Finding the optimal policy in Budgeted MABs is known
to be np-hard, due to the “knapsack problem” [ 16]. However, always
choosing arm 1 leads to a suboptimality of at most 2𝜇𝑟
1/𝜇𝑐
1, negligible
for not too small budgets [ 23]. Thus, previous work [ 18,19,22–24],
as well as our own approach, aim to minimize regret relative to a
1https://github.com/heymarco/OmegaUCB
 
1074Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD ’24, August 25–29, 2024, Barcelona, Spain
policy that always selects arm 1:
Regret =𝐾∑︁
𝑖=1𝜇𝑐
𝑘Δ𝑘E[𝑛𝑘(𝑇𝐵)],where Δ𝑘=𝜇𝑟
1
𝜇𝑐
1−𝜇𝑟
𝑘
𝜇𝑐
𝑘
3 RELATED WORK
There exists many different MAB-related settings and policies. We
refer to [ 11] for an overview and focus on algorithms developed
for the Budgeted MAB setting in this section.
Tran-Thanh et al. [ 16] introduced the Budgeted MAB problem
and proposed an 𝜀-first policy. Subsequent policies KUBE [ 17] and
PD-BwK [ 4] propose to formulate the problem as Bandits with
Knapsacks, where the size of the knapsack represents the available
budget. Both policies require knowledge of 𝐵. This restricts their ap-
plicability when 𝐵is an unknown quantity. However, we argue that
such a restriction is unnecessary, since the advantage of exploiting
𝐵becomes negligible for sufficiently large budgets (cf. Section 2).
Another approach, UCB-BV1 [ 10], addresses the special case of dis-
crete random costs. Later solutions [ 18,19,22–24] adapted concepts
from traditional MABs, such as Upper Confidence Bound (UCB) [ 2]
or Thompson sampling [ 14,15] and can deal with continuous ran-
dom costs and unknown budget. However, the one policy based on
Thompson sampling, BTS [ 23], requires transforming continuous
rewards and costs into Bernoulli-samples. As a result, the policy
disregards information about the variance of rewards and costs,
causing over-pessimism (i2) when the variance of the reward or
cost distribution is small. MRCB [ 22] deals with the challenge of
playing multiple arms in each time step; when playing only one
arm at a time, the policy becomes m-UCB [ 24] that is similar to our
policy. However, m-UCB relies on Hoeffding’s inequality, which
does not take the distance between a random variable’s sample
mean and boundaries into account. To see why this is problematic,
consider the following example2:
Example 2 (m-UCB). Assume two arms with 𝜇𝑟
1=0.8,𝜇𝑐
1=0.2,
𝜇𝑟
2=0.1,𝜇𝑐
2=0.1. Clearly, arm 1 should be preferred for a reason-
ably large budget. However, m-UCB is biased towards pulling arm 2.
For instance, if 𝑡=10000 and𝑛1=𝑛2=1000, using m-UCB with
𝛼=1would yield reward-cost UCB values of ≈2.95for arm 1 and
≈48.6for arm 2 due to the high influence of the denominator in
Eq.(1)(rhs). m-UCB would hence pull arm 2. In comparison, 𝜔-UCB
would compute values of 5.5and2.1, and pull arm 1.
More recent algorithms are adaptations of exiting ones to spe-
cific scenarios. For example, Avadhanula et al. [ 3] develop a multi-
platform algorithm for Bandits with Knapsacks, and Das et al. [ 9]
extend BwK to the combinatorial setting in which the algorithm
can pull one or more arms in each round. However, the authors
assume that an arm’s cost is a known, fixed quantity, while we
address the challenge of dealing with cost distributions. [ 7] pro-
poses a novel ‘bandits with interruptions’ framework in which a
player can interrupt a taken action to limit the cost. [ 8] extends
Budgeted MABs to handle unbounded cost and reward distributions
and presents policies for various settings. Policy UCB-B2 is tailored
for our setting of uncorrelated, bounded rewards and costs, and
2One can construct analogous examples for other symmetric bounds, such as Bern-
stein’s inequality.relies on an empirical version of Bernstein’s inequality. However,
using this bound does not resolve the bias illustrated in Example 2.
The above policies either have issues (i1)–(i3) [ 8,18,19,22–24],
are not designed for continuous random costs [10, 17, 23], require
knowledge of 𝐵[4,17], and/or have been shown to perform in-
ferior to others [ 10,16,17]. Figure 2 provides a compilation of
existing head-to-head empirical comparisons between various poli-
cies. Upwards pointing triangles indicate that the policy in the
corresponding row outperformed the policy in the corresponding
column in the respective paper, while downward pointing triangles
indicate the opposite. Circles represent cases where both policies
performed similarly, while horizontal lines indicate that the policies
have not been compared. One sees that KUBE [ 17] outperforms
𝜀-first [ 16], while UCB-BV1 [ 10] and BTS[ 22] outperform KUBE.
UCB-BV1 is inferior to more recent policies [ 18,19,22,23]. BTS [ 23],
b-greedy [24], and {i, c, m}-UCB [24] outperform PD-BwK [4]. We
will compare our policy to the best performing existing policies
(BTS, Budget-UCB, {i, c, m}-UCB, b-greedy, and UCB-SC+) and to
the most recent UCB-B2.
4 OUR POLICY
We now detail our policy 𝜔-UCB and analyze it theoretically.
4.1𝜔-UCB
Our policy𝜔-UCB, summarized in Alg. 1, starts by playing each
arm once. At each subsequent time step 𝑡, the policy chooses the
arm𝑘𝑡with the highest upper confidence bound of the ratio of the
expected reward 𝜇𝑟
𝑘to the expected cost 𝜇𝑐
𝑘. Let𝜔𝑟
𝑘+(𝛼,𝑡)denote the
upper confidence bound of 𝜇𝑟
𝑘for a confidence level 1−𝛼. Similarly,
𝜔𝑐
𝑘−(𝛼,𝑡)is the lower confidence bound of 𝜇𝑐
𝑘.𝜔-UCB chooses 𝑘𝑡
according to:
𝑘𝑡=arg max
𝑘∈[𝐾]Ω𝑘(𝛼,𝑡),where Ω𝑘(𝛼,𝑡)=𝜔𝑟
𝑘+(𝛼,𝑡)
𝜔𝑐
𝑘−(𝛼,𝑡)(2)
Unlike other policies that rely on the same principle [ 4,8,22,
24,25],𝜔-UCB calculates asymmetric confidence bounds that are
shifted towards the center of the range of the random variable. This
leads to tighter UCB for the reward-cost ratio especially when an
arm’s expected cost or the number of plays is low.
Theorem 1 (Asymmetric confidence interval for bounded random
variables). Let𝑋be a random variable bounded in the interval
[𝑚,𝑀], with unknown expected value 𝜇∈[𝑚,𝑀]and variance 𝜎2.
Let𝑧denote the number of standard deviations required to achieve
1−𝛼confidence in coverage of the standard normal distribution.
Letˆ𝜇be the sample mean of 𝑛iid samples of 𝑋. Then,
Pr[𝜇∉[𝜔−(𝛼),𝜔+(𝛼)]]≤𝛼,
with
𝜔±(𝛼)=𝐵
2𝐴±√︂
𝐵2
4𝐴2−𝐶
𝐴, (3)
where
𝐴=𝑛+𝑧2𝜂, 𝐵=2𝑛ˆ𝜇+𝑧2𝜂(𝑀+𝑚), 𝐶=𝑛ˆ𝜇2+𝑧2𝜂𝑀𝑚
and
𝜂=(𝜎2
(𝑀−𝜇)(𝜇−𝑚)if𝜇∈(𝑚,𝑀)
1 if𝜇∈{𝑚,𝑀}.
 
1075KDD ’24, August 25–29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard Fouché, and Klemens Böhm
Policy
𝜀-first
KUBE
UCB-BV1
PD-BwK
Budget-UCB
BTS
MRCB
m-UCB
b-greedy
c-UCB
i-UCB
KL-UCB-SC+
UCB-SC+
UCB-B2
ε-/f_irst
KUBE
UCB-BV1
PD-BwK
Budget-UCB
BTS
MRCB
m-UCB
b-greedy
c-UCB
i-UCB
KL-UCB-SC+
UCB-SC+
UCB-B2Year Type Compared
2010 – ×
2012 – ×
2013 h ×
2013 c ×
2015 h✓
2015 –✓
2016 c –
2017 c✓
2017 –✓
2017 h✓
2017 u✓
2017 u –
2018 u✓
2020 u✓
Figure 2: Empirical performance of different Budgeted MAB policies according to related work
Proof. Using the central limit theorem and Bhatia-Davis in-
equality, we follow similar steps as [ 21]. We first handle the case
where𝜇∈(𝑚,𝑀). We then address the cases 𝜇=𝑚and𝜇=𝑀.
Case𝜇∈(𝑚,𝑀).The central limit theorem states that for a large
enough sample size, ˆ𝜇approximately follows a normal distribution
with mean𝜇and variance 𝜎2/𝑛. I.e.,
ˆ𝜇∼N 
𝜇,√︂
𝜎2
𝑛!
⇐⇒ˆ𝜇−𝜇√︃
𝜎2
𝑛∼N( 0,1).
Therefore, ˆ𝜇likely falls into an interval that is centered around
𝜇and scaled by 𝜎. The value𝑧is the number of standard deviations
such that ˆ𝜇falls out of the corresponding confidence interval with
a probability of 𝛼.
Pr
ˆ𝜇∉
𝜇−𝜎√𝑛𝑧,𝜇+𝜎√𝑛𝑧
=𝛼 (4)
Next, we apply the Bhatia-Davis inequality [ 5] to express 𝜎as
a function of 𝜇. It states that 𝜎2≤(𝑀−𝜇)(𝜇−𝑚). Hence, there
exists a factor 𝜂∈[0,1]such that
𝜎2=𝜂(𝑀−𝜇)(𝜇−𝑚). (5)
This gives us an expression for the interval bounds in Eq. (4)
that is quadratic w.r.t. 𝜇:
(ˆ𝜇−𝜇)2=𝜎2
𝑛𝑧2=𝜂(𝑀−𝜇)(𝜇−𝑚)
𝑛𝑧2(6)
Solving Eq. (6)for𝜇yields the endpoints 𝜔−(𝛼)and𝜔+(𝛼)of
our confidence interval:
Pr[𝜇∉[𝜔−(𝛼),𝜔+(𝛼)]]=𝛼
with
𝜔−(𝛼),𝜔+(𝛼)=𝐵
2𝐴±√︂
𝐵2
4𝐴2−𝐶
𝐴Algorithm 1 𝜔-UCB
Require:𝐾 ⊲Number of arms
Require:𝐵 ⊲Available budget
Require:𝜌 ⊲CI scaling parameter, defaults to 1/4
Require:®𝜂𝑟⊲Reward variance parameters, defaults to [1]𝐾
Require:®𝜂𝑐⊲Cost variance parameters, defaults to [1]𝐾
𝑡←0
®𝑛←[0]𝐾
while𝐵>0do
if𝑡<𝑘then ⊲Play each arm once
Play arm𝑡
Observe𝑟𝑡,𝑐𝑡and update ˆ𝜇𝑟
𝑘(𝑡),ˆ𝜇𝑐
𝑘(𝑡)
®𝑛[𝑡]=1
else
𝑧𝜌(𝑡)=√︁
2𝜌log𝑡⊲Corresponds to 𝛼(𝑡)<1−√
1−𝑡−𝜌
for all arms𝑘∈1...𝐾 do
Compute Ω𝑘(𝛼,𝑡)from Eq. (2)and Eq. (3)with𝑧𝜌(𝑡),
®𝜂𝑟[𝑘],®𝜂𝑐[𝑘],®𝑛[𝑘],ˆ𝜇𝑟
𝑘(𝑡), and ˆ𝜇𝑐
𝑘(𝑡)
Find𝑘𝑡=arg max𝑘Ω𝑘(𝛼,𝑡)
Play arm𝑘𝑡
Observe𝑟𝑡,𝑐𝑡and update ˆ𝜇𝑟
𝑘(𝑡),ˆ𝜇𝑐
𝑘(𝑡)
®𝑛[𝑘𝑡]=®𝑛[𝑘𝑡]+1
𝐵=𝐵−𝑐𝑡
𝑡=𝑡+1
and
𝐴=𝑛+𝑧2𝜂, 𝐵 =2𝑛ˆ𝜇+𝑧2𝜂(𝑀+𝑚), 𝐶=𝑛ˆ𝜇2+𝑧2𝜂𝑀𝑚.
Cases𝜇=𝑚and𝜇=𝑀.For𝜇=𝑚(the case for 𝜇=𝑀is
analogous), the probability that 𝜇is not in the confidence interval
[𝜔−(𝛼),𝜔+(𝛼)]is zero, which is less than 𝛼. Additionally, we have
 
1076Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD ’24, August 25–29, 2024, Barcelona, Spain
𝜎2=(𝑀−𝜇)(𝜇−𝑚)=0, which implies that Eq. (5)holds for
any choice of 𝜂. However, since 𝜇is an unknown quantity, we can
never be certain that 𝜇=𝑚based on some sample from 𝑋. In the
worst-case scenario, 𝑋is a random variable that takes only extreme
values, i.e., 𝑋∈{𝑚,𝑀}, with𝜇greater than and approximately
equal to𝑚. In this case, 𝜂=1by definition. Hence, we define 𝜂=1
for𝜇∈𝑚,𝑀 . Combining the special case that 𝜇∈{𝑚,𝑀}with the
result from the previous paragraph gives Theorem 1. □
Illustration of asymmetry. The center of the confidence interval
is a weighted average of the sample mean and the center of the
range of the random variable. This leads to confidence intervals that
are shifted towards the center of the range of the random variable.
To see this, we can compare the distance between ˆ𝜇and the interval
center𝐵/2𝐴to half the width of the confidence interval:
Asymmetry =ˆ𝜇−𝐵
2𝐴√︃
𝐵2
4𝐴2−𝐶
𝐴∈[0,1]
For Bernoulli random variables, after some derivations and inserting
the definitions of 𝐴,𝐵,𝐶 as specified in Theorem 1, we obtain
Asymmetry =(2ˆ𝜇−1)2𝑧2
4𝑛ˆ𝜇(1−ˆ𝜇)+𝑧2(7)
Figure 3 plots the asymmetry measure for different values of 𝑛
over ˆ𝜇and𝑧=3. (1) For ˆ𝜇=1and ˆ𝜇=0, the asymmetry takes on
a maximum value of 1, while for ˆ𝜇=0.5, asymmetry is 0. (2) For
a given value of ˆ𝜇∈(0,1), asymmetry decreases with increasing
sample size. (3) Related to this, we see that asymmetry is maximal
for a given ˆ𝜇for𝑛=1.
0.00 0.25 0.50 0.75 1.00
µ01Asymmetryn
1
10
100
1000
10000
Figure 3: Asymmetry measure from Eq. (7)for Bernoulli re-
wards and costs for different 𝑛and𝜇for𝑧=3.
Discussion. According to the Bhatia-Davis inequality [ 5],0≤
𝜎2≤(𝑀−𝜇)(𝜇−𝑚), and hence 𝜂∈[0,1]. For the special case
of Bernoulli random variables, 𝜂=1,𝑚=0,𝑀=1, and Theo-
rem 1 recovers Wilson’s original confidence interval for Binomial
proportions [ 21]. However, this theorem is more flexible than Wil-
son’s original method. It enables tighter confidence intervals for
non-Bernoulli costs or rewards, by setting 𝜂<1when an estimate
or an upper bound of the variance is available. Our experiments
demonstrate that this flexibility leads to a significant performance
improvement.
The following theorem defines an upper confidence bound Ω(𝛼)
for the ratio of the expected values of two random variables. Com-
bined with Theorem 1, it facilitates the computation of an arm’s
index according to Eq. (2).Theorem 2 (UCB for ratio of expected values of two random vari-
ables). Let𝑅and𝐶be two bounded random variables with expected
values𝜇𝑟≥0and𝜇𝑐>0. Let𝜔𝑟+(𝛼)≥0denote the upper confi-
dence bound of 𝑅and𝜔𝑐−(𝛼)>0the lower confidence bound of 𝐶
as given in Theorem 1. Let Ω(𝛼)=𝜔𝑟+(𝛼)/𝜔𝑐−(𝛼). Then,
Pr𝜇𝑟
𝜇𝑐>Ω(𝛼)
≤𝛼
Proof. Define events 𝐸1=𝜇𝑟>𝜔𝑟+(𝛼)and𝐸2=𝜇𝑐<𝜔𝑐−(𝛼).
A violation of the UCB of the reward-cost ratio requires that ei-
ther𝐸1or𝐸2occurs, or that both events happen simultaneously.
Therefore, by the union bound we have that Pr[𝜇𝑟/𝜇𝑐>Ω𝑘]=
Pr
𝜇𝑟/𝜇𝑐>𝜔𝑟+(𝛼)/𝜔𝑐−(𝛼)
≤Pr[𝐸1]+Pr[𝐸2].𝐸1and𝐸2both oc-
cur with probability ≤𝛼/2, hence Pr[𝜇𝑟/𝜇𝑐>Ω𝑘]≤𝛼.□
A UCB-sampling policy that keeps parameter 𝛼constant leads
to linear regret in the worst case. This is because such a policy
will eventually stop exploring arms that may have high costs and
low rewards in the beginning. To avoid this problem, 𝜔-UCB de-
creases the value of 𝛼as the time 𝑡progresses, similarly to the
UCB1-policy [ 2]. This adaptive approach helps to ensure continued
exploration of arms and guarantees sub-linear regret. Theorem 3
introduces the scaling law and relates it to the confidence level.
Theorem 3 (Time-adaptive confidence interval). For an arm𝑘, let
𝜇𝑟
𝑘be its expected reward, 𝜇𝑐
𝑘its expected cost, and Ω𝑘(𝛼,𝑡)the
upper confidence bound for 𝜇𝑟
𝑘/𝜇𝑐
𝑘, as in Eq. (2). For𝜌,𝑡>0, and
𝛼(𝑡)<1−√
1−𝑡−𝜌it holds that
Pr"
Ω𝑘(𝛼,𝑡)≥𝜇𝑟
𝑘
𝜇𝑐
𝑘#
≥1−𝛼(𝑡),
that is, the upper confidence bound holds asymptotically almost
surely.
Proof. We start from Theorem 1 and equate the confidence level
1−𝛼of the individual reward and cost distributions to the number of
standard deviations 𝑧. This involves the cumulative density function
(cdf) of the standard normal distribution. We then replace the cdf
with an approximation that has a closed form solution for 𝑧. Our
choice of𝑧cancels out the exponential term in this approximation,
similar to the UCB1-policy [ 2]. Last, we apply Theorem 2 to obtain
the final result.
Step 1. We relate the confidence level 1−𝛼(𝑡)at time𝑡to the
cumulative density function of the standard normal distribution
(erf abbreviates the error function).
1−𝛼(𝑡)
2=1
2
1+erf𝑧√
2
Solving for𝛼(𝑡)yields:
𝛼(𝑡)=1−erf𝑧√
2
Step 2. We now replace the error function erf
𝑧√
2
in the equa-
tion above with a series expansion based on Bürmann’s theorem [ 13,
20]; we summarize all but the first addend in a remainder term
 
1077KDD ’24, August 25–29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard Fouché, and Klemens Böhm
𝛾
𝑧√
2
>0. This term has a maximum of 𝛾(0.71)≈0.0554 and
approaches 0 for larger 𝑧:
𝛼(𝑡)=1−©­
«√︄
1−exp
−𝑧2
2
+𝛾𝑧√
2
ª®
¬
Omitting the 𝛾-term gives an upper bound for 𝛼(𝑡):
𝛼(𝑡)<1−√︄
1−exp
−𝑧2
2
(8)
Step 3. Next, we choose 𝑧as a function of log𝑡and𝜌>0,𝑧𝜌(𝑡)=√︁
2𝜌log𝑡. This results in a time-increasing confidence level 1−𝛼(𝑡):
Pr[𝜇∉[𝜔−(𝛼(𝑡)),𝜔+(𝛼(𝑡))]]≤𝛼(𝑡)with𝛼(𝑡)<1−√
1−𝑡−𝜌
Step 4. Applying Theorem 2 gives
Pr"
𝜇𝑟
𝑘
𝜇𝑐
𝑘>Ω𝑘(𝛼,𝑡)#
≤𝛼(𝑡)with𝛼(𝑡)<1−√
1−𝑡−𝜌.
The complementary event, Ω𝑘(𝛼,𝑡)≥𝜇𝑟
𝑘/𝜇𝑐
𝑘, holds with a prob-
ability of at least 1−𝛼(𝑡),
Pr"
Ω𝑘(𝛼,𝑡)≥𝜇𝑟
𝑘
𝜇𝑐
𝑘#
≥1−𝛼(𝑡)with𝛼(𝑡)<1−√
1−𝑡−𝜌,
which is the result given in Theorem 3.
□
With𝛼(𝑡)<1−√
1−𝑡−𝜌, the confidence level 1−𝛼approaches
1as time𝑡goes to infinity. This encourages exploration of arms that
are played less frequently. Moreover, it establishes a logarithmic
dependence between 𝑧in Theorem 1 and 𝑡, i.e.,𝑧𝜌(𝑡)=√︁
2𝜌log𝑡,
which will be useful in our regret analysis. The next section analyzes
the worst-case regret of 𝜔-UCB. To simplify notation, we abbreviate
Ω(𝛼,𝑡)asΩ(𝑡),𝜔𝑐
𝑘−(𝛼,𝑡)as𝜔𝑐
𝑘−(𝑡), and𝜔𝑟
𝑘+(𝛼,𝑡)as𝜔𝑟
𝑘+(𝑡)in our
regret analysis.
4.2 Regret Analysis
We first bound the expected number of suboptimal plays E[𝑛𝑘(𝜏)]
before some time step 𝜏. We use the result to derive the regret
bound of𝜔-UCB (cf. Theorem 5). Due to space restrictions, we omit
longer proofs here but provide them in Appendix B.
Theorem 4 (Number of suboptimal plays). For𝜔-UCB, the ex-
pected number of plays of a suboptimal arm 𝑘>1before time step
𝜏,E[𝑛𝑘(𝜏)], is upper-bounded by
E[𝑛𝑘(𝜏)]≤ 1+𝑛∗
𝑘(𝜏)+𝜉(𝜏,𝜌), (9)
where
𝜉(𝜏,𝜌)=(𝜏−𝐾)
2−√
1−𝜏−𝜌
−𝜏∑︁
t=K+1√
1−𝑡−𝜌,
𝑛∗
𝑘(𝜏)=8𝜌log𝜏
𝛿2
𝑘max(
𝜂𝑟
𝑘𝜇𝑟
𝑘
1−𝜇𝑟
𝑘,𝜂𝑐
𝑘(1−𝜇𝑐
𝑘)
𝜇𝑐
𝑘)
,
𝛿𝑘=Δ𝑘/(Δ𝑘+1
𝜇𝑐
𝑘), and𝐾andΔ𝑘are defined as before, cf. Section 2.The1in Eq. (9)represents the very first pull of arm 𝑘. We inter-
pret the term 𝑛∗
𝑘(𝜏)as a minimum number of plays that leads to a
“sufficiently small” deviation between 𝜇𝑟
𝑘/𝜇𝑐
𝑘andˆ𝜇𝑟
𝑘(𝑡)/ˆ𝜇𝑐
𝑘(𝑡). This
number is logarithmic w.r.t. 𝜏. The term𝜉(𝜏,𝜌)represents those
plays that occur despite the arms’ sufficiently small deviation be-
tween the true reward-cost ratio and the ratio of the sample means.
For each suboptimal play, a policy suffers a greater-than-zero
regret in expectation. Hence, the number of suboptimal plays is
closely linked to a policy’s regret. Lemma 4 in [ 24] establishes this
connection for Budgeted MABs. In combination with Theorem 4,
this lemma thus yields the worst-case regret for 𝜔-UCB.
Theorem 5 (Finite-budget instance-dependent regret). Define
Δ𝑘,𝑛∗
𝑘(𝜏𝐵)and𝜉(𝜏𝐵,𝜌)as before, and 𝜏𝐵=j
2𝐵/min𝑘∈[𝐾]𝜇𝑐
𝑘k
.
For any𝜌>0,𝜔-UCB suffers instance-dependent regret of
Regret≤𝐾∑︁
𝑘=2Δ𝑘
1+𝑛∗
𝑘(𝜏𝐵)+𝜉(𝜏𝐵,𝜌)
+X(𝐵)𝐾∑︁
𝑘=2Δ𝑘+2𝜇𝑟
1
𝜇𝑐
1,(10)
whereX(𝐵)is inO
(𝐵/𝜇𝑐
𝑚𝑖𝑛)𝑒−0.5𝐵𝜇𝑐
𝑚𝑖𝑛
.
Proof. Lemma 4 of [ 24] provides a policy-independent regret
expression for Budgeted MAB policies:
Regret≤𝐾∑︁
𝑘=2Δ𝑘E[𝑛𝑘(𝜏𝐵)]+X(𝐵)𝐾∑︁
𝑘=2Δ𝑘+2𝜇𝑟
1
𝜇𝑐
1(11)
where
𝜏𝐵=$
2𝐵
min𝑘∈[𝐾]𝜇𝑐
𝑘%
andX(𝐵)is inO
(𝐵/𝜇𝑐
𝑚𝑖𝑛)𝑒−0.5𝐵𝜇𝑐
𝑚𝑖𝑛
.
Substituting E[𝑛𝑘(𝜏𝐵)]in Eq. (11)with the result from Theorem 4
completes the proof. □
Theorem 5 and our definition of 𝑛∗
𝑘(𝜏)show that the regret of
𝜔-UCB decreases for small 𝜂𝑟
𝑘and𝜂𝑘𝑐. I.e., when the reward and cost
distributions have small variance compared to a Bernoulli variable
with the same expected value.
The term𝜉(𝜏,𝜌)decreases, while 𝑛∗
𝑘(𝜏)increases with 𝜌. Fur-
ther derivations show that for increasingly large budgets, 𝜉(𝜏,𝜌)
converges for 𝜌>1, grows logarithmic for 𝜌=1, and superloga-
rithmic (on the order of O(𝐵1−𝜌)) for𝜌<1; see Appendix B.4 for
the details. This results in the following asymptotic behavior:
Theorem 6 (Asymptotic regret). The regret of 𝜔-UCB is in
O
𝐵1−𝜌
for0<𝜌<1,and inO(log𝐵)for𝜌≥1.
5 EXPERIMENTAL SETUP
This section presents the experimental setup used to evaluate the
policies. We introduce the MAB settings, followed by the configura-
tions of𝜔-UCB and its competitors. We conducted the experiments
on an Ubuntu 20.04 server with x86-architecture, using 32 cores,
each running at 2.0 GHz, and 128 GB of RAM.
 
1078Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD ’24, August 25–29, 2024, Barcelona, Spain
5.1 Budgeted MAB Settings
We use MAB settings based on synthetic and real data, which we
describe separately. Each setting comprises a specific combination
of reward and cost distributions, and the number of arms 𝐾. See
Table 1 for a summary.
Table 1: Evaluation settings
Type Distr. Params K Used in Id
Synth.BernoulliU(0,1)10 [23, 24] S-Br-10
50 [24] S-Br-50
100 [22, 23] S-Br-100
General.
BernoulliU(0,1)10 [23, 25] S-GBr-10
50 [25] S-GBr-50
100 [23] S-GBr-100
BetaU(0,5)10 [24, 25] S-Bt-10
50 [24, 25] S-Bt-50
100 [22] S-Bt-100
Face-
bookBernoulli given [2,97] – FB-Br
Beta random [2,97] – FB-Bt
Synthetic Data. Previous studies on Budgeted Multi-Armed Ban-
dits (MABs) use synthetic settings with rewards and costs drawn
from discrete (Bernoulli or Generalized Bernoulli with possible
outcomes{0.0,0.25,0.5,0.75,1.0}) or continuous (Beta) distribu-
tions [ 18,19,22,24]. These studies typically generate parame-
ters randomly within a given range [ 22–25] and use 10 to 100
arms [ 19,22–25]. We adopt this and set the parameter ranges to
those used in related work.
Social-Media Advertisement Data. We also evaluate our policy
in a social media advertisement scenario described in Example 1.
We use real-world data from [ 12]. It contains information about
different ads based on their target gender (female or male) and age
category (30–34, 34–39, 40–44, 45–49), along with the number of
displays and clicks, the total cost, and the number of purchases.
We group the ads by target gender and age category, resulting in
19 “advertisement campaigns” (Budgeted MAB settings). Each cam-
paign has between 2 and 93 ads (arms). We compute the expected
rewards𝜇𝑟
𝑘and costs𝜇𝑐
𝑘of each ad as the average revenue per click
and average cost per click, respectively. We model both discrete
and continuous rewards and costs. For the discrete case, we sample
rewards and costs from two Bernoulli distributions with expected
values of𝜇𝑟
𝑘and𝜇𝑐
𝑘, respectively. In the continuous case, we use
a Beta distribution and sample the distribution parameters from a
uniform distribution with a range of (0, 5). We then adjust one of
the parameters to ensure that the expected values of rewards and
costs match 𝜇𝑟
𝑘and𝜇𝑐
𝑘.
5.2 Budgeted MAB Policies
We test the performance of two variants of our policy: 𝜔-UCB
and𝜔∗-UCB. The𝜔-UCB variant uses a fixed value of 𝜂=1for
rewards and costs. The 𝜔∗-UCB uses𝜂𝑟
𝑘=𝜂𝑐
𝑘=1as default but
estimates their values using sample variance and mean once arm𝑘has been played sufficiently many times ( 𝑛𝑘(𝑇) ≥ 30), ˆ𝜂𝑘=
ˆ𝜎2
𝑘/((𝑀−ˆ𝜇𝑘)(ˆ𝜇𝑘−𝑚)), where bars refer to sample estimates as
before. We experiment with two values of the hyperparameter 𝜌:
𝜌=1and𝜌=1/4. The former is the minimum value for which we
have proven logarithmic regret. The latter has performed well in
our sensitivity study, as we will demonstrate in Section 6.2.
We compare the performance of our policy to several other state-
of-the-art Budgeted MAB policies, including BTS, Budget-UCB, i-
UCB, c-UCB, m-UCB, b-greedy, UCB-SC+, and UCB-B2. We set the
hyperparameters for each competitor to the values recommended
in their respective papers. Appendix A features details about the
policies and their hyperparameters.
6 RESULTS
To observe the asymptotic behavior of the policies, we set the budget
𝐵to1.5·105times the minimum expected cost. We run each policy
until the available budget is depleted and report the average results
over 100 independent repetitions with the repetition index as the
seed for the random number generator. Since we draw the expected
cost𝜇𝑐
𝑘and expected reward 𝜇𝑟
𝑘randomly in each repetition of the
experiment, we normalize the budget in our graphs. Note that, in
some instances, an approach may not appear in a graph if its regret
exceeded the graph’s upper y-axis limit.
6.1 Performance of Budgeted MAB Policies
6.1.1 Synthetic Bernoulli. Figure 4a shows the regret of the policies
for Bernoulli-distributed rewards and costs with 95% confidence
intervals. We do not present 𝜔∗-UCB in this experiment since its
results are almost identical to 𝜔-UCB.𝜔-UCB and BTS achieve
logarithmic regret and demonstrate better asymptotic behavior than
other methods. Although {i,c,m}-UCB may outperform 𝜔-UCB with
𝜌=1for small budgets, their regret grows rapidly as the budget
increases, indicating poor asymptotic behavior. One surprising
observation is that UCB-B2 is not competitive in our experiments
despite its tight and accurate UCB (cf. Fig. 1). Our assumption is
that this approach decreases the confidence level too fast, which we
found can result in over-exploration. For 𝐾=50and𝐾=100, our
policy has lower regret than BTS. BTS outperforms 𝜔-UCB with𝜌=
1only on the 10-armed bandit. 𝜔-UCB with𝜌=1/4outperforms
all other policies on small and large budgets and regardless of 𝐾.
Comparing𝜌=1/4with𝜌=1, one can see that the curve for 𝜌=1
is linear (the x-axis is logarithmic), while for 𝜌=1/4it is convex.
We conclude that 𝜌=1/4leads to smaller regret than 𝜌=1for not
too large budgets but that 𝜌=1performs better asymptotically.
6.1.2 Synthetic Generalized Bernoulli and Synthetic Beta. Figure 4b
shows the policies’ regret for rewards and costs drawn from Gen-
eralized Bernoulli distributions and Figure 4c for Beta distributed
rewards and costs. Besides 𝜔-UCB, we also present the results for
𝜔∗-UCB which estimates 𝜂𝑟
𝑘and𝜂𝑐
𝑘from the sample variance of
rewards and costs. 𝜔-UCB and𝜔∗-UCB with𝜌=1/4outperform
their competitors, except for 𝐾=100in the Beta bandit where
m-UCB performs comparable. We also notice that BTS is not com-
petitive in this evaluation setting, likely because the policy cannot
account for the (often very small) variance of the sampled Beta
distributions. Our 𝜔∗-UCB policy is advantageous in such cases.
 
1079KDD ’24, August 25–29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard Fouché, and Klemens Böhm
10−1100
Normalized Budget02000Regret
K= 10
10−1100
Normalized Budget05000
K= 50
10−1100
Normalized Budget010000
K= 100ω-UCB (ρ= 1)
ω-UCB (ρ=1
4)ω∗-UCB (ρ= 1)
ω∗-UCB (ρ=1
4)UCB-SC+
Budget-UCBUCB-B2
m-UCBc-UCB
i-UCBBTS
b-greedy
(a) Settings S-Br-{10,50,100}
10−1100
Normalized Budget010002000Regret
K= 10
10−1100
Normalized Budget020004000
K= 50
10−1100
Normalized Budget020004000
K= 100
(b) Settings S-GBr- {10,50,100}
10−1100
Normalized Budget02000Regret
K= 10
10−1100
Normalized Budget05000
K= 50
10−1100
Normalized Budget010000
K= 100
(c) Settings S-Bt-{10,50,100}
10−1100
Normalized Budget01000Regret
(d) Setting FB-Br
10−1100
Normalized Budget01000Regret
 (e) Setting FB-Bt
0.00 0.25 0.50 0.75 1.00
µc
k0.00.51.0
µr
k (f) Example kernel density estimate (kde)
1
641
321
161
81
41
212
ρ103104RegretBernoulli
1
641
321
161
81
41
212
ρBeta
1
641
321
161
81
41
212
ρGen. Bern.
BTS
ω-UCB
ω∗-UCB
(g) Sensitivity study showing the regret for different choices of 𝜌
Figure 4: Evaluation results
 
1080Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD ’24, August 25–29, 2024, Barcelona, Spain
6.1.3 Social-Media Advertisement Data. Figure 4d and Figure 4e
show the results of our study on social-media advertisement data3
[12]. Here, the default choice 𝜌=1/4outperforms all other com-
petitors.𝜌=1/4also outperforms 𝜌=1significantly, although
both choices show good asymptotic behavior. The advertisement
data settings seem to be easier for some competitors (BTS, KL-UCB-
SC+) and harder for others (i,c,m-UCB). The likely cause is that the
distribution of expected rewards and costs between arms is non-
uniform: costs are biased towards 1, and rewards are biased towards
the boundaries of[0,1], as the kernel density estimate (kde) plot
for the MAB with 𝐾=33in Figure 4f illustrates exemplary. Last,
we observe that 𝜔∗-UCB has lower regret than 𝜔-UCB, although
the effect is not as strong as in the synthetic settings.
6.2 Sensitivity Study
We investigate the performance difference between 𝜔-UCB and
𝜔∗-UCB, as well as the sensitivity of our policy with respect to
the hyperparameter 𝜌. The results based on our synthetic settings
(cf. Table 1) for 𝐾=50are shown in Figure 4g. We omit the results
for𝐾=10and𝐾=100as they look similar to 𝐾=50.𝜔-UCB
and𝜔∗-UCB perform best with 𝜌=1/4when rewards and costs
follow Bernoulli distributions. Both policies achieve comparable
performance in this case. It appears that estimating 𝜂(which is
known to be 1 in Bernoulli bandits) does not result in a performance
decrease of𝜔∗-UCB compared to 𝜔-UCB. Also, even though 𝜌=1/8
works well for 𝜔-UCB when rewards and costs follow a generalized
Bernoulli or Beta distribution, 𝜌=1/4remains a near-optimal
choice for𝜔∗-UCB. Based on these results, we recommend using
𝜔∗-UCB with𝜌=1/4as a default.
7 CONCLUSIONS
We presented a new approach for Budgeted MABs called 𝜔-UCB.
It combines UCB sampling with asymmetric confidence intervals
to address issues of existing approaches. Our interval generalizes
Wilson’s score interval to arbitrary bounded random variables. An
extension of our approach, 𝜔∗-UCB, tracks the variance of the
reward and cost distributions on the fly to tighten the confidence
intervals. This leads to even better performance when rewards
or costs are continuous. Our analysis shows that 𝜔-UCB achieves
logarithmic regret for 𝜌≥1, while𝜌=1/4performed best in our
experiments. In the future, one could extend our approach to the
successive elimination framework which repeatedly eliminates bad
arms, or derive an instance-independent regret bound. One could
also extend our approach to the non-stationary setting where the
reward and cost distributions change over time. This is particularly
relevant in scenarios like online advertising, where companies want
to promote their products and services continuously.
Acknowledgements. This work was supported by the German Re-
search Foundation (DFG) as part of the Research Training Group
GRK 2153: Energy Status Data – Informatics Methods for its Col-
lection, Analysis, and Exploitation and by the Baden-Württemberg
Foundation via the Elite Program for Postdoctoral Researchers.
3Available at https://www.kaggle.com/datasets/madislemsalu/facebook-ad-campaign
under a PDDL license.REFERENCES
[1]Danilo Ardagna, Barbara Panicucci, and Mauro Passacantando. 2011. A game
theoretic formulation of the service provisioning problem in cloud systems. In
WWW. ACM, 177–186.
[2]Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. 2002. Finite-time Analysis
of the Multiarmed Bandit Problem. Mach. Learn. 47, 2-3 (2002), 235–256. https:
//doi.org/10.1023/A:1013689704352
[3]Vashist Avadhanula, Riccardo Colini-Baldeschi, Stefano Leonardi, Karthik Abinav
Sankararaman, and Okke Schrijvers. 2021. Stochastic bandits for multi-platform
budget optimization in online advertising. In WWW ’21, Jure Leskovec, Marko
Grobelnik, Marc Najork, Jie Tang, and Leila Zia (Eds.). ACM / IW3C2, 2805–2817.
https://doi.org/10.1145/3442381.3450074
[4]Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. 2013.
Bandits with Knapsacks. In FOCS. IEEE Computer Society, 207–216.
[5]Rajendra Bhatia and Chandler Davis. 2000. A better bound on the variance. Amer.
Math. Monthly 107, 4 (2000), 353–357.
[6]Christian Borgs, Jennifer T. Chayes, Nicole Immorlica, Kamal Jain, Omid Ete-
sami, and Mohammad Mahdian. 2007. Dynamics of bid optimization in online
advertisement auctions. In WWW. ACM, 531–540.
[7]Semih Cayci, Atilla Eryilmaz, and R. Srikant. 2019. Learning to control renewal
processes with bandit feedback. Proc. ACM Meas. Anal. Comput. Syst. 3, 2 (2019),
43:1–43:32. https://doi.org/10.1145/3341617.3326158
[8]Semih Cayci, Atilla Eryilmaz, and R. Srikant. 2020. Budget-constrained bandits
over general cost and reward distributions. In AISTATS (PMLR, Vol. 108), Silvia
Chiappa and Roberto Calandra (Eds.). PMLR, 4388–4398. http://proceedings.mlr.
press/v108/cayci20a.html
[9] Debojit Das, Shweta Jain, and Sujit Gujar. 2022. Budgeted Combinatorial Multi-
Armed Bandits. In AAMAS. International Foundation for Autonomous Agents
and Multiagent Systems (IFAAMAS), 345–353.
[10] Wenkui Ding, Tao Qin, Xu-Dong Zhang, and Tie-Yan Liu. 2013. Multi-Armed
Bandit with Budget Constraint and Variable Costs. In AAAI, Vol. 27. AAAI Press,
232–238. https://doi.org/10.1609/aaai.v27i1.8637
[11] Tor Lattimore and Csaba Szepesvári. 2020. Bandit algorithms. Cambridge Univer-
sity Press, Cambridge. https://doi.org/10.1017/9781108571401
[12] Madis Lemsalu. 2017. Facebook ad campaign. https://www.
kaggle.com/madislemsalu/facebook-ad-campaign howpublished: Kaggle
(https://www.kaggle.com/madislemsalu/facebook-ad-campaign).
[13] HM Schöpf and PH Supancic. 2014. On Bürmann’s theorem and its application
to problems of linear and nonlinear heat transfer and diffusion. The Mathematica
Journal 16 (2014), 1–44.
[14] William R. Thompson. 1933. On the Likelihood that One Unknown Probability
Exceeds Another in View of the Evidence of Two Samples. Biometrika 25, 3/4
(1933), 285–294. https://doi.org/10.2307/2332286
[15] William Robin Thompson. 1935. On the Theory of Apportionment. American
Journal of Mathematics 57 (1935), 450.
[16] Long Tran-Thanh, Archie C. Chapman, Enrique Munoz de Cote, Alex Rogers, and
Nicholas R. Jennings. 2010. Epsilon-First Policies for Budget-Limited Multi-Armed
Bandits. In AAAI, Vol. 24. AAAI Press. https://doi.org/10.1609/aaai.v24i1.7758
[17] Long Tran-Thanh, Archie C. Chapman, Alex Rogers, and Nicholas R. Jennings.
2012. Knapsack Based Optimal Policies for Budget-Limited Multi-Armed Bandits.
InAAAI, Vol. 26. AAAI Press, 1134–1140. https://doi.org/10.1609/aaai.v26i1.8279
[18] Ryo Watanabe, Junpei Komiyama, Atsuyoshi Nakamura, and Mineichi Kudo.
2017. KL-UCB-Based Policy for Budgeted Multi-Armed Bandits with Stochastic
Action Costs. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 100-A, 11
(2017), 2470–2486.
[19] Ryo Watanabe, Junpei Komiyama, Atsuyoshi Nakamura, and Mineichi Kudo.
2018. UCB-SC: A Fast Variant of KL-UCB-SC for Budgeted Multi-Armed Bandit
Problem. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 101-A, 3 (2018),
662–667.
[20] E.T. Whittaker and G.N. Watson. 2020. A course of modern analysis; an intro-
duction to the general theory of infinite processes and of analytic functions (4 ed.).
Cambridge University Press, Cambridge. pages: 208 section: 7.3.
[21] Edwin B. Wilson. 1927. Probable Inference, the Law of Succession, and Statistical
Inference. J. Amer. Statist. Assoc. 22, 158 (1927), 209–212. https://doi.org/10.2307/
2276774
[22] Yingce Xia, Wenkui Ding, Xu-Dong Zhang, Nenghai Yu, and Tao Qin. 2015.
Budgeted Bandit Problems with Continuous Random Costs. In ACML (JMLR
Workshop and Conference Proceedings, Vol. 45). JMLR.org, 317–332.
[23] Yingce Xia, Haifang Li, Tao Qin, Nenghai Yu, and Tie-Yan Liu. 2015. Thompson
Sampling for Budgeted Multi-Armed Bandits. In IJCAI. AAAI Press, 3960–3966.
[24] Yingce Xia, Tao Qin, Wenkui Ding, Haifang Li, Xudong Zhang, Nenghai Yu,
and Tie-Yan Liu. 2017. Finite budget analysis of multi-armed bandit problems.
Neurocomputing 258 (2017), 13–29. https://doi.org/10.1016/j.neucom.2016.12.079
[25] Yingce Xia, Tao Qin, Weidong Ma, Nenghai Yu, and Tie-Yan Liu. 2016. Budgeted
Multi-Armed Bandits with Multiple Plays. In IJCAI. IJCAI/AAAI Press, 2210–
2216.
 
1081KDD ’24, August 25–29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard Fouché, and Klemens Böhm
A RELATED UCB APPROACHES
Table 2 summarizes our direct competitors and how they compute
the arm selection indexes Ω𝑘(𝑡).
B PROOFS
B.1 Proof of Theorem 4
The proof starts with a general expression for the number of plays of
a suboptimal arm 𝑘>1, where 1{·}denotes the indicator function.
𝑛𝑘(𝜏)≤1+𝜏∑︁
t=K+11
Ω𝑘(𝑡)≥Ω𝑗(𝑡),∀𝑗≠𝑖	
≤1+𝜏∑︁
t=K+11{Ω𝑘(𝑡)≥Ω1(𝑡)}
This is upper-bounded by
𝑛𝑘(𝜏)≤1+𝜏∑︁
t=K+1
1
Ω𝑘(𝑡)≥Ω1(𝑡),Ω1(𝑡)<𝜇𝑟
1
𝜇𝑐
1
+1
Ω𝑘(𝑡)≥Ω1(𝑡),Ω1(𝑡)≥𝜇𝑟
1
𝜇𝑐
1 
≤1+𝜏∑︁
t=K+1
1
Ω1(𝑡)<𝜇𝑟
1
𝜇𝑐
1
+1
Ω𝑘(𝑡)≥𝜇𝑟
1
𝜇𝑐
1 
The expected number of plays E[𝑛𝑘(𝜏)]is given by the proba-
bilities of the individual events:
E[𝑛𝑘(𝜏)]≤ 1+𝜏∑︁
t=K+1
Pr
Ω1(𝑡)<𝜇𝑟
1
𝜇𝑐
1
|                {z                }
Pr[𝐴]+Pr
Ω𝑘(𝑡)≥𝜇𝑟
1
𝜇𝑐
1
|                {z                }
Pr[𝐵]
(12)
We now evaluate the sum in the equation above.
Sum of Pr[𝐴].We apply Theorem 2:
𝜏∑︁
t=K+1Pr[𝐴]<𝜏∑︁
t=K+1h
1−√
1−𝑡−𝜌i
=(𝜏−𝐾)−𝜏∑︁
t=K+1√
1−𝑡−𝜌
(13)
Sum of Pr[𝐵].For this step, let us first introduce a helpful lemma:
Lemma 1 bounds Pr
Ω𝑘(𝑡)≥𝜇𝑟
1/𝜇𝑐
1
after a minimum number of
plays𝑛∗
𝑘(𝜏), which grows logarithmic with 𝜏.
Lemma 1. Define𝛿𝑘=Δ𝑘
Δ𝑘+1/𝜇𝑐
𝑘and𝑛∗
𝑘(𝜏)as follows:
𝑛∗
𝑘(𝜏)=8𝜌log𝜏
𝛿2
𝑘max(
𝜂𝑟
𝑘𝜇𝑟
𝑘
1−𝜇𝑟
𝑘,𝜂𝑐
𝑘(1−𝜇𝑐
𝑘)
𝜇𝑐
𝑘)
The following inequality holds whenever 𝑛𝑘(𝑡)≥𝑛∗
𝑘(𝜏):
Pr
Ω𝑘(𝑡)≥𝜇𝑟
1
𝜇𝑐
1
<1−√
1−𝜏−𝜌
Appendix B.3 contains the proof of Lemma 1. See Appendix B.2
for the derivation of 𝛿𝑘.The lemma allows to decomposeÍ𝜏
t=K+1Pr[𝐵]into “initial plays”
(𝑛𝑘(𝑡)<𝑛∗
𝑘(𝜏)) and “later plays” ( 𝑛𝑘(𝑡)≥𝑛∗
𝑘(𝜏)):
𝜏∑︁
t=K+1Pr[𝐵]=𝜏∑︁
t=K+1Pr
Ω𝑘(𝑡)≥𝜇𝑟
1
𝜇𝑐
1
=𝑛∗
𝑘(𝜏)+𝜏∑︁
t=K+1Pr
Ω𝑘(𝑡)≥𝜇𝑟
1
𝜇𝑐
1,𝑛𝑘(𝑡)≥𝑛∗
𝑘(𝜏)
We now apply Lemma 1 to evaluate the sum in above equation,
𝜏∑︁
t=K+1Pr[𝐵]≤𝑛∗
𝑘(𝜏)+𝜏∑︁
t=K+1h
1−√
1−𝜏−𝜌i
=𝑛∗
𝑘(𝜏)+(𝜏−𝐾)
1−√
1−𝜏−𝜌
. (14)
Inserting the results of Eq. (13)and Eq. (14)in Eq. (12)yields a
bound on E[𝑛𝑘(𝜏)]:
E[𝑛𝑘(𝜏)]≤ 1+𝑛∗
𝑘(𝜏)+(𝜏−𝐾)
2−√
1−𝜏−𝜌
−𝜏∑︁
t=K+1√
1−𝑡−𝜌
|                                                 {z                                                 }
𝜉(𝜏,𝜌)
B.2 Derivation of Proportional 𝛿-gap
Let𝛿𝑘be the proportional 𝛿-gap of arm 𝑘. It measures how much
one must increase 𝜇𝑟
𝑘and decrease 𝜇𝑐
𝑘in order to make arm 𝑘have
the same reward-cost ratio as arm 1, or in other words, to bridge
the suboptimality gap Δ𝑘between arm 𝑘and arm 1. The 𝛿-gap is
proportional to the possible increase in rewards (decrease in costs)
without violating their range [0,1]. We start our derivation with
Eq. (15), which states this mathematically.
𝜇𝑟
𝑘+𝛿𝑘(1−𝜇𝑟
𝑘)
𝜇𝑐
𝑘−𝛿𝑘𝜇𝑐
𝑘=𝜇𝑟
1
𝜇𝑐
1(15)
Rearranging the equation yields
𝜇𝑟
1
𝜇𝑐
1−𝜇𝑟
𝑘
𝜇𝑐
𝑘=𝛿𝑘 
𝜇𝑟
1
𝜇𝑐
1−𝜇𝑟
𝑘
𝜇𝑐
𝑘+1
𝜇𝑐
𝑘!
.
Now, recall the definition of an arm’s suboptimality, Δ𝑘=𝜇𝑟
1/𝜇𝑐
1−
𝜇𝑟
𝑘/𝜇𝑐
𝑘, and solve for 𝛿𝑘:
𝛿𝑘=Δ𝑘
Δ𝑘+1
𝜇𝑐
𝑘
B.3 Proof of Lemma 1
Recall Lemma 1:
Lemma 1. Define𝛿𝑘=Δ𝑘
Δ𝑘+1/𝜇𝑐
𝑘and𝑛∗
𝑘(𝜏)as follows:
𝑛∗
𝑘(𝜏)=8𝜌log𝜏
𝛿2
𝑘max(
𝜂𝑟
𝑘𝜇𝑟
𝑘
1−𝜇𝑟
𝑘,𝜂𝑐
𝑘(1−𝜇𝑐
𝑘)
𝜇𝑐
𝑘)
The following inequality holds whenever 𝑛𝑘(𝑡)≥𝑛∗
𝑘(𝜏):
Pr
Ω𝑘(𝑡)≥𝜇𝑟
1
𝜇𝑐
1
<1−√
1−𝜏−𝜌
 
1082Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Overview of our competitors
Policy Ω𝑘(𝑡) Comment
BTS [23]sample from Beta(𝛼𝑟(𝑡),𝛽𝑟(𝑡))
sample from Beta(𝛼𝑐(𝑡),𝛽𝑐(𝑡))𝛼𝑟(𝑡)=𝑛𝑘(𝑡)ˆ𝜇𝑟
𝑘(𝑡)+1
𝛽𝑟(𝑡)=𝑛𝑘(𝑡)+2−𝛼𝑟(𝑡)
𝛼𝑐(𝑡)=𝑛𝑘(𝑡)ˆ𝜇𝑐
𝑘(𝑡)+1
𝛽𝑐(𝑡)=𝑛𝑘(𝑡)+2−𝛼𝑐(𝑡)
Discretization of con-
tinuous values
m-UCB [24]min{ˆ𝜇𝑟
𝑘(𝑡)+𝜀𝑘(𝑡),1}
max{ˆ𝜇𝑐
𝑘(𝑡)−𝜀𝑘(𝑡),0}𝜀𝑘(𝑡)=𝛼√︃
log(𝑡−1)
𝑛𝑘(𝑡)
Recommendation: 𝛼=2−4
c-UCB [24]ˆ𝜇𝑟
𝑘(𝑡)
ˆ𝜇𝑐
𝑘(𝑡)+𝜀𝑘(𝑡)
ˆ𝜇𝑐
𝑘(𝑡)𝜀𝑘(𝑡)=𝛼√︃
log(𝑡−1)
𝑛𝑘(𝑡)
Recommendation: 𝛼=2−3
i-UCB [24]ˆ𝜇𝑟
𝑘(𝑡)
ˆ𝜇𝑐
𝑘(𝑡)+𝜀𝑘(𝑡)𝜀𝑘(𝑡)=𝛼√︃
log(𝑡−1)
𝑛𝑘(𝑡)
Recommendation: 𝛼=2−2
Budget UCB [22]ˆ𝜇𝑟
𝑘(𝑡)
ˆ𝜇𝑐
𝑘(𝑡)+𝜀𝑘(𝑡)
ˆ𝜇𝑐
𝑘(𝑡)
1+min{ˆ𝜇𝑟
𝑘(𝑡)+𝜀𝑘(𝑡),1}
max{ˆ𝜇𝑐
𝑘(𝑡)−𝜀𝑘(𝑡),𝜆}𝜀𝑘(𝑡)=√︃
log(𝑡−1)
𝑛𝑘(𝑡)
𝜆>0: minimum cost
UCB-SC+ [19]ˆ𝜇𝑟
𝑘(𝑡)+𝛼𝑘(𝑡)ˆ𝜇𝑐
𝑘(𝑡)
ˆ𝜇𝑐
𝑘(𝑡)−𝛼𝑘(𝑡)ˆ𝜇𝑟
𝑘(𝑡),ifˆ𝜇𝑐
𝑘(𝑡)2>log𝑡
𝑛𝑘(𝑡)
2𝑛𝑘(𝑡)
∞, else𝛼𝑘(𝑡)=√︄
log𝑡
𝑛𝑘(𝑡)
2𝜅𝑛𝑘(𝑡)−log𝑡
𝑛𝑘(𝑡)
with𝜅=ˆ𝜇𝑟
𝑘(𝑡)2+ˆ𝜇𝑐
𝑘(𝑡)2
UCB-B2 [8]1.4𝜖𝑘,𝑡+ˆ𝑟𝑘,𝑡
ˆ𝜇𝑐
𝑘(𝑡)+ˆ𝑟𝑘,𝑡 if condition 7 in [8] for 𝜆=1.28
∞, elseˆ𝑟𝑘,𝑡=max{0,ˆ𝜇𝑟
𝑘(𝑡)}
max{minimum cost ,ˆ𝜇𝑐
𝑘(𝑡)}
𝜖𝑘,𝑡=√︂
2ˆ𝑉𝑟
𝑘,𝑡log𝑡𝛼
𝑛𝑘(𝑡)+3 log𝑡𝛼
𝑛𝑘(𝑡)
𝜂𝑘,𝑡=√︂
2ˆ𝑉𝑐
𝑘,𝑡log𝑡𝛼
𝑛𝑘(𝑡)+3 log𝑡𝛼
𝑛𝑘(𝑡)
ˆ𝑉𝑟
𝑘,𝑡,ˆ𝑉𝑐
𝑘,𝑡: variance estimates
𝛼>2; small choices preferable
The proof is structured in four steps. First, we derive an ex-
pression for the maximum deviation between the observed sample
mean and the unknown expected value of an arm’s rewards and
costs that we use later on. Second, we decompose the probability
Pr
Ω𝑘(𝑡)≥𝜇𝑟
1/𝜇𝑐
1
. Third, we evaluate the decomposed probabili-
ties for cases where 𝑛𝑘(𝑡)is sufficiently large, that is, 𝑛𝑘(𝑡)≥𝑛∗
𝑘(𝜏).
Finally, we recombine the decomposed probabilities to obtain the
final result.
Deviation between sample mean and expected value. Letˆ𝜇𝑘(𝑡)be
the sample mean and 𝜇𝑘the expected value of arm 𝑘’s rewards or
costs at time 𝑡. To quantify the deviation between mean ˆ𝜇𝑘(𝑡)and𝜇𝑘
we start from Eq. (6)(central limit theorem) and set [𝑚,𝑀]=[0,1]
and𝑛=𝑛𝑘(𝑡)):
(ˆ𝜇𝑘(𝑡)−𝜇𝑘)2≤𝜂𝑘𝜇𝑘(1−𝜇𝑘)
𝑛𝑘(𝑡)𝑧2(16)
The above inequality holds with the same probability as our
confidence interval since it is the basis of the confidence interval
derivation. This allows us to bound the deviation between sample
mean and expected value, denoted 𝜀𝑘(𝑡), for our choice of 𝑧𝜌(𝑡)=√︁
2𝜌log𝑡:
Pr[|ˆ𝜇𝑘(𝑡)−𝜇𝑘|>𝜀𝑘(𝑡)]≤𝛼(𝑡) (17)
with
𝜀𝑘(𝑡)=√︄
2𝜂𝑘𝜇𝑘(1−𝜇𝑘)𝜌log𝑡
𝑛𝑘(𝑡)and𝛼(𝑡)<1−√
1−𝑡−𝜌
Whenever we refer to 𝜀𝑘(𝑡)w.r.t. rewards or costs we use the
notations𝜀𝑟
𝑘(𝑡)and𝜀𝑐
𝑘(𝑡).
Decomposition of Pr
Ω𝑘(𝑡)≥𝜇𝑟
1/𝜇𝑐
1.Next, we decompose the
probability that Ω𝑘(𝑡)≥𝜇𝑟
1/𝜇𝑐
1. The decomposition is analogous
to the one in the proof of Theorem 2 and thus omitted here:
Pr
Ω𝑘(𝑡)≥𝜇𝑟
1
𝜇𝑐
1
=Pr"
𝜔𝑟
𝑘+(𝑡)
𝜔𝑐
𝑘−(𝑡)≥𝜇𝑟
1
𝜇𝑐
1#
=Pr"
𝜔𝑟
𝑘+(𝑡)
𝜔𝑐
𝑘−(𝑡)≥𝜇𝑟
𝑘+(1−𝜇𝑟
𝑘)𝛿𝑘
𝜇𝑐
𝑘−𝜇𝑐
𝑘𝛿𝑘#
(18)
≤Pr
𝜔𝑟
𝑘+(𝑡)≥𝜇𝑟
𝑘+(1−𝜇𝑟
𝑘)𝛿𝑘
+Pr
𝜔𝑐
𝑘−(𝑡)≤𝜇𝑐
𝑘−𝜇𝑐
𝑘𝛿𝑘
(19)
 
1083KDD ’24, August 25–29, 2024, Barcelona, Spain Marco Heyden, Vadim Arzamasov, Edouard Fouché, and Klemens Böhm
For the next step, note that if 𝜇𝑟
𝑘≤𝜔𝑟
𝑘+(𝑡)(and𝜇𝑐
𝑘≥𝜔𝑐
𝑘−(𝑡)),
we have that 𝜇𝑟
𝑘−ˆ𝜇𝑟
𝑘(𝑡)≤𝜀𝑟
𝑘(𝑡)(and ˆ𝜇𝑐
𝑘(𝑡)−𝜇𝑐
𝑘≤𝜀𝑐
𝑘(𝑡)). This
allows us to rewrite the terms in Eq. (19) as follows:
Pr
𝜔𝑟
𝑘+(𝑡)≥𝜇𝑟
𝑘+(1−𝜇𝑟
𝑘)𝛿𝑘
=Pr
𝜇𝑟
𝑘+(1−𝜇𝑟
𝑘)𝛿𝑘−ˆ𝜇𝑟
𝑘(𝑡)≤𝜀𝑟
𝑘(𝑡)
=Prˆ𝜇𝑟
𝑘(𝑡)−𝜇𝑟
𝑘≥(1−𝜇𝑟
𝑘)𝛿𝑘−𝜀𝑟
𝑘(𝑡)
(20)
Pr
𝜔𝑐
𝑘−(𝑡)≤𝜇𝑐
𝑘−𝜇𝑐
𝑘𝛿𝑘
=Prˆ𝜇𝑐
𝑘(𝑡)−(𝜇𝑐
𝑘−𝜇𝑐
𝑘𝛿𝑘)≤𝜀𝑐
𝑘(𝑡)
=Pr
𝜇𝑐
𝑘−ˆ𝜇𝑐
𝑘(𝑡)≥𝜇𝑐
𝑘𝛿𝑘−𝜀𝑐
𝑘(𝑡)
(21)
In the next two paragraphs, we evaluate Eq. (20)(confidence
bound of rewards) and Eq. (21)(confidence bound of costs). We
combine both results afterwards.
Evaluation of Eq. (20)for𝑛𝑘(𝑡)≥𝑛∗,𝑟
𝑘(𝜏).We now consider the
cases in which the number of times arm 𝑘was played is at least
logarithmic w.r.t. 𝜏, i.e.,𝑛𝑘(𝑡)≥𝑛∗,𝑟
𝑘(𝜏)with
𝑛∗,𝑟
𝑘(𝜏)=2𝜌log𝜏
𝛿2
𝑘(1−𝜅)2𝜂𝑟
𝑘𝜇𝑟
𝑘
1−𝜇𝑟
𝑘,for any𝜅∈(0,1).
In those cases, 𝜀𝑟
𝑘(𝑡)≤( 1−𝜅)𝛿𝑘(1−𝜇𝑟
𝑘). One can verify this by
inserting𝑛∗,𝑟
𝑘(𝜏)in the definition of 𝜀𝑟
𝑘(𝑡), cf. Eq. (17). This gives
the following inequality for the right side of Eq. (20):
Pr
𝜔𝑟
𝑘+(𝑡)≥𝜇𝑟
𝑘+(1−𝜇𝑟
𝑘)𝛿𝑘
≤Prˆ𝜇𝑟
𝑘(𝑡)−𝜇𝑟
𝑘≥𝜅(1−𝜇𝑟
𝑘)𝛿𝑘
Last, we compute the number of standard deviations 𝑧∗that
corresponds to a deviation between ˆ𝜇𝑟
𝑘(𝑡)and𝜇𝑟
𝑘of at maximum
𝜅(1−𝜇𝑟
𝑘)𝛿𝑘based on Eq. (16). In particular, we solve the right-most
inequality in the expression below:
(ˆ𝜇𝑟
𝑘(𝑡)−𝜇𝑟
𝑘)2≤𝜂𝑟
𝑘𝜇𝑟
𝑘(1−𝜇𝑟
𝑘)
𝑛𝑘(𝑡)𝑧∗2
≤𝜂𝑟
𝑘𝜇𝑟
𝑘(1−𝜇𝑟
𝑘)
𝑛∗,𝑟
𝑘(𝜏)𝑧∗2≤(𝜅(1−𝜇𝑟
𝑘)𝛿𝑘)2
With our choice of 𝑛∗,𝑟
𝑘(𝜏), this yields𝑧∗= 2𝜌log𝜏𝜅2(1−𝜅)−21
2.
Inserting𝑧∗in Eq. (8)(upper bound for 𝛼(𝑡)) results in the following
bound:
Pr
𝜔𝑟
𝑘+(𝑡)≥𝜇𝑟
𝑘+(1−𝜇𝑟
𝑘)𝛿𝑘
<1
2©­
«1−√︂
1−𝜏−𝜅2𝜌
(1−𝜅)2ª®
¬(22)
Evaluation of Eq. (21) for𝑛𝑘(𝑡)>𝑛∗,𝑐
𝑘(𝜏).Again, we consider
the cases in which the number of times arm 𝑘was played is at least
logarithmic in 𝜏, i.e.,𝑛𝑘(𝑡)≥𝑛∗,𝑐
𝑘(𝜏)with
𝑛∗,𝑐
𝑘(𝜏)=2𝜌log𝜏
𝛿2
𝑘(1−𝜅)2𝜂𝑐
𝑘(1−𝜇𝑐
𝑘)
𝜇𝑐
𝑘, 𝜅∈(0,1).
In those cases, 𝜀𝑐
𝑘(𝑡)≤( 1−𝜅)𝛿𝑘𝜇𝑐
𝑘. Following analogous steps
as in the previous paragraph yields Eq. (23):
Pr
𝜔𝑐
𝑘−(𝑡)≤𝜇𝑐
𝑘−𝜇𝑐
𝑘𝛿𝑘
<1
2 
1−√︃
1−𝜏−(1−𝜅)2𝜌
𝜅2!
(23)Obtaining the final result. With the results in Eq. (22)and Eq. (23)
we can finally evaluate Eq. (18). A choice of 𝜅=0.5and
𝑛∗
𝑘(𝜏)=8𝜌log𝜏
𝛿2
𝑘max(
𝜂𝑟
𝑘𝜇𝑟
𝑘
1−𝜇𝑟
𝑘,𝜂𝑐
𝑘(1−𝜇𝑐
𝑘)
𝜇𝑐
𝑘)
yields the bound given in Lemma 1:
Pr
Ω𝑘(𝑡)≥𝜇𝑟
1
𝜇𝑐
1
<1−√
1−𝜏−𝜌, 𝑛𝑘(𝑡)≥𝑛∗
𝑘(𝜏)
B.4 Proof of Theorem 6
First note that the latter two terms and 𝑛∗
𝑘(𝜏𝐵)in Eq. (10)are in
O(log𝐵)[24]. Next we show that 𝜉(𝜏𝐵,𝜌)is inO(log𝐵)for𝜌≥1
and inO(𝐵1−𝜌)for0<𝜌<1.
We exploit two inequalities in our proof; the latter is based on
the integral test for convergence and holds for continuous, positive,
decreasing functions.
√
1−𝑡−𝜌≥1−𝑡−𝜌, 𝑡≥1,𝜌>0 (24)
𝜏𝐵∑︁
𝑡=𝐾+1𝑡−𝜌≤(𝐾+1)−𝜌+∫𝜏𝐵
𝑡=𝐾+1𝑡−𝜌𝑑𝑡 (25)
We use Eq. (24)to obtain an integrable expression for the sum in
𝜉(𝜏𝐵,𝜌). We replace the sum with an integral-based upper bound
as in Eq. (25):
𝜉(𝜏𝐵,𝜌)=(𝜏𝐵−𝐾)
2−√︃
1−𝜏−𝜌
𝐵
−𝜏𝐵∑︁
𝑡=𝐾+1√
1−𝑡−𝜌
≤(𝜏𝐵−𝐾)
2−√︃
1−𝜏−𝜌
𝐵
−𝜏𝐵∑︁
𝑡=𝐾+11−𝑡−𝜌
=(𝜏𝐵−𝐾)
1−√︃
1−𝜏−𝜌
𝐵
+𝜏𝐵∑︁
𝑡=𝐾+1𝑡−𝜌
≤(𝜏𝐵−𝐾)
1−√︃
1−𝜏−𝜌
𝐵
+(𝐾+1)−𝜌+∫𝜏𝐵
𝑡=𝐾+1𝑡−𝜌𝑑𝑡
Next, we evaluate the integral for the cases 𝜌=1and𝜌≠1.
Case 1:𝜌=1.
𝜉(𝜏𝐵,𝜌)≤(𝜏𝐵−𝐾)
1−√︃
1−𝜏−1
𝐵
+(𝐾+1)−1+log𝜏𝐵−log(𝐾+1)
For𝜌=1, the first term in above equation converges, so 𝜉(𝜏𝐵,𝜌=
1)is inO(log𝐵). This implies that the overall regret of 𝜔-UCB is
inO(log𝐵)for𝜌=1.
Case 2:𝜌≠1.
𝜉(𝜏𝐵,𝜌)≤(𝜏𝐵−𝐾)
1−√︃
1−𝜏−𝜌
𝐵
+(𝐾+1)−𝜌+1
1−𝜌
𝜏1−𝜌
𝐵−(𝐾+1)1−𝜌
For𝜌>1,𝜉(𝜏𝐵,𝜌)converges and thus the overall regret is in
O(log𝐵). For 0<𝜌<1, one can show that 𝜉(𝜏𝐵,𝜌)is inO(𝐵1−𝜌).
Hence, the regret of 𝜔-UCB is inO(𝐵1−𝜌)for0<𝜌<1.
To summarize, the regret of our policy is in O(𝐵1−𝜌)for0<
𝜌<1and inO(log𝐵)for𝜌≥1.
 
1084