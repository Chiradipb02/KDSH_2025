Future Impact Decomposition in Request-level Recommendations
Xiaobei Wang∗†
Peking University
Beijing, China
wangxiaobei@stu.pku.edu.cnShuchang Liu∗
Kuaishou Technology
Beijing, China
liushuchang@kuaishou.comXueliang Wang
Kuaishou Technology
Beijing, China
wangxueliang03@kuaishou.com
Qingpeng Cai‡
Kuaishou Technology
Beijing, China
caiqingpeng@kuaishou.comLantao Hu
Kuaishou Technology
Beijing, China
hulantao@kuaishou.comHan Li
Kuaishou Technology
Beijing, China
lihan08@kuaishou.com
Peng Jiang‡
Kuaishou Technology
Beijing, China
jiangpeng@kuaishou.comKun Gai
Unaffiliated
Beijing, China
gai.kun@qq.comGuangming Xie‡
Peking University
Beijing, China
xiegming@pku.edu.cn
ABSTRACT
In recommender systems, reinforcement learning solutions have
shown promising results in optimizing the interaction sequence
between users and the system over the long-term performance. For
practical reasons, the policy’s actions are typically designed as rec-
ommending a list of items to handle users’ frequent and continuous
browsing requests more efficiently. In this list-wise recommen-
dation scenario, the user state is updated upon every request in
the corresponding MDP formulation. However, this request-level
formulation is essentially inconsistent with the user’s item-level
behavior. In this study, we demonstrate that an item-level opti-
mization approach can better utilize item characteristics and opti-
mize the policy’s performance even under the request-level MDP.
We support this claim by comparing the performance of standard
request-level methods with the proposed item-level actor-critic
framework in both simulation and online experiments. Further-
more, we show that a reward-based future decomposition strategy
can better express the item-wise future impact and improve the
recommendation accuracy in the long term. To achieve a more
thorough understanding of the decomposition strategy, we propose
a model-based re-weighting framework with adversarial learning
that further boost the performance and investigate its correlation
with the reward-based strategy.
†Work was done during an internship at Kuaishou Technology.
∗The first two authors contributed equally to this work.
‡Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671506CCS CONCEPTS
•Information systems →Recommender systems; •Theory
of computation→Reinforcement learning.
KEYWORDS
Recommender Systems, Reinforcement Learning, User Modeling
ACM Reference Format:
Xiaobei Wang∗†, Shuchang Liu∗, Xueliang Wang, Qingpeng Cai‡, Lantao
Hu, Han Li, Peng Jiang‡, Kun Gai, and Guangming Xie‡. 2024. Future
Impact Decomposition in Request-level Recommendations. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671506
1 INTRODUCTION
Recommender systems serve as the user interfaces for a wide range
of web services including e-commerce, news, and short-video plat-
forms, and the general purpose is to filter contents of interests for
users and increase their engagement/interaction with the system.
Recently, the optimization of users’ engagement through reinforce-
ment learning [ 2,6,7,42–44] has become a subject of significant
interest, since it has the ability to optimize not only the user’s
immediate feedback on the recommended items but also the long-
term rewards of future interactions. The key idea is to model the
user-system interaction [ 16,49] sequence as a Markov Decision
Process (MDP) [ 32,33,36]. Specifically, each context-aware request
is represented by a user state that dynamically changes after each
interaction, and the policy infers a recommendation action based on
the current request. Then, the user provides immediate feedback for
the recommendation, which is later used to calculate the action’s
reward during training. Adopting the general reinforcement learn-
ing (RL) paradigm, the goal is to maximize a cumulative reward
over the expected future interactions.
For practical reasons, the recommendation policy is typically
designed to provide a list of items/contents rather than a single
item as an action in order to efficiently process the frequent and
continuous browsing requests [ 21]. This design choice is often seen
5905
KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaobei Wang et al.
in scenarios like short video, news, and blog recommendation sys-
tems, where users may typically consume/browse a large number
of contents in a short time. However, this list-wise recommendation
mechanism is fundamentally inconsistent with the user’s item-wise
behavior. Intuitively, a user only pays attention to one item at a
time and his/her actual “state” changes after the interaction with
each item, but the system is assumed to infer a list, obtain a list-wise
feedback, and then update the user’s state on each request.
In this paper, we emphasize this inconsistency in RL-based rec-
ommendation and formulate a request-level MDP with item-wise
rewards. Specifically, the system can only observe how the state
changes in the request-level MDP, but we assume that the ground-
truth item-level state transition in the user’s viewpoint is not ob-
servable. A straightforward solution under the request-level MDP
would simply optimize the cumulative list-wise reward using any
standard RL approach [ 23,31], but this may induce information
loss on the item characteristics for future reward estimation. As an
intuitive example, Tom receives a list of movie recommendations
from either sports or documentary categories and gives positive
feedback to sports movies but no feedback to documentary ones.
If using a list-level optimization, the system may not discover the
item-level differences and Tom’s preference for sports. Fortunately,
the item-wise user feedback throughout the interaction history is
available, which augments the list-wise reward with a more de-
tailed view. Yet, each items’ long-term value consists of both the
immediate feedback and its future impact. And it is reasonable to
believe that the causal effects of a certain item on the user’s future
interactions[ 16] are not evenly distributed in the list of exposure.
Following the previous example, recommending sports movies may
satisfy Tom’s demands and gain more trust from him, so potentially
increasing Tom’s engagement in the future. In contrast, recom-
mending a documentary might be an unsatisfactory action as it
could spoil the system’s impression to Tom and lead to a decrease
in user engagement in the future. In general, we need an RL-based
solution that can optimize the policy under the list-wise state tran-
sition, but also exploits both the item-level reward and item-wise
future impact.
To verify the aforementioned assumptions, we first propose an
item-decomposed advantage actor-critic (ItemA2C) learning frame-
work to distinguish items with different immediate feedback. The
model optimizes the same cumulative reward in the request-level
MDP as standard request-level A2C, but both the critic’s temporal
difference loss and the actor’s advantage maximization loss are
decomposed into an item-wise optimization. The empirical results
demonstrate that this item-level paradigm is superior to its request-
level counterpart. To further investigate how the item decomposi-
tion mechanism manipulates the optimization of the future impact
of items, we design a re-weighting strategy where the evaluation
of each item’s future value may vary with respect to its immediate
reward. In other words, this strategy assumes that better immediate
rewards indicate better future impact. The resulting framework the-
oretically recovers the request-level actor-critic framework while
providing separate item-wise long-term value estimation. Knowing
that there may exist other valid choices in the re-weighting space,
we generalize the heuristic design of weights into a weight model
which is optimized through adversarial optimization.
We summarize our contribution as follows:•We specify the challenge of inconsistency between users’
item-wise view and the recommender system’s list-wise view,
and formulate a more practical request-level MDP formula-
tion with item-wise reward.
•We propose an item-decomposed advantage actor-critic (Item-
A2C) framework and verify its superiority on multiple public
datasets and an online A/B test.
•We further illustrate and validate the correctness of the item-
level future impact decomposition by a re-weighting strategy
under the Item-A2C framework. And we theoretically show
that the decomposition still recovers the request-level A2C
in the objective functions.
•We also extend the heuristic approach into a learnable weight
model that can further improve the recommendation perfor-
mance, proving the existence of a wider feasible re-weighting
space.
In the remainder of this paper, we first illustrate the challenges
in RL-based recommendations and existing related solutions in
section 2. Then section 3 presents our solution framework with
theoretical analysis, and section 4 describes the experiments that
address our claims.
2 RELATED WORK
2.1 List-wise Recommendation
For recommendation scenarios that take a list of items as an action,
they can generally categorized in to one-step list recommendation
and sequential list-wise recommendation problems. The first study
mainly focuses on the items mutual influences [ 3,5,8,39], and
early approaches only discuss this issue in a top-k recommendation
scenario where the goal is to directly optimize a list-wise metric.
The latter research efforts provide solutions to a more complicated
sequential next-list recommendation scenario, where the list-wise
MDP is introduced [ 21,25,35]. As one of the most representative
works, SlateQ [ 21] is a reinforcement learning method for recom-
mender systems (RL4RS). It uses a DQN framework and proposes a
decomposition technique that learns to distribute the list’s Q-value
toward each individual item, and formulates the optimization prob-
lem of finding a slate with the maximum expected Q-value as a
fractional linear program. Though SlateQ is effective in modeling
the list-wise reward, its "single-choice" assumption mismatches our
setting where all items in the lists may have non-zero rewards. In
general, we distinguish our problem setting from existing works as
it emphasizes the existing item-wise feedback under the sequential
list-wise recommendation scenario.
2.2 Reinforcement Learning for
Recommendation
The RL-based recommendation system(RS) [ 1,37,47] adopts the
MDP framework and aims to optimize the cumulative reward that
reflects long-term user satisfaction. Although tabular-based [ 29]
methods can be used to optimize an evaluation table in a simple set-
ting, they are limited to a fixed set of state-action pairs. Therefore,
value-based methods [ 34,38,46,50] and policy gradient meth-
ods [ 9,10,18,19,22,40] have been proposed to learn to evaluate
and optimize the action policy based on the sampled long-term
5906Future Impact Decomposition in Request-level Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
Request TStart interactionRequest 1𝑠!𝑎!=𝑖!,#𝑖!,$…𝑖!,%&#𝑖!,%𝑠#𝑅(𝑠#,𝑎#):𝑟#,#𝑟#,$…𝑟#,%&#𝑟#,%............EndinteractionPolicy
EnvironmentRecommender SystemUser. . .. . .Request 2𝑠$𝑎$𝑅(𝑠$,𝑎$)𝑎#=𝑖#,#𝑖#,$…𝑖#,%&#𝑖#,%Request-level RewardItem-level Reward𝑅(𝑠!,𝑎!):𝑟!,#𝑟!,$…𝑟!,%&#𝑟!,%
Figure 1: Request-level MDP with observable item-level reward. A special case is K=1 that is equivalent to item-level MDP.
reward. The actor-critic [ 7,41,47,48] paradigm integrates these
two methods by simultaneously learning an action evaluator and
an action generator. Our method belongs to this later paradigm.
Despite its effectiveness, reinforcement learning also faces several
challenges when accommodating the recommender system, includ-
ing but not limited to the exploration of combinatorial state/actions
space [ 13,21,24,26], unstable user behavior [ 4,11], heterogeneous
user feedback [ 7,12], and multi-task learning [ 27,28,30]. Among
all existing works, we consider HAC [ 26] as closest to our work
since it also included item-wise components in an RL framework.
In comparison, HAC aims to represent a list in a hyper-embedding
space for end-to-end training, while our work focus on the item-
level long-term effect modeling. Note that we also emphasize the
importance of item-level future impact attribution which could
compliment the HAC model.
3 METHOD
In this section, we first explain the request-level MDP formulation
and illustrate our solution framework that consists of an actor-
critic learning backbone, an item-wise decomposition framework,
and two reweighting methods for item-wise future impact. As an
overview, Figure 1 provides the details of the problem formulation
and Figure 2 summarizes our learning framework.
3.1 Problem Formulation
In this work, we formulate the recommendation problem as a
request-level MDP with item-wise rewards and provide an overview
as Figure 1. Specifically, the system considers an item candidate
poolIof size𝑁, and we consider a recommendation policy that
is responsible for providing an item list of size 𝐾to the user upon
each user request. The user interacts/consumes the recommended
items in the list one by one (e.g., watching each short video) and
provides personalized feedback(e.g. click/like) that will later be used
to calculate the rewards. Then we can express the components of
the MDP as follows:
•S: the continuous representation space of user state. Note
that a user state encodes the user request which contains
information of the user’s static profile features (e.g. gender
and age) as well as recent interaction history.•A : the action space corresponds to the possible recommen-
dation lists. Without loss of generality, we consider the list
of fixed size 𝐾so the action space is A=I𝐾. And we can
define each list-wise action as 𝑎𝑡=[𝑖𝑡,1,...,𝑖𝑡,𝐾].
•𝑅(𝑠𝑡,𝑎𝑡): the immediate reward observed after taking action
𝑎𝑡∈ A on state𝑠𝑡∈ S. In our setting, we assume that
this list-wise reward is a linear aggregation of item-wise
reward, i.e. 𝑅(𝑠𝑡,𝑎𝑡)=Í
𝑖𝑡,𝑘∈𝑎𝑡𝑟(𝑠𝑡,𝑖𝑡,𝑘)where each item-
wise reward 𝑟𝑡,𝑘=𝑟(𝑠𝑡,𝑖𝑡,𝑘)is also observable.
•P : transition probability 𝑃(𝑠𝑡+1|𝑠𝑡,𝑎𝑡)reflects the proba-
bility reaching the next state 𝑠𝑡+1from the current state 𝑠𝑡
given certain action 𝑎𝑡. We assume that this is implicitly
modeled by the user request encoder as it takes user interac-
tion history as input. As we have discussed in section 1, the
state change can only be seen at this request level and the
item-wise state transition is not observable.
•𝜋:S→A : the recommendation policy that chooses a list
of items at each request.
And given this request-level MDP with item-wise feedback, the
sample records in the experience replay buffer take the form
(𝑠𝑡,𝑎𝑡,𝑟𝑡,1,...,𝑟𝑡,𝐾,𝑠𝑡+1,𝑑)where𝑑∈{0,1}represents whether
the session ends after taking action 𝑎𝑡.
In summary, we present three distinguishable assumptions in our
formulation: 1) the request-level state transition; 2) the observable
item-wise reward; and 3) list-wise reward as a linear aggregation
of item-wise reward. Our Goal is to learn a policy that maximizes
the user’s expected cumulative reward over the interactions:
E[R(𝑠𝑡,𝑎𝑡)]=E[∞∑︁
𝑖=0𝛾𝑖𝑅(𝑠𝑡+𝑖,𝑎𝑡+𝑖)] (1)
3.2 Request-level A2C
To better investigate the differences between item-wise and list-
wise optimization, we first develop an advantage actor-critic (A2C)
learning algorithm as the backbone that follows a standard RL
framework [ 31]. We denote this framework as “request-level A2C”.
It consists of an actor 𝜋𝜃(𝑎𝑡|𝑠𝑡)that serves as the recommendation
policy, and a critic 𝑉(𝑠𝑡)that aims to model the expected long-
term cumulative reward of a given state (as in Eq. (1)) and guide the
optimization of the actor.
5907KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaobei Wang et al.
Figure 2: The ItemA2C learning framework, where ⊙repesents the score function items and ⊕represents summation.
During optimization, the training samples come from an experi-
ence replay buffer and each sample takes the form of (𝑠𝑡,𝑎𝑡,𝑅(𝑠𝑡,𝑎𝑡)
,𝑠𝑡+1,𝑑). The optimization of critic networks follows a time-difference
(TD) minimization objective:
Lcritic=
Ψ(𝑠𝑡,𝑎𝑡)−𝑉(𝑠𝑡)2
Ψ(𝑠𝑡,𝑎𝑡)=𝑅(𝑠𝑡,𝑎𝑡)+𝛾(1−𝑑)𝑉(𝑠𝑡+1)(2)
where Ψrepresents the target value for the current state’s critic
estimation and 𝑉(𝑠𝑡+1)is the target critic network that is widely
used in RL to regulate the optimization of 𝑉(𝑠𝑡)and stabilize the
training. When the critic is well-trained, it can guide the learning
of the actor through the following advantage-boosting objective:
Lactor=−𝐴(𝑠𝑡,𝑎𝑡)log𝜋𝜃(𝑎𝑡|𝑠𝑡)
𝐴(𝑠𝑡,𝑎𝑡)=𝑅(𝑠𝑡,𝑎𝑡)+𝛾(1−𝑑)𝑉(𝑠𝑡+1)−𝑉(𝑠𝑡)(3)
where the advantage estimates how much a certain action 𝑎𝑡re-
sults in a better state than the average. Note that we can also re-
gard this advantage score as the difference between the target
state-action value and the expected value of the current state, i.e.
𝐴(𝑠𝑡,𝑎𝑡)=Ψ(𝑠𝑡,𝑎𝑡)−𝑉(𝑠𝑡). This indicates that an action that gen-
erates a higher-than-average target value will tend to increase its
recommendation probability.
3.3 Item-wise Decomposition of A2C
The aforementioned request-level A2C framework is a pure list-
wise approach that does not fully exploit the item-wise reward,
so we introduce our decomposition method under the setting of
section 3.1. Specifically, we take advantage of the linear relationship
between the item-wise reward and list-wise reward (i.e. 𝑅(𝑠𝑡,𝑎𝑡)=Í
𝑖𝑡,𝑘∈𝑎𝑡𝑟𝑡,𝑘) and derive an item-wise target function credit assign-
ment for each item 𝑖𝑡,𝑘∈𝑎𝑡:
Ψ(𝑠𝑡,𝑖𝑡,𝑘)=𝑟𝑡,𝑘+1
𝐾𝛾(1−𝑑)𝑉(𝑠𝑡+1) (4)
where the same state is shared for all items in the same list due to
the unavailable item-wise state transition under the request-level
MDP. In this decomposition, there is no need to break the list-wise
state transition, and each state value 𝑉(𝑠𝑡)is reused for K times
when calculating the target for items in the list.
3.3.1 Weighting Item-wise Future Impact. In general, we believe
that the total effect of an item 𝑖𝑡,𝑘in the list𝑎𝑡includes both theshort-term and long-term perspectives. While the immediate re-
ward𝑟𝑖,𝑘expresses the short-term effect, the items’ share/split of
the future impact 𝑉(𝑠𝑡+1)reflects their long-term effect. The di-
rect decomposition in Eq. (4)can distinguish items through their
immediate reward but not the future impact because of the evenly
distributed𝑉(𝑠𝑡+1). Following this limitation, a natural question is
whether we can break the uniform split. And if we can, how should
we address the future influence?
Intuitively, one possible assumption is that the items with better
immediate user feedback are expected to impress the user more
positively and thus improve the user’s future behaviors, as we have
discussed with examples in section 1. To verify this assumption,
we propose a future re-weighting strategy that assigns 𝑉(𝑠𝑡)in the
target function with a weight that is positively related to the item’s
immediate reward:
Ψ𝑤(𝑠𝑡,𝑖𝑡,𝑘)=𝑟𝑡,𝑘+𝑤𝑡,𝑘(𝛾(1−𝑑)𝑉(𝑠𝑡+1))
𝑤𝑡,𝑘=𝛼𝑟𝑡,𝑘+(1−𝛼)
𝛼𝑅(𝑠𝑡,𝑎𝑡)+(1−𝛼)𝐾(5)
where𝛼is the hyperparameter that controls the balance between
the pure re-weighting strategy and pure equal-weight strategy.
As two special cases in this design: when setting 𝛼=0, these
weights recover the equal-weight strategy Eq. (4); when setting
𝛼=1.0, these weights become a pure reward-based strategy 𝑤𝑡,𝑘=
𝑟𝑡,𝑘/𝑅(𝑠𝑡,𝑎𝑡). Note that in practice, the weight should always be
made positive for consistent learning behavior, but we are NOT
restricting𝛼to[0,1], since setting 𝛼<0means that immediate
reward is assumed negatively related to the future impact and
setting𝛼>1indicates a negative bias on the immediate reward’s
relation to the future impact. In section 4, we will illustrate the
effect of𝛼in this design with empirical results.
For actor training, rather than 𝜋𝜃(𝑎𝑡|𝑠𝑡), we can apply the item-
wise optimization on 𝜋𝜃(𝑖𝑡,𝑘|𝑠𝑡)with the modified actor loss:
Lactor(𝑖𝑡,𝑘)=−𝐴(𝑠𝑡,𝑖𝑡,𝑘,𝑤𝑡,𝑘)log𝜋𝜃(𝑖𝑡,𝑘|𝑠𝑡)
𝐴(𝑠𝑡,𝑖𝑡,𝑘,𝑤𝑡,𝑘)=Ψ𝑤(𝑠𝑡,𝑖𝑡,𝑘)−𝑉(𝑠𝑡)
𝐾(6)
where the advantage estimates how much a certain item affects the
cumulative reward compared to average items.
3.3.2 Model-based Future Impact Re-weighting. To further verify
the idea of future impact re-weighting, explore the reweighting
space, and analyze the correctness of the reward-based re-weighting
5908Future Impact Decomposition in Request-level Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
Algorithm 1 ItemA2C Learning Framework
1:procedure Online Training
2: Initialize all trainable parameters in the actor and critics.
3: Initialize the weight model if using model-based reweight-
ing.
4: while True do
5: Obtain mini-batch sample from the replay buffer.
6: Mini-batch gradient descent for 𝑉(𝑠𝑡)with Eq.(2).
7: Soft update of target network 𝑉(𝑠𝑡+1).
8: Mini-batch gradient descent for 𝑤(·)with Eq. (7)if using
model-based reweighting.
9: Mini-batch gradient descent for 𝜋with Eq. (6)for𝑖𝑡,𝑘∈
𝑎𝑡.
10: end while
11:end procedure
strategy in the previous section, we also design a model-based solu-
tion that can automatically learn to reweigh the future impact de-
composition. Specifically, we generalize the original reward-based
weighting strategy into a neural model 𝑤𝑡,𝑘=𝑤(𝑖𝑡,𝑘,𝑠𝑡,𝑠𝑡+1,𝑟𝑡,𝑘)
which takes not only the item’s immediate reward but also the
future state and the corresponding item features as input. These
extra inputs are included because both the items feature and the
next state𝑠𝑡+1are relevant to the item’s future impact. The model’s
output corresponds to the respective weights that replace 𝑤𝑡,𝑘in
Eq.5 and we ensure that the sum of weights equals one by applying
a softmax function across the items in the same list.
One challenge for including this weight model is the lack of
clear learning signal. In practice, we design an adversarial learning
framework and find it effective with the following objective:
L𝑤𝑒𝑖𝑔ℎ𝑡 =−∑︁
𝑖𝑡,𝑘∈𝑎𝑡Lactor(𝑖𝑡,𝑘)
=∑︁
𝑖𝑡,𝑘∈𝑎𝑡𝐴(𝑠𝑡,𝑖𝑡,𝑘,𝑤𝑡,𝑘)log𝜋𝜃(𝑖𝑡,𝑘|𝑠𝑡)(7)
which is also an item-level optimization. This objective in Eq. (7)is
essentially the opposite of the actor loss Eq. (6). Intuitively, this will
make the actor “harder” and force the actor to learn from “harder”
samples. Yet, we notice that the actual learning behaviors of the
weight model and the actors could be far more complicated as we
will describe in section 4. Note that there might also exist other
valid solutions like hyperparameter searching [ 15] or auto machine
learning approaches [ 14], we consider them as complementary to
our research and focus on proving the existence of better weighting
models than the reward-based strategy.
3.4 Overall Item-level Learning Framework
In general, the overall learning framework involves a TD learning
for critics as Eq. (2), an advantage-based actor learning (with or
without reweighting) as Eq. (6), and an adversarial learning for the
weight model as Eq. (7). Algorithm 1 summarizes the details of
the step-wise training and Figure 2 summarizes the learning of
the decomposed A2C with model-based reweighting. During the
weight model optimization, the actor network and critic networksare fixed. Similarly, the weight model is fixed during the critic’s TD
learning and the actor learning.
Mathematically, the weights of the future impact have propertyÍ
𝑖𝑡,𝑘∈𝑎𝑡𝑤𝑡,𝑘=1, which means the item-wise target function can
reconstruct the original list-wise target:
∑︁
𝑖𝑡,𝑘∈𝑎𝑡Ψ𝑤(𝑠𝑡,𝑖𝑡,𝑘)=∑︁
𝑖𝑡,𝑘∈𝑎𝑡𝑟𝑡,𝑘+∑︁
𝑖𝑡,𝑘∈𝑎𝑡𝑤𝑡,𝑘𝛾(1−𝑑)𝑉(𝑠𝑡+1)
=𝑅(𝑠𝑡,𝑎𝑡)+𝛾(1−𝑑)𝑉(𝑠𝑡+1)=Ψ(𝑠𝑡,𝑎𝑡)(8)
which means that we can safely use the value functions 𝑉(𝑠𝑡)
and𝑉(𝑠𝑡+1)when guiding the actor learning, since the TD mini-
mization is the same as Eq. (2)which only uses the request-level
target function Ψ(𝑠𝑡,𝑎𝑡)and ignores the details in the item-wise
target functions Ψ(𝑠𝑡,𝑖𝑡,𝑘). Though it is possible to apply a similar
re-weighting strategy in the TD learning, we believe keeping the
request-level critic learning is still a better choice for the items’
mutual interactions within the same list can only be modeled in the
request-level. In other words, we only apply the item decomposition
and reweighting during item-wise actor learning.
4 EXPERIMENTS
To verify the effectiveness of the item-level decomposition under
request-level MDP and our proposed reweighting methods, we first
empirically study their performance and learning behavior in an
online simulator of two public datasets as pre-online evaluation,
then further validate our solution in online A/B test.
4.1 Offline Experiments with Simulator
4.1.1 Datasets and Online Simulator. We use two publicly avail-
able datasets: ML1M[ 20] and KuaiRand1K[ 17]. The ML1M dataset
includes users’ rating records of movies, while KuaiRand1K is a
dataset that contains user interaction records with short videos. We
preprocess both datasets by a similar process in [ 45], in which each
record is organized chronologically and users/items with less than
10 interactions are removed (10-core filtering).
Our simulator is constructed based on the session-based simula-
tor as in KuaiSim [ 45] for both datasets. We train a user interaction
model to estimate the probability of a user’s click based on their dy-
namic interaction history and static profile features. During online
RL the user simulator will sample immediate feedback according to
this model and serve as the interactive environment. For simplicity,
we directly consider the click-or-not signal as the reward, where
a click results in a reward of 1.0 and a missing click leads to a re-
ward of -0.2 following the same setting in [ 26]. Though the reward
design is not the focus of this work, one can easily generalize our
solution for other reward settings as we will show in section 4.2.
For all environments, we limit the maximum episode depths to 20
and set the list size to 𝐾=6. When a user’s episode ends, a new
user is randomly picked to fill in the blank. And each episode step
simultaneously runs the simulator for a batch of 64 users.
4.1.2 Evaluation Protocol. With the online simulator ready, we can
apply pseudo-online training for RL models, and evaluate its perfor-
mance in the last 100 episode steps (minimum 320 user episodes).
We observed that the majority of RL-based techniques achieved con-
vergence and stabilization within 30,000 iterations. For long-term
evaluation metrics, we report the total reward (without discount)
5909KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaobei Wang et al.
Table 1: The full outputs for main results. The best performances in bold and second best in Underline.
type ModelML1M
Total Reward Depth
Average Maximum Minimum Average Maximum Minimum
request-levelDDPG 12.97 ±(2.95) 19.47±(0.76) 2.52±(1.37) 13.72±(2.62) 19.52±(0.69) 4.49±(1.21)
A2C 16.88 ±(0.58) 19.94±(0.08) 7.37±(0.97) 17.32±(0.50) 19.95±(0.07) 9.29±(0.79)
HAC 17.53 ±(0.13) 19.97±(0.03) 7.78±(1.26) 17.90±(0.12) 19.97±(0.03) 9.67±(1.10)
Supervision 12.57 ±(2.14) 19.72±(0.37) 1.18±(0.79) 13.71±(1.81) 19.76±(0.32) 4.14±(0.62)
item-levelSlateQ 4.62 ±(0.19) 12.04±(0.41) 0.06±(0.07) 6.96±(0.16) 13.19±(0.35) 3.22±(0.04)
itemA2C(𝛼=0) 17.58±(0.62) 19.92±(0.03) 8.58±(1.95) 17.94±(0.53) 19.93±(0.02) 10.33±(1.63)
itemA2C-W( 𝛼=0.5) 17.57±(0.54) 19.96±(0.05) 8.55±(0.83) 17.93±(0.46) 19.96±(0.04) 10.31±(0.71)
itemA2C-W( 𝛼=1) 17.80±(0.73) 19.97±(0.06) 8.83±(1.30) 18.12±(0.63) 19.97±(0.05) 10.81±(1.12)
itemA2C-M 17.94±( 0.47) 20.00±(0.02) 9.79±(1.22) 18.24±(0.40) 20.00±(0.02) 11.33±(1.02)
type ModelKuaiRand
Total Reward Depth
Average Maximum Minimum Average Maximum Minimum
request-levelDDPG 11.71 ±(0.98) 19.43±( 0.35) 0.40±( 0.42) 12.93±(0.83) 19.50±( 0.32) 3.52±( 0.34)
A2C 10.01±( 0.86) 19.47±(0.28) -0.24±( 0.10) 11.56±( 0.72) 19.54±( 0.25) 3.04±( 0.09)
HAC 12.65±( 0.26) 19.58±(0.24) 0.82±( 0.43) 13.72±( 0.22) 19.63±(0.22) 3.86±(0.34)
Supervision 9.78±( 3.15) 18.76±(1.34) 0.16±( 0.63) 11.32±( 2.66) 18.92±( 1.18) 3.35±( 0.51)
item-levelSlateQ 3.39 ±(0.20) 12.65±(0.23) -0.3±(0.05) 5.99±(0.17) 13.67±(0.19) 3.00±(0.04)
itemA2C(𝛼=0) 13.45±(0.08) 19.58±(0.26) 1.01±(0.46) 14.41±(0.07) 19.64±(0.22) 4.01±(0.37)
itemA2C-W( 𝛼=0.5) 14.77±(0.41) 19.69±(0.20) 2.26±(0.72) 15.52±(0.35) 19.73±(0.18) 5.03±(0.58)
itemA2C-W( 𝛼=1) 15.48±(0.61) 19.65±(0.24) 3.58±(0.92) 16.13±(0.53) 19.69±(0.21) 6.11±(0.77)
itemA2C-M 16.03±(0.53) 19.60±(0.24) 5.21±(0.68) 16.59±(0.45) 19.64±(0.21) 7.48±(0.55)
anddepth of user sessions/episodes. Higher values for these met-
rics indicate superior performance.
4.1.3 Baselines. To better compare the proposed methods with
other feasible solutions under the request-level MDP, we imple-
mented the following baselines:
•SlateQ [ 21]: a DQN method that learns to estimate the Q-
value of a state-item pair, it also considers the item-decomposition
under the list-wise scenario.
•DDPG [ 23]: a request-level Deep DPG method that uses a
vectorized hyper-action to represent a whole list for both
the actors and critics, so that end-to-end training could be
applied for actor training.
•Supervision: the supervised learning method which included
a transformer-based history and profile encoder like that
in SASRec and a dot product item selector like HAC that
directly optimizes the item-wise scoring function through
binary cross entropy loss on item-wise labels.
•HAC [ 26]: The state-of-the-art request-level RL method which
is also an actor-critic framework that uses a vectorized hyper
action to represent each item list. The optimization process
includes TD learning for a request-level critic, Q value maxi-
mization for a request-level actor, action space regularization
and item-wise supervision.
•A2C(section 3.2): The request-level Actor-Critic method.
(a) Performance in ML1M (b) Performance in KuaiRand
Figure 3: Learning curves of all methods.
We present the details of the specifications in Appendix C.
4.1.4 Model Alternatives. We denote our item-decomposition frame-
work with equal weights as itemA2C, the reward-based re-weighting
strategy as itemA2C-W, and the extended framework that uses
weight model with adversarial learning is noted as itemA2C-
M(model). For reproduction of our empirical study, we provide
implementation and training details as well as the hyperparameters
of best results in our grid search space in our released source code1.
4.1.5 Main Results. For each experiment across all models, we
run online training and evaluation for five rounds with different
random seeds and report the average results in Table 1. We can
1https://github.com/wangxiaobei565/ItemDecomposition
5910Future Impact Decomposition in Request-level Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Performance for different list size.
list sizeML1M
HAC itemA2C( 𝛼=0) itemA2C-W( 𝛼=0.5) ItemA2C-M
Reward Depth Reward Depth Reward Depth Reward Depth
1 19.30±( 0.28) 19.61±( 0.15) 19.10±( 0.19) 19.51±( 0.12) 19.10±( 0.19) 19.51±( 0.12) 19.10±( 0.19) 19.51±( 0.12)
2 18.66±( 0.29) 19.07±( 0.21) 18.69±( 0.35) 19.09±( 0.25) 18.84±( 0.18) 19.20±( 0.14) 18.85±( 0.46) 19.08±( 0.32)
4 18.14±( 0.51) 18.51±( 0.41) 18.17±( 0.41) 18.54±( 0.34) 18.30±( 0.54) 18.63±( 0.44) 18.39±( 0.45) 18.72±( 0.35)
8 17.11±( 0.70) 17.45±( 0.61) 17.19±( 0.47) 17.51±( 0.42) 17.21±( 0.83) 17.54±( 0.73) 17.33±( 0.48) 17.64±( 0.42)
16 15.65±( 0.74) 15.96±( 0.69) 16.12±( 0.96) 16.39±( 0.91) 16.23±( 0.91) 16.43±( 0.90) 16.27±( 0.99) 16.51±( 0.93)
32 14.72±( 0.82) 14.93±( 0.78) 14.79±( 1.02) 15.00±( 0.98) 14.58±( 0.73) 14.68±( 0.67) 14.85±( 1.13) 15.05±( 1.09)
list sizeKuaiRand
HAC itemA2C( 𝛼=0) itemA2C-W( 𝛼=0.5) ItemA2C-M
Reward Depth Reward Depth Reward Depth Reward Depth
1 16.03±( 0.39) 17.98±( 0.20) 16.93±( 0.40) 18.42±( 0.23) 16.93±( 0.40) 18.42±( 0.23) 16.93±( 0.40) 18.42±( 0.23)
2 15.01±( 0.29) 16.59±( 0.19) 15.50±( 0.29) 16.92±( 0.20) 15.97±( 0.51) 16.53±( 0.34) 16.52±( 0.27) 17.73±( 0.19)
4 13.37±( 0.33) 14.69±( 0.25) 14.07±( 0.30) 15.26±( 0.24) 15.03±( 0.50) 15.58±( 0.40) 16.29±( 0.32) 17.02±( 0.26)
8 12.14±( 0.71) 13.03±( 0.63) 12.67±( 0.39) 13.50±( 0.35) 14.04±( 0.50) 14.43±( 0.45) 15.05±( 0.62) 15.59±( 0.55)
16 11.51±( 0.35) 11.94±( 0.30) 11.28±( 0.57) 11.93±( 0.53) 12.74±( 0.26) 13.37±( 0.24) 13.73±( 0.77) 14.18±( 0.71)
32 10.37±( 0.22) 10.71±( 0.19) 9.71±( 0.64) 10.11±( 0.61) 11.29±( 0.36) 11.63±( 0.33) 11.39±( 0.41) 11.71±( 0.39)
Figure 4: Training curves of all methods on KuaiRand. We
also show the loss curves about critic and actor.
see that most RL-based methods (DDPG, A2C, HAC, and ItemA2C)
can improve the recommendation result over supervised learning,
indicating the superiority of optimizing the cumulative reward over
immediate reward. The SlateQ method applies item decomposition
but is not well suited for our problem setting since it decomposes
the request-level total reward without using the item-wise reward,
which results in sub-optimal performance. Among the baselines,
the best request-level method is HAC in terms of the total reward
and depth metrics. Including the itemwise supervision might be
the reason why HAC surpasses other baselines, but its lack of item-
wise decomposition of future impact is an inferior design which is
reflected in the empirical results compared to our itemA2C alter-
natives. In comparison, ItemA2C-M with weight model achievesthe best overall performance and significantly (with 𝑝<0.05) im-
proves the reward by 27% and the depth by 20% in KuaiRand1K
and marginally (though not statistically significant) improves the
reward by 2.3% and depths by 1.8% in ML1M. Currently, we be-
lieve the difference between the improvement made on ML1M and
KuaiRand is related to the consistency of user behaviors (more
consistent interactions in the short video platform than that in the
movie) and the number of candidate items (3706 items in ML1M and
11643 items in KuaiRand). These data characteristics make the item
selection process easier for the baseline models on ML1M dataset
to approach the optimal performance.
Additionally, compared to the request-level A2C baseline, all of
our ItemA2C alternatives demonstrated significant improvement
in both user environments. Besides, All item-level RL methods
have relatively the same level of reward variance and appear to
be smaller than request-level methods, which can also be seen in
their learning processes as shown in Figure 4. This verifies the
effectiveness of the item-level decomposition and its consistent
superiority and learning stability over its request-level counter-
part. In comparison to equal-weight future decomposition, both the
strategy-based ItemA2C-W and model-based ItemA2C-M resulted
in even better recommendation performance. Moreover, increasing
𝛼in the strategy-based reweighting alternative amplifies future im-
pact and further improves the recommendation result. This proves
the effectiveness of the future impact decomposition in Eq.(5).
4.1.6 Strategic vs. Model-based Reweighting. As shown in Table
1, ItemA2C-M achieves better performance than ItemA2C-W in
both long-term reward and reward variance. This indicates the exis-
tence of other feasible solutions to the reweighting strategy for the
item-level future impact decomposition. To better understand the
similarity and differences between the two methods, in Figure 5 we
5911KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaobei Wang et al.
Figure 5: The Trends of Cosine Similarity and Pearson Cor-
relation Coefficient between weight model and heuristic re-
weighting during training.
plot the cosine similarity and the Pearson correlation coefficient be-
tween the weights of Eq. (5)in ItemA2C-W and the learned weights
in ItemA2C-M. Our results reveal that the weights exhibit high
cosine similarity and positive correlation between reward-based
reweighting strategy and model-based extension in both datasets.
This finding proves the validity of the design of Eq. (5)assuming
the generality of the model-based solution. However, both met-
rics decrease at the beginning, which proves that the adversarial
learning in model-based reweighting method may gradually find
different feasible weighting strategies. We also find inconsistent
behaviours in that ItemA2C-M becomes increasingly dissimilar
in the cosine metric and relatively stable in Pearson correlation
in ML1M, but observes decreasing Pearson correlation and stable
cosine similarity in KuaiRand. Note that the Pearson correlation
calculates item-wise correlation separately but cosine similarity
calculates the relation between the entire weight vector of size 𝐾.
This means that the relative weights among items in ItemA2C-M
tend to be more aligned with Eq. (5)even though the two methods
absolution values of weights diverge in KuaiRand. In contrast, the
relative weights in ItemA2C-M slowly diverge from Eq. (5)in ML1M.
In general, this may indicate that the optimal reweighting strategy
of different recommendation services may not take the same form.
4.1.7 Ablation. Influence of 𝛼:Recall that𝛼controls the balance
between equal-weight strategy and full re-weighting strategy in
ItemA2C-W. In this study, we search the parameter space of 𝛼
in [0.0, 2.0] and include a negative value 𝛼=0.5to represent a
possible negative effect of reward-based re-weighting. Note that
𝛼>1.0represents an “over-weighting” strategy where the equal-
weight strategy contributes negatively to the value. We present
the resulting total session reward under different 𝛼in Figure 7. In
both datasets, the best setting is observed between [1.0,1.5], which
further proves the effectiveness of the future impact reweighting.
Besides, the sensitivity of 𝛼is larger in KuaiRand than ML1M since
the total reward in KuaiRand varies on a larger scale compared to
its variance. This is also related to the easier learning environment
in the ML1M data than KuaiRand as discussed in section 4.1.5.Effect of list size: The main experiment in our paper set the
list size to𝐾=6to mirror the real scenario presented in the online
system. but it is reasonable to believe that the choice of list size
also affects the recommendation result. We conducted experiments
with𝐾∈{1,2,4,6,8,16,32}which changes both the MDP of the
RL model but also the user environment. We test both request-
level A2C and item-level ItemA2C, ItemA2C-W, ItemA2C-M and
run each experiment for five rounds with different random seeds
and report the average values in Table 2. The performance of all
methods gradually deteriorated as the list size increases. This is a
direct indicator of the existence of the inconsistent view between
request-level system MDP and item-level user MDP, and the most
accurate view aligns with the item-wise user view. The proposed
ItemA2C-M achieves the best result across different list size set-
tings. In our empirical study on both datasets, ItemA2C consistently
achieves better performances than the best baseline HAC and all
other methods for 𝐾>2. Moreover, the superiority of ItemA2C-M
over ItemA2C-W is also verified for different settings of 𝐾.
4.2 Live Experiments
4.2.1 Experimental Setup. To further verify our proposed method
in a real-world environment, we conduct A/B testing on an indus-
trial video recommendation platform. The platform serves over 100+
million users every day and Figure 6 summarizes the recommenda-
tion workflow. Specifically, we are considering the recommendation
task that recommends 𝐾items for each user request, which corre-
sponds to the refined ranking stage where the candidate set size
is around 500 (filtered by previous retrieval and ranking stages)
and the output list size is 𝐾=6. Different from the single behavior
settings in our offline experiments, the user behaviors in the online
system are more complicated and we include watch time, like, fol-
low, collect, and comment as targets. The corresponding item-wise
reward design is a linear combination:
𝑟(𝑠𝑡,𝑖𝑡,𝑘)=𝑤1∗I[quantile_watch_time]+ 𝑤2∗I[like]
+𝑤3∗I[follow]+𝑤4∗I[collect]+𝑤5∗I[comment](9)
where the weight for each signal is set to a positive empirical value
in[0.1,1.0]and we use quantization for the watch time signal so
that all feedback signals are in discrete space.
We randomly segregate the total traffic for the experiments, and
10% of traffic for each of the selected methods including a request-
level method (similar to A2C), ItemA2C, and ItemA2C-W. For all
three methods, the actor uses a 4-layer MLP as the network structure
and the input encoding is a 400-dimension vector that encodes the
user request and the candidate items. The output ranking scores
of items correspond to the policy output 𝜋𝜃(𝑖|𝑠𝑡). To give a fair
comparison, we also include our baseline model (with 20% traffic
hold-out) that adopts learning-to-rank supervision at the item level.
4.2.2 Results Analysis. We keep each experiment online for one
week and summarize the average results in Table 3. Note that the
“watch time” metric expresses the average length the user watches
a video without discretization, and all other metrics express the
rate of a certain behavior on recommended videos. Considering
the overall performance over all metrics, the request-level A2C
improves the (non-RL) baseline model on most metrics except for
5912Future Impact Decomposition in Request-level Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Online performances of item-level A2C and all results are statistically significant.
Method Watch Time Like Follow Collect Comment
request-level A2C v.s.baseline -0.073% +0.713% +0.205% +0.736% +0.328%
ItemA2C v.s.request-level A2C +0.129% +1.103% +0.300% +0.963% +0.221%
ItemA2C-W( 𝛼=1)v.s.ItemA2C -0.013% +0.451% +0.636% +0.616% +0.258%
User
Request
Millions 
of 
items
Retrieval
Thousands 
of 
items
Immediate 
Feedback
Other 
Label 
Generators
New 
Request 
for 
t+1
Candidate 
items
Initial 
Ranking
Refined 
Ranking
Hundreds 
of 
items
Rerank
K 
items
K 
items
Users
Reward 
Calculator
Deployment 
of 
ItemA2C
Figure 6: The entire workflow consists of a relevant video
retrieval stage, an efficient initial ranking stage, a refined
ranking stage, and a final reranking . The ItemA2C is de-
ployed in the refined ranking stage.
a slight decrease in watch time, which verifies the effectiveness
of RL. The ItemA2C can significantly improve the performance
in all metrics over request-level RL, which validates our claim on
the superiority of item-level optimization. The ItemA2C-W further
improves the metrics except an (statistically) insignificant decrease
in watch time, which proves the effectiveness of future impact
decomposition and that our proposed method can generalize to
more complicated reward designs. Additionally, we also observe
that ItemA2C methods achieve a minor increase of daily active
users (DAU) by 0.028% and significant improvement in the user
retention (indicating the probability of a user’s return in the next
week) by 0.016%. This means that the selected metrics are positively
correlated with the user’s activity level.
5 CONCLUSION AND DISCUSSION
In this paper, we present the challenge of inconsistent viewpoints
between request-level recommendation policy and item-level user
behaviors. To address this challenge, we formulated a practical
Figure 7: Total reward under different 𝛼and the results in-
clude mean, median, quartiles, and min-max.
list-wise MDP formulation incorporating item-wise reward and
proposed an item-wise decomposition method under the actor-
critic learning framework. Based on the proposed method, we show
that one should not only utilize the item-wise reward for the opti-
mization of each immediate user feedback but also distinguish each
item’s future impact through the value function decomposition. The
proposed method introduces no extra learnable parameters than
A2C and is well-suited for item-wise parallel training. We addition-
ally proposed a model-based method with adversarial learning that
automatically learns to decompose the future impact and proves
that the reward-based strategy is reasonable but could be further
improved. Note that the model-based re-weighting solution may
introduce extra modeling and training cost but no extra computa-
tion during inference. In terms of the data collection, one has to
construct the data stream for the request-level MDP with item-wise
feedback to implement the solution. Fortunately, this data is usu-
ally easy to collect in most industrial recommender systems. As an
extension to our model-based approach, future research on new
ways to represent the future impact of each item may also give in-
sights into user behavior modeling, improving the recommendation
systems and user satisfaction.
Additionally, our design does not restrict the reward settings
to click-based reward design as in the offline experiment or linear
combination of multi-response as in the online A/B test, future re-
search on more complicated non-linear reward design (e.g. intra-list
diversity) may further improve recommender performance. During
deployment, our approach is well-suited for the last ranking stage
or the reranking stage where user feedback are provided for all the
output items, and some extra efforts might needed to accommodate
previous ranking stages, since the candidate pool size is much larger
and the output size are not necessarily K. In the RL’s viewpoint, our
proposed method naturally extends the A2C framework, and we
believe it is also worthwhile to investigate valid designs for future
impact decomposition under DDPG or HAC.
5913KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaobei Wang et al.
REFERENCES
[1]M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2021. Reinforcement learning
based recommender systems: A survey. ACM Computing Surveys (CSUR) (2021).
[2]M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2022. Reinforcement learning
based recommender systems: A survey. Comput. Surveys 55, 7 (2022), 1–38.
[3]Qingyao Ai, Keping Bi, Jiafeng Guo, and W Bruce Croft. 2018. Learning a deep
listwise context model for ranking refinement. In The 41st international ACM
SIGIR conference on research & development in information retrieval. 135–144.
[4]Xueying Bai, Jian Guan, and Hongning Wang. 2019. A model-based reinforcement
learning with adversarial training for online recommendation. Advances in Neural
Information Processing Systems 32 (2019).
[5]Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An
overview. Learning 11, 23-581 (2010), 81.
[6]Qingpeng Cai, Shuchang Liu, Xueliang Wang, Tianyou Zuo, Wentao Xie, Bin
Yang, Dong Zheng, Peng Jiang, and Kun Gai. 2023. Reinforcing User Retention
in a Billion Scale Short Video Recommender System. In Companion Proceedings
of the ACM Web Conference 2023. 421–426.
[7]Qingpeng Cai, Zhenghai Xue, Chi Zhang, Wanqi Xue, Shuchang Liu, Ruohan
Zhan, Xueliang Wang, Tianyou Zuo, Wentao Xie, Dong Zheng, et al .2023. Two-
Stage Constrained Actor-Critic for Short Video Recommendation. In Proceedings
of the ACM Web Conference 2023. 865–875.
[8]Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning
to rank: from pairwise approach to listwise approach. In Proceedings of the 24th
international conference on Machine learning. 129–136.
[9]Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang,
Yuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation with
tree-structured policy gradient. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 33. 3312–3320.
[10] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and
Ed H Chi. 2019. Top-k off-policy correction for a REINFORCE recommender
system. In Proceedings of the Twelfth ACM International Conference on Web Search
and Data Mining. 456–464.
[11] Minmin Chen, Bo Chang, Can Xu, and Ed H Chi. 2021. User response models
to improve a reinforce recommender system. In Proceedings of the 14th ACM
International Conference on Web Search and Data Mining. 121–129.
[12] Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei Xu, and Liming Zhu.
2021. Generative inverse deep reinforcement learning for online recommenda-
tion. In Proceedings of the 30th ACM International Conference on Information &
Knowledge Management. 201–210.
[13] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy
Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and
Ben Coppin. 2015. Deep reinforcement learning in large discrete action spaces.
arXiv preprint arXiv:1512.07679 (2015).
[14] Tongtong Fang, Nan Lu, Gang Niu, and Masashi Sugiyama. 2020. Rethinking
importance weighting for deep learning under distribution shift. Advances in
neural information processing systems 33 (2020), 11996–12007.
[15] Matthias Feurer and Frank Hutter. 2019. Hyperparameter optimization. Auto-
mated machine learning: Methods, systems, challenges (2019), 3–33.
[16] Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang,
Shiqi Wang, Zhong Zhang, and Xiangnan He. 2023. Alleviating Matthew Effect of
Offline Reinforcement Learning in Interactive Recommendation. In Proceedings
of the 46th International ACM SIGIR Conference on Research and Development in
Information Retrieval (Taipei, Taiwan) (SIGIR ’23). 11 pages. https://doi.org/10.
1145/3539618.3591636
[17] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei,
Peng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Rec-
ommendation Dataset with Randomly Exposed Videos. In Proceedings of the
31st ACM International Conference on Information and Knowledge Management
(Atlanta, GA, USA) (CIKM ’22). 5 pages. https://doi.org/10.1145/3511808.3557624
[18] Yingqiang Ge, Shuchang Liu, Ruoyuan Gao, Yikun Xian, Yunqi Li, Xiangyu Zhao,
Changhua Pei, Fei Sun, Junfeng Ge, Wenwu Ou, and Yongfeng Zhang. 2021.
Towards Long-term Fairness in Recommendation. In Proceedings of the 14th ACM
International Conference on Web Search and Data Mining. 445–453.
[19] Yingqiang Ge, Xiaoting Zhao, Lucia Yu, Saurabh Paul, Diane Hu, Chu-Cheng
Hsieh, and Yongfeng Zhang. 2022. Toward Pareto efficient fairness-utility trade-
off in recommendation through reinforcement learning. In Proceedings of the
fifteenth ACM international conference on web search and data mining. 316–324.
[20] F Maxwell Harper. 2015. The movielens datasets: History and context. Acm
transactions on interactive intelligent systems (tiis) 5 4 (2015) 1–19. F Maxwell
Harper and Joseph A Konstan. 2015. The movielens datasets: History and context.
Acm transactions on interactive intelligent systems (tiis) 5 4 (2015) 1–19.
[21] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu,
Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A Tractable
Decomposition for Reinforcement Learning with Recommendation Sets. In Pro-
ceedings of the Twenty-eighth International Joint Conference on Artificial Intelli-
gence (IJCAI-19). Macau, China, 2592–2599. See arXiv:1905.12767 for a related
and expanded paper (with additional material and authors)..[22] Zelong Li, Jianchao Ji, Yingqiang Ge, and Yongfeng Zhang. 2022. AutoLossGen:
Automatic Loss Function Generation for Recommender Systems. In Proceedings
of the 45th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 1304–1315.
[23] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control
with deep reinforcement learning. In ICLR 2016, San Juan, Puerto Rico, May 2-
4, 2016, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.).
http://arxiv.org/abs/1509.02971
[24] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen,
Huifeng Guo, Yuzhou Zhang, and Xiuqiang He. 2020. State representation mod-
eling for deep reinforcement learning based recommendation. Knowledge-Based
Systems 205 (2020), 106170.
[25] Shuchang Liu, Qingpeng Cai, Zhankui He, Bowen Sun, Julian McAuley, Dong
Zheng, Peng Jiang, and Kun Gai. 2023. Generative Flow Network for List-
wise Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (Long Beach, CA, USA) (KDD ’23) . As-
sociation for Computing Machinery, New York, NY, USA, 1524–1534. https:
//doi.org/10.1145/3580305.3599364
[26] Shuchang Liu, Qingpeng Cai, Bowen Sun, Yuhao Wang, Ji Jiang, Dong Zheng,
Peng Jiang, Kun Gai, Xiangyu Zhao, and Yongfeng Zhang. 2023. Exploration and
Regularization of the Latent Action Space in Recommendation. In Proceedings of
the ACM Web Conference 2023. 833–844.
[27] Ziru Liu, Shuchang Liu, Zijian Zhang, Qingpeng Cai, Xiangyu Zhao, Kesen Zhao,
Lantao Hu, Peng Jiang, and Kun Gai. 2024. Sequential Recommendation for
Optimizing Both Immediate Feedback and Long-term Retention. arXiv preprint
arXiv:2404.03637 (2024).
[28] Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu,
Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, et al .2023. Multi-Task
Recommendations with Reinforcement Learning. In Proceedings of the ACM Web
Conference 2023. 1273–1282.
[29] Tariq Mahmood and Francesco Ricci. 2007. Learning and adaptivity in interactive
recommender systems. In Proceedings of the ninth international conference on
Electronic commerce. 75–84.
[30] Chang Meng, Hengyu Zhang, Wei Guo, Huifeng Guo, Haotian Liu, Yingxue
Zhang, Hongkun Zheng, Ruiming Tang, Xiu Li, and Rui Zhang. 2023. Hierarchical
projection enhanced multi-behavior recommendation. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 4649–4660.
[31] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim-
othy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous methods for deep reinforcement learning. In International conference
on machine learning. PMLR, 1928–1937.
[32] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg
Ostrovski, et al .2015. Human-level control through deep reinforcement learning.
nature 518, 7540 (2015), 529–533.
[33] Ling Pan, Qingpeng Cai, and Longbo Huang. 2020. Softmax deep double deter-
ministic policy gradients. Advances in Neural Information Processing Systems 33
(2020), 11767–11777.
[34] Changhua Pei, Xinru Yang, Qing Cui, Xiao Lin, Fei Sun, Peng Jiang, Wenwu Ou,
and Yongfeng Zhang. 2019. Value-aware recommendation based on reinforcement
profit maximization. In The World Wide Web Conference. 3123–3129.
[35] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-
izing personalized markov chains for next-basket recommendation. In Proceedings
of the 19th international conference on World wide web. 811–820.
[36] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).
[37] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-
duction. MIT press.
[38] Nima Taghipour, Ahmad Kardan, and Saeed Shiry Ghidary. 2007. Usage-based
web recommendations: a reinforcement learning approach. In Proceedings of the
2007 ACM conference on Recommender systems. 113–120.
[39] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise
approach to learning to rank: theory and algorithm. In Proceedings of the 25th
international conference on Machine learning. 1192–1199.
[40] Yikun Xian, Zuohui Fu, Shan Muthukrishnan, Gerard De Melo, and Yongfeng
Zhang. 2019. Reinforcement knowledge graph reasoning for explainable rec-
ommendation. In Proceedings of the 42nd international ACM SIGIR conference on
research and development in information retrieval. 285–294.
[41] Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M Jose. 2020.
Self-supervised reinforcement learning for recommender systems. In Proceedings
of the 43rd International ACM SIGIR conference on research and development in
Information Retrieval. 931–940.
[42] Xin Xin, Tiago Pimentel, Alexandros Karatzoglou, Pengjie Ren, Konstantina
Christakopoulou, and Zhaochun Ren. 2022. Rethinking reinforcement learning for
recommendation: A prompt perspective. In Proceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
5914Future Impact Decomposition in Request-level Recommendations KDD ’24, August 25–29, 2024, Barcelona, Spain
1347–1357.
[43] Wanqi Xue, Qingpeng Cai, Zhenghai Xue, Shuo Sun, Shuchang Liu, Dong Zheng,
Peng Jiang, Kun Gai, and Bo An. 2023. PrefRec: Recommender Systems with
Human Preferences for Reinforcing Long-Term User Engagement. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(Long Beach, CA, USA) (KDD ’23). Association for Computing Machinery, New
York, NY, USA, 2874–2884. https://doi.org/10.1145/3580305.3599473
[44] Wanqi Xue, Qingpeng Cai, Ruohan Zhan, Dong Zheng, Peng Jiang, and Bo An.
2022. ResAct: Reinforcing Long-term Engagement in Sequential Recommendation
with Residual Actor. arXiv preprint arXiv:2206.02620 (2022).
[45] Kesen Zhao, Shuchang Liu, Qingpeng Cai, Xiangyu Zhao, Ziru Liu, Dong Zheng,
Peng Jiang, and Kun Gai. 2023. KuaiSim: A comprehensive simulator for recom-
mender systems. arXiv preprint arXiv:2309.12645 (2023).
[46] Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang, Xiaobing
Liu, Hui Liu, and Jiliang Tang. 2021. DEAR: Deep Reinforcement Learning for
Online Advertising Impression in Recommender Systems. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 35. 750–758.
[47] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang
Tang. 2018. Deep reinforcement learning for page-wise recommendations. In
Proceedings of the 12th ACM Conference on Recommender Systems. 95–103.
[48] Xiangyu Zhao, Long Xia, Lixin Zou, Hui Liu, Dawei Yin, and Jiliang Tang. 2020.
Whole-chain recommendations. In Proceedings of the 29th ACM international
conference on information & knowledge management. 1883–1891.
[49] Xiangyu Zhao, Long Xia, Lixin Zou, Hui Liu, Dawei Yin, and Jiliang Tang. 2021.
UserSim: User Simulation via Supervised GenerativeAdversarial Network. In
Proceedings of the Web Conference 2021. 3582–3589.
[50] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin.
2018. Recommendations with negative feedback via pairwise deep reinforcement
learning. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining. 1040–1048.
A ANALYSIS ON ITEM DECOMPOSITION
A.1 An Intuitive Example
In this section, we use some examples to show how different item
decomposition strategies may influence the learning process. Con-
sidering the relation between request-level and item-level reward:
𝑅(𝑠𝑡,𝑎𝑡)=∑︁
𝑖𝑡,𝑘∈𝑎𝑡𝑟𝑡,𝑘 (10)
Suppose we observe item-wise click-or-not reward as
[1.0,0.0,0.0,1.0,0.0,0.0]for a list of size 𝐾=6, meaning that only
the first and the fourth item got a click, then the total reward of the
list is𝑅(𝑠𝑡,𝑎𝑡)=2.0. Recall our re-weighting strategy as:
𝑤𝑡,𝑘=𝛼𝑟𝑡,𝑘+(1−𝛼)
𝛼𝑅(𝑠𝑡,𝑎𝑡)+(1−𝛼)𝐾(11)
Considering 𝛼∈{0,0.5,1.0}, we have the following re-weighting
strategies:
[1
6,1
6,1
6,1
6,1
6,1
6]if𝛼=0
[1
4,1
8,1
8,1
4,1
8,1
8]if𝛼=0.5
[1
2,0,0,1
2,0,0]if𝛼=1.0(12)
Note that if using a pure re-weighting strategy with 𝛼=1.0, the
items with clicks will split the future reward 𝑉(𝑠𝑡+1)in half, and
other items with no click signals will not account for any of the
future impacts. And the setting 𝛼=0.5will find a smoother so-
lution than this re-weighting strategy and captures the item-wise
differences compared to the equal-weight strategy.
B DECOMPOSITION FOR CRITIC LEARNING
In our method, the critic is following the TD learning objective
of request level optimization where can be easily decomposed byfuture impact. We provide reward performance of this alternative in
the "itema2c-W(C)" column of the following table. It is not even sur-
passing itemA2C (without reweighting). We believe that this may
be related to the fact that we deploy the actor instead of the critic
during recommendation. Theoretically, it is possible to decompose
the critic learning into item-level, but the items in the same state
will eventually aggregate their results under request-level MDP.
C THE DETAILS OF ALL METHODS
All models and baselines use the same user request encoder and the
main difference locates in the design of the learning framework and
the actor/critic design. Except for the SlateQ that generate K items
simultaneously and optimizes the joint action, all other baselines
output a single vector action and adopt KNN-style top-K selection.
We list details of these differences as follows:
•SlateQ: The framework of SlateQ learning uses TD error on
the state-action Q-value function 𝑄(𝑠𝑡,𝑖𝑡,𝑘)and generalizes
the "single-choice" assumption:
LSlateQ =
(𝑟𝑡,𝑘+1
𝐾𝛾(1−𝑑)
∑︁
𝑖𝑡+1,𝑘′∈𝑎𝑡+1𝑄(𝑠𝑡+1,𝑖𝑡+1,𝑘′)−𝑄(𝑠𝑡,𝑖𝑡,𝑘)2 (13)
•DDPG: The actor learning of Deep DPG method is to max-
imize𝑄(𝑠𝑡,𝑎𝑡), and the critic learning is derived from the
TD error on 𝑄(𝑠𝑡,𝑎𝑡), and the actor :
Lcritic=(𝑅(𝑠𝑡,𝑎𝑡)+𝛾(1−𝑑)𝑄(𝑠𝑡+1,𝑎𝑡+1)−𝑄(𝑠𝑡,𝑎𝑡))2
Lactor=−𝑄(𝑠𝑡,𝑎𝑡), where𝑎𝑡∼𝜋𝜃(𝑎𝑡|𝑠𝑡)(14)
•Supervision: The supervised learning directly use binary
cross entropy loss with real labels.
LBCE=−∑︁
𝑖𝑡,𝑘∈𝑎𝑡
𝑌𝑖𝑡,𝑘log𝑃(𝑖𝑡,𝑘|𝑠𝑡)
+(1−𝑌𝑖𝑡,𝑘)log(1−𝑃(𝑖𝑡,𝑘|𝑠𝑡))(15)
where𝑌𝑖𝑡,𝑘is the click-or-not label for item 𝑖𝑡,𝑘.
•HAC: An embedding vector (i.e. the hyper-action 𝑍𝑡) is used
to represent the output list (i.e. the effect-action 𝑎𝑡), and
the output list is deterministically generated from 𝑍𝑡. It also
includes an inverse module 𝑔to infer the hyper-action back
from the effect action in order to align the two action spaces
through regularization. The learning framework uses DDPG
as the backbone, and includes both the supervision and the
action-space alignment to enhance its performance:
Lcritic= 𝑅(𝑠𝑡,𝑎𝑡)+𝛾(1−𝑑)𝑄(𝑠𝑡+1,𝑔(𝑍𝑡+1))−𝑄(𝑠𝑡,𝑔(𝑎𝑡))2
Lactor=−𝐴(𝑠𝑡,𝑍𝑡)log𝜋𝜃(𝑎𝑡|𝑠𝑡)
Lhyper =|𝑍𝑡−𝑔(𝑎𝑡)|2
Lsupervision =LBCE
(16)
where𝐴(𝑠𝑡,𝑍𝑡)=𝑅(𝑠𝑡,𝑎𝑡)+𝛾(1−𝑑)𝑄(𝑠𝑡+1,𝑎𝑡+1)−𝑄(𝑠𝑡,𝑎𝑡)
and the effect-action 𝑎𝑡(i.e. the item list) is deterministically
selected from 𝑍𝑡.
5915KDD ’24, August 25–29, 2024, Barcelona, Spain Xiaobei Wang et al.
Table 4: Component analysis of ItemA2C where 𝛼=0.5in re-weighting strategy.
Dataset Performance ItemA2C ItemA2C-W(A) ItemA2C-W(C) ItemA2C-M(A)
ML1MReward 17.58±( 0.62) 17.57±( 0.54) 17.57±( 0.54) 17.94±( 0.47)
Depth 17.94±( 0.53) 17.93±( 0.46) 17.92±( 0.46) 18.24±( 0.40)
KuaiRandReward 13.45±( 0.08) 14.77±( 0.54) 13.41±( 0.40) 16.03±( 0.53)
Depth 14.41±( 0.07) 15.52±( 0.35) 14.38±( 0.45) 16.59±( 0.45)
(a) Performance in ML1M
 (b) Performance in KuaiRand
Figure 8: Training curves of all methods.
D FULL TRAINING CURVES
In Figure 8 we present the training curves of all metrics (reward,
variance, and depths) and training losses (critic and actor) in the
online learning at both user environments. We include HAC as
the strongest baseline and compare our ItemA2C alternatives with
it. We can see that ItemA2C-M outperforms others on all metricsin the ML1M environment and significantly outperforms others
in KuaiRand. Noted that Lactor andLcritic of gradually converge
during the training process. We can also observe the lower reward
variance of ItemA2C-M’s across users, indicating that the learned
policy is more capable of providing good and stable actions under
different user states.
5916