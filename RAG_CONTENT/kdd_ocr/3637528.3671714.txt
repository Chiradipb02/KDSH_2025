Sketch-Based Replay Projection for Continual Learning
Jack Julian
School of Computer Science,
University of Auckland
Auckland, New Zealand
jjul482@aucklanduni.ac.nzYun Sing Koh
School of Computer Science,
University of Auckland
Auckland, New Zealand
y.koh@auckland.ac.nzAlbert Bifet
AI Institute, University of Waikato
Hamilton, New Zealand LTCI
Télécom Paris, IP Paris, France
abifet@waikato.ac.nz
ABSTRACT
Continual learning closely emulates human learning, which al-
lows a model to learn from a stream of tasks sequentially without
forgetting previously learned knowledge. Replay-based continual
learning methods mitigate forgetting and improve performance
by reintroducing data belonging to old tasks, however a replay
method’s performance may deteriorate when the reintroduced data
does not effectively represent all experienced data. To address this
concern, we propose the Sketch-based Replay Projection (SRP)
method to capture and retain the original data stream’s distribution
within stored memory. SRP augments existing replay frameworks
and introduces a two-fold approach. First, we develop a sketch-
based sample selection technique to approximate feature distribu-
tions within distinct tasks, thereby capturing a wide distribution of
examples for subsequent replay. Second, we propose a data compres-
sion method which projects examples into a reduced-dimensional
space while preserving inter-example relationships and emphasiz-
ing inter-class disparities, encouraging diverse representations of
each class while maintaining memory requirements similar to ex-
isting replay methodologies. Our experimental results demonstrate
that SRP enhances replay diversity and improves the performance
of existing replay models.
CCS CONCEPTS
•Computing methodologies →Online learning settings; Fea-
ture selection ; Supervised learning.
KEYWORDS
Continual Learning, Replay Learning, Feature Selection, Dimen-
sionality Reduction
ACM Reference Format:
Jack Julian, Yun Sing Koh, and Albert Bifet. 2024. Sketch-Based Replay
Projection for Continual Learning. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671714
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671714
Apply Random
ProjectionExamples from Batch Examples from Batch 
Apply Random
Projection
Update transformFigure 1: The Replay Buffer Projection process. Examples
𝑋𝑦𝑖with class label 𝑦𝑖and dimension R𝑛are projected to
a subspace R𝑘to produce the projected examples 𝑋𝑅𝑃𝑦𝑖. The
transform 𝑅𝑖is updated to increase the distance 𝑟between
classes by scaling factor 𝜅with error 𝜖.
1 INTRODUCTION
Continual learning (CL) [ 10,19,28,33,40], learns a sequence of
tasks incrementally to achieve two main objectives: overcoming
catastrophic forgetting and encouraging knowledge transfer across
tasks in dynamic environments.
The rehearsal-based technique reduces the difficulty of CL by
jointly training the model on the new data and a subset of previous
data stored in a memory buffer, also known as a replay buffer. To
leverage the replay buffer efficiently, current methods employ a
sampling method [ 5,17,21,27,32,49], such as reservoir sampling
or misclassified examples, to update the replay buffer over time.
Each example has the same probability of being stored in the buffer
without knowing the length of the input data stream in advance.
Moreover, research in rehearsal-based CL has primarily focused on
maintaining a diverse set of examples in the replay buffer [ 15,24],
yet gaps persist in addressing the nuanced challenges presented by
non-i.i.d. features and the efficient utilisation of memory resources.
As machine learning applications encounter increasingly dynamic
and evolving datasets, existing methods may struggle to adapt,
leading to outdated or biased data representations in the replay
buffer.
We address the open problem of how to populate the replay
buffer to capture an evolving understanding of each learned task,
specifically in handling concept drift, memory constraints, and dy-
namic data distributions. To address these gaps, we introduce two
 
1325
KDD ’24, August 25–29, 2024, Barcelona, Spain Jack Julian, Yun Sing Koh, & Albert Bifet
novel components, Class-Feature Sketching (CFS) and Replay Buffer
Projection (RBP), to address key limitations in existing approaches.
CFS is designed to handle non-i.i.d. features by approximating the
distribution of features within each class. CFS leverages the Count-
Min Sketching algorithm to approximate the distribution of features
within each class encountered in a dynamic data stream, effectively
addressing issues of concept drift within classes. We discuss the
details of CFS in Section 4.1 On the other hand, RBP explores how
more examples may be added to the replay buffer without com-
promising memory complexity, optimising memory usage through
random projection. RBP not only enhances the memory efficiency
of the replay buffer but also actively updates the transformation to
improve the distinguishability of examples from different classes
in fewer dimensions, as depicted in Figure 1, where a transform
𝑅𝑖projects a batch of examples into a lower dimensional subspace
and updates 𝑅𝑖so that the proceeding batch of examples will have
a larger average L2-norm between class examples after projection.
An increased distance between classes enhances learning for ML
systems [ 31]. We provide an in-depth description of RBP in Sec-
tion 4.3. We combine CFS and RBP to develop the Sketch-Based
Replay Projection (SRP) method.
Our contributions are threefold, specifically:
•Reactive sketching process. We developed a novel sketch-
ing component for a replay-based approach that minimizes
catastrophic forgetting by efficiently populating the replay
buffer.
•Optimize memory usage. We developed the Replay Buffer
Projection method to optimise memory usage in the replay
buffer by projecting instances into a lower-dimensional space
while preserving the similarities between examples.
•Combination of adaptation and optimisation for sam-
pling. We demonstrate that SRP aims to bridge critical
gaps in existing replay-based CL methods, offering a com-
prehensive solution to the challenges posed by dynamic data
streams and memory constraints.
2 PRELIMINARIES
2.1 Notation and Definitions
Here, we define the notations used in the paper. First, we define
a data stream 𝑆={(𝑥𝑖,𝑦𝑖)}∞
𝑖=1, containing a potentially infinite
number of examples𝑥𝑖each mapped to a class𝑦𝑖∈𝑌, where𝑌is
the set of all classes. Each example is a vector comprised of features
denoted as𝑥𝑖=(𝑥1
𝑖,...,𝑥𝑛
𝑖), where𝑛is a constant determined by
the dataset denoting the number of features in an example.
Acontinual learning system is a model Mwhich trains on a
data stream 𝑆to predict a class ˆ𝑦𝑖∈𝑌for each example 𝑥𝑖such
that ˆ𝑦𝑖=𝑦𝑖. Areplay-based model is a CL system that maintains
areplay bufferR⊂𝑆of examples encountered by model Min𝑆.
Examples in the replay buffer are later reintroduced for training in
unison with a new example from 𝑆to avoid forgetting.
We assume that 𝑆is atask-based setting, where at some time
step𝑡,(𝑥𝑡,𝑦𝑡)is drawn from a non-i.i.d distribution 𝐷𝑠. The cur-
rent distribution 𝐷𝑠experiences sudden or gradual shifts to a new
distribution 𝐷𝑠+1throughout model training. We refer to each dis-
tribution𝐷𝑠as a task and assume that each task in 𝑆is unknown
before training.We define the class distinguishability of sets of examples 𝑋1and
𝑋2as the average pairwise L2-norm of examples, similar to the
Ward Function [29]:
distinguish C(𝑋1,𝑋2)=1
|𝑋1|·|𝑋2|∑︁
𝑥𝑖∈𝑋1∑︁
𝑥𝑗∈𝑋2dist(𝑥𝑖,𝑥𝑗),
where dist(𝑥𝑖,𝑥𝑗)is the L2-norm of examples 𝑥𝑖and𝑥𝑗. For a
dataset𝐷={𝑋𝑦𝑖:𝑦𝑖∈𝑌}, where each 𝑋𝑦𝑖is a set of examples
from class𝑦𝑖, we define distinguishability as:
distinguish(𝐷)=|𝑌|(|𝑌|−1)
2|𝑌|−1∑︁
𝑖=1|𝑌|∑︁
𝑗=𝑖+1distinguish L(𝑋𝑦𝑖,𝑋𝑦𝑗).
A higher distinguishability indicates a higher degree to which indi-
vidual classes or examples are separable from one another or noise
within a dataset. Identical classes will have a distinguishability of 0.
2.2 Count-Min Sketch
Count-Min Sketch (CMS) is a sketching algorithm used to approxi-
mate the frequency of items. CMS consists of a matrix denoted as
𝐴𝑤×𝑑comprising counters, with 𝑤representing the width in terms
of columns and 𝑑signifying the depth in rows. The parameters
𝑤and𝑑are determined by the approximation variables 𝜆and𝛿,
such that, with a probability of 1−𝛿, the estimated counts derived
from the sketch exhibit an absolute error of 𝜆. Each of the 𝑑rows
is associated with a distinct hash function denoted as ℎ0,...,ℎ𝑑,
whereℎ𝑖(𝑥)=(𝑎𝑖𝑥+𝑏𝑖)mod𝑝𝑖for random integers 𝑎𝑖,𝑏𝑖and a
random prime 𝑝𝑖. The hash function ℎ𝑖determines which among
the𝑤counters on row 𝑖is incremented. The initialization of matrix
𝐴𝑤×𝑑is determined by the 𝜆and𝛿parameters: 𝑑=𝑒
𝜆and𝑤=ln1
𝛿,
with all cells set to 0.
The count of an item 𝑥in row𝑔of𝐴𝑤×𝑑iscount(𝐴𝑤×𝑑,ℎ𝑔(𝑥)),
whereℎ𝑔(𝑥)is the column mapped to item 𝑥. The approximate
count of item 𝑥when queried in 𝐴𝑤×𝑑is:
𝑑
min
𝑔=0count(𝐴𝑤×𝑑,ℎ𝑔(𝑥)).
Assuming a data stream which has seen 𝑁items, let𝑐𝑥denote the
true count of an item 𝑥undergoing estimation. The estimated count
for item𝑥is at least𝑐𝑥, as all cells are non-negative. The counts
may be overestimated due to features incorrectly incrementing
cells, reaching a maximum of 𝑐𝑥+𝜆·𝑁with a probability of at
least 1−𝛿.
3 RELATED WORK
This section summarises existing work on replay-based approaches
in CL. We focus on algorithms that populate, maintain, and select
from the replay buffer.
Replay buffer selection refers to selecting examples to store in
memory for later reintroduction to the model. As it is not practical
for the replay buffer to store all examples experienced in a data
stream [ 38,42], it is important to consider which examples should
be replayed [ 9,48]. Greedy Sample Selection (GSS) [ 3] encourages
diversity of examples in the replay buffer with differing gradients
of loss to internal parameters, indicating that a diverse selection of
examples improves the accuracy of a replay model. Gradient-based
Coresets for Replay-based CL (GCR) [ 41] creates coresets, which
 
1326Sketch-Based Replay Projection for Continual Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
are informative weighted data subsets that approximate specific
attributes, including loss, gradients and logits of original data. Core-
sets are maintained to approximate the parameters’ gradient for
all data experienced by the model. Coresets promote diversity and
indicate that a different representation of data can be useful for
replay. Reactive Subspace Buffer (RSB) [ 24] creates clusters cen-
tred on classes maintained in the replay buffer, where examples
are removed when concept drift is detected. These examples no
longer represent the class state. Class-Balancing Reservoir Sampling
(CBRS) [ 4] stores an i.i.d sample for each class with balanced sizes
by applying reservoir sampling such that class balance is respected,
demonstrating that a replay model benefits when the distribution of
classes in its replay buffer reflects the actual distribution of classes in
the data stream. Continual Learning by Modeling Intra-Class Varia-
tion (MOCA) [ 49] avoids catastrophic forgetting by diversifying the
replay buffer for target classes, reinforcing the importance of accu-
rately representing independent classes. Proxy-based Contrastive
Replay (PCR) [ 27] also addresses the issue of class imbalance in
the replay buffer by dynamically replacing contrastive examples
with proxies. Error-Sensitive Reservoir Sampling (ESMER) [ 36] uses
a modified reservoir sampling algorithm to prioritise lower-loss
examples, representing previous tasks more accurately.
The above work employs sampling methods, a common tech-
nique for populating a replay buffer. Sketching has not widely been
applied in research despite its ability to quickly and reliably approx-
imate data distributions.
Replay sample selection selects examples from the replay buffer
for training. Replay may be improved when examples are chosen to
benefit model accuracy best [ 7,12,16]. Experience Replay (ER) [ 35]
uses reservoir sampling to select a replay buffer. Gradient-based
Memory Editing (GMED) [ 21] adjusts examples stored in the replay
buffer with small gradient updates to increase model loss when rein-
troduced. Adversarial Shapley Value Experience Replay [ 37] uses a
kNN Shapley value to select samples representative of the latent
space. Maximally Interfered Retrieval (MIR) [ 2] applies a look-ahead
probing step to replay samples negatively forgotten by incoming
parameter updates. Lifelong Zero-Shot Learning (LZSL) [ 46] uses
a Variational Auto-Encoder to replay semantic representations of
data. Most replay methods focus on static or gradually changing
environments [ 22,23]. There is a lack of comprehensive exploration
regarding how these methods adapt to highly dynamic environ-
ments where rapid shifts in data distributions occur, potentially
leading to outdated or biased representations in the replay buffer.
4 SKETCH-BASED REPLAY PROJECTION
Our Sketch-Based Replay Projection (SRP) consists of two key com-
ponents, Class-Feature Sketching (CFS) and Replay Buffer Projec-
tion (RBP), which collectively address challenges in replay-based
CL. In the CFS component, we introduce a method which efficiently
populates the replay buffer. This method ensures the representa-
tion of diverse examples from each class in a dynamic data stream,
maintaining the feature distribution and mitigating the impact of
concept drift. Complementing this, the RBP component focuses on
improving the replay buffer’s memory usage efficiency by project-
ing examples into a lower-dimensional space. RBP leverages ran-
dom projection to preserve pairwise distances between examples,
 
 
 
 
Perform CountMin
Sketch
Use  to calculate
probability Figure 2: Class Feature Sketching to calculate the probability
𝑃𝑥𝑖of adding example 𝑥𝑖to the replay buffer.
dynamically updating the transformation to enhance distinguisha-
bility between different classes. Together, these components balance
the need for diverse representation in the replay buffer and efficient
memory utilisation through dimensionality reduction. Our imple-
mentation is provided here: https://github.com/jjul482/Sketched-
Replay-Projection.
4.1 Class-Feature Sketching
We developed CFS, a replay-based CL setting, which approximates
the distribution of features within each class encountered in a data
stream. Evolving data streams may experience concept drift within
classes [ 11,24]. Let us consider an example of concept drift in the
context of weather, where the statistical properties of the target
variable, in this case, weather conditions, may change over time due
to various factors such as seasonal variations or climate change. As
weather conditions evolve, it becomes crucial for a model to adapt
and learn from these changes to maintain accurate predictions or
classifications. CFS aims to account for concept drift within classes
and ensure that the replay buffer maintains a representative feature
distribution. It sketches examples belonging to each class in a data
stream by maintaining the feature distribution of each class. By
saving these examples for replay, we avoid losing any characteristics
of a class due to concept drift. Furthermore, CFS only requires a
single pass over the data stream to populate the replay buffer.
CFS implements the CMS algorithm described in Section 2.2
as a foundation for its sketching component. A separate sketch
of each class in the data stream allows the model to focus on the
feature distribution for individual classes. In CFS, we initialize a
set of matrices 𝐴𝑐,∀𝑐∈𝑌. Upon the arrival of a new example 𝑥𝑖
with class𝑐within the data stream 𝑆, for each feature 𝑥𝑗
𝑖, every
hash function ℎ𝑖is applied, resulting in the corresponding counter
being incremented in 𝐴𝑐. Subsequently, to determine whether an
example is to be included in the replay buffer, the associated counts
using the same hash functions are retrieved. This is accomplished
by determining the minimum value across all hashed cells, as given
that each cell has been incremented upon observing a feature, each
cell inherently signifies an upper bound on the actual value.
Every feature of 𝑥𝑖is sketched to find the approximate frequency
of each. By using all cells in 𝐴𝑐, we approximate the overall propor-
tion𝑃𝑥𝑖of features in an example for the features in all examples
 
1327KDD ’24, August 25–29, 2024, Barcelona, Spain Jack Julian, Yun Sing Koh, & Albert Bifet
Batch Transform Replay Buf fer Projection (RBP)
Update  to increase the distances
between clusters in the next transform.
Class Feature Sketching (CFS)
Sketch  to determine the
probability  of adding  to Batch Replay Buf fer 
Model Update Sketch Matrix 
Train
examples
from Batch 
on model.
Figure 3: Overview of the Sketch-Based Replay Projection architecture.
encountered from class 𝑐as in Equation 1. Here,
𝑑
min
𝑔=0count(𝐴𝑐,ℎ𝑔(𝑥𝑗
𝑖))
is the approximate frequency of 𝑥𝑗
𝑖for the CMS matrix 𝐴𝑐.
𝑃𝑥𝑖=1Í
𝑎,𝑏𝐴𝑐[𝑎][𝑏]∑︁
𝑗∈[0,𝑛)𝑑
min
𝑔=0count(𝐴𝑐,ℎ𝑔(𝑥𝑗
𝑖)) (1)
We denote 𝑃𝑥𝑖as the probability that example 𝑥𝑖is added to
the replay buffer. Integers 𝑎,𝑏iterate over all cells in 𝐴𝑐, with
(𝑎,𝑏)<(𝑤,𝑑). The CFS algorithm for populating the replay buffer
Rfrom a class 𝑐is provided in Algorithm 1, where CMS is a function
that returns an updated 𝐴𝑐via CMS with respect to some feature
𝑥𝑗
𝑖.
Algorithm 1 Class-Feature Sketching (CFS)
Input:𝑆={(𝑥𝑖,𝑦𝑖)}∞
𝑖=1- Data stream, 𝜆,𝛿∈R,ℎ0,...,ℎ𝑑- Hash
functions,𝑐- Class label.
Output: Populated replay buffer R.
1:𝑤←𝑒
𝜆
2:𝑑←ln1
𝛿
3:𝐴𝑐←0𝑤×𝑑
4:for(𝑥𝑖,𝑦𝑖)∈𝑆do
5:if𝑐=𝑦𝑖then
6:𝐹←{}
7: forFeature𝑥𝑗
𝑖∈𝑥𝑖do
8:𝐴𝑐←CMS(𝐴𝑐,𝑥𝑗
𝑖,ℎ0,...,ℎ𝑑)
9: end for
10:𝐹←Í
𝑗∈[0,𝑛)𝑑
min
𝑔=0count(𝐴𝑐,ℎ𝑔(𝑥𝑗
𝑖))
11: GrandSum←Í
𝑎,𝑏𝐴𝑐[𝑎][𝑏]
12: if1
GrandSumÍ
𝑓∈𝐹𝑓>Random(0,1)then
13:R←R∪(𝑥𝑖,𝑦𝑖){Replay buffer is updated.}
14: end if
15: end if
16:end for
17:returnR4.2 Convolutional CFS
Most feature representation techniques assume that features are
i.i.d. However, this assumption is untrue for many datasets. For
instance, features in an image dataset are highly dependent on their
ordering and information is lost if surrounding features are ignored.
Therefore, we introduce a convolution layer to CFS to introduce
context from neighbouring features to the sketching process.
𝐶(𝑥𝑗
𝑖,𝐾)=𝐾1∑︁
𝑝=1𝐾2∑︁
𝑞=1𝑥(𝑝,𝑞)
𝑖·𝐾(𝑝,𝑞)(2)
The feature convolution process for images is described in Equa-
tion 2, where 𝑥(𝑝,𝑞)
𝑖represents the feature at position (𝑝,𝑞)in image
𝑥𝑖and𝐾is the convolutional filter with dimensions 𝐾1×𝐾2. The
value of the filter at position (𝑝,𝑞)is denoted𝐾(𝑝,𝑞).
For datasets containing non-i.i.d features, we apply the Convo-
lutional CFS, which applies the CFS algorithm and sketches each
𝐶(𝑥𝑗
𝑖,𝐾). For instance, to calculate 𝑃𝑥𝑖in Convolutional CFS, we
apply Equation 1 but substitute 𝑥𝑗
𝑖for𝐶(𝑥𝑗
𝑖,𝐾). We avoid optimiz-
ing𝐾during training and set the size of the convolutional frame
as a constant prior to training time.
4.3 Replay Buffer Projection
Our second method explores the upper limit of the replay buffer’s
capacity without compromising memory complexity. This involves
projecting instances in the replay buffer to a lower-dimensional
space, aiming to maintain the similarity of examples. We update
the projection at each new batch encountered in the data stream,
enhancing the L2-norm between examples from distinct classes to
improve their distinguishability in fewer dimensions.
As a general principle for replay-based learning, a greater vari-
ation of examples in the replay buffer improves the classification
accuracy of a method [49]. Intuitively, variation is easily achieved
if more examples are stored in the replay buffer. However, a large
replay buffer demands expensive memory requirements. RBP de-
creases the size of each example in the replay buffer through random
projection, allowing more examples to be stored within the same
memory constraints. By increasing the number of examples in mem-
ory, we more accurately capture the distributions of each class in
the replay buffer, thereby improving the replay’s effectiveness.
 
1328Sketch-Based Replay Projection for Continual Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
RBP leverages random projection to transform examples to a
lower-dimensional space while preserving the similarities between
examples. In our case, the dimensionality of our data is equivalent
to the number of features 𝑛for each example. Using matrix notation,
if𝑋𝑛×𝑁is the original set of 𝑁examples with 𝑛features each, then
the projection of the data onto a lower 𝑘-dimensional subspace is:
𝑋𝑅𝑃
𝑘×𝑁=𝑅𝑘×𝑛𝑋𝑛×𝑁.
where𝑅𝑘×𝑛is a random matrix generated using a Gaussian dis-
tribution. Following the Johnson-Lindenstrauss lemma (Lemma
1) [44], if𝑘<<𝑛, then the pairwise distances between examples
are approximately preserved. Specifically, distances are reduced
between all examples in 𝑋𝑛×𝑁by an approximate constant factor
𝜔, and a theoretical guarantee for 0<𝜖<1and for all vectorized
examples𝑥𝑖,𝑥𝑗∈col(𝑋𝑛×𝑁)is:
(1−𝜖)||𝑥𝑖−𝑥𝑗||2≤∥𝑅𝑥𝑖
𝜔−𝑅𝑥𝑗
𝜔∥2≤(1+𝜖)||𝑥𝑖−𝑥𝑗||2.
As distances between examples are roughly preserved, a model
can classify the 𝑘-dimensional examples with a distance distortion
of𝑛-dimensional examples.
𝑘≥24
3𝜖2−2𝜖3log𝑛 (3)
/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000017
/uni0000004f/uni00000052/uni0000004a/uni0000000b/uni00000051/uni0000000c/uni00000014/uni00000013/uni00000015/uni00000014/uni00000013/uni00000016/uni0000004f/uni00000052/uni0000004a/uni0000000b/uni0000004e/uni0000000cε=/uni00000013/uni00000011/uni00000015 
ε/uni00000020/uni00000013/uni00000011/uni00000017 
ε/uni00000020/uni00000013/uni00000011/uni00000019 
ε/uni00000020/uni00000013/uni00000011/uni0000001b
Figure 4: The effects of varying 𝜖on a range of initial di-
mensions𝑛. The projected dimension, 𝑘, is calculated using
Equation 3.
The error𝜖is determined by the scale of the dimension reduction
fromR𝑛toR𝑘and is characterised by the relationship in Equation 3
according to the Johnson-Lindenstrauss lemma [ 39]. A higher 𝜖
value indicates that there may be more distortion of the distance
between the initial and projected data. The impact of different 𝜖
values over a range of initial dimensions 𝑛is plotted in Figure 4.
When examples are compressed, there is a trade-off between the
quantity of data stored in the replay buffer and the quality of exam-
ples for training [ 45]. Despite the theoretical guarantees of random
projection [ 14], some information may be lost in reduction to a
lower dimension [ 6,18]. To offset the quality-quantity trade-off, we
update𝑅𝑖during training such that the squared L2-norm between
examples belonging to different classes is scaled by some factor 𝜅.
This process aims to project examples for each class into distinct
sets and extend the distance between each class set. Intuitively,
these class sets allow the model to classify new examples more
accurately by improving the quality of examples in memory while
preserving the quantity.Let𝑋𝑅𝑃𝑦1and𝑋𝑅𝑃𝑦2be two projected batches of 𝑁examples in
matrix form belonging to classes 𝑦1and𝑦2respectively. First, we
compute the squared L2-norm Δ𝑅𝑖between𝑅𝑖𝑋𝑅𝑃𝑦1and𝑅𝑖𝑋𝑅𝑃𝑦2:
Δ𝑅𝑖=𝑡𝑟
𝑅⊤
𝑖·(𝑋𝑅𝑃
𝑦1−𝑋𝑅𝑃
𝑦2)⊤·(𝑋𝑅𝑃
𝑦1−𝑋𝑅𝑃
𝑦2)·𝑅𝑖
.
We use L2-norm because data is rarely sparse after projection.
Therefore, L2-norm is discriminant even on high-dimensional data [ 20].
We then update 𝑅𝑖to𝑅𝑖+1using a scaling factor 𝜅in Equation 4.
𝑅𝑖+1=𝜅·𝑅𝑖·vutΔ𝑅𝑖
𝑡𝑟
𝑅⊤
𝑖·𝑅𝑖 (4)
The update rule for the transform 𝑅𝑖shown in Equation 4 scales
by a factor that preserves the relationship:
||𝑅𝑖+1𝑋𝑅𝑃
𝑦1−𝑅𝑖+1𝑋𝑅𝑃
𝑦2||2=𝜅||𝑅𝑖𝑋𝑅𝑃
𝑦1−𝑅𝑖𝑋𝑅𝑃
𝑦2||2.
This update ensures that the transformation 𝑅𝑖+1maintains a pro-
portional relationship in the squared L2-norm of the transformed
examples between classes, allowing a change in 𝑅𝑖based on the
relative distances of 𝑋𝑅𝑃𝑦1and𝑋𝑅𝑃𝑦2. Intuitively, if classes are more
distinguishable, the accuracy of a classification model is enhanced.
The transformation is updated at the end of each training batch
𝑋𝑅𝑃𝑦𝑖. Examples in the replay buffer Rfrom all experienced classes
are used to perform the update. Algorithm 2 describes projecting a
new batch of examples.
Algorithm 2 Replay Buffer Projection (RBP)
Input:𝑁- Batch size. 𝑆- Data stream of batches
𝑋𝑦={(𝑥1,𝑦),...,(𝑥𝑁,𝑦)},𝑘- Dimension of transformed
examples,𝑘<<𝑛.𝜅- Scaling factor for transformation updates.
Output: Updated Replay Buffer R.
1:𝑅∼N( 0,1)∈R𝑘×𝑛
2:R←∅
3:for𝑋𝑦𝑖∈𝑆do
4:𝑋𝑅𝑃𝑦𝑖←𝑅𝑋𝑦𝑖
5:for𝑋𝑅𝑃𝑦𝑗∈{𝑋𝑦𝑗∈R :𝑦𝑖≠𝑦𝑗}do
6: Δ𝑅←𝑡𝑟
𝑅⊤(𝑋𝑅𝑃𝑦𝑖−𝑋𝑅𝑃𝑦𝑗)⊤(𝑋𝑅𝑃𝑦𝑖−𝑋𝑅𝑃𝑦𝑗)𝑅
7:𝑅←𝜅·𝑅·√︃
Δ𝑅
𝑡𝑟(𝑅⊤·𝑅)
8:end for
9:R←R∪𝑋𝑅𝑃𝑦𝑖
10:end for
11:returnR
The hyperparameter 𝜅∈ [1,∞)scales the L2-norm between
classes in the replay buffer. Setting 𝜅=1results in no change to
𝑅. Because𝜅scales the error 𝜖of random projection by 𝜅𝜖, it is
important that 𝜅is not high. Moreover, a high 𝜅introduces bias to
anomalous examples and edits 𝑅without considering concept drift.
Figure 3 shows the architecture of the SRP method, combining
CFS and RBP to capture the distribution of a data stream in mem-
ory more effectively. CFS selects examples from the data stream,
and RBP represents examples for more effective replay and more
efficient memory usage.
 
1329KDD ’24, August 25–29, 2024, Barcelona, Spain Jack Julian, Yun Sing Koh, & Albert Bifet
Table 1: Classification accuracy of baseline replay algorithms with CFS, RBP and SRP on benchmark datasets. Gain is the
percentage accuracy gain between the baseline model with and without SRP. Bold results represent the highest result for each
baseline on each dataset with a statistically significant difference to the baseline model.
Methods MNIST Fashion MNIST CIFAR-10 CIFAR-100 Mini ImageNet CoverType COIL-100
ER [35] 29.61±0.16 27.88±0.18 31.81±0.19 18.25±0.21 15.23±0.24 24.27±0.32 13.06±1.05
ER+CFS 30.51±0.22 28.21±0.19 31.94±0.25 19.13±0.17 16.12±0.21 25.51±0.29 13.11±1.12
ER+RBP 33.12±0.10 31.24±0.19 32.80±0.31 21.01±0.19 18.13±0.30 25.04±0.55 14.51±1.36
ER+SRP 33.51±0.36 32.34±0.21 32.85±0.34 22.67±0.22 19.21±0.29 25.62±0.71 14.66±1.51
Gain↑13.17↑15.99↑3.27↑24.22↑26.13↑5.56↑12.25
GMED [21] 31.43±0.71 30.11±0.65 32.72±0.91 21.11±0.78 17.51±0.70 27.81±0.91 13.42±1.51
GMED+CFS 32.87±0.69 31.81±0.66 33.04±1.01 22.54±0.71 18.11±0.78 29.96±1.12 13.45±1.55
GMED+RBP 33.51±0.89 32.98±0.77 34.95±1.21 22.91±0.81 20.01±0.89 28.01±1.04 15.19±1.47
GMED+SRP 33.74±0.96 33.47±0.75 35.27±1.25 23.92±0.78 21.05±0.86 28.19±1.20 15.22±1.48
Gain ↑7.35↑11.16↑10.88↑13.31↑20.21↑1.32↑13.41
MOCA [49] 60.61±0.58 58.51±0.62 61.74±0.69 30.05±1.02 10.62±0.45 21.39±0.39 13.51±1.72
MOCA+CFS 60.89±0.61 59.41±0.64 62.41±0.65 31.16±1.05 11.71±0.53 22.87±0.45 13.52±1.86
MOCA+RBP 61.51±0.65 60.01±0.67 64.50±0.78 33.01±1.21 12.51±0.58 22.31±0.70 15.51±2.01
MOCA+SRP 61.31±0.81 60.50±0.61 65.41±0.88 33.62±1.15 13.45±0.61 22.91±0.71 15.84±2.12
Gain ↑1.15↑3.40↑5.94↑11.88↑26.65↑7.11↑17.25
PCR [27] 43.71±0.71 40.87±0.77 44.38±1.25 21.85±0.92 20.91±0.94 25.91±0.77 13.78±1.45
PCR+CFS 43.69±0.72 41.52±0.69 45.71±1.43 22.98±0.95 21.74±0.98 26.52±0.81 13.71±1.82
PCR+RBP 44.01±0.76 43.81±0.79 47.06±1.25 24.37±0.89 23.85±1.04 26.01±0.84 15.31±1.36
PCR+SRP 44.13±0.79 44.04±0.81 47.87±1.52 24.91±0.94 24.51±1.02 26.17±0.89 15.36±1.21
Gain ↑0.96↑7.76↑7.86↑14.00↑17.21↑1.00↑11.47
ESMER [36] 50.51±0.65 45.72±0.71 55.31±0.64 31.51±1.01 20.84±1.05 22.95±0.65 13.91±1.52
ESMER+CFS 50.17±0.75 45.84±0.77 55.62±0.75 31.16±1.12 20.63±1.22 23.01±0.72 13.93±1.55
ESMER+RBP 53.74±0.66 47.99±0.83 58.11±0.71 34.64±1.36 24.45±1.51 23.08±0.79 16.01±1.62
ESMER+SRP 53.53±0.80 47.55±0.91 57.83±0.78 34.37±1.40 24.94±1.55 23.75±0.80 16.34±1.75
Gain ↑5.98↑4.00↑4.56↑9.08↑19.67↑3.49↑17.45
5 EXPERIMENTS
We conduct experiments to assess the effectiveness of the CFS
and RBS algorithms. For a thorough evaluation, we introduce 7
benchmark datasets and present our experiment hyperparameters
before comparing the classification accuracy of CFS and RBP against
state-of-the-art replay methods. We then investigate the sensitivity
of our method by varying key hyperparameters 𝜆,𝛿,𝜖 and𝜅and
comparing classification accuracy. Through these experiments, we
aim to answer the following research questions.
•RQ1: How may the replay buffer be more effectively popu-
lated to reflect the distribution of the data stream?
•RQ2: How may memory allocation for the replay buffer be
more efficiently used to improve the replay process?
5.1 Datasets
We use 7 benchmark datasets for all experiments: MNIST [ 13],
Fashion MNIST [ 47], CIFAR-10 [ 25], CIFAR-100 [ 25], Mini Ima-
geNet [ 34], CoverType [ 8] and COIL-100 [ 30]. We order each dataset
as a sequence of classes and introduce examples sequentially to
all methods, ensuring that the distribution of incoming examples
changes over time. For all datasets, the testing set used to determine
the classification accuracy is a random 20%split of examples.Table 2: Overview of the datasets.
Dataset # Examples # Features ( 𝑛) # Classes Type
MNIST 70000 784 10 Image
Fashion MNIST 70000 784 10 Image
CIFAR-10 60000 3072 10 Image
CIFAR-100 70000 3072 100 Image
Mini ImageNet 60000 12288 100 Image
COIL-100 7200 49152 100 Image
CoverType 581012 52 7 Tabular
Table 2 presents an overview of each dataset and further details
of each are provided in the supplementary. Many of the features
encountered in our benchmark datasets are continuous. This may be
problematic when CFS approximates the frequency of each feature
as even a slight variation in continuous data is nonetheless sketched
differently. Addressing this challenge involves assuming that the
datasets conform to Gaussian distributions. This study employs
a straightforward preprocessing technique wherein all numeric
features transform into discrete features. The discretization process
is performed before sketching, wherein each numerical attribute is
discretized into an equal-width histogram comprising 50 bins.
 
1330Sketch-Based Replay Projection for Continual Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
5.2 Baseline Methods
We evaluate existing memory replay approaches for such as ER [ 35],
GMED [ 21], MOCA [ 49], PCR [ 27] and ESMER [ 36]. We then im-
plement our approaches on top of these baselines to compare clas-
sification accuracy on benchmark datasets. We do not adjust the
internal machine learning architecture of these baselines, and thus
this study does not compare between baselines methods.
We compare existing image compression methods for replay
learning with RBP. Although there is limited work on data compres-
sion for replay learning, we use Memory Replay Data Compression
(MRDC) [ 45], which leverages JPEG compression, and Soft-to-Hard
Vector Quantization Autoencoder (SHVQ) [ 1], which compresses
images using an autoencoder.
5.3 Reproducibility
The experiments were implemented and evaluated in the Python
programming language. They were conducted on an NVIDIA Tesla
V100 GPU with CUDA compatibility. Our models are deployed
on top of existing baseline models and do not interfere with the
internal model architecture. Except for sensitivity tests, we use
the following hyperparameters obtained via grid search for our
experiments: 𝜆=0.1,𝛿=0.05,𝜖=0.2,𝜅=1.5. Additionally,
we set a maximum number of features, which we refer to as a
feature capacity, for the replay buffer at 100,000for MNIST, Fashion
MNIST and CoverType, 300,000for CIFAR-10 and CIFAR-100, and
5,000,000for Mini ImageNet and COIL-100. For example, as each
MNIST example contains 28×28features by default, the replay
buffer can contain 100,000/(28×28)=128examples without
projection. We set the size of each training batch at 128, and replay
50examples per batch. The choice of 𝑘is determined by 𝜖according
to Equation 3. For example, 𝜖=0.2for the MNIST dataset ( 𝑛=
28×28) yields𝑘=668.
A learning system’s results are susceptible to the ordering of
tasks, as previous research has indicated that the shaping of data
may influence how such models perform [ 26]. We, therefore, con-
duct all experiments ten times with randomised class order to avoid
bias to a particular dataset ordering and ten times for each class
order to avoid bias due to a random seed. Each experiment is, there-
fore, performed one hundred times. Our results report the mean
and standard deviation values to avoid bias from dataset ordering.
Convolutional CFS is used in all experiments except for experi-
ments on the CoverType dataset, as features in CoverType may be
assumed i.i.d. We use a 3×3convolution frame.
5.4 Classification Accuracy
Baselines: Table 1 compares the accuracy of baseline replay mod-
els with and without CFS, RBP and SRP. Accuracy refers to the
average classification accuracy over all classes once a model has
finished training. When applied to existing replay methods, the
results consistently validate the effectiveness of SRP and the RBP
and CFS components. We provide the percentage accuracy gain
for each baseline when SRP is applied. CFS results demonstrate an
accuracy gain when trained on datasets with fewer features, such
as CoverType, as important features are more distinguishable. CFS
achieves incremental gain on image datasets with a greater number
of features. Results of the RBP component show a limited gain ondatasets that already have fewer features. On Mini Imagenet and
COIL-100, which have the highest number of features per example
of our selected benchmark datasets, we observe a greater accu-
racy gain in the RBP results, emphasising the importance of feature
compression in replay for datasets with high memory requirements.
Compression Techniques: Table 8 shows the classification
accuracy of RBP with competing example compression methods
applied to ER. Each method compresses examples to the same size
produced by RBP with 𝜖=0.2for each dataset, and we set the same
replay memory capacity as in Section 5.3. ER is implemented with
reservoir sampling to avoid bias from different replay sampling
policies. RBP is, therefore, applied to ER without CFS. Our results
show that RBP outperforms existing compression techniques on
ER, indicating that RBP enhances the representation of examples
with fewer features compared to baselines. Results on all datasets
and replay methods can be found in Appendix C.
Table 3: Accuracy of Experience Replay (ER) with compres-
sion techniques for replay learning.
Methods Fashion MNIST CIFAR-100 Mini ImageNet
ER 27.88±0.18 18.25±0.21 15.23±0.24
ER+MRDC 29.19±0.21 19.98±0.42 16.98±0.31
ER+SHVQ 30.01±0.35 20.13±0.37 17.19±0.36
ER+RBP (Ours) 31.24±0.19 21.01±0.19 18.13±0.30
15202530
0.1 0.2 0.3 0.4 0.5
λAccuracy (%)30
25
20
15
0.05 0.10 0.15 0.20 0.25
δAccuracy (%)
1015202530
0.1 0.2 0.3 0.4 0.5
εAccuracy (%)1015202530
1.0 1.5 2.0 2.5 3.0
κAccuracy (%)
CIFAR−100 Fashion MNIST Mini ImageNet0.30
Figure 5: Classification accuracy of varying 𝜆,𝛿,𝜖 and𝜅for
ER+SRP over Fashion MNIST, CIFAR-100 and Mini ImageNet.
5.5 Sensitivity
We investigate the sensitivity of our model to the following hy-
perparameters: sketching approximation variables 𝜆,𝛿, projection
error𝜖, and transform scaling variable 𝜅. Figure 5 provides the
accuracy of SRP with ER on CIFAR-100, Fashion MNIST and Mini
ImageNet over a range of values for each hyperparameter. While
 
1331KDD ’24, August 25–29, 2024, Barcelona, Spain Jack Julian, Yun Sing Koh, & Albert Bifet
50
 0 5050
050
(a) Fashion MNIST examples.
50
 0 5050
050 (b) Projected by RBP.
50
 0 5025
02550 (c) Compressed by MRDC.
50
 0 5050
25
02550 (d) Compressed by SHVQ.
10 1010010
0
(e) COIL-100 examples.
10 1010010
0
 (f) Projected by RBP.
10
 0 1010
010 (g) Compressed by MRDC.
10
 0 1010
010 (h) Compressed by SHVQ.
Figure 6: Visualization using t-SNE plots of three data compression techniques, RBP, MRDC and SHVQ, on classes from
benchmark image datasets compared to the original dataset. Colours represent distinct classes.
varying a hyperparameter, other hyperparameters are set to the fol-
lowing default values: 𝜆=0.1,𝛿=0.05,𝜖=0.2,𝜅=1.5. Accuracy
decreases as 𝜖increases because, with a higher 𝜖, fewer features will
be used to represent each example. A high 𝜅may lead to excessive
feature changes during a transform, which impacts accuracy. The
results indicate the optimal setting for 𝜅changes depending on the
dataset. Higher 𝜆and𝛿increase the error in CFS by increasing the
probability that a hash function increments the wrong cell.
5.6 Projection Representation
We visualise how examples are transformed by RBP compared to
other data compression methods for replay and show the classifica-
tion accuracy of existing compression methods applied to ER.
Figure 6 represents compressed examples from benchmark image
datasets with t-SNE [ 43] plots. The first row plots examples from
the Fashion MNIST dataset, and the second row plots examples
from the COIL-100 dataset. We compare examples from the original
dataset to those projected by RBP and those compressed by MRDC
and SHVQ. RBP and SHVQ are trained on the complete dataset
before transforming examples for each plot. On Fashion MNIST,
RBP segments classes incrementally more than MRDC and SHVQ,
where the classes in red and green are not segmented. COIL-100
examples projected by RBP closely resemble the original dataset,
while classes begin to lose shape after compression through MRDC
and SHVQ.
5.7 Distinguishability
Following our definition of distinguishability in Section 2, we com-
pare the distinguishability of classes in RBP with other baseline
compression methods. Table 4 shows the distinguishability of the
original datasets and of the datasets after examples are transformed
by MRDC, SHVQ and RBP. There is no variance of distinguisha-
bility on the raw datasets. Our results show that RBP achieves thehighest distinguishability over all datasets compared to existing
compression techniques.
Table 4: Distinguishability of benchmark image datasets after
projection by RBP and compression by baseline techniques.
A higher value indicates data is more distinguishable.
Methods Fashion MNIST CIFAR-100 Mini ImageNet
Raw Dataset 0.121 0 .063 0 .044
MRDC 0.119±0.01 0.061±0.02 0.049±0.02
SHVQ 0.139±0.05 0.074±0.04 0.062±0.05
RBP (Ours) 0.157±0.06 0.082±0.04 0.077±0.06
6 CONCLUSION
Our replay approach captures the distribution of a data stream by
approximating the frequency of features within classes, thereby
improving the efficacy of replay. We further posit that replay may
be improved by projecting examples in the replay buffer to a lower
dimension, increasing the number of examples that may be stored
without increased memory requirements. Our Sketch-based Replay
Projection method probabilistically selects and projects examples
from the data stream while preserving the distance between them.
We demonstrated that our models may be deployed on existing
replay architectures and more effectively populate the replay buffer
to reflect the distribution of a data stream while more efficiently
utilizing the memory allocation for replay.
ACKNOWLEDGMENTS
This research is supported by MBIE Strategic Science Investment
Fund (SSIF) Data Science platform - Time-Evolving Data Science /
Artificial Intelligence for Advanced Open Environmental Science
(UOWX1910).
 
1332Sketch-Based Replay Projection for Continual Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
REFERENCES
[1]Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu
Timofte, Luca Benini, and Luc V Gool. 2017. Soft-to-hard vector quantization for
end-to-end learning compressible representations. Advances in Neural Informa-
tion Processing Systems 30 (2017).
[2]Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Lau-
rent Charlin, and Tinne Tuytelaars. 2019. Online Continual Learning with Maxi-
mally Interfered Retrieval. Conference on Neural Information Processing Systems
(2019).
[3]Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. 2019. Gradi-
ent based sample selection for online continual learning. Advances in Neural
Information Processing Systems 32 (2019).
[4]Suresh Kumar Amalapuram, Thushara Tippi Reddy, Sumohana S Channappayya,
and Bheemarjuna Reddy Tamma. 2021. On handling class imbalance in continual
learning based network intrusion detection systems. In Proceedings of the First
International Conference on AI-ML Systems. 1–7.
[5]Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. 2022. Learning Fast, Learning
Slow: A General Continual Learning Method based on Complementary Learning
System. In International Conference on Learning Representations.
[6]Rosa I Arriaga and Santosh Vempala. 2006. An algorithmic theory of learning:
Robust concepts and random projection. Machine learning 63 (2006), 161–182.
[7]Benedikt Bagus and Alexander Gepperth. 2021. An investigation of replay-based
approaches for continual learning. In 2021 International Joint Conference on Neural
Networks (IJCNN). IEEE, 1–9.
[8]Jock Blackard. 1998. Covertype. UCI Machine Learning Repository. DOI:
https://doi.org/10.24432/C50K5N.
[9]Daniel Brignac, Niels Lobo, and Abhijit Mahalanobis. 2023. Improving Replay
Sample Selection and Storage for Less Forgetting in Continual Learning. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 3540–
3549.
[10] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone
Calderara. 2020. Dark experience for general continual learning: a strong, simple
baseline. Advances in neural information processing systems 33 (2020), 15920–
15930.
[11] Fernando E Casado, Dylan Lema, Marcos F Criado, Roberto Iglesias, Carlos V
Regueiro, and Senén Barro. 2022. Concept drift detection and adaptation for
federated and continual learning. Multimedia Tools and Applications (2022), 1–23.
[12] Aristotelis Chrysakis and Marie-Francine Moens. 2020. Online continual learning
from imbalanced data. In International Conference on Machine Learning. PMLR,
1952–1961.
[13] Li Deng. 2012. The MNIST database of handwritten digit images for machine
learning research. IEEE Signal Processing Magazine 29, 6 (2012), 141–142.
[14] Dmitriy Fradkin and David Madigan. 2003. Experiments with random projections
for machine learning. In Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining. 517–522.
[15] Yasir Ghunaim, Adel Bibi, Kumail Alhamoud, Motasem Alfarra, Hasan Abed
Al Kader Hammoud, Ameya Prabhu, Philip HS Torr, and Bernard Ghanem. 2023.
Real-time evaluation in online continual learning: A new hope. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11888–
11897.
[16] Saisubramaniam Gopalakrishnan, Pranshu Ranjan Singh, Haytham Fayek,
Savitha Ramasamy, and ArulMurugan Ambikapathi. 2022. Knowledge capture
and replay for continual learning. In Proceedings of the IEEE/CVF winter conference
on applications of computer vision. 10–18.
[17] Tyler L Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya, and Christopher
Kanan. 2020. Remind your neural network to prevent catastrophic forgetting. In
European Conference on Computer Vision. Springer, 466–483.
[18] Long He and Ho-Yin Mak. 2023. Prescriptive PCA: Dimensionality Reduction for
Two-stage Stochastic Optimization. KDD (2023).
[19] Stella Ho, Ming Liu, Lan Du, Longxiang Gao, and Yong Xiang. 2023. Prototype-
Guided Memory Replay for Continual Learning. IEEE Transactions on Neural
Networks and Learning Systems (2023).
[20] Michael E Houle, Hans-Peter Kriegel, Peer Kröger, Erich Schubert, and Arthur
Zimek. 2010. Can shared-neighbor distances defeat the curse of dimensionality?.
InScientific and Statistical Database Management: 22nd International Conference,
SSDBM 2010, Heidelberg, Germany, June 30–July 2, 2010. Proceedings 22. Springer,
482–500.
[21] Xisen Jin, Arka Sadhu, Junyi Du, and Xiang Ren. 2021. Gradient-based editing of
memory examples for online task-free continual learning. Advances in Neural
Information Processing Systems 34 (2021), 29193–29205.
[22] Chris Dongjoo Kim, Jinseo Jeong, and Gunhee Kim. 2020. Imbalanced continual
learning with partitioning reservoir sampling. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII
16. Springer, 411–428.
[23] Chris Dongjoo Kim, Jinseo Jeong, Sangwoo Moon, and Gunhee Kim. 2021. Con-
tinual learning on noisy data streams via self-purified replay. In Proceedings of
the IEEE/CVF international conference on computer vision. 537–547.[24] Lukasz Korycki and Bartosz Krawczyk. 2021. Class-incremental experience
replay for continual learning under concept drift. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 3649–3658.
[25] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features
from tiny images. https://www.cs.toronto.edu/~kriz/cifar.html
[26] Kai A Krueger and Peter Dayan. 2009. Flexible shaping: How learning in small
steps helps. Cognition 110, 3 (2009), 380–394.
[27] Huiwei Lin, Baoquan Zhang, Shanshan Feng, Xutao Li, and Yunming Ye. 2023.
PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual
Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 24246–24255.
[28] Divyam Madaan, Hongxu Yin, Wonmin Byeon, Jan Kautz, and Pavlo Molchanov.
2023. Heterogeneous Continual Learning. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. 15985–15995.
[29] Fionn Murtagh and Pierre Legendre. 2014. Ward’s hierarchical agglomerative
clustering method: which algorithms implement Ward’s criterion? Journal of
classification 31 (2014), 274–295.
[30] Nayar. 1996. Columbia Object Image Library (COIL100). https://cir.nii.ac.jp/
crid/1572824501191844864
[31] John Paparrizos and Luis Gravano. 2017. Fast and accurate time-series clustering.
ACM Transactions on Database Systems (TODS) 42, 2 (2017), 1–49.
[32] Quang Pham, Chenghao Liu, and Steven Hoi. 2021. Dualnet: Continual learning,
fast and slow. Advances in Neural Information Processing Systems 34 (2021),
16131–16144.
[33] Quang Pham, Chenghao Liu, Doyen Sahoo, and HOI Steven. 2020. Contextual
transformation networks for online continual learning. In International Confer-
ence on Learning Representations.
[34] Sachin Ravi and Hugo Larochelle. 2016. Optimization as a model for few-shot
learning. In International Conference on Learning Representations.
[35] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu,
and Gerald Tesauro. 2018. Learning to learn without forgetting by maximizing
transfer and minimizing interference. arXiv preprint arXiv:1810.11910 (2018).
[36] Fahad Sarfraz, Elahe Arani, and Bahram Zonooz. 2023. Error Sensitivity Mod-
ulation based Experience Replay: Mitigating Abrupt Representation Drift in
Continual Learning. International Conference for Learning Representations (2023).
[37] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and
Jongseong Jang. 2021. Online class-incremental continual learning with adversar-
ial shapley value. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 35. 9630–9638.
[38] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. 2017. Continual
learning with deep generative replay. Advances in neural information processing
systems 30 (2017).
[39] R Siddharth and Gnanasekaran Aghila. 2020. Randpro-A practical implementation
of random projection-based feature extraction for high dimensional multivariate
data analysis in R. SoftwareX 12 (2020), 100629.
[40] James Seale Smith, Junjiao Tian, Shaunak Halbe, Yen-Chang Hsu, and Zsolt Kira.
2023. A closer look at rehearsal-free continual learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2409–2419.
[41] Rishabh Tiwari, Krishnateja Killamsetty, Rishabh Iyer, and Pradeep Shenoy. 2022.
Gcr: Gradient coreset based replay buffer selection for continual learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .
99–108.
[42] Gido M Van de Ven, Hava T Siegelmann, and Andreas S Tolias. 2020. Brain-
inspired replay for continual learning with artificial neural networks. Nature
communications 11, 1 (2020), 4069.
[43] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of Machine Learning Research 9, 11 (2008).
[44] K. Lindenstrauss W. Johnson. 1982. Extensions of Lipschitz mappings into a
Hilbert space. Conference in Modern Analysis and Probability 26 (1982), 189–206.
[45] Liyuan Wang, Xingxing Zhang, Kuo Yang, Longhui Yu, Chongxuan Li, Lanqing
Hong, Shifeng Zhang, Zhenguo Li, Yi Zhong, and Jun Zhu. 2022. Memory
replay with data compression for continual learning. International Conference for
Learning Representations (2022).
[46] Kun Wei, Cheng Deng, Xu Yang, et al .2020. Lifelong Zero-Shot Learning.. In
International Joint Conference on Artificial Intelligence. 551–557.
[47] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 (2017).
[48] Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang. 2022. Online
coreset selection for rehearsal-based continual learning. International Conference
for Learning Representations (2022).
[49] Longhui Yu, Tianyang Hu, Lanqing Hong, Zhen Liu, Adrian Weller, and Weiyang
Liu. 2023. Continual Learning by Modeling Intra-Class Variation. Transactions
on Machine Learning Research (2023).
 
1333KDD ’24, August 25–29, 2024, Barcelona, Spain Jack Julian, Yun Sing Koh, & Albert Bifet
APPENDIX
A COMPUTATIONAL COMPLEXITY
RBS Complexity. The computational complexity of the RBS pro-
cess mainly depends on the dimensions of the matrices involved
and the operations performed. Let’s analyze the key components
of the update rule:
𝑅𝑖+1=𝜅·𝑅𝑖·vutΔ𝑅𝑖
𝑡𝑟
𝑅⊤
𝑖·𝑅𝑖
Here,𝑅𝑖is a matrix of size 𝑘×𝑛, and
Δ𝑅𝑖=𝑡𝑟
𝑅⊤
𝑖·(𝑋𝑅𝑃
𝑦1−𝑋𝑅𝑃
𝑦2)⊤·(𝑋𝑅𝑃
𝑦1−𝑋𝑅𝑃
𝑦2)·𝑅𝑖
.
The computational complexities of the key components are as
follows: Matrix multiplication 𝑅⊤
𝑖·𝑅𝑖:Assuming that 𝑅𝑖is a
dense matrix, the complexity is 𝑂(𝑘𝑛2).Calculate Δ𝑅𝑖:Complexity
𝑂(𝑛2)for the𝑅⊤
𝑖·(𝑋𝑅𝑃𝑦1−𝑋𝑅𝑃𝑦2)⊤operations. The dominant term
in the computation is the matrix multiplication 𝑅⊤
𝑖·𝑅𝑖, hence the
overall complexity for the update rule in RBS can be approximated
as𝑂(𝑘𝑛2).
CFS Complexity. As CFS leverages Count-Min Sketch to per-
form updates, the computational complexity of updating 𝐴𝑐is𝑂(𝑑).
For an example with 𝑛features, the update complexity is therefore
𝑂(𝑛𝑤).
B FURTHER PRELIMINARIES
TheCMS(𝐴𝑐,𝑥𝑗
𝑖,ℎ0,...,ℎ𝑑)function in Algorithm 1 is provided in
Algorithm 3.
Algorithm 3 Count-Min Sketch (CMS)
Input:𝐴𝑐- Sketch matrix of size 𝑤×𝑑,𝑥𝑗
𝑖- Feature to be
sketched,ℎ0,...,ℎ𝑑- Hash functions.
Output: Updated 𝐴𝑐.
1:for𝑎∈{0,...,𝑑}do
2:𝐴𝑐[𝑎][ℎ𝑎(𝑥𝑗
𝑖)]←𝐴𝑐[𝑎][ℎ𝑎(𝑥𝑗
𝑖)]+1
3:end for
4:return𝐴𝑐
We provide a short proof for the lemma used in Equation 3.
Theorem B.1. For any 0<𝜖<1and any integer n, let k be a
positive integer such that
𝑘≥24
3𝜖2−2𝜖3log𝑛.
Then for any set 𝑉of𝑛points in R𝑑, there is a map 𝑓: R𝑑→R𝑘
such that for all 𝑢,𝑣∈𝑉,
(1−𝜖)||𝑢−𝑣||2≤∥𝑓(𝑢)−𝑓(𝑣)∥2≤(1+𝜖)||𝑢−𝑣||2.
Proof. Let𝑋1,...,𝑋𝑑be𝑑independent random variables, and
let𝑌=1
∥𝑋∥(𝑋1,...,𝑋𝑑). Then𝑌is a point chosen randomly and
uniformly from the surface of the 𝑑-dimensional subspace 𝑆𝑑−1.
Letˆ𝑌∈R𝑘be the projection of 𝑌onto the first 𝑘coordinates, and
let𝐿=∥ˆ𝑌∥2. Then E[𝐿]=𝑘
𝑑[44].C EXPERIMENTS
Datasets. Here we provide a thorough description of each baseline
dataset used in or experiments. The MNIST dataset [ 13] consists
of70000 images of 28×28pixel greyscale images of handwritten
digits 0-9. The Fashion MNIST dataset [ 47] contains 70000 images
of28×28pixel greyscale images of various articles of clothing. The
CIFAR-10 dataset [ 25] contains 70000 coloured, photo-realistic 32×
32images of animals making up 10 classes. We consider a pixel to be
composed of three features corresponding to the RGB components.
The CIFAR-100 dataset [ 25] contains 70000 coloured, photo-realistic
32×32images of animals, vehicles and objects making up 100
classes of distinct items. We consider a pixel to be composed of three
features corresponding to the RGB components. The Mini ImageNet
dataset [ 34] contains 60000 coloured, photo-realistic images scaled
to64×64evenly spread over 100classes. Classes consist of everyday
items, vehicles and animals. As in CIFAR-100, we consider each pixel
to be composed of three features. The forest CoverType dataset
was obtained from US Forest Service (USFS) Region 2 Resource
Information System (RIS) data [ 8]. It contains 54features pertaining
to geological and geographical information and 581012 examples.
The COIL-100 [ 30] dataset contains 7200 128x128 images of 100
classes each representing a household object. There are 72images
per object each posed at a different angle. Our training set consists
of60examples for each class resulting in a training set of size
6000 and a test set of size 1200. The validation set is split from the
training set with the size of 500.
Distinguishability. Here, we provide further results for our
distinguishability experiments over all baseline datasets. Table 5
extends results presented in Table 4 to include other benchmark
datasets. We observe that RBP achieves a higher distinguishability
than the original dataset and other methods, with the exception of
the CoverType dataset, which may decrease in distinguishability
when features are reduced.
Table 5: Distinguishability of different benchmark image
datasets before and after projection by RBP and compression
by baseline techniques.
Methods MNIST CIFAR-10 CoverType COIL-100
Raw Dataset 0.11 0 .07 0 .10 0 .08
MRDC 0.10±0.01 0.07±0.02 0.08±0.02 0.07±0.01
SHVQ 0.11±0.02 0.08±0.03 0.09±0.04 0.09±0.02
RBP (Ours) 0.11±0.02 0.09±0.03 0.10±0.04 0.10±0.02
Compression Techniques. Table 6 provides further results
for comparing compression techniques with RBP over all baseline
datasets. Our results follow trends from Table 1 and demonstrate
RBP’s effectiveness when compared to existing compression tech-
niques for replay.
Run Time Comparison Here we present results on time re-
quirements for training SRP. From our discussion of time complexity
in Section A, we expect SRP to add some time requirements to train-
ing. Table 7 provides the number of seconds required to complete
training on benchmark datasets for our baseline replay methods
with and without SRP. We find that SRP increases training time by
approximately 10%.
 
1334Sketch-Based Replay Projection for Continual Learning KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 6: Classification accuracy of baseline replay algorithms with compression techniques for replay learning over baseline
datasets.
Methods MNIST Fashion MNIST CIFAR-10 CIFAR-100 Mini ImageNet CoverType COIL-100
ER [35] 29.61±0.16 27.88±0.18 31.81±0.19 18.25±0.21 15.23±0.24 24.27±0.32 13.06±1.05
ER+MRDC 31.69±0.19 29.19±0.21 32.11±0.25 19.98±0.42 16.98±0.31 24.01±0.29 13.44±1.21
ER+SHVQ 32.81±0.31 30.01±0.35 32.42±0.30 20.13±0.37 17.19±0.36 24.91±0.62 14.04±1.65
ER+RBP 33.12±0.10 31.24±0.19 32.80±0.31 21.01±0.19 18.13±0.30 25.04±0.55 14.51±1.36
GMED [21] 31.43±0.71 30.11±0.65 32.72±0.91 21.11±0.78 17.51±0.70 27.81±0.91 13.42±1.51
GMED+MRDC 31.93±0.73 30.94±0.74 33.04±0.73 21.42±0.94 17.96±0.73 25.64±2.08 14.12±1.63
GMED+SHVQ 32.75±0.81 31.51±0.95 34.01±0.99 22.31±0.92 19.42±0.81 27.71±1.42 14.99±1.58
GMED+RBP 33.51±0.89 32.98±0.77 34.95±1.21 22.91±0.81 20.01±0.89 28.01±1.04 15.19±1.47
MOCA [49] 60.61±0.58 58.51±0.62 61.74±0.69 30.05±1.02 10.62±0.45 21.39±0.39 13.51±1.72
MOCA+MRDC 60.85±0.64 58.99±0.63 62.73±0.94 31.58±1.14 10.93±0.63 21.32±0.41 13.92±1.93
MOCA+SHVQ 61.04±0.79 59.47±0.77 63.92±0.88 32.67±1.63 11.74±0.61 21.98±0.46 14.75±2.04
MOCA+RBP 61.51±0.65 60.01±0.67 64.50±0.78 33.01±1.21 12.51±0.58 22.31±0.70 15.51±2.01
PCR [27] 43.71±0.71 40.87±0.77 44.38±1.25 21.85±0.92 20.91±0.94 25.91±0.77 13.78±1.45
PCR+MRDC 43.92±0.74 41.29±0.96 45.82±0.95 22.82±1.73 21.14±0.84 25.71±0.99 14.01±1.46
PCR+SHVQ 44.02±0.93 42.53±0.84 46.93±1.38 23.95±1.46 22.98±1.42 25.95±1.21 14.86±1.28
PCR+RBP 44.01±0.76 43.81±0.79 47.06±1.25 24.37±0.89 23.85±1.04 26.01±0.84 15.31±1.36
ESMER [36] 50.51±0.65 45.72±0.71 55.31±0.64 31.51±1.01 20.84±1.05 22.95±0.65 13.91±1.52
ESMER+MRDC 51.82±0.69 46.35±0.84 56.83±0.93 32.71±1.24 22.48±1.34 22.51±0.92 14.02±1.45
ESMER+SHVQ 53.25±0.84 47.31±1.03 57.62±1.04 33.94±1.55 23.41±1.82 22.78±1.31 15.61±1.82
ESMER+RBP 53.74±0.66 47.99±0.83 58.11±0.71 34.64±1.36 24.45±1.51 23.08±0.79 16.01±1.62
Table 7: Comparison of model training time in seconds when compared to existing baselines with and without the SRP
component.
Methods MNIST Fashion MNIST CIFAR-10 CIFAR-100 Mini ImageNet CoverType COIL-100
ER [35] 7.42±1.01 8.22±1.13 42.32±4.41 52.32±3.51 104.23±7.63 3.51±0.11 39.92±3.12
ER+SRP 8.36±1.14 9.04±1.25 45.02±4.69 56.44±3.57 112.15±8.24 3.68±0.12 44.01±3.43
GMED [21] 10.22±1.30 14.04±0.72 150.16±7.28 241.32±12.67 481.08±23.55 7.94±0.81 252.36±12.21
GMED+SRP 11.24±1.43 15.45±0.79 165.76±8.00 266.45±13.98 531.91±26.09 8.73±0.89 277.60±13.45
MOCA [49] 30.31±1.19 32.95±2.28 289.95±21.89 512.71±26.35 1078.18±63.19 27.94±1.57 489.12±24.67
MOCA+SRP 33.35±1.31 36.25±2.52 318.12±24.16 564.98±29.09 1186.82±69.51 30.74±1.72 538.03±27.14
PCR [27] 20.63±1.36 23.49±2.76 251.63±15.01 462.63±21.46 786.51±68.37 19.74±1.06 398.82±34.18
PCR+SRP 22.69±1.42 26.18±2.92 276.42±16.51 508.90±23.60 867.89±75.21 21.71±1.16 438.70±37.60
ESMER [36] 33.31±1.86 39.95±2.23 314.95±19.35 574.43±23.95 1141.15±59.94 30.34±2.53 502.11±23.37
ESMER+SRP 36.61±2.05 43.95±2.45 346.45±21.29 630.87±26.35 1255.27±65.93 33.38±2.80 552.67±25.71
Table 8: Accuracy of Experience Replay (ER) with compres-
sion techniques for replay learning with noise introduced to
returning tasks in each dataset.
Methods Fashion MNIST CIFAR-100 Mini ImageNet
ER 25.85±0.23 16.15±0.20 14.19±0.34
ER+MRDC 26.61±0.25 17.38±0.31 15.45±0.31
ER+SHVQ 27.38±0.29 18.24±0.30 16.13±0.34
ER+RBP (Ours) 30.39±0.15 19.17±0.18 17.10±0.28Introduced Noise We investigate how changing returning tasks
with noise introduction influences the SRP process. We reorder our
baseline datasets Fashion MNIST, CIFAR-100 and Mini ImageNet so
that tasks reappear in the data stream. For a dataset with Ttasks,
we split each task 𝑇𝑖in half to create tasks 𝑇1
𝑖and𝑇2
𝑖. We then
arrange the tasks sequentially as follows: {𝑇1
1,...,𝑇1
T,𝑇2
1,...,𝑇2
T}.
For each task 𝑇2
𝑖, we introduce 20%Gaussian noise.
 
1335