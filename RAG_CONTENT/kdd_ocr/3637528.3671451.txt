Foundation Models for Time Series Analysis:
A Tutorial and Survey
Yuxuan Liang
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, China
yuxliang@outlook.comHaomin Wen
Beijing Jiao Tong University & The
Hong Kong University of Science and
Technology (Guangzhou)
Beijing, China
wenhaomin@bjtu.edu.cnYuqi Nie
Princeton University
Princeton, USA
ynie@princeton.edu
Yushan Jiang
University of Connecticut
Storrs, USA
yushan.jiang@uconn.eduMing Jin
Monash University
Melbourne, Australia
ming.jin@monash.eduDongjin Song
University of Connecticut
Storrs, USA
dongjin.song@uconn.edu
Shirui Pan
Griffith University
Brisbane, Australia
s.pan@griffith.edu.auQingsong Wen
Squirrel AI
Seattle, USA
qingsongedu@gmail.com
Abstract
Time series analysis stands as a focal point within the data min-
ing community, serving as a cornerstone for extracting valuable
insights crucial to a myriad of real-world applications. Recent ad-
vances in Foundation Models (FMs) have fundamentally reshaped
the paradigm of model design for time series analysis, boosting
various downstream tasks in practice. These innovative approaches
often leverage pre-trained or fine-tuned FMs to harness generalized
knowledge tailored for time series analysis. This survey aims to
furnish a comprehensive and up-to-date overview of FMs for time
series analysis. While prior surveys have predominantly focused on
either application or pipeline aspects of FMs in time series analysis,
they have often lacked an in-depth understanding of the underlying
mechanisms that elucidate why and how FMs benefit time series
analysis. To address this gap, our survey adopts a methodology-
centric classification, delineating various pivotal elements of time-
series FMs, including model architectures, pre-training techniques,
adaptation methods, and data modalities. Overall, this survey serves
to consolidate the latest advancements in FMs pertinent to time
series analysis, accentuating their theoretical underpinnings, recent
strides in development, and avenues for future exploration.
Q. Wen is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671451
Figure 1: Roadmaps of representative TSFMs.
CCS Concepts
•Information systems →Spatial-temporal systems.
Keywords
Time series, foundation model, deep learning
ACM Reference Format:
Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin
Song, Shirui Pan, and Qingsong Wen. 2024. Foundation Models for Time
Series Analysis: A Tutorial and Survey. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3671451
1 Introduction
Time series data are characterized by their sequential order and
temporal dependencies, encapsulating valuable information about
the dynamics of diverse systems and processes [ 44,89,117]. Various
time series data (e.g., stock price, traffic flow, electricity) present
unique challenges and opportunities for computational analysis,
6555
KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxuan Liang et al.
Table 1: Comparison between our survey and related surveys.
Sur
vey Taxonomy Standard TS Spatial TS Others (e.g., event)
Jin
et al. [47] Data ✔ ✔ ✘
Jiang et al. [44] Pipeline ✔ ✘ ✔
Zhang et al. [118] Pipeline ✔ ✘ ✘
Miller et al. [67] Pipeline ✔ ✘ ✘
(
Ours) Methodology ✔ ✔ ✔
each requiring tailored approaches to effectively capture their in-
herent properties. The analysis and understanding of time series
data is an important piece of data mining, facilitating crucial in-
sights and decisions in many domains [ 45,97], including finance
[19,68,112], healthcare [ 52,63], cloud computing [ 110,120], envi-
ronments [ 14,72], energy [ 75,127], and urban computing [ 82,91].
In recent years, the advancements of deep learning, especially
the transformer-based models [ 88], have revolutionized the field of
time series analysis [ 98]. Following the pioneering work [ 54], there
has been a surge in research interest exploring the application of
transformers for time series analysis [ 104,122,123]. The motiva-
tion behind deep learning and transformers lies in their ability to
automatically learn comprehensive representations from raw data,
thus capturing complex nonlinear relationships and temporal de-
pendencies without the need for manual feature engineering. Such
capability leads to significant performance improvements compared
with traditional statistical methods across numerous time series ap-
plications. Foundation models (FMs), such as large language models
(LLMs) in natural language processing (NLP) [ 121] and advanced
models in computer vision (CV) [ 2], have emerged as powerful
paradigms capable of achieving state-of-the-art performances in
their respective fields. The success of these FMs can be attributed
to their ability to leverage vast amounts of data to cultivate general-
purpose representations, subsequently fine-tuning them, or even
deploying them directly in a zero-shot manner to excel across a
diverse spectrum of downstream tasks. This approach not only
economizes on the need for task-specific model development but
also encapsulates a broad understanding of the world, endowing
these models with exceptional versatility and efficiency [6, 48].
Inspired by the remarkable achievements of FMs in broad do-
mains like CV and NLP, the concept of Time Series Foundation
Models (TSFMs) has garnered attention as a promising direction
for time series analysis. TSFMs aim to harness the power of the
foundation model paradigm to develop generalized models profi-
cient in understanding and forecasting time series data spanning
diverse domains. By capitalizing on large-scale time series datasets,
TSFMs hold the promise of attaining superior performance on a
spectrum of time series tasks, offering a unified framework that
can accelerate research and application developments in this field.
Despite the promising prospects and rapid development of TSFMs,
a systematic analysis of TSFMs from a methodological standpoint
has been notably absent in prior literature. Existing studies, as de-
picted in Table 1, have concentrated on either the data perspective
[47] or the pipeline perspective [ 44] of TSFMs. To bridge this gap,
this survey aims to provide a comprehensive methodological anal-
ysis of foundation models for learning a variety of time series. This
examination will center on scrutinizing their model architectures,
pre-training techniques, adaptation methods, and data modalities.Through this endeavor, we seek to illuminate an overall picture of
core elements in TSFMs, thereby enhancing comprehension regard-
ing the rationale behind their efficacy and the mechanisms driving
their substantial potential in time series analysis.
In contrast to previous surveys, this manuscript incorporates
the most extensive array of time series data types (see Table 1),
spatial time series, as well as other types such as the trajectory
and event. We further summarize the developmental roadmap of
current TSFMs in Figure 1, in order to foster further innovations
and understanding in the dynamic and ever-evolving landscape of
TSFMs. In short, our major contributions lie in three aspects:
•Comprehensive and up-to-date survey. We offer a compre-
hensive and up-to-date survey on foundation models for a wide
spectrum of time series, encompassing standard time series, spa-
tial time series, and other types (i.e., trajectories and events).
•Novel methodology-centric taxonomy. We introduce a novel
taxonomy that offers a thorough analysis from a methodological
standpoint on TSFMs with the first shot, enabling a full under-
standing of the mechanism on why and how FMs can achieve
admirable performance in time series data.
•Future research oppotunities. We discuss and highlight future
avenues for enhancing time series analysis using foundation
models, urging researchers to delve deeper into this area.
2 Background
Foundation Models. Foundation models (FMs), also known as
large pre-trained models, are a class of deep models that are pre-
trained on vast amounts of data, thus equipped with a wide range of
general knowledge and patterns. To this end, these models serve as
a versatile starting point for various tasks across different domains.
Specifically, FMs can be fine-tuned or adapted to specific tasks with
relatively small amounts of task-specific data, showcasing remark-
able flexibility and efficiency. In CV, FMs such as text-prompted
model CLIP [ 76] and visual-prompted model SAM [ 51] have pro-
pelled advancements in image recognition, object detection, and
more. In NLP, FMs such as BERT [ 25] and GPT-3 [ 8] have revolu-
tionized text understanding and generation tasks. Inspired by the
great success of FMs in the above domains, this survey delves into
the utilization of these models in the realm of time series analysis.
Concretely, we investigate TSFMs from a methodology view –
the components of FMs encompass the data modality, architecture,
pre-training, and adaptation techniques: 1) data modality refers to
the type of data used for model training, from single modality to
multi-modality; 2) architecture refers to which deep neural network
is adopted as the backbone of FM, with Transformers [ 88,98] being a
popular choice for their ability to handle sequential data effectively;
3) Pre-training involves how to train the model on large, diverse
datasets to gain a broad understanding of the data, using supervised
or self-supervised learning; 4) Adaptation, such as fine-tuning or
few-shot learning, is employed to accommodate the pre-trained FMs
to specific tasks. This comprehensive framework of FMs, spanning
from data modality to adaptation, facilitates the understanding of
using them in time series analysis.
Categories of Time Series. A time series is commonly described as
an ordered sequence of data points. Figure 2 depicts various types of
time series discussed in this survey, including standard time series,
6556Foundation Models for Time Series Analysis: A Tutorial and Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
Trajectory
Standard Time SeriesTimeSpatial Time SeriesSpatial
Spatial
TimeTimeEventTime
……Other Time Series
Figure 2: Illustration of various types of time series.
spatial time series, trajectories, and events. Note that trajectories
and events can be regarded as time series as each data point is
associated with a specific timestamp (and location), allowing for
analysis using time series techniques such as anomaly detection.
Definition 1 (Standard Time Series). It is defined as a sequence of
𝑇data points ordered by time, denoted by X={x1,x2,···,x𝑇}∈
R𝑇×𝐷, where x𝑡∈R𝐷is the data point at time step 𝑡, and𝐷is the
dimension of each data points. When 𝐷=1,Xis referred to as a
univariate time series, while 𝐷>1,Xis a multivariate time series.
Definition 2 (Spatial Time Series). It refers to a sequence of data
points with both temporal and spatial dimensions, which can be rep-
resented byX={X1,X2,···,X𝑇}∈R𝑁×𝑇×𝐷, where X𝑡∈R𝑁×𝐷
denotes the signals generated by 𝑁sensors with each equipped
with𝐷features. Besides, the 𝑁sensors are usually associated with
spatial correlations, according to which the spatial time series can be
further divided into two subtypes: i) spatio-temporal graph, when
the spatial correlation of those sensors is described by a graph 𝐺
with adjacent matrix A∈R𝑁×𝑁; ii) spatio-temporal raster, when
sensors are distributed uniformly as a grid in geographical space.
Definition 3 (Trajectory). A trajectory is a sequence of times-
tamped locations that describe the movements of an object in the
geographical space. It can be formulated as T={(𝑙1,𝑙2,···,𝑙𝑇}∈
R𝑇×2, where𝑙𝑡means the object’s location at time 𝑡.
Definition 4 (Event Sequence). An event sequence is a temporally
ordered set of events E={(𝑒1,𝑡1),(𝑒2,𝑡2),···,(𝑒𝑛,𝑡𝑛)}that de-
scribe the progression of actions or occurrences within a specific
context, where 𝑒𝑖is an event and 𝑡𝑖denotes the time when 𝑒𝑖occurs.
3 Taxonomy
The proposed taxonomy is illustrated in Figure 3. The proposed
taxonomy unfolds a structured and comprehensive classification
to enhance the understanding of foundation models on time series
analysis. It is organized into four hierarchical levels, starting with
the data category, followed by the model architecture, pre-training
techniques, and finally, the application domain. Unlike previous
taxonomies, ours distinguishes itself by delving deeper into the
foundation models from the methodology perspective, with a keen
focus on their architectural designs, pre-training, and adaptation
techniques. This method-centric view is pivotal for researchers,
providing valuable insights into the mechanisms of why and how
foundation models show great potential for time series analysis.
Diving into the details of data categories, we classify the time
series data into three distinct types: standard time series, spatial
time series, and others, which encompass trajectory and event data.
Standard time series data, characterized by their sequential order
and temporal dependencies, form the backbone of traditional timeseries analysis. Spatial time series data, on the other hand, introduce
an additional layer of complexity by incorporating geographical
or spatial information, making them crucial for applications in
urban computing and environmental monitoring. Lastly, the “others”
category, including trajectory and event data, represents diverse
datasets where time plays a critical role, such as the movement of
objects over time or the occurrence of specific events, offering a
broadened perspective on time series analysis.
From the methodology perspective: i) regarding model architec-
ture, the proposed taxonomy highlights three primary categories:
Transformer-based, non-Transformer-based, and diffusion-based
models. Transformer-based models leverage self-attention mecha-
nisms to capture long-range dependencies within time series, of-
fering significant advantages in handling sequential data. Non-
transformer-based models, with their diverse architectures, cater
to a wide range of time series tasks by efficiently processing tem-
poral patterns. Diffusion-based models, a novel addition, employ
stochastic processes to model the data generation process, present-
ing innovative solutions for time series analysis. ii) In terms of
pre-training techniques, the proposed taxonomy divides them into
fully-supervised and self-supervised methods, the latter of which
includes contrastive, generative, and hybrid approaches. This clas-
sification shows how different FMs are trained with or without
labels. iii) Adaptation strategies, such as zero-shot learning, prompt
engineering, tokenization, and fine-tuning, further exemplify the
versatility of FMs in customizing to specific time series applications.
4 Data Perspective
In this section, we explore advancements in TSFMs from various
data perspectives: standard time series ,spatial time series, and others.
We further categorize our discussion within each subsection into
task-oriented orgeneral-purpose foundation models.
4.1 Standard Time Series
Standard time series possess diverse properties, including varying
sampling rates and temporal patterns, which pose significant chal-
lenges in developing relevant FMs. These models aim to identify
universal patterns within extensive time series data from varied
sources, either to enhance specific tasks or for broad analysis.
Most of the existing attempts are in the category of task-oriented
standard time series foundation models. They leverage single or
multiple data modalities to craft robust models targeting particular
time series tasks, typically forecasting or classification. For models
involved only in a single (i.e., time series) modality, they may either
be developed from scratch or on existing pre-trained models from
other domains like large language or vision models [121].
In the first group, Lag-Llama [ 78] and TimeGPT-1 [ 34] repre-
sent typical forecasting foundation models. Both models undergo
pre-training on a vast collection of time series data spanning mul-
tiple domains. Lag-Llama employs a decoder-only transformer ar-
chitecture, utilizing lags as covariates, whereas TimeGPT-1 fea-
tures an encoder-decoder structure with several transformer lay-
ers, facilitating efficient zero-shot forecasting. Another notewor-
thy contribution is TTMs [ 31], a recent endeavor in creating a
domain-agnostic forecasting model built upon TSMixer [ 30], which
itself is pre-trained on diverse time series datasets from various
6557KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxuan Liang et al.
Foundation
Models
for Time SeriesStandar
d Time SeriesT
ransformer-basedPr
e-trained
LLM, AM, VLMGeneral:
Time-LLM [46], OFA [124], LLM4TS [11], PromptCast [105],
TEMPO [10], LLMTime [35], Voice2Series [108], AutoTimes [64], UniTime [61]
Finance:
Yuet al. [112], Chen et al. [19], Xie et al. [103], Wimmer et al. [99]
Healthcar
e: Liu et al. [63]
Self-sup
ervisedGenerativ
eGeneral: PatchTST
[70], Moirai [100], Lag-Llama [78],
TimeSiam [26], Timer [65], Das et al. [24], UniTS [33],
TimeGPT-1 [34], Chronos [1], MTSMAE [87]
Contrastiv
eGeneral: TEST
[86], TimeCLR [111]
Healthcar
e:METS [52]
Hybrid General: SimMTM
[27]
Fully-sup
ervised General:
TimeXer [93], UniTS [33]
non-
Transformer-base d
(MLP RNN CNN)Self-sup
ervisedGenerativ
e General: TSMixer
[30]
Contrastiv
e General: TF-C
[119] , TS2Vec [115] , CLUDA [71]
Fully-sup
ervised General:
TTMs [31], TimesNet [101], RWKV-TS [39]
Diffusion-base
dGeneral: TimeGrad
[79] , D3VAE [55] , TransFusion [85] , ScoreGrad [107] ,
Biloš et al. [5] , Crabbé et al. [23] , TimeDiff [83] , Wang et al. [92] , DiffTime [22]
Finance: FTS-Diffusion
[41]
Po
wer: DiffLoad [95]
Spatial
Time SeriesT
ransformer-base dPr
e-traine dLLMT
ransportation: ST-LLM [58], TPLLM [80]
General: GA
TGPT [18]
Self-sup
ervised Generativ
eT
ransportation: STEP [82]
Climate: W
-MAE [66], MetePFL [14], FengWu [13]
General: UniST
[113]
Fully-sup
ervisedT
ransportation: CPPBTR [29] , TFM [91]
Climate: Four
CastNet [72] , FedWing [15] , Pangu-Weather [3], ClimaX [69]
non-
Transformer-base d
(MLP RNN CNN)Self-sup
ervised General: SPGCL
[53] , STGCL [62]
Diffusion-base
d General: DiffST
G [96] , DSTPP [114] , DYffusion [9] , Yun et al. [116] , USTD [40] , PriSTI [60]
OthersT
ransformer-base dPr
e-traine dLLMMobility: A
uxMobLCast [106] , LLM-Mob [90]
Ev
ent: LAMP [84] , Gunjal & Durrett et al. [37]
Self-sup
ervisedGenerativ
eMobility: GTM
[56]
Ev
ent: NYUTron [43] , GatorTron [109]
Contrastiv
e Mobility: T
rajCL [12]
non-
Transformer-base d
(MLP RNN CNN)Self-sup
ervisedGenerativ
e Mobility: T
rembr [32]
Contrastiv
e Mobility: MMTEC
[57]
Hybrid Mobility: ST
ART [42]
Diffusion-base
d Mobility: T
rajGDM [21] , DiffTraj [126]
Figure 3: A comprehensive taxonomy of TSFMs, categorized according to data and methodologies.
domains. Echoing Lag-Llama’s appr oach, TimesFM [24]emerges
asadecoder-only modelexhibiting strongzero-shot forecasting
capabilities. Concurr ently ,Moirai [100]introduces anappr oach
with itsmaske denco der-base duniversal forecasting transformer ,
couple dwith anewpre-training dataset (LOTSA),containing 27
billion obser vations fromnine distinct domains. Additionally ,the
exploration extends todiffusion models likeTimeGrad [79]and
TransFusion [85],which primarily focusonoptimizing avariational
bound ondata likeliho od,transforming white noise intomeaningful
samples ofthetarget distribution.Pre-training fromscratch canbeexpensiv e,which hasspurr ed
thedevelopment ofalternativ eapproaches thatleverage pre-traine d
models fromother domains, such aslarge language ,vision, and
acoustic models. Forinstance ,LLM4TS [11]andTEMPO [10]suc-
cessfully perform time series forecasting acrossvarious datasets by
fine-tuning GPT-2[77]backb ones, predicate donthenotion that
LLMs canbeadapte dtoprocess non-linguistic datasets byactivat-
ingtheir inher entcapabiliti es.Similarly ,Voice2Series [108]engages
inthesynchr onization oftime series andacoustic data toharness
theclassification prowessofanacoustic modelfortime series data.
Another approach ispresente dbyWimmer etal.[99], who utilize
6558Foundation Models for Time Series Analysis: A Tutorial and Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
vision-language models (VLMs) to predict market changes. Beyond
fine-tuning existing models, a distinct methodology involves di-
rect inference from LLMs for time series forecasting, showcasing
commendable zero-shot performance. A notable example of this
is LLMTime [ 35], which introduces various strategies for effec-
tively tokenizing time series data and transforming discrete token
distributions into flexible continuous value densities.
Beyond approaches that focus solely on a single data modal-
ity of time series, there have been initiatives towards developing
multi-modal, task-oriented foundation models. A notable example
is Time-LLM [46], which introduces a reprogramming framework
to integrate textual and time series information, repurposing an
existing LLM into time series forecasters without additional compu-
tational costs. In a similar vein, METS [ 52] employs a trainable ECG
encoder alongside a frozen language model to process paired ECG
and clinical reports. Further, there is emerging research on directly
prompting LLMs for specific time series tasks. For instance, Prompt-
Cast [ 105] converts numerical inputs and outputs into prompts,
framing the forecasting task as a sentence-to-sentence conversion
to leverage language models directly for forecasting. Other studies,
such as one involving LLMs prompted with historical stock price
data, company metadata, and past economic/financial news, aim to
enhance stock return forecasting [ 112]. Another example combines
GNNs with ChatGPT to predict stock movements [ 19], illustrat-
ing the diverse applications of these methodologies. Additional
noteworthy efforts include [103] and [63].
Notably, recent efforts have been moved towards creating general-
purpose, single-modality standard TSFMs. TS2Vec [ 115] is a pio-
neering effort by introducing a universal framework for represent-
ing time series via contrastive learning. SimMTM [ 27] explores
cross-domain applications, where pre-trained models via masked
time series modeling exhibit superior fine-tuning performance in
forecasting and classification tasks. More recent works, such as
Timer [ 65] and UniTS [ 33], further advance the field by facilitat-
ing general time series analysis through single, large-scale pre-
trained models. Moreover, there is a growing interest in adapting
pre-trained models, such as LLMs, for broad time series analysis.
OFA [ 124] and TEST [ 86] exemplify this trend, though both ap-
proaches necessitate end-to-end fine-tuning for specific tasks.
4.2 Spatial Time Series
In complex real-world systems, time series data often display intri-
cate spatial dependencies alongside temporal dynamics, manifest-
ing in forms such as spatio-temporal graphs andrasters. Similar to
the discussion in Sec. 4.1, research on spatial time series typically
encompasses areas such as forecasting and classification. Unlike
foundation models for standard time series, most existing research
on spatial time series is still in its early stages, often characterized
by being domain-specific, single-modality, and task-oriented. In
the following, we categorize related works into two specific data
modalities and discuss them in different subsections.
4.2.1 Spatio-Temporal Graph. Most foundation models for spatio-
temporal graphs are task-oriented and only focused on graph data.
In the transportation sector, TFM [ 91] utilizes graph structures
and algorithms to analyze the behavior and interactions within
transportation systems, showing promising results in urban trafficforecasting. ST-LLM [ 58] combines spatio-temporal information
with a partially frozen LLM to improve traffic predictions, while
DiffSTG [ 96] applies denoising diffusion models to spatio-temporal
graphs for probabilistic traffic forecasting. Efforts towards domain-
agnostic models include STEP [ 82], which links spatio-temporal
GNNs with a pre-trained transformer for enhanced forecasting by
learning from extensive historical data. Similarly, STGCL [ 62] and
SPGCL [ 53] explore the integration of contrastive learning into
spatio-temporal graph forecasting, indicating its potential benefits.
Research on general-purpose models for spatio-temporal graphs is
limited. A notable example, USTD [ 40], introduces a unified model
for both forecasting and kriging tasks, employing an uncertainty-
aware diffusion approach to address diverse challenges effectively.
4.2.2 Spatio-Temporal Raster. Spatio-temporal raster refers to a
data modality that captures and organizes spatial information over
various time points in a grid-like format. This modality is primar-
ily utilized in climate foundation models. For instance, FourCast-
Net [ 72] is a global, data-driven weather forecasting model deliver-
ing accurate short to medium-range predictions worldwide. Similar
models, such as FengWu [ 13] and W-MAE [ 66], follow suit. No-
tably, Pangu-Weather [ 4], which is trained on 39 years of global data,
achieves superior deterministic forecasting outcomes across all eval-
uated variables compared to leading numerical weather prediction
systems. On a different note, ClimaX [ 69] aims at general-purpose
climate foundation models, pre-trained with diverse datasets cover-
ing various variables, spatio-temporal scopes, and physical contexts.
It is designed for fine-tuning across a wide array of climate and
weather-related tasks, such as forecasting, projection, and down-
scaling, even for atmospheric variables and spatio-temporal scales
not encountered during its pre-training phase. However, there is
a scarce number of domain-agnostic models for spatio-temporal
raster data. DYffusion [ 9], for example, capitalizes on the tempo-
ral dynamics inherent in raster data, integrating these dynamics
directly with the model’s diffusion steps to create a stochastic, time-
conditioned interpolator and forecasting network.
4.3 Others
In addition to standard and spatial time series, various other types
of data incorporate the temporal dimension, including trajecto-
ries, events, and clinical records. A majority of studies in this cat-
egory focus on trajectory data. For Transformer-based models,
AuxMobLCast [ 106] fine-tunes pre-trained LLMs through mobility
prompting and auxiliary POI Category classification to forecast hu-
man mobility patterns, effectively bridging the gap between natural
language processing and temporal sequence prediction. LLM-Mob
[90] encodes human mobility data into structured prompts that in-
struct LLMs to consider both long-term and short-term behavioral
patterns, along with time-specific context, to generate accurate and
explainable predictions of future locations. For non-Transformer-
based models, Trembr [32] leverages auto-encoding techniques to
extract road network and temporal information embedded in trajec-
tories effectively. While START [ 42] introduces a hybrid approach
to trajectory embedding learning by combining masked language
model [ 25] and SimCLR [ 17] to enhance its learning capability.
More recently, GTM [ 56] separates trajectory features into three
domains, which can be masked and generated independently to
6559KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxuan Liang et al.
(a) Transformer-based(b) Non-Transformer-based(c) Diffusion-based
Figure 4: Architectures of TSFMs.
meet specific input and output requirements of a given task. Then,
GTM is pre-trained by reconstructing densely sampled trajectories
in an auto-regressive manner given re-sampled sparse counter-
parts. For the diffusion-based model, DiffTraj [ 126] reconstructs
and synthesizes geographic trajectories from white noise through
a conditioned reverse trajectory denoising process.
5 Methodology Perspective
In this section, we dissect TSFMs from a methodology perspective,
focusing on architecture andpipeline (including pre-train and adap-
tation) intricacies. This discussion aims to elucidate the intricate
mechanisms driving these models’ efficacy and adaptability.
5.1 Architecture
As shown in Figure 4, we first delve into the architecture of TSFMs,
including Transformer-based models, non-Transformer-based nodels
anddiffusion-based models, focusing on the underlying mechanisms
that shape their capabilities, as well as how they could be applied
on various time series.
5.1.1 Transformer-based Models. The architecture of FMs has seen
a significant convergence towards the Transformer [ 88], a model
architecture first introduced for NLP tasks. The core innovation of
the Transformer lies in its utilization of the attention mechanism,
which allows the model to dynamically focus on different parts of
the input data. The attention function can be succinctly described
asAttention(𝑄,𝐾,𝑉)=Softmax(𝑄𝐾𝑇/√︁
𝑑𝑘)𝑉, where𝑄,𝐾, and
𝑉represent the queries, keys, and values matrices respectively,
each with dimensions 𝑇×𝑑𝑘, and𝑑𝑘serves as a scaling factor
to moderate the dot products’ magnitude. It is evident from the
formula that the attention mechanism has the capability to learn
global, long-range dependencies in data. This distinguishes it from
previous architectures, which were often limited by their local
receptive fields or dependency windows. Besides, the Transformer’s
design is inherently friendly to parallelization, which allows for
significant scalability, enabling the processing of large datasets
and the construction of models with billions of parameters. Such
scalability and efficiency in capturing intricate data patterns have
led to the widespread adoption of the Transformer architecture
beyond its initial application in NLP [ 25] to fields including CV,
speech, video, time series, and beyond.
The choice of foundation model framework remains debated
in the realm of time series analysis, contrasting the trend towards
decoder-only models in natural language processing. Notable works
in this area includes encoder-only [ 33,70,100], encoder-decoder
[1,26,34], and decoder-only [ 24,65,78] models. Ansari et al. [1]
analyze the applicability of the encoder-decoder framework todecoder-only models. Liu et al. [65] discuss that while the encoder-
only model is favored in time series forecasting for its effectiveness
on small datasets, the decoder-only architecture, with its strong
generalization and capacity, could be preferred for large-scale time
series models. The diversity in the architectural choices underscores
the potential and necessity for further exploration within this field.
In terms of standard time series analysis, the Transformer archi-
tecture leverages its sequence modeling capabilities to capture tem-
poral dynamics. This includes either repurposing pretrained LLMs
for time series to leverage their preexisting sequence modeling
strengths [ 105], or directly using Transformer as a base for TSFMs,
training from scratch to achieve models best suited for the specifics
of time series data [ 34]. Besides, various techniques have been inno-
vated to augment the functionality of Transformer models in time
series analysis comprehensively. A common practice in TSFMs seg-
ments time series into patches, which can effectively encapsulate lo-
cal dynamics within input tokens [ 10,11,20,24,46,70,78,100,124].
Another critical design is the normalization layer, where reversible
instance normalization [ 50] techniques, standardizing data through
instance-specific mean and variance then reverting it at the output
layer, have found extensive application across the above models.
Moreover, specialized approaches such as multi-resolution analy-
sis, exemplified by Moirai [ 100] through the employment of vary-
ing patch sizes, and decomposition strategies, as implemented by
TEMPO [ 10] via the separation of complex interactions into the
trend, seasonal, and residual components, have been shown to
enhance model efficacy substantially.
For spatial time series, the attention mechanism is utilized to
model both the spatial and temporal dependency. For instance, ST-
LLM [ 58] employs a novel partially frozen attention strategy for
traffic prediction, leveraging spatial-temporal embeddings to cap-
ture the intricate dynamics of traffic data across space and time.
Conversely, other studies opt for independent modeling of spa-
tial and temporal relationships. TFM [ 91] is a case in point, which
employs attention mechanisms within a dynamic graph encoder
for spatial modeling, integrating time encoding for temporal as-
pects, embodying principles of transformers in addressing traffic
system’s spatial-temporal dependencies. Besides simultaneously
modeling spatial and temporal relationships, there exists an alterna-
tive approach that augments the Transformer model with additional
spatial models or external spatial information to enhance its capa-
bilities in the temporal modeling of time series. An example of this
is STEP [ 82], which uses unsupervised pre-trained TransFormer
blocks to model temporal relationship from long-term history time
series, while applying a graph structure learner and spatio-temporal
GNNs based on the representation of TransFormer blocks. Further-
more, the application of Transformer models extends to the domain
of spatial-temporal prompt learning, as evidenced by initiatives
such as MetePFL [14] and FedWing [15].
In addition to conventional time series data, Transformer has
shown efficacy across diverse datasets, such as trajectory and health-
care records, as summarized in Figure 3. This expansion highlights
Transformer’s versatile capacity for temporal analysis.
5.1.2 Non-Transformer-based Models. Excluding the widespread
adoption of Transformers, a diverse array of traditional pre-training
methods leveraged models such as Multi-Layer Perceptrons (MLPs)
6560Foundation Models for Time Series Analysis: A Tutorial and Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
[31], Recurrent Neural Networks (RNNs) [ 32], and Convolutional
Neural Networks (CNNs) [ 101] as the backbone for pre-training.
These models, each with their unique strengths, are notable for
their effectiveness in both conventional and spatial time series data.
MLPs and CNNs are both acclaimed for their capabilities in
modeling spatial and temporal data effectively. CNN-based archi-
tectures, in particular, have garnered significant attention in self-
supervised learning for general time series representation, with
a notable emphasis on the usage of ResNet [ 27,119] and dilated
convolution layers [ 71,115] as foundational backbones. Those ap-
proaches predominantly employ 1D convolutional operations. In
contrast, TimesNet [ 101] introduces a novel perspective by convert-
ing 1D time series data into 2D tensors, facilitating the adaptive
identification of multi-periodicity and the extraction of complex
temporal variations through the use of a parameter-efficient incep-
tion block. MLP-based models, on the other hand, are lauded for
their lightweight design, offering benefits in terms of reduced com-
putational time and cost. TSMixer [ 30] and TTMs [ 31], as instances,
both claiming superior efficiency in memory usage and processing
speed while still delivering competitive performance.
RNNs have been acknowledged for their proficiency in temporal
data modeling [ 32,38]. Recently, there has been a resurgence of
interest in RNN architectures, which poses a compelling challenge
to the prevailing Transformer-based models. This trend is driven
by the quest for models that are not only more resource-efficient
but also adept at handling longer sequences through their inherent
linear complexity. A notable embodiment is the RWKV-TS [ 39],
which leverages the RWKV [ 74], an RNN-type foundation model
architecture, demonstrating promising potential for general time
series analysis. This emerging trend presents a valuable opportunity
for time series research and applications.
5.1.3 Diffusion-based Models. Diffusion-based FMs have gained
prominence in CV [ 73,81] and video [ 7] due to their proficiency in
learning complex data distributions, yet their exploration in time
series analysis remains nascent. These models function by gradually
introducing and then reversing noise to data, effectively learning
the generative process of original data through the reverse diffusion
process. This unique mechanism equips diffusion models with great
potential to serve as versatile foundation models capable of tackling
prediction, imputation, and anomaly detection in time series.
In standard time series and other temporal data, diffusion models
predict future states by capturing temporal dynamics, generating
smooth transitions from current to potential future states [ 79,95].
Applied to spatial time series, they extend this capability to model
spatial correlations alongside temporal ones, providing insights
into the interplay between space and time, particularly beneficial
in fields like traffic forecasting [96].
5.2 Pipeline
In this part, we review TSFMs from the pipeline perspective, in-
cluding diverse model acquisition and adaptation mechanisms.
5.2.1 Pre-training. Pre-training is an initial and crucial step for
building TSFMs, since the knowledge learned in this phase enables
the models to generalize across different contexts and quickly adapt
to various downstream tasks with minimal adaptations. On theother hand, the diverse nature of pre-training data ( e.g., standard
time series, spatial time series, and trajectories), as well as the way
the data is used, lead to a wide spectrum of pre-training mechanisms
when building and deploying foundation model. In this survey, we
propose a new perspective mostly based on learning objectives in
the pre-training phase, to categorize existing methods for TSFMs.
These mechanisms include fully-supervised, self-supervised (gener-
ative, contrastive, hybrid of generative and contrastive), and others.
Fully-supervised pre-training refers to the strategy where the
foundation model is initially trained on one or multiple large time
series datasets with labels to capture the complex temporal dynam-
ics and learn generalizable representations. TTMs [ 31] proposes a
universal time series foundation model supervised framework that
is able to handle the heterogeneity of multiple time series datasets
and effectively build the forecasting capability during pre-training,
via the design of multi-resolution enhancements (e.g., adaptive
patching, data augmentation via downsampling, etc.). LPTM [ 49]
on the other hand addresses the heterogeneity of different datasets
during pre-training by learning domain specific segmentation meth-
ods directly from data to optimize pre-training. Fully-supervised
pre-training for TSFMs is particularly suited for scenarios where
there is sufficient labeled historical data. Moreover, this pre-training
technique is more frequently used in some domain-specific applica-
tions such as transportation [ 29,91] and climate [ 3,72], where the
model can be directly tailored for downstream forecasting tasks
with the ease of minimal adaptations.
We categorize the generative pre-training strategy as a general
modeling of time series representations, including reconstruction
and probabilistic modeling of time series inputs. In reconstruction-
based pre-training, an effective learning objective is to recover the
original input space via masked autoencoding strategies [ 14,82]. In
the probabilistic modeling methods, the latent representation space
formed from temporal or spatial-temporal encoders is optimized to-
ward an estimated density via maximizing log-likelihood, based on
which the forecasts can be sampled [ 79,96]. Moreover, it is also ben-
eficial to leverage contrastive learning to enhance the robustness of
pre-training time series foundation models. The key is to construct
and utilize the self-supervision signals by generating informative
positive pairs as well as filtering out unsuitable negative pairs when
performing augmentation [ 62]. In addition to the aforementioned
two self-supervised strategies, the efficacy of the hybrid variant
has also been validated, where the pre-trained model on fewer time
series data outperforms the supervised counterpart [42].
In general, self-supervised pre-training enables foundation mod-
els to exploit the vast amounts of unlabeled time series data, pro-
viding generic temporal knowledge that can be further fine-tuned
for specific downstream tasks. Compared with fully-supervised
pre-training, it provides a more generic and realistic solution for
the acquisition of a time series foundation model.
Note that the above pre-training methods typically build the
model from scratch and obtain the universal knowledge from data
with the same modality (i.e., time series). Nevertheless, recent ad-
vancements in time series research have heightened the usage of
LLMs [ 10,11,19,35,37,46,58,63,64,84,90,103,105,106,112,124],
VLMs [ 99], and AMs [ 108] that are pre-trained from other data
modalities (text sequence, image-text sequence, acoustic signals).
6561KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxuan Liang et al.
5.2.2 Adaptation. The adaptation phase tailors TSFM to specific
tasks or datasets, enhancing its performance on those tasks by lever-
aging the learned generic temporal knowledge. We partition prior
methods into four branches, including direct usage, fine-tuning,
prompt engineering, and time series tokenization (see Figure 5).
Direct usage (also called zero-shot), means no further fine-tuning
on the target datasets, suggesting the sufficient capability of a
pre-trained model for downstream tasks. It can also indicate the
homogeneity between the pre-trained dataset and target dataset,
especially for some real-world applications where a foundation
model is built to fulfill domain-specific tasks [4, 13].
Fine-tuning is a common strategy to adapt foundation models to
target tasks. Based on the way the foundation model is used on the
target dataset, there are three mainstream works: fine-tuning the
whole model [ 66,69,72] or specific components (e.g., training posi-
tional embeddings and layer normalization, while keeping feedfor-
ward and attention layers frozen when fine-tuning LLMs) [ 11,124],
to directly infer results, or integrate foundation models as part of
the whole model [19, 52, 86, 106].
Prompt engineering is more specialized in LLM-based TSFMs.
The prompt can be handcrafted with task-specific textual input and
directly used to query the output for downstream prediction [ 90,
105] or intermediate embedding as feature enhancement [ 106]. Be-
sides, the prompt can also be parameterized vectors and end-to-end
learnable when optimizing the model on target datasets [ 10,86]. In
comparison to static prompts, the use of trainable prompts enhances
the ability of LLMs to comprehend and match the context of given
time series inputs. For example, TEMPO [ 10] constructs a trainable
prompt pool with distinct key-value pairs, and retrieves the most
representative prompt candidates with the highest similarity scores.
Time series tokenization aims to effectively represent the time
series as embeddings, which is also more frequently adopted in
transformer-based architectures [ 10,46,70]. Common tokeniza-
tion techniques include reversible instance normalization [ 50] that
mitigates distribution shift, patching with channel independence
strategy that effectively and efficiently extracts the time series con-
text [ 70], as well as the joint usage of time series decomposition
to explicitly represent explainable components [ 10] for the ease of
subsequent temporal modeling. In addition to the main branches
of adapting TSFMs, it is also worth noting that some fine-tuning
strategies take real-world constraints into account. For example, the
fine-tuning is performed in a privacy-preserved manner [14, 16].
5.3 Modality
During the pre-training/adaptation of TSFMs, prior methods in-
volve single or multiple data modalities, where standard time series,
trajectory, raster, and text can be treated as different forms with
unique domain perspectives. In this part, we review the data modal-
ities that are used in existing TSFMs across different domains.
5.3.1 Single-modality. A majority of current TSFMs are constructed
and tailored on the basis of single-modal data. Compared with
multi-modal methods, the single-modal time series modeling strat-
egy gains the advantages of inherent simplicity and bypasses the
challenges of handling modality gaps, yet frequently demonstrates
excellent empirical results across a wide range of real-world appli-
cations, such as traffic [58, 82] and climate forecasting [13, 69].
(a)Directusage(b)Tuning-based(c)Prompting-based(d)Tokenization-basedTSFM
inferenceForecastImputeDetectTSFM
inferenceForecastImputeDetectFine-tune
LLM
ForecastImputeDetectPromptEngineeringInstructionTSFM/LLM
ForecastImputeDetectPromptingInstruction
Token.
Figure 5: Illustration of different adaptation techniques.
5.3.2 Multi-modality. However, the single-modal methods may
not encapsulate the full picture for several challenging downstream
tasks in finance [ 19,112] and healthcare domains [ 52,63]. To cope
with this issue, there have been initiatives towards developing multi-
modal, task-oriented FMs, where additional information provides
useful information to enhance the model capability. In Chen et al.,
an external ChatGPT is queried to construct an evolving graph
structure representing companies, based on the analysis of news
headlines at specific time steps. As such, the inferred graph and
stock prices are fed into the time series model (that uses GNN and
LSTM for information propagation) to generate stock price move-
ment predictions. Another example in healthcare also demonstrated
the effectiveness of multi-modal medical context modeling, which
aligns the embedding of ECG (Electrocardiogram) and correspond-
ing medical text reports under a self-supervised contrastive learning
framework and performs ECG classification. In general multi-modal
time series analysis, similar cross-modality alignment strategies
(e.g., contrastive learning [ 86], reprogramming [ 46], token-wise
prompting [ 64]) are adopted, where the multi-modal inputs are of-
ten the textual description of datasets and pre-training word embed-
ding from LLMs. As a notable example, Time-LLM [ 46] introduces
a reprogramming framework that aligns the language knowledge
from pre-trained word embedding and time series information via
linear projection and multi-head attention, where the handcrafted
dataset descriptions are also used to quired text token embeddings
as prompts, which further enhances the embedding space and in-
forms the LLM to comprehend the task contexts. As such, utilizing
multi-modal data facilitates the repurpose of the existing LLM into
time series forecasters without additional computational costs.
6 Conclusion
The rapid development of FMs has revolutionized the research fields
in different domains. In this survey, we provide a comprehensive
and updated review of FMs specifically designed for time series anal-
ysis. A novel taxonomy is proposed from a methodology-centric
perspective by classifying FMs based on key components including
model architecture, pre-training technique, adaptation technique,
and data modality. Our survey facilitates understanding the under-
lying mechanism of applying the FMs to time series. Furthermore,
we believe that the consolidation of the latest advancements, as
well as the potential future direction (see Appendix), can inspire
more innovative works within the field of time series analysis.
Acknowledgements
This work is mainly supported by the Guangzhou-HKUST(GZ)
Joint Funding Program (No. 2024A03J0620). It is also funded by
Guangzhou Municipal Science and Technology Project 2023A03J0011.
6562Foundation Models for Time Series Analysis: A Tutorial and Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
References
[1]Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro
Mercado, Huibin Shen, Oleksandr Shchur, Syama Syndar Rangapuram, Sebas-
tian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix,
Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-
Schneider, and Yuyang Wang. 2024. Chronos: Learning the Language of Time
Series. arXiv preprint arXiv:2403.07815 (2024).
[2]Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer,
Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan.
2023. Foundational Models Defining a New Era in Vision: A Survey and Outlook.
arXiv preprint arXiv:2307.13721 (2023).
[3]Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian.
2023. Accurate medium-range global weather forecasting with 3D neural net-
works. Nature (2023), 1–6.
[4]Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian.
2023. Accurate medium-range global weather forecasting with 3D neural net-
works. Nature 619, 7970 (2023), 533–538.
[5]Marin Biloš, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and Stephan
Günnemann. 2022. Modeling temporal data as continuous functions with process
diffusion. arXiv preprint arXiv:2211.02590 (2022).
[6]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,
Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al .2021. On the opportunities and risks of foundation models.
arXiv preprint arXiv:2108.07258 (2021).
[7]Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David
Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and
Aditya Ramesh. 2024. Video generation models as world simulators. (2024).
https://openai.com/research/video-generation-models-as-world-simulators
[8]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[9]Salva Rühling Cachay, Bo Zhao, Hailey James, and Rose Yu. 2023. DYffusion:
A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting. arXiv
preprint arXiv:2306.01984 (2023).
[10] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye,
and Yan Liu. 2023. TEMPO: Prompt-based Generative Pre-trained Transformer
for Time Series Forecasting. arXiv preprint arXiv:2310.04948 (2023).
[11] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. 2023. LLM4TS: Two-Stage
Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. arXiv preprint
arXiv:2308.08469 (2023).
[12] Yanchuan Chang, Jianzhong Qi, Yuxuan Liang, and Egemen Tanin. 2023. Con-
trastive Trajectory Similarity Learning with Dual-Feature Attention. In 2023
IEEE 39th International Conference on Data Engineering (ICDE). IEEE, 2933–2945.
[13] Kang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, Xi
Chen, Leiming Ma, Tianning Zhang, Rui Su, et al .2023. FengWu: Pushing the
Skillful Global Medium-range Weather Forecast beyond 10 Days Lead. arXiv
preprint arXiv:2304.02948 (2023).
[14] Shengchao Chen, Guodong Long, Tao Shen, and Jing Jiang. 2023. Prompt
Federated Learning for Weather Forecasting: Toward Foundation Models on
Meteorological Data. In International Joint Conference on Artificial Intelligence.
[15] Shengchao Chen, Guodong Long, Tao Shen, Tianyi Zhou, and Jing Jiang. 2023.
Spatial-temporal Prompt Learning for Federated Weather Forecasting. arXiv
preprint arXiv:2305.14244 (2023).
[16] Shengchao Chen, Guodong Long, Tao Shen, Tianyi Zhou, and Jing Jiang.
2023. Spatial-temporal Prompt Learning for Federated Weather Forecasting.
arXiv:2305.14244 [cs.LG]
[17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.
A Simple Framework for Contrastive Learning of Visual Representations. In
ICML, Vol. 119. 1597–1607.
[18] Yakun Chen, Xianzhi Wang, and Guandong Xu. 2023. Gatgpt: A pre-trained large
language model with graph attention network for spatiotemporal imputation.
arXiv preprint arXiv:2311.14332 (2023).
[19] Zihan Chen, Lei Nico Zheng, Cheng Lu, Jialu Yuan, and Di Zhu. 2023. ChatGPT
Informed Graph Neural Network for Stock Movement Prediction. arXiv preprint
arXiv:2306.03763 (2023).
[20] Jinguo Cheng, Chunwei Yang, Wanlin Cai, Yuxuan Liang, and Yuankai Wu.
2024. NuwaTS: Mending Every Incomplete Time Series. arXiv preprint
arXiv:2405.15317 (2024).
[21] Chen Chu, Hengcai Zhang, Peixiao Wang, and Feng Lu. 2024. Simulating
human mobility with a trajectory generation framework based on diffusion
model. International Journal of Geographical Information Science (2024), 1–32.
[22] Andrea Coletta, Sriram Gopalakrishnan, Daniel Borrajo, and Svitlana Vyetrenko.
2024. On the constrained time-series generation problem. Advances in Neural
Information Processing Systems 36 (2024).
[23] Jonathan Crabbé, Nicolas Huynh, Jan Stanczuk, and Mihaela van der Schaar.
2024. Time Series Diffusion in the Frequency Domain. arXiv preprintarXiv:2402.05933 (2024).
[24] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. 2023. A
decoder-only foundation model for time-series forecasting. arXiv preprint
arXiv:2310.10688 (2023).
[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
InNAACL-HLT. 4171–4186.
[26] Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, Jianmin
Wang, and Mingsheng Long. 2024. TimeSiam: A Pre-Training Framework for
Siamese Time-Series Modeling. arXiv preprint arXiv:2402.02475 (2024).
[27] Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Ming-
sheng Long. 2023. SimMTM: A Simple Pre-Training Framework for Masked
Time-Series Modeling. Advances in Neural Information Processing Systems (2023).
[28] Yuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, and
Chongjun Wang. 2021. Adarnn: Adaptive learning and forecasting of time series.
InCIKM. 402–411.
[29] Wenying Duan, Liu Jiang, Ning Wang, and Hong Rao. 2019. Pre-Trained Bidirec-
tional Temporal Representation for Crowd Flows Prediction in Regular Region.
IEEE Access 7 (2019), 143855–143865.
[30] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant
Kalagnanam. 2023. TSMixer: Lightweight MLP-Mixer Model for Multivariate
Time Series Forecasting. arXiv preprint arXiv:2306.09364 (2023).
[31] Vijay Ekambaram, Arindam Jati, Nam H Nguyen, Pankaj Dayama, Chandra
Reddy, Wesley M Gifford, and Jayant Kalagnanam. 2024. TTMs: Fast Multi-
level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of
Multivariate Time Series. arXiv preprint arXiv:2401.03955 (2024).
[32] Tao-Yang Fu and Wang-Chien Lee. 2020. Trembr: Exploring Road Networks for
Trajectory Representation Learning. ACM TIST 11, 1 (2020), 10:1–10:25.
[33] Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros
Tsiligkaridis, and Marinka Zitnik. 2024. UniTS: Building a Unified Time Series
Model. arXiv preprint arXiv:2403.00131 (2024).
[34] Azul Garza and Max Mergenthaler-Canseco. 2023. TimeGPT-1. arXiv preprint
arXiv:2310.03589 (2023).
[35] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. 2023. Large
Language Models Are Zero-Shot Time Series Forecasters. Advances in Neural
Information Processing Systems (2023).
[36] Albert Gu and Tri Dao. 2023. Mamba: Linear-Time Sequence Modeling with
Selective State Spaces. arXiv:2312.00752 [cs.LG]
[37] Anisha Gunjal and Greg Durrett. 2023. Drafting Event Schemas using Language
Models. arXiv preprint arXiv:2305.14847 (2023).
[38] Hansika Hewamalage, Christoph Bergmeir, and Kasun Bandara. 2021. Recurrent
neural networks for time series forecasting: Current status and future directions.
International Journal of Forecasting 37, 1 (2021), 388–427.
[39] Haowen Hou and F Richard Yu. 2024. RWKV-TS: Beyond Traditional Recurrent
Neural Network for Time Series Tasks. arXiv preprint arXiv:2401.09093 (2024).
[40] Junfeng Hu, Xu Liu, Zhencheng Fan, Yuxuan Liang, and Roger Zimmermann.
2023. Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal
Graph Learning. arXiv:2310.17360 [cs.LG]
[41] Hongbin Huang, Minghua Chen, and Xiao Qiao. 2024. Generative Learning for
Financial Time Series with Irregular and Scale-Invariant Patterns. In The Twelfth
International Conference on Learning Representations. https://openreview.net/
forum?id=CdjnzWsQax
[42] Jiawei Jiang, Dayan Pan, Houxing Ren, Xiaohan Jiang, Chao Li, and Jingyuan
Wang. 2022. Self-supervised Trajectory Representation Learning with Temporal
Regularities and Travel Semantics. CoRR abs/2211.09510 (2022).
[43] Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin,
Duo Wang, Anas Abidin, Kevin Eaton, Howard Antony Riina, Ilya Laufer,
Paawan Punjabi, et al .2023. Health system-scale language models are all-
purpose prediction engines. Nature (2023), 1–6.
[44] Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, Yuriy
Nevmyvaka, and Dongjin Song. 2024. Empowering Time Series Analysis with
Large Language Models: A Survey. arXiv preprint arXiv:2402.03182 (2024).
[45] Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Ge-
offrey I Webb, Irwin King, and Shirui Pan. 2023. A survey on graph neural
networks for time series: Forecasting, classification, imputation, and anomaly
detection. arXiv preprint arXiv:2307.03759 (2023).
[46] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi,
Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al .2023. Time-LLM:
Time series forecasting by reprogramming large language models. arXiv preprint
arXiv:2310.01728 (2023).
[47] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang,
James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al .2023. Large models for
time series and spatio-temporal data: A survey and outlook. arXiv preprint
arXiv:2310.10196 (2023).
[48] Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jin-
dong Wang, Shirui Pan, and Qingsong Wen. 2024. Position: What Can Large
Language Models Tell Us about Time Series Analysis. In International Conference
on Machine Learning (ICML’24).
6563KDD ’24, August 25–29, 2024, Barcelona, Spain Yuxuan Liang et al.
[49] Harshavardhan Kamarthi and B Aditya Prakash. 2023. Large Pre-trained time
series models for cross-domain Time series analysis tasks. arXiv preprint
arXiv:2311.11413 (2023).
[50] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and
Jaegul Choo. 2021. Reversible instance normalization for accurate time-series
forecasting against distribution shift. In ICLR.
[51] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al .
2023. Segment anything. In ICCV. 4015–4026.
[52] Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. 2023. Frozen
Language Model Helps ECG Zero-Shot Learning. In Medical Imaging with Deep
Learning.
[53] Rongfan Li, Ting Zhong, Xinke Jiang, Goce Trajcevski, Jin Wu, and Fan Zhou.
2022. Mining spatio-temporal relations via self-paced graph contrastive learning.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 936–944.
[54] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,
and Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-
neck of transformer on time series forecasting. Advances in neural information
processing systems 32 (2019).
[55] Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. 2022. Generative time series
forecasting with diffusion, denoise, and disentanglement. Advances in Neural
Information Processing Systems 35 (2022), 23009–23022.
[56] Yan Lin, Jilin Hu, Shengnan Guo, Bin Yang, Christian S. Jensen, Youfang Lin, and
Huaiyu Wan. 2024. GTM: General Trajectory Modeling with Auto-regressive
Generation of Feature Domains. arXiv:2402.07232 [cs.LG]
[57] Yan Lin, Huaiyu Wan, Shengnan Guo, Jilin Hu, Christian S. Jensen, and Youfang
Lin. 2023. Pre-training General Trajectory Embeddings with Maximum Multi-
view Entropy Coding. arXiv:2207.14539 [cs.CV]
[58] Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li, and
Rui Zhao. 2024. Spatial-temporal large language model for traffic prediction.
arXiv preprint arXiv:2401.10134 (2024).
[59] Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Ka-
marthi, Aditya B Sasanur, Megha Sharma, Jiaming Cui, Qingsong Wen, Chao
Zhang, and B Aditya Prakash. 2024. Time-MMD: A New Multi-Domain Multi-
modal Dataset for Time Series Analysis. arXiv preprint arXiv:2406.08627 (2024).
[60] Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and Yanjie Fu. 2023.
PriSTI: A Conditional Diffusion Framework for Spatiotemporal Imputation.
arXiv preprint arXiv:2302.09746 (2023).
[61] Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, and
Roger Zimmermann. 2024. UniTime: A Language-Empowered Unified Model
for Cross-Domain Time Series Forecasting. arXiv:2310.09751 [cs.LG]
[62] Xu Liu, Yuxuan Liang, Chao Huang, Yu Zheng, Bryan Hooi, and Roger Zimmer-
mann. 2022. When do contrastive learning signals help spatio-temporal graph
forecasting?. In Proceedings of the 30th International Conference on Advances in
Geographic Information Systems. 1–12.
[63] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine,
Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel.
2023. Large Language Models are Few-Shot Health Learners. arXiv preprint
arXiv:2305.15525 (2023).
[64] Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingsheng Long.
2024. AutoTimes: Autoregressive Time Series Forecasters via Large Language
Models. arXiv preprint arXiv:2402.02370 (2024).
[65] Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and
Mingsheng Long. 2024. Timer: Transformers for Time Series Analysis at Scale.
arXiv preprint arXiv:2402.02368 (2024).
[66] Xin Man, Chenghong Zhang, Changyu Li, and Jie Shao. 2023. W-MAE: Pre-
trained weather model with masked autoencoder for multi-variable weather
forecasting. arXiv preprint arXiv:2304.08754 (2023).
[67] John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas
Rana, I. Budak Arpinar, and Ninghao Liu. 2024. A Survey of Deep Learning and
Foundation Models for Time Series Forecasting. arXiv:2401.13912 [cs.LG]
[68] John M Mulvey, Junhan Gu, Margaret Holen, and Yuqi Nie. 2022. Applications
of Machine Learning in Wealth Management. Journal of Investment Consulting
21, 1 (2022), 66–82.
[69] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and
Aditya Grover. 2023. ClimaX: A foundation model for weather and climate.
International Conference on Machine Learning (2023).
[70] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.
A time series is worth 64 words: Long-term forecasting with transformers. arXiv
preprint arXiv:2211.14730 (2022).
[71] Yilmazcan Ozyurt, Stefan Feuerriegel, and Ce Zhang. 2022. Contrastive
learning for unsupervised domain adaptation of time series. arXiv preprint
arXiv:2206.06243 (2022).
[72] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh
Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li,
Kamyar Azizzadenesheli, et al .2022. Fourcastnet: A global data-driven high-
resolution weather model using adaptive fourier neural operators. arXiv preprintarXiv:2202.11214 (2022).
[73] William Peebles and Saining Xie. 2023. Scalable diffusion models with trans-
formers. In ICCV. 4195–4205.
[74] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV,
et al.2023. Rwkv: Reinventing rnns for the transformer era. arXiv preprint
arXiv:2305.13048 (2023).
[75] Dalin Qin, Chenxi Wang, Qingsong Wen, Weiqi Chen, Liang Sun, and Yi Wang.
2023. Personalized federated darts for electricity load forecasting of individual
buildings. IEEE Transactions on Smart Grid (2023).
[76] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-
hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al .
2021. Learning transferable visual models from natural language supervision.
InInternational conference on machine learning. PMLR, 8748–8763.
[77] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et al .2019. Language models are unsupervised multitask learners.
OpenAI blog 1, 8 (2019), 9.
[78] Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George
Adamopoulos, Rishika Bhagwatkar, Marin Biloš, Hena Ghonia, Nadhir Vincent
Hassen, Anderson Schneider, et al .2023. Lag-llama: Towards foundation models
for time series forecasting. arXiv preprint arXiv:2310.08278 (2023).
[79] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Au-
toregressive denoising diffusion models for multivariate probabilistic time series
forecasting. In International Conference on Machine Learning. PMLR, 8857–8868.
[80] Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, and Zhiyong Cui.
2024. TPLLM: A Traffic Prediction Framework Based on Pretrained Large
Language Models. arXiv preprint arXiv:2403.02221 (2024).
[81] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer. 2022. High-resolution image synthesis with latent diffusion models. In
CVPR. 10684–10695.
[82] Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training en-
hanced spatial-temporal graph neural network for multivariate time series
forecasting. In KDD. 1567–1577.
[83] Lifeng Shen and James Kwok. 2023. Non-autoregressive Conditional Diffusion
Models for Time Series Prediction. arXiv preprint arXiv:2306.05043 (2023).
[84] Xiaoming Shi, Siqiao Xue, Kangrui Wang, Fan Zhou, James Y Zhang, Jun Zhou,
Chenhao Tan, and Hongyuan Mei. 2023. Language Models Can Improve Event
Prediction by Few-Shot Abductive Reasoning. In Advances in Neural Information
Processing Systems.
[85] Md Fahim Sikder, Resmi Ramachandranpillai, and Fredrik Heintz. 2023. Trans-
fusion: generating long, high fidelity time series using diffusion models with
transformers. arXiv preprint arXiv:2307.12667 (2023).
[86] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. 2023. TEST: Text
Prototype Aligned Embedding to Activate LLM’s Ability for Time Series. arXiv
preprint arXiv:2308.08241 (2023).
[87] Peiwang Tang and Xianchao Zhang. 2022. MTSMAE: Masked Autoencoders for
Multivariate Time-Series Forecasting. In 2022 IEEE 34th International Conference
on Tools with Artificial Intelligence (ICTAI). IEEE, 982–989.
[88] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. In Advances in Neural Information Processing Systems 30. 5998–6008.
[89] Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, and
Qingsong Wen. 2024. Deep Learning for Multivariate Time Series Imputation:
A Survey. arXiv preprint arXiv:2402.04059 (2024).
[90] Xinglei Wang, Meng Fang, Zichao Zeng, and Tao Cheng. 2023. Where
Would I Go Next? Large Language Models as Human Mobility Predictors.
arXiv:2308.15197 [cs.AI]
[91] Xuhong Wang, Ding Wang, Liang Chen, and Yilun Lin. 2023. Building Trans-
portation Foundation Model via Generative Graph Transformer. arXiv preprint
arXiv:2305.14826 (2023).
[92] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang,
Zhengyang Zhou, and Yang Wang. 2023. An observed value consistent dif-
fusion model for imputing missing values in multivariate time series. In KDD.
2409–2418.
[93] Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Yunzhong Qiu, Haoran
Zhang, Jianmin Wang, and Mingsheng Long. 2024. TimeXer: Empowering
Transformers for Time Series Forecasting with Exogenous Variables. arXiv
preprint arXiv:2402.19072 (2024).
[94] Zepu Wang, Yuqi Nie, Peng Sun, Nam H Nguyen, John Mulvey, and H Vincent
Poor. 2023. St-mlp: A cascaded spatio-temporal linear framework with channel-
independence strategy for traffic forecasting. arXiv preprint arXiv:2308.07496
(2023).
[95] Zhixian Wang, Qingsong Wen, Chaoli Zhang, Liang Sun, and Yi Wang. 2023.
DiffLoad: uncertainty quantification in load forecasting with diffusion model.
arXiv preprint arXiv:2306.01001 (2023).
[96] Haomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Qingsong Wen, Roger
Zimmermann, and Yuxuan Liang. 2023. DiffSTG: Probabilistic spatio-temporal
graph forecasting with denoising diffusion models. In the 31st ACM International
6564Foundation Models for Time Series Analysis: A Tutorial and Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
Conference on Advances in Geographic Information Systems. 1–12.
[97] Qingsong Wen, Linxiao Yang, Tian Zhou, and Liang Sun. 2022. Robust time
series analysis and applications: An industrial perspective. In 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 4836–4837.
[98] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,
and Liang Sun. 2023. Transformers in time series: A survey. In International
Joint Conference on Artificial Intelligence(IJCAI). 6778–6786.
[99] Christopher Wimmer and Navid Rekabsaz. 2023. Leveraging vision-language
models for granular market change prediction. arXiv preprint arXiv:2301.10166
(2023).
[100] Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese,
and Doyen Sahoo. 2024. Unified training of universal time series forecasting
transformers. arXiv preprint arXiv:2402.02592 (2024).
[101] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2022. Timesnet: Temporal 2d-variation modeling for general time series
analysis. In The eleventh international conference on learning representations.
[102] Yutong Xia, Yuxuan Liang, Haomin Wen, Xu Liu, Kun Wang, Zhengyang Zhou,
and Roger Zimmermann. 2024. Deciphering spatio-temporal graph forecasting:
A causal lens and treatment. NeurIPS 36 (2024).
[103] Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023.
The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal
Stock Movement Prediction Challenges. arXiv preprint arXiv:2304.05351 (2023).
[104] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly
Transformer: Time Series Anomaly Detection with Association Discrepancy. In
International Conference on Learning Representations.
[105] Hao Xue and Flora D Salim. 2022. PromptCast: A New Prompt-based Learning
Paradigm for Time Series Forecasting. arXiv preprint arXiv:2210.08964 (2022).
[106] Hao Xue, Bhanu Prakash Voutharoja, and Flora D Salim. 2022. Leveraging
language foundation models for human mobility forecasting. In the 30th Inter-
national Conference on Advances in Geographic Information Systems. 1–9.
[107] Tijin Yan, Hongwei Zhang, Tong Zhou, Yufeng Zhan, and Yuanqing Xia. 2021.
ScoreGrad: Multivariate probabilistic time series forecasting with continuous
energy-based generative models. arXiv preprint arXiv:2106.10121 (2021).
[108] Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen. 2021. Voice2series:
Reprogramming acoustic models for time series classification. In International
conference on machine learning. PMLR, 11808–11819.
[109] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith,
Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G
Flores, et al .2022. A large language model for electronic health records. NPJ
Digital Medicine 5, 1 (2022), 194.
[110] Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. 2023.
Dcdetector: Dual attention contrastive representation learning for time series
anomaly detection. In Proceedings of the 29th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining. 3033–3045.
[111] Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng, Yujie Fan, Audrey
Der, Vivian Lai, Zhongfang Zhuang, Junpeng Wang, Liang Wang, et al .[n. d.].
Toward a foundation model for time series data. In Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management.
[112] Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, Zongyi Liu, and Yanbin Lu.
2023. Temporal Data Meets LLM–Explainable Financial Time Series Forecasting.
arXiv preprint arXiv:2306.11025 (2023).
[113] Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, and Yong Li. 2024. UniST: A
Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction.
arXiv:2402.11838 [cs.LG]
[114] Yuan Yuan, Jingtao Ding, Chenyang Shao, Depeng Jin, and Yong Li. 2023. Spatio-
temporal Diffusion Point Processes. arXiv preprint arXiv:2305.12403 (2023).
[115] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang,
Yunhai Tong, and Bixiong Xu. 2022. Ts2vec: Towards universal representation
of time series. In AAAI, Vol. 36. 8980–8987.
[116] Taeyoung Yun, Haewon Jung, and Jiwoo Son. 2023. Imputation as Inpainting:
Diffusion models for SpatioTemporal Data Imputation. (2023).
[117] Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu,
James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al .2023. Self-
supervised learning for time series analysis: Taxonomy, progress, and prospects.
arXiv preprint arXiv:2306.10125 (2023).
[118] Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, and Jingbo Shang. 2024.
Large Language Models for Time Series: A Survey. arXiv:2402.01801 [cs.LG]
[119] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. [n. d.].
Self-supervised contrastive pre-training for time series via time-frequency con-
sistency. Advances in Neural Information Processing Systems 35 ([n. d.]).
[120] Yingying Zhang, Zhengxiong Guan, Huajie Qian, Leili Xu, Hengbo Liu, Qing-
song Wen, Liang Sun, Junwei Jiang, Lunting Fan, and Min Ke. 2021. CloudRCA:
A root cause analysis framework for cloud computing platforms. In CIKM.
4373–4382.
[121] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al .2023. A survey
of large language models. arXiv preprint arXiv:2303.18223 (2023).[122] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-
quence time-series forecasting. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 35. 11106–11115.
[123] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
2022. Fedformer: Frequency enhanced decomposed transformer for long-term
series forecasting. In International conference on machine learning. 27268–27286.
[124] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. 2023. One Fits
All: Power General Time Series Analysis by Pretrained LM. Advances in Neural
Information Processing Systems (2023).
[125] Zhengyang Zhou, Qihe Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang,
Yuxuan Liang, and Yang Wang. 2023. Maintaining the Status Quo: Capturing
Invariant Relations for OOD Spatiotemporal Learning. In KDD. 3603–3614.
[126] Yuanshao Zhu, Yongchao Ye, Shiyao Zhang, Xiangyu Zhao, and James Yu. 2024.
Difftraj: Generating gps trajectory with diffusion probabilistic model. Advances
in Neural Information Processing Systems 36 (2024).
[127] Zhaoyang Zhu, Weiqi Chen, Rui Xia, Tian Zhou, Peisong Niu, Bingqing Peng,
Wenwei Wang, Hengbo Liu, Ziqing Ma, Xinyue Gu, et al .2023. Energy fore-
casting with robust, flexible, and explainable machine learning algorithms. AI
Magazine 44, 4 (2023), 377–393.
A Appendix
In this section, we discuss the future research directions.
Incooporating Multi-modalities. As illustrated in this survey,
a majority of current foundation models for time series are based on
a single modality. However, many real-world dynamic systems are
coupled with various modalities (time series, text, even image data).
The recent introduction of the Time-MMD [ 59] represents a pio-
neering effort in this domain, offering a high-quality multi-domain
multimodal time series dataset and demonstrating the superiority
of incorporating multimodality in time-series forecasting. It would
be a promising direction to leverage various modalities along with
the time series in TSFM to learn more comprehensive and general-
ized knowledge, therefore significantly boosting the performance
of different downstream tasks.
Exploring more Efficient Architectures. Currently, the Trans-
former serves as the dominant architecture for building the foun-
dation model. Though promising, Transformer-based foundation
models have quadratic scaling with respect to the sequence length
due to their self-attention mechanism. This makes them compu-
tationally expensive and memory-intensive for processing long
sequences. Therefore, it is an interesting avenue for future study
to explore more efficient FM backbone architectures, such as state-
space models Mamba [36].
Developing more Effective Pipelines. Time series data has
unique properties such as temporal distribution shift [ 28,94,125]
(i.e., the data distribution will evolve over time) and causality (i.e.,
casual relationship can exist between different points in the time
series) [ 102]. Therefore, it would be another existing as well as
challenging future direction to develop TSFMs that can well address
the temporal distribution shift or have a powerful Interpretability
for downstream tasks.
Protecting Privacy. Protecting privacy is an essential concern
when training foundation models on diverse sources and modali-
ties of data, which raises potential risks of exposing sensitive in-
formation. As such, one future direction is the development of
robust privacy-preserving techniques for training the TSFM from
multi-source datasets, as well as keeping the utility of the trained
FMs. This may include the advancement of federated learning ap-
proaches, where models can be trained across multiple decentral-
ized devices or servers without exchanging raw data.
6565