Interpretable Transformer Hawkes Processes: Unveiling Complex
Interactions in Social Networks
Zizhuo Meng∗
The University of Technology Sydney
Sydney, Australia
zizhuo.meng@student.uts.edu.auKe Wan∗
University of Illinois
Urbana-Champaign
Illinois, United States
kewan2@illinois.eduYadong Huang
Zhoushan Academy of Marine Data
Science
Zhoushan, China
huang-yd14@tsinghua.org.cn
Zhidong Li
The University of Technology Sydney
Sydney, Australia
zhidong.li@uts.edu.auYang Wang
The University of Technology Sydney
Sydney, Australia
yang.wang@uts.edu.auFeng Zhou§
Center for Applied Statistics and
School of Statistics, Renmin
University of China
Beijing Advanced Innovation Center
for Future Blockchain and Privacy
Computing
Beijing, China
feng.zhou@ruc.edu.cn
ABSTRACT
Social networks represent complex ecosystems where the interac-
tions between users or groups play a pivotal role in information
dissemination, opinion formation, and social interactions. Effec-
tively harnessing event sequence data within social networks to
unearth interactions among users or groups has persistently posed
a challenging frontier within the realm of point processes. Cur-
rent deep point process models face inherent limitations within the
context of social networks, constraining both their interpretabil-
ity and expressive power. These models encounter challenges in
capturing interactions among users or groups and often rely on
parameterized extrapolation methods when modeling intensity
over non-event intervals, limiting their capacity to capture complex
intensity patterns beyond observed events. To address these chal-
lenges, this study proposes modifications to Transformer Hawkes
processes (THP), leading to the development of interpretable Trans-
former Hawkes processes (ITHP). ITHP inherits the strengths of
THP while aligning with statistical nonlinear Hawkes processes,
thereby enhancing its interpretability and providing valuable in-
sights into interactions between users or groups. Additionally, ITHP
enhances the flexibility of the intensity function over non-event
intervals, making it better suited to capture complex event propa-
gation patterns in social networks. Experimental results, both on
synthetic and real data, demonstrate the effectiveness of ITHP in
overcoming the identified limitations. Moreover, they highlight
ITHP’s applicability in the context of exploring the complex impact
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671720of users or groups within social networks. Our code is available
at https://github.com/waystogetthere/Interpretable-Transformer-
Hawkes-Process.git.
CCS CONCEPTS
•Computing methodologies →Machine learning approaches .
KEYWORDS
Temporal Point Process, Interpretability, Social Networks
ACM Reference Format:
Zizhuo Meng∗, Ke Wan∗, Yadong Huang, Zhidong Li, Yang Wang, and Feng
Zhou§. 2024. Interpretable Transformer Hawkes Processes: Unveiling Com-
plex Interactions in Social Networks. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671720
1 INTRODUCTION
Event sequences are pervasive in social networks [ 10,29], including
platforms such as Stack Overflow, Amazon, and Taobao. Under-
standing and mining these event sequences to uncover interactions
between different users or groups within social networks is a criti-
cal research topic [ 8,39]. This analysis can help identify influential
users, user groups, and trending topics, offering practical insights
for platform optimization and user engagement strategies [ 30,35].
For instance, consider the Stack Overflow platform, where devel-
opers ask and answer questions related to programming. Event
sequences in this context could consist of events such as question
postings, answers, comments, and votes. Analyzing this data can
reveal insights into user interactions.
Temporal point processes (TPP) [ 3] play a fundamental role in
modeling event sequences. The Poisson process [ 4], a basic temporal
*Equal contributions.
§Corresponding author.
 
2200
KDD ’24, August 25–29, 2024, Barcelona, Spain Zizhuo Meng∗, Ke Wan∗, Yadong Huang, Zhidong Li, Yang Wang, and Feng Zhou§
point process, assumes that events occur uniformly and indepen-
dently over time. Besides, the Hawkes process [ 9] is an extension of
the Poisson process that allows for event dependencies. While these
models have been useful in many scenarios, they may not always
capture the complexities present in real-world event sequences,
which often exhibit more intricate dependencies and interactions.
Therefore, more sophisticated and flexible models are needed.
With the advancement of deep learning, deep architectures have
demonstrated remarkable performance in modeling sequence data.
For example, models utilizing either vanilla RNN [ 7] or long short-
term memory (LSTM) networks [ 14] have exhibited improved likeli-
hood fitting and event prediction compared to earlier parameterized
models. Moreover, models relying on transformer architectures or
self-attention mechanisms [ 28,40] have shown even better per-
formance. These deep learning approaches have opened up new
possibilities for effectively capturing intricate patterns within event
sequences, enhancing the overall predictive accuracy and efficiency
in various applications.
However, current deep point process models still have some in-
herent limitations, which restrict the interpretability and expressive
power. Firstly, such models are unable to capture the interactions
between different event types explicitly. Deep point process models
often model interactions between event types implicitly, which may
hinder their interpretability due to the lack of explicit representa-
tion for these interactions. Understanding the interactions between
different event types is crucial in social networks. For example, on
Amazon, event sequences encompass a wide range of user activities,
which can be considered events of various types, including product
searches, purchases, reviews, and recommendations. Analyzing the
interactions among these types can yield valuable insights into
user-level and product-level interactions, providing Amazon with
strategic advantages. Secondly, most existing deep point process
models only perform encoding for the positions where events have
occurred. For non-event intervals, intensity functions are modelled
using parameterized extrapolation methods. For examples, Eq. (11)
in Du et al . [7], Eq. (7) in Mei and Eisner [14], and Eq. (6) in Zuo
et al. [40] . This approach introduces a parameterized assumption,
which restricts the model’s expressive power.
In order to address the aforementioned issues, we propose a
novel interpretable TPP model based on Transformer Hawkes pro-
cesses (THP). The proposed model aligns THP perfectly with the
statistical nonlinear Hawkes processes, greatly enhancing the inter-
pretability. Thus, we refer to this enhanced model as interpretable
Transformer Hawkes processes (ITHP). In ITHP, the attention mech-
anism’s product of the historical event’s key and the subsequent
event’s query corresponds precisely to a time-varying trigger ker-
nel in the statistical nonlinear Hawkes processes. By establishing a
clear correspondence with statistical Hawkes processes, ITHP offers
valuable insights into the interactions between different event types.
This advancement is significant for enhancing the interpretability
of THP in social network applications. Meanwhile, for the inten-
sity function over non-event intervals, we do not adopt a simple
parameterized extrapolation method. Instead, we utilize a “fully
attention mechanism” to express the conditional intensity function
at any position. This improvement increases the flexibility of the
intensity function over non-event intervals, consequently elevatingthe model’s expressive power. Specifically, our contributions are as
follows:
•ITHP explicitly captures interactions between event types,
providing insights into interactions and improving model
interpretability;
•ITHP’s fully attention mechanism for the conditional in-
tensity function over non-event intervals enhances model
flexibility, allowing it to capture complex intensity patterns
beyond the observed events;
•ITHP is validated with synthetic and real social network
data, demonstrating its superior ability to interpret event
interactions and outperform alternatives in expressiveness.
2 RELATED WORK
Enhancing the expressive power of point process models has long
been a challenging endeavor. Currently, mainstream approaches fall
into two categories. The first approach entails the utilization of sta-
tistical non-parametric methods to augment their expressive capac-
ity. For instance, methodologies grounded in both frequentist and
Bayesian nonparametric paradigms are employed to model the in-
tensity function of point processes [ 6,12,13,19,32,34,35]. The sec-
ond significant category is deep point process models. These models
harness the capabilities of deep learning architectures to infer the
intensity function from data, including RNNs [ 7], LSTM [ 14,26],
Transformers [ 27,28,40], normalizing flow [ 21], adversarial learn-
ing [ 16,25], reinforcement learning [ 22], deep kernel [ 5,18,38],
and intensity-free frameworks [ 20]. These architectural choices em-
power the modeling of temporal dynamics within event sequences
and unveil the underlying patterns. However, in contrast to statisti-
cal point process models, the enhanced expressive power of deep
point process models comes at the cost of losing interpretability,
rendering deep point process models akin to “black-box” constructs.
To the best of our knowledge, there has been limited exploration
into explicitly capturing interactions between event types and en-
hancing the interpretability of deep point process models [ 23,36].
This paper introduces an innovative attention-based ITHP model,
whose intensity function aligns seamlessly with statistical nonlin-
ear Hawkes processes, substantially enhancing the interpretability.
Our work serves as a catalyst for advancing the interpretability
of deep point process models, greatly promoting their utility in
uncovering interactions between different users or groups within
social networks.
3 PRELIMINARY KNOWLEDGE
In this section, we provide some background knowledge on some
relevant key concepts.
3.1 Hawkes Process
The multivariate Hawkes process [ 9] is a widely used temporal
point process model for capturing interactions among multiple
event types. The key feature of the multivariate Hawkes process
lies in its conditional intensity function. The conditional intensity
function𝜆𝑘(𝑡|H𝑡)for event type 𝑘at time𝑡is defined as the in-
stantaneous event rate conditioned on the historical information
 
2201Interpretable Transformer Hawkes Processes: Unveiling Complex Interactions in Social Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
H𝑡={(𝑡𝑖,𝑘𝑖)|𝑡𝑖<𝑡}:
𝜆𝑘(𝑡|H𝑡)=𝜇𝑘+∑︁
𝑡𝑖<𝑡𝜙𝑘,𝑘𝑖(𝑡−𝑡𝑖),
where𝜇𝑘is the base rate for event type 𝑘and𝜙𝑘,𝑘𝑖(𝑡−𝑡𝑖)is the trig-
ger kernel representing the excitation effect from event 𝑡𝑖with type
𝑘𝑖to𝑡with type𝑘. It expresses the expected number of occurrences
of event type 𝑘at time𝑡given the past history of events.
The interpretability of Hawkes processes stems from its explicit
representation of event dependencies through the trigger kernel.
The model allows us to quantify the impact of past events with dif-
ferent event types on the occurrence of a specific event, providing
insights into the interactions between event types. As a result, the
multivariate Hawkes process serves as a powerful tool in social net-
work applications where understanding the interactions between
event types (users or groups) is of utmost importance.
3.2 Nonlinear Hawkes Process
In contrast to the original Hawkes process, which assumes only non-
negative trigger kernels (excitatory interactions) between events
to avoid generating negative intensities, the nonlinear Hawkes
process [ 2] offers a more flexible modeling framework by incorpo-
rating both excitatory and inhibitory effects among events. In the
nonlinear Hawkes process, the conditional intensity function for
event type𝑘at time𝑡is defined as:
𝜆𝑘(𝑡|H𝑡)=𝜎 
𝜇𝑘+∑︁
𝑡𝑖<𝑡𝜙𝑘,𝑘𝑖(𝑡−𝑡𝑖)!
,
where𝜎(·)is a nonlinear mapping from RtoR+, ensuring the non-
negativity of the intensity. Hence this trigger kernel can be positive
(excitatory) or negative (inhibitory), thus enabling the modeling of
complex interactions between different event types.
In the aforementioned models, the trigger kernel depends solely
on the relative time 𝑡−𝑡𝑖, implying that the trigger kernel is shift-
invariant. However, in dynamic Hawkes process models [ 1,31–33],
the trigger kernel is further extended to vary with absolute time,
denoted as 𝜙(𝑡−𝑡𝑖,𝑡𝑖). By incorporating the absolute time, the
trigger kernel becomes capable of capturing time-varying patterns,
offering the model more degrees of freedom in its representation.
3.3 Transformer Hawkes Process
Our work is built upon THP [ 40], so we concisely introduce the
framework of THP here. Given a sequence S={(𝑡𝑖,𝑘𝑖)}𝐿
𝑖=1where
each event is characterized by a timestamp 𝑡𝑖and an event type
𝑘𝑖, THP leverages two types of embeddings, namely temporal em-
bedding and event type embedding, to represent these two kinds
of information. To encode event timestamps, THP represents each
timestamp𝑡𝑖using an embedding vector z(𝑡𝑖)∈R𝑀:
𝑧𝑗(𝑡𝑖)=(
cos(𝑡𝑖/10000𝑗−1
𝑀)if𝑗is odd,
sin(𝑡𝑖/10000𝑗
𝑀)if𝑗is even,
where𝑧𝑗(𝑡𝑖)is the𝑗-th entry of z(𝑡𝑖)and𝑗=0,...,𝑀−1. The collec-
tion of time embeddings is represented as Z=[z(𝑡1),..., z(𝑡𝐿)]⊤∈
R𝐿×𝑀. For encoding event types, the model utilizes a learnable
matrix U∈R𝑀×𝐾, where𝐾is the number of event types. For eachevent type𝑘𝑖, its embedding e(𝑘𝑖)is computed as:
e(𝑘𝑖)=Uy𝑖∈R𝑀,
where y𝑖is the one-hot encoding of the event type 𝑘𝑖. The collec-
tion of type embeddings is E=[e(𝑘1),..., e(𝑘𝐿)]⊤∈R𝐿×𝑀. The
final embedding is the summation of the temporal and event type
embeddings:
X=Z+E∈R𝐿×𝑀, (1)
where each row of Xrepresents the complete embedding of a single
event in the sequence S.
After embedding, the model focuses on learning the dependence
among events using self-attention mechanism. The attention output
Sis computed as:
S=softmaxQK⊤
√𝑀𝐾
V∈R𝐿×𝑀𝑉,
Q=XW𝑄∈R𝐿×𝑀𝐾,K=XW𝐾∈R𝐿×𝑀𝐾,V=XW𝑉∈R𝐿×𝑀𝑉,
where Q,K,Vare the query, key and value matrices. Matrices
W𝑄∈R𝑀×𝑀𝐾,W𝐾∈R𝑀×𝑀𝐾andW𝑉∈R𝑀×𝑀𝑉are the learn-
able parameters. To preserve causality and prevent future events
from influencing past events, we mask out the entries in the upper
triangular region of QK⊤.
Finally, the attention output Sis passed through a two-layer MLP
to produce the hidden state H:
H=ReLU(SW1+b1)W2+b2∈R𝐿×𝑀,
where W1∈R𝑀×𝑀𝐻,W2∈R𝑀𝐻×𝑀,b1∈R𝑀𝐻andb2∈R𝑀are
the learnable parameters. The 𝑘-type conditional intensity function
of THP is designed as:
𝜆𝑘(𝑡|H𝑡)=softplus
𝛼𝑘(𝑡−𝑡𝑖)/𝑡𝑖+w⊤
𝑘h(𝑡𝑖)+𝑏𝑘
,(2)
where𝑡𝑖is the last event before 𝑡,𝛼𝑘,w𝑘,𝑏𝑘are learnable parame-
ters, the nonlinear function is chosen to be softplus to ensure that
the intensity is non-negative, h(𝑡𝑖)is the transpose of the 𝑖-th row
ofHexpressing the historical impact on event 𝑡𝑖.
4INTERPRETABLE TRANSFORMER HAWKES
PROCESSES
As mentioned earlier, THP has two prominent limitations: (1) THP
implicitly model the dependency between events, which hinders the
explicit representation of interactions between different event types
and makes it challenging to understand the interactions among
event types. (2) Like many other deep point process models, THP
applies attention encoding only to the event occurrence positions,
while using parameterized extrapolation methods to model the
intensity on non-event intervals (the red term in Eq. (2)). This
approach introduces a parameterized assumption restricting the
model’s expressive power.
To enhance the model’s interpretability and expressiveness, our
work introduces modifications to the THP model. Specifically, we
make modifications to (1) the event embedding, (2) the at-
tention module and (3) the conditional intensity function in
THP. Interestingly, the modified THP corresponds perfectly to the
statistical nonlinear Hawkes processes. This leads to significantly
improved interpretability and a better characterization of the inter-
actions between event types. Additionally, the new design of the
 
2202KDD ’24, August 25–29, 2024, Barcelona, Spain Zizhuo Meng∗, Ke Wan∗, Yadong Huang, Zhidong Li, Yang Wang, and Feng Zhou§
conditional intensity function can avoid the restrictions imposed
by parameterized extrapolation, enabling the model to effectively
capture complex intensity patterns beyond the observed events.
In following sections, we outline the step-by-step process of mod-
ifying THP to achieve the aforementioned goals. For each modifica-
tion, we provide theoretical proofs to demonstrate the rationality
and validity of the respective changes.
4.1 Modified Event Embedding
In ITHP, we maintain the same temporal embedding and event type
embedding methods as in THP. However, our modification lies in
replacing the summation operation in Eq. (1) with concatenation:
X=Z+E∈R𝐿×𝑀⇒X=[Z,E]∈R𝐿×2𝑀. (3)
The reason for this modification is that the original summa-
tion operation introduces a similarity between timestamps and
event types (or vice versa) of the preceding and succeeding events.
However, in statistical Hawkes processes, known for their inter-
pretability, the interaction between two events is the magnitude of
a kernel determined by the similarity (correlation) between their
types and the similarity (distance) between their timestamps. No
cross-similarity is introduced. To maintain a similar level of inter-
pretability, we replace summation with concatenation here.
Theorem 4.1. In Eq. (3), the concatenation operation enables us
to explicitly capture the desired temporal and event type similari-
ties, while simultaneously avoiding any cross-similarities between
timestamps and event types.
Proof. Suppose we define Xusing concatenation, and in subse-
quent attention module computation, it is necessary to calculate
the product XX⊤to measure the similarity between different data
points. The similarity between the 𝑖-th point and the 𝑗-th point can
be expressed as follows:
X𝑖X⊤
𝑗=[Z𝑖,E𝑖][Z𝑗,E𝑗]⊤=Z𝑖Z⊤
𝑗+E𝑖E⊤
𝑗.
Instead, if we define Xusing addition, X𝑖X⊤
𝑗is as follows:
(Z𝑖+E𝑖)(Z𝑗+E𝑗)⊤=Z𝑖Z⊤
𝑗+E𝑖E⊤
𝑗+Z𝑖E⊤
𝑗+E𝑖Z⊤
𝑗.
It is evident that by defining Xthrough concatenation, temporal
and event type similarities are captured separately. Otherwise, the
cross-similarities emerge. □
4.2 Modified Attention Module
In ITHP, we still use self-attention to capture the influences of
historical events on subsequent events. However, unlike THP, in the
modified attention module, we use distinct query and key matrices:
S=softmaxQK⊤
√𝑀𝐾
V∈R𝐿×𝑀𝑉,
Q=XW𝑄∈R𝐿×𝑀𝐾⇒Q=X∈R𝐿×2𝑀,
K=XW𝐾∈R𝐿×𝑀𝐾⇒K=X∈R𝐿×2𝑀,
V=XW𝑉∈R𝐿×𝑀𝑉,(4)
where the𝑖-th row of S,S𝑖, represents the historical influence on the
𝑖-th event. The calculation of S𝑖can be explicitly expressed as thesummation over all events preceding event 𝑖, where the attention
weights are normalized by the softmax:
S𝑖=∑︁
𝑗<𝑖softmax
𝑗<𝑖 X𝑖X⊤
𝑗√
2𝑀!
V𝑗∈R𝑀𝑉. (5)
The reason for this modification is that after removing W𝑄
andW𝐾, the attention weights can be simply represented as XX⊤.
Compared to the original QK⊤,XX⊤has a clearer physical meaning.
InXX⊤, the entry in the 𝑖-th row and 𝑗-th column can be expressed
as a shift-invariant function 𝑔𝑘𝑖,𝑘𝑗(𝑡𝑖−𝑡𝑗). This representation
allows for a more meaningful interpretation of the relationship
between events. In contrast, QK⊤does not achieve this clarity.
Theorem 4.2. Assuming that Xis obtained through the concate-
nation operation in Eq. (3), after omitting W𝑄andW𝐾in Eq. (4), the
entry at the 𝑖-th row and 𝑗-th column of XX⊤can be expressed as a
shift-invariant function 𝑔𝑘𝑖,𝑘𝑗(𝑡𝑖−𝑡𝑗), where𝑡𝑗<𝑡𝑖.
Proof. When we use Eq. (3) to obtain Xand remove W𝑄and
W𝐾in Eq. (4), the similarity between the 𝑖-th and𝑗-th points, de-
noted as X𝑖X⊤
𝑗, is expressed as X𝑖X⊤
𝑗=Z𝑖Z⊤
𝑗+E𝑖E⊤
𝑗. If we assume
that𝑀is even, Z𝑖Z⊤
𝑗can be further represented as:
Z𝑖Z⊤
𝑗=𝑀
2+1∑︁
𝑚=1cos(𝑡𝑖𝜔𝑚)cos(𝑡𝑗𝜔𝑚)+sin(𝑡𝑖𝜔𝑚)sin(𝑡𝑗𝜔𝑚)
=𝑀
2+1∑︁
𝑚=1cos((𝑡𝑖−𝑡𝑗)𝜔𝑚),
where𝜔𝑚=1/100002(𝑚−1)/𝑀. It is clear that X𝑖X⊤
𝑗can be ex-
pressed as a shift-invariant function 𝑔𝑘𝑖,𝑘𝑗(𝑡𝑖−𝑡𝑗), with𝑡𝑖−𝑡𝑗
originating from Z𝑖Z⊤
𝑗, and the subscripts 𝑘𝑖,𝑘𝑗arising from E𝑖E⊤
𝑗.
While retaining W𝑄andW𝐾leads to the following expression:
Q𝑖K⊤
𝑗=[Z𝑖,E𝑖]W𝑄W𝐾⊤[Z𝑗,E𝑗]⊤,
where the introduction of W𝑄W𝐾⊤can once again introduce un-
desired cross-similarities and render the temporal similarity term
Z𝑖[W𝑄W𝐾⊤]Z𝑖Z𝑗Z⊤
𝑗unable to be expressed in a shift-invariant
function form. □
Corollary 4.3. Given Theorem 4.2, we can further simplify Eq. (5)
as follows:
S𝑖=∑︁
𝑗<𝑖g⊤
𝑘𝑖,𝑘𝑗(𝑡𝑖−𝑡𝑗,𝑡𝑗)∈R𝑀𝑉, (6)
where gis an𝑀𝑉dimensional vector function.
Proof. According to Theorem 4.2, X𝑖X⊤
𝑗can be expressed as a
shift-invariant function 𝑔𝑘𝑖,𝑘𝑗(𝑡𝑖−𝑡𝑗). After normalization through
softmax in Eq. (5), we obtain ˜𝑔𝑘𝑖,𝑘𝑗(𝑡𝑖−𝑡𝑗)satisfyingÍ
𝑗<𝑖˜𝑔𝑘𝑖,𝑘𝑗(𝑡𝑖−
𝑡𝑗)=1. When multiplied by V𝑗,˜𝑔𝑘𝑖,𝑘𝑗(𝑡𝑖−𝑡𝑗)V𝑗yields a vectorized,
time-varying and non-normalized function g⊤
𝑘𝑖,𝑘𝑗(𝑡𝑖−𝑡𝑗,𝑡𝑗)where
the additional 𝑡𝑗stems from the introduction of V𝑗. □
 
2203Interpretable Transformer Hawkes Processes: Unveiling Complex Interactions in Social Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
4.3 Modified Conditional Intensity Function
The form of Eq. (6) naturally reminds us of the trigger kernel sum-
mation in statistical Hawkes processes. The only difference is that
gin Eq. (6) is a vector, whereas the trigger kernel in statistical
Hawkes processes is a scalar function. Taking inspiration from this,
we propose a more interpretable conditional intensity function:
𝜆𝑘(𝑡|H𝑡)=softplus ∑︁
𝑡𝑖<𝑡g⊤
𝑘,𝑘𝑖(𝑡−𝑡𝑖,𝑡𝑖)w𝑘+𝑏𝑘!
=softplus ∑︁
𝑡𝑖<𝑡softmax𝑡𝑖<𝑡X𝑡X⊤
𝑖√
2𝑀
V𝑖w𝑘+𝑏𝑘!
,(7)
where w𝑘is a learnable parameter used to aggregate the vector
ginto a scalar value and 𝑏𝑘is a learnable bias term. The newly
designed conditional intensity aligns perfectly with the nonlinear
Hawkes processes with a time-varying trigger kernel. The green
term𝑏𝑘in Eq. (7) corresponds to the base rate 𝜇𝑘in Section 3.2,
the yellow term g⊤
𝑘,𝑘𝑖(𝑡−𝑡𝑖,𝑡𝑖)w𝑘in Eq. (7) corresponds to
the time-varying trigger kernel 𝜙𝑘,𝑘𝑖(𝑡−𝑡𝑖,𝑡𝑖)in Section 3.2,
and the softplus function serves as a non-linear mapping
ensuring the non-negativity of the intensity. This leads to
improved interpretability as the trigger kernel can be explicitly
expressed in our design, in contrast to the original THP.
4.4 Fully Attention-based Intensity Function
In point process model training with maximum likelihood estima-
tion (MLE), it is vital to compute the intensity integral over the
entire time domain, which requires modeling the intensity both
at event positions and on non-event intervals. In the RNN-based
deep point process models [ 7,14], due to the limitations of the
RNN framework in solely modeling latent representations at event
positions, the aforementioned works adopted parameterized ex-
trapolation methods to model the intensity on non-event intervals,
see Eq. (11) in [ 7] and Eq. (7) in [ 14]. THP [ 40] also adopted the
same approach to model the intensity on non-event intervals (the
red term in Eq. (2)). However, we emphasize that attention-based
deep point process models do not necessarily require the
parameterized extrapolation methods to model the intensity
on non-event intervals. Our design Eq. (7) employs the attention
mechanism to model the intensity function whether it is at event
positions or not. Therefore, we refer to it as a “fully attention-based
intensity function”. The fully attention-based intensity function
circumvents the limitations of parameterization and ensures that
the model can effectively capture intricate intensity patterns at
non-event positions, thus enhancing the model’s expressive power.
4.5 Model Training
For a given sequence S={(𝑡𝑖,𝑘𝑖)}𝐿
𝑖=1on[0,𝑇], the point process
model training can be performed by the MLE approach. The log-
likelihood of a point process is expressed in the following form:
L(S) =𝐿∑︁
𝑖=1log𝜆𝑘𝑖(𝑡𝑖|H𝑡𝑖)−∫𝑇
0𝜆(𝑡|H𝑡)𝑑𝑡, (8)
where𝜆(𝑡|H𝑡)=Í𝐾
𝑘=1𝜆𝑘(𝑡|H𝑡).For ITHP, we estimate its parameters by maximizing the log-
likelihood. Regarding the first term, we only need to compute the
intensity function at event positions using Eq. (7). As for the second
term, the intensity integral generally lacks an analytical expression.
Here, we employ numerical integration by discretizing the time axis
into a sufficiently fine grid and calculating the intensity function at
each grid point using Eq. (7).
Complexity: The utilization of a fine grid does not significantly
increase computational time. This is because the attention mecha-
nism facilitates parallel computation of attention outputs for each
point. This parallelized computation improves the scalability of
ITHP. Parallel computation with more grid points would require ad-
ditional memory. Fortunately, for one-dimensional temporal point
processes, a large number of grids is not necessary. In subsequent
experiments, all datasets can run smoothly with only 8GB memory.
5 EXPERIMENT
We assess the performance of ITHP using both synthetic and public
datasets. With the synthetic dataset, our objective is to validate the
interpretability of our model by accurately identifying the underly-
ing ground-truth trigger kernel. For the public datasets, we conduct
a comprehensive evaluation of ITHP by comparing its performance
against popular baseline models. The goal here is twofold: to quan-
titatively demonstrate the superior expressive power of ITHP and
to qualitatively analyze its interpretability on real datasets.
5.1 Synthetic Data
We validate the interpretability of ITHP using two sets of 2-variate
Hawkes processes data. Each dataset is simulated from a 2-variate
Hawkes processes described in Section 3.1, using the thinning algo-
rithm [ 17]. Both datasets share a common base rate ( 𝜇=0.2), but
they possess distinct trigger kernels:
•Exponential Decay Kernel This kernel assumes that the
influence of historical events decays exponentially as time
elapses:𝜙𝑖𝑗(𝜏)=𝛼𝑖𝑗exp(−𝛽𝑖𝑗𝜏)for𝜏>0, where𝑗is the
source type and 𝑖is the target type. Specifically, 𝛼11=𝛼22=
3,𝛼12=2,𝛼21=1and𝛽𝑖𝑗=5for all𝑖,𝑗.
•Half Sinusoidal Kernel This kernel assumes the influ-
ence of historical events follows a sinusoidal pattern as time
elapses and disappears when the interval surpasses 𝜋. The
kernel function is given by: 𝜙𝑖𝑗(𝜏)=𝛼𝑖𝑗sin(𝜏)for0<𝜏<𝜋.
Likewise,𝑗is the source type and 𝑖is the target type. Specif-
ically,𝛼11=𝛼22=0.33,𝛼12=0.1, and𝛼21=0.05.
Further elaboration on the simulation process and statistical aspects
of the synthetic dataset can be found in Appendix A.
Results: We validate the interpretability of ITHP by reconstruct-
ing the trigger kernel. In ITHP, the trigger kernel is represented
asg⊤
𝑘,𝑘𝑖(𝑡−𝑡𝑖,𝑡𝑖)w𝑘, which is time-varying. To uncover the time-
invariant trigger kernel inherent in the synthetic dataset, we evalu-
ate trigger kernels at various time points and compute their mean.
This approach enables us to extract the desired time-invariant trig-
ger kernel [ 28]. The results are presented in Figs. 1a and 1b, re-
vealing a noticeable alignment between the learned kernel trends
and the patterns exhibited by the ground-truth kernels. Moreover,
as depicted in Figs. 1c and 1d, the learned intensity function from
 
2204KDD ’24, August 25–29, 2024, Barcelona, Spain Zizhuo Meng∗, Ke Wan∗, Yadong Huang, Zhidong Li, Yang Wang, and Feng Zhou§
012311()
True
AVG Pred12()
0.00 0.25 0.50 0.75 1.00 1.25
Time012321()
0.00 0.25 0.50 0.75 1.00 1.2522()
(a) Trigger Kernel Recover (Exp)
0.00.10.20.30.411()
True
AVG Pred12()
0 1 2 3 4
Time0.00.10.20.30.421()
0 1 2 3 422()
 (b) Trigger Kernel Recover (Sin)
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.505101520Intensity dim-1
Predicted
True
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
Time051015IntensityIntensity dim-2 (c) Intensity Recover (Exp)
0 20 40 60 80 1000123Intensity dim-1
Predicted
True
0 20 40 60 80 100
Time0.51.01.52.0IntensityIntensity dim-2 (d) Intensity Recover (Sin)
Figure 1: Experimental results with synthetic data. (a)(b) Comparison between the ground-truth trigger kernel and the one
learned by ITHP: (a) exponential decay and (b) half sinusoidal. (c)(d) Comparison between the ground-truth intensity and the
one learned by ITHP: (c) exponential decay and (d) half sinusoidal.
Figure 2: The attention weight matrix for events and grids for
exponential decay kernel. Horizontal axis: source point, Ver-
tical axis: target point. Events have an impact on the future
which decays over time. Grids within non-event intervals do
not exert any influence as they are not actual events.
ITHP exhibits a striking resemblance to the ground-truth inten-
sity function. This observation underscores ITHP’s capability to
accurately capture the true conditional intensity function for both
exponential decay and half sinusoidal Hawkes processes. We also vi-
sualize the learned attention map of ITHP, which provides a deeper
insight into the influence patterns. As depicted in Fig. 2, this is the
attention weight matrix of a testing sequence in the context of ex-
ponential decay Hawkes process data. The sequence encompasses
both event timestamps and grids within the non-event intervals. In
the matrix, the rows and columns correspond to events and grids
on the sequence (arranged chronologically). The horizontal axis
represents the source point, while the vertical axis represents the
target point. Only events have the potential to impact subsequent
points, whereas grids, lacking actual event occurrences, cannot af-
fect future points. As a result, it is evident that numerous columns
corresponding to grids have values of 0. Due to a masking oper-
ation, the upper triangular section, including the diagonal, is set
to0, which restricts events from influencing the past. Moreover,
the color of event columns becomes progressively lighter as time
advances, which aligns with the characteristics of the ground-truth
exponential decay trigger kernel.
5.2 Public Data
We extensively evaluate ITHP by comparing it to baselines across
several public datasets. We have selected several network-sequencedatasets, including social media (StackOverflow), online shopping
(Amazon, Taobao), traffic networks (Taxi), and a widely used public
synthetic dataset (Conttime).
5.2.1 Datasets. We investigate five public datasets, each accompa-
nied by a concise description. More details can be found in Appen-
dix B.
•StackOverflow1[11]: This dataset has two years of user
awards on a question-answering website: StackOverflow.
Each user received a sequence of badges (Nice Question,
Good Answer, ...) and there are 𝐾=22kinds of badges.
•Amazon2[15]: This dataset includes user online shopping
behavior events on Amazon website (browsing, purchasing,
...) and there are in total 𝐾=16event types.
•Taobao3[37]: This dataset is released for the 2018 Tianchi
Big Data Competition and comprises user activities on Taobao
website (browsing, purchasing, ...) and there are in total
𝐾=17event types.
•Taxi4[24]: While our main focus is social networks, our
model can also be applied to other domains. This dataset
comprises traffic-network sequences, including taxi pick-up
and drop-off incidents across five boroughs of New York
City. Each borough, whether involved in a pick-up or drop-
off event, represents an event type and there are in total
𝐾=2×5=10event types.
•Conttime[ 14]: This dataset is a popular public synthetic
dataset designed for Hawkes processes, which comprises ten
thousand event sequences with event types 𝐾=5.
5.2.2 Baselines. In the experiments, we conduct a comparative
analysis against the following popular baseline models:
•RMTPP [7] is a RNN-based model. It learns the represen-
tation of influences from historical events and takes event
intervals as input explicitly.
•NHP [14] utilizes a continuous-time LSTM network, which
incorporates intensity decay, allowing for a more natural rep-
resentation of temporal dynamics without requiring explicit
encoding of event intervals as inputs to the LSTM.
1https://snap.stanford.edu/data/
2https://nijianmo.github.io/amazon/
3https://tianchi.aliyun.com/dataset/649
4https://chriswhong.com/open-data/foil_nyc_taxi/
 
2205Interpretable Transformer Hawkes Processes: Unveiling Complex Interactions in Social Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
•SAHP [28] uses self-attention to characterize the influence
of historical events and enhance its predictive capabilities
by capturing intricate dependencies within the data.
•THP [40] is another attention-based model that utilizes
Transformer to capture event dependencies while maintain-
ing computational efficiency.
5.2.3 Metrics. We assess ITHP and other baseline models using
two distinct metrics:
•TLL: the log-likelihood on the test data which quantifies the
model’s ability to capture the underlying data distribution
and effectively predict future events.
•ACC: the event type prediction accuracy on the test data
which characterizes the model’s accuracy in predicting the
specific types of events, thereby gauging its capacity to dis-
criminate between different event categories.
5.2.4 Quantitative Analysis. We conduct a comparative experiment
across five datasets using all baseline models. The results, as shown
in Table 1, demonstrate that ITHP can achieve competitive perfor-
mance. A more intuitive visualization is presented in Fig. 3a, where
each model’s TLL is standardized by subtracting the TLL of ITHP.
It is worth noting that, to achieve interpretability, ITHP undergoes
a certain degree of simplification, resulting in a reduction of its
number of parameters. Interestingly, we observe that ITHP achieves
comparable performance to other models with larger number of
parameters. ITHP can equivalently be regarded as a non-parametric,
time-varying, and nonlinear statistical Hawkes process. The results
in Fig. 3a provide some reflections: while deep point processes
claim to outperform statistical point processes, it is evident that a
sufficiently flexible (non-parametric, time-varying, and nonlinear)
statistical point process can also achieve competitive performance.
Furthermore, ITHP maintains excellent interpretability, both at the
event level and the event type level. Fig. 3b displays the attention
weight matrix of a testing sequence from StackOverflow, illustrat-
ing the impact between events: the influence from past events tends
to decrease as time elapsed. Moreover, ITHP can describe the in-
fluence functions between event types. Take StackOverflow as an
example: Fig. 3c presents the learned influence functions from types
1,3,4,5,9,12 to type 4, which is the most prevalent type. Generally,
these influences tend to decay over time.
5.2.5 Qualitative Analysis. Our model can provide useful insights
into the interaction among event types. To demonstrate this, we
first quantify the magnitude of influence between event types. We
compute∫
𝜙𝑖𝑗(𝜏)𝑑𝜏for each influence function, representing the
extent of influence from type 𝑗(source type) to type 𝑖(target type).
Specifically, each learned∫
𝜙𝑖𝑗(𝜏)𝑑𝜏is a scalar and can be demon-
strated in heat maps. In this section, we analyse these datasets by
looking into their learned heat maps: Figs. 3e to 3h.
StackOverflow: In this dataset, there are 22 event types related
to “badges” awarded to users based on their actions. As depicted
in Fig. 3e, many of these types have a strong positive influence on
both type 4 (“Popular Question”) and type 9 (“Notable Question”).
This observation aligns with the fact that “Popular Question” and
“Notable Question” are the two most frequent events. Our model
captures this trend and associates a significant positive impact from
other types to them. Furthermore, a noticeable link between type6 (“Nice Answer”, awarded when a user’s answer first achieves a
score of 10) and type 14 (“Enlighten”, given when a user’s answer
reaches a score of 10) is identified, which have nearly identical
meanings. This mirrors the real-world progression from receiving
a “Nice Answer” badge to later earning an “Enlighten” badge. This
congruence demonstrates that our model accurately captures the
dataset’s characteristics and effectively highlights the interplay
between different event types.
Amazon, Taobao: Both of these datasets pertain to customer be-
havior on shopping platforms and share some commonalities. Each
event type represents a category of the browsing item (Taobao)
or purchased item (Amazon), with Taobao having 𝐾=16types
and Amazon having 𝐾=17types. The learned heat maps are
presented in Figs. 3f and 3g. Interestingly, our model uncovers
two common insights: (1) The dark diagonals observed indicate
strong self-excitation for each type. This suggests customers tend
to browse items of the same category consecutively in a short pe-
riod. In Taobao and Amazon, with over 15 types in total, there
are approximately 58.3%and21.4%of events involving subsequent
events of the same type. This behavior reflects how customers often
browse items of the same category in a short period to decide which
one to purchase. Additionally, Amazon’s subscription purchases ex-
emplify this pattern: vendors offer extra savings to customers who
subscribe. These items are then regularly scheduled for delivery.
(2) In Figs. 3f and 3g, rows 1 and 17 appear the darkest, indicating
that these two types receive the most significant excitation from
others. In reality, these two categories are the most prevalent in
their respective datasets, implying that they should also have the
highest intensity. What our model learns aligns empirically with
the ground truth patterns in the datasets. Moreover, we conducted
a statistical analysis on Amazon in Fig. 3d, calculating the percent-
ages of various event types (“Total”), the percentages of the next
event being of the same type (“Same type follower”), and the per-
centages of the next event being of type 1 (“Type 1 follower”). It is
evident that the latter two constitute a significant portion ( ∼50%),
indicating strong self-excitation effects and a pronounced exciting
effect on type 1, which aligns with the learned heatmap in Fig. 3f.
Taxi: In this dataset, there are 10 types of events representing
taxi pick-up and drop-off across the five boroughs of New York
City. Types 1-5 categorize “drop-off” actions, whereas types 6-10
correspond to “pick-up” actions in the respective boroughs. The
learned heatmap (Fig. 3h) reveals three key insights: (1) Among the
“drop-off” actions (types 1-5), type 4 experiences the most signifi-
cant influence from types 6-10 (“pick-up”). This aligns with the fact
that type 4 (drop-off in Manhattan) is the most common drop-off
event, accounting for over 40%and thereby possessing the highest
intensity. (2) The “pick-up” and “drop-off” events always occur
alternately. One driver can’t pick up or drop off consecutively. As
Fig. 3h shows, type 6-10 (“pick-up”) have much more excitation on
type 1-5 (“drop-off”) rather than on themselves because a “pick-up”
action will stimulate a consecutive “drop-off” action rather than an-
other “pick-up” action. Likewise, type 1-5 have much less excitation
on themselves. (3) Type 9 and 4, pick-ups and drop-offs in Man-
hattan, display the most significant mutual influence, as indicated
by the two darkest cells in Fig. 3h. This is consistent because most
pick-up (44.61%) and drop-off (42 .89%) actions occur in Manhattan.
Furthermore, these two types always occur in tandem: 90.8%of
 
2206KDD ’24, August 25–29, 2024, Barcelona, Spain Zizhuo Meng∗, Ke Wan∗, Yadong Huang, Zhidong Li, Yang Wang, and Feng Zhou§
Table 1: The TLL and ACC of ITHP and other four baselines on five public datasets. Champion is in bold, runner-up is underlined.
Note that the Ex-ITHP is the extrapolated ITHP, which is a modified version of ITHP used for ablation study. More details are
provided in Section 5.3.
Modelstackoverflow amazon taobao taxi conttime
TLL(↑) ACC(↑) TLL(↑) ACC(↑) TLL(↑) ACC(↑) TLL(↑) ACC(↑) TLL(↑) ACC(↑)
RMTPP−2.87±0.02 0.43±0.01−2.68±0.03 0.30±0.01−3.81±0.05 0.44±0.03 0.17±0.04 0.91±0.01−1.88±0.03 0.38±0.01
NHP−2.80±0.01 0.43±0.02−2.70±0.05 0.27±0.01−3.10±0.020.45±0.01 0.24±0.040.93±0.04−1.54±0.010.41±0.03
SAHP−1.96±0.02 0.45±0.01−1.42±0.04 0.35±0.01−4.70±0.03 0.46±0.010.21±0.030.94±0.01−2.22±0.020.42±0.01
THP−3.41±0.010.46±0.01−3.26±0.21 0.34±0.01−4.76±0.11 0.44±0.05 0.22±0.05 0.93±0.02−3.16±0.19 0.34±0.01
ITHP−2.50±0.030.46±0.01−2.10±0.020.36±0.01−3.09±0.020.47±0.010.25±0.050.94±0.01−1.43±0.01 0.38±0.01
Ex-ITHP−3.58±0.01 0.43±0.03−4.65±0.02 0.33±0.01−4.80±0.01 0.41±0.02 0.18±0.03 0.85±0.03−3.74±0.04 0.31±0.02
STACK
OVERFLOWAMAZON TAOBAO TAXI CONTTIME2.0
1.5
1.0
0.5
0.00.51.01.52.0Standardized TLL
RMTPP
NHP
SAHP
THP
ITHP
(a) TLL Comparison
Only eventhas effectNon Eventmask (b) Attention Map (StackOverflow)
0 5 10 15 20 25 30 35
012345678()
Type1 to Type4
Type3 to Type4
Type4 to Type4
Type5 to Type4
Type9 to Type4
Type12 to Type4 (c) Estimated ˆ𝜙(𝜏)(StackOverflow)
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Amazon customers reviews (%)Type-1
Type-5
Type-9
Type-13Total
Type 1 followers
Same type followers (d) Amazon Statistics
1 3 5 7 9 11 13 15 17 19 211
3
5
7
9
11
13
15
17
19
21
012345678
(e) Heatmap of StackOverflow
1 3 5 7 9 11 13 151
3
5
7
9
11
13
15
1234567 (f) Heatmap of Amazon
1 3 5 7 9 11 13 15 171
3
5
7
9
11
13
15
17
2.55.07.510.012.515.017.520.0 (g) Heatmap of Taobao
1 2 3 4 5 6 7 8 9 101
2
3
4
5
6
7
8
9
10
246810 (h) Heatmap of Taxi
Figure 3: Experimental results with public data. (a) Comparative analysis of standardized TLL across models on five public
datasets. (b) Attention weight matrix for the first 200 events and grids in a StackOverflow sequence. Similar patterns to synthetic
data were observed. (c) Learned influence function ˆ𝜙(𝜏)among event types in StackOverflow, with influences generally decaying
over time. (d) A statistical analysis on Amazon: the percentages of various event types (“Total”), the percentages of the next
event being of the same type (“Same type follower”), and the percentages of the next event being of type 1 (“Type 1 follower”).
(e)(f)(g)(h) Heatmaps of impact magnitudes between event types. Horizontal axis: source type, Vertical axis: target type.
passengers picked up in Manhattan are also dropped off there, and
96.2%of drivers who complete a trip in Manhattan will pick up
their next customer within the same borough. This behavior is a
clear short-term pattern captured by our model and is evident in
the dataset.
5.3 Ablation Study
Our model has reduced the parameter count but still achieves com-
parable or even better results compared to THP. This improvement
is attributed to the “fully attention-based intensity function” (Sec-
tion 4.4). THP relies on the parameterized extrapolated intensity,
assuming that the intensity function on non-event intervals follows
an approximately linear pattern (red term in Eq. (2)). However, suchan assumption does not align with the actual patterns in real data
and can impact the expressive capability of the model. We conduct
further ablation studies to illustrate the limitations of the param-
eterized extrapolation method in Table 1. We implement an extra
revised model Ex-ITHP which essentially is “interpretable Trans-
former” + “extrapolated intensity”. More details about Ex-ITHP is
provided in Appendix C. THP, ITHP, and Ex-ITHP naturally consti-
tute an ablation study. Ex-ITHP has fewer parameters as it removes
the parameters W𝑄andW𝐾, and uses a less flexible extrapolated
intensity. In Table 1, the Ex-ITHP exhibits the poorest performance
due to its fewer parameters and restricted intensity flexibility. THP
performs moderately, having more parameters but still restricted
intensity flexibility. Conversely, the ITHP, despite having fewer
 
2207Interpretable Transformer Hawkes Processes: Unveiling Complex Interactions in Social Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
50 55 60 65 70 75 80 85
Time0.00.51.01.52.02.5IntensityITHP
THP
true
events
Figure 4: Comparison of the learned intensity for a segment
of a sequence by ITHP and THP. THP fails to learn the fluc-
tuating intensity on non-event intervals, but only maintains
an approximately linear pattern due to the extrapolation
assumption. In contrast, our proposed ITHP demonstrates
greater flexibility, successfully capturing the fluctuating in-
tensity, and accurately fitting the scale level.
parameters, outperforms THP on most datasets owing to its more
flexible intensity expression. Additionally, we visualize the differ-
ence between the learned fully attention-based intensity and the
learned extrapolated intensity for a segment of the sequence in
Half Sinusoidal Kernel Hawkes Synthetic dataset (Section 5.1). As
depicted in Fig. 4, on non-event intervals, THP, constrained by the
approximately linear extrapolation, struggles to capture the fluc-
tuating intensity patterns and can only learn an intensity that is
approximately linear. Additionally, due to the limited variation in
intensity on non-event intervals, large jumps are required when a
new event occurs to maintain a height similar to the ground-truth
intensity. In contrast, our proposed ITHP demonstrates greater flex-
ibility, successfully capturing the fluctuating pattern on non-event
intervals, and accurately fitting the scale level.
5.4 Hyperparameter Analysis
Our model’s configuration primarily encompasses two dimensions:
the encoding dimension, denoted as 𝑀, and the Value dimension,
denoted as𝑀𝑉. We maintain the skip connection in the encoder
which necessitates that 𝑀𝑉=2𝑀. We test the sensitivity of model
performance to hyperparameters by using various hyperparameter
configurations on one toy dataset and one public dataset: the half-
sine and Taxi datasets. The results of our experiments are shown
in Table 2. The results indicate that our model is not significantly af-
fected by the hyperparameter variation. Additionally, it can achieve
reasonably good performance even with fewer parameters.
6 CONCLUSION
To model interactions in social networks using event sequence
data, we introduce ITHP as a novel approach to enhance the in-
terpretability and expressive power of deep point processes model.
Specifically, ITHP not only inherits the strengths of Transformer
Hawkes processes but also aligns with statistical nonlinear Hawkes
processes, offering practical insights into user or group interac-
tions. It further enhances the flexibility of intensity functions overTable 2: Experiments of different hyperparameters configu-
rations on datasets: half-sine and Taxi. The results indicate
that our model is robust to these hyperparameters.
ConfigTaxi Half-Sine
TLL ACC TLL ACC
𝑀=64,𝑀𝑉=128 0.2513 0.97 -0.7714 0.58
𝑀=128,𝑀𝑉=256 0.2501 0.97 -0.7909 0.58
𝑀=256,𝑀𝑉=512 0.2520 0.97 -0.7822 0.58
𝑀=512,𝑀𝑉=1024 0.2498 0.97 -0.7852 0.59
non-event intervals. Our experiments have demonstrated the ef-
fectiveness of ITHP in overcoming inherent limitations in existing
deep point process models. Our findings open new avenues for
research in understanding and modeling the complex dynamics of
social ecosystems, ultimately contributing to the broader under-
standing of these intricate networks.
ACKNOWLEDGMENTS
This work was supported by NSFC Project (No. 62106121), the MOE
Project of Key Research Institute of Humanities and Social Sciences
(22JJD110001), and the Public Computing Cloud, Renmin University
of China.
REFERENCES
[1]Moinak Bhaduri, Dhruva Rangan, and Anurag Balaji. 2021. Change detection
in non-stationary Hawkes processes through sequential testing. In ITM Web of
Conferences, Vol. 36. EDP Sciences, 01005.
[2]Pierre Brémaud and Laurent Massoulié. 1996. Stability of nonlinear Hawkes
processes. The Annals of Probability (1996), 1563–1588.
[3]Daryl J Daley and David Vere-Jones. 2003. An introduction to the theory of point
processes. Vol. I. Probability and its Applications.
[4]Daryl J Daley and David Vere-Jones. 2007. An Introduction to the Theory of Point
Processes: Volume II: General Theory and Structure. Springer Science & Business
Media.
[5]Zheng Dong, Xiuyuan Cheng, and Yao Xie. 2022. Spatio-temporal point processes
with deep non-stationary kernels. arXiv preprint arXiv:2211.11179 (2022).
[6]Christian Donner and Manfred Opper. 2018. Efficient Bayesian inference of
sigmoidal Gaussian Cox processes. Journal of Machine Learning Research 19, 1
(2018), 2710–2743.
[7]Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-
Rodriguez, and Le Song. 2016. Recurrent marked temporal point processes:
embedding event history to vector. In Proceedings of the 22nd ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining. ACM, 1555–1564.
[8]Mehrdad Farajtabar, Yichen Wang, Manuel Gomez Rodriguez, Shuang Li,
Hongyuan Zha, and Le Song. 2015. Coevolve: A joint point process model for
information diffusion and network co-evolution. Advances in Neural Information
Processing Systems 28 (2015).
[9]Alan G Hawkes. 1971. Spectra of some self-exciting and mutually exciting point
processes. Biometrika 58, 1 (1971), 83–90.
[10] Quyu Kong, Pio Calderon, Rohit Ram, Olga Boichak, and Marian-Andrei Rizoiu.
2023. Interval-censored transformer hawkes: Detecting information operations
using the reaction of social systems. In Proceedings of the ACM Web Conference
2023. 1813–1821.
[11] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network
Dataset Collection. http://snap.stanford.edu/data.
[12] Erik Lewis and George Mohler. 2011. A nonparametric EM algorithm for multi-
scale Hawkes processes. Journal of Nonparametric Statistics 1, 1 (2011), 1–20.
[13] Chris Lloyd, Tom Gunter, Michael Osborne, and Stephen Roberts. 2015. Varia-
tional inference for Gaussian process modulated Poisson processes. In Interna-
tional Conference on Machine Learning. 1814–1822.
[14] Hongyuan Mei and Jason Eisner. 2017. The Neural Hawkes Process: A Neurally
Self-Modulating Multivariate Point Process. In Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Sys-
tems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von
 
2208KDD ’24, August 25–29, 2024, Barcelona, Spain Zizhuo Meng∗, Ke Wan∗, Yadong Huang, Zhidong Li, Yang Wang, and Feng Zhou§
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,
and Roman Garnett (Eds.). 6754–6764.
[15] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations
using distantly-labeled reviews and fine-grained aspects. In Proceedings of the
2019 conference on empirical methods in natural language processing and the 9th
international joint conference on natural language processing (EMNLP-IJCNLP).
188–197.
[16] Kimia Noorbakhsh and Manuel Rodriguez. 2022. Counterfactual Temporal Point
Processes. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mo-
hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Asso-
ciates, Inc., 24810–24823. https://proceedings.neurips.cc/paper_files/paper/2022/
file/9d3faa41886997cfc2128b930077fa49-Paper-Conference.pdf
[17] Yosihiko Ogata. 1998. Space-time point-process models for earthquake occur-
rences. Annals of the Institute of Statistical Mathematics 50, 2 (1998), 379–402.
[18] Maya Okawa, Tomoharu Iwata, Takeshi Kurashima, Yusuke Tanaka, Hiroyuki
Toda, and Naonori Ueda. 2019. Deep Mixture Point Processes: Spatio-temporal
Event Prediction with Rich Contextual Information. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,
KDD 2019, Anchorage, AK, USA, August 4-8, 2019, Ankur Teredesai, Vipin Kumar,
Ying Li, Rómer Rosales, Evimaria Terzi, and George Karypis (Eds.). ACM, 373–
383.
[19] Zhimeng Pan, Zheng Wang, Jeff M Phillips, and Shandian Zhe. 2021. Self-
adaptable point processes with nonparametric time decays. Advances in Neural
Information Processing Systems 34 (2021), 4594–4606.
[20] Oleksandr Shchur, Marin Bilos, and Stephan Günnemann. 2020. Intensity-Free
Learning of Temporal Point Processes. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net.
[21] Oleksandr Shchur, Nicholas Gao, Marin Biloš, and Stephan Günnemann. 2020.
Fast and flexible temporal point processes with triangular maps. Advances in
Neural Information Processing Systems 33 (2020), 73–84.
[22] Utkarsh Upadhyay, Abir De, and Manuel Gomez Rodriguez. 2018. Deep rein-
forcement learning of marked temporal point processes. Advances in Neural
Information Processing Systems 31 (2018).
[23] Song Wei, Yao Xie, Christopher S Josef, and Rishikesan Kamaleswaran. 2023.
Granger Causal Chain Discovery for Sepsis-Associated Derangements via
Continuous-Time Hawkes Processes. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 2536–2546.
[24] Chris Whong. 2014. FOILing NYC’s taxi trip data. FOILing NYCs Taxi Trip Data.
Np18 (2014).
[25] Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song, and Hongyuan
Zha. 2017. Wasserstein learning of deep generative point process models. Ad-
vances in neural information processing systems 30 (2017).
[26] Shuai Xiao, Junchi Yan, Xiaokang Yang, Hongyuan Zha, and Stephen M. Chu.
2017. Modeling the Intensity Function of Point Process Via Recurrent Neural
Networks. In Proceedings of the Thirty-First AAAI Conference on Artificial Intel-
ligence, February 4-9, 2017, San Francisco, California, USA, Satinder Singh and
Shaul Markovitch (Eds.). AAAI Press, 1597–1603.
[27] Lu-ning Zhang, Jian-wei Liu, Zhi-yan Song, and Xin Zuo. 2022. Temporal atten-
tion augmented transformer Hawkes process. Neural Computing and Applications
(2022), 1–15.
[28] Qiang Zhang, Aldo Lipani, Ömer Kirnap, and Emine Yilmaz. 2020. Self-Attentive
Hawkes Process. In Proceedings of the 37th International Conference on Machine
Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine
Learning Research, Vol. 119). PMLR, 11183–11193.
[29] Yizhou Zhang, Defu Cao, and Yan Liu. 2022. Counterfactual neural temporal
point process for estimating causal influence of misinformation on social media.
Advances in Neural Information Processing Systems 35 (2022), 10643–10655.
[30] Qingyuan Zhao, Murat A Erdogdu, Hera Y He, Anand Rajaraman, and Jure
Leskovec. 2015. Seismic: A self-exciting point process model for predicting tweet
popularity. In Proceedings of the 21th ACM SIGKDD international conference on
knowledge discovery and data mining. 1513–1522.
[31] Feng Zhou, Quyu Kong, Zhijie Deng, Jichao Kan, Yixuan Zhang, Cheng Feng, and
Jun Zhu. 2022. Efficient Inference for Dynamic Flexible Interactions of Neural
Populations. Journal of Machine Learning Research 23, 211 (2022), 1–49.
[32] Feng Zhou, Zhidong Li, Xuhui Fan, Yang Wang, Arcot Sowmya, and Fang Chen.
2020. Efficient inference for nonparametric Hawkes processes using auxiliary
latent variables. Journal of Machine Learning Research 21, 241 (2020), 1–31.
[33] Feng Zhou, Zhidong Li, Xuhui Fan, Yang Wang, Arcot Sowmya, and Fang Chen.
2020. Fast multi-resolution segmentation for nonstationary Hawkes process
using cumulants. International Journal of Data Science and Analytics 10 (2020),
321–330.
[34] Feng Zhou, Yixuan Zhang, and Jun Zhu. 2021. Efficient Inference of Flexible Inter-
action in Spiking-neuron Networks. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
[35] Ke Zhou, Hongyuan Zha, and Le Song. 2013. Learning triggering kernels for
multi-dimensional Hawkes processes. In International Conference on Machine
Learning. 1301–1309.[36] Zihao Zhou and Rose Yu. 2023. Automatic Integration for Fast and Interpretable
Neural Point Processes. In Learning for Dynamics and Control Conference. PMLR,
573–585.
[37] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai.
2018. Learning tree-based deep model for recommender systems. In Proceedings
of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 1079–1088.
[38] Shixiang Zhu, Minghe Zhang, Ruyi Ding, and Yao Xie. 2021. Deep fourier
kernel for self-attentive point processes. In International Conference on Artificial
Intelligence and Statistics. PMLR, 856–864.
[39] Joseph R Zipkin, Frederic P Schoenberg, Kathryn Coronges, and Andrea L
Bertozzi. 2016. Point-process models of social network interactions: Parameter
estimation and missing data recovery. European journal of applied mathematics
27, 3 (2016), 502–529.
[40] Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. 2020.
Transformer hawkes process. In International conference on machine learning.
PMLR, 11692–11702.
APPENDIX
A TOY DATA
We simulate two synthetic datasets: the exponential decay Hawkes
processes and the half sinusoidal Hawkes processes, with the max-
imum observed length 𝑇=20and𝑇=100, respectively. The
simulation is performed using the thinning algorithm [ 17] that
is outlined in the algorithm below. The kernel function 𝜙𝑚𝑛(𝜏)
is defined in Section 5.1, where 𝑚is the target type and 𝑛is the
source type; 𝜙𝑚∗(𝜏)indicates all kernels whose target type is 𝑚.
The statistics of our toy datasets are listed in Table 3. Additionally,
attention weight matrix of a testing sequence in the half-sine toy
data is depicted in Fig. 6a.
Algorithm 1 Simulation of an 𝑀-variate Hawkes processes with
kernels𝜙𝑚𝑛(·)for𝑚,𝑛=1,2,...,𝑀 on[0,𝑇].
Require: {𝜇𝑛,𝜙𝑚𝑛(·)} for𝑚,𝑛=1,2,...,𝑀 , the observation win-
dow[0,𝑇]
InitializeT1=···=T𝑀=∅,𝑛1=···=𝑛𝑀=0,𝑠=0;
while𝑠<𝑇do
Set¯𝜆=Í𝑀
𝑚=1𝜆𝑚(𝑠−)+max(𝜙𝑚∗(·));
Generate𝑤∼exponential(1/¯𝜆);
Set𝑠=𝑠+𝑤;
Generate𝐷∼uniform(0,1);
if𝐷¯𝜆≤Í𝑀
𝑚=1𝜆𝑚(𝑠)then
𝑘∼categorical([𝜆1(𝑠),...,𝜆𝑀(𝑠)]/¯𝜆);
𝑛𝑘=𝑛𝑘+1;
𝑡𝑘
𝑛𝑘=𝑠
T𝑘=T𝑘∪{𝑡𝑘
𝑛𝑘}
end if
end while
if𝑡𝑘
𝑛𝑘≤𝑇then
return{T𝑚}for𝑚=1,2,...,𝑀
else
returnT1...T𝑘/{𝑡𝑘
𝑛𝑘}...T𝑀
end if
 
2209Interpretable Transformer Hawkes Processes: Unveiling Complex Interactions in Social Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: The statistics of two synthetic datasets. Details such as the number of events, the sequence length and the event
intervals are provided below.
Da
taset Split # of EventsSeqence Length Event Interval
Max Min Mean(Std) Max Min Mean(Std)
Exponential-Deca
ytraining 70644 877 47 282.58 (150.25) 21.86 1.91e-06 0.28 (150.25)
validation 36521 877 47 292.17 (153.94) 20.90 1.91e-06 0.27 (153.94)
test 33716 894 59 269.73 (144.97) 20.31 3.81e-06 0.29 (144.97)
Half-Sinetraining
95714 858 158 382.86 (96.29) 21.21 3.83e-07 0.52 (96.29)
validation 48814 858 158 390.51 (106.12) 21.21 3.83e-07 0.51 (106.12)
test 50376 717 223 403.01 (102.19) 19.57 5.48e-06 0.49 (102.19)
Table 4: The statistics of five public datasets. Details such as the number of events, the statistics of event interval are provided.
Da
taset Split # of EventsSeqence Length Event Interval
Max Min Mean(Std) Max Min Mean(Std)
ST
ACKOVERFLOWtraining 90497 101 41 64.59 (20.46) 20.34 1.22e-4 0.88 (20.46)
validation 25313 101 41 63.12 (19.85) 16.68 1.22e-4 0.90 (19.85)
test 26518 101 41 66.13(20.77) 17.13 1.22e-4 0.85 (20.77)
T
AOBAOtraining 75205 64 40 57.85 (6.64) 2.00 9.99e-05 0.22 (6.64)
validation 11497 64 40 57.49 (6.82) 1.99 9.99e-05 0.22 (6.82)
test 28455 64 32 56.91 (7.82) 1.00 4.21e-06 0.05 (7.82)
AMAZONtraining
288377 94 14 44.68 (17.88) 0.80 0.010 0.51 (17.88)
validation 40088 94 15 43.48 (16.60) 0.80 0.010 0.50 (16.60)
test 84048 94 14 45.41 (18.19) 0.80 0.010 0.51(18.19)
T
AXItraining 51854 38 36 37.04 (1.00) 5.72 2.78e-4 0.22 (1.00)
validation 7422 38 36 37.11 (1.00) 5.52 2.78e-4 0.22 (1.00)
test 14820 38 36 37.05 (1.00) 5.25 8.33e-4 0.22 (1.00)
CON
TTIMEtraining 479467 100 20 59.93 (23.13) 4.03 1.91e-06 0.24 (23.13)
validation 60141 100 20 60.14 (22.97) 3.94 2.86e-06 0.24 (22.97)
test 61781 100 20 61.78 (23.21) 4.47 9.54e-07 0.24 (23.21)
B PUBLIC DATA
B.1 Public Data Statistics
This section presents the main statistics of five multivariate public
datasets in Table 4. Visualizations of event percentages are shown
in Fig. 5.
B.2 Additional Attention Map
Additional attention weight matrices for other four public datasets
are presented in Fig. 6, which is more intuitive to demonstrate how
events affect each other in the sequence.
C IMPLEMENTATION OF EX-ITHP
The Ex-ITHP(extrapolation iTHP) is the iTHP(removing parameters
𝑊𝑄,𝑊𝐾) utilising “extrapolation intensity”. In this section, we
introduce the implementation of Ex-ITHP. Given a sequence S=
{(𝑡𝑖,𝑘𝑖)}𝐿
𝑖=1where each event is characterized by a timestamp 𝑡𝑖
and an event type 𝑘𝑖, Ex-ITHP utilize the same temporal embedding
and type embedding and concatenates them as iTHP does:
X=[Z,E]∈R𝐿×2𝑀, (9)
where Z∈R𝐿×𝑀andE∈R𝐿×𝑀are the temporal encoding
and type encoding of S. The encoder output Sis calculated in the
same way as Eq. (4). S𝑖indicates the 𝑖-th row of S, which is therepresentation of event 𝑖:
S𝑖=∑︁
𝑗<𝑖softmax
𝑗<𝑖 X𝑖X⊤
𝑗√
2𝑀!
V𝑗∈R𝑀𝑉. (10)
The encoder output S𝑖is then passed through a MLP to get the final
representation of event 𝑖:
H𝑖=ReLU(S𝑖W1+b1)W2+b2∈R𝑀.
Then, given a type 𝑘and time t (𝑡𝑖<𝑡≤𝑡𝑖+1), the corresponding
intensity is given by the extrapolation method:
𝜆𝑘(𝑡|H𝑡)=softplus(𝛼𝑘𝑡−𝑡𝑖
𝑡𝑖+w⊤
𝑘H𝑖+𝑏𝑘). (11)
Finally, the log-likelihood to be optimized is given by:
L(S) =𝐿∑︁
𝑖=1log𝜆𝑘𝑖(𝑡𝑖|H𝑡𝑖)−𝐾∑︁
𝑘=1∫𝑇
0𝜆𝑘(𝑡|H𝑡)𝑑𝑡, (12)
In summary, Ex-ITHP employs identical encoding techniques, specif-
ically temporal encoding and event type encoding, while also elim-
inating the use of 𝑊𝑄and𝑊𝐾, akin to iTHP. However, it utilizes
the extrapolation method to formulate the intensity as THP does.
 
2210KDD ’24, August 25–29, 2024, Barcelona, Spain Zizhuo Meng∗, Ke Wan∗, Yadong Huang, Zhidong Li, Yang Wang, and Feng Zhou§
1 4 7 10 13 16 19 220.00.10.20.30.4Train
1 4 710 13 16 19 220.00.10.20.30.4Dev
1 4 710 13 16 190.00.10.20.30.4Test
(a) StackOverflow
1 4 7 10 13 160.00.10.20.30.4Train
1 4 7 10 13 160.00.10.20.30.4Dev
1 4 7 10 13 160.00.10.20.30.4Test (b) Taobao
1 3 5 7 9 11 13 150.00.10.20.3Train
135791113150.00.10.20.3Dev
135791113150.00.10.20.3Test (c) Amazon
1 3 5 7 90.00.10.20.30.4Train
1 3 5 7 90.00.10.20.30.4Dev
1 3 5 7 90.00.10.20.30.4Test
(d) Taxi
1 2 3 4 50.00.10.20.3Train
1 2 3 4 50.00.10.20.3Dev
1 2 3 4 50.00.10.20.3Test (e) Conttime
Figure 5: All the public datasets consist of multiple event types. Specifically, Amazon has 𝐾=17event types, StackOverflow
has𝐾=22, Taxi has 𝐾=10, Taobao has 𝐾=16, and Conttime has 𝐾=5. We provide visualizations of the event percentages in
each dataset. Each subplot illustrates the distribution of event types in the training, validation, and testing sets, respectively.
Notably, there is a significant imbalance in event types observed in StackOverflow, Taxi, and Taobao datasets.
0 20 40 60 80 100 120 1400
20
40
60
80
100
120
140
0.00.20.40.60.81.0
(a) Half-sine
0 50 100 150 200 2500
50
100
150
200
250
0.00.20.40.60.81.0 (b) Taobao
0 50 100 150 200 2500
50
100
150
200
250
0.00.20.40.60.81.0 (c) Amazon
0 20 40 60 80 100 120 1400
20
40
60
80
100
120
140
0.00.20.40.60.81.0
(d) Taxi
0 50 100 150 200 2500
50
100
150
200
250
0.00.20.40.60.81.0 (e) Conttime
Figure 6: Each subplot demonstrates the attention map of a testing sequence in the half-sine toy data and the other four public
datasets. Similarly, the horizontal axis represents the source point, while the vertical axis represents the target point. It is
apparent that events have a lasting effect on subsequent events. Grids within non-event intervals do not exert any future
influence, as they do not correspond to actual event occurrences. Each vertical rectangle signifies a single event’s impact on
various future events. The color fading from top to bottom within each rectangle indicates that the impact diminishes over
time.
 
2211