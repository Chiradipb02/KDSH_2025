Distributed Thresholded Counting with Limited Interaction
Xiaoyi Zhu
22110980022@m.fudan.edu.cn
Fudan University
School of Data Science
Shanghai, ChinaYuxiang Tian
tianyx22@m.fudan.edu.cn
Fudan University
School of Data Science
Shanghai, ChinaZengfeng Huang∗
huangzf@fudan.edu.cn
Fudan University
School of Data Science
Shanghai, China
Abstract
Problems in the area of distributed computing have been extensively
studied. In this paper, we focus on the Distributed Thresholded
Counting problem in the coordinator model. In this problem, we
have𝑘sites holding their input and communicating with a central
coordinator. The coordinator’s task is to determine whether the
sum of inputs is larger than a threshold.
While the communication complexity of this basic problem has
been studied for decades, it is still not well understood. Our work
considers the worst-case communication cost for an algorithm
that uses limited interaction — i.e. a bounded number of rounds
of communication. Algorithms in previous research usually need
𝑂(log log𝑁)or𝑂(𝑘)rounds. In comparison, in the deterministic
case, our algorithm achieves optimal communication complexity in
only𝛼(𝑘)rounds, where 𝛼(𝑘)denotes the inverse Ackermann func-
tion and is nearly constant. We also give a randomized algorithm
that balances communication, rounds, and error probability.
CCS Concepts
•Theory of computation →Distributed algorithms.
Keywords
communication complexity; distributed counting; algorithm
ACM Reference Format:
Xiaoyi Zhu, Yuxiang Tian, and Zengfeng Huang. 2024. Distributed Thresh-
olded Counting with Limited Interaction. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671868
1 Introduction
In many practical applications, massive data is collected and stored
on a large number of nodes possibly deployed at different locations,
while we want to learn properties of the union of the data. For
example, user data generated by edge devices [ 8], measurements
collected from large-scale sensor networks [ 19], application data
from location based services [ 22]. In such distributed systems and
∗Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671868applications, communication costs is often the major concern, since
communication is much slower than local computation. Another
reason is that communication is the biggest source of battery drain
on smartphones and sensors [ 16], and it is always desirable to save
energy on such devices.
In this paper, we consider distributed thresholded counting. In
this problem, each distributed node holds a private value 𝑛𝑖, and
the goal is to determine whether the sum of the values surpasses a
given threshold 𝑁. This is a ubiquitous query and a basic task for
many applications including network traffic monitoring, anomaly,
and attack detection [ 17]. For example, in a network monitoring
system, we want to know whether a destination receives more than
20GB of traffic in an hour. We aim to understand the communication
complexity of this problem in a multi-party distributed model.
To formalize the theoretical study, we consider the number-in-
hand coordinator model introduced in [ 12]. In this model, there are
𝑘sites each holding his private input 𝑛𝑖, and an additional site called
coordinator that receives no input. Sites can only communicate with
the coordinator. The task of the coordinator is to collaborate with
all the sites to correctly output the value of a joint function of
their inputs. Our goal is to minimize the worst-case communication
costs, i.e., the total number of bits exchanged, and the number of
communication rounds. This model has received increased attention
and become a standard model for studying the communication
complexity of distributed computing [4–6, 24].
In the thresholded counting problem, the coordinator needs to
determine whether the sum of the inputsÍ𝑘
𝑖=1𝑛𝑖is larger than a
given threshold 𝑁. Perhaps surprisingly, the communication com-
plexity of this basic problem, although has been studied in the
computational complexity community for decades [ 20], is still not
well understood. Due to motivations from circuit complexity, the
complexity community mainly focuses on the case where 𝑁is expo-
nentially larger than 𝑘. However, there is still a large gap between
the best upper and lower bounds [ 23] for this case, and closing
this gap was posed as an extremely challenging open problem.
Specifically, assuming 𝑘=(log𝑁)𝑜(1), [23] provides a random-
ized algorithm with 𝑂(𝑘log𝑘log log𝑁)bits of communication
and𝑂(log log𝑁)rounds, while the current best randomized lower
bound on communication is Ω(max{𝑘,log log𝑁})[13, 18, 23].
In this paper, we focus on the regime where 𝑁=𝑘𝑂(1), which
is arguably more practical. In this regime, the naive one-round
algorithm in which all sites send their inputs incurs 𝑂(𝑘log𝑁)bits
of communication. There is another straightforward determinis-
tic algorithm (see Section 4) that uses 𝑂
𝑘log
𝑁
𝑘
bits, which
matches the deterministic lower bound proved in [ 10] and is better
when𝑁=𝑂(𝑘). However, it requires 𝑘rounds in the worst case.
4664
KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoyi Zhu, Yuxiang Tian, & Zengfeng Huang.
The main question we try to answer in this paper is whether there
is an algorithm with optimal communication cost and 𝑂(1)rounds.
We make significant progress on this question. First, we provide
three deterministic algorithms with 𝑂
𝑘log
𝑁
𝑘
bits of commu-
nication cost, and their round complexity is 𝑂(log𝑘),log∗(𝑘)and
𝑂(𝛼(𝑘))respectively. Here 𝛼(𝑘)is the inverse Ackermann function,
which grows extremely slowly and practically 𝑂(1). Moreover, we
present a randomized algorithm with error probability at most 2−√
𝑘.
It has the same communication cost and uses at most 3 rounds.
1.1 Problem Definition and Our Results
In this paper, we study the Distributed Thresholded Counting (DTC)
problem and consider both exact and approximate versions, which
are defined next.
Definition 1.1 (Exact DTC). Each site𝑛𝑖holds a private integer
𝑛𝑖∈[0,𝑁), the job of the coordinator is to output YES ifÍ𝑘
𝑖=1𝑛𝑖>
𝑁and output NO ifÍ𝑘
𝑖=1𝑛𝑖≤𝑁.
Definition 1.2 (Approximate DTC). Each site𝑛𝑖holds a private
integer𝑛𝑖∈[0,𝑁), the job of the coordinator is to output YES ifÍ𝑘
𝑖=1𝑛𝑖>𝑁and output NO ifÍ𝑘
𝑖=1𝑛𝑖≤(1−𝜀)𝑁. The coordinator
can output anything if (1−𝜀)𝑁<Í𝑘
𝑖=1𝑛𝑖≤𝑁.
Here we assume 𝑁,𝜀,𝑘 are public information.
Reduction of the problems. As will be discussed in Section 3,
both problems of interest can be reduced to a special case of exact
DTC, in which 𝑁=𝑂(𝑘). Moreover, this special case is of great
interest in its own right, since the gap of communication costs
between the naive algorithm and the best algorithm is the largest.
Thus, we mainly focus on analyzing algorithms for 𝑁=𝑂(𝑘)and
provide a discussion on how to extend to general 𝑁and approxi-
mate algorithm in Section 3.
Deterministic algorithms. We develop three deterministic algo-
rithms with progressively intricate designs. Our final algorithm
achieves the following result that has the strongest guarantees on
both communication cost and rounds. 𝛼(𝑘)is the inverse Acker-
mann function defined in Section 2, which is a small constant in
practice.
Theorem 1.3 (Deterministic algorithm). The exact DTC with
𝑁=𝑂(𝑘)can be solved deterministically with 𝑂(𝑘)communication
and𝑂(𝛼(𝑘))rounds.
Randomized Algorithm. With the power of randomization, we
propose an algorithm making use of a sampling approach that
balances communication, rounds, and error probability.
Theorem 1.4 (Randomized Algorithm). The exact DTC with
𝑁=𝑂(𝑘)can be solved in 𝑟rounds with randomized communication
cost𝑂
𝐸𝑘1
𝑟−1+𝑘𝑟
and failure probability at most 𝛿=2−𝐸.
1.2 Related Work
Distributed Sum-Equal. In the Sum-Equal problem, the coordina-
tor needs to decide whether the sum of the inputs equals a specific
value. Several works [ 2,20,23] have considered this problem and
current best upper bound is 𝑂(𝑘log(𝑘/𝛿))[23]. It is worth not-
ing that adapting their method to the DTC problem usually needs
𝑂(log𝑁)for performing dichotomy on log(𝑁)length bits.Distributed Tracking. A setting similar to the DTC problem is
distributed tracking [ 9,10,15], where the input of each site is given
gradually over time instead of directly. [ 10] provides a tight de-
terministic algorithm with communication cost of Θ(𝑘log(𝑁/𝑘))
bits. Under the assumption that the data follows a certain distri-
bution, [ 25] improves the results to 𝑂(𝑘log log𝑁)words. While
distributed tracking enjoys many similarities with our problems,
directly translating their algorithms to our setting will incur an
unacceptable round complexity of 𝑘.
Rounds and Communication Tradeoff. Plenty of works study
the tradeoff between communication rounds and the communica-
tion cost. Under the blackboard model, [ 3,5] study the tradeoff
between the number of interaction rounds and the bits needed to
solve the problem of set disjointness and maximal independent set
respectively. For the problems of set intersection and equality test-
ing, [ 7,21] studies tradeoff between rounds, communication cost,
and [ 14] further studies tradeoff among two mentioned parameters
and probability of error.
2 Preliminary
For brevity, we use the notation log(𝑥)=log2(𝑥)andexp(𝑥)=2𝑥
throughout the paper. Let log∗(𝑥)denote the iterated logarithm of
𝑥, which is defined as the number of times the logarithm function
must be applied before the result is less than or equal to 1. That is
log∗(𝑥)=min{𝑧|𝑧times
z      }|      {
log(···log(𝑥))≤ 1}.
The iterated logarithm is useful with its application in algorithms
and computational complexity [ 1,11] and grows at a slow rate.
Moreover, based on this definition, We can define log∗∗(·)in a sim-
ilar way that it denotes the number of times the iterated logarithm
function must be applied before the result is less than or equal to 1:
log∗∗(𝑥)=min{𝑧|𝑧times
z         }|         {
log∗(···log∗(𝑥))≤1}.
It is clear that log∗∗(·)grows even more slowly than log∗(·). Gen-
eralizing the definitions to higher degrees, we have the form of
log𝑣timesz}|{
∗···∗(𝑥)=min{𝑧|log𝑣−1timesz}|{
∗···∗(···log𝑣−1timesz}|{
∗···∗
|                         {z                         }
𝑧times(𝑥))≤1},
where we let log0timesz}|{
∗···∗(𝑥)=log(𝑥). The growth rate of the func-
tion declines for higher degrees and we are finally ready to define
the inverse Ackermann function as
𝛼(𝑥)=min{𝑧|log𝑧timesz}|{
∗···∗(𝑥)≤3}.
The inverse Ackermann function grows at an extremely slow rate
that for example, 𝛼
22265536
=2. Based on these definitions, for
any integer 𝑣,𝑤≥0, we use𝑓(𝑣,𝑤)to denote the function that
𝑓(𝑣,𝑤)=log𝑣timesz}|{
∗···∗(𝑓(𝑣,𝑤+1)),
4665Distributed Thresholded Counting with Limited Interaction KDD ’24, August 25–29, 2024, Barcelona, Spain.
where𝑓(𝑣,0)=1for non-negative integer 𝑣. This function de-
notes how the inverse of the iterated functions grows. For exam-
ple, when we take 𝑣=0, by𝑓(0,𝑤)=log(𝑓(0,𝑤+1)),𝑓(0,𝑤)
shows the growth of the inverse of iterated logarithm. To be more
clearly, we have that log∗(1)=0,𝑓(0,0)=1; log∗(2)=1,𝑓(0,1)=
2;log∗(4)=2,𝑓(0,2)=4;log∗(16)=3,𝑓(0,3)=16;generally we
have that𝑓(0,log∗(𝑘))=𝑘.
3 Reduction of the problems
In this section, we will reduce the problems defined in Definition 1.1
and 1.2 to the exact DTC with 𝑁=𝑂(𝑘). Firstly, for the problem
with general 𝑁, we have
Lemma 3.1 (Reduction of Exact DTC). An algorithm for the
Exact DTC with threshold ˜𝑁=𝑂(𝑘)that communicates 𝐶𝐶bits in𝑟
rounds implies an algorithm for the Exact DTC with general 𝑁that
communicates 𝐶𝐶+𝑂
𝑘log
𝑁
𝑘
bits in(𝑟+1)rounds.
Proof. The proof of Lemma 3.1 can be found in Section A. The
idea is to interpret the counter of each site 𝑛𝑖as𝑛𝑖=𝑎𝑖·𝑁
𝑘+𝑏𝑖.
Since𝑏𝑖can be encoded within 𝑂
log
𝑁
𝑘
bits, we only need to
consider whetherÍ
𝑖𝑎𝑖is larger than 𝑂(𝑘). □
[10] has shown that the deterministic lower bound of this prob-
lem is Ω
𝑘log
𝑁
𝑘
. Therefore, this term is tight and we only need
to consider the communication used for solving the problem when
𝑁=𝑂(𝑘). For the relaxed version of the problem, we have
Lemma 3.2 (Reduction of Approximate DTC). An algorithm
for the Exact DTC with threshold ˜𝑁=𝑂(𝑘)that communicates 𝐶𝐶
bits in𝑟rounds implies an algorithm for the Approximate DTC with
𝑁that communicates 𝐶𝐶+𝑂
𝑘log
1
𝜀
bits in(𝑟+1)rounds.
Proof. The proof of Lemma 3.2 can be found in Section A. The
idea is to interpret the counter of each site 𝑛𝑖as
𝑛𝑖=𝑎0
𝑖·𝑁
𝑘+𝑎1
𝑖·𝑁
2𝑘+···+𝑎𝑙
𝑖𝑁
2𝑙𝑘+𝑏𝑖.
Choosing𝑙=𝑂
1
𝜀
, we must haveÍ
𝑖𝑏𝑖<𝜀𝑁and will not affect
the value of the problem. Moreover, we can use 𝑂
log
1
𝜀
bits
to encode𝑎1
𝑖,···,𝑎𝑙
𝑖. We only need to whetherÍ
𝑖𝑎𝑖is larger than
𝑂(𝑘). □
The deterministic communication lower bound of the relaxed
problem is Ω
𝑘log
1
𝜀𝑘
. Our reduction is almost optimal and
matches the state-of-the-art algorithm that uses 𝑂
𝑘log
1
𝜀
bits
[10]. It is worth noting that their algorithm needs 𝑂(𝑘)rounds.
4 Deterministic Algorithms
One naive solution is that all site directly send their value to the
coordinator, which is a one-round algorithm. However, since the
value of each site can be as large as 𝑂(𝑘), this leads to a total com-
munication complexity of 𝑂(𝑘log𝑘)bits rather than the optimal
𝑂(𝑘). Another simple algorithm is that the coordinator asks for
the value of one site in each round, and stops immediately if the
current total sum becomes larger than the threshold. Let 𝑛1,···,𝑛𝑠Algorithm 1: Site𝑖: One-Layer Deterministic DTC
Input: Counter𝑛𝑖, Number of Rounds 𝑟
Initialize: Set˜𝑛𝑖,0=𝑛𝑖,{𝑡𝑗}𝑟
𝑗=1,𝑙=1
1When site𝑖receives the signal of the new round :
2 if˜𝑛𝑖,𝑙−1>𝑡𝑙then
3 Send to the server a signal that 1˜𝑛𝑖,𝑙−1>𝑡𝑙=1.
4 ˜𝑛𝑖,𝑙←˜𝑛𝑖,𝑙−1−𝑡𝑙.
5 else
6 Send to the server a signal that 1˜𝑛𝑖,𝑙−1>𝑡𝑙=0
7 Send to the server the value of ˜𝑛𝑖,𝑙−1.
8𝑙←𝑙+1.
be the values sent during the algorithm. We haveÍ𝑠
𝑖=1𝑛𝑖≤2𝑘, and
thus the communication costÍ𝑠
𝑖=1log𝑛𝑖≤𝑂(𝑘)by concavity of
thelogfunction. However, the number of rounds can be 𝑘in the
worst case due to the sequential nature of the algorithm.
Thus, one intriguing question is: Is there a deterministic algorithm,
which achieves both 𝑂(𝑘)communication cost and 𝑂(1)rounds?
This section investigates the above question comprehensively.
We provide three algorithms that use 𝑂(𝑘)bits with𝑂(log𝑘),
𝑂(log∗(𝑘))and𝑂(𝛼(𝑘))rounds, respectively. These algorithms are
built successively, incorporating several new ideas along the way.
Furthermore, the first 𝑂(log(𝑘))algorithm can be seen as a warm-
up to other algorithms and provides some intuition. Building upon
this, we propose our second 𝑂(log∗(𝑘))algorithm. This algorithm
is not only straightforward to implement but also achieves near-
optimal performance in many practical scenarios. For cases where
𝑘is extremely large and interaction is limited, our third algorithm
becomes particularly useful. Although it may be more challenging
to implement compared to the previous two algorithms, it offers
high theoretical interest with a round complexity of only 𝑂(𝛼(𝑘)).
We remark that the last algorithm with 𝛼(𝑘)rounds is practically
an𝑂(1)-round algorithm as the inverse Ackermann function grows
extremely slowly. However, whether it can be further reduced to
truly constant remains a major open question.
Overall, our three algorithms provide a smooth tradeoff between
implementation difficulty and efficiency in terms of round complex-
ity. The third algorithm can be employed if the scenario allows only
limited interaction. In many other practical settings, the ease of im-
plementation and good performance make the first two algorithms
viable choices.
4.1 An𝑂(log𝑘)Rounds Algorithm
We start by introducing the algorithm that uses 𝑂(𝑘)communi-
cation and𝑂(log𝑘)rounds. Actually, the algorithm can achieve a
smooth tradeoff between communication and round complexities.
Overview. The algorithm receives an integer 𝑟as input and pro-
ceeds in𝑟rounds. In the 𝑙-th round, there is a predefined threshold
ℎ𝑙and each site 𝑖notifies the coordinator whether 𝑛𝑖>ℎ𝑙or not.
If not, the site 𝑖sends𝑛𝑖−ℎ𝑙−1and will not participate in the
algorithm since then. In the first round, all sites are active. The
algorithm stops if all sites become inactive or the current sum main-
tained at the coordinator exceeds 𝑁. Clearly, the key is how to set
the grid of thresholds, and it turns out that the equidistant grid is
4666KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoyi Zhu, Yuxiang Tian, & Zengfeng Huang.
Algorithm 2: Coordinator: One-Layer Deterministic DTC
Input: Threshold𝑁, Number of Rounds 𝑟
Initialize: Set𝑚0=0,𝑟𝑖,0=1for each𝑖∈[𝑘],{𝑡𝑗}𝑟
𝑗=1
1ForRound𝑙from 1to𝑟:
2 ForSite𝑖from 1to𝑘:
3 if𝑟𝑖,𝑙−1=1then
4 Send to site 𝑖a signal of the new round.
5𝑟𝑖,𝑙←0.
6𝑚𝑙←𝑚𝑙−1.
7 When coordinator receives a signal from some site 𝑖:
8 if1˜𝑛𝑖,𝑙−1>𝑡𝑙=1then
9 𝑚𝑙←𝑚𝑙+𝑡𝑙.
10 𝑟𝑖,𝑙←1.
11 else
12 𝑚𝑙←𝑚𝑙+˜𝑛𝑖,𝑙−1.
13 if𝑚𝑙≥𝑁then
14 Output YES.
15Output NO.
not a good choice. Observing that the communication cost of each
site in round 𝑙is at most log(ℎ𝑙−ℎ𝑙−1)bits, which increases slowly
as the gap gets large. On the other hand, the number of active
sites decreases over rounds. Thus a natural choice is to double the
distance between two consecutive thresholds, i.e., ℎ𝑙−ℎ𝑙−1, after
each round, and we still expect the overall communication cost
decreases over rounds.
To implement the above idea we set 𝑢1=2log(𝑁)−𝑟+1,𝑢𝑙=2𝑢𝑙−1,
𝑙=2,···,𝑟and𝑡𝑙=⌈𝑢𝑙⌉for all𝑙∈[𝑟]. The algorithm is shown in
Algorithm 1 and 2. The threshold in round 𝑙isℎ𝑙=Í𝑙
𝑖=1𝑡𝑖.
Theorem 4.1 (Upper Bound of log(𝑘)rounds algorithm).
The Exact DTC with 𝑁=𝑂(𝑘)can be solved deterministically with 𝑟
rounds and communication 𝑂(𝑘(log(𝑘)−𝑟)+𝑘)bits.
Proof. Observe that at the point when site 𝑖becomes inactive,
the coordinator knows the exact value of 𝑛𝑖. Then, the correctness
of the algorithm is straightforward. For the NO case, the coordi-
nator will eventually get the exact values of all sites and thus the
output must be correct. For the YES case, the algorithm will either
(1) output YES (which is correct) or (2) run until all sites become
inactive. When (2) happens, the coordinator gets the exact values
of all sites and the output must be correct. Next, we focus on the
communication cost.
The First Round. Since each site can send a value smaller than
𝑡1≤𝑢1+1=2log(𝑁)−𝑟+1+1, the communication cost is at most
𝑘log(𝑡1+1)≤𝑘log
2log(𝑁)−𝑟+1+2
≤𝑘(log(𝑁)−𝑟+2)
≤𝑂(𝑘(log(𝑘)−𝑟)+𝑘).
The𝑙-th Round. Let𝑅𝑙=Í𝑘
𝑖=1𝑟𝑖,𝑙denote the number of active
sites in the𝑙-th round. Note, for any active site in the 𝑙-th round,
its value must be larger than 𝑡𝑙−1=⌈2log(𝑁)−𝑟+𝑙−1⌉. Therefore,
𝑅𝑙≤𝑁
⌈2log(𝑁)−𝑟+𝑙−1⌉.Otherwise, we must have that 𝑚𝑙≥𝑁andstop the iteration. In the 𝑙-th round, the communication cost of
each site is bound by
log(𝑡𝑙+1)≤log
2log(𝑁)−𝑟+𝑙+2
≤2(log(𝑁)−𝑟+𝑙).
Therefore, the total cost for the 𝑙-th round is bound by
𝑅𝑙+𝑅𝑙·log(𝑡𝑙+1)≤𝑁
⌈2log(𝑁)−𝑟+𝑙−1⌉+2𝑁(log(𝑁)−𝑟+𝑙)
⌈2log(𝑁)−𝑟+𝑙−1⌉
≤6𝑁(log(𝑁)−𝑟+𝑙)
2log(𝑁)−𝑟+𝑙,
where the first 𝑅𝑙denotes the cost of sending the signals.
Total Communication Cost. Let𝐶𝐶denote the cost of the algo-
rithm. Combining the costs for the first and all the other rounds,
the total cost is less than
𝐶𝐶≤𝑘(log(𝑁)−𝑟+2)+𝑟∑︁
𝑙=16𝑁(log(𝑁)−𝑟+𝑙)
2log(𝑁)−𝑟+𝑙
=𝑘(log(𝑁)−𝑟+2)+6𝑁log(𝑁)∑︁
𝑙=log(𝑁)−𝑟+1𝑙
2𝑙.
Now for the second part, denote 𝑥=Ílog(𝑁)
𝑙=log(𝑁)−𝑟+1𝑙
2𝑙, we
should have
𝑥−1
2𝑥=log(𝑁)−𝑟+1
2log(𝑁)−𝑟+1+log(𝑁)∑︁
𝑙=log(𝑁)−𝑟+21
2𝑙−log(𝑁)
2log(𝑁)+1
≤log(𝑁)−𝑟+1
2log(𝑁)−𝑟+1+1
2log(𝑁)−𝑟+1
=log(𝑁)−𝑟+2
2log(𝑁)−𝑟+1,
where the first inequation holds by the upper bound of the sum
of geometric progression. Therefore,
𝐶𝐶≤𝑘(log(𝑁)−𝑟+2)+6𝑁log(𝑁)∑︁
𝑙=log(𝑁)−𝑟+1𝑙
2𝑙
≤𝑘(log(𝑁)−𝑟+2)+6𝑁2(log(𝑁)−𝑟+2)
2log(𝑁)−𝑟+1
=𝑘(log(𝑁)−𝑟+2)+6·2𝑟(log(𝑁)−𝑟+2)
=𝑂(𝑘(log(𝑘)−𝑟)+𝑘),
where the last equality holds because 2𝑟≤2log(𝑁)=𝑂(𝑘).□
Taking𝑟=log(𝑁)=𝑂(log(𝑘)), we directly get
Corollary 4.2. The Exact DTC with 𝑁=𝑂(𝑘)can be solved
deterministically with 𝑂(𝑘)bits and𝑂(log(𝑘))rounds.
4.2 An𝑂(log∗𝑘)Rounds Algorithm
The log𝑘-round algorithm is already a significant improvement
over the naive 𝑘-round algorithm. Next we refine the idea further
to get an log∗𝑘-round algorithm. Compared to the log𝑘-round
algorithm, the only difference is a better choice of threshold grid,
where the increase rate is doubly exponential:
𝑢′
1=𝑓(0,log∗(𝑁)−𝑟+1), 𝑢′
𝑙=exp
𝑢′
𝑙−1
, 𝑙=2,···,𝑟,
4667Distributed Thresholded Counting with Limited Interaction KDD ’24, August 25–29, 2024, Barcelona, Spain.
and𝑡′
𝑙=l
𝑢′
𝑙m
for all𝑙∈[𝑟]. See Section 2 for the definition of 𝑓.
The idea behind the design of new grids is to maximize the size
of the grids under the constraint that the communication in each
round does not exceed 𝑂(𝑘). Moreover, we utilize a more careful
analysis to control the total communication up to 𝑟rounds.
Theorem 4.3 (Upper Bound of log∗(𝑘)rounds algorithm).
The Exact DTC with 𝑁=𝑂(𝑘)can be solved deterministically with 𝑟
rounds and communication 𝑂(𝑘𝑓(0,log∗(𝑘)−𝑟)+𝑘)bits.
Proof. By the same argument as above, the correctness is straight-
forward, and thus we focus on analyzing communication cost. Re-
call that by the definition of 𝑓(0,𝑤), we have that
𝑓(0,𝑤)=log(𝑓(0,𝑤+1)),and𝑓(0,0)=1.
We use𝐶𝐶𝑗to denote the communication for the 𝑗-th round. For the
communication cost, the worst case happens when the algorithm
stops after one round. As long as the algorithm goes to the second
round, the communication cost is bounded by 𝑂(𝑘)regardless of
the actual number of rounds in the execution. Next, we analyze the
two cases separately.
Case 1: stop after one round. Each site can send a value smaller
than𝑡′
1≤𝑢′
1+1=𝑓(0,log∗(𝑁)−𝑟+1)+1, the cost is at most
𝑘log(𝑡1+1)≤𝑘log 𝑓(0,log∗(𝑁)−𝑟+1)+2
≤𝑂(𝑘𝑓(0,log∗(𝑘)−𝑟)+𝑘).
Case 2: stops after 𝑙rounds with 𝑙≥2.The communication cost
is divided into two parts: the cost for round 1to𝑙−1and the cost
for the𝑙-th round. For the first 𝑙−1rounds, the communication is
𝑙−1∑︁
𝑗=1𝐶𝐶𝑗=𝑙−1∑︁
𝑗=1𝑘∑︁
𝑖=1
1𝑟𝑖,𝑗−1=1log min
𝑡𝑗,˜𝑛𝑖,𝑗−1	
+1+𝑟𝑖,𝑗−1
≤𝑙−1∑︁
𝑗=1𝑘∑︁
𝑖=1
1𝑟𝑖,𝑗−1=1min
𝑡𝑗,˜𝑛𝑖,𝑗−1	
+2𝑟𝑖,𝑗−1
≤3𝑙−1∑︁
𝑗=1𝑘∑︁
𝑖=1min
𝑡𝑗,˜𝑛𝑖,𝑗−1	
≤3𝑚𝑙−1,
where the second inequality holds because
𝑟𝑖,𝑗−1=1˜𝑛𝑖,𝑗−1>0≤min
𝑡𝑗,˜𝑛𝑖,𝑗−1	
.
Moreover, since we can pass to the 𝑙-th round, we must have that
𝑚𝑙−1≤𝑁.This implies the communication for the first 𝑙−1rounds
is at most𝑂(𝑁)=𝑂(𝑘)bits.
For the𝑙-th round, let 𝑅𝑙=Í𝑘
𝑖=1𝑟𝑖,𝑙denote the number of active
sites in the𝑙-th round. For any active site in 𝑙-th round, its value
is larger than 𝑡𝑙−1=⌈𝑓(0,log∗(𝑁)−𝑟+𝑙−1)⌉. Therefore, 𝑅𝑙≤
𝑁
⌈𝑓(0,log∗(𝑁)−𝑟+𝑙−1)⌉, since otherwise, we must have that 𝑚𝑙≥𝑁
and stop the iteration. In the 𝑙-th round, the communication cost of
each active site is bound by
log(𝑡𝑙+1)≤log 𝑓(0,log∗(𝑁)−𝑟+𝑙)+2
≤𝑓(0,log∗(𝑁)−𝑟+𝑙−1)+2.Algorithm 3: Site𝑖: Multi-Layer Deterministic DTC
Input: Counter𝑛𝑖, Number of Rounds 𝑟
Initialize: Set˜𝑛𝑖=𝑛𝑖,𝑙=1
1Compute the thresholds 𝑛𝑖belongs to and get 𝑗𝑖
𝑙such that
ℎ𝑟−𝑙,0
𝑗𝑖
𝑙≤𝑛𝑖<ℎ𝑟−𝑙,0
𝑗𝑖
𝑙+1for all𝑙∈[0,𝑟−1]. (Note that 𝑗𝑖𝑟=𝑛𝑖.)
2When site𝑖receives the signal of the new round :
3 if𝑛𝑖≤𝑓(0,𝑟)then
4 Send to the server a signal that 1𝑛𝑖≤𝑓(0,𝑟)=1.
5 if˜𝑛𝑖>𝑓(0,𝑙)then
6 Send to the server a signal that 1˜𝑛𝑖>𝑓(0,𝑙)=1.
7 ˜𝑛𝑖←˜𝑛𝑖−𝑓(0,𝑙).
8 else
9 Send to the server a signal that 1˜𝑛𝑖>𝑓(0,𝑙)=0.
10 Send to the server the value of ˜𝑛𝑖.
11 else
12 Send to the server a signal that 1𝑛𝑖≤𝑓(0,𝑟)=0.
13 if𝑙>1then
14 Send to the server 𝑗𝑖
𝑙−ℎ𝑟−𝑙+1,𝑟−𝑙
𝑗𝑖
𝑙−1.
15 else
16 Send to the server 𝑗𝑖
𝑙.
17𝑙←𝑙+1.
Therefore, the communication for 𝑙rounds each round is less than
𝑙∑︁
𝑗=1𝐶𝐶𝑗=𝑙−1∑︁
𝑗=1𝐶𝐶𝑗+𝐶𝐶𝑙≤3𝑁+𝑅𝑙+𝑅𝑙·log(𝑡𝑙+1)
≤3𝑁+𝑁(𝑓(0,log∗(𝑁)−𝑟+𝑙−1)+3)
⌈𝑓(0,log∗(𝑁)−𝑟+𝑙−1)⌉≤7𝑁=𝑂(𝑘).
This bound holds for any 𝑙≥2. □
Taking𝑟=log∗(𝑁)−1=𝑂(log∗(𝑘)), we get
Corollary 4.4. The Exact DTC with 𝑁=𝑂(𝑘)can be solved
deterministically with 𝑂(𝑘)bits and𝑂(log∗(𝑘))rounds.
4.3 An𝑂(𝛼(𝑘))Rounds Algorithm
While𝑂(log∗(𝑘))is already very small for any reasonable 𝑘, in this
section, we show that the round complexity can be further reduced
to𝑂(𝛼(𝑘)). As is introduced in Section 2, function 𝛼(𝑘)grows at
an extremely slow rate and is less than 3even if𝑘=22265536
. The
algorithms are shown in Algorithm 3 and 4. We will first introduce
the idea that brings us to our final algorithm.
Overview. Imagine that we place [0,𝑁]on a straight line as the
bottom layer 0. Our previous algorithms effectively divide this line
into multiple grids. Their job is first to determine the grid to which
each site’s value belongs by systematically testing from the smallest
grid to the largest one and then obtaining the exact value in this grid.
The grids are designed so that the number of them (corresponding
to the number of rounds) and communication of each round are
bounded.
4668KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoyi Zhu, Yuxiang Tian, & Zengfeng Huang.
Algorithm 4: Coordinator: Multi-Layer Deterministic DTC
Input: Threshold𝑁, Number of Rounds 𝑟
Initialize: Set𝑚0=0,𝑟𝑖,0=1for each𝑖∈[𝑘]
1ForRound𝑙from 1to𝑟:
2 ForSite𝑖from 1to𝑘:
3 if𝑟𝑖,𝑙−1=1then
4 Send to site 𝑖a signal of the new round.
5𝑟𝑖,𝑙←0.
6𝑚𝑙←𝑚𝑙−1
7 When coordinator receives a signal from some site 𝑖:
8 if1𝑛𝑖≤𝑓(0,𝑟)=1then
9 if˜𝑛𝑖>𝑓(0,𝑙)then
10 𝑚𝑙←𝑚𝑙+𝑓(0,𝑙),𝑟𝑖,𝑙←1.
11 else
12 𝑚𝑙←𝑚𝑙+˜𝑛𝑖.
13 else
14 if𝑙>1then
15 Compute𝑗𝑖
𝑙by adding back ℎ𝑟−𝑙+1,𝑟−𝑙
𝑗𝑖
𝑙−1.
16 𝑚𝑙←𝑚𝑙+ℎ𝑟−𝑙,0
𝑗𝑖
𝑙−ℎ𝑟−𝑙+1,0
𝑗𝑖
𝑙−1,𝑟𝑖,𝑙←1.
17 else
18 𝑚𝑙←𝑚𝑙+ℎ𝑟−𝑙,0
𝑗𝑖
𝑙,𝑟𝑖,𝑙←1.
19 if𝑛≥𝑁then
20 Output Yes.
21Output No.
Compared to that, a more effective way is to lift the grids to layer
1and let them act as an "express lane" for the layer below. That is,
in the first round, we directly get the value in layer 1, which helps
us to know which grid the value belongs to, and then in the next
round, we get the exact value in this grid at layer 0. This gives us an
algorithm that runs in 2 rounds and uses 𝑂(𝑘log(log∗(𝑁)))bits.
This idea can be generalized to get higher layers. That is, we
build higher layers to determine the grids that the value belongs
to in lower layers quickly. The main hardness of the method is to
construct such multiple-layer grids while satisfying the constraints
of communication complexity, which is our focus in the next part.
Construction of Grids. We use𝑑𝑙to denote the number of grids
at the𝑙th layer. Initially, at the bottom layer 0, we have 𝑡0
0=0,𝑡0
1=
1,···,𝑡0
𝑑0=𝑁,𝑑 0=𝑁.The construction of the first layer is similar
to the grids we use in Section4.2. We let 𝑡1
0=0,𝑡1
1=1and
𝑡1
𝑗+1=𝑡1
𝑗+exp
𝑡0
𝑡1
𝑗
=𝑡1
𝑗+exp
𝑡1
𝑗
.
Those points[𝑡1
𝑗,𝑡1
𝑗+1]in layer 0 belongs to grid 𝑗in layer 1. Now
for higher layers, assume that we already have grids at layer 𝑙being
𝑡𝑙
0,···,𝑡𝑙
𝑑𝑙and we would like to build the grids at the (𝑙+1)th layer.
Start with𝑡𝑙+1
0=0,𝑡𝑙+1
1=1. For𝑡𝑙+1
𝑗+1, We will firstly compute
ℎ𝑙+1,𝑙+1
𝑗=𝑡𝑙+1
𝑗,ℎ𝑙+1,𝑠
𝑗=𝑡𝑠
ℎ𝑙+1,𝑠+1
𝑗,𝑠=𝑙,···,0.Specifically, ℎ𝑙+1,𝑠
𝑗is the minimum value in the 𝑠th level that will
be assigned to the corresponding grid 𝑡𝑙+1
𝑗. For example, assume
that[4,16]in layer 0 belongs to [3,5]in layer 1 and finally 𝑡2
𝑗=2
in layer 2, then we have that ℎ𝑙+1,1
𝑗=3andℎ𝑙+1,0
𝑗=4.
We let𝑡𝑙+1
𝑗+1=𝑡𝑙+1
𝑗+exp
ℎ𝑙+1,0
𝑗
and keep computing until 𝑡𝑙+1
𝑗>
𝑑𝑙, and𝑑𝑙+1=minn
𝑗|𝑡𝑙+1
𝑗>𝑑𝑙o
.We have the upper bound of the
number of grids of each layer and the property of the grids that
Lemma 4.5 (Upper Bound of Number of grids). For the grids
constructed in Section 4.3, we have
𝑑𝑙≤log𝑙timesz}|{
∗···∗(𝑁)+1,∀𝑙∈[1,𝑟−1].
Proof. The proof of Lemma 4.5 can be found in Section B. The
idea is to first prove
𝑡𝑙
𝑗≥𝑓(𝑙−1,𝑗),∀𝑙∈[1,𝑟−1]
by induction and then gives the upper bound of the number of
thresholds. □
Lemma 4.6 (Property of grids). For the grids constructed in
Section 4.3, any 0≤𝑙≤𝛼(𝑘),𝛼in the layer 𝑙,𝛽in the layer 𝑙+1
and𝛼belongs to grid 𝛽,we have𝛼−ℎ𝑙+1,𝑙
𝛽≤ℎ𝑙,0
𝛼−ℎ𝑙+1,0
𝛽.
Proof. The proof of Lemma 4.6 can be found in Section B. □
Algorithm. With the above construction of grids, we can now
introduce our algorithms in Algorithm 3 and 4. For sites with a
value less than 𝑓(0,𝑟), we just perform the Algorithm 1 and 2 with
grids being 𝑓(0,1),···,𝑓(0,𝑟). For sites with large values, firstly,
we assign the value of grids on each layer based on our construction.
Then in each round 𝑙, sites send their value of grids in 𝑟−𝑙layers
to the coordinator. (Note that when 𝑙=𝑟, we are sending the value
in the bottom layer, which is also the exact value.) Using this value,
the coordinator updates the minimum value each site can be and
compares the sum to 𝑁. We have the guarantee that
Theorem 4.7 (Upper Bound of 𝛼(𝑘)Rounds Algorithm). The
Exact DTC with 𝑁=𝑂(𝑘)can be solved deterministically with 𝑟
rounds and communication 𝑂(𝑘log(log𝑟−1timesz}|{
∗···∗(𝑘))+𝑘)bits.
Proof. The correctness holds again due to our exact computa-
tion that in the last round, all sites will eventually send their exact
value. Next, We focus on the communication cost. For sites with a
value less than 𝑓(0,𝑟), their protocol is similar to Algorithms 1and 2
with grids𝑓(0,𝑙)for each round. Therefore, their communication
can be bounded by 𝑂(𝑘)with the same arguments in proof of The-
orem 4.1. We then consider sites with a value more than 𝑓(0,𝑟). We
use𝐶𝐶𝑗to denote the communication for the 𝑗-th round.
4669Distributed Thresholded Counting with Limited Interaction KDD ’24, August 25–29, 2024, Barcelona, Spain.
Case 1: stop after one round. In the first round each site can send
a value in[0,𝑑𝑟−1]. By Lemma 4.5, we have the cost is at most
𝑘log(𝑑𝑟−1+1)≤𝑘log(log𝑟−1timesz}|{
∗···∗(𝑁)+2)
≤𝑂(𝑘log(log𝑟−1timesz}|{
∗···∗(𝑘))+𝑘).
Case 1: stop after 𝑝rounds with 𝑝≥2.The communication cost
for𝑝rounds can be divided into two parts. The cost for 1to𝑝−1
rounds and the cost for the 𝑝-th round. For 𝑝≥2, since we can
pass the first round, each survival site must have a value larger
than𝑓(0,𝑟), and the number of them is less than𝑁
𝑓(0,𝑟). Therefore,
sending signals to them for 𝑟rounds needs bits𝑁
𝑓(0,𝑟)·𝑟≤𝑁=
𝑂(𝑘).For the first 𝑝−1rounds, the bits we have sent are
𝑝−1∑︁
𝑙=1𝐶𝐶𝑙=𝐶𝐶1+𝑝−1∑︁
𝑙=2𝐶𝐶𝑙
=∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟)log
𝑗𝑖
1+1
+𝑝−1∑︁
𝑙=2∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟)log
𝑗𝑖
𝑙−ℎ𝑟−𝑙+1,𝑟−𝑙
𝑗𝑖
𝑙−1+1
.
Since log𝑥≤𝑥for𝑥≥1, we have the communication is upper
bound by
∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟) 
𝑗𝑖
1+𝑝−1∑︁
𝑙=2
𝑗𝑖
𝑙−ℎ𝑟−𝑙+1,𝑟−𝑙
𝑗𝑖
𝑙−1!
+𝑝−1∑︁
𝑙=1∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟)2.
Now for the first part, applying Lemma 4.6 gives us
∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟) 
𝑗𝑖
1+𝑝−1∑︁
𝑙=2
𝑗𝑖
𝑙−ℎ𝑟−𝑙+1,𝑟−𝑙
𝑗𝑖
𝑙−1!
≤∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟) 
ℎ𝑟−1,0
𝑗𝑖
1+𝑝−1∑︁
𝑙=2
ℎ𝑟−𝑙,0
𝑗𝑖
𝑙−ℎ𝑟−𝑙+1,0
𝑗𝑖
𝑙−1!
≤∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟)ℎ𝑟−𝑝+1,0
𝑗𝑖
𝑝−1≤𝑁.
The last ineqation holds due to the condition to pass to the 𝑝-th
round. For the second part, we have
𝑝−1∑︁
𝑙=1∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟)2≤𝑝−1∑︁
𝑙=12𝑁
𝑓(0,𝑟)≤2𝑟·𝑁
𝑓(0,𝑟)≤2𝑁,
where the first and second inequation holds by direct counting and
the final inequation holds since function 𝑓(0,𝑟)grows significantly
faster than𝑟. Combining the results, we have that the communica-
tion in the first 𝑝−1rounds is bound by
𝑝−1∑︁
𝑙=1𝐶𝐶𝑙≤∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟)ℎ𝑟−𝑝+1,0
𝑗𝑖
𝑝−1+2𝑁,Algorithm 5: Site𝑖: Sampling-Procedure
Input: Counter𝑛𝑖, Sum Parameter 𝑝, Communication
parameter𝐶𝐶, Error probability 𝛿
1if𝐶𝐶≥𝑝·log
2
𝛿
then
2 Set𝑞←log(2
𝛿)
𝑁.
3else
4 Set𝑞←𝐶𝐶
𝑁𝑝.
5Generate ˜𝑛𝑖∼Binomial(𝑛𝑖,𝑞).
6Send ˜𝑛𝑖to the coordinator.
Algorithm 6: Coordinator: Sampling-Procedure
Input: Threshold𝑁, Sum Parameter 𝑝, Communication
parameter𝐶𝐶, Error probability 𝛿
Initialize: Set˜𝑛=0.
1if𝐶𝐶≥𝑝·log
2
𝛿
then
2 Set𝑇←4 log
6𝑟
𝛿
.
3else
4 Set𝑇←𝐶𝐶
𝑝+3 log
2
𝛿
.
5When coordinator receives signals from some site 𝑖:
6 ˜𝑛←˜𝑛+˜𝑛𝑖.
7if˜𝑛≥𝑇then
8 Return Yes.
9Return No.
In the𝑝-th round, the communication of each site is bound by
log
ℎ𝑟−𝑝,0
𝑗𝑖
𝑝−ℎ𝑟−𝑝+1,0
𝑗𝑖
𝑝−1+1
≤log
exp
ℎ𝑟−𝑝+1,0
𝑗𝑖
𝑝−1
+1
≤ℎ𝑟−𝑝+1,0
𝑗𝑖
𝑝−1+1,
where the first inequality holds due to our construction of grids.
Therefore, the communication for 𝑝rounds each round is less than
𝑝∑︁
𝑙=1𝐶𝐶𝑙=𝑝−1∑︁
𝑙=1𝐶𝐶𝑗+𝐶𝐶𝑝
≤∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟)ℎ𝑟−𝑝+1,0
𝑗𝑖
𝑝−1+2𝑁+∑︁
𝑖:𝑛𝑖>𝑓(0,𝑟)
ℎ𝑟−𝑝+1,0
𝑗𝑖
𝑝−1+1
≤4𝑁+𝑁
𝑓(0,𝑟)≤5𝑁.
This holds for any 𝑝≥2. The proof completes by taking 𝑁=𝑂(𝑘).
□
Taking𝑟=𝛼(𝑁)=𝑂(𝛼(𝑘)), we have the following corollary.
Corollary 4.8 (Theorem 1.3 restatement). The Exact DTC
with𝑁=𝑂(𝑘)can be solved deterministically with 𝑂(𝑘)bits com-
munication and 𝑂(𝛼(𝑘))rounds.
4670KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoyi Zhu, Yuxiang Tian, & Zengfeng Huang.
5 Randomized Algorithms
We have shown that there is a deterministic algorithm with optimal
communication cost and 𝛼(𝑘)rounds, but whether there is an 𝑂(1)-
round deterministic algorithm with similar communication cost
is unclear. On the other hand, we show that such an algorithm
exists if we allow randomization and a tiny probability of error. The
algorithms are shown in Algorithm 7 and Algorithm 8.
Overview. The key challenge in our problem is that there may
be many sites with large values. Even if their sum exceeds the
threshold by a significant margin, sending these values can cause
substantial communication costs. To address this challenge, instead
of constructing grids as in our deterministic algorithm, we utilize a
sampling approach to gradually narrow down the possible range
of our sum. To avoid significant communication, we start with a
lower sampling probability to filter out larger values in the sum and
increase this probability over rounds. The sampling procedure is
shown in algorithm 5 and 6. Here we use an extra input parameter
𝑝which would influence this procedure. We have the following
property of this process summarized in Lemma 5.1.
Lemma 5.1 (Property of Sampling Procedure). For the sam-
pling procedure shown in algorithm 5 and 6, if the sum 𝑛=Í𝑘
𝑖=1𝑛𝑖≤
12𝑁𝑝, with probability at least 1−2𝛿, the procedure will output 0 when
𝑛≤𝑁and use less than 24𝐶𝐶bits for communication. Moreover,
•Forlog
2
𝛿
<𝐶𝐶<𝑝·log
2
𝛿
, with probability at least 1−𝛿,
the procedure will output 1 when 𝑛>12𝑁𝑝·log(2
𝛿)
𝐶𝐶>12𝑁.
•For𝐶𝐶≥𝑝·log
2
𝛿
, with probability at least 1−𝛿, the
procedure will output 1 when 𝑛>12𝑁.
Proof. The proof of Lemma 5.1 can be found in Section C. The
idea is to apply Chernoff bound to control the probability of the
sampling procedure outputting 1when𝑛is small and the probability
of outputting 0when𝑛is large. □
With Lemma 5.1, we have the guarantee of our algorithm 7 and 8.
Theorem 5.2 (Theorem 1.4 restatement). The Exact DTC with
𝑁=𝑂(𝑘)can be solved in 𝑟rounds with randomized communication
cost𝑂
𝐸𝑘1
𝑟−1+𝑘𝑟
bits with failure probability at most 𝛿=2−𝐸.
This shows that we can achieve the optimal 𝑂(𝑘)in a constant
number of rounds and small failure probability. For example, we
only need 3 rounds and have failure probability 𝛿=2−√
𝑘.
Proof. We set the error probability for the sampling procedure
to𝛿
3𝑟. By union bound, the property in Lemma 5.1 holds for 𝑟−1
rounds invocation with probability at least 1−𝑟−1
𝑟𝛿. Moreover, we
will choose 𝐶𝐶=𝑘1
𝑟−1log
6𝑟
𝛿
in our algorithm.
Correctness. Initially, the total sum is upper bounded by 𝑘𝑁and
corresponding 𝑝1=𝑘
12. Using Lemma 5.1, if the sum is guaranteed
to be less than 12𝑁𝑝𝑙after round𝑙, we have that for the 𝑙+1rounds,
•either the sampling procedure outputs 1, which means that
𝑛>12𝑁and we can stop the iteration;
•or the sampling procedure outputs 0, and we have the new
guarantee𝑛≤12𝑁𝑝𝑙+1with𝑝𝑙+1=𝑝𝑙·log(6𝑟
𝛿)
𝐶𝐶.Therefore, with probability at least 1−𝛿, when the sampling proce-
dure outputs 1, we are confirmed thatÍ𝑘
𝑖=1𝑛𝑖>𝑁for the first𝑟−1
rounds. In the last round, we let each site send its value directly.
The correctness holds for exact computation.
Communication Cost. If the algorithm passes the 𝑙th round for
𝑙≤𝑟−2, the sum is smaller than 12𝑁𝑝𝑙=𝑘𝑁
log
6𝑟
𝛿
/𝐶𝐶𝑙
.
Therefore, considering all 𝑙+1rounds we need to sample it, let ˜𝑁
be the total sum of sampled value, the expectation is at most
E[˜𝑁]≤𝑘𝑁©­­
«log
6𝑟
𝛿
𝐶𝐶ª®®
¬𝑙
𝑙+1∑︁
𝑗=112𝐶𝐶
𝑘𝑁
log(6𝑟
𝛿)
𝐶𝐶𝑗−1+12𝑁𝑝𝑟−2log
6𝑟
𝛿
𝑁
≤12𝐶𝐶©­­­
«𝑙+1∑︁
𝑗=1©­­
«log
6𝑟
𝛿
𝐶𝐶ª®®
¬𝑙−𝑗+1
+1ª®®®
¬≤12𝐶𝐶
1−log(6𝑟
𝛿)
𝐶𝐶+12𝐶𝐶
=12𝐶𝐶
1−1
𝑘1
𝑟−1+12𝐶𝐶≤36𝐶𝐶,
where the last inequality holds for 𝑘≥2𝑟(Otherwise, we can do
it deterministically with 𝑟=𝑂(𝛼(𝑘))). We can now compute the
total number of samples we may take. By Chernoff Bound, we have
Pr[˜𝑁≥72𝐶𝐶]<Pr[|𝑁−𝐸[𝑁]|≥36𝐶𝐶]<2 exp
−36𝐶𝐶
3
<2 exp
−12 log6𝑟
𝛿
=2𝛿
6𝑟12
<𝛿
𝑟.
With probability at least 1−𝛿
𝑟, The number of samples we take
is upper bounded by 𝑂(𝐶𝐶)=𝑂
𝑘1
𝑟−1log
6𝑟
𝛿
. Moreover, our
choice of𝐶𝐶=𝑘1
𝑟−1log
6𝑟
𝛿
guarantees that after at most 𝑟−2
rounds, we have 𝑛<12𝑁𝑝𝑟−1with
𝑝𝑟−1·log6𝑟
𝛿
=1
12𝑘©­­
«log
6𝑟
𝛿
𝐶𝐶ª®®
¬𝑟−2
log6𝑟
𝛿
=1
12𝑘1
𝑟−1log6𝑟
𝛿
≤𝐶𝐶.
With the second part of Lemma 5.1, we will have that 𝑛≤12𝑁if
the sampling procedure in the 𝑟−1round outputs 0. Therefore,
the communication for the last round is at most 𝑂(𝑁)=𝑂(𝑘)by
concavity of the log function. The extra 𝑂(𝑘𝑟)is used for alerting
the new rounds. Taking 𝛿=2−𝐸completes the proof.
□
6 Experimental Evaluation
In this section, we evaluate the proposed algorithms on a machine
running Intel(R) Xeon(R)CPU E5-2678 v3 @ 2.50GHZ. The four algo-
rithms are compared under the synthetic datasets that we construct.
For brevity, we use the names: log,star ,alpha andrandomize to
denote the deterministic algorithms with round complexity log(𝑘),
log∗(𝑘),𝛼(𝑘)and the randomized algorithm respectively. We set
the error probability of the randomized algorithm being 10−8.
4671Distributed Thresholded Counting with Limited Interaction KDD ’24, August 25–29, 2024, Barcelona, Spain.
Algorithm 7: Site𝑖: Randomized DTC
Input: Counter𝑛𝑖, Number of Rounds 𝑟, Communication
parameter𝐶𝐶, Error probability 𝛿
Initialize: Set𝑙=1,𝐶𝐶=𝑘1
𝑟−1log
6𝑟
𝛿
,𝑝1=𝑘
12.
1When site𝑖receives the signal of the new round :
2𝑆𝑎𝑚𝑝𝑙𝑖𝑛𝑔−𝑃𝑟𝑜𝑐𝑒𝑑𝑢𝑟𝑒
𝑛𝑖,𝑝𝑙,𝐶𝐶,𝛿
3𝑟
.
3𝑙←𝑙+1,𝑝𝑙←𝑝𝑙−1
log(6𝑟
𝛿)
𝐶𝐶
.
4When site𝑖receives the special signal :
5 Send to the coordinator the value of counter 𝑛𝑖.
Algorithm 8: Coordinator: Randomized DTC
Input: Threshold𝑁, Number of Rounds 𝑟, Communication
parameter𝐶𝐶, Error probability 𝛿
Initialize: Set𝑛=0,𝐶𝐶=𝑘1
𝑟−1log
6𝑟
𝛿
,𝑝1=𝑘
12.
1ForRound𝑙from 1to(𝑟−1):
2 ForSite𝑖from 1to𝑘:
3 Send to site 𝑖a signal of the new round
4𝑚𝑙←𝑆𝑎𝑚𝑝𝑙𝑖𝑛𝑔−𝑃𝑟𝑜𝑐𝑒𝑑𝑢𝑟𝑒
𝑁,𝑝𝑙,𝐶𝐶,𝛿
3𝑟
.
5 if𝑚𝑙=Yes then
6 Output Yes.
7𝑝𝑙←𝑝𝑙−1
log(6𝑟
𝛿)
𝐶𝐶
.
8ForSite𝑖from 1to𝑘:
9 Send to site 𝑖a special signal
10When coordinator receives 𝑛𝑖from some site 𝑖:
11𝑛←𝑛+𝑛𝑖
12if𝑛>𝑁then
13 Output Yes.
14Output No.
In the synthetic datasets, the number of the sites 𝑘ranges from
1,000,000to100,000,000and the threshold 𝑁=2𝑘. For each gen-
erated value 𝑥, if it is not an integer, we take ⌈𝑥⌉. For1
10fraction
of the sites, we generate random value from a uniform distribution
on[0,𝑁), and for the rest of the sites, we simply assign 0.
It is important to note that our algorithms perform well with
worst-case communication and round upper bounds regardless of
the inputs received. The choice of specific inputs in our evaluation
aims to demonstrate the worst-case capabilities of our algorithms.
In this particular scenario, directly transmitting all the inputs would
result in suboptimal communication complexity of 𝑂(𝑘log𝑘), while
employing the algorithm described in [ 10] requires𝑂(𝑘)rounds if
the sites with values are the last ones to be inquired. In comparison,
our algorithms achieve better communication-round trade-offs in
this setting.
The main parameters in our algorithm are the number of sites 𝑘
and the round complexity 𝑟and we will fix them respectively.Fix Round Complexity. The first row of Figure 1 illustrates the
result of the algorithms when the number of rounds is fixed to be 2
or3. While the communication of all the algorithms grows nearly
linearly over 𝑘, our final deterministic algorithm alpha and ran-
domized algorithm randomize perform the best with a significantly
lower slope, which is consistent with theoretical results.
Moreover, Algorithm alpha performs better with 2 rounds while
algorithm randomize is superior with 3 rounds. This is expected as
our 2-round randomized algorithm has a complexity of 𝑂(𝑘log
1
𝛿
+
𝑘), where the first term dominates. For 3 rounds, the first term drops
to only𝑂√
𝑘log
1
𝛿
, leading to better performance.
Finally, there are discontinuous parts appearing on the both
lines. This can be attributed to the discontinuous nature of the grid
construction. The grids constructed based on the inputs remain
unchanged until the size of 𝑘is doubled. However, from an asymp-
totic perspective, the lines still exhibit a linear relationship with 𝑘,
which aligns with our analysis.
Fix Number of Sites. We fix the number of sites 𝑘and compute
the minimum communication that algorithms need within different
round complexity 𝑟. The results are shown in the second row of
Figure 1. The convergence of the algorithms illustrates the number
of rounds required to attain optimal communication. Compared
tolog,alpha andrandomize exhibit a better performance with
lower communication complexity and fewer rounds to converge.
This shows that our algorithms successfully balance the round
complexity and communication cost.
0.0 0.2 0.4 0.6 0.8 1.0
number of sites1e80.00.51.01.52.02.5comunication complexity (bits)1e8 2 rounds
log
star
alpha
randomize
0.0 0.2 0.4 0.6 0.8 1.0
number of sites1e80.00.51.01.52.02.5comunication complexity (bits)1e8 3 rounds
log
star
alpha
randomize
0 5 10 15 20 25
number of rounds0.00.51.01.52.02.5comunication complexity (bits)1e7 k=10,000,000
log
star
alpha
randomize
0 5 10 15 20 25
number of rounds0.00.51.01.52.02.5comunication complexity (bits)1e8 k=100,000,000
log
star
alpha
randomize
Figure 1: First Row: The communication complexity v.s. num-
ber of sites under a fixed number of rounds
Second Row: The communication complexity v.s. number of
rounds under a fixed number of sites
7 Acknowledgements
This work is supported by National Natural Science Foundation of
China No. 62276066, No. U2241212
4672KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoyi Zhu, Yuxiang Tian, & Zengfeng Huang.
References
[1]Noga Alon and Yossi Azar. 1989. Finding an approximate maximum. SIAM J.
Comput. 18, 2 (1989), 258–267.
[2]Daniel Apon, Jonathan Katz, and Alex J Malozemoff. 2013. One-round multi-party
communication complexity of distinguishing sums. Theoretical Computer Science
501 (2013), 101–108.
[3]Sepehr Assadi, Gillat Kol, and Zhijun Zhang. 2022. Rounds vs communication
tradeoffs for maximal independent sets. In 2022 IEEE 63rd Annual Symposium on
Foundations of Computer Science (FOCS). IEEE, 1193–1204.
[4]Mark Braverman and Rotem Oshman. 2015. On information complexity in the
broadcast model. In Proceedings of the 2015 ACM Symposium on Principles of
Distributed Computing. 355–364.
[5]Mark Braverman and Rotem Oshman. 2017. A rounds vs. communication trade-
off for multi-party set disjointness. In 2017 IEEE 58th Annual Symposium on
Foundations of Computer Science (FOCS). IEEE, 144–155.
[6]Joshua Brody, Amit Chakrabarti, Ranganath Kondapally, David P Woodruff,
and Grigory Yaroslavtsev. 2014. Beyond set disjointness: the communication
complexity of finding the intersection. In Proceedings of the 2014 ACM symposium
on Principles of distributed computing. 106–113.
[7]Joshua Brody, Amit Chakrabarti, Ranganath Kondapally, David P Woodruff,
and Grigory Yaroslavtsev. 2014. Certifying equality with limited interaction. In
Approximation, Randomization, and Combinatorial Optimization. Algorithms and
Techniques (APPROX/RANDOM 2014). Schloss Dagstuhl-Leibniz-Zentrum fuer
Informatik.
[8]Badrish Chandramouli, Suman Nath, and Wenchao Zhou. 2013. Supporting
distributed feed-following apps over edge devices. Proceedings of the VLDB
Endowment 6, 13 (2013), 1570–1581.
[9]Graham Cormode. 2013. The continuous distributed monitoring model. ACM
SIGMOD Record 42, 1 (2013), 5–14.
[10] Graham Cormode, Shanmugavelayutham Muthukrishnan, and Ke Yi. 2011. Algo-
rithms for distributed functional monitoring. ACM Transactions on Algorithms
(TALG) 7, 2 (2011), 1–20.
[11] Olivier Devillers. 1992. Randomization yields simple 𝑜(𝑛log∗𝑛)algorithms
for difficult𝜔(n) problems. International Journal of Computational Geometry &
Applications 2, 01 (1992), 97–111.
[12] Danny Dolev and Tomás Feder. 1989. Multiparty communication complexity. IBM
Thomas J. Watson Research Division.
[13] Jürgen Forster, Niels Schmitt, Hans Ulrich Simon, and Thorsten Suttorp. 2003.
Estimating the optimal margins of embeddings in euclidean half spaces. Machine
Learning 51 (2003), 263–281.
[14] Dawei Huang, Seth Pettie, Yixiang Zhang, and Zhijun Zhang. 2021. The com-
munication complexity of set intersection and multiple equality testing. SIAM J.
Comput. 50, 2 (2021), 674–717.
[15] Zengfeng Huang, Ke Yi, and Qin Zhang. 2019. Randomized algorithms for
tracking distributed count, frequencies, and ranks. Algorithmica 81, 6 (2019),
2222–2243.
[16] Philo Juang, Hidekazu Oki, Yong Wang, Margaret Martonosi, Li Shiuan Peh,
and Daniel Rubenstein. 2002. Energy-efficient computing for wildlife tracking:
Design tradeoffs and early experiences with ZebraNet. In Proceedings of the 10th
international conference on Architectural support for programming languages and
operating systems. 96–107.
[17] Ram Keralapura, Graham Cormode, and Jeyashankher Ramamirtham. 2006.
Communication-efficient distributed monitoring of thresholded counts. In Pro-
ceedings of the 2006 ACM SIGMOD international conference on Management of
data. 289–300.
[18] Nati Linial and Adi Shraibman. 2009. Learning complexity vs communication
complexity. Combinatorics, Probability and Computing 18, 1-2 (2009), 227–245.
[19] Samuel R Madden, Michael J Franklin, Joseph M Hellerstein, and Wei Hong. 2005.
TinyDB: an acquisitional query processing system for sensor networks. ACM
Transactions on database systems (TODS) 30, 1 (2005), 122–173.
[20] Noam Nisan. 1993. The communication complexity of threshold gates. Combina-
torics, Paul Erdos is Eighty 1 (1993), 301–315.
[21] Mert Saglam and Gábor Tardos. 2013. On the communication complexity of sparse
set disjointness and exists-equal problems. In 2013 IEEE 54th Annual Symposium
on Foundations of Computer Science. IEEE, 678–687.
[22] Jochen Schiller and Agnès Voisard. 2004. Location-based services. Elsevier.
[23] Emanuele Viola. 2015. The communication complexity of addition. Combinatorica
35 (2015), 703–747.
[24] David P Woodruff and Qin Zhang. 2013. When distributed computation does not
help. CoRR, abs/1304.4636 5 (2013).
[25] Hao Wu, Junhao Gan, and Rui Zhang. 2020. Learning based distributed tracking.
InProceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 2040–2050.A Proof in Section 3
Lemma A.1 (Lemma 3.1 Restatement). An algorithm for the
Exact DTC with threshold ˜𝑁=𝑂(𝑘)that communicates 𝐶𝐶bits in𝑟
rounds implies an algorithm for the Exact DTC with general 𝑁that
communicates 𝐶𝐶+𝑂
𝑘log
𝑁
𝑘
bits in(𝑟+1)rounds.
Proof. We assume that𝑁
𝑘is an integer without loss of general-
ity, otherwise replacing it by ⌊𝑁
𝑘⌋we can get the same result up to
a constant. By Euclid’s division lemma, the counter of each site 𝑛𝑖
can be interpreted as 𝑛𝑖=𝑎𝑖·𝑁
𝑘+𝑏𝑖for some𝑏𝑖∈
0,𝑁
𝑘
. Thus
we can encode each 𝑏𝑖within𝑂
log
𝑁
𝑘
bits.
To solve the problem for general 𝑁, we first run the algorithm
for𝑁=𝑂(𝑘)when each site holds their 𝑎𝑖and the threshold is 𝑘.
If the algorithm outputs 1, we must have that
𝑘∑︁
𝑖=1𝑛𝑖>𝑁
𝑘𝑘∑︁
𝑖=1𝑎𝑖>𝑁.
If the algorithm outputs 0, we let each site send their 𝑎𝑖and𝑏𝑖to
the coordinator. The correctness comes from the exact computation
and the communication is bound by
𝑘∑︁
𝑖=1(log(𝑎𝑖+1)+log(𝑏𝑖+1))≤𝑘∑︁
𝑖=1𝑎𝑖+𝑘∑︁
𝑖=1log𝑁
𝑘
+2𝑘
≤𝑂
𝑘log𝑁
𝑘
.
□
Lemma A.2 (Lemma 3.2 Restatement). An algorithm for the
Exact DTC with threshold ˜𝑁=𝑂(𝑘)that communicates 𝐶𝐶bits in𝑟
rounds implies an algorithm for the Approximate DTC with general
𝑁that communicates 𝐶𝐶+𝑂
𝑘log
1
𝜀
bits in(𝑟+1)rounds.
Proof. We assume that𝑁
𝑘is an integer without loss of general-
ity, otherwise replacing it by ⌊𝑁
𝑘⌋we can get the same result up to
a constant. By Euclid’s division lemma, the counter of each site 𝑛𝑖
can be interpreted as
𝑛𝑖=𝑎0
𝑖·𝑁
𝑘+𝑎1
𝑖·𝑁
2𝑘+···+𝑎𝑙
𝑖𝑁
2𝑙𝑘+𝑏𝑖.
Notice that we have for all 𝑗∈[1,𝑙],𝑎𝑗
𝑖∈{0,1}. This is because if
𝑎𝑗
𝑖≥2, we can subtract 2 from 𝑎𝑗
𝑖and add 1 to 𝑎𝑗−1
𝑖. We choose 𝑙
to be the smallest number such that1
2𝑙<𝜀. It is easy to verify that
𝑙=𝑂
1
𝜀
and𝑏𝑖<𝜀𝑁
𝑘.
To solve the problem for the relaxed problem, we first run the
algorithm for 𝑁=𝑂(𝑘)when each site holds their 𝑎0
𝑖and the
threshold is 𝑘. If the algorithm outputs 1, we must have that
𝑘∑︁
𝑖=1𝑛𝑖>𝑁
𝑘𝑘∑︁
𝑖=1𝑎0
𝑖>𝑁.
4673Distributed Thresholded Counting with Limited Interaction KDD ’24, August 25–29, 2024, Barcelona, Spain.
If the algorithm outputs 0, we let each site send their 𝑎𝑗
𝑖,𝑗∈[0,𝑙]
to the coordinator and then compute the value of
𝑘∑︁
𝑖=1
𝑎0
𝑖·𝑁
𝑘+𝑎1
𝑖·𝑁
2𝑘+···+𝑎𝑙
𝑖𝑁
2𝑗𝑘
.
If this value is less than (1−𝜀)𝑁, we output 0 and we output
1 otherwise. This is correct because: If the true sum is less than
(1−𝜀)𝑁, we must have that this value is also less than (1−𝜀)𝑁
and output correctly. Moreover, if the true sum is larger than 𝑁,
we must have that this value is larger than (1−𝜀)𝑁and we output
1 correctly:
𝑘∑︁
𝑖=1
𝑎0
𝑖·𝑁
𝑘+𝑎1
𝑖·𝑁
2𝑘+···+𝑎𝑙
𝑖𝑁
2𝑗𝑘
=𝑛∑︁
𝑖=1𝑛𝑖−𝑘∑︁
𝑖=1𝑏𝑖
>𝑁−𝑘∑︁
𝑖=1𝜀𝑁
𝑘=(1−𝜀)𝑁.
For the communication complexity, in the last round, we send
𝑘∑︁
𝑖=1©­
«𝑙∑︁
𝑗=0log(𝑎𝑗
𝑖+1)ª®
¬≤𝑘∑︁
𝑖=1
𝑎0
𝑖+1
+𝑘∑︁
𝑖=1©­
«𝑙∑︁
𝑗=1log(𝑎𝑗
𝑖+1)ª®
¬
≤2𝑘+𝑘∑︁
𝑖=1©­
«𝑙∑︁
𝑗=11ª®
¬≤𝑂
𝑘log1
𝜀
.
□
B Proof in Section 4.3
We will first prove the lower bound of the value of the threshold by
induction. By the definition of the 𝑓(𝑣,𝑤)function, we can prove
the following lemma.
Lemma B.1 (Property of 𝑓(𝑣,𝑤)).For the function 𝑓(𝑣,𝑤)de-
fined in Section 2, we have 𝑓(𝑣,𝑤)=𝑓(𝑣−1,𝑓(𝑣,𝑤−1)).
Proof. By the definition,
𝑓(𝑣,𝑤)=log𝑣timesz}|{
∗···∗(𝑓(𝑣,𝑤+1)),𝑓(𝑣,0)=1
log𝑣timesz}|{
∗···∗(𝑥)=min{𝑧|𝑧times
z                               }|                               {
log𝑣−1timesz}|{
∗···∗(···log𝑣−1timesz}|{
∗···∗(𝑥))≤1},
For value𝑥, we must have that
𝑥=𝑓(𝑣,log𝑣+1timesz}|{
∗···∗(𝑥)).
Therefore,
𝑓(𝑣,𝑤)=𝑓(𝑣−1,log𝑣timesz}|{
∗···∗(𝑓(𝑣,𝑤)))=𝑓(𝑣−1,𝑓(𝑣,𝑤−1)).
□
Lemma B.2 (Lower Bound of Value of Thresholds). For the
grids constructed in 4.3, we have
𝑡𝑙
𝑗≥𝑓(𝑙−1,𝑗),∀𝑙∈[1,𝑟−1].Proof. Level 1:𝑡1
𝑗≥𝑓(0,𝑗)
Note that this clearly holds for 𝑡1
1. Assume this holds for 𝑡1
𝑗. Then
for𝑡1
𝑗+1, we shall have ℎ1,0
𝑗=𝑡0
𝑡1
𝑗=𝑡1
𝑗and thus
𝑡1
𝑗+1=𝑡1
𝑗+exp
𝑡1
𝑗
≥exp
𝑡1
𝑗
≥exp(𝑓(0,𝑗))=𝑓(0,𝑗+1).
We then consider the induction. Assume that we already have
that at Level l:𝑡𝑙
𝑗≥𝑓(𝑙−1,𝑗), then we would like to prove that
atLevel i+1:𝑡𝑙+1
𝑗≥𝑓(𝑙,𝑗).
It is again easy to show that this holds for 𝑡𝑙+1
1. Assume this holds
for𝑡𝑙+1
𝑗. Then for𝑡𝑙+1
𝑗+1, we shall have ℎ𝑙+1,0
𝑗=ℎ𝑙,0
𝑡𝑙+1
𝑗and thus
𝑡𝑙+1
𝑗+1=𝑡𝑙+1
𝑗+exp
ℎ𝑙+1,0
𝑗
≥𝑡𝑙+1
𝑗+exp
ℎ𝑙,0
𝑡𝑙+1
𝑗
≥1
2𝑡𝑙
𝑡𝑙+1
𝑗+1
≥1
2𝑓
𝑙−1,𝑡𝑙+1
𝑗+1
≥𝑓
𝑙−1,𝑡𝑙+1
𝑗
≥𝑓(𝑙−1,𝑓(𝑙,𝑗))=𝑓(𝑙,𝑗+1).
□
With Lemma B.2, we prove by induction and get the final lemma.
Lemma B.3 (Lemma 4.5 Restatement). For the grids constructed
in Section 4.3, we have
𝑑𝑙≤log𝑙timesz}|{
∗···∗(𝑁)+1,∀𝑙∈[1,𝑟−1].
Proof. Note that we have 𝑑0=𝑘and𝑑𝑙+1=minn
𝑗|𝑡𝑙+1
𝑗>𝑑𝑙o
.
AtLevel 1:𝑑1≤log∗(𝑁)+1.We have that 𝑁=𝑑0≥𝑡1
𝑑1−1≥
𝑓(0,𝑑1−1),𝑑1≤log∗(𝑁)+1.
We then consider the induction. Assume that we already have
that at Level𝑙:𝑑𝑙≤log𝑙timesz}|{
∗···∗(𝑁)+1. We would like to prove that
atLevel𝑙+1:𝑑𝑙+1≤log𝑙+1timesz}|{
∗···∗(𝑁)+1.
We have that
log𝑙timesz}|{
∗···∗(𝑁)+1≥𝑑𝑙≥𝑡𝑙+1
𝑑𝑙+1−1≥𝑓(𝑙,𝑑𝑙+1−1),
𝑑𝑙+1≤log𝑙+1timesz}|{
∗···∗©­­­
«log𝑙timesz}|{
∗···∗(𝑁)+1ª®®®
¬+1≤log𝑙+1timesz}|{
∗···∗(𝑁)+1.
□
We then prove the property of the grids.
Lemma B.4 (Lemma 4.6 Restatement). For the grids constructed
in Section 4.3, any 0≤𝑙≤𝛼(𝑘),𝛼in the layer𝑙,𝛽in the layer𝑙+1
and𝛼belongs to grid 𝛽,we have
𝛼−ℎ𝑙+1,𝑙
𝛽≤ℎ𝑙,0
𝛼−ℎ𝑙+1,0
𝛽.
Proof. By definition, we have that 𝛼=ℎ𝑙,𝑙
𝛼. Therefore, it is
sufficient for us to prove
ℎ𝑙,𝑗
𝛼−ℎ𝑙+1,𝑗
𝛽≤ℎ𝑙,𝑗−1
𝛼−ℎ𝑙+1,𝑗−1
𝛽,∀1≤𝑗≤𝑙.
4674KDD ’24, August 25–29, 2024, Barcelona, Spain. Xiaoyi Zhu, Yuxiang Tian, & Zengfeng Huang.
Ifℎ𝑙,𝑗
𝛼−ℎ𝑙+1,𝑗
𝛽=0, the bound holds trivially. Therefore, we consider
the case when ℎ𝑙,𝑗
𝛼−ℎ𝑙+1,𝑗
𝛽≥1. Since𝛼belongs to grid 𝛽, we must
have that
ℎ𝑙,𝑗
𝛼−ℎ𝑙+1,𝑗
𝛽≤exp 
ℎ𝑗,0
ℎ𝑙+1,𝑗
𝛽!
=exp
ℎ𝑙+1,0
𝛽
.
On the other hand, we have that
ℎ𝑙,𝑗−1
𝛼−ℎ𝑙+1,𝑗−1
𝛽=𝑡𝑗−1
ℎ𝑙,𝑗−1
𝛼−𝑡𝑗−1
ℎ𝑙+1,𝑗−1
𝛽≥𝑡𝑗−1
ℎ𝑙+1,𝑗−1
𝛽+1−𝑡𝑗−1
ℎ𝑙+1,𝑗−1
𝛽
≥exp 
ℎ𝑗−1,0
ℎ𝑙+1,𝑗−1
𝛽!
=exp
ℎ𝑙+1,0
𝛽
.
□
C Proof in Section 5
Lemma C.1 (Lemma 5.1 Restatement). For the sampling proce-
dure shown in algorithm 5 and 6, if the sum 𝑛=Í𝑘
𝑖=1𝑛𝑖≤12𝑁𝑝,
with probability at least 1−2𝛿, the procedure will output 0 when
𝑛≤𝑁and uses less than 24𝐶𝐶bits for communication. Moreover,
•Forlog
2
𝛿
<𝐶𝐶<𝑝·log
2
𝛿
, with probability at least 1−𝛿,
the procedure will output 1 when 𝑛>12𝑁𝑝·log(2
𝛿)
𝐶𝐶.
•For𝐶𝐶≥𝑝·log
2
𝛿
, with probability at least 1−𝛿, the
procedure will output 1 when 𝑛>12𝑁.
We prove the two parts of the Lemma 5.1 separately.
Proof of Lemma 5.1(first part). Forlog
2
𝛿
<𝐶𝐶<𝑝·log
2
𝛿
,
we set the sampling probability being 𝑞=𝐶𝐶
𝑁𝑝and the threshold
𝑇=𝐶𝐶
𝑝+3 log
2
𝛿
.
Communication Cost. Since the actual sum is 𝑛≤12𝑁𝑝, we
shall have that the sampled number ˜𝑛∼Binomial
𝑛,𝐶𝐶
𝑁𝑝
. Thus,
the expectation of ˜𝑛is less than 12𝐶𝐶. Applying the Chernoff Bound
gives us
Pr[˜𝑛≥24𝐶𝐶]<Pr[|˜𝑛−𝐸[˜𝑛]|≥12𝐶𝐶]<2 exp(−4𝐶𝐶)
<2 exp
−4 log2
𝛿
=2𝛿
24
<𝛿.
This shows that our sampled value will not exceed 24𝐶𝐶with
high probability, which controls our communication cost.
When𝑛≤𝑁.Since the sum is less than or equal to 𝑁, we shall
have expectation of ˜𝑛is less than𝐶𝐶
𝑝. By Chernoff bound,
Pr[˜𝑛≥𝑇]<Pr
|˜𝑛−𝐸[˜𝑛]|≥3 log2
𝛿
<2 exp©­­
«−©­­
«3𝑝log
2
𝛿
𝐶𝐶ª®®
¬2
𝐶𝐶
3𝑝ª®®
¬.
For𝐶𝐶<𝑝·log
2
𝛿
, we have that
Pr[˜𝑛≥𝑇𝑖]<2 exp©­­
«−©­­
«9𝑝log
2
𝛿
𝐶𝐶ª®®
¬𝐶𝐶
𝑝ª®®
¬<𝛿.This shows that with probability at least 1−𝛿, we will not screen
out the sum which is less than 𝑁.
When𝑛>3𝑇·𝑁𝑝
𝐶𝐶.We now have the expectation of ˜𝑛is larger
than 3𝑇. By Chernoff Bound,
Pr[˜𝑛≤𝑇]<Pr[|˜𝑛−𝐸[˜𝑛]|≥2𝑇]<2 exp 
−2
32
𝑇!
<2 exp
−4
3·log2
𝛿
<𝛿,
This shows that with probability at least 1−𝛿, cases where the
sum is larger than 3𝑇·𝑁𝑝
𝐶𝐶will be filtered out. Note that
3𝑇·𝑁𝑝
𝐶𝐶=3𝑁+9𝑁𝑝·log
2
𝛿
𝐶𝐶<12𝑁𝑝·log
2
𝛿
𝐶𝐶.
□
For the second part, we have
Proof of Lemma 5.1(second part). The proof is the same ex-
cept that now we have the sampling probability 𝑞=log(2
𝛿)
𝑁, and
𝑇=4 log
2
𝛿
.
Communication Cost. Since the actual sum is 𝑛≤12𝑁𝑝, we
shall have that the sampled number ˜𝑛∼Binomial
𝑛,log(2
𝛿)
𝑁
. Thus,
the expectation of ˜𝑛is less than 12𝑝·log
2
𝛿
≤12𝐶𝐶. Applying
the Chernoff Bound gives us
Pr[˜𝑛≥24𝐶𝐶]<Pr[|˜𝑛−𝐸[˜𝑛]|≥12𝐶𝐶]<2 exp(−4𝐶𝐶)
<2 exp
−12 log2
𝛿
=2𝛿
212
<𝛿.
This shows that our sampled value will not exceed 24𝐶𝐶with
high probability, which controls our communication cost.
When𝑛≤𝑁.Since the sum is less than or equal to 𝑁, we shall
have expectation of ˜𝑛is less than log
2
𝛿
. By Chernoff bound,
Pr[˜𝑛≥𝑇]<Pr
|˜𝑛−𝐸[˜𝑛]|≥3 log2
𝛿
<2 exp©­­
«−32log
2
𝛿
3ª®®
¬<𝛿.
This shows that with probability at least 1−𝛿, we will not screen
out the sum which is less than or equal to 𝑁.
When𝑛>3𝑇·𝑁
log(2
𝛿)=12𝑁.We now have the expectation of
˜𝑛is larger than 3𝑇. By Chernoff Bound,
Pr[˜𝑛≤𝑇]<Pr[|˜𝑛−𝐸[˜𝑛]|≥2𝑇]<2 exp 
−2
32
𝑇!
<2 exp
−4
3·log2
𝛿
<𝛿,
This shows that with probability at least 1−𝛿, cases where the
sum is larger than 12𝑁will be filtered out.
□
4675