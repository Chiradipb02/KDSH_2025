XRL-Bench: A Benchmark for Evaluating and Comparing
Explainable Reinforcement Learning Techniques
Yu Xiong
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
xiongyu1@corp.netease.comZhipeng Hu
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
zphu@corp.netease.comYe Huang
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
huangye3@corp.netease.com
Runze Wu
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
wurunze1@corp.netease.comKai Guan
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
guankai1@corp.netease.comXingChen Fang
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
fangxingchen@corp.netease.com
Ji Jiang
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
jiangji@corp.netease.comTianze Zhou
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
zhoutianze@corp.netease.comYuJing Hu
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
huyujing@corp.netease.com
Haoyu Liu
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
liuhaoyu03@corp.netease.comTangjie Lyu
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
hzlvtangjie@corp.netease.comChangjie Fan
Fuxi AI Lab, NetEase Inc.
Hangzhou, China
fanchangjie@corp.netease.com
Abstract
ReinforcementLearning(RL)hasdemonstratedsubstantialpoten-
tial across diverse fields, yet understanding its decision-making
process, especially in real-world scenarios where rationality and
safetyareparamount,isanongoingchallenge.Thispaperdelvesin
toExplainableRL(XRL),asubfieldofExplainableAI(XAI)aimed
at unravelling the complexities of RL models. Our focus rests on
state-explaining techniques, a crucial subset within XRL methods,
astheyrevealtheunderlyingfactorsinfluencinganagent’sactionsatanygiventime.Despitetheirsignificantrole,thelackofaunified
evaluation framework hinders assessment of their accuracy and
effectiveness. To address this, we introduce XRL-Bench1, a unified
standardizedbenchmarktailoredfortheevaluationandcomparison
ofXRLmethods,encompassingthreemainmodules:standardRL
environments, explainers based on state importance, and standard
evaluators.XRL-Benchsupportsbothtabularandimagedataforstate explanation. We also propose TabularSHAP, an innovative
and competitive XRL method. We demonstrate the practical utility
of TabularSHAP in real-world online gaming services and offer
1https://github.com/fuxiAIlab/xrl-bench
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671595an open-source benchmark platform for the straightforward im-
plementationandevaluationofXRLmethods.Ourcontributions
facilitate the continued progression of XRL technology.
CCS Concepts
•Informationsystems →Onlineanalytical processing; Data
mining.
Keywords
Benchmark, Explainable RL, Explainable AI, Reinforcement Learn-
ing, Evaluation Metric, TabularSHAP
ACM Reference Format:
Yu Xiong, Zhipeng Hu, Ye Huang, Runze Wu, Kai Guan, XingChen Fang, Ji
Jiang, Tianze Zhou, YuJing Hu, Haoyu Liu, Tangjie Lyu, and Changjie Fan.
2024.XRL-Bench:ABenchmarkforEvaluatingandComparingExplainable
ReinforcementLearningTechniques.In Proceedingsofthe30thACMSIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3637528.3671595
1 Introduction
ReinforcementLearning(RL)isapopularmachinelearningpara-
digmwhere anagentlearnsto maximizetheaccumulatedreward
throughinteractionwiththeenvironment,andhasdemonstrated
immensepotentialacrossvariousdomains,includinggameAI[ 54],
robotics[ 55],andindustrialcontrolsystems[ 36].Despitetheno-
table advances in RL, understanding the decision-making process,
a critical aspect contributing to its credibility, reliability, and trans-
parency,remainsasignificantchallenge,especiallyinreal-world
deploymentofRL,whererationalityandsafetyarestronglydesired.
6073
KDD ’24, August 25–29, 2024, Barcelona, Spain Yu Xiong et al.
Explainable AI (XAI) [ 50], a research field aimed at making the
predictiveanddecision-makingprocessofmachinelearningmod-
els transparent and understandable, has been a growing interest
in recently years. Explainable RL (XRL) [ 51], as a subset of XAI,
sharesthesamegoalininterpretingRLmodels.However,dueto
the complexcharacteristics ofRL, suchas its interactivityand dy-
namism with the environment, the interpretability of RL is even
more challenging.
AsignificantcategorywithintherealmofXRLmethodsisthe
state-explaining techniques. These methods offer intuitive explana-
tionsbymeasuringtheimportanceofstatefeaturestoanagent’s
decisions. Extensive research [ 14,18,41] has been conducted in
thisparticularcategory,expandingtheliteratureofXAIandpaving
the way for the application of XAI methodologies in XRL contexts.
However,despitethecrucialrolethesemethodsplayinunderstand-
ing and explaining decisions made by RL model, challenges similar
tothose foundin theXAIfield persist.Specifically,assessing the
accuracy and effectivenessof these methods remains asignificant
challengeduetotheabsenceofaunifiedstandardevaluationframe-
work. This gap hinders the steady and continuous advancement of
XRL technologies.
Toaddressthischallenge,weproposeanddevelopXRL-Bench,
abenchmarkfor XRLmethods.XRL-Benchconsistsof threemain
modules:1)theRLenvironments,utilizingtheopen-sourcegame
environments;2)explainersbasedonstateimportance,including
representative XRL methods and well-known feature attributionmethod from XAI field; and 3) evaluators that mainly include fi-
delity and stability measures. Currently, XRL-Bench supports both
tabular and image data input for state explanation. This work aims
toprovideasolidfoundationandvaluableresourcefortheongoing
developmentandevaluationofXRLmethods.Themaincontribu-
tions of this paper are summarized as follows:
•Weproposeastandardandunifiedframeworkforevaluating
andcomparingXRLmethods forthefirsttime.Thisframe-
work is instrumental in understanding the decision-making
process of RL models, thereby enhancing their credibility,
reliability and transparency.
•We introduce a novel XRL method, TabularSHAP, which
has demonstrated competitive performance against other
XRLmethods.Itspracticalapplicationinreal-worldservices
showcased its significant practical value.
•We provide an open-source benchmark platform that allows
researchers to easily implement and evaluate representative
XRL methods through simple APIs. This platform enables
the extension of the proposed framework in terms of XRL
methods, environements, and evaluation metrics.
2 RELATED WORK
2.1 Explainable RL
ExplainableRLattemptstoaddresstheinterpretableissuebyenrich-
ingRLmodelswiththeabilitytoprovidehuman-understandable
explanationsfortheiractions.Fourprimarycategoriesexistwithin
XRLapproaches,eachcenteredontheexplainabilityofdistinctcom-
ponentsoftheRLframework:model-explaining,reward-explaining,
state-explaining and task-explaining methods.Model-explainingmethodsfocusonextractingtheinternallogic
to create explanations. Depending on the explanation logic type,
there are two divisions within these methods: self-explainable and
explanation-generating parts. Self-explainable models are designed
to be self-explanatory during training, primarily by limiting the
model’s complexity [ 8,40]. Such models can include decision trees
[5,26], logic rules [ 20,38], or formula expressions [ 16]. In con-
trast, explanation-generating models utilize an auxiliary explicit
explanationlogictogenerateexplanations.Examplesincludecoun-
terfactual [31, 37] and instruction-based explanations [11].
Reward-explainingmethodsinterprettheRLagents’decision-
making process by tracking the weight of considered factors in
therewardfunctionanddeterminingreasonablerewardfunction
weights. These methods can be split into two types: reward decom-
position and reward shaping. Reward decomposition [ 10,22,52]
disassembles the reward function to analyze the influence of its
components on the decision-making process and their interrela-
tionships.Reward shaping[ 21,30,33],on theotherhand, aimsto
identify an understandable reward function directly.
State-explaining methods provide explanations based on the
environment’sstatesandtheirinfluenceontheagent’sbehavior.
Thesemethodsarecriticalforunderstandinganagent’sdecision-
making process, as they shed light on the factors affecting the
agent’s actions at any given moment. Notable contributions in this
category include post-hoc explanation methods like saliency maps
[14,41], LIME [ 42], SHAP [ 29], and LRP[ 3], as well as attention-
based methods [25, 49].
Task-explaining methods achievemulti-stage explainability by
decomposing the task. The main concept behind Hierarchical Rein-
forcementLearning(HRL)[ 4]istocreateahigh-levelcontrollerthat
selects macro-actions and several low-level controllers that choose
primitive actions. This division of labor in HRL provides higher
architecturalexplainabilitybyillustratinghowahigh-levelagent
scheduleslow-leveltasks.TheHRLworkcanbecategorizedinto
whole top-down structure with multi-level task division [ 35,46]
andsimpletaskdecompositionwithtwo-leveltaskdivision[ 19,47].
Insummary,XRLoffersabroadspectrumofexplanationmeth-
ods, each with unique strengths and applications. However, the
absenceofaunifiedevaluationframeworkremainsasignificantand
pressingissueforcomparingtheseXRLmethodsandgenerating
high-quality explanations [ 32]. To our knowledge, XRL-Benchis
the only one that provides standardized environments and evalua-
tionmetrics thatcanbe usedtoassess andcomparethe qualityof
explanations generated by various XRL methods. The benchmarks
and competitions established by our framework hold the potential
to propel the advancement of future XRL research.
2.2 Evaluation Metrics for Explanations
The significance of evaluation metrics in the design process of
XRL systems is widely recognized. Researchers from various dis-
ciplinesconcentrateondistinctobjectivesofXAIresearch,which
presents difficulties in determining a suitable evaluation methodol-
ogy. Evaluations of XRL can be broadly categorized into subjective
and objective types, depending on the data source for evaluation.
Subjective evaluation assesses explainable frameworks from the
human perspective. Key metrics for subjective evaluation can be
6074XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques KDD ’24, August 25–29, 2024, Barcelona, Spain
classified into mental model and user-centric properties. The men-
tal model[51, 53] pertainsto anindividual’s understanding ofthe
modelprocess,withtheexplanationfacilitatingusersinbuilding
this mental model. One quantitative approach involves allowing
testerstopredicttheagent’sdecision,andcomputingthehitrate
[42,43].User-centricpropertiessuchasusersatisfaction,trust,and
reliance are also considered [ 12,24], reflecting the explanation’s
complexity, transparency, and utility. However, an excessive re-
lianceonhumanevaluationmayshiftthefocustowardsthepersua-
siveness of the explanation, neglecting other more abstract aspects
suchassystemtransparency,ashumanstendtofavorsimpleand
effective explanations [17].
Objective evaluation, in contrast, is independent of human as-
sessment.Itconcentratesondirectlymeasuringthepropertiesof
theexplainableframework,andcanbedividedintofidelity,stability,
and fairness. Fidelity pertains to the explanation’s correlation with
the actual rationale for agent decision-making [ 1,27,56], while
stability ensures that the explanations remain largely consistent
despiteminorperturbationstotheinput[ 2,13,39].Fairnessensures
the absence of group-based disparities in the fidelity or stability of
explanations [7].
The effectiveness of subjective evaluation hinges on the com-
petency of the testers, leading to potential variations in the con-
clusionsdrawnbytesterswithdifferentskilllevels.Additionally,
subjectiveevaluationmightoverlooktheintrinsiccharacteristicsof
the XRL systems and incurs substantial additional time and human
resourcecosts.Hence,ourresearchemphasizesobjectiveevaluation.
WehavedevelopedXRL-Bench,aframeworkthatfocusesontwo
primary types of objective evaluation: fidelity and stability. Within
this framework, we have devised and implemented five persuasive
metrics to facilitate the comprehensive evaluation and comparison
of various XRL methods.
3 Overview of XRL-Bench Framework
The proposed XRL benchmark framework, XRL-Bench, principally
comprisesthreecoremodules:theRLenvironmentsfortrainingpol-
icymodelsandgeneratinginteractiondatasets,theRLexplainers,
andtheexplanationevaluators.ThishasestabilishedastandardizedandunifiedframeworkforevaluatingXRLmethods,asillustratedin
Figure 1. XRL-Bench also provides a comprehensive programmatic
platform that facilitates researchers and practitioners in the devel-
opment, testing, and comparative analysis of their state-explaining
methods.
3.1 Environments, Policy Models and Datasets
The currently released version of XRL-Bench framework incorpo-
rates four tabular input environments and two image input en-
vironments. This includes a commercial online basketball game
environment,DunkCityDynasty2,developedbyNetEaseGames,
aswellasfivegymgameenvironments3,allofwhicharepublicly
accessible.Eachenvironmentfeaturesanagenttrainedusingthe
Deep Reinforcement Learning (DRL) algorithm to accomplish a
2https://www.dunkcitymobile.com/
3https://gymnasium.farama.org/Table 1: Description of Pre-generated Datasets. S-A Pair Size
means the size of the dataset.
Datasets State Size Action Size S-A Pair Size
Dunk City Dynasty 520 52 18,889
Lunar Lander 8 4 219,392
Cart Pole 4 2 125,228
Flappy Bird 12 2 129,248
Break Out (3,84,84)4 3,776
Pong (3,84,84)6 4,000
proficientstrategy.Thecorrespondingmodelsandsubstantialin-
teractiondatasetsareretainedtoguaranteereproducibilityofall
implementations within our XRL-Bench framework.
Environments. Within the tabular state form, we offer four
game environments: Dunk City Dynasty, Lunar Lander, Cart Pole,
and Flappy Bird. The state space of Dunk City Dynasty is a one-dimensional vector of length 520, comprised of the states of six
playersonthecourtandtheglobalstate.LunarLander’sstatespace
dimension is 8, with four potential actions. Cart Pole has a state
spacedimensionalityoffour,withtwoviableactions.FlappyBird’sstatespacesizeis12,withtwopossibleactions.Fortheimagestate
form,whereeachstateisdepictedbyagamescreenshot,weprovide
two game environments: Break Out and Pong. Break Out’s state
spacesizeis210 ∗160∗3,withfourpossibleactions,andPong’sstate
spacesizematchesthis,withsixpotentialactions.Ourendeavors
concentrateonthesesixstandardpublicgameenvironments,where
we train policy models, accumulate interaction data, explain the
outcomes, and ultimately evaluate the explanations.
Policy Models. For the Dunk City Dynasty environment, we
utilize a PPO algorithm [ 44] with a neural network comprising
four fully connected layers with ReLU activation functions for pol-
icy learning. Categorical states, such as player ID, are processed
throughanembeddinglayer.Forothertabularinputenvironments,
we adopt a DQN algorithm [ 34] with a neural networkconsisting
of three fully connected layers with ReLU activation functions for
policy learning. Training is concluded once the agent reaches a
competitive level, such as when Lunar Lander’s most recent 100
episodesaveragescoresurpasses 220points.Thesepolicymodels
are preserved for generating subsequent agent-environment inter-
actiondata.Fortheimagestates,policylearningalsoutilizesthe
DQNalgorithm,employinganeuralnetworkthatincludesthree
two-dimensional convolutional layers and two fully connected lay-
ers, all with ReLU activation functions.
Datasets. We generate data via interaction with the environ-
ments using policy models and retain it in datasets. For the tab-ular states, two formats are available:
𝐶𝑆𝑉and the𝑀𝐷𝑃𝐷𝑎𝑡𝑎𝑠𝑒𝑡
fromthe𝑑3𝑟𝑙𝑝𝑦library.The 𝐶𝑆𝑉formataffordssuperiordatavi-
sualizationcapabilities,whilethe 𝑀𝐷𝑃𝐷𝑎𝑡𝑎𝑠𝑒𝑡 format,specifically
designed forreinforcement learning,presents amore streamlined
structure. For the image states, data is offered in the 𝑀𝐷𝑃𝐷𝑎𝑡𝑎𝑠𝑒𝑡
form. Table 1 provides detailed descriptions about each environ-
ment’s dataset. It is noteworthy that the state size of the dataset in
the image state form is post data preprocessing.
6075KDD ’24, August 25–29, 2024, Barcelona, Spain Yu Xiong et al.
Environment
State-Action Dataset Policy Model ExplainersExplanations
 Evaluators
Fidelity Stability
Evaluation and 
ComparisonInteract
Explain Evaluate
Figure 1: The XRL-Bench framework.
The XRL-Bench framework provides an abstract 𝐸𝑛𝑣𝑖𝑟𝑜𝑛𝑚𝑒𝑛𝑡
class, encapsulating diverse environments while offering a uni-
fied and succinct interface. Researchers can conveniently load pre-
trained policy models or train their own models based on the se-
lectedenvironment. Theycan alsoload pre-generateddatasets or
create their own. The following code snippet illustrates how to
import the Environment class and utilize it to load a dataset corre-
sponding to the environment:
Listing 1: environment.py
from xrlbench . environments import Environment
environment = Environment(en vironment_name= "
lunarLander" )
dataset = environment . get_dataset ( generate= False)
3.2 Explainers
XRL-Bench offers implementations of seven cutting-edge explain-
ablemethodsfortabularstateform.TheseincludeTabularSHAP,
TabularLIME [ 42], Perturbation Saliency (PS) [ 14], SARFA [ 41],
DeepSHAP[ 29],GradientSHAP[ 9],andIntegratedGradient(IG)
[48]. Each method will be introduce briefly, with a special focus on
the proposed TabularSHAP method.
TabularLIME, a model-agnostic XAI algorithm, is renowned for
generatinglocalexplanationsbyutilizingtabulardata.PSformu-
lates saliency maps by applying Gaussian blur perturbations to
the state-describing inputs and subsequently gauging the policy
changespostinformationremoval.SARFA,anothersaliencymap
generatingmethod,differsfromPSthatarenotspecifictotheaction
of interest. Instead, SARFA focuses on specificity and relevance.Specificity measures the impact of perturbations on the Q-valueof the action being explained, while relevance downweights fea-
tures that alter the expected rewards of actions other than the onebeing explained. DeepSHAP, a SHAP-based adaptation of the orig-
inal DeepLIFT algorithm [ 45], is recognized as the fastest neural
network explainability approach as it can decompose the output
predictionofaneuralnetworkonaspecificinputbybackpropagat-ing.IGinterpolatesbetweenabaselineinputandtheinstancetobe
explained,calculatinggradientsateachpointandintegratingthesegradients along the path, which provides a detailed decomposition
of the network’s predictions. GradientSHAP combines concepts
fromIGandSHAPtoassignimportancevaluestoinputfeatures,
providing a comprehensive understanding of feature contributions.
Fortheinterpretationofimagestates,XRL-Benchprovidesim-
plementationsoffivestate-of-the-artexplainablemethods,which
are also applicable to tabular states. These include PS, SARFA,
DeepSHAP, GradientSHAP, and IG. Each of these methods havedemonstrated significant capabilities in interpreting neural net-
works.
TabularSHAP. WeproposeaneffectiveandefficientXRLmethod
TabularSHAPforexplainingtabularstates.TabularSHAPaddresses
thechallengeofdirectlyinterpretingdeepneuralnetworks.Initially,itcollectsinteractiondatafromDRLmodelsandemploysensemble
tree models (e.g., LightGBM [ 23]) to learn state-action mapping
relationships. As states in tabular form have distinct meanings
withoutstrongmulti-scaletemporalorspatialstructures,ensemble
tree models are often successful as student models in mimicking
the strategies of teacher models. Subsequently, TreeSHAP [ 28], an
XAI method for tree-based models, is employed to interpret the
ensemble tree model. TreeSHAP enables the tractable computation
ofoptimallocalexplanations,knownasSHAPvalues,asdefinedbydesirablepropertiesfromtheclassicgame-theoreticShapleyvalues.
State importance is defined as the change in the expected value of
the policy model’s output when a state is observed versus unob-
served.TheShapleyvalues 𝜙𝑖(𝑓,𝑥),explainingaprediction 𝑓(𝑥),
areanallocationofcreditamongthevariousstatesin 𝑥.Givena
specific prediction 𝑓(𝑥), we can compute the Shapley values by
usingaweightedsumthatrepresentstheimpactofeachstatebeing
added to the model averaged over all possible orders of states:
𝜙𝑖(𝑓,𝑥)=/summationdisplay.1
𝑆⊆𝑆𝑎𝑙𝑙/{𝑖}|𝑆|!(𝑁−|𝑆|−1)!
𝑁![𝑓𝑥(𝑆∪𝑖)−𝑓𝑥(𝑆)]
=/summationdisplay.1
𝑆⊆𝑆𝑎𝑙𝑙/{𝑖}1
(𝑁𝑐ℎ𝑜𝑜𝑠𝑒|𝑆|)(𝑁−|𝑆|)[𝑓𝑥(𝑆∪𝑖)−𝑓𝑥(𝑆)](1)
where𝑆representsthesubsetofall 𝑁inputstatesand 𝑥𝑠denotesa
subset of the input vector containing only the states within the set
𝑆.ByusingTreeSHAPtocalculatetheinfluenceofstatesonactions,
6076XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques KDD ’24, August 25–29, 2024, Barcelona, Spain
globalanalysisandepisodeanalysisforRLagentscanbeformed.
This method has been practically applied in DRL-based businesses
to provide a clearer understanding of the decision-making process
of black-box RL models, and facilitate the quick identification and
resolution of issues like unexpected actions.
XRL-BenchoffersanExplainerabstractclasstostreamlinethe
utilization of various XRL methods. Once researchers have pre-
pared the necessary data or the policy model, they can instantiate
the relevant explainer by inputting the desired XRL method name.
The process includes a simple preprocessing of the dataset to ex-
tract state and action data, the instantiation of the TabularSHAP
explainer, and ultimately, the generation of explanations for the
statedataofinterestusingtheinstantiatedexplainer.Thefollowing
code snippet illustrates this process:
Listing 2: explainer.py
from xrlbench. explainers import Explainer
actions = dataset[ 'action ' ]
states = dataset .drop([ 'action ' ,'reward ' ] , axis=1)
explainer = Explainer(method= "TabularSHAP" , state=
states , action=actions)
explanations = explainer . explain(state=st ates)
3.3 Evaluation Metrics
XRL-Bench incorporates five key quantitative evaluation measures
designed to assess the fidelity and stability of XRL methods. For
fidelityassessment,thefollowingmetricsareprovided:AccuracyonImportantfeaturesMaskedbyreferencepadding(AIM),Accu-
racyonUnimportantfeaturesMaskedbyreferencepadding(AUM),PredictionGaponImportantfeatureperturbation(PGI),andPredic-
tion Gap on Unimportant feature perturbation (PGU). Meanwhile,
RelativeInputStability(RIS)servesasthemetricforstabilityassess-
ment. These metrics collectively offer a comprehensive evaluation
framework for XRL methods.
Fidelity. Thefidelityofanexplainerreferstothedegreeofalign-
mentbetweenthegeneratedexplanationandtheactualunderlying
rationale of an agent’s decision. High fidelity of an XRL method
impliesthat theproducedexplanations accuratelymirrorthe true
logic driving the agent’s decision-making process. We initially use
post-hocaccuracy,asintroducedinL2X[ 6],tomeasurefidelity.This
methodmasksunimportantwords,asindicatedbytheexplainer,
viazero-padding,andthenfeedstheseintothemodelforinference.
Thedegreeofalignmentbetweenthemodel’sinferredcategories
before andafter masking iscompared and usedto determine the
explainer’s post-hoc accuracy.
Inspired bythis intuitiveand applicable XAIevaluation metric,
we devised two fidelity measures, namely AIM and AUM. AIM
masksimportantstateswithreferencepaddingvalues(e.g.,zero-
padding)andobservesthedegreeofalignmentbetweenthemodel’s
outputs before and after state masking. Conversely, AUM masks
unimportantstateswithreferencepaddingvaluesandobservesthe
degreeofdisparitybetweenthemodel’soutputsbeforeandafter
statemasking.Thisisconsideredasthepost-hocaccuracyofthe
XRL method.
We also employ PGI and PGU fidelity metrics, as detailed in
[7,39]. These two metrics compute the difference in predictionvaluebyperturbingimportantandunimportantstates,respectively.
For PGI, a higher metric value indicates a higher fidelity of theexplanation. A perturbed instance,
𝑥/prime, in the local neighborhood
of x is generated by slightly perturbing the values of the top- 𝑘
importantstatesbyaddingasmallamountofGaussiannoise,while
keeping all other states constant. Finally, the expected value of the
prediction difference between the original and perturbed instances
is computed as:
𝑃𝐺𝐼(𝑥,𝑓,𝑒𝑥,𝑘)=𝐸𝑥/prime∼𝑝𝑒𝑟𝑡𝑢𝑟𝑏(𝑥,𝑒𝑥,𝑡𝑜𝑝−𝐾)[|𝑓(𝑥)−𝑓(𝑥/prime)|](2)
where𝑓represents the underlying model, and 𝑒𝑥signifies an
explanation for the model’s prediction of 𝑥. Similar computations
are employed for PGU:
𝑃𝐺𝑈(𝑥,𝑓,𝑒𝑥,𝑘)=𝐸𝑥/prime∼𝑝𝑒𝑟𝑡𝑢𝑟𝑏(𝑥,𝑒𝑥,𝑏𝑜𝑡𝑡𝑜𝑚−𝐾)[|𝑓(𝑥)−𝑓(𝑥/prime)|](3)
Stability. We adopt the evaluation metric presented in [ 2]t o
assess the robustness of an explanation to minor input perturba-
tions.TheRelativeInputStability(RIS)isemployedtogaugethe
maximumalterationintheexplanationrelativetotheinput.This
evaluation metric can be formalized as follows:
𝑅𝐼𝑆(𝑥,𝑥/prime,𝑒𝑥,𝑒𝑥/prime)=𝑚𝑎𝑥
𝑥/prime||𝑒𝑥−𝑒𝑥/prime
𝑒𝑥||𝑝
𝑚𝑎𝑥(||𝑥−𝑥/prime
𝑥||𝑝,𝜖𝑚𝑖𝑛),∀𝑥/prime𝑠.𝑡.𝑥/prime∈𝑁𝑥
(4)
where𝑁𝑥denotes a neighborhood of instance 𝑥/primearound𝑥. The
numerator of the metric calculates the 𝑝-norm of the percentage
change of explanation 𝑒𝑥/primeon the perturbed instance 𝑥/primerelative
to the explanation 𝑒𝑥on the original point 𝑥. The denominator
measuresthe 𝑝-normbetweenthenormalizedinputs 𝑥and𝑥/prime.The
maximum term in the denominator safeguards against division by
zero.
XRL-BenchalsooffersanabstractEvaluator classdesignedfor
evaluation metrics. By inputting the name of the metric methodand the environment, an evaluator instance can be instantiated.
Thisinstanceallowsresearcherstoswiftlyevaluatethegenerated
explanations. The code snippet illustrates this streamlined process:
Listing 3: evaluator.py
from xrlbench . evaluator import Evaluator
evaluator = Evaluator( metric= "AIM" , environment=
environment)
accuracy = ev aluator . evaluate(st ates , actions ,
explanations , k=3)
Benchmarking. XRL-Benchoffersaunifiedplatformthatac-
commodates a broad spectrum of environments, explanation meth-
ods, and evaluation metrics. It aids researchers and practitioners
in the development, testing, and benchmarking of XRL algorithms.
Userscanaccomplishtheentireprocess,fromenvironmentloading,data generation, explainer instantiation, explanation generation, to
the final explanation evaluation, with just a few lines of code by
constructinginstancesoftheEnvironment,Explainer,andEvalu-
atorabstractclasses.Thissignificantlysimplifiesandstreamlines
the development of cutting-edge XRL methods.
6077KDD ’24, August 25–29, 2024, Barcelona, Spain Yu Xiong et al.
4 Benchmarking Analysis
Following, we utilize the XRL-Bench framework to perform bench-
mark tests on the previously discussed state-of-the-art XRL meth-
ods.Wethenproceedwith acomparativeanalysis anddiscussion
ofthebenchmarkingresults.Allbenchmarktestexperimentsare
reproducible,withthecoderepositoryaccessibleatthefollowing
address: https://github.com/fuxiAIlab/xrl-bench.
4.1 Experimental Setup
We executed benchmark tests on tabular states across four dis-
tinct environments: 𝐷𝑢𝑛𝑘𝐶𝑖𝑡𝑦𝐷𝑦𝑛𝑎𝑠𝑡𝑦 −𝑣1,𝐿𝑢𝑛𝑎𝑟𝐿𝑎𝑛𝑑𝑒𝑟 −𝑣2,
𝐶𝑎𝑟𝑡𝑃𝑜𝑙𝑒 −𝑣0, and𝐹𝑙𝑎𝑝𝑝𝑦𝐵𝑖𝑟𝑑 −𝑣0. Additionally, for the image
states, we assessed XRL methods in two environments, namely
𝐵𝑟𝑒𝑎𝑘𝑜𝑢𝑡 −𝑣0and𝑃𝑜𝑛𝑔−𝑣0.InXRL-Bench,thereferenceguide-
linesforthecodeimplementationofeachXRLmethodareprovided
within the comments of the respective method class code. All XRL
methods adhered to the default hyperparameters from their origi-
nal implementations. For the 𝐷𝑢𝑛𝑘𝐶𝑖𝑡𝑦𝐷𝑦𝑛𝑎𝑠𝑡𝑦 −𝑣1, a four-layer
fully connected network with 832, 256, 128, 52 hidden nodes re-
spectively was utilized for the policy training. For other tabular
input environments, we use a three-layer fully connected network
with64hiddennodes.Fortheimagestates,thepolicynetworkcon-
sistedofathree-layertwo-dimensionalconvolutionallayerwith32,
64, and 64 convolution kernels respectively, followed by two fully
connected layers, with parameters initialized using the Kaiming
normalmethod[ 15].Thesenetworkswereconstructedandtrained
usingthePytorchframework.Fortheexperimentaldatasets,trained
agents interacted with their corresponding environments over a
predetermined number of episodes with a set maximum number ofsteps,resultinginthepre-generateddatasetsasdepictedinTable1.
4.2 Fidelity
WeconductedanevaluationandcomparisonoftheXRLmethods
withinXRL-Bench,utilizingfourfidelityevaluationmetrics:AIM,
AUM,PGI,andPGU.Initially,itiscrucialtocomprehendtheconcept
of state importance value as provided by the XRL methods. The
questionarises:whatdeterminesastate’simportance?Formethods
such as SARFA and PS, which produce only positive values, the
answerisstraightforward-stateswithhigherimportancevalues
are deemed more important. However, for other methods that may
generate negative values, states with high negative impacts cannot
alwaysbedisregardedastheyoftenrepresentcriticalcounterpoints
to the model’s decisions. Consequently, we define the Top-K states
in two ways: 1) The K states with the highest absolute values of
stateimportance;2)TheKstateswiththehighestoriginalvalues
ofstateimportance.Similardefinitionsareappliedtothebottom-K
states.Allevaluationmethodswerecalculatedaccordingtothese
two definitions, and the superior results were used for benchmark
comparison.ThevaluesofthefourfidelityevaluationmetricswerecalculatedbasedontheAreaUndertheCurve(AUC)overallvalues
of K. For AIM and AUM, zero-padding was employed to mask the
most and least important states.
ComparisoninTabularStateForm. Thecomparativeanalysis
of XRL methods in tabular state form is displayed in Tables 2. The
results demonstrate that TabularSHAP surpasses other methods
inAIMandAUMmetrics,registeringthetopresultsin7outof8evaluationsacrossthreedatasets.ThisunderscoresTabularSHAP’s
superior capacity to select important states, suggesting that thebulk of RL model strategies can be replicated by focusing on a
minority of critical states. Notably, in the Flappy Bird dataset, char-
acterized by an imbalance where the ratio of 𝑑𝑜𝑛𝑜𝑡ℎ𝑖𝑛𝑔 to𝑓𝑙𝑎𝑝
actionsapproximates10 : 1,allothermethodswereunabletoac-
curatelyreconstructmodeldecisionsusingtheirimportantstates.
Their AUC on AUM consistently surpassed that on AIM. Only Tab-
ularSHAP was able to successfully identify the important states
underthesechallengingconditions.TabularSHAP’sperformance
onPGIandPGUmetricsisaverage,whichweattributetotheuncer-tainty introduced by perturbations. The perturbation concept lacks
asolidtheoreticalframeworkandoftenproducesresultsthatlack
precisioninpracticalapplication.Theintricatedecisionboundaries
of complex models in high-dimensional data, with perturbationsin varying directions and scales, lead to diverse effects. The sub-
par performance observed in perturbation-based methods such as
SARFA,PS,andTabularLIMEfurtherunderscoresthelimitations
of perturbation-based approaches.
DeepSHAP, GradientSHAP, and IG display similar and com-
petitive performances, all leveraging gradients to calculate state
importance.Thesemethodsachievedthetopresultsin3outof8
PGIandPGUmetricsacrossthreedatasets,indicatingthatgradient-
based methods excel at capturing the variance in the original RL
model’soutputsafterstateperturbations.DespiteSARFAandPS
alsocomputingstateimportancebasedontheperturbationconcept,theydifferfromtheperturbationsinPGIandPGUmetrics.PGIand
PGUfocus solelyontheprediction resultdifferencesin thetarget
action,whileSARFAandPSconcentrateonthepredictionresult
differencesacrossallactions,leadingtotheirlessstablefidelityper-formance. In general, TabularLIME exhibits moderate performance,
but it demonstrates exceptional results in PGI and PGU, achieving
topperformanceinfourevaluations.Thisimpliesthatpromising
resultsarelikelywhenthetargetactionofinterestforperturbation
in the method coincides with that of the metrics.
Comparison in Image State Form. Given the extensive state
space associated with image input, calculating the AUC over allK values is computationally demanding. To facilitate a more ef-ficient evaluation and comparison of XRL methods, we computethe AUC for K mod 10. The comparative results are displayed in
Table 3. Based on the evaluation results of AIM and AUM across
twoenvironments,DeepSHAPandGradientSHAP,especiallythe
former, exhibit superior performance. This underscores the robust
theoretical foundation of SHAP-based methods, with IG coming
next,thusemphasizingtheeffectivenessofgradient-basedmethodsininterpretingneuralnetworks.Asimilarconclusioncanbedrawn
fromthePGIandPGUevaluationresults,namelythatthefidelity
ofgradient-basedexplainablemethodssignificantlyoutperforms
that of perturbation-based explainable methods, further indicating
that the theoretical foundation of the latter requires bolstering.
4.3 Stability
WeevaluatedandcomparedthestabilityofsevenXRLmethodsfor
tabularstatesacrossfourenvironments,asshowninTable4.The
resutls reveal that DeepSHAP yielded the most stable explanations,
followed by IG and GradientSHAP. This demonstrates the efficacy
6078XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: The fidelity evaluation and comparison of seven XRL methods across four tabular input environments.
ExplainerDunk City Dynasty Lunar Lander Cart Pole Flappy Bird
AIM↓AUM↑PGI↑PGU↓AIM↓AUM↑PGI↑PGU↓AIM↓AUM↑PGI↑PGU↓AIM↓AUM↑PGI↑PGU↓
TabularSHAP 0.214 0.894 0.905 0.662 0.116 0.693 5.258 4.895 0.134 0.960 0.452 0.522 0.3310.566 3.019 1.685
DeepSHAP 0.337 0.493 0.790 0.712 0.188 0.663 5.988 4.321 0.377 0.740 0.651 0.324 0.542 0.472 2.970 1.759
GradientSHAP 0.326 0.523 0.766 0.690 0.203 0.614 5.963 4.317 0.268 0.827 0.6550.3120.631 0.517 2.953 1.792
IG 0.323 0.522 0.808 0.688 0.203 0.618 5.930 4.375 0.260 0.837 0.654 0.3110.609 0.511 3.1041.738
SARFA 0.361 0.709 0.952 0.665 0.388 0.363 4.953 5.169 0.602 0.558 0.503 0.569 0.5930.573 2.598 2.671
PS 0.364 0.687 0.951 0.680 0.382 0.353 4.847 5.528 0.556 0.574 0.411 0.569 0.584 0.565 2.568 2.639
TabularLIME 0.215 0.779 0.954 0.274 0.323 0.472 6.179 3.755 0.613 0.564 0.646 0.329 0.559 0.511 3.017 2.038
Table 3: The fidelity evaluation and comparison of five XRL
methods across two image input environments.
ExplainerBreak Out
AIM↓AUM↑PGI↑PGU↓
DeepSHAP 0.1620.630 1.748 0.347
GradientSHAP 0.260 0.655 1.755 0.384
IG 0.292 0.652 1.8120.364
SARFA 0.253 0.270 1.225 0.991
PS 0.258 0.387 1.370 0.621
ExplainerPong
AIM↓AUM↑PGI↑PGU↓
DeepSHAP 0.1110.160 2.271 0.699
GradientSHAP 0.114 0.153 2.4531.164
IG 0.128 0.169 2.213 0.770
SARFA 0.147 0.148 2.375 1.500
PS 0.148 0.15 2.346 1.077
ofgradient-basedmethodsinexplainingneuralnetworks,withgra-
dientintegrationservingtoenhancethesmoothnessoftheprocess.
Despite being an indirect interpretation of the RL model, Tabu-
larSHAPshowscommendablestability.Coupledwithitssuperior
performance in fidelity, this reflects the feasibility of the approach
of indirectly explaining RL models. On the other hand, the stabilityperformanceofperturbation-basedexplainablemethodswasunsat-
isfactory.WefurtherevaluatedthestabilityoffiveXRLmethods
for image states across two environments, as presented in Table 4.
DeepSHAP and IG again achieves the best stability results, indi-
cating thatthese two methodscan maintain considerablestability
acrossdifferentdataformats.Thestabilityperformanceofthere-
maining methods was relatively mediocre in general.
4.4 Computational Efficiency
The generation of explanations inherently requires additional time.
An XRL methods with higher efficiency can produce explanations
more promptly, which often enhances its practical utility. We con-
ductedcomparativeexperimentsonthecomputationalefficiencyof
seven XRL methods on a PC with an Intel Core i9 3.6 GHz 16-core
processor.Figure2presentsthetimetakentogenerateexplanations
for each sample. In the Lunar Lander environment, which has a
smallstatespace,allsevenmethodstooklessthan0.15stocomputeanexplanation.Inthemorecomplexgameenvironment,DunkCity
Dynasty, which features a larger state space, TabularSHAP demon-
strated a clear advantage of efficiency, requiring only 0.003s perTabularSHAPDeepSHAPGradientSHAPIG
SARFAPS
TabularLIME
XRL Methods02468101214Time(s)LunarLander
DunkCityDynasty
BreakOut
Figure 2: The computational efficiency comparison of seven
XRL methods.
sample.Thisisparticularlybeneficialforapplicationsthathandle
large-scaledataandneedtoprovideexplanationspromptly.Con-
versely,GradientSHAPandTabularLIMEwerelessefficient,with
explanation times averaging around 10 seconds per sample. In the
image input environment of Break Out, DeepSHAP outperformed
theotherfourmethods,generatingexplanationsforeachsample
in just 0.234 seconds.
5Real-World Application of XRL: A Case Study
Despitethe proliferationofXRL methodsinrecent years,theiref-
fective deployment in practical environments remains a challenge
due to the inefficiencies, ineffectiveness, or obscurity of the gen-
eratedexplanations.Thissectionpresentsacasestudywherewe
successfully apply XRL methods in a RL AI project focused on on-
line gaming. We aim to assist practitioners in narrowing the divide
betweentheoreticalalgorithmsandtheirpracticalimplementations,
thereby fostering further research interest in the XRL field.
5.1 Role of AI Bots in Online Gaming
OnlinegamingnecessitateavarietyofAIcomponents,including
non-player characters (NPCs), boss monsters, and matchmakingbots. These elements contribute to the realism and diversity ofthegaming world. However, relyingon traditionaldevelopers to
manually encode behavior logic often leads to subpar performance
6079KDD ’24, August 25–29, 2024, Barcelona, Spain Yu Xiong et al.
Table 4: The RIS stability evaluation and comparison of the XRL methods across four tabular input environments and two
image input environments.
Explainer Dunk City Dynasty Lunar Lander Cart Pole Flappy Bird Break Out Pong
TabularSHAP 1.023 2.646 30.734 11.403 \ \
DeepSHAP 1.261 2.623 21.104 7.355 0.375 0.044
GradientSHAP 1.419 3.134 18.520 10.732 0.659 5.490
IG 1.465 2.800 13.713 10.339 0.109 0.758
SARFA 1.071 6.408 54.193 10.419 0.653 2.291
PS 1.357 4.878 54.694 11.940 0.649 3.570
TabularLIME 1.901 2.871 34.063 14.691 \ \
forsimpleAI,highdevelopmentcostsforcomplexAI,andissues
suchaspredictableactions,repetitivepatterns,anddifficultiesin
managing complex scenarios. By applying RL techniques, which
enablealgorithmstocontinuallylearnAIcontrol,theseproblems
can be effectively mitigated.
RL AI bots, developed by NetEase, have been successfully in-
tegratedintovariousgames,provingadvantageousinplayerver-
sus environment (PvE) and player versus player (PvP) scenarios
andmeetingavarietyofplayerneeds.However,theintricateand
opaquestructureofRLmodelscanresultininefficientdebugging
processes during model training. In practical applications, when
an AI’s behavior does not meet human expectations, engineers
typically follow a three-step process of problem assumption, exper-
imental verification, and adjustment improvement. This process
oftenrequiresnumerousiterations,istime-consuming,andineffi-
cientduetotheslowtrainingnatureofRLmodels.Italsoheavily
depends on the engineer’s experience. Consequently, understand-
ingthedecision-makingmechanismoftheRLmodelandrendering
AIbehaviorintelligibletohumansiscrucialforexpeditingproblem
identification and accelerating AI bot development.
5.2 Methodology for XRL Implementation
To expedite the analysis of AI bot behavior patterns in online gam-
ing, and to promptly identify and rectify anomalous actions, we
devised and executed a precise, efficient, and pragmatic XRL strat-
egy. This approach, rooted in the TabularSHAP method detailed in
Section 3.2, is utilized to interpret the decision-making mechanism
of RL models. The strategy unfolds in four distinct phases:
(1)Collection of State-Action Data : We select the RL model
for interpretation, execute combat games repeatedly, andcollect the ensuing state-action data pairs. This creates a
state-actiondatasetofsize N,whichisemployedforsubse-
quent explanation analysis.
(2)EstimationofState-to-ActionInfluence:Weutilizethe
TabularSHAPmethodtocalculatethestatefactorsthatinsti-
gate each action, along with their respective SHAP values.
(3)Global analysis: The computed SHAP values are graph-ically represented using Summary and Dependence Plots,
asdepictedinFigure3andFigure4.Thisallowsengineers
toquicklyunderstandthebroaderpictureofthefactorsin-
fluencingtheAIbot’sbehavior,andfurtherscrutinizeany
outcomes that do not conform to human expectations.
(4)Episodeanalysis:Foranepisodeofanomalousbotbehavior,
wecomputetheSHAPvaluesofthestate-to-actionwithintheepisodedatausingTabularSHAP.Forananomalousac-
tion at a specific timestep, we employ the Waterfall Plot,as seen in Figure 5, to illustrate the key state factors that
influencetheoccurrenceoftheaction.Simultaneously,we
delvedeeperintothesignificantfactorsofanactionthatwas
anticipated but did not occur, identifying the core factorsthat inhibited the activation of the action. Through these
factualandcounterfactual analysis,wecan swiftlyidentify
the anomalous state, providing a strong foundation for trou-
bleshootingandimprovingtheefficiencyofproblem-solving.
5.3 Case Study
The application case4of XRL comes from Naraka:Bladepoint5,a
highlysoughtaftermultiplayeraction-competitivegamebyNetEase
Games. In situations where AI bots struggle to seamlessly connect
the𝐿𝑒𝑓𝑡𝐴𝑡𝑡𝑎𝑐𝑘 tothe𝑆ℎ𝑒𝑛𝑔𝐿𝑜𝑛𝑔 skill- amove consideredexpert
routine. Instead, the AI bots tend to perform 𝐿𝑒𝑓𝑡𝐴𝑡𝑡𝑎𝑐𝑘 twice
beforetransitioningtothe 𝑆ℎ𝑒𝑛𝑔𝐿𝑜𝑛𝑔 skill.Initially,aglobalanaly-
sis isconductedto scrutinizethe principalfactors thattrigger the
activationofthe 𝑆ℎ𝑒𝑛𝑔𝐿𝑜𝑛𝑔 skill.AsdepictedinFigure3andFig-
ure4,thetopfivefactorsaretheavailabilityofthe 𝑆ℎ𝑒𝑛𝑔𝐿𝑜𝑛𝑔 skill,
the player’s state in the combo chart, the player’s high hit point,the availability of the
𝑆ℎ𝑜𝑐𝑘skill, and the opponent’s grounded
state.Theseanalyticalresultscoincidewithhumanintuition,val-
idating the overall normal functioning of the RL model. Subse-
quently, an episode analysis is conducted on a dataset that exhibits
the aforementioned anomaly. We zero in on the segment where
𝐿𝑒𝑓𝑡𝐴𝑡𝑡𝑎𝑐𝑘 isexecutedatthe10thtimestep,followedbyanother
𝐿𝑒𝑓𝑡𝐴𝑡𝑡𝑎𝑐𝑘 atthe11thtimestep,andfinally 𝑆ℎ𝑒𝑛𝑔𝐿𝑜𝑛𝑔 atthe12th
timestep.Thefocusisonunderstandingtherationalebehindexe-
cuting𝐿𝑒𝑓𝑡𝐴𝑡𝑡𝑎𝑐𝑘 instead of𝑆ℎ𝑒𝑛𝑔𝐿𝑜𝑛𝑔 at the 11th timestep. The
analysis, as illustrated in Figure 5, reveals that the primary reason
for not executing 𝑆ℎ𝑒𝑛𝑔𝐿𝑜𝑛𝑔 is its unavailability (legal is 0). Upon
furtherinvestigation,itwasfoundthataprogramminginterface
bugwasresponsibleforthisissue.TheAIbot,afterexecutingan
attack, was not recognized as still being on the ground, leadingto a false legal status for
𝑆ℎ𝑒𝑛𝑔𝐿𝑜𝑛𝑔 . Once this bug was rectified,
theproblemwasresolved,andtheAIbotssuccessfullylearnedto
connect𝐿𝑒𝑓𝑡𝐴𝑡𝑡𝑎𝑐𝑘 to𝑆ℎ𝑒𝑛𝑔𝐿𝑜𝑛𝑔 , mastering the expert routine.
4https://github.com/fuxiAIlab/xrl-bench/tree/main/video
5https://www.narakathegame.com/
6080XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques KDD ’24, August 25–29, 2024, Barcelona, Spain
9.'6\GR[KOSVGIZUTSUJKRU[ZV[Z2KMGRE9NKTM2UTM
9KRLE)USHUE9ZGZK
9KRLE.OZ6UOTZ
2KMGRE9NUIQ
+TKS_E/Y5T-XU[TJ
+TKS_E)USHUE9ZGZK
9KRLE/Y5T-XU[TJ
+TKS_E9NOKRJ
9NKTM2UTM
+TKS_E*OYZ
9KRLE)NGXMK:OSK
9KRLE9NOKRJ
+TKS_E+TKXM_
+TKS_E6UYE?
4UZ)NGXMK(R[K
9KRLE+TKXM_
)XU[IN
9KRLE,UX]GXJE?
9KRLE=KGVUT5TKNUZ
9[SUL5ZNKX,KGZ[XKY
2U].OMN
9ZGZK\GR[K
Figure 3: The Summary plot for XRL global analysis. Sum-
maryplotorganizesstatesbasedonthecumulativemagni-
tudeoftheirSHAPvaluesandusesthesevaluestodepictthe
distribution of each state’s influence.
9KRLE)USHUE9ZGZK9.'6\GR[KLUX
9KRLE)USHUE9ZGZK
Figure4:TheDependenceplotforXRLglobalanalysis.De-
pendence plot uses theSHAP value of a specific state asthe
y-axis, while the corresponding feature value is represented
on the x-axis.
6 Conclusion
ThispaperhasadvancedthefieldofXRLbyproposingXRL-Bench,
abenchmarkingframeworkforXRLmethodologies.Theprimary
challengethatthisworkaddressesisthelackofaunifiedevaluation      EUZNKXELKGZ[XKY#+TKS_E)USHUE9ZGZK#+TKS_E2KLZ6XUMXKYY#9KRLE.OZ6UOTZ#9NKTM2UTM#9KRLE9NOKRJ#+TKS_E/Y5T-XU[TJ#9KRLE)USHUE9ZGZK#2KMGRE9NKTM2UTM
#2KMGRE9NUIQ
Figure5:TheWaterfallplotforXRLepisodeanalysis.Water-
fallplotdemonstrateseachstate’scontributioninpushing
themodeloutputfromitsbasevalue(theaveragemodelout-
put over the dataset) to the final model output. States that
increasethemodelpredictionaredepictedinred,whilethose
that decrease it are in blue.
framework for XRL techniques, which has been a significant obsta-
cletotheconsistentprogressionofXRLtechnologies.Oursolution,
XRL-Bench,encompassesthreemainmodules:RLenvironments,
explainersbasedonstateimportance,andevaluatorswhichprimar-
ily include fidelity and stability measures. It supports both tabular
andimagedatainputforstateexplanation,therebyextendingits
applicability across various RL scenarios. In addition, we intro-
ducedanovelXRLmethod,TabularSHAP,whichhasdemonstrated
competitive performance against other XRL methods. Its practi-
cal application in real-world online gaming services showcased its
practical relevance and received wide recognition. Furthermore,
we have provided an open-source benchmark platform that allows
researchers and practitioners to easily implement and evaluate
representativeXRLmethodsthroughsimpleAPIs.Thisplatform
enables theextension ofthe proposed frameworkin termsof XRL
methods, environments, and evaluation metrics. Overall, this work
contributesasolidfoundationandavaluableresourcefortheongo-
ingdevelopmentandevaluationofXRLmethods,therebypaving
the way for further advancements in this crucial field of study.
References
[1]Chirag Agarwal, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari
Johnson,IshaPuri,MarinkaZitnik,andHimabinduLakkaraju.2022. Openxai:
Towards a transparent evaluation of model explanations. Advances in Neural
Information Processing Systems 35 (2022), 15784–15799.
[2]David Alvarez-Melis and Tommi S Jaakkola. 2018. On the robustness of inter-
pretability methods. arXiv preprint arXiv:1806.08049 (2018).
[3]Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen,
Klaus-Robert Müller, and Wojciech Samek. 2015. On pixel-wise explanations for
non-linear classifier decisions by layer-wise relevance propagation. PloS one10,
7 (2015), e0130140.
[4]Andrew G Barto and Sridhar Mahadevan. 2003. Recent advances in hierarchical
reinforcement learning. Discrete event dynamic systems 13, 1-2 (2003), 41–77.
[5]OsbertBastani,YewenPu,andArmandoSolar-Lezama.2018. Verifiablereinforce-
mentlearningviapolicyextraction. Advancesinneuralinformationprocessing
systems31 (2018).
6081KDD ’24, August 25–29, 2024, Barcelona, Spain Yu Xiong et al.
[6]JianboChen,LeSong,MartinWainwright,andMichaelJordan.2018. Learning
to explain: An information-theoretic perspective on model interpretation. In
International conference on machine learning. PMLR, 883–892.
[7]JessicaDai,SohiniUpadhyay,UlrichAivodji,StephenHBach,andHimabindu
Lakkaraju.2022. Fairnessviaexplanationquality:Evaluatingdisparitiesinthe
qualityofposthocexplanations.In Proceedingsofthe2022AAAI/ACMConference
on AI, Ethics, and Society. 203–214.
[8]Mengnan Du, Ninghao Liu, and Xia Hu. 2019. Techniques for interpretable
machine learning. Commun. ACM 63, 1 (2019), 68–77.
[9]GabrielErion,JosephDJanizek,PascalSturmfels,ScottMLundberg,andSu-In
Lee. 2019. Learning explainable models using attribution priors. (2019).
[10]JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShi-
monWhiteson.2018. Counterfactualmulti-agentpolicygradients.In Proceedings
of the AAAI conference on artificial intelligence, Vol. 32.
[11]YosukeFukuchi,MasahikoOsawa,HiroshiYamakawa,andMichitaImai.2017.
Autonomous self-explanation of behavior for interactive reinforcement learn-
ing agents. In Proceedings of the 5th International Conference on Human Agent
Interaction. 97–101.
[12]Fatih Gedikli, Dietmar Jannach,and Mouzhi Ge. 2014. How should I explain? A
comparison of different explanation types for recommender systems. Interna-
tional Journal of Human-Computer Studies 72, 4 (2014), 367–382.
[13]AmirataGhorbani,AbubakarAbid,andJamesZou.2019. Interpretationofneuralnetworks is fragile. In Proceedings of the AAAI conference on artificial intelligence,
Vol. 33. 3681–3688.
[14]SamuelGreydanus,AnuragKoul,JonathanDodge,andAlanFern.2018. Visu-alizingandunderstandingatariagents.In Internationalconferenceonmachine
learning. PMLR, 1792–1801.
[15]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2015. Delvingdeep
into rectifiers: Surpassing human-level performance on imagenet classification.
InProceedingsoftheIEEEinternationalconferenceoncomputervision.1026–1034.
[16]Daniel Hein, Steffen Udluft, and Thomas A Runkler. 2018. Interpretable policies
for reinforcement learning by genetic programming. Engineering Applications of
Artificial Intelligence 76 (2018), 158–169.
[17]BerneaseHerman.2017. Thepromiseandperilofhumanevaluationformodel
interpretability. arXiv preprint arXiv:1711.07414 (2017).
[18]Rahul Iyer, Yuezhang Li, Huao Li, Michael Lewis, Ramitha Sundar, and KatiaSycara. 2018. Transparency and explanation in deep reinforcement learning
neuralnetworks.In Proceedingsofthe2018AAAI/ACMConferenceonAI,Ethics,
and Society. 144–150.
[19]Yiding Jiang, Shixiang Shane Gu, Kevin P Murphy, and Chelsea Finn. 2019. Lan-
guage as an abstraction for hierarchical deep reinforcement learning. Advances
in Neural Information Processing Systems 32 (2019).
[20]Zhengyao Jiang and Shan Luo. 2019. Neural logic reinforcement learning. In
International conference on machine learning. PMLR, 3110–3119.
[21]Mu Jin, Zhihao Ma, Kebing Jin, Hankz Hankui Zhuo, Chen Chen, and Chao
Yu.2022. Creativityofai:Automaticsymbolicoptiondiscoveryforfacilitating
deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 36. 7042–7050.
[22]Zoe Juozapaitis, Anurag Koul, Alan Fern, Martin Erwig, and Finale Doshi-Velez.
2019. Explainable reinforcement learning via reward decomposition. In IJCAI/E-
CAI Workshop on explainable artificial intelligence.
[23]Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting
decision tree. Advances in neural information processing systems 30 (2017).
[24]Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel J
Gershman, and Finale Doshi-Velez. 2019. Human evaluation of models built for
interpretability.In ProceedingsoftheAAAIConferenceonHumanComputation
and Crowdsourcing, Vol. 7. 59–67.
[25]EdouardLeurentandJeanMercat.2019.Socialattentionforautonomousdecision-
making in dense traffic. arXiv preprint arXiv:1911.12250 (2019).
[26]Guiliang Liu, Oliver Schulte, Wang Zhu, and Qingcan Li. 2019. Toward inter-
pretabledeepreinforcementlearningwithlinearmodelu-trees.In MachineLearn-
ing and Knowledge Discovery in Databases: European Conference, ECML PKDD
2018, Dublin, Ireland, September 10–14, 2018, Proceedings, Part II 18. Springer,
414–429.
[27]YangLiu,SujayKhandagale,ColinWhite,andWillieNeiswanger.2021. Syntheticbenchmarksforscientificresearchinexplainablemachinelearning. arXivpreprint
arXiv:2106.12543 (2021).
[28]ScottMLundberg,GabrielGErion,andSu-InLee.2018. Consistentindividualized
feature attribution for tree ensembles. arXiv preprint arXiv:1802.03888 (2018).
[29]ScottMLundbergandSu-InLee.2017. Aunifiedapproachtointerpretingmodel
predictions. Advances in neural information processing systems 30 (2017).
[30]Daoming Lyu, Fangkai Yang, Bo Liu, and Steven Gustafson. 2019. SDRL: in-
terpretableanddata-efficientdeepreinforcementlearningleveragingsymbolic
planning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33.
2970–2977.[31]PrashanMadumal,TimMiller,LizSonenberg,andFrankVetere.2020. Explain-
able reinforcement learning through a causal lens. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 34. 2493–2500.
[32]Stephanie Milani, Nicholay Topin, Manuela Veloso, and Fei Fang. 2022. A survey
of explainable reinforcement learning. arXiv preprint arXiv:2202.08434 (2022).
[33]SuvirMirchandani,SiddharthKaramcheti,andDorsaSadigh.2021. Ella:Explo-
ration through learned language abstraction. Advances in Neural Information
Processing Systems 34 (2021), 29529–29540.
[34]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou,DaanWierstra,andMartinRiedmiller.2013. Playingatariwithdeep
reinforcement learning. arXiv preprint arXiv:1312.5602 (2013).
[35]GeraudNangueTasse,StevenJames,andBenjaminRosman.2020. Abooleantask
algebraforreinforcementlearning. AdvancesinNeuralInformationProcessing
Systems33 (2020), 9497–9507.
[36]RuiNian,JinfengLiu,andBiaoHuang.2020. Areviewonreinforcementlearning:
Introductionandapplicationsinindustrialprocesscontrol. Computers&Chemical
Engineering 139 (2020), 106886.
[37]Matthew L Olson, Roli Khanna, Lawrence Neal, Fuxin Li, and Weng-Keen Wong.
2021. Counterfactualstateexplanationsforreinforcementlearningagentsvia
generative deep learning. Artificial Intelligence 295 (2021), 103455.
[38]AliPayaniandFaramarzFekri.2020. Incorporatingrelationalbackgroundknowl-
edge into reinforcement learning via differentiable inductive logic programming.
arXiv preprint arXiv:2003.10386 (2020).
[39]VitaliPetsiuk,AbirDas,andKateSaenko.2018. Rise:Randomizedinputsampling
for explanation of black-box models. arXiv preprint arXiv:1806.07421 (2018).
[40]ErikaPuiuttaandEricMSPVeith.2020. Explainablereinforcementlearning:A
survey. In International cross-domain conference for machine learning and knowl-
edge extraction. Springer, 77–95.
[41]NikaashPuri,SukritiVerma,PiyushGupta,DhruvKayastha,ShripadDeshmukh,
Balaji Krishnamurthy, and Sameer Singh. 2019. Explain your move: Understand-
ingagentactionsusingspecificandrelevantfeatureattribution. arXivpreprint
arXiv:1912.12191 (2019).
[42]MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2016. "Whyshouldi
trustyou?"Explainingthepredictionsofanyclassifier.In Proceedingsofthe22nd
ACMSIGKDDinternationalconferenceonknowledgediscoveryanddatamining.
1135–1144.
[43]MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2018. Anchors:High-
precision model-agnostic explanations. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 32.
[44]John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximalpolicyoptimizationalgorithms. arXivpreprintarXiv:1707.06347
(2017).
[45]AvantiShrikumar,PeytonGreenside,andAnshulKundaje.2017. Learningim-
portant features through propagating activation differences. In International
conference on machine learning. PMLR, 3145–3153.
[46]Tianmin Shu, Caiming Xiong, and Richard Socher. 2017. Hierarchical and inter-
pretable skill acquisition in multi-task reinforcement learning. arXiv preprint
arXiv:1712.07294 (2017).
[47]Shagun Sodhani, Amy Zhang, and Joelle Pineau. 2021. Multi-task reinforce-
ment learningwith context-based representations.In International Conferenceon
Machine Learning. PMLR, 9767–9779.
[48]MukundSundararajan,AnkurTaly,andQiqiYan.2017. Axiomaticattribution
for deep networks. In International conference on machine learning. PMLR, 3319–
3328.
[49]Yujin Tang and David Ha. 2021. The sensory neuron as a transformer:
Permutation-invariant neural networks for reinforcement learning. Advances in
Neural Information Processing Systems 34 (2021), 22574–22587.
[50]Giulia Vilone and Luca Longo. 2020. Explainable artificial intelligence: a system-
atic review. arXiv preprint arXiv:2006.00093 (2020).
[51]GeorgeAVouros.2022. Explainabledeepreinforcementlearning:stateoftheart
and challenges. Comput. Surveys 55, 5 (2022), 1–39.
[52]Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. 2020. Shapley
Q-value:Alocalrewardapproachtosolveglobalrewardgames.In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 34. 7285–7292.
[53]RomanVYampolskiyandJoshuaFox.2013. Artificialgeneralintelligenceand
thehumanmentalmodel. In Singularityhypotheses:Ascientificandphilosophical
assessment. Springer, 129–145.
[54]Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng
Yu,ShaojieYang,XipengWu,QingweiGuo,QiaoboChen,YinyutingYin,Hao
Zhang, Tengfei Shi, Liang Wang, Qiang Fu, Wei Yang, and Lanxiao Huang. 2020.
Mastering complex control in moba games with deep reinforcement learning. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 6672–6679.
[55]WenshuaiZhao,JorgePeñaQueralta,andTomiWesterlund.2020. Sim-to-real
transfer in deep reinforcement learning for robotics: a survey. In 2020 IEEE
symposium series on computational intelligence (SSCI). IEEE, 737–744.
[56]Jianlong Zhou, Amir H Gandomi, Fang Chen, and Andreas Holzinger. 2021.
Evaluatingthequalityofmachinelearningexplanations:Asurveyonmethods
and metrics. Electronics 10, 5 (2021), 593.
6082