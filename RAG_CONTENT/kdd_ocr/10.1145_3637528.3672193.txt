From Word-prediction to Complex Skills: Compositional
Thinking and Metacognition in LLMs
Sanjeev Arora
Princeton University
Princeton, USA
arora@cs.princeton.edu
ABSTRACT
The talk will present evidence that today’s large language models
(LLMs) display somewhat deeper “understanding” than one would
naively expect.
1. When asked to solve a task by combining a set of k simpler
skills (“test of compositional capability”), they are able to do so
despite not having seen the same combination of skills during their
training.
2. They demonstrate ability to reason about of their own learning
processes, which is analogous to "metacognitive knowledge"[Flavel’76]
in humans. For instance, given examples of an evaluation task, they
can produce a catalog of suitably named skills that are relevant
for solving each example of that task. Furthermore, this catalog of
skills is meaningful, in the sense that incorporating it into train-
ing pipelines improves performance (including of other unrelated
LLMs) on that task.
We discuss mechanisms by which such complex understanding
could arise (including a theory by [Arora,Goyal’23] that tries to
explain (a)) and also give examples of how to leverage LLM meta
knowledge to improve LLM training pipelines as well as evaluations.
KEYWORDS
keynote talk
ACM Reference Format:
Sanjeev Arora. 2024. From Word-prediction to Complex Skills: Composi-
tional Thinking and Metacognition in LLMs. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 1 page.
https://doi.org/10.1145/3637528.3672193
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.36721931 SPEAKER BIO
Sanjeev Arora is the director of the Princeton Language and Intelli-
gence initiative and the Charles C. Fitzmorris Professor of Computer
Science at Princeton University. He works on developing concep-
tual understanding and better design methods for today’s large AI
models. He has received the Packard Fellowship (1997), Simons
Investigator Award (2012), Gödel Prize (2001 and 2010), ACM Prize
in Computing (formerly the ACM-Infosys Foundation Award in the
Computing Sciences) (2012), and the Fulkerson Prize in Discrete
Math (2012). He is an ACM Fellow, a fellow of the American Acad-
emy of Arts and Sciences, and member of the National Academy of
Science.
 
1
