Estimated Judge Reliabilities for Weighted Bradley-Terry-Luce
Are Not Reliable
Andrew F. Dreher
The University of Texas at Austin
Austin, Texas, USA
afdreher@utexas.eduEtienne Vouga
The University of Texas at Austin 
Austin, Texas, USA
vouga@cs.utexas.eduDonald S. Fussell
The University of Texas at Austin
Austin, Texas, USA
fussell@cs.utexas.edu
ABSTRACT
There are many applications for which we want to learn a latent
scale for subjective properties, such as the excitement of a photo
or the legibility of a font; however, obtaining human-labeled data
is costly and time-consuming. One oft-used method for acquiring
these labels, despite the cost being quadratic in the number of items,
is the method of pairwise comparisons since this method minimizes
the effect of biases and generally can be used effectively outside of
a controlled environment.
Crowdsourcing appears to be a panacea since online platforms
provide affordable access to numerous people, but these partici-
pants, judges, vary in diligence and expertise. Several methods have
been proposed to assign weights to judges based on their responses
relative to everyone else, the goal being to reduce exposure to poor
performers, hopefully upgrading the quality of the data.
Our research focuses on two natural extensions to the Bradley-
Terry-Luce formulation of scaling that jointly optimize for both
scale value and judge weights. While both methods appear to per-
form at least as well as the unweighted formulation on average
with well-behaved judges, we report a previously unknown flaw,
revealing that the resultant judge weights should not be interpreted
as reliabilities. Consequently, these values should not be leveraged
for decisions about the judges, such as for active sampling or to
validate the participant pool.
CCS CONCEPTS
•Information systems →Rank aggregation; •Theory of
computation→Theory and algorithms for application domains ;•
Human-centered computing →Collaborative and social comput-
ing theory, concepts and paradigms .
KEYWORDS
pairwise comparisons, crowdsourced data, ranking, reliability
ACM Reference Format:
Andrew F. Dreher, Etienne Vouga, and Donald S. Fussell. 2024. Estimated 
Judge Reliabilities for Weighted Bradley-Terry-Luce Are Not Reliable. In 
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery 
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New 
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671907
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36719071 INTRODUCTION
This paper demonstrates a fundamental flaw that arises when
jointly optimizing for latent scale values and judge reliabilities
using pairwise comparisons. Although it may appear that careful
data analysis can overcome any problems, our work explores how
techniques for learning subjective properties may lead to the cre-
ation of a small minority of judges that behave as dictators, which
fundamentally alters the results.
Subjective properties are an important feature in data modeling,
essential for creating, analyzing, and enhancing the human-like
qualities of modern machine learning models. Consider the task
of designing a captivating book cover using generative AI. While
generating a cover is straightforward, in order to create a “captivat-
ing” cover, the model must capture human notions of quality. For
example, it might mean that the background photo is compelling
while the title is simultaneously legible. Yet “compelling” and “legi-
ble” are also subjective properties, whose interpretation depends
on the beholder.
Superficially, evaluating the quality of these properties is straight-
forward: we determine correctness based on human opinions. We
could collect many responses from a target audience, score them
on a scale of 1 to 10, and then take the average of these responses.
This would provide some metric for evaluation, but it would be
subject to numerous biases and issues as participants (also called
raters, annotators, or judges) will not share the same underlying
latent scale.
This is a classic problem with well-understood principles, and
extensive literature explores the issues of biases and rater agree-
ment. For a review, see [ 40,49]. Notably, one way to avoid many of
these issues is to use pairwise preferences.
The Bradley-Terry (BT) or Bradley-Terry-Luce (BTL) model is
a popular method to estimate the value of a stimulus, such as a
particular image, on a latent scale from pairwise preferences. It is a
natural fit for crowdsourced data because pairwise judgments can
recover subtle differences in the latent values, even when surveying
non-experts, though, it requires numerous samples.
The quality of online judges varies significantly in terms of skill
and motivation. Extensions to the BTL model estimate not only
the latent scale values but also the reliability of the judges, which
could both improve the resulting estimates and reduce the required
number of samples. In the same way that judges are not equally
reliable, the differences between pairs are not equally difficult to
discern. For example, deciding weights that differentiate an elephant
from a dog is easier than differentiating a small dog from a large
cat. Because of this, it appears desirable to simultaneously solve for
both latent quantities (i.e ., scale values and judge reliability). For
example, we could penalize judges more for missing “easy” pairs,
642
KDD ’24, August 25–29, 2024, Barcelona, Spain Andrew F. Dreher, Etienne Vogua, and Donald S. Fussell
while adjusting our estimation of the difficulty based on how many
judges incorrectly rate the pair.
Unfortunately, such weighted extensions to BTL do not reliably
estimate the judge weights, leading to an issue where a small num-
ber of judges are all but guaranteed to become “dictators” that
control the learned scale values. Although we do not provide a
solution to this problem, which may be unlikely due to the implicit
to the act of weighing judge reliability, we discuss techniques and
strategies that researchers can use to determine whether their re-
sults have been unduly skewed by these issues. Our goal in this
research is to bring to light this hitherto unknown and overlooked
flaw. Since this class of techniques is particularly appealing in the
modern machine learning space, researchers should be aware of
these fundamental issues to ensure the validity of their learned
latent spaces.
2 BACKGROUND
A method for estimating real values of psychological concepts from
pairwise comparisons was first formalized by Thurstone, who pro-
posed that the probability of a person, a judge,𝑢, choosing one item,
a stimulus,𝑠, over another is proportional to the distance between
the stimuli along a latent scale [ 42,43]. Let𝑠𝑖∈Srepresent a stim-
ulus from the set of stimuli Sindexed by𝑖with a corresponding
latent scale value 𝜍𝑖∈R. The model assumes that every stimulus
𝑠𝑖is represented by a latent Gaussian random variable with mean
𝜍𝑖on the latent scale and that when a pair is presented to a judge,
the judge draws samples from the corresponding distributions and
reports which variable produced the larger value.
Let a single judge 𝑢make a decision between a pair of stim-
uli𝑠𝑗and𝑠𝑘. This creates an ordered decision, 𝑣𝑢(𝑠𝑘,𝑠𝑗), where
𝑣𝑢(𝑠𝑘,𝑠𝑗)=1if u determines 𝑠𝑘≻𝑠𝑗and 0 otherwise. By collect-
ing many judgments about the pair from different judges, 𝑢𝑖∈U,
we can compute an empirical probability that 𝑠𝑘is preferred to 𝑠𝑗,
Pr(𝑠𝑘≻𝑠𝑗)=Í
𝑢𝑖∈U𝑣𝑢𝑖(𝑠𝑘,𝑠𝑗)
𝑣𝑢𝑖(𝑠𝑘,𝑠𝑗)+𝑣𝑢𝑖(𝑠𝑗,𝑠𝑘).Pr(𝑠𝑘≻𝑠𝑗)can then be
related to a distance between the two scale values representing the
stimuli,𝜍𝑘−𝜍𝑗orΔ𝜍𝑘𝑗, since the distribution of the differences is
Gaussian. This model is known as Thurstone’s Law of Comparative
Judgment. For a detailed treatment, see Torgerson [ 45]. Although
Thurstone developed several cases with different assumptions and
restrictions, generally only the most restricted version, Thrustone’s
case V, which assumes that the random variables are independent
and share the same variance, is commonly used.
Zermelo [ 51] and Bradly and Taylor [ 6] each developed a model
similar to Thurstone’s case V but using the logit function instead of
the probit function. This model is often called the Bradley-Terry (BT)
model, or because of the contributions by Luce [ 31], the Bradley-
Terry-Luce (BTL) model. A good review of the model is available
from Cattelan [ 10]. Hamilton et al. [20] demonstrate a myriad of
ways to arrive at the BTL model, stressing its importance to many
fields.
Pr
BTL(𝑠𝑘≻𝑠𝑗)=𝑒𝜍𝑘
𝑒𝜍𝑘+𝑒𝜍𝑗=1
1+𝑒𝜍𝑗−𝜍𝑘=1
1+𝑒−Δ𝜍𝑘𝑗(1)
The BTL model, eq. (1), relates the scale values 𝜍𝑗and𝜍𝑘using
a logistic function for a single pair, but we need to estimate the
values of all 𝑠𝑖∈S,𝝇. We collect all of the decisions made by all of
the judges into a single dataset, V. Then, we find 𝝇by maximizing
the joint probability for all decisions in the dataset, 𝑣𝑖∈V , orequivalently, minimizing the negative log-likelihood, L(eq. (2)).
min𝝇∑︁
𝑢𝑖∈U∑︁
𝑠𝑘,∈S,𝑠𝑗∈S,𝑠𝑘≠𝑠𝑗−𝑣𝑢𝑖 𝑠𝑘,𝑠𝑗log
Pr
BTL 𝑠𝑘,𝑠𝑗
(2)
One issue with eq. (2) is that if 𝑠𝑘is universally preferred to
𝑠𝑗,PrBTL(𝑠𝑘≻𝑠𝑗)=1, then the distance between the stimuli
should be infinite. This necessitates the introduction of a bounding
mechanism.
A simple mechanism is to use box bounds (e.g .,𝜍𝑖∈[− 100,100]).
The box bounds prevent the problem of infinite values, but the
bounds need to be sufficiently loose to minimize distortion. For
example, restricting 𝜍𝑖∈ [− 0.1,0.1]makes the maximum Δ𝜍=
0.2, and thus, the maximum representable probability is approxi-
mately 0.55using eq. (1). A similar restriction on Pr(·)(e.g.,Pr(·)∈
[0.01,0.99]) to avoid unanimity is an alternative.
Instead of bounds, Mease solved this problem by adding a dummy
stimulus,𝑠0, sometimes called a virtual node, and typically set
to𝜍0=0, which is given both a win and a loss against every
other stimulus as shown in eq. (3) [ 32]. Equation (3) is added to
eq. (2) as a penalty term for the optimization, thereby adding two
new decisions for every 𝑠𝑖: one where the 𝑠𝑖≻𝑠0, and one where
𝑠𝑖≺𝑠0. The penalty can modulated using a weight 𝜆, typically set
to𝜆=1. This method not only elegantly handles the problem with
unanimous decisions, which would ordinarily require an infinite
separation, but also uniquely identifies the scale, which would
otherwise be non-unique with respect to translation.
−𝜆∑︁
𝑠𝑖∈Slog Pr
BTL(𝑠𝑖≻𝑠0)+log Pr
BTL(𝑠0≻𝑠𝑖) (3)
This penalty is proportional to the distance between 𝑠𝑖and𝑠0.
When we care about 𝝇rather than the order alone, this penalty dis-
torts the relationship between the latent distance and the empirical
probability: two pairs with the same input empirical probability may
no longer have the same estimated latent distance along the scale.
Furthermore, the number of decisions between pairs of stimuli is
often non-uniform, but the penalty is applied once per stimulus.
Therefore, using the penalty complicates the analysis because, in
the minimization, the penalty does not scale proportionately with
the number of terms containing a given stimulus.
We focus our work on the box-bounded variant because the
analysis is simpler, but our conclusions do not depend significantly
on this choice. We will show experiments using the virtual node
method in section 4.3.
2.1 Judge Reliability
Judges are not equally skilled, diligent, or observant. Two prototyp-
ical undesirable judge behaviors are:
(1)A judge may act randomly, perhaps to minimize the time
spent on the decision and maximize their reward [ 14,16,25,
41]. These judges contribute no information, adding only
noise to the final solution, so we increase the signal-to-noise
ratio of the final solution if we eliminate their votes.
(2)A judge may either purposefully or inadvertently mistake the
direction of the task [ 14,25]. These judges provide consistent
but contrary votes, so unlike the random judges, these judges
would contribute useful signal to the final solution if we
invert rather than eliminate their votes.
 
643Estimated Judge Reliabilities for Weighted Bradley-Terry-Luce Are Not Reliable KDD ’24, August 25–29, 2024, Barcelona, Spain
Chen et al. [11] and O’Donovan et al. [33] proposed different,
natural extensions to the BTL model to address these problematic
behaviors by adding per-judge weights. How these weights are
incorporated differs due to distinct assumptions about the origin of
the behaviors.
Despite the philosophical differences, both models simultane-
ously estimate the latent scale values, 𝝇, and the judge weights.
They do so because the latent scale values necessarily influence the
estimate of how reliable a judge is. If the judge makes an “error,”
the distance between the pair determines the severity: the larger
the difference, the more obvious the pair should be, and hence the
worse the error. Reciprocally, accounting for the trustworthiness of
the judges means that the judge weights influence the latent scale
values.
−10 −5 0 5 1000.250.50.751
∆ςPr (sk≻sj,·)Chen et al. (eq. (3))
O’Donovan et al. (eq. (4))
Figure 1: Plot of the logistic functions eqs. (4) and (5) for
varying judge reliabilities. We set 𝜇=2𝜂−1in eq. (5) and
use𝜂={0,0.125,...,1}to plot these equations on the same
scale. The two curves are identical for 𝜂={0,0.5,1}; how-
ever, as Δ𝜍→∞ , O’Donovan et al.’s method asymptotically
approaches sgn(𝜇), while Chen et al .’s asymptotically ap-
proaches𝜂.
Chen et al.’s method, called Crowd-BT, models the judges as
unreliable witnesses of the underlying process. Crowd-BT (eq. (4))
introduces a variable 𝜂𝑖as an estimate of judge reliability, where
the judge is assumed to sample from the latent order using the
BTL model but only reports that result with probability 𝜂𝑖. With
probability 1−𝜂𝑖, the judge reports the opposite. Using the law of
total probability, 0≤𝜂𝑖≤1.
Pr
Crowd-BT 𝑠𝑘≻𝑠𝑗,𝜂𝑖=𝜂𝑖Pr
BTL(𝑠𝑘≻𝑠𝑗)+(1−𝜂𝑖)
1−Pr
BTL(𝑠𝑘≻𝑠𝑗)
(4)
With eq. (4), a judge with 𝜂𝑖=1always reports the BTL sampled
order, a judge with 𝜂𝑖=0.5reports a random decision, and a judge
with𝜂𝑖=0always reports the opposite of the BTL sampled order.
This formulation is similar to that of Carterette and Petkova [ 9],
except that𝜂is continuous rather than binary.
A common criticism of Crowd-BT is the assumption that a judge
has fixed stochasticity regardless of the distance between stimuli
since this does not align with Thurstone’s Law of Comparative
Judgment. Nonetheless, it is popular and useful for a wide range
of tasks since this approach is suited for modeling judges who
behave randomly or mistakenly rather than those with varying skill
levels. For example, Cui et al. [13] extend Crowd-BT to discover the
popularity of latent geospatial points of interest, while Zhang et
al. [53] show that Crowd-BT performs competitively in the top- 𝑘
problem. Recently, Zhang and Kashima [ 52] combine Crowd-BT
with reinforcement learning, replacing the per-judge 𝜂𝑖with a per-
pair parameter.O’Donovan et al. modify eq. (1) by introducing a weight 𝜇𝑖as
the scale parameter in the logistic distribution, yielding eq. (5). This
parameter,𝜇𝑖, is related to the standard scaling parameter of the
logistic distribution, 𝑠=1
𝜇, and thus changes the slope of the logistic
function. Because the logistic function in BTL approximates the
Gaussian distribution in Thrustone’s case V, 𝜇𝑖is therefore directly
related to the variance of the assumed Gaussians on a per-judge
basis, making this a natural extension for modeling judges with
varying skill levels.
Pr
O’Donovan(𝑠𝑘≻𝑠𝑗,𝜇𝑖)=1
1+𝑒−𝜇𝑖Δ𝜍𝑘𝑗(5)
In eq. (5), more reliable judges, those with larger |𝜇𝑖|, can decide
a pair with less perceptual separation since they have smaller vari-
ances in the underlying Gaussians. Unlike Crowd-BT, for 𝜇𝑖≠0,
once Δ𝜍is large enough, even less-reliable judges can consistently
order the stimuli (fig. 1). Negative values of 𝜇indicate adversar-
ial judges, who, although they perceive the difference, report the
opposite of the true underlying order.
Incorporating the standard scaling parameter, 𝑠, of a logistic
distribution is obviously not unique in the literature; others have
proposed and used equations similar to eq. (5). Whitehill et al. de-
velop a method for labeling images with unreliable users similar
to eq. (5) except that their variables are binary [ 48]. The successful
RankNet family [ 7,8] uses a feed-forward neural network to pro-
vide the final ranking but uses a construction similar to eq. (5) for
estimating the quality of a document pair for the learning phase.
Instead of having a per-judge 𝜇𝑖, RankNet has a single parameter, 𝜎,
shared by all decisions; Burges notes that the choice of value for 𝜎
has no effect for RankNet [ 8]. Internally, Koyama et al. [26] employ
a construction similar to eq. (5) to acquire crowdsourced data to
manipulate images in multiple dimensions. Unlike O’Donovan et
al., they either fix the value of 𝜇, similar to Burges [ 8], or update it
per pair based on the variance of the group’s selected slider position,
which is the input mechanism they use.
Unfortunately, as we will now demonstrate, because of the in-
terplay between the latent scale values and the judge reliabilities,
jointly optimizing for them can lead to paradoxical solutions where
a small minority of judges act as dictators.
3 CORE BEHAVIOR
To appreciate how both weighted methods fail, it is important first
to understand how the judge weights change the models’ behavior
from the unweighted BTL.
Consider the simplest possible example: two judges U={𝑢1,𝑢2},
each of whom makes a decision about a single pair of stimuli S=
{𝑠1,𝑠2}. Each judge chooses either 𝑠1≺𝑠2or𝑠1≻𝑠2. Because of
the sign symmetry, we only need to consider two cases: one where
the judges agree and one where they do not.
We will assume that 𝝇is constrained by some constant limits
𝜍−,𝜍+. Although not part of O’Donovan et al.’s original formu-
lation, we impose box bounds on 𝜇𝑖∈[𝜇−,𝜇+]and, for simplicity,
we assume𝜇−=−𝜇+.
 
644KDD ’24, August 25–29, 2024, Barcelona, Spain Andrew F. Dreher, Etienne Vogua, and Donald S. Fussell
3.1 Agreement
When𝑢1and𝑢2agree, the decision is unanimous, and the optimal
solution for eqs. (1), (4) and (5) is to maximize the difference in the
latent scale values, setting one to 𝜍−and the other to 𝜍+.
For both weighted methods, setting the judge weight to the limit
is optimal. For Crowd-BT this is 𝜂𝑖=1; and for O’Donovan et al.’s
method𝜇𝑖=𝜇+.
3.2 Disagreement
When the judges disagree, the votes are in direct opposition.
3.2.1 Bradley-Terry-Luce. Combining each judge using eq. (1), the
negative log-likelihood becomes eq. (6), which has a minimum
atΔ𝜍=0, meaning 𝜍1=𝜍2. If we use the virtual node technique
(eq. (3)) to eliminate the translation ambiguity, then 𝜍1=𝜍2=𝜍0=0.
This is the expected result: the stimuli are indistinguishable since
the decisions are equal and opposite.
L
BTL(Δ𝜍)=−ln1
1+𝑒Δ𝜍
−ln1
1+𝑒−Δ𝜍
(6)
3.2.2 Chen et al.’s Crowd-BT. Combining the two votes using
eq. (4), gives us eq. (7).
L(Δ𝜍,𝜼)
Crowd-BT=−ln
𝜂11
1+𝑒Δ𝜍+(1−𝜂1)1
1+𝑒−Δ𝜍
−ln
𝜂21
1+𝑒−Δ𝜍+(1−𝜂2)1
1+𝑒Δ𝜍
(7)
If𝜂1=𝜂2, the result is equivalent to the BTL solution of eq. (6);
however, this is not optimal. We can minimize eq. (7) by setting
𝜂1=1,𝜂2=0, and maximizing Δ𝜍, separating 𝑠1and𝑠2as much as
allowed by
𝜍−,𝜍+
.
Importantly, although the input data are the same, this solution
changes the interpretation from one where the two stimuli are
indistinguishable to one where the separation between the two
stimuli is evident, and instead of having equally reliable judges, we
have one honest judge and one dishonest one.
Because of the vote symmetry, there are two global minima
to eq. (7) that differ only in which judge is labeled honest versus
dishonest. Numerical optimization of eq. (7) arbitrarily chooses one
of these solutions influenced by the starting parameters.
3.2.3 O’Donovan et al.. The analysis of O’Donovan et al.’s method
parallels that of Crowd-BT. Because of the symmetry of multiplica-
tion, eq. (8) is a hyperbolic paraboloid when both 𝜇andΔ𝜍are free.
The two minima are found by maximizing the value of 𝜇Δ𝜍.
L(Δ𝜍,𝝁)
O’Donovan=−ln1
1+𝑒−𝜇1Δ𝜍
−ln1
1+𝑒𝜇2Δ𝜍
(8)
As with eq. (7), the result is that the stimuli are maximally sepa-
rated with one honest and one dishonest judge. The same arbitrary
choice between the two global minima applies here as well.
While arguments can be made supporting any of the solutions,
the critical point is the difference in behavior: the weighted models
maximize the separation, freely inverting a judge’s vote to achieve
it, while BTL results in indistinguishable stimuli.
4 CREATING DICTATORS
We now demonstrate cases where the weighted models violate our
expectations.Let there be 𝑛judges,U={𝑢1,𝑢2,...,𝑢𝑛}, who all vote for 𝑛+1
stimuli,S={𝑠1,𝑠2,...,𝑠𝑛,𝑠𝑛+1}, according to a strictly known order
𝒔★=𝑠1≺···≺𝑠𝑛≺𝑠𝑛+1except at a single location corresponding
to the judge’s index. Thus, the 𝑖’th judge,𝑢𝑖, has a corresponding
“errored” decision 𝑠𝑘≻𝑠𝑘+1. Formally, let 𝑣𝑖(𝑘)represent a single
vote by𝑢𝑖between stimuli 𝑠𝑘and𝑠𝑘+1be defined according to
eq. (9). The set of all votes V={𝑣𝑖(𝑘)|𝑖,𝑘∈1,...,𝑛}. Importantly,
the decisions do not create a cycle, and the estimated order ˆ𝒔will
be equivalent to 𝒔★when using eq. (2).
𝑣𝑖(𝑘)=(
𝑠𝑘≻𝑠𝑘+1𝑘=𝑖
𝑠𝑘≺𝑠𝑘+1otherwise(9)
Now consider two different, idealized outcomes of minimizing
the negative log-likelihood:
Equality Every judge has the same weight, and ˆ𝒔=𝒔★.
Dictatorial There is a set of “dictators,” 𝑢𝑖∈ D , given the
most extreme weight possible: 𝜂𝑖,𝑢𝑖∈D=1for Crowd-BT
and𝜇𝑖,𝑢𝑖∈D=𝜇+for O’Donovan et al.’s method. For sim-
plicity, we assume that |D|=1and that ˆ𝒔matchesV𝑖,𝑢𝑖∈D,
meaning thatL𝑢𝑖,𝑢𝑖∈D(·)→ 0.
By construction, the judges are symmetric: each has an equiva-
lent number of votes and agrees with 𝑛−1other judges at every
position except one. We, therefore, intuitively, would likethe equal-
ity result to be preferred to the dictatorial one. Unfortunately, both
weighted methods fail to meet this expectation.
4.1 Chen et al.’s Crowd-BT
Assume the equality solution. Each judge has weight 𝜂𝑖=𝜂𝐸since
they contribute equally. For a single 𝑢𝑖, there are𝑛−1locations
where𝑢𝑖’s preference matches 𝒔★, yielding a penalty of −ln(𝜂𝐸)
each, and a single location where it does not, yielding −ln(1−𝜂𝐸).
With𝑛judges, we have eq. (10).
Lequality
Crowd-BT(𝑛)=−𝑛
(𝑛−1)ln(𝜂𝐸)+ln(1−𝜂𝐸)
(10)
We now assume the dictatorial solution. Since the final solution
matches the dictator, there is no loss due to 𝑢𝑖∈D. There are𝑛−1
non-dictator judges who have 𝜂𝑖,𝑢𝑖∉D=𝜂𝑅, each of whom matches
𝑛−1of the dictator’s decisions and disagrees once, yielding eq. (11).
Note that𝜂𝑅>1
2because𝑢𝑖∉Dgenerally agree with 𝑢𝑖∈D.
Ldictator
Crowd-BT(𝑛)=−(𝑛−1)
(𝑛−1)ln(𝜂𝑅)+ln(1−𝜂𝑅)
(11)
Solving for𝜂∗𝑛, the optimal 𝜂for a given𝑛, forLdictator
Crowd-BT(𝑛)and
Lequality
Crowd-BT(𝑛)fortunately yields 𝜂∗𝑛=𝑛−1
𝑛for both. Substituting𝑛−1
𝑛
for𝜂∗𝑛into eqs. (10) and (11) to get the expressions in terms of a
common𝑛yields eq. (12).
L∗
equality
Crowd-BT(𝑛)=−𝑛ln1
𝑛
+L∗
dictator
Crowd-BT(𝑛) (12)
Since𝑛≥2, the value−𝑛ln
1
𝑛
>0, and thus the negative
log-likelihood for the equality result will always be worse than
for the dictatorial one. The global minimizer for eq. (4), therefore,
cannot be one where all of the judges have equal weight. This is
surprising, given the symmetry of the problem. We start with judges
who behave symmetrically, but the result is non-symmetric: at least
one of the judges will always be promoted, albeit potentially very
slightly, over the others.
 
645Estimated Judge Reliabilities for Weighted Bradley-Terry-Luce Are Not Reliable KDD ’24, August 25–29, 2024, Barcelona, Spain
Critically, the creation of a dictatorship is guaranteed; it does
not depend on implementation details.
4.2 O’Donovan et al.
Again, we start with the equality solution, where 𝜇𝑖=𝜇𝐸. Since all
the judges share 𝜇𝐸, this value uniformly scales Δ𝜍, which means
we can ignore 𝜇𝐸, making eq. (5) is the same as eq. (1). Because of the
symmetry of the problem, we know that PrBTL(𝑠𝑖+1≻𝑠𝑖)=𝑛−1
𝑛,
andPrBTL(𝑠𝑖≻𝑠𝑖+1)=1−PrBTL(𝑠𝑖+1≻𝑠𝑖)=1
𝑛.
Since there are 𝑛judges who each have 𝑛−1decisions where
𝑠𝑖+1≻𝑠𝑖and one decision where 𝑠𝑖≻𝑠𝑖+1, we can substitute in the
probabilities to yield eq. (13).
Lequality
O’Donovan(𝑛)=−(𝑛2−𝑛)ln𝑛−1
𝑛
−𝑛ln1
𝑛
(13)
Now, we consider the dictatorial case. Unlike Crowd-BT, we can
make the simplifying assumption that 𝜇𝑖,𝑢𝑖∉D=0by assuming that
Δ𝜍is large. By doing so, assuming 𝜇𝑖≥0, when𝑢𝑖agrees with the
dictator−ln
1
1+𝑒−𝜇𝑖Δ𝜍
→0; where𝑢𝑖disagrees with the dictator
−ln
1
1+𝑒𝜇𝑖Δ𝜍
>−ln
1
1+𝑒0
. Setting𝜇𝑖<0would cause the loss at
the agreement locations to no longer be negligible, and since there
are more agreement locations than disagreement locations, this
would be non-optimal. We, therefore, have eq. (14) as the negative
log-likelihood for the dictatorial case.
Ldictator
O’Donovan(𝑛)≈−(𝑛2−𝑛)ln1
2
(14)
Setting eqs. (13) and (14) equal, and solving for 𝑛yields𝑛≈4.403.
This result is important because, unlike Crowd-BT, it is possible,
although not guaranteed, for O’Donovan et al.’s method to yield
symmetrically weighted judges. Although eq. (13) is found using
PrBTL, the𝝇estimated using BTL will not be equivalent to the one
estimated using O’Donovan et al. even when 𝑛≥5because of the
scaling due to the judge weights, 𝝁.
Numerical results demonstrating the indicated behavior for all
of the methods are provided in the supplementary material.
4.3 Breaking the Symmetry
Breaking the symmetry in the previous example can exacerbate
the mismatch between our expectations and the results for both
weighted methods.
We invert a single “correct” decision in a single judge’s set of
decisions, creating an additional “error.” Now, there are 𝑛−1judges
with 1 inversion from 𝒔★each and one judge with 2 inversions
from 𝒔★, meaning that we now have 𝑛−1pairs where 𝑛−1judges
agree and one pair where 𝑛−2judges agree. Restricting 𝑛≥5, the
majority always agrees, and therefore optimizing using eq. (2) will
again result in the 𝒔★order.
Regarding judge reliabilities, our preferred outcome would be
one where the 𝑛−1judges with a single inversion from 𝒔★have
similar reliability scores while the judge with 2 inversions from
𝒔★is deemed the least reliable. Unfortunately, we show that both
weighted methods can elevate the least reliable judge into D.
For this example, we invert 𝑣1(4), replacing𝑠4≺𝑠5with𝑠4≻𝑠5
in𝑢1’s decisions. By construction, 𝑢1is now the least reliable judge,
and the additional inversion in 𝑢1’s decisions matches the inversion
location for 𝑢4,𝑣4(4).When performing numerical optimizations, we define the set of
dictators as having close to the maximum value allowed; in our ex-
periments, we use 𝑀=0.95, meaning thatD={𝑢𝑖||2𝜂𝑖−1|≥𝑀}
for CrowdBT andD=
𝑢𝑖|𝜇𝑖|≥𝑀𝜇+	for O’Donovan et al., as-
suming𝜇−=−𝜇+.
Crowdt-BT O’Donovan et al.
𝜆𝜂1𝜂2𝜂3𝜂4𝜂5𝜇1𝜇2𝜇3𝜇4𝜇5
0.01.0 0.5 0.5 1.0 0.5 10.0 0.0 0.0 10.0 -0.0
1.01.0 0.4 0.3 1.0 0.0 10.0 -1.1 -1.1 10.0 -1.1
Table 1: Judge weights for the broken symmetry example.
Bold entries are 𝑢𝑖∈D, while italic entries are judges deemed
to be adversarial.
Optimization results from Mathematica [ 21] are shown in table 1
using𝜆∈{0,1}. We show these two 𝜆values because we use 𝜆=0
for our experiments in section 5, mirroring O’Donovan et al.’s
formula, while 𝜆=1is the original version of Mease and a value
used in Chen et al. Naturally, outcomes are parameter dependent.
4.3.1 Chen et al.’s Crowd-BT. Numeric optimizations of Crowd-BT
often become trapped at suboptimal values because of the shallow
gradients, and although starting with randomly initialized values
of𝝇often results in similar solutions, we want to represent the
best outcome for Crowd-BT. We, therefore, initialize the values of 𝝇
using the BTL solution, the recommended best practice in Chen et
al. [11]. We set𝜂initial=0.9, which was found to be one of the best
positions using a line search over 𝜂initial∈{0.5,1}in steps of 0.01.
Because of the symmetry, judges with either 𝜂𝑖≥0.95or𝜂𝑖≤
0.05are𝑢𝑖∈D. The results in table 1 show that for the unpenalized
case,𝜆=0,{𝑢1,𝑢4}∈D , meaning that our least reliable judge is
now considered perfectly reliable, and the remaining judges are
thought to be random. In the penalized case, 𝜆=1,𝑢5joinsDbut
is now a completely adversarial judge, while the remaining judges
are considered mildly adversarial.
These results are disturbing because we know that the judges
are neither random nor adversarial: judges 𝑢𝑖>1agree with 𝒔★4
5’s
of the time. Clearly, the model does not reliably estimate the judge
weights.
4.3.2 O’Donovan et al.’s Method. For O’Donovan et al., it is not
necessary to start with the BTL solution, although doing so does not
change the result for the simulations we ran. We set 𝜇𝑖∈[− 10,10].
Similar to Crowd-BT, {𝑢2,𝑢3,𝑢5}are all deemed to be acting
randomly in the unpenalized case. 𝑢1, despite being the least reliable
judge, is estimated to be more reliable than the {𝑢2,𝑢3,𝑢5}because
𝑢1agrees with 𝑢4in one additional location; this agreement causes
𝑢4to become the dictator. The penalized case is worse because
{𝑢1,𝑢4}∈D, and, as in the Crowd-BT case, the remaining judges,
who agree amongst themselves, receive a negative weight, inverting
their decisions. As before, this is particularly disturbing because
the judges are not adversarial; they mostly agree with 𝒔★.
4.3.3 Discussion. The results show that both of these models can
be worse than merely unstable: an unfortunately placed inversion
can result in nearly any judge becoming a dictator, including the
 
646KDD ’24, August 25–29, 2024, Barcelona, Spain Andrew F. Dreher, Etienne Vogua, and Donald S. Fussell
least reliable one. This is possible despite starting with a favorable
initial position and can occur both with or without the virtual node
penalty of eq. (3). Sometimes, the penalty exacerbates the issue.
5DICTATORS IN O’DONOVAN ET AL .’S FONTS
In the previous sections, we demonstrated how both weighted
models can generate unreliable estimates of judge reliability when
provided with adversarially designed inputs. Adversarial inputs
are not, however, required for the formation of dictators. Here, we
show the formation of dictators in real-world data using the Fonts
dataset of O’Donovan et al. [33].
The Fonts dataset is a particularly useful dataset for understand-
ing scaling methods because of its size and quality. This dataset
consists of crowdsourced pairwise decisions on 200 fonts using 31
different attributes (e.g ., thin, graceful, charming). They collected
800 pairs for each attribute, with 8 individuals rating each pair,
yielding 4.02% of all possible pairs, which is dense for datasets of
this variety. Although the number of pairs is the same for every at-
tribute, the specific pairs selected are not. Importantly for studying
scaling methods, there are no disconnected stimuli for any attribute.
By collecting data on multiple attributes, the Fonts dataset captures
different distributions of opinions.
While we can evaluate the presence or absence of dictators, we
cannot evaluate the quality of the estimated scale or judge weights
since the true values are unknown. We will address this limitation
in section 6.
We implemented eqs. (1), (4) and (5) in Julia [ 4] using JuMP
[15] with IPOPT [ 47] as the optimizer. Details on reproduction are
available in appendix A and in the supplementary materials.
In O’Donovan et al., eq. (5) is expanded from a single latent scale
to simultaneously solving for a latent scale for each of the attributes
by sharing the judge weights; for results on Fonts, unless otherwise
noted, we use that same configuration. In fig. 2, we compare the
distribution of 𝝁for our implementation versus O’Donovan et al.’s
original results. We set 𝜇+=6.146since this matches the most
extreme value found in their results, and set 𝜆=0both to match
the original formulation and to remove any biasing effect of 𝜆on
the values of 𝝁.
x
−2−1 0 1 2 3 4 5 60%10%20%30%40%50%
Dictators ( ui∈D)
Judge Weight ( µi)Percent of JudgesOur Implementation
O’Donovan et al.’s Results
Figure 2: Difference is distribution of judge weights between
O’Donovan et al.’s results and our implementation.
As shown in fig. 2, while O’Donovan et al.’s original distribution
is quasi-normal, our result is bimodal. Our result for Fonts shows a
behavior similar to what we presented in section 4.3.2: there is a
group of dictators, 𝑢𝑖∈Dgiven𝜇+, while the remaining judges, 𝑢𝑖∉D,
are given near 0 weight. Despite the differences in the distributions,
our implementation fairly represents their method. In fact, ourimplementation yields a more optimal negative log-likelihood value
across all attributes: 110444.631 versus 110736.050, an improvement
of0.263%. Additional details are given in table 4, but this shows
that the dilemma of dictatorships is not confined to adversarially
constructed input.
Using Kendall’s 𝜏[23] normalized to[−1,1]to measure the
fit between the orders, we find, unsurprisingly, that our result
and O’Donovan et al.’s result is similar. Across all attributes, 𝜏∈
[0.981,1.000]; the full results are given in table 4. This result means
that the incorrect judge weights should not impact the conclusions
of works that use O’Donovan et al.’s published results, such as
[3, 27–29, 39, 50].
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10%10%20%30%40%50%60%70%80%
Judge Weight ( ηi)Percent of JudgesThin
Graceful
Figure 3: Distribution of judge weights using Crowd-BT for
graceful andthin.
Since the original version of Crowd-BT estimates a single latent
scale, we also solve for a latent scale for each attribute individually.
However, we use 𝜆=0to match O’Donovan et al.
The distributions of 𝜼computed using Crowd-BT on the at-
tributes graceful andthin are shown in fig. 3. We highlight these
attributes because most of the attributes in the Fonts dataset have a
vote distribution similar to graceful and thus have similar estimated
judge weights, while the attribute thin, presumably because it is
more closely aligned with a physical attribute, has the most skewed
distribution. The results show that for graceful, around 60%of the
total judges are part of the dictatorship, 𝑢𝑖∈D, but more than a
quarter of these are considered adversarial.
Using the estimated reliabilities for the judges who participate in
two attributes ( 𝑁=325forgraceful andthin), we can compute the
Pearson correlation coefficient. For Crowd-BT, this 𝜌𝜼=−0.014,
while for O’Donovan et al.’s method, solving each attribute indi-
vidually,𝜌𝝁=0.061. Across all pairs of attributes, though, we find
𝜌𝜼∈[− 0.188,0.261]and𝜌𝝁∈[− 0.135,0.207], suggesting at best
a small linear relationship and, generally, no correlation. These
results strengthen the doubt about the judge reliability estimates.
By itself, a bimodal distribution may be suspicious but not im-
plausible. Although the prevalence of inattention or adversarial
judges is beyond the scope of this work, there is a long-standing
interest in researching how attentive human subjects are, both
in a laboratory setting and online; more detail is available from
[12,16,18,37]. Nonetheless, Kittur et al. found that invalid re-
sponses from Amazon’s Mechanical Turk (MTurk) were largely
due to workers attempting to reduce the effort required and that
low-quality responses diminished greatly when the same effort was
required to correctly complete the task as to create a plausible but
incorrect response [ 25]. The large fraction of adversarial judges
 
647Estimated Judge Reliabilities for Weighted Bradley-Terry-Luce Are Not Reliable KDD ’24, August 25–29, 2024, Barcelona, Spain
created by Crowd-BT is unlikely since the effort required to cre-
ate an adversarial response here is equally difficult as to create a
valid one. However, because this is a binary choice, it is possible
that a fraction of judges confused the direction of the task. With
O’Donovan et al.’s method, our population could be composed of
a small fraction of diligent workers and a large fraction of low-
quality ones. Unfortunately, for both cases, we cannot evaluate the
reasonableness of the distributions from these data.
6 SIMULATION
We use simulated data to understand better why these weighted
methods form dictatorships and what impact the dictatorships may
have on the resulting scale. We create synthetic data modeled after
the Fonts dataset to enable comparisons with the results in section 5.
Parameter Values
Seeds 30 different seeds
Separations ( 𝜔) 0.02, 0.1, 0.5, and 1.0
Assignment Same and Convenience
Judges Per Pair 8, 16, 24, and 32
Pair Counts{100𝑥|𝑥∈[2,...,9]};
{1000𝑥|𝑥∈[1,...,19]};
and 19900
Table 2: Parameters used in modeling synthetic data
Matching O’Donovan et al., we use 200 stimuli (200 =|S|), but
we simplify the analysis by idealizing both the stimuli and the
judges. Our true order, 𝒔★, is the index position, and the expected
latent value is that index position multiplied by a value 𝜔repre-
senting the distance between neighbors: 𝜍★
𝑖=𝑖𝜔where𝑖∈[1,|S|].
This forces all neighboring stimuli 𝑠★
𝑖and𝑠★
𝑖+1to be𝜍★
𝑖+1−𝜍★
𝑖=𝜔.
To simulate dataset density, we vary the pair counts from 200,
|S|, to 19,900, |S|
2, representing 1% to 100% of possible pairs. Using
algorithm 1, we ensure a single connected component by choosing
the first|S|− 1pairs between neighbors of a shuffled list containing
all the stimuli. We add additional pairs randomly until all pairs are
present. This mechanism, a modified version of Erdős-Rényi, means
that every set works with a connected graph and consists of the
entire prior set. We used 30 different seeds to select the pairs in our
trials.
Similar to the stimuli, we model all of the judges as ideal,𝜂★
𝑖=
𝜇★
𝑖=1, meaning they behave exactly as assumed by BTL and mak-
ing eqs. (4) and (5) identical to eq. (1). With idealized stimuli and
judges, a pair is decided by sampling a random variable whose
probability is given by eq. (1). Concretely, if 𝜔=1.0, there is a 73.1%
chance a judge selects 𝑠★
𝑖+1≻𝑠★
𝑖. Importantly, the disagreements be-
tween judges are notdue to unreliability; instead, these differences
are due to following the Law of Comparative Judgment.
As in Fonts, a judge participates only once per pair for each
dataset, but the specific number of judges depends on how judges
are allocated pairs to rate. We consider two different options:
Same Assignment Every judge receives every pair. This is
the classical experimental design used by both Thurstone
[42,43] and Torgerson [ 45]. This is impractical for real-world
data because of the large number of decisions required, butAlgorithm 1: Pair Selection for a Single Seed
Input: The known, true order of stimuli, 𝒔★
The desired number of pairs, 𝑐
A pre-seeded pseudo-random generator, rng
Precondition:𝒔★≥2,𝑐≥𝒔★−1
Result: A set of pairsRwhere the stimuli in each pair are
ordered by 𝒔★
𝑛←|𝒔★| ▷Number of stimuli
R←{} ▷Set of pairs
𝒔rnd←shuffle 𝒔★,rng▷Permute 𝒔★
for𝑖=0,...,𝑛−1do
▷Order neighbors in 𝑠rndby their position in 𝒔★
(𝑠1,𝑠2)← order (𝒔rnd[𝑖],𝒔rnd[𝑖+1]),𝒔★
R←R∪(𝑠1,𝑠2)
end
▷Rcontains pairs defining a single connected component
if𝑐≥𝑛then
𝒅←[] ▷Candidates
for𝑖=0,...,𝑛−2,𝑗=𝑖+1,...,𝑛−1do
(𝑠1,𝑠2)← order (𝒔rnd[𝑖],𝒔rnd[𝑗]),𝒔★
𝒅←𝒅∪(𝑠1,𝑠2)
end
▷R∪𝒅contain all pairs
𝒅←shuffle(𝒅,rng) ▷Permute 𝒅
R←R∪ 𝒅[0,...,𝑐−𝑛−1]
end
returnR
since these judges are simulated, this serves as our theoretical
baseline. Algorithm 2 generates decisions using the same
assignment.
Convenience Assignment Each judge rates a subset of the
total pairs, mimicking crowdsourcing. We assume that judges
are free to take as many tasks as they want, subject to
some limits. We fit a gamma distribution of the number
of pairs rated by each judge in the Fonts dataset using Scikit
Learn[ 36], where the best fit was found to be 𝐶∼Γ(𝛼=
0.3655,𝜃=834.8795)+ 4.
Because we know 𝒔★,𝝇★,𝜼★, and 𝝁★, we can use standard mea-
surement methods for evaluating the quality of ˆ𝒔. As with the Fonts
results, use Kendall’s 𝜏normalized to[−1,1]to measure the quality
of the final ordering.
6.1 Results
For our simulated data, we say a dictatorship forms when |D|>0,
where the conditions for 𝑢𝑖∈D are given in section 4.3.
Crowd-BT always resulted in |D|>0. This is not surprising
since this method tends to estimate that a significant of the judges
are very reliable 𝜂𝑖≈1. In 96.973% of the trials with the same
assignment,|D|=|𝒖|, while it fell to 62.793% when using the
convenience assignment. However, the convenience assignment still
displays a large fraction of 𝑢𝑖∈D;98.794% of settings result in at
least 80% of the judges in D.
 
648KDD ’24, August 25–29, 2024, Barcelona, Spain Andrew F. Dreher, Etienne Vogua, and Donald S. Fussell
The results where|D|
|𝒖|≤0.4is mostly for the same assignment
and always for a low number of pairs ( ≤500). This is likely similar
behavior to section 4.3.
The results for O’Donovan et al.’s method are more complex.
We show the percentage of seeds leading to a dictatorship for the
different parameters in fig. 4. In fig. 4, the top graph uses the same
assignment, and the bottom graph uses the convenience assignment.
No combination of parameters resulted in |D|=|𝒖|.
0%20%40%60%80%100%Sets where |D|>0
Same Assignment
1031040%20%40%60%80%100%
Number of Pairs RatedSets where |D|>0
Convenience Assignmentω Judges
0.02 8
0.1 16
0.5 24
1.0 32
Figure 4: Percentage of sets with a dictatorship versus the
number of pairs using eq. (5).
Importantly, the decisions used are identical ; the only differ-
ence is the judge credited with making the decision. The standard
implementation of eq. (1) will result in identical orders for both.
It is not possible to completely avoid the formation of dictator-
ships, even when using same sampling. However, we note that the
number of pairs at which a dictatorship becomes unlikely depends
on𝜔. This suggests that the formation of dictatorships is largely
due to the disagreement rate between judges and their overlap. A
closer spacing, such as 𝜔=0.02, has more disagreements; thus, the
cutoff point is earlier than for a wider spacing, such as 𝜔=1.0.
This assignment pattern generally yields a positive correlation be-
tween the known and estimated reliability values but would quickly
become cost-prohibitive for real-world applications.
With a crowdsourced approach to data collection, the formation
of dictatorships is almost guaranteed for O’Donovan et al.’s method
regardless of the amount of data collected – and even with ideal
judges. Unfortunately, the only reasonable way to collect data at
this scale is to rely on crowdsourcing platforms such as MTurk,
yet sourcing data from online platforms, where human judges are
incentivized to complete as many responses as possible, makes
estimating judge weights more attractive because the incentives
run counter to high-quality data collection.
We suspect that the formation of the dictatorships is due to the
phenomena discussed in sections 3 and 4.3. Unlike the case of the
same assignment, with the convenience assignment, judges typically
only overlap on a few pairs. This frees the optimizer to select a
subset of judges that it can call dictators in a process that is concep-
tually similar, but not identical, to set coverage since the optimizer
can ignore a pair without penalty when a non-dictator agrees with
a dictator. We believe this occurs because judges who agree can begiven a weight close to 0 without affecting the performance. This
is likely the reason why the distribution of fig. 4 has so many 0
weight judges.
Additionally, we found that |D|is often undercounted because
there is no need to assign a judge who agrees with a dictator any
particular weight – these can be regarded as “don’t care” values.
10310400.20.40.60.81
Number of Rated PairsNormalized τ(s,s⋆)∈[−1,1]
ω Method
0.02 BTL
0.1 O’Donovan
0.5 Crowd-BT
1.0
Figure 5:𝜏for simulated ideal judges using the convenience
sample.
The value of 𝜏for different settings is shown in fig. 5. Given
the same input data, all of the methods converge to a similar order
fairly quickly, although the convergence rate depends largely on 𝜔,
regardless of the method employed. This is unsurprising because
the judges are perfect.
One interesting aspect is that 𝜏for the weighted methods is at
least as good as that of BTL for the trials with very sparse data
(the far left of fig. 5). This is an important region because collecting
human responses is expensive. Thus, although O’Donovan et al.’s
solution is not as optimal as ours, the estimated 𝝇are similar, which
we believe is a significant factor in why the instability of eq. (5).
7 FUTURE WORK
The slight differences in the orders for the weighted methods and
BTL do not demonstrate their robustness but do ameliorate con-
cerns around research currently based on these results. Future work
should explore models of unreliable judges to establish when using
these methods is acceptable. We conducted some preliminary re-
search with the simulated judges that showed the methods could
produce mild correlated values, but more work is required to define
a robust measure.
Active sampling or learning seems to be the most compelling
application if the judge’s reliabilities could be dependably estimated.
Indeed, this is one of Chen et al.’s [11] primary goals, and Pfeiffer et
al. [38] use it for crowd estimates. Works to consider for extension
include Jamieson and Nowak [ 22] and Ailon [ 2], who show opti-
mal methods for selecting pairs to rank; and Glickman and Jensen
[17], Oh et al. [35], and Gunasekar et al. [19] who show various
imputation methods.
Whether or not it is possible to obtain dependable estimates
solely from pairwise data is likewise future work. Estimating both
the reliability of a judge and the difficulty of the decisions are
complementary aspects, but the interplay between them creates a
feedback loop that breaks the current methods.
Potentially productive avenues for addressing this problem lie
in taking a more Bayesian approach to the weight updates since
 
649Estimated Judge Reliabilities for Weighted Bradley-Terry-Luce Are Not Reliable KDD ’24, August 25–29, 2024, Barcelona, Spain
current methods freely change the estimates with little data and
in augmenting the pairwise data to account for more than just di-
rection. Arresti [ 1] and Bockenholt and Dillon [ 5] both incorporate
multiple levels, similar to collecting responses on a Likert scale
[30], to generate a higher quality estimate of the latent scale values.
Other potential extensions include mixing in complementary data,
such as Oh and Shah [ 34], who mix the pairwise preferences with
ordinal data, or Tran et al. [46], who have a flexible architecture
that can learn a latent scale from a wide variety of data. Extending
reliability to account for schools of thought, such as in Kim et al.
[24] or Tian and Zhu [44], may also be helpful.
8 CONCLUSIONS
Measuring human preferences has been a bedrock of psychometrics
since the development of Thrustone’s models. While the era of on-
line crowdsourcing vastly expands the data’s size and complexity, it
comes with a price: increasingly unreliable judges whose incentives
may be unaligned with the investigators’.
We have shown two natural and principled extensions of the
BTL model to account for unreliable participants. Crowd-BT at-
tributes this unreliability to a fixed stochastic process, which, while
simple and sometimes quite effective, does not align well with the
principles of Thurstone’s latent model. O’Donovan et al.’s model
matches Thurstone’s latent model by adjusting the variance of the
assumed latent random variables from case V. Nonetheless, despite
their differences, neither generates trustworthy estimates of the
judges’ reliabilities.
ACKNOWLEDGMENTS
We thank the UT Graphics Lab, particularly Sarah Abraham, for
their helpful feedback through the many revisions of this work and
the anonymous reviewers for their critical feedback. The contribu-
tions of both undoubtedly improved the narrative.
We thank Peter O’Donovan and Aaron Hertzmann for sharing
their code, which immensely helped us to understand the differ-
ences in our results.
REFERENCES
[1]Alan Agresti. 1988. A Model for Agreement Between Ratings on an Ordinal Scale.
44, 2 (June 1988), 539–548. https://doi.org/10.2307/2531866
[2]Nir Ailon. 2012. An Active Learning Algorithm for Ranking from Pairwise
Preferences with an Almost Optimal Query Complexity. Journal of Machine
Learning Research 13, 1 (2012), 137–164. https://doi.org/10.5555/2503308.2188390
[3]Kenneth C Arnold and Krzysztof Z Gajos. 2015. Effective interactions for per-
sonalizing spatial visualizations of collections. In Adjunct Proceedings of the 28th
Annual ACM Symposium on User Interface Software & Technology (Charlotte, NC,
USA) (UIST ’15). ACM, New York, NY, USA, 77–78.
[4]Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. 2017. Julia:
A fresh approach to numerical computing. SIAM review 59, 1 (2017), 65–98.
https://doi.org/10.1137/141000671
[5]Ulf Böckenholt and William R Dillon. 1997. Modeling within-subject dependen-
cies in ordinal paired comparison data. Psychometrika 62, 3 (1997), 411–434.
[6]Ralph Allan Bradley and Milton E Terry. 1952. Rank Analysis of Incomplete
Block Designs: I. The Method of Paired Comparisons. Biometrika 39, 3/4 (1952),
324–345. https://doi.org/10.2307/2334029
[7]Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton,
and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings
of the 22nd International Conference on Machine learning (Bonn, Germany) (ICML
’05). ACM, New York, NY, USA, 89–96.
[8]Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An
overview. Learning 11, 23-581 (2010), 81.
[9]Ben Carterette and Desislava Petkova. 2006. Learning a ranking from pairwise
preferences. In Proceedings of the 29th Annual International ACM SIGIR Conferenceon Research and Development in Information Retrieval (Seattle, WA, USA) (SIGIR
’06). ACM, New York, NY, USA, 629–630. https://doi.org/10/cq4x92
[10] Manuela Cattelan. 2012. Models for paired comparison data: A review with
emphasis on dependent data. Statist. Sci. 27, 3 (8 2012), 412–433. https://doi.org/
10.1214/12-STS396
[11] Xi Chen, Paul N Bennett, Kevyn Collins-Thompson, and Eric Horvitz. 2013.
Pairwise ranking aggregation in a crowdsourced setting. In Proceedings of the
sixth ACM International Conference on Web search and Data Mining (Rome, Italy)
(WDSM ’13). ACM, New York, NY, USA, 193–202. https://doi.org/10.1145/2433396.
2433420
[12] Alexander Coppock. 2019. Generalizing from Survey Experiments Conducted on
Mechanical Turk: A Replication Approach. Political Science Research and Methods
7, 3 (7 2019), 613–628. https://doi.org/10.1017/psrm.2018.10
[13] Yue Cui, Liwei Deng, Yan Zhao, Bin Yao, Vincent W. Zheng, and Kai Zheng.
2019. Hidden POI Ranking with Spatial Crowdsourcing. In Proceedings of the
25th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (Anchorage, AK, USA). ACM, New York, NY, USA, 814–824. https:
//doi.org/10.1145/3292500.3330844
[14] Julie S Downs, Mandy B Holbrook, Steve Sheng, and Lorrie Faith Cranor. 2010.
Are your participants gaming the system? Screening Mechanical Turk workers.
InProceedings of the SIGCHI Conference on Human Factors in Computing Systems.
2399–2402.
[15] Iain Dunning, Joey Huchette, and Miles Lubin. 2017. JuMP: A Modeling Language
for Mathematical Optimization. SIAM review 59, 2 (2017), 295–320. https:
//doi.org/10.1137/15M1020575
[16] John B. Ford. 2017. Amazon’s Mechanical Turk: A Comment. Journal of Ad-
vertising 46, 1 (2017), 156–158. https://doi.org/10.1080/00913367.2016.1277380
arXiv:https://doi.org/10.1080/00913367.2016.1277380
[17] Mark E. Glickman and Shane T. Jensen. 2005. Adaptive paired comparison
design. Journal of Statistical Planning and Inference 127, 1 (2005), 279–293. https:
//doi.org/10.1016/j.jspi.2003.09.022
[18] Joseph K Goodman, Cynthia E Cryder, and Amar Cheema. 2013. Data collection in
a flat world: The strengths and weaknesses of Mechanical Turk samples. Journal
of Behavioral Decision Making 26, 3 (2013), 213–224.
[19] Suriya Gunasekar, Oluwasanmi O Koyejo, and Joydeep Ghosh. 2016. Preference
Completion from Partial Rankings. In Advances in Neural Information Processing
Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (Eds.), Vol. 29.
Curran Associates, Inc., 1370–1378. http://papers.nips.cc/paper/6272-preference-
completion-from-partial-rankings
[20] Ian Hamilton, Nick Tawn, and David Firth. 2023. The many routes to the ubiqui-
tous Bradley-Terry model. arXiv:2312.13619 [math.ST]
[21] Wolfram Research, Inc. 2022. Mathematica, Version 12.2.0.0. https://www.
wolfram.com/mathematica
[22] Kevin G Jamieson and Robert Nowak. 2011. Active Ranking using Pairwise
Comparisons. In Advances in Neural Information Processing Systems, J. Shawe-
Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger (Eds.), Vol. 24.
Curran Associates, Inc., 2240–2248. http://papers.nips.cc/paper/4427-active-
ranking-using-pairwise-comparisons
[23] M G Kendall. 1938. A New Measure of Rank Correlation. 30, 1/2 (Jan. 1938), pp.
81–93. http://www.jstor.org/stable/2332226
[24] Kun Ho Kim, Oisin Mac Aodha, and Pietro Perona. 2018. Context Embedding
Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR ’18). IEEE, 8679–8687. arXiv:1710.01691v3 [cs.LG] http://arxiv.
org/abs/1710.01691v3
[25] Aniket Kittur, Ed H Chi, and Bongwon Suh. 2008. Crowdsourcing user studies
with Mechanical Turk. In Proceedings of the SIGCHI Conference on Human Factors
in Computing Systems (Florence, Italy) (CHI ’08). ACM, New York, NY, USA,
453–456. https://doi.org/10.1145/1357054.1357127
[26] Yuki Koyama, Issei Sato, Daisuke Sakamoto, and Takeo Igarashi. 2017. Sequential
line search for efficient visual design optimization by crowds. ACM Transactions
on Graphics (TOG) 36, 4 (2017), 1–11.
[27] Tugba Kulahcioglu and Gerard De Melo. 2018. Fontlex: A typographical lexi-
con based on affective associations. In Proceedings of the Eleventh International
Conference on Language Resources and Evaluation (Miyazaki, Japan) (LREC ’18).
European Language Resources Association (ELRA).
[28] Tugba Kulahcioglu and Gerard De Melo. 2018. Predicting semantic signatures of
fonts. In 2018 IEEE 12th International Conference on Semantic Computing (ICSC).
IEEE, 115–122.
[29] Tugba Kulahcioglu and Gerard De Melo. 2020. Affect-Aware Word Clouds. ACM
Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1–25.
[30] Rensis Likert. 1932. A technique for the measurement of attitudes. Archives of
psychology (1932).
[31] R Duncan Luce. 1959. Individual choice behavior: A theoretical analysis . John
Wiley & Sons, Inc. https://doi.org/10.1037/14396-000
[32] David Mease. 2003. A penalized maximum likelihood approach for the ranking of
college football teams independent of victory margins. The American Statistician
57, 4 (2003), 241–248. https://doi.org/10.1198/0003130032396
 
650KDD ’24, August 25–29, 2024, Barcelona, Spain Andrew F. Dreher, Etienne Vogua, and Donald S. Fussell
[33] Peter O’Donovan, J ¯anis L ¯ıbeks, Aseem Agarwala, and Aaron Hertzmann. 2014.
Exploratory font selection using crowdsourced attributes. ACM Transactions on
Graphics (TOG) 33, 4 (7 2014), 1–9. https://doi.org/10.1145/2601097.2601110
[34] Sewoong Oh and Devavrat Shah. 2014. Learning Mixed Multinomial Logit
Model from Ordinal Data. In Advances in Neural Information Processing Systems,
Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (Eds.),
Vol. 27. Curran Associates, Inc. http://papers.nips.cc/paper/5225-learning-mixed-
multinomial-logit-model-from-ordinal-data
[35] Sewoong Oh, Kiran K Thekumparampil, and Jiaming Xu. 2015. Collaboratively
Learning Preferences from Ordinal Data. In Advances in Neural Information
Processing Systems, C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett
(Eds.), Vol. 28. Curran Associates, Inc., 1909–1917. http://papers.nips.cc/paper/
5818-collaboratively-learning-preferences-from-ordinal-data
[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[37] Eyal Peer, Joachim Vosgerau, and Alessandro Acquisti. 2014. Reputation as a
sufficient condition for data quality on Amazon Mechanical Turk. Behavior
research methods 46, 4 (12 2014), 1023–1031.
[38] Thomas Pfeiffer, Xi Gao, Yiling Chen, Andrew Mao, and David Rand. 2012. Adap-
tive polling for information aggregation. In Proceedings of the Twenty-Sixth AAAI
Conference on Artificial Intelligence (Toronto, Ontario, Canada) (AAAI’12, Vol. 26).
AAAI Press, 122–128. https://doi.org/10.5555/2900728.2900746
[39] S Rakshith, Rishabh Khurana, Vibhav Agarwal, Jayesh Rajkumar Vachhani, and
Guggilla Bhanodai. 2021. Fontnet: On-Device Font Understanding and Prediction
Pipeline. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP). IEEE, 2155–2159.
[40] Eric L Slattery, Courtney CJ Voelker, Brian Nussenbaum, Jason T Rich, Randal C
Paniello, and J Gail Neely. 2011. A practical guide to surveys and questionnaires.
Otolaryngology–Head and Neck Surgery 144, 6 (2011), 831–837.
[41] Scott M. Smith, Catherine A. Roster, Linda L. Golden, and Gerald S. Albaum. 2016.
A multi-group analysis of online survey respondent data quality: Comparing a
regular USA consumer panel to MTurk samples. Journal of Business Research 69,
8 (2016), 3139–3148. https://doi.org/10.1016/j.jbusres.2015.12.002
[42] Louis L Thurstone. 1927. A law of comparative judgment. Psychological review
34, 4 (1927), 273.
[43] Louis L Thurstone. 1928. Attitudes can be measured. American journal of Sociology
33, 4 (1928), 529–554.
[44] Yuandong Tian and Jun Zhu. 2012. Learning from crowds in the presence of
schools of thought. In Proceedings of the 18th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining. 226–234. https://doi.org/10.1145/
2339530.2339571
[45] Warren S Torgerson. 1958. Theory and methods of scaling. Wiley. https://doi.
org/10.2307/2333553
[46] Truyen Tran, Dinh Phung, and Svetha Venkatesh. 2013. Thurstonian Boltzmann
Machines: Learning from Multiple Inequalities. In Proceedings of the 30th Interna-
tional Conference on Machine Learning (Proceedings of Machine Learning Research,
Vol. 28), Sanjoy Dasgupta and David McAllester (Eds.). PMLR, Atlanta, Georgia,
USA, 46–54. https://proceedings.mlr.press/v28/tran13.html
[47] Andreas Wächter and Lorenz T Biegler. 2006. On the implementation of an
interior-point filter line-search algorithm for large-scale nonlinear programming.
Mathematical programming 106, 1 (3 2006), 25–57.
[48] Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R Movellan, and Paul L
Ruvolo. 2009. Whose vote should count more: Optimal integration of labels from
labelers of unknown expertise. Advances in neural information processing systems
22 (2009), 2035–2043. http://papers.nips.cc/paper/3644-whose-vote-should-
count-more-optimal-integration-of-labels-from-labelers-of-unknown-expe
[49] Edward W Wolfe. 2004. Identifying rater effects using latent trait models. Psy-
chology Science 46 (2004), 35–51.
[50] Chang Xiao, Cheng Zhang, and Changxi Zheng. 2018. Fontcode: Embedding
information in text documents using glyph perturbation. ACM Transactions on
Graphics (TOG) 37, 2 (2018), 1–16.
[51] Ernst Zermelo. 1929. Die Berechnung der Turnier-Ergebnisse als ein Maxi-
mumproblem der Wahrscheinlichkeitsrechnung. Mathematische Zeitschrift 29, 1
(1929), 436–460. https://doi.org/10.1007/BF01180541
[52] Guoxi Zhang and Hisashi Kashima. 2023. Batch Reinforcement Learning
from Crowds. In Machine Learning and Knowledge Discovery in Databases, Massih-
Reza Amini, Stéphane Canu, Asja Fischer, Tias Guns, Petra Kralj Novak, and
Grigorios Tsoumakas (Eds.). Springer Nature Switzerland, Cham, 38–51.
[53] Xiaohang Zhang, Guoliang Li, and Jianhua Feng. 2016. Crowdsourced top-k
algorithms: An experimental evaluation. Proceedings of the VLDB Endowment 9,
8 (2016), 612–623.
A REPRODUCTION
Working demonstrations of the examples are available in the ac-
companying Mathematica Notebook and Julia code.We used Julia 1.7.2 running on either MacOS 11.7.8 (20G1351) for
x86 or Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-76-generic x86_64).
The specific package versions are provided in the README file.
A.1 Creating Dictators
All of the results use 𝝇∈[− 100,100], where the initial positions are
randomly assigned. For Chen et al.’s Crowd-BT, we use 𝜼initial=0.9,
as in section 4.3.1; for O’Donovan et al.’s method, we set 𝝁initial=
1.0and𝜇+=10.
BTL Crowd-BT O’Donovan et al.
n𝑖 𝜍𝑖𝜍𝑖𝜂𝑖𝜍𝑖𝜇𝑖
41 -1.707 -7.282 1.000 -100.0 0.0321
2 -0.609 -6.588 1.000 -99.361 0.0321
3 0.490 -5.895 0.500 -98.722 10.000
4 1.589 9.484 1.000 -99.363 -0.0321
5 2.687 10.177 99.999
51 -3.021 -15.940 0.722 -1.844 1.879
2 -1.634 0.728 1.000 -1.106 1.879
3 -0.248 1.996 1.000 -0.368 1.879
4 1.138 3.264 1.000 0.369 1.879
5 2.525 4.532 1.000 1.107 1.879
6 3.911 5.800 1.845
Table 3: Numeric results for the symmetric problem in sec-
tion 4 with 𝜆=0. Members of the dictatorship, 𝑢𝑖∈D, are
shown in bold.
The results for 𝑛=4and𝑛=5are shown in table 3. As expected
from section 4, Crowd-BT generates a dictatorship for both, while
O’Donovan et al.’s method only generates the dictatorship for the
𝑛=4example. For 𝑛=4, neither weighted method reproduces the
𝒔★order, yet when 𝑛=5, all of the methods produce the 𝒔★order,
including Crowd-BT, despite the dictatorship.
A.2 Fonts
Our provided experimental code can be run using the raw data
available from O’Donovan et al. [33]. Unfortunately, we cannot
provide their code, which was used to reproduce their results in
fig. 2 and table 4; however, all of our results are available.
4/4 5/3 6/2 7/1 8/00%12.5%25%37.5%50%62.5%75%
Decision lopsidedness (For/Against Majority Decision)Percentage of PairsThin
Graceful
Others
Figure 6: Vote distribution for O’Donovan et al .’s Fonts
dataset, emphasizing graceful andthin.
 
651Estimated Judge Reliabilities for Weighted Bradley-Terry-Luce Are Not Reliable KDD ’24, August 25–29, 2024, Barcelona, Spain
Algorithm 2: Pair Assignment (Same)
Input: The scaling constant 𝜔
The set of pairsR, such as from algorithm 1
The list of judges 𝒖
Precondition: 𝜔>0,|R|≥1,|𝒖|≥1
Result: A set of attributed decisions V𝑠where the tuples are
(𝑢,𝑠1,𝑠2,choice).
V𝑠←{} ▷Set of decisions
for(𝑠1,𝑠2)∈Rdo
𝑝←1
1+𝑒𝜔(𝜍1−𝜍2)▷Probability from eq. (1)
for𝑢∈udo
𝑏←uniform(0,1,rng)≤𝑝 ▷𝑢’s choice
V𝑠←V𝑠∪(𝑢,𝑠1,𝑠2,𝑏)
end
end
returnV𝑠
Algorithm 3: Convenience Pair Counts
Input: The number of decisions 𝑛𝑣=|V𝑠|from algorithm 2
The function 𝑓returning a random number
The number of judges per pair, 𝑛
A minimum number of pairs per judge, 𝑐𝑚
Precondition: 𝑛𝑣>0,𝑛>0,𝑛𝑣%𝑛=0,min(𝑓)≥𝑐𝑚
Result: Requested decision counts for each judge
𝒂←[] ▷List of requested pairs
whileÍ𝒂<𝑛𝑣do ▷Draw samples
𝒂←𝒂∪sample(𝑓,rng)
end
whileÍ𝒂>𝑛𝑣do ▷Randomly remove pairs
𝒊←indices(𝒂>𝑐𝑚) ▷Available indices
𝑠←choose(𝒊,rng) ▷Random selection
𝒂[𝑠]←𝒂[𝑠]−1 ▷Adjusted value
end
return 𝒂
Algorithm 4: generate_mappings (V𝑠)
Input: DecisionsV𝑠from algorithm 2
Result: A mapCfrom pairs of stimuli to an index value
A list 𝒄from index value to a pairs of stimuli
A mapBfrom index value to list of decisions made for that
pair inV𝑠
C←{:} ▷Map from pair to index
𝒄←[] ▷List of known pairs
B←{:} ▷Map from pairs to available choices
for(𝑢,𝑠1,𝑠2,𝑏)∈V𝑠do
if(𝑠1,𝑠2)∉Cthen ▷Unseen pair
𝒄←𝒄∪(𝑠1,𝑠2)
C[(𝑠1,𝑠2)]←|𝒄|
end
𝑖←C[(𝑠1,𝑠2)]
B[𝑖]←B[𝑖]∪𝑏
end
return(C,𝒄,B)Algorithm 5: Convenience Pair Assignment
Input: DecisionsV𝑠from algorithm 2
Requested decision counts for each judge, 𝒂
The number of judges per pair, 𝑛
Precondition: 𝑛𝑣>0,𝑛>0,𝑛𝑣%𝑛=0,min(𝑓)≥𝑐𝑚
Result: Set of attributed decisions V𝑐
𝑐𝑣←|V𝑠|/𝑛 ▷Number of pairs
(C,𝒄,B)←generate_mappings (V𝑠) ▷algorithm 4
𝒑←shuffle(repeat([1,...,𝑐𝑣],𝑛)) ▷Pair order†
D←{:} ▷Pairs assigned to each judge
𝒖𝑎←[1,...,|𝒂|] ▷Available judges
for𝑝∈𝒑do
U𝑐←{𝑢|𝑝∉D[𝑢],𝑢∈𝒖𝑎}
▷U𝑐contains the available judges who have not rated 𝑝
ifU𝑐≠∅then
𝑢←choose(U𝑐,rng) ▷Random judge
D[𝑢]←D[𝑢]∪𝑝 ▷Assign𝑝to𝑢
assigned←𝑢
else ▷Need a donor
𝒖𝑑←[𝑢|𝑝∉D[𝑢],𝑢∈|𝒂|]
▷𝒖𝑑contains alljudges who have not rated 𝑝, ignoring
assignment count limits
for𝑢𝑑∈shuffle(U𝑑,rng)do
for𝑢𝑠∈shuffle(𝒂,rng)do
▷𝑢𝑠has space but also p, otherwise U𝑐≠∅
Pdiff←D[𝑢𝑑]\D[𝑢𝑠]
ifPdiff≠∅then ▷Perform a swap
𝑡←choose(Pdiff,rng) ▷Get a pair
D[𝑢𝑠]←D[𝑢𝑠]∪𝑡
D[𝑢𝑑]←D[𝑢𝑑]\𝑡
D[𝑢𝑑]←D[𝑢𝑑]∪𝑝
assigned←𝑢
break
end
end
end
end
𝒂[assigned]←𝒂[assigned]−1 ▷Decrement count
if𝒂[assigned]=0then
𝒖𝑎←𝒖𝑎\assigned ▷Remove from available
end
end
for(𝑢,𝑝)∈D do
𝑏←choose(B[𝑝],rng) ▷Get an available choice
(𝑠1,𝑠2)←𝒄[𝑝]
V𝑐←V𝑐∪(𝑢,𝑠1,𝑠2,𝑏)
end
returnV𝑐
4/4 5/3 6/2 7/1 8/00%12.5%25%37.5%50%62.5%75%87.5%100%
Decision lopsidedness (For/Against Majority Decision)Percentage of PairsAttribute ω
Thin 0.02
Graceful 0.1
Wide 0.5
1.0
Figure 7: Simulated vote distribution with different 𝜔.
 
652KDD ’24, August 25–29, 2024, Barcelona, Spain Andrew F. Dreher, Etienne Vogua, and Donald S. Fussell
Attribute BTL♣O’Donovan et al.♦𝜇+=1𝜇+=6.146♠𝜇+=10𝜇+=50𝜏(♣,♦)𝜏(♣,♠)𝜏(♦,♠)
angular 4113.183 4063.436 4055.213 4050.547 4049.944 4048.947 0.965 0.955 0.990
artistic 3400.394 3257.784 3255.679 3252.307 3251.801 3251.308 0.991 0.988 0.998
attention-grabbing 3760.494 3668.454 3660.466 3655.349 3654.748 3653.760 0.986 0.981 0.997
attractive 3769.857 3706.579 3700.530 3694.149 3693.448 3692.350 0.984 0.978 0.996
bad 3763.270 3645.027 3638.933 3635.154 3634.735 3634.054 0.983 0.982 0.999
boring 4111.513 4073.104 4072.196 4066.413 4065.567 4064.033 0.967 0.942 0.981
calm 3620.193 3509.986 3504.300 3501.625 3501.367 3500.327 0.986 0.985 0.998
charming 3811.745 3752.271 3747.013 3742.186 3741.542 3740.353 0.982 0.978 0.997
clumsy 3688.072 3605.140 3599.532 3597.491 3597.248 3596.822 0.990 0.989 1.000
complex 3474.487 3330.210 3324.369 3321.896 3321.559 3320.825 0.989 0.987 0.999
delicate 4026.085 3986.783 3976.614 3969.515 3968.870 3967.333 0.980 0.976 0.996
disorderly 3333.739 3244.929 3242.948 3240.329 3240.054 3239.629 0.993 0.992 0.999
dramatic 3639.096 3515.371 3510.593 3508.690 3508.284 3507.561 0.986 0.982 0.998
formal 3716.860 3685.327 3685.238 3680.311 3679.638 3678.524 0.988 0.982 0.995
fresh 3998.783 3962.259 3960.254 3951.363 3950.070 3947.825 0.964 0.950 0.991
friendly 3989.303 3977.773 3974.413 3967.126 3966.131 3964.415 0.983 0.969 0.991
gentle 3870.322 3843.637 3843.367 3839.002 3838.411 3836.986 0.983 0.973 0.994
graceful 3981.986 3958.618 3948.251 3942.560 3941.857 3940.219 0.976 0.970 0.996
happy 3949.384 3910.268 3906.382 3901.874 3901.277 3900.249 0.980 0.971 0.994
legible 3773.662 3712.765 3703.985 3701.368 3701.262 3701.189 0.988 0.985 0.998
modern 3944.242 3848.250 3846.132 3839.929 3839.085 3837.548 0.980 0.970 0.994
playful 3568.848 3436.434 3424.911 3421.732 3421.494 3421.085 0.989 0.988 0.999
pretentious 3967.606 3909.588 3902.277 3898.244 3897.732 3896.865 0.982 0.979 0.997
sharp 4155.955 4154.306 4146.931 4139.544 4138.792 4137.617 0.973 0.963 0.991
sloppy 3638.609 3577.854 3569.408 3566.919 3566.693 3566.252 0.986 0.986 0.999
soft 3300.035 3205.419 3198.487 3196.841 3196.717 3196.611 0.986 0.985 1.000
strong 2481.652 2252.668 2249.253 2247.423 2247.038 2246.327 0.991 0.990 1.000
technical 4112.975 4058.410 4058.128 4051.718 4050.613 4048.622 0.974 0.951 0.981
thin 1288.462 1155.082 1164.541 1146.626 1143.395 1137.170 0.875 0.872 0.995
warm 4034.202 4030.800 4027.994 4022.051 4021.285 4020.019 0.969 0.948 0.988
wide 2771.438 2697.518 2694.862 2694.349 2694.191 2694.140 0.865 0.928 0.988
Total 113056.450 110736.050 110593.199 110444.631 110424.847 110388.964
% Difference with♦ 1.037% – -0.129% -0.263% -0.281% -0.313%
Table 4: Comparison of our negative log-likelihood results for various limits against those of O’Donovan et al. [33]. Lower
values indicate a better fit. The best NLL values are shown in bold, while the worst are shown in italics.
In table 4,we show that despite producing very different weight
estimates for the judges, our implementation is a fair implemen-
tation of eq. (5). While larger values of 𝜇𝑖improve the negative
log-likelihood of the result, even when provided with less range,
such as𝜇+=1, our version generally finds a better fit of the input
data than the original implementation.
For our code, we set 𝜆=0,𝜍𝑖∈[− 100,100], the stopping toler-
ance to 1e−6, and each judge’s starting weight to 𝝁initial=1.0.
Figure 6 shows the vote distribution for the Fonts dataset. We
highlighted graceful in section 5 and particularly in fig. 3 because it
displays a distribution similar to many of the attributes, while thin
serves as a marked contrast.
A.3 Simulation
A synthetic dataset with many of the same characteristics as Fonts
but with a simple, idealized generation model is needed to under-
stand the behavior in section 5. Here, we sketch the algorithms and
justify the𝜔parameter.A.3.1 Separation ( 𝜔).The separation parameter 𝜔in table 2 tunes
the difficulty of the decisions. It does not account for any potential
differences in concept, such as whether graceful means something
akin to either italic or script. Nonetheless, in fig. 7, we show that
different values of 𝜔are reasonable, if imperfect, proxies of many
attributes in fig. 6. Although we chose the 𝜔values independently
and did not attempt to find values using the Fonts data, we found
that𝜔=0.02approximates attributes like wide while𝜔=0.1
approximates thin.
A.3.2 Creating the data. As described in section 6, we start by
generating a set of pairs using algorithm 1. We then create thesame
assignment using algorithm 2. To simulate crowdsourcing (conve-
nience assignment), we simulate judges drawing work from the
queue (algorithm 3) and then map judges to pairs such that ev-
ery judge rates the number assigned to them while respecting the
required number of judges per pair (algorithm 5). Bookkeeping
(algorithm 4) ensures that the votes in V𝑐are the same asV𝑠.
 
653