Multimodal Pretraining, Adaptation, and Generation
for Recommendation: A Survey
Qijiong Liu∗†
The HK PolyU
Hong Kong, China
liu@qijiong.workJieming Zhu∗
Huawei Noah’s Ark Lab
Shenzhen, China
jiemingzhu@ieee.orgYanting Yang†
Zhejiang Unversity
Hangzhou, China
yantingyang@zju.edu.cn
Quanyu Dai
Huawei Noah’s Ark Lab
Shenzhen, China
daiquanyu@huawei.comZhaocheng Du
Huawei Noah’s Ark Lab
Shenzhen, China
zhaochengdu@huawei.comXiao-Ming Wu
The HK PolyU
Hong Kong, China
xiao-ming.wu@polyu.edu.hk
Zhou Zhao
Zhejiang Unversity
Hangzhou, China
zhaozhou@zju.edu.cnRui Zhang
Huazhong University of Science and
Technology, China
www.ruizhang.info
rayteam@yeah.netZhenhua Dong
Huawei Noah’s Ark Lab
Shenzhen, China
dongzhenhua@huawei.com
ABSTRACT
Personalized recommendation serves as a ubiquitous channel for
users to discover information tailored to their interests. However,
traditional recommendation models primarily rely on unique IDs
and categorical features for user-item matching, potentially over-
looking the nuanced essence of raw item contents across multiple
modalities such as text, image, audio, and video. This underutiliza-
tion of multimodal data poses a limitation to recommender systems,
especially in multimedia services like news, music, and short-video
platforms. The recent advancements in large multimodal models of-
fer new opportunities and challenges in developing content-aware
recommender systems. This survey seeks to provide a comprehen-
sive exploration of the latest advancements and future trajectories
in multimodal pretraining, adaptation, and generation techniques,
as well as their applications in enhancing recommender systems.
Furthermore, we discuss current open challenges and opportuni-
ties for future research in this dynamic domain. We believe that
this survey, alongside the curated resources1, will provide valuable
insights to inspire further advancements in this evolving landscape.
CCS CONCEPTS
•Information systems →Recommender systems.
∗Equal contribution. Correspondence to: Jieming Zhu.
†The work was done when the authors were visiting at Huawei Noah’s Ark Lab.
1Github repository: https://mmrec.github.io/survey
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671473KEYWORDS
Recommender Systems, Multimodal Pretraining, Multimodal Adap-
tation, Multimodal Generation
ACM Reference Format:
Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai, Zhaocheng Du, Xiao-
Ming Wu, Zhou Zhao, Rui Zhang, and Zhenhua Dong. 2024. Multimodal
Pretraining, Adaptation, and Generation for Recommendation: A Survey. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671473
1 INTRODUCTION
Recommender systems have been widely employed in various on-
line applications, including e-commerce websites, advertising sys-
tems, streaming services, and social media platforms, to deliver
personalized recommendations to users. Their primary goal is to
enhance user experience, boost user engagement, and facilitate
the discovery of items tailored to individual interests. However,
traditional recommendation models primarily rely on unique IDs
(e.g., user/item IDs) and categorical features (e.g., tags) for user-item
matching [ 162], potentially overlooking the nuanced essence of raw
item contents across multiple modalities such as text, image, audio,
and video [ 149]. This underutilization of multimodal data poses
a limitation to recommender systems, especially in multimedia
services like news, music, and short-video platforms [14].
To tackle this limitation, researchers have extensively investi-
gated multimodal recommendation techniques for over a decade,
resulting in a large body of research work that explores the inte-
gration of multimodal item features into recommendation models.
For a comprehensive review, interested readers can refer to re-
cent surveys [ 14,73,84,157]. These surveys primarily delve into
techniques such as multimodal feature extraction [ 84], feature rep-
resentation [ 14], feature interaction [ 73], feature alignment [ 14],
feature enhancement [ 73], and multimodal fusion [ 157] for rec-
ommendation models. However, the majority of these approaches
6566
KDD ’24, August 25–29, 2024, Barcelona, Spain Qijiong Liu et al.
rely on extracted multimodal feature embeddings, leaving other
aspects of multimodal pretraining and generation relatively unex-
plored. Nowadays, pretrained large models have gained significant
popularity in the domains of natural language processing (NLP),
computer vision (CV), and multimodal systems (MM). The emer-
gence of language models like the GPT [ 7] and Llama [ 116] series
has ushered in a new era of capabilities for understanding and gen-
erating language, while the CV field has witnessed breakthroughs
with models such as ViT [ 21] and DINOv2 [ 90]. Leveraging these
successes in unimodal domains, the multimodal community has
concentrated on on aligning content across different modalities,
such as CLIP [ 92], CLAP [ 24] and BLIP-2 [ 54]. Notably, the recent in-
troduction of groundbreaking technologies, such as ChatGPT [ 88],
SD [99], and Sora [ 80], has further advanced the generation capa-
bilities of pretrained large models to unprecedented levels. These
recent advancements in pretrained large multimodal models of-
fer new opportunities and challenges in developing content-aware
recommender systems.
In this survey, our objective is to provide a comprehensive overview
of multimodal recommendation techniques from a new perspective,
focusing on leveraging pretrained multimodal models. We explore
the latest advancements and future trajectories in multimodal pre-
training, adaptation, and generation techniques, along with their
applications to recommender systems. Different from prior works,
our survey capitalizes on recent progress in multimodal language
models [ 54,89], prompt and adapter tuning [ 60,159], and genera-
tion techniques such as stable diffusion [ 99]. Additionally, we delve
into the most recent practical developments and remaining open
challenges in applying pretrained multimodal models for recom-
mendation tasks.
More specifically, Section 2 introduces the task of multimodal pre-
training for recommendation, emphasizing methods to enhance in-
domain multimodal pretraining using domain-specific data. Section
3 examines multimodal adaptation for recommendation, elucidating
how pretrained multimodal models can be adapted to downstream
recommendation tasks through techniques such as representation
transfer, model finetuning, adapter tuning, and prompt tuning. Sec-
tion 4 delves into the emerging topic of multimodal generation for
recommendation, with a focus on the application of AI-generated
content (AIGC) techniques in recommendation contexts. In Sec-
tion 5, we outline a range of common applications that necessitate
multimodal recommendation, followed by a discussion of open chal-
lenges and opportunities for future research in Section 6. Finally, we
conclude the survey in Section 7. We hope that this survey, along
with the curated resources, will inspire further research efforts to
advance this evolving landscape.
2 MULTIMODAL PRETRAINING FOR
RECOMMENDATION
In contrast to supervised learning directly on domain-specific data,
self-supervised pretraining learns from a large-scale unlabeled cor-
pus and then adapts the pretrained model to downstream tasks.
This approach allows for the acquisition of rich external knowledge
in pretraining data, thus leading to the widespread recognition of its
effectiveness. In this section, we will first provide a review of major
pretraining paradigms and then introduce how they are utilized inthe recommendation domain. Figure 1a presents an overview of
multimodal pretraining techniques.
2.1 Self-supervised Pretraining Paradigms
We broadly categorize self-supervised pretraining paradigms into
three types according to their pretraining tasks.
Reconstructive Paradigm. This pretraining paradigm aims
to teach models to reconstruct raw inputs within the information
bottleneck framework. Examples include mask prediction methods
for partial reconstruction and autoencoder methods for complete
reconstruction. Mask prediction methods were initially introduced
in BERT [ 18], where input tokens are randomly masked, prompt-
ing the model to learn to predict them based on the surrounding
context. In contrast, autoencoder methods (e.g., AE [ 5], VAE [ 50])
encode input data into a concise latent space and subsequently learn
to fully recover the input from this latent representation. These
methods have found extensive use in self-supervised pretraining
across various domains such as text [ 52], vision [ 21,36,119], au-
dio [ 150], and multimodal data [ 97,99]. Following their success,
researchers have applied the reconstructive pretraining paradigm
to recommendation tasks. For instance, methods like mask item pre-
diction in Bert4Rec [ 112], mask token prediction in Recformer [ 55],
autoencoder-based item tokenization [ 72,96], and masked node
feature reconstruction in PMGT [ 79] have emerged. Despite signif-
icant progress, relying solely on this reconstructive paradigm may
not capture proximity information from user-item interactions ef-
fectively. Consequently, these methods are typically complemented
with a contrastive learning paradigm in practice.
Contrastive Paradigm. This pretraining focuses on pairwise
similarity, distinguishing between similar and dissimilar data sam-
ples by maximizing distances between negative pairs and mini-
mizing them for positive pairs within a representation space. It
has proven effective in enhancing the quality of representations
across different domains. Examples such as SimCSE [ 28] for text,
SimCLR [ 11] for images, CLMR [ 110] for music, and CLIP [ 92] for
multimodal representations highlight its versatility and applicabil-
ity. Given its ability to capture pairwise similarities, this paradigm
finds extensive use in aligning user-item preferences. Applications
like MGCL [ 69], MMSSL [ 126], MMCPR [ 82], MSM4SR [ 152], and
MISSRec [ 121] exemplify the utilization of contrastive learning for
enhancing multimodal pretraining in recommender systems.
Autoregressive Paradigm. This paradigm has recently achieved
remarkable success, particularly with the rise of large language
models (LLMs) such as the GPT family [ 7,89,93,94]. It gener-
ates sequence data token by token in an autoregressive manner,
where each token is predicted based on previous observations. In
other words, this approach operates in a unidirectional, left-to-right
generation framework, which is different from the reconstructive
paradigm that employs bidirectional context to predict masked to-
kens. It has also gained rapid adoption in the CV domain [ 98] and
multimodal domain [ 151]. In the realm of recommender systems,
user behavior sequences naturally lend themselves to sequential
processing, fostering the development of numerous autoregressive
sequential recommendation models such as SASRec [ 48]. Recent
studies, such as P5 [ 31] and VIP5 [ 32], have explored the integration
of LLMs or pretrained multimodal models into recommendation
6567Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
Sequence
Graph
Interaction General-domain 
Pretraining In-domain 
Pretraining Reconstructive 
Task Autoregressive 
Task Contrastive 
Task
 Post-pretrain
Text
Image
VideoText
Image
Video
 External Data Domain Data
(
a) Multimodal Pretraining.
Representation Transfer Model Finetuning
 Adapter Tuning Prompt Tuning Recommendation Model Recomme nd Item Identi er
LLM ViT CLIP
 Pretrained Models (
b) Multimodal Adaptation.
Text Generation Image Generation
 Video GenerationGenerate Item Content
text image video
Sora LLM SD
 Generative Models Personalized Generation (
c) Multimodal Generation.
Figur
e 1: An overview of multimodal pretraining, adaptation, and generation tasks for recommendation.
tasks. Concurrently, generative recommendation, which frames rec-
ommendation as autoregressive sequence generation, has emerged
as a burgeoning area of research [96, 123, 125].
2.2 Content-aware Pretraining for
Recommendation
Content-aware recommender systems strive to incorporate the
semantic content of items to improve recommendation accuracy.
Consequently, numerous studies have explored content-enhanced
pretraining methods for recommendation systems. In this section,
we categorize existing research based on the modalities employed
for pretraining and discuss their application both generally and
within recommendation systems.
Text-based Pretraining. Texts are among the most prevalent
forms of content in recommender systems, applied in contexts
such as news recommendation and review-based recommenda-
tion. Within the domain of natural language processing (NLP),
pretrained language models like BERT [ 18] and T5 [ 95] have been
developed to capture context-aware representations of text. These
models typically follow a pretraining-finetuning paradigm tailored
to specific tasks. Recently, large language models (LLMs) such as
ChatGPT [ 88] and LLaMa [ 115] have demonstrated significant ca-
pabilities in language-related tasks, leveraging techniques such as
prompting and in-context learning. Building on their success, text-
enhanced pretraining has gained traction in recommender systems.
Notable examples include MINER [ 56] for news recommendation,
Recformer [ 55] for sequential recommendation, UniSRec [ 38] for
cross-domain recommendation, and P5 [ 31] for LLM-based interac-
tive recommendation.
Audio-based Pretraining. Music recommendation represents
a prominent scenario heavily reliant on audio modalities to capture
content semantics. Analogous to the NLP domain, various pretrain-
ing techniques have been employed to enhance audio representa-
tions, including Wav2Vec [ 101], MusicBert [ 150], MART [ 145], and
MERT [ 61]. In the context of music recommendation, researchers
explore leveraging these audio pretraining methods by utilizing
user-item interactions as supervision signals to finetune music
representations. For example, Chen et al.[ 10] propose learning
user-audio embeddings from track data and user interests throughcontrastive learning techniques. Furthermore, Huang et al.[ 43] in-
tegrate pairwise textual and audio features into a convolutional
model to jointly learn content embeddings in a similarity metric.
Interested readers can find additional examples in a comprehensive
review paper [15].
Vision-based Pretraining. Images and videos constitute the
primary visual data in multimedia recommendation scenarios as
ads, movies, and videos. In the CV domain, the evolution of vision-
based pretraining has transitioned from CNN-based architectures
like ResNet [ 37] to transformer architectures such as ViT [ 21] and
DINOv2 [ 90], enabling the extraction of versatile visual features.
These pretrained models have significantly advanced vision-aware
recommendation systems. Researchers like Liu et al.[ 68] and Chen
et al.[ 12] leverage pretrained CNN encoders with category priors to
extract image features for industrial recommendation tasks, finetun-
ing image encoders alongside recommendation models. Similarly,
Wang et al.[ 121] and Wei et al.[ 125] utilize pretrained transformer
encoders, such as ViT [ 92], to encode images and sequences of user
behaviors. Vision foundation models continue to evolve rapidly,
promising future applications in recommendation tasks. Looking
ahead, there is a growing interest in exploring the use of newly pre-
trained models for recommendation tasks. For instance, leveraging
pretrained video transformers like Video-LLaVA [ 62] remains an
unexplored area in building video recommendation models.
Multimodal Pretraining. In current literature, most studies
tend to focus on modeling the primary modality of content, such as
text for news recommendation [ 56], audio for music recommenda-
tion [ 10], and images for e-commerce recommendation [ 68]. How-
ever, multimedia content inherently involves multiple modalities.
For instance, news articles often include titles, descriptions, and
accompanying images. Similarly, video recommendation involves
handling visual frames, audio signals, and subtitles.
Unlike single-modal techniques, multimodal models must cap-
ture both commonalities and complementary information across
multimodal data sources through techniques like cross-modal align-
ment and fusion. In recent years, multimodal pretraining has seen
rapid development, resulting in a plethora of pretrained models,
including single-stream models (e.g., VL-BERT [ 111]), dual-stream
models (e.g., CLIP [ 92]), and hybrid models (e.g., FLAVA [ 106] and
6568KDD ’24, August 25–29, 2024, Barcelona, Spain Qijiong Liu et al.
CoCa [ 147]). Recent research has also focused on achieving uni-
fied representations of multimodal data, exemplified by models
like ImageBind [ 33], MetaTransformer [ 154], and UnifiedIO-2 [ 83].
Another emerging trend involves integrating multimodal encoders
with large language models, resulting in multimodal large language
models such as BLIP-2 [ 54], Flamingo [ 1], and Llava [ 67]. These
advancements offer promising opportunities for building modern
multimodal recommender systems.
Initial efforts in this direction include models like MISSRec [ 121],
Rec-GPT4V [ 77], MMSSL [ 126], MSM4SR [ 152], and AlignRec [ 81].
However, current research primarily focuses on utilizing off-the-
shelf pretrained multimodal encoders and integrating them with
sequence-based or graph-based pretraining techniques for handling
recommendation data. There remains a gap in how to specifically
pretrain domain-specific multimodal models tailored for recommen-
dation tasks. Pioneering efforts have been made in the e-commerce
domain, as evidenced by models such as M5Product [ 20], K3M [ 165],
ECLIP [ 47], and CommerceMM [ 148]. Future research is expected
to further explore and advance in this direction.
3 MULTIMODAL ADAPTION FOR
RECOMMENDATION
While most existing pretrained models are trained on general data
corpora, adapting them for recommender systems requires strategic
methods to fully utilize their learned knowledge. This section sum-
marizes four major adaptation techniques: representation transfer,
model finetuning, adapter tuning, and prompt tuning. Each tech-
nique provides a distinct approach to harnessing the benefits of a
pretrained model. Figure 1b presents an overview of multimodal
adaptation techniques.
3.1 Representation Transfer
Representation transfer is one of the most commonly used adap-
tation techniques for transferring pretrained knowledge to recom-
mendation models. Specifically, item representations are extracted
from frozen pretrained models and used as additional features
alongside ID embeddings. These representations provide supple-
mentary general information to recommender systems, addressing
the cold-start problem where new or infrequently interacted items
may have inadequate ID embeddings derived from limited interac-
tions [ 74]. Approaches based on representation transfer have been
extensively studied and proven effective across various domains,
e.g., text-based recommendation [ 132], vision-based recommenda-
tion [ 4,102], multimodal recommendation [ 22,41,64,128,140].
For multimodal scenarios specifically, significant efforts have been
directed towards fusing representations from multiple modalities.
This includes techniques such as early fusion [ 41,75,79,161], in-
termediate fusion [ 22,134,140], and late fusion [ 114,130]. An-
other research focus involves aligning multimodal representations
within user behavior spaces using methods like content-ID align-
ment [ 78,129], item-to-item matching [ 44], user sequence model-
ing [38], and graph neural networks [131].
However, straightforward and efficient representation transfer
may encounter a significant domain generalization gap due to mis-
alignment between the semantic space and the behavior space,
which may not consistently lead to performance improvementsin practice. Model finetuning offers a direct solution to address
this issue. Furthermore, as noted by KDSR [ 41], there is a risk of
forgetting modality features in representations, leading them to
resemble those trained without incorporating such features. One
viable approach involves integrating explicit constraints [ 41], while
another potential solution introduces semantic tokenization tech-
niques [ 46,72,96,107], which quantize item content representations
into discrete tokens.
3.2 Model Finetuning
Model finetuning refers to the process of further training a pre-
trained model on task-specific data. Its goal is to adapt the model
parameters to effectively capture domain-specific nuances, thereby
improving its performance on the specific downstream task. This
pretraining-finetuning paradigm has proven successful in various
practical applications. Specifically, finetuning can involve aligning
the semantic space of pretrained models with the behavior space
of recommendation models. Depending on the application of pre-
trained models, they can extract item representations [ 56,141],
user representations [ 113], or both [ 55,74,121]. Moreover, based
on the types of downstream tasks, current research can be classi-
fied into representation-based matching tasks [ 56,121,133] and
scoring-based ranking tasks [68, 141, 153].
However, end-to-end finetuning of pretrained models with rec-
ommendation data faces challenges related to training efficiency.
Recommendation tasks often require processing millions or even
billions of samples daily. Fully finetuning a pretrained model sub-
stantially amplifies training overhead, which poses practical limita-
tions in large-scale recommender systems, particularly given the
scale of large language and multimodal models [ 54,115]. Moreover,
finetuning with large volumes of data easily leads to the issue of cat-
astrophic forgetting, where previously learned knowledge rapidly
deteriorates during continual training.
3.3 Adapter Tuning
To reduce training overhead with pretrained large models, parameter-
efficient finetuning (PEFT) methods have been developed. One
prominent approach is through parameter-efficient adapters like
LoRAs [ 40], which integrate compact, task-specific modules di-
rectly into pretrained models. This strategy effectively reduces the
number of parameters needed for finetuning and facilitates rapid
model adaptation. Widely recognized for its efficacy across var-
ious domains, PEFT techniques have gained significant traction
in recommendation systems [ 27,32,38,71,135]. For example, the
ONCE framework [ 71] leverages the pretrained Llama model [ 115]
with LoRAs as item encoders to enhance content-aware recom-
mendation. Similarly, UniSRec [ 38] employs an MOE-based adapter
with the BERT model to improve semantic representations of items
across diverse domains. In the realm of multimodal recommen-
dation, TransRec [ 27] and VIP5 [ 32] have introduced layerwise
adapters. M3SRec utilizes modality-specific MOE adapters [ 6], while
EM3 employs multimodal fusion adapters [ 16] during finetuning.
Nonetheless, the ongoing challenge lies in designing adapters that
effectively balance both effectiveness and efficiency, which remains
an active area of research.
6569Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
3.4 Prompt Tuning
With the emergence of large language models, prompting has be-
come a pivotal technique in harnessing their capabilities to gener-
ate desired outputs or perform specific tasks [ 70]. Instead of using
handcrafted prompts, prompt tuning aims to learn task-adaptable
prompts from task-specific data while keeping the model param-
eters frozen. As a result, prompt tuning can avoid catastrophic
forgetting and enable fast adaptation with only prompt tokens as
tunable parameters. Depending on whether prompts are optimized
in a discrete token space, they can be further categorized into hard
prompt tuning, such as AutoPrompt [ 105], and soft prompt tuning,
like Prefix-Tuning [ 60]. Prompt tuning has been successful in visual
learning [ 159] and multimodal learning [ 23,49], enhancing model
performance.
For multimodal recommendation tasks, prompt tuning has emerged
as a novel technique to adapt pretrained models. Recent studies
such as RecPrompt [ 66], ProLLM4Rec [ 138], Prompt4NR [ 155], and
PBNR [ 59] employ prompting methods to customize large lan-
guage models (LLMs) for news recommendation tasks. Additionally,
DeepMP [ 125], VIP5 [ 32], and PromptMM [ 127] employ prompt
tuning to integrate and adapt multimodal content knowledge to
enhance recommendation. Despite recent advancements, this area
of research remains underexplored. An interesting direction is the
development of personalized multimodal prompting techniques to
advance multimodal recommendation systems.
4 MULTIMODAL GENERATION FOR
RECOMMENDATION
With recent advancements in generative models, AI-generated con-
tent (AIGC) has gained significant popularity across diverse appli-
cations. In this section, we explore potential research avenues for
employing AIGC techniques within recommender systems. Figure
1c presents an overview of multimodal generation techniques.
4.1 Text Generation
With the support of powerful large language models (LLMs), text
generation has become a mature capability and is now being applied
in various tasks within the recommendation domain [86].
•Keyword Generation: Keyword tagging plays a pivotal role in
content understanding for ads targeting and recommendation.
Previous techniques mostly rely on explicit keyword extraction
from textual content, potentially missing important keywords
absent from the text. Consequently, keyword generation tech-
niques have been widely applied to enhance the keyword tagging
process [53, 108].
•News Headline Generation: The demand for personalized and
engaging news content has fueled the exploration of news head-
line generation. Conventionally, headline generation is framed as
a text summarization task, condensing input text or multimodal
content into a title [ 19,51]. However, typical news headlines
may lack appeal or relevance to specific users, prompting the
need for personalized approaches. Consequently, personalized
headline generation has emerged as a compelling research topic,
focusing on generating titles tailored to individual users’ reading
preferences and available news content [35, 100].•Marketing Copy Generation: Marketing copy refers to the text
used to promote a product and motivate consumers to purchase.
It plays a vital role in capturing users’ interest and enhancing
engagement. Recent efforts have focused on automatic marketing
copywriting based on LLMs [85, 158]
•Explanation Generation: In interactive scenarios, the demand
for explainable recommendations is growing significantly. This
involves generating natural language explanations to justify the
recommendation of items to individual users, thereby enhancing
user understanding and trust in the system [57, 156].
•Dialogue Generation: Dialogue generation [ 142] is essential in
conversational recommender systems, encompassing the genera-
tion of responses that describe recommended items [ 26]. More-
over, it entails generating questions to guide users towards fur-
ther rounds of conversation and interaction [124].
While these tasks benefit from powerful LLMs, two critical chal-
lenges persist in text generation for recommendation: 1) Control-
lable Generation: Industrial applications necessitate precise con-
trol over generated texts to ensure correctness of product descrip-
tions, use unique selling propositions, or adhere to specific writing
styles [ 160]. 2) Knowledge-Enhanced Generation: Existing LLMs
often lack explicit awareness of domain-specific knowledge, such
as product entities, categories, and selling points. Recent research
has concentrated on integrating domain-specific knowledge bases
to achieve more satisfactory results [117, 139].
4.2 Image and Video Generation
Text-to-image generation has achieved remarkable success with
the prevalence of diffusion models (e.g., SD [ 99]). In this section,
we delve into their potential applications in e-commerce and ad-
vertising. Unlike natural image generation, generating product
images and ad banners involves dealing with complex layouts, en-
compassing various elements such as products, logos, and textual
descriptions. Consequently, unique challenges arise in designing a
coherent layout and effectively integrating text with appropriate
fonts and colors to create visually appealing posters.
Specifically, Inoue et al. [ 45] propose LayoutDM, a model de-
signed to effectively handle structured layout data and facilitate
the discrete diffusion process. Hsu et al. [ 39] enable content-aware
layout generation (namely PosterLayout) by arranging predefined
spatial elements on a given canvas. Lin et al. [ 63] develop Auto-
Poster, a highly automated and content-aware system for gener-
ating advertising posters. Concurrently, some studies explore text
design for poster generation. For example, Gao et al. [ 29] introduce
TextPainter, a novel multimodal approach that leverages contextual
visual information and corresponding text semantics to generate
text images. Tuo et al. [ 118] propose a diffusion-based multilingual
visual text generation and editing model, AnyText, which addresses
how to render accurate and coherent text in the image.
More recently, video generation has made significant strides.
Sora [ 80] emerges as a groundbreaking technology showcasing im-
mense potential for generating advertising videos for products. In
this context, Gong et al. [ 34] introduce AtomoVideo, a high-fidelity
image-to-video generation solution that effectively transforms prod-
uct images into engaging promotional videos for advertising pur-
poses. Additionally, Liu et al. [ 65] have devised a system capable
6570KDD ’24, August 25–29, 2024, Barcelona, Spain Qijiong Liu et al.
of automatically generating visual storylines from a given set of
visual materials, producing compelling promotional videos tailored
for e-commerce. Furthermore, Wang et al. [ 122] have developed an
integrated approach, merging text-to-image models, video motion
generators, reference image embedding modules, and frame interpo-
lation modules into an end-to-end video generation pipeline, which
is valuable for micro-video recommendation platforms. We believe
that this field is rapidly expanding, enabling the advancement of
AIGC-based recommendation and advertising applications.
4.3 Personalized Generation
With the rise of AIGC, there is a notable shift towards personalized
generation, aiming to enhance the customization and personaliza-
tion of generated content. This trend holds particular significance
in recommendation scenarios, where personalized content can bet-
ter cater to users’ interests. Pioneering work has been undertaken
in various domains, including personalized news headline gener-
ation [ 2,3,8,100], personalized product description generation
in e-commerce [ 17], personalized answer generation [ 17], person-
alized image generation with identity preservation [ 21], and per-
sonalized multimodal generation [ 104]. Integrating recommender
systems with personalized generation techniques shows promise
for developing next-generation recommender systems.
5 APPLICATIONS
In this section, we summarize some common application domains
that require multimodal recommendation techniques.
•E-commerce Recommendation. E-commerce represents one
of the most extensively studied application domains in recom-
mender systems research, aimed at assisting users in discover-
ing items they are likely to purchase. The abundance of multi-
modal data in e-commerce, including product titles, descriptions,
images, and reviews, poses a challenge in integrating different
modalities with user interaction data to enhance recommenda-
tion quality. To address this challenge, numerous research efforts
have been undertaken. Notable examples include works by Al-
ibaba [30, 58, 141], JD.com [68, 136], and Pinterest [4].
•Advertisement Recommendation. Online advertising serves
as a primary revenue source for many web applications. Adver-
tising creatives play a pivotal role in this ecosystem, spanning
various formats such as images, titles, and videos. Aesthetic cre-
atives have the potential to engage potential users and enhance
the click-through rate (CTR) of products [ 9]. There is also a press-
ing need to understand ad creatives better to effectively align
advertisements with users’ interests [143, 144].
•News Recommendation. Personalized news recommendation
is a crucial technique for assisting users in discovering news of
interest. To enhance recommendation accuracy and diversity, rec-
ommender systems must comprehend news content and extract
semantic information from a user’s reading history. This often in-
volves learning semantic representations of news titles, abstracts,
body text, and cover images. Recent research has focused on
modeling features from multiple modalities, as exemplified by
MM-Rec [134] and IMRec [140].
•Video Recommendation. With the surge in popularity of micro-
video platforms, video recommendation has garnered significantattention within the community. Videos encapsulate a multi-
tude of modalities, including titles, thumbnail images, frames,
audio tracks, transcripts, and more. Current research efforts have
been concentrated on integrating and adapting multimodal infor-
mation within micro-video recommendation models. [ 131,146].
Notably, Ni et al. [ 87] have recently introduced a comprehensive
micro-video recommendation dataset, enriched with abundant
multimodal side information, to foster further research in this
domain.
•Music Recommendation. The realm of music streaming ser-
vices represents another prominent domain that necessitates
multimodal recommendation techniques. Within this sphere, a
diverse array of multimodal data is involved, including music
audio, scores, lyrics, tags, and reviews. Leveraging these various
types of music data has proven effective in crafting more per-
sonalized recommendations aimed at engaging users; notable
examples can be found in [ 10,43]. Additionally, Shen et al. [ 103]
propose that incorporating multimodal information from users’
social media can offer insights into their personalities, emotions,
and mental well-being, thereby enhancing the accuracy of music
recommendation.
•Fashion Recommendation . With the visual and aesthetic na-
ture of fashion products, fashion recommendation has emerged
as a distinct vertical domain. Unlike traditional recommender
systems, fashion recommendation not only suggests individual
items but also outfits that complement multiple items. Multi-
modal understanding capabilities play a pivotal role in this area,
including tasks such as localizing fashion items from images,
identifying their attributes, and computing compatibility scores
for multiple items [ 13,109]. Moreover, pioneering work [ 164] has
developed text-to-image diffusion models that allow users to vir-
tually try on clothes. These techniques are expected to enhance
the personalization of fashion recommendation and elevate user
experience to the next level.
•LBS Recommendation. Location-based services (LBS) have be-
come ubiquitous, offering a wide range of services including
taxi travel, food delivery, and restaurant recommendation. In
these contexts, users can share their Points of Interest (POI)
check-ins, photos, opinions, and comments, which encompass a
rich array of multimodal spatio-temporal data. Integrating this
multimodal information and understanding spatio-temporal cor-
relations among locations enables more accurate modeling of
user preferences. Notable examples can be found in [76, 91].
6 CHALLENGES AND OPPORTUNITIES
In this section, we discuss the persistent challenges and emerging
opportunities for future research.
•Multimodal Information Fusion. Multimodal fusion has been
extensively explored in research. Within recommender systems,
current studies primarily concentrate on fusing and adapting
multimodal feature embeddings of items to recommendation
models [157]. However, multimodal information for recommen-
dation inherently adopts a hierarchical structure, ranging from
user behavior sequences to individual items, each comprising
multiple modalities and further subdivided into semantic tokens
and objects. Additionally, the impact of information from diverse
6571Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
modalities and regions can vary significantly among different
users. As a result, the challenge lies in effectively fusing multi-
modal information in a hierarchical and personalized manner to
optimize recommendations.
•Multimodal Multi-domain Recommendation. Multimodal
information provides rich semantic insights into item content.
Despite considerable research into multimodal recommendation
and cross-domain recommendation, effectively leveraging multi-
modal information to bridge the information gap across domains
remains an open challenge [ 113]. For instance, recommending
music based on a user’s reading habits entails semantic alignment
across modalities (audio vs. text) and domains (music vs. books).
•Multimodal Foundation Models for Recommendation. While
large language models and large multimodal models have emerged
as foundation models in the NLP and CV domains, there exists
a compelling opportunity to extend this exploration into the
recommendation domain. An ideal recommendation foundation
model should demonstrate robust in-context learning capabili-
ties while maintaining generalizability across diverse tasks and
domains [ 42]. Potential avenues for exploration include adapt-
ing existing multimodal LLMs for recommendation tasks (e.g.,
[32]), or conducting the pretraining of a multimodal generative
model from scratch using large-scale multimodal multi-domain
recommendation data.
•AIGC for Recommendation. The integration of AIGC repre-
sents a notable advancement in recommender systems, offering
an opportunity to significantly enhance user personalization,
engagement, and overall experience. This encompasses personal-
ized news headlines, tailored advertising creatives, and explana-
tory content across diverse recommendation contexts. This field
is rapidly expanding, with the primary challenge lying in achiev-
ing a comprehensive understanding of both content and users,
facilitating controllable generation, and ensuring accurate format-
ting to optimize the user experience. Additionally, it is imperative
to address potential ethical and privacy concerns arising from
the use of AIGC.
•Multimodal Recommendation Agent. LLM-based agents [ 120]
have demonstrated exceptional proficiency in automating tasks
through extensive knowledge and strong reasoning capabili-
ties. The integration of these agents has introduced innovative
prospects in the field of recommendation, particularly in con-
versational recommendation [ 25]. This entails directly engaging
users in the task completion process, thereby enhancing the user
experience and the effectiveness of recommender systems. As a
concrete example, integrating conversation and virtual try-on
generation [ 164] capabilities may present new opportunities for
fashion recommendation.
•Efficiency of Training and Inference. Recommendation tasks
typically have stringent latency requirements to meet real-time
service demands. Therefore, ensuring training and inference ef-
ficiency becomes imperative when applying multimodal pre-
training and generation techniques in practice. There is a high
demand for the development of efficient strategies to leverage the
capabilities of multimodal models. Pioneer efforts in this direc-
tion include speeding up training by merging item sets to avoid
redundant encoding operations [ 137] and enhancing inference
speed [12, 74] through caching item and user representations.7 CONCLUSION
Multimodal recommendation, an immensely promising field, has
garnered significant attention in recent years, fueled by advance-
ments in both multimodal machine learning and the recommenda-
tion system community. The advent of large multimodal models has
transformed the multimodal recommendation landscape, endow-
ing it with enhanced capabilities for comprehension and content
generation. This paper provides a systematical overview of the
current multimodal recommendation framework, focusing on key
aspects such as multimodal pretraining, adaptation, and generation.
Additionally, we delve into its applications, challenges, and future
prospects. Our aim is to offer this survey as a resourceful guide to
aid subsequent research in the field.
ACKNOWLEDGMENTS
We thank Dr. Xin Zhou and Dr. Chuhan Wu for the discussion and
contribution to the tutorial materials of multimodal pretraining and
generation for recommendation presented at WWW 2024 [163].
REFERENCES
[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana
Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
et al.2022. Flamingo: a visual language model for few-shot learning. Advances
in Neural Information Processing Systems (NeurIPS) (2022), 23716–23736.
[2]Xiang Ao, Ling Luo, Xiting Wang, Zhao Yang, Jiun-Hung Chen, Ying Qiao,
Qing He, and Xing Xie. 2023. Put Your Voice on Stage: Personalized Headline
Generation for News Articles. TKDD 18, 3 (2023).
[3]Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He, and Xing Xie. 2021. PENS:
A Dataset and Generic Framework for Personalized News Headline Generation.
InProceedings of ACL/IJCNLP. 82–92.
[4]Paul Baltescu, Haoyu Chen, Nikil Pancha, Andrew Zhai, Jure Leskovec, and
Charles Rosenberg. 2022. Itemsage: Learning product embeddings for shop-
ping recommendations at pinterest. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD). 2703–2711.
[5]Dor Bank, Noam Koenigstein, and Raja Giryes. 2020. Autoencoders. CoRR
abs/2003.05991 (2020).
[6]Shuqing Bian, Xingyu Pan, Wayne Xin Zhao, Jinpeng Wang, Chuyuan Wang,
and Ji-Rong Wen. 2023. Multi-modal Mixture of Experts Represetation Learning
for Sequential Recommendation. In Proceedings of the 32nd ACM International
Conference on Information and Knowledge Management (CIKM). 110–119.
[7]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in Neural
Information Processing Systems (NeurIPS) (2020), 1877–1901.
[8]Pengshan Cai, Kaiqiang Song, Sangwoo Cho, Hongwei Wang, Xiaoyang Wang,
Hong Yu, Fei Liu, and Dong Yu. 2023. Generating User-Engaging News Headlines.
InProceedings of ACL. 3265–3280.
[9]Jin Chen, Ju Xu, Gangwei Jiang, Tiezheng Ge, Zhiqiang Zhang, Defu Lian, and
Kai Zheng. 2021. Automated Creative Optimization for E-Commerce Advertising.
InThe ACM Web Conference (WWW). 2304–2313.
[10] Ke Chen, Beici Liang, Xiaoshuan Ma, and Minwei Gu. 2021. Learning audio
embeddings with user listening data for content-based music recommenda-
tion. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 3015–3019.
[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.
A Simple Framework for Contrastive Learning of Visual Representations. In
Proceedings of the 37th International Conference on Machine Learning (ICML).
1597–1607.
[12] Xin Chen, Qingtao Tang, Ke Hu, Yue Xu, Shihang Qiu, Jia Cheng, and Jun Lei.
2022. Hybrid CNN Based Attention with Category Prior for User Image Behavior
Modeling. In Proceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR). 2336–2340.
[13] Yashar Deldjoo, Fatemeh Nazary, Arnau Ramisa, Julian J. McAuley, Giovanni
Pellegrini, Alejandro Bellogín, and Tommaso Di Noia. 2024. A Review of Modern
Fashion Recommender Systems. ACM Comput. Surv. 56, 4 (2024), 87:1–87:37.
[14] Yashar Deldjoo, Markus Schedl, Paolo Cremonesi, and Gabriella Pasi. 2020.
Recommender systems leveraging multimedia content. Comput. Surveys 53, 5
(2020), 1–38.
[15] Yashar Deldjoo, Markus Schedl, and Peter Knees. 2021. Content-driven Mu-
sic Recommendation: Evolution, State of the Art, and Challenges. CoRR
6572KDD ’24, August 25–29, 2024, Barcelona, Spain Qijiong Liu et al.
abs/2107.11803 (2021).
[16] Xiuqi Deng, Lu Xu, Xiyao Li, Jinkai Yu, Erpeng Xue, Zhongyuan Wang, Di
Zhang, Zhaojie Liu, Guorui Zhou, Yang Song, Na Mou, Shen Jiang, and Han
Li. 2024. End-to-end training of Multimodal Model and ranking Model. CoRR
abs/2404.06078 (2024).
[17] Yang Deng, Yaliang Li, Wenxuan Zhang, Bolin Ding, and Wai Lam. 2022. To-
ward Personalized Answer Generation in E-Commerce via Multi-perspective
Preference Modeling. ACM Trans. Inf. Syst. 40, 4 (2022), 87:1–87:28.
[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for Language Under-
standing. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies
(NAACL-HLT). 4171–4186.
[19] Zijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel R. Tetreault, and Ale-
jandro Jaimes. 2023. Harnessing the power of LLMs: Evaluating human-AI text
co-creation through the lens of news headline generation. In Findings of EMNLP.
3321–3339.
[20] Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Michael C. Kampffmeyer,
Xiaoyong Wei, Minlong Lu, Yaowei Wang, and Xiaodan Liang. 2022. M5Product:
Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretrain-
ing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
21220–21230.
[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,
Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An
Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In
9th International Conference on Learning Representations (ICLR).
[22] Xiaoyu Du, Xiang Wang, Xiangnan He, Zechao Li, Jinhui Tang, and Tat-Seng
Chua. 2020. How to learn item representation for cold-start multimedia rec-
ommendation?. In Proceedings of the 28th ACM International Conference on
Multimedia. 3469–3477.
[23] Haoyi Duan, Yan Xia, Mingze Zhou, Li Tang, Jieming Zhu, and Zhou Zhao.
2023. Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-
Visual Downstream Tasks. In Advances in Neural Information Processing Systems
(NeurIPS).
[24] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.
2023. Clap learning audio concepts from natural language supervision. In IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 1–5.
[25] Jiabao Fang, Shen Gao, Pengjie Ren, Xiuying Chen, Suzan Verberne, and
Zhaochun Ren. 2024. A Multi-Agent Conversational Recommender System.
CoRR abs/2402.01135 (2024).
[26] Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang,
Kun Gai, and Fei Sun. 2023. A Large Language Model Enhanced Conversational
Recommender System. CoRR abs/2308.06212 (2023).
[27] Junchen Fu, Fajie Yuan, Yu Song, Zheng Yuan, Mingyue Cheng, Shenghui Cheng,
Jiaqi Zhang, Jie Wang, and Yunzhu Pan. 2024. Exploring adapter-based transfer
learning for recommender systems: Empirical studies and practical insights. In
Proceedings of the 17th ACM International Conference on Web Search and Data
Mining (WSDM). 208–217.
[28] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive
Learning of Sentence Embeddings. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing (EMNLP). 6894–6910.
[29] Yifan Gao, Jinpeng Lin, Min Zhou, Chuanbin Liu, Hongtao Xie, Tiezheng Ge,
and Yuning Jiang. 2023. TextPainter: Multimodal Text Image Generation with
Visual-harmony and Text-comprehension for Poster Design. In ACM MM. 7236–
7246.
[30] Tiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huiming Yi,
Zelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, Pengtao Yi, Sui Huang, Zhiqiang
Zhang, Xiaoqiang Zhu, Yu Zhang, and Kun Gai. 2018. Image Matters: Visually
Modeling User Behaviors Using Advanced Model Server. In CIKM. 2087–2095.
[31] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.
Recommendation as language processing (rlp): A unified pretrain, personalized
prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on
Recommender Systems (RecSys). 299–315.
[32] Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. 2023.
VIP5: Towards Multimodal Foundation Models for Recommendation. In Findings
of the Association for Computational Linguistics: EMNLP 2023. 9606–9620.
[33] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev
Alwala, Armand Joulin, and Ishan Misra. 2023. Imagebind: One embedding
space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR). 15180–15190.
[34] Litong Gong, Yiran Zhu, Weijie Li, Xiaoyang Kang, Biao Wang, Tiezheng Ge,
and Bo Zheng. 2024. AtomoVideo: High Fidelity Image-to-Video Generation.
(2024). arXiv:2403.01800
[35] Xiaotao Gu, Yuning Mao, Jiawei Han, Jialu Liu, You Wu, Cong Yu, Daniel Finnie,
Hongkun Yu, Jiaqi Zhai, and Nicholas Zukoski. 2020. Generating RepresentativeHeadlines for News Stories. In The Web Conference 2020 (WWW). 1773–1784.
[36] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Gir-
shick. 2022. Masked autoencoders are scalable vision learners. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
16000–16009.
[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 770–778.
[38] Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-
Rong Wen. 2022. Towards universal sequence representation learning for
recommender systems. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD). 585–593.
[39] HsiaoYuan Hsu, Xiangteng He, Yuxin Peng, Hao Kong, and Qing Zhang. 2023.
PosterLayout: A New Benchmark and Approach for Content-Aware Visual-
Textual Presentation Layout. In CVPR. 6018–6026.
[40] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of
Large Language Models. In The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022.
[41] Hengchang Hu, Qijiong Liu, Chuang Li, and Min-Yen Kan. 2024. Lightweight
Modality Adaptation to Sequential Recommendation via Correlation Supervi-
sion. arXiv preprint arXiv:2401.07257 (2024).
[42] Chengkai Huang, Tong Yu, Kaige Xie, Shuai Zhang, Lina Yao, and Julian J.
McAuley. 2024. Foundation Models for Recommender Systems: A Survey and
New Perspectives. CoRR abs/2402.11143 (2024).
[43] Qingqing Huang, Aren Jansen, Li Zhang, Daniel PW Ellis, Rif A Saurous, and
John Anderson. 2020. Large-scale weakly-supervised content embeddings for
music recommendation and tagging. In IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). 8364–8368.
[44] Yanhua Huang, Weikun Wang, Lei Zhang, and Ruiwen Xu. 2021. Sliding spec-
trum decomposition for diversified recommendation. In Proceedings of the 27th
ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD). 3041–
3049.
[45] Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yam-
aguchi. 2023. LayoutDM: Discrete Diffusion Model for Controllable Layout
Generation. In CVPR. 10167–10176.
[46] Mengqun Jin, Zexuan Qiu, Jieming Zhu, Zhenhua Dong, and Xiu Li. 2024.
Contrastive Quantization based Semantic Code for Generative Recommendation.
CoRR abs/2404.14774 (2024).
[47] Yang Jin, Yongzhi Li, Zehuan Yuan, and Yadong Mu. 2023. Learning Instance-
Level Representation for Large-Scale Multi-Modal Pretraining in E-Commerce.
InIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
11060–11069.
[48] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential rec-
ommendation. In IEEE International Conference on Data Mining (ICDM). IEEE,
197–206.
[49] Muhammad Uzair Khattak, Hanoona Abdul Rasheed, Muhammad Maaz,
Salman H. Khan, and Fahad Shahbaz Khan. 2023. MaPLe: Multi-modal Prompt
Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR). 19113–19122.
[50] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In
2nd International Conference on Learning Representations (ICLR), Yoshua Bengio
and Yann LeCun (Eds.).
[51] Mateusz Krubinski and Pavel Pecina. 2024. Towards Unified Uni- and Multi-
modal News Headline Generation. In Proceedings of EACL. 437–450.
[52] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:
Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics (ACL). 7871–7880.
[53] Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, and Ying Shan. 2023. TagGPT: Large
Language Models are Zero-shot Multimodal Taggers. CoRR abs/2304.03022
(2023).
[54] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Boot-
strapping language-image pre-training with frozen image encoders and large
language models. In International Conference on Machine Learning (ICML). 19730–
19742.
[55] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian
McAuley. 2023. Text is all you need: Learning language representations for
sequential recommendation. In Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD). 1258–1267.
[56] Jian Li, Jieming Zhu, Qiwei Bi, Guohao Cai, Lifeng Shang, Zhenhua Dong, Xin
Jiang, and Qun Liu. 2022. MINER: Multi-interest matching network for news
recommendation. In Findings of the Association for Computational Linguistics
(ACL). 343–352.
[57] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Personalized Prompt Learning for
Explainable Recommendation. arXiv:2202.07371
6573Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
[58] Xiang Li, Chao Wang, Jiwei Tan, Xiaoyi Zeng, Dan Ou, and Bo Zheng. 2020.
Adversarial Multimodal Representation Learning for Click-Through Rate Pre-
diction. In WWW. 827–836.
[59] Xinyi Li, Yongfeng Zhang, and Edward C. Malthouse. 2023. PBNR: Prompt-based
News Recommender System. CoRR abs/2304.07862 (2023).
[60] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous
Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Associ-
ation for Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (ACL/IJCNLP). 4582–4597.
[61] Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, et al .2023. MERT:
Acoustic Music Understanding Model with Large-Scale Self-supervised Training.
CoRR abs/2306.00107 (2023).
[62] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. 2023.
Video-LLaVA: Learning United Visual Representation by Alignment Before
Projection. CoRR abs/2311.10122 (2023).
[63] Jinpeng Lin, Min Zhou, Ye Ma, Yifan Gao, Chenxi Fei, Yangjian Chen, Zhang Yu,
and Tiezheng Ge. 2023. AutoPoster: A Highly Automatic and Content-aware
Design System for Advertising Poster Generation. In ACM MM. 1250–1260.
[64] Chang Liu, Xiaoguang Li, Guohao Cai, Zhenhua Dong, Hong Zhu, and Lifeng
Shang. 2021. Noninvasive self-attention for side information fusion in sequential
recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI). 4249–4256.
[65] Chang Liu, Han Yu, Yi Dong, Zhiqi Shen, Yingxue Yu, Ian Dixon, Zhanning Gao,
Pan Wang, Peiran Ren, Xuansong Xie, Lizhen Cui, and Chunyan Miao. 2020.
Generating Engaging Promotional Videos for E-commerce Platforms (Student
Abstract). In AAAI. 13865–13866.
[66] Dairui Liu, Boming Yang, Honghui Du, Derek Greene, Aonghus Lawlor, Ruihai
Dong, and Irene Li. 2023. RecPrompt: A Prompt Tuning Framework for News
Recommendation Using Large Language Models. CoRR abs/2312.10463 (2023).
[67] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruc-
tion Tuning. In Advances in Neural Information Processing Systems (NeurIPS).
[68] Hu Liu, Jing Lu, Hao Yang, Xiwei Zhao, Sulong Xu, Hao Peng, Zehua Zhang,
Wenjie Niu, Xiaokun Zhu, Yongjun Bao, et al .2020. Category-Specific CNN
for Visual-aware CTR Prediction at JD. com. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD).
2686–2696.
[69] Kang Liu, Feng Xue, Dan Guo, Peijie Sun, Shengsheng Qian, and Richang Hong.
2023. Multimodal graph contrastive learning for multimedia-based recommen-
dation. IEEE Transactions on Multimedia (2023).
[70] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
Graham Neubig. 2023. Pre-train, Prompt, and Predict: A Systematic Survey of
Prompting Methods in Natural Language Processing. ACM Comput. Surv. 55, 9
(2023), 195:1–195:35.
[71] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boost-
ing content-based recommendation with both open-and closed-source large
language models. In Proceedings of the 17th ACM International Conference on
Web Search and Data Mining (WSDM). 452–461.
[72] Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, and Xiao-
Ming Wu. 2024. Discrete Semantic Tokenization for Deep CTR Prediction. In
Proceedings of the ACM Web Conference (WWW).
[73] Qidong Liu, Jiaxi Hu, Yutian Xiao, Jingtong Gao, and Xiangyu Zhao. 2023.
Multimodal Recommender Systems: A Survey. CoRR abs/2302.03883 (2023).
https://doi.org/10.48550/ARXIV.2302.03883 arXiv:2302.03883
[74] Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiao-Ming Wu. 2022. Boosting deep
CTR prediction with a plug-and-play pre-trainer for news recommendation. In
Proceedings of the 29th International Conference on Computational Linguistics.
2823–2833.
[75] Shang Liu, Zhenzhong Chen, Hongyi Liu, and Xinghai Hu. 2019. User-video
co-attention network for personalized micro-video recommendation. In The
ACM Web Conference (WWW). 3020–3026.
[76] Xiaoqian Liu, Xiuyun Li, Yuan Cao, Fan Zhang, Xiongnan Jin, and Jinpeng Chen.
2023. Mandari: Multi-Modal Temporal Knowledge Graph-aware Sub-graph
Embedding for Next-POI Recommendation. IEEE International Conference on
Multimedia and Expo (ICME) (2023), 1529–1534.
[77] Yuqing Liu, Yu Wang, Lichao Sun, and Philip S. Yu. 2024. Rec-GPT4V:
Multimodal Recommendation with Large Vision-Language Models. CoRR
abs/2402.08670 (2024).
[78] Yuting Liu, Enneng Yang, Yizhou Dang, Guibing Guo, Qiang Liu, Yuliang Liang,
Linying Jiang, and Xingwei Wang. 2023. ID Embedding as Subtle Features of
Content and Structure for Multimodal Recommendation. CoRR abs/2311.05956
(2023).
[79] Yong Liu, Susen Yang, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong Zhang,
Aixin Sun, and Chunyan Miao. 2021. Pre-training graph transformer with
multimodal side information for recommendation. In Proceedings of the 29th
ACM International Conference on Multimedia (MM). 2853–2861.
[80] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing
Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. 2024.
Sora: A Review on Background, Technology, Limitations, and Opportunities ofLarge Vision Models. arXiv:2402.17177
[81] Yifan Liu, Kangning Zhang, Xiangyuan Ren, Yanhua Huang, Jiarui Jin, Yingjie
Qin, Ruilong Su, Ruiwen Xu, and Weinan Zhang. 2024. An Aligning and Training
Framework for Multimodal Recommendations. CoRR abs/2403.12384 (2024).
[82] Zhuang Liu, Yunpu Ma, Matthias Schubert, Yuanxin Ouyang, and Zhang Xiong.
2022. Multi-modal contrastive pre-training for recommendation. In Proceedings
of the 2022 International Conference on Multimedia Retrieval. 99–108.
[83] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan
Marten, Derek Hoiem, and Aniruddha Kembhavi. 2023. Unified-IO 2: Scaling
Autoregressive Multimodal Models with Vision, Language, Audio, and Action.
CoRR abs/2312.17172 (2023).
[84] Daniele Malitesta, Giandomenico Cornacchia, Claudio Pomo, Felice Antonio
Merra, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. Formalizing Multime-
dia Recommendation through Multimodal Deep Learning. CoRR abs/2309.05273
(2023).
[85] Masato Mita, Soichiro Murakami, Akihiko Kato, and Peinan Zhang. 2023. CAM-
ERA: A Multimodal Dataset and Benchmark for Ad Text Generation. CoRR
abs/2309.12030 (2023).
[86] Soichiro Murakami, Sho Hoshino, and Peinan Zhang. 2023. Natural Language
Generation for Advertising: A Survey. arXiv:2306.12719
[87] Yongxin Ni, Yu Cheng, Xiangyan Liu, Junchen Fu, Youhua Li, Xiangnan He,
Yongfeng Zhang, and Fajie Yuan. 2023. A Content-Driven Micro-Video Recom-
mendation Dataset at Scale. CoRR abs/2309.15379 (2023).
[88] OpenAI. 2023. ChatGPT. https://chat.openai.com/chat.
[89] R OpenAI. 2023. Gpt-4 technical report. arxiv 2303.08774. View in Article 2, 5
(2023).
[90] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec,
et al.2023. DINOv2: Learning Robust Visual Features without Supervision.
CoRR abs/2304.07193 (2023).
[91] Yanjun Qin, Yuchen Fang, Haiyong Luo, Fang Zhao, and Chenxing Wang. 2022.
Next Point-of-Interest Recommendation with Auto-Correlation Enhanced Multi-
Modal Transformer Network. Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR).
[92] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-
hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al .
2021. Learning transferable visual models from natural language supervision.
InInternational Conference on Machine Learning (ICML). PMLR, 8748–8763.
[93] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al .2018.
Improving language understanding by generative pre-training. (2018).
[94] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et al .2019. Language models are unsupervised multitask learners.
OpenAI blog 1, 8 (2019), 9.
[95] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text transformer. Journal of Machine
Learning Research 21, 140 (2020), 1–67.
[96] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan,
Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Tran, Jonah Samost, et al .
2024. Recommender systems with generative retrieval. Advances in Neural
Information Processing Systems (NeurIPS) 36 (2024).
[97] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
2022. Hierarchical text-conditional image generation with clip latents. arXiv
preprint arXiv:2204.06125 1, 2 (2022), 3.
[98] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Gener-
ation. In Proceedings of the 38th International Conference on Machine Learning
(ICML), Vol. 139. 8821–8831.
[99] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models.
InIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
10674–10685.
[100] Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. 2023.
LaMP: When Large Language Models Meet Personalization. CoRR (2023).
[101] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019.
wav2vec: Unsupervised Pre-Training for Speech Recognition. In 20th Annual
Conference of the International Speech Communication Association (Interspeech).
3465–3469.
[102] Yu Shang, Chen Gao, Jiansheng Chen, Depeng Jin, Meng Wang, and Yong Li.
2023. Learning fine-grained user interests for micro-video recommendation.
InProceedings of the 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR). 433–442.
[103] Tiancheng Shen, Jia Jia, Yan Li, Hanjie Wang, and Bo Chen. 2020. Enhancing
music recommendation with social media content: an attentive multimodal
autoencoder approach. In 2020 International Joint Conference on Neural Networks
(IJCNN). IEEE, 1–8.
[104] Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, Jieming Zhu, and Xi Xiao. 2024. PMG:
Personalized Multimodal Generation with Large Language Models. In The ACM
Web Conference (WWW).
6574KDD ’24, August 25–29, 2024, Barcelona, Spain Qijiong Liu et al.
[105] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer
Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with
Automatically Generated Prompts. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP). 4222–4235.
[106] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Woj-
ciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. FLAVA: A Foundational
Language And Vision Alignment Model. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR). 15617–15629.
[107] Anima Singh, Trung Vu, Raghunandan H. Keshavan, Nikhil Mehta, Xinyang Yi,
Lichan Hong, Lukasz Heldt, Li Wei, Ed H. Chi, and Maheswaran Sathiamoorthy.
2023. Better Generalization with Semantic IDs: A case study in Ranking for
Recommendations. CoRR abs/2306.08121 (2023).
[108] Mingyang Song, Haiyun Jiang, Shuming Shi, Songfang Yao, Shilong Lu, Yi Feng,
Huafeng Liu, and Liping Jing. 2023. Is ChatGPT A Good Keyphrase Generator?
A Preliminary Study. CoRR abs/2303.13001 (2023).
[109] Xuemeng Song, Chun Wang, Changchang Sun, Shanshan Feng, Min Zhou, and
Liqiang Nie. 2023. MM-FRec: Multi-Modal Enhanced Fashion Item Recom-
mendation. IEEE Transactions on Knowledge and Data Engineering 35 (2023),
10072–10084.
[110] Janne Spijkervet and John Ashley Burgoyne. 2021. Contrastive Learning of
Musical Representations. In Proceedings of the 22nd International Society for
Music Information Retrieval Conference (ISMIR). 673–681.
[111] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
2019. Vl-bert: Pre-training of generic visual-linguistic representations. arXiv
preprint arXiv:1908.08530 (2019).
[112] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng
Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder
representations from transformer. In Proceedings of the 28th ACM International
Conference on Information and Knowledge Management (CIKM). 1441–1450.
[113] Wenqi Sun, Ruobing Xie, Shuqing Bian, Wayne Xin Zhao, and Jie Zhou. 2023.
Universal Multi-modal Multi-domain Pre-trained Recommendation. CoRR
abs/2311.01831 (2023).
[114] Zhulin Tao, Yinwei Wei, Xiang Wang, Xiangnan He, Xianglin Huang, and
Tat-Seng Chua. 2020. Mgat: Multimodal graph attention network for recom-
mendation. Information Processing & Management 57, 5 (2020), 102277.
[115] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[116] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al .2023. Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 (2023).
[117] Bayu Distiawan Trisedya, Jianzhong Qi, Wei Wang, and Rui Zhang. 2022. GCP:
Graph Encoder With Content-Planning for Sentence Generation From Knowl-
edge Bases. IEEE Trans. Pattern Anal. Mach. Intell. 44, 11 (2022), 7521–7533.
https://doi.org/10.1109/TPAMI.2021.3118703
[118] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong
Xie. 2023. AnyText: Multilingual Visual Text Generation And Editing. CoRR
abs/2311.03054 (2023).
[119] Aaron Van Den Oord, Oriol Vinyals, et al .2017. Neural discrete representation
learning. Advances in Neural Information Processing Systems (NeurIPS) 30 (2017).
[120] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended
Embodied Agent with Large Language Models. CoRR abs/2305.16291 (2023).
[121] Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, Tianxiang
Li, Jun Yuan, Rui Zhang, Hai-Tao Zheng, and Shu-Tao Xia. 2023. Missrec: Pre-
training and transferring multi-modal interest-aware sequence representation
for recommendation. In Proceedings of the 31st ACM International Conference on
Multimedia (MM). 6548–6557.
[122] Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low,
Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng.
2024. MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation. CoRR
abs/2401.04468 (2024).
[123] Ye Wang, Jiahao Xun, Mingjie Hong, Jieming Zhu, Tao Jin, Wang Lin, Haoyuan
Li, Linjun Li, Yan Xia, Zhou Zhao, and Zhenhua Dong. 2024. EAGER: Two-
Stream Generative Recommender with Behavior-Semantic Collaboration. In
Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD).
[124] Zhenduo Wang, Yuancheng Tu, Corby Rosset, Nick Craswell, Ming Wu, and
Qingyao Ai. 2023. Zero-shot Clarifying Question Generation for Conversational
Search. In Proceedings of the ACM Web Conference (WWW). 3288–3298.
[125] Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun,
Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, and Xianfeng Tang. 2024.
Towards Unified Multi-Modal Personalization: Large Vision-Language Models
for Generative Recommendation and Beyond. CoRR (2024).
[126] Wei Wei, Chao Huang, Lianghao Xia, and Chuxu Zhang. 2023. Multi-modal
self-supervised learning for recommendation. In Proceedings of the ACM WebConference 2023. 790–800.
[127] Wei Wei, Jiabin Tang, Lianghao Xia, Yangqin Jiang, and Chao Huang. 2024.
PromptMM: Multi-Modal Knowledge Distillation for Recommendation with
Prompt-Tuning. In Proceedings of the ACM on Web Conference (WWW). 3217–
3228.
[128] Yinwei Wei, Wenqi Liu, Fan Liu, Xiang Wang, Liqiang Nie, and Tat-Seng Chua.
2023. Lightgt: A light graph transformer for multimedia recommendation. In
Proceedings of the 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR). 1508–1517.
[129] Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-
Seng Chua. 2021. Contrastive Learning for Cold-Start Recommendation. CoRR
abs/2107.05315 (2021).
[130] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. 2020.
Graph-Refined Convolutional Network for Multimedia Recommendation with
Implicit Feedback. In The 28th ACM International Conference on Multimedia
(MM). 3541–3549.
[131] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-
Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for per-
sonalized recommendation of micro-video. In Proceedings of the 27th ACM
International Conference on Multimedia (MM). 1437–1445.
[132] Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie.
2019. Neural news recommendation with multi-head self-attention. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing
and the International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP). 6389–6394.
[133] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering
news recommendation with pre-trained language models. In Proceedings of
the 44th international ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR). 1652–1656.
[134] Chuhan Wu, Fangzhao Wu, Tao Qi, Chao Zhang, Yongfeng Huang, and Tong
Xu. 2022. MM-Rec: Visiolinguistic Model Empowered Multimodal News Rec-
ommendation. In The 45th International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR). 2560–2564.
[135] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang,
Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards open-world recom-
mendation with knowledge augmentation from large language models. arXiv
preprint arXiv:2306.10933 (2023).
[136] Fangxiong Xiao, Lixi Deng, Jingjing Chen, Houye Ji, Xiaorui Yang, Zhuoye Ding,
and Bo Long. 2022. From Abstract to Details: A Generative Multimodal Fusion
Framework for Recommendation. In MM. 258–267.
[137] Shitao Xiao, Zheng Liu, Yingxia Shao, Tao Di, Bhuvan Middha, Fangzhao Wu,
and Xing Xie. 2022. Training Large-Scale News Recommenders with Pretrained
Language Models in the Loop. In The 28th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD). 4215–4225.
[138] Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Mingchen Cai, Wayne Xin
Zhao, and Ji-Rong Wen. 2024. Prompting Large Language Models for Recom-
mender Systems: A Comprehensive Framework and Empirical Analysis. CoRR
abs/2401.04997 (2024).
[139] Song Xu, Haoran Li, Peng Yuan, Yujia Wang, Youzheng Wu, Xiaodong He, Ying
Liu, and Bowen Zhou. 2021. K-PLUG: Knowledge-injected Pre-trained Language
Model for Natural Language Understanding and Generation in E-Commerce. In
Findings of EMNLP. 1–17.
[140] Jiahao Xun, Shengyu Zhang, Zhou Zhao, Jieming Zhu, Qi Zhang, Jingjie Li,
Xiuqiang He, Xiaofei He, Tat-Seng Chua, and Fei Wu. 2021. Why do we click:
visual impression-aware news recommendation. In Proceedings of the 29th ACM
International Conference on Multimedia (MM). 3881–3890.
[141] Guipeng Xv, Si Chen, Chen Lin, Wanxian Guan, Xingyuan Bu, Xubin Li, Hongbo
Deng, Jian Xu, and Bo Zheng. 2022. Visual Encoding and Debiasing for CTR
Prediction. In Proceedings of the 31st ACM International Conference on Information
& Knowledge Management (CIKM). 4615–4619.
[142] Shiquan Yang, Rui Zhang, Sarah M. Erfani, and Jey Han Lau. 2022. An Inter-
pretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue
Generation. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (ACL). 4918–4935.
[143] Xiao Yang, Tao Deng, Weihan Tan, Xutian Tao, Junwei Zhang, Shouke Qin, and
Zongyao Ding. 2019. Learning Compositional, Visual and Relational Represen-
tations for CTR Prediction in Sponsored Search. In CIKM. 2851–2859.
[144] Zhiguang Yang, Lu Wang, Chun Gan, Liufang Sang, and et al. 2023. Parallel
Ranking of Ads and Creatives in Real-Time Advertising Systems. CoRR (2023).
[145] Dong Yao, Jieming Zhu, Jiahao Xun, Shengyu Zhang, Zhou Zhao, Liqun Deng,
Wenqiao Zhang, Zhenhua Dong, and Xin Jiang. 2024. MART: Learning Hierarchi-
cal Music Audio Representations with Part-Whole Transformer. In Companion
Proceedings of the ACM on Web Conference (WWW). 967–970.
[146] Zixuan Yi, Xi Wang, Iadh Ounis, and Craig Macdonald. 2022. Multi-modal
Graph Contrastive Learning for Micro-video Recommendation. Proceedings of
the 45th International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR) (2022).
6575Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey KDD ’24, August 25–29, 2024, Barcelona, Spain
[147] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
Yonghui Wu. 2022. CoCa: Contrastive Captioners are Image-Text Foundation
Models. Trans. Mach. Learn. Res. 2022 (2022).
[148] Licheng Yu, Jun Chen, Animesh Sinha, Mengjiao Wang, Yu Chen, Tamara L.
Berg, and Ning Zhang. 2022. CommerceMM: Large-Scale Commerce Multi-
Modal Representation Learning with Omni Retrieval. In The 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD). 4433–4442.
[149] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu
Pan, and Yongxin Ni. 2023. Where to go next for recommender systems? id-
vs. modality-based recommender models revisited. In Proceedings of the 46th
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR). 2639–2649.
[150] Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and Tie-Yan Liu. 2021.
MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training. In
Findings of the Association for Computational Linguistics (ACL). 791–800.
[151] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, et al .2024. AnyGPT: Unified
Multimodal LLM with Discrete Sequence Modeling. CoRR abs/2402.12226 (2024).
[152] Lingzi Zhang, Xin Zhou, and Zhiqi Shen. 2023. Multimodal pre-training frame-
work for sequential recommendation via contrastive learning. arXiv preprint
arXiv:2303.11879 (2023).
[153] Qi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, Jieming Zhu, Zhaowei Wang,
and Xiuqiang He. 2021. UNBERT: User-News Matching BERT for News Recom-
mendation.. In IJCAI, Vol. 21. 3356–3362.
[154] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli
Ouyang, and Xiangyu Yue. 2023. Meta-Transformer: A Unified Framework for
Multimodal Learning. CoRR abs/2307.10802 (2023).
[155] Zizhuo Zhang and Bang Wang. 2023. Prompt Learning for News Recommenda-
tion. In Proceedings of the 46th International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR). 227–237.
[156] Guoshuai Zhao, Hao Fu, Ruihua Song, Tetsuya Sakai, Zhongxia Chen, Xing Xie,
and Xueming Qian. 2019. Personalized Reason Generation for Explainable SongRecommendation. ACM Trans. Intell. Syst. Technol. 10, 4 (2019), 41:1–41:21.
[157] Hongyu Zhou, Xin Zhou, Zhiwei Zeng, Lingzi Zhang, and Zhiqi Shen. 2023.
A Comprehensive Survey on Multimodal Recommender Systems: Taxonomy,
Evaluation, and Future Directions. CoRR abs/2302.04473 (2023).
[158] Jianghui Zhou, Ya Gao, Jie Liu, Xuemin Zhao, Zhaohua Yang, Yue Wu, and
Lirong Shi. 2024. GCOF: Self-iterative Text Generation for Copywriting Using
Large Language Model. arXiv:2402.13667
[159] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning
to Prompt for Vision-Language Models. Int. J. Comput. Vis. 130, 9 (2022), 2337–
2348.
[160] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and
Mrinmaya Sachan. 2023. Controlled Text Generation with Natural Language
Instructions. In Proceedings of International Conference on Machine Learning
(ICML). 42602–42613.
[161] Xin Zhou and Zhiqi Shen. 2023. A tale of two graphs: Freezing and denoising
graph structures for multimodal recommendation. In Proceedings of the 31st
ACM International Conference on Multimedia (MM). 935–943.
[162] Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi
Xiao, and Rui Zhang. 2022. BARS: Towards Open Benchmarking for Recom-
mender Systems. In The 45th International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR). 2912–2923.
[163] Jieming Zhu, Xin Zhou, Chuhan Wu, Rui Zhang, and Zhenhua Dong. 2024.
Multimodal Pretraining and Generation for Recommendation: A Tutorial. In
Companion Proceedings of the ACM on Web Conference 2024 (WWW). 1272–1275.
[164] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan
Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. 2023. TryOn-
Diffusion: A Tale of Two UNets. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR). 4606–4615.
[165] Yushan Zhu, Huaixiao Zhao, Wen Zhang, Ganqiang Ye, Hui Chen, Ningyu
Zhang, and Huajun Chen. 2021. Knowledge Perceived Multi-modal Pretraining
in E-commerce. In ACM Multimedia Conference (MM). 2744–2752.
6576