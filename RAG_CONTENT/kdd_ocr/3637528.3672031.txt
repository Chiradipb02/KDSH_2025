Scalable Differentiable Causal Discovery in the Presence of
Latent Confounders with Skeleton Posterior
Pingchuan Ma
pmaab@cse.ust.hk
HKUST
Hong Kong SARRui Ding‚àó
juding@microsoft.com
Microsoft Research
Beijing, ChinaQiang Fu
qifu@microsoft.com
Microsoft Research
Beijing, ChinaJiaru Zhang
jiaruzhang@sjtu.edu.cn
Shanghai Jiao Tong
University
Shanghai, China
Shuai Wang
shuaiw@cse.ust.hk
HKUST
Hong Kong SARShi Han
shihan@microsoft.com
Microsoft Research
Beijing, ChinaDongmei Zhang
dongmeiz@microsoft.com
Microsoft Research
Beijing, China
ABSTRACT
Differentiable causal discovery has made significant advancements
in the learning of directed acyclic graphs. However, its application to
real-world datasets remains restricted due to the ubiquity of latent
confounders and the requirement to learn maximal ancestral graphs
(MAGs). To date, existing differentiable MAG learning algorithms
have been limited to small datasets and failed to scale to larger ones
(e.g., with more than 50 variables).
The key insight in this paper is that the causal skeleton, which
is the undirected version of the causal graph, has potential for im-
proving accuracy and reducing the search space of the optimization
procedure, thereby enhancing the performance of differentiable
causal discovery. Therefore, we seek to address a two-fold challenge
to harness the potential of the causal skeleton for differentiable
causal discovery in the presence of latent confounders: (1) scalable
and accurate estimation of skeleton and (2) universal integration of
skeleton estimation with differentiable causal discovery.
To this end, we propose SPOT (Skeleton Posterior-guided OpTi-
mization), a two-phase framework that harnesses skeleton posterior
for differentiable causal discovery in the presence of latent con-
founders. On the contrary to a ‚Äúpoint-estimation‚Äù, SPOT seeks to
estimate the posterior distribution of skeletons given the dataset.
It first formulates the posterior inference as an instance of amor-
tized inference problem and concretizes it with a supervised causal
learning (SCL)-enabled solution to estimate the skeleton posterior.
To incorporate the skeleton posterior with differentiable causal dis-
covery, SPOT then features a skeleton posterior-guided stochastic
optimization procedure to guide the optimization of MAGs.
Extensive experiments on various datasets show that SPOT sub-
stantially outperforms SOTA methods for MAG learning. SPOT also
demonstrates its effectiveness in the accuracy of skeleton posterior
‚àóCorresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . . $15.00
https://doi.org/10.1145/3637528.3672031estimation in comparison with non-parametric bootstrap-based,
or more recently, variational inference-based methods. Finally, we
observe that the adoption of skeleton posterior exhibits strong
promise in various causal discovery tasks.
CCS CONCEPTS
‚Ä¢Mathematics of computing ‚ÜíBayesian networks; Causal
networks; ‚Ä¢Computing methodologies ‚ÜíCausal reasoning and
diagnostics .
KEYWORDS
causal discovery, Bayesian network
ACM Reference Format:
Pingchuan Ma, Rui Ding, Qiang Fu, Jiaru Zhang, Shuai Wang, Shi Han,
and Dongmei Zhang. 2024. Scalable Differentiable Causal Discovery in the
Presence of Latent Confounders with Skeleton Posterior. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3672031
1 INTRODUCTION
Causal discovery in the presence of latent confounders is a long-
standing problem [ 35,43]. Under this setting, the causal relations
are typically represented by a maximal ancestral graph (MAG) [ 35],
a special class of acyclic directed mixed graphs (ADMGs). MAG
learning has historically been conducted by either constraint-based
methods, such as FCI [ 35,43], RFCI [ 10] and ICD [ 32], or by score-
based methods, such as M3HC [ 37], AGIP [ 7] and GPS [ 9]. In recent
years, differentiable causal discovery has emerged as a promising
approach to enhance the accuracy and efficiency of existing meth-
ods [ 39,45]. By recasting the combinatorial constraint of graph
structure into a differentiable form, continuous optimization tech-
niques can be applied in an ‚Äúout-of-the-box‚Äù manner.
The goal of differentiable methods is to identify the ancestral
ADMG, and apply maximal ancestral projection, a standard process,
to generate the corresponding MAG. Despite the encouraging re-
sults achieved thus far, they struggle with large-scale causal graphs,
particularly those containing more than 20 variables. One widely-
used method, ABIC [ 5], acknowledges its limitations with datasets
of only 10‚Äì15 variables (i.e., a causal graph with 10-15 nodes), which
restricts its broader applicability. Likewise, N-ADMG [ 3], another
 
2148
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Pingchuan Ma et al.
Table 1: Boosting differentiable causal discovery with skele-
ton information. (avg. on ten datasets with 50-100 variables.)
Metho
d Skeleton
F1 Arr
owhead F1 T
ail F1
ABIC 0.84 0.76 0.67
w/
True Skeleton 1.00 +19% 0.96 +26% 0.96 +33%
w/
FCI 0.87 +4% 0.84 +11% 0.71 +6%
SPO
T 0.91 +8% 0.86 +13% 0.78 +16%
differentiable method for ADMG learning, suffers from similar scal-
ability issues. Upon closer examination, we find that these methods
are both inefficient and inaccurate when working with large-scale
causal graphs. For example, ABIC may take days to converge and
sometimes produces cyclic graphs when dealing with 50 nodes. This
is likely due to the inherent challenges of learning from causally
insufficient data, as ABIC must manage a more complicated ob-
jective function, additional optimization variables, and complex
constraints. Consequently, the search space of large datasets poses
significant challenges for standard optimization techniques, such
as L-BFGS [15] or gradient descent.
On the other hand, it has been demonstrated that adjacency in-
formation (i.e., the skeleton) can be utilized as a pre/post-processing
step to facilitate differentiable methods for learning DAGs (directed
acyclic graphs). For instance, CDRL [ 46] uses the CAM [ 6] to
rule out spurious edges while ML4S [ 26] investigates using skele-
ton in a pre-processing step to preclude superfluous variables for
NOTEARS [ 45]. These approaches show the potential for boosting
differentiable DAG learning by leveraging skeleton information.
Key Observation. These promising results on DAG learning mo-
tivate us to investigate whether MAG learning can benefit from a
high-quality estimation of the skeleton. To this end, we implement
two ABIC variants, one using the ground-truth skeleton as prior
knowledge and the other using the skeleton learnt by FCI, to vali-
date our hypotheses. In both variants, we black out the edges that
are not in the skeleton and then run ABIC to only optimize the
remaining edges. In this way, the optimization procedure is only
applied on a subset of variables rather than the entire adjacency
matrix. Then, we evaluate them on ten synthetic datasets with
50-100 variables and 1000 samples (see Table 1). In particular, we
find that ABIC is impressively accurate when the true skeleton is
known (3rd row in Table 1). This result, to a considerable extent,
validates our hypothesis that the skeleton can be used to reduce
the search space of the optimization procedure and therefore boost
the performance of differentiable MAG learning.
On the contrary, we also find that when the ground-truth skele-
ton is unknown and the used skeleton is learned by FCI (4th row
in Table 1), the improvement becomes modest because many erro-
neous/missing edges induced by FCI propagate to the subsequent
optimization procedure. This indicates the practical hurdle of using
the skeleton to facilitate differentiable causal discovery. Indeed, due
to finite samples, learning a high-quality skeleton is challenging.
Simply putting a ‚Äúpoint estimation ‚Äù of the skeleton as a hard con-
straint on the optimization procedure can result in considerable
missing edges (i.e., false negatives) and finally impair performance.
The above preliminary results shed light on a possible solution
that synergistically combines differentiable causal discovery with
skeleton information, while alleviating error propagations. Recently,much research has promoted the value of posterior distribution
of DAGs [ 11,24,25]. Enlightened by these works, we advocate
estimating a posterior distribution (i.e.,ùëù(ùëÜ| D) whereùëÜis the
skeleton andDis the dataset) over all skeletons to replace the
conventional ‚Äúpoint estimation.‚Äù (i.e., arg maxùëÜùëù(ùëÜ| D) ) The
posterior effectively quantifies epistemic uncertainty and the degree
of confidence to any skeletons w.r.t. the given dataset. Nonetheless,
unlike DAGs, skeletons over causally insufficient variables do not
have an explicit form of likelihood function (i.e., ùëù(D,ùëÜ)).
To address these challenges, we propose SPOT (Skeleton Posterior-
guided OpTimization) as a two-phase framework for facilitating
differentiable causal discovery in the presence of latent confounders.
SPOT first performs amortized variational inference to estimate
skeleton posterior, SPOT then employs a novel optimization proce-
dure for boosting subsequent differentiable causal discovery. Specif-
ically, SPOT leverages a recent advancement to amortize the varia-
tional inference into the joint distribution of data and corresponding
skeletonùëù(D,ùëÜ)and alleviates the need of an explicit likelihood
function. Then, it concretizes the amortized inference with a su-
pervised model to estimate the skeleton posterior. To effectively
facilitate the optimization procedure, SPOT employs the skeleton
posterior to stochastically update variables in each optimization
step, instead of deterministically updating optimization variables
with gradients.
As shown in the last row of Table 1, SPOT improves the perfor-
mance of differentiable causal discovery methods by a notable mar-
gin. In Sec. 4, we conduct extensive experiments on various large-
scale datasets and show that SPOT delivers 8% improvement on
skeleton F1 score and 13% and 16% improvement on arrowhead and
tail F1 scores, respectively, which are representative metrics for eval-
uating the accuracy of the learned ADMGs. We also demonstrate
that the skeleton posterior estimated by SPOT is highly accurate
compared to other variational inference-based and non-parametric
bootstrap-based solutions. Finally, we explore the versatile applica-
tions of SPOT in MAG learning for non-linear causally insufficient
datasets and also in DAG learning methods. Our empirical results
indicate the strong potential of SPOT in such scenarios.
In summary, we make the following contributions:
(1)Conceptually, we advocate a novel focus of using the skele-
ton posterior to facilitate differentiable causal discovery in the
presence of latent confounders.
(2)Technically, we formulate the problem of skeleton posterior
estimation under amortized inference framework and propose
a supervised learning-based solution to estimate the skeleton
posterior from observational data.
(3)On the basis of the skeleton posterior, we propose SPOT, a novel
stochastic optimization procedure, to facilitate differentiable
causal discovery in the presence of latent confounders, which
incorporates the skeleton posterior in a stochastic manner.
(4)Empirically, SPOT demonstrates superior performance on nearly
all evaluation metrics and various datasets, substantially im-
proving upon its counterpart. We also explore the extension of
SPOT to other causal discovery settings, including non-linear
ADMG and DAG. Our results show that SPOT consistently
improves state-of-the-art methods in these settings.
 
2149Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Open-source and Extended Version. Our code will be made pub-
licly available at [ 31]. We also maintain an extended version of this
paper on arXiv [ 27] with additional experiments and discussions.
2 PRELIMINARY
As in many previous works [ 5,7,9,37], we focus on discovering
MAG from a linear Gaussian structural causal model (SCM) in the
presence of latent confounders and assume the absence of selection
bias (no undirected edges in ADMGs). In this section, we introduce
preliminary knowledge of linear Gaussian SCM with latent con-
founders, differentiable causal discovery and amortized inference.
2.1 Linear Gaussian SCM with Latent
Confounders
We first start with the definition of a linear Gaussian SCM without
latent confounders. Consider a linear SCM with ùëëobservable vari-
ables parameterized by a coefficient matrix ùõø‚ààRùëë√óùëë. The SCM
can be written as
ùëâùëñ‚Üê‚àëÔ∏Å
ùëâùëó‚ààPAùëñùõøùëóùëñùëâùëó+ùúñùëñ (1)
where PAùëñare the parents of ùëâùëñandùúñùëñis a noise term that is mutu-
ally independent of all other noise terms.
The noise term ùúñùëñis mutually independent of others if and only
ifùëâùëñhas no latent confounder. Otherwise, ùúñùëñis correlated with ùúñùëóif
ùëâùëóshares a latent confounder with ùëâùëñ. When the noise terms are
Gaussian, the correlation can be expressed by a covariance matrix
ùõΩ=E[ùúñùúñùëá]and the joint distribution marginalized over observable
variables ùëΩOforms a zero-mean multivariate Gaussian distribution
with covariance matrix as Œ£=(ùêº‚àíùõø)‚àíùëáùõΩ(ùêº‚àíùõø)‚àí1. The induced
graphùê∫is an ADMG and contains two types of edges, including
directed edges (‚Üí) and bidirected edges ( ‚Üî), which implies two
adjacency matrices, namely ùê∑andùêµ. In the special case where
there is no latent confounder, the ADMG is equivalent to a DAG
and the adjacency matrix of bidirected edges ùêµis all zeros.ùëâùëñ‚Üíùëâùëó
exists inùê∫andùê∑ùëñùëó=1if and only if ùõøùëñùëó‚â†0.ùëâùëñ‚Üîùëâùëóexists inùê∫
andùêµùëñùëó=1if and only if ùõΩùëñùëó‚â†0. It is commonly assumed that
ùõøùëñùëó=ùõøùëóùëñ=ùõΩùëñùëó=ùõΩùëóùëñ=0if and only if ùê∑ùëñùëó=ùê∑ùëóùëñ=ùêµùëñùëó=ùêµùëóùëñ=0.
Remark. In the context of ADMGs, the noise term ùúñdoes not exclu-
sively represent the exogenous variables. In particular, ùúñùëñin Eqn. 1 in-
cludes both the exogenous variable and the confounding effect from ùëâùë¢
(the latent confounder). Similarly, the noise term on another variable
ùëâùëó,ùúñùëó, also includes its own exogenous variable and the confounding
effect fromùëâùë¢. Therefore,ùúñùëñandùúñùëóare jointly influenced by the con-
founding effect from ùëâùë¢, which makes them dependent. We provide a
more detailed explanation below.
Consider the causal graph ùëâùë¢ùëñ‚Üíùëâùëñ‚Üêùëâùë¢‚Üíùëâùëó‚Üêùëâùë¢ùëó. Without
loss of generality on the linear setting, we have ùëâùëñ=ùëâùë¢ùëñ+ùëéùëâùë¢and
ùëâùëó=ùëâùë¢ùëó+ùëèùëâùë¢, whereùëé,ùëèare non-zero coefficients. Given that only
ùëâùëñandùëâùëóare observed, the resulting ADMG should be ùëâùëñ‚Üîùëâùëó, based
on Sec. 2.2 of [ 43]. In Eqn. 1,ùëâùëñ=√ç
ùëâùëò‚ààùëÉùëé(ùëâùëñ)ùõøùëòùëñùëâùëò+ùúñùëñwhereùëÉùëé(ùëâùëñ)
are the parents of ùëâùëñin the ADMG. Since there is a bidirectional edge
ùëâùëñ‚Üîùëâùëó, neitherùëâùëñis the parent of ùëâùëónorùëâùëóthe parent of ùëâùëñ. Hence,
ùëÉùëé(ùëâùëñ)=ùëÉùëé(ùëâùëó)=‚àÖ. Thus, we have ùëâùëñ=ùúñùëñandùëâùëó=ùúñùëó. However,
sinceùëâùëñandùëâùëóare dependent, it follows that ùúñùëñandùúñùëómust also be
dependent.According to the causal Markov and faithfulness assumption,
ancestral ADMG, MAG and skeleton can be defined as follows.
Definition 1 (Ancestral Acyclic Directed Mixed Graph). ùê∫is an
ancestral ADMG if it is a mixed graph with directed edges ( ‚Üí)
and bidirected edges ( ‚Üî) and contains no directed cycles or almost
directed cycles [43].
Definition 2 (Maximal Ancestral Graph). An ancestral ADMG
ùê∫is a Maximal Ancestral Graph if for each pair of non-adjacent
nodes, there exists a set of nodes that make them m-separated [ 43].
Definition 3 (Skeleton). An undirected graph ùëÜis a skeleton of an
MAGùê∫ifùëÜis obtained from ùê∫by replacing all directed edges with
undirected edges.
Therefore, two nodes ùëâùëñ,ùëâùëóare adjacent in the skeleton if and
only if‚àÄùíÅ‚äÜùëΩO\{ùëâùëñ,ùëâùëó},ùëâùëñÃ∏‚ä•ùëâùëó|ùíÅ, where‚ä•andÃ∏‚ä•denote
conditional independence and dependence, respectively. Hence, it
is clear that given a dataset whether two nodes are adjacent is not
influenced by the adjacencies of other nodes.
Parameter Estimation of Linear Gaussian SCM. For DAGs,
one easily can estimate the parameters of the SCM by solving a
simple least squares regression problem. However, for ADMGs, the
estimation of the parameters is more challenging due to the pres-
ence of latent confounders. Drton et al . [14] proposed an iterative
algorithm to estimate the parameters of the SCM called Residual
Iterative Conditional Fitting (RICF). RICF generally works for bow-
free ADMGs and ancestral ADMGs, which are our focus in this
paper, are special cases of bow-free ADMGs [ 5]. This algorithm
iteratively fits the SCM to the data and updates the covariance ma-
trix. The algorithm is guaranteed to converge to a local minimum
when the corresponding ADMGs are ancestral.
2.2 Differentiable Causal Discovery
Score-based methods aim to maximize the score (e.g., log-likelihood)
of the graph on the given data, which can be written in the following
form:
arg max
ùê∫ùëì(ùê∫)s.t.ùê∫is acyclic (2)
whereùëì(¬∑)is the score function. Given the acyclicity constraint,
the optimization process is combinatorial. Recently, differentiable
methods reformulate this combinatorial constraint into a constraint
‚ÑéDAG(ùëä)=tr(ùëíùëä‚ó¶ùëä)‚àíùëësuch that
‚ÑéDAG(ùëä)=0‚áê‚áíùê∫is acyclic (3)
whereùëë=|ùëΩO|andùëäis a weighted adjacency matrix of ùê∫. For
linear SCM, an element in ùëärepresents the linear coefficient. As a
continuous optimization with equality constraints, the augmented
Lagrangian method (ALM) is commonly used to convert the con-
strained optimization problem into several unconstrained subprob-
lems and use standard optimizers to solve them separately [45].
Despite the success of differentiable methods for DAG learning,
the algebraic characterization in Eqn. 3 cannot be directly applied
to ADMGs (and MAGs). As aforementioned, ADMGs requires two
adjacency matrices, ùê∑andùêµ, to represent directed edges and bidi-
rected edges, respectively. To extend the algebraic characterization
to ADMGs, ABIC [5] modified it as
‚ÑéADMG(ùê∑,ùêµ)=tr(ùëíùê∑)‚àíùëë+sum(ùëíùê∑‚ó¶ùêµ) (4)
 
2150KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Pingchuan Ma et al.
Algorithm 1: ABIC [5]
Input: DatasetD
Output: Adjacency Matrix ùê∑,ùêµ
1Initializeùõø(1,1),ùõΩ(1,1);
2Define‚Ñé(ùõø,ùõΩ)according to Eqn. 4;
3foreachùë°1=1,¬∑¬∑¬∑,ùëáALMdo
4 foreachùë°2=1,¬∑¬∑¬∑,ùëáRICFdo
5 Update pseudovariables by ùõø(ùë°1,ùë°2),ùõΩ(ùë°1,ùë°2);
6 Constituteùëì(ùõø,ùõΩ);
7ùõø(ùë°1,ùë°2+1),ùõΩ(ùë°1,ùë°2+1)‚Üêarg minùëì(ùõø,ùõΩ);
8 end
9 Updateùõº(ùë°1),ùúå(ùë°1),ùúÜ(ùë°1);
10end
11ùê∑‚Üê1(|ùõø(ùëáALM,ùëáRICF)>ùúî|);
12ùêµ‚Üê1(|ùõΩ(ùëáALM,ùëáRICF)>ùúî|);
13returnùê∑,ùêµ;
where‚ó¶denotes the Hadamard product, i.e., element-wise multi-
plication, and sum(¬∑)is the sum of all elements in a matrix. It has
been proved that ‚ÑéADMG(ùê∑,ùêµ)=0‚áê‚áíùê∫is ancestral ADMG .
In a nutshell, tr(ùëíùê∑)‚àíùëëimplies the standard directed acyclicity
constraint and the last term sum(ùëíùê∑‚ó¶ùêµ)is a term to ensure that
the bidirected edges does not introduce ‚Äúalmost directed cycles‚Äù. In
the following, we use ‚Ñéto denote‚ÑéADMG for simplicity.
Now, we introduce ABIC [ 5] in Alg. 1. To ease presentation, we
omit technical details (e.g., calculations of ùõº,ùúå,ùúÜ , pseudovariables,
and stopping criteria) and refer readers to the original paper for the
full version. The ABIC algorithm aims to maximize the likelihood of
ADMG over the observational data D, ensuring‚ÑéADMG(ùõø,ùõΩ)=0.
It employs two nested loops: the outer loop applies the augmented
Lagrangian method (ALM) to convert the equality-constrained prob-
lem intoùëáALM steps of unconstrained ones (lines 3‚Äì10 in Alg. 1).
After ALM,ùõøandùõΩrepresent the ADMG‚Äôs linear coefficients and co-
variance, respectively. The adjacency matrix is derived by applying
a threshold to remove negligible coefficients (lines 22‚Äì23), yielding
the ancestral ADMG. If a MAG is required, the maximal ancestral
projection, a standard post-processing procedure, is performed.
Inside the inner loop, Alg. 1 iteratively adjust the objective func-
tion (lines 5‚Äì6) and solve the unconstrained optimization problem
accordingly (line 7). In particular, the objective function ùëì(ùõø,ùõΩ)
(line 6) consists of the least square loss to fit the ADMG to the
data, the constraint ‚Ñé(ùõø,ùõΩ), and a regularization term to enforce
sparsity. The form of ùëì(ùõø,ùõΩ)is updated according to the current
pseudovariables (line 5). Then, in the optimization phase (line 7),
optimization algorithms are used in an ‚Äúout-of-the-box‚Äù manner
to find a minimum of ùëì(ùõø,ùõΩ). However, due to the complexity and
non-convex nature of ùëì(ùõø,ùõΩ), it is challenging for standard opti-
mization techniques to find a plausible solution, especially when
the ADMG is large, as we will show in Sec. 4.
2.3 Amortized Inference for Causal Discovery
In the conventional variational inference framework, the posterior
distribution is approximated by a parametric distribution ùëûùúô(ùê∫|D)
with parameters ùúô. Here,ùê∫is the causal graph and Dis the ob-
served data in the context of causal discovery. The parameters arelearned by minimizing the KL divergence between the true poste-
riorùëù(ùê∫|D)and the parametric distribution ùëûùúô(ùê∫|D). Typically,
variational inference requires the likelihood function ùëù(D,ùê∫)to
compute the KL divergence. Recently, amortized inference has been
proposed to alleviate the need of an explicit likelihood function
by introducing a simulator that can yield samples from the joint
distribution ùëù(ùê∫,D)[2]. Lorch et al . [25] follows this line of work
and introduces AVICI, which amortizes the inference process by
introducing a sampler that can generate samples from the simulator
and capture the domain-specific inductive biases that would have
been difficult to characterize (e.g., gene regulatory networks or
non-linear functions of random Fourier features).
Recently, SCL (supervised causal learning) has emerged as a
promising paradigm for causal discovery [ 12,22,26]. Compared
to standard machine learning models, SCL models features many
advantages tailored for causal discovery (e.g., invariant to the per-
mutation of variables). It is worth noting that the concept of ‚Äúper-
mutation invariance‚Äù is different from the one in machine learning
setting. One dataset (a ùëÄ√óùëÅmatrix) is a sample to the SCL model.
It implies that the model is permutation in- and equivariant with
respect to the observation and variable dimensions of the provided
dataset, respectively [ 22]. SCL models can be trained on samples
from a known simulator. In essence, SCL can be viewed as an in-
stantiation of amortized inference when the simulator is in line
with the underlying causal mechanism of the observational data.
3SPOT
As aforementioned, skeleton information can be leveraged to en-
hance the optimization procedure of differentiable causal discovery
in the presence of latent confounders. However, a ‚Äúpoint estimation‚Äù
of the skeleton is prone-to-error. In this section, we propose SPOT
(Skeleton Posterior-guided OpTimization) to leverage a probabilistic
skeleton posterior to guide the optimization of ADMGs.
As shown in Fig. 1, SPOT consists of four steps: ‚ûÄa static simu-
lator first generates an initial set of data/skeleton pairs to obtain an
initial amortized inference model (blue one in Fig. 1). ‚ûÅwhile the ini-
tial amortized inference model already provides a good estimation
of the skeleton posterior, SPOT can optionally use a dynamic simu-
lator to generate more in-domain training instances with respect
to the input dataset. ‚ûÇwith samples from the dynamic simulator,
SPOT adapts the initial amortized inference model to obtain an
updated model (yellow one in Fig. 1) for skeleton posterior infer-
ence.‚ûÉSPOT uses the inferred skeleton posterior to enhance the
optimization procedure of differentiable causal discovery.
Conceptual Complexity. While our solution provides an ad-
ditional layer of complexity compared to standard differentiable
causal discovery, we argue that the level of complexity is generally
manageable and comparable to other causal discovery algorithms.
First, the two stages in our pipeline are independent and do not
involve joint training. Second, constraint-based causal discovery
algorithms, such as FCI, also involve two similar steps: learning the
skeleton in the first step and orienting the edges in the second step.
3.1 Skeleton Variational Objective
LetD={ùíô1,¬∑¬∑¬∑,ùíôùëõ}‚àºùëù(ùëâ)be the observational dataset, where
ùíôùëñis sampled from the joint distribution ùëù(ùëâ)andùëõis sample
 
2151Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
skeleton posteriorguide
differentiable causal discoveryInput DataStatic SimulatorData + Skeletontraining instancestrainingùëÉ(ùëÜ‚à£ùê∑)Data + Skeletontraining instancesdomainadaptionamortized inference modelamortized inference modelDynamic SimulatortrainingùëÉ(ùëÜ‚à£ùê∑)bootstrap
Figure 1: SPOT workflow.
size. We aim to approximate the posterior over skeletons ùëù(ùëÜ|D)
with a variational distribution ùëû(ùëÜ;ùëé)whereùëÜis the (symmetric)
adjacency matrix of the skeleton and ùëéis the variational parameters.
Thus, the skeleton estimation can be decomposed into a set of edge
estimations (i.e., the probability of adjacency) independently. Then,
the variational family of ùëû(ùëÜ;ùëé)is defined as
ùëû(ùëÜ;ùëé)=√ñ
ùëñ<ùëóùëû(ùëÜùëñùëó;ùëéùëñùëó)withùëÜùëñùëó‚àºBernoulli(ùëéùëñùëó). (5)
In this regard, we aim to find an inference model ùëìùúô(D) that pre-
dictsùëé. This procedure can be attained by minimizing the expected
forward KL divergence from ùëù(ùëÜ|D):
min
ùúôEùëù(D)
KL ùëù(ùëÜ|D)||ùëû(ùëÜ;ùëìùúô(D))
(6)
Following the principle of amortized inference [ 2], we amortize
the inference and rewrite the objective as
Eùëù(D)KL ùëù(ùëÜ|D)||ùëû(ùëÜ;ùëìùúô(D))
=Eùëù(D)Eùëù(ùëÜ|D)
logùëù(ùëÜ|D)‚àí logùëû(ùëÜ;ùëìùúô(D))
=‚àíEùëù(ùëÜ)Eùëù(D|ùëÜ)
logùëû(ùëÜ;ùëìùúô(D))
+const.
=‚àíEùëù(D,ùëÜ)
logùëû(ùëÜ;ùëìùúô(D))
+const.(7)
Since the constant does not depend on ùúô, we can merely minimize
L(ùúô)B‚àíEùëù(D,ùëÜ)
logùëû(ùëÜ;ùëìùúô(D))to obtainùúô. In other words,
the problem is recast to train a predictive model ùëìùúô(¬∑):D‚Ü¶‚ÜíùëÜ
over the distribution ùëù(D,ùëÜ).
Estimateùëù(D,ùëÜ)via Static Simulator. Naturally, we can esti-
mateùëù(D,ùëÜ)by using a simulator (e.g., Erd≈ës-R√©nyi random graph
model) then generate the corresponding dataset with pre-defined
functional forms. The simulator serves a direct sampler of ùëù(D,ùëÜ)if
the test dataD(i.e., the input dataset on which we need to conduct
causal discovery) is in the same distribution of the static simulator.
In other words, the pair of Dand the skeleton ùëÜis known to be
drawn from ùëùa priori.
Estimateùëù(D,ùëÜ)via Dynamic Simulator. To further feed the
model with more in-domain training instances, we propose to use
nonparametric bootstrap method [ 16] to estimate a MAG ÀÜùê∫with
a random subset of observational data. Then, we fit the parame-
ters of the underlying SCM (i.e., ùõø,ùõΩ; see Sec. 2.1) on the given
test observational data using the RICF algorithm [ 14]. Finally, we
regenerate the new observational data ÀÜDfrom the fitted SCM. By
repeating the above procedure, we obtain a set of data/MAG pairs
{(ÀÜD1,ÀÜùê∫1),¬∑¬∑¬∑} from which we can derive the samples of ùëù(D,ùëÜ)as{(ÀÜD1,ÀÜùê∫1+ÀÜùê∫ùëá
1),¬∑¬∑¬∑}.1We note that, while the way latent vari-
ables affect the observed variables is implicit, in the linear Gaussian
setting, we can characterize the marginalized distribution over ob-
served variables as a zero-mean multivariate Gaussian distribution
with a covariance matrix defined as Œ£=(ùêº‚àíùõø)‚àíùëáùõΩ(ùêº‚àíùõø)‚àí1, where
ùõøandùõΩare parameters fitted by RICF. In this way, we presume
this dynamic (dataset-dependent) simulator would provide more
relevant training instances with respect to the input dataset D.
Remark. In summary, the static simulator features a robust model
that generalizes well to the test data, even with potential domain shift,
as validated in [ 22,25]. On the other hand, the dynamic simulator
provides more in-domain training instances, which may lead to a
better performance at a fairly lightweight cost on the runtime, as will
be shown shortly in Sec. 3.2.
3.2 Skeleton Posterior Inference
ùê∑!,ùëÜ!training instances from static simulatorahead-of-time trainingInput Datastatic modelruntime trainingùê∑!,ùëÜ!training instances from dynamic simulatordynamic modeldomain adaption
SCL Model
Figure 2: Skeleton Inference Procedure.
In light of Sec. 3.1, the predictive model ùëìùúô(¬∑):D‚Ü¶‚ÜíùëÜcan be
instantiated by any supervised learning model. Enlightened by the
prosperous progress in SCL, we propose to use SCL model to enable
the amortized inference and estimate the skeleton posterior ùëù(ùëÜ|
D). Below, we elaborate the design considerations of instantiating
the SCL model in SPOT.
‚ûÄModel Architecture. Existing SCL methods either use an end-
to-end model (e.g., Transformer) to directly predict the adjacency
matrix [ 22] or use a simple classifier (e.g., xgboost) to predict
the local structure (e.g., the adjacency of two variables or the v-
structure) [ 12,26]. While the design of SPOT is agnostic to the
model architecture, we anticipate to use a simple classifier, as it is
more effective and efficient for estimating ùëùùëñùëóin practice. Hence,
in our implementation, we adopt a ML4S-like cascade model [ 26]
to predict the adjacency and constitute the skeleton posterior. We
formulate the problem of efficient skeleton posterior inference as
1ùê∫andùê∫ùëáare the adjacency matrix and its transpose, ÀÜùê∫1+ÀÜùê∫ùëá
1is the skeleton
adjacency matrix.
 
2152KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Pingchuan Ma et al.
predicting a series of adjacencies and construct the skeleton poste-
rior. Specifically, we aim to estimate
ùëìùúô(D)ùëñùëó=Eùëù(D,ùëÜ)
ùëù(ùëâùëñ‚àíùëâùëó‚ààùëÜ|D)
, (8)
We focus on the probability ùëù(ùëâùëñ‚àíùëâùëó|D) for the adjacency of
ùëâùëñ,ùëâùëógivenD, which helps constitute the skeleton posterior (we
useùëùùëñùëóas an abbreviation of ùëù(ùëâùëñ‚àíùëâùëó| D) ). Besides, we also
reproduce Ke et al . [22] ‚Äôs Transformer-based model, and the results
show that the cascade model is more effective and efficient.
‚ûÅObjective Refinement. Instead of standard causal discovery ob-
jectives (e.g., the higher F1 score), we prefer higher recall over pre-
cision to prevent missed edges. This is because the spurious edges
will be offloaded to and eventually removed by differentiable causal
discovery algorithms. Specifically, even though the SCL model may
predict a spurious edge and corresponding parameters ùõøùëñùëó,ùõΩùëñùëótend
to be non-zero, the differentiable causal discovery algorithm will
gradually move the coefficients to zero to minimize the objective
function. In this regard, the spurious edges are eventually removed.
However, the missed edges are hard to recover due to the sparse
nature of the ADMG. To this end, we can either set a conservative
threshold (subject to the specific design of the SCL model) or apply
label smoothing trick to the training instances to prevent the model
from predicting zero probability for the true edges.
‚ûÇEfficient Training. When using the dynamic simulator, the
model has to be trained from scratch with the training instances
sampled from non-parametric bootstrap. However, this would im-
pose a heavy sampling and training overheads in the runtime. To
alleviate this issue, we adopt a widely-used domain adaptation par-
adigm to reduce the training overheads by leveraging the static
model. Specifically, given a dataset, we only sample a few training
instances from the dynamic simulator. Then, we use the output
and intermediate values of the static model to augment the origi-
nal features of the samples from the dynamic simulator and train
an adapted dynamic model efficiently. In this way, the dynamic
model can rapidly adapt to the new dataset with few-shot training
instances and yields a better performance with mild cost.
3.3 Posterior-guided Optimization
Non-adjacency between ùëâùëñ,ùëâùëóimpliesùõøùëñùëó=ùõøùëóùëñ=ùõΩùëñùëó=ùõΩùëóùëñ=0.
When ground-truth skeleton is available, we enforce extra equality
constraints to optimize. However, the true skeleton is unattainable
in practice, necessitating estimating a skeleton from observational
data. We advocate using the skeleton posterior for optimization as
it encodes epistemic uncertainty.
It is unclear how to incorporate a skeleton posterior in the opti-
mization procedure. The estimated skeleton posterior from Sec. 3.2
is a continuous adjacency matrix ùëùùëñùëó‚àà[0,1]. Thus, we cannot de-
rive meaningful constraints for coefficients/covariances. Formally,
ùëù(ùëâùëñ‚àíùëâùëó|D)=ùëùùëñùëóimplies the probability of the union of (dis-
joint) events ùëÉ(ùõøùëñùëó‚â†0‚à™ùõøùëóùëñ‚â†0‚à™ùõΩùëñùëó‚â†0|D) equalsùëùùëñùëó. Higher
posterior probability indicates higher likelihood of non-zero coeffi-
cients. In this regard, ùëùùëñùëócannot provide any additional information
regarding the value of non-zero coefficients. For example, ùõøùëñùëó=0.1
andùõøùëñùëó=0.9both comply with arbitrary positive posterior ùëùùëñùëó>0
equally well. To account for the probabilistic nature of the skeleton
posterior, we propose the posterior-guided optimizer for structureAlgorithm 2: Skeleton Posterior-guided Optimizer
Input: Skeleton Posterior ùëù, Optimization Variables ùõø,ùõΩ,
Objective Function ùëì, ALM Stepùë°, Temperature
Constantùëê
Output: Optimized Variables ùõø,ùõΩ
1ùõø‚àó,ùõΩ‚àó‚Üêarg minùëì(ùõø,ùõΩ);
2forallùõøùëñùëóandùõΩùëñùëódo
3 ifùõøùëñùëó√óùúïùëì(ùõø,ùõΩ)
ùúïùõøùëñùëó>0then
4ùõøùëñùëó‚Üêùõø‚àó
ùëñùëó
5 else
6 Updateùõøùëñùëó‚Üêùõø‚àó
ùëñùëówith probability(ùëùùëñùëó+ùëê)1
ùë°+1;
7 end
8end
9returnùõø,ùõΩ;
learning as a replacement of the standard optimizer in differentiable
causal discovery (e.g., the gradient descent or the L-BFGS).
Alg. 2 presents the skeleton posterior-guided optimizer. In accor-
dance with many standard differentiable causal discovery methods,
we first optimize the objective function ùëì(ùõø,ùõΩ)with the standard op-
timizer (e.g., gradient descent) to obtain the optimal coefficients and
covariances ùõø‚àó,ùõΩ‚àó(line 1). Then, instead of directly taking ùõø‚àó,ùõΩ‚àóas
the final result, we introduce a stochastic update scheme to update
ùõø,ùõΩbased on the skeleton posterior ùëù(lines 2‚Äì8) as follows:
ùëÉ(ùõøùëñùëó‚Üêùõø‚àó
ùëñùëó)=(
1 ùõøùëñùëó√óùúïùëì(ùõø,ùõΩ)
ùúïùõøùëñùëó>0
(ùëùùëñùëó+ùëê)1
ùë°+1 otherwise(9)
Ifùõøùëñùëó√óùúïùëì(ùõø,ùõΩ)
ùúïùõøùëñùëó>0, the update is accepted unconditionally (line
3‚Äì4); otherwise, it is accepted with a probability of (ùëùùëñùëó+ùëê)1
ùë°+1(line
12‚Äì15; Fig. 3). If rejected, we keep ùõøùëñùëóunchanged (line 13). ùõΩis also
optimized in a similar way.
Conceptually, the overall procedure can be viewed as a modified
simulated annealing process [ 23].ùõøùëñùëó√óùúïùëì(ùõø,ùõΩ)
ùúïùõøùëñùëó>0implies that the
coefficient is ‚Äúmoving to zero‚Äù (i.e., a weakened edge) in this update.
Given that real-world ADMGs are often sparse, ‚Äúmoving-to-zero‚Äù
update is encouraged and therefore accepted unconditionally. This
design consideration is thus an analogy of the unconditional update
in simulated annealing.
0.0 0.2 0.4 0.6 0.8 1.0
pij0.40.60.81.0Update Probabilitystep=1
step=3
step=5
step=10
step=20
Figure 3: The probability of accepting an update for different
ùëùùëñùëóandùë°withùëê=0.1.
In contrast, if the coefficient is ‚Äúmoving to non-zero‚Äù (i.e., a
strengthened edge), the update is accepted with a probability com-
puted byùëùùëñùëóandùë°jointly. Recall that ùëùùëñùëóis the probability of the
 
2153Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
edge existence. The update is thus more encouraged with higher ùëùùëñùëó.
1
ùë°is a analogy of the temperature parameter in simulated annealing.
As shown in Fig. 3, an update is more likely to be rejected with
lowerùë°and higherùë°yields(ùëùùëñùëó+ùëê)1
ùë°‚âà1. With the increase of ùë°,
the probability of accepting an update is gradually similar to what
would be in the vanilla optimization procedure even with a low
ùëùùëñùëó. In this way, the optimization procedure is gradually stabilized.
Furthermore, it is worth noting that the scheme is robust to the
accuracy of the skeleton posterior. For spurious edges, even if ùëùùëñùëóis
high, the optimization procedure of differentiable causal discovery
is still able to move the coefficients to zero given the unconditional
update for ‚Äúedge-weakening‚Äù updates.
Convergence. In the context of differentiable causal discovery, the
concept of convergence is three-fold. First, in accordance with the
ABIC [ 5], asymptotically, convergence to the global optimum of its
objective function implies that the corresponding ADMG is within
the Markov equivalence class of the true ADMG. Second, given the
non-convexity of the objective function, it is not guaranteed that the
optimization procedure converges to the global optimum; instead, it
may result in a local minimum or a saddle point. Finally, we discuss
the impact of the stochastic update scheme on convergence to the
local minima in the following sense: Since the probability of an
update reaches 1 (see Fig. 3), the optimization procedure becomes
equivalent to the vanilla optimizer when ùë°is sufficiently large. In
this regard, the stochastic update scheme does not negatively affect
the convergence to the local minima. In fact, our empirical results
show that our stochastic update scheme yields much better results
than ABIC and also surpasses other SOTA methods.
Probabilistic Update vs. Regularization. A natural question
arises: why not use regularization (e.g., a penalty term) to incorpo-
rate the skeleton posterior? We argue that the penalty term may
face several hurdles in practice. First, the objective function is often
unstable due to the acyclic term (Eqn. 4). Its value can range from
1√ó10‚àí5to1√ó105over different stages of the optimization. Second,
as aforementioned, ùõø,ùõΩare not directly comparable with posterior
probability. This necessitates considerable efforts for implementa-
tion, thus we did not adopt it.
4 EVALUATION
In evaluation, we aim to evaluate the performance of SPOT from
three perspectives: ‚ûäend-to-end causal discovery performance; ‚ûã
standalone skeleton posterior inference performance; ‚ûåextension
to other differentiable causal structure learning algorithms. We also
compare SPOT with a wide range of baselines.
Data Generation. We follow the similar setting from previous
works [ 4,5] to generate synthetic datasets. We generates ADMGs
from Erd≈ës-R√©nyi (ER) model with various nodes whose average
indegree is‚àà [1,1.5]. Each ADMG contains 5‚àí15% bidirected
edges. Then, it is parameterized by the same rules of [ 5]: ifùëâùëñ‚Üíùëâùëó,
ùõøùëñùëóis uniformly sampled from ¬±[0.5,2.0]; ifùëâùëñ‚Üîùëâùëó,ùõΩùëñùëó=ùõΩùëóùëñis
uniformly sampled from ¬±[0.4,0.7]; andùõΩùëñùëñis uniformly sampled
from¬±[0.7,1.2]and add sum(|ùõΩùëñ,‚àíùëñ|)to ensure positive definiteness.
For each ADMG, we generate 1000 samples.
Baselines. We compare our method with a wide range of base-
lines, including constraint-based methods (FCI [ 35], RFCI [ 10] andICD [ 32]), score-based methods (M3HC [ 37] and GPS [ 9]) and also
the differentiable causal discovery methods (ABIC [ 5]). The com-
parison is made on synthetic datasets with 50‚Äì100 nodes. We ob-
serve that ABIC occasionally outputs cyclic graphs with the default
threshold, which is unwarranted for maximal ancestral projection.
In such cases, we use the minimal threshold that produces an acyclic
graph. We also tentatively tried AGIP [ 7] algorithm and excluded
it from comparison, because its preprocessing step is consider-
ably slow (i.e., taking several days) for large datasets. Addition-
ally, we omit algorithms lacking open-source implementation (e.g.,
GreedySPo [ 4]). Since we focus on linear Gaussian data, we omit the
algorithms that do not comply with this assumption (e.g., [ 34,40])
in the main comparison. In Sec. 4.3, we explore the effectiveness of
SPOT on other settings.
Running Time. Many algorithms can take a long time to run on
large datasets and the convergence condition is not always satisfied.
To make the comparison fair, we set a timeout of 24 CPU hours for
each algorithm. For algorithms that take more than 24 CPU hours,
we halt the algorithm and use the best-so-far graph as their output.
Metrics. Under linear Gaussian SCM [ 35], we can only up to iden-
tify a Markov equivalence class. To make all baselines comparable,
for those algorithms that output an MAG, we convert it to a PAG
(Partial Ancestral Graph) and compare it with the true PAG. Fol-
lowing [ 5], we report the F1 score, TPR (True Positive Rate), and
FDR (False Discovery Rate) on the skeleton, arrowheads, and tails.
4.1 End-to-end Comparison
We report the results in Table 2 on ten datasets with nodes sampled
between[50,100]. We observe that the large datasets pose a con-
siderable challenge to all methods while SPOT accurately identifies
the underlying causal structure. ICD fails on one dataset; M3HC
and GPS frequently terminate with convergence warnings on these
datasets. We also observe that constraint-based methods are usually
more powerful at identifying the correct skeleton while all methods
have notable difficulties to accurately identify the arrowheads and
tails. In contrast, SPOT consistently manifests its superiority across
nearly all criteria and yields more precise and stable estimation of
skeletons, arrowheads, and tails. Over the ten datasets, it signifi-
cantly improves the performance of its counterpart, ABIC (with
p-value of 0.008). We also investigate the criterion (i.e., Skeleton‚Äôs
FDR) on which SPOT is sub-optimal while M3HC is the best. Hav-
ing said that, we observe that M3HC also suffers from the lowest
TPR and F1 scores on skeleton. Hence, we conclude that M3HC
may be too strict on confirming an edge while SPOT is more robust
and attains the best F1 scores.
It is worth noting that SPOT‚Äôs strong performance can be at-
tributed to its ability to effectively balance precision and recall in
estimating the underlying causal relationships. Additionally, the
more consistent performance of SPOT could potentially make it
more suitable for various real-world applications where accuracy
and robustness are crucial factors.
Comparison by Node Size. In addition to the aggregated results,
we also report the edge-wise F1 scores under different node sizes
in Fig. 4 where the data point of each node size is the average of
three datasets and the error bar is the standard deviation. First, we
observe that SPOT consistently outperforms all baselines across all
 
2154KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Pingchuan Ma et al.
Table 2: End-to-end comparison (avg. on ten datasets with nodes ‚àà[50,100]).
Metho
dSkeleton Arr
owhead T
ail #Faile
d
F1
TPR FDR F1
TPR FDR F1
TPR FDR Datasets
FCI 0.84¬±0.06
0.79¬±0.10 0.10¬±0.05 0.57¬±0.05
0.78¬±0.15 0.54¬±0.04 0.53¬±0.18
0.46¬±0.20 0.31¬±0.15 0/10
RFCI 0.85¬±0.07
0.77¬±0.09 0.05¬±0.03 0.62¬±0.09
0.76¬±0.12 0.45¬±0.12 0.51¬±0.15
0.42¬±0.17 0.27¬±0.14 0/10
ICD 0.82¬±0.09
0.81¬±0.07 0.17¬±0.11 0.56¬±0.10
0.82¬±0.06 0.57¬±0.12 0.48¬±0.15
0.41¬±0.15 0.39¬±0.18 1/10
M3HC 0.73¬±0.09
0.59¬±0.12 0.03¬± 0.03 0.56¬±0.15
0.46¬±0.16 0.27¬±0.11 0.32¬±0.11
0.24¬±0.10 0.48¬±0.13 0/10
GPS 0.73¬±0.10
0.81¬±0.11 0.33¬±0.11 0.62¬±0.09
0.85¬±0.11 0.50¬±0.10 0.43¬±0.12
0.62¬±0.19 0.66¬±0.10 0/10
ABIC 0.84¬±0.04
0.89¬±0.07 0.19¬±0.07 0.76¬±0.07
0.85¬±0.15 0.29¬±0.11 0.67¬±0.07
0.80¬±0.15 0.40¬±0.07 0/10
SPO
T 0.91¬±0.03 0.94¬±0.04
0.11¬±0.04 0.86¬±0.05 0.89¬±0.10 0.16¬±0.06 0.78¬±0.09 0.86¬±0.13 0.27¬±0.13 0/10
10 20 30 40 50 60 70 80 90 100
Number of nodes0.20.40.60.81.0F1
ABIC SPOT
Figure 4: Comparison of ABIC and SPOT on different node
sizes.
node sizes except for 20-node graphs. For 20-node graphs, poten-
tially due to randomness, though ABIC slightly outperforms SPOT,
the difference is not statistically significant ( ùëù=0.51). In other
settings, especially for large graphs, SPOT outperforms ABIC (the
second-best method) by a large margin (e.g., ùëù=0.001on 90-node
graphs). Second, the performance of ABIC significantly drops on
100-node graphs where it fails to learn a meaningful causal struc-
ture. In contrast, SPOT, by incorporating the skeleton posterior in
the optimization, manifests stable and strong performance across
all node sizes with a small standard deviation.
4.2 Effectiveness of Skeleton Posterior
Inference
Through this experiment, our goal is to determine if SPOT is a reli-
able skeleton posterior estimator. We assess the posterior quality by
calculating the KL-divergence between the estimated posterior and
ground-truth skeleton. Following earlier research [ 25], we report
AUPRC and AUROC for edge predictions to evaluate the quality of
the estimated posterior.
Using the same datasets from Sec. 4.1, we compare SPOT to
three baselines: AVICI [ 25], FCI* (Nonparametric Bootstrap FCI),
and RFCI* (Nonparametric Bootstrap RFCI). We tried to include
more baselines (e.g., DiBS [ 24] and N-ADMG [ 3]) but they either
frequently crash or encounter out-of-memory issues when process-
ing large graphs used in the experiment. These baselines are not
designed for skeleton posterior inference; however, we sum the
probabilities of all edge types, calibrate to a maximum of 1, and
recast their output as a skeleton posterior for a meaningful com-
parison in Table 3. In addition, we also conduct an ablation study
by replacing the ML4S-like model with an end-to-end Transformer-
based model (denoted as SPOT (w/ Ke et al . [22] )) to justify our
design consideration in Sec. 3.2.Table 3: Evaluation on skeleton posterior inference (avg. on
ten datasets). AUROC and AUPRC: higher is better; KL: lower
is better.
Metric A
VICI FCI* RFCI* SPO
T
AUROC 0.96 0.97 0.95 0.99
AUPRC 0.83 0.92 0.91 0.97
KL 0.05 0.06 0.10 0.03
Comparison with Baselines. Overall, we observe that SPOT con-
sistently outperforms the baselines across all criteria in Table 3.
This finding is interpreted as encouraging and reasonable. First,
SPOT employs a machine learning model to estimate the adjacency
probability while FCI and RFCI apply hard thresholds over p-values
to reject edges. Since the relationship between p-values and adja-
cency probabilities may be intricate, SPOT is more adaptable in
capturing these dependencies. Second, despite having a similar fo-
cus on variational inference, AVICI is not intended for use with
causally insufficient data, and the size of the graph may make it
more challenging to perform whole-graph inference. In contrast,
SPOT efficiently resolves this problem by edge decomposition.
Ablation Study. We also observe that the Transformer-based
model (SPOT (w/ Ke et al . [22] )) yields a lower AUROC (0.95) and
AUPRC (0.77) as well as a higher KL (0.07) than the ML4S-like
cascade model (i.e., worse performance on all criteria). This find-
ing is consistent with our design consideration in Sec. 3.2 that the
cascade model is more suitable for the task of skeleton posterior in-
ference. We presume that the cascade model effectively decompose
the whole graph into edge predictions and reduce the complexity
of the task, which is crucial for large graphs.
Out-of-Distribution Setting. In addition, we also explore the
effectiveness of the domain adaption procedure in SPOT for out-of-
distribution datasets. To do so, we generate ten ADMGs from the
Scale-Free (SF) model as test datasets and evaluate the enhancement
of our domain adaption strategy. On the SF graphs, the original
model trained on ER graphs suffers from a downgrade on KL (from
0.03 to 0.06). When augmented with the lightweight domain adap-
tion step, the KL is further reduced to 0.05 (16.7% enhancement).
4.3 Extension to Other Settings
Though the primary focus of SPOT lies in linear Gaussian data with
latent confounders, we also explore its effectiveness on other types
of data. In particular, we integrate SPOT with neural variational
ADMG learning [ 3] (N-ADMG) which handles non-linear data and
 
2155Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
neural variational DAG learning [ 17] (DECI) and GFlowNets [ 13]
on linear Gaussian data without latent confounders.
Table 4: Integration with neural variational ADMG learning
(avg. on 100 sampled ADMGs).
SHD FDR TPR
N-
ADMG [3] 2.61 0.39 1.0
N-
ADMG + SPOT 2.07 -21% 0.34 -13% 1.0
Neural Variational ADMG Learning. Due to the scalability issue
of N-ADMG, we are unable to apply it to datasets with similar
sizes as in Sec. 4.1. Instead, we use the same dataset in its original
paper [ 3] with five nodes and 2000 samples under the non-linear
Gaussian setting. Here, we use the kernel-based conditional inde-
pendence test [ 44] to compute the required statistics in Sec. 3.2.
However, it is worth noting that SPOT itself is agnostic to the func-
tional form of the SCM as long as the distribution is faithful and the
required statistics can be appropriately computed. Since N-ADMG
yields a distribution over ADMGs, we sample 100 ADMGs from the
posterior and report the average performance in Table 4. Note that,
in this setting, because the ADMG is identifiable, we compare the
generated ADMG with the ground-truth ADMG. We observe that
SPOT significantly improves the performance of N-ADMG on all
criteria. In particular, it reduces the FDR by 13% and the SHD by 21%.
In the dataset, N-ADMG is capable of identifying all true edges (i.e.,
TPR=1.0). However, it also introduces many false positives. SPOT
effectively reduces the number of false positives and improves the
overall performance. In this regard, SPOT offers a principled way
to discourage potentially superfluous edges and thus alleviates the
overfitting issue.
Table 5: Integration with neural variational DAG learning
(avg. on 100 sampled DAGs).
Skeleton
F1 Orientation
F1 SHD
DECI
[17] 0.55 0.44 121.2
DECI
+SPOT 0.58 +5% 0.48 +9% 118.7 -2%
Neural Variational DAG Learning. Since MAG is a generaliza-
tion of DAG, our framework is naturally applicable to infer the
skeleton posterior of DAGs and thus can be integrated with neural
variational DAG learning. To demonstrate this potential extension
ofSPOT, we integrate it with DECI [ 17] and report the result in
Table 5. Here, even though SPOT is originally designed for MAGs,
we can observe a non-trivial performance boost on DAGs. In par-
ticular, SPOT improves the F1 score of skeleton and orientation by
5% and 9%, respectively, and reduces the SHD by 2%. This finding
indicates a potential avenue for future research to further improve
the performance of differentiable DAG learning algorithms with
skeleton posterior. In addition to DECI, we also tried to integrate
SPOT with GFlowNets [ 13] which estimates the posterior of DAGs
using generative flow networks. With the same setting, we observe
thatSPOT reduces the expected SHD of the posterior from 339.8 to
224.4, indicating a 34% improvement. We present the full results in
the appendix due to the space limit.5 RELATED WORK
Causal Discovery with Latent Confounders. Causal discovery
with latent confounders involves constraint-based methods [ 10,28,
32,35,43] and score-based methods [ 7,9,37]. Constraint-based
methods establish causal graphs based on conditional indepen-
dence [ 30], while score-based methods find maximal likelihood
estimations. Differentiable causal discovery utilizes continuous
optimization techniques [ 5]. With the prosperity of its practical
applications [ 18‚Äì21,29,40],SPOT advances scalable and accurate
causal discovery with latent confounders for real-world use.
Posterior Inference of Causal Structure. Traditional causal dis-
covery provides a maximum-likelihood point estimation, but reli-
ability is often criticized for small sample sizes [ 11]. Providing a
posterior distribution is more desired using MCMC [ 38], variational
inference [ 11,24,25], reinforcement learning [ 1,41], generative
flow network [ 13], or supervised learning [ 26]. All methods es-
timate DAGs‚Äô posterior distributions and is infeasible for MAGs.
Among these methods, AVICI [ 25] is the most relevant to SPOT as
it also uses amortized variational inference. In a nutshell, it trains
an inference model over samples from a known (static) simulator
and learn the DAGs from data. In the context of ADMG learning,
N-ADMG [ 3] presents a variational inference method to estimate
the posterior of ADMGs on non-linear data. However, it is not
scalable well to large datasets, as shown in Sec. 4. SPOT trains an
inference model on these samples from static/dynamic simulators
and alleviate the additional assumption of the data.
The Role of Skeleton in Causal Discovery. Skeleton learning
is crucial for constraint-based methods [ 42] and affects overall
performance. Score-based DAG learning methods like MMHC use
skeletons to reduce search space [ 36]. Ma et al. improve the per-
formance of NOTEARS via a more precise skeleton learned by
ML4S [ 26].SPOT demonstrates the importance of skeleton learn-
ing in differentiable causal discovery. Taking these evidences into
consideration, it may become clear that skeleton learning can serve
as the backbone for general causal discovery tasks.
6 CONCLUSION
In this paper, we introduce a framework for differentiable causal
discovery in the presence of latent confounders with skeleton poste-
rior. To this end, we propose SPOT, which features a highly efficient
variational inference algorithm to estimate the underlying skele-
ton distribution from given observational data. It also provides a
stochastic optimization procedure to seamlessly incorporate the
skeleton posterior with the differentiable causal discovery pipeline
in an ‚Äúout-of-the-box‚Äù manner. The results of our experiments are
highly encouraging and show that SPOT outperforms all existing
methods to a notable extent.
ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers for their
valuable comments. The authors from HKUST were supported in
part by a RGC CRF grant under the contract C6015-23G.
REFERENCES
[1]Raj Agrawal, Caroline Uhler, and Tamara Broderick. 2018. Minimal I-MAP MCMC
for scalable structure discovery in causal DAG models. In International Conference
 
2156KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Pingchuan Ma et al.
on Machine Learning. PMLR, 89‚Äì98.
[2]Luca Ambrogioni, Umut G√º√ßl√º, Julia Berezutskaya, Eva Borne, Ya Àágmur G√º√ßl√ºt√ºrk,
Max Hinne, Eric Maris, and Marcel Gerven. 2019. Forward amortized inference for
likelihood-free variational marginalization. In The 22nd International Conference
on Artificial Intelligence and Statistics. PMLR, 777‚Äì786.
[3]Matthew Ashman, Chao Ma, Agrin Hilmkil, Joel Jennings, and Cheng Zhang.
2022. Causal Reasoning in the Presence of Latent Confounders via Neural ADMG
Learning. In The Eleventh International Conference on Learning Representations.
[4]Daniel Bernstein, Basil Saeed, Chandler Squires, and Caroline Uhler. 2020.
Ordering-based causal structure learning in the presence of latent variables. In
International Conference on Artificial Intelligence and Statistics. PMLR, 4098‚Äì4108.
[5]Rohit Bhattacharya, Tushar Nagarajan, Daniel Malinsky, and Ilya Shpitser. 2021.
Differentiable causal discovery under unmeasured confounding. In International
Conference on Artificial Intelligence and Statistics. PMLR, 2314‚Äì2322.
[6]Peter B√ºhlmann, Jonas Peters, and Jan Ernest. 2014. CAM: Causal additive models,
high-dimensional order search and penalized regression. Annals of Stats. (2014).
[7]Rui Chen, Sanjeeb Dash, and Tian Gao. 2021. Integer programming for causal
structure learning in the presence of latent variables. In International Conference
on Machine Learning. PMLR, 1550‚Äì1560.
[8]Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
InProceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785‚Äì794.
[9]Tom Claassen and Ioan G Bucur. 2022. Greedy equivalence search in the presence
of latent confounders. In Uncertainty in Artificial Intelligence. PMLR, 443‚Äì452.
[10] Diego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richardson.
2012. Learning high-dimensional directed acyclic graphs with latent and selection
variables. Annals of Stats. (2012).
[11] Chris Cundy, Aditya Grover, and Stefano Ermon. 2021. Bcd nets: Scalable varia-
tional approaches for bayesian causal discovery. Advances in Neural Information
Processing Systems 34 (2021), 7095‚Äì7110.
[12] Haoyue Dai, Rui Ding, Yuanyuan Jiang, Shi Han, and Dongmei Zhang. 2023.
Ml4c: Seeing causality through latent vicinity. In Proceedings of the 2023 SIAM
International Conference on Data Mining (SDM). SIAM, 226‚Äì234.
[13] Tristan Deleu, Ant√≥nio G√≥is, Chris Emezue, Mansi Rankawat, Simon Lacoste-
Julien, Stefan Bauer, and Yoshua Bengio. 2022. Bayesian structure learning with
generative flow networks. In Uncertainty in Artificial Intelligence. PMLR, 518‚Äì528.
[14] Mathias Drton, Michael Eichler, and Thomas S Richardson. 2009. Computing
Maximum Likelihood Estimates in Recursive Linear Models with Correlated
Errors. Journal of Machine Learning Research 10, 10 (2009).
[15] Roger Fletcher. 1987. Practical methods of optimization. A Wiley Interscience
Publication (1987).
[16] Nir Friedman, Moises Goldszmidt, and Abraham Wyner. 1999. Data analysis
with bayesian networks: a bootstrap approach. In Proceedings of the Fifteenth
conference on Uncertainty in artificial intelligence. 196‚Äì205.
[17] Tomas Geffner, Javier Antoran, Adam Foster, Wenbo Gong, Chao Ma, Emre
Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Agrin Hilmkil, et al .2022.
Deep End-to-end Causal Inference. In NeurIPS 2022 Workshop on Causality for
Real-world Impact.
[18] Zhenlan Ji, Pingchuan Ma, Zongjie Li, and Shuai Wang. 2023. Benchmarking and
Explaining Large Language Model-based Code Generation: A Causality-Centric
Approach. arXiv preprint arXiv:2310.06680 (2023).
[19] Zhenlan Ji, Pingchuan Ma, and Shuai Wang. 2023. Perfce: Performance debugging
on databases with chaos engineering-enhanced causality analysis. In 2023 38th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
IEEE, 1454‚Äì1466.
[20] Zhenlan Ji, Pingchuan Ma, Shuai Wang, and Yanhui Li. 2023. Causality-aided
trade-off analysis for machine learning fairness. In 2023 38th IEEE/ACM Interna-
tional Conference on Automated Software Engineering (ASE). IEEE, 371‚Äì383.
[21] Zhenlan Ji, Pingchuan Ma, Yuanyuan Yuan, and Shuai Wang. 2023. Cc: Causality-
aware coverage criterion for deep neural networks. In 2023 IEEE/ACM 45th
International Conference on Software Engineering (ICSE). IEEE, 1788‚Äì1800.
[22] Nan Rosemary Ke, Silvia Chiappa, Jane X Wang, Jorg Bornschein, Anirudh
Goyal, Melanie Rey, Theophane Weber, Matthew Botvinick, Michael Curtis Mozer,
and Danilo Jimenez Rezende. 2022. Learning to Induce Causal Structure. In
International Conference on Learning Representations.
[23] Scott Kirkpatrick, C Daniel Gelatt Jr, and Mario P Vecchi. 1983. Optimization by
simulated annealing. Science (1983).
[24] Lars Lorch, Jonas Rothfuss, Bernhard Sch√∂lkopf, and Andreas Krause. 2021.
Dibs: Differentiable bayesian structure learning. Advances in Neural Information
Processing Systems 34 (2021), 24111‚Äì24123.
[25] Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, and Bernhard
Sch√∂lkopf. 2022. Amortized Inference for Causal Structure Learning.
arXiv:2205.12934 (2022).
[26] Pingchuan Ma, Rui Ding, Haoyue Dai, Yuanyuan Jiang, Shuai Wang, Shi Han,
and Dongmei Zhang. 2022. Ml4s: Learning causal skeleton from vicinal graphs.
InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 1213‚Äì1223.[27] Pingchuan Ma, Rui Ding, Qiang Fu, Jiaru Zhang, Shuai Wang, Shi Han, and
Dongmei Zhang. 2024. Scalable Differentiable Causal Discovery in the Presence of
Latent Confounders with Skeleton Posterior (Extended Version). arXiv:2406.10537
(2024).
[28] Pingchuan Ma, Rui Ding, Shuai Wang, Shi Han, and Dongmei Zhang. 2023.
XInsight: eXplainable Data Analysis Through The Lens of Causality. Proceedings
of the ACM on Management of Data 1, 2 (2023), 1‚Äì27.
[29] Pingchuan Ma, Zhenlan Ji, Qi Pang, and Shuai Wang. 2022. Noleaks: Differentially
private causal discovery under functional causal model. IEEE Transactions on
Information Forensics and Security 17 (2022), 2324‚Äì2338.
[30] Pingchuan Ma, Zhenlan Ji, Peisen Yao, Shuai Wang, and Kui Ren. 2024. Enabling
Runtime Verification of Causal Discovery Algorithms with Automated Condi-
tional Independence Reasoning. In Proceedings of the 46th IEEE/ACM International
Conference on Software Engineering. 1‚Äì13.
[31] Microsoft. 2024. reliableAI. https://github.com/microsoft/reliableAI.
[32] Raanan Y Rohekar, Shami Nisimov, Yaniv Gurwicz, and Gal Novik. 2021. Iterative
causal discovery in the possible presence of latent confounders and selection
bias. Advances in Neural Information Processing Systems 34 (2021), 2454‚Äì2465.
[33] Karen Sachs, Omar Perez, Dana Pe‚Äôer, Douglas A Lauffenburger, and Garry P
Nolan. 2005. Causal protein-signaling networks derived from multiparameter
single-cell data. Science (2005).
[34] Saber Salehkaleybar, AmirEmad Ghassami, Negar Kiyavash, and Kun Zhang.
2020. Learning linear non-Gaussian causal models in the presence of latent
variables. The Journal of Machine Learning Research 21, 1 (2020), 1436‚Äì1459.
[35] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. 2000.
Causation, prediction, and search. MIT press.
[36] Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. 2006. The
max-min hill-climbing Bayesian network structure learning algorithm. Machine
learning 65 (2006), 31‚Äì78.
[37] Konstantinos Tsirlis, Vincenzo Lagani, Sofia Triantafillou, and Ioannis Tsamardi-
nos. 2018. On scoring maximal ancestral graphs with the max‚Äìmin hill climbing
algorithm. International Journal of Approximate Reasoning 102 (2018), 74‚Äì85.
[38] Jussi Viinikka, Antti Hyttinen, Johan Pensar, and Mikko Koivisto. 2020. To-
wards scalable bayesian learning of causal dags. Advances in Neural Information
Processing Systems 33 (2020), 6584‚Äì6594.
[39] Matthew J Vowels, Necati Cihan Camgoz, and Richard Bowden. 2022. D‚Äôya like
dags? a survey on structure learning and causal discovery. Comput. Surveys 55, 4
(2022), 1‚Äì36.
[40] Y Samuel Wang and Mathias Drton. 2023. Causal discovery with unobserved
confounding and non-Gaussian data. Journal of Machine Learning Research 24,
271 (2023), 1‚Äì61.
[41] Dezhi Yang, Guoxian Yu, Jun Wang, Zhengtian Wu, and Maozu Guo. 2023. Rein-
forcement causal structure learning on order graph. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 10737‚Äì10744.
[42] Kui Yu, Jiuyong Li, and Lin Liu. 2016. A review on algorithms for constraint-based
causal discovery. arXiv preprint arXiv:1611.03977 (2016).
[43] Jiji Zhang. 2008. On the completeness of orientation rules for causal discovery
in the presence of latent confounders and selection bias. Artificial Intelligence
172, 16-17 (2008), 1873‚Äì1896.
[44] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Sch√∂lkopf. 2011. Kernel-
based conditional independence test and application in causal discovery. In Pro-
ceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence.
804‚Äì813.
[45] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. 2018. Dags
with no tears: Continuous optimization for structure learning. Advances in neural
information processing systems 31 (2018).
[46] Shengyu Zhu, Ignavier Ng, and Zhitang Chen. 2019. Causal Discovery with
Reinforcement Learning. In International Conference on Learning Representations.
A PRELIMINARY
To keep the paper self-contained, we provide additional definitions
and notations used in the main text.
Definition 4 (Directed Cycle). Given an ADMG ùê∫with a set of
nodesùëâand a set of edges ùê∏, a directed cycle exists when there is
a directed path from a node ùëâùëñback to itself.
Definition 5 (Almost Directed Cycle). Given an ADMG ùê∫with a
set of nodes ùëâand a set of edges ùê∏, an almost directed cycle exists
when there is a bidirected edge ùëâùëñ‚Üîùëâùëósuch thatùëâùëñ‚ààùë®ùíèùê∫(ùëâùëó).
ùë®ùíèùê∫(ùëâùëó)denotes the set of ancestors (by directed paths) of ùëâùëóin
ùê∫.
 
2157Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Definition 6 (Bow). Given an ADMG ùê∫with a set of nodes ùëâand
a set of edges ùê∏, a bow exists when there are two edges ùëâùëñ‚Üíùëâùëó
andùëâùëó‚Üîùëâùëó.
Definition 7 (Bow-free ADMG). An ADMG is bow-free if it does
not contain any directed or a bow.
A path(ùëã,ùëä 1,¬∑¬∑¬∑,ùëäùëò,ùëå)is said to be blocked byùëç‚äÜùëø\{ùëã,ùëå}
if there exists a node ùëäùëñ‚àà{ùëä1,¬∑¬∑¬∑,ùëäùëò}such that a) ùëäùëñis not a
collider but a member of ùëç, or b)ùëäùëñis a collider but not an ancestor
of any nodes of ùëç. We now introduce m-separation.
Definition 8 (m-separation [ 43]).ùëã,ùëå are m-separated by ùëç(de-
noted byùëã‚´´Gùëå|ùëç) if all paths between ùëã,ùëå are blocked by ùëç.
B IMPLEMENTATION OF SCL MODELS IN
SPOT
In accordance with ML4S [ 26], we implement the supervised causal
learning (SCL) model in SPOT as a series of cascade xgboost [ 8]
classifiers with default hyperparameters. For linear datasets, we
use Fisher-z test to calculate the conditional independence and
constitute the features for the classifier. For non-linear datasets,
we use the kernel-based conditional independence test [ 44] in-
stead. We use the output of the last layer of the cascade xgboost
classifiers as the posterior probability of the presence of an edge.
The implementation of the SCL-enabled SPOT is available at https:
//anonymous.4open.science/r/Spot-F67F.
0 20 40 60 80 100 120
Number of epochs0.250.500.751.00Metric value
AUROC AUPRC KL
Figure 5: Convergence of the model in Ke et al. [22].
Since the model used in Ke et al . [22] is not publicly available, we
made our best effort to replicate the model based on the description
in the paper. The experiments are conducted on a server with
NVIDIA A6000 GPU and 256GB RAM. The model is trained with
128 epochs and a batch size of 1. We use the Adam optimizer with a
learning rate of 0.0003. When trained with 128 epochs, the metrics
is seen to converge, as shown in Fig. 5. On the large-scale causal
graphs with 50‚Äì100 nodes, each dataset contains 1,000 observational
samples. However, the GPU (with 48GB memory which is the largest
GPU memory available to us) can at most handle 600 samples at a
time. Hence, we drop the last 400 samples in each dataset. This issue,
to a certain extent, indicates the scalability limitation of end-to-end
supervised causal learning models and promotes the necessity of
ML4S-like methods, which are much more scalable and efficient.
C ABLATION STUDY ON SPARSITY PRIOR
In our optimization procedure, we enforce a heuristic in which
sparsifying proposals are always accepted. This design attempts toanalogize a range of the regularization term in score-based causal
discovery algorithms where sparse causal graphs are preferred over
dense ones. To evaluate the impact of this design, we conduct an
ablation study by removing the sparsity prior from the optimization
procedure. The results are shown in Table 6.
Table 6: Ablation study on sparsity prior.
Skeleton
F1 Head
F1 T
ail F1
ABIC 0.84 0.76 0.67
SPO
Tw/o Sparsity 0.90 0.81 0.71
SPO
T 0.91 0.86 0.78
Overall, we observed that the strategy indeed provides a non-
trivial improvement to performance and believe that such sparsity
consideration plays an important role in ‚Äúpruning‚Äù dense causal
graphs.
D COMPUTE TIME
We refrain from providing a table regarding compute time because
different algorithms are computed on different architectures, mak-
ing it difficult to draw meaningful conclusions. For instance, FCI
can only use a single core, whereas ABIC/SPOT can utilize up to
32 cores, and N-ADMG primarily relies on GPU. Additionally, on
some large datasets, ABIC may fail to terminate even after run-
ning for one week. These issues complicate direct comparisons.
Following your suggestion and consistent with the main setup in
our paper, we report the compute times for ABIC and SPOT on the
same computing architecture below.
Overall, we believe SPOT enables lower compute time as well as
better performance.
E COMPARISON BY DIFFERENT NUMBER OF
NODES
We report the full results of the comparison of different methods on
different methods, including FCI, RFCI, ABIC and SPOT. Aligned
with our observations in Sec. 4.1, SPOT manifests superior perfor-
mance in all node sizes except on 20-node datasets.
F INTEGRATION WITH GFLOWNETS
We also explore the integration of SPOT with GFlowNets [ 13] to
further improve the performance of differentiable causal discovery.
GFlowNets is a generative flow network that learns the posterior
distribution of DAGs. We use the same experimental setup as in
Sec. 4.3, and the results are shown in Table 8. We observe that SPOT
improves the performance of GFlowNets by 13.6% in F1 score and
Table 7: Average compute time (in seconds) for ABIC and
SPOT.
Algorithm Avg. Time (sec.)
ABIC 3139
SPOT 2883
 
2158KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Pingchuan Ma et al.
10 20 30 40 50 60 70 80 90 100
Number of nodes0.20.40.60.81.0F1
FCI RFCI ABIC SPOT
Figure 6: Comparison of methods on different node sizes.
Table 8: Integration with GFlowNets [13].
F1 Pr
ecision Re
call
GF
lowNets [13] 0.22 0.14 0.45
GF
lowNets + SPOT 0.25 0.18 0.44
28.6% in precision at the cost of a slight decrease in recall. This
result demonstrates the synergistical effect of SPOT in differentiable
causal discovery.G REAL-WORLD DATA
PKC PKA
JNKP38
RafMek
ErkPlcg
AktPIP2PIP3
Figure 7: Application on Sachs dataset.
We also explore the application of our proposed framework on
Sachs [ 33], a real-world dataset with 853 entries on protein expres-
sions involved in human immune system cells. Fig. 7 presents the
promising results of SPOT applied to the Sachs dataset. Since ev-
ery edge endpoint of Sachs is unidentifiable, its PAG corresponds
exactly to the skeleton. Therefore, we evaluate the learned graph‚Äôs
quality at the skeleton level. The discovered skeleton in Fig.7 achieves
an F1 score of 0.63. Although there is a decrease in performance
compared to the results in Sec. 4.1, it still significantly outperforms
ABIC (with an F1 score of 0.48) and provides satisfactory results.
The decrease in performance can be partially attributed to the non-
Gaussian nature of real-world data, which affects the fundamental
premise of ABIC, the backbone of our framework. Since SPOT itself
(i.e., skeleton posterior inference phase) is agnostic to Gaussianity,
it delivers an impressive 31.2% improvement over vanilla ABIC. We
believe that the performance of SPOT has the potential for even
further improvement by incorporating differentiable methods that
handle non-Gaussian data in future work.
 
2159