Decision Focused Causal Learning for Direct Counterfactual
Marketing Optimization
Hao Zhou∗
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
Meituan
Beijing, China
zhouhao29@meituan.comRongxiao Huang∗
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
rxhuang@smail.nju.edu.cnShaoming Li
Meituan
Beijing, China
shaoming.li@outlook.com
Guibin Jiang
Meituan
Beijing, China
jiangguibin@meituan.comJiaqi Zheng†
State Key Laboratory for Novel
Software Technology
Nanjing University
Nanjing, China
jzheng@nju.edu.cnBing Cheng
Meituan
Beijing, China
bing.cheng@meituan.com
Wei Lin
Meituan
Beijing, China
lwsaviola@163.com
Abstract
Marketing optimization plays an important role to enhance user
engagement in online Internet platforms. Existing studies usually
formulate this problem as a budget allocation problem and solve it
by utilizing two fully decoupled stages, i.e., machine learning (ML)
and operation research (OR). However, the learning objective in
ML does not take account of the downstream optimization task in
OR, which causes that the prediction accuracy in ML may be not
positively related to the decision quality.
Decision Focused Learning (DFL) integrates ML and OR into
an end-to-end framework, which takes the objective of the down-
stream task as the decision loss function and guarantees the con-
sistency of the optimization direction between ML and OR. How-
ever, deploying DFL in marketing is non-trivial due to multiple
technological challenges. Firstly, the budget allocation problem in
marketing is a 0-1 integer stochastic programming problem and
the budget is uncertain and fluctuates a lot in real-world settings,
which is beyond the general problem background in DFL. Secondly,
∗Both authors contributed equally to this research.
†Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672353the counterfactual in marketing causes that the decision loss can-
not be directly computed and the optimal solution can never be
obtained, both of which disable the common gradient-estimation
approaches in DFL. Thirdly, the OR solver is called frequently to
compute the decision loss during model training in DFL, which
produces huge computational cost and cannot support large-scale
training data. In this paper, we propose a decision focused causal
learning framework (DFCL) for direct counterfactual marketing
optimization, which overcomes the above technological challenges.
Both offline experiments and online A/B testing demonstrate the
effectiveness of DFCL over the state-of-the-art methods. Currently,
DFCL has been deployed in several marketing scenarios in Meituan,
one of the largest online food delivery platform in the world.
CCS Concepts
•Computing methodologies →Machine learning approaches;
•Applied computing →Electronic commerce.
Keywords
Causal Inference, Decision Focused Learning, Marketing Optimiza-
tion
ACM Reference Format:
Hao Zhou, Rongxiao Huang, Shaoming Li, Guibin Jiang, Jiaqi Zheng, Bing
Cheng, and Wei Lin. 2024. Decision Focused Causal Learning for Direct
Counterfactual Marketing Optimization. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3672353
6368
KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Zhou et al.
1 Introduction
Conducting marketing campaigns is a popular and effective way
used by online Internet platforms to boost user engagement and
revenue. For example, coupons in Taobao[ 37] can stimulate user
activity, dynamic pricing in Airbnb[36] and discounts in Uber[11]
encourage users to use the products.
Despite the incremental revenues, marketing campaigns could
incur significant costs. In order to be sustainable, a marketing cam-
paign is usually conducted under a limited budget. In other words,
only a portion of individuals (e.g., shops or goods) may receive
marketing treatments due to a limited budget. Hence, assigning the
appropriate marketing treatments to different individuals is essen-
tial for the effectiveness of a marketing campaign since users would
respond differently to various promotional offers. Such decision
problems can be formalized as resource allocation problems and
have been investigated for decades.
The mainstream solution for these problems is a two-stage method
[2,3,11,32,38]. In the first stage, the individual-level (incremental)
response under different treatments is predicted using ML models.
The second stage is OR, and the predictions are fed into the combi-
nation optimization algorithms to achieve optimal overall revenue.
However, the objectives of the two stages are not aligned: the for-
mer focuses on the predictive precision of the ML models, while
the latter focuses on the quality of decisions. The method has some
defects due to the isolation of ML and OR. First, the prediction preci-
sion of ML models has no strict positive correlation with the quality
of the final decision. This is because standard loss functions (e.g.,
mean square error, cross-entropy error) do not take the interplay
between the predictions into account, which can affect decision
quality. Second, ML models often fall short of perfect precision, and
the complex operations performed on the predictions in OR lead to
the amplification or accumulation of prediction errors. Thus, the
two-stage method usually obtains suboptimal decisions and is even
inferior to heuristic strategies in some scenarios.
Recently, Decision-Focused Learning (DFL) [ 4,12,20,26] has
received increasing attention as an appropriate alternative to the
two-stage method. The paradigm integrates prediction and opti-
mization into an end-to-end system, which effectively aligns the
objectives of both stages and achieves better performance on many
challenging tasks. The key idea is to train ML models using a loss
function that directly measures the quality of the decisions obtained
from the predictions. Specifically, the ML models are trained un-
der the predict-then-optimize framework [ 12], which (1) makes
predictions based on historical data, (2) solves the optimization
problem based on the predictions, and (3) computes the decision
loss to update the ML model parameters using stochastic gradient
descent (SGD).
Nevertheless, deploying DFL in marketing is non-trivial due to
the following challenges.
Uncertainty of constraints. Most prior works of DFL have
investigated the optimization problem where the unknown param-
eters appear in the objective function. The reason behind this is
that the unknown parameters in the constraints lead to uncertainty
in the solution space, and the optimal solution derived from the
predictions may not be feasible under the real parameters. Within
the constraints of our optimization problem, there are two distinctforms of uncertainty: intrinsic and extrinsic. The inherent uncer-
tainty in the constraints refers to the costs consumed by the indi-
viduals under different treatments, which can be predicted based
on historical data. Extrinsic uncertainty is the frequently changing
marketing budget, determined by the external environment. An
ML model is required to guarantee superior performance under
different marketing budgets. Thus, our optimization objective is the
effectiveness of the decision under any budget, and the optimization
problem is a 0-1 integer stochastic programming.
Counterfactuals in marketing. Computing decision loss in
marketing is challenging due to the presence of counterfactuals.
Specifically, observing the values and costs of an individual under
different treatments is impossible because the individual can only
receive one treatment, which is also called the fundamental problem
of causal inference [ 27]. In addition, the optimal solution of the
optimization problem cannot be obtained based on offline data
due to the counterfactuals, which disables the common gradient-
estimation methods (e.g., SPO [12], LODL [28], LTR [18]) in DFL.
Computational cost of large-scale dataset. Computational
cost is one of the major roadblocks for DFL involving large-scale
optimization. As mentioned above, DFL integrates prediction and
optimization into an end-to-end system, where the solver will be
called frequently during training to solve the optimization problem.
Therefore, the computational cost of DFL is high, leading prior
works to investigate toy-level problems with few decision variables.
In real-world applications, we need to train models for tens of
millions of data, which is unsupportable by traditional DFL.
In this paper, we propose Decision-Focused Causal-Learning
(DFCL) to address the above challenges. The main contributions of
this work can be summarized as follows.
Generalization. In order to address both endogenous uncer-
tainty (cost of individual consumption) and exogenous uncertainty
(marketing budget) in the constraints, the uncertainty constraints
are transformed into the objective function of the dual problem
using Lagrangian duality theory. The optimization objective of
the dual problem is then used as the decision loss. Moreover, we
prove that the budget of the primal problem corresponds to the
Lagrange multipliers of the dual problem, and thus optimizing the
dual solution under different Lagrange multipliers is equivalent to
optimizing the quality of decisions under different budgets.
Counterfactual Decision Loss. Optimal solution, decision loss,
and gradient cannot be computed directly due to the existence of
counterfactuals in marketing, thus we propose two solutions: (1)
surrogate loss function and (2) black-box optimization based on
the Expected Outcome Metric (EOM) [ 2,39,40]. Inspired by Policy
Gradient in Reinforcement Learning, we transform the decision
problem of discrete actions into the problem which maximizes ex-
pected revenue under the probability distribution of the actions,
and combine the Maximum Entropy Regularizer as well as the
Lagrangian duality theory to give two kinds of surrogate loss func-
tions: Policy Learning Loss and Maximun Entropy Regularized Loss.
We theoretically guarantee continuity, convexity and equivalence
of the surrogate loss functions. For black-box optimization, we em-
ploy the EOM to give an unbiased estimation of the decision loss
and improve the finite difference strategy to develop an efficient
estimator of the gradient, which enables us to update the model
parameters using gradient descent.
6369Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD ’24, August 25–29, 2024, Barcelona, Spain.
Scalability. In real-world applications, we need to train models
for tens-of-millions of data. The surrogate functions proposed in
this paper are smooth convex loss functions with almost the same
computational efficiency as the two-stage method. For black-box
optimization, frequently solving the optimization problem after
perturbation incurs huge computational overhead. We accelerate
the problem solving and modify the gradient estimator using the
Lagrangian duality theory, which significantly improves the train-
ing efficiency and reduces the training time from hour-level to
second-level per epoch compared to the black-box method based
on the primal problem.
We conduct extensive experiments to evaluate the performance
of DFCL. Both offline experiments and online A/B testing show the
superior performance of our method over state-of-the-art baselines.
DFCL is deployed to several scenarios in Meituan, an online food
delivery platform, and achieves significant revenue.
2 Related Works
Two-stage Method. The mainstream solution to the resource al-
location problem in marketing usually follows the two-stage para-
digm [ 2,3,32,38], which handles the two stages—machine learn-
ing (ML) and operation research (OR)—independently. In the first
stage, the uplift models are deployed to predict the treatment ef-
fects of individuals. Some prior works have focused on the design
of uplift models, including Meta-Learners [ 16,23], Causal Forests
[2,5,31,39], representation learning [ 13,29,35] and rank model
[8,17]. However, standard loss functions (such as mean square error
and cross-entropy error) for training uplift models do not take the
downstream OR into account. In the second stage, the resource
allocation problem is represented as a multi-choice knapsack prob-
lem (MCKP), which is NP-Hard and efficiently solved based on
Lagrangian duality theory [2, 3, 32, 40].
Decision-Focused Learning(DFL). DFL is considered an appro-
priate alternative to the two-stage method, which integrates predic-
tion and optimization into an end-to-end system. Since computing
decision loss requires solving optimization problems, which usu-
ally involve non-differentiable operations, automatic differentiation
in machine learning frameworks (such as Pytorch [ 25] and Ten-
sorflow [ 1]) cannot give the correct gradient. Three categories of
approaches to gradient computation are proposed by prior DFL
works: analytical smoothing of optimization mappings, smooth-
ing by random perturbations, and differentiation of surrogate loss
function. The first method derives the analytic gradient of deci-
sion loss by using the KKT condition or the homogenous self-dual
formulation, including Optnet [ 4], DQP [ 10], QPTL [ 33], and In-
tOpt [ 19]. However, when the optimization problem is discrete, the
method requires a continuous relaxation of the primal problem,
which results in suboptimality. A potential resolution is to consider
every optimization problem as a black-box optimization and utilize
random perturbations, such as DBB [ 26], DPO [ 7], and I-MLE [ 24],
to generate approximate gradient. Furthermore, the decision loss
is typically discontinuous and nonconvex, so some of these works
suggest convex surrogate functions, including SPO [ 12], LTR [ 18],
NCE [22], and LODL [28], for the decision loss.
The most related works to ours are DRP [ 11] and DPM [ 40]. DRP
proposes to directly learn ROI (ratio between incremental valuesand incremental costs) to rank and choose individuals in the binary
treatment setting. It has been shown by [ 40] that the loss function in
DRP is unable to converging to a stable extreme point. DPM extend
the idea to the multiple treatments setting by directly learning the
unbiased estimation of the decision factor in OR. However, the
construction of the decision factor in multi-treatment setting relies
on the law of diminishing marginal utility, which does not hold
strictly in some scenarios of marketing.
3 Problem Formulation
In this section, we formalize the resource allocation problem and
introduce the overall optimization objective in marketing.
We start with a common marketing scenario that has 𝑀types
of treatments. Let 𝑟𝑖𝑗and𝑐𝑖𝑗be the revenue and cost of individual
𝑖under treatment 𝑗, respectively. The objective is to find an opti-
mal allocation strategy for a group of individuals to maximize the
revenue of the platform, given a limited budget 𝐵. Therefore, the
budget allocation problem with multiple treatments (MTBAP) can
be formulated as an integer programming problem (1):
max𝑧𝐹(𝑧,𝐵)=∑︁
𝑖∑︁
𝑗𝑧𝑖𝑗𝑟𝑖𝑗,
s.t.∑︁
𝑖∑︁
𝑗𝑧𝑖𝑗𝑐𝑖𝑗≤𝐵,
∑︁
𝑗𝑧𝑖𝑗=1,∀𝑖,
𝑧𝑖𝑗∈{0,1},∀𝑖,𝑗,(1)
where𝑧𝑖𝑗∈{0,1}is the decision variable to denote whether to as-
sign treatment 𝑗to individual 𝑖. The first constraint is the limitation
of the budget and the second one requires that only one treatment
is assigned to each individual. Since the budget 𝐵fluctuates a lot in
real-world settings, the objective is regared as a function of the bud-
get and the overall marketing goal is to maximize revenue 𝐹(𝑧,𝐵)
within arbitrary given budget.
Combinatorial Optimization Algorithm. When the value of
𝑟𝑖𝑗and𝑐𝑖𝑗are known in advance, MTBAP is a classical multiple
choice knapsack problem (MCKP) [ 30], which remains NP-Hard.
Existing studies usually solve this problem by using greedy algo-
rithms or Lagrangian duality theory, both of which can provide a
approximation ratio of
𝜌=1−max𝑖𝑗𝑟𝑖𝑗
OPT,
where OPT is the optimal solution. In the above equation, max𝑖𝑗𝑟𝑖𝑗
refers to the revenue of one individual (e.g., one user or one shop),
which is negligible compared with OPT that is the sum of the
revenue of all the individuals in marketing. Therefore, it indicates
that both greedy algorithms and Lagrangian duality theory can
achieve near optimal performance, which are also the most common
algorithms to solve MTBAP in marketing. The details can be found
in existing works, which will not be discussed in this paper.
Model Prediction. However, the value of 𝑟𝑖𝑗and𝑐𝑖𝑗are unknown
during decision making in real-world applications, which are usu-
ally replaced with the prediction value. Therefore, how to make the
prediction of 𝑟𝑖𝑗and𝑐𝑖𝑗plays important roles in marketing effec-
tiveness, which will be addressed in this paper. In the traditional
6370KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Zhou et al.
two-stage approaches, the machine learning (ML) model is trained
with the direction of optimizing prediction accuracy, which may
be not consistent with the direction of optimizing decision quality.
In the following sections, we mainly focus on the design of the loss
function, to make a tradeoff between the prediction accuracy and
the decision quality.
4 Learning Framework of DFCL
In the learning framework, the loss function includes two parts, the
prediction loss and the decision loss, i.e.,
L𝐷𝐹𝐶𝐿 =𝛼L𝑃𝐿+L𝐷𝐿.
The formerL𝑃𝐿aims to decrease the prediction error, which con-
tributes to improving the generalization ability of a ML model. The
latterL𝐷𝐿measures the decision quality of the downstream task,
which is exactly the objective of marketing optimization.
4.1 Prediction Loss
In the traditional two-stage method, the ML model is trained by min-
imizing the difference between the predictions ˆ𝑟,ˆ𝑐and the ground-
truth values 𝑟,𝑐. For instance, in a regression problem, the mean
squared error (MSE) is usually used to train the ML model:
L𝑀𝑆𝐸(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=1
𝑁𝑀∑︁
𝑖∑︁
𝑗(𝑟𝑖𝑗−ˆ𝑟𝑖𝑗)2+(𝑐𝑖𝑗−ˆ𝑐𝑖𝑗)2.(2)
Due to the counterfactuals in marketing, observing the revenue
or cost of an individual under different treatments is impossible
because each individual can only receive one treatment, which is
also called the fundamental problem of causal inference.
Definition 1 (The fundamental problem of causal infer-
ence). For all individuals, only one of all the potential outcomes
under different treatments can be observed in real-world data.
Therefore,L𝑀𝑆𝐸 cannot be directly computed according to Eq. 2
since𝑟𝑖𝑗1and𝑟𝑖𝑗2(or equivalently, 𝑐𝑖𝑗1and𝑐𝑖𝑗2) cannot be simul-
taneously observed for any 𝑗1≠𝑗2. To solve this problem, we
first formulate the training data set and then develop a equivalent
prediction loss in marketing.
Data Set. Suppose that there is a data set of size 𝑁collected
from random control trials (RCT). The 𝑖-th sample is denoted by
(𝑥𝑖,𝑡𝑖,𝑟𝑖𝑡𝑖,𝑐𝑖𝑡𝑖), where𝑥𝑖is the features of individual 𝑖,𝑡𝑖is the
assigned treatment, and 𝑟𝑖𝑡𝑖,𝑐𝑖𝑡𝑖are the revenue and the cost of
individual𝑖under treatment 𝑡𝑖. Denote the count of the samples
(individuals) receiving treatment 𝑗by𝑁𝑗.
Prediction Loss. Given the above data set, we present the predic-
tion loss in marketing in Eq. (3). Theorem 1 presents the equivalency
and the detailed proof can be found in Appendix A.
L𝑃𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=1
𝑀∑︁
𝑖1
𝑁𝑡𝑖[(𝑟𝑖𝑡𝑖−ˆ𝑟𝑖𝑡𝑖)2+(𝑐𝑖𝑡𝑖−ˆ𝑐𝑖𝑡𝑖)2].(3)
Theorem 1. The prediction lossL𝑃𝐿is equivalent toL𝑀𝑆𝐸, i.e.,
L𝑃𝐿=L𝑀𝑆𝐸.4.2 Decision Loss
As is stated in Sec. 3, the ground-truth value of 𝑟and𝑐are usually
unknown in advance, which are replaced with the prediction ˆ𝑟and
ˆ𝑐during decision making. Therefore, denote the original optimiza-
tion problem 𝐹(𝑧,𝐵)by𝐹(𝑧,𝐵, ˆ𝑟,ˆ𝑐), and the solution 𝑧∗(𝐵,ˆ𝑟,ˆ𝑐)is
obtained by solve MTBAP 𝐹(𝑧,𝐵, ˆ𝑟,ˆ𝑐), i.e.,
𝑧∗(𝐵,ˆ𝑟,ˆ𝑐)=arg max𝑧𝐹(𝑧,𝐵, ˆ𝑟,ˆ𝑐).
The objective value achieved by the current solution 𝑧∗(𝐵,ˆ𝑟,ˆ𝑐)can
be expressed with the ground-truth value of 𝑟as∑︁
𝑖∑︁
𝑗𝑟𝑖𝑗𝑧∗
𝑖𝑗(𝐵,ˆ𝑟,ˆ𝑐).
The decision loss under budget 𝐵is defined as the negative of
the objective value with ground-truth 𝑟and predicted decision
𝑧∗(𝐵,ˆ𝑟,ˆ𝑐), i.e.,
L𝐷𝐿(𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐)=−∑︁
𝑖∑︁
𝑗𝑟𝑖𝑗𝑧∗
𝑖𝑗(𝐵,ˆ𝑟,ˆ𝑐).
As is descripted in Sec. 3, the budget 𝐵fluctuates a lot in real-world
settings and the overall marketing objective is to maximize the
revenue under arbitrary budget. Therefore, the decision loss in
marketing is defined as
L𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=∫∞
0L𝐷𝐿(𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐)𝑑𝐵
=∫∞
0−∑︁
𝑖∑︁
𝑗𝑟𝑖𝑗𝑧∗
𝑖𝑗(𝐵,ˆ𝑟,ˆ𝑐)𝑑𝐵.
For ease of calculation, we can also discretize the budget and com-
pute the decision loss by
L𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=∑︁
𝐵L𝐷𝐿(𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐).
4.3 Learning Framework
Algorithm 1 presents the framework of decision focused causal
learning (DFCL). The most crucial step in this framework is the
gradient estimation of L𝐷𝐹𝐶𝐿 in line 10 of Algorithm 1. However,
it is non-trival in marketing due to the following technological
challenges, i.e., uncertainty of constraints, counterfactual and com-
putation cost. In this paper, we will show how to address these
challenges and how to deploy DFCL in marketing optimization.
Algorithm 1 Decision Focused Causal Learning (DFCL)
Input: training data D≡{(𝑥𝑖,𝑡𝑖,𝑟𝑖𝑡𝑖,𝑐𝑖𝑡𝑖)}𝑁
𝑖=1
1:Initialize𝜔
2:foreach epoch do
3: ˆ𝑟,ˆ𝑐=𝑚𝜔(𝑥).
4:L𝑃𝐿=Í
𝑖1
𝑁𝑡𝑖[(𝑟𝑖𝑡𝑖−ˆ𝑟𝑖𝑡𝑖)2+(𝑐𝑖𝑡𝑖−ˆ𝑐𝑖𝑡𝑖)2].
5:foreach budget 𝐵do
6:𝑧∗(𝐵,ˆ𝑟,ˆ𝑐)=arg max𝑧𝐹(𝑧,𝐵, ˆ𝑟,ˆ𝑐).
7:L𝐷𝐿(𝐵)=−Í
𝑖Í
𝑗𝑟𝑖𝑗𝑧∗
𝑖𝑗(𝐵,ˆ𝑟,ˆ𝑐).
8:L𝐷𝐿=Í
𝐵L𝐷𝐿(𝐵)
9:L𝐷𝐹𝐶𝐿 =𝛼L𝑃𝐿+L𝐷𝐿.
10:𝜔=𝜔−𝜂𝜕L𝐷𝐹𝐶𝐿
𝜕𝜔
6371Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD ’24, August 25–29, 2024, Barcelona, Spain.
5 Gradient Estimation of DFCL
The loss of DFCL consists of the prediction loss and the decision loss.
The former is a continuously differentiable function whose gradient
can be directly computed. Hence, the gradient estimation of the
decision loss is the key focus of this section. Firstly, we introduce the
equivalent dual decision loss to remove the uncertain constraints
and reduce the computation cost of combinatorial optimization
algorithms. Secondly, we develop two surrogate loss functions and
improve the black-box optimization algorithm to provide a gradient
estimation of the dual decision loss.
5.1 Dual Decision Loss
Based on the Lagrangian duality theory, the upper bound of the orig-
inal problem 𝐹(𝑧,𝐵,𝑟,𝑐)can be obtained by solving the following
dual problem (4).
min
𝜆≥0©­­
«max𝑧𝜆𝐵+Í
𝑖Í
𝑗(𝑟𝑖𝑗−𝜆𝑐𝑖𝑗)𝑧𝑖𝑗
𝑠.𝑡.Í
𝑗𝑧𝑖𝑗=1,∀𝑗
𝑧𝑖𝑗∈{0,1},∀𝑖,𝑗ª®®
¬
=min
𝜆≥0max𝑧𝐻(𝑧,𝜆,𝐵,𝑟,𝑐)
=min
𝜆≥0𝐺(𝜆,𝐵,𝑟,𝑐). (4)
The optimal Lagrange multiplier 𝜆∗for the dual problem (4)can be
obtained by using a gradient descent algorithm or a binary search
method with the terminal condition of 𝐵−Í
𝑖Í
𝑗𝑐𝑖𝑗𝑧𝑖𝑗≤𝜖or𝜆≤𝜖.
In addition, an approximately optimal solution for the original
problem can be derived by maximizing 𝐻(𝑧,𝜆∗,𝐵,𝑟,𝑐). Theorem 2
presents the relationship between the original problem 𝐹(𝑧,𝐵,𝑟,𝑐)
and the dual problem 𝐺(𝜆,𝐵,𝑟,𝑐).
Theorem 2. Denote by𝐹𝑐(𝑧,𝐵,𝑟,𝑐)the relaxation form of 𝐹(𝑧,𝐵,𝑟,𝑐)
where the decision variables 𝑧are relaxed to continuous variables (i.e.,
𝑧𝑖𝑗∈[0,1]for∀𝑖,𝑗). Denote the optimal solution by
𝑧∗
𝑐(𝐵,𝑟,𝑐)=arg max𝑧𝐹𝑐(𝑧,𝐵,𝑟,𝑐),
𝑧∗(𝐵,𝑟,𝑐)=arg max𝑧𝐹(𝑧,𝐵,𝑟,𝑐),
𝜆∗(𝐵,𝑟,𝑐)=arg min
𝜆≥0𝐺(𝜆,𝐵,𝑟,𝑐).
Given the optimal Lagrange multiplier 𝜆∗, an approximation solution
for the original problem can be derived by
𝑧𝑑(𝜆∗,𝐵,𝑟,𝑐)=arg max𝑧𝐻(𝑧,𝜆∗,𝐵,𝑟,𝑐).
Based on these definitions, we claim that 𝜆∗is monotonic decreasing
with the increment of the budget 𝐵, and we have
𝐹(𝑧𝑑,𝐵,𝑟,𝑐)≤𝐹(𝑧∗,𝐵,𝑟,𝑐)
≤𝐹𝑐(𝑧∗
𝑐,𝐵,𝑟,𝑐)
=𝐺(𝜆∗,𝐵,𝑟,𝑐)
≤𝐹(𝑧𝑑,𝐵,𝑟,𝑐)+max
𝑖𝑗𝑟𝑖𝑗
The detailed proof can be found in [ 14]. Given the optimal 𝜆∗,
Theorem 2 indicates that the solution 𝑧𝑑(𝜆∗,𝐵,𝑟,𝑐)obtained bymaximizing 𝐻(𝑧,𝜆∗,𝐵,𝑟,𝑐)is approximately optimal with an ap-
priximation ratio of
𝜌=𝐹(𝑧𝑑,𝐵,𝑟,𝑐)
𝐹(𝑧∗,𝐵,𝑟,𝑐)≥𝐹(𝑧∗,𝐵,𝑟,𝑐)−max𝑖𝑗𝑟𝑖𝑗
𝐹(𝑧∗,𝐵,𝑟,𝑐)
=1−max𝑖𝑗𝑟𝑖𝑗
𝐹(𝑧∗,𝐵,𝑟,𝑐)
≈1
The last equality holds because 𝐹(𝑧∗,𝐵,𝑟,𝑐)is the sum of the rev-
enue of millions of individuals in marketing, which means that
𝐹(𝑧∗,𝐵,𝑟,𝑐)≫ max𝑖𝑗𝑟𝑖𝑗.
Therefore, instead of the original problem, the optimization of
the dual problem 𝐻(𝑧,𝜆∗,𝐵,𝑟,𝑐)is taken as the learning objective,
which we call the dual decision loss. Given the optimal 𝜆∗and
the prediction value ˆ𝑟,ˆ𝑐, the solution 𝑧𝑑(𝜆∗,𝐵,ˆ𝑟,ˆ𝑐)is obtained by
maximizing 𝐻(𝑧,𝜆∗,𝐵,ˆ𝑟,ˆ𝑐), i.e.,
𝑧𝑑(𝜆∗,𝐵,ˆ𝑟,ˆ𝑐)=arg max𝑧𝐻(𝑧,𝜆∗,𝐵,ˆ𝑟,ˆ𝑐).
Notice that 𝜆∗𝐵can be taken as a constant, and removing it from
𝐻(𝑧,𝜆∗,𝐵,ˆ𝑟,ˆ𝑐)does not influence 𝑧𝑑. Therefore, the solution 𝑧𝑑can
be rewritten as
𝑧𝑑(𝜆∗,ˆ𝑟,ˆ𝑐)=arg max𝑧𝐻(𝑧,𝜆∗,ˆ𝑟,ˆ𝑐),
where𝐻(𝑧,𝜆∗,ˆ𝑟,ˆ𝑐)is the form of 𝐻(𝑧,𝜆∗,𝐵,ˆ𝑟,ˆ𝑐)after removing
𝜆∗𝐵. The dual decision loss achieved by the current solution 𝑧𝑑is
L𝐷𝐷𝐿(𝜆∗,𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐)=−(𝜆∗𝐵+∑︁
𝑖∑︁
𝑗(𝑟𝑖𝑗−𝜆∗𝑐𝑖𝑗)𝑧𝑑
𝑖𝑗(𝜆∗,ˆ𝑟,ˆ𝑐)).
Similarly, since 𝜆∗and𝐵is irrelevant to the prediction value ˆ𝑟,ˆ𝑐,
𝜆∗𝐵can be regarded as a constant and removed from the dual
decision loss. According to Theorem 2, 𝜆∗is monotonic decreasing
with the increment of the budget 𝐵and there is an unique 𝜆∗for
the dual problem when given the budget 𝐵. Therefore, the decision
lossL𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)in the original problem under arbitrary budget 𝐵
can be transformed to the dual decision loss L𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)under
arbitrary Lagrange multiplier 𝜆∗, i.e.,
L𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=∫∞
0L𝐷𝐷𝐿(𝜆∗,𝑟,𝑐, ˆ𝑟,ˆ𝑐)𝑑𝜆∗
=∫∞
0L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)𝑑𝜆
=−∫∞
0∑︁
𝑖∑︁
𝑗(𝑟𝑖𝑗−𝜆𝑐𝑖𝑗)𝑧𝑑
𝑖𝑗(𝜆,ˆ𝑟,ˆ𝑐)𝑑𝜆.
By discretizing the Lagrange multiplier 𝜆, the dual decision loss
can be computed by
L𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=∑︁
𝜆L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐).
5.2 Policy Learning Loss
Notice that the dual problem 𝐻(𝑧,𝜆,ˆ𝑟,ˆ𝑐)can be solved by
max𝑧𝐻(𝑧,𝜆,ˆ𝑟,ˆ𝑐)=©­­
«max𝑧Í
𝑖Í
𝑗(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)𝑧𝑖𝑗
𝑠.𝑡.Í
𝑗𝑧𝑖𝑗=1,∀𝑗
𝑧𝑖𝑗∈{0,1},∀𝑖,𝑗ª®®
¬
=∑︁
𝑖max
𝑗(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)
6372KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Zhou et al.
Therefore, the solution 𝑧𝑑(𝜆,ˆ𝑟,ˆ𝑐)=arg max𝑧𝐻(𝑧,𝜆,ˆ𝑟,ˆ𝑐)can be
expressed by
𝑧𝑑
𝑖𝑗(𝜆,ˆ𝑟,ˆ𝑐)=I𝑗=arg max𝑗ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗.
Hence, the dual decision loss is rewritten as
L𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=−∑︁
𝜆∑︁
𝑖∑︁
𝑗(𝑟𝑖𝑗−𝜆𝑐𝑖𝑗)I𝑗=arg max𝑗ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗
However,L𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)is not differentiable with respect to ˆ𝑟and
ˆ𝑐due to the indicator function. Instead, we utilize a softmax function
to smooth the dual decision loss, i.e.,
L′
𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=−∑︁
𝜆∑︁
𝑖∑︁
𝑗(𝑟𝑖𝑗−𝜆𝑐𝑖𝑗)exp(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)Í
𝑘exp(ˆ𝑟𝑖𝑘−𝜆ˆ𝑐𝑖𝑘)(5)
Let𝑝𝑖𝑗(𝜆,ˆ𝑟,ˆ𝑐)=exp(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)/Í
𝑘exp(ˆ𝑟𝑖𝑘−𝜆ˆ𝑐𝑖𝑘)be the proba-
bility of assigning treatment 𝑗to individual 𝑖. Take𝑟𝑖𝑗−𝜆𝑐𝑖𝑗as the
reward of assigning treatment 𝑗to individual 𝑖. Hence, minimizing
L′
𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)is equivalent to maximizing the expected reward of
policy𝜋=𝑝𝑖𝑗(𝜆,ˆ𝑟,ˆ𝑐)under different Lagrange multipliers. There-
fore,L′
𝐷𝐷𝐿is also called the policy learning loss.
Due to the counterfactual in marketing, L′
𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)cannot
be directly computed by Eq. (5)in training data sets. Instead, we
propose a surrogate loss, i.e.,
L𝑃𝐿𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=−∑︁
𝜆∑︁
𝑖𝑁
𝑁𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)exp(ˆ𝑟𝑖𝑡𝑖−𝜆ˆ𝑐𝑖𝑡𝑖)Í
𝑗exp(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗).
Theorem 3 presents the equivalence between the original dual
decision lossL𝐷𝐷𝐿 and the surrogate policy learning loss 𝐿𝑃𝐿𝐿.
The detailed proof can be found in Appendix B.
Theorem 3.L𝐷𝐷𝐿,L′
𝐷𝐷𝐿andL𝑃𝑃𝐿are equivalent, i.e.,
L𝑃𝐿𝐿(𝜆,ˆ𝑟,ˆ𝑐)=L′
𝐷𝐷𝐿(𝜆,ˆ𝑟,ˆ𝑐)
and
min
ˆ𝑟,ˆ𝑐L𝑃𝐿𝐿(𝜆,ˆ𝑟,ˆ𝑐)=min
ˆ𝑟,ˆ𝑐L𝐷𝐷𝐿(𝜆,ˆ𝑟,ˆ𝑐).
5.3 Maximum Entropy Regularized Loss
In order to obtain a differentiable closed form of 𝑧𝑑(𝜆,ˆ𝑟,ˆ𝑐)with
respect to ˆ𝑟and ˆ𝑐, we relax the discrete constraint 𝑧∈{0,1}to a
continuous one 𝑥∈[0,1]and add a maximum entropy regular-
izer to the objective function in 𝐻(𝑧,𝜆,ˆ𝑟,ˆ𝑐). Hence,𝐻(𝑧,𝜆,ˆ𝑟,ˆ𝑐)is
transformed to a nonlinear convex function, i.e.,
max𝑧∑︁
𝑖∑︁
𝑗(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)𝑧𝑖𝑗−𝜏∑︁
𝑖∑︁
𝑗𝑧𝑖𝑗ln𝑧𝑖𝑗,
𝑠.𝑡.∑︁
𝑗𝑧𝑖𝑗=1,∀𝑖,
𝑧𝑖𝑗∈[0,1],
where𝜏denotes the penalty weight. The Lagrange relaxation func-
tion can be further rewritten as
𝐿(𝑧,𝛽)=𝑁∑︁
𝑖=1𝑀∑︁
𝑗=1(𝑟𝑖𝑗−𝜆𝑐𝑖𝑗)𝑧𝑖𝑗−𝜏𝑁∑︁
𝑖=1𝑀∑︁
𝑗=1𝑧𝑖𝑗ln𝑧𝑖𝑗−∑︁
𝑖𝛽𝑖(1−∑︁
𝑗𝑧𝑖𝑗),where𝛽is the dual variables on the equality constraint. When
𝜕𝐿(𝑧,𝛽)
𝜕𝑧=0and𝜕𝐿(𝑧,𝛽)
𝜕𝛽=0, the optimal solution is obtained by
𝑧𝑑
𝑖𝑗=exp[(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)/𝜏]Í
𝑘exp[(ˆ𝑟𝑖𝑘−𝜆ˆ𝑐𝑖𝑘)/𝜏],
which is continuously differentiable with respect to ˆ𝑟andˆ𝑐. Hence,
the dual decision loss can be rewritten as
L′′
𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=−∑︁
𝜆∑︁
𝑖∑︁
𝑗(𝑟𝑖𝑗−𝜆𝑐𝑖𝑗)exp[(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)/𝜏]Í
𝑘exp[(ˆ𝑟𝑖𝑘−𝜆ˆ𝑐𝑖𝑘)/𝜏]
Similarly,L′′
𝐷𝐷𝐿cannot be directly computed due to the counter-
factual in marketing. We propose a surrogate loss L𝑀𝐸𝑅𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
as follows, which we call the maximum entropy regularized loss,
L𝑀𝐸𝑅𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=−∑︁
𝜆∑︁
𝑖𝑁
𝑁𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)exp[(ˆ𝑟𝑖𝑡𝑖−𝜆ˆ𝑐𝑖𝑡𝑖)/𝜏]Í
𝑗exp[(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)/𝜏].
Notice thatL𝑃𝐿𝐿is a special case of L𝑀𝐸𝑅𝐿 , where the solution
𝑧𝑑can be regarded as a temperature softmax function in L𝑀𝐸𝑅𝐿 .
5.4 Improved Finite-Difference Strategy
In addition to constructing surrogate loss functions, we can also use
the Expected Outcome Metric (EOM) [ 2,39,40] to give an unbiased
estimate of the decision loss and leverage black-box optimization
for decision-focused learning.
EOM is a commonly used method for offline strategy evaluation
based on randomized dataset. Given a batch of 𝑁random sam-
ples and model predictions ˆ𝑟andˆ𝑐, an arbitrary allocation strategy
𝑧(ˆ𝑟,ˆ𝑐)can be evaluated: (1) find the set of individuals whose re-
ceived treatment is equal to the treatment in the allocation strategy
𝑧(ˆ𝑟,ˆ𝑐), (2) then empirically estimate their per capita revenue and
per capita cost:
¯𝑟(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=1
𝑁∑︁
𝑖1
𝑝𝑡𝑖𝑟𝑡𝑖I𝑡𝑖=arg max𝑗𝑧𝑖𝑗,
¯𝑐(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=1
𝑁∑︁
𝑖1
𝑝𝑡𝑖𝑐𝑡𝑖I𝑡𝑖=arg max𝑗𝑧𝑖𝑗,
where𝑝𝑡𝑖denotes the probability that a treatment is equal to 𝑡𝑖in
the randomized dataset. For the primal MCKP with budget 𝐵, we
can use binary search to empirically estimate the per capita revenue
under a per capita budget𝐵
𝑁as is shown in Appendix C.
Therefore, we can redefine the decision loss as follows:
L𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=−∑︁
𝐵¯𝑟(𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐).
Since the computation of ¯𝑟(𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐)involves many nondifferen-
tiable operations, we consider them as black-box functions and
estimate the gradient by perturbation. Using the finite difference
strategy, the gradient of the decision quality with respect to ˆ𝑟𝑖𝑗is
estimated as:
𝜕L𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟𝑖𝑗=L𝐷𝐿(𝑟,𝑐, ˆ𝑟+𝑒𝑖𝑗ℎ,ˆ𝑐)−L𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
ℎ,
whereℎis a small constant, and 𝑒𝑖𝑗∈ {0,1}𝑁×𝑀is a matrix
where only the element in the i-th row and j-th column is 1, and
all other elements are 0. The gradient term𝜕L𝐷𝐿(𝑟,𝑐,ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐𝑖𝑗can be
computed similarly. We estimate the gradient by perturbing the
predictions one by one and obtain the gradient matrix𝜕L𝐷𝐿(𝑟,𝑐,ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟
6373Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD ’24, August 25–29, 2024, Barcelona, Spain.
and𝜕L𝐷𝐿(𝑟,𝑐,ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐∈𝑅𝑁×𝑀. Finally, we derive the following loss
function, which is able to train the ML model via gradient descent:
L𝐹𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=∑︁
𝑖∑︁
𝑗𝜕L𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟𝑖𝑗ˆ𝑟𝑖𝑗+𝜕L𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐𝑖𝑗ˆ𝑐𝑖𝑗.
Since the perturbations are performed one by one, ¯𝑟(𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
requires frequent evaluation, leading to a considerable time com-
plexity of black-box optimization based on the primal MCKP [ 34].
In practice, we find that the number of samples 𝑁tends to be in
the millions or even tens of millions, so the time consumption for a
training epoch reaches the level of hours. A possible approach is to
only perturb some of the samples by sampling, but this may incur
the loss of much of the gradient information.
Instead, we accelerate the problem solving and modify the gradi-
ent estimator by using Lagrangian duality theory. Since the budget
𝐵in the primal MCKP corresponds one-to-one to the 𝜆in the duality
problem, the dual decision loss can be redefined as:
L𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=−∑︁
𝜆¯𝑟(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)−𝜆¯𝑐(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐).
Although we avoid solving the primal MCKP, it is still necessary
to frequently evaluate the per capita revenue and per capita cost
after perturbation under multiple Lagrangian multipliers. We ob-
serve that the decision making is independent for each individual
thanks to the decomposition of the Lagrangian duality theory. Thus,
for each sample, the smallest perturbation that causes a change
in the dual decison loss is first calculated, and the loss after the
perturbation is obtained by correcting only the original result. Ap-
pendix D provides details of the modified gradient estimator, which
greatly reduces the computational overhead. Finally, the black-box
optimization loss function can be rewritten as
L𝐼𝐹𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=∑︁
𝑖∑︁
𝑗𝜕L𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟𝑖𝑗ˆ𝑟𝑖𝑗+𝜕L𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐𝑖𝑗ˆ𝑐𝑖𝑗.
It is sufficient to support model training on tens of million of data
since the computational cost of incremental updating 𝐿𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
after perturbation is much less than that of re-evaluating it. To im-
prove numerical stability in training, we truncate the perturbation
matrix𝑃∈R𝑁×𝑀. Further, the loss function can be smoothed
using Softmax to reduce the difficulty of training.
L𝐼𝐹𝐷𝐿−𝑆𝑜𝑓𝑡𝑚𝑎𝑥(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=∑︁
𝑖∑︁
𝑗𝜕L𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕𝑎𝑖𝑗𝑎𝑖𝑗,
where𝑎𝑖𝑗=𝑆𝑜𝑓𝑡𝑚𝑎𝑥(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗), and only𝑎is perturbed for gra-
dient estimation and no longer for ˆ𝑟,ˆ𝑐.
6 Evaluation
In this section, we will conduct large-scale offline and online exper-
iments to compare our methods with other benchmarks to validate
their performance.
6.1 Offline Experiment
6.1.1 Dataset. Two types of datasets are provided in this paper.
•CRITEO-UPLIFT v2. This public dataset is provided by the
AdTech company Criteo in the AdKDD’18 workshop[ 9]. The
dataset contains 13.9 million samples collected from a randomcontrol trial (RCT) that prevents a random part of users from be-
ing targeted by advertising. Each sample has 12 features, 1 binary
treatment indicator and 2 response labels(visit/conversion). In
order to study resource allocation problem under limited budget
using the dataset, we follow[ 40] and take the visit/conversion
label as the cost/value respectively. We randomly sample 70%
samples for training and the remaining samples for test.
•Marketing data. Discounting is a common marketing campaign
in Meituan, an online food delivery platform. We conduct a two-
week RCT to collect data in this platform. The online shops on
the platform offer daily discounts to users. Note that to avoid
price discrimination, the discount of a shop is the same for all
individuals, but it changes randomly each day and varies from
shop to shop. The data in the first week is used for training and
the other for test. The discount 𝑇∈{0,5,10,15,20}is taken as
the treatment, where 𝑇=𝑡means𝑡%off for each order whose
price meets a given threshold. The dataset contains 2.8 million
samples, and each sample has 107 features, 1 treatment label and
2 response labels (daily cost/orders).
6.1.2 Evaluation Metrics. Multiple evaluation metrics are provided
for offline evaluation in this experiment. In addition to adopting
the evaluation metrics commonly used in two-stage models, such
as Logloss and MSE, we also use the following metrics for policy
evaluation with counterfactuals, which are more significant.
•AUCC (Area under Cost Curve). A common metric used in
existing works [ 2,11,40], which is designed for evaluating the
performance to rank ROI of individuals in the binary treatment
setting. We use the metric to compare the performance of differ-
ent methods in CRITEO-UPLIFT v2.
•EOM (Expected Outcome Metric). EOM is also commonly
used in [ 2,39,40]. Based on RCT data, an unbiased estimation
of the expected outcome (per-capita revenue/per-capita cost) for
arbitrary budget allocation policy can be obtained. The details
of EOM are shown in Sec. 5.4. We use the metric to compare the
performance of different methods in Marketing data.
6.1.3 Benchmarks. For each dataset in this paper, multiple models
and algorithms are implemented and taken as benchmarks.
•TSM-SL. The two-stage method is mentioned in many exsting
works[ 2,3,32,38]. In the first stage, a well-trained S-Learner
model is used to predict the response (revenue/cost) of individu-
als under different treatments. In the second stage, we find the
optimal budget allocation solution for an MCKP formulation
based on the predictions.
•TSM-CF. Also a two-stage method, the difference with TSM-SL
is that instead of S-learner, we use a Causal Forests [ 5] to predict
the incremental response in the first stage. It is implemented
here base on EconML packages [ 6], which can support binary
treatment and multiple treatments.
•DPM. This method[ 40] designs the decision factor for the MCKP,
and proposes a surrogate loss to directly learn the decision factor.
•CN. This method[ 32] imposes a monotonic constraint between
outcome predictions and treatments, which is particularly useful
for ITE estimation under multiple treatments. The method is
trained with MSE loss and evaluated only on marketing data,
which is a multi-treatment experiment.
6374KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Zhou et al.
•CN+DFCL-PL. The constraint network is trained with Decision-
Focused Causal Learning (DFCL) loss, which comprises MSE loss
(L𝑃𝐿) and policy learning loss ( L𝑃𝐿𝐿).
•DFCL-PL. The DFCL method based on policy learning loss pro-
posed in this paper.
•DFCL-MER. The DFCL method proposed in this paper utilizes
the surrogate loss derived by Maximun Entropy Regularizer
•DFCL-IFD. The DFCL method proposed in this paper for gradient
estimation using the improved finite difference strategy.
6.1.4 Implementation Details.
•CRITEO-UPLIFT v2. For the baseline methods (TSM-SL, TSM-
CF and DPM), we cite the results directly from[ 40]. The DFCL
model uses the same DNN architecture with a shared layer that
is a single-layer MLP of dimension 128 and four head networks
that are two-layer MLPs of dimension [64, 1]. Except for the
final output layer, the remaining layers use ReLU activations.
For DFCL-MER, we set the temperature 𝜏=3. Our models are
trained for 40 epochs with the Adam optimizer [15]. In order to
accelerate the training, the first twenty epochs are warmstarting
[21] using the cross-entropy loss, and then the models are trained
using the DFCL loss.
•Marketing data. In the multi-treatment experiment, the models
need to predict the revenue and cost under five treatments. TSM-
SL, CN, CN+DFCL-PL, DFCL-PL, DFCL-MER and DFCL-IFD use
the same DNN architecture: a 4 layers MLP (64-32-32-10). The
first five outputs of the models are the predicted revenue, and the
remaining outputs are the predicted cost. For DFCL-MER, we set
the temperature 𝜏=0.01. For DPM, a S-Learner model is trained
using the customized loss proposed in [ 40] to directly predict
marginal utility under different treatments. The DPM model has
4 layers of MLP (64-32-32-1), with the last layer using a sigmoid
activation and each of the remaining layers with ReLU activations.
All neural network-based models are trained for 500 epochs using
the Adam optimizer. For TSM-CF, we set 𝑛_𝑒𝑠𝑡𝑖𝑚𝑎𝑡𝑜𝑟𝑠 =256,
𝑚𝑖𝑛_𝑠𝑎𝑚𝑝𝑙𝑒 _𝑙𝑒𝑎𝑓=300and𝑑𝑒𝑝𝑡ℎ =24.
All experiments are run on AMD EPYC 7502P Rome 32x@ 2.50GHz
processor with 64GB memory.
Table 1: Comparison of common metrics, noting that DPM
and TSM-CF predict the decision factor and the incremental
intervention effect, respectively, and thus do not apply to
these two metrics.
ModelCRITEO-UPLIFT v2 Marketing data
Logloss MSE
TSM-SL 0.2165±0.0001 0.2625±0.0009
CN / 0.2639±0.0012
CN+DFCL-PL / 0.2703±0.0015
DFCL-PL 0.2186±0.0008 0.2678±0.0010
DFCL-MER 0.2178±0.0012 0.2650±0.0005
DFCL-IFD 0.2170±0.0003 0.2642±0.0009
6.2 Experimental Results
6.2.1 Overall performance. In Table 1, we present the prediction
loss of different models on the two datasets. Clearly, the two-stageTable 2: AUCC(CRITEO-UPLIFT v2)
Model AUCC Improvement
TSM-SL 0.7561±0.0113 /
TSM-CF 0.7558±0.0012 -0.03%
DPM 0.7739±0.0002 +2.35%
DFCL-PL 0.7713±0.0025 +2.01%
DFCL-MER 0.7727±0.0015 +2.20%
DFCL-IFD 0.7859±0.0021 +3.94%
0.0 0.2 0.4 0.6 0.8 1.0
Incremental cost0.00.20.40.60.81.0Incremental rewardTSM-SL
TSM-CF
DPM
DFCL-IFD
DFCL-PL
DFCL-MER
Random
(a) AUCC(CRITEO-UPLIFT v2)
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019
/uni00000025/uni00000058/uni00000047/uni0000004a/uni00000048/uni00000057/uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000015/uni00000018/uni00000014/uni00000011/uni00000013/uni00000018/uni00000013/uni00000014/uni00000011/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000013/uni00000014/uni00000011/uni00000014/uni00000015/uni00000018/uni00000014/uni00000011/uni00000014/uni00000018/uni00000013/uni00000014/uni00000011/uni00000014/uni0000001a/uni00000018/uni0000002c/uni00000051/uni00000046/uni00000055/uni00000048/uni00000050/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047
/uni00000037/uni00000036/uni00000030/uni00000010/uni00000036/uni0000002f
/uni00000037/uni00000036/uni00000030/uni00000010/uni00000026/uni00000029
/uni00000027/uni00000033/uni00000030
/uni00000026/uni00000031
/uni00000026/uni00000031/uni0000000e/uni00000027/uni00000029/uni00000026/uni0000002f/uni00000010/uni00000033/uni0000002f
/uni00000027/uni00000029/uni00000026/uni0000002f/uni00000010/uni0000002c/uni00000029/uni00000027
/uni00000027/uni00000029/uni0000002f/uni00000010/uni00000033/uni0000002f
/uni00000027/uni00000029/uni0000002f/uni00000010/uni00000030/uni00000028/uni00000035 (b) EOM (Marketing data)
Figure 1: Offline experiment results
method performs best on common metrics, which minimizes MSE
or Logloss on the training set. However, what we really focus on is
the decision quality of predictions. Fig. 1a and Table 2 present the
comparison between our proposed methods and other benchmarks
in CRITEO-UPLIFT v2 on AUCC [ 11], which represents the deci-
sion quality under binary treatments. We can see that DFCL-IFD
achieves the best performance, DFCL-PL, and DFCL-MER perform
similarly to DPM, and the two-stage methods perform the worst.
In marketing data, we use EOM method to calculate per-capita
orders and per-capita budgets based on predictions. The results are
shown in Table 3 and Fig. 1b. Our models significantly outperform
the baseline models in terms of per-capita orders at all per-capita
budgets. DPM is on par with the two stage methods in the low
per-capita budgets and outperforms them in the high per-capita
budgets. CN has a marginal improvement of 0.16% compared to
the two-stage methods. Further evaluation is carried out on the
model trained with DFCL loss, which comprises MSE loss ( L𝑃𝐿)
and policy learning loss ( L𝑃𝐿𝐿). The integration of policy learning
loss yielded a notable enhancement in performance, with the con-
strained network showing a significant increase of 1.26%. These
findings suggest that our proposed DFCL approach is versatile
and can be integrated into existing methodologies. Interestingly,
the constraint network combined with policy learning loss (CN +
DFCL-PL) did not outperform DFCL-PL alone. We hypothesize that
this may be due to the predefined constraints within the network,
which potentially restrict the expansiveness of the decision space.
6.2.2 Prediction Loss vs Decision Loss tradeoff. As mentioned above,
we integrate the prediction loss as a regularizer into the training
objective. In this experiment, we will consider how the weight
of the prediction loss affects the performance of DFCL. We set
𝛼∈{0.1,0.5,1,2,3,4,5,10}and measure the per-capita orders under
a fixed per-capita budget. As shown in Fig. 2a, increasing 𝛼in a
certain range does not lead to a decrease in model performance.
6375Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 3: EOM (Marketing data)
Mo
delBudgetImprovement1
2 3 4 5 6
TSM-SL 1.0000±0.0023
1.0300±0.0022 1.0611±0.0023 1.0873±0.0022 1.1140±0.0020 1.1437±0.0021 /
TSM-CF 1.0006±0.0007 1.0306±0.0006 1.0592±0.0004 1.0866±0.0006 1.1109±0.0003 1.1353±0.0008 -0.19%
DPM 0.9983±0.0011 1.0366±0.0010 1.0720±0.0006 1.1050±0.0003 1.1305±0.0010 1.1594±0.0007 1.00%
CN 1.0047±0.0013 1.0339±0.0010 1.0622±0.0007 1.0910±0.0009 1.1151±0.0011 1.1384±0.0015 0.16%
CN+DFCL-PL 0.9995±0.0003 1.0366±0.0008 1.0739±0.0009 1.1071±0.0005 1.1367±0.0006 1.1650±0.0009 1.26%
DFCL-PL 1.0104±0.0005 1.0465±0.0006 1.0812±0.0004 1.1118±0.0007 1.1407±0.0011 1.1638±0.0018 1.98%
DFCL-MER 1.0178±0.0008 1.0501±0.0005 1.0810±0.0002 1.1121±0.0010 1.1410±0.0013 1.1674±0.0009 2.06%
DFCL-IFD 1.0197±0.0012 1.0574±0.0022 1.0902±0.0024 1.1221±0.0026 1.1516±0.0028 1.1796±0.0030 2.85%
However, if𝛼is too large, the prediction loss dominates the training
objective and the model will be reduced to the two-stage method.
The experiment suggests it is possible to choose a value of 𝛼so that
we can achieve better performance and more accurate predictions.
6.2.3 Impact of Lagrange multiplier. Next, we would like to discuss
the impact of the Lagrange multiplier 𝜆on the performance of
DFCL model. Since a given Lagrange multiplier 𝜆corresponds to
the MCKP for a certain budget constraint, DFCL model can learn
allocation policies for different budgets simultaneously by changing
or adding𝜆to the DFCL loss. We set up different combinations
of Lagrange multipliers ( 𝜆∈{{0.1},{0.1,0.5},{0.1,0.5,1.0}}) and
use to train DFCL models. Fig. 2b shows the results using DFCL-
IFD models trained by combinations of Lagrange multipliers. We
can observe that 𝜆is a hyperparameter that can have a significant
impact on model performance. A small 𝜆enables the model to learn
the allocation policy efficiently under high budget and vice versa.
Moreover, models trained with multiple Lagrange multipliers can
balance performance with different budgets.
0 2 4 6 8 10
Alpha1.11001.11251.11501.11751.12001.12251.12501.1275Incremental Reward (Under a given budget)
(a) Impact of the prediction loss
weight𝛼
1 2 3 4 5 6
Budget1.021.041.061.081.101.121.141.161.18Incremental Reward
DFCL_{0.1}
DFCL_{0.1,0.5}
DFCL_{0.1,0.5,1.0}(b) Impact of Lagrange multiplier
Figure 2: Offline experiment results
6.3 Online A/B testing
6.3.1 Setups. We deploy DFCL, DPM and TSM-SL to support a dis-
count campaign in Meituan (a food delivery platform), and conduct
an online A/B testing for four weeks. The experiment contains 310K
online shops and they are randomly divided every day into three
groups called G-DFCL, G-DPM and G-TSL respectively. Each shop
will be assigned a discount 𝑡∈{0,5,10,15,20}as the treatmemt,
which means 𝑡%off for each order whose price meets a given thresh-
old. The marketing goal is to maximize the orders by assigning an
appropriate discount to each store every day for a limited budget.
The online deployment of DFCL is shown in Fig. 3a: (1) Beforethe campaign starts each day, we use the DFCL model to make
predictions and allocate the appropriate discounts to each store
based on budget and other constraints in an offline environment.
(2) The users visit the online shop and get discounts which will
stimulate them to make purchases. (3) During model training, we
use historical random data and resource allocation optimizer to
update the model parameters.
(a) Online deployment of DFCL
1 2 3 4
Week0.951.001.051.101.151.201.25Incremental order
G-TSL
G-DPM
G-DFCL (b) Orders
Figure 3: Online A/B testing
6.3.2 Results. Fig. 3b illustrates the improvement in weekly orders
for G-DFCL and G-DPM relative to G-TSM. In order to preserve
data privacy, all data points in Fig. 3b have been normalized that are
divided by the orders of TSM-SL in the first week. We can see that
DFCL achieves a significant average improvement of 2.17% relative
to TSM-SL and also outperforms DPM with a relative improvement
of 0.85%. The detailed results can be found in Appendix E.
7 Conclusion
In this paper, we propose a decision focused causal learning frame-
work (DFCL) for direct counterfactual marketing optimization,
which overcomes the technological challenges of DFL deployment
in marketing. By designing surrogate losses and constructing black-
box optimisation, we efficiently align the objectives of ML and OR.
Both offline experiments and online A/B testing demonstrate the
effectiveness of DFCL over the state-of-the-art methods.
Acknowledgments
This work was supported in part by National Key R&D Program
of China (2023YFB4502400), the NSF of China (62172206), and the
Xiaomi Foundation.
6376KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Zhou et al.
References
[1]Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al .
2016. Tensorflow: Large-scale machine learning on heterogeneous distributed
systems. arXiv preprint arXiv:1603.04467 (2016).
[2]Meng Ai, Biao Li, Heyang Gong, Qingwei Yu, Shengjie Xue, Yuan Zhang, Yunzhou
Zhang, and Peng Jiang. 2022. LBCF: A Large-Scale Budget-Constrained Causal
Forest Algorithm. In Proceedings of the ACM Web Conference 2022. 2310–2319.
[3]Javier Albert and Dmitri Goldenberg. 2022. E-commerce promotions personaliza-
tion via online multiple-choice knapsack with uplift modeling. In Proceedings of
the 31st ACM International Conference on Information & Knowledge Management.
2863–2872.
[4]Brandon Amos and J Zico Kolter. 2017. Optnet: Differentiable optimization as a
layer in neural networks. In International Conference on Machine Learning. PMLR,
136–145.
[5]Susan Athey, Julie Tibshirani, and Stefan Wager. 2019. Generalized random
forests. (2019).
[6]Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna
Oprescu, and Vasilis Syrgkanis. 2019. EconML: A Python package for ML-Based
heterogeneous treatment effects estimation. Version 0. x (2019).
[7]Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe
Vert, and Francis Bach. 2020. Learning with differentiable pertubed optimizers.
Advances in neural information processing systems 33 (2020), 9508–9519.
[8]Artem Betlei, Eustache Diemert, and Massih-Reza Amini. 2021. Uplift model-
ing with generalization guarantees. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining. 55–65.
[9]Eustache Diemert, Artem Betlei, Christophe Renaudin, and Massih-Reza Amini.
2018. A large scale benchmark for uplift modeling. In KDD.
[10] Priya Donti, Brandon Amos, and J Zico Kolter. 2017. Task-based end-to-end model
learning in stochastic optimization. Advances in neural information processing
systems 30 (2017).
[11] Shuyang Du, James Lee, and Farzin Ghaffarizadeh. 2019. Improve User Retention
with Causal Learning. In The 2019 ACM SIGKDD Workshop on Causal Discovery.
PMLR, 34–49.
[12] Adam N Elmachtoub and Paul Grigas. 2022. Smart “predict, then optimize”.
Management Science 68, 1 (2022), 9–26.
[13] Fredrik Johansson, Uri Shalit, and David Sontag. 2016. Learning representations
for counterfactual inference. In International conference on machine learning.
PMLR, 3020–3029.
[14] Hans Kellerer, Ulrich Pferschy, David Pisinger, Hans Kellerer, Ulrich Pferschy,
and David Pisinger. 2004. The multiple-choice knapsack problem. Knapsack
Problems (2004), 317–347.
[15] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[16] Sören R Künzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. 2019. Metalearners for
estimating heterogeneous treatment effects using machine learning. Proceedings
of the national academy of sciences 116, 10 (2019), 4156–4165.
[17] Finn Kuusisto, Vitor Santos Costa, Houssam Nassif, Elizabeth Burnside, David
Page, and Jude Shavlik. 2014. Support vector machines for differential prediction.
InMachine Learning and Knowledge Discovery in Databases: European Conference,
ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II 14.
Springer, 50–65.
[18] Jayanta Mandi, Vıctor Bucarey, Maxime Mulamba Ke Tchomba, and Tias Guns.
2022. Decision-focused learning: through the lens of learning to rank. In Interna-
tional Conference on Machine Learning. PMLR, 14935–14947.
[19] Jayanta Mandi and Tias Guns. 2020. Interior point solving for lp-based prediction+
optimisation. Advances in Neural Information Processing Systems 33 (2020), 7272–
7282.
[20] Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey,
Tias Guns, and Ferdinando Fioretto. 2023. Decision-focused learning: Foun-
dations, state of the art, benchmark and future opportunities. arXiv preprint
arXiv:2307.13565 (2023).
[21] Jayanta Mandi, Peter J Stuckey, Tias Guns, et al .2020. Smart predict-and-optimize
for hard combinatorial optimization problems. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, Vol. 34. 1603–1610.
[22] Maxime Mulamba, Jayanta Mandi, Michelangelo Diligenti, Michele Lombardi,
Victor Bucarey, and Tias Guns. 2020. Contrastive losses and solution caching for
predict-and-optimize. arXiv preprint arXiv:2011.05354 (2020).
[23] Xinkun Nie and Stefan Wager. 2021. Quasi-oracle estimation of heterogeneous
treatment effects. Biometrika 108, 2 (2021), 299–319.
[24] Mathias Niepert, Pasquale Minervini, and Luca Franceschi. 2021. Implicit MLE:
backpropagating through discrete exponential family distributions. Advances in
Neural Information Processing Systems 34 (2021), 14567–14579.
[25] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019.
Pytorch: An imperative style, high-performance deep learning library. Advances
in neural information processing systems 32 (2019).[26] Marin Vlastelica Pogančić, Anselm Paulus, Vit Musil, Georg Martius, and Michal
Rolinek. 2019. Differentiation of blackbox combinatorial solvers. In International
Conference on Learning Representations.
[27] Jasjeet S Sekhon. 2008. The Neyman-Rubin Model of Causal Inference and
Estimation via Matching Methods. The Oxford Handbook of Political Methodology
2 (2008), 1–32.
[28] Sanket Shah, Kai Wang, Bryan Wilder, Andrew Perrault, and Milind Tambe. 2022.
Decision-focused learning without decision-making: Learning locally optimized
decision losses. Advances in Neural Information Processing Systems 35 (2022),
1320–1332.
[29] Claudia Shi, David Blei, and Victor Veitch. 2019. Adapting Neural Networks for
the Estimation of Treatment Effects. Advances in Neural Information Processing
Systems (NIPS) 32 (2019).
[30] Prabhakant Sinha and Andris A Zoltners. 1979. The multiple-choice knapsack
problem. Operations Research 27, 3 (1979), 503–515.
[31] Stefan Wager and Susan Athey. 2018. Estimation and inference of heterogeneous
treatment effects using random forests. J. Amer. Statist. Assoc. 113, 523 (2018),
1228–1242.
[32] Chao Wang, Xiaowei Shi, Shuai Xu, Zhe Wang, Zhiqiang Fan, Yan Feng, An You,
and Yu Chen. 2023. A Multi-stage Framework for Online Bonus Allocation Based
on Constrained User Intent Detection. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 5028–5038.
[33] Bryan Wilder, Bistra Dilkina, and Milind Tambe. 2019. Melding the data-decisions
pipeline: Decision-focused learning for combinatorial optimization. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 33. 1658–1665.
[34] Ziang Yan, Shusen Wang, Guorui Zhou, Jingjian Lin, and Peng Jiang. 2023. An
End-to-End Framework for Marketing Effectiveness Optimization under Budget
Constraint. arXiv preprint arXiv:2302.04477 (2023).
[35] Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. 2018.
Representation learning for treatment effect estimation from observational data.
Advances in neural information processing systems 31 (2018).
[36] Peng Ye, Julian Qian, Jieying Chen, Chen-hung Wu, Yitong Zhou, Spencer
De Mars, Frank Yang, and Li Zhang. 2018. Customized regression model for
airbnb dynamic pricing. In Proceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining. 932–940.
[37] Yang Zhang, Bo Tang, Qingyu Yang, Dou An, Hongyin Tang, Chenyang Xi,
Xueying Li, and Feiyu Xiong. 2021. BCORLE ( 𝜆): An Offline Reinforcement
Learning and Evaluation Framework for Coupons Allocation in E-commerce
Market. Advances in Neural Information Processing Systems 34 (2021), 20410–
20422.
[38] Kui Zhao, Junhao Hua, Ling Yan, Qi Zhang, Huan Xu, and Cheng Yang. 2019. A
unified framework for marketing budget allocation. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
1820–1830.
[39] Yan Zhao, Xiao Fang, and David Simchi-Levi. 2017. Uplift modeling with mul-
tiple treatments and general response types. In Proceedings of the 2017 SIAM
International Conference on Data Mining. SIAM, 588–596.
[40] Hao Zhou, Shaoming Li, Guibin Jiang, Jiaqi Zheng, and Dong Wang. 2023. Direct
heterogeneous causal learning for resource allocation problems in marketing. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 5446–5454.
A The Proof of Theorem 1
Proof. First of all, we introduce some notations. Following the
potential outcome framework [ 27], let𝑋∈R𝑑denote the feature
vector and𝑇∈{1,2,...,𝑀}be the treatment. Let 𝑌𝑟(𝑇)and𝑌𝑐(𝑇)
be the potential outcome of the revenue and the cost respectively
when the individual receives treatment 𝑇. letb𝑌𝑟(𝑇)andb𝑌𝑐(𝑇)be
the predicted outcome of the revenue and the cost respectively
when the individual receives treatment 𝑇. ForL𝑀𝑆𝐸, we have
L𝑀𝑆𝐸(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=1
𝑁𝑀∑︁
𝑖∑︁
𝑗(𝑟𝑖𝑗−ˆ𝑟𝑖𝑗)2+(𝑐𝑖𝑗−ˆ𝑐𝑖𝑗)2
=E𝑋,𝑇[(𝑌𝑟(𝑇)−b𝑌𝑟(𝑇))2+(𝑌𝑐(𝑇)−b𝑌𝑐(𝑇))2].
6377Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization KDD ’24, August 25–29, 2024, Barcelona, Spain.
ForL𝑃𝐿, we have
L𝑃𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
=1
𝑀∑︁
𝑖1
𝑁𝑡𝑖[(𝑟𝑖𝑡𝑖−ˆ𝑟𝑖𝑡𝑖)2+(𝑐𝑖𝑡𝑖−ˆ𝑐𝑖𝑡𝑖)2]
=1
𝑀∑︁
𝑗∑︁
𝑖:𝑡𝑖=𝑗1
𝑁𝑡𝑖[(𝑟𝑖𝑡𝑖−ˆ𝑟𝑖𝑡𝑖)2+(𝑐𝑖𝑡𝑖−ˆ𝑐𝑖𝑡𝑖)2]
=1
𝑀∑︁
𝑗1
𝑁𝑡𝑖∑︁
𝑖:𝑡𝑖=𝑗[(𝑟𝑖𝑡𝑖−ˆ𝑟𝑖𝑡𝑖)2+(𝑐𝑖𝑡𝑖−ˆ𝑐𝑖𝑡𝑖)2]
=1
𝑀∑︁
𝑗E𝑋[(𝑌𝑟(𝑗)−b𝑌𝑟(𝑗))2+(𝑌𝑐(𝑗)−b𝑌𝑐(𝑗))2|𝑇𝑖=𝑗]
=1
𝑀∑︁
𝑗E𝑋[(𝑌𝑟(𝑗)−b𝑌𝑟(𝑗))2+(𝑌𝑐(𝑗)−b𝑌𝑐(𝑗))2] (𝑇⊥𝑋)
=E𝑋,𝑇[(𝑌𝑟(𝑇)−b𝑌𝑟(𝑇))2+(𝑌𝑐(𝑇)−b𝑌𝑐(𝑇))2],
where𝑇⊥𝑋holds because the data set is from random control
trials (RCT). (RCT). Therefore, we finish the proof. □
B The Proof of Theorem 3
Proof. Follow the notations in Appendix A and let
softmax(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)=exp(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)Í
𝑘exp(ˆ𝑟𝑖𝑘−𝜆ˆ𝑐𝑖𝑘)
be the softmax function. Hence, L′
𝐷𝐷𝐿can be rewritten as
L′
𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
=−∑︁
𝜆∑︁
𝑖∑︁
𝑗(𝑟𝑖𝑗−𝜆𝑐𝑖𝑗)softmax(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)
=−𝑁𝑀∑︁
𝜆1
𝑁𝑀∑︁
𝑖∑︁
𝑗(𝑟𝑖𝑗−𝜆𝑐𝑖𝑗)softmax(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)
=−𝑁𝑀∑︁
𝜆E𝑋,𝑇[(𝑌𝑟(𝑇)−𝜆𝑌𝑐(𝑇))softmax(b𝑌𝑟(𝑇)−𝜆b𝑌𝑐(𝑇))]
In addition, we have
L𝑃𝐿𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)
=−∑︁
𝜆∑︁
𝑖𝑁
𝑁𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)softmax(ˆ𝑟𝑖𝑡𝑖−𝜆ˆ𝑐𝑖𝑡𝑖)
=−∑︁
𝜆∑︁
𝑖𝑁
𝑁𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)softmax(ˆ𝑟𝑖𝑡𝑖−𝜆ˆ𝑐𝑖𝑡𝑖)
=−𝑁∑︁
𝜆∑︁
𝑗∑︁
𝑖:𝑡𝑖=𝑗1
𝑁𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)softmax(ˆ𝑟𝑖𝑡𝑖−𝜆ˆ𝑐𝑖𝑡𝑖)
=−𝑁∑︁
𝜆∑︁
𝑗E𝑋[(𝑌𝑟(𝑗)−𝜆𝑌𝑐(𝑗))softmax(b𝑌𝑟(𝑗)−𝜆b𝑌𝑐(𝑗))|𝑇𝑖=𝑗]
=−𝑁∑︁
𝜆∑︁
𝑗E𝑋[(𝑌𝑟(𝑗)−𝜆𝑌𝑐(𝑗))softmax(b𝑌𝑟(𝑗)−𝜆b𝑌𝑐(𝑗))]
=−𝑁𝑀∑︁
𝜆1
𝑀∑︁
𝑗E𝑋[(𝑌𝑟(𝑗)−𝜆𝑌𝑐(𝑗))softmax(b𝑌𝑟(𝑗)−𝜆b𝑌𝑐(𝑗))]
=−𝑁𝑀∑︁
𝜆E𝑋,𝑇[(𝑌𝑟(𝑇)−𝜆𝑌𝑐(𝑇))softmax(b𝑌𝑟(𝑇)−𝜆b𝑌𝑐(𝑇))].
Therefore,L′
𝐷𝐷𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)=L𝑃𝐿𝐿(𝑟,𝑐, ˆ𝑟,ˆ𝑐)holds.Algorithm 2 Lagrangian duality gradient estimator
Input: training data D≡{(𝑥𝑖,𝑡𝑖,𝑟𝑖𝑡𝑖,𝑐𝑖𝑡𝑖)}𝑁
𝑖=1, Lagrange multiplier
𝜆, the predicted revenue/cost ˆ𝑟/ˆ𝑐
Output:𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟,𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐
1:Initialize𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟=0,𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐=0,𝑧𝑖𝑗=0∀𝑖,𝑗
2:𝑎=ˆ𝑟−𝜆ˆ𝑐
3:∀𝑖,𝑗, 𝑧𝑖𝑗=I𝑗=arg max𝑗(𝑎𝑖𝑗)
4:¯𝑟(ˆ𝑟,ˆ𝑐,𝜆)=1
𝑁Í
𝑖1
𝑝𝑡𝑖𝑟𝑡𝑖I𝑡𝑖=arg max𝑗𝑧𝑖𝑗
5:¯𝑐(ˆ𝑟,ˆ𝑐,𝜆)=1
𝑁Í
𝑖1
𝑝𝑡𝑖𝑐𝑡𝑖I𝑡𝑖=arg max𝑗𝑧𝑖𝑗
6:L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)=¯𝑟(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)−𝜆¯𝑐(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
7:matching_indices= {𝑖|𝑡𝑖=arg max𝑗𝑧𝑖𝑗,∀𝑖}
8:mismatching_indices= {𝑖|𝑡𝑖≠arg max𝑗𝑧𝑖𝑗,∀𝑖}
9:for all𝑖∈matching_indices do
10:ℎ𝑟
𝑖𝑡𝑖=𝑚𝑎𝑥𝑗≠𝑡𝑖𝑎𝑖𝑗−𝑎𝑖𝑡𝑖,ℎ𝑐
𝑖𝑡𝑖=(𝑎𝑖𝑡𝑖−max𝑗≠𝑡𝑖𝑎𝑖𝑗)
𝜆
11:𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟𝑖𝑡𝑖=−1
𝑁𝑝𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)
ℎ𝑟
𝑖𝑡𝑖
12:𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐𝑖𝑡𝑖=−1
𝑁𝑝𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)
ℎ𝑐
𝑖𝑡𝑖
13: for all𝑗∈{1,2,...,𝑀}𝑎𝑛𝑑 𝑗 ≠𝑡𝑖do
14:ℎ𝑟
𝑖𝑗=𝑎𝑖𝑡𝑖−𝑎𝑖𝑗,ℎ𝑐
𝑖𝑗=(𝑎𝑖𝑗−𝑎𝑖𝑡𝑖)
𝜆
15:𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟𝑖𝑗=−1
𝑁𝑝𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)
ℎ𝑟
𝑖𝑗
16:𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐𝑖𝑗=−1
𝑁𝑝𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)
ℎ𝑐
𝑖𝑗
17:for all𝑖∈mismatching_indices do
18:𝑗=arg max𝑗𝑎𝑖𝑗
19:ℎ𝑟
𝑖𝑡𝑖=𝑎𝑖𝑗−𝑎𝑖𝑡𝑖,ℎ𝑟
𝑖𝑗=−ℎ𝑟
𝑖𝑡𝑖
20:ℎ𝑐
𝑖𝑡𝑖=(𝑚𝑎𝑥𝑗𝑎𝑖𝑗−𝑎𝑖𝑡𝑖)
𝜆,ℎ𝑐
𝑖𝑗=−ℎ𝑐
𝑖𝑡𝑖
21:𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟𝑖𝑡𝑖=1
𝑁𝑝𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)
ℎ𝑟
𝑖𝑡𝑖
22:𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐𝑖𝑡𝑖=1
𝑁𝑝𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)
ℎ𝑐
𝑖𝑡𝑖
23:𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟𝑖𝑗=1
𝑁𝑝𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)
ℎ𝑟
𝑖𝑗
24:𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐𝑖𝑗=1
𝑁𝑝𝑡𝑖(𝑟𝑖𝑡𝑖−𝜆𝑐𝑖𝑡𝑖)
ℎ𝑐
𝑖𝑗
25:return𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑟,𝜕L𝐷𝐷𝐿(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
𝜕ˆ𝑐
For∀𝑖,𝑗=arg max𝑘𝑟𝑖𝑘−𝜆𝑐𝑖𝑘, let ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗→+∞ ; for∀𝑖,𝑗≠
arg max𝑘𝑟𝑖𝑘−𝜆𝑐𝑖𝑘, letˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗→−∞. Hence, we have
softmax(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)→I𝑗=arg max𝑘ˆ𝑟𝑖𝑘−𝜆ˆ𝑐𝑖𝑘.
Therefore, we further get
min
ˆ𝑟,ˆ𝑐L𝑃𝐿𝐿(𝜆,ˆ𝑟,ˆ𝑐)=min
ˆ𝑟,ˆ𝑐L′
𝐷𝐷𝐿(𝜆,ˆ𝑟,ˆ𝑐)=min
ˆ𝑟,ˆ𝑐L𝐷𝐷𝐿(𝜆,ˆ𝑟,ˆ𝑐).
□
C Policy Evaluation Based on EOM
Given a batch of 𝑁random samples and model predictions ˆ𝑟and
ˆ𝑐, we can use binary search to empirically estimate the per capita
6378KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Zhou et al.
Table 4: Online A/B testing results with the confidence interval
GroupWeekImprovement1st 2nd 3rd 4th
G-TSL 1.0000±0.00285 1.1706±0.00298 1.1565±0.00293 1.0851±0.00289 /
G-DPM 1.0113±0.00284 1.1891±0.00298 1.1704±0.00293 1.1000±0.00288 1.32%
G-DFCL 1.0235±0.00285 1.2062±0.00297 1.1786±0.00293 1.1000±0.00288 2.17%
Algorithm 3 An implementation of per capita revenue estimation
for primal MCKP with budget 𝐵
Input: training data D≡{(𝑥𝑖,𝑡𝑖,𝑟𝑖𝑡𝑖,𝑐𝑖𝑡𝑖)}𝑁
𝑖=1, the budget 𝐵, the
predicted revenue/cost ˆ𝑟/ˆ𝑐, a small constant 𝜖
Output: the expected per capita revenue ¯𝑟(𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
1:Initialize𝜆min=0,𝜆max=max𝑖,𝑗(ˆ𝑟𝑖𝑗
ˆ𝑐𝑖𝑗),𝑧𝑖𝑗=0∀𝑖,𝑗
2:while𝐵
𝑁−¯𝑐(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)<𝜖do
3:𝜆=𝜆max+𝜆min
2
4:∀𝑖,𝑗, 𝑧𝑖𝑗=I𝑗=arg max𝑗(ˆ𝑟𝑖𝑗−𝜆ˆ𝑐𝑖𝑗)
5: ¯𝑟(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)=1
𝑁Í
𝑖1
𝑝𝑡𝑖𝑟𝑡𝑖I𝑡𝑖=arg max𝑗𝑧𝑖𝑗
6: ¯𝑐(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)=1
𝑁Í
𝑖1
𝑝𝑡𝑖𝑐𝑡𝑖I𝑡𝑖=arg max𝑗𝑧𝑖𝑗
7:if𝐵
𝑁−¯𝑐(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)>0then
8:𝜆max=𝜆
9:else
10:𝜆min=𝜆
11:¯𝑟(𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐)=¯𝑟(𝜆,𝑟,𝑐, ˆ𝑟,ˆ𝑐)
12:return the expected per capita revenue ¯𝑟(𝐵,𝑟,𝑐, ˆ𝑟,ˆ𝑐)revenue under a per capita budget𝐵
𝑁, Algorithm 3 summarizes this
approach.
D Lagrangian Duality Gradient Estimator
The decision making is independent for each individual thanks
to the decomposition of the Lagrangian duality theory. Thus, for
each sample, the smallest perturbation that causes a change in the
dual decison loss is first calculated, and the loss after the perturba-
tion is obtained by correcting only the original result. Algorithm 2
provides details of the modified gradient estimator, which greatly
reduces the computational overhead. Note that for comprehensibil-
ity, Algorithm 2 is described with for loops, while in practice we
work with matrix operations.
E Supplementary Experimental Results
Tabel 4 presents the detailed online A/B testing results. In order
to preserve data privacy, all data points have been normalized by
dividing by the orders of TSM-SL in the first week. The confidence
interval (𝛼=0.05) is computed by a t-test.
6379