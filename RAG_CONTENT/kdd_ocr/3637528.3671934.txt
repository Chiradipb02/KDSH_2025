TDNetGen: Empowering Complex Network Resilience Prediction
with Generative Augmentation of Topology and Dynamics
Chang Liu
Department of Electronic Engineering,
BNRist, Tsinghua University
Beijing, China
lc23@mails.tsinghua.edu.cnJingtao Ding∗
Department of Electronic Engineering,
BNRist, Tsinghua University
Beijing, China
dingjt15@tsinghua.org.cn
Yiwen Song
Shenzhen International Graduate School,
Tsinghua University
Shenzhen, Guangdong, China
songyw23@mails.tsinghua.edu.cnYong Li∗
Department of Electronic Engineering,
BNRist, Tsinghua University
Beijing, China
liyong07@tsinghua.edu.cn
ABSTRACT
Predicting the resilience of complex networks, which represents
the ability to retain fundamental functionality amidst external per-
turbations or internal failures, plays a critical role in understanding
and improving real-world complex systems. Traditional theoretical
approaches grounded in nonlinear dynamical systems rely on prior
knowledge of network dynamics. On the other hand, data-driven
approaches frequently encounter the challenge of insufficient la-
beled data, a predicament commonly observed in real-world sce-
narios. In this paper, we introduce a novel resilience prediction
framework for complex networks, designed to tackle this issue
through generative data augmentation of network topology and
dynamics. The core idea is the strategic utilization of the inherent
joint distribution present in unlabeled network data, facilitating
the learning process of the resilience predictor by illuminating the
relationship between network topology and dynamics. Experiment
results on three network datasets demonstrate that our proposed
framework TDNetGen can achieve high prediction accuracy up
to 85%-95%. Furthermore, the framework still demonstrates a pro-
nounced augmentation capability in extreme low-data regimes,
thereby underscoring its utility and robustness in enhancing the
prediction of network resilience. We have open-sourced our code in
the following link, https://github.com/tsinghua-fib-lab/TDNetGen.
CCS CONCEPTS
•Computing methodologies →Knowledge representation
and reasoning; •Applied computing →Physics .
KEYWORDS
Complex Network; Resilience Prediction; Diffusion models; Semi-
supervised Learning; Data Augmentation
∗Jingtao Ding and Yong Li are corresponding authors.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671934
Real-world systemsComplex network
Nodal state dynamicsPerturbationRecoveredPerturbationNot-recoveredResilient networkNon-resilient networkNetwork resilience
EcosystemsGene regulatoryNeuron interactionFigure 1: Resilience of complex networks. ⟨x⟩denotes the
averaged nodal state of the network.
ACM Reference Format:
Chang Liu, Jingtao Ding, Yiwen Song, and Yong Li. 2024. TDNetGen: Em-
powering Complex Network Resilience Prediction with Generative Aug-
mentation of Topology and Dynamics. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671934
1 INTRODUCTION
Real-world complex systems across various domains, such as eco-
logical [ 19], gene regulatory [ 3], and neurological networks [ 51,52],
are often described as complex networks composed of intercon-
nected nodes with weighted links. A fundamental characteristic
of these systems is their resilience [ 17,38], that is, the ability to
maintain functionality in the face of disruptions. From the per-
spective of dynamical systems, nodal state evolution of complex
networks is driven by underlying nonlinear dynamics. Specifically,
with the functionality of each node represented by its state value, a
resilient network can recover from disruptions (on its nodes) and
dynamically evolve into a stable phase where all nodes operate at a
high level of activity (see Figure 1). Understanding and predicting
this critical property of resilience in complex networks not only
enhances our ability to analyze and intervene in natural and social
systems [ 17,42,44,59] but also offers valuable insights for the
design of engineered infrastructures [53].
To predict network resilience, theories grounded in nonlinear
dynamical systems have been developed [ 17,26,30,60]. These
frameworks strive to separate the influences of network structure
and dynamics to derive analytical solutions for complex, high-
dimensional systems [ 6,16,41]. However, theoretical approaches
 
1875
KDD ’24, August 25–29, 2024, Barcelona, Spain Chang Liu, Jingtao Ding, Yiwen Song and Yong Li
often presuppose a detailed understanding of network evolution
dynamics, which is usually not available in practical scenarios. In
contrast, data-driven methods are capable of extracting both struc-
tural and dynamic information about networks directly from obser-
vational data [ 12,32,35,36,50], allowing for resilience predictions
without the need for predefined knowledge. From this perspective,
the task of predicting network resilience can be reinterpreted as
agraph classification problem based on data of network structure
and dynamics using machine learning techniques. Nonetheless, the
crucial role of resilience in system functionality means that collect-
ing extensive labeled datasets from real-world complex networks is
both expensive and impractical. As a result, the majority of network
observations remain unlabeled, possessing information on network
topology and nodal state trajectories but lacking resilience labels.
In this paper, we focus on addressing the problem of predicting
network resilience amidst a scarcity of labeled data, identifying two
primary obstacles:
Firstly, designing models for resilience prediction is inherently
complex due to the intricate interplay between network structure
and dynamics. A network is considered resilient if it can consistently
return to a state where all nodes are active following a prolonged pe-
riod of self-evolution and neighborly interactions. However, while
topological data is readily available, constructing a practical model
requires the capability to make accurate predictions based on partial
evolution trajectories collected from a short time window.
Secondly, enhancing prediction accuracy in the face of scarce
labels involves leveraging the intrinsic information embedded in
unlabeled data regarding network structure and dynamics. Existing
methodologies mainly include pseudo-labeling [ 4,31,45], exempli-
fied by self-training [ 25], and self-supervised learning [ 21,28,48,
54]. Pseudo-labeling tends to underperform with high model un-
certainty, and self-supervised learning often overlooks the critical
interplay between structure and dynamics, treating state evolution
trajectories merely as node attributes. The graph data augmen-
tation method [ 18] emerges as a leading technique by utilizing
unlabeled data distribution to generate diverse augmented samples
for improved training. However, the challenge of comprehensively
characterizing the distribution of both topology and dynamics in
unlabeled data has yet to be tackled, especially with limited obser-
vations, such as a few labeled networks and incomplete evolution
trajectories.
To fully resolve these challenges, we introduce a novel resilience
prediction framework called TDNetGen, which utilizes generative
augmentation of network topology and dynamics. The core of TD-
NetGen is a neural network-based predictor that integrates a graph
convolutional network-based topology encoder together with a
transformer-based trajectory encoder, capturing the complex rela-
tionship between network structure and dynamics. This predictor is
further refined through training on an augmented dataset compris-
ing resilient and non-resilient samples, i.e., networks with topology
information and evolution trajectories.
TDNetGen leverages a generative data augmentation approach
by 1) capturing the underlying joint distribution of topology and
dynamics in unlabeled data, and 2) obtaining the corresponding con-
ditional distribution for each class label through a classifier-guided
approach [ 11]. To facilitate effective generative learning in the vast
joint space of topology and dynamics, we decouple the generationprocess into topology generation using a topology denoising dif-
fusion module and dynamics simulation with a dynamics learning
module. To ensure robust learning with limited observations, we
incorporate a fine-tuning step for the resilience predictor on gen-
erated trajectories, thereby improving its generalization ability on
unseen data. To summarize, our main contributions are as follows.
•We tackle the critical problem of predicting complex network re-
silience under label sparsity issue and provide a novel perspective
of improving by data augmentation.
•We design a generative augmentation framework that benefits
resilience predictor learning of interplay between network topol-
ogy and dynamics by exploiting the underlying joint distribution
in unlabeled data.
•Empirical results on three network datasets demonstrate the
superiority of our TDNetGen over state-of-the-art baselines in
terms of increasing network resilience prediction accuracy up
to 85%-95%. Moreover, aided by a generative learning capability
of both topology and dynamics, TDNetGen can provide robust
augmentation in low-data regimes, maintaining 98.3%of perfor-
mance even when dynamic information cannot be observed in
unlabeled data.
2 PRELIMINARIES
2.1 Resilience Prediction
Network resilience articulates that a resilient system is character-
ized by its invariable convergence towards a desired, non-trivial
stable equilibrium following perturbation [ 17]. Formally, given a
complex network 𝐺=(V,A), where V={𝑣1,𝑣2,···,𝑣𝑁}repre-
sents its node set and Adenotes the adjacency matrix. The state of
node𝑖can be represented as 𝑥𝑖, usually governed by the following
non-linear ordinary differential equations (ODEs) as the nodal state
dynamics:
𝑑𝑥𝑖
𝑑𝑡=𝐹(𝑥𝑖)+𝑁∑︁
𝑗=1𝐴𝑖𝑗𝐺(𝑥𝑖,𝑥𝑗), (1)
where𝐹(𝑥𝑖)represents the self-dynamics of nodes and 𝐺(𝑥𝑖,𝑥𝑗)
denotes interaction dynamics. The complex network 𝐺is consid-
ered resilient if it consistently converges to only the desired nodal
state equilibrium as time 𝑡approaches infinity, irrespective of any
perturbation and varying initial conditions with the exception of
its fixed points.
2.2 Problem Formulation
Considering the challenge of obtaining detailed knowledge of the
underlying equations that govern nodal state dynamics in real-
world scenarios, in this work, we advocate for a purely data-driven
approach to predict network resilience. In the context of the re-
silience prediction task, our dataset comprises network samples
from which we can extract both topology and the initial 𝑇steps of
𝑀nodal state trajectories prior to reaching a steady state. Formally,
for a network comprising 𝑁nodes, the topology is represented by
an adjacency matrix A∈R𝑁×𝑁, while the observed nodal state
trajectories are denoted as X∈R𝑀×𝑁×𝑇. As demonstrated in
Section 2.1, determining the resilience of a network precisely neces-
sitates knowledge of its steady-state conditions, a requirement that
is often prohibitive to meet due to the high observational costs (e.g.,
long-term species population growth). Consequently, only a limited
 
1876TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain
Resilience predictorTopology diffusion moduleDynamics learning module1Pre-training
Unlabeled networksLabeled networks……+ Resilience label2Data augmentation...3Predictor re-trainingGenerated resilient topology(Setting 𝒚𝑮=𝟏)(Setting 𝒚𝑮=𝟎)Dynamics learning moduleResilience predictor
Frozen
TrainingInput/OutputGuidanceGenerated non-resilient topology
...Dynamics learning module
Resilience predictor…12NNodeTimeNodal state trajectories𝐺%𝐺&𝐺'PredictionPreset 𝑦! 
GradientsGuidance loss calculationBCE Loss
…12NNodeTimeNodal state trajectories
Figure 2: Overview of the proposed framework TDNetGen.
subset of network samples are labeled, denoted as P, with the ma-
jority remaining unlabeled, denoted as Q, where|P|is significantly
smaller than|Q|. The reliance on a narrow labeled dataset Pfor
training the resilience prediction model could result in sub-optimal
performance due to the constrained sample size. In this work, we
endeavor to leverage the untapped potential of the unlabeled data Q
to enhance the training process of the resilience prediction model,
with the objective of achieving superior predictive accuracy.
3 METHODOLOGY
3.1 Overview of Proposed Framework
In this section, we propose an effective method named TDNetGen
to address the problem of complex network resilience prediction
with limited labeled data samples via generative augmentation of
topology and dynamics. Figure 2 illustrates the holistic design of
TDNetGen, which consists of the following components:
•Topology diffusion module. To facilitate resilience prediction
performance and address the lack of labeled data, we design a
diffusion module to model the distribution of unlabeled network
topology. Therefore, we can sample new network topologies from
the learned distribution.
•Dynamics learning module. We propose a neural ODE [ 8,58]-
based dynamics learning module to learn nodal state changes
of networks from observed trajectories. It can simulate nodal
state trajectories for the generated topologies from the topology
diffusion module.
•Resilience predictor. We design a resilience predictor empow-
ered by Transformer and graph convolutional networks (GCNs),
which jointly models nodal state dynamics and node interactions
from observed trajectories and network topologies, respectively.
It learns a low-dimensional embedding for each network and
predicts its resilience based on this representation.
In our proposed framework, we first train both the dynamics learn-
ing module and the topology diffusion module utilizing unlabeled
as well as labeled nodal state trajectories and network topologies,
respectively, which is then followed by the pre-training of the re-
silience predictor using accessible labeled data. Subsequently, we
generate new samples facilitated by the topology diffusion module
and dynamics learning module, with the guidance provided by the
resilience predictor. The newly generated samples further enhancethe training of the resilience predictor, thereby creating a synergis-
tic feedback loop that significantly improves its predictive accuracy.
3.2 Topology Diffusion Module
Existing continuous graph diffusion models [ 27,40] undermine the
sparsity nature of topologies and usually result in complete graphs
lacking physically meaningful edges. Consequently, they fail to
capture the structural properties of complex networks. Therefore,
we propose to model the distribution of network topologies using
the discrete-space diffusion model [ 5,49], as illustrated in Figure 3.
Different from diffusion models for images with continuous Gauss-
ian noise, here we apply a discrete type of noise on each edge, and
the type of each edge can transition to another during the diffusion
process. Here, we define the transition probabilities of all edges
at time step 𝑠as matrix Q𝑠, where Q𝑠
𝑖𝑗=𝑞(𝑒𝑠=𝑗|𝑒𝑠−1=𝑖)de-
notes the type of edge 𝑒transits to𝑗from𝑖at time step 𝑠. The
forward process of adding noise of each time step to graph struc-
ture𝐺is equivalent to sampling the edge type from the categorical
distribution, formulated as:
𝑞(𝐺𝑠|𝐺𝑠−1)=E𝑠−1Q𝑠,𝑞(𝐺𝑠|𝐺)=E𝑠−1¯Q𝑠, (2)
where E∈R𝑁×𝑁×2is the expanded adjacency matrix from 𝐴. Its
last dimension is a 2-D one-hot vector where [0,1]denotes an edge
exists between the corresponding nodes, while [1,0]denotes there
is no edge. ¯Q𝑠=Q1...Q𝑠. The reverse process aims to gradually
recover the clean graph 𝐺given a noisy graph 𝐺𝑠. Towards this
end, inspired by existing works [ 5,49], we train a parameterized
neural network ℎ𝜃which takes the noisy graph 𝐺𝑠as input and
predicts the structure of the clean graph 𝐺,i.e.,all the probability
ˆ𝑝𝑖𝑗of the existence of an edge 𝑒𝑖𝑗between node 𝑖and𝑗in the clean
graph𝐺. We use the cross-entropy loss to optimize parameters 𝜃,
formulated as follows:
L𝐵𝐶𝐸=1
𝑁2∑︁
1≤𝑖,𝑗≤𝑁CrossEntropy(𝑒𝑖𝑗,ˆ𝑝𝑖𝑗). (3)
For the parameterization of ℎ𝜃, we employ the widely-recognized
backbone of multi-layer graph transformers proposed by Dwivedi et
al. [14]. Intuitively, node features are updated in each layer through
the self-attention mechanism, and edge features are updated from
 
1877KDD ’24, August 25–29, 2024, Barcelona, Spain Chang Liu, Jingtao Ding, Yiwen Song and Yong Li
Noisy network (𝐺!)...𝑞(𝐺!|𝐺!"#)𝑞(𝐺!"#|𝐺!,𝐺)𝐺!𝐺!"#Forward processReverse processOriginal network (𝐺)...
ℎ$𝐺!=𝑝$(𝐺|𝐺!)𝑝$(𝐺|𝐺!)𝑝$(𝐺!"#|𝐺!,𝐺)𝑞(𝐺"|𝐺)
Cross-entropyPredicted network (&𝐺)
Figure 3: Illustration of topology diffusion module.
the information of its head and tail nodes. We describe the details
of the parameterization network in Appendix A.1.
Once we train the neural network ℎ𝜃, it can be applied to gener-
ate new network topologies. Specifically, the reverse process needs
to estimate𝑝𝜃(𝐺𝑠−1|𝐺𝑠), which can be decomposed as follows,
𝑝𝜃(𝐺𝑠−1|𝐺𝑠)=Ö
1≤𝑖,𝑗≤𝑁𝑝𝜃(𝑒𝑠−1
𝑖𝑗|𝐺𝑠). (4)
Each term in Equ. (4) can be formulated as,
𝑝𝜃(𝑒𝑠−1
𝑖𝑗|𝐺𝑠)=∑︁
𝑒∈E𝑝𝜃(𝑒𝑠−1
𝑖𝑗|𝑒𝑖𝑗=𝑒,𝐺𝑠)ˆ𝑝𝑖𝑗(𝑒), (5)
where
𝑝𝜃
𝑒𝑠−1
𝑖𝑗|𝑒𝑖𝑗=𝑒,𝐺𝑠
=(
𝑞
𝑒𝑠−1
𝑖𝑗|𝑒𝑖𝑗=𝑒,𝑒𝑠
𝑖𝑗
if𝑞
𝑒𝑠−1
𝑖𝑗|𝑒𝑖𝑗=𝑒
>0
0 otherwise
(6)
can be calculated with Bayesian rule. After sampling for preset 𝑆
steps, we can generate new network topologies which follow the
distribution of the training dataset.
3.3 Dynamics Learning Module
Through the topology diffusion module, we can generate new net-
work topologies for the training of resilience predictor. Nonetheless,
we also need to obtain their nodal states trajectories to predict their
resilience. As illustrated in Section 2.1, nodal state dynamics in com-
plex networks usually have the generalized form of an ordinary
differential equation (ODE) as:
𝑑x(𝑡)
𝑑𝑡=𝑓(x,𝐺,W,𝑡), (7)
where x(𝑡)∈R𝑁represents nodal states of 𝑁-nodes network 𝐺
at time step 𝑡,𝑓(·)denotes the dynamics function, and Wdenotes
all dynamics parameters. Therefore, we develop a dynamics learn-
ing module designed to infer changes in nodal states solely from
data, which learns nodal state dynamics in the expressive hidden
space based on neural-ODE [ 8,58]. Given the initial state x(0)of
all network nodes, for each time step 𝑡, the process initiates by
mapping the state of the nodes to a latent space through an encoder
𝑓𝑒. Subsequently, graph neural networks (GNNs) are utilized as a
parameterization technique to facilitate the learning of dynamics
within this latent space. The transition from latent space represen-
tation back to the nodal state at each time step is accomplished by
employing a decoder function 𝑓𝑑, which decodes the hidden spaceembeddings to reconstruct the nodal states. The procedure can be
represented as:
xℎ(𝑡)=𝑓𝑒(x(𝑡)), (8)
𝑑xℎ(𝑡)
𝑑𝑡=GNN(xℎ(𝑡)), (9)
ˆx(𝑡+𝛿)=x(𝑡)+∫𝑡+𝛿
𝑡𝑓𝑑(𝑑xℎ(𝑡)
𝑑𝑡), (10)
where GNN can be implemented as an arbitrary design type of
graph neural network layers. In our works, without the loss of
generality, we choose to implement both encoder 𝑓𝑒and decoder 𝑓𝑑
functions using MLPs. Furthermore, GNN is instantiated through
graph convolutional networks [ 29], thereby leveraging their robust
capabilities in capturing and processing the inherent topological
features of graphs. We use ℓ1-loss to train the dynamics learning
module, formulated as follows:
L1=1
|P|+|Q||P|+|Q|∑︁
𝑖=1∫𝑇
0|x𝑖(𝑡)−ˆx𝑖(𝑡)|. (11)
As shown in Equ. (11), we train the dynamics learning module on
both labeled dataset Pand unlabeled dataset Qto achieve a better
performance. It is noteworthy that in Section 4.2, we demonstrate
the dynamics learning module can also perform well even when
the nodal states of unlabeled data are inaccessible.
3.4 Resilience Predictor
We design a resilience predictor to jointly model the dynamics and
topology of networks, which leverages stacked Transformer [ 47]
encoder layers and graph convolutional layers [ 29] to encode the
temporal correlations of nodal states and learn spatial interactions
within network topology, respectively. We illustrate its architecture
in Figure 4. Specifically, for a network with 𝑁nodes, we denote
the nodal states with 𝑑observed steps and 𝑀trajectories of node 𝑢
asx𝑢∈R𝑑×𝑀. For the𝑘-th trajectory x𝑢,𝑘∈R𝑑, we first input its
states of each time step to a feed-forward layer, and further encode
the temporal correlation between time steps with Transformer
encoder layers, formulated as follows,
z𝑢,𝑘=TransformerEncoder (x𝑢,𝑘W1+b1), (12)
where W1∈R1×𝑑𝑒andb1∈R𝑑𝑒are trainable parameters. Af-
ter that, we integrate the embedding of the terminal time step of
all nodes in the network, denoted by Z𝑘∈R𝑁×𝑑𝑒, as their𝑘-th
trajectory embeddings.
To capture the interactions of nodes within the topology, we
design a graph convolutional network (GCN) empowered by multi-
layer message-passing. Given the adjacency matrix of network
topology A, we first calculate the Laplacian operator Ψ=I−
D−1
2
𝑖𝑛AD−1
2
𝑜𝑢𝑡, where the diagonal of D𝑖𝑛andD𝑜𝑢𝑡represent the in-
and out-degree of nodes. We input the 𝑘-th trajectory embeddings
of nodes to the graph convolutional network. The 𝑙-th layer message
passing of the designed GCN can be represented as follows:
Z(𝑙)
𝑘=𝑓(Z(𝑙−1)
𝑘)+𝑔(ΨZ(𝑙−1)
𝑘), (13)
where𝑓(·)and𝑔(·)are implemented as MLPs. Such message-
passing design is motivated by Equ. (1), aiming to more precisely
model the effects from both the node itself and its neighborhood
on a specific node.
 
1878TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain
It is noteworthy that during the aforementioned procedure, dif-
ferent trajectories are processed in parallel. We further introduce a
trajectory attention module to integrate the information from dif-
ferent trajectories for network-level representation. Specifically, we
treat node embedding matrix of different trajectories after 𝐿layers’
message passing as a combination of feature maps, and denote the
results after mean and max pooling as Z(𝐿)
𝑎𝑣𝑔∈R𝑀andZ(𝐿)
𝑚𝑎𝑥∈R𝑀,
respectively. After that, we feed them into a shared MLP, add and
activate the outputs to compute attention weights of trajectories,
formulated as:
𝛼=𝜎(MLP(Z(𝐿)
𝑎𝑣𝑔)+MLP(Z(𝐿)
𝑚𝑎𝑥)), (14)
where𝛼∈R𝑀, and𝜎(·)denotes the sigmoid activation function.
Therefore, the fused node embedding matrix can be derived from:
Z=𝑀∑︁
𝑘=1𝛼𝑘⊙Z(𝐿)
𝑘, (15)
where Z∈R𝑁×𝑑𝑒,𝛼𝑘is the attention weight for the 𝑘-th trajectory,
and⊙denotes Hadamard product. We use a readout function to
derive the embedding of the entire network, i.e.,e𝑛𝑒𝑡=Readout(Z).
Here, we implement the readout function as mean pooling between
nodes. We then predict the resilience ˆ𝑦of the network using e𝑛𝑒𝑡
as follows:
ˆ𝑦=MLP(e𝑛𝑒𝑡). (16)
Then we can train the resilience predictor with binary cross-entropy
(BCE) loss,
LBCE=|N|∑︁
𝑖=1𝑦𝑖log(ˆ𝑦𝑖)+(1−𝑦𝑖)log(1−ˆ𝑦𝑖), (17)
where𝑦𝑖and𝑦denote the ground truth and the prediction result
of the𝑖-th network.|N|is the number of networks used for train-
ing. However, its predictive performance typically falls below the
optimal level, primarily attributed to the scarcity of data.
It is noteworthy that after training resilience predictor on la-
beled data, we further fine-tune the predictor on identical topolo-
gies wherein the nodal state trajectories are generated through
the neural-ODE of the dynamics learning module. It enables the
resilience predictor to accurately accommodate the minor discrep-
ancies observed between the ground-truth trajectories and those
generated through simulation, thereby ensuring the robust predic-
tive performance.
3.5 Joint Data Augmentation of Topology and
Dynamics
The above modules enable us to generate network samples with
both topology and nodal state trajectories. However, it is important
to note that the simulated nodal states are confined to the initial tem-
poral period, corresponding to the maximal duration present within
the training dataset, and compelling the dynamics learning mod-
ule to simulate time steps beyond its training scope yields results
of questionable reliability. Consequently, the principal challenge
arises from the inability to ascertain the steady-state conditions of
the generated networks. This limitation obstructs the direct acquisi-
tion of resilience labels, presenting a significant impediment to the
data augmentation. To overcome this problem, we advocate for the
strategy of guiding the topology diffusion module, enabling it to
123NNode…………TimeTimeTrajectory 1Trajectory 𝑀GCN layers
Layer 1Layer 2Nodal state trajectoriesTransformer encodersFully-connected layer
TanhNetwork topologyEncoder layer 1Encoder layer 2Encoder layer 𝑛...Trajectory 1...Trajectory 𝑀
Trajectory 1...Trajectory 𝑀Trajectory embedding aggregationResilience predictionFigure 4: Architecture of the resilience predictor.
generate networks with predefined resilience characteristics. More
precisely, we integrate classifier guidance [ 11] into the topology
diffusion model, which leverages signals derived from the resilience
predictor trained on the labeled dataset. The conceptual basis of the
guidance mechanism involves that the resilience predictor provides
the resilience condition of the clean samples from the intermediate
samples generated by the diffusion model, which in turn, steers the
generation process towards exhibiting desired resilience character-
istics. To formally define the guided diffusion process, we provide
the following lemma from [11]:
Lemma 3.1. Denote the forward process conditioned on 𝑦𝐺asˆ𝑞,
and the unconditional forward process as 𝑞. Given the reasonable
assumption ˆ𝑞(𝐺𝑠|𝐺,𝑦𝐺)=𝑞(𝐺𝑠|𝐺), we have
ˆ𝑞(𝐺𝑠−1|𝐺𝑠,𝑦𝐺)∝𝑞(𝐺𝑠−1|𝐺𝑠)ˆ𝑞(𝑦𝐺|𝐺𝑠−1) (18)
An direct estimation of 𝑞(𝐺𝑠−1|𝐺𝑠)ˆ𝑞(𝑦𝐺|𝐺𝑠−1)is to use
𝑝𝜃(𝐺𝑠−1|𝐺𝑠)𝑝𝜂(𝑦𝐺|𝐺𝑠−1), (19)
where𝑝𝜂are parameterized by the resilience predictor. However,
we cannot evaluate all possible values of 𝐺𝑠−1. A viable method is
to treat𝐺as a continuous tensor of order 𝑁2, and use the first-order
approximation from Taylor expansion [49], as
logˆ𝑞(𝑦𝐺|𝐺𝑠−1)≈logˆ𝑞(𝑦𝐺|𝐺𝑠)+
∇𝐺logˆ𝑞 𝑦𝐺|𝐺𝑠,𝐺𝑠−1−𝐺𝑠
(20)
≈∑︁
1≤𝑖,𝑗≤𝑁D
∇𝑒𝑖𝑗logˆ𝑞 𝑦𝐺|𝐺𝑠,𝑒𝑠−1
𝑖𝑗E
+𝐶(𝐺𝑠),
(21)
where𝐶(𝐺𝑠)is a function that only relates to 𝐺𝑠. Assume that
ˆ𝑞(𝑦𝐺|𝐺𝑠)∼Bernoulli(𝑓𝜂(𝐺𝑠)), where𝑓𝜂is the resilience predictor,
we have
∇𝐺𝑠logˆ𝑞𝜂 𝑦|𝐺𝑠∝−∇𝐺𝑠L𝐵𝐶𝐸(ˆ𝑦,𝑦𝐺) (22)
Drawing upon the aforementioned theoretical framework, at the
step𝑠of the reverse process, we first employ the resilience predictor
𝑓𝜂to predict𝑦𝐺,i.e.,ˆ𝑦𝐺=𝑓𝜂(𝐺𝑠), and estimate the 𝑝𝜂(𝑦𝐺|𝐺𝑠−1)
as
𝑝𝜂(𝑦𝐺|𝐺𝑠−1)∝exp(−𝜆⟨∇𝐺𝑠L𝐵𝐶𝐸(ˆ𝑦𝐺,𝑦𝐺),𝐺𝑠−1⟩),(23)
 
1879KDD ’24, August 25–29, 2024, Barcelona, Spain Chang Liu, Jingtao Ding, Yiwen Song and Yong Li
where𝜆represents the guidance intensity. Hence, we can sample
𝐺𝑠−1from
𝐺𝑠−1∼𝑝𝜃(𝐺𝑠−1|𝐺𝑠)𝑝𝜂(𝑦𝐺|𝐺𝑠−1), (24)
where𝑝𝜃(𝐺𝑠−1|𝐺𝑠)can be calculate from Equ. (4)-(6).
Consequently, by setting 𝑦𝐺=1and𝑦𝐺=0, we can generate
novel labeled network topologies guided by the resilience predictor.
These topologies subsequently serve as inputs to simulate their re-
spective nodal state trajectories via the dynamics learning module.
This approach facilitates the augmentation of our datasets with ad-
ditional fully labeled data, which, in turn, allows for the re-training
of the resilience predictor. Such a method is anticipated to signifi-
cantly enhance the predictive accuracy of the resilience predictor,
ensuring a more reliable assessment of network resilience under
conditions of data sparsity.
3.6 Time Complexity Analysis
We define𝑁as the number of nodes in a graph, and analyze time
complexity of each module in our framework as follows.
•Topology diffusion module is parameterized using Graph-
Transformer layers (Appendix A.1). It exhibits a time complexity
ofO(𝑁2)per layer, attributable to the computation of attention
scores and the prediction process for each edge.
•Dynamics learning module is based on neural-ODE and pa-
rameterized through GCN layers. This module also demonstrates
a time complexity of O(𝑁2)resulting from convolution opera-
tions and the application of a fourth-order Runge-Kutta ODE
solver.
•Resilience predictor leverages stacked Transformer encoder
layers to capture temporal correlations among nodal states, while
spatial interactions within the network topology are discerned
through GCN layers. Time complexities of the Transformer en-
coder layers and GCN layers are O(𝑁𝑇2)andO(𝑁2), respec-
tively, with 𝑇representing the trajectory length. Typically, 𝑇is
significantly smaller than 𝑁for most graph structures.
Consequently, the overall time complexity of TDNetGen is domi-
nantlyO(𝑁2), signifying its scalability and efficiency in processing
large graph structures. In practical experiments, our framework
takes about 10 seconds to generate a 200-nodes graph with nodal
state trajectories and 20 milliseconds to predict its resilience. Since
resilience inference is not a real-time task, such time complexity is
acceptable for application.
4 EXPERIMENTS
In this section, we demonstrate the superior performance of our
framework TDNetGen, aiming to answer the following research
questions:
•RQ1: How does our framework TDNetGen compare to poten-
tial baseline methods of harnessing unlabeled data to enhance
predictive performance?
•RQ2: How do different designs of TDNetGen affect the model
performance?
•RQ3: How does TDNetGen perform across limited numbers of
original labeled samples and lengths of nodal state trajectories?
•RQ4: How does TDNetGen perform with different network types
and scales?
4.1 Experimental SettingsTable 1: Statistics of network datasets.
Mutualistic
Regulatory Neuronal
#Unlab
eled networks 1900 1900 1900
#Labeled networks 100 100 100
Average #nodes 36 44 45
Average #edges 99 115 112
4.1.1 Dataset. To construct the dataset, we synthesize complex
networks with three nodal state dynamics from physics and life
sciences. Denote 𝑥𝑖(𝑡)as the state of node 𝑖at time step 𝑡, the
dynamics are as follows,
•Mutualistic dynamics. The mutualistic dynamics [ 19]𝑑𝑥𝑖
𝑑𝑡=
𝐵+𝑥𝑖 1−𝑥𝑖
𝐾  𝑥𝑖
𝐶−1+Í𝑁
𝑗=1𝐴𝑖𝑗𝑥𝑖𝑥𝑗
𝐷+𝐸𝑥𝑖+𝐻𝑥𝑗describes the al-
terations in species populations that are engendered by the mi-
gration term 𝐵, logistic growth term with environment capacity
𝐾[57], Allee effect [ 2] term with threshold 𝐶, and mutualistic
interaction between species with interaction network A.
•Regulatory dynamics. The regulatory dynamics, also called
Michaelis-Menten dynamics [ 3], is described by𝑑𝑥𝑖
𝑑𝑡=−𝐵𝑥𝑓
𝑖+
Í𝑁
𝑗=1𝐴𝑖𝑗𝑥ℎ
𝑗
𝑥ℎ
𝑗+1.𝑓represents the degradation ( 𝑓=1) or dimer-
ization (𝑓=2). Additionally, the second term in the equation
is designed to capture genetic activation with Hill coefficient ℎ,
which serves to quantify the extent of gene regulation collabora-
tion.
•Neuronal dynamics. The neuronal dynamics, also called Wilson-
Cowan dynamics [51, 52], is described by the equation of𝑑𝑥𝑖
𝑑𝑡=
−𝑥𝑖+Í𝑁
𝑗=1𝐴𝑖𝑗1
1+𝑒𝜇−𝛿𝑥𝑗. For each node in the network, it receives
cumulative inputs from its neighbors. The second term of the
equation represents the activation signal that is collectively con-
tributed by all neighboring nodes.
For each dynamics, we synthesize Erdős-Rényi networks [ 15] with
edge creation probability uniformly sampled in [0,0.15], and use
the fourth-order Runge-Kutta stepper [ 13] to simulate their nodal
state trajectories. For more details, please refer to the Appendix A.2.
We create 2000 network samples for training, 200for validation, and
another 200samples for testing. In the training stage, we randomly
select 100(5%) samples as labeled data and keep other 1900 samples
as unlabeled. The statistics of datasets are shown in Table 1.
4.1.2 Baselines and metrics. In the following parts, we define the
model trained only on original labeled data as the vanilla model.
Besides this, there are mainly three kinds of baseline methods de-
signed to leverage unlabeled data for enhancing the predictive
performance of the vanilla predictor.
•Self-training methods. They utilize the predictor to assign
pseudo labels to unlabeled data, thereby augmenting the labeled
training dataset. We abbreviate these method as ST.
•Self-supervised learning methods. They employ hand-crafted
tasks to derive insights from unlabeled data, thereby facilitating
the pre-training of model parameters. Subsequently, they un-
dergo further supervised training on the labeled dataset. This ap-
proach is predicated on the premise that integrating pre-training
phases with subsequent supervised learning phases leverages
both unlabeled and labeled datasets, thereby enhancing the model’s
learning efficacy and predictive accuracy. Such methods include
 
1880TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Overall predictive performance of models. w/o trajectories represents that nodal state trajectories of unlabeled data are
unknown. The performance of TDNetGen is marked in bold, and the best baseline is underlined.
Mutualistic Regulatory Neuronal
# Training samples 100 100 100
Model F1 ACC F1 ACC F1 ACC
Vanilla model 0.838 0.848 0.806 0.780 0.775 0.784
Self-training ST 0.807 0.827 0.780 0.735 0.728 0.764
Self-supervised learningEdgePred [21] 0.840 0.851 0.813 0.791 0.776 0.784
AttrMask [21] 0.831 0.845 0.817 0.793 0.770 0.779
ContextPred [21] 0.843 0.847 0.815 0.789 0.772 0.781
InfoMax [48] 0.829 0.815 0.875 0.870 0.787 0.805
GraphLog [54] 0.808 0.796 0.796 0.769 0.772 0.732
D-SLA [28] 0.810 0.799 0.855 0.840 0.780 0.805
Graph data augmentationTRY [17] 0.891 0.886 0.896 0.898 0.818 0.833
G-Mixup [18] 0.875 0.888 0.900 0.899 0.786 0.812
TDNetGen (w/o trajectories) 0.913 0.913 0.922 0.923 0.805 0.810
TDNetGen 0.929 0.934 0.944 0.946 0.845 0.873
Improvement 4.26% 5.18% 4.89% 5.23% 3.30% 4.80%
EdgePred, AttrMask, ContextPred [ 21], InfoMax [ 48], GraphLog [ 54],
and D-SLA [28].
•Graph data augmentation (GDA) methods. They incorporate
new graphs with labels to train the model, including theory-
guided method (TRY [ 17], detailed in Appendix A.3) and G-
Mixup [18].
For both self-training and graph data augmentation methods, the
quantity of newly generated samples is the same as that produced by
our method. Similarly, within the realm of self-supervised learning,
we also select the same count of unlabeled samples as the volume
of new samples generated by our framework. We use F1-score (F1)
and Accuracy (ACC) to evaluate the predictive performance of the
resilience predictor.
4.1.3 Implementation details. We implement our model in PyTorch
and complete all training and test tasks on a single NVIDIA RTX
4090 GPU. With our framework, we generate 1000 new networks as-
signing 500 resilient and 500 non-resilient networks. Subsequently,
we randomly select half of these networks to serve as the aug-
mented data. We set the guidance intensity 𝜆=2000. In our study,
model parameters are optimized using the Adam optimizer, coupled
with Xavier initialization, which ensures a robust starting point for
learning. For each experiment, we conduct a minimum of at least
5 times employing distinct random seeds and report the average
value.
4.2 Overall Performance (RQ1)
We report the performance of our framework with mean value and
standard deviation in Table 2. From the experimental results, we
have the following conclusions:
•Our framework effectively empowers predictive perfor-
mance via generative augmentation of both topology and
dynamics. The results demonstrate that with the help of our
proposed data augmentation framework, the predictive perfor-
mance of the resilience predictor can be effectively improved. For
example, on mutualistic dataset, the F1-score of the resilience
predictor previously trained on 100 labeled data increases from
0.838 to 0.929 (+10.86%), and its ACC increases from 0.848 to0.934 (+10.14%) after training on the augmented data. Moreover,
our framework also improves the best baseline among all self-
training, self-supervised learning, and GDA methods w.r.t. F1-
score by 4.26%, 4.89%, 3.30%, and w.r.t. ACC by 5.18%, 5.23%, 4.80%,
on mutualistic, regulatory and neuronal dataset, respectively. All
these results demonstrate the outstanding performance of our
proposed framework. We find that the best baseline methods on
three datasets belong to the category of graph data augmentation.
Compared with TRY and G-mixup, we achieve to jointly model
network topology and dynamics in a fine-grained manner.
•Robustness performance without nodal state trajectories
of unlabeled data. In certain contexts, the requirement to ob-
tain the nodal states, even only for an initial phase of evolution,
still proves to be difficult or costly. Consequently, we analyze
scenarios where the nodal state trajectories of unlabeled data are
inaccessible and we can only train the dynamics learning module
on those of limited labeled data Pin Equ. (11). Results in Table 2
demonstrate that our framework is capable of sustaining com-
mendable performance even under such constrained conditions
and surpassing the best baseline in most scenarios. It underscores
the versatility of our framework and its potential effectiveness
under more limited data availability of the real world scenarios.
•Self-training cannot universally guarantee a positive im-
pact on model performance. The results demonstrate that
self-training methods have a relatively small positive effect on
predictive performance among all three datasets compared to our
framework TDNetGen. For example, on regulatory datasets, the
F1-score of the resilience predictor increases 0.051, and its ACC
increases 0.079 compared to the vanilla model. This is because
the labels assigned to the augmented data in the self-training
process originate from the model itself with sub-optimal predic-
tive performance. This approach inherently carries the risk of
generating labels that are incongruent with the ground truth and
partially introduce contradictory information into the training
dataset. The presence of such inaccurately labeled data can con-
found the learning algorithm, leading to a deterioration in the
model’s capacity to make accurate predictions.
 
1881KDD ’24, August 25–29, 2024, Barcelona, Spain Chang Liu, Jingtao Ding, Yiwen Song and Yong Li
Mutualistic Regulatory Neuronal0.60.70.80.91.0F1-scorew/o CG
w/o Diff
w/o FT
TDNetGen
(a) F1-score
Mutualistic Regulatory Neuronal0.60.70.80.91.0ACCw/o CG
w/o Diff
w/o FT
TDNetGen (b) ACC
Figure 5: Ablation studies on datasets. CG: Classifier guid-
ance, Diff: Diffusion module, FT: Fine-tuning on dynamics
learning module-produced trajectories.
•Extracting knowledge from unlabeled data via hand-crafted
self-supervised tasks offers marginal benefits to the re-
silience prediction. We also find that models trained on self-
supervised tasks can only extract limited knowledge from un-
labeled data to benefit the resilience prediction task. From the
results, the improvement to vanilla model from competitive self-
supervised learning methods (ContextPred [ 21] and Infomax [ 48])
is still relatively marginal compared to our framework (+10 .86%,
+17.12%and +9.03%, on mutualistic, regulatory, neuronal dataset,
respectively). The primary reason for the observed discrepancy
lies in the substantial divergence between conventional hand-
crafted tasks, which only focus on the modeling of topological
structures. In the resilience prediction task, however, we also
need to consider nodal state dynamics of networks.
4.3 Ablation Study (RQ2)
To provide a comprehensive analysis and assess the effect of our
designed components quantitatively, we conduct several ablation
experiments via removing each design elements, and present the
evaluation results in Figure 5.
•Effectiveness of Classifier guidance. We first remove the de-
sign of classifier guidance and generate new network topologies
via only unconditional topology diffusion module. The nodal
state trajectories are simulated utilizing the dynamics learning
module, and its resilience label is determined by the resilience
predictor that has been trained on the labeled dataset. The results
reveal that the F1-score of the ablation model significantly de-
clines 6.14%, 9.75%, and 5.68%compared to the full model design,
which underscores the importance of guided generation.
•Effectiveness of resilience predictor fine-tuning on dynam-
ics learning module-produced trajectories. In this experi-
ment, we remove the fine-tuning procedure of the resilience
predictor on dynamics learning module-produced trajectories, in-
stead utilizing the one trained with ground-truth trajectories. The
results illustrate that fine-tuning could significantly enhance its
guidance capabilities to generate higher-quality data, ultimately
empowering the resilience predictor to be re-trained on it.
•Architecture analysis. We compare our diffusion-based topol-
ogy generation module with generative adversarial network
(GAN) module. Specifically, we replace topology generation mod-
ule as a GAN-based module proposed in [ 37]. We use the topolo-
gies of unlabeled data to train the GAN model, and sample new
topologies from it. The nodal state trajectories and the resilience
label are produced by our dynamics learning module and the
20 40 60 80 100
#Labeled samples0.70.80.91.0F1-score
Vanilla Enhanced(a) F1-score
20 40 60 80 100
#Labeled samples0.60.70.80.91.0ACC
Vanilla Enhanced (b) ACC
Figure 6: Model performance with less labeled samples on
mutualistic dataset.
resilience predictor, respectively. Experiments demonstrate that
our original design of diffusion models exhibit superior gener-
ative performance compared to GANs, which underscores the
efficacy of diffusion models in capturing the underlying topology
data distribution, thereby facilitating more accurate and reliable
topology generation.
4.4 Augmentation with Limited Labels and
Observations (RQ3)
In this section, we investigate data augmentation capabilities of
our proposed framework under conditions of more limited number
of labeled samples and reduced observed trajectory lengths, repre-
senting more challenging scenarios. We illustrate the results on the
mutualistic dataset in Figure 6-7.
•Less labeled samples. We investigate the performance of the
vanilla model, where the numbers of labeled networks are in
{20,40,60,80,100}, as well as the enhanced model trained on the
augmented data generated by TDNetGen. From the results, we
find that the predictive performance of the vanilla model is gener-
ally proportional to the number of labeled data used for training.
TDNetGen is robust to the limitation of labeled data, which can
still generate reasonable samples to benefit the predictive per-
formance of the vanilla model. These findings underscore the
versatility and potential of our proposed framework, particularly
in scenarios characterized by a scarcity of labeled data, which
constitutes a small portion of the available dataset.
•Shorter nodal state trajectories. We also investigate the per-
formance of the vanilla model and TDNetGen while using shorter
nodal state trajectories, which contain {3,4,5,6}time steps. We
discover that the performance of the vanilla model improves with
the increase in trajectory length since the model can extract more
knowledge about nodal state dynamics from data to make more
accurate resilience predictions. In this scenario, TDNetGen can
also help to augment the model’s performance, which suggests
that even in situations where nodal state trajectories are costly to
acquire, our framework remains applicable and effective for data
augmentation purposes of simultaneously generating plausible
topologies and nodal state trajectories of complex networks.
4.5 Robustness against Different Network Types
and Scales (RQ4)
We consider other network models, including Barabási–Albert
model [ 1],S1/H2model [ 43], and stochastic block model (SBM) [ 20],
which have more complex and heterogeneous structural proper-
ties. Moreover, we also evaluate the scalability of our framework
 
1882TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain
3 4 5 6
Trajectory length0.60.70.80.91.0F1-score
Vanilla Enhanced
(a) F1-score
3 4 5 6
Trajectory length0.50.60.70.80.91.0ACC
Vanilla Enhanced (b) ACC
Figure 7: Model performance with shorter nodal state trajec-
tories on the mutualistic dataset.
Table 3: Overall predictive performance of models on BA, S1,
SBM, and brain networks.
BA S1SBM
Brain
# Training samples 100 100 100 100
Mo
del F1 ACC F1 ACC F1 ACC F1 ACC
V
anilla model 0.814 0.798 0.776 0.828 0.767 0.841 0.792 0.827
Self-training
ST 0.767 0.754 0.774 0.787 0.778 0.802 0.805 0.796
Self-sup
ervised
learningEdgePred [21] 0.797 0.780 0.747 0.806 0.784 0.733 0.725 0.685
AttrMask [21] 0.788 0.776 0.750 0.805 0.755 0.760 0.733 0.741
ContextPred [21] 0.792 0.790 0.771 0.819 0.754 0.758 0.727 0.722
InfoMax [48] 0.776 0.765 0.784 0.820
0.812 0.833 0.743 0.775
GraphLog [54] 0.783 0.713 0.780 0.816 0.759 0.764 0.745 0.757
D-SLA [28] 0.817 0.823 0.790 0.813 0.825 0.820 0.772 0.765
Graph
data
augmentationTRY [17] 0.837 0.840
0.791 0.796 0.855 0.858
0.826 0.831
G-Mixup [18] 0.834 0.837 0.807
0.811 0.852 0.851 0.839 0.844
TDNetGen
(w/o trajectories) 0.842 0.846 0.823 0.830 0.886 0.890 0.873 0.870
TDNetGen 0.870 0.850 0.856 0.875 0.935 0.937 0.914 0.907
Impr
ovement 3.99% 1.19% 6.07% 6.71% 9.36% 9.21% 8.93% 7.46%
on large-scale empirical brain networks with 998 nodes in max-
imum. For each dataset, we obtain nodal states of networks via
neuronal dynamics, and other experimental settings are the same
as Section 4.1. The details of dataset construction are shown in Ap-
pendix A.2. We demonstrate the results in Table 3, which indicates
that our framework can still achieve the best augmentation perfor-
mance on more broad types and scales of networks with complex
structural properties and different network sizes.
5 RELATED WORKS
5.1 Resilience Prediction of Complex Networks
Existing works on resilience prediction are mainly categorized to
analytical estimations from physical theories [ 17,30,39]. Gao et
al. [17] propose to reduce the dimension of complex networks to
single-parameter systems based on mean-field theory, thus we can
easily analyze the equilibrium of 1-D ODE problem and predict the
resilience of complex networks. Laurence et al. [ 30] perform dimen-
sion reduction based on spectral graph theory on the dominant
eigenvalues and eigenvectors of adjacency matrices. Morone et
al. [39] develop a resilience prediction methodology by quantifying
the k-core structure within networks. Despite their effectiveness,
they often pre-suppose a detailed understanding of nodal state dy-
namics, which is usually not available in practical scenarios. In our
work, we design data-driven methods that extract topology and
nodal state dynamics information from observational data, allowing
for resilience predictions without the need for prior knowledge.
5.2 Diffusion Models on Graphs
Diffusion probabilistic models have been widely used in text, im-
age, audio generation, etc. [ 5,34,55,56]. Recently, some existingworks have applied the diffusion model to the field of graph gener-
ation [ 9,22,46,49]. Huang et al. [ 22] define a stochastic differential
equation (SDE) that smoothly converts graphs with complex dis-
tribution to random graphs, and samples new graphs by solving
the reverse-time SDE. Tseng et al. [ 46] propose GraphGUIDE to
achieve interpretable and controllable graph generation, wherein
edges in graph are flipped or set at each discrete time step. Chen et
al. [9] propose to leverage graph sparsity during each step of diffu-
sion process, which only focuses on a small portion of nodes and
considers edge changes between them. In contrast to existing contri-
butions focused primarily on graph structures, our research extends
to the generation of complex networks, which encompasses not
merely the graph topology but also integrates nodal state trajecto-
ries, thereby facilitating the generation of comprehensive network
data.
5.3 Learning from Unlabeled Data
Typical approaches of learning from unlabeled data for graph clas-
sification include pre-training on self-supervised tasks [ 28,54], self-
training [ 4,23,25,45], and graph data augmentation [ 18]. Although
pre-training proves to be effective for vision and language-related
tasks, it can hardly help the resilience prediction task because of
the disparity between hand-crafted and downstream prediction
tasks [ 24,28]. Therefore, we still lack a universal self-supervised
task that learns from unlabeled graphs and improves the perfor-
mance of downstream scenarios. Self-training tasks assign pseudo-
labels to unlabeled data by leveraging the model itself, followed
by the retraining of the model with pseudo-labeled data. Existing
works [ 4,23,45] focus on uncertainty estimation of assigned labels
to minimize the impact of noisy pseudo-labels. Furthermore, Liu et
al. [33] learn data distributions from unlabeled graphs with diffu-
sion models, and to generate task-specific labeled graphs for data
augmentation. Compared with their work, our proposed TDNetGen
framework considers more intricate scenarios of complex networks
with interplay between topology and nodal state dynamics. Our
framework can extract knowledge from full unlabeled complex
network samples, thereby generating high-quality augmented data
that benefits the training of prediction models.
6 CONCLUSIONS
In this work, we propose an effective framework, TDNetGen, for
complex network resilience prediction. It not only addresses the
problem in a data-driven manner without prior knowledge about
groud-truth dynamics, but also solves labeled data sparsity prob-
lem with the generative augmentation of jointly modeling network
topology and dynamics. Extensive experiments demonstrate the su-
periority of TDNetGen and also highlight its robustness within less
labeled data and dynamics information conditions. The method-
ology introduced in this paper provides a novel perspective for
improving resilience prediction through data augmentation, that is,
leveraging the untapped potential of unlabeled data to enhance the
learning process.
ACKNOWLEDGMENT
This work is supported in part by National Natural Science Foun-
dation of China under U23B2030, 62272260, U21B2036.
 
1883KDD ’24, August 25–29, 2024, Barcelona, Spain Chang Liu, Jingtao Ding, Yiwen Song and Yong Li
REFERENCES
[1] Réka Albert and Albert-László Barabási. 2002. Statistical mechanics of complex
networks. Reviews of modern physics 74, 1 (2002), 47.
[2]Warder Clyde Allee, Orlando Park, Alfred E Emerson, Thomas Park, Karl P
Schmidt, et al .1949. Principles of animal ecology. Number Edn 1. WB Saundere
Co. Ltd.
[3]Uri Alon. 2019. An introduction to systems biology: design principles of biological
circuits. CRC press.
[4]Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. 2020.
Deep evidential regression. Advances in Neural Information Processing Systems
33 (2020), 14927–14937.
[5]Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van
Den Berg. 2021. Structured denoising diffusion models in discrete state-spaces.
Advances in Neural Information Processing Systems 34 (2021), 17981–17993.
[6]Sai Balaji, M Madan Babu, Lakshminarayan M Iyer, Nicholas M Luscombe, and
Lakshminarayan Aravind. 2006. Comprehensive analysis of combinatorial regu-
lation using the transcriptional regulatory network of yeast. Journal of molecular
biology 360, 1 (2006), 213–227.
[7]Ed Bullmore and Olaf Sporns. 2009. Complex brain networks: graph theoretical
analysis of structural and functional systems. Nature reviews neuroscience 10, 3
(2009), 186–198.
[8]Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. 2018.
Neural ordinary differential equations. Advances in neural information processing
systems 31 (2018).
[9]Xiaohui Chen, Jiaxing He, Xu Han, and Li-Ping Liu. 2023. Efficient and Degree-
Guided Graph Generation via Discrete Diffusion Modeling. arXiv preprint
arXiv:2305.04111 (2023).
[10] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. 2020. Can graph
neural networks count substructures? Advances in neural information processing
systems 33 (2020), 10383–10395.
[11] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on
image synthesis. Advances in neural information processing systems 34 (2021),
8780–8794.
[12] Jingtao Ding, Chang Liu, Yu Zheng, Yunke Zhang, Zihan Yu, Ruikun Li, Hongyi
Chen, Jinghua Piao, Huandong Wang, Jiazhen Liu, et al .2024. Artificial Intel-
ligence for Complex Network: Potential, Methodology and Application. arXiv
preprint arXiv:2402.16887 (2024).
[13] John R Dormand and Peter J Prince. 1980. A family of embedded Runge-Kutta
formulae. Journal of computational and applied mathematics 6, 1 (1980), 19–26.
[14] Vijay Prakash Dwivedi and Xavier Bresson. 2020. A generalization of transformer
networks to graphs. arXiv preprint arXiv:2012.09699 (2020).
[15] Paul Erdős, Alfréd Rényi, et al .1960. On the evolution of random graphs. Publ.
math. inst. hung. acad. sci 5, 1 (1960), 17–60.
[16] Socorro Gama-Castro, Verónica Jiménez-Jacinto, Martin Peralta-Gil, Alberto
Santos-Zavaleta, Mónica I Peñaloza-Spinola, Bruno Contreras-Moreira, Juan
Segura-Salazar, Luis Muniz-Rascado, Irma Martinez-Flores, Heladia Salgado, et al .
2008. RegulonDB (version 6.0): gene regulation model of Escherichia coli K-12
beyond transcription, active (experimental) annotated promoters and Textpresso
navigation. Nucleic acids research 36, suppl_1 (2008), D120–D124.
[17] Jianxi Gao, Baruch Barzel, and Albert-László Barabási. 2016. Universal resilience
patterns in complex networks. Nature 530, 7590 (2016), 307–312.
[18] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. 2022. G-mixup: Graph
data augmentation for graph classification. In International Conference on Machine
Learning. PMLR, 8230–8248.
[19] J Nathaniel Holland, Donald L DeAngelis, and Judith L Bronstein. 2002. Popula-
tion dynamics and mutualism: functional responses of benefits and costs. The
American Naturalist 159, 3 (2002), 231–244.
[20] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. 1983. Sto-
chastic blockmodels: First steps. Social networks 5, 2 (1983), 109–137.
[21] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,
and Jure Leskovec. 2019. Strategies for pre-training graph neural networks. arXiv
preprint arXiv:1905.12265 (2019).
[22] Han Huang, Leilei Sun, Bowen Du, Yanjie Fu, and Weifeng Lv. 2022. Graphgdp:
Generative diffusion processes for permutation invariant graph generation. In
2022 IEEE International Conference on Data Mining (ICDM). IEEE, 201–210.
[23] Kexin Huang, Vishnu Sresht, Brajesh Rai, and Mykola Bordyuh. 2022. Uncertainty-
aware pseudo-labeling for quantum calculations. In Uncertainty in Artificial
Intelligence. PMLR, 853–862.
[24] Eric Inae, Gang Liu, and Meng Jiang. 2023. Motif-aware Attribute Masking for
Molecular Graph Pre-training. arXiv preprint arXiv:2309.04589 (2023).
[25] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. 2019. Label
propagation for deep semi-supervised learning. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. 5070–5079.
[26] Chunheng Jiang, Jianxi Gao, and Malik Magdon-Ismail. 2020. Inferring de-
grees from incomplete networks and nonlinear dynamics. arXiv preprint
arXiv:2004.10546 (2020).
[27] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. 2022. Score-based generative model-
ing of graphs via the system of stochastic differential equations. In InternationalConference on Machine Learning. PMLR, 10362–10383.
[28] Dongki Kim, Jinheon Baek, and Sung Ju Hwang. 2022. Graph self-supervised
learning with accurate discrepancy learning. Advances in Neural Information
Processing Systems 35 (2022), 14085–14098.
[29] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[30] Edward Laurence, Nicolas Doyon, Louis J Dubé, and Patrick Desrosiers. 2019.
Spectral dimension reduction of complex dynamical networks. Physical Review
X9, 1 (2019), 011042.
[31] Dong-Hyun Lee et al .2013. Pseudo-label: The simple and efficient semi-
supervised learning method for deep neural networks. In Workshop on challenges
in representation learning, ICML, Vol. 3. Atlanta, 896.
[32] Ruikun Li, Huandong Wang, Jinghua Piao, Qingmin Liao, and Yong Li. 2024.
Predicting Long-term Dynamics of Complex Networks via Identifying Skeleton
in Hyperbolic Space. to appear in Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (2024).
[33] Gang Liu, Eric Inae, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. 2024.
Data-centric learning from unlabeled graphs with diffusion model. Advances in
neural information processing systems 36 (2024).
[34] Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu
Ma. 2022. Antigen-specific antibody design and optimization with diffusion-
based generative models for protein structures. Advances in Neural Information
Processing Systems 35 (2022), 9754–9767.
[35] Jinzhu Mao, Liu Cao, Chen Gao, Huandong Wang, Hangyu Fan, Depeng Jin, and
Yong Li. 2023. Detecting vulnerable nodes in urban infrastructure interdepen-
dent network. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 4617–4627.
[36] Jinzhu Mao, Dongyun Zou, Li Sheng, Siyi Liu, Chen Gao, Yue Wang, and Yong Li.
2024. Identify Critical Nodes in Complex Network with Large Language Models.
arXiv preprint arXiv:2403.03962 (2024).
[37] Karolis Martinkus, Andreas Loukas, Nathanaël Perraudin, and Roger Wattenhofer.
2022. Spectre: Spectral conditioning helps to overcome the expressivity limits
of one-shot graph generators. In International Conference on Machine Learning.
PMLR, 15159–15179.
[38] Robert M May. 1977. Thresholds and breakpoints in ecosystems with a multiplicity
of stable states. Nature 269, 5628 (1977), 471–477.
[39] Flaviano Morone, Gino Del Ferraro, and Hernán A Makse. 2019. The k-core as a
predictor of structural collapse in mutualistic ecosystems. Nature physics 15, 1
(2019), 95–102.
[40] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and
Stefano Ermon. 2020. Permutation invariant graph generation via score-based
generative modeling. In International Conference on Artificial Intelligence and
Statistics. PMLR, 4474–4484.
[41] Jeff Ollerton, Duncan McCollin, Daphne G Fautin, and Gerald R Allen. 2007. Find-
ing NEMO: nestedness engendered by mutualistic organization in anemonefish
and their hosts. Proceedings of the Royal Society B: Biological Sciences 274, 1609
(2007), 591–598.
[42] Hillel Sanhedrai, Jianxi Gao, Amir Bashan, Moshe Schwartz, Shlomo Havlin, and
Baruch Barzel. 2022. Reviving a failed network through microscopic interventions.
Nature Physics 18, 3 (2022), 338–349.
[43] M Ángeles Serrano, Dmitri Krioukov, and Marián Boguná. 2008. Self-similarity
of complex networks and hidden metric spaces. Physical review letters 100, 7
(2008), 078701.
[44] Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, and Yong Li. 2024. Rumor
Mitigation in Social Media Platforms with Deep Reinforcement Learning. In
Companion Proceedings of the ACM on Web Conference 2024. 814–817.
[45] Natasa Tagasovska and David Lopez-Paz. 2019. Single-model uncertainties for
deep learning. Advances in Neural Information Processing Systems 32 (2019).
[46] Alex M Tseng, Nathaniel Diamant, Tommaso Biancalani, and Gabriele Scalia.
2023. GraphGUIDE: interpretable and controllable conditional graph generation
with discrete Bernoulli diffusion. arXiv preprint arXiv:2302.03790 (2023).
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[48] Petar Veličković, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio,
and R Devon Hjelm. 2019. Deep graph infomax. ICLR (2019).
[49] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher,
and Pascal Frossard. 2023. Digress: Discrete denoising diffusion for graph gener-
ation. ICLR (2023).
[50] Huandong Wang, Huan Yan, Can Rong, Yuan Yuan, Fenyu Jiang, Zhenyu Han,
Hongjie Sui, Depeng Jin, and Yong Li. 2023. Multi-Scale Simulation of Complex
Systems: A Perspective of Integrating Knowledge and Data. Comput. Surveys
(2023).
[51] Hugh R Wilson and Jack D Cowan. 1972. Excitatory and inhibitory interactions
in localized populations of model neurons. Biophysical journal 12, 1 (1972), 1–24.
[52] Hugh R Wilson and Jack D Cowan. 1973. A mathematical theory of the functional
dynamics of cortical and thalamic nervous tissue. Kybernetik 13, 2 (1973), 55–80.
 
1884TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics KDD ’24, August 25–29, 2024, Barcelona, Spain
[53] Mengkai Xu, Srinivasan Radhakrishnan, Sagar Kamarthi, and Xiaoning Jin. 2019.
Resiliency of mutualistic supplier-manufacturer networks. Scientific reports 9, 1
(2019), 13559.
[54] Minghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. 2021. Self-
supervised graph-level representation learning with local and global structure.
InInternational Conference on Machine Learning. PMLR, 11548–11558.
[55] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian
Zou, and Dong Yu. 2023. Diffsound: Discrete diffusion model for text-to-sound
generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing
(2023).
[56] Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, and Yong Li. 2024. Spatio-
Temporal Few-Shot Learning via Diffusive Neural Network Generation. In The
Twelfth International Conference on Learning Representations. https://openreview.
net/forum?id=QyFm3D3Tzi
[57] Chengxi Zang, Peng Cui, Christos Faloutsos, and Wenwu Zhu. 2018. On power
law growth of social networks. IEEE Transactions on Knowledge and Data Engi-
neering 30, 9 (2018), 1727–1740.
[58] Chengxi Zang and Fei Wang. 2020. Neural dynamics on complex networks.
InProceedings of the 26th ACM SIGKDD international conference on knowledge
discovery & data mining. 892–902.
[59] Huixin Zhang, Qi Wang, Weidong Zhang, Shlomo Havlin, and Jianxi Gao. 2022.
Estimating comparable distances to tipping points across mutualistic systems by
scaled recovery rates. Nature Ecology & Evolution 6, 10 (2022), 1524–1536.
[60] Yongtao Zhang, Cunqi Shao, Shibo He, and Jianxi Gao. 2020. Resilience centrality
in complex networks. Physical Review E 101, 2 (2020), 022304.
A APPENDIX
A.1 Details of Parameterization Network
A.1.1 Graph Transformer. The parameterization of the denoising
networkℎ𝜃employs the Graph Transformer architecture [ 11]. The
input consists of the noisy graph features, and the output is the edge
distribution of clean graphs. Each layer of the Graph Transformer
can be represented as follows:
ℎ𝑙+1
𝑖=𝑂𝑙
ℎ∥𝐾
𝑘=1©­
«∑︁
𝑟𝑗∈N𝑟𝑖𝛼𝑘,𝑙
𝑖𝑗𝑉𝑘,𝑙ℎ𝑙
𝑗ª®
¬, (25)
𝑒𝑙+1
𝑖𝑗=𝑂𝑙
𝑒∥𝐾
𝑘=1
𝑎𝑘,𝑙
𝑖𝑗
, (26)
𝛼𝑘,𝑙
𝑖𝑗=softmax𝑗
𝑎𝑘,𝑙
𝑖𝑗
, (27)
𝑎𝑘,𝑙
𝑖𝑗= 𝑄𝑘,𝑙ℎ𝑙
𝑖·𝑃𝑘,𝑙ℎ𝑙
𝑗√︁
𝑑𝑘!
+𝑊𝑘,𝑙𝑒𝑙
𝑖𝑗, (28)
whereℎ𝑙
𝑖denotes the embedding of node 𝑖of the𝑙-th layer,𝑒𝑙
𝑖𝑗rep-
resents the embedding of edge connecting node 𝑖and𝑗.𝑂,𝑄,𝑃,𝑉,
and𝑊with different superscripts are trainable parameters. 𝑘=
{1,2,···,𝐾}denotes the attention heads, and ∥represents the con-
catenation operator. After the last layer of Graph Transformer, edge
embeddings are fed into an MLP to predict the existence of edges
{𝑒𝑖𝑗}1≤𝑖,𝑗≤𝑁, where𝑁is the number of nodes in the network.
A.1.2 Node features. We include node features in both structural
and spectral domains to enhance the performance of the Graph
Transformer, and choose the same features as in [ 49]. Specifically,
structural features include the cycles, indicating i.e.,how many
𝑘-cycles the node belongs to, since message-passing cannot detect
cycle structures [ 10]. For spectral features, we first compute the
graph Laplacian then consider the number of connected compo-
nents and the two first eigenvectors of the non-zero eigenvalues.
A.2 Details of Data Collection
A.2.1 Mutualistic dynamics. Nodal state trajectories of networks
in mutualistic dataset are simulated via the following differentialequations:
𝑑𝑥𝑖
𝑑𝑡=𝐵+𝑥𝑖
1−𝑥𝑖
𝐾 𝑥𝑖
𝐶−1
+𝑁∑︁
𝑗=1𝐴𝑖𝑗𝑥𝑖𝑥𝑗
𝐷+𝐸𝑥𝑖+𝐻𝑥𝑗.(29)
We use the fourth-order Runge-Kutta stepper, with a high initializa-
tionx=5and a low initialization x=0, simulating two trajectories,
which represent a thriving stable ecosystem and an ecosystem after
a catastrophe, respectively. We set the terminal simulation time
as𝑇𝑚𝑎𝑥=50, and the interval Δ𝑡=0.5. Nodal states of networks
with mutualistic dynamics can encounter a bifurcation [ 19], tran-
sitioning from a resilient phase characterized by a single, desired
high equilibrium x𝐻to a non-resilient phase with both the desired
equilibrium x𝐻and the low equilibrium x𝐿. We denote the aver-
aged nodal states of the network from high and low initializations
as⟨x(ℎ)⟩and⟨x(𝑙)⟩; therefore, to define the resilience labels of
networks, we compare ⟨x(ℎ)⟩and⟨x(𝑙)⟩at the terminal time. If
|⟨x(ℎ)⟩−⟨x(𝑙)⟩|>𝑟, we conclude that the network cannot recover
after perturbations and has two equilibrium x𝐻andx𝐿, thus it is
non-resilient. 𝑟is a pre-defined threshold, and we set 𝑟=3.5in our
experiments.
A.2.2 Regulatory dynamics. Nodal state trajectories of networks
in regulatory dataset are simulated via the following differential
equations:
𝑑𝑥𝑖
𝑑𝑡=−𝐵𝑥𝑓
𝑖+𝑁∑︁
𝑖=1𝐴𝑖𝑗𝑥ℎ
𝑗
𝑥ℎ
𝑗+1. (30)
Similar to mutualistic dynamics, we use the fourth-order Runge-
Kutta stepper, set the terminal simulation time 𝑇𝑚𝑎𝑥=50and the
interval Δ𝑡=0.5. Regulatory dynamics in Equ. (30) has a trivial
fixed point (as well as equilibrium) x=⟨x⟩=0, and for resilient net-
works with this dynamics, its nodal states have another equilibrium
⟨x⟩>0[3]. To avoid the fixed-point equilibrium, we randomly
initialize the model with x=[1,5]and use the terminal nodal state
⟨x⟩to determine its resilience. Specifically, a network is deemed
resilient if⟨x⟩>0. Conversely, the non-resilient network can only
converge to the equilibrium of ⟨x⟩=0.
A.2.3 Neuronal dynamics. Nodal state trajectories of networks
in neuronal dataset are simulated with the following differential
equations:
𝑑𝑥𝑖
𝑑𝑡=−𝑥𝑖+𝑁∑︁
𝑗=1𝐴𝑖𝑗1
1+𝑒𝜇−𝛿𝑥𝑗. (31)
The ODE solver, terminal, and interval time settings are the same
as mutualistic and regulatory dynamics. Non-resilient networks
exhibit either a bi-stable phase, wherein both a high equilibrium,
denoted as x𝐻, and a low equilibrium, denoted as x𝐿, can exist, or
only a single low equilibrium x𝐿exists. On the other hand, resilient
neuronal networks are distinguished by their maintenance of a
high equilibrium x𝐻[51]. We initialize the nodal state with a high
initialization x=5and a low initialization x=0, simulating two
trajectories. We compare ⟨x(ℎ)⟩and⟨x(𝑙)⟩at the terminal time to
define the resilience labels of networks. If |⟨x(ℎ)⟩−⟨ x(𝑙)⟩|>𝑟or
⟨x(ℎ)⟩<𝑚and⟨x(𝑙)⟩<𝑚, we conclude that the network cannot
recover after perturbations and have two equilibrium x𝐻andx𝐿,
thus it is non-resilient. Otherwise, the network is resilient. 𝑟and
 
1885KDD ’24, August 25–29, 2024, Barcelona, Spain Chang Liu, Jingtao Ding, Yiwen Song and Yong Li
0 2 4 6 8
#Generated/#Labeled0.70.80.91.0F1-score
(a) F1-score
0 2 4 6 8
#Generated/#Labeled0.70.80.91.0ACC
 (b) ACC
Figure 8: Model performance with different number of gen-
erated samples on mutualistic dataset.
𝑚are pre-defined thresholds, and we set 𝑟=3.5and𝑚=3in our
experiments.
A.2.4 Barabási-Albert (BA) model. BA network starts with a small
number of nodes, and at each time step, a new node with 𝑚edges
is added to the network. These 𝑚edges link the new node to 𝑚dif-
ferent nodes that have already present in network. The probability
Πthat a new node will connect to an existing node 𝑖is proportional
to the degree 𝑘𝑖of node𝑖. Mathematically, Π(𝑘𝑖)=𝑘𝑖Í
𝑗𝑘𝑗, where
the summation is over all existing nodes 𝑗in the network. This
means that nodes with higher degrees have a higher likelihood
of receiving new links, leading to a “rich-get-richer” effect. The
resulting network from the BA model exhibits a power-law degree
distribution, 𝑃(𝑘)∼𝑘−𝛾, where𝛾is typically in the range of 2 to
3. In our BA network datasets, each network consists of 100 ∼200
nodes, and we set 𝑚=4.
A.2.5 S1/H2model. It is also called hyperbolic geometric graph
model. In this model, nodes are placed in a hyperbolic disk. Each
node is assigned to a radial coordinate 𝑟and an angular coordinate
𝜃.𝑟follows an exponential distribution capturing the heterogeneity
of node degrees, while 𝜃is uniformly distributed between 0and
2𝜋representing the similarity or latent feature spaces of nodes.
The connection probability of node 𝑖and𝑗depends on their hyper-
bolic distance 𝑑𝑖𝑗=𝑟𝑖+𝑟𝑗+2log(sin(𝜃𝑖𝑗
2)). Nodes are more likely
connected if they are close in the hyperbolic space, reflecting the
principle of preferential attachment and similarity. In our datasets,
each network consists of 100 ∼200 nodes, and the inverse temper-
ature controlling the clustering coefficient 𝛽, the exponent of the
power-law distribution for hidden degrees 𝛾, and the mean degree
of the network are set to 1.5,2.7, and 5, respectively.
A.2.6 Stochastic block model (SBM). In this model, nodes are par-
titioned into 𝐾distinct groups. The probability of an edge existing
between any two nodes depends solely on the groups to which these
nodes belong. In our SBM datasets, each network consists of [2,5]
communities and[20,40]nodes (both sampled uniformly). The
inter-community edge probability is 0.3, and the intra-community
edge probability is 0.05.
A.2.7 Empirical brain networks. we employ an empirical brain
networks [ 7,42] with 998 brain regions (nodes), which represents
the physical fiber bundle connections between them. The empirical
network also has a natural modular structure owing to the brain’s
two hemispheresthis, indicating its complex structural properties.We generate 1900 topologies for unlabeled data, 100 topologies for
labeled data, and 200 test data by randomly remove 0% ∼15% nodes
from the empirical topology.
A.3 Details of Theoretical Baseline
In this section, we detail how we incorporate resilience theory from
physics to provide insights on leveraging unlabeled data.
A.3.1 Gao-Barzel-Barabási (GBB) theory. From Gao-Barzel-Barabási
(GBB) theory [ 17], for a network with 𝑁nodes,𝑁-dimensional
nodal state dynamics represented by Equ. (1) can be condensed to
a 1-dimentional equation as:
d𝑥eff
d𝑡=𝐹(𝑥eff)+𝛽eff𝐺(𝑥eff,𝑥eff), (32)
𝑥eff=1𝑇Ax
1𝑇A1=⟨s𝑜𝑢𝑡x⟩
⟨s⟩, (33)
𝛽eff=1𝑇As𝑖𝑛
1𝑇A1=⟨s𝑜𝑢𝑡s𝑖𝑛⟩
⟨s⟩, (34)
where s𝑜𝑢𝑡=(𝑠𝑜𝑢𝑡
1,···,𝑠𝑜𝑢𝑡
𝑁)denotes the out-degrees of nodes and
s𝑖𝑛=(𝑠𝑖𝑛
1,···,𝑠𝑖𝑛
𝑁)denotes their in-degrees. ⟨s𝑜𝑢𝑡x⟩=1
𝑁Í𝑁
𝑖=1𝑠𝑜𝑢𝑡
𝑖𝑥𝑖
and⟨s⟩=⟨s𝑖𝑛⟩=⟨s𝑜𝑢𝑡⟩. Therefore, we can observe that network
topology A∈R𝑁×𝑁is condensed to a scalar 𝛽eff∈R, which
embeds the features of network topologies. From dynamics equa-
tion,𝑓(𝛽eff,𝑥eff)=𝐹(𝑥eff)+𝛽eff𝐺(𝑥eff,𝑥eff), we can identify the
bifurcation point of this dynamics, 𝛽𝑐
eff. If𝛽eff<𝛽𝑐
eff, the undesired
equilibrium will emerge, or the desired equilibrium will vanish.
Otherwise, it has only one desirable equilibrium. Since 𝛽effis cal-
culated from network topology, we can conclude that a network
with𝛽eff<𝛽𝑐
effis non-resilient, and that of 𝛽eff>𝛽𝑐
effis resilient.
Therefore, GBB theory provides an effective tool to predict net-
work resilience. Its limitation is that precise analytical forms of 𝐹(·)
and𝐺(·)are required, which is hard to determine in real-world
scenarios.
A.3.2 Theory-guided data augmentation. Here we discuss how to
use GBB theory to perform data augmentation. For each network
topology in labeled and unlabeled dataset, we calculate its 𝛽eff
from Equ. (34). After that, we denote the minimum𝛽effof resilient
networks in the labeled dataset as 𝛽+, and denote the maximum𝛽eff
of non-resilient networks in the labeled dataset as 𝛽−. Therefore,
for each network in the unlabeled dataset, if its 𝛽eff>𝛽+, we label
it as the resilient network; if its 𝛽eff<𝛽−, we label it as the non-
resilient network. Then the resilience predictor can further train on
these newly-labeled data to enhance its predictive performance.
A.4 Additional Experiments
Number of generated samples. We investigate the effect of used
number of generated samples on the augmentation performance.
The results on mutualistic dataset are shown in Figure 8, where
the point in the 0 position of x-axis indicates the performance of
the vanilla model. We find that there is an upper bound on the
improvement introduced by data augmentation. Since we use the
sub-optimal resilience predictor to guide the generation process, it
is unavoidable to introduce generated data with fault labels. When
the number of introduced generated data exceeds a threshold, the
defect of noisy labels will exceed the positive effect of new training
data, leading to the decrease of model performance.
 
1886