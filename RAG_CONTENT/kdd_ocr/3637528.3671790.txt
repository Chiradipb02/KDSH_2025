Temporal Prototype-Aware Learning for Active Voltage Control
on Power Distribution Networks
Feiyang Xu∗
Polytechnic Institute, Zhejiang
University
State Key Laboratory of Blockchain
and Security, Zhejiang University
Hangzhou High-Tech Zone (Binjiang)
Institute of Blockchain and Data
Security
Hangzhou, ChinaShunyu Liu∗†
State Key Laboratory of Blockchain
and Security, Zhejiang University
Hangzhou High-Tech Zone (Binjiang)
Institute of Blockchain and Data
Security
Hangzhou, ChinaYunpeng Qing
Yihe Zhou
Yuwen Wang
Mingli Song
State Key Laboratory of Blockchain
and Security, Zhejiang University
Hangzhou High-Tech Zone (Binjiang)
Institute of Blockchain and Data
Security
Hangzhou, China
ABSTRACT
Active Voltage Control (AVC) on the Power Distribution Netwo-
rks (PDNs) aims to stabilize the voltage levels to ensure efficient and
reliable operation of power systems. With the increasing integration
of distributed energy resources, recent efforts have explored em-
ploying multi-agent reinforcement learning (MARL) techniques to
realize effective AVC. Existing methods mainly focus on the acquisi-
tion of short-term AVC strategies, i.e., only learning AVC within the
short-term training trajectories of a singular diurnal cycle. However,
due to the dynamic nature of load demands and renewable energy,
the operation states of real-world PDNs may exhibit significant dis-
tribution shifts across varying timescales (e.g., daily and seasonal
changes). This can render those short-term strategies suboptimal
or even obsolete when performing continuous AVC over extended
periods. In this paper, we propose a novel temporal prototype-
aware learning method, abbreviated as TPA, to learn time-adaptive
AVC under short-term training trajectories. At the heart of TPA
are two complementary components, namely multi-scale dynamic
encoder and temporal prototype-aware policy, that can be readily
incorporated into various MARL methods. The former component
integrates a stacked transformer network to learn underlying tem-
poral dependencies at different timescales of the PDNs, while the
latter implements a learnable prototype matching mechanism to
construct a dedicated AVC policy that can dynamically adapt to
the evolving operation states. Experimental results on the AVC
benchmark with different PDN sizes demonstrate that the proposed
TPA surpasses the state-of-the-art counterparts not only in terms
of control performance but also by offering model transferability.
Our code is available at https://github.com/Canyizl/TPA-for-AVC.
∗Authors contributed equally to this research. E-mail: xufeiyang@zju.edu.cn
†Corresponding author. E-mail: liushunyu@zju.edu.cn
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671790CCS CONCEPTS
•Computing methodologies →Sequential decision making .
KEYWORDS
Prototypes Learning, Active Voltage Control, Multi-agent Rein-
forcement Learning
ACM Reference Format:
Feiyang Xu, Shunyu Liu, Yunpeng Qing, Yihe Zhou, Yuwen Wang, and Min-
gli Song. 2024. Temporal Prototype-Aware Learning for Active Voltage
Control on Power Distribution Networks. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671790
1 INTRODUCTION
Low-carbon energy technologies have made an attractive devel-
opment path for addressing sustainability concerns in power sys-
tems [ 11]. Among these technologies, rooftop solar photovoltaics
(PVs) in Power Distribution Networks (PDNs) are particularly promi-
nent, commonly installed atop residential and commercial struc-
tures to harness solar energy [ 4]. However, the widespread adoption
of PVs has introduced greater variability and intermittency in power
generation, thereby exacerbating the voltage fluctuations and pos-
ing challenges to the reliability and stability of power systems [ 9].
Thus, modern PDNs necessitate advanced Active Voltage Control
(AVC) schemes that can dynamically adapt to rapid changes in PV
generation and load conditions, ensuring voltage profiles within
permissible limits using the controllable apparatuses (e.g., PV in-
verters and static var compensators) [3, 18, 30].
To stabilize the voltage levels, the conventional AVC strategy on
PDNs involves solving constrained optimization problems. Such
optimization methods can be mainly categorized into two classes:
Optimal Power Flow (OPF)-based methods [ 2,32] and droop con-
trol (DC)-based ones [ 16,25]. OPF-based methods propose to com-
pute an optimal set of operating points for the entire power system
that adheres to power balance constraints. In spite of its effective-
ness, a key limitation of OPF is its dependency on an exact model of
the power system, which can be difficult to obtain. Additionally, the
complexity of OPF often results in time-consuming optimization
processes, making it challenging to apply in real-time scenarios [ 28].
3598
KDD ’24, August 25–29, 2024, Barcelona, Spain Feiyang Xu et al.
On the other hand, the DC-based methods are typically based on
pre-defined parameters that are manually set. While this method
can be implemented more rapidly than OPF, it is generally consid-
ered to be sub-optimal due to its reliance on these fixed parameters,
which also limits its adaptability across different systems [26].
To alleviate these issues, deep Multi-Agent Reinforcement Learn-
ing (MARL), a data-driven learning paradigm, has emerged as a
promising AVC solution for PDNs due to its rapid response time and
effective coordination capabilities. Existing MARL approaches typi-
cally begin by partitioning the PDN into multiple regions according
to its tree-like structure. This allows individual agents to be assigned
the task of AVC within each region [ 6,29]. While these region-based
approaches have demonstrated effectiveness in smaller-scale PDNs,
they may suffer from limitations when applied to larger networks
due to the inconsistent regional structures [ 33,34]. Thus, the pi-
oneering work of Wang et al . [33] suggests a shift in focus from
regional control to the management of individual distributed energy
resources, which has shown promise in more complex tasks. Fur-
thermore, to capture the varying significance of nodes within large-
scale PDNs, a more recent work [ 34] incorporates the transformer
network [31] to enhance the representation ability of agents.
Despite the promising results achieved, existing works mainly
emphasize the development of short-term AVC strategies, i.e., only
learning AVC within the short-term training trajectories of a singu-
lar diurnal cycle. They often slice the original continuous long-term
annual data into short-term daily segments for training. However,
PDNs are dynamically evolving systems across varying timescales,
exhibiting significant temporal distribution shifts in real-world op-
eration states. It can be observed that the temporal distribution
shifts are not only influenced by the habits of load demands (sub-
stantial disparities between nighttime and daytime) [ 2], but also
maintain a stable offset with seasonal changes (increased photo-
voltaic generation in summer compared to winter) [ 22,37]. Thus,
employing short-term strategies directly for continuous AVC over
extended durations may lead to unsatisfactory performance. Even
with long-term data for agent training, the model capacity is insuf-
ficient to adaptively represent the entire dynamic characteristics
of the PDNs. Additionally, longer trajectories necessitate higher
computational resources, making it impractical when extending
trajectories to hundreds of times.
In this paper, we introduce a novel temporal prototype-aware
learning method, termed as TPA, to learn time-adaptive AVC under
short-term training trajectories. The proposed TPA comprises two
key components, namely multi-scale dynamic encoder and temporal
prototype-aware policy, which can be seamlessly integrated with
various MARL methods to tackle the challenges arising from the
temporal distribution shifts. Specifically, the multi-scale dynamic
encoder employs a stacked transformer network to integrate the
minute-level temporal observation with season-level task guidance,
which enables the agents to capture temporal dependencies across
different timescales within the PDNs. To derive the final temporal
prototype-aware AVC policy, we propose a learnable prototype
matching mechanism that realizes time-adaptive adjustments under
various operation states. The prototypes are constrained by several
customized losses to serve as representative temporal patterns for
diverse climate impacts, thereby offering corresponding decision
support. Our contributions can be summarized as follows:•We delve into the challenge of learning the long-term dynamic
control policy under short-term training trajectories, a highly
practical yet largely overlooked problem in previous data-driven
studies of power system control applications.
•We propose a novel Temporal Prototype-Aware (TPA) learning
method to learn time-adaptive AVC on the PDNs. By integrating
the multi-scale dynamic encoder and the temporal prototype-
aware policy, TPA enables agents to make decisions that dynam-
ically adapt to the temporal distribution shifts.
•Extensive experiments on the Bus-141 and Bus-322 PDN bench-
marks showcase that the proposed TPA is readily applicable to
various MARL methods and yields results superior to the state-of-
the-art counterparts in both controllable rate and power genera-
tion loss, especially in the long-term operation cycles. Moreover,
the additional temporal prototype analysis demonstrates the
transferability of TPA across different PDN sizes.
2 RELATED WORKS
2.1 AVC Methods on PDNs
Traditional optimization constraint methods for active voltage con-
trol (AVC) can be roughly classified into two categories: the optimal
power flow (OPF)-based methods [ 2,32] and the droop control (DC)-
based methods [ 16,25]. OPF-based methods are regarded as an
optimization problem minimizing the total power loss under di-
verse voltage constraints. Vovos et al . [32] minimizes the power
loss while fulfilling reactive power limits and bus voltage limits to
balance power constraints. This method provides the optimal AVC
performance if given the exact system model, which is always hard
to obtain. DC-based methods establish a piece-wise linear relation-
ship between PV generation and voltage. This relationship can be
utilized to regulate the local bus without acquiring total voltage di-
visions. DC-based methods [ 16,26] rely on their manually-designed
parameters of the linear relationship, which also limits its adaptabil-
ity across different systems. As the PDN scale increases, it is difficult
for both of these two methods to react to the rapid environmental
changes when facing the instability of new energy.
Recently, the data-driven deep MARL technique [ 19,20] has
drawn broad attention in addressing AVC task [ 6,7,33] due to
the fast response and effective interaction ability. Existing MARL
methods for AVC can be broadly classified into two categories:
region-based methods [ 6,7,29] and PV-based methods [ 33,34].
The region-based methods partition the PDN into multiple regions
according to its tree-like structure and treat each region as an indi-
vidual agent to interact with environments. Then common MARL
algorithms can be integrated to solve the AVC tasks. Cao et al. [6]
applies MADDPG [ 21] on each region agent to control the reactive
power of PV inverters. To regular regional reactive power, Sun and
Qiu[29] combines the OPF-based method to MADDPG improving
regional control accuracy. Cao et al . [7] utilizes spectral clustering
algorithms to formulate the control relationship between regions
agent and then applies MATD3 [1] on them, which provides a sig-
nificant reduction in communication requirements. While these
region-based approaches have demonstrated great effectiveness
in smaller-scale PDNs, they suffer from the inconsistent regional
number of PVs and inconsistent regional topology when applied
to larger networks [33]. Thus PV-based methods have emerged to
3599Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
avoid inconsistent regional structures by modeling each PV inverter
as an agent. Wang et al . [33] enable PV agents within the same
region to share their regional observation, which has demonstrated
promise in large-scale benchmarks. Recently, Wang et al . [34] in-
troduce the transformer [ 31] to capture varying significances of
every PV in large-scale PDNs, enabling effective representations
for the characteristics of the power network.
However, all of these methods focus on the short-term strategy
while ignoring the dynamicity of PDNs under varying timescales.
This oversight results in the final suboptimal strategies over ex-
tended periods. Instead, our TPA enables the agents to be actively
aware of the temporal dependencies under different timescales.
2.2 Prototype Learning
The prototype learning technique [ 8,15,36] enables the deep
model to generate output based on a small number of human-
understandable instances. These instances, referred to as prototypes,
represent prominent features that capture essential characteristics
of different classes or groups. They make significant influences on
the final decision while enhancing the representation capability and
explainability of deep models [ 17,40]. Prototype learning has been
widely explored across diverse fields. In the image recognition task,
Chen et al . [8] dissect the image by identifying prototypical parts
and integrating evidence from these prototypes to generate the final
classification, while Xu et al . [36] shared the global attribute proto-
types among different classes to better extend intermediate features
to unseen images. Meanwhile in the text recognition field, Hong
et al. [14] propose prototype trajectories under RNN backbones to
emulate human text analysis, identifying the most similar prototype
for each sentence. For further intuitive and fine-grained interpreta-
tion, Sourati et al . [27] selects sub-sequences of different concepts as
prototype parts, which are further compared with the text input to
understand model decisions. As for the reinforcement learning field,
recent works introduce visual observation prototype [ 10,24,38]
to further extract effective representations of observations. Yarats
et al. [38] pre-train prototypes using observation data from multiple
tasks to extract the summarization of exploratory experiences. The
prototypes then further formulate an intrinsic reward to incentivize
the exploration of uncharted regions within the state space.
To augment the interpretability of RL decision-making processes
in visual games, Ragodos et al . [24] employs contrastive learning to
generate prototypes identified as key scenarios within the task. The
state inputs are compared with these prototypes to compute a simi-
larity measure for explanation and determine the final action out-
put. Deng et al . [10] learn visual prototypes from the observations,
which significantly discards the task-irrelevant visual distractions
for decision-making. Unlike previous methods in RL that passively
partition the state space and extract prototypes, our work actively
considers temporal priors that potentially influence the strategy,
maintaining global temporal prototypes served as representative
temporal patterns reflecting various climate impacts.
3 PRELIMINARY
3.1 AVC Problem on PDNs
In this paper, the power distribution networks (PDNs) installed with
roof-top photovoltaics (PVs) are divided into multiple regions based
Figure 1: An example of the PDN. Each bus is indexed by a
circle with a number. “G" denotes the external generator. “L"
denotes loads. “sun" denotes the location of a PV installed.
We control the voltages on bus 2–12. Bus 0–1 represents the
main system with the constant voltage outside the PDN.
on its realistic tree topology G=(V,E), whereV={0,1,...,𝑁𝑣}
andE={0,1,...,𝑁𝑒}indicate the set of buses (nodes) and the
set of branches (edges), respectively [ 13]. For each bus 𝑖∈ V ,
𝑝𝑖+𝑗𝑞𝑖represents the complex power injection, where 𝑝𝑖is active
power and𝑞𝑖is reactive power. There are complex and non-linear
relationships that satisfy the power system dynamic rules [33].
As shown in Figure. 1, different control regions are divided ac-
cording to their shortest path from the terminal bus to the main
branch [ 33]. Each node carries its own load, and some nodes also
carry PV. In our problem, each PV is considered as an agent and
controls the voltage around a stationary value denoted as 𝑣𝑟𝑒𝑓. For
safe operation of the distribution network, 5% voltage fluctuation
is usually under consideration. Specifically, let the standard value
𝑣𝑟𝑒𝑓=1.00per unit (𝑝.𝑢.). Voltage amplitude of each bus needs
to satisfy 0.95𝑝.𝑢.≤𝑣𝑟𝑒𝑓≤1.05𝑝.𝑢.,∀𝑖∈V\{0}. Some extreme
situations may lead to exceeding safety thresholds. For example,
the end-user voltage could be smaller than 0.95𝑝.𝑢.when the load
is heavy during the nighttime, whereas the solar energy during
summer may cause the voltage amplitude to exceed 1.05𝑝.𝑢..
3.2 Dec-POMDP of AVC
We define the cooperation among PV agents as MARL tasks, further
formulated as a Dec-POMDP [ 23]. A formal Dec-POMDP is formu-
lated as a tuple such that (I,S,A,O,T,𝑟,Ω,𝛾). The components
of the problem definition are described in detail as follows:
Agents:I={1,...,𝑛}denotes the set of 𝑛agents. In the AVC,
we treat every PV invert as an agent.
State and Observation: Sis the state set and Ois the obser-
vation set, where the observation 𝑜𝑖of agent𝑖is drawn from the
observation function Ω(𝑠,𝑖). In this problem, all agents (i.e., PVs)
can only observe partial information from their own region instead
of the global state of the power system. Because the regional PV
ownership may be independent and separated [ 5]. The observation
set is defined asO=D×P×R×M , whereD=
(𝒑ℓ,𝒒ℓ)|ℓ∈Vℓ	
is a set of active and reactive powers of loads. Vℓis the set of all
load nodes.P=
(𝒑𝑝𝑣,𝒒𝑝𝑣)|𝑝𝑣∈V𝑝𝑣	is a set of current active
powers and the preceding reactive powers. V𝑝𝑣is the set of all PV
nodes.R={(𝑣ℓ,𝜔ℓ)|ℓ∈Vℓ}is a set of voltage wherein 𝑣ℓis a
3600KDD ’24, August 25–29, 2024, Barcelona, Spain Feiyang Xu et al.
Figure 2: Illustration of the proposed method. The module (i) is employed for extracting multi-scale temporal dependencies
while module (ii) constructs a prototype matching mechanism to enable agents to dynamically adjust their strategies.
vector of voltage magnitudes and 𝜔ℓis a vector of voltage phases
measured in radius. Mis the predefined zone index for each bus.
Action: Each agent𝑖∈Iis equipped with a continuous action
setA, which represents the ratio of reactive power it generates.
The transition probability T:S×A𝑛×S→[ 0,1]denotes any
state𝑠∈Sto any state 𝑠′∈Safter taking action a∈A𝑛.
Reward: The reward function is defined as follows:
𝑟=−1
|V|∑︁
𝑖∈Vℓ𝑣(𝑣𝑖)−𝛼·ℓ𝑞(𝒒𝑝𝑣), (1)
whereℓ𝑣(·)is a voltage barrier function (detailed in Appendix A).
ℓ𝑞(𝒒𝑝𝑣)=1
|N|||𝒒𝑝𝑣||1is the reactive power generation loss. 𝛼is
a hyper-parameter to balance the ℓ𝑣(·)andℓ𝑞(𝒒𝑝𝑣)on networks
of various scales. The ℓ𝑣(·)loss measures the voltage deviations
whether in the safe range. ℓ𝑞(𝒒𝑝𝑣)controls the reactive power
generation as little as possible to reduce the power waste. We denote
the return asÍ∞
𝑘=0𝛾𝑘𝑟𝑘+1, where𝛾∈[0,1)is the discount factor.
4 METHOD
In this section, we further detail the proposed Temporal Prototype-
Aware learning (TPA) method. As shown in Figure 2, TPA consists
of two core modules, namely multi-scale dynamic encoder and
temporal prototype-aware policy. In the former module, we intro-
duce a stacked transformer network to encode input features with
short-term memory and seasonal labels, capturing the temporal
dependencies. In the latter module, the encoded features are em-
ployed to derive the prototype learning with temporal patterns of
season climates and offer tailored decision support.
4.1 Multi-scale Dynamic Encoder
Due to the dynamic nature of load demands and renewable energy,
the operation states of real-world PDNs may exhibit significant
distribution shifts across varying timescales (e.g., daily and seasonalchanges). The goal of the multi-scale dynamic encoder is to take
network states as input and enhance them with both minute-level
temporal observation and season-level task guidance, training the
stacked transformer network that accomplishes extracting multi-
scale underlying temporal dependencies for each PV agent.
The observation of agent 𝑖at time step 𝑡is denoted as 𝑜𝑡
𝑖∈O,
which contains features of buses in the same region. For simplicity,
we omit the index 𝑖of the agent in the subsequent section. We
extract network representations from 𝑜𝑡via a projection layer 𝑔𝑜.
The obtained network static feature 𝑥𝑜=𝑔𝑜(𝑜𝑡)∈R𝑟×ℎrepresents
the regional network features at 𝑡step, where 𝑟is the number of
buses within the same region and ℎis the latent dimension. To
introduce temporal dependencies to static features, we collect the
period of𝐾steps as a short-term memory 𝑚𝑡=[𝑞𝑝𝑣
𝑡−𝐾,...,𝑞𝑝𝑣
𝑡−1],
where𝑞𝑝𝑣
𝑡−𝑘denotes the previous reactive power 𝑞𝑝𝑣of the agent
at𝑡−𝑘step. The short-term memory 𝑚𝑡intuitively reflects the
current trend at a minute level, so we use a bidirectional long short-
term memory model 𝑔𝑚to extract the fine-grain dynamic features
𝑥𝑚=𝑔𝑚(𝑚)∈R𝑟×ℎ.
To capture temporal dependencies from static and dynamic in-
puts, we use the stacked transformer network [ 31] to obtain en-
coded features layer by layer. As depicted in the left column of
Figure 2, we use transformer 𝑓(·;𝜓):R𝑟×2ℎ→R2ℎparameterized
by𝜓to encode essential static and dynamic features with dimension
2ℎfollowing the general attention mechanism as:
𝑄(ℓ),𝐾(ℓ),𝑉(ℓ)=h
𝑊(ℓ−1)
𝑄,𝑊(ℓ−1)
𝐾,𝑊(ℓ−1)
𝑉i
·𝐸(ℓ−1),
¯𝑌(ℓ)=softmax 
𝑄𝑙𝐾(ℓ)⊤
√︁
𝑑𝑘·𝑉(ℓ)!
,
𝐸(ℓ)=LayerNorm
𝐸(ℓ−1)+𝑊𝑌¯𝑌(ℓ)
,(2)
3601Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
where𝑊𝑄,𝑊𝐾,𝑊𝑉and𝑊𝑌represent learnable parameters. 𝑑𝑘is a
scaling factor used to adjust the scale of attention scores. 𝐸ℓ∈R2ℎ
are the encoded features with dimension 2ℎcomputed after ℓsteps
of the transformer network. The input features 𝐸0is initialized
using the pair of latent inputs [𝑥𝑖,𝑥𝑗], where𝑥𝑖denotes the static-
based inputs and 𝑥𝑗denotes the dynamic inputs. After running 𝐿
iterations of Eq. (2), the stacked transformer network can generate
the final encoded features as F=𝐸(𝐿)∈R2ℎ. Here we adopt two
transformer branches to capture two temporal dependencies.
In the fine-grain branch, we concatenate inputs [𝑥𝑜,𝑥𝑚]and feed
to the stacked transformer network, focusing on capturing latent
regional features enhanced with fine-grain temporal dependencies.
The transformer generates memory-encoded features as:
F𝑚=𝑓([𝑥𝑜,𝑥𝑚];𝜓𝑚)∈R2ℎ. (3)
After the fine-grain branch, we get the one-hot season labels 𝑧at
𝑡step, which is clearly shared by all agents. We use an embed-
ding layer𝑔𝑧to obtain multi-task embeddings 𝑥𝑧=𝑔𝑧(𝑧)∈R𝑟×ℎ
to guide the large variation of temporal dependencies during dif-
ferent seasons under every region. To concatenate F𝑚and𝑰𝑠,
we implement another linear layer 𝑔ℎto reduce dimensions of
ˆF𝑚=𝑔ℎ(F𝑚):R2ℎ→Rℎ. Then we feed the season-level inputs
[ˆF𝑚,𝑥𝑧]to the course-gain branch of the stacked transformer net-
work, further introducing broad temporal dependencies on the foun-
dation of memory-enhanced network features. The transformer
generates final encoded features as:
F𝑧=𝑓([ˆF𝑚,𝑥𝑧];𝜓𝑧)∈R2ℎ. (4)
Here we employ the transformer to learn network representations
from static and dynamic input pairs. The multi-scale dynamic en-
coder can capture the multi-scale temporal dependencies from the
raw observations and temporal features. The encoded features F𝑧
are further employed by the policy module.
4.2 Temporal Prototype-Aware Policy
Existing methods suffer from that are easily suboptimal or even
obsolete when performing continuous AVC over extended periods.
The policy should dynamically adapt to the evolving operation
states. At the heart of the temporal prototype-aware policy is global
temporal patterns of season climates, which enables the agent to
explicitly perceive season climates.
Drawing inspiration from effective load forecasting through 24
solar terms [ 35], we partitioned each season into 6 adaptable proto-
types, resulting in 24 temporal prototypes over a year. We feed the
whole day data (one prototype corresponds to one day) into LSTM
to initialize temporal prototypes P=n
𝑝1,𝑝2,...,𝑝 24|𝑝𝑖∈Rℎo
. To
minimize additional data as much as possible, we select three major
power states 𝑥𝑝∈R𝑑×3(where𝑑denotes the real-time control
period number on the whole day): the active and reactive powers
of loads, and the active powers of PVs to initialize the temporal
prototypes. We match the highest similarity prototypes 𝑝∗to make
the policy be aware of the current season climate, following the
calculation of similarity:
sim(𝑝𝑖,F𝑧)=log 
||𝑝𝑖−F𝑧||2
2+1
||𝑝𝑖−F𝑧||2
2+𝜖!
, (5)where𝜖is set to a small value preventing division by zero. The
linear layer𝑔𝑐with parameters 𝜙is employed to dynamically adapt
encoded features to evolving climate features, extracting the re-
trieval featuresF𝑝=𝑔𝑐(F𝑧,𝑝∗;𝜙)∈Rℎ. We use multilayer per-
ceptrons (MLPs) as the action prediction network 𝑔𝑎to predict the
logits of action in continuous control 𝑎=𝑔𝑎(F𝑝), which is the
action in MARL interacting with environments.
The objective function of prototype learning L𝑝𝑙is defined as:
L𝑝𝑙=L𝑐𝑒(𝑐◦𝑔𝑝◦F𝑧,𝑧)+𝜆1L𝑐𝑙𝑠𝑡+𝜆2L𝑠𝑒𝑝+𝜆3L𝑑𝑖𝑣,(6)
where𝑐is a fully connected layer that predicts season probability
and𝑔𝑝is the prototype layer. L𝑐𝑒represents the cross-entropy
loss for season classification and 𝑧𝑠is season labels. 𝜆1,𝜆2,𝜆3are
hyper-parameters controlling the weights of losses.
As shown in Eq. (6), several constraints are proposed to construct
the final prototypes [ 40]. Firstly, the cluster cost L𝑐𝑙𝑠𝑡in Eq. (7)
motivates encoded features to exhibit proximity to one prototype
corresponding to their season.
L𝑐𝑙𝑠𝑡=1
𝑛N∑︁
𝑖=1min
𝑗:𝑝𝑗∈P𝑦𝑖||F𝑧−𝑝𝑗||2
2, (7)
whereP𝑦𝑖is the set of prototypes under 𝑦𝑖class. Secondly, the
separation costL𝑠𝑒𝑝in Eq. (8) promotes the distancing of encoded
features from prototypes that do not belong to their seasons.
L𝑠𝑒𝑝=−1
𝑛N∑︁
𝑖=1min
𝑗:𝑝𝑗∉P𝑦𝑖||F𝑧−𝑝𝑗||2
2. (8)
Finally, the diversity loss L𝑑𝑖𝑣in Eq. (9) encourages the diversity
of the learned prototypes via penalizing prototypes too close.
L𝑑𝑖𝑣=4∑︁
𝑘=1∑︁
𝑖≠𝑗
𝑝𝑖,𝑝𝑗∈P𝑘max 0,cos 𝑝𝑖,𝑝𝑗−𝜉, (9)
where𝑘={1,2,3,4}denotes 4 seasons. 𝜉is the threshold of the
cosine similarity in the diversity loss.
For MARL process, there are 𝑛agents with policies 𝝁={𝜇𝜃1,...,𝜇𝜃𝑛},
where each policy is parameterized by 𝜃𝑖all proceed through the
subsequent steps. At each control step 𝑡, the agent𝑖with a given ob-
servation𝑜𝑖under state𝑠will select the action 𝑎𝑖with exploration
noises and storage to the experience replay buffer B. Then the agent
will receive shared rewards 𝑟according to the reward function in
Eq. (1) and the next observation 𝑠′. The global Q-function 𝑄𝝁
𝑔𝑙𝑜𝑏𝑎𝑙
embedded the value of actions. The joint agents with policies can be
optimized by our total objective function L𝑝𝑙+L𝑎𝑐. The objective
function of prototype learning L𝑝𝑙is formulated in Eq. (6). We
perform deterministic policy gradient [21] over L𝑎𝑐as:
∇𝜃𝑖L𝑎𝑐= 𝔼𝑠,𝑎∼Bh
∇𝜃𝑖𝜇𝑖(𝑎𝑖|𝑜𝑖)∇𝑎𝑖𝑄𝝁
𝑔𝑙𝑜𝑏𝑎𝑙(𝑠,𝑎 1,...,𝑎𝑛)|𝑎𝑖=𝜇𝑖(𝑜𝑖)i
.
(10)
Then the critic network estimates the long-term impact of actions
compared with real rewards, which are optimized by
L𝑐𝑟𝑖𝑡𝑖𝑐 = 𝔼𝑠,𝑎,𝑟,𝑠′h
𝑄𝝁
𝑔𝑙𝑜𝑏𝑎𝑙(𝑠,𝑎 1,...,𝑎𝑛)−𝑦
,2i
,
𝑦=𝑟+𝛾𝑄𝝁′
𝑔𝑙𝑜𝑏𝑎𝑙(𝑠′,𝑎′
1,...,𝑎′
𝑛)|𝑎′
𝑖=𝝁′(𝑜𝑖).(11)
3602KDD ’24, August 25–29, 2024, Barcelona, Spain Feiyang Xu et al.
MADDPG
 MAPPO
 MATD3
 T-MADD P G
 T-MATD 3
 COMA
 TPA-MA T D 3
 TPA-MA D D P G
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c
(a) CR-L1-322
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c (b) CR-BOWL-322
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c (c) CR-L2-322
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c (d) CR-L1-141
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c (e) CR-BOWL-141
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c (f) CR-L2-141
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000013/uni00000017/uni00000011/uni00000013/uni00000019/uni00000011/uni00000013/uni0000001b/uni00000011/uni00000013/uni00000034/uni0000002f/uni00000003/uni0000000b×102/uni0000000c
(g) QL-L1-322
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000013/uni00000017/uni00000011/uni00000013/uni00000019/uni00000011/uni00000013/uni0000001b/uni00000011/uni00000013/uni00000034/uni0000002f/uni00000003/uni0000000b×102/uni0000000c
 (h) QL-BOWL-322
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000013/uni00000017/uni00000011/uni00000013/uni00000019/uni00000011/uni00000013/uni0000001b/uni00000011/uni00000013/uni00000034/uni0000002f/uni00000003/uni0000000b×102/uni0000000c
 (i) QL-L2-322
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000034/uni0000002f (j) QL-L1-141
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000034/uni0000002f (k) QL-BOWL-141
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000034/uni0000002f (l) QL-L2-141
Figure 3: Median CR and QL of algorithms with different voltage barrier functions. The sub-caption indicates metric-Barrier-
scenario. “TPA-” refers to the combination of our framework with other methods, while “T-” represents the incorporation of
the previous TMAAC with other methods. All experimental results are illustrated with the mean and the standard deviation
of the metrics over 5 random seeds for a fair comparison. To make the results clearer for readers, we adopt a 50% confidence
interval to plot the error region.
Table 1: Test results of algorithms in 322-bus and 141-bus system. Bold denotes the best results. ±corresponds to the standard
deviation of testing episodes.
Metho
dsSpring
Summer Fall Winter Average
CR(%)
QL(MW
MV
AR) CR(%)
QL(MW
MV
AR) CR(%)
QL(MW
MV
AR)CR(%)
QL(MW
MV
AR)CR(%)
QL(MW
MV
AR)
322-MAPPO 57.5±9.4 0.035 41.2±5.8 0.035 56.1±11.2 0.035 78.7±17.8 0.036 58.4
0.035
322-COMA 82.4±18.3 0.038 70.3±17.2 0.038 83.4±18.1 0.038 96.9±5.3 0.038 83.3
0.038
322-MATD3 87.0±17.4 0.038 74.3±17.3 0.039 89.2±17.9 0.038 99.0±4.5 0.038 87.4
0.038
322-MADDPG 86.2±18.1 0.032 72.0±18.6 0.033 87.5±18.4 0.032 98.3±5.7 0.031 86.0
0.032
322-T-MADDPG 91.2±14.8 0.031 82.5±21.9 0.033 92.9±13.1 0.031 97.8±7.8 0.029 91.1
0.031
322-T-MATD3 93.5±13.0 0.030 87.5±21.3 0.029 95.4±12.2 0.027 97.7±7.8 0.030 93.5 0.029
322-
TPA-MATD3 93.2±12.8 0.032 86.3±21.8 0.031 93.3±13.4 0.030 97.2±6.6 0.030 92.5
0.030
322-
TPA-MADDPG 95.1±11.5 0.029 89.9±21.0 0.029 97.1±10.4 0.029 99.1±3.1 0.029 95.3
0.029
141-MAPPO 76.3±20.2 1.025 61.0±13.9 0.946 78.4±23.0 1.027 97.2±6.2 1.148 78.2
1.037
141-COMA 87.1±4.8 1.631 86.3±9.9 1.707 88.3±4.1 1.631 87.2±3.5 1.554 87.2
1.631
141-MADDPG 97.1±6.9 1.140 96.6±11.3 1.105 99.4±3.5 1.148 99.8±0.7 1.232 98.2
1.156
141-MATD3 97.5±5.7 1.134 97.4±9.3 1.151 99.9±0.7 1.140 100±0.0 1.144 98.7 1.142
141-
T-MATD3 97.0±4.3 0.816 97.4±7.9 0.836 99.9±0.8 0.810 100±0.0 0.836 98.5
0.825
141-T-MADDPG 97.4±5.1 0.877 97.8±8.8 0.871 99.8±0.6 0.877 100±0.0 0.938 98.7 0.891
141-
TPA-MATD3 97.1±4.1 0.889 96.8±7.6 0.889 98.5±0.7 0.868 100±0.0 0.909 98.1
0.888
141-
TPA-MADDPG 97.1±4.2 0.734 97.8±7.4 0.793 99.9±0.5 0.789 100±0.0 0.772 98.7
0.772
where 𝝁′={𝜇𝜃′
1,...,𝜇𝜃′𝑛}is the set of target policies with delayed
parameters 𝜃′
𝑖. In addition, the proposed TPA serves as a plug-
and-play module readily applicable to various MARL algorithms
including MADDPG [21] and MATD3 [1].
5 EXPERIMENTS
To illustrate the effectiveness of the proposed TPA method, we con-
duct experiments on the MAPDN benchmark (Wang et al . [33] ). We
aim to answer the following questions: (1) Can TPA outperform thestate-of-the-art MARL methods on both the singular diurnal cycle
and longer cycles? (Section 5.2 and Section 5.3) (2) How do different
components of TPA contribute to the overall performance? (Sec-
tion 5.4) (3) Can the TPA offer the prototypes transferability? (Sec-
tion 5.5) Besides, the visualization analysis is given in Section 5.6.
5.1 Experimental Settings
Tasks and Datasets. We consider the active voltage control task
on 141-bus and 322-bus power distribution networks with large
3603Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
scale in MAPDN benchmark [ 33]. The 141-bus network is divided
into 9 zones and contains 84 loads and 22 PVs, while the 322-bus
network is divided into 22 zones and contains 337 loads and 38
PVs. The main objective of the 141 network setting is reducing
power waste losses, while the 322 network setting aims to promote
control rates. The reward function is defined by Eq. (1), where
the weight𝛼assigned to ℓ𝑞(𝒒pv)in the reward function balances
the control stability and power generation wastes on networks of
various scales. The load and PV data, extracted from three years of
real-world data, are interpolated at a 3-minute resolution to match
the real-time control period and further randomly initialize the
network scenarios. During training, we randomly sample the initial
state for an episode and each episode lasts for 240 time steps (i.e.
a half day) [ 33]. More details of datasets and implementation are
provided in Appendix A.
Evaluation Metrics. We utilize the Controllable Rate (CR) and Q
Loss (QL) to qualitatively evaluate the performance following Wang
et al. [33] . CR quantifies the effectiveness by measuring the ratio
of time steps during which all bus voltages are under control. QL
indicates the mean reactive power generations by agents per time
step to assess the power waste.
Comparison methods. We compare the proposed TPA to sev-
eral state-of-the-art MARL methods in continuous action space,
including MAPPO [ 39], COMA [ 12], MADDPG [ 21], MATD3 [ 1].
For a fair comparison, we follow MAPDN [ 33] to strengthen the
basic MARL methods by PV-based modeling. Both T-MADDPG and
T-MATD3 are proposed in the TMAAC [ 34], which incorporates
the transformer to capture regional dependencies in PDNs.
5.2 Results on Short-term Operation Cycles
We first compare the performances of our TPA and other baselines
under the singular diurnal cycle in active voltage control tasks.
Figure 3 presents the learning curves of comparison methods on
different power networks, while the final best performance is shown
in Table 1. The average metrics are used for learning curves.
The results show that TPA can effectively address the AVC prob-
lem in the singular diurnal cycle, while also reducing power waste.
Specifically, from the perspective of effective voltage control, the
TPA-based architecture significantly outperforms the common one.
For example on the 322-bus system, all TPA-based methods achieve
over 90% in the average CR metric, while the common state-of-art
method MATD3 only achieves 87.4%. Meanwhile, the quantitative
results showcase that the best model of TPA leads the TMAAC
ones by 1.8% in the average CR metric. The comparison highlights
that agents can be effectively enhanced with multi-scale temporal
dependencies. From the perspective of power loss, TPA achieves
comparable and superior CR while exhibiting a low QL value. For
example on the 141-bus system, other methods like MADDPG and
MADTD3 exhibit larger QL when achieving the relatively high
CR metric in all test cycles. In contrast, our approach not only
attains state-of-the-art on the CR metric but also reaches the min-
imum on the QL metric. The superiority of our TPA method can
be attributed to the temporal prototypes that effectively utilize sea-
sonal climates and dynamically adapt to evolving operational states.
This approach has resulted in a significant improvement in voltage
quality compared to alternative methods.Table 2: Test results of our TPA and other baselines in 322-bus
and 141-bus systems under longer testing cycles. The average
metrics are used for all cycles.
Metho
dDay
Month Year
CR(%)
QL(MW
MV
AR)CR(%)
QL(MW
MV
AR)CR(%)
QL(MW
MV
AR)
322-MAPPO 58.4
0.035 58.6
0.035 58.6
0.035
322-COMA 83.3
0.038 81.6
0.038 81.6
0.038
322-MATD3 87.4
0.038 85.9
0.039 85.7
0.039
322-MADDPG 86.0
0.032 85.7
0.032 85.5
0.032
322-T-MADDPG 91.1
0.031 89.3
0.030 88.2
0.030
322-T-MATD3 93.5 0.029 91.5 0.028 90.4
0.031
322-
TPA-MATD3 92.5
0.030 91.9
0.032 93.4
0.029
322-
TPA-MADDPG 95.3
0.029 93.4 0.030 92.2
0.030
141-MAPPO 78.2
1.037 77.3
1.025 77.0
1.025
141-COMA 87.2
1.631 85.8
1.642 85.8
1.644
141-MADDPG 98.2
1.156 96.5
1.161 96.4
1.161
141-MATD3 98.7 1.142 97.2
1.154 97.1
1.154
141-T-MADDPG 98.5
0.825 97.3
0.910 97.2
0.910
141-T-MATD3 98.7 0.891 97.7
0.839 97.5
0.841
141-
TPA-MATD3 98.1
0.888 97.5
0.904 97.5
0.903
141-
TPA-MADDPG 98.7
0.772 98.1
0.815 98.0
0.815
Meanwhile, TPA exhibits high robustness in extreme situations.
As shown in Table 1, summer is the most challenging season of all
year. The CR metric of all methods in summer shows the lowest CR
and higher standard deviation in all seasons, due to its intense pho-
tovoltaic fluctuations. However, our TPA makes a more significant
improvement in the safety of the strategy in summer to achieve
high CR and low QL. Meanwhile, the standard deviation of our TPA
in other seasons also achieves the lowest value compared to other
methods. A detailed visualization analysis of the strategy in four
seasons is presented in Appendix 5.6.
5.3 Results on Long-term Operation Cycles
To further evaluate our TPA performances under long-term opera-
tion cycles, we conduct experiments on the 141-bus and 322-bus
networks under 3 different cycles: day, month, and year. Concretely,
we still train our TPA with the short-term training trajectories of
a half-day, yet assess the performance on longer cycles. The final
performances on average metrics are presented in Table 2.
The results demonstrate that TPA performs with high robustness
under longer cycles. Specifically, as the test cycle grew longer, the
metrics of all other methods showed a decline, which to some extent
confirmed our initial idea that longer cycles would bring the prob-
lems of multi-scale temporal distribution shifts. However, our TPA
with robustness on the temporal scale can alleviate this issue and
surpass other methods with minimal declines. When performing
continuous AVC over extended periods, the short-term strategies of
other methods will deviate from the initial temporal dependencies,
gradually moving towards suboptimal solutions. Instead, TPA can
dynamically adapt to the evolving operation states and maintain
robust control stability even over prolonged periods.
Moreover, the test results under both the singular diurnal cycle
and longer cycles show that TPA can effectively serve as a plug-
and-play module. As shown in Figure 3, two TPA-based MARL al-
gorithms both demonstrate powerful performance. The TPA-based
methods consistently sustain CR surpassing 90% in longer cycles,
3604KDD ’24, August 25–29, 2024, Barcelona, Spain Feiyang Xu et al.
TPA
 TPA w/o  P
 TPA w/o  S
TPA w/o  M
 T-MADD P G  w i t h  S
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c
(
a) CR-L1-322
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000013/uni00000017/uni00000011/uni00000013/uni00000019/uni00000011/uni00000013/uni0000001b/uni00000011/uni00000013/uni00000034/uni0000002f/uni00000003/uni0000000b×102/uni0000000c
 (b) QL-L1-322
/uni00000036/uni00000053/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000036/uni00000058/uni00000050/uni00000050/uni00000048/uni00000055/uni00000029/uni00000044/uni0000004f/uni0000004f/uni0000003a/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c
(c) CR-322-Test
/uni00000036/uni00000053/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000036/uni00000058/uni00000050/uni00000050/uni00000048/uni00000055/uni00000029/uni00000044/uni0000004f/uni0000004f/uni0000003a/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000015/uni00000011/uni00000016/uni00000015/uni00000011/uni00000019/uni00000015/uni00000011/uni0000001c/uni00000016/uni00000011/uni00000015/uni00000016/uni00000011/uni00000018/uni00000016/uni00000011/uni0000001b/uni00000034/uni0000002f/uni00000003/uni0000000b×102/uni0000000c
 (d) QL-322-Test
Figure 4: Training curves and test results of TPA and its abla-
tions on the IEEE 322-bus system. “S” denotes the seasonal
labels and "M" denotes short-term memory. "P" represents the
learnable prototypes. "T-" represents the previous TMAAC
method. "TPA" selects MADDPG as the basic algorithm.
outperforming the common MADDPG and MATD3 methods. Espe-
cially on the longest year cycle, the TPA-MATD3 method outpaces
MATD3 by 7.7%, and the TPA-MADDPG method surpasses MAD-
DPG by 6.7%. The TPA module is readily applicable to various MARL
algorithms and enhances their strategies with temporal awareness.
5.4 Ablation Study
To analyze the impact of different components in TPA, we carry
out ablation studies to quantitatively evaluate the contribution
of different TPA components. The comparison results of various
ablations on the IEEE 322-bus system are shown in Figure 4. The
training curves present the average metrics.
T-MADDPG with S. To assess the impact of prior temporal
knowledge for the TMAAC method, we re-train T-MADDPG on
the input short-term data enhanced by the one-hot encoded sea-
sonal feature. The quantitative results indicate that the re-trained
T-MADDPG gets even worse results than the normal performances
in Table 1. The improper processing of the introduced temporal
inputs can easily result in training failures, diminishing the ability
of encoders to effectively address temporal distribution shifts and
hampering their expressiveness.
TPA w/o M and TPA w/o S. To evaluate the effect of short-term
memory and the seasonal labels, we implement 2 variant: 1) with-
out short-term memory input, and 2) without the seasonal labels
input. The results show that both of them have a positive effect
on the final strategy; however, the impact of short-term memory
surpasses that of seasonal labels significantly. While short-term
memory intuitively reflects the current trend, the seasonal labels
Origin
 Pretraine d
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c(
a) CR-L1-141
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000034/uni0000002f (
b) QL-L1-141
/uni00000036/uni00000053/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000036/uni00000058/uni00000050/uni00000050/uni00000048/uni00000055/uni00000029/uni00000044/uni0000004f/uni0000004f/uni0000003a/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni0000001c/uni00000015/uni0000001c/uni00000017/uni0000001c/uni00000019/uni0000001c/uni0000001b/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c
(
c) CR-141-Test
/uni00000036/uni00000053/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000036/uni00000058/uni00000050/uni00000050/uni00000048/uni00000055/uni00000029/uni00000044/uni0000004f/uni0000004f/uni0000003a/uni0000004c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000017/uni00000011/uni00000018/uni00000018/uni00000011/uni00000018/uni00000019/uni00000011/uni00000018/uni0000001a/uni00000011/uni00000018/uni0000001b/uni00000011/uni00000018/uni00000034/uni0000002f/uni00000003/uni0000000b×101/uni0000000c
 (
d) QL-141-Test
Figure 5: Training curves and test results of transferability
experiment on the 141-bus network. "Origin" denotes the
normal TPA model. "Pretrained" denotes the TPA model with
pre-trained prototypes on the 322-bus network. MADDPG is
selected as the basic algorithm.
emphasize capturing broader temporal dependencies. In compari-
son to seasonal labels, short-term memory distinctly captures the
trend to facilitate adjustments at each moment, while the sensitivity
of seasonal labels to intermittent fluctuations is significantly lower.
TPA w/o P. To examine the effectiveness of the temporal pro-
totype, we remove the temporal prototype to predict the action
based on encoded features. The results show the significant impact
of temporal prototypes on the stability of the learning process. Fig-
ure 4 illustrates that "TPA w/o P" only obtained suboptimal results.
Although the former encoder pays attention to multi-scale depen-
dencies, the agent performance deteriorates and wavers around
optimal decisions under seasonal fluctuations, due to the lack of
season climates introduced by temporal prototypes.
5.5 Transferability Analysis
To further evaluate the transferability of the generated temporal
prototypes, we evaluate the TPA performance on the 141-bus net-
work with fixed temporal prototypes trained under the 322-bus
network. Concretely, we fully trained a TPA model on the 322-bus
network to acquire the pre-trained temporal prototypes. Subse-
quently, in the new 141-bus environment, we re-trained a new
TPA model while initializing it with these pre-trained temporal
prototypes and keeping them fixed during the training process.
The results compared to the original TPA methods are presented
in Figure 5. The training curves and testing results indicate that TPA
with pre-trained prototypes from other networks performs equally
well on the target network. In the spring, summer, and the final av-
erage CR metric, TPA with pre-trained prototypes can even surpass
the original TPA method. TPA with pre-trained prototypes also
3605Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
No Contr o l
 MADDPG
 T-MADD P G
 TPA-MA D D P G
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017
/uni0000002b/uni00000052/uni00000058/uni00000055/uni00000003/uni0000000b/uni0000004b/uni0000000c/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000039/uni00000052/uni0000004f/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000053/uni00000011/uni00000058/uni00000011/uni0000000c
(a) Voltage of bus 254 in spring.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017
/uni0000002b/uni00000052/uni00000058/uni00000055/uni00000003/uni0000000b/uni0000004b/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000003a/uni00000003/uni0000000d/uni00000003/uni00000014/uni00000048/uni00000010/uni00000018/uni0000000c (b) Loss of bus 254 in spring.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017
/uni0000002b/uni00000052/uni00000058/uni00000055/uni00000003/uni0000000b/uni0000004b/uni0000000c/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000039/uni00000052/uni0000004f/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000053/uni00000011/uni00000058/uni00000011/uni0000000c (c) Voltage of bus 254 in summer.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017
/uni0000002b/uni00000052/uni00000058/uni00000055/uni00000003/uni0000000b/uni0000004b/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000003a/uni00000003/uni0000000d/uni00000003/uni00000014/uni00000048/uni00000010/uni00000018/uni0000000c (d) Loss of bus 254 in summer.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017
/uni0000002b/uni00000052/uni00000058/uni00000055/uni00000003/uni0000000b/uni0000004b/uni0000000c/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000039/uni00000052/uni0000004f/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000053/uni00000011/uni00000058/uni00000011/uni0000000c
(e) Voltage of bus 254 in fall.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017
/uni0000002b/uni00000052/uni00000058/uni00000055/uni00000003/uni0000000b/uni0000004b/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000003a/uni00000003/uni0000000d/uni00000003/uni00000014/uni00000048/uni00000010/uni00000018/uni0000000c (f) Loss of bus 254 in fall.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017
/uni0000002b/uni00000052/uni00000058/uni00000055/uni00000003/uni0000000b/uni0000004b/uni0000000c/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001c/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000013/uni00000018/uni00000014/uni00000011/uni00000014/uni00000013/uni00000039/uni00000052/uni0000004f/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000053/uni00000011/uni00000058/uni00000011/uni0000000c (g) Voltage of bus 254 in winter.
/uni00000013 /uni00000017 /uni0000001b /uni00000014/uni00000015 /uni00000014/uni00000019 /uni00000015/uni00000013 /uni00000015/uni00000017
/uni0000002b/uni00000052/uni00000058/uni00000055/uni00000003/uni0000000b/uni0000004b/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000016/uni00000011/uni00000013/uni00000033/uni00000052/uni0000005a/uni00000048/uni00000055/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000003a/uni00000003/uni0000000d/uni00000003/uni00000014/uni00000048/uni00000010/uni00000018/uni0000000c (h) Loss of bus 254 in winter.
Figure 6: Compare TPA with other methods on a bus during one day for the 322-bus network. The green and black dotted lines
in (a) represent the safety voltage range and ideal voltage respectively.
has a higher CR at the beginning stages of training. The improved
performance could be attributed to the precise guidance offered
by pre-trained prototypes, assisting the former encoder module in
learning with temporal dependencies. While conventional MARL
methods dynamically adjust the model structure based on the vary-
ing scales of the power networks, our prototypes provide global
temporal patterns that are independent of network topology.
5.6 Visualization Analysis
To further explain the learned time-adaptive strategies from TPA,
we conduct a qualitative analysis. The voltage and power loss of
a single bus over one day across different seasons under the 322-
bus network are visualized in Figure 6. We compared our TPA-
MADDPG method with no control, common MADDPG, and the
previous state-of-the-art T-MADDPG. The visual results show that
the proposed method successfully learns time-adaptive policies for
different seasons, which significantly stabilizes the voltage. Specif-
ically, in extreme cases such as the noon in summer, a large pen-
etration of PVs leads to reverse current flow that would increase
𝑣𝑖out of the nominal range. Benefiting from multi-scale temporal
offset adaptation, the TPA method can effectively avoid risks at
such moments and ensure safety by controlling the overall voltage
within a lower range during summer.
6 CONCLUSIONS
In this paper, we explore the challenge of learning long-term time-
adaptive policy under short-term training trajectories. Unlike the
previous methods within the confines of a day cycle, we propose
a temporal prototype-aware (TPA) method to effectively control
even on month and year cycles. The proposed method can not onlylearn underlying temporal dependencies by the multi-scale dynamic
encoder but also dynamically adapt to the evolving season climates
by matching the temporal prototypes. Experimental results show
that TPA outperforms other methods in both controllable rate and
power generation loss, especially in the long-term operation cycles.
Moreover, TPA is readily applicable to various MARL methods and
shows the transferability of prototypes across different PDN sizes.
In our future work, we will enhance the interpretability of our
model to ensure more reliable practical applications.
ACKNOWLEDGMENTS
This work was supported in part by the Joint Funds of the Zhe-
jiang Provincial Natural Science Foundation of China under Grant
LHZSD24F020001, in part by the Zhejiang Province “LingYan" Re-
search and Development Plan Project under Grant 2024C01114,
and in part by the Zhejiang Province High-Level Talents Special
Support Program “Leading Talent of Technological Innovation of
Ten-Thousands Talents Program" under Grant 2022R52046.
REFERENCES
[1]Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. 2019.
Reducing overestimation bias in multi-agent domains using double centralized
critics. arXiv preprint arXiv:1910.01465 (2019).
[2]Yashodhan P Agalgaonkar, Bikash C Pal, and Rabih A Jabr. 2013. Distribution
voltage control considering the impact of PV generation on tap changers and
autonomous regulators. IEEE Transactions on Power Systems 29, 1 (2013), 182–192.
[3]Johanna Barr and Ritwik Majumder. 2014. Integration of distributed genera-
tion in the volt/var management system for active distribution networks. IEEE
Transactions on Smart Grid 6, 2 (2014), 576–586.
[4]Stratis Batzelis, Zakir Hussain Rather, John Barton, Bonu Ramesh Naidu, Billy
Wu, Firdous Ul Nazir, Onyema Sunday Nduka, Wei He, Jerome Nsengiyaremye,
Bandopant Pawar, et al .2021. Solar integration in the UK and India: technical
barriers and future directions. In Report of the UK-India Joint Virtual Clean Energy
Centre.
3606KDD ’24, August 25–29, 2024, Barcelona, Spain Feiyang Xu et al.
[5]Scott Burger, Jesse D Jenkins, Carlos Batlle López, and José Ignacio Pérez Ar-
riaga. 2018. Restructuring revisited: competition and coordination in electricity
distribution systems. Center for Energy and Environmental Policy Research (2018).
[6]Di Cao, Weihao Hu, Junbo Zhao, Qi Huang, Zhe Chen, and Frede Blaabjerg.
2020. A multi-agent deep reinforcement learning based voltage regulation using
coordinated PV inverters. IEEE Transactions on Power Systems 35, 5 (2020), 4120–
4123.
[7]Di Cao, Junbo Zhao, Weihao Hu, Fei Ding, Qi Huang, and Zhe Chen. 2020.
Distributed voltage regulation of active distribution system based on enhanced
multi-agent deep reinforcement learning. arXiv preprint arXiv:2006.00546 (2020).
[8]Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan
Su. 2019. This Looks Like That: Deep Learning for Interpretable Image Recogni-
tion. In Conference on Neural Information Processing Systems, Hanna M. Wallach,
Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and
Roman Garnett (Eds.). 8928–8939.
[9]Xin Chen, Guannan Qu, Yujie Tang, Steven Low, and Na Li. 2022. Reinforcement
learning for selective key applications in power systems: Recent advances and
future challenges. IEEE Transactions on Smart Grid 13, 4 (2022), 2935–2958.
[10] Fei Deng, Ingook Jang, and Sungjin Ahn. 2022. Dreamerpro: Reconstruction-
free model-based reinforcement learning with prototypical representations. In
International Conference on Machine Learning. PMLR, 4956–4975.
[11] Reid Detchon and Richenda Van Leeuwen. 2014. Policy: Bring sustainable energy
to the developing world. Nature 508, 7496 (2014), 309–311.
[12] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and
Shimon Whiteson. 2018. Counterfactual multi-agent policy gradients. In AAAI
Conference on Artificial Intelligence, Vol. 32.
[13] Lingwen Gan, Na Li, Ufuk Topcu, and Steven H Low. 2013. Optimal power flow
in tree networks. In In Proceedings of the IEEE Conference on Decision and Control.
IEEE, 2313–2318.
[14] Dat Hong, Stephen S Baek, and Tong Wang. 2020. Interpretable sequence classi-
fication via prototype trajectory. arXiv preprint arXiv:2007.01777 (2020).
[15] Jiacong Hu, Jingwen Ye, Zunlei Feng, Jiazhen Yang, Shunyu Liu, Lingxiang Jia,
and Mingli Song. 2024. Improving Adversarial Robustness via Feature Pattern
Consistency Constraint. In Proceedings of the Thirty-third International Joint
Conference on Artificial Intelligence, IJCAI-24.
[16] Pedram Jahangiri and Dionysios C Aliprantis. 2013. Distributed Volt/VAr control
by PV inverters. IEEE Transactions on power systems 28, 3 (2013), 3429–3439.
[17] Eoin M Kenny, Mycal Tucker, and Julie Shah. 2022. Towards interpretable deep
reinforcement learning with human-friendly prototypes. In International Confer-
ence on Learning Representations.
[18] Shunyu Liu, Wei Luo, Yanzhen Zhou, Kaixuan Chen, Quan Zhang, Huating
Xu, Qinglai Guo, and Mingli Song. 2024. Transmission Interface Power Flow
Adjustment: A Deep Reinforcement Learning Approach Based on Multi-Task
Attribution Map. IEEE Transactions on Power Systems 39, 2 (2024), 3324–3335.
[19] Shunyu Liu, Jie Song, Yihe Zhou, Na Yu, Kaixuan Chen, Zunlei Feng, and Mingli
Song. 2024. Interaction Pattern Disentangling for Multi-Agent Reinforcement
Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).
[20] Shunyu Liu, Yihe Zhou, Jie Song, Tongya Zheng, Kaixuan Chen, Tongtian Zhu,
Zunlei Feng, and Mingli Song. 2023. Contrastive Identity-Aware Learning for
Multi-Agent Value Decomposition. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 37. 11595–11603.
[21] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor
Mordatch. 2017. Multi-agent actor-critic for mixed cooperative-competitive
environments. In Conference on Neural Information Processing Systems.
[22] CL Masters. 2002. Voltage rise: the big issue when connecting embedded gen-
eration to long 11 kV overhead lines. Power engineering journal 16, 1 (2002),
5–12.
[23] Frans A Oliehoek, Christopher Amato, et al .2016. A concise introduction to
decentralized POMDPs. Vol. 1. Springer.[24] Ronilo Ragodos, Tong Wang, Qihang Lin, and Xun Zhou. 2022. ProtoX: Ex-
plaining a Reinforcement Learning Agent via Prototyping. Conference on Neural
Information Processing Systems 35 (2022), 27239–27252.
[25] Johannes Schiffer, Thomas Seel, Jörg Raisch, and Tevfik Sezi. 2015. Voltage
stability and reactive power sharing in inverter-based microgrids with consensus-
based distributed voltage control. IEEE Transactions on Control Systems Technology
24, 1 (2015), 96–109.
[26] Ankit Singhal, Venkataramana Ajjarapu, Jason Fuller, and Jacob Hansen. 2018.
Real-time local volt/var control under external disturbances with high PV pene-
tration. IEEE Transactions on Smart Grid 10, 4 (2018), 3849–3859.
[27] Zhivar Sourati, Darshan Deshpande, Filip Ilievski, Kiril Gashteovski, and Sascha
Saralajew. 2023. Robust Text Classification: Analyzing Prototype-Based Networks.
arXiv preprint arXiv:2311.06647 (2023).
[28] Hongbin Sun, Qinglai Guo, Junjian Qi, Venkataramana Ajjarapu, Richard Bravo,
Joe Chow, Zhengshuo Li, Rohit Moghe, Ehsan Nasr-Azadani, Ujjwol Tamrakar,
et al.2019. Review of challenges and research opportunities for voltage control
in smart grids. IEEE Transactions on Power Systems 34, 4 (2019), 2790–2801.
[29] Xianzhuo Sun and Jing Qiu. 2021. Two-stage volt/var control in active distri-
bution networks with multi-agent deep reinforcement learning method. IEEE
Transactions on Smart Grid 12, 4 (2021), 2903–2912.
[30] Konstantin Turitsyn, Petr Sulc, Scott Backhaus, and Michael Chertkov. 2011.
Options for control of reactive power by distributed photovoltaic generators.
Proc. IEEE 99, 6 (2011), 1063–1073.
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you
Need. In Conference on Neural Information Processing Systems, Isabelle Guyon,
Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vish-
wanathan, and Roman Garnett (Eds.). 5998–6008.
[32] Panagis N Vovos, Aristides E Kiprakis, A Robin Wallace, and Gareth P Harri-
son. 2007. Centralized and distributed voltage control: Impact on distributed
generation penetration. IEEE Transactions on power systems 22, 1 (2007), 476–483.
[33] Jianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim C. Green.
2021. Multi-Agent Reinforcement Learning for Active Voltage Control on Power
Distribution Networks. In Conference on Neural Information Processing Systems,
Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and
Jennifer Wortman Vaughan (Eds.). 3271–3284.
[34] Minrui Wang, Mingxiao Feng, Wengang Zhou, and Houqiang Li. 2022. Stabilizing
Voltage in Power Distribution Networks via Multi-Agent Reinforcement Learning
with Transformer. In ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, Aidong Zhang and Huzefa Rangwala (Eds.). ACM, 1899–1909.
[35] Jingrui Xie and Tao Hong. 2018. Load forecasting using 24 solar terms. Journal
of Modern Power Systems and Clean Energy 6, 2 (2018), 208–214.
[36] Wenjia Xu, Yongqin Xian, Jiuniu Wang, Bernt Schiele, and Zeynep Akata. 2020.
Attribute Prototype Network for Zero-Shot Learning. In Conference on Neural
Information Processing Systems, Hugo Larochelle, Marc’Aurelio Ranzato, Raia
Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).
[37] Guangya Yang, Francesco Marra, Miguel Juamperez, Søren Bækhøj Kjær, Seyed-
mostafa Hashemi, Jacob Østergaard, Hans Henrik Ipsen, and Kenn HB Frederik-
sen. 2015. Voltage rise mitigation for solar PV integration at LV grids Studies
from PVNET. dk. Journal of Modern Power Systems and Clean Energy 3, 3 (2015),
411–421.
[38] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. 2021. Reinforce-
ment learning with prototypical representations. In International Conference on
Machine Learning. PMLR, 11920–11931.
[39] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen,
and Yi Wu. 2022. The surprising effectiveness of ppo in cooperative multi-agent
games. In Conference on Neural Information Processing Systems.
[40] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. 2022. Protgnn:
Towards self-explaining graph neural networks. In AAAI Conference on Artificial
Intelligence, Vol. 36. 9127–9135.
3607Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
A MORE IMPLEMENTATION DETAILS
A.1 Datasets
The 141-bus network is divided into 9 zones and contains 84 loads
and 22 PVs. The 322-bus network is divided into 22 zones and
contains 337 loads and 38 PVs. Following the previous work [ 33],
we set the range of actions for each scenario, [−0.6,0.6]for 141-bus
network, and[−0.8,0.8]for 322-bus network . To tune the trade-off
between CR and QL, it is suggested that 𝛼of reward function in
Eq. (1) is set to 0.1 in the 322-bus network and 0.01 in the 141-bus
network [ 33]. Each time step represents a duration of 3 minutes. So
every training episode lasts for 240 time steps and every singular
diurnal cycle testing lasts for 480 time steps.
0.90 0.95 1.00 1.05 1.10
Voltage (p.u.)0.000.020.040.060.080.10Penalty
(a) L1-Shape
0.90 0.95 1.00 1.05 1.10
Voltage (p.u.)0.0000.0050.0100.0150.020Penalty (b) L2-Shape
0.90 0.95 1.00 1.05 1.10
Voltage (p.u.)0.000.020.040.060.080.10Penalty (c) BOWL-Shape
Figure 7: Three types of voltage barrier function.
There are three types of different voltage barrier functions pro-
posed in the MAPDN environment [ 33], as shown in Figure 7. The
voltage constraint is difficult to handle in MARL, so we need to
use a barrier function to represent the constraint. Each barrier
function has a different objective, for example, L1-Shape will se-
verely punish fluctuating voltage, but fluctuations within a safe
range are acceptable, which can easily lead to voltage waste. L2-
Shape is the opposite, which can easily lead to exceeding the safety
threshold. Bowl-Shape has made a compromise by combining the
characteristics of both. We conduct the main experiments with
different voltage barrier functions. In most experiments, L1-Shape
can provide the best performance.
A.2 Training and Testing.
During training, each experiment is run with 5 random seeds. Each
experiment is evaluated on the validation dataset (1 day per month)
every 20 episodes and the evaluation results during training are
given by the 50% ci shading. For singular diurnal cycle testing, we
use the test dataset provided by [ 34] which picks 10 days per month
and has 120 days in total. For the longer testing cycles, we divide
the data for the entire 2014 year into continuous operation times of
one month and one year to test existing strategies. During testing,
we follow the previous work to choose the best voltage barrier
function for every method.
A.3 Implementation Details.
Following previous works [ 33,34], we train our model in a total
of 400 epochs. We also use the same optimizer, learning rate, and
update frequency to optimize our model. The batch size is 32 dur-
ing training and the hidden dimension is 64 for all layers [ 33,34].
Each transformer-based encoder has a 3-layer transformer [ 34]. All
experiments use the same hardware and software configuration.B ADDITIONAL EXPERIMENTS
B.1 Experiment Results on year-times cycles
Table 3: Test results on the IEEE 322-bus system under longer
year-times cycles. Bold denotes the best results.
Metho
d1-
Year 2-Year 3-Year
CR
QL CR
QL CR
QL
322-MADDPG 85.5
0.032 86.2 0.029 87.6
0.030
322-T-MADDPG 88.2
0.031 88.9
0.032 89.1
0.031
322-
TPA-MADDPG 92.2
0.030 93.9
0.029 94.3
0.029
As shown in Table 3, we also conduct experiments on the longer
year-times cycles. Extending the test cycle on year-times increases
the overall performance because the stable and easy periods of the
year are significantly longer than the unstable and extreme periods.
It can still illustrate the inadequacies of current AVC methods in
addressing long-term operational variability, while our TPA model
has achieved the best performance in all year-times cycle tests. The
advantage becomes more apparent with the increase in test cycles.
B.2 Same season divisions as TMAAC
Table 4: Test results with the same season division as TMAAC.
Bold denotes the best results. ±corresponds to the standard
deviation of testing episodes.
Metho
dsSpring
Summer Fall Winter All
CR
QL CR
QL CR
QL CR
QL CR
QL
322-MA
TD3 80.7±20.2 0.039 77.3±18.2 0.039 93.5±13.4 0.038 98.3±3.9
0.038 87.4
0.038
322-MADDPG 80.1±18.9 0.033 74.6±19.8 0.033 92.7±14.0 0.032 96.7±7.6
0.031 86.0
0.032
322-T-MATD3 90.8±17.5 0.028 88.8±20.4 0.032 97.4±8.1 0.029 97.2±8.6 0.028 93.5 0.029
322-
T-MADDPG 87.8±19.8 0.032 83.3±20.7 0.033 96.2±8.7 0.031 97.1±8.5
0.030 91.1
0.031
322-
TPA-MATD3 89.9±18.7 0.028 86.7±21.2 0.032 96.2±9.1 0.029 97.2±5.7
0.029 92.5
0.030
322-
TPA-MADDPG 92.7±17.1 0.028 91.1±20.3 0.030 98.4±5.7 0.029 99.0±3.3 0.028 95.3
0.029
We have conducted experiments on the same month divisions
for each season as TMAAC, as shown in Table 4. In our original
setting, we used different month divisions compared to TMAAC.
For example, we divide spring into 2, 3, and 4. While they divide
spring into 3, 4, and 5. After unifying the settings, their original
322-T-MATD3 achieved 90.6% in spring, whereas our 322-T-MATD3
achieved 93.5%. The new results are also more similar to the values
they reported in TMAAC [34].
B.3 Experiments on 33-bus network
We compare the performances of our TPA and other baselines under
the singular diurnal cycle on the 33-bus network. Test performances
are shown in Table 5. In smaller networks, the control rates (CR)
between all methods are maintained at a high level, and power gen-
eration loss (QL) becomes the key to evaluating model performance.
This trend is very similar to the trend on the 141-bus system. Our
TPA method still achieved the optimal CR and QL.
3608KDD ’24, August 25–29, 2024, Barcelona, Spain Feiyang Xu et al.
Table 5: Test results of algorithms in 33-bus system. Bold
denotes the best results. ±corresponds to the standard devia-
tion of testing episodes.
Metho
dsSpring
Summer Fall Winter Average
CR
QL CR
QL CR
QL CR
QL CR
QL
33-MA
TD3 99.5±0.02 0.53 98.6±0.05 0.49 99.9±0.01 0.53 100.0±0.0 0.59 99.5
0.54
33-MADDPG 99.0±0.04 0.36 97.8±0.08 0.32 99.9±0.01 0.36 99.9±0.01 0.41 99.2
0.36
33-T-MADDPG 98.9±0.04 0.41 97.6±0.09 0.37 99.6±0.01 0.41 100.0±0.0 0.48 99.0
0.42
33-T-MATD3 99.9±0.01 0.34 99.9±0.01 0.35 100.0±0.0 0.34 99.9±0.01 0.35 99.9 0.35
33-
TPA-MATD3 99.5±0.02 0.25 98.9±0.03 0.26 99.9±0.01 0.25 100.0±0.0 0.25 99.6 0.25
33-
TPA-MADDPG 99.9±0.01 0.26 99.9±0.01 0.26 100.0±0.0 0.26 100.0±0.0 0.26 99.9 0.26
Table 6: Longer trajectories under 33-bus network.
T
rajectory Length half-day(240) one day(480) one month(14400) one year(175200)
Get
one trajectory 0.89s 1.53s 2.05s 3.42s
Train one epoch 24.5s 55.4s 1170.9s 16534.8s
B.4 Resources Consumption
We conducted experiments on a basic MADDPG under the 33-bus
network to investigate the higher computational resources brought
by longer trajectories.
As shown in Table 6, we extend the training trajectory of a half
day to one day, one month and even one year. Previous works and
our TPA set 240 timesteps as the default training segment, which is
appropriate for the total data capacity of three years and fast model
interaction time. The extension of trajectory length has minimal
impact on getting trajectories, but it has very serious requirements
for model training resources. If we want to incorporate seasonal
climates into training segments, the training time will increase
by thousands of times. Therefore, simply lengthening the training
trajectory is extremely wasteful of resources. It is also should be
noted that our TPA method and other methods did not utilize long
trajectories for training in our original manuscript.B.5 Randomly Initialized Prototypes
Origin T P A
 Random T P A
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000026/uni00000035/uni00000003/uni0000000b/uni00000008/uni0000000c
(
a) CR-322
/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013
/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056/uni00000003/uni0000000b×102/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000013/uni00000017/uni00000011/uni00000013/uni00000019/uni00000011/uni00000013/uni0000001b/uni00000011/uni00000013/uni00000034/uni0000002f/uni00000003/uni0000000b×102/uni0000000c
 (
b) QL-322
Figure 8: Training curves of TPA and randomly initialized
prototypes on the IEEE 322-bus system.
Table 7: Test results of TPA and randomly initialized proto-
types on the 322-bus system. Bold denotes the best results. ±
corresponds to the standard deviation of testing episodes.
Metho
dsSpring
Summer Fall Winter All
CR
QL CR
QL CR
QL CR
QL CR
QL
322-MADDPG 80.1±18.9 0.033 74.6±19.8 0.033 92.7±14.0 0.032 96.7±7.6 0.031 86.0
0.032
322-T-MADDPG 87.8±19.8 0.032 83.3±20.7 0.033 96.2±8.7 0.031 97.1±8.5 0.030 91.1
0.031
322-
TPA-Random-MADDPG 89.5±19.3 0.030 85.6±20.2 0.030 97.1±6.8 0.029 96.4±10.3 0.029 92.1
0.030
322-
TPA-MADDPG 92.7±17.1 0.028 91.1±20.3 0.030 98.4±5.7 0.029 99.0±3.3 0.029 95.3
0.029
In the original experiments, we directly adopted the load feature
of buses to initialize the prototype. To study the adaptability and
robustness of the prototypes, we have additionally conducted exper-
iments to verify the performance of randomly initialized prototypes.
As shown in Figure 8 and Table 7. During the training phase, the
initial fitting of the random prototype clearly requires more epochs.
Although the final performance of Random TPA slightly drops, it
still obtains better results than baselines. PPA with random proto-
types can still learn long-term temporal features without the strong
requirements of manual design. In future work, we can also adopt
the concept of codebook learning to realize more effective learnable
prototypes. We can apply factorized and normalized codes to ensure
codebook quality with a larger prototype space.
3609