Balanced Confidence Calibration for Graph Neural Networks
Hao Yang
yanghao@nudt.edu.cn
National University of Defense
Technology
ChangSha, ChinaMin Wang
wangminwm@nudt.edu.cn
National University of Defense
Technology
ChangSha, ChinaQi Wang
18274848940@163.com
National University of Defense
Technology
ChangSha, China
Mingrui Lao
laomingrui@vip.sina.cn
National University of Defense
Technology
ChangSha, ChinaYun Zhou∗
zhouyun@nudt.edu.cn
National University of Defense
Technology
ChangSha, China
ABSTRACT
This paper delves into the confidence calibration in prediction when
using Graph Neural Networks (GNNs), which has emerged as a
notable challenge in the field. Despite their remarkable capabilities
in processing graph-structured data, GNNs are prone to exhibit
lower confidence in their predictions than what the actual accuracy
warrants. Recent advances attempt to address this by minimizing
prediction entropy to enhance confidence levels. However, this
method inadvertently risks leading to over-confidence in model
predictions. Our investigation in this work reveals that most exist-
ing GNN calibration methods predominantly focus on the highest
logit, thereby neglecting the entire spectrum of prediction prob-
abilities. To alleviate this limitation, we introduce a novel frame-
work called Balanced Calibrated Graph Neural Network (BCGNN),
specifically designed to establish a balanced calibration between
over-confidence and under-confidence in GNNs’ prediction. To the-
oretically support our proposed method, we further demonstrate
the mechanism of the BCGNN framework in effective confidence
calibration and significant trustworthiness improvement in predic-
tion. We conduct extensive experiments to examine the developed
framework. The empirical results show our method’s superior per-
formance in predictive confidence and trustworthiness, affirming
its practical applicability and effectiveness in real-world scenarios.
CCS CONCEPTS
•Computing methodologies →Neural networks .
KEYWORDS
Graph Neural Networks, Model Calibration, Model Robustness,
Trustworthy Decision
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671741ACM Reference Format:
Hao Yang, Min Wang, Qi Wang, Mingrui Lao, and Yun Zhou. 2024. Balanced
Confidence Calibration for Graph Neural Networks. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671741
1 INTRODUCTION
Deep neural networks excel in processing images, videos, texts,
and voices [ 25]. However, multiple-layer perceptrons encounter
challenges in representation when it comes to relational datasets
like social and financial networks [ 21,27,42]. To this end, Graph
Neural Networks (GNNs) have emerged as an effective solution for
handling these intricate, interconnected networks. GNNs specialize
in managing the web of relationships in non-Euclidean spaces,
making them suitable for tasks involving complex interrelated data
structures [19, 52].
Despite their advantages, recent studies have pointed out an
urgent issue with GNNs where their predictions tend to be un-
trustworthy [ 11,20,39,41,43,49]. Ideally, a model’s prediction
confidence should align with its actual accuracy for realistic imple-
mentation. However, recent works [ 20,41,43] reveal an undesired
behavior for GNNs to display lower prediction confidence than their
accuracy warrants, leading to an under-confidence issue, which
contrasts the over-confidence phenomena commonly observed in
DNNs [ 7,14,16,29,35,50]. This discrepancy hampers the practi-
cal use of GNNs, especially in the realistic scenarios emphasizing
safety and reliability [ 15,18,22,47]. For instance, in financial net-
works, such under-confidence could result in substantial monetary
losses, while in social media, it might lead to the spread of misin-
formation [ 9,12]. These concerns encourage researchers to explore
approaches to improve the accuracy and maintain the reliability of
GNN predictions [2, 28].
To enhance the reliability of GNNs in safety-critical domains,
confidence calibration [ 41,43,51] has become a prominent research
topic to minimize the discrepancy between confidence levels and
prediction accuracy. Many methods are proposed to understand
the causes of under-confidence in GNNs and find ways to improve
it[20,41,43,45]. One main focus has been on the role of the loss
function, as it has been shown to have a strong influence on calibra-
tion. A recent competitive method in [ 41] seeks to reduce prediction
3747
KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yang, Min Wang, Qi Wang, Mingrui Lao, & Yun Zhou
entropy in GNNs for increasing confidence levels, thereby allevi-
ating the issue of GNNs’ underconfidence. However, it could also
suffer from over-confidence since it penalizes all the output proba-
bility logits to a high level [ 7]. Therefore, it is necessary to achieve
a balanced algorithm design without overconfidence and undercon-
fidence issues so as to ensure their reliability and effectiveness in
decision-making processes.
Most existing calibration methods are dominated by the highest
prediction value, which potentially causes GNNS to neglect the
global predictive distribution. For instance, in a three-class classifi-
cation problem, a trained GNN might produce two prediction sets
[0.5,0.49,0.01]and[0.5,0.25,0.25]and treat them identically due
to the same highest value of 0.5. This method overlooks the differ-
ences in confidence and uncertainty between the two scenarios. By
focusing solely on the highest value, GNNs miss the subtleties in
the overall probability distribution, potentially leading to inaccu-
rate predictions. Hence, it is necessary to develop a more subtle
approach that considers the entire range of predictive distributions.
To this end, we introduce a novel balanced calibrated GNN
(BCGNN) framework, where we consider not only the largest logit
but also the subsequent one, aiming to achieve a better balance
between over-confidence and under-confidence. To theoretically
verify the rationality of our approach, we detail the mechanism of
the BC-GNN framework in effectively confidence calibration and
significant trustworthiness improvement in prediction.
In this work, our primary contribution is two-fold:
•Our proposed balanced calibrated loss function is designed
to account for both the highest prediction logit and the sub-
sequent one, offering a more integrated view of the GNNs’
predictive capabilities.
•We provide a theoretical foundation to explain the effective-
ness of our approach in achieving trustworthy calibration
of GNNs, striking a balance between over-confidence and
under-confidence.
The results of the comprehensive experiments demonstrate that
BC-GNN achieves superior calibration performance and signifi-
cantly reduces the Expected Calibration Error (ECE) compared to
traditional methods. These findings highlight the practicality and
robustness of our approach in real-world applications.
2 RELATED WORK
2.1 Graph Neural Networks
GNNs have demonstrated powerful capabilities in a wide range of
applications [ 4,26,42,48]. Based on their core principles of repre-
sentation, GNNs are primarily categorized into two types: spectral-
domain GNNs and spatial-domain GNNs [ 33,46,53]. The concept
of spectral domain convolution in GNNs, which originated with
Bruna et al. [ 6], is grounded in graph spectral analysis. However,
these initial spectral methods encountered significant challenges,
primarily due to their requirement to process the entire graph,
leading to high computational and memory demands.
Recent advances in addressing these limitations include Cheb-
Net [ 10] and GCN [ 23]. These methods optimized the spectral do-
main convolution kernel through parameterization, significantly
reducing both computational time and memory requirements. In-
terestingly, these methods, though classified as spectral, began toadopt interpretations akin to spatial domain approaches, paving
the way for the rise of spatial domain GNNs.
Spatial domain GNNs take a more straightforward approach to
graph convolution. They focus on aggregating information from
the immediate neighbors of a node. This shift led to the creation
of several innovative GNN architectures. For example, the Graph
Attention Network (GAT) [ 38] integrates attention mechanisms in
the node domain, thereby refining the model’s focus and improving
the efficiency of information aggregation.
Simultaneously, models like GraphSAGE [ 17] and FastGCN [ 8]
brought significant advancements in GNN training. These models
deviate from traditional full-graph training methods by focusing
computations on batches of nodes. This approach eliminates the
necessity of processing the entire graph in each training step, sig-
nificantly speeding up the training process.
These developments across spectral and spatial domain GNNs
have shown remarkable performance improvements in various
graph-related tasks. Their evolution underscores both approaches’
significant impact and potential in pushing the boundaries of GNN
research and applications.
2.2 Model Calibration
The purpose of model calibration is to align the predictive proba-
bilities of a model with actual real-world probabilities [ 1,13,32,35,
40,44,45,54]. Initially, calibration techniques like Platt scaling [ 16]
were introduced for binary classification models. This method ad-
justs raw model outputs into genuine class probability distributions
using learned parameters optimized on a validation dataset.
Built on this, Temperature Scaling (TS) [ 16] evolved from Platt
scaling to cater to multi-class scenarios. It modifies the model’s
output distribution using a calibration coefficient, fine-tuned on a
validation set. This helps to bring the model’s predicted probabilities
closer to actual probabilities.
As the field progressed, more advanced methods were devel-
oped. Zhang et al. [ 50] combined parametric and non-parametric
approaches in mixed-calibration. Rahimi et al. [ 31] explored using
neural networks for learning post-hoc calibration parameters. In
GNNs, specific challenges like miscalibration and underconfidence
have been identified. Teixeira et al. [ 36] highlighted miscalibration
issues in supervised GNNs. In contrast, Wang et al. [ 43] examined
underconfidence in semi-supervised GNN settings, suggesting us-
ing GNNs for determining calibration coefficients in temperature
scaling. Similarly, Hsu et al. [ 20] proposed the use of GATs to more
effectively learn the calibration coefficient.
However, a gap still remains in applying these methods directly
to GNNs, particularly in end-to-end calibration. Traditional meth-
ods, generally involving two stages, require separate training of
calibration coefficients for different tasks. This does not inherently
result in a reliable GNN model.
To address this, Wang et al. [ 41] proposed an end-to-end trust-
worthy framework for GNNs, aiming to minimize prediction en-
tropy to increase confidence. But, this led to potential overcon-
fidence. In our study, we introduce a balanced calibrated GNN
framework to balance overconfidence and underconfidence, thus
enhancing the reliability of GNN decisions. Our approach seeks
3748Balanced Confidence Calibration for Graph Neural Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
to build inherently more reliable predictive models for networked
data, establishing a foundation for trust in GNN-based decisions.
3 PRELIMINARIES OF GNNS CALIBRATION
3.1 Math Notations
In the context of a 𝑪-class node classification task, the graph under
consideration is represented as G=(V,E). Here,Vdenotes the
set of nodes, andEsignifies the edges between these nodes, with
the total number of nodes being |V|=𝑁. The adjacency matrix
𝑨, indicating the connections between nodes, is expressed in the
dimension of R𝑁×𝑁. The training dataset for this task is denoted
asDand can be formulated as D(X,Y)=n
𝑥(𝑖),𝑦(𝑖)o𝑁
𝑖=1. In
this representation, 𝑥(𝑖)∈X corresponds to the 𝑖𝑡ℎnode, while
𝑦(𝑖)∈Y⊆ R𝐶represents the true label of the 𝑖𝑡ℎnode, encoded
as unique one-hot vectors.
We characterize a GNN classifier as a function 𝑓𝜃:X → R𝐶,
which encodes the input data Xinto𝑪distinct classes. We denote
the model parameter by 𝜃∈Θ. The post-classification, the logical
output for the 𝑖𝑡ℎnode,𝑥(𝑖), is given by 𝐿(𝑖)=𝑓𝜃(𝑥(𝑖)), where
𝐿=(𝑙𝑐)1≤𝑐≤𝐶encompasses the logits for each class. The predicted
probability𝑝(𝑖)for each node is computed using the softmax func-
tion, as delineated in the following equation:
𝑝(𝑖)= 
𝑒𝑖𝑐
Í𝐶
𝑗=1𝑒𝑖𝑗!(𝑖)
1≤𝑐≤𝐶(1)
Such a formulation normalizes the logits to yield a probability
distribution across the 𝑪classes for each node, facilitating the
determination of the most likely class for each node in the graph.
Typically, in a classification task, the prediction confidence of
a model is determined by the largest prediction logit. This can be
represented as ˆ𝑝=max𝑘𝑒𝑖𝑘
Σ𝐾
𝑗=1𝑒𝑖𝑗. Consequently, the predicted label
ˆ𝑦can be defined as the class with the highest logit value, formulated
asˆ𝑦=argmax𝑘𝑒𝑖𝑘Í𝐾
𝑗=1𝑒𝑖𝑗.
3.2 Challenges in GNNs’ Calibration
Ideally, a well-calibrated model is expected to exhibit a consistent
relationship between its prediction confidence and accuracy. This
suggests the model’s outputs should accurately reflect the true
class posterior probabilities. Mathematically, this consistency can
be expressed as follows:
P
ˆ𝑦(𝑡)=𝑦(𝑡)|ˆ𝑝(𝑡)=𝑝
=𝑝,∀𝑝∈[0,1] (2)
Here,𝑝(𝑖)represents the predicted probability for node 𝑖, and𝑝
denotes the actual probability of correct classification.
However, accurately capturing the actual class probability is
challenging since it is a continuous rather than a discrete value.
While it is impossible to ascertain the exact probability, it can be
estimated. Guo et al. [ 16] proposed a method for this estimation,
which involves dividing the prediction confidence into 𝑀uniform
intervals. Nodes with different confidence levels are then allocated
to their corresponding intervals, allowing for the computation of
the average classification accuracy for each interval.Under this method, we expect a well-calibrated model to exhibit
an equal average classification accuracy and confidence level across
all intervals. For example, if nodes in the confidence interval [0.8,
0.9] have an average confidence of 0.86, then the average accuracy
for these nodes should ideally also be 0.86. This approach, noted
for its simplicity and effectiveness, has become a widely accepted
metric for evaluating model calibration.
4 METHODOLOGY
4.1 Problem Statement
We study the graph node classification task in the paradigm of semi-
supervised learning, where 𝑥represents the data and 𝑦represents
the labels, the prediction logit is defined as 𝑞𝑖=𝑒𝑧𝑖Í𝐶
𝑐=1𝑒𝑧𝑐. Typically,
the cross-entropy loss is employed to optimize the predictions of
GNNs, which can be expressed as:
L𝑐𝑒(𝑓𝜃(𝑥𝑡),𝑦𝑡)=1
𝑁(−𝑝𝑖·log(𝑞𝑖)), (3)
where𝑝𝑖corresponds to the actual label of class 𝑖, usually in a
one-hot vector format, while 𝑞𝑖denotes the softmax-transformed
model output logits. These logits fall within the range [0,1]and
sum up to 1.
Recent studies have shown that the predictions of GNNs typically
exhibit a phenomenon of prediction with under confidence. Wang
et al.[ 41] propose to utilize entropy minimization to minimize the
prediction uncertainty while increasing the prediction confidence,
which can be formulated as:
L𝐺𝐶𝐿(𝑓𝜃(𝑥𝑡),𝑦𝑡)=1
𝑁𝐶∑︁
𝑖=1(−(1+𝛾𝑞𝑖)𝑝𝑖log(𝑞𝑖)), (4)
where𝛾is the pre-defined hyperparameter to control the strength of
entropy minimization, and 𝛾=0is equivalent to the cross-entropy
loss.
However, there exists a lasting concern about adopting GCL loss
in calibration. The GCL loss could suffer from over-confidence since
it penalizes all the output probability logits to a high level[ 7], and
predictions are optimized to form a tight probability distribution.
Although the conservative predictions guided by the GCL loss have
a smaller ECE, the higher confidence scores may not accurately
reflect the actual probability. Thus, a trade-off exists between over-
confidence and under-confidence in the loss function design for
calibration.
4.2 Proposed Method
Distinguished from previous work [ 41], our research motivation is
todevelop the regularization to encourage the networks to generate
confident yet accurate confidence scores while retaining the benefits
of entropy minimization in optimization.
Here, we identify a crucial limitation in previous works: they
mainly consider only the largest network prediction. For instance,
Wang et al.[ 41] focus on making the largest logit close to 1. However,
this approach does not account for the relationship between the
largest prediction and other logits 𝑞𝑖,𝑖≠𝑙𝑎𝑟𝑔𝑒𝑠𝑡 , which is crucial
for addressing over/under-confidence issues in the loss function.
Let us consider a three-classification scenario where the true
class probability is fixed at 0.5. The loss function dominated by the
3749KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yang, Min Wang, Qi Wang, Mingrui Lao, & Yun Zhou
largest prediction will yield the same loss value regardless of how
the remaining confidence is distributed among the other logits. For
example, the network will pay equal attention to the prediction
distributions[0.5,0.49,0.01]and[0.5,0.25,0.25]and treat them
equally because they have the same maximum value of 0.5.
Notably, the prediction distribution can be seen as an implicit
regularization to adjust the balance between overconfidence and
underconfidence. For example, the tight prediction distribution is
highly relevant to high prediction confidence, and the loose pre-
diction distribution is highly relevant to low prediction confidence.
However, since it is difficult to introduce the entire prediction distri-
bution, it is promising to use the dual logits as a proxy for balanced
confidence calculation. Thus, we argue that relying solely on the
largest logit is insufficient for the loss function, especially for balanced
confidence estimation.
In this paper, we propose to reformulate Eq.4 to consider both
the largest and the second largest prediction. For simplicity, we
denote the largest network prediction 𝑞𝑖in Eq.4 as𝑞𝑎. Besides, we
introduce the second largest logit in the prediction distribution,
which we denote as 𝑞𝑏. Formally, our confidence-calibration loss is
represented as follows:
L𝐵𝐶𝐺𝑁𝑁(𝑓𝜃(𝑥𝑡),𝑦𝑡)=1
𝑁𝐶∑︁
𝑖=1(−(1+𝛾(𝑞𝑎−𝑞𝑏))log(𝑞𝑖)),
where𝑞𝑏(𝑥)=max
𝑖{𝑞𝑖(𝑥)|𝑞𝑖(𝑥)<𝑞𝑎(𝑥)}.(5)
In Eq. 5,𝑞𝑎(𝑥)is the highest prediction logit, 𝑞𝑏(𝑥)is the highest
value after𝑞𝑎(𝑥)in descending order of predictions 𝑞1:𝐶(𝑥),𝛾is
the hyperparameter to control the regularization strength. This
formulation inherits the structure of entropy minimization, and re-
tains the benefits of entropy minimization in confidence calibration.
Besides, it also aims to minimize the gap between 𝑞𝑎and𝑞𝑏, so that
the network will not excessive over-confidence. We also show theo-
retically that the proposed method can reduce the over-confidence
region compared to GCL loss in Section 4.3.
4.3 Theoretical Analysis
This section provides more theoretical insights into the effective-
ness of our proposed balanced calibrated loss function in enhancing
the trustworthiness of GNNs predictions.
4.3.1 Sample-wise Conditional Risk. Our approach is rooted in
the principles established in prior works [ 3,37], which focus on
convexity and consistency in the presence of risk minimization.
We define the sample-wise conditional risk Rfor BCGNN as
follows:
R𝐵𝐶𝐺𝑁𝑁 =1
𝑁𝐶∑︁
𝑖=1𝜂𝑖(𝑥)(−(1+𝛾(𝑞𝑎−𝑞𝑏))log(𝑞𝑖)) (6)
In this equation, 𝑥signifies an individual data point, while 𝜂rep-
resents the actual class-posterior probability, correlating to the
calibrated confidence score from the classifier. Therefore, Rex-
presses the expected penalty assigned to a data point 𝑥when𝑞is
the scoring function. By examining the instance-wise conditional
riskR, we aim to uncover the interplay between the scoring func-
tion𝑞and the true class-posterior probability 𝜂. This relationshipis pivotal in understanding the impact of our loss function on the
calibration of classification.
Previous studies [ 7] have extensively analyzed the sample-wise
conditional risks, particularly focusing on focal losses. Our study
extends these discussions to the BCGNN loss function, elucidating
its theoretical advantages. We aim to demonstrate that BCGNN not
only aligns with the principles of classification calibration but also
surpasses existing methods in terms of effectiveness and reliabil-
ity, particularly in the domain of GNNs. This analysis underpins
our assertion that the proposed BCGNN loss function is a substan-
tial improvement in the field of GNN calibration, ensuring more
trustworthy predictions.
The optimization of R𝐵𝐶𝐺𝑁𝑁 can be represented as
min𝑞1
𝑁𝐶∑︁
𝑖=1𝜂𝑖(−(1+𝛾(𝑞𝑎−𝑓(𝑞𝑎)))log(𝑞𝑖)),
subject to𝐶∑︁
𝑖=1𝑞𝑖=1,(7)
where we represent 𝑞𝑏as𝑓(𝑞𝑎).
To resolve the constrained optimization problem in Eq. 7, we
can consider the Lagrangian form as:
L(𝑞,𝜆)=1
𝑁𝐶∑︁
𝑖=1𝜂𝑖(−[1+𝛾(𝑞𝑖−𝑓(𝑞𝑖))]log(𝑞𝑖))+𝜆 𝐶∑︁
𝑖=1𝑞𝑖−1!
,
(8)
where𝜆is the Lagrange multiplier for the constraint. Through
taking the derivatives with respect to 𝑞𝑖,𝜆can be solved at the
optimal𝑞∗as
𝜕
𝜕𝑞𝑖L(𝑞,𝜆)
𝑞=𝑞∗=0,
−
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝜂𝑖𝛾log𝑞∗
𝑖−𝜂𝑖 1+𝛾 𝑞∗
𝑖−𝑓 𝑞∗
𝑖
𝑞∗
𝑖+𝜆=0,
𝜆=𝜂𝑖 
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾log𝑞∗
𝑖+ 1+𝛾 𝑞∗
𝑖−𝑓 𝑞∗
𝑖
𝑞∗
𝑖!
.
𝜂𝑖=𝜆
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾log𝑞∗
𝑖+(1+𝛾(𝑞∗
𝑖−𝑓(𝑞∗
𝑖)))
𝑞∗
𝑖(9)
Then we rewrite 𝜂𝑖with a function of 𝜆and𝑞∗
𝑖with Eq. 9. Since 𝜂𝑖
follows the probability distribution, 𝜆can be rewritten as a function
of𝑞∗
𝑖as
𝐶∑︁
𝑖𝜂𝑖=1=𝐶∑︁
𝑖𝜆
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾log𝑞∗
𝑖+(1+𝛾(𝑞∗
𝑖−𝑓(𝑞∗
𝑖)))
𝑞∗
𝑖,
𝜆=𝐶∑︁
𝑖 
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾log𝑞∗
𝑖+ 1+𝛾 𝑞∗
𝑖−𝑓 𝑞∗
𝑖
𝑞∗
𝑖!
.(10)
By replacing the above 𝜆in Eq. 9, we can write 𝜂𝑖as a function
of𝑞∗
𝑖as
𝜂𝑖=𝑞∗
𝑖
𝜙(𝑞∗
𝑖)
Í𝐶𝑐𝑞∗
𝑖
𝜙(𝑞∗
𝑖)(11)
3750Balanced Confidence Calibration for Graph Neural Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
where𝜙 𝑞∗
𝑖=
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾𝑞∗
𝑖log𝑞∗
𝑖+ 1+𝛾 𝑞∗
𝑖−𝑓 𝑞∗
𝑖
. As a
result, given 𝑞∗
𝑖, one can recover the true class-posterior probability
𝜂𝑖, by using the transformation in Eq. 11.
4.3.2 Over-confidence and Under-confidence. Next, we discuss the
over-confidence and under-confidence scenarios of the risk mini-
mizer𝑞∗. First, we define the 𝜂-over-confidence ( 𝜂OC) and𝜂-under-
confidence ( 𝜂UC) at data 𝑥
max
𝑖𝑞∗
𝑖(𝑥)−max
𝑖𝜂𝑖(𝑥)>0,𝑞∗
𝑖is𝜂OC,
max
𝑖𝑞∗
𝑖(𝑥)−max
𝑖𝜂𝑖(𝑥)<0,𝑞∗
𝑖is𝜂UC.(12)
Next, we show the proposed loss function is classification calibrated
[3, 37] compared to entropy minimization.
Theorem 4.1. For any 0<𝛾<1, the proposed loss function is
classification-calibrated and has the strictly order-preserving property.
The detailed proof of Theorem 4.1 is provided in Appendix A.
Theorem 1 indicates that the risk minimizer preserves the order
of true class-posterior probability as 𝑞∗𝑎<𝑞∗
𝑏⇒𝜂𝑎<𝜂𝑏. In other
words, the max-index of 𝑞∗and𝜂are identical. Together with Eq. 24,
we can reformulate 𝜂𝑈𝐶 in Eq. 12 as
𝑞∗
𝑚(𝑥)−𝜂𝑚(𝑥)<0,
𝐶∑︁
𝑖𝑞∗
𝑖(𝑥)
𝜙
𝑞∗
𝑖(𝑥)<1
𝜙(𝑞∗𝑚(𝑥)(13)
where𝑚=𝑎𝑟𝑔𝑚𝑎𝑥𝑖𝑞∗
𝑖(𝑥). Eq. 13 holds if 𝜙(𝑞∗𝑚(𝑥))<𝜙(𝑞∗
𝑖(𝑥))
for all𝑖∈[1,𝐶]and the scenario of 𝜂𝑂𝐶 can be explored through
flipping the sign of above inequalities. Thus, the monotone of the
function𝜙plays an important role in 𝜂𝑂𝐶/𝜂𝑈𝐶 , we then explore
the properties of 𝜙with the proposed loss function. In the proposed
loss function, the function 𝑓(𝑣𝑖)maps𝑣𝑗to the second maximum
value𝑣𝑘in the descending order 𝑣1:𝐶after𝑣𝑗, which is defined as
𝑓(𝑣𝑖)=𝑣𝑘,
where𝑣𝑘=max
𝑖
𝑣𝑖|𝑣𝑖<𝑣𝑗	 (14)
We denote𝑣𝑘=𝐶for simplicity. To explore the influence of intro-
duced𝑓(𝑣𝑖)in𝜂𝑂𝐶/𝜂𝑈𝐶 . We then examine the function’s proper-
ties𝜙in the following Proposition.
Proposition 4.2. For the scenario where 𝑗≠𝑘,𝜙(0)=1−𝛾𝐶,
and𝜙(1)=1+𝛾(1−𝐶), there exists v 𝑚=1e-2, for𝑣𝑖∈(0,1𝑒−2),
𝜕𝜙
𝜕𝑣𝑖<0, and for𝑣𝑖∈(1𝑒−2,1),𝜕𝜙
𝜕𝑣𝑖>0.
The detailed proof of Proposition 4.2 is provided in the Appendix
A. Proposition 4.2 shows that 𝜙first decrease in[0,1𝑒−2), and
then increase in(1𝑒−2,1], we have known 𝜙(0)<1and𝜙(1)>1,
so there exists a 𝑣𝑢in[1𝑒−2,1], that𝜙(𝑣𝑢)=𝜙(0), compared with
GCL (entropy minimization), with the assumption of the same 𝛾
in proposed loss function and GCL, the value of 𝑣𝑢to 0 is closer
than proposed loss function, it is easy to show that the regions of
over-confidence is reduced in our proposed loss function compared
to GCL. Thus, the proposed loss function can alleviate the over-
confidence problem of entropy minimization; we empirically verify
the proposed method achieves a better trade-off in the experimental
section.5 EXPERIMENT
5.1 Datasets
In our study, we employ four widely-adopted datasets extensively
used in GNN research: Cora [ 34], Citeseer [ 34], Pubmed [ 34], and
CoraFull [ 5]. These datasets provide a varied range of complexities
and backgrounds, tailored to a comprehensive analysis of GNNs.
Cora consists of 2708 scientific papers from the machine learning
domain, classified into seven distinct classes. Each paper in the net-
work is a node characterized by a 1433-dimensional feature vector
representing word associations. Citeseer includes 3327 scholarly
articles, each described by a more extensive 3703-dimensional fea-
ture vector. Pubmed includes 19,717 scientific publications from the
PubMed database, interconnected by 44,338 links, each represented
by a 500-word vector. CoraFull expands upon the original Cora
dataset, offering a larger and more challenging network for testing
GNN methodologies. Table 1 provides an in-depth overview of each
dataset, including their structural and thematic characteristics.
Table 1: Characteristics of the Datasets.
Dataset
Cora Citeseer Pubmed CoraFull
No
des 2708 3327 19717 19793
Edges 5429 4732 44338 126842
Features 1433 3703 500 8710
Classes 7 6 3 70
Training 140/280/420 120/240/360 60/120/180 1400/2800/4200
Validation 500 500 500 500
Test 1000 1000 1000 1000
5.2 Comparison Methods and Experimental
Setup
Our study focuses on the classical semi-supervised node classifica-
tion task in graph data. To evaluate model calibration, we consider
three widely-used GNN architectures: GCN[ 23], GAT [ 38], and
GraphSAGE[17].
GCN adapts convolutional operations from traditional Euclidean
data to graph data. The core idea is to create a new node represen-
tation by aggregating its own features with those of its neighbors
through a learned mapping function. GAT introduces an atten-
tion mechanism into GNNs, which, during the aggregation process,
combines outputs from multiple models to generate key-targeted
random walks. GraphSAGE extends GCN to inductive learning
by training a function to aggregate neighbors’ features, enabling
generalization to unseen nodes.
Our research compares several post-hoc calibration methods
commonly applied to neural networks, including Temperature Scal-
ing and Matrix Scaling (MS) [ 24]. We also compare CaGCN[ 43],
the first calibration method designed specifically for GNNs, and
GCL[ 41], the first end-to-end calibration method for GNNs. For a
fair comparison, results for GCL are taken directly from the original
publication[41, 43].
In our experiments, we use a standard two-layer configuration
for GCN, GAT, and GraphSAGE, with hidden layer dimensions
set to 64, 8, and 64, respectively. All experiments are performed
3751KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yang, Min Wang, Qi Wang, Mingrui Lao, & Yun Zhou
Table 2: Expected Calibration Error (ECE) of GCN using different calibration methods across various datasets and labeling rates
(20, 40, 60). Bold indicates the best calibration result for that scenario.
Dataset L/CGCN
Uncal. TS MS CaGCN GCL Proposed
Cora20 0.1347 0.0488 0.0414 0.0401 0.0394 0.0386
40 0.1134 0.0417 0.0372 0.0407 0.0371 0.0302
60 0.0937 0.0386 0.0364 0.0376 0.0353 0.0299
Citeseer20 0.1248 0.0641 0.0644 0.0595 0.0579 0.0522
40 0.0957 0.0601 0.0538 0.0545 0.0575 0.0474
60 0.0806 0.0559 0.0521 0.0546 0.0501 0.0473
Pubmed20 0.0586 0.0541 0.0476 0.0405 0.0394 0.0328
40 0.0444 0.0446 0.0436 0.0402 0.0395 0.0337
60 0.0445 0.0367 0.0318 0.0311 0.0310 0.0310
CoraFull20 0.1986 0.1013 - 0.0776 0.0846 0.0749
40 0.2321 0.1117 - 0.0701 0.0743 0.0631
60 0.2337 0.0981 - 0.0768 0.0754 0.0674
Table 3: Expected Calibration Error (ECE) of GAT using different calibration methods across various datasets and labeling rates
(20, 40, 60). Bold indicates the best calibration result for that scenario.
Dataset L/CGAT
Uncal. TS MS CaGCN GCL Proposed
Cora20 0.1558 0.0717 0.0544 0.0450 0.0444 0.0425
40 0.1340 0.0485 0.0491 0.0365 0.0356 0.0277
60 0.1201 0.0393 0.0411 0.0313 0.0258 0.0200
Citeseer20 0.1534 0.0916 0.0633 0.0572 0.0660 0.0571
40 0.1252 0.0797 0.0590 0.0532 0.0603 0.0532
60 0.1090 0.0648 0.0519 0.0525 0.0522 0.0471
Pubmed20 0.0835 0.0656 0.0501 0.0356 0.0417 0.0291
40 0.0869 0.0658 0.0539 0.0308 0.0309 0.0306
60 0.0993 0.0669 0.0483 0.0308 0.0304 0.0302
CoraFull20 0.2119 0.1101 - 0.0788 0.0882 0.0779
40 0.2438 0.1133 - 0.0738 0.0765 0.0729
60 0.2497 0.1133 - 0.0849 0.0842 0.0832
following the official code [ 43], and results are averaged over ten
runs, we report the average results and std in experiments.
5.3 Evaluation Metrics
For evaluating the calibration performance of our models, we em-
ploy the widely recognized Expected Calibration Error (ECE) as
defined by Naeini et al.[ 30]. The ECE metric is formulated as fol-
lows:
𝐸𝐶𝐸=𝑀∑︁
𝑚=1|𝐵𝑚|
𝑁|𝑎𝑐𝑐(𝐵𝑚)−conf(𝐵𝑚)| (15)
The computation of ECE is conducted in several steps. Initially,
the confidence level is segmented into 𝑀equal parts, with a typical
value of𝑀being 20. Subsequently, the samples from the model’s
output are distributed into these intervals based on their confidence
levels. The term|𝐵𝑚|represents the count of samples within the 𝑚-
th interval. The primary operation involves calculating the absolute
difference between two key metrics: the average confidence level ofthe samples in each interval, denoted as conf(𝐵𝑚), and the average
accuracy of these samples, represented as 𝑎𝑐𝑐(𝐵𝑚). This difference
is then multiplied by the proportion of the number of samples in the
interval (|𝐵𝑚|) to the total number of samples ( 𝑁). The sum of these
values across all intervals quantifies the model’s expected calibra-
tion error. A lower ECE value indicates that a model’s confidence
is more aligned with its accuracy, suggesting better calibration.
5.4 Calibration Performance
5.4.1 Experimental Results. The experimental results, as shown
in Table 2, Table 3, and Table 4, illustrate the performance of our
proposed calibration method on three GNN models: GCN, GAT, and
GraphSAGE. We report the average results over ten runs in these
experiments, and the std value is presented in the supplementary
material.
From Table 2, it’s evident that the uncalibrated GCN model ex-
hibits a high ECE, underscoring the need for effective model calibra-
tion in GNNs. Our analysis also reveals that traditional calibration
3752Balanced Confidence Calibration for Graph Neural Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: Expected Calibration Error (ECE) of GraphSAGE using different calibration methods across various datasets and
labeling rates (20, 40, 60). Bold indicates the best calibration result for that scenario.
Dataset L/CGraphSAGE
Uncal. TS MS CaGCN GCL Proposed
Cora20 0.1037 0.0463 0.0371 0.0398 0.0382 0.0364
40 0.0847 0.0330 0.0418 0.0368 0.0363 0.0309
60 0.0803 0.0323 0.0332 0.0339 0.0310 0.0187
Citeseer20 0.11351 0.08081 0.0866 0.0691 0.0677 0.0654
40 0.09421 0.06571 0.0586 0.0544 0.0533 0.0502
60 0.0684 0.0506 0.0416 0.0497 0.0437 0.0368
Pubmed20 0.0338 0.0337 0.0342 0.0364 0.0348 0.0308
40 0.0310 0.0275 0.0327 0.0275 0.0302 0.0266
60 0.0280 0.0321 0.0315 0.0239 0.0241 0.0234
CoraFull20 0.1485 0.0617 - 0.0751 0.0758 0.0575
40 0.1881 0.0483 - 0.0553 0.0573 0.0471
60 0.1974 0.0509 - 0.0614 0.0552 0.0475
methods, typically used in non-GNN contexts, still hold consider-
able efficacy when applied to uncalibrated GNN models. However,
it’s important to note that the effectiveness of MS may be compro-
mised when dealing with datasets having a large number of classes,
as MS tends to overfit smaller validation sets due to its proportional
increase in parameters with class numbers.
Our proposed calibration method substantially lowers the ECE
values across various dataset label rates, indicating superior calibra-
tion performance compared to existing techniques. Notably, when
benchmarked against the first end-to-end GNN calibration method,
GCL, our method demonstrates enhanced calibration effectiveness
across different label rates in the datasets.
Similar conclusions can be drawn from the results in Table 3
and Table 4 for the GAT and GraphSAGE models. In their uncali-
brated state, these models also exhibit significant underconfidence
issues. The proposed calibration framework consistently outper-
forms current methods, including post-hoc calibration techniques
like CaGCN and end-to-end methods like GCL. Compared to GCL,
our method experimentally achieves a better balance between over-
confidence and under-confidence. This suggests that our approach
not only enhances the intrinsic trustworthiness of the model but
also sets a new benchmark in the realm of GNN calibration.
5.4.2 Visualization Analysis. To provide a clear and intuitive un-
derstanding of the calibration effects of our proposed method, we
present visualizations in the form of reliability histograms and con-
fidence distribution histograms for the GCN model, particularly
focusing on a label rate of 20%, across different datasets. These
visualizations are shown in Figure 1.
In Figure 1(Top), the x-axis segments the model’s confidence into
20 equal intervals, while the y-axis displays the average accuracy
for each interval. The grey area represents the expected output
values, and the blue area shows the actual output values from the
model. For the uncalibrated model, a noticeable trend is that the
average output confidence is typically lower than the expected
average, indicating a tendency of the model towards underconfi-
dence in its predictions. Our proposed method effectively mitigatesthis underconfidence issue, aligning the model’s confidence more
closely with its accuracy.
Figure 1(Bottom) offers a different perspective, where the x-axis
denotes the model’s confidence levels, and the y-axis represents the
density of predictions at each confidence level. By multiplying the
density of confidence in each interval by the confidence interval,
we obtain the frequency of samples in that range. The blue portion
signifies correct classifications by the model, while the grey portion
indicates misclassifications. This visualization reveals that, in the
case of the uncalibrated model, a significant number of correctly
predicted samples are clustered in the lower-confidence intervals.
Conversely, with the application of our method, most correctly pre-
dicted samples are appropriately situated in the higher-confidence
intervals.
Furthermore, Figure 1(Top) underscores an important observa-
tion: despite high classification accuracy, current graph network
models exhibit a substantial gap in trustworthiness. Our research
introduces a balanced calibrated end-to-end trustworthy graph neu-
ral network framework to enhance the credibility of these models.
This framework improves model calibration and advances the trust-
worthiness and reliability of GNNs in various application scenarios.
5.5 Further Analysis
To explore how BCGNN effect on model calibration, we trained a 2-
layer GraphSAGE on the Cora dataset with a label rate of 𝐿/𝐶=20,
employing either cross-entropy loss or BCGNN with 𝛾values set
to 0.003, 0.005, 0.007, 0.009, and 0.011. The results, illustrated in
Figure 2, particularly highlight the test Negative Log-Likelihood
(NLL) and test Expected Calibration Error (ECE) for models trained
with BCGNN, which are consistently lower than those trained with
cross-entropy loss. This finding significantly indicates that BCGNN
enhances calibration performance.
Moreover, we analyzed the NLL trends of both correctly and
incorrectly classified examples during testing, using different loss
functions. The data reveal that BCGNN effectively reduces the NLL
of correctly classified samples, thus increasing their confidence as
3753KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yang, Min Wang, Qi Wang, Mingrui Lao, & Yun Zhou
/uni00000011/uni0000000f/uni00000011 /uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000015/uni00000011/uni0000000f/uni00000017/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000f/uni00000011/uni00000022/uni00000044/uni00000044/uni00000056/uni00000053/uni00000042/uni00000044/uni0000005a/uni00000036/uni0000004f/uni00000044/uni00000042/uni0000004d/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni00000050/uni00000053/uni00000042/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000030/uni00000056/uni00000055/uni00000051/uni00000056/uni00000055/uni00000054
/uni00000026/uni00000059/uni00000051/uni00000046/uni00000044/uni00000055/uni00000046/uni00000045
/uni00000011/uni0000000f/uni00000011 /uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000015/uni00000011/uni0000000f/uni00000017/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000f/uni00000011/uni00000022/uni00000044/uni00000044/uni00000056/uni00000053/uni00000042/uni00000044/uni0000005a/uni00000036/uni0000004f/uni00000044/uni00000042/uni0000004d/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni0000004a/uni00000055/uni00000046/uni00000054/uni00000046/uni00000046/uni00000053/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000030/uni00000056/uni00000055/uni00000051/uni00000056/uni00000055/uni00000054
/uni00000026/uni00000059/uni00000051/uni00000046/uni00000044/uni00000055/uni00000046/uni00000045
/uni00000011/uni0000000f/uni00000011 /uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000015/uni00000011/uni0000000f/uni00000017/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000f/uni00000011/uni00000022/uni00000044/uni00000044/uni00000056/uni00000053/uni00000042/uni00000044/uni0000005a/uni00000036/uni0000004f/uni00000044/uni00000042/uni0000004d/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000031/uni00000056/uni00000043/uni0000004e/uni00000046/uni00000045/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000030/uni00000056/uni00000055/uni00000051/uni00000056/uni00000055/uni00000054
/uni00000026/uni00000059/uni00000051/uni00000046/uni00000044/uni00000055/uni00000046/uni00000045
/uni00000011/uni0000000f/uni00000011 /uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000015/uni00000011/uni0000000f/uni00000017/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000f/uni00000011/uni00000022/uni00000044/uni00000044/uni00000056/uni00000053/uni00000042/uni00000044/uni0000005a/uni00000036/uni0000004f/uni00000044/uni00000042/uni0000004d/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni00000050/uni00000053/uni00000042/uni00000027/uni00000056/uni0000004d/uni0000004d/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000030/uni00000056/uni00000055/uni00000051/uni00000056/uni00000055/uni00000054
/uni00000026/uni00000059/uni00000051/uni00000046/uni00000044/uni00000055/uni00000046/uni00000045
/uni00000011/uni0000000f/uni00000011 /uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000015/uni00000011/uni0000000f/uni00000017/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000f/uni00000011/uni00000022/uni00000044/uni00000044/uni00000056/uni00000053/uni00000042/uni00000044/uni0000005a/uni00000023/uni00000024/uni00000028/uni0000002f/uni0000002f/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni00000050/uni00000053/uni00000042/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000030/uni00000056/uni00000055/uni00000051/uni00000056/uni00000055/uni00000054
/uni00000026/uni00000059/uni00000051/uni00000046/uni00000044/uni00000055/uni00000046/uni00000045
/uni00000011/uni0000000f/uni00000011 /uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000015/uni00000011/uni0000000f/uni00000017/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000f/uni00000011/uni00000022/uni00000044/uni00000044/uni00000056/uni00000053/uni00000042/uni00000044/uni0000005a/uni00000023/uni00000024/uni00000028/uni0000002f/uni0000002f/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni0000004a/uni00000055/uni00000046/uni00000054/uni00000046/uni00000046/uni00000053/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000030/uni00000056/uni00000055/uni00000051/uni00000056/uni00000055/uni00000054
/uni00000026/uni00000059/uni00000051/uni00000046/uni00000044/uni00000055/uni00000046/uni00000045
/uni00000011/uni0000000f/uni00000011 /uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000015/uni00000011/uni0000000f/uni00000017/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000f/uni00000011/uni00000022/uni00000044/uni00000044/uni00000056/uni00000053/uni00000042/uni00000044/uni0000005a/uni00000023/uni00000024/uni00000028/uni0000002f/uni0000002f/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000031/uni00000056/uni00000043/uni0000004e/uni00000046/uni00000045/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000030/uni00000056/uni00000055/uni00000051/uni00000056/uni00000055/uni00000054
/uni00000026/uni00000059/uni00000051/uni00000046/uni00000044/uni00000055/uni00000046/uni00000045
/uni00000011/uni0000000f/uni00000011 /uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000015/uni00000011/uni0000000f/uni00000017/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000f/uni00000011/uni00000022/uni00000044/uni00000044/uni00000056/uni00000053/uni00000042/uni00000044/uni0000005a/uni00000023/uni00000024/uni00000028/uni0000002f/uni0000002f/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni00000050/uni00000053/uni00000042/uni00000027/uni00000056/uni0000004d/uni0000004d/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000030/uni00000056/uni00000055/uni00000051/uni00000056/uni00000055/uni00000054
/uni00000026/uni00000059/uni00000051/uni00000046/uni00000044/uni00000055/uni00000046/uni00000045
/uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni00000012/uni00000011/uni00000013/uni00000011/uni00000014/uni00000011/uni00000015/uni00000011/uni00000025/uni00000046/uni0000004f/uni00000054/uni0000004a/uni00000055/uni0000005a/uni00000036/uni0000004f/uni00000044/uni00000042/uni0000004d/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni00000050/uni00000053/uni00000042/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni0000002a/uni0000004f/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni00000016/uni00000012/uni00000011/uni00000012/uni00000016/uni00000013/uni00000011/uni00000013/uni00000016/uni00000025/uni00000046/uni0000004f/uni00000054/uni0000004a/uni00000055/uni0000005a/uni00000036/uni0000004f/uni00000044/uni00000042/uni0000004d/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni0000004a/uni00000055/uni00000046/uni00000054/uni00000046/uni00000046/uni00000053/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni0000002a/uni0000004f/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000016 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000018 /uni00000011/uni0000000f/uni00000019 /uni00000011/uni0000000f/uni0000001a /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni00000012/uni00000011/uni00000013/uni00000011/uni00000014/uni00000011/uni00000015/uni00000011/uni00000025/uni00000046/uni0000004f/uni00000054/uni0000004a/uni00000055/uni0000005a/uni00000036/uni0000004f/uni00000044/uni00000042/uni0000004d/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000031/uni00000056/uni00000043/uni0000004e/uni00000046/uni00000045/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni0000002a/uni0000004f/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni00000011/uni0000000f/uni00000011 /uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni00000016/uni00000012/uni00000011/uni00000012/uni00000016/uni00000013/uni00000011/uni00000025/uni00000046/uni0000004f/uni00000054/uni0000004a/uni00000055/uni0000005a/uni00000036/uni0000004f/uni00000044/uni00000042/uni0000004d/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni00000050/uni00000053/uni00000042/uni00000027/uni00000056/uni0000004d/uni0000004d/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni0000002a/uni0000004f/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni00000013/uni00000016/uni00000016/uni00000011/uni00000018/uni00000016/uni00000012/uni00000011/uni00000011/uni00000012/uni00000013/uni00000016/uni00000012/uni00000016/uni00000011/uni00000025/uni00000046/uni0000004f/uni00000054/uni0000004a/uni00000055/uni0000005a/uni00000023/uni00000024/uni00000028/uni0000002f/uni0000002f/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni00000050/uni00000053/uni00000042/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni0000002a/uni0000004f/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni00000016/uni00000012/uni00000011/uni00000012/uni00000016/uni00000013/uni00000011/uni00000013/uni00000016/uni00000014/uni00000011/uni00000025/uni00000046/uni0000004f/uni00000054/uni0000004a/uni00000055/uni0000005a/uni00000023/uni00000024/uni00000028/uni0000002f/uni0000002f/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni0000004a/uni00000055/uni00000046/uni00000054/uni00000046/uni00000046/uni00000053/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni0000002a/uni0000004f/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000016 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000018 /uni00000011/uni0000000f/uni00000019 /uni00000011/uni0000000f/uni0000001a /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni00000016/uni00000012/uni00000011/uni00000012/uni00000016/uni00000013/uni00000011/uni00000013/uni00000016/uni00000014/uni00000011/uni00000014/uni00000016/uni00000025/uni00000046/uni0000004f/uni00000054/uni0000004a/uni00000055/uni0000005a/uni00000023/uni00000024/uni00000028/uni0000002f/uni0000002f/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000031/uni00000056/uni00000043/uni0000004e/uni00000046/uni00000045/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni0000002a/uni0000004f/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni00000011/uni0000000f/uni00000013 /uni00000011/uni0000000f/uni00000015 /uni00000011/uni0000000f/uni00000017 /uni00000011/uni0000000f/uni00000019 /uni00000012/uni0000000f/uni00000011
/uni00000024/uni00000050/uni0000004f/uni00000047/uni0000004a/uni00000045/uni00000046/uni0000004f/uni00000044/uni00000046/uni00000011/uni00000013/uni00000011/uni00000015/uni00000011/uni00000017/uni00000011/uni00000019/uni00000011/uni00000025/uni00000046/uni0000004f/uni00000054/uni0000004a/uni00000055/uni0000005a/uni00000023/uni00000024/uni00000028/uni0000002f/uni0000002f/uni0000000f/uni00000001/uni0000000e/uni00000001/uni00000024/uni00000050/uni00000053/uni00000042/uni00000027/uni00000056/uni0000004d/uni0000004d/uni00000001/uni0000000e/uni00000001/uni00000013/uni00000011/uni00000001/uni0000000e/uni00000001/uni00000028/uni00000024/uni0000002f
/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
/uni0000002a/uni0000004f/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055
Figure 1: Reliability diagrams (Top) and Confidence distribution histograms (Bottom) for GCN on different datasets of Label
rate𝐿/𝐶=20before and after calibration.
/uni00000011 /uni00000013/uni00000011/uni00000011 /uni00000015/uni00000011/uni00000011 /uni00000017/uni00000011/uni00000011 /uni00000019/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011
/uni00000035/uni00000053/uni00000042/uni0000004a/uni0000004f/uni0000004a/uni0000004f/uni00000048/uni00000001/uni00000046/uni00000051/uni00000050/uni00000044/uni00000049/uni00000011/uni0000000f/uni00000016/uni00000011/uni00000011/uni0000000f/uni00000018/uni00000016/uni00000012/uni0000000f/uni00000011/uni00000011/uni00000012/uni0000000f/uni00000013/uni00000016/uni00000012/uni0000000f/uni00000016/uni00000011/uni00000012/uni0000000f/uni00000018/uni00000016/uni00000013/uni0000000f/uni00000011/uni00000011/uni00000035/uni00000046/uni00000054/uni00000055/uni00000001/uni0000002f/uni0000002d/uni0000002d/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000014/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000016/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000018/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000001a/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000012/uni00000012/uni0000000a
/uni00000024/uni00000053/uni00000050/uni00000054/uni00000054/uni00000026/uni0000004f/uni00000055/uni00000053/uni00000050/uni00000051/uni0000005a
/uni00000011 /uni00000013/uni00000011/uni00000011 /uni00000015/uni00000011/uni00000011 /uni00000017/uni00000011/uni00000011 /uni00000019/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011
/uni00000035/uni00000053/uni00000042/uni0000004a/uni0000004f/uni0000004a/uni0000004f/uni00000048/uni00000001/uni00000046/uni00000051/uni00000050/uni00000044/uni00000049/uni00000011/uni0000000f/uni00000016/uni00000012/uni0000000f/uni00000011/uni00000012/uni0000000f/uni00000016/uni00000013/uni0000000f/uni00000011/uni00000035/uni00000046/uni00000054/uni00000055/uni0000002f/uni0000002d/uni0000002d/uni00000009/uni00000024/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055/uni0000000a/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000014/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000016/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000018/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000001a/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000012/uni00000012/uni0000000a
/uni00000024/uni00000053/uni00000050/uni00000054/uni00000054/uni00000026/uni0000004f/uni00000055/uni00000053/uni00000050/uni00000051/uni0000005a
/uni00000011 /uni00000013/uni00000011/uni00000011 /uni00000015/uni00000011/uni00000011 /uni00000017/uni00000011/uni00000011 /uni00000019/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011
/uni00000035/uni00000053/uni00000042/uni0000004a/uni0000004f/uni0000004a/uni0000004f/uni00000048/uni00000001/uni00000046/uni00000051/uni00000050/uni00000044/uni00000049/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000012/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000014/uni00000011/uni0000000f/uni00000015/uni00000035/uni00000046/uni00000054/uni00000055/uni0000002f/uni0000002d/uni0000002d/uni00000009/uni0000002a/uni0000004f/uni00000044/uni00000050/uni00000053/uni00000053/uni00000046/uni00000044/uni00000055/uni0000000a/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000014/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000016/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000018/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000001a/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000012/uni00000012/uni0000000a
/uni00000024/uni00000053/uni00000050/uni00000054/uni00000054/uni00000026/uni0000004f/uni00000055/uni00000053/uni00000050/uni00000051/uni0000005a
/uni00000011 /uni00000013/uni00000011/uni00000011 /uni00000015/uni00000011/uni00000011 /uni00000017/uni00000011/uni00000011 /uni00000019/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011
/uni00000035/uni00000053/uni00000042/uni0000004a/uni0000004f/uni0000004a/uni0000004f/uni00000048/uni00000001/uni00000046/uni00000051/uni00000050/uni00000044/uni00000049/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000000f/uni00000012/uni00000011/uni0000000f/uni00000013/uni00000011/uni0000000f/uni00000014/uni00000011/uni0000000f/uni00000015/uni00000011/uni0000000f/uni00000016/uni00000011/uni0000000f/uni00000017/uni00000035/uni00000046/uni00000054/uni00000055/uni00000001/uni00000026/uni00000024/uni00000026/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000014/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000016/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000018/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni0000001a/uni0000000a
/uni00000030/uni00000056/uni00000053/uni00000054/uni00000009/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000012/uni00000012/uni0000000a
/uni00000024/uni00000053/uni00000050/uni00000054/uni00000054/uni00000026/uni0000004f/uni00000055/uni00000053/uni00000050/uni00000051/uni0000005a
Figure 2: The effects on model calibration of using either
cross-entropy loss or BCGNN with 𝛾set to 0.003, 0.005, 0.007,
0.009, and 0.011, when training a 2-layer GraphSAGE on the
Cora dataset with a label rate of 𝐿/𝐶=20.opposed to when using cross-entropy loss. Conversely, for misclas-
sified samples, BCGNN results in a higher NLL than cross-entropy
loss, suggesting a reduction in the confidence of these incorrectly
classified samples. This effect not only showcases the robustness of
BCGNN to variations in the 𝛾parameter but also underscores its
ability to make correctly classified samples more confident while
appropriately lowering confidence in misclassified ones, thereby
achieving a better-calibrated model.
6 CONCLUSION
This study addresses the critical issues of over-confidence and
under-confidence in GNNs by introducing a novel balanced cal-
ibrated loss function. This function uniquely considers not only
the highest prediction confidence but also the second-highest logit,
providing a more nuanced approach to model calibration.
Our theoretical analysis substantiates the effectiveness of this
method in enhancing the trustworthiness of GNN predictions. The
experimental results further reinforce this, demonstrating that our
proposed approach not only effectively calibrates GNN models but
also achieves state-of-the-art performance in various test scenarios.
3754Balanced Confidence Calibration for Graph Neural Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
The balanced calibrated loss function presented in this paper
marks a significant advancement in GNNs, offering a robust solution
to the often overlooked issue of model confidence calibration. By
ensuring that GNNs provide reliable and credible predictions, our
method paves the way for more trustworthy applications of GNNs
across diverse domains.
REFERENCES
[1]Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mo-
hammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra
Acharya, et al .2021. A review of uncertainty quantification in deep learning:
Techniques, applications and challenges. Information fusion 76 (2021), 243–297.
[2]Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Ben-
netot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel
Molina, Richard Benjamins, et al .2020. Explainable Artificial Intelligence (XAI):
Concepts, taxonomies, opportunities and challenges toward responsible AI. In-
formation fusion 58 (2020), 82–115.
[3]Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. 2006. Convexity, classifi-
cation, and risk bounds. J. Amer. Statist. Assoc. 101, 473 (2006), 138–156.
[4]Deyu Bo, Xiao Wang, Yang Liu, Yuan Fang, Yawen Li, and Chuan Shi. 2023. A
Survey on Spectral Graph Neural Networks. arXiv preprint arXiv:2302.05631
(2023).
[5]Aleksandar Bojchevski and Stephan Günnemann. 2017. Deep gaussian embed-
ding of graphs: Unsupervised inductive learning via ranking. arXiv preprint
arXiv:1707.03815 (2017).
[6]Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral net-
works and locally connected networks on graphs. arXiv preprint arXiv:1312.6203
(2013).
[7]Nontawat Charoenphakdee, Jayakorn Vongkulbhisal, Nuttapong Chairatanakul,
and Masashi Sugiyama. 2021. On focal loss for class-posterior probability esti-
mation: A theoretical perspective. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 5202–5211.
[8]Jie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast learning with graph
convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247
(2018).
[9]Giovanni Luca Ciampaglia, Alexios Mantzarlis, Gregory Maus, and Filippo
Menczer. 2018. Research challenges of digital misinformation: Toward a trust-
worthy web. AI Magazine 39, 1 (2018), 65–74.
[10] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-
tional neural networks on graphs with fast localized spectral filtering. Advances
in neural information processing systems 29 (2016).
[11] Shide Du, Zihan Fang, Shiyang Lan, Yanchao Tan, Manuel Günther, Shiping
Wang, and Wenzhong Guo. 2023. Bridging Trustworthiness and Open-World
Learning: An Exploratory Neural Approach for Enhancing Interpretability, Gener-
alization, and Robustness. In Proceedings of the 31st ACM International Conference
on Multimedia. 8719–8729.
[12] Sebastian Fritz-Morgenthal, Bernhard Hein, and Jochen Papenbrock. 2022. Fi-
nancial risk management and explainable, trustworthy, responsible AI. Frontiers
in artificial intelligence 5 (2022), 779799.
[13] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok
Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung,
Ribana Roscher, et al .2023. A survey of uncertainty in deep neural networks.
Artificial Intelligence Review 56, Suppl 1 (2023), 1513–1589.
[14] Arindam Ghosh, Thomas Schaaf, and Matthew Gormley. 2022. AdaFocal:
Calibration-aware Adaptive Focal Loss. Advances in Neural Information Pro-
cessing Systems 35 (2022), 1583–1595.
[15] Mara Graziani, Lidia Dutkiewicz, Davide Calvaresi, José Pereira Amorim, Katerina
Yordanova, Mor Vered, Rahul Nair, Pedro Henriques Abreu, Tobias Blanke, Valeria
Pulignano, et al .2023. A global taxonomy of interpretable AI: unifying the
terminology for the technical and social sciences. Artificial intelligence review 56,
4 (2023), 3473–3504.
[16] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of
modern neural networks. In International conference on machine learning. PMLR,
1321–1330.
[17] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[18] Zongbo Han, Changqing Zhang, Huazhu Fu, and Joey Tianyi Zhou. 2022. Trusted
multi-view classification with dynamic evidential fusion. IEEE transactions on
pattern analysis and machine intelligence 45, 2 (2022), 2551–2566.
[19] Andreas Holzinger, Bernd Malle, Anna Saranti, and Bastian Pfeifer. 2021. Towards
multi-modal causability with graph neural networks enabling information fusion
for explainable AI. Information Fusion 71 (2021), 28–37.
[20] Hans Hao-Hsun Hsu, Yuesong Shen, Christian Tomani, and Daniel Cremers.
2022. What Makes Graph Neural Networks Miscalibrated? Advances in NeuralInformation Processing Systems 35 (2022), 13775–13786.
[21] He-xuan Hu, Chengcheng Cao, Qiang Hu, and Ye Zhang. 2024. Federated learning
enabled graph convolutional autoencoder and factorization machine for potential
friendship prediction in social networks. Information Fusion 102 (2024), 102042.
[22] Alex Pappachen James and Belur Dasarathy. 2015. A review of feature and data
fusion with medical images. arXiv preprint arXiv:1506.00097 (2015).
[23] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[24] Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song,
and Peter Flach. 2019. Beyond temperature scaling: Obtaining well-calibrated
multi-class probabilities with dirichlet calibration. Advances in neural information
processing systems 32 (2019).
[25] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature
521, 7553 (2015), 436–444.
[26] Zizhuo Li and Jiayi Ma. 2023. Learning Feature Matching via Matchable Keypoint-
Assisted Graph Neural Network. arXiv preprint arXiv:2307.01447 (2023).
[27] Ambuj Mehrish, Navonil Majumder, Rishabh Bharadwaj, Rada Mihalcea, and
Soujanya Poria. 2023. A review of deep learning techniques for speech processing.
Information Fusion (2023), 101869.
[28] Khan Muhammad, Amin Ullah, Jaime Lloret, Javier Del Ser, and Victor Hugo C
de Albuquerque. 2020. Deep learning for safe autonomous driving: Current
challenges and future directions. IEEE Transactions on Intelligent Transportation
Systems 22, 7 (2020), 4316–4336.
[29] Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr,
and Puneet Dokania. 2020. Calibrating deep neural networks using focal loss.
Advances in Neural Information Processing Systems 33 (2020), 15288–15299.
[30] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtaining
well calibrated probabilities using bayesian binning. In Proceedings of the AAAI
conference on artificial intelligence.
[31] Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, and Byron
Boots. 2020. Intra order-preserving functions for calibration of multi-class neural
networks. Advances in Neural Information Processing Systems 33 (2020), 13456–
13467.
[32] Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grégoire Montavon,
Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert Müller.
2021. A unifying review of deep and shallow anomaly detection. Proc. IEEE 109,
5 (2021), 756–795.
[33] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini. 2008. The graph neural network model. IEEE transactions on neural
networks 20, 1 (2008), 61–80.
[34] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93–93.
[35] Linwei Tao, Minjing Dong, and Chang Xu. 2023. Dual Focal Loss for Calibration.
arXiv preprint arXiv:2305.13665 (2023).
[36] Leonardo Teixeira, Brian Jalaian, and Bruno Ribeiro. 2019. Are graph neural
networks miscalibrated? arXiv preprint arXiv:1905.02296 (2019).
[37] Ambuj Tewari and Peter L Bartlett. 2007. On the Consistency of Multiclass
Classification Methods. Journal of Machine Learning Research 8, 5 (2007).
[38] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[39] Iris Vos, Ishaan Bhat, Birgitta Velthuis, Ynte Ruigrok, and Hugo Kuijf. 2023.
Calibration techniques for node classification using graph neural networks on
medical image data. In Medical Imaging with Deep Learning.
[40] Dongdong Wang, Boqing Gong, and Liqiang Wang. 2023. On Calibrating Semantic
Segmentation Models: Analyses and an Algorithm. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 23652–23662.
[41] Min Wang, Hao Yang, and Qing Cheng. 2022. GCL: Graph Calibration Loss for
Trustworthy Graph Neural Network. In Proceedings of the 30th ACM International
Conference on Multimedia. 988–996.
[42] Qi Wang and Herke Van Hoof. 2022. Model-based meta reinforcement learn-
ing using graph structured surrogate models and amortized policy search. In
International Conference on Machine Learning. PMLR, 23055–23077.
[43] Xiao Wang, Hongrui Liu, Chuan Shi, and Cheng Yang. 2021. Be confident!
towards trustworthy graph neural networks via confidence calibration. Advances
in Neural Information Processing Systems 34 (2021), 23768–23779.
[44] Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. 2022.
Mitigating neural network overconfidence with logit normalization. In Interna-
tional Conference on Machine Learning. PMLR, 23631–23644.
[45] Longfeng Wu, Bowen Lei, Dongkuan Xu, and Dawei Zhou. 2023. Towards
reliable rare category analysis on graphs via individual calibration. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
2629–2638.
[46] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems 32, 1 (2020), 4–24.
3755KDD ’24, August 25–29, 2024, Barcelona, Spain Hao Yang, Min Wang, Qi Wang, Mingrui Lao, & Yun Zhou
[47] Hao Yang, Min Wang, Yun Zhou, and Yongxin Yang. 2021. Towards Stochastic
Neural Network via Feature Distribution Calibration. In 2021 IEEE International
Conference on Data Mining (ICDM). IEEE, 1445–1450.
[48] Liang Yang, Xiaochun Cao, Dongxiao He, Chuan Wang, Xiao Wang, and Weixiong
Zhang. 2016. Modularity based community detection with deep learning.. In
IJCAI, Vol. 16. 2252–2258.
[49] He Zhang, Bang Wu, Xingliang Yuan, Shirui Pan, Hanghang Tong, and Jian Pei.
2022. Trustworthy graph neural networks: Aspects, methods and trends. arXiv
preprint arXiv:2205.07424 (2022).
[50] Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. 2020. Mix-n-match: Ensem-
ble and compositional methods for uncertainty calibration in deep learning. In
International conference on machine learning. PMLR, 11117–11128.
[51] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. 2022. When and
how mixup improves calibration. In International Conference on Machine Learning.
PMLR, 26135–26160.
[52] Fan Zhou, Tianliang Wang, Ting Zhong, and Goce Trajcevski. 2022. Identifying
user geolocation with hierarchical graph neural networks and explainable fusion.
Information Fusion 81 (2022), 1–13.
[53] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,
Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:
A review of methods and applications. AI open 1 (2020), 57–81.
[54] Fei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-Lin Liu. 2022. Rethinking
confidence calibration for failure prediction. In European Conference on Computer
Vision. Springer, 518–536.
A PROOF OF THEOREM 4.1 AND
PROPOSITION 4.2
Theorem 4.1. For any 0<𝛾<1, the proposed loss function
is classification-calibrated and has the strictly order-preserving
property.
Proof. Note that we can represent 𝜙 𝑞∗
𝑖as
𝜙 𝑞∗
𝑖=
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾𝑞∗
𝑖log𝑞∗
𝑖+ 1+𝛾 𝑞∗
𝑖−𝑓 𝑞∗
𝑖
(16)
and𝜂𝑖can be formulated as𝑞∗
𝑖
𝜙(𝑞∗
𝑖)
Í𝐶
𝑐𝑞∗
𝑖
𝜙(𝑞∗
𝑖)in Eq. 24. Now we simplify
Eq. 24 through taking ℎ(𝑣𝑖)=𝑞∗
𝑖
𝜙(𝑣𝑖)and function ℎ(𝑣)can be
formulated as
ℎ(𝑣)=𝑣
𝜙(𝑣)=(𝑣
1+𝛾𝑣log𝑣, 𝑗 =𝑘
𝑣
𝛾𝑣log𝑣+1+𝛾(𝑣−𝐶), 𝑗≠𝑘(17)
With function ℎ, Eq. 24 can be rewritten as
𝜂𝑖=ℎ 𝑞∗
𝑖
Í𝐶𝑐ℎ(𝑞∗𝑐). (18)
Considering the scenario 𝑗=𝑘, the derivative of ℎ(𝑣)is
𝜕ℎ
𝜕𝑣=1
(1+𝛾𝑣log𝑣)−𝑣(𝛾log𝑣+𝛾)
(1+𝛾𝑣log𝑣)2=1−𝛾𝑣
(1+𝛾𝑣log𝑣)2(19)
note that𝑣∈(0,1), for𝛾<1
𝑣,𝜕ℎ
𝜕𝑣>0, so the sufficient condition is
𝛾<1.
Considering the scenario 𝑗≠𝑘, we take the derivative of ℎ(𝑣)as
𝜕ℎ
𝜕𝑣=1
𝛾𝑣log𝑣+1+𝛾(𝑣−𝐶)−𝑣(𝛾(1+log𝑣)+𝛾)
(𝛾𝑣log𝑣+1+𝛾(𝑣−𝐶))2
=𝛾𝑣log𝑣+1+𝛾(𝑣−𝐶)−𝑣(𝛾(1+log𝑣)+𝛾)
(𝛾𝑣log𝑣+1+𝛾(𝑣−𝐶))2
=1−𝛾(𝑣+𝐶)
(𝛾𝑣log𝑣+1+𝛾(𝑣−𝐶))2(20)
for𝛾<1
𝑣+𝐶,𝜕ℎ
𝜕𝑣>0, and𝑣+𝐶∈(0,1), so the sufficient condition
is𝛾<1.In this way, we can conclude that for any 𝛾<1, we have𝜕ℎ
𝜕𝑣>0,
which makes ℎa strictly increasing function, in other words, we
have𝑞∗𝑎<𝑞∗
𝑏⇒ℎ 𝑞∗𝑎<ℎ
𝑞∗
𝑏
. Together with 𝜂𝑖=ℎ(𝑞∗
𝑖)Í𝐶
𝑐ℎ(𝑞∗𝑐),
we have𝑞∗𝑎<𝑞∗
𝑏⇒𝜂𝑎<𝜂𝑏. Thus, the proposed loss function
is strictly order-perserrving, which is sufficient for classification-
calibration.
Proposition 4.2. For the scenario where 𝑗≠𝑘,𝜙(0)=1−𝛾𝐶,
and𝜙(1)=1+𝛾(1−𝐶), there exists v 𝑚=1e-2, for𝑣𝑖∈(0,1𝑒−2),
𝜕𝜙
𝜕𝑣𝑖<0, and for𝑣𝑖∈(1𝑒−2,1),𝜕𝜙
𝜕𝑣𝑖>0.
Proof. We explore the property of function 𝜙via its derivative as
𝜕𝜙
𝜕𝑣𝑖=−𝜕2𝑓
𝜕𝑣2
𝑖𝛾𝑣𝑖log𝑣𝑖+(1−𝜕𝑓
𝜕𝑣𝑖)𝛾(1+log𝑣𝑖)+𝛾−𝛾𝜕𝑓
𝜕𝑣𝑖
=−𝜕2𝑓
𝜕𝑣2
𝑖𝛾𝑣𝑖log𝑣𝑖+(1−𝜕𝑓
𝜕𝑣𝑖)𝛾(2+log𝑣𝑖)(21)
for function 𝑓, it maps𝑣𝑖to the second maximum vale 𝑣𝑘in the
ascending order 𝑣1:𝐾after𝑣𝑗. According to the definition in Eq. 14,
we can simplify the Eq. 21 as
𝜕𝜙
𝜕𝑣𝑖=𝛾(2+log𝑣𝑖) (22)
Since𝜕2𝑓
𝜕𝑣2
𝑖=𝛾
𝑣𝑖>0and𝜕𝜙
𝜕𝑣𝑖|𝑣𝑖=1𝑒−2=0, thus, for𝑣𝑖∈(0,1𝑒−2),
𝜕𝜙
𝜕𝑣𝑖<0, and for𝑣𝑖∈(1𝑒−2,1),𝜕𝜙
𝜕𝑣𝑖>0.
Proposition 4.2 shows that 𝜙first decrease in [0,1e-2), and then
increase in (1e-2,1], we have known 𝜙(0)<1and𝜙(1)>1, so
there exists a 𝑣𝑢in [1e-2,1], that 𝜙(𝑣𝑢)=𝜙(0), compared with
GCL(entropy minimization), with the assumption of the same 𝛾
in proposed loss function and GCL, the value of 𝑣𝑢to 0 is closer
than proposed loss function, it is easy to show that the regions of
over-confidence is reduced in our proposed loss function compared
to GCL. Thus, the proposed loss function can alleviate the over-
confidence problem of entropy minimization; we empirically verify
the proposed method achieves a better trade-off in the experimental
section.
B PROOF OF EQ .11
𝐶∑︁
𝑖𝜂𝑖=1=𝐶∑︁
𝑖𝜆
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾log𝑞∗
𝑖+(1+𝛾(𝑞∗
𝑖−𝑓(𝑞∗
𝑖)))
𝑞∗
𝑖,
𝜆=𝐶∑︁
𝑖 
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾log𝑞∗
𝑖+ 1+𝛾 𝑞∗
𝑖−𝑓 𝑞∗
𝑖
𝑞∗
𝑖!
.(23)
3756Balanced Confidence Calibration for Graph Neural Networks KDD ’24, August 25–29, 2024, Barcelona, Spain
By replacing the above 𝜆in Eq. 9, we can write 𝜂𝑖as a function
of𝑞∗
𝑖as
𝜂𝑖=Í𝐶
𝑖
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾log𝑞∗
𝑖+(1+𝛾(𝑞∗
𝑖−𝑓(𝑞∗
𝑖)))
𝑞∗
𝑖

1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾log𝑞∗
𝑖+(1+𝛾(𝑞∗
𝑖−𝑓(𝑞∗
𝑖)))
𝑞∗
𝑖
=𝑞∗
𝑖 
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾𝑞∗
𝑖log𝑞∗
𝑖+(1+𝛾(𝑞∗
𝑖−𝑓(𝑞∗
𝑖)))
𝑞∗
𝑖
Í𝐶
𝑖
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾𝑞∗
𝑖log𝑞∗
𝑖+(1+𝛾(𝑞∗
𝑖−𝑓(𝑞∗
𝑖)))
=𝑞∗
𝑖
𝜙(𝑞∗
𝑖)
Í𝐶𝑐𝑞∗
𝑖
𝜙(𝑞∗
𝑖)(24)
where𝜙 𝑞∗
𝑖=
1−𝜕𝑓
𝜕𝑞∗
𝑖
𝛾𝑞∗
𝑖log𝑞∗
𝑖+ 1+𝛾 𝑞∗
𝑖−𝑓 𝑞∗
𝑖
. As a
result, given 𝑞∗
𝑖, one can recover the true class-posterior probability
𝜂𝑖, by using the transformation in Eq. 24.
C SUMMARY OF USED PARAMETERS
We summarize the 𝛾parameters used in GCN, GAT and GraphSAGE
as follows,
Dataset𝐿/𝐶GCN GAT GraphSAGE
𝛾 𝛾 𝛾
Cora20 0.05 0.80 0.80
40 0.05 0.80 0.80
60 0.05 0.80 0.80
Citeseer20 0.05 0.50 0.80
40 0.08 0.50 0.80
60 0.08 0.50 0.80
Pubmed20 0.05 0.05 0.05
40 0.05 0.05 0.05
60 0.05 0.05 0.05
CoraFull20 0.01 0.20 0.10
40 0.01 0.20 0.10
60 0.01 0.20 0.10
Table 5: Summary of parameters used in BCGNN.
D THE STD VALUES OF COMPARISON
RESULTSTable 6: The GCN Std Values of Comparison Results.
Dataset
L/CGCN
Uncal.
TS MS CaGCN GCL Proposed
Cora20 0.0063
0.0055 0.0057 0.0067 0.0058 0.0049
40 0.0047 0.0072 0.0046 0.0054 0.0044 0.0041
60 0.0049 0.0054 0.0061 0.0044 0.0059 0.0043
Citese
er20 0.0071 0.0087 0.0037 0.0072 0.0080 0.0035
40 0.0077 0.0042 0.0057 0.0055 0.0057 0.0034
60 0.0064 0.0050 0.0064 0.0034 0.0040 0.0031
Pubme
d20 0.0077 0.0038 0.0042 0.0060 0.0041 0.0032
40 0.0055 0.0063 0.0063 0.0040 0.0061 0.0036
60 0.0097 0.0060 0.0064 0.0048 0.0048 0.0043
CoraFull20 0.0061
0.0038 - 0.0064 0.0065 0.0033
40 0.0054 0.0065 - 0.0039 0.0041 0.0034
60 0.0040 0.0038 - 0.0034 0.0037 0.0029
Table 7: The GAT Std Values of Comparison Results.
Dataset
L/CGAT
Uncal.
TS MS CaGCN GCL Proposed
Cora20 0.0089
0.0098 0.0094 0.0056 0.0067 0.0051
40 0.0054 0.0077 0.0060 0.0056 0.0035 0.0033
60 0.0033 0.0061 0.0053 0.0032 0.0031 0.0028
Citese
er20 0.0050 0.0087 0.0098 0.0068 0.0082 0.0048
40 0.0087 0.0031 0.0054 0.0054 0.0079 0.0030
60 0.0059 0.0071 0.0091 0.0076 0.0022 0.0019
Pubme
d20 0.0031 0.0046 0.0037 0.0063 0.0055 0.0029
40 0.0046 0.0065 0.0060 0.0054 0.0046 0.0036
60 0.0041 0.0063 0.0057 0.0052 0.0043 0.0037
CoraFull20 0.0036
0.0051 - 0.0060 0.0063 0.0030
40 0.0042 0.0083 - 0.0048 0.0076 0.0038
60 0.0018 0.0052 - 0.0069 0.0072 0.0017
Table 8: The GraphSAGE Std Values of Comparison Results.
Dataset
L/CGraphSAGE
Uncal.
TS MS CaGCN GCL Proposed
Cora20 0.0071
0.0064 0.0071 0.0069 0.0070 0.0063
40 0.0063 0.0066 0.0062 0.0064 0.0066 0.0060
60 0.0063 0.0061 0.0070 0.0046 0.0072 0.0035
Citese
er20 0.0008 0.0011 0.0065 0.0061 0.0066 0.0054
40 0.0011 0.0016 0.0081 0.0065 0.0075 0.0060
60 0.0093 0.0059 0.0070 0.0047 0.0073 0.0038
Pubme
d20 0.0079 0.0075 0.0077 0.0048 0.0080 0.0043
40 0.0053 0.0056 0.0066 0.0021 0.0056 0.0020
60 0.0073 0.0049 0.0047 0.0087 0.0062 0.0033
CoraFull20 0.0050
0.0077 - 0.0066 0.0074 0.0047
40 0.0034 0.0045 - 0.0067 0.0070 0.0031
60 0.0038 0.0070 - 0.0084 0.0091 0.0026
3757