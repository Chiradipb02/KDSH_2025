SimDiff: Simple Denoising Probabilistic Latent Diffusion Model
for Data Augmentation on Multi-modal Knowledge Graph
Ran Li
HKUST
Hong Kong SAR, China
rlibb@connect.ust.hkShimin Di
HKUST
Hong Kong SAR, China
sdiaa@connect.ust.hkLei Chen
HKUST(GZ), HKUST
Guangzhou, China
leichen@hkust-gz.edu.cnXiaofang Zhou
HKUST
Hong Kong SAR, China
zxf@cse.ust.hk
Abstract
In this paper, we address the challenges of data augmentation
in Multi-Modal Knowledge Graphs (MMKGs), a relatively under-
explored area. We propose a novel diffusion-based generative model,
the Simple Denoising Probabilistic Latent Diffusion Model (SimDiff).
SimDiff is capable of handling different data modalities including
the graph topology in a unified manner by the same diffusion model
in the latent space. It enhances the utilization of multi-modal data
and encourages the multi-modal fusion and reduces the dependency
on limited training data. We validate our method in downstream
Entity Alignment (EA) tasks in MMKGs, demonstrating that even
when using only half of the seed entities in training, our meth-
ods can still achieve superior performance. This work contributes
to the field by providing a new data generation or augmentation
method for MMKGs, potentially paving the way for more effective
use of MMKGs in various applications. Code is made available at
https://github.com/ranlislz/SimDiff.
CCS Concepts
•Information systems →Multimedia and multimodal retrieval ;
Data mining ;Information integration .
Keywords
Entity alignment, multi-modal learning, knowledge graph, data
augmentation, generative model
ACM Reference Format:
Ran Li, Shimin Di, Lei Chen, and Xiaofang Zhou. 2024. SimDiff: Simple
Denoising Probabilistic Latent Diffusion Model for Data Augmentation on
Multi-modal Knowledge Graph . In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671769
1 Introduction
Knowledge graphs (KGs) [ 1,2,33] are structured representations
of real-world facts that are represented by entities and the connec-
tion between them. The applications of KGs are diverse, including
question answering [ 10,11,37], recommender systems [ 51,53],
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671769
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Seed entity Percentage0.30.40.50.60.7H@1
FB15K-DB15K
MMEA
MCLEA
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Seed entity Percentage0.30.40.50.6H@1
FB15K-YAGO15K
MMEA
MCLEAFigure 1: The percentage of seed entities, or pre-aligned enti-
ties, is important to the performance of the entity alignment
on multi-modal knowledge graph. The performance is sensi-
tive and heavily relies on the number of seed entities used
across different models. This indicates that one may try to
use the limited seed entities more effectively to improve the
performance.
and natural language understanding [ 29,50,63]. A Multi-Modal
Knowledge Graph (MMKG) is an extension of a traditional knowl-
edge graph that incorporates multiple modalities of information,
such as text, images, and audio, which can provide rich and comple-
mentary information for entity representation and matching [ 30].
However, the current datasets of MMKGs, which have around 10K
entities [ 30], are relatively small compared with the size of those
in the image [ 15] and language domains [ 3]. This could hinder
the performance of the MMKGs on downstream tasks like entity
alignment. In some settings, only 20% (around 2K) of samples can
be used for training. Entity alignment (EA) aims to find equivalent
entities across KGs. This can help enrich and integrate KGs [ 60]
by using the limited pre-aligned seed entities as training signals.
As shown in Fig 1, the EA tasks on MMKGs [ 7,26] heavily rely
on seed entities. These are often scarce and expensive to obtain in
practice [ 27]. Collecting more MMKG data is ideal. However, given
the current situation, it would be beneficial to use the data more
effectively by creating additional MMKG data to aid downstream
tasks.
To address this, one promising solution is data augmentation, a
technique that enhances data efficiency and model performance by
generating new versions of existing data instances. It is beneficial
in various data structures, such as images [ 47], languages [ 24], and
graphs [ 59]. However, data augmentation for Knowledge Graphs
(KGs) is relatively underexplored, particularly for Multi-Modal KGs.
Some existing works [ 5,6,40,46,57,64] propose augmenting
KGs using adversarial learning, reverse relationships, mixup, rule-
based methods, and counterfactual generation for knowledge graph
 
1631
KDD ’24, August 25–29, 2024, Barcelona, Spain Ran Li et al.
data augmentation. However, these methods necessitate human-
designed augmentation rules and may struggle to preserve the
semantics of entities. Moreover, they primarily focus on relation-
wise augmentation rather than the entity itself, and they do not
consider other modalities. A more direct approach involves gener-
ating new data using generative models. The generated data can
potentially enhance model performance in the language [ 23,58]
and image domains [ 21,48]. For MMKGs, the graph topology that
connects different data modalities makes it hard to apply current
data generation techniques from other domains. Moreover, to bene-
fit downstream tasks such as Entity Alignment (EA), the generative
model should be trained stably with limited data and effectively
fuse multi-modal information. Thus, a new data generation or aug-
mentation method for MMKGs is needed.
To address this, we propose using a generative model [ 19] to
generate augmented data for MMKGs and validate our methods
on downstream Entity Alignment (EA) tasks. This concept is in-
spired by recent advancements in generative models, particularly
diffusion models [ 13,22]. These models are a powerful and popular
generative method that learns the reverse process of adding noise
to perturb data until a defined noise level is reached, following a
Markov chain. Diffusion models [ 56] have demonstrated impressive
results in generating high-quality, diverse images, text, and audio.
However, applying diffusion models to MMKGs presents several
challenges: (1) How can the diffusion model be extended to handle
different data modalities, including graph structures for KGs, rather
than designing modality-specific diffusion models? (2) How can the
diffusion model be trained on a relatively small dataset of MMKGs
without leading to unstable training? (3) How can information ex-
change among different modalities be further encouraged while
preserving the semantics and diversity of multi-modal knowledge
graphs? To overcome these challenges, we propose the Simple De-
noising Probabilistic Latent Diffusion Model (SimDiff) for MMKGs,
making the following contributions:
•Our novel diffusion-based generative model for MMKGs can han-
dle different data modalities, including graph structures, in a uni-
fied manner. The diffusion model allows information exchange
among different modalities and results in better embedding qual-
ities compared with the original ones.
•We address the instability of co-training the diffusion model on
relatively small heterogeneous datasets alongside the training
goal for downstream tasks. We help utilize the multi-modal data
more effectively and reduce the dependency on the limited train-
ing data by using the generated embeddings as augmented data.
•We validate our method in downstream EA tasks in MMKGs. We
obtain augmented embeddings for MMKGs without disturbing
the semantics of KGs or necessitating human-designed augmen-
tation techniques for each modality. We demonstrate that, even
when using only half of the seed entities in training, our methods
can still achieve superior performance.
2 Related Work
2.1 Data Augmentation for KGs
Data augmentation is a technique that enhances data efficiency and
model performance by generating new views of data instances from
existing ones. It is a common technique for different data structures,like images [ 47], languages [ 24], and graphs [ 59]. However, data
augmentation for KGs has been relatively less explored, especially
for multi-modal KGs. Some existing works [ 5,6,40,46,57,64]
have proposed to augment KGs using adversarial learning, reverse
relationships, mixup, rule-based methods, and counterfactual gen-
eration for knowledge graph data augmentation. However, these
methods were not designed for multi-modal KGs, and they might
not be able to preserve the multi-modal information or the align-
ment relations.
2.2 Data Augmentation for Multi-modal Data
On the other hand, some works have proposed to augment multi-
modal data in various domains, such as image-text pairs or audio-
visual data. For example, MixGen [ 18] uses interpolation to augment
the image-text pair. It first interpolates the image with another one
and then concatenates the corresponding language descriptions.
LeMDA [ 32], which is similar to our method, uses generative meth-
ods like VAE [ 14] in the feature space to tackle the augmentation
for image and text data. However, these methods are not suitable
for KGs, as they do not consider the graph structure or the limited
seed entities.
2.3 Entity alignment in MMKG
Several methods have been proposed to tackle the EA on multi-
modal knowledge graphs (MMKGs) by integrating the informa-
tion from different data modalities of the same real-world entities.
MMEA [ 7] integrates the different modalities into a joint embed-
ding to represent the entities. EVA [ 28] uses the visual feature
to create the initial pre-aligned entities that guide the training.
MSNEA [ 8] uses visual features to guide the learning of relation
features and utilizes the contrastive learning [ 9] to do the entity
alignment. MCLEA [ 26] obtains the joint embedding by weighted
concatenation and further leverages the contrastive learning to
align the modality embeddings with the joint embedding. How-
ever, these methods suffer from the limited seed entities and fail
to explore fusion methods other than the direct concatenation of
different modalities embeddings.
3 Methodology
Our methods are shown in Figure 2. We present ours in three parts.
(1) Learning the MMKG entity distribution, along with the graph
topology and the associated text and visual features, is challenging.
Therefore, we begin by decoupling the structural information and
related features, utilizing separate encoders to obtain the uni-modal
embeddings. (2) We present our SimDiff model. It is capable of
approximating the posterior distribution of the entities, given the
information from different modalities. Fusion is achieved through
a shared workspace that processes all modality embeddings in
a unified latent space, facilitating information exchange. The L-
net is specifically designed to ensure stable training in situations
with limited data. (3) We conduct an evaluation of our model on
downstream Entity Alignment (EA) tasks in MMKGs and propose
the multi-view contrastive learning for EA on MMKGs.
 
1632SimDiff: Simple Denoising Probabilistic Latent Diffusion Model
for Data Augmentation on Multi-modal Knowledge Graph KDD ’24, August 25–29, 2024, Barcelona, Spain
Unimodal embeddingsMulti-view contrastive learning𝒉
𝒑#𝒛
(b) Entity alignmentjoint embeddingsGenerated viewOriginal viewPerturbed view𝐿!+𝐿"#$%&+𝐿'(!+ 𝐿&#)
Shared workspace𝒉𝒖𝒕𝐿"#$Denoising EncoderDenoising decoder
Entity eGATBag-of-wordResnet…𝒉𝒈𝒉𝒏𝒉𝒗$𝒛𝒈$𝒛𝒏$𝒛𝒗Weighted concatenation $𝒛
𝒉Denoising L-net
Structure g Relation rImage i Entity e in KG1Entity e in KG2(a) SimDiff
Figure 2: An overview of our method. The SimDiff model on the left receives input from the different modality embeddings
and outputs the generated ones. The shared workspace encourages the multi-modal information fusion and utilizes the L-net
for stable co-training with limited training data. The output is then applied to the downstream tasks on the right. Based on
the generated data, we propose the multi-view contrastive learning to enhance the EA on MMKG with different views of the
original data.
3.1 Decoupling and Uni-modal Embeddings for
Multi-modal Knowledge Graphs
Rather than directly applying a generative model to the original
data, we decouple the graph structure, associated images, and other
attributes, and utilize separate encoders to obtain uni-modal embed-
dings. The choice of encoders affects the quality and performance
of the embeddings. Given that the downstream tasks involve en-
tity alignment in MMKGs, we use the same encoders as previous
models [ 26,27] for a fair comparison. The details of the model are
shown in the appendix.
Graph Embeddings: We use a Graph Attention Network (GAT) [ 49]
to capture the structural patterns. The graph embedding 𝒉𝑔
𝑖for en-
tity𝑒𝑖is:
𝒉𝑔
𝑖=𝜎©­
«∑︁
𝑝∈N𝑖𝛽𝑖𝑝𝑾𝑠𝒉𝑔
𝑝ª®
¬, (1)
where 𝑾𝑠∈R𝑑×𝑑is a diagonal matrix that linearly manipulates
the states [ 25],N𝑖is the neighbor node and 𝜎is the ReLU activation
function. The attention factor 𝛽𝑚𝑝evaluates the importance of
node 𝒏𝑝for node 𝒏𝑚.
Image Embeddings: We employ a CNN like ResNet-152 as our
“Image Encoder” to extract image features 𝒉𝑣
𝑖=CNN(𝑰𝑖). The
image embedding is obtained by passing the image through the
CNN and taking the output of the last layer.Attribute, Relational, and Name and Character Embeddings:
We encode the relations, attributes, and names of entities as bag-
of-words [ 62] features : 𝒉𝑙
𝑖, where𝑙∈{𝑟,𝑎,𝑛,𝑐}. A simple feed-
forward layer transforms these features into their corresponding
embeddings. For the name feature, we use the average of the pre-
trained GloVe [ 39] vectors of the name tokens. We also use character
bigrams [ 26,34] of entity names as complementary information for
missing names.
Embeddings perturbation To enhance the robustness of the
model, we perturb the original embeddings to obtain an alternative
view for comparison. We add controlled Gaussian noise to the orig-
inal embeddings to obtain their perturbed counterparts. The scale
of noise is controlled by a factor 𝛼. Formally, given an entity 𝑒𝑖with
embeddings from different modalities ℎ𝑚
𝑖, where𝑚∈{𝑣,𝑟,𝑎,𝑛,𝑐,𝑔},
we obtain its perturbed embedding 𝑝𝑚
𝑖as follows:
𝒑𝑚
𝑖=𝒉𝑚
𝑖+𝛼𝝐 (2)
where 𝝐∼N( 0,𝐼)is the noise vector from a standard Gaussian
distribution. The factor 𝛼determines the magnitude of the noise
and can be tuned as a hyperparameter.
 
1633KDD ’24, August 25–29, 2024, Barcelona, Spain Ran Li et al.
3.2 Simple Denoising Probabilistic Latent
Diffusion Model for MMKGs
We present our SimDiff model. It can approximate the posterior
distribution of the entities given the different modalities informa-
tion. Fusion is achieved through a shared workspace that processes
all modality embeddings in a unified latent space, encouraging
information exchange in a simple way. Instead of U-net [ 41], we
stack linear layers as the backbone denoising network to ensure
stable training in situations with limited data. Training the diffusion
model is not easy and usually requires a large amount of data. For
typical entity alignment tasks, only a part, specifically 20-30% of
seed entities are used to guide the training. The limited data pose
a new challenge to train a good diffusion model. Moreover, in our
case, incorporating six different modalities of embedding is even
more challenging.
Why does SimDiff work? SimDiff is a class of latent variable
models that can generate realistic samples from complex data dis-
tributions by modeling the joint distribution of different modalities.
Formally, SimDiff can be formulated as follows:
Given a set of data points 𝒁with different modalities 𝒛𝑚, where
𝑚=1,...,𝑀 , SimDiff defines a generative process that starts from
a standard Gaussian prior 𝒖0∼N( 0,𝐼)and adds Gaussian noise at
each step𝑡=1,...,𝑇 to obtain a corrupted latent variable 𝒖𝑡:
𝒖𝑡=𝒖𝑡−1+N( 0,𝛽𝑡𝐼), (3)
where𝛽𝑡is the noise level at step 𝑡. The final latent variable 𝒖𝑇is
then mapped to the data modalities 𝒛𝑚by a decoder function 𝒇𝑚:
𝒛𝑚=𝒇𝑚(𝒖𝑇)+𝑁(0,𝜎2
𝑚𝐼), (4)
where𝜎2𝑚is the observation noise for modality 𝑚. The joint distri-
bution of the data modalities and the latent variables is then given
by:
𝑝(𝒛1,..., 𝒛𝑀,𝒖0,..., 𝒖𝑇)
=𝑝(𝒖0)𝑇Ö
𝑡=1𝑝(𝒖𝑡|𝒖𝑡−1)𝑀Ö
𝑚=1𝑝(𝒛𝑚|𝒖𝑇), (5)
where𝑝(𝒖𝑡|𝒖𝑡−1)and𝑝(𝒛𝑚|𝒖𝑇)are Gaussian distributions with
means and variances defined by (3) and (4), respectively.
We derive the variational bound that is weighted and use Langevin
dynamics to help denoising score matching for the diffusion proba-
bilistic models on the latent space inspired by [ 22]. The weighted
variational bound is given by:
L𝑤=𝑇∑︁
𝑡=1𝑤𝑡"
E𝑞(𝒖𝑡|𝒛1,...,𝒛𝑀)
 𝑀∑︁
𝑚=1|𝒛𝑚−𝒇𝑚(𝒖𝑡)|2
2𝜎2𝑚+
|∇𝒖𝑡log𝑞(𝒖𝑡|𝒛1,..., 𝒛𝑀)+𝒖𝑡/𝛽𝑡|2
2!#
, (6)
where𝑞(𝒖𝑡|𝒛1,..., 𝒛𝑀)is an approximate posterior distribution
that maps the data modalities to the latent variable at step 𝑡, and
𝑤𝑡is a weight coefficient that balances the contribution of each
step. The approximate posterior distribution is parameterized byan encoder function 𝑔that takes the data modalities as input and
outputs a Gaussian distribution’s mean and variance :
𝑞(𝒖𝑡|𝒛1,..., 𝒛𝑀)=N(𝑔(𝒛1,..., 𝒛𝑀;𝑡)), (7)
where𝑡indicates the step of the diffusion process.
This suggests that the model maps the Gaussian noise to the
multi-modal latent embedding distribution. Given a noise (or the
noised data), the model can generate new data that considers differ-
ent data modalities and fits the original distribution. Then, we can
use the trained SimDiff to generate novel data augmentations for
MMKGs based on their multi-modal information. In practice, we
employ a reconstruction loss instead of the weighted variational
lower bound, and there are also some challenges in training.
Shared Workspace for fusion. We use all six modalities to-
gether instead of training a modality-wise diffusion model. How-
ever, different modality embeddings including graph structure em-
bedding, image embeddings, etc., contain heterogeneous informa-
tion. To process them, we use a shared workspace to receive the
input of all six modalities one by one. The shared layer transforms
the embeddings into a common latent space before they are fed into
the diffusion model. The shared workspace is a linear layer that
projects the embeddings from different modalities into a common
latent space. The equation for the shared workspace is as follows:
𝒛𝑚
𝑖=𝑊𝑠𝒉𝑚
𝑖+𝑏𝑠 (8)
where 𝒛𝑚
𝑖is the projected embedding of entity 𝑒𝑖from modality
𝑚,𝒉𝑚
𝑖is the original embedding of entity 𝑒𝑖from modality 𝑚,𝑊𝑠
and𝑏𝑠are the linear layer parameters which can be learned, and
𝑚∈{𝑣,𝑟,𝑎,𝑛,𝑐,𝑔}.
Denoising L-Net for training on limited MMKGs data. For
the denoising neural network, instead of the more complex U-net
structure [ 22,41] with multi-head attention, we use the stacked
L-net, linear layers, to do the denoising. Different from the image,
our input after the shared workspace is unified into 1D embeddings.
Compared with the CNN (U-Net), the L-net is more stable and suit-
able for our case. We verify this in the ablation study. In fact, with
limited data and co-training with contrastive learning, a complex
diffusion model is rather unstable and leads to poor performance.
The L-net is a denoising network composed of stacked linear
layers that learn to denoise the noisy embeddings generated by the
diffusion model. The equation for the L-net is as follows:
ˆ𝒛𝑚
𝑖=𝑓𝑙
𝜃(˜𝒛𝑚
𝑖,𝑡)=𝜎(𝑊𝑙
𝑡˜𝒛𝑚
𝑖+𝑏𝑙
𝑡) (9)
where𝑓𝑙
𝜃is the L-net with parameter 𝜃,𝑡is the diffusion step, 𝜎
stands for ReLU, and 𝑊𝑙
𝑡and𝑏𝑙
𝑡are corresponding parameters of
the l-th linear layer at step 𝑡. It outputs ˆ𝒛𝑚
𝑖, the denoised embedding
of entity𝑒𝑖from modality 𝑚, where ˜𝒛𝑚
𝑖is the noisy embedding of
entity𝑒𝑖from modality 𝑚generated by the diffusion model.
3.2.1 Generation. Then, we feed the embeddings into the diffu-
sion model to generate new embeddings ˜𝒛𝑚
𝑖as follows:
˜𝒛𝑚
𝑖=𝒛𝑚
𝑖+𝜖𝑡 (10)
where𝜖𝑡∼N( 0,𝛽𝑡𝐼)is a Gaussian noise vector with variance 𝛽𝑡,
and𝑡is the diffusion step. The diffusion model learns to denoise the
 
1634SimDiff: Simple Denoising Probabilistic Latent Diffusion Model
for Data Augmentation on Multi-modal Knowledge Graph KDD ’24, August 25–29, 2024, Barcelona, Spain
noisy embeddings using a denoising network 𝑓𝜃in Eq. (9). The diffu-
sion model is trained by minimizing the following L2 reconstruction
loss:
L𝑟𝑒𝑐=1
𝑇𝑇∑︁
𝑡=1𝒛𝑚
𝑖−ˆ𝒛𝑚
𝑖2
2(11)
where𝑇is the total number of diffusion steps.
3.3 Application on EA in MMKGs
A Multi-Modal Knowledge Graph (MMKG) is an extension of a
traditional knowledge graph that incorporates multiple modalities
of information, such as text, images, and audio. Formally, an MMKG
can be defined as 𝐺=(𝐸,𝑅,𝐴,𝑀,𝐹), where𝐸,𝑅,𝐴,𝑀 represent the
entity, relation attribute, multi-modal features sets respectively and
𝐹is the triples representing the facts.
In the context of MMKG, the goal of Entity Alignment (EA)
is to find a mapping function 𝑓:𝐸1→𝐸2that aligns equiva-
lent entities from two MMKGs 𝐺1=(𝐸1,𝑅1,𝐴1,𝑉1,𝐹1)and𝐺2=
(𝐸2,𝑅2,𝐴2,𝑉2,𝐹2), considering not only the structural information
but also the multi-modal features. The 𝑀is replaced with 𝑉, the
image set for the datasets in this paper.
We use weighted concatenation to obtain the joint entity em-
bedding which can adaptively assign different weights to different
modalities according to their importance and relevance. Formally,
given an entity 𝑒𝑖with embeddings from different modalities 𝒉𝑚
𝑖,
where𝑚∈𝑣,𝑟,𝑎,𝑛,𝑐 , we obtain its joint embedding ˆ𝒉𝑖by:
ˆ𝒉𝑖=Ê
𝑚∈Mexp(𝑤𝑚)Í
𝑗∈Mexp(𝑤𝑗)𝒉𝑚
𝑖
, (12)
Éstands for the concatenation and 𝑤𝑚is the weight for modal-
ity𝑚. We also obtain its semantic embedding 𝒉𝑠𝑒𝑚
𝑖and topology
embedding 𝒉𝑡𝑜𝑝
𝑖by using different subsets of modalities: 𝒉𝑠𝑒𝑚
𝑖=É
𝑚𝒉𝑚
𝑖,𝑚=𝑣,𝑐,𝑛,𝑎 ,𝒉𝑡𝑜𝑝
𝑖=É
𝑚𝒉𝑚
𝑖,𝑚=𝑔,𝑟
The semantic embedding captures the image, name, and attribute
information of an entity, while the topology embedding captures
the structural and relational information of an entity.
3.3.1 Multi-view contrastive learning. We use contrastive learning
to align the entities from different multi-modal knowledge graphs.
We use InfoNCE-based loss [ 38] for entity alignment, which mea-
sures the similarity between the representations of the same entity
from different graphs and maximizes it while minimizing the simi-
larity with other entities. The contrastive learning is performed at
three levels: low-level with each modality, mid-level with topology
or semantic representation, and high-level using the entity repre-
sentation as shown in Figure 2. Low-level contrastive learning is
applied to each modality embedding of the entity, such as image,
name, attribute, relation, and structure:
L𝑚=−E(ℎ1𝑖,ℎ2𝑖)∼𝑃𝑝𝑜𝑠"
logexp(𝑓𝑚(ℎ1𝑖,ℎ2𝑖)/𝜏1)
Í𝐾
𝑗=1exp(𝑓𝑚(ℎ1𝑖,ℎ2𝑗)/𝜏1)#
(13)
where𝑓𝑚(·)is the similarity function for modality 𝑚,𝜏1is a tem-
perature parameter, and 𝐾represents the size of negative sample
set.𝑃𝑝𝑜𝑠means the positive pairs distribution of entities that are
aligned across MMKGs, 𝐿𝑚indicates how likely that 𝑒𝑖
1and𝑒𝑖
2
are aligned given their modality embedding. Mid-level contrastive
learning is applied to the semantic in Eq. (3.3) and topology inEq.(3.3) embedding of the entity. High-level contrastive learning is
applied to the joint embedding of the entity, which is obtained by
weighted concatenation of all modalities as shown in (12).
Optimization function The total contrastive loss combines all
the three level views and is represented as:
L𝑐𝑜𝑛=∑︁
𝑚∈ML𝑚+L𝑠𝑒𝑚+L𝑡𝑜𝑝+L𝑗𝑜𝑖𝑛𝑡. (14)
The overall goal together with the diffusion loss is
L=∑︁
𝑚∈ML𝑟𝑒𝑐+∑︁
𝑗L𝑗
𝑐𝑜𝑛, (15)
where𝑗=𝑝,ℎ,𝑧 , the perturbation view, original view and the
generated view.
4 Experiments
4.1 Experimental Setup
Datasets. There are two classes of datasets for EA on MMKGs:
bilingual datasets and cross-KG datasets. The bilingual datasets
consist of four versions of DBP15K [ 27], which are ZH, JA, FR,
and EN denoting Chinese, Japanese, French, and English, respec-
tively. The EA tasks are on X-EN, where X can be ZH, JA, or FR.
The cross-KG datasets include two pairs of KGs: FB15K-DB15K
and FB15K-YAGO15K [ 30], where FB15K, DB15K, and YAGO15K
are subsets of Freebase [ 2], DBpedia [ 1], and YAGO [ 33], respec-
tively. Please refer to Table 5 for the detailed statistics for the above
datasets. Each dataset contains heterogeneous data modalities for
entities, such as text (names and descriptions), images, and graph
structures (relations and neighbors). Since some entities do not
have the corresponding visual data in the datasets, we adopt the
settings in EVA [ 27] and MCLEA [ 26] to replace the missing ones
with a random vector to represent the visual information. For each
dataset, a certain percentage of seed entities are given to train the
model. For DBP15K, we use 30% of seed entities as in [ 27]. For
cross-KG datasets, we use 20%, 50%, and 80% of seed entities as
in [30].
Baselines. We have chosen 21 strong EA methods as baselines.
Structure-based methods utilize only structural information of KGs
to align entities, such as BootEA [ 43], MUGNN [ 4], KECG [ 25],
NAEA [ 65], and AliNet [ 44]. Auxiliary-enhanced methods incorpo-
rate auxiliary information such as names and descriptions to im-
prove the alignment performance, such as MultiKE [ 61], HMAN [ 54],
RDGCN [ 52], AttrGNN [ 31], BERT-INT [ 45] and ERMC [ 55]. Multi-
modal EA methods with proper fusion and alignment strategy
for multi-modal data including MMEA [ 7], HMEA [ 17], PoE [ 30],
EVA [ 28], MCLEA [ 26] and MSNEA [ 8]. We reproduced the MSNEA
based on their public code and used the iterative training strategy as
MCLEA. Unsupervised methods do not require any seed entity for
training, such as RREA [ 36], MRAEA [ 35], EASY [ 16], and SEU [ 34].
Implementation. We introduce the details of the implementation
as follows: The optimizer used is AdamW and we train the model
with 1000 epochs with the last 500 epochs as the iterative train-
ing as MCLEA [ 26] with batch size 3000. Regarding the hidden
dimension: The image embedding is obtained from the pre-trained
CNN: ResNet-152 with dimension 2048 for the bilingual datasets
and VGG-16 with dimension 4096 for the cross-KG datasets simi-
lar to MCLEA. The attribute and relational embedding with size
 
1635KDD ’24, August 25–29, 2024, Barcelona, Spain Ran Li et al.
Table 1: Results on two cross-KG datasets with different seed entity percentage 20%, 50% and 80% as the training signal. The
results of EVA reproduced from MCLEA paper. The results of MSNEA are reproduced following the same setting as MCLEA.
20% 50% 80%
Methods Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRRFB15K
-DB15KMMEA 0.265 0.541 0.357 0.417 0.703 0.512 0.590 0.869 0.685
EVA∗ 0.134 0.338 0.201 0.223 0.471 0.307 0.370 0.585 0.444
HMEA 0.127 0.369 – 0.262 0.581 – 0.417 0.786 –
PoE 0.126 0.251 0.170 0.464 0.658 0.533 0.666 0.820 0.721
MSNEA∗ 0.158 0.403 0.242 0.365 0.662 0.462 0.572 0.821 0.659
MCLEA 0.445 0.705 0.534 0.573 0.800 0.652 0.730 0.883 0.784
SimDiff(Ours) 0.615 0.820 0.678 0.731 0.880 0.786 0.829 0.929 0.865
Improv. best % 38.20 16.31 26.97 27.57 10.00 20.55 13.56 5.21 10.33FB15K
-YAGO15KMMEA 0.234 0.480 0.317 0.403 0.645 0.486 0.598 0.839 0.682
EVA∗ 0.098 0.276 0.158 0.240 0.477 0.321 0.394 0.613 0.471
HMEA 0.105 0.313 – 0.265 0.581 – 0.433 0.801 –
PoE 0.113 0.229 0.154 0.347 0.536 0.414 0.573 0.746 0.635
MSNEA∗ 0.145 0.357 0.221 0.389 0.660 0.479 0.605 0.821 0.677
MCLEA 0.388 0.641 0.474 0.543 0.759 0.616 0.653 0.835 0.715
SimDiff(Ours) 0.530 0.736 0.595 0.659 0.820 0.716 0.743 0.886 0.791
Improv. best % 36.6 14.82 25.53 21.36 8.037 16.23 13.78 6.108 10.63
1000 are extracted from the bag-of-word model. The name and
character embedding, which have a size of 300, are from the GloVe
model. The structure information is encoded by Graph Attention
Network (GAT). Before feeding into our model, we unify the hidden
dimension to 300 for the above modality embedding. The scale of
the perturbation is 0.001 to control the noise added to the original
embedding. For contrastive learning , the temperature 𝜏1for the
uni-modal and concatenated embedding is 0.1. For latent diffusion
model, the hidden dimension is 64 for a 4 layer L-net as encoder
and decoder. Since surface forms (SF), name information are very
helpful for entity alignment in bilingual case [ 31], we divide our
setting into two depending on whether the surface forms are in-
cluded as MCLEA. For the cross-KG tasks, we do not incorporate
the surface forms to do the comparison. Besides, we conduct EA
without the seed entity as the unsupervised tasks, where name (-N)
or image(-V) is used to measure the entity similarity and create
’aligned’ entities.
Evaluation Method . We employ three metrics for evaluation pur-
poses: Hits@1 represents the percentage of correct alignments
among the top-ranked candidates. Similarly, Hits@10 considers
the top 10 ranked candidates instead of the single top one. Mean
Reciprocal Rank (MRR) is the averaged reciprocal ranks given a set
of aligned entities.
4.2 Main results on cross-KG datasets
The table 1 shows the comparative experimental results on two
cross-KG datasets. The datasets are FB15K-DB15K and FB15K-YAGO15K,
which are derived from the original FB15K dataset by aligning it
with DBpedia and YAGO, respectively. The model will align the
same entity across different MMKGs with part of the seed entity as
training data. The percentage of seed entities used for training is
denoted by X%.The table shows that the proposed model (SimDiff) demonstrates
the best performance across all scenarios, achieving the highest
scores on all metrics on both datasets. The improvement over the
best baseline model (MCLEA) is significant, ranging from 6.1%
to 38.20%. This indicates that the proposed model can effectively
learn cross-KG entity embeddings and capture the semantic and
structural information of KGs. It further shows that the performance
improves as more seed entity used to guide the training which is
expected since more training data can help the models learn how
to align entities better. The improvement is particularly evident
when fewer seed entities are used for training (20% and 50%). This
indicates that SimDiff is more effective at leveraging limited training
data to achieve better performance. We will further show this in
the following part.
4.3 Main results on bilingual KG datasets
We present the results of Entity Alignment (EA) in bilingual Knowl-
edge Graph (KG) datasets under various settings: supervised (with
and without surface forms) and unsupervised (without pre-aligned
seed entities). The tasks are three X-ENs, where X can be ZH, JA
or FR, which are based on the original English DBpedia dataset by
aligning it with Chinese, Japanese and French versions, respectively.
Table 2 presents the results.
With or without surface forms We use two settings: with sur-
face forms (w/ SF) and without surface forms (w/o SF). Surface
forms are the natural language expressions of entities, such as la-
bels or names. They can help to bridge the gap between different
languages and enhance the entity alignment accuracy. In both cases
our method outperforms all other methods. It’s worth mentioning
that while entity descriptions can provide a performance boost,
they are not always available for every entity, and their collection
can be labor-intensive. However, our proposed model that encour-
ages information exchange among different modalities still beats
 
1636SimDiff: Simple Denoising Probabilistic Latent Diffusion Model
for Data Augmentation on Multi-modal Knowledge Graph KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: The results comparison on three bilingual datasets in different settings: without using Surface Forms (w/o SF), with
Surface Forms (w/SF) and unsupervised(unsup) setting.
MethodsDBP15K𝑍𝐻−𝐸𝑁 DBP15K𝐽𝐴−𝐸𝑁 DBP15K𝐹𝑅−𝐸𝑁
Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@1 Hits@10 MRRw/o
SFKECG [25] .478 .835 .598 .490 .844 .610 .486 .851 .610
MUGNN [4] .494 .844 .611 .501 .857 .621 .495 .870 .621
AliNet [44] .539 .826 .628 .549 .831 .645 .552 .852 .657
BootEA [43] .629 .847 .703 .622 .854 .701 .653 .874 .731
NAEA [65] .650 .867 .720 .641 .873 .718 .673 .894 .752
EVA [28] .761 .907 .814 .762 .913 .817 .793 .942 .847
MCLEA[26] .816 .948 .865 .812 .952 .865 .834 .975 .885
SimDiff(Ours) .829 .963 .877 .835 .966 .883 .861 .980 .905w/
SFMultiKE [61] .437 .516 .466 .570 .643 .596 .714 .761 .733
HMAN [54] .562 .851 – .567 .969 – .540 .871 –
RDGCN [52] .708 .846 – .767 .895 – .886 .957 –
AttrGNN [31] .777 .920 .829 .763 .909 .816 .942 .987 .959
BERT-INT [45] .968 .990 .977 .964 .991 .975 .992 .998 .995
ERMC [55] .903 .946 .899 .942 .944 .925 .962 .982 .973
MCLEA[26] .972 .996 .981 .986 .999 .991 .997 1.00 .998
SimDiff(Ours) .974 .998 .983 .989 .999 .993 .997 1.00 .998unsupMRAEA .778 .832 – .889 .927 – .950 .970 –
RREA .822 .964 – .918 .978 – .963 .992 –
EASY .898 .979 .930 .943 .990 .960 .980 .998 .990
SEU .900 .965 .924 .956 .991 .969 .988 .999 .992
MCLEA-V .959 .995 .974 .977 .999 .987 .990 1.00 .994
MCLEA-N* .955 .994 .971 .983 .999 .990 .995 1.00 .997
SimDiff-N .959 .995 .973 .982 .999 .989 .997 1.00 .998
SimDiff-V .964 .996 .976 .984 .999 .990 .995 1.00 .997
the baselines even without surface forms, showing its robustness
and efficiency.
Unsupervised learning The table 2 also shows the experiment
in an unsupervised setting, where no labels are used for entity
alignment. our proposed model (SimDiff) has two variants: SimDiff-
V and SimDiff-N, which use different ways to measure the similarity
of the entities and create the entity pairs. SimDiff-V uses the image
signal to measure the similarity, while SimDiff-N uses the name to
measure the similarity. The table shows that SimDiff-V achieves
the best performance. This indicates that our proposed model can
effectively align bilingual entities without using any labels.
4.4 Efficiency (Training time)
We evaluate the efficiency of our model compared with MCLEA. As
shown in Table 3, we compare the training time of our model, SimD-
iff with a strong baseline model MCLEA [ 26] on different datasets
and report the seconds per epoch. To make a fair comparison, we
use the same setting with MCLEA [ 26] including the epochs, batch
size and other hyper-parameters. Our design requires the diffusion
process and thus takes a longer time to train but the improved the
performance by the generated data is also significant.
4.5 Ablation study
The table 4 shows the ablation study by analyzing the contribution
of different components on the performance in cross-KG datasets.Table 3: Comparison of efficiency on different datasets.
Dataset Model Training Time per epoch (s)
FB15K-DB15KSimDiff 4.4
MCLEA 3.7
FB15K-YAGO15KSimDiff 3.6
MCLEA 3.9
Each component of our proposed model contributes to improving
the performance.
The most important component is the augmentation, especially
the structure-wise augmentation. Removing it results in the largest
drop in performance, showing that the augmented embedding can
help alleviate the issue of limited seed entities. For the modality
aspect, the structure information is most critical to the MMKG. This
verifies the importance of the structural information in MMKG that
reflects the connection between different entities and the different
data modalities. sem_emb, which is the concatenated embedding on
the semantic level; top_emb, which is the concatenated embedding
on the topology level; and joint_emb, which is the concatenated
embedding on the entity level. These enhance contrastive learning
with multi-view embeddings.
 
1637KDD ’24, August 25–29, 2024, Barcelona, Spain Ran Li et al.
0.000.050.100.150.200.250.300.350.400.450.500.550.60
Seed Entity Percentage0.30.40.50.60.7Hit@1
FB15K-DB15K
MCLEA
OURS
0.000.050.100.150.200.250.300.350.400.450.500.550.60
Seed Entity Percentage0.40.50.60.70.8MRR
FB15K-DB15K
MCLEA
OURS
0.000.050.100.150.200.250.300.350.400.450.500.550.60
Seed Entity Percentage0.20.30.40.50.60.7Hit@1
FB15K-YAGO15K
MCLEA
OURS
0.000.050.100.150.200.250.300.350.400.450.500.550.60
Seed Entity Percentage0.30.40.50.60.7MRR
FB15K-YAGO15K
MCLEA
OURS
Figure 3: Performance comparison of MCLEA and our model on two cross-KG datasets with different percentages of seed
entities. Our model achieves better or comparable results with fewer seed entities, demonstrating its efficiency and robustness
for cross-KG entity alignment.
Table 4: An ablation study is conducted by analyzing the
contribution of different components to the performance in
cross-KG datasets. w/o_aug X represents the SimDiff model
without augmenting the modality X. aug_emb stands for the
augmented embedding. sem_emb, top_emb and joint_emb
represent the concatenated embedding on semantic, topology
and the entity level respectively
MethodsFB15K-DB15K FB15K-YAGO15K
Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR
SimDiff(Ours) 0.615 0.820 0.678 0.530 0.736 0.595Mo
dalityw/o_aug structure 0.451 0.710 0.532 0.409 0.644 0.488
w/o_aug visual 0.601 0.811 0.674 0.520 0.713 0.587
w/o_aug relation 0.596 0.812 0.671 0.518 0.721 0.590
w/o_aug attribute 0.607 0.812 0.676 0.503 0.702 0.571Multi-vie
ww/o aug_emb 0.468 0.729 0.551 0.423 0.663 0.506
w/o sem_emb 0.600 0.818 0.675 0.526 0.727 0.595
w/o top_emb 0.601 0.820 0.675 0.524 0.727 0.594
w/o joint_emb 0.497 0.727 0.577 0.478 0.671 0.547
4.6 Ablation study on the diffusion model
design
We try to use more popular and powerful diffusion models, the
Diff(U-net) and Diff(attention) in our scenario, which are based
on the popular diffusion model in the image domain [ 13,22]. The
performance is relatively bad as shown in Table. 6. The Diff(U-net)
uses the U-net as the backbone model and the Diff(attention) further
incorporates the self-attention mechanism to learn the multi-modal
information.
However, the results are not ideal. This is because the size of the
training data is small and the multi-modal features including the
graph topology make the training quite unstable.4.7 Improved data usability with our generative
model
Shown in Figure 3, the models are trained with different percentages
of seed entities. We compared SimDiff with the MCLEA on cross-
KG datasets. Our model achieves better or comparable performance
with fewer seed entities than MCLEA. For example, on the FB15K-
YAGO15K dataset, our model (25%) outperforms MCLEA (50%) on
both metrics, which means that our model can align entities across
MMKGs more accurately with only half of the seed entities used.
On the FB15K-DB15K dataset, our model (25%) also achieves better
results than MCLEA (50%) on Hits@1.
This suggests that the generated embeddings as augmentation
can improve the training of the contrastive learning. The improve-
ment is more significant with fewer seed entities used. It further
suggests that the training of contrastive learning and our multi-
modal latent diffusion model are possible with only a small number
of the seed entities (5%).
4.8 Quality of generated embeddings
As shown in Figure 4, the performance of two versions of model
MCLEA on the two cross-KG datasets, FB15K-DB15K and FB15K-
YAGO15K. MCLEA(diff) is trained with the generated data from
our diffusion model, while MCLEA is trained with the original data.
MCLEA(diff) exceeds MCLEA on both datasets, which reveals that
our methods can produce better embeddings for cross-KG entity
alignment. Our SimDiff method generates better embedding for
cross-KG entity alignment by allowing the information exchange
among different data modalities.
4.9 Hyper-parameters
We evaluate the effect of the hidden dimension of the diffusion
model on our proposed model. The results of the hidden dimension
with 16, 32, 64, 128 and 256 on the two cross-KG datasets, FB15K-
DB15K and FB15K-YAGO15K, are reported. Figure 5 shows that our
model achieves the best performance with a hidden dimension of
64 on both datasets. A smaller or larger hidden dimension leads
to a lower score, which indicates that there is an optimal trade-off
between the model complexity and the representation capacity.
 
1638SimDiff: Simple Denoising Probabilistic Latent Diffusion Model
for Data Augmentation on Multi-modal Knowledge Graph KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 5: The datasets statistics of both the cross-KG and bilingual tasks.
Task KG dataset Entity Rel. Attr. Rel. Triples Attr. Triples Image pre-aligned pairs
FB15K-DB15KFB15K 14,951 1,345 116 592,213 29,395 13,44412,846DB15K 12,842 279 225 89,197 48,080 12,837
FB15K-YAGO15KFB15K 14,951 1,345 116 592,213 29,395 13,44411,199YAGO15K 15,404 32 7 122,886 23,532 11,194
DBP15K𝑍𝐻−𝐸𝑁ZH(Chinese) 19,388 1,701 8,111 70,414 248,035 15,91215,000EN(English) 19,572 1,323 7,173 95,142 343,218 14,125
DBP15K𝐽𝐴−𝐸𝑁JA(Japanese) 19,814 1,299 5,882 77,214 248,991 12,73915,000EN(English) 19,780 1,153 6,066 93,484 320,616 13,741
DBP15K𝐹𝑅−𝐸𝑁FR(French) 19,661 903 4,547 105,998 273,825 14,17415,000EN(English) 19,993 1,208 6,422 115,722 351,094 13,858
Table 6: An ablation study of the diffusion models. We com-
pare ours with the popular diffusion models in image domain
that utilize the U-net structure and attention mechanism.
MethodsFB15K-DB15K FB15K-YAGO15K
Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR
SimDiff(Ours) 0.615 0.820 0.678 0.530 0.736 0.595
Diff(U-net) 0.106 0.499 0.166 0.098 0.489 0.157
Diff(Attention) 0.097 0.480 0.155 0.091 0.471 0.150
H@1 H@10 MRR
Metrics0.00.10.20.30.40.50.60.7ScoresFB15K-DB15K
MCLEA
MCLEA(diff)
H@1 H@10 MRR
Metrics0.00.10.20.30.40.50.60.7ScoresFB15K-YAGO15K
MCLEA
MCLEA(diff)
Figure 4: The performance of MCLEA and MCLEA(diff) on
two cross-KG datasets. MCLEA(diff) is trained with embed-
dings generated by our methods, which incorporate and fuse
different modality information. Our methods improve the
performance of MCLEA on both datasets, as shown by the
higher values of Hits@1, Hits@10, and MRR.
5 Conclusion
In conclusion, this work presents a novel approach to data aug-
mentation in Multi-Modal Knowledge Graphs (MMKGs) using a
diffusion-based generative model, SimDiff. We have demonstrated
that SimDiff can effectively handle different data modalities and
enhance the utilization of multi-modal data, even in situations with
limited training data. Our model can fuse different data modalities
in a unified latent space, allowing information exchange among
16 32 64 128 256
Hidden dimension of diffusion model0.560.580.600.620.640.660.68score
FB15K-DB15K
Hit@1
MRR
16 32 64 128 256
Hidden dimension of diffusion model 0.480.500.520.540.560.580.600.62score
FB15K-YAGO15K
Hit@1
MRRFigure 5: Performance with different sizes of diffusion hidden
dimension on cross-KG datasets is shown by Hits@1 and
MRR.
different modalities. It can generate realistic and diverse embedding
for MMKGs, which can be used as augmented data for downstream
Entity Alignment (EA) tasks. Our method has shown promising
results in these tasks, outperforming previous methods by further
enhancing the power of contrastive learning. With only half of
the seed entities used, we can achieve better or comparable perfor-
mance.
Acknowledgments
Lei Chen’s work is partially supported by National Key Research
and Development Program of China Grant No. 2023YFF0725100,
National Science Foundation of China (NSFC) under Grant No.
U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project
R6020-19, AOE Project AoE/E-603/18, Theme-based project TRS
T41-603/20R, CRF Project C2004-21G, Guangdong Province Science
and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF
grants MHX/078/21 and PRP/004/22FX, Zhujiang scholar program
2021JC02X170, Microsoft Research Asia Collaborative Research
Grant and HKUST-Webank joint research lab grants. Shimin DI is
the corresponding author of this paper.
 
1639KDD ’24, August 25–29, 2024, Barcelona, Spain Ran Li et al.
References
[1]Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In international
semantic web conference. Springer, 722–735.
[2]Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph database for structuring human
knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on
Management of data. 1247–1250.
[3]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[4]Yixin Cao, Zhiyuan Liu, Chengjiang Li, Juanzi Li, and Tat-Seng Chua. 2019.
Multi-channel graph neural network for entity alignment. arXiv preprint
arXiv:1908.09898 (2019).
[5]Heng Chang, Jie Cai, and Jia Li. 2023. Knowledge Graph Completion with
Counterfactual Augmentation. Proceedings of the ACM Web Conference 2023
(2023). https://api.semanticscholar.org/CorpusID:257219892
[6]Jatin Chauhan, Priyanshu Gupta, and Pasquale Minervini. 2021. A Probabilistic
Framework for Knowledge Graph Data Augmentation. ArXiv abs/2110.13205
(2021). https://api.semanticscholar.org/CorpusID:239885786
[7]Liyi Chen, Zhi Li, Yijun Wang, Tong Xu, Zhefeng Wang, and Enhong Chen.
2020. MMEA: Entity Alignment for Multi-modal Knowledge Graph. In Interna-
tional Conference on Knowledge Science, Engineering and Management. Springer,
134–147.
[8]Liyi Chen, Zhi Li, Tong Xu, Han Wu, Zhefeng Wang, Nicholas Jing Yuan, and
Enhong Chen. 2022. Multi-modal siamese network for entity alignment. In
Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data
mining. 118–126.
[9]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.
A simple framework for contrastive learning of visual representations. arXiv
preprint arXiv:2002.05709 (2020).
[10] Yu Chen, Lingfei Wu, and Mohammed J Zaki. 2019. Bidirectional attentive
memory networks for question answering over knowledge bases. arXiv preprint
arXiv:1903.02188 (2019).
[11] Zihang Dai, Lei Li, and Wei Xu. 2016. Cfo: Conditional focused neural question
answering with large-scale knowledge bases. arXiv preprint arXiv:1606.01994
(2016).
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:
A large-scale hierarchical image database. 2009 IEEE conference on computer vision
and pattern recognition (2009), 248–255.
[13] Prafulla Dhariwal and Alex Nichol. 2021. Diffusion Models Beat GANs on Image
Synthesis. arXiv:2105.05233 [cs.LG]
[14] Carl Doersch. 2016. Tutorial on variational autoencoders. arXiv preprint
arXiv:1606.05908 (2016).
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al .2020. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).
[16] Congcong Ge, Xiaoze Liu, Lu Chen, Baihua Zheng, and Yunjun Gao. 2021. Make
it easy: An effective end-to-end entity alignment framework. In Proceedings of
the 44th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 777–786.
[17] Hao Guo, Jiuyang Tang, Weixin Zeng, Xiang Zhao, and Li Liu. 2021. Multi-modal
entity alignment in hyperbolic space. Neurocomputing 461 (2021), 598–607.
[18] Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang, Wanqian Zhang, Boyang
Li, and Mu Li. 2022. MixGen: A New Multi-Modal Data Augmentation. 2023
IEEE/CVF Winter Conference on Applications of Computer Vision Workshops
(WACVW) (2022), 379–389. https://api.semanticscholar.org/CorpusID:249712192
[19] GM Harshvardhan, Mahendra Kumar Gourisaria, Manjusha Pandey, and Sid-
dharth Swarup Rautaray. 2020. A comprehensive survey and analysis of
generative models in machine learning. Comput. Sci. Rev. 38 (2020), 100285.
https://api.semanticscholar.org/CorpusID:224930744
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[21] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song
Bai, and Xiaojuan Qi. 2022. Is synthetic data from generative models ready for
image recognition? arXiv preprint arXiv:2210.07574 (2022).
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in neural information processing systems 33 (2020), 6840–6851.
[23] Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. 2023. Exploit-
ing asymmetry for synthetic training data generation: Synthie and the case of
information extraction. arXiv preprint arXiv:2303.04132 (2023).
[24] Sosuke Kobayashi. 2018. Contextual augmentation: Data augmentation by words
with paradigmatic relations. arXiv preprint arXiv:1805.06201 (2018).[25] Chengjiang Li, Yixin Cao, Lei Hou, Jiaxin Shi, Juanzi Li, and Tat-Seng Chua. 2019.
Semi-supervised entity alignment via joint knowledge embedding model and
cross-graph model. Association for Computational Linguistics.
[26] Zhenxi Lin, Ziheng Zhang, Meng Wang, Yinghui Shi, Xian Wu, and Yefeng Zheng.
2022. Multi-modal contrastive representation learning for entity alignment. arXiv
preprint arXiv:2209.00891 (2022).
[27] Fangyu Liu, Muhao Chen, Dan Roth, and Nigel Collier. 2020. Visual Pivoting for
(Unsupervised) Entity Alignment. arXiv:2009.13603 [cs.CL]
[28] Fangyu Liu, Muhao Chen, Dan Roth, and Nigel Collier. 2021. Visual pivoting
for (unsupervised) entity alignment. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 35. 4257–4266.
[29] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping
Wang. 2020. K-bert: Enabling language representation with knowledge graph. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 2901–2908.
[30] Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, and
David S Rosenblum. 2019. MMKG: Multi-Modal Knowledge Graphs. In European
Semantic Web Conference. Springer, 287–302.
[31] Zhiyuan Liu, Yixin Cao, Liangming Pan, Juanzi Li, and Tat-Seng Chua. 2020.
Exploring and evaluating attributes, values, and structures for entity alignment.
arXiv preprint arXiv:2010.03249 (2020).
[32] Zichang Liu, Zhiqiang Tang, Xingjian Shi, Aston Zhang, Mu Li, Anshumali Shri-
vastava, and Andrew Gordon Wilson. 2022. Learning Multimodal Data Augmen-
tation in Feature Space. ArXiv abs/2212.14453 (2022). https://api.semanticscholar.
org/CorpusID:255340832
[33] Farzaneh Mahdisoltani, Joanna Biega, and Fabian M Suchanek. 2013. Yago3: A
knowledge base from multilingual wikipedias. In CIDR.
[34] Xin Mao, Wenting Wang, Yuanbin Wu, and Man Lan. 2021. From alignment to
assignment: Frustratingly simple unsupervised entity alignment. arXiv preprint
arXiv:2109.02363 (2021).
[35] Xin Mao, Wenting Wang, Huimin Xu, Man Lan, and Yuanbin Wu. 2020. MRAEA:
an efficient and robust entity alignment approach for cross-lingual knowledge
graph. In Proceedings of the 13th International Conference on Web Search and Data
Mining. 420–428.
[36] Xin Mao, Wenting Wang, Huimin Xu, Yuanbin Wu, and Man Lan. 2020. Rela-
tional reflection entity alignment. In Proceedings of the 29th ACM International
Conference on Information & Knowledge Management. 1095–1104.
[37] Salman Mohammed, Peng Shi, and Jimmy Lin. 2017. Strong baselines for simple
question answering over knowledge graphs with and without neural networks.
arXiv preprint arXiv:1712.01969 (2017).
[38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[39] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP). 1532–1543.
[40] Guang pu Li, Zequn Sun, Lei Qian, Qiang Guo, and Wei Hu. 2021. Rule-based
data augmentation for knowledge graph embedding. AI Open 2 (2021), 186–196.
https://api.semanticscholar.org/CorpusID:244630255
[41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolu-
tional networks for biomedical image segmentation. In Medical Image Computing
and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234–241.
[42] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[43] Zequn Sun, Wei Hu, Qingheng Zhang, and Yuzhong Qu. 2018. Bootstrapping
entity alignment with knowledge graph embedding.. In IJCAI, Vol. 18.
[44] Zequn Sun, Chengming Wang, Wei Hu, Muhao Chen, Jian Dai, Wei Zhang, and
Yuzhong Qu. 2020. Knowledge graph alignment network with gated multi-hop
neighborhood aggregation. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 34. 222–229.
[45] Xiaobin Tang, Jing Zhang, Bo Chen, Yang Yang, Hong Chen, and Cuiping Li. 2020.
BERT-INT: a BERT-based interaction model for knowledge graph alignment.
interactions 100 (2020), e1.
[46] Zhenwei Tang, Shichao Pei, Zhao Zhang, Yongchun Zhu, Fuzhen Zhuang, R.
Hoehndorf, and Xiangliang Zhang. 2022. Positive-Unlabeled Learning with Adver-
sarial Data Augmentation for Knowledge Graph Completion. In International Joint
Conference on Artificial Intelligence. https://api.semanticscholar.org/CorpusID:
248496746
[47] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip
Isola. 2020. What makes for good views for contrastive learning? Advances in
neural information processing systems 33 (2020), 6827–6839.
[48] Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani,
Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. 2018.
Training deep networks with synthetic data: Bridging the reality gap by domain
randomization. In Proceedings of the IEEE conference on computer vision and
pattern recognition workshops. 969–977.
[49] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
 
1640SimDiff: Simple Denoising Probabilistic Latent Diffusion Model
for Data Augmentation on Multi-modal Knowledge Graph KDD ’24, August 25–29, 2024, Barcelona, Spain
[50] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu,
Juanzi Li, and Jian Tang. 2021. KEPLER: A unified model for knowledge embed-
ding and pre-trained language representation. Transactions of the Association for
Computational Linguistics 9 (2021), 176–194.
[51] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat:
Knowledge graph attention network for recommendation. In Proceedings of the
25th ACM SIGKDD international conference on knowledge discovery & data mining .
950–958.
[52] Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Rui Yan, and Dongyan Zhao.
2019. Relation-aware entity alignment for heterogeneous knowledge graphs.
arXiv preprint arXiv:1908.08210 (2019).
[53] Yikun Xian, Zuohui Fu, Shan Muthukrishnan, Gerard De Melo, and Yongfeng
Zhang. 2019. Reinforcement knowledge graph reasoning for explainable rec-
ommendation. In Proceedings of the 42nd international ACM SIGIR conference on
research and development in information retrieval. 285–294.
[54] Hsiu-Wei Yang, Yanyan Zou, Peng Shi, Wei Lu, Jimmy Lin, and Xu Sun. 2019.
Aligning cross-lingual entities with multi-aspect information. arXiv preprint
arXiv:1910.06575 (2019).
[55] Jinzhu Yang, Ding Wang, Wei Zhou, Wanhui Qian, Xin Wang, Jizhong Han, and
Songlin Hu. 2021. Entity and relation matching consensus for entity alignment. In
Proceedings of the 30th ACM International Conference on Information & Knowledge
Management. 2331–2341.
[56] Ling Yang, Zhilong Zhang, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia
Shao, Wentao Zhang, Ming-Hsuan Yang, and Bin Cui. 2022. Diffusion Models:
A Comprehensive Survey of Methods and Applications. ArXiv abs/2209.00796
(2022). https://api.semanticscholar.org/CorpusID:252070859
[57] Y. Yao, Zhao Zhang, Yongjun Xu, and Chao Li. 2022. Data Augmentation for
Few-Shot Knowledge Graph Completion from Hierarchical Perspective. In Inter-
national Conference on Computational Linguistics. https://api.semanticscholar.
org/CorpusID:252819516
[58] Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao
Yu, and Lingpeng Kong. 2022. Zerogen: Efficient zero-shot learning via dataset
generation. arXiv preprint arXiv:2202.07922 (2022).
[59] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in
neural information processing systems 33 (2020), 5812–5823.
[60] Kaisheng Zeng, Chengjiang Li, Lei Hou, Juanzi Li, and Ling Feng. 2021. A
comprehensive survey of entity alignment for knowledge graphs. AI Open 2
(2021), 1–13.
[61] Qingheng Zhang, Zequn Sun, Wei Hu, Muhao Chen, Lingbing Guo, and Yuzhong
Qu. 2019. Multi-view knowledge graph embedding for entity alignment. arXiv
preprint arXiv:1906.02390 (2019).
[62] Yin Zhang, Rong Jin, and Zhi-Hua Zhou. 2010. Understanding bag-of-words
model: a statistical framework. International Journal of Machine Learning and
Cybernetics 1, 1-4 (2010), 43–52.
[63] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu.
2019. ERNIE: Enhanced language representation with informative entities. arXiv
preprint arXiv:1905.07129 (2019).
[64] Zhenjie Zhao, Evangelos E. Papalexakis, and Xiaojuan Ma. 2020. Learning Physi-
cal Common Sense as Knowledge Graph Completion via BERT Data Augmen-
tation and Constrained Tucker Factorization. In Conference on Empirical Meth-
ods in Natural Language Processing. https://api.semanticscholar.org/CorpusID:
226262270
[65] Qiannan Zhu, Xiaofei Zhou, Jia Wu, Jianlong Tan, and Li Guo. 2019.
Neighborhood-Aware Attentional Representation for Multilingual Knowledge
Graphs.. In IJCAI. 1943–1949.
A Appendix: Embedding models
A.1 Graph embeddings
In order to capture the structural patterns of 𝑮, we employ a Graph
Attention Network (GAT) [ 49], a kind of neural network that is
proficient in processing graph data. Each node 𝒏has a hidden vector
𝒉𝑔
𝑖inR𝑑, which is refined by gathering the states of its adjacent
nodes (including itself) denoted as N𝑖:
𝒉𝑔
𝑖=𝜎©­
«∑︁
𝑝∈N𝑖𝛽𝑖𝑝𝑾𝑠𝒉𝑔
𝑝ª®
¬, (16)
where 𝑾𝑠∈R𝑑×𝑑is a diagonal matrix that linearly manipulates
the states [ 25].𝜎is the ReLU activation function. The attention
factor𝛽𝑚𝑝evaluates the importance of node 𝒏𝑝for node 𝒏𝑚, andis calculated as:
𝛽𝑖𝑝=exp
𝜙
𝛾⊤h
𝑾𝑠𝒉𝑔
𝑖⊕𝑾𝑠𝒉𝑔
𝑝i
Í
𝑞∈N𝑖exp
𝜙
𝛾⊤
𝑾𝑠𝒉𝑔
𝑖⊕𝑾𝑠𝒉𝑔
𝑞, (17)
where𝛾∈R2𝑑is a tunable vector, ⊕represents concatenation and
𝜙is the LeakyReLU activation function. To ensure the stability
of the learning process, we implement a multi-head attention to
formulate two distinct representations 𝒉𝑔
𝑖for each node on each
GAT layer. We then concatenate these representations to derive the
graph(structure) embedding 𝒉𝑖𝑔on each layer, and utilize the last
GAT layer (L= 2) output 𝒉𝑖𝑔as the final graph (structure) embedding.
𝒉𝑔
𝑖=𝐿Ê
𝑙=1𝜎©­
«∑︁
𝑝∈N𝑖𝛽𝑙
𝑖𝑝𝒉𝑔
𝑝ª®
¬, (18)
where𝛽𝑙
𝑖𝑝is the coefficient of the 𝑙-th attention layer.
A.2 Image embeddings
We use ResNet-152 [ 20] or VGG-16 [ 42], pre-trained visual models
that has been trained on ImageNet [ 12], to learn image embeddings
for entities adopted by the previous MMKG models [ 26]. VGG-16
is the VGG model that have 16 convolution layers. ResNet-152 is
a ResNet model that consists of 152 layers with residual blocks to
make the training on very deep models possible. A residual block
is a subnetwork that adds a shortcut or a skip connection between
its input and output.
The equation for the residual block is as follows:
𝒚=𝐹(𝒙)+𝒙 (19)
where 𝒙is the input, 𝒚is the output, and 𝐹(𝒙)is the residual func-
tion that represents the operations in the block, such as convolution,
batch normalization, and activation.
As shown in Figure 2, we use CNN like ResNet-152 as our “Image
Encoder" to extract image features from the input images. Formally,
given an entity 𝒆𝑖with an image 𝑰𝑖, we obtain its image embedding
𝒉𝑣
𝑖by passing 𝑰𝑖through CNN and taking the output of the last
layer as follows:
𝒉𝑣
𝑖=CNN(𝑰𝑖) (20)
The image embedding 𝒉𝑣
𝑖is a 2048 or 4096-dimensional vector that
captures the high-level visual information of the entity and then
projected before multi-modal processing:
However, this embedding may not be suitable for our multi-
modal fusion and alignment task, because it may contain irrelevant
or redundant information that is not related to the entity’s identity
or semantics. Therefore, we further project the image embedding
into a lower-dimensional space by using a linear layer:
𝒉𝑣
𝑖=𝑾𝑣𝒉𝑣
𝑖+𝒃𝑣 (21)
where 𝑾𝑣and𝒃𝑣are learnable parameters of the linear layer, and 𝒉𝑣
𝑖
is the projected image embedding of entity 𝒆𝑖. The projected image
embedding 𝒉𝑣
𝑖is a 300-dimensional vector that is more compact
and compatible with the other modalities. We use 𝒉𝑣
𝑖as the input
of the diffusion model and the multi-modal fusion component.
 
1641KDD ’24, August 25–29, 2024, Barcelona, Spain Ran Li et al.
A.3 Attribute, relational and name and
character embeddings
Inspired by [ 54] and follow [ 26], we encode the relations, attributes,
and names of entity 𝒆𝑖as bag-of-words [ 62] features. A bag-of-
words feature is to represents the frequency of words for text data,
without considering the order and grammar of the words [ 62]. We
use a simple feed-forward layer to transform the bag-of-words fea-
tures into embeddings 𝒉𝑙
𝑖, where𝑙∈{𝑟,𝑎,𝑛,𝑐}denotes the relation,
attribute, or name type. Formally, we have:
𝒉𝑙
𝑖=𝑾𝑙𝒙𝑙
𝑖+𝒃𝑙 (22)
where 𝑾𝑙and𝒃𝑙represents the feed forward network parameters,
and𝒙𝑙
𝑖is the obtained feature with bag-of-words model for type
𝑙. For the name feature 𝒙𝑛
𝑖, we use the average of the pre-trained
GloVe [ 39] vectors of the name tokens as a baseline. Following the
previous works, we use the character bigrams [ 26,34] of entity
names as complementary information for the missing name ofsome entities since proper nouns (e.g., person names) are missing.
A character bigram is a pair of two adjacent characters in a string,
which can capture some morphological information of the words.
To obtain the bag-of-words feature 𝒙𝑙
𝑖for type𝑙, we first con-
struct a vocabulary 𝑽𝑙that contains all the unique words or char-
acter bigrams that appear in the type 𝑙of all entities. Then, we
represent each word or character bigram as a one-hot vector 𝒗𝑘of
length|𝑽𝑙|, where𝑘is the index of the word or character bigram
in𝑽𝑙. Finally, we compute the bag-of-words feature 𝒙𝑙
𝑖by combin-
ing the one-hot vectors of all the names or character bigrams in
modality𝑙of entity 𝒆𝑖. Formally, we have:
𝒙𝑙
𝑖=|𝑽𝑙|∑︁
𝑘=1𝒗𝑘I(𝑘∈𝒆𝑙
𝑖) (23)
where I(𝑘∈𝒆𝑙
𝑖)is an indicator function that returns 1 if word or
character bigram 𝑘is in type𝑙of entity 𝒆𝑖, and 0 otherwise.
 
1642