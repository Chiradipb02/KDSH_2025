Federated Graph Learning with Structure Proxy Alignment
Xingbo Fu
University of Virginia
Charlottesville, Virginia, USA
xf3av@virginia.eduZihan Chen
University of Virginia
Charlottesville, Virginia, USA
brf3rx@virginia.eduBinchi Zhang
University of Virginia
Charlottesville, Virginia, USA
epb6gw@virginia.edu
Chen Chen
University of Virginia
Charlottesville, Virginia, USA
zrh6du@virginia.eduJundong Li
University of Virginia
Charlottesville, Virginia, USA
jundong@virginia.edu
ABSTRACT
Federated Graph Learning (FGL) aims to learn graph learning mod-
els over graph data distributed in multiple data owners, which has
been applied in various applications such as social recommenda-
tion and financial fraud detection. Inherited from generic Federated
Learning (FL), FGL similarly has the data heterogeneity issue where
the label distribution may vary significantly for distributed graph
data across clients. For instance, a client can have the majority
of nodes from a class, while another client may have only a few
nodes from the same class. This issue results in divergent local
objectives and impairs FGL convergence for node-level tasks, es-
pecially for node classification. Moreover, FGL also encounters a
unique challenge for the node classification task: the nodes from a
minority class in a client are more likely to have biased neighboring
information, which prevents FGL from learning expressive node
embeddings with Graph Neural Networks (GNNs). To grapple with
the challenge, we propose FedSpray, a novel FGL framework that
learns local class-wise structure proxies in the latent space and
aligns them to obtain global structure proxies in the server. Our
goal is to obtain the aligned structure proxies that can serve as reli-
able, unbiased neighboring information for node classification. To
achieve this, FedSpray trains a global feature-structure encoder and
generates unbiased soft targets with structure proxies to regularize
local training of GNN models in a personalized way. We conduct
extensive experiments over four datasets, and experiment results
validate the superiority of FedSpray compared with other baselines.
Our code is available at https://github.com/xbfu/FedSpray.
CCS CONCEPTS
•Computing methodologies →Distributed artificial intelli-
gence; Neural networks.
KEYWORDS
Federated Learning, Graph Neural Network, Knowledge Distillation
ACM Reference Format:
Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, and Jundong Li. 2024.
Federated Graph Learning with Structure Proxy Alignment. In Proceedings of
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671717the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671717
1 INTRODUCTION
Graph Neural Networks (GNNs) [ 46] are a prominent approach
for learning expressive representations from graph-structured data.
Typically, GNNs follow a message-passing mechanism, where the
embedding of each node is computed by aggregating attribute in-
formation from its neighbors [ 11,17,44]. Thanks to their powerful
capacity for jointly embedding attribute and graph structure in-
formation, GNNs have been widely adopted in a wide variety of
applications, such as node classification [ 9,12] and link predic-
tion [ 2,5]. The existing GNNs are mostly trained in a centralized
manner where graph data is collected on a single machine before
training. In the real world, however, a large number of graph data
is generated by multiple data owners. These graph data cannot be
assembled for training due to privacy concerns and commercial
competitions [ 41], which prevents the traditional centralized man-
ner from training powerful GNNs. Taking a financial system with
four banks in Figure 1 as an example, each bank in the system has
its local customer dataset and transactions between customers. As
we take the customers in a bank as nodes and transactions between
them as edges, the bank’s local data can naturally form a graph.
These banks aim to jointly train a GNN model for classification
tasks, such as predicting a customer’s occupation (i.e., Doctor or
Teacher ) without sharing their local data with each other.
Federated Learning (FL) [25] is a prevalent distributed learning
scheme that enables multiple data owners (i.e., clients) to collab-
oratively train machine learning models under the coordination
of a central server without sharing their private data. One critical
challenge in FL is data heterogeneity, where data samples are not
independent and identically distributed (i.e., non-IID) across the
clients. For instance, assume that Bank A in Figure 1 locates in a
community adjacent to a hospital. Then most customers in Bank
A are therefore likely to be labeled as Doctor while only a few cus-
tomers are from other occupations (e.g., Teacher ). In contrast, Bank
C adjoining a school has customers labeled mostly as Teacher and
only a few as Doctor. Typically, the nodes from a class that claims
the very large proportion of the overall data in a client are the
majority nodes (e.g., Doctor in Bank A) while minority nodes (e.g.,
Teacher in Bank A) account for much fewer samples. The data het-
erogeneity issue results in divergent local objectives on the clients
and consequently impairs the performance of FL [ 15]. A number
 
827
KDD ’24, August 25–29, 2024, Barcelona, Spain Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, & Jundong Li
A
B
C
D
Company
Model
Figure 1: An example of a financial system including four
banks. The four banks aim to jointly train a model for pre-
dicting a customer’s occupation (i.e., Doctor orTeacher ) or-
chestrated by a third-party company over their local data
while keeping their private data locally.
of approaches have been proposed to address this issue, to name a
few [20, 42, 45].
When we train GNNs over distributed graph data in a federated
manner, however, the data heterogeneity issue can get much more
severe. This results from a unique challenge in Federated Graph
Learning (FGL) [ 10]:the high heterophily of minority nodes,
i.e., their neighbors are mostly from other classes [ 35]. A majority
node in a client (e.g., Teacher in Bank D) can benefit from the
message-passing mechanism and obtain an expressive embedding
as its neighbors are probably from the same class. On the contrary,
a minority node in another client (e.g., Teacher in Bank A) may
obtain biased information from its neighbors when they are from
other classes (e.g., Doctor in Bank A). In FGL, this challenge is
usually entangled with the data heterogeneity issue. As a result, the
minority nodes will finally get underrepresented embeddings given
adverse neighboring information and be more likely to be predicted
as the major class, which results in unsatisfactory performance.
Although a few studies have investigated the data heterogeneity
issue about graph structures in FGL [ 38,47], they did not fathom
the divergent impact of neighboring information across clients for
node classification.
To tackle the aforementioned challenges in FGL, we propose
FedSpray, a novel FGL framework with structure proxy alignment
in this study. The goal of FedSpray is to learn personalized GNN
models for each client while avoiding underrepresented embed-
dings of the minority nodes in each client caused by their adverse
neighboring information in FGL. To achieve this goal, we first intro-
duce global class-wise structure proxies [ 7] which aim to provide
nodes with informative, unbiased neighboring information, espe-
cially for those from the minority classes in each client. Moreover,
FedSpray learns a global feature-structure encoder to obtain reli-
able soft targets that only depend on node features and aligned
structure proxies. Then, FedSpray uses the soft targets to regu-
larize local training of personalized GNN models via knowledge
distillation [ 13]. We conduct extensive experiments over five graph
datasets, and experimental results corroborate the effectiveness of
the proposed FedSpray compared with other baselines.
We summarize the main contributions of this study as follows.
•Problem Formulation. We formulate and make an initial
investigation on a unique issue of unfavorable neighboring
information for minority nodes in FGL.•Algorithmic Design. We propose a novel framework Fed-
Spray to tackle the above problem in FGL. FedSpray aims
to learn unbiased soft targets by a global feature-structure
encoder with aligned class-wise structure proxies which
provide informative, unbiased neighboring information for
nodes and guide local training of personalized GNN models.
•Experimental Evaluation. We conduct extensive experi-
ments over four graph datasets to verify the effectiveness
of the proposed FedSpray. The experimental results demon-
strate that our FedSpray consistently outperforms the state-
of-the-art baselines.
2 PROBLEM FORMULATION
2.1 Preliminaries
2.1.1 Notations. We use bold uppercase letters (e.g., X) to repre-
sent matrices. For any matrix, e.g., X, we denote its 𝑖-th row vector
asx𝑖. We use letters in calligraphy font (e.g., V) to denote sets.|V|
denotes the cardinality of set V.
2.1.2 Graph Neural Networks. LetG=(V,E,X)denote an
undirected attributed graph, where V={𝑣1,𝑣2,···,𝑣𝑛}is the set
of|V|nodes,Eis the edge set, and X∈R|V|×𝑑𝑥is the node feature
matrix.𝑑𝑥is the number of node features. Given each node 𝑣𝑖∈V,
N(𝑣𝑖)denotes the set of its neighbors. The ground-truth label of
each node𝑣𝑖∈V can be denoted as a 𝑑𝑐-dimensional one-hot
vector y𝑖where𝑑𝑐is the number of classes. The node homophily
[23, 49] is defined as
ℎ𝑖=|{𝑣𝑗|𝑣𝑗∈N(𝑣𝑖)andy𝑗=y𝑖}|
|N(𝑣𝑖)|, (1)
where|N(𝑣𝑖)|denotes the degree of node 𝑣𝑖. Typically, an 𝐿-layer
GNN model 𝑓parameterized by 𝜃maps each node to the outcome
space via a message-passing mechanism [ 11,17]. Specifically, each
node𝑣𝑖aggregates information from its neighbors in the 𝑙-th layer
of a GNN model by
h𝑙
𝑖=𝑓𝑙(h𝑙−1
𝑖,{h𝑙−1
𝑗:𝑣𝑗∈N(𝑣𝑖)};𝜃𝑙), (2)
where h𝑙
𝑖is the embedding of node 𝑣𝑖after the𝑙-th layer𝑓𝑙, and
𝜃𝑙is the parameters of the message-passing function in 𝑓𝑙. The
raw feature of each node 𝑣𝑖is used as the input layer, i.e., h0
𝑖=x𝑖.
For the node classification task, the node embedding h𝐿
𝑖after the
final layer is used to compute the predicted label distribution ˆy𝑖=
Softmax(h𝐿
𝑖)∈R𝑑𝑝by the softmax operator.
2.1.3 Personalized FL. Given a set of 𝐾clients, each client 𝑘has
its private dataset D(𝑘)={(x(𝑘)
𝑖,y(𝑘)
𝑖)}𝑁(𝑘)
𝑖=1, where𝑁(𝑘)is the
number of samples in client 𝑘. The overall objective of the clients is
min
(𝜃(1),𝜃(2),···,𝜃(𝐾))𝐾∑︁
𝑘=1𝑁(𝑘)
𝑁L(𝑘)(D(𝑘);𝜃(𝑘)), (3)
whereL(𝑘)(𝜃(𝑘))is the local average loss (e.g., the cross-entropy
loss) over local data in client 𝑘, and𝑁=Í𝐾
𝑘=1𝑁(𝑘). Standard FL
methods aim to learn a global model 𝜃=𝜃(1)=𝜃(2)=···=
𝜃(𝐾). As a representative method in FL, FedAvg [ 25] performs local
updates in each client and uploads local model parameters to a
 
828Federated Graph Learning with Structure Proxy Alignment KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: The statistics of the majority class and other minority
classes in 7 clients from the PubMed dataset. Majority and
Minority represent the majority class and other minority
classes, respectively.
ClientMajority Num. of Nodes Avg. Homophily
Class Majority Minority Majority Minority
1 1 1,384 384 0.91 0.33
2 1 1,263 152 0.97 0.24
3 2 2,001 286 0.92 0.17
4 2 1,236 97 0.98 0.48
5 1 1,160 140 0.95 0.41
6 0 934 467 0.84 0.47
7 2 948 806 0.83 0.70
central server, where they are averaged by
𝜃=𝐾∑︁
𝑘=1𝑁(𝑘)
𝑁𝜃(𝑘)(4)
during each round. However, a single global model may have poor
performance due to the data heterogeneity issue in FL [ 19]. To
remedy this, personalized FL [ 37] allows a customized 𝜃(𝑘)in each
client𝑘with better performance on local data while still benefiting
from collaborative training.
2.2 Problem Setup
Given a set of 𝐾clients, each client 𝑘owns a local graph G(𝑘)=
(V(𝑘),E(𝑘),X(𝑘)). For the labeled node set V(𝑘)
𝐿⊂V(𝑘)in client
𝑘, each node𝑣(𝑘)
𝑖∈V(𝑘)
𝐿is associated with its label y(𝑘)
𝑖. The goal
of these clients is to train personalized GNN models 𝑓(𝜃(𝑘))in each
client𝑘for the node classification task while keeping their private
graph data locally. Based on the aforementioned challenge and pre-
liminary analysis, this study aims to enhance collaborative training
by mitigating the impact of adverse neighboring information on
node classification, especially for minority nodes.
3 MOTIVATION
In this section, we first conduct an empirical study on the PubMed
dataset [31] to investigate the impact of divergent neighboring in-
formation across clients on minority nodes when jointly training
GNNs in FGL. The observation from this study is consistent with
our example in Figure 1 and motivates us to learn global structure
proxies as favorable neighboring information. We then develop the-
oretical analysis to explain how aligning neighboring information
across clients can benefit node classification tasks in FGL.
3.1 Empirical Observations
To better understand the divergent neighboring information across
clients with its impact on the node classification task in FGL, we
conduct preliminary experiments to compare the performance of
federated node classification with MLP and GNNs as local models
on the PubMed dataset [ 31]. Following the data partition strategy in
previous studies [ 14,51], we synthesize the distributed graph data
by splitting each dataset into multiple communities via the Louvain
1 2 3 4 5 6 7
Client ID020406080100Accuracy of Minority NodesFedAvg with MLP FedAvg with GNNFigure 2: Classification accuracy (%) of minority nodes in
each client by training MLP and GNN via FedAvg over the
PubMed dataset. Average accuracy for all nodes: 82.35% for
MLP VS 87.06% for GNN.
algorithm [ 1]. We retain seven communities with the largest number
of nodes; each community is regarded as an entire graph in a client.
Table 1 shows the statistics of each client. According to Table 1,
although one client may have the majority class different from
another, the average node-level homophily of the majority class
is consistently higher than that of the other classes for all the
clients. For instance, the nodes in client 2 that do not belong to
class 1 have only 24% neighbors from the same class on average. It
means that the minority nodes will absorb unfavorable neighboring
information via GNNs and probably be classified incorrectly.
To validate our conjecture, we perform collaborative training
for MLPs and GNNs following the standard FedAvg [ 25] over the
PubMed dataset. Figure 2 illustrates the classification accuracy of
minority nodes in each client by MLPs and GNNs. We can observe
that MLPs consistently perform better than GNNs on minority
nodes across the clients, although GNNs have higher overall accu-
racy for all nodes. Given that MLPs and GNNs are trained over the
same node label distribution, we argue that the performance gap
on minority nodes results from aggregating adverse neighboring
information from other classes via the message-passing mechanism
in GNNs, especially from the majority class. On the contrary, MLPs
only need node features and do not require neighboring informa-
tion throughout the training; therefore, they can avoid predicting
more nodes as the majority class.
3.2 Theoretical Motivation
According to the above empirical observations, minority nodes
with the original neighboring information are more likely to be
misclassified. One straightforward approach to this issue is enabling
nodes to leverage favorable neighboring information from other
clients for generating node embeddings. Specifically, we consider
constructing global neighboring information in the feature space.
The server collects neighboring feature vectors from each client
and computes the global class-wise neighboring information via
FedAvg [ 25]. We aim to theoretically investigate whether the global
neighboring information can benefit node classification tasks when
replacing the original neighbors of nodes. Following prevalent ways
of graph modeling [ 8,24,40], we first generate random graphs in
each client using a variant of contextual stochastic block model
[40] with two classes.
3.2.1 Random Graph Generation. The generative model gen-
erates a random graph in each client via the following strategy.
 
829KDD ’24, August 25–29, 2024, Barcelona, Spain Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, & Jundong Li
𝓖(𝒌)=𝓥(𝒌),𝓔(𝒌),𝑿(𝒌)
𝐱𝒊(𝒌)Feature-structure 
encoder 𝒈𝜔 GNN model 𝒇𝜃𝒌 
𝐲𝒊(𝒌)
𝐩𝒊(𝒌)Client 𝒌
Mode l 
aggregation
𝜔←෍
𝒌=𝟏𝑲𝑵𝒌
𝑵𝜔(𝒌)
Structure Proxy  
aggregation
𝐒←෍
𝒌=𝟏𝑲
𝒂𝒌𝐒(𝒌)𝜔(𝒌),𝐒(𝒌)
𝜔,𝐒
𝐪𝒊(𝒌)Serv er
𝐬𝒊(𝒌)ො𝐲𝒊(𝒌)
𝓛𝑮_𝑪𝑬(𝒌)
𝓛𝑭_𝑪𝑬(𝒌)𝓛𝑮_𝑲𝑫(𝒌)
𝓛𝑭_𝑲𝑫(𝒌)𝐪𝒊(𝒌)𝐩𝒊(𝒌)
𝐱𝒊(𝒌)
𝐬𝒊(𝒌)Embedding
layer  𝒈𝒆𝜔𝒆 Com bineClassifier
𝒈𝒑𝜔𝒑Projector
𝒈𝒒𝜔𝒒 
(b) Feature-struct ure enco der (a) The ov erview of FedSpra y
Figure 3: (a) An overview of the proposed FedSpray. The backbone of FedSpray is personalized GNN models 𝑓(𝜃(𝑘)). A global
feature-structure encoder 𝑔(𝜔)with structure proxies Sis also employed in FedSpray to tackle underrepresented node embed-
dings caused by adverse neighboring information in FGL. (b) An illustration of the feature-structure encoder in FedSpray.
In the generated graph G(𝑘)in client𝑘, the nodes are labeled by
two classes 𝑐1and𝑐2. For each node 𝑣(𝑘)
𝑖, its initial feature vec-
torx(𝑘)
𝑖∈R𝑑𝑥is sampled from a Gaussian distribution 𝑁(𝝁1,I)
if labeled as class 𝑐1or𝑁(𝝁2,I)if labeled as class 𝑐2(𝝁1∈R𝑑𝑥,
𝝁2∈R𝑑𝑥, and 𝝁1≠𝝁2). For each client 𝑘, a neighbor of each node
is from the majority with probability 𝑝(𝑘)and from the minority
with probability 1−𝑝(𝑘). The ratio of minority nodes and major-
ity nodes is 𝑞(𝑘). In our setting, we assume1
2<𝑝(𝑘)<1and
0<𝑞(𝑘)<1. We denote each graph generated from the above
strategy in client 𝑘asG(𝑘)∼Gen(𝝁1,𝝁2,𝑝(𝑘),𝑞(𝑘)).
3.2.2 Better Separability with Global Neighboring Informa-
tion. To figure out the influence of global neighboring information,
we focus on the separability of the linear GNN classifiers with the
largest margin when leveraging global neighboring information.
Concretely, we aim to find the expected Euclidean distance from
each class to the decision boundary of the optimal linear GNN
classifier when it uses either the original neighboring information
or the global neighboring information. We use 𝑑𝑖𝑠𝑡 and𝑑𝑖𝑠𝑡′to
denote the expected Euclidean distances in these two scenarios,
respectively. We summarize the results in the following proposition.
Proposition 3.1. Given a set of 𝐾clients, each client 𝑘owns a
local graphG(𝑘)∼Gen(𝝁1,𝝁2,𝑝(𝑘),𝑞(𝑘)),𝑑𝑖𝑠𝑡=||𝝁1−𝝁2||2
2, which
is smaller than 𝑑𝑖𝑠𝑡′=
1+Í𝐾
𝑘=1(1−𝑞(𝑘))(𝑝(𝑘)−1
2)||𝝁1−𝝁2||2
2.
A detailed proof can be found in Appendix A. According to
Proposition 3.1, we will have a larger expected distance 𝑑𝑖𝑠𝑡′when
using the global neighboring information. Typically, the larger
the distance is, the smaller the misclassification probability is [ 24].
Therefore, the optimal linear GNN classifier will obtain better clas-
sification performance.
However, directly uploading neighboring feature vectors is im-
plausible in FGL since it contains many sensitive raw features in
the clients. To overcome this issue, we propose a novel frameworkFedSpray to learn global structure proxies in the latent space and
elaborate on the details of FedSpray in Section 4.
4 METHODOLOGY
In this section, we present the proposed FedSpray in detail. Figure
3(a) illustrates an overview of FedSpray. The goal of FedSpray is
to let the clients learn personalized GNN models over their private
graph data while achieving higher performance by mitigating the
impact of adverse neighboring information in GNN models. To
reach this goal, FedSpray employs a lightweight global feature-
structure encoder which learns class-wise structure proxies and
aligns them on the central server. The feature-structure encoder
generates reliable unbiased soft targets for nodes given their raw
features and the aligned structure proxies to regularize local train-
ing of GNN models.
4.1 Personalized GNN Model
We first introduce personalized GNN models in FedSpray.
4.1.1 GNN backbone Model. Considering their exceptional abil-
ity to model graph data, we use GNNs as the backbone of the pro-
posed framework. In this study, we propose to learn GNN models
for each client in a personalized manner to tackle the data hetero-
geneity issue in FGL. Specifically, the personalized GNN model
𝑓(𝜃(𝑘))in client𝑘outputs the predicted label distribution ˆy(𝑘)
𝑖for
each node𝑣(𝑘)
𝑖∈V(𝑘)
𝐿. Note that FedSpray is flexible. Any GNNs
that follow the message-passing mechanism as the structure of Eq.
(2) can be used as the backbone, such as GCN [17] and SGC [44].
4.1.2 Loss formulation. During local training, 𝜃(𝑘)can be up-
dated by minimizing the cross-entropy loss between y(𝑘)
𝑖andˆy(𝑘)
𝑖
for each labeled node 𝑣(𝑘)
𝑖∈V(𝑘)
𝐿
L(𝑘)
𝐺_𝐶𝐸=1
|V(𝑘)
𝐿|∑︁
𝑣(𝑘)
𝑖∈V(𝑘)
𝐿CE(y(𝑘)
𝑖,ˆy(𝑘)
𝑖), (5)
 
830Federated Graph Learning with Structure Proxy Alignment KDD ’24, August 25–29, 2024, Barcelona, Spain
where CE(·,·)denotes the cross-entropy loss. However, simply min-
imizingL(𝑘)
𝐺_𝐶𝐸can lead𝜃(𝑘)to overfitting during local training
[19,39]. In addition, the minority nodes are particularly prone to
obtaining underrepresented embeddings due to biased neighboring
information, as discussed above. To tackle this challenge, we pro-
pose to design an extra knowledge distillation term and use it to
regularize local training of 𝜃(𝑘). More concretely, we first employ
the soft target p(𝑘)
𝑖∈R𝑑𝑝for each node 𝑣(𝑘)
𝑖∈V(𝑘)generated by
the global feature-structure encoder to guide local training of 𝜃(𝑘)
in client𝑘. Typically, we hope p(𝑘)
𝑖to be generated with unbiased
neighboring information for node 𝑣(𝑘)
𝑖(we will elaborate on how
to obtain proper p(𝑘)
𝑖in Section 4.2). Then, we encourage ˆy(𝑘)
𝑖to
approximate p(𝑘)
𝑖by minimizing the discrepancy between p(𝑘)
𝑖and
ˆy(𝑘)
𝑖for each node 𝑣(𝑘)
𝑖∈V(𝑘)in client𝑘. Specifically, we achieve
this via knowledge distillation [13] as
L(𝑘)
𝐺_𝐾𝐷=1
|V(𝑘)|∑︁
𝑣(𝑘)
𝑖∈V(𝑘)KL(p(𝑘)
𝑖∥ˆy(𝑘)
𝑖), (6)
where KL(·∥·) is to compute the Kullback-Leibler divergence (KL-
divergence). Therefore, the overall loss for training 𝜃(𝑘)in client𝑘
can be formulated by combining the two formulations together
L(𝑘)
𝐺=L(𝑘)
𝐺_𝐶𝐸+𝜆1L(𝑘)
𝐺_𝐾𝐷, (7)
where𝜆1is a predefined hyperparameter that controls the contribu-
tion of the knowledge distillation term in L(𝑘)
𝐺. When𝜆1is set as
0, FedSpray will be equivalent to training GNN models individually
in each client.
4.2 Global Feature-Structure Encoder with
Structure Proxies
In this subsection, we will elucidate our design for the global feature-
structure encoder and class-wise structure proxies in FedSpray. The
feature-structure encoder aims to generate a reliable soft target (i.e.,
p(𝑘)
𝑖) for each node with its raw features and structure proxy.
4.2.1 Structure Proxies. As discussed above, a minority node
can obtain adverse neighboring information from its neighbors via
the message-passing mechanism, given its neighbors are proba-
bly from other classes. To mitigate this issue, we propose to learn
unbiased class-wise structure proxies in FedSpray, providing favor-
able neighboring information for each node. Here, we formulate
each structure proxy in a vectorial form. Let S∈R𝑑𝑐×𝑑𝑠denote
class-wise structure proxies, and each row s𝑗∈Sdenotes the 𝑑𝑠-
dimensional structure proxy of the 𝑗-th node class. For each node
𝑣(𝑘)
𝑖∈V(𝑘)
𝐿, its structure proxy s(𝑘)
𝑖will be s𝑗if it is from the 𝑗-th
class. Then, the structure proxies will be used as the input of the
feature-structure encoder.
4.2.2 Feature-Structure Encoder. In FedSpray, we employ a
lightweight feature-structure encoder to generate a reliable soft
target for a node with its raw feature and structure proxy as the
input. Figure 3(b) illustrates our design for the feature-structure
encoder. Let 𝑔(𝜔)denote the feature-structure encoder 𝑔param-
eterized by 𝜔. Given a node 𝑣(𝑘)
𝑖∈V(𝑘)
𝐿, the feature-structureencoder𝑔generates its soft target p(𝑘)
𝑖with its feature vector x(𝑘)
𝑖
and structure proxy s(𝑘)
𝑖by
p(𝑘)
𝑖=𝑔(x(𝑘)
𝑖,s(𝑘)
𝑖;𝜔). (8)
Fusion of node features and structure proxies. Here, the prob-
lem is to determine a proper scheme for fusing a node’s raw feature
and its structure proxy in the feature-structure encoder. A straight-
forward way is to combine x(𝑘)
𝑖ands(𝑘)
𝑖together as the input of
the feature-structure encoder. Ideally, s(𝑘)
𝑖can serve as surrogate
neighboring information of node 𝑣(𝑘)
𝑖in the feature space. In this
case, it requires s(𝑘)
𝑖to have the same dimension as that of x(𝑘)
𝑖.
However, this brings us a new challenge: when x(𝑘)
𝑖is of high di-
mension in graph data (e.g., 500 for PubMed [ 31]), directly learning
high-dimensional s(𝑘)
𝑖in the feature space will be intractable. Con-
sidering this, we propose to learn s(𝑘)
𝑖in the latent space instead.
Specifically, we split the feature-structure encoder into an embed-
ding layer𝑔𝑒(𝜔𝑒)and a classifier 𝑔𝑝(𝜔𝑝). The embedding layer first
maps the raw feature x(𝑘)
𝑖of a node v(𝑘)
𝑖∈V(𝑘)into the latent
space to obtain its low-dimensional feature embedding e(𝑘)
𝑖. Then
we combine the feature embedding e(𝑘)
𝑖and the structure proxy
s(𝑘)
𝑖together as the input of the classifier to get the soft target p(𝑘)
𝑖.
Mathematically, we can formulate this procedure as
p(𝑘)
𝑖=𝑔(x(𝑘)
𝑖,s(𝑘)
𝑖;𝜔)=𝑔𝑝(Combine(e(𝑘)
𝑖,s(𝑘)
𝑖);𝜔𝑝)(9)
where e(𝑘)
𝑖=𝑔𝑒(x(𝑘)
𝑖;𝜔𝑒). Here, Combine(·,·)is the operation to
combine e(𝑘)
𝑖ands(𝑘)
𝑖together (e.g., addition).
Structure proxies for unlabeled nodes. The feature-structure
encoder can generate soft targets only for labeled nodes by Eq. (9)
because the structure proxy s(𝑘)
𝑖requires the ground-truth label
information of node v(𝑘)
𝑖. To better regularize local training of the
GNN model, we need to obtain soft targets for unlabeled nodes
and use them to compute L(𝑘)
𝐺_𝐾𝐷by Eq. (6). To achieve this, we
design a projector 𝑔𝑞(𝜔𝑞)in the feature-structure encoder. It has
the same structure as the classifier 𝑔𝑝. The difference is that the
projector𝑔𝑞generates soft targets only based on feature embed-
dings. Specifically, we can obtain the soft label q(𝑘)
𝑖for each node
v(𝑘)
𝑖∈V(𝑘)\V(𝑘)
𝐿with its feature embedding e(𝑘)
𝑖by
q(𝑘)
𝑖=𝑔𝑞(e(𝑘)
𝑖;𝜔𝑞). (10)
Therefore, we obtain the structure proxy s(𝑘)
𝑖=⟨q(𝑘)
𝑖,S⟩by com-
puting the product of q(𝑘)
𝑖andSfor each node v(𝑘)
𝑖∈V(𝑘)\V(𝑘)
𝐿.
Since q(𝑘)
𝑖is normalized by the softmax operation, the inner prod-
uct can also be viewed as the weighted average of S.
4.2.3 Loss formulation. During local training, we aim to update
𝜔={𝜔𝑒,𝜔𝑝,𝜔𝑞}andSusing both ground-truth labels and predic-
tions from the GNN model. Specifically, we formulate the overall
loss for training 𝜔andSin client𝑘as
L(𝑘)
𝐹=L(𝑘)
𝐹_𝐶𝐸+𝜆2L(𝑘)
𝐹_𝐾𝐷, (11)
 
831KDD ’24, August 25–29, 2024, Barcelona, Spain Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, & Jundong Li
where𝜆2is a hyperparameter. Here L(𝑘)
𝐹_𝐶𝐸is the average cross-
entropy loss between y(𝑘)
𝑖andq(𝑘)
𝑖for each node 𝑣(𝑘)
𝑖∈V(𝑘)
𝐿
L(𝑘)
𝐹_𝐶𝐸=1
|V(𝑘)
𝐿|∑︁
𝑣(𝑘)
𝑖∈V(𝑘)
𝐿CE(y(𝑘)
𝑖,q(𝑘)
𝑖). (12)
L(𝑘)
𝐹_𝐾𝐷is the average KL-divergence between p(𝑘)
𝑖and ˆy(𝑘)
𝑖to
encourage p(𝑘)
𝑖to approach ˆy(𝑘)
𝑖for each node 𝑣(𝑘)
𝑖∈V(𝑘)
𝐿
L(𝑘)
𝐹_𝐾𝐷=1
|V(𝑘)
𝐿|∑︁
𝑣(𝑘)
𝑖∈V(𝑘)
𝐿KL(ˆy(𝑘)
𝑖∥p(𝑘)
𝑖). (13)
4.3 Server Update
As stated above, FedSpray will learn the feature-structure encoder
and the structure proxies globally. In this subsection, we present
the global update in the central server for the feature-structure
encoder and the structure proxies, respectively.
4.3.1 Update global feature-structure encoder. During each
round𝑟, the server performs weighted averaging of local feature-
structure encoders following the standard FedAvg [ 25] with each
coefficient determined by the local node size
𝜔𝑟←𝐾∑︁
𝑘=1𝑁(𝑘)
𝑁𝜔(𝑘)
𝑟. (14)
4.3.2 Structure proxy alignment. Instead of using the local
node size, we propose to assign higher weights to majority classes
than minority classes for structure proxy alignment. More specif-
ically, the server updates global structure proxy s𝑗,𝑟∈S𝑟during
round𝑟by
s𝑗,𝑟←𝐾∑︁
𝑘=1𝑎(𝑘)
𝑗
𝑎𝑗s(𝑘)
𝑗,𝑟, (15)
where𝑎(𝑘)
𝑗is the ratio of nodes from the 𝑗-th class amongV(𝑘)
𝐿in
client𝑘and𝑎𝑗=Í𝐾
𝑘=1𝑎(𝑘)
𝑗.
4.4 Overall Algorithm
Algorithm 1 shows the overall algorithm of the proposed FedSpray.
During each round, each client performs local updates with two
phases. In Phase 1, each client trains its personalized GNN models
for𝐸epochs. We first compute p(𝑘)
𝑖for node𝑣(𝑘)
𝑖by the global
feature-structure encoder 𝑔(𝜔𝑟−1)with its feature x(𝑘)
𝑖and cor-
responding structure proxy s(𝑘)
𝑖(line 5). Then p(𝑘)
𝑖is utilized to
computeL(𝑘)
𝐺(line 9) for training the GNN model (line 10). In
Phase 2, the feature-structure encoder and structure proxies will
be optimized for 𝐸epochs. In client 𝑘, we first obtain ˆy(𝑘)
𝑖for node
𝑣(𝑘)
𝑖by the up-to-date GNN model (line 14). ˆy(𝑘)
𝑖for node𝑣(𝑘)
𝑖
will be used to compute L(𝑘)
𝐹(line 19). Then we update 𝜔(𝑘)
𝑟,𝑡and
s(𝑘)
𝑖via gradient descent (line 20-21). At the end of each round,
s𝑗∈S(𝑘)
𝑟will be updated by averaging s(𝑘)
𝑖of nodes from the 𝑗-th
class (line 23). At the end of each round, the local feature-structure
encoder and structure proxies will be sent to the central serverAlgorithm 1 FedSpray
Input: initial personalized 𝜃(𝑘)for each client 𝑘, global𝜔0andS0
foreach round𝑟=1,···,𝑅do
foreach client𝑘in parallel do
𝜔(𝑘)
𝑟,S(𝑘)
𝑟←LocalUpdate(𝜔𝑟−1,S𝑟−1)
end for
Update𝜔𝑟by Eq. (14)
Update S𝑟by Eq. (15)
end for
LocalUpdate(𝜔𝑟−1,S𝑟−1):
1:================== Phase 1 =====================
2:e(𝑘)
𝑖=𝑔𝑒(x(𝑘)
𝑖;𝜔𝑒,𝑟−1)
3:q(𝑘)
𝑖=𝑔𝑞(e(𝑘)
𝑖;𝜔𝑞,𝑟−1)
4:Compute local s(𝑘)
𝑖from S𝑟−1
5:p(𝑘)
𝑖=𝑔𝑝(Combine(e(𝑘)
𝑖,s(𝑘)
𝑖);𝜔𝑝,𝑟−1)
6:𝜃(𝑘)
𝑟=𝜃𝑟−1
7:for𝑡=1,···,𝐸do
8: ˆy(𝑘)
𝑖=𝑓(x(𝑘)
𝑖,G(𝑘);𝜃(𝑘)
𝑟)
9: ComputeL(𝑘)
𝐺by Eq. (7) using p(𝑘)
𝑖
10: Update the local GNN model 𝜃(𝑘)
𝑟←𝜃(𝑘)
𝑟−𝜂𝑓∇L(𝑘)
𝐺
11:end for
12:================== Phase 2 =====================
13:𝜔(𝑘)
𝑟=𝜔𝑟−1
14:ˆy(𝑘)
𝑖=𝑓(x(𝑘)
𝑖,G(𝑘);𝜃(𝑘)
𝑟)
15:for𝑡=1,···,𝐸do
16: e(𝑘)
𝑖=𝑔𝑒(x(𝑘)
𝑖;𝜔(𝑘)
𝑒,𝑟)
17: q(𝑘)
𝑖=𝑔𝑞(e(𝑘)
𝑖;𝜔(𝑘)
𝑞,𝑟)
18: p(𝑘)
𝑖=𝑔𝑝(Combine(e(𝑘)
𝑖,s(𝑘)
𝑖);𝜔(𝑘)
𝑝,𝑟)
19: ComputeL(𝑘)
𝐹by Eq. (11) using ˆy(𝑘)
𝑖
20: Update the local feature-structure encoder
𝜔(𝑘)
𝑟←𝜔(𝑘)
𝑟−𝜂𝑔∇L(𝑘)
𝐹
21: Update the local structure proxy
s(𝑘)
𝑖←s(𝑘)
𝑖−𝜂𝑠∇L(𝑘)
𝐹
22:end for
23:Update s𝑗∈S(𝑘)
𝑟by averaging s(𝑘)
𝑖of nodes from class 𝑗
24:return𝜔(𝑘)
𝑟,S(𝑘)
𝑟
for training in the next round (line 24). In the central server, Fed-
Spray updates the feature-structure encoder by Eq. (14) and aligns
structure proxies by Eq. (15).
4.5 Discussion
FedSpray exhibits superior advantages from various perspectives,
including communication efficiency, privacy preservation, and com-
putational cost. We provide an in-depth discussion about FedSpray’s
principal properties as follows.
4.5.1 Privacy Preservation. The proposed FedSpray uploads the
parameters of local feature-structure encoders following the preva-
lent frameworks in FL [ 18–20]. Here, we mainly discuss the privacy
 
832Federated Graph Learning with Structure Proxy Alignment KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Classification accuracy (Average ±Std) of FedSpray and other baselines on node classification over four datasets. Overall
andMinority represent all nodes and minority nodes in the test sets, respectively.
Dataset MethodGCN SGC GraphSAGE
Overall Minority Overall Minority Overall Minority
PubMedLocal 87.49±0.24 51.00±1.20 86.27±0.34 40.83±0.93 86.86±0.26 48.66±1.14
Fedavg 87.06±0.61 55.77±0.90 82.23±1.89 56.21±1.38 85.92±0.87 61.35±2.76
APFL 86.44±0.66 49.25±2.07 83.10±0.34 28.25±5.69 86.23±0.55 45.41±1.50
GCFL 86.74±0.69 48.85±3.25 71.81±8.22 51.10±8.66 85.35±0.46 45.60±2.53
FedStar 81.25±0.67 12.01±2.89 82.06±1.32 19.24±7.71 80.57±1.21 7.78±7.01
FedLit 57.95±3.82 47.39±2.84 84.82±0.72 57.54±1.76 70.73±7.61 57.15±8.35
FedSpray 87.71±0.65 62.12±2.73 87.13±1.41 59.23±1.25 87.02±1.01 61.59±0.96
WikiCSLocal 81.43±0.58 41.36±1.78 81.66±0.62 40.02±1.41 81.57±0.35 42.93±1.85
Fedavg 80.53±0.74 35.48±2.52 79.90±0.60 34.24±1.17 80.23±0.20 47.60±0.99
APFL 79.81±0.72 38.33±5.43 78.46±0.63 27.40±1.52 80.54±0.64 42.16±1.21
GCFL 75.79±1.56 36.94±1.80 74.85±1.65 29.08±0.69 73.34±3.24 37.79±2.90
FedStar 75.61±0.53 16.72±3.23 76.95±0.73 21.90±2.82 74.48±0.51 10.66±1.71
FedLit 49.08±4.98 29.85±2.85 56.18±9.39 32.79±3.73 60.37±4.33 36.90±4.32
FedSpray 81.51±0.45 47.43±1.31 81.87±0.59 46.60±0.00 81.93±0.30 52.04±0.51
PhysicsLocal 94.62±0.16 72.75±0.73 94.82±0.28 76.24±9.07 94.14±0.30 69.50±0.97
Fedavg 94.13±0.40 66.45±2.28 94.40±0.25 66.58±1.01 94.60±0.34 74.27±0.95
APFL 94.27±0.20 72.83±3.73 94.52±0.27 69.27±1.67 84.31±3.76 38.65±7.33
GCFL 88.97±2.61 60.90±2.04 94.02±0.29 66.54±1.87 80.71±3.91 50.22±5.20
FedStar 89.86±0.43 33.44±3.27 91.37±0.40 45.27±4.73 89.78±0.41 32.91±3.52
FedLit 85.11±2.58 60.57±2.42 87.57±1.47 61.96±0.81 86.68±0.27 66.36±0.88
FedSpray 95.59±0.24 80.98±1.39 95.08±0.32 82.43±1.62 94.73±0.37 83.26±1.25
FlickrLocal 43.18±0.55 25.96±1.94 46.82±0.93 25.39±1.46 49.72±0.85 25.25±1.70
Fedavg 44.53±1.36 26.45±0.46 47.03±1.39 27.24±2.52 47.51±1.40 26.13±0.82
APFL 32.27±3.58 19.44±6.16 46.93±0.50 23.50±0.49 34.59±2.83 18.39±2.96
GCFL 47.31±1.29 19.71±2.20 46.56±1.71 26.48±3.60 44.84±2.10 16.76±2.45
FedStar 47.73±0.85 13.82±3.05 48.45±0.58 14.59±3.39 46.36±1.04 11.33±4.38
FedLit 45.38±1.73 23.62±8.74 49.62±0.36 24.46±0.81 44.06±2.26 18.12±2.30
FedSpray 48.21±1.03 29.72±0.75 50.07±0.75 28.46±2.12 51.45±0.72 27.52±0.42
concern about uploading local structure proxies first. In fact, struc-
ture proxies naturally protect data privacy. First, they are synthetic
1D vectors to provide high-quality neighboring information in the
latent space. In other words, they do not possess any raw feature
information. Second, they are generated by averaging the structure
proxies from the same class, which is an irreversible operation.
Moreover, we can employ various privacy-preserving techniques
to further improve confidentiality.
4.5.2 Communication Efficiency. The proposed FedSpray re-
quires clients to upload local feature-structure encoders and struc-
ture proxies. As we introduced above, the feature-structure encoder
is a relatively lightweight model. As for structure proxies, their
size is generally much smaller than that of model parameters given
𝑑𝑠≪𝑑𝑥. In addition, we can further reduce the number of uploaded
parameters by setting smaller 𝑑𝑠.
4.5.3 Computational Cost. The additional computational cost
in FedSpray is mainly on local updates for the feature-structure
encoder and structure proxies. Compared with GNN models, the
feature-structure encoder and structure proxies require fewer oper-
ations for updating parameters. Training GNN models is usuallytime-consuming since GNN models need to aggregate node infor-
mation via the message-passing mechanism during the forward
pass [ 52]. However, the feature-structure encoder only incorpo-
rates node features and structure proxies with fully connected
layers to obtain soft targets. Therefore, the time complexity of local
updates for the feature-structure encoder and structure proxies will
be smaller than GNN models. Let 𝑁,𝑑𝑥, and𝐸denote the number
of nodes of the local graph in a client, the number of node fea-
tures, and the number of edges, respectively. Considering a 2-layer
GCN model with hidden size 𝑑ℎ, its computational complexity is
approximately 𝑂(𝑁𝑑ℎ𝑑𝑥+𝐸𝑑𝑥). Similarly, we can conclude that
the computational complexity of the feature-structure encoder with
the𝑑𝑠-dimensional structure proxy is about 𝑂(𝑁𝑑𝑠𝑑𝑥), apparently
smaller than the GCN model when we set 𝑑ℎ=𝑑𝑠. Therefore, the
feature-structure encoder in FedSpray does not introduce signifi-
cant extra computational costs compared with FedAvg using GCN.
Furthermore, setting a smaller 𝑑𝑠can also reduce computation costs.
5 EXPERIMENTS
In this section, we conduct empirical experiments to demonstrate
the effectiveness of the proposed framework FedSpray and perform
detailed analysis of FedSpray.
 
833KDD ’24, August 25–29, 2024, Barcelona, Spain Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, & Jundong Li
0.1 1.0 5.0 10.0 20.0
1
2030405060708090100Accuracy
(a) PubMed with GraphSAGEAll Minority
0.1 1.0 5.0 10.0 20.0
1
2030405060708090100Accuracy
(b) WikiCS with GraphSAGEAll Minority
Figure 4: Classification accuracy (%) of FedSpray on all nodes
and minority nodes in the test sets with different values of
𝜆1over (a) PubMed and (b) WikiCS with GraphSAGE.
5.1 Experiment Setup
5.1.1 Datasets. We synthesize the distributed graph data based
on four common real-world datasets from various domains, i.e.,
PubMed [ 31], WikiCS [ 26], Coauthor Physics [ 33], and Flickr [ 50].
We follow the strategy in Section 3.1 to simulate the distributed
graph data and summarize the statistics and basic information about
the datasets in Appendix B.1. We randomly select nodes in clients
and let 40% for training, 30% for validation, and the remaining for
testing. We report the average classification accuracy for all nodes
and minority nodes over the clients for five random repetitions.
5.1.2 Baselines. We compare FedSpray with six baselines includ-
ing (1) Local where each client train its GNN model individually; (2)
FedAvg [25], the standard FL algorithm; (3) APFL [6], an adaptive
approach in personalized FL; (4) GCFL [47], (5) FedStar [38], and
(6)FedLit [48], three state-of-the-art FGL methods. More details
about the above baselines can be found in Appendix B.2.
5.1.3 Hyperparameter setting. As stated previously, FedSpray
is compatible with most existing GNN architectures. In the exper-
iments, we adopt three representative ones as backbone models:
GCN [ 17], SGC [ 44], and GraphSAGE [ 11]. Each GNN model in-
cludes two layers with a hidden size of 64. The size of feature
embeddings and structure proxies is also set as 64. Therefore, the
feature-structure encoder has similar amounts of parameters with
GNN models. Each component in the feature-structure encoder
is implemented with one layer. We use an Adam optimizer [ 16]
with learning rates of 0.003 for the global feature-structure encoder
and personalized GNN models, 0.02 for structure proxies. The two
hyperparameters 𝜆1and𝜆2are set as 5 and 1, respectively. We run
all the methods for 300 rounds, and the local epoch is set as 5.
5.2 Effectiveness of FedSpray
We first show the performance of FedSpray and other baselines on
node classification over the four datasets with three backbone GNN
models. Table 2 reports the average classification accuracy on all
nodes and minority nodes in the test set across clients.
First, we analyze the results of overall accuracy on all test nodes.
According to Table 2, our FedSpray consistently outperforms all
the baselines on node classification accuracy for overall test nodes
across clients. Local and FedAvg achieve comparable performance
4 8 16 32 64 128 256
ds2030405060708090100Accuracy
(a) WikiCS with GCNAll Minority
4 8 16 32 64 128 256
ds2030405060708090100Accuracy
(b) Physics with GCNAll MinorityFigure 5: Classification accuracy (%) of FedSpray on all nodes
and minority nodes in the test sets with different 𝑑𝑠over (a)
WikiCS and (b) Physics with GCN.
over the four datasets. In the meantime, APFL does not surpass
Local and FedAvg. As for FGL methods, GCFL, FedStar, FedLit fail
to show remarkable performance gain. Although GCFL and FedStar
tackle the data heterogeneity issue of graph structures across clients
in FGL, they do not take the node-level heterophily into account.
While FedLit models latent link types between nodes via multi-
channel GNNs, it involves more GNN parameters that are hard to
be well trained within limited communication rounds.
Second, we analyze the results of accuracy on minority nodes
in the test set. Note that FedSpray aims to learn reliable unbiased
structure information for guiding local training of personalized
GNN models, particularly for minority nodes. We can observe that
FedSpray outperforms all the baselines by a notable margin. Even
though Local and FedAvg achieve comparable performance on
overall test nodes, they show different accuracy results on minority
nodes. Among the three FGL methods, FedStar encounters signifi-
cant performance degradation on minority nodes since the design
of structure embeddings in FedStar does not provide beneficial
neighboring information for node-level tasks.
5.3 Analysis of FedSpray
5.3.1 Influence of hyperparameter 𝜆1.The hyperparameter
𝜆1controls the contribution of the regularization term in L𝐺(𝑘).
We conduct the sensitivity analysis on 𝜆1in FedSpray. Figure 4
reports the classification accuracy of FedSpray on all nodes and
test nodes in the test sets with different values of 𝜆1over PubMed
(left) and WikiCS (right) with GraphSAGE. The accuracy on all
nodes remains high when 𝜆1is relatively small (i.e., 𝜆1=0.1,1,5).
However, the accuracy of minority nodes will decrease when 𝜆1is
too small because the feature-structure encoder cannot sufficiently
regularize local training of GNN models with too small 𝜆1. When
𝜆1gets too large, the accuracy of all nodes decreases in both figures.
In this case, the regularization term weighs overwhelmingly in
the loss for training GNN models; then GNN models cannot be
sufficiently trained with label information. According to the above
observations, we will recommend 10 for PubMed with GraphSAGE
and 5 for WikiCS with GraphSAGE as the best setting for 𝜆1.
5.3.2 Influence to structure proxy dimension. Since FedSpray
incorporates structure proxies in the feature-structure encoder, we
may set a different dimension 𝑑𝑠of structure proxies. We evaluate
 
834Federated Graph Learning with Structure Proxy Alignment KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Classification accuracy (Average ±Std) of FedSpray
with S =0 over PubMed and Physics with GCN.
Dataset Method Overall Minority
PubMedFedSpray 87.71±0.65 62.12±2.73
FedSpray (S =0) 77.11±0.43 42.01±0.81
PhysicsFedSpray 95.59±0.24 80.98±1.39
FedSpray (S =0) 93.23±0.27 72.57±0.38
the performance of FedSpray with different values of 𝑑𝑠while
fixing the hidden dimension of the GNN model as 64. Figure 5
demonstrates the classification accuracy of FedSpray on all nodes
and test nodes in the test sets with different values of 𝑑𝑠over
WikiCS (left) and Physics (right) with GCN as the backbone. We
can observe that FedSpray can obtain comparable accuracy with
𝑑𝑠smaller than 64 (e.g., 𝑑𝑠=32). In the meantime, FedSpray does
not obtain significant performance gain when 𝑑𝑠is larger than 64.
From the above observation, we can reduce communication and
computation costs by setting 𝑑𝑠a smaller value such as 32.
5.3.3 Effectiveness of structure proxies. In this study, we de-
sign structure proxies in FedSpray to serve as global unbiased neigh-
boring information for guiding local training of GNN models. To
validate the effectiveness of structure proxies, we investigate the
performance of the proposed framework when structure proxies
are removed. Specifically, we set class-wise structure proxies Sas
0consistently during training. We report the performance of Fed-
Spray with S=0over PubMed and WikiCS in Table 3. According
to Table 3, we can observe that FedSpray suffers from significant
performance degradation when removing structure proxies. It sug-
gests that structure proxies play a significant role in FedSpray.
Without them, the feature-structure encoder generates soft targets
only based on node features [ 52]. In this case, the soft labels can
be unreliable when node labels are not merely dependent on node
features and, therefore, provide inappropriate guidance on local
training of personalized GNN models in FedSpray.
5.3.4 More Experimental Results. Due to the page limit, we
provide experimental results of FedSpray with varying local epochs
in Appendix B.3.
6 RELATED WORK
6.1 Federated Learning
Recent years have witnessed the booming of techniques in FL and its
various applications in a wide range of domains, such as computer
vision [ 3,28], healthcare [ 21,36], and social recommendation [ 22,
43]. The most important challenge in FL is data heterogeneity across
clients (i.e., the non-IID problem). A growing number of studies
have been proposed to mitigate the impact of data heterogeneity. For
instance, FedProx [ 20] adds a proximal term to the local training
loss to keep the updated parameters close to the global model.
Moon [ 18] uses a contrastive loss to increase the distance between
the current and previous local models. FedDecorr [ 34] mitigates
dimensional collapse to prevent representations from residing in
a lower-dimensional space. In the meantime, a battery of studies
proposed personalized model-based methods. For example, pFedHN[32] trains a central hypernetwork to output a unique personalized
model for each client. APFL [ 6] learns a mixture of local and global
models as the personalized model. FedProto [ 39] and FedProc [ 27]
utilize the prototypes to regularize local model training. FedBABU
[28] proposes to keep the global classifier unchanged during the
feature representation learning and perform local adoption by fine-
tuning in each client.
6.2 Federated Graph Learning
Due to the great prowess of FL, it is natural to apply FL to graph
data and solve the data isolation issue. Recently, a cornucopia of
studies has extended FL to graph data for different downstream
tasks, such as node classification [ 48], knowledge graph completion
[4], and graph classification [ 38,47], cross-client missing informa-
tion completion [ 29,51]. Compared with generic FL, node attributes
and graph structures get entangled simultaneously in the data het-
erogeneity issue of FGL. To handle this issue, a handful of studies
proposed their approaches. For example, GCFL [ 47] and FedStar
[38] are two recent frameworks for graph classification in FGL. The
authors of GCFL [ 47] investigate common and diverse properties in
intra- and cross-domain graphs. They employ Clustered FL [ 30] in
GCFL to encourage clients with similar properties to share model
parameters. A following work FedStar [ 38] aims to jointly train a
global structure encoder in the feature-structure decoupled GNN
across clients. FedLit [ 48] mitigates the impact of link-type hetero-
geneity underlying homogeneous graphs in FGL via an EM-based
clustering algorithm.
7 CONCLUSION
In this study, we investigate the problem of divergent neighbor-
ing information in FGL. With the high node heterophily, minority
nodes in a client can aggregate adverse neighboring information
in GNN models and obtain biased node embeddings. To grapple
with this issue, we propose FedSpray, a novel FGL framework that
aims to learn personalized GNN models for each client. FedSpray
extracts and shares class-wise structure proxies learned by a global
feature-structure encoder. The structure proxies serve as unbiased
neighboring information to obtain soft targets generated by the
feature-structure encoder. Then, FedSpray uses the soft labels to reg-
ularize local training of the GNN models and, therefore, eliminate
the impact of adverse neighboring information on node embeddings.
We conduct extensive experiments over four real-world datasets
to validate the effectiveness of FedSpray. The experimental results
demonstrate the superiority of our proposed FedSpray compared
with the state-of-the-art baselines.
ACKNOWLEDGMENTS
This work is supported in part by the National Science Founda-
tion under grants IIS-2006844, IIS-2144209, IIS-2223769, IIS-2331315,
CNS-2154962, and BCS-2228534; the Commonwealth Cyber Ini-
tiative Awards under grants VV-1Q23-007, HV-2Q23-003, and VV-
1Q24-011; the JP Morgan Chase Faculty Research Award; the Cisco
Faculty Research Award; and Snap gift funding.
 
835KDD ’24, August 25–29, 2024, Barcelona, Spain Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, & Jundong Li
REFERENCES
[1]Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-
vre. 2008. Fast unfolding of communities in large networks. Journal of statistical
mechanics: theory and experiment (2008).
[2]Lei Cai, Jundong Li, Jie Wang, and Shuiwang Ji. 2021. Line graph neural networks
for link prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence
(2021).
[3]Hong-You Chen and Wei-Lun Chao. 2022. On Bridging Generic and Personal-
ized Federated Learning for Image Classification. In International Conference on
Learning Representations.
[4]Mingyang Chen, Wen Zhang, Zonggang Yuan, Yantao Jia, and Huajun Chen. 2021.
Fede: Embedding knowledge graphs in federated setting. In The 10th International
Joint Conference on Knowledge Graphs.
[5]Daniel Daza, Michael Cochez, and Paul Groth. 2021. Inductive entity repre-
sentations from text via link prediction. In Proceedings of the Web Conference
2021.
[6]Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. 2020. Adaptive
personalized federated learning. arXiv preprint arXiv:2003.13461 (2020).
[7]Yushun Dong, Binchi Zhang, Yiling Yuan, Na Zou, Qi Wang, and Jundong Li. 2023.
Reliant: Fair knowledge distillation for graph neural networks. In Proceedings of
the 2023 SIAM International Conference on Data Mining (SDM).
[8]Santo Fortunato and Darko Hric. 2016. Community detection in networks: A
user guide. Physics reports 659 (2016), 1–44.
[9]Xingbo Fu, Chen Chen, Yushun Dong, Anil Vullikanti, Eili Klein, Gregory Madden,
and Jundong Li. 2023. Spatial-Temporal Networks for Antibiogram Pattern
Prediction. In 2023 IEEE 11th International Conference on Healthcare Informatics
(ICHI).
[10] Xingbo Fu, Binchi Zhang, Yushun Dong, Chen Chen, and Jundong Li. 2022. Feder-
ated graph machine learning: A survey of concepts, techniques, and applications.
ACM SIGKDD Explorations Newsletter (2022).
[11] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In Advances in neural information processing systems.
[12] Yilin He, Chaojie Wang, Hao Zhang, Bo Chen, and Mingyuan Zhou. 2022. A
variational edge partition model for supervised graph representation learning.
Advances in Neural Information Processing Systems 35 (2022), 12339–12351.
[13] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al .2015. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531 (2015).
[14] Wenke Huang, Guancheng Wan, Mang Ye, and Bo Du. 2023. Federated graph
semantic and structural learning. In Proc. Int. Joint Conf. Artif. Intell.
[15] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
Stich, and Ananda Theertha Suresh. 2020. Scaffold: Stochastic controlled averag-
ing for federated learning. In International Conference on Machine Learning.
[16] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-
mization. In International Conference on Learning Representations.
[17] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph
convolutional networks. In International Conference on Learning Representations.
[18] Qinbin Li, Bingsheng He, and Dawn Song. 2021. Model-contrastive federated
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition.
[19] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. 2021. Ditto: Fair and
robust federated learning through personalization. In International Conference on
Machine Learning.
[20] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith. 2020. Federated optimization in heterogeneous networks. In
Proceedings of Machine learning and systems.
[21] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann Heng. 2021. Feddg:
Federated domain generalization on medical image segmentation via episodic
learning in continuous frequency space. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition.
[22] Zhiwei Liu, Liangwei Yang, Ziwei Fan, Hao Peng, and Philip S Yu. 2021. Feder-
ated social recommendation with graph neural network. ACM Transactions on
Intelligent Systems and Technology (2021).
[23] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan
Zhang, Xiao-Wen Chang, and Doina Precup. 2022. Revisiting heterophily for
graph neural networks. Advances in neural information processing systems (2022).
[24] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. 2022. Is Homophily a Ne-
cessity for Graph Neural Networks?. In International Conference on Learning
Representations.
[25] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial intelligence and statistics.
[26] Péter Mernyei and Cătălina Cangea. 2020. Wiki-cs: A wikipedia-based benchmark
for graph neural networks. arXiv preprint arXiv:2007.02901 (2020).
[27] Xutong Mu, Yulong Shen, Ke Cheng, Xueli Geng, Jiaxuan Fu, Tao Zhang, and
Zhiwei Zhang. 2023. Fedproc: Prototypical contrastive federated learning on
non-iid data. Future Generation Computer Systems (2023).[28] Jaehoon Oh, Sangmook Kim, and Se-Young Yun. 2022. Fedbabu: Towards en-
hanced representation for federated image classification. In International Confer-
ence on Learning Representations.
[29] Liang Peng, Nan Wang, Nicha Dvornek, Xiaofeng Zhu, and Xiaoxiao Li. 2022.
Fedni: Federated graph learning with network inpainting for population-based
disease prediction. IEEE Transactions on Medical Imaging (2022).
[30] Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. 2020. Clustered feder-
ated learning: Model-agnostic distributed multitask optimization under privacy
constraints. IEEE Transactions on Neural Networks and Learning Systems (2020).
[31] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine
(2008).
[32] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. 2021. Personalized
federated learning using hypernetworks. In International Conference on Machine
Learning.
[33] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).
[34] Yujun Shi, Jian Liang, Wenqing Zhang, Vincent YF Tan, and Song Bai. 2023.
Towards Understanding and Mitigating Dimensional Collapse in Heterogeneous
Federated Learning. In International Conference on Learning Representations.
[35] Jaeyun Song, Joonhyung Park, and Eunho Yang. 2022. TAM: topology-aware
margin loss for class-imbalanced node classification. In International Conference
on Machine Learning.
[36] Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuantao Xie, and Weijian Sun.
2020. Feded: Federated learning via ensemble distillation for medical relation
extraction. In Proceedings of the 2020 conference on empirical methods in natural
language processing (EMNLP).
[37] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. 2022. Towards person-
alized federated learning. IEEE Transactions on Neural Networks and Learning
Systems (2022).
[38] Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang.
2023. Federated learning on non-iid graphs via structural knowledge sharing. In
Proceedings of the AAAI conference on artificial intelligence.
[39] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and
Chengqi Zhang. 2022. Fedproto: Federated prototype learning across heteroge-
neous clients. In Proceedings of the AAAI Conference on Artificial Intelligence.
[40] Anton Tsitsulin, Benedek Rozemberczki, John Palowitch, and Bryan Perozzi.
2022. Synthetic graph generation to benchmark graph learning. arXiv preprint
arXiv:2204.01376 (2022).
[41] Song Wang, Yushun Dong, Binchi Zhang, Zihan Chen, Xingbo Fu, Yinhan He,
Cong Shen, Chuxu Zhang, Nitesh V Chawla, and Jundong Li. 2024. Safety in
Graph Machine Learning: Threats and Safeguards. arXiv preprint arXiv:2405.11034
(2024).
[42] Song Wang, Xingbo Fu, Kaize Ding, Chen Chen, Huiyuan Chen, and Jundong
Li. 2023. Federated few-shot learning. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining.
[43] Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, and Xing Xie. 2021.
Fedgnn: Federated graph neural network for privacy-preserving recommendation.
InInternational Conference on Machine Learning Workshops.
[44] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In International
conference on machine learning.
[45] Yebo Wu, Li Li, Chunlin Tian, and Chengzhong Xu. 2024. Breaking the Memory
Wall for Heterogeneous Federated Learning with Progressive Training. arXiv
preprint arXiv:2404.13349 (2024).
[46] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems (2020).
[47] Han Xie, Jing Ma, Li Xiong, and Carl Yang. 2021. Federated graph classification
over non-iid graphs. In Advances in neural information processing systems.
[48] Han Xie, Li Xiong, and Carl Yang. 2023. Federated node classification over graphs
with latent link-type heterogeneity. In Proceedings of the ACM Web Conference
2023.
[49] Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.
2022. Two sides of the same coin: Heterophily and oversmoothing in graph
convolutional neural networks. In IEEE International Conference on Data Mining.
[50] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor
Prasanna. 2020. Graphsaint: Graph sampling based inductive learning method.
InInternational Conference on Learning Representations.
[51] Ke Zhang, Carl Yang, Xiaoxiao Li, Lichao Sun, and Siu Ming Yiu. 2021. Subgraph
federated learning with missing neighbor generation. In Advances in neural
information processing systems.
[52] Shichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. 2022. Graph-less Neural
Networks: Teaching Old MLPs New Tricks Via Distillation. In International
Conference on Learning Representations.
 
836Federated Graph Learning with Structure Proxy Alignment KDD ’24, August 25–29, 2024, Barcelona, Spain
A PROOF OF PROPOSITION 3.1
Proposition 3.1. Given a set of 𝐾clients, each client 𝑘owns a local
graphG(𝑘)∼Gen(𝝁1,𝝁2,𝑝(𝑘),𝑞(𝑘)),𝑑𝑖𝑠𝑡=||𝝁1−𝝁2||2
2, which is
smaller than 𝑑𝑖𝑠𝑡′=
1+Í𝐾
𝑘=1(1−𝑞(𝑘))(𝑝(𝑘)−1
2)||𝝁1−𝝁2||2
2.
Proof. Without loss of generality, we assume that the majority
class is𝑐1for each client 𝑘=1,2,···,𝑀and𝑐2for each client
𝑘=𝑀+1,𝑀+2,···,𝐾. Based on the neighborhood distributions,
the neighboring features aggregated by the message-passing mech-
anism in GNNs follow Gaussian distribution
h(𝑘)
𝑖∼𝑁 
𝑝(𝑘)𝝁1+(1−𝑝(𝑘))𝝁2,I√︁
|N(𝑣𝑖)|!
(16)
for each client 𝑘=1,2,···,𝑀, and
h(𝑘)
𝑖∼𝑁 
(1−𝑝(𝑘))𝝁1+𝑝(𝑘)𝝁2,I√︁
|N(𝑣𝑖)|!
(17)
for each client 𝑘=𝑀+1,𝑀+2,···,𝐾.
The expectation of node embeddings after the message-passing
mechanism will be E𝑐1[x(𝑘)
𝑖+h(𝑘)
𝑖]for class𝑐1andE𝑐2[x(𝑘)
𝑖+h(𝑘)
𝑖]
for class𝑐2. We omit the linear transformation because it can be
absorbed in the linear GNN classifiers. The decision boundary of
the optimal linear classifier is defined by the hyperplane Pthat is
orthogonal to
E𝑐1[x(𝑘)
𝑖+h(𝑘)
𝑖]−E𝑐2[x(𝑘)
𝑖+h(𝑘)
𝑖]
=E𝑐1[x(𝑘)
𝑖]+E𝑐1[h(𝑘)
𝑖]−E𝑐2[x(𝑘)
𝑖]−E𝑐2[h(𝑘)
𝑖](18)
For each client 𝑘, we have E𝑐1[h(𝑘)
𝑖]=E𝑐2[h(𝑘)
𝑖]. Therefore,
E𝑐1[x(𝑘)
𝑖+h(𝑘)
𝑖]−E𝑐2[x(𝑘)
𝑖+h(𝑘)
𝑖]
=E𝑐1[x(𝑘)
𝑖]−E𝑐2[x(𝑘)
𝑖]=𝝁1−𝝁2,(19)
and the distance from each class to Pis
𝑑𝑖𝑠𝑡=||𝝁1−𝝁2||2
2. (20)
Let the server collect neighboring information from each client
via FedAvg. The global neighboring information will be
s1=𝑀∑︁
𝑘=1h(𝑘)
𝑖+𝐾∑︁
𝑘=𝑀+1𝑞(𝑘)h(𝑘)
𝑖(21)
for class 1 and
s2=𝑀∑︁
𝑘=1𝑞(𝑘)h(𝑘)
𝑖+𝐾∑︁
𝑘=𝑀+1h(𝑘)
𝑖(22)
for class 2. In this case, we replace h(𝑘)
𝑖in Eq. (19) and get the new
hyperplaneP′that is orthogonal to
E𝑐1[x(𝑘)
𝑖+s1]−E𝑐2[x(𝑘)
𝑖+s2]
=E𝑐1[x(𝑘)
𝑖]+E𝑐1[s1]−E𝑐2[x(𝑘)
𝑖]−E𝑐2[s2]
=E𝑐1[x(𝑘)
𝑖]−E𝑐2[x(𝑘)
𝑖]+E𝑐1[s1]−E𝑐2[s2]
=𝝁1−𝝁2+E𝑐1[s1]−E𝑐2[s2],(23)
where
E𝑐1[s1]−E𝑐2[s2]=E𝑐1"𝑀∑︁
𝑘=1h(𝑘)
𝑖+𝐾∑︁
𝑘=𝑀+1𝑞(𝑘)h(𝑘)
𝑖#
−E𝑐2"𝑀∑︁
𝑘=1𝑞(𝑘)h(𝑘)
𝑖+𝐾∑︁
𝑘=𝑀+1h(𝑘)
𝑖#
=𝑀∑︁
𝑘=1E𝑐1[h(𝑘)
𝑖]+𝐾∑︁
𝑘=𝑀+1𝑞(𝑘)E𝑐1[h(𝑘)
𝑖]
−𝑀∑︁
𝑘=1𝑞(𝑘)E𝑐2[h(𝑘)
𝑖]−𝐾∑︁
𝑘=𝑀+1E𝑐2[h(𝑘)
𝑖]
=𝑀∑︁
𝑘=1(1−𝑞(𝑘))(𝑝(𝑘)𝝁1+(1−𝑝(𝑘))𝝁2)
+𝐾∑︁
𝑘=𝑀+1(𝑞(𝑘)−1)((1−𝑝(𝑘))𝝁1+𝑝(𝑘)𝝁2)
= 𝐾∑︁
𝑘=1(1−𝑞(𝑘))𝑝(𝑘)−𝐾∑︁
𝑘=𝑀+1(1−𝑞(𝑘))!
𝝁1
+ 𝐾∑︁
𝑘=1(𝑞(𝑘)−1)𝑝(𝑘)−𝑀∑︁
𝑘=1(𝑞(𝑘)−1)!
𝝁2
=𝐾∑︁
𝑘=1(1−𝑞(𝑘))𝑝(𝑘)(𝝁1−𝝁2)
+𝑀∑︁
𝑘=1(1−𝑞(𝑘))𝝁2−𝐾∑︁
𝑘=𝑀+1(1−𝑞(𝑘))𝝁1
=𝐾∑︁
𝑘=1(1−𝑞(𝑘))(𝑝(𝑘)−1
2)(𝝁1−𝝁2)
+(𝝁1+𝝁2
2) 𝑀∑︁
𝑘=1(1−𝑞(𝑘))−𝐾∑︁
𝑘=𝑀+1(1−𝑞(𝑘))!
.
Given the balanced global distribution where we have the same
number of nodes from class 𝑐1and𝑐2,Í𝑀
𝑘=1(1−𝑞(𝑘))−Í𝐾
𝑘=𝑀+1(1−
𝑞(𝑘))in the second term will be equal to 0. Therefore, the above
equation can be simplified as
E𝑐1[s1]−E𝑐2[s2]=𝐾∑︁
𝑘=1(1−𝑞(𝑘))(𝑝(𝑘)−1
2)(𝝁1−𝝁2).(24)
Then the new hyperplane P′is orthogonal to
 
1+𝐾∑︁
𝑘=1(1−𝑞(𝑘))(𝑝(𝑘)−1
2)!
(𝝁1−𝝁2), (25)
which is in the same direction of 𝝁1−𝝁2. Given 0<𝑞(𝑘)<1and
1
2<𝑝(𝑘)<1for each client 𝑘, the distance from each class to P′is
𝑑𝑖𝑠𝑡′= 
1+𝐾∑︁
𝑘=1(1−𝑞(𝑘))(𝑝(𝑘)−1
2)!
||𝝁1−𝝁2||2
2,(26)
which completes the proof.
□
 
837KDD ’24, August 25–29, 2024, Barcelona, Spain Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, & Jundong Li
Table 4: The statistics and basic information about the four
datasets adopted for our experiments.
Dataset PubMed WikiCS Physics Flickr
Clients 7 12 12 20
Node Features 500 300 8,415 500
Average Nodes 1,608 861 2,651 4,441
Average Edges 3,600 11,721 14,790 14,331
Classes 3 10 5 7
Table 5: Classification accuracy of FedSpray and Fedavg on
node classification over WikiCS with GCN.
Epochs Node sets FedAvg FedSpray
𝐸=3Overall 80.38±0.85 81.37±0.39
Minority 35.04±2.79 47.43±1.39
𝐸=5Overall 80.53±0.74 81.51±0.45
Minority 35.48±2.52 47.43±1.31
𝐸=10Overall 80.23±0.70 81.43±0.51
Minority 35.13±2.47 46.85±1.18
B EXPERIMENT DETAILS
B.1 Datasets
Here we provide a detailed description of the four datasets we
adopted to support our argument. These datasets are commonly
used in graph learning from various domains: PubMed in citation
network, WikiCS in web knowledge, Physics in co-author graph,
and Flickr in social images. Table 4 summarizes the statistics and
basic information of the distributed graph data.
B.2 Baselines
We compare our FedSpray with six baselines in our experiments.
We provide the details of these baselines as follows.•Local: Models are locally trained on each client using its
local data, without any communication with the server or
other clients for collaborative training.
•FedAvg [25]: It is a foundation method of FL that operates
by aggregating local updates from clients and computing a
weighted average of the updates to update the global model.
•APFL [6]: APFL empowers clients to utilize a combination
of local and global models as their personalized model. Ad-
ditionally, during training, APFL autonomously determines
the optimal mixing parameter for each client, ensuring su-
perior generalization performance, even in the absence of
prior knowledge regarding the diversity among the data of
different clients.
•GCFL [47]: GCFL employs a clustering mechanism based
on gradient sequences to dynamically group local models
using GNN gradients, effectively mitigating heterogeneity
in both graph structures and features.
•FedStar [38]: FedStar is devised to extract and share struc-
tural information among graphs. It accomplishes this through
the utilization of structure embeddings and an independent
structure encoder, which is shared across clients while pre-
serving personalized feature-based knowledge.
•FedLit [48]: FedLit is an FL framework tailored for graphs
with latent link-type heterogeneity. It employs a clustering al-
gorithm to dynamically identify latent link types and utilizes
multiple convolution channels to adapt message-passing ac-
cording to these distinct link types.
B.3 Extra Experimental Results
B.3.1 Results with Varying Local Epochs. In FL, clients usually
perform multiple local training epochs before global aggregation to
reduce communication costs. We show the results of FedSpray and
FedAvg with varying local epochs in Table 5. The results demon-
strate that FedSpray can consistently outperform FedAvg with dif-
ferent local epochs.
 
838