Diverse Intra- and Inter-Domain Activity Style Fusion for
Cross-Person Generalization in Activity Recognition
Junru Zhang
Zhejiang University
Hangzhou, China
junruzhang@zju.edu.cnLang Feng
Zhejiang University
Hangzhou, China
langfeng@zju.edu.cnZhidan Liu
Shenzhen University
Shenzhen, China
liuzhidan@szu.edu.cn
Yuhan Wu
Zhejiang University
Hangzhou, China
wuyuhan@zju.edu.cnYang He
Zhejiang University
Hangzhou, China
he_yang@zju.edu.cnYabo Dong‚àó
Zhejiang University
Hangzhou, China
dongyb@zju.edu.cn
Duanqing Xu
Zhejiang University
Hangzhou, China
xdq@zju.edu.cn
Abstract
Existing domain generalization (DG) methods for cross-person
generalization tasks often face challenges in capturing intra- and
inter-domain style diversity, resulting in domain gaps with the tar-
get domain. In this study, we explore a novel perspective to tackle
this problem, a process conceptualized as domain padding. This pro-
posal aims to enrich the domain diversity by synthesizing intra- and
inter-domain style data while maintaining robustness to class labels.
We instantiate this concept using a conditional diffusion model and
introduce a style-fused sampling strategy to enhance data genera-
tion diversity. In contrast to traditional condition-guided sampling,
our style-fused sampling strategy allows for the flexible use of one
or more random styles to guide data synthesis. This feature presents
a notable advancement: it allows for the maximum utilization of
possible permutations and combinations among existing styles to
generate a broad spectrum of new style instances. Empirical evalu-
ations on a broad range of datasets demonstrate that our generated
data achieves remarkable diversity within the domain space. Both
intra- and inter-domain generated data have proven to be signifi-
cant and valuable, contributing to varying degrees of performance
enhancements. Notably, our approach outperforms state-of-the-art
DG methods in all human activity recognition tasks.
CCS Concepts
‚Ä¢Human-centered computing ‚ÜíUbiquitous computing; ‚Ä¢
Computing methodologies ‚ÜíTransfer learning.
‚àóCorresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671828Keywords
Human Activity Recognition; Domain Generalization; Diffusion
Model; Domain Padding
ACM Reference Format:
Junru Zhang, Lang Feng, Zhidan Liu, Yuhan Wu, Yang He, Yabo Dong,
and Duanqing Xu. 2024. Diverse Intra- and Inter-Domain Activity Style
Fusion for Cross-Person Generalization in Activity Recognition. In Proceed-
ings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671828
1 introduction
Human activity recognition (HAR) is a crucial application of time
series data collected from wearable devices like smartphones and
smartwatches, garnering substantial attention in recent years [ 6,22,
52]. Deep learning (DL) techniques have proven effective in time
series classification (TSC) for HAR tasks [ 50,61,63]. However, a
common assumption underpinning these models is that training and
test data distributions are identically and independently distributed
(i.i.d.) [ 45], a condition that does not often hold up in real life due to
individual differences in activity styles influenced by factors such as
age and gender [ 31,34]. For instance, sensor data distributions can
diverge significantly between younger and older individuals due to
variations in walking speed and frequency, leading to challenges in
achieving cross-person generalization with standard DL models.
Domain generalization (DG) seeks to address this issue [ 45].
Approaches such as domain-invariant [ 1,10,13,30,31,65] and
domain-specific [ 5,28,46,62] methods are designed to extract ro-
bust inter-domain and intra-domain features that can withstand
data distribution shifts across various domains. However, their ef-
fectiveness is reliant on the diversity and breadth of the training
data [ 53]. The challenge arises in HAR tasks, where the collected
training data is often small-scale and lacks the necessary diversity
due to resource constraints on edge devices [ 34,47]. This inherent
diversity scarcity in source domain training data can lead to overfit-
ting to local and narrow inter- or intra-domain features, resulting
in poor generalization to new, unseen domains. As shown in Fig. 1
4213
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Junru Zhang et al.
(a) and (b), the learned features lack required intra- or inter-domain
feature robustness, thereby impeding their generalization to target
domains (red circles).
One promising solution is to enrich training distributions by data
generation. Recent research [ 34] has focused on enhancing training
data richness through standard data augmentation like rotation and
scaling; however, it primarily enhances intra-domain diversity and
falls short of addressing inter-domain variability. As shown in Fig. 1
(c), the augmented data (stars) for source domains (orange and blue
circles) tends to cluster tightly, yet fails to generate the necessary
inter-domain data. The target domain (red circles) thus cannot be
comprehensively represented.
In this work, we focus on the generation of highly diverse data
distributions to address the issue of limited domain diversity in
HAR. We explore a novel perspective to tackle this problem. As
depicted in Fig. 1 (d), the core idea involves enabling synthetic data
(stars) to fill the empty spaces within and across source domains
while maintaining robustness to class labels, a process we concep-
tualize as ‚Äúdomain padding‚Äù. For instance, as illustrated in Fig. 1(e),
we can combine multiple walking styles of an elderly man and a
young man to create a novel inter-domain style or merge multiple
walking styles of a young man to generate a new intra-domain style.
Compared to existing DG methods, our domain padding holds great
potential to generate a more extensive range of unknown style dis-
tributions. This enables TSC models to comprehensively explore a
wide array of intra- and inter-domain variations, contributing to
enhanced generalization in HAR scenarios.
We instantiate our concept using conditional diffusion proba-
bilistic models [ 16,39]. To generate samples with instance-level
diversity, we first design a contrastive learning pipeline [ 9]. It aims
to extract the activity style representations of the available data in
the source domains while preserving their robustness for classifica-
tion tasks. The resulting style representation, denoted as ùëÜùëñ, can be
interpreted as ‚Äúa [class] activity performed in [ ùëÜùëñ] style ‚Äù. We then pro-
pose a novel style-fused sampling strategy for the diffusion model
to achieve domain padding requirements. This involves randomly
combining one or multiple style representations of training samples
within the same class. Styles in each combination are then utilized
to jointly guide the diffusion to generate novel activity samples that
fuse the styles. This innovation presents a notable advancement:
the randomness of the combination (whether originating from dif-
ferent or the same domains) ensures diversity in both inter-domain
and intra-domain, thereby achieving the domain padding, as shown
in Fig. 1 (d) and (e). Moreover, it allows for the maximum utilization
of possible permutations and combinations among existing styles
to generate a broad spectrum of new style instances. Hence, we
term our approach as Diversified Intra- and Inter-domain distribu-
tions via activity Style-fused Diffusion modeling (DI2SDiff)1. We
summarize our main contributions as follows:
‚Ä¢We explore a pivotal challenge hampering the effectiveness of
current DG methods in HAR: diversity scarcity of source do-
main features. In response, we introduce the concept of ‚Äúdomain
padding‚Äù, offering a fresh perspective for enhancing domain di-
versity and ultimately improving DG models‚Äô performance.
1Our code is available at https://github.com/jrzhang33/DI2SDiff.
Domain A Domain B
(a)D
omain-invariant (b) D
omain-specific
(c) S
tandard DA (d ) Ou
rs (e)D
iversify I2domain StyleInter. Intra.A [walking] activity 
in [    ] style
A [walking] activity 
in [    ] styleA [walking] activity 
in [    ] styleA [walking] activity 
in [?] style
A [walking] activity 
in [?] styleElderly
YoungYoungDomain C (target) Augmented Data
SqueezeGap Gap
Gap
ProximityPaddingFigure 1: T-SNE visualization of time-series features extracted
by various methods across three domains in HAR. Existing
representation learning methods result in domain gaps as in
both (a) and (b), covering a small portion of target domain
(red circles). Standard data augmentation (DA) leads to aug-
mented data (stars), with source domains (orange/blue circles)
remaining in close proximity to each other and failing to fill
gaps. Our method (d) creates a comprehensive feature space
by padding domain gaps via the idea of (e).
‚Ä¢We propose to use activity style features as conditions to guide
the diffusion process, extending the information available at the
instance-level beyond mere class labels.
‚Ä¢We propose a novel style-fused sampling strategy, which can
flexibly fuse one or more style conditions to generate new, unseen
samples. This strategy achieves data synthesis diversity both
within and across domains, enabling DI2SDiff to instantiate the
concept of domain padding.
‚Ä¢We conduct extensive empirical evaluations of DI2SDiff across a
board of HAR tasks. Our findings reveal that it markedly diversi-
fies the intra- and inter-domain distribution without introducing
class label noise. Leveraging these high-quality samples, DI2SDiff
outperforms existing solutions, achieving state-of-the-art results
across all cross-person activity recognition tasks.
2 Related Work
Human activity recognition (HAR) uses wearable sensors for
recognizing activities in healthcare and human-computer interac-
tion [ 7,51]. The complexity of daily activities, varying among indi-
viduals with different personal styles, makes recognition challeng-
ing [ 31,61]. With the rise of deep learning, deep neural networks
have been increasingly utilized to extract informative features from
activity signals [ 20,56,59,61]. For example, DeepConvLSTM in-
corporates convolutional and LSTM units for multimodal wearable
sensors [ 32]. MultitaskLSTM extracts features using shared weights,
then classifies activities and estimates intensity separately [3].
Domain generalization (DG) aims to improve model perfor-
mance across different domains. Early works [ 1,10,13,30,65]
focused on utilizing multiple source domains and enforcing do-
main alignment constraints to extract robust features. For example,
DANN [ 1] employed adversarial training to accomplish this task,
but it requires target data during training. To recall more beneficial
features, several methods [ 5,28,46,62] such as mDSDI [ 5] have
been proposed to preserve domain-specific features. Another line
4214Diverse Intra- and Inter-Domain Activity Style Fusion for Cross-Person Generalization in Activity Recognition KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
of research in DG focuses on data augmentation techniques [ 14,
15,23,44,55,64] to explore more robust patterns for improved
generalization, such as generating adversarial examples [14].
Given the practical significance of DG learning for HAR tasks,
researchers [26, 31, 33, 34, 49] have turned their focus to studying
DG problems in this field. For instance, Wilson et al . [49] proposed
an adversarial approach to learn domain-invariant features, which
requires labeled data in the target domain during training. Qian
et al. [31] improved variational autoencoder (VAE) framework [ 21]
to disentangle domain-agnostic and domain-specific features auto-
matically, but domain labels are required. DDLearn [ 34] is a recent
advanced approach that enriches feature diversity by contrasting
augmented views but is limited to standard augmentation tech-
niques that only enrich intra-domain features.
Diffusion models have showcased their remarkable potential
in generating diverse and high-quality samples in various domains,
like computer vision [ 27], natural language processing [ 24], and
decision-making [ 11]. Furthermore, classifier-free guidance models
[17] have achieved impressive outcomes in multimodal modeling,
with wide applications in tasks such as text-to-image synthesis [ 37]
and text-to-motion [ 41]. Considering the potential non-stationary
distribution of time-series data [ 18], we propose harnessing the
power of diffusion models to generate diverse data in HAR tasks,
and thereby enhancing the model‚Äôs generalization ability. Intrigu-
ingly, diffusion models have received limited attention in HAR
tasks. A recent survey on time-series diffusion models [ 25] indi-
cates that although some successful attempts have been made to
apply diffusion models to time-series tasks like interpolation [ 40]
and forecasting [ 4,35], comprehensive investigations in time-series
generation tasks are still lacking. Our study not only establishes
diffusion models for time-series generation but also guides the dif-
fusion model to produce diverse samples, effectively addressing the
challenges of DG in HAR tasks. Our work thus presents a novel
and challenging contribution to the field.
3 Preliminaries
3.1 Problem Statement
In cross-person activity recognition [ 34], a domain is characterized
by a joint probability distribution ùëÉùëã,ùëåacross the product space
of time-series instances X, and the corresponding label space Y.
Each instance Xùëñ‚ààRùêæ√óùêørepresents the values of each time series
obtained from sensors, where ùêæis the dimensionality of features,
andùêøis the temporal length of the series. Moreover, each instance
Xùëñcorresponds to an activity class label ùë¶ùëñ‚àà{1,2,...,ùê∂}, indicat-
ing the specific activity category performed by the subjects, with ùê∂
denoting the total number of activity categories. The domain gen-
eralization challenge lies in the nonintersection and domain differ-
ences between the training and testing sets. Typically, the training
setùê∑ùë†={(Xùëñ,ùë¶ùëñ)}ùëõùë†
ùëñ=1is collected from the labeled source-domain
subjects, where ùëõùë†represents the number of training instances.
Importantly, ùëõùë†is often small in cross-person activity recognition
scenarios, presenting the small-scale challenge. On the other hand,
the test set ùê∑ùë°={(Xùëñ,ùë¶ùëñ)}ùëõùë°
ùëñ=1consists ofùëõùë°instances obtained
from the unseen target-domain subjects and satisfies the condi-
tionùê∑ùë†‚à©ùê∑ùë°=‚àÖ. In addition, source and target domains havedifferent joint probability distributions while sharing the identi-
cal feature space and class label space, i.e., ùëÉùë†(Xùëñ,ùë¶ùëñ)‚â†ùëÉùë°(Xùëñ,ùë¶ùëñ),
andXùë†=Xùë°,Yùë†=Yùë°. The primary objective is to leverage the
available data in ùê∑ùë†to train a TSC model ùëì:X‚ÜíY capable of
effectively generalizing to an inaccessible, unseen test domain ùê∑ùë°,
without any prior exposure to target domain data or domain labels
during training. This task is inherently more challenging than con-
ventional transfer learning settings [ 31,34] due to the disparate
distributions across source and target domains, compounded by
the small-scale settings of the training data.
3.2 Diffusion Probabilistic Model
Diffusion model [ 39] involves training a model distribution ùëùùúÉ(ùë•)
to closely approximate the target ground-truth data distribution
ùëû(ùë•). It assumes distribution ùëùùúÉ(ùë•)as a Markov chain of Gauss-
ian transitions: ùëùùúÉ(ùë•0)=‚à´
ùëùùúÉ(ùë•ùëá)√éùëá
ùë°=1ùëùùúÉ(ùë•ùë°‚àí1|ùë•ùë°)ùëëùë•1:ùêæ, where
ùë•1,...,ùë•ùëádenote the latent variables with the same dimensionality
as original (noiseless) data ùë•0.ùëùùúÉ(ùë•ùëá)‚àºN( 0,I)is the Gaussian
prior.ùëùùúÉ(ùë•ùë°‚àí1|ùë•ùë°)is the trainable reverse process given by
ùëùùúÉ(ùë•ùë°‚àí1|ùë•ùë°):=N(ùë•ùë°‚àí1;ùúáùúÉ(ùë•ùë°,ùë°),ùúéùúÉ(ùë•ùë°,ùë°)). (1)
Diffusion predefines a forward process that progressively adds
Gaussian noise to ùë•0inùëásteps, defined as
ùëû(ùë•ùë°|ùë•ùë°‚àí1):=N
ùë•ùë°;‚àöÔ∏Å
1‚àíùõΩùë°ùë•ùë°‚àí1,ùõΩùë°I
, (2)
whereùõΩùë°‚àà(0,1)is variance schedule for noise control.
Training procedure. The diffusion model‚Äôs parameters ùúÉare
optimized by maximizing the evidence lower bound of the log-
likelihood of the data, i.e., logùëùùúÉ(ùë•0), which can be further simpli-
fied as a surrogate loss [16]:
L(ùúÉ):=Eùë•0,ùë°‚àºU,ùúñ‚àºN( 0,I)
||ùúñ‚àíùúñùúÉ(ùë•ùë°,ùë°)||2
, (3)
whereUis the uniform distribution and the noise predictor ùúñùúÉ(ùë•ùë°,ùë°),
parameterized with a deep neural network, aims to estimate the
noiseùúñat timeùë°givenùë•ùë°. AsùúáùúÉ(ùë•ùë°,ùë°)is determined by ùúñùúÉ(ùë•ùë°,ùë°),
the targetùëùùúÉ(ùë•ùë°‚àí1|ùë•ùë°)can be consequently derived.
Sampling procedure. Given a well-trained ùëùùúÉ, the data gen-
eration procedure begins with a Gaussian noise ùë•ùëá‚àº N( 0,I)
and proceeds by iteratively denoising ùë•ùë°forùë°=ùëá,..., 1through
ùëùùúÉ(ùë•ùë°‚àí1|ùë•ùë°), culminating in the generation of the new data ùë•0.
4 Domain Padding
A major obstacle to achieving domain generalization in HAR tasks
is the limited data diversity of the source domain. This presents
representation learning methods from extracting robust features
to distribution shifts across domains. Moreover, data augmenta-
tion also shows insufficient data richness within the domain space,
particularly the inter-domain distribution.
In response, our work aims to achieve highly diverse data gen-
eration to enrich the training distributions. We propose a novel
perspective, which we refer to as ‚Äúdomain padding‚Äù. The core idea is
to achieve the richness of the domain space by ‚Äúpadding‚Äù the distri-
butional gaps within and between source domains, as demonstrated
in Fig. 1 (d). To ensure the generation of high-quality, diverse data
that can effectively augment the training datasets for HAR models,
domain padding adheres to two key criteria:
4215KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Junru Zhang et al.
‚Ä¢Class-Preserved Generation: The generated data should main-
tain alignment with the original data in terms of class labels,
ensuring consistency.
‚Ä¢Intra- and Inter-Domain Diversity: The generated data should
not only boost diversity within individual domains (i.e., intra-
domain diversity) but should also enrich distribution between
distinct domains (i.e., inter-domain diversity).
The first criterion ensures that the enhanced diversity does not
compromise the semantic integrity of the data. By maintaining con-
sistency in class labels, domain padding contributes meaningfully
to model learning without introducing label noise or confusion. The
second criterion guarantees that the models are exposed to a wide
range of domain variations, thereby enhancing their robustness
against shifts in data distribution.
5 Methodology
We implement domain padding using conditional diffusion mod-
els given their highly-expressive generative capabilities [ 16,57].
The iterative denoising process of diffusion models makes them
exceptionally suited for flexible conditioning mechanisms. In this
framework, given the original dataset ùê∑ùë†, we generate a new sam-
pleÀúùë•0‚àºÀúXùë†using conditional information ùë†‚ààXcondto guide the
generation process. The ensemble of all generated data constitutes
a synthetic dataset, denoted as Àúùê∑ùë†={(ÀúXi,ùë¶ùëñ)}Àúùëõùë†
ùëñ=1, with Àúùëõùë†denoting
the total count of generated samples. Next, we use Àúùë•0to denote an
example of the synthetic samples. The generation objective is to
estimate the conditional data distribution ùëû(Àúùë•|ùë†). This allows us to
generate a synthetic sample Àúùë•0given a specific constraint ùë†. The
conditional diffusion process can be described by:
ùëû(Àúùë•ùë°|Àúùë•ùë°‚àí1,ùë†), ùëùùúÉ(Àúùë•ùë°‚àí1|Àúùë•ùë°,ùë†). (4)
Sequentially performing ùëùùúÉenables the generation of new samples
to capture the attributes of ùë†. However, realizing domain padding
is not a trivial task due to a key aspect: how to guide the diffusion
model to generate diverse activity samples meeting two criteria of
domain padding.
In the following, we introduce the DI2SDiff framework, designed
to enable diffusion to achieve domain padding. In ¬ß5.1, we present
a contrastive learning pipeline that extracts style features to serve
as conditions for the diffusion model. Given a style condition, we
employ classifier-free guidance [ 17] to generate new samples that
meet the first criterion in ¬ß5.2. For the second criterion, we construct
a diverse style combination space for the condition space Xcond
and introduce a style-fused sampling strategy to generate highly
diverse intra and inter-domain data in ¬ß5.3. We finally provide the
workflow of DI2SDiff in ¬ß5.4.
5.1 Activity Style Condition
Conditional diffusion models are typically guided by label or text
prompts that provide task-specific knowledge, such as ‚Äúcreate a
[cartoonish] [cat] image‚Äù [ 29,48,58]. However, the generation of
instance-level time-series data introduces distinct challenges. It is
difficult to capture the complex patterns solely through label or text
prompts due to the inherently high-dimensional and non-stationary
nature [ 18]. To address this issue, we propose the development of astyle conditioner using a contrastive learning approach [ 9]. This ap-
proach has demonstrated robustness in extracting representations
from unlabeled time-series data. The transformed data can retain
the distinctive characteristics of the original data while preserving
the semantic information of the classes. Thus, it is well-suited for
extracting robust instance-level representation, termed as ‚Äústyle ‚Äù,
which can serve as conditions to guide diffusion models.
Delving into specifics, the contrastive learning pipeline consists
of a feature encoder and Transformer on the available training
data. The objective is to maximize the similarity between different
contexts of the same sample and minimize the similarity between
contexts of different samples. Once the module is trained, we utilize
it as style conditioner denoted asùëìstyle. When extracting the style
from the original data Xùëñ, the style conditioner produces a style
vectorùëÜùëñ=ùëìstyle(Xùëñ)‚ààRùêª, whereùêªdenotes the length of the
vector. Consequently, each activity style condition can be inter-
preted as ‚Äúa [ ùë¶ùëñ] activity performed in [ ùëÜùëñ] style ‚Äù, whereùë¶ùëñdenotes
the class of the original data. This approach takes an important
step towards the first criteria of domain padding due to the preser-
vation of class semantics. The aggregation of all context vectors
fromùëõùë†training instances constitutes a set S={ùëÜùëñ}ùëõùë†
ùëñ=1, which can
be further divided into ùê∂class-specific subsets corresponding to ùê∂
classes. Each subset contains style vectors pertaining to a specific
class, expressed asS={S1‚à™S2‚à™¬∑¬∑¬∑‚à™Sùê∂}. In the appendix, we
provide the details of the contrastive learning approach [9].
5.2 Synthesizing with Classifier-Free Guidance
To control the generation of time-series samples, we can leverage
the style inSto guide the conditional sampling process ùëùùúÉ(Àúùë•ùë°‚àí1|Àúùë•ùë°,ùë†)
presented in Eq. (4). To this end, we adopt the classifier-free guid-
ance [ 17], which has proven to be effective in generating data with
specific characteristics. In this framework, the training process is
modified to learn a conditional ùúñùúÉ(Àúùë•ùë°,ùë°,ùë†)and an unconditional
ùúñùúÉ(Àúùë•ùë°,ùë°,‚àÖ), where‚àÖsymbolizes the absence of the condition ùë†. The
loss function is formulated as follows:
L(ùúÉ):=Eùë•0‚àºXùë†,ùúñ‚àºN( 0,I),ùë°‚àºU,ùë†‚àºS
‚à•ùúñ‚àíùúñùúÉ(Àúùë•ùë°,ùë°,ùë†)‚à•2
,(5)
where condition ùë†is one style feature in Sderived from the pre-
trained conditioner, and it is randomly dropped during the training.
During the sampling phase, a sequence of samples Àúùë•ùëá,..., Àúùë•0
is generated starting from Àúùë•ùëá‚àºN( 0,I). For each timestep ùë°, the
model refines the process of denoising Àúùë•ùë°‚àí1based on Àúùë•ùë°through
the following operation:
ÀÜùúñùúÉ=ùúñùúÉ(Àúùë•ùë°,ùë°,‚àÖ)+ùúî ùúñùúÉ(Àúùë•ùë°,ùë°,ùë†)‚àíùúñùúÉ(Àúùë•ùë°,ùë°,‚àÖ), (6)
whereùúîis a scalar hyperparameter that controls alignment between
the guidance signal and the sample [ 17]. Through the iterative
application of Eq. (6), the diffusion model is capable of sampling
new time-series samples that conform to specific styles ùë†‚ààS. It is
worth noting that the styles ùëÜ1,...,ùëÜùëõùë†are robust to the class labels,
the generation process guarantees the first criterion of domain
padding: each generated sample belongs to a known class under
the guidance of a single condition.
5.3 Beyond One Activity Style
So far our approach has not yet achieved the second criterion of
domain padding, as samples conditioned on a singular style ùë†‚ààS
4216Diverse Intra- and Inter-Domain Activity Style Fusion for Cross-Person Generalization in Activity Recognition KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Style
 Conditioner
A sample of [ walking ] label 
with [New]style(a) [walking ] Activities Activity Styles
Condition Space
{     ,     }
{     ,     ,    }{    } {    } {    }{     ,     }{     ,     }
(d)
Noise
Diffusion(b)t
tt(c)
Add
Select{Intra. }
      { Inter. }Data Space 
Pad the gap
Figure 2: Illustration of the diffusion within DI2SDiff. It contains a style conditioner to produce styles and a conditional
diffusion for data generation. Suppose we have three original walking samples: X1,X2, and X3, where X1is from a different
domain while X2and X3come from the same domain. (a) The style conditioner generates style features from the original data.
The style features are randomly combined to build the condition space, in which the combination of inter-domain styles is
indicated by blue brackets and the combination of intra-domain styles is indicated by grey brackets. (b) During training, the
diffusion retrieves each data sample with one style for the forward process. (c) During sampling, the diffusion receives noise
and a style combination, e.g., [ùëÜ1,ùëÜ3], for the reverse process. (d) The generated sample ÀúXùëñis used to diversify the data space.
could demonstrate a limited range of variation within the intra-
domain space. Therefore, we then propose a style-fused sampling
strategy to further enhance the diversity. This strategy guides the
diffusion to generate new data that satisfy any number and combi-
nation of styles (conditions), rather than just one. By doing so, the
generated data fuse diverse inter and intra-domain styles, effectively
meeting the second criterion of domain padding.
Random style combination. Random style combination entails
the ensemble of one or multiple style features under a unified class
to establish a new diffusion sampling condition. Importantly, the
ensemble styles must belong to the same class to preserve class
consistency. For each class label ùëê, we randomly select any number
of style features from the class-specific set Sùëêand combine them
in all possible ways. This will end up with 2ùëò‚àí1different style
combinations (excluding the empty set), where ùëò=|Sùëê|is the
number of styles in Sùëê. Mathematically, the collection of all possible
style combinations for class ùëêcan be expressed by the power set
P(Sùëê)ofSùëê:
P(Sùëê)={Dùëó|Dùëó‚äÜSùëê,Dùëó‚â†‚àÖ}. (7)
For instance in Fig. 2 (c), three styles in Sùëê={ùëÜ1,ùëÜ2,ùëÜ3}results
in 7 different combinations2. This operation is replicated across
all classes 1,...,ùê∂ , integrating them into a comprehensive style
combination setD={P(S1)‚à™¬∑¬∑¬∑‚à™P(Sùê∂)}. The randomness
in selecting style combinations can significantly foster diversity
within and between domains, and maximize the exploitation of
existing styles to generate highly diverse condition space Xcond.
Style-fused sampling. Subsequently, our efforts are directed
towards empowering the diffusion model to fuse multiple styles dur-
ing the data generation conditioned on a specific style combination
Dùëó‚ààD. Assuming the diffusion has learned the data distributions
2P(Sùëê)=
{ùëÜ1},{ùëÜ2},{ùëÜ3},{ùëÜ1,ùëÜ2},{ùëÜ1,ùëÜ3},{ùëÜ2,ùëÜ3},{ùëÜ1,ùëÜ2,ùëÜ3}	{ùúñùúÉ(Àúùë•ùë°,ùë°,ùë†)}ùëõùë†
ùëñ=1through Eq. (5), sampling from the composed data
distribution ùëû(Àúùë•0|Dùëó)for any given style combination Dùëó‚ààD is
achieved using the below perturbed noise:
ÀÜùúñùúÉ=ùúñùúÉ(Àúùë•ùë°,ùë°,‚àÖ)+ùúî‚àëÔ∏Å
ùë†‚ààDùëó
ùúñùúÉ(Àúùë•ùë°,ùë°,ùë†)‚àíùúñùúÉ(Àúùë•ùë°,ùë°,‚àÖ)
. (8)
The derivation of Eq. (8) is provided in the appendix. This indicates
that while the diffusion training process primarily focuses on an
individual style, we can flexibly combine these styles during sam-
pling. For instance, consider the combination of Dùëó={ùëÜ1,ùëÜ3}in
Fig. 2 (c) and (d). Each element represents a style associated with
the [walking ] activity. Eq. (8) can generate new samples with the
[walking ] label possesses unique characteristics that fuse these
two styles. This is critical for inter and intra-domain diversity in
domain padding: diffusion can flexibly incorporate class-specific
instance-level styles from different or the same domains to gener-
ate new samples with novel domain distribution. Moreover, given
the existence of sub-domains within each domain, our diffusion
model is capable of synthesizing novel domains, even from sam-
pling instances within the same domain (we verify this later in the
experiments).
5.4 Workflow of DI2SDiff
Finally, we elaborate on the comprehensive workflow of our ap-
proach, which we refer to as Diversified Intra- and Inter-domain dis-
tributions via activity Style-fused Diffusion modeling (DI2SDiff).
Architectural design. The diffsuion model ùúñùúÉ:ÀúXùë†√óN√ó
Xcond‚ÜíÀúXùë†is built upon a UNet architecture [ 17] with repeated
convolutional residual blocks. To accommodate the characteristics
of time series input, we adapt 2D convolution to 1D temporal convo-
lution. The model incorporates a timestep embedding module and
4217KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Junru Zhang et al.
a condition embedding module, each of which is a multi-layer per-
ceptron (MLP). The condition embedding module is used to encode
each activity style ùë†‚ààS, and in the unconditional case ùë†=‚àÖ, we
zero out the entries of ùë†. These embeddings are then concatenated
and fed into each block of the UNet.
Training. During the training stage, as shown in Fig. 2 (a) and
(b), the pre-trained style conditioner extracts style features {ùëÜùëñ}ùëõùë†
ùëñ=1
for the training instances {Xi}ùëõùë†
ùëñ=1. Each data instance Xi, paired
with its style ùëÜùëñand a randomly sampled timestep ùë°‚àºU, forms a
tripartite input(Xi,ùë°,ùëÜùëñ). This setup facilitates the optimization of
the model against a loss function defined by Eq. (5).
Sampling. During the sampling stage, we construct the style
combination setDby Eq. (7). As shown in Fig. 2 (c), a specific
style combination Dùëó‚ààD is then selected to guide the diffusion
process, generating the new sample that fuses the styles in Dùëó. The
sampling operates under single-condition guidance when the style
combination comprises a single style, i.e., |Dùëó|=1. Conversely,
when the style combination comprises multiple styles, i.e., |Dùëó|>1,
the sampling proceeds under multiple-condition guidance.
Domain space diversity. Through the iterative execution of
the sampling procedure, we can generate a diverse range of new,
unseen samples that meet domain padding criteria. These synthetic
samples collectively form a synthetic dataset Àúùê∑ùë†for TSC models‚Äô
training. This process involves two hyperparameters ùúÖandùëú.ùúÖ
denotes the proportion of synthetic to original training samples,
effectively managing the volume of synthetic samples. ùëúdenotes
the maximum number of style features that can be combined in
each style set.
Training TSC model. Utilizing the synthetic dataset Àúùê∑ùë†, we are
able to augment the training dataset to {Àúùê∑ùë†‚à™ùê∑ùë†}={(Xùëñ,ùë¶ùëñ)}ùëõùë†+Àúùëõùë†
ùëñ=1.
This augmented dataset can be straightforwardly utilized for stan-
dard TSC task training. To extract more value from the dataset,
we consider the diversity learning strategy from [ 34] for the TSC
model‚Äôs training. The main thought goes beyond just minimizing
not only the standard cross-entropy loss for correct classification.
It also involves minimizing the additional cross-entropy loss to
effectively differentiate between synthetic and original samples.
For more details, please refer to the appendix.
6 EXPERIMENTS
In this section, we conduct a comprehensive evaluation of DI2SDiff
across various cross-person activity recognition tasks to demon-
strate (1)its ability to achieve domain padding and significantly
diversify the domain space; (2)its outstanding performance in do-
main generalization; (3)a detailed ablation and sensitivity analysis;
and(4)its versatility in boosting existing DG baselines.
6.1 Experimental Setup
Datasets. We assess our method on three widely used HAR datasets:
UCI Daily and Sports Dataset (DSADS) [ 2], PAMAP2 dataset [ 36]
and USC-HAD dataset [ 60]. We follow the same experimental set-
tings in [ 34] that provided a generalizable cross-person scenario.
Specifically, the subjects are organized into separate groups for
leave-one-out validation. We assign the data of one group as the
target domain and utilize the remaining subjects‚Äô data as the source
domain. Each subject is treated as an independent task.
Sigl cond .
Sigl cond .
Sigl cond .
Mult cond .
(
a) DSADS
Mult cond .
 (
b) PAMAP2
 (
c) USC-HAD
Figure 3: T-SNE visualization of DSADS, PAMAP2, USC-HAD
datasets. Each method generates the same amount of syn-
thetic data. The original and synthetic data are represented
by shapes dots and crosses, and each class is denoted by a
color. Best viewed in color and zoom in.
(a) DSADS (b) PAMAP2 (c) USC-HAD
Figure 4: T-SNE visualization of DSADS, PAMAP2, and USC-
HAR datasets. Each method generates the same amount of
synthetic data. Each domain category is represented by a
color, and the target domain is represented by a red dot. The
original and synthetic data are represented by shapes dots
and crosses, respectively. Best viewed in color and zoom in.
Baselines. We compare our approach with a wide range of
closely related, strong baselines adapted to TSC tasks. We first select
Mixup [54],RSC[19],SimCLR [8],Fish [38], and DDLearn [34], given
their outstanding performance in most recent study [34]. Notably,
DDLearn [34] is ranked as the top-performing method. We also
include TS-TCC [9] for its remarkable generalization performance
in self-supervised learning. Additionally, we incorporate DANN [12]
andmDSDI [5], which are designed to address domain-invariant and
domain-specific feature learning, respectively. In our analysis, the
standard data augmentation (DA) techniques [ 42] are identical to
those employed in [34], such as scaling and jittering.
Architecture and implementation. For fairness, we adopt the
same feature extractor as described in [ 34], which consists of two
blocks for DSADS and PAMAP2, and three blocks for USC-HAD.
4218Diverse Intra- and Inter-Domain Activity Style Fusion for Cross-Person Generalization in Activity Recognition KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 1: Classification accuracy (%) ( ¬±standard deviation) on three public datasets, where each task only comprises 20% of
training data. The best results are marked in bold. ‚ÄúT0-T‚Äù represent different cross-person activity recognition tasks.
T
ar Mixup
[54] RSC [19] SimCLR [8] Fish [38] DANN [12] mDSDI [5] TS-TCC [9] DDLearn [34] OursDSADST0 74.77
(¬±1.76) 54.32 (¬±2 .19) 72.48 (¬±3 .18) 55.06 (¬±1 .60) 72.49 (¬±3 .21) 76.91 (¬±2 .34) 80.47 (¬±0 .53) 87.88 (¬±1 .92) 89.93 (¬±2 .57)
T1 75.78
(¬±3.95) 63.62 (¬±10 .56) 76.61 (¬±2 .56) 62.28 (¬±3 .13) 69.61 (¬±1 .96) 76.02 (¬±1 .56) 79.68 (¬±0 .42) 88.80 (¬±1 .11) 90.17 (¬±0 .84)
T2 74.18
(¬±4.36) 66.48 (¬±1 .80) 78.25 (¬±0 .92) 68.15 (¬±1 .60) 78.97 (¬±4 .06) 72.71 (¬±0 .98) 84.37 (¬±1 .87) 89.21 (¬±1 .23) 91.39 (¬±1 .31)
T3 75.85
(¬±3.45) 64.29 (¬±3 .37) 76.49 (¬±0 .91) 68.83 (¬±3 .83) 78.54 (¬±2 .14) 79.58 (¬±1 .29) 82.09 (¬±2 .51) 85.63 (¬±1 .13) 88.95 (¬±1 .79)
Avg 75.15
(¬±2.36) 62.18 (¬±4 .32) 75.96 (¬±1 .25) 63.58 (¬±0 .37) 74.90 (¬±2 .63) 76.31 (¬±1 .56) 81.65 (¬±1 .33) 87.88 (¬±0 .82) 90.11 (¬±1 .63)P
AMAP2T0 57.81
(¬±0.55) 55.99 (¬±1 .29) 63.28 (¬±3 .33) 54.04 (¬±4 .31) 54.02 (¬±3 .52) 58.70 (¬±3 .14) 64.08 (¬±1 .98) 75.55 (¬±0 .79) 79.58 (¬±2 .46)
T1 81.51
(¬±3.94) 83.08 (¬±2 .42) 81.25 (¬±1 .59) 85.16 (¬±1 .39) 77.21 (¬±3 .79) 83.82 (¬±1 .62) 86.55 (¬±2 .28) 90.07 (¬±2 .40) 94.12 (¬±1 .20)
T2 77.34
(¬±3.33) 78.65 (¬±3 .99) 78.65 (¬±1 .87) 79.69 (¬±4 .00) 78.80 (¬±1 .87) 79.15 (¬±2 .72) 80.21 (¬±0 .52) 85.51 (¬±0 .76) 89.57 (¬±2 .48)
T3 70.31
(¬±5.64) 68.10 (¬±6 .27) 71.09 (¬±1 .99) 72.53 (¬±0 .49) 61.96 (¬±2 .11) 78.61 (¬±0 .49) 77.32 (¬±0 .47) 80.67 (¬±1 .78) 84.75 (¬±3 .72)
Avg 71.74
(¬±1.37) 71.45 (¬±2 .55) 73.57 (¬±1 .21) 72.85 (¬±0 .37) 68.00 (¬±2 .66) 75.07 (¬±1 .99) 77.04 (¬±1 .29) 82.95 (¬±0 .60) 87.01 (¬±1 .94)USC-HADT0 68.66
(¬±4.67) 75.69 (¬±4 .28) 69.36 (¬±2 .34) 73.70 (¬±3 .97) 57.79 (¬±4 .73) 59.71 (¬±1 .23) 78.96 (¬±0 .79) 79.06 (¬±2 .11) 88.33 (¬±1 .70)
T1 68.75
(¬±1.29) 72.40 (¬±2 .88) 66.62 (¬±1 .44) 72.05 (¬±2 .93) 64.95 (¬±2 .68) 67.35 (¬±2 .46) 79.55 (¬±1 .23) 80.15 (¬±1 .11) 81.64 (¬±0 .28)
T2 71.79
(¬±0.65) 72.83 (¬±3 .62) 76.04 (¬±1 .61) 69.10 (¬±2 .93) 71.97 (¬±3 .23) 63.89 (¬±3 .69) 78.15 (¬±2 .15) 80.81 (¬±0 .74) 88.37 (¬±1 .46)
T3 61.29
(¬±3.90) 63.19 (¬±5 .30) 61.24 (¬±1 .06) 58.51 (¬±3 .66) 45.65 (¬±2 .18) 63.87 (¬±4 .92) 64.35 (¬±1 .58) 70.93 (¬±1 .87) 77.84 (¬±1 .10)
T4 65.63
(¬±4.55) 66.75 (¬±3 .25) 62.85 (¬±2 .17) 63.72 (¬±8 .31) 54.94 (¬±3 .56) 55.95 (¬±6 .15) 70.25 (¬±0 .88) 75.87 (¬±2 .99) 83.84 (¬±0 .88)
Avg 67.22
(¬±2.41) 70.17 (¬±3 .51) 67.22 (¬±0 .39) 67.42 (¬±3 .91) 59.06 (¬±2 .65) 62.15 (¬±3 .08) 74.25 (¬±1 .16) 77.36 (¬±0 .99) 84.00 (¬±1 .09)
A
vg All 71.37
67.93 72.25 67.95 67.32 71.18 77.65 82.73 87.04
Each block includes a convolution layer, a pooling layer, and a
batch normalization layer. All baselines, except TS-TCC [ 9], employ
this feature extractor. In each experiment, we report the average
performance and standard deviation over three random seeds. For
detailed experimental setups, including dataset details and training
procedures, please refer to the appendix.
6.2 Domain Padding and Diversity Evaluation
In this part, we demonstrate whether our approach can effectively
diversify the domain space and generate diverse samples that meet
domain padding criteria. To this end, we adopt T-SNE [ 43] to visual-
ize the latent feature space in terms of class and domain dimensions.
(1) Class-Preserved Generation. Firstly, we evaluate the class
consistency of synthetic data, i.e., the first criterion of domain
padding. We employ a class feature extractor, trained with class
labels, to map both original and synthetic data into a class-specific
latent space. The results of single-condition guidance ( |Dùëó|=1)
and multiple-condition guidance ( |Dùëó|>1) are shown in Fig. 3.
It can be observed that all synthetic samples (crosses) are closely
clustered around their corresponding original instances and classes
(dots). This clustering indicates that our method effectively main-
tains class information, avoiding the introduction of class noise; im-
portantly, this holds true under both single and multiple-condition
guidance. Moreover, the use of multiple-condition guidances ap-
pears to enhance class discriminability more than single-condition
guidance in Fig. 3. This enhancement is likely because more guid-
ance signals provide more robust class semantics (akin to ensemble
learning), therefore resulting in a better class alignment.
(2) Intra- and Inter-Domain Diversity. Next, we evaluate
the intra- and inter-domain diversity of the synthetic data, i.e., the
second criterion of domain padding. We train the domain feature
extractor on source domains with domain labels. We then map
source and target data into a domain-specific latent space, and
compare the synthetic data from the standard DA method, single-
condition guidance ( |Dùëó|=1), and multiple-condition guidance
(|Dùëó|>1). The results are shown in Fig. 4.
The findings reveal that the standard DA method generates
tightly clustered samples (crosses) around the original data (dots),falling short of diversifying the domain space, particularly the inter-
domain space. Our single-condition guidance method offers a par-
tial solution and generates sparse data between different domains
thanks to diffusion‚Äôs probabilistic nature. However, relying solely
on a single-style guidance approach has limitations for domain
padding. The introduction of our style combinations in Eq. (7)
makes a substantial improvement: the multiple-condition guidance
excels in ‚Äúpadding‚Äù the distributional gaps both within and between
source domains, as demonstrated in Fig. 4.
Moreover, as indicated in Fig. 4, through multiple-condition
guidance, the synthetic samples (crosses) closely resemble the target
domain data (red dots) and demonstrate less dependence on the
specific characteristics of source domains. This demonstrates that
the fusion of multiple style features creates a new style. This is
of importance for the domain generalization in TSC. Given the
significant differences in individual styles and the small-scale nature
of source domain data, our style-fused sampling demonstrates great
potential to simulate various new and unseen distributions, from
which the TSC model can better adapt to the target domains.
In addition, we can observe in Fig. 4 (c) that the USC-HAD
dataset presents an additional challenge of intra-domain gaps due
to its fragmented and sparsely distributed source domains with
distinct sub-domains. These gaps contribute to an increased distri-
bution shift, posing difficulties for existing DG methods to perform
effectively (We show their results in Tab. 1 in later). Through ran-
dom instance-level style fusion, our approach effectively addresses
this sub-domain challenge, enabling the synthesis of new data dis-
tribution within sub-domains. As a result, our method can yield
exceptional performance on the complex tasks like USC-HAD.
6.3 Generalization Performance
Now we conduct a series of experiments to evaluate the generaliza-
tion performance of DI2SDiff against other strong DG baselines.
Overall performance. Tab. 1 presents a comparative analysis
of the classification accuracies achieved by all DG methods across
three datasets, each task of which comprises 20% of the training
data. As we can see, representation learning baselines that focus
solely on learning domain-invariant features, such as DANN [ 12],
4219KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Junru Zhang et al.
Table 2: Classification accuracy (%) on three public datasets
with varying percentages (%) of used training data.
Per
ct. Mixup
RSC SimCLR Fish DANN mDSDI TS-TCC DDLearn OursDSADS20% 75.15
62.18 75.96 63.58 74.90 76.31 81.65 87.88 90.11
40% 82.48
67.70 75.76 65.82 75.45 76.55 82.54 89.71 91.25
60% 82.70
69.98 75.61 67.65 76.55 77.89 83.78 90.43 92.56
80% 81.58
75.37 74.69 66.03 76.89 79.25 84.12 90.97 94.58
100% 83.44
75.58 76.22 69.35 80.52 79.58 86.57 91.95 95.23
Avg 81.07
70.16 75.65 66.49 76.86 77.92 83.73 90.19 92.75P
AMAP220% 71.74
71.45 73.57 72.85 68.00 75.07 77.04 82.95 87.01
40% 76.69
73.73 74.25 77.02 69.85 72.55 78.35 84.34 87.66
60% 77.83
75.72 74.71 76.04 70.88 76.56 80.15 85.03 88.75
80% 78.00
76.17 74.09 75.13 77.82 77.53 81.78 86.67 89.92
100% 79.72
77.96 74.25 75.49 79.56 78.83 83.45 86.31 90.96
Avg 76.80
75.01 74.17 75.31 73.22 76.11 80.15 85.06 88.86USC-HAD20% 67.22
70.17 67.22 67.42 59.06 62.15 74.25 77.36 84.00
40% 75.30
77.31 69.16 73.54 61.52 68.85 75.32 80.72 84.97
60% 78.14
77.59 71.38 76.09 68.71 76.75 77.84 80.88 87.53
80% 79.76
78.65 71.99 77.21 68.52 77.72 78.91 82.49 89.25
100% 81.27
79.41 72.14 78.92 72.05 78.59 79.15 82.51 91.13
Avg 76.34
76.62 70.38 74.64 65.97 72.81 77.09 80.80 87.38
exhibit suboptimal performance due to the limited diversity of the
training data in HAR. The method mDSDI [ 5], on the other hand,
achieves improved performance by additionally learning domain-
specific features. However, it does not match the performance of
DDLearn [ 34], which utilizes data augmentation, underscoring the
importance of training data diversity in enhancing generalization
in HAR. In contrast, our DI2SDiff, leveraging advanced synthesis
of highly diverse data across both intra- and inter-domain space,
markedly surpasses all baseline methods in every task. In addition,
we observe that all baselines, including DDLearn, demonstrate poor
performance on the USC-HAD dataset. As we discussed in Fig. 4
(c), this decline is due to the presence of sub-domains within the
source domain, which poses a highly challenging problem in DG.
Nevertheless, DI2SDiff adeptly addresses this issue by integrating
instance-level style fusion, thereby synthesizing new data distribu-
tions between the sub-domains. As a result, our approach achieves
outstanding performance, outperforming the second-best method
by a clear margin (6.64%) in USC-HAD.
Data proportion analysis. In Tab. 2, we assess DI2SDiff‚Äôs per-
formance over a range of data volumes by adjusting the propor-
tion of training data from 20% to 100%. The results demonstrate
DI2SDiff‚Äôs consistent superiority over the baseline methods across
various proportions of training data. This highlights the ability
of our approach to efficiently generate informative samples from
varying amounts of available data and effectively learn from them.
As the size of the training sample increases, the advantage of our
method becomes more pronounced. For instance, as we increase
the size from 20% to 100% of USC-HAD, the accuracy improvement
grows from 6.64% to 8.62% compared to the second-best baseline
(DDLearn). This is because the number of style combinations in-
creases exponentially (2ùëò‚àí1) with larger training data volumes,
as shown in Eq. (7). Hence, enlarging the training dataset can pro-
vide significant diversity enhancement of data synthesis, leading
to more substantial gains in the model‚Äôs generalization ability.
6.4 Ablation and Sensitivity Analysis
In this section, we perform an ablation study that focuses on the
main step of DI2SDiff, i.e., generating diverse time-series data viaTable 3: The results of the ablation study on three datasets
and each task is averaged for an overall assessment.
V
ariantsDSADS
PAMAP2 USC-HAD
20%
100% 20% 100% 20% 100%
Standar
d DA 75.58
82.57 70.31 86.41 69.14 75.45
Class Label Guidance 76.25
84.67 72.78 88.52 70.85 76.26
Single Style Sampling 86.98
91.12 83.07 89.64 75.27 83.57
Style-Fused Sampling 90.11
95.23 87.01 90.96 84.00 91.13
(
a)ùëú
 (
b)ùúÖ
Figure 5: Hyperparameter sensitivity analysis on ùëúandùúÖ.
diffusion for data augmentation. We keep the number of synthetic
samples and the training strategy of TSC models the same for
all variants. Additionally, we conduct a sensitivity analysis that
focuses on its two hyperparameters: ùëú, which controls the maximum
number of style features in each style combination, and ùúÖ, which
controls the volume of synthetic data.
Ablation on diffusion model. Our findings, as presented in
Table 3, indicate that the standard DA method and class label guid-
ance3falter in performance. The failure of the class label guidance
sampling suggests that using class labels alone, without instance-
related information, cannot generate high-quality data. In contrast,
the diffusion model that utilizes a single style feature as a condition
achieves better performance, suggesting that leveraging represen-
tation features for instance-specific sampling can boost the quality
of generated data. Moreover, the incorporation of style-fused sam-
pling can further improve generalization by producing samples
with distinct features.
Hyperparameter sensitive analysis. We analyze the sensi-
tivity of our hyperparameters by varying one parameter while
maintaining the others constant. As shown in Fig. 5, increasing the
complexity of style combinations ( ùëú) and the volume of synthetic
data (ùúÖ) generally leads to performance improvement. It becomes
non-sensitive when the value is too large. We find that an ùëúvalue
of 5 for the DSADS and PAMAP2 and an ùëúvalue of 10 for the USC-
HAD sufficiently ensure a diverse range of styles. ùúÖvalues of 1 or 2
strike an effective balance between accuracy and training overhead
for all three datasets. By flexibly tuning these hyperparameters, we
can achieve even greater performance improvements for the TSC
model across various tasks while meeting specific needs.
6.5 Benifits to other DG baselines
We demonstrate the versatility of our approach in boosting the
performance of existing DG baselines. The results are shown in Fig.
6. By incorporating our synthetic data into the training datasets
of baselines, we consistently observe performance improvements
3This involves directly using the class labels, rather than the style features, as the
condition to guide diffusion.
4220Diverse Intra- and Inter-Domain Activity Style Fusion for Cross-Person Generalization in Activity Recognition KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
DSADS_20% PAMAP2_20% USC-HAD_20% DSADS_100% PAMAP2_100% USC-HAD_100%020406080100Accuracy (%)DANN
DANN+
mDSDI
mDSDI+
DDLearn
DDLearn+
Figure 6: Enhancing the performance of DANN [ 12], mDSDI
[5] and DDLearn [ 34] with our data generation (+) on 20%
and 100% training data in three datasets.
across the board, including DANN [ 12], mDSDI [ 5]4, and DDLearn
[34]5. This demonstrates the versatility of integrating our method
to provide additional gains, making it a practical solution for im-
mediate application. The diverse synthetic data of DI2SDiff is thus
ready for use, offering a straightforward way to bolster various
baselines without necessitating further data generation.
7 Conclusion
In this paper, we tackle the key issue of DG in cross-person ac-
tivity recognition, i.e., the limited diversity in source domain. We
introduce a novel concept called ‚Äúdomain padding‚Äù and propose
DI2SDiff to realize this concept. Our approach generates highly
diverse inter- and intra-domain data distributions by utilizing ran-
dom style fusion. Through extensive experimental analyses, we
demonstrate that our generated samples effectively pad domain
gaps. By leveraging these new samples, our DI2SDiff outperforms
advanced DG methods in all HAR tasks. A notable advantage of
our work is its efficient generation of diverse data from a limited
number of labeled samples. This potential enables DI2SDiff to pro-
vide data-driven solutions to various models, thereby reducing the
dependence on costly human data collection.
ACKNOWLEDGEMENTS
This work is supported by Zhejiang Science and Technology Plan
Project under Grant 2023C03183, Key Scientific Research Base for
Digital Conservation of Cave Temples (Zhejiang University), China
NSFC under Grant 62172284, and Guangdong Basic and Applied
Basic Research Foundation under Grant 2022A1515010155.
References
[1]Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran√ßois Laviolette, and
Mario Marchand. 2014. Domain-adversarial neural networks. arXiv preprint
arXiv:1412.4446 (2014).
[2]Billur Barshan and Murat Cihan Y√ºksek. 2014. Recognizing daily and sports
activities in two open source machine learning environments using body-worn
sensor units. Comput. J. 57, 11 (2014), 1649‚Äì1667.
[3]Onur Barut, Li Zhou, and Yan Luo. 2020. Multitask LSTM model for human
activity recognition and intensity estimation using wearable sensor data. IEEE
Internet of Things Journal 7, 9 (2020), 8760‚Äì8768.
[4]Marin Bilo≈°, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and Stephan
G√ºnnemann. 2022. Modeling temporal data as continuous functions with process
diffusion. arXiv preprint arXiv:2211.02590 (2022).
[5]Manh-Ha Bui, Toan Tran, Anh Tran, and Dinh Phung. 2021. Exploiting domain-
specific features to enhance domain generalization. Advances in Neural Informa-
tion Processing Systems 34 (2021), 21189‚Äì21201.
4In mDSDI, our synthetic data is treated as a new domain.
5In DDLearn, our synthetic data is treated as a new augmentation method.[6]Andreas Bulling, Ulf Blanke, and Bernt Schiele. 2014. A tutorial on human activity
recognition using body-worn inertial sensors. ACM Computing Surveys (CSUR)
46, 3 (2014), 1‚Äì33.
[7]Kaixuan Chen, Dalin Zhang, Lina Yao, Bin Guo, Zhiwen Yu, and Yunhao Liu.
2021. Deep learning for sensor-based human activity recognition: Overview,
challenges, and opportunities. ACM Computing Surveys (CSUR) 54, 4 (2021), 1‚Äì40.
[8]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In Interna-
tional conference on machine learning. PMLR, 1597‚Äì1607.
[9]Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong
Kwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-series representation learning via
temporal and contextual contrasting. arXiv preprint arXiv:2106.14112 (2021).
[10] Sarah Erfani, Mahsa Baktashmotlagh, Masud Moshtaghi, Xuan Nguyen, Christo-
pher Leckie, James Bailey, and Rao Kotagiri. 2016. Robust domain generali-
sation by enforcing distribution invariance. In Proceedings of the Twenty-Fifth
International Joint Conference on Artificial Intelligence (IJCAI-16). AAAI Press,
1455‚Äì1461.
[11] Lang Feng, Pengjie Gu, Bo An, and Gang Pan. 2024. Resisting Stochastic Risks
in Diffusion Planners with the Trajectory Aggregation Tree. arXiv preprint
arXiv:2405.17879 (2024).
[12] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, Fran√ßois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The journal of machine learning
research 17, 1 (2016), 2096‚Äì2030.
[13] Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. 2019. Dlow: Domain flow
for adaptation and generalization. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. 2477‚Äì2486.
[14] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).
[15] Kehua Guo, Rui Ding, Tian Qiu, Xiangyuan Zhu, Zheng Wu, Liwei Wang, and Hui
Fang. 2023. Single domain generalization via unsupervised diversity probe. In
Proceedings of the 31st ACM International Conference on Multimedia. 2101‚Äì2111.
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in neural information processing systems 33 (2020), 6840‚Äì6851.
[17] Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv
preprint arXiv:2207.12598 (2022).
[18] Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey, Ruben Sanchez-Romero,
Clark Glymour, and Bernhard Sch√∂lkopf. 2020. Causal discovery from heteroge-
neous/nonstationary data. The Journal of Machine Learning Research 21, 1 (2020),
3482‚Äì3534.
[19] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. 2020. Self-challenging
improves cross-domain generalization. In Computer Vision‚ÄìECCV 2020: 16th
European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part II 16.
Springer, 124‚Äì140.
[20] Wenchao Jiang and Zhaozheng Yin. 2015. Human activity recognition using
wearable sensors by deep convolutional neural networks. In Proceedings of the
23rd ACM international conference on Multimedia. 1307‚Äì1310.
[21] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114 (2013).
[22] Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-
nition using wearable sensors. IEEE communications surveys & tutorials 15, 3
(2012), 1192‚Äì1209.
[23] Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi, Zhengze
Yu, Xiaoya Li, and Boyang Xia. 2021. Progressive domain expansion network
for single domain generalization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 224‚Äì233.
[24] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B
Hashimoto. 2022. Diffusion-lm improves controllable text generation. In Advances
in Neural Information Processing Systems, Vol. 35. 4328‚Äì4343.
[25] Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, and Junbin Gao. 2023. Diffusion
models for time-series applications: a survey. Frontiers of Information Technology
& Electronic Engineering (2023), 1‚Äì23.
[26] Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. 2022. Out-
of-distribution Representation Learning for Time Series Classification. In The
Eleventh International Conference on Learning Representations.
[27] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte,
and Luc Van Gool. 2022. Repaint: Inpainting using denoising diffusion proba-
bilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 11461‚Äì11471.
[28] Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. 2018.
Best sources forward: domain generalization through source-specific nets. In 2018
25th IEEE international conference on image processing (ICIP). IEEE, 1353‚Äì1357.
[29] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu
Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recent
advances in natural language processing via large pre-trained language models:
A survey. Comput. Surveys 56, 2 (2023), 1‚Äì40.
[30] Krikamol Muandet, David Balduzzi, and Bernhard Sch√∂lkopf. 2013. Domain
generalization via invariant feature representation. In International conference on
machine learning. PMLR, 10‚Äì18.
4221KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Junru Zhang et al.
[31] Hangwei Qian, Sinno Jialin Pan, and Chunyan Miao. 2021. Latent independent
excitation for generalizable sensor-based cross-person activity recognition. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 11921‚Äì11929.
[32] Meiying Qiao, Shuhao Yan, Xiaxia Tang, and Chengkuan Xu. 2020. Deep convo-
lutional and LSTM recurrent neural networks for rolling bearing fault diagnosis
under strong noises and variable loads. Ieee Access 8 (2020), 66257‚Äì66269.
[33] Xin Qin, Jindong Wang, Yiqiang Chen, Wang Lu, and Xinlong Jiang. 2022. Do-
main generalization for activity recognition via adaptive feature fusion. ACM
Transactions on Intelligent Systems and Technology 14, 1 (2022), 1‚Äì21.
[34] Xin Qin, Jindong Wang, Shuo Ma, Wang Lu, Yongchun Zhu, Xing Xie, and Yiqiang
Chen. 2023. Generalizable Low-Resource Activity Recognition with Diverse and
Discriminative Representation Learning. arXiv preprint arXiv:2306.04641 (2023).
[35] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Au-
toregressive denoising diffusion models for multivariate probabilistic time series
forecasting. In International Conference on Machine Learning. PMLR, 8857‚Äì8868.
[36] Attila Reiss and Didier Stricker. 2012. Introducing a new benchmarked dataset for
activity monitoring. In 2012 16th international symposium on wearable computers.
IEEE, 108‚Äì109.
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn
Ommer. 2022. High-resolution image synthesis with latent diffusion models. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition .
10684‚Äì10695.
[38] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas
Usunier, and Gabriel Synnaeve. 2021. Gradient matching for domain generaliza-
tion. arXiv preprint arXiv:2104.09937 (2021).
[39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
2015. Deep unsupervised learning using nonequilibrium thermodynamics. In
International conference on machine learning. PMLR, 2256‚Äì2265.
[40] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. Csdi: Con-
ditional score-based diffusion models for probabilistic time series imputation.
Advances in Neural Information Processing Systems 34 (2021), 24804‚Äì24816.
[41] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and
Amit H Bermano. 2022. Human motion diffusion model. arXiv preprint
arXiv:2209.14916 (2022).
[42] Terry T Um, Franz MJ Pfister, Daniel Pichler, Satoshi Endo, Muriel Lang, Sandra
Hirche, Urban Fietzek, and Dana Kuliƒá. 2017. Data augmentation of wearable
sensor data for parkinson‚Äôs disease monitoring using convolutional neural net-
works. In Proceedings of the 19th ACM international conference on multimodal
interaction. 216‚Äì220.
[43] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[44] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino,
and Silvio Savarese. 2018. Generalizing to unseen domains via adversarial data
augmentation. Advances in neural information processing systems 31 (2018).
[45] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu,
Yiqiang Chen, Wenjun Zeng, and Philip Yu. 2022. Generalizing to unseen domains:
A survey on domain generalization. IEEE Transactions on Knowledge and Data
Engineering (2022).
[46] Shujun Wang, Lequan Yu, Kang Li, Xin Yang, Chi-Wing Fu, and Pheng-Ann Heng.
2020. Dofe: Domain-oriented feature embedding for generalizable fundus image
segmentation on unseen datasets. IEEE Transactions on Medical Imaging 39, 12
(2020), 4237‚Äì4248.
[47] Yucheng Wang, Yuecong Xu, Jianfei Yang, Zhenghua Chen, Min Wu, Xiaoli Li,
and Lihua Xie. 2023. Sensor alignment for multivariate time-series unsupervised
domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 37. 10253‚Äì10261.
[48] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren,
Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. 2022. Learning to
prompt for continual learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 139‚Äì149.
[49] Garrett Wilson, Janardhan Rao Doppa, and Diane J Cook. 2020. Multi-source deep
domain adaptation with weak supervision for time-series sensor data. In Proceed-
ings of the 26th ACM SIGKDD international conference on knowledge discovery &
data mining. 1768‚Äì1778.
[50] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng
Long. 2022. Timesnet: Temporal 2d-variation modeling for general time series
analysis. arXiv preprint arXiv:2210.02186 (2022).
[51] Huatao Xu, Pengfei Zhou, Rui Tan, and Mo Li. 2023. Practically Adopting Human
Activity Recognition. In Proceedings of the 29th Annual International Conference
on Mobile Computing and Networking. 1‚Äì15.
[52] Huatao Xu, Pengfei Zhou, Rui Tan, Mo Li, and Guobin Shen. 2021. Limu-bert:
Unleashing the potential of unlabeled data for imu sensing applications. In
Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems.
220‚Äì233.
[53] Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S Du, Ken-ichi Kawarabayashi, and
Stefanie Jegelka. 2020. How neural networks extrapolate: From feedforward to
graph neural networks. arXiv preprint arXiv:2009.11848 (2020).
[54] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and
Wenjun Zhang. 2020. Adversarial domain adaptation with domain mixup. In
Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 6502‚Äì6509.
[55] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. 2020.
Robust and generalizable visual representation learning via random convolutions.
arXiv preprint arXiv:2007.13003 (2020).
[56] Jianbo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiaoli Li, and Shonali Kr-
ishnaswamy. 2015. Deep convolutional neural networks on multichannel time
series for human activity recognition.. In Ijcai, Vol. 15. Buenos Aires, Argentina,
3995‚Äì4001.
[57] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao,
Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023. Diffusion models: A
comprehensive survey of methods and applications. Comput. Surveys 56, 4 (2023),
1‚Äì39.
[58] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li,
Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. 2022.
Glipv2: Unifying localization and vision-language understanding. Advances in
Neural Information Processing Systems 35 (2022), 36067‚Äì36080.
[59] Junru Zhang, Lang Feng, Yang He, Yuhan Wu, and Yabo Dong. 2023. Temporal
Convolutional Explorer Helps Understand 1D-CNN‚Äôs Learning Behavior in Time
Series Classification from Frequency Domain. In Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management. 3351‚Äì3360.
[60] Mi Zhang and Alexander A Sawchuk. 2012. USC-HAD: A daily activity dataset
for ubiquitous activity recognition using wearable sensors. In Proceedings of the
2012 ACM conference on ubiquitous computing. 1036‚Äì1043.
[61] Shibo Zhang, Yaxuan Li, Shen Zhang, Farzad Shahabi, Stephen Xia, Yu Deng,
and Nabil Alshurafa. 2022. Deep learning in human activity recognition with
wearable sensors: A review on advances. Sensors 22, 4 (2022), 1476.
[62] Yi-Fan Zhang, Jindong Wang, Jian Liang, Zhang Zhang, Baosheng Yu, Liang
Wang, Dacheng Tao, and Xing Xie. 2023. Domain-Specific Risk Minimization for
Domain Generalization. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 3409‚Äì3421.
[63] Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang, and Robert X
Gao. 2019. Deep learning and its applications to machine health monitoring.
Mechanical Systems and Signal Processing 115 (2019), 213‚Äì237.
[64] Guangtao Zheng, Mengdi Huai, and Aidong Zhang. 2024. AdvST: Revisiting Data
Augmentations for Single Domain Generalization. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 38. 21832‚Äì21840.
[65] Fan Zhou, Zhuqing Jiang, Changjian Shui, Boyu Wang, and Brahim Chaib-draa.
2020. Domain generalization with optimal transport and metric learning. arXiv
preprint arXiv:2007.10573 2 (2020).
4222