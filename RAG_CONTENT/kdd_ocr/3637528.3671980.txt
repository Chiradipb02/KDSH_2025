Dynamic Neural Dowker Network: Approximating Persistent
Homology in Dynamic Directed Graphs
Hao Li
Wuhan University
Electronic Information School
Wuhan, Hubei, China
whulh@whu.edu.cnHao Jiang∗
Wuhan University
Electronic Information School
Wuhan, Hubei, China
jh@whu.edu.cnFan Jiajun
Wuhan University
Electronic Information School
Wuhan, Hubei, China
2017301200224@whu.edu.cn
Dongsheng Ye
Wuhan University
Electronic Information School
Wuhan, Hubei, China
YesDong@whu.edu.cnLiang Du
Wuhan University
Electronic Information School
Wuhan, Hubei, China
duliang@whu.edu.cn
ABSTRACT
Persistent homology, a fundamental technique within Topological
Data Analysis (TDA), captures structural and shape characteristics
of graphs, yet encounters computational difficulties when applied
to dynamic directed graphs. This paper introduces the Dynamic
Neural Dowker Network (DNDN), a novel framework specifically
designed to approximate the results of dynamic Dowker filtration,
aiming to capture the high-order topological features of dynamic
directed graphs. Our approach creatively uses line graph transfor-
mations to produce both source and sink line graphs, highlighting
the shared neighbor structures that Dowker complexes focus on.
The DNDN incorporates a Source-Sink Line Graph Neural Network
(SSLGNN) layer to effectively capture the neighborhood relation-
ships among dynamic edges. Additionally, we introduce an innova-
tive duality edge fusion mechanism, ensuring that the results for
both the sink and source line graphs adhere to the duality principle
intrinsic to Dowker complexes. Our approach is validated through
comprehensive experiments on real-world datasets, demonstrating
DNDN’s capability not only to effectively approximate dynamic
Dowker filtration results but also to perform exceptionally in dy-
namic graph classification tasks.
CCS CONCEPTS
•Theory of computation →Dynamic graph algorithms; •
Computing methodologies →Machine learning algorithms.
KEYWORDS
Topological Machine Learning, Dynamic Graphs, Graph Neural
Networks, Persistent Homology, Dowker Filtration, Line Graphs
∗Corresponds Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . . $15.00
https://doi.org/10.1145/3637528.3671980ACM Reference Format:
Hao Li, Hao Jiang, Fan Jiajun, Dongsheng Ye, and Liang Du. 2024. Dy-
namic Neural Dowker Network: Approximating Persistent Homology in
Dynamic Directed Graphs. In Proceedings of the 30th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD ’24), August 25–
29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3671980
1 INTRODUCTION
In recent years, an increasing number of researchers are focusing on
integrating high-order topological features with graph learning for
downstream tasks such as node classification, link prediction, and
graph classification [ 6,14,27]. As a method under the framework of
TDA, persistent homology captures multi-scale features of graphs
to describe their structural and shape characteristics [ 11,14,31].
Nevertheless, when the subject of study is dynamic directed graphs
commonly found in the real world, existing methods tailored for
undirected static graphs are inadequate in capturing the topological
information of graphs.
Dynamic Dowker filtration is an effective persistent homology
method for capturing the structural and shape characteristics of
dynamic directed graphs [ 7,29,30]. As demonstrated in fig. 1a,
Dowker complexes focus on capturing the shared neighbor struc-
ture of graphs. This approach is particularly sensitive to the direc-
tion and weight of edges, a feature illustrated with a simple example
in fig. 1b. Such sensitivity makes Dowker complexes adept at an-
alyzing complex, continuously evolving dynamic directed graphs
in the real world. Their ability to discern nuanced relationships
based on edge directionality and weights allows for a more detailed
understanding of these graphs’ topological characteristics.
Similar to other persistent homology methods, Dowker com-
plexes face computational challenges, struggling to efficiently han-
dle dynamic graphs. The work in [ 19] explores the capability of
neural networks to learn persistent homology features in digital
images and filtered cubical complexes. Inspired by graph neural
executors [ 24,25], we aim to develop a learning-based method to
approximate the complex results of Dowker computations. Graph
neural executors are a type of neural network designed to approx-
imate the execution of algorithms on graphs. The literature [ 28]
 
1554
KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Li et al.
𝑣𝑣1
𝑣𝑣2 𝑣𝑣3𝑣𝑣4𝑣𝑣1
𝑣𝑣2𝑣𝑣4Dowker  Source Complex
Dowker  Source ComplexAdd Edge1-dim Complex
2-dim Complex𝑡𝑡0𝑡𝑡1𝑡𝑡2
𝑡𝑡3𝑡𝑡4
𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5𝐺𝐺𝑎𝑎
0-dim PD of Dowker  Source Filtration
0-dim PD of Vietoris –Rips Filtration𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5𝐺𝐺𝑏𝑏𝑡𝑡2𝑡𝑡1 𝑡𝑡0
𝑡𝑡3𝑡𝑡4
𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5
𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5𝐺𝐺𝑐𝑐𝑡𝑡0𝑡𝑡1𝑡𝑡2
𝑡𝑡3𝑡𝑡4
(a)
𝑣𝑣1
𝑣𝑣2 𝑣𝑣3𝑣𝑣4𝑣𝑣1
𝑣𝑣2𝑣𝑣4Dowker  Source Complex
Dowker  Source ComplexAdd Edge1-dim Complex
2-dim Complex𝑡𝑡0𝑡𝑡1𝑡𝑡2
𝑡𝑡3𝑡𝑡4
𝐺𝐺𝑎𝑎
0-dim persistent  barcodes  of Dowker  source filtration
0-dim persistent barcodes  of Vietoris –Rips filtration𝐺𝐺𝑏𝑏𝑡𝑡2𝑡𝑡1 𝑡𝑡0
𝑡𝑡3𝑡𝑡4
𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5 𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5 𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5
𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5 𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5 𝑎𝑎0𝑎𝑎1𝑎𝑎2𝑎𝑎3𝑎𝑎4𝑎𝑎5𝐺𝐺𝑐𝑐𝑡𝑡0𝑡𝑡1𝑡𝑡2
𝑡𝑡3𝑡𝑡4 (b)
Figure 1: (a) A simple example of a Dowker source complex. The existence of a shared neighbor (in this case, 𝑣4) between𝑣1and
𝑣2creates a higher-order relationship, which the Dowker source complex captures. (b) Illustration of Dowker filtration sensitive
to edge weights and directions in graphs: 𝐺𝑎represents a common type of subgraph in social media diffusion graphs. Swapping
the timestamps of two edges ( 𝐺𝑏) or changing the direction of an edge ( 𝐺𝑐) leads to different diffusion. Dowker source filtration
can effectively distinguish these three types of graphs, whereas Vietoris-Rips (VP) filtration generates the same persistent
barcode for all three cases.
has utilized graph neural networks to approximate extended per-
sistence diagrams (EPDs), demonstrating that persistent homology
results can be effectively predicted through embeddings of edges.
However, applying graph neural executors to Dowker dynamic
filtration presents two key challenges. (1) How to approximate
with a neural network the structural features captured by
Dowker complexes. Traditional graph neural networks, relying
on information transfer between a node and its neighbors, are not
adept at directly representing the computational results of Dowker
complexes. (2) How to preserve the inherent duality charac-
teristic of Dowker complexes. Dowker complexes, designed for
directed graphs, can be divided into source Dowker complexes and
sink Dowker complexes [ 1]. According to the principle of duality,
under the same filtration, the Persistence Diagrams (PDs) corre-
sponding to both types of complexes should be identical [ 8]. It is
imperative for the neural executor to ensure consistency between
these two forms of complexes. This involves designing a neural
network architecture that can simultaneously capture and reconcile
the distinct yet complementary information presented in the source
and sink Dowker complexes, reflecting their dual nature in the PDs.
To address the aforementioned challenges, we focus on line
graphs [ 2,3,18]. A line graph 𝐿(𝐺)transforms the edges of a graph
𝐺into nodes of a new graph and connects edges that have a com-
mon node in 𝐺. As depicted in fig. 2, the line graph establishes
a critical linkage between Dowker complexes and Graph Neural
Networks (GNNs), facilitating the direct computation of edge em-
beddings essential for predicting persistent homology. Further, this
paper expands on the definition of directed line graphs. Based on
the direction of the edges, a directed graph is transformed into
𝑣𝑣1
𝑣𝑣2 𝑣𝑣3𝑣𝑣4
⃑𝑥𝑥3(𝑡𝑡)⃑𝑥𝑥1(𝑡𝑡)
⃑𝑥𝑥4(𝑡𝑡)
⃑𝑥𝑥2(𝑡𝑡)𝑈𝑈(⃑𝑥𝑥1(𝑡𝑡),𝑀𝑀(⃑𝑥𝑥1𝑡𝑡,⃑𝑥𝑥4𝑡𝑡))⃑𝑥𝑥1(𝑡𝑡+1)
Dowker  Source Complex Node- based MPNN
𝑒𝑒43 𝑒𝑒42𝑒𝑒41
𝑒𝑒42 𝑒𝑒43𝑒𝑒41⃑𝑒𝑒41(𝑡𝑡)
⃑𝑒𝑒42(𝑡𝑡)⃑𝑒𝑒43(𝑡𝑡)⃑𝑒𝑒41(𝑡𝑡+1)
𝑈𝑈(⃑𝑒𝑒41𝑡𝑡,𝑀𝑀(⃑𝑒𝑒41𝑡𝑡,⃑𝑒𝑒42𝑡𝑡,⃑𝑒𝑒43(𝑡𝑡)))
Edge- based MPNN Source Line GraphFigure 2: An example demonstrating the relationship be-
tween the source Dowker complex and the source line graph
for a directed graph. In the source line graph, the presence of
an edge between nodes representing 𝑒41and𝑒42reflects the
shared neighbor ( 𝑣4) between𝑣1and𝑣2in the original graph.
This edge-based perspective is particularly allows the neu-
ral network to focus on the interactions and relationships
between edges.
source and sink line graphs, corresponding to Dowker complexes.
This approach aligns well with the duality requirement of Dowker
complexes.
Specifically, we develop the Dynamic Neural Dowker Network
(DNDN), designed to approximate the computational results of
dynamic Dowker filtration and further validated its effectiveness
 
1555Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
in dynamic graph classification tasks. Initially, we employ a line
graph transformation method to convert the original directed graph
into both source and sink line graphs, capturing the intricate edge
transformation processes in dynamic directed graphs. Subsequently,
we utilize a Source-Sink Line Graph Neural Network (SSLGNN) to
capture the neighbor structures of dynamic edges, aiming to align
with the computational outcomes of dynamic Dowker filtration
and adapt to the evolving nature of dynamic graphs. Within the
SSLGNN framework, we introduce a duality edge fusion mecha-
nism, ensuring that the neural execution results for both the sink
and source line graphs meet the duality requirements of Dowker
complexes. In essence, our main contributions can be summarized
as follows:
•We proposed an innovative line graph transformation method,
which creates sink and source line graphs to directly repre-
sent the shared neighbor structures of interest in Dowker
complexes.
•We developed the Dynamic Neural Dowker Network (DNDN),
leveraging Source-Sink Line Graph Neural Network to an-
alyze complex high-order structures in dynamic directed
graphs. Additionally, we designed a duality edge fusion
mechanism to align with the unique properties of Dowker
complexes.
•We conducted experiments on real-world datasets and found
that DNDN performs well in approximating the computa-
tional results of dynamic Dowker filtration and in graph
classification tasks. These results demonstrate the effective-
ness of our approach in practical applications.
2 RELATED WORKS
In this section, we discuss the relevant research on dynamic graph
neural networks and persistent homology with graph learning.
2.1 Dynamic graph neural networks
Recent studies on dynamic graphs mainly rely on temporal granu-
larity and are divided into two types: discrete-time dynamic graphs
using time snapshots and continuous-time dynamic graphs with
edge timestamps [ 22]. Existing research on dynamic graph neural
networks focuses on extending traditional static GNNs to dynamic
graphs. An intuitive approach involves using time-aware encoders
to capture the dynamic evolution of nodes between discrete snap-
shots. For example, EvolveGCN [ 20] dynamically updates the GNN
model weights using LSTM and GRU. DySAT [ 21] and DHGAT [ 17]
employ attention mechanisms to capture node embeddings through
structural and temporal evolution. Recent dynamic GNNs based on
meta-learning aim to model temporal factors, with meta-learning
adeptly adapting to new temporal data. The Roland framework [ 32]
generates new node embeddings through meta real-time updates.
WinGNN [ 34] utilizes a meta-learning strategy to model associa-
tions in sliding windows of adjacent and consecutive snapshots,
without the need for specific time-aware encoders. However, these
dynamic network studies, based on neighborhood aggregation GNN
architectures, struggle to capture the global topological character-
istics of dynamic graphs from a holistic perspective. Our approach
employs graph neural networks to approximate the results of dy-
namic Dowker filtration, obtaining global topological features ofdynamic graphs and providing a new perspective on understanding
the evolution of dynamic graphs.
2.2 Persistent Homology with Graph Learning
As a significant component in topological machine learning, the in-
tegration of persistent homology with graph learning has garnered
widespread attention. Given the superiority of persistent homology
in capturing the global structural information of graphs, a consid-
erable amount of research has focused on predicting the labels of
entire graphs, that is, graph classification tasks [ 4,6,11–13,15,31].
Some methods [ 5,33] utilize the topological features of node neigh-
borhood subgraphs as representations for node classification tasks.
Similarly, [ 27] constructs neighborhood graphs for each pair of
target nodes and calculates their topological features, applying the
outcomes to link prediction tasks. TOGL [ 14] developed a universal
layer that integrates topological information into the hidden repre-
sentations of nodes, capable of computing topological features at
all scales, and demonstrated the efficacy of this approach in both
graph classification and node classification tasks.
Due to the high computational complexity of traditional persis-
tent homology calculations, some methods have focused on using
neural networks to approximate the results of persistent homology,
especially for large-scale graphs. Inspired by neural executors [ 24],
the work in [ 28] reinterprets the computation of extended persis-
tent homology as a prediction problem of paired edges, which can
be resolved using a union-find algorithm. Subsequently, a graph
neural network is designed to learn this union-find algorithm, and
the effectiveness of the resulting Edge-based Persistence Diagrams
(EPDs) in downstream tasks has been validated.
These studies successfully integrate the topological features of
graphs with graph learning methods. However, most of these ap-
proaches focus predominantly on static, undirected graphs, thereby
overlooking the rich and complex information present in real-world
directed dynamic graphs. Addressing this gap, our paper introduces
the Dynamic Neural Dowker Network, which focuses on machine
learning methods for computing persistent homology, an area less
explored in existing work. We combine edge-based Dowker com-
plexes with neural networks to execute persistence diagram (PD)
computations and apply this approach to dynamic graph classifica-
tion tasks. This innovation extends the application of topological
data analysis to more complex and dynamic network structures.
3 PRELIMINARY AND BACKGROUND
In this section, we first provide a brief overview of the concepts that
are relevant to persistent homology. We then describe the Dowker
filtration on graphs, and finally, we discuss the concepts related to
line graphs.
3.1 Persistent Homology
Persistent homology is a key technique in topological data analysis,
which studies the shape and structure of data. The central idea is to
analyze datasets by constructing a series of topological spaces and
observing the persistence of features across changes in a parameter.
When focusing on persistent homology in the context of graphs, the
approach typically involves examining how graph-based structures
evolve.
 
1556KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Li et al.
Given a graphG=(V,E), whereVis the set of nodes and E
is the set of edges, with 𝑒𝑚𝑛∈E representing an edge between
nodes𝑣𝑚,𝑣𝑛∈V. The key step in computing persistent homology
involves creating a nested sequence of subgraphs G1⊆G 2⊆...⊆
G𝑘=Gbased on a filter function 𝑓, letC𝑖be the simplicial complex
induced byG𝑖. This nested sequence of simplicial complexes C1⊆
C2⊆...⊆C𝑘is referred to as a filtration of G.
Using the additional information provided by the filtration, we
can obtain the persistent homology groups 𝐻and their ranks 𝛽.
The𝛽𝑖,𝑗
𝑘captures the topological features of the graph at various
dimensions 𝑘, such as connected components for 𝑘=0and loops
for𝑘=1,𝑖and𝑗represent the birth and death of these topological
features. The computed persistent homology can be encoded as a
multi-point set in 𝑅2, known as the persistence diagram (PD), where
the𝑥and𝑦coordinates represent the birth and death of topological
features, respectively. This approach to analyzing graphs captures
their underlying topological characteristics, providing valuable in-
sights into their structure and dynamics beyond what is observable
through traditional graph metrics.
3.2 Dowker Filtration
The Dowker Complex is a type of simplicial complex specifically
designed for directed graphs. Given a weighted directed graph G𝑑=
(V𝑑,E𝑑,W𝑑), whereV𝑑is the set of nodes,E𝑑is the set of directed
edges, and𝑒𝑚𝑛∈E𝑑represents a directed edge from source node
𝑣𝑚to target node 𝑣𝑛, withW𝑑(𝑒𝑚𝑛)being the weight of the edge
𝑒𝑚𝑛. The Dowker 𝛿-sink complex is defined as a simplicial complex
as follows:
𝔇𝑠𝑖
𝛿,G:={𝜎=[𝑣0,...,𝑣 𝑛]:there exists 𝑣𝑎∈V
such thatW(𝑒𝑏𝑎)≤𝛿for each𝑣𝑏∈{𝑣 0,...,𝑣 𝑛}}.(1)
The node𝑣𝑎is defined as a 𝛿-sink of a simplex 𝜎if, for all nodes
𝑣𝑏in𝜎, there is a directed edge 𝑒𝑏𝑎from𝑣𝑏to𝑣𝑎with a weight less
than or equal to 𝛿.
Similarly, the Dowker 𝛿-source complex is symmetrically defined
with the roles of source and target reversed:
𝔇𝑠𝑜
𝛿,G:={𝜎=[𝑣0,...,𝑣 𝑛]:there exists 𝑣𝑎∈V
such thatW(𝑒𝑎𝑏)≤𝛿for each𝑣𝑏∈{𝑣 0,...,𝑣 𝑛}}.(2)
With increasing 𝛿, we naturally obtain a sequence of Dowker
complexes, known as the Dowker 𝛿-sink filtration or Dowker 𝛿-
source filtration. This filtration process allows for the examination
of the evolving topological structure of the graph as the parameter
𝛿changes, providing insights into the hierarchical and directional
properties of the graph.

𝔇𝑠𝑖
𝛿↩→𝔇𝑠𝑖
𝛿′	
0≤𝛿≤𝛿′, (3)

𝔇𝑠𝑜
𝛿↩→𝔇𝑠𝑜
𝛿′	
0≤𝛿≤𝛿′. (4)
According to [ 30], by converting the temporal information car-
ried by edges into weights, we can achieve a stable dynamic
Dowker filtration. Given the sensitivity of Dowker filtration to
edge directionality and weights, dynamic Dowker filtration is capa-
ble of deeply analyzing the high-order interaction relationships in
dynamic graphs as they evolve over time. This approach leveragesthe inherent temporal dynamics of edges, allowing for a more nu-
anced understanding of how graphs change and how these changes
impact the topological features of interest.
Dowker Duality. For a given directed graph G𝑑, any threshold
value𝛿∈R, and dimension 𝑘≥0, the persistent modules induced
by the Dowker sink filtration and the Dowker source filtration
are isomorphic. This statement reflects a fundamental property
of Dowker complexes in topological data analysis. The duality of
Dowker complexes necessitates that, in designing neural networks,
we must consider both the distinctions and the eventual consistency
between source Dowker complexes and sink Dowker complexes.
3.3 Line Graph
Given an undirected graph G=(V,E)with𝐸≠∅, the correspond-
ing line graph 𝐿(G)=(V𝐿(G),E𝐿(G))is defined as follows:
V𝐿(G)={𝑣𝑒|𝑒∈E}, (5)
E𝐿(G)={{𝑣𝑒,𝑣𝑓}|𝑒≠𝑓and𝑒∩𝑓≠∅}, (6)
The threshold 𝛿in Dowker filtration is defined on the edges,
which contrasts with the node-centric approach common in tra-
ditional graph neural networks. To address this, our paper ex-
tends the concept of line graphs, introducing sink line graphs and
source line graphs, corresponding to Dowker complexes. Specifi-
cally, given a weighted directed graph G𝑑=(V,E), its correspond-
ing source line graph 𝐿(G𝑑)𝑠𝑜=(V𝑠𝑜
𝐿(G𝑑),E𝑠𝑜
𝐿(G𝑑))and sink line
graph𝐿(G𝑑)𝑠𝑖=(V𝑠𝑖
𝐿(G𝑑),E𝑠𝑖
𝐿(G𝑑))are defined as follows:
V𝑠𝑜
𝐿(G𝑑)=V𝑠𝑖
𝐿(G𝑑)={𝑣𝑒|𝑒∈E}, (7)
E𝑠𝑜
𝐿(G𝑑)={(𝑣𝑎,𝑣𝑏)|𝑎≠𝑏,and𝑠𝑜𝑢𝑟𝑐𝑒(𝑎)=𝑠𝑜𝑢𝑟𝑐𝑒(𝑏)},(8)
E𝑠𝑖
𝐿(G𝑑)={(𝑣𝑎,𝑣𝑏)|𝑎≠𝑏,and𝑡𝑎𝑟𝑔𝑒𝑡(𝑎)=𝑡𝑎𝑟𝑔𝑒𝑡(𝑏)},(9)
where𝑠𝑜𝑢𝑟𝑐𝑒(𝑎)and𝑡𝑎𝑟𝑔𝑒𝑡(𝑎)respectively refer to the source
and target nodes of an edge 𝑎,𝐿(G𝑑)𝑠𝑜and𝐿(G𝑑)𝑠𝑖, are defined as
undirected graphs.This adaptation allows the integration of edge-
centric filtration methods into graph neural network frameworks,
suitable for analyzing dynamic directed graphs.
4 METHOD
Given a dynamic directed graph G𝑡=(V𝑡,E𝑡), our goal is to learn
a function𝐹:E𝑡→R2to approximate the 0-dimensional and
1-dimensional PDs under dynamic Dowker filtration. To predict
the high-order topological features of dynamic graphs, we propose
the overall framework of the Dynamic Neural Dowker Network
(DNDN) as shown in fig. 3. It specifically includes the following
modules: (1) Line Graph Embedding Module. The DNDN obtains
embeddings for each edge in the dynamic graph through a source-
sink line graph neural network combined with an edge fusion layer.
(2) Joint Prediction Module. The DNDN divides the prediction task
into two joint tasks: PD prediction and graph label prediction, and
designs the loss function incorporating the Wasserstein distance.
 
1557Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
Source Edge 
EmbeddingSink Line GraphSSLGNN
SSLGNNInput  𝐺𝐺𝑯𝑯𝟎𝟎=𝑓𝑓(𝑒𝑒)Filtration 𝑓𝑓
Source Line GraphSink Edge Embedding
Edge FusionLayer 1
Edge 
Embedding
𝑯𝑯𝒏𝒏SSLGNN
SSLGNNEdge FusionLayer N ……
0-PDs 
1-PDs 
Graph
Labels
Figure 3: The framework of DNDN
4.1 Line Graph Embedding Module
To obtain edge embeddings for predicting persistent homology un-
der dynamic Dowker filtration, this paper utilizes line graph trans-
formations to derive the source and sink line graphs corresponding
to the dynamic directed graphs. Two distinct line graph neural
networks are employed to calculate embeddings for the source and
sink line graphs, respectively. Finally, an edge fusion mechanism is
implemented to ensure the duality of Dowker complexes.
Line graph transformation. Initially, for the given input dy-
namic directed graph G𝑡=(V𝑡,E𝑡), we transform it into source
and sink line graphs according to eq. (1) and eq. (2). The temporal at-
tributes ofE𝑡are transferred to the nodes in the line graphs V𝑠𝑖
𝐿(G𝑡)
andV𝑠𝑜
𝐿(G𝑡), facilitating subsequent processing by the source-sink
line graph neural network.
Source-sink line graph neural network. Next, we introduce
the backbone neural network layer used for generating edge embed-
dings in line graphs. After the transformation, the original directed
graph is divided into source line graphs and sink line graphs. We
have designed a Source-Sink Line Graph Neural Network (SSLGNN)
to generate their edge embeddings. The SSLGNN consists of two
distinct line graph neural networks and an edge fusion module.
Specifically, the computation process of the 𝑚-th layer of SSLGNN
is as follows:
ℎ𝑚
𝑢𝑠𝑜=𝐴𝐺𝐺𝑚n
𝑀𝑆𝐺𝑚
ℎ𝑚−1
𝑣𝑒𝑓
,𝑣𝑠𝑜∈N(𝑢𝑠𝑜)o
,ℎ𝑚−1
𝑢𝑒𝑓
(10)
ℎ𝑚
𝑢𝑠𝑖=𝐴𝐺𝐺𝑚n
𝑀𝑆𝐺𝑚
ℎ𝑚−1
𝑣𝑒𝑓
,𝑣𝑠𝑖∈N(𝑢𝑠𝑖)o
,ℎ𝑚−1
𝑢𝑒𝑓
(11)
ℎ𝑚
𝑢𝑒𝑓=𝐸𝑑𝑔𝑒𝐹𝑢𝑠𝑖𝑜𝑛
ℎ𝑚
𝑢𝑠𝑜,ℎ𝑚
𝑢𝑠𝑖
(12)
whereℎ𝑚𝑢𝑠𝑜andℎ𝑚𝑢𝑠𝑖represent the features of edge 𝑢at the𝑚-th
layer in the source line graph and sink line graph, respectively, while
ℎ𝑚𝑢𝑒𝑓denotes the features after edge fusion. N(𝑢𝑠𝑜)andN(𝑢𝑠𝑖)
respectively denote the neighborhoods of edge 𝑢in the source line
graph and sink line graph. 𝐴𝐺𝐺 represents the aggregation function,
and𝑀𝑆𝐺 denotes the message-passing function. The configurations
of𝐴𝐺𝐺 and𝑀𝑆𝐺 are consistent with those described in [28].The initial features of an edge ℎ0𝑢𝑠𝑜=ℎ0𝑢𝑠𝑖are computed using a
dynamic Dowker filtration. We have developed a simple yet efficient
dynamic Dowker filtration W(·) to meet both the requirements of
dynamic Dowker duality and dynamic Dowker structural stability
[30]. The initial feature calculation is defined as follows:
ℎ0
𝑢𝑠𝑜=ℎ0
𝑢𝑠𝑖=W(𝑢)=𝑡𝑢−𝑡𝑚𝑖𝑛
𝑡𝑚𝑎𝑥−𝑡𝑚𝑖𝑛, (13)
where𝑡𝑢represents the time at which the edge 𝑢appears, and
𝑡𝑚𝑖𝑛and𝑡𝑚𝑎𝑥 are respectively the minimum and maximum values
among all edge appearance times Tin the dynamic graph.
4.2 Joint Prediction Module
The topological features at different dimensions of a graph represent
various structural aspects, such as connected components for 𝑘=0
and loops for 𝑘=1. However, the quantity of Dowker PDs in a
graph does not directly correlate with the number of nodes or edges.
To better characterize topological features across dimensions, we
have designed a prediction module for simultaneously predicting
the 0-dimensional and 1-dimensional Persistence Diagrams (0-PDs
and 1-PDs) of dynamic graphs. Simultaneously, inspired by the
concept of joint learning, we have designed a graph label prediction
module to accommodate the needs of downstream tasks.
0-PD Prediction. We extend the concept from [ 31] and classify
the elements of a graph’s 0-PD into three elements:
(1)Paired points(𝑎,𝑏), representing a connected component
born at𝑎and dying at 𝑏(merging into a higher-dimensional
topological structure).
(2)Unpaired points(𝑎,+∞), representing a connected compo-
nent that is born but does not die.
(3)Disappearing points (𝑎,𝑎), indicating that the corresponding
connected component quickly dies after formation.
As shown in fig. 4, in graph 𝐺𝑜, with𝑡0<𝑡1<𝑡2<𝑡3<𝑡4,
(𝑡0,+∞),(𝑡1,+∞),(𝑡3,+∞) correspond to the unpaired points for
𝑒𝑡0,𝑒𝑡1,𝑒𝑡3respectively,(𝑡4,𝑡5)corresponds to the paired point for
𝑒𝑡4, and the edges 𝑒𝑡2,𝑒𝑡5forming higher-dimensional complexes
correspond to the disappearing points (𝑡2,𝑡2),(𝑡5,𝑡5). This approach
 
1558KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Li et al.
𝑒𝑒𝑡𝑡0
𝐺𝐺𝑜𝑜𝑒𝑒𝑡𝑡1𝑒𝑒𝑡𝑡2𝑒𝑒𝑡𝑡3𝑒𝑒𝑡𝑡5𝑒𝑒𝑡𝑡41-dim Complex
1-dim Complex𝑣𝑣1
𝑣𝑣2
𝐺𝐺𝑚𝑚
𝐺𝐺𝑛𝑛𝑣𝑣1
𝑣𝑣2𝑣𝑣3𝑒𝑒31
𝑒𝑒32𝑣𝑣3
0-dim Complex
Vietoris -Rips Complex
Dowker Source ComplexVietoris -Rips Complex
Dowker Source Complex
0-dim Complex𝑣𝑣4
𝑣𝑣5𝑒𝑒45𝑒𝑒12
Figure 4: An example demonstrating the 0-PD of Dowker
complexes.
directly correlates edges with 0-PDs without altering the true com-
putational results. Furthermore, we can utilize an MLP layer to
directly predict the PD corresponding to an edge.
𝑃𝐷0=𝑀𝐿𝑃(Hn), (14)
where𝑃𝐷0represents the predicted results for 0-dimensional Per-
sistence Diagrams (0-PD), and 𝐻𝑛denotes the edge embeddings
output by the Source-Sink Line Graph Neural Network (SSLGNN).
1-PD Prediction. The interpretation of the birth and death
of 1-dimensional Dowker PDs from a geometric perspective is
challenging, as each PD point is associated with multiple edges.
Therefore, we propose a neighborhood-based aggregation method
that utilizes dynamic Dowker filtration values W(·) to weight the
aggregation of each edge’s neighborhood, resulting in a subgraph
structural representation. Subsequently, a Multilayer Perceptron
(MLP) layer is employed to calculate the 1-PD.
ℎ𝑛
agg=∑︁
𝑣∈N(𝑢𝑠𝑖)ÐN(𝑢𝑠𝑜)W(𝑣)·ℎ𝑛
𝑣, (15)
𝑃𝐷1=𝑀𝐿𝑃(H𝑛
agg), (16)
in this formulation, ℎ𝑛𝑣belongs to H𝑛and the neighborhood of edge
𝑢, denoted asN(𝑢𝑠𝑖)∪N(𝑢𝑠𝑜), includes its neighborhoods in both
the source and sink line graphs.
Graph Label Prediction. We propose a graph classification
module based on the idea of joint learning, which predicts the
labels of graphs after pooling the edge embeddings.
ˆ𝑦=𝑓(Pooling(H𝑛)) (17)
where ˆ𝑦represents the predicted label of the graph, and Pooling
is the pooling operation, for which we opt for max pooling in our
method.
For the PD prediction task, we employ the 2-Wasserstein distance
between the predicted results and the ground truth as the loss func-
tion. Simultaneously, for the graph classification task, we choose
cross-entropy as the loss function to train the model. This dual
approach ensures that our model is not only capable of accurately
approximating PDs but also effectively classifies graphs, leveraging
the topological features captured by the PDs for enhanced perfor-
mance in classification tasks.5 EXPERIMENTS
In this section, we provide a thorough evaluation of the proposed
Dynamic Neural Dowker Network from three perspectives. Sec-
tion 5.1 details the datasets and baselines employed in our experi-
ments. In Section 5.2, we assess the model’s proficiency in approxi-
mating true persistent homology outcomes. Section 5.3 evaluates
the model’s transferability and efficiency. Building upon the in-
sights gained from the initial experiments, Section 5.4 is dedicated
to validating the model’s performance in graph classification tasks.
The results demonstrate that our algorithm successfully approxi-
mates the computational results of Dowker complexes and can be
effectively applied to downstream graph classification tasks, with
notable efficiency and transferability to larger graphs.1
5.1 Datasets and baselines
The datasets used in the experiments are categorized into two
types: static and dynamic, with both large and small graph datasets
constructed for each category. The static datasets include REDDIT-
BINARY, REDDIT-MULTI-5K and REDDIT-MULTI-12K, where Red-
dit serves as an online forum, with nodes representing users and
edges representing discussion threads. The dynamic datasets en-
compass four categories: citation graphs, Bitcoin graphs, Q&A
graphs and social graphs. The Bitcoin graphs consist of who-trusts-
whom graphs from two Bitcoin platforms: Bitcoin OTC and Bitcoin
Alpha. The citation graphs include two distinct domains of paper
citation graphs: HEP-PH (high energy physics phenomenology) and
HEP-TH (high energy physics theory). The Q&A graphs compile
records from various websites, including StackOverflow, MathOver-
flow, SuperUser, and AskUbuntu. The social graph datasets consist
of Hashtag diffusion graphs from the Weibo platform, categorized
into two types: entertainment and current affairs topics. Table 1
displays the details of these datasets.
Table 1: Statistics of the datasets
DatasetsSmail Graphs Large Graphs
Classes GraphsAvg. Avg.GraphsAvg. Avg.
Nodes Edges Nodes Edges
REDDIT-B 2 1600 233 274 400 1212 1392
REDDIT-5K 5 4000 375 433 1000 1043 1246
REDDIT-12K 11 9507 258 277 2390 924 1066
Citation 2 400 812 898 100 2886 4288
Bitcoin 2 160 412 977 40 880 2996
Q&A 4 800 918 1397 200 4295 5795
Social 2 800 492 458 200 2713 2410
All datasets are divided into 80% small graphs and 20% large
graphs. For the two static network datasets, they are sorted in
ascending order of node count, with the top 80% categorized as
small graphs and the bottom 20% as large graphs, which are then
shuffled. The four dynamic graph datasets are sampled based on
a set target number of edges, where small and large networks are
sampled differently, ensuring no overlap between the two sizes.
1The source code of DNDN is available at https://github.com/Lihaogx/DNDN
 
1559Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
For the persistent diagram approximation experiments, the base-
lines used include PDGNN [ 28], TOGL [ 14], and RePHINE [ 15]. For
the dynamic graph classification experiments, we utilized static
graph embedding methods such as GCN [ 16], GAT [ 23], GraphSage
[10], and GIN [ 26], as well as dynamic graph embedding methods
like DHGAT [ 17], DySAT [ 21], Roland [ 32], EGCNO, and EGCNH
[20].
•PDGNN [28]: A method designed to approximate the ex-
tended persistent homology of graphs based on the Vietoris-
Rips (VP) complexes.
•TOGL [14]: A novel layer that integrates the persistent ho-
mology into Graph Neural Networks (GNNs) based on the
Vietoris-Rips (VP) complexes.
•RePHINE [15]: A method that combines vertex- and edge-
level PH to create a more expressive topological descriptor,
which can be incorporated into GNN layers, enhancing their
ability to learn topological features.
•DySAT [21]: A Euclidean dynamic graph embedding ap-
proach that employs self-attention mechanisms across both
structural and temporal layers.
•DHGAT [17]: A dynamic hyperbolic graph attention net-
work that utilizes a spatiotemporal self-attention mechanism
based on hyperbolic distances.
•EvolveGCN [20]: A Euclidean dynamic graph embedding
model that uses GCNs to capture structural information of
nodes and RNNs to update the GCN parameters directly.
This category includes two architectures: EGCN−Oand
EGCN−H.
•Roland [32]: A Euclidean dynamic graph embedding model
that adapts static GNNs for dynamic graphs by treating node
embeddings at different GNN layers as hierarchical states
and updating them over time. It also approaches the training
process as a meta-learning problem for quick adaptation.
5.2 Approximating PD
In this subsection, we evaluate the approximation error between
the PDs predicted by different methods and the ground truth.
Experimental Setup. Consistent with the setup in [ 28], we use
the following two metrics to assess the quality of the approxima-
tion: (1) The 2-Wasserstein distance between the predicted PD and
the ground truth. (2) The total squared distance between the persis-
tence images (PI) converted from the predicted PD and the ground
truth, denoted as PIE. Our DNDN method uses the 2-Wasserstein
distance(WD) as the loss function, and the suffix _𝑃𝐼indicates that
the method’s loss function is designed based on PIE.
Experiments were conducted on small graph datasets, with the
dataset being randomly split into an 80%/20%distribution for the
training/testing set. For our Dynamic Neural Dowker Network
(DNDN) method, edge features are directly input into the graph
as initial features. On static datasets, the edge filter function is
designed based on the degree of nodes connected by edges. Con-
currently, the line graph transformation generates identical source
and sink line graphs. For models that are designed based on graph
neural networks, node features result from the aggregation of their
edges’ features.Results. As seen in table 2 and table 2, our method exhibits a
significant advantage in approximating Dowker persistent homol-
ogy results, with the best results highlighted in bold. The methods
GIN_PI and GAT_PI, which use PI as the loss function, do not
produce PD outputs, hence the 2-Wasserstein distance cannot be
calculated for them. PDGNN, designed to capture EPD, performs
suboptimally and lacks support for dynamic directed graph data,
leading to poorer performance on some dynamic datasets. Across
both static and dynamic datasets, our method significantly out-
performs the node-based GNN baselines, demonstrating that our
edge-based line graph neural network effectively captures the topo-
logical features corresponding to Dowker complexes.
5.3 Transferability and Efficiency
In this subsection, we designed experiments to investigate the trans-
ferability and efficiency of the DNDN algorithm, focusing primarily
on its adaptability to real-world large graphs that are computation-
ally expensive to analyze.
Experimental Setup. For transferability, we employed pre-
trained models obtained from training on small graph datasets
within the same category and tested them on large graph datasets.
This approach was used to verify DNDN’s capability to learn topo-
logical features from small graphs of the same category and apply
this knowledge to larger graphs. Regarding efficiency, we compared
the time taken by DNDN and the GUDHI[ 9] method to compute
Dowker PDs.
Results. In table 4, "Standard" refers to the results obtained by
training directly on large graph datasets, "Pre_train" denotes the re-
sults of testing pretrained models (trained on small graph datasets)
on large graph datasets, and "Fine_tune" represents the outcomes
after fine-tuning the pretrained models for 10 epoches. Across all
datasets, the performance of models after fine-tuning surpasses
that of models trained directly from scratch. This improvement is
likely attributed to the fine-tuned models having learned more data
information. On dynamic datasets, although "Pre_train" does not
surpass Standard, "fine_tune" generally yields better results than di-
rect training. These experiments validate our model’s transferability
to large graphs, which is crucial for addressing the computational
expense of persistent homology methods on large dynamic datasets.
In table 5, we compare the efficiency of algorithms in GUDHI
and DNDN in processing Dowker PDs on large graph datasets. It is
evident that our method significantly outperforms GUDHI on large
graph datasets, demonstrating the efficiency and applicability of
DNDN in handling complex, large-scale topological data analysis
tasks.
5.4 Graph Classification
In this subsection, having validated DNDN’s ability to approximate
Dowker PDs and its transferability, we explore the algorithm’s
accuracy in downstream tasks.
Experimental Setup. To ascertain DNDN’s efficacy in graph
classification tasks, we orchestrated two experiments: (1) Classifica-
tion on small graphs, employing 5-fold cross-validation across each
dataset for final outcome derivation. (2) Transferable classification
experiment, where the model, initially trained on small graphs,
was subjected to graph classification tasks on larger graphs. All
 
1560KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Li et al.
Table 2: Approximation error on static datasets
MethodREDDIT-B REDDIT-5K REDDIT-12K
WD PIE WD PIE WD PIE
GIN_PI - 1.78e-03 ±7.0e-04 - 2.20e-04 ±4.3e-04 - 5.18e-04 ±4.3e-04
GAT_PI - 1.57e-03 ±3.5e-04 - 5.49e-04 ±1.7e-04 - 7.82e-04 ±2.2e-04
GAT 0.910 ±0.12 7.73e-03±1.0e-02 0.731±0.01 5.36e-04±2.0e-04 0.794±0.01 1.48e-03±3.8e-04
PDGNN 0.679 ±0.29 2.91e-03±3.0e-03 0.697±0.04 5.10e-04±2.2e-04 0.744±0.03 5.10e-04±2.2e-04
TOGL 1.114 ±0.19 1.82e-03±5.0e-04 0.829±0.08 1.95e-03±7.3e-04 1.021±0.04 1.95e-03±7.3e-04
RePHINE 0.816 ±0.01 6.78e-04±1.4e-05 0.523±0.01 4.12e-04±2.5e-04 0.685±0.04 4.12e-04±2.5e-04
DNDN-EF 0.610 ±0.05 7.68e-04±1.3e-04 0.498±0.07 3.78e-04±4.2e-05 0.595±0.03 3.78e-04±4.2e-05
DNDN 0.499±0.01 1.56e-04±1.5e-05 0.317±0.05 5.21e-05±1.5e-05 0.389±0.05 6.73e-05±1.6e-05
Table 3: Approximation error on dynamic datasets
MethodCitation Q&A Bitcoin Social
WD PIE WD PIE WD PIE WD PIE
GIN_PI - 4.71e-04 ±1.6e-04 - 2.73e-03 ±9.1e-04 - 5.15e-03 ±1.8e-03 - 1.44e-03 ±1.9e-04
GAT_PI - 7.82e-04 ±2.2e-04 - 1.93e-03 ±3.6e-04 - 2.80e-03 ±7.9e-04 - 9.04e-04 ±1.1e-04
GAT 0.960±0.11 1.40e-03±6.3e-03 2.508±0.11 1.09e-01±1.8e-01 3.185±1.10 1.44e-01±2.4e-01 0.900±0.01 9.20e-04±3.7e-04
PDGNN 1.313±0.44 1.87e-02±2.0e-02 2.016±0.44 6.81e-02±9.4e-02 3.708±1.74 4.13e-01±4.5e-01 1.010±0.16 2.34e-03±3.4e-03
TOGL 0.935±0.07 2.45e-03±2.0e-03 1.622±0.07 2.12e-02±3.3e-02 2.064±0.19 8.73e-03±3.2e-02 0.943±0.04 1.54e-03±1.3e-03
RePHINE 0.775±0.02 3.38e-04±1.5e-04 1.867±0.02 2.50e-02±8.8e-03 2.270±0.01 4.88e-02±2.3e-02 0.703±0.01 6.81e-04±4.6e-04
DNDN-EF 0.815±0.01 3.98e-04±4.7e-05 1.364±0.01 8.28e-03±1.0e-02 1.442±0.23 1.22e-02±1.1e-02 0.654±0.12 3.15e-04±3.1e-04
DNDN 0.591±0.02 1.29e-04±3.1e-05 0.804±0.02 1.33e-03±7.4e-04 0.908±0.04 2.13e-03±2.1e-04 0.514±0.03 1.01e-04±1.37e-05
Table 4: Transferability across graph datasets of varying sizes (WD)
MethodStatic Network Dynamic Network
REDDIT-B REDDIT-5K REDDIT-12K Citation Bitcoin Q & A Social
Standard 0.553±0.01 0.488±0.04 0.409±0.12 0.838±0.05 0.849±0.04 1.355±0.12 0.737±0.01
Pre_train 0.438±0.01 0.177±0.01 0.183±0.01 0.850±0.05 0.924±0.02 1.268±0.02 0.739±0.01
Fine_tune 0.424±0.01 0.173±0.01 0.176±0.01 0.704±0.01 0.831±0.01 1.121±0.02 0.708±0.01
Table 5: Time evaluation on different datasets (seconds)
MethodStatic Network Dynamic Network
REDDIT-B REDDIT-5K REDDIT-12K Citation Q & A Bitcoin Social
GUDHI 16.46±0.001 4.64±0.002 5.10±0.002 1.37±0.001 1.33±0.002 3.73±0.001 4.85±0.002
DNDN 1.69±0.003 0.66±0.003 0.70±0.003 0.13±0.003 0.14±0.003 0.17±0.001 0.31±0.003
experimental approaches leveraged the mean-pooling method to
derive graph embeddings for label prediction.
Results. As indicated in table 6, ’Small’ denotes the classification
experiments conducted directly on small graph datasets, whereas
’Large’ refers to graph classification on larger datasets after training
on small graph datasets. Dynamic network embedding methods
like DySAT, EGCNO, and DHGAT, which focus on capturing in-
dividual node’s evolving patterns, tend to lose some evolutionary
information after mean-pooling. Conversely, our method achieved
optimal performance in most scenarios, primarily because it learns
Dowker PDs that represent the graph’s topological features, thus
capturing the graph’s global characteristics. Most baseline methods
are designed around a node’s neighborhood, failing to adequatelycapture the graph’s comprehensive information. Particularly in
the transferable classification experiments, DNDN outperformed
across mostly datasets, demonstrating that insights learned from
small graph datasets can be effectively transferred to larger graph
datasets.
6 CONCLUSION
In conclusion, our study introduces the Dynamic Neural Dowker
Network (DNDN), a pioneering framework designed to tackle the
computational complexities associated with applying persistent
homology to dynamic graphs. Through a novel integration of line
graph transformations and a Source-Sink Line Graph Neural Net-
work (SSLGNN), coupled with a duality edge fusion mechanism,
 
1561Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 6: Accuracy on graph classification task
MethodStatic Network Dynamic Network
REDDIT-B REDDIT-5K Citation Q&A Bitcoin Social
Small Large Small Large Small Large Small Large Small Large Small Large
GCN 73.8±0.5 50.1±0.1 33.0±0.3 20.7±0.0 50.0±0.0 52.5±0.7 65.0±0.8 49.5±0.0 84.4±1.1 52.3±0.3 84.0±0.6 88.2±0.2
GAT 79.4±1.2 52.1±0.0 38.1±0.2 20.0±0.0 51.2±0.1 49.0±0.1 41.9±0.2 57.0±0.4 87.5±1.2 50.1±0.3 71.3±1.2 81.3±0.6
GraphSage 79.1±0.4 52.3±0.2 33.2±0.2 21.5±0.0 67.5±0.3 56.0±0.1 76.3±0.7 53.0±0.4 78.1±0.1 62.3±0.3 91.8±1.5 89.5±1.2
GIN 73.8±0.2 51.1±0.0 20.9±0.0 25.5±0.1 83.8±0.3 51.1±0.6 70.0±1.2 68.0±1.8 68.8±1.3 71.2±1.2 81.3±0.6 78.5±0.6
DySAT 78.3±0.7 54.5±0.0 32.2±0.1 22.3±0.0 86.9±1.3 63.0±1.6 78.4±2.5 77.3±2.7 87.4±1.4 72.2±0.7 91.2±1.5 75.2±2.0
DHGAT 76.4±0.1 56.2±0.1 36.7±0.2 21.7±0.0 85.8±1.2 65.9±0.3 76.7±0.1 78.4±0.2 86.4±0.3 65.9±0.3 92.3±0.4 86.5±0.2
EGCN-O 82.2±0.4 61.0±0.0 34.8±0.1 30.1±0.1 84.5±0.8 63.4±0.3 79.2±0.4 78.3±0.6 89.3±0.6 73.3±0.2 89.2±0.4 81.2±0.9
EGCN-H 82.5±0.5 59.5±0.2 32.5±0.0 28.3±0.0 86.7±0.2 62.4±0.5 80.4±0.5 80.2±0.6 89.6±0.4 75.5±0.6 88.6±1.2 80.5±1.4
Roland 84.6±0.2 65.2±0.2 47.2±0.1 40.2±0.2 87.5±0.2 68.2±0.7 78.5±0.6 78.6±0.4 80.2±0.2 72.1±0.5 90.5±0.2 91.2±0.3
DNDN 83.4±0.2 73.6±1.2 56.7±0.4 41.3±1.0 85.6±0.2 72.4±0.4 83.4±0.4 81.2±0.5 84.5±0.5 77.6±1.0 94.5±0.2 89.2±0.1
DNDN effectively captures and approximates the Dowker persistent
homology results of dynamic networks. Our experimental evalu-
ation, spanning both static and dynamic datasets, demonstrates
DNDN’s superior performance in approximating true persistent
homology, highlighting its potential to enhance graph classification
tasks. Notably, the method showcases remarkable transferability,
proving its efficacy in learning topological features from smaller
graphs and applying them to larger counterparts with enhanced
efficiency. In the future, we will focus on two critical directions
to address the current limitations of DNDN: (1) extending the dy-
namic Dowker filtration method to node-level tasks by constructing
dynamic neighborhood subgraphs of nodes to study their higher-
order evolutionary patterns, and (2) building on the approximation
of 0-dimensional and 1-dimensional persistence diagrams (PDs),
exploring methods to approximate higher-dimensional PDs.
ACKNOWLEDGMENTS
This work was supported in part by the National Natural Science
Foundation of China under Grant U19B2004.
REFERENCES
[1]Mehmet E Aktas, Esra Akbas, and Ahmed El Fatmaoui. 2019. Persistence homol-
ogy of networks: methods and applications. Applied Network Science 4, 1 (2019),
1–28.
[2]Lowell W Beineke and Jay S Bagga. 2021. Line graphs and line digraphs. Springer.
[3]Lei Cai, Jundong Li, Jie Wang, and Shuiwang Ji. 2021. Line graph neural networks
for link prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence
44, 9 (2021), 5103–5113.
[4]Mathieu Carrière, Frédéric Chazal, Yuichi Ike, Théo Lacombe, Martin Royer, and
Yuhei Umeda. 2020. Perslay: A neural network layer for persistence diagrams
and new graph topological signatures. In International Conference on Artificial
Intelligence and Statistics. PMLR, 2786–2796.
[5]Yuzhou Chen, Baris Coskunuzer, and Yulia Gel. 2021. Topological relational
learning on graphs. Advances in neural information processing systems 34 (2021),
27029–27042.
[6]Yuzhou Chen, Elena Sizikova, and Yulia R Gel. 2022. TopoAttn-Nets: Topological
Attention in Graph Representation Learning. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases. Springer, 309–325.
[7]Samir Chowdhury and Facundo Mémoli. 2016. Persistent homology of directed
networks. In 2016 50th Asilomar Conference on Signals, Systems and Computers .
IEEE, 77–81.
[8]Clifford H Dowker. 1952. Homology groups of relations. Annals of mathematics
(1952), 84–95.
[9]GUDHI Editorial Board. 2015. GUDHI User and Reference Manual. The GUDHI
Project. Available at http://gudhi.gforge.inria.fr/doc/latest/.
[10] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30(2017).
[11] Christoph Hofer, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland
Kwitt. 2020. Graph filtration learning. In International Conference on Machine
Learning. PMLR, 4314–4323.
[12] Christoph Hofer, Roland Kwitt, Marc Niethammer, and Andreas Uhl. 2017. Deep
learning with topological signatures. Advances in neural information processing
systems 30 (2017).
[13] Christoph D Hofer, Roland Kwitt, and Marc Niethammer. 2019. Learning repre-
sentations of persistence barcodes. Journal of Machine Learning Research 20, 126
(2019), 1–45.
[14] Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, and
Karsten Borgwardt. 2021. Topological graph neural networks. arXiv preprint
arXiv:2102.07835 (2021).
[15] Johanna Immonen, Amauri Souza, and Vikas Garg. 2024. Going beyond persistent
homology using persistent homology. Advances in Neural Information Processing
Systems 36 (2024).
[16] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[17] Hao Li, Hao Jiang, Dongsheng Ye, Qiang Wang, Liang Du, Yuanyuan Zeng,
Yingxue Wang, Cheng Chen, et al .2024. DHGAT: Hyperbolic representation
learning on dynamic graphs via attention networks. Neurocomputing 568 (2024),
127038.
[18] Jinbi Liang and Cunlai Pu. 2023. Line Graph Neural Networks for Link Weight
Prediction. arXiv preprint arXiv:2309.15728 (2023).
[19] Guido Montufar, Nina Otter, and Yu Guang Wang. 2020. Can neural networks
learn persistent homology features?. In TDA{\&}Beyond.
[20] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura,
Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn:
Evolving graph convolutional networks for dynamic graphs. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 34. 5363–5370.
[21] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. 2020.
Dysat: Deep neural representation learning on dynamic graphs via self-attention
networks. In Proceedings of the 13th International Conference on Web Search and
Data Mining. 519–527.
[22] Joakim Skarding, Bogdan Gabrys, and Katarzyna Musial. 2021. Foundations and
modeling of dynamic networks using dynamic graph neural networks: A survey.
iEEE Access 9 (2021), 79143–79168.
[23] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[24] Petar Veličković, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell.
2019. Neural execution of graph algorithms. arXiv preprint arXiv:1910.10593
(2019).
[25] Louis-Pascal Xhonneux, Andreea-Ioana Deac, Petar Veličković, and Jian Tang.
2021. How to transfer algorithmic reasoning knowledge to learn new algorithms?
Advances in Neural Information Processing Systems 34 (2021), 19500–19512.
[26] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[27] Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, and Chao Chen. 2021. Link pre-
diction with persistent homology: An interactive view. In International conference
on machine learning. PMLR, 11659–11669.
[28] Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, Yusu Wang, and Chao Chen.
2022. Neural approximation of graph topological features. Advances in Neural
Information Processing Systems 35 (2022), 33357–33370.
 
1562KDD ’24, August 25–29, 2024, Barcelona, Spain. Hao Li et al.
[29] Dongsheng Ye, Hao Jiang, Jiajun Fan, and Qiang Wang. 2024. Low-rank persistent
probability representation for higher-order role discovery. Expert Systems with
Applications 236 (2024), 121381.
[30] Dongsheng Ye, Hao Jiang, Ying Jiang, and Hao Li. 2023. Stable distance of
persistent homology for dynamic graph comparison. Knowledge-Based Systems
278 (2023), 110855.
[31] Xue Ye, Fang Sun, and Shiming Xiang. 2023. TREPH: A Plug-In Topological Layer
for Graph Neural Networks. Entropy 25, 2 (2023), 331.
[32] Jiaxuan You, Tianyu Du, and Jure Leskovec. 2022. ROLAND: graph learning
framework for dynamic graphs. In Proceedings of the 28th ACM SIGKDD conference
on knowledge discovery and data mining. 2358–2366.
[33] Qi Zhao, Ze Ye, Chao Chen, and Yusu Wang. 2020. Persistence enhanced graph
neural network. In International Conference on Artificial Intelligence and Statistics.
PMLR, 2896–2906.
[34] Yifan Zhu, Fangpeng Cong, Dan Zhang, Wenwen Gong, Qika Lin, Wenzheng Feng,
Yuxiao Dong, and Jie Tang. 2023. WinGNN: dynamic graph neural networks with
random gradient aggregation window. In Proceedings of the 29th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3650–3662.
A DOWKER COMPLEXES
A.1 Diffences between VP complexes and
Dowker complexes
As fig. 5 illustrated, Vietoris-Rips (VP) complexes and Dowker com-
plexes focus on different graph structures. VP complexes directly
concern the neighbor structures on the graph, whereas Dowker
complexes are interested in the shared neighbor structures on the
graph. In graph 𝐺𝑚,𝑣1and𝑣2are connected by edge 𝑒12, corre-
sponding to a one-dimensional complex. In graph 𝐺𝑛,𝑣1and𝑣2
correspond to the same one-dimensional complex because 𝑒31and
𝑒32share a common source node 𝑣3, even though 𝑣1and𝑣2may
not be directly connected by an edge. Traditional graph neural
networks aggregate the neighbors of a node, making them suit-
able for VP complexes but not necessarily for Dowker complexes.
However, we find that the relationship between 𝑒31and𝑒32can be
transformed into the form of a line graph, thus adapting line graph
neural networks for computation with Dowker complexes.
𝑒𝑒𝑡𝑡0
𝐺𝐺𝑜𝑜𝑒𝑒𝑡𝑡1𝑒𝑒𝑡𝑡2𝑒𝑒𝑡𝑡3𝑒𝑒𝑡𝑡5𝑒𝑒𝑡𝑡41-dim Complex
1-dim Complex𝑣𝑣1
𝑣𝑣2
𝐺𝐺𝑚𝑚
𝐺𝐺𝑛𝑛𝑣𝑣1
𝑣𝑣2𝑣𝑣3𝑒𝑒31
𝑒𝑒32𝑣𝑣3
0-dim Complex
Vietoris -Rips Complex
Dowker Source ComplexVietoris -Rips Complex
Dowker Source Complex
0-dim Complex𝑣𝑣4
𝑣𝑣5𝑒𝑒45𝑒𝑒12
Figure 5: An example demonstrating the difference Vietoris-
Rips (VP) complexes and Dowker complexes.B PSEUDOCODE
In this section, we introduce the pseudocode for constructing source
line graphs and sink line graphs, referenced as algorithm 1, and the
pseudocode describing the DNDN model, cited as algorithm 2.
Algorithm 1: Algorithm to construct the source and sink
line graphs from a directed graph G𝑑
Input: Directed graphG𝑑=(V,E), whereVis the set of
vertices andEis the set of directed edges
Output: Source line graph 𝐿(G𝑑)𝑠𝑜=(V𝑠𝑜
𝐿(G𝑑),E𝑠𝑜
𝐿(G𝑑))
and Sink line graph 𝐿(G𝑑)𝑠𝑖=(V𝑠𝑖
𝐿(G𝑑),E𝑠𝑖
𝐿(G𝑑))
1Function ConstructLineGraphs( 𝐺):
2 InitializeV𝑠𝑜
𝐿(G𝑑)←∅;
3 InitializeE𝑠𝑜
𝐿(G𝑑)←∅;
4 InitializeV𝑠𝑖
𝐿(G𝑑)←∅;
5 InitializeE𝑠𝑖
𝐿(G𝑑)←∅;
/* Constructing the Source Line Graph */
6 foreach edge𝑒=(𝑢,𝑣)∈E do
7 Add𝑒toV𝑠𝑜
𝐿(G𝑑);
8 end
9 foreach pair of edges 𝑒1=(𝑢,𝑣)and𝑒2=(𝑢,𝑤)∈E do
10 if𝑒1≠𝑒2then
11 Add edge(𝑒1,𝑒2)toE𝑠𝑜
𝐿(G𝑑);
12 end
13 end
/* Constructing the Sink Line Graph */
14 foreach edge𝑒=(𝑢,𝑣)∈E do
15 Add𝑒toV𝑠𝑖
𝐿(G𝑑);
16 end
17 foreach pair of edges 𝑒1=(𝑢,𝑣)and𝑒2=(𝑤,𝑣)∈E do
18 if𝑒1≠𝑒2then
19 Add edge(𝑒1,𝑒2)toE𝑠𝑖
𝐿(G𝑑);
20 end
21 end
22 return𝐿(G𝑑)𝑠𝑜=(V𝑠𝑜
𝐿(G𝑑),E𝑠𝑜
𝐿(G𝑑)),
𝐿(G𝑑)𝑠𝑖=(V𝑠𝑖
𝐿(G𝑑),E𝑠𝑖
𝐿(G𝑑));
 
1563Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
Algorithm 2: Dynamic Neural Dowker Network (DNDN)
Forward Propagation Algorithm
Input: Source line graph 𝐿(G𝑑)𝑠𝑜=(V𝑠𝑜
𝐿(G𝑑),E𝑠𝑜
𝐿(G𝑑));
sink line graph 𝐿(G𝑑)𝑠𝑖=(V𝑠𝑖
𝐿(G𝑑),E𝑠𝑖
𝐿(G𝑑));
dynamic Dowker filtration W;aggregation function
𝐴𝐺𝐺 ; message-passing function 𝑀𝑆𝐺 ; Edge Fusion
function𝐸𝑑𝑔𝑒𝐹𝑢𝑠𝑖𝑜𝑛 ; neighborhood function N;
multilayer perceptron function 𝑀𝐿𝑃 ; total number
of layers in the network 𝑀
[1]foreach𝑢∈V𝑠𝑜
𝐿(G𝑑)=V𝑠𝑖
𝐿(G𝑑)do
[2]ℎ0𝑢𝑠𝑜←W(𝑢)
[3]ℎ0𝑢𝑠𝑖←W(𝑢)
[4]end
[5]foreach layer𝑚=1to𝑀do
[6] foreach𝑢∈V𝑠𝑜
𝐿(G𝑑)=V𝑠𝑖
𝐿(G𝑑)do
[7] Calculateℎ𝑚𝑢𝑠𝑜for source line graph: ℎ𝑚𝑢𝑠𝑜←
𝐴𝐺𝐺𝑚n
𝑀𝑆𝐺𝑚
ℎ𝑚−1𝑣𝑒𝑓
,𝑣𝑠𝑜∈N(𝑢𝑠𝑜)o
,ℎ𝑚−1𝑢𝑒𝑓
[8] Calculateℎ𝑚𝑢𝑠𝑖for sink line graph: ℎ𝑚𝑢𝑠𝑖←
𝐴𝐺𝐺𝑚n
𝑀𝑆𝐺𝑚
ℎ𝑚−1𝑣𝑒𝑓
,𝑣𝑠𝑖∈N(𝑢𝑠𝑖)o
,ℎ𝑚−1𝑢𝑒𝑓
[9] Perform edge fusion to update features:
ℎ𝑚𝑢𝑒𝑓←𝐸𝑑𝑔𝑒𝐹𝑢𝑠𝑖𝑜𝑛
ℎ𝑚𝑢𝑠𝑜,ℎ𝑚𝑢𝑠𝑖
[10] end
[11]end
[12]Hn←n
ℎ𝑛𝑢𝑒𝑓:𝑢∈V𝑠𝑜
𝐿(G𝑑)=V𝑠𝑖
𝐿(G𝑑)o
[13]𝑃𝐷0←𝑀𝐿𝑃(Hn)
[14]ℎ𝑛agg←Í
𝑣∈N(𝑢𝑠𝑖)ÐN(𝑢𝑠𝑜)W(𝑣)·ℎ𝑛𝑣
[15]H𝑛agg←n
ℎ𝑛aggo
[16]𝑃𝐷1←𝑀𝐿𝑃(H𝑛agg)
 
1564