AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation
Weigang Lu
wglu@stu.xidian.edu.cn
Xidian University
Xi’an, ChinaZiyu Guan∗
ziyuguan@xidian.edu.cn
Xidian University
Xi’an, China
Wei Zhao
ywzhao@mail.xidian.edu.cn
Xidian University
Xi’an, ChinaYaming Yang
yym@xidian.edu.cn
Xidian University
Xi’an, China
Abstract
Graph Neural Networks (GNNs) have revolutionized graph-based
machine learning, but their heavy computational demands pose
challenges for latency-sensitive edge devices in practical indus-
trial applications. In response, a new wave of methods, collectively
known as GNN-to-MLP Knowledge Distillation, has emerged. They
aim to transfer GNN-learned knowledge to a more efficient MLP
student, which offers faster, resource-efficient inference while main-
taining competitive performance compared to GNNs. However,
these methods face significant challenges in situations with insuffi-
cient training data and incomplete test data, limiting their applica-
bility in real-world applications. To address these challenges, we
propose AdaGMLP , an AdaBoosting GNN-to-MLP Knowledge Distil-
lation framework. It leverages an ensemble of diverse MLP students
trained on different subsets of labeled nodes, addressing the issue of
insufficient training data. Additionally, it incorporates a Node Align-
ment technique for robust predictions on test data with missing or
incomplete features. Our experiments on seven benchmark datasets
with different settings demonstrate that AdaGMLP outperforms ex-
isting G2M methods, making it suitable for a wide range of latency-
sensitive real-world applications. We have submitted our code to
the GitHub repository (https://github.com/WeigangLu/AdaGMLP-
KDD24).
CCS Concepts
•Computing methodologies →Machine learning; •Networks
→Network algorithms.
Keywords
Graph Neural Networks, Knowledge Distillation, GNN-to-MLP
Knowledge Distillation
∗Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671699ACM Reference Format:
Weigang Lu, Ziyu Guan, Wei Zhao, and Yaming Yang. 2024. AdaGMLP:
AdaBoosting GNN-to-MLP Knowledge Distillation. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671699
1 Introduction
Graph Neural Networks (GNNs) [ 8,13,14,16,18,25,27,33] have
revolutionized the field of graph-based machine learning, enabling
state-of-the-art performance in various domains, including social
networks [ 17,20], recommendation systems [ 6], and bioinformat-
ics [37]. However, the neighbor-fetching operations in GNNs make
it hard for practical industrial applications, particularly when it
comes to latency constraints in numerous edge devices.
The quest for more efficient alternatives to GNNs has given rise
to a new generation of methods, known as Graph Neural Network
to Multi-Layer Perceptrons (MLPs) Knowledge Distillation (G2M
KD) techniques [ 2,24,28,32]. The primary idea is to transfer the
knowledge learned by a GNN teacher into a MLP student via knowl-
edge distillation [ 10], which is graph-agnostic. G2M methods enable
faster and less resource-intensive inference while maintaining com-
petitive performance compared to GNNs.
Despite their promise, G2M KD methods face two critical chal-
lenges that restrict their real-world applicability: insufficient train-
ing data andincomplete test data. In many real-world scenarios,
acquiring labeled graph data is a costly and time-consuming process
and they often contain nodes with missing or incomplete features,
particularly in the context of test (unseen) data. For example, in
industries like finance and e-commerce, dealing with insufficient or
incomplete data is a daily challenge since many customers refuse
to provide (part of) their information. Ensuring the robustness of
students in the presence of insufficient training data and incomplete
test data is crucial for making informed decisions.
Unfortunately, the above challenges are ignored by existing G2M
methods. In the insufficient training data case, traditional G2M
methods employing a single MLP student can easily memorize the
limited training data rather than learn general patterns from it,
inducing degraded performance on test data. It is a more serious
challenge on G2M than GNNs since GNNs can at least fetch neigh-
bor information to obtain a more general picture of the graph. In
the incomplete test data case, current G2M methods, which are typ-
ically designed for complete data, may struggle to make inference
over the feature-missing data.
 
2060
KDD ’24, August 25–29, 2024, Barcelona, Spain Weigang Lu, Ziyu Guan, Wei Zhao, and Yaming Yang
In response to these challenges, we propose AdaGMLP (AdaBoost-
ing GNN-to-MLP Knowledge Distillation), a novel framework de-
signed to address the limitations of existing G2M methods. It draws
inspiration from ensemble learning [ 4,36] to leverage multiple MLP
students for improved distilled knowledge via our developed Ad-
aBoost Knowledge Distillation. Specifically, for each MLP student,
we introduce a Random Classification and Node Alignment mech-
anism to enhance its generalization capabilities. This framework
allows us to mitigate overfitting in scenarios with limited training
data and ensure robust predictions on test data with missing or
incomplete features. Through comprehensive experiments on seven
benchmark graph datasets, we demonstrate that AdaGMLP surpasses
the performance of state-of-the-art (SOTA) G2M methods across
various scenarios, making it a promising solution for deploying
efficient and adaptable models in real-world applications.
Our main contributions are summarized as follows:
•Tackling Real-world Challenges: We identify two often-
neglected challenges of insufficient training data and incom-
plete test data in current G2M KD methods and present
experimental analysis in Sec. 4. These issues are particularly
pronounced in G2M contexts, presenting a more serious chal-
lenge compared to their impact on GNNs. GNNs inherently
leverage message passing to incorporate neighbor informa-
tion, somewhat mitigating these issues. AdaGMLP introduces
innovative solutions to both issues, which are critically needed
for real-world applications.
•Novel Ensemble Architecture for G2M: To address the
above challenges, we propose AdaGMLP as a novel framework
consisting of Random Classification, Node Alignment, and
AdaBoost Knowledge Distillation techniques. For the first
time within the G2M knowledge distillation domain, our
work pioneers the introduction of an ensemble architecture,
making a significant departure from existing strategies fo-
cused on enhancing G2M through complex modifications
or augmentations. The prior efforts, while valuable, have
not ventured into establishing a generalizable G2M architec-
ture. We have specifically tailored and extended AdaBoost
for G2M, using it as a mechanism to significantly boost the
generalization ability of individual MLP students.
•Comprehensive Empirical Analysis of AdaGMLP :Exten-
sive experiments reveal that AdaGMLP surpasses SOTA G2M
methods in almost all the cases, underscoring its great effec-
tiveness and generalization ability for practical applications.
2 Related Works
In this section, we introduce the works of transferring knowledge
from a larger GNN teacher to a smaller student GNN or MLP. Specif-
ically, we represent them as G2G (GNN-to-GNN) or G2M (GNN-
to-MLP) KD(Knowledge Distillation), respectively.
Graph-to-Graph Knowledge Distillation. Prior researches [ 3,12,
15,21,21,29,39,40] have primarily focused on training compact stu-
dent GNNs from more expansive GNNs using KD techniques [ 1,10].
For example, methodologies like LSP [ 38] and TinyGNN [ 34] facili-
tate the transfer of localized structural insights from teacher GNNs
to student GNNs. RDD [ 42] delves into the reliability aspects of
nodes and edges to enhance the G2G KD. Although the studentmodel used in CPF [ 35] is MLP, it additionally leverages label prop-
agation, which still requires latency-inducing neighbor fetching.
Nevertheless, these approaches still necessitate neighbor fetching,
which can be impractical for applications where latency is a critical
concern.
Graph-to-MLP Knowledge Distillation. In response to latency
concerns, recent advancements propose employing MLP students,
eliminating the need for message passing during inference and
showcasing competitive performance against GNN students. A
pioneer work, GLNN [ 41] introduces a general G2M framework
without propagations. It trains an MLP student guided by both
ground-truth labels and soft labels from the GNN teacher. KRD [ 30]
develops a reliable sampling strategy to train MLPs with confident
knowledge. Additionally, NOSMOG [ 24] combines both structural
and attribute features, which serve as inputs to MLPs, thus es-
tablishing a structure-aware MLP student. Similarly, GSDN [ 31]
introduces topological information into student training stage. Be-
sides, FF-G2M [ 28] explores and provides low- and high-frequency
knowledge from the graph for the student. While traditional G2M
methods have made notable strides in mitigating latency concerns
and enabling efficient knowledge transfer, they still exhibit certain
limitations, particularly when faced with challenges related to lim-
ited training data and feature missing scenarios. We will discuss
both limitations in Sec. 4.
3 Preliminaries
Notions. We denote a graph as G=(V,E), whereVandEare
the node set and edge set, respectively. let 𝑁represent the total
number of nodes. Node features are usually represented by the
matrix X∈R𝑁×𝑑, where each row x𝑖corresponds to the node 𝑖’s
𝑑-dimensional feature vector. The adjacency matrix A∈R𝑁×𝑁
indicates neighbor connections, where A𝑖𝑗=1if there is an edge
(𝑖,𝑗) ∈E , and 0 otherwise. In this paper, we use capital letters
to represent matrices, with corresponding lowercase letters used
to denote specific rows within these matrices. For example, x𝑖
represents the 𝑖-th row vector of X.
Node Classification Problem Statement. The label matrix is
represented by Y∈R𝑁×𝐶consisting of 𝑁one-hot vectors, where
𝐶is the number of classes. We use the superscript 𝐿and𝑈to divide
all the nodes into labeled ( V𝐿,X𝐿, and Y𝐿) and unlabeled parts
(V𝑈,X𝑈, and Y𝑈). The goal of node classification problem is to
predict Y𝑈with A,X, and Y𝐿available.
Graph Neural Networks. Generally, most GNNs follow the message-
passing scheme. That is, The representation h𝑖of each node 𝑖un-
dergoes iterative updates within each layer by gathering messages
from its neighbors, denoted as N(𝑖). In the𝑙-th layer, h(𝑙)
𝑖is com-
puted from the representation of the previous layer through an
aggregation process denoted as AGGR , which is then followed by
anUPDATE operation. This can be formally expressed as:
˜h(𝑙)
𝑖=AGGR(𝑙)({h(𝑙−1)
𝑖:𝑖∈N(𝑖)}) (1)
h(𝑙)
𝑖=UPDATE(𝑙)(˜h(𝑙)
𝑖,h(𝑙−1)
𝑖). (2)
 
2061AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation KDD ’24, August 25–29, 2024, Barcelona, Spain
Graph-to-MLP Knowledge Distillation. [10] first introduces
the concept of KD to enforce a simple student to mimic a more
complex teacher. Notably, [ 41] proposed a G2M KD framework,
wherein GNNs function as teachers and MLPs serve as students.
LetZ𝑔∈R𝑁×𝐶andZ𝑚∈R𝑁×𝐶represent the final outputs (prior
to Softmax) of a GNN and an MLP, respectively. The G2M objective
encompasses both the cross-entropy CE(·,·)between the predic-
tions of the MLP and ground-truth labels:
LCE=1
|V𝐿|∑︁
𝑖∈V𝐿CE(𝜎(z𝑚
𝑖),y𝑖), (3)
as well as the KL-divergence DKL(·,·)calculated between the soft
labels generated by the GNN and MLP:
LKL=1
|V|∑︁
𝑖∈VDKL(𝜎(z𝑔
𝑖/𝜏),𝜎(z𝑚
𝑖/𝜏)), (4)
where𝜎is the Softmax function and 𝜏∈(0,1]is the distillation
temperature hyperparameter. Then, the overall objective LG2M is
defined as follows:
LG2M=𝜆LCE+(1−𝜆)LKL, (5)
where𝜆∈(0,1)is a weighted parameter.
4 Motivation
4.1 Challenges in Existing G2M KD
Recently, G2M KD methods [ 24,30,41] have demonstrated remark-
able results on graph-based tasks, showcasing their superiority
over traditional GNNs and G2G methods. This superiority primar-
ily stems from their minimal inference computational overhead.
However, these current G2M KD methods face significant chal-
lenges, often overlooked but highly relevant in practical scenarios:
1% 2% 3%
Label Rate607080Accuracy(%)Cora
KRD
GLNN
NOSMOG
AdaGMLP (Ours)
1% 2% 3%
Label Rate40506070Citeseer
KRD
GLNN
NOSMOG
AdaGMLP (Ours)
Figure 1: [Challenge 1] Insufficient Training Data. The single-
MLP G2M methods with a single MLP student exhibit higher
sensitivity to changes in label rates compared to vanilla
GNNs. Notably, as the label rate decreases, there is a dis-
cernible trend of increasing box heights and the distance
between outliers and box boundaries.
[Challenge 1] Insufficient Training Data. GNNs inherently pos-
sess strong generalization capabilities, benefiting from their ability
to leverage unlabeled nodes via structural relationships for making
predictions on unseen data. However, transferring GNNs’ knowl-
edge into an MLP becomes problematic when training data is scarce.
The first principle of G2M is “latency comes first. " Therefore, MLP
sacrifices the ability of fetching neighbor information so that it can
be readily applied to latency-sensitive machines. In scenarios withlimited training data, relying solely on a single distilled MLP can
lead to overfitting or getting stuck in local optima, resulting in infer-
ence bias. This concern motivates us to explore the generalization
ability of G2M KD methods, especially in scenarios with limited
data, as shown in Figure 1. We evaluate SOTA G2M KD methods, i.e.,
GLNN [ 41], KRD [ 30], and NOSMOG [ 24] with GCN as the teacher
using Cora andCiteseer datasets under varying label rates. The
variability in accuracy within each method, as demonstrated by the
height of the boxes and the separation between outliers and the
box boundaries, reveals that the present single-MLP G2M meth-
ods exhibit higher sensitivity to changes in label rates. In contrast,
ourAdaGMLP gets benefits from an AdaBoost-style ensemble and
Random Classification strategy (discussed in Sec. 5) to obtain more
stable performance.
10% 20% 30% 40% 50%
Feature Missing Rate6065707580Accuracy (%)
Cora
GCN
GLNN
KRD
NOSMOG
AdaGMLP (Ours)
10% 20% 30% 40% 50%
Feature Missing Rate5055606570
Citeseer
GCN
GLNN
KRD
NOSMOG
AdaGMLP (Ours)
Figure 2: [Challenge 2] Incomplete Test Data. Traditional
G2M methods suffer from performance consistent drops
when more features are missing. Our AdaGMLP consistently
maintains a high accuracy level, outperforming other G2M
methods as the fraction of missing features increases.
[Challenge 2] Incomplete Test Data. Real-world graph data is
frequently incomplete, with missing features in test (new) nodes.
However, traditional G2M KD methods ignore such situations and
are trained under the complete datasets. When faced with feature-
missing test data, they may yield suboptimal results due to the
lack of mechanisms to effectively cope with this inherent incom-
plete features issue. This limitation becomes increasingly critical
when making predictions on real-world graphs with incomplete
information. In Figure 2, we visualize the performance of differ-
ent G2M methods under varying levels of missing features on the
Cora andCiteseer datasets. Unlike GCN that achieves a relatively
stable performance, the performance of traditional G2M methods
gradually decrease as more features are masked since they fail to
teach the MLP student how to handle feature-missing situations.
Instead, AdaGMLP tends to achieve more stable performance than
counterparts due to our Node Alignment module (discussed in
Sec. 5).
4.2 Towards Addressing these Challenges
To address these aforementioned challenges, we propose an Ad-
aBoosting GNN-to-MLP KD ( AdaGMLP ) framework to address these
situations that impede the performance of existing G2M methods.
Regarding Challenge 1, we tackle this by harnessing an AdaBoost-
style [ 9] ensemble [ 4,36] of multiple MLP students trained on
different subsets of labeled nodes. This strategy encourages diver-
sity in learned patterns and mitigates the risk of over-reliance on
 
2062KDD ’24, August 25–29, 2024, Barcelona, Spain Weigang Lu, Ziyu Guan, Wei Zhao, and Yaming Yang
specific subsets during training. Figure 1 shows AdaGMLP ’s great
generalization ability to deal with scarce label resources. To tackle
Challenge 2, we introduce the Node Alignment technique for each
MLP student, aligning representations between labeled nodes with
complete and masked features. This mechanism ensures robust pre-
dictions on test data with missing or incomplete features, thereby
extending its applicability to real-world scenarios. As shown in
Figure 2, AdaGMLP maintains a high and consistent accuracy level,
demonstrating AdaGMLP ’s superiority in handling feature-missing
data and its potential for real-world applications.
5 Methodology
NodeAlignmentMaskMLP t Layer 1……InputsHiddenOutputs(a) Overview of AdaGMLP
(b) Detailed  Training Process in t-th MLPSample…MLP 1PretrainedGNN
MLP KMLP 1OutputsMLP KOutputsGNNOutputs
…Graph…UpdateWeightsLabelsLabelsFeatures…KnowledgeDistillationClassification…
Figure 3: Illustration of AdaGMLP . In (a), for each MLP, we com-
pute the KL loss using node weights, which are determined by
the difference between MLP and corresponding GNN outputs
(Knowledge Distillation). Additionally, we calculate the CE
loss by comparing the sampled labeled nodes with their re-
spective ground-truth labels (Random Classification). In (b),
we begin by obtain incomplete nodes with randomly mask-
ing the features of the selected nodes and inputting them
into the MLP. Subsequently, we employ Mean Squared Error
(MSE) loss to align their hidden representations and outputs
(Node Alignment).
In this section, we introduce AdaGMLP , a methodology designed
to tackle the challenges of G2M distillation while bolstering general-
ization and model capacity. AdaGMLP consists of a pre-trained GNN
as the teacher and a compact student network with 𝐾MLPs with𝐿
layers. Figure 3 illustrates the architecture, showcasing three funda-
mental components: Random Classification (RC), Node Alignment
(NA), and AdaBoosting Knowledge Distillation (AdaKD).5.1 Random Classification
We denote each MLP student as MS 1, MS 2, ..., MS𝐾. Their respective
outputs are represented as Z𝑚1,Z𝑚2, ...,Z𝑚𝐾∈R𝑁×𝐶. To enhance
the student network’s generalizability, we introduce randomness
into the inputs for MS 1, MS 2, ..., MS𝐾−1by selecting⌊|V𝐿|/𝐾⌋nodes
randomly fromV𝐿without replacement. The remaining nodes are
used as the input for MS 𝐾, where⌊·⌋represents the floor function.
Assume the labeled node subset of MS 𝑘isV𝐿
𝑘, the classification
objectiveL(𝑘)
CEfor MS𝑘can be written as:
L(𝑘)
CE=1
|V𝐿
𝑘|∑︁
𝑖∈V𝐿
𝑘CE(𝜎(z𝑚𝑘
𝑖),y𝑖). (6)
By training different MLP students on different subsets of labeled
nodes, this encourages the student network to capture various
patterns present in the dataset and avoids over-reliance on a specific
subset of labeled nodes, leading to improved and stable performance.
The Random Classification objective, LRC, is presented as:
LRC=1
𝐾𝐾∑︁
𝑘=1L(𝑘)
CE(7)
5.2 Node Alignment
The primary idea behind Node Alignment is to align the represen-
tations of nodes with complete features (labeled nodes) and those
with masked features (masked nodes) since we often encounter
datasets where labeled nodes have complete feature information,
while unlabeled nodes have missing features. To illustrate this, let
x𝑖∈R𝑑represent a complete node, and ˜x𝑖signify a corrupted node
with a fraction 𝜌of its features randomly masked, where 𝜌∈(0,1).
Consequently, we obtain outputs z𝑚𝑘
𝑖and˜z𝑚𝑘
𝑖as well as hidden
representations h𝑚𝑘,𝑙
𝑖andh𝑚𝑘,𝑙
𝑖, where𝑙∈{1,2,···,𝐿−1}. Now,
let’s delve into the two critical aspects of Node Alignment.
Output Alignment (NA-O). In the NA-O phase, our objective is to
ensure that AdaGMLP ’s predictions on labeled nodes with completed
and masked features are consistent. By minimizing the squared L2
norm between the predictions for complete and masked features,
as expressed by the loss LNA-O which is expressed as:
LNA-O =1
𝐾𝐾∑︁
𝑘=1L(𝑘)
NA-O=𝐾∑︁
𝑘=1Í
𝑖∈V𝐿
𝑘∥z𝑚𝑘
𝑖−˜z𝑚𝑘
𝑖∥2
𝐾|V𝐿
𝑘|. (8)
NA-O encourages the model to produce similar predictions for both
labeled and masked nodes. This consistency contributes to stable
model behavior and facilitates robust predictions.
Hidden Representation Alignment (NA-H). In the NA-H phase,
we focus on aligning the hidden representations of nodes at different
layers of the model. Similar to NA-O, we minimize the squared L2
norm between the hidden representations for complete and masked
features for each layer:
LNA-H =1
𝐾𝐾∑︁
𝑘=1L(𝑘)
NA-H=Í𝐿−1
𝑙=1Í
𝑖∈V𝐿
𝑘∥h𝑚𝑘,𝑙
𝑖−˜h𝑚𝑘,𝑙
𝑖∥2
𝐾|V𝐿
𝑘|(𝐿−1). (9)
This consistency ensures that the model maintains a coherent un-
derstanding of nodes with varying feature completeness across
different layers.
 
2063AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation KDD ’24, August 25–29, 2024, Barcelona, Spain
By incorporating both NA-O and NA-H with a controlling pa-
rameter𝜆NA∈(0,1)to optimize the following objective LNA:
LNA=𝜆NALNA-O+(1−𝜆NA)LNA-H, (10)
AdaGMLP achieves the dual goals of producing consistent predic-
tions and maintaining coherent representations across nodes with
varying feature completeness. This results in a more robust, gener-
alizable, and stable model.
5.3 AdaBoosting Knowledge Distillation
We leverage AdaBoosting to obtain the collective power of multiple
MLP students, further enhancing MLP students’s generalization
and performance. To achieve this, we adapt SAMME (Stagewise
Additive Modeling using a Multi-class Exponential loss function)
algorithm [ 9], which is an extension of the standard two-class
AdaBoost, to propose the KD-SAMME algorithm for combining
MLP students in the context of G2M.
In KD-SAMME, we compute weighted error 𝑒(𝑘), relying on
KL-divergence for quantifying knowledge point (node) dissimilar-
ity. The divergence between each node pair is denoted as 𝑑(𝑘)
𝑖=
DKL(𝜎(z𝑔
𝑖),𝜎(z𝑚𝑘
𝑖)). The error𝑒(𝑘)is determined as:
𝑒(𝑘)=Í𝑁
𝑖=1𝑤𝑖
1−exp(−𝛽𝑑(𝑘)
𝑖)
Í𝑁
𝑖=1𝑤𝑖, (11)
where𝑤denotes the weight of 𝑖-th node and 𝛽>0controls the
sensitivity to divergence between knowledge point pairs. This di-
vergence captures the dissimilarity between individual knowledge
points extracted from both the teacher and student models.
Subsequently, we leverage this error information to compute a
corresponding combining weight 𝛼(𝑘)for each MLP student as:
𝛼(𝑘)=max{log1−𝑒(𝑘)
𝑒(𝑘),𝜖}, (12)
where𝜖is an extremely small. value Further, node weights 𝑤𝑖are
updated by adjusting them based on 𝛼(𝑘)and𝑒(𝑘):
𝑤𝑖←𝑤𝑖·exp
𝛼(𝑘)
1−𝑒−𝛽𝑑(𝑘)
𝑖
,𝑖=1,···,𝑁 (13)
Then, node weights 𝑤𝑖are normalized. The KD objective for
MS𝑘can be written as:
L(𝑘)
KL=∑︁
𝑖∈V𝑤𝑖DKL(𝜎(z𝑔
𝑖/𝜏),𝜎(z𝑚𝑘
𝑖/𝜏)). (14)
In summary, we obtain the AdaBoosting KD objective LAdaKD :
LAdaKD =1
𝐾𝐾∑︁
𝑘=1L(𝑘)
KL, (15)
5.4 Training and Inference
Training. We define the overall AdaGMLP objectiveLAdaGMLP as:
LAdaGMLP =𝜆LRC+(1−𝜆)LAdaKD+L NA, (16)
where𝜆∈(0,1)is a parameter to control the weight between RC
and AdaKD. We also present the algorithm of AdaGMLP in Algo-
rithm 1.Algorithm 1 AdaGMLP Algorithm (Transductive)
1:Input: GNN teacher’s output Z𝑚, hyperparameters 𝜏,𝛽,𝜆, and
𝜆NA
2:Initialize Node weights 𝑤𝑖=1
𝑁for all𝑖∈ V , combining
weights𝛼(𝑘)=1for𝑘=1,2,...,𝐾
3:for𝑡=1to𝑇do
4: // Student MLP training
5:for𝑘=1to𝐾do
6: // Random Classification
7: Sample labeled nodes to obtain a subset V𝐿
𝑘
8: Train MS𝑘withV𝐿
𝑘via RC objective Eq. (6)
9: // Node Alignment
10: Randomly mask features of nodes in V𝐿
𝑘
11: Train MS𝑘using nodes with completed and partially
masked features via NA objective Eq. (10)
12: // AdaBoost Knowledge Distillation
13: Train MS𝑘with Z𝑚and𝑤(𝑘)via AdaKD objective Eq. (14)
14: Calculate the error 𝑒(𝑘)via Eq. (11)
15: Calculate the combining weight 𝛼(𝑘)via Eq. (12)
16: Update node weights 𝑤𝑖via Eq. (13)
17: end for
18: Normalize node weights 𝑤𝑖←𝑤𝑖Í𝑁
𝑖=1𝑤𝑖
19:end for
20:Obtain final prediction 𝑝𝑖for node𝑖via Eq. (17)
Inference. After the training process, we obtain a student network
comprising 𝐾MLPs, each associated with corresponding weights
𝛼(1),𝛼(2),···,𝛼(𝐾). We aggregate predictions from these distinct
MLPs in an AdaBoost-like manner to generate the final predicted
label ˆy𝑖for the𝑖-th node:
ˆy𝑖=arg max
𝑐𝐾∑︁
𝑘=1¯𝛼(𝑘)𝜎(z𝑚𝑘
𝑖) (17)
Here, arg max
𝑐denotes the selection of the class with the highest
value among all classes and ¯𝛼(𝑘)is the normalized version of 𝛼(𝑘).
Larger𝛽values emphasize these under-distilled instances more,
effectively making them “stronger" in knowledge transferring.
5.5 Complexity
AdaGMLP ’s computational complexity primarily derives from the
multiple MLPs in the ensemble and the operations involved in Node
Alignment and AdaBoosting techniques. Assuming each MLP in the
ensemble comprises two layers, including a transformation from 𝑚-
dimensional input features to 𝑑-dimensional hidden representations
and a projection from these hidden dimensions to 𝑐-dimensional
outputs, the computational complexity for each MLP is 𝑂(𝑚𝑑+𝑑𝑐).
For𝐾MLPs, the combined complexity for Node Alignment amounts
to𝑂(2𝐾𝑑(𝑚+𝑐)). Furthermore, the AdaBoosting process, which
updates weights and combines predictions across MLPs, contributes
additional complexity. This aspect of the process is proportional to
the number of nodes and the ensemble size, represented as 𝑂(𝑛𝐾),
where𝑛is the number of nodes. Therefore, the time complexity of
 
2064KDD ’24, August 25–29, 2024, Barcelona, Spain Weigang Lu, Ziyu Guan, Wei Zhao, and Yaming Yang
Table 1: Classification accuracy ±std ( %) in the Transductive Setting and Inductive Setting.
T
eacher Student Cora
Citeseer Pubmed Photo CS Physics ogbn-arxiv ¯Δ𝐺
𝐿𝑁𝑁
Transductive
Setting
MLPs
- 56.66±2.02 59.88±0.59 71.94±1.24 78.16±2.76 87.17±1.04 87.24±0.61 53.60±1.31 -
GCN- 82.02±0.98 71.88±0.34 77.24±0.23 90.60±2.15 89.73±0.67 92.29±0.58 71.22±0.18 -
GLNN 82.08±1.14 73.46±0.47 80.40±0.59 91.44±2.23 92.39±0.53 93.16±0.63 67.23±0.68 -
NOSMOG 82.65±1.31 73.47±1.49 80.95±2.21 92.39±1.95 93.71±0.63 93.49±0.42 71.07±0.24↑1.41%
KRD 82.42±1.19 74.24±0.75 81.44±0.58 91.76±2.46 93.77±0.23 94.13±0.39 70.12±0.37↑1.42%
A
daGMLP (ours) 84.26±0.8375.42±0.3981.88±0.5392.60±0.3793.79±0.3394.38±0.27 71.45±0.10↑2.51%
GraphSA
GE- 82.04±1.33 70.66±0.31 78.30±0.58 90.24±2.13 89.28±0.34 91.99±1.03 70.91±0.26 -
GLNN 82.24±1.11 71.90±0.76 79.78±1.46 91.44±2.23 92.86±0.28 93.28±0.94 68.63±0.12 -
NOSMOG 82.74±1.53 71.95±1.39 80.70±1.31 92.09±1.62 93.04±0.93 93.92±1.29 70.57±0.41↑0.89%
KRD 83.50±0.96 72.62±0.59 81.08±0.53 91.57±2.67 93.99±0.17 94.03±0.77 71.20±0.52↑1.44%
A
daGMLP (ours) 84.10±0.4673.26±0.2981.18±0.6092.55±2.3194.06±0.2194.17±0.57 71.46±0.53↑1.93%
GA
T- 80.24±1.34 71.24±0.73 77.20±0.68 86.98±5.76 90.93±0.23 92.39±0.80 71.10±0.10 -
GLNN 81.06±1.70 69.42±3.37 80.78±0.37 86.64±9.86 93.34±0.12 93.63±0.77 68.40±0.16 -
NOSMOG 81.30±1.24 70.52±1.47 80.42±2.25 92.92±1.13 94.20±0.17 93.98±0.52 71.47±0.18↑2.07%
KRD 82.58±1.31 69.00±3.38 81.13±0.58 89.06±2.46 94.12±0.15 94.23±0.30 71.46±0.14↑1.49%
A
daGMLP (ours) 83.78±0.7272.30±1.0181.68±0.5993.00±1.6294.35±0.1694.33±0.28 71.70±0.33↑3.23%
Inductive
Setting
MLPs
- 60.20±0.44 60.00±0.30 72.80±0.71 77.20±3.19 88.97±1.12 90.16±0.42 56.39±0.56 -
GCN- 79.20±0.4671.88±0.36 77.36±0.71 88.67±1.22 89.55±0.48 92.47±0.45 70.80±0.48 -
GLNN 72.80±0.21 70.34±0.60 78.22±0.55 88.53±2.84 91.72±0.73 93.17±0.70 61.03±0.25 -
NOSMOG 74.55±1.74 70.94±0.49 80.83±2.49 88.93±1.93 92.93±1.9393.97±0.78 68.60±0.24↑3.09%
KRD 73.52±0.21 70.36±0.65 80.72±1.26 88.16±2.02 92.09±0.61 93.79±0.48 60.41±0.26↑0.55%
A
daGMLP (ours) 75.02±0.44 70.84±0.2881.10±0.1591.15±1.1193.28±0.28 93.96±0.51 64.30±0.21↑2.62%
GraphSA
GE- 80.32±0.16 70.44±0.42 77.40±0.32 89.40±1.66 88.94±0.54 91.89±1.67 70.86±0.40 -
GLNN 70.56±1.54 70.16±1.00 79.44±1.06 88.55±2.69 91.19±0.35 92.89±1.26 61.08±0.38 -
NOSMOG 71.27±2.58 70.38±1.41 80.91±2.79 89.37±1.90 91.32±1.90 93.16±1.08 68.48±0.20↑2.38%
KRD 70.90±1.38 70.26±0.47 80.08±0.44 89.32±1.47 92.67±0.47 93.55±1.12 61.05±0.18↑0.65%
A
daGMLP (ours) 74.78±0.3070.47±0.1381.34±0.2491.77±0.4393.99±0.4693.98±0.12 65.16±0.26↑2.70%
GA
T- 80.24±0.90 69.72±0.61 77.00±0.68 89.97±1.86 90.22±0.94 89.95±2.29 70.52±0.47 -
GLNN 71.66±1.20 69.38±1.21 79.24±1.83 89.55±1.62 91.07±1.30 92.09±1.92 60.91±0.45 -
NOSMOG 72.68±2.23 70.50±2.46 81.43±3.38 89.31±1.14 91.31±1.24 93.34±1.98 68.72±0.49↑2.85%
KRD 71.44±1.31 69.26±1.53 80.52±1.36 89.49±2.85 91.68±0.36 92.83±1.38 60.95±0.60↑0.37%
A
daGMLP (ours) 73.92±0.6871.72±0.9481.86±0.3291.44±1.1891.78±0.7593.98±0.36 63.82±0.32↑2.79%
our AdaGML is 𝑂(2𝐾𝑑(𝑚+𝑐)+𝑛𝐾)for training and 𝑂(𝐾𝑑(𝑚+
𝑐)+𝑛𝐾)for inference.
In most cases, the hidden dimensionality 𝑑often exceeds 𝐾,
allowing AdaGMLP to utilize relatively lighter MLPs with smaller 𝑑
while still maintaining high performance. This approach not only
enhances computational efficiency but also ensures that the model
remains robust and effective across various learning scenarios.
6 Experiments
In this section, we conduct a series of experiments to evaluate the
performance of AdaGMLP on real-world graph datasets, addressing
the following questions:Q1: How does AdaGMLP perform in diverse settings (both trans-
ductive and inductive), across various real-world graphs, and
with different GNN teachers (including GCN, GraphSAGE,
and GAT)?
Q2: How does AdaGMLP compare to SOTA G2M KD methods
when confronted with insufficient training data?
Q3: How effective is AdaGMLP in handling incomplete test data
compared to SOTA G2M KD methods?
Q4: IsAdaGMLP sensitive to the choice of hyper-parameters, i.e.,
𝜆,𝜆NA,𝛽?
Q5: How does the size of the ensemble ( 𝐾) impact on perfor-
mance?
 
2065AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation KDD ’24, August 25–29, 2024, Barcelona, Spain
Q6: To what extent do the individual components of AdaGMLP con-
tribute to its overall performance?
Q7: Can AdaGMLP fulfill the requirements of real-world applica-
tions?
6.1 Experiment Setting
Dataset. Similar to [ 30], we use six public benchmark graphs, i.e.,
Cora [ 22],Citeseer [7],Pubmed [19],Coauthor-CS ,Coauthor-
Physics ,Amazon-Photo [23], and a large-scale graph ogbn-arxiv [11].
The statistics of datasets are provided in Appendix B.
Baselines. There are three types of baselines in this paper: (1) GNN
Teachers including GCN [ 13], GraphSAGE [ 8], and GAT [ 25];(2)
SOTA G2M Methods containing GLNN [ 41], NOSMOG [ 24], and
KRD [ 30];(3) SOTA G2G Methods including CPF [ 35], RDD [ 42],
TinyGNN [ 34], GNN-SD [ 3], and LSP [ 38]. The comparison between
AdaGMLP and G2G methods is described in Appendix C.
Implementation. The code of AdaGMLP is built on [ 30] via DGL
library [ 26] and we implement each MLP student with the same
configuration (hidden dimensionality, number of layers) as its GNN
teachers. We tune 𝐾∈{2,3,4}for all the experiments except for the
hyper-parameter analysis. Due to the space limitation, we present
the search spaces of other hyper-parameters in the Appendix A.
6.2 Classification Performance Comparison
(Q1)
We evaluate AdaGMLP in both transductive and inductive settings, as
shown in Table 1. For all the comparing G2M methods, we evaluate
the models with their released parameters. The best metrics are
marked by bold.
In the transductive setting, AdaGMLP demonstrates superior clas-
sification accuracy compared to other G2M methods and even ex-
ceeds its GNN teachers on various datasets. In the inductive set-
ting, AdaGMLP competes well with the SOTA methods. The average
improvement over GLNN ( ¯Δ𝐺𝐿𝑁𝑁 ), which is the representative
method, varies across datasets.
It’s worth noting that AdaGMLP doesn’t consistently outperform
NOSMOG in some cases as it dose in the transductive setting. It is
because NOSMOG benefits from access to test (unseen) node struc-
tural information, which is not typically available in real-world
scenarios. Considering this, the strong performance of NOSMOG
in the inductive setting should be interpreted with caution. It may
not be the best choice for real-world scenarios where structural
information about test nodes is unknown. In contrast, AdaGMLP per-
forms competitively in the inductive setting without relying on
any information about unseen nodes. This highlights its practical
applicability and versatility, as it can handle scenarios where the
test node’s structure is not available, making it a more robust choice
for real-world applications.
6.3 Insufficient Training Data Case (Q2)
We conducted experiments with varying label rates (1%, 2%, and
3%) on the Cora andCiteseer datasets in Table 2. The goal was
to assess how well AdaGMLP could perform compared to GCN and
other G2M methods in scenarios with limited labeled data.Table 2: Classification accuracy ±std (%) in the Insufficient
Training Data Setting with various label rates.
DatasetLab
el GCN Student
Rate
Teacher GLNN KRD NOSMOG AdaGMLP
Cora1% 67.90±4.24 63.24±5.94 68.40±4.18 68.37±7.2571.20±4.22
2% 76.81±2.15 72.48±4.68 76.18±3.08 75.84±4.4577.92±1.22
3% 79.83±1.01 74.31±4.46 79.26±1.50 78.18±2.4980.38±1.05
Citeseer1% 64.14±1.72 62.74±4.38 55.02±9.64 63.16±5.2863.84±2.13
2% 67.10±1.34 65.54±6.39 59.34±14.54 66.42±4.1666.46±2.61
3% 69.06±1.82 69.78±3.71 67.30±3.69 69.35±4.1369.96±1.94
Traditional G2M methods struggle to match the performance of
the GCN teacher in the low-label-rate settings. This is primarily
because these single-student methods might be easily over-fit to
limited labeled data. As a result, they tend to show higher standard
deviations compared to GCN teacher and our AdaGMLP.
AdaGMLP demonstrates superior adaptability and performance in
this setting. Its ability to capture and utilize information efficiently
from limited labeled nodes allows it to outperform traditional G2M
methods and even the GCN teacher in some cases. Additionally,
AdaGMLP ’s robustness (smaller standard deviation) across different
label rates demonstrates its potential in real-world applications
where obtaining a large amount of labeled data is challenging or
expensive.
6.4 Incomplete Testing Data Case (Q3)
In the feature-missing setting, we conducted extensive experiments
on the Cora ,Citeseer , and Pubmed datasets to evaluate the per-
formance of AdaGMLP and compare it with GCN, and three G2M
methods. We examine the impact of missing rates (10%, 20%, 30%,
40%, and 50%) on classification accuracy. The overall results are
provided in Table 3. As the missing rate increases, traditional G2M
Table 3: Classification accuracy ±std (%) in the
Feature-missing Setting with test node features ran-
domly masked according to the missing rate.
DatasetMissing
GCN Student
Rate
Teacher GLNN KRD NOSMOG AdaGMLP
Cora10% 81.58±1.42 68.84±4.71 73.47±1.43 68.01±3.9782.42±0.72
20% 81.20±1.45 67.32±2.20 73.41±1.26 67.31±4.3481.32±1.49
30% 79.94±1.99 65.70±3.26 72.80±1.93 67.10±3.6680.46±1.32
40% 78.96±2.59 63.76±3.55 70.72±1.95 65.00±7.4579.78±1.73
50% 78.02±1.73 59.34±4.07 70.42±1.83 64.82±7.1478.54±1.38
Citeseer10% 71.92±0.84 60.06±4.04 68.62±1.42 66.52±4.3073.14±0.57
20% 70.32±0.86 59.70±3.74 67.01±1.96 66.46±4.3372.74±1.25
30% 69.56±1.91 59.12±4.13 65.80±1.83 66.24±4.7371.16±1.73
40% 69.38±1.69 58.24±3.93 65.72±0.95 65.79±4.8569.42±1.24
50% 68.64±1.85 57.02±4.10 63.42±1.94 60.40±3.67 64.36±1.45
Pubmed10% 77.22±0.52 67.64±3.60 77.94±0.73 74.19±3.4180.26±0.28
20% 76.86±0.38 67.24±3.48 77.44±0.89 73.14±3.9379.37±0.43
30% 76.34±0.35 65.34±2.57 76.60±1.02 73.09±2.2378.14±1.16
40% 76.14±0.82 64.66±4.06 75.42±1.14 72.92±3.3877.42±0.56
50% 76.05±0.98 60.01±4.55 74.31±1.47 72.43±4.2976.48±1.25
methods suffer from a significant drop in accuracy. This indicates
their vulnerability to missing data, limiting their practicality in
 
2066KDD ’24, August 25–29, 2024, Barcelona, Spain Weigang Lu, Ziyu Guan, Wei Zhao, and Yaming Yang
real-world scenarios where data completeness cannot be guaran-
teed. It is mainly due to three reasons: (1) Lack of Mechanisms
for Feature-missing Data: existing G2M methods are typically
trained on complete datasets where all features are available with
the lack of mechanisms to effectively cope with missing features;
(2) Limited Feature Information: these G2M methods, relying
on fixed feature vectors for prediction, cannot generalize well in
the feature-missing test data.
AdaGMLP consistently outperforms other G2M methods across
all missing rates and even exhibits better performance over GCN in
almost all the cases. It can be attributed to the Node Alignment mod-
ule that teaches each MLP student to align feature-missing nodes
and complete nodes. Additionally, the AdaBoost-style ensemble
approach encourages each student to collectively compensate for
the missing information by aggregating diverse knowledge from
different subsets, resulting in more robust predictions.This robust-
ness demonstrates AdaGMLP ’s ability to handle real-world scenarios
with incomplete data effectively.
6.5 Hyper-parameters Analysis (Q4)
In this section, we provide comprehensive analysis on Cora dataset
to probe into three hyper-parameters in AdaGMLP , i.e., balance
weight𝜆ofLRCandLAdaKD , balance weight 𝜆NAof NA-H and
NA-O, and sensitivity weight 𝛽of knowledge point pairs divergence.
To obtain more focused analysis, we remove the Node Alignment
module when the interested hyper-parameter is not 𝜆NA.
0.0 0.5 1.060.065.070.075.080.085.0Accuracy (%)
GCN
SAGE
GAT
(a)𝜆
0.2 0.5 0.876.078.080.082.0Accuracy (%)
10% Miss
30% Miss
50% Miss
70% Miss (b)𝜆NA
2.0 4.080.081.082.083.084.0Accuracy (%)
GCN
SAGE
GAT (c)𝛽
Figure 4: Hyper-parameter Analysis on 𝜆,𝜆NA, and𝛽.
In Figure 4(a), we tune 𝜆from 0 to 1 with interval of 0.1 using
various GNN teachers. We can observe a noticeable and consistent
drop in performance across all teacher models when 𝜆exceeds a
certain large threshold, e.g., 0.9. This phenomenon can be explained
by considering the role of 𝜆in balancing classification loss LRCand
knowledge distillation loss LAdaKD . When𝜆is set to be very large,
the model places an overwhelming emphasis on minimizing the
classification error during training. This may lead to overfitting on
the training data and the teacher’s knowledge not being effectively
distilled into the student. More interestingly, AdaGMLP maintains
high performance at 𝜆=0(complete knowledge distillation). It can
be attributed to our AdaBoost Knowledge Distillation, which allows
students to effectively transfer valuable knowledge from GNNs.
In Figure 4(b), we observe a notable phenomenon: in high feature
missing settings (e.g., 70% missing rate), smaller values of 𝜆NAlead
to better results, while in low feature missing scenarios (e.g., 10%
missing rate), larger values of 𝜆NAare more effective. With a higherfeature missing rate, retaining information through NA-H becomes
crucial since the limited available features in test nodes can be
hardly classified. Smaller 𝜆NAvalues emphasize NA-H and allow
the model to focus more on preserving hidden representations,
which are essential in recovering information from incomplete
features, thereby obtaining higher performance. Conversely, with
a substantial portion of features available, there is ample feature
information available for most nodes. Consequently, the model
can exploit this rich data to generate meaningful outputs. A larger
𝜆NAvalue allocates more importance to the alignment of nodes
based on their output representations, which acts like consistency
regularization over label information [ 13,43], to obtain more robust
predictions.
In Figure 4(c), we explore the sensitivity of different teacher
models to varying values of 𝛽from 0.5 to 4 with interval of 0.5.
The parameter 𝛽plays a significant role in AdaBoost Knowledge
Distillation, as it controls the importance of individual instances
in the ensemble. Larger 𝛽values make student more sensitive to
under-distilled node pairs. The analysis suggest that we should
avoid using extremely small 𝛽.
6.6 Ensemble Size Analysis (Q5)
GCN SAGE GAT808182838485Accuracy (%)Dataset = Cora
K = 2
K = 3
K = 4
K = 5
GCN SAGE GAT70727476Dataset = Citeseer
K = 2
K = 3
K = 4
K = 5
GCN SAGE GAT80.080.581.081.582.0Dataset = Pubmed
K = 2
K = 3
K = 4
K = 5
Figure 5: Ensemble Size ( 𝐾) Analysis.
In this ensemble size ( 𝐾) ablation experiment conducted using
AdaGMLP across various datasets and teacher models, we aim to
explore the sensitivity of 𝐾to model performance. The results
reveal following noteworthy insights.
Across all datasets and teacher models, we observe that as 𝐾
increases, the classification accuracy generally improves. This sug-
gests that increasing the ensemble size contributes positively to
the model’s performance. However, it’s essential to note that the
improvement tends to saturate as 𝐾becomes larger. It indicates
that there is an optimal point beyond which further increasing 𝐾
may not significantly benefit the model’s performance. The sensi-
tivity of𝐾to model performance suggests that AdaGMLP can benefit
from larger 𝐾. Researchers can tailor the ensemble size based on
their available computational resources and the dataset at hand.
Smaller𝐾may suffice for some cases, while others may require
larger ensembles to maximize accuracy.
6.7 Ablation Study (Q6)
In this ablation experiment, we investigate the impact of different
modules within AdaGMLP under two different settings, including
insufficient training data and incomplete test data. We use GCN
as the teacher model and set 𝜆=0.5,𝜆NA=0.5,𝐾=2,𝛽=3as
the default setting. Different modules (RC, AdaKD, NA-O, NA-H,
NA) within AdaGMLP are systematically disabled to analyze their
 
2067AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation KDD ’24, August 25–29, 2024, Barcelona, Spain
1% 2% 3%
Label Rate (%)607080Accuracy (%)Dataset = Cora
w/o RC
w/o AdaKD
w/o NA-Ow/o NA-H
w/o NA
AdaGMLP
1% 2% 3%
Label Rate (%)5560657075Dataset = Citeseer
w/o RC
w/o AdaKD
w/o NA-Ow/o NA-H
w/o NA
AdaGMLP
(a) Insufficient Training Data Setting
1% 2% 3%
Features Missing Rate (%)7075808590Accuracy (%)Dataset = Cora
w/o RC
w/o AdaKD
w/o NA-Ow/o NA-H
w/o NA
AdaGMLP
1% 2% 3%
Features Missing Rate (%)5560657075Dataset = Citeseer
w/o RC
w/o AdaKD
w/o NA-Ow/o NA-H
w/o NA
AdaGMLP
(b) Incomplete Test Data Setting
Figure 6: Ablation Study.
individual contributions. The results (shown in Figure 6) provide
insights into the role of each module.
Random Classification (RC). Removing the RC module, which
involves randomly sampling training data for each student, leads
to a significant drop in accuracy across all label rates and datasets.
The decline is more obvious in insufficient training data setting,
as shown in Figure 6(a). This is because, in the absence of RC,
students are trained on a fixed subset of data, potentially leading
to overfitting. The randomness introduced by RC helps mitigate
overfitting and ensures that students see diverse examples during
training. It is essential for improving generalization and robustness
in the presence of insufficient training data.
AdaBoosting Knowledge Distillation (AdaKD). Eliminating
AdaKD results in a noticeable performance decrease in all the cases.
AdaKD contributes to improving the student’s knowledge by boost-
ing its ability of knowledge transferring. Its role is vital for main-
taining high accuracy. Moreover, it has a significant impact on
performance with insufficient training data. This is because when
there is limited supervision, AdaKD can help student learn from
the teacher’s soft labels and provide additional supervision.
Node Alignment (NA). The NA module, formed by integrating
NA-H and NA-O, is effective in maintaining model performance,
especially under the incomplete test data setting, as shown in Fig-
ure 6(b). Removing both NA-H and NA-O leads to more pronounced
performance drops, highlighting the value of their synergy within
the NA module. These modules enable students to recover represen-
tations from the corrupted nodes, which is vital when dealing with
incomplete test data. Without this alignment, students struggle to
make predictions on unseen or partially observed nodes.
In summary, these modules serve complementary roles, and
their removal impacts performance differently based on the specific
challenges posed by insufficient training data or incomplete test
data.6.8 Efficiency Analysis (Q7)
100 101
Log Scale Inference Time (ms)78808284Accuracy (%)
GLNN32GLNN128GLNN1024
KRD32KRD128
KRD1024
NOSMOG32NOSMOG
128NOSMOG1024
AdaGMLP32AdaGMLP128AdaGMLP
1024
GLNN
NOSMOG
KRD
AdaGMLP
Figure 7: Accuracy vs.Inference Time (ms).
In Figure 7, we study the trade-off between performance and
efficiency (inference time cost) of AdaGMLP and SOTA G2M meth-
ods on Pubmed dataset. For fair comparison, we use 3-layer GCN
with 1024 hidden units as the teacher model and tune the hidden
units in{32,128,1024}for all the student model(s). We fix 𝐾at 3
forAdaGMLP.
Despite a slight increase in inference time compared to other
methods, AdaGMLP offers significantly better accuracy. This trade-
off is often acceptable in real-world applications, where predictive
performance is paramount. Besides, AdaGMLP achieves impressive
accuracy (83.34%) even with a relatively low hidden dimension
of 128. Therefore, the increase in inference time cost due to the
use of multiple MLPs is counterbalanced because of the compact
student model design. The MLPs in AdaGMLP are designed to be
compact, with fewer parameters compared to the potentially other
MLP students which demand more parameters to maintain the
expressive ability. This design choice significantly reduces the com-
putational load for each MLP. Another essential practical advantage
ofAdaGMLP is its inherent parallelizability. AdaGMLP ’s architecture
allows for efficient parallel computation across multiple student
models. This feature can significantly reduce inference time in
scenarios where parallel processing is feasible.
7 Conclusion
In this work, we introduce AdaGMLP , a novel ensemble framework
for GNN-to-MLP Knowledge Distillation. Furthermore, we high-
light the significant impact of limited training data and insufficient
test data in G2M contexts, which pose even greater challenges.
Through an extensive series of experiments, we shed light on
AdaGMLP ’s strengths by evaluating it on various scenarios, demon-
strating its great potential for real-world applications.
Acknowledgments
This work was supported in part by the National Natural Science
Foundation of China under Grants 62133012, 61936006, 62073255,
and 62303366, in part by the Innovation Capability Support Program
of Shaanxi under Grant 2021TD-05, in part by the Fundamental Re-
search Funds for the Central Universities under Grants QTZX23039,
XJSJ23030, and in part by the Innovation Fund of Xidian University
under Grant YJSJ24015.
 
2068KDD ’24, August 25–29, 2024, Barcelona, Spain Weigang Lu, Ziyu Guan, Wei Zhao, and Yaming Yang
References
[1]Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep? Advances
in neural information processing systems 27 (2014).
[2]Jie Chen, Shouzhen Chen, Mingyuan Bai, Junbin Gao, Junping Zhang, and Jian
Pu. 2022. SA-MLP: Distilling Graph Knowledge from GNNs into Structure-Aware
MLP. arXiv preprint arXiv:2210.09609 (2022).
[3]Yuzhao Chen, Yatao Bian, Xi Xiao, Yu Rong, Tingyang Xu, and Junzhou Huang.
2020. On self-distilling graph neural network. arXiv preprint arXiv:2011.02255
(2020).
[4] Thomas G Dietterich. 2000. Ensemble methods in machine learning. In Interna-
tional workshop on multiple classifier systems. Springer, 1–15.
[5] Thomas G Dietterich. 2000. Ensemble methods in machine learning. In Interna-
tional workshop on multiple classifier systems. Springer, 1–15.
[6]Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In The world wide web
conference. 417–426.
[7]C Lee Giles, Kurt D Bollacker, and Steve Lawrence. 1998. CiteSeer: An automatic
citation indexing system. In Proceedings of the third ACM conference on Digital
libraries. 89–98.
[8]Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In Advances in neural information processing systems.
1024–1034.
[9]Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou. 2009. Multi-class adaboost.
Statistics and its Interface 2, 3 (2009), 349–360.
[10] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in
a neural network. arXiv preprint arXiv:1503.02531 (2015).
[11] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen
Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets
for machine learning on graphs. arXiv preprint arXiv:2005.00687 (2020).
[12] Chaitanya K Joshi, Fayao Liu, Xu Xun, Jie Lin, and Chuan Sheng Foo. 2022. On rep-
resentation knowledge distillation for graph neural networks. IEEE Transactions
on Neural Networks and Learning Systems (2022).
[13] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[14] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. 2018. Pre-
dict then propagate: Graph neural networks meet personalized pagerank. arXiv
preprint arXiv:1810.05997 (2018).
[15] Carlos Lassance, Myriam Bontonou, Ghouthi Boukli Hacene, Vincent Gripon,
Jian Tang, and Antonio Ortega. 2020. Deep geometric knowledge distillation with
graphs. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP). IEEE, 8484–8488.
[16] Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang, and Long Jin. 2024. NodeMixup:
Tackling Under-Reaching for Graph Neural Networks. Proceedings of the AAAI
Conference on Artificial Intelligence 38, 13 (Mar. 2024), 14175–14183. https:
//doi.org/10.1609/aaai.v38i13.29328
[17] Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang, Yuanhai Lv, Lining Xing,
Baosheng Yu, and Dacheng Tao. 2023. Pseudo contrastive learning for graph-
based semi-supervised learning. arXiv preprint arXiv:2302.09532 (2023).
[18] Weigang Lu, Yibing Zhan, Binbin Lin, Ziyu Guan, Liu Liu, Baosheng Yu, Wei Zhao,
Yaming Yang, and Dacheng Tao. 2024. SkipNode: On Alleviating Performance
Degradation for Deep Graph Convolutional Networks. IEEE Transactions on
Knowledge and Data Engineering (2024), 1–14. https://doi.org/10.1109/TKDE.
2024.3374701
[19] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore.
2000. Automating the construction of internet portals with machine learning.
Information Retrieval 3, 2 (2000), 127–163.
[20] Shengjie Min, Zhan Gao, Jing Peng, Liang Wang, Ke Qin, and Bo Fang. 2021.
STGSN—A Spatial–Temporal Graph Neural Network framework for time-
evolving social networks. Knowledge-Based Systems 214 (2021), 106746.
[21] Yating Ren, Junzhong Ji, Lingfeng Niu, and Minglong Lei. 2021. Multi-task
Self-distillation for Graph-based Semi-Supervised Learning. arXiv preprint
arXiv:2112.01174 (2021).
[22] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93–93.
[23] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
Günnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprintarXiv:1811.05868 (2018).
[24] Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, and Nitesh Chawla.
2022. Learning mlps on graphs: A unified view of effectiveness, robustness, and
efficiency. In The Eleventh International Conference on Learning Representations.
[25] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[26] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou,
Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang
Li, and Zheng Zhang. 2019. Deep Graph Library: A Graph-Centric, Highly-
Performant Package for Graph Neural Networks. arXiv preprint arXiv:1909.01315
(2019).
[27] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian
Weinberger. 2019. Simplifying graph convolutional networks. In International
conference on machine learning. PMLR, 6861–6871.
[28] Lirong Wu, Haitao Lin, Yufei Huang, Tianyu Fan, and Stan Z Li. 2023. Extracting
Low-/High-Frequency Knowledge from Graph Neural Networks and Injecting
it into MLPs: An Effective GNN-to-MLP Distillation Framework. arXiv preprint
arXiv:2305.10758 (2023).
[29] Lirong Wu, Haitao Lin, Yufei Huang, and Stan Z Li. 2022. Knowledge distillation
improves graph structure augmentation for graph neural networks. Advances in
Neural Information Processing Systems 35 (2022), 11815–11827.
[30] Lirong Wu, Haitao Lin, Yufei Huang, and Stan Z Li. 2023. Quantifying the Knowl-
edge in GNNs for Reliable Distillation into MLPs. arXiv preprint arXiv:2306.05628
(2023).
[31] Lirong Wu, Jun Xia, Haitao Lin, Zhangyang Gao, Zicheng Liu, Guojiang Zhao,
and Stan Z Li. 2022. Teaching Yourself: Graph Self-Distillation on Neighborhood
for Node Classification. arXiv preprint arXiv:2210.02097 (2022).
[32] Taiqiang Wu, Zhe Zhao, Jiahao Wang, Xingyu Bai, Lei Wang, Ngai Wong, and
Yujiu Yang. 2023. Edge-free but Structure-aware: Prototype-Guided Knowledge
Distillation from GNNs to MLPs. arXiv preprint arXiv:2303.13763 (2023).
[33] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[34] Bencheng Yan, Chaokun Wang, Gaoyang Guo, and Yunkai Lou. 2020. Tinygnn:
Learning efficient graph neural networks. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 1848–1856.
[35] Cheng Yang, Jiawei Liu, and Chuan Shi. 2021. Extract the Knowledge of Graph
Neural Networks and Go Beyond It: An Effective Knowledge Distillation Frame-
work. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW
’21). Association for Computing Machinery, New York, NY, USA, 1227–1237.
https://doi.org/10.1145/3442381.3450068
[36] Jing Yang, Xiaoqin Zeng, Shuiming Zhong, and Shengli Wu. 2013. Effective
neural network ensemble approach for improving generalization performance.
IEEE transactions on neural networks and learning systems 24, 6 (2013), 878–887.
[37] Yaming Yang, Ziyu Guan, Wei Zhao, Weigang Lu, and Bo Zong. 2022. Graph
substructure assembling network with soft sequence and context attention. IEEE
Transactions on Knowledge and Data Engineering 35, 5 (2022), 4894–4907.
[38] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. 2020.
Distilling knowledge from graph convolutional networks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7074–7083.
[39] Hanlin Zhang, Shuai Lin, Weiyang Liu, Pan Zhou, Jian Tang, Xiaodan Liang,
and Eric P Xing. 2023. Iterative graph self-distillation. IEEE Transactions on
Knowledge and Data Engineering (2023).
[40] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and
Kaisheng Ma. 2019. Be your own teacher: Improve the performance of con-
volutional neural networks via self distillation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision. 3713–3722.
[41] Shichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. 2021. Graph-less Neural
Networks: Teaching Old MLPs New Tricks Via Distillation. In International
Conference on Learning Representations.
[42] Wentao Zhang, Xupeng Miao, Yingxia Shao, Jiawei Jiang, Lei Chen, Olivier Ruas,
and Bin Cui. 2020. Reliable Data Distillation on Graph Convolutional Network. In
Proceedings of the 2020 ACM SIGMOD International Conference on Management of
Data (Portland, OR, USA) (SIGMOD ’20). Association for Computing Machinery,
New York, NY, USA, 1399–1414. https://doi.org/10.1145/3318464.3389706
[43] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. 2003. Semi-supervised
learning using gaussian fields and harmonic functions. In Proceedings of the 20th
International conference on Machine learning (ICML-03). 912–919.
 
2069AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: Datasets Statics.
Dataset
# Nodes # Edges # Features # Classes Label Rate
Cora 2,708
5,278 1,433 7 5.2%
Citeseer 3,327 4,614 3,703 6 3.6%
Pubmed 19,717 44,324 500 3 0.3%
Photo 7,650 119,081 745 8 2.1%
CS 18,333 81,894 6,805 15 1.6%
Physics 34,493 247,962 8,415 5 0.3%
ogbn-arxiv 169,343 1,166,243 128 40 53.7%
A Implement Details
Hyper-parameters. We set the max training epochs at 500for all
the trails. The search space of the hyper-parameters is as follows:
•Hidden Dimensionality 𝐹={128,256,512,1024,2048}
•Number of Layer 𝐿={2,3}
•Ensemble Size 𝐾={2,3}
•Balance Parameter 𝜆,𝜆NA={0.1,0.2,···,0.9}
•Divergence Sensitivity Parameter 𝛽={0.5,1,2,3,4}
For masking rate 𝜌, we fix it at 0.1in the normal setting and set to
the same value as the feature missing rate in the incomplete test
data setting.
Hardware and Software. AdaGMLP is implemented based on the
DGL library [ 26] and PyTorch 1.7.1 with Intel(R) Core(TM) i9-
10980XE CPU @ 3.00GHz and 2 NVIDIA TITAN RTX GPUs.
B Dataset Statics
Table 4 presents a summary of the statistical characteristics of these
datasets. Data splitting strategies differ depending on the dataset’s
scale:
•For the three small-scale datasets, Cora ,Citeseer , and Pubmed ,
we adopt the data splitting strategy outlined in [13].
•ForCoauthor-CS ,Coauthor-Physics , and Amazon-Photo ,
we follow the procedures from [ 35,41] to perform random
data splits into training, validation, and test sets.
•For the large-scale dataset, ogbn-arxiv , we strictly follow
the publicly available data splits in [11].
CPerformance Comparison with G2G Methods
We compare our AdaGMLP with SOTA GNN-to-GNN (G2G) methods,
i.e., CPF [ 35], RDD [ 42], TinyGNN [ 34], GNN-SD [ 3], and LSP [ 38],
in Table 5. All the methods use GCN as the teacher model. We
reuse the results of G2G methods from [ 30].¯Δ𝐺𝐶𝑁 is the average
improvements across all the datasets over GCN.
We can observe that AdaGMLP consistently shows better per-
formance across the majority of the datasets. The improvements
in accuracy are most notable in the Cora ,Citeseer , and Pubmeddatasets. On Coauthor-CS andogbn-arxiv ,AdaGMLP still demon-
strates competitive performance, although not the top performer. It
maintains robust results but with a slightly lower margin compared
to the top performer (RDD and FreeKD).
AdaGMLP not only enhances efficiency but also maintains com-
petitive accuracy. It achieves higher accuracy than G2G methods
across multiple datasets. Unlike G2G methods, which require mes-
sage propagation during inference, AdaGMLP operates without this
need. This efficiency is crucial in real-world applications, espe-
cially in scenarios with latency constraints and resource limita-
tions, making AdaGMLP an optimal choice for such settings. This
balance between efficiency and accuracy is a significant advantage
for practical applications where both factors are essential.
D Performance Comparison with Ensemble
Methods
In this section, we compare our AdaGMLP against some two well-
known ensemble strategies. i.e., Vote, Bagging [ 5], and a simple
average ensemble strategy that uses the average predictions from
each MLP student. All the strategies use the same configuration.
We conduct experiments under three different settings, including
the transductive setting, insufficient training data setting, and in-
complete test data setting on Cora and Citeseer. The results are
provided in Table 6
In the transductive setting, our AdaBoost strategies achieve the
highest accuracy on both the Cora and Citeseer datasets. Other
strategies, such as average, vote, and bagging, perform relatively
close to the baseline GLNN method but fall short of surpassing Ad-
aBoost. This is attributed to AdaBoost’s adaptive weighting of each
student and emphasis on unaligned knowledge points, allowing it
to focus on the difficult-to-extract knowledge and improve overall
predictive performance.
In the insufficient training data setting, we can see that simple
ensemble strategies can also achieve better performance compared
to GLNN in some cases. However, there is still a performance gap
between them and AdaBoost. It indicates that the AdaBoost’s abil-
ity to adaptively weigh weak learners is particularly effective in
tackling the challenges posed by limited labeled data.
In the incomplete test data setting, we can observe that simple
ensemble strategies can also bring performance improvement when
test data is corrupted. It demonstrates that combining multiple MLP
students is a promising and simple way to mitigate the incomplete
test data issue.
Overall, the results show that AdaBoost outperforms other en-
semble strategies in various settings. Its adaptability, emphasis
on challenging knowledge points, and weighting mechanism con-
tribute to its superior performance. Additionally, the experiments
highlight the potential benefits of ensemble methods for improving
performance of G2M.
 
2070KDD ’24, August 25–29, 2024, Barcelona, Spain Weigang Lu, Ziyu Guan, Wei Zhao, and Yaming Yang
Table 5: Performance comparison with G2G methods.
Metho
d Cora
Citeseer Pubmed Photo CS Physics ogbn-arxiv ¯Δ𝐺
𝐶𝑁
MLPs 56.66±2.02 59.88±0.59 71.94±1.24 78.16±2.76 87.17±1.04 87.24±0.61 53.60±1.31 -
GCN 82.02±0.98 71.88±0.34 77.24±0.23 90.60±2.15 89.73±0.67 92.29±0.58 71.22±0.18 -
LSP 82.70±0.43 72.68±0.62 80.86±0.50 91.74±1.42 92.56±0.45 92.85±0.46 71.57±0.25↑1.73%
GNN-SD 82.54±0.36 72.34±0.55 80.52±0.37 91.83±1.58 91.92±0.51 93.22±0.66 70.90±0.23↑1.41%
Tiny
GNN 83.10±0.53 73.24±0.72 81.20±0.44 92.03±1.49 93.78±0.38 93.70±0.56 72.18±0.27↑2.47%
RDD 83.68±0.40 73.64±0.50 81.74±0.44 92.18±1.4594.20±0.48 94.14±0.39 72.34±0.17↑2.94%
Fr
eeKD 83.84±0.47 73.92±0.47 81.48±0.38 92.38±1.54 93.65±0.43 93.87±0.48 72.50±0.29↑2.91%
A
daGMLP (ours) 84.26±0.8375.42±0.3981.88±0.5392.60±0.37 93.79±0.3394.38±0.27 71.45±0.10↑3.28%
Table 6: Performance comparison with Ensemble methods.
Ensemble
Method Cora Citeseer ¯Δ𝐺
𝐿𝑁𝑁
Transductive
Setting
None
(GLNN) 82.08±1.14 73.46±0.47 -
A
verage 81.62±0.97 72.12±0.40↓1.19%
V
ote 82.04±1.17 72.44±0.37↓0.71%
Bagging 82.68±0.92 73.06±0.52↑0.09%
AdaGMLP 84.26±0.82 75.42±0.39↑2.66%
Insufficient
Training Data Setting
Lab
el Rate 1%
2% 3% 1%
2% 3%
None
(GLNN) 63.24±5.94 72.48±4.68 74.31±4.46 62.74±4.38 65.54±6.39 69.78±3.71 -
A
verage 62.36±5.97 72.68±2.72 75.26±1.64 57.16±6.92 64.12±2.49 67.28±1.08↓2.14%
V
ote 61.40±5.53 73.90±1.99 76.94±2.48 56.36±7.34 63.74±2.48 67.20±0.75↓2.07%
Bagging 61.98±5.72 74.40±1.73 77.28±2.51 56.88±6.70 64.06±2.28 68.03±0.94↓1.31%
AdaGMLP 71.20±4.2277.92±1.2280.38±1.0563.84±2.1366.46±2.6169.96±1.91↑5.58%
Incomplete
Test Data Setting
Featur
e Missing Rate 10%
30% 50% 10%
30% 50%
None
(GLNN) 68.84±4.71 65.70±3.26 59.34±4.07 60.06±4.04 59.12±4.13 57.02±4.10 -
A
verage 71.90±0.82 68.00±0.56 61.94±0.82 61.34±0.41 60.30±0.90 58.10±0.61↑3.05%
V
ote 71.82±0.58 67.98±0.43 61.92±0.92 61.02±0.21 59.86±0.56 57.84±0.54↑2.73%
Bagging 71.94±1.10 68.00±0.58 62.03±1.14 61.28±0.09 59.99±0.47 57.98±0.57↑2.95%
AdaGMLP 82.42±0.7280.46±1.3278.54±1.3873.14±0.5771.16±1.7364.36±1.45↑21.59%
 
2071