Multi-source Unsupervised Domain Adaptation on Graphs with
Transferability Modeling
Tianxiang Zhao
The Pennsylvania State University
State College, USA
tkz5084@psu.eduDongsheng Luo
Florida International University
Miami, USA
dluo@fiu.edu
Xiang Zhang
The Pennsylvania State University
State College, USA
xzz89@psu.eduSuhang Wang
The Pennsylvania State University
State College, USA
szw494@psu.edu
Abstract
In this paper, we tackle a new problem of multi-source unsupervised
domain adaptation (MSUDA) for graphs, where models trained on
annotated source domains need to be transferred to the unsuper-
vised target graph for node classification. Due to the discrepancy
in distribution across domains, the key challenge is how to se-
lect good source instances and how to adapt the model. Diverse
graph structures further complicate this problem, rendering previ-
ous MSUDA approaches less effective. In this work, we present the
framework Selective Multi-source Adaptation for Graph (SelMAG),
with a graph-modeling-based domain selector, a sub-graph node
selector, and a bi-level alignment objective for the adaptation. Con-
cretely, to facilitate the identification of informative source data,
the similarity across graphs is disentangled and measured with
the transferability of a graph-modeling task set, and we use it as
evidence for source domain selection. A node selector is further
incorporated to capture the variation in transferability of nodes
within the same source domain. To learn invariant features for
adaptation, we align the target domain to selected source data both
at the embedding space by minimizing the optimal transport dis-
tance and at the classification level by distilling the label function.
Modules are explicitly learned to select informative source data
and conduct the alignment in virtual training splits with a meta-
learning strategy. Experimental results on five graph datasets show
the effectiveness of the proposed method.
CCS Concepts
‚Ä¢Computing methodologies ‚ÜíNeural networks; Statistical
relational learning; Transfer learning.
Keywords
graph neural networks, transfer learning, domain adaptation
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671829ACM Reference Format:
Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang. 2024.
Multi-source Unsupervised Domain Adaptation on Graphs with Trans-
ferability Modeling. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671829
1 Introduction
Graph neural networks (GNNs) have shown great ability in rep-
resentation learning on graphs, especially the node classification
task [ 36,68]. Nevertheless, the success of GNNs heavily relies on la-
bel information; while for many real-world applications, obtaining
label information is costly and time-consuming [ 33,46]. The lack
of label information challenges many existing GNNs. In practice,
one often has access to multiple annotated domains in training. For
example, social networks may be collected from different platforms,
communities of different ethnicities, and users speaking different
languages. For a newly-collected social network, typically node
labels are unknown and we want to classify them with models
trained from those source domains. Recent years have featured a
trend toward transferring knowledge across datasets to alleviate
the lack of supervision [ 10,21], which motivates us to explore the
application of GNNs trained on annotated datasets (source domains)
to the new unlabeled dataset (target domain) [ 30,63] (the induc-
tive setting). This scenario can be generalized to a new learning
problem: multi-source unsupervised domain adaptation (MSUDA) for
graphs as shown in Fig. 1.
There are some significant challenges in solving the MSUDA
task [ 56,74]: (1) the mismatch between data distributions of the
source and target domains, which requires an implicit or explicit
adaptation of the trained model [ 4,5]; (2) A clear discrepancy also
exists among multiple source domains, hampering the effectiveness
of mainstream single-source domain adaptation methods [ 15,58].
Explorations have been made addressing these challenges. Existing
methods differentiate the effects of source domains on the target
one by computing domain similarities from the lens of different
perspectives, such as conditional distribution probability [ 12,55],
and estimation with an adversarial discriminator [ 74], etc. Then,
they can transform it into a single-source problem via re-weighting
and conduct alignment for the adaptation [ 43]. Furthermore, [ 3,
4479
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang
Figure 1: An example of MSUDA for graphs. We want to trans-
fer knowledge from annotated source domains {ùëÆ1ùë†,ùëÆ2ùë†,ùëÆ3ùë†}to
ùëÆùë°. Regions of the same color denote similar node attributes.
It can be observed that source domains and sub-graphs of
each domain are of different importance in adapting to ùëÆùë°
w.r.t node distributions.
16,41] provide a performance bound of adapting from a weighted
combination of source domains to the target domain.
However, all the aforementioned methods are designed for in-
dependent and identically distributed (i.i.d) data; while the com-
plex graph structures bring new challenges to MSUDA. In graph-
structured data, nodes are interconnected with edges, and the num-
ber of possible topology structures grows exponentially with the
graph size. This rich and highly-diverse input space offers increased
freedom for the mapping in <graph structure, node annotation>
pairs. As a result, computing domain similarities in the input or
embedding space and taking it as the estimation of informativeness
for node classification could be unreliable, and it is unsafe to assume
that nodes of the same source domain would carry a similar level
of knowledge for this transfer, as the example in Fig. 1. It is difficult
to identify subsets of source domains containing discriminative
knowledge to transfer to the target domain due to the intricate
nature of graphs. Furthermore, complex graph structures also rise
difficulties in alleviating discrepancy among domains, rendering the
conventional approach of aligning only in the embedding domain
less-effective [15, 40].
To address the aforementioned challenges, we propose to com-
prehensively depict the similarity between graphs from multiple
views, such as distribution of node attributes, edge existences, topo-
logical structures, etc. The informativeness of source graphs should
further be modeled in aware of the downstream node classification
task, and at both graph and node levels to prevent sub-optimal adap-
tations. As shown in [ 5], the performance of domain adaptation is
bounded by the discrepancy in distribution between the source and
the target data set, and a tighter adaptation bound can be derived
by identifying subsets of source data that are more similar to the
target domain [ 3,16]. With beneficial source data identified, we can
achieve the adaptation through aligning the embedding learning
and label prediction processes.
Concretely, we design the first framework, SelMAG, for MSUDA
on graphs. First, a set of graph modeling tasks are selected to capture
the graph distribution of each domain. The transferability of models
trained on these tasks provides a numeric estimation of domain
similarity from different perspectives. Then, based on obtained
similarity measurements, a source-graph selector is designed to
identify informative source domains towards the target domain on
the target downstream task. Furthermore, a sub-graph node selectoris adopted to assign different important scores to nodes of the same
source domain. It can help conduct more fine-grained selections
and meanwhile alleviate the target shift problem (like when class
proportions are different) [ 47]. Virtual training splits are designed
to explicitly optimize these selectors in capturing transferability
across graph domains with Meta-learning [ 24]. Finally, an optimal-
transport-based alignment is conducted both in the embedding
space and the classification space. Our main contributions are:
‚Ä¢We study a novel problem of MSUDA for graph-structured data,
by selecting informative source data and adapting in both the
embedding and classification space.
‚Ä¢To identify sub-graphs of source domains that can transfer
discriminative knowledge, we design a modeling-based graph
selector and a sub-graph node selector, and explicitly train them
on the selection-and-adapt pipeline with meta-learning.
‚Ä¢We design an adaptation objective by conducting bi-level align-
ments with optimal transport and knowledge distillation, which
can intrinsically incorporate the learned informativeness of
source domains simultaneously.
‚Ä¢Experimental evaluations show that SelMAG achieves state-of-
the-art performance on five datasets, validating its design. Case
studies further show the ability of SelMAG in capturing the
informativeness of source domains.
2 Related Work
2.1 Graph Neural Network
With the increasing need for learning on relational data struc-
tures [ 18,22], various graph neural networks have been designed,
which can be categorized into spectral approaches [ 11,36,57] and
spatial approaches [ 2,30,66]. Despite their differences, most GNNs
fit within the framework of message-passing [ 28], in which nodes
are iteratively updated by aggregating messages from local neigh-
borhoods. For instance, GCN [ 36] passes messages from neighbor-
ing nodes with fixed weights, GAT [ 59] applies the self-attention
mechanism to learn different attention scores and selects neighbor-
hood messages dynamically. Some works[ 1,39,67] augment GNNs
with explicit prototypes to hierarchically model the motif structures
and increase the data efficiency. Other works [ 65,71,77] propose
to uncover latent groups of nodes or edges and pass messages on
the disentangled graph. Recently, explorations have also been made
over the trustworthiness of graph neural networks [ 20,51,52,61]
and their explainability [19, 75].
Despite the great success of GNNs, their success usually hinges
upon the availability of labeled training data, especially for the
task of node classification [ 36,78]. However, in practice, for a
newly-forming or low-resource target graph, its nodes are usu-
ally unlabeled. The lack of labeled data challenges many existing
GNN classifiers. Thus, we often need to adapt a model trained from
mature source domains which have abundant label information
to the target domain. However, distribution shifts often exist be-
tween the source domains and the target domain, which calls for
the development of domain adaptation algorithms on graph [ 63].
Particularly, as there are usually multiple labeled source domains
can be exploited, we propose to study a novel problem of multi-
source domain unsupervised adaptation on graphs, which aims to
4480Multi-source Unsupervised Domain Adaptation on Graphs with Transferability Modeling KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
adapt classifiers from multiple source graphs to the target unlabeled
graph.
2.2 Unsupervised Domain Adaptation
Unsupervised domain adaptation (UDA) aims to transfer the knowl-
edge learned from labeled source domains to the unlabeled target
domain [ 4,5,49,60]. To adapt the knowledge from the source do-
mains to the target domain, one major challenge is how to address
the distribution shift between source and target domains. Most
recent UDA methods focus on aligning source and target domains
by learning domain-invariant features [ 23,58], encouraging fea-
tures to follow the same distribution regardless of which domain
they come from [ 60]. To reduce the distribution discrepancy, some
works [ 40,53,69] propose to minimize a divergence that measures
the distance between distributions. [ 8,27] propose to learn repre-
sentations that can reconstruct the data distribution of the target
domain. Other works [ 15,58] use adversarial learning to fool a
discriminator which is trained to differentiate between two distri-
butions. The aforementioned approaches are all designed for i.i.d
data. Recently, there are few works on domain adaptation on other
data structures, e.g., graph data [ 42,73], text data [ 13,48,50,64,72],
etc. For example, UDA-GCN [ 63] extends adversary alignment [ 58]
to graphs with an attentive feature extractor to learn the invariant
features. Overall, it is non-trivial to formulate graph distribution
and design adaptive models considering its highly-diverse topology,
graph size, and node features [62, 63].
Particularly, this work is related to multi-source unsupervised
domain adaptation. This task is complicated further by distribu-
tion discrepancy among source domains[ 14,38,44,70]. Theoretical
analyses have been provided w.r.t the performance bound of multi-
source unsupervised domain adaptation [ 3,16,41], showing the im-
portance of selecting important source domains. Explorations have
been made in measuring domain similarities with conditional distri-
bution probability from the smoothness assumption [ 12,55]. Zhao
et al. [74] uses an adversarial discriminator and conducts the worst-
case alignment, and Nguyen et al . [43] adopts a model-based simi-
larity estimation. However, all these methods are designed for i.i.d
data and have difficulty in applying to graphs. In this work, ad-
dressing the highly-diverse graph structures, we propose to design
a domain selector based on disentangled similarity measurements
with graph-modeling tasks and further conduct sub-domain se-
lection with a node-level selector, which is better in estimating
informativeness of source domains w.r.t node classification of the
target graph.
3 Preliminary
3.1 Notations and Problem Definition
Semi-supervised Node Classification. We focus on the node-
level classification task in this work. Specifically, we use G=
(V,E;ùë≠,ùë®)to denote a graph, where Vis the node set and E‚äÇ
V√óV is the set of edges. Nodes are accompanied by an attribute
matrix ùë≠‚ààR|V|√óùëë, andùëñ-th row of ùë≠is theùëë-dimensional at-
tributes of the corresponding node. Eis described by an adjacency
matrix ùë®‚ààR|V|√ó|V|.ùê¥ùë£ùë¢=1if there is an edge between node ùë£
andùë¢; otherwise, ùê¥ùë£ùë¢=0.ùíÄ‚ààR|V|is the class information for
nodes in G, obtained with an unknown labeling function ùëì, andùëÖ(ùíÄ)is the number of classes. During training, only a subset of ùíÄ,
¬ØùíÄis available, containing the labels for the training node set. Based
on those labeled nodes, a hypothesis model ‚Ñéis trained to recover
the unknown function ùëìand to predict node classes:
‚Ñé(ùë£;G)‚Üí ùíÄùë£,‚àÄùë£‚ààV. (1)
Multi-source Unsupervised Graph Adaptation. We use domain
to define a distribution over the graph generation and its latent
labeling function, denoted as ‚ü®ùê∑,ùëì‚ü©. In this multi-source graph
adaptation, we have partially labeled graphs collected from ùêæsource
domains asGùë†={Gùëòùë†,¬ØùíÄùëòùë†}ùêæ
ùëò=1and an unsupervised graph Gùë°from
the target domain. Each source graph Gùëòùë†is generated following the
distribution ùê∑ùëòùë†and its ¬ØùíÄùëòùë†is obtained with latent labeling function
ùëìùëòùë†. The objective is to build a hypothesis classifier ‚Ñéùë°that works
well on the target domain ‚ü®ùê∑ùë°,ùëìùë°‚ü©, predicting classes of nodes in
Gùë°accurately. Concretely, the task can be formalized as:
Givenùêæpartially labeled graphs {Gùëòùë†,¬ØùíÄùëòùë†}ùêæ
ùëò=1from different source
domains and an unsupervised graph Gùë°from the target domain
‚ü®ùê∑ùë°,ùëìùë°‚ü©, we aim to train a node classification model ‚Ñéùë°to simulateùëìùë°
with a small lossL(ùëìùë°(Gùë°),‚Ñéùë°(Gùë°))on the target graph Gùë°.
3.2 Optimal Transport for DA
Optimal Transport (OT) provides a theoretic tool for computing
distances between probability distributions and for alignment-based
domain adaptation [ 15], on which our method SelMAG is developed.
In this section, we provide an introduction to its key concepts and
some important results used in the following sections. The OT
problem searches for a plan with the minimum cost to transform
a distribution ùê∑ùë†(over space Œ©ùë†) to another distribution ùê∑ùë°(over
space Œ©ùë°). The cost of transforming each element is measured by
a cost function ùëê:Œ©ùë†√óŒ©ùë°‚ÜíR+. Following the Monge mass
transfer problem [ 6], the search of transport plan ùëáwith minimum
total transportation cost ùê∂(ùëá)can be expressed as:
arg min
ùëáùê∂(ùëá)=‚à´
Œ©ùë†ùëê(ùíô,ùëá(ùíô))ùëëùê∑ùë†(ùíô),
s.t.ùëá#ùê∑ùë†=ùê∑ùë°,(2)
whereùëá()denotes the transport plan and ùê∑ùë†()means the distribu-
tion function in the source domain. ùëá#ùê∑ùë†is the image measure ofùê∑ùë†
byùëá[15], another probability measure defined over Œ©ùë°satisfying:
ùëá#ùê∑ùë†(ùêµ)=ùê∑ùë†(ùëá‚àí1(ùêµ)),‚àÄBorel subset ùêµ‚äÇŒ©ùë° (3)
This guarantees that ùëáis a transport map from ùê∑ùë†toùê∑ùë°. In practice,
cost function ùëêis usually set as the squared Euclidean distance [ 15].
As shown in [35], a convex relaxation of Eq. (2) can be derived as:
arg min
ùõæ‚ààŒ†‚à´
Œ©ùë†√óŒ©ùë°ùëê(ùíôùë†,ùíôùë°)ùëëùõæ(ùíôùë†,ùíôùë°), (4)
where Œ†is defined to be the set of all probabilistic couplings in
ùëÉ(Œ©ùë†√óŒ©ùë°)with marginals ùê∑ùë†andùê∑ùë°.ùõæcan be understood as a
joint probability measure for the transportation plan.
To expose the connection between optimal transport and domain
adaptation, following previous analysis [ 4,47], the error bound of
applying source models to the target domain can be summarized
4481KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang
Figure 2: Adaptation process of SelMAG. Source importance
is estimated with a global graph selector and sub-graph node
selector, and is incorporated into this bi-level alignment.
as:
ùúñùë°(‚Ñé)‚â§ùêæ‚àëÔ∏Å
ùëò=1ùõºùëò¬∑ùúñùëò(‚Ñé)+ùúÜ‚àó+ùëëH(ùêæ‚àëÔ∏Å
ùëò=1ùõºùëòùê∑ùëò,ùê∑ùë°), (5)
where the first term represents errors on source domains, ùúÜ‚àó=
min‚Ñé{ùúñùë°(‚Ñé)+√çùêæ
ùëò=1ùõºùëòùúñùëò(‚Ñé)}is the optimal joint error and is a
constant, while ùëëH(√çùêæ
ùëò=1ùõºùëòùê∑ùëò,ùê∑ùë°)measuresH-divergence across
two distributions. To achieve a low error on the target domain, the
discrepancy between the distribution of the target domain and that
of source domains needs to be minimized (the third term), which can
be achieved by aligning them in the embedding space, corresponds
to learning a generalizable feature extractor across domains and
minimizing the optimal transportation cost minùëáùê∂(ùëá)on it [15].
4 Methodology
In this section, we provide details of SelMAG, which can identify in-
formative subsets of source domains utilizing disentangled domain-
level similarities together with sub-domain node-level selections.
An overview of the adaptation process is provided in Fig. 2.
4.1 Learning of Source Models
As we have labels for source domains, we can train a set of source
models{‚Ñéùëòùë†}ùêæ
ùëò=1, one for each domain, using the labeled data. Those
models will be used to train a target model ‚Ñéùë°later. Specifically, each
hypothesis model ‚Ñéis composed of two parts, a feature extraction
moduleùëîext(with stacked GNN layers) and a classifier module ùëîcls
(as an MLP). Taking node ùë£from Gùëñùë†as an example, the feature
extraction module first learns node representation of ùë£as:
ùíôùë£=ùëîùëñ
ext(ùë£;Gùëñ
ùë†), (6)
With the learned node representation ùë•ùë£, the classifier module
predict the label of node ùë£:
ÀÜùë∑ùë£=ùëîùëñ
cls(ùíôùë£), (7)
ÀÜùë∑ùë£‚ààRùëÖ(ùíÄ)with each dimension representing the predicted prob-
ability ofùë£belonging to the corresponding class. For each source
graph Gùëñùë†, cross-entropy loss is used for learning:
min
‚Ñéùëñùë†Lùê∂ùê∏=‚àí‚àëÔ∏Å
ùë£‚àà¬ØVùëñùëÖ(ùíÄ)‚àëÔ∏Å
ùëê=11(¬Øùëåùë£==ùëê)¬∑log(ÀÜùëÉùë£[ùëê]), (8)where ¬ØVùëñdenotes the labeled node set of Gùëñùë†. To promote the align-
ment in the embedding space, we adopt a hard parameter sharing on
the feature extractor of all source domains and leave the classifier
modules domain-specific.
The target model ‚Ñéùë°is learned upon two objectives: (1) aligning
embeddings of the target graph to those of selected informative
subsets of source graphs with a loss derived from optimal transport
theory, and (2) the classification loss on pseudo labels generated
with source models through a weighted distillation strategy. Details
of them will be introduced in the following sections.
4.2 Estimate Transferability of Source Domains
Distribution shifts result in different levels of transferability from
source domains to Gùë°, and sub-graphs of the same domain would
also vary in importance for transferring discriminative features. To
identify informative subsets of source graphs that can be transferred
to the target domain, we adopt a coarse-to-fine paradigm. First, a
graph-level selector is designed conditioned on factorized similarity
measurements and then a node-level selector is proposed to capture
informative subsets of each source graph. We will go into detail
about them in this section.
4.2.1 Modeling-based Graph Selector. In this part, we introduce
the design of the graph-level selector based on similarities that are
factorized into the transferability of different graph modeling tasks.
Generally, if prediction models trained on graph Gùëñcan perform
well on Gùë°, then GùëñandGùë°are similar in some ways. A set of self-
supervision tasks can be designed to model the graph distribution
from different perspectives, and we can train on Gùëòùë†and test the
performance on Gùë°to obtain disentangled similarity measurements.
Various self-supervision tasks can be adopted. In this paper, we
adopt the following three self-supervision tasks related to graph
structure/properties, which can better model graph distribution for
measuring the transferability. We leave the exploration of other
tasks as future work.
‚Ä¢Node Prediction [ 32]. In this task, we randomly mask a ratio of
nodes of the graph as input, and the objective is to predict the
attributes of those masked nodes.
‚Ä¢Edge Prediction [ 34]. We randomly sample a set of edges for
training, and the objective is to predict the existence of edges for
a random pair of nodes.
‚Ä¢Context Prediction [ 32]. Based on node attributes, we cluster the
nodes of those graphs into multiple groups with the K-Means
algorithm. Then, for each node, we obtain its so-called ‚Äúcontext‚Äù
as the group distribution of its direct neighborhood. The objective
is to predict obtained context for each node.
It can be seen that these self-supervised learning (SSL) tasks focus
on different aspects of the graph and have different reliance upon
typical graph elements: nodes, edges, and neighborhood topology.
The transferability of these tasks between each source graph and the
target graph can depict their similarities from different perspectives,
which could be used to evaluate the informativeness of each source
graph towards the downstream node classification task.
We useùëîùëñ
ùëáto denote the prediction module w.r.t SSL task ùëá
on graph Gùëñ, as shown in Fig. 2. Concretely, first, we train mod-
ules{ùëîùëá}ùëá‚ààTw.r.t these graph modeling tasks (denoted as T) for
4482Multi-source Unsupervised Domain Adaptation on Graphs with Transferability Modeling KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
each graph respectively. Then, we quantify the transferability of
these SSL tasks from graph ùëóto graphùë°as the performance after
exchanging the trained modules:
ùë†ùëó‚Üíùë°
ùëá=Eùë£‚ààVùë°Lùëá(ùëîùëó
ùëá(ùíôùë£),Gùë°), (9)
whereVùë°is the node set of Gùë°,ùíôùë£is extracted node embedding
following Eq. 6, and Lùëá(ùëîùëó
ùëá(ùíôùë£),Gùë°)measures loss of task ùëáfor
nodeùë£ofGùë°.ùë†ùëó‚Üíùë°
ùëádenotes the obtained performance of module ùëîùëó
ùëá
when applied to Gùë°. Finally, the graph-level selector ùëîglobal
seltakes
the transferred performance scores on those SSL tasks as input
evidence and predicts the informativeness of graph GùëótoGùë°w.r.t
the downstream task as:
ÀÜùë†ùëó‚Üíùë°
global=ùëîglobal
sel {ùë†ùëó‚Üíùë°
ùëá}ùëá‚ààT,{ùë†ùëñ‚Üíùë°
ùëá}ùëá‚ààT
ùë†ùëó‚Üíùë°
global=ÀÜùë†ùëó‚Üíùë°
global
√ç
ùëó‚â†ùëñÀÜùë†ùëó‚Üíùë°
global,(10)
where the obtained ùë†ùëó‚Üíùë°
global‚àà[0,1]. Note we incorporate the perfor-
manceùë†ùëñ‚Üíùë°
ùëáinto the input, which can expose relative performance
drop and help evaluate the distribution shift.
4.2.2 Sub-graph Node Selector. The modeling-based selector as-
signs a weight to each source graph encoding its importance as a
whole. However, for each source graph, some of its sub-graphs may
be more important than others in adapting to the target domain
‚ü®ùê∑ùë°,ùëìùë°‚ü©. To conduct a more fine-grained selection and identify in-
formative node groups of each source graph, we further design a
subgraph-level node selector in this part. Without loss of generality,
we implement it as a 2-layer MLP. For each candidate node ùë£of the
source graph Gùëó, we concatenate its embedding with the global
representation of target graph Gùë°as the input:
ùë†ùëó‚Üíùë°,ùë£
local=ùëîlocal
sel ùíôùëó
ùë£,poolingùë¢‚ààGùë°(ùíôùëñ
ùë¢), (11)
where ùíôùëó
ùë£denotes the embedding of node ùë£from graph Gùëóobtained
with its feature extractor following Eq. 6. We take pooled node
embeddings of Gùëóas its graph-level representation. Specifically,
we adopt both max-pooling and mean-pooling and concatenate
them together to preserve both distinct parts and global patterns of
Gùë°[76]. This selector will give similar weights to source data with
similar embeddings in effect.
However, it is challenging to learn these two selectors with back-
propagation on the adaptation performance due to the unsupervised
target domain. Hence, we propose to optimize them with meta-level
updates, which will be introduced in Sec. 4.4.
4.3 Alignment-based Domain Adaptation
With transferability between source graphs and the target graph
encoded, in this section, we introduce our strategy to incorporate it
into the adaptation process to train the classifier that works for Gùë°.
Previous analysis shows that the error of cross-domain adaptation is
bounded by both the global divergence across two domains and the
class-wise distribution shifts (which can be measured as the optimal
joint error) [ 4,56]. Therefore, we design the learning objectives of
model‚Ñéùë°by mapping nodes of Gùë°into the same space as selected
data of source domains for aligning embeddings and imitating classi-
fication behavior of selected source models for aligning the labelingfunctions. Specifically, we design an optimal-transport-based algo-
rithm that is able to utilize extracted transferability intrinsically
along with a weighted knowledge distillation mechanism. Details
are provided in the following parts.
4.3.1 Selective Optimal Transport for Adaptation. Based on the
analysis in Sec. 3.2 and the error bound presented in Eq. 5, adapting
the GNN model requires minimizing the distance between the target
and source distributions, which can be achieved by reducing the
minimum total transportation cost in Eq. 4. To obtain a smoother
transport plan and increase the optimization efficiency [ 17], an
entropy-based regularization ùëÅùê∏is added and the alignment loss
can be formulated as:
minùõæLOT=‚à´
Œ©ùë†√óŒ©ùë°ùëê(ùíôùë†,ùíôùë°)ùëëùõæ(ùíôùë†,ùíôùë°)+ùúñùëÅùê∏(ùõæ), (12)
whereùëÅùê∏(ùõæ)=‚à´
Œ©ùë†√óŒ©ùë°ùõæ(ùíôùë†,ùíôùë°)logùõæ(ùíôùë†,ùíôùë°)ùëëùíôùë†ùëëùíôùë°is the negen-
tropy of transport plan ùõæ. This loss can guide the adaptation of the
target model to reduce the optimal transport distance between the
embedding of source graphs and that of the target graph.
However, this objective neglects the difference in informative-
ness among source instances toward the target data. In multi-source
graph adaptation, different source graphs and sub-graphs of the
same graph may contribute differently to the learning of the tar-
get model. Addressing this problem, we incorporate the predicted
transferability ùë†global andùë†local into the selective OT-based adap-
tation objective by augmenting the transport cost. Our basic idea
is that the alignment should be focused on source data that have
high importance for knowledge transfer to the target graph, and
contrarily for the rest of the source data to prevent the problem
of negative transfer. Hence, for node ùë£from source graph Gùëó, the
transport cost measurement w.r.t target graph Gùë°can be calculated
as:
ùëêsel(ùíôùë£,ùíôùë°)=ùëê(ùíôùë£,ùíôùë°)¬∑ùë†ùëó‚Üíùë°
global¬∑ùë†ùëó‚Üíùë°,ùë£
local, (13)
ùëêsel(ùíôùë£,ùíôùë°)is used to replace the original cost measurement ùëê(ùíôùë†,ùíôùë°)
in Eq. 12. This design enables us to select and highlight informative
parts of the source graph set during the embedding alignment for
model adaptation. For tractable optimization, we can obtain its dual
form with the Fenchel-Rockafellar theorem [17]:
LSelOT =max
ùõΩ‚à´
Œ©ùë†ùõΩ(ùíôùë†)ùëëùíôùë†+‚à´
Œ©ùë°ùõΩùëê
ùúñ(ùíôùë°)ùëëùíôùë°	
where
ùõΩùëê
ùúñ(ùíôùë°):=(
minùíôùë†{ùëêsel(ùíôùë†,ùíôùë°)‚àíùõΩ(ùíôùë†)}, ùúñ=0
‚àíùúñlog Eùíôùë†[ùëíùë•ùëùùõΩ(ùíôùë†)‚àíùëêsel(ùíôùë†,ùíôùë°)
ùúñ], ùúñ>0,(14)
in whichùõΩ()is a scoring function and can be simulated with a
network. We set ùúñas a small positive number following [43].
4.3.2 Weighted Knowledge Distillation. As the domain adaptation
performance is bounded both by the shifts on the embedding space
and the shifts on the learned labeling functions [ 43], to transfer the
classification information from source models, we further adopt a
knowledge distillation loss to provide training signals in the label
space. Concretely, based on the importance score ùë†global of each
source model ‚Ñéùëòùë†, the soft pseudo label of node ùë£inGùë°can be
obtained as
¬ØùíÄùë°,ùë£=‚àëÔ∏Å
Gùëó‚ààGùë†‚Ñéùëò
ùë†(ùë£;Gùë°)¬∑ùë†ùëó‚Üíùë°
global(15)
4483KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang
Then the target model ‚Ñéùë°can be trained as
LKD=Eùë£‚ààGùë°‚àíùëÖ(ùíÄ)‚àëÔ∏Å
ùëñ=1¬ØùíÄùë°,ùë£[ùëñ]log‚Ñéùë°(ùë£;Gùë°)[ùëñ], (16)
whereùëÖ(ùíÄ)is the number of classes, same as the dimension of ¬ØùíÄùë°,ùë£.
‚Ñéùë°(ùë£;Gùë°)is the output of target model ‚Ñéùë°for nodeùë£ofGùë°, with its
ùëñ-th dimension as predicted probabiliy of falling into class ùëñ. This
cross-entropy loss will distill the knowledge of learned labeling
functions of source domains to the target domain.
4.3.3 Objective Function of SelMAG. Putting everything together,
the final learning loss for the alignment-based domain adaptation
can be written as:
min
‚Ñéùë°ùúÜ¬∑LSelOT+(1‚àíùúÜ)¬∑L KD, (17)
whereùúÜcontrols the balance between alignment on the embedding
space and that on the labeling function space respectively.
4.4 Optimization with Meta-learning
The adaptation objective in Eq. 17 emphasizes reducing the align-
ment distance. Directly optimizing the proposed selectors on this do-
main adaptation task is improper and may result in trivia solutions,
as it provides no signals over transferring class-discriminative infor-
mation and has the danger of highlighting uniformly distributed but
non-informative (sub-)graphs. To improve the selective knowledge
transfer for domain adaptation, we design a meta-learning-based
optimization strategy [ 24] by simulating the unsupervised domain
adaptation setting and guiding the learning of selectors based on
performance after adaptation. This ‚Äúlearning to learn‚Äù pipeline can
provide explicit learning signals for selectors.
Concretely, to make the meta-training process in conformity
with the adaptation process and directly optimize it, we propose to
directly learn to select from source graphs by iteratively conducting
two learning steps, inner update, and outer update. Below, we will
show how these two steps are designed.
Inner Update. To guarantee consistency, setting on the inner up-
date is designed to be also a multi-source graph adaptation task.
For available source graph set Gùë†, in each iteration, we sample a
pseudo target graph ÀÜGùë°and use its complement as pseudo source
graphs ÀÜGùë†. The target model ‚Ñéùë°is updated for Tsteps on ÀÜGùë°fol-
lowing Eq. 17, simulating the process of adaptation. At ùëò-th step,
parameters are updated with:
ùúÉùëò
ùë°=ùúÉùëò‚àí1
ùë°‚àíùõº¬∑‚àáùúÉùëò‚àí1
ùë°(ùúÜ¬∑ÀÜLSelOT+(1‚àíùúÜ)¬∑ÀÜLKD),(18)
whereùúÉùë°is the parameter of model ‚Ñéùë°andùõºis the optimization step
size. Losses ÀÜLSelOT and ÀÜLKDare calculated on the pseudo source
and target graphs.
Outer Update. Nodes of ÀÜGùë°are labeled and can be utilized to eval-
uate the performance of graph adaptation. Gradients from adapted
‚Ñéùë°can be back-propagated to the weights ÀÜùë†global ,ÀÜùë†localof pseudo
source graphs ÀÜGùë†, and further, be utilized to optimize the two se-
lectors. Concretely, ùúÉselcan be optimized accordingly to explicitly
improve upon ‚Äúlearning to select‚Äù as:
ùúÉsel=ùúÉsel‚àíùúÇ¬∑‚àëÔ∏Å
ùë£‚ààÀÜGùëñ‚àáùúÉselLcls(ùë£;ÀÜGùë°), (19)Algorithm 1 Full Training Algorithm
Input: Gùë°=(Vùë°,ùë¨ùë°;ùë≠ùë°,ùë®ùë°),Gùë†={Gùëòùë†,¬ØùíÄùëòùë†}ùêæ
ùëò=1
Output: Node classifier ‚Ñéùë°that works for Gùë°
1:Train node classifiers {‚Ñéùëòùë†}ùêæ
ùëò=1for source graphs
2:For each graph Gùëñ‚ààGùë†, train modules{ùëîùëñ
ùëá}ùëá‚ààTw.r.t graph
modeling tasksTin Sec. 4.2.1
3:while Not Converged do
4: Sample ÀÜGùë°,ÀÜGùë†fromGùë†
5:forTinner update steps do
6: Estimateùë†global from ÀÜGùë†toÀÜGùë°withùëîglobal
sel
7: Estimateùë†localfrom ÀÜGùë†toÀÜGùë°withùëîlocal
sel
8: Update the transport function ùõæbased on Eq. 14 for trans-
portation cost estimation
9: Update‚Ñéùë°following Eq. 18
10: end for
11: Evaluate‚Ñéùë°for node classification on ÀÜGùë°
12: Update selector parameters ùúÉselfollowing Eq. 19
13:end while
14:Repeat line 5to line 10to update‚Ñéùë°on the target graph Gùë°
with source graphs Gùë†
15:return Trained model ‚Ñéùë°for the target graph Gùë°
whereùúÉselis the parameter of model ùëîglobal
selandùëîlocal
sel,ùúÇis the
learning rate, andLclsis node classification loss on labeled nodes
ofÀÜGùë°, with model parameters learned from inner update.
These two steps are performed iteratively and can be summarized
in Alg. 1. With inner and outer updates, both selectors are trained
to predict informative subsets of source graphs for the target graph
w.r.t the downstream node classification task. Therefore, they can
be trusted to be applied to Gùë°afterward this meta-training phase.
Following meta-training steps, in adapting to Gùë°, we will fine-tune
the parameter of ‚Ñéùë°following Eq. 17, as shown in Alg. 1.
5 Experiment
In this section, we evaluate the proposed SelMAG on 5real-world
datasets. Specifically, we seek to answer the following questions:
‚Ä¢RQ1: Can the proposed approach improve the adaptation per-
formance on real-world graph datasets?
‚Ä¢RQ2: What is the importance of each component for the success
of SelMAG? How would different hyperparameter configura-
tions influence the effectiveness of SelMAG?
‚Ä¢RQ3: Would SelMAG be able to capture the importance of each
source domain for this knowledge transfer?
5.1 Datasets
To evaluate the effectiveness of SelMAG, we conduct experiments
on five publicly available datasets, including Citation [ 63], Twitch [ 54],
Yelp1, Cora_full [ 7] and Arxiv [ 31]. We provide details of these
datasets and our preprocessing in Appendix A.
1https://www.yelp.com/dataset
4484Multi-source Unsupervised Domain Adaptation on Graphs with Transferability Modeling KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 1: Multi-source graph adaptation performance
Single-Source Multi-Source
Datasets Metrics Direct MMD Reverse Adversarial OptimalT UDA-GCN DistMDA MDAN MLDG SelMAG
CitationACC 66.5¬±0.13 62.7¬±0.31 65.2¬±1.22 64.8¬±0.83 55.2¬±0.69 65.9¬±0.35 67.3¬±0.24 66.5¬±0.19 66.2¬±0.1268.5¬±0.22
AUROC 84.1¬±0.11 84.4¬±0.29 85.7¬±0.32 86.1¬±0.29 79.6¬±0.34 84.7¬±0.28 84.5¬±0.24 85.5¬±0.14 85.6¬±0.0786.3¬±0.09
MacroF 64.0¬±0.15 61.7¬±0.35 62.2¬±0.41 60.6¬±0.33 48.5¬±0.47 62.8¬±0.36 63.2¬±0.23 62.1¬±0.12 62.5¬±0.1864.8¬±0.21
TwitchACC 47.2¬±0.12 48.3¬±0.44 44.7¬±0.0125 48.2¬±0.22 52.2¬±0.34 49.9¬±0.47 51.9¬±0.16 52.3¬±0.11 53.6¬±0.1657.7¬±0.19
AUROC 50.9¬±0.08 51.1¬±0.19 51.3¬±0.24 50.7¬±0.16 50.9¬±0.15 51.2¬±0.16 51.1¬±0.17 50.9¬±0.16 51.6¬±0.1551.7¬±0.12
MacroF 47.6¬±0.14 46.7¬±0.22 44.9¬±0.46 46.3¬±0.37 48.3¬±0.26 47.1¬±0.31 50.2¬±0.14 50.5¬±0.08 50.9¬±0.0751.5¬±0.08
YelpACC 74.3¬±0.12 74.6¬±0.16 74.2¬±0.09 74.1¬±0.13 74.7¬±0.15 75.9¬±0.17 76.8¬±0.18 78.6¬±0.25 76.2¬±0.1179.8¬±0.09
AUROC 94.1¬±0.17 93.5¬±0.13 93.6¬±0.11 93.0¬±0.09 93.7¬±0.04 94.1¬±0.15 94.4¬±0.12 94.9¬±0.14 94.2¬±0.0595.5¬±0.06
MacroF 74.4¬±0.05 74.8¬±0.12 74.0¬±0.08 75.3¬±0.09 74.1¬±0.13 76.5¬±0.16 77.8¬±0.15 77.6¬±0.19 77.3¬±0.0879.5¬±0.11
Cora_fullACC 29.4¬±0.12 30.3¬±0.12 31.5¬±0.21 31.9¬±0.16 25.4¬±0.14 32.2¬±0.17 31.9¬±0.15 33.3¬±0.07 34.2¬±0.1538.3¬±0.19
AUROC 78.6¬±0.17 80.1¬±0.14 80.4¬±0.19 80.8¬±0.08 77.9¬±0.07 81.0¬±0.13 81.1¬±0.11 81.5¬±0.04 81.9¬±0.2283.2¬±0.25
MacroF 19.7¬±0.09 19.9¬±0.16 20.7¬±0.15 21.5¬±0.10 16.2¬±0.13 21.5¬±0.14 21.8¬±0.16 24.1¬±0.12 23.5¬±0.1326.4¬±0.08
ArxivACC 52.7¬±0.14 52.6¬±0.19 52.8¬±0.14 53.1¬±0.16 53.3¬±0.21 53.4¬±0.17 53.5¬±0.21 53.8¬±0.18 54.2¬±0.1955.4¬±0.09
AUROC 91.5¬±0.09 91.6¬±0.13 90.9¬±0.13 91.6¬±0.17 91.9¬±0.10 91.8¬±0.16 92.0¬±0.15 92.2¬±0.14 91.8¬±0.1892.4¬±0.06
MacroF 27.4¬±0.21 27.1¬±0.18 28.9¬±0.08 28.6¬±0.14 27.5¬±0.25 27.9¬±0.18 29.2¬±0.17 30.1¬±0.16 30.4¬±0.2531.2¬±0.17
5.2 Experiment Settings
5.2.1 Baselines. The proposed method is compared with represen-
tative and state-of-the-art single-source adaptation approaches and
multi-source adaptation methods. For empirical comparisons, we
use the following baselines: Direct, MMD [69],Reverse [25],Ad-
versarial [58] [63],OptimalT [15],UDA-GCN [63],DistMDA [55],
MDAN [74], and MLDG [37]. Details are provided in Appendix B.
5.2.2 Configurations. In the experiments, for each dataset, we fix
the first 5graphs as source domains and use the last one as the target
domain, and label 10%of nodes for training. All methods share the
same backbone network structure of a two-layer GCN [ 36] and are
trained until convergence with the maximum epoch number set
to2,000. For meta-learning, the maximum epoch number of the
outer update is also set to 2,000and the inner update step Tis set
to5following [ 24]. The Adam optimizer is adopted for all methods,
with the learning rate initialized to 0.01 and weight decay as 5e-4.
For a fair comparison, for all baselines, we use grid search to set
their hyperparameters. In SelMAG, the hyper-parameter ùúÜis set to
0.3andùúáis set to 0.001if not stated otherwise.
5.2.3 Evaluation Metrics. We evaluate the adapted model on the
target graph and adopt widely used evaluation metrics for node
classification, including accuracy (ACC), macro AUC-ROC (AU-
ROC) [ 9], and macro F1 (MacroF). Both MacroAUC and MacroF are
calculated and averaged per class, hence are more indicative of the
existence of class imbalance.
5.3 Graph Adaptation Performance
To answer RQ1, we evaluate the performance of different methods
after adapting to the target domain on all five datasets. Each experi-
ment is conducted for 5times with random parameter initialization
and train/test splits of source domains, and we report the average
results in Table 1. Both the mean and standard deviation w.r.t each
metric are presented, with the best performance emboldened. From
the results, we can make the following observations:‚Ä¢Our proposed method, SelMAG, achieves the best performance
across all these datasets and outperforms baselines with a clear
margin. For example, on dataset Citation and Twitch, it shows an
improvement of 1.2%and4.1%in terms of accuracy compared to
the best baseline.
‚Ä¢Adaptation by taking all source domains as equal and neglecting
their varying transferability to the target domain often shows
weak performances, and may even result in a performance drop
compared to no adaptation at all. For example, on dataset Citation,
all single-source domain alignment methods show a performance
drop in both accuracy and Macro F score compared to the baseline
Direct, which trains a global source model and directly applies it
to the target domain.
‚Ä¢Compared to other multi-source adaptation methods like Dist-
MDA, SelMAGachieves further improvement, which supports
our motivation to capture transferability between graph domains
to cope with the highly diverse topology structure.
These results validate the effectiveness of SelMAG, showing its
strong performance in the multi-source graph adaptation task.
5.4 Ablation Study
In order to examine the importance of each component and an-
swer RQ2, in this subsection, we conduct a set of ablation studies
by removing different parts of SelMAG and test the performance.
All hyper-parameters are left unchanged, and each experiment is
conducted three times on datasets Twitch, Yelp, and Cora_full with
random initialization and train/test splits of source domains. Mean
performance and standard deviations are summarized in Table 2.
5.4.1 Source-Graph Selectors. To evaluate the importance of differ-
ent weights assigned to source domains, we test the performance
after removing model-based graph selector (w/o ùëîglobal
sel) and sub-
graph node selector (w/o ùëîlocal
sel). These two variants correspond to
removingùë†global andùë†localin Eq. 13, respectively. From Table 2, it
4485KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang
Table 2: Ablation Study on each framework component.
Twitch Yelp Cora_full
Methods ACC AUROC MacroF ACC AUROC MacroF ACC AUROC MacroF
SelMAG 57.7¬±0.1951.7¬±0.1251.5¬±0.0879.8¬±0.0995.5¬±0.0679.5¬±0.1138.3¬±0.1983.2¬±0.2526.4¬±0.08
w/oùëîglobal
sel54.3¬±0.13 51.3¬±0.15 50.9¬±0.09 77.0¬±0.11 93.6¬±0.08 77.5¬±0.14 35.5¬±0.25 82.8¬±0.22 25.0¬±0.16
w/oùëîlocal
sel55.2¬±0.16 51.5¬±0.14 51.0¬±0.12 77.7¬±0.12 94.7¬±0.13 77.8¬±0.12 35.7¬±0.31 82.6¬±0.13 25.4¬±0.28
w/o KD 53.7¬±0.16 51.4¬±0.15 51.1¬±0.10 78.9¬±0.10 95.1¬±0.11 78.1¬±0.16 36.5¬±0.23 83.0¬±0.19 25.7¬±0.13
SelAdv 55.3¬±0.13 51.6¬±0.25 51.4¬±0.19 79.5¬±0.13 95.2¬±0.12 79.2¬±0.09 36.8¬±0.22 83.1¬±0.09 25.9¬±0.29
0.2 0.4 0.6 0.8
0.7850.7900.795score
ACC
F1
(a) Yelp
0.2 0.4 0.6 0.8
0.520.540.560.58score
ACC
F1 (b) Twitch
Figure 3: Influence of the weight of domain alignment loss.
is shown that both selectors play a positive influence in this multi-
source adaptation. Particularly, the model-based graph selector is
more important than the sub-graph node selector in most cases,
with the variant w/o ùëîglobal
selshowing the worst accuracy in all three
datasets. The reason could lie in the utilization of graph modeling
tasks which provides clearer evidence for estimating transferability.
5.4.2 Domain Alignment Objectives. To evaluate the importance
of aligning the classification function, we implement a variant by
removing the weighted knowledge distillation loss LKDin Eq. 17
and annotate it as ‚Äúw/o KD‚Äù. To test the generalizability of SelMAG
with different domain alignment objectives, we further implement
a variant SelAdv by replacing OT-based alignment in LSelOT with
adversary-based alignment [ 74]. Learned weights of source do-
mains can be incorporated into the adversarial aligning process via
re-weighting. From Table 2, it can be observed that the weighted
knowledge distillation loss is also helpful for the adaptation process,
with ‚Äúw/o KD‚Äù showing a drop in adaptation performance. Besides,
it is shown that our method can also work for the adversarial align-
ment with a moderate performance drop, validating its robustness
across alignment strategies.
5.5 Sensitivity Analysis
5.5.1 Alignment Weight. To evaluate the domain adaptation objec-
tive and partially answer RQ2, we analyze the sensitivity of the Sel-
MAG on hyper-parameters ùúÜ, which controls the balance between
the embedding alignment loss LSelOT and the knowledge distil-
lation lossLKDin Eq. 17. We vary it as {0.1,0.2,0.3,0.5,0.6,0.8},
and experiments are conducted on Twitch and Yelp, with other
configurations remaining the same as the main experiment. Each
experiment is conducted 3times, and we report the average accu-
racy and Macro F score in Fig. 3. It is shown that ùúÜis better set
to the range[0.3,0.6]for both datasets. Setting it too small would
weaken the embedding alignment signal while setting it too high
would fail to explicitly align the classifier.
103
101
0.79000.79250.79500.7975score
ACC
F1(a) Yelp
103
101
0.520.540.56score
ACC
F1 (b) Twitch
Figure 4: Influence of the hyper-parameter ùúñ, which controls
optimal transport in Eq. 14.
5.5.2 Optimal Transport Configuration. To analyze the sensitivity
of SelMAG on the optimal transport configuration, in this part
we varyùúñas{0.0001,0.001,0.01,0.1,0.2}which regularizes the
transport in Eq. 14. A larger ùúñencourages the transport plan to be
smoother [ 43]. Again, experiments are conducted on Twitch and
Yelp for 3times with all other configurations unchanged. The aver-
age results are presented in Fig 4. It is shown that SelMAG achieves
relatively stable performance with a small ùúñwithin[0,0.1], in ac-
cordance with previous observations [26].
5.6 Analysis on Global and Local Weights
In this subsection, we provide some analysis over learned weights
ùë†global andùë†local of source graph domains towards the target graph
in order to answer RQ3. However, there is no ground-truth trans-
ferability between graph pairs w.r.t the downstream node classifica-
tion task. Therefore, we take a surrogate strategy by evaluating the
single-source graph adaptation performance. Single-source graph
adaptation is conducted by taking only one source graph as avail-
able and using optimal transport for embedding alignment. The
accuracy obtained on the target domain after adaptation is reported
as DA-ACC. Experiments are conducted on Yelp and Citation, with
optimization hyper-parameters set to be the same as introduced in
Sec. 5.2.2. For ease of analysis, we apply only the graph selector or
the sub-graph node selector respectively to exclude the influence
of the other one. Results are summarized in Fig. 5. For ùë†local, we
report its mean across nodes of the corresponding domain.
From Fig. 5, we can observe that SelMAG tends to assign a higher
weight to source domains that show a strong performance in single-
source UDA. For example on Yelp, the largest global and local
weights are both assigned to 4-th graph, which also shows the
strongest UDA performance. On Citation, the 5-th source graph is
generated from the same dataset as Gùë°, and its weights are also high.
For important source domains, their average local node weights are
also high. These results validate the ability of SelMAG in estimating
transferability between graph domains.
4486Multi-source Unsupervised Domain Adaptation on Graphs with Transferability Modeling KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
1 2 3 4 5
Source graph0.650.700.750.80DA-ACC
0.00.20.4
WeightsWeightG
WeightL
(a) Yelp
1 2 3 4 5
Source graph0.3500.3750.4000.4250.450DA-ACC
0.00.20.4
WeightsWeightG
WeightL (b) Citation
Figure 5: Analysis on weights assigned to source domains by
ùëîglobal
selandùëîlocal
sel, by comparing them to the accuracy obtained
with single-source domain adaptation.
6 Conclusion
In this work, we propose a novel framework to identify informative
sub-graphs for knowledge transfer in the multi-source graph adap-
tation task. We depict graph similarities from three perspectives,
each captured by a self-supervised graph modeling task, and esti-
mate task-specific cross-domain transferability with a meta-learned
selector. An optimal-transport objective and a weighted knowledge
distillation objective are designed to incorporate obtained selec-
tion scores into the domain alignment process. Experiments on
five datasets show that SelMAG outperforms existing methods for
MSDA on graphs. In the future, we plan to extend SelMAG to work
with emerging new classes. In adapting source models to the target
domain, there could be shifts in the label space, like novel classes
unseen during training. This scenario calls for the development of
a new algorithm.
7 Acknowledgement
This material is based upon work supported by, or in part by the
National Science Foundation (NSF) under grant number IIS-1909702,
Army Research Office (ARO) under grant number W911NF-21-1-
0198, Department of Homeland Security (DHS) CINA under grant
number E205949D, and Cisco Faculty Research Award.
References
[1]Amir Hosein Khas Ahmadi. 2020. Memory-based graph networks. Ph. D. Disserta-
tion. University of Toronto (Canada).
[2]James Atwood and Don Towsley. 2016. Diffusion-convolutional neural networks.
InAdvances in neural information processing systems. 1993‚Äì2001.
[3]Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
Jennifer Wortman Vaughan. 2010. A theory of learning from different domains.
Machine learning 79, 1 (2010), 151‚Äì175.
[4]Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2006. Anal-
ysis of representations for domain adaptation. Advances in neural information
processing systems 19 (2006).
[5]John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man. 2007. Learning bounds for domain adaptation. Advances in neural informa-
tion processing systems 20 (2007).
[6]Vladimir I Bogachev and Aleksandr V Kolesnikov. 2012. The Monge-Kantorovich
problem: achievements, connections, and perspectives. Russian Mathematical
Surveys 67, 5 (2012), 785.
[7]Aleksandar Bojchevski and Stephan G√ºnnemann. 2018. Deep Gaussian Embed-
ding of Graphs: Unsupervised Inductive Learning via Ranking. In International
Conference on Learning Representations. 1‚Äì13.
[8]Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan,
and Dumitru Erhan. 2016. Domain separation networks. Advances in neural
information processing systems 29 (2016).
[9]Andrew P. Bradley. 1997. The use of the area under the ROC curve in the
evaluation of machine learning algorithms. Pattern Recognit. 30, 7 (1997), 1145‚Äì
1159. https://doi.org/10.1016/S0031-3203(96)00142-2
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877‚Äì1901.[11] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral net-
works and locally connected networks on graphs. arXiv preprint arXiv:1312.6203
(2013).
[12] Rita Chattopadhyay, Qian Sun, Wei Fan, Ian Davidson, Sethuraman Panchanathan,
and Jieping Ye. 2012. Multisource domain adaptation and its application to early
detection of fatigue. ACM Trans. Knowl. Discov. Data 6, 4 (2012), 18:1‚Äì18:26.
https://doi.org/10.1145/2382577.2382582
[13] Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong. 2009. Extracting discrimi-
native concepts for domain adaptation in text mining. In Proceedings of the 15th
ACM SIGKDD international conference on Knowledge discovery and data mining.
179‚Äì188.
[14] Dong Chen, Hongqing Zhu, Suyi Yang, and Yiwen Dai. 2023. Unsupervised multi-
source domain adaptation with graph convolution network and multi-alignment
in mixed latent space. Signal, Image and Video Processing 17, 3 (2023), 855‚Äì863.
[15] Nicolas Courty, R√©mi Flamary, Amaury Habrard, and Alain Rakotoma-
monjy. 2017. Joint distribution optimal transportation for domain adapta-
tion. In Advances in Neural Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Ben-
gio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (Eds.). 3730‚Äì3739. https://proceedings.neurips.cc/paper/2017/hash/
0070d23b06b1486a538c0eaa45dd167a-Abstract.html
[16] Koby Crammer, Michael Kearns, and Jennifer Wortman. 2008. Learning from
Multiple Sources. Journal of Machine Learning Research 9, 8 (2008).
[17] Marco Cuturi. 2013. Sinkhorn distances: Lightspeed computation of optimal
transport. Advances in neural information processing systems 26 (2013).
[18] Enyan Dai, Wei Jin, Hui Liu, and Suhang Wang. 2022. Towards Robust Graph Neu-
ral Networks for Noisy Graphs with Sparse Labels. arXiv preprint arXiv:2201.00232
(2022).
[19] Enyan Dai and Suhang Wang. 2021. Towards self-explainable graph neural
network. In Proceedings of the 30th ACM International Conference on Information
& Knowledge Management. 302‚Äì311.
[20] Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Junjie Xu, Zhimeng Guo, Hui Liu,
Jiliang Tang, and Suhang Wang. 2022. A Comprehensive Survey on Trustworthy
Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability. arXiv
preprint arXiv:2204.08570 (2022).
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[22] Wenqi Fan, Y. Ma, Qing Li, Yuan He, Y. Zhao, Jiliang Tang, and D. Yin. 2019. Graph
Neural Networks for Social Recommendation. The World Wide Web Conference
(2019).
[23] Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. 2013. Un-
supervised Visual Domain Adaptation Using Subspace Alignment. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV).
[24] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-
learning for fast adaptation of deep networks. In International conference on
machine learning. PMLR, 1126‚Äì1135.
[25] Yaroslav Ganin and Victor S. Lempitsky. 2015. Unsupervised Domain Adap-
tation by Backpropagation. In Proceedings of the 32nd International Conference
on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 (JMLR Workshop
and Conference Proceedings, Vol. 37), Francis R. Bach and David M. Blei (Eds.).
JMLR.org, 1180‚Äì1189. http://proceedings.mlr.press/v37/ganin15.html
[26] Aude Genevay, Marco Cuturi, Gabriel Peyr√©, and Francis Bach. 2016. Stochastic
optimization for large-scale optimal transport. Advances in neural information
processing systems 29 (2016).
[27] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and
Wen Li. 2016. Deep reconstruction-classification networks for unsupervised
domain adaptation. In Computer Vision‚ÄìECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11‚Äì14, 2016, Proceedings, Part IV 14. Springer,
597‚Äì613.
[28] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural Message Passing for Quantum Chemistry. In ICML.
[29] Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. [n. d.]. GOOD: A Graph
Out-of-Distribution Benchmark. In Thirty-sixth Conference on Neural Information
Processing Systems Datasets and Benchmarks Track.
[30] William L. Hamilton, Zhitao Ying, and J. Leskovec. 2017. Inductive Representation
Learning on Large Graphs. In NIPS.
[31] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for
machine learning on graphs. Advances in neural information processing systems
33 (2020), 22118‚Äì22133.
[32] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,
and Jure Leskovec. 2019. Strategies for Pre-training Graph Neural Networks. In
International Conference on Learning Representations.
[33] Srikanth Jagabathula, Dmitry Mitrofanov, and Gustavo Vulcano. 2022. Personal-
ized retail promotions through a directed acyclic graph‚Äìbased representation of
customer preferences. Operations Research 70, 2 (2022), 641‚Äì665.
4487KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang
[34] Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang
Tang. 2020. Self-supervised learning on graphs: Deep insights and new direction.
arXiv preprint arXiv:2006.10141 (2020).
[35] Leonid V Kantorovich. 2006. On the translocation of masses. Journal of mathe-
matical sciences 133, 4 (2006), 1381‚Äì1382.
[36] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. 2018. Learning to
generalize: Meta-learning for domain generalization. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 32.
[38] Yunsheng Li, Lu Yuan, Yinpeng Chen, Pei Wang, and Nuno Vasconcelos. 2021.
Dynamic transfer for multi-source domain adaptation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10998‚Äì11007.
[39] Shuai Lin, Chen Liu, Pan Zhou, Zi-Yuan Hu, Shuojia Wang, Ruihui Zhao, Yefeng
Zheng, Liang Lin, Eric Xing, and Xiaodan Liang. 2022. Prototypical graph con-
trastive learning. IEEE Transactions on Neural Networks and Learning Systems
(2022).
[40] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. 2015. Learning
transferable features with deep adaptation networks. In International conference
on machine learning. PMLR, 97‚Äì105.
[41] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. 2008. Domain
adaptation with multiple sources. Advances in neural information processing
systems 21 (2008).
[42] Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and
Dongmei Zhang. 2021. Source free unsupervised graph domain adaptation. arXiv
preprint arXiv:2112.00955 (2021).
[43] Tuan Nguyen, Trung Le, He Zhao, Quan Hung Tran, Truyen Nguyen, and Dinh
Phung. 2021. Most: Multi-source domain adaptation via optimal transport for
student-teacher learning. In Uncertainty in Artificial Intelligence. PMLR, 225‚Äì235.
[44] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang.
2019. Moment matching for multi-source domain adaptation. In Proceedings of
the IEEE/CVF international conference on computer vision. 1406‚Äì1415.
[45] Jeffrey Pennington, R. Socher, and Christopher D. Manning. 2014. Glove: Global
Vectors for Word Representation. In EMNLP.
[46] Devakunchari Ramalingam and Valliyammai Chinnaiah. 2018. Fake profile
detection techniques in large-scale online social networks: A comprehensive
review. Computers & Electrical Engineering 65 (2018), 165‚Äì177.
[47] Ievgen Redko, Nicolas Courty, R√©mi Flamary, and Devis Tuia. 2019. Optimal
transport for multi-source domain adaptation under target shift. In The 22nd
International Conference on Artificial Intelligence and Statistics . PMLR, 849‚Äì858.
[48] Weijieying Ren and Vasant G Honavar. 2024. EsaCL: An Efficient Continual
Learning Algorithm. In Proceedings of the 2024 SIAM International Conference on
Data Mining (SDM). SIAM, 163‚Äì171.
[49] Weijieying Ren, Xiaoting Li, Huiyuan Chen, Vineeth Rakesh, Zhuoyi Wang,
Mahashweta Das, and Vasant G Honavar. [n. d.]. TabLog: Test-Time Adaptation
for Tabular Data Using Logic Rules. In Forty-first International Conference on
Machine Learning.
[50] Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, and Wei Qin. 2024. An-
alyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning.
arXiv preprint arXiv:2402.18865 (2024).
[51] Weijieying Ren, Lei Wang, Kunpeng Liu, Ruocheng Guo, Lim Ee Peng, and
Yanjie Fu. 2022. Mitigating Popularity Bias in Recommendation with Unbalanced
Interactions: A Gradient Perspective. arXiv preprint arXiv:2211.01154 (2022).
[52] Weijieying Ren, Pengyang Wang, Xiaolin Li, Charles E Hughes, and Yanjie Fu.
2022. Semi-supervised Drifted Stream Learning with Short Lookback. arXiv
preprint arXiv:2205.13066 (2022).
[53] Weijieying Ren, Tianxiang Zhao, Wei Qin, and Kunpeng Liu. 2023. T-SaS: Toward
Shift-aware Dynamic Adaptation for Streaming Data. In Proceedings of the 32nd
ACM International Conference on Information and Knowledge Management. 4244‚Äì
4248.
[54] Benedek Rozemberczki and Rik Sarkar. 2021. Twitch Gamers: a Dataset for
Evaluating Proximity Preserving and Structural Role-based Node Embeddings.
arXiv:2101.03091 [cs.SI]
[55] Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, and Jieping Ye. 2011.
A two-stage weighting framework for multi-source domain adaptation. Advances
in neural information processing systems 24 (2011).
[56] Shiliang Sun, Honglei Shi, and Yuanbin Wu. 2015. A survey of multi-source
domain adaptation. Information Fusion 24 (2015), 84‚Äì92.
[57] S. Tang, Bo Li, and Haijun Yu. 2019. ChebNet: Efficient and Stable Construc-
tions of Deep Neural Networks with Rectified Power Units using ChebyshevApproximations. ArXiv abs/1911.05467 (2019).
[58] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. 2017. Adversarial
Discriminative Domain Adaptation. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. IEEE
Computer Society, 2962‚Äì2971. https://doi.org/10.1109/CVPR.2017.316
[59] Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[60] Garrett Wilson and Diane J Cook. 2020. A survey of unsupervised deep domain
adaptation. ACM Transactions on Intelligent Systems and Technology (TIST) 11, 5
(2020), 1‚Äì46.
[61] Bingzhe Wu, Jintang Li, Junchi Yu, Yatao Bian, Hengtong Zhang, CHaochao Chen,
Chengbin Hou, Guoji Fu, Liang Chen, Tingyang Xu, et al .2022. A Survey of
Trustworthy Graph Learning: Reliability, Explainability, and Privacy Protection.
arXiv preprint arXiv:2205.10014 (2022).
[62] Jun Wu, Jingrui He, and Elizabeth Ainsworth. 2022. Non-IID Transfer Learning
on Graphs. arXiv preprint arXiv:2212.08174 (2022).
[63] Man Wu, Shirui Pan, Chuan Zhou, Xiaojun Chang, and Xingquan Zhu. 2020.
Unsupervised Domain Adaptive Graph Convolutional Networks. In WWW ‚Äô20:
The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, Yennun Huang, Irwin
King, Tie-Yan Liu, and Maarten van Steen (Eds.). ACM / IW3C2, 1457‚Äì1467.
https://doi.org/10.1145/3366423.3380219
[64] Jiaren Xiao, Quanyu Dai, Xiaochen Xie, Qi Dou, Ka-Wai Kwok, and James Lam.
2022. Domain adaptive graph infomax via conditional adversarial networks. IEEE
Transactions on Network Science and Engineering 10, 1 (2022), 35‚Äì52.
[65] Teng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, and Suhang Wang.
2022. Decoupled Self-supervised Learning for Non-Homophilous Graphs. arXiv
preprint arXiv:2206.03601 (2022).
[66] Teng Xiao, Zhengyu Chen, Donglin Wang, and Suhang Wang. 2021. Learning
how to propagate messages in graph neural networks. In Proceedings of the 27th
ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 1894‚Äì1903.
[67] Junjie Xu, Enyan Dai, Xiang Zhang, and Suhang Wang. 2022. HP-GMN: Graph
Memory Networks for Heterophilous Graphs. arXiv preprint arXiv:2210.08195
(2022).
[68] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[69] Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, and Wangmeng
Zuo. 2017. Mind the Class Weight Bias: Weighted Maximum Mean Discrepancy
for Unsupervised Domain Adaptation. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017.
IEEE Computer Society, 945‚Äì954. https://doi.org/10.1109/CVPR.2017.107
[70] Luyu Yang, Yogesh Balaji, Ser-Nam Lim, and Abhinav Shrivastava. 2020. Cur-
riculum manager for source selection in multi-source domain adaptation. In
Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August
23‚Äì28, 2020, Proceedings, Part XIV 16. Springer, 608‚Äì624.
[71] Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang. 2020. Factorizable
graph convolutional networks. Advances in Neural Information Processing Systems
33 (2020), 20286‚Äì20296.
[72] Yaping Zhang, Shuai Nie, Wenju Liu, Xing Xu, Dongxiang Zhang, and Heng Tao
Shen. 2019. Sequence-to-sequence domain adaptation network for robust text
image recognition. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. 2740‚Äì2749.
[73] Yizhou Zhang, Guojie Song, Lun Du, Shuwen Yang, and Yilun Jin. 2019. Dane:
Domain adaptive network embedding. arXiv preprint arXiv:1906.00684 (2019).
[74] Han Zhao, Shanghang Zhang, Guanhang Wu, Jos√© MF Moura, Joao P Costeira,
and Geoffrey J Gordon. 2018. Adversarial multiple source domain adaptation.
Advances in neural information processing systems 31 (2018).
[75] Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang. 2022. On Con-
sistency in Graph Neural Network Interpretation. arXiv preprint arXiv:2205.13733
(2022).
[76] Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang. 2023. Faithful
and Consistent Graph Neural Network Explanations with Rationale Alignment.
arXiv preprint arXiv:2301.02791 (2023).
[77] Tianxiang Zhao, Xiang Zhang, and Suhang Wang. 2022. Exploring edge disen-
tanglement for node classification. In Proceedings of the ACM Web Conference
2022. 1028‚Äì1036.
[78] Tianxiang Zhao, Xiang Zhang, and Suhang Wang. 2024. Disambiguated Node
Classification with Graph Neural Networks. arXiv preprint arXiv:2402.08824
(2024).
4488Multi-source Unsupervised Domain Adaptation on Graphs with Transferability Modeling KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
A Dataset Description
‚Ä¢Citation. In this dataset, we use three citation networks collected
from ACM (ACM-V9), DBLP (DBLP-V7), and Microsoft Academic
Graph (Citation-V1) respectively. Each node represents a paper
and its descriptions are extracted as attributes using Bog-of-words.
Edges denote citations, and nodes are labeled on the paper domain.
We randomly split each network into two graphs and create a
dataset of 6graphs, which can increase the graph number and
provide us with some prior knowledge on the similarity between
graph pairs at the same time. This design can also help evaluate
the effectiveness of our framework in selecting source domains.
A sub-graph of Citation-V1 is used as the target.
‚Ä¢Twitch. This dataset is collected from the Twitch gamer platform,
with nodes as Twitch users and edges as mutual follower relation-
ships between them. This binary node classification task predicts
whether a user streams explicit content. Six graphs are obtained
based on the language used by the user: French, Spanish, Por-
tuguese, German, English or Russian. The graph with language
Russian is used as the target graph.
‚Ä¢Yelp . This dataset contains user reviews on Yelp to various point-
of-interests (POIs) in different cities. We transform the reviews
in each city into a graph, with each node representing a POI and
each edge representing a co-review relationship. POI features
are obtained by averaging the word embedding of its reviews,
which is taken from the pre-trained language model GLOVE [ 45].
We perform classification on five classes, {Food, Shop, Home Ser-
vice, Health Service, Finance }, and select six cities of different
scales: {Madison, Glendale, Gilbert, Las Vegas, Toronto, Phoenix }.
City Phoenix is used as the target graph.
‚Ä¢Cora_full. It is a citation network with nodes for papers and
edges for citation relations. We cluster papers into 6groups
based on different frequencies in selected-words usage following
GOOD [29] to generate 6graphs and select one as the target.
‚Ä¢Arxiv. It is a large citation network among the computer science
(CS) arXiv papers. The task is to predict the subject area of each
paper. We split it into 6disjoint graphs based on the published
year of each paper [29], and use the most recent one as Gùë°.
The main statistics of these datasets are summarized in Table 3,
including average node numbers and average edge numbers among
graphs of each dataset.
Table 3: Statistics of processed graph datasets.
Name #Graphs #Nodes #Edges #Attributes #Classes
Citation 6 3,963 6,428 6,775 5
Twitch 6 5,687 148,724 128 2
Yelp 6 13,903 232,525 100 6
Cora_full 6 3,298 3,697 8,710 70
Arxiv 6 28,223 66,166 128 40
B Baseline Introduction
‚Ä¢Direct. Instead of conducting explicit alignment in this baseline,
we train a global model by using all the source domains and
directly apply it to the target graph.
‚Ä¢MMD [69]. This baseline conducts cross-domain alignment by
minimizing the maximum mean discrepancy (MMD) of embed-
ding distributions between the source and target graphs.‚Ä¢Reverse [25]. By augmenting feed-forward models with a simple
new gradient reversal layer, this baseline can encourage discov-
ering features that are not predictive towards domains.
‚Ä¢Adversarial [58] [63]. To capture generalizable features that are
invariant across domains, this baseline adversarially trains a do-
main discriminator to distinguish the source and target domains.
Model is learned to extract features that are both discriminative
towards node labels and can fool the domain discriminator.
‚Ä¢OptimalT [15]. This baseline assumes a non-linear transforma-
tion between the joint feature/label space distributions across
domains, and the model is adapted by minimizing this total trans-
formation cost.
‚Ä¢UDA-GCN [63]: This method aligns source and target graphs
with a domain classifier and includes classification entropy to
promote classification boundaries of the target domain.
‚Ä¢DistMDA [55]. This method is designed for multi-source unsuper-
vised domain adaptation, which makes a smoothness assumption
on data distribution and estimates the weight of each source do-
main by minimizing the marginal probability difference. After
obtaining source weights, Optimal transport is used for alignment.
We extend it to graph-structured data by computing probability
differences in the embedding space.
‚Ä¢MDAN [74]. A worst-case selection strategy is used in this work
for multi-source unsupervised domain adaptation. We use its
Soft-Max version to assign weights and then conduct OT-based
adaptation.
‚Ä¢MLDG [37]. Meta-learning with synthesized virtual testing do-
mains is utilized in this method in order to explicitly learn to
generalize to the unseen domain with multiple source domains
available. We implement it in the same setting as our SelMAG,
with MAML [24] for the conduction of meta-learning part.
Note that MMD, Reverse, Adversarial, OptimalT, and UDA-GCN are
originally designed for single-source domain adaptation. We extend
them to MSUDA by taking all source domains as one, equivalent to
giving the same weights to all source graphs.
C Time Complexity
In this part, we provide some analysis over the additional computa-
tion cost of our method. Note that during model adaptation process,
at each batch, both baselines and our method have the same back-
bone GNN architectures and the domain distance estimator for
alignment. In addition to these backbone modules, our algorithm
needs an additional source domain selector. Denoting number of
GNN layers as ùêø, number of edges in the dataset as ùê∏, number of
nodes asùëõand embedding dimension as ùëë, for the backbone model,
time complexity of GNN is ùëÇ(ùêøùê∏ùëë+ùêøùëõùëë2)and time complexity of
distance estimator is ùëÇ(ùëõùëë2). For our model, the additional domain
selector has the time complexity of ùëÇ(ùëõùëë2). Therefore, the proposed
ùëÜùëíùëôùëÄùê¥ùê∫ will not result in a significant computation increase in
adaptation. During learning, the time-consuming part of our model
is to pretrain the graph modeling tasks. For a concrete example, on
dataset Arxiv, which has 169,338 nodes, the pretraining step takes
around one hour. Note that this pretraining step is only needed once
across different running, hence its cost would not pose a severe
problem.
4489