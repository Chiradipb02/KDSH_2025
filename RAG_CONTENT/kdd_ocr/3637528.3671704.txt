An Unsupervised Learning Framework Combined with Heuristics
for the Maximum Minimal Cut Problem
Huaiyuan Liu
Harbin Institute of Technology
Harbin, China
hyliu@hit.edu.cnXianzhang Liu
Harbin Institute of Technology
Harbin, China
liuxz_hit@foxmail.comDonghua Yang
Harbin Institute of Technology
Harbin, China
yang.dh@hit.edu.cn
Hongzhi Wang∗
Harbin Institute of Technology
Harbin, China
wangzh@hit.edu.cnYinchi Long
Harbin Institute of Technology
Harbin, China
i@lyc.devMengtong Ji
Harbin Institute of Technology
Harbin, China
2512508310@qq.com
Dongjing Miao
Harbin Institute of Technology
Harbin, China
miaodongjing@hit.edu.cnZhiyu Liang
Harbin Institute of Technology
Harbin, China
zyliang@hit.edu.cn
Abstract
The Maximum Minimal Cut Problem (MMCP), a NP-hard combina-
torial optimization (CO) problem, has not received much attention
due to the demanding and challenging bi-connectivity constraint.
Moreover, as a CO problem, it is also a daunting task for machine
learning, especially without labeled instances. To deal with these
problems, this work proposes an unsupervised learning framework
combined with heuristics for MMCP that can provide valid and
high-quality solutions. As far as we know, this is the first work that
explores machine learning and heuristics to solve MMCP. The unsu-
pervised solver is inspired by a relaxation-plus-rounding approach,
the relaxed solution is parameterized by graph neural networks,
and the cost and penalty of MMCP are explicitly written out, which
can train the model end-to-end. A crucial observation is that each
solution corresponds to at least one spanning tree. Based on this
finding, a heuristic solver that implements tree transformations
by adding vertices is utilized to repair and improve the solution
quality of the unsupervised solver. Alternatively, the graph is sim-
plified while guaranteeing solution consistency, which reduces the
running time. We conduct extensive experiments to evaluate our
framework and give a specific application. The results demonstrate
the superiority of our method against two techniques designed.
CCS Concepts
•Mathematics of computing →Combinatorial optimization;
•Computing methodologies →Unsupervised learning.
∗Hongzhi Wang is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671704Keywords
combinatorial optimization, maximum minimal cut problem, unsu-
pervised learning, heuristics
ACM Reference Format:
Huaiyuan Liu, Xianzhang Liu, Donghua Yang, Hongzhi Wang, Yinchi Long,
Mengtong Ji, Dongjing Miao, and Zhiyu Liang. 2024. An Unsupervised Learn-
ing Framework Combined with Heuristics for the Maximum Minimal Cut
Problem. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671704
1 Introduction
The graph is a fundamental structure used to depict diverse relation-
ships among entities [ 14,41], such as social networks, communica-
tion networks, biological information, and power grids. Combinato-
rial optimization (CO) on graphs is a crucial field in computational
mathematics, with typical examples including the Max-cut [ 34],
Maximum Independent Set (MIS) [ 8], and Traveling Salesman Prob-
lem (TSP) [ 11] applied in various scenarios. The exact solutions
for most combinatorial optimization problems are intractable to
search, primarily due to their NP-hard nature. Consequently, sig-
nificant research efforts have been dedicated to solving these tasks
by generating approximate solutions [17, 31].
The Max-cut problem, a classic NP-hard problem in graph the-
ory, has been extensively studied for decades, while the Maximum
Minimal Cut (MMC) problem as a variant of the max-cut problem
is rarely mentioned. Given a connected graph 𝐺=(𝑉,𝐸), the objec-
tive of the max-cut problem is to discover a partition method that
divides all vertices into two complementary sets, denoted as 𝐾and
𝐿, in a way that maximizes the cardinality of edges between 𝐾and𝐿.
The maximum minimal cut problem (MMCP) adds the requirement
that both𝐾and𝐿be connected based on the max-cut problem, also
called the largest bond problem [ 28]. As a well-known fact, a cut
𝐶=(𝐾,𝐿)of𝐺is minimal if and only if both subgraphs induced by
𝐾and𝐿are connected [ 38], where𝐾,𝐿∈𝑉and𝐿=𝑉\𝐾. There-
fore, a minimal cut is regarded as a two-sided connected cut [ 18].
Additionally, the Connected Maximum Cut (CMC) problem is a
 
1921
KDD ’24, August 25–29, 2024, Barcelona, Spain Huaiyuan Liu et al.
variant of the max-cut problem that only demands the connectivity
of𝐾. For example, in Figure 1, {𝑣1,𝑣4},{𝑣1,𝑣2,𝑣7}and{𝑣1,𝑣4,𝑣6,𝑣7}
are max-cut, maximum minimal cut, and connected maximum cut
of𝐺, respectively. Note that the notable distinction among the three
problems lies in the connectivity of the two subsets after separation.
(a) Max-cut.
 (b) MMC.
 (c) CMC.
Figure 1: An example to illustrate the difference among max-
cut, maximum minimal cut, and connected maximum cut.
The Max-cut, as a fundamental problem of CO, can find appli-
cations in image segmentation [ 19,21], network analysis [ 9], and
statistical physics [ 10], etc., while the MMCP rarely applied in real-
world scenarios. Recently, we noticed the significant importance of
MMCP as a mathematical form of computation in identifying key
cross-sections for power grids [ 53], and we left the specifics for Sec-
tion 5.5. Inspired by this, the MMCP will play an indispensable role
in the monitoring and early warning of related complex networks.
Over the past decades, researchers have made significant efforts to
prove the complexity and design parameterized algorithms for this
issue [ 28,32]. However, no fast solver for MMCP has been released
so far, which is challenging due to the following reasons:
Special Constraint. Unsupervised learning is widely used as a pop-
ular paradigm for combinatorial optimization problems [1, 42, 47]
that obtains solutions by minimizing a differentiable loss function,
whose successful training hinges on empirically-identified correc-
tion terms [ 2,23]. However, no methods can effectively represent
the bi-connectivity constraints of the MMCP in an algebraic form
and cannot be trained through back-propagation. Alternatively, it
is exceptionally difficult to efficiently decode the relaxed solution of
a neural network to a discrete solution that satisfies the constraints
[30], especially without a fully labeled solution.
Lack of Equivalence. We claim that designing solvers for MMCP
is often of greater difficulty than traditional CO problems because a
general solver cannot be implemented for MMCP due to the lack of
closed-form equivalence of solutions. In other words, compressing
the solution space to a solution space that satisfies the constraints
is a formidable task. No study can equate the solutions of MMCP to
the solutions that disregard the constraints. For heuristics, it is more
efficient to explore a larger objective value in constraint-satisfying
solutions than to judge the legitimacy after obtaining the solutions.
Weak Universality. Connectivity is a feature strongly associated
with the graph structure, leading to the large obstacle of solving
MMCP uniformly for the graphs with different structures. Regret-
tably, most existing approaches to address MMCP are based on the
assumptions of graph structures, e.g., planar graph and simple 3-
connected graph [ 6,32]. Only graphs that satisfy specific structureconstraints are considered. Therefore, overcoming the effects of
graph structures to construct a unified framework is crucial.
Contributions. To tackle the aforementioned issues and fill the
gap, we design a novel unsupervised learn ing framew ork combi ned
with h euristics (PIONEER) and provide new ideas for solving the
maximum minimal cut problem in this work. In summary, the main
contributions of our work are summarized as follows:
•Pioneering. We propose PIONEER, to the best of our knowl-
edge, this is the first study on the unsupervised learning and
heuristics for the MMCP with arbitrary graph structure.
•Graph Simplification. We clarify that target cut-edges can
only be obtained on bridges or within connected components
formed after breaking all bridges (Theorem 1), which can
simplify the scale of the graph and reduce the running time.
•Guaranteed Unsupervised. The cost and penalty of MMCP
are explicitly written for the first time. An efficient unsu-
pervised learning pipeline with a performance guarantee is
designed to quickly find high-quality solutions that satisfy
the constraint (Theorem 3).
•Solution Forest. We prove that all feasible solutions can be
equivalently achieved by disconnecting an edge in one of the
spanning trees of the graph, i.e., each solution corresponds
to at least one spanning tree (Theorem 4).
•Novel Heuristics. A novel heuristic approach is proposed to
repair and improve the solutions of the unsupervised solver
that searches for better solutions by adding or removing
vertices to realize a transformation of the spanning tree.
2 Related Work
This section briefly reviews the related work on the maximum
minimal cut problem and the combinatorial optimization solvers.
Maximum Minimal Cut Problem (MMCP). Unlike general
CO problems, the study of MMCP is still in its infancy. As a NP-hard
issue, the optimal solution of MMCP cannot be approximated by
a constant factor in polynomial time unless P = NP [ 16]. Based on
this point, researchers have developed a wide range of complexity
analyses for the MMCP, while there exist rare results about the
approximate solvers of the MMCP in general graphs [ 6,40]. Notably,
Flynn et al. [ 32] showed that the size of the largest bond is at
leastΩ(𝑛log32)for any simple 3-connected graph 𝐺(𝑉,𝐸), where
𝑛=|𝑉|. Later, Duarte et al. [ 28] proved that the MMCP is NP-
complete even on planar bipartite graphs and split graphs. Moreover,
they gave aO∗(2O(𝑡𝑤log𝑡𝑤))-time algorithm for the MMCP from
the perspective of the parameterized complexity. Although these
methods have achieved breakthroughs in the computational theory
of MMCP, they are still limited in the application of the real scene.
Combinatorial Optimization (CO) Solver. Due to the high
time complexity of exact algorithms [ 36,39], approximation ap-
proaches have become the mainstream for solving CO problems
[4,12]. Heuristics are one of the most efficient and effective ap-
proaches for solving the CO problem which obtains a sub-optimal
solution within a reasonable time [ 22,49,51]. Unfortunately, heuris-
tics are problem-specific and time-consuming. With the blowout
of artificial intelligence, the approaches of neural networks for CO
emerge as the times require [ 29,35,45]. Most neural approaches to
CO are supervised, and such methods rely on training large-scale
 
1922An Unsupervised Learning Framework Combined with Heuristics for the Maximum Minimal Cut Problem KDD ’24, August 25–29, 2024, Barcelona, Spain
labeled instances [ 13,44]. In general, training unlabeled data is
more challenging, which leads to two technical routes, i.e. rein-
forcement learning (RL) [ 25,52] and unsupervised learning [ 26,33].
RL is known to be notoriously unstable to train. The works that
are more relevant to ours are those unsupervised learning frame-
works trained in an end-to-end manner. Wang et al. [ 42] adopt a
relaxation-plus-rounding mechanism based on [ 23], which guar-
antees performance by an entry-wise concave principle. However,
the inability to write the loss of MMCP and the particularity of the
connectivity prevents the application of the above methods.
3 Preliminaries
In this section, we describe the key concept applied in the paper.
Firstly, We give some notations that will be used.
Notation. Let𝐺(𝑉,𝐸,𝑊)be a weighted, undirected, connected
graph with 𝑛=|𝑉|vertices,𝑚=|𝐸|edges, and weights 𝑊=
(𝜔1,...,𝜔𝑚), where𝜔∈R+and𝑛,𝑚∈N∗. Unweighted graphs
can be regarded as weighted graphs with all edge weights equal
to1. There are two disjoint connected subgraphs 𝐾=(𝑉1,𝐸1)
and𝐿=(𝑉2,𝐸2)of𝐺,𝐾and𝐿can be connected by the edge set
𝐹=(𝑒1,...,𝑒𝑐)which has weight 𝑊𝐹=(𝜔′
1,...,𝜔′𝑐). Besides, ˆ𝑆and
𝑆denote relaxed and discrete solutions throughout the paper.
Then, the problem that we are addressing, i.e. the maximum
minimal cut problem (MMCP), is formulated.
Problem Formulation. The MMCP is to find a set of edges that
divides the connected graph into two connected subgraphs so that
the sum of the weight (or cardinality) on the edge set is the largest.
The formal definition is given as follows.
Definition 1 (The Maximum Minimal Cut Problem). If𝐾
and𝐿satisfy: 1)𝑉1∪𝑉2=𝑉and𝑉1∩𝑉2=∅; 2)𝐸1,𝐸2,𝐹⊂𝐸,
𝐸1∪𝐸2∪𝐹=𝐸and𝐸1∩𝐸2∩𝐹=∅; 3)𝑐∈N∗.
Then𝐶=(𝐾,𝐿)can be called a minimal cut of 𝐺, and𝐹is named
the cut-set of 𝐺. Alternatively, 𝐾and𝐿are known as the connected
cut. Each𝐶and𝐹corresponds uniquely, where the cut value of |𝐹|is
given byÍ𝑐
𝑖=0𝜔′
𝑖. The maximum minimal cut is the minimal cut 𝐶∗
with the largest cut value among all 𝐶of𝐺.
In the above definition, we can consider 𝐹or𝐶𝑣as the solution
of MMCP, where 𝐶𝑣=(𝑉1,𝑉2)is the vertex set form of 𝐶. We assert
that MMCP is more challenging than the Max-cut problem [7].
4 Methodology
In this section, the proposed framework, its components, and related
theories will be elaborated comprehensively. Moreover, a random
algorithm is presented as a baseline.
4.1 Overview
In summary, we achieve acceleration through graph partitioning
and the performance-guaranteed unsupervised solver, and further
repair and obtain higher-quality solutions by leveraging the heuris-
tic solver. We overview the proposed unsupervised learning frame-
work combined with heuristics (PIONEER) in Figure 2.
Given a connected graph 𝐺=(𝑉,𝐸)as input, the connected
components of 𝐺can be formed by removing all bridges of 𝐺, which
does not affect the final solution and also reduces the graph size
(see Section 4.2). The connected components with a larger numberof vertices are solved jointly utilizing the proposed unsupervised
solver (see Section 4.3) and heuristic solver (see Section 4.5), while
the solutions of smaller parts are obtained quickly by the exact
algorithm. It is worth noting that all the solutions satisfying the
constraints correspond to at least one spanning tree of 𝐺, which is
the design source of the proposed heuristic solver (see Section 4.4).
Specifically, given 𝐺, we construct connected subgraphs 𝐺𝑙and
𝐺𝑠with a large and small number of vertices respectively according
to graph partitioning (Theorem 1). The solution 𝑆𝑠of𝐺𝑠is obtained
by the brute force search algorithm whose cut value is 𝑊𝑠.
Following the paradigm of unsupervised learning [ 23,43], we
adopts a relaxation-plus-rounding approach. We optimize the loss
functionLto generate relaxed solutions ˆ𝑆𝑙∈ [0,1]𝑛, which is
followed by a deterministic rounding to transform the solution in
discrete space{0,1}𝑛. The issue is whether the discrete solution 𝑆𝑙
can be achieved with assurance. Our key observation is that such
success for MMCP essentially depends on how to explicitly write
the loss and valid rounding. Therefore, one of our contributions is to
expressly write the cost and constraint for MMCP. Furthermore, we
show that the unsupervised solver can produce valid and low-cost
solutions (Theorem 3) through reasonable rounding (Definition 2).
Despite the success of the proposed unsupervised solver on
MMCP, an open question is how to repair the illegal solutions and
further improve the quality of the solutions. Due to the fragile
connectivity of sparse graphs and the strong correlation between
connectivity and graph structure, relying only on network learning
may not always result in solutions that satisfy the constraint. An
important finding is that all feasible solutions correspond to at least
one spanning tree of graph 𝐺(Theorem 4). With this conclusion, we
utilize the results of the unsupervised solver to construct spanning
trees that are treated as the starting point of the heuristic solver for
further exploration, and then we have new 𝑆′
𝑙and𝑊′
𝑙. Finally, the
optimal solution 𝑆∗=arg max𝑆{𝑊′
𝑙1,...,𝑊′
𝑙𝑎,𝑊𝑠1,...,𝑊𝑠𝑏}with the
largest cut value is selected as the final result, where 𝑎and𝑏denote
the maximum number of large and small subgraphs, separately.
In the rest of this section, we elaborately state the key theory and
components of the proposed PIONEER, in order graph partitioning,
unsupervised combinatorial solver, solution forest, and heuristic tree
transformation. Then, we design a random tree search as a baseline.
4.2 Graph Partitioning
Considering that the time complexity of solving MMCP grows expo-
nentially with the graph size, we exploit the theorem presented in
this subsection to simplify the graph without affecting the solutions.
The bridge is a special edge in the undirected connected graph,
also called cut-edge. The removal of the bridge will increase the
number of connected components in the graph. Suppose that 𝐺1
and𝐺2are the connected components obtained by disconnecting
one bridge𝑒𝑏of𝐺, which satisfies 𝐺1∪𝐺2∪𝑒𝑏=𝐺,𝐺1∩𝐺2=∅.
In addition,𝐹,𝐹1, and𝐹2are the maximum minimal cut-set of 𝐺,𝐺 1,
and𝐺2, respectively. Then, we have the following theorem.
Theorem 1. The maximum minimal cut-set 𝐹of𝐺can only be
obtained on one of the two sides of the bridge or on the bridge, that is,
𝐹=𝑚𝑎𝑥{𝑒𝑏,𝐹1,𝐹2}.
The above theorem manifests that the solution of MMCP for a
connected graph 𝐺can be obtained by separately solving connected
 
1923KDD ’24, August 25–29, 2024, Barcelona, Spain Huaiyuan Liu et al.
Figure 2: Overview of the PIONEER pipeline. (a) Remove all bridges to induce connected subgraphs. (b) Utilize the unsupervised
solver with a performance guarantee to obtain discrete solutions. (c) Construct spanning trees based on the discrete solutions.
(d) Further enhance the solutions through the heuristic solver. (e) Compare all solutions to determine the optimal solution.
subgraphs of 𝐺which are divided by removing all bridges of 𝐺.
We solve for connected subgraphs based on this graph partitioning
instead of the whole graph. Therefore, the entire solving process
can be accelerated due to the reduction of the graph scale.
4.3 The Unsupervised Combinatorial Solver
We aim to leverage unsupervised learning for solving MMCP. Gen-
erally, combinatorial optimization (CO) problems on graphs admit
solutions that are binary vectors 𝑆={𝑥1,...,𝑥𝑛}∈{ 0,1}𝑛of the
set of vertices to denote whether the vertex is selected or not. Thus,
a CO problem on graphs is to minimize a cost 𝑓given a constraint
𝑔by solving the following equation.
min
𝑆⊆𝑉𝑓(𝑆;𝐺)subject to𝑔(𝑆;𝐺)≤1 (1)
Figure 3: The pipeline of unsupervised solver. The parameter
𝜃is learned by graph neural networks, trained through graph
instances without bridges. For testing, the network outputs
relaxed solutions that are rounded to discrete solutions.Learning for MMCP. The learning for MMCP is to learn an
algorithmA𝜃(𝐺)→𝑆∈{0,1}𝑛, e.g. a neural network (NN) pa-
rameterized by 𝜃to solve MMCP. Given an undirected weighted
graph𝐺(𝑉,𝐸,𝜔)with|𝑉|=𝑛and|𝐸|=𝑚, whose degree matrix
and adjacency matrix are denoted by 𝐷and𝐴, respectively. The
complete pipeline of unsupervised solver is given in Figure 3.
Inspired by [ 43], we adopt a relaxation-plus-rounding mecha-
nism and add a term to the loss function that penalizes deviations
from the constraint. Let a continuous vector ˆ𝑆={ˆ𝑥1,...,ˆ𝑥𝑛} ∈
[0,1]𝑛as the relaxed solution obtained by graph neural networks
(GNNs). Formally, we define a new loss function for MMCP:
min
𝜃L(𝜃;𝐺)≜𝛼·𝑓(ˆ𝑆;𝐺)+𝛽·𝑔(ˆ𝑆;𝐺)),𝛼,𝛽 >0 (2)
where,𝛼=𝑚/𝑛or1,𝛽=max𝑓(𝑆;𝐺). Note that𝛽=Í𝑚
𝑖=1𝜔𝑖for
weighted graph and 𝛽=𝑚for unweighted graph. Since connectiv-
ity is a feature that is strongly correlated with the number of edges,
𝛼controls the balance between cost and penalty.
Cost Function. To ensure the non-negativity of the loss function,
we translate the cost function by minimizing the difference between
the maximum value 𝛾of the objective and the sum of cut-set weights.
Typically, the sum of edge weights or cardinality of the graph is
employed as 𝛾. We define the cost function as follows.
𝑓(ˆ𝑆;𝐺)=𝛾−∑︁
𝑖,𝑗∈𝑉,𝑖<𝑗𝜔𝑖𝑗(ˆ𝑥𝑖−ˆ𝑥𝑗)2(3)
Penalty Function. The connected constrain 𝑔(ˆ𝑆;𝐺))for MMCP is
then considered. According to the definition of a maximal minimal
cut, we devote to dividing the graph 𝐺into two cuts by encoding all
the vertices as 0or1. Thus, the constraint is satisfied if the graph 𝐺′,
with the removal of edges between 0and1, has only two connected
components; otherwise, it is illegal and should be punished.
We enlighten from the Matrix-Tree theorem [ 15] and the prop-
erty of Laplacian matrix, i.e. the number of eigenvalues with value
0of Laplacian matrix is equal to the number of connected compo-
nents of𝐺. Consequently, the number of eigenvalues with value
0of the Laplace matrix 𝐿(𝐺′)for𝐺′should be equal to 2. For-
mally, let𝜆1≤...≤𝜆𝑛is ordered eigenvalues of 𝐿(𝐺′), there is
 
1924An Unsupervised Learning Framework Combined with Heuristics for the Maximum Minimal Cut Problem KDD ’24, August 25–29, 2024, Barcelona, Spain
𝜆1=𝜆2=0,𝜆3>0if the solution satisfies the constraint. Note that
𝐿=𝐷−𝐴. Then, the adjacency matrix of 𝐺′can be written by the
relaxed solution ˆ𝑆:
𝐴(𝐺′)=(0, 𝑎 𝑖𝑖
(1−ˆ𝑥𝑖−ˆ𝑥𝑗)2·𝑎𝑖𝑗(𝐺), 𝑎𝑖𝑗,𝑖≠𝑗(4)
where𝑎𝑖𝑗(𝐺)is the elements of 𝐴(𝐺),𝑎𝑖𝑖and𝑎𝑖𝑗are the elements
of𝐴(𝐺′). The degree matrix can be built by summing the elements
of each row in 𝐴(𝐺′). Therefore, the corresponding eigenvalues of
the relaxed solution can be easily obtained.
In addition, for graphs with different sizes, there is a significant
difference in the magnitude of the eigenvalues of the Laplace matrix,
which is detrimental to learning. Thus, we attempt to find an upper
bound of the third small eigenvalue of 𝐺′based on Theorem 2 which
guarantees the robustness of the model. The proof of Theorem 2 is
shown in Appendix B.2.
Theorem 2 (Eigenvalue Interlacing for Laplacian). Let𝐿
and𝐿′be Laplacian matrices of graph 𝐺and its subgraph 𝐺with
respective ordered eigenvalues 𝜇1≤...≤𝜇𝑛and𝜈1≤...≤𝜈𝑛. Then
the following inequality holds:
𝜈𝑖≤𝜇𝑖,𝑖=1,...,𝑛.
According to the above theorem, we can regard 𝜆3(𝐺)of𝐿(𝐺)
as an upper bound of 𝜆3(𝐺′)of all𝐿(𝐺′)and denote 𝜆3(𝐺′)as
𝜆3(𝑆)as well. Meanwhile, the exponential function is employed to
ensure that the learning process is differentiable and the necessary
monotonic relation. The following penalty function is defined:
𝑔(ˆ𝑆;𝐺))=𝑒−[𝑙𝑛𝜖/𝜆3(𝐺)]·[𝜆3(ˆ𝑆)−𝜆2(ˆ𝑆)](5)
where𝜏=𝑙𝑛𝜖/𝜆3(𝐺)>0is a self-adaptive coefficient for each
graph, used to reconcile the differences in the upper bounds of the
eigenvalues due to different sizes of graphs, and 𝜖is a small value,
e.g.𝜖=10−4.
Deterministic Rounding. After network training, assuming
that the optimized parameter 𝜃enableLto be small, and we expect
the relaxed solution ˆ𝑆can be deterministically rounded to a valid
discrete solution 𝑆by sequential decoding as following forms.
Definition 2 (Deterministic Rounding). Given a continu-
ous vector ˆ𝑆={ˆ𝑥1,...,ˆ𝑥𝑛} ∈ [ 0,1]𝑛, w.o.l.g.,𝑖=1,...,𝑛 . Set ¯𝑆=
arg min𝑗=0,1L(¯𝑥1,...,𝑗,..., ¯𝑥𝑛), i.e. round ˆ𝑥𝑖into 0or1and fix all
the other elements unchanged, then repeat the procedure until all
the variables become discrete. If L(ˆ𝑆)≤L( ¯𝑆), then ¯𝑥𝑖is rounded to
1−¯𝑥𝑖to minimize the number of 0eigenvalues, which induces a new
discrete solution 𝑆. Then, repeat the procedure until L(ˆ𝑆)>L(𝑆).
Performance Guarantee. We prove the following theorem to
show that our unsupervised framework allows generating a feasible
and low-cost solution 𝑆after deterministic rounding in theory. We
move the proof to Appendix B.3 due to the interest of space.
Theorem 3 (Performance Guarantee). Let𝛽>max𝑆∈Ω𝑓(𝑆;𝐺)
and min𝑆∈Ω𝑓(𝑆;𝐺) ≥ 0. Suppose that the learned parameter 𝜃
achievesL(𝜃,𝐺)<𝛼·𝛽. Then, rounding the relaxed solution ˆ𝑆=
A𝜃(𝐺)to a discrete solution 𝑆∈Ωsuch that𝛼·𝑓(𝑆;𝐺)<L(𝜃;𝐺).
The above theorem indicates that the loss will not increase after
the deterministic rounding. On this basis, once the parameter 𝜃getsoptimized toL(𝜃;𝐺)<𝛼·𝛽, there is𝛼·𝑓(𝑆;𝐺)+𝛽·𝑔(𝑆;𝐺)≤
L(𝜃;𝐺)<𝛼·𝛽. Owing to 𝑓(·) ≥ 0and𝑔(·) ≥ 0, we have 𝛼·
𝑓(𝑆;𝐺)<L(𝜃;𝐺), s.t𝑔(𝑆;𝐺)≤1.
Constraint-prior Rounding. Furthermore, as that connectivity
might pose more challenges for network learning, to address po-
tential infeasible solutions and prioritize the assurance of solution
feasibility, we employ an additional constraint-prior rounding, i.e.
set𝑆=arg max𝑗=0,1𝜆3(𝑥1,...,𝑗,...,𝑥𝑛)for the illegal solutions after
the aforementioned deterministic rounding.
4.4 Solution Forest
A natural idea for addressing CO problems is to initially discover
a solution and subsequently verify whether the constraints are
met. However, if the solution adhering to the constraints can be
reformulated into a directly obtainable equivalent form, it would
substantially enhance the efficiency of the search. Consequently,
we attempt to convert the Bernoulli solution of the MMCP into
another closed form such that all solutions achieved by a given
algorithm must satisfy the constraints.
It is well-known that the spanning tree of the graph possesses
some interesting properties [ 50], such as, each edge of the tree is a
bridge. Based on this conclusion, we state the following theorem.
Theorem 4. Suppose that 𝐶=(𝐾,𝐿)is a maximum minimal cut
with cut value 𝜓of𝐺, which can definitely be obtained by discon-
necting an edge of a certain spanning tree 𝑇=(𝑉𝑇,𝐸𝑇)of𝐺.
The theorem above indicates that each feasible solution of MMCP
corresponds to at least one spanning tree of 𝐺. In other words, we
can explore legal solutions for the MMCP by transforming the span-
ning tree, eliminating the need to check for constraint violations,
which is the core idea behind the design of our subsequent heuristic
solver. Additionally, to facilitate the subsequent description of the
heuristic solver, we define a notion of disconnect-vertex as follows.
Definition 3 (Disconnected-Vertex). An optimal solution of
𝐺can be obtained by disconnecting the edge 𝑒𝑑=(𝑣𝑖,𝑣𝑗)of spanning
tree𝑇. Such edge is called disconnected-edge of 𝑇and𝑣𝑖,𝑣𝑗are named
disconnected-vertex of 𝑇.
4.5 Heuristic Tree Transformation
To further repair and improve the solutions of the unsupervised
solver while disregarding the impact of the graph structure, we aim
to design a heuristic that achieves the transformation of the span-
ning tree by adding vertices for the cut to explore better solutions.
Suppose𝐶𝑣=(𝑉1,𝑉2)is the maximum minimal cut of 𝐺obtained
from the spanning tree 𝑇of𝐺, where𝑉1=𝑉\𝑉2, w.l.o.g, we assume
|𝑉1|<|𝑉2|. Let𝑝𝑘∈𝑉1and𝑝𝑘+1∈𝑉2. We aim to optimize the
existing spanning tree by introducing an appropriate 𝑝𝑘+1related
to𝑝𝑘into𝑉1, seeking enhanced solutions. The transformation of
the current spanning tree will undergo the following four phases.
Phase I. Selection .
As not all vertices in 𝑉2are suitable to join 𝑉1, the selection of
𝑝𝑘+1must meet the constraint that 𝑝𝑘+1is the neighbor of 𝑝𝑘, which
ensures𝑉1is connectable after each vertex movement. Therefore,
different neighbors of 𝑉1can be chosen to join 𝑉1, altering the shape
of the spanning tree in conjunction with the subsequent phases.
Phase II. Disconnect edge .
 
1925KDD ’24, August 25–29, 2024, Barcelona, Spain Huaiyuan Liu et al.
Generally speaking, the selected vertex 𝑝𝑘+1may also have other
neighbors. Therefore, we disconnect the edges connected to 𝑝𝑘+1in
𝑇to isolate𝑝𝑘+1, making it convenient for the subsequent addition
of edges.
Phase III. Add edge .
After disconnecting all feasible edges of 𝑝𝑘+1, the current graph
consists of several connected components centered around 𝑝𝑘+1.
At present, the edge addition involves those of 𝑝𝑘+1and its neigh-
bor𝑛. For the addition of the edge connected to 𝑝𝑘+1, we directly
link(𝑝𝑘,𝑝𝑘+1)to incorporate 𝑝𝑘+1into𝑉1. Note that the current
connected component containing 𝑉1∪𝑃𝑘+1forms a tree, referred
to as𝑇𝑏. Handling the edge addition for 𝑛introduces additional
complexity and requires the following processing.
Candidate. The edges that need to be added are not arbitrary, they
have to meet certain constraints. (i) The connected edges belong
to𝐺. (ii) The vertices planned to be connected must belong to 𝑉2.
(iii) Only one edge is added to the neighbor of each 𝑃𝑘+1in𝑉2. (iv)
There is no self-loop. The vertices that satisfy the above conditions
are selected as the candidate set when adding edges.
Addition. After selecting the vertex, we connect it with 𝑝𝑘+1.
However, an unfortunate observation arises that the coupling rela-
tionship between different connected components results in high
time complexity when adding edges. The main objective of adding
edges is to recombine discrete connected components to induce a
new spanning tree. To address this issue, we construct the spanning
tree𝑇𝑏𝑒using𝑉2\𝑝𝑘+1, providing a new way to achieve the same
effect of adding edges in practice. Notably, 𝑉2\𝑝𝑘+1can be capable
of constructing a spanning tree as there is no bridge after graph
partitioning.
Montage. It is evident that the current graph consists of two trees,
containing𝑉2\𝑝𝑘+1and𝑉1∪𝑝𝑘+1, respectively. We concatenate 𝑇𝑏
and𝑇𝑏𝑒while preserving the original tree edge of 𝑇connecting
𝑉1and𝑉2, ensuring that the graph obtained after transformation
remains a spanning tree 𝑇𝑛𝑒𝑤of𝐺.
Phase IV. Dislodge vertex .
Note that, for(𝑉1∪𝑝𝑘+1,𝑉2\𝑝𝑘+1), it is not necessarily the opti-
mal solution for 𝑇𝑛𝑒𝑤. The optimal solution 𝐶∗𝑣=(𝑉∗
1,𝑉∗
2)of𝑇𝑛𝑒𝑤
may potentially be found at other disconnected-edges. Additionally,
an observation is that removing some vertices from 𝑉∗
1might lead
to a greater increase in cut value. Hence, we move vertices of 𝑉∗
1to
𝑉∗
2in turn to explore better solutions. The movement is restricted
to the vertices in 𝑉∗
1who have neighbors in 𝑉∗
2.
It is noteworthy that the idea of transforming spanning trees
to search for better solutions can be a general scheme for solving
MMCP, which is worth further exploration in the future.
4.6 Random Tree Search
According to Theorem 4, we designed a random search algorithm as
a baseline based on the conclusion that transforming the shape of
the spanning tree can lead to diverse feasible solutions. Drawing in-
spiration from the Kruskal algorithm [ 27] employed in constructing
minimum spanning trees, we arrange all edges in ascending order
based on their weights. For unweighted graphs, the order of edges
is randomized. In each iteration, the edge sequence is randomly
shuffled, and subsequently, the spanning tree 𝑇𝑟of the graph 𝐺is
constructed based on the shuffled edge sequence. Following this, afeasible solution for 𝐺is obtained by disconnecting a single edge
from𝑇𝑟. The feasible solution 𝑆𝑇𝑟with the maximum cut value is
then selected as the optimal solution 𝑆∗for𝑇𝑟. The demonstration
of random tree search is given in Figure 4.
Figure 4: An instance to show that randomly shuffle edge
sequence to construct a new spanning tree.
5 Experimental Evaluation
This section discusses our experimental setup and results. In addi-
tion, as a case study, we provide a specific real-world application of
MMCP, i.e. identifying the critical cross-sections for power grids.
5.1 Experimental Setup
We conduct extensive experiments to evaluate the practical ef-
fectiveness of the proposed method. Here we briefly describe the
hardware, datasets, baselines, and evaluation metrics.
Implementations. We implement the PIONEER by Python 3.7.11
and PyTorch 1.9.2. All experiments are conducted on an Ubuntu ma-
chine with NVIDIA GeForce RTX 3090 (24 GB VRAM) and Intel(R)
Xeon(R) Platinum 8260 CPU @ 2.40GHz (256 GB RAM). For the
unsupervised solver, the training, validating, and testing datasets
use an 8:1:1 dataset split, and each synthetic dataset contains 10,000
graphs. In addition, in the interest of fairness, our unsupervised
solver and [ 23] share the same neural network structure and param-
eter setting without the GAT layer. Given the significant variations
and sparsity observed in real-world datasets, we select the suitable
𝛼based on the parameter study in the Appendix. We set 𝛼=𝑚/𝑛
for synthetic graphs, allowing the model to explore the solutions
that yield larger objective values. For the random tree search, we
report the best result by running the algorithm for 500rounds. Our
code can be available at https://github.com/luckyseasalt/PIONEER.
Datasets. We assess our methods in a wide variety of experiments,
rigorously conducted on synthetic and real-world datasets. Syn-
thetic datasets are constructed with 36vertices based on the pictures
in MNIST [ 5]. There are 60,000pictures in Mnist which correspond
to different one-digit [ 5]. The construction of the synthetic datasets
is inspired by [ 37]. Here, we first generate a complete graph Gas a
motherboard. We associated each edge with 𝜙={0,1}|𝐸|and each
vertex with the picture. Thus, synthetic graphs can be built by a ran-
dom sequence 𝜙whose element denotes whether the corresponding
edge inGexists. We give an instance in Figure 5.
 
1926An Unsupervised Learning Framework Combined with Heuristics for the Maximum Minimal Cut Problem KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 5: An instance to show the synthetic datasets. The
vertices in the graph correspond to random pictures of Mnist,
whose values 𝑣𝑖are the one-digit numbers that match the
pictures. And the weight of edge 𝑒𝑖𝑗is equal to 𝑣𝑖+𝑣𝑗+𝑣𝑖·𝑣𝑗.
Considering the impact of various graph structures and the re-
quirements of our framework, constraints can be imposed on the
random graph generation process to obtain different types of graphs.
It should be noted that the unsupervised solver is employed to han-
dle graphs that have undergone graph partitioning. Hence, we
focus on the scenario without bridges when creating the dataset. In
summary, the synthetic Mnist datasets are classified into two pri-
mary types, i.e. isomorphic graphs and heterogeneous graphs with
the notation ‘I-36| 𝑚’ and ‘H-36’, where 36 and 𝑚are the number
of vertices and edges, respectively. We count all the constructed
datasets as shown in Table 1 and all synthetic graphs are undirected,
weighted, and connected graphs without bridges.
Table 1: Statistics of the synthetic datasets. We give a range
of values for the datasets with different numbers of edges.
Dataset Task # Data #Vertex #Edge
I-36|𝑚Train 8000
3660/120/180/240/300 Valid 1000
Test 1000
H-36Train 8000 [265, 359]
Valid 1000 [278, 359]
Test 1000 [278, 350]
Real-world datasets are all unweighted graphs, including EN-
ZYMES1[48], IMDB2[24], and REDDIT3[46] datasets that are
widely used in other CO problems. Noteworthy, the real-world
datasets also include actual power grids and associated parameters,
where the parameters of IEEE118 and IEEE300 are derived from the
1https://paperswithcode.com/dataset/enzymes
2http://www.graphlearning.io/
3https://www.reddit.com/results of built-in power flow calculation by PyPower4. The edge
weights signify the average active power on lines.
Baselines. As few algorithms can be fully adapted to solve MMCP,
we mainly designed the random algorithm in Section 4.6 and a brute
force algorithm as baselines, which can individually obtain the
approximate and exact solutions. Moreover, we adaptively modified
the genetic algorithm (GA) [ 3] combined with depth-first search
(DFS). To ensure fairness, all baselines also use graph simplification.
Metrics. Consistent with existing studies, all results are measured
by widely used metrics. Unfortunately, due to the unavailability of
labeled datasets that offer accurate solutions to MMCP, we resort
to the value of cut-set and execution time to compare the different
approaches. Assume that the solution is 𝑆∈{0,1}𝑛, and the edges
between the labeled vertices 0and1is𝐹with weight 𝜔1,...,𝜔𝑚,
then the value of the cut-set is 𝑊𝐹=Í𝑚
𝑖=1𝜔𝑖. Execution time is
measured in sec. per graph (s/g). We record the mean and standard
deviation by running the results for 5times.
5.2 Main Results
Table 2, 3 and 4 report the results on synthetic datasets and real-
world datasets, respectively. Note that the results of the brute force
algorithm are not included because it is still too time-consuming
even when adopting graph partitioning to simplify. All methods
are graphically simplified and accelerated by graph partitioning.
In summary, the proposed PIONEER achieves better solutions
than the competitors on all datasets and runs quickly. The re-
sults show that PIONEER exhibits superior performance in solving
MMCP with various graph structures. We discuss the results of
each task in more detail below. Note that the best results among
the methods are highlighted in bold across all Tables.
Table 2: Performance comparison on synthetic datasets.
Dataset Random Tree PIONEER
I-36|60 726.84±0.49 (2.74 s/g) 872.34±0.76 (0.95 s/g)
I-36|120 1864.13±2.22 (2.87 s/g) 2374.61±2.43 (3.19 s/g)
I-36|180 2886.29±3.74 (2.99 s/g) 3543.06±3.22 (5.76 s/g)
I-36|240 3881.32±2.69 (3.09 s/g) 4573.49±4.17 (6.46 s/g)
I-36|300 4861.74±2.40 (3.22 s/g) 5514.47±4.79 (8.45 s/g)
Heter-36 5033.12±2.44 (3.27 s/g) 5659.22±6.43 (8.46 s/g)
Experiments on synthetic datasets. The results of the synthetic
tasks are shown in Table 2, where PIONEER demonstrates com-
petitive performance across all datasets. It consistently delivers
solutions of the highest quality while maintaining a rapid process-
ing speed. Surprisingly, the random tree search method achieves
acceptable results within a relatively short time frame, benefiting
from Theorem 4. However, the performance of the proposed ran-
dom algorithm weakens as the number of edges increases. This
phenomenon is attributed to the significant increase in the num-
ber of spanning trees for the graphs with more edges, limiting
its ability to find higher-quality solutions. In contrast, PIONEER
does not rely on edge sequences for spanning tree construction. In-
stead, it achieves tree transformation by adding vertices, effectively
overcoming this limitation and finding better solutions.
4https://github.com/rwl/PYPOWER
 
1927KDD ’24, August 25–29, 2024, Barcelona, Spain Huaiyuan Liu et al.
Experiments on real-world datasets. To further demonstrate
the superiority of our PIONEER, we perform experiments on real-
world datasets. We note that the graphs under consideration exhibit
sparsity and the presence of bridges, rendering graph partitioning
highly efficacious. Table 3 reports the statistics of the real-world
datasets and the results of graph partitioning. More details on the
processing of the dataset are in the Appendix A. We categorize
graphs containing fewer than 16vertices as small graphs and em-
ploy a brute force algorithm to precisely determine their values.
Otherwise, we employ the PIONEER method to address MMCP.
Table 3: Statistics of the real-world datasets. We counted the
range of values of the corresponding variables after graph
partitioning (Train, Valid, and Test).
Dataset
Type # Vertex # Edge# Subgraph
𝑛≤16𝑛>16
ENZYMESOrigin
[2, 126] [1, 149] 575
Train [16, 116] [27, 144] 385 337
Valid [18, 66] [36, 120] 52 46
Test [16, 60] [31, 114] 75 39
IMDBOrigin [12, 136] [52, 2498] 1000
Train [16, 136] [35, 1249] 358 442
Valid [16, 49] [36, 445] 37 63
Test [16, 55] [39, 785] 51 49
REDDITOrigin [6, 3782] [8, 8142] 2000
Train [16, 711] [21, 1631] 593 1493
Valid [16, 86] [20, 165] 123 75
Test [16, 112] [21, 194] 127 85
In Table 4, we can see that PIONEER outperforms the baselines
across all datasets. Specifically, there is no statistically significant
difference in terms of their performance on the IMDB and RED-
DIT, while PIONEER requires less time. We would like to empha-
size that the random algorithm works wonders for certain tree-
structured graphs, whereas PIONEER offers a versatile framework
that can be applied to various graph structures and guarantees
high-quality solutions. It should be noted that PIONEER achieves
significantly larger solutions in terms of addressing the MMCP
of ENZYMES, which reveals that even in sparse tree-structured
graphs, our method can still find superior solutions effortlessly.
Table 4: Performance comparison on real-world datasets.
Dataset Random Tree PIONEER
ENZYMES 19.85±0.32 (3.49 s/g) 31.32±0.43 (2.46 s/g)
IMDB 56.65±0.03 (1.63 s/g) 56.70±0.10 (1.08 s/g)
REDDIT 16.42±0.05 (3.27 s/g) 17.97±0.02 (1.58s/g)
5.3 Ablation Study
To validate the effectiveness of the key components in PIONEER,
we conduct ablation studies on all aforementioned datasets. Only
two critical results are reported here.
w/o Unsupervised solver. We remove the unsupervised solver
in our framework and directly construct a random spanning treeas input for the heuristic solver to evaluate the effectiveness of
the unsupervised solver. As can be seen in Figure 6, the variant
without an unsupervised solver can solve with almost the same
quality as PIONEER (Avg. 3750.48 vs 3756.20). However, the speed
of implementation of the two showed marked differences (Avg. 9.45
s/g vs 5.54 s/g). This is an interesting finding that the unsupervised
solver serves as an effective starting point for the heuristic solver,
leading to a substantial improvement in search speed.
Figure 6: Effectiveness of the unsupervised solver. We report
the results on the synthetic dataset here. The bars denote the
cut values, and the lines denote the execution time.
w/o Heuristic solver. Similarly, we assess the heuristic by directly
taking the output of the unsupervised solver as a result. The results
are summarized in Table 5, showing the heuristic solver plays a cru-
cial role in repairing and enhancing the solutions. Encouragingly,
although the unsupervised solver does not yield optimal solutions,
its fastest running speed and acceptable solution quality are also im-
pressive. Furthermore, we have observed that unsupervised solvers
exhibit surprisingly effective performance in dense graphs.
Table 5: Effectiveness of the heuristic solver. The cut values
are reported by mean values for correctly solved instances.
DatasetUnsupervised SolverPIONEERViolation Cut Value
I-36|60 4.52 % 551.49 (0.13 s/g) 872.34 (0.95 s/g)
I-36|120 0 % 2069.82 (0.13 s/g) 2374.61 (3.19 s/g)
I-36|180 0 % 3363.73 (0.41 s/g) 3543.06 (5.76 s/g)
I-36|240 0 % 4319.44 (0.14 s/g) 4573.49 (6.46 s/g)
I-36|300 0 % 5222.96 (0.14 s/g) 5514.47 (8.45 s/g)
H-36 0 % 5372.27 (0.14 s/g) 5659.22 (8.46 s/g)
ENZYMES 0 % 12.62 (0.17 s/g) 31.32 (2.46 s/g)
IMDB 4.90 % 41.99 (0.07 s/g) 56.70 (1.08 s/g)
REDDIT 1.65 % 8.07(0.16 s/g) 17.97 (1.58 s/g)
5.4 More Results
We conduct more experiments to further analyze the key compo-
nents and parameters of PIONEER. For detailed information, please
refer to Appendix A due to space limitations.
 
1928An Unsupervised Learning Framework Combined with Heuristics for the Maximum Minimal Cut Problem KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 6: Statistics and performance comparison on the task of the power grid. The numbers in ( ·) are highlighted in underlined
to indicate the maximum number of vertices in the subgraphs after graph partitioning.
Code of
power grid# Vertex # Edge # Bridge# SubgraphBrute Force Random Tree PIONEER𝑛≤16𝑛>16
36-vertices 36 42 12 0 1 (24 )26.12 (328.43 s/g) 26.12 (1.14 s/g) 26.12 (1.57 s/g)
IEEE118 118 186 9 0 1 (109 ) / 1831.67 (63.21 s/g) 2659.34 (13.37 s/g)
IEEE300 300 744 90 2 1 (206 ) / 3610.43 (223.83 s/g) 4151.21 (56.48 s/g)
Area 1 2582 3028 1466 1 1 (1079 ) / 169.55 (2516.48 s/g) 276.92 (8604.68 s/g)
Area 2 2799 3161 1794 36 5 (328 ) / 50.83 (1635.30 s/g) 59.28 (142.56 s/g)
Supplemental ablation study. To validate the effectiveness
of graph partitioning, rounding, and the upper bound 𝜆3(𝐺), ad-
ditional ablation experiments are performed. The results indicate
that graph partition can dramatically accelerate execution, the pro-
posed rounding methods improve the ability to generate constraint-
satisfying solutions, and 𝜆3(𝐺)enables the model to better learn
the bi-connectivity for different graph structures.
Parameter study. Parameter experiments are conducted to
study the key parameters, including the balanced control parameter
𝛼and the self-adaptive coefficient 𝜖. The results show that suitable
𝛼and𝜖can enhance the learning process of the unsupervised solver,
leading to improved quality and legality of the solutions.
Performance guarantee study. Our method avoids the intri-
cate analysis of loss monotonicity, ensuring Theorem 3 through
deterministic rounding. When the relaxed loss is successfully mini-
mized to a sufficiently small value, the inequality L(𝑆;𝐺)<L(ˆ𝑆;𝐺)
<𝛼·𝛽is satisfied, yielding low-cost and feasible solutions.
Heuristic study. A constraint checking is incorporated into
a genetic algorithm to perform a comparative study. The results
reveal that exploring solutions from spanning trees is significantly
superior to the method of finding cuts and then checking legality.
5.5 Case Study
Recall that we mentioned a demand for the power grid motivat-
ing this work. Hence, we utilize cross-section identification in the
power grid as a case study. The power grid is a vast and intricate
network of diverse components representing a connected graph
with vertices and edges. The cross-section identification entails de-
termining a channel set consisting of several lines with maximum
power summation, meeting the bi-connectivity constraint. The exit
or failure of any element in the channel set may trigger cascading
failures if the cross-section power surpasses the set value and en-
suring connectivity on both sides of the cross-section is also crucial
for uninterrupted power supply, highlighting the significance of
identifying them for the safe and stable operation of the power
system. An example of a cross-section is shown in Figure 7.
Therefore, we study the performance of PIONEER on the power
grid. Additionally, training is not feasible due to the limited dataset
containing only five graphs, we utilize models trained on the RED-
DIT dataset, which exhibits relative similarity to the power grid.
As shown in Table 6, our method exhibits superior performance
across all cases. Notably, PIONEER appears to take more time on
Area 1 primarily due to the insufficient training of the unsupervised
solver, resulting in the heuristic requiring more time for exploration.
In contrast, relying solely on the heuristic solver requires a time of
Figure 7: An example of a cross-section in power grid. We con-
struct the undirected connected graph by abstracting buses as
vertices, AC/DC lines and transformers as edges, and active
power flowing on them as the weights of edges.
11141.55s/g, indicating that even in this extreme case, the unsuper-
vised solver can still contribute to a speedup for the heuristic.
6 Conclusions and Future Works
In this paper, we propose a novel unsupervised learning framework
combined with heuristics named PIONEER for MMCP. In particular,
we demonstrate the theoretical foundation of graph simplification
and the equivalent form of solutions for MMCP. On these bases, we
design an unsupervised framework with mathematical guarantees,
which is not only sufficiently rapid in acquiring acceptable solu-
tions independently but also enhances the speed of downstream
solvers. We also construct a heuristic solver for further repairing
and improving the quality of solutions. Extensive experiments man-
ifest that PIONEER outperforms the baselines in various graph
structures. In the future, a promising direction is to explore a more
explicit relationship between spanning trees and optimal solutions.
Acknowledgments
This paper was supported by the Science and Technology Project
of State Grid: Research on artificial intelligence analysis technology
of available transmission capacity (ATC) of the key section under
multiple power grid operation modes (5100-202255020A-1-1-ZN).
 
1929KDD ’24, August 25–29, 2024, Barcelona, Spain Huaiyuan Liu et al.
References
[1]Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. 2018. Learning to solve
circuit-sat: An unsupervised differentiable approach. In International Conference
on Learning Representations.
[2]Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. 2019. PDP: A general
neural framework for learning constraint satisfaction solvers. arXiv preprint
arXiv:1903.01969 (2019).
[3]Nawal Benabbou, Cassandre Leroy, and Thibaut Lust. 2020. An interactive regret-
based genetic algorithm for solving multi-objective combinatorial optimization
problems. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34.
2335–2342.
[4]Ulrich A Brodowsky, Stefan Hougardy, and Xianghui Zhong. 2023. The approxi-
mation ratio of the k-opt heuristic for the euclidean traveling salesman problem.
SIAM J. Comput. 52, 4 (2023), 841–864.
[5]Li Deng. 2012. The mnist database of handwritten digit images for machine
learning research. IEEE signal processing magazine 29, 6 (2012), 141–142.
[6]G. Ding, S. Dziobiak, and Wu H. 2016. Large-or-minors in 3-connected graphs.
Journal of Graph Theory 82, 2 (2016), 207–217.
[7]Haglin D.J. and Venkatesan S.M. 1991. Approximation and intractability results
for the maximum cut problem and its variants. IEEE Trans. Comput. 40, 1 (1991),
110–113.
[8]Miller R. E. and Muller D. E. 1960. A problem of maximum consistent subsets. IBM
Research Report RC-240. JT Watson Research Center, Yorktown Heights, NY.
[9]Barahona F. 1996. Network design using cut inequalities. SIAM Journal on
optimization 6, 3 (1996), 823–837.
[10] Barahona F., Grötschel M., Jünger M., and Reinelt G. 1988. An application
of combinatorial optimization to statistical physics and circuit layout design.
Operations Research 36, 3 (1988), 493–513.
[11] Voigt B. F. 1831. Der Handlungsreisende, wie er sein soll und was er zu thun
hat, um Aufträge zu erhalten und eines glücklichen Erfolgs in seinen Geschäften
gewiss zu sein. Commis-Voageur, Ilmenau (1831), 69–72.
[12] Xiangyu Gao, Jianzhong Li, and Dongjing Miao. 2022. Dynamic Approximate
Maximum Independent Set on Massive Graphs. In 2022 IEEE 38th International
Conference on Data Engineering (ICDE). IEEE, 1835–1847.
[13] Maxime Gasse, Didier Chételat, Nicola Ferroni, Laurent Charlin, and Andrea
Lodi. 2019. Exact combinatorial optimization with graph convolutional neural
networks. Advances in neural information processing systems 32 (2019).
[14] Youming Ge, Zitong Chen, and Yubao Liu. 2023. An efficient keywords search in
temporal social networks. Data Science and Engineering 8, 4 (2023), 368–384.
[15] Arpita Ghosh and Stephen Boyd. 2006. Growing well-connected graphs. In
Proceedings of the 45th IEEE Conference on Decision and Control. IEEE, 6605–6611.
[16] Duarte G.L., Lokshtanov D., Pedrosa L.L.C., Schouery R.C.S., and Souza U.S. 2019.
Computing the largest bond of a graph. In Proceedings of the 14th International
Symposiumon Parameterized and Exact Computation (IPEC 2019). 12:1–12:15.
[17] Williamson D. P. Goemans M. X. 1994. . 879-approximation algorithms for max
cut and max 2sat. In Proceedings of the Twenty-sixth Annual ACM Symposium on
Theory of Computing. 422–431.
[18] Eto H., Hanaka T., Kobayashi K., and Kobayashi Y. 2019. Parameterized Algo-
rithms for Maximum Cut with Connectivity Constraints. In 14th International
Symposium on Parameterized and Exact Computation (IPEC 2019), Vol. 148. 13:1–
13:15.
[19] Lange J. H., Andres B., and Swoboda P. 2019. Combinatorial persistency criteria
for multicut and max-cut. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR 2019). 6093–6102.
[20] Roger A Horn and Charles R Johnson. 2012. Matrix analysis. Cambridge university
press.
[21] Dunning I., Gupta S., and Silberholz J. 2018. What works best when? A systematic
evaluation of heuristics for Max-Cut and QUBO. INFORMS Journal on Computing
30, 3 (2018), 608–624.
[22] Helsgaun K. 2000. An effective implementation of the Lin-Kernighan traveling
salesman heuristic. European Journal of Operational Research 126, 1 (2000), 106–
130.
[23] Nikolaos Karalias and Andreas Loukas. 2020. Erdos goes neural: an unsupervised
learning framework for combinatorial optimization on graphs. Advances in
Neural Information Processing Systems 33 (2020), 6659–6672.
[24] Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion
Neumann. 2016. Benchmark Data Sets for Graph Kernels. http://graphkernels.
cs.tu-dortmund.de
[25] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. 2017. Learn-
ing combinatorial optimization algorithms over graphs. Advances in neural
information processing systems 30 (2017).
[26] Elias B Khalil, Christopher Morris, and Andrea Lodi. 2022. Mip-gnn: A data-
driven framework for guiding combinatorial solvers. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 36. 10219–10227.
[27] Jon Kleinberg and Eva Tardos. 2006. Algorithm design. Pearson Education India.
[28] Duarte G. L., Eto H., Hanaka T., Kobayashi Y., Kobayashi Y., Lokshtanov D.,
Pedrosa L., Schouery R., and VSouza U. 2021. Computing the Largest Bond andthe Maximum Connected Cut of a Graph. Algorithmica 83, 5 (2021), 1421–1458.
[29] Sirui Li, Zhongxia Yan, and Cathy Wu. 2021. Learning to delegate for large-scale
vehicle routing. Advances in Neural Information Processing Systems 34 (2021),
26198–26211.
[30] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. 2018. Combinatorial optimization
with graph convolutional networks and guided tree search. Advances in neural
information processing systems 31 (2018).
[31] S. Lin and B. W. Kernighan. 1973. An Effective Heuristic Algorithm for the
Traveling-Salesman Problem. Operations Research 21, 2 (1973), 498–516.
[32] Flynn M. 2017. The largest bond in 3-connected graphs. Ph.D. thesis, The University
of Mississippi (2017).
[33] Yimeng Min, Frederik Wenkel, Michael Perlmutter, and Guy Wolf. 2022. Can
Hybrid Geometric Scattering Networks Help Solve the Maximum Clique Problem?
Advances in Neural Information Processing Systems 35 (2022), 22713–22724.
[34] Garey M.R. and Johnson D.S. 1979. Computers and Intractability: A Guide to the
Theory of NP-Completeness.
[35] Rasmus Palm, Ulrich Paquet, and Ole Winther. 2018. Recurrent relational net-
works. Advances in neural information processing systems 31 (2018).
[36] Joseph F Pekny and Donald L Miller. 1990. An exact parallel algorithm for the
resource constrained traveling salesman problem with application to scheduling
with an aggregate deadline. In Proceedings of the 1990 ACM annual conference on
cooperation. 208–214.
[37] Marin Vlastelica Pogančić, Anselm Paulus, Vit Musil, Georg Martius, and Michal
Rolinek. 2019. Differentiation of blackbox combinatorial solvers. In International
Conference on Learning Representations.
[38] Diestel R. 2017. Graph Theory. Graduate Texts in Mathematics, Vol. 173. Springer-
Verlag, Berlin, Germany.
[39] Daniel Rehfeldt and Thorsten Koch. 2019. Combining NP-hard reduction tech-
niques and strong heuristics in an exact algorithm for the maximum-weight
connected subgraph problem. SIAM Journal on Optimization 29, 1 (2019), 369–
398.
[40] Aldred R.E.L., Van Dyck D., Brinkmann G., V. Fack, and McKay B.D. 2009. Graph
structural properties of non-Yutsis graphs allowing fast recognition. Discrete
Appl. Math. 157, 2 (2009), 377–386.
[41] Tao Sun, Jianqiu Xu, and Caiping Hu. 2022. An efficient algorithm of star subgraph
queries on urban traffic knowledge graph. Data Science and Engineering 7, 4
(2022), 383–401.
[42] Haoyu Peter Wang and Pan Li. 2023. Unsupervised Learning for Combinatorial
Optimization Needs Meta Learning. In The Eleventh International Conference on
Learning Representations.
[43] Haoyu Peter Wang, Nan Wu, Hang Yang, Cong Hao, and Pan Li. 2022. Unsuper-
vised learning for combinatorial optimization with principled objective relaxation.
Advances in Neural Information Processing Systems 35 (2022), 31444–31458.
[44] Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. 2019. Satnet: Bridging
deep learning and logical reasoning using a differentiable satisfiability solver. In
International Conference on Machine Learning. PMLR, 6545–6554.
[45] Liang Xin, Wen Song, Zhiguang Cao, and Jie Zhang. 2021. NeuroLKH: Combining
deep learning model with Lin-Kernighan-Helsgaun heuristic for solving the
traveling salesman problem. Advances in Neural Information Processing Systems
34 (2021), 7472–7483.
[46] Pinar Yanardag and SVN Vishwanathan. 2015. Deep graph kernels. In Proceedings
of the 21th ACM SIGKDD international conference on knowledge discovery and
data mining. 1365–1374.
[47] Weichi Yao, Afonso S Bandeira, and Soledad Villar. 2019. Experimental perfor-
mance of graph neural networks on random instances of max-cut. In Wavelets
and Sparsity XVIII, Vol. 11138. SPIE, 242–251.
[48] Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. 2021.
Identity-aware graph neural networks. In Proceedings of the AAAI conference on
artificial intelligence, Vol. 35. 10737–10745.
[49] Tang Z., Jiao Y., and Ravi R. 2022. Combinatorial heuristics for inventory routing
problems. INFORMS Journal on Computing 34, 1 (2022), 370–384.
[50] Shijie Zhang, Jiong Yang, and VenuMadhav Cheedella. 2006. Monkey: Approx-
imate graph mining based on spanning trees. In 2007 IEEE 23rd International
Conference on Data Engineering. IEEE, 1247–1249.
[51] Yao Zhang, Yun Xiong, Yun Ye, Tengfei Liu, Weiqiang Wang, Yangyong Zhu,
and Philip S Yu. 2020. SEAL: Learning heuristics for community detection
with generative adversarial networks. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 1103–1113.
[52] Jiongzhi Zheng, Kun He, Jianrong Zhou, Yan Jin, and Chu-Min Li. 2021. Com-
bining reinforcement learning with Lin-Kernighan-Helsgaun algorithm for the
traveling salesman problem. In Proceedings of the AAAI conference on artificial
intelligence, Vol. 35. 12445–12452.
[53] Darui Zhu, Rui Wang, Jiandong Duan, and Wenji Cheng. 2021. Comprehensive
weight method based on game theory for identify critical transmission lines in
power system. International Journal of Electrical Power & Energy Systems 124
(2021), 106362.
 
1930An Unsupervised Learning Framework Combined with Heuristics for the Maximum Minimal Cut Problem KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 7: Effectiveness of the deterministic rounding.
DatasetRound to Nearest Integer Only Phase II Only Phase I Unsupervised (Ours)
Violation Cut Value Violation Cut Value Violation Cut Value Violation Cut Value
I-36|60 20.58 %197.78
(0.01 s/g)36.24 %187.28
(0.09 s/g)10.94 %580.16
(0.13 s/g)4.52 %551.49
(0.13 s/g)
I-36|120 14.02 %1756.47
(0.02 s/g)0.24 %1345.72
(0.10 s/g)3.34 %2093.41
(0.13 s/g)0 %2066.12
(0.12 s/g)
I-36|180 0.48 %2379.58
(0.02 s/g)0.24 %2079.59
(0.10 s/g)0 %3363.73
(0.41 s/g)0 %3363.73
(0.41 s/g)
I-36|240 94.88 %3676.77
(0.02 s/g)0 %3199.41
(0.10 s/g)0 %4319.44
(0.14 s/g)0 %4319.44
(0.14 s/g)
I-36|300 96.28 %3489.93
(0.03 s/g)0.04 %3949.84
(0.11 s/g)0 %5222.96
(0.15 s/g)0 %5222.96
(0.15 s/g)
H-36 98.44 %3603.00
(0.03 s/g)0.14 %4122.81
(0.11 s/g)0 %5372.27
(0.16 s/g)0 %5372.27
(0.16 s/g)
ENZYMES 100 % - (0.02 s/g) 100 % - (0.12 s/g) 0% 12.62 (0.17 s/g) 0% 12.62 (0.17 s/g)
IMDB 13.47 % 9.77 (0.03 s/g) 22.45 % 26.06 (0.06 s/g) 4.90 % 41.99 (0.07 s/g) 4.90 % 41.99 (0.07 s/g)
REDDIT 16.94 % 2.98 (0.02 s/g) 47.29 % 3.71 (0.13 s/g) 4.00 % 7.92 (0.16 s/g) 1.65 % 8.07 (0.16 s/g)
Appendix
In the Appendix, we provide more results and proof of theorems.
A More Experimental Results
Due to space constraints in the main text, we show more exper-
imental results outlined in Section 5.4. In all the tables, the best
results are highlighted in bold and the cut values are only reported
by mean values for correctly solved instances.
Table 8: Effectiveness of 𝜆3(𝐺). The significantly worse results
are highlighted in underlined.
Datasetw/o𝜆3(𝐺) Unsupervised (Ours)
Violation Cut Value Violation Cut Value
I-36|60 88.48 % 549.90 4.52 % 551.49
I-36|120 1.70 % 1540.14 0 % 2066.12
I-36|180 0 % 3280.54 0 % 3216.96
I-36|240 0 % 4308.04 0 % 4319.44
I-36|300 0 % 5334.66 0 % 5222.96
H-36 0 % 5499.91 0 % 5372.27
ENZYMES 74.36 % 13.21 0 % 12.62
IMDB 2.04 % 32.48 4.90 % 41.99
REDDIT 59.53 % 9.34 1.65 % 8.07
Table 9: Comparison of heuristics on real-world datasets.
DatasetGenetic Algorithm Heuristics (Ours)
Violation Cut Value Violation Cut Value
ENZYMES 57.44 %27.08
(6.16 s/g)0 %31.25
(2.52 s/g)
IMDB 27.76 %71.53
(36.92 s/g)0 %56.67
(2.65 s/g)
REDDIT 52.00 %11.59
(6.52 s/g)0 %17.71
(1.68 s/g)
Figure 8: Effectiveness of the graph partitioning. We report
the results on the real-world datasets. The bars denote the
mean of cut values, and the lines denote the execution time.
Figure 9: Compare the loss functions of relaxed solutions,
discrete solutions, and the value of 𝛼·𝛽.
B The Proof OF Theorems
B.1 Proof of Theorem 1
Proof. Consider whether 𝑒𝑑is in the cut-set 𝐹of𝐺. If𝑒𝑑∈𝐹,
such that𝐹=𝑒𝑑, that is the maximum minimal cut 𝐶=(𝐺1,𝐺2).
Without loss of generality, we let 𝐶=(𝐾,𝐿)and𝑒𝑑∈𝐾if𝑒𝑑∉𝐹.
Suppose𝑣1,𝑣2are on the different sides of 𝑒𝑑, and𝑣1,𝑣2cannot be
 
1931KDD ’24, August 25–29, 2024, Barcelona, Spain Huaiyuan Liu et al.
Table 10: Parameter study of the adjustment parameter 𝜖.
Dataset𝜖=0.1 𝜖=0.01 𝜖=0.001 Unsupervised (Ours)
Violation Cut Value Violation Cut Value Violation Cut Value Violation Cut Value
I-36|60 2.24 % 335.10 0.16 % 369.32 2.04 % 459.19 4.52 % 551.49
I-36|120 1.04 % 1544.07 1.60 % 1550.81 0 % 2023.97 0 % 2069.82
I-36|180 0.04 % 3292.11 0 % 3247.04 0 % 3342.81 0 % 3363.73
I-36|240 0 % 4342.85 0 % 4303.53 0 % 4373.97 0 % 4319.44
I-36|300 0 % 5230.20 0 % 5169.11 0 % 5187.31 0 % 5222.96
H-36 0 % 5355.65 0 % 5330.92 0 % 5346.43 0 % 5372.27
ENZYMES 97.44 % 8.00 100 % - 46.15 % 12.29 0% 12.62
IMDB 3.67 % 25.79 6.94 % 44.21 14.69 % 32.05 4.90 % 41.99
REDDIT 25.41 % 6.23 13.65 % 8.61 7.06 % 6.95 1.65 % 8.07
Table 11: Parameter study of the control parameter 𝛼.
Dataset𝛼=1 𝛼=𝑚/𝑛
Violation Cut Value Violation Cut Value
I-36|60 4.68% 453.95 4.52 % 551.49
I-36|120 0 % 1762.39 0 % 2069.82
I-36|180 0 % 2754.60 0 % 3363.73
I-36|240 0 % 3491.16 0 % 4319.44
I-36|300 0 % 1956.11 0 % 5222.96
H-36 0 % 1264.93 0 % 5372.27
ENZYMES 48.72 % 10.75 0 % 12.62
IMDB 4.90 % 41.99 31.43% 58.91
REDDIT 1.65 % 8.07 24.00 % 10.77
connected if 𝑣1,𝑣2∈𝐿. Thus, the cut-set 𝐹of𝐺will be equal to the
𝐹of𝐺1if𝑣1,𝑣2∈𝐾. And the case where 𝑒𝑑∈𝐿can be proved.□
B.2 Proof of Theorem 2
Theorem 5 (Weyl’s ineqality). Let𝐻and𝑈be𝑛×𝑛Hermitian
matrices, with their ordered eigenvalues 𝜆1(·)≤...≤𝜆𝑛(·). Then the
following inequality holds:
𝜆𝑖(𝐻)+𝜆𝑛(𝑈)≤𝜆𝑖(𝐻+𝑈)≤𝜆𝑖(𝐻)+𝜆1(𝑈),𝑖=1,...,𝑛.
Corollary 1. Let𝐻be a𝑛×𝑛Hermitian matrices with ordered
eigenvalues 𝜆1≤...≤𝜆𝑛. For nonzero vector 𝑢∈C𝑛and𝑛≥2,
let the eigenvalues of 𝑍=𝐻+𝑢⊗𝑢be𝜇1≤..≤𝜇𝑛. Then the
eigenvalues of 𝐻and𝑍interlace,
𝜆1≤𝜇1≤𝜆2≤𝜇2≤..≤𝜆𝑛≤𝜇𝑛.
The above corollary can be drawn from the eigenvalue interlac-
ing theorem [20] as Theorem 5. Then, we can prove Theorem 2.
Proof. Let𝐿be a Laplacian matrix of graph 𝐺, without loss of
generality, we remove edge 𝑒𝑢𝑣from𝐺in turn to obtain subgraphs
𝐺′of𝐺. We consider the variation of the adjacency matrix from
the subgraph 𝐺′to𝐺when removing one edge 𝑒𝑢𝑣from𝐺.
𝐿(𝐺′)= 
∑︁
𝑖≠𝑗𝑎𝑖𝑗, 𝑙𝑖𝑖
−𝑎𝑖𝑗, 𝑙𝑖𝑗,𝑖≠𝑗(6)
Δ𝐿=𝐿(𝐺′+𝑒)−𝐿(𝐺′)=1, 𝑙𝑢𝑢,𝑙𝑣𝑣
−1, 𝑙𝑢𝑣,𝑙𝑣𝑢(7)Let𝑝is an nonzero vector with element 𝑝𝑢=1and𝑝𝑣=−1, then
Δ𝐿=𝑝𝑝∗. There is𝜆𝑖(𝐿(𝐺′))≤𝜆𝑖(𝐿(𝐺′+𝑒))=𝜆𝑖(𝐿(𝐺))based
on the Corollary 1. Similarly, all subgraphs of 𝐺can be obtained
in this way so that the same conclusions can be reached by just
superimposing 𝑝𝑝∗. Consequently, the eigenvalues of the Laplacian
matrices of all subgraphs of 𝐺are correspondingly smaller than the
eigenvalues of the Laplacian matrix of 𝐺. □
B.3 Proof of Theorem 3
Proof. Let𝜔(ˆ𝑆)=Í
𝑖,𝑗∈𝑉,𝑖<𝑗𝜔𝑖𝑗(ˆ𝑥𝑖−ˆ𝑥𝑗)2denote a weight
obtained by a relaxed solution ˆ𝑆=A𝜃(𝐺),ˆ𝑥∈[0,1]𝑛. We analyze
the rounding process of the relaxed solution ˆ𝑆into the discrete
solution𝑆∈{0,1}𝑛. Let ˆ𝑥𝑖and𝑥𝑖,𝑖=1,2,...,𝑛 denote their entries
respectively. The rounding procedure has no requirement on the
order of the rounding sequence, w.l.o.g, suppose we round from
index𝑖=1to𝑖=𝑛.
In the deterministic rounding phase, we opt for the rounding
method that minimizes the loss per round. Additionally, further
rounding is conducted for the cases that do not meet the loss in-
equality. Then the following inequality holds:
L(𝜃;𝐺)=𝛼·𝑓(ˆ𝑆;𝐺)+𝛽·𝑔(ˆ𝑆;𝐺)
=𝛼·∑︁
𝑖,𝑗∈𝑉,𝑖<𝑗𝜔𝑖𝑗(ˆ𝑥𝑖−ˆ𝑥𝑗)2+𝛽·𝑒−𝜏·[𝜆3(ˆ𝑆)−𝜆2(ˆ𝑆)]
(a)
≥𝛼·𝑓([𝑥1,𝑥2,...,𝑥𝑛];𝐺)+𝛽·𝑔([𝑥1,𝑥2,...,𝑥𝑛];𝐺)
=𝛼·𝑓(𝑆;𝐺)+𝛽·𝑔(𝑆;𝐺)(8)
where, (a) denotes the phase of deterministic rounding. □
B.4 Proof of Theorem 4
Proof. Let𝐾𝑣=[𝑣1,𝑣2,...,𝑣𝑙]⊆𝑉denote a feasible solution
with weight 𝜓of𝐺. Consider the tree obtained from 𝐾𝑣by adding
edges(𝑣𝑖,𝑣𝑗)∈𝐸,𝑣𝑖,𝑣𝑗∈𝐾𝑣until𝐾𝑣become to a tree 𝑇𝐾. The tree
𝑇𝑉\𝐾𝑣of𝑉\𝐾𝑣can be constructed in the same way. Set 𝑣𝑝∈𝐾𝑣and
𝑣𝑞∈𝑉\𝐾𝑣, the edge set 𝐹connects the cut 𝐾𝑣and𝑉\𝐾𝑣according
to the definition of maximum minimal cut. We can construct a
spanning tree 𝑇of𝐺by selecting one edge 𝑒from𝐹and connecting
it with𝑇𝐾and𝑇𝑉\𝐾𝑣. Hence, the feasible solution with weight 𝜓of
𝐺can be obtained by disconnecting an edge of 𝑇.□
 
1932