Unifying Graph Convolution and Contrastive Learning in
Collaborative Filtering
Yihong Wu
Université de Montréal
Montréal, Canada
yihong.wu@umontreal.caLe Zhang
Mila - Quebec AI Institute
Montréal, Canada
le.zhang@mila.quebecFengran Mo
Université de Montréal
Montréal, Canada
fengran.mo@umontreal.ca
Tianyu Zhu∗
MIIT Key Laboratory of Data
Intelligence and Management
Beihang University
Beijing, China
ztybuaa@126.comWeizhi Ma
Institute for AI Industry Research
(AIR), Tsinghua University
Beijing, China
mawz@tsinghua.edu.cnJian-Yun Nie
Université de Montréal
Montréal, Canada
nie@iro.umontreal.ca
Abstract
Graph-based models and contrastive learning have emerged as
prominent methods in Collaborative Filtering (CF). While many ex-
istingmodelsinCFincorporatethesemethodsintheirdesign,there
seems to be a limited depth of analysis regarding the foundational
principles behind them. This paper bridges graph convolution, a
pivotal element of graph-based models, with contrastive learning
through a theoretical framework. By examining the learning dy-
namics and equilibrium of the contrastive loss, we offer a fresh lens
to understand contrastive learning via graph theory, emphasizing
its capability to capture high-order connectivity. Building on this
analysis, we further show that the graph convolutional layers often
usedingraph-basedmodelsarenotessentialforhigh-orderconnec-
tivity modeling and might contribute to the risk of oversmoothing.
Stemming from our findings, we introduce Simple Contrastive Col-
laborative Filtering (SCCF), a simple and effective algorithm based
on a naive embedding model and a modified contrastive loss. The
efficacy of the algorithm is demonstrated through extensive experi-
ments across four public datasets. The experiment code is available
at https://github.com/wu1hong/SCCF.
CCS Concepts
•Information systems !Recommender systems; Collabora-
tive filtering.
Keywords
Collaborative Filtering, Contrastive Learning, Graph Neural Net-
works
∗This work was done at Université de Montréal.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671840ACM Reference Format:
Yihong Wu, Le Zhang, Fengran Mo, Tianyu Zhu, Weizhi Ma, and Jian-
Yun Nie. 2024. Unifying Graph Convolution and Contrastive Learning in
Collaborative Filtering. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671840
1 Introduction
Recently,contrastivelearninghasbecomethestate-of-the-artmethod
forself-supervisedlearning,markingsignificantachievementsacross
computer vision [ 4,9,11,43], natural language processing [ 5], and
multi-modality [ 26]. Given its success in various domains, there
is burgeoning interest in harnessing contrastive learning within
Collaborative Filtering (CF) [ 19,34,47,49]. Wu et al . [42]explored
the effectiveness of the Sampled SoftMax loss function within CF.
Chen et al . [3]examined the impact of the temperature parameter
on a model’s performance in contrastive learning. Both Zhou et al .
[49]and Zhang et al . [47]highlighted the ability of contrastive loss
to address bias in recommendation applications. Zhou et al . [50]
employedcontrastivelearningtoaddresscold-startproblemsinrec-
ommendation. Yet, none of these studies address the fundamental
question: How does contrastive learning operate within collaborative
filtering? While studies such as Wang and Isola [36]and Wang et al .
[34]haveelucidatedthatcontrastivelearningservestoaligntheem-
beddings of interacted user-item pairs and to uniformly distribute
embeddings across a hypersphere, the mechanism and equilibrium
of these forces remain veiled.
Beyondcontrastivelearning,graph-basedmethodsconstitutean-
other burgeoning research avenue in CF. Drawing inspiration from
the success of Graph Convolutional Networks (GCN) [ 16], CF re-
searchers have discovered that integrating graph convolutional lay-
ers with basic learnable embeddings significantly enhances recom-
mendationquality[ 12,37].Whileinstancesexistwhereresearchers
simultaneously employ contrastive learning and graph-based ap-
proaches [ 23,34], these domains are often treated as distinct and
separate, resulting in a lack of comprehensive exploration. Moti-
vated by the recent breakthrough on a theoretical link between
contrastive learning and graph theory [ 10,31,39,40], this paper
3425
KDD ’24, August 25–29, 2024, Barcelona, Spain Yihong Wuet al.
aims to bridge the existing gap in CF research and address the fun-
damental questions of contrastive learning through the framework
of graph theory.
Specifically, we establish the equivalence between contrastive
learning and graph convolution within the context of collaborative
filtering. We demonstrate that the learning dynamics (embedding
update) of the contrastive loss constitute a combination of two
distinct graph convolution processes. One graph convolution oper-
ation, represented by positive samples, enhances the smoothness
of embeddings across the graph. Conversely, another convolution
operation, characterized by negative samples, reduces this smooth-
ness, thereby preventing the collapse of embeddings. These two
operations serve as opposing forces within the system, dynamically
attracting and dispersing embeddings, respectively. Furthermore,
achieving equilibrium (a state of balance) within these two forces
necessitates the model’s estimations to align with the empirical
distribution.
A significant advantage of this equivalence is the ability of con-
trastive learning to effectively model High-Order Connectivity
(HOC) [37], a capability traditionally pursued through the incorpo-
ration of graph convolutional layers in most existing graph-based
methods [ 12,37,41]. However, our findings challenge the necessity
of graph convolutional layers for HOC modeling, indicating that
an increase in the number of these layers may lead to oversmooth-
ing. In response to the limitations posed by convolutional layers
and leveraging the potential of contrastive loss for HOC modeling,
we propose the Simple Contrastive Collaborative Filtering (SCCF)
approach. This approach comprises a naive embedding model and a
tailored contrastive loss function. Without any convolutional layer,
our minimalist model attains performance that matches or even
surpasses state-of-the-art methods in collaborative filtering.
Our contributions are summarized as follows: (1) We present
a theoretical framework that integrates contrastive learning with
graphconvolution.Withinthisframework,thecontrastivelossisde-
composed into two distinct convolution processes: one that attracts
embeddings closer together and another that disperses them. (2)
We highlight the capacity of contrastive loss to model High-Order
Connectivity (HOC) and discuss the necessity of graph convolu-
tional layers. (3) We propose the Simple Contrastive Collaborative
Filtering (SCCF) method, consisting only of a naive embedding
model and a contrastive objective. Extensive experiments confirm
its efficacy and its ability to model HOC.
2 Preliminaries
2.1 Task Definition
Consider a classic CF setting with implicit feedback for Top-K rec-
ommendation, where only binary values indicating the interactions
between users and items are available. Our goal is to model a sim-
ilarity function B¹D8ºreflecting the degree of interest user Dhas
in item8. In a Top-K recommendation scenario, the recommender
presents a user with K items, selected based on their similarity to
the user’s demonstrated interests.
2.2 Notation
We define some necessary notations for this paper. Let Urepresent
the set of users and Idenote the set of items. Let dataset D=f¹D181º¹D282º¹D=8=ºgdenotethesetofobservedinteractions
betweenusersanditems, D*=fD1D2D=gdenotethemultiset
of users observed in the dataset, D=f81828=gdenote the
multiset of items, jDj==be the number of interactions. The
interactions between users and items are represented by a matrix
R2RjUjjIjas follows:
RD8=(
1¹D8º 2 D ;
0¹D8º8D(1)
Theinteractionsofusersanditemscanberepresentedbyabipartite
graph [46], whose adjacency matrix A2R¹jUj¸jIjº¹jUj¸jIjºis
defined as
A=0 R
R>0
 (2)
LetD=3806¹31323=ºdenote the degree matrix where 38is
thedegreeofnode 8.LetL=D AdenotestheLaplacianmatrix.We
denotetheembeddingofuser Danditem8aseDande8,respectively,
regardless of the encoder and use the inner product function to
decode the similarity from embeddings, i.e., B¹D8º=e>De8.
2.3 Graph Convolution
Consider a graph G=¹VEºwith=nodes, where Vis the set of
verticesand Eisthesetofedges,anditsadjacencymatrix A2R==.
Agraph signal is a function x:V ! Rmapping a node to a
real value. Intuitively, a smooth signal should share similar values
acrossconnectednodes.Consequently,weusethe normalizedgraph
quadratic form [27] to measure the smoothness of a graph signal
defined as(¹xº=x>Lxkxk2=Í
8989 G8 G92kxk2A low
value(¹xºindicates a smooth signal x. Since the Laplacian matrix
Lis real and symmetric, the eigendecomposition yields L=UU>
where =3806¹_1_2_=ºis a diagonal matrix whose entries
are eigenvalues with _1_2    _=,U=»u1u2 u=¼
is a matrix of eigenvectors, u82R=is the unitary eigenvector
corresponding to eigenvalue _8. Observing that (¹u8º=_8, it can
be inferred that eigenvectors associated with smaller eigenvalues
tend to be smoother. This observation motivates us to define the
graph frequencies as the Laplacian eigenvalues and the Graph
Fourier Transformation (GFT) based on the eigenvectors U[29].
The GFT of signal xis represented as ˆx=U>x. This transformation
maps any signal xinto the graph frequency space, in which ˆx
serves as the new coordinate of xand reflects the importance of
frequencies. To end this section, we give the definition of graph
filterandgraph convolution.
Definition2.1(GraphFilter). Givenagraphanditscorresponding
Laplacian matrix, eigenvalues, and eigenvectors, the graph filter
H ¹Rºis defined as the following
H ¹Rº=1Õ
8=08L8 (3)
where8is the coefficient for L8.
Definition 2.2 (Graph Convolution). Given a graph signal xand
a graph filter H ¹Rº, graph convolution represents the process of
applying a graph signal to a graph filter, i.e., H ¹Rºx.
3426Unifying Graph Convolution and Contrastive Learning in Collaborative Filtering KDD ’24, August25–29, 2024, Barcelona,Spain
3Unifying Graph Convolution and Contrastive
Learning
In this section, we begin by defining the contrastive loss function
foranalysis.Subsequently,weexaminethelearningdynamicsofthe
contrastivelossanditsrelationshiptographconvolution.Following
this, we identify the conditions necessary to achieve equilibrium
within the contrastive loss framework. The section concludes with
a discussion on alternative forms of loss functions.
3.1 Definition of Contrastive Loss
In the context of contrastive learning, defining positive and nega-
tive samples with respect to a given anchor is crucial. In the field
of Computer Vision, positive samples are typically generated by
augmentingthesameimage,whereasaugmentationsfromdifferent
images are considered negative pairs [ 4,11]. Conversely, in CF,
positive samples for a user are identified as items with which the
user has interacted, while items with no interaction history are
deemed negative. Perhaps the most widely adopted contrastive loss
function in the domain of collaborative filtering is the Sampled
SoftMax (SSM) function [3, 42, 47]:
;SSM= 1
jDjÕ
¹D8º2Dlogexp¹e>De8ºÍ
92D8exp¹e>De9º (4)
Assuming the joint probability of a user Dshowing interest in an
item8, denoted as ?¹D8º, is proportional to exp¹e>De8º, the expres-
sion exp¹e>De8ºÍ
92D8exp¹e>De9ºcan be interpreted as the con-
ditional probability of user Dinteracting with item 8, given the
user,?¹D8jDº. This formulation implies that the SSM function is
designed to maximize the log-likelihood of this conditional proba-
bility for a given user. To enable a more straightforward analysis,
we propose an alternative loss function that directly maximizes the
log-likelihood of the joint probability from the observed data:
;= 1
jDjÕ
¹D8º2Dlogexp e>De8
Í
¹G~º2D *Dexp e>Ge~
= 1
jDjÕ
¹D8º2Dlogexp e>De8
Í
¹G~º2UI3G3~exp e>Ge~(5)
In Equation (5), for a positive pair ¹D8º, we consider all other possi-
ble combinations between D*andDas negative pairs. This nega-
tive sampling strategy is equivalent to the batch negative sampling
trick in the case of unlimited batch size. Moreover, the denomina-
torÍ
¹G~º2D *Dexp e>Ge~can be streamlined by focusing on
unique user-item pairs and their co-occurrence frequencies. This
leadstoasimplifiedexpressionÍ
¹D8º2UI3D38exp e>De8,which
accounts for the redundancy of pairs in the original formulation.
3.2 Learning Dynamics of Contrastive Loss
In this section, we will derive the learning dynamic for the con-
trastive loss. According to a recent analysis on contrastive learning
[36], a contrastive loss encompasses two principal components:
alignment and uniformity. The alignment component is designed to
minimize the distance between positive user-item pairs within the
embedding space, thereby enhancing their similarity. Conversely,
theuniformitycomponentaimstoincreasethedistanceamongneg-
ative pairs, thereby preventing the embeddings from convergingto a singular point. Furthermore, this decomposition simplifies our
discussion since the analysis of both the alignment and uniformity
components are congruent. We decompose Equation (5) as follows:
;=;align¸;uniform (6)
;align= 1
jDjÕ
¹D8º2De>
De8 (7)
;uniform =logÕ
¹D8º2UI3D38exp e>
De8 (8)
We commence by examining the learning dynamics associated
with the alignment loss function, as described by Equation (7). The
derivative of this function can be articulated as:
m;align
meD= 1
jDjÕ
82N¹Dºe8m;align
me8= 1
jDjÕ
D2N¹8ºeD(9)
where N ¹Dºdenotes the set of neighbors of user DandN ¹8ºfor
item8. The gradient descent update of embeddings at step C¸1is
eD¹C¸1º=eD¹Cº  Wm;
meD=eD¹Cº ¸W
jDjÕ
82N¹Dºe8¹Cº
e8¹C¸1º=e8¹Cº  Wm;
me8=e8¹Cº ¸W
jDjÕ
D2N¹8ºeD¹Cº(10)
whereWis the learning rate. Equation (10) delineates a message-
passing mechanism, illustrating how embeddings are propagated
across edges in the interaction graph. For a more compact repre-
sentation, we rewrite Equation (10) in a matrix form as
E¹C¸1º=E¹Cº ¸W
jDjAE¹Cº=
I¸W
jDjA
E¹Cº(11)
where E¹Cº 2R¹jUj¸jIjº 3is the concatenation of user and item
embeddingsatstep C.Similarly,wedelveintothelearningdynamics
associated with the uniformity objective, as represented by Equa-
tion (8). Let /=Í
¹G~º2UI3G3~exp e>Ge~. The derivatives
corresponding to this uniformity are expressed as:
m;uniform
meD=Õ
82I3D38exp e>De8
/e8
m;uniform
me8=Õ
D2U3D38exp e>De8
/eD(12)
The embedding update for the uniformity in a matrix form is
E¹C¸1º=E¹Cº  WA0¹CºE¹Cº= I WA0¹CºE¹Cº(13)
where
A0
89¹Cº=3839exp e>
8¹Cºe9¹Cº
Í
¹G~º2UI3G3~exp e>G¹Cºe~¹Cº(14)
if¹89ºis a user-item or item-user pair; or A0
89¹Cº=0if¹89º
is a user-user or item-item pair. Equation (12) elucidates another
message-passing mechanism across the complete graph weighted
by their respective degrees and embeddings. Now we combine
Equation (11) and Equation (13) to get the embedding update for
Equation (5):
E¹C¸1º=E¹Cº ¸WA00¹CºE¹Cº= I¸WA00¹CºE¹Cº(15)
3427KDD ’24, August 25–29, 2024, Barcelona, Spain Yihong Wuet al.
where
A00¹Cº=AjDj   A0¹Cº (16)
Equation (15) represents the whole learning dynamics for the con-
trastive loss defined in Equation (5).
3.3 Equivalence of Contrastive Loss and Graph
Convolution
In the preceding section, we derived the embedding update formula
associated with the contrastive loss, Equation (5). This section aims
to demonstrate that the embedding update process is functionally
equivalenttographconvolutions.Tobeginwith,weintroducesome
important propositions.
Proposition 3.1. Given a graph and its corresponding Laplacian
matrix L, eigenvalues of Lsuch that_1_2    _=,0W
1_=, graph filter I WLis a low-pass filter and graph filter I¸WLis
a high-pass filter.
Proposition3.2. Foranygraphsignal x,low-passfilter H!,high-
pass filter H, graph convolution with low-pass filter H!xincreases
signal’s smoothness on the graph, i.e., (¹H!xº (¹xº. Graph con-
volution with high-pass filter Hdecreases signal’s smoothness, i.e.,
(¹Hxº (¹xº.
All the proofs can be found in [ 15]. By definition, a low-pass
filter retains the low-frequency components of a graph signal while
suppressing the high-frequency ones. As previously discussed in
section 2.3, low-frequency components correspond to eigenvectors
associated with smaller eigenvalues, thereby leading to smoother
signals. Recall that we use the graph quadratic form (¹xºto mea-
sure the smoothness of graph signal x. Applying a low-pass filter
to a graph signal consequently results in a smoother outcome, with
the converse holding for high-pass filters. Interested readers are
encouraged to consult [ 15,27] for a more comprehensive under-
standing of graph filters. Having established these foundational
propositions,wearriveatatheoremthatarticulatestheequivalence
between contrastive loss and graph convolution.
Theorem 3.3. For a small enough learning rate W, graph filter
I¸WAjDjincreases signal’s smoothness on the user-item interaction
graph; graph filter I WA0¹Cºdecreases signal’s smoothness on the
affinity graph.
The proof can be found in Appendix A. With Theorem 3.3, we
deduce that the alignment loss, Equation (11), functionally acts as
a graph convolution to enhance the smoothness of embeddings
on the user-item interaction graph. Conversely, the uniformity
loss, Equation (13), operates as a graph convolution to reduce the
smoothness of embeddings on the affinity graph.
3.4 Equilibrium of Contrastive Learning
We demonstrate that the equilibrium in contrastive learning—the
convergence of the learning process—necessitates the alignment of
the model’s estimation with the empirical distribution derived from
thedata.Whenthesystemstabilizesintothisequilibrium,itadheres
to the condition: E¹1º=E¹1º ¸WA00E¹1º. From this, it follows
that: A00E¹1º=0. Since Ecannot be a zero matrix 0, it follows
that either A00is0orEis a null solution for A00. Summarizing the
above reasoning, we have the following theorem:Theorem 3.4. The contrastive loss reaches its equilibrium if and
only if A0=AjDj
The proof can be found in Appendix B. Observe that AjDjcan
be considered as the empirical distribution of interactions between
users and items denoted as %4¹D8º. Formally,
AD8
jDj=%4¹D8º=(
1jDj¹D8º 2 D ;
0 ¹D8º8D(17)
This empirical distribution assigns weights uniformly across every
observeduser-itempairing,devoidofanypriorassumptions.Onthe
other hand, A0¹Cºcan be considered as the Boltzmann distribution
reflecting the model’s estimation:
A0
D8¹Cº=%¹D8º=3D38exp e>De8
/ (18)
A Boltzmann distribution [ 1] is a probability distribution estimated
byembeddingsthroughanenergyfunction exp¹e>
8e9º.Theorem3.4
articulates that achieving equilibrium necessitates the alignment
of the model’s estimated probability with the empirical probabil-
ity. Moreover, by solving Equation A0=AjDj, we obtain an
expression for the similarity between the user and item e>De8=
logAD8
jDj3D38¸log/. This relationship can also be represented in
matrix form:
EE>=logD 1AD 1 logjDj ¸ log/ (19)
Equation (19) reveals that optimization under the contrastive loss
is equivalent to performing an implicit matrix factorization [ 20,25].
This equivalence provides an alternative perspective to assess the
quality of embeddings.
3.5 Discussion
In summary, we describe the learning dynamics of the contrastive
lossandidentifyitsequivalencewithgraphconvolutions.Moreover,
thealignmentlosscorrespondstothegraphconvolutionsmoothing
embeddingsontheuser-iteminteractiongraphwhiletheuniformity
loss corresponds to the graph convolution dispersing embeddings
on the affinity graph. Lastly, we demonstrate that the equilibrium
of contrastive loss requires the model’s estimation to match the
empirical distribution and that the optimization via the contrastive
loss is an implicit matrix factorization.
Furthermore, our analysis paradigm (learning dynamics and
equilibrium) is not limited to Equation (5). It could also be applied
to the SSM function (Equation (4)), the Mean Squared Error (MSE)
loss (Matrix Factorization [ 18]), the Bayesian Personalized Ranking
(BPR) loss [ 28], or even the recently proposed DirecAU loss [ 34]
which adopted negative samples as user-user and item-item pairs.
4 High-Order Connectivity Modeling
High-Order Connectivity (HOC) [ 37] (or High-Order Proximity
[44]) modeling is a desired property for CF methods and has been
the main motivation for graph-based CF methods. The HOC model-
ing requires the similarity between node embeddings should reflect
their closeness on the graph. Traditional methods such as Matrix
Factorization (MF) [ 18] and BPR [ 28] are often considered inad-
equate for modeling HOC. This perceived limitation stems from
these methods’ primary focus on distinguishing observed items
3428Unifying Graph Convolution and Contrastive Learning in Collaborative Filtering KDD ’24, August25–29, 2024, Barcelona,Spain
from unobserved ones, while largely neglecting to explicitly model
the latent relationships among unobserved entities [ 37,44]. To
mitigate this limitation, earlier graph-based collaborative filtering
approaches have employed random walks to derive HOC scores.
These scores are then directly utilized to inform recommendations
[24,32]. HOR-rec [ 44] enhances the BPR loss function by integrat-
ing HOC through weighted coefficients, alongside an expanded set
of positive and negative pairs derived from random walks. Subse-
quently, NGCF [ 37] and subsequent graph-based methods [ 12,41]
draw upon the Graph Convolutional Network (GCN) framework
[16], employing multiple convolutional layers to facilitate the prop-
agation of embeddings. These GCN-inspired models represent the
cutting edge in CF, underscoring the critical role of graph convolu-
tional layers in modeling HOC.
However, this paper posits different perspectives that the inte-
gration of the contrastive loss function within any encoder can
equivalently achieve HOC modeling, from the previous theoretical
analysis, and that a careless use of the convolutional layer might
lead to suboptimal results.
4.1 High-Order Connectivity from Contrastive
Loss
Consider E¹0ºas the embeddings randomly initialized at step 0.
By iteratively applying Equation (15) for )steps, we derive the
embeddings at time step )
E¹)º=) 1Ö
C=0 I¸WA00¹CºE¹0º (20)
effectively stacking )graph convolution operations. To a certain
extent, E¹)ºintheequationmirrorsa )-layeredGCNmodel—albeit
devoid of linear transformations and nonlinear activations between
layers.Notably,LightGCN[ 12]demonstratedthateliminatingthese
elements enhances model performance in the CF context. Typically,
)isconsiderablylarge,whichimpliestheapplicationofamultitude
of graph convolution operations on the embeddings. Such opera-
tions foster message exchange between nodes [ 7]. With massive
convolutions, not only is information from a node propagated to its
high-orderneighbors,butalsothemessageexchangerecursuntilan
equilibrium is attained. If we posit that the embeddings effectively
capture high-order connectivity, then it is reasonable to anticipate
that the embedding of a single node can assimilate information
from its high-order neighbors. While traditional graph-based meth-
ods utilize convolutional layers to assimilate neighbor information,
contrastive learning implicitly incorporates convolutions within
its learning process. Viewed from this angle, we advocate that con-
trastive learning bestows the capability of embeddings to model
high-order connectivity.
4.2 On The Necessity of Graph Convolutional
layers
Given that contrastive learning can effectively capture high-order
connectivity, relevant questions arise: How does the combination
of a contrastive objective and a graph-based model perform? andIs
there a genuine need for graph convolutional layer designs in CF? To
probe these concerns, we explore LightGCN [ 12], a state-of-the-
art graph-based model, with DirectAU [ 34], a recently introducedTable1:TheperformanceofnaiveembeddingandLightGCNwith
the DirectAU loss [ 34] on the Yelp2018 dataset. # T.L. denotes the
numberoftraininglayersand#I.L.denotesthenumberofinference
layers. No.S. denotes the experiment setting number.
No. S. # T.L. # I.L. Recall@20 NDCG@20
0 0 0 0.1097 0.0684
1 1 1 0.1083 0.0677
2 2 2 0.1088 0.0686
3 3 3 0.1070 0.0675
4 3 0 0.0909 0.0564
5 3 1 0.1103 0.0691
6 3 2 0.1103 0.0692
contrastive objective directly optimizing embedding alignment and
uniformity:
;align=E¹D8º?poskeD e8k2
;uniform =logEDD0?user4 2keD eD0k2
¸logE880?item4 2ke8 e80k2
;DirectAU =;align¸V;uniform(21)
4.2.1 A Brief Review of LightGCN. The LightGCN model adopts
the following graph convolution to propagate the embeddings:
E¹:¸1º=
D 1
2AD 1
2
E¹:º (22)
where:is the number of layers and E¹0ºis the learnable embed-
dings. The final embedding for the prediction of a  -layered Light-
GCN is a weighted sum of each layer’s embeddings
E= Õ
8=0U8˜AE¹8º=  Õ
8=0U8˜A8!
E¹0º (23)
where ˜A=D 12AD 12.
4.2.2 Empirical Results. We compare the naive embedding model
(LightGCN without linear filter) with the LightGCN model of dif-
ferent layers to examine their respective performance with the
recently proposed contrastive loss DirectAU [ 34], Equation (21). To
investigate its impact, we manipulate the number of layers applied
to the learnable embeddings during both the training and inference
phases. Specifically, ”training layers” refer to those utilized during
the model training process, whereas ’inference layers’ are applied
to derive the results.
As depicted in Table 1, there are two groups of experiments: set-
ting0,1,2,and3arethefirstgroup–theyareinaconsistentsetting
that the numbers of layers in both stages are the same. Setting 3, 4,
5,and6arethesecondgroup–theyhavethesamenumberoflayers
in training but different number of layers in inference. In the first
group, setting 0, 1, 2, 3, the naive model (No.0) marginally outper-
forms consistently layered LightGCN setups (No.1, No.2, No.3). In
the second group, comparing No.3 with No.5 and No.6, with exactly
the same learnable embeddings, the 1-layered inference model out-
performs the 3-layered one. All these results indicate an increase in
layersdoesnotalwaysenhanceperformance.Heetal.[ 12]reported
similar observations on the Yelp2018 and Amazon-Book datasets
3429KDD ’24, August 25–29, 2024, Barcelona, Spain Yihong Wuet al.
with the BPR loss function. We confirm this phenomenon in our
experiments.
This gives rise to a pivotal question: Are graph convolutional
layers indispensable for CF? Recent graph-based models for CF
have largely been inspired by GNN [ 12,23,37,41]. One of the
disadvantages of graph convolutional layers is that this operation
is discrete, meaning its inability to regulate the augmentation of
smoothness in the learning process. This lack of control results in
oversmoothing, smoother embedding but worse performance. In
contrast, contrastive learning provides a more controllable alter-
native by progressively updating while preventing collapse due
to negative samples. From this perspective, contrastive learning
appears to be a more intuitive choice for modeling HOC. Notably,
we are not discouraging any design of encoder, but do warn about
the misuse of graph convolutional layers.
5 A simple and effective approach
In preceding discussions, we presented a theoretical analysis un-
derscoring the HOC modeling potential of contrastive loss and
empirically elucidated the inherent risks associated with graph
convolutional layers. To corroborate this theoretical understanding,
we introduce Simple Contrastive Collaborative Filtering (SCCF),
a model based on a naive embedding model, without embedding
propagation, and a refined contrastive loss function. One notable
advantage of our method is its time complexity. The time complex-
ity of SCCF for obtaining a single node embedding is $¹1º, as it
relies on a look-up table. In contrast, GNN-based models have a
time complexity of $¹3;º, where3represents the greatest node
degree, and;denotes the number of layers. In the following section,
we provide a detailed elaboration on the SCCF method.
LetB=f¹D181º¹D<8<ºgbe a collection of data sampled
from Duniformly with batch size <andB#=f¹D889º j89=
1< gbe the collection of all possible user-item pair of batch B.
We design the following learning objective:
;= 1
<Õ
¹D8º2Blog¹sim¹eDe8ºº (24)
¸log©­
«1
<2Õ
¹D080º2B #sim¹eD0e80ºª®
¬
The similarity function sim ¹ºis defined as the following:
sim¹eDe8º=expe>De8
gjjeDjj2jje8jj2
¸exp 
1
ge>De8
jjeDjj2jje8jj22!

(25)
wheregis identified as the temperature parameter [ 4], and jj  jj 2
represents the !2norm. Intriguingly, Equation (25) can be inter-
preted as a mixture of two exponential kernels [ 31]. It is pivotal to
pinpoint some nuances differentiating Equation (5) from Equation
(25): first, the introduction of the temperature parameter g; second,
the shift from the inner product to cosine similarity; third, the in-
corporation of a second-order cosine similarity to transcend mere
linearity.
First, the integration of the temperature parameter has been em-
pirically shown to be crucial [ 3,4,11] as it modulates the relative
disparities between samples [ 35]. Equation (5) can be interpretedTable2: Statistics of datasets.
Dataset # Users # Items # Interactions Sparsity
Amazon-Beauty 22,363 12,101 198,502 99.93%
Gowalla 29,858 40,891 1,027,370 99.92%
Yelp2018 31,668 38,048 1,561,406 99.87%
Pinterest 55,187 9,912 1,445,622 99.74%
as a special case where gequals 1. Second, the cosine similarity can
be considered as the inner product between embeddings with !2
normalization. This normalization not only acts as a regularization
but also imposes constraints on gradients [ 3]. Moreover, the !2
normalization establishes a link between the inner product and
the Mean Squared Error (MSE), which is evident from the relation-
ship: ¹x ~º2=2 x>~forjjxjj2=jj~jj2=1. Given the softmax
function’s invariant nature [ 8], the normalized inner product is
congruent with MSE. This alignment relates our contrastive objec-
tive with the radial basis function kernel [ 14]. Third, the inclusion
of the second-order cosine similarity aims to infuse non-linearity
into our learning objective. As substantiated by [ 31], the mixture
of varied kernels (similarity functions) augments performance.
6 Experiments
This section is organized as follows: Initially, the experimental set-
tings are given. Subsequently, we show that our proposed model,
SCCF, demonstrates equivalent or superior performance in compar-
ison to several state-of-the-art methods. Later, we present evidence
indicating that the incorporation of graph convolution may lead to
a suboptimal performance. Lastly, we conduct ablation studies to
analyze the impact of various components in SCCF.
6.1 Experimental Settings
6.1.1 Datasets. We utilize four real-world datasets for our ex-
periments: Amazon-Beauty1comprises users’ online shopping
records on the Amazon website; Gowalla2consists of users’ check-
in information from a social networking website; Yelp20183in-
cludes information about businesses, reviews, and user data for
academic purposes; Pinterest4originally is proposed in [ 6] and
later adopted by [13] for image recommendation. Dataset statistic
information is provided in Table 2.
6.1.2 Evaluation Protocols. For each dataset, we randomly split
each user’s interactions into training/validation/test sets with a
ratio of 80%/10%/10%. For the evaluation of Top-K recommenda-
tion performance, we employ two metrics: Recall and Normalized
Discounted Cumulative Gain (NDCG). Recall@K assesses whether
the test ground-truth items are present in the retrieved Top-K list.
NDCG@K evaluates the position of the ground-truth items in the
Top-K list, considering the relevance and rank positions.
6.1.3 ImplementationDetail. To ensure fair comparisons, we em-
ployedtheRecBoleframework[ 48]acrossallexperimentalmethods.
The embedding size was fixed at 64 and initialized using Xavier
1https://cseweb.ucsd.edu/ jmcauley/datasets.html#amazon_reviews
2http://snap.stanford.edu/data/loc-gowalla.html
3https://www.yelp.com/dataset
4https://sites.google.com/site/xueatalphabeta/academic-projects
3430Unifying Graph Convolution and Contrastive Learning in Collaborative Filtering KDD ’24, August25–29, 2024, Barcelona,Spain
Table 3: Comparison between baselines and our method. The zdenotes significant improvements with t-test at ?005over all compared
methods. The best results are in bold and the second best results are underlined .
Dataset Metric BPR DAU LGCN-B LGCN-D Mult-VAE SimpleX NGCF DGCF SGL SCCF
Beauty Recall@20 0.1210 0.1344 0.1249 0.1437 0.1114 0.1188 0.1072 0.1142 0.1387 0.1470z
NDCG@20 0.0564 0.0665 0.0591 0.0683 0.0541 0.0550 0.0490 0.0538 0.0681 0.0713z
Gowalla Recall@20 0.1303 0.2020 0.1914 0.1994 0.1775 0.1114 0.1580 0.1825 0.2139 0.2185z
NDCG@20 0.0771 0.1145 0.1103 0.1136 0.1008 0.0557 0.0904 0.1076 0.1271 0.1304z
Yelp2018 Recall@20 0.0612 0.1097 0.0896 0.1070 0.0922 0.0715 0.0808 0.0852 0.1064 0.1160z
NDCG@20 0.0375 0.0684 0.0550 0.0675 0.0559 0.0422 0.0485 0.0527 0.0669 0.0728z
Pinterest Recall@20 0.1278 0.1477 0.1625 0.1764 0.1692 0.1376 0.1334 0.1571 0.1775 0.1776
NDCG@20 0.0645 0.0834 0.0840 0.0921 0.1010 0.0666 0.0675 0.0807 0.0952 0.1035z
normalinitializationforallmodels.Trainingepochsweresetto300,
from which the model with the best performance on the validation
set was selected. For the SCCF method, we used naive SGD as the
optimizer. The batch sizes were set to 10,000, 100,000, 100,000, and
60,000 for the Beauty, Gowalla, Yelp2018, and Pinterest datasets,
respectively. The temperature parameter gwas set to 0.25, 0.1, 0.2,
and 0.1 for the Beauty, Gowalla, Yelp2018, and Pinterest datasets,
respectively.
6.1.4 Baselines. We compare our method with several baseline al-
gorithms,including:(1) BPR[28]:Thenaiveembeddingmodelwith
BPR loss, where the negative item is randomly sampled from the
item set. (2) DAU[34]: The naive embedding model incorporates
the DirectAU loss, which directly optimizes alignment and unifor-
mity. (3)LGCN-B [12]: The LightGCN model is with the BPR loss
from the original paper. (4) LGCN-D [34]: The LightGCN model
combines with the DirectAU loss. (5) Mult-VAE [21]: A variational
autodecoder beyond linear models. (6) SimpleX [22]: A simple
but strong baseline consisted of a SVD++-like model and cosine
contrastive loss. (7) NGCF[37]: This model employs a GCN-like
architecture with learnable linear transformations and non-linear
activation functions. (8) DGCF[38]: Focusing on modeling diverse
relationships, This graph-based model focuses on modeling diverse
user intent disentanglement. (9) SGL[41]: This method introduces
three data augmentations for graph-based CF models.
6.2 Comparison with Other Baselines
Table 3 provides a comparison of the different models and loss
functions in the metric of Recall@20 and NDCG@20. A compre-
hensive table, Table 8, is provided in the Appendix for a detailed
comparison. Across all evaluated datasets, our method, SCCF, con-
sistently outperforms competing approaches, achieving the best
performance. Among the alternative models, LGCN-D and SGL
demonstrate substantial efficacy. Notably, despite SGL’s application
of three data augmentation techniques—edge drop, node drop, and
random walk—to bolster training, our SCCF model, without any
augmentations, still manages to exceed SGL’s performance. This
outcome underscores the effectiveness of the SCCF approach.
Furthermore, we can categorize the evaluated methods into two
distinct groups. The first group, comprising BPR, DAU, and SCCF,
implements a naive embedding model with specifically designed
loss functions. The second group includes all graph-based methods:
LGCN-B, LGCN-D, NGCF, DGCF, SGL, and SCCF. Within the firstgroup, the primary distinction lies in the design of the loss func-
tion. Our proposed loss functions, Equation (24), and DirectAU’s
loss, Equation (21), differentiate themselves by incorporating multi-
ple negative samples, in contrast to BPR’s single negative sample
approach. This multiplicity of negative samples may explain the
superior performance of our and DirectAU’s loss functions over
BPR’s. Specifically, our loss function utilizes user-item pairs as neg-
ative samples, whereas DirectAU employs user-user and item-item
pairs. This distinction suggests that our method, by directly expand-
ing the distances between user and item embeddings, may offer
advantages over DirectAU, which indirectly achieves this objective
by expanding distances between user-user and item-item. These
observations highlight the design of the loss function.
In the second group, all graph-based models implement graph
convolutional layers as a fundamental component. In contrast, our
SCCF model eschews graph convolutional layers yet achieves su-
perior performance across these graph-based approaches. This ob-
servation suggests that contrastive loss provides a more adaptable
mechanism for embedding propagation, potentially surpassing the
capabilities of graph convolutional layers. Furthermore, consider-
ing the prevalent assumption that graph convolutional layers are
effective in modeling HOC, the outperformance of SCCF invites
a reevaluation of this premise. Specifically, if SCCF, which relies
on contrastive loss, surpasses graph-based models in performance,
it implies that contrastive loss is either equally capable of model-
ing HOC or challenges the notion that HOC is advantageous for
CF tasks. The findings from this group indicate that graph convo-
lutional layers are not indispensable for HOC modeling. Instead,
they highlight the efficacy of contrastive loss in achieving, and
potentially exceeding, the modeling capabilities attributed to graph
convolutions.
6.3Comparison between NaiveEmbedding and
LightGCN
Although the preceding section demonstrates the effectiveness of
SCCF by comparing it with various graph-based models, this com-
parison might not be entirely fair, given that different models may
employ distinct loss functions. To more accurately assess the influ-
ence of graph convolutional layers on performance, we conduct a
comparative analysis employing various encoders (LightGCN with
different layers and the naive embedding model) under a consistent
contrastivelossfunction,Equation(24).AsTable4shows,thenaive
3431KDD ’24, August 25–29, 2024, Barcelona, Spain Yihong Wuet al.
model outperforms all LightGCN models, with significant improve-
menton theGowallaand Yelp2018datasets.A morecomprehensive
Table 7 is provided in the Appendix. One reason to account for this
phenomenon is that the graph convolutional layer introduces an
inductive bias conducive to modeling HOC. A weak loss function,
such as the BPR loss, may result in embeddings that lack sufficient
concentration; in such contexts, the inductive bias introduced by
graphconvolutioncanbebeneficialinenhancingfocus.Conversely,
when the model is equipped with a contrastive loss, embeddings
tend to be well-concentrated, rendering the additional inductive
bias unnecessary and increasing the risk of oversmoothing.
Table 4: Effectiveness of different encoders. The bold denotes the
bestresult.”LGCN-x”denotesLightGCNwithxlayers.”NE”denotes
the naive embedding model without any convolutional layer.
Dataset Metric LGCN-1 LGCN-2 LGCN-3 NE
BeautyRecall@20 0.1418 0.1437 0.1437 0.1470
NDCG@20 0.0669 0.0685 0.0684 0.0713
GowallaRecall@20 0.2020 0.2065 0.2004 0.2185
NDCG@20 0.1168 0.1203 0.1169 0.1304
Yelp2018Recall@20 0.1062 0.1079 0.1062 0.1160
NDCG@20 0.0665 0.0679 0.0667 0.0728
PinterestRecall@20 0.1712 0.1768 0.1676 0.1776
NDCG@20 0.0960 0.0992 0.0918 0.1035
6.4 Hyperparameter and Ablation Study
6.4.1 Impact of !2normalization. Table 5 displays the effective-
ness of various similarities during the training and inference stages.
Remarkably, employing cosine similarity during training and the
inner product during inference yields the most superior results.
The optimization using cosine similarity in training is more chal-
lenging than the inner product because it disregards the magnitude
of the embeddings. Practically speaking, popular items or active
users generally exhibit larger magnitudes and subsequently have
higher scores than their inactive counterparts. By disregarding
magnitude, we effectively mitigate popularity bias, enabling the
mining of patterns beyond mere frequency. Conversely, during
the inference stage, the magnitude of embeddings becomes pivotal
as it reflects the popularity of user/item, playing a crucial role in
recommendations.
6.4.2 Impact of temperature. As illustrated in Figure 1, the perfor-
mance of the model exhibits variation with respect to the tempera-
ture parameter. The temperature parameter gcontrols the smooth-
ness of the similarity distribution, thereby regulating the impact of
negative samples. A smaller value of gmakes the model more sensi-
tivetohardnegativesamples,astheycontributesignificantlytothe
loss. Conversely, as gincreases, the model becomes less sensitive
to individual samples and focuses more on the overall distribution.
Noticing that the optimal value of gmay vary across datasets, in-
dicates the importance of selecting the appropriate temperature
based on the specific characteristics of the dataset being used.
6.4.3 Impactofembeddingsize. AsFigure2shows,themodel’sper-
formance improves with the increase of embedding dimension andTable5:Comparisonofvarioussimilarityfunctionsduringthetrain-
ingandinferencestagesontwodatasets.”Cos”denotesthecosine
similarity and ”IP” denotes the inner product.
Dataset Training Inference Recall@20 NDCG@20
BeautyCos Cos 0.1127 0.0551
Cos IP 0.1470 0.0713
IP Cos 0.0390 0.0171
IP IP 0.0773 0.0370
GowallaCos Cos 0.1501 0.0785
Cos IP 0.2185 0.1304
IP Cos 0.0335 0.0163
IP IP 0.1487 0.0805
0.1 0.2 0.3 0.4 0.5 1.0
T emperature4.04.55.05.56.06.57.0NDCG@20%Beauty
Y elp
Figure 1: Performance with different temperatures on Amazon-
Beauty and Yelp2018 dataset.
64 128 256 512 1024
Embedding Size0.21720.23210.23980.24550.2473R ecall@20R ecall@20
64 128 256 512 1024
Embedding Size0.12940.13840.14470.14810.1491NDCG@20NDCG@20
Figure 2: Embedding size vs. Recall@20 (left) and NDCG@20 (right)
on the Gowalla dataset.
is saturated when the dimension reaches a certain degree, which
aligns with our common sense that more parameters, better per-
formance. The preference for lower dimensions can be attributed
not only to computational efficiency but also to early works on MF
like Funk’s SVD5and SVD++ [ 17], which essentially are low-rank
approximationsoftheinteractionmatrix.Thefoundationalassump-
tion of these methods is that low-rank decomposition, specifically
those leading singular values and their corresponding singular vec-
tors, suffices to approximate the original matrix accurately. At first
glance, our results might appear to challenge this low-rank para-
digm. One hypothesis proposed to explain these phenomena is the
independence assumption [ 20], which posits that only sufficiently
large dimensionalities can effectively decouple the correlations
among different user-item pairs. The violation of the independence
5https://sifter.org/ simon/journal/20061211.html
3432Unifying Graph Convolution and Contrastive Learning in Collaborative Filtering KDD ’24, August25–29, 2024, Barcelona,Spain
Table6:Theeffectivenessofthesquareofcosinesimilarityontwo
datasets, where ”w/” and ”w/o” denote the model with or without
the second-order similarity.
Beauty Pinterest
Recall@20 NDCG@20 Recall@20 NDCG@20
w/o 0.1419 0.0701 0.1734 0.1015
w/ 0.1470 0.0713 0.1776 0.1035
assumption might lead to an inaccuracy modeling of the empirical
distribution. In other words, a low-dimension embedding may not
be able to characterize the complex constraints on embeddings, i.e.,
the complex interaction between users and items, explaining the
suboptimal performance with low-dimension.
6.4.4 Impact of square of cosine similarity. In Table 6, it is evident
thatintegratingthesquareofcosinesimilarityenhancesthemodel’s
performance. While the primary performance can be attributed to
the first-order cosine similarity, the squared term still provides a
notable improvement. We postulate that this enhancement arises
from the second-order interaction between embeddings, thereby
facilitating a more refined representation.
7 Related Work
CollaborativeFiltering. Collaborative Filtering (CF) is a funda-
mental and important algorithm for recommender systems. Model-
based methods have gained more popularity over memory-based
methods since the latter often relies on heuristics. A notable ex-
ample of model-based methods is Matrix Factorization [ 18], which
decomposes the interaction matrix into two low-dimensional ma-
trices. NeuMF [ 13] and Multi-VAE [ 21] are two representative non-
linear methods for CF. Recently, GNN-based embedding models
have gained much attention in CF. Inspired by GCN [ 16], NGCF
[37] utilizes graph convolutional layers to propagate embeddings.
LightGCN [ 12] removes linear transformations and non-linear ac-
tivations in graph convolutional layers to improve performances.
Additionally, Shen et al . [30]provided a unified analysis through
low-pass filtering, and proposed an effective graph filtering model,
GF-CF. Existing GNN-based models in CF employ graph convo-
lutional layers to capture high-order connectivity. Nonetheless,
our findings reveal naive embedding with a contrastive objective
demonstrates comparable capability in modeling such connectivity.
Contrastive Learning in CF. The BPR [ 28] loss function is a pio-
neer of contrastive learning in CF. CLRec [ 49] uses the contrastive
loss function to reduce exposure bias through inverse propensity
weighting.DrawinginsightsfromBYOL[ 9],BUIR[ 19]incorporates
a momentum update, enabling the training of embedding encoders
devoid of negative samples. Wang and Isola [36]identified two
properties for contrastive learning, the alignment for positive sam-
ples and the uniformity of features’ distribution. Inspired by this
idea, Wang et al . [34]proposed the DirectAU loss as a means to
enhance the alignment and uniformity of embeddings in CF.
Graph Contrastive Learning in CF . Another significant topic is
graph contrastive learning, which aims to learn invariant represen-
tations through graph perturbation. SGL [ 41] proposed three graphaugmentation methods: node dropout, edge dropout, and random
walk. SimGCL [ 45] introduced the idea of adding uniform noise
to node embeddings. LightGCL [ 2] aligned node embeddings with
their SVD-augmented counterparts. However, all of these methods
rely on graph convolutional layers to implement graph augmenta-
tion.Forsimplicityandgenerality,ourdiscussiononthecontrastive
objective does not involve any graph augmentation and remains
independentofspecificmodels.Theinvestigationofmodel-agnostic
augmentation methods is left for future work.
Theory of Contrastive Learning. HaoChen et al . [10]elucidated
the role of contrastive learning in performing spectral clustering
on the augmentation graph. Taking the concept of augmentation
overlap into account, Wang et al . [40]illustrated how aligned data
augmentationsfacilitatetheclusteringofintra-classsamples.Wang
et al. [39]proposed that the learning dynamics of contrastive learn-
ing resonates with message-passing mechanisms on the augmenta-
tion graphs and affinity graph. While our analysis bears similarities
to that of Wang et al . [39], there are distinct differences in focus.
Wangetal .[39]concentratesongeneralcontrastivelearning,incor-
porating data augmentation techniques. In contrast, our research
specifically targets the CF setting, eschewing data augmentation in
favor of a deeper exploration of graph theory.
8 Conclusion
In this study, we reexamine graph convolution and contrastive
learning in the context of collaborative filtering and reveal the
equivalence between them. This equivalence offers a new perspec-
tive to analyze contrastive learning via graph theory. By doing so,
we show the capacity of contrastive learning for high-order con-
nectivity modeling. Moreover, we examine whether it is necessary
to add graph convolutional layers to model high-order connectivity.
We show that this is unnecessary.
Based on the above analysis, we propose a simple and effective
algorithm using a new contrastive loss, enabling the model to pro-
duceequivalentorevensuperiorperformancecomparedwithother
graph-based methods. This further confirms that a model using
contrastive loss can successfully capture high-order connectivity,
which was believed to be obtained only with graph convolution.
This paper is a first step in trying to better understand CF al-
gorithms using graph convolution and contrastive learning. We
believe the problem should be further investigated to gain more
insight into the models for designing better CF algorithms.
Acknowledgments
This work was partly supported by the NSERC discovery grant
and the Canada Research Chair on natural language information
processing and applications.
References
[1]David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. 1985. A learning
algorithm for Boltzmann machines. Cognitive science 9, 1 (1985), 147–169.
[2]Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: Simple
yet effective graph contrastive learning for recommendation. arXiv preprint
arXiv:2302.08191 (2023).
[3]Jiawei Chen, Junkang Wu, Jiancan Wu, Sheng Zhou, Xuezhi Cao, and Xiang-
nan He. 2023. Adap-tau: Adaptively Modulating Embedding Magnitude for
Recommendation. arXiv preprint arXiv:2302.04775 (2023).
3433KDD ’24, August 25–29, 2024, Barcelona, Spain Yihong Wuet al.
[4]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A
simple framework for contrastive learning of visual representations. In Interna-
tional conference on machine learning. PMLR, 1597–1607.
[5]Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive
learning of sentence embeddings. arXiv preprint arXiv:2104.08821 (2021).
[6]Xue Geng, Hanwang Zhang, Jingwen Bian, and Tat-Seng Chua. 2015. Learning
image and user features for recommendation in social networks. In Proceedings
of the IEEE international conference on computer vision. 4274–4282.
[7]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In International
conference on machine learning. PMLR, 1263–1272.
[8]Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . MIT
Press. http://www.deeplearningbook.org.
[9]Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan
Guo, Mohammad Gheshlaghi Azar, et al .2020. Bootstrap your own latent-a new
approach to self-supervised learning. Advances in neural information processing
systems 33 (2020), 21271–21284.
[10]Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. 2021. Provable guar-
antees for self-supervised deep learning with spectral contrastive loss. Advances
in Neural Information Processing Systems 34 (2021), 5000–5011.
[11]Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-
mentum contrast for unsupervised visual representation learning. In Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition . 9729–9738.
[12]Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for
recommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval . 639–648.
[13]Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international
conference on world wide web . 173–182.
[14]Thomas Hofmann, Bernhard Schölkopf, and Alexander J Smola. 2008. Kernel
methods in machine learning. (2008).
[15]Elvin Isufi, Fernando Gama, David I Shuman, and Santiago Segarra. 2024. Graph
filters for signal processing and machine learning on graphs. IEEE Transactions
on Signal Processing (2024).
[16]ThomasNKipf andMax Welling.2016. Semi-supervised classificationwith graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[17]Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted
collaborativefilteringmodel.In Proceedingsofthe14thACMSIGKDDinternational
conference on Knowledge discovery and data mining. 426–434.
[18]Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-
niques for recommender systems. Computer 42, 8 (2009), 30–37.
[19]Dongha Lee, SeongKu Kang, Hyunjun Ju, Chanyoung Park, and Hwanjo Yu. 2021.
Bootstrapping user and item representations for one-class collaborative filtering.
InProceedings of the 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval . 317–326.
[20]Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix
factorization. Advances in neural information processing systems 27 (2014).
[21]Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018.
Variational autoencoders for collaborative filtering. In Proceedings of the 2018
world wide web conference. 689–698.
[22]Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao,
and Xiuqiang He. 2021. SimpleX: A simple and strong baseline for collaborative
filtering. In Proceedings of the 30th ACM International Conference on Information
& Knowledge Management. 1243–1252.
[23]Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He.
2021. UltraGCN: ultra simplification of graph convolutional networks for recom-
mendation.In Proceedingsofthe30thACMInternationalConferenceonInformation
& Knowledge Management. 1253–1262.
[24]Bibek Paudel, Fabian Christoffel, Chris Newell, and Abraham Bernstein. 2016.
Updatable, accurate, diverse, and scalable recommendations for interactive ap-
plications. ACM Transactions on Interactive Intelligent Systems (TiiS) 7, 1 (2016),
1–34.
[25]Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 2018.
Network embedding as matrix factorization: Unifying deepwalk, line, pte, and
node2vec. In Proceedings of the eleventh ACM international conference on web
search and data mining. 459–467.
[26]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
etal.2021. Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InInternational conference on machine learning. PMLR, 8748–8763.
[27]Raksha Ramakrishna, Hoi-To Wai, and Anna Scaglione. 2020. A user guide to
low-pass graph signal processing and its applications: Tools and applications.
IEEE Signal Processing Magazine 37, 6 (2020), 74–85.[28]SteffenRendle,ChristophFreudenthaler,ZenoGantner,andLarsSchmidt-Thieme.
2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint
arXiv:1205.2618 (2012).
[29]Aliaksei Sandryhaila and José MF Moura. 2013. Discrete signal processing on
graphs. IEEE transactions on signal processing 61, 7 (2013), 1644–1656.
[30]Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, B Khaled Letaief, and
DongshengLi.2021. Howpowerfulisgraphconvolutionforrecommendation?.In
Proceedings of the 30th ACM international conference on information & knowledge
management. 1619–1629.
[31]ZhiquanTan,YifanZhang,JingqinYang,andYangYuan.2023. ContrastiveLearn-
ing Is Spectral Clustering On Similarity Graph. arXiv preprint arXiv:2303.15103
(2023).
[32]Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. 2006. Fast random walk
with restart and its applications. In Sixth international conference on data mining
(ICDM’06). IEEE, 613–622.
[33]Ferdinand Verhulst. 2006. Nonlinear differential equations and dynamical systems .
Springer Science & Business Media.
[34]Chenyang Wang, Yuanqing Yu, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu,
and Shaoping Ma. 2022. Towards representation alignment and uniformity in
collaborative filtering. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 1816–1825.
[35]Feng Wang and Huaping Liu. 2021. Understanding the behaviour of contrastive
loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 2495–2504.
[36]TongzhouWangandPhillipIsola.2020. Understandingcontrastiverepresentation
learning through alignment and uniformity on the hypersphere. In International
Conference on Machine Learning. PMLR, 9929–9939.
[37]Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural graph collaborative filtering. In Proceedings of the 42nd international ACM
SIGIR conference on Research and development in Information Retrieval . 165–174.
[38]Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng
Chua. 2020. Disentangled graph collaborative filtering. In Proceedings of the 43rd
international ACM SIGIR conference on research and development in information
retrieval. 1001–1010.
[39]Yifei Wang, Qi Zhang, Tianqi Du, Jiansheng Yang, Zhouchen Lin, and Yisen
Wang. 2023. A message passing perspective on learning dynamics of contrastive
learning. arXiv preprint arXiv:2303.04435 (2023).
[40]Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. 2022.
Chaos is a ladder: A new theoretical understanding of contrastive learning via
augmentation overlap. arXiv preprint arXiv:2203.13457 (2022).
[41]Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and
Xing Xie. 2021. Self-supervised graph learning for recommendation. In Proceed-
ings of the 44th international ACM SIGIR conference on research and development
in information retrieval. 726–735.
[42]Jiancan Wu, Xiang Wang, Xingyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu,
and Xiangnan He. 2022. On the effectiveness of sampled softmax loss for item
recommendation. arXiv preprint arXiv:2201.02327 (2022).
[43]Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervised
feature learning via non-parametric instance discrimination. In Proceedings of
the IEEE conference on computer vision and pattern recognition. 3733–3742.
[44]Jheng-Hong Yang, Chih-Ming Chen, Chuan-Ju Wang, and Ming-Feng Tsai. 2018.
HOP-rec: high-order proximity for implicit recommendation. In Proceedings of
the 12th ACM conference on recommender systems. 140–144.
[45]Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung
Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive
learning for recommendation. In Proceedings of the 45th international ACM SIGIR
conference on research and development in information retrieval . 1294–1303.
[46]HongyuanZha,XiaofengHe,ChrisDing,HorstSimon,andMingGu.2001. Bipar-
titegraphpartitioninganddataclustering.In Proceedingsofthetenthinternational
conference on Information and knowledge management. 25–32.
[47]An Zhang, Wenchang Ma, Xiang Wang, and Tat-Seng Chua. 2022. Incorporating
Bias-aware Margins into Contrastive Loss for Collaborative Filtering. arXiv
preprint arXiv:2210.11054 (2022).
[48]Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu
Pan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al .2021. Recbole:
Towards a unified, comprehensive and efficient framework for recommendation
algorithms.In Proceedingsofthe30thACMInternationalConferenceonInformation
& Knowledge Management. 4653–4664.
[49]Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021.
Contrastive learning for debiased candidate generation in large-scale recom-
mendersystems.In Proceedingsofthe27thACMSIGKDDConferenceonKnowledge
Discovery & Data Mining. 3985–3995.
[50]Zhihui Zhou, Lilin Zhang, and Ning Yang. 2023. Contrastive Collaborative
Filtering for Cold-Start Item Recommendation. arXiv preprint arXiv:2302.02151
(2023).
3434Unifying Graph Convolution and Contrastive Learning in Collaborative Filtering KDD ’24, August25–29, 2024, Barcelona,Spain
A Proof for Theorem 3.3
The core of proof lies in the inequality
(¹¹I¸DºxºkI¸Dmaxk2
kI¸Dk2(¹xº (26)
where xis the graph signal, Dis the degree matrix, Dmaxis a
diagonal matrix filled by the greatest degree. The key observation
is that(¹¹I¸WAjDjº xºis bounded by (¹¹I WLjDjº xº, a low-
pass filter; with small enough learning rate W, eventually (¹¹I 
WLjDjºxº (¹xº. Let’s prove Equation (26) first:
(¹¹I¸Dºxº=x>x¸x>DLx¸x>LDx¸x>DLD x
kI¸Dk2x>x
x>x¸x>DmaxLx¸x>LDmaxx¸x>DmaxLDmaxx
kI¸Dk2x>x
=kI¸Dmaxk2
kI¸Dk2(¹¹I¸Dmaxºxº
=kI¸Dmaxk2
kI¸Dk2(¹xº
The first and the third equality are obtained by the definition of
(¹º. The less than sign is obtained by the fact that ¹Dmax DºL
is a semi-positive matrix. The last equality is due to the fact that
(¹xº=(¹¹I¸Dmaxºxº. Replacing Iwith I WLjDjandDwith
WDjDjin Equation (26), we have
(¹¹I¸WAjDjºxº kI WLjDj ¸WDjDjk2
kI¸WAjDjk2(¹¹I WLjDjº xº
(27)
By Proposition 3.2 and Equation (27), we can obtain small enough
Wso that(¹¹I¸WAjDjºxº(¹xº:I¸WAjDjincrease signal’s
smoothness. The proof of I WA0¹Cºcan be obtained in a similar
way.
B Proof for Theorem 3.4
Whenthesystemstabilizesintoequilibrium,itadherestothecondi-
tion:E¹1º=E¹1º ¸WA00E¹1º. It follows that A00E¹1º=0. Since
Ecannot be a zero matrix 0, either A00is0orEis a non-zero solu-
tion for A00E=0when A00<0. We will demonstrate that in the
second case, the non-zero solution Eis unstable; any perturbation
at this stationary point causes the system to move away from it.
Consequently, the embedding system by the contrastive objective
reaches its equilibrium if and only if A00=0.
To determine the stability of the stationary points, we turn to
the knowledge of dynamical systems [ 33]. When3E3C=A00E=0,
the stability of Eis determined by A00: the embeddings are stable
if and only if all the eigenvalues of A00are equal to or less than 0.
Since our graph has no self-loop, the diagonal of A00are zeros. Thiszero trace indicates two cases: (1) all the eigenvalues are zeros; (2)
there exist at least one positive and one negative eigenvalues. Since
A00is symmetric, A00is not defective; i.e., it has full eigenvectors
correspondingtoitsnumberofrows.Itisimpossiblefor A00tohave
all zero eigenvalues unless A00=0, which contradicts our assump-
tion that A00<0. Therefore, there must be some eigenvalues that
are non-zero. Recall the fact that those non-zero eigenvalues must
sum up to zero, then at least there exists one positive eigenvalue
which makes the stationary point unstable.
C Tables
Tables 7 and 8 provide detailed experimental results with additional
metrics.
Table 7: Performance of different models on four datasets.
The bold denotes the best result. ”NE” denotes naive embed-
dings.
Dataset Metric LGCN-1 LGCN-2 LGCN-3 NE
BeautyRecall@10 0.0982 0.1028 0.1026 0.1060
Recall@20 0.1418 0.1437 0.1437 0.1470
Recall@50 0.2089 0.2130 0.2143 0.2132
NDCG@10 0.0555 0.0579 0.0577 0.606
NDCG@20 0.0669 0.0685 0.0684 0.0713
NDCG@50 0.0806 0.0827 0.0828 0.0849
GowallaRecall@10 0.1394 0.1422 0.1382 0.1552
Recall@20 0.2020 0.2065 0.2004 0.2185
Recall@50 0.3162 0.3203 0.3157 0.3318
NDCG@10 0.0987 0.1017 0.0989 0.1122
NDCG@20 0.1168 0.1203 0.1169 0.1304
NDCG@50 0.1448 0.1482 0.1450 0.1581
Yelp2018Recall@10 0.0656 0.0668 0.0659 0.0726
Recall@20 0.1062 0.1079 0.1062 0.1160
Recall@50 0.1923 0.1944 0.1920 0.2042
NDCG@10 0.0530 0.0542 0.0532 0.0583
NDCG@20 0.0665 0.0679 0.0667 0. 0728
NDCG@50 0.0915 0.0928 0.0914 0. 0982
PinterestRecall@10 0.1136 0.1175 0.1092 0.1200
Recall@20 0.1712 0.1768 0.1676 0.1776
Recall@50 0.2805 0.2922 0.2766 0.2911
NDCG@10 0.0778 0.0778 0.0734 0.0853
NDCG@20 0.0960 0.0992 0.0918 0.1035
NDCG@50 0.1232 0.1278 0.1189 0.1313
3435KDD ’24, August 25–29, 2024, Barcelona, Spain Yihong Wuet al.
Table 8: Comparison of different systems performance on four datasets. The zdenotes significant improvements with t-test at
?005over all compared methods. The bold denotes the best results. The second best results are underlined.
Dataset Metric BPR DAU LGCN-B LGCN-D Mult-VAE SimpleX NGCF DGCF SGL SCCF
BeautyRecall@10 0.0832 0.0973 0.0876 0.1004 0.0784 0.0831 0.0743 0.0776 0.0992 0.1060z
Recall@20 0.1210 0.1344 0.1249 0.1437 0.1114 0.1188 0.1072 0.1142 0.1387 0.1470z
Recall@50 0.1831 0.1922 0.1887 0.2113 0.1676 0.1833 0.1671 0.1722 0.2008 0.2132
NDCG@10 0.0466 0.0568 0.0494 0.0571 0.0445 0.0457 0.0404 0.0443 0.0578 0.0606z
NDCG@20 0.0564 0.0665 0.0591 0.0683 0.0541 0.0550 0.0490 0.0538 0.0681 0.0713z
NDCG@50 0.0691 0.0784 0.0721 0.0820 0.0656 0.0683 0.0612 0.0658 0.0809 0.0849z
GowallaRecall@10 0.0896 0.1360 0.1328 0.1370 0.1226 0.0637 0.1083 0.1265 0.1494 0.1552z
Recall@20 0.1303 0.2020 0.1914 0.1994 0.1775 0.1114 0.1580 0.1825 0.2139 0.2185z
Recall@50 0.2078 0.2989 0.2986 0.3103 0.2827 0.2056 0.2561 0.2877 0.3253 0.3318z
NDCG@10 0.0655 0.0975 0.0934 0.0955 0.0849 0.0427 0.0759 0.0915 0.1087 0.1122z
NDCG@20 0.0771 0.1145 0.1103 0.1136 0.1008 0.0557 0.0904 0.1076 0.1271 0.1304z
NDCG@50 0.0961 0.1400 0.1365 0.1407 0.1263 0.0785 0.1143 0.1332 0.1544 0.1581z
Yelp2018Recall@10 0.0364 0.0685 0.0549 0.0670 0.0563 0.0412 0.0480 0.0515 0.0666 0.0726z
Recall@20 0.0612 0.1097 0.0896 0.1070 0.0922 0.0715 0.0808 0.0852 0.1064 0.1160z
Recall@50 0.1147 0.1918 0.1657 0.1905 0.1686 0.1416 0.1511 0.1588 0.1895 0.2042z
NDCG@10 0.0292 0.0546 0.0433 0.0543 0.0437 0.0318 0.0373 0.0413 0.0536 0.0583z
NDCG@20 0.0375 0.0684 0.0550 0.0675 0.0559 0.0422 0.0485 0.0527 0.0669 0.0728z
NDCG@50 0.0530 0.0921 0.0772 0.0917 0.0780 0.0625 0.0689 0.0740 0.0909 0.0982z
PinterestRecall@10 0.0771 0.0981 0.1012 0.1100 0.1140 0.0795 0.0813 0.0976 0.1143 0.1200z
Recall@20 0.1278 0.1477 0.1625 0.1764 0.1692 0.1376 0.1334 0.1571 0.1775 0.1776
Recall@50 0.2369 0.2414 0.2846 0.3063 0.2812 0.2658 0.2481 0.2786 0.3023 0.2911
NDCG@10 0.0486 0.0677 0.0647 0.0711 0.0836 0.0483 0.0511 0.0619 0.0752 0.0853
NDCG@20 0.0645 0.0834 0.0840 0.0921 0.1010 0.0666 0.0675 0.0807 0.0952 0.1035z
NDCG@50 0.0916 0.1065 0.1144 0.1243 0.1288 0.0983 0.0959 0.1107 0.1261 0.1313
3436