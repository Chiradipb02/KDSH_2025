Unifie
d Dual-Intent Translation for Joint Modeling of Search
and Recommendation
Yuting Zhang∗
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
zhangyuting21s@ict.ac.cnYiqing Wu∗
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
iwu_yiqing@163.comRuidong Han
Meituan
Beijing, China
hanruidong@meituan.com
Ying Sun
Thrust of Artificial Intelligence, The
Hong Kong University of Science and
Technology (Guangzhou)
Guangzhou, China
yings@hkust-gz.edu.cnYongchun Zhu
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
zhuyc0204@gmail.comXiang Li
Wei Lin
Meituan
Beijing, China
lixiang245@meituan.com
linwei31@meituan.com
Fuzhen Zhuang†
Institute of Artificial Intelligence,
Beihang University
Beijing, China
Zhongguancun Laboratory
Beijing, China
zhuangfuzhen@buaa.edu.cnZhulin An†
Yongjun Xu
Institute of Computing Technology,
Chinese Academy of Sciences
Beijing, China
anzhulin@ict.ac.cn
xyj@ict.ac.cn
ABSTRACT
Recommendation systems, which assist users in discovering their
preferred items among numerous options, have served billions of
users across various online platforms. Intuitively, users’ interac-
tions with items are highly driven by their unchanging inherent
intents (e.g., always preferring high-quality items) and changing
demand intents (e.g., wanting a T-shirt in summer but a down
jacket in winter). However, both types of intents are implicitly
expressed in recommendation scenario, posing challenges in
leveraging them for accurate intent-aware recommendations. For-
tunately, in search scenario, often found alongside recommenda-
tion on the same online platform, users express their demand
intents explicitly through their query words . Intuitively, in
both scenarios, a user shares the same inherent intent and his/her
interactions may be influenced by the same demand intent. It is
thereforefeasibletoutilizetheinteractiondatafrombothscenarios
to reinforce the dual intents for joint intent-aware modeling. But
the joint modeling should deal with two problems: (1) accurately
modeling users’ implicit demand intents in recommenda-
tion; (2) modeling the relation between the dual intents and
the interactive items . To address these problems, we propose
∗Y
uting Zhang and Yiqing Wu are also at University of Chinese Academy of Sciences
†Fuzhen Zhuang and Zhulin An are Corresponding authors.
This
work is licensed under a Creative Commons Attribu-
tion International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671519a novel model named U nifie
d Dual-Intents
Translation
for joint
modelingofS ear
chandR e
commendation(UDITSR).Toaccurately
simulateusers’demandintentsinrecommendation,weutilizereal
queries from search data as supervision information to guide its
generation. To explicitly model the relation among the triplet <in-
herentintent,demandintent,interactiveitem>,weproposeadual-
intent translation propagation mechanism to learn the triplet in
thesamesemanticspaceviaembeddingtranslations.Extensiveex-
periments demonstrate that UDITSR outperforms SOTA baselines
both in search and recommendation tasks. Moreover, our model
has been deployed online on Meituan Waimai platform, leading
to an average improvement in GMV (Gross Merchandise Value) of
1.46% and CTR(Click-Through Rate) of 0.77% over one month.
CCS CONCEPTS
•Information systems →Recommender systems.
KEYWORDS
Joint learning, Search and recommendation, Dual intent modeling,
Intent translation
ACM Reference Format:
YutingZhang,YiqingWu,RuidongHan,YingSun,YongchunZhu,XiangLi,
Wei Lin, Fuzhen Zhuang, Zhulin An, and Yongjun Xu. 2024. Unified Dual-
Intent Translation for Joint Modeling of Search and Recommendation. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM,
New York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671519
6291
KDD’24, August 25–29, 2024, Barcelona, Spain Yuting Zhang et al.
1 INTRODUCTION
Aimingtohelpusersdiscoveritemsofinterestfromavastarrayof
options, recommendation systems have become an essential com-
ponent of various online platforms, such as e-commerce [26, 48,
49] and digital news services [7, 19, 35]. Existing recommendation
models [15, 16, 48, 49] typically exploit users’ implicit feedback,
such as click history, to predict their interests. For instance, tra-
ditional Collaborative Filtering (CF) [16] assumes that users will
interact with items similar to those with which they’ve previously
interacted. Furthermore, various models [48, 49] have been devel-
opedtocapturethesequentialdynamicsofusers’implicitfeedback
to model their evolving interests.
In practice, user feedback patterns in recommendation systems
are highly driven by their complex intents, which can be broadly
categorizedintounchanginginherentintentsandchangingdemand
intents. For example, Amy and Tom may have the same noodle de-
mand but choose different restaurants due to Amy’s inherent in-
tent for spicy flavors and Tom’s for sweet. Besides, a single user’s
interactions can vary due to their changing demands. Yet, these
intents are often implicitly expressed in the recommendation, pre-
senting a challenge for accurate intent-aware recommendations.
Existingintent-awarerecommendationmodels[5,23,50]typically
rely on users’ implicit feedback to learn their intents. However,
these models encounter a significant problem: different users may
have different inherent or demand intents despite similar histori-
calfeedback.AsshowninFigure1(a),Amy’sinteractionwithPizza
Hut might indicate a demand intent for pasta, while Tom may de-
mand pizza instead. Ideally, recommendation systems should sug-
gest pasta-related options to Amy and pizza-related ones to Tom.
However,withoutany explicitintent information,existingmodels
struggle to distinguish between these intents, resulting in inaccu-
rate recommendations.
Fortunately, in search services, which often accompany recom-
mendation services on the same online platform, users explicitly
express their demand intents through query words, as shown in
Figure 1(b). Such explicit search demand information can serve
as additional explicit information to assist in learning implicit de-
mandintentsforrecommendation.Indeed,bothsearchandrecom-
mendation tasks aim to comprehend users’ intents to aid them in
obtaining desired items [2]. In addition, in search scenario, users’
interactions are influenced not only by their explicit demand in-
tents but also by their personalized inherent intents. Yet, search
models typically focus on the match between search results and
users’ demand intents, often overlooking the impact of their per-
sonalized inherent intents, which are indeed significant [33]. Intu-
itively,inbothscenarios,ausermaintainsthesameinherentintent
and his/her behaviors are likely to be determined by the same de-
mand intent. Therefore, it is feasible to leverage interaction data
from both scenarios to reinforce or complement each other’s dual
intents for joint intent-aware modeling. Nevertheless, this joint
modeling is not trivial due to the following challenges:
(1)How to accurately model a user’s implicit demand in-
tent in recommendation with search data? A user’s demand
intentisimplicitwithinrecommendationbutisexplicitlyindicated
bysearchqueries. Ifthe changing demandintents in recommenda-
tion can be accurately generated, search and recommendation can
hamburger
(b) Search
(a) RecommendationAmyTomrecommendationinteractionsearch interactionwith query words**
hamburger
pastapizza
Figur
e 1: Examples of interaction behaviors in recommen-
dation and search scenarios.
be well modeled in a unified manner. The existing method, SRJ-
Graph [47], employs the unchanging padding query in recommen-
dationforunifiedmodeling.Thisapproachassumesanunchanging
demandintentacrossallrecommendationinteractions,whichmay
hinderrecommendationperformance.Tolearndemandintents,an
intuitiveapproachistosimplyincorporateusers’historicalqueries
asadditionaldemandinformationintotherecommendationmodel.
However,withoutexplicitsupervisiontoverifytheaccuracyofde-
mand intents, there may be a significant discrepancy between the
learned and the actual demand intents.
(2)How to couple the dual intents to model the relation
amongtheintentsandtheinteractiveitems? Bothinherentin-
tent and demand intent affect the interactive item. Intuitively, the
superimposition of inherent intents (e.g., preferring cheapitems)
and changing demand intents (needing a T-shirtin summer but a
down jacket in winter) leads to changing interactive results (inter-
acting with a cheap T-shirt andcheap down jacket, respectively). In
essence, the demand intent can be regarded as the changing devi-
ation from the inherent intent to the changing interactive item. A
common approach is to simply feed the two intents as input fea-
tures, but it cannot fully capture the relation between the dual in-
tents and the interactive item.
To tackle these challenges, we propose a novel model named
Unified Dual-Intent Translation for joint modeling of Search and
Recommendation (UDITSR). Overall, UDITSR comprises a search-
supervised demand intent generator and a dual-intent translation
module.Specifically,inthedemandintentgenerator,searchqueries
serve as supervision information, allowing us to learn and under-
stand a user’s changing demand intent for recommendations both
reliably and accurately. Moreover, we develop a dual-intent trans-
lation propagation mechanism. This mechanism explicitly models
the interpretable relation among the triplet elements --user’s <in-
herent intent, demand intent, interactive item> --within a shared
semanticspacebyemployingembeddingtranslations.Particularly,
wedesignanintenttranslationcontrastivelearningtofurthercon-
strain the translation relation. Extensive offline and online exper-
iments were conducted to demonstrate our model’s effectiveness.
Togaindeeperinsightsintotheeffectivenessofourmodel,wealso
provide a visual analysis of relevant intents.
6292Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
2 RELATED WORK
2.1 Recommendation and Search Models
Recommendation aims to filter items from vast candidate pools to
matchuser interests.Traditionalmodels,such asCollaborativeFil-
tering (CF), assume users with similar behaviors share item pref-
erences [6, 13, 15, 31]. Later studies [11, 34, 49] focus on decod-
ing users’ evolving interests from their historical behaviors, us-
ingtechniqueslikeDIN[49],whichemploysattentionmechanism
to connect past behaviors with current targets. Recognizing that
users’interactionsaredrivenbytheirintrinsicintents,recentstud-
ies[5,37,38,50]exploitusers’historicalbehaviorsequencestoun-
derstand their changing intents, aiming to better meet user needs.
For instance, KA-MemNN [50] uses item categories from user be-
havior as intent proxies, implementing memory networks for dy-
namic intent modeling. However, these approaches often deduce
intentsfrominteractionbehaviorsordirectlyequatebehaviorwith
intent,withoutminingrealintrinsicintents.Incontrast,ourmodel
utilizes the user’s actual demand intents in the search scenario as
supervision information to imitate the intents in recommendation.
Search and recommendation services often coexist on the same
platform [27]. Earlier research [2] suggests their goals are essen-
tiallyequivalent --helpingpeoplegettheitemstheywant,prompt-
ingstudiesontheirjointoptimization.Forexample,JSR[44]intro-
duces a shared-parameter framework, with user and item embed-
dingsshared.USER[43]treatsrecommendationbehaviorasaform
of search behavior with unchanging padding query, unifying the
modeling of search and recommendation sequences. Furthermore,
SRJgraph [47] constructs a unified graph from search and recom-
mendation behaviors, incorporating search queries and a padding
query for recommendation as attributes of user-item edges. These
models assume the query-related intents in recommendation are
unchangingwhilethematchingdegreebetweenthequeryandthe
candidate items significantly affects search performance. This as-
sumptioncreatesasignificantgapbetweenthemodelingofsearch
and recommendation, greatly hindering the effectiveness of joint
modeling approaches. Our model, however, adapts to learn per-
sonalizedandchangingquery-relatedintentsfordistinctuser-item
pairs in recommendation, thus enhancing the unification of joint
search and recommendation.
2.2 Graph Neural Network
Graph Neural Networks (GNNs) [32, 41] have gained tremendous
attention in recent years due to their remarkable ability to pro-
cessgraph-structureddata.Forinstance,GraphConvolutionalNet-
work (GCN) [18] employs a localized filter to aggregate informa-
tion from neighbors, and Graph Attention Network (GAT) [36]
leverages the attention mechanism to weigh the importance of
eachneighbornodeduringtheaggregationprocess.Sincethen,nu-
merous variants of GNNs [8, 42, 46] have been proposed to tackle
various types of graphs. Nowadays, Graph Neural Networks have
shown great potential in a wide range of applications, such as rec-
ommendation [14, 39, 40] and search [10, 22, 25] scenarios. In this
work,weproposeincorporatingdemandintentsthataregenerated
throughsearchsupervisioninrecommendationscenario,aswellas
explicitly stated search intents, into the construction of a unified
graph. Specifically, these demand intents serve as the attributesof the edges connecting users and items. Moreover, the invariant
node representations for a user across different interactions are
usedtoindicatetheirinherentintents.Basedonthegraph,wepro-
pose a novel dual-intent translation propagation for unified dual
intent-aware modeling.
3 PRELIMINARY
LetUandIdenote the universal sets of users and items in both
searchandrecommendationscenarios.Inordertodistinguishthese
two scenarios, we define the interaction records in each scenario
as follows:
Definition 1 .search scenario : In the search data X𝑠, each in-
teraction record 𝑥𝑠∈ X𝑠can be formulated as 𝑥𝑠=(𝑢, 𝑖, 𝑞 ), which
represents that user 𝑢∈ Uclicked item 𝑖∈ Iwith the explicit
query 𝑞. The query 𝑞can be segmented into several shorter terms
as𝑞=[𝑤1,· · ·, 𝑤|𝑞|],where 𝑤𝑖denotesthe 𝑖-thtermand |𝑞|isthe
number of terms in query 𝑞.
Definition 2 .recommendation scenario : In the recommenda-
tion data X𝑟, each interaction record 𝑥𝑟∈ X 𝑟can be formulated
as𝑥𝑟=(𝑢, 𝑖), which represents user 𝑢∈ Uclicked item 𝑖∈ I
without an explicit query.
Thereby, the double-scenario graph including all user click be-
haviors in both scenarios can be constructed as follows:
Definition 3.double-scenario graph : Given the set of all user
click behaviors in both scenarios, denoted as X=X𝑠∪ X 𝑟, the
double-scenario graph can be formulated as G=(U ∪ I ,E𝑠∪ E𝑟).
Each search edge 𝜖∈ E 𝑠corresponds to a record ( 𝑢, 𝑖, 𝑞) inX𝑠,
while each recommendation edge 𝜖∈ E𝑟corresponds to a record
(𝑢, 𝑖) inX𝑟.
In Figure 2(a), there is an example of our double-scenario graph.
For instance, user 𝑢1searches for query 𝑞12and then clicks item 𝑖2
in search scenario. Thus, an edge exists between nodes 𝑢1and 𝑖2,
with query 𝑞12assigned as an attribute of this edge. Likewise, in
recommendationscenario,whenuser 𝑢1clicksitem 𝑖1,anedgealso
existsbetweenuser 𝑢1anditem 𝑖1,butwithoutanyqueryattribute.
Based on the above definitions, the joint modeling of search and
recommendation can be defined as follows:
Problem definition. Given search data X𝑠, recommendation
dataX𝑟and double-scenario graph G, this task is to train a joint
model of search and recommendation to predict the most appro-
priate items 𝑖∈ Ithat user 𝑢∈ Uwill interact.
4 METHODOLOGY
In this section, we introduce UDITSR for dual intent-aware joint
modeling of search and recommendation, as depicted in Figure 2.
Webeginwiththemodel’s embedding layer inSection4.1.Then,in
Section 4.2, we detail a search-supervised demand intent generator
that leverages search query data to infer recommendation intents,
which allows us to convertthe double-scenario graph into a unified
graph.Utilizingthisgraph,wedescribe dual-intent translation prop-
agation to couple inherent intents and demand intents, enhanced
by a contrastive loss to constrain the translation relation. Finally,
the prediction layer and optimization are illustrated in Section 4.4.
6293KDD’24, August 25–29, 2024, Barcelona, Spain Yuting Zhang et al.
search edgedemand intent generator𝑢!ℒ"#ℒ!"
𝑒$%=𝑚𝑒𝑎𝑛(𝑒&%'!+̃𝑒()̃𝑒(+𝑦&,$𝑒&%=𝑚𝑒𝑎𝑛(𝑒$%'!−̃𝑒()MLP*/MLP+ℒ#intent translation contrastive lossBPR loss𝑒$∗𝑒&∗
+𝑞!!𝑢-𝑢!𝑞-!𝑞!-+𝑞!!𝑖-𝑖!𝑢!𝑞𝟏-𝑞𝟏/...𝑖!𝑞-𝟏𝑞/𝟏...searchrecommendationsearch-supervisedgenerationloss 𝑞!-𝑞!/𝑞!0𝑖-𝑞/!𝑢-𝑢!𝑖/𝑖0𝑢/+𝑞!!+𝑞--𝑞-!+𝑞/0𝑖!
𝑖!𝑙=1𝑙=2
𝑙=1𝑙=2𝑞!-𝑞!/𝑞!0𝑖-𝑞/!𝑢-𝑢!𝑖/𝑖0𝑢/𝑞-!𝑖!(a) Search-supervised demand intent generation(b) Dual-intent translationgenerated demand intentrecommendation edgerecommendation edge with generated demand̃𝑒(=4𝑒𝑚𝑏𝑞forsearch𝑒𝑚𝑏+𝑞forrecommendation
Figur
e2:OverallframeworkofourproposedUDITSR.The meanindual-intent translation representsthemean-poolingopera-
tioninEq.5.Forclarity,onlytwointeractionexamplesaredisplayedforeachgraphaggregationinthe dual-intent translation.
4.1 Embedding Layer
By feeding the user ID and item ID into the user and item embed-
ding matrices respectively, we can obtain the embeddings of user
𝑢and item 𝑖ase𝑢,e𝑖. Since each query 𝑞is a sequence of shorter
terms as [ 𝑤1,𝑤2,· · ·,𝑤|𝑞|], we can obtain the representation of
query 𝑞by combining the embeddings of its terms:
e𝑞=𝑓(e𝑤1,e𝑤2,· · ·,e𝑤|𝑞|), (1)
where e𝑤𝑘represents the embedding of the 𝑘-th query term in 𝑞
and 𝑓(·)denotes a combination function. In this study, we choose
theelement-wisesum-poolingoperationbecauseitisbothefficient
and effective for this combination through empirical analysis.
4.2 Demand Intent Generation
4.2.1 Search-Supervised Demand Intent Generator. Thenotabledif-
ference between search and recommendation is that a user explic-
itlyexpressesdemandintentsinsearch,whereasrecommendation
lacks such explicit intents. To bridge this gap, we propose to uti-
lize the abundant query information from search to supervise the
generationofusers’demandintentsinrecommendation.Belowwe
describe the generator in detail.
Sincetheuser’shistoricalqueries 𝑞𝑢=[𝑤𝑢
1, 𝑤𝑢
2,· · ·, 𝑤𝑢
|𝑞𝑢|]and
the item’s historical queries 𝑞𝑖=[𝑤𝑖
1, 𝑤𝑖
2,· · ·, 𝑤𝑖
|𝑞𝑖|]contain abun-
dant demand intent information, we leverage them as auxiliary in-
formation to simulate the user’s demand intent for recommenda-
tion. Similar to the processing of 𝑞in Eq. 1, we adopt the element-
wise sum-pooling operation to obtain the representation of 𝑞𝑢ase𝑞𝑢=|𝑞𝑢|Í
𝑘=1e𝑤𝑢
𝑘,where e𝑤𝑢
𝑘istheembeddingofthe 𝑘-thqueryterm
in𝑞𝑢.Since 𝑞𝑖containsquerywordsfrommultipleusers,weintro-
duce a user-aware gate mechanism to model personalized demand
intents. Particularly, the user-aware gating network 𝑔yields a dis-
tribution over the |𝑞𝑖|query words. The personalized representa-
tion of 𝑞𝑖is then formulated as the weighted sum of the embed-
dings of its query words, as follows:
Kg=Wg(e𝑢∥e𝑤𝑢
1∥ · · · ∥e𝑤𝑢
|𝑞𝑢|),
𝑔(𝑤𝑖
𝑘)=exp(Kg×e𝑤𝑖
𝑘⊤)
|𝑞𝑖|Í
𝑘=1e
xp(Kg×e𝑤𝑖
𝑘⊤),
e𝑞𝑖=|𝑞𝑖|Õ
𝑘=1𝑔(𝑤𝑖
𝑘)e𝑤𝑖
𝑘,(2)
where ·∥·denotestheconcatenationoperation; Wgisusedtomatch
the dimensions of vector e𝑤𝑖
𝑘and the concatenated vector. Then,
with user-related representations e𝑢,e𝑞𝑢and item-related repre-
sentations e𝑖,e𝑞𝑖, the user’s demand intent about the item can be
estimated as follows:
ˆe𝑞=MLP(e𝑢∥e𝑖∥e𝑞𝑢∥e𝑞𝑖), (3)
where MLPdenotes a multi-layer perceptron. Since the ground
truth queries in search data serve as the supervision information
6294Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
forgeneratingdemandintent,wedesignthegenerationlossasfol-
lows:
L𝑆𝐺=Õ
(𝑢,𝑖,𝑞) ∈X 𝑠(e𝑞−ˆe𝑞)2.(4)
4.2.2 Unified Graph. After generating the demand intents, each
recommendation record ( 𝑢, 𝑖) inX𝑟can be converted into a triplet
(𝑢, 𝑖,ˆ𝑞), where the embedding of ˆ𝑞corresponds to the generated
intents ˆe𝑞. For simplicity, we directly generate the representa-
tion of intent ˆe𝑞instead of indirectly predicting the specific
query ˆ𝑞. With the generated demand intents, the double-scenario
graphcan be converted into a unified graph . Specifically, an addi-
tional attribute ˆ𝑞is attached to each recommendation edge ( 𝑢, 𝑖) in
G. For brevity, we use e𝑞/ee𝑞to uniformly represent the real 𝑞/e𝑞
in search scenario and the generated ˆ𝑞/ˆe𝑞in recommendation sce-
nario correspondingly. Based on the unified graph, we implement
the unified modeling of recommendation and search below.
4.3 Dual-Intent Translation Propagation
To explicitly model the relation among the dual intents and the
interactive items, we propose a dual-intent translation module in-
spired by the triplet-based representation learning in knowledge
graphs [4]. Specifically, we use the user’s embedding representa-
tion, which remains inherent for a single user, to represent their
inherentintent.Thesearchqueryrepresentationandthegenerated
demandintentinrecommendationrepresenttheuser’sdemandin-
tent. The representation of an interactive item is given by its em-
bedding.Weassumethatauser’schanginginteractiveitemshould
beclosetotheirinherentintentpluschangingdemandintent.Con-
sequently, we aggregate the neighbor embeddings as follows:
e𝑙
𝑖=𝑚𝑒𝑎𝑛_𝑝𝑜𝑜𝑙𝑖𝑛𝑔 ({e𝑙−1
𝑢+ee𝑞,∀𝑢∈ N 𝑖}),
e𝑙
𝑢=𝑚𝑒𝑎𝑛_𝑝𝑜𝑜𝑙𝑖𝑛𝑔 ({e𝑙−1
𝑖−ee𝑞,∀𝑖∈ N 𝑢}),(5)
where N𝑢andN𝑖denote the neighboring nodes of user 𝑢and
item 𝑖respectively, in the unified graph; e0𝑢=e𝑢ande0
𝑖=e𝑖.
In particular, the subtraction aggregation operation, as opposed
to the addition operation, for aggregating the embeddings of user
neighboring nodes to simulate users’ inherent intents. Finally, the
weighted-pooling operation is applied to generate the aggregated
representations by operating on the propagated 𝐿layers:
e∗
𝑖=𝐿Õ
𝑙=0𝛼𝑙e𝑙
𝑖,e∗
𝑢=𝐿Õ
𝑙=0𝛼𝑙e𝑙
𝑢, (6)
where 𝛼𝑙indicates the importance of the 𝑙-th layer representation
in constituting the final embedding. Following LightGCN [14], we
set𝛼𝑙as1
(𝑙+1),
as the focus of our work is not on its selection.
Tofurtherconstrainthetranslationrelation,wedesignanintent
translation contrastive learning approach that adopts a margin-
based ranking criterion. Specifically, we aim to ensure that e∗𝑢+
ee𝑞≈e∗
𝑖(i.e., the ground truth interactive item e∗
𝑖should be near
to the translated intent e∗𝑢+ee𝑞), while the negative e∗
𝑖′should be
distant from e∗𝑢+ee𝑞, as follows:
L𝐶𝐿=Õ
(𝑢,𝑖,𝑖′) ∈𝑌−ln𝜎[(e∗
𝑢+ee𝑞−e∗
𝑖′)2− (e∗
𝑢+ee𝑞−e∗
𝑖)2],(7)whereee𝑞denotes the representation of real query in search or the
generated demand intent in recommendation for ( 𝑢, 𝑖) pair; 𝑌=
{(𝑢, 𝑖, 𝑖′)|(𝑢, 𝑖) ∈ 𝑅+,(𝑢, 𝑖′) ∈ 𝑅−}denotes the pairwise training
data where 𝑅+indicates the positive observed interaction set, and
𝑅−represents the randomly-sampled negative set; 𝜎(·)stands for
the sigmoid function.
4.4 Model Prediction and Optimization
After obtaining the representations e∗𝑢,e∗
𝑖,ee𝑞, we fuse them to ob-
tain the overall representation for the input sample 𝑥=(𝑢, 𝑖,e𝑞):
e𝑢,𝑖,e𝑞=e∗
𝑢∥e∗
𝑖∥ee𝑞. (8)
Then, two different MLPs are employed to make prediction for
search and recommendation tasks, respectively:
ˆ𝑦𝑢,𝑖,e𝑞=MLP s(e𝑢,𝑖,e𝑞)if𝑥∈ X𝑠,
MLP r(e𝑢,𝑖,e𝑞)if𝑥∈ X𝑟.(9)
We adopt pairwise training to train the model. Specifically, we
adopt the Bayesian Personalized Ranking (BPR) [30] loss to em-
phasize that the observed interaction should be assigned a higher
score than the unobserved one as follows:
L𝑜=Õ
(𝑢,𝑖,𝑖′) ∈𝑌−ln𝜎(ˆ𝑦𝑢,𝑖,e𝑞−ˆ𝑦𝑢,𝑖′,e𝑞′),(10)
where the representation of e𝑞′denotes the demand intent for the
negative pair ( 𝑢, 𝑖′). Finally, the overall loss Lis defined using
hyper-parameters 𝜆1and𝜆2as:
L=L𝑜+𝜆1L𝑆𝐺+𝜆2L𝐶𝐿. (11)
5 EXPERIMENTS
In this section, we present empirical results to demonstrate the
effectiveness of our proposed UDITSR. These experiments are de-
signedtoanswerthefollowingresearchquestions: RQ1Howdoes
UDITSRperformcomparedwithstate-of-the-artsearchandrecom-
mendation models? RQ2What are the effects of the demand in-
tentgeneratoranddual-intenttranslationmechanism inUDITSR?
RQ3WhycouldUDITSRperformbetter? RQ4HowdoesUDITSR
performinreal-worldonlinerecommendationswithpracticalmet-
rics?RQ5How do the hyper-parameters in UDITSR impact the
search and recommendation performance?
5.1 Experimental Settings
5.1.1 Dataset Description. Weconductedexperimentsontworeal-
worlddatasets,denotedasMT-LargeandMT-Smalldatasets1.These
two datasets are obtained from the Meituan platform, one of the
largesttakeawayplatformsinChina.Bothdatasetsspaneightdays
across two cities. Each sample in the datasets contains a user and
an item, and each search sample additionally contains a query.
Specifically, with 111,891 search and 65,035 recommendation in-
teractionscollected,our MT-Small datasetcomprises56,887users
and 4,059 items and the average number of split words per query
recordis1.6801.With1,527,869searchand1,168,491recommenda-
tioninteractionscollected,the MT-Large datasetcontains433,573
users and 22,967 items and the average number of split words per
1W
e collected this dataset because there was no public dataset that includes
both search and recommendation data. Our code and data will be available at
https://github.com/17231087/UDITSR.
6295KDD’24, August 25–29, 2024, Barcelona, Spain Yuting Zhang et al.
query is 1.5561. To evaluate model performance, we split the first
six days’ data for training, the seventh day’s data for validation,
andthelastday’sdatafortesting.Foreachgroundtruthtestrecord,
we randomly sampled 99 items that the user did not interact with
as negative samples.
Table 1: Network Configuration
Name V
alue
optimizer A
damW
batch size 256
learning
rate 1e-4
w
eight decay 1e-5
v
ocab size of words in querys 5,000
dimension
of embeddings 100
depth
of aggregation 2
numb
er of words per query 3
numb
er of words per user’s historical query 3
numb
er of words per item’s historical query 10
hidden
sizes of MLPin demand intent generator [200,100]
hidden
sizes of MLP s/MLP r [150,75]
5.1.2
Implementation Details. We implement all models using Py-
Torch2, a well-known software library for deep learning. In Sec-
tion5.6,wereporttheimpactofessentialhyper-parametersinour
model,includingthelossweights 𝜆1and𝜆2,andweutilizethebest
settings for these hyper-parameters. The remaining network con-
figurations are presented in Table 1. To ensure a fair comparison,
we apply the above-mentioned settings across all models. More-
over, we search for optimal values of the other hyper-parameters
of the baseline models as suggested in their respective original pa-
pers. Finally, we employ the early stopping strategy based on the
models’ performance on the validation set to avoid overfitting.
5.1.3 Evaluation Metrics. To evaluate our model’s performance,
weutilizefourwidely-usedrankingmetrics:Hit@K,NDCG@K[17]
(we set K as 5 by default), MRR [28] and Average position of the
Clicked items (Avg.C) [43]. Additionally, we adopt an accuracy
metric, AUC [12] for the recommendation task.
5.1.4 Baselines. In our work, we evaluate the performance of our
model with two groups of baselines to examine its effectiveness.
(1) Graph-free baselines
•NeuMF [15] combines traditional matrix decomposition with
the MLP to extract low-dimensional and high-dimensional fea-
tures simultaneously.
•DNNcombines the embedding layer described in Section 4.1
with the prediction layer described in Section 4.4.
•xDeepFM [21] consists of a compressed interaction network
(CIN) and an MLP for prediction, where CIN generates explicit
feature interactions at the vector-wise level.
•DIN[49]utilizesanattentionmechanismbetweenthehistorical
behavior sequence and the target item to model the evolving
interests.
2https://p
ytorch.org/•AEM[1] allocates different attention values to the previous be-
havior sequence based on the current search queries.
•TEM[3] feeds the sequence of query and user behavior history
into a transformer layer to extract the search intents.
•JSR[45] integrates neural collaborative filtering and language
modeling to reconstruct query text descriptions, enabling the
joint model of search and recommendation.
•SimpleX [24] is a simplified variant of the two-tower model
with user behavior modeling.
•MGDSPR [20] utilizes an attention mechanism to model the
relationship between users’ query multi-grained semantics and
their personalized behaviors for prediction.
(2) Graph-based baselines
•GAT[36] utilizes the attention mechanism to measure the im-
portance of neighbor nodes during the aggregation process.
•NGCF[39]enhancestheGraphConvolutionalNetworks(GCN)
by incorporating user-item interactions.
•LightGCN [14]streamlinesGCNby relyingsolelyon neighbor-
hoodaggregationtocapturecollaborativefiltering,omittingfea-
ture transformation and non-linear activation components.
•GraphSRRL [22]exploitsthreespecificstructuralpatternswithin
a user-query-item graph.
•SRJGraph [47] incorporates padding queries for recommenda-
tion and search queries as attributes into interaction edges, en-
abling joint modeling of both tasks.
•DCCF[29] leverages an adaptive self-supervised augmentation
to disentangle intents behind user-item interactions.
Specifically, NeuMF, xDeepFM, DIN, DCCF, SimpleX, NGCF and
LightGCNareproposedforthe recommendationtask ,whileAEM,
TEM, MGDSPR and GraphSRRL are proposed for the search task.
JSR and SRJGraph are designed for joint learning of both tasks.
To adapt these baselines for both tasks, real query representa-
tions for search and padding query representations for rec-
ommendation are incorporated into the prediction layer de-
scribed in Section 4.4. Previous studies [44, 47] have demonstrated
that joint optimization of search and recommendation models can
improve performance, so all baselines are directly trained on both
search and recommendation data. All baselines use the same set-
tings for the embedding layer and the prediction layer, and the
interaction graph is built on both search and recommendation in-
teractions.
5.2 Overall Performance Comparison (RQ1)
WepresenttheresultsonthetwoadopteddatasetsinTable2.From
the results, we can observe that:
•UDITSR significantly outperforms all the competitive baselines
onbothtasks.Specifically,comparedtothebest-performingbase-
lines,UDITSRgainsanaverageimprovementof6.22%and3.06%
in the search and recommendation tasks, respectively.
•Mostgraph-basedmethods,suchasNGCF,LightGCN,andGraph-
SRRL, perform well in both tasks, potentially due to their ability
to effectively capture complex high-order interactive patterns.
•SRJgraph assumes that query-related intents in recommenda-
tionremainunchangingwhereasinsearch,thematchingdegree
6296Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Dataset Mo
delSear
ch Re
commendation
Hit@5
NDCG@5 MRR Avg.C ↓Hit@5
NDCG@5 MRR AUC
MT
-SmallNeuMF 0.5510
0.4264 0.4150 12.1907 0.3147
0.2306 0.2374 0.8160
DNN 0.5877
0.4594 0.4465 9.6208 0.3241
0.2177 0.2246 0.8150
xDe
epFM 0.5053
0.3886 0.3815 14.3603 0.3184
0.2139 0.2218 0.8155
DIN 0.5892
0.4726 0.4613 11.6023 0.3632
0.2510 0.2545 0.8213
AEM 0.5053
0.3666 0.3568 11.7953 0.3967
0.2703 0.2686 0.7982
TEM 0.5362
0.4185 0.4084 13.6472 0.2933
0.1970 0.2078 0.7947
JSR 0.6143
0.4828 0.4678 8.6276 0.3460
0.2448 0.2457 0.7532
SimpleX 0.6237
0.4864 0.4699 8.0841 0.3314
0.2288 0.2336 0.8081
MGDSPR 0.6150
0.4743 0.4570 8.5362 0.2974
0.2032 0.2122 0.7862
GA
T 0.6025
0.4679 0.4497 10.8707 0.4202
0.3109 0.3032 0.7935
NGCF 0.6418
0.5173 0.5000 9.7943 0.4564
0.3346 0.3284 0.8232
LightGCN 0.6665
0.5402 0.5195 9.9139 0.4577 0.3296
0.3185 0.8174
GraphSRRL 0.6688 0.5267
0.5042 8.0724 0.4540
0.3249 0.3159 0.7883
SRJgraph 0.6186
0.4850 0.4647 12.1989 0.4140
0.3074 0.2997 0.7474
DCCF 0.5013
0.3760 0.3615 19.6477 0.4323
0.3380 0.3304 0.7239
UDI
TSR 0.7008*
0.5691* 0.5470* 7.5257* 0.4841*
0.3528* 0.3422* 0.8285*
Impr
.%4.7847
5.3499 5.2936 6.7725 5.7680
4.3787 3.5714 0.6438
MT
-LargeNeuMF 0.8668
0.7855 0.7682 3.3001 0.5390
0.4235 0.4129 0.8573
DNN 0.8788
0.7874 0.7664 3.0520 0.5153
0.3962 0.3881 0.8610
xDe
epFM 0.8552
0.7417 0.7147 4.0106 0.4926
0.3897 0.3828 0.8077
DIN 0.8914
0.7934 0.7693 2.7283 0.6005
0.4489 0.4292 0.9082
AEM 0.8760
0.7654 0.7389 2.9007 0.5865
0.4597 0.4435 0.8940
TEM 0.8611
0.7522 0.7269 3.4096 0.5031
0.3526 0.3419 0.8899
JSR 0.8691
0.7748 0.7537 3.0903 0.5023
0.3789 0.3683 0.8393
SimpleX 0.8896
0.7895 0.7651 2.6466 0.5004
0.3790 0.3691 0.8640
MGDSPR 0.8726
0.7709 0.7473 3.0325 0.5412
0.4037 0.3888 0.8751
GA
T 0.8761
0.7796 0.7572 2.9530 0.5880
0.4540 0.4347 0.8706
NGCF 0.8821
0.7892 0.7670 2.7377 0.6325 0.4966
0.4780 0.9096
LightGCN 0.8937
0.8016 0.7795 2.4076 0.6158
0.4785 0.4593 0.8920
GraphSRRL 0.8966 0.7992
0.7755 2.3504 0.6106
0.4726 0.4543 0.8891
SRJgraph 0.8836
0.7883 0.7659 2.6849 0.5873
0.4494 0.4315 0.8942
DCCF 0.8568
0.7459 0.7205 3.2666 0.6201
0.5007 0.4802 0.8292
UDI
TSR 0.9178*
0.8382* 0.8183* 1.9819* 0.6566*
0.5157* 0.4936* 0.9146*
Impr
.%2.3645
4.5659 4.9775 15.6782 3.8103
2.9958 2.7905 0.5497
T
able 2: Overall performance on both datasets. ↓represents that a smaller Avg.C metric value indicates better performance.
Impr.% indicates the relative improvements of the best-performing method (bolded) over the strongest baselines (underlined).
* indicates 0.05 significance level from a paired t-test comparing UDITSR with the best baselines.
between the query and the candidate items is deemed crucial.
Consequently, such an assumption may limit the model’s per-
formance, particularly when compared to our UDITSR, which
learns and adapts to changing query-related intents.
5.3 Ablation Study (RQ2)
As the demand intent generator and dual-intent translation propa-
gationarethecoreofourmodel,weconductthefollowingablation
studies to investigate their effectiveness:
•UDITSR(w/o DeIntGen) masks all generated demand intents ee𝑞
by assigning the embedding of the padding query to each rec-
ommended record.•UDITSR(w/oIntTrans)replacesdual-intenttranslationwithclas-
sicalmean-poolingpropagationbetweentheuseranditemnodes.
•UDITSR(w/o DeIntGen & IntTrans) removes both the demand
intent generator and dual-intent translation propagation, as de-
scribed in the two ablation studies above.
From the results of ablation studies in Table 3, we can find that:
•UDITSR(w/o DeIntGen & IntTrans) performs the worst on both
search and recommendation tasks, suggesting that the signifi-
cant improvement of our model stems from our proposed de-
mand intent generator and dual-intent translation propagation.
•UDITSR(w/oIntTrans)performsworsethantheoriginalUDITSR,
highlightingtheeffectivenessofourproposedintenttranslation
propagation mechanism.
6297KDD’24, August 25–29, 2024, Barcelona, Spain Yuting Zhang et al.
Dataset AblationSear
ch Re
commendation
Hit@5
NDCG@5 MRR Avg.C ↓Hit@5
NDCG@5 MRR AUC
MT
-SmallUDI
TSR(w/o DeIntGen & IntTrans) 0.6479
0.5152 0.4949 10.1960 0.4185
0.3052 0.3032 0.8179
UDI
TSR(w/o IntTrans) 0.6454
0.5130 0.4924 10.5527 0.4352
0.3206 0.3154 0.8225
UDI
TSR(w/o DeIntGen) 0.6959
0.5543 0.5307 8.1389 0.4510
0.3237 0.3178 0.8186
UDI
TSR 0.7008*
0.5691* 0.5470* 7.5257* 0.4841*
0.3528* 0.3422* 0.8285*
MT
-LargeUDI
TSR(w/o DeIntGen & IntTrans) 0.8660
0.7586 0.7337 3.1185 0.6183
0.4818 0.4623 0.9031
UDI
TSR(w/o IntTrans) 0.8870
0.7866 0.7623 2.6399 0.6228
0.4879 0.4696 0.9061
UDI
TSR(w/o DeIntGen) 0.9089
0.8192 0.7969 2.2053 0.6303
0.4890 0.4685 0.9058
UDI
TSR 0.9178*
0.8382* 0.8183* 1.9819* 0.6566*
0.5157* 0.4936* 0.9146*
T
able 3: Ablation study on our proposed search-supervised demand intent generator and dual-intent translation propagation.
(
a) Intents in UDITSR(w/o
IntTrans) for search data
(
b) Inherent intents in
UDITSR for search data
(
c) Translated intents in
UDITSR for search data
(
d) Intents in UDITSR(w/o
IntTrans) for recommenda-
tion data
(
e) Inherent intents in
UDITSR for recommenda-
tion data
(f)
Translated intents in
UDITSR for recommenda-
tion data
Figure 3: t-SNE visualization of learned intents and interac-
tive items. Blue dots represent the interactive items(i.e., e∗
𝑖)
and orange dots represent the learned intents.
•UDITSR(w/oDeIntGen)performsworsethanUDITSR,especially
for recommendation task, indicating that the search-supervised
demandintentgeneratorcanhelpUDITSRlearnimplicitintents
more accurately in recommendation.
5.4 Intent Visualization (RQ3)
In this section, we visualize the learned intents to further inves-
tigate why our model performs better. We compare UDITSR with
itsablatedversionwithoutthedual-intenttranslationpropagation
(UDITSR(w/oIntTrans),detailedinSection5.3).Weemploythede-
fault setting of the t-SNE [9] provided by Scikit-learn to visualize
thedistributionofthelearnedintentsandtheinteractiveitems.For
clarity, we randomly sample 100 positive records from the search
andrecommendationtestdatasetsrespectivelyforplotting.Specif-
ically, in UDITSR(w/o IntTrans), user embeddings (i.e., e∗𝑢) are re-
gardedasthelearnedintents,asshowninFigures3(a)and(d),sim-
ilar to preference/intent captured by models like NGCF and Light-
GCN. UDITSR, however, couples inherent and demand intents via
intent translation to form the final intents (i.e., e∗𝑢+ee𝑞), as shown
0.00.51.01.52.00.620.650.680.71search
0.350.400.450.50
recommendation
Hit@5
search
recommend
0.00.51.01.52.00.460.500.540.58search
0.240.280.320.36
recommendation
NDCG@5
search
recommendFigur
e 4: Performance w.r.t 𝜆1of search-supervised demand
intent generator for search and recommendation tasks.
0.00.20.40.60.81.00.680.690.700.71search
0.470.480.490.50
recommendation
Hit@5
search
recommend
0.00.20.40.60.81.00.5450.5550.5650.575search
0.340.350.360.37
recommendation
NDCG@5
search
recommend
Figur
e 5: Performance w.r.t 𝜆2of the intent translation con-
trastive learning for search and recommendation tasks.
in Figure 3(c) and (f). To ensure a fair comparison, we present the
inherent intents (i.e., e∗𝑢) learned by UDITSR in Figure 3(b) and (e).
Ideally, the distribution of learned intents should match that of
interactiveitemrepresentations.Figures3(a)and(d)revealthatthe
intents learned by UDITSR(w/o IntTrans) are concentrated while
the positive interactive items are scattered, indicating a mismatch.
Meanwhile, the inherent intents learned by UDITSR are relatively
scattered, indicating that our model can better learn the personal-
ized inherent intents of different users. However, there still exist
obvious gaps between the intents and items, highlighting the ne-
cessity of learning demand intents. In contrast, the translated in-
tents learned by UDITSR are scattered in the space of the target
interactive items, demonstrating its excellent intent modeling ca-
pability.Thebetterfitofthedistributionoftranslatedintentstothe
targetinteractivedistributioncouldbethefundamentalreasonfor
the better overall performance of UDITSR.
5.5 Online A/B test (RQ4)
Owing to the distinct architectural differences between the search
and recommender systems on the Meituan Waimai platform, we
6298Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
haveinitiallyfocusedourmethodologicaldeploymentonthehome-
page recommender systems. We conducted a month-long online
A/B test from December 18, 2023, to January 17, 2024. Specifically,
we utilized the search data with query information to guide the
learning of user demand intent representation and leveraged the
learnedgraphembeddingsasadditionalfeaturesinthedownstream
recommendation model. The control bucket was the original on-
line recommendation method of Meituan Waimai platform. The
deploymentofourmethodincreasedthe GMV(GrossMerchandise
Volume) by 1.46% and CTR(Click-Through Rate) by 0.77%, which
demonstrated the effectiveness of our method. In the future, we
will continue to conduct comprehensive online experiments that
encompass both search and recommendation scenarios.
5.6 Hyper-Parameter Studies (RQ5)
Inthissection,weconductexperimentsonthelossweights( 𝜆1,𝜆2)
in Eq. 11 on MT-Small dataset to explore their impact.
(1) Loss weight of the demand intent generator ( 𝜆1). We vary
𝜆1within {0,0.5,1.0,1.5,2.0}. The results in Figure 4 indicate that
performance improves and then declines with increasing 𝜆1. With
𝜆1=0, the demand intent generator degenerates to an ordinary
generatorwithoutanysearch-supervisioninformation.Allmodels
with search supervision (i.e. 𝜆1≠0) outperform models without
it (i.e. 𝜆1=0). This may stem from UDITSR’s effective learning
of user demand intents through explicit supervision from
search. Furthermore, our model excels across most metrics for
both search and recommendation tasks at 𝜆1=1.5. Thus, we set
𝜆1=1.5for MT-Small dataset. After a similar experiment con-
ductedonMT-Largedataset,weadoptthebest-performingsetting
(𝜆1=1).
(2)Lossweightoftheintenttranslationcontrastivelearning( 𝜆2).
To investigate the impact of our proposed intent translation con-
trastive learning, we vary 𝜆2in{0.0,0.2,0.4,0.6,0.8,1.0}. Overall,
the performance initially increases and then decreases with the in-
crease of 𝜆2. Particularly, our model with 𝜆2set in {0.2, 0.4, 0.6}
outperforms the version without translation contrastive learning
𝜆2=0on all metrics, demonstrating a proper loss weight of
intent translation contrastive learning can aid in intent re-
lationmodeling .Theoptimal 𝜆2forsearchis0.2whileforrecom-
mendationtask,itis 0.2forHit@5and 0.4forNDCG@5.Therefore,
weset 𝜆2=0.2forMT-Smalldataset.Also,afterconductingasimi-
larexperimentonMT-Largedataset,weadoptthebest-performing
setting 𝜆2=0.4.
6 CONCLUSION
Thispaperintroducedanovelapproachtounifiedintention-aware
modeling for joint optimization of search and recommendation
tasks. We recognized that user behaviors were motivated by their
inherentintentsandchangingdemandintents.Toaccuratelylearn
users’ implicit demand intents for recommendation, we innovated
ademandintentgeneratorthatutilizedexplicitqueriesfromsearch
data for supervised learning. Furthermore, we proposed a dual-
intent translation propagation mechanism for interpretive model-
ing of the relation between users’ dual intents and their interac-
tive items. In particular, we introduced an intent translation con-
trastive method to further constrain this relation. Our extensiveoffline experiments demonstrated that UDITSR outperformed the
leading baselines in both search and recommendation tasks. Be-
sides, online A/B tests further confirmed the superiority of our
model.Finally,theintentvisualizationclearlyexplainedthedeeper
reason for the remarkable improvement of our model.
ACKNOWLEDGMENTS
This research work is supported by the National Key Research and
Development Program of China under Grant No.2021ZD0113602,
the National Natural Science Foundation of China under Grant
No.62176014 and No.62306255, the Fundamental Research Funds
fortheCentralUniversitiesandtheFundamentalResearchProject
of Guangzhou under Grant No. 2024A04J4233.
REFERENCES
[1] QingyaoAi,DanielNHill,SVNVishwanathan,andWBruceCroft.2019. Azero
attention model for personalized product search. In Proceedings of the 28th ACM
International Conference on Information and Knowledge Management. 379–388.
[2] NicholasJBelkinandWBruceCroft.1992. Informationfilteringandinformation
retrieval: Two sides of the same coin? Commun. ACM 35, 12 (1992), 29–38.
[3] Keping Bi, Qingyao Ai, and W Bruce Croft. 2020. A transformer-based embed-
ding model for personalized product search. In Proceedings of the 43rd Interna-
tional ACM SIGIR Conference on Research and Development in Information Re-
trieval. 1521–1524.
[4] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. Advances in neural information processing systems 26 (2013).
[5] Tong Chen, Hongzhi Yin, Hongxu Chen, Rui Yan, Quoc Viet Hung Nguyen, and
Xue Li. 2019. Air: Attentional intention-aware recommender systems. In 2019
IEEE 35th International Conference on Data Engineering (ICDE). IEEE, 304–315.
[6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7–10.
[7] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for
youtube recommendations. In Proceedings of the 10th ACM conference on recom-
mender systems. 191–198.
[8] TylerDerr,YaoMa,andJiliangTang.2018.Signedgraphconvolutionalnetworks.
In2018 IEEE International Conference on Data Mining (ICDM). IEEE, 929–934.
[9] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric
Tzeng, and Trevor Darrell. 2014. Decaf: A deep convolutional activation feature
for generic visual recognition. In International conference on machine learning.
PMLR, 647–655.
[10] Lu Fan, Qimai Li, Bo Liu, Xiao-Ming Wu, Xiaotong Zhang, Fuyu Lv, Guli Lin,
Sen Li, Taiwei Jin, and Keping Yang. 2022. Modeling user behavior with graph
convolutionforpersonalizedproductsearch.In Proceedings of the ACM Web Con-
ference 2022. 203–212.
[11] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and
Keping Yang. 2019. Deep session interest network for click-through rate pre-
diction. In Proceedings of the 28th International Joint Conference on Artificial In-
telligence. 2301–2307.
[12] Cesar Ferri, José Hernández-Orallo, and Peter A Flach. 2011. A coherent in-
terpretation of AUC as a measure of aggregated classification performance. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11).
657–664.
[13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. In
Proceedings of the 26th International Joint Conference on Artificial Intelligence.
1725–1731.
[14] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang.2020. Lightgcn:Simplifyingandpoweringgraphconvolutionnetworkfor
recommendation. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval . 639–648.
[15] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua.2017. Neuralcollaborativefiltering.In Proceedings of the 26th international
conference on world wide web . 173–182.
[16] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for
implicit feedback datasets. In 2008 Eighth IEEE international conference on data
mining. Ieee, 263–272.
[17] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation
of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002),
422–446.
6299KDD’24, August 25–29, 2024, Barcelona, Spain Yuting Zhang et al.
[18] Thomas N Kipf and Max Welling. 2016. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Repre-
sentations.
[19] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-
banditapproachtopersonalizednewsarticlerecommendation.In Proceedings of
the 19th international conference on World wide web . 661–670.
[20] Sen Li, Fuyu Lv, Taiwei Jin, Guli Lin, Keping Yang, Xiaoyi Zeng, Xiao-Ming Wu,
and Qianli Ma. 2021. Embedding-based product retrieval in taobao search. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
Mining. 3181–3189.
[21] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and
Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-
teractions for recommender systems. In Proceedings of the 24th ACM SIGKDD
international conference on knowledge discovery & data mining. 1754–1763.
[22] Shang Liu, Wanli Gu, Gao Cong, and Fuzheng Zhang. 2020. Structural relation-
ship representation learning with graph embedding for personalized product
search. In Proceedings of the 29th ACM International Conference on Information
& Knowledge Management. 915–924.
[23] ZhiweiLiu,XiaohanLi,ZiweiFan,StephenGuo,KannanAchan,andSYuPhilip.
2020. Basket recommendation with multi-intent translation graph neural net-
work. In 2020 IEEE International Conference on Big Data (Big Data) . IEEE, 728–
737.
[24] Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao,
and Xiuqiang He. 2021. SimpleX: A simple and strong baseline for collaborative
filtering. In Proceedings of the 30th ACM International Conference on Information
& Knowledge Management. 1243–1252.
[25] XichuanNiu,BofangLi,ChenliangLi,RongXiao,HaochuanSun,HongboDeng,
and Zhenzhong Chen. 2020. A dual heterogeneous graph attention network to
improve long-tail performance for shop search in e-commerce. In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 3405–3415.
[26] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong se-
quential behavior data for click-through rate prediction. In Proceedings of the
29th ACM International Conference on Information & Knowledge Management .
2685–2692.
[27] Chuan Qin, Le Zhang, Rui Zha, Dazhong Shen, Qi Zhang, Ying Sun, Chen Zhu,
Hengshu Zhu, and Hui Xiong. 2023. A comprehensive survey of artificial intel-
ligence techniques for talent analytics. arXiv preprint arXiv:2307.03195 (2023).
[28] DragomirRRadev,HongQi,HarrisWu,andWeiguoFan.2002. EvaluatingWeb-
based Question Answering Systems.. In LREC. Citeseer.
[29] Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, and Chao Huang. 2023. Dis-
entangled Contrastive Collaborative Filtering. In Proceedings of the 46th Interna-
tional ACM SIGIR Conference on Research and Development in Information Re-
trieval(, Taipei, Taiwan,) (SIGIR ’23) . Association for Computing Machinery,
New York, NY, USA, 1137–1146. https://doi.org/10.1145/3539618.3591665
[30] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-
Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence .
452–461.
[31] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-
based collaborative filtering recommendation algorithms. In Proceedings of the
10th international conference on World Wide Web . 285–295.
[32] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and
Gabriele Monfardini. 2008. The graph neural network model. IEEE transactions
on neural networks 20, 1 (2008), 61–80.
[33] Parikshit Sondhi, Mohit Sharma, Pranam Kolari, and ChengXiang Zhai. 2018. A
taxonomyofqueriesfore-commercesearch.In The 41st International ACM SIGIR
Conference on Research & Development in Information Retrieval . 1245–1248.[34] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
2019. BERT4Rec: Sequential recommendation with bidirectional encoder repre-
sentations from transformer. In Proceedings of the 28th ACM international con-
ference on information and knowledge management. 1441–1450.
[35] Chang-You Tai, Liang-Ying Huang, Chien-Kun Huang, and Lun-Wei Ku. 2021.
User-centric path reasoning towards explainable recommendation. In Proceed-
ings of the 44th International ACM SIGIR Conference on Research and Development
in Information Retrieval . 879–889.
[36] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-
ference on Learning Representations .
[37] Shoujin Wang, Liang Hu, Yan Wang, Quan Z Sheng, Mehmet Orgun, and Long-
bing Cao. 2019. Modeling multi-purpose sessions for next-item recommenda-
tions via mixture-channel purpose routing networks. In International Joint Con-
ference on Artificial Intelligence . International Joint Conferences on Artificial In-
telligence.
[38] Shoujin Wang, Liang Hu, Yan Wang, Quan Z Sheng, Mehmet Orgun, and Long-
bing Cao. 2020. Intention2basket: A neural intention-driven approach for dy-
namic next-basket planning. In Twenty-Ninth International Joint Conference on
Artificial Intelligence and Seventeenth Pacific Rim International Conference on Ar-
tificial Intelligence {IJCAI-PRICAI-20 }. International Joint Conferences on Arti-
ficial Intelligence Organization.
[39] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neuralgraphcollaborativefiltering.In Proceedings of the 42nd international ACM
SIGIR conference on Research and development in Information Retrieval. 165–174.
[40] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural
networksinrecommendersystems:asurvey. Comput. Surveys 55,5(2022),1–37.
[41] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems 32, 1 (2020), 4–24.
[42] Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial temporal graph convo-
lutional networks for skeleton-based action recognition. In Proceedings of the
AAAI conference on artificial intelligence, Vol. 32.
[43] Jing Yao, Zhicheng Dou, Ruobing Xie, Yanxiong Lu, Zhiping Wang, and Ji-Rong
Wen. 2021. USER: A unified information search and recommendation model
based on integrated behavior sequence. In Proceedings of the 30th ACM Interna-
tional Conference on Information & Knowledge Management. 2373–2382.
[44] Hamed Zamani and W Bruce Croft. 2018. Joint modeling and optimization of
search and recommendation. arXiv preprint arXiv:1807.05631 (2018).
[45] HamedZamaniandWBruceCroft.2020. Learningajointsearchandrecommen-
dationmodelfromuser-iteminteractions.In Proceedings of the 13th international
conference on web search and data mining. 717–725.
[46] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V
Chawla. 2019. Heterogeneous graph neural network. In Proceedings of the 25th
ACM SIGKDD international conference on knowledge discovery & data mining.
793–803.
[47] Kai Zhao, Yukun Zheng, Tao Zhuang, Xiang Li, and Xiaoyi Zeng. 2022. Joint
learningofe-commercesearchandrecommendationwithaunifiedgraphneural
network. In Proceedings of the Fifteenth ACM International Conference on Web
Search and Data Mining. 1461–1469.
[48] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang
Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate
prediction.In Proceedings of the AAAI conference on artificial intelligence ,Vol.33.
5941–5948.
[49] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma,
Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for
click-through rate prediction. In Proceedings of the 24th ACM SIGKDD interna-
tional conference on knowledge discovery & data mining. 1059–1068.
[50] Nengjun Zhu, Jian Cao, Yanchi Liu, Yang Yang, Haochao Ying, and Hui Xiong.
2020. Sequentialmodelingofhierarchicaluserintentionandpreferencefornext-
item recommendation. In Proceedings of the 13th international conference on web
search and data mining. 807–815.
6300