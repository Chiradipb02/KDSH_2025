Leveraging Pedagogical Theories to Understand Student Learning
Process with Graph-based Reasonable Knowledge Tracing
Jiajun Cui
cuijj96@gmail.com
East China Normal University
Shanghai, ChinaHong Qian
hqian@cs.ecnu.edu.cn
East China Normal University
Shanghai, China
Bo Jiang
bjiang@deit.ecnu.edu.cn
East China Normal University
Shanghai, ChinaWei Zhang∗
zhangwei.thu2011@gmail.com
East China Normal University
Shanghai, China
ABSTRACT
Knowledge tracing (KT) is a crucial task in intelligent education,
focusing on predicting students’ performance on given questions to
trace their evolving knowledge. The advancement of deep learning
in this field has led to deep-learning knowledge tracing (DLKT)
models that prioritize high predictive accuracy. However, many
existing DLKT methods overlook the fundamental goal of track-
ing students’ dynamical knowledge mastery. These models do not
explicitly model knowledge mastery tracing processes or yield un-
reasonable results that educators find difficulty to comprehend and
apply in real teaching scenarios. In response, our research conducts
a preliminary analysis of mainstream KT approaches to highlight
and explain such unreasonableness. We introduce GRKT, a graph-
based reasonable knowledge tracing method to address these issues.
By leveraging graph neural networks, our approach delves into the
mutual influences of knowledge concepts, offering a more accurate
representation of how the knowledge mastery evolves throughout
the learning process. Additionally, we propose a fine-grained and
psychological three-stage modeling process as knowledge retrieval,
memory strengthening, and knowledge learning/forgetting, to con-
duct a more reasonable knowledge tracing process. Comprehensive
experiments demonstrate that GRKT outperforms eleven baselines
across three datasets, not only enhancing predictive accuracy but
also generating more reasonable knowledge tracing results. This
makes our model a promising advancement for practical imple-
mentation in educational settings. The source code is available at
https://github.com/JJCui96/GRKT.
∗Corresponding author. This work was supported in part by National Key R&D Program
of China (No. 2023YFC3341200), National Natural Science Foundation of China (No.
92270119 and No. 62072182), and Key Laboratory of Advanced Theory and Application
in Statistics and Data Science, Ministry of Education.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671853CCS CONCEPTS
•Computing methodologies →Neural networks; •Applied
computing→Education; •Information systems →Data min-
ing.
KEYWORDS
knowledge tracing, student behavior modeling, data mining, peda-
gogical theory, reasonable knowledge tracing
ACM Reference Format:
Jiajun Cui, Hong Qian, Bo Jiang, and Wei Zhang. 2024. Leveraging Ped-
agogical Theories to Understand Student Learning Process with Graph-
based Reasonable Knowledge Tracing. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671853
1 INTRODUCTION
In personalized learning, Knowledge Tracing (KT) is crucial for
tracking students’ evolving knowledge mastery based on their his-
torical question responses [ 3,33]. Early researchers addressed this
challenge by leveraging the monotonicity assumption [ 7], linking
better mastery of one knowledge concept (KC) to a higher probabil-
ity of correctly answering related questions. They trained models to
predict student responses on given questions, proposing typical ma-
chine learning-based KT methods [ 3,21]. Consequently, predicting
student performance became the primary task, with prediction accu-
racy as the mainstream metric for evaluating KT models, promoting
the emergence of deep learning knowledge tracing (DLKT) methods.
However, many DLKT approaches prioritize prediction ability over
the fundamental objective of knowledge tracing, sometimes forgo-
ing tracing altogether [ 2,11]. Others use internal network weights
to represent knowledge mastery [ 27,38], facing challenges in con-
structing meaningful tracing results due to the low interpretability
and reasonability of deep neural network structures. Hidden neu-
rons in these networks adaptively learn from data without explicit
meaning [ 12]. It is worth noting that the cognitive diagnosis task
also assesses knowledge mastery but usually focuses on static test-
ing instead of dynamic learning process [ 15,17]. Therefore, we do
not delve into it within this paper.
Figure 1 illustrates the traced dynamic knowledge mastery of an
example student by two DLKT models: DKT [ 23] and LPKT [ 27].
DKT is a pioneering approach that directly applies recurrent neural
 
502
KDD ’24, August 25–29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
LPKT
Ideal Model
Area TriangleTimeline
Calculations with similar Figures Conversion of Fraction Decimals Percents Ordering Integers Multiplication and Division Positive DecimalsDKT
Learning Curve
Mastery Change of Unrelated KCs
 Inconsistent Mastery Change Direction
No Mastery Change of Related KCsLPKT
Testing Effects
 Transfer of LearningForgetting Curve
Figure 1: Illustration of a student’s evolving knowledge mastery while answering ten questions, traced by two DLKT models,
along with an assumed ideal tracing result. The student is sampled from the ASSIST12 dataset, introduced in Section 5.1.1.
networks (RNNs) to the KT task. In this case, when the student
responds to the initial four questions related to the blue KC Calcula-
tions with Similar Figures, their knowledge mastery of the unrelated
green KC Ordering Integers increases, presenting an unreasonable
outcome. Furthermore, a correct response to the sixth question
results in a contrary decrease in its corresponding KC’s mastery,
demonstrating an inconsistent change in direction. LPKT, as a time-
aware method, models learning and forgetting processes for more
reasonable knowledge tracing. However, it struggles to capture
the relation between the yellow KC Area Triangle and the blue
KCCalculations with Similar Figures, as evidenced by the decreas-
ing mastery of the blue curve following a correct response to an
question of yellow. Both of these two KCs examine students’ calcu-
lations about the base and height of triangles, which suggests their
underlying relation. Beneath the figure is a tracing result from an
assumed ideal model, which we design based on comprehensive
pedagogical effects. As shown, the student mastery will increase
and drop according to their right/wrong responses based on the
testing effect [ 24]. The mastery of the yellow KC would relatedly
increase due to the correct response to the sixth orange KC, accord-
ing to the transfer of learning [ 22]. Besides, the mastery between
responses should also vary due to students’ learning and forgetting
behaviors modeled by the learning and forgetting curves [6, 36].
From this example, we summarize three deficiencies of current
DLKT methods in dynamic knowledge tracing reasonability: (i)
Mastery change of unrelated KCs - learning one KC affects un-
related KC mastery; (ii) No mastery change of related KCs -
learning one KC does not impact related KCs; (iii) Inconsistent
mastery change direction - correct answers may decrease KCmastery, and vice versa. These stem from opaque deep neural net-
works, whose parameters serve the overarching objective of perfor-
mance prediction. Moreover, many researches use RNNs to model
knowledge application and update by the recurrent units’ output
and state transition [ 16,23,26,27]. This mixes the effects of students
answering questions and their spontaneous behaviors, leading to
confusing tracing results. For example, incorrect responses may
strengthen wrong knowledge retrieval and get a mastery drop of
the related KC. But when they get feedback and learn from their
errors, they can make a final progress. This fine-grained knowledge
mastery changing is not captured. To address these above issues, we
introduce GRKT, a Graph-based Reasonable Knowledge Tracing
to enhance knowledge tracing reasonability while retaining neural
networks’ representational power.
To be specific, we integrate pedagogical theories [ 6,22,24,36]
into the KT modeling, dividing the learning process into three
distinct stages. (i) The knowledge retrieval stage analyzes how
students respond to questions. This stage draws from cognitive psy-
chology [ 18], viewing learning as encoding, storing, and retrieving
memories. When students answer questions, retrieval from mem-
ory becomes crucial. We start this stage by retrieving the encoded
memory related to the question’s KC and project it into a mastery
value. We then compare this value with the question’s difficulty
score to predict if the student could correctly answer the question.
(ii) The memory strengthening stage focuses on how answering
questions impacts students’ knowledge mastery. Here, students
strengthen their memory retrieval routes, aligning with the Testing
Effect theory [ 14,24]. Correct retrievals enhance learning, while
incorrect ones reinforce errors. We encode this positive/negative
 
503Graph-based Reasonable Knowledge Tracing KDD ’24, August 25–29, 2024, Barcelona, Spain
memory strengthening in the knowledge memory of the relevant
KC based on whether the question is correctly solved. (iii) The
knowledge learning/forgetting stage explores what students
do after question answering. This stage aims to model the active
learning and natural forgetting behaviors based on the Learning
curve [36] and the Forgetting curve [6]. Both curves suggest a de-
creasing rate of learning and forgetting over time. Concretely, we
first introduce a learning decider to determine whether students
will continue learning the KCs just practiced or the KCs for future
study. Then, we employ KC-specific time-aware kernels to model
the learning/forgetting curves of all involved KCs based on these
decisions. By applying this three-stage modeling process iteratively
across students’ response sequences, we establish a coherent and
reasonable knowledge tracing framework. This approach effectively
captures mastery changes resulting from question answering and
subsequent behaviors, addressing the issue of inconsistent mas-
tery change direction.
To handle the two other issues of mastery change of unrelated
KCs andno mastery change of related KCs, we utilize the mes-
sage passing mechanism of graph neural networks (GNNs) applied
to KC relation graphs. This mechanism establishes clear boundaries
between related and unrelated KCs. Specifically, changes in knowl-
edge mastery of one KC are propagated through the graph edges
to its related KCs within a specific number of hops. From the peda-
gogical perspective, this message passing aligns with the Transfer
of Learning theory [22], which explains humans’ ability to transfer
knowledge between similar fields to solve problems and acquire
skills. We integrate this understanding into our three-stage learning
process modeling using KC relation-based GNNs. For instance, in
the first stage of GRKT, instead of solely retrieving knowledge from
the target question’s KC, we utilize graph aggregation to synthesize
the memory of the KC’s neighbors for solving the question. Simi-
larly, during the second stage, the memory strengthening process
involves propagating the gain and loss of knowledge mastery to
the KC’s neighbors, and this process is also applied in the third
stage’s knowledge learning. Additionally, we exploit the homophily
of GNNs to generate similar time-aware kernels for related KCs, ef-
fectively modeling their similar learning/forgetting processes. This
defines the boundaries between related and unrelated KCs based
on the number of hops in GNN operations, effectively addressing
challenges associated with mastery changes between different KCs.
It’s worth noting that KCs have various types of relations, including
prerequisite, similarity, collaboration, remedial, and hierarchy [ 10].
In GRKT, we primarily focus on leveraging the two most commonly
used relations: prerequisite and similarity.
To the best of our knowledge, this work represents the first
comprehensive analysis of the reasonability issues in current DLKT
methods, and integrates multiple pedagogical theories to address
these concerns. The main contributions of this paper are as follows:
•Motivation. We identify the reasonability issues arising from the
widespread adoption of deep learning techniques in the KT task.
Many DLKT methods tend to excessively prioritize student per-
formance prediction, often overlooking unreasonable knowledge
tracing results due to the inherent interpretability challenges
posed by neural networks.•Methods. We outline three primary reasonability issues preva-
lent in current DLKT methods. To address these issues, we intro-
duce GRKT, a graph-based reasonable knowledge tracing, which
establishes a three-stage learning process modeling. Additionally,
we utilize the KC relation graph to mitigate mutual effects among
KCs. The incorporation of multiple pedagogical theories provide
sufficient support for our proposed method.
•Experiments. Comprehensive experimental results showcase
that our GRKT exhibits superior prediction performance and
yields reasonable knowledge tracing results when compared to
eleven baselines across three widely-used datasets.
2 RELATED WORK
2.1 Reasonable Knowledge Tracing
Early KT methods in machine learning, such as Bayesian Knowledge
Tracing (BKT) [ 3], initially showcased reasonable results due to
their transparent and interpretable internal structure. BKT utilizes
Hidden Markov Models (HMMs) to probabilistically represent the
student learning process. It transitions knowledge mastery and
emits probabilities of correct responses, while also considering
guessing and slipping behaviors. Subsequent KT methods expanded
upon BKT by incorporating additional pedagogical factors such as
question difficulty [21] or prior student information [39].
However, traditional methods show inferior prediction perfor-
mance when compared to subsequent emerging DLKT methods [ 2,
5,11,16,20,23,26], which reach high prediction performance due
to the power of neural networks. Even so, these DLKT methods
fail to produce reasonable knowledge tracing results due to their
inherently opaque structures. Efforts have been made to tackle this
challenge. Shen et al. [ 27] proposed Learning Process-consistent
Knowledge Tracing (LPKT), which utilizes student response dura-
tion and interval time to capture learning and forgetting behaviors.
However, it only focuses on knowledge learning and forgetting
and does not model the interplay of knowledge mastery changes
between KCs, limiting its reasonability. Similarly, Yin et al. [ 38]
introduced the Diagnostic Transformer (DTransformer), which di-
agnoses student knowledge mastery from each tackled question
and employs a contrastive learning framework to produce more sta-
ble knowledge tracing. While this stability enhances reasonability
to some extent, its transformer-based structures do not adequately
reflect the transition of knowledge mastery between continuous stu-
dent responses. Therefore, while these approaches improve model
reasonability from specific angles, they do not offer a comprehen-
sive method to generate reasonable knowledge tracing results cov-
ering both KC relations and continuous learning processes.
We address this gap with our proposed GRKT, which utilizes
GNNs to model KC relations and introduces a three-stage learn-
ing process to capture evolving knowledge mastery. By integrat-
ing these techniques, GRKT achieves high prediction performance
while also generating more reasonable knowledge tracing results.
2.2 Graph-based Knowledge Tracing
Graph Neural Networks (GNNs) [ 25] serve as an efficient tool to
capture intricate relations between instances in real-world sce-
narios. Their message aggregation and propagation operations on
 
504KDD ’24, August 25–29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
graphs yield deep representations for node features, enhancing per-
formance in various downstream tasks across different domains. In
the context of KT, researchers explore various structures to harness
the power of GNNs. Nakagawa et al. [ 19] pioneered the incorpora-
tion of GNNs into KT by reformulating it as a time-series node-level
classification problem based on KC relation graphs. Gan et al. [ 9]
leveraged this structure to enhance graph representation learning,
generating more informative question and concept embeddings.
Except for KC relations, question-question and question-KC rela-
tions are also widely considered. For instance, Bi-CLKT [ 28] applied
contrastive learning to question-KC and KC-KC graphs to generate
question embeddings enriched with question and KC structural
information. Another work [ 35] leveraged question-KC relations
to address question sparsity and multi-skill problems. In our paper,
we specifically focus on utilizing GNNs to model the mutual ef-
fects of KCs during students’ knowledge leveraging and changing,
constructing a more reasonable approach to knowledge tracing.
It is worth noting that some other GNN-based or memory-based
methods (e.g., GKT [ 19] and DKVMN [ 40]) also update mastery
between KCs. However, their knowledge state updating is still
potentially performed by the erase-followed-by-add mechanism,
which uses GRU/LSTM cells unable to solve the reasonability is-
sues such as not guaranteeing the direction of consistency change
between KCs.
3 PRELIMINARY
3.1 Task Formulation
Knowledge tracing aims to trace the dynamic evolution of students’
knowledge mastery throughout their learning processes character-
ized by their responses to questions. Suppose there are a student
setU, a question setQ, and a KC setC. Each student 𝑢∈U has
a historical response sequence H𝑢={𝑟𝑢
1,𝑟𝑢
2,···,𝑟𝑢
|H𝑢|}, where
each response 𝑟𝑢
𝑡= 𝑞𝑢
𝑡,𝑎𝑢
𝑡,𝑐𝑢
𝑡,𝑇𝑢
𝑡comprises the involved ques-
tion𝑞𝑢
𝑡∈Q, the correctness 𝑎𝑢
𝑡∈{0,1}(where𝑎𝑢
𝑡=1means a
correct response), the KC 𝑐𝑢
𝑡∈Cexamined by the question, and
the timestamp 𝑇𝑢
𝑡of the response. It is worth noting that there
could be multiple KCs associated with one question. To be con-
cise, we use the notations with just one KC to describe the task
setting and the proposed method, but our method is easily ex-
tended to the setting of multiple KCs (e.g., averaging the KC repre-
sentations as mentioned in Section 4.3). The objective is to track
and monitor the evolving knowledge mastery of 𝑢after each re-
sponse,M𝑢={m𝑢
1,m𝑢
2,···,m𝑢
|H𝑢|}where m𝑢
𝑡is stacked with
{𝑚𝑢
𝑐𝑖,𝑡|𝑐𝑖∈C} and𝑚𝑢
𝑐𝑖,𝑡signifies the student’s knowledge mastery
of the KC𝑐𝑖at time step𝑡. A higher value denotes a superior level of
mastery. However, the absence of annotated mastery levels neces-
sitates researchers to resort to the student performance prediction
task as a surrogate measure [ 17]. In this paradigm, given H𝑢, the
objective is to predict whether student 𝑢can correctly answer a
new question 𝑞𝑢
|H𝑢|+1, with its associated KC 𝑐𝑢
|H𝑢|+1at timestamp
𝑇𝑢
|H𝑢|+1. This hinges on the monotonicity assumption [ 7], which
posits that higher knowledge mastery leads to a higher probability
of answering questions correctly. For brevity, we omit the super-
script𝑢in the later method description.4 METHODOLOGY
As shown in Figure 2, GRKT conducts a recurrent modeling within a
three-stage learning process: knowledge retrieval, memory strength-
ening, and knowledge learning/forgetting. The proposed KC relation-
based graph neural networks capture knowledge mastery variation
between KCs throughout these stages. This section introduces the
KC relation-based GNNs first, then explains the three-stage learn-
ing process modeling with these GNNs. For ease of understanding
GRKT, we list and explain all relevant notations in Appendix A.
4.1 KC Relation-based Graph Neural Networks
Based on the transfer of learning theory [ 22], we introduce KC
relation-based GNNs to transfer the knowledge leveraging and
changing throughout the three-stage learning process, as shown in
Figure 2. To avoid repetition, we first elaborate on a prototype of
KC relation-based GNNs in this section and highlight differences
when applied to different stages in the subsequent sections.
Due to the lack of KC relation annotations, we follow previous
works [ 19,28] that construct KC relations based on the data statis-
tics. Details could be referred to in Appendix B. Besides, we focus
on the two most common relations, prerequisite and similarity and
extend three relation graphs P,S,R, whose edges denote one KC
being prerequisite/subsequent/relevant (similar) to another one.
This is because the forward and backward message passed along
the unidirectional prerequisite relation should be differentiated.
Based on this, we design the KC relation-based GNNs with multiple
layers. They receive KC node features such as knowledge memory,
knowledge gain/loss, or knowledge learnt in the three stages, which
would be introduced later. To capture the graph information, each
layer first aggregates the features of each node’s neighbors for each
graphG∈{P,S,R}from the last layer as
¯fG,(𝑙)
𝑐𝑖=1
|G(𝑐𝑖)|∑︁
𝑐𝑗∈G(𝑐𝑖)
𝛽G
𝑐𝑖,𝑐𝑗·˜f(𝑙−1)
𝑐𝑗WG,(𝑙)
𝑝𝑟𝑜𝑡𝑜
(1)
˜fG,(𝑙)
𝑐𝑖=ReLU
¯fG,(𝑙)
𝑐𝑖
OG,(𝑙)
𝑝𝑟𝑜𝑡𝑜. (2)
where WG,(𝑙)
𝑝𝑟𝑜𝑡𝑜∈R𝑑𝑙−1×𝑑𝑙−1andOG,(𝑙)
𝑝𝑟𝑜𝑡𝑜∈R𝑑𝑙−1×𝑑𝑙are the learn-
able weight matrices in this layer. G(·) is the neighbor function of
G.ReLU(·)is an activation function to introduce non-linearity to
enhance model representability. 𝛽G
𝑐𝑖,𝑐𝑗is the correlation score of KC
𝑐𝑖and𝑐𝑗on graphG, obtained by
𝛽G
𝑐𝑖,𝑐𝑗=𝜎
kT
𝑐𝑖WG
𝑐𝑜𝑟k𝑐𝑗
. (3)
k𝑐𝑖,k𝑐𝑗∈R1×𝑑𝑒are the two KCs’ embeddings where 𝑑𝑒is the
number of embedding dimensions. WG
𝑐𝑜𝑟∈R𝑑𝑒×𝑑𝑒is the trainable
matrix forG, and𝜎(·)denotes the sigmoid function, which regular-
izes the score in(0,1). We then fuse the aggregated features from
the three graphs by
˜f(𝑙)
𝑐𝑖=(Í
G∈{P,S,R}˜fG,(𝑙)
𝑐𝑖+˜f(𝑙−1)
𝑐𝑖,if𝑑𝑙−1=𝑑𝑙,
Í
G∈{P,S,R}˜fG,(𝑙)
𝑐𝑖, if𝑑𝑙−1≠𝑑𝑙,(4)
where we apply a residual connection [ 29] when𝑑𝑙−1=𝑑𝑙to
stabilize the training process. In this prototype, we denote the
input features of all KCs as ˜F(0)∈R|𝐶|×𝑑0and one of them as
 
505Graph-based Reasonable Knowledge Tracing KDD ’24, August 25–29, 2024, Barcelona, Spain
Aggregated Memory 
ො𝑎=𝜎( − )
Stage1:  Knowledge RetrievalTarget question’s repr.
Target KC’s memory
Knowledge LossKnowledge Gain
Stage2: Memory StrengtheningMastery
ProjectorTarget question’s repr
Target KC’s memoryNext question’s repr
Next KC’s memory
Knowledge LearntKnowledge Forgot
Stage3: Knowledge Learning/ForgettingLearn?
DeciderHidden Knowledge 
Memory
Mastery DifficultyKC Embeddings
Forgetting
Curve
Learning
Curve
Target question’s concept
 Target question’s binary correctness KC relation -based 
GNNs ො𝑎 𝜎 Estimated score Sigmoid functionRetrieval Update𝒕−𝒕
Update
𝒕−,𝒕,𝒕+𝟏−Time stepHidden Knowledge 
MemoryHidden Knowledge 
Memory𝒕+𝟏−
Mastery
Projector
Mastery
Knowledge TracingTrace
Single KC  Memory 𝒕−𝟏− 𝒕+𝟏−Timeline Timeline
Figure 2: The entire framework of GRKT encompasses three recurrent stages: knowledge retrieval, memory strengthening, and
knowledge learning/forgetting.
˜f(0)
𝑐𝑖∈R1×𝑑0for KC𝑐𝑖, and the output features as ˜F(𝐿)∈R|𝐶|×𝑑𝐿
and˜f(𝐿)
𝑐𝑖∈R1×𝑑𝐿, where𝑑0,𝑑𝐿are the numbers of input and output
feature dimensions. Then this prototype GNN is formulated as:
˜F(𝐿)=GNN𝑝𝑟𝑜𝑡𝑜(˜F(0)|𝑑0,𝑑1,···,𝑑𝐿) (5)
˜f(𝐿)
𝑐𝑖=GNN𝑝𝑟𝑜𝑡𝑜(˜f(0)
𝑐𝑖|𝑑0,𝑑1,···,𝑑𝐿). (6)
This prototype is then extended for different student learning stages
to construct reasonable knowledge tracing based on the transfer of
learning theory. Besides, the number of layers 𝐿controls the number
of hops the feature propagates on the graphs, which clarifies the
boundary between related and non-related KCs.
4.2 Knowledge Memory & Knowledge Tracing
GRKT aims to model the process of student retrieving and learning
knowledge with their memory. Therefore, we employ a dynamic
knowledge memory bank denoted as H∈R|𝐶|×𝑑𝑘, where each
rowh𝑐𝑖encodes the current knowledge memory of KC 𝑐𝑖for the
student. Here, 𝑑𝑘signifies the number of memory dimensions. This
memory bank evolves alongside the student’s learning process,
represented as H𝑡, with a learnable initial state H0representing
their prior knowledge before engaging in any learning behavior.
To track the knowledge mastery of a specific KC, we apply a non-
negative projection vector wℎ∈R𝑑𝑘×1
≥0toh𝑐𝑖,𝑡using the equation:
ˆ𝑚𝑐𝑖,𝑡=h𝑐𝑖,𝑡·wℎ, (7)
which yields the mastery of KC 𝑐𝑖at time step 𝑡. The non-negative
constraint on the network weights guarantees the monotonic re-
lationship between mastery and each memory dimension. This
technique has been widely adopted in numerous studies [ 31,32] to
satisfy the monotonicity assumption. Moreover, we leverage this
constraint to establish a foundation for reasonable knowledge trac-
ing, which would be gradually refined in subsequent descriptions.4.3 Stage I: Knowledge Retrieval
In this stage, students retrieve stored knowledge from memory
to solve given questions, a mechanism explained by memory the-
ory [ 18]. Additionally, the transfer of learning theory [ 22] suggests
that learners transfer knowledge from similar fields to tackle prob-
lems. Leveraging this insight, we employ a KC relation-based GNN
to model knowledge transfer from related KCs. Specifically, we
aggregate the knowledge memory of the given KC 𝑐𝑡to solve its
corresponding question 𝑞𝑡before time step 𝑡(represented as 𝑡−):
˜h(𝐿)
𝑐𝑡,𝑡−=GNN𝑟𝑡𝑣(˜h(0)
𝑐𝑡,𝑡−|{𝑑𝑘}𝐿+1), (8)
with initializing ˜h(0)
𝑐𝑡,𝑡−=h𝑐𝑡,𝑡−. Recognizing that different ques-
tions have different mastery requirements of KCs, we incorporate
question-KC correlation scores into the aggregation process in this
GNN, which are calculated by:
𝛼𝑞𝑖,𝑐𝑗=𝜎
eT
𝑞𝑖W𝑟𝑒𝑞k𝑐𝑗
, (9)
where e𝑞𝑖∈R𝑑𝑒×1andk𝑐𝑗are the embeddings of 𝑞𝑖and𝑐𝑗, and
W𝑟𝑒𝑞∈R𝑑𝑒×𝑑𝑒is a learnable matrix. Then, the graph message
aggregation process of Equation 8 is actually
˜hG,(𝑙)
𝑐𝑡=1
|G(𝑐𝑡)|∑︁
𝑐𝑖∈G(𝑐𝑡)
𝛼𝑞𝑡,𝑐𝑖·𝛽G
𝑐𝑡,𝑐𝑖·˜h(𝑙−1)
𝑐𝑖WG,(𝑙)
𝑟𝑡𝑣
.(10)
We also remove the non-linear feed-forward process and restrict
WG,(𝑙)
𝑟𝑡𝑣∈R𝑑𝑘×𝑑𝑘
≥0to ensure higher values of the related KCs’ mem-
ory bring higher knowledge mastery. After getting the aggregating
knowledge memory from this GNN, we get the knowledge mastery
as Equation 7 and compare it with the question difficulty 𝑑𝑞𝑡to
generate the predictive probability of solving the question:
ˆ𝑎𝑡=𝜎
˜h(𝐿)
𝑐𝑡,𝑡·wℎ−𝑑𝑞𝑡
. (11)
 
506KDD ’24, August 25–29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
For multi-KC questions, we average the KCs’ memory. The difficulty
𝑑𝑞𝑡of question𝑞𝑡is generated by a Multi-Layer Perception (MLP):
𝑑𝑞𝑡=ReLU
¯e𝑞𝑡W(1)
𝑑𝑖𝑓𝑓+b(1)
𝑑𝑖𝑓𝑓
W(2)
𝑑𝑖𝑓𝑓+b(2)
𝑑𝑖𝑓𝑓. (12)
Here, ¯e𝑞𝑡=[k𝑐𝑡⊕e𝑞𝑡]is the concatenated representation of 𝑞𝑡and
its examined KC 𝑐𝑡’s embeddings. For multi-KC questions, we use
the KCs’ average embedding. W(1)
𝑑𝑖𝑓𝑓∈R2𝑑𝑒×𝑑ℎ,W(2)
𝑑𝑖𝑓𝑓∈R𝑑ℎ×1,
b(1)
𝑑𝑖𝑓𝑓∈R1×𝑑ℎ, and b(2)
𝑑𝑖𝑓𝑓∈R1×1are learnable matrices and
vectors.𝑑ℎis the number of hidden dimensions. We denote the pro-
cess of this two-layer MLP as 𝑑𝑞𝑡=MLP𝑑𝑖𝑓𝑓(¯e𝑞𝑡|2𝑑𝑒,𝑑ℎ,1), and a
similar notation is applied for brevity in subsequent descriptions.
Hereinafter, we accurately model the process whereby students
retrieve knowledge from memory to answer new questions.
4.4 Stage II: Memory Strengthening
The testing effect theory [ 24] reveals that a correct retrieval strength-
ens the storage of knowledge in memory, while an unsuccessful
retrieval can lead to incorrect strengthening. Without correction
or active learning after the error, this may reduce knowledge mas-
tery [ 14]. In this stage, we determine the memory strengthening
process based on whether the examined KC is correctly retrieved
to solve the question, resulting in either knowledge gain or loss.
Additionally, these knowledge changes are propagated to related
KCs based on the transfer of learning theory. To enhance memory
from a correct response to question 𝑞𝑡, we first combine and input
the current memory h𝑐𝑡,𝑡−of KC𝑐𝑡and the question information
¯e𝑞𝑡into an MLP to obtain an initial memory feature:
g𝑐𝑡,𝑡=MLP𝑔𝑎𝑖𝑛 [h𝑐𝑡,𝑡−⊕¯e𝑞𝑡]|𝑑𝑘+2𝑑𝑒,𝑑ℎ,𝑑𝑘. (13)
For multi-KC question, we calculate all the associated KCs’ features.
This feature serves as a spark to propagate knowledge changes
via another KC relation-based GNN. Specifically, by initializing an
input feature matrix ˜G(0)
𝑡, where ˜g(0)
𝑐𝑖,𝑡=g𝑐𝑡,𝑡if𝑐𝑖=𝑐𝑡and˜g(0)
𝑐𝑖,𝑡=0
if𝑐𝑖≠𝑐𝑡, the knowledge gain for all KCs is obtained as follows:
˜G(𝐿)
𝑡=ReLU(GNN𝑔𝑎𝑖𝑛(˜G(0)
𝑡|{𝑑𝑘}𝐿+1)). (14)
The ReLU(·)activation function ensures that the knowledge
gain to be positive. Moreover, due to the zero feature initialization
except for the examined KC, the knowledge gain is only propagated
to KCs within 𝐿hops, delineating a boundary between related and
unrelated KCs. Similarly, we could derive the negative knowledge
loss ˜L(𝐿)
𝑡when students provide incorrect responses and wrongly
strengthen their memory, by using a similar network GNN 𝑙𝑜𝑠𝑠(·).
Subsequently, we update the knowledge memory bank with
respect to the response 𝑎𝑡as follows:
H𝑡=H𝑡−+𝑎𝑡˜G(𝐿)
𝑡+(1−𝑎𝑡)˜L(𝐿)
𝑡. (15)
It is worth noting that different questions also have different effects
on strengthening students’ memory of KCs. Therefore, similar to
Equation 10, these two GNNs also add the question-KC correla-
tion scores during message passing. Henceforth, the second stage,
memory strengthening, is reasonably modeled based on the testing
effect and the transfer of learning.4.5 Stage III: Knowledge Learning/Forgetting
After students answer questions, their subsequent actions vary de-
pending on the feedback received. They may review their correct
answers or correct their mistakes. Besides, they might prepare for
the next question’s KC they would encounter. These active learn-
ing behaviors contribute to improving their knowledge mastery,
which we model as the knowledge learning process in this stage.
Concretely, the KC of the last question and the next question both
influence the student’s learning target. Therefore, we use an MLP
to determine if the student actively learns them based on his/her
current knowledge memory and the involved questions’ informa-
tion. For KC 𝑐𝑖∈{𝑐𝑡,𝑐𝑡+1}(or more involved KCs for multi-KC
questions), the two-dimension policy distribution is calculated by:
𝜋𝑐𝑖,𝑡=softmax MLP𝑑𝑐𝑠([h𝑐𝑖,𝑡⊕¯e𝑞𝑡⊕¯e𝑞𝑡+1]|𝑑𝑘+4𝑑𝑒,𝑑ℎ,2).
(16)
Here, argmax𝜋𝑐𝑖,𝑡=0indicates that the first dimension is bigger.
We suppose there is no active learning. Contrarily, argmax𝜋𝑐𝑖,𝑡=1
indicates the student would learn 𝑐𝑖. Under this circumstance, we
calculate the progress of learning 𝑐𝑖in a similar way:
p𝑐𝑖,𝑡=MLP𝑝𝑟𝑔([h𝑐𝑖,𝑡⊕¯e𝑞𝑡⊕¯e𝑞𝑡+1]|𝑑𝑘+4𝑑𝑒,𝑑ℎ,𝑑𝑘)).(17)
Based on the transfer of learning theory, this progress is also prop-
agated to related KCs using another KC relation-based GNN. Af-
ter initializing ˜P(0)
𝑡where ˜p(0)
𝑐𝑖,𝑡=p𝑐𝑖,𝑡for𝑐𝑖∈ {𝑐𝑡,𝑐𝑡+1}with
argmax𝜋𝑐𝑖,𝑡=1, and ˜p(0)
𝑐𝑖,𝑡=0otherwise, we compute
˜P(𝐿)
𝑡=ReLU(GNN𝑝𝑟𝑔(˜P(0)
𝑡|{𝑑𝑘}𝐿+1)). (18)
This active learning process continues until the student answers the
next question, allowing us to model each KC’s progress ˜p(𝐿)
𝑐𝑖,𝑡,𝑐𝑖∈C
with a KC-specific time-aware kernel function to update:
h𝑐𝑖,(𝑡+1)−=h𝑐𝑖,𝑡+𝝓𝑐𝑖(˜p(𝐿)
𝑐𝑖,𝑡,Δ𝑇𝑡+1) (19)
where Δ𝑇𝑡+1=𝑇𝑡+1−𝑇𝑡is the time duration until the next question.
According to the learning curve [ 36], the efficiency of students
in learning a specific KC tends to be high initially and gradually
decreases over both the learning time and frequency. Therefore, we
design the kernel function in an exponential form:
𝝓𝑐𝑖(˜p(𝐿)
𝑐𝑖,𝑡,Δ𝑇𝑡+1)=˜p(𝐿)
𝑐𝑖,𝑡⊙(1−exp(−(𝑛𝑐𝑖,𝑡+1)Δ𝑇𝑡+1·˜𝜸(𝐿)
𝑐𝑖)),(20)
where⊙is the Hadamard product. 𝑛𝑐𝑖,𝑡is the number of times that
𝑐𝑖has been learned by the student. ˜𝜸(𝐿)
𝑐𝑖represents the KC-specific
kernel parameters of 𝑐𝑖generated by another KC relation-based
GNN. It leverages the property of graph homophily that makes
related KCs have similar learning ratios:
˜𝜸(𝐿)
𝑐𝑖=softplus(GNN𝑙𝑟𝑛(˜𝜸(0)
𝑐𝑖|𝑑𝑒,{𝑑𝑘}𝐿)), (21)
with initializing ˜𝜸(0)
𝑐𝑖=k𝑐𝑖which is𝑐𝑖’s embedding. Here, softplus(·)
is an activation function to restrict the parameter to be positive.
On the other hand, for KCs that students have acquired before but
they do not choose to learn, we introduce the knowledge forgetting
process. Therefore, for the KCs students do not make progress on
(i.e., ˜p(𝐿)
𝑐𝑖,𝑡=0), their previously acquired knowledge fades over
time:
h𝑐𝑖,(𝑡+1)−=h𝑐𝑖,𝑡−𝜿𝑐𝑖(Δh𝑐𝑖,𝑡,Δ𝑇𝑡+1) (22)
 
507Graph-based Reasonable Knowledge Tracing KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Statistics of the three preprocessed datasets.
Dataset ASSIST09 ASSIST12 Junyi
#response 0.4m 2.6m 25.4m
#sequence 7.4k 38.1k 325.4k
#question 13.5k 51.0k 2.8k
#concept 140 198 722
#concept/question 1.22 1.0 1.0
where Δh𝑐𝑖,𝑡=h𝑐𝑖,𝑡−h𝑐𝑖,0represents the total knowledge ac-
quisition the student has accumulated. According to the forget-
ting curve [ 6], the speed that students forget knowledge follows a
pattern of initially rapid decay and then a gradual decrease over
time and the review frequency. Therefore, we similarly design KC-
specific forgetting kernel functions in an exponential form:
𝜿𝑐𝑖(Δh𝑐𝑖,𝑡,Δ𝑇𝑡+1)=Δh𝑐𝑖,𝑡⊙(1−exp(−(𝑛𝑐𝑖,𝑡+1)Δ𝑇𝑡+1·˜𝜽(𝐿)
𝑐𝑖)),
(23)
where the kernel parameters ˜𝜽(𝐿)
𝑐𝑖are similarly generated by an-
other KC relation-based GNN:
˜𝜽(𝐿)
𝑐𝑖=softplus(GNN𝑓𝑔𝑡(˜𝜽(0)
𝑐𝑖|𝑑𝑒,{𝑑𝑘}𝐿)) (24)
with initializing ˜𝜽(0)
𝑐𝑖=k𝑐𝑖. Consequently, based on the learning
and forgetting curves, we have derived the updated knowledge
memory H(𝑡+1)−in this stage, which is recursively used for an-
swering the next question.
4.6 Model Training
The three-stage modeling is recurrent along the student response
sequence. After learning/forgetting knowledge in the third stage,
the updated knowledge memory is prepared for the first stage to
answer the next question. This makes GRKT an end-to-end style
so we directly train the model by the binary cross-entropy loss,
aligning the predictive probability ˆ𝑎𝑢
𝑡from Equation 11 with the
ground-truth response correctness label 𝑎𝑢
𝑡:
L=−∑︁
𝑢∈U∑︁
𝑟𝑢
𝑡∈H𝑢𝑎𝑢
𝑡logˆ𝑎𝑢
𝑡+(1−𝑎𝑢
𝑡)log(1−ˆ𝑎𝑢
𝑡). (25)
Here, we omit the averaging notation for brevity. Besides, we also
apply the𝑙2normalization to the model parameters during the
training process to avoid the over-fitting issue.
5 EXPERIMENTS
In this section, we design comprehensive experiments to address
the following research questions:
Q1: Does GRKT achieve competitive results in terms of both pre-
diction performance and knowledge tracing reasonability
compared to current state-of-the-art DLKT methods?
Q2: What are the roles and impacts of different components of
GRKT on the overall performance and reasonability?
Q3: How reasonable is the knowledge mastery traced by GRKT
from an intuitive perspective?
Additionally, we conduct other experiments such as hyper-parameter
analysis. Due to space constraints, we include them in Appendix C.4.5.1 Experimental Setup
5.1.1 Datasets. We evaluate the performance of GRKT on three
widely-used public KT datasets:
•ASSIST09 [8]1: ASSISTments is an online tutoring system for
mathematics, which collected this dataset from 2009 to 2010. We
use the combined version. For the missing timestamp information,
we approximate it using the field order_id.
•ASSIST12 [8]2: Another dataset from ASSISTments, collected
during the period of 2012 to 2013.
•Junyi [1]3: This dataset is collected from the Junyi Academy
online platform in 2015. It contains a part of annotated KC rela-
tionships which are suitable for the requirements of GRKT. We
use the junyi_ProblemLog_original.csv version.
For preprocessing each dataset, we partition the response se-
quences of every student into subsequences, each containing 100
responses. Subsequences containing fewer than 10 responses are
eliminated, while those with less than 100 responses are padded
with zeros to meet the required length. Statistics of the processed
datasets can be found in Table 1.
5.1.2 Evaluation. As a binary classification task of predicting stu-
dent responses, we utilize the area under the curve (AUC) and ac-
curacy (ACC) as the evaluation metrics for prediction performance.
For evaluating model reasonability, we introduce three metrics:
•Consistency: We propose this metric to measure the ratio of
consistent variation between the mastery of KCs. When a stu-
dent’s mastery of the corresponding KC declines after answering
a certain question, the mastery of other KCs should either decline
(for related KCs) or remain unchanged (for unrelated KCs). We
calculate this percentage.
•GAUCM: This metric calculates the average AUC scores with
respect to the mastery of each question’s examined KC. Its reflects
the monotonicity assumption: a question could be more likely to
be correctly answered if students have higher mastery of its KC.
This metric is proposed by Zhang et al. [41].
•Repetition: This metric is proposed by Yeung et al. [ 37], stating
that a reasonable KT method should satisfy: after a student has
finished a question and is given this same question again, the
response result (correct or incorrect) should remain the same.
We calculate the accuracy under this circumstance.
The formulas of these metrics are presented in Appendix C.1. More-
over, we employ a five-fold cross-validation to assess the model’s
performance. 10% of the sequences of each fold serve as the val-
idation set for parameter tuning. We stop the training when the
validation performance fails to improve for 10 consecutive epochs.
5.1.3 Baselines. To compare with mainstream DLKT methods cov-
ering different aspects, we select eleven baselines from 2015 to 2023,
including DKT [ 23], DKVMN [ 40], DKT+ [ 37], SAKT [ 20], GKT [ 19],
AKT [ 11], SKT [ 30], LPKT [ 27], DIMKT [ 26], Dtransformer [ 38] and
LBKT [ 34]. Among them, GKT and SKT leverages the KC graph,
and LPKT leverage the timestamp information. DKT+, LPKT and
Dtransformer consider some aspects of model reasonability: the
1https://sites.google.com/site/assistmentsdata/home/2009-2010-assistment-data
2https://sites.google.com/site/assistmentsdata/home/2012-13-school-data-with-
affect
3https://pslcdatashop.web.cmu.edu/Files?datasetId=1198
 
508KDD ’24, August 25–29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
Table 2: Results of the main experiments. The best results among GRKT and the baselines are in bold. The second ones are in
italic. * indicates statistical significance over the best baseline, measured by T-test with p-value ≤0.05. “CONS”, “GAUC” and
“RPT” are short for the three metrics for reasonability, consistency, GAUCM and Repetition.
Dataset ASSIST09 ASSIST12 Junyi
Metric AUC ACC CONS GAUC RPT AUC ACC CONS GAUC RPT AUC ACC CONS GAUC RPT
DKT 0.7695 0.7246 0.6463 0.7172 0.8131 0.7303 0.7358 0.6772 0.6929 0.7955 0.8003 0.8541 0.7432 0.6415 0.8790
DKVMN 0.7680 0.7239 0.8708 0.7116 0.8061 0.7279 0.7349 0.9273 0.6729 0.7971 0.8004 0.8541 0.9455 0.6379 0.8780
DKT+ 0.7707 0.7245 0.6364 0.7089 0.8395 0.7300 0.7353 0.6809 0.6766 0.8172 0.7993 0.8539 0.7624 0.6436 0.8869
SAKT 0.7634 0.7206 0.8539 0.7101 0.7749 0.7227 0.7329 0.8202 0.6866 0.7797 0.7995 0.8535 0.8600 0.6387 0.8747
GKT 0.7702 0.7252 0.6697 0.7183 0.8124 0.7339 0.7372 0.7450 0.6971 0.7986 0.8023 0.8547 0.7403 0.6398 0.8788
AKT 0.7820 0.7320 0.5870 0.7113 0.8184 0.7665 0.7514 0.5909 0.6892 0.8172 0.8161 0.8593 0.5810 0.6398 0.8734
SKT 0.7732 0.7273 0.7023 0.7098 0.8092 0.7354 0.7398 0.7813 0.6952 0.7934 0.8045 0.8552 0.7792 0.6420 0.8805
LPKT 0.7869 0.7369 0.7909 0.7124 0.8205 0.7740 0.7556 0.8174 0.6839 0.8255 0.8153 0.8585 0.7238 0.6453 0.8845
DIMKT 0.7814 0.7351 0.7899 0.7153 0.8221 0.7711 0.7550 0.8099 0.6995 0.8198 0.8163 0.8594 0.8945 0.6424 0.8850
DTrans 0.7858 0.7345 0.8928 0.7126 0.8253 0.7720 0.7542 0.9217 0.6863 0.8249 0.8149 0.8577 0.9274 0.6420 0.8893
LBKT 0.7865 0.7372 0.8054 0.7134 0.8225 0.7763 0.7562 0.8123 0.6814 0.8230 0.8140 0.8568 0.8123 0.6409 0.8871
GRKT 0.7914* 0.7398* 1.0000* 0.7209* 0.8486* 0.7794* 0.7576 1.0000* 0.7064* 0.8319* 0.8207* 0.8624* 1.0000* 0.6473* 0.8957*
improv. 0.57% 0.35% 12.01% 0.36% 1.09% 0.40% 0.19% 8.50% 0.98% 0.78% 0.54% 0.35% 7.83% 0.31% 0.72%
Table 3: Results of the ablation experiments.
Dataset ASSIST09 ASSIST12 Junyi
Metric AUC ACC CONS GAUC RPT AUC ACC CONS GAUC RPT AUC ACC CONS GAUC RPT
GRKT 0.7914 0.7398 1.0000 0.7209 0.8486 0.7794 0.7576 1.0000 0.7064 0.8319 0.8207 0.8624 1.0000 0.6473 0.8957
-LF 0.7871 0.7367 1.0000 0.7066 0.8243 0.7767 0.7558 1.0000 0.6809 0.8276 0.8170 0.8598 1.0000 0.6401 0.8815
-SIM-PRE 0.7578 0.7161 1.0000 0.6197 0.8246 0.7502 0.7424 1.0000 0.6223 0.8291 0.7921 0.8481 1.0000 0.6084 0.8781
-SIM 0.7896 0.7375 1.0000 0.7135 0.8402 0.7777 0.7564 1.0000 0.6888 0.8259 0.8186 0.8611 1.0000 0.6447 0.8862
-PRE 0.7897 0.7384 1.0000 0.7149 0.8437 0.7779 0.7563 1.0000 0.6915 0.8264 0.8191 0.8615 1.0000 0.6452 0.8897
knowledge tracing stability or learning/forgetting behaviors, but
not comprehensively address the DLKT unreasonableness issue. For
the methods not providing the proxy of tracing knowledge mastery,
AKT and DIMKT, we follow previous works [ 4,16] that replace
input question features with zeros to estimate the mastery. We note
that cognitive diagnosis baselines are not considered because they
usually focus on static testing environments [ 15] but we study in
the dynamic learning situation.
5.1.4 Implementation Details. We employ the Adam optimizer [ 13]
for all methods to achieve their best performance. We choose their
learning rates from {1e-2, 5e-3, 1e-3, 5e-4, 1e-4}, and fixed the em-
bedding and hidden dimension numbers at 128 for fairness. We
strictly follow the original papers of all methods to set their hyper-
parameters. For GRKT, detailed hyper-parameter setting is referred
in the Appendix C.2. Furthermore, for the non-negative constraint
on the specified network weights in Equations 7 and 10, we use
the softmax operation along the knowledge memory dimension,
which performs best in practice. Besides, the Junyi dataset includes
some labeled relations, which we experiment with and present the
results in Appendix C.3.
5.2 Overall Performance (Q1)
Table 2 illustrates the comprehensive performance comparison be-
tween GRKT and eleven other baselines. Notably, GRKT showcases
the highest efficacy, surpassing the leading baselines by marginsranging from 0.19% to 12.01% across both prediction performance
and reasonability metrics. For metrics such as AUC and ACC, which
primarily gauge predictive accuracy, the state-of-the-art DLKT tech-
niques, LPKT, and DIMKT exhibit exemplary performance owing
to their sophisticated neural architectures. Besides, methods that
emphasize aspects of reasonability, such as enhancing knowledge
tracing stability and explicitly modeling learning and forgetting be-
haviors, DKT+, LPKT, and DTransformer, demonstrate competitive
performance across reasonableness metrics. These methods secure
seven out of nine second-place positions in reasonability metrics.
Remarkably, GRKT achieves a perfect score of 1.0 on the consistency
metric, signifying its ability to effectively address the challenge of
maintaining consistency in knowledge mastery changes across KCs
by the network constraints.
5.3 Ablation Study (Q2)
The ablation study aims to evaluate the impact of each component
in GRKT by removing specific techniques and comparing the results
with the full model. Four components are removed:
•-LF: Removal of the third stage, knowledge learning/forgetting.
•-SIM: Removal of the similarity relation.
•-PRE: Removal of the prerequisite relation.
•-SIM-PRE: Removal of the leverage of KC relation graphs.
As shown in Table 3, GRKT-SIM-PRE experiences the most sig-
nificant deterioration, emphasizing the crucial role of KC relations
in the KT task. Moreover, when only one of these two relations is
 
509Graph-based Reasonable Knowledge Tracing KDD ’24, August 25–29, 2024, Barcelona, Spain
Area TriangleTimeline
Calculations with similar Figures Conversion of Fraction Decimals Percents Ordering Integers Multiplication and Division Positive DecimalsGRKT
Forgetting Curve
Learning CurveTesting Effect
No Mastery Change of Unrelated KCConsistent Mastery Change Direction
Mastery Change of Related KC :
Transfer  of Learnin g
Figure 3: Case study of the same student’s evolving knowledge mastery exemplified in Section 1.
GRKT
LPKT
DKTTimeline testing effect forgetting curve
no mastery change of related KCs mastery change of unrelated KCs
Addition and Subtraction Integers Multiplication and Division Integers Square Rootshigh mastery
low mastery
Correct Responses
Incorrect Responses
Figure 4: Knowledge tracing heatmap of GRKT, LPKT and DKT tracing one another student’s mastery on KC Addition and
Subtraction Integer s. Different colors represent different KCs.
utilized, there is a notable improvement in performance, indicating
that each provides meaningful information for GRKT. Moreover,
the performance is further enhanced when both relations are used
together. Additionally, the degradation of GRKT-LF underscores the
importance of modeling the knowledge learning/forgetting stage.
5.4 Reasonable Knowledge Tracing (Q3)
To intuitively validate the resonability of GRKT, we present one
student’s dynamic knowledge mastery traced by GRKT in Figure 3.
As depicted, the result aligns well with our hypothesis of a com-
prehensive and reasonable knowledge tracing model integrating
various effects based on pedagogical theories. Furthermore, it ad-
dresses three key issues in the reasonableness of existing DLKT
methods: mastery changes of unrelated KCs, not mastery changes
of related KCs, and inconsistent mastery change direction. We also
present GRKT, LPKT and DKT tracing one another student’s mas-
tery on KC Addition and Subtraction Integers in Figure 4. As shown,
GRKT yields reasonable knowledge tracing results such as the fine-
grained knowledge changing from testing effects and the faded
knowledge with forgetting curves. LPKT and DKT still have rea-
sonable issues such as the mastery change of unrelated KCs and no
mastery change of related KCs.
5.5 Complexity Analysis
Although the detailed methodology description of GRKT, its inter-
nal composition of only GNNs and MLPs does not make the infer-
ence complicated. Suppose 𝑡is the length of response sequence,
𝐶is the KC set, 𝐸is the KC relation edge set, 𝑑is the hidden di-
mension number we set as a small value of 16, and 𝑘is the GRKT’smemory dimension number. The time complexity of GRKT is then
𝑂(𝑡|𝐸|𝑘+|𝐸|𝑑+𝑡|𝐶|𝑘2+|𝐶|𝑑2+𝑡𝑑2), consisting of feature ag-
gregation𝑂(𝑡|𝐸|𝑘+|𝐸|𝑑)and feature non-linear transformation
𝑂(𝑡|𝐶|𝑘2+|𝐶|𝑑2)of the GNNs, and 𝑂(𝑡𝑑2)of the MLPs. In contrast,
other comparable attention or RNN-based methods usually have
time complexity 𝑂(𝑡𝑑2+𝑡2𝑑). In real scenarios, 𝑡,|𝐶|,𝑑usually lie
in 100-200 and the KC relation graphs are sparse. Therefore, we
can approximately assume 𝑡=𝑑=|𝐶|=𝑘2=𝑛and|𝐸|=𝑘·|𝐶|to
facilitate the complexity comparison, which indicates the GRKT’s
time complexity is actually in the same order of magnitude 𝑂(𝑛3)
as other methods. We also test the inference speed of GRKT. It av-
eragely costs 60ms for one student, which is acceptable in practice.
6 CONCLUSION
In this paper, we point out the issue that many existing DLKT
approaches prioritize predictive accuracy over tracking students’
dynamic knowledge mastery. This often results in models that
yield unreasonable outcomes, complicating their application in
real teaching scenarios. To this end, our study introduces GRKT, a
graph-based reasonable knowledge tracing. It employs graph neural
networks and consists of a finer-grained three-stage modeling pro-
cess based on pedagogical theories, conducting a more reasonable
knowledge tracing. Extensive experiments across multiple datasets
demonstrate that GRKT not only enhances predictive accuracy but
also generates more reasonable knowledge tracing results. In the
future, we plan to address certain limitations of GRKT, such as en-
hancing the model’s ability to provide more fine-grained responses,
including multiple-choice or essay answers. Furthermore, we would
evaluate GRKT in real teaching scenarios.
 
510KDD ’24, August 25–29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
REFERENCES
[1]Haw-Shiuan Chang, Hwai-Jung Hsu, and Kuan-Ta Chen. 2015. Modeling Exercise
Relationships in E-Learning: A Unified Approach.. In EDM. 532–535.
[2]Youngduck Choi, Youngnam Lee, Junghyun Cho, Jineon Baek, Byungsoo Kim,
Yeongmin Cha, Dongmin Shin, Chan Bae, and Jaewe Heo. 2020. Towards an ap-
propriate query, key, and value computation for knowledge tracing. In Proceedings
of the seventh ACM conference on learning@ scale. 341–344.
[3]Albert T Corbett and John R Anderson. 1994. Knowledge tracing: Modeling the
acquisition of procedural knowledge. User modeling and user-adapted interaction
4 (1994), 253–278.
[4]Jiajun Cui, Zeyuan Chen, Aimin Zhou, Jianyong Wang, and Wei Zhang. 2023. Fine-
Grained Interaction Modeling with Multi-Relational Transformer for Knowledge
Tracing. ACM Transactions on Information Systems 41, 4 (2023), 1–26.
[5]Jiajun Cui, Minghe Yu, Bo Jiang, Aimin Zhou, Jianyong Wang, and Wei Zhang.
2024. Interpretable Knowledge Tracing via Response Influence-based Counter-
factual Reasoning. In Proceedings of the 40th IEEE International Conference on
Data Engineering.
[6]Hermann Ebbinghaus. 1885. Über das gedächtnis: untersuchungen zur experi-
mentellen psychologie. Duncker & Humblot.
[7]Susan E Embretson and Steven P Reise. 2013. Item response theory. Psychology
Press.
[8]Mingyu Feng, Neil Heffernan, and Kenneth Koedinger. 2009. Addressing the
assessment challenge with an online system that tutors as it assesses. User
modeling and user-adapted interaction 19 (2009), 243–266.
[9]Wenbin Gan, Yuan Sun, and Yi Sun. 2022. Knowledge structure enhanced graph
representation learning model for attentive knowledge tracing. International
Journal of Intelligent Systems 37, 3 (2022), 2012–2045.
[10] Weibo Gao, Hao Wang, Qi Liu, Fei Wang, Xin Lin, Linan Yue, Zheng Zhang,
Rui Lv, and Shijin Wang. 2023. Leveraging transferable knowledge concept
graph embedding for cold-start cognitive diagnosis. In Proceedings of the 46th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 983–992.
[11] Aritra Ghosh, Neil Heffernan, and Andrew S Lan. 2020. Context-aware atten-
tive knowledge tracing. In Proceedings of the 26th ACM SIGKDD international
conference on knowledge discovery & data mining. 2330–2339.
[12] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca
Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining black
box models. ACM computing surveys (CSUR) 51, 5 (2018), 1–42.
[13] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[14] Nate Kornell, Matthew Jensen Hays, and Robert A Bjork. 2009. Unsuccessful
retrieval attempts enhance subsequent learning. Journal of Experimental Psychol-
ogy: Learning, Memory, and Cognition 35, 4 (2009), 989.
[15] Jacqueline Leighton and Mark Gierl. 2007. Cognitive diagnostic assessment for
education: Theory and applications. Cambridge University Press.
[16] Qi Liu, Zhenya Huang, Yu Yin, Enhong Chen, Hui Xiong, Yu Su, and Guoping Hu.
2019. Ekt: Exercise-aware knowledge tracing for student performance prediction.
IEEE Transactions on Knowledge and Data Engineering 33, 1 (2019), 100–115.
[17] Qi Liu, Shuanghong Shen, Zhenya Huang, Enhong Chen, and Yonghe Zheng.
2021. A survey of knowledge tracing. arXiv preprint arXiv:2105.15106 (2021).
[18] Arthur W Melton. 1963. Implications of short-term memory for a general theory
of memory. Journal of verbal Learning and verbal Behavior 2, 1 (1963), 1–21.
[19] Hiromi Nakagawa, Yusuke Iwasawa, and Yutaka Matsuo. 2019. Graph-based
knowledge tracing: modeling student proficiency using graph neural network.
InIEEE/WIC/ACM International Conference on Web Intelligence. 156–163.
[20] Shalini Pandey and George Karypis. 2019. A Self-Attentive Model for Knowledge
Tracing. International Educational Data Mining Society (2019).
[21] Zachary A Pardos and Neil T Heffernan. 2011. KT-IDEM: Introducing item
difficulty to the knowledge tracing model. In User Modeling, Adaption and Per-
sonalization: 19th International Conference, UMAP 2011, Girona, Spain, July 11-15,
2011. Proceedings 19. Springer, 243–254.
[22] David N Perkins, Gavriel Salomon, et al .1992. Transfer of learning. International
encyclopedia of education 2 (1992), 6452–6457.
[23] Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami,
Leonidas J Guibas, and Jascha Sohl-Dickstein. 2015. Deep knowledge tracing.
Advances in neural information processing systems 28 (2015).
[24] Henry L Roediger III and Jeffrey D Karpicke. 2006. Test-enhanced learning:
Taking memory tests improves long-term retention. Psychological science 17, 3
(2006), 249–255.
[25] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini. 2008. The graph neural network model. IEEE transactions on neural
networks 20, 1 (2008), 61–80.
[26] Shuanghong Shen, Zhenya Huang, Qi Liu, Yu Su, Shijin Wang, and Enhong Chen.
2022. Assessing Student’s Dynamic Knowledge State by Exploring the Question
Difficulty Effect. In Proceedings of the 45th International ACM SIGIR Conference
on Research and Development in Information Retrieval. 427–437.[27] Shuanghong Shen, Qi Liu, Enhong Chen, Zhenya Huang, Wei Huang, Yu Yin, Yu
Su, and Shijin Wang. 2021. Learning process-consistent knowledge tracing. In
Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data
mining. 1452–1460.
[28] Xiangyu Song, Jianxin Li, Qi Lei, Wei Zhao, Yunliang Chen, and Ajmal Mian. 2022.
Bi-CLKT: Bi-graph contrastive learning based knowledge tracing. Knowledge-
Based Systems 241 (2022), 108274.
[29] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. 2017.
Inception-v4, inception-resnet and the impact of residual connections on learning.
InProceedings of the AAAI conference on artificial intelligence, Vol. 31.
[30] Shiwei Tong, Qi Liu, Wei Huang, Zhenya Hunag, Enhong Chen, Chuanren Liu,
Haiping Ma, and Shijin Wang. 2020. Structure-based knowledge tracing: An
influence propagation view. In 2020 IEEE international conference on data mining
(ICDM). IEEE, 541–550.
[31] Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yu Yin, Shijin Wang, and Yu Su.
2022. NeuralCD: a general framework for cognitive diagnosis. IEEE Transactions
on Knowledge and Data Engineering (2022).
[32] Xinping Wang, Caidie Huang, Jinfang Cai, and Liangyu Chen. 2021. Using knowl-
edge concept aggregation towards accurate cognitive diagnosis. In Proceedings of
the 30th ACM International Conference on Information & Knowledge Management.
2010–2019.
[33] Siyu Wu, Yang Cao, Jiajun Cui, Runze Li, Hong Qian, Bo Jiang, and Wei Zhang.
2024. A Comprehensive Exploration of Personalized Learning in Smart Education:
From Student Modeling to Personalized Recommendations. arXiv:2402.01666
[34] Bihan Xu, Zhenya Huang, Jiayu Liu, Shuanghong Shen, Qi Liu, Enhong Chen,
Jinze Wu, and Shijin Wang. 2023. Learning behavior-oriented knowledge tracing.
InProceedings of the 29th ACM SIGKDD conference on knowledge discovery and
data mining. 2789–2800.
[35] Yang Yang, Jian Shen, Yanru Qu, Yunfei Liu, Kerong Wang, Yaoming Zhu, Weinan
Zhang, and Yong Yu. 2021. GIKT: a graph-based interaction model for knowledge
tracing. In Machine Learning and Knowledge Discovery in Databases: European
Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings,
Part I. Springer, 299–315.
[36] Louis E Yelle. 1979. The learning curve: Historical review and comprehensive
survey. Decision sciences 10, 2 (1979), 302–328.
[37] Chun-Kit Yeung and Dit-Yan Yeung. 2018. Addressing two problems in deep
knowledge tracing via prediction-consistent regularization. In Proceedings of the
fifth annual ACM conference on learning at scale. 1–10.
[38] Yu Yin, Le Dai, Zhenya Huang, Shuanghong Shen, Fei Wang, Qi Liu, Enhong
Chen, and Xin Li. 2023. Tracing Knowledge Instead of Patterns: Stable Knowledge
Tracing with Diagnostic Transformer. In Proceedings of the ACM Web Conference
2023. 855–864.
[39] Michael V Yudelson, Kenneth R Koedinger, and Geoffrey J Gordon. 2013. Individ-
ualized bayesian knowledge tracing models. In Artificial Intelligence in Education:
16th International Conference, AIED 2013, Memphis, TN, USA, July 9-13, 2013.
Proceedings 16. Springer, 171–180.
[40] Jiani Zhang, Xingjian Shi, Irwin King, and Dit-Yan Yeung. 2017. Dynamic key-
value memory networks for knowledge tracing. In Proceedings of the 26th inter-
national conference on World Wide Web. 765–774.
[41] Moyu Zhang, Xinning Zhu, Chunhong Zhang, Wenchen Qian, Feng Pan, and
Hui Zhao. 2023. Counterfactual Monotonic Knowledge Tracing for Assessing
Students’ Dynamic Mastery of Knowledge Concepts. In Proceedings of the 32nd
ACM International Conference on Information and Knowledge Management. 3236–
3246.
A NOTATION TABLE
We list and explain the notations in our methodology introduction
in Table 4 and 5.
B METHOD DETAILS
B.1 KC Relation Graph Construction
In the absence of KC relation annotations in the datasets, we con-
struct the KC relation graph based on data statistics. For the sim-
ilarity between KCs 𝑐𝑖and𝑐𝑗, we estimate their similarity score
using:
𝑠𝑖𝑚𝑐𝑖,𝑐𝑗=Í
𝑢∈UÍ
𝑟𝑢
𝑡,𝑟𝑢
𝑡′∈H𝑢𝐼(𝑎𝑢
𝑡=𝑎𝑢
𝑡′,𝑐𝑢
𝑡=𝑐𝑖,𝑐𝑢
𝑡′=𝑐𝑗)
Í
𝑢∈UÍ
𝑟𝑢
𝑡,𝑟𝑢
𝑡′∈H𝑢𝐼(𝑐𝑢
𝑡=𝑐𝑖,𝑐𝑢
𝑡′=𝑐𝑗),
where𝐼(·)is the indicator function that takes value 1 if the condi-
tion is satisfied. This approximates the probability that a student
 
511Graph-based Reasonable Knowledge Tracing KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 4: The notation table of GRKT. We omit the superscript
of the target student 𝑢whose knowledge is to be traced.
Task formulation
U,Q,C sets of students, questions, KCs
𝑐𝑖,𝑐𝑗,𝑞𝑖,𝑞𝑗 certain KCs, questions
𝑢,𝑡 the target student, time step
H response history of 𝑢
𝑟𝑡 response of 𝑢at𝑡
𝑞𝑡,𝑐𝑡 question and examined KC of 𝑟𝑡
𝑎𝑡,𝑇𝑡 binary correctness and timestamp of 𝑟𝑡
M evolving knowledge mastery of 𝑢
m𝑡 knowledge mastery of 𝑢at𝑡
𝑚𝑐𝑖,𝑡 knowledge mastery of 𝑐𝑖of𝑢at𝑡
KC relation-based GNN
P,S,R prerequisite, subsequence, similarity graphs
P(·),S(·),R(·) neighbor functions of P,S,R
G certain graph inP,S,R
G(·) neighbor function of G
𝐿 number of GNN layers
GNN𝑝𝑟𝑜𝑡𝑜 prototype of KC relation-based GNN
𝑑0,𝑑1,...,𝑑𝐿 # of dimensions of prototype GNN’s layers
˜f(0)
𝑐𝑖,˜F(0)prototype input of 𝑐𝑖and all to GNN 𝑝𝑟𝑜𝑡𝑜
˜f(𝐿)
𝑐𝑖,˜F(𝐿)prototype output of 𝑐𝑖and all from GNN 𝑝𝑟𝑜𝑡𝑜
˜f(𝑙)
𝑐𝑖,˜F(𝑙) prototype intermedium of 𝑐𝑖and all of
𝑙𝑡ℎlayer of GNN 𝑝𝑟𝑜𝑡𝑜
WG,(𝑙)
𝑝𝑟𝑜𝑡𝑜,OG,(𝑙)
𝑝𝑟𝑜𝑡𝑜weight matrices of 𝑙𝑡ℎof GNN𝑝𝑟𝑜𝑡𝑜 forG
˜fG,(𝑙)
𝑐𝑖prototype intermedium of 𝑐𝑖of𝑙𝑡ℎlayer
of GNN𝑝𝑟𝑜𝑡𝑜 forG
GRKT basic factors
e𝑞𝑖,e𝑞𝑡,k𝑐𝑖,k𝑐𝑡 embeddings of 𝑞𝑖,𝑞𝑡,𝑐𝑖,𝑐𝑡
¯e𝑞𝑖,¯e𝑞𝑡 concatenation of 𝑞𝑖and its KC’s embeddings
𝛼𝑞𝑡,𝑐𝑗 requirement score of 𝑞𝑡requiring𝑐𝑗
W𝑟𝑒𝑞 matrix to calculate requiring scores
𝛽G
𝑐𝑖,𝑐𝑗correlation score of 𝑐𝑖and𝑐𝑗forG
WG
𝑐𝑜𝑟 matrix to calculate correlation scores for G
H0 initial knowledge memory of 𝑢
H𝑡− knowledge memory of 𝑢at a moment before 𝑡
H𝑡 knowledge memory of 𝑢at𝑡
h𝑐𝑖,𝑡 knowledge memory of 𝑐𝑖of𝑢at𝑡
𝑑𝑒,𝑑𝑘,𝑑ℎ embedding, memory, and hidden dimensions
wℎ vector to project knowledge memory to mastery
ˆ𝑚𝑐𝑖,𝑡 modeled knowledge mastery of 𝑐𝑖at𝑡
𝑑𝑞𝑡 question difficulty of 𝑞𝑡
MLP𝑑𝑖𝑓𝑓 MLP to generate question difficulty
W(1)
𝑑𝑖𝑓𝑓,W(2)
𝑑𝑖𝑓𝑓weight matrices in MLP 𝑑𝑖𝑓𝑓
b(1)
𝑑𝑖𝑓𝑓,b(2)
𝑑𝑖𝑓𝑓weight vectors in MLP 𝑑𝑖𝑓𝑓
could answer questions of 𝑐𝑖correctly while he/her could also an-
swer questions of 𝑐𝑗correctly (or both incorrectly), indicating an
underlying similarity between them.
For the prerequisite relationship between 𝑐𝑖and𝑐𝑗, we assume
that if𝑐𝑖is prerequisite to 𝑐𝑗, then answering questions of 𝑐𝑖cor-
rectly but𝑐𝑗incorrectly is more likely than answering questions ofTable 5: The continuing notation table for the three stages.
Stage I: knowledge retrieval
GNN𝑟𝑡𝑣 KC relation-based GNN for knowledge retrieval
˜h(0)
𝑐𝑖,𝑡−,˜H(0)
𝑡− memory input of 𝑐𝑖and all to GNN 𝑟𝑡𝑣before𝑡
˜h(𝐿)
𝑐𝑖,𝑡−,˜H(𝐿)
𝑡−memory output of 𝑐𝑖and all from GNN 𝑟𝑡𝑣before𝑡
ˆ𝑎𝑡 predictive probability of 𝑎𝑡
Stage II: memory strengthening
MLP𝑔𝑎𝑖𝑛 MLP to get memory feature for knowledge gain
g𝑐𝑡,𝑡 memory feature of 𝑐𝑡at𝑡for knowledge gain
GNN𝑔𝑎𝑖𝑛 KC relation-based GNN for knowledge gain
˜g(0)
𝑐𝑡,𝑡,˜G(0)
𝑡 memory feature input of 𝑐𝑡and all to GNN 𝑔𝑎𝑖𝑛 at𝑡
˜g(𝐿)
𝑐𝑡,𝑡,˜G(𝐿)
𝑡 knowledge gain of 𝑐𝑡and all from GNN 𝑔𝑎𝑖𝑛 at𝑡
MLP𝑙𝑜𝑠𝑠 MLP to get memory feature for knowledge loss
l𝑐𝑡,𝑡 memory feature of 𝑐𝑡at𝑡for knowledge loss
GNN𝑙𝑜𝑠𝑠 KC relation-based GNN for knowledge loss
˜l(0)
𝑐𝑡,𝑡,˜L(0)
𝑡 memory feature input of 𝑐𝑡and all to GNN 𝑙𝑜𝑠𝑠at𝑡
˜l(𝐿)
𝑐𝑖,𝑡,˜L(𝐿)
𝑡 knowledge loss of 𝑐𝑡and all from GNN 𝑙𝑜𝑠𝑠at𝑡
Stage III: knowledge learning/forgetting
MLP𝑑𝑠𝑐 MLP to get policy distribution for active learning
𝜋𝑐𝑖,𝑡 policy distribution if 𝑢decide to learn 𝑐𝑖at𝑡
MLP𝑝𝑟𝑔 MLP to get initial knowledge progress
p𝑐𝑖,𝑡 initial knowledge progress of 𝑐𝑖at𝑡
GNN𝑝𝑟𝑔 KC relation-based GNN for knowledge progress
˜p(0)
𝑐𝑖,𝑡,˜P(0)
𝑡 initial progress input of 𝑐𝑖and all to GNN 𝑝𝑟𝑔at𝑡
˜p(𝐿)
𝑐𝑖,𝑡,˜P(𝐿)
𝑡 knowledge progress of 𝑐𝑖and all from GNN 𝑝𝑟𝑔at𝑡
Δ𝑇𝑡+1 time interval between 𝑇𝑡and𝑇𝑡+1
𝝓𝑐𝑖KC-specific time-aware kernel for learning 𝑐𝑖
𝑛𝑐𝑖,𝑡 # of times𝑢has learnt𝑐𝑖
GNN𝑙𝑟𝑛 KC relation-based GNN to get parameters of 𝜸𝑐𝑖
˜𝜸(0)
𝑐𝑖input feature of 𝑐𝑖initialized as k𝑐𝑖to GNN𝑙𝑟𝑛
˜𝜸(𝐿)
𝑐𝑖output parameters of 𝜸𝑐𝑖for𝑐𝑖from GNN 𝑙𝑟𝑛
𝜿𝑐𝑖 KC-specific time-aware kernel for forgetting 𝑐𝑖
GNN𝑓𝑔𝑡 KC relation-based GNN to get parameters of 𝜽𝑐𝑖
˜𝜽(0)
𝑐𝑖input feature of 𝑐𝑖initialized as k𝑐𝑖to GNN𝑓𝑔𝑡
˜𝜽(𝐿)
𝑐𝑖output parameters of 𝜿𝑐𝑖for𝑐𝑖from GNN 𝑓𝑔𝑡
𝑐𝑖incorrectly but 𝑐𝑗correctly. Therefore, we use:
𝑝𝑟𝑒𝑐𝑖,𝑐𝑗=Í
𝑢∈UÍ
𝑟𝑢
𝑡,𝑟𝑢
𝑡′∈H𝑢𝐼(𝑎𝑢
𝑡=1,𝑎𝑢
𝑡′=0,𝑐𝑢
𝑡=𝑐𝑖,𝑐𝑢
𝑡′=𝑐𝑗)
Í
𝑢∈UÍ
𝑟𝑢
𝑡,𝑟𝑢
𝑡′∈H𝑢𝐼(𝑎𝑢
𝑡≠𝑎𝑢
𝑡′,𝑐𝑢
𝑡=𝑐𝑖,𝑐𝑢
𝑡′=𝑐𝑗),
to approximate the probability that 𝑐𝑖is a prerequisite to 𝑐𝑗.
Finally, we set a threshold 𝜂to determine whether 𝑐𝑖is simi-
lar/prerequisite to 𝑐𝑗(by𝑠𝑖𝑚𝑐𝑖,𝑐𝑗≥𝜂and𝑝𝑟𝑒𝑐𝑖,𝑐𝑗≥𝜂, respectively).
Additionally, KC pairs with a co-occurrence frequency under 10
times in the dataset are not considered.
C SUPPLEMENTS FOR EXPERIMENTS
C.1 Metrics for Reasonability
We formulate the three metrics for model reasonability in this
section:
 
512KDD ’24, August 25–29, 2024, Barcelona, Spain Jiajun Cui, Hong Qian, Bo Jiang, & Wei Zhang
Table 6: Hyperparameter setting of GRKT applying for the
three datasets.
Parameter ASSIST09 ASSIST12 Junyi
𝑙𝑟 5e-3 5e-3 5e-3
𝐿 2 2 2
𝑑𝑘 16 16 16
𝜂 0.6 0.7 0.8
𝑙2 1e-6 1e-5 1e-5
C.1.1 Consistency. This metric measures the ratio of consistent
variation between the mastery of KCs:
𝐶𝑜𝑛𝑠𝑖𝑠𝑡𝑒𝑛𝑐𝑦 =∑︁
𝑢∈U∑︁
𝑟𝑢
𝑡∈H𝑢Í
𝑐𝑖∈C𝐼(𝑚𝑢
𝑐𝑖,𝑡≥𝑚𝑢
𝑐𝑖,𝑡+1)
Í
𝑐𝑖∈C𝐼(𝑚𝑢
𝑐𝑢
𝑡,𝑡≥𝑚𝑢
𝑐𝑢
𝑡,𝑡+1). (26)
Here, we omit the averaging operation over the students and re-
sponses for conciseness. We only consider the situation where a
student’s mastery of the learnt KC of the current question declines
while other KCs do not increase, instead of the current one increas-
ing and the others declining. This is because the latter case might
be due to natural forgetting behaviors.
C.1.2 GAUCM. This metric calculates the average AUC scores
with respect to the mastery of each question’s examined KC:
𝐺𝐴𝑈𝐶𝑀 =Í
𝑞𝑖∈Q𝑁(𝑞𝑖)·𝐴𝑈𝐶h
{ˆ𝑚𝑢
𝑐𝑢
𝑡,𝑡},{𝑎𝑢
𝑡}i𝑞𝑢
𝑡=𝑞𝑖
𝑢∈U,𝑟𝑢
𝑡∈H𝑢
Í
𝑞𝑖∈Q𝑁(𝑞𝑖).
(27)
𝐴𝑈𝐶[ˆY,Y]𝐵
𝐴indicates the AUC score of the prediction set ˆYand
the ground-truth set Y, given the range 𝐴and the condition 𝐵.
𝑁(𝑞𝑖)is the number of 𝑞𝑖being answered. For evaluating GRKT,
we use the aggregated mastery instead of the single KC’s mastery to
calculate AUC because we consider the transfer of learning theory
that students may leverage related KCs to solve questions.
C.1.3 Repetition. This metric supposes that a reasonable KT method
should adhere to the following rule: after a student has finished a
question and is given the same question again, the response result
(correct or incorrect) should remain the same:
𝑅𝑒𝑝𝑒𝑡𝑖𝑡𝑖𝑜𝑛 =𝐴𝐶𝐶
{KT(𝑞𝑢
𝑡|{𝑟𝑢
𝑡′|1≤𝑡′≤𝑡})},{𝑎𝑢
𝑡}
𝑢∈U,𝑟𝑢
𝑡∈H𝑢.
(28)
𝐴𝐶𝐶(·)denotes the accuracy score whose notation is similar to
the𝐴𝑈𝐶(·)in Equation 27. KT(𝑞𝑢
𝑡|{𝑟𝑢
𝑡′|1≤𝑡′≤𝑡})denotes the
prediction score if 𝑢could correctly answer 𝑞𝑢
𝑡given his/her past 𝑡
responses{𝑟𝑢
𝑡′|1≤𝑡′≤𝑡}including the response to 𝑞𝑢
𝑡itself.
C.2 Hyper-parameter Setting
We provide the hyper-parameter settings in Table 6. The notations
on the left side indicate the learning rate, the number of GNN
layers, the number of knowledge memory dimensions, the graph
construction threshold, and the value of 𝑙2normalization.
C.3 Experimental Results Using labeled Graph
Relations
The Junyi dataset includes KC similarity and prerequisite relations
annotated by experts with confidence scores ranging from 1 to
9. We select relations with average scores higher than 5 as graphTable 7: Comparison of GRKT applied to the Junyi dataset
with labeled KC relations (GRKT-L), statistics-based relations
(GRKT-S), and no relations (GRKT-0). The two values in the
“sparsity” column respectively denote the constructed KC
similarity and prerequisite graphs’ sparsity.
Model Sparsity AUC ACC CONS GAUC RPT
GRKT-S 0.171, 0.169 0.8207 0.8624 1.0000 0.6473 0.8957
GRKT-L 0.006, 0.003 0.8108 0.8562 1.0000 0.6423 0.8861
GRKT-0 0.000, 0.000 0.7921 0.8481 1.0000 0.6084 0.8781
1.000
1.0000.525
0.3050.245
0.2210.055
0.140 0.008
0.072
0.001
0.025
0.000
0.000
1.000
1.0000.574
0.3290.296
0.2370.076
0.1520.017
0.085
0.003
0.038
0.000
0.000
Figure 5: Experimental results analyzing the effects of hyper-
parameters in GRKT are presented. The green and red deci-
mals on the right side respectively indicate the sparsity of
the constructed KC similarity and prerequisite graphs based
on the specified threshold.
edges. Table 7 presents the experimental results of GRKT leveraging
expert-labeled relations compared with statistics-based relations
and no relations. As shown, the graphs established on expert anno-
tations are too sparse, with only an average of 1-2 related KCs for
one KC, which may not reflect real scenarios. Despite the experi-
mental results based on expert-labeled relations being inferior to the
statistics-based version, they still exhibit noticeable improvement
compared to the version without any relations.
C.4 Hyper-parameter Analysis
We conduct experiments to analyze the effects of various hyperpa-
rameters on GRKT’s performance. The experiments are performed
on the two ASSIST datasets, as shown in Figure 5. The results show
that setting the number of layers in the KC relation-based graphs
to 2 achieves the best performance for GRKT, suggesting that re-
trieving information from further distances over the graph can
enhance the model. However, employing more layers may lead to
overfitting issues. For the KC graph construction threshold, the
performance peaks at around 0.6 to 0.8. In this interval, the sparsity
of the two graphs ranges from 0.01 to 0.3, indicating that too many
relations lead to structural redundancy, while too few result in
limited information sharing between KCs.
 
513