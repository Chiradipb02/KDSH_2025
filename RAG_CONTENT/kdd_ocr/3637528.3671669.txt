DIET: Customized Slimming for Incompatible Networks in
Sequential Recommendation
Kairui Fu
Zhejiang University
Hangzhou, China
fukairui.fkr@zju.edu.cnShengyu Zhang∗
Zhejiang University
Hangzhou, China
Shanghai Institute for Advanced
Study of Zhejiang University
Shanghai, China
sy_zhang@zju.edu.cnZheqi Lv
Zhejiang University
Hangzhou, China
zheqilv@zju.edu.cn
Jingyuan Chen
Zhejiang University
Hangzhou, China
jingyuanchen@zju.edu.cnJiwei Li
Zhejiang University
Hangzhou, China
jiwei_li@shannonai.com
ABSTRACT
Due to the continuously improving capabilities of mobile edges,
recommender systems start to deploy models on edges to allevi-
ate network congestion caused by frequent mobile requests. Sev-
eral studies have leveraged the proximity of edge-side to real-time
data, fine-tuning them to create edge-specific models. Despite their
significant progress, these methods require substantial on-edge
computational resources and frequent network transfers to keep
the model up to date. The former may disrupt other processes
on the edge to acquire computational resources, while the latter
consumes network bandwidth, leading to a decrease in user satis-
faction. In response to these challenges, we propose a customizeD
slImming framework for incompatiblE neTworks(DIET). DIET de-
ploys the same generic backbone (potentially incompatible for a
specific edge) to all devices. To minimize frequent bandwidth usage
and storage consumption in personalization, DIET tailors specific
subnets for each edge based on its past interactions, learning to
generate slimming subnets(diets) within incompatible networks
for efficient transfer. It also takes the inter-layer relationships into
account, empirically reducing inference time while obtaining more
suitable diets. We further explore the repeated modules within net-
works and propose a more storage-efficient framework, DIETING,
which utilizes a single layer of parameters to represent the entire
network, achieving comparably excellent performance. The exper-
iments across four state-of-the-art datasets and two widely used
models demonstrate the superior accuracy in recommendation and
efficiency in transmission and storage of our framework.
∗The corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671669CCS CONCEPTS
•Information systems →Collaborative and social computing
systems and tools; Recommender systems.
KEYWORDS
Sequential Recommendation; Edge-cloud Collaborative Learning
ACM Reference Format:
Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, and Jiwei Li. 2024.
DIET: Customized Slimming for Incompatible Networks in Sequential Rec-
ommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona,
Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.
3671669
1 INTRODUCTION
With the advent of the era of big data, recommender systems have
become indispensable in various aspects of our daily lives, encom-
passing e-commerce, restaurants, and movies [7,12,24–26,30,32,41–
43,53]. It is well known that each user frequently interacts with
recommender systems, leading to increased latency in adverse net-
work conditions or with large data volumes, due to the necessity
of transmitting data back and forth to the cloud for processing.
To address this issue, edge-cloud collaborative learning[ 33–35,47]
begins to leverage the advancing capabilities of mobile edges to de-
ploy models directly on edges. This approach allows user reranking
requests to be processed locally, thereby reducing response latency.
Furthermore, various edges have the opportunity to possess distinct
models, allowing for differences between cloud and edge models
and accommodating both interest and resource heterogeneity.
Unfortunately, despite significant advancements in mobile edge
capabilities, conducting model training on edges for adaptation
of on-edge data still requires a considerable amount of time, and
limited data on edges can easily lead to overfitting and performance
degradation. In response to this, existing research addresses this
issue without introducing much overhead for mobile edges through
two perspectives. One approach involves splitting the larger cloud-
side model into two parts, deploying the part with a large amount of
computation on cloud and lightweight computation on edges[ 4,14].
 
816
KDD ’24, August 25–29, 2024, Barcelona, Spain Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, & Jiwei Li
Device 1 Cloud Device 2(a) Interest Heterogeneity across Cloud and Devices  
𝒕𝒕𝟏𝟏 𝒕𝒕𝟐𝟐𝒕𝒕𝟑𝟑
Daily Promotion World Cup(b) Frequently Changing User Interests (d) Inference between
Cloud and Devices
(c) Model Storage on Devices𝒕𝒕𝟒𝟒 𝟎𝟎 𝒕𝒕𝟎𝟎Updated models
Interest 
shiftInterest 
shift
Cloud
Device 1 Device 2
…
Each scenario 
corresponds 
to one model
food
 electronic sports
 food
 electronic sports
food
 electronic sports… … …click click
Home page After purchase 
 Advertise…
Same model for 
both devices 
and cloudabundant 
resources
limited 
resources
fast
slowgradientsclickPromotion Daily World Cup
Figure 1: (a): There always exists data distribution shifts across cloud and edges, where cloud holds massive user historical
interaction data and each edge only accesses their own data. (b): In each edge, user’s interest would change frequently due to
some other factors. This would cause cloud to send the latest models to each edge for adaptation, causing massive transmission
delays. (c): In recommender systems, different scenarios need different models to provide services, which makes that user
devices are flooded with models. (d): Due to the difference in computing resources between cloud and edges, although the
model can quickly complete gradient updates and inference on cloud, it still takes a long time on edges.
Another technique primarily focuses on providing each edge with
a personalized model tailored to local data in Figure 1. Most of
them[ 16,27,28,46] distribute gradient computation across edges,
which are then gathered by cloud for aggregation. Despite their
advancements, they may result in significant latency due to fine-
tuning and transmission delays in order to keep in line with dy-
namic interests. Other methods[ 5,11,22,23,31,39] alleviate this
problem by removing redundant parameters to minimize model
size or expedite inference speed. Nevertheless, parameters discov-
ered might be merely redundant and not compatible with some
edges. Remaining incompatible parameters lead to poor results for
heterogeneous edges, making them degrade a lot.
In light of these considerations, we propose to investigate how
to achieve higher performance on different edges under strict re-
source constraints. Building on this objective, we aim to address
three key issues in our investigation: (i) Transmission efficiency.
As we discussed above, current methods would download models
from cloud frequently to fit user’s frequently changing interests in
Figure 1(b). However, ongoing transmission might lead to severe
network congestion and non-negligible transmission delay. Thus,
how to design the representation and transmission of the model
is a matter worth considering. (ii) Storage consumption. On a
superplatform (e.g., Amazon and Taobao), each user has the op-
portunity to access recommender systems with multiple channels,
e.g., home page recommendation, recommendation after purchase ,
short video recommendation. Under these circumstances, as shown
in Figure 1(c), previous methods send substantial models to edges,
resulting in significant storage pressure. Intuitively, models across
different channels are expected to share some similarities, which
might help reduce storage occupancy on edges. (iii) Inference cost.
The inference speed of edge-side models determines the waitingtime after user makes a request. With faster inference speed, rec-
ommender system can handle more requests within the same time
frame. It presents us with an additional obstacle in decreasing the
time required for inference on resource-constrained edges as shown
in Figure 1(d).
Table 1: Brief comparison of DIET and others in edge cloud
collaborative learning.
Base Fine-tuning Compr
ess1DIET
Compact % % ! !
Lo
w Latency % % ! !
Generalizability % ! % !
Fast
Inference % % ! !BenefitsMetho
ds
Towards this end, we propose a lightweight and efficient edge-
cloud collaborative recommendation framework called DIET aim-
ing at customizeD slImming for incompatiblE neTworks. DIET
is dedicated to eliminating incompatible parameters within given
networks between cloud and edges while addressing edge-side con-
straints. Ideally, for distinct edges with various interests, DIET is
supposed to assign customized models(including parameters and
structures) and minimize costs wherever possible to fit their local
interests. Specifically, given each user’s real-time sequence, DIET
learns to generate personalized dietat both the element level and
the filter level. A diet consists of a series of binary masks, each of
which represents whether the parameter at this location is compat-
ible. In technique, we employ sequence extractor and layer-wise
mask generators to extract information from user interactions and
1It is challenging to achieve two advantages simultaneously through compression.
 
817DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
learn both element level and filter level importance in the frozen
network. Element-level importance makes parameters transmitted
between cloud and edges shift from a dense network to lightweight
binary masks, reducing the transmission overhead. Filter-level im-
portance not only ensures the exactness of the consolidated learned
diets but also accelerates the inference speed empirically. In this
situation, for different scenarios, each edge only needs to store one
set of parameters, while the cloud can meet the diverse needs of the
edge side by training different generators. Once user sends his real-
time interactions, cloud will generate the corresponding diet and
send it back at once, which prevents user from fine-tuning locally
and the time for inference on cloud can be negligible. Motivated by
repeated blocks in widely-used recommenders, e.g.CNN[ 40] and
transformer[ 21],DIETING is then proposed to use stacked small
blocks within the network to represent the entire network, making
it more storage-friendly for edges with limited storage. Table 1
provides a brief comparison of DIET and other related methods in
edge-cloud collaborative learning, demonstrating the superiority
ofDIET in edge-cloud collaborative recommendation.
We conduct experiments on four real-world datasets with two
widely used recommenders. Further ablation study and parameter
analyses consistently validate the generalizability and effectiveness
of our approach in edge-cloud collaborative recommendation. We
summarize the main contributions of this work as follows:
•To the best of our knowledge, we are the first to achieve both
structure and parameter personalization under strict edge
constraints in edge-cloud collaborative recommendation.
•We propose to generate edge-specific diets with user’s past
interactions, making it fast adaptation of user interests and
light-weighted for transfer and storage.
•We account for the importance at filter level to correct the
generated diets, which improves the stability of the frame-
work. This approach also empirically reduces the inference
cost of edges.
•We conduct extensive experiments on four real-world bench-
marks, which demonstrate that DIET dramatically achieves
better performance under the constraints of limited compu-
tational resources and transmission latency on edges.
2 RELATED WORK
2.1 Sequential Recommendation
Owing to the potential to capture dynamic user interests, sequen-
tial recommendation has garnered significant attention in both
academia and industry. Initial sequential recommender system
based on Markov chains[ 37] utilizes the probabilistic modeling of
user-item transitions to predict and recommend subsequent items
in a sequence. With the flourishing development of deep learning,
a series of methods have been applied to recommender system
to better capture users’ long and short-term interests, including
RNN-based[ 18,19], transformer-based[ 21,45,49,52,54] and CNN-
based[ 40]. Some researchers employed causal-related methods to
eliminate bias in recommender systems[ 48,50]. Additionally, some
works[ 20,29] leveraged transferable knowledge from large lan-
guage models to enhance the effectiveness of recommender system.2.2 Edge-cloud Collobrative Learning
Recent methods of edge cloud collaborative learning can be catego-
rized into model splitting and model personalization. The former[ 2,
36] divide the model into two parts, which are placed on edges and
cloud, thus preventing heavy end-side burden. EdgeRec[ 14] deploys
the embedding matrix on cloud, while the remaining lightweight
model is deployed on user edges. By applying multiple environmen-
tal constraints. Auto-Split[ 4] automatically segments each model
into cloud-side and edge-side parts, thus avoiding the difficulty of
manual segmentation. The latter[ 9,46] commit to improving the
personality of each edge and their local performance. Detective[ 51]
and [ 6] focus on improving the generalization and robustness of
cloud models by leveraging multiple distributions and multi-domain
data on devices. DCCL[ 46] proposes an edge-cloud collaborative
framework, introducing a meta-patching mechanism that not only
alleviates the computational burden on the edge side but also en-
sures edge-side personalization. DUET [ 34] and IntellectReq [ 33]
construct a comprehensive and effective device-cloud collaborative
system, respectively addressing the issue of how and when models
can be efficiently generalized.
2.3 Network Compression
Proper model pruning can help reduce model complexity and com-
putational requirements, improving inference speed and reducing
resource consumption. Based on the purpose, model pruning can
be categorized into two major types: structured pruning and un-
structured pruning. Structured pruning[ 5,31,39] mainly focuses on
speeding up inference through removing entire neurons, channels,
or layers from the neural network, leading to a more structured
model. Although it improves the model’s inference speed, it’s usu-
ally challenging to reduce the model size without significantly com-
promising the final performance. Unstructured pruning[ 17] aims to
remove individual connections in the neural network, leading to a
more irregular reduction of the model size. After LTH[ 11] confirmed
that there exists a sparse network that can be trained to compara-
ble accuracy in isolation, a variety of methods starts to prune the
models during training[3, 38, 55] or even before training[22, 23].
3 METHOD
The overall framework of our method is shown in Figure 2. In this
section, DIET is divided into three parts for a clearer introduction: (i)
storage saving and efficient transfer in Section 3.2. (ii) edge-specific
diets in Section 3.3. (iii) connections correction and light-weighted
inference in Section 3.4.
3.1 Preliminary
In the settings of edge-cloud collaborative learning in recommender
system, suppose there is an item set 𝐼=[𝑖1,𝑖2,...,𝑖𝑛]and an edge
set𝐷=[𝑑1,𝑑2,...,𝑑𝑚], where𝑛and𝑚are the number of items and
edges, separately. Each edge has limited computation resources and
cannot access data from other edges. In our paper we rigorously ac-
count for model transmission delays and other constraints, aiming
to alleviate the issues mentioned in Section 1. Apart from this, there
exists another cloud server 𝐶with abundant computing resources
to provide various services for each edge. The cloud server has
access to the historical interactions 𝑋𝑖=[𝑥𝑖
1,𝑥𝑖
2,...,𝑥𝑖
𝑇]of each
 
818KDD ’24, August 25–29, 2024, Barcelona, Spain Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, & Jiwei Li
(a) Device-specific Structure (b) Connections Correction
(d) Model Slimming on Incompatible Network(c) Schematic Diagram of DIET between Cloud and D evices
t1 t2 t3 t4
t5
…
…
Transformation
Masks for selection 
one layer
multiple masks
t1
t2
t3
t4
t5
Target Item�𝔂𝔂
×
Mask Function 𝐹𝐹𝑒𝑒…
Item Embedding
Device 2Device 1
……
…
t1 t2 t3 t4
t5
t1 t2 t3 t4t5
…
✗
Generator
𝑳𝑳Sequence Extractors
𝑳𝑳Score Generators…
𝑡𝑡𝑡𝑡𝑡embedding
sequence embedding 
of 𝐿𝐿𝑡𝑡𝑡layerfilter-level importance 
of 𝐿𝐿𝑡𝑡𝑡layer
Hidden vector Frozen Parameterssharing model𝑳𝑳Importance Generators
Figure 2: Overview of DIET. (a): Cloud will generate edge-specific diets(subnets) condition on real-time interactions for various
data distribution on edges. (b): Another module on cloud to discover inter-layer filter relationships in networks, generated
filter-level importance will post-process diets from (a) to correct less important connections. (c): A brief schematic diagram of
DIET and DIETING(the portion marked with orange dashed lines). The edges send their real-time samples and cloud customizes
diets for them. The transmission over the network consists of a series of binary masks, with all edges sharing the same network.
(d): With specific diets from cloud, edges construct their own models, enabling fast adaptation.
edge, where 𝑥𝑖
𝑡∈𝐼for𝑖∈[1,𝑚]and𝑡∈[1,𝑇]. Each edge also has
its own real-time interactions 𝑋𝑖′=[𝑥𝑖′
1,𝑥𝑖′
2,...,𝑥𝑖′
𝑡]. Our task in
edge-cloud collaborative learning is to improve the generalizability
of the cloud model 𝐹and further make it adapt to changing envi-
ronments and resource constraints on edge conditions based on
their real-time samples:
min
𝜃𝑖=𝑚∑︁
𝑖=1𝑗=𝑡∑︁
𝑗=1𝐹(𝑥𝑖
𝑗|𝑥𝑖
1,𝑥𝑖
2,...,𝑥𝑖
𝑗−1;𝜃),
s.t.minC.(1)
HereCrepresents the set of all edge constraints and 𝜃denotes the
parameters of cloud model 𝐹.
For a similar consideration, we adopt the local reranking settings
as in some mainstream works[ 14,33,34] in recommender system.
At the beginning of each session or only when user interest changes
dramatically, cloud will send the newest models along with the can-
didate embeddings to each edge. Those embeddings would be stored
in local cache for subsequent reranking. Under these circumstances,
cloud does not need to participate in edge reranking unless user in-
terests change drastically. As candidate embeddings only occupy a
small amount of space, in this paper we mainly consider the storage
and transmission of network parameters.
3.2 Model Slimming for Incompatible
Parameters
Network bandwidth and storage resources for the edges continue
to be in short supply despite the rapid development of technol-
ogy. The former one makes it more expensive for the edges toacquire the latest model from cloud. The latter one may become
some obstacles where each edge needs to keep multiple models for
various scenarios, thereby adding burden to itself. This situation is
especially obvious in recommendation systems, where the user’s
interests are changing frequently and each user may interact with
recommenders through various ways in some large e-commerce
system. Fortunately, previous work[ 55] has proved the potential
of random-initialized networks, i.e. where there is a sub-network
that can achieve comparable performance through parameter se-
lection as training the entire network. Inspired by this, we propose
to discover whether this strategy can be effective in recommender
system.
Consider that the proposed model 𝐹𝑃with𝐾transformers, each
with parameters 𝑊𝑖=[𝑤0
𝑖,𝑤1
𝑖,...,𝑤𝑝−1
𝑖], where𝑖∈[0,𝐾)and𝑝
is the number of linear layers in 𝑖th transformer. For those trans-
formers, the weights 𝑊𝑖are frozen and the training process can be
converted to find an optim mask 𝑀𝑖=[𝑚0
𝑖,𝑚1
𝑖,...,𝑚𝑝−1
𝑖]for each
linear layer. Then the weight of each linear layer for inference will
be:
𝑊′
𝑖=[𝑤0
𝑖⊙𝑚0
𝑖,𝑤1
𝑖⊙𝑚1
𝑖,...,𝑤𝑝−1
𝑖⊙𝑚𝑝−1
𝑖]. (2)
In pursuit of this, for each linear layer, we assign another param-
eter𝑆𝑖=[𝑠0
𝑖,𝑠1
𝑖,...,𝑠𝑝−1
𝑖]to indicate the importance score of each
element in this layer. Then Equation 2 can be written as:
𝑊′
𝑖=[𝑤0
𝑖⊙𝐹′(𝑠0
𝑖),𝑤1
𝑖⊙𝐹′(𝑠1
𝑖),...,𝑤𝑝−1
𝑖⊙𝐹′(𝑠𝑝−1
𝑖)],(3)
where𝐹′is a specific function used to extract 𝑚0
𝑖from𝑠0
𝑖. Given the
hyperparameter 𝛼, the output of 𝐹′will be 1 if the absolute value
of the element is among the top 𝛼%absolute values in 𝑠0
𝑖, otherwise
 
819DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
the output is 0. To overcome the non-differential problem of 𝐹′, we
choose straight through estimator rather than performing sampling
on continuous variable 𝑆𝑖to prevent extra sampling process:
𝜕𝐿
𝜕𝑠0
𝑖≈𝜕𝐿
𝜕𝑊′
𝑖𝜕𝑊′
𝑖
𝜕𝐹′(𝑠0
𝑖). (4)
Since current networks are mainly composed of stacked blocks,
we begin to conjecture whether the initialization values in the
network are important. In light of this, we propose another variant
of DIET named DIETING, which is more lightweight than DIET.
DIETING proposes that since the network consists of identical
blocks, it might be also feasible to initialize the parameters of each
layer and use one of the layers to initialize the entire network. That
is, for all𝑖∈𝐾and𝑗∈𝑃,
𝑤0
𝑖=𝑊𝑚𝑎𝑥[:𝑠𝑖𝑧𝑒(𝑤0
𝑖)], (5)
where𝑊𝑚𝑎𝑥 contains the most parameters among all layers. Under
these circumstances, all models can be represented with one layer
and a series of masks. This not only reduces the storage overhead
but also the transmission delay because for cloud the transferred
parameters become binary form, reducing the size of transmission.
3.3 Edge-specific Diets to Enhance Personality
Although sending binary masks between cloud and edges reduces
the latency required for transmission, fewer meaningful parameters
on edges may reduce the model’s generalizability, as parameters
suitable for cloud may not necessarily be suitable for edges. Moti-
vated by the widely used hypernetwork[ 1,10,15], which generates
the corresponding parameters with learnable vectors, this capabil-
ity can be integrated into our framework to produce personalized
diets for each edge. Formally, the hypernetwork 𝐺get an input 𝑧,
then output the specific weight 𝑤𝐺:
𝑤𝐺=𝐺(𝑧). (6)
Despite the personalized parameters it produces, the hypernet-
work uses a random latent vector 𝑧as its input, which needs to be
retrained for each edge on their local data. However, for resource-
constrained edges, finetuning the model on edges to get distinctive
𝑧is not feasible. We thereby introduce an extra sequence extractor
to learn to generate the latent vector 𝑧from user’s recent behaviors
ourselves. Each edge uploads its recent interactions and cloud can
extract valuable information from them to generate personalized
diets. Then Equation 6 becomes:
𝑤𝐺=𝐺(𝐸(𝑋)), (7)
where𝑋is the real-time interactions of each edge.
3.3.1 Sequence Extractor. Given the length of the interaction as
𝑙and its representation 𝑔∈𝑅𝑙×𝑑, with𝑑being the dimension of
the item embedding, the sequence extractor aims to extract those
underlying information 𝑔′∈𝑅𝑑representing the data distribution
of each edge. Though most of the DNN architectures can be used
as the sequence extractor, in our framework we use GRU[ 8] as the
sequence extractor which offers efficient training, reduced complex-
ity, and comparable performance to LSTMs in capturing long-range
dependencies. In order to prevent mutual interference between
different layers, each layer has its own independent extractor when
calculating its unique mask.3.3.2 HyperNetwork. In this section, we will outline how to utilize
the extracted sequence features 𝑔′and hypernetwork 𝐺to generate
diet for each layer. Take the linear layer as an example, the dimen-
sion of it is 𝑑𝑜𝑢𝑡×𝑑𝑖𝑛, with𝑑𝑜𝑢𝑡and𝑑𝑖𝑛are the dimension of its
output and input, respectively. The hypernetwork can consist of
a fully connected layer or a complex MLP. Similarly, every layer
has its hypernetwork to ensure an adequate level of parameter
personalization.
The generated score will then replace the importance score 𝑆𝑖to
produce the corresponding mask 𝑊′
𝑖as in Equation 3:
𝑊′
𝑖=[𝑤0
𝑖⊙𝐹′(𝐺0
𝑖(𝐸0
𝑖(𝑋))),...,𝑤𝑝−1
𝑖⊙𝐹′(𝐺𝑝−1
𝑖(𝐸𝑝−1
𝑖(𝑋)))],(8)
where𝐺𝑝−1
𝑖and𝐸𝑝−1
𝑖are the hypernetwork and sequence extractor
of the𝑝−1th linear layer in the 𝑖th transfomer.
Despite extra model parameters we include to generate masks,
those modules won’t be sent to each edge but rather kept in cloud.
Hence it won’t add overhead for edges. Once users upload their
real-time samples 𝑋to the cloud, the sequence extractors and hy-
pernetworks of each layer will generates the personalized diets
according to the extracted information of real-time data samples
and send them to the edge. Multiple extractors and hypernetworks
can be executed in parallel during an inference process, thus incur-
ring minimal waiting time overhead.
3.4 Connections Correction for Each Diet
Recall that when generating masks in the previous steps, each
element in the linear layer is independently predicted and only
element-level dependencies are considered. Inspired by previous
work[ 13] which delved into the inter-layer filter relationships and
emphasized the retention of filters containing substantial infor-
mation across adjacent layers, we propose to leverage inter-layer
dependencies to capture more comprehensive information and en-
hance the overall model performance.
Similarly, we take the hypernetwork mentioned in Section 3.3 to
generate inter-layer importance. To simplify the architecture, the
element-level sequence extractor is repurposed for the filter level.
In our view, those extracted common characteristics can be used for
both importance calculations. Formally, given the extracted features
𝑔′, another hypernetwork 𝐺′
𝑖maps it to the row level importance
𝑟𝑖∈𝑅𝑑𝑜𝑢𝑡, which will be expanded and multiply the generated
diets:
𝑤0′
𝑖[𝑎,𝑏]=𝑤0
𝑖[𝑎,𝑏]×𝐹′(𝐺0
𝑖(𝐸0
𝑖(𝑋)[𝑎,𝑏]×𝑟0
𝑖[𝑎]), (9)
where
𝑟0
𝑖=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝐺0′
𝑖(𝐸0
𝑖(𝑋))), (10)
thesoftmax function here is to prevent some outputs from being
negative because the mask generate function 𝐹′will use the abso-
lute value to determine whether to retain an element at a certain
position.
Taking inter-layer dependencies into consideration not only
provides a better diet for edges but also empirically speeds up the
inference on each edge. The results will be described in Section
4.2. Given the varying importance of each row/filter within a layer,
elements in less important filters will receive lower scores. This
prompts the importance of the elements within this filter to be
positioned towards the latter part of the entire layer, ultimately
 
820KDD ’24, August 25–29, 2024, Barcelona, Spain Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, & Jiwei Li
resulting in the entire filter not being selected and therefore not
needing to participate in the inference calculation, thereby reducing
the inference overhead on edges.
Table 2: Statistics of the datasets.
Dataset
#Users #Items #Interactions #SeqLen #Sparsity
ML-1M
6,040 3,012 994,852 164.71 94.52%
ML-100K 943 928 94,672 100.45 89.16%
CD 78,318 57,326 1,955,164 24.96 99.96%
TV 31,482 68,308 867,853 27.56 99.96%
4 EXPERIMENTS
In order to demonstrate the effectiveness and efficiency of our
method, we compare it with other outstanding methods on two
most widely used models and four real-world recommendation
datasets. We choose SASRec andCaser as the architectures they use
are CNN and transformer respectively. All results are the average
of five experiments with five individual random seeds.
4.1 Experimental Setup
4.1.1 Datasets. The experiments are conducted on four state-of-
art benchmarks Movielens-1M1,Movielens-100K2,Amazon-CD and
Amazon-TV3. Detailed statistics of them are shown in Table 2. To
keep data quality, we use 20-core setting for Movielens datasets and
10-core settings for Amazon datasets according to different data
sparsity of each dataset. We treat the interactions with positive
ratings as positive samples. Following previous works, we sort
the user-item interactions in chronological order and treat the last
interacted item of each user as test sample.
4.1.2 Evaluation Metrics. For the edge-cloud collaborative learn-
ing in recommendation, we are supposed to consider recommender
performance, transmission latency, and inference speed simultane-
ously. For the recommender performance, we use Hit and NDCG,
two frequently used metrics in recommender system. For the trans-
mission latency, we evaluate it with the number of bits needed to
transfer when updating. For the inference speed, we use Floating-
point operations (FLOPs) of a single iteration during inference.
Detailed descriptions can be found in Appendix.
4.1.3 Base model & Baselines. To verify the universality of our
method towards both convolutional layer and linear layer, we
choose the following two widely used sequential recommenders:
•SASRec is a transformer-based sequential recommender, it
leverages the attention mechanism to capture user’s interests
and assign weights for each item towards the target item.
•Caser regards each input interaction as picture and makes
use of horizontal and vertical convolutional layers to capture
both point-level and union-level interests of each user.
We compare our method with a variety of compression-based
methods, of which the descriptions are shown as below:
1https://grouplens.org/datasets/movielens/1m
2https://grouplens.org/datasets/movielens/100k
3https://nijianmo.github.io/amazon/index.html•Base simply trains the sequential recommender on cloud
and deloys it on edge without any extra training.
•Random randomly masks weights in each layer then train
the sparse model from scratch.
•LTH[ 11] tends to find a lottery ticket from a randomly initial-
ized network, which can achieve comparable performance
after training for at most the same number of iterations.
•SNIP[ 23] chooses to selectively prune redundant connec-
tions based on their sensitivity even before any training.
•GEM-MINER[ 38] suggests that subnet with better accuracy
at initialization can achieve better accuracy after training.
•SuperMask[ 55] first discover that there exists a subnetwork
from a randomly initialized network that can get similar
performance without any training.
•PEMN[ 3] further validate the potential of random weight
by learning various masks.
•Gater[ 5] aims to get rid of unnecessary calculations and
acquire higher inference speed through structure pruning.
•STTD[ 44] represents each layer with small matrices through
multiplication to reduce parameters.
4.2 Overall Performance
The comparison of the recommendation performance of our method
and other baselines on four datasets is shown in Table 3. From the
table, it is evident that most of the pruning methods perform worse
than the basic recommender without any fine-tuning, indicating
that the lottery ticket found by them does not have a guarantee
of comparable performance with base recommender. Surprisingly,
those data-dependent pruning methods sometimes perform slightly
worse than random pruning. This could be attributed to the rapid
changes in user interests within recommender systems, leading
to a distribution gap between real-time data, rendering the sparse
network learned through training data ineffective.
In a nutshell, our approach consistently outperforms the base
recommender and other baselines across two models, four datasets,
and two evaluation metrics. Although it does not exhibit significant
improvement over Caser on Movielens-1M, it still yields the best
performance among all the other baselines, which drops consider-
ably compared to the original recommender. Moreover, DIET im-
proves the base recommender by a large margin on various datasets.
For example, on SASRec-based Movielens-100K, we achieve almost
20%improvement over the base recommender and on Amazon-CD
dataset, we improve the two models by more than 10%concerning
NDCG and Hit through searching the specific diets.
Simply learning from given networks is far from enough and
it might cause inferior results. Those sparse selection-based meth-
ods, like SuperMask andPEMN, become the two worst-performing
methods. The heterogeneity of interests poses more challenges to
those methods, making them less likely to find the proper mask.
In contrast, DIET confronts the heterogeneity of user behaviors
by generating corrected personalized diet for each edge given its
recent interactions.
We also present the number of parameters to be transferred and
FLOPs for inference relative to the base recommender. According
to the result, we can find that all of the pruning methods reduce the
model size a lot, among which those sparse selection-based methods
 
821DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 3: Overall Performance on recommendation and resources. We use bold font to denote the best model and underline the
next best-performing model. ↑and↓denote that larger and smaller metrics lead to better performance, respectively.
Mo
del Metho
dDataset
Mo
vieLens-1M Mo
vieLens-100k Amazon-CD Amazon-
TV
NDCG↑ Hit↑ Param↓FLOPs↓NDCG↑ Hit↑ Param↓FLOPs↓NDCG↑ Hit↑ Param↓FLOPs↓NDCG↑ Hit↑ Param↓FLOPs↓
SASRe
cBase 0.0974 0.1849 1.3107 0.2086 0.0517 0.1077 1.3107 0.2086 0.0386 0.0529 1.3107 0.2086 0.0665 0.0835 1.3107 0.2086
Random 0.0926 0.1836 0.7859 0.2086 0.0569 0.1265 0.2618 0.2086 0.0409 0.0557 0.2618 0.2082 0.0694 0.0871 0.2618 0.2086
LTH 0.0962 0.1852 0.7859 0.2086 0.0564 0.1222 0.2618 0.2086 0.0408 0.0564 0.2618 0.2086 0.0703 0.0891 0.2618 0.2082
SNIP 0.0932 0.1835 0.7860 0.2073 0.0582 0.1266 0.2620 0.1999 0.0405 0.0545 0.2620 0.1439 0.0699 0.0875 0.2618 0.1375
Rare
Gem 0.0949 0.1868 0.7866 0.2086 0.0578 0.1243 0.2624 0.2044 0.0421 0.0582 0.2624 0.1961 0.0696 0.0883 0.2624 0.1919
Supermask 0.0723 0.1505 0.0410 0.2086 0.0518 0.1147 0.0410 0.1135 0.0301 0.0398 0.0410 0.1580 0.0574 0.0701 0.0410 0.1532
PEMN 0.0859 0.1769 0.0410 0.2078 0.0538 0.1239 0.0410 0.1743 0.0408 0.0554 0.0410 0.1769 0.0687 0.0863 0.0410 0.1916
Gater 0.0963 0.1886 1.4459 0.1167 0.0555 0.1177 0.8110 0.0671 0.0412 0.0568 1.2329 0.1197 0.0701 0.0888 0.9544 0.0783
STTD 0.0863 0.1714 0.0461 0.3110 0.0582 0.1283 0.0461 0.3110 0.0385 0.0503 0.0461 0.3110 0.0692 0.0858 0.0461 0.3110
DIET 0.1008 0.1929 0.0410 0.1022 0.0635 0.1319 0.0410 0.0416 0.0425 0.0590 0.0410 0.1154 0.0707 0.0896 0.0410 0.0764
Improv 3.49% 4.33%×31.97×2.04 22.82% 22.47%×31.97×5.01 10.96% 11.53%×31.97×1.81 6.32% 7.31%×31.97×2.73
CaserBase 0.0984 0.1820 0.4922 0.0586 0.0518 0.1065 0.4922 0.0586 0.0310 0.0424 0.4922 0.0586 0.0569 0.0719 0.4922 0.0586
Random 0.0826 0.1565 0.0983 0.0580 0.0483 0.1001 0.0983 0.0580 0.0251 0.0363 0.1967 0.0580 0.0468 0.0606 0.0983 0.0580
LTH 0.0959 0.1759 0.0986 0.0580 0.0477 0.0973 0.0986 0.0570 0.0322 0.0424 0.1971 0.0583 0.0586 0.0724 0.0519 0.0986
SNIP 0.0904 0.1712 0.0986 0.0577 0.0467 0.0968 0.0986 0.0577 0.0298 0.0405 0.1971 0.0583 0.0497 0.0634 0.0986 0.0572
Rare
Gem 0.0953 0.1757 0.0986 0.0577 0.0509 0.1067 0.0986 0.5523 0.0308 0.0426 0.1970 0.0580 0.0537 0.0676 0.0986 0.0561
Supermask 0.0797 0.1572 0.0154 0.0586 0.0360 0.0797 0.0154 0.0578 0.0213 0.0318 0.0154 0.0586 0.0403 0.0522 0.0154 0.0576
PEMN 0.0781 0.1495 0.0154 0.0364 0.0371 0.0812 0.0154 0.0552 0.0213 0.0316 0.0154 0.0544 0.0358 0.0487 0.0154 0.0577
Gater 0.0837 0.1573 0.3891 0.0298 0.0541 0.1060 0.6929 0.0456 0.0257 0.0358 0.3400 0.0343 0.0537 0.0676 0.5983 0.0496
STTD 0.0989 0.1826 0.0231 0.0996 0.0566 0.1103 0.0231 0.0996 0.0270 0.0360 0.0231 0.0996 0.0555 0.0697 0.0231 0.0996
DIET 0.0987 0.1821 0.0154 0.0280 0.0572 0.1143 0.0154 0.0333 0.0356 0.0488 0.0154 0.0294 0.0617 0.0771 0.0154 0.0488
Improv 0.31% 0.06%×31.96×2.09 10.42% 7.32%×31.96×1.76 14.84% 15.09%×31.96×1.99 8.44% 7.23%×31.96×1.20
get more significant effects, reducing to 3%of the origin model size.
Despite the substantial achievement in model compression, these
methods do not improve the inference time, as the model still takes
about the same amount of time for inference. While Gater largely
reduces the inference time, it needs to transfer a large number of
parameters and perform poorly on some datasets. Although STTD
achieves better results on Movielens-1M of Caser, it degrades a lot
compared to even the base model on other datasets. Moreover, due
to the additional computation required for the semi-tensor prod-
uct’s matrix multiplication, STTD requires more computational
resources for each inference. In comparison, DIET shows consistent
improvement on both transmission and inference, especially achiev-
ing the best results over Caser on Amazon-CD. All the above results
demonstrate the capability and efficiency of DIET in edge-cloud
collaborative learning.
Table 4: Ablation Study
Dataset MetricMetho
d
SASRe
c-based Caser-base
d
Base +mask +MG DIET Base +mask +MG DIET
Mo
vielens-1MNDCG@10 0.0974 0.0859 0.0990 0.1009 0.0984 0.0781 0.0930 0.0987
Hit@10 0.1849 0.1769 0.1898 0.1929 0.1820 0.1495 0.1744 0.1821
Params 1.3107 0.0410 0.0410 0.0410 0.4922 0.0154 0.0154 0.0154
FLOPs 0.2086 0.2078 0.2078 0.1022 0.0586 0.0364 0.0364 0.0280
Mo
vielens-100KNDCG@10 0.0517 0.0538 0.0631 0.0635 0.0518 0.0371 0.0546 0.0572
Hit@10 0.1077 0.1239 0.1313 0.1319 0.1064 0.0812 0.1080 0.1143
Params 1.3107 0.0410 0.0410 0.0410 0.4922 0.0154 0.0154 0.0154
FLOPs 0.2086 0.1743 0.1743 0.0416 0.0586 0.0552 0.0552 0.0333
Amazon-CDNDCG@10 0.0386 0.0408 0.0409 0.0425 0.0310 0.0213 0.0357 0.0356
Hit@10 0.0529 0.0554 0.0553 0.0590 0.0424 0.0316 0.0485 0.0488
Params 1.3107 0.0410 0.0410 0.0410 0.4922 0.0154 0.0154 0.0154
FLOPs 0.2086 0.1769 0.1769 0.0815 0.0586 0.0544 0.0544 0.0294
4.3 Ablation Study
After having a full comparison between our method and others,
we would like to learn more about its details and check whether
each part of this design has played its intended role. To achieve this,
we incrementally incorporate various components into DIET and
analyze the impact of the resulting models individually. The ablated
models are presented below, and results are depicted in Table 4:•Base are the two origin sequential recommenders SASRec
and Caser. Both of them exhibit similar performance on
Movielens-1M and Movielens-100K. However, we notice that
on Amazon-CD SASRec gains more NDCG and Hit improve-
ments against Caser. This owns to the more parameters of
SASRec to capture high-order user interests.
•+mask neither use hypernetwork to generate personalized
mask for each user nor correct the mask. This means that we
will generate the same mask for all users. From the table we
can observe a clear performance drop of +mask over Caser
on all three datasets even if it yields better performance over
Movielens-100K and Amazon-CD. We attribute this to the
complex and ever-changing user interests in recommender
system as mentioned in Section 4.2. Not surprisingly, by em-
ploying lightweight diets, the number of parameters requir-
ing transfer is significantly reduced, resulting in a substantial
decrease in communication costs.
•+MG generate personalized masks based on the past few
interactions but it does not consider filter-level importance
and treats each connection as independent. This model trans-
mits the same number of parameters as +mask due to the
same sparse selection strategy. What’s more, we observe a
huge improvement on recommendation performance over
almost all datasets. This underscores the effectiveness of
the personalized mask generation method based on user in-
terests. Nevertheless, introducing hypernetwork does not
change the number of FLOPs during inference and it obtains
similar results and poorer results to +mask andBase models
individually.
•DIET aims to rectify the generated mask by taking the filter-
level importance into consideration, thereby creating interde-
pendencies among elements. Compared to the +mask model,
DIET superior results on all metrics. First, owning to the
corrected mask, the recommendation performance improves
 
822KDD ’24, August 25–29, 2024, Barcelona, Spain Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, & Jiwei Li
greatly and becomes the best among all the settings. Addi-
tionally, by reducing the weights of less important filters,
more filters contain elements that are all zero, leading to a
substantial reduction in FLOPs and accelerated inference on
edges. The number of Floating Point Operations has dropped
by at least 50%from the original basis. These findings once
again confirm the effectiveness of our framework in edge-
cloud recommendation.
4.4 Influence of DIETING
In this section, we aim to illustrate within our framework that the
connections learned during the training process take precedence
over the initial values. Through experiments conducted on the
aforementioned datasets, the results are presented in Table 5. Sur-
prisingly, DIETING, which initializes the model with just one layer,
achieves comparable performance to DIET. This further confirms
that the initial parameters of the model have minimal impact on the
final results. Instead, the crucial factor lies in the customization of
diets for each edge. The composition of elements within the neural
network plays a pivotal role in determining the ultimate outcome.
With DIETING, the parameters stored on edges can consist of only
one layer, regardless of the scenario.
Table 5: Overall comparison between DIETING and DIET
Dataset Metho
dMetric
SASRe
c-based Caser-base
d
NDCG Hit Params FLOPs NDCG Hit Params FLOPs
Mo
vieLens-1MDIET 0.1008 0.1929 0.0410 0.1022 0.0987 0.1821 0.0154 0.0280
DIETING 0.1005 0.1919 0.0410 0.0726 0.0974 0.1791 0.0154 0.0273
Mo
vieLens-100KDIET 0.0635 0.1319 0.0410 0.0416 0.0572 0.1143 0.0154 0.0333
DIETING 0.0638 0.1290 0.0410 0.0997 0.0562 0.1090 0.0154 0.0314
Amazon-CDDIET 0.0425 0.0590 0.0410 0.1154 0.0356 0.0488 0.0154 0.0355
DIETING 0.0424 0.0589 0.0410 0.0780 0.0358 0.0489 0.0154 0.0294
Amazon-
TVDIET 0.0707 0.0896 0.0410 0.0764 0.0617 0.0771 0.0154 0.0488
DIETING 0.0705 0.0891 0.0410 0.0764 0.0620 0.0776 0.0154 0.0438
4.5 In-depth Analysis
4.5.1 Detail performance analysis. To further elucidate the effec-
tiveness of our method, we plot the NDCG and Hit when training
in Figure 3. It is evident that the personalized-based method outper-
forms mask, indicating that personalization brings a huge perfor-
mance improvement. The results also indicate that consistent diets
for all users might lead to prediction oscillation, which is particu-
larly severe on Caser. However, this figure also shows that simply
assigning personalized diets might degrade the performance, such
as Caser. In addition, our method with weight correction addresses
this problem with better results than base model.
4.5.2 Analysis on the sparsity ratio. As the model on edges is frozen,
the quantity of 0s and 1s in the mask sent from the cloud side de-
termines the performance of the final model. Towards this end, we
tune the sparsity of each model to gain insights into the potential
of these subnets in incompatible networks. Denote 𝛼as the ratio of
the number of elements equal to 0 in each layer to all elements. The
range of𝛼is[0.6,0.7,0.8,0.9,0.95], and we plot the corresponding
change of NDCG and Hit on Movielens-1M in Figure 4. On the one
hand, when the value of 𝛼is small, then most of the mask is 1s
and the mask learned is less effective. On the other hand, when
𝛼approaches 1, only a small fraction of elements can be selected,Figure 3: The variation on the test set during training.
leading to a loss of model information, thereby decreasing the per-
formance. From the figure we can clearly observe the performance
of the two models on Movielens-1M first continues to increase as
𝛼grows, reaching a certain point after which it starts to decrease,
for reasons as discussed earlier. Moreover, the results presented
above confirm that properly finding specific 𝛼for each model is
significant to balance the inference speed and the performance as
more 0s lead to faster inference speed.
Figure 4: Sensitivity of each model towards 𝛼.
4.5.3 Analysis on the correction part. In Section 4.3, we have demon-
strated that the weight correction reduces the number of floating-
point operations and accelerates the inference speed of each edge.
We deduce that this is due to the presence of more filters that consist
entirely of zeros, which do not need to be involved in the inference
process. As shown in figure 5, even with a small fraction of 1s in
the mask, there are still a few filters that do not need to participate
in calculation, especially in SASRec. This situation changes a little
within Caser due to the small size of each filter in the convolutional
layer. It suggests that reweighting each filter in each layer reduces
the number of floating-point operations as assigning lower weights
to less important filters reduces the scores of these elements in the
final ranking, prompting the elements in these filters to become 0s.
These results empirically verified the effectiveness of connections
correction on both recommendation and rapid inference.
4.5.4 Analysis on the generalizability. Table 6 shows the perfor-
mance of our method on two synthetic datasets in order to prove
 
823DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
Figure 5: Emperical study of the influence caused by con-
nection correlation. We calculate the percent of non-zero
rows/filters in each layer before and after correction.
Table 6: Performace comparison on two synthetic OOD
datasets(Amazon-CD and Amazon-TV).
Mo
del Metho
dDataset
Amazon-CD Amazon-
TV
NDCG@10 Hit@10 NDCG@10 Hit@10
SASRe
cbase 0.0360 0.0481 0.0747 0.0935
+mask 0.0390 0.0523 0.0722 0.0905
+MG 0.0379 0.0503 0.0744 0.0930
DIET 0.0416 0.0577 0.0755 0.0946
Caserbase 0.0309 0.0421 0.0585 0.0755
+mask 0.0114 0.0191 0.0304 0.0447
+MG 0.0343 0.0461 0.0656 0.0820
DIET 0.0344 0.0461 0.0669 0.0831
the robustness of our score generator for those unseen user behav-
ior sequences. Random 80%users are used for training and the rest
are for testing. We choose to use those Amazon datasets because
of their large sparsity, this ensures that the sequence distribution
of the training data and the testing data are inconsistent and helps
justify whether models can learn accurate subnets from different
user representations. From the table, we have the following find-
ings: first, without personalized subnets for each edge, training
a consistent model for all edges mostly yields the worst perfor-
mance, especially for Caser on both datasets, where both metrics
decreased by more than half. Second, those personalized models are
consistently superior to it, demonstrating the high generalization
capability of the hypernetwork condition on each user sequence.
4.6 Case Study
We present a case study in Figure 6 to illustrate the concept of
compatibility as described in our paper. To demonstrate this, we
extract two interactions from the MovieLens-1M dataset. The action
denoted as G(the green one) is utilized to create compatible masks,while the action denoted as Y(the yellow one) is used to generate
incompatible masks. Our findings reveal distinct preferences: G
favors genres such as Animation, Children’s, and Musical, whereas
Yleans towards Horror, Crime, and Mystery.
…
…Base Model
DIETInput User interaction
Generate incompatible masksCompatible masks
Incompatible masksMask generator
t1 t2 t3 t4t5
t1 t2 t3 t4t5
Base model
DIET
Incompatible 
masks 
Next- clicked Item
Animation, Children’s, Musical
Children’s, Drama, Comedy
Drama, Musical, RomanceHorror, Crime, 
MysteryAnimation, Children's, 
Musical𝑮𝑮
𝒀𝒀
Figure 6: Case study for illustration of the compatible net-
works.
Initially, we input Ginto SASRec, and observe that the top-3 rec-
ommended items do not include the next-clicked movies, but rather
relevant movies. However, when employing compatible masks de-
rived from Gusing DIET, the results significantly improve, with
the next-clicked item and more relevant movies appearing in the
recommended list. Conversely, when utilizing Yto generate masks,
which exhibits disparate preferences compared to G, it recommends
few relevant movies. Better performance compared to the trained
SASRec and inicompatible network highlights the superiority of
DIET, underscoring that identifying compatible parameters can not
only reduce resource costs but also yield superior results.
5 CONCLUSION
In this work, we investigate the problem of edge-cloud collabora-
tive recommendation under communication costs and mobile edge
resource constraints. In response to potential constraints during col-
laborative training, we have categorized them into three challenges
and substantiated the necessity of addressing them. We propose
an efficient framework named DIET, which takes both element
and filter/row level importance into consideration and searches the
most suitable diets for each edge, addressing the aforementioned
challenges successfully. Based on it, we propose DIETING to uti-
lize only one layer of parameters to represent the entire model but
get comparable performance, thereby more storage-friendly. Fur-
ther experiments on real-world datasets and widely used models,
accompanied by insightful analyses, once again demonstrate the
effectiveness of our framework.
ACKNOWLEDGEMENT
This work was supported by the National Science and Technology
Major Project (2022ZD0119100), National Natural Science Foun-
dation of China (62441605, 62376243, 62037001, U20A20387), Key Re-
search and Development Program of Zhejiang Province(2024C03270),
the StarryNight Science Fund of Zhejiang University Shanghai In-
stitute for Advanced Study (SN-ZJU-SIAS-0010), Scientific Research
Fund of Zhejiang Provincial Education Department (Y202353679).
 
824KDD ’24, August 25–29, 2024, Barcelona, Spain Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, & Jiwei Li
REFERENCES
[1]Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano. 2022.
Hyperstyle: Stylegan inversion with hypernetworks for real image editing. In
Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition .
18511–18521.
[2]Alia Asheralieva, Dusit Niyato, and Zehui Xiong. 2021. Auction-and-learning
based lagrange coded computing model for privacy-preserving, secure, and re-
silient mobile edge computing. IEEE Transactions on Mobile Computing (2021).
[3]Yue Bai, Huan Wang, Xu Ma, Yitian Zhang, Zhiqiang Tao, and Yun Fu. 2022.
Parameter-Efficient Masking Networks. Advances in Neural Information Process-
ing Systems 35 (2022), 10217–10229.
[4]Amin Banitalebi-Dehkordi, Naveen Vedula, Jian Pei, Fei Xia, Lanjun Wang, and
Yong Zhang. 2021. Auto-split: A general framework of collaborative edge-cloud
AI. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining.
[5]Zhourong Chen, Yang Li, Samy Bengio, and Si Si. 2019. You look twice: Gaternet
for dynamic filter selection in cnns. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 9172–9180.
[6]Zhengyu Chen, Teng Xiao, Kun Kuang, Zheqi Lv, Min Zhang, Jinluan Yang,
Chengqiang Lu, Hongxia Yang, and Fei Wu. 2024. Learning to Reweight for
Generalizable Graph Neural Network. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 38. 8320–8328.
[7]Zhengyu Chen, Ziqing Xu, and Donglin Wang. 2021. Deep transfer tensor decom-
position with orthogonal constraint for recommender systems. In Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 35. 4010–4018.
[8]Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).
[9]Chuntao Ding, Ao Zhou, Yunxin Liu, Rong N Chang, Ching-Hsien Hsu, and
Shangguang Wang. 2020. A cloud-edge collaboration framework for cognitive
service. IEEE Transactions on Cloud Computing 10, 3 (2020), 1489–1499.
[10] Tan M Dinh, Anh Tuan Tran, Rang Nguyen, and Binh-Son Hua. 2022. Hyper-
inverter: Improving stylegan inversion via hypernetwork. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 11389–11398.
[11] Jonathan Frankle and Michael Carbin. 2018. The Lottery Ticket Hypothesis: Find-
ing Sparse, Trainable Neural Networks. In International Conference on Learning
Representations.
[12] Kairui Fu, Qiaowei Miao, Shengyu Zhang, Kun Kuang, and Fei Wu. 2023. End-to-
End Optimization of Quantization-Based Structure Learning and Interventional
Next-Item Recommendation. In CAAI International Conference on Artificial Intel-
ligence. 415–429.
[13] Madan Ravi Ganesh, Jason J Corso, and Salimeh Yasaei Sekeh. 2021. Mint: Deep
network compression via mutual information-based neuron trimming. In 2020
25th International Conference on Pattern Recognition (ICPR). IEEE, 8251–8258.
[14] Yu Gong, Ziwen Jiang, Yufei Feng, Binbin Hu, Kaiqi Zhao, Qingwen Liu, and
Wenwu Ou. 2020. EdgeRec: recommender system on edge in Mobile Taobao. In
Proceedings of the 29th ACM International Conference on Information & Knowledge
Management. 2477–2484.
[15] David Ha, Andrew M. Dai, and Quoc V. Le. 2017. HyperNetworks. In International
Conference on Learning Representations.
[16] Jialiang Han, Yun Ma, Qiaozhu Mei, and Xuanzhe Liu. 2021. Deeprec: On-device
deep learning for privacy-preserving sequential recommendation in mobile com-
merce. In Proceedings of the Web Conference 2021. 900–911.
[17] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman coding.
arXiv preprint arXiv:1510.00149 (2015).
[18] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2015. Session-based recommendations with recurrent neural networks. arXiv
preprint arXiv:1511.06939 (2015).
[19] Balázs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and Domonkos
Tikk. 2016. Parallel recurrent neural network architectures for feature-rich
session-based recommendations. In Proceedings of the 10th ACM conference on
recommender systems. 241–248.
[20] Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023. Learning
vector-quantized item representation for transferable sequential recommenders.
InProceedings of the ACM Web Conference 2023. 1162–1171.
[21] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-
mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE,
197–206.
[22] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer,
and Amir Gholami. 2022. A fast post-training pruning framework for transform-
ers.Advances in Neural Information Processing Systems 35 (2022), 24101–24116.
[23] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. 2018. Snip:
Single-shot network pruning based on connection sensitivity. arXiv preprint
arXiv:1810.02340 (2018).
[24] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. 2023.
Propensity matters: Measuring and enhancing balancing for recommendation.
InInternational Conference on Machine Learning. PMLR, 20182–20194.[25] Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, and Xiao-
Hua Zhou. 2024. Debiased Recommendation with Noisy Feedback. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
[26] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian
McAuley. 2023. Text Is All You Need: Learning Language Representations for
Sequential Recommendation. arXiv preprint arXiv:2305.13731 (2023).
[27] Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Dongxiao Yu, Jun Ma,
Maarten de Rijke, and Xiuzhen Cheng. 2020. Meta matrix factorization for
federated rating predictions. In Proceedings of the 43rd International ACM SIGIR
Conference on Research and Development in Information Retrieval. 981–990.
[28] Zhaohao Lin, Weike Pan, and Zhong Ming. 2021. FR-FMSS: Federated recom-
mendation via fake marks and secret sharing. In Proceedings of the 15th ACM
Conference on Recommender Systems. 668–673.
[29] Peng Liu, Lemei Zhang, and Jon Atle Gulla. 2023. Pre-train, prompt and recom-
mendation: A comprehensive survey of language modelling paradigm adaptations
in recommender systems. arXiv preprint arXiv:2302.03735 (2023).
[30] Shuchang Liu, Qingpeng Cai, Zhankui He, Bowen Sun, Julian McAuley, Dong
Zheng, Peng Jiang, and Kun Gai. 2023. Generative flow network for listwise rec-
ommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 1524–1534.
[31] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting
Cheng, and Jian Sun. 2019. Metapruning: Meta learning for automatic neural
network channel pruning. In Proceedings of the IEEE/CVF international conference
on computer vision. 3296–3305.
[32] Zheqi Lv, Feng Wang, Shengyu Zhang, Wenqiao Zhang, Kun Kuang, and Fei
Wu. 2023. Parameters Efficient Fine-Tuning for Long-Tailed Sequential Recom-
mendation. In CAAI International Conference on Artificial Intelligence. Springer,
442–459.
[33] Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, and Kun Kuang. 2024.
Intelligent model update strategy for sequential recommendation. In Proceedings
of the ACM on Web Conference 2024. 3117–3128.
[34] Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei
Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, et al .2023. DUET:
A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework
for Efficient Device Model Generalization. In Proceedings of the ACM Web Confer-
ence 2023. 3077–3085.
[35] Jed Mills, Jia Hu, and Geyong Min. 2021. Multi-task federated learning for
personalised deep neural networks in edge computing. IEEE Transactions on
Parallel and Distributed Systems 33, 3 (2021), 630–641.
[36] Xufeng Qian, Yue Xu, Fuyu Lv, Shengyu Zhang, Ziwen Jiang, Qingwen Liu, Xiaoyi
Zeng, Tat-Seng Chua, and Fei Wu. 2022. Intelligent request strategy design in
recommender system. In Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 3772–3782.
[37] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-
izing personalized markov chains for next-basket recommendation. In Proceedings
of the 19th international conference on World wide web. 811–820.
[38] Kartik Sreenivasan, Jy-yong Sohn, Liu Yang, Matthew Grinde, Alliot Nagle,
Hongyi Wang, Eric Xing, Kangwook Lee, and Dimitris Papailiopoulos. 2022. Rare
gems: Finding lottery tickets at initialization. Advances in Neural Information
Processing Systems 35 (2022), 14529–14540.
[39] Suraj Srinivas, Akshayvarun Subramanya, and R Venkatesh Babu. 2017. Training
sparse neural networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition workshops. 138–145.
[40] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommenda-
tion via convolutional sequence embedding. In Proceedings of the eleventh ACM
international conference on web search and data mining. 565–573.
[41] Shisong Tang, Qing Li, Xiaoteng Ma, Ci Gao, Dingmin Wang, Yong Jiang, Qian
Ma, Aoyang Zhang, and Hechang Chen. 2022. Knowledge-based temporal fusion
network for interpretable online video popularity prediction. In Proceedings of
the ACM Web Conference 2022. 2879–2887.
[42] Shisong Tang, Qing Li, Dingmin Wang, Ci Gao, Wentao Xiao, Dan Zhao, Yong
Jiang, Qian Ma, and Aoyang Zhang. 2023. Counterfactual Video Recommendation
for Duration Debiasing. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 4894–4903.
[43] Xiaolei Wang, Kun Zhou, Xinyu Tang, Wayne Xin Zhao, Fan Pan, Zhao Cao, and
Ji-Rong Wen. 2023. Improving Conversational Recommendation Systems via
Counterfactual Data Simulation. arXiv preprint arXiv:2306.02842 (2023).
[44] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, and Quoc
Viet Hung Nguyen. 2022. On-device next-item recommendation with self-
supervised knowledge distillation. In Proceedings of the 45th International ACM
SIGIR Conference on Research and Development in Information Retrieval. 546–555.
[45] Yuhao Yang, Chao Huang, Lianghao Xia, Yuxuan Liang, Yanwei Yu, and Chen-
liang Li. 2022. Multi-behavior hypergraph-enhanced transformer for sequential
recommendation. In Proceedings of the 28th ACM SIGKDD conference on knowledge
discovery and data mining. 2263–2274.
[46] Jiangchao Yao, Feng Wang, Kunyang Jia, Bo Han, Jingren Zhou, and Hongxia Yang.
2021. Device-cloud collaborative learning for recommendation. In Proceedings
of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.
 
825DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation KDD ’24, August 25–29, 2024, Barcelona, Spain
3865–3874.
[47] Jiangchao Yao, Shengyu Zhang, Yang Yao, Feng Wang, Jianxin Ma, Jianwei Zhang,
Yunfei Chu, Luo Ji, Kunyang Jia, Tao Shen, et al .2022. Edge-cloud polarization and
collaboration: A comprehensive survey for ai. IEEE Transactions on Knowledge
and Data Engineering 35, 7 (2022), 6866–6886.
[48] Shengyu Zhang, Fuli Feng, Kun Kuang, Wenqiao Zhang, Zhou Zhao, Hongxia
Yang, Tat-Seng Chua, and Fei Wu. 2023. Personalized Latent Structure Learn-
ing for Recommendation. IEEE Transactions on Pattern Analysis and Machine
Intelligence (2023).
[49] Shuai Zhang, Yi Tay, Lina Yao, and Aixin Sun. 2018. Next item recommendation
with self-attention. arXiv preprint arXiv:1808.06414 (2018).
[50] Shengyu Zhang, Dong Yao, Zhou Zhao, Tat-Seng Chua, and Fei Wu. 2021.
Causerec: Counterfactual user sequence synthesis for sequential recommen-
dation. In Proceedings of the 44th International ACM SIGIR Conference on Research
and Development in Information Retrieval. 367–377.
[51] Wenqiao Zhang and Zheqi Lv. 2024. Revisiting the Domain Shift and Sample
Uncertainty in Multi-source Active Domain Transfer. In CVPR. IEEE.
[52] Yipeng Zhang, Xin Wang, Hong Chen, and Wenwu Zhu. 2023. Adaptive disen-
tangled transformer for sequential recommendation. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 3434–3445.
[53] Zeyu Zhang, Heyang Gao, Hao Yang, and Xu Chen. 2023. Hierarchical Invariant
Learning for Domain Generalization Recommendation. In Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 3470–3479.
[54] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining. 1059–1068.
[55] Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. 2019. Deconstructing
lottery tickets: Zeros, signs, and the supermask. Advances in neural information
processing systems 32 (2019).
A EVALUATION METRICS
For the edge-cloud collaborative learning in recommendation, we
are supposed to consider recommender performance, transmission
latency and inference speed simultaneously. For the recommender
performance, we use Hit and NDCG, two frequently used metrics in
recommender system . The first metric assesses whether the target
items appear in the recommender’s provided recommendation list.
This is equivalent to recall when the number of target items is 1,
as in our experiments. The second metric is used to measure the
quality of recommendation lists and the accuracy of their ordering,
it combines the relevance of recommended items with the impact
of their ranking. Detailed calculation process is as follows:
•NDCG takes into account both the relevance of the items
and their positions in the ranked list. It considers that highly
relevant items appearing at the top of the list are more valu-
able to the user:
𝑁𝐷𝐶𝐺 @𝑁=𝐷𝐶𝐺 @𝑁
𝐼𝐷𝐶𝐺 @𝑁, (11)
where the numerator 𝐷𝐶𝐺 @𝑁=Í𝑖=𝑁
𝑖=0𝑟𝑖
𝑙𝑜𝑔2(𝑖+1)and the
denominator 𝐼𝐷𝐶𝐺 @𝑁=Í𝑖=min(𝑁,𝐾)
𝑖=01
𝑙𝑜𝑔2(𝑖+1).𝑁and𝐾
in the above equations are the customized length of items
on top of the recommender list and the length of the user
interaction list for evaluation (in our experiment the last
interaction items). 𝑟𝑖in the first equation denotes whether
item𝑖in the recommender list is in the user interaction list
for evaluation.
•Hit Rate represents whether there exists any item in our
recommendation list:
𝐻𝑖𝑡@𝑁=𝑖=𝑁∑︁
𝑖=0𝑟𝑖>0. (12)
For the transmission latency, we evaluate it with the number of
bits needed to transfer when updating. For the base models likeSASRec andCaser, the transmission cost is calculated as 32×𝑁𝑝,
where𝑁𝑝is the number of parameters in the models. Aligned with
the popular pruning settings, compressed sparse column (CSC) or
compressed sparse row (CSR) formats are used to store the pa-
rameters for most baselines. They are storage formats for sparse
matrices, efficiently designed for column-oriented operations by
storing values, column/row indices, and offsets. Transmission cost
of them is calculated as 32×2×𝛼×𝑁𝑝.𝛼in the equation is the
proportion of non-zero elements in the mask of each layer. For
DIET, DIETING, and those sparse-selection methods, we use binary
format to store the masks, which is 𝑁𝑝. For the inference speed, we
use Floating-point operations(FLOPs) of a single iteration during
inference.
Table 1: Performace comparison between DIET and DIETING
on two synthetic OOD datasets.
Mo
del Metho
dDataset
Amazon-CD Amazon-
TV
NDCG@10 Hit@10 NDCG@10 Hit@10
SASRe
cDIET 0.0416 0.0577 0.0755 0.0946
DIETING 0.0411 0.0573 0.0748 0.0937
CaserDIET 0.0344 0.0461 0.0669 0.0831
DIETING 0.0345 0.0469 0.0665 0.0832
B IMPLEMENTATION DETAILS
Due to the potential inconsistency in performance of different pre-
trained models when searching for recipes, we therefore use random
initialized parameters in our experiments to get rid of the unfair-
ness brought by different pre-trained model. For all the models
mentioned above, we use Adam as the optimizer with a triangular
learning rate scheduler. The learning rate is 0.001 for Movielens-
100K and 0.1 for others. The size item embedding we use in our
experiment is 64. We use Xavier normal initialization for all the
model parameters. The number of horizontal and vertical convo-
lution filters are 4 and 16 in Caser, respectively. For the activation
functions𝜙𝑎and𝜙𝑐, we simply use tanh in our experiments. For
SASRec we use two self-attention blocks and four heads in each
block. The dropout rate is set to zero and the max length of each
input interaction is 5. The hyper-parameter 𝛼across datasets and
models is significantly different. In Caser𝛼is set to 0.1, 0.1, 0.2,
0.1 on Movielens-1M, Movielens-100K, Amazon-CD and Amazon-
Movies, while in SASRec𝛼is set to 0.3, 0.1, 0.1, 0.1 respectively.
As none of SuperMask and Gater have a precise way to control
the model sparsity, we try to modify the initialized value and the
coefficient of the penalty term as much as possible.
C GENERALIZABILITY OF DIETING
In order to further investigate the feasibility of using one layer to ob-
tain all the models, we test it on those synthetic out-of-distribution
datasets, and results are shown in Table 1. From the table, we can
observe that with personalized mask generator and inter-layer
corrections, storing one layer on edges still keeps reliable general-
izability.
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
 
826