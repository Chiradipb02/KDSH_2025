A Survey on Safe Multi-Modal Learning Systems
Tianyi Zhao∗
tzhao566@usc.edu
University of Southern California
Los Angeles, USALiangliang Zhang∗
zhangl41@rpi.edu
Rensselaer Polytechnic Institute
Troy, USA
Yao Ma
may13@rpi.edu
Rensselaer Polytechnic Institute
Troy, USALu Cheng
lucheng@uic.edu
University of Illinois Chicago
Chicago, USA
ABSTRACT
In the rapidly evolving landscape of artificial intelligence, mul-
timodal learning systems (MMLS) have gained traction for their
ability to process and integrate information from diverse modality
inputs. Their expanding use in vital sectors such as healthcare has
made safety assurance a critical concern. However, the absence
of systematic research into their safety is a significant barrier to
progress in this field. To bridge the gap, we present the first tax-
onomy that systematically categorizes and assesses MMLS safety.
This taxonomy is structured around four fundamental pillars that
are critical to ensuring the safety of MMLS: robustness, alignment,
monitoring, and controllability. Leveraging this taxonomy, we re-
view existing methodologies, benchmarks, and the current state
of research, while also pinpointing the principal limitations and
gaps in knowledge. Finally, we discuss unique challenges in MMLS
safety. In illuminating these challenges, we aim to pave the way for
future research, proposing potential directions that could lead to
significant advancements in the safety protocols of MMLS.
CCS CONCEPTS
•Computing methodologies →Artificial intelligence.
KEYWORDS
Multi-modal Learning, AI Safety, Responsible AI
ACM Reference Format:
Tianyi Zhao, Liangliang Zhang, Yao Ma, and Lu Cheng. 2024. A Survey
on Safe Multi-Modal Learning Systems. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3671462
∗Both authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36714621 INTRODUCTION
Multimodal learning has rapidly evolved across various domains
in the past few years. Compared to unimodal learning, it could
more closely mimic human information processing, where multiple
modalities, such as text, image, and audio, are incorporated to
understand and interact with the world [3, 14].
Despite significant advancements in related research areas, de-
ploying multimodal learning systems (MMLS) in real-world sit-
uations, especially in high-stakes environments like healthcare
and autonomous driving, naturally brings safety challenges. These
challenges are notably more intricate and acute than in unimodal
environments [ 26]. Firstly, when implemented in real-world situa-
tions, MMLS are likely to face various types of uncommon events
and risks. These include natural distribution shifts and deliberately
crafted attacks intended to disrupt the systems. The complexity in-
troduced by the multimodal setting may exacerbate these concerns,
as it not only involves mismatches within each modality but also
introduces domain shifts across modalities [ 55]. Moreover, the rich
information spanned by multiple modalities opens doors for cross-
modal attacks by malicious entities, further increasing MMLS vul-
nerability [ 103]. The growing risk of privacy is another significant
concern. The integration of multiple modalities enhances model
efficacy but simultaneously provides attackers with additional infor-
mation like modality alignment details [ 70], exacerbating breaches
of privacy. Furthermore, the increasing scales of MMLS further
aggravate the challenge of pre-trained models inadvertently memo-
rizing sensitive information from their training data [ 58]. MMLS are
also at high risk of losing control and generating harmful responses
such as discriminatory information or misinformation [ 44,72]. Due
to their increasing complexity, especially in large-scale MMLS, mon-
itoring these systems becomes more challenging. This increased
difficulty in oversight makes them more vulnerable to manipula-
tion, resulting in a greater likelihood of performing harmful actions
compared to their unimodal counterparts.
To advance the development of MMLS, it is essential to thor-
oughly consider these unique safety challenges and develop ap-
propriate strategies. This survey serves to review recent advances
in safe MMLS from four primary pillars: robustness, alignment,
monitoring, andcontrollability. For robustness, we explore two
main aspects: the resilience of MMLS against distribution shifts,
and their robustness in the face of adversarial perturbations and
attacks. Regarding alignment, we first examine the issue of the
misalignment phenomenon in MMLS, noting their susceptibility to
6655
KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianyi Zhao, Liangliang Zhang, Yao Ma, and Lu Cheng
MMLS	SafetyRobustnessMonitoringControllabilityAlignmentDistributional	RobustnessAdversarial	RobustnessMisalignmentAligning	MMLSAnomaly	DetectionReliable	OutputExplainabilityPrivacyFairnessUnderstandMMLS’	DRImproveMMLS’	DRMultimodalAdversarialAttackImproveMMLS’	ARMultimodalJailbreakingIssuesAlignmentTechniquesSystematicFailureDetectionMADApplicationMMLSMiscalibrationCalibration&UncertaintyQuantificationMultimodalXAIIssuesPrivacyPreservingMMLSImproveMMLSFairnessAnte-hocMXAIPost-hocMXAI
Figure 1: Taxonomy for Safety of Multimodal Learning Systems.
being jailbroken and producing harmful content. We then proceed
to introduce the current techniques developed for aligning MMLS.
Formonitoring, our focus is twofold. First, we examine anomaly
detection, which is crucial for promptly identifying potential haz-
ards. Second, we investigate the reliability of model outputs to
enhance the trustworthiness of MMLS. Controllability is divided
into three subcategories. The first, interpretability, aims to study
and increase the transparency and understandability of MMLS. The
second, privacy, discusses unique privacy challenges within MMLS,
and various paradigms for preserving privacy in these contexts.
Lastly, fairness addresses inherent bias in these systems to ensure
more equitable and fair responses. We also discuss the limitations
and summarize pertinent benchmarks related to these pillars and
conclude the paper with open challenges.
In summary, our main contributions are Taxonomy: We present
the first taxonomy (Figure 1) for MMLS safety; Review: Following
this new taxonomy, we conduct a comprehensive review of existing
works, highlighting their limitations; and Open Challenges: We
identify some unique challenges related to the safety of MMLS and
suggest potential prospects.
2 ROBUSTNESS
Robustness emerges as a critical research problem when addressing
safety concerns in MMLS. In this section, we delineate the prob-
lem along two dimensions: robustness against distributional shift,
referring to the ability of MMLS to adapt to unexpected natural
variations; and adversarial robustness, denoting their resilience to
deliberately crafted threats.
2.1 Robustness against Distributional Shift
A standard assumption in ML is consistent distribution, but real-
world scenarios often show mismatches between training and test-
ing data. In MMLS settings, these mismatches are more complex,
involving shifts within and across modalities. Understanding and
enhancing MMLS robustness against distribution shifts is crucial
for their safe deployment.
2.1.1 Understanding distributional robustness in MMLS. Most prior
research on robust MMLS focuses on vision-language models (VLMs),which are susceptible to distribution shifts in both image and text
modalities. Yet they generally show greater robustness than uni-
modal models [ 21]. Understanding the reasons behind this robust-
ness gain is vital for advancing MMLS safety. For instance, [ 21]
identifies diverse image training data as the key to CLIP’s robust-
ness, excluding language supervision and contrastive loss function.
However, contrasting views exist [ 95], suggesting these latter fac-
tors also make contributions. Consensus on these findings is yet to
be established. Modality mismatch represents a unique distribution
shift in MMLS, which occurs when the data modalities during test-
ing differ from those in training. Earlier studies primarily explored
the impact of missing modalities on MMLS’s robustness [ 54]. More
recent research starts considering more generalizable settings [ 55],
investigating the effects of both missing and newly added modalities
during test time on MMLS robustness.
2.1.2 Improving MMLS distributional robustness. Current efforts
to improve the distributional robustness of MMLS often involve
data augmentation techniques and robust training strategies.
Data Augmentation. Data augmentation techniques, proven effec-
tive in unimodal systems for enhancing model robustness, are natu-
rally extended to MMLS. However, applying unimodal strategies in-
dependently to each modality risks losing semantic coherence [ 51].
Consequently, multimodal techniques preserving modality con-
sistency while improving robustness have emerged. For example,
RobustMixGen for VLMs uses a pre-synthesis data separation mod-
ule to separate data into objects and backgrounds. It maintains
cross-modal semantics and boosts model robustness by reducing
spurious correlations [38].
Robust Training. Research in robust training strategies is diverse
and spans from improving pre-training strategies to adaptation
and fine-tuning methods. For instance, a multi-task optimization
algorithm has been developed to automatically identify the optimal
data fusion strategies, enhancing models’ robustness against cross-
modal distribution shifts [ 54]. Andonian et al.[ 2] presents a novel
contrastive learning method for pretraining, combining knowledge
distillation and soft alignments to improve robustness. Other stud-
ies include devising robust adaptation methods. For instance, [ 90]
shows that fusing weights from zero-shot and fine-tuned CLIPs
6656A Survey on Safe Multi-Modal Learning Systems KDD ’24, August 25–29, 2024, Barcelona, Spain.
enhances distributional robustness, with further improvements
observed through weight averaging from CLIPs fine-tuned under
different hyperparameters.
2.2 Adversarial Robustness
MMLS robustness to adversarial attacks faces unique challenges.
Malicious users might use the information from multiple modalities
and cross-modal adversarial attacks, such as subtle alterations in an
image affecting text predictions, further complicate the issue [ 103].
2.2.1 Multimodal adversarial attack. Multimodal adversarial attack
aims to trick a target MMLS by subtly altering its multimodal in-
puts. The objective function for multimodal adversarial attacks can
generally be formulated as [84]:
arg max
{𝑥𝑎𝑑𝑣
𝑖}𝑛
𝑖=1L({𝑥𝑎𝑑𝑣
𝑖}𝑛
𝑖=1,𝑦;𝜃)𝑠.𝑡.||𝑥𝑎𝑑𝑣
𝑖−𝑥𝑖||𝑝≤𝜖𝑖, (1)
where{𝑥𝑖}𝑛
𝑖=1denotes the input composed of 𝑛modalities,{𝑥𝑎𝑑𝑣
𝑖}𝑛
𝑖=1
are the generated adversarial samples, and 𝜖𝑖is the perturbation
budget for the 𝑖-th modality. The attacker optimizes Lparameter-
ized by 𝜃to induce model mispredictions within permissible bud-
gets. An effective multimodal adversarial attack needs to properly
address the modality gap between multiple input data modalities.
[103] studies multimodal attacks on VLMs, discovering that launch-
ing unimodal attacks independently often fails. They then pro-
pose Co-Attack, which coordinates image and text attacks through
a sequential strategy Another challenge lies in executing effec-
tive, downstream-agnostic attacks, especially within the prevalent
‘pretrain-then-finetune’ framework for MMLS, as designing specific
attacks for a pre-trained encoder targeting each task is impracti-
cal. Pioneering work by [ 108] proposes a method for generating
broadly applicable adversarial noise, ensuring adversarial samples’
significant deviation from decision boundary in the feature space
via a topology-deviation-based generative adversarial network.
2.2.2 Enhance adversarial robustness of multimodal learning models.
Current works on improving the adversarial robustness of MMLS
center around robust fusion strategy andadversarial training.
Robust Fusion and Alignment. The unique modality alignment pro-
cess of MMLS, compared to unimodal models, has catalyzed ex-
tensive research into enhancing their adversarial robustness. For
instance, [ 97] improves MMLS single-source adversarial robustness
by designing a fusion strategy that detects inconsistencies across
modalities and executes feature fusion to counteract the perturbed
modality. A constrastive learning framework has been proposed
that captures both intra- and inter-modality dynamics, filtering
out noise present in various modalities and thereby yielding more
robust representations [47].
Adversarial Training. Adversarial training enhances robustness ef-
fectively, yet its application in MMLS presents challenges, including
higher computational demands and scalability issues. To approach
these, [ 23] adopts the free adversarial training strategy that obtains
parameters’ gradients with almost no extra computational expense.
Another challenge lies in improving the generalization of adversar-
ial training procedures within the emerging pretrain-then-finetune
paradigm for MMLS. An adversarial pre-training strategy for VLMsis introduced in [ 23], focusing on adversarial training in the embed-
ding space of each modality rather than directly perturbing image
pixels or text tokens, effectively enhancing model performance in
a task-agnostic manner.
2.3 Benchmark
The assessment of model distributional robustness commonly em-
ploys metrics such as Effective Robustness, along with Relative Ro-
bustness and the MultiModal Impact Score to measure average per-
formance drops during distributional shifts [ 21,68]. Systematic
benchmarks are constantly being proposed as well. For instance,
[42] integrates nine VQA datasets to study four generic robustness
in VLMs. Recently, benchmarks for large MMLS have emerged,
such as the one introduced in [ 27]. This benchmark assesses GPT-
4V’s [ 1] zero-shot generalization on 13 varied datasets, offering
insights into its adaptability to distribution shifts.
2.4 Discussion
Despite the ongoing research, a clear understanding of what under-
pins the robustness of MMLS remains under-explored, highlighting
the need for further studies. Current studies tend to concentrate on
VLMs, suggesting the necessity of expanding research focus on a
broader spectrum of MMLS. Besides, MMLS vulnerability to sophis-
ticated real-world attacks underscores the importance of enhancing
their adversarial robustness. This generally involves devising potent
attacks and corresponding defenses, both of which must navigate
the complexities of cross-modal information interplay in MMLS.
Another challenge is ensuring these defenses can be adapted to the
unpredictable nature of real-world attacks.
3 ALIGNMENT
Aligning MMLS with human values is a crucial yet challenging
aspect of safe deployment. Recent studies have highlighted vulner-
abilities in current foundation models, revealing that they can be
manipulated to yield harmful and misaligned outputs. This issue
could be further exacerbated in MMLS than unimodal foundation
models such as LLMs. Therefore, delving into this phenomenon and
developing more safely-aligned MMLS is a critical area of interest.
We discuss this issue from two perspectives: the misalignment in
MMLS and strategies for their alignment.
3.1 Misalignment: Jailbreaking MMLS
The development of foundation models introduces a safety chal-
lenge termed ‘jailbreak’, where malicious users attempt to circum-
vent safeguards and instruct the model to produce harmful re-
sponses that are misaligned with ethical standards. Previous re-
search has mainly focused on unimodal foundation models, par-
ticularly LLMs, with studies on MMLS being relatively new and
concentrating largely on VLMs. One of the most prevalent jailbreak
approaches is adversarial prompting. Compared to LLMs, MMLS
can be more vulnerable to jailbreak attacks owing to the integration
of additional modalities. For instance, [ 8] successfully jailbreaks
VLMs via constructing adversarial image input. Besides, [ 67] de-
signs a simple attack that maximizes the generation probability
of harmful responses, effectively jailbreaking VLMs with a single
visual adversarial example. More recent work [ 61] investigates the
6657KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianyi Zhao, Liangliang Zhang, Yao Ma, and Lu Cheng
potential of constructing adversarial textual input based on image
jailbreak prompt, introducing new approaches to jailbreak MMLS.
3.2 Aligning MMLS
At present, the two most common techniques for achieving model
alignment are Instruction Tuning [ 49], which finetunes models
using collections of pairs of instructions and faithful outputs, and
Reinforcement Learning from Human Feedback (RLHF), which
employs reinforcement learning techniques to optimize model out-
puts [ 63,78]. When aligning MMLS, a significant hurdle is the
scarcity of available data. Multimodal data pairs are more challeng-
ing to gather from the internet compared to textual data alone.
Another issue is the quality of the data collected. For example,
an image paired with a brief description is not as informative as
one accompanied by a comprehensive narrative. LLaVA-RLHF [ 82]
marks a pioneering effort in adapting RLHF from purely textual
applications to vision-language alignment tasks, using human anno-
tated preferences to fine-tune multimodal models via reinforcement
learning. It effectively addresses multimodal misalignment at a rel-
atively low cost for annotations. [ 43] first constructs a high-quality
instruction set based on various multimodal instruction tuning
sources and AI annotations, and then employs direct preference
optimization [ 69] to enhance VLMs’ alignment, resulting in more
faithful systems and mitigating misalignment behaviors.
3.3 Benchmark
Common evaluation metrics include Attack Success Rate, Recognition
Success Rate, andDefense Success Rate [93]. Recently, researchers
have begun to establish systematic benchmarks for measuring
misalignment in MMLS. For instance, [ 44] proposes a multimodal
dataset RTVLM for a comprehensive assessment of VLMs across
various red-teaming cases. [ 61] expands upon AdvBench to con-
struct a multimodal benchmark, AdvBench-M, which consists of
eight distinct categories of harmful behaviors for evaluation.
3.4 Discussion
Aligning multimodal foundation models is notably more complex
because of the involvement of multiple modalities [ 67]. This com-
plexity increases the potential for vulnerabilities and intensifies
the challenge of establishing robust defenses. For instance, incorpo-
rating visual data into LLMs creates opportunities for adversarial
attacks, which can be executed more readily than with purely dis-
crete textual inputs. Consequently, this facilitates the jailbreaking of
MMLS, often leading to the generation of toxic content, misinforma-
tion, and hallucination. Although researchers have devised various
strategies for aligning MMLS, the effectiveness of these methods is
still under scrutiny. Even with attempts at safety alignment, MMLS
can be compromised with relative ease. Thus, the pursuit of align-
ment within MMLS is fraught with difficulties and remains an area
in need of more thorough exploration and understanding.
4 MONITORING-ANOMALY DETECTION
Monitoring is crucial for AI safety. Here, we discuss an important
step for monitoring system behavior: detecting anomalies. Anomaly
detection (AD) refers to the problem of finding patterns in data thatdo not conform to expected behavior. While AD has been exten-
sively discussed in various fields, MMLS introduce extra complexity
owing to heterogeneous modality information, making Multimodal
Anomaly Detection (MAD) more challenging. Our review of MAD
includes MAD assumptions, research techniques, and benchmarks.
4.1 Failure Detection in MAD
A common practice of MAD in MMLS is to detect failure, which
can serve as an early warning system for potential failures by iden-
tifying irregularities. Deployment failures of MMLS are prevalent
despite rigorous testing before development. One of the challenges
lies in anticipating and testing all potential failures in advance. To
bridge this gap, MULTIMON [ 85] is introduced to automatically de-
tect systematic failures in deployed MMLS. It first identifies individ-
ual failures based on CLIP embeddings, which encode multimodal
information, and erroneous agreement, and then leverages language
models to describe patterns of failures. The framework proposed
by [32] aims to detect failure modes within the latent space. It seeks
to discover a hyperplane that optimally distinguishes between cor-
rect and incorrect examples, interprets the patterns of errors, and
then enhances model reliability during training along the axes of
failure. Similarly, DOMINO [ 20] tackles the challenge of identifying
underperforming subsets within MMLS, particularly dealing with
audiovisual inputs. Recent studies [ 17,98] have presented Slice Dis-
covery Methods (SDMs) that use model representations to detect
underperforming segments, however, these methods often lack a
robust framework for quantitative analysis. Thus DOMINO has
been developed as an extensive evaluation framework for SDMs in
various contexts and introduced as an innovative SDM that employs
cross-modal embeddings and an error-aware mixture model for im-
proved performance. It outperforms prior methods, demonstrating
improved coherence and performance in slice discovery.
4.2 Other Applications of MAD
4.2.1 Vision-Language MAD. Vision modality faces sample varia-
tions due to factors like color and light, causing latent space sparsity
and disrupted correlations, which impact task performance. Con-
versely, the more uniform language modality often performs better.
Existing research [ 64] mostly focuses on unimodality, particularly
vision, overlooking multimodal benefits. Meanwhile integrating
multimodalities also introuces challenges to be addressed.
Zero-shot VLMs AD. Motivated by the challenges posed by the long-
tailed distribution and privacy concerns in AD data, zero-shot AD
emerges, offering versatility across various AD tasks. The same has
been observed in MAD. VLMs like CLIP acquire expressive repre-
sentations through extensive training in vision-language annotated
pairs. Based on this, text prompts can be used to extract knowledge
from the models without additional fine-tuning, enabling zero-
/few-shot transfer to downstream tasks. Given image-text pairs
{(𝑥𝑡, 𝑠𝑡)}𝑇
𝑡=1, CLIP trains a text encoder 𝑔and a visual encoder 𝑓to
maximize the correlation between them in terms of cosine similar-
ity<𝑓(𝑥), 𝑔(𝑠)>. Given an input image 𝑥and a set of text 𝑆, CLIP
can predict the probability of 𝑥belonging to class 𝑐as follows:
𝑃(𝑓(𝑥), 𝑔(𝑠𝑖))=exp(<𝑓(𝑥), 𝑔(𝑠𝑖)>/𝜏)Í
𝑠∈𝑆exp(<𝑓(𝑥), 𝑔(𝑠𝑖)>/𝜏), (2)
6658A Survey on Safe Multi-Modal Learning Systems KDD ’24, August 25–29, 2024, Barcelona, Spain.
where 𝜏>0is a hyperparameter. In CLIP, the text prompt template
commonly looks like “ a photo of a [c] ”, where [c]represents
the target class name. It differs from vision tasks that involve many
objects and use objects’ names as [c]. WinCLIP [ 33] employs anom-
aly text prompts and calculates the corresponding scores using Eq.2.
For prompt optimization, a linear combination of the focal loss [ 48]
and dice loss [ 57] is usually used. Subsequent research mainly fol-
lows this approach to compute anomaly scores [11, 16, 107].
Adding high-quality anomaly description. Most existing MAD meth-
ods only provide anomaly scores and require manual thresholds to
differentiate between normal and abnormal samples, limiting wider
use. Incorporating high-quality anomaly descriptions can enhance
understanding and facilitate practical applications. AnomalyGPT
[24] designs an MMLS-based image decoder for anomaly mapping
and employs prompt embedding for fine-tuning. It supports multi-
turn dialogues and exhibits impressive few-shot in-context learning
capabilities. Yet, it underutilizes MMLS’s vision comprehension
potential. Myriad [ 46] addresses this by skillfully incorporating
domain knowledge into embeddings, aligning vision-language fea-
tures, and introducing an Expert Perception module. This module
embeds prior knowledge from vision experts as tokens intelligible
to LLMs, ensuring precise AD with detailed descriptions, including
the anomaly location, type, and explanation of the judgment.
4.2.2 Video Surveillance MAD. Another common multimodal data
is video. Video Surveillance [ 64] plays a crucial role in monitoring
areas for security. It is often treated as an AD problem due to
the substantial amount of unlabeled data. Meanwhile, multimodal
information in video frames is often underutilized, indicating the
need for more effective use. Cutting-edge video MAD methods
involve incorporating multimodal signals. For example, MSAF [ 89]
integrates RGB, optical-flow, and audio modalities, transforming
video-level ground truth into pseudo clip-level labels for subsequent
training, thereby aligning diverse information to enhance learning.
Unlike current works that directly use extracted features for frame-
level classification, VadCLIP [ 92] devises an approach that leverages
the frozen CLIP and multimodal associations between video features
and fine-grained language-image alignment information. Video-
AnomalyCLIP [ 101] further explore the potential of CLIP, effectively
learning text-driven directions for abnormal events by manipulating
the latent feature space and projecting anomalous frames onto these
directions to exhibit large feature magnitudes for a particular class.
4.3 Benchmark
In failure detection, datasets typically consist of normal data with-
out explicitly labeled anomaly subsets. For failure detection in
MAD, datasets vary widely, covering areas from medical imaging
to image-text corpora. Conversely, in AD scenarios, datasets are
specifically annotated to highlight anomalies. When evaluating
Vision-Language MAD, commonly used benchmark datasets in-
clude MVTec-AD [ 4], VisA [ 109], KSDD2 [ 6], MPDD [ 34], MTD
[31], etc. These industrial inspection datasets are annotated at the
pixel level and cover a wide range of object subsets. Datasets for
Video Surveillance MAD include ShanghaiTech [ 50], UCF-Crime
[81] and XD-Violence [ 91], an audiovisual abnormal dataset. Per-
formance evaluation is primarily based on metrics such as the area
under the ROC curve, area under the precision-recall curve, andF1 score. Other metrics include Per-Region Overlap scores, which
assess anomaly segmentation performance, and Average Precision.
4.4 Discussion
Existing MAD works primarily center around CLIP, resulting in
limited exploration of broader multimodal applications [ 35,87,99].
Moreover, current efforts predominantly focus on addressing unsu-
pervised or weakly-supervised MAD challenges, while overlooking
the issue of handling incomplete and inaccurate anomaly labels. No-
tably, limited efforts have been put into studying supervised MAD,
where labeling costs might be high. Additionally, while AD has
been applied to diverse scenarios like financial fraud and medical
systems, the applications of MAD remain limited.
5MONITORING-RELIABLE MODEL OUTPUTS
Ensuring MMLS’s safe deployment requires establishing monitor-
ing mechanisms to assess model reliability. Existing ML methods
face challenges in accurately conveying their understanding and
providing reliable responses. In MMLS, these issues may become
more pronounced given their increased complexity. We discuss
this problem from two perspectives: firstly, the phenomenon of
miscalibration in MMLS, and secondly, the strategies for achieving
calibration and quantifying uncertainty within these systems.
5.1 Miscalibration in MMLS
Miscalibrations in AI systems can manifest as overconfidence, where
models inaccurately assign high probabilities to incorrect predic-
tions, or underconfidence, where models fail to recognize the accu-
racy of their correct predictions with sufficient certainty. Similar to
uni-modal learning, miscalibration is a prevalent issue in MMLS yet
remains underexplored. Despite the involvement of various modali-
ties, MMLS may exhibit a bias towards relying on a single modality
for decision-making, resulting in the issue of over-confidence [ 53].
Recently, few pilot studies have emerged focusing on the foun-
dation MMLS. For example, concerns about miscalibration have
been raised regarding zero-shot VLMs such as CLIP. [ 62] conducts
further studies on fine-tuned VLMs, uncovering that traditional
fine-tuning strategies can significantly affect model calibration. En-
suring MMLS are well-calibrated is vital for their safe use, with re-
search concentrating on model calibration and reliable uncertainty
estimation (specifically when models are not well-calibrated).
5.2 Calibrating and Estimating Uncertainty
A well-calibrated model should make predictions with high un-
certainty when its accuracy is in doubt, and conversely, it should
predict with confidence when it is likely to be accurate. The di-
versity of modalities in MMLS presents both challenges and po-
tential breakthroughs for the development of new approaches for
calibrating MMLS. For instance, a regularization-based technique
CML is proposed for better calibration of MMLS, which calibrates
the relationship between confidence and the number of modali-
ties involved in multimodal learning [ 53]. Recent uncertainty es-
timation research primarily focuses on Bayesian and Conformal
Prediction (CP) methods, with extensive work in unimodal set-
tings (e.g., [ 79,106]) but limited studies in multimodal contexts.
6659KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianyi Zhao, Liangliang Zhang, Yao Ma, and Lu Cheng
Bayesian-based approaches place a distribution over model param-
eters and employ marginalization for predictive distribution. There
are generally two types of predictive uncertainty: aleatoric uncer-
tainty and epistemic uncertainty. [ 104] pioneers the application
of Dempster-Shafer theory [ 74] to model epistemic uncertainty
for each modality and introduces an uncertainty-aware weight-
ing method for dynamic modality fusion. Meanwhile, Subedar et
al. [80] introduce a Bayesian framework for audiovisual applica-
tions, quantifying modality-wise epistemic uncertainty and deep-
ening Bayesian DNNs by combining deterministic and variational
layers. While Bayesian-based approaches are widely used, their
computational intensity renders them resource-heavy. Conversely,
CP-based methods [ 75] have emerged as scalable, efficient, and
model-agnostic alternatives for uncertainty estimation. CP yields
statistically valid prediction sets or intervals from any black box
model, with the sole assumption of data exchangeability. Currently,
the application of CP in MMLS is limited. [ 18] proposes a CP proce-
dure for foundation models using multimodal web data. It generates
plausibility scores based on modality alignment, and then uses them
for Monte Carlo-based CP procedure.
5.3 Benchmark
To measure the reliability of model outputs, a commonly used met-
ric is accuracy vs uncertainty (AvU) [ 40], which considers four types
of predictions: accurate and certain, inaccurate and uncertain, ac-
curate and uncertain, inaccurate and certain. Other measurements
include Expected Calibration Error (ECE) [ 25], Uncertainty Cali-
bration Error (UCE) [ 41], reliability diagrams [ 28], and Bayesian
active learning by disagreement (BALD) [ 29]. Datasets used for
benchmarking can differ widely depending on the task and method.
5.4 Discussion
Advancing MMLS’s reliable output stands as a critical yet presently
inadequately investigated area of research. To improve reliability,
it is necessary to address the prevalent issues of miscalibration
that these systems often exhibit. One of the major challenges in
this endeavor involves tackling the intricate interactions between
different modalities. Although this complexity and variance con-
tribute to miscalibration, it also presents opportunities for effective
calibration and precise uncertainty estimation. Current multimodal
calibration and uncertainty quantification primarily center around
Bayesian-based approaches, which tend to be computationally in-
tensive. The emerging CP-based method offers a scalable and model-
agnostic alternative. However, its application in MMLS is still in its
infancy, highlighting a ripe area for further investigation.
6 CONTROLLABILITY - EXPLAINABILITY
Being able to comprehend and explain the decisions made by the
system is critical for the controllability of MMLS. The complex-
ity of MMLS makes it difficult to interpret, opaque, and black-box
models with little or no understanding of their internal states and
decision-making process [ 37]. Gaining meaningful knowledge and
understanding of how and why the model arrived at a particular
decision or outcome is crucial in model explainability, making it
one of the important evaluation metrics In this section, we offer a
concise overview of Multimodal Explainable Artificial Intelligence,categorizing MXAI into ante-hoc and post-hoc approaches. A com-
prehensive review of MXAI can be found in [71]
6.1 Multimodal XAI
6.1.1 Ante-hoc MXAI. Ante-hoc methods integrate interpretability
modules directly into the primary model for simultaneous learning.
In multimodal contexts, emphasizing the modeling of cross-modal
interactions and assessing modalities’ relevance is essential. For
example, [ 45] introduces a multitask learning approach for VQA
that concurrently trains an answer predictor and a textual explana-
tion generator, leveraging an attention mechanism for cross-modal
dynamics. Recently, research on multimodal foundation models
has emerged. [ 88] devises an interactive multi-agent VQA frame-
work based on VLMs. This framework consists of a seeker and
responder that actively generates and exchanges information and
candidate responses related to the query, and an integrator that
synthesizes candidates and generated hypotheses from the seeker
and responder to formulate the final answer.
6.1.2 Post-hoc MXAI. Post-hoc methods generally refer to train-
ing an interpretability module based on the output of the primary
model. To address the fundamental challenges of capturing inter-
actions across modalities, DIME [ 52] proposes to disentangle the
black-box model into unimodal contributions and multimodal inter-
actions. This allows for fine-grained analysis of multimodal models
while maintaining generality across arbitrary modalities, model
architectures, and tasks. Another post-hoc method [ 65] focuses on
understanding individual neuron functionality in tasks, utilizing
VLMs and unique network layers to achieve high activation sparsity.
It enables the generation of simple textual descriptions, facilitating
a direct investigation of the network’s decision process.
6.2 Benchmark
MXAI datasets are typically organized based on tasks, I/O types,
dates, and brief descriptions. In evaluating MXAI, the focus is on
assessing the effectiveness of explanations in elucidating model
decisions. Evaluation methods can be categorized into three types:
application-grounded, involving domain experts for real-world util-
ity; human-grounded, testing understandability with individuals
regardless of expertise; and functionally-grounded, using formal
criteria to assess explanation quality. Detailed descriptions of MXAI
benchmarks can be found in [71].
6.3 Discussion
Formalizing definitions and terminology is crucial for the MXAI
field’s development, as current informal practices and inconsistent
definitions hinder progress. Specifically, it is crucial to define ex-
planations precisely and establish standardization for measuring
MXAI’s efficiency to enable comparative evaluation of different
methods. Another significant challenge arises from the absence of
ground-truth modality explanations. For instance, in contrast to
textual explanations, the lack of corresponding ground-truth visual
explanation data hampers the development of related explanation
methods, especially in supervised learning scenarios.
6660A Survey on Safe Multi-Modal Learning Systems KDD ’24, August 25–29, 2024, Barcelona, Spain.
7 CONTROLLABILITY - FAIRNESS
MMLS incorporates data from various sources, presenting unique
fairness challenges. Prior work indicates that biases in MMLS stem
from individual modalities, which could then be amplified dur-
ing later modality fusion process [ 96]. It is crucial to identify and
address the specific fairness challenges and underlying causes in
MMLS to effectively mitigate discrimination.
7.1 Improving Fairness in MMLS
Addressing fairness in multimodal scenarios is more challenging
compared to unimodal due to cross-modality correlation. Following
categorization in uni-modal fair machine learning [ 15], we organize
fairness algorithms in MMLS into three groups: pre-processing, in-
processing, and post-processing.
7.1.1 Pre-processing. Pre-processing techniques operate on orig-
inal input data to remove its inherent bias. For instance, In the
context of Automated Video Interviews, Booth et al. [ 5] investigate
and tackle gender bias by analyzing verbal, paraverbal, and visual
features. They present two strategies to mitigate gender-related
bias within the feature set: first, gender-norming, which involves
separately normalizing the features of men and women before train-
ing, and second, iterative gendered predictor removal, a process
that systematically eliminates the predictors most indicative of
gender information. This study also indicates that combining mul-
tiple modalities without addressing biases hardly boosts accuracy
and tends to amplify unfairness compared to unimodal models.
Nonetheless, bias mitigation techniques like feature reduction [ 73]
can achieve an admissible fairness-accuracy trade-off.
7.1.2 In-processing. In-processing techniques involve devising train-
ing algorithms to mitigate discrimination. Compared to the pre-
processing techniques, in-processing methods tend to achieve supe-
rior trade-off between fairness and task performance. For instance,
FairCVtest [ 66] offers a fictional recruitment testbed demonstrating
how biases can be affected by unimodal factors. It further ensures
fair multimodal outcomes by generating agnostic representations
based on SensitiveNets learning strategy. However, addressing un-
fairness in MMLS involves more than just tackling unimodal biases.
FMMRec [ 10] finds that multimodal recommend systems’ bias stems
from repetitively identifying sensitive information across modali-
ties. Adversarial learning method is then employed to disentangle
and filter out sensitive information from modality representations.
Other approaches involve integrating the aforementioned tech-
niques. For instance, [ 96] mitigates gender and race biases through
a combination of adversarial learning and data balancing, employ-
ing both in-processing and post-processing strategies.
7.1.3 Post-processing. Post-processing techniques are performed
after training by accessing a holdout set not involved during train-
ing. For instance, [ 9] determines group-specific random thresholds
based on the intersection of group-specific ROC curves. By adding
this to the clinical MMLS outputs, the prediction bias can be re-
moved. Notably, post-processing methods are exclusively usable in
black-box settings where the model and training data are inaccessi-
ble. Although pre-processing and post-processing are more general-
izable, easier to implement, and less time-consuming, they may lead
to uncertain outcomes if not customized for specific algorithms[ 56].7.2 Benchmark
Datasets crucial for ensuring fairness in MMLS are limited due
to privacy concerns and the intricate nature of multimodal data.
Given the scarcity of research in the field, we highlight several
representative datasets for reference. MovieKens [ 60] is a widely
used multimodal benchmark dataset for movie recommendation.
IEMOCAP [ 7] is a emotion recognition dataset covering audio, text,
and video. Additionally, the clinical database MIMIC-III [ 36] and the
image-text corpus Multi30K [ 19] also contribute to fairness research
in clinical data and multilingual contexts, respectively. Notions of
fairness vary in different systems. Various fairness notions (e.g.,
Equalized Odds andStatistical Parity [56]) in MMLS draw inspira-
tion from concepts established in unimodal settings. Key fairness
categories encompass individual fairness, focusing on individual
samples, and group fairness, targeting protected groups such as
Black or Female [ 9]. In the context of multilingual MMLS, additional
fairness metrics like Multilingual Individual andMultilingual Group
Fairness have been introduced [ 86]. The former necessitates similar
semantics across languages, linking text to images, while the latter
measures equalized predictive performance across languages.
7.3 Discussions
In contrast to the abundance of datasets available for fairness stud-
ies in unimodal scenarios, there is a notable absence of reliable
datasets for fairness studies in MMLS. This gap is attributed to the
intricate nature of multi-modal information, privacy concerns, and
the diverse system considerations related to fairness. Existing work,
such as [ 73], compares biases introduced by different modalities and
their combinations, yet falls short of delving into the fundamental
causes of multimodal unfairness and proposing effective solutions.
One may explore the causes from an uncertainty perspective [ 83].
Furthermore, there is a crucial need for the introduction of new def-
initions of fairness. While some notable work has suggested some
[86], it lacks a thorough explanation of why this new definition is
necessary or when it should be applied.
8 CONTROLLABILITY - PRIVACY
8.1 Privacy Issues in MMLS
Significant privacy concerns regarding MMLS have emerged due
to their high risk of memorizing and leaking private information
[12,58]. Membership Inference Attacks [ 39], for example, could
reveal an individual’s health status by checking if their record is
in a multimodal disease database. Besides, reconstruction attacks
may exploit MMLS’s memorized information to extract private
data, such as identifying residents in a geospatial system using
modality alignment information between street images and resi-
dent details [ 70]. The recent emerging large MMLS following the
pretrain-then-finetune paradigm further intensifies privacy con-
cerns. Each stage of these models, pre-training, fine-tuning, and
prompt interaction, carries a high risk of privacy leakage [70].
8.2 Preserving Privacy in MMLS
8.2.1 Differentially private training. The differential privacy (DP)
technique, recognized as a gold standard in privacy preservation,
6661KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianyi Zhao, Liangliang Zhang, Yao Ma, and Lu Cheng
ensures data protection by introducing randomness into its compu-
tational processes. DP in MMLS remains understudied. Multimodal
DP was first introduced to address the heightened risk of privacy
leakage arising from MMLS memorizing sensitive training data
information [ 30]. In this study, DP-CLIP is proposed to safeguard
vision-language tasks by learning differentially private image and
text representations. It employs per-batch, instead of per-sample,
noisy stochastic gradient descent to train the encoder, considering
that the CLIP loss requires contrasting data from different pairs.
8.2.2 Machine unlearning. Machine unlearning aims at selectively
erasing information (e.g., sensitive information) from a trained
model without the need for complete retraining. Progress in ma-
chine unlearning has advanced in unimodal learning, but its ap-
plication in MMLS is still in its infancy. Compared to unimodal
scenarios, the key challenge lies in effectively addressing the inher-
ent interdependencies across modalities. One work [ 76] presents a
modality-specific attention mechanism that generates individual
attention scores per modality, averages them across modalities for
each data point to determine its final score, and then decides on data
removal and model parameter updates based on this score. Recently
a more comprehensive framework for multimodal unlearning has
been proposed [ 13], encompassing modality decoupling, unimodal
knowledge retention, and multimodal knowledge retention. Its first
module ensures that individual data modality in deleted pairs are
treated as unrelated by the model, while the latter two focus on
preserving essential knowledge to maintain the model performance.
8.2.3 Federated learning. Federated learning enables collaborative
model training without the need to share clients’ local data, offering
a more privacy-enhancing approach compared to centralized train-
ing methods. The MMFed framework, proposed in [94], addresses
modality discrepancies in MMLS settings through a unified mul-
timodal federated learning approach. It utilizes a cross-attention
mechanism to identify key correlations across modalities on lo-
cal clients, then uploads the parameters of both the classifier and
attention module to a central server. CreamFL further advances
multimodal federated learning by enabling larger server models and
aggregation of clients with diverse architectures and data modali-
ties, thereby aligning more closely with practical scenarios [100].
8.3 Benchmark
Current memorization measurements lack standardization and are
primarily restricted to VLMs. Common metrics include Frechet In-
ception Distance score [77] and Memorization score [102]. Machine
unlearning evaluation parallels that in unimodal settings, including
attack-based methods like Membership Inference Robustness, and
accuracy-based metrics like Mean Squared Error, alongside others
like Activation Distance and Retraining Time [ 59]. Benchmarking
multimodal federated learning is still in its early stages. FedMul-
timodal is the first systematic benchmark [ 22], offering 10 repre-
sentative datasets from 5 key MMLS applications and providing a
pipeline for effective multimodal federated learning assessment.
8.4 Discussion
Due to the diversity and richness of input data modalities, the po-
tential risk of privacy leakage is concerning in MMLS. This involvesissues such as the recovery of removed sensitive information after
multimodal fusions. Besides, the increasing scale of MMLS further
exacerbates the challenge of memorization. Despite these concerns,
privacy issues and their underlying causes are still not thoroughly
explored, lacking formal conceptualizations and investigations. One
potential direction is to extend findings in [ 105] to multimodal data.
9 CONCLUSION AND OPEN CHALLENGES
This paper presents the first comprehensive survey on safe MMLS,
introducing a systematic taxonomy centering on four critical di-
mensions: robustness, alignment, monitoring, andcontrollability. Fol-
lowing this taxonomy, we review and summarize current research
and progress. We hope that this survey offer insights for future
research. Below, we highlight several challenges in MMLS safety.
Memorization Problem in MMLS. Memorization in large-scale
models is a significant contributor to privacy leakage. While this
issue has been extensively explored in unimodal systems, research
on memorization within MMLS remains limited. There is an urgent
need for a formal definition of this problem and the establishment
of unified metrics to measure it, especially given the recent rapid
advancement of multimodal foundation models.
Differentially Private Training. Differential privacy techniques
demonstrate their effectiveness in safeguarding privacy in unimodal
systems. Yet, adapting these techniques for MMLS faces challenges
and is in its infancy. Some strategies to enhance differential privacy
are to apply modality-specific noise addition and coordinate across
modalities by considering the interdependence between modalities.
Calibration and Uncertainty Quantification. Miscalibration
in multimodal learning, a critical yet under-explored issue, poses
challenges to MMLS safety. Future research should focus on under-
standing this phenomenon and developing advanced calibration
methods. Besides, conformal prediction, known for its scalability
and minimal assumptions, emerges as a promising solution for
quantifying uncertainty. Developing CP methods tailored explicitly
for MMLS could significantly benefit practical applications.
Advancing Datasets for MMLS Safety. Challenges with multi-
modal datasets include the scarcity of open-source large models
capable of effectively handling multimodal inputs, limited inter-
pretability of existing models that hinders understanding and trust,
and the difficulty of quality control and data collection. In addi-
tion, research in MMLS is often restricted to specific use cases, and
uneven dataset distribution hinders model generalization. These
issues require more efforts in data collection and refinement.
Safety Challenges in Multimodal LLMs. Multimodal LLMs (e.g.,
GPT 4V [ 1]) are designed to understand and generate content across
different modalities. Safeguarding multimodal LLMs is very chal-
lenging due to various reasons including the additional safety rift
induced by different data modalities, often black-box API access,
unknown data sources, and emerging LLM safety issues such as
hallucination, data leakage, and jailbreak. However, existing efforts
to achieve safe multimodal LLMs have been scarce.
ACKNOWLEDGEMENTS
This material is based upon work supported by the National Science
Foundation (NSF) Grant #2312862, #2406648, #2406647 and a Cisco
gift grant.
6662A Survey on Safe Multi-Modal Learning Systems KDD ’24, August 25–29, 2024, Barcelona, Spain.
REFERENCES
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al .2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).
[2]Alex Andonian, Shixing Chen, and Raffay Hamid. 2022. Robust cross-modal
representation learning with progressive self-distillation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition . 16430–16441.
[3]Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2018. Multi-
modal machine learning: A survey and taxonomy. IEEE transactions on pattern
analysis and machine intelligence 41, 2 (2018), 423–443.
[4]Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. 2019.
MVTec AD–A comprehensive real-world dataset for unsupervised anomaly
detection. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. 9592–9600.
[5]Brandon M Booth, Louis Hickman, Shree Krishna Subburaj, Louis Tay, Sang Eun
Woo, and Sidney K D’Mello. 2021. Bias and fairness in multimodal machine
learning: A case study of automated video interviews. In Proceedings of the 2021
International Conference on Multimodal Interaction. 268–277.
[6]Jakob Božič, Domen Tabernik, and Danijel Skočaj. 2021. Mixed supervision for
surface-defect detection: From weakly to fully supervised learning. Computers
in Industry 129 (2021), 103459.
[7]Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower,
Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. 2008.
IEMOCAP: Interactive emotional dyadic motion capture database. Language
resources and evaluation 42 (2008), 335–359.
[8]Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski,
Irena Gao, Pang Wei W Koh, Daphne Ippolito, Florian Tramer, and Ludwig
Schmidt. 2024. Are aligned neural networks adversarially aligned? Advances in
Neural Information Processing Systems 36 (2024).
[9]John Chen, Ian Berlot-Attwell, Safwan Hossain, Xindi Wang, and Frank Rudzicz.
2020. Exploring text specific and blackbox fairness algorithms in multimodal
clinical nlp. arXiv preprint arXiv:2011.09625 (2020).
[10] Weixin Chen, Li Chen, Yongxin Ni, Yuhan Zhao, Fajie Yuan, and Yongfeng
Zhang. 2023. FMMRec: Fairness-aware Multimodal Recommendation. arXiv
preprint arXiv:2310.17373 (2023).
[11] Xuhai Chen, Jiangning Zhang, Guanzhong Tian, Haoyang He, Wuhao Zhang,
Yabiao Wang, Chengjie Wang, Yunsheng Wu, and Yong Liu. 2023. CLIP-AD: A
Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection.
arXiv preprint arXiv:2311.00453 (2023).
[12] Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, and Alan Ritter. 2023. Can
Language Models be Instructed to Protect Personal Information? arXiv preprint
arXiv:2310.02224 (2023).
[13] Jiali Cheng and Hadi Amiri. 2023. Multimodal Machine Unlearning. arXiv
preprint arXiv:2311.12047 (2023).
[14] Lu Cheng, Jundong Li, Yasin N Silva, Deborah L Hall, and Huan Liu. 2019. Xbully:
Cyberbullying detection within a multi-modal context. In Proceedings of the
twelfth acm international conference on web search and data mining. 339–347.
[15] Lu Cheng, Kush R Varshney, and Huan Liu. 2021. Socially responsible ai al-
gorithms: Issues, purposes, and challenges. Journal of Artificial Intelligence
Research 71 (2021), 1137–1181.
[16] Hanqiu Deng, Zhaoxiang Zhang, Jinan Bao, and Xingyu Li. 2023. AnoVL:
Adapting Vision-Language Models for Unified Zero-shot Anomaly Localization.
arXiv preprint arXiv:2308.15939 (2023).
[17] Greg d’Eon, Jason d’Eon, James R Wright, and Kevin Leyton-Brown. 2022. The
spotlight: A general method for discovering systematic errors in deep learning
models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability,
and Transparency. 1962–1981.
[18] Shiladitya Dutta, Hongbo Wei, Lars van der Laan, and Ahmed M Alaa. 2023.
Estimating Uncertainty in Multimodal Foundation Models using Public Internet
Data. arXiv preprint arXiv:2310.09926 (2023).
[19] Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. 2016. Multi30k:
Multilingual english-german image descriptions. arXiv preprint arXiv:1605.00459
(2016).
[20] Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher
Lee-Messer, Jared Dunnmon, James Zou, and Christopher Ré. 2022. Domino:
Discovering systematic errors with cross-modal embeddings. arXiv preprint
arXiv:2203.14960 (2022).
[21] Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar,
Achal Dave, and Ludwig Schmidt. 2022. Data determines distributional robust-
ness in contrastive language image pre-training (clip). In International Conference
on Machine Learning. PMLR, 6216–6234.
[22] Tiantian Feng, Digbalay Bose, Tuo Zhang, Rajat Hebbar, Anil Ramakrishna,
Rahul Gupta, Mi Zhang, Salman Avestimehr, and Shrikanth Narayanan. 2023.
FedMultimodal: A Benchmark For Multimodal Federated Learning. arXiv
preprint arXiv:2306.09486 (2023).[23] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. 2020.
Large-scale adversarial training for vision-and-language representation learning.
Advances in Neural Information Processing Systems 33 (2020), 6616–6628.
[24] Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, and Jinqiao
Wang. 2023. Anomalygpt: Detecting industrial anomalies using large vision-
language models. arXiv preprint arXiv:2308.15366 (2023).
[25] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration
of modern neural networks. In International conference on machine learning.
PMLR, 1321–1330.
[26] Zongbo Han, Fan Yang, Junzhou Huang, Changqing Zhang, and Jianhua Yao.
2022. Multimodal dynamics: Dynamical fusion for trustworthy multimodal
classification. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. 20707–20717.
[27] Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Xing Xie, Tailin
Wu, Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, et al .2023. How Well
Does GPT-4V (ision) Adapt to Distribution Shifts? A Preliminary Investigation.
arXiv preprint arXiv:2312.07424 (2023).
[28] Holly C Hartmann, Thomas C Pagano, Soroosh Sorooshian, and Roger Bales.
2002. Confidence builders: Evaluating seasonal climate forecasts from user
perspectives. Bulletin of the American Meteorological Society 83, 5 (2002), 683–
698.
[29] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. 2011.
Bayesian active learning for classification and preference learning. arXiv preprint
arXiv:1112.5745 (2011).
[30] Alyssa Huang, Peihan Liu, Ryumei Nakada, Linjun Zhang, and Wanrong Zhang.
2023. Safeguarding Data in Multimodal AI: A Differentially Private Approach
to CLIP Training. arXiv preprint arXiv:2306.08173 (2023).
[31] Yibin Huang, Congying Qiu, and Kui Yuan. 2020. Surface defect saliency of
magnetic tile. The Visual Computer 36, 1 (2020), 85–96.
[32] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. 2022. Dis-
tilling model failures as directions in latent space. arXiv preprint arXiv:2206.14754
(2022).
[33] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichan-
dran, and Onkar Dabeer. 2023. Winclip: Zero-/few-shot anomaly classification
and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 19606–19616.
[34] Stepan Jezek, Martin Jonak, Radim Burget, Pavel Dvorak, and Milos Skotak.
2021. Deep learning-based defect detection of metal parts: evaluating current
methods in complex conditions. In 2021 13th International congress on ultra
modern telecommunications and control systems and workshops (ICUMT). IEEE,
66–71.
[35] Tianchen Ji, Sri Theja Vuppala, Girish Chowdhary, and Katherine Driggs-
Campbell. 2020. Multi-modal anomaly detection for unstructured and uncertain
environments. arXiv preprint arXiv:2012.08637 (2020).
[36] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng,
Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi,
and Roger G Mark. 2016. MIMIC-III, a freely accessible critical care database.
Scientific data 3, 1 (2016), 1–9.
[37] Gargi Joshi, Rahee Walambe, and Ketan Kotecha. 2021. A review on explainabil-
ity in multimodal deep neural nets. IEEE Access 9 (2021), 59800–59821.
[38] Sunwoo Kim, Hun Im, Woojun Lee, Seonggye Lee, and Pilsung Kang. 2023. Ro-
bustMixGen: Data Augmentation for Enhancing Robustness of Visual-Language
Models in the Presence of Distribution Shift. Authorea Preprints (2023).
[39] Myeongseob Ko, Ming Jin, Chenguang Wang, and Ruoxi Jia. 2023. Practical
Membership Inference Attacks Against Large-Scale Multi-Modal Models: A
Pilot Study. In Proceedings of the IEEE/CVF International Conference on Computer
Vision. 4871–4881.
[40] Ranganath Krishnan and Omesh Tickoo. 2020. Improving model calibration
with accuracy versus uncertainty optimization. Advances in Neural Information
Processing Systems 33 (2020), 18237–18248.
[41] Max-Heinrich Laves, Sontje Ihler, Karl-Philipp Kortmann, and Tobias Ortmaier.
2019. Well-calibrated model uncertainty with temperature scaling for dropout
variational inference. arXiv preprint arXiv:1909.13550 (2019).
[42] Linjie Li, Zhe Gan, and Jingjing Liu. 2020. A closer look at the robustness of
vision-and-language pre-trained models. arXiv preprint arXiv:2012.08673 (2020).
[43] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng
Yang, Benyou Wang, and Lingpeng Kong. 2023. Silkie: Preference distillation
for large visual language models. arXiv preprint arXiv:2312.10665 (2023).
[44] Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, and Qi Liu. 2024.
Red Teaming Visual Language Models. arXiv preprint arXiv:2401.12915 (2024).
[45] Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and Jiebo Luo. 2018. Vqa-e: Explain-
ing, elaborating, and enhancing your answers for visual questions. In Proceedings
of the European Conference on Computer Vision (ECCV). 552–567.
[46] Yuanze Li, Haolin Wang, Shihao Yuan, Ming Liu, Debin Zhao, Yiwen Guo, Chen
Xu, Guangming Shi, and Wangmeng Zuo. 2023. Myriad: Large Multimodal
Model by Applying Vision Experts for Industrial Anomaly Detection. arXiv
preprint arXiv:2310.19070 (2023).
6663KDD ’24, August 25–29, 2024, Barcelona, Spain. Tianyi Zhao, Liangliang Zhang, Yao Ma, and Lu Cheng
[47] Ronghao Lin and Haifeng Hu. 2022. Multimodal Contrastive Learning via Uni-
Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis.
arXiv preprint arXiv:2210.14556 (2022).
[48] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017.
Focal loss for dense object detection. In Proceedings of the IEEE international
conference on computer vision. 2980–2988.
[49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual
instruction tuning. Advances in neural information processing systems 36 (2024).
[50] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. 2018. Future frame
prediction for anomaly detection–a new baseline. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 6536–6545.
[51] Zichang Liu, Zhiqiang Tang, Xingjian Shi, Aston Zhang, Mu Li, Anshumali
Shrivastava, and Andrew Gordon Wilson. 2022. Learning Multimodal Data
Augmentation in Feature Space. arXiv preprint arXiv:2212.14453 (2022).
[52] Yiwei Lyu, Paul Pu Liang, Zihao Deng, Ruslan Salakhutdinov, and Louis-Philippe
Morency. 2022. Dime: Fine-grained interpretations of multimodal models via
disentangled local explanations. In Proceedings of the 2022 AAAI/ACM Conference
on AI, Ethics, and Society. 455–467.
[53] Huan Ma, Qingyang Zhang, Changqing Zhang, Bingzhe Wu, Huazhu Fu,
Joey Tianyi Zhou, and Qinghua Hu. 2023. Calibrating multimodal learning. In
International Conference on Machine Learning. PMLR, 23429–23450.
[54] Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, and Xi Peng. 2022.
Are multimodal transformers robust to missing modality?. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18177–18186.
[55] Brandon McKinzie, Vaishaal Shankar, Joseph Yitan Cheng, Yinfei Yang, Jonathon
Shlens, and Alexander T Toshev. 2023. Robustness in multimodal learning under
train-test modality mismatch. In International Conference on Machine Learning.
PMLR, 24291–24303.
[56] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
Galstyan. 2021. A survey on bias and fairness in machine learning. ACM
computing surveys (CSUR) 54, 6 (2021), 1–35.
[57] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. 2016. V-net: Fully
convolutional neural networks for volumetric medical image segmentation. In
2016 fourth international conference on 3D vision (3DV). Ieee, 565–571.
[58] Ali Naseh, Jaechul Roh, and Amir Houmansadr. 2023. Understanding (Un)
Intended Memorization in Text-to-Image Generative Models. arXiv preprint
arXiv:2312.07550 (2023).
[59] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung
Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. 2022. A survey of machine
unlearning. arXiv preprint arXiv:2209.02299 (2022).
[60] Yongxin Ni, Yu Cheng, Xiangyan Liu, Junchen Fu, Youhua Li, Xiangnan He,
Yongfeng Zhang, and Fajie Yuan. 2023. A content-driven micro-video recom-
mendation dataset at scale. arXiv preprint arXiv:2309.15379 (2023).
[61] Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin. 2024. Jail-
breaking attack against multimodal large language model. arXiv preprint
arXiv:2402.02309 (2024).
[62] Changdae Oh, Mijoo Kim, Hyesu Lim, Junhyeok Park, Euiseog Jeong, Zhi-Qi
Cheng, and Kyungwoo Song. 2023. Towards Calibrated Robust Fine-Tuning of
Vision-Language Models. arXiv preprint arXiv:2311.01723 (2023).
[63] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .2022.
Training language models to follow instructions with human feedback. Advances
in Neural Information Processing Systems 35 (2022), 27730–27744.
[64] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel.
2021. Deep learning for anomaly detection: A review. ACM computing surveys
(CSUR) 54, 2 (2021), 1–38.
[65] Konstantinos Panousis and Sotirios Chatzis. 2024. DISCOVER: Making Vision
Networks Interpretable via Competition and Dissection. Advances in Neural
Information Processing Systems 36 (2024).
[66] Alejandro Pena, Ignacio Serna, Aythami Morales, and Julian Fierrez. 2020. Bias
in multimodal AI: Testbed for fair automatic recruitment. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops.
28–29.
[67] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal.
2023. Visual adversarial examples jailbreak aligned large language models. In
The Second Workshop on New Frontiers in Adversarial Machine Learning.
[68] Jielin Qiu, Yi Zhu, Xingjian Shi, Florian Wenzel, Zhiqiang Tang, Ding Zhao,
Bo Li, and Mu Li. 2022. Are Multimodal Models Robust to Image and Text
Perturbations? arXiv preprint arXiv:2212.08044 (2022).
[69] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano
Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neural Information Processing
Systems 36 (2024).
[70] Jinmeng Rao, Song Gao, Gengchen Mai, and Krzysztof Janowicz. 2023. Building
Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation
Models (Vision Paper). In Proceedings of the 31st ACM International Conference
on Advances in Geographic Information Systems. 1–4.[71] Nikolaos Rodis, Christos Sardianos, Georgios Th Papadopoulos, Panagiotis
Radoglou-Grammatikis, Panagiotis Sarigiannidis, and Iraklis Varlamis. 2023.
Multimodal Explainable Artificial Intelligence: A Comprehensive Review of
Methodological Advances and Future Research Directions. arXiv preprint
arXiv:2306.05731 (2023).
[72] Christian Schlarmann and Matthias Hein. 2023. On the adversarial robustness
of multi-modal foundation models. In Proceedings of the IEEE/CVF International
Conference on Computer Vision. 3677–3685.
[73] Matheus Schmitz, Rehan Ahmed, and Jimi Cao. 2022. Bias and fairness on
multimodal emotion detection algorithms. arXiv preprint arXiv:2205.08383
(2022).
[74] Glenn Shafer. 1976. A mathematical theory of evidence. Vol. 42. Princeton
university press.
[75] Glenn Shafer and Vladimir Vovk. 2008. A tutorial on conformal prediction.
Journal of Machine Learning Research 9, 3 (2008).
[76] Thanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, Taotao Cai, Xiaofeng Zhu, and
Qing Li. 2023. FRAMU: Attention-based Machine Unlearning using Federated
Reinforcement Learning. arXiv preprint arXiv:2309.10283 (2023).
[77] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom
Goldstein. 2023. Understanding and Mitigating Copying in Diffusion Models.
arXiv preprint arXiv:2305.20086 (2023).
[78] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to
summarize with human feedback. Advances in Neural Information Processing
Systems 33 (2020), 3008–3021.
[79] Jiayuan Su, Jing Luo, Hongwei Wang, and Lu Cheng. 2024. Api is enough:
Conformal prediction for large language models without logit-access. arXiv
preprint arXiv:2403.01216 (2024).
[80] Mahesh Subedar, Ranganath Krishnan, Paulo Lopez Meyer, Omesh Tickoo, and
Jonathan Huang. 2019. Uncertainty-aware audiovisual activity recognition using
deep bayesian variational inference. In Proceedings of the IEEE/CVF international
conference on computer vision. 6301–6310.
[81] Waqas Sultani, Chen Chen, and Mubarak Shah. 2018. Real-world anomaly
detection in surveillance videos. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 6479–6488.
[82] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang
Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al .2023.
Aligning large multimodal models with factually augmented rlhf. arXiv preprint
arXiv:2309.14525 (2023).
[83] Anique Tahir, Lu Cheng, and Huan Liu. 2023. Fairness through aleatoric uncer-
tainty. In Proceedings of the 32nd ACM International Conference on Information
and Knowledge Management. 2372–2381.
[84] Yapeng Tian and Chenliang Xu. 2021. Can audio-visual integration strengthen
robustness under multimodal attacks?. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition. 5601–5611.
[85] Shengbang Tong, Erik Jones, and Jacob Steinhardt. 2024. Mass-producing failures
of multimodal systems with language models. Advances in Neural Information
Processing Systems 36 (2024).
[86] Jialu Wang, Yang Liu, and Xin Eric Wang. 2021. Assessing multilingual fairness
in pre-trained multimodal representations. arXiv preprint arXiv:2106.06683
(2021).
[87] Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao Wang, and Chengjie
Wang. 2023. Multimodal industrial anomaly detection via hybrid fusion. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
8032–8041.
[88] Zeqing Wang, Wentao Wan, Runmeng Chen, Qiqing Lao, Minjie Lang, and
Keze Wang. 2023. Towards Top-Down Reasoning: An Explainable Multi-Agent
Approach for Visual Question Answering. arXiv preprint arXiv:2311.17331
(2023).
[89] Donglai Wei, Yang Liu, Xiaoguang Zhu, Jing Liu, and Xinhua Zeng. 2022. MSAF:
Multimodal supervise-attention enhanced fusion for video anomaly detection.
IEEE Signal Processing Letters 29 (2022), 2178–2182.
[90] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael
Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon,
Simon Kornblith, et al .2022. Model soups: averaging weights of multiple
fine-tuned models improves accuracy without increasing inference time. In
International conference on machine learning. PMLR, 23965–23998.
[91] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and
Zhiwei Yang. 2020. Not only look, but also listen: Learning multimodal violence
detection under weak supervision. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16. Springer,
322–339.
[92] Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng
Wang, and Yanning Zhang. 2023. Vadclip: Adapting vision-language models
for weakly supervised video anomaly detection. arXiv preprint arXiv:2308.11681
(2023).
[93] Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, and Lichao Sun. 2023. Jailbreak-
ing gpt-4v via self-adversarial attacks with system prompts. arXiv preprint
6664A Survey on Safe Multi-Modal Learning Systems KDD ’24, August 25–29, 2024, Barcelona, Spain.
arXiv:2311.09127 (2023).
[94] Baochen Xiong, Xiaoshan Yang, Fan Qi, and Changsheng Xu. 2022. A unified
framework for multi-modal federated learning. Neurocomputing 480 (2022),
110–118.
[95] Yihao Xue, Siddharth Joshi, Dang Nguyen, and Baharan Mirzasoleiman. 2023.
Understanding the Robustness of Multi-modal Contrastive Learning to Distri-
bution Shift. arXiv preprint arXiv:2310.04971 (2023).
[96] Shen Yan, Di Huang, and Mohammad Soleymani. 2020. Mitigating biases in
multimodal personality assessment. In Proceedings of the 2020 International
Conference on Multimodal Interaction. 361–369.
[97] Karren Yang, Wan-Yi Lin, Manash Barman, Filipe Condessa, and Zico Kolter.
2022. Defending multimodal fusion models against single-source adversaries.
In 2021 IEEE. In CVF Conference on Computer Vision and Pattern Recognition
(CVPR). IEEE Xplore.
[98] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and
Pradeep Ravikumar. 2020. On completeness-aware concept-based explanations
in deep neural networks. Advances in neural information processing systems 33
(2020), 20554–20565.
[99] Youngjae Yoo, Chung-Yeon Lee, and Byoung-Tak Zhang. 2021. Multimodal
anomaly detection based on deep auto-encoder for object slip perception of
mobile manipulation robots. In 2021 IEEE International Conference on Robotics
and Automation (ICRA). IEEE, 11443–11449.
[100] Qiying Yu, Yang Liu, Yimu Wang, Ke Xu, and Jingjing Liu. 2023. Multimodal
federated learning via contrastive representation ensemble. arXiv preprint
arXiv:2302.08888 (2023).[101] Luca Zanella, Benedetta Liberatori, Willi Menapace, Fabio Poiesi, Yiming Wang,
and Elisa Ricci. 2023. Delving into CLIP latent space for Video Anomaly Recog-
nition. arXiv preprint arXiv:2310.02835 (2023).
[102] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi.
2023. Forget-me-not: Learning to forget in text-to-image diffusion models. arXiv
preprint arXiv:2303.17591 (2023).
[103] Jiaming Zhang, Qi Yi, and Jitao Sang. 2022. Towards adversarial attack on vision-
language pre-training models. In Proceedings of the 30th ACM International
Conference on Multimedia. 5005–5013.
[104] Qingyang Zhang, Haitao Wu, Changqing Zhang, Qinghua Hu, Huazhu Fu,
Joey Tianyi Zhou, and Xi Peng. 2023. Provable Dynamic Fusion for Low-Quality
Multimodal Data. arXiv preprint arXiv:2306.02050 (2023).
[105] Tianyi Zhao, Hui Hu, and Lu Cheng. 2023. Unveiling the Role of Message
Passing in Dual-Privacy Preservation on GNNs. In Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management. 3474–3483.
[106] Tianyi Zhao, Jian Kang, and Lu Cheng. 2024. Conformalized Link Prediction on
Graph Neural Networks. In KDD.
[107] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. 2023.
Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection.
arXiv preprint arXiv:2310.18961 (2023).
[108] Ziqi Zhou, Shengshan Hu, Minghui Li, Hangtao Zhang, Yechao Zhang, and Hai
Jin. 2023. Advclip: Downstream-agnostic adversarial examples in multimodal
contrastive learning. In Proceedings of the 31st ACM International Conference on
Multimedia. 6311–6320.
[109] Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer.
2022. Spot-the-difference self-supervised pre-training for anomaly detection and
segmentation. In European Conference on Computer Vision. Springer, 392–408.
6665