FLAIM: AIM-based Synthetic Data Generation in the Federated
Setting
Samuel Maddock
University of Warwick
Coventry, UK
s.maddock@warwick.ac.ukGraham Cormode
Meta AI & University of Warwick
Coventry, UK
gcormode@meta.comCarsten Maple
University of Warwick
Coventry, UK
cm@warwick.ac.uk
ABSTRACT
Preserving individual privacy while enabling collaborative data
sharing is crucial for organizations. Synthetic data generation is
one solution, producing artificial data that mirrors the statistical
properties of private data. While numerous techniques have been
devised under differential privacy, they predominantly assume data
is centralized. However, data is often distributed across multiple
clients in a federated manner. In this work, we initiate the study of
federated synthetic tabular data generation. Building upon a SOTA
central method known as AIM, we present DistAIM andFLAIM. We
first show that it is straightforward to distribute AIM, extending a
recent approach based on secure multi-party computation which
necessitates additional overhead, making it less suited to federated
scenarios. We then demonstrate that naively federating AIM can
lead to substantial degradation in utility under the presence of
heterogeneity. To mitigate both issues, we propose an augmented
FLAIM approach that maintains a private proxy of heterogeneity.
We simulate our methods across a range of benchmark datasets
under different degrees of heterogeneity and show we can improve
utility while reducing overhead.
CCS CONCEPTS
•Security and privacy →Privacy-preserving protocols.
KEYWORDS
Synthetic Data, Federated Learning, Differential Privacy
ACM Reference Format:
Samuel Maddock, Graham Cormode, and Carsten Maple. 2024. FLAIM: AIM-
based Synthetic Data Generation in the Federated Setting. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671990
1 INTRODUCTION
Modern computational applications are predicated on the availabil-
ity of significant volumes of high-quality data. Increasingly, such
data is not freely available: it may not be collected in the volume
needed, and may be subject to privacy concerns. Recent regulations
such as the General Data Protection Regulation (GDPR) restrict
the extent to which data collected for a specific purpose may be
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671990processed for some other goal. The aim of synthetic data generation
(SDG) is to solve this problem by allowing the creation of realistic
artificial data that shares the same structure and statistical proper-
ties as the original data source. SDG is an active area of research,
offering the potential for organisations to share useful datasets
while protecting the privacy of individuals [2, 36, 50].
SDG methods fall into two categories: deep learning [ 15,25,53]
and statistical models [ 55,56]. Nevertheless, without strict privacy
measures in place, it is possible for SDG models to leak informa-
tion about the data it was trained on [19, 38, 46]. It is common for
deep learning approaches such as Generative Adversarial Networks
(GANs) to produce verbatim copies of training data, breaching pri-
vacy [ 45,49]. A standard approach to prevent leakage is to use
Differential Privacy (DP) [ 12]. DP is a formal definition which en-
sures the output of an algorithm does not depend heavily on any one
individual’s data by introducing calibrated random noise. Under DP,
statistical models have become state-of-the-art (SOTA) for tabular
data and often outperform deep learning counterparts [ 14,29,47].
Approaches are based on Bayesian networks [ 56], Markov random
fields [33] and iterative marginal-based methods [3, 28, 32].
Private SDG methods perform well in centralized settings where
a trusted curator holds all the data. However, in many settings, data
cannot be easily centralized. Instead, there are multiple participants
each holding a small private dataset who wish to generate synthetic
data. Federated learning (FL) is a paradigm that applies when mul-
tiple parties wish to collaboratively train a model without sharing
data directly [ 23]. In FL, local data remains on-device, and only
model updates are transmitted back to a central aggregator [ 35].
FL methods commonly adopt differential privacy to provide formal
privacy guarantees and is widely used in deep learning [ 20,22,34].
However, there has been minimal focus on federated SDG: we only
identify a recent effort of Pereira et al. to distribute Multiplicative
Weights with Exponential Mechanism (MWEM) via secure multi-
party computation (SMC) [ 42]. Their work focuses on a distributed
setting which assumes a small number of participants are allavail-
able to secret-share data before the protocol begins. This is not
suited for the fully federated setting where there may be thousands
of clients and only a small proportion available at a particular round.
In this work, we study generating differentially private tabular
data in the federated setting where only a small proportion of clients
are available per-round who exhibit strong data heterogeneity. We
propose FLAIM, a novel federated analogue to the current SOTA
central DP algorithm AIM [ 32]. We show how an analog to tradi-
tional FL training can be formed with clients performing a number
of local steps before sending model updates to the server in the
form of noisy marginals. We highlight how this naive extension can
suffer severely under strong heterogeneity which is exacerbated
 
2165
KDD ’24, August 25–29, 2024, Barcelona, Spain Samuel Maddock, Graham Cormode, and Carsten Maple
1 2 3 4 5
Heterogeneity Parameter ( β)0.00.20.40.60.81.01.21.41.6Average Workload Error
AIM
AugFLAIM (Private)DistAIM
NaiveFLAIM
Figure 1: Average error over a workload of marginals for
(FL)AIM trained with 𝜀=1on a toy federated dataset. 𝛽
varies client feature skew where large 𝛽results in less skew.
when only a few clients participate per round. To circumvent this,
we modify FLAIM by replacing components of central AIM with
newly-built steps that are better suited to the federated setting, such
as augmenting clients’ local choices via a private proxy of skew to
ensure decisions are not adversely affected by heterogeneity.
Example. Figure 1 presents a federated scenario where 10%of 100
clients participate per round. Each client holds data with varying
degrees of feature skew, where a larger 𝛽implies less heterogeneity.
We use four variations: Centralised AIM (black); Distributed AIM,
our adaptation of Pereira et al . [42] (purple); our naive federated
AIM approach (red); and our improved federated version (green). We
plot the𝐿1error over a workload of marginal queries trained with
𝜀=1. Due to client availability, there is an inevitable utility gap
between central and distributed AIM. By naively federating AIM, client
decisions made in local training are strongly affected by heterogeneity
while distributed AIM is not, resulting in a big loss in utility. This gap
is almost closed in high skew scenarios (small 𝛽) by penalising clients’
local decisions via a private measure of heterogeneity (AugFLAIM).
Our main contributions are as follows:
•We are the first to study marginal-based methods in the fed-
erated setting. We extend the work of Pereira et al. [ 42] who
focus on a strongly synchronized distributed setting with
MWEM to instead form a distributed protocol that replaces
MWEM with AIM to obtain greater utility (DistAIM).
•Motivated to reduce the overheads present in DistAIM, we
propose FLAIM, our federated analogue of AIM [32] that is
designed specifically for the federated setting. We propose
novel extensions based on augmenting utility scores in AIM
decisions via a private proxy that reduces the effect hetero-
geneity has on local decisions, resulting in increased model
performance and smaller overheads.•We show empirically our FLAIM method outperforms feder-
ated deep learning approaches such as DP-CTGAN, which
extends conclusions of prior studies to the federated setting.
•We perform an extensive empirical study on 7realistic tabu-
lar datasets. We show FLAIM obtains utility matching Dis-
tAIM but reduces the need for heavyweight SMC, resulting in
less overhead. Furthermore, we show our FLAIM approaches
are resistant to varying levels of heterogeneity1.
2 PRELIMINARIES
We assume the existence of 𝐾participants each holding local datasets
𝐷1,...,𝐷𝐾over a set of 𝑑attributes such that the full dataset is
denoted𝐷:=∪𝑘𝐷𝑘. Additionally, we assume that each attribute is
categorical2. For a record 𝒙:=(𝑥1,...,𝑥𝑑)∈𝐷we denote𝑥𝑖as the
value of attribute 𝑖. For each attribute 𝑖∈[𝑑], we define𝐴𝑖as the set
of discrete values that 𝑥𝑖can take. For a subset of attributes 𝑞⊆[𝑑]
we abuse notation and let 𝑥𝑞be the subset of 𝒙with attributes in the
set𝑞. We are mostly concerned with computing marginal queries
over𝐷(or individual 𝐷𝑘). Let𝑞⊆[𝑑]and define𝐴𝑞:=Î
𝑖∈𝑞𝐴𝑖,
as the set of values 𝑞can take and 𝑛𝑞:=|𝐴𝑞|as the cardinality of 𝑞.
Definition 2.1 (Marginal Query). A marginal query for a
subset of features 𝑞⊆ [𝑑]is a function 𝑀𝑞:D → R𝑛𝑞where
each entry is a count of the form (𝑀𝑞(𝐷))𝑗:=Í
𝑥∈𝐷1[𝑥𝑞=𝑎𝑗],
∀𝑗∈[𝑛𝑞],𝑎𝑗∈𝐴𝑞.
As an example, consider a dataset with two features: unemploy-
ment and age where 𝐴1={0,1}and𝐴2={1,2,...,99}. The output
of the marginal query 𝑞={𝑢𝑛𝑒𝑚𝑝𝑙𝑜𝑦𝑚𝑒𝑛𝑡,𝑎𝑔𝑒 }is a vector where
an entry is a count of each record that satisfies a possible combi-
nation of feature values e.g., {𝑢𝑛𝑒𝑚𝑝 =0,𝑎𝑔𝑒=18}. The goal in
workload-based synthetic data generation is to generate a synthetic
dataset ˆ𝐷that minimises Err(𝐷,ˆ𝐷)over a given workload of (mar-
ginal) queries 𝑄. We follow existing work and study the average
workload error under the 𝐿1norm [32].
Definition 2.2 (Average Workload Error). Denote the work-
load𝑄={𝑞1,...,𝑞𝑚}as a set of marginal queries where each
𝑞⊆[𝑑]. The average workload error for synthetic dataset ˆ𝐷is defined
Err(𝐷,ˆ𝐷;𝑄):=1
|𝑄|Í
𝑞∈𝑄∥𝑀𝑞(𝐷)−𝑀𝑞(ˆ𝐷)∥1
We are interested in producing a synthetic dataset ˆ𝐷with marginals
close to that of 𝐷. However, in the federated setting it is often im-
possible to form the global dataset 𝐷:=∪𝑘𝐷𝑘due to privacy
restrictions or client availability. Instead the goal is to gather suf-
ficient information from local datasets 𝐷𝑘and train a model that
learns𝑀𝑞(𝐷). For any𝐷𝑘, the marginal query 𝑀𝑞(𝐷𝑘)and local
workload error Err(𝐷𝑘,ˆ𝐷)are defined analogously.
Differential Privacy (DP) [11] is a formal notion that guarantees
the output of an algorithm does not depend heavily on any individ-
ual. We seek to guarantee (𝜀,𝛿)-DP, where the parameter 𝜀is called
theprivacy budget and determines an upper bound on the privacy
leakage of the algorithm. The parameter 𝛿defines the probability
of failing to meet this, and is set very small. DP has many attrac-
tive properties including sequential composition, meaning that if
two algorithms are (𝜀1,𝛿1)-DP and(𝜀2,𝛿2)-DP respectively, then
1Our code is available at https://github.com/Samuel-Maddock/flaim
2We discretize continuous features via uniform binning, see Appendix B.1 for details.
 
2166FLAIM: AIM-based Synthetic Data Generation in the Federated Setting KDD ’24, August 25–29, 2024, Barcelona, Spain
their joint output on a specific dataset satisfies (𝜀1+𝜀2,𝛿1+𝛿2)-DP.
Tighter bounds are obtained via zero-Concentrated DP (zCDP) [ 7]:
Definition 2.3 ( 𝜌-zCDP). A mechanismMis𝜌-zCDP if for
any two neighbouring datasets 𝐷,𝐷′and all𝛼∈(1,∞)we have
𝐷𝛼(M(𝐷)|M(𝐷′)≤𝜌·𝛼, where𝐷𝛼is Renyi divergence of order 𝛼.
One can convert 𝜌-zCDP to obtain an (𝜀,𝛿)-DP guarantee. The
notion of “adjacent” datasets can lead to different privacy defini-
tions. We assume example-level privacy , which defines two datasets
𝐷,𝐷′to be adjacent if 𝐷′can be formed from the addition/removal
of a single row from 𝐷. To satisfy DP it is common to require
bounded sensitivity of the function we wish to privatize.
Definition 2.4 (Sensitivity). Let𝑓:D→R𝑑be a function
over a dataset. The 𝐿2sensitivity of 𝑓, denoted Δ2(𝑓), is defined as
Δ2(𝑓):=max𝐷∼𝐷′∥𝑓(𝐷)−𝑓(𝐷′)∥2, where𝐷∼𝐷′represents the
example-level relation between datasets. Similarly, Δ1(𝑓)is defined
with the𝐿1norm as Δ1(𝑓):=max𝐷∼𝐷′∥𝑓(𝐷)−𝑓(𝐷′)∥1.
We use two foundational DP methods that are core to many DP-SDG
algorithms, the Gaussian and Exponential mechanisms [12].
Definition 2.5 (Gaussian Mechanism). Let𝑓:D → R𝑑,
the Gaussian mechanism is defined as 𝐺𝑀(𝑓)=𝑓(𝐷)+Δ2(𝑓)·
N(0,𝜎2𝐼𝑑). The Gaussian mechanism satisfies1
2𝜎2-zCDP.
Definition 2.6 (Exponential Mechanism). Let𝑢(𝑞;·):D→
Rbe a utility function defined for all 𝑞∈𝑄. The exponential mecha-
nism releases 𝑞with probability P[M(𝐷)=𝑞]∝exp(𝜀
2Δ·𝑢(𝑞;𝐷)),
withΔ:=max𝑞Δ1(𝑢(𝑞;𝐷)). This satisfies𝜀2
8-zCDP.
Iterative Methods (Select-Measure-Generate). Recent methods
for private tabular data generation follow the “Select-Measure-
Generate” paradigm which is also the core focus of our work. These
are broadly known as iterative methods [ 28] and usually involve
training a graphical model via noisy marginals over a number of
steps. In this work, we focus on AIM [ 32], an extension of the classi-
cal MWEM algorithm [ 16], which replaces the multiplicative weight
update with a graphical model inference procedure called Private-
PGM [ 33]. PGM learns a Markov Random Field (MRF) and applies
post-processing optimisation to ensure consistency in the gener-
ated data. PGM can answer queries without directly generating
data from the model, thus avoiding additional sampling errors.
In outline, given a workload of queries 𝑄, AIM proceeds as fol-
lows (further details are in the full technical report):
(1)At each round 𝑡, via the exponential mechanism, select a
query𝑞∈𝑄that is worst-approximated by the current
synthetic dataset.
(2)Under the Gaussian mechanism measure the chosen mar-
ginal and update the graphical model via PGM.
(3)At any point, we can generate synthetic data via PGM that
best explains the observed measurements.
AIM begins round 𝑡by computing utility scores for each query
𝑞∈𝑄of the form,
𝑢(𝑞;𝐷)=𝑤𝑞·(∥𝑀𝑞(𝐷)−𝑀𝑞(ˆ𝐷(𝑡−1))∥1−√︃
2
𝜋·𝜎𝑡·𝑛𝑞),
where ˆ𝐷(𝑡−1)is the current PGM model. The core idea is to select
marginals that are high in error (first term) balanced with the ex-
pected error from measuring the query under Gaussian noise withvariance𝜎2
𝑡(second term). The utility scores are weighted by 𝑤𝑞:=Í
𝑟∈𝑄|𝑟∩𝑞|, which calculates the overlap of other marginals in the
workload with 𝑞. The sensitivity of the resulting exponential mech-
anism is Δ=max𝑞𝑤𝑞since measuring∥𝑀𝑞(𝐷)−𝑀𝑞(ˆ𝐷(𝑡−1))∥1
has sensitivity 1which is weighted by 𝑤𝑞. Once a query is selected
it is measured by the Gaussian mechanism with variance 𝜎2
𝑡and
sensitivity 1. An update to the model via PGM is then applied using
all observed measurements so far.
Towards Decentralized Synthetic Data. Given a set of 𝐾clients
with datasets 𝐷1,... 𝐷𝐾and workload 𝑄, the goal is to learn a
synthetic dataset ˆ𝐷that best approximates 𝐷:=∪𝑘𝐷𝑘over𝑄e.g.,
𝑀𝑞(ˆ𝐷)≈𝑀𝑞(𝐷),∀𝑞∈𝑄. However, computing statistics directly
from𝐷is not possible as each 𝐷𝑘is private. We make an important
distinction here between the highly-synchronized distributed and
loosely-coordinated federated settings. In the distributed setting,
all participants are available to collaboratively share 𝑀𝑞(𝐷𝑘)and
some central server(s) compute steps of AIM in a strongly syn-
chronized manner, with high communication overhead. This is the
original setting of Pereira et al. [ 42]. Instead we are mainly inter-
ested in the federated setting where we assume that participants
are more weakly engaged, and may become unavailable or dropout
at any moment. We model this by assuming that each participant
participates in the current round only with probability 𝑝. We also
assume each 𝐷𝑘exhibits heterogeneity which could manifest as
significant feature-skew or a varying number of samples. We detail
how we model heterogeneity in Section 5.
3 DISTRIBUTED AIM
Our first proposal, DistAIM, translates the AIM algorithm directly
into the federated setting by having computing servers jointly
calculate each step, attempting to mirror what would be computed
in the central setting. To do so, computing servers must collaborate
privately and securely, such that no one participant’s raw query
answers,𝑀𝑞(𝐷𝑘), are revealed. The “select” and “measure” steps
require direct access to private local datasets 𝐷𝑘, and hence we
need to implement distributed DP mechanisms for these steps. We
present an overview here with full details in Appendix A.1.
Pereira et al. [ 42], describe one such approach for MWEM. They
utilize various secure multi-party computation (SMC) primitives
based on secret-sharing [ 1]. However, a key difference is they as-
sume a distributed setting where allparticipants first secret-share
their workload answers to computing servers before the protocol
begins. These computing servers implement secure exponential and
Laplace mechanisms over shares of marginals via standard SMC op-
erations [ 24]. This is a key difference to our federated setting where
we assume partial participation of clients over multiple rounds.
Their approach also has two drawbacks: first, their cryptographic
solution incurs both a computation and communication overhead
which may be prohibitive in federated scenarios. Secondly, their
approach is based on MWEM which results in a significant loss
in utility. Furthermore, MWEM is memory-intensive and does not
scale to high-dimensional datasets.
Instead, we apply the framework of Pereira et al. [ 42] to AIM,
and adapt this for our federated setting. Compared to AIM and
Pereira et al., our DistAIM approach has important differences:
 
2167KDD ’24, August 25–29, 2024, Barcelona, Spain Samuel Maddock, Graham Cormode, and Carsten Maple
Client participation: At each round only a subset of participants
are available to join the AIM round. For simplicity, we assume
clients are sampled with probability 𝑝. In expectation, 𝑝𝐾clients
contribute their secret-shared workload answers e.g., {J𝑀𝑞(𝐷𝑘)K:
𝑞∈𝑄}to computing servers. This is an immediate difference with
the setting of Pereira et al. [ 42], where it is assumed all clients
are available to secret-share marginals before training. Instead, in
DistAIM the secret-shares from participants are aggregated across
rounds and the “select” and “measure” steps are carried out via
computing servers over the updated aggregate shares at a particular
round. Compared to the central setting, DistAIM incurs additional
error due to subsampling.
Select step: A key difficulty extending AIM (or MWEM) to a dis-
tributed setting is the use of the exponential mechanism. In order to
apply this, the utility scores 𝑢(𝑞;𝐷)must be calculated. Following
Protocol 2 of Pereira et al. [ 42], sampling from the exponential mech-
anism can be done over secret-shares of the marginal J𝑀𝑞(𝐷𝑘)K
since utility scores 𝑢(𝑞;𝐷)depend only linearly in 𝑀𝑞(𝐷𝑘).
Measure step: Once a marginal has been sampled, it must be
measured. Protocol 3 in Pereira et al. [ 42] proposes one way to
securely generate Laplace noise between computing servers. This
is then added to the aggregate sum of a secret-shared marginal. To
remain consistent with AIM, we use Gaussian noise instead.
Estimate step: Under the post-processing properties of DP the
computing server(s) are free to use the measured noisy marginals
with PGM to update the graphical model, as in the centralized case.
4 FLAIM: FL ANALOG FOR AIM
While DistAIM is one solution, it is not defined within the standard
federated paradigm where clients typically perform a number of
local steps before sending model updates to a server. Furthermore,
the SMC-based approach can have large overheads which is prohib-
itive for federated clients who have limited bandwidth (we explore
this in Section 5.3). This leads us to design an AIM approach that is
analogous to traditional Federated Learning (FL), where only light-
weight SMC is needed in the form of secure-aggregation (SecAgg)
[4]. In FL, the paradigm for training models is to do computation
on-device, having clients perform multiple local steps before send-
ing a model update. The server aggregates all client updates and
performs an update to the global model [ 35]. When combined with
DP, model updates are aggregated via SecAgg schemes and noise is
added either by a trusted server or in a distributed manner. In the
case of AIM, we denote our analogous FL approach as FLAIM. In
FLAIM, the selection step of AIM is performed locally by clients
(across multiple local training steps). Each clients’ chosen marginals
are then sent to a trusted server via SecAgg and noise is added.
In more detail, FLAIM is outlined in Algorithm 1. We present
three variations, with differences highlighted in color. Shared be-
tween all variations are the key differences with DistAIM displayed
inblue underline . First is NaiveFLAIM, a straightforward transla-
tion of AIM into the federated setting. In Section 4.1, we explain
the shortcomings of such an approach which stems from scenarios
where clients’ local data exhibits strong heterogeneity. Motivated
by this, Section 4.2 proposes AugFLAIM (Oracle) a variant of FLAIM
that assumes oracle access to a measure of skew which can be used
to augment local utility scores. This skew measure is non-privateAlgorithm 1 FLAIM
Input:𝐾participants with data 𝐷1,...,𝐷𝑘, sampling rate 𝑝, global
rounds𝑇, local rounds 𝑠, workload𝑄, privacy parameters (𝜀,𝛿)
1:foreach global round 𝑡=1...𝑇 do
2: Form𝑃𝑡by sampling each client 𝑘with probability 𝑝
3: foreach client𝑘∈𝑃𝑡do
4: foreach local step 𝑙=1...𝑠do
5: Filter workload 𝑄←𝑄\{|𝑞|=1 :𝑞∈𝑄}(Private)
6: Compute a heterogeneity measure for each 𝑞∈𝑄
˜𝜏𝑘(𝑞)← 
0 Naive
𝜏𝑘(𝑞):=∥𝑀𝑞(𝐷𝑘)−𝑀𝑞(𝐷)∥1 Oracle
1
|𝑞|Í
𝑗∈𝑞∥𝑀{𝑗}(𝐷𝑘)−𝑀{𝑗}(ˆ𝐷(𝑡−1))∥1Private
7: Select𝑞𝑡+𝑙∈𝑄 using Exp-
Mech with utility score(s) 𝑢(𝑞;𝐷𝑘) :=
𝑤𝑞
∥𝑀𝑞(𝐷𝑘)−𝑀𝑞(ˆ𝐷(𝑡−1)+𝑙)∥1−√︃
2
𝜋·𝜎(𝑡−1)+𝑙·𝑛𝑞−˜𝜏𝑘(𝑞)
8: Measure marginal ˜𝑀𝑞(𝐷𝑘):=𝑀𝑞(𝐷𝑘) +
N
0,𝜎2
(𝑡−1)+𝑙𝐼
9: Estimate new local model via PGM as
ˆ𝐷(𝑡−1)+𝑙
𝑘←arg min𝑝∈SÍ(𝑡−1)+𝑙
𝑖=11
𝜎𝑖∥𝑀𝑞𝑖(𝑝)−˜𝑀𝑞𝑖(𝐷𝑘)∥2
10: Share all 1-way marginals under SecAgg,
M1
𝑘←{(𝑡,𝑗, J𝑀{𝑗}(𝐷𝑘)K}𝑗∈[𝑑](Private only)
11: ComputeM𝑘←{(𝑡,𝑞𝑡+𝑙,J𝑀𝑞𝑡+𝑙(𝐷𝑘)K)}𝑙∈[𝑠]∪M1
𝑘
12: SendM𝑘to the server via SecAgg
13: Server updates measurement list M𝑡:=∪𝑘∈𝑃𝑡M𝑘
14: foreach unique 𝑞∈M𝑡aggregate marginals and add noise
˜𝑀𝑡𝑞:=Í
{𝑘:𝑀𝑞∈M 𝑘}J𝑀𝑞(𝐷𝑘)K+𝑁(0,𝜎2
𝑡)
15: foreach ˜𝑀𝑡𝑞compute𝛼𝑡𝑞:= 
1/𝜎𝑡, Naive
𝑁𝑡𝑞/𝜎𝑡,Oracle
˜𝑁𝑡𝑞/𝜎𝑡,Private
16: Server updates measurement list Mwith each(𝑡,𝑞, ˜𝑀𝑡𝑞,𝛼𝑡𝑞)
and updates the global model
ˆ𝐷𝑡←arg min𝑝∈SÍ
(𝑡,𝑞, ˜𝑀𝑡𝑞,𝛼𝑡𝑞)∈M𝛼𝑡𝑞∥𝑀𝑞(𝑝)− ˜𝑀𝑡𝑞∥2
and not obtainable in practice, but provides an idealized baseline.
Lastly, Section 4.3 introduces AugFLAIM (Private), which again aug-
ments local utility scores but with a private proxy of heterogeneity
alongside other heuristics to improve utility.
All FLAIM variants proceed by sampling clients to participate
in round𝑡. Each client performs a number of local steps 𝑠, which
consist of performing a local selection step using the exponential
mechanism, measuring the chosen marginal under local noise and
updating their local model via PGM. When each client finishes
local training, they send back each chosen query 𝑞alongside the
associated marginal J𝑀𝑞(𝐷𝑘)K, which are aggregated via secure-
aggregation and noise is added by the central server. Hence, local
training is done under local differential privacy (LDP) to not leak
any privacy between steps, whereas the resulting global update is
under a form of distributed DP where noise is added by the central
 
2168FLAIM: AIM-based Synthetic Data Generation in the Federated Setting KDD ’24, August 25–29, 2024, Barcelona, Spain
server to the securely-aggregated marginals. We assume all AIM
methods run for 𝑇global rounds. AIM can also set 𝑇adaptively
via budget annealing and we explore this in our experiments (see
Appendix B.4 for details).
4.1 NaiveFLAIM and Heterogeneous Data
NaiveFLAIM is our first attempt at a SDG in the federated setting,
by directly translating the AIM algorithm. However, in federated
settings, participants often exhibit strong heterogeneity in their
local datasets. That is, clients’ local datasets 𝐷𝑘can differ signif-
icantly from the global dataset 𝐷. Such heterogeneity will affect
AIM in both the “select” and “measure” steps. If 𝐷𝑘and𝐷are sig-
nificantly different then the local marginal 𝑀𝑞(𝐷𝑘)will differ from
the true marginal 𝑀𝑞(𝐷). We quantify heterogeneity for a client 𝑘
and query𝑞∈𝑄via the𝐿1distance:
𝜏𝑘(𝑞):=∥𝑀𝑞(𝐷𝑘)−𝑀𝑞(𝐷)∥1.
This can be viewed as a measure of query skew. In FLAIM, we
proceed by clients perform a number of local steps. The first stage
involves carrying out a local “select” step based on utility scores
of the form 𝑢(𝑞;𝐷𝑘)∝∥𝑀𝑞(𝐷𝑘)−𝑀𝑞(ˆ𝐷(𝑡−1))∥1. Suppose for a
particular client 𝑘there exists a query 𝑞∈𝑄such that𝑀𝑞(𝐷𝑘)
exhibits strong heterogeneity. If at step 𝑡the current model ˆ𝐷(𝑡−1)
is a good approximation of 𝐷, then it is probable that client 𝑘ends
up selecting any query that has high heterogeneity since 𝑢(𝑞;𝐷𝑘)∝
||𝑀𝑞(𝐷𝑘)−𝑀𝑞(ˆ𝐷(𝑡−1))||1≈||𝑀𝑞(𝐷𝑘)−𝑀𝑞(𝐷)||1=𝜏𝑘(𝑞). This
mismatch can harm model performance and is compounded by
having many clients select (multiple and possibly differing) skewed
marginals and so the model is updated in a way that drifts from 𝐷.
4.2 AugFLAIM (Oracle): Tackling Heterogeneity
The difficulty above arises as clients choose marginals via local
applications of the exponential mechanism with a score that does
not account for underlying skew. We have 𝑢(𝑞;𝐷𝑘)∝
∥𝑀𝑞(𝐷𝑘)−𝑀𝑞(ˆ𝐷)∥1≤∥𝑀𝑞(𝐷)−𝑀𝑞(ˆ𝐷)∥+∥𝑀𝑞(𝐷𝑘)−𝑀𝑞(𝐷)∥1
∝𝑢(𝑞;𝐷)+𝜏𝑘(𝑞)
To circumvent this, we should correct local utility scores by down-
weighting marginals based on 𝜏𝑘(𝑞)and modify utility scores as:
𝑢(𝑞;𝐷𝑘)∝∥𝑀𝑞(𝐷𝑘)−𝑀𝑞(ˆ𝐷)∥−𝜏𝑘(𝑞)
where𝜏𝑘(𝑞)is an exact 𝐿1measure of heterogeneity for client
𝑘at a marginal 𝑞. Unfortunately, measuring 𝜏𝑘(𝑞)under privacy
constraints is not feasible. That is, 𝜏𝑘(𝑞)depends directly on 𝑀𝑞(𝐷),
which is exactly what we are trying to learn via AIM. Still, we
introduce AugFLAIM (Oracle) as an idealized baseline to compare
with. AugFLAIM (Oracle) is a variation of FLAIM that assumes
oracle access to 𝜏𝑘(𝑞)and augments local utility scores as above.
4.3 AugFLAIM (Private): Heterogeneity Proxy
Since augmenting utility scores directly via 𝜏𝑘(𝑞)is not feasible,
we seek a proxy ˜𝜏𝑘(𝑞)that is reasonably correlated with 𝜏𝑘(𝑞)and
can be computed under privacy. This proxy measure can be used
to correct local utility scores, penalising queries via ˜𝜏𝑘(𝑞). This
helps ensure clients select queries that are not adversely affectedTable 1: Comparison of FLAIM approaches against baselines
for negative log-likelihood (NLL), 𝜀=5. Smaller NLL is better.
Method / Dataset Adult Credit Covtype
Fed DP-CTGAN 37.1 83.8 62.7
FedNaiveBayes 25.33 18.02 44.9
FLAIM (Random) 83.9 47.7 58.4
NaiveFLAIM 29.4 18 45.4
AugFLAIM (Private) 20.87 16.2 41.6
DP-CTGAN 28.6 27.6 45.9
AIM 19.2 15.57 40.92
by heterogeneity. We propose the following proxy
˜𝜏𝑘(𝑞):=1
|𝑞|Í
𝑗∈𝑞∥𝑀{𝑗}(𝐷𝑘)− ˜𝑀{𝑗}(𝐷)∥1
Instead of computing a measure for each 𝑞∈𝑄, we compute one
for each feature 𝑗∈[𝑑], where ˜𝑀{𝑗}(𝐷𝑘)is a noisy estimate of
the 1-way marginal for feature 𝑗. For a particular query 𝑞∈𝑄, we
average the skew of the associated features contained in 𝑞. Such
a˜𝜏𝑘(𝑞)relies only on estimating the distribution of each feature.
This estimate can be refined across multiple federated rounds as
each participant can measure 𝑀{𝑗}(𝐷𝑘)for each𝑗∈[𝑑]and have
the server sum and add noise (via SecAgg) to produce a new private
estimate ˜𝑀{𝑗}(𝐷)each round. We add two further enhancements:
1. Filtering and combining 1-way marginals (Line 10). As we
require clients to estimate all features at every round, we remove 1-
way marginals from the workload to prevent clients from measuring
the same marginal twice. All 1-way marginals that are estimated
for˜𝜏𝑘(𝑞)are fed back into PGM to improve the global model.
2. Weighting Marginals (Line 15). In PGM, measurements are
weighted by 𝛼=1/𝜎𝑡, so those that are measured with less noise
have more importance in the optimisation of model parameters.
Both AugFLAIM variations adopt an additional weighting scheme
that includes the total sample size that contributed to a particular
marginal𝑞at round𝑡,𝑁𝑡𝑞:=Í
{𝑘:𝑀𝑡𝑞∈M 𝑘}|𝐷𝑘|where the weight
becomes𝛼𝑡𝑞=𝑁𝑡𝑞/𝜎𝑡. This relies on knowing the number of samples
that are aggregated. In some cases, the size of datasets may be
deemed private. In such scenarios, it can be estimated from the
noisy marginal ˜𝑀𝑞by summing the counts to produce ˜𝑁𝑞.
The privacy guarantees of all FLAIM variations follow directly
from those of AIM. The use of a heterogeneity measure incurs
an additional sensitivity cost for the exponential mechanism and
AugFLAIM (Private) incurs an additional privacy cost as it measures
each of the𝑑features at every round. The following lemma captures
this. See Appendix A.2 for the full proof.
Lemma 4.1. For any number of global rounds 𝑇and local rounds 𝑠,
FLAIM satisfies(𝜀,𝛿)-DP , under Gaussian budget allocation 𝑟∈(0,1)
by computing 𝜌according to Lemma A.2, and setting
𝜎𝑡= 
√︃
𝑇𝑠+𝑑
2·𝑟·𝜌, Naive orOracle√︃
𝑇(𝑠+𝑑)
2·𝑟·𝜌,Private, 𝜀𝑡=√︂
8·(1−𝑟)·𝜌
𝑇𝑠
For AugFLAIM methods, the exponential mechanism is applied with
sensitivity Δ:=max𝑞2𝑤𝑞.
 
2169KDD ’24, August 25–29, 2024, Barcelona, Spain Samuel Maddock, Graham Cormode, and Carsten Maple
0 5 10 15 20
Global AIM Round ( T)0.350.400.450.500.550.600.65Average Workload Error
NaiveFLAIM
˜τk(q) only
˜τk(q) + ﬁlter + combineτk(q) only
τk(q) + ﬁlter + combine
(a) Adult
0 10 20 30 40 50 60
Global AIM Round ( T)0.050.100.150.200.250.300.35Average Workload Error
NaiveFLAIM
˜τk(q) only
˜τk(q) + ﬁlter + combineτk(q) only
τk(q) + ﬁlter + combine (b) Credit
0 10 20 30 40 50 60
Global AIM Round ( T)0.2000.2250.2500.2750.3000.3250.3500.375Average Workload Error
NaiveFLAIM
˜τk(q) only
˜τk(q) + ﬁlter + combineτk(q) only
τk(q) + ﬁlter + combine (c) Covtype
Figure 2: Ablation study, comparing utility for FLAIM variations that augment local utility scores, 𝜀=5,𝑇=10,𝑠=1,𝑝=0.1
5 EXPERIMENTAL EVALUATION
For our experiments, we utilize realistic benchmark tabular datasets
from the UCI repository [ 10]: Adult, Magic, Marketing and Covtype.
We further use datasets common for benchmarking synthetic data:
Census and Intrusion from the Synthetic Data Vault (SDV) [ 40]. We
also construct a toy dataset with feature-skew denoted SynthFS.
Full details on all datasets are contained in Appendix B.1.
We evaluate our methods in three ways: average workload error
(as defined in Section 2), average negative log-likelihood (evaluated
on test data) and the area under the curve (AUC) of a decision tree
model trained on synthetic data and evaluated on test data.
For all datasets, we simulate heterogeneity by forming non-IID
splits in one of two ways: The first is by performing dimensionality
reduction and clustering nearby points to form client partitions that
have strong feature-skew. We call this the “clustering” approach.
For experiments that require varying heterogeneity, we form splits
via an alternative label-skew method popularized by Li et al. [ 27].
This samples a label distribution 𝑝𝑐∈[0,1]𝐾for each class 𝑐from
a Dirichlet(𝛽)where larger 𝛽results in less heterogeneity. See Ap-
pendix B.2 for full details. In the following sections, all experiments
have𝐾=100clients with partitions formed from the clustering
approach unless stated otherwise. We train (FL)AIM models on a
fixed workload of 3-way marginal queries chosen at random with
|𝑄|=64and average results over 10independent runs. Further
plots on datasets besides Adult and additional experiments are
contained in the Appendix of the full technical report.3
5.1 Comparison with Existing Baselines
We begin with an experiment comparing AugFLAIM (Private) to
other federated baselines. One such SOTA approach is CTGAN
[53]. We utilise the DP-CTGAN implementation within OpenDP’s
smartnoise-sdk [ 39] to compare to AIM. For the federated setting
we train CTGAN using DP-FedSGD via FLSim [ 44]. For details on
hyperparameters see Appendix B.4. We further compare AugFLAIM
(Private) against two AIM baselines. FLAIM (Random) which takes
3The full version of this paper can be obtained at https://arxiv.org/abs/2310.03447FLAIM and randomly chooses a query 𝑞∈𝑄without utilising the
exponential mechanism. Instead, all privacy budget is spent on
the “Measure” step. The other is FedNaiveBayes which restricts the
workload𝑄to only 1-way marginals and is equivalent to training a
NaiveBayes model. In Table 1 we present the negative log-likelihood
(NLL) for models trained to an 𝜀=5across three datasets. Methods
achieving lowest NLL for a particular dataset are in bold.
For the central setting, AIM achieves better performance than
DP-CTGAN across each dataset. This confirms prior studies such
as [29] that show graphical model approaches achieve better utility
than deep learning methods for tabular data. For the federated set-
ting, we note FedNaiveBayes and FLAIM (Random) both perform
poorly in comparison to AugFLAIM (Private). This illustrates two
main points: utilising the exponential mechanism does result in a
substantial increase in utility (i.e., randomly choosing 𝑞∈𝑄is poor)
and that utilising a workload of 𝑘-way marginals with 𝑘>1gives
best utility (i.e., NaiveBayes is poor). Further note, AugFLAIM (Pri-
vate) has better utility than NaiveFLAIM which shows augmenting
utility scores in the Exponential mechanism does improve utility.
We explore this further in Section 5.3. Finally, Fed DP-CTGAN per-
forms very poorly compared to AugFLAIM. Even NaiveFLAIM and
occasionally FedNaiveBayes outperform it. There are further issues
for practitioners: first, Fed DP-CTGAN requires a large number
of hyperparameters to be tuned for best utility such as client and
server learning rates and the clipping norm. This is in contrast to
(FL)AIM methods that only have a single hyperparameter - the total
number of global rounds 𝑇. Secondly, CTGAN requires a large num-
ber of training epochs. In this experiment we train for 50 epochs
which is equivalent to 𝑇=500rounds whereas the FLAIM methods
achieve better utility in only 𝑇=10rounds. For these reasons, in
further experiments, we do not compare to federated CTGAN.
5.2 Ablation Study: Utility of AugFLAIM
To understand what determines the utility of AugFLAIM (Private)
we present an ablation study in Figure 2. Here we train FLAIM
models with 𝜀=5on Adult, Credit and Covtype whilst varying
 
2170FLAIM: AIM-based Synthetic Data Generation in the Federated Setting KDD ’24, August 25–29, 2024, Barcelona, Spain
100101102
Privacy Parameter ( /epsilon1)0.20.30.40.50.60.70.80.91.0Average Workload Error
AIM
AugFLAIM (Oracle)
AugFLAIM (Private)DistAIM
NaiveFLAIM
(a) Varying privacy budget (𝜀)
0 20 40 60 80 100
Global AIM Round ( T)0.20.40.60.81.01.2Average Workload Error
AIM
AugFLAIM (Oracle)
AugFLAIM (Private)DistAIM
NaiveFLAIM (b) Varying global rounds (𝑇)
0.0 0.1 0.2 0.3 0.4 0.5
Sample rate ( p)0.20.40.60.81.01.21.41.61.8Average Workload Error
AugFLAIM (Oracle)
AugFLAIM (Private)
DistAIMNaiveFLAIM
AIM (c) Varying client sample-rate (𝑝)
0.0 0.2 0.4 0.6 0.8 1.0
Heterogeneity Parameter ( β)0.20.40.60.81.01.2Average Workload Error
AugFLAIM (Oracle)
AugFLAIM (Private)
DistAIMNaiveFLAIM
AIM
(d) Varying𝛽in label-skew
0 5 10 15 20 25 30
Global AIM Round ( T)0.30.40.50.60.70.80.91.0Average Workload Error
AugFLAIM (Oracle) s=1
AugFLAIM (Oracle) s=4
AugFLAIM (Private) s=1
AugFLAIM (Private) s=4DistAIM s=1
NaiveFLAIM s=1
NaiveFLAIM s=4 (e) Varying𝑇,𝑠- Workload Error
0 5 10 15 20 25 30
Global AIM Round ( T)0.450.500.550.600.650.700.750.800.85Test AUC
AugFLAIM (Oracle) s=1
AugFLAIM (Oracle) s=4
AugFLAIM (Private) s=1
AugFLAIM (Private) s=4DistAIM s=1
NaiveFLAIM s=1
NaiveFLAIM s=4 (f) Varying𝑇,𝑠- Test AUC
Figure 3: Varying (FL)AIM Parameters on Adult; Unless otherwise stated 𝑇=10,𝑠=1,𝑝=0.1,𝐾=100,𝜀=1
the global rounds 𝑇. We present NaiveFLAIM compared with varia-
tions that augment the utility scores of the Exponential mechanism.
These are: using the true heterogeneity measure 𝜏𝑘(𝑞)only (other-
wise denoted AugFLAIM (Oracle)); using 𝜏𝑘(𝑞)with the filter-and-
combine heuristic; using the private heterogeneity proxy ˜𝜏𝑘(𝑞)only
and using ˜𝜏𝑘(𝑞)with the filter and combine heuristic (otherwise
denoted AugFLAIM (Private)). On the Credit dataset, using 𝜏𝑘(𝑞)
or˜𝜏𝑘(𝑞)only results in a clear improvement over NaiveFLAIM, and
when combined with the heuristics the lowest error is obtained.
On Adult and Covtype, using only 𝜏𝑘(𝑞)or the private proxy ˜𝜏𝑘(𝑞)
does not immediately result in lower workload error than Naive-
FLAIM. Instead, utilising the filter and combine heuristics results in
the best workload error overall. In further experiments, we denote
AugFLAIM (Private) as the method which augments utility scores
with ˜𝜏𝑘(𝑞)and uses the filter and combine heuristic whereas we
denote AugFLAIM (Oracle) as the method that has oracle access to
𝜏𝑘(𝑞)only (without further heuristics).5.3 Parameter Settings
Having concluded that AugFLAIM (Private) achieves the best per-
formance against other federated baselines, we now present a de-
tailed set of experiments comparing FLAIM methods with DistAIM
across a variety of federated settings. We compare AIM and Dis-
tAIM against NaiveFLAIM and our two variants that augment local
utility scores: AugFLAIM (Oracle) using 𝜏𝑘(𝑞)only and AugFLAIM
(Private) using proxy ˜𝜏𝑘(𝑞)with filtering and combining 1-ways.
Varying the privacy budget (𝜀).In Figure 3a, we plot the work-
load error whilst varying 𝜀on Adult, sampling 10%of clients per
round and setting 𝑇=10. First, we observe a clear gap in perfor-
mance between DistAIM and central AIM due to the error from
subsampling a small number of clients per round. We observe that
naively federating AIM gives the worst performance even as 𝜀
becomes large. Furthermore, augmenting utility scores makes a
clear improvement in workload error, particularly for 𝜀>1. By
estimating feature distributions at each round, AugFlaim (Private)
can obtain performance that matches or sometimes improves upon
DistAIM for larger values of 𝜀. We further note that AugFLAIM
(Private) has lower error than AugFLAIM (Oracle) which may seem
 
2171KDD ’24, August 25–29, 2024, Barcelona, Spain Samuel Maddock, Graham Cormode, and Carsten Maple
counter-intuitive. However, AugFLAIM (Oracle) is still trained un-
der DP, only oracle access to 𝜏𝑘(𝑞)is assumed which is non-private
and trained without heuristics as explored in Section 5.2.
Varying the number of global AIM rounds (𝑇).In Figure 3b, we
vary the number of global AIM rounds and fix 𝜀=1. Additionally,
we plot the setting where 𝑇is chosen adaptively by budget anneal-
ing. This is shown in dashed lines for each method. First observe
with DistAIM, the workload error decreases as 𝑇increases. Since
computing servers aggregate secret-shares across rounds, then as 𝑇
grows large, most clients will have been sampled and the server(s)
have workload answers over most of the (central) dataset. For all
FLAIM variations, the workload error usually increases when 𝑇is
large, since they are more sensitive to the increased amount of noise
that is added. For NaiveFLAIM, this is worsened by client hetero-
geneity. Further, we observe that for AugFLAIM (Private), the utility
matches that of DistAIM unless the choice of 𝑇is very large. At
𝑇=100, the variance in utility is high, sometimes even worse than
that of NaiveFLAIM. This is since the privacy cost scales in both the
number of rounds and features, resulting in too much noise. In the
case of annealing, 𝑇is chosen adaptively by an early stopping con-
dition (see Appendix B.4). While annealing has good performance
in central AIM, it obtains poor utility across all federated methods.
For annealing on Adult, AugFLAIM (Private) matches AugFLAIM
(Oracle) and both perform better than NaiveFLAIM. Overall, we
found choosing 𝑇to be small(≤30)gives best performance for
AugFLAIM and should avoid using budget annealing.
Client-participation (𝑝).In Figure 3c, we plot the average work-
load error whilst varying the per-round participation rate ( 𝑝) with
𝑇=10,𝜀=1. We observe clearly the gap in performance between
central AIM and DistAIM is caused by the error introduced by sub-
sampling and when 𝑝≥0.5performance is almost matched. For
NaiveFLAIM, we observe the performance improvement as 𝑝in-
creases is slower than other methods. When 𝑝is large, NaiveFLAIM
receives many measurements, each likely to be highly heteroge-
neous and thus the model struggles to learn consistently. For both
AugFLAIM variations, we observe the utility improves with client
participation but does eventually plateau. AugFLAIM (Private) con-
sistently matches the error of DistAIM except when 𝑝is large, but
we note this is not a practical regime in FL.
Varying heterogeneity ( 𝛽).In Figure 3d, we plot the average work-
load error on the Adult dataset over client splits formed by varying
the heterogeneity parameter (𝛽)to produce label-skew. Here, a
larger𝛽corresponds to a more uniform partition and therefore less
heterogeneity. In the label-skew setting, data is both skewed accord-
ing to the class attribute of Adult and the number of samples, with
only a few clients holding the majority of the dataset. We observe
that when the skew is large ( 𝛽<0.1), all methods struggle. As 𝛽
increases and skew decreases, NaiveFLAIM performs the worst and
AugFLAIM (Private) has stable error, close to that of DistAIM.
Varying local rounds ( 𝑠).A benefit of the federated setting is
that clients can perform a number of local steps before sending all
measured marginals to the server. However, for FLAIM methods,
this incurs an extra privacy cost in the number of local rounds
(𝑠). In Figure 3e, we vary 𝑠∈{1,4}and plot the workload error.
Although there is an associated privacy cost with increasing 𝑠, the
errors are not significantly different for small 𝑇. As we vary 𝑇,
the associated privacy cost becomes larger and the workload errorincreases for methods that perform 𝑠=4local updates. Although
increasing the number of local rounds (𝑠)does not result in lower
workload error, and in cases where 𝑇is misspecified can give far
worse performance, it is instructive to instead study the test AUC
of a classification model trained on the synthetic data. In Figure 3f
we see that performing more local updates can give better test AUC
after fewer global rounds. For AugFLAIM (Private), this allows us
to match the test AUC performance of DistAIM on Adult.
Comparison across datasets. Table 2 presents results across all
datasets with client data partitioned via the clustering approach.
We set𝜀=1,𝑝=0.1and𝑇=10. For each method we present both
the average workload error and the negative log-likelihood over a
holdout set. The first is a form of training error and the second a
measure of generalisation. We observe that on 5of the 7datasets
AugFLAIM (Private) achieves the lowest negative log-likelihood
and workload error. On the other datasets, AugFLAIM (private)
closely matches DistAIM in utility but with lower overheads.
Distributed vs. Federated AIM. Table 3 presents the overhead of
DistAIM compared to AugFLAIM (Private) including average client
throughput (sent and received communication) across protocols.
We set𝑇∈[1,200]that achieves lowest workload error. Observe on
Adult, DistAIM requires twice as many rounds to achieve optimal
error and results in a large (1300 ×) increase in client throughput
compared to AugFLAIM. However, this results in 2×lower work-
load error and an 11%improvement in NLL. This highlights one
of the chief advantages of FLAIM, where, for a small loss in utility,
we can obtain much lower overheads. Furthermore, while a 2×gap
in workload error seems significant, we refer back to Figure 3f,
which shows the resulting classifier has AUC that is practical for
downstream tasks. We note the overhead of DistAIM is significantly
larger than FLAIM when queries in the workload have large car-
dinality (e.g., on Adult and Magic). Datasets with much smaller
feature domains still have communication overhead but it is not as
significant (e.g., Covtype which has many binary features).
6 RELATED WORK
Synthetic data has gained substantial traction due to its potential
to mitigate privacy concerns and address limitations for sharing
real-world data. Many generative deep learning approaches exist
including GANs [ 15], VAEs [ 51] and diffusion models [ 18]. Recent
work has extended these synthetic data generators (SDGs) to satisfy
central differential privacy (DP) [ 30,48,52] and whilst results are
promising for image data, performance on tabular data remains
limited. Only a few generative tabular approaches exist including
that of CTGAN [ 13,53]. However, recent work has shown that
private tabular approaches like DP-CTGAN often fail to provide
good utility when compared to simpler models [ 14,47]. Indeed, in
the central setting of DP many successful methods are based on
graphical models such as PrivBayes [ 56], PrivSyn [ 57], PGM [ 33]
and AIM [ 32]. Recent work has shown the class of iterative methods
[28] are SOTA on tabular data and we choose to focus on one of
these methods, AIM, in our work. Meanwhile, research into SDGs
in the federated setting remains limited. Recent federated SDGs are
focused on image data such as MD-GAN [ 17], FedGAN [ 43] and
FedVAE [ 54]. We do not compare with these in our work as they do
not support tabular data or DP. The closest work to ours is that of
 
2172FLAIM: AIM-based Synthetic Data Generation in the Federated Setting KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: Performance on datasets 𝐾=100,𝑝=0.1,𝜀=1,𝑇=10. Results show workload error and negative log-likelihood. Metrics
are bold if a federated method achieves lowest on a specific dataset.
Method / Dataset Adult Magic Census Covtype Credit Intrusion Marketing
NaiveFLAIM 0.8 / 29.28 1.64 / 2587.5 0.72 / 322.29 0.3 / 47.44 0.42 / 20.22 0.7 / 27.92 1.05 / 186.44
AugFLAIM (Oracle) 0.62 / 23.91 1.18 / 62.84 0.61 / 44.5 0.26 / 45.11 0.21 / 16.84 0.48 / 17.92 0.87 / 38.27
AugFLAIM (Private) 0.43 / 21.74 1.07 /28.9 0.48 /41.87 0.17 /42.61 0.14 /16.33 0.32 / 15.05 0.64 /30.17
DistAIM 0.42 /21.41 1.08 / 35.04 0.54 / 55.19 0.2 / 45.94 0.18 / 16.9 0.32 /13.19 0.66 / 40.57
AIM 0.2 / 19.3 0.85 / 23.7 0.28 / 34.26 0.06 / 41.06 0.07 / 15.62 0.12 / 9.96 0.24 / 20.87
Table 3: DistAIM vs. FLAIM at optimal 𝑇, metrics with↑show
overhead of DistAIM and ↓show %improvement of DistAIM.
Client throughput is additionally stated in megabytes (MB).
Dataset 𝑇(↑) Throughput (↑)Err(↓) NLL (↓)
Adult 2× 1300× (80/ 0.06) 58% 11%
Magic 3.2× 1643× (80 / 0.04) 20% 14%
Census 1.5x 64x (29.6 / 0.46) 79% 33%
Intrusion 2.5x 366x (101 / 0.28) 82% 52%
Marketing 2.0x 97x (18 / 0.19) 77% 35%
Credit 1.0x 167x (93 / 0.55) 45% 6%
Covtype 1.25x 10x (7.6 / 0.76) 64% 3%
Pereira et al. [ 42] who propose a distributed DP version of MWEM
[16] using secure multiparty computation (SMC) to distribute noise
generation across computing servers. This approach has two main
drawbacks: it assumes all clients are available to secret-share their
workload answers and as it is based on MWEM, obtains subpar
utility. Our work is motivated to extend their approach to AIM
and to study an alternative and more natural federation of these
methods. We also note the concurrent work of Pentyala et al. [ 41]
which extends [42] to work with AIM via SMC.
7 CONCLUSION
Overall, we have shown that naively federating AIM under the chal-
lenges of FL causes a large decrease in utility when compared to the
SMC-based DistAIM. To counteract this, we propose AugFLAIM
(Private), which augments local decisions with a proxy for het-
erogeneity and obtains utility close to DistAIM while lowering
overheads. In the future, we plan to extend our approaches to sup-
port user-level DP where clients hold multiple data items related to
the same individual.
ACKNOWLEDGMENTS
Work performed at Warwick University is supported by the UKRI
Engineering and Physical Sciences Research Council (EPSRC) un-
der grant EP/W523793/1; the UKRI Prosperity Partnership Scheme
(FAIR) under EPSRC grant EP/V056883/1; and the UK NCSC Aca-
demic Centre of Excellence in Cybersecurity Research (ACE-CSR).
REFERENCES
[1]Toshinori Araki, Jun Furukawa, Yehuda Lindell, Ariel Nof, and Kazuma Ohara.
2016. High-throughput semi-honest secure three-party computation with an
honest majority. In ACM SIGSAC CCS. Vienna, 805–817.[2]Samuel A Assefa, Danial Dervovic, Mahmoud Mahfouz, Robert E Tillman,
Prashant Reddy, and Manuela Veloso. 2020. Generating synthetic data in fi-
nance: opportunities, challenges and pitfalls. In Proceedings of the First ACM
International Conference on AI in Finance. ACM, New York, 1–8.
[3]Sergul Aydore, William Brown, Michael Kearns, Krishnaram Kenthapadi, Luca
Melis, Aaron Roth, and Ankit A Siva. 2021. Differentially private query release
through adaptive projection. In ICML. PMLR, 457–467.
[4]James Henry Bell, Kallista A Bonawitz, Adrià Gascón, Tancrède Lepoint, and
Mariana Raykova. 2020. Secure single-server aggregation with (poly) logarithmic
overhead. In ACM SIGSAC CCS. Online, 1253–1269.
[5]Jock Blackard. 1998. Covertype. UCI Machine Learning Repository. DOI:
https://doi.org/10.24432/C50K5N.
[6]R. Bock. 2007. MAGIC Gamma Telescope. UCI Machine Learning Repository.
DOI: https://doi.org/10.24432/C52C8B.
[7]Mark Bun and Thomas Steinke. 2016. Concentrated differential privacy: Simpli-
fications, extensions, and lower bounds. In Theory of Cryptography Conference.
Springer, Springer, Berlin, 635–658.
[8]Clément L Canonne, Gautam Kamath, and Thomas Steinke. 2020. The discrete
gaussian for differential privacy. Advances in Neural Information Processing
Systems 33 (2020), 15676–15688.
[9]DARPA. 1999. Darpa intrusion detection evaluation,. https://www.ll.mit.edu/r-
d/datasets/1999-darpa-intrusion-detection-evaluation-dataset
[10] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http:
//archive.ics.uci.edu/ml
[11] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Cali-
brating noise to sensitivity in private data analysis. In Theory of cryptography
conference. Springer, Springer, 265–284.
[12] Cynthia Dwork, Aaron Roth, et al .2014. The algorithmic foundations of differ-
ential privacy. Foundations and Trends® in Theoretical Computer Science 9, 3–4
(2014), 211–407.
[13] Mei Ling Fang, Devendra Singh Dhami, and Kristian Kersting. 2022. Dp-ctgan:
Differentially private medical data generation using ctgans. In International
Conference on Artificial Intelligence in Medicine. Springer, 178–188.
[14] Georgi Ganev, Kai Xu, and Emiliano De Cristofaro. 2023. Understanding
how Differentially Private Generative Models Spend their Privacy Budget.
arXiv:2305.10994 [cs.LG]
[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial
networks. Commun. ACM 63, 11 (2020), 139–144.
[16] Moritz Hardt, Katrina Ligett, and Frank McSherry. 2012. A simple and practical
algorithm for differentially private data release. Advances in neural information
processing systems 25 (2012).
[17] Corentin Hardy, Erwan Le Merrer, and Bruno Sericola. 2019. Md-gan: Multi-
discriminator generative adversarial networks for distributed datasets. In 2019
IEEE international parallel and distributed processing symposium (IPDPS). IEEE,
866–877.
[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in neural information processing systems 33 (2020), 6840–6851.
[19] Florimond Houssiau, James Jordon, Samuel N Cohen, Owen Daniel, Andrew
Elliott, James Geddes, Callum Mole, Camila Rangel-Smith, and Lukasz Szpruch.
2022. TAPAS: a toolbox for adversarial privacy auditing of synthetic data. NeurIPS
Workshop on Synthetic Data for Empowering ML Research (2022).
[20] Dzmitry Huba, John Nguyen, Kshitiz Malik, Ruiyu Zhu, Mike Rabbat, Ashkan
Yousefpour, Carole-Jean Wu, Hongyuan Zhan, Pavel Ustinov, Harish Srinivas,
et al.2022. Papaya: Practical, private, and scalable federated learning. Proceedings
of Machine Learning and Systems 4 (2022), 814–832.
[21] Kaggle. 2017. Credit card fraud dataset. https://www.kaggle.com/datasets/mlg-
ulb/creditcardfraud
[22] Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep
Thakurta, and Zheng Xu. 2021. Practical and private (deep) learning without
sampling or shuffling. In ICML. PMLR, 5213–5225.
 
2173KDD ’24, August 25–29, 2024, Barcelona, Spain Samuel Maddock, Graham Cormode, and Carsten Maple
[23] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El Rouayheb,
David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B.
Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo,
Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Kho-
dak, Jakub Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer
Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar,
Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha
Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng
Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. 2019. Advances and Open
Problems in Federated Learning. arXiv:1912.04977 [cs.LG]
[24] Marcel Keller. 2020. MP-SPDZ: A versatile framework for multi-party computa-
tion. In ACM SIGSAC CCS. 1575–1590.
[25] Diederik P Kingma, Max Welling, et al .2019. An introduction to variational
autoencoders. Foundations and Trends® in Machine Learning 12, 4 (2019), 307–392.
[26] Ronny Kohavi and Barry Becker. 1996. Adult dataset. UCI Machine Learning
Repository. http://archive.ics.uci.edu/ml/nomao
[27] Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. 2022. Federated learning
on non-iid data silos: An experimental study. In IEEE ICDE. 965–978.
[28] Terrance Liu, Giuseppe Vietri, and Steven Z Wu. 2021. Iterative methods for
private synthetic data: Unifying framework and new methods. Advances in
Neural Information Processing Systems 34 (2021), 690–702.
[29] Yucong Liu, Chi-Hua Wang, and Guang Cheng. 2022. On the Utility Recovery
Incapability of Neural Net-based Differential Private Tabular Training Data
Synthesizer under Privacy Deregulation. arXiv:2211.15809 [cs.LG]
[30] Saiyue Lyu, Michael F Liu, Margarita Vinaroz, and Mijung Park. 2023. Differen-
tially private latent diffusion models. arXiv preprint arXiv:2305.15759 (2023).
[31] Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform man-
ifold approximation and projection for dimension reduction. arXiv preprint
arXiv:1802.03426 (2018).
[32] Ryan McKenna, Brett Mullins, Daniel Sheldon, and Gerome Miklau. 2022. AIM:
An Adaptive and Iterative Mechanism for Differentially Private Synthetic Data.
arXiv preprint arXiv:2201.12677 (2022).
[33] Ryan McKenna, Daniel Sheldon, and Gerome Miklau. 2019. Graphical-model
based estimation and inference for differential privacy. In ICML. PMLR, 4435–
4444.
[34] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial intelligence and statistics . PMLR,
PMLR, 1273–1282.
[35] Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2017. Learning
differentially private recurrent language models. arXiv preprint arXiv:1710.06963
(2017).
[36] Ofer Mendelevitch and Michael D Lesh. 2021. Fidelity and privacy of synthetic
medical data. arXiv preprint arXiv:2101.08658 (2021).
[37] Sérgio Moro, Paulo Cortez, and Paulo Rita. 2014. A data-driven approach to
predict the success of bank telemarketing. Decision Support Systems 62 (2014),
22–31.
[38] Sasi Kumar Murakonda, Reza Shokri, and George Theodorakopoulos. 2021. Quan-
tifying the privacy risks of learning high-dimensional graphical models. In AIS-
TATS. PMLR, PMLR, 2287–2295.
[39] OpenDP. 2021. smartnoise-sdk. https://github.com/opendp/smartnoise-sdk
[40] Neha Patki, Roy Wedge, and Kalyan Veeramachaneni. 2016. The Synthetic data
vault. In IEEE International Conference on Data Science and Advanced Analytics
(DSAA). IEEE, Canada, 399–410. https://doi.org/10.1109/DSAA.2016.49
[41] Sikha Pentyala, Mayana Pereira, and Martine De Cock. 2024. CaPS: Collaborative
and Private Synthetic Data Generation from Distributed Sources. arXiv preprint
arXiv:2402.08614 (2024).
[42] Mayana Pereira, Sikha Pentyala, Anderson C. A. Nascimento, Rafael T. de Sousa Jr.,
and Martine De Cock. 2022. Secure Multiparty Computation for Synthetic Data
Generation from Distributed Data. CoRR abs/2210.07332 (2022). https://doi.org/
10.48550/ARXIV.2210.07332 arXiv:2210.07332
[43] Mohammad Rasouli, Tao Sun, and Ram Rajagopal. 2020. Fedgan: Federated gen-
erative adversarial networks for distributed data. arXiv preprint arXiv:2006.07228
(2020).
[44] Facebook Research. 2021. FLSim. https://github.com/facebookresearch/FLSim
[45] Akash Srivastava, Lazar Valkov, Chris Russell, Michael U Gutmann, and Charles
Sutton. 2017. Veegan: Reducing mode collapse in GANs using implicit variational
learning. Advances in neural information processing systems 30 (2017).
[46] Theresa Stadler, Bristena Oprisanu, and Carmela Troncoso. 2022. Synthetic data–
anonymisation groundhog day. In 31st USENIX Security Symposium (USENIX
Security 22). USENIX, Vancouver, 1451–1468.
[47] Yuchao Tao, Ryan McKenna, Michael Hay, Ashwin Machanavajjhala, and Gerome
Miklau. 2022. Benchmarking differentially private synthetic data generation
algorithms. In Workshop on Privacy-Preserving Artificial Intelligence, AAAI 2022.
AAAI, Vancouver.[48] Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. 2019. Dp-cgan:
Differentially private synthetic data and label generation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 0–0.
[49] Boris van Breugel, Hao Sun, Zhaozhi Qian, and Mihaela van der Schaar. 2023.
Membership Inference Attacks against Synthetic Data through Overfitting De-
tection. arXiv preprint arXiv:2302.12580 (2023).
[50] Boris van Breugel and Mihaela van der Schaar. 2023. Beyond Privacy: Nav-
igating the Opportunities and Challenges of Synthetic Data. arXiv preprint
arXiv:2304.03722 (2023).
[51] Zhiqiang Wan, Yazhou Zhang, and Haibo He. 2017. Variational autoencoder
based synthetic data generation for imbalanced learning. In 2017 IEEE symposium
series on computational intelligence (SSCI). IEEE, 1–7.
[52] Benjamin Weggenmann, Valentin Rublack, Michael Andrejczuk, Justus Mattern,
and Florian Kerschbaum. 2022. DP-VAE: Human-readable text anonymization for
online reviews with differentially private variational autoencoders. In Proceedings
of the ACM Web Conference 2022. 721–731.
[53] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni.
2019. Modeling tabular data using conditional gan. Advances in neural information
processing systems 32 (2019).
[54] Haomiao Yang, Mengyu Ge, Kunlan Xiang, Xuejun Bai, and Hongwei Li. 2023.
FedVAE: Communication-Efficient Federated Learning With Non-IID Private
Data. IEEE Systems Journal (2023).
[55] Jim Young, Patrick Graham, and Richard Penny. 2009. Using Bayesian networks
to create synthetic data. Journal of Official Statistics 25, 4 (2009), 549.
[56] Jun Zhang, Graham Cormode, Cecilia M Procopiuc, Divesh Srivastava, and Xi-
aokui Xiao. 2017. Privbayes: Private data release via bayesian networks. ACM
Transactions on Database Systems (TODS) 42, 4 (2017), 1–41.
[57] Zhikun Zhang, Tianhao Wang, Jean Honorio, Ninghui Li, Michael Backes, Shibo
He, Jiming Chen, and Yang Zhang. 2021. Privsyn: Differentially private data
synthesis. (2021).
A ALGORITHM DETAILS
A.1 DistAIM
Algorithm 2 DistAIM
Input: Participants 𝑃1,...𝑃𝑘with local datasets 𝐷1,...,𝐷𝑘, pri-
vacy parameters(𝜀,𝛿)
1:Initialise parameters as in central AIM
2:foreach round 𝑡do
3: Sample participants 𝑃𝑡⊆[𝑘]with probability 𝑝and remove
those who have already participated
4: For each𝑘∈𝑃𝑡who have not participated before, secret-
share the workload answers {J𝑀𝑞(𝐷𝑘)K:𝑞∈𝑊}to computing
servers [1]
5: Aggregate: The computing servers aggregate shares of the
received answers and combine with previously received shares
J𝑀𝑞(˜𝐷𝑡)K:=Í𝑡−1
𝑖=1Í
𝑘∈𝑃𝑖J𝑀𝑞(𝐷𝑘)K+Í
𝑘∈𝑃𝑡J𝑀𝑞(𝐷𝑘)K
6: Select: Computing servers select 𝑞𝑡∈𝑄using the expo-
nential mechanism over secret shares J𝑀𝑞(˜𝐷𝑡)Kvia Protocol 2
in [42] with AIM utility scores
𝑢(𝑞;𝐷𝑡):=𝑤𝑞·(∥𝑀𝑞(˜𝐷𝑡)−𝑀𝑞(ˆ𝐷𝑡−1)∥1−√︂
2
𝜋·𝜎𝑡·𝑛𝑞)
7: Measure:𝑞𝑡is measured using J𝑀𝑞𝑡(˜𝐷𝑡)Kunder a varia-
tion of Protocol 3 in [ 42], replacing Laplace noise with Gaussian
to produce ˜𝑀𝑞𝑡(˜𝐷𝑡)
8: Estimate the new model via PGM using the received noisy
measurements e.g.
ˆ𝐷𝑡←arg min
𝑝∈S𝑡∑︁
𝑖=11
𝜎𝑖∥𝑀𝑞𝑖(𝑝)− ˜𝑀𝑞𝑖(˜𝐷𝑖)∥2
 
2174FLAIM: AIM-based Synthetic Data Generation in the Federated Setting KDD ’24, August 25–29, 2024, Barcelona, Spain
We describe in full detail the DistAIM algorithm introduced
in Section 3 and outlined in Algorithm 2. The algorithm can be
seen as an adaptation of [ 42] who propose a secure multi-party
computation (SMC) approach for distributing MWEM. The key
differences are that we replace MWEM with AIM and consider a
distributed setting where not all participants are available at any
particular round. The approach relies on participants secret-sharing
their query answers to computing servers who then perform a
number of SMC operations over these shares to train the model.
The resulting algorithm is identical to AIM in outline but has a few
subtle differences:
•Secret Sharing: Participants must secret-share the required
quantities to train AIM. In [ 42], it is assumed that the full
workload answers {J𝑀𝑞(𝐷)K:𝑞∈𝑄}have already been
secret-shared between a number of computing servers. In
DistAIM, we assume that clients sampled to participate at
a particular round contribute their secret-shared workload
answers{J𝑀𝑞(𝐷𝑘)K:𝑞∈𝑄}which are aggregated with
the shares of current and past participants from previous
rounds. Thus, as the number of global rounds 𝑇increases, the
secret-shared answers approach that of the central dataset.
We assume the same SMC framework as [ 42] which is a
3-party scheme based on [1].
•Client participation: At each round only a subset of the par-
ticipants are available to join the AIM round. In expectation
𝑝𝐾clients will contribute their local marginals J𝑀𝑞(𝐷𝑘)Kin
the form of secret-shares. Compared to the central setting,
DistAIM incurs additional error due to this subsampling.
•Select step: One key obstacle in extending AIM to a dis-
tributed setting is the exponential mechanism. Since each
client holds a local dataset 𝐷𝑘, they cannot share their data
with the central server. Instead the quality functions 𝑢(𝑞;𝐷)
must be computed in a distributed manner between the com-
puting servers who hold shares of the workload answers.
•Measure step: Once the marginal 𝑞𝑡has been selected by
a secure exponential mechanism, it must be measured. As
[42] utilise MWEM, they measure queries under Laplace
noise which can be easily generated in an SMC setting. AIM
instead uses Gaussian noise and this is also what we use
in DistAIM. In practice, one can also implement this under
SMC e.g., using the Box-Muller method.
A.2 FLAIM: Privacy Guarantees
In this section, we present and prove the privacy guarantees of the
FLAIM approach. For completeness, we provide additional defini-
tions and results, starting with the definition of (𝜀,𝛿)-DP.
Definition A.1 (Differential Privacy [ 12]).A randomised
algorithmM:D→R satisfies(𝜀,𝛿)-differential privacy if for any
two adjacent datasets 𝐷,𝐷′∈D and any subset of outputs 𝑆⊆R,
P(M(𝐷)∈𝑆)≤𝑒𝜀P(M(𝐷′)∈𝑆)+𝛿.
While we work using the more convenient formulation of 𝜌-
zCDP (Definition 2.3), it is common to translate this guarantee to
the more interpretable (𝜀,𝛿)-DP setting via the following lemma.Lemma A.2 (zCDP to DP [ 8]).If a mechanismMsatisfies𝜌-zCDP
then it satisfies(𝜀,𝛿)-DP for all𝜀>0with
𝛿=min
𝛼>1exp((𝛼−1)(𝛼𝜌−𝜀))
𝛼−1
1−1
𝛼𝛼
We restate the privacy guarantees of FLAIM and its variations.
Lemma A.3 (Lemma 4.1 restated). For any number of global
rounds𝑇and local rounds 𝑠, FLAIM satisfies(𝜀,𝛿)-DP , under Gauss-
ian budget allocation 𝑟∈(0,1)by computing 𝜌according to Lemma
A.2, and setting
𝜎𝑡= 
√︃
𝑇𝑠+𝑑
2·𝑟·𝜌, Naive orOracle√︃
𝑇(𝑠+𝑑)
2·𝑟·𝜌,Private, 𝜀𝑡=√︂
8·(1−𝑟)·𝜌
𝑇𝑠
For AugFLAIM methods, the exponential mechanism is applied with
sensitivity Δ:=max𝑞2𝑤𝑞.
Proof. For NaiveAIM, the result follows almost directly from
AIM, since𝑇rounds in the latter correspond to 𝑇·𝑠in the former.
We then apply the existing privacy bounds for AIM. Similarly, for
AugFLAIM (Private), the 1-way marginals of every feature are in-
cluded in the computation, thus increasing the number of measured
marginals under Gaussian noise to 𝑇·(𝑠+𝑑). In all variations, the
exponential mechanism is only applied once for each local round
and thus𝑇𝑠times in total. For AugFLAIM, the augmented util-
ity scores𝑢(𝑞;𝐷𝑘)lead to a doubling of the sensitivity compared
to AIM, since 𝑀𝑞(𝐷𝑘)is used twice in the utility score and thus
Δ:=2·max𝑞𝑤𝑞. □
B EXPERIMENTAL SETUP
B.1 Datasets
In our experiments we use a range of tabular datasets from the UCI
repository [ 10] and others available directly from the Synthetic Data
Vault (SDV) package [ 40]. Additionally, we use one synthetic dataset
that we construct ourselves. A summary of all datasets in terms
of the number of training samples, features and classes is detailed
in Table 4. All datasets are split into a train and test set with 90%
forming the train set. From this, we form clients’ local datasets via
a partitioning method (see Appendix B.2). All continuous features
are binned uniformly between the minimum and maximum which
we assume to be public knowledge. We discretize our features
with 32bins, although experiments varying this size presented no
significant change in utility. This follows the pre-processing steps
taken by prior work [3, 32].
B.1.1 SynthFS. In order to simulate feature-skew in an ideal setting
for FLAIM, we construct a synthetic dataset that we denote SynthFS.
We draw independent features from a Gaussian distribution where
the mean is chosen randomly from a Zipfian distribution whose
parameter𝛽controls the skew. This is done in the following manner:
For each client 𝑘∈[𝐾]and feature 𝑚∈[𝑑]sample mean 𝜇𝑘𝑚∼
Zipf(𝛽,𝑛zipf), then for each feature 𝑚∈[𝑑], sample𝑛/𝐾examples
for client𝑘from𝑁(𝜇𝑘𝑚,1). In our experiments we set 𝑛=50,000
such that for 𝐾=100each client is assigned 500 samples. In order
to form a test set we sample 10% from the dataset and assign the
rest to clients. We fix 𝑑=10and𝑛zipf=40in all constructions.
 
2175KDD ’24, August 25–29, 2024, Barcelona, Spain Samuel Maddock, Graham Cormode, and Carsten Maple
Table 4: Datasets - those marked * have been subsampled for computational reasons.
Dataset # of training samples # of features # of classes
Adult [26] 43,598 14 2
Credit [21] 284,807 30 2
Covtype* [5] 116,203 55 7
Census* [40] 89,786 41 2
Intrusion*[9] 197,608 40 5
Marketing [37] 41,188 21 2
Magic [6] 17,118 11 2
SynthFS (see Appendix B.1.1) 45,000 10 N/A
Table 5: Average heterogeneity over a workload of uniform
queries computed as1
𝐾Í
𝑘Í
𝑞∈𝑄𝜏𝑘(𝑞)whilst varying differ-
ent client partition methods with 𝐾=100total clients.
Data / Partition IID Cluster Label-skew Label-skew
(𝛽=0.1) (𝛽=0.8)
Adult 0.241 0.525 0.531 0.332
Magic 0.538 0.792 0.767 0.603
B.2 Heterogeneity: Non-IID Client Partitions
In order to simulate heterogeneity we take one of the tabular
datasets outlined in Appendix B.1 and form partitions for each
client. The aim is to create client datasets that exhibit strong data
heterogeneity by varying the number of samples and inducing
feature-skew. We do this in two ways:
•Clustering Approach — We apply dimensionality reduc-
tion via UMAP [ 31] and cluster the embedding via 𝐾-means
where𝐾=100is the total number of clients. The UMAP
embedding is used only to map the original data to clients
and is not used for any model training.
•Label-skew Approach — While the clustering approach
works well to form non-IID client partitions, there is no sim-
ple parameter to vary the heterogeneity of partitions. Where
we wish to vary heterogeneity, we follow the approach out-
lined by [ 27]. For each class, we sample the distribution
𝑝𝐶∼Dirichlet(𝛽)⊆[ 0,1]𝐾and assign examples with class
value𝐶to the clients using 𝑝𝐶. This produces client parti-
tions that are skewed via the class variable of the dataset,
where a larger 𝛽decreases skew and reduces heterogeneity.
Table 5 presents the average heterogeneity for a fixed workload
of queries with different partition methods for 𝐾=100clients. We
look at the following methods: IID sampling, clustering approach,
label-skew with 𝛽=0.1(large-skew) and label-skew with 𝛽=0.8
(small-skew). Observe in all cases that our non-IID methods have
higher heterogeneity than IID sampling. Specifically, the clustering
approach works well to induce heterogeneity and can result in
twice as much skew across the workload. Note also that increasing
𝛽from 0.1to0.8decreases average heterogeneity and at 𝛽=0.8,
the skew is close to IID sampling. This confirms that simulating
client partitions in this way is useful for our experiments.B.3 Evaluation
In our experiments we evaluate methods with three metrics:
1. Average Workload Error. We mainly evaluate (FL)AIM meth-
ods via the average workload error. For a fixed workload of mar-
ginal queries 𝑄, we measure Err(𝐷,ˆ𝐷;𝑄):=1
|𝑄|Í
𝑞∈𝑄∥𝑀𝑞(𝐷)−
𝑀𝑞(ˆ𝐷)∥1where𝐷:=∪𝑘𝐷𝑘. This can be seen as a type of training
error since the models are trained to answer the queries in 𝑄.
2. Negative Log-likelihood. An alternative is the (mean) neg-
ative log-likelihood of the synthetic dataset sampled from our
(FL)AIM models when compared to a heldout test set. This metric
can be viewed as a measure of generalisation, since the metric is
agnostic to the specific workload chosen.
3. Test ROC-AUC. In some cases we evaluate our models by
training a gradient boosted decision tree (GBDT) on the synthetic
data it produces. We test the performance of the classifier on a test
set and evaluate the ROC-AUC.
B.4 Experiment Hyperparameters
CTGAN: We use the DP-CTGAN implementation contained in the
SDV package [ 40]. We performed a hyperparameter search over
epochs, LRs and gradient clipping norm. We found training for 20
epochs, with a gradient norm of 1, batch size of 128, discriminator
LR of 1𝑒−3and generator LR of 1𝑒−5gave best performance. For
the federated setting we train the DP-CTGAN using DP-FedSGD
implemented via the FLSim framework [ 44]. We found training for
50epochs with a local batch size of 128, clipping norm of 0.5, server
LR of 0.5and discriminator/generator LRs of 1𝑒−4performed best.
(FL)AIM: The number of PGM iterations determines how many
optimisation steps are performed to update the parameters of the
graphical model. AIM has two parameters, one for the number
of training iterations between global rounds and one for the final
number of iterations performed at the end of training. We set this
to 100 training iterations and 1000 final iterations. This is notably
smaller than the default parameters used in central AIM, but we
verified there is no significant impact on utility. To initialise AIM,
we follow the same procedure as central AIM, where every 1-way
marginal is estimated to initialise. In our federated settings, we
take a random sample of clients and have them estimate the 1-way
marginals to initialise. When using budget annealing, the initial
noise is calibrated under a high number of global rounds of 𝑇=16·𝑑
which results in a large amount of noise. We instead set this as
𝑇=8·𝑑since empirically we found a smaller number of global
rounds is better for performance in the federated setting.
 
2176