Heuristic Learning with Graph Neural Networks: A Unified
Framework for Link Prediction
Juzheng Zhang
Department of Electronic Engineering,
Tsinghua University
Beijing, ChinaLanning Wei
Department of Electronic Engineering,
Tsinghua University
Beijing, China
Zhen Xu
Department of Electronic Engineering,
Tsinghua University
Beijing, ChinaQuanming Yao‚àó
Department of Electronic Engineering,
Tsinghua University
Beijing, China
ABSTRACT
Link prediction is a fundamental task in graph learning, inherently
shaped by the topology of the graph. While traditional heuristics
are grounded in graph topology, they encounter challenges in gen-
eralizing across diverse graphs. Recent research efforts have aimed
to leverage the potential of heuristics, yet a unified formulation
accommodating both local and global heuristics remains undiscov-
ered. Drawing insights from the fact that both local and global
heuristics can be represented by adjacency matrix multiplications,
we propose a unified matrix formulation to accommodate and gener-
alize various heuristics. We further propose the Heuristic Learning
Graph Neural Network (HL-GNN) to efficiently implement the for-
mulation. HL-GNN adopts intra-layer propagation and inter-layer
connections, allowing it to reach a depth of around 20 layers with
lower time complexity than GCN. Extensive experiments on the
Planetoid, Amazon, and OGB datasets underscore the effectiveness
and efficiency of HL-GNN. It outperforms existing methods by a
large margin in prediction performance. Additionally, HL-GNN is
several orders of magnitude faster than heuristic-inspired methods
while requiring only a few trainable parameters. The case study
further demonstrates that the generalized heuristics and learned
weights are highly interpretable.1 2
CCS CONCEPTS
‚Ä¢Theory of computation ‚ÜíGraph algorithms analysis; ‚Ä¢
Computing methodologies ‚ÜíNeural networks.
KEYWORDS
Graph learning; Link prediction; Graph neural networks; Heuristic
methods
‚àóCorrespondence is to Q. Yao at qyaoaa@tsinghua.edu.cn.
1The full version of the paper, including the Appendix, is available at arXiv:2406.07979.
2The code is available at https://github.com/LARS-research/HL-GNN.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671946ACM Reference Format:
Juzheng Zhang, Lanning Wei, Zhen Xu, and Quanming Yao. 2024. Heuristic
Learning with Graph Neural Networks: A Unified Framework for Link
Prediction. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3637528.3671946
1 INTRODUCTION
Link prediction stands as a cornerstone in the domain of graph
machine learning, facilitating diverse applications from knowledge
graph reasoning [ 34] to drug interaction prediction [ 27,35] and
recommender systems [ 11]. While its significance is unquestionable,
research in this area has not reached the same depth as that for
node or graph classification [13, 20, 28, 29].
In graph machine learning, two fundamental sources of informa-
tion play a pivotal role: node features and graph topology [ 5,10].
Link prediction task is inherently driven by graph topology [ 17,
32,33]. Heuristics, which derive exclusively from graph topology,
naturally align with link prediction. The appeal of heuristics lies in
their simplicity and independence of learning. While heuristics are
crafted from human intuition and insights, they can be broadly cate-
gorized into two types: local heuristics, which focus on neighboring
nodes, and global heuristics, which focus on global paths [17].
Effective link prediction benefits from both local and global
topological information [ 18]. For instance, in a triangular network,
each pair of nodes shares two common neighbors, making local
heuristics effective. Conversely, in a hexagonal network, where
only length-5 paths connect each node pair, global heuristics may
yield better results. Hence, the adaptive integration of multi-range
topological information from both local and global heuristics is
essential for accurate predictions.
While heuristics prove effective in link prediction tasks, they
inherently capture specific topology patterns, posing challenges in
their generalization to diverse graphs [ 32,38]. Moreover, heuris-
tics are unable to leverage node features, limiting their efficacy on
attributed graphs [ 33]. To make heuristics more universal and gen-
eral, recent research efforts have been directed toward establishing
formulations for heuristics and learning heuristics from these for-
mulations. Notable examples include SEAL [ 33], NBFNet [ 38], and
Neo-GNN [ 32]. SEAL‚Äôsùõæ-decaying framework and NBFNet‚Äôs path
formulation are tailored for global heuristics, while Neo-GNN‚Äôs
MLP framework is tailored for local ones. To obtain multi-range
topological information, a unified formulation that accommodates
4223
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Juzheng Zhang, Lanning Wei, Zhen Xu, and Quanming Yao
both local and global heuristics is necessary, yet it remains undis-
covered.
Our motivation for constructing the unified formulation stems
from the observation that both local and global heuristics can be
expressed through adjacency matrix multiplications. Therefore, we
unify local and global heuristics into a matrix formulation, enabling
the accommodation and generalization of various local and global
heuristics. In contrast to previous works that construct formula-
tions based on abstract functions such as SEAL [ 33], NBFNet [ 38],
and Neo-GNN [ 32], our unified formulation is developed through
direct matrix operations. This unified formulation ensures rigorous
equivalence to numerous local and global heuristics under specific
configurations.
To learn generalized heuristics and acquire multi-range informa-
tion, we propose the Heuristic Learning Graph Neural Network
(HL-GNN) to efficiently implement the formulation. HL-GNN incor-
porates intra-layer propagation and inter-layer connections while
excluding transformation and activation functions. This enables
HL-GNN to effectively reach a depth of around 20 layers, while
only requiring the training of a global GNN with a time complexity
even lower than GCN. The adaptive weights in HL-GNN facilitate
the integration of multi-range topological information, and govern
the trade-off between node features and topological information.
Our comprehensive experiments, conducted on the Planetoid,
Amazon, and OGB datasets, confirm the effectiveness and efficiency
of our proposed HL-GNN. It consistently achieves state-of-the-art
performance across numerous benchmarks, maintains excellent
scalability, and stands out as the most parameter-efficient method
among existing GNN methods. Furthermore, it demonstrates su-
perior speed, surpassing existing heuristic-inspired methods by
several orders of magnitude. HL-GNN is highly interpretable, as
evidenced by the generalized heuristics and learned weights on
real-world datasets as well as synthetic datasets.
Our contributions can be summarized as follows:
‚Ä¢We unify local and global heuristics into a matrix formulation,
facilitating the accommodation and generalization of heuristics.
We demonstrate that numerous traditional heuristics align
with our formulation under specific configurations.
‚Ä¢We propose HL-GNN to efficiently implement the formulation,
capable of reaching a depth of around 20 layers with lower time
complexity than GCN. HL-GNN can adaptively balance the
trade-off between node features and topological information.
‚Ä¢Comprehensive experiments demonstrate that HL-GNN out-
performs existing methods in terms of performance and effi-
ciency. The interpretability of HL-GNN is highlighted through
the analysis of generalized heuristics and learned weights.
2 RELATED WORKS
2.1 Graph Neural Networks
Graph Neural Networks (GNNs) have emerged as a powerful para-
digm for learning node representations by exploiting neural net-
works to manipulate both node features and graph topology. These
networks employ a message-passing mechanism, with notable ex-
amples including Graph Convolutional Network (GCN) [ 10], Graph-
SAGE [ 7], and Graph Attention Network (GAT) [ 26]. Through iter-
ative message propagation, GNNs enable each node representationto accumulate information from its neighboring nodes, thereby
facilitating downstream tasks.
While GNNs have emerged as potent solutions for node and
graph classification [ 13,20,28,29], they sometimes fall short in link
prediction scenarios compared to traditional heuristics like Com-
mon Neighbors (CN) [ 2] or the Resource Allocation Index (RA) [ 36].
The primary issue lies in the inherent intertwining of node fea-
tures and graph topology during the message-passing process in
conventional GNNs. This entanglement causes node features to
interfere with graph topology, impeding the effective extraction of
topological information for link prediction tasks.
Although in principle an arbitrary number of GNN layers can
be stacked, practical GNNs are usually shallow, typically consist-
ing of 2-3 layers, as conventional GNNs often experience a sharp
performance drop after just 2 or 3 layers. A widely accepted explana-
tion for this performance degradation with increasing depth is the
over-smoothing issue [ 14,30], which refers to node representations
becoming non-discriminative when going deep. While the adap-
tive integration of both local and global topological information
is essential for link prediction, conventional GNNs usually can-
not penetrate beyond 3 layers, restricting the extraction of global
topological information.
2.2 Link Prediction
Link prediction predicts the likelihood of a link forming between
two nodes in a graph. The problem of link prediction has tra-
ditionally been addressed by heuristic methods. These methods
are primarily concerned with quantifying the similarity between
two nodes based on the graph topology. Heuristic methods can be
broadly categorized into two groups: local and global [17, 18].
Local heuristics can be further divided into entirety-based heuris-
tics and individual-based heuristics. Entirety-based heuristics, like
Common Neighbors (CN) [ 2] and the Local Leicht-Holme-Newman
Index (LLHN) [ 12], consider the cumulative count of common neigh-
bors. In contrast, individual-based heuristics, exemplified by the
Resource Allocation Index (RA) [ 36], focus on nodes within the
common neighborhood and incorporate detailed topological infor-
mation such as the degree of each node.
Global heuristics, on the other hand, leverage the entire graph
topology. Methods such as the Katz Index (KI) [ 9] and the Global
Leicht-Holme-Newman Index (GLHN) [ 12] consider all possible
paths between node pairs. The Random Walk with Restart (RWR) [ 3]
assesses the similarity between two nodes based on random walk
probabilities. Some global heuristics are tailored to specific path
lengths, like the Local Path Index (LPI) [ 16] and the Local Random
Walks (LRW) [15].
Traditional heuristic methods are manually designed and show
limitations on complex real-world graphs, prompting a shift toward
learning-based approaches. Embedding methods, including Matrix
Factorization [ 11], DeepWalk [ 19], LINE [ 24], and Node2vec [ 6],
factorize network representations into low-dimensional node em-
beddings. However, embedding methods face limitations due to
their inability to leverage node features on attributed graphs.
Recent advancements have focused on enhancing GNNs with
valuable topological information. Subgraph GNNs like SEAL [ 33],
GraIL [ 25], and SUREL [ 31] explicitly encode subgraphs around
4224Heuristic Learning with Graph Neural Networks: A Unified Framework for Link Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
node pairs. However, they require the running of a subgraph GNN
with the labeling trick for each link during training and inference.
Taking a different perspective, models like NBFNet [ 38] and RED-
GNN [ 34] adopt source-specific message passing, drawing inspira-
tion from global heuristics. However, they require training a global
GNN for each source node. Some methods opt for a single global
GNN to improve scalability and efficiency. Neo-GNN [ 32] uses two
MLPs, while SEG [ 1] uses a GCN layer and an MLP to approxi-
mate a heuristic function. BUDDY [ 4] develops a novel GNN that
passes subgraph sketches as messages. However, these methods
primarily focus on local topological information and struggle to
capture global topological information. In contrast, the proposed
HL-GNN can capture long-range information up to 20 hops while
only requiring the training of a global GNN. Further details about
the comparison of HL-GNN with existing methods are provided in
Section 4.2.
3 UNIFIED HEURISTIC FORMULATION
LetG=(V,E)denote a graph, with nodes Vand edgesE. In this
work, we consider undirected and unweighted graphs. We define
|V|=ùëÅas the number of nodes and |E|=ùëÄas the number of
edges. Node features are characterized by the node feature matrix
ùëø‚ààRùëÅ√óùêπ, whereùêπindicates the number of features. The graph
topology is encapsulated by the adjacency matrix ùë®‚àà{0,1}ùëÅ√óùëÅ.
The matrix Àúùë®=ùë®+ùë∞ùëÅrepresents the adjacency matrix with self-
loops, where Àúùëéùëñùëó=1signifies an edge between nodes ùëñandùëó. The
node degree ùëñwith self-loops is given by Àúùëëùëñ=√ç
ùëóÀúùëéùëñùëó, with the diago-
nal degree matrix with self-loops denoted as Àúùë´=diag(Àúùëë1,¬∑¬∑¬∑,ÀúùëëùëÅ).
The set Œìùë•represents the 1-hop neighbors of node ùë•, encompassing
nodeùë•itself.
We introduce a set of normalized adjacency matrices, detailed in
Table 1. This set comprises the symmetrically normalized matrix
Àúùë®sym, the row-stochastic normalized matrix Àúùë®rs, and the column-
stochastic normalized matrix Àúùë®cs, which encompass diverse nor-
malization techniques (left multiplication, right multiplication, or
both) applied to the adjacency matrix. Next, we define the propaga-
tion operator Ato offer a choice among different types of adjacency
matrices:
Definition 3.1. (Propagation operator ). The propagation operator
A‚ààRùëÅ√óùëÅis defined as A‚àà{Àúùë®,Àúùë®sym,Àúùë®rs,Àúùë®cs}. The expressions
for the adjacency matrices Àúùë®,Àúùë®sym,Àúùë®rs,Àúùë®csare detailed in Table 1.
The propagation operator encapsulates the prevalent propaga-
tion mechanisms commonly employed in GNNs. By substituting the
adjacency matrix Àúùë®with the propagation operator A, we can opt for
various propagation mechanisms that deliver diverse information.
Given that heuristics are fundamentally influenced by graph
topology, it is possible to express various heuristics using adjacency
matrices. The(ùëñ,ùëó)entry of the 2-order adjacency matrix multi-
plication denotes the count of common neighbors for nodes ùëñand
ùëó. The(ùëñ,ùëó)entry of the ùëô-order adjacency matrix multiplications
denotes the number of length- ùëôpaths between nodes ùëñandùëó. Hence,
by employing distinct orders of adjacency matrix multiplications,
we can extract varying insights from neighbors or paths. Following
this intuition, we can express diverse heuristics in matrix form.
We provide a concise summary of heuristics, their mathematicalTable 1: Notations and expressions of adjacency matrices.
Adjacency Matrix Notation Expression
Matrix with Self-Loops Àúùë® ùë® +ùë∞ùëÅ
Symmetrical Matrix Àúùë®symÀúùë´‚àí1/2Àúùë®Àúùë´‚àí1/2
Row-Stochastic Matrix Àúùë®rsÀúùë´‚àí1Àúùë®
Column-Stochastic Matrix Àúùë®csÀúùë®Àúùë´‚àí1
Propagation Operator A{Àúùë®,Àúùë®sym,Àúùë®rs,Àúùë®cs}
expressions, and their corresponding matrix forms in Table 2. Next,
we introduce the definition of the heuristic formulation:
Definition 3.2. (Heuristic formulation ). Aheuristic formulation is
denoted by a matrix ùëØ‚ààRùëÅ√óùëÅ. Each entry(ùëñ,ùëó)in this matrix
corresponds to the heuristic score for the link (ùëñ,ùëó), denoted as
ùëØùëñ,ùëó=‚Ñé(ùëñ,ùëó).
We can unify both local and global heuristics in a formulation
based on matrix forms of heuristics. Our proposed heuristic formu-
lation parameterizes a combination of matrix multiplications:
ùëØ=ùêø‚àëÔ∏Å
ùëô=0 
ùõΩ(ùëô)ùëô√ñ
ùëö=0A(ùëö)!
, (1)
where A(ùëö)‚àà{Àúùë®,Àúùë®sym,Àúùë®rs,Àúùë®cs}for1‚â§ùëö‚â§ùêørepresent the
propagation operators, and A(0)=ùë∞ùëÅ. The coefficients ùõΩ(ùëô)for
0‚â§ùëô‚â§ùêømodulate the weights of different orders of matrix mul-
tiplications, and ùêøis the maximum order. Numerous traditional
heuristics align with our formulation under specific configurations.
Table 2 showcases a selection of traditional heuristics and illustrates
their alignment with our formulation through propagation opera-
torsA(ùëö)and weights ùõΩ(ùëô). We assert the formulation‚Äôs ability to
accommodate heuristics in Proposition 3.1:
Proposition 3.1. Our formulation can accommodate a broad
spectrum of local and global heuristics with propagation operators
A(ùëö)for1‚â§ùëö‚â§ùêø, weight parameters ùõΩ(ùëô)for0‚â§ùëô‚â§ùêø, and
maximum order ùêø.
Unlike previous methods that exclusively cater to either local
or global heuristics, our formulation seamlessly integrates both as-
pects, presenting a unified solution. In contrast to prior works rely-
ing on abstract functions for heuristic approximation [ 1,32,33,38],
our formulation is developed through direct matrix operations. This
formulation facilitates rigorous equivalence to numerous local and
global heuristics under specific configurations. It is crucial to note
that our heuristic formulation does not aim to accommodate all pos-
sible heuristics. Instead, it aims to distill the critical characteristics
of heuristics, with a specific focus on extracting common neighbors
from local heuristics and global paths from global heuristics.
Existing heuristics are primarily handcrafted and may not be
optimal for real-world graphs. Leveraging the propagation opera-
tors, weight parameters and maximum order offers the potential to
learn generalized, possibly more effective heuristics, which we will
discuss in Section 5.5.1.
4225KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Juzheng Zhang, Lanning Wei, Zhen Xu, and Quanming Yao
Table 2: A selection of traditional local and global heuristics, their mathematical expressions, their matrix forms, and specific
configurations within the unified heuristic formulation for alignment.
Type Method Expression Matrix Form Propagation Operators A(ùëö)Weight Parameters ùõΩ(ùëô)
Local CN |Œìùëñ‚à©Œìùëó| ( Àúùë®2)ùëñ,ùëó A(1)=A(2)=Àúùë® ùõΩ(0)=ùõΩ(1)=0,ùõΩ(2)=1
Local LLHN|Œìùëñ‚à©Œìùëó|
ÀúùëëùëñÀúùëëùëó(Àúùë®rsÀúùë®cs)ùëñ,ùëó A(1)=Àúùë®rs,A(2)=Àúùë®cs ùõΩ(0)=ùõΩ(1)=0,ùõΩ(2)=1
Local RA√ç
ùëò‚ààŒìùëñ‚à©Œìùëó1
Àúùëëùëò(Àúùë®csÀúùë®)ùëñ,ùëó A(1)=Àúùë®cs,A(2)=Àúùë®*ùõΩ(0)=ùõΩ(1)=0,ùõΩ(2)=1
Global KI√ç‚àû
ùëô=1ùõæùëô|pathsùëô
ùëñ,ùëó|√ç‚àû
ùëô=1ùõæùëôÀúùë®ùëô
ùëñ,ùëóA(ùëö)=Àúùë®forùëö‚â•1 ùõΩ(0)=0,ùõΩ(ùëô)=ùõæùëôforùëô‚â•1
Global GLHN√ç‚àû
ùëô=0ùúôùëô|pathsùëô
ùëñ,ùëó|
ùë∞ùëÅ+√ç‚àû
ùëô=1ùúôùëôÀúùë®ùëô
ùëñ,ùëóA(ùëö)=Àúùë®forùëö‚â•1 ùõΩ(0)=1,ùõΩ(ùëô)=ùúôùëôforùëô‚â•1
Global RWR [ùùÖùëñ(‚àû)] ùëó√ç‚àû
ùëô=0(1‚àíùõº)ùõºùëôÀúùë®ùëô
rs
ùëñ,ùëóA(ùëö)=Àúùë®rsforùëö‚â•1 ùõΩ(ùëô)=(1‚àíùõº)ùõºùëôforùëô‚â•0
Global LPI√çùêø
ùëô=2ùõæùëô‚àí2|pathsùëô
ùëñ,ùëó|√çùêø
ùëô=2ùõæùëô‚àí2Àúùë®ùëô
ùëñ,ùëóA(ùëö)=Àúùë®for1‚â§ùëö‚â§ùêø ùõΩ(0)=ùõΩ(1)=0,ùõΩ(ùëô)=ùõæùëô‚àí2for2‚â§ùëô‚â§ùêø
Global LRWÀúùëëùëñ
2ùëÄ[ùùÖùëñ(ùêø)]ùëó√çùêø‚àí1
ùëô=0Àúùëëùëñ
2ùëÄ(1‚àíùõº)ùõºùëôÀúùë®ùëô
rs
ùëñ,ùëóA(ùëö)=Àúùë®rsfor1‚â§ùëö‚â§ùêø‚àí1ùõΩ(ùëô)=Àúùëëùëñ
2ùëÄ(1‚àíùõº)ùõºùëôfor0‚â§ùëô‚â§ùêø‚àí1
*When setting A(1)=Àúùë®,A(2)=Àúùë®rsin the formulation, it also aligns with the RA Index.
4 HEURISTIC LEARNING GRAPH NEURAL
NETWORK (HL-GNN)
4.1 Heuristic Learning Graph Neural Network
4.1.1 Motivation. Direct matrix multiplication serves as a straight-
forward method to implement the heuristic formulation in Equa-
tion (1). However, it comes with high computational and mem-
ory costs. The time complexity of direct matrix multiplication is
O(ùêø2ùëÅ3)and the space complexity is O(ùêøùëÅ2), whereùëÅdenotes
the number of nodes. This is attributed to executing up to ùêø-order
matrix multiplications for ùêøtimes. The significant time and space
complexities present two major challenges of ensuring scalability
and maintaining depth:
‚Ä¢Scalability. To be scalable, the model must effectively handle large
graphs. Datasets like OGB are substantially larger than those like
Planetoid, making the value of ùëÅa considerable strain on the
time and space complexities.
‚Ä¢Depth. To effectively integrate global heuristics into the heuristic
formulation, the value of ùêømust be sufficiently large to encapsu-
late messages from distant nodes. However, increasing ùêøfurther
strains the time and space complexities.
Consequently, there is a pressing need to mitigate the burdens of
both time and space complexities.
4.1.2 Architecture. The construction and computation of ùëÅ√óùëÅma-
trices impose significant computational and memory demands. One
potential technique is initializing A(0)=ùëøinstead of A(0)=ùë∞ùëÅ.
This approach effectively reduces the feature dimensionality from
ùëÅtoùêπ, resulting in substantial time and space savings. Moreover,
since heuristics cannot leverage node features on attributed graphs,
initializing with A(0)=ùëøallows the heuristic formulation to
utilize node features. Even if node features are of low quality or
completely absent, we can still train embeddings for each node.
Therefore, ùëøcan represent either raw node features or learnable
node embeddings.
Further, we exploit the sparsity of the graph and employ a Graph
Neural Network to compute the heuristic formulation. We propose
the efficient and scalable Heuristic Learning Graph Neural Network(HL-GNN), expressed as:
ùíÅ(0)=ùëø,ùíÅ(ùëô)=A(ùëô)ùíÅ(ùëô‚àí1),ùíÅ=ùêø‚àëÔ∏Å
ùëô=0ùõΩ(ùëô)ùíÅ(ùëô), (2)
withùõΩ(ùëô)representing the learnable weight of the ùëô-th layer, and ùêø
representing the model‚Äôs depth. An illustration of our proposed HL-
GNN is provided in Figure 1. We do not impose constraints on ùõΩ(ùëô),
allowing them to take positive or negative values. Adaptive weights
ùõΩ(ùëô)facilitate the integration of multi-range topological information
and govern the trade-off between node features and topological
information. Given the discrete nature of the propagation operators
A(ùëô)‚àà {Àúùë®,Àúùë®sym,Àúùë®rs,Àúùë®cs}, they obstruct the back-propagation
process, necessitating their relaxation to a continuous form:
A(ùëô)=ùõº(ùëô)
1Àúùë®rs+ùõº(ùëô)
2Àúùë®cs+ùõº(ùëô)
3Àúùë®sym,for 1‚â§ùëô‚â§ùêø, (3)
whereùõº(ùëô)
1,ùõº(ùëô)
2,ùõº(ùëô)
3are layer-specific learnable weights harmo-
nizing three propagation mechanisms. The continuous relaxation
of the propagation operators enables gradient back-propagation,
thereby allowing the model to be trained end-to-end. We exclude the
adjacency matrix Àúùë®in Equation (3)to ensure that the eigenvalues of
A(ùëô)fall within the range [0,1]. Moreover, we apply a softmax func-
tion to ùú∂(ùëô), where softmax(ùõº(ùëô)
ùëñ)=exp(ùõº(ùëô)
ùëñ)/√ç3
ùëó=1exp(ùõº(ùëô)
ùëó)for
ùëñ=1,2,3. Controlling the eigenvalues of A(ùëô)helps prevent numer-
ical instabilities as well as issues related to exploding gradients or
vanishing gradients.
HL-GNN employs intra-layer propagation and inter-layer con-
nections as described in Equation (2). The salient trait is its elimi-
nation of representation transformation and non-linear activation
at each layer, requiring only a few trainable parameters. We assert
the relationship between the learned representations ùíÅand the
heuristic formulation ùëØin Proposition 4.1:
Proposition 4.1. The relationship between the learned representa-
tions ùíÅin Equation (2)and the heuristic formulation ùëØin Equation (1)
is given by ùíÅ=ùëØùëø, where ùëøis the node feature matrix.
According to Proposition 4.1, the learned representations ùíÅuti-
lize heuristics as weights to combine features from all nodes. The
4226Heuristic Learning with Graph Neural Networks: A Unified Framework for Link Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
ùëøùî∏!ùî∏"ùî∏#ùî∏$ùíÅùõΩ%ùõΩ!ùõΩ"ùõΩ$ùíÅ%ùíÅ!ùíÅ"ùíÅ$%ùë®&',%ùë®(',%ùë®')*%ùë®&',%ùë®(',%ùë®')*
Figure 1: Illustration of the proposed Heuristic Learning
Graph Neural Network (HL-GNN). Every rounded rectangle
symbolizes a left multiplication operation.
heuristic formulation ùëØcan be effectively distilled through the
message-passing process in HL-GNN. Consequently, HL-GNN has
the ability to accommodate and generalize both local and global
heuristics. Our method can be viewed as topological augmentation,
employing the topological information embedded in ùëØto enhance
raw node features ùëø.
For sparse graphs, the time complexity of HL-GNN is O(ùêøùëÄùêπ),
whereùëÄis the number of edges. The space complexity of HL-GNN
isO(ùëÅùêπ). On a large graph, typically containing millions of nodes,
HL-GNN leads to remarkable time and space savings ‚Äì ten and
five orders of magnitude, respectively ‚Äì compared to direct matrix
multiplication.
4.1.3 Training. After acquiring the node representations, we em-
ploy a predictor to compute the likelihood for each link by ùë†ùëñùëó=
ùëìùúÉ(ùíõùëñ‚äôùíõùëó), whereùëìùúÉis a feed-forward neural network, ùíõùëñandùíõùëó
represent the representations of node ùëñandùëórespectively, and the
symbol‚äôdenotes the element-wise product.
Many methods categorize link prediction as a binary classifica-
tion problem and conventionally employ the cross-entropy loss
function. However, this might not always be the suitable strategy.
Standard evaluation procedures in link prediction do not label pos-
itive pairs as 1 and negative pairs as 0. The primary objective is
to rank positive pairs higher than negative pairs, aligning with
the maximization of the Area Under the Curve (AUC). In light of
this, we adopt the AUC loss as described in [ 21], ensuring it aligns
conceptually with the evaluation procedure:
L=min
ùõº,ùõΩ,ùúÉ‚àëÔ∏Å
(ùëñ,ùëó)‚ààE‚àëÔ∏Å
(ùëñ,ùëò)‚ààE‚àíùõæùëñùëó max(0,ùõæùëñùëó‚àíùë†ùëñùëó+ùë†ùëñùëò)2.(4)
Here,E‚àísignifies the negative links uniformly sampling from the
setV√óV‚àíE , andùõæùëñùëóis an adaptive margin between positive
link(ùëñ,ùëó)and negative link(ùëñ,ùëò). The model is trained end-to-end,
jointly optimizing the GNN parameters ùõºandùõΩ, along with the
predictor parameters ùúÉ.
4.2 Comparison with Existing Methods
We evaluate the heuristic-learning ability, information range, and
time complexity of HL-GNN by comparing it with conventional
GNNs and heuristic-inspired GNN methods. A summary of these
comparisons is provided in Table 3. HL-GNN excels at accommodat-
ing and generalizing a wide range of both local and global heuristics.
In contrast, SEAL [ 33] focuses on subgraphs to learn local heuristics,Table 3: Comparison of heuristic-learning ability, informa-
tion range, and time complexity of HL-GNN with conven-
tional GNNs and heuristic-inspired GNN methods.
Learned Heuristics Range Time Complexity
GCN N.A. 3 hops O(ùêøùêπ(ùëÄ+ùëÅùêπ))
GAT N.A. 3 hops O(ùêøùêæùëÅùê∑2ùêπ2)
SEAL Local 3 hops O(ùëÄ(ùëâ2+ùêøùê∏ùêπ))
NBFNet Global 6 hops O(ùêøùëÅùêπ(ùëÄ+ùëÅùêπ))
Neo-GNN Local 2 hops O(ùêøùëÄùêπ+ùëÅùê∑ùêπ2)
BUDDY Local 3 hops O(ùêøùëÄ(ùêøùêª+ùêπ2))
HL-GNN Local / Global 20 hops O(ùêøùëÄùêπ)
while NBFNet [ 38] concentrates on paths to learn global heuristics.
Neo-GNN [ 32] leverages two MLPs for local heuristic learning, and
BUDDY [ 4] uses subgraph sketches to represent local heuristics.
Notably, most of these methods are limited to topological informa-
tion within a 3-hop range. In contrast, HL-GNN can reach a depth
of approximately 20 layers, providing a broader information range.
Adaptive weights in HL-GNN enable the integration of both local
and global topological information.
HL-GNN has a time complexity of O(ùêøùëÄùêπ), which is the lowest
among the compared methods. Unlike conventional GNNs, HL-
GNN solely utilizes propagation mechanisms and omits transforma-
tion and activation functions. SEAL requires running a subgraph
GNN with the labeling trick for each link, and NBFNet requires
running a global GNN for each source node during training and in-
ference. In contrast, HL-GNN only requires running a single global
GNN during training and inference. Furthermore, HL-GNN avoids
the need to extract topological information from common neigh-
bors and subgraph sketches, as required by Neo-GNN and BUDDY,
respectively.
5 EXPERIMENTS
5.1 Experiment Setup
5.1.1 Datasets. We utilize nine datasets from three sources: Plan-
etoid [ 22], Amazon [ 23], and OGB [ 8]. The Planetoid datasets in-
clude Cora ,Citeseer , and Pubmed . The Amazon datasets include
Photo and Computers . The OGB datasets include ogbl-collab ,
ogbl-ddi, ogbl-ppa, and ogbl-citation2.
5.1.2 Baselines. We compare our model against a diverse set of
baseline methods, including heuristics like CN [ 2], RA [ 36], KI [ 9],
and RWR [ 3], traditional embedding-based methods such as MF [ 11],
Node2vec [ 6], and DeepWalk [ 19], as well as conventional GNNs
like GCN [ 10] and GAT [ 26]. Additionally, we benchmark HL-GNN
against heuristic-inspired GNN methods like SEAL [ 33], NBFNet [ 38],
Neo-GNN [ 32], and BUDDY [ 4]. This comprehensive comparison
enable us to assess the performance and effectiveness of the pro-
posed HL-GNN.
5.1.3 Experimental settings. In accordance with previous works [ 4,
38], we randomly sample 5% and 10% of the links for validation
and test sets on non-OGB datasets. We sample the same number
of non-edge node pairs as negative links. For the OGB datasets,
we follow their official train/validation/test splits. Following the
4227KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Juzheng Zhang, Lanning Wei, Zhen Xu, and Quanming Yao
Table 4: Results on link prediction benchmarks including the Planetoid, Amazon, and OGB datasets. Results are presented as
average¬±standard deviation. The best and second-best performances are marked with bold and underline , respectively. OOM
denotes out of GPU memory.
Cora Citeseer Pubmed Photo Computers collab ddi ppa citation2
Hits@100 Hits@100 Hits@100 AUC AUC Hits@50 Hits@20 Hits@100 MRR
CN 33.92¬±0.46 29.79¬±0.90 23.13¬±0.15 96.73¬±0.00 96.15¬±0.00 56.44¬±0.00 17.73¬±0.00 27.65¬±0.00 51.47¬±0.00
RA 41.07¬±0.48 33.56¬±0.17 27.03¬±0.35 97.20¬±0.00 96.82¬±0.00 64.00¬±0.00 27.60¬±0.00 49.33¬±0.00 51.98¬±0.00
KI 42.34¬±0.39 35.62¬±0.33 30.91¬±0.69 97.45¬±0.00 97.05¬±0.00 59.79¬±0.00 21.23¬±0.00 24.31¬±0.00 47.83¬±0.00
RWR 42.57¬±0.56 36.78¬±0.58 29.77¬±0.45 97.51¬±0.00 96.98¬±0.00 60.06¬±0.00 22.01¬±0.00 22.16¬±0.00 45.76¬±0.00
MF 64.67¬±1.43 65.19¬±1.47 46.94¬±1.27 97.92¬±0.37 97.56¬±0.66 38.86¬±0.29 13.68¬±4.75 32.29¬±0.94 51.86¬±4.43
Node2vec 68.43¬±2.65 69.34¬±3.04 51.88¬±1.55 98.37¬±0.33 98.21¬±0.39 48.88¬±0.54 23.26¬±2.09 22.26¬±0.88 61.41¬±0.11
DeepWalk 70.34¬±2.96 72.05¬±2.56 54.91¬±1.25 98.83¬±0.23 98.45¬±0.45 50.37¬±0.34 26.42¬±6.10 35.12¬±0.79 55.58¬±1.75
GCN 66.79¬±1.65 67.08¬±2.94 53.02¬±1.39 98.61¬±0.15 98.55¬±0.27 47.14¬±1.45 37.07¬±5.07 18.67¬±1.32 84.74¬±0.21
GAT 60.78¬±3.17 62.94¬±2.45 46.29¬±1.73 98.42¬±0.19 98.47¬±0.32 55.78¬±1.39 54.12¬±5.43 19.94¬±1.69 86.33¬±0.54
SEAL 81.71¬±1.30 83.89¬±2.15 75.54¬±1.32 98.85¬±0.04 98.70¬±0.18 64.74¬±0.43 30.56¬±3.86 48.80¬±3.16 87.67¬±0.32
NBFNet 71.65¬±2.27 74.07¬±1.75 58.73¬±1.99 98.29¬±0.35 98.03¬±0.54 OOM 4.00¬±0.58 OOM OOM
Neo-GNN 80.42¬±1.31 84.67¬±2.16 73.93¬±1.19 98.74¬±0.55 98.27¬±0.79 62.13¬±0.58 63.57¬±3.52 49.13¬±0.60 87.26¬±0.84
BUDDY 88.00¬±0.44 92.93¬±0.27 74.10¬±0.78 99.05¬±0.21 98.69¬±0.34 65.94¬±0.58 78.51¬±1.36 49.85¬±0.20 87.56¬±0.11
HL-GNN 94.22¬±1.64 94.31¬±1.51 88.15¬±0.38 99.11¬±0.07 98.82¬±0.21 68.11¬±0.54 80.27¬±3.98 56.77¬±0.84 89.43¬±0.83
convention in previous works [ 4,37], we use Hits@100 as the
evaluation metric for the Planetoid datasets, and we use AUC for
the Amazon datasets. For the OGB datasets, we use their official
evaluation metrics, such as Hits@50 for ogbl-collab , Hits@20
forogbl-ddi , Hits@100 for ogbl-ppa , and Mean Reciprocal Rank
(MRR) for ogbl-citation2 [8].
We include a linear layer as preprocessing before HL-GNN to
align the dimension of node features with the hidden channels of
HL-GNN. We also leverage node embeddings on the OGB datasets
to enhance the node representations. For the ogbl-collab dataset,
we follow OGB‚Äôs guidelines and use the validation set for training.
We evaluate HL-GNN over 10 runs without fixing the random seed.
5.2 Main Results
As shown in Table 4, HL-GNN consistently outperforms all the
baselines on all of the datasets, highlighting its effectiveness and
robustness for link prediction tasks. Table 4 reports the averaged
results with standard deviations. Notably, HL-GNN achieves a re-
markable gain of 7.0% and 16.7% in Hits@100 compared to the
second-best method on the Planetoid datasets Cora andPubmed ,
respectively. Moreover, our HL-GNN demonstrates its ability to
handle large-scale graphs effectively, as evidenced by its superior
performance on the OGB datasets. Specifically, HL-GNN achieves
a gain of 13.9% in Hits@100 on ogbl-ppa , and achieves 68.11%
Hits@50 on ogbl-collab and 89.43% MRR on ogbl-citation2 .
Even when node features are absent or of low quality, HL-GNN
maintains consistent performance by learning embeddings for each
node, as demonstrated on datasets like ogbl-ddi , which lack node
features.
HL-GNN outperforms all listed heuristics, indicating its capacity
to generalize heuristics and integrate them with node features. Ac-
cording to Table 4, local heuristics like CN and RA perform better
than global heuristics on the OGB datasets, while global heuristicslike KI and RWR perform better on the Planetoid and Amazon
datasets. This underscores the importance of establishing a unified
formulation that accommodates both local and global heuristics.
Notably, we can use the configuration in Table 2 to recover the
heuristic RA from HL-GNN without training, achieving a perfor-
mance of 49.33% Hits@100 on ogbl-ppa . This result serves as a
compelling lower bound for HL-GNN‚Äôs performance on ogbl-ppa .
HL-GNN significantly outperforms conventional GNNs like GCN
and GAT across all datasets. Additionally, HL-GNN also surpasses
existing heuristic-inspired GNN methods, including SEAL, NBFNet,
Neo-GNN, and BUDDY, suggesting that integrating information
from multiple ranges is beneficial for link prediction tasks.
5.3 Ablation Studies
5.3.1 Different information ranges. The adaptive weights ùõΩ(ùëô)in
HL-GNN facilitate the integration of multi-range information, en-
compassing both local and global topological information. To inves-
tigate the impact of information ranges, we conduct experiments
isolating either local or global information. We train a GNN variant
using skip-connections of the first 3 layers as the output, exclu-
sively considering local topological information. Similarly, we train
another GNN variant using the final-layer output with GNN depth
ùêø‚â•5to exclusively consider global topological information. Fig-
ure 2 demonstrates that HL-GNN consistently outperforms GNN
variants focusing solely on local or global topological information.
This underscores HL-GNN‚Äôs efficacy in adaptively combining both
types of information.
5.3.2 Sufficient model depths. In HL-GNN, achieving sufficient
model depth is crucial for learning global heuristics and capturing
long-range dependencies. Our model can effectively reach a depth
of around 20 layers without performance deterioration, as shown in
Figure 3. In contrast, conventional GNNs often experience a sharp
performance drop after just 2 or 3 layers. For the Planetoid datasets
4228Heuristic Learning with Graph Neural Networks: A Unified Framework for Link Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
510152025303540
GNN Depth (L)84868890929496Hits@100 (%)
HL-GNN
Local
Global
(a)Cora.
510152025303540
GNN Depth (L)5456586062646668Hits@50 (%)
HL-GNN
Local
Global (b)ogbl-collab.
Figure 2: Ablation study on information ranges. We compare
HL-GNN with two GNN variants, focusing on either local or
global topological information, with different GNN depths.
10 20 30 40
GNN Depth (L)94.094.595.095.5Hits@100 (%)
Cora
Pubmed87.087.588.0
Hits@100 (%)
(a) Planetoid datasets.
10 20 30 40
GNN Depth (L)66.567.067.568.0Hits@50 (%)
 ogbl-collab
ogbl-ddi657075
Hits@20 (%)
 (b) OGB datasets.
Figure 3: Test performance on the Planetoid and OGB datasets
with different GNN depths.
Cora andPubmed , shallow models yield poor performance, likely
due to the absence of global topological information. Conversely,
for the OGB datasets ogbl-collab andogbl-ddi , deeper models
(exceeding 15 layers) result in decreased performance, possibly
due to the introduction of non-essential global information, which
dilutes the crucial local information needed for accurate predictions.
5.4 Efficiency Analysis
5.4.1 Time efficiency. Our HL-GNN demonstrates exceptional time
efficiency with the lowest time complexity, as indicated in Table 3.
The wall time for a single training epoch is provided in Table 5.
Although HL-GNN generally has a larger depth ùêøcompared to
conventional GNNs, its experimental wall time per training epoch
is comparable to models like GCN and GAT. In practice, HL-GNN
requires slightly more time than GCN or GAT due to its increased
depth. However, HL-GNN is several orders of magnitude faster
than heuristic-inspired GNN methods such as SEAL, NBFNet, and
Neo-GNN, thanks to its avoidance of running multiple GNNs and
time-consuming manipulations like applying the labeling trick.
5.4.2 Parameter efficiency. HL-GNN only demands a few param-
eters per layer, with the primary parameter cost incurred by the
preprocessing step and the MLP predictor. Table 6 compares the
number of parameters in HL-GNN with other GNN methods, clearly
highlighting HL-GNN‚Äôs superior parameter efficiency. Our model
stands out as the most parameter-efficient among the listed con-
ventional GNNs and heuristic-inspired GNN methods.Table 5: Wall time per epoch (in seconds) for training HL-
GNN compared to other GNN methods. The shortest and
second shortest times are marked with bold and underline ,
respectively.
Cora Citeseer Pubmed collab ddi
GCN 0.02 0.03 0.4 5.3 9.2
GAT 0.05 0.06 0.5 5.8 10.4
SEAL 28.7 27.3 310 5,130 15,000
NBFNet 129 115 1,050 / 52,000
Neo-GNN 2.6 1.4 19.5 101 172
BUDDY 0.1 0.1 0.8 10.5 17.6
HL-GNN 0.06 0.05 0.5 6.7 16.2
Table 6: Number of parameters for HL-GNN compared to
other GNN methods. The least and second least number of pa-
rameters are marked with bold and underline , respectively.
Cora Citeseer Pubmed collab ddi
GCN 565k 1.15M 326k 231k 1.36M
GAT 566k 1.15M 327k 394k 1.55M
SEAL 2.30M 3.46M 1.82M 1.63M 6.19M
NBFNet 3.71M 5.02M 3.03M OOM 11.04M
Neo-GNN 631k 1.21M 392k 297k 1.36M
BUDDY 2.52M 4.85M 1.57M 1.19M 2.71M
HL-GNN 433k 1.01M 194k 99k 1.22M
While conventional GNNs excel in efficiency but may lack in
performance, and heuristic-inspired GNN methods are effective
but time and parameter-intensive, HL-GNN strikes a balance. It
consistently achieves top-tier prediction performance on numerous
link prediction benchmarks, maintains excellent scalability and time
efficiency, and stands out as the most parameter-efficient method.
5.5 Interpretability Analysis
5.5.1 Generalized heuristics and learned weights. Leveraging the
capabilities of the unified formulation, we can derive generalized
heuristics by analyzing the learned parameters of HL-GNN. The
generalized heuristics and learned weights ùõΩ(ùëô)provide insights
into the graph-structured data. The learned weights ùõΩ(ùëô)are visually
depicted in Figure 4.
For the Cora andCiteseer datasets, the learned weights mono-
tonically decrease, indicating that the graph filter serves as a low-
pass filter. The weight ùõΩ(0)has the largest magnitude, suggesting
that crucial information is primarily contained in node features.
Local topological information from nearby neighbors plays a major
role, while global topological information from distant nodes serves
as a complementary factor. Conversely, for the ogbl-collab and
ogbl-ddi datasets, the weights do not monotonically increase or
decrease. Instead, they experience a significant change, especially in
the first 5 layers, indicating that the graph filter serves as a high-pass
filter. The weight ùõΩ(2)has the largest magnitude, suggesting that
crucial information lies in local topology rather than node features.
Moreover, for large values of ùëôon the ogbl-collab andogbl-ddi
4229KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Juzheng Zhang, Lanning Wei, Zhen Xu, and Quanming Yao
0 5 10 15 20
lth-layer0.000.050.100.150.20(l)
(a)Cora.
0 5 10 15 20
lth-layer0.000.050.100.150.20(l)
 (b)Citeseer.
0246810121416
lth-layer0.3
0.2
0.1
0.00.10.20.30.40.5(l)
(c)ogbl-collab.
0246810121416
lth-layer0.000.250.500.751.001.251.50(l)
 (d)ogbl-ddi.
Figure 4: Learned weights ùõΩ(ùëô)withùêø=20for the Cora
and Citeseer datasets, and ùêø=15for the ogbl-collab and
ogbl-ddi datasets.
Table 7: Total training time (in seconds) for training from
scratch compared to training only the MLP predictor using
the generalized heuristics.
Cora
Citeseer Pubmed collab ddi
Fr
om Scratch 6.3 5.6 150 5,360 8,100
Predictor Only 2.8 2.1 0.8 823 572
datasets, the weights ùõΩ(ùëô)become negative, suggesting that global
topological information from distant nodes compensates for exces-
sive information from nearby neighbors. The learnable weights ùõΩ(ùëô)
govern the trade-off between node features and topological informa-
tion, enabling the adaptive integration of multi-range topological
information.
5.5.2 Leveraging generalized heuristics. With the generalized heuris-
tic for each dataset, there is no need to train a GNN and an MLP
predictor from scratch. Instead, we can simply follow the general-
ized heuristic and train an MLP predictor only, which is significantly
more efficient than training from scratch. The performance of train-
ing the MLP alone is comparable to training from scratch, but it
converges more quickly. The training time for training from scratch
versus training only the MLP is shown in Table 7. The slight de-
crease in performance can likely be attributed to the fact that, when
training the GNN and MLP together, the gradients flow through
both blocks, allowing them to adapt to each other. In contrast, train-
ing the MLP alone limits its ability to capture complex interactions
between the two blocks.
0 5 10 15 20
lth-layer0.00.10.20.30.40.50.60.7(l)
(a) Triangular network.
0 5 10 15 20
lth-layer0.00.10.20.30.40.50.6(l)
 (b) Hexagonal network.
Figure 5: Learned weights ùõΩ(ùëô)withùêø=20for the synthetic
triangular and hexagonal networks.
5.6 Case Study
We construct two synthetic datasets, a triangular network, and a
hexagonal network, to assess HL-GNN‚Äôs ability to learn the most
effective heuristic and obtain the desired range of information.
The triangular network consists of 1000 nodes, with every three
nodes forming a triangle. As each pair of nodes shares two common
neighbors, we anticipate that the learned heuristic would resemble a
local heuristic focusing on 2-hop information. The learned weights
are presented in Figure 5, with ùõΩ(2)having the largest magnitude,
corresponding to a local heuristic.
The hexagonal network also comprises 1000 nodes, with every
six nodes forming a hexagon. Here, we expect the learned heuris-
tic to resemble a global heuristic focusing on 5-hop information.
As shown in Figure 5, the weight ùõΩ(5)has the largest magnitude,
corresponding to a global heuristic. In both cases, HL-GNN demon-
strates its ability to adaptively learn the most effective heuristic
based on the specific topology. This also emphasizes the importance
of developing a formulation that can effectively accommodate both
local and global heuristics.
6 CONCLUSION
We introduce a unified formulation that accommodates and gen-
eralizes both local and global heuristics using propagation oper-
ators and weight parameters. Additionally, we propose HL-GNN,
which efficiently implements this formulation. HL-GNN combines
intra-layer propagation and inter-layer connections, allowing the
integration of multi-range topological information. Experiments
demonstrate that HL-GNN achieves state-of-the-art performance
and efficiency. This study is confined to undirected graphs; for
directed graphs, we preprocess them by converting them into undi-
rected graphs. Extensions to multi-relational graphs, such as knowl-
edge graphs, are left for future work.
ACKNOWLEDGMENT
This work is supported by National Key Research and Development
Program of China (under Grant No.2023YFB2903904), National
Natural Science Foundation of China (under Grant No.92270106)
and Beijing Natural Science Foundation (under Grant No.4242039).
4230Heuristic Learning with Graph Neural Networks: A Unified Framework for Link Prediction KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
REFERENCES
[1]Baole Ai, Zhou Qin, Wenting Shen, and Yong Li. 2022. Structure enhanced graph
neural networks for link prediction. arXiv preprint arXiv:2201.05293 (2022).
[2]Albert-L√°szl√≥ Barab√°si and R√©ka Albert. 1999. Emergence of scaling in random
networks. science 286, 5439 (1999), 509‚Äì512.
[3]Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual
web search engine. Computer networks and ISDN systems 30, 1-7 (1998), 107‚Äì117.
[4]Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca,
Thomas Markovich, Nils Hammerla, Michael M Bronstein, and Max Hansmire.
2022. Graph neural networks for link prediction with subgraph sketching. arXiv
preprint arXiv:2209.15486 (2022).
[5]Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2021. Adaptive Universal
Generalized PageRank Graph Neural Network. In International Conference on
Learning Representations.
[6] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD international conference on
Knowledge discovery and data mining. 855‚Äì864.
[7]Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. Advances in neural information processing systems 30
(2017).
[8]Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for
machine learning on graphs. Advances in neural information processing systems
33 (2020), 22118‚Äì22133.
[9]Leo Katz. 1953. A new status index derived from sociometric analysis. Psychome-
trika 18, 1 (1953), 39‚Äì43.
[10] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[11] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-
niques for recommender systems. Computer 42, 8 (2009), 30‚Äì37.
[12] Elizabeth A Leicht, Petter Holme, and Mark EJ Newman. 2006. Vertex similarity
in networks. Physical Review E 73, 2 (2006), 026120.
[13] Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and Junzhou Huang.
2019. Semi-supervised graph classification: A hierarchical graph perspective. In
The World Wide Web Conference. 972‚Äì982.
[14] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph
convolutional networks for semi-supervised learning. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 32.
[15] Weiping Liu and Linyuan L√º. 2010. Link prediction based on local random walk.
Europhysics Letters 89, 5 (2010), 58007.
[16] Linyuan L√º, Ci-Hang Jin, and Tao Zhou. 2009. Similarity index based on local
paths for link prediction of complex networks. Physical Review E 80, 4 (2009),
046122.
[17] Linyuan L√º and Tao Zhou. 2011. Link prediction in complex networks: A survey.
Physica A: statistical mechanics and its applications 390, 6 (2011), 1150‚Äì1170.
[18] V√≠ctor Mart√≠nez, Fernando Berzal, and Juan-Carlos Cubero. 2016. A survey of
link prediction in complex networks. ACM computing surveys (CSUR) 49, 4 (2016),
1‚Äì33.
[19] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning
of social representations. In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. 701‚Äì710.
[20] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. 2019. Dropedge:
Towards deep graph convolutional networks on node classification. arXiv preprint
arXiv:1907.10903 (2019).[21] Nir Rosenfeld, Ofer Meshi, Danny Tarlow, and Amir Globerson. 2014. Learn-
ing structured models with the AUC loss and its generalizations. In Artificial
Intelligence and Statistics. PMLR, 841‚Äì849.
[22] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93‚Äì93.
[23] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan
G√ºnnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint
arXiv:1811.05868 (2018).
[24] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. Line: Large-scale information network embedding. In Proceedings of the
24th international conference on world wide web. 1067‚Äì1077.
[25] Komal Teru, Etienne Denis, and Will Hamilton. 2020. Inductive relation prediction
by subgraph reasoning. In International Conference on Machine Learning. PMLR,
9448‚Äì9457.
[26] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, Yoshua Bengio, et al .2017. Graph attention networks. stat1050, 20 (2017),
10‚Äì48550.
[27] Yaqing Wang, Zaifei Yang, and Quanming Yao. 2024. Accurate and interpretable
drug-drug interaction prediction enabled by knowledge subgraph learning. Com-
munications Medicine 4, 1 (2024), 59.
[28] Lanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. 2021. Pooling archi-
tecture search for graph classification. In Proceedings of the 30th ACM International
Conference on Information & Knowledge Management. 2091‚Äì2100.
[29] Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Node-
former: A scalable graph structure learning transformer for node classification.
Advances in Neural Information Processing Systems 35 (2022), 27387‚Äì27401.
[30] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. In International conference on machine learn-
ing. PMLR, 5453‚Äì5462.
[31] Haoteng Yin, Muhan Zhang, Yanbang Wang, Jianguo Wang, and Pan Li. 2022. Al-
gorithm and system co-design for efficient subgraph-based graph representation
learning. arXiv preprint arXiv:2202.13538 (2022).
[32] Seongjun Yun, Seoyoon Kim, Junhyun Lee, Jaewoo Kang, and Hyunwoo J Kim.
2021. Neo-gnns: Neighborhood overlap-aware graph neural networks for link
prediction. Advances in Neural Information Processing Systems 34 (2021), 13683‚Äì
13694.
[33] Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural
networks. Advances in neural information processing systems 31 (2018).
[34] Yongqi Zhang and Quanming Yao. 2022. Knowledge graph reasoning with
relational digraph. In Proceedings of the ACM web conference 2022. 912‚Äì924.
[35] Yongqi Zhang, Quanming Yao, Ling Yue, Xian Wu, Ziheng Zhang, Zhenxi Lin,
and Yefeng Zheng. 2023. Emerging drug interaction prediction enabled by a flow-
based graph neural network with biomedical network. Nature Computational
Science 3, 12 (2023), 1023‚Äì1033.
[36] Tao Zhou, Linyuan L√º, and Yi-Cheng Zhang. 2009. Predicting missing links via
local information. The European Physical Journal B 71 (2009), 623‚Äì630.
[37] Huaisheng Zhu, Dongsheng Luo, Xianfeng Tang, Junjie Xu, Hui Liu, and Suhang
Wang. 2023. Self-Explainable Graph Neural Networks for Link Prediction. arXiv
preprint arXiv:2305.12578 (2023).
[38] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. 2021.
Neural bellman-ford networks: A general graph neural network framework for
link prediction. Advances in Neural Information Processing Systems 34 (2021),
29476‚Äì29490.
4231