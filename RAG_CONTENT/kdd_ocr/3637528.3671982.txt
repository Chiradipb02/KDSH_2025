ZeroG: Investigating Cross-dataset Zero-shot Transferability in
Graphs
Yuhan Li
HKUST (GZ)
Guangzhou, China
yli258@connect.hkust-gz.edu.cnPeisong Wang
THU
Shenzhen, China
wps22@mails.tsinghua.edu.cnZhixun Li
CUHK
Hong Kong SAR, China
zxli@se.cuhk.edu.hk
Jeffrey Xu Yu
CUHK
Hong Kong SAR, China
yu@se.cuhk.edu.hkJia Li∗
HKUST (GZ)
Guangzhou, China
jialee@ust.hk
ABSTRACT
With the development of foundation models such as large language
models, zero-shot transfer learning has become increasingly sig-
nificant. This is highlighted by the generative capabilities of NLP
models like GPT-4, and the retrieval-based approaches of CV mod-
els like CLIP, both of which effectively bridge the gap between seen
and unseen data. In the realm of graph learning, the continuous
emergence of new graphs and the challenges of human labeling
also amplify the necessity for zero-shot transfer learning, driving
the exploration of approaches that can generalize across diverse
graph data without necessitating dataset-specific and label-specific
fine-tuning. In this study, we extend such paradigms to Zero -shot
transferability in Graphs by introducing ZeroG , a new framework
tailored to enable cross-dataset generalization. Addressing the in-
herent challenges such as feature misalignment, mismatched label
spaces, and negative transfer, we leverage a language model to en-
code both node attributes and class semantics, ensuring consistent
feature dimensions across datasets. We also propose a prompt-based
subgraph sampling module that enriches the semantic information
and structure information of extracted subgraphs using prompt-
ing nodes and neighborhood aggregation, respectively. We further
adopt a lightweight fine-tuning strategy that reduces the risk of
overfitting and maintains the zero-shot learning efficacy of the
language model. The results underscore the effectiveness of our
model in achieving significant cross-dataset zero-shot transferabil-
ity, opening pathways for the development of graph foundation
models1.
CCS CONCEPTS
•Computing methodologies →Artificial intelligence.
∗Corresponding author.
1Codes and data are available at https://github.com/NineAbyss/ZeroG
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD’24, August 25-29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671982KEYWORDS
cross-dataset; zero-shot; graph transfer learning
ACM Reference Format:
Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li∗. 2024. ZeroG:
Investigating Cross-dataset Zero-shot Transferability in Graphs. In KDD’24:
SIGKDD Conference on Knowledge Discovery and Data Mining, August 25-29,
2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/
10.1145/3637528.3671982
1 INTRODUCTION
When encountering a completely new problem, humans typically
start to compare and connect it with the knowledge that they are
familiar with [ 4]. The same idea is also applied to Machine Learning
(ML). ML methods usually focus on classifying instances whose
classes have already been seen in training. However, in practice,
many applications necessitate classifying instances whose classes
have not been seen previously, requiring models to utilize the knowl-
edge gained before to begin reasoning and problem-solving. For
example, a model trained on animal images may have never seen
a “zebra” during its training. Yet, by understanding the concept of
“stripes” and “horse-like” animals from its training, it can success-
fully identify a zebra when it encounters one for the first time.
This trend of zero-shot capabilities in ML, particularly after the
advent of foundation models such as large language models (LLMs),
has demonstrated considerable advancements in the field of AI.
Zero-shot learning is a powerful learning paradigm, in which the
classes covered by training instances and the classes we aim to
classify are disjoint. In the natural language processing (NLP) field,
zero-shot learning is unified by a generative paradigm, with LLMs
such as GPT-4 [ 1] and LLaMA [ 53] tackling new data they haven’t
been explicitly trained based on their extensive pre-training on
massive corpora. While in the computer vision (CV) field, zero-shot
learning relies on a retrieval paradigm, where models like CLIP
[39] map images to text in a shared space, allowing recognition of
new images by their conceptual similarity to label descriptions.
Graphs are prevalent across multiple disciplines with diverse
applications [ 26,29,50,51,60,64,65,68]. However, we notice that
graph learning faces two major challenges: (1)the emergence of
new graphs , which makes it impractical to train graph models
like Graph Neural Networks (GNNs) [ 25,55] on each individual
graph; (2)the difficulty of human labeling due to the complex
and diverse nature of graph-structured data. Therefore, zero-shot
1725
KDD’24, August 25-29, 2024, Barcelona, Spain Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li∗
learning is worth exploring in graph learning, as it allows graph
models to generalize and perform reasoning over graphs that have
never been seen before. This is also essential for reaching the goal
of graph foundation models that can adapt to different data without
extra fine-tuning. Due to the lack of detailed discussion and analysis
on this task in prior work, our goal is to investigate zero-shot
transferability in graphs for conducting cross-dataset transfers,
which involves a pre-training phase with multiple datasets that
have non-overlapping classes with the test datasets. According to
recent surveys [ 27,32], we consider this setting well-suited for
the graph foundation model era, emphasizing broad generalization
across different data sources. We offer a comprehensive discussion
on how our setting differs with other related tasks in Section 2.
Graph transfer learning usually follows a “pre-training and fine-
tuning” strategy, which aims to learn some general knowledge
for the graph model with easily accessible information to reduce
the annotation costs of new graphs [ 66]. Effective pre-training
techniques include node-level comparison [ 69] or reconstruction
[10,19], edge-level pretext like edge prediction [ 23], and graph-level
contrastive learning such as GraphCL [ 63]. However, to perform
zero-shot transfer across datasets, the increased number of pre-
training datasets and the absence of labels for the target dataset
present unique challenges for these transfer learning methods.
Primary challenges. Traditional GNNs face challenges in zero-
shot transfer primarily due to dimension misalignment, mismatched
label spaces, and negative transfer. Dimension misalignment arises
when different datasets use varying shallow embedding techniques,
such as bag-of-words or TF-IDF [ 41], leading to feature dimensions
that are incompatible across datasets. This makes it difficult for a
model trained on one dataset to adapt to another with different
feature dimensions. Another issue is mismatched label spaces. A
GNN trained on a dataset with a certain number of classes might fail
on a dataset with a varying number of classes or where classes with
the same count signify different meanings. In addition, negative
transfer can also occur when GNNs overfit to the peculiarities of
the training data, reducing its ability to generalize to new datasets
with distinct structures or semantics [ 47]. Recently, efforts have
been made to use LLMs directly as classifiers for zero-shot inference
in graphs [ 9,22]. For example, for each node (i.e., one paper) in a
citation network, we can directly input the title and abstract of the
paper into a LLM and then ask which category the paper belongs
to. However, it faces two main challenges. The first issue is data
leakage. LLMs pre-trained on comprehensive text corpora may have
been exposed to parts of the target datasets, possibly leading to an
unfair advantage in their performance [ 18]. Secondly, it is hard for
LLMs to incorporate graph structure, which is vital for graph tasks
such as node classification that depend on both the attributes of
the nodes and the connections between them [ 30]. We provide a
more detailed discussion of the challenges in Section 3.
Presented work. We introduce ZeroG, a framework designed for
tackling cross-dataset zero-shot transfer in graphs. Specifically, to
address the dimension misalignment challenge, we propose unify-
ing the graph representation learning through a pre-trained lan-
guage model (LM) to encode both text attributes associated with
nodes and descriptions associated with classes. This approach maps
the node and class features of various datasets to a unified semanticspace of the same dimension, thereby addressing the issue of dimen-
sion misalignment in cross-dataset transfer. To address the issue
of mismatched label spaces, we reformulate the node classification
task into a text similarity task in both pre-training and inference
phases. To tackle the negative transfer challenge, we introduce a
prompt-based subgraph sampling strategy that enhances the gen-
eral semantic information of datasets by prompting nodes while
incorporating structural information through simple neighborhood
aggregation. Additionally, we employ a lightweight fine-tuning
approach to ensure suitability for pre-training datasets, minimizing
the risk of overfitting and preserving the LM’s zero-shot learning
capabilities.
Our main contributions can be summarized as follows: (1) Com-
prehensive Analysis. We are the first to systematically summarize
and discuss the existing attempts and challenges associated with
cross-dataset zero-shot transferability in graphs (Section 2 and 3).
(2)Architecture Design. We proposed ZeroG, a model designed for
the lightweight training of language models to enable zero-shot
transfer across diverse text-attribute graphs (Section 4). (3) Supe-
rior Performance. Our model has demonstrated effective zero-shot
transferability across seven well-known benchmark datasets, i.e.,it
can even achieve results comparable to semi-supervised methods
on Pubmed (Section 5).
2 PRELIMINARIES
Notations. To maintain consistency of notations, we use bold up-
percase and lowercase letters to represent matrices and vectors, and
calligraphic font types to denote sets. Given a Graph G=(V,A,T),
whereV={𝑣1,𝑣2,...,𝑣 𝑁}is the set of 𝑁nodes; A∈{0,1}𝑁×𝑁is
the adjacency matrix, if 𝑣𝑖and𝑣𝑗are connected, A𝑖 𝑗=1, otherwise
A𝑖 𝑗=0;T={𝑡1,𝑡2,...,𝑡 𝑁}is the set of node attributes, and each
node𝑣𝑖is associated with attributes 𝑡𝑖. Typically, in most previous
graph machine learning literature, such attribute information can
be encoded into shallow embeddings X=[x1,x2,..., x𝑁]∈R𝑁×𝑓
by naive methods (e.g., bag-of-words or TF-IDF [ 41]), where𝑓is
the dimension of embeddings.
Existing Explorations. Most of the existing works in node classifi-
cation task use labeled nodes to train GNNs to predict the unlabeled
nodes with the same label space of training set on a single graph,
which we refer to in-dataset semi-supervised learning. However,
these works of literature ignore the generalization of GNNs to a
completely new graph dataset. In recent years, several works have
started to focus on the transferability of GNNs, and there are two
popular scenarios: unsupervised graph domain adaptation (UGDA)
and in-dataset zero-shot. Specifically, let G𝑠=(V𝑠,A𝑠,X𝑠)and
G𝑡=(V𝑡,A𝑡,X𝑡)be source graph and target graph. UGDA aims to
learn a classification model on a fully labeled source graph G𝑠, and
accurately classify the nodes in the target graph G𝑡with the same
label space, which can be denoted as G𝑠∩G𝑡=∅,Y𝑠=Y𝑡. While
in-dataset zero-shot focuses on transferability within a single graph,
whose objective is categorizing unlabeled nodes into unseen classes
within target label space Y𝑡, given that all labeled nodes belong to
the seen classesY𝑠, which can be denoted as G𝑠=G𝑡,Y𝑠∩Y𝑡=∅.
Cross-dataset zero-shot. Recently, a multitude of models that
demonstrate cross-dataset transferability have emerged in fields like
1726ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs KDD’24, August 25-29, 2024, Barcelona, Spain
Unsupervised Graph
Domain Adaptation
In-dataset
Zero -shot Transfer
Cross -dataset
Zero -shot Transfer
Figure 1: Analysis of tasks associated with the zero-shot trans-
fer in graphs.
NLP and CV. In this work, we mainly focus on cross-dataset zero-
shot node classification task. We aim to train a classification model
learned on fully labeled source graph G𝑠and generate satisfactory
predictions on a completely different target graph G𝑡with distinct
label spaceY𝑡, which can be denoted as G𝑠∩G𝑡=∅,Y𝑠∩Y𝑡=∅.
The difference between cross-dataset zero-shot and the above two
scenarios is illustrated in Figure 1. In fact, both UDGA and in-
dataset zero-shot scenarios have significant limitations in practical
applications. For instance, UGDA requires consistent label space
between the source and target graphs, making it unable to handle
downstream inference with unseen classes. In-dataset zero-shot can
only perform zero-shot within a single graph, and such methods
will become ineffective when encountering a completely new graph.
Cross-dataset zero-shot is a very practical scenario, but it is largely
under-explored.
3 CHALLENGES
❶Why do GNNs fall short in cross-dataset zero-shot node classi-
fication? (1) Dimension misalignment . During pre-training on
multiple datasets, aligning feature dimensions is crucial for graph
models to handle data consistently. Currently, the use of shallow
embeddings such as bag-of-words, skip-gram [ 38], and TF-IDF [ 41]
in mainstream benchmark datasets can lead to dimension misalign-
ment issues across different source datasets (e.g., Cora with 1433
dimensions, while Citeseer with 3703 dimensions). This is hard to
perform zero-shot transfer due to a graph model pre-trained on one
dataset may struggle to process the other due to this inconsistency
in feature dimension. In addition, dimension misalignment between
source datasets and the target datasets can also make it challenging
to apply the model directly to the downstream dataset, as the dimen-
sions it has learned to process may not equal to those it encounters
during transfer. (2) Mismatched label spaces. A GNN’s classifica-
tion head, fixed to the number of classes seen during pre-training,
cannot easily adapt to a different number of classes in the target
dataset, leading to potential mismatched issues. Additionally, even
when the number of labels is the same, labels may carry different
meanings across datasets. For example, the categories in citation
networks may not easily translate to those in web link datasets due
to the distinct contexts they represent. (3) Negative transfer. In
graph data, negative transfer continues to be a topic of enduring
discussion [ 15,23,47]. It often occurs when there is a significant
disparity in structure or semantics between graphs. Fully adapting
graph models to upstream datasets often causes overfitting, where
the model becomes too specialized to the pre-training data’s charac-
teristics. This may hurt the model’s performance on target datasetsthat differ in structure or meaning, as it may fail to capture the
broader patterns necessary for zero-shot transfer learning.
❷Why do large language models fall short in cross-dataset zero-shot
node classification? (1) Data leakage. Data leakage in LLMs has be-
come a focal point of discussion [ 2]. Given that LLMs such as GPT-4
[1] and LLaMA [ 53] undergo pre-training on extensive text corpora,
it’s likely that they may have seen and memorized at least part of
the test data of the common benchmark datasets. This can lead to an
overestimation of the capabilities of language models during evalu-
ation [ 9,18,22].(2) Lack of structural information. In the node
classification task, the prediction of a node is determined by both
its own features and the contextual subgraph. However, language
models typically rely on a textual item for prediction and do not
consider structural information, which will jeopardize the utility of
language models in node classification task. In recent years, several
works have designed an auxiliary task (e.g., neighbor prediction
or link prediction) to provide structural awareness capability to
language models [ 11,13]. Although they can effectively improve
the accuracy of node classification on text-attributed graphs, they
all require employing a GNN on top of the language model for pre-
diction, which limits their ability to perform cross-dataset zero-shot
learning. In this paper, we attempt to tackle the aforementioned
issues and strive to mitigate them.
4 METHODOLOGY
Our proposed ZeroG posits a paradigm shift in zero-shot graph
learning, drawing inspiration from the adaptive nature of LLMs.
By fine-tuning a relatively small language model in a low-resource
setting on source datasets for pre-training, ZeroG is capable of
exhibiting substantial zero-shot learning abilities on downstream
target datasets. Figure 2 illustrates the pipeline of ZeroG, which
can be divided into three parts. Firstly, we integrate the encoding of
node features and class descriptions within the graph via employing
language models, achieving a standardized representation frame-
work. In the second part, we extract diverse subgraphs from source
datasets and organically integrate dataset-specific information into
subgraphs leveraging a graph prompting paradigm. Finally, a light-
weight alignment projector is adapted to map source graph features
into the original word embedding space.
4.1 Unified Graph Representation
The most mainstream benchmark datasets (e.g., Cora and Pubmed)
often adopt naive methods to encode node attribute information
using shallow embeddings, such as bag-of-words, skip-gram [ 38],
or TF-IDF [41]. However, in the zero-shot setting, traditional shal-
low embeddings often encounter issues with feature dimension
alignment across different datasets (e.g., Cora has 1433 dimensions,
while Citeseer has 3703 dimensions). This misalignment leads to
two main challenges: (1) Inconsistency in pre-training. During
the pretraining phase, aligning different source datasets becomes
problematic due to varying feature dimensions, hindering the de-
velopment of a coherent model understanding. (2) Difficulty in
applying to target datasets. The lack of uniformity in feature
dimensions between source and target datasets poses significant
challenges in applying pre-trained models effectively to new, un-
seen data.
1727KDD’24, August 25-29, 2024, Barcelona, Spain Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li∗
Theory : This category covers 
theoretical aspects of 
machine learning and AI.
Reinforcement Learning : 
This category includes...
Title: Total Text A 
Comprehensive Dataset ...
Abstract : Text in curve 
orientation, despite being one 
of the common text ...
LMsSource
Textual Item
Source
Class Description
Text Embeddings
Class Embeddings
...Restricted
Extraction
Neighbor -aware
EmbeddingsUpstream Pre -training
Neighbor -aware
Embeddings
Pretrained
Weights
A
LoRA Fine -Tuning
Output
Embeddings
Semantic 
Similarity Loss
Downstream InferenceB
LMsTarget
Textual Item
Target
Class Description
Prompt -based
Graph
…(a)
(b)
: Prompting Node
: Central Node
Source Graph
Figure 2: Our proposed pipeline ZeroG facilitates cross-dataset zero-shot node classification with three key compo-
nents: (a) It uses unified graph representations to merge node and class encodings via language models. (b) It employs
prompt-based subgraph sampling to create pre-training data from rich subgraphs. (c) Its upstream pre-training adopts a
parameter-efficient approach to suit various datasets while preserving zero-shot abilities and preventing overfitting. Finally,
we can leverage the pre-trained model to perform downstream inference on target datasets.
To address the two challenges mentioned above, we employ a
unified pre-trained LM to encode both node attributes and descrip-
tions associated with classes. Specifically, let 𝑡𝑛denote the node
attributes of 𝑣,𝑡𝑐denote the descriptions of class 𝑐, and LMbe a
pre-trained LM. The representations of node attributes and class
descriptions can be obtained by applying the LMas follows:
h𝑛=LM(𝑡𝑛)∈R𝑑,h𝑐=LM(𝑡𝑐)∈R𝑑, (1)
where h𝑛andh𝑐represent the outputs from the final hidden layer
corresponding to the [CLS] token for inputs 𝑡𝑛and𝑡𝑐, respectively,
and𝑑denotes the dimension of the hidden layer in the LM. An ex-
ample of node features and class descriptions is shown in Appendix
B.1. By doing so, we map the node and class features of various
datasets to a unified semantic space and the same dimension 𝑑,
addressing the issue of dimension misalignment in cross-dataset
transfer.
4.2 Prompt-based Subgraph Sampling
To facilitate zero-shot transfer capabilities in target datasets, trans-
ferring both structural and semantic information from the source
datasets is crucial, ensuring that target datasets are not only en-
riched with the foundational structural patterns but also imbued
with the contextual semantic nuances from the source [ 52]. In
ZeroG, we introduce a novel prompt-based subgraph sampling
strategy, which captures essential structural and semantic features
by extracting subgraphs from source graphs to construct the pre-
training set and introduce a prompt node that enriches the unique
semantics of these source datasets.
4.2.1 Restricted Extraction. Given that source datasets can be nu-
merous and varied, with significant variations in graph size, density,
node types, edge types, and overall topology, we employ restricted
subgraph extraction to avoid overly simplistic subgraphs and limit
the number of subgraphs extracted. For example, a subgraph that istoo small will not provide enough structural information for dataset
transfer. Similarly, if a subgraph only represents few classes, even
a single class, it may fall short in offering the diverse semantic
guidance needed for transfer.
For each source dataset, we iteratively extract 𝑘-hop subgraphs
using every node as a center. Adjusting 𝑘provides flexibility in
controlling both individual subgraph sizes and the overall number
of subgraphs, tailored to the dataset’s specific characteristics. Next,
we apply a class-based filtering criterion in which we only consider
those subgraphs where the number of distinct classes surpasses
half of the total classes in the dataset, i.e., for a subgraph 𝑠of
source dataset 𝑣, it holds that|𝐶𝑣𝑠|≥1
2|𝐶𝑣|, where|𝐶𝑣𝑠|denotes the
cardinality of the set of distinct classes in 𝑠and|𝐶𝑣|denotes the
cardinality of the set of all classes in 𝑣. This criterion guarantees that
each subgraph meaningfully reflects the dataset’s class diversity,
facilitating a thorough transfer of the source dataset’s fundamental
semantic content.
4.2.2 Prompting Node. Graph prompt techniques have been exten-
sively explored for low-resource graph learning. For instance, graph
prompts leveraged in well-known graph transfer models such as
ProG [ 47] and GraphPrompt [ 35] are randomly initialized and then
made learnable, adapting specifically to the source dataset they
are connected with. However, existing graph prompting methods
struggle in zero-shot scenarios because inserting a trained dataset-
specific prompt graph into a target graph is impractical, especially
when dealing with multiple source datasets. In addition, the initial
random state of graph prompts lacks the rich semantic information
that characterizes both the source datasets and target datasets.
InZeroG, we introduce semantic-enhanced prompting nodes
as unique identifiers that carry general knowledge relevant to spe-
cific datasets. To create the prompting node for each dataset, we
design a general description that serves as the text attribute for
the corresponding prompting node, encompassing the fundamental
1728ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs KDD’24, August 25-29, 2024, Barcelona, Spain
attributes of the dataset it represents. More details about the dataset
descriptions can be found in Appendix B.2. We then integrate the
prompting node into each extracted subgraph in the pre-training
set as a unique identifier, providing more general semantics of the
source dataset. The insertion pattern we employ is fully connected,
that is, for each subgraph, the prompting node of the source dataset
to which the subgraph belongs is connected to all nodes within the
subgraph. The representation of the prompting node is initialized
by the same LM described in Section 4.1.
4.2.3 Neighborhood Aggregation. Formally, supposing that one
subgraph has adjacency matrix A, degree matrix Mand feature
matrix Hprocessed by LM. Neighborhood aggregation involves
the normalization of the adjacency matrix and subsequent aggrega-
tion. Initially, a self-loop is added to each node in the graph. This
step integrates the node’s own features with those of its neighbors.
Subsequently, the normalized adjacency matrix of the subgraph
is then calculated as: Anorm =M−1
2AM−1
2. Using this normal-
ized adjacency matrix, neighborhood aggregation is performed
iteratively. In each iteration, the node features are updated by ag-
gregating features from their immediate neighbors, including the
nodes themselves. This process is mathematically represented as
H(𝑡+1)=AnormH(𝑡), where H(𝑡)denotes the feature matrix at
iteration𝑡. The iteration continues for a predefined number of
steps, allowing the node embeddings to progressively integrate
information from their extended neighborhood.
4.3 Upstream Pre-training
One significant concern of pre-training LMs for source dataset
compatibility is the resource-intensive nature of conventional full-
parameter pre-training [ 5], which also carries a substantial risk of
overfitting, especially in zero-shot scenarios [ 7,8]. To circumvent
these limitations, we utilize LoRA [ 20], a parameter-efficient pre-
training strategy, which ensures suitability for upstream datasets
while minimizing overfitting risks and preserving the LM’s zero-
shot learning abilities.
LoRA injects trainable low-rank matrices into transformer layers
of LM to approximate the weight updates. Consider a pre-trained
weight matrix W∈R𝑑×𝑘. LoRA updates this matrix with a low-
rank decomposition expressed as W+Δ𝑊=W+WdownWup,
where Wdown∈R𝑑×𝑟andWup∈R𝑟×𝑘are the trainable low-rank
matrices. Specifically, we apply these updates to the query and value
projection matrices, W𝑞andW𝑣, within the multi-head attention
sub-layer. For an input xto this linear projection, LoRA modifies
the resulting projection output hin the following:
h=h+𝛼·xW downWup, (2)
where𝛼≥1is a tunable scalar hyperparameter. Based on LoRA,
we utilize a cross-entropy loss for pre-training. Formally, denoting
the total pre-training set as Tpreand the node set of the subgraph
𝑠as𝑁𝑠, the loss function is defined as follows:
Lpre(Θ)=−∑︁
𝑠∈Tpre∑︁
𝑛∈𝑁𝑠logexp sim h𝑛,h𝑦𝑛
Í
𝑐∈𝑌𝑠exp(sim(h𝑛,h𝑐)),(3)
where𝑦𝑛represents the actual label of node 𝑛, and𝑌𝑠represents
the set of classes associated with subgraph 𝑠.sim(·)is a similarity
measurement used to calculate the semantic similarity betweennode embeddings and class embeddings, where we employ the dot
product here. Note that the loss is parameterized by Θ, representing
the weights of the low-rank matrices. The goal of learning is to
minimize the lossLpre.
4.4 Downstream Inference
After pre-training on source datasets, we acquire a pre-trained
LM enriched with upstream semantics and structure. This LM is
designed for zero-shot transferability, allowing direct application
to target datasets without any fine-tuning. We try to unify the
pre-training and inference stages, enabling effective knowledge
transfer as the datasets in the two phases are made more compatible
by following a common template. However, it is still important
to distinguish different downstream datasets, in order to capture
dataset individuality and achieve dataset-specific optimum.
Briefly, given a target dataset, we first apply the LM to gener-
ate embeddings for its nodes and classes, respectively. Creating a
universal prompting node and updating node embeddings through
neighborhood aggregation allows our LM to capture the target
dataset’s unique global context and local structures. Finally, the
class that yields the highest similarity score is predicted to be the
class of the node, which can be formalized as follows:
𝑦′=argmax𝑖(sim(h𝑛,h𝑐𝑖)|𝑖∈{1,...,𝑁}), (4)
where𝑦′is the predicted label of the node 𝑛and𝑁denotes the
total number of classes. For sim(·), we also use the dot product as
the similarity function here.
5 EXPERIMENTS
We evaluate ZeroG on real-world datasets for node classification to
assess its performance in enhancing cross-dataset zero-shot graph
transfer learning. In particular, we wish to answer the following
research questions: Q1: How effective is ZeroG in a zero-shot
learning scenario when the source and target datasets are within
the same domain? Q2: How adaptable is our model when applied
across different domains? Q3: How do the main components of
our model impact the performance? Q4: What is the impact of
hyper-parameter on performance? Q5: How does ZeroG compare
to state-of-the-art methods in training efficiency?
5.1 Experimental Protocols
5.1.1 Datasets. Our experiments are conducted on several public
graph datasets. First we consider four commonly used citation net-
works, i.e.,Cora [ 36], Citeseer [ 17], Pubmed [ 42], and ogbn-arxiv
[21]. To ensure data distribution practices in NLP and CV, where
source datasets are typically larger than target datasets, we do not
consider ogbn-arxiv as a target dataset but only employ it as one of
the source datasets. Apart from this, to further explore in-domain
transfer, we develop two co-purchase networks from ogbn-products
[21], named P-Home and P-Tech. We carefully curate these datasets
to ensure that they are non-overlapping with respect to their classes.
More details about these two datasets can be found in Appendix
A. In addition, in cross-domain transfer, we introduce another web
linkdataset Wiki-CS [ 37] to provide additional domain variance.
For a fair comparison, we follow OFA [ 31] to provide Cora, Pubmed,
ogbn-arxiv, and Wiki-CS with texts, both for nodes and classes. For
1729KDD’24, August 25-29, 2024, Barcelona, Spain Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li∗
Table 1: Statistics of datasets.
Dataset #
Domain #
Nodes #
Edges #
Avg. D #
Classes
Cora Citation 2,708 5,429 4.00 7
Citese
er Citation 3,186 4,277 2.57 6
Pubme
d Citation 19,717 44,338 4.50 3
ogbn-ar
xiv Citation 169,343 1,166,243 13.77 40
P-Home Co-pur
chase 9,790 131,841 26.93 5
P-
Tech Co-pur
chase 47,428 2,077,241 87.60 3
Wiki-CS W
eb link 11,701 216,123 36.94 10
CiteSeer, P-Home, and P-Tech, we use raw texts processed by Chen
et al. [9].
5.1.2 Baselines. We note that few methods can be directly em-
ployed to perform zero-shot transfer across different graphs. In this
case, we make adaptations of commonly used models or techniques
with zero-shot capabilities, to serve as our baselines. Our base-
lines can be organized into three main categories: (1) Graph self-
supervised learning-based methods. Graph SSL focuses on the
transferability of learned structures without the need of labels. We
here consider three prominent graph SSL approaches including DGI
[56], GraphCL [ 63], and GraphMAE [ 19]. For each target dataset,
we employ GNN trained with SSL techniques on its own structure
and then apply Eq. 4 to perform zero-shot classification, without an
explicit classification head. (2) Semantic similarity-based meth-
ods. Intuitively, we can perform zero-shot classification by directly
calculating the semantic similarity between each node and class and
then choosing the class with the highest similarity as the prediction
for that node. We leverage four widely used LM as the baseline en-
coders, such as BERT [ 12], RoBERTa [ 34], E5 [ 58], and SentenceBert
[40]. (3) Graph foundation-based methods. OFA [ 31] proposes
a unified way for cross-dataset transfer on graph data. It represents
all nodes and edges as human-readable texts and utilizes a single
LM to embed these textual descriptions from various datasets into
a shared embedding space. A single GNN is then pre-trained on
multiple graphs, and its capability is evaluated in zero-shot learning
scenarios. Additionally, we report semi-supervised results on our
dataset with GCN [25] and GAT [55].
5.1.3 Implementations. To make a fair comparison, we follow OFA
[31] to utilize SentenceBert [ 40] as our LM. The dimension 𝑑of
both node and class representations is 768, and Adam [ 24] with an
initial learning rate 1e-4 is utilized as the optimizer. We apply a
weight decay of 0.01 during pre-training. The maximum sequence
length is set to 256. The rank and scaling factor of the LoRA adapter
are set to 4 and 16, respectively. Dropout with a probability of 0.1
is used to alleviate over-fitting. The batch size is set to 1 since each
batch processes a single subgraph. The iterations of neighborhood
aggregation 𝜆and the hops of extracted subgraph 𝑘are variable,
depending on the size and sparsity of the dataset. It is noted that
since we do not have a validation set, we fix the number of epochs
(i.e.,3) for all datasets to ensure a fair comparison. All experiments
are implemented by PyTorch Framework with a single NVIDIA
A800 (80G) GPU.Table 2: Test accuracy (%) on target graphs for in-domain
transferability. For each citation network (resp. co-purchase
network) as target dataset, we consider the other citation
networks (resp. co-purchase networks) as source datasets. A
andSindicate whether structural and semantic information
are used, respectively. GCN* and GAT* report results under
the semi-supervised setting, which, except for P-Home and
P-Tech, are all directly obtained from the original papers.
Metho
dsA
S Cora
Pubmed Citeseer P-Home
P-Tech
zer
o-shot settings
DGI
[56] ✓
✗ 19.97
43.89 21.12 33.06
55.83
GraphCL [63] ✓
✗ 26.22
43.73 20.59 37.44
62.63
GraphMAE [19] ✓
✗ 34.79 48.23
34.62 37.04 73.37
BERT
[12] ✗
✓ 19.90
34.79 23.76 37.32
56.44
RoBERTa [34] ✗
✓ 28.91
27.33 30.95 35.50
66.31
E5 [58] ✗
✓ 39.70
41.93 45.89 57.56
59.17
Sent-BERT [40] ✗
✓ 52.25
41.71 47.52 63.22
67.21
OF
A [31] ✓
✓ 27.07
37.87 37.92 32.86
71.03
ZeroG (
ours) ✓
✓68.72 78.02 64.94 73.20 82.96
semi-sup
ervised settings
GCN*
[25] -
-81.50
79.00 70.30 73.85
93.28
GAT* [55] -
-83.00
79.00 72.50 73.46
88.89
5.2 In-domain Transferability (RQ1)
We first consider transferring within citation networks or co-purchase
networks. The results are reported in Table 2 where for each cita-
tion network (resp. co-purchase network) as the target dataset, we
use the other citation networks (resp. co-purchase networks) as
the source datasets. It is observed that ZeroG achieves significant
performance gains on all the target datasets within both citation
networks and co-purchase networks compared to baseline mod-
els, demonstrating the efficacy of our model. Remarkably, ZeroG
demonstrates its effectiveness by achieving 78.02% accuracy on
the Pubmed dataset. This performance is even on par with that
of two semi-supervised learning methods, both of which boast an
accuracy of 79.00%. Compared to methods that focus solely on
structure, such as graph SSL-based methods, and those that rely
purely on text similarity, like semantic similarity-based methods,
our model achieves a comprehensive utilization of both structural
and semantic information to facilitate zero-shot transfer. OFA [ 31],
however, does not perform as well as our model or even other
baselines that concentrate on either structures or semantics alone.
The possible reasons are two-fold. Firstly, by freezing the LM pa-
rameters, OFA restricts the semantic space and domain-specific
tuning, thereby failing to achieve satisfactory zero-shot transfer-
ability. Secondly, OFA employs a generic linear layer for all classes,
that is, the probability associated with each node for every class is
derived through shared parameters. Since class semantics are often
unique, this shared approach can result in insufficient variation
between classes. In contrast, ZeroG computes dot product simi-
larity between nodes and classes directly, preserving the inherent
characteristics of each class. We also note that a higher number of
pre-training classes relative to target classes, such as transferring
from Arxiv+Cora+Citeseer to Pubmed or from P-Home to P-Tech,
1730ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs KDD’24, August 25-29, 2024, Barcelona, Spain
Table 3: Test accuracy (%) on target graphs for cross-domain
transferability. We use red and blue to signify performance
increases or decreases, respectively, when pre-training with
graphs from other domains versus in-domain (In-D) transfer.
T
est Pr
e-training OF
A In-D ZeroG
Wiki-CS Ar
xiv∪Cora∪Pubmed∪Citeseer 48.42
- 53.28
Wiki-CS P-Home∪P-
Tech 21.09
- 60.97
Cora P-Home∪P-
Tech 18.57
68.72 67.65(-1.07%)
Pubme
dP-Home∪P-
Tech 31.89
78.02 69.12(-8.90%)
Citese
er P-Home∪P-
Tech 20.78
64.94 53.17(-11.77%)
P-Home Ar
xiv∪Cora∪Pubmed∪Citeseer 35.73
73.20 71.45(-1.75%)
P-
Tech Ar
xiv∪Cora∪Pubmed∪Citeseer 62.10
82.96 83.20 (+0.24%)
typically leads to notably better transfer performance, indicating
that source data diversity enhances zero-shot transfer.
5.3 Cross-domain Transferability (RQ2)
We then explore the more challenging cross-domain transfer set-
ting, where the difficulty arises from the larger variance in the
underlying data-generating distributions and semantics between
domains compared to in-domain transfer. The results are presented
in Table 3. ZeroG consistently outperforms the baseline model
OFA [ 31] across all datasets, demonstrating robust cross-dataset
transfer capabilities. Specifically, for the web link dataset Wiki-CS,
which is utilized for testing cross-domain transfer, we notice that
pre-training on co-purchase networks yields better results than pre-
training on citation networks. This indicates intrinsic properties
of co-purchase networks may be more transferable or relevant to
Wiki-CS. Additionally, compared with in-domain transfer results, it
is reasonable that there is a performance drop for almost all datasets
due to the gaps between domains. Some datasets such as Cora and
P-Home are less impacted by domain transfer, with decreases of
1.07% and 1.75% respectively compared to in-domain transfer. On
the other hand, datasets like Citeseer and Pubmed are significantly
affected, showing decreases of 11.77% and 8.90%, respectively. This
reveals that they rely more heavily on domain-specific structures
and knowledge for reasoning.
5.4 Ablation Studies (RQ3)
We conduct ablation studies of our proposed ZeroG to validate
its key components, and present the results in Table 4. The vari-
ant “w/o𝑝” refers to our method without prompting node used
during subgraph extraction introduced in Section 4.2.2; “w/o NA”
denotes our method without neighborhood aggregation discussed
in 4.2.3; “w/o norm” indicates the absence of any normalizations
within the subgraph; “w/o LoRA” means updating all parameters of
the LM. From Table 4 we can see that all these components signifi-
cantly contribute to the final results. Specifically, the integration
of the prompting node not only serves as a unique identifier for
the subgraph but also supplements it with general semantic con-
text, leading to performance improvements across all datasets. The
absence of neighborhood aggregation decreases the performance,
for example, a 25.43% decrease in Cora and a 30.81% decrease inTable 4: Ablation study for ZeroG.
Metho
ds Cora
Pubmed Citeseer P-Home
P-Tech
ZeroG 68.72
78.02 64.94 73.20
82.96
-
(w/o𝑝) 68.25(
-0.47%) 76.49(-1.53%) 61.64(-3.30%) 70.46(
-2.74%) 79.68(-7.18%)
-
(w/o NA) 43.31(
-25.43%) 47.21(-30.81%) 48.68(-16.26%) 60.26(
-12.94%) 58.91(-27.95%)
-
(w/o norm) 54.43(
-13.39%) 39.25(-38.77%) 34.84(-30.10%) 41.26(
-31.94%) 72.17(-14.69%)
-
(w/o LoRA) 17.36(
-51.36%) 46.49(-31.35%) 23.98(-40.96%) 39.77(
-33.43%) 87.22(+4.26%)
2 4 6 8 10
λ4050607080Accuracy (%)
1 2 3 4 5
k4050607080
Cora Citeseer Pubmed P-Home P-Tech
Figure 3: Hyperparameter study of iterations 𝜆and the num-
ber of hops 𝑘.
Pubmed, which indicates the importance of leveraging structural
information across datasets. Interestingly, normalization appears to
be a critical factor, which may be attributed to its role in adjusting
for different subgraph sizes and densities, enabling the model to
learn in a generalizable manner. Moreover, “w/o LoRA” shows that
allowing full parameter updates can lead to severe overfitting. For
example, on the Cora and Citeseer datasets, we observe perfor-
mance declines of 51.36% and 40.96%, respectively, demonstrating
the effectiveness of LoRA in such scenarios.
5.5 Hyper-parameter Sensitivity (RQ4)
We investigate the variation of ZeroG’s performance w.r.t. 𝜆(the
iterations of neighborhood aggregation performed) and 𝑘(the num-
ber of hops for subgraph extraction during pre-training) on both
citation networks and co-purchase networks, respectively. We con-
duct experiments with 𝜆varied from 1 to 10, 𝑘varied from 1 to 5,
and plot the results in Figure 3. Overall, our model is sensitive to
both𝑘and𝜆.ZeroG generally performs better with larger 𝜆and
𝑘initially, but the performance peaks and then gradually declines.
For𝜆, this can be attributed to an over-smoothing effect, where
the features of the nodes become overly similar, making it difficult
to classify. As for 𝑘, increasing its value leads to larger subgraphs,
which can introduce overly dispersed semantics. In addition, the
limitation on the number of nodes in subgraphs will result in a
reduced number of subgraphs available for pre-training with larger
𝑘. According to Figure 3, for citation networks, the parameters
lambda and k are set to 8 and 2, respectively, while for co-purchase
networks, they are both set to 4.
5.6 Efficiency Analysis (RQ5)
In Table 4, we analyze the efficiency of ZeroG, which includes
the time taken to train one epoch on the Cora, Citeseer, and P-
Home datasets, performance results, and the amount of parameters
1731KDD’24, August 25-29, 2024, Barcelona, Spain Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li∗
0 500 1000 1500 2000
Time (s)10203040506070Accuracy (%)ZeroG (0.07 M)
OFA (20.7 M)
Full (66.4 M)Cora
0 500 1000 1500 2000
Time (s)203040506070
ZeroG (0.07 M)
OFA (20.7 M)
Full (66.4 M)Citeseer
100 125 150 175 200 225
Time (s)3040506070
ZeroG (0.07 M)
OFA (20.7 M)
Full (66.4 M)P-Home
Figure 4: Efficiency Analysis of ZeroG.
that require tuning for ZeroG, OFA, and full-parameter tuning.
We note that ZeroG demands more training time than OFA [ 31]
but yields significantly better performance, presenting a trade-off
between efficiency and effectiveness. In addition, ZeroG only needs
0.07M parameters to be tuned, which is substantially less than the
20.7M required by OFA and the 66.4M needed for full-parameter
tuning. Reduced parameters mitigate negative transfer in zero-shot
scenarios and prevent the model from overfitting.
5.7 Visualization
As a supplementary study of the model effectiveness, we visualize
both the node and class representations with/without our proposed
ZeroG. To be specific, we use t-SNE [ 54] to map the representations
of Cora into two-dimensional vectors for visualization. Figure 5
shows that after equipping with our proposed ZeroG (1) clusters of
nodes with the same class (i.e., color in our visualization) are more
cohesive in the embedding space and (2) node representations from
different classes are more discriminative.
6 RELATED WORK
6.1 Graph Prompt
Prompt is a technique that involves augmenting foundation models
with task-specific hints to help the model adopt unseen data and
new tasks. It has greatly promoted the flourishing development
of Artificial General Intelligence (AGI), making significant strides
in the fields of natural language processing and computer vision.
Different from the typical learning scheme of “pre-training and fine-
tuning”, which is based on the assumption that the pre-training
stage and downstream tasks share a certain common intrinsic space,
the new paradigm of “pre-training, prompting, and fine-tuning”,
which aims to reformulate input data and tasks to fit the pretext,
achieves increasing attention nowadays [16].
This idea has also been naturally applied to the graph learning
area [ 48]. GPPT [ 46] is a pioneer work of graph prompt learning,
which adopts masked edge prediction as a pre-training strategy
and reformulates downstream node classification as link prediction.
GraphPrompt [ 35] unifies the downstream classification tasks and
upstream link prediction pre-training into a common task template
and tunes a learnable prompt token in the readout operation. In
addition, ProG [ 47] proposes a multi-task prompting framework,
which unifies the format of graph prompts and language prompts.
OFA [ 31] adapts LM-embedded vectors into an NOI prompting node
that contains task information to conduct various downstream tasks
adaptively. However, all these studies focus on graph prompts for
cross-task transfer. In this paper, we investigate the potential of ap-
plying graph prompt techniques to cross-dataset transfer, utilizing
Figure 5: Embedding visualization of Cora. Circles ( •) repre-
sent nodes, while stars ( ★) represent classes.
a hard prompting node to provide unique identifiers and additional
general semantics for both source and target datasets.
6.2 LM for Graphs
Recently, language models (LMs) have been increasingly utilized
in graph-related tasks [ 28,43,44]. Current methodologies can be
classified into three categories, distinguished by the role of LMs
in graph-related tasks: as the enhancer, predictor, and alignment
component [ 27]. Specifically, LM-as-enhancer approaches corre-
spond to enhancing the quality of node embeddings with the help
of powerful LMs. For example, TAPE [ 18] utilizes LLMs to enrich
initial node embeddings in GNNs with semantic knowledge rele-
vant to the nodes, notably improving embedding quality. WalkLM
[49] generates roughly meaningful textual sequences by an auto-
mated textualization program and fine-tunes an LM using them. In
addition, taking LM as predictors, several works [ 6,14,57,62,67]
utilize LMs to directly make predictions for a wide range of graph
tasks, such as classifications and reasonings, within a unified gen-
erative paradigm. Lastly, GNN-LM alignment ensures that each
encoder’s unique functionalities are preserved while coordinating
their embedding spaces at a specific stage. For example, MoMu [ 45],
MoleculeSTM [ 33], ConGraT [ 3], and RLMRec [ 59] merge GNN
and LM embeddings, enriching graphs with textual knowledge to
boost reasoning. In ZeroG, we follow the approach of using LM
as an enhancer, employing the LM for unified graph representa-
tion learning to bridge the semantic gap between feature and label
spaces across source and target graphs.
7 CONCLUSION AND FUTURE WORK
In this paper, we systematically summarize and analyze cross-
dataset zero-shot transfer for graph data and introduce ZeroG,
a novel approach to address this task. The key design of our model
involves unifying graph representations via a language model, us-
ing the prompt-based subgraph sampling strategy to embed graph
structures and semantics into pre-training datasets, and employing
a lightweight fine-tuning approach to minimize the risk of over-
fitting. Comprehensive experiments validate the effectiveness of
our model. ZeroG reveals great potential as the future foundation
model on the graph. Future work will explore expanding ZeroG’s
capabilities to include link-level and graph-level tasks and incorpo-
rate regression tasks to broaden the applicability.
1732ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs KDD’24, August 25-29, 2024, Barcelona, Spain
ACKNOWLEDGMENTS
This work was supported by NSFC Grant No. 62206067, HKUST-
HKUST(GZ) 20 for 20 Cross-campus Collaborative Research Scheme
C019 and Guangzhou-HKUST(GZ) Joint Funding Scheme 2023A03J0673.
REFERENCES
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al .2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).
[2]Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. 2023. Can we
trust the evaluation on ChatGPT? arXiv preprint arXiv:2303.12767 (2023).
[3]William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy, Jad
Kabbara, and Deb Roy. 2023. ConGraT: Self-Supervised Contrastive Pretraining
for Joint Graph and Text Embeddings. arXiv preprint arXiv:2305.14321 (2023).
[4]Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. 2023.
Understanding and improving visual prompting: A label-mapping perspective. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .
19133–19143.
[5]Aochuan Chen, Yimeng Zhang, Jinghan Jia, James Diffenderfer, Jiancheng Liu,
Konstantinos Parasyris, Yihua Zhang, Zheng Zhang, Bhavya Kailkhura, and
Sijia Liu. 2023. Deepzero: Scaling up zeroth-order optimization for deep model
training. arXiv preprint arXiv:2310.02025 (2023).
[6]Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. 2024. GraphWiz: An Instruction-
Following Language Model for Graph Problems. arXiv preprint arXiv:2402.16029
(2024).
[7]Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue
Wang, and Jia Li. 2023. Large language models meet harry potter: A dataset
for aligning dialogue agents with characters. In Findings of the Association for
Computational Linguistics: EMNLP 2023. 8506–8520.
[8]Shengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Qing Li, and Xiao
Huang. 2024. Entity Alignment with Noisy Annotations from Large Language
Models. arXiv preprint arXiv:2405.16806 (2024).
[9]Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei,
Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al .2023. Exploring the
potential of large language models (llms) in learning on graphs. arXiv preprint
arXiv:2307.03393 (2023).
[10] Jiashun Cheng, Man Li, Jia Li, and Fugee Tsung. 2023. Wiener Graph Deconvolu-
tional Network Improves Graph Self-Supervised Learning. In AAAI. 7131–7139.
[11] Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Ol-
gica Milenkovic, and Inderjit S Dhillon. 2021. Node feature extraction by self-
supervised multi-scale neighborhood prediction. arXiv preprint arXiv:2111.00064
(2021).
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[13] Keyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan, Wei Tsang Ooi, Qizhe Xie,
and Junxian He. 2023. Simteg: A frustratingly simple approach improves textual
graph learning. arXiv preprint arXiv:2308.02565 (2023).
[14] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. 2023. Talk like a graph:
Encoding graphs for large language models. arXiv preprint arXiv:2310.04560
(2023).
[15] Ziqi Gao, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li, Peilin Zhao,
Huanming Yang, Yong Huang, and Jia Li. 2023. Hierarchical graph learning for
protein–protein interaction. Nature Communications 14, 1 (2023), 1093.
[16] Ziqi Gao, Xiangguo Sun, Zijing Liu, Yu Li, Hong Cheng, and Jia Li. 2024. Protein
Multimer Structure Prediction via PPI-guided Prompt Learning. In The Twelfth
International Conference on Learning Representations.
[17] C Lee Giles, Kurt D Bollacker, and Steve Lawrence. 1998. CiteSeer: An automatic
citation indexing system. In Proceedings of the third ACM conference on Digital
libraries. 89–98.
[18] Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi. 2023. Explanations
as Features: LLM-Based Features for Text-Attributed Graphs. arXiv preprint
arXiv:2305.19523 (2023).
[19] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang,
and Jie Tang. 2022. Graphmae: Self-supervised masked graph autoencoders. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 594–604.
[20] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu
Wang, Weizhu Chen, et al .2021. LoRA: Low-Rank Adaptation of Large Language
Models. In International Conference on Learning Representations.
[21] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets for
machine learning on graphs. Advances in neural information processing systems
33 (2020), 22118–22133.[22] Jin Huang, Xingjian Zhang, Qiaozhu Mei, and Jiaqi Ma. 2023. Can llms effec-
tively leverage graph structural information: when and why. arXiv preprint
arXiv:2309.16595 (2023).
[23] Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang
Tang. 2020. Self-supervised learning on graphs: Deep insights and new direction.
arXiv preprint arXiv:2006.10141 (2020).
[24] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[25] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[26] Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and Junzhou Huang.
2019. Semi-supervised graph classification: A hierarchical graph perspective. In
The World Wide Web Conference. 972–982.
[27] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and
Jeffrey Xu Yu. 2023. A survey of graph meets large language model: Progress
and future directions. arXiv preprint arXiv:2311.12399 (2023).
[28] Yuhan Li, Wei Shen, Jianbo Gao, and Yadong Wang. 2022. Community ques-
tion answering entity linking via leveraging auxiliary data. arXiv preprint
arXiv:2205.11917 (2022).
[29] Zhixun Li, Dingshuo Chen, Qiang Liu, and Shu Wu. 2022. The Devil is in the
Conflict: Disentangled Information Graph Neural Networks for Fraud Detection.
In2022 IEEE International Conference on Data Mining (ICDM). IEEE, 1059–1064.
[30] Zhixun Li, Liang Wang, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao
Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, et al .2023. GSLB: The Graph Structure
Learning Benchmark. arXiv preprint arXiv:2310.05174 (2023).
[31] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen,
and Muhan Zhang. 2023. One for All: Towards Training One Graph Model for
All Classification Tasks. arXiv preprint arXiv:2310.00149 (2023).
[32] Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting
Bai, Yuan Fang, Lichao Sun, Philip S Yu, et al .2023. Towards graph foundation
models: A survey and beyond. arXiv preprint arXiv:2310.11829 (2023).
[33] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu,
Jian Tang, Chaowei Xiao, and Animashree Anandkumar. 2023. Multi-modal mol-
ecule structure–text model for text-based retrieval and editing. Nature Machine
Intelligence 5, 12 (2023), 1447–1457.
[34] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[35] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. 2023. Graphprompt:
Unifying pre-training and downstream tasks for graph neural networks. In Pro-
ceedings of the ACM Web Conference 2023. 417–428.
[36] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore.
2000. Automating the construction of internet portals with machine learning.
Information Retrieval 3 (2000), 127–163.
[37] Péter Mernyei and Cătălina Cangea. 2020. Wiki-cs: A wikipedia-based benchmark
for graph neural networks. arXiv preprint arXiv:2007.02901 (2020).
[38] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality.
Advances in neural information processing systems 26 (2013).
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.2021. Learning transferable visual models from natural language supervision.
InInternational conference on machine learning. PMLR, 8748–8763.
[40] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings
using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019).
[41] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in
automatic text retrieval. Information processing & management 24, 5 (1988),
513–523.
[42] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and
Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29,
3 (2008), 93–93.
[43] Wei Shen, Yuhan Li, Yinan Liu, Jiawei Han, Jianyong Wang, and Xiaojie Yuan.
2021. Entity linking meets deep learning: Techniques and solutions. IEEE Trans-
actions on Knowledge and Data Engineering 35, 3 (2021), 2556–2578.
[44] Yiheng Shu, Zhiwei Yu, Yuhan Li, Börje F Karlsson, Tingting Ma, Yuzhong Qu, and
Chin-Yew Lin. 2022. Tiara: Multi-grained retrieval for robust question answering
over large knowledge bases. arXiv preprint arXiv:2210.12925 (2022).
[45] Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun,
Zhiwu Lu, and Ji-Rong Wen. 2022. A molecular multimodal foundation model as-
sociating molecule graphs with natural language. arXiv preprint arXiv:2209.05481
(2022).
[46] Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. 2022. Gppt:
Graph pre-training and prompt tuning to generalize graph neural networks. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 1717–1727.
[47] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One:
Multi-Task Prompting for Graph Neural Networks. In Proceedings of the 26th
1733KDD’24, August 25-29, 2024, Barcelona, Spain Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li∗
ACM SIGKDD international conference on knowledge discovery & data mining
(KDD’23). 2120–2131.
[48] Xiangguo Sun, Jiawen Zhang, Xixi Wu, Hong Cheng, Yun Xiong, and Jia Li. 2023.
Graph Prompt Learning: A Comprehensive Survey and Beyond. arXiv preprint
arXiv:2311.16534 (2023).
[49] Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, and Carl Yang. 2023. Walklm: A
uniform language model fine-tuning framework for attributed graph embedding.
InThirty-seventh Conference on Neural Information Processing Systems.
[50] Jianheng Tang, Fengrui Hua, Ziqi Gao, Peilin Zhao, and Jia Li. 2023. GADBench:
Revisiting and Benchmarking Supervised Graph Anomaly Detection. In Thirty-
seventh Conference on Neural Information Processing Systems.
[51] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. 2022. Rethinking graph neural
networks for anomaly detection. In International Conference on Machine Learning.
PMLR, 21076–21089.
[52] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin,
and Chao Huang. 2023. Graphgpt: Graph instruction tuning for large language
models. arXiv preprint arXiv:2310.13023 (2023).
[53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al .2023. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 (2023).
[54] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[55] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[56] Petar Veličković, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio,
and R Devon Hjelm. 2018. Deep graph infomax. arXiv preprint arXiv:1809.10341
(2018).
[57] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and
Yulia Tsvetkov. 2023. Can Language Models Solve Graph Problems in Natural
Language? arXiv preprint arXiv:2305.10037 (2023).
[58] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,
Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised
contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).
[59] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng
Wang, Dawei Yin, and Chao Huang. 2023. Llmrec: Large language models with
graph augmentation for recommendation. arXiv preprint arXiv:2311.00423 (2023).
[60] Liqi Yang, Linhao Luo, Xiaofeng Zhang, Fengxin Li, Xinni Zhang, Zelin Jiang,
and Shuai Tang. 2022. Why do semantically unrelated categories appear in the
same session? a demand-aware method. In Proceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
2065–2069.
[61] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016. Revisiting semi-
supervised learning with graph embeddings. In International conference on ma-
chine learning. PMLR, 40–48.
[62] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. 2023.
Natural language is all a graph needs. arXiv preprint arXiv:2308.07134 (2023).
[63] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in
neural information processing systems 33 (2020), 5812–5823.
[64] Xinni Zhang, Yankai Chen, Cuiyun Gao, Qing Liao, Shenglin Zhao, and Irwin King.
2022. Knowledge-aware neural networks with personalized feature referencing
for cold-start recommendation. arXiv preprint arXiv:2209.13973 (2022).
[65] Haihong Zhao, Bo Yang, Jiaxu Cui, Qianli Xing, Jiaxing Shen, Fujin Zhu, and
Jiannong Cao. 2023. Effective fault scenario identification for communication
networks via knowledge-enhanced graph neural networks. IEEE Transactions on
Mobile Computing (2023).
[66] Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, and Jia Li. 2024.
Weakly Supervised Anomaly Detection via Knowledge-Data Alignment. arXiv
preprint arXiv:2402.03785 (2024).
[67] Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein,
Zhaocheng Zhu, and Jian Tang. 2023. Graphtext: Graph reasoning in text space.
arXiv preprint arXiv:2310.01089 (2023).
[68] Yaping Zheng, Xiaofeng Zhang, Shiyi Chen, Xinni Zhang, Xiaofei Yang, and Di
Wang. 2021. When convolutional network meets temporal heterogeneous graphs:
an effective community detection method. IEEE Transactions on Knowledge and
Data Engineering 35, 2 (2021), 2173–2178.
[69] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2021.
Graph contrastive learning with adaptive augmentation. In Proceedings of the
Web Conference 2021. 2069–2080.
A DATASET CONSTRUCTION
We constructed two datasets, P-Home and P-Tech, both of which
were extracted from the ogbn-products dataset. We use raw textsprocessed by Chen et al. [ 9] as the description of nodes and la-
bels. The ogbn-products dataset serves as a large-scale, undirected,
and unweighted graph that encapsulates an Amazon product co-
purchasing network. Within this graph, nodes are products avail-
able on Amazon, while the edges signify co-purchasing relation-
ships, indicating that two products have been bought together by
customers. The features of the nodes are the descriptions of the
products. Since each product (node) belongs to only one category,
our method of extracting based on category ensures that there is
no overlap between P-Home and P-Tech.
P-Home: For P-Home, we selected categories related to house-
hold items: “Baby Products”, “Appliances”, “All Beauty”, “Office &
School Supplies”, and “Home Improvement”. The number of nodes
in each category is as follows: 3653, 3024, 1969, 630, and 514.
P-Tech: For P-Tech, we chose products related to technology,
including “Software”, “Video Games”, and “Industrial & Scientific”.
The number of nodes of each category is 3079, 20911, and 17438.
B TEXUAL INFORMATION
B.1 Node Features and Class Descriptions
We follow OFA [ 31] to collect the node features and class descrip-
tions for all datasets. Taking citation networks as examples, the
node features are the titles and abstracts of papers. The class de-
scriptions of citation networks are the class names with detailed
descriptions. An example from Cora is illustrated in Table 5.
B.2 Dataset Descriptions
In this section, we will enumerate the description of each dataset
utilized, which is also used for prompt texts. Generally, we describe
the classes and average degree of each dataset. The dataset descrip-
tions are generated by GPT-4 and subsequently manually refined.
B.2.1 Cora. The Cora [ 61] dataset is a fundamental resource in
the field of graph learning, particularly within the realm of machine
learning research. It represents a network of scientific publications.
There are 7 categories in Cora: Theory, covering theoretical aspects
of machine learning and AI; Reinforcement Learning, including
research on reinforcement learning, a type of machine learning;
Genetic Algorithms, dealing with genetic algorithms, a type of opti-
mization algorithm inspired by natural evolution. Neural Networks,
focusing on artificial neural networks, a subset of machine learn-
ing. Probabilistic Methods, pertaining to research on probabilistic
methods in machine learning, using probability mathematics to
handle uncertainty and make predictions. Case Based, focusing on
case-based reasoning in AI, a method that solves new problems by
referring to similar past cases. Rule Learning, involving the genera-
tion of rules for decision-making systems. The average degree of
Cora is 4.
B.2.2 Citeseer. The Citeseer [ 61] dataset is a prominent academic
resource in the field of computer science, categorizing publications
into six distinct areas. These are Agents, focusing on intelligent
agents; Machine Learning (ML), covering all aspects of learning
techniques and applications; Information Retrieval (IR), dealing
with data and text indexing and retrieval; Databases (DB), related
to database management and data mining; Human-Computer In-
teraction (HCI), emphasizing computer technology interfaces for
1734ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs KDD’24, August 25-29, 2024, Barcelona, Spain
Table 5: Example of node features and class descriptions
Dataset Node feature
CoraLearning sparse perceptrons. We introduce a new algorithm designed to learn sparse perceptrons over input representations
which include high-order features. Our algorithm, which is based on a hypothesis-boosting method, is able to PAC-learn a
relatively natural class of target concepts. Moreover, the algorithm appears to work well in practice: on a set of three problems
domains, the algorithm produces classifiers that utilize small numbers of features yet exhibit good generalization performance.
Perhaps most importantly, our algorithm generates concept descriptions that are easy for humans to understand.
Dataset Class description
CoraTheory. The “Theory” category likely refers to research papers that delve into the theoretical aspects of machine learning
and artificial intelligence. This includes a broad array of topics such as theoretical foundations of various machine learning
algorithms, performance analysis, studies on learning theory, statistical learning, information theory, and optimization
methods. Additionally, it could encompass the development of new theoretical frameworks, investigations into the essence
of intelligence, the potential for artificial general intelligence, as well as the ethical implications surrounding AI. Essentially,
the “Theory” category encapsulates papers that primarily focus on theoretical concepts and discussions, contrasting with
more application-oriented research which centers on specific techniques and their practical implementation.
humans; and Artificial Intelligence (AI), a broad category encom-
passing general AI theory and applications, excluding certain sub-
fields. The average degree of this graph is 2.
B.2.3 Pubmed. The PubMed [ 61] dataset comprises three cate-
gories: Experimental studies on diabetes mechanisms and therapies,
Type 1 Diabetes research focusing on autoimmune processes and
treatments, and Type 2 Diabetes studies emphasizing insulin resis-
tance and management strategies. Each category addresses specific
aspects of diabetes research, aiding in understanding and treating
this complex disease. The average degree of this graph is 4.5.
B.2.4 Arxiv. The arXiv dataset is a notable resource in the field
of graph learning, particularly in the area of computer science re-
search. This dataset forms a directed graph representing the citation
network among all Computer Science papers on arXiv, as indexed
by the Microsoft Academic Graph (MAG). Each node in this net-
work corresponds to a paper, and directed edges indicate citations.
The dataset’s primary challenge is predicting the 40 subject areas
of arXiv CS papers, such as cs.AI, cs.LG, and cs.OS. The task is
structured as a 40-class classification problem.
B.2.5 P-Home. This graph is of amazon products about home us-
ing. There are six categories. Baby Products: A category dedicated to
items designed for infants and toddlers, including hygiene, feeding,
and skin care essentials; Appliances: This section features electrical
machines and devices intended for household tasks, such as cook-
ing, cleaning, and food preservation; All Beauty: A broad range of
personal care products aimed at enhancing or maintaining physical
appearance and hygiene; Office & School Supplies: Items and tools
used for writing, organizing, and conducting daily activities in edu-
cational and professional settings; Home Improvement: Products
and materials focused on repairing, enhancing, or maintaining thefunctionality and aesthetics of living spaces. The average degree of
this graph is 26.93.
B.2.6 P-Tech. This graph is of amazon products about technolo-
gies. There are three categories. Software: Computer programs and
applications developed to perform specific tasks on computing de-
vices, ranging from productivity to creative design; Video Games:
Interactive entertainment software and accessories designed for
recreational play on consoles, computers, and portable devices;
Industrial & Scientific: Equipment, tools, and materials used in in-
dustrial operations and scientific research, including measurement,
fabrication, and experimental applications. The average degree of
this graph is 87.60.
B.2.7 Wiki-CS. The Wiki-CS [ 37] dataset is a comprehensive col-
lection of Wikipedia entries, systematically categorized into ten
distinct areas of computer science. These categories include Com-
putational Linguistics, focusing on the intersection of computer
science and linguistics; Databases, covering database technologies
and theories; Operating Systems, detailing the software that man-
ages computer hardware; Computer Architecture, exploring the
design and structure of computer systems; Computer Security, ad-
dressing the protection of information systems; Internet Protocols,
discussing the rules governing internet data exchange; Computer
File Systems, about methods for storing and organizing computer
files; Distributed Computing Architecture, concerning computa-
tions spread across multiple machines; Web Technology, focus-
ing on the technologies underpinning the web; and Programming
Language Topics, which includes various aspects of programming
languages. This dataset serves as a valuable resource for understand-
ing diverse computer science topics as represented in Wikipedia,
reflecting the breadth and depth of the field.
1735