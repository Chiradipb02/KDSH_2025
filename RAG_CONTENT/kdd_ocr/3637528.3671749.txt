ProCom: A Few-shot Targeted Community Detection Algorithm
Xixi Wu
Kaiyu Xiong
Shanghai Key Laboratory of Data
Science, School of Computer Science,
Fudan University
Shanghai, China
xxwu1120@gmail.com
kyxiong22@m.fudan.edu.cnYun Xiong‚àó
Shanghai Key Laboratory of Data
Science, School of Computer Science,
Fudan University
Shanghai, China
yunx@fudan.edu.cnXiaoxin He
National University of Singapore
Singapore
xiaoxin@comp.nus.edu.sg
Yao Zhang
Shanghai Key Laboratory of Data
Science, School of Computer Science,
Fudan University
Shanghai, China
yaozhang@fudan.edu.cnYizhu Jiao
University of Illinois at
Urbana-Champaign
Urbana-Champaign, IL, USA
yizhuj2@illinois.eduJiawei Zhang
IFM Lab, Department of Computer
Science, University of California,
Davis
Davis, CA, USA
jiawei@ifmlab.org
Abstract
Targeted community detection aims to distinguish a particular type
of community in the network. This is an important task with a lot of
real-world applications, e.g., identifying fraud groups in transaction
networks. Traditional community detection methods fail to capture
the specific features of the targeted community and detect all types
of communities indiscriminately. Semi-supervised community de-
tection algorithms, emerged as a feasible alternative, are inherently
constrained by their limited adaptability and substantial reliance on
a large amount of labeled data, which demands extensive domain
knowledge and manual effort.
In this paper, we address the aforementioned weaknesses in
targeted community detection by focusing on few-shot scenarios.
We propose ProCom, a novel framework that extends the ‚Äúpre-
train, prompt‚Äù paradigm, offering a low-resource, high-efficiency,
and transferable solution. Within the framework, we devise a dual-
level context-aware pre-training method that fosters a deep un-
derstanding of latent communities in the network, establishing a
rich knowledge foundation for downstream tasks. In the prompt
learning stage, we reformulate the targeted community detection
task into pre-training objectives, allowing the extraction of specific
knowledge relevant to the targeted community to facilitate effective
and efficient inference. By leveraging both the general community
knowledge acquired during pre-training and the specific insights
gained from the prompt communities, ProCom exhibits remarkable
adaptability across different datasets. We conduct extensive exper-
iments on five benchmarks to evaluate the ProCom framework,
‚àóCorresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671749demonstrating its SOTA performance under few-shot scenarios,
strong efficiency, and transferability across diverse datasets.
CCS Concepts
‚Ä¢Information systems ‚ÜíData mining.
Keywords
Community Detection; Semi-supervised Community Detection;
Data Mining; Graph Prompt Learning
ACM Reference Format:
Xixi Wu, Kaiyu Xiong, Yun Xiong, Xiaoxin He, Yao Zhang, Yizhu Jiao, and Ji-
awei Zhang. 2024. ProCom: A Few-shot Targeted Community Detection
Algorithm. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671749
1 Introduction
Networks serve as powerful structures for modeling diverse rela-
tional information among objects across social [ 1], natural [ 17],
and academic domains [ 26]. A crucial step to understand a network
is identifying and analyzing closely related subgraphs, i.e., commu-
nities [ 13]. However, sometimes there may exist various types of
communities in the same network, while people may only focus
on a specific type of community, i.e., the targeted community [ 40].
Formally, the research task of distinguishing such a targeted com-
munity from others is known as targeted community detection [ 49].
This is an important task with a lot of real-world applications, such
as identifying fraud groups in transaction networks, and detecting
social spammer circles in social networks [12, 46].
However, traditional community detection methods [ 5,13,42,43]
are ill-suited for the targeted setting. This is because they exhaus-
tively extract all types of communities from the whole network,
regardless of whether they are of the same type as the targeted
community or not. Taking the trading network in Figure 1(a) as
an example, traditional community detection algorithm tends to
identify not only fraud groups but also irrelevant ones, deviating
3414
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xixi Wu et al.
(a) Traditional Community DetectionSemi-supervised Community Detection Algorithm
(b) Semi-supervised Community Detection
(c) ‚ÄúPre-train, Prompt‚Äù Paradigm for Targeted Community Detectiontargetedcommunity
TraditionalCommunity Detection Algorithm
irrelevant communities
Network
Network100-500samples Low Efficiency
An unseen networkA Transferable Scenario 
1-shot sample
‚ùÑPrompt Learning
Pre-trainingA GNN Model
üî•tunefreeze
Network
Pre-trainingGNN Model
üî•train
‚ùÑfreezePrompt Learning
‚ûïPrompt Learning
‚ùÑ
sample
¬∑¬∑¬∑Training Set
Pre-train, Prompt
Figure 1: A subgraph of a trading network with both normal
and fraud communities. (a) Traditional community detec-
tion tends to identify both kinds of communities. (b) Semi-
supervised community detection may pinpoint the remain-
ing fraud community but requires a substantial amount of
labeled data. (c) ProCom applies the ‚Äúpre-train, prompt‚Äù par-
adigm to tackle the task under few-shot settings, typical in
low-resource learning, efficient and transferable inference.
from our intended goal of targeted community detection. Mean-
while, semi-supervised community detection algorithms [ 2,40,49]
emerge as a potential alternative. These methods take some labeled
targeted communities as input and aim to identify potential similar
communities within the network. Nevertheless, a significant draw-
back of these approaches is their heavy reliance on a large amount
of labeled data (100-500 labeled instances [ 40,49]). Labeling such a
substantial number of instances is laborious and requires extensive
domain knowledge, making it impractical in real-world scenarios.
Moreover, semi-supervised methods have limited adaptability as
they necessitate gathering new relevant unlabeled data and under-
going an exhaustive re-training process to adapt to a new targeted
community detection task.
To overcome these weaknesses, the adoption of the few-shot
setting for targeted community detection emerges as a promising
panacea. This setting empowers models to learn under low-resourceconditions, requiring only a limited number of labeled samples. A
concrete practice that aligns with this setting is the ‚Äúpre-train,
prompt‚Äù two-phase paradigm [ 4]. Originated from the Natural Lan-
guage Processing (NLP) domain [ 18,22], this paradigm aims to
reformulate downstream task into pretexts, facilitating efficient in-
ference with minimal downstream supervision [ 32]. In the context
of targeted community detection, the ‚Äúpre-train, prompt‚Äù paradigm
offers several advantages: (1) Leveraging Pre-trained Models for
Generalization. During the pre-training stage, the model can gain
a comprehensive understanding of various community structures
and characteristics, establishing a rich knowledge foundation for
downstream tasks. (2) Using Prompt for Targeted Community
Specification. By introducing few-shot samples from the targeted
community, the prompt learning stage can extract preserved knowl-
edge specific to the targeted community type, enabling the identifi-
cation of remaining targeted communities. (3) Adaptability. Since
the pre-trained model already incorporates general community
knowledge, adapting to a new targeted community requires only
a few relevant samples, resulting in minimal tuning burden and
downstream supervision for seamless adaptation.
Inspired by the above insights, we propose a novel framework,
ProCom, that extends the ‚Äúpre-train, prompt‚Äù paradigm to the
targeted com munity detection task, providing a low-resource, high-
efficiency, and transferable solution. It incorporates a Dual-level
Context-aware Pre-training method and a Targeted Community-
guided Prompting Mechanism. Specifically, ProCom‚Äôs pre-training
strategy is uniquely designed to understand the latent communities
in the network, equipping the model with a rich knowledge foun-
dation for downstream tasks. The dual-level pre-training objectives
involve node-to-context proximity and context distinction, captur-
ing the underlying structures of latent communities and discrimi-
nate their distinctive features. During the prompt learning stage,
we reformulate the downstream targeted community detection
task into pretexts, enabling the extraction of specific knowledge to
facilitate efficient inference. ProCom generates candidate commu-
nities through proximity analysis and conducts similarity matching
between candidates and prompt communities. This tailored de-
tection process aligns with pre-training objectives, ensuring both
effective and efficient targeted community identification. Beyond
low-resource learning and efficient inference, ProCom also exhibits
transferability. Instead of relying on end-to-end frameworks, the
prompting stage of ProCom can work as a plug-and-play compo-
nent, swiftly adapting to any new types of targeted communities
across different datasets. In summary, our contributions are as fol-
lows:
‚Ä¢We extend the ‚Äúpre-train, prompt‚Äù paradigm for few-shot targeted
community detection, addressing the heavy reliance on labeled
communities. To the best of our knowledge, this is the first work
that explores prompt learning specifically for community tasks.
‚Ä¢In the framework of ProCom, we propose a dual-level context-
aware pre-training method that enables the model to acquire a
rich understanding of latent communities within the network.
‚Ä¢We further devise a targeted-community guided prompting mech-
anism. By aligning the downstream task with pretexts, this mech-
anism can extract specific knowledge relevant to the targeted
community, facilitating both effective and efficient inference.
3415ProCom: A Few-shot Targeted Community Detection Algorithm KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
‚Ä¢Extensive experiments are conducted on diverse real-world datasets
to show the SOTA performance of ProCom under few-shot set-
tings, robustness to different numbers of prompts, strong effi-
ciency, and even transferability across different datasets.
2 Related Works
2.1 Community Detection
Community detection aims to partition graph nodes into multiple
groups, where internal nodes are more similar than the external
[13]. Traditional community detection methods can be classified
into three groups: (1) Optimization-based methods [ 3,7,29] re-
veal underlying communities by optimizing some metrics such as
modularity. (2) Matrix factorization methods [ 19,37] learn latent
representations for communities by decomposing adjacency ma-
trices. (3) Generative models [ 42,43] infer communities by fitting
the original graph. Recently, some frameworks that combine graph
representation learning and community detection have been pro-
posed [ 5,13,14,25,30,38,48]. However, these community detection
works fail to pinpoint a particular kind of community, i.e., targeted
community.
Therefore, to identify the targeted community, semi-supervised
community detection methods are proposed. Bespoke [ 2] and SEAL
[49] are seed-based methods that first locate seed nodes and then
generate communities around the seed. CLARE [ 40] is the state-
of-the-art semi-supervised model that proposes a novel subgraph-
based inference framework. However, all these semi-supervised
community detection methods necessitate a substantial number of
labeled communities for effective model training.
2.2 ‚ÄúPre-train, Prompt‚Äù on the Graph Domain
Prompt-based tuning methods, originated from the NLP domain
[4,18,22], have been widely used to facilitate the adaptation of
pre-trained language models to various downstream tasks in a
parameter-efficient manner. Recently, prompt learning has also
emerged as a promising tool to make a pre-trained graph model
seamlessly adapt to specific downstream tasks [ 9,23,31‚Äì33,50,52].
GPPT [31] performs pre-training to acquire the general structural
knowledge inherent in the graph via link prediction. It then trans-
forms the downstream node classification task into the link predic-
tion via a hand-crafted prompting mechanism. All-in-One [ 32], a
representative work within the graph prompt learning domain, in-
troduces a uniform prompting design, encompassing prompt tokens,
prompt structures, and inserting patterns to create a prompted graph
to enable downstream inference. Follow-up works then extend this
paradigm to heterogeneous graph [ 24,47], temporal interaction
graphs [6], text-attributed graphs [20], and molecular graphs [39].
Nevertheless, these methods are not directly applicable to the tar-
geted community detection task due to two key limitations. Firstly,
their pre-training stages lack the understanding of communities.
Secondly, their prompt learning methods are specifically designed
for classification tasks and rely on manipulating features to fit the
provided samples. As a result, there exists a gapbetween these
methods and the requirements of the community-level task.Table 1: Important Notations
Symbol Description
ùê∫(V,E,X) Graph
ùëö The number of prompts
ùëÅ The number of predicted communities
GNN Œò(¬∑) GNN encoder parameterized by Œò
PTŒ¶(¬∑) Prompting function parameterized by Œ¶
¬§C(ùëñ)Theùëñ-th ground-truth community
N(ùëò)
ùë£/Nùë£ (ùëò)-ego net of node ùë£
eNùë£ Corrupted context of node ùë£
ÀÜCùë£=PTŒ¶(Nùë£) Distilled community from Nùë£
z(ùë£),z(C) Embedding of node ùë£, communityC
3 Methodology
In this section, we introduce the proposed ProCom framework. We
start with the preliminaries in Section 3.1, and then introduce the
pre-training and prompt learning stages in Sections 3.2 and 3.3,
respectively. Notations are summarized in Table 1.
3.1 Preliminary
We first give a definition of targeted community detection task and
then present the ProCom pipeline to facilitate comprehension.
3.1.1 Targeted Community Detection. Given a graph ùê∫=(V,E,X)
whereVis the set of nodes, Eis the set of edges, and Xdenotes
node feature matrix. Within the graph ùê∫, a community,C‚äÇV ,
denotes a subset of nodes that maintain certain desired characteris-
tics in their edge connections or features. With the set of ùëölabeled
communities{¬§C(ùëñ)}ùëö
ùëñ=1as training data, targeted community detec-
tion task is defined as identifying the set of other potential similar
communities{ÀÜC}within the graph. Existing semi-supervised meth-
ods [ 40,49] rely on a large number of training samples by setting
ùëöas 500. To reduce the reliance on labeled information, we follow
the few-shot setting as fixing ùëöto a much smaller number, e.g., 10.
3.1.2 ProCom Pipeline. We implement the ProCom framework
following the ‚Äúpre-train, prompt‚Äù two-phase paradigm. In the pre-
training stage, we aim to acquire an understanding of the latent com-
munities in the graph by employing carefully-designed pre-training
objectives. This allows us to learn a GNN-based encoder GNN Œò(¬∑).
Subsequently, we obtain each node‚Äôs embedding z(ùë£)‚ààRùëë,‚àÄùë£‚ààV,
and we compute the embedding of a community Cvia aggregating
members‚Äô representations as z(C)=√ç
ùë£‚ààCz(ùë£). During the prompt
learning phase, with ùëö-shot labeled communities as prompts, our
objective is to predict potential similar communities within the
network while keeping GNN Œò(¬∑)frozen. In this way, we leverage
preserved knowledge to identify the targeted community with min-
imal tuning burden.
3.2 Dual-level Context-aware Pre-training
We leverage the pre-training stage to make the GNN encoder
GNN Œò(¬∑)aware of various structural contexts present in the net-
work, acquiring a rich understanding of communities to benefit
downstream task. Specifically, we devise dual-level pre-training
3416KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xixi Wu et al.
1:Node-to-Context Proximity2:Context Distinction
Centric NodeContextCorrupted Context
similardissimilar
Pre-trainingGNN Model
üî•train
Prompt Learning
1-shot sample
‚ùÑfreeze1:Candidates Generation via Proximity Analysis ùêèùêìùöΩ(¬∑)
ùêèùêìùöΩ(¬∑)‚àÖ
ùêèùêìùöΩ(¬∑)ùêèùêìùöΩ(¬∑)2:Final Predictions via Similarity Matching
Prompt Community
Candidate CommunitiesSimilar?
‚Ä¶
Networksimilardissimilar
Figure 2: Overview of ProCom. During the pre-training stage,
we devise a dual-level pre-training method to guide the model
in understanding the latent communities in the network. In
the subsequent prompt learning stage, aided with few-shot
samples, we reformulate the targeted community detection
task into pretexts, facilitating prediction in a parameter-
efficient manner.
objectives, i.e., node-to-context proximity and context distinction.
These objectives capture the underlying structures of latent com-
munities and discriminate their distinctive features.
3.2.1 Node-to-Context Proximity. In the first objective, we enable
the model to learn the relationships between individual nodes and
the broader contexts in which they exist. Contexts often exhibit
dense connections and share similar features [ 8,10,16], making
them promising candidates for communities. By guiding the model
to understand these relationships, it gains valuable insights into
the community structures present in the network.
For implementation, we begin by randomly sampling a batch
of nodesB‚äÇV . Then, for each node ùë£‚ààB, we extract its con-
text as itsùëò-ego netN(ùëò)
ùë£. This subgraph consists of the central
nodeùë£and its neighbors within ùëòhops, effectively capturing the
node‚Äôs surrounding environment [ 21,32]. For simplicity, we omit
the superscript ùëòand represent it with Nùë£. After performing for-
ward propagation on GNN, we obtain both node‚Äôs and context‚Äôs
representations as z(ùë£)andz(Nùë£)=√ç
ùëü‚ààNùë£z(ùëü). To encourage
the alignment between each node and its surrounding context, we
employ the following InfoNCE loss [35]:
Ln2c(Œò)=‚àëÔ∏Å
ùë£‚ààB‚àílogexp(sim(z(ùë£),z(Nùë£))/ùúè)√ç
ùë¢‚ààBexp(sim(z(ùë£),z(Nùë¢))/ùúè),(1)
where ‚Äú n2c‚Äù denotes ‚Äúnode-to-context‚Äù, ùúèis a temperature hyper-
parameter, and the loss is parameterized by GNN‚Äôs weights Œò. Weemphasize that for the loss, both Mean- and Sum-pooling for com-
puting z(Nùë£)result in identical outcomes with proof provided in
Appendix B. By maximizing the similarity between each node and
its surrounding context, the model learns to capture the intrinsic
relationships between nodes and their potential affiliated communi-
ties, facilitating the understanding of community structures within
the network.
3.2.2 Context Distinction. In the second objective, we aim to en-
hance the model‚Äôs understanding of the characteristics of latent
communities by encouraging it to differentiate between different
contexts based on their unique features.
For implementation, as we have obtained node ùë£‚Äôs contextNùë£,
we perform structural perturbation via randomly dropping nodes or
edges [ 45] to create a corrupted context eNùë£. Then, the optimization
objective is designed to guide the alignment between representa-
tions of raw context z(Nùë£)and corrupted context z(eNùë£):
Lc2c(Œò)=‚àëÔ∏Å
ùë£‚ààB‚àílogexp
sim(z(Nùë£),z(eNùë£))/ùúè
√ç
ùë¢‚ààBexp
sim(z(Nùë£),z(eNùë¢))/ùúè,(2)
where the subscript notation ‚Äú c2c‚Äù denotes ‚Äúcontext-to-context‚Äù.
By maximizing the similarity within a single context, we enable
the model to gain insights into the distinct characteristics of latent
communities present in the graph.
3.2.3 Summary & Discussion. The overall optimization objective
combines both pre-training objectives as Lpre-train =Ln2c+ùúÜ¬∑Lc2c
whereùúÜ‚â•0is a hyper-parameter that decides the importance
weight ofLc2c. By applyingLpre-train to learn the GNN encoder,
we can obtain latent community knowledge inherent in the graph.
A detailed summary of the pre-training process can be found in
Algorithm 2 in the Appendix.
Furthermore, we list several widely used strategies for graph pre-
training, including Node Attribute Masking [ 11], Link Prediction
[15,23], Node-to-Node Consistency [ 51], Node-to-Graph Mutual
Information Maximization [ 36], and Graph-to-Graph Consistency
[45]. However, these approaches do not take into account the pres-
ence of communities in the network, resulting in a semantic gap
between the pre-training process and the downstream community
detection task.
3.3 Prompt Learning
After the pre-training stage, the learned GNN Œò(¬∑)has already ac-
quired insights into latent communities and their distinct char-
acteristics within the network. Then, we can obtain node-level
representations by feeding the graph into this well-learned encoder,
resulting in representations Z=GNN Œò(X,E)for all nodes, where
z(ùë£)represents the embedding of node ùë£.
In the prompt learning stage, our objective is to predict ùëÅnew
communities that are similar to a given set of ùëö-shot targeted com-
munities, denoted as {¬§C(ùëñ)}ùëö
ùëñ=1. As the pre-trained model contains
rich knowledge about the targeted community, we aim to reformu-
late the targeted community detection task into pretexts to retrieve
such knowledge. First, we generate candidate communities by per-
forming proximity analysis between nodes and their surrounding
3417ProCom: A Few-shot Targeted Community Detection Algorithm KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
contexts, aligning with the first pretext. This analysis helps identify
potential communities based on the relationships learned by the
model between nodes and their affiliated contexts. Next, we con-
duct similarity matching between the candidate communities and
the provided prompt communities to make the final predictions,
aligning with the second pretext. In this way, we can effectively
leverage preserved knowledge to enhance downstream task.
3.3.1 Candidate Community Generation. In the candidate genera-
tion phase, our objective is to generate a set of candidate communi-
ties. Communities typically exhibit stronger connections within a
certain neighborhood [ 2,8,13]. Therefore, we aim to extract the
most promising community for any given node within its context.
Specifically, we introduce a prompting function PTŒ¶(¬∑)parameter-
ized by Œ¶as follows:
ÀÜCùë£=PTŒ¶(Nùë£),
where ÀÜCùë£represents the distilled community centered around node
ùë£,Nùë£represents the context. Note that the output of the prompting
function may be an empty set, indicating that there does not exist
a promising community for that particular node. In such cases, we
disregard the result.
Intuitively, for each neighboring node ùë¢within the context Nùë£,
we assign it to the distilled promising community ÀÜCùë£only if its
proximity to the central node ùë£exceeds a certain threshold value ùõº.
As the representations of nodes, z(ùë¢)andz(ùë£), already contain such
proximity information guided by the first pretext (node-to-context
proximity), we can leverage this preserved knowledge encoded in
their representations. However, instead of directly utilizing pre-
trained node embeddings, we introduce an additional Multi-layer
Perceptron [ 28] to measure proximity in a learnable manner. This
allows us to extract specific knowledge relevant to the targeted com-
munity, learning the heuristics of their distinct structural patterns.
The formula is given as follows:
ÀÜCùë£=PTŒ¶(Nùë£)={ùë¢‚ààN ùë£andùúé(MLPŒ¶(z(ùë¢)‚à•z(ùë£))‚â•ùõº}.
Here, we implement PTŒ¶(¬∑)using an MLP, z(ùë¢),z(ùë£)denote nodes
ùë¢,ùë£‚Äôs representations, respectively, ùúé(¬∑)denotes Sigmoid function,
andùõº‚àà[0,1]represents a threshold parameter. By tuning the MLP
with the provided prompt communities, we effectively utilize the
preserved knowledge to understand the specific community struc-
tures related to the targeted community in a parameter-efficient
manner.
For the optimization of prompting function PTŒ¶(¬∑), we harness
the supervision signals provided by the given prompt communities,
which offer insights about structural patterns of the targeted com-
munity. Technically, we randomly select a node ùë£from a ground-
truth community ¬§C(ùëñ)(ùëñ=1,2,..,ùëö ) and extract its context Nùë£.
Since each node ùë¢‚ààN ùë£can be categorized as either belonging to
the ground-truth community ¬§C(ùëñ)or not, its status can serve as
supervision for distinguishing whether a node should be included
in the community. Therefore, we employ the Cross-Entropy loss to
optimize Œ¶as follows:
Lpt(Œ¶)=ùëö‚àëÔ∏Å
ùëñ=1‚àëÔ∏Å
ùë£‚àà¬§C(ùëñ),ùë¢‚ààNùë£I(ùë¢,¬§C(ùëñ))logùúé(MLPŒ¶(z(ùë¢)‚à•z(ùë£)))
+
1‚àíI(ùë¢,¬§C(ùëñ))
(1‚àílogùúé(MLPŒ¶(z(ùë¢)‚à•z(ùë£)))),(3)Algorithm 1: ProCom Pipeline
Input: Graphùê∫(V,E,X), Prompt Communities {¬§C(ùëñ)}ùëö
ùëñ=1,
Number of Predicted Communities ùëÅ
1/* Pre-training */
2Perform pre-training on graph and obtain GNN Œò(¬∑)based
on Algorithm 2
3/* Prompt Tuning */
4Perform prompt tuning and obtain Prompting Function
PTŒ¶(¬∑)based on Algorithm 3
5/* Prompt-assisted Inference */
6Generate the set of candidate communities
{ÀÜCùë£}={PTŒ¶(Nùë£)}ùë£‚ààV
7Encode prompt communities as z(¬§C(ùëñ)), ùëñ=1,2,...,ùëö
8Encode each candidate community as z(ÀÜCùë£)
9Based on distance in the embedding space between each
z(¬§C(ùëñ))andz(ÀÜCùë£)as a measure of similarity, retrieve the
ùëÅmost similar candidate communities as predicted
communities
Output: Set of Final Predicted Communities {ÀÜC}
where I(ùë¢,C)is an indicator function that returns 1 only if node
ùë¢‚ààC. Note that during the prompt tuning stage, we only optimize
the prompting function PTŒ¶(¬∑)while keeping GNN Œò(¬∑)frozen. Ad-
ditionally, due to the multiple choices for selecting node ùë£and
corresponding contexts, we can generate a substantial number of
supervision signals for learning PTŒ¶(¬∑), even when given an ex-
tremely small number of ùëö. The detailed prompt tuning process is
summarized in Algorithm 3 in the Appendix.
3.3.2 Final Predictions. In the second pretext, which focuses on
context distinction, we guide the model in learning the distinct
properties of different contexts. To align with this pretext and re-
trieve preserved knowledge, we reformulate the final predictions
of the targeted community as a similarity measure between the
candidate communities and prompt communities.
After the prompt tuning process, we move into the inference
stage as leveraging the well-learned prompting function PTŒ¶(¬∑)
to generate candidates. Specifically, we feed each node‚Äôs ego-net
Nùë£(ùë£‚ààV) to the prompting function PTŒ¶(¬∑), obtaining a set of
candidate communities {ÀÜCùë£}. With both provided prompt commu-
nities{¬§C(ùëñ)}ùëö
ùëñ=1and candidate communities {ÀÜCùë£}ùë£‚ààV, we leverage
the pre-trained encoder to obtain their respective representations:
{z(¬§C(ùëñ))}ùëö
ùëñ=1for the prompt communities and {z(ÀÜCùë£)}ùë£‚ààVfor the
candidate communities. The ùêø2distance in the latent space between
these representations serves as a measure of similarity. To make the
final predictions, we select the most similar candidate communities
for each prompt community. For example, if we aim to predict ùëÅ
new communities, we return the top ùëõ=ùëÅ
ùëömost similar candidate
communities for each prompt community ¬§C(ùëñ).
3.4 Complexity Analysis
Due to space limitations, we move the complexity analysis to the
Appendix C. In summary, the complexity of both pre-training and
3418KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xixi Wu et al.
Table 2: Statistics of datasets. The first 4 columns denote
the numbers of nodes, edges, communities, and attributes,
respectively.|C|denotes the average community size.
Dataset # N # E # C # A|C|
Facebook 3,622 72,964 130 317 15.6
Amazon 13,178 33,767 4,517 - 9.3
Livejournal 69,860 911,179 1,000 - 13.0
DBLP 114,095 466,761 4,559 - 8.4
Twitter 87,760 1,293,985 2,838 27,201 10.9
inference stages (i.e., candidate generation and similarity match-
ing) scale linearly with the size of the graph while the prompt
tuning stage (learning of PTŒ¶(¬∑)) scales linearly with the number
of provided samples.
4 Experiments
In this section, we present our experimental setup and empirical
results. Our experiments are designed to answer the following
research questions (RQs):
‚Ä¢RQ1 (Overall Performance) How does ProCom perform com-
pared with traditional community detection and semi-supervised
community detection methods?
‚Ä¢RQ2 (Transferability Study) How about the generalization
ability of ProCom?
‚Ä¢RQ3 (Prompt Sensitivity Study) How do different numbers of
prompt communities affect the performance of ProCom?
‚Ä¢RQ4 (Efficiency Study) How is the efficiency of ProCom com-
pared to that of other methods?
‚Ä¢RQ5 (Ablation Study) How do the pre-training stage and prompt-
learning stage affect the ProCom‚Äôs performance?
4.1 Experimental Setups
4.1.1 Datasets. Following previous works [ 2,40,49], we choose
five common real-world datasets containing overlapping communi-
ties from SNAP1, including Facebook, Amazon, DBLP, Livejournal,
and Twitter. Note that these datasets are partially labeled, i.e., most
nodes do not belong to any community. Thus, we can view that
there are other types of communities in the networks, and our tar-
geted communities are the labeled ones [ 40]. Statistics of datasets
are listed in Table 2. Since Amazon, DBLP, and Livejournal do
not provide nodes‚Äô attributes, we augment node features x(ùë£)=
[deg(ùë£),max(DN(ùë£)),min(DN(ùë£)),mean(DN(ùë£)),std(DN(ùë£))]
where deg(ùë£)denotes the degree of node ùë£, and DN(ùë£)={deg(ùë¢)|ùë¢‚àà
Nùë£}as SEAL [ 49] and CLARE [ 40] do. The pre-processing details
for the Livejournal dataset are explained in CLARE, while the pre-
processing details for the other datasets are provided in SEAL.
4.1.2 Baselines. To show the effectiveness of ProCom, we compare
it with both representative traditional community detection and
semi-supervised community detection methods as follows.
Traditional Community Detection Methods:
1http://snap.stanford.edu/data/Table 3: Hyper-parameters in ProCom
Stage Hyp
er-parameter V
alue
Pr
e-trainBatch
size|B| 256
Numb
er of epochs 30
Learning
rate 1e-3
Basic
unit of GNN Œò(¬∑) GCN
ùëò&
GNN layers Sear
ch from{1,2}
Emb
edding Dimension 128
T
emperature ùúè 0.1
Remaining
ratio ùúåfor corruption 0.85
Loss
Weight ùúÜforLc2cSear
ch from
{0.001,0.01,0.1,1}
Pr
omptImplementation
of PT Œ¶(¬∑) 2
layers MLP
Numb
er of epochs 30
Numb
er of prompts ùëö 10
Thr
eshold value ùõºSear
ch from
{0.1,0.2,0.3}
Learning
rate 1e-3
‚Ä¢BigClam [42]: This is a strong community detection baseline for
overlapping community detection based on matrix factorization.
‚Ä¢ComE [5]: This is a framework that jointly optimizes community
embedding, community detection, and node embedding.
‚Ä¢CommunityGAN: This is a method that extends the generative
model of BigClam from edge-level to motif-level.
Semi-supervised Community Detection Methods:
‚Ä¢Bespoke [2]: This is a semi-supervised community detection
method based on structure and size information.
‚Ä¢SEAL [49]: This method aims to learn heuristics for the targeted
community based on Generative Adversarial Networks.
‚Ä¢CLARE [40]: This is the state-of-the-art semi-supervised com-
munity detection algorithm that proposes a subgraph-based in-
ference framework, including a locator and rewriter.
4.1.3 Evaluation Metrics. For networks with ground-truth com-
munities, the most used evaluation metrics are bi-matching F1and
Jaccard scores [2,40,42,49]. GivenùëÄground-truth communi-
ties{¬§C(ùëñ)}andùëÅpredicted communities {ÀÜC(ùëó)}, the scores are
computed as:
1
2¬©¬≠
¬´1
ùëÅ‚àëÔ∏Å
ùëómax
ùëñùõø(ÀÜC(ùëó),¬§C(ùëñ))+1
ùëÄ‚àëÔ∏Å
ùëñmax
ùëóùõø(ÀÜC(ùëó),¬§C(ùëñ))¬™¬Æ
¬¨,
whereùõøcan be F1 or Jaccard function.
4.1.4 Implementation Details. We implement ProCom with Py-
Torch and Pytorch-Geometric. We conduct all experiments on GPU
machines of NVIDIA V100-32GB. Following previous works [ 40,49],
for Facebook, the number of predicted communities ùëÅis set as
200 while 1000 for Livejournal and 5000 for Amazon, DBLP, and
Twitter, respectively. We set ùëö=10for all datasets if there is no
special explanation. For all experiments, we report the averaged
score and standard deviation over 10 trials. Details of ProCom
hyper-parameter setting are shown in Table 3. The source codes
are released at https://github.com/WxxShirley/KDD2024ProCom.
3419ProCom: A Few-shot Targeted Community Detection Algorithm KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 4: Overall performance comparison under 10-shot setting (results in percent ¬±standard deviation).
‚ÄúN/A‚Äù denotes algorithm failing to converge within 2 days. Results of traditional community detection methods are reported from CLARE and
SEAL where the original papers do not provide deviations. The best and second-best results are highlighted with brown colors.
Metric DatasetT
raditional Community Detection Semi-sup
ervisedProCom Impr
ov.BigClam
ComE CommunityGAN Besp
oke SEAL CLARE
F1Faceb
ook 32.92
27.92 32.05 29.67¬±0.85 31.10¬±3.84 28.53¬±1.36 38.57¬±2.02 +17.2%
Amazon 53.79
48.23 51.09 80.38¬±0.64 82.26¬±4.04 78.89¬±2.10 84.36¬±0.23 +2.6%
Liv
ejournal 39.17
N/A 40.67 30.98¬±1.55 42.85¬±2.60 45.38¬±4.07 54.35¬±3.04 +19.8%
DBLP 40.41
25.24 N/A 41.55¬±0.40 41.74¬±6.35 48.75¬±2.51 50.96¬±1.57 +4.5%
T
witter 24.33
15.89 N/A 29.85¬±0.15 16.97¬±1.32 20.05¬±0.88 31.09¬±0.35 +4.2%
Jaccar
dFaceb
ook 23.47
18.47 21.04 20.52¬±0.76 23.02¬±2.98 19.64¬±1.16 28.05¬±1.85 +19.5%
Amazon 45.27
38.38 41.71 72.13¬±0.55 75.44¬±4.69 68.50¬±2.90 75.84¬±0.24 +0.5%
Liv
ejournal 31.02
N/A 31.83 24.97¬±1.40 35.03¬±3.27 36.38¬±3.69 44.93¬±2.96 +23.5%
DBLP 28.90
15.73 N/A 35.42¬±0.54 33.25¬±7.05 38.30¬±2.21 39.47¬±1.64 +3.1%
T
witter 15.57
8.99 N/A 19.39¬±0.13 10.55¬±0.98 12.52¬±0.63 20.78¬±0.29 +7.2%
Table 5: Transferability study. The intra-mode where pre-training and prompt learning are performed on the same dataset is
marked with lightbrown. Transferred results that outperform intra-mode are marked with brown.
Pr
e-trainPrompt F1 Jaccar
d
Faceb
ook Amazon Livejournal DBLP Twitter Faceb
ook Amazon Livejournal DBLP Twitter
Faceb
ook 38.57¬±2.02 84.46¬±0.23 53.10¬±2.74 51.64¬±1.32 31.20¬±0.42 28.05¬±1.85 75.95¬±0.24 43.54¬±2.67 40.49¬±1.52 20.85¬±0.36
Amazon 38.04¬±1.23 84.36¬±0.23 53.66¬±2.88 50.59¬±1.30 30.15¬±0.71 27.59¬±1.11 75.84¬±0.24 44.06¬±2.86 39.27¬±1.43 19.91¬±0.55
Liv
ejournal 36.50¬±1.87 83.97¬±0.80 54.35¬±3.04 51.87¬±1.67 28.62¬±1.24 26.34¬±1.66 75.46¬±0.79 44.93¬±2.96 40.42¬±1.78 18.80¬±0.89
DBLP 37.73¬±1.30 84.16¬±0.32 54.12¬±2.60 50.96¬±1.57 29.11¬±1.19 27.28¬±1.25 75.62¬±0.31 44.56¬±2.58 39.47¬±1.64 19.17¬±0.86
T
witter 37.85¬±2.66 84.49¬±0.26 54.03¬±2.50 51.23¬±1.37 31.09¬±0.35 27.59¬±1.99 75.97¬±0.28 44.48¬±2.55 39.92¬±1.60 20.78¬±0.29
4.2 Overall Performance (RQ1)
We provide the overall performance comparison in Table 4. For
traditional community detection algorithms, we report their perfor-
mance from SEAL [ 49] and CLARE [ 40] where the original papers
do not provide variations. For semi-supervised algorithms and Pro-
Com, we randomly select 10 communities as training data (prompts
forProCom) during each experiment, while the remaining com-
munities are utilized for testing. Based on the results, we note the
following key observations:
‚Ä¢..............Traditional ...............community .............detection ..............algorithms ................demonstrate ...........inferior
................performance .........under .....the...........targeted ..........setting , as evidenced by their
consistently suboptimal performance across all datasets.
‚Ä¢.....................Semi-supervised ..............algorithms ........show .................vulnerability .......when ...............confronted
......with ..........limited ...........training .......data. In the original papers of SEAL and
CLARE, the number of training communities is set as 100 or
500. However, when the training data is limited to 10 communi-
ties, the performance noticeably deteriorates, falling behind even
traditional community detection methods.
‚Ä¢............................ProCom outperforms ....all.............baselines ...in........both ..............evaluation ...........metrics
........across ....all............datasets, .................highlighting .....its...............superiority. This improve-
ment can be attributed to both the pre-training stage, which
enables the acquisition of a rich understanding of latent com-
munities in the graph, and the prompting stage, which distills
specific knowledge about targeted community.4.3 Transferability Study (RQ2)
In this subsection, we investigate the generalization ability of Pro-
Com by evaluating its performance when pre-training and prompt
learning are conducted on different datasets. To ensure consistent
input feature dimensions across datasets and enable the transfer
of pre-trained graph models, we uniformly set node features
based on their structural patterns, as described in Section 4.1.1.
The experimental results are summarized in Table 5. Based on the
results, we have the following key observations:
‚Ä¢......................ProCom exhibits .........strong ..................generalization ..........ability. Applying the pre-
trained model to arbitrary datasets yields satisfactory perfor-
mance, and, in some cases, even outperforms the intra-mode
where the pre-trained model is applied on the same dataset. This
can be attributed to the rich understanding of latent communi-
ties acquired during the pre-training stage, enabling the seamless
adaptation of the pre-trained model to retrieve specific targeted
communities from any dataset.
‚Ä¢Denser graphs such as Facebook and Twitter, which exhibit a
greater abundance of structural patterns and latent community
characteristics compared to sparser graphs like Amazon and
DBLP, provide a richer foundation for pre-training. As a re-
sult, graph models pre-trained on these denser networks tend to
achieve better performance when applied to other datasets.
3420KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xixi Wu et al.
510202550100# Prompt2834404652F1 score (%)BespokeSEALCLAREOurs510202550100# Prompt2029384756F1 score (%)BespokeSEALCLAREOurs510202550100# Prompt713192631F1 score (%)BespokeSEALCLAREOursDBLPLivejournalTwitter
Figure 3: Prompt sensitivity study on varying numbers of
prompts (training samples for semi-supervised methods).
4.4 Prompt Sensitivity Study (RQ3)
In this subsection, we delve into the sensitivity of various methods
to the number of prompts. Specifically, we compare the perfor-
mance of various semi-supervised community detection methods,
including Bespoke [ 2], SEAL [ 49], and CLARE [ 40], with that of
ProCom under varying numbers of prompts. Note that for semi-
supervised algorithms, these prompt communities are regarded as
training communities, while for ProCom, they serve as prompt. We
change the number of prompts from 5 to 100, and the results are
presented in Figure 3, where we have the following observations:
‚Ä¢..............Sensitivity ...of......................Semi-supervised .............Methods: Semi-supervised com-
munity detection algorithms exhibit significant sensitivity to the
number of training communities. When provided with a lim-
ited number of communities, their performance substantially
deteriorates.
‚Ä¢..............Robustness ....of.............ProCom: (1) Even when provided with only 5
prompt communities, it consistently achieves superior perfor-
mance across datasets; (2) As the number of prompt increases,
the performance of ProCom tends to improve further; (3) The
overall performance change within ProCom with the number of
prompt ranging from 5 to 100, is relatively minor.
4.5 Efficiency Study (RQ4)
In this subsection, we aim to demonstrate the superior efficiency of
ProCom framework. As traditional community detection methods
are found to be unfeasible for the targeted task, we investigate
the efficiency of semi-supervised community detection methods
and compare them with ProCom. Specifically, we provide the total
running times required for each method in Table 6. For ProCom,
we also present the required time for each stage, including pre-
training, prompt tuning, and inference. It is important to note that
the total running time includes not only the time required for the
three stages but also the data loading and evaluation processes.
By examining the numerical values presented in the table, we can
observe the remarkable efficiency of ProCom. Even on the largest
dataset, Twitter, the total running time is a mere 446 seconds. This
showcases its strong efficiency, highlighting its ability to handle
the targeted setting effectively within a reasonable time frame.Table 6: Efficiency study with numerical values of total run-
ning times. For ProCom, values in parentheses indicate the
pre-training, prompt tuning, and inference times, respec-
tively. ‚Äús‚Äù, ‚Äúm‚Äù, and ‚Äúh‚Äù denote second, minute, and hour.
Dataset Bespoke SEAL CLARE ProCom
Facebook 4s 2h27m 275s 30s (22s / 3s / 4s)
Amazon 110s 1h3m 529s 144s (15s / 1s/ 14s)
Livejournal 41s 3h35m 832s 260s (97s / 10s / 136s)
DBLP 139s 50m 22m 367s (54s / 4s / 175s)
Twitter 126s 2h28m 36m 446s (132s / 75s / 137s)
4.6 Ablation Study (RQ5)
4.6.1 Comparison with Graph Pre-training Methods. To demon-
strate the effectiveness of ProCom‚Äôs context-aware pre-training
method, we compare it with several widely used strategies for pre-
training GNNs. These strategies include:
‚Ä¢.......Node ............Attribute ............Masking These methods [ 11,34] typically first
mask node attributes and then let GNNs predict those attributes
based on neighboring structures. We omit these methods from
our comparison as most experimental datasets lack meaningful
node attributes.
‚Ä¢......Link ..............Prediction This strategy, employed by models such as GAE
[15], focuses on the reconstruction task of predicting the exis-
tence of edges between pairs of nodes.
‚Ä¢..................Node-to-Node .................Consistency These methods [ 27,51] aim to max-
imize the mutual information between identical nodes under
different augmented views to obtain robust node-level repre-
sentations. For comparison, we use the augmentation strategies
proposed in GraphCL [45] and SimGRACE [41] to generate
different augmented views.
‚Ä¢...................Node-to-Graph ...........Mutual ................Information ..................Maximization These meth-
ods such as DGI [36] learn the GNNs by maximizing the mutual
information between a single node and the entire graph.
‚Ä¢.....................Graph-to-Graph ................Consistency These methods [ 44] maximize the
mutual information between identical graphs under different
augmented views. We omit these methods from comparison as
they are applied to multi-graph datasets, whereas each of our
experimental datasets consists of a single graph.
Specifically, we replace our context-aware graph pre-training
method with each of the above methods while keeping all other
stages consistent within the ProCom framework. The results are
summarized in Table 7. From the results, it is evident that utiliz-
ing other graph pre-training methods consistently leads to perfor-
mance degradation. This can be attributed to the fact that existing
graph pre-training methods do not explicitly consider the latent
communities in the network. Instead, they focus on preserving
individual node features or simple contextual information. Conse-
quently, even when provided with prompts, the retained knowledge
may not be valuable in enhancing downstream predictions. ....On.....the
............contrary, .....our...................context-aware .................pre-training ............approach ....is...............specifically
...........designed ....to...........capture .....the................community ...............knowledge .............inherent ...in......the
........graph, ..............benefiting ...............subsequent .................downstream ......task.
4.6.2 Effectiveness of Both Pretexts and Prompt Learning. To eval-
uate the effectiveness of both pre-training objectives and prompt
3421ProCom: A Few-shot Targeted Community Detection Algorithm KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Table 7: Ablation study on the effectiveness of our Dual-level Context-aware Pre-training. The experiments are conducted by
replacing it with other graph pre-training methods within the ProCom framework.
F1 Jaccar
dDatasetGAE DGI GraphCL SimGRA
CE Ours GAE DGI GraphCL SimGRA
CE Ours
Faceb
ook 35.64¬±2.19 33.11¬±3.43 29.88¬±4.59 31.42¬±6.22 38.57¬±2.02 25.40¬±1.75 23.47¬±3.02 20.52¬±3.81 21.99¬±5.24 28.05¬±1.85
Amazon 83.70¬±0.68 83.85¬±0.64 84.25¬±0.28 84.25¬±0.24 84.36¬±0.23 75.21¬±0.59 75.30¬±0.62 75.68¬±0.31 75.69¬±0.27 75.84¬±0.24
Liv
ejournal 52.23¬±4.39 52.25¬±2.75 52.98¬±3.09 53.44¬±2.90 54.35¬±3.04 42.88¬±4.12 42.67¬±2.58 43.38¬±3.00 43.79¬±2.86 44.93¬±2.96
DBLP 46.42¬±2.88 47.80¬±3.29 49.79¬±1.89 50.68¬±1.76 50.96¬±1.57 35.33¬±2.78 36.79¬±2.97 38.30¬±1.95 39.17¬±1.82 39.47¬±1.64
T
witter 23.28¬±4.29 24.62¬±1.65 24.65¬±3.10 28.22¬±2.46 31.09¬±0.35 15.09¬±2.93 15.60¬±1.22 15.68¬±2.30 18.38¬±1.89 20.78¬±0.29
w/o. Promptw/o. n2cw/o. c2cFull
F1Jaccard2025303540Score (%)Facebook
F1Jaccard3540455055Livejournal
F1Jaccard25354555DBLP
Figure 4: Ablation study on the effectiveness of both pretexts
and prompt learning within the ProCom framework.
learning, we conduct the ablation study using the following variants
of the ProCom framework:
‚Ä¢w/o. n2c This variant removes the first pre-training objective,
node-to-context proximity Ln2c, and only utilizes the second
pretext during the pre-training stage. All other stages remain
consistent within the ProCom.
‚Ä¢w/o. c2c Similarly, this variant is designed to evaluate the effec-
tiveness of the second pre-training objective, context distinction.
We omitLc2cduring the pre-training stage while keeping all
other designs consistent.
‚Ä¢w/o. Prompt This variant is designed to assess the effective-
ness of prompt learning. After pre-training, we omit the prompt
tuning stage and directly use the provided 10-shot communities
as patterns. We consider each node‚Äôs ùëò-ego net as a candidate
community, and predictions are made via similarity matching
without any learnable process.
The results of the ablation study are shown in Figure 4, where
we can observe that: (1) Both pre-training objectives significantly
contribute to enhancing the performance of ProCom, as removing
either of them consistently leads to poorer performance. (2) The
prompt learning stage plays a crucial role within the ProCom, as the
performance drops significantly when ‚Äúw/o. Prompt‚Äù variant is used.
This is because the assumption of ùëò-ego net as a candidate commu-
nity is too rigid and inflexible, resulting in inferior performance
when directly matching ego-net candidates with prompt communi-
ties. In contrast, the prompt learning stage provides insights into the
structural patterns specific to the targeted community, facilitating
a more precise candidate generation process.
To further illustrate the effectiveness of the prompt learning
process, we conducted a case study on the DBLP and Livejournal
Raw ContextDistilled CommunityDBLPRaw ContextDistilled CommunityLivejournal
Node in the same ground-truth communityNode not in the same ground-truth communityFigur
e 5: Case study of how prompting function works.
datasets. For each predicted community, we compared its origi-
nal context, i.e., ego-net, with the distilled community to assess
the effectiveness of the prompting function in eliminating irrele-
vant nodes in Figure 5. .....The.........results ................demonstrate ......that.....the..............prompting
...........function .............effectively ..............eliminates .............irrelevant .........nodes, ............resulting ...in..........distilled
.................communities ............resemble .....the..................ground-truth.
5 Conclusion
In this paper, we address the challenge of heavy reliance on labeled
data in the targeted community detection task. To overcome this
challenge, we apply the ‚Äúpre-train, prompt‚Äù paradigm and propose
ProCom. Our framework leverages a context-aware pre-training
method to establish a rich knowledge foundation of communities
present in the graph. By incorporating few-shot samples from the
targeted community, the prompt learning stage extracts specific
preserved knowledge, facilitating accurate inference with mini-
mal tuning burden. ProCom showcases remarkable transferability
across datasets, indicating its potential in building a graph founda-
tional model specifically for the community detection task.
Acknowledgements
This work is funded in part by the National Natural Science Foun-
dation of China Projects No. U1936213. This work is also partially
supported by NSF through grants IIS-1763365 and IIS-2106972.
3422KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xixi Wu et al.
References
[1]Lars Backstrom, Daniel P. Huttenlocher, Jon M. Kleinberg, and Xiangyang Lan.
2006. Group formation in large social networks: membership, growth, and
evolution. In Knowledge Discovery and Data Mining.
[2]Arjun Bakshi, Srinivasan Parthasarathy, and Kannan Srinivasan. 2018. Semi-
Supervised Community Detection Using Structure and Size. In ICDM. 869‚Äì874.
[3]Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-
vre. 2008. Fast unfolding of communities in large networks. Journal of Statistical
Mechanics: Theory and Experiment 2008 (2008), 10008.
[4]Tom B. Brown, Benjamin Mann, and Nick Ryder et al. 2020. Language Models
are Few-Shot Learners. ArXiv abs/2005.14165 (2020).
[5]Sandro Cavallari, V. Zheng, HongYun Cai, K. Chang, and E. Cambria. 2017. Learn-
ing Community Embedding with Community Detection and Node Embedding
on Graphs. In CIKM.
[6]Xi Chen, Siwei Zhang, Yun Xiong, Xixi Wu, Jiawei Zhang, Xiangguo Sun, Yao
Zhang, Yinglong Zhao, and Yulin Kang. 2024. Prompt Learning on Temporal
Interaction Graphs. arXiv:2402.06326 (2024). arXiv:2402.06326
[7]Aaron Clauset, Mark E. J. Newman, and Cristopher Moore. 2004. Finding com-
munity structure in very large networks. Physical review. E, Statistical, nonlinear,
and soft matter physics 70 6 Pt 2 (2004).
[8]Shuheng Fang, Kangfei Zhao, Guanghua Li, and Jeffrey Xu Yu. 2023. Community
Search: A Meta-Learning Approach. In 2023 IEEE 39th International Conference
on Data Engineering (ICDE). 2358‚Äì2371.
[9] Taoran Fang, Yunchao Zhang, Yang Yang, Chunping Wang, and Lei Chen. 2023.
Universal Prompt Tuning for Graph Neural Networks. In Thirty-seventh Confer-
ence on Neural Information Processing Systems.
[10] Yixiang Fang, Reynold Cheng, Siqiang Luo, and Jiafeng Hu. 2016. Effective
community search for large attributed graphs. Proc. VLDB Endow. 9, 12 (aug
2016), 1233‚Äì1244. https://doi.org/10.14778/2994509.2994538
[11] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, C. Wang, and
Jie Tang. 2022. GraphMAE: Self-Supervised Masked Graph Autoencoders. Pro-
ceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (2022).
[12] Xia Hu, Jiliang Tang, and Huan Liu. 2014. Online Social Spammer Detection. In
AAAI Conference on Artificial Intelligence.
[13] Yuting Jia, Qinqin Zhang, Weinan Zhang, and Xinbing Wang. 2019. Community-
GAN: Community Detection with Generative Adversarial Nets. In WWW.
[14] Di Jin, Zhizhi Yu, Pengfei Jiao, Shirui Pan, Philip S. Yu, and Weixiong Zhang.
2021. A Survey of Community Detection Approaches: From Statistical Modeling
to Deep Learning. CoRR abs/2101.01669 (2021).
[15] Thomas Kipf and Max Welling. 2016. Variational Graph Auto-Encoders. ArXiv
abs/1611.07308 (2016).
[16] Gueorgi Kossinets and Duncan J. Watts. 2009. Origins of Homophily in an
Evolving Social Network1. Amer. J. Sociology 115 (2009), 405 ‚Äì 450.
[17] Nevan J. Krogan, Gerard Cagney, Haiyuan Yu, Gouqing Zhong, Xinghua Guo,
and et al Alex Ignatchenko, Joyce Li. 2006. Global landscape of protein complexes
in the yeast Saccharomyces cerevisiae. Nature 440 (2006), 637‚Äì643.
[18] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale
for Parameter-Efficient Prompt Tuning. In Conference on Empirical Methods in
Natural Language Processing.
[19] Ye Li, Chaofeng Sha, Xin Huang, and Yanchun Zhang. 2018. Community Detection
in Attributed Graphs: An Embedding Approach. In AAAI.
[20] Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. 2024. ZeroG: Inves-
tigating Cross-dataset Zero-shot Transferability in Graphs. ArXiv abs/2402.11235
(2024).
[21] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen,
and Muhan Zhang. 2024. One for All: Towards Training One Graph Model for
All Classification Tasks. In International Conference on Learning Representations
(ICLR).
[22] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
Graham Neubig. 2023. Pre-train, Prompt, and Predict: A Systematic Survey of
Prompting Methods in Natural Language Processing. ACM Comput. Surv. 55
(2023), 35 pages.
[23] Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. 2023. GraphPrompt:
Unifying Pre-Training and Downstream Tasks for Graph Neural Networks. Pro-
ceedings of the ACM Web Conference 2023 (2023).
[24] Yihong Ma, Ning Yan, Jiayu Li, Masood Mortazavi, and Nitesh V. Chawla. 2024.
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous
Graph Neural Networks. In Proceedings of the ACM Web Conference 2024.
[25] Namyong Park, Ryan Rossi, Eunyee Koh, Iftikhar Ahamath Burhanuddin,
Sungchul Kim, Fan Du, Nesreen Ahmed, and Christos Faloutsos. 2022. CGC: Con-
trastive Graph Clustering ForCommunity Detection and Tracking. In Proceedings
of the ACM Web Conference 2022. 1115‚Äì1126.
[26] Bryan Perozzi, Leman Akoglu, Patricia Iglesias S√°nchez, and Emmanuel M√ºller.
2014. Focused clustering and outlier detection in large attributed graphs. Proceed-
ings of the 20th ACM SIGKDD international conference on Knowledge discoveryand data mining (2014).
[27] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding,
Kuansan Wang, and Jie Tang. 2020. GCC: Graph Contrastive Coding for Graph
Neural Network Pre-Training. Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (2020).
[28] Frank Rosenblatt. 1963. PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS
AND THE THEORY OF BRAIN MECHANISMS. American Journal of Psychology
76 (1963), 705.
[29] Jianbo Shi and Jitendra Malik. 1997. Normalized cuts and image segmentation.
InCVPR. 731‚Äì737.
[30] Fan-Yun Sun, Meng Qu, Jordan Hoffmann, Chin-Wei Huang, and Jian Tang.
2019. vGraph: A Generative Model for Joint Community Detection and Node
Representation Learning. In NIPS.
[31] Mingchen Sun, Kaixiong Zhou, Xingbo He, Ying Wang, and Xin Wang. 2022.
GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Net-
works. Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (2022).
[32] Xiangguo Sun, Hongtao Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One:
Multi-Task Prompting for Graph Neural Networks. Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (2023).
[33] Xiangguo Sun, Jiawen Zhang, Xixi Wu, Hong Cheng, Yun Xiong, and Jia Li. 2023.
Graph Prompt Learning: A Comprehensive Survey and Beyond. arXiv:2311.16534
(2023).
[34] Zhen Tan, Ruocheng Guo, Kaize Ding, and Huan Liu. 2023. Virtual Node Tuning
for Few-shot Node Classification. Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (2023).
[35] A√§ron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning
with Contrastive Predictive Coding. ArXiv abs/1807.03748 (2018).
[36] Petar Velickovic, William Fedus, William L. Hamilton, Pietro Lio‚Äô, Yoshua Bengio,
and R. Devon Hjelm. 2018. Deep Graph Infomax. ArXiv abs/1809.10341 (2018).
[37] Xiao Wang, Di Jin, Xiaochun Cao, Liang Yang, and Weixiong Zhang. 2016. Se-
mantic Community Identification in Large Attribute Networks. In AAAI.
[38] Yuyao Wang, Jie Cao, Zhan Bu, Jia Wu, and Youquan Wang. 2023. Dual Struc-
tural Consistency Preserving Community Detection on Social Networks. IEEE
Transactions on Knowledge and Data Engineering 35, 11 (2023), 11301‚Äì11315.
[39] Yingying Wang, Yun Xiong, Xixi Wu, Xiangguo Sun, and Jiawei Zhang. 2024.
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt
Learning. arXiv:2402.11472 (2024). arXiv:2402.11472
[40] Xixi Wu, Yun Xiong, Yao Zhang, Yizhu Jiao, Caihua Shan, Yiheng Sun, Yangyong
Zhu, and Philip S. Yu. 2022. CLARE: A Semi-supervised Community Detection
Algorithm. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining.
[41] Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z. Li. 2022. SimGRACE: A
Simple Framework for Graph Contrastive Learning without Data Augmentation.
InProceedings of the ACM Web Conference 2022.
[42] Jaewon Yang and Jure Leskovec. 2013. Overlapping community detection at scale:
a nonnegative matrix factorization approach. In WSDM. 587‚Äì596.
[43] Jaewon Yang, Julian McAuley, and Jure Leskovec. 2013. Community Detection in
Networks with Node Attributes. In ICDM.
[44] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. 2021. Graph Con-
trastive Learning Automated. In International Conference on Machine Learning.
[45] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and
Yang Shen. 2020. Graph Contrastive Learning with Augmentations. In NIPS.
[46] Jianke Yu, Hanchen Wang, Xiaoyang Wang, Zhao Li, Lu Qin, Wenjie Zhang,
Jian Liao, and Ying Zhang. 2023. Group-based Fraud Detection Network on
e-Commerce Platforms. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining. 5463‚Äì5475.
[47] Xingtong Yu, Yuan Fang, Zemin Liu, and Xinming Zhang. 2024. HGPROMPT:
Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learn-
ing. In AAAI Conference on Artificial Intelligence.
[48] Tianqi Zhang, Yun Xiong, Jiawei Zhang, Yao Zhang, Yizhu Jiao, and Yangyong
Zhu. 2020. CommDGI: Community Detection Oriented Deep Graph Infomax. In
Proceedings of the 29th ACM International Conference on Information & Knowledge
Management. 1843‚Äì1852.
[49] Yao Zhang, Yun Xiong, Yun Ye, Tengfei Liu, Weiqiang Wang, Yangyong Zhu,
and Philip S. Yu. 2020. SEAL: Learning Heuristics for Community Detection
with Generative Adversarial Networks. In Proceedings of the 26th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining.
[50] Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, and Jia Li. 2024. All
in One and One for All: A Simple yet Effective Method towards Cross-domain
Graph Pretraining. ArXiv abs/2402.09834 (2024).
[51] Yanqiao Zhu, Yichen Xu, Feng Yu, Q. Liu, Shu Wu, and Liang Wang. 2020. Deep
Graph Contrastive Representation Learning. ArXiv abs/2006.04131 (2020).
[52] Chenyi Zi, Haihong Zhao, Xiangguo Sun, Yiqing Lin, Hong Cheng, and Jia Li.
2024. ProG: A Graph Prompt Learning Benchmark. arXiv:2406.05346 (2024).
arXiv:2406.05346
3423ProCom: A Few-shot Targeted Community Detection Algorithm KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
A Algorithm Details
In this section, we show the detailed algorithm processs of pre-
training and prompt tuning in Algorithms 2 and 3, respectively.
Algorithm 2: ProCom Pre-training
Input: Graphùê∫(V,E,X)
1Initialize GNN Œò(¬∑)
2while not converge do
3 Randomly sample a batch of nodes B‚äÇV
4 forùë£‚ààBdo
5 Extract node ùë£‚Äôs contextNùë£, represent its
corresponding subgraph as (Xùë£,Aùë£)
6 Encode this subgraph as Zùë£= GNN Œò(Xùë£,Aùë£),
retrieve node representation z(ùë£)and context
representation z(Nùë£)=Sum-Pooling(Zùë£)
7 Compute corrupted context representation
z(eNùë£)=Sum-Pooling(GNN Œò(Xùë£,eAùë£))
8 Update Œòby applying gradient descent to minimize
Lpre-train based on Equations 1 and 2
Output: Pre-trained Graph Model GNN Œò(¬∑)
Algorithm 3: ProCom Prompt Tuning
Input: Graphùê∫(V,E,X), Pre-trained Model GNN Œò(¬∑),
Prompt Communities {¬§C(ùëñ)}ùëö
ùëñ=1
1Obtain all nodes‚Äô representations as Z=GNN Œò(X,E)
2Initialize Prompting Function PT Œ¶(¬∑)
3while not converge do
4 Pick a node ùë£from a randomly sampled prompt
community¬§C
5 Extractùë£‚Äôs ego-netNùë£, retrieve node embeddings
{z(ùë¢)}ùë¢‚ààNùë£from Z
6 Update Œ¶by applying gradient descent to minimize Lpt
based on Equation 3
Output: Tuned Prompting Function PT Œ¶(¬∑)
B Discussion of Pooling Operations
We provide a proof that both Mean- and Sum-pooling methods for
computing z(Nùë£)result in identical outcomes when calculating
Lpre-train .
Recall thatLn2c=√ç
ùë£‚ààB‚àílogexp(sim(z(ùë£),z(Nùë£))/ùúè)√ç
ùë¢‚ààBexp(sim(z(ùë£),z(Nùë¢))/ùúè). Given
sim(a,b)=a¬∑b
‚à•a‚à•‚à•b‚à•, if we compute z(Nùë£)via Mean-Pooling as
z(Nùë£)=1
|Nùë£|√ç
ùëò‚ààNùë£z(ùëò), the factor1
|Nùë£|can be divided by boththe numerator and the denominator in the sim(¬∑,¬∑). Consequently,
this is equivalent to computing z(Nùë£)using Sum-pooling. There-
fore, we can conclude that both Mean- and Sum-pooling operations
yield the same results for this loss function. Similar reasoning can
be applied toLc2c, leading to the conclusion that both Mean- and
Sum-pooling operations yield the same results for Lpre-train .
C Complexity Analysis
In this section, we analyze the complexity of the ProCom frame-
work. We discuss the complexity of each stage in detail with the
graph defined as ùê∫=(V,E,X).
‚Ä¢Pre-training (Algorithm 2) Within each epoch, we randomly
sample a batch of nodes Band extract their contexts to compute
Ln2c. The complexity of corresponding graph convolution is
O(|E‚Ä≤|ùêøùëë)where|E‚Ä≤|‚â™|E| represents the sum of edges within
each context, ùêø=ùëòdenotes the number of convolutional layers,
andùëërepresents the embedding dimension. Additionally, the
complexity of computing Ln2cisO(|B|ùëë+|B|2ùëë)and we have
|B| ‚â™ |V| . Next, we analyze the computation of Lc2c. The
complexity of graph convolution on the corrupted context is
O(ùúå|E‚Ä≤|ùêøùëë)whereùúå‚àà(0,1]represents the ratio of remaining
edges. Therefore, the complexity within one pre-training epoch
isO (1+ùúå)|E‚Ä≤|ùêøùëë+2(|B|ùëë+|B|2ùëë)<O(|E|+|V|) .
‚Ä¢Prompt Tuning (Algorithm 3) Before prompt tuning, we first
compute the embeddings of all nodes based on the pre-trained
graph model (Line 1), resulting in a complexity of O(|E|ùêøùëë). We
use ¬Ø|N|‚â™|V| to denote the average size of a node‚Äôs context,
i.e., ego-net. Within each epoch of prompt tuning (Lines 4-6),
the complexity isO(ùëö¬Ø|N|ùêø‚Ä≤ùëë)‚â™O(|V|) whereùêø‚Ä≤denotes the
number of layers within PTŒ¶(¬∑)andùëörefers to the number of
prompt communities.
‚Ä¢Inference (Lines 6-9 in Algorithm 1) The inference process
includes two steps, i.e., candidate generation and similarity match-
ing. For candidate generation, we extract the context of each node
and feed it to the well-tuned prompting function (Line 6), result-
ing in a complexity of O(|V|| ¬ØN|ùêø‚Ä≤ùëë)where|¬ØN|still represents
the average size of a node‚Äôs context. Before similarity matching,
we encode both candidate and prompt communities (Lines 7-8)
with a complexity that does not exceed O(|E|ùêøùëë). And the com-
plexity of similarity matching ( ùêø2Distance) between candidates
and prompts is less than O(ùëö|V|ùëë)since the number of distilled
candidates is less than the number of nodes |V|in the graph.
Therefore, the overall complexity of the inference stage is smaller
thanO(|V|| ¬ØN|ùêø‚Ä≤ùëë+|E|ùêøùëë+ùëö|V|ùëë)<O(|E|+|V|) .
........Based ....on....the........above ............analysis, .....the..............complexity ...of.......both ................pre-training
.....and............inference ........stages .........within ....................ProCom scales ..........linearly .......with ....the...........number
...of........nodes .....and........edges ...in.....the.........graph. .....The...............complexity ....of..........prompt .........tuning
.......scales ...........linearly ......with .....the..........number ...of............provided ...........samples, ....................demonstrating
....the..............efficiency.
3424