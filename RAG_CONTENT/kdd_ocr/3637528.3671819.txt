Approximate Matrix Multiplication over Sliding Windows
Ziqi Yao∗
East China Normal University
Shanghai, China
51265902073@stu.ecnu.edu.cnLianzhi Li∗
East China Normal University
Shanghai, China
51265902102@stu.ecnu.edu.cnMingsong Chen
East China Normal University
Shanghai, China
mschen@sei.ecnu.edu.cn
Xian Wei
East China Normal University
Shanghai, China
xwei@sei.ecnu.edu.cnCheng Chen†
East China Normal University
Shanghai, China
chchen@sei.ecnu.edu.cn
ABSTRACT
Large-scale streaming matrix multiplication is very common in
various applications, sparking significant interest in develop effi-
cient algorithms for approximate matrix multiplication (AMM) over
streams. In addition, many practical scenarios require to process
time-sensitive data and aim to compute matrix multiplication for
most recent columns of the data matrices rather than the entire ma-
trices, which motivated us to study efficient AMM algorithms over
sliding windows. In this paper, we present two novel deterministic
algorithms for this problem and provide corresponding error guar-
antees. We further reduce the space and time costs of our methods
for sparse matrices by performing an approximate singular value
decomposition which can utilize the sparsity of matrices. Exten-
sive experimental results on both synthetic and real-world datasets
validate our theoretical analysis and highlight the efficiency of our
methods.
CCS CONCEPTS
•Mathematics of computing →Computations on matrices.
KEYWORDS
Streaming data, Sliding window, Approximate Matrix Multiplica-
tion
ACM Reference Format:
Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei, and Cheng Chen. 2024.
Approximate Matrix Multiplication over Sliding Windows. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/3637528.3671819
∗Both authors contributed equally to this research.
†Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36718191 INTRODUCTION
Matrix multiplication is one of the most fundamental subroutines
in machine learning and data mining. It has been applied to many
machine learning tasks such as regression [ 26,35], clustering [ 8,12],
online learning [ 1,14,21,33] and canonical component analy-
sis [16,36]. The ever-increasing size of data matrices poses great
challenges to the computational efficiency of matrix multiplica-
tion, and reinforces efforts to design efficient approximate matrix
multiplication (AMM) methods.
Many recent works [ 13,20,24,32,35,36] consider AMM in
the streaming model, where columns of the data matrices arrive
sequentially. Among these methods, Co-occurring Directions (COD)
and its variants enjoy smaller approximation errors and higher
robustness. Despite the success of COD in streaming AMM, it does
not fully address the situation where the timeliness of data is very
important, i.e., people are more interested in the recent data rather
than the outdated data. Such setting is very common in many real-
world applications. For example, in social media analysis, each
column of the data matrix corresponds to a document (e.g. the
content of a Twitter post) along with a corresponding timestamp.
Usually, advertisers are more interested in the tweets posted in
the most recent week or month. In user behavior analysis, the
correlation between user searches and ad clicks in the most recent
period are more attractive to researchers.
In these time-sensitive applications, the sliding window model is
more appropriate than the unbounded streaming model [ 3,6,7,23,
27,34]. The sliding window model [ 10] only considers a window
of size𝑁, i.e., the𝑁most recent columns of the data matrices.
We call the data in the window active data, in contrast to the so-
called expired data. In this paper, we study AMM algorithms over
sliding windows. Specifically, We have two streaming matrices
X∈R𝑚𝑥×𝑛andY∈R𝑚𝑦×𝑛. The algorithm sequentially receives
the columns of matrices Xand Yand maintains two low-rank
sketches A∈R𝑚𝑥×ℓandB∈R𝑚𝑦×ℓ, where the number of columns
ℓis significantly smaller than 𝑛. The objective of the algorithm
is to guarantee that AB𝑇is a good approximation of XY𝑇(i.e.,
AB𝑇≈XY𝑇) while minimizing the algorithm’s space and time
cost. Though streaming AMM has been widely studied, few works
explore the AMM over sliding windows. Generally, computing
matrix sketching in the sliding window model is harder that in the
streaming model. Wei et al . [34] showed that if X=Y, maintaining
XY𝑇only requires 𝑂(𝑚𝑥𝑚𝑦)time and𝑂(𝑚𝑥𝑚𝑦)space but tracking
XY𝑇exactly in the sliding window models requires the algorithm
3896
KDD ’24, August 25–29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
to store all columns in the window. Thus developing algorithms for
AMM in the sliding window model requires novel techniques.
In this paper, we provide two algorithms for AMM over sliding
windows, called Exponential Histogram Co-occurring Directions
(EH-COD) and Dyadic Interval Co-occurring Directions (DI-COD).
The EH-COD algorithm, based on the idea of Exponential His-
togram [ 10], stores stream data in blocks and maintains an ap-
proximation of the window content with space much smaller than
the window size through regular merging. The DI-COD algorithm,
based on the dyadic interval structure [ 2], constructs a tree-like
structure from the stream data blocks, and concatenate sketches
from each layer to generate the final approximation matrix. The DI-
COD algorithm has better approximation quality than the EH-COD
algorithm when the maximum column norm of the data matrix is
small. We further improve the time and space costs of our meth-
ods for sparse matrices by leveraging the subspace power method
(SPM) [ 25,35] which can utilize the sparsity of matrices. Our main
contributions can be summarized as follows:
•We develop two efficient algorithms for AMM in the sliding
window models. We provide error analysis for both algo-
rithms as well as their time and space complexities.
•We propose improved algorithm for approximate sparse ma-
trix multiplication in the sliding window models.
•We empirically study the performance of proposed methods
on both synthetic and real-world datasets. The experimental
results validate the effectiveness of the proposed approaches.
Paper Organization. The rest of the paper is organized as follows.
In Section 2, we introduce previous works related to this paper.
Then we define the notation used in this paper and introduce the
Co-occurring directions algorithm in Section 3. In Section 4, we
presents our EH-COD and DI-COD algorithms and show their
approximation error bound and complexity analysis. In Section
5, we describe the improved variants of our methods for sparse
matrices. We present our empirical results in Section 6 and provide
conclusions in Section 7.
2 RELATED WORKS
In this section, we first review prior works on approximate matrix
multiplication. Then we introduce sketching methods in the sliding
window model.
2.1 Approximate Matrix Multiplication
The AMM problem has been widely studied in recent decades. Ex-
isting approaches to the AMM problem can be broadly categorized
into two types: randomized algorithms, exemplified by random
sampling [ 13] and random projection [ 9,22,31], and deterministic
algorithms, represented by FD-AMM [ 36] and COD [ 24]. Random-
ized algorithms exhibit favorable time complexity and come with
theoretical guarantees, yet these methods require larger sketch
size to achieve same approximation error as deterministic methods.
The deterministic algorithms leverage singular value decomposi-
tion (SVD) to generate approximate matrices and usually enjoys
smaller approximation errors and better adaptability to the stream-
ing setting. The FD-AMM [ 36] algorithm employs the Frequent
Directions (FD) [ 15,18] to process the stacked matrix Z=[X;Y]
column by column, where XandYare the input matrices. The CODalgorithm computes the correlation sketch by shrinking the singu-
lar values of XandYin each iteration. Compared with FD-AMM,
the COD algorithm [ 24] usually have better empirical performance.
Recently, Wan and Zhang [32] and Luo et al . [20] proposed sparse
variants of COD algorithm, which utilize the sparsity of the input
matrices to improve the time and space costs. Blalock and Guttag
[5]proposed a learning-augmented methods for AMM problem
and achieves better speed-quality tradeoff. However, their method
requires training data and is difficult to adapt to the streaming
or sliding window model. To the best of our knowledge, none of
previous works adapt well to AMM in the sliding window models.
2.2 Sliding Window Algorithms
The sliding window model [ 11] is designed for analyzing data
within a window of most recent elements in the stream. Many
existing works studied various queries over sliding windows such
as distinct elements, frequency count, top- 𝑘and skyline. Datar et al .
[10] proposed an effective sliding window framework based on the
Exponential Histogram, creating a logarithmic hierarchical struc-
ture that can effectively approximate the count, sum, and vector
norms of the elements within the window. Arasu and Manku [2]
addressed the problems of element counting and quantiles within
the window using a structure called dyadic intervals.
Random sampling can generate sketches of the data within the
window by maintaining a collection of random samples from the
window [ 3,19]. Recently, Braverman et al . [7] adapted the random
sampling technique to numerical linear algebra over sliding win-
dows. They achieve near optimal space cost in several linear algebra
tasks such as spectral approximation and low-rank approximation.
Badeau et al . [4] studied SVD algorithm in the sliding window
model, but their method needs to store all data in the window rather
than store a sketch. Recently, Wei et al . [34] studied approximating
covariance matrix over sliding windows, which can be regard as
a special case of AMM, i.e., X=Y. Their proposed Logarithmic
Method and Dyadic Interval framework combined with the FD
algorithm can achieve 𝜀-approximation with logarithmic space
complexity relative to the window size. Their methods are later
generalized to the distributed setting by Zhang et al . [37] . However,
few works studied AMM problem in the sliding window model.
3 PRELIMINARIES
In this section, we first introduce the notation used throughout the
paper. Then, we present our problem definition, followed by the
description of the COD algorithm and its theoretical guarantees.
3.1 Notations
We let I𝑛be the𝑛×𝑛identity matrix, and 0m×nbe the𝑚×𝑛matrix of
all zeros. We can denote a 𝑚×𝑛matrix as X=[x1,x2,..., x𝑛], where
x𝑖∈R𝑚is the𝑖-th column of X. We use[X1,X2]to denote their
concatenation on their column dimensions. For a vector x∈R𝑑,
we let∥x∥2=√︃Í𝑑
𝑖=1𝑥2
𝑖be itsℓ2-norm. For a matrix X∈R𝑚×𝑛,
we let∥X∥=max u:∥u∥=1∥Xu∥be its spectral norm and ∥X∥𝐹=√︃Í𝑛
𝑖=1∥x𝑖∥2be its Frobenius norm. The condensed singular value
decomposition (SVD) of matrix X, written as SVD(X), is defined
asUΣV𝑇where U∈R𝑚×𝑟andV∈R𝑛×𝑟are column orthonormal
3897Approximate Matrix Multiplication over Sliding Windows KDD ’24, August 25–29, 2024, Barcelona, Spain.
Algorithm 1 Co-occurring Directions(COD)
1:Input: X∈R𝑚𝑥×𝑛,Y∈R𝑚𝑦×𝑛and sketch size ℓ.
2:A←0𝑚𝑥×ℓ,B←0𝑚𝑦×ℓ.
3:for𝑖=1,2,...,𝑛 do
4: Insert x𝑖into a zero valued column of A.
5: Insert y𝑖into a zero valued column of B.
6:ifAorBhas no zero valued columns then
7:[Q𝑥,R𝑥]← QR(A).
8:[Q𝑦,R𝑦]← QR(B).
9:[U,Σ,V]← SVD(R𝑥R𝑇𝑦).
10:𝛿←𝜎ℓ/2(Σ).
11: ˆΣ←max(Σ−𝛿Iℓ,0).
12: A←Q𝑥U√︁
ˆΣ,B←Q𝑦V√︁
ˆΣ.
13: end if
14:end for
15:Output: AandB.
andΣis a diagonal matrix with nonzero singular values 𝜎1(X)≥
𝜎2(X)≥···≥𝜎𝑟(X)>0. We use nnz(X)to denote number of
nonzero elements of matrix X.
3.2 Problem Setup
We first provide the definition of correlation sketch as follows:
Definition 3.1 ([ 24]).LetX∈R𝑚𝑥×𝑛,Y∈R𝑚𝑦×𝑛,A∈R𝑚𝑥×ℓ
andB∈R𝑚𝑦×ℓwhere𝑛≥max(𝑚𝑥,𝑚𝑦)andℓ≤min(𝑚𝑥,𝑚𝑦).
We call the pair(A,B)is an𝜀-correlation sketch of (X,Y)if the
correlation error satisfies
corr-err
XY𝑇,AB𝑇
≜XY𝑇−AB𝑇2
∥X∥𝐹∥Y∥𝐹≤𝜀.
This paper considers the problem of approximate matrix multi-
plication (AMM) over sliding windows. At time step 𝑡, the algorithm
receives column pairs (x𝑡,y𝑡)from the original matrices XandY.
Assuming the window size is 𝑁, we use𝑊to denote the sliding
window, which consists of 𝑁most recent columns of XandY. We
use matrices X𝑊andY𝑊to denote the submatrices of XandYin
the windows, respectively. The algorithm aims to maintain a pair
(A,B), which is an 𝜀-correlation sketch of (X𝑊,Y𝑊). Similar to
[34], we assume that both XandYdo not contain zero columns,
and the square norms of the columns in XandYare normalized to
[1,𝑅X]and[1,𝑅Y], respectively. We let 𝑅=max(𝑅X,𝑅Y).
3.3 Co-occurring Directions
Co-occurring directions (COD) [ 24] is a deterministic algorithm for
correlation sketching. COD continuously receives columns from
X∈R𝑚𝑥×𝑛andY∈R𝑚𝑦×𝑛in a streaming way and maintains two
sketches A∈R𝑚𝑥×ℓandB∈R𝑚𝑦×ℓto approximate XY𝑇with
AB𝑇. The details of COD are presented in Algorithm 1. COD has
following theoretical guarantees.
Lemma 1 ([24]).The output of co-occurring directions (Algorithm
1) gives a correlation sketch (A,B)of(X,Y)which satisfies: for a
correlation sketch of length ℓ≤min(𝑚𝑥,𝑚𝑦), we have:
XY𝑇−AB𝑇2≤2
ℓ∥X∥𝐹∥Y∥𝐹.Algorithm 1 runs in 𝑂(𝑛(𝑚𝑥+𝑚𝑦+ℓ)ℓ)time and requires a space
of𝑂((𝑚𝑥+𝑚𝑦+ℓ)ℓ).
4APPROXIMATE MATRIX MULTIPLICATION
OVER SLIDING WINDOWS
In this section, we propose two algorithms for computing approxi-
mate matrix multiplication over sliding windows.
4.1 The EH-COD Algorithm
We first introduce Exponential Histogram Co-occurring Direc-
tions (EH-COD), which leverages the ideas of Exponential His-
tograms [ 10] and incorporates the COD technique for efficiently
approximating matrix multiplication in the sliding window model.
4.1.1 Algorithm Description.
Main Idea. We show the main idea of the EH-COD algorithm in
Figure 1 and describe the detailed steps in Algorithm 2. In the EH-
COD algorithm, columns within a sliding window are segmented
into blocks with different sizes. The blocks are organized into 𝐿
levels (L[1],L[2],...,L[𝐿]in Algorithm 2) and each level con-
tains up to𝑏=𝑂(1/𝜀)blocks. Each block 𝐾stores two matrices A′
andB′as well as the start time, the end time and the size of 𝐾. We
define the size of a block 𝐾as𝐾.𝑠𝑖𝑧𝑒 =∥A′∥𝐹∥B′∥𝐹. The sketches
A′andB′are approximations of columns of the streaming data
matrices XandYfrom the start time to the end time, respectively.
The EH-COD algorithm will guarantee that the size of blocks in
level𝑖between 2𝑖−1ℓand2𝑖ℓ, ensuring a structured and efficient
data hierarchy.
Update Algorithm. The EH-COD algorithm employs a buffer 𝐵∗
to collect columns x𝑡,y𝑡from matrices XandY. Once the size
of𝐵∗exceedsℓ, EI-COD will move it to the first level, where
the blocks hold raw data columns without approximation. When
level 1fills up and contains 𝑏blocks, we combine the two oldest
blocks𝐾1and𝐾2into block𝐾, where𝐾.A′=[𝐾1.A′,𝐾2.A′]and
𝐾.B′=[𝐾1.B′,𝐾2.B′]with updated size and timeframe. If the
number of columns in 𝐾.A′and𝐾.B′does not exceeds ℓ, we directly
move it to level 2. Otherwise we compress the matrices 𝐾.A′and
𝐾.B′intoℓcolumns by the correlation shrinkage (CS) procedure
shown in Algorithm 3, which is a variant of the COD algorithm.
This process of merging and compressing also continues in
higher levels. The algorithm will remove outdated blocks in the
level𝐿whose start time is earlier than 𝑡−𝑁. To guarantee that
the block sizes in level 𝑖are between 2𝑖−1ℓand2𝑖ℓ, blocks whose
sizes are larger than 2𝑖ℓin the level𝑖are directly moved up without
merging. They are merged only when they reach level 𝑗and their
sizes are not larger than 2𝑗ℓ.
Query Algorithm. The query algorithm of EH-COD is presented
in Algorithm 4. Notice that the EH-COD algorithm maintains a
sequence of blocks with sketches approximating the X𝑊andY𝑊
matrices. We can generate the approximate matrix multiplication
ofX𝑊andY𝑊by merging the sketches from non-expiring blocks
in all levels and then performing the COD algorithm for once.
4.1.2 Approximation Error Analysis. The correlation error of the
EH-COD algorithm originates from three parts: the error from
expiring block, the aggregate errors across all block sketches and
3898KDD ’24, August 25–29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
Figure 1: The illustration of the EH-COD Algorithm
Algorithm 2 EH-COD
1:Input: X∈R𝑚𝑥×𝑛,Y∈R𝑚𝑦×𝑛,sketch sizeℓ.
2:for𝑡=1,2,...,𝑛 do
3: Remove expiring blocks in L[𝐿].
4:𝐵∗.insert(x𝑡,y𝑡).
5:𝐵∗.𝑒𝑛𝑑←𝑡.
6:if𝐵∗.size≥ℓthen
7:L[1].append(𝐵∗).
8: initialize an empty 𝐵∗with𝐵∗.start =𝐵∗.end=𝑡.
9:end if
10: for𝑖=1,2,...,𝐿 do
11: iflength(L[𝑖])≥𝑏+1then
12: Find the furthest blocks 𝐾1and𝐾2inL[𝑖].
13:𝐾←MERGE(𝐾1,𝐾2).
14:L[𝑖+1].append(𝐾).
15: Remove𝐾1and𝐾2fromL[𝑖].
16: end if
17: end for
18:end for
19:procedure MERGE(𝐾1,𝐾2)
20:𝐾.A′←[𝐾1.A′;𝐾2.A′],𝐾.B′←[𝐾1.B′;𝐾2.B′].
21:𝐾.size←∥𝐾.A′∥𝐹∥𝐾.B′∥𝐹.
22:𝐾.start←𝐾1.start,𝐾.end←𝐾2.end.
23: ifcolumns(𝐾.A′)>ℓthen
24:(𝐾.A′,𝐾.B′)← CS(𝐾.A′,𝐾.B′,ℓ).
25: end if
26: return𝐾.
27:end procedure
Algorithm 3 Correlation Shrinkage (CS)
1:Input: X∈R𝑚𝑥×𝑛,Y∈R𝑚𝑦×𝑛,sketch sizeℓ.
2:[Q𝑥,R𝑥]← QR(X),[Q𝑦,R𝑦]← QR(Y).
3:[U,Σ,V]← SVD(R𝑥R⊤𝑦).
4:𝛿←𝜎ℓ(Σ),ˆΣ←max(Σ−𝛿I𝑛,0).
5:A←Q𝑥U√︁
ˆΣ,B←Q𝑦V√︁
ˆΣ.
6:Output: AandB.
the error from merging sketches. The main idea is to bound all parts
of errors to be 𝑂(𝜀). We present our approximation error bound in
the following theorem and defer the proof to the appendix.Algorithm 4 Query for EH-COD
1:Input: sketch sequenceL, sketch size ℓ.
2:C←empty, D←empty.
3:for𝑖=1,2,...,𝐿 do
4:for𝑗=1,2,..., length(L[𝑖])do
5: C←[C;L[𝑖][𝑗].A′],D←[D;L[𝑖][𝑗].B′].
6:end for
7:end for
8:(A,B)← COD(C,D,ℓ).
9:Output: AandB.
Theorem 1. If we setℓ=𝑏=8
𝜀, the EH-COD can generate sketches
AandBof size𝑂(ℓ), with the correlation error upper bounded by
corr-err
X𝑊Y𝑇
𝑊,AB𝑇
≤𝜀.
4.1.3 Complexity Analysis.
Space Complexity. The EH-COD algorithm can guarantee that
each block only contains at most ℓcolumns. Also, the number of
blocks is at most 𝐿𝑏. Since we have∥x∥∥y∥≥1for any column pair
x,y, we know that blocks in level 1contain no more than ℓcolumns.
In addition, the CS algorithm guarantee that the blocks at higher lev-
els also contain no more than ℓcolumns. Since the number of blocks
is at most𝐿𝑏, the total columns are bounded by 𝐿𝑏ℓ. According
to Lemma 4 in appendix B, we have 𝐿≤log
2
ℓ𝑏∥X𝑊∥𝐹∥Y𝑊∥𝐹
.
Thus, the total space usage is bounded as
𝐿𝑏ℓ(𝑚𝑥+𝑚𝑦)≤log2
ℓ𝑏∥X𝑊∥𝐹∥Y𝑊∥𝐹8
𝜀ℓ(𝑚𝑥+𝑚𝑦)
=𝑂𝑚𝑥+𝑚𝑦
𝜀2log(𝜀𝑁𝑅)
,
where𝑁is the window size and 𝑅is the maximum squared column
norm of XandY.
Time complexity. In the worst case, each column will be com-
pressed for 𝐿times by the CS procedure since we have 𝐿levels.
The time to run a CS procedure is 𝑂 (𝑚𝑥+𝑚𝑦)ℓ2and a CS pro-
cedure can compress ℓnew columns. Thus the EH-COD algorithm
takes𝑂((𝑚𝑥+𝑚𝑦)ℓ)to compress a column in average. Therefore,
the amortized time cost for processing each column is bounded by
𝑂((𝑚𝑥+𝑚𝑦)ℓ𝐿)=𝑂𝑚𝑥+𝑚𝑦
𝜀log(𝜀𝑁𝑅)
.
3899Approximate Matrix Multiplication over Sliding Windows KDD ’24, August 25–29, 2024, Barcelona, Spain.
Algorithm 5 DI-COD
1:Input: X∈R𝑚𝑥×𝑛,Y∈R𝑚𝑦×𝑛, sketch size ℓand the size of
window𝑁.
2:𝑠𝑖𝑧𝑒 X←0,𝑠𝑖𝑧𝑒 Y←0.
3:for𝑡=1,2,...,𝑛 do
4:for𝑖=1,2,...,𝐿 do
5: Remove expiring blocks in L[𝑖].
6:𝐵∗
𝑖.insert(x𝑡,y𝑡).
7:𝐵∗
𝑖.𝑒𝑛𝑑←𝑡.
8: ifcolumns(𝐵∗
𝑖.A′)=2ℓ𝑖then
9: 𝐵∗
𝑖.A′,𝐵∗
𝑖.B′=CS(𝐵∗
𝑖.A′,𝐵∗
𝑖.B′,ℓ𝑖).
10: end if
11: end for
12:𝑠𝑖𝑧𝑒 X←𝑠𝑖𝑧𝑒 X+∥x𝑡∥2,𝑠𝑖𝑧𝑒 Y←𝑠𝑖𝑧𝑒 Y+∥y𝑡∥2.
13: if𝑠𝑖𝑧𝑒 X≥𝑁𝑅X/2𝐿or𝑠𝑖𝑧𝑒 Y≥𝑁𝑅Y/2𝐿then
14:𝑣←binary trailing zeros (L[1].𝑙𝑒𝑛𝑔𝑡ℎ)+1.
15: for𝑖=1,2,...,𝑣 do
16: Append𝐵∗
𝑖toL[𝑖].
17: Initialize empty 𝐵∗
𝑖with𝐵∗
𝑖.𝑠𝑡𝑎𝑟𝑡 =𝐵∗
𝑖.𝑒𝑛𝑑=𝑡.
18: end for
19: end if
20:end for
4.2 The DI-COD Algorithm
The second proposed algorithm for sliding window AMM is the
Dyadic Interval Co-occurring Directions (DI-COD) algorithm, which
utilizes a tree structure to store data within the window. The DI-
COD algorithm requires less space usage than the EH-COD algo-
rithm when the column norm of the data matrices is small.
4.2.1 Algorithm Description.
Main Idea. We show the main idea of the DI-COD algorithm in
Figure 2 and describe the detailed steps in Algorithm 5. The DI-COD
algorithm uses a hierarchical structure with 𝐿levels, each of which
contains a dynamic number of blocks. Each level contains three
types of blocks: expiring, inactive and buffer. Each block contains
two matrices A′andB′. For inactive blocks in level 𝑖, we let both
A′andB′containℓ𝑖columns. This ensures that the approximation
error of level 𝑖satisfies𝜀𝑖=1
ℓ𝑖=1
2𝑖𝐿. We define the size of a block
𝐾as𝐾.𝑠𝑖𝑧𝑒 =∥A′∥𝐹∥B′∥𝐹. Notice that for DI-COD, the size of
blocks in the same level are the same, thus we let 𝑠𝑖𝑧𝑒𝑖be the size
of each blocks in level 𝑖. For each level 𝑖, we maintain a buffer 𝐵∗
𝑖to
receive XandY. When the number of columns of 𝐵∗
𝑖reaches 2ℓ𝑖, we
perform the CS procedure to compress the matrices in 𝐵∗
𝑖, so that
𝐵∗
𝑖can continue to receive new columns. When 𝐵∗.𝑠𝑖𝑧𝑒 satisfies
the condition in line 13 (here 𝑅Xand𝑅Yare the maximum column
norm of XandY, respectively), we append it to L[𝑖]and regard it
as an inactive block.
Query Algorithm. Algorithm 6 presents the query algorithm
for DI-COD. As DI-COD uses the tree structure, we need to select
appropriate blocks from top to bottom in each level and concatenate
them to form the final correlation sketch pair (A,B). We set two
timestamps, 𝑙=𝑡−𝑁and𝑟=𝑡, to select suitable blocks in each
level. The algorithm first identifies the highest level 𝑣containing
inactive blocks and proceed downwards for selection. Within eachAlgorithm 6 Query for DI-COD on window (𝑡−𝑁,𝑡)
1:Input: sketch sequenceLand𝑡.
2:A←𝑒𝑚𝑝𝑡𝑦, B←𝑒𝑚𝑝𝑡𝑦 .
3:𝑙←𝑡−𝑁,𝑟←𝑡.
4:Find the highest level 𝑣with at least 1inactive block.
5:for𝑖=𝑣,𝑣−1,..., 1do
6: Find the leftmost block L[𝑖][𝑗].
7:ifL[𝑖][𝑗].𝑒𝑛𝑑≤𝑟then
8: A←[L[𝑖][𝑗].A′;A],B←[L[𝑖][𝑗].B′;B].
9:𝑟←L[𝑖][𝑗].𝑠𝑡𝑎𝑟𝑡 .
10: end if
11: Find the rightmost block L[𝑖][𝑘].
12: if𝑘=𝑗then
13:𝑙←L[𝑖][𝑘].𝑒𝑛𝑑.
14: else ifL[𝑖][𝑘].𝑠𝑡𝑎𝑟𝑡≥𝑙then
15: A←[A;L[𝑖][𝑗].A′],B←[B;L[𝑖][𝑗].B′].
16:𝑙←L[𝑖][𝑘].𝑒𝑛𝑑.
17: end if
18:end for
19:Output: AandB.
level, we locates the furthest block L[𝑖][𝑗]. If the𝑒𝑛𝑑timestamp
of this block is less than 𝑟, it indicates that this block needs to be
combined on the left, and we update 𝑟to its𝑠𝑡𝑎𝑟𝑡 timestamp. Then
we find the most recent block L[𝑖][𝑘]in the same level. If it is the
same as the leftmost block, we shift 𝑙to the right and update it to
its𝑒𝑛𝑑timestamp. IfL[𝑖][𝑘].𝑠𝑡𝑎𝑟𝑡 is greater than or equal to 𝑙, it
signifies that this block needs to be combined on the right, and we
update𝑙toL[𝑖][𝑘].𝑒𝑛𝑑. We repeat this process until we reach the
first level. The blocks selected are highlighted in bold in figure 2.
4.2.2 Approximation Error Analysis. The approximation error of
the DI-COD algorithm can be divided into two parts: the error from
expiring block and the aggregate errors across all block sketches.
The main idea is to control both parts of errors to be 𝑂(𝜀). We
present our approximation error bound in the following theorem
and defer the proof to the appendix.
Theorem 2. If we set𝐿=log𝑅
𝜀andℓ𝑖=2𝑖𝐿, the DI-COD can
generate sketches AandBof size𝑂
𝑅
𝜀log𝑅
𝜀
, with the correlation
error upper bounded by
corr-err
X𝑊Y𝑇
𝑊,AB𝑇
≤𝜀.
4.2.3 Complexity Analysis.
Space Complexity. Since the window size satisfies 𝑠𝑖𝑧𝑒𝑊≤𝑁𝑅
and𝑠𝑖𝑧𝑒𝑖≥𝑁𝑅
2𝐿−𝑖+1, the𝑖-th level can have at most 2𝐿−𝑖+1blocks.
Thus the space complexity of DI-COD is
2𝐿∑︁
𝑖=1ℓ𝑖2𝐿−𝑖+1=𝐿∑︁
𝑖=12𝑖𝐿2𝐿−𝑖+2=4𝑅
𝜀log2𝑅
𝜀=𝑂𝑅
𝜀log2𝑅
𝜀
.
Time complexity. The amortized time for each column during
the CS process is 𝑂((𝑚𝑥+𝑚𝑦)ℓ). In the worst case, CS operations
are performed concurrently across all 𝐿levels. Thus, the time com-
plexity amounts to 𝑂((𝑚𝑥+𝑚𝑦)ℓ𝐿)=𝑂𝑚𝑥+𝑚𝑦
𝜀log𝑅
𝜀
.
3900KDD ’24, August 25–29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
Figure 2: The illustration of the DI-COD Algorithm
Algorithm 7 EH-SCOD
1:Input:X∈R𝑚𝑥×𝑛,Y∈R𝑚𝑦×𝑛,sketch sizeℓ,ratio𝜅.
2:for𝑡=1,2,...,𝑛 do
3: Remove expiring blocks in L[𝐿].
4:𝐵∗.insert(x𝑡,y𝑡).
5:if𝐵∗.size≥𝜀𝑁then
6:𝐵∗←SPM(𝐵∗,ℓ,𝑞).
7: initialize an empty 𝐵∗with𝐵∗.start =𝐵∗.end=𝑡.
8:end if
9:for𝑖=1,2,...,𝐿 do
10: iflength(L[𝑖])≥𝑏+1then
11: find the furthest blocks 𝐾1and𝐾2inL[𝑖].
12:𝐾←merge(𝐾1,𝐾2).
13:L[𝑖+1].append(𝐾).
14: Remove𝐾1and𝐾2fromL[𝑖].
15: end if
16: end for
17:end for
5 IMPROVED ALGORITHMS FOR SPARSE
MATRICES
Many data matrices in real-world applications are sparse. In this sec-
tion we propose variants of EH-COD and DI-COD which enhance
the performance over sparse matrices.
5.1 The EH-SCOD Algorithm
We describe details of Exponential Histogram Sparse Co-occurring
Directions (EH-SCOD) in algorithm 7, which is an improved version
of the EH-COD algorithm. The EH-SCOD algorithm enlarge the
capacity of buffer 𝐵∗to𝑂(𝜀𝑁). When the buffer size exceeds 𝜀𝑁,
EH-SCOD uses the subspace power method (SPM) algorithm [ 25]
(outlined in Algorithm 8) to compress the matrices within the buffer
down toℓcolumns, so as not to exceed the space budget. The com-
pressed block is then placed in level 1, where subsequent merging
and compression operations remain consistent with EH-COD. The
query method of EH-SCOD is the same as that of EH-COD, pro-
ducing a final sketch of size ℓ. We have the following theoretical
results for the proposed EH-SCOD algorithm.Algorithm 8 Subspace Power Method (SPM)
1:Input: block𝐾, target rank ℓand integer 𝑞.
2:M←(𝐾.A′)(𝐾.B′)𝑇∈R𝑚𝑥×𝑚𝑦.
3:Generate G=[𝐺𝑖𝑗]∈R𝑚𝑦×ℓ, where𝐺𝑖𝑗∼N( 0,1).
4:F←(MM𝑇)𝑞MG∈R𝑚𝑥×ℓ.
5:Z←orthonormal column basis of F.
6:[U,Σ,V]← SVD(Z𝑇(𝐾.A′)𝑇(𝐾.B′)).
7:𝐾.A′←ZU√
Σ,𝐾.B′←V√
Σ.
8:Output: block𝐾.
Theorem 3. If we choose 𝑏=ℓ=8
𝜀and𝑞=Θ(log(ℓ/𝛿))for EH-
SCOD and SPM algorithms, then with probability 1−𝛿, EH-SCOD
can generate sketches AandBof size𝑂(ℓ), with the correlation error
upper bounded by
corr-err
X𝑊Y𝑇
𝑊,AB𝑇
≤𝜀,
In addition, the EH-SCOD algorithm only requires 𝑂𝑚𝑥+𝑚𝑦
𝜀2log𝑅
space cost and 𝑂𝑚𝑥+𝑚𝑦
𝜀log𝑅
time cost in each round.
Remark 1. Since𝑁≫ℓ=𝑂(1
𝜀), we have log𝑅<log𝜀𝑁𝑅 , which
means EH-SCOD is more efficient than EH-COD.
5.2 The DI-SCOD Algorithm
We describe details of the proposed DI-SCOD algorithm in Algo-
rithm 9, an enhanced version of the DI-COD algorithm. In DI-SCOD,
data within each buffer 𝐵∗
𝑖at every level is stored sparsely without
undergoing any CS procedure until it reaches its designated size.
Once the buffer hits this size threshold, the SPM is applied to effec-
tively reduce the number of columns in the buffer. Subsequently,
the CS procedure is applied just once per buffer, thereby reducing
the frequency of CS operations required across inactive blocks. The
query steps remain unchanged. We have the following theoretical
results for the proposed DI-SCOD algorithm.
Theorem 4. If we set𝐿=log𝑅
𝜀,ℓ𝑖=2𝑖𝐿and𝑞=Θ(log(ℓ/𝛿))
for DI-SCOD and SPM algorithms, then with probability 1−𝛿, DI-
SCOD can generate sketches AandBof size𝑂
𝑅
𝜀log𝑅
𝜀
, with the
3901Approximate Matrix Multiplication over Sliding Windows KDD ’24, August 25–29, 2024, Barcelona, Spain.
Dataset 𝑛 𝑚𝑥𝑚𝑦 density(X) density(Y) 𝑅X𝑅Y
DENSE 1×1042000 1000 1.0 1.0 370 719
APR 2.32×1042.80×1044.28×1046.31×10−44.53×10−4773 618
PAN11 8.90×1045.12×1049.96×1044.38×10−42.43×10−45655 6188
EURO0 4.76×1057.25×1048.77×1043.46×10−43.47×10−4103370 112506
Table 1: The statistics of datasets, where density(X) = nnz (X)/(𝑛𝑚𝑥)and density(Y) = nnz (Y)/𝑛𝑚𝑦.
Algorithm 9 DI-SCOD
1:Input: X∈R𝑚𝑥×𝑛,Y∈R𝑚𝑦×𝑛, sketch size ℓ.
2:𝑠𝑖𝑧𝑒 X←0,𝑠𝑖𝑧𝑒 Y←0.
3:for𝑡=1,2,...,𝑛 do
4:for𝑖=1,2,...,𝐿 do
5: Remove expiring blocks in L[𝐿].
6:𝐵∗
𝑖.insert(x𝑡,y𝑡).
7:𝐵∗
𝑖.𝑒𝑛𝑑←𝑡.
8:end for
9:𝑠𝑖𝑧𝑒 X←𝑠𝑖𝑧𝑒 X+∥x𝑡∥2,𝑠𝑖𝑧𝑒 Y←𝑠𝑖𝑧𝑒 Y+∥y𝑡∥2.
10: if𝑠𝑖𝑧𝑒 X≥𝑁𝑅X/2𝐿or𝑠𝑖𝑧𝑒 Y≥𝑁𝑅Y/2𝐿then
11:𝑣←binary trailing zeros (L[1].𝑙𝑒𝑛𝑔𝑡ℎ)+1.
12: for𝑖=1,2,...,𝑣 do
13:𝐵∗
𝑖=SPM(𝐵∗
𝑖,2ℓ𝑖,𝑞).
14: 𝐵∗
𝑖.A′,𝐵∗
𝑖.B′=CS(𝐵∗
𝑖.A′,𝐵∗
𝑖.B′,ℓ𝑖).
15: Append𝐵∗
𝑖toL[𝑖].
16: Initialize empty 𝐵∗
𝑖with𝐵∗
𝑖.𝑠𝑡𝑎𝑟𝑡 =𝐵∗
𝑖.𝑒𝑛𝑑=𝑡.
17: end for
18: end if
19:end for
correlation error upper bounded by
corr-err
X𝑊Y𝑇
𝑊,AB𝑇
≤𝜀.
In addition, in each round DI-SCOD requires 𝑂
𝑅
𝜀log2𝑅
𝜀
space cost
and𝑂𝑛𝑛𝑧(X𝑊)+𝑛𝑛𝑧(Y𝑊)
𝑁𝜀+𝑚𝑥+𝑚𝑦
𝑁𝜀2
log𝑅
𝜀
time cost.
Remark 2. We have𝑁≫1
𝜀since the size of final sketches should
be much less than the window size. Thus we have𝑚𝑥+𝑚𝑦
𝑁𝜀2≪𝑚𝑥+𝑚𝑦
𝜀.
In addition, for the sparse matrices X𝑊andY𝑊, we have𝑛𝑛𝑧(X𝑊)+
𝑛𝑛𝑧(Y𝑊)≪𝑁(𝑚𝑥+𝑚𝑦). Thus DI-SCOD requires less computational
cost than DI-COD.
6 EXPRIMENTS
In this section, we empirically study performance of proposed al-
gorithms1. We compare proposed algorithms with a baseline al-
gorithm: priority sampling, which randomly select each column
pair(x𝑡,y𝑡)according to its weight 𝜌=𝑢1/(∥ x∥∥y∥)
𝑡. Here𝑢𝑡is
uniformly sampled from (0,1).
Datasets. We employ one dense synthetic dataset named DENSE
along with three sparse cross-language datasets: APR, PAN11, and
EURO0 [ 17,28–30] for evaluating our methods. The statistics of
the datasets are provided in Table 1.
1The code is available at: https://github.com/lilianzhi0/AMMSWThe DENSE dataset consists of two synthetic random matrices
where each element is uniformly sampled from [0,1]. The matrices
XandYhave 2,000and1,000rows, respectively. Both matrices
have 10,000columns.
The Amazon Product Reviews (APR) dataset is a publicly avail-
able collection that includes product reviews and related informa-
tion from the Amazon website. The dataset contains millions of
sentences in English and French. We construct it into a review ma-
trix where the Xmatrix has 28,017rows and Yhas42,833rows,
with both matrices sharing 23,235columns.
PANPC-11 (PAN11) is a dataset for text analysis, focusing on
tasks such as plagiarism detection, author identification, and near-
duplicate detection. Its texts are in English and French. The two
data matrices XandYcontain 51,219and99,595rows, respectively.
Both of them have 88,977columns.
The Europarl (EURO) dataset is a widely used multilingual paral-
lel corpus, composed of the proceedings of the European Parliament.
We only utilize parts of its English and French texts to form EURO0.
The matrix XandYcontains 72,470and87,686rows, respectively.
They both have 475,834columns.
Setup. We set the window size to be 4,000for the DENSE dataset
and10,000for the larger sparse datasets APR, PAN11, and EURO0.
We evaluate all algorithms according to their correlation errors
corr-err(X𝑊Y𝑇
𝑊,AB𝑇), where X𝑊andY𝑊are the original ma-
trices in the window, and AandBare the generated correlation
sketches. We compare both average and maximum errors through
all sampled windows. All algorithms are implemented in MATLAB
(R2021a). The experiments are executed on a Windows server with
32GB memory and a single core of Intel i9-13900K.
Performance. For the time comparison, we present error vs. final
sketch size and error vs. total space usage in Figure 3 and Figure
4, respectively. The final sketch size is the number of columns
of the sketches generated by the algorithms for query, and the
total space usage is the maximum number of columns required by
the algorithms. In these figures, we find that EH-SCOD and DI-
SCOD require much less space than EH-COD and DI-COD when
the data matrices are sparse. In addition, Figure 4 shows that DI-
SCOD outperforms EH-SCOD when the maximum column norm
𝑅Xand𝑅Yare large, and vice versa. This phenomenon matches
our theoretical results that the space cost of DI-SCOD is linear to
the maximum column norm while the space cost of EH-SCOD is
logarithmic to the maximum column norm. Figure 3 compares the
final sketch size of proposed algorithms. We can find that EH-COD
and EH-SCOD outperforms DI-COD and DI-SCOD on all datasets.
For the time comparison, we present time vs. final sketch size
in Figure 5, which shows that EH-SCOD and DI-SCOD are signifi-
cantly faster than EH-COD and DI-COD on the sparse datasets. We
3902KDD ’24, August 25–29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
0 100 200 300 400 500 600
Final Sketch Size00.050.10.150.2Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(a) DENSE
0 100 200 300 400
Final Sketch Size00.10.20.30.40.50.6Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (b) APR
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.3Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (c) PAN11
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.3Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (d) EURO0
0 100 200 300 400
Final Sketch Size00.050.10.150.20.25Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(e) DENSE
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.30.35Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (f) APR
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.30.35Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (g) PAN11
0 100 200 300 400
Final Sketch Size00.050.10.150.20.250.3Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (h) EURO0
Figure 3: error vs. final sketch size
0 500 1000 1500 2000 2500
Total Space Usage (columns)00.050.10.150.2Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(a) DENSE
0 500 1000 1500 2000 2500 3000
Total Space Usage (columns)00.050.10.150.20.25Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (b) APR
0 500 1000 1500 2000
Total Space Usage (columns)00.050.10.150.20.250.30.350.4Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (c) PAN11
0 500 1000 1500 2000 2500
Total Space Usage (columns)00.050.10.150.20.250.3Average ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (d) EURO0
0 500 1000 1500 2000 2500
Total Space Usage (columns)00.050.10.150.20.25Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(e) DENSE
0 500 1000 1500 2000 2500
Total Space Usage (columns)00.050.10.150.20.25Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (f) APR
0 500 1000 1500 2000 2500 3000
Total Space Usage (columns)00.050.10.150.20.250.30.35Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (g) PAN11
0 500 1000 1500 2000 2500 3000
Total Space Usage (columns)00.050.10.150.20.250.3Max ErrorSampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (h) EURO0
Figure 4: error vs. total space usage
0 100 200 300 400
Final Sketch Size00.511.522.533.54Time Cost Per Column (ms)Sampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD
(a) DENSE
0 100 200 300 400 500 600
Final Sketch Size010203040506070Time Cost Per Column (ms)Sampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (b) APR
0 100 200 300 400 500 600
Final Sketch Size050100150200250300Time Cost Per Column (ms)Sampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (c) PAN11
0 100 200 300 400
Final Sketch Size020406080100120140160Time Cost Per Column (ms)Sampling
DI-COD
DI-SCOD
EH-COD
EH-SCOD (d) EURO0
Figure 5: time cost per column vs. final sketch size
3903Approximate Matrix Multiplication over Sliding Windows KDD ’24, August 25–29, 2024, Barcelona, Spain.
surprisingly find that EH-SCOD is faster than EH-COD on dense
data while DI-SCOD is faster than DI-COD. We think this is because
EH-SCOD increases the buffer capacity and enables each block to
store more data than EH-COD. Thus EH-SCOD performs fewer CS
procedures than EH-COD and thus requires less amortized running
time. On the other hand, DI-SCOD needs to perform more SPM pro-
cedures than EI-SCOD. Since SPM is expensive on dense matrices,
DI-SCOD is not as efficient as EH-SCOD on dense datasets, and is
even slower than DI-COD.
7 CONCLUSION
In this paper, we introduce two novel algorithms, called EH-COD
and DI-COD, for approximate matrix multiplication over sliding
windows. We also provide their error analysis as well as the com-
putational complexities. In addition, we propose sparse variants of
EH-COD and DI-COD which enjoy better performance when the
data matrices are sparse. The experimental results show that the
proposed algorithms have better performance than baseline. In the
future, we will explore the lower bound of the space complexity
of the AMM problem in the sliding window model and study the
optimality of the proposed algorithms.
ACKNOWLEDGMENTS
Cheng Chen is supported by National Natural Science Foundation
of China (62306116). Mingsong Chen is supported by National
Natural Science Foundation of China (62272170) and “Digital Silk
Road” Shanghai International Joint Lab of Trustworthy Intelligent
Software (22510750100).
REFERENCES
[1]Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril
Zhang, and Yi Zhang. 2019. Efficient full-matrix adaptive regularization. In
International Conference on Machine Learning. PMLR, 102–110.
[2]Arvind Arasu and Gurmeet Singh Manku. 2004. Approximate counts and quan-
tiles over sliding windows. In Proceedings of the twenty-third ACM SIGMOD-
SIGACT-SIGART symposium on Principles of database systems. 286–296.
[3]Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev Motwani, and Jennifer
Widom. 2002. Models and issues in data stream systems. In Proceedings of the
twenty-first ACM SIGMOD-SIGACT-SIGART symposium on Principles of database
systems. 1–16.
[4]Roland Badeau, Gaël Richard, and Bertrand David. 2004. Sliding window adaptive
SVD algorithms. IEEE Transactions on Signal Processing 52, 1 (2004), 1–10.
[5]Davis Blalock and John Guttag. 2021. Multiplying matrices without multiplying.
InInternational Conference on Machine Learning. PMLR, 992–1004.
[6]Michele Borassi, Alessandro Epasto, Silvio Lattanzi, Sergei Vassilvitskii, and
Morteza Zadimoghaddam. 2020. Sliding window algorithms for k-clustering
problems. Advances in Neural Information Processing Systems 33 (2020).
[7]Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj
Upadhyay, David P Woodruff, and Samson Zhou. 2020. Near optimal linear
algebra in the online and sliding window models. In 2020 IEEE 61st Annual
Symposium on Foundations of Computer Science (FOCS). IEEE, 517–528.
[8]Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina
Persu. 2015. Dimensionality reduction for k-means clustering and low rank
approximation. In Proceedings of the forty-seventh annual ACM symposium on
Theory of computing. 163–172.
[9]Michael B Cohen, Jelani Nelson, and David P Woodruff. 2015. Optimal approx-
imate matrix product in terms of stable rank. arXiv preprint arXiv:1507.02268
(2015).
[10] Mayur Datar, Aristides Gionis, Piotr Indyk, and Rajeev Motwani. 2002. Main-
taining stream statistics over sliding windows. SIAM journal on computing 31, 6
(2002), 1794–1813.
[11] Mayur Datar and Rajeev Motwani. 2016. The sliding-window computation model
and results. In Data Stream Management: Processing High-Speed Data Streams.
Springer, 149–165.[12] Inderjit S Dhillon. 2001. Co-clustering documents and words using bipartite
spectral graph partitioning. In Proceedings of the 7th ACM SIGKDD international
conference on Knowledge discovery and data mining. 269–274.
[13] Petros Drineas, Ravi Kannan, and Michael W Mahoney. 2006. Fast Monte Carlo
algorithms for matrices I: Approximating matrix multiplication. SIAM J. Comput.
36, 1 (2006), 132–157.
[14] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods
for online learning and stochastic optimization. The Journal of Machine Learning
Research 12, 7 (2011).
[15] Mina Ghashami, Edo Liberty, Jeff M Phillips, and David P Woodruff. 2016. Fre-
quent directions: Simple and deterministic matrix sketching. SIAM J. Comput. 45,
5 (2016), 1762–1792.
[16] Harold Hotelling. 1936. Relations between two sets of variates. Biometrika (1936).
[17] Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine transla-
tion. In Proceedings of machine translation summit x: papers. 79–86.
[18] Edo Liberty. 2013. Simple and deterministic matrix sketching. In Proceedings of
the 19th ACM SIGKDD international conference on Knowledge discovery and data
mining. 581–588.
[19] Zhang Longbo, Li Zhanhuai, Zhao Yiqiang, Yu Min, and Zhang Yang. 2007.
A priority random sampling algorithm for time-based sliding windows over
weighted streaming data. In Proceedings of the 2007 ACM symposium on Applied
computing. 453–456.
[20] Luo Luo, Cheng Chen, Guangzeng Xie, and Haishan Ye. 2021. Revisiting Co-
Occurring Directions: Sharper Analysis and Efficient Algorithm for Sparse Ma-
trices. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35.
8793–8800.
[21] Luo Luo, Cheng Chen, Zhihua Zhang, Wu-Jun Li, and Tong Zhang. 2019. Robust
frequent directions with application in online learning. The Journal of Machine
Learning Research 20, 1 (2019).
[22] Avner Magen and Anastasios Zouzias. 2011. Low rank matrix-valued Chernoff
bounds and approximate matrix multiplication. In Proceedings of the twenty-
second annual ACM-SIAM symposium on Discrete Algorithms. SIAM, 1422–1436.
[23] Gurmeet Singh Manku and Rajeev Motwani. 2002. Approximate frequency counts
over data streams. In VLDB’02: Proceedings of the 28th International Conference
on Very Large Databases. Elsevier, 346–357.
[24] Youssef Mroueh, Etienne Marcheret, and Vaibahava Goel. 2017. Co-occurring
directions sketching for approximate matrix multiply. In Artificial Intelligence
and Statistics. PMLR, 567–575.
[25] Cameron Musco and Christopher Musco. 2015. Randomized block krylov methods
for stronger and faster approximate singular value decomposition. Advances in
Neural Information Processing Systems 28 (2015).
[26] Imran Naseem, Roberto Togneri, and Mohammed Bennamoun. 2010. Linear
regression for face recognition. IEEE transactions on pattern analysis and machine
intelligence (TPAMI) 32, 11 (2010), 2106–2112.
[27] Odysseas Papapetrou, Minos Garofalakis, and Antonios Deligiannakis. 2015.
Sketching distributed sliding-window data streams. The VLDB Journal 24 (2015),
345–368.
[28] Martin Potthast, Alberto Barrón-Cedeno, Benno Stein, and Paolo Rosso. 2011.
Cross-language plagiarism detection. Language Resources and Evaluation 45
(2011), 45–62.
[29] Martin Potthast, Benno Stein, Alberto Barrón-Cedeño, and Paolo Rosso. 2010. An
evaluation framework for plagiarism detection. In Coling 2010: Posters. 997–1005.
[30] Peter Prettenhofer and Benno Stein. 2010. Cross-language text classification using
structural correspondence learning. In Proceedings of the 48th annual meeting of
the association for computational linguistics. 1118–1127.
[31] Tamas Sarlos. 2006. Improved approximation algorithms for large matrices via
random projections. In 2006 IEEE 47th Annual Symposium on Foundations of
Computer Science (FOCS). IEEE, 143–152.
[32] Yuanyu Wan and Lijun Zhang. 2021. Approximate Multiplication of Sparse
Matrices with Limited Space. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 35. 10058–10066.
[33] Yuanyu Wan and Lijun Zhang. 2022. Efficient Adaptive Online Learning via Fre-
quent Directions. IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI) 44, 10 (2022), 6910–6923.
[34] Zhewei Wei, Xuancheng Liu, Feifei Li, Shuo Shang, Xiaoyong Du, and Ji-Rong
Wen. 2016. Matrix sketching over sliding windows. In Proceedings of the 2016
International Conference on Management of Data. 1465–1480.
[35] David P Woodruff et al .2014. Sketching as a tool for numerical linear algebra.
Foundations and Trends® in Theoretical Computer Science 10, 1–2 (2014), 1–157.
[36] Qiaomin Ye, Luo Luo, and Zhihua Zhang. 2016. Frequent direction algorithms
for approximate matrix multiplication with applications in CCA. Computational
Complexity 1, m3 (2016), 2.
[37] Haida Zhang, Zengfeng Huang, Zhewei Wei, Wenjie Zhang, and Xuemin Lin.
2017. Tracking matrix approximation over distributed sliding windows. In 2017
IEEE 33rd International Conference on Data Engineering (ICDE). IEEE, 833–844.
3904KDD ’24, August 25–29, 2024, Barcelona, Spain. Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei & Cheng Chen
A AUXILIARY LEMMAS FOR THE COD
ALGORITHM
We present some properties of the COD algorithm, which are used
in our proofs.
Lemma 2. Co-occurring directions algorithm is parallelizable. Let
X=[X1;X2]∈R𝑚𝑥×(𝑛1+𝑛2), and Y=[Y1;Y2]∈R𝑚𝑦×(𝑛1+𝑛2).
(A1,B1)is the correlation sketch of (X1,Y1), and(A2,B2)is the
correlation sketch of (X2,Y2). Then, the correlation sketch (C,D)of
([A1;A2],[B1;B2])is a correlation sketch of (X,Y), and satisfies
XY𝑇−CD𝑇≤2
ℓ∥X∥𝐹∥Y∥𝐹.
Lemma 3. Given matrices X∈R𝑚𝑥×𝑛andY∈R𝑚𝑦×𝑛, suppose
we decompose them into 𝑘sub-matrices X=[X1;...;X𝑘]andY=
[Y1;...;Y𝑘], in which X𝑗andY𝑗both have𝑛𝑗columns. Let(A𝑖,B𝑖)=
𝐶𝑂𝐷(X𝑖,Y𝑖,ℓ𝑖), such that∥X𝑖Y𝑇
𝑖−A𝑖B𝑇
𝑖∥≤𝜀𝑖∥X𝑖∥𝐹∥Y𝑖∥𝐹. Then
A=[A1;...;A𝑘]andB=[B1;...;B𝑘]is an approximation for Xand
Ywith error bound
𝑋𝑌𝑇−𝐴𝐵𝑇2≤𝑘∑︁
𝑖=1𝜀𝑖∥X𝑖∥𝐹∥Y𝑖∥𝐹.
B THE PROOF OF THEOREM 1
We first bound the number of levels during the algorithm’s opera-
tion in the following lemma.
Lemma 4. In EH-COD, the number of levels can be upper bounded
by
𝐿≤log2
ℓ𝑏∥X𝑊∥𝐹∥Y𝑊∥𝐹
.
Proof. Consider the level L-1, which is already filled with 𝑏
blocks. All these blocks cover columns that are subsets of the entire
window, and the combined size of these blocks certainly does not ex-
ceed∥X𝑊∥𝐹∥Y𝑊∥𝐹. Therefore, we have 𝑏2𝐿−1ℓ≤∥X𝑊∥𝐹∥Y𝑊∥𝐹.
Taking the logarithm of this inequality yields an upper bound for
𝐿. □
We partition the matrices X𝑊andY𝑊into two submatrices,
denoted as X𝑊=[X0,˜X]andY𝑊=[Y0,˜Y]. Here, X0andY0
represent the submatrices formed by columns covered by expiring
blocks in the last level, while ˜Xand ˜Yare the rest parts that have
been effectively approximated. The final correlation sketches are A
andB. Assuming we approximate X0andY0with zero matrices,
we can derive the covariance errorX𝑊Y𝑇
𝑊−AB𝑇2=
X0Y𝑇
0+˜X˜Y𝑇
−
0+AB𝑇2
≤X0Y𝑇
02+˜X˜Y𝑇−AB𝑇2,
where∥X0Y𝑇
0∥≤∥ X0∥𝐹∥Y0∥𝐹, this denotes the size of the expiring
block. The maximum size of the expiring block at level L is 2𝐿ℓ. FromLemma 3, we can deduce the error associated with the expiring
blockX0Y𝑇
02≤2𝐿ℓ≤2
𝑏∥X𝑊∥𝐹∥Y𝑊∥𝐹=𝜀
4∥X𝑊∥𝐹∥Y𝑊∥𝐹.
Then, we consider the approximation error in the part˜X˜Y𝑇−AB𝑇2.
Recalling the process of obtaining AandB, we approximated ˜X
and ˜Yin blocks, maintaining a sequence of correlation sketches
(A1,B1),(A2,B2),...,(A𝑘,B𝑘). During the approximation of matrix
multiplication within the query window, we merged all the sketches
and performed a final COD, resulting in
𝐴,𝐵=𝐶𝑂𝐷([A1,...,A𝑘],[B1,...,B𝑘],ℓ)=𝐶𝑂𝐷(C,D,ℓ).
Precisely calculating the error in this process is complex, but we can
roughly view the approximation error as consisting of two parts:
the sum of covariance errors from the individual submatrices and
the error introduced by the COD after merging. We now explain
why the error can be computed in this manner:˜X˜Y𝑇−AB𝑇2=˜X˜Y𝑇−AB𝑇+CD𝑇−CD𝑇2
≤˜X˜Y𝑇−CD𝑇2+CD𝑇−AB𝑇2.
Here, the first part can be further expanded and calculated in detail˜X˜Y𝑇−CD𝑇2=
˜X1˜Y𝑇
1+...+˜X𝑘˜Y𝑇
𝑘
−
A1B𝑇
1+...+A𝑘B𝑇
𝑘2
≤𝑘∑︁
𝑗=1˜X𝑗˜Y𝑇
𝑗−A𝑗B𝑇
𝑗2.
We assume that˜X𝑖˜Y𝑇
𝑖−A𝑖B𝑇
𝑖
2represents the covariance error
of a block located at level i. Due to the parallelizability, the error
for each block adheres to the upper bound introduced by COD, that
is,˜X𝑖˜Y𝑇
𝑖−A𝑖B𝑇
𝑖
2≤𝜀
8∥˜X𝑖∥𝐹∥˜Y𝑖∥𝐹≤𝜀
82𝑖ℓ. Therefore, the first
part of the approximation error can be bounded by
𝐿∑︁
𝑖=1𝑏˜X𝑖˜Y𝑇
𝑖−A𝑖B𝑇
𝑖2≤𝐿∑︁
𝑖=1𝑏𝜀
82𝑖ℓ
=𝜀
4𝑏ℓ(2𝐿−1)
≤𝜀
2∥X𝑊∥𝐹∥Y𝑊∥𝐹−𝜀
4𝑏ℓ
≤𝜀
2∥X𝑊∥𝐹∥Y𝑊∥𝐹.
We can easily derive the second part of the approximation errorCD𝑇−AB𝑇2≤𝜀
8∥C∥𝐹∥D∥𝐹≤𝜀
8∥X𝑊∥𝐹∥Y𝑊∥𝐹.
Consequently, there is an established upper bound for the corre-
lation error between the matrix within the window and the final
correlation sketch. Then we prove Theorem 1 as follows:
X𝑊Y𝑇
𝑊−AB𝑇2≤X0Y𝑇
02+˜𝑋˜Y𝑇−CD𝑇2+CD𝑇−AB𝑇2
≤𝜀
4+𝜀
2+𝜀
8
∥X𝑊∥𝐹∥Y𝑊∥𝐹
≤𝜀∥X𝑊∥𝐹∥Y𝑊∥𝐹.
3905Approximate Matrix Multiplication over Sliding Windows KDD ’24, August 25–29, 2024, Barcelona, Spain.
C PROOF OF THEOREM 2
We divide the entire window 𝑁into two parts: expired blocks X0,Y0
and non-expiring blocks ˜X,˜Y, then X𝑊=
X0;˜X
,Y𝑊=
Y0;˜Y
and ˜X=˜X1;...;˜X𝑙𝑎𝑠𝑡
,˜Y=˜Y1;...;˜Y𝑙𝑎𝑠𝑡. Let AandBrepre-
sent sketches of ˜Xand˜Y, respectively. Then, A=[A1;...;A𝑙𝑎𝑠𝑡],B=
[B1;...;B𝑙𝑎𝑠𝑡].
From lemma 2, we can get
˜X˜Y𝑇−AB𝑇≤𝑙𝑎𝑠𝑡∑︁
𝑖=1˜X𝑖˜Y𝑇
𝑖−A𝑖B𝑇
𝑖≤𝑙𝑎𝑠𝑡∑︁
𝑖=1𝜀𝑖˜X𝑖𝐹˜Y𝑖𝐹.
The approximation error can be proved as follows:X𝑊Y𝑇
𝑊−AB𝑇=X0Y𝑇
0+˜X˜Y𝑇−AB𝑇
≤X0Y𝑇
0+˜X˜Y𝑇−AB𝑇
≤X0Y𝑇
0+𝑙𝑎𝑠𝑡∑︁
𝑖=1𝜀𝑖˜X𝑖𝐹˜Y𝑖𝐹.
We can easily deduce that covering the entire window requires
selecting at most two blocks in each level. Furthermore, the maxi-
mum number of expired blocks is at most one block from the first
level. Then 𝑙𝑎𝑠𝑡≤2𝐿,X0Y𝑇
0≤𝑠𝑖𝑧𝑒 1=𝑁𝑅
2𝐿−1and˜X𝑖𝐹˜Y𝑖𝐹=
√𝑠𝑖𝑧𝑒𝑖2=𝑠𝑖𝑧𝑒𝑖≤𝑁𝑅
2𝐿−𝑖, thus
X0Y𝑇
0+𝑙𝑎𝑠𝑡∑︁
𝑖=1𝜀𝑖˜X𝑖𝐹˜Y𝑖𝐹
≤𝑁𝑅
2𝐿−1+𝑙𝑎𝑠𝑡∑︁
𝑖=11
2𝑖𝐿𝑁𝑅
2𝐿−𝑖
≤𝑁𝑅
2𝐿−1+2𝐿𝑁𝑅
2𝐿𝐿
=𝑁𝑅
2𝐿
=𝜀𝑁,
We constrain that 1≤∥x∥2≤𝑅and1≤∥y∥2≤𝑅. Then𝑁≤
∥X𝑊∥𝐹∥Y𝑊∥𝐹, thus𝜀𝑁≤𝜀∥X𝑊∥𝐹∥Y𝑊∥𝐹.
D PROOF OF THEOREM 3
During the move from buffer to Level 1, EH-SCOD handles more
columns without increasing the space budget, and simultaneously
expands the size range of all blocks by a multiple of 𝜅, where𝜅=
𝑂(𝜀𝑁
ℓ). For a block in level 𝑖, its size satisfies 2𝑖−1ℓ𝜅≤𝑠𝑖𝑧𝑒≤2𝑖ℓ𝜅.
Similar to Lemma 2, we obtain 𝐿≤log
2
ℓ𝜅𝑏∥X𝑊∥𝐹∥Y𝑊∥𝐹
. Since
the maximum number of blocks per level remains 𝑏, EH-SCOD
reduces the total number of blocks that need to be maintained forthe same window size. Assuming any column pair xandymeets
∥x∥∥y∥ ≤𝑅, it is evident that ∥X𝑊∥𝐹∥Y𝑊∥𝐹≤𝑁𝑅. We thus
derive the new space complexity:
𝐿𝑏ℓ(𝑚𝑥+𝑚𝑦)≤log2
ℓ𝜅𝑏∥X𝑊∥𝐹∥Y𝑊∥𝐹8
𝜀ℓ(𝑚𝑥+𝑚𝑦)
≤8ℓ
𝜀log∥X𝑊∥𝐹∥Y𝑊∥𝐹
4𝑁
(𝑚𝑥+𝑚𝑦)
=𝑂
(𝑚𝑥+𝑚𝑦)ℓ2log𝑅
.
Not only is there a substantial reduction in space overhead, but the
decrease in merge operations also significantly cuts down on time
costs. The amortized time expense for maintaining the sequence of
sketches is𝑂((𝑚𝑥+𝑚𝑦)ℓ𝐿)=𝑂((𝑚𝑥+𝑚𝑦)ℓlog𝑅).
We conduct a brief analysis of the error introduced by EH-SCOD.
Let¯X𝑖,¯Y𝑖=𝑆𝑃𝑀(˜X𝑖,˜Y𝑖), and assume that the error introduced
by the SPM compression can be bounded by˜X𝑖˜Y𝑇
𝑖−¯X𝑖¯Y𝑇
𝑖
2≤
𝜎∥˜X𝑖∥𝐹∥˜Y𝑖∥𝐹. The error from expiring block is still defined by its
size: 2𝐿ℓ𝜅≤𝜀
4∥X𝑊∥𝐹∥Y𝑊∥𝐹. The upper bound for the approxima-
tion error of blocks in level 𝑖is˜X𝑖˜Y𝑇
𝑖−A𝑖B𝑇
𝑖
2≤˜X𝑖˜Y𝑇
𝑖−¯X𝑖¯Y𝑇
𝑖
2+¯X𝑖¯Y𝑇
𝑖−A𝑖B𝑇
𝑖
2≤ 𝜎+𝜀
82𝑖ℓ𝜅. Therefore, the second part of
the error isÍ𝐿
𝑖=1𝑏 𝜎+𝜀
82𝑖ℓ𝜅≤ 4𝜎+𝜀
2∥X𝑊∥𝐹∥Y𝑊∥𝐹. And
the third part, the sketch merging error remains unchanged at
𝜀
8∥X𝑊∥𝐹∥Y𝑊∥𝐹. By the choice of 𝑞and the property of the SPM
method [25], we have 𝜎=𝜀
32and thus achieve the conclusion.
E PROOF OF THEOREM 4
We conduct a brief analysis of the error introduced by DI-SCOD.
By the property of the SCOD method [ 20], we can still guarantee˜X𝑖˜Y𝑇
𝑖−A𝑖B𝑇
𝑖≤𝜀𝑖˜X𝑖𝐹˜Y𝑖𝐹. Thus, we also have
X𝑊Y𝑇
𝑊−AB𝑇≤X0Y𝑇
0+𝑙𝑎𝑠𝑡∑︁
𝑖=1𝜀𝑖˜X𝑖𝐹˜Y𝑖𝐹.
Similar to appendix C, we can getX𝑊Y𝑇
𝑊,AB𝑇2≤𝜀∥X∥𝐹∥Y∥𝐹.
For space complexity, DI-SCOD dose not use additional space.
Thus, the DI-SCOD algorithm only requires 𝑂(ℓ𝑅log2(ℓ𝑅))space
cost.
For time complexity, the amortized time for the SPM and CS
procedures to process a column is
𝑂𝑛𝑛𝑧(X𝑊)+𝑛𝑛𝑧(Y𝑊)
𝑁𝜀+𝑚𝑥+𝑚𝑦
𝑁𝜀2
.
In the worst-case scenario, SPM and CS operations are performed
concurrently across all 𝐿levels. Thus, the time complexity amounts
to𝑂𝑛𝑛𝑧(X𝑊)+𝑛𝑛𝑧(Y𝑊)
𝑁𝜀+𝑚𝑥+𝑚𝑦
𝑁𝜀2
log𝑅
𝜀
.
3906