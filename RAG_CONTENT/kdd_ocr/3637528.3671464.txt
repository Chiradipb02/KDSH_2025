Symbolic Regression: A Pathway to Interpretability Towards
Automated Scientific Discovery
Nour Makke∗
Qatar Computing Research Institute, HBKU
Doha, Qatar
nmakke@hbku.edu.qaSanjay Chawla
Qatar Computing Research Institute, HBKU
Doha, Qatar
schawla@hbku.edu.qa
ABSTRACT
Symbolic regression is a machine learning technique employed for
learning mathematical equations directly from data. Mathematical
equations capture both functional and causal relationships in the
data. In addition, they are simple, compact, generalizable, and in-
terpretable models, making them the best candidates for i) learning
inherently transparent models and ii) boosting scientific discov-
ery. Symbolic regression has received a growing interest since the
last decade and is tackled using different approaches in supervised
and unsupervised deep learning, thanks to the enormous progress
achieved in deep learning in the last twenty years. Symbolic regres-
sion remains underestimated in conference coverage as a primary
form of interpretable AI and a potential candidate for automating
scientific discovery. This tutorial overviews symbolic regression:
problem definition, approaches, and key limitations, discusses why
physical sciences are beneficial to symbolic regression, and explores
possible future directions in this research area.
CCS CONCEPTS
•Computing methodologies: Machine learning; Artificial
intelligence; Modeling and simulation; •Applied computing:
Physical sciences and engineering;
KEYWORDS
Symbolic regression, Interpretable AI, Model discovery, Physical
sciences
ACM Reference Format:
Nour Makke and Sanjay Chawla. 2024. Symbolic Regression: A Pathway to
Interpretability Towards Automated Scientific Discovery. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
9 pages. https://doi.org/10.1145/3637528.3671464
1 INTRODUCTION
The deep learning revolution, mainly powered by convolutional
and recurrent neural networks, has led to enormous success in areas
∗N. Makke is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671464such as images [ 1], machine translation [ 2,3], and reinforcement
learning [ 4]. Its remarkable success led to a surge in its application
in nearly all domains, including critical disciplines that affect human
lives, such as criminal justice and healthcare. Along with it has
arisen the concept of interpretable AI with the call to stop explaining
“black-box” models, referred to as “explainable AI”, and developing
instead “white-box” models, referred to as “interpretable AI”, as
presented and discussed in [5].
Symbolic regression (SR), a specific application of symbolic learn-
ing whose success dates back to the early days of AI, emerges as a
primary form of interpretable AI. SR aims to infer from data math-
ematical equations that not only fit the data well but also provide
interpretable insights into the underlying relationships between
the (input) features and the (output) prediction. Early approaches
to SR relied on symbolic manipulation techniques and heuristic
search algorithms to explore the space of mathematical expressions.
However, these methods often struggled with scalability and ef-
ficiency issues, especially when dealing with large and complex
datasets. In recent years, advancements in optimization algorithms,
computational power, and machine learning (ML) techniques have
revitalized interest in SR. Modern SR algorithms combine symbolic
manipulation with evolutionary algorithms, namely genetic pro-
gramming and deep learning tools, to efficiently search for symbolic
expressions that optimize predefined criteria, such as model accu-
racy and complexity. Symbolic regression is particularly suited for
use in sciences, in general, and in physical sciences, in particular,
given that physics is mainly expressed by mathematical formulae.
In addition, symbolic regression was historically introduced as a
revolutionary tool for boosting scientific discovery. However, the
overall application of AI in physical sciences has been restricted,
given the non-transparent nature of deep neural networks.
This tutorial aims to shed light on symbolic regression as a
fundamental form of interpretable AI, with an in-depth presentation
of the SR problem, its key limitations, and an overview of the
soundest approaches used to solve it. It highlights the importance
of applying SR to real problems and examines whether we have
reached a point of greater trust in SR compared to black-box models.
2 EXPLAINABILITY VS. INTERPRETABILITY
Explainable AI (XAI) focuses on explaining the predictions made by
deep neural networks, known as “black-box” models, i.e., provides
post-hoc explanations of the underlying factors supporting a deci-
sion in human-understandable terms (e.g., input features). It usually
involves feature importance scores and decision trees to help users
comprehend the rationale behind a model’s prediction. Explainable
AI models, such as DeepLIFT [ 6] and LIME [ 7], are mainly deployed
6588
KDD ’24, August 25–29, 2024, Barcelona, Spain Nour Makke and Sanjay Chawla
in high-stakes applications where trust and transparency are crit-
ical (e.g., criminal justice, healthcare). Interpretable AI (IAI) [ 5],
instead, focuses on learning ML-based models that are inherently
interpretable, i.e., models that ease human understanding of these
models and allow reasoning to enable trust for AI models, using
particular model architecture (e.g., linear models, decision trees,
mathematical equations).
While both XAI and IAI aim to improve the transparency of
ML-based models learned from data, they are conceptually and
practically different, although still interchangeably used in litera-
ture. XAI attempts to explain models’ decisions after making them,
whereas IAI focuses on designing inherently interpretable models
from the outset. These terminologies are discussed in more detail
in [5]. Although IAI is far more advantageous than XAI, there are
still open questions on how to define IAI and whether AI should
be interpretable to humans. Within the IAI framework, symbolic
regression holds significant potential for model discovery through
mathematical equations.
3 WHY MATHEMATICAL EQUATIONS?
Learning mathematical equations, which is the main focus of sym-
bolic regression, has several advantages over using (large) numeri-
cal models because of the following properties:
(1) Simple : They provide a (relatively) simple description of
complex phenomena, making them easier to understand and
work with than (large) numerical models.
(2) Compact : Mathematical equations are a compact and com-
pressed representation of a (wide) range of observations,i.e.,
lots of knowledge.
(3) Generalizable : A mathematical equation can be easily ap-
plied and tested on new data, thus allowing for meaningful
extrapolation and prediction beyond training datasets.
(4) Interpretable : Mathematical equations are expressed in hu-
man language, i.e., symbols and the functional forms relating
input features to output, allowing for understanding and rea-
soning.
(5) Transferable : Mathematical equations learned in one context
can be transferred and applied to similar problems within
and across various domains.
These characteristics provide primary insights into why SR
stands out as an effective ML technique compared to others and
why it would be particularly suited for use in scientific disciplines.
4 A GLIMPSE OF SR HISTORY
The first instance of using data not to “fit” an equation but to “dis-
cover the equation” was solved in the 16th century by J. Kepler,
when he developed an analytical model for planetary motion us-
ing the most accurate astronomical measurements of the era by T.
Brahe (1546–1601), resulting in elliptic orbits described by a power
law. Developments in symbolic regression in the era of AI date back
to the 1960s when AI demonstrated remarkable successes in solving
symbolic problems in mathematics, such as theorem proving, a few
decades before symbolic regression emerged. The history of SR
could be marked by three key moments. It was initially developedin the 1970s by D. Gerwin and P. Langley as a means of discovering
physical laws directly from data, with the creation of the BACON
software [ 8] and its subsequent developments in [ 9,10], as well as
the following-up FAHRENHEIT software [ 11]. BACON, the first
software to employ data-driven heuristics, successfully uncovered
fundamental physics laws from empirical data, such as the ideal gas
law, Kepler’s third law, Coulomb’s law, Galileo’s law, and Ohm’s law.
The rise of genetic algorithms and the newly introduced represen-
tation of mathematical equations using expression trees (Sec. 5.2)
in the pioneering work by J. Koza [ 12] has reshaped SR, which was
since developed within the active genetic programming community
for a few decades. Two systems followed up in 1995 [ 13], QNM
and LAGRANGE, to discover laws that govern the behavior of dy-
namical systems based on ideas from inductive logic programming
and machine discovery. The 2000 decade marked the third moment
with the development of the process model [ 14] by P. Langley and
collaborators in 2007 and the creation of EUREQA [ 15–17] in 2009,
a commercial software developed for data-driven model discovery
in real-world and scientific data. Previous attempts to promote the
discovery of analytical models by machines through BACON and
EUREQA were unsuccessful.
More recently, SR has experienced a revival as a potential sci-
entific discovery tool and, more importantly, as an ML-based tech-
nique for learning interpretable and/or human-understandable mod-
els directly from data. Despite being an active ML area now and
having notable advantages over other regression methods, SR re-
mains relatively undermined in the realm of ML, and its adoption
in practice has been very limited compared to other ML techniques.
In addition, it is primarily developed and utilized by scientists,
with research focusing on developing new methods using various
techniques. However, the evaluation of these methods and their
comparison often rely on synthetic datasets, which may not provide
a fair and transparent assessment of their performance. Will this
resurgence of SR be a pivotal moment for the future of this field?
5 SYMBOLIC REGRESSION
In the following, we define the SR problem and present state-of-
the-art methods in this field. In Sec. 5.1, we define the SR problem
and discuss how it differs from conventional regression problems.
Then, we discuss its key limitations in Sec. 5.3 and overview various
approaches to solving SR in Sec. 5.4. An overview and discussion
of available datasets used in SR are provided in Sec. 6. In Sec. 7, we
highlight the benefits of using SR in sciences and discuss why and
how the physical sciences stand so beneficial to SR.
5.1 Problem definition
Symbolic regression searches for a closed-form expression directly
from data. Given a dataset D={(x𝑖,𝑦𝑖)}with𝑖∈ [1,···,𝑁],
where x∈R𝑑is the input vector of features, and 𝑦∈Ris the
scalar output, SR searches in the function class F, i.e., the space
of mathematical expressions, the mapping 𝑓:R𝑑→Rthat best
matchesDguided by a real-valued loss function, 𝑙(𝑓𝑤(𝑋),𝑦), that
measures the approximation error of the regression model 𝑦and
the measured target 𝑓, where𝑤refers to the model’s weights. The
6589Symbolic Regression: A Pathway to Interpretability Towards Automated Scientific Discovery KDD ’24, August 25–29, 2024, Barcelona, Spain
−
+
1 1*
x x
Figure 1: Expression-tree structure of the mathematical equa-
tion𝑓(𝑥)=2−𝑥2. Root (green), internal (red), and terminal
(blue) nodes are differently colored.
average of the loss over the training set is called the empirical risk :
R(𝑓𝑤)=1
𝑁𝑁∑︁
𝑖=1𝑙(𝑓(x𝑖),𝑦𝑖) (1)
Unlike conventional regression problems, the optimization task in
SR stands to minimize the loss function over the function class F,
which can be regarded as the space of admissible models:
𝑓∗(x)=arg min
𝑓∈FR(𝑓𝑤) (2)
Now, the task is to define that search space. A common approach
is to defineFon a set of underlying primitives 𝑝𝑖:R𝑎𝑖→R
with arity𝑎𝑖, e.g., unary and binary mathematical functions. This
set is a user-defined set commonly referred to as a “library” in re-
search papers. It consists of basic arithmetic (binary) operations (i.e.,
+,−,∗,÷), a selection of (unary) mathematical functions (e.g., sin(·),
cos(·),exp(·),log(·), pow(·), etc.), and finally variables ( 𝑥𝑗,𝑗∈N𝑑)
and free constants ( 𝑐). Given the set of primitives, the function class
is defined by all possible combinations of the elementary functions
inL. For instance, the basic library L:={+,∗,𝑥,𝑐, 1}is sufficient
to construct all forms of monomials and polynomials of any degree
(𝑛) in the variable 𝑥, i.e.,
𝑃(𝑥)=𝑎𝑛𝑥𝑛+𝑎𝑛−1𝑥𝑛−1+···+𝑎1𝑥+𝑎0 (3)
5.2 Expression-tree representation
Equivalently, a well-suited representation of mathematical equa-
tions is the expression tree, where internal nodes are mathematical
operations that connect a tree’s root to its terminal nodes represent-
ing the operands (i.e., variables and constants), as illustrated in the
exemplary equation in Fig. 1. A key advantage of the expression
tree representation is that every equation can be represented by a
tree, and every tree can be traversed into a unique sequence of sym-
bols following the Polish notation [ 18], i.e., the function presented
in Fig. 1 can be expressed as a sequence of length 7, as follows:
−+11∗𝑥𝑥
This representation allows one to technically approach an equation
as a text. This opens the door for using natural language process-
ing (NLP) techniques, namely, transformer neural networks, which
have shown great performance, to mathematical equations in the
context of SR. At this stage, the SR problem reduces to solving the
optimization task defined in Eq. 2, representing the most challeng-
ing part of SR, as discussed below.5.3 Key challenges
(C1) Nature of the search space: The search space in SR is by
definition discrete. In the past twelve years, gradient-descent has
been shown an incredibly powerful tool for solving similar and
complex optimization problems in deep learning by taking gradi-
ents of the loss function 𝑙(𝑤)with respect to models’ parameters
(weights)𝑤∈R𝑚as follows, where 𝛾is known as the learning rate :
𝑤𝑡+1=𝑤𝑡−𝛾∇𝑤R(𝑓𝑤) (4)
In SR, the optimization problem ranges over a discrete set; thus,
derivatives do not exist. To clarify why this is important, assume a
candidate solution 𝑓0with𝑙(𝑓0)=0.3. Backpropagation automati-
cally gets a direction of improvement to find a better solution. In
SR, however, to get a better candidate, one has to search for another
solution, like in a trial-and-error approach, propose a new candidate
solution𝑓1and evaluate it, which could simply be a worse candidate
solution, e.g., 𝑙(𝑓1)=0.7. Here lies the main difference between
a discrete optimization problem and a continuous optimization
problem.
A common approach for encountering this challenge is by pa-
rameterizing the search space, thus transforming the problem of
Eq. 2 into a regular optimization problem in ML, i.e.:
𝑓∗(x)=𝑓(x,𝑤∗), 𝑤∗=arg min
𝑤∈R𝑚R(𝑓𝑤) (5)
(C2) Size of the search space: In SR, the size of the search
space grows exponentially with model complexity, i.e., the length
of the equation. For the simple equation 𝑓(x)=𝑥1/𝑥2
2consist-
ing of five symbols {÷,𝑥1,pow,𝑥2,2}, and a library including 20
mathematical operations such as:
L={+,−,∗,÷,cos,sin,tan,log,exp,n2,n3,n4,√,inv,Abs,𝑥,𝑦, 1,2,3}
Fitting the dataset with a naive brute-force search will have to con-
sider up to 206=3.2×106candidate solutions without accounting
for optimizing numerical constants when applicable. The number
of trials increases with the length of an equation, making SR an
“NP-hard” problem, as discussed and shown in [ 19]. In physics, for
example, a relatively simple formula with four independent vari-
ables x∈R4, i.e., (𝑥1, 𝑥2, 𝜃1, 𝜃2) such as:
𝑓(x)=√︃
𝑥2
1+𝑥2
2−2𝑥1𝑥2cos(𝜃1−𝜃2)
leads up to 2016≈6.6×1020candidate solutions to consider in
a brute-force search. Besides this being beyond computational re-
sources today, it becomes challenging and impractical for more
complex formulas and an extensive library choice.
The best manner to encounter this challenge is to consider prior
domain-specific knowledge, e.g. [ 20], to reduce the size of the li-
brary. For instance, one may exclude trigonometric functions (e.g.,
sin,cos,tan, etc.) when periodic behavior is neither expected nor
observed in the data. Also, a priori constraints may be applied to
reduce the search space. For example, the DSR [ 21] method applies
the following set of constraints: (1) Expressions are limited to a
pre-specified minimum and maximum length. A minimum length
of 4 is chosen to prevent trivial expressions and a maximum length
of 30 to ensure expressions are tractable. (2) The children of an
operator should not all be constants, as the result would simply
6590KDD ’24, August 25–29, 2024, Barcelona, Spain Nour Makke and Sanjay Chawla
be a different constant. (3) The child of a unary operator should
not be the inverse of that operator; e.g., log(exp(𝑥))is not allowed.
(4) Descendants of trigonometric operators should not be trigono-
metric operators, e.g. sin(𝑥+cos(𝑥))is not allowed because cosine
is a descendant of sine. While still semantically meaningful, such
composed trigonometric operators do not appear in virtually any
scientific domain.
5.4 Approaches for solving SR
The SR problem was initially tackled with heuristic search until the
1970s when P. Koza introduced the expression-tree representation
for mathematical equations. Since then, SR has traditionally been
approached with genetic programming (GP) within a broad commu-
nity that gathers yearly at the Genetic and Evolutionary Computa-
tion Conference (GECCO). However, with the latest achievements
of deep learning, new attempts to solve SR using new techniques
have emerged in supervised and unsupervised machine learning.
All existing approaches are presented in the following categories.
We will overview each category’s concepts and list its fundamental
limitations.
(1)Regression-based SR:
•Sparse Linear regression: The target model ( ˆ𝑓𝜃) is a
linear combination of non-linear functions ( 𝑓𝑖):
ˆ𝑓𝜃(x)=𝜃𝑘𝑓𝑘(x)+𝜃𝑘−1𝑓𝑘−1(x)+···+𝜃0 (6)
A coefficient ( 𝜃𝑖) is assigned to each function ( 𝑓𝑖) in the
library, hence acting as an activation coefficient, such that
the sparse vector of coefficients Θ≡[𝜃1𝜃2···𝜃𝑘]deter-
mines which non-linear functions are active. To accom-
plish this, a matrix U(X) is constructed by evaluating all
𝑓𝑗∈L on each input vector x𝑖. Given an exemplary li-
braryL={𝑓1,𝑓2,···,𝑓𝑘}, this yields ( 𝑁×𝑘)- dimensional
matrix U, such that:
𝑦1
𝑦2
...
𝑦𝑁|{z}
𝑌∈R=𝑓1(𝑥1)𝑓2(𝑥1) ···𝑓𝑘(𝑥1)
𝑓1(𝑥2)𝑓2(𝑥2) ···𝑓𝑘(𝑥2)
...
𝑓1(𝑥𝑁)𝑓2(𝑥𝑁) ···𝑓𝑘(𝑥𝑁)|                                         {z                                         }
𝑈(𝑋)∈R𝑁×𝑘𝜃1
𝜃2
...
𝜃𝑘|{z}
Θ∈R𝑘(7)
The SR problem is reduced into a system of linear equa-
tions:
Y=U(X)Θ (8)
A null coefficient 𝜃𝑗cancels out the contribution of the
𝑗thfunction, and a non-zero 𝜃𝑗determines the weight of
𝑓𝑗, as illustrated in Fig. 2. It is noteworthy that, in this
case, the structure is only defined by two binary opera-
tors (add, mul), and the library only includes elementary
mathematical functions (e.g., cos,sin,exp,log, sqrt, pow,
etc). This method suffers two key limitations. First is the
dimensionality of the matrix U, which can infinitely grow
for a large number of features ( 𝑑>>) and an extended
libraryL. Second, the model is defined as a linear combina-
tion of non-linear functions. For instance, a specific func-
tional form (e.g., exp(−𝑥1𝑥2),𝑥1𝑥2cos(𝑥3),𝑥2
1exp(−𝑥2))
will not be discovered unless it is explicitly part of thelibrary. This implies that applying linear regression-based
SR to real-world problems, where underlying models are
fully unknown, is very limited given the endless number
of possible functions.
Y
=1𝑥𝑥2𝑥3
···Θ
Figure 2: Schematic of the system of linear equations of Eq. 8
for𝑓(𝑥)=1+𝛼𝑥3. A library matrix U(X)of non-linear func-
tions of the input is constructed, where 𝐿={1,𝑥,𝑥2,𝑥3,···}.
The marked entries in the Θvector denote the non-zero coef-
ficients determining which library functions are active.
•Non-linear regression: Neural Networks (NNs) are
used to learn symbolic expressions by replacing the tradi-
tional activation functions in an NN (e.g., sigmoid, ReLu,
Tanh) with regular mathematical operations (i.e., unary
and binary operators). This is conceptually similar to lin-
ear SR because the NN’s parameters would zero out the
contribution of some functions by learning null (or al-
most null) weights. The main difference between linear
and non-linear SR is in the form of the target model, i.e.,
Eq 6 compared to Eq.9, where 𝑔∈L, and𝐿refers to the
network’s depth.
ˆ𝑓w(X)=𝑔[𝐿]
𝑊[𝐿]𝑔[𝐿−1]
···𝑔[1]
𝑊[1]X)
(9)
Take-away: A Key limitation in regression-based SR meth-
ods is the pre-defined model structure. If the structure of the
underlying model does not match the pre-defined structure,
this category could only learn an approximation of the target
model. However, it’s worth mentioning that non-linear SR
is more flexible than linear SR.
(2)Expression tree-based SR : Mathematical equations can
be represented by expression trees. This representation en-
sures sufficient flexibility for learning unknown functions.
Methods in this category have shown outperformance com-
pared to other SR approaches.
•Genetic Programming (GP): GP is a type of evolu-
tionary algorithms. It operates on an initial population
of individuals (i.e., candidate solutions) using a set of op-
erations (or rules) that are analogous to natural genetic
processes, guided by a fitness function that evaluates the
quality of each solution. Among natural processes is the
mutation operation (cf. Fig. 3), which involves operations
such as changing an operation or an operand at a randomly
selected node, adding or removing subtrees, or chang-
ing constants. Another one is the crossover operation (cf.
Fig. 4), which involves swapping subtrees between two
parent individuals at randomly selected crossover points.
6591Symbolic Regression: A Pathway to Interpretability Towards Automated Scientific Discovery KDD ’24, August 25–29, 2024, Barcelona, Spain
−
∗
x x
2−
*
x x
cos
x
𝑥2−2 𝑥2−cos(𝑥)
Figure 3: A mutation operation applied to an expression tree
that involves changing the operand (i.e., 2) at a randomly
selected node with another subtree (i.e., cos(𝑥)).
−
∗
x x
*
2
 x+
1
 e
xp
x
𝑥2−2𝑥 1+exp(𝑥)
−
∗
x x
e
xp
x+
1
 *
2
 x
𝑥2−e
xp(𝑥) 1+2𝑥
Figure 4: A crossover operation between two expression trees.
It involves swapping subtrees between two parent individ-
uals at randomly selected crossover points. Upper row: two
individuals in a generation. Lower row: the two individuals
after swapping their subtrees.
The evolution process occurs over multiple generations,
with selection pressure favoring individuals with higher
fitness. The algorithm iterates this procedure until a sat-
isfactory level of accuracy is achieved. GP is the most
widely used method for solving SR problems and gener-
ally outperforms other SR approaches when applied to
real-world problems where underlying models are com-
pletely unknown and are not expected to reflect any causal
relationship in the data.
Disadvantage of GP-based SR : A key limitation of GP is
that it does not leverage past experience: every new prob-
lem is learned from scratch. In addition, it has been shownthat GP-based methods do not scale well to high dimen-
sional data sets and are highly sensitive to hyperparame-
ters [21].
•Transformers: A transformer neural network (TNN)
is an NN architecture designed to capture long-range de-
pendencies in sequential data, a key component in natural
language processing applications. TNNs [ 3] are sequence-
to-sequence (seq2seq) models, i.e., both input and output
are sequences. The key advantage of TNNs is that they
learn the context in a given sequence through the atten-
tion mechanism introduced in [ 3], and predict a sequence
on a token-by-token basis using the context learned via
the attention blocks. For instance, the two following sen-
tences are identical except for the last word, cf. “sick” in
the first sentence (S1) and “expired” in the second one (S2):
S1The
animal didn’t eat the meat because it was sick .
S2The animal didn’t eat the meat because it was expired .
However, the context is different between the two sen-
tences, i.e., the pronoun “it” refers to “animal” in the first
sentence, whereas “it” in the second one refers to “meat”.
The attention blocks capture the dependencies between
“it” and the last word in S1 and S2 such that their transla-
tion from English to French is different as follows:
S1L’animal n’a pas mangé la viande parcequ’ ilétait
malade.
S2L’animal n’a pas mangé la viande parcequ’ elleétait
périmé.
TNNs have recently been introduced to SR [ 22,23] as set-
to-sequence models, where the input is a set of numerical
data points, and the output is a sequence of symbols. In
SR, TNNs have the potential to learn the causal relation-
ships between independent input variables. For example,
in the function 𝑓(𝑥)=𝑥+cos(𝑥), which is represented as
a sequence of symbols, cf. the Polish notation, as follows:
+cos𝑥𝑥
the third token, “ 𝑥”, is the operand of the unary oper-
ator cos, whereas the fourth token, “ 𝑥”, is the second
operand of the binary operator addition. It’s notewor-
thy that TNNs are generative models made up of either
an encoder-decoder or a decoder-only model. Whereas
TNNs are seq2seq models in NLP, they are set-to-sequence
(set2seq) models in SR, i.e., they input a set of numerical
points that are expressed as sequences, where an input
number is represented by a sequence of (sign, mantissa,
exponent ). For example, the speed of light 𝑐=3.108is rep-
resented as[+,3,𝑒+8]. This applies to all input numbers.
In addition, the input order in TNNs, when used in an SR
task, does not count, unlike NLP, where the order of the in-
put characters is crucial to learning the correct context in
6592KDD ’24, August 25–29, 2024, Barcelona, Spain Nour Makke and Sanjay Chawla
a sentence. Finally, two approaches are considered in TNN-
based SR methods: the target model is either predicted as
a skeleton, and then the numerical values of its constants
are determined from fit to data, or the model’s structure
and constants are simultaneously predicted. TNNS have
been used to develop SR methods that outperformed other
SR methods.
Advantages of TNN-based SR :
⋄The search for the target model is not driven by data-
model matching only but inherently guided by the at-
tention blocks that capture the correlations between the
different features and the target.
⋄TNNs can be pre-trained on large datasets, and the pre-
trained version could be used for inference in real-time
without re-training for every new problem, allowing for
fast computation.
Take-away: The expression tree-based SR methods are ad-
vantageous given the flexibility to learn the model structure
as a parse tree along its numerical parameters, which can be
either learned simultaneously with the structure or found at
a later stage by fitting data.
(3)Science-oriented SR: Few SR methods are developed
using theoretical concepts from physics and mathematics.
This category includes two approaches:
•Physics-oriented SR: Physics is expressed by math-
ematical equations, and physical laws all share common
key properties, which are listed below:
⋄Compositionality: A physical model is usually a compo-
sition of a small set of elementary (unary and binary)
functions.
⋄Factorization: A physical model can be factorized into
independent functions with no common variables.
⋄Symmetry: A physical model exhibits some symmetry
(translational, rotational, scaling) with respect to some
or all of its variables.
⋄Generalizability: Models are expected to be general-
izable at different levels (cf. out of range and out of
distribution).
⋄Compactness: physical models are extremely compact
compared to the simplest numerical models. For exam-
ple, the following set of coupled equations, known as
Maxwell’s equations:
∇·E=𝜌
𝜖0∇×E=−𝜕B
𝜕𝑡
∇·B=0∇×B=𝜇0𝑗+1
𝑐2𝜕E
𝜕𝑡
is a very compact and compressed representation of
the classical electromagnetic theory, which covers a
wide range of experimental observations. Such a physi-
cal model identifies the main underlying constituents(electric and magnetic fields), and their interactions are
modeled through this set of equations.
⋄Dimensional analysis: well-known in physics, the units
of measurement of an equation on both sides must
match.
These properties could be considered to simplify the search
for any mathematical equations, not limited to physical
problems, except the last property, which is specifically
applicable to physics. They perfectly align with the pur-
pose of SR. In this context, AIFeynamn [ 24], a physics-
based SR, uses these properties to simplify the search for
mathematical expressions. It relies on the aforementioned
properties to reduce the number of independent variables
in a problem (using dimensional analysis) or to divide
the problem into subproblems (using symmetry) through
the use of a neural network. Finally, it uses polynomial fit
and brute-force search to solve the identified subproblems.
•Mathematics-oriented SR: The Meijer 𝐺-functions
represent a class of complex-valued contour integrals that
depend on real-valued parameters and whose solutions re-
duce to familiar algebraic, analytic, and closed-form func-
tions for different parameters settings. This parameterized
representation of symbolic expressions was introduced in
the framework of the symbolic metamodel [ 25]. The key
advantage of using the Meijer 𝐺-function is that it effi-
ciently minimizes the models’ loss function using gradient
descent, which is one of the main challenges in solving SR
(cf. Sec. 5.3). To further clarify, the Meijer 𝐺-function is
characterized by a set of indices and two sets of real-values
parameters, 𝐺𝑚,𝑛
𝑝,𝑞(a𝑝,b𝑞|𝑥), and an instance of it is spec-
ified by the sets(a,b). For example, 𝐺1,2
2,2takes different
forms for two different sets of aandb, i.e.,
𝐺1,2
2,2(1,1
1,0|𝑥)=log(1+𝑥)
𝐺1,2
2,21
2,1
1
2,0|𝑥
=2 arctan(𝑥)(10)
Given that, the optimization problem reduces to find the
set of parameters ( 𝜃∗), where𝜃=(a𝑝,b𝑞), that minimized
the loss function:
𝜃∗=arg min
𝜃∈Θ𝑙(𝐺(x,𝜃),𝑦) (11)
This is a conventional optimization problem in ML and
the target model is simply given by:
𝑔∗(x)=𝐺(x,𝜃∗) (12)
Take-away: This category, by introducing scientific tools
and theoretical concepts, allows for a well-structured and
guided search of expressions in the search space, in contrast
to genetic programming, for example, where the search is
only guided by data.
State-of-the-art SR methods are summarized in Table 1. For each
method, the category, its open-source code, and a brief description
6593Symbolic Regression: A Pathway to Interpretability Towards Automated Scientific Discovery KDD ’24, August 25–29, 2024, Barcelona, Spain
are all specified. A full review and discussion of SR methods, ap-
plications, and datasets can be found in [ 26] along with a living
review of SR in [27].
6 DATASETS FOR SR
Data represent a key ingredient in any ML field. For symbolic
regression, datasets can be categorized into two main groups:
(1)Ground-truth problems are characterized by a known
mathematical expression of the underlying model. This cate-
gory includes equations originating either from mathematics
or physics. The main difference between the two subcate-
gories is that physical equations describe natural physical
phenomena and thus inherit causal relationships between
the independent variables, whereas mathematical equations
consider any arbitrary combination of elementary functions.
Data points (x 𝑖) are generated in some intervals of real val-
ues and the model ( 𝑦𝑖) is evaluated using the mathematical
expression ( 𝑓) such that𝑦𝑖=𝑓(x𝑖). This category includes
various datasets that are summarized in [26, 27].
The Feynman benchmark [ 24] is the main physics dataset. It
consists of 119 physical equations from Feynman’s lecture
notes and covers different topics in physics. The richness of
this benchmark relies on i) a large number of equations, ii)
a large number of independent variables that goes up to 8,
and iii) the structures of the equations which can be quite
complex because they reflect underlying phenomena and
include numerical constants. At the same time, the Feynman
benchmark suffers a few key limitations. First, the constants
are considered independent variables, i.e., they are part of
the input feature vector x, and their numerical values are
generated i) in intervals of real values (whereas they should
be constant) and ii) in fully unphysical ranges. In practice, the
numerical values of the physical constants are fixed and are
usually either very small or very large, e.g., 𝜖=8.8×10−12,
𝑐=2.99×108,𝜇=9.27×10−24,ℎ=6.6×10−34,𝐺=
6.6×10−11,𝑘=1.38×10−23, etc. For example, the speed
of light𝑐=3×108(m/s), which is a physical constant, is
considered in the same way as the independent variables
(velocity and mass) and generated between 1 and 5 in the
relativistic energy’s equation:
𝐸=𝑚𝑐2
√︁
1−𝑣2/𝑐2
This applies to almost all physical constants that emerge
in the equations documented in AIFeynman [ 24]. This lim-
itation implies a bias in evaluating the performance of SR
approaches using this benchmark, given that optimizing the
numerical constants represents an important step in learning
mathematical equations. A recently introduced repository of
datasets [ 33] overcomes these limitations by removing phys-
ical constants from input feature vectors and considering
their real values in the data simulation. Other benchmark
datasets belong to this category, they are pure mathemat-
ical equations introduced in different research works onSR, namely, Nguyen [ 34], Koza [ 35], Keijer [ 36], Vladislavl-
eva [37], Korns [38], R [39], Jin [40], and Livermore [21].
Ground-truth problems are very important and useful for:
⋄generating synthetic data points that serve as training and
test datasets, especially when large datasets are needed
(e.g., generative pre-trained models).
⋄evaluating the performance of SR methods given that the
outcome is known and thus an accurate estimation of the
error can be performed.
(2)Real-world problems are characterized by unknown
mathematical expressions of underlying models in datasets.
This category includes any dataset in any area, whether in
sciences (lab measurements of new observables) or not. This
could be data from Netflix, Facebook, commercials, etc.
7 THE ROLE OF PHYSICAL SCIENCES
Physical sciences aim to study and understand the formation of
matter and its interaction in the universe. It involves data taking
(i.e., lab measurements) and analysis (i.e., observable measurements)
to learn mathematical forms of underlying theories. Data analysis
relies on statistical methods This field stands so beneficial to SR
due to many factors:
(1)Physical phenomena are naturally expressed by mathemati-
cal equations, which are symbolic expressions.
(2)Physics has a variety of fundamental laws and hundreds
of mathematical equations that cover physical disciplines,
e.g., classical mechanics, electromagnetism, electrodynam-
ics, fluid dynamics, thermodynamics, etc. This allows the
generation of a variety of synthetic data with known un-
derlying models for testing and evaluation of SR methods’
performance.
(3)Physical phenomena inherently exhibit a causal relationship
between independent physical variables. This characteristic
renders physics data particularly meaningful for testing SR
methods. Such testing evaluates the ability of SR methods
not only to learn a model that fits data well in the form of
a mathematical expression but also to discern the causal
relationship among different variables within the data.
(4)Physics is the field of measurement. Modern datasets are
high-dimensional and high-statistical. Analyzing data and/or
discovering simple symbolic relationships becomes almost
impossible without using automated tools.
Apart from useful physical concepts that can be used to ease the
search for mathematical expressions using physical laws’ properties,
physics is an enormous source of datasets, which is a key ingredient
for data-driven model discovery.
8 STATE-OF-THE-ART SR AND LIMITATIONS
Symbolic regression is an active area of research that is rapidly
evolving. It holds a specific interest in domains that require the
application of interpretable and safe AI. At the same time, it could
be applied to any domain. Currently, the primary emphasis in SR
lies in developing new methods that compete with existing ones,
often overlooking the necessity of establishing a proper baseline
for a meaningful comparison. It’s worth noting that:
6594KDD ’24, August 25–29, 2024, Barcelona, Spain Nour Makke and Sanjay Chawla
Table 1: Table summarizing symbolic regression applications. A full list of methods is available in [27]
Method Category Link Implementation Year Description
SINDY [28] LSR URL Python 2016 Sparse Identification of non-linear Dynamics: Formulates the
SR problem as a system of linear equations built using the set
of allowable operators and learns the coefficients of candidate
functions using a deep autoencoder (AE) network.
EQL [29] DL (NN) URL Python 2016 Equation learner: A fully differentiable shallow neural network
with non-linear activation functions (e.g., algebraic operators
and analytical functions) instead of traditional ones (e.g., sig-
moid, relu, softmax, etc.)..
EQL÷ [30] DL (NN) URL Python 2018 This is an extendable version of the original EQL[URL] that
includes the division among candidate activation functions.
PySR [31] EA URL Pyhton/Julia 2023 Fast and parallelized symbolic regression in Python/Julia based
on evolutionary algorithms (EA). Treats mathematical expres-
sions as trees and uses tournament selection
DSR [21] Mix URL Pyhton 2021 Consists of a generative RNN model of symbolic expressions
trained using reinforcement learning
NeSymReS [22] DL (TNN) URL Pyhton 2021 A pre-trained transformer that predicts SR models directly from
data through a two-step approach: predict the equation skeleton
and then fit numerical constants.
E2ET [23] DL (TNN) URL Pyhton 2022 A pre-trained transformer that predicts SR models directly from
data following a one-step approach: simultaneously predict
equation skeleton and constants.
𝜙-SO [32] DL (RL) URL Python 2023 Consists of a generative RNN model of symbolic expressions
trained using reinforcement learning (DSR method) while fully
leveraging physical units (of measurement) constraints.
AIFeynman [24] Mix URL Pyhton 2019 A physics-informed SR method that uses NN to learn symme-
tries in data (translational, rotational, vibrational) to simply the
global SR problem into simpler SR subproblems.
SM [25] SB URL python* 2019 Use Meijer 𝐺-functions to search for mathematical equations.
⋄Most SR methods evaluation and their comparison are based
on synthetic datasets, among which is the Feynman bench-
mark, which suffers fundamental limitations, making the
comparison metrics meaningless.
⋄The comparison of different methods can be done in various
manner. A set of metrics has to be defined for each appli-
cation domain. For example, what needs to be achieved in
physics differs from what needs to be achieved in biology-
related problems.
⋄Applications of SR on real problems where data points are
measured and not generated, and underlying models are
unknown are rare.
Symbolic regression is expected to have a high potential as a
model discovery tool. However, it deserves to be spotlighted, and
further applications of SR on real problems are highly needed to
define future direction in this research area.
Last but not least, why do we need SR? Should AI be understandable
to humans? From a physics perspective, the goal of expressing mod-
els as mathematical expressions is that, apart from gaining a deep
understanding of underlying physical phenomena, such an under-
standing allows physicists to generalize and transfer the knowledge
among different topics in physics. For example, quantum electro-
dynamics is known to be a well-established theory. It played therole of a fundamental foundation stone for developing quantum
chromodynamics, the theory of strong interaction, given the anal-
ogy between the two theories. Therefore, learning a “black-box”
model that is very accurate and highly predictive would achieve a
lot; however, it won’t help physicists learn and discover more.
9 SUMMARY AND PERSPECTIVES
Symbolic regression is becoming a prominent tool for learning
inherently interpretable models directly from data. Its application is
likely to grow in the future because it balances prediction accuracy
and interpretability.
Symbolic regression is one of the most suited ML tools for use
in sciences, especially in physics, because it uses the physics lan-
guage, i.e., mathematical equations. A key limitation of SR is the
extensive use of synthetic data in SR with very few applications
on experimental noisy (scientific) data, i.e., data points originating
from experiments rather than simulations. However, despite the
limited application of SR to real data, its few existing applications
are promising. A potential path to boost progress in this subfield is
to apply SR to experimental data. This tutorial aims to introduce
symbolic regression to those unfamiliar with it while also explor-
ing its key accomplishments and limitations for those with prior
knowledge of the subject.
6595Symbolic Regression: A Pathway to Interpretability Towards Automated Scientific Discovery KDD ’24, August 25–29, 2024, Barcelona, Spain
REFERENCES
[1]A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep
convolutional neural networks,” in Advances in Neural Information Processing
Systems (F. Pereira, C. Burges, L. Bottou, and K. Weinberger, eds.), vol. 25, Curran
Associates, Inc., 2012.
[2]D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
learning to align and translate,” 2016.
[3]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,
and I. Polosukhin, “Attention is all you need,” CoRR, vol. abs/1706.03762, 2017.
[4]V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and
M. Riedmiller, “Playing atari with deep reinforcement learning,” 2013.
[5]C. Rudin, “Stop explaining black box machine learning models for high stakes
decisions and use interpretable models instead,” Nature Machine Intelligence,
vol. 1, no. 5, pp. 206–215, 2019.
[6]A. Shrikumar, P. Greenside, and A. Kundaje, “Learning important features through
propagating activation differences,” in Proceedings of the 34th International Con-
ference on Machine Learning - Volume 70, ICML’17, p. 3145–3153, JMLR.org, 2017.
[7]M. T. Ribeiro, S. Singh, and C. Guestrin, “"why should i trust you?": Explaining the
predictions of any classifier, ” in Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD ’16, (New York, NY,
USA), p. 1135–1144, Association for Computing Machinery, 2016.
[8]P. Langley, “Rediscovering physics with bacon.3,” in Proceedings of the 6th In-
ternational Joint Conference on Artificial Intelligence - Volume 1, IJCAI’79, (San
Francisco, CA, USA), p. 505–507, Morgan Kaufmann Publishers Inc., 1979.
[9]P. Langley, G. L. Bradshaw, and H. A. Simon, “Bac0n.5: the discovery of conser-
vation laws,” in Proceedings of the 7th International Joint Conference on Artificial
Intelligence - Volume 1, IJCAI’81, (San Francisco, CA, USA), p. 121–126, Morgan
Kaufmann Publishers Inc., 1981.
[10] P. W. Langley, “Bacon: a production system that discovers empirical laws,” in
Proceedings of the 5th International Joint Conference on Artificial Intelligence - Vol-
ume 1, IJCAI’77, (San Francisco, CA, USA), p. 344, Morgan Kaufmann Publishers
Inc., 1977.
[11] P. Langley and J. M. Zytkow, “Data-driven approaches to empirical discovery,”
Artificial Intelligence, vol. 40, no. 1, pp. 283–312, 1989.
[12] J. R. Koza, “Genetic programming as a means for programming computers by
natural selection,” Satistics and Computing, vol. 4, pp. 87–112, Jun 1994.
[13] S. Dzeroski and L. Todorovski, “Discovering dynamics: From inductive logic
programming to machine discovery,” Journal of Intelligent Information Systems,
vol. 4, no. 1, pp. 89–108, 1995.
[14] S. R. Borrett, W. Bridewell, P. Langley, and K. R. Arrigo, “A method for represent-
ing and developing process models, ” Ecological Complexity, vol. 4, no. 1, pp. 1–12,
2007.
[15] B. Josh and H. Lipson, “Automated reverse engineering of nonlinear dynamical
systems,” Proceedings of the National Academy of Sciences, vol. 104, no. 24, pp. 9943–
9948, 2007.
[16] M. Schmidt and H. Lipson, “Distilling free-form natural laws from experimental
data,” Science, vol. 324, no. 5923, pp. 81–85, 2009.
[17] M. Schmidt and H. Lipson, Symbolic Regression of Implicit Equations, pp. 73–85.
Boston, MA: Springer US, 2010.
[18] R. Robinson, “Jan Łukasiewicz: Aristotle’s syllogistic from the standpoint of
modern formal logic. second edition enlarged. pp. xvi 222. oxford: Clarendon
press, 1957. cloth, 305. net., ” The Classical Review, vol. 8, no. 3-4, p. 282–282, 1958.
[19] M. Virgolin and S. P. Pissis, “Symbolic regression is np-hard,” 2022.[20] L. Todorovski and S. DŽeroski, “Using domain knowledge on population dynam-
ics modeling for equation discovery, ” in Machine Learning: ECML 2001 (L. De Raedt
and P. Flach, eds.), (Berlin, Heidelberg), pp. 478–490, Springer Berlin Heidelberg,
2001.
[21] B. K. Petersen, “Deep symbolic regression: Recovering mathematical expressions
from data via policy gradients,” CoRR, vol. abs/1912.04871, 2019.
[22] L. Biggio, T. Bendinelli, A. Neitz, A. Lucchi, and G. Parascandolo, “Neural symbolic
regression that scales,” CoRR, vol. abs/2106.06427, 2021.
[23] P.-A. Kamienny, S. d’Ascoli, G. Lample, and F. Charton, “End-to-end symbolic
regression with transformers,” 2022.
[24] S.-M. Udrescu and M. Tegmark, “Ai feynman: a physics-inspired method for
symbolic regression,” 2019.
[25] A. M. Alaa and M. van der Schaar, “Demystifying black-box models with symbolic
metamodels,” in Advances in Neural Information Processing Systems (H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, eds.), vol. 32,
(New York), Curran Associates, Inc., 2019.
[26] N. Makke and S. Chawla, “Interpretable scientific discovery with symbolic re-
gression: a review,” Artificial Intelligence Review, vol. 57, no. 1, p. 2, 2024.
[27] N. Makke and S. Chawla, “A living review of symbolic regression,”
https://github.com/nmakke/SR-LivingReview, 2022.
[28] S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Discovering governing equations from
data by sparse identification of nonlinear dynamical systems,” Proceedings of the
National Academy of Sciences, vol. 113, no. 15, pp. 3932–3937, 2016.
[29] G. Martius and C. H. Lampert, “Extrapolation and learning equations,” CoRR,
vol. abs/1610.02995, 2016.
[30] S. S. Sahoo, C. H. Lampert, and G. Martius, “Learning equations for extrapolation
and control,” CoRR, vol. abs/1806.07259, 2018.
[31] M. Cranmer, “Interpretable machine learning for science with pysr and symboli-
cregression.jl,” 2023.
[32] W. Tenachi, R. Ibata, and F. I. Diakogiannis, “Physical symbolic optimization,”
2023.
[33] Y. Matsubara, N. Chiba, R. Igarashi, and Y. Ushiku, “Rethinking symbolic regres-
sion datasets and benchmarks for scientific discovery,” 2024.
[34] N. Q. Uy, N. X. Hoai, M. O’Neill, R. I. McKay, and E. G. López, “Semantically-
based crossover in genetic programming: application to real-valued symbolic
regression,” Genetic Programming and Evolvable Machines, vol. 12, pp. 91–119,
2010.
[35] J. R. Koza, “Genetic programming as a means for programming computers by
natural selection,” Proceedings of the National Academy of Sciences, vol. 4, no. 2,
pp. 87–112, 1994.
[36] M. Keijzer, “Improving symbolic regression with interval arithmetic and linear
scaling,” in Genetic Programming (C. Ryan, T. Soule, M. Keijzer, E. Tsang, R. Poli,
and E. Costa, eds.), (Berlin, Heidelberg), pp. 70–82, Springer Berlin Heidelberg,
2003.
[37] E. Vladislavleva, G. Smits, and D. den Hertog, “Order of nonlinearity as a com-
plexity measure for models generated by symbolic regression via pareto genetic
programming,” IEEE Transactions on Evolutionary Computation , vol. 13, pp. 333–
349, 2009.
[38] M. F. Korns, Accuracy in Symbolic Regression, pp. 129–151. New York, NY: Springer
New York, 2011.
[39] K. Krawiec and T. Pawlak, “Approximating geometric crossover by semantic
backpropagation, ” in Proceedings of the 15th Annual Conference on Genetic and Evo-
lutionary Computation, GECCO ’13, (New York, NY, USA), p. 941–948, Association
for Computing Machinery, 2013.
[40] Y. Jin, W. Fu, J. Kang, J. Guo, and J. Guo, “Bayesian symbolic regression,” 2019.
6596