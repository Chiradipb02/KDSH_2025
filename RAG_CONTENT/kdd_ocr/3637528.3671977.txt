Fake News in Sheep’s Clothing: Robust Fake News Detection
Against LLM-Empowered Style Attacks
Jiaying Wu
National University of Singapore
Singapore
jiayingwu@u.nus.eduJiafeng Guo
University of Chinese Academy of
Sciences
Institute of Computing Technology,
CAS
Beijing, China
guojiafeng@ict.ac.cnBryan Hooi
National University of Singapore
Singapore
bhooi@comp.nus.edu.sg
ABSTRACT
It is commonly perceived that fake news and real news exhibit
distinct writing styles, such as the use of sensationalist versus ob-
jective language. However, we emphasize that style-related features
can also be exploited for style-based attacks. Notably, the advent
of powerful Large Language Models (LLMs) has empowered mali-
cious actors to mimic the style of trustworthy news sources, doing
so swiftly, cost-effectively, and at scale. Our analysis reveals that
LLM-camouflaged fake news content significantly undermines the
effectiveness of state-of-the-art text-based detectors (up to 38%
decrease in F1 Score), implying a severe vulnerability to stylistic
variations. To address this, we introduce SheepDog, a style-robust
fake news detector that prioritizes content over style in determin-
ing news veracity. SheepDog achieves this resilience through (1)
LLM-empowered news reframings that inject style diversity into the
training process by customizing articles to match different styles;
(2) a style-agnostic training scheme that ensures consistent veracity
predictions across style-diverse reframings; and (3) content-focused
veracity attributions that distill content-centric guidelines from
LLMs for debunking fake news, offering supplementary cues and
potential intepretability that assist veracity prediction. Extensive
experiments on three real-world benchmarks demonstrate Sheep-
Dog’s style robustness and adaptability to various backbones.1
CCS CONCEPTS
•Information systems →Data mining; •Computing method-
ologies→Natural language processing.
KEYWORDS
Fake News; Large Language Models; Adversarial Robustness
ACM Reference Format:
Jiaying Wu, Jiafeng Guo, and Bryan Hooi. 2024. Fake News in Sheep’s
Clothing: Robust Fake News Detection Against LLM-Empowered Style
Attacks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain.
ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671977
1Data and code are available at: https://github.com/jiayingwu19/SheepDog.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671977
A 33-year-old father from the U.K. is completely cancer-free, but not because of chemotherapy or radiation. … he successfully eliminated this cancer on his own by taking therapeutic doses of cannabis oil …In a remarkable turn of events, a 33-year-old father from the United Kingdom has defied medical expectations and overcome terminal bowel cancer without the use of chemotherapy or radiation. … took matters into his own hands and found an unconventional solution to his dire situation: therapeutic doses of cannabis oil…Ground Truth: Fake News
LLM-Empowered Style Attacks“Fake”
“Real”
LLM“use the style of The New York Times”Detector
Detector
Figure 1: A motivating example of LLM-empowered style
attacks on text-based fake news detectors, where fake news
is camouflaged with the style of reliable news publishers.
1 INTRODUCTION
Psychological theories, such as the Undeutsch hypothesis [ 3], sug-
gest that genuine and fake statements exhibit distinct linguistic
styles. Indeed, reputable news sources uphold journalistic integrity,
emphasize accuracy and fact-checking, and maintain a balanced
tone [ 5]. In contrast, unreliable outlets often resort to sensation-
alism, lack credible sources, and may exhibit partisan biases [ 19].
Building upon these stylistic differences, recent advances in auto-
mated fake news detection have incorporated sentiment features
[1,64] to enhance the detector, and highlighted the significance of
styles in discerning between hyperpartisan news and well-balanced
mainstream reporting [44].
While style-related features serve as key indicators in identify-
ing fake news, they also offer a direct avenue for malicious users
to conduct style-based attacks. This problem is exacerbated by
the advent of powerful Large Language Models (LLMs) [ 7,37,39],
whose unprecedented capabilities for reasoning and generative
tasks [ 55,66] bridges the gap between machine-generated and
human-written news. Consequently, malicious actors now possess
the capability to mimic the style of reputable news sources, in an
attempt to evade automated detection. As shown in Figure 1, using
3367
KDD ’24, August 25–29, 2024, Barcelona, Spain Jiaying Wu, Jiafeng Guo, and Bryan Hooi
a style-oriented prompt (i.e., “style of The New York Times”), LLM-
camouflaged fake news successfully bypasses a RoBERTa [ 30]-based
fake news detector.
The impact of stylistic variations on fake news detectors has
received limited attention, despite prior investigations into attacks
regarding social engagements [ 54], word-level perturbations [ 27],
and machine-generated malicious comments [ 29]. To assess the
robustness of text-based fake news detectors, we introduce a series
of style-based attacks, specifically by tailoring news articles to ad-
versarial writing styles (detailed in Section 4.1). Our experiments,
as summarized in Table 1, reveal a significant performance degra-
dation in state-of-the-art text-based detectors, with some suffering
up to a 38% decline in F1 Score. Existing detectors, well-fitted to
real news from trustworthy sources and fake news from unreli-
able sources, struggle to adapt to real-world scenarios where news
content is presented in diverse styles.
To address the style-related vulnerability of text-based detectors,
we introduce SheepDog, a style-robust approach that consistently
recognizes trustworthy content and identifies deceptive content,
even when concealed within the LLM-empowered “sheep’s cloth-
ing”. Built upon a pretrained language model (LM) backbone, which
can be fully fine-tuned to capture task-specific salient features, and
leveraging the strong zero-shot reasoning and generative capabili-
ties of LLMs, SheepDog effectively combines the strengths of both
(fine-tuned LM: specialized; LLM: versatile).
The style-agnostic nature of SheepDog stems from its utilization
ofLLM-empowered news reframings. To accommodate the wide
spectrum of writing styles presented in real-world news articles, we
harness the impressive capabilities of LLMs in adhering to complex
real-world instructions [ 17]. By doing so, we can reframe training
articles into style-diverse expressions while preserving the integrity
of their content. Subsequently, through style-agnostic training , we
aim to ensure consistency in the model’s veracity predictions across
each news article and its style-diverse reframings. This training
scheme encourages SheepDog to discount style-related features,
enabling it to focus on capturing style-agnostic veracity signals
from the news content.
To reinforce the emphasis of our approach on news content over
style, we propose to incorporate content-focused veracity attribu-
tions from an LLM to inform veracity predictions. Leveraging the
extensive world knowledge and reasoning capabilities within LLMs
[4,33], we elicit explanatory outputs from LLMs regarding the ve-
racity of news articles, with reference to a set of content-centric
fake news debunking rationales specifically related to news content
(detailed in Section 5.3). By converting these rationales into precise
pseudo-labels, we introduce additional weak supervision that steers
SheepDog towards learning robust, style-agnostic news representa-
tions. Our approach leverages these attribution-level predictions
not only to facilitate style robustness but also to potentially offer
explainability into the veracity of news articles.
Our key contributions are summarized as follows:
•Empirical Finding: We present a novel finding on the style-
related vulnerability of state-of-the-art text-based fake news de-
tectors to LLM-empowered style attacks.
•LLM-Empowered Style Robustness: We introduce SheepDog,
a style-agnostic fake news detector that achieves robustnessthrough style-agnostic training and content-focused veracity
attribution prediction, synergized within a multi-task learning
paradigm.
•Effectiveness: Extensive experiments demonstrate that Sheep-
Dog achieves significantly superior style robustness across multi-
ple style-based adversarial settings, and yields consistent perfor-
mance gains when combined with representative LM and LLM
backbones.
2 RELATED WORK
Fake News Detection. Automated fake news detection has been ex-
plored using a wide range of neural architectures [ 42,49,67]. Apart
from extracting lexical [ 45] and sentiment features [ 44] within the
news article text, many methods incorporate auxiliary features
to supplement veracity prediction, including user comments [ 49],
news environments [ 48], knowledge bases [ 10,13], temporal pat-
terns from users [ 46], and social graphs [ 36,59,60]. Recent studies
also seek to address challenges including temporal shift [ 22], en-
tity bias [ 69] and domain shift [ 34,35,70] in fake news detection
scenarios. In this work, we adopt a text-based perspective, specifi-
cally focusing on enhancing the robustness of fake news detectors
against stylistic variations.
Adversarial Attack on Fake News Detectors. Investigating the
vulnerabilities of fake news detectors is central to improving their
real-world applicability. Hence, existing efforts [ 15,20,27,32,54,68]
have studied the impact of different attacks from multiple aspects,
including manipulation of social engagements [ 32,54] and user
behavior [ 15], fact distortion [ 27], subject-object exchange [ 68], and
blocking of data availability [ 20]. However, the impact of writing
styles remains underexplored. To bridge this gap, we investigate
the resilience of text-based detectors against LLM-empowered style
attacks, and propose a style-agnostic solution.
LLM Capabilities and Misinformation. LLMs [ 37,38,52] have
demonstrated remarkable reasoning capabilities that even match or
surpass human performance in certain scenarios [ 55,66]. However,
the impressive strengths of LLMs have also attracted increasing
attention towards LLM-generated misinformation [ 28]. Recent in-
vestigations have found that LLMs can act as high-quality misin-
formation generators [ 25,31,41,62], and that LLM-generated mis-
information is generally harder to detect [ 8,9]. On a related front,
recent work explore the role of LLMs as fact-checkers [ 14,40,65]
and fake news detectors [ 9,43], and leverage the commonsense rea-
soning capabilities to elicit supplementary explanations from LLMs
[4,18,21,33] that facilitate a wide range of tasks. Although we
also instruct an LLM to generate style-related adversarial articles,
our goal is to simulate real-world scenarios where news are pre-
sented in diverse styles. Additionally, instead of leveraging LLMs
to make veracity judgments that distinguish false information from
the truth [ 9,21,31,43], in this work, we investigate the role of
LLMs in enhancing the style robustness of text-based fake news
detectors, specifically through injecting style-diverse reframings
and content-centric cues into the training process.
3 PROBLEM DEFINITION
LetDbe a news dataset consisting of 𝑁questionable news pieces,
denoted as 𝑝1, 𝑝2, . . . , 𝑝𝑁. Among the news pieces, P𝐿⊂D is a
3368Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks KDD ’24, August 25–29, 2024, Barcelona, Spain
set of labeled news articles. Each news article in P𝐿is assigned a
ground-truth veracity label 𝑦. In line with prior work [ 49,64,67],𝑦
is a binary label that represents either real news or fake news.
As we focus on style-related issues, we consider a text-based
setting. Formally, the problem can be defined as follows:
Problem 1 (Text-Based Fake News Detection). Given a news
datasetDwith training labels Y𝐿, the goal is to predict the veracity
labels of unlabeled news pieces P𝑈=D\P𝐿.
4 LLM-EMPOWERED STYLE ATTACKS
In this section, we establish a series of LLM-empowered style at-
tacks, and conduct preliminary analysis to assess the robustness of
state-of-the-art text-based fake news detectors.
4.1 Attack Formulation
The impressive capabilities of LLMs [ 7,37,39] enable malicious
users to disguise fake news with restyling prompts, resulting in
camouflaged articles that closely resemble reliable sources. In this
work, we explore a direct form of style-based attack utilizing news
publisher names (e.g., “CNN”). These names possess distinct styles
that can be readily adopted by producers of fake news, making
them a likely occurrence in real-world scenarios.
To simulate the adversarial situations where news articles are
restyled in relation to various publishers, we manipulate the styles
of both trustworthy and unreliable news. Specifically, among the
test samples, we utlize an LLM to rephrase real news in the style
of tabloids, and fake news in the style of mainstream sources. Our
general prompt format is shown as follows:
Rewrite the following article using the style of [publisher
name]: [news article]
Base donpublisher popularity ,intheplace of[publisher name],
weselect“National Enquir er"totransform realnews,and“CNN”
totransform fake news.These LLM-r estyle dtestarticles arethen
emplo yedtoevaluate theresilience ofadetectoragainst style-base d
attacks, asillustrate dinFigur e1.
4.2 Style-Relate dDete ctorVulnerability
Automate dfake newsdetection becomes increasingly difficult against
LLM-emp oweredstyle attacks. Inthissubse ction, weconduct pre-
liminar yanalysis onreal-w orld newarticles toevaluate theinflu-
ence ofwriting styles ontext-base ddetectors. Ouranalysis isbased
ontheFakeNe wsNet [50]benchmark (consisting ofPolitiFact and
GossipCop datasets) andtheLabeledUnreliable News(LUN) dataset
[45],with dataset descriptions relegate dtoSection 6.1.1 andTable2.
Specifically ,weinvestigate thefollowing question: Towhat extent
cantext-base dfake newsdetectors withstand LLM-emp owered
style attacks?
InTable 1,weexamine 13representativ etext-base ddetectors
under bothoriginal andadversarial settings (detaile dmetho dde-
scriptions areprovidedinSection 6.1.2). These detectors encompass
threecategories: (1)text-base dfake newsdetectors with diversetask-
specificarchite ctures,including RecurrentNeural Netw orks (RNNs)
[49],Conv olutional Neural Netw orks (CNNs) [67],Graph NeuralTable 1:Under LLM-emp oweredstyle attacks, existing text-
basedfake newsdetectors suffer severeperformance deterio-
ration interms ofF1Score(%).(O:original; A(↓):gapbetween
original unperturb edperformance andadversarial perfor-
mance onthetestsetformulate dinSection 4.1).
Metho
dPolitiFact GossipCop LUN
O
A(↓) O A(↓) O A(↓)
dEFEND\c
[49] 82.59 12.15 70.74 4.34 80.92 19.16
SAFE\v [67] 79.85 8.74 70.64 2.93 79.46 13.12
SentGCN [53] 80.77 13.82 69.29 5.59 79.66 16.65
DualEmo [64] 87.76 15.34 75.36 5.89 81.52 24.97
BERT
[12] 84.99 12.68 74.50 5.52 80.96 24.61
RoBERT a[30] 87.40 11.23 74.05 3.05 82.12 29.65
DeBERT a[16] 86.30 11.73 73.80 2.85 83.67 30.34
UDA[61] 87.74 10.14 74.22 4.54 82.94 20.71
PET [47] 85.51 11.02 74.63 3.08 83.66 31.08
KPT [23] 87.70 13.26 74.23 2.63 84.06 31.83
GPT
-3.5[37] 69.61 27.48 56.30 16.71 79.97 20.34
InstructGPT [39] 64.59 20.69 50.38 9.13 68.16 11.39
LLaMA2-13B [52] 63.15 29.91 53.54 27.75 70.97 38.33
Netw orks (GNNs) applie dtodocument graphs [53],andTrans-
formers [64];(2)Fine-tune dLMs [12,16,23,30,47,61]onthefake
newsdetection benchmark datasets; and(3)LLMs [37,39,52]with
zero-shot prompting. Few-shot andfine-tune dLLM experiments
arerelegate dtoAppendix A.
Weevaluate therobustness ofdetectors against LLM-emp owered
style attack basedontheir performance under theadversarial set-
ting outline dinSection 4.1.Our empirical results inTable 1and
Appendix Ayield thefollowing twoimplications:
Observ ation 1(Style-Rela tedVulnerability ofFake News
Detect ors). State-of-the-art text-base dfake newsdetectors aresus-
ceptible toLLM-emp oweredstyle attacks. This susceptibility results
insubstantial performance degradation,with anF1Scoredecline of
upto38.3% ontheadversarial testset.
Observ ation 2(Insuffic iency ofLLMs asFake News De-
tect ors). LLMs, despite their impr essiv ezero-shot capabilities as
general-purp osefoundation models, exhibit inferior detection perfor-
mance compar edtotext-base dfake newsdetectors andpre-traine d
LMs fine-tune dspecifically forfake newsdetection.
Ourtwofindings suggest afundamental limitation oftext-base d
fake newsdetectors inachie ving robust veracity predictions against
stylistic variations. Dete ctors overlyinfluence dbystyles struggle
toreliably differ entiate betweenrealandfake news,andeventhe
powerful LLMs may proveinade quate forthespecific demands of
fake newsdetection. Inthedynamic digital landscap e,thestyles of
newsarticles evolverapidly ,while theaccessibility formalicious
users tomanipulate style using LLMs exacerbates these variations.
Ther efore,foreffectivedeplo yment, afake newsdetector must
prioritize theassessment ofnewscontent overstyle .This objective
motivates oursubse quent innovations towardastyle-agnostic fake
newsdetection approach.
3369KDD ’24, August 25–29, 2024, Barcelona, Spain Jiaying Wu, Jiafeng Guo, and Bryan Hooi
Fake News Detection Lossℒ𝐧𝐞𝐰𝐬BREAKING:ObamasDaughterBUSTEDByTheCopsForDoingThisAttheDemocraticNationalConvention,MichelleObamatriedtouseherdaughtersasshiningexamples…MaliaObama'sRecentActivitiesRaiseConcernsRecently,reportshaveemergedregardingthebehaviorofMaliaObama,daughterofformerPresidentBarackObamaandMichelleObama.AttheDemocraticNationalConvention…
SHOCKING:MaliaObamaCAUGHTintheActatDemocraticNationalConvention!BraceyourselfaswerevealthescandaloustruthbehindMichelleObama'sfailedattempttoshowcaseherdaughtersasrolemodelsforAmerica…LMUnreliable-Style Reframing 𝒑𝑭(e.g., “sensational”)Reliable-Style Reframing  𝒑𝑹(e.g., “objective”)
LLMNews Article 𝒑(Fake)!𝐲"𝐲𝑹"𝐲𝑭“Fake”VeracityPredictor
AttributionPredictor!𝐬"𝐬𝑹"𝐬𝑭Style Alignment Loss 𝓛𝐬𝐭𝐲𝐥𝐞
Veracity Attribution Loss 𝓛𝐚𝐭𝐭𝐫011……“false or misleading information””biased opinion”Content-FocusedVeracity Attributionsfrom LLMs011……011……
011……𝐬𝑹
𝐬𝑭𝐬
LLM“…”“…”
Figure 2: Overview of the proposed SheepDog framework for style-agnostic fake news detection.
5 PROPOSED APPROACH
Building upon our empirical findings on the style-related vulnera-
bility of text-based fake news detectors, we introduce SheepDog,
a style-agnostic detector that reliably assesses news veracity. As
overviewed in Figure 2, SheepDog obtains style robustness from
two core objectives within a multi-task learning paradigm: (1)style-
agnostic training, which flexibly adapts an LM to the fake news
classification task, while ensuring consistent veracity predictions
across a diverse array of LLM-empowered news reframings; and (2)
content-focused veracity attribution prediction, which lever-
ages veracity-related insights from LLMs to inform model pre-
dictions. To enhance usability and composability, we design our
method to be simple and modular, allowing it to be integrated with
any LM and LLM backbone.
5.1 LLM-Empowered News Reframing
As suggested by our Observation 1, text-based fake news detectors
fitted to style-consistent real and fake articles exhibit limited adapt-
ability against stylistic variations. To overcome this limitation, our
key idea is to inject style diversity into the training stage through
a process we term reframing, where each news article is presented
in various styles.
SheepDog’s reframing strategy is driven by two sub-goals: (1)en-
compassing a wide range of styles and (2)maintaining the integrity
of the original news content. LLMs, capable of following complex
real-world instructions [ 17], inherently meet both criteria. This is
further validated by our analysis in Appendix C, where LLMs gener-
ally prove effective in transforming the tone of news articles while
preserving content consistency. Hence, for each training article,
we generate a series of prompts, each comprising the news article
and a style-oriented reframing instruction. The general structure
of the prompt is as follows, with a detailed example presented in
Appendix B.4:Rewrite the following article in a / an [specified] tone:
[news article]
Togenerate newsexpressions that simulate bothreliable and
unreliable sources,weestablish asetoffour general style-oriente d
adjectivesfortheprompt: “objectiveandprofessional” and“neu-
tral” toemulate reliable sources,and“emotionally triggering” and
“sensational” forunreliable sources.During thetraining stage ,for
labelednewsarticle 𝑝∈P𝐿,werandomly selectonereliable-style
reframing prompt andoneunreliable-style reframing prompt to
generate diverse expressions. Through quer ying theLLM, weob-
tain twocorresponding reframings: onereliable-style reframing
denote das𝑝𝑅,andoneunreliable-style reframing denote das𝑝𝐹.
5.2 Style- Agnostic Training
Astyle-r obust fake newsdetector must becapable ofdiscerning
theveracity ofnewsarticles basedontheir content, without being
influence dbystylistic featur es.Tothisend, weintroduce astyle
alignment objectivethatensur esclose alignment among theveracity
predictions ofnewsarticle 𝑝,itsreliable-style reframing 𝑝𝑅,andits
unreliable-style reframing 𝑝𝐹.This objectiveisderiv edasfollows.
LetMbeapre-traine dLanguage Model(LM) such asRoBERT a
[30].Emplo ying anLMasthebackb oneofthedetectoroffers advan-
tages, asLMs canbereadily fine-tune dforthefake newsdetection
task, which enables them toeffectivelyextract salient task-sp ecific
featur esfromthenewscontent. ThroughM,basedon𝑝,𝑝𝑅and
𝑝𝐹,weacquir earticle representations h∈R𝑑,andreframing repre-
sentations h𝑅,h𝐹∈R𝑑:
h𝑝=M(𝑝),h𝑅=M(𝑝𝑅),h𝐹=M(𝑝𝐹). (1)
Subse quently ,weapply aMulti-Lay erPerceptr on(MLP) to𝑝,𝑝𝑅
and𝑝𝐹toobtain corresponding veracity predictions ˜y,˜y𝑅,˜y𝐹∈R2:
3370Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks KDD ’24, August 25–29, 2024, Barcelona, Spain
˜y=MLP𝑝𝑟𝑒𝑑(h),˜y𝑅=MLP𝑝𝑟𝑒𝑑(h𝑅),˜y𝐹=MLP𝑝𝑟𝑒𝑑(h𝐹).
(2)
Each of ˜y,˜y𝑅,˜y𝐹contain two logits that correspond to the real and
fake classes, respectively.
Despite differences in style, the fundamental news content re-
mains consistent across the original news article 𝑝and its refram-
ings 𝑝𝑅and𝑝𝐹. Ideally, 𝑝𝑅and𝑝𝐹should yield the same veracity
prediction as 𝑝. To this end, we formulate the following style align-
ment loss defined as:
Lstyle=MEAN(L1(˜y𝑅,˜y),L1(˜y𝐹,˜y)), (3)
whereL1represents the The Kullback-Leibler (KL) divergence loss.
While aligning the predictions of 𝑝𝑅and𝑝𝐹with 𝑝, it is crucial
to ensure accurate veracity prediction for 𝑝. Therefore, we also
incorporate a fake news detection loss:
Lnews=L2(˜y,𝑦), (4)
whereL2represents the standard cross entropy (CE) loss.
5.3 Content-Focused Veracity Attributions
In addition to tuning the LM backbone with the style alignment
objective, which discounts style-related features and encourages
style-robust predictions, we further propose to integrate auxiliary
veracity-related knowledge and reasoning to inform veracity pre-
dictions. To achieve this goal, leveraging the impressive zero-shot
reasoning capabilities of general-purpose LLMs [4, 33] serves as a
promising solution.
Specifically, we elicit content-focused veracity attributions from
an LLM, which provides explanatory outputs on why each fake
news article in the training set is flagged as fake. Our prompt
consists of a fake news article and a predefined set Cof content-
oriented rationales for debunking fake news (e.g., “lack of credible
sources” and “false or misleading information”; detailed rationales
are described in Appendix B.3). This prompt efficiently leverages
the LLM’s reasoning capabilities and prior knowledge to identify
characteristics associated with fake news, referencing the rationales
inC. The general prompt format is as follows:
Article: [fake news article]
Question: [given a list of content-centric rationales for
debunking fake news, ask the LLM to identify rationales
fulfilled by the fake news article]
Uponquerying theLLM, weobtain alistofveracity attributions
basedontheinput article .These attributions arethen conv erted
into|C|-dimensional pseudo-lab els,wher eeach rationale inCis
represente dbyadistinct binar ylabel.Foragivennewsarticle
𝑝∈P𝐿,thisprocess yields pseudo-lab elss∈R|C|,which con-
tains supplementar yveracity-r elate dinformation. Similarly ,for
reframings 𝑝𝑅and𝑝𝐹,weobtain pseudo-lab elss𝑅,s𝐹∈R|C|,re-
spectively.Notably ,sinceCfocuses solely onfake newsindicators,
thepseudo-lab elsforrealnewsanditsreframings areuniformly
settoallzeros.
Todistill theveracity-informe dknowledgefromthese attribu-
tions, weintroduce amulti-lab elattribution prediction objective.Table 2:Dataset statistics.
Dataset PolitiFact GossipCop LUN
# News Articles 450 7,916 7,500
#Real News 225 3,958 3,750
#Fake News 225 3,958 3,750
This enriches ourframe workwith additional content-cen tricguid-
ance,andoffers potential explanability forarticles identifie dasfake
during theinfer ence stage (exemplifie dinFigur e3).
AsshowninEq.1,welearn newsrepresentations h,h𝑅,h𝐹∈R𝑑
fornewsarticle 𝑝anditsreframings 𝑝𝑅and𝑝𝐹,respectively,using
apre-traine dLMM.Then, theattribution-le velprediction scores
˜s,˜s𝑅,˜s𝐹∈R|C|are computed through another MLP:
˜s=MLP𝑎𝑡𝑡𝑟(h),˜s𝑅=MLP𝑎𝑡𝑡𝑟(h𝑅),˜s𝐹=MLP𝑎𝑡𝑡𝑟(h𝐹).(5)
Theveracity attribution loss is then defined as:
Lattr=MEAN(L3(ˆs,s),L3(ˆs𝑅,s𝑅),L3(ˆs𝐹,s𝐹)), (6)
wher eL3represents thebinar ycrossentropy(BCE) loss, andˆs
denotes thesigmoid-transforme dscoresin˜scorresponding toeach
rationale .
5.4 Final Obje ctiveFunction ofSheepDog
Byenfor cing consistency among style-div ersenewsreframings and
exploiting thecontent-fo cusedattributions fromtheLLM, thefinal
objectivefunction ofSheepDog isdefine dasalinear combination
ofthethestyle alignment loss(Eq.3),thenewsclassification loss
(Eq.4),andtheveracity attribution loss(Eq.6):
L=Lstyle+Lnews+Lattr. (7)
SheepDog isdesigne dasanend-to-end frame work,wher ethe
style-agnostic newsveracity predictor andthecontent-fo cused
attribution predictor aretraine dsimultane ously .
6EXPERIMEN TS
Inthissection, weempirically evaluate SheepDog toinvestigate
thefollowing sixresear chquestions:
•Robustness Against Style Attacks (Section 6.2): Howrobust
isSheepDog against LLM-emp oweredstyle attacks?
•Effectiveness onUnperturb edArticles (Section 6.3): How
effectivelycanSheepDog identify fake newswithin theoriginal
unperturb edtestarticles?
•Adaptability toDiffer entBackb ones (Section 6.4): Howwell
doesSheepDog perform when combine dwith differ entLMand
LLM backb ones?
•Ablation Study (Section 6.5): What aretherespectiveroles
ofstyle-agnostic training andcontent-fo cusedattributions on
SheepDog’s style robustness?
•Stability AcrossReframing Prompts (Section 6.6):DoesSheep-
Dog yield consistent impr ovements acrossdiverse setsofnews
reframing prompts?
•Case Study (Section 6.7): Howcanweinterpr etSheepDog’s
rationale fordebunking fake newsthrough itspredictions on
content-fo cusedveracity attributions?
3371KDD ’24, August 25–29, 2024, Barcelona, Spain Jiaying Wu, Jiafeng Guo, and Bryan Hooi
Table 3: SheepDog significantly outperforms competitive baselines on four adversarial test settings under LLM-empowered
style attacks (formulated in Section 4.1), in terms of F1 Score (%) . Bold (underlined) values indicate the best overall (baseline)
performance. Statistical significance over the most competitive baselines, computed using the Wilcoxon signed-rank test [ 56],
is indicated with∗(𝑝<.01). (G1: text-based fake news detectors; G2: LMs fine-tuned to the fake news detection task; G3: LLMs)
Metho
dPolitiFact GossipCop LUN
A
B C D A B C D A B C D
G1dEFEND\c
70.44 69.77 73.67 72.98 66.40 66.55 68.93 69.07 61.76 62.28 72.95 72.50
SAFE\v 71.11 70.80 75.55 75.24 67.71 67.05 68.31 67.65 66.34 67.08 72.40
73.16
SentGCN 66.95 62.50 69.54 65.08 63.70 63.07 63.61 63.01 63.01 62.50 76.11 75.56
DualEmo
72.42 71.23 77.07 75.80 69.47 68.50 71.69 70.71 56.55 54.78 68.53 66.80
G2BERT
72.31 71.37 77.23 76.24 68.98 68.17 71.95 71.11 56.35 54.61 68.50 66.74
RoBERTa 76.17 74.95 78.28 77.05 71.00 70.47 72.56 72.02 52.47 53.62 68.31 69.46
DeBERTa 74.57 74.36 80.60 80.35 70.95
71.15 72.51
72.71 53.33 55.45 67.16 69.27
UDA 77.60 75.57 79.21
77.17 69.68 69.33 72.16 71.80 62.23 61.80 68.25 67.80
PET 74.49 70.75 75.49 71.76 71.55 70.85 73.74 73.02
52.58 53.30 63.71 64.33
KPT 74.44 73.32 77.73 76.60 71.60 71.01
73.69 73.10 52.23
53.62 65.71 67.15
G3GPT
-3.5 42.13 43.44 56.61 58.17 39.59 38.67 48.44 47.38 59.63 61.24 65.74 67.43
InstructGPT 43.90 43.90 54.21 54.21 41.25 40.18 44.26 43.12 56.77 57.15 58.93 59.32
LLaMA2-13B 33.24 34.48 53.64 55.45 25.79 26.06 37.07 37.40 32.64 33.00 50.81 51.33
Ours She
epDog 80.99∗79.89∗82.36∗81.24 74.45∗74.38∗75.95∗75.88∗85.63∗86.06∗87.89∗88.32∗
Table 4: Notations and setup for the four style-based adver-
sarial test sets in Section 6.2, denoted as A through D.
[publisher name] CNN The New York Times
National Enquirer A B
The Sun C D
6.1 Experimental Setup
6.1.1 Datasets. We evaluate our approach on three widely-used
real-world benchmark datasets: the FakeNewsNet public bench-
mark [ 50], which consists of the PolitiFact andGossipCop datasets,
and the Labeled Unreliable News (LUN) dataset [ 45]. Table 2 de-
scribes the dataset statistics. PolitiFact and LUN center on political
discourse, while GossipCop focuses on celebrity gossip. For the
LUN dataset, which further classifies unreliable news into three
sub-categories: satire, hoax, and propaganda, we conduct binary
classification between reliable (real) and unreliable (fake) news,
and ensure an equal number of unreliable news from each of these
fine-grained categories. To better simulate real-world scenarios, we
follow prior work [ 59] and adopt temporal data splitting on Politi-
Fact and GossipCop, where temporal information is available. The
most recent 20% real and fake news articles constitute the test set,
and the remaining 80% articles posted earlier serve as the training
set. We adopt random 80/20 training / test splits on LUN.
6.1.2 Baselines. We benchmark SheepDog against thirteen repre-
sentative baseline methods, which can be categorized as:
Text-based fake news detectors (G1) employ neural archi-
tectures tailored specifically for the fake news detection task. dE-
FEND\c is a variant of dEFEND [ 49] based on the news article textthat adopts RNN-based hierarchical co-attention. SAFE\v is a text-
based variant of SAFE [ 67] that leverages a CNN-based architecture
to learn semantic features. SentGCN [53] encodes veracity-related
sentence interaction patterns within each article using a GNN, and
DualEmo [64] incorporates emotion features from news publishers
and news comments. As our SheepDog approach does not involve
user comments, we implement DualEmo on a BERT-base [ 12] back-
bone with publisher emotion features for a fair comparison.
Fine-tuned LMs (G2) adapts pre-trained LMs to the fake news
detection task, and has proven effective in handling misinformation
scenarios [ 42]. In addition to three widely-recognized LMs, namely
BERT [12],RoBERTa [30], and DeBERTa [16], we include UDA
[61], a representative BERT-based model that employs diverse text
augmentations to yield consistent model predictions against input
noise. We also select two methods under the popular prompting par-
adigm: PET [47], which converts textual inputs into cloze questions
that contain a task description; and KPT, [ 24] which expands the
label word space with varied class-related tokens. For a fair compar-
ison, as our proposed approach does not involve unlabeled articles,
we implement UDA using consistency training on the supervised
training data, and exclude the self-training and PLM ensemble com-
ponents for PET. All methods in this category are implemented
with base version LMs, in line with our approach.
LLMs (G3) conduct zero-shot veracity prediction. We select
three representative baseline LLMs: GPT-3.5 [37],InstructGPT
[39], and LLaMA2-13B [52], detailed in Appendix B.1.
6.1.3 Implementation Details. We implement SheepDog and its
variants based on PyTorch 1.10.0 with CUDA 11.1. We utilize pre-
trained RoBERTa-base weights from HuggingFace Transformers
4.13.0 [ 58]. The LM backbone for SheepDog was configured with a
maximum sequence length of 512, a batch size of 4, and a learning
rate of 2×10−5. We prompt GPT-3.5 to generate news reframings,
3372Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks KDD ’24, August 25–29, 2024, Barcelona, Spain
content-focused veracity attributions, and adversarial test articles.
(detailed prompting descriptions and examples are presented in
Appendix B.2 and B.3). For SheepDog’s attribution prediction and
veracity prediction, we employ two MLPs, each with a single layer
(we also implement a variant with 2-layer MLPs in Section 6.5). Our
model is fine-tuned for 5epochs. For the implementation of base-
line methods, we adhere to the architectures and hyperparameters
recommended by their respective authors.
We evaluate model performance using Accuracy (%) and macro-
F1 Score (%). For all experiments except those involving LLMs, we
report averaged metrics over 10 runs of each method to provide a
comprehensive evaluation. In the case of LLM zero-shot predictions,
we employ greedy decoding and conduct each experiment once.
6.2 Robustness Against Style Attacks
We establish a series of LLM-empowered style attacks to assess
SheepDog’s robustness. Following our prompt template formulated
in Section 4.1, in the place of [publisher name], we select “National
Enquirer” and “The Sun” to camouflage real news, and “CNN” and
“The New York Times” for fake news, according to publisher popu-
larity. This yields 2×2=4distinct adversarial test sets, labeled as A
through D in Table 4. Note that we report the results of SheepDog
on adversarial set A in all other subsections, unless otherwise
specified.
Table 3 compares the performance of SheepDog with competi-
tive baselines across adversarial test sets A through D under LLM-
empowered style attacks. We can observe that: (1)All baseline
methods are highly susceptible to LLM-empowered style attacks.
This vulnerability suggests that existing methods exhibit a tendency
towards over-fitting on style-related attributes. (2)UDA, which
leverages back-translation to generate diverse text augmentations,
consistently demonstrates higher robustness compared to its BERT
backbone. This suggests the efficacy of incorporating augmenta-
tions. However, UDA still struggles to fully adapt to significant
stylistic variances in the input articles. This limitation may be at-
tributed to the fact that augmentations through back-translation
alone cannot provide sufficient variance. (3)On the challenging
adversarial test sets of LUN, CNN-based SAFE\v and GNN-based
SentGCN are more robust than LM-based baselines, which suggests
that LMs can be more prone to overfit to style-related features. (4)
SheepDog outperforms the most competitive baseline by significant
margins. Across the three benchmarks, this improvement averages
to2.59%, 2.77%, and 15.70% across the four adversarial test sets, in
terms of F1 score. The significantly greater improvements on LUN
might be attributed to dataset-specific stylistic attributes, toward
which we present a detailed discussion in Section 6.8.
6.3 Effectiveness on Unperturbed Articles
A desirable fake news detector should achieve style robustness
under adversarial settings without compromising its effectiveness
under the unperturbed setting. Our empirical results, presented in
Table 5, demonstrate that SheepDog excels in this regard. When
tested on the original, unaltered articles, SheepDog consistently
matches (on PolitiFact and GossipCop) or surpasses (on LUN) the
performance of the most competitive baseline, in terms of both
accuracy and F1 score. Similar to our observation (4)in Section 6.2,Table 5: SheepDog achieves performance (%) that is compara-
ble or superior to competitive baselines on the unperturbed
original test sets. Bold (underlined) values indicate the best
overall (baseline) performance, and∗indicates 𝑝<.01using
the Wilcoxon signed-rank test [56].
Metho
dPolitiFact GossipCop LUN
A
cc. F1 Acc. F1 Acc. F1
dEFEND\c
82.67 82.59 70.85 70.74 81.33 80.92
SAFE\v 79.89 79.85 70.71 70.64 79.93 79.46
SentGCN 81.11 80.77 69.38 69.29 80.07 79.66
DualEmo 87.78 87.7675.51 75.36 81.78
81.52
BERT
85.22 84.99 74.60 74.50 81.13 80.96
RoBERTa 88.00 87.40
74.14 74.05 82.53 82.12
DeBERTa 86.33 86.30 73.86 73.80 84.01 83.67
UDA 87.77 87.74 74.28 74.22 83.02 82.94
PET 85.56 85.51 74.75 74.63 84.00 83.66
KPT 87.78 87.70 74.38 74.23 84.40 84.06
GPT
-3.5 71.11 69.61 61.49 56.30 80.67 79.97
InstructGPT 67.78 64.59 58.33 50.38 70.87 68.16
LLaMA2-13B 65.56 63.15 55.74 53.54 72.47 70.97
She
epDog 88.44 88.39 75.77 75.75 93.05∗93.04∗
Table 6: On different LM backbones, SheepDog demonstrates
stable and significant improvements (in F1 %). Statistical sig-
nificance over the respective LM backbone is computed using
the Wilcoxon signed-rank test [56], denoted by∗(𝑝<.01).
Method PolitiFact GossipCop LUN
RoBERTa 76.17 71.00 52.47
SheepDog-RoBERTa 80.99∗74.45∗85.63∗
BERT 72.31 68.98 53.97
SheepDog-BERT 81.37∗73.54∗80.36∗
DeBERTa 74.57 70.95 53.33
SheepDog-DeBERTa 81.10∗73.89∗82.58∗
the significant performance gains on LUN might also stem from
dataset-specific stylistic features (detailed in Section 6.8).
6.4 Adaptability to LM / LLM Backbones
To assess the flexibility of SheepDog, we evaluate the performance
of SheepDog combined with three representative LMs: RoBERTa,
BERT and DeBERTa. We also evaluate RoBERTa-based SheepDog
combined with two representative LLMs: the closed-source GPT-3.5
and the open-source LLaMA2-13B.
As demonstrated in Table 6, SheepDog (1)substantially enhances
the performance of each respective LM backbone. Additionally, as
shown in Table 7, SheepDog (2)also achieves superior style ro-
bustness when utilizing both closed-source and open-source LLMs.
This adaptability highlights SheepDog’s style robustness from style-
invariant training and content-focused attribution prediction, im-
plying its practical utility in real-world scenarios where different
LMs and LLMs may be preferred or more readily available.
3373KDD ’24, August 25–29, 2024, Barcelona, Spain Jiaying Wu, Jiafeng Guo, and Bryan Hooi
Table 7: Leveraging closed-source and open-source LLM back-
bones, SheepDog demonstrates stable and significant im-
provements (in F1 %). Statistical significance over the fine-
tuned RoBERTa backbone is computed using the Wilcoxon
signed-rank test [56], denoted by∗(𝑝<.01).
Method PolitiFact GossipCop LUN
SheepDog 80.99∗74.45∗85.63∗
SheepDog-LLaMA2 80.82∗74.04∗81.87∗
RoBERTa 76.17 71.00 52.47
Table 8: Ablation of SheepDog demonstrates benefits of LLM-
empowered news reframing (denoted as R) and content-
focused veracity attributions (denoted as A) in F1 Score (%).
Method PolitiFact GossipCop LUN
SheepDog 80.99 74.45 85.63
w/ 2-layer MLP 79.83 74.03 84.75
- R 76.71 70.98 53.27
- A 80.73 73.74 84.83
RoBERTa 76.17 71.00 52.47
6.5 Ablation Study
To gain deeper insights into the functioning of SheepDog and the
role of its different components, we compare SheepDog with the
following three model variants:
•SheepDog w/ 2-layer MLP, which employs 2-layer MLPs with
hidden size of 64as attribution detector and veracity detector.
•SheepDog-R, which excludes style-diverse news reframings and
the style-agnostic training component.
•SheepDog-A, which excludes content-focused veracity attribu-
tions and the attribution prediction component.
Results in Table 8 suggest that: (1)SheepDog-R without news re-
framings only yields slight improvements over fine-tuned RoBERTa,
which suggests the key role of diverse reframings in SheepDog’s ro-
bustness. (2)While the improvement of SheepDog over SheepDog-
A may seem slight, incorporating veracity attributions assists in
guiding the model to prioritize content over style. Furthermore,
SheepDog’s attribution predictor equips the framework with ex-
planatory outputs during inference stage, facilitating easier human
verification in real-world scenarios (see Figure 3 and Section 6.7 for
a concrete illustration of this functionality). (3)SheepDog, utilizing
one single layer for veracity prediction and attribution predictions,
slightly outperforms the variant employing 2-layer MLPs. This
suggests that the expressiveness of LMs, harnessed through our
proposed objectives, yields article representations that contain rich
indicators related to both attributions and veracity.
6.6 Stability Across Reframing Prompts
As detailed in Section 5.1, we utilize an LLM to generate two types of
reframings for a given training article 𝑝: a reliable-style reframing
denoted as 𝑝𝑅, and an unreliable-style reframing denoted as 𝑝𝐹.
To assess SheepDog’s stability across different reframing prompts,Table 9: Across different sets of news reframing prompts,
SheepDog demonstrates stable and significant improvements
over the most competitive baseline (in F1 %).
Method PolitiFact GossipCop LUN
Baseline (Best) 77.60 71.60 66.34
SheepDog 80.99 74.45 85.63
w/ R1 79.02 74.22 77.42
w/ R2 79.93 74.16 86.18
w/ R3 80.36 73.55 76.77
w/ R4 79.71 74.01 85.55
A 33-year-old father from the U.K. is completely cancer-free, but not because of chemotherapy or radiation. … he successfully eliminated this cancer on his own by taking therapeutic doses of cannabis oil …
In a remarkable turn of events, a 33-year-old father from the United Kingdom has defied medical expectations and overcome terminal bowel cancer without the use of chemotherapy or radiation. … took matters into his own hands and found an unconventional solution to his dire situation: therapeutic doses of cannabis oil… Ground Truth: Fake News
LLM-Empowered Style Attacks“Fake”
“Fake”SheepDog
SheepDog
A 33-year-old father from the U.K. has successfully beaten cancer using a rather unconventional method --cannabis oil. … After undergoing unsuccessful chemotherapy, radiation, and surgery, <name> decided to take matters into his own hands … “Fake”SheepDog
“use the style of The New York Times”“use the style of CNN”
LLMTop predicted attribution: false or misleading information(probability: 0.985)Top predicted attribution: false or misleading information(probability: 0.491)Top predicted attribution: false or misleading information(probability: 0.784)
Figure 3: Across the original fake news article and its LLM-
camouflaged counterparts, SheepDog maintains consistency
and accuracy in both its veracity prediction and the top-
predicted veracity attribution for debunking fake news.
we examine its performance using four diverse combinations of
prompts, denoted as R1 through R4.
Recall that we adopt the following template for news reframing:
Rewrite the following article in a / an [specified] tone: [ 𝑝]
For
R1through R4,thespecifiedtones aredefine das:(𝑝𝑅/𝑝𝐹)
•R1:“objectiveandprofessional” /“emotionally triggering” .
•R2:“objectiveandprofessional” /“sensational” .
•R3:“neutral” /“emotionally triggering” .
•R4:“neutral” /“sensational” .
AsshowninTable9,SheepDog consistently demonstrates stable
andsignificant impr ovements overthemost comp etitiv ebaseline .
This validates thegeneralizability ofourapproach, suggesting its
potential ineffectivelycombating deceptiv einformation inthe
ever-evolving digital landscap e.Notably ,ourSheepDog approach
conducts sampling betweentworeliable-style reframings andtwo
unreliable-style reframings, leading tostronger versatility .
6.7 Case Study
Toillustrate SheepDog’s potential foroffering explanator youtputs
during modelinfer ence viaitsattribution predictions (detaile din
3374Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks KDD ’24, August 25–29, 2024, Barcelona, Spain
Section 5.3), we present a case study based on a fake news article
from the LUN test set. As shown in Figure 3, the article falsely claims
the effectiveness of cannabis oil in treating cancer, aiming to mis-
lead readers, despite contradicting established medical knowledge.
While the baseline RoBERTa detector correctly flags the original ar-
ticle as fake news, it misclassifies two style-transformed adversarial
articles as real news. In contrast, SheepDog accurately identifies the
original article as fake news, a prediction that remains consistent
for its two adversarial counterparts. Remarkably, leveraging the
softmax-converted probabilities from attribution-level prediction
scores (Eq. 5), SheepDog consistently identifies "false and mislead-
ing information" as the top-predicted attribution for debunking fake
news. This style robustness is invaluable for practitioners seeking
to comprehend the rationale behind each flagged fake news, aiding
in human verification and assessment of prediction reliability.
6.8 Discussion: Why is SheepDog Yielding
Greater Performance Gains on LUN?
SheepDog shows significantly greater performance improvements
on LUN compared to PolitiFact and GossipCop in both adversarial
(Section 6.2) and original unperturbed settings (Section 6.3). Specif-
ically, it achieves [A] significant improvements on the original
unperturbed LUN test set while maintaining performance compara-
ble to the best baseline on PolitiFact and GossipCop (Table 5); and
[B]notably greater improvements on LUN compared to PolitiFact
and GossipCop on style-based adversarial test sets (Table 3).
These phenomena can be attributed to the unique style-related
features of LUN, which include distinct writing styles of (1)indi-
vidual news publishers and and (2)different news types. (e.g., hoax).
Unlike PolitiFact and GossipCop, LUN’s publishers (i.e., news sites)
in the training and test sets do not overlap [ 45], creating an inherent
distribution shift between training and test data. Furthermore, as
described in Section 6.1.1, the fake news articles in LUN encompass
satire, hoax, and propaganda with distinctive writing styles. As a
result, fake news detectors trained on LUN are expected to be more
reliant on writing style.
Recall that our Observation 1 reveals the heavy reliance of ex-
isting text-based detectors on styles rather than news content for
veracity prediction. Trained on LUN, the baseline models become
overly reliant on styles specific to both publishers and news types,
which potentially explains [A] and [B]:
•On the original unperturbed LUN test set (Table 5), publisher-
specific style features used by baseline models fail to generalize
to test articles, as test articles are produced by news sites not
included in the training data. In contrast, SheepDog, being style-
agnostic, remains unaffected by changes in news publisher styles
and yields significant improvements.
•On the adversarial LUN test sets (Table 3), both publisher-specific
and news type-specific style features utilized by baselines fail
to generalize. Notably, LLM-empowered style attacks reverse
the styles of both reliable and unreliable news (Section 4.1).
These style variations make type-specific style features detri-
mental for veracity prediction. In contrast, SheepDog achieves
style robustness through LLM-empowered news reframings and
content-focused veracity attributions, thereby reliably detecting
fake news.7 CONCLUSION AND FUTURE WORK
In this paper, we address the critical aspect of style-related ro-
bustness in fake news detection. Motivated by our empirical find-
ing on the susceptibility of state-of-the-art text-based detectors to
LLM-empowered style attacks, we introduce SheepDog, a style-
agnostic fake news detector that emphasizes content veracity over
style. Jointly leveraging the strengths of task-specific LM backbones
and versatile general-purpose LLMs, SheepDog adopts a multi-task
learning paradigm, which integrates style-agnostic training and
content-focused veracity attribution prediction. Extensive experi-
ments on three real-world benchmarks demonstrate SheepDog’s
robustness and effectiveness across various style-based adversarial
settings, news reframing prompts, and representative backbones.
Moving forward, SheepDog lays a solid foundation for developing
more resilient and adaptable models in the ever-changing online
landscape, and demonstrates promising potential to be further ex-
tended to multi-modal scenarios.
8 ACKNOWLEDGEMENTS
This work was supported in part by the National Research Founda-
tion Singapore, NCS Pte. Ltd. and National University of Singapore
under the NUS-NCS Joint Laboratory (Grant A-0008542-00-00).
REFERENCES
[1]Oluwaseun Ajao, Deepayan Bhowmik, and Shahrzad Zargari. 2019. Sentiment
Aware Fake News Detection on Online Social Networks. In ICASSP. 2507–2511.
[2]Hunt Allcott and Matthew Gentzkow. 2017. Social Media and Fake News in the
2016 Election. Journal of Economic Perspectives 31, 2 (2017), 211–36.
[3]Bárbara G. Amado, Ramón Arce, and Francisca Fariña. 2015. Undeutsch hypoth-
esis and Criteria Based Content Analysis: A meta-analytic review. The European
Journal of Psychology Applied to Legal Context 7, 1 (2015), 3–12.
[4]Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024.
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.
InICLR.
[5]Philipp Bachmann, Mark Eisenegger, and Diana Ingenhoff. 2021. Defining and
Measuring News Media Quality: Comparing the Content Perspective and the
Audience Perspective. The International Journal of Press/Politics 27 (2021).
[6]Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Man-
ning. 2015. A large annotated corpus for learning natural language inference. In
EMNLP. 632–642.
[7]Tom Brown, Benjamin Mann, Nick Ryder, et al .2020. Language Models are
Few-Shot Learners. In NeurIPS, Vol. 33. 1877–1901.
[8]Canyu Chen and Kai Shu. 2023. Combating Misinformation in the Age of LLMs:
Opportunities and Challenges. arXiv preprint arXiv: 2311.05656 (2023).
[9]Canyu Chen and Kai Shu. 2024. Can LLM-Generated Misinformation Be De-
tected?. In ICLR.
[10] Limeng Cui, Haeseung Seo, Maryam Tabar, Fenglong Ma, Suhang Wang, and
Dongwon Lee. 2020. DETERRENT: Knowledge Guided Graph Attention Network
for Detecting Healthcare Misinformation. In KDD. 492–502.
[11] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.
QLoRA: Efficient Finetuning of Quantized LLMs. In NeurIPS.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
NAACL. 4171–4186.
[13] Yaqian Dun, Kefei Tu, Chen Chen, Chunyan Hou, and Xiaojie Yuan. 2021. KAN:
Knowledge-aware Attention Network for Fake News Detection. AAAI 35, 1
(2021), 81–89.
[14] Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, and Hao Peng.
2023. Language Models Hallucinate, but May Excel at Fact Verification.
arXiv:2310.14564 [cs.CL]
[15] Bing He, Mustaque Ahamad, and Srijan Kumar. 2021. PETGEN: Personalized Text
Generation Attack on Deep Sequence Embedding-Based Classification Models.
InKDD. 575–584.
[16] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTa:
Decoding-enhanced BERT with Disentangled Attention. In ICLR.
[17] Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe
Zhou, Lida Chen, Xintao Wang, Yuncheng Huang, Haoning Ye, Zihan Li, Shisong
Chen, Yikai Zhang, Zhouhong Gu, Jiaqing Liang, and Yanghua Xiao. 2024. Can
3375KDD ’24, August 25–29, 2024, Barcelona, Spain Jiaying Wu, Jiafeng Guo, and Bryan Hooi
Large Language Models Understand Real-World Complex Instructions?. In AAAI.
18188–18196.
[18] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and
Bryan Hooi. 2023. Harnessing Explanations: LLM-to-LM Interpreter for Enhanced
Text-Attributed Graph Representation Learning. arXiv:2305.19523 [cs.LG]
[19] Nolan Higdon. 2020. What is Fake News? A Foundational Question for Devel-
oping Effective Critical News Literacy Education. Democratic Communiqué 279
(2020). Issue 1.
[20] Benjamin D. Horne, Jeppe Nørregaard, and Sibel Adali. 2019. Robust Fake News
Detection Over Time and Attack. ACM Trans. Intell. Syst. Technol. 11, 1, Article 7
(2019).
[21] Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang, and Peng
Qi. 2024. Bad Actor, Good Advisor: Exploring the Role of Large Language Models
in Fake News Detection. In AAAI. 22105–22113.
[22] Beizhe Hu, Qiang Sheng, Juan Cao, Yongchun Zhu, Danding Wang, Zhengjia
Wang, and Zhiwei Jin. 2023. Learn over Past, Evolve for Future: Forecasting
Temporal Trends for Fake News Detection. In ACL. 116–125.
[23] Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li,
Wei Wu, and Maosong Sun. 2022. Knowledgeable Prompt-tuning: Incorporating
Knowledge into Prompt Verbalizer for Text Classification. In ACL. 2225–2240.
[24] Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li,
Wei Wu, and Maosong Sun. 2022. Knowledgeable Prompt-tuning: Incorporating
Knowledge into Prompt Verbalizer for Text Classification. In ACL. 2225–2240.
[25] Kung-Hsiang Huang, Kathleen McKeown, Preslav Nakov, Yejin Choi, and Heng
Ji. 2023. Faking Fake News for Real Fake News Detection: Propaganda-Loaded
Training Data Generation. In ACL. 14571–14589.
[26] Anna-Katharina Jung, Björn Ross, and Stefan Stieglitz. 2020. Caution: Rumors
ahead—A case study on the debunking of false information on Twitter. Big Data
& Society 7 (2020).
[27] Camille Koenders, Johannes Filla, Nicolai Schneider, and Vinicius Woloszyn. 2021.
How Vulnerable Are Automatic Fake News Detection Methods to Adversarial
Attacks? arXiv:2107.07970 [cs.CL]
[28] Sarah Kreps, R. Miles McCain, and Miles Brundage. 2022. All the News That’s
Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation. Journal
of Experimental Political Science 9, 1 (2022), 104–117.
[29] Thai Le, Suhang Wang, and Dongwon Lee. 2020. MALCOM: Generating Malicious
Comments to Attack Neural Fake News Detection Models. In ICDM. 282–291.
[30] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]
[31] Jason Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya
Rohatgi, and Dongwon Lee. 2023. Fighting Fire with Fire: The Dual Role of LLMs
in Crafting and Detecting Elusive Disinformation. In EMNLP. 14279–14305.
[32] Yuefei Lyu, Xiaoyu Yang, Jiaxin Liu, Sihong Xie, Philip Yu, and Xi Zhang. 2023.
Interpretable and Effective Reinforcement Learning for Attacking against Graph-
based Rumor Detection. In IJCNN. 1–9.
[33] Sachit Menon and Carl Vondrick. 2023. Visual Classification via Description from
Large Language Models. In ICLR.
[34] Qiong Nan, Juan Cao, Yongchun Zhu, Yanyan Wang, and Jintao Li. 2021. MD-
FEND: Multi-domain fake news detection. In CIKM. 3343–3347.
[35] Qiong Nan, Danding Wang, Yongchun Zhu, Qiang Sheng, Yuhui Shi, Juan Cao,
and Jintao Li. 2022. Improving Fake News Detection of Influential Domain via
Domain- and Instance-Level Transfer. In COLING. 2834–2848.
[36] Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. 2020.
FANG: Leveraging Social Context for Fake News Detection Using Graph Repre-
sentation. In CIKM. 1165–1174.
[37] OpenAI. 2022. ChatGPT: Optimizing language models for dialogue.
[38] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[39] Long Ouyang, Jeffrey Wu, Xu Jiang, et al .2022. Training language models to
follow instructions with human feedback. In NeurIPS, Vol. 35. 27730–27744.
[40] Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang,
Min-Yen Kan, and Preslav Nakov. 2023. Fact-Checking Complex Claims with
Program-Guided Reasoning. (2023), 6981–7004.
[41] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and
William Wang. 2023. On the Risk of Misinformation Pollution with Large Lan-
guage Models. In Findings of EMNLP 2023. 1389–1403.
[42] Kellin Pelrine, Jacob Danovitch, and Reihaneh Rabbany. 2021. The Surpris-
ing Performance of Simple Baselines for Misinformation Detection. In WWW.
3432–3441.
[43] Kellin Pelrine, Anne Imouza, Camille Thibault, Meilina Reksoprodjo, Caleb Gupta,
Joel Christoph, Jean-François Godbout, and Reihaneh Rabbany. 2023. Towards
Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4.
arXiv:2305.14928 [cs.CL]
[44] Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, and Benno
Stein. 2018. A Stylometric Inquiry into Hyperpartisan and Fake News. In ACL.
231–240.
[45] Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi.
2017. Truth of Varying Shades: Analyzing Language in Fake News and PoliticalFact-Checking. In EMNLP. 2931–2937.
[46] Natali Ruchansky, Sungyong Seo, and Yan Liu. 2017. CSI: A Hybrid Deep Model
for Fake News Detection. In CIKM. 797–806.
[47] Timo Schick and Hinrich Schütze. 2021. Exploiting Cloze-Questions for Few-Shot
Text Classification and Natural Language Inference. In EACL. 255–269.
[48] Qiang Sheng, Juan Cao, Xueyao Zhang, Rundong Li, Danding Wang, and
Yongchun Zhu. 2022. Zoom Out and Observe: News Environment Perception for
Fake News Detection. In ACL. 4543–4556.
[49] Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu. 2019. DEFEND:
Explainable Fake News Detection. In KDD. 395–405.
[50] Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu.
2020. FakeNewsNet: A Data Repository with News Content, Social Context, and
Spatiotemporal Information for Studying Fake News on Social Media. Big Data
8, 3 (2020), 171–188.
[51] Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. 2017. Fake News
Detection on Social Media: A Data Mining Perspective. SIGKDD Explor. Newsl.
19, 1 (2017), 22–36.
[52] Hugo Touvron, Louis Martin, Kevin Stone, et al .2023. Llama 2: Open Foundation
and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]
[53] Vaibhav Vaibhav, Raghuram Mandyam, and Eduard Hovy. 2019. Do Sentence
Interactions Matter? Leveraging Sentence Level Representations for Fake News
Classification. In TextGraphs-13. 134–139.
[54] Haoran Wang, Yingtong Dou, Canyu Chen, Lichao Sun, Philip S. Yu, and Kai Shu.
2023. Attacking Fake News Detectors via Manipulating News Social Engagement.
InWWW. 3978–3986.
[55] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
Fedus. 2022. Emergent Abilities of Large Language Models. Transactions on
Machine Learning Research (2022).
[56] Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods. Biometrics
Bulletin 1 (1945), 80–83.
[57] Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage
Challenge Corpus for Sentence Understanding through Inference. In NAACL.
1112–1122.
[58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al .
2020. Transformers: State-of-the-Art Natural Language Processing. In EMNLP.
38–45.
[59] Jiaying Wu and Bryan Hooi. 2023. DECOR: Degree-Corrected Social Graph
Refinement for Fake News Detection. In KDD. 2582–2593.
[60] Jiaying Wu, Shen Li, Ailin Deng, Miao Xiong, and Bryan Hooi. 2023. Prompt-
and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection.
InCIKM. 2726–2736.
[61] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Un-
supervised Data Augmentation for Consistency Training. In NeurIPS, Vol. 33.
6256–6268.
[62] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,
Franziska Roesner, and Yejin Choi. 2019. Defending Against Neural Fake News.
InNeurIPS, Vol. 32.
[63] Amy X. Zhang, Aditya Ranganathan, Sarah Emlen Metz, Scott Appling, Con-
nie Moon Sehat, Norman Gilmore, Nick B. Adams, Emmanuel Vincent, Jennifer
Lee, Martin Robbins, Ed Bice, Sandro Hawke, David Karger, and An Xiao Mina.
2018. A Structured Response to Misinformation: Defining and Annotating Credi-
bility Indicators in News Articles. In Companion Proceedings of WWW. 603–612.
[64] Xueyao Zhang, Juan Cao, Xirong Li, Qiang Sheng, Lei Zhong, and Kai Shu. 2021.
Mining Dual Emotion for Fake News Detection. In WWW. 3465–3476.
[65] Xuan Zhang and Wei Gao. 2023. Towards LLM-based Fact Verifica-
tion on News Claims with a Hierarchical Step-by-Step Prompting Method.
arXiv:2310.00305 [cs.CL]
[66] Haoyi Zheng and Huichun Zhan. 2023. ChatGPT in Scientific Writing: A Cau-
tionary Tale. The American Journal of Medicine 136, 8 (2023), 725–726.e6.
[67] Xinyi Zhou, Jindi Wu, and Reza Zafarani. 2020. SAFE: Similarity-Aware Multi-
modal Fake News Detection. In PAKDD. 354–367.
[68] Zhixuan Zhou, Huankang Guan, Meghana Bhat, and Justin Hsu. 2019. Fake News
Detection via NLP is Vulnerable to Adversarial Attacks. In Proceedings of the 11th
International Conference on Agents and Artificial Intelligence.
[69] Yongchun Zhu, Qiang Sheng, Juan Cao, Shuokai Li, Danding Wang, and Fuzhen
Zhuang. 2022. Generalizing to the Future: Mitigating Entity Bias in Fake News
Detection. In SIGIR.
[70] Yongchun Zhu, Qiang Sheng, Juan Cao, Qiong Nan, Kai Shu, Minghui Wu, Jindong
Wang, and Fuzhen Zhuang. 2022. Memory-Guided Multi-View Multi-Domain
Fake News Detection. IEEE Transactions on Knowledge and Data Engineering
(2022).
3376Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks KDD ’24, August 25–29, 2024, Barcelona, Spain
A DISCUSSION: EFFECT OF REFRAMINGS ON
LLM STYLE ROBUSTNESS
This section investigates the effects of incorporating our style-
diverse news reframings (Section 5.1) on LLM style robustness.
Following the experimental setup for evaluating style robustness
(Section 6), we report results on adversarial set A (formulation
described in Table 4) unless otherwise specified . We explore two
methods for adapting LLMs to diverse news styles: (1)in-context
learning and(2)fine-tuning, with prompt templates and LLM
configurations detailed in Appendix B.1. To mitigate randomness,
we present averaged metrics from three runs of each approach.
In-Context Learning. Table 10 compares a zero-shot GPT-3.5
detector with the following three in-context learning variants:
(1)GPT-3.5+ICL-2, which incorporates 2 randomly selected in-
context samples (1 real and 1 fake). (2)GPT-3.5+ICL-2-R, which
incorporates the same 2 in-context samples as GPT-3.5+ICL-2, en-
riched with style-diverse reframings. For each in-context sample,
we randomly select one reliable-style reframing and one unreliable-
style reframing. (3)GPT-3.5+ICL-4, which incorporates 4 ran-
domly selected in-context samples (2 real and 2 fake).
Table 10: GPT-3.5 in-context learning performance.
Method PolitiFact GossipCop LUN
GPT-3.5 42.13 39.59 59.63
+ ICL-2 38.57 38.88 57.51
+ ICL-2-R 41.76 41.83 60.29
+ ICL-4 42.35 41.47 60.25
Results in Table 10 indicate three key findings: (1)LLMs benefit
from more in-context samples, as GPT-3.5+ICL-4 outperforms GPT-
3.5+ICL-2. (2)Incorporation of news reframings effectively enhances
LLM style robustness, as shown by the consistent gains of GPT-
3.5+ICL-2-R over GPT-3.5+ICL-2. (3)On the fake news detection task,
LLM in-context learning does not offer clear benefits over LLM zero-
shot. The long sequence lengths of news articles limit the number
of in-context demonstrations, resulting in insufficient context and
overly lengthy prompts that LLMs struggle to process.
Fine-Tuning. Table 11 compares a zero-shot LLaMA2-13B de-
tector with the following two fine-tuned variants: (1)LLaMA2-
13B+FT, which fine-tunes LLaMA2 using the original training
articles. (2)LLaMA2-13B+FT-R, which fine-tunes LLaMA2 using
the original training articles and their style-diverse reframings. For
each article, we randomly incorporate one reliable-style reframing
and one unreliable-style reframing as fine-tuning data.
Table 11: LLaMA2-13B fine-tuning performance.
Method PolitiFact GossipCop LUN
LLaMA2-13B 33.24 25.79 32.64
+ FT 36.96 47.83 34.31
+ FT-R 44.44 56.06 42.22
Both LLaMA2 variants are fine-tuned using QLoRA [ 11] for 1
epoch with a batch size of 16and a learning rate of 1×10−4. FromTable 11, we observe: (1)Fine-tuning generally enhances task-specific
LLM capabilities on fake news detection, as indicated by consistent
improvements of LLaMA2-13B+FT over LLaMA2-13B. (2)Incorpo-
rating style-diverse news reframings as fine-tuning samples further
improves style robustness, as indicated by consistent improvements
of LLaMA2-13B+FT-R over LLaMA2-13B+FT. (3)LLaMA2-13B+FT-
R still falls short compared to GPT-3.5 on the LUN adversarial test
set and performs worse than task-specific detectors and fully fine-
tuned LMs in Table 3. This suggests the need for larger, task-specific
corpora for fully adapting LLMs to fake news detection, further
highlighting our motivation for leveraging LLM general-purpose
capabilities in a zero-shot manner, which are reasonably strong and
more readily available.
B LLM PROMPTING CONFIGURATIONS
B.1 LLM Baselines
In section 6.1.2, we select three representative LLMs as fake news de-
tection baselines: GPT-3.5 [37] (model name: gpt-3.5-turbo-0301 ),
InstructGPT [39] (model name: gpt-3.5-turbo-instruct ), and
LLaMA2-13B [52] (model name: Llama-2-13b-chat-hf).
For GPT-3.5 and InstructGPT, we use their APIs from OpenAI,
and set the temperature to 0for stable veracity predictions. For
LLaMA2-13B, we employ model weights from HuggingFace Trans-
formers [ 58] version 4.31.0, with do_sample set to False for greedy
decoding. All models adopt the following prompt for zero-shot fake
news detection:
Question: Does the following contain real or fake news (or infor-
mation)? Answer in one word with either ‘Real’ or ‘Fake’, then
explain why. [news article]
Answer: [starts with a predicted veracity label]
B.2
Obtaining SheepDog’s NewsReframings
RecallfromSection 5.1that wegenerate reliable-style reframings
andunreliable-style reframings with GPT-3.5foreach labelednews
article .Among thefour prompt templates presente dbelow,weuse
thefirst twotemplates forreliable-style reframings, andtheother
twoforunreliable-style reframings.
During reframing generation, wesetthetemp eratur eto0.7and
limit themaximum numb erofresponse tokens to512.SeeAppendix
B.4foradetaile dreframing example .
Rewrite the following article in an objective and professional tone:
[news article]
Rewrite the following article in a neutral tone: [news article]
Rewrite the following article in an emotionally triggering tone:
[news article]
Rewrite the following article in a sensational tone: [news article]
3377KDD ’24, August 25–29, 2024, Barcelona, Spain Jiaying Wu, Jiafeng Guo, and Bryan Hooi
B.3 Obtaining SheepDog’s Veracity Attributions
In Section 5.3, we use an LLM to elicit auxiliary content-centric in-
formation directly related to news veracity. Specifically, we prompt
the LLM to provide explanatory outputs for each fake news article
in the training set, based on predefined content-focused rationales
aimed at debunking fake news.
We draw from existing literature, which commonly emphasizes
two key aspects: (1)sources of information [26,63], and (2)
correctness of news content [2,51]. Inspired by these insights,
we devise four rationales:
•Source-related rationales: [A]lack of credible sources; and [B]
inconsistencies with reputable sources. These rationales are in-
formed by prior research emphasizing the importance of rep-
utable sources in validating conclusions [ 63] and correcting false
information [26].
•Content-related rationales: [C]false or misleading information;
and[D]biased opinion. Fake news, defined as intentionally and
verifiably false information [ 2,51], often manipulates biased
opinions to exploit cognitive vulnerabilities in news consumers.
Consequently, we devise [C]in alignment with the definition of
fake news, and [D]to represent a common strategy employed
by fake news producers.
Our four rationales cover the key aspects of source- and content-
related indicators in debunking fake news. We utilize the following
prompt to obtain content-focused veracity attributions, and set the
temperature to 0to ensure stability. An example is presented in
Appendix B.4.
Article: [fake news article]
Question: Which of the following problems does this article have?
Lack of credible sources, False or misleading information, Biased
opinion, Inconsistencies with reputable sources. If multiple op-
tions apply, provide a comma-separated list ordered from most to
least related. Answer “No problems” if none of the options apply.
B.4
Reframing andAttribution Examples
Due tothecamera-r eady page limit, detaile dillustrations ofSheep-
Dog’s newsreframings andveracity attributions areprovidedin
Table 13andTable 14ofthearXiv version ofourwork2.
CANAL YSIS ONCON TENTCONSISTENCY OF
LLM-EMPO WERED NEWS REFRAMINGS
LLM-emp owerednewsreframings play akeyroleinachie ving style
robustness. Appendix B.4provides acomparison betweenanews
article anditsreliable-style reframing, illustrating concr etely the
impr essiv ecapability ofLLMs tointroduce differ enttones while
preserving thenewscontent.
Tooffer amorecompr ehensiv eassessment ofcontent consis-
tency betweentheoriginal newsarticle anditsreframings, we
investigate claim entailment forastraightfor wardandquantifiable
estimation. Specifically ,theoriginal article should ideally entail the
central factual claims within itsreframing, andvice versa.
2Available at: https://arxiv.org/pdf/2310.10830.Giventhat newsarticles exhibit significantly longer sequences
andmorecomple xlogical structur escompar edtothesentence pairs
innatural language infer ence (NLI) benchmarks (e.g.,SNLI [6]and
MultiNLI [57]),weoptagainst utilizing NLImodelspre-traine don
these benchmarks. Instead, weinstruct aGPT-3.5modeltoextract
claims andinfer claim entailment. Theprompt template forclaim
extraction isasfollows:
Extract and summarize the central factual claim in the following
article. Article: [news article A]. Claim:
Quer ying theLLM yields asuccinct summarization ofthearti-
cle’scentral factual claim, typically comp osedofseveralsub-claims.
Anexample response isprovidedasfollows:
Response Example for Claim Extraction: Financial experts are
concerned about the negative impact of China’s undervalued yuan
on both Asia and the United States. They are calling on regional
governments and the Group of 20 leaders to take action to prevent
potential currency and trade wars. The experts emphasize the need
for neighboring countries to urge China to relax its exchange rate
controls in order to address the global current account imbalance.
They also highlight the adverse effects of US monetary easing
and China’s low exchange rate on emerging market economies.
The experts are urging the G20 to address the currency problem
at its upcoming summit in South Korea and to oppose unilateral
devaluation moves and support currency stability.
With theextracte dclaim, wepredictclaim entailment using the
following prompt:
Question: Does the following article entail the claim: [claim ex-
tracted from news article A]? Answer in one word with either
‘Yes’ or ‘No’. Article: [news article B].
While preciseentailment assessment requiresdetaile danalysis
ofeach article ’snuances andspecific claim wordings, evaluating
theproportion ofarticle pairs with predicte dclaim entailment
provides ageneral estimation ofcontent consistency .Toensur e
compr ehensiv ecoverage ,weanalyze twolarger-scale benchmark
datasets: GossipCop andLUN (dataset statistics inTable 2).
Table12:Claim entailment (%)betweenoriginal newsarticles
andtheir objective-style reframings.
Dataset original-entail-obje
ctiveobjective-entail-original
GossipCop
86.20 89.17
LUN 89.22 87.53
AsshowninTable 12,theproportions ofclaim entailment range
from86.20% to89.22% acrossbothdatasets, suggesting areasonably
high consistency betweenthecentral factual claims presente dinthe
original newsarticles andtheir reframings. This consistency further
validates theeffectiveness ofourreframing approach inpreserving
thecorecontent ofnewswhile injecting stylistic variations.
3378