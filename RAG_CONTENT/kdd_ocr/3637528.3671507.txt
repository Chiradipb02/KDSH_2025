Interpretable Cascading Mixture-of-Experts for Urban Traffic
Congestion Prediction
Wenzhao Jiang
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, Guangdong, China
wjiang431@connect.hkust-gz.edu.cnJindong Han
The Hong Kong University of Science
and Technology
Hong Kong, China
jhanao@connect.ust.hkHao Liu∗
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, Guangdong, China
The Hong Kong University of Science
and Technology
Hong Kong, China
liuh@ust.hk
Tao Tao
Didichuxing Co. Ltd
Beijing, China
taotao@didiglobal.comNaiqiang Tan
Didichuxing Co. Ltd
Beijing, China
tannaiqiang@didiglobal.comHui Xiong
The Hong Kong University of Science
and Technology (Guangzhou)
Guangzhou, Guangdong, China
The Hong Kong University of Science
and Technology
Hong Kong, China
xionghui@ust.hk
ABSTRACT
Rapid urbanization has significantly escalated traffic congestion,
underscoring the need for advanced congestion prediction services
to bolster intelligent transportation systems. As one of the world’s
largest ride-hailing platforms, DiDi places great emphasis on the
accuracy of congestion prediction to enhance the effectiveness and
reliability of their real-time services, such as travel time estimation
and route planning. Despite numerous efforts have been made on
congestion prediction, most of them fall short in handling hetero-
geneous and dynamic spatio-temporal dependencies (e.g., periodic
and non-periodic congestions), particularly in the presence of noisy
and incomplete traffic data. In this paper, we introduce a Conges-
tion Prediction Mixture-of-Experts, CP-MoE, to address the above
challenges. We first propose a sparsely-gated Mixture of Adaptive
Graph Learners (MAGLs) with congestion-aware inductive biases
to improve the model capacity for efficiently capturing complex
spatio-temporal dependencies in varying traffic scenarios. Then,
we devise two specialized experts to help identify stable trends
and periodic patterns within the traffic data, respectively. By cas-
cading these experts with MAGLs, CP-MoE delivers congestion
predictions in a more robust and interpretable manner. Further-
more, an ordinal regression strategy is adopted to facilitate effective
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671507collaboration among diverse experts. Extensive experiments on real-
world datasets demonstrate the superiority of our proposed method
compared with state-of-the-art spatio-temporal prediction models.
More importantly, CP-MoE has been deployed in DiDi to improve
the accuracy and reliability of the travel time estimation system.
CCS CONCEPTS
•Information systems →Spatial-temporal systems; •Com-
puting methodologies →Machine learning; •Applied com-
puting→Transportation.
KEYWORDS
congestion prediction; spatiotemporal modeling; mixture-of-experts
ACM Reference Format:
Wenzhao Jiang, Jindong Han, Hao Liu, Tao Tao, Naiqiang Tan, and Hui
Xiong. 2024. Interpretable Cascading Mixture-of-Experts for Urban Traffic
Congestion Prediction. In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ’24), August 25–29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.
1145/3637528.3671507
1 INTRODUCTION
Rapid urbanization in recent years has led to an unprecedented
increase in traffic volumes, posing significant challenges to modern
Intelligent Transportation Systems (ITS). As an integral part of
ITS, traffic congestion prediction aims to anticipate future traffic
conditions (i.e., fast, slow, and congested) on the roads, which plays
a pivotal role in human livelihood and urban governance. For ex-
ample, accurate prediction of traffic congestion enables drivers to
make informed trip planning decisions in advance, thereby largely
reducing travel time and fuel consumption. Furthermore, it also
empowers various decision-making tasks in transportation man-
agement, such as route planning [ 35], public transit scheduling [ 51],
5206
KDD ’24, August 25–29, 2024, Barcelona, Spain Wenzhao Jiang et al.
and emergency response planning [ 12]. As a result, congestion pre-
diction has been extensively studied in both academia and industry.
Accurate congestion prediction relies on effective modeling of
spatio-temporal dependencies within traffic data. In the past decade,
many efforts have been made to develop advanced Deep Learn-
ing (DL) models for tackling this problem [ 1,26]. To name a few,
Maet al. [37] leverages deep Restricted Boltzmann Machines (RBMs)
and Recurrent Neural Networks (RNNs) to model the temporal dy-
namics of congestion. Chen et al. [4] incorporates Convolutional
Neural Networks (CNNs) to jointly model the recent and periodic
congestion patterns. Xia et al. [54] and Wang et al. [50] adopt Spa-
tial Temporal Graph Neural Networks (STGNNs) to capture the
intricate traffic propagation patterns for congestion level prediction.
Despite fruitful progress in this field, building industry-level
congestion prediction systems still faces the following challenges.
(1) Urban traffic data exhibit heterogeneous and dynamic spatio-
temporal dependencies in non-congestion, periodic congestion and
non-periodic congestion scenarios. For example, main roads may
experience periodic congestion with regular propagation patterns
during rush hours, while roads around landmarks (e.g., sports sta-
diums) more often encounter non-periodic congestion, showcasing
diverse ranges in space and time depending on specific events. Even
within the same location, the traffic propagation patterns during
congested peak hours can be distinct from those in non-congested
periods. Simply introducing a more sophisticated model architec-
ture or increasing the model size is insufficient to handle such
dynamic and heterogeneous traffic patterns. Besides, a large pa-
rameter size also introduces substantial computational costs, which
hinders the model deployment in production. Therefore, it is chal-
lenging to develop a cost-effective congestion prediction model for
diverse spatio-temporal dependency preservation. (2) Real-world
traffic data suffer from frequent missingness and noises [ 60]. For in-
stance, in ride-hailing platforms like DiDi, real-time traffic features
such as congestion levels are calculated from the GPS trajectories
of ride-hailing vehicles. However, the sparseness of ride-hailing
vehicles in specific areas or at particular time slots might lead to
data missing issues. Moreover, poor GPS signals and unpredictable
driver behaviors can introduce additional noises that could distort
the actual traffic patterns. Therefore, it is challenging to develop
congestion prediction models that are robust to these missing val-
ues and noises. (3) Interpretability is critical for the industry-scale
deployment of congestion prediction models [ 43]. Stakeholders
such as the product manager and customer need to understand
why certain areas are predicted to be congested to make informed
traffic management or travel decisions. However, it is difficult for
humans to interpret the reasoning process of deep learning models
due to their inherent black-box nature. Therefore, how to enhance
the prediction interpretability is another challenge.
To tackle the above challenges, in this paper, we present a Con-
gestion Prediction Mixture-of-Experts (CP-MoE), which consists
of three major modules. First, inspired by the recent success of
sparsely-gated Mixture-of-Experts (MoE) in handling large-scale
heterogeneous data [ 2,42,45], we propose a Mixture of Adaptive
Graph Learners (MAGLs) module. By training multiple specialized
graph learning experts on varied subgroups of data and selectively
activating them on specific samples via a sparse gating mechanism,MAGLs possess a much larger capacity to accommodate heteroge-
neous and evolving traffic patterns while maintaining superior infer-
ence efficiency. Second, to enhance the model’s robustness against
potential data missingness and noise, we introduce two specialized
experts focusing on capturing stable trends and periodic patterns,
respectively. By cascading these experts with MAGLs, this approach
not only directs MAGL’s focus towards more complex samples but
also enhances the model’s decision-making transparency via the
interpretable expert aggregation weights. Finally, We adopt ordinal
regression strategy [ 11] to alleviate the experts’ over-confidence
issue caused by their varied inductive biases and the inherent class
imbalance, enabling beneficial collaboration among experts.
Our main contributions are summarized as follows. (1) To the best
of our knowledge, this is the first attempt to apply the MoE architec-
ture to an industry-level congestion prediction application. (2) We
propose a progressive expert design grounded in the characteristics
of urban traffic data to construct an expressive and scalable conges-
tion prediction model. We further organize the experts cascadingly
to boost its robustness and interpretability. (3) We devise an ordinal
regression strategy to balance the confidence of experts in order to
prevent the collapse of expert collaboration. (4) We conduct exten-
sive experiments on real-world traffic datasets to demonstrate the
superiority of CP-MoE against advanced spatio-temporal predic-
tion models. We further utilize the congestion prediction results to
improve the travel time estimation service in production.
2 PROBLEM STATEMENT
This paper focuses on urban traffic congestion prediction. We first
introduce several important definitions as follows.
Definition 1 (Traffic Network). The traffic network is defined
as a directed weighted graph G=(𝑉,𝐸), where𝑣𝑖∈𝑉denotes road
link and𝑒𝑖𝑗∈𝐸denotes the adjacent relation between 𝑣𝑖and𝑣𝑗. At
time interval 𝑡, the dynamic traffic features is denoted as X𝑡∈R𝑁×𝐶,
where𝑁=|𝑉|and𝐶is the number of dynamic traffic feature type.
Besides, we let X𝑡
𝑖∈R𝐶denote the dynamic features of link 𝑣𝑖.
Definition 2 (Congestion Level). We assess traffic conditions
of links using three discrete congestion levels: fast, slow, and congested,
denoted as class 0, 1, and 2, respectively.
With the above concepts, we next formulate the target problem.
Problem 1 (Congestion Prediction). Given traffic feature se-
quence X𝑡−𝑇𝑝+1:𝑡:=(X𝑡−𝑇𝑝+1,X𝑡−𝑇𝑝+2,...,X𝑡)∈R𝑇𝑝×𝑁×𝐶from
previous𝑇𝑝time intervals, historical traffic feature set Hand traffic
networkG, we aim to learn a mapping function F(·) to predict the
congestion level in the future 𝑇𝑓time intervals,
F:(X𝑡−𝑇𝑝+1:𝑡,H;G)↦→ ˆY𝑡+1:𝑡+𝑇𝑓∈{0,1,2}𝑇𝑓×𝑁. (1)
3 DATA DESCRIPTION AND ANALYSIS
3.1 Data Description
We study our problem on real-world datasets collected from Beijing
and Shanghai, two metropolises in China. Table 1 summarizes their
detailed statistics. The two datasets range from September 24, 2023,
to November 03, 2023, and from October 30, 2023, to December 09,
2023, respectively. They are constructed from trajectory records
5207Interpretable Cascading Mixture-of-Experts for Urban Traffic Congestion Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
(a) Spatial dependency in peak hours.
 (b) Spatial dependency in nonpeak hours.
 (c) Congestion level shift probability.
 (d) Periodicity of congestion.
Figure 1: Primary data analysis on Beijing dataset. In (a) and (b), deep color indicates higher dependency. In (c), deeper color
implies higher instability of traffic condition. In (d), deeper color implies a higher likelihood of periodic congestion.
Table 1: Statistics of two real-world datasets.
Beijing Shanghai
# of Time intervals 11808
Time interval 5 minute
# of road links 568 707
Congestion ratio 18.2% 6.6%
Missing ratio 0.7% 2.3%
in DiDi’s ride-hailing platform, covering hundreds of links from
urban arterial roads, where congestion happens frequently.
The datasets encompass a range of static link attributes and
dynamic traffic features. The static attributes include link length,
width, speed limit, number of lanes, longitude and latitude. The
dynamic traffic features of each link comprise real-time traffic speed
and congestion level. Traffic speed is computed by averaging the
speed of trajectories that traverse a specific road link. Congestion
level is labeled by practical rules developed by DiDi, taking into
account factors like static link attributes and average speed.
3.2 Feature Construction
We use two categories of features for each road link: static features
and dynamic features. The static features comprise link attributes
S∈R𝑁×𝐷𝑠,where𝐷𝑠is the number of static features, and link
distance feature 𝑟𝑖𝑗is calculated based on the longitude and latitude
of link𝑣𝑖and𝑣𝑗.The dynamic features are originally updated every
1 minute. We aggregate them into 5-minute intervals by averaging
the traffic speed features and selecting the most frequent congestion
level. The recent dynamic features are collected from previous 12 in-
tervals. The historical features Hare extracted from previous days
and weeks, comprising H𝑑={X(𝑡+1−𝑑·𝑇𝑑):(𝑡+𝑇𝑓−𝑑·𝑇𝑑)}1≤𝑑≤𝑁𝑑
andH𝑤={X(𝑡+1−𝑤·𝑇𝑤):(𝑡+𝑇𝑓−𝑤·𝑇𝑤)}1≤𝑤≤𝑁𝑤,where𝑇𝑑,𝑇𝑤is
the number of time intervals in a day and a week, respectively, and
𝑁𝑑,𝑁𝑤is the maximum number of days or weeks we trace back.
In practice, we set 𝑁𝑑=4and𝑁𝑤=3.
3.3 Data Analysis
In this section, we conduct primary analysis on the Beijing datasets
to intuitively illustrate the challenges of congestion prediction.
3.3.1 Spatio-temporal dependencies. To understand the complex
spatio-temporal dependencies in urban traffic data, we sample 40inter-connected arterial road links that cross multiple city function
regions and visualize the Pearson correlation matrix [ 8] of them
during peak and non-peak hours, respectively in Figure 1(a) and
1(b). In both figures, we can observe strong intra-region dependen-
cies (deeper color around the main diagonal) and much weaker
inter-region dependencies, which reveals the intricate spatial het-
erogeneity. Moreover, by comparing the two figures, we observe
a clear dependency variation from peak hours to non-peak hours,
which motivates us to improve the model capacity for capturing
heterogeneous traffic patterns in varied scenarios.
3.3.2 Stability and periodicity of congestion. In Figure 1(c) and 1(d),
we analyze over 30 links the probability of congestion level shift and
congestion occurrence across 288 daily time intervals, respectively.
Deeper colors indicate higher traffic instability or higher likelihood
of periodic congestion, respectively. We can observe the stability
and periodicity of traffic at night, contrasting with the significant
variability observed during daytime. Specifically, Box 1 comparison
indicates the existence of persistent stationary congestion, while
Box 2 comparison suggests non-stationary congestion with fre-
quently shifting traffic conditions. Box 3 comparison reveals the
non-stationary evolution in non-congested scenarios. Moreover,
the three boxes in Figure 1(d) also reflect the existence of both
periodic and non-periodic congestion. These findings necessitates
an adaptive utilization of trend and periodic patterns to enhance
model accuracy without affecting its learning on complex scenarios.
4 THE PROPOSED APPROACH
4.1 Model Overview
Figure 2(a) depicts the overall framework of CP-MoE, which con-
sists of three major modules: (1) Mixture of Adaptive Graph Learn-
ers (MAGLs) : It comprises multiple MAGL layers built upon the
sparsely-gated MoE architecture that selectively route samples to
specialized graph learning experts for comprehensive and efficient
exploration of spatio-temporal dependencies. (2) Cascading Integra-
tion of Trend and Periodic Experts (CITPE) : It adaptively integrates
two specialized experts to capture trend and periodic patterns, en-
hancing the model’s robustness to deal with corrupted data. (3)
Expert Confidence Balancing (ECB) : It harnesses ordinal regression
to guide the experts in recognizing ordinal relations among con-
gestion levels, mitigating overconfidence in their predictions and
fostering effective expert collaboration.
5208KDD ’24, August 25–29, 2024, Barcelona, Spain Wenzhao Jiang et al.
Recent Traffic
Historical Traffic𝑵𝒅 daysTrend Expert𝑳=𝟏𝑳=𝟐𝑳=𝟑
Periodic ExpertTrend Decoupler
Cascading Integration of Trend & Periodic Expert (CITPE)
!𝐏!!𝐏"#!𝐏$%#
!𝐏#%!𝐏
Hard Label 𝐘Soft Label 𝐘"#$𝐃𝑲𝑳(!𝐏||𝐘(#))Expert Confidence Balancing (ECB)MAGL RegularizersImportanceBalancingLoad Balancing(𝒍+𝟏𝑳𝓛𝒊𝒎𝒑𝒍(𝒍+𝟏𝑳𝓛𝒍𝒐𝒂𝒅𝒍
Ordinal RegressionVKQMulti-head Self-AttentionAdd & Norm𝑻𝒑 steps
𝑵𝒘 weeksMLPTime Embeds𝑻𝒇×𝑵𝒅+𝑵𝒘𝑵Spatial Embeds……
Link Attributes
MLPmMLPtrMLPper
Confidence Function
𝑵1.右半部分的高度调整到和左半部分差不多；2.左右半边箭头不一致；3.小圆圈的边框粗细不太一致；4.其他觉得丑的地方
updownSparseGateMixture of Adaptive Graph Learners (MAGLs)
𝐇𝒕∈ℝ𝑻𝒑×𝑵×𝑫TCN
'𝐇𝒕∈ℝ𝑻𝒑×𝑵×𝑫Spatial Pooling
Temporal Embeds
Spatial Embeds
Link AttributesMLP Gate
Noisy Logits
Upstream ExpertsDownstream ExpertsGlobal Experts
Top-K Logits
𝒗𝒊
Spatial Aggregation
UpstreamDownstreamConfidence FunctionConf. Func.
(a) Overall framework of CP-MoE.
Recent Traffic
Historical Traffic𝑵𝒅 daysTrend Expert𝑳=𝟏𝑳=𝟐𝑳=𝟑
Periodic ExpertTrend Decoupler
Cascading Integration of Trend & Periodic Expert (CITPE)
!𝐏!!𝐏"#!𝐏$%#
!𝐏#%!𝐏
Hard Label 𝐘Soft Label 𝐘"#$𝐃𝑲𝑳(!𝐏||𝐘(#))Expert Confidence Balancing (ECB)MAGL RegularizersImportanceBalancingLoad Balancing(𝒍+𝟏𝑳𝓛𝒊𝒎𝒑𝒍(𝒍+𝟏𝑳𝓛𝒍𝒐𝒂𝒅𝒍
Ordinal RegressionConf. Func.VKQMulti-head Self-AttentionAdd & Norm𝑻𝒑 steps
𝑵𝒘 weeksMLPTime Embeds𝑻𝒑×𝑵𝒅+𝑵𝒘𝑵Spatial Embeds……
Link Attributes
MLPmMLPtrMLPper
Conf. Func.
𝑵1.右半部分的高度调整到和左半部分差不多；2.左右半边箭头不一致；3.小圆圈的边框粗细不太一致；4.其他觉得丑的地方
updownSparseGateMixture of Adaptive Graph Learners (MAGLs)
𝐇𝒕∈ℝ𝑻𝒑×𝑵×𝑫TCN
'𝐇𝒕∈ℝ𝑻𝒑×𝑵×𝑫Spatial Pooling
Temporal Embeds
Spatial Embeds
Link AttributesMLP Gate
Noisy Logits
Upstream ExpertsDownstream ExpertsGlobal Experts
Top-K Logits
𝒗𝒊
Spatial Aggregation
UpstreamDownstream (b) Overview of a MAGL layer.
Figure 2: Overall framework of CP-MoE.
4.2 Mixture of Adaptive Graph Learners
As shown in Figure 2(b), a MAGL layer leverages a learnable sparse
gate to select specific experts from a shared expert pool for each
link at a specific time slot. Formally, a MAGL layer is defined as
H𝑡
𝑖(𝑙+1)=𝑁𝑒∑︁
𝑛=1𝐺𝑛
𝐼
H𝑡
𝑖(𝑙)
·𝐸𝑛
H𝑡
𝑖(𝑙)
, (2)
where𝑁𝑒is the number of experts, 𝐺(·)denotes the sparse gate
function and 𝐺𝑛(·)is the𝑛-th element of the output vector from
𝐺(·), which determines the importance of the 𝑛-th expert𝐸𝑛(·);
𝐼(·)indicates a profiling function designed to augment the context
features provided to the gate; H𝑡
𝑖(𝑙)∈R𝑇𝑝×𝐷is the output of the
𝑙-th layer for link 𝑣𝑖at time interval 𝑡, and H𝑡
𝑖(0)=FC(X𝑡−𝑇𝑝+1:𝑡
𝑖),
where FC(·)stands for a fully connected layer.
In practice, we stack 𝐿MAGL layers and utilize a Multi-Layer
Perceptron (MLP) to generate the congestion level logits for all road
links over next 𝑇𝑓time steps,
ˆP𝑡+1:𝑡+𝑇𝑓
𝑚 =MLP𝑚(H𝑡
𝑖(𝐿)). (3)
We detail the gate function and expert design below. For simplicity,
we omit the layer index in the superscript.
4.2.1 Sparse gate with fine-grained inputs. The sparse gate function
in MAGL layer is crucial for learning diverse spatio-temporal pat-
terns present in traffic data. A good gate function should route input
samples to the most suitable experts under specific context [ 14].
To achieve this goal, we curate a collection of fine-grained context
features as gate inputs to enhance the sample distinguishability.
As convolution networks is sensitive to high-frequency signals
(e.g., unexpected congestion) [ 39], we first leverage gated Temporal
Convolution Networks (TCN) [ 53] to extract temporal dynamics
representations H𝑡
𝑖′from the input H𝑡
𝑖. The details of gated TCN
are presented in Appendix A.1. Afterwards, we derive the short-
term spatio-temporal context by using a lightweight sum operator
˜H𝑡
𝑖=Í
𝑣𝑗∈N𝑘H𝑡
𝑗′, whereN𝑘
𝑖denotes the 𝑘-hop neighbors of link𝑣𝑖. In practice, we find that the sum operator is more efficient and
effective than other learnable aggregator functions, e.g., GNNs, the
empirical evidence of which can be found in Appendix A.3.
However, short-term information may lack reliability and dis-
tinguishability across various contexts [ 9,44]. Consequently, we
further incorporate three types of stable features: (1) static link
attributes S, (2) a learnable spatial embedding E𝑠∈R𝑁×𝐷𝑙that en-
capsulates unique spatial characteristics, and (3) Time-of-Day (ToD)
and Day-of-Week (DoW) embeddings E𝑇𝑜𝐷∈R288×𝐷𝑙,E𝐷𝑜𝑊∈
R7×𝐷𝑙that encode regular temporal patterns. Overall, the input of
the gate function w.r.t. link𝑣𝑖at time𝑡can be written as
𝐼(H𝑡
𝑖)=˜H𝑡
𝑖∥MLP𝑠(S𝑖)∥E𝑠
𝑖∥E𝑇𝑜𝐷
𝑡∥E𝐷𝑜𝑊
𝑡. (4)
Based on the resulting features c𝑡
𝑖=𝐼(H𝑡
𝑖), we follow previous
work on MoE [45] and apply noisy top-K gating mechanism, i.e.,
𝐺(c𝑡
𝑖)=Softmax(TopK(MLP𝑔(c𝑡
𝑖)+𝜖·Softplus(MLP𝑛(c𝑡
𝑖))),(5)
where Softplus(·)is an activation function [ 16], the output logits of
MLP𝑔(·)is added with Gaussian noise 𝜖∈N( 0,1)to avoid model
collapse (i.e., over-reliance on a few experts), and TopK(·)sparsely
activates𝐾experts based on the largest entries in the noisy logits.
4.2.2 Congestion-aware graph experts. As discussed in Section 3.3.1,
there exists a substantial discrepancy in spatio-temporal patterns
between congestion and non-congestion scenarios. Such pattern
discrepancy may introduce noises or even mutually contradictory
knowledge, making it difficult to train a unified model that per-
fectly recognizes patterns across different scenarios. To tackle the
challenge, we devise three groups of graph learning experts, each
endowed with a dedicated inductive bias that enables specialization
in a particular pattern type.
Specifically, our expert design is motivated by the following key
insight: traffic congestion usually spreads from downstream to up-
stream links, whereas traffic flow freely propagates from upstream
to downstream links during non-congestion periods. Based on this
insight, we assign two specialized expert groups, namely upstream
5209Interpretable Cascading Mixture-of-Experts for Urban Traffic Congestion Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
experts anddownstream experts, to model the two distinct propa-
gation dynamics. In general, each expert 𝐸(·)is implemented with
an edge-aware graph attention network [ 36,48], which adaptively
aggregate neighboring information into the target link via
𝐸(H𝑡
𝑖)=∑︁
𝑗∈N𝑖𝛼𝑖𝑗W𝑗H𝑡
𝑗+H𝑡
𝑖, (6)
𝛼𝑖𝑗=exp
LeakyReLU
a𝑇h
WH𝑡
𝑖WH𝑡
𝑗W𝑟𝑟𝑖𝑗i
Í
𝑘∈N𝑖exp
LeakyReLU
a𝑇h
WH𝑡
𝑖WH𝑡
𝑘W𝑟𝑟𝑖𝑘i,(7)
where W,W𝑟anda𝑇are learnable mappings. For upstream ex-
perts,N𝑖consists of the upstream links of 𝑣𝑖, while for downstream
experts,N𝑖only encompasses the downstream links.
However, the graph topology built from the road network is
often noisy and incomplete, which may not reflect the actual re-
lationships among road links. Therefore, we additionally assign a
group of global experts to specialize in identifying latent propaga-
tion patterns [ 3,53]. Each global expert is equipped with a unique
learnable link embedding E𝑠∈R𝑁×𝐷𝑙to encode the inherent spa-
tial characteristics. The hidden dependency between link 𝑣𝑖and𝑣𝑗
can then be inferred via
𝛼𝑖𝑗=Softmax(ReLU(E𝑠
𝑖E𝑠
𝑗𝑇)), (8)
which can be used for spatial aggregation by following Equation (6).
4.3 Cascading Integration of Trend and Periodic
Experts
Despite MAGLs’ strengths, their effectiveness diminishes when
short-term traffic propagation patterns are distorted by noises and
missing data. In such cases, the trend and periodic patterns are
more useful for forecasting, as they are insensitive to neighboring
interference. Driven by this insight, we improve model robustness
by constructing a trend expert and a periodic expert to capture stable
trend and periodicity, respectively. On the other hand, the analysis
in Section 3.3.2 indicates the existence of intricate non-stationary
and non-periodic traffic patterns, requiring greater efforts from
MAGL for accurate prediction. This necessitates an adaptive ap-
proach to cascade the trend and periodic expert with MAGL. We
introduce the detailed design as follows.
4.3.1 Trend decoupling and modeling. The trend of traffic condi-
tion is represented by low-frequency signals within the short-term
traffic observations, which is often coupled with high-frequency
signals that fluctuate over time. Therefore, we first adopt Discrete
Wavelet Transform (DWT) [ 21] to decompose input X𝑡+𝑇𝑝−1:𝑡into
components at varied frequency scales, and use inverse DWT to
reconstruct the trend signals R𝑡+𝑇𝑝−1:𝑡from low-frequency com-
ponents. We defer details of this procedure to Appendix A.2. After
that, we adopt a Multi-head Self-Attention (MSA) [ 47] based trend
expert followed by a MLP layer to output the prediction logits of
future traffic conditions,
ˆP𝑡+1:𝑡+𝑇𝑓
𝑡𝑟=MLP𝑡𝑟(MSA(R𝑡+𝑇𝑝−1:𝑡)). (9)
4.3.2 Periodicity modeling. When there exists severer corruption
within short-term data, the underlying periodic patterns of histor-
ical data facilitate a more robust prediction. The historical trafficfeaturesH, as introduced in Section 3.2, encompass global periodic
patterns driven by daily human routines like morning commutes
and local patterns influenced by external factors, such as recent
weather variations. To capture such multifaceted periodicity, we
incorporate learnable spatio-temporal embeddings and develop an
efficient MLP-based periodic expert for future prediction,
ˆP𝑡+1:𝑡+𝑇𝑓
𝑝𝑒𝑟 =MLP𝑝𝑒𝑟(H,E𝑠,E𝑇𝑜𝐷,E𝐷𝑜𝑊). (10)
Please refer to Appendix A.4 for more implementation details.
4.3.3 Cascading expert integration. The primary goal of adaptively
integrating trend and periodic expert is to ensure that these experts
dominate corrupted data, while MAGL are reserved for complex pat-
terns. However, the distribution similarity between the two types of
data make it difficult to learn such an ideal routing strategy without
explicit supervision signals. Motivated by the recent study [ 57], we
mitigate this issue by leveraging the experts’ prediction confidences
to determine their influence on the final prediction.
Concretely, the trend or periodic expert will be assigned a weight
computed from its output logits via a learnable score function
𝐶(ˆP)=MLP𝑐(𝐷(ˆP)).Here𝐷(·)is a dispersion function that cal-
culates the variance and negative entropy of logits to measure
prediction confidence. MLP𝑐(·)is trained to map the dispersion
to an expert weight within the range [0,1].The time indices of
logits ˆPare omitted for brevity. Furthermore, the ordering of ex-
pert aggregation is guided by two principles: (1) Activate stronger
experts only when the confidence levels of all weaker experts are
low in order to promote focused learning of complex patterns; (2)
The periodic expert is considered weaker than the trend expert
due to its inaccessibility to the latest traffic observations. These
principles lead to a cascading expert aggregation strategy, which
derives the final prediction logits of CP-MoE from the outputs of
different experts as
ˆP𝑟𝑒=𝐶2(ˆP𝑡𝑟)ˆP𝑡𝑟+
1−𝐶2(ˆP𝑡𝑟)
ˆP𝑚, (11)
ˆP=𝐶1(ˆP𝑝𝑒𝑟)ˆP𝑝𝑒𝑟+
1−𝐶1(ˆP𝑝𝑒𝑟)
ˆP𝑟𝑒, (12)
where𝐶1(·)and𝐶2(·)are two learnable confidence functions. No-
tably, this approach also possesses great interpretability as the
model decision process can be explained by the weights of experts.
4.4 Ordinal Regression for Expert Confidence
Balancing
The diversified inductive biases among experts, coupled with the
varying degree of congested class imbalance within their assigned
data subsets, can lead to considerable disparities in confidence
levels. Consequently, overly confident experts may undermine the
contributions of others, potentially leading the CITPE module to
make biased predictions.
To this end, we leverage ordinal regression [ 11] to mitigate the
overconfidence issue of experts. This approach smooths one-hot
labels into soft labels by redistributing a portion of probability from
target class to other classes, thereby reducing experts’ confidence
in a single class. Moreover, the classes closer to the target class will
be assigned a larger probability to preserve the natural ordering
among classes. Such a strategy further enriches each class in the
5210KDD ’24, August 25–29, 2024, Barcelona, Spain Wenzhao Jiang et al.
label space with additional information from nearby classes [ 17,55],
effectively reducing overconfidence caused by class imbalance.
Formally, the 𝑖-th element of the one-hot label encoding is ad-
justed to𝑦𝑜𝑟𝑑[𝑖]=𝑒−𝜙(𝑖,𝑦)/Í
𝑗𝑒−𝜙(𝑗,𝑦),where𝑦denotes the tar-
get class and 𝜙(·,·)is a pre-defined distance metric that penalizes
the probability of distant classes. In the context of congestion pre-
diction, given the finite number of classes, 𝜙(·,·)can be determined
through hyperparameter tuning. In practice, we further constrain
the distance metric to satisfy 𝜙(𝑖,𝑗)+𝜙(𝑗,𝑘)=𝜙(𝑖,𝑘),∀0≤𝑖≤
𝑗≤𝑘,thereby narrowing the tuning space to {𝜙(𝑖,𝑖+1)}𝑖≥0.
4.5 Optimization Objectives
The optimization objectives of CP-MoE consist of two parts. The
first part is the ordinal regression loss to encourage a balanced
confidence levels among experts, which computes the Kullback-
Leibler (KL) divergence between the CP-MoE’s output logits ˆPand
the ordinally smoothed congestion level labels Y𝑜𝑟𝑑,i.e.,
L𝑜𝑟𝑑=𝐷𝐾𝐿(ˆP∥Y𝑜𝑟𝑑). (13)
The second part comprises two types of expert balancing regulariz-
ers to prevent the model collapse issue of MAGLs [ 45]. To be more
specific, each MAGL layer is equipped with an importance balanc-
inglossL𝑖𝑚𝑝,which limits the variation in the weights assigned
to different graph experts, and a load balancing lossL𝑙𝑜𝑎𝑑,which
ensures equitable activation frequencies across experts. Formally,
L𝑖𝑚𝑝=𝐶𝑉𝑗(∑︁
𝑥∈B𝐺𝑗(𝑥)),L𝑙𝑜𝑎𝑑=𝐶𝑉𝑗(∑︁
𝑥∈B𝑃𝑟𝑗), (14)
where𝐶𝑉(·)is the coefficient of variation and 𝑃𝑟𝑗is the probability
of the𝑗-th expert been activated over a batch of samples B. More
details on the differentiability of L𝑙𝑜𝑎𝑑 can be found in Appendix
A of work [45].
Overall, we train CP-MoE by jointly optimizing the objectives
L=L𝑜𝑟𝑑+𝜆1𝐿∑︁
𝑙=1L(𝑙)
𝑖𝑚𝑝+𝜆2𝐿∑︁
𝑙=1L(𝑙)
𝑙𝑜𝑎𝑑, (15)
whereL(𝑙)
𝑖𝑚𝑝andL(𝑙)
𝑙𝑜𝑎𝑑are the importance balancing loss and load
balancing loss of the 𝑙-th MAGL layer, 𝜆1and𝜆2are hyperparame-
ters that controls the extent of expert balancing.
5 EXPERIMENTS
We conduct comprehensive experiments to answer the following
research questions. RQ 1: How is the overall performance of CP-
MoE compared with the SOTA baselines on real-world datasets?
RQ 2: How robust is CP-MoE w.r.t. varied missing and noise ra-
tios? RQ 3: How do different modules in CP-MoE affect the model
performance? RQ 4: How is CP-MoE’s interpretability? RQ 5: Can
CP-MoE benefit travel time estimation in the ride-hailing service?
5.1 Experimental Setup
5.1.1 Metrics. For the congestion prediction task, we use Accuracy,
Recall, Precision, F1-score (C-F1) and Weighted F1-score (W-F1)
for evaluation. Specifically, Recall, Precision, C-F1 are calculated
w.r.t. congested class. W-F1 is the average of the F1-scores for three
classes, with the weights being 0.2, 0.2 and 0.6, respectively. Forthe travel time estimation task, we use Mean Average Error (MAE),
Root Mean Squared Error (RMSE), and Bad Case Rate (BCR) [34].
5.1.2 Baselines. We compare our proposed framework with the
following baselines. Rule-based strategies: (1) CurrentTime (CT):
It uses the present congestion level as the predictions for future 𝑇𝑓
time intervals; (2) HistoricalAverage (HA): It predicts the conges-
tion level of link 𝑣𝑖at time𝑡as its most frequent historical conges-
tion level at time interval 𝑡in the training set. General STGNNs:
DCRNN [ 31], ASTGNN [ 18], GWNet [ 53], AGCRN [ 3], STID [ 44],
STWave [ 13], ST-MoE [ 30] and STAEformer [ 33].Congestion pre-
diction methods: DuTraffic [ 54] and STTF [ 50]. Please refer to
Appendix B.1 for more details on these methods and Appendix B.2
for implementation details of CP-MoE.
5.2 Overall Performance Comparison (RQ 1)
Table 2 reports the 12-step prediction results of CP-MoE and all the
compared baselines w.r.t. five evaluation metrics. Since the perfor-
mance is very close in the offline and online environments, we only
report the offline results. From Table 2, we can make the following
observations. First, the proposed model outperforms all baselines
in terms of all metrics on two datasets. Significant improvements
in Recall and C-F1 highlight its adeptness at learning complex
traffic congestion patterns, while enhancements in Precision and
W-F1 confirm its ability to maintain accuracy across congested
and non-congested scenarios, owing to the tailored sparse gating
mechanism and specialized expert designs. Besides, the methods
which aggregate spatial information on pre-defined graph topol-
ogy (DCRNN, ASTGNN) perform significantly worse than those
on learnable adaptive graphs (GWNet, AGCRN, STWave), under-
scoring the complexity of congestion propagation patterns and the
need for enlarging the capacity of spatial modeling module. With
a more capable architecture, STAEformer and ST-MoE achieves
further improvements on both datasets. However, due to the lack of
task-customization, they still underperform CP-MoE. In addition,
on the Shanghai dataset with a higher data missing ratio, STID out-
performs complex adaptive graph based methods. This is because
STID’s learnable embeddings can capture regular spatio-temporal
patterns resilient to data anomalies. Our method shows further
improvement, which validates the efficacy of CITPE module in en-
hancing the model robustness without compromising the ability to
learn complex patterns. Moreover, the two congestion prediction
models underperform the selected STGNNs, indicating their limited
customization and the universality of state-of-the-art STGNNs.
5.3 Robustness Check (RQ 2)
To further justify CP-MoE’s robustness during training, we syn-
thesize various levels of data missingness or noises by randomly
masking or flipping 𝑝%observed traffic features in the training set
of the Beijing dataset, where 𝑝∈{20,40,60,80}. The performance
variations of representative models are shown in Figure 3. Due to
similar variation trends w.r.t. C-F1 and W-F1, we visualize only the
C-F1 performance. Clearly, all methods experience performance
degradation as 𝑝increases, as they encounter more unrealistic traf-
fic patterns. AGCRN struggles with high noise or missing data
due to its over reliance on complex spatial dependency modeling.
Removing CITPE module from CP-MoE (CP-MoE-WoC ) performs
5211Interpretable Cascading Mixture-of-Experts for Urban Traffic Congestion Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 2: The 12-step congestion prediction performance on Beijing and Shanghai datasets. The best results are in bold, the
second-best are underlined, and the third-best are marked with a star.
Beijing ShanghaiModelAccuracy(%) Recall(%) Precision(%) W-F1(%) C-F1(%) Accuracy(%) Recall(%) Precision(%) W-F1(%) C-F1(%)
CT 80.25 70.79 70.67 69.44 70.73 87.59 59.05 58.85 65.20 58.95
HA 78.57 55.52 73.10 62.65 63.11 88.25 53.54 61.01 64.41 57.04
DCRNN 81.27±0.10 71.78±0.95 71.46±0.74 69.32±0.31 71.61±0.15 88.67±0.07 52.24±1.68 67.41±1.16 65.27±0.43 58.83±0.70
ASTGNN 81.53±0.27 72.14±3.29 72.05±2.74 70.33±0.74 71.97±0.52 88.83±0.65 65.13±3.73* 62.04 ±4.23 68.19±1.08 63.31±1.11
GWNet 84.13±0.16 76.10±2.16 76.21±1.25 74.26±0.40 76.12±0.50 90.81±0.05 63.55±2.85 73.03±2.33* 72.55±0.56 67.86±0.70
AGCRN 84.30±0.11 77.77±1.33* 75.76 ±1.10 74.48±0.24 76.73±0.20 90.72±0.09 64.51±1.18 71.71±1.16 72.54±0.27 67.90±0.27
STID 83.81±0.16 75.42±0.99 75.49±0.79 73.60±0.36 75.45±0.30 90.95±0.06* 63.02±1.35 73.70±1.06 72.75±0.27* 67.92±0.39
STWave 84.21±0.13 77.36±1.18 75.58±1.05 74.38±0.18 76.44±0.15 90.56±0.07 64.01±1.64 71.10±1.42 72.05±0.30 67.34±0.37
ST-MoE 84.39±0.10* 77.92±1.12 75.88±0.91 74.60±0.22* 76.89±0.18* 90.83±0.11 64.60±1.01 71.98±1.12 72.70±0.29 68.09±0.27*
STAEformer 84.71±0.14 77.73±1.68 76.85±1.39 75.21±0.25 77.26±0.30 91.05±0.07 65.49±1.69 72.26±1.55 73.38±0.15 68.67±0.26
DuTraffic 81.66±0.15 72.58±1.10 71.76±0.70 69.41±0.25 72.17±0.21 88.62±0.10 52.54±1.46 66.70±1.36 65.02±0.40 58.78±0.70
STTF 84.19±0.16 76.40±2.01 76.28±1.34* 74.30±0.40 76.34±0.58 90.64±0.09 63.12±2.67 72.87±2.19 72.35±0.57 67.65±0.93
CP-MoE 85.09±0.13 80.30±1.01 76.82±0.76 75.92±0.38 78.52±0.20 91.20±0.12 66.38±1.39 73.90±1.01 74.09±0.61 69.94±0.42
Figure 3: Robustness check on the Beijing dataset.
Figure 4: Ablation study on Beijing and Shanghai datasets.
well at low levels of missingness or noise, but fails significantly un-
der extreme data noise, mirroring AGCRN’s limitation. Differently,
ASTGNN and STID capture corruption-resilient long-term patterns
by explicitly modeling periodic features or incorporating learnable
spatio-temporal embeddings, achieving robust performance across
varied settings. CP-MoE achieves comparable performance fluctu-
ation with STID in missing data scenarios, and significantly less
fluctuation in noisy conditions compared to all baselines. Mean-
while, it consistently outperforms baselines in all scenarios. These
results affirm the value of trend and periodicity modeling for ro-
bustness and emphasize the efficacy of adaptive expert aggregation
in maintaining ability to learn complex patterns.5.4 Ablation Study (RQ 3)
To validate the effectiveness of each module in CP-MoE, we com-
pare the performances of the following variants: (1) -WoLE re-
moves both spatial and temporal learnable embeddings in MAGLs’
gate inputs. (2) -WoPL removes the local pooling when compiling
MAGL’s gate inputs. (3) -WoIB removes the spatial inductive biases
in MAGL’s experts. (4) -WoC removes the CITPE module. (5) -WA
replaces the confidence-based expert aggregation with simple av-
erage aggregation. (6) -WoR removes the ordinal regression. As
shown in Figure 4, we can make the following observations.
5.4.1 The efficacy of the MAGL design. First, removing the learn-
able embeddings and spatial pooling results in performance degra-
dation, confirming the importance of discriminative inputs for effec-
tive gating strategy learning. Additionally, removing expert induc-
tive biases in MAGLs leads to performance drops, underscoring the
value of expert specialization in handling diverse traffic conditions.
5.4.2 The efficacy of the CITPE design. First, the performance drop
caused by removing CITPE module validates the importance of
capturing stable trend and periodic patterns to model robustness.
Second, replacing the confidence-based aggregation with average
aggregation also causes the performance drop, which verifies the
importance of adaptive expert integration for the model to excel in
both data-imperfect and complex scenarios.
5.4.3 The efficacy of ordinal regression. We also observe the im-
provement achieved by the ordinal regression strategy. This vali-
dates its effectiveness in alleviating the over-confidence issue of
experts and fostering collaborative efforts among.
5.5 Interpretability Analysis (RQ 4)
In this part, we provide both global and local analysis of experts
weights to justify the inherent interpretability of CP-MoE.
5.5.1 Expert weight distribution. By examining the weights of trend
and periodic experts, we discover that both experts dominate the
prediction for a few samples, with their weights larger than 0.5.
Specifically, as shown in Figure 5(a) and 5(b), the samples dominated
by trend expert are mostly undergoing stable traffic evolution, while
those dominated by periodic expert show strong periodic patterns.
5212KDD ’24, August 25–29, 2024, Barcelona, Spain Wenzhao Jiang et al.
(a) Trend expert dominated
sample distribution.
(b) Periodic expert domi-
nated sample distribution.
(c) Trend and periodic ex-
perts’ weight distributions.
Figure 5: Expert weight distribution on Shanghai Dataset.
Figure 6: Case study on Shanghai Dataset. Each case shows
the traffic evolution of 8 links that are arranged sequentially
from upstream to downstream. CP-MoE predicts the conges-
tion levels of the link marked in blue box for the future 12-24
intervals, based on the observations in previous 12 intervals.
On the other hand, Figure 5(c) reveals the predominant role of
MAGLs on a majority of samples. These observations confirm that
the expert weights generated by the model offer insightful and
human-understandable interpretations for its decisions, thereby
validating the inherent interpretability of CP-MoE.
5.5.2 Local interpretation study. In Figure 6, we present three cases
where CP-MoE’s predictions are dominated by the trend expert,
periodic expert, and MAGLs respectively, as reflected by the expert
weights. In Figure 6(a), the target link undergoes prolonged con-
gestion while its adjacent links vary significantly. MAGLs are not
suitable for this case since over spatial aggregation may introduce
noise, whereas the trend expert, focusing on the target’s stable
trend, can predict accurately. Figure 6(b) shows a rare prolonged
slow-moving case which the trend expert struggle. But the periodic
expert can predict accurately due to the pattern’s high periodicity
strength discovered from the link’s historical. In Figure 6(c), MAGLs
can capture clear traffic propagation patterns between links for ac-
curate predictions, while trend and periodic experts might fail by
solely relying on target link histories. These cases verify the efficacy
of expert weights for meaningful model interpretations.
5.6 Online Travel Time Estimation Test (RQ 5)
We further conduct online experiments to justify the utility of CP-
MoE’s prediction results on Travel Time Estimation (TTE), one of
the fundamental services in DiDi that directly influences various
downstream applications and user experiences. DiDi’s current TTE
model utilizes a self-attention mechanism to process static and real-
time traffic attributes, including congestion levels, at the start of
the order for end-to-end TTE [ 19,34]. Despite DiDi’s TTE systems
being highly optimized, accurately predicting travel time for longerTable 3: Utility of CP-MoE on travel time estimation.
Criterion Model MAE(sec) RMSE(sec) BCR(%)
𝐴𝑇𝑇∈[40,50]Base 222.33 295.73 7.10
Base-WCP 221.87 294.61 7.04
𝐴𝑇𝑇∈[50,60]Base 290.58 385.52 8.59
Base-WCP 288.20 381.56 8.52
𝐴𝑇𝑇∈[60,+∞]Base 463.23 631.10 13.48
Base-WCP 454.39 617.06 13.12
𝐷𝑅∈[0.5,1.0]Base 236.94 338.24 8.93
Base-WCP 229.71 327.13 8.37
orders, particularly in non-periodic congestion scenarios, remains a
system bottleneck. Our study aims to address this gap by enhancing
the TTE system’s ability to anticipate future road conditions.
5.6.1 Congestion prediction integration strategy. We propose to
expose the TTE model to potential future traffic variations by re-
placing the original congestion level features with CP-MoE’s pre-
dicted ones. Specifically, we begin by grouping the orders in the
validation setObased on their estimated travel times ˆ𝑡𝑒, which are
calculated using the historical average speed of the road links and
the route length. Each order group 𝑖corresponds to the travel time
interval[5𝑖min,5(𝑖+1)min) , where 0≤𝑖≤12. For orders falling
into group𝑖, we replace with CP-MoE’s congestion predictions on
their relevant links at future time step 𝑡, where 0≤𝑡≤12, and
identify the optimal replacement time 𝑡𝑖that minimizes the MAE
of the TTE model. During testing, an order is first matched to its
group𝑖based on its ˆ𝑡𝑒. Then, the congestion predictions at time
𝑡𝑖are fed into the TTE model for prediction. Notably, training an
industry-level TTE model from scratch requires a huge effort. Our
strategy is much easier to integrate and apply at scale.
5.6.2 System Deployment. We utilized Spark [ 56] to develop an
efficient data processing pipeline with two principal steps: (i) extrac-
tion of link-level traffic features from ride-hailing car trajectories
at one-minute intervals, and (ii) compilation of diverse traffic data
every five minutes for future congestion prediction via CP-MoE.
The deployed CP-MoE conducts prediction on circular road links
within Beijing’s 5th Ring Road, covering areas with over 500,000
daily ride-hailing orders. These forecasts are generated for the next
hour (12 time steps) and are updated every five minutes using data
from the preceding hour. CP-MoE and the optimal replacement time
of congestion level features in the TTE model are daily updated.
Once the updation is completed, the model is pushed to online
servers to provide real-time congestion predictions. For each TTE
query, its congestion level features are now selected from CP-MoE’s
predictions. For the complete online processing pipeline of the TTE
system, please refer to Appendix A.4.3 in our previous work [19].
5.6.3 Online testing results. In Table 3, we report the TTE perfor-
mances of the base TTE model and our strategy (-WCP ) on one
week’s ride-hailing orders with Actual Travel Time (ATT) exceed-
ing 40 minutes. Our strategy achieves consistent improvements
over the base model across all metrics. We further select the orders
with more than half of the links experiencing a congestion level
deviation from departure to arrival, which we denote as order set
{𝐷𝑅∈[0.5,1]}where𝐷𝑅stands for Deviation Ratio. Our strategy
5213Interpretable Cascading Mixture-of-Experts for Urban Traffic Congestion Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
achieves more substantial improvements on them, verifying the
efficacy of exposing future traffic conditions to the TTE model, par-
ticularly when these conditions exhibit significant variations. More
importantly, the effectiveness of the TTE enhancement strategy in
turn corroborates the utility of CP-MoE in predicting future traffic
conditions in real-world production environment.
6 RELATED WORK
Traffic congestion prediction. Existing research on congestion
prediction spans various formulations, such as post-congestion
propagation prediction [ 46], congestion event prediction [ 24], and
congestion level prediction [ 54]. In this paper, we focus on conges-
tion level prediction. Early studies leverage data mining techniques
and shallow machine learning models to capture traffic patterns and
uncertainties, including pattern mining [ 22], clustering [ 28], hidden
Markov models [ 40] and Bayesian networks [ 25]. While fast to im-
plement and interpretable, they fail to capture the nonlinear spatio-
temporal dependencies. Deep Learning (DL) models have spurred
numerous studies to enhance congestion prediction accuracy [ 26].
Classical DL models such as RNNs and CNNs have been applied to
capture both short-term and long-term temporal dynamics [ 4,37].
Recent DL methods focus on capturing intricate spatio-temporal
propagation patterns. Cheng et al. [6] combines CNNs with RNNs
to extract the spatial-temporal features, while Di et al. [10] fur-
ther enhances CNNs with mined congestion propagation matrix.
Xiaet al. [54] incorporates Graph Convolutional Networks (GCNs),
RNNs and MSA to model trajectory data. Li et al. [29] adopts TCNs,
GCNs and learnable spatio-temporal embeddings to capture short-
term and long-term patterns. Wang et al. [50] uses Transformer to
directly handle 3D spatio-temporal feature tensors. However, train-
ing a single model is limited in capturing diverse traffic patterns
and keeping robustness to data anomalies. The black-box nature of
DL models also limits their practical application.
Spatio-temporal graph neural networks . STGNNs adeptly
capture the intricate spatio-temporal data dependencies by inte-
grating graph learning and temporal learning methods, facilitat-
ing advanced analytics and forecasting in various applications. In
terms of spatial modeling, existing research can be categorized
into static graph based and adaptive graph based methods. Static
graph based methods aggregate information from spatial neigh-
bors determined by pre-defined graphs. For instance, DCRNN [ 31]
conducts graph diffusion enhanced RNN on the traffic network
structure for traffic prediction. ASTGNN [ 18] performs graph con-
volution on the traffic network weighted by the dynamic node
feature similarity. Adaptive graph based methods automatically
discover hidden dependencies among graph nodes from data. For
example, GWNet [ 53], AGCRN [ 3] and MTGNN [ 52] generate adap-
tive adjacency matrix through the similarities among learnable node
embeddings. MugRep [ 58] conducts hierarchical graph learning to
capture multi-level urban spatio-temporal dynamics. BigST [ 20]
further boosts the adaptive graph learning to linear complexity for
large-scale applications. STWave [ 13] proposes a disentanglement
module to enhance the spatio-temporal modeling. ST-MoE [ 30]
integrates multiple STGNNs into an MoE framework, which only
takes observed traffic features as gate inputs and does not support
flexible MoE integration. Some recent works revisit the necessityof GNN in STGNN architecture and instead use MLP [ 44] or Trans-
former [ 33] to conduct spatio-temporal modeling. The robustness
risk of STGNN training is also revealed in work [ 32]. In this work,
we elevate the model capacity with tailored sparse-gated spatio-
temporal MoE layers. Moreover, compared with general STGNNs,
we incorporate task-customized expert designs to boost the utility,
robustness and interpretability of congestion prediction.
Mixture-of-experts. The concept of MoE was initially intro-
duced by Jacobs et al. [23] to reduce the negative impact of task
interference during training a single neural network. This approach
achieved large-scale success when Shazeer et al. [45] refined the
gating mechanism with Top- 𝐾sparsity constraints. This innovation
enabled scaling of the model to billions of parameters with MoE
layers integrated, enhancing its capacity remarkably. Subsequent
developments in the field have included novel advancements in
gating design [ 15,27], optimization algorithms [ 61], and distributed
training frameworks [ 15,41]. These advancements have further
elevated the capabilities and transferability of MoE-based large
language models. The impressive achievements of MoE in the NLP
field have inspired researchers to apply MoE’s scalable architecture
and conditional computation abilities to fields such as computer
vision [ 42], multi-modal learning [ 38] and graph learning [ 49,57].
For instance, GraphMoE [ 49] employed multiple experts to gather
information from node neighbors at different hops, effectively cap-
turing diverse structural knowledge. Mowst [ 57] combined simple
MLPs and GNNs, leveraging nuanced collaboration between weak
and strong experts to improve graph learning. In contrast, our ap-
proach designed a capable, robust and interpretable spatio-temporal
MoE layer with task-customized designs for congestion prediction.
7 CONCLUSION
In this study, we propose an effective Congestion Prediction Mixture-
of-Experts ( CP-MoE) to handle the traffic congestion prediction
problem. We first propose a Mixture of Adaptive Graph Learners
(MAGLs) with a tailored sparse gating mechanism and congestion-
aware expert biases to effectively capture heterogeneous and evolv-
ing traffic patterns. Additionally, we incorporate two specialized
experts to capture stable trend and periodic patterns, and adaptively
cascade them with MAGLs to boost CP-MoE’s robustness and in-
terpretability. An ordinal regression strategy is further employed
to alleviate over-confidence issues among experts and promote ef-
fective collaboration. Extensive experiments on real-world datasets
validate CP-MoE’s superior performance against various baselines.
Notably, CP-MoE has been successfully deployed in DiDi to en-
hance the reliability of its travel time estimation system. In the
future, we plan to investigate the utility of CP-MoE in a broader
range of ride-hailing services, such as route planning, to further
enhance operational efficiency and user experiences.
ACKNOWLEDGMENTS
This work was supported by the National Natural Science Foun-
dation of China (Grant No.62102110, No.92370204), National Key
R&D Program of China (Grant No.2023YFF0725004), Guangzhou Ba-
sic and Applied Basic Research Program (Grant No.2024A04J3279),
Education Bureau of Guangzhou Municipality.
5214KDD ’24, August 25–29, 2024, Barcelona, Spain Wenzhao Jiang et al.
REFERENCES
[1]Mahmuda Akhtar and Sara Moridpour. 2021. A review of traffic congestion
prediction using artificial intelligence. Journal of Advanced Transportation 2021
(2021), 1–18.
[2]Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam
Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Girid-
haran Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis
Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo, Jeffrey Wang, Luke Zettle-
moyer, Mona T. Diab, Zornitsa Kozareva, and Veselin Stoyanov. 2022. Efficient
Large Scale Language Modeling with Mixtures of Experts. In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing. 11699–
11732.
[3]Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020. Adaptive Graph
Convolutional Recurrent Network for Traffic Forecasting. In Advances in Neural
Information Processing Systems 33.
[4]Meng Chen, Xiaohui Yu, and Yang Liu. 2018. PCNN: Deep convolutional networks
for short-term traffic congestion prediction. IEEE Transactions on Intelligent
Transportation Systems 19, 11 (2018), 3550–3559.
[5]Zhengyu Chen, Teng Xiao, and Kun Kuang. 2022. BA-GNN: On Learning Bias-
Aware Graph Neural Network. In 2022 IEEE 38th International Conference on Data
Engineering. 3012–3024.
[6]Xingyi Cheng, Ruiqing Zhang, Jie Zhou, and Wei Xu. 2018. DeepTransport:
Learning Spatial-Temporal Dependency for Traffic Condition Forecasting. In
2018 International Joint Conference on Neural Networks. 1–8.
[7]Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.
CoRR abs/1412.3555 (2014).
[8]Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jing-
dong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson correlation coefficient.
Noise Reduction in Speech Processing (2009), 1–4.
[9]Jinliang Deng, Xiusi Chen, Renhe Jiang, Xuan Song, and Ivor W. Tsang. 2021.
ST-Norm: Spatial and Temporal Normalization for Multi-variate Time Series
Forecasting. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 269–278.
[10] Xiaolei Di, Yu Xiao, Chao Zhu, Yang Deng, Qinpei Zhao, and Weixiong Rao. 2019.
Traffic Congestion Prediction by Spatiotemporal Propagation Patterns. In 20th
IEEE International Conference on Mobile Data Management. 298–303.
[11] Raul Diaz and Amit Marathe. 2019. Soft Labels for Ordinal Regression. In 2019
IEEE Conference on Computer Vision and Pattern Recognition. 4738–4747.
[12] Soufiene Djahel, Nicolas Smith, Shen Wang, and John Murphy. 2015. Reducing
emergency services response time in smart cities: An advanced adaptive and
fuzzy approach. In 2015 IEEE First International Smart Cities Conference. 1–8.
[13] Yuchen Fang, Yanjun Qin, Haiyong Luo, Fang Zhao, and Kai Zheng. 2023.
STWave+: A Multi-Scale Efficient Spectral Graph Attention Network With Long-
Term Trends for Disentangled Traffic Flow Forecasting. IEEE Transactions on
Knowledge and Data Engineering (2023).
[14] William Fedus, Jeff Dean, and Barret Zoph. 2022. A Review of Sparse Expert
Models in Deep Learning. CoRR abs/2209.01667 (2022).
[15] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers:
Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Journal
of Machine Learning Research 23 (2022), 120:1–120:39.
[16] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep Sparse Rectifier
Neural Networks. In Proceedings of the 14th International Conference on Artificial
Intelligence and Statistics, Vol. 15. 315–323.
[17] Yu Gong, Greg Mori, and Frederick Tung. 2022. RankSim: Ranking Similar-
ity Regularization for Deep Imbalanced Regression. In Proceedings of the 39th
International Conference on Machine Learning, Vol. 162. 7634–7649.
[18] Shengnan Guo, Youfang Lin, Huaiyu Wan, Xiucheng Li, and Gao Cong. 2022.
Learning Dynamics and Heterogeneity of Spatial-Temporal Graph Data for Traffic
Forecasting. IEEE Transactions on Knowledge and Data Engineering 34, 11 (2022),
5415–5428.
[19] Jindong Han, Hao Liu, Shui Liu, Xi Chen, Naiqiang Tan, Hua Chai, and Hui
Xiong. 2023. iETA: A Robust and Scalable Incremental Learning Framework for
Time-of-Arrival Estimation. In Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining. 4100–4111.
[20] Jindong Han, Weijia Zhang, Hao Liu, Tao Tao, Naiqiang Tan, and Hui Xiong. 2024.
BigST: Linear Complexity Spatio-Temporal Graph Neural Network for Traffic
Forecasting on Large-Scale Road Networks. Proceedings of the VLDB Endowment
17, 5 (2024), 1081–1090.
[21] Christopher E Heil and David F Walnut. 1989. Continuous and discrete wavelet
transforms. SIAM Rev. 31, 4 (1989), 628–666.
[22] Ryo Inoue, Akihisa Miyashita, and Masatoshi Sugita. 2016. Mining spatio-
temporal patterns of congested traffic in urban areas from traffic sensor data. In
19th IEEE Intelligent Transportation Systems Conference. 731–736.
[23] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton.
1991. Adaptive Mixtures of Local Experts. Neural Computing 3, 1 (1991), 79–87.
[24] Guangyin Jin, Lingbo Liu, Fuxian Li, and Jincai Huang. 2023. Spatio-Temporal
Graph Neural Point Process for Traffic Congestion Event Prediction. In Proceed-
ings of the 37th AAAI Conference on Artificial Intelligence. 14268–14276.[25] Jiwon Kim and Guangxing Wang. 2016. Diagnosis and prediction of traffic
congestion on urban road networks using Bayesian networks. Transportation
Research Record 2595, 1 (2016), 108–118.
[26] Nishant Kumar and Martin Raubal. 2021. Applications of deep learning in con-
gestion detection, prediction and alleviation: A survey. Transportation Research
Part C: Emerging Technologies 133 (2021), 103432.
[27] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat,
Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. GShard:
Scaling Giant Models with Conditional Computation and Automatic Sharding.
In9th International Conference on Learning Representations.
[28] Fuliang Li, Junfeng Gong, Yunyi Liang, and Jiali Zhou. 2016. Real-time congestion
prediction for urban arterials using adaptive data-driven methods. Multimedia
Tools and Applications 75 (2016), 17573–17592.
[29] Fuxian Li, Huan Yan, Hongjie Sui, Deng Wang, Fan Zuo, Yue Liu, Yong Li, and
Depeng Jin. 2023. Periodic Shift and Event-aware Spatio-Temporal Graph Con-
volutional Network for Traffic Congestion Prediction. In Proceedings of the 31st
ACM International Conference on Advances in Geographic Information Systems.
50:1–50:10.
[30] Shuhao Li, Yue Cui, Yan Zhao, Weidong Yang, Ruiyuan Zhang, and Xiaofang
Zhou. 2023. ST-MoE: Spatio-Temporal Mixture-of-Experts for Debiasing in Traffic
Prediction. In Proceedings of the 32nd ACM International Conference on Information
& Knowledge Management. 1208–1217.
[31] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion Convolutional
Recurrent Neural Network: Data-Driven Traffic Forecasting. In 6th International
Conference on Learning Representations.
[32] Fan Liu, Hao Liu, and Wenzhao Jiang. 2022. Practical Adversarial Attacks on
Spatiotemporal Traffic Forecasting Models. In Advances in Neural Information
Processing Systems 35. 19035–19047.
[33] Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quan-
jun Chen, and Xuan Song. 2023. Spatio-temporal adaptive embedding makes
vanilla transformer sota for traffic forecasting. In Proceedings of the 32nd ACM
International Conference on Information & Knowledge Management. 4125–4129.
[34] Hao Liu, Wenzhao Jiang, Shui Liu, and Xi Chen. 2023. Uncertainty-Aware Proba-
bilistic Travel Time Prediction for On-Demand Ride-Hailing at DiDi. In Proceed-
ings of the 29th ACM SIGKDD Conference on Knowledge Discovery & Data Mining .
4516–4526.
[35] Hao Liu, Ying Li, Yanjie Fu, Huaibo Mei, Jingbo Zhou, Xu Ma, and Hui Xiong.
2020. Polestar: An Intelligent, Efficient and National-Wide Public Transportation
Routing Engine. In Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining. 2321–2329.
[36] Qingsong Lv, Ming Ding, Qiang Liu, Yuxiang Chen, Wenzheng Feng, Siming
He, Chang Zhou, Jianguo Jiang, Yuxiao Dong, and Jie Tang. 2021. Are we really
making much progress?: Revisiting, benchmarking and refining heterogeneous
graph neural networks. In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining. 1150–1160.
[37] Xiaolei Ma, Haiyang Yu, Yunpeng Wang, and Yinhai Wang. 2015. Large-scale
transportation network congestion evolution prediction using deep learning
theory. PloS One 10, 3 (2015), e0119044.
[38] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil
Houlsby. 2022. Multimodal Contrastive Learning with LIMoE: the Language-
Image Mixture of Experts. In Advances in Neural Information Processing Systems
35.
[39] Namuk Park and Songkuk Kim. 2022. How Do Vision Transformers Work?. In
10th International Conference on Learning Representations.
[40] Yan Qi and Sherif Ishak. 2014. A Hidden Markov Model for short term prediction
of traffic conditions on freeways. Transportation Research Part C: Emerging
Technologies 43 (2014), 95–111.
[41] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani
Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed-
MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-
Generation AI Scale. In Proceedings of the 39th International Conference on Machine
Learning, Vol. 162. 18332–18346.
[42] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe
Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. 2021. Scal-
ing Vision with Sparse Mixture of Experts. In Advances in Neural Information
Processing Systems 34. 8583–8595.
[43] Cynthia Rudin. 2019. Stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. Nature Machine
Intelligence 1, 5 (2019), 206–215.
[44] Zezhi Shao, Zhao Zhang, Fei Wang, Wei Wei, and Yongjun Xu. 2022. Spatial-
Temporal Identity: A Simple yet Effective Baseline for Multivariate Time Series
Forecasting. In Proceedings of the 31st ACM International Conference on Informa-
tion & Knowledge Management. 4454–4458.
[45] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le,
Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously Large Neural Networks:
The Sparsely-Gated Mixture-of-Experts Layer. In 5th International Conference on
Learning Representations.
5215Interpretable Cascading Mixture-of-Experts for Urban Traffic Congestion Prediction KDD ’24, August 25–29, 2024, Barcelona, Spain
[46] Yidan Sun, Guiyuan Jiang, Siew Kei Lam, and Peilan He. 2021. Predicting Traffic
Congestion Evolution: A Deep Meta Learning Approach. In Proceedings of the
30th International Joint Conference on Artificial Intelligence. 3031–3037.
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems 30. 5998–6008.
[48] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In 6th International
Conference on Learning Representations.
[49] Haotao Wang, Ziyu Jiang, Yuning You, Yan Han, Gaowen Liu, Jayanth Srinivasa,
Ramana Kompella, and Zhangyang Wang. 2023. Graph Mixture of Experts:
Learning on Large-Scale Graphs with Explicit Diversity Modeling. In Advances
in Neural Information Processing Systems 36.
[50] Xing Wang, Ruihao Zeng, Fumin Zou, Lyuchao Liao, and Faliang Huang. 2023.
STTF: An Efficient Transformer Model for Traffic Congestion Prediction. Inter-
national Journal of Computational Intelligence Systems 16, 1 (2023), 2.
[51] Keji Wei, Vikrant Vaze, and Alexandre Jacquillat. 2022. Transit planning opti-
mization under ride-hailing competition and traffic congestion. Transportation
Science 56, 3 (2022), 725–749.
[52] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi
Zhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting with
Graph Neural Networks. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 753–763.
[53] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.
Graph WaveNet for Deep Spatial-Temporal Graph Modeling. In Proceedings of
the 28th International Joint Conference on Artificial Intelligence. 1907–1913.
[54] Deguo Xia, Xiyan Liu, Wei Zhang, Hui Zhao, Chengzhou Li, Weiming Zhang,
Jizhou Huang, and Haifeng Wang. 2022. DuTraffic: Live Traffic Condition Pre-
diction with Trajectory Data and Street Views at Baidu Maps. In Proceedings of
the 31st ACM International Conference on Information & Knowledge Management.
3575–3583.
[55] Yuzhe Yang, Kaiwen Zha, Ying-Cong Chen, Hao Wang, and Dina Katabi. 2021.
Delving into Deep Imbalanced Regression. In Proceedings of the 38th International
Conference on Machine Learning, Vol. 139. 11842–11851.
[56] Matei Zaharia, Reynold S Xin, Patrick Wendell, Tathagata Das, Michael Armbrust,
Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J
Franklin, et al .2016. Apache spark: a unified engine for big data processing.
Commun. ACM 59, 11 (2016), 56–65.
[57] Hanqing Zeng, Hanjia Lyu, Diyi Hu, Yinglong Xia, and Jiebo Luo. 2023. Mixture
of Weak & Strong Experts on Graphs. CoRR abs/2311.05185 (2023).
[58] Weijia Zhang, Hao Liu, Lijun Zha, Hengshu Zhu, Ji Liu, Dejing Dou, and Hui
Xiong. 2021. MugRep: A Multi-Task Hierarchical Graph Representation Learning
Framework for Real Estate Appraisal. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining. 3937–3947.
[59] Yihua Zhang, Ruisi Cai, Tianlong Chen, Guanhua Zhang, Huan Zhang, Pin-Yu
Chen, Shiyu Chang, Zhangyang Wang, and Sijia Liu. 2023. Robust Mixture-of-
Expert Training for Convolutional Neural Networks. In 2023 IEEE/CVF Interna-
tional Conference on Computer Vision. 90–101.
[60] Yu Zheng, Licia Capra, Ouri Wolfson, and Hai Yang. 2014. Urban Computing:
Concepts, Methodologies, and Applications. ACM Transactions on Intelligent
Systems and Technology 5, 3 (2014), 38:1–38:55.
[61] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean,
Noam Shazeer, and William Fedus. 2022. Designing Effective Sparse Expert
Models. CoRR abs/2202.08906 (2022).
A MODEL DETAILS
A.1 Gated Temporal Convolutional Networks
The gated TCN comprises of dilated causal convolution layers and
an output gate. The causal convolution, an extension of 1D convo-
lution, preserves temporal ordering by only convolving over the
preceding time intervals. The dilated causal convolution stacks
multiple causal convolutions with exponentially increasing sliding
steps, enabling efficient extraction of multi-level temporal patterns
over a much wider receptive field. Formally, a dilated causal convo-
lution operation for a 1D sequence xat time𝑡is defined as
(x∗𝑑Θ)(𝑡)=𝑆−1∑︁
𝑠=0Θ(𝑠)·x(𝑡−𝑑·𝑠), (16)
where∗𝑑denotes the dilated convolution, Θ(·)is the learnable
convolution filter, 𝑑is the dilation factor, and 𝑆is the size of filter.Furthermore, a output gate is incorporated to control the ratios of
information flowing through dilated convolution layers. Overall,
the short-term temporal encodings of link 𝑣𝑖at time interval 𝑡are
computed as
H𝑡
𝑖′=𝑔(H𝑡
𝑖∗𝑑Θ1+b)⊙𝜎(H𝑡
𝑖∗𝑑Θ2+c), (17)
where bandcare learnable bias terms, 𝑔(·)is an activation function
and𝜎(·)is the sigmoid function.
A.2 Discrete Wavelet Transform
The Discrete Wavelet Transform (DWT) is a powerful mathematical
tool that allows us to analyze various frequency components of a
signal with a resolution matched to each scale. It decomposes the
original signal 𝑥(𝑡)into approximation coefficients 𝐴and detail
coefficients{𝐷𝑖}by convolving 𝑥(𝑡)with low-pass ( 𝑓𝑙) and high-
pass (𝑓ℎ) filters respectively, followed by downsampling,
𝐴(𝑘)=(𝑥∗𝑓𝑙)(2𝑘), 𝐷(𝑘)=(𝑥∗𝑓ℎ)(2𝑘). (18)
The Inverse DWT (IDWT) reconstructs the original signal by first
upsampling the chosen coefficients from 𝐴and{𝐷𝑖}, followed by
convolving them with inverse low-pass filter 𝑔𝑙and high-pass filters
𝑔ℎand summing the results as below,
ˆ𝑥(𝑡)=(𝐴↑2∗𝑔𝑙)(𝑡)+(𝐷↑2∗𝑔ℎ)(𝑡). (19)
Here the upsampling operator ↑2inserts a zero between each pair
of consecutive elements in the input signal. DWT and IDWT enable
an efficient decoupling and analysis of multi-scale components
in the signals, which have been widely applied in various fields
including image compression, signal denoising [21].
A.3 Evidence of Efficiency for the Aggregator
From Table 4, we can see that the sum operator in CP-MoE achieves
SOTA performance over GCN and GAT aggregators in terms of
Accuracy, Recall, W-F1 and C-F1. This superiority can be attrib-
uted to two factors: (i) Learnable graph aggregators tend to ex-
cessively highlight similarities between adjacent nodes [ 5]. This
could result in over-reliance on the link correlation induced by the
often similar congestion levels within a local region in our problem.
Consequently, the learnable aggregators insufficiently capture the
congestion propagation patterns, which are often associated with
congestion level heterophily between adjacent links. This is also
corroborated by the performance differences w.r.t. Recall and Preci-
sion; (ii) Under the MoE architecture, the bias in gate inputs induced
by learnable aggregators compromises the stability of routing. This,
in turn, adversely affects the training of multiple experts, leading
to a severe decline in overall performance [59].
A.4 Lightweight Periodic Expert
The periodic expert first concatenates the historical traffic sequences
inHinto one sequence 𝑋𝑝∈R𝑇𝑓(𝑁𝑑+𝑁𝑤)×𝑁×𝐶along the time
dimension. Then each slice is concatenated with its corresponding
temporal embeddings to enhance the expert’s understanding of rel-
ative time positions. Afterwards, we feed the temporally enriched
historical sequence into multiple MLP layers to obtain the temporal
encodings𝐻𝑝∈R𝑁×𝑑,which are further attached with learnable
spatial embeddings to increase the spatial discriminability of sam-
ples. Finally, the combined embeddings are input to another MLP
5216KDD ’24, August 25–29, 2024, Barcelona, Spain Wenzhao Jiang et al.
Table 4: The 12-step congestion prediction performance on Beijing and Shanghai datasets w.r.t. different spatial aggregator
choices, which are used to compile short-term spatio-temporal contextual gate inputs.
Beijing ShanghaiModelAccuracy(%) Recall(%) Precision(%) W-F1(%) C-F1(%) Accuracy(%) Recall(%) Precision(%) W-F1(%) C-F1(%)
CP-MoE 0.8521 0.8091 0.7651 0.7590 0.7865 0.9126 0.6775 0.7288 0.7433 0.7022
w/ GCN aggregator 0.8519 0.7973 0.7723 0.7579 0.7846 0.9126 0.6651 0.7392 0.7395 0.7002
w/ GAT aggregator 0.8509 0.7859 0.7774 0.7560 0.7817 0.9121 0.6573 0.7436 0.7364 0.6978
to produce the future predictions logits ˆ𝑃𝑡+1:𝑡+𝑇𝑓
𝑝𝑒𝑟∈R𝑇𝑓×𝑁×3.In
practice, the learnable spatio-temporal embeddings are shared with
MAGLs as defined in Section 4.2.1.
B EXPERIMENT DETAILS
B.1 Baseline Details
We detailedly introduce the compared deep learning baselines as
follows. General STGNNs: (1) DCRNN [ 31]: It integrates graph
diffusion operation into GRU [ 7] and make traffic prediction in
an encoder-decoder manner; (2) ASTGNN [ 18]: It harnesses self-
attention to model the recent and periodic context and a dynamic
GCN to capture spatial heterogeneity, making traffic prediction in
an encoder-decoder manner; (3) GWNet [ 53]: It combines gated
TCN and GCN with a learnable adaptive graph for traffic prediction;
(4) AGCRN [ 3]: It enhances GRU with graph convolutions based on
adaptive graph and proposes a node adaptive parameter learning
mechanism; (5) STID [ 44]: It leverages efficient MLP layers with
learnable spatial and temporal embeddings for multivariate time
series forecasting. (6) STWave [ 13]: It disentangles traffic data into
trend and event signals, models them separately, and adaptively
fuses them via attention mechanism for prediction. (7) ST-MoE [ 30]:
It integrates multiple STGNNs into an MoE framework. Each ex-
pert is chosen as AGCRN, which outperforms all others expert
choices according to our empirical results; (8) STAEformer [ 33]: It
incorporates three types of spatio-temporal adaptive embeddings
to strengthen the vanilla Transformer on spatio-temporal forecast-
ing.Congestion prediction methods: (9) DuTraffic [ 54]: It is a
congestion prediction model deployed at BaiduMap, where we only
compared with its multi-task learning framework due to the un-
availability of the visual data; (10) STTF [ 50]: It is an autoregressive
spatio-temporal transformer designed for congestion prediction.
Since baselines (9) and (10) are not open-source, we reproduced
their algorithms independently.
B.2 Implementation Details
The hidden dimension of CP-MoE is𝐷=32. The number of MAGL
layers is 2. In each MAGL layer, the number of upstream, down-
stream and global experts are 4, 4 and 2, respectively. Top-6 experts
will be activated at a time. The weights of expert balancing losses
are𝜆1=10−3,𝜆2=10−3.The number of TCN layers is 2 and
we extract 5-hop neighbors for spatial aggregation. The dimen-
sion of learnable embeddings is 𝐷𝑙=10.For trend expert, we use
Daubechies 1 wavelet for trend decoupling and set the head of
MSA as 2. The class distance function in ordinal regression satisfies
𝜙(0,1)=1,𝜙(1,2)=2.We divide each dataset with rate 7:1:2 along
the timeline for training, validation and testing, respectively. We
use Adam Optimization with learning rate 10−3, weight decay rate
5×10−7, dropout rate 0.15 and early stopping for 30 epochs. Themodel is trained on a high-performance server equipped with four
Intel Xeon E5-2630 V4 CPUs, 45 GB of memory, and a single Nvidia
Tesla P40 GPU.
B.3 Parameter Sensitivity Study
We study the sensitivity of CP-MoE on five hyperparameters: (1)
the number of upstream experts 𝑁𝑢𝑝in each MAGL layer, (2) the
number of downstream experts 𝑁𝑑𝑜𝑤𝑛 in each MAGL layer, (3) the
number of activated experts 𝑘for each sample in each MAGL layer,
(4) the weight of important balancing loss for MAGLs 𝜆1, (5) the
weight of load balancing loss for MAGLs 𝜆2.
First, we vary 𝑁𝑢𝑝and𝑁𝑑𝑜𝑤𝑛 from 2 to 5, respectively. As re-
ported in Figure 7(a) and 7(a), the model achieves the best perfor-
mance when 𝑁𝑢𝑝=𝑁𝑑𝑜𝑤𝑛 =4. Intuitively, insufficient experts can
limit the model’s learning capability in certain scenarios, whereas
too many experts can render the model hard to converge.
Second, we vary 𝑘from 4 to 7 as depicted in Figure 7(c). The
model achieves the best performance when 𝑘=6.Overly sparse
activation may compromise prediction accuracy while overly dense
activation may hinder the specialized training of experts.
Third, we vary 𝜆1and𝜆2from 10−4to10−1,respectively. As
shown in Figure 7(d) and 7(e), the best performance is achieved
when𝜆1=𝜆2=10−3.Intuitively, over-emphasis on expert balanc-
ing in MAGLs may interfere the specialization of different experts,
whereas insufficient expert balancing can lead to suboptimal expert
collaboration modes.
Overall, CP-MoE’s performances w.r.t. C-F1 vary within an ac-
ceptable range on the Beijing dataset, demonstrating its robustness
against different hyperparameters.
2 3 4 5
# of Upstream Expert77.678.078.478.8C-F1
(a)Effect of upstream
expert number.
2 3 4 5
# of Downstream Expert77.277.878.479.0C-F1
(b)Effect of down-
stream expert number.
4 5 6 7
T op-k77.177.778.378.9C-F1
(c)Effect of Top- 𝑘acti-
vation.
104
103
102
101
1
78.078.378.678.9C-F1
(d)Effect of𝜆1.
104
103
102
101
2
77.678.078.478.8C-F1
 (e)Effect of𝜆2.
Figure 7: Parameter sensitivity analysis on Beijing dataset.
5217