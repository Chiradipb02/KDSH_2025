Learning Metrics that Maximise Power for Accelerated A/B-Tests
Olivier Jeunen
ShareChat
Edinburgh, United Kingdom
jeunen@sharechat.coAleksei Ustimenko
ShareChat
London, United Kingdom
aleksei.ustimenko@sharechat.co
Abstract
Online controlled experiments are a crucial tool to allow for con-
fident decision-making in technology companies. A North Star
metric is defined (such as long-term revenue or user retention),
and system variants that statistically significantly improve on this
metric in an A/B-test can be considered superior. North Star met-
rics are typically delayed and insensitive. As a result, the cost of
experimentation is high: experiments need to run for a long time,
and even then, type-II errors (i.e. false negatives ) are prevalent.
We propose to tackle this by learning metrics from short-term
signals that directly maximise the statistical power they harness
with respect to the North Star. We show that existing approaches
are prone to overfitting, in that higher average metric sensitivity
does not imply improved type-II errors, and propose to instead
minimise the 𝑝-values a metric would have produced on a log of
past experiments. We collect such datasets from two social media
applications with over 160 million Monthly Active Users each, to-
talling over 153 A/B-pairs. Empirical results show that we are able
to increase statistical power by up to 78% when using our learnt
metrics stand-alone, and by up to 210% when used in tandem with
the North Star. Alternatively, we can obtain constant statistical
power at a sample size that is down to 12% of what the North Star
requires, significantly reducing the cost of experimentation.
CCS Concepts
•General and reference →Experimentation; •Mathematics
of computing→Hypothesis testing and confidence inter-
val computation; •Computing methodologies →Machine
learning.
Keywords
A/B-Testing; Evaluation Metrics; Statistical Power
ACM Reference Format:
Olivier Jeunen and Aleksei Ustimenko. 2024. Learning Metrics that Maximise
Power for Accelerated A/B-Tests. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11pages. https:
//doi.org/10.1145/3637528.3671512
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36715121 Introduction & Motivation
Modern platforms on the web need to continuously make decisions
about their product and user experience, which are often central to
the business at hand. These decisions range from design and inter-
face choices to back-end technology adoption and machine learning
models that power personalisation. Online controlled experiments,
the modern web-based extension of Randomised Controlled Trials
(RCTs) [ 32], provide an effective tool to allow for confident decision-
making in this context [22] (bar some common pitfalls [16, 21]).
A North Star metric is adopted, such as long-term revenue or
user retention, and system variants that statistically significantly
improve the North Star metric are considered superior to the tested
alternative [ 7]. Proper use of statistical hypothesis testing tools
such as Welch’s 𝑡-test [ 42], then allows us to define and measure
statistical significance in a mathematically rigorous manner.
However effective this procedure is, it is far from efficient. Indeed,
experiments typically need to run for a long time, and statistically
significant changes to the North Star are scarce. This can either
be due to false negatives (i.e. type-II error), or simply because the
North Star is not moved by short-term experiments. In these cases,
we need to resort to second-tier metrics (e.g. various types of user
engagement signals) to make decisions instead. These problems
are common in industry, as evidenced by a wide breadth of related
work. A first line of research leverages control variates to reduce the
variance of the North Star metric, directly reducing type-II errors by
increasing sensitivity [ 2,3,8,12,29,43]. Another focuses on identi-
fying second-tier “proxy ” or “surrogate ” metrics that are promising
to consider instead of the North Star [ 31,37,41], or to predict long-
term effects from short-term data [ 1,11,36]. Finally, several works
learn metric combinations that maximise sensitivity [7, 19, 37].
This paper synthesises, generalises and extends several of the
aforementioned works into a general framework to learn A/B-
testing metrics that maximise the statistical power they harness.
We specifically extend the work of Kharitonov et al .[19] to appli-
cations beyond web search, where the North Star can be delayed
and insensitive. We highlight how their approach of maximising
the average 𝑧-score does not accurately reflect downstream metric
utility in our case, in that it does not penalise disagreement with
the North Star sufficiently (i.e. type-III/S errors [ 10,18,27,38]).
Indeed: whilst this approach maximises the mean𝑧-score, it does
not necessarily improve the median𝑧-score, and does not lead to
improved statistical power in the form of reduced type-II error as a
result.
Alternatively, optimising the learnt metric to minimise 𝑝-values
—either directly or after applying a log-transformation— more equi-
tably ditributes gains over multiple experiments, leading to more
statistically significant results instead of a few extremely signifi-
cant results. Furthermore, we emphasise that learnt metrics are not
meant to replace existing metrics, but rather to complement them.
5183
KDD ’24, August 25–29, 2024, Barcelona, Spain Olivier Jeunen and Aleksei Ustimenko
As such, their evaluation should be done through multiple hypoth-
esis testing (with appropriate corrections [ 34]) ifanyof the North
Star, available vetted proxies and surrogates, orlearnt metrics are
statistically significant under the considered treatment variant. We
can then either adopt a conservative plug-in Bonferroni correction
to temper type-I errors, or analyse synthetic A/A experiments to
ensure the final procedure matches the expected confidence level.
We empirically validate these insights through two dataset of
past logged A/B results from large-scale short-video platforms with
over 160 million monthly active users each: ShareChat and Moj.
Experimental results highlight that our learnt metrics provide sig-
nificant value to the business: learnt metrics can increase statistical
power by up to 78% over the North Star, and up to 210% when used
in tandem. Alternatively, if we wish to retain constant statistical
power as we do under the North Star, we can do so with down to
12% of the original required sample size. This significantly reduces
the cost of online experimentation to the business. Our learnt met-
rics are currently used for confident, high-velocity decision-making
across ShareChat and Moj business units.
2 Background & Problem Setting
We deal with online controlled experiments, where two system vari-
ants𝐴and𝐵are deployed to a properly randomised sub-population
of users, adhering to best practices [16, 22].
For every system variant, for every experiment, we measure
various metrics that describe how users interact with the platform.
These metrics include types of implicit engagement (e.g. video-
plays and watch time), as well as explicit engagement (e.g. likes
and shares) as well as longer-term retention or revenue signals. For
each metric, we log empirical means, variances and covariances (of
the sample mean). For metrics 𝑚𝑖with 1≤𝑖≤𝑁, that is:
𝝁=[𝜇1,...𝜇𝑁],and𝚺=𝜎1... 𝜎 1𝑖... 𝜎 1𝑁
...............
𝜎𝑖1... 𝜎𝑖... 𝜎𝑖𝑁
...............
𝜎𝑁1... 𝜎𝑁𝑖... 𝜎𝑁.(1)
Superscripts denote measurements pertaining to different variants
in an experiment: e.g. 𝝁𝐴and𝝁𝐵.
2.1 Statistical Significance Testing
We want to assess whether the mean of metric 𝑚𝑖is statistically
significantly higher under variant 𝐴compared to variant 𝐵. To this
end, we define a significance level 𝛼(often𝛼≈0.05), correspond-
ing to the false-positive-rate we deem acceptable. Then, we apply
Welch’s𝑡-test. The test statistic (also known as the 𝑧-score) for
metric𝑚𝑖and the given variants is given by:
𝑧𝐴≻𝐵
𝑖=𝜇𝐴
𝑖−𝜇𝐵
𝑖√︃
𝜎𝐴
𝑖+𝜎𝐵
𝑖. (2)
We then transform this to a 𝑝-value for a two-tailed test as:
𝑝𝐴≠𝐵
𝑖=2·min(Φ(𝑧𝐴≻𝐵
𝑖); 1−Φ(𝑧𝐴≻𝐵
𝑖)). (3)Here,𝐴≻𝐵denotes a partial ordering between variants, implying
that𝐴is preferred over 𝐵.Φ(·)represents the cumulative distribu-
tion function (CDF) for a standard Gaussian. For completeness, this
CDF is given by:
Φ(𝑧)=1√
2𝜋∫𝑧
−∞𝑒−𝑡2
2d𝑡. (4)
When𝑝𝐴≠𝐵
𝑖<𝛼, we can confidently reject the null-hypothesis
that𝐴and𝐵are equivalent w.r.t. the mean of metric 𝑚𝑖. Note that
𝑧-scores are signed, whereas two-tailed 𝑝-values are not. Indeed:
relabeling the variants changes the 𝑧-score but not the 𝑝-value,
which leaves room for faulty conclusions of directionality, known
as type-III errors [ 18,27,38] orsign errors [ 10]. We discuss these
phenomena in detail, further in this article.
A one-tailed 𝑝-value for the one-tailed null hypothesis 𝐴⊁𝐵is
given by𝑝𝐴⊁𝐵
𝑖=1−Φ(𝑧𝐴≻𝐵
𝑖), and rejected when <𝛼
2. Throughout,
we use two-tailed 𝑝-values unless mentioned otherwise.
2.1.1𝑝-value corrections. The above procedure is valid for a sin-
gle metric, a single hypothesis, and importantly, a single decision.
Nevertheless, this is not how experiments run in practice. Without
explicit corrections on the 𝑝-values (or corresponding 𝑧-scores), vio-
lations of these assumptions lead to inflated false-positive-rates. We
consider two common cases: a (conservative) multiple testing cor-
rection when an experiment has several treatments, and a sequen-
tial testing correction when experiments have no predetermined
end-date or sample size at which to conclude. These corrections
are applied as experiment-level corrections, to ensure that for any
metric𝑚𝑖and variants 𝐴,𝐵, the obtained 𝑝-values accurately reflect
what they should reflect, yielding the specified coverage at varying
confidence levels 𝛼.
Multiple comparisons. Often, launched experiments will have
multiple treatments deployed, leading to the infamous “multiple
hypothesis testing ” problem [ 34]. We apply a Bonferroni correction
to deal with this. When there are 𝑇treatments, we consider a
treatment to be statistically significantly different from control
when a two-tailed 𝑝<𝛼
𝑇, instead of the original 𝑝<𝛼threshold.
We can equivalently apply this correction on 𝑧-scores instead,
allowing us to directly compare 𝑧-scores across experiments with
varying numbers of treatments. Recall that the percentile point
function is the inverse of the CDF. We obtain a one-tailed 𝑝-value
as𝑝=1−Φ(𝑧), and we reject the one-tailed null hypothesis when
𝑝<𝛼
2. Now, instead, we reject when 𝑝<𝛼
2𝑇. As such, computing
corrected𝑧-scores as ¯𝑧=𝑧Φ−1(𝛼
2)
Φ−1(𝛼
2𝑇)controls type-I errors effectively.
Always-Valid-Inference and peeking. A statistical test should only
be performed once, at the end of an experiment. When the treatment
effect is large, this implies we may have been able to conclude
the experiment earlier. To this end, sequential hypothesis tests
have been proposed in the literature [ 40]. Modern versions make
use of Always-Valid-Inference (AVI) [ 13] to allow for continuous
peeking at intermediate results and making decisions based on them,
whilst controlling type-I errors. Here, analogously, we can apply a
5184Learning Metrics that Maximise Power for Accelerated A/B-Tests KDD ’24, August 25–29, 2024, Barcelona, Spain
correction on the 𝑧-scores as follows:
¯𝑧=𝑧√︂
(𝑁𝐴𝐵+𝜌)
𝑁𝐴𝐵log𝑁𝐴𝐵+𝜌
𝜌𝛼2, 𝜌=10 000
log(log(𝑒
𝛼2))−2 log(𝛼),
(5)
where𝑁𝐴𝐵is the total number of samples over both variants com-
bined. For a detailed motivation, see Schmit and Miller [33].
These corrections are applied on a per-experiment level, both
in the objective functions of methods introduced in the following
Sections and when evaluating the metrics that they produce.
2.2 Learning Metrics that Maximise Sensitivity
The observation that we can learn parameters to maximise statis-
tical sensitivity is not new. Yue et al .apply such ideas specifically
for interleaving experiments in web search [ 44].Kharitonov et al .
extend this to A/B-testing in web search, aiming to learn combina-
tions of metrics that maximise the average 𝑧-score [ 19].Deng and
Shidiscuss “lessons learned” from applying similar techniques [ 7].
We introduce the approach presented by Kharitonov et al .[19], as
our proposed improvements build on their foundations.
We consider new metrics as linear transformations on 𝝁:
𝜔=𝝁𝑤⊺,where𝑤∈R1×𝑁. (6)
The advantage of restricting ourselves to linearity, it that we can
write out the 𝑧-score of the new metric as a function of its weights:
𝑧𝐴≻𝐵
𝜔=𝝁𝐴𝑤⊺−𝝁𝐵𝑤⊺
√︁
𝑤𝚺𝐴𝑤⊺+𝑤𝚺𝐵𝑤⊺. (7)
These𝑧-scores can be used exactly as before to obtain 𝑝-values.
An intuitive property of the 𝑧-score, is that a relative 𝑧-score of
𝑟=𝑧𝐴≻𝐵
𝜔
𝑧𝐴≻𝐵
𝑖implies that 𝜔requires a factor 𝑟2fewer samples to
achieve the same significance level as 𝑚𝑖[4]. This can directly
be translated to the cost of experimentation, as it allows us to run
experiments for shorter time-periods or on smaller sub-populations.
As such, it comes naturally to frame the objective as learning
the weights 𝑤that maximise the 𝑧-score on the training data. This
training data consists of a set of experiments with pairs of variants
E={(𝐴,𝐵)𝑖}|E|
𝑖=1. We consider three distinct relations between pairs
of deployed system variants:
(1)Known outcomes:(𝐴,𝐵)∈E+, where𝐴≻𝐵,
(2)Unknown outcomes:(𝐴,𝐵)∈E?, where𝐴?𝐵,
(3)A/A outcomes:(𝐴,𝐵)∈E≃, where𝐴≃𝐵.
Here,𝐴≻𝐵implies that there is a known and vetted preference
of variant𝐴over𝐵— typically because the North star or other
guardrail metrics showed statistically significant improvements.
These experiments are further validated by replicating outcomes,
observing long-term holdouts, or because the experiment was part
of an intentional degradation test. We denote inconclusive exper-
iments as𝐴?𝐵, implying statistically insignificant outcomes on
the North Star. In rare cases, the North star might have gone up at
the expense of important guardrail metrics, rendering conclusions
ambiguous. We only include experiments into the inconclusive set
for which we have a very strong intuition that something changed
(and we “know” the null hypothesis should be rejected), but we are
unable to make a confident directionality decision. This ensuresthat we can use this set to truly measure type-II errors. Finally,
𝐴≃𝐵represents A/A experiments, where we know the null hy-
pothesis to hold true (by design). The first set of experiments is
used to measure type-III/S errors. Known and unknown outcomes
are used to measure type-II errors, and A/A experiments can in-
form us about type-I errors. This dataset of past A/B experiments is
collected and labelled by hand, from natural experiments occurring
on the platform over time.
2.2.1 Optimising Metric Weights with a Geometric Heuristic. Note
that𝑧as a function of 𝑤isscale-free. That is, the direction of the
weight vector 𝑤matters, but its scale does not. As Kharitonov et al .
write [ 19], we can compute the optimal direction of 𝑤using the
Lagrange multipliers method, to obtain:
𝑤★
𝐴≻𝐵∝(𝚺𝐴+𝚺𝐵+𝜖𝐼)−1(𝝁𝐴−𝝁𝐵). (8)
Here,𝜖∈Ris a small number to ensure that the matrix to be
inverted is not singular. Kharitonov et al .fix this value at 𝜖=0.01
and never adjust it throughout the paper. We wish to highlight
that technique is known as Ledoit-Wolf shrinkage [ 23,24], and
that it can have substantial influence on the obtained direction.
Indeed: it acts as a regularisation term pushing the weights closer
to𝑤=(𝝁𝐴−𝝁𝐵). This can be seen by observing that as 𝜖→inf, the
inverse becomes1
𝜖𝐼, and the solution hence becomes 𝑤=(𝝁𝐴−𝝁𝐵)
𝜖.
As we only care about the direction we can ignore the denominator.
To ensure fair comparison, we also set 𝜖=0.01. Exploring the effects
of Ledoit-Wolf shrinkage as a regularisation technique where 𝜖is a
hyper-parameter, gives an interesting avenue for future work.
In order to include observations from multiple experiments into
a single set of learned weights, they propose to compute the optimal
direction per experiment, normalise, and average the weights:
𝑤★
E+=1
|E+|∑︁
(𝐴,𝐵)∈E+𝑤★
𝐴≻𝐵𝑤★
𝐴≻𝐵
2. (9)
Whilst this procedure provides no guarantees about the sensitivity
of the obtained metric on the overall set of experiments, it is efficient
to compute and provides a strong baseline method.
2.2.2 Optimising Metric Weights via Gradient Ascent on 𝑧-scores. A
more principled approach is to cast the above as an optimisation
problem. The objective function for this optimisation problem con-
sists of three parts. First, we wish to maximise the𝑧-score for all
variant pairs with known outcomes:
L+
𝑧(𝑤;E+)=1
|E+|∑︁
(𝐴,𝐵)∈E+𝑧𝐴≻𝐵
𝜔.(10)
Second, we wish to maximise theabsolute𝑧-score for all variant
pairs with inconclusive outcomes under the North Star:
L?
𝑧(𝑤;E?)=1
|E?|∑︁
(𝐴,𝐵)∈E?𝑧𝐴≻𝐵
𝜔. (11)
Third, we wish to minimise theabsolute𝑧-score for all variant pairs
that are equivalent (i.e. A/A-pairs):
L≃
𝑧(𝑤;E≃)=1
|E≃|∑︁
(𝐴,𝐵)∈E≃𝑧𝐴≻𝐵
𝜔. (12)
5185KDD ’24, August 25–29, 2024, Barcelona, Spain Olivier Jeunen and Aleksei Ustimenko
This gives rise to a combined objective as a weighted average:
L𝑧(𝑤;E)=L+
𝑧(𝑤;E+)+𝜆?L?
𝑧(𝑤;E?)−𝜆≃L≃
𝑧(𝑤;E≃).(13)
Kharitonov et al .demonstrate that, for a variety of different metrics
in a web search engine, these approaches can exhibit improved
sensitivity [ 19]. We apply this method to learn instantaneously
available proxies to a delayed North Star metric in general scenarios,
and propose several extensions, detailed in the following Sections.
3 Methodology & Contributions
3.1 Learning Metrics that Maximise Power
When directly optimising 𝑧-scores, an implicit assumption is made
that the utility we obtain from increased 𝑧-scores is linear. This is
seldom a truthful characterisation of reality, considering how we
wish to actually usethese metrics downstream. We provide a toy
example in Table 1, reporting 𝑧-scores and one-tailed 𝑝-values for
two experiments and three possible metrics 𝑚1,𝑚2,𝑚3, inspired
by real data. In this toy example, we know that 𝐴≻𝐵, based on a
hypothetical North Star metric. Nevertheless, as we do not know
this beforehand, we typically test for the null hypothesis 𝐴≃𝐵with
two-tailed𝑝-values. In the table, this means that the outcome is
statistically significant if the reported one-tailed values are 𝑝<𝛼
2
or𝑝>1−𝛼
2. We report the power of every metric at varying
significance levels 𝛼∈{0.05,0.01}, reporting whether (i) the null
hypothesis is correctly rejected ( 𝑝<𝛼
2,✓), (ii) the outcome is
inconclusive (?), i.e. a type-II error, or (iii) the null hypothesis is
rejected, but for the wrong reason (𝑝>1−𝛼
2,×).
This latter case is deeply problematic, as it signifies disagreement
with the North Star. Such errors have been described as type-III or
type-S errors in the statistical literature [ 10,18,27,38]. Naturally, we
would rather have a metric that fails to reject the null than one that
confidently declares a faulty variant to be superior. Indeed, Deng
and Shi argue that both directionality andsensitivity are desirable
attributes for any metric [ 7]. Nevertheless, considering candidate
metrics in Table 1, type-III errors are not sufficiently penalised by
the average 𝑧-score: metric 𝑚3maximises this objective despite
yielding statistical power that is on par with a coin flip.
Directly maximising power might prove cumbersome, as it is
essentially a discrete step-function w.r.t. the 𝑧-score, dependent
on the significance level 𝛼. Instead, it comes natural to minimise
the one-tailed 𝑝-value reported in Table 1. Indeed, the 𝑝-value
transformation models diminishing returns for high𝑧-scores, which
allows type-III errors to be sufficiently penalised. When considering
this objective, 𝑚3is clearly suboptimal whilst 𝑚2is preferred.
Note that the change in objective would not affect the geomet-
ric heuristic as described in Section 2.2.1. As we simply apply a
monotonic transformation on the 𝑧-scores, the weight direction that
maximises the 𝑧-score equivalently minimises its 𝑝-value. When
learning via gradient descent, however, the 𝑝-value transformation
affects how we aggregate and attribute gains over different input
samples. This allows us to stop focusing on increasing sensitiv-
ity for experiments that are already “sensitive enough”, and more
equitably consider all experiments in the training data.
This change in objective provides an intuitive and efficient ex-
tension to existing approaches, allowing us to directly optimise
the confidence we have to correctly reject the null-hypothesis. For𝑧-score𝑝-value Power
𝛼=0.05𝛼=0.01
𝑚1Exp. 1 1.97 2.44e−2 ✓ ?
Exp. 2 1.97 2.44e−2 ✓ ?
Mean 1.97 2.44e−2 100% 0%
𝑚2Exp. 1 1.90 2.87e−2 ? ?
Exp. 2 3.50 2.33e−4 ✓ ✓
Mean 2.70 1.45e−2 50% 50%
𝑚3Exp. 1−2.58 9.95e−1× ×
Exp. 2 8.00 6.66e−16 ✓ ✓
Mean 5.29 4.98e−1 50% 50%
Table 1: An example setting highlighting that maximising
average𝑧-scores (𝑚1) does not imply improved statistical
power. Statistical power is dependent on the significance
level𝛼,𝑝-values provide a good proxy to optimise for. We
observe similar trade-offs from real past A/B experiments.
−3−2−1 0 1 2 3
z-score0246Lossp-value
−p(log(1−p))
p∈{0.025,0.975}
Figure 1: Visualising our proposed optimisation objectives
for learnt metrics, as a function of their 𝑧-score.
known outcomes, the loss function is given by:
L+
𝑝(𝑤;E+)=1
|E+|∑︁
(𝐴,𝐵)∈E+1−Φ(𝑧𝐴≻𝐵
𝜔)
=1
|E+|∑︁
(𝐴,𝐵)∈E+1−Φ 
𝝁𝐴𝑤⊺−𝝁𝐵𝑤⊺
√︁
𝑤𝚺𝐴𝑤⊺+𝑤𝚺𝐵𝑤⊺!
,(14)
and analogously extended to unknown L?𝑝and A/A-outcomes L≃𝑝.
Nevertheless, we wish to point out that we only want to maximise
𝑝-values for A/A-outcomes if type-I error becomes problematic. As
we will show empirically in Section 4.3, this is not a problem we
encounter. For this reason, we set 𝜆≃≡0.
Note that whilst direct optimisation of 𝑝-values is an improve-
ment over myopic consideration of 𝑧-scores, there is another caveat:
the “worst-case” loss of a type-III/S error is bounded at 1, which
does not reflect our true utility function: metrics that disagree with
the North Star are farless reliable than those that simply remain
inconclusive. As such, we also consider another variant of the ob-
jective, where ¯𝑝=−𝑝log(1−𝑝). Figure 1provides visual intuition
to clarify how this monotonic transformation on the 𝑝-values more
heavily penalises type-III/S errors, whilst retaining the optimum.
From a theoretical perspective, this function provides a convex
relaxation for minimising the number of type-III/S errors a metric
produces. As a result, we expect this surrogate to exhibit strong gen-
eralisation. We refer to this objective as minimising the log𝑝-value.
5186Learning Metrics that Maximise Power for Accelerated A/B-Tests KDD ’24, August 25–29, 2024, Barcelona, Spain
Note that one could envision further extensions here where
the significance level 𝛼isdirectly incorporated into the objective
function to maximise statistical power at a given significance level 𝛼.
Nevertheless, we conjecture that their discrete nature might hamper
effective optimisation and generalisation, compared to the strictly
convex and smooth surrogate we obtain from the log𝑝-value.
3.2 Accelerated Convergence for Scale-Free
Objectives via Spherical Regularisation
The objective functions we describe —either 𝑧-scores or(log)𝑝-
values— are scale-free w.r.t. the weights that are being optimised.
As a result, out-of-the-box gradient-based optimisation techniques
are not well-equipped to handle this efficiently.
Consider a simple toy example where we have two observed
metrics for an experiment with a known preference 𝐴≻𝐵, and:
𝝁𝐴=[1.0,1.0], 𝝁𝐵=[0.5,0.5], 𝚺𝐴=𝚺𝐵=𝐼. (15)
For this low-dimensional problem, we can visualise the 𝑧-score
as a function of the metric weights in a contour plot, as shown in
Figure 2a. Here, it becomes visually clear that whilst the direction of
the𝑤=[𝑤1,𝑤2]vector matters, its scale does not. The consequence
is that the gradient vectors w.r.t. the objective on the right-hand plot
can lead to slow convergence, even in this concave objective. Indeed,
for poor initialisation in the bottom left quadrant (e.g. 𝑤=[−1,−2]),
the gradient direction is perpendicular w.r.t. the optima.
Recent work makes a similar observation for discrete scale-free
objectives as they appear in ranking problems [ 39]. They propose to
adopt projected gradient descent, normalising the gradients before
every update. Whilst effective, in our setting we would prefer to use
out-of-the-box optimisation methods for practitioners’ ease-of-use.
Instead, we introduce a simple regularisation term that represents
the distance between the scale of the 𝑤vector and a hyper-sphere:
L∥𝑤∥=−𝛿
𝑁−∥𝑤∥2
22
. (16)
All optima for this objective function are also optima to the origi-
nal function—but the gradient field is more amenable to iterative
gradient-based optimisation techniques. Figure 2bvisualises how
this transforms the loss surface. Under this regularised objective,
it is visually clear that gradient-based optimisation methods are
likely to exhibit faster convergence. Our empirical results confirm
this, for a variety of initialisation weights and learning objectives.
4 Experiments & Discussion
To empirically validate the methods proposed in this work, we
require a dataset containing logged metrics (sample means, their
variances and covariances), together with preference orderings over
competing system variants that were collected from real-world A/B-
tests, ideally spanning large user-bases and several weeks.
Existing work on this topic used a private dataset from Yandex fo-
cused on web search experiments that ran between 2013–2014 [ 19].
They report type-I and -II errors for 8 metrics and a fixed 5% signif-
icance level, over 118 A/B-tests and 472 A/A-tests.
In this work, we consider more general metrics that are relevant
for use-cases beyond web search (i.a. user retention and various
engagement signals). Furthermore, we report type-I/II/III/S errorsat varying significance levels, providing insights into the learnt met-
rics’ behaviour. For this, we leverage logs of past A/B-experiments
on two large-scale short-video platforms with over 160 million
monthly active users each: ShareChat and Moj. The datasets consist
of 153 A/B-experiments (of which 58 were conclusive) total that
ran in 2023, and over 25 000 A/A-pairs. In total, we have access to
roughly 100 metrics detailing various types of interactions with
the platform, engagements, and delayed retention signals. Because
our dataset is limited in size (a natural consequence of the problem
domain), we are bound to overfit when using all available metrics
as input features. As such, we limit ourselves to 10 input metrics to
learn from, and evaluate them w.r.t. the delayed North Star. This
feature selection step also ensures that our linear model consists
of fewer parameters, which increases practitioners’ and business
stakeholders’ trust in its output. We focus on non-delayed signals,
including activity metrics such as the number of sessions and active
days, and counters for positive and negative feedback engagements
of various types. These are selected through an analysis of their
type-I/II/III/S errors w.r.t. the North Star, as well as their 𝑧-scores:
focusing on metrics with high sensitivity and limited disagreement.
The research questions we wish to answer empirically using this
data, are the following:
RQ1 Do learnt metrics effectively improve on their objectives?
RQ2 How do learnt metrics behave in terms of type-III/S errors?
RQ3 How do learnt metrics’ type-I/II errors behave when considered
as stand-alone evaluation metrics?
RQ4 How do learnt metrics’ type-I/II errors behave when used in
conjunction with the North Star and top proxy metrics?
RQ5 How do learnt metrics influence required sample sizes when
used in conjunction with the North Star and top proxy metrics?
RQ6 Do we observe accelerated convergence over varying objectives
via the proposed spherical regularisation technique?
We report results for the ShareChat platform in what follows,
and provide further empirical results for Moj in Appendix A.
4.1 Effectiveness of Learnt Metrics (RQ1)
We learn and evaluate metrics through leave-one-out cross-validation:
for every experiment, we train a model on all other experiments and
evaluate the 𝑧-score (Eq. 7) and𝑝-value (Eq. 3) the metric yields for
the held-out experiment. We report the mean and median 𝑧-scores
and𝑝-values we obtain for all A/B-pairs with known outcomes (i.e.
E+) in Table 2. Best performers for every column (either maximising
𝑧-scores or minimising 𝑝-values) are highlighted in boldface. Em-
pirical observations match our theoretical expectations: whilst the
𝑧-score objective does effectively maximise the average𝑧-score, it is
the worst performer for both mean and median 𝑝-values, and even
the median𝑧-score. Our proposed log𝑝-value objective effectively
improves both the median 𝑧-score and𝑝-value over alternatives.
4.2 Agreement with the North Star (RQ2)
From the obtained 𝑧-scores and 𝑝-values summarised in Table 2,
we can additionally derive (dis-)agreement with the North Star, for
varying significance levels 𝛼. We visualise these results in Figure 3:
if the obtained 𝑝-value under a learnt metric is lower than 𝛼, that
metric yields a statistically significant result (agreement). If the
obtained𝑝-value for the alternative hypothesis (i.e. 𝐵≻𝐴when
5187KDD ’24, August 25–29, 2024, Barcelona, Spain Olivier Jeunen and Aleksei Ustimenko
−2 0 2
w1−3−2−10123w2
−0.495−0.475
−0.450−0.400−0.350−0.300
−0.250−0.200
−0.150−0.100
−0.0500.000
0.0500.100
0.1500.2000.250
0.3000.3500.4000.450
0.4750.495
−2 0 2
w1w2
∇z
w⋆
−0.50.00.5
(a) Original scale-free objective function.
−2 0 2
w1−3−2−10123w2
−0.500−0.450−0.400−0.350
−0.300−0.250−0.200
−0.150−0.100−0.050
0.0000.050
0.1000.1500.200
0.2500.300
0.3500.400
0.4000.4500.475
0.495
−2 0 2
w1w2
∇z
w⋆−0.50.00.5
 (b) With added spherical regularisation ( 𝛿=5e−4).
Figure 2: Directly maximising 𝑧-scores yields a scale-free objective that is not amenable to efficient optimisation with gradient
ascent (left ). Spherical regularisation retains all optima, whilst providing a gradient direction that is more aligned (right ).
0% 20% 40% 60% 80% 100%w⋆(p-value)w⋆(logp-value)w⋆(heuristic)w⋆(z-score)α= 1e−01
Agreement Inconclusive ( type-II ) Disagreement ( type-III )0% 20% 40% 60% 80% 100%α= 5e−02
0% 20% 40% 60% 80% 100%α= 1e−02
0% 20% 40% 60% 80% 100%α= 1e−03
0% 20% 40% 60% 80% 100%α= 1e−04
Figure 3: Learnt metrics and their agreement with known North Star outcomes over varying significance levels. A learnt metric
that maximises the average 𝑧-score exhibits significant type-III/S error, which can be alleviated by minimising 𝑝-values instead.
10−410−310−210−1
α10−510−410−310−210−1Type-I Error
Expected (α) North Star Top-Proxy w⋆(p-value) w⋆(logp-value)10−410−310−210−1
α0.50.60.70.8Type-II Error
Sensitivity & Power for Stand-alone Metrics
(a) False-positive rate (type-I error) and false-negative rate (type-II error, 1−power) for stand-alone metrics.
10−410−310−210−1
α10−510−410−310−210−1Type-I Error
10−410−310−210−1
α0.30.40.50.60.70.8Type-II Error
North Star + Top-Proxy or +w⋆(p−value) or +w⋆(logp−value)Sensitivity & Power for Sets of Metrics (a/f_ter Bonferroni correction)
(b) False-positive rate (type-I error) and false-negative rate (type-II error, 1−power) for sets of metrics, after Bonferroni correction.
Figure 4: When considering only learnt metrics, we improve type-II error significantly without hurting specificity. At 𝛼=0.05, we
increase statistical power by 67% (upper plot ). In conjunction with the North Star and top proxy metrics, Bonferroni corrections
are slightly conservative (type-I error <𝛼), and allow us to improve statistical power by 133% for 𝛼=0.05(lower plot ).
5188Learning Metrics that Maximise Power for Accelerated A/B-Tests KDD ’24, August 25–29, 2024, Barcelona, Spain
Objective 𝑧-score (↑) 𝑝-value (↓)
Mean Median Mean Median
heuristic 7.31 3.07 1.88e−1 1.18e−3
𝑧-score 7.55 2.67 2.33e−1 3.88e−3
𝑝-value 5.22 3.08 4.32e−2 1.09e−3
log𝑝-value 4.33 3.17 5.19e−2 8.60e−4
Table 2: Sensitivity results for learnt metrics, computed
via leave-one-out cross-validation on all experiments with
known outcomes. We observe that minimising the log𝑝-value
effectively improves median sensitivity over alternatives.
we know𝐴≻𝐵) is lower than 𝛼, we have statistically significant
disagreement, or type-III error. This is a capital sin we wish to
avoid at all costs, as it severely diminishes the trust we can put in
the learnt metric. If the 𝑝-value reveals a statistically insignificant
result, we say the result is inconclusive, implying a type-II error. We
observe that both the 𝑧-score-maximising metric and the heuristic
approach fail to steer clear from type-III error. Optimising ( log)𝑝-
values instead, alleviates this issue. For this reason, we only consider
these metrics for further evaluation. Indeed: an analysis of type-II
error is rendered meaningless when type-III errors are present.
Figure 7ain Appendix Ahighlights that for the Moj platform as
well, type-III errors are common in the case of 𝑧-score-maximising
or heuristic metrics. As such, we only consider ( log)𝑝-values to
assess increases in statistical power and potential reductions to the
cost of running online experiments.
4.3 Power Increase from Learnt Metrics (RQ3–4)
Until now, we have leveraged experiments with known outcomes
to assess sensitivity and agreement with the North Star. Now, we
additionally consider A/A-experiments ( E≃) and experiments with
unknown outcomes (E?) to measure type-I and type-II error respec-
tively. We measure this for the North Star, for the best-performing
proxy metric that serves as input to the learnt metrics, and for learnt
metrics that exhibit no empirical disagreement with the North Star.
We plot the type-I error (i.e. fraction of A/A-pairs in E≃that are
statistically significant at significance level 𝛼) and the type-II error
(i.e. fraction of A/B-pairs in {E+∪E?}that are statistically insignifi-
cant at significance level 𝛼) for varying values of 𝛼in Figure 4a. We
observe that we are able to significantly reduce type-II errors com-
pared to the North Star (up to 78%), whilst keeping type-I errors at
the required level (i.e. 𝛼). However, we also observe that the type-II
error we obtain when using learnt metrics does not significantly
improve over the top proxy metric, when considered in isolation.
Nonetheless, this is not how evaluation metrics are used in prac-
tice. Indeed, we track several metrics and can draw conclusions if
anyof them are statistically significant. As such, metrics should be
evaluated on their complementary sensitivity. That is, we compute
𝑝-values for a set of metrics, apply a Bonferroni correction, and
assess statistical significance. The statistical power that we obtain
through this procedure is visualised in Figure 4b. We consider either
the North Star in isolation, the North Star in conjunction with the
top-proxy, or a further combination with any learnt metric. Here,
we observe that the learnt metric provides a substantial increase in
statistical power: statistical power (i.e. 1−type-II error) is increased
by up to a relative 210% compared to the North Star alone, and
10−410−310−210−1
α12345678Median Rel. z2
North Star
+ Top-Proxy
or +w⋆(p−value)
or +w⋆(logp−value)Relative Sample Size Decrease for Sets of Metrics (a/f_ter Bonferroni)Figure 5: When considering learnt metrics in conjunction
with the North Star and top proxy metrics, we require signif-
icantly reduced sample sizes to obtain the same statistical
significance level as we would get from the North Star.
25–30% over the North Star plus proxies. Furthermore, as the Bon-
ferroni correction is slightly conservative, we observe lower than
expected type-I error for higher significance levels 𝛼. This implies
that a more fine-grained multiple testing correction can further
improve statistical power. We empirically observe that this works
as expected, but its effects are negligible in practice.
4.4 Cost Reduction from Learnt Metrics (RQ5)
So far, we have shown that metrics learnt to minimise ( log)𝑝-values
are effective at improving sensitivity (Table 2), whilst minimising
type-III error (Figure 3) and improving statistical power (Figure 4).
On one hand, powerful learnt metrics can lead to more con-
fident decisions from statistically significant A/B-test outcomes.
Another view is that we could make the same amount of decisions
based on fewer data points, as we reach statistical significance with
smaller sample sizes. This implies a cost reduction, as we can run
experiments either on smaller portions of user traffic or for shorter
periods of time, directly impacting experimentation velocity.
This reduction in required sample size is equal to the square of
the relative 𝑧-score [ 4,19]. We visualise this quantity in Figure 5,
for varying significance levels 𝛼, for the same Bonferroni-corrected
procedure as Figure 4b. To obtain a 𝑧-score for a setof metrics, we
simply take the maximal score and apply a Bonferroni correction
to it as laid out in Section 2.1.1. Note that this procedure depends
on𝛼, explaining the slope in Figure 5. We observe that our learnt
metrics can achieve the same level of statistical confidence as the
North Star with up to 8 times fewer samples, i.e. a reduction down
to12.5%. This significantly reduces the cost of experimentation for
the business, further strengthening the case for our learnt metrics.
4.5 Spherical Regularisation (RQ6)
Our goal is to assess and quantify the effects of the proposed spheri-
cal regularisation method in Section 3.2. We train models on all avail-
able data with known outcomesE+, where we have a vetted prefer-
ence over variants 𝐴≻𝐵. We consider three weight initialisation
strategies to set 𝑤init, and normalise weights to ensure L∥𝑤init∥=0:
(i)good initialisation at 𝑤init=1
|E+|Í
(𝐴,𝐵)∈E+𝝁𝐴−𝝁𝐵, (ii) con-
stant initialisation at the all-one vector 𝑤init=®1, and (iii) bad
initialisation at 𝑤init=1
|E+|Í
(𝐴,𝐵)∈E+𝝁𝐵−𝝁𝐴. We train mod-
els for all learning objectives we deal with in this paper: 𝑧-scores,
5189KDD ’24, August 25–29, 2024, Barcelona, Spain Olivier Jeunen and Aleksei Ustimenko
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
×104−0.50.00.51.0Averagez-score×101
 winit=−µA−µB
0.0 0.5 1.0 1.5 2.0
×1040.900.951.00×101
 winit= 1
δ= 0e+ 00
δ= 1e−05
δ= 1e−04
δ= 1e−03
δ= 5e−03Converged
Converged
Converged
Converged
Converged
0.0 0.5 1.0 1.5 2.0 2.5
×1040.850.900.951.00×101
 winit=µA−µB
0 1 2 3 4
×10410−1100Averagep-value
0.0 0.5 1.0 1.5 2.0 2.5 3.0
×10410−1
3×10−24×10−26×10−2
δ= 0e+ 00
δ= 1e−08
δ= 5e−08
δ= 1e−07
δ= 1e−06Converged
Converged
Converged
Converged
Converged
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
×10410−1
3×10−24×10−26×10−2
0 1 2 3 4 5 6
Optimisation steps×10410−210−1100101Average logp-value
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Optimisation steps×10410−2
δ= 0e+ 00
δ= 1e−08
δ= 1e−07
δ= 1e−06
δ= 5e−06Converged
Converged
Converged
Converged
Converged
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
Optimisation steps×10410−210−1
Figure 6: Spherical regularisation significantly accelerates convergence for all considered objectives, up to 40%.
𝑝-values, and log𝑝-values; whilst varying the strength of the spher-
ical regularisation term 𝛿. As discussed, this term does not affect
the optima, but simply transforms the loss to be more amenable to
gradient-based optimisation methods. Thus, we expect convergence
after fewer training iterations. All models are trained until conver-
gence with the adam optimiser [ 20], initialising the learning rate
at5e−4and halving it every 1 000 steps where we do not observe
improvements. We use the radam variant to avoid convergence
issues [ 25,30], and have validated that this choice does not sig-
nificantly alter our obtained results and conclusions. We consider
a model converged if there are no improvements to the learning
objective after 10 000 steps. All methods are implemented using
Python3.9 and PyTorch [28].
Figure 6visualises the evolution of the learning objective over
optimisation steps, for all mentioned learning objectives, initialisa-
tion strategies, and regularisation strengths. We observe that the
method is robust, significantly improving convergence speed for all
settings, requiring up to 40% fewer iterations until convergence is
reached. This positively influences the practical utility of the learnt
metric pipeline for researchers and practitioners.
We provide source code to reproduce Figure 2and our regularisa-
tion method at github.com/olivierjeunen/learnt-metrics-kdd-2024.
5 Insights from Learnt Metrics
In this Section, we briefly discuss insights that arose through our
empirical evaluation of all metrics: the North Star, classical sur-
rogates and proxies, as well as learnt metrics. These insights are
specific to our platforms, but we believe they can contribute to a
general intuition and understanding of metrics for online content
platforms and broader application areas.Ratio metrics are easily fooled. Often, important metrics can be
framed as a ratio of the means (or sums) of two existing met-
rics [ 2,3]. Examples include click-through rate (i.e. clicks / im-
pressions), variants of user retention (i.e. retained users / active
users), or general engagement ratios (e.g. likes / video-plays). We
observe that, whilst these metrics can be important from a business
perspective, they typically exhibit significant type-III/S errors w.r.t.
the North Star. Indeed, in the examples above both the numerator
and denominator represent positive signals we wish to increase.
Suppose an online experiment increases the number of video-plays
by𝑌%, and the overall number of likes by 𝑋%<𝑌%. These two
positive signals will lead to a decreasing ratio, whilst we are likely
to still prefer the treatment w.r.t. the North Star if 𝑋and𝑌are
substantially large. Similar observations cautioning the use of ratio
metrics have been made by Dmitriev et al. [9].
We believe this is connected to common offline ranking evalua-
tion metrics prevalent in the recommender systems field [ 14,35].
Indeed, such metrics are cumulative in nature, optimising overall
value instead of some notion of value-per-item [17].
User-level aggregations conquer general counters. In the previous
example, we describe general count metrics for the number of likes
and the number of video-play events. User behaviour on online
platforms often follows a power-law distribution: a few “power
users” generate the majority of such events [ 5]. As a result, such
metrics are easily skewed, and they are not guaranteed to accurately
reflect improvements for the full population of users—empirically
leading to type-III/S errors w.r.t. the North Star. Aggregating such
counters per users (e.g. count the number of days a user has at least
𝑋video-plays) instead of using raw event counters, provides strong
and sensitive proxies to the North Star.
Interestingly, this framing is reminiscent of recall, as we effec-
tively measure the coverage of users about whom we have positive
5190Learning Metrics that Maximise Power for Accelerated A/B-Tests KDD ’24, August 25–29, 2024, Barcelona, Spain
signals. Recall metrics are again strongly connected to offline eval-
uation practices in recommender systems, especially in the first
stage of two-stage systems common in industry [6, 26].
6 Conclusions & Outlook
A/B-testing is a crucial tool for decision-making in online busi-
nesses, and it has widely been adopted as a go-to approach to allow
for continuous system improvements. Notwithstanding their popu-
larity, online experiments are often expensive to perform. Indeed:
many experiments lead to statistically insignificant outcomes, pre-
senting an obstacle for confident decision-making. Experiments
that do lead to significant outcomes are costly too: by their very def-
inition, a portion of user traffic interacts with a sub-optimal system
variant. As such, we want to maximise the number of decisions we
can make based on the experiments we run, and we want to min-
imise the required sample size for statistically significant outcomes.
In this work, we achieve this by learning metrics that maximise
the statistical power they harness. We present novel learning ob-
jectives for such metrics, and provide a thorough evaluation of the
effectiveness of our proposed approaches. Our learnt metrics are
currently used for confident, high-velocity decision-making across
ShareChat and Moj business units.
We believe our work opens several avenues for future work
improving the efficacy of learnt metrics, by e.g. relaxing the linearity
constraint we rely on. Furthermore, we wish to leverage our learnt
metrics as reward signals for personalisation through machine
learning models [15].
References
[1]Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019. The
Surrogate Index: Combining Short-Term Proxies to Estimate Long-Term Treatment
Effects More Rapidly and Precisely. Working Paper 26463. National Bureau of
Economic Research. https://doi.org/10.3386/w26463
[2]Shubham Baweja, Neeti Pokharna, Aleksei Ustimenko, and Olivier Jeunen. 2024.
Variance Reduction in Ratio Metrics for Efficient Online Experiments. In Proc. of
the 46th European Conference on Information Retrieval (ECIR ’24). Springer.
[3]Roman Budylin, Alexey Drutsa, Ilya Katsev, and Valeriya Tsoy. 2018. Consistent
Transformation of Ratio Metrics for Efficient Online Controlled Experiments.
InProc. of the Eleventh ACM International Conference on Web Search and Data
Mining (WSDM ’18). ACM, 55–63. https://doi.org/10.1145/3159652.3159699
[4]Olivier Chapelle, Thorsten Joachims, Filip Radlinski, and Yisong Yue. 2012. Large-
Scale Validation and Analysis of Interleaved Search Evaluation. ACM Trans. Inf.
Syst. 30, 1, Article 6 (mar 2012), 41 pages. https://doi.org/10.1145/2094072.2094078
[5]Ed H. Chi. 2020. From Missing Data to Boltzmann Distributions and Time
Dynamics: The Statistical Physics of Recommendation. In Proc. of the 13th In-
ternational Conference on Web Search and Data Mining (WSDM ’20). ACM, 1–2.
https://doi.org/10.1145/3336191.3372193
[6]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for
YouTube Recommendations. In Proc. of the 10th ACM Conference on Recommender
Systems (RecSys ’16). ACM, 191–198. https://doi.org/10.1145/2959100.2959190
[7]Alex Deng and Xiaolin Shi. 2016. Data-Driven Metric Development for Online
Controlled Experiments: Seven Lessons Learned. In Proc. of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD ’16).
ACM, 77–86. https://doi.org/10.1145/2939672.2939700
[8]Alex Deng, Ya Xu, Ron Kohavi, and Toby Walker. 2013. Improving the Sensitivity
of Online Controlled Experiments by Utilizing Pre-Experiment Data. In Proc. of
the Sixth ACM International Conference on Web Search and Data Mining (WSDM
’13). ACM, 123–132. https://doi.org/10.1145/2433396.2433413
[9]Pavel Dmitriev, Somit Gupta, Dong Woo Kim, and Garnet Vaz. 2017. A Dirty
Dozen: Twelve Common Metric Interpretation Pitfalls in Online Controlled
Experiments. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD ’17). ACM, 1427–1436. https:
//doi.org/10.1145/3097983.3098024
[10] Andrew Gelman and John Carlin. 2014. Beyond Power Calculations: Assessing
Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological
Science 9, 6 (2014), 641–651. https://doi.org/10.1177/1745691614551642 PMID:
26186114.[11] Graham Van Goffrier, Lucas Maystre, and Ciarán Mark Gilligan-Lee. 2023. Es-
timating long-term causal effects from short-term experiments and long-term
observational data with unobserved confounding. In Proc. of the Second Con-
ference on Causal Learning and Reasoning (Proc. of Machine Learning Research,
Vol. 213), Mihaela van der Schaar, Cheng Zhang, and Dominik Janzing (Eds.).
PMLR, 791–813. https://proceedings.mlr.press/v213/goffrier23a.html
[12] Yongyi Guo, Dominic Coey, Mikael Konutgan, Wenting Li, Chris Schoener, and
Matt Goldman. 2021. Machine Learning for Variance Reduction in Online Ex-
periments. In Advances in Neural Information Processing Systems, Vol. 34. Curran
Associates, Inc., 8637–8648.
[13] Steven R. Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. 2021.
Time-uniform, nonparametric, nonasymptotic confidence sequences. The Annals
of Statistics 49, 2 (2021), 1055 – 1080. https://doi.org/10.1214/20-AOS1991
[14] Olivier Jeunen. 2019. Revisiting Offline Evaluation for Implicit-Feedback Recom-
mender Systems. In Proc. of the 13th ACM Conference on Recommender Systems
(RecSys ’19). ACM, 596–600. https://doi.org/10.1145/3298689.3347069
[15] Olivier Jeunen. 2021. Offline Approaches to Recommendation with Online Success.
Ph. D. Dissertation. University of Antwerp.
[16] Olivier Jeunen. 2023. A Common Misassumption in Online Experiments with
Machine Learning Models. SIGIR Forum 57, 1, Article 13 (dec 2023), 9 pages.
https://doi.org/10.1145/3636341.3636358
[17] Olivier Jeunen, Ivan Potapov, and Aleksei Ustimenko. 2024. On (Normalised)
Discounted Cumulative Gain as an Offline Evaluation Metric for Top- 𝑛Rec-
ommendation. In Proc. of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining (KDD ’24) . arXiv:2307.15053 [cs.IR]
[18] Henry F Kaiser. 1960. Directional statistical decisions. Psychological Review 67, 3
(1960), 160.
[19] Eugene Kharitonov, Alexey Drutsa, and Pavel Serdyukov. 2017. Learning Sen-
sitive Combinations of A/B Test Metrics. In Proc. of the Tenth ACM Interna-
tional Conference on Web Search and Data Mining (WSDM ’17). ACM, 651–659.
https://doi.org/10.1145/3018661.3018708
[20] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-
mization. In Proc. of the 3rd International Conference on Learning Representations
(ICLR ’14). arXiv:1412.6980 [cs.LG]
[21] Ron Kohavi, Alex Deng, and Lukas Vermeer. 2022. A/B Testing Intuition Busters:
Common Misunderstandings in Online Controlled Experiments. In Proc. of the
28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
’22). ACM, 3168–3177. https://doi.org/10.1145/3534678.3539160
[22] Ron Kohavi, Diane Tang, and Ya Xu. 2020. Trustworthy online controlled experi-
ments: A practical guide to A/B testing. Cambridge University Press.
[23] Olivier Ledoit and Michael Wolf. 2004. A well-conditioned estimator for large-
dimensional covariance matrices. Journal of Multivariate Analysis 88, 2 (2004),
365–411. https://doi.org/10.1016/S0047-259X(03)00096-4
[24] Olivier Ledoit and Michael Wolf. 2020. The Power of (Non-)Linear
Shrinking: A Review and Guide to Covariance Matrix Estimation.
Journal of Financial Econometrics 20, 1 (06 2020), 187–218. https:
//doi.org/10.1093/jjfinec/nbaa007 arXiv:https://academic.oup.com/jfec/article-
pdf/20/1/187/42274902/nbaa007.pdf
[25] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
Gao, and Jiawei Han. 2020. On the Variance of the Adaptive Learning Rate
and Beyond. In International Conference on Learning Representations (ICLR ’20).
https://arxiv.org/abs/1908.03265
[26] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi Tang, Lichan Hong,
and Ed H Chi. 2020. Off-policy learning in two-stage recommender systems. In
Proc. of The Web Conference 2020. 463–473.
[27] Frederick Mosteller. 1948. A k-Sample Slippage Test for an Extreme Population.
The Annals of Mathematical Statistics 19, 1 (1948), 58–65. http://www.jstor.org/
stable/2236056
[28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-
maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learn-
ing Library. In Advances in Neural Information Processing Systems, H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32.
Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2019/
file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf
[29] Alexey Poyarkov, Alexey Drutsa, Andrey Khalyavin, Gleb Gusev, and Pavel
Serdyukov. 2016. Boosted Decision Tree Regression Adjustment for Variance
Reduction in Online Controlled Experiments. In Proc. of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD ’16).
ACM, 235–244. https://doi.org/10.1145/2939672.2939688
[30] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. 2018. On the Convergence of
Adam and Beyond. In International Conference on Learning Representations (ICLR
’18). https://openreview.net/forum?id=ryQu7f-RZ
[31] Lee Richardson, Alessandro Zito, Dylan Greaves, and Jacopo Soriano. 2023. Pareto
optimal proxy metrics. arXiv:2307.01000 [stat.ME]
5191KDD ’24, August 25–29, 2024, Barcelona, Spain Olivier Jeunen and Aleksei Ustimenko
[32] Donald B Rubin. 1974. Estimating causal effects of treatments in randomized and
nonrandomized studies. Journal of educational Psychology 66, 5 (1974), 688.
[33] Sven Schmit and Evan Miller. 2022. Sequential confidence intervals for relative
lift with regression adjustments. (2022).
[34] Juliet Popper Shaffer. 1995. Multiple Hypothesis Testing. Annual Review of
Psychology 46, 1 (1995), 561–584. https://doi.org/10.1146/annurev.ps.46.020195.
003021 arXiv:https://doi.org/10.1146/annurev.ps.46.020195.003021
[35] Harald Steck. 2013. Evaluation of recommendations: rating-prediction and rank-
ing. In Proc. of the 7th ACM Conference on Recommender Systems (RecSys ’13).
ACM, 213–220. https://doi.org/10.1145/2507157.2507160
[36] Ziyang Tang, Yiheng Duan, Steven Zhu, Stephanie Zhang, and Lihong Li. 2022.
Estimating Long-Term Effects from Experimental Data. In Proc. of the 16th ACM
Conference on Recommender Systems (RecSys ’22). ACM, 516–518. https://doi.
org/10.1145/3523227.3547398
[37] Nilesh Tripuraneni, Lee Richardson, Alexander D’Amour, Jacopo Soriano, and
Steve Yadlowsky. 2023. Choosing a Proxy Metric from Past Experiments.
arXiv:2309.07893 [stat.ME]
[38] Julián Urbano, Harlley Lima, and Alan Hanjalic. 2019. Statistical Significance
Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and
Type III Errors. In Proc. of the 42nd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR’19). ACM, 505–514. https:
//doi.org/10.1145/3331184.3331259
[39] Aleksei Ustimenko and Liudmila Prokhorenkova. 2020. StochasticRank: Global
Optimization of Scale-Free Discrete Functions. In Proc. of the 37th International
Conference on Machine Learning (ICML ’20’, Vol. 119). PMLR, 9669–9679. https:
//proceedings.mlr.press/v119/ustimenko20a.html
[40] Abraham Wald. 1945. Sequential Tests of Statistical Hypotheses. The Annals of
Mathematical Statistics 16, 2 (1945), 117 – 186. https://doi.org/10.1214/aoms/
1177731118
[41] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson,
Lisa Chung, Ed H. Chi, and Minmin Chen. 2022. Surrogate for Long-Term
User Experience in Recommender Systems. In Proc. of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (Washington DC, USA)
(KDD ’22). ACM, 4100–4109. https://doi.org/10.1145/3534678.3539073
[42] Bernard Lewis WELCH. 1947. The Generalization of ‘Student’s’ Problem when
Several Different Population Variances are Involved. Biometrika 34, 1-2 (01 1947),
28–35. https://doi.org/10.1093/biomet/34.1-2.28
[43] Huizhi Xie and Juliette Aurisset. 2016. Improving the Sensitivity of Online
Controlled Experiments: Case Studies at Netflix. In Proc. of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining (KDD ’16).
ACM, 645–654. https://doi.org/10.1145/2939672.2939733
[44] Yisong Yue, Yue Gao, Oliver Chapelle, Ya Zhang, and Thorsten Joachims. 2010.
Learning More Powerful Test Statistics for Click-Based Retrieval Evaluation. In
Proc. of the 33rd International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR ’10). ACM, 507–514. https://doi.org/10.1145/
1835449.1835534A Additional Experimental Results
To further empirically validate our theoretical insights w.r.t. the
proposed methods, we repeat the experiments reported in Section 4
on data collected for the Moj platform, and reproduce Figures 3–5.
Results are visualised in Figure 7. Observations match our expecta-
tions, further strengthening trust in the replicability of our results.
All improvements in sensitivity and statistical power are a similar
order of magnitude as those for ShareChat: learnt metrics that
minimise ( log)𝑝-values can substantially reduce type-II/III errors
without affecting type-I errors. We observe an improvement over
ShareChat data in Figure 7d: learnt metrics for Moj exhibit a 12-
fold reduction in the sample size that is required to attain constant
statistical confidence as to the North Star.
5192Learning Metrics that Maximise Power for Accelerated A/B-Tests KDD ’24, August 25–29, 2024, Barcelona, Spain
0% 20% 40% 60% 80% 100%w⋆(p-value)w⋆(logp-value)w⋆(heuristic)w⋆(z-score)α= 1e−01
Agreement Inconclusive ( type-II ) Disagreement ( type-III )0% 20% 40% 60% 80% 100%α= 5e−02
0% 20% 40% 60% 80% 100%α= 1e−02
0% 20% 40% 60% 80% 100%α= 1e−03
0% 20% 40% 60% 80% 100%α= 1e−04
(a) Figure 3 reproduced with data from Moj.
10−410−310−210−1
α10−510−410−310−210−1Type-I Error
Expected (α) North Star Top-Proxy w⋆(p-value) w⋆(logp-value)10−410−310−210−1
α0.60.70.80.9Type-II Error
Sensitivity & Power for Stand-alone Metrics
(b) Figure 4a reproduced with data from Moj.
10−410−310−210−1
α10−510−410−310−210−1Type-I Error
10−410−310−210−1
α0.30.40.50.60.70.80.9Type-II Error
North Star + Top-Proxy or +w⋆(p−value) or +w⋆(logp−value)Sensitivity & Power for Sets of Metrics (a/f_ter Bonferroni correction)
(c) Figure 4b reproduced with data from Moj.
10−410−310−210−1
α24681012Median Rel. z2
North Star
+ Top-Proxy
or +w⋆(p−value)
or +w⋆(logp−value)Relative Sample Size Decrease for Sets of Metrics (a/f_ter Bonferroni)
(d) Figure 5 reproduced with data from Moj.
Figure 7: Additional experimental results for Moj. Empirical observations match those for ShareChat.
5193