Conditional Logical Message Passing Transformer for Complex
Query Answering
Chongzhi Zhang
South China University of Technology
Guangzhou, China
cschongzhizhang@mail.scut.edu.cnZhiping Peng
Guangdong University of Petrochemical Technology
Maoming, China
Jiangmen Polytechnic
Jiangmen, China
pengzp@foxmail.com
Junhao Zheng
South China University of Technology
Guangzhou, China
junhaozheng47@outlook.comQianli Ma∗
South China University of Technology
Guangzhou, China
qianlima@scut.edu.cn
Abstract
Complex Query Answering (CQA) over Knowledge Graphs (KGs)
is a challenging task. Given that KGs are usually incomplete, neu-
ral models are proposed to solve CQA by performing multi-hop
logical reasoning. However, most of them cannot perform well
on both one-hop and multi-hop queries simultaneously. Recent
work proposes a logical message passing mechanism based on the
pre-trained neural link predictors. While effective on both one-
hop and multi-hop queries, it ignores the difference between the
constant and variable nodes in a query graph. In addition, during
the node embedding update stage, this mechanism cannot dynami-
cally measure the importance of different messages, and whether
it can capture the implicit logical dependencies related to a node
and received messages remains unclear. In this paper, we propose
Conditional Logical Message Passing Transformer (CLMPT), which
considers the difference between constants and variables in the case
of using pre-trained neural link predictors and performs message
passing conditionally on the node type. We empirically verified
that this approach can reduce computational costs without affect-
ing performance. Furthermore, CLMPT uses the transformer to
aggregate received messages and update the corresponding node
embedding. Through the self-attention mechanism, CLMPT can
assign adaptive weights to elements in an input set consisting of
received messages and the corresponding node and explicitly model
logical dependencies between various elements. Experimental re-
sults show that CLMPT is a new state-of-the-art neural CQA model.
https://github.com/qianlima-lab/CLMPT.
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671869CCS Concepts
•Computing methodologies →Reasoning about belief and
knowledge; Knowledge representation and reasoning.
Keywords
Knowledge Graph; Logical Reasoning; Complex Query Answering;
Graph Neural Network
ACM Reference Format:
Chongzhi Zhang, Zhiping Peng, Junhao Zheng, and Qianli Ma. 2024. Condi-
tional Logical Message Passing Transformer for Complex Query Answering.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3637528.3671869
1 Introduction
Knowledge graphs (KGs) store factual knowledge in the form of
triples that can be utilized to support a variety of downstream tasks
[44,56,60]. However, given that modern KGs are usually auto-
generated [ 48] or built through crowd-sourcing [ 52], so real-world
KGs [ 9,11,45] are often considered noisy and incomplete, which
is also known as the Open World Assumption [ 24,28]. Answering
queries on such incomplete KGs is a fundamental yet challenging
task. To alleviate the incompleteness of KGs, knowledge graph rep-
resentation methods [ 10,17,46,49], which can be viewed as neural
link predictors [ 3], have been developed. They learn representa-
tions based on the available triples and generalize them to unseen
triples. Such neural link predictors can answer one-hop atomic
queries on incomplete KGs. But for multi-hop complex queries,
the query-answering models need to perform multi-hop logical
reasoning over incomplete KGs. That is, not only to utilize avail-
able knowledge to predict the unseen one but also execute logical
operators, such as conjunction ( ∧), disjunction (∨), and negation
(¬) [41, 59].
Recently, neural models [ 2,5,41,71] have been proposed to
solve Complex Query Answering (CQA) over incomplete KGs. The
complex queries that these models aim to address belong to an
important subset of the first-order queries, specifically Existentially
quantified First Order queries with a single free variable (EFO-1)
[58,59]. Such EFO-1 queries contain logical operators, including
4119
KDD ’24, August 25–29, 2024, Barcelona, Spain. Chongzhi Zhang, Zhiping Peng, Junhao Zheng, and Qianli Ma
Figure 1: EFO-1 query and its query graph for the question
“Who starred the films that were directed by Mizoguchi but
never won a Venice Film Festival Award?”.
conjunction, disjunction, and negation, as well as the existential
quantifier (∃) [41]. As shown in Figure 1, one can get the corre-
sponding EFO-1 formula and query graph given a question. The
query graph is a graphical representation of the EFO-1 query, where
each edge is an atomic formula that contains a predicate with a
(possible) negation operator, and each node represents an input
constant entity or a variable. Most neural CQA models convert the
query graph into the computation graph, where logical operators
are replaced with corresponding set operators. These models embed
the entity set into specific vector spaces [ 12,41,66,71] and execute
set operations parameterized by the neural networks according to
the computational graph. However, these models cannot perform
well on both one-hop and multi-hop queries simultaneously. They
tend to be less effective than classical neural link predictors [ 49]
on one-hop atomic queries [3, 58].
To this end, Logical Message Passing Neural Network (LMPNN)
[58] proposes a message passing framework based on pre-trained
neural link predictors. Specifically, for each edge in the query graph,
LMPNN uses the pre-trained neural link predictor to infer an in-
termediate embedding for a node given neighborhood information.
The intermediate embedding can be interpreted as a logical mes-
sage passed by the neighbor on the edge. Figure 1 demonstrates
the logical message passing with red/blue arrows. Then, LMPNN
aggregates messages and updates node embeddings in a manner
similar to Graph Isomorphism Network (GIN) [ 62]. While effec-
tive on both one-hop and multi-hop queries, LMPNN ignores the
difference between constant and variable nodes. Logical message
passing can be viewed as learning an embedding for the variable by
utilizing information of constant entities. It makes this embedding
similar or close to the embedding of the solution entity in the em-
bedding space of the pre-trained neural link predictor. However, as
shown by the red arrows in Figure 1, LMPNN also passes messages
to constant entitiy nodes with pre-trained embeddings and updates
their embeddings, which we believe is unnecessary and experi-
mentally demonstrated. In addition, for node embedding updating,
despite the expressive power of GIN in graph structure learning
[62], it cannot dynamically measure the importance of different
logical messages. Furthermore, it is unclear whether the GIN-like
approach can capture the implicit complex logical dependencies be-
tween messages received by a node and between messages and that
node. Explicitly modeling the above implicit logical dependencieswhile being able to measure the importance of different messages
dynamically is an open challenge we take on in this paper.
We propose Conditional Logical Message Passing Transformer
(CLMPT), a special Graph Neural Network (GNN) for CQA. Each
CLMPT layer has two stages: (1) passing the logical message of
each node to all of its neighbors that are not constant entity nodes;
(2) updating the embedding of each variable node based on the
logical messages received by the variable node. This means that
during forward message passing, CLMPT does not use the pre-
trained neural link predictor to infer the intermediate embeddings
(i.e. logical messages) for constant entity nodes with pre-trained
embeddings and does not update constant node embeddings. We
call such a mechanism Conditional Logical Message Passing, where
the condition depends on whether the node type is constant or
variable. We empirically verified that it can reduce computational
costs without affecting performance. Furthermore, CLMPT uses
the transformer [ 51] to aggregate messages and update the corre-
sponding node embedding. The self-attention mechanism in the
transformer can dynamically measure the importance of different
elements in the input and capture the interactions between any two
elements. It allows us to explicitly model the logical dependencies
between messages received by a node and between messages and
that node. We conducted experiments on three popular KGs: FB15k
[10], FB15k-237 [ 48], and NELL995 [ 61]. The experimental results
show that CLMPT can achieve a strong performance.
The main contributions of this paper are summarized as follows:
•We propose a conditional logical message passing mecha-
nism, which can effectively reduce the computational costs
and even improve the model performance to some extent.
•We propose a transformer-based node embedding update
scheme. It can dynamically measure the importance of differ-
ent intermediate embeddings of a variable node and capture
possible implicit dependencies between these embeddings.
•Extensive experiments results on several benchmark datasets
show that CLMPT is a new state-of-the-art neural CQA
model.
2 Related Work
Neural Link Predictors. Reasoning over KGs with missing knowl-
edge is one of the fundamental problems in Artificial Intelligence
and has been widely studied. Traditional KG reasoning tasks such
as link prediction [ 19,49] are essentially one-hop atomic query
problems. Representative methods for link prediction are knowl-
edge graph representation methods [ 10,17,37,46,49,65], which
can be regarded as neural link predictors [ 3]. Specifically, they em-
bed entities and relations into continuous vector spaces and predict
unseen triples by scoring triples with a well-defined scoring func-
tion. Such latent feature models can effectively answer one-hop
atomic queries over incomplete KGs. Other methods for link pre-
diction include rule learning [ 42,69], text representation learning
[43, 53, 55], and GNNs [50, 70, 72].
Neural Complex Query Answering. Complex queries over KGs
can be regarded as one-hop atomic queries combined with existen-
tial first-order logic operators. The scope of complex queries that
existing works can answer is expanded from conjunctive queries
[22,26], to Existential Positive First-Order (EPFO) queries [ 3,40],
4120Conditional Logical Message Passing Transformer for Complex Query Answering KDD ’24, August 25–29, 2024, Barcelona, Spain.
and more recently to the existential first-order [ 41], or more specifi-
cally to EFO-1 [ 58,59] and EFO𝑘queries [ 67]. Recently, neural query
embedding (QE) models have been proposed to answer complex
queries. Most QE models embed entity sets into specific continu-
ous vector spaces with various forms, including geometric shapes
[2,6,14,22,29,36,40,71], probabilistic distributions [ 13,41,66],
fuzzy logic [ 12], and bounded histograms on uniform grids [ 57].
They represent the complex query in the form of the computation
graph and perform neural set operations in the corresponding em-
bedding spaces according to the computation graph. Such neural
set operations are usually modeled by complex neural networks
[47,51]. In spite of getting query embedding with neural set opera-
tions, other QE models represent the queries as graphs to embed
queries with GNNs [ 1,16] or Transformers [ 7,26,30,54]. However,
most of these graph representation learning models only focus on
EPFO queries, and how they handle the logical negation operator
is unclear. Moreover, the aforementioned neural models are still
less effective than a simple neural link predictor [ 49] on one-hop
atomic queries.
There is also a class of neural CQA models based on pre-trained
neural link predictors. CQD-CO [ 3] proposes a continuous opti-
mization strategy that uses pre-trained neural link predictors [ 49]
and logical t-norms [ 25] to solve complex queries. However, ac-
cording to [ 58], CQD-CO performs badly on negative queries. Q2T
[63] is based on graph transformers [ 68] and pre-trained neural
link predictors to encode complex queries. LMPNN [ 58], which is
most related to our work, proposes a logical message passing frame-
work based on pre-trained neural link predictors. Through one-hop
inference on atomic formulas, LMPNN performs bidirectional log-
ical message passing on each edge in the query graph. However,
such a mechanism ignores the difference between the constant and
variable nodes. The essence of logical message passing lies in lever-
aging the pre-trained entity embeddings corresponding to constant
nodes to infer variable node embeddings. It aligns the variable node
embeddings more closely to the embeddings of solution entities
in the embedding space of the pre-trained neural link predictor.
It is important to note that using the intermediate states of the
variable nodes to update the embeddings of constant nodes can
introduce noise. This occurs because constant nodes possess stable,
pre-trained embeddings corresponding to specific entities, which
do not require updates via message passing. Therefore, updating
constant node embeddings with information from variable nodes
disrupts the integrity of these established embeddings. In addition,
at the node level, LMPNN aggregates the received logical messages
and updates the node embeddings in a manner similar to GIN [ 62].
Despite the expressive power of GIN in graph structure learning, it
cannot dynamically measure the importance of different messages.
Whether such a GIN-like approach can capture the implicit logical
dependencies between messages received by a node and between
messages and that node remains unclear. By contrast, our work only
performs conditional message passing on the query graph, which
avoids unnecessary computation costs. Furthermore, by using the
transformer [ 51] to aggregate the message and update the corre-
sponding node embedding, our work can assign adaptive weights
to elements in an input set consisting of received messages and
the corresponding node and explicitly model logical dependencies
between various elements.Symbolic Integration Models. In addition to neural CQA models,
some recent studies integrate symbolic information into neural mod-
els, which can be called symbolic integration models, also known
as neural-symbolic models [ 3,4,8,64,73]. These models estimate
the probability of whether each entity is the answer at each inter-
mediate step in the process of multi-hop logical reasoning. This
makes the size of the intermediate states for symbolic reasoning
in symbolic integration models scale linearly with the size of the
KG. Therefore, compared with neural CQA models where the inter-
mediate embeddings are of fixed dimension, symbolic integration
models require more computing resources and are more likely to
suffer from scalability problems on large-scale KGs [39, 58].
3 Background
Model-theoretic Concepts for Knowledge Graphs. A Knowledge
GraphKG consists of a set of entities Vand a set of relations
R. It can be defined as a set of triples E=(𝑒ℎ𝑖,𝑟𝑖,𝑒𝑡𝑖) ⊆ V×
R×V , namelyKG=(V,E,R). A first-order language Lcan
be defined as(F,R,C), whereF,R, andCare sets of symbols
for functions, relations, and constants, respectively [ 58]. Under
languageLKG, theKG can be represented as a first-order logic
knowledge base [ 3]. In this case, the relation symbols in Rdenote
binary relations, the constant symbols in Crepresent the entities,
and the function symbol set satisfies F=∅. In other words,KG
is anLKG-structure, where each entity 𝑒∈V is also a constant
𝑐∈ C=Vand each relation 𝑟∈ R is a set𝑟⊆ V×V . We
have𝑟(𝑡1,𝑡2)=𝑇𝑟𝑢𝑒 when(𝑡1,𝑡2)∈𝑟. An atomic formula is either
𝑟(𝑡1,𝑡2)or¬𝑟(𝑡1,𝑡2), where𝑡𝑖is a term that can be a constant or a
variable, and 𝑟is a relation that can be viewed as a binary predicate
[3]. A variable is a bound variable when associated with a quantifier.
Otherwise, it is a free variable. By adding connectives (conjunction
∧, disjunction∨, and negation¬) to such atomic formulas and
adding quantifiers (existential ∃and universal∀) to variables, we
can inductively define the first order formula [35, 58].
EFO-1 Queries. In this paper, we consider Existential First Order
queries with a single free variable (EFO-1) [ 58,59]. Such EFO-1
queries are an important subset of first-order queries, using exis-
tential quantifier, conjunction, disjunction, and atomic negation.
Following the previous studies [ 40,41,58], we define the EFO-1
query as the first order formula in the following Disjunctive Normal
Form (DNF) [15]:
𝑞[𝑦,𝑥1,...,𝑥𝑘]=∃𝑥1,...,∃𝑥𝑘(𝑎1
1∧...∧𝑎1
𝑛1)
∨...∨(𝑎𝑑
1∧...∧𝑎𝑑
𝑛𝑑),(1)
where𝑦is the only free variable, 𝑥1,...,𝑥𝑘are existential variables.
𝑎𝑖
𝑗are atomic formulas that can be either negated or not:
𝑎𝑖
𝑗=𝑟(𝑒𝑐,𝑣)𝑜𝑟 𝑟(𝑣,𝑣′)
¬𝑟(𝑒𝑐,𝑣)𝑜𝑟¬𝑟(𝑣,𝑣′), (2)
where𝑒𝑐∈V is an input constant entity, 𝑣,𝑣′∈{𝑦,𝑥1,...,𝑥𝑘}are
variables,𝑣≠𝑣′. To address the EFO-1 query 𝑞, the corresponding
answer entity set⟦𝑞⟧⊆V needs to be determined, where ⟦𝑞⟧is
a set of entities such that 𝑒∈⟦𝑞⟧iff𝑞[𝑦=𝑒,𝑥1,...,𝑥𝑘]=𝑇𝑟𝑢𝑒 . In
particular, since we use DNF to represent the first order formula, the
Equation 1 can also be expressed as the disjunction of conjunctive
4121KDD ’24, August 25–29, 2024, Barcelona, Spain. Chongzhi Zhang, Zhiping Peng, Junhao Zheng, and Qianli Ma
queries:
𝑞[𝑦,𝑥1,...,𝑥𝑘]=𝐶𝑄1(𝑦,𝑥1,...,𝑥𝑘)∨...∨𝐶𝑄𝑑(𝑦,𝑥1,...,𝑥𝑘),(3)
where𝐶𝑄𝑖=∃𝑥1,...,∃𝑥𝑘𝑎𝑖
1∧...∧𝑎𝑖𝑛𝑖is a conjunctive query.
According to [ 40,58], we can get the answer set ⟦𝑞⟧by taking
the union of the answer sets of each conjunctive query, namely
⟦𝑞⟧=Ð𝑑
𝑖=1⟦𝐶𝑄𝑖⟧. This means that solving all conjunctive queries
𝐶𝑄𝑖,1≤𝑖≤𝑑yields the answer set ⟦𝑞⟧for the EFO-1 query 𝑞.
Such DNF-based processing can solve complex queries involving
disjunction operators in a scalable way [38–40].
Query Graph. Since disjunctive queries can be solved in a scalable
manner by transforming queries into the disjunctive normal form,
it is only necessary to define query graphs for conjunctive queries.
We follow previous works [ 40,58] and represent the conjunctive
query as the query graph where the terms are represented as nodes
connected by the atomic formulas. That is, each edge in the query
graph is an atomic formula. According to the definition in Equation
2, an atomic formula contains relations, negation information, and
terms. Therefore, each node in the query graph is either a constant
entity or a free or existential variable, as illustrated in Figure 1. In
this paper, the term "constants" has specific meanings depending
on the context. When discussing an EFO-1 query, "constants" refers
to the input constant entities within that query. In the context
of the query graph, "constants" denotes the constant nodes that
correspond to these input constant entities.
Neural Link Predictors. A neural link predictor has a correspond-
ing scoring function that is used to learn the embeddings of entities
and relations in KG. For a triple, the embeddings of the head entity,
relation, and tail entity are ℎ,𝑟, and𝑡, respectively. The scoring
function𝜙(ℎ,𝑟,𝑡)can calculate the likelihood score of whether the
triple exists. By using the scoring function 𝜙(ℎ,𝑟,𝑡)and the sig-
moid function 𝜎, neural link predictors can produce a continuous
truth value𝜓(ℎ,𝑟,𝑡)∈[ 0,1]for a triple. For example, the scoring
function of ComplEx [49] is as follows:
𝜙(ℎ,𝑟,𝑡)=𝑅𝑒 
ℎ⊗𝑟,𝑡, (4)
where⊗represents the element-wise complex number multiplica-
tion,⟨·,·⟩is the complex inner product, and 𝑅𝑒means taking the
real part of a complex number. Based on 𝜙(ℎ,𝑟,𝑡), we can obtain
the corresponding truth value:
𝜓(ℎ,𝑟,𝑡)=𝜎(𝜙(ℎ,𝑟,𝑡)). (5)
In this work, the neural link predictor handles not only specific
entity embeddings but also variable embeddings. That is, the em-
beddingsℎ,𝑡correspond to embedding of terms, which can be the
embeddings of constant entities or the embeddings of variables.
Following previous works [ 3,58], in our work, we use ComplEx-N3
[27, 49] as the neural link predictor of CLMPT.
One-hop Inference on Atomic Formulas. As shown in Figure 1,
each edge in a query graph is an atomic formula containing the
information of relation, logical negation, and terms. Passing a logi-
cal message on an edge is essentially an operation of utilizing this
information to perform one-hop inference on an atomic formula.
On each edge, when a node is at the head position, its neighbor is
at the tail position, and vice versa. A logical message passed from a
neighbor to a node is essentially an intermediate embedding of thatnode. Prior work [ 58] proposes to obtain the intermediate embed-
dings of nodes by one-hop inference that maximizes the continuous
truth value of the (negated) atomic formulas. Specifically, a logical
message encoding function 𝜌is proposed to perform one-hop infer-
ence. Such a function has four input parameters, including neighbor
embedding, relation embedding, direction information ( ℎ→𝑡or
𝑡→ℎ), and logical negation information (0 for no negation and 1
for with negation), and it has four cases depending on the input
parameters. Given the tail embedding 𝑡and relation embedding 𝑟
on a non-negated atomic formula, 𝜌is formulated in the form of
continuous truth value maximization to infer the head embedding
ˆℎ:
ˆℎ=𝜌(𝑡,𝑟,𝑡→ℎ,0):=𝑎𝑟𝑔𝑚𝑎𝑥
𝑥∈D𝜓(𝑥,𝑟,𝑡),(6)
whereDis the search domain for the embedding 𝑥. Similarly, the
tail embedding ˆ𝑡on a non-negated atomic formula can be inferred
givenℎand𝑟:
ˆ𝑡=𝜌(ℎ,𝑟,ℎ→𝑡,0):=𝑎𝑟𝑔𝑚𝑎𝑥
𝑥∈D𝜓(ℎ,𝑟,𝑥).(7)
According to the fuzzy logic negator [ 21], one has𝜓(ℎ,¬𝑟,𝑡)=
1−𝜓(ℎ,𝑟,𝑡). Therefore, the estimation of head and tail embeddings
on negated atomic formulas can be defined as follows:
ˆℎ=𝜌(𝑡,𝑟,𝑡→ℎ,1):=𝑎𝑟𝑔𝑚𝑎𝑥
𝑥∈D[1−𝜓(𝑥,𝑟,𝑡)],(8)
ˆ𝑡=𝜌(ℎ,𝑟,ℎ→𝑡,1):=𝑎𝑟𝑔𝑚𝑎𝑥
𝑥∈D[1−𝜓(ℎ,𝑟,𝑥)].(9)
For the situation where ComplEx-N3 [ 27,49] is selected as the
neural link predictor, according to the propositions and proofs in
[58], the logical message encoding function 𝜌has the following
closed-form with respect to the complex embedding 𝑟and𝑡when
inferring the head embedding ˆℎon a non-negated atomic formula:
𝜌(𝑡,𝑟,𝑡→ℎ,0)=𝑎𝑟𝑔𝑚𝑎𝑥
𝑥∈C𝑑
𝑅𝑒(⟨𝑟⊗𝑡,𝑥⟩)−𝜆∥𝑥∥3	
=𝑟⊗𝑡√︁
3𝜆∥𝑟⊗𝑡∥,(10)
Similarly, for the other three cases, the encoding functions 𝜌are as
follows:
𝜌(ℎ,𝑟,ℎ→𝑡,0)=𝑟⊗ℎ√︁
3𝜆∥𝑟⊗ℎ∥, (11)
𝜌(𝑡,𝑟,𝑡→ℎ,1)=−𝑟⊗𝑡√︁
3𝜆∥𝑟⊗𝑡∥, (12)
𝜌(ℎ,𝑟,ℎ→𝑡,1)=−𝑟⊗ℎ√︁
3𝜆∥𝑟⊗ℎ∥. (13)
For Equations 10–13, 𝜆is a hyperparameter. In our work, we fol-
low prior work [ 58] and use the above logical message encoding
function𝜌to compute messages passed to the variable nodes.
4 Proposed Method
In this section, we propose Conditional Logical Message Passing
Transformer (CLMPT) to answer complex queries. As a special
message passing neural network [ 20], each CLMPT layer has two
stages: (1) passing the logical message of each node to all of its
neighbors that are not constant entity nodes; (2) updating the em-
bedding of each variable node based on the messages received by
4122Conditional Logical Message Passing Transformer for Complex Query Answering KDD ’24, August 25–29, 2024, Barcelona, Spain.
Figure 2: An illustration of the two-stage procedures of CLMPT at the 𝑙-th layer. (a) Passing the logical message to the variable
node in the neighborhood. (b) Updating the existential variable node embedding with the received messages and a transformer
encoder. Similarly, such a node embedding update scheme is applied to other variable nodes, such as the free variable node 𝑦.
the variable node. Figure 2 illustrates how these two stages work
using the query graph in Figure 1. Then, we can use the final layer
embedding of the free variable node to predict the answer entities
to the corresponding query. In the following subsections, we first
describe how each layer of CLMPT performs conditional logical
message passing and updates the variable node embeddings. After
that, we represent the training objective of our method. Finally, we
introduce how to answer complex queries with CLMPT.
4.1 Conditional Logical Message Passing
A query graph contains two types of nodes: constant entity nodes
and variable nodes. We decide whether to pass logical messages to
a node based on its type. Specifically, for any neighbor of a node,
when the neighbor is a variable node, we use the logical message
encoding function 𝜌to calculate the corresponding message and
pass it to the neighbor; when the neighbor is a constant entity node,
we do not pass the logical message to it. Figure 2 (a) demonstrates
the conditional logical message passing with blue arrows. Every
node in the query graph except the constant entity nodes receives
the logical message from all of its neighbors.
4.2 Node Embedding Conditional Update
Scheme
For a constant entity node, we denote its embedding at the 𝑙-th
layer as𝑧(𝑙)
𝑒and let𝑧(𝑙)
𝑣be the embedding of a variable node at
the𝑙-th layer. Next, we discuss how to calculate the 𝑧(𝑙)
𝑒and𝑧(𝑙)
𝑣
from the input layer 𝑙=0to latent layers 𝑙>0. For a constant
entity node, 𝑧(0)
𝑒is the corresponding entity embedding in the pre-
trained neural link predictor, which can be frozen or optimized
continuously. For 𝑧(0)
𝑣, we follow previous works [ 58] and assign
two learnable embeddings 𝑣𝑥,𝑣𝑦for the existential variable node 𝑥
and the free variable node 𝑦respectively, and set that all existential
variable nodes 𝑥𝑖share one𝑣𝑥, namely𝑧(0)
𝑥𝑖=𝑣𝑥and𝑧(0)
𝑦=𝑣𝑦.
Similar to conditional logical message passing, for updating the
node embeddings, we also do not consider constant entity nodes.
That is, we only update node embeddings for variable nodes in the
query graph. Thus, the embeddings of the constant entity nodes are
the same at each layer, namely 𝑧(𝑙)
𝑒=𝑧(0)
𝑒. While for𝑧(𝑙)
𝑣, we use
the corresponding information from the (𝑙−1)-th layer to update it.
Specifically, for a variable node 𝑣, we represent its neighbor set in
the query graph asN(𝑣). For each neighbor node 𝑛∈N(𝑣), one can
obtain information about the edge (i.e., the atomic formula) between𝑛and𝑣, which contains the neighbor embedding 𝑧(𝑙−1)
𝑛∈D, the
relation𝑟𝑛𝑣∈R, the direction 𝐷𝑛𝑣∈{ℎ→𝑡,𝑡→ℎ}, and the
negation indicator 𝑁𝑒𝑔𝑛𝑣∈{0,1}. Based on the edge information,
we can use the logical message encoding function 𝜌to compute the
logical message 𝑚(𝑙)that𝑛passes to𝑣:
𝑚(𝑙)=𝜌(𝑧(𝑙−1)
𝑛,𝑟𝑛𝑣,𝐷𝑛𝑣,𝑁𝑒𝑔𝑛𝑣). (14)
Let𝑘𝑣be the number of neighbor nodes in N(𝑣), where𝑘𝑣≥1.
For the variable node 𝑣, it receives 𝑘𝑣logical messages from 𝑘𝑣
neighbors:𝑚(𝑙)
1,...,𝑚(𝑙)
𝑘𝑣. We use the transformer [ 51] to encode the
input set consisting of these logical messages and the variable node
embedding𝑧(𝑙−1)
𝑣 to obtain the updated embedding of node 𝑣:
𝑧(𝑙)
𝑣=𝑀𝑒𝑎𝑛(𝑇𝐸(𝑙)(𝑚(𝑙)
1,...,𝑚(𝑙)
𝑘𝑣,𝑧(𝑙−1)
𝑣)), (15)
where𝑀𝑒𝑎𝑛 is a mean pooling layer. 𝑇𝐸is a standard transformer
encoder stacked with multiple layers of transformer encoder blocks,
where the attention computation approach is consistent with the
bidirectional multi-head self-attention of the vanilla transformer
[51]. Specifically, given an input embedding set 𝑋, it is first pro-
jected to query ( 𝑄), key (𝐾), and value ( 𝑉) matrices through a linear
projection such that 𝑄=𝑋𝑊𝑄,𝐾=𝑋𝑊𝐾and𝑉=𝑋𝑊𝑉respec-
tively. Then, the self-attention can be compute via
𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄,𝐾,𝑉)=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑄𝐾𝑇
√︁
𝑑𝐾)𝑉, (16)
where𝑑𝐾represents the dimension of 𝐾, and𝑊𝑄,𝑊𝐾,𝑊𝑉are
parameter matrices. We use multi-head attention, which concate-
nates multiple instances of Equation 16 and then obtains the output
through a linear projection. It is worth noting that since the input
to Equation 15 is a set consisting of the node embedding and the
messages rather than a sequence, we do not use any positional en-
coding. This means that our architecture is permutation invariant.
Through the self-attention mechanism, we can explicitly model
the complex logical dependencies between various elements in the
input set and dynamically measure the importance of different ele-
ments. Figure 2 (b) shows how to obtain the updated embedding
of the existential variable node 𝑥through the encoding process
defined by Equation 15.
4.3 Training Objective
We use the Noisy Contrastive Estimation (NCE) loss proposed in
[34] to train CLMPT. Specifically, we denote the positive data as
4123KDD ’24, August 25–29, 2024, Barcelona, Spain. Chongzhi Zhang, Zhiping Peng, Junhao Zheng, and Qianli Ma
{(𝑎𝑖,𝑞𝑖)}𝑁
𝑖=1, where𝑎𝑖∈⟦𝑞𝑖⟧. Our optimization involves 𝐾uni-
formly sampled noisy answers from the entity set. The training
objective is as follows:
𝐿𝑁𝐶𝐸=−1
𝑁𝑁∑︁
𝑖=1𝑙𝑜𝑔"
𝐹(𝑧𝑎𝑖,𝑧(𝑞𝑖),𝑇)
𝐹(𝑧𝑎𝑖,𝑧(𝑞𝑖),𝑇)+Í𝐾
𝑘=1𝐹(𝑧𝑛𝑖𝑘,𝑧(𝑞𝑖),𝑇)#
,
(17)
𝐹(𝑥,𝑦,𝑧)=𝑒𝑥𝑝[𝑐𝑜𝑠(𝑥,𝑦)/𝑧], (18)
where𝑐𝑜𝑠(·,·)is the cosine similarity, 𝑇is a hyperparameter, 𝑧𝑎𝑖
is the embedding of positive answer 𝑎𝑖,𝑧𝑛𝑖𝑘is the embedding of
the𝑘-th noisy samples, and 𝑧(𝑞𝑖)is the embedding of the predicted
answer to the query 𝑞𝑖, corresponding to the embedding of the free
variable of the query 𝑞𝑖at the final layer of CLMPT.
4.4 Answering Complex Queries with CLMPT
For a complex query defined in Equation 3, i.e., a DNF query, we
follow the previous works [ 40,41] and use DNF-based processing
to get answer entities. Specifically, we estimate the predicted an-
swer embeddings for each sub-conjunctive query of the complex
query. Then, the answer entities are ranked by the maximal cosine
similarity against these predicted answer embeddings. Therefore,
CLMPT only needs to consider the query graph of the conjunctive
query.
For a given conjunctive query 𝑞, let the depth of CLMPT be 𝐿, and
we apply the CLMPT layers 𝐿times to the query graph of 𝑞. Then,
we can obtain the free variable embedding 𝑧(𝐿)
𝑦at the final layer. We
use𝑧(𝐿)
𝑦as the embedding of the predicted answer entity, namely
𝑧(𝑞)=𝑧(𝐿)
𝑦. Based on the cosine similarity between 𝑧(𝑞)and the
entity embeddings in the pre-trained neural link predictor, we can
rank the entities to retrieve answers. For the depth 𝐿, according
to the analyses in [ 58],𝐿should be the largest distance between
the constant entity nodes and the free variable node to ensure the
free variable node successfully receives all logical messages from
the constant entity nodes. This means 𝐿is not determined, and 𝐿
should change dynamically with different conjunctive query types.
Thus, we follow prior work [ 58] and assume all 𝐿CLMPT layers
share the same transformer encoder.
In this case, the parameters in CLMPT include two embeddings
for existential and free variables, a transformer encoder, and the
parameters in the pre-trained neural link predictor. For the pre-
trained neural link predictor, it can be frozen or not. In our work, we
continue to optimize pre-trained neural link predictors by default,
that is, we do not freeze pre-trained embeddings of entities and
relations.
5 Experiments
5.1 Experimental Settings
5.1.1 Datasets and Queries. We evaluate our method on three
commonly-used standard knowledge graphs: FB15k [ 10], FB15k-237
[48], and NELL995 [ 61]. For a fair comparison with previous works,
we use the datasets of complex queries proposed by BetaE [ 41],
which consist of five conjunctive query types (1 𝑝/2𝑝/3𝑝/2𝑖/3𝑖),
five query types with atomic negations (2 𝑖𝑛/3𝑖𝑛/𝑖𝑛𝑝/𝑝𝑖𝑛/𝑝𝑛𝑖), and
four zero-shot query types ( 𝑝𝑖/𝑖𝑝/2𝑢/𝑢𝑝), as illustrated in Figure
Figure 3: All the query types considered in our experiments,
where𝑝,𝑖,𝑢, and𝑛represent projection, intersection, union,
and negation, respectively. The naming of each query type
reflects how they were generated in the BetaE paper [41].
Table 1: Statistics of different query types used in the bench-
mark datasets.
Split
Query Types FB15k FB15k-237 NELL995
T
rain1p, 2p, 3p, 2i, 3i 273,710 149,689 107,982
2in, 3in, inp, pin, pni 27,371 14,968 10,798
V
alid1p 59,078 20,094 16,910
Others 8,000 5,000 4,000
T
est1p 66,990 22,804 17,021
Others 8,000 5,000 4,000
3. The datasets provided by BetaE introduce complex queries with
hard answers that cannot be obtained by traversing the KG directly.
That is, the evaluation focuses on discovering such hard answers to
complex queries. We follow previous works [ 41,58,71] and train
our model with five conjunctive query types and five query types
with atomic negations. When evaluating the model, we consider all
query types including the four zero-shot query types. The statistics
for each dataset are presented in Table 1.
5.1.2 Evaluation Protocol. The evaluation scheme follows the pre-
vious works [ 41,58], which divides the answers to each complex
query into easy and hard sets. For test and validation splits, we de-
fine hard answers as those that cannot be obtained by direct graph
traversal on KG. In order to get these hard answers, the model is re-
quired to impute at least one missing edge, which means the model
needs to complete non-trivial reasoning [ 40,41]. Specifically, for
each hard answer of a query, we rank it against non-answer entities
based on their cosine similarity with the free variable embedding
of the query and calculate the Mean Reciprocal Rank (MRR).
5.1.3 Baselines. We consider the state-of-the-art neural complex
query answering models for EFO-1 queries in recent years as our
baselines to compare: BetaE [ 41], ConE [ 71], Q2P [ 6], MLP [ 2],
GammaE [ 66], CylE [ 36], CQD-CO [ 3], and LMPNN [ 58], where
CQD-CO and LMPNN use pre-trained neural link predictor, while
4124Conditional Logical Message Passing Transformer for Complex Query Answering KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 2: The MRR results of baselines and our model over three KGs. Avg prepresents the average score of EPFO queries and
Avg ndenote the average score of queries with negation. The boldface indicates the best results, and the second-best ones are
marked with underlines .
Dataset
Model 1p 2p 3p 2i 3i pi ip 2u up 2in 3in inp pin pni Avg pAvg n
FB15kBetaE
65.1 25.7 24.7 55.8 66.5 43.9 28.1 40.1 25.2 14.3 14.7 11.5 6.5 12.4 41.6 11.8
ConE 73.3 33.8 29.2 64.4 73.7 50.9 35.7 55.7 31.4
17.9 18.7 12.5 9.8 15.1 49.8 14.8
Q2P 82.6 30.8 25.5 65.1 74.7 49.5 34.9 32.1 26.2 21.9 20.8
12.5 8.9 17.1 46.8
16.4
MLP 67.1 31.2 27.2 57.1 66.9 45.7 33.9 38.0 28.0 17.2 17.8 13.5 9.1 15.2 43.9 14.5
GammaE 76.5 36.9 31.4 65.4
75.1 53.9 39.7
53.5 30.9 20.1 20.5 13.5 11.8 17.1 51.3
16.6
CylE 78.8 37.0 30.9 66.9 75.7 53.8 40.8 59.4 33.5 15.7
16.3 13.7 7.8 13.9 53.0 13.5
(
Using pre-trained neural link predictor)
CQD-CO 89.4 27.6 15.1 63.0 65.5 46.0 35.2 42.9 23.2 0.2 0.2 4.0 0.1 18.4 45.3 4.6
LMPNN 85.0 39.9 28.6
68.2 76.5 46.7
43.0 36.7
31.4 29.1 29.4 14.9 10.2
16.4 50.6 20.0
CLMPT 86.1 43.3
33.9 69.1 78.2 55.1 46.6 46.1 37.3 21.2 22.9 17.0
13.0 15.3 55.1 17.9
FB15k-237BetaE
39.0 10.9 10.0 28.8 42.5 22.4 12.6 12.4 9.7 5.1 7.9 7.4 3.6 3.4 20.9 5.4
ConE 41.8 12.8 11.0 32.6
47.3 25.5 14.0 14.5 10.8
5.4 8.6 7.8 4.0 3.6 23.4 5.9
Q2P 39.1 11.4 10.1 32.3 47.7 24.0 14.3 8.7 9.1 4.4 9.7 7.5 4.6 3.8 21.9 6.0
MLP 42.7 12.4 10.6 31.7 43.9 24.2 14.9 13.7 9.7 6.6 10.7 8.1 4.7 4.4 22.6 6.9
GammaE 43.2 13.2 11.0 33.5
47.9 27.2 15.9
13.9 10.3 6.7 9.4 8.6 4.8 4.4
24.0 6.8
CylE 42.9 13.3 11.3 35.0 49.0 27.0
15.7 15.3 11.2 4.9 8.3 8.2 3.7
3.4 24.5 5.7
(
Using pre-trained neural link predictor)
CQD-CO 46.7 10.3 6.5 23.1 29.8 22.1 16.3 14.2 8.9 0.2 0.2 2.1 0.1 6.1 19.8 1.7
LMPNN 45.9 13.1
10.3 34.8 48.9 22.7 17.6 13.5
10.3 8.7 12.9 7.7
4.6 5.2 24.1
7.8
CLMPT
45.7 13.7 11.3 37.4 52.0 28.2 19.0 14.3 11.1 7.7 13.7 8.0 5.0 5.1 25.9
7.9
NELL995BetaE
53.0 13.0 11.4 37.6 47.5 24.1 14.3 12.2 8.5 5.1 7.8 10.0 3.1 3.5 24.6 5.9
ConE 53.1 16.1 13.9 40.0 50.8 26.3 17.5 15.3 11.3 5.7 8.1 10.8 3.5 3.9 27.2 6.4
Q2P 56.5 15.2 12.5 35.8 48.7 22.6 16.1 11.1 10.4 5.1 7.4 10.2 3.3 3.4 25.5 6.0
MLP 55.2 16.8 14.9 36.4 48.0 22.7 18.2 14.7 11.3 5.1 8.0 10.0 3.6 3.6 26.5 6.1
GammaE 55.1 17.3 14.2 41.9 51.1 26.9 18.3 15.1 11.2 6.3 8.7 11.4 4.0 4.5
27.9 7.0
CylE
56.5 17.5 15.6 41.4 51.2 27.2
19.6 15.7 12.3 5.6 7.5 11.2 3.4 3.7 28.5 6.3
(
Using pre-trained neural link predictor)
CQD-CO 60.8 18.3 13.2
36.5 43.0 30.0 22.5 17.6 13.7
0.1 0.1 4.0 0.0 5.2 28.4 1.9
LMPNN 60.6 22.1 17.5 40.1
50.3 28.4 24.9 17.2
15.7 8.5
10.8 12.2 3.9 4.8 30.7 8.0
CLMPT
58.9 22.1 18.4 41.8 51.9
28.8 24.4 18.6
16.2 6.6 8.1
11.8 3.8
4.5 31.3 7.0
other models do not. In addition, some other neural CQA models
cannot handle logical negation operators, such as BIQE [ 26], PREM
[13], kgTransformer [ 30], etc., which use the EPFO queries gener-
ated by Q2B [ 40] for training and evaluation. We compare these
works on the Q2B datasets in Appendix B. We also compare our
work with symbolic integration CQA models in Appendix C.
5.1.4 Model Details. Following LMPNN [ 58] and CQD-CO [ 3], we
choose ComplEx-N3 [ 27,49] as the neural link predictor in our
work and use the ComplEx-N3 checkpoints released by CQD-CO
to conduct experiments. The rank of ComplEx-N3 is 1,000, and the
epoch for the checkpoints is 100. For all datasets, we use a two-
layer transformer encoder with eight self-attention heads, where
the dimension of the hidden layer of the feed-forward network
(FFN) in the transformer is 8,192. For the training objective, the
negative sample size 𝐾is 128, and𝑇is chosen as 0.05 for FB15k-
237 and FB15k and 0.1 for NELL995. For more details about the
implementation and experiments, please refer to Appendix A.
5.2 Main Results
Table 2 shows the MRR results of CLMPT and neural CQA base-
lines about answering EFO-1 queries over three KGs. It is foundTable 3: The average MRR results of models with or with-
out conditional logical message passing on answering EFO-1
queries.↓Memory and↓Time represent the GPU memory
usage reduction and time savings with conditional logical
message passing, respectively. The boldface indicates the best
results.
Dataset
Model Avg pAvg n↓Memory↓Time
FB15k-237LMPNN
24.1 7.8
↓1.2%↓5.0%LMPNN w/ C 24.3 7.8
CLMPT
w/o C 25.8 7.7
↓9.6%↓12.3%CLMPT 25.9 7.9
NELL995LMPNN
30.7 8.0
↓0.6%↓8.3%LMPNN w/ C 31.0 8.2
CLMPT
w/o C 31.4 7.0
↓8.0%↓11.0%CLMPT 31.3 7.0
that CLMPT reaches the best performance on average for EPFO
queries across all three datasets. Compared with LMPNN, which is
most relevant to our work, CLMPT achieves average performance
improvements of 8.9%, 7.5%, and 2.0% for EPFO queries on FB15k,
4125KDD ’24, August 25–29, 2024, Barcelona, Spain. Chongzhi Zhang, Zhiping Peng, Junhao Zheng, and Qianli Ma
Table 4: The MRR results of different hyperparameter settings and different pooling approaches. The best results are bold.
Mo
del 1p 2p 3p 2i 3i pi ip 2u up 2in 3in inp pin pni Avg pAvg n
𝑇
𝐸layer = 1 45.8 13.2 10.9 37.3 51.9 27.7 18.2 14.6 10.9 7.7 13.3 8.1 5.3 4.5 25.6 7.8
𝑇𝐸layer = 3 45.8 13.9 11.4 37.3 51.8 29.0 19.2 14.4 11.2 7.6 13.1 8.1 5.0 4.9 26.0 7.7
𝑑FFN=
4096 45.8 13.5 11.1 37.3 51.7 27.9 18.6 14.3 11.2 7.7 13.2 8.1 5.1 4.9 25.7 7.8
self-attention
head = 4 45.9 13.6 11.3 37.4 52.0 28.1 18.7 14.6 11.3 7.4 13.1 8.1 5.0 4.8 25.9 7.7
self-attention head = 16 45.0 12.8 10.3 36.4 50.4 26.8 18.0 14.5 10.6 7.5 12.9 7.7 5.2 4.3 25.0 7.5
𝑇=
0.01 45.8 12.9 10.3 35.1 48.9 26.4 16.8 14.1 10.3 7.1 10.8 7.4 5.6 4.0 24.5 7.0
𝑇= 0.1 44.9 13.5 10.7 34.4 47.2 26.5 18.7 15.6 11.1 7.6 10.2 8.1 4.2 5.8 24.7 7.2
Max
Pooling 46.1 13.8 11.1 36.8 50.5 26.0 17.5 14.4 11.1 5.1 9.5 7.3 3.7 3.4 25.2 5.8
Sum Pooling 45.9 13.8 11.0 37.5 51.9 28.5 18.7 14.3 11.3 7.6 13.6 8.2 5.0 4.9 25.9 7.9
Default
Setting 45.7 13.7 11.3 37.4 52.0 28.2 19.0 14.3 11.1 7.7 13.7 8.0 5.0 5.1 25.9 7.9
FB15K-237, and NELL995, respectively. Since conditional logical
message passing has little impact on the model performance (it
will be later discussed in Section 5.3.1), these performance improve-
ments are mainly due to the transformer-based node embedding
update scheme. This shows that this approach, which explicitly
models dependencies between input elements by assigning adap-
tive weights to the messages and the corresponding node through
the self-attention mechanism, is more suitable for the logical query
graphs of EPFO queries than the GIN-like approach. For the neg-
ative queries, CLMPT achieves the best average performance on
FB15k-237 and sub-optimal performance on the remaining two KGs.
One potential explanation for these sub-optimal results is that the
transformer architecture lacks inductive biases and relies more on
the training data. As shown in the statistics in Table 1, the number
of negative queries in the training set is an order of magnitude less
than the EPFO queries, which may lead to the tendency of the trans-
former encoder to learn the patterns relevant to answering EPFO
queries, resulting in insufficient modeling of relevant patterns that
answer negative queries.
5.3 Ablation Study
5.3.1 Conditional Logical Meassage Passing. The logical message
passing mechanism can be viewed as learning an embedding for
the variable by utilizing information of constant entities. It makes
this embedding similar to the embedding of the solution entity
in the embedding space of the pre-trained neural link predictor.
As in the previous analyses, we argue that for a constant node
with the corresponding entity embedding, it is unnecessary to
update its embedding in the forward passing of the model. To
verify this, we conduct experiments on whether to pass messages
to the constant node and update its embedding, and the results are
shown in Table 3, where "w/ C" indicates "with conditional logical
message passing" and "w/o C" indicates "without conditional logical
message passing". From the experimental results, the performance
of LMPNN with conditional logical message passing on the two
KGs is not reduced but improved. CLMPT achieves better results
on FB15k-237 than the variant that does not use conditional logical
message passing, only slightly underperforming the variant on
the EPFO queries of NELL995. In addition, we evaluate how much
unnecessary computational cost the conditional logical message
passing mechanism can avoid on an RTX 3090 GPU. Specifically, we
calculate the relative percentage of the reduction in GPU memoryusage and the relative percentage of training time saved:
↓Memory =(𝑢𝑠𝑎𝑔𝑒𝑤𝑜𝐶−𝑢𝑠𝑎𝑔𝑒𝐶)/𝑢𝑠𝑎𝑔𝑒𝑤𝑜𝐶, (19)
↓Time =(𝑡𝑤𝑜𝐶−𝑡𝐶)/𝑡𝑤𝑜𝐶, (20)
where𝑢𝑠𝑎𝑔𝑒𝐶and𝑡𝐶respectively refer to the GPU memory usage
and the required training time when the model uses conditional log-
ical message passing, while 𝑢𝑠𝑎𝑔𝑒𝑤𝑜𝐶 and𝑡𝑤𝑜𝐶 correspond to the
situation without conditional logical message passing. As shown
in Table 3, conditional logical message passing reduces compu-
tational costs without significantly negatively impacting model
performance. In particular, the computational cost reduction is
particularly significant for CLMPT, which uses the more complex
transformer architecture. The above results confirm our view that
the model does not need to consider the constant entity nodes in
the query graph during the forward message passing.
Table 5: The average MRR results for experiments on whether
to freeze the Neural Link Predictor (NLP). The boldface indi-
cates the best results.
Dataset
ModelFrozen NLP Trainable NLP
A
vgpAvg nAvg p Avg n
FB15kLMPNN
50.6 20.0 51.5 18.9
CLMPT 54.2 17.3 55.1 17.9
FB15k-237LMPNN
24.1 7.8 24.4 7.5
CLMPT 25.4 7.7 25.9 7.9
NELL995LMPNN
30.7 8.0 30.1 7.9
CLMPT 32.0 6.8 31.3 7.0
5.3.2 Hyperparamaters and Pooling Approaches. We evaluate the
performance of CLMPT with different hyperparameter settings and
pooling approaches on FB15k-237. The default setting we adopted
is described in Section 5.1.4. As is shown in Table 4, regardless of
reducing the number of transformer encoder layers or halving the
dimensions of the hidden layer of the FFN, the model still achieves
the best performance than the baselines on FB15k-237, which re-
flects the effectiveness of our proposed method. When the number
of transformer encoder layers increases, the performance changes
little, meaning the two-layer transformer encoder is sufficient. In
addition, we can find that 𝑇in NCE loss is very important to the
4126Conditional Logical Message Passing Transformer for Complex Query Answering KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 6: The MRR results of CLMPT-small using frozen neural link predictor and using trainable neural link predictor. Param T
andParam Frepresent trainable and frozen parameters of the model, respectively. The F in parentheses indicates that the model
uses a frozen neural link predictor, and T indicates that it uses a trainable one.
Dataset
Model 1p 2p 3p 2i 3i pi ip 2u up 2in 3in inp pin pni Avg pAvg nParam TParam F
FB15k-237LMPNN
45.9 13.1 10.3 34.8 48.9 22.7 17.6 13.5 10.3 8.7 12.9 7.7 4.6 5.2 24.1 7.8 16M 30M
CLMPT-small(F) 46.1 12.6 10.0 36.1 50.4 26.7 17.5 14.9 10.3 7.7 11.6 7.5 4.4 5.0 25.0 7.3 32M 30M
CLMPT-small(T) 45.7 13.2 11.0 37.2 51.6 27.4 18.2 14.3 11.1 7.4 13.1 8.1 5.1 4.7 25.5 7.7 62M 0M
NELL995LMPNN
60.6 22.1 17.5 40.1 50.3 28.4 24.9 17.2 15.7 8.5 10.8 12.2 3.9 4.8 30.7 8.0 33M 128M
CLMPT-small(F) 60.6 21.7 17.7 42.2 51.7 30.7 24.5 19.4 15.6 6.4 7.9 11.0 3.7 4.5 31.6 6.7 32M 128M
CLMPT-small(T) 59.1 21.3 17.5 41.8 51.8 29.0 23.5 19.0 15.6 6.4 7.9 11.2 3.8 4.5 31.0 6.8 160M 0M
performance. When 𝑇is 0.01 or 0.1, the performance of the model
is still competitive on EPFO queries, but there is a large gap with
the average performance under the default settings. For the pool-
ing approaches, we evaluate max pooling and sum pooling. The
experimental results show max pooling does not perform as well
as sum pooling and mean pooling.
5.3.3 Frozen or Trainable Neural Link Predictors. For the pre-
trained neural link predictor, it can be frozen or trainable. As men-
tioned above (see Section 4.4), in our work, we optimize all param-
eters in the pre-trained neural link predictor by default. It is worth
noting that the pre-trained neural link predictor is frozen in LMPNN.
In order to explore the impact of freezing the pre-trained neural
link predictor on model performance, we conduct experiments on
three KGs, and the results are shown in Table 5.
For LMPNN, using frozen neural link predictors performs better
than using trainable ones, except for EPFO queries on FB15k and
FB15k-237. Therefore, in the main results, we only compare the
MRR results of LMPNN using frozen neural link predictors, i.e. the
results reported in the original paper [ 58]. For CLMPT, with the
exception of EPFO queries on NELL995, CLMPT using the trainable
neural link predictor performs relatively better. Thus, we continue
to optimize the pre-trained neural link predictor on complex queries
by default. For these results of CLMPT, according to the progress
of pre-training and fine-tuning paradigms [ 18,31], one possible
reason is that the neural link predictor is pre-trained on one-hop
queries, and tuning its parameters on complex queries can make the
embeddings of entities and relations more suitable for the patterns
and characteristics of CQA. The better performance of LMPNN
using trainable neural link predictors on EPFO queries of FB15k and
FB15K-237 can also support our argument to some extent because
the number of EPFO queries in the datasets is much more than
negative ones; in this case, the model is more inclined to learn
patterns that answer EPFO queries.
Although CLMPT, which uses the trainable neural link predictor,
has more training parameters, it still significantly outperforms
LMPNN on EPFO queries across all datasets when the neural link
predictor is frozen. This reflects the effectiveness of CLMPT.
5.4 Analyses on Model Parameters
Since CLMPT introduces the transformer, it has more parameters
than LMPNN. To further evaluate the effectiveness of CLMPT, we
compare LMPNN using a smaller model whose parameters are
close to those of LMPNN. Specifically, we set 𝑇𝐸layer =1and𝑑FFN=4096, and we represent such a smaller model as CLMPT-
small. Since the parameters of the neural link predictor in LMPNN
are frozen, we consider using both frozen and trainable neural link
predictors for fair comparison. We represent the models in these
two cases as CLMPT-small(F) and CLMPT-small(T), respectively.
The experimental results are shown in Table 6. When their model
parameters are close, CLMPT still achieves the best average per-
formance on EPFO queries. In the case of freezing the neural link
predictor, CLMPT-small(F) has fewer trainable parameters than
LMPNN on NELL995, but it still significantly outperforms LMPNN
on average in anwsering EPFO queries. These experimental results
reflect the effectiveness of CLMPT.
6 Conclusion
In this paper, we propose CLMPT, a special message passing neural
network, to answer complex queries over KGs. Based on one-hop
inference by pre-trained neural link predictor on atomic formu-
las, CLMPT performs logical message passing conditionally on the
node type. In the ablation study, we verify that this conditional mes-
sage passing mechanism can effectively reduce the computational
costs and even improve the performance of the model to some
extent. Furthermore, CLMPT uses the transformer to aggregate
messages and update the corresponding node embedding. Through
the self-attention mechanism, CLMPT can explicitly model logical
dependencies between messages received by a node and between
messages and that node by assigning adaptive weights to the mes-
sages and the node. The experimental results show that CLMPT
achieves a strong performance through this transformer-based node
embedding update scheme. Future work may integrate symbolic
information into CLMPT to improve the performance in answering
negative queries. For limitations, please refer to Appendix D.
Acknowledgments
We thank the anonymous reviewers for their helpful feedbacks.
The work described in this paper was partially funded by the Na-
tional Natural Science Foundation of China (Grant Nos. 62272173,
62273109), the Natural Science Foundation of Guangdong Province
(Grant Nos. 2024A1515010089, 2022A1515010179), the Science and
Technology Planning Project of Guangdong Province (Grant No.
2023A0505050106), and the National Key R&D Program of China
(Grant No. 2023YFA1011601).
4127KDD ’24, August 25–29, 2024, Barcelona, Spain. Chongzhi Zhang, Zhiping Peng, Junhao Zheng, and Qianli Ma
References
[1]Dimitrios Alivanistos, Max Berrendorf, Michael Cochez, and Mikhail Galkin.
2022. Query Embedding on Hyper-Relational Knowledge Graphs. In Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
4rLw09TgRw9
[2]Alfonso Amayuelas, Shuai Zhang, Susie Xi Rao, and Ce Zhang. 2022. Neural
Methods for Logical Reasoning over Knowledge Graphs.. In Proceedings of the
Tenth International Conference on Learning Representations 2022.
[3]Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. 2020.
Complex Query Answering with Neural Link Predictors. In International Confer-
ence on Learning Representations.
[4]Erik Arakelyan, Pasquale Minervini, Daniel Daza, Michael Cochez, and Isabelle
Augenstein. 2023. Adapting Neural Link Predictors for Data-Efficient Complex
Query Answering. In Thirty-seventh Conference on Neural Information Processing
Systems.
[5]Jiaxin Bai, Chen Luo, Zheng Li, Qingyu Yin, Bing Yin, and Yangqiu Song. 2023.
Knowledge graph reasoning over entities and numerical values. In Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
57–68.
[6]Jiaxin Bai, Zihao Wang, Hongming Zhang, and Yangqiu Song. 2022.
Query2Particles: Knowledge Graph Reasoning with Particle Embeddings. In
Findings of the Association for Computational Linguistics: NAACL 2022. 2703–
2714.
[7]Jiaxin Bai, Tianshi Zheng, and Yangqiu Song. 2023. Sequential query encoding for
complex query answering on knowledge graphs. arXiv preprint arXiv:2302.13114
(2023).
[8]Yushi Bai, Xin Lv, Juanzi Li, and Lei Hou. 2023. Answering complex logical queries
on knowledge graphs via query computation tree optimization. In International
Conference on Machine Learning. PMLR, 1472–1491.
[9]Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph database for structuring human
knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on
Management of data. 1247–1250.
[10] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-
sana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. Advances in neural information processing systems 26 (2013).
[11] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam Hruschka,
and Tom Mitchell. 2010. Toward an architecture for never-ending language
learning. In Proceedings of the AAAI conference on artificial intelligence, Vol. 24.
1306–1313.
[12] Xuelu Chen, Ziniu Hu, and Yizhou Sun. 2022. Fuzzy logic based logical query an-
swering on knowledge graphs. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 36. 3939–3948.
[13] Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chan-
dan Reddy. 2021. Probabilistic entity representation model for reasoning over
knowledge graphs. Advances in Neural Information Processing Systems 34 (2021),
23440–23451.
[14] Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chan-
dan K Reddy. 2021. Self-supervised hyperboloid representations from logical
queries over knowledge graphs. In Proceedings of the Web Conference 2021. 1373–
1384.
[15] Brian A Davey and Hilary A Priestley. 2002. Introduction to lattices and order.
Cambridge university press.
[16] Daniel Daza and Michael Cochez. 2020. Message passing query embedding. arXiv
preprint arXiv:2002.02406 (2020).
[17] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.
Convolutional 2d knowledge graph embeddings. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 32.
[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[19] Lise Getoor and Ben Taskar. 2007. Introduction to statistical relational learning.
MIT press.
[20] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In International
conference on machine learning. PMLR, 1263–1272.
[21] Petr Hájek. 2013. Metamathematics of fuzzy logic. Vol. 4. Springer Science &
Business Media.
[22] Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec.
2018. Embedding logical queries on knowledge graphs. Advances in neural
information processing systems 31 (2018).
[23] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous
graph transformer. In Proceedings of the web conference 2020. 2704–2710.
[24] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. 2021.
A survey on knowledge graphs: Representation, acquisition, and applications.
IEEE transactions on neural networks and learning systems 33, 2 (2021), 494–514.[25] Erich Peter Klement, Radko Mesiar, and Endre Pap. 2013. Triangular norms. Vol. 8.
Springer Science & Business Media.
[26] Bhushan Kotnis, Carolin Lawrence, and Mathias Niepert. 2021. Answering
complex queries in knowledge graphs with bidirectional sequence encoders. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 4968–4977.
[27] Timothée Lacroix, Nicolas Usunier, and Guillaume Obozinski. 2018. Canonical
tensor decomposition for knowledge base completion. In International Conference
on Machine Learning. PMLR, 2863–2872.
[28] Leonid Libkin and Cristina Sirangelo. 2009. Open and Closed World Assumptions
in Data Exchange. Description Logics 477 (2009).
[29] Lihui Liu, Boxin Du, Heng Ji, ChengXiang Zhai, and Hanghang Tong. 2021.
Neural-answering logical queries on knowledge graphs. In Proceedings of the
27th ACM SIGKDD conference on knowledge discovery & data mining. 1087–1097.
[30] Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei
Wu, Yuxiao Dong, and Jie Tang. 2022. Mask and reason: Pre-training knowledge
graph transformers for complex logical queries. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. 1120–1130.
[31] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[32] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.
InInternational Conference on Learning Representations. https://openreview.net/
forum?id=Bkg6RiCqY7
[33] Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet K. Dokania,
Mark Coates, Philip H. S. Torr, and Ser-Nam Lim. 2023. Graph Inductive Biases in
Transformers without Message Passing. In International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of
Machine Learning Research, Vol. 202). PMLR, 23321–23337. https://proceedings.
mlr.press/v202/ma23c.html
[34] Zhuang Ma and Michael Collins. 2018. Noise Contrastive Estimation and Neg-
ative Sampling for Conditional Models: Consistency and Statistical Efficiency.
InProceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing. 3698–3707.
[35] David Marker. 2006. Model theory: an introduction. Vol. 217. Springer Science &
Business Media.
[36] Chau Duc Minh Nguyen, Tim French, Wei Liu, and Michael Stewart. 2023. CylE:
Cylinder embeddings for multi-hop reasoning over knowledge graphs. In Pro-
ceedings of the 17th Conference of the European Chapter of the Association for
Computational Linguistics. 1736–1751.
[37] Maximilian Nickel, Volker Tresp, Hans-Peter Kriegel, et al .2011. A three-way
model for collective learning on multi-relational data.. In Icml, Vol. 11. 3104482–
3104584.
[38] Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Denny Zhou, Jure Leskovec,
and Dale Schuurmans. 2022. Smore: Knowledge graph completion and multi-hop
reasoning in massive knowledge graphs. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 1472–1482.
[39] Hongyu Ren, Mikhail Galkin, Michael Cochez, Zhaocheng Zhu, and Jure Leskovec.
2023. Neural graph reasoning: Complex logical query answering meets graph
databases. arXiv preprint arXiv:2303.14617 (2023).
[40] Hongyu Ren*, Weihua Hu*, and Jure Leskovec. 2020. Query2box: Reasoning
over Knowledge Graphs in Vector Space Using Box Embeddings. In Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
BJgr4kSFDS
[41] Hongyu Ren and Jure Leskovec. 2020. Beta embeddings for multi-hop logical
reasoning in knowledge graphs. Advances in Neural Information Processing
Systems 33 (2020), 19716–19726.
[42] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang.
2019. Drum: End-to-end differentiable rule mining on knowledge graphs. Ad-
vances in Neural Information Processing Systems 32 (2019).
[43] Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. 2022. Sequence-to-
Sequence Knowledge Graph Completion and Question Answering. In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). 2814–2828.
[44] Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. 2020. Improving multi-hop
question answering over knowledge graphs using knowledge base embeddings.
InProceedings of the 58th annual meeting of the association for computational
linguistics. 4498–4507.
[45] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of
semantic knowledge. In Proceedings of the 16th international conference on World
Wide Web. 697–706.
[46] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowl-
edge Graph Embedding by Relational Rotation in Complex Space. In Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
HkgEQnRqYQ
[47] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob
Uszkoreit, et al .2021. Mlp-mixer: An all-mlp architecture for vision. Advances in
4128Conditional Logical Message Passing Transformer for Complex Query Answering KDD ’24, August 25–29, 2024, Barcelona, Spain.
neural information processing systems 34 (2021), 24261–24272.
[48] Kristina Toutanova and Danqi Chen. 2015. Observed versus latent features
for knowledge base and text inference. In Proceedings of the 3rd workshop on
continuous vector space models and their compositionality. 57–66.
[49] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume
Bouchard. 2016. Complex embeddings for simple link prediction. In International
conference on machine learning. PMLR, 2071–2080.
[50] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. 2020.
Composition-based Multi-Relational Graph Convolutional Networks. In Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
BylA_C4tPr
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[52] Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: a free collaborative
knowledgebase. Commun. ACM 57, 10 (2014), 78–85.
[53] B Wang, T Shen, G Long, T Zhou, Y Wang, and Y Chang. 2021. Structure-
augmented text representation learning for efficient knowledge graph completion.
InProceedings of the Web Conference 2021. ACM.
[54] Siyuan Wang, Zhongyu Wei, Meng Han, Zhihao Fan, Haijun Shan, Qi Zhang,
and Xuanjing Huang. 2023. Query Structure Modeling for Inductive Logical
Reasoning Over Knowledge Graphs. arXiv preprint arXiv:2305.13585 (2023).
[55] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu,
Juanzi Li, and Jian Tang. 2021. KEPLER: A unified model for knowledge embed-
ding and pre-trained language representation. Transactions of the Association for
Computational Linguistics 9 (2021), 176–194.
[56] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu,
Xiangnan He, and Tat-Seng Chua. 2021. Learning intents behind interactions
with knowledge graph for recommendation. In Proceedings of the web conference
2021. 878–887.
[57] Zihao Wang, Weizhi Fei, Hang Yin, Yangqiu Song, Ginny Y Wong, and Simon
See. 2023. Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with
Local Comparison and Global Transport. arXiv preprint arXiv:2305.04034 (2023).
[58] Zihao Wang, Yangqiu Song, Ginny Wong, and Simon See. 2023. Logical Message
Passing Networks with One-hop Inference on Atomic Formulas. In The Eleventh
International Conference on Learning Representations. https://openreview.net/
forum?id=SoyOsp7i_l
[59] Zihao Wang, Hang Yin, and Yangqiu Song. 2022. Benchmarking the Combinatorial
Generalizability of Complex Query Answering on Knowledge Graphs. Proceedings
of the Neural Information Processing Systems Track on Datasets and Benchmarks 1
(NeurIPS Datasets and Benchmarks 2021) (2022).
[60] Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit semantic ranking
for academic search via knowledge graph embedding. In Proceedings of the 26th
international conference on world wide web. 1271–1279.
[61] Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. DeepPath: A
Reinforcement Learning Method for Knowledge Graph Reasoning. In Proceedings
of the 2017 Conference on Empirical Methods in Natural Language Processing.
564–573.
[62] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful
are Graph Neural Networks?. In International Conference on Learning Representa-
tions. https://openreview.net/forum?id=ryGs6iA5Km
[63] Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, and Jun Zhao. 2023.
Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries
over Knowledge Graphs. In Findings of the Association for Computational Linguis-
tics: EMNLP 2023. 11369–11382.
[64] Zezhong Xu, Wen Zhang, Peng Ye, Hui Chen, and Huajun Chen. 2022. Neural-
symbolic entangled framework for complex query answering. Advances in Neural
Information Processing Systems 35 (2022), 1806–1819.
[65] Bishan Yang, Scott Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015.
Embedding Entities and Relations for Learning and Inference in Knowledge Bases.
InProceedings of the International Conference on Learning Representations (ICLR)
2015.
[66] Dong Yang, Peijun Qing, Yang Li, Haonan Lu, and Xiaodong Lin. 2022. GammaE:
Gamma Embeddings for Logical Queries on Knowledge Graphs. In Proceedings
of the 2022 Conference on Empirical Methods in Natural Language Processing.
745–760.
[67] Hang Yin, Zihao Wang, Fei Weizhi, and Yangqiu Song. 2023. EFO 𝑘-CQA: Towards
Knowledge Graph Complex Query Answering beyond Set Operation. (2023).
[68] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,
Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badly
for graph representation? Advances in Neural Information Processing Systems 34
(2021), 28877–28888.
[69] Wen Zhang, Bibek Paudel, Liang Wang, Jiaoyan Chen, Hai Zhu, Wei Zhang,
Abraham Bernstein, and Huajun Chen. 2019. Iteratively learning embeddings
and rules for knowledge graph reasoning. In The world wide web conference.
2366–2377.[70] Yongqi Zhang and Quanming Yao. 2022. Knowledge graph reasoning with
relational digraph. In Proceedings of the ACM web conference 2022. 912–924.
[71] Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, and Feng Wu. 2021. Cone:
Cone embeddings for multi-hop reasoning over knowledge graphs. Advances in
Neural Information Processing Systems 34 (2021), 19172–19183.
[72] Zhanqiu Zhang, Jie Wang, Jieping Ye, and Feng Wu. 2022. Rethinking graph
convolutional networks in knowledge graph completion. In Proceedings of the
ACM Web Conference 2022. 798–807.
[73] Zhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, and Jian Tang. 2022. Neural-
symbolic models for logical queries on knowledge graphs. In International Con-
ference on Machine Learning. PMLR, 27454–27478.
[74] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. 2021.
Neural bellman-ford networks: A general graph neural network framework for
link prediction. Advances in Neural Information Processing Systems 34 (2021),
29476–29490.
A More Experimental Details
Our code is implemented using PyTorch. We use NVIDIA Tesla
A10 GPU (24GB) and NVIDIA GeForce RTX 3090 GPU (24GB) to
conduct all of our experiments. We use the AdamW [ 32] optimizer,
whose weight decay is 1e-4, to tune the parameters. The learning
rate is 5e-5 for NELL995 and FB15k-237 and 1e-4 for FB15k. The
batch size is 512 for NELL995 and FB15k-237 and 1,024 for FB15k.
The training epoch for all datasets is 100. Following CQD-CO [ 3]
and LMPNN [ 58], we choose ComplEx-N3 [ 27,49] as the neural
link predictor for fair comparison. In this case, we consider the
logical message encoding function 𝜌corresponding to Equations
10–13. For these equations, 𝜆is a hyperparameter that needs to
be determined. In LMPNN application, for simplicity, LMPNN just
lets3𝜆∥·∥=1and then all denominators in these closed-form
expressions are 1, namely:
𝜌(𝑡,𝑟,𝑡→ℎ,0):=𝑟⊗𝑡, (21)
𝜌(ℎ,𝑟,ℎ→𝑡,0):=𝑟⊗ℎ, (22)
𝜌(𝑡,𝑟,𝑡→ℎ,1):=−𝑟⊗𝑡, (23)
𝜌(ℎ,𝑟,ℎ→𝑡,1):=−𝑟⊗ℎ. (24)
To make a fair comparison with LMPNN, in CLMPT application,
we also use the logical message encoding function 𝜌defined in
Equations 21–24 to compute messages passed to the variable nodes.
For the experiments (see Section 5.3.1) to evaluate how much unnec-
essary computational costs can be avoided by conditional logical
message passing mechanism, we repeat the experiments three times
on an RTX 3090 GPU and then average the calculated ↓Memory
(see Equation 19) and ↓Time (see Equation 20) metrics.
B Comparison with More Neural CQA Models
on Q2B Datasets
To further evaluate the performance of CLMPT, we also consider
comparing neural CQA models that cannot handle logical negation
and are trained and evaluated on Q2B datasets [ 40], including GQE
[22], Q2B [ 40], BIQE [ 26], PERM [ 13], kgTransformer [ 30], and
SILR [ 54]. Among these models, BIQE, kgTransformer, and SILR
all use the transformer [ 51] to encode complex queries directly.
BIQE and SILR serialize the query graph and treat the CQA task as
the sequence learning task to solve. kgTransformer uses the pre-
training and fine-tuning strategies based on Heterogeneous Graph
Transformer (HGT) [ 23] to answer complex queries. In essence,
kgTransformer is also a transformer-based graph neural network.
4129KDD ’24, August 25–29, 2024, Barcelona, Spain. Chongzhi Zhang, Zhiping Peng, Junhao Zheng, and Qianli Ma
Table 7: The Hits@3 results of other neural CQA baselines
on answering EPFO queries generated by Q2B [ 40]. Avg rep-
resents the average results of all query types. The boldface
indicates the best results.
Dataset
Model 1p 2p 3p 2i 3i pi ip 2u up Avg
FB237-Q2BGQE
40.5 21.3 15.3 29.8 41.1 18.2 8.5 16.7 16.0 23.0
Q2B 46.7 24.0 18.6 32.4 45.3 20.5 10.8 23.9 19.3 26.8
BIQE 47.2 29.3 24.5 34.5 47.3 21.1 9.6 7.5 14.5 26.2
PERM 52.0 28.6 21.6 36.1 49.0 21.2 12.8 30.5 23.9 30.6
kgTransformer 45.9 31.2 27.6 39.8 52.8 28.6 18.9 26.3 21.4 32.5
SILR 47.1 30.2 24.9 35.8 48.4 22.2 11.3 28.3 18.1 29.6
CLMPT 50.5 31.3 26.7 39.0 52.1 27.7 16.3 28.6 24.1 32.9
NELL-Q2BGQE
41.7 23.1 20.3 31.8 45.4 18.8 8.1 20.0 13.9 24.8
Q2B 55.5 26.6 23.3 34.3 48.0 21.2 13.2 36.9 16.3 30.6
BIQE 63.2 31.0 33.2 37.0 52.5 16.4 9.1 17.3 18.4 30.9
PERM 58.1 28.6 24.3 35.2 50.8 19.5 14.3 46.0 20.0 32.8
kgTransformer 62.5 40.1 36.7 40.5 54.6 30.6 20.3 46.9 27.0 39.9
SILR 64.1 32.9 33.7 37.6 53.2 17.7 5.5 45.0 18.0 34.2
CLMPT 64.3 39.9 36.5 42.1 54.1 28.1 22.1 48.9 33.4 41.1
Table 8: The MRR results of symbolic integration models and
CLMPT. The boldface indicates the best results.
Dataset
Model 1p 2p 3p 2i 3i pi ip 2u up 2in 3in inp pin pni Avg pAvg n
FB15k-237GNN-QE
42.8 14.7 11.8 38.3 54.1 31.1 18.9 16.2 13.4 10.0 16.8 9.3 7.2 7.8 26.8 10.2
ENeSy 44.7 11.7 8.6 34.8 50.4 27.6 19.7 14.2 8.4 10.1 10.4 7.6 6.1 8.1 24.5 8.5
CLMPT
-small(F) 46.1 12.6 10.0 36.1 50.4 26.7 17.5 14.9 10.3 7.7 11.6 7.5 4.4 5.0 25.0 7.3
CLMPT 45.7 13.7 11.3 37.4 52.0 28.2 19.0 14.3 11.1 7.7 13.7 8.0 5.0 5.1 25.9 7.9
NELL995GNN-QE
53.3 18.9 14.9 42.4 52.5 30.8 18.9 15.9 12.6 9.9 14.6 11.4 6.3 6.3 28.9 9.7
ENeSy 59.0 18.0 14.0 39.6 49.8 29.8 24.8 16.4 13.1 11.3 8.5 11.6 8.6 8.8 29.4 9.8
CLMPT
-small(F) 60.6 21.7 17.7 42.2 51.7 30.7 24.5 19.4 15.6 6.4 7.9 11.0 3.7 4.5 31.6 6.7
CLMPT 58.9 22.1 18.4 41.8 51.9 28.8 24.4 18.6 16.2 6.6 8.1 11.8 3.8 4.5 31.3 7.0
We use the results reported in these papers [ 30,54] for comparison.
Since they report the Hits@3 results, for a fair comparison, we
also use Hits@3 as the evaluation metric, which calculates the
proportion of correct answer entities ranked among the top 3.
As shown in the results in Table 7, CLMPT reaches the best per-
formance on average across all datasets. For those models that use
the transformer to encode the entire query graph directly, namely
BIQE, SILR, and kgTransformer, we speculate that their insufficient
graph inductive biases may negatively affect their performance. To
encode the entire query graph at once using the transformer, BIQE
designs a special positional encoding scheme, and SILR designs a
particular graph serialization method and structural prompt. These
designs can be viewed as introducing specific graph inductive biases
into the transformer to encode the query graph. For kgTransformer,
it relies primarily on the graph inductive biases of the HGT ar-
chitecture. According to the progress of the graph transformers
[33,68], graph inductive biases are critical to using the transformer
to encode graph data. In particular, structural encoding is important
for the graph transformers without message passing. Therefore, we
speculate that the graph inductive biases introduced by these CQA
models may not be sufficient to encode the query graph that defines
complex logical dependencies. On the other hand, it is not clear how
to introduce appropriate graph inductive biases for query graphs
that define first-order logical dependencies. By contrast, CLMPT,
based on logical message passing, uses the transformer to aggregate
messages and update node embeddings rather than to encode the
entire query graph. This difference makes CLMPT less dependent
on graph inductive biases than the transformer-based CQA models
mentioned above. However, it also suggests that appropriate graph
inductive biases may enhance the performance of CLMPT. This
extension can be explored in the future.C Comparison with Symbolic Integration
Models
We discuss the differences between symbolic integration CQA mod-
els and neural CQA models in Section 2. The model proposed in
this paper is a neural CQA model, so our comparisons and related
discussions are mainly carried out within the scope of neural CQA
models. Since symbolic information can enhance the performance
of neural CQA models [ 58], in this section, we consider two repre-
sentative symbolic integration models, GNN-QE [ 73] and ENeSy
[64], for comparison with CLMPT to show the potential. We list
the MRR results reported in their original papers. The results are
shown in Table 8. It is found that CLMPT is also competitive even
with the symbolic integration models on answering EPFO queries.
Specifically, CLMPT achieves a better average performance than
these symbolic integration models on EPFO queries for NELL995.
For FB15k-237, CLMPT still has a gap in performance with GNN-
QE, but it can outperform ENeSy on EPFO queries. For the negative
queries, there are still gaps between the neural CQA models and
these symbolic integration models because the fuzzy sets equipped
with symbolic information used in the reasoning process of these
models can effectively deal with logical negation through proba-
bilistic values. These results suggest that neural models can be po-
tentially improved with symbolic integration. However, it is worth
noting that introducing additional symbolic information requires
larger computational costs.
According to [ 39,58], for larger knowledge graphs, neural CQA
models and symbolic integration models have different scalabil-
ities. As we discussed in Section 2, symbolic integration models
require more computing resources and are more likely to suffer
from scalability issues. For example, GNN-QE employs NBFNet [ 74]
to compute message passing on the whole KG, resulting in complex-
ity that is linear to (|E|+|V|)𝑑 , where|E|is the number of edges
in KG,|V|is the number of nodes in KG, and 𝑑is the embedding
dimension. For CLMPT, which only performs message passing on
the query graph, the complexity is just 𝑂(𝑑). Besides, GNN-QE es-
timates the probability of whether each entity is the answer at each
intermediate step, making the size of its fuzzy sets scale linearly
with|V|. As a result, GNN-QE requires much more computational
costs, requiring 128GB GPU memory to run a batch size of 32. For
CLMPT-small(F), which has only 32M trainable parameters, it only
requires less than 12GB GPU memory to run a batch size of 1024.
Even in this case, CLMPT-small(F) can still achieve performance
competitive with GNN-QE on NELL995. We suspect that integrating
symbolic information into CLMPT can improve the performance,
especially on negative queries. However, exploring how to integrate
symbolic information into CLMPT is beyond the scope of this paper.
This extension is left for future work.
D Limitations
Due to the use of the transformer architecture in the node em-
bedding update scheme in CLMPT, more computational costs are
required compared to some previous works based on multi-layer
perceptron. In addition, CLMPT only achieves sub-optimal perfor-
mance on negative queries. We suspect that integrating symbolic
information into CLMPT may improve the performance on negative
queries.
4130