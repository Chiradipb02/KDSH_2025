Transportation Marketplace Rate Forecast Using Signature
Transform
Haotian Gu
University of California, Berkeley
Berkeley, CA, USA
haotian_gu@berkeley.eduXin Guo
Amazon.com Inc.
Worldwide Operations Research
Science
Santa Clara, CA, USA
University of California, Berkeley
Berkeley, CA, USA
xinguo@berkeley.eduTimothy L. Jacobs
Amazon.com Inc.
Worldwide Operations Research
Science
Bellevue, WA, USA
timojaco@amazon.com
Philip Kaminsky
Amazon.com Inc.
Worldwide Operations Research
Science
Santa Clara, CA, USA
University of California, Berkeley
Berkeley, CA, USA
philipka@amazon.comXinyu Li∗
University of California, Berkeley
Berkeley, CA, USA
xinyu_li@berkeley.edu
Abstract
Freight transportation marketplace rates are typically challenging
to forecast accurately. In this work, we have developed a novel
statistical technique based on signature transforms and have built
a predictive and adaptive model to forecast these marketplace rates.
Our technique is based on two key elements of the signature trans-
form: one being its universal nonlinearity property, which linearizes
the feature space and hence translates the forecasting problem into
linear regression, and the other being the signature kernel, which
allows for comparing computationally efficiently similarities be-
tween time series data. Combined, it allows for efficient feature
generation and precise identification of seasonality and regime
switching in the forecasting process.
An algorithm based on our technique has been deployed by
Amazon trucking operations, with far superior forecast accuracy
and better interpretability versus commercially available industry
models, even during the COVID-19 pandemic and the Ukraine con-
flict. Furthermore, our technique is able to capture the influence of
business cycles and the heterogeneity of the marketplace, improv-
ing prediction accuracy by more than fivefold, with an estimated
annualized saving of $50 million.
CCS Concepts
•Applied computing →Forecasting; •Computing method-
ologies→Regularization .
∗Corresponding author.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671637Keywords
Time series forecast; signature transform; two-step LASSO; adaptive
regression
ACM Reference Format:
Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li.
2024. Transportation Marketplace Rate Forecast Using Signature Transform.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 9 pages. https://doi.org/10.1145/3637528.3671637
1 Introduction
Overview. Linehaul freight transportation costs make up a sig-
nificant portion of overall Amazon transportation costs. To manage
these costs, Amazon has developed a variety of tools to manage
linehaul capacity mix and procurement. One key input to all of
these models is a forecast of transportation freight marketplace
rates, which however are notoriously difficult to forecast – they
are driven by a number of factors: the ever-changing network of
tens of thousands of drivers, shippers of all sizes with a mix of
occasional, seasonal, and regular demands, a huge set of brokers,
traditional and digital exchanges, and local, regional, national, and
international economic factors of all kinds. In addition, the trans-
portation marketplace frequently goes through fundamental shifts
– either because of wars, pandemics, fuel prices, or due to shifting
international trade patterns.
Although Amazon has purchased externally created forecasts for
some time, these forecasts are neither explainable nor sufficiently
accurate to meet specific Amazon needs. To address this challenge,
we have built a forecasting model based on time series data to pre-
dict weekly freight marketplace rates for the North America market,
at both the national and the regional levels. Our approach incorpo-
rates an innovative signature-based statistical technique capable
4997
KDD ’24, August 25–29, 2024, Barcelona, Spain Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li
of efficiently capturing significant fluctuations in transportation
marketplace rates.
The key challenges in time series forecasting. Time series data
consists of sequential observations recorded over time and is ubiq-
uitous: finance, economics, transportation, weather, and energy
prices. Given time series data, forecasting additional data points is
critical for informed decision-making and process optimization in
almost every organization and industry.
Time series prediction models such as Autoregressive Integrated
Moving Average (ARIMA) [ 26] and Exponential Smoothing [ 13]
assume that the time series are stationary, which is not the case
for freight marketplace rates. Moreover, ARIMA has limited ability
to capture seasonality and long-term trends [ 20], and Exponential
Smoothing may be insufficient for abrupt changes or outliers and
produce unstable forecasts [ 29]. Furthermore, these methods rely
solely on historical data from the time series, which is inadequate
in capturing the causal relation between the economic factors and
the marketplace rates. Meanwhile, machine learning algorithms
such as Long Short-Term Memory Neural Networks [ 33] and Gated
Recurrent Units [ 10], though capable of capturing nonlinear re-
lationship and complex patterns in time series data, will require
substantially more training data which is not available in our case.
Indeed, one of the main challenges in analyzing time series data
is their ever-changing statistical properties, due to factors includ-
ing changes in business and economic cycles, shifts in policy, or
changes in market conditions. In our case, the market itself has re-
cently experienced shifts in regimes andseasonality [16], in terms of
volatility, trends, and cyclical patterns, partly due to the COVID-19
pandemic and the Ukraine conflict.
Machine learning models and signature transform. Much of sta-
tistical learning theory relies on finding a feature map that em-
beds the data (for instance, samples of time series) into a high-
dimensional feature space. Two requirements for an ideal feature
map are universality, meaning that non-linear functions of the data
are approximated by linear functionals in the feature space; and
characteristicness, meaning that the expected value of the feature
map characterizes the law of the random variable. It is shown that
with the technique of the signature transform these two proper-
ties are in duality and therefore often equivalent [ 27]. This is the
primary inspiration for our proposed signature-based forecasting
technique for our forecast models.
Originally introduced and studied in algebraic topology [ 5,6],
thesignature transform, sometimes referred to as the path signature
or simply signature, has been further developed in rough path
theory [ 12,24], introduced for financial applications [ 1,17,22,23]
and machine learning [ 4,18,21,31,32], and most recently to time
series data analysis [ 9,11,25]. Given any continuous or discrete
time series, their signature transform produces a vector of real-
valued features that extract information such as order and area,
and explicitly considers combinations of different channels. The
signature of time series uniquely determines the time series, and
does so in a computationally efficient way. Most importantly, every
continuous function of a time series data may be asymptotically
approximated by a linear functional of its signature. In other words,
signature transform linearizes the otherwise complicated featurespace, and thus is a powerful tool for feature generation and pattern
identification in machine learning.
Our work. We propose a novel signature-based statistical tech-
nique for the time series forecasting problem. This is based on two
key elements of the signature transform. The first is the universal
nonlinearity property of the signature transform, which linearizes
the feature space of the time series data and hence translates the
forecasting problem into a linear regression. The second is the signa-
ture kernel which allows for computationally efficient comparison
of similarities between time series data. Technically, this is to iden-
tify different “signature feature maps”, the statistical counterpart
of identifying different distributions for a given time series data,
albeit in the linearized feature space from the signature transform.
Our approach starts by collecting data including a hundred of
market supply and demand factors, and runs a correlation test be-
tween the marketplace rate and the factors to remove non-significant
factors and to identify factors that may be colinear. We then exploit
the universal nonlinearity property of signature transform to con-
struct signature features as suitable candidates for the “internal”
features. To avoid issues of overfitting and co-linearity between
the signature feature and the external factors, and to improve the
forecast accuracy, we adopt the two-step LASSO for the regres-
sion analysis [ 2]. Finally, this two-step LASSO is enhanced with
adaptive weight using a signature kernel, which enables captur-
ing changes in regimes or seasonality. Combined, this leads to our
signature-based adaptive two-step LASSO approach. This novel
signature-transform-based technique for data analysis allows for
efficient feature generation and more precise identification of sea-
sonality and regime switching embedded in the data.
Implementation and real-time performance. This signature-based
adaptive two-step LASSO algorithm has been implemented for
the trucking operations in Amazon since November 2022. Perfor-
mance analysis shows that our forecast model presents superior
performance than commercially available forecast models, while
providing significantly better interpretability. Despite the onset of
COVID-19 and the Ukraine conflict, it captures the influence of
business cycles and heterogeneity of the marketplace, improves
prediction accuracy by more than fivefold, and has an estimated
annualized saving of approximately $50million.
2 Technical Background
2.1 Signatures of Continuous Paths
We begin with the definition of signatures of continuous piecewise
smooth paths.
Notation. LetR𝑑1⊗R𝑑2⊗···⊗ R𝑑𝑛denote the space of all real
tensors with shape 𝑑1×𝑑2×···×𝑑𝑛. Define a binary operation
called tensor product, denoted by ⊗, which maps a tensor of shape
(𝑑1,...,𝑑 𝑛)and a tensor of shape (𝑒1,...,𝑒 𝑚)to a tensor of shape
(𝑑1,...,𝑑 𝑛,𝑒1,...,𝑒 𝑚)via 𝐴𝑖1,...,𝑖𝑛,𝐵𝑗1,...,𝑗 𝑚↦→𝐴𝑖1,...,𝑖𝑛𝐵𝑗1,...,𝑗 𝑚.
When applied to two vectors, it reduces to the outer product. Let
R𝑑⊗𝑘
=R𝑑⊗···⊗ R𝑑, and𝑣⊗𝑘=𝑣⊗···⊗𝑣for𝑣∈R𝑑, in each
case with𝑘−1many⊗.
4998Transportation Marketplace Rate Forecast Using Signature Transform KDD ’24, August 25–29, 2024, Barcelona, Spain
Definition 1. Let𝑎<𝑏∈R, and𝑋=
𝑋1,...,𝑋𝑑
:[𝑎,𝑏]→
R𝑑be a continuous piecewise smooth path. The signature of 𝑋is then
defined as the collection of iterated integrals
Sig(𝑋)=∫
𝑎<𝑡1<···<𝑡𝑘<𝑏d𝑋𝑡1⊗···⊗ d𝑋𝑡𝑘
𝑘≥0
= ∫
𝑎<𝑡1<···<𝑡𝑘<𝑏d𝑋𝑖1
𝑡1···d𝑋𝑖𝑘
𝑡𝑘
1≤𝑖1,...,𝑖𝑘≤𝑑!
𝑘≥0,
where⊗denotes the tensor product, d𝑋𝑡=d𝑋𝑡
d𝑡d𝑡, and the𝑘=0
term is taken to be 1∈R. The truncated signature of depth 𝑁of𝑋is
defined as
Sig𝑁(𝑋)=∫
𝑎<𝑡1<···<𝑡𝑘<𝑏d𝑋𝑡1⊗···⊗ d𝑋𝑡𝑘
0≤𝑘≤𝑁.(1)
Remark 1. The signature can be defined more generally on paths
of bounded variation [ 11], but the above definition suffices for our
purposes.
Example 1. Suppose𝑋:[𝑎,𝑏]→R𝑑is the linear interpolation of
two points𝑥,𝑦∈R𝑑, so that𝑋𝑡=𝑥+𝑡−𝑎
𝑏−𝑎(𝑦−𝑥). Then its signature
is the collection of tensor products of its total increment:
Sig(𝑋)=
1,𝑦−𝑥,1
2(𝑦−𝑥)⊗2,1
6(𝑦−𝑥)⊗3,...,1
𝑘!(𝑦−𝑥)⊗𝑘,...
,
which is independent of 𝑎,𝑏.
Example 2. Suppose𝑋:[𝑎,𝑏]→Ris a one-dimensional smooth
path. Then its signature is the collection of powers of its total incre-
ment:
Sig(𝑋)=
1,𝑋(𝑏)−𝑋(𝑎),1
2(𝑋(𝑏)−𝑋(𝑎))2,1
6(𝑋(𝑏)−𝑋(𝑎))3,
...,1
𝑘!(𝑋(𝑏)−𝑋(𝑎))𝑘,...
,
which is independent of 𝑋(𝑡),𝑡∈(𝑎,𝑏). Furthermore, when 𝑋(𝑡)is a
random process, the expected signature
E[Sig(𝑋)]=
1,E[𝑋(𝑏)−𝑋(𝑎)],1
2E
(𝑋(𝑏)−𝑋(𝑎))2
,
...,1
𝑘!Eh
(𝑋(𝑏)−𝑋(𝑎))𝑘i
,...
,
whenever it exists, describes precisely the moments of 𝑋(𝑏)−𝑋(𝑎).
Thus, for a high-dimensional stochastic process 𝑋(𝑡), the expected
signature naturally forms the generalization of the moments of the
process. In other words, for a stochastic process 𝑋(𝑡), its expected
signature characterizes the law of 𝑋(𝑡)up to tree-like equivalence, as
proved in [8].
Example 2 shows that the signature for a one-dimensional path
only depends on its total increment. In general, it implies that
the signature of a path itself may not carry sufficient information
to fully characterize the path. Nevertheless, this problem may be
resolved by considering the time-augmented version of the original
path (see Definition 4, Theorem 1 and 2 below).2.2 Signature of Discrete Data
To define and compute signatures of discrete data streams, one can
simply do linear interpolations and then apply signature transforms.
Definition 2. The space of streams of data is defined as
S
R𝑑
=n
𝑥𝑥𝑥=(𝑥𝑥𝑥1,...,𝑥𝑥𝑥𝑛):𝑥𝑥𝑥𝑖∈R𝑑,𝑛∈No
.
Given𝑥𝑥𝑥=(𝑥𝑥𝑥1,...,𝑥𝑥𝑥𝑛)∈S
R𝑑
, the integer 𝑛is called the length
of𝑥𝑥𝑥.Furthermore for 𝑎,𝑏∈Rsuch that𝑎<𝑏, fix
𝑎=𝑢1<𝑢2<···<𝑢𝑛−1<𝑢𝑛=𝑏. (2)
Let𝑋=
𝑋1,...,𝑋𝑑
:[𝑎,𝑏]→R𝑑be continuous such that 𝑋𝑢𝑖=
𝑥𝑥𝑥𝑖for all𝑖, and linear on the intervals in between. Then 𝑋is called a
linear interpolation of 𝑥𝑥𝑥.
Definition 3. Let𝑥𝑥𝑥=(𝑥𝑥𝑥1,...,𝑥𝑥𝑥𝑛)∈S
R𝑑
be a stream of data.
Let𝑋be a linear interpolation of 𝑥𝑥𝑥. Then the signature of 𝑥𝑥𝑥is defined
asSig(𝑥𝑥𝑥)=Sig(𝑋),and the truncated signature of depth 𝑁of𝑥𝑥𝑥is
defined as Sig𝑁(𝑥𝑥𝑥)=Sig𝑁(𝑋).
Definition 4. Given a path 𝑋:[𝑎,𝑏]→R𝑑, define the corre-
sponding time-augmented path by b𝑋𝑡=(𝑡,𝑋𝑡), a path in R𝑑+1.
2.3 Key Properties of Signature
Theorem 1 (Uniqeness [ 15]).Let𝑋:[𝑎,𝑏]→R𝑑be a contin-
uous piecewise smooth path. Then Sig(b𝑋)uniquely determines 𝑋up
to translation.
In fact, the signature not only determines a path uniquely up
to translation, but also linearizes any continuous functions of the
path, as stated in the next theorem.
Theorem 2 (Universal nonlinearity [ 1]).Let𝐹be a real-
valued continuous function on continuous piecewise smooth paths in
R𝑑and letKbe a compact set of such paths. Furthermore assume
that𝑋0=0for all𝑋∈K. (To remove the translation invariance.) Let
𝜀>0. Then there exists a linear functional 𝐿such that for all 𝑋∈K,
|𝐹(𝑋)−𝐿(Sig(b𝑋))|<𝜀.
This universal nonlinearity is the key property of the signature
transform and is important for our model, and in general for appli-
cations in feature augmentations. See [ 23], [21], [25] for examples.
Note that the signature by definition is an infinite-dimensional
tensor. In practice, one can only compute the truncated signature
Sig𝑁in(1)up to some depth 𝑁. The next result guarantees that
reminder terms in the truncation decay factorially.
Theorem 3 (Factorial decay [ 24]).Let𝑋:[𝑎,𝑏]→R𝑑be a
continuous piecewise smooth path and let ∥·∥ be a tensor norm on
R𝑑⊗𝑘
. Then
∫
𝑎<𝑡1<···<𝑡𝑘<𝑏d𝑋𝑡1⊗···⊗ d𝑋𝑡𝑘≤𝐶(𝑋)𝑘
𝑘!,
where𝐶(𝑋)is a constant depending on 𝑋.
The next property about signatures, Theorem 4, is the invariance
to time reparameterizations. It implies that the signature encodes the
data by its arrival order and independently of its arrival time. This
4999KDD ’24, August 25–29, 2024, Barcelona, Spain Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li
is a desired property in many applications such as hand-writing
recognition [ 32], [31]. Meanwhile, there is an interesting interplay
between Theorem 2 and Theorem 4: in a problem where time param-
eterizations are irrelevant, it suffices to compute the signature of 𝑋
by Theorem 4; However, if time parameterization is important, then
according to Theorem 2, applying the signature transform to the
time-augmented path b𝑋ensures that parameterization-dependent
features are still learned.
Theorem 4 (Invariance to time reparameterizations [ 24]).
Let𝑋:[0,1] →R𝑑be a continuous piecewise smooth path. Let
𝜓:[0,1]→[ 0,1]be continuously differentiable, increasing, and
surjective. Then Sig(𝑋)=Sig(𝑋◦𝜓).
Note that by Theorem 4, the signature of a stream of data is
independent of the choice of 𝑢𝑖in a linear interpolation in (2).
Meanwhile, by Theorem 2, in order to learn parameterization-
dependent features, one can apply the signature transform to the
time-augmented data stream b𝑥𝑥𝑥= b𝑥𝑥𝑥1,...,b𝑥𝑥𝑥𝑛, where b𝑥𝑥𝑥𝑖=(𝑡𝑖,𝑥𝑥𝑥𝑖)∈
R𝑑+1, and𝑡𝑖is the time when the data point 𝑥𝑥𝑥𝑖arrives.
Let𝑥𝑥𝑥=(𝑥𝑥𝑥1,...,𝑥𝑥𝑥𝑛)∈S
R𝑑
be a data stream of length 𝑛in
R𝑑.Then Sig𝑁(𝑥𝑥𝑥)has
𝑀(𝑑,𝑁):=𝑁∑︁
𝑘=0𝑑𝑘=𝑑𝑁+1−1
𝑑−1(3)
components. In particular, the number of components does not
depend on the length of the data stream 𝑛. The truncated signature
maps the infinite-dimensional space of streams of data S
R𝑑
into
a finite-dimensional space of dimension
𝑑𝑁+1−1
/(𝑑−1). Thus
the signature is an efficient way to tackle long streams of data, or
streams of variable length.
2.4 Computation of Signature Transform
The signature transform of a data stream can be computed in an
efficient and tractable way, with the help of Chen’s identity [ 4,24].
It starts by introducing the following ⊠operation: with 𝐴0=𝐵0=1,
define ⊠by
⊠: 𝑁Ö
𝑘=1
R𝑑⊗𝑘!
× 𝑁Ö
𝑘=1
R𝑑⊗𝑘!
→𝑁Ö
𝑘=1
R𝑑⊗𝑘
,
(𝐴1,...𝐴 𝑁)⊠(𝐵1,...,𝐵 𝑁)↦→©­
«𝑘∑︁
𝑗=0𝐴𝑗⊗𝐵𝑘−𝑗ª®
¬1≤𝑘≤𝑁.(4)
Chen’s identity [ 12] states that the image of the signature transform
forms a group structure with respect to ⊠. That is, given a sequence
of data(𝑥1,...,𝑥 𝐿)∈S
R𝑑
and some𝑗∈{2,...,𝐿−1},
Sig𝑁((𝑥1,...,𝑥 𝐿))=Sig𝑁  𝑥1,...,𝑥 𝑗⊠Sig𝑁  𝑥𝑗,...,𝑥 𝐿.
Furthermore, from Example 1, the signature of a sequence of length
two can be computed explicitly from the definition. Letting
exp :R𝑑→𝑁Ö
𝑘=1
R𝑑⊗𝑘
,exp :𝑣→
𝑣,𝑣⊗2
2!,𝑣⊗3
3!,...,𝑣⊗𝑁
𝑁!
,
(5)then
Sig𝑁((𝑥1,𝑥2))=exp(𝑥2−𝑥1).
Chen’s identity further implies that the signature transform can be
computed by
Sig𝑁((𝑥1,...,𝑥 𝐿))=exp(𝑥2−𝑥1)⊠exp(𝑥3−𝑥2)
⊠···⊠exp(𝑥𝐿−𝑥𝐿−1).(6)
(6)implies that computing the signature of an incoming stream of
data is efficient and scalable. Indeed, suppose one has obtained a
stream of data and computed its signature. Then after the arrival
of some more data, in order to compute the signature of the entire
signal, one only needs to compute the signature of the new piece of
information, which is then computed via the tensor product with
the previously-computed signature.
Improving computational efficiency. Recall from (6)that the signa-
ture may be computed by evaluating several ⊠in(4)andexpin(5).
We begin by noticing that the key component in the computation
is to evaluate 𝑁Ö
𝑘=1
R𝑑⊗𝑘!
×R𝑑→𝑁Ö
𝑘=1
R𝑑⊗𝑘
, 𝐴,𝑧↦→𝐴⊠exp(𝑧).
Instead of computing 𝐴⊠exp(𝑧)conventionally through the com-
position of expand⊠, [18] suggests to speed up the computation
by Horner’s method. More specifically, it is to expand
𝐴⊗exp(𝑧)= 𝑘∑︁
𝑖=0𝐴𝑖⊗𝑧⊗(𝑘−𝑖)
(𝑘−𝑖)!!
1≤𝑘≤𝑁,
so that the𝑘-th term can be computed by
𝑘∑︁
𝑖=0𝐴𝑖⊗𝑧⊗(𝑘−𝑖)
(𝑘−𝑖)!=  
···𝑧
𝑘+𝐴1
⊗𝑧
𝑘−1+𝐴2
⊗𝑧
𝑘−2+···!
⊗𝑧
2+𝐴𝑘−1!
⊗𝑧+𝐴𝑘.
As proved in [ 18], this method has uniformly (over 𝑑,𝑁) fewer
scalar multiplications than the conventional approach, and reduces
the asymptotic complexity of this operation from O
𝑁𝑑𝑁
to
O
𝑑𝑁
. Furthermore, this rate is asymptotically optimal, since
the size of the result (an element ofÎ𝑁
𝑘=1
R𝑑⊗𝑘
), is itself of size
O
𝑑𝑁
.
3 Forecasting Problem and Our Approach
3.1 Forecasting Problem
The freight marketplace rate forecast problem involves two time se-
ries{𝑥𝑥𝑥𝜏}𝜏∈N+and{𝑦𝜏}𝜏∈N+. Here,𝑥𝑥𝑥𝜏∈X⊆ R𝑑0is a𝑑0-dimensional
vector representing values of the key economic factors that drive
the supply and demand in the freight marketplace at time 𝜏. Factors
from the market supply side include information regarding the sup-
ply of drivers and trucks and fuel/oil prices. Market demand factors
include imports, agriculture information, manufacturing activities,
housing indexes, and railway transport. Additionally, 𝑦𝜏∈Y⊆ R
is the freight marketplace rate at time 𝜏. Previously, Amazon relied
5000Transportation Marketplace Rate Forecast Using Signature Transform KDD ’24, August 25–29, 2024, Barcelona, Spain
on a commercial service to obtain forecasts for future marketplace
rates. However, those forecasts lacked accuracy and transparency.
To address these, we consider the following forecast problem.
The general goal of our forecast problem is to find models 𝑓∗
Δ𝑡∈
F ⊆ {𝑓|𝑓:X → Y} such that𝑓∗
Δ𝑡(𝑥𝑥𝑥𝜏) ≈𝑦𝜏+Δ𝑡, for Δ𝑡=
1,2,3,···,Δ𝑇,where Δ𝑇∈N+denotes the longest forecast horizon
andFis the class of all admissible models. More precisely, given
the data up to time 𝑡:{(𝑥𝑥𝑥𝜏,𝑦𝜏)}𝜏=1,2,···,𝑡, to make prediction for
𝑦𝑡+Δ𝑡, one standard approach to find 𝑓∗
Δ𝑡∈F is by solving the
following optimization problem:
𝑓∗
Δ𝑡∈arg min
𝑓∈F(
1
𝑡−Δ𝑡𝑡−Δ𝑡∑︁
𝜏=1𝐿(𝑓(𝑥𝑥𝑥𝜏),𝑦𝜏+Δ𝑡))
, (7)
where𝐿:Y×Y→ Ris a loss function measuring the difference
between the model prediction 𝑓(𝑥𝑥𝑥𝜏)and the actual 𝑦𝜏+Δ𝑡. Once𝑓∗
Δ𝑡
is obtained, the prediction of 𝑦𝑡+Δ𝑡is given by b𝑦𝑡+Δ𝑡:=𝑓∗
Δ𝑡(𝑥𝑥𝑥𝑡).
3.2 Our Approach
We will present a signature-based adaptive two-step LASSO ap-
proach that we have developed and implemented in Amazon, which
has demonstrated excellent performance in solving this problem.
Data. Our approach and experiment start by collecting data
involving over a hundred of national and regional market supply
and demand factors, downloaded from the governmental public
websites, including Federal Reserve Bank and Bureau of Labor
Statistics, as well as industrial databases such as Logistic Manager.
The time range for the data is from 2018 to 2022.
External factor preprocessing. We first run a correlation test be-
tween the marketplace rate and the factors to remove non-significant
factors, with further correlation analysis to identify factors that
may be colinear. After this round of elimination, over forty factors
remain, including consumer price index, housing index, oil and
gas drilling, logistic managers’ index, employment information,
weather, and other market benchmarks.
Internal features via signature transform. Besides the “external”
factors𝑥𝑥𝑥𝜏, most time series forecasting approaches, such as ARIMA,
also construct “internal” features from the history of 𝑦𝜏. Those “in-
ternal” features may help to characterize the trend, momentum,
and stationarity of 𝑦𝜏. We, instead, exploit the universal nonlin-
earity property of signature transform (Theorem 2), and construct
signature features as suitable candidates for the “internal” features.
More specifically, for any time step 𝜏∈N+and time window size
𝑙∈N, denote𝑦𝜏−𝑙:𝜏:=(𝑦𝜏−𝑙,···,𝑦𝜏)as the slice of the time series
{𝑦𝑡}𝑡∈N+from time𝜏−𝑙to𝜏. The feature vector for predicting
𝑦𝜏+Δ𝑡consists of both the economic factors 𝑥𝑥𝑥𝜏and the depth- 𝑁
signature features Sig𝑁(𝑦𝜏−𝑙:𝜏). We denote the concatenation of
those two sets of features as
𝒙𝜏,Sig𝑁(𝑦𝜏−𝑙:𝜏)
, whose dimension
is denoted by 𝑑.
Two-step LASSO. The universal nonlinearity property of the sig-
nature transform linearizes the feature space, hence translating
the forecasting problem into a linear regression analysis. Since the
dimension𝑑of the feature vector may be relatively large compared
to the number of historical samples, especially when the time step 𝑡
Figure 1: The Data Flow of the Adaptive Two-step LASSO via
Signature Kernel (Algorithm 2)
is small, we adopt the approach of two-step LASSO to avoid overfit-
ting and the issue of co-linearity especially between the signature
feature and the external factors. The first step is to select the factors
by solving the standard LASSO regression [ 30], [34], [35]. This is to
add an𝐿1-regularization to model coefficients in the ordinary least
square objective. This 𝐿1-regularization will encourage the sparsity
of model coefficients, and prevent the over-fitting problem. In the
second step, an OLS with only the selected factors is applied. This
two-step LASSO estimation procedure has been shown to produce
a smaller bias than standard LASSO for a range of models [2], [7].
More precisely, recall that the LASSO regression is to solve the
following optimization problem:
b𝜽𝜆
LASSO ,Δ𝑡∈arg min
𝜽∈R𝑑(
1
𝑡−Δ𝑡𝑡−Δ𝑡∑︁
𝜏=1
𝑦𝜏+Δ𝑡
−h
𝒙𝜏,Sig𝑁(𝑦𝜏−𝑙:𝜏)i
·𝜽2
+𝜆∥𝜽∥1)
.(8)
Here the constant 𝜆, called the regularization parameter, controls
the sparsity of coefficients: a higher value of 𝜆leads to a smaller
number of nonzero coefficients in b𝜽𝜆
LASSO ,Δ𝑡. In the two-step LASSO,
the first step is to select the factors by solving the LASSO regression
in(8), and get b𝜽𝜆
LASSO ,Δ𝑡. In the second step, the subsequent OLS
refitting is to find 𝜽𝜆
LASSO ,Δ𝑡such that
𝜽𝜆
LASSO ,Δ𝑡∈ arg min
supp[𝜽]=supph
b𝜽𝜆
LASSO ,Δ𝑡i(9)
(
1
𝑡−Δ𝑡𝑡−Δ𝑡∑︁
𝜏=1
𝑦𝜏+Δ𝑡−h
𝒙𝜏,Sig𝑁(𝑦𝜏−𝑙:𝜏)i
·𝜽2)
.
5001KDD ’24, August 25–29, 2024, Barcelona, Spain Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li
Adaptive weight via signature kernel. In the classical approach of
two-step LASSO, each historical sample is given equal weight in
the optimization problem to obtain the model at time 𝑡. However,
this equal-weight scheme may fail to account for changes of regime
or seasonality. Instead, a more effective approach would be to dy-
namically assign weights based on their similarity to the current
period. This is precisely what we propose, as elaborated below.
First, recall the signature feature map (Theorem 2),
Φ:𝑋↦→Sig(b𝑋) (10)
is a universal feature map from the path space to the linear space of
signatures [ 9]. To avoid computation over a large space of functions,
we kernelize the signature feature map Φin(10), and define the
signature kernel 𝑘(𝑎𝑎𝑎,𝑏𝑏𝑏):=⟨Φ(𝑎𝑎𝑎),Φ(𝑏𝑏𝑏)⟩, for any discrete time series
𝑎𝑎𝑎and𝑏𝑏𝑏,as suggested in [ 9]. Here⟨·,·⟩is the inner product on the
linear space of signatures.
Next, to measure the similarity between two discrete time series
𝑎𝑎𝑎and𝑏𝑏𝑏, consider the distance induced by the signature kernel
[3, 9, 14, 19, 28],
𝑑Sig(𝑎𝑎𝑎,𝑏𝑏𝑏)=𝑘(𝑎𝑎𝑎,𝑎𝑎𝑎)−2𝑘(𝑎𝑎𝑎,𝑏𝑏𝑏)+𝑘(𝑏𝑏𝑏,𝑏𝑏𝑏). (11)
Small𝑑Sig(𝑎𝑎𝑎,𝑏𝑏𝑏)implies a higher similarity between patterns in 𝑎𝑎𝑎
and𝑏𝑏𝑏, which suggests that 𝑎𝑎𝑎and𝑏𝑏𝑏come from the same regime
and share the similar seasonality. In practice, one may truncate
the signature to depth 𝑁when computing (11), and we denote the
distance computed from the depth- 𝑁truncated signature by 𝑑Sig,𝑁.
Finally, we adapt the weights in the two-step LASSO according
to the signature kernel, this is called AdaWeight.Sig. It takes five
hyper-parameters: a forecast horizon Δ𝑡, a window size 𝑙∈N,
a signature depth 𝑁∈N+, a temperature parameter 𝛾≥0, and
the distance metric 𝑑Sig,𝑁. More precisely, define 𝑧𝑧𝑧𝜏:=(𝑥𝑥𝑥𝜏,𝑦𝜏+Δ𝑡)
for any𝜏∈N+; for any𝜏1,𝜏2∈N+and𝜏1<𝜏2, denote𝑧𝑧𝑧𝜏1:𝜏2:=
(𝑧𝑧𝑧𝜏1,···,𝑧𝑧𝑧𝜏2)as a slice of the time series {𝑧𝑧𝑧𝑡}𝑡∈N+from time𝜏1
to𝜏2. At each time 𝑡,AdaWeight.Sigtakes the historical samples
{(𝑥𝑥𝑥𝜏,𝑦𝜏+Δ𝑡)}1≤𝜏≤𝑡−Δ𝑡as input, and outputs an adaptive weight
vector
𝑊𝑊𝑊(Δ𝑡):=(𝑤(Δ𝑡)
1,𝑤(Δ𝑡)
2,···,𝑤(Δ𝑡)
𝑡−Δ𝑡)∈R𝑡−Δ𝑡
≥0, (12)
withÍ𝑡−Δ𝑡
𝜏=1𝑤(Δ𝑡)
𝜏=1. That is, AdaWeight.Sigwill assign a higher
weight𝑤(Δ𝑡)
𝜏 to the sample(𝑥𝑥𝑥𝜏,𝑦𝜏+Δ𝑡)if the seasonality pattern
near time𝜏is more similar to the current seasonality pattern embed-
ded in the sample(𝑥𝑥𝑥𝑡−Δ𝑡,𝑦𝑡). Thus, when a new data sample arrives,
the weight vector 𝑊𝑊𝑊(Δ𝑡)will be recomputed by the AdaWeight.Sig
module, to adapt to changes in the recent data samples.
Adaptive two-step LASSO via signature kernel. Measuring simi-
larity via signature kernel leads to a novel LASSO-based approach,
in which we propose to adapt weights according to the similari-
ties between time series data to capture seasonality and regime
switching embedded in the data. In the case of forecasting models
with signature transforms, comparing similarities of data trans-
lates to identifying “signature feature maps”. This is the statistical
equivalence of identifying different distributions for a given set
of data, albeit in the linearized features space from the signature
transform. Algorithm 2 summarizes this approach of adaptive two-
step LASSO via signature kernel, by integrating AdaWeight.Sig
outlined in Algorithm 1 with the LASSO approach.Algorithm 1 Adaptive Weight via Signature Kernel
(AdaWeight .Sig)
1:Input: forecast horizon Δ𝑡, window size 𝑙, signature depth 𝑁,
temperature parameter 𝛾, kernel-based distance metric 𝑑Sig,𝑁
(11), data set 𝐷={𝑧𝑧𝑧𝜏=(𝑥𝑥𝑥𝜏,𝑦𝜏+Δ𝑡)}1≤𝜏≤𝑡−Δ𝑡.
2:for𝜏=1,2,···,𝑡−Δ𝑡do
3: Compute the distance 𝛿𝜏between the truncated depth- 𝑁
signatures of 𝑧𝑧𝑧𝜏−𝑙:𝜏and𝑧𝑧𝑧𝑡−Δ𝑡−𝑙:𝑡−Δ𝑡:
𝛿𝜏:=𝑑Sig,𝑁(𝑧𝑧𝑧𝜏−𝑙:𝜏,𝑧𝑧𝑧𝑡−Δ𝑡−𝑙:𝑡−Δ𝑡). (13)
4:end for
5:for𝜏=1,2,···,𝑡−Δ𝑡do
6: Compute the weight 𝑤(Δ𝑡)
𝜏 according to
𝑤(Δ𝑡)
𝜏 :=exp(−𝛾·𝛿𝜏)
Í𝑡−Δ𝑡
𝜏=1exp(−𝛾·𝛿𝜏). (14)
7:end for
8:Output: the weight vector 𝑊𝑊𝑊(Δ𝑡)=(𝑤(Δ𝑡)
1,···,𝑤(Δ𝑡)
𝑡−Δ𝑡).
Algorithm 2 Adaptive two-step LASSO with Signature Kernel
1:Input: regularization parameter 𝜆, window size 𝑙, signature
depth𝑁, temperature parameter 𝛾, kernel-based distance met-
ric𝑑Sig,𝑁, data set𝐷={𝑧𝑧𝑧𝜏=(𝑥𝑥𝑥𝜏,𝑦𝜏+Δ𝑡)}1≤𝜏≤𝑡−Δ𝑡.
2:forΔ𝑡=1,2,···,Δ𝑇do
3: Regime identification: Run AdaWeight.Sig(Algorithm 1)
withΔ𝑡,𝑙,𝑁,𝛾,𝑑Sig,𝑁, and𝐷. Output𝑊𝑊𝑊(Δ𝑡)in (12).
4: 2-step LASSO: Apply the 2-step LASSO method on the data
set𝐷, with adaptive weight 𝑊𝑊𝑊(Δ𝑡)and regularization param-
eter𝜆:
b𝜽𝜆
LASSO ,Δ𝑡∈arg min
𝜽∈R𝑑(𝑡−Δ𝑡∑︁
𝜏=1𝑤(Δ𝑡)
𝜏·
𝑦𝜏+Δ𝑡
−h
𝒙𝜏,Sig𝑁(𝑦𝜏−𝑙:𝜏)i
·𝜽2
+𝜆∥𝜽∥1)
.(15)
𝜽𝜆
LASSO ,Δ𝑡∈ arg min
supp[𝜽]=supph
b𝜽𝜆
LASSO ,Δ𝑡i(𝑡−Δ𝑡∑︁
𝜏=1𝑤(Δ𝑡)
𝜏·

𝑦𝜏+Δ𝑡−h
𝒙𝜏,Sig𝑁(𝑦𝜏−𝑙:𝜏)i
·𝜽2)
. (16)
5: Given𝑥𝑥𝑥𝑡,𝑦𝑡, make prediction on 𝑦𝑡+Δ𝑡:
b𝑦𝑡+Δ𝑡=h
𝒙𝑡,Sig𝑁(𝑦𝑡−𝑙:𝑡)i
·𝜽𝜆
LASSO ,Δ𝑡.
6:end for
7:Output: the forecast(b𝑦𝑡+1,···,b𝑦𝑡+Δ𝑇).
4 Real-time performance
Our signature-based adaptive two-step LASSO algorithm has been
implemented for the trucking operations in Amazon since No-
vember 2022. Performance analysis shows that our forecast model
presents superior performance than commercially available fore-
cast models, with prediction accuracy improvement by more than
fivefold, and has an estimated annualized savings of $50 million.
5002Transportation Marketplace Rate Forecast Using Signature Transform KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1: Comparing national-level 3-month-ahead industry predictions with our model predictions
Month
in 2021Actual Industrial
prediction% Error
Ind. predictionOur prediction % Error
Our prediction% Accuracy im-
provement
Apr $2.44 $1.89 23% $2.39 2% 1050 %
May $2.51 $1.82 27% $2.45 2% 1250%
Jun $2.53 N/A N/A $2.48 2% N/A
Jul $2.57 $2.18 15% $2.53 2% 650%
Aug $2.61 $2.21 15% $2.58 1% 1400%
Sep $2.71 $2.23 18% $2.70 1% 1700%
Oct $2.72 $2.36 13% $2.69 1% 1200%
Nov $2.72 $2.38 13% $2.71 1% 1200%
Table 2: 12-week ahead model predictions across different regions.
# of week
ahead
predictionN.A. A B C D E
1 1.06% 1.88% 1.53% 1.38% 1.26% 1.53%
2 1.53% 1.93% 2.76% 1.92% 2.08% 2.06%
3 1.55% 2.95% 2.18% 1.19% 2.21% 2.07%
4 1.61% 3.83% 1.87% 1.50% 2.67% 2.34%
5 1.33% 2.57% 2.73% 1.05% 2.99% 1.32%
6 1.44% 2.86% 2.96% 1.85% 2.78% 1.20%
7 1.64% 2.33% 5.69% 1.73% 2.77% 2.51%
8 1.86% 1.71% 5.25% 3.64% 2.55% 2.64%
9 1.88% 2.99% 4.86% 2.55% 3.69% 2.15%
10 2.30% 3.80% 3.31% 4.10% 2.13% 2.53%
11 2.60% 4.71% 5.54% 4.33% 2.06% 2.85%
12 2.38% 5.65% 5.25% 4.76% 2.74% 5.33%
Table 3: Comparison between predictions with and without adaptive signature kernel
Week Actual Prediction
without
signature
kernel% Error with-
out signature
kernelPrediction
with signa-
ture kernel% Error with
signature
kernel
10/31/21 3.37 3.30 2.31% 3.32 1.53%
11/7/21 3.45 3.27 5.19% 3.34 3.22%
11/14/21 3.46 3.25 5.88% 3.38 2.15%
11/21/21 3.52 3.22 8.65% 3.37 4.25%
11/28/21 3.48 3.16 9.28% 3.35 3.66%
12/5/21 3.49 3.15 9.85% 3.31 5.15%
Below we will present the real-time performance using data
from April 2021 to July 2022. While this timeframe precedes the
model’s actual implementation due to confidentiality restrictions,
it represents a particularly challenging period marked by both
the COVID-19 pandemic and the Ukraine conflict, and allows us to
demonstrate the model’s effectiveness in volatile market conditions.
We will showcase the national-level prediction in North America
(N.A.) along with five representative regions within North America,designated A, B, C, D, and E to protect proprietary information.
We apply the relative error to measure model performances where
relative error B|actual rate−forecast rate |
|actual rate|.
Predictions comparison between industry models and our model.
We compare the performance of our model at the national-level
predictions with the standard industry predictions for the April
2021 - November 2021 time period. Both our model predictions and
5003KDD ’24, August 25–29, 2024, Barcelona, Spain Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, and Xinyu Li
Table 4: Our model predictions posted on June 25 2022
Region Week Actual Prediction % Error
N.A.7/3/2022 1.95 1.93 1.34%
7/10/2022 1.95 1.93 1.12%
7/17/2022 1.94 1.95 0.37%
7/24/2022 1.93 1.96 1.25%
7/31/2022 1.94 1.95 0.90%
A7/3/2022 1.8 1.78 0.68%
7/10/2022 1.8 1.76 2.06%
7/17/2022 1.76 1.76 0.04%
7/24/2022 1.72 1.74 1.46%
7/31/2022 1.66 1.73 4.03%
B7/3/2022 1.26 1.26 0.39%
7/10/2022 1.21 1.27 4.68%
7/17/2022 1.21 1.25 3.44%
7/24/2022 1.21 1.25 3.28%
7/31/2022 1.24 1.25 1.25%
C7/3/2022 1.1 1.1 0.40%
7/10/2022 1.08 1.09 0.70%
7/17/2022 1.02 1.08 5.32%
7/24/2022 1.02 1.04 2.01%
7/31/2022 1.02 1.01 0.60%
D7/3/2022 1.92 1.95 1.86%
7/10/2022 1.94 1.96 0.99%
7/17/2022 1.98 1.98 0.28%
7/24/2022 2.01 2.03 1.06%
7/31/2022 2.01 2.07 2.86%
E7/3/2022 1.79 1.76 1.70%
7/10/2022 1.76 1.75 0.75%
7/17/2022 1.76 1.74 0.94%
7/24/2022 1.76 1.73 1.89%
7/31/2022 1.75 1.73 1.01%
industry predictions are made three months (twelve weeks) ahead
of time, with monthly predictions obtained by aggregating weekly
predictions. The detailed results are listed in Table 1. In particular,
our predictions (with a relative error of around 2%) are far superior
to standard industry predictions (with a relative error of around
20%). The prediction accuracy is improved by more than fivefold.
Relative prediction errors up to twelve weeks. Table 2 reports the
relative prediction error of our model for the national level and
five regional predictions (A, B, C, D, and E), up to a twelve-week
horizon. The prediction error moderately increases from around
1%for the one-week prediction to approximately 5%for the twelve-
week prediction, remaining significantly lower than the industry
standard of 15%.
Necessity of adaptive signature kernel. To demonstrate the neces-
sity of the adaptive signature kernel, the key and novel component
of our model, we compare the predictions from Algorithm 2 with
and without the signature kernel in (9). The results are reported in
Table 3. The predictions presented here are for one representativeregion in North America on October 24, 2021. Evidently from Table
3, the errors without the signature kernel are larger ( >8%) even for
short-term predictions. In contrast, the signature kernel method
captures better the seasonality, and obtains a smaller relative error
(<5%) for short-term predictions. This table shows the effectiveness
of incorporating the signature kernel in the forecast model.
Short-term prediction error. Table 4 reports the relative prediction
error of our model for the national level and five regional predictions
(A, B, C, D, and E), for a five-week horizon, with model predictions
made on June 25, 2022. Most of the prediction errors are shown to
be less than 2%.
5 Conclusion
This work presents a novel, highly accurate signature-based adap-
tive two-step LASSO forecasting model for transportation market-
place rates. Deployed at Amazon since November 2022, it delivers
more than fivefold forecast accuracy improvements compared to
5004Transportation Marketplace Rate Forecast Using Signature Transform KDD ’24, August 25–29, 2024, Barcelona, Spain
industry models even amidst major market disruptions, with sig-
nificant cost savings.
References
[1]Imanol Perez Arribas. 2018. Derivatives pricing using signature payoffs. arXiv
preprint arXiv:1809.09466 (2018).
[2]Alexandre Belloni and Victor Chernozhukov. 2013. Least squares after model
selection in high-dimensional sparse models. Bernoulli 19, 2 (2013), 521–547.
[3]Alain Berlinet and Christine Thomas-Agnan. 2011. Reproducing Kernel Hilbert
Spaces in Probability and Statistics. Springer Science & Business Media.
[4]Patric Bonnier, Patrick Kidger, Imanol Perez Arribas, Cristopher Salvi, and Terry
Lyons. 2019. Deep signature transforms. In Proceedings of the 33rd International
Conference on Neural Information Processing Systems. 3105–3115.
[5]Kuo-Tsai Chen. 1954. Iterated Integrals and Exponential Homomorphism. Pro-
ceedings of the London Mathematical Society s3-4, 1 (1954), 502–512.
[6]Kuo-Tsai Chen. 1957. Integration of Paths, Geometric Invariants and a General-
ized Baker-Hausdorff Formula. Annals of Mathematics 65, 1 (1957), 163–178.
[7]Didier Chételat, Johannes Lederer, and Joseph Salmon. 2017. Optimal two-step
prediction in regression. Electronic Journal of Statistics 11, 1 (2017), 2519–2546.
[8]Ilya Chevyrev and Terry Lyons. 2016. Characteristic functions of measures on
geometric rough paths. Annals of Probability 44, 6 (2016), 4049–4082.
[9]Ilya Chevyrev and Harald Oberhauser. 2022. Signature moments to characterize
laws of stochastic processes. Journal of Machine Learning Research 23, 176 (2022),
1–42.
[10] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
arXiv preprint arXiv:1412.3555 (2014).
[11] Peter K Friz and Martin Hairer. 2020. A course on rough paths. Springer.
[12] Peter K Friz and Nicolas B Victoir. 2010. Multidimensional stochastic processes as
rough paths: theory and applications. Vol. 120. Cambridge University Press.
[13] Everette S Gardner Jr. 2006. Exponential smoothing: The state of the art—Part II.
International journal of forecasting 22, 4 (2006), 637–666.
[14] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and
Alexander Smola. 2012. A kernel two-sample test. Journal of Machine Learning
Research 13, 1 (2012), 723–773.
[15] Ben Hambly and Terry Lyons. 2010. Uniqueness for the signature of a path of
bounded variation and the reduced path group. Annals of Mathematics (2010),
109–167.
[16] James D Hamilton. 1989. A new approach to the economic analysis of nonstation-
ary time series and the business cycle. Econometrica: Journal of the econometric
society (1989), 357–384.
[17] Jasdeep Kalsi, Terry Lyons, and Imanol Perez Arribas. 2020. Optimal execution
with rough path signatures. SIAM Journal on Financial Mathematics 11, 2 (2020),
470–493.
[18] Patrick Kidger and Terry Lyons. 2020. Signatory: differentiable computations of
the signature and logsignature transforms, on both CPU and GPU. In InternationalConference on Learning Representations.
[19] Franz J Király and Harald Oberhauser. 2019. Kernels for sequentially ordered
data. Journal of Machine Learning Research 20, 31 (2019), 1–45.
[20] S Vasantha Kumar and Lelitha Vanajakshi. 2015. Short-term traffic flow predic-
tion using seasonal ARIMA model with limited input data. European Transport
Research Review 7, 3 (2015), 1–9.
[21] Chenyang Li, Xin Zhang, and Lianwen Jin. 2017. LPSNet: a novel log path
signature feature based hand gesture recognition framework. In Proceedings of
the IEEE International Conference on Computer Vision Workshops. 631–639.
[22] Terry Lyons, Sina Nejad, and Imanol Perez Arribas. 2019. Numerical method
for model-free pricing of exotic derivatives in discrete time using rough path
signatures. Applied Mathematical Finance 26, 6 (2019), 583–597.
[23] Terry Lyons, Hao Ni, and Harald Oberhauser. 2014. A feature set for streams and
an application to high-frequency financial tick data. In Proceedings of the 2014
International Conference on Big Data Science and Computing. 1–8.
[24] Terry J Lyons, Michael Caruana, and Thierry Lévy. 2007. Differential equations
driven by rough paths. Springer.
[25] James Morrill, Adeline Fermanian, Patrick Kidger, and Terry Lyons. 2020. A
generalised signature method for multivariate time series feature extraction.
arXiv preprint arXiv:2006.00873 (2020).
[26] Robert H Shumway, David S Stoffer, Robert H Shumway, and David S Stoffer.
2017. ARIMA models. Time series analysis and its applications: with R examples
(2017), 75–163.
[27] Carl-Johann Simon-Gabriel and Bernhard Schölkopf. 2018. Kernel distribution
embeddings: Universal kernels, characteristic kernels and kernel metrics on
distributions. Journal of Machine Learning Research 19, 1 (2018), 1708–1736.
[28] Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf,
and Gert RG Lanckriet. 2010. Hilbert space embeddings and metrics on probability
measures. The Journal of Machine Learning Research 11 (2010), 1517–1561.
[29] James W Taylor. 2004. Smooth transition exponential smoothing. Journal of
Forecasting 23, 6 (2004), 385–404.
[30] Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal
of the Royal Statistical Society: Series B (Methodological) 58, 1 (1996), 267–288.
[31] Zecheng Xie, Zenghui Sun, Lianwen Jin, Hao Ni, and Terry Lyons. 2017. Learning
spatial-semantic context with fully convolutional recurrent network for online
handwritten chinese text recognition. IEEE transactions on pattern analysis and
machine intelligence 40, 8 (2017), 1903–1917.
[32] Weixin Yang, Lianwen Jin, and Manfei Liu. 2015. Chinese character-level writer
identification using path signature feature, DropStroke and deep CNN. In 2015
13th International Conference on Document Analysis and Recognition (ICDAR).
IEEE, 546–550.
[33] Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. 2019. A review
of recurrent neural networks: LSTM cells and network architectures. Neural
computation 31, 7 (2019), 1235–1270.
[34] Peng Zhao and Bin Yu. 2006. On model selection consistency of Lasso. The
Journal of Machine Learning Research 7 (2006), 2541–2563.
[35] Hui Zou. 2006. The adaptive lasso and its oracle properties. Journal of the
American statistical association 101, 476 (2006), 1418–1429.
5005