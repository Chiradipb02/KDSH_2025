An Energy-centric Framework for Category-free
Out-of-distribution Node Detection in Graphs
Zheng Gong
Thrust of Artificial Intelligence
Hong Kong University of Science and Technology
(Guangzhou)
Guangzhou, Guangdong, China
zgong768@connect.hkust-gz.edu.cnYing Sun∗
Thrust of Artificial Intelligence
Hong Kong University of Science and Technology
(Guangzhou)
Guangzhou, Guangdong, China
yings@hkust-gz.edu.cn
ABSTRACT
Graph neural networks have garnered notable attention for effec-
tively processing graph-structured data. Prevalent models prioritize
improving in-distribution (IND) data performance, frequently over-
looking the risks from potential out-of-distribution (OOD) nodes
during training and inference. In real-world graphs, the automated
network construction can introduce noisy nodes from unknown
distributions. Previous research into OOD node detection, typically
referred to as entropy-based methods, calculates OOD measure-
ments from the prediction entropy alongside category classification
training. However, the nodes in the graph might not be pre-labeled
with specific categories, rendering entropy-based OOD detectors in-
applicable in such category-free situations. To tackle this issue, we
propose an energy-centric density estimation framework for OOD
node detection, referred to as EnergyDef. Within this framework,
we introduce an energy-based GNN to compute node energies that
act as indicators of node density and reveal the OOD uncertainty of
nodes. Importantly, EnergyDef can efficiently identify OOD nodes
with low-resource OOD node annotations, achieved by sampling
hallucinated nodes via Langevin Dynamics and structure estima-
tion, along with training through Contrastive Divergence. Our
comprehensive experiments on real-world datasets substantiate
that our framework markedly surpasses state-of-the-art methods
in terms of detection quality, even under conditions of scarce or
entirely absent OOD node annotations.
CCS CONCEPTS
•Mathematics of computing →Graph algorithms; •Comput-
ing methodologies →Neural networks ;Anomaly detection.
KEYWORDS
Out-of-distribution Node Detection; Energy-based Model; Graph
Neural Network
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08. . .
https://doi.org/10.1145/3637528.3671939ACM Reference Format:
Zheng Gong and Ying Sun. 2024. An Energy-centric Framework for Category-
free Out-of-distribution Node Detection in Graphs. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671939
1 INTRODUCTION
As the expansion of graph-structure data continues to surge, Graph
Neural Networks (GNNs) have drawn increasing interest due to
their remarkable proficiency in capturing complex relationships
inherent in graphs [ 4,12,58]. In real-world scenarios, typified by
openness and unpredictability, it is a common practice to incre-
mentally construct graphs without any guarantee that all nodes
will conform to the same distribution. This is due to the seman-
tic disparity arising between newly integrated nodes and existing
nodes within the graphs, which underscores the crucial need for
out-of-distribution (OOD) node detection, that is, distinguishing
semantic disparity among unlabeled nodes in contrast to those orig-
inating from in-distribution (IND). This task plays a pivotal role
in detecting instances that do not belong to the distribution the
model has been trained on to avoid unreliable predictions when
the model encounters unseen nodes. Furthermore, it has a broad
spectrum of safety-critical applications, such as financial fraud de-
tection [ 51], sensor fault identification [ 11], as well as counterfeit
reviews recognition [38].
The prior investigations into OOD node detection are generally
referred to as entropy-based methods, which derive OOD metrics
from the entropy of predictive categorical distribution alongside
category classification training, identifying nodes with high predic-
tion entropy as OOD nodes, as illustrated in Figure 1. For instance,
OODGAT [ 48] discriminates pseudo-OOD nodes through the pre-
diction entropy and makes the prediction category distribution of
pseudo-OOD nodes close to a uniform distribution. Additionally,
GNNSafe [ 53] advocates the minimization of category classification
loss is equivalent to lowering the proposed OOD scores of IND
nodes, an approach derived from the prediction logits of a GNN-
based classifier. These methods typically assume the availability of
multi-category labels to train a GNN classifier, rendering them im-
practical for application in category-free scenarios. Fundamentally,
the focal point of OOD detection centers on exposing semantic
disparities between IND and OOD data, a phenomenon not always
distinctly characterized within discrete category spaces [ 54]. Since
graph-structured data often stems from open and unpredictable
contexts, nodes within these graphs frequently lack categorical
 
908
KDD ’24, August 25–29, 2024, Barcelona, Spain. Zheng Gong and Ying Sun
Figure 1: Comparison of (top) entropy-based methods that
train a GNN with supervised multi-category labels to detect
OOD nodes via prediction entropy and (bottom) our frame-
work, which estimates the density of nodes to derive OOD
scores in category-free scenarios.
metadata. For example, when predicting the authenticity of re-
views in user-review graphs[ 39], trustworthy reviews serve as the
known IND data while deceptive reviews represent OOD oddities
due to their diverse patterns and scarcity. As these reviews typically
contain unstructured text without categorical metadata, detecting
OOD deceptive content with such diversity and sparsity poses chal-
lenges for entropy-based classifiers. This necessitates developing
alternative detection methodologies that can effectively distinguish
anomalous deceptive reviews from trustworthy reviews.
Considering nodes in graph-structured data possess two crucial
characteristics: node attributes, representing individual properties,
and topology structures, signifying connectivity patterns between
pairs, it becomes a formidable task to extract semantic disparity
from the intertwined attributes and topology of OOD nodes in
the absence of supervised category signals. Moreover, the acquisi-
tion of OOD annotations can be expensive and resource-intensive
due in part to the high costs of expert knowledge and strict data
privacy constraints, exacerbating the difficulties in training an effi-
cient OOD node detector. Furthermore, in practical applications of
OOD node detection, such as identifying financial fraudster [ 51],
evil fraudsters tend to connect with numerous benign entities to
camouflage themselves, creating what is known as heterophilic
links. Commonly-used GNNs, such as Graph Convolution Net-
work (GCN) [ 20], as well as GNNSafe [ 53], the latest OOD node
detector, implicitly rely on the homophilic assumption, where nodes
of the same class tend to form edges. This potentially obscures the
unique characteristics of OOD nodes concerning IND nodes, as the
representation of OOD nodes might aggregate a substantial pro-
portion of IND nodes, leading to the over-smoothing problem [3].
With these considerations in mind, our study explores conduct-
ing density estimation in category-free scenarios, which aims at
capturing the likelihood of nodes and taking into account their
attributes and structures from a generative standpoint. Our ob-
jective is to assign low densities to OOD nodes while assigning
high densities to IND ones. The difference between previous work
and our approach is depicted in Figure 1. Specifically, we pro-
pose an Energy -centric Density estimation framework, abbrevi-
ated as EnergyDef for OOD node detection. EnergyDef builds anon-homophilic energy-based GNN to derive node energies and si-
multaneously alleviate the over-smoothing problem. These energies
serve as indicators of node density, reflecting the OOD uncertainty
of nodes, all without relying on supervised signals from multi-
category information. Remarkably, within EnergyDef framework,
we innovatively propose the sampling of hallucinated nodes us-
ing Langevin Dynamics and structure sampling, combined with
Contrastive Divergence training, thereby enabling the efficient de-
tection of OOD nodes even in situations where OOD annotations
are limited or absent. Extensive experiments on two task settings
of real-world datasets demonstrate the superior performance of
EnergyDef in the absence of OOD annotations, as well as its ef-
fectiveness in utilizing limited OOD annotations. To the best of our
knowledge, Energy Defis the first method to model the density of
nodes for OOD node detection.
In summary, the key contributions of our work are as follows:
•We formalize the problem of detecting out-of-distribution
(OOD) nodes in category-free graph scenarios with scarce
or entirely absent OOD annotations.
•We propose EnergyDef, a novel density estimation frame-
work for identifying OOD nodes in category-free scenarios
from the perspective of energy-based model.
•In EnergyDef, we propose an approach to enable efficient de-
tection of OOD nodes by innovatively sampling hallucinated
nodes using Langevin Dynamics and structure sampling,
combined through Contrastive Divergence training.
•Comprehensive evaluations conducted on real-world datasets
demonstrate EnergyDef ’s superior performance in the com-
plete absence of OOD annotations, as well as its efficacy in
leveraging limited OOD data when available.
2 RELATED WORK
In this section, we first introduce the concepts of GNNs. Next, we
thoroughly review two typical methods for OOD detection: entropy-
based anddensity-based approaches. At last, we summarize recent
advances in OOD detection specifically tailored for graph data.
2.1 Graph Neural Networks
Graph Neural Networks have been extensively applied and shown
promising performance in solving a variety of real-world problems,
such as drug design [ 55], traffic forecasting [ 56] and social influ-
ence estimation [ 37]. Usually, GNNs take as input two sources of
information: node attributes and graph topology. For instance, in
social networks, nodes represent users that have different prop-
erties by node attributes, and graph topology memorizes observ-
able friendships or collaborations between two users. Numerous
prevalent GNNs, e.g., GCN [ 20], execute successive aggregation of
neighborhood nodes to facilitate predictions or classifications. How-
ever, successive iterations can yield excessively homogenous node
embeddings and a erosion of discriminative signals as the layers
intensify, known as the over-smoothing problem [ 3]. In the context
of OOD node detection, this issue can become more pronounced
since OOD nodes may exhibit distinct characteristics compared to
IND nodes. The over-smoothing effect risks obscuring these salient
differences, posing barriers to accurately identifying OOD nodes.
 
909An Energy-centric Framework for Category-free Out-of-distribution Node Detection in Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
2.2 Out-of-distribution Detection
2.2.1 Entropy-based Methods. Initially, Hendrycks and Gimpel
[14] propose using the maximum softmax probability as OOD
score, and Liang et al . [27] further improved its reliability through
temperature scaling. Consider the limitation of neural networks,
which can easily produce arbitrarily high softmax confidence for
inputs far away from training data [ 33], some semi-/supervised
methods [ 15,17,24] incorporate an auxiliary loss to maximize
the prediction entropy for OOD inputs. Apart from softmax con-
fidence, GOAD [ 1] and NeuTral AD [ 36] augment training data
into independent transformation spaces and obtain OOD scores via
the distances between OOD inputs and centers of transformation
spaces. SSD [ 43] incorporates contrastive learning to obtain the
representation of samples and employs Mahalanobis distance as
OOD scores instead.
2.2.2 Density-based Methods. Lee et al . [25] calculate the Maha-
lanobis distance between input and class-conditional Gaussian dis-
tribution derived from training data and category labels, which
corresponds to measuring the log-likelihood of data. Zisselman
and Tamar [61] introduce Residual Flow to estimate the density
of data on non-Gaussian distribution. Some works [ 21,42] notice
that the density learned by flow-based models and autoregressive
models cannot efficiently distinguish IND and OOD inputs, due to
background statistics [ 40], input complexity [ 44], and mismatch
between high-density regions and the typical set [ 31,32]. Moreover,
suffering from the expressiveness of flow-based models and autore-
gressive models, these methods are not applicable to graph-data.
Most recently, energy-based models (EBM), one of the powerful
generative models, have received increasing attention for OOD
detection. Zhai et al . [57] train an EBM with score matching and
choose a threshold to detect OOD inputs in terms of energy. Du and
Mordatch [10] investigate the capabilities of EBMs such as corrupt
image reconstruction and inpainting, as well as a generalization
to OOD input. Grathwohl et al . [13] model the joint distribution
of image data and category labels through an EBM. However, the
density estimation of nodes in graphs remains under-explored due
to the complex inter-dependence of node pairs.
2.2.3 OOD / outlier detection on graphs. While substantial research
efforts have examined OOD detection for Euclidean data modal-
ities such as images and text, methodologies for distinguishing
between IND and OOD node data remain relatively understudied.
The most related works to our paper in OOD node detection are
OODGAT [ 48] and GNNSafe [ 53]. Both of these are entropy-based
OOD detectors that derive the OOD score from the activation space
of a GNN-based classifier. Prior graph-based anomaly/outlier de-
tection methods [ 28], such as DOMINANT [ 7] and SL-GAD [ 60],
identify outlier nodes in an entirely unsupervised manner by cal-
culating various forms of reconstruction error as outlier metrics.
Different from existing methods, we directly model the density of
nodes belonging to the in-distribution from the standpoint of gener-
ative EBMs, a perspective that, to the best of our knowledge, has not
been explored previously. We defer further discussions, including
task settings and a comparison of our proposed methodology with
previous and existing techniques, to Appendix A.
Figure 2: The illustration of zero-shot and few-shot OOD
node detection problems.
3 PROBLEM DEFINITION
Before formulating the OOD node detection task formally, we first
provide some fundamental notations. Let G=(V,E,X)represent
an attribute graph, where Vis the set of nodes, and Eis the set
of edges. X∈R𝑁×𝑑denotes the attribute matrix, where 𝑁=|V|
is the number of nodes and 𝑑is the dimension of node attributes.
We represent the node space by V, andV ⊂ V. Alternatively,
the topology of graph Gcan be represented as binary adjacency
matrixA, whereA𝑖𝑗=1denotes that node 𝑣𝑖connects to node
𝑣𝑗, whileA𝑖𝑗=0otherwise. The node set can be divided into
V=Vin∪V out, whereVinandVoutrepresents the IND node
set and OOD node set, respectively. We assume IND nodes are
sampled from distribution Pin
V, and OOD nodes are sampled from
distribution Pout
V. We formally define OOD node detection task:
Definition 1: OOD node detection. Given a collection of nodes
sampled from Pin
VandPout
V, the objective is to correctly determine the
source distribution, Pin
VorPout
V, for each node.
We examine OOD node detection in graphs under the transduc-
tive learning paradigm, where both IND and OOD nodes coexist
within a single graph — a prevalent scenario across many realistic
applications, such as financial fraud detection [51].
Considering the practical scarcity of OOD annotations across
domains, we focus principally on more challenging detection frame-
works, specifically, zero-shot andfew-shot OOD node detection. We
formulate these paradigms of the task as follows:
Definition 2: Zero-shot OOD node Detection. At training, only
node attributes X, edge setEand IND annotations of a node subset
V′
in⊂V inare available.
Definition 3: Few-shot OOD node Detection. At training, only
node attributes X, edge setE, identities of a subset V′
in⊂V inand a
handful of OOD nodes V′
out⊂V out(|V′
out|≪|V out|)are available.
These two paradigms of OOD node detection are briefly depicted
in Figure 2. Please note that our method is capable of handling
both cases in category-free scenarios, i.e., without requiring node
category metadatas. To the best of our knowledge, we are the first to
propose a unified framework for OOD node detection that can handle
both zero- and few-shot category-free scenarios.
4 NON-HOMOPHILIC ENERGY-BASED GNN
In this section, we introduce an energy-based GNN for inferring the
energy of each node while concurrently alleviating over-smoothing
issue mentioned in subsection 2.1. After that, we illustrate how the
energy can serve as OOD scores.
 
910KDD ’24, August 25–29, 2024, Barcelona, Spain. Zheng Gong and Ying Sun
𝓖
GNN
𝑬𝜽(𝒗)
… …
Revised:
Figure 3: The training of EnergyDef starts with the sampling attributes and structures of hallucinated nodes via Langevin
Dynamics and structure estimation. To accommodate zero-shot and few-shot OOD node detection, original hallucinated nodes’
attributes are initialized using either a uniform distribution or OOD node attributes. The training objective aims at increasing
the energies of the hallucinated nodes and decreasing the energies of IND nodes.
The essence of the energy-based models (EBM) [ 23] is to build
and optimize a function 𝐸𝜃(x):R𝑑→Rthat map a point xof input
space to a single, non-probabilistic scalar, also known as energy.
We adapt the definition to graph domain and architect an energy-
based GNN to calculate node energies, which will function as an
OOD score. Bearing in mind the over-smoothing issue prevalent of
homophily-based GNNs, as discussed in subsection 2.1, we propose
to encapsulate the connectivity 𝜙𝑖𝑗between node 𝑣𝑖and𝑣𝑗, thereby
regulating the influence stemming from non-homophilic neighbors
and further improving the distinguishability of OOD nodes. Specif-
ically, in alignment with established method [ 26], we adopt cosine
similarity based on the attributes of node pairs to depict the value
of connectivity 𝜙𝑖𝑗:
𝜙𝑖𝑗=cos(𝑼x𝑖,𝑼x𝑗)+1
2∈[0,1], (1)
where 𝑼∈R𝑓×𝑑are learnable parameters. Next, we develop a
simple yet potent GNN architecture with 𝐾layers to compute the
energy for each node. We transform node attributes using a linear
layer h(0)
𝑖=W0x𝑖+b0, where h(0)
𝑖,b0∈R𝑓,W0∈R𝑓×𝑑, and𝑓
is the hidden dimension of node representations. Following that,
the representation of node 𝑖in layer𝑘of the GNN is obtained by
gathering neighborhood information and controlling the flow of
messages from non-homophilic neighbors via 𝜙𝑖𝑗:
h(𝑘)
𝑖=𝛾(𝑘)
h(𝑘−1)
𝑖+∑︁
𝑗∈N(𝑖)𝜙𝑖𝑗√︁
(𝑑𝑖+1)(𝑑𝑗+1)h(𝑘−1)
𝑗
,(2)
where𝛾(𝑘)∈R𝑓×𝑓is a learnable transformation, and 𝑑𝑖,𝑑𝑗denotes
the degrees of node 𝑖and𝑗, respectively.
To enhance the capability of 𝜙𝑖𝑗in assimilating pre-existing edge
information and predicting the connectivity of unseen node pairs,
we initialize 𝑼by following the link prediction strategy [ 19], i.e.,
optimizing𝜙𝑖𝑗to 1 for each existing edge 𝑒𝑖𝑗∈Ewhere𝑖,𝑗∈V′
in.
Note that if we only optimize 𝜙𝑖𝑗based on the observed edges E,𝑼will yield trivial solutions which predict 𝜙𝑖𝑗to 1 for all node
pairs. To remedy this issue, we randomly sample one edge 𝑒˜𝑖˜𝑗of
disconnected node pair for each edge 𝑒𝑖𝑗∈E, and optimize 𝜙˜𝑖˜𝑗to
0. After initializing 𝑼,𝜙𝑖𝑗serves a dual purpose. In addition to con-
trolling the message flow in our GNN aggregation, it can estimate
the structure for unseen nodes based on the learned patterns from
the existing nodes:
𝑝(A𝑢𝑣=1|x𝑢,x𝑣)=𝜙𝑢𝑣. (3)
To mitigate the escalating over-smoothing issue with deeper
GNN layers, we introduce an energy function 𝐸𝜃(𝑣𝑖), which com-
putes the energy of node 𝑣𝑖by aggregating node representations
across all GNN layers, a concept inspired by PageRank strategies [ 6]:
𝐸𝜃(𝑣𝑖)=W𝐾 𝐾∑︁
𝑘=0𝛽𝑘h(𝑘)
𝑖!
, (4)
where𝛽𝑘∈Ris a learnable parameter, and W𝐾∈R1×𝑓transforms
the node representations to the energy scalar.
Our expectation is to find a proper OOD decision function 𝑔,
such that a greater energy associated with node 𝑣corresponds to a
higher uncertainty OOD score, thereby making energy a direct and
effective OOD score:
𝑔(𝑣;𝜏)=(
0if𝐸𝜃(𝑣)≤𝜏,
1if𝐸𝜃(𝑣)>𝜏,(5)
where𝜏represents the energy threshold.
However, within the context of zero-shot and few-shot OOD
node detection, the task of training the learnable parameters by
maximizing the energies of OOD nodes becomes intractable due
to the absence of OOD annotations. Hence, we turn to density
estimation as a viable strategy for zero-shot and few-shot OOD
node detection.
 
911An Energy-centric Framework for Category-free Out-of-distribution Node Detection in Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
5ZERO- / FEW-SHOT OOD NODE DETECTION
5.1 Density Estimation
Regarding graph-structured data, the probability density (a.k.a like-
lihood) defined by energy function 𝐸𝜃over each node 𝑣can be
mathematically expressed using the Boltzmann distribution [23]:
𝑝𝜃(𝑣)=exp(−𝐸𝜃(𝑣))
𝑍(𝜃)∝𝑒−𝐸𝜃(𝑣), (6)
where𝑍(𝜃)=∫
exp(−𝐸𝜃(𝑣))𝑑𝑣is the normalizing constant with
respect to𝑣(a.k.a partition function ) and usually intractable. 𝜃are
learnable parameters mentioned before.
Intuitively, the ideal energy function described in Equation (4)
should attribute higher densities to IND nodes and lower densities
to OOD nodes. However, given the absence of OOD annotations,
a straightforward approach is to train the energy-based GNN by
maximizing the likelihood of IND nodes. This can be achieved
through a simple strategy of minimizing the negative loglikelihood
of IND nodes:
LNLL=E𝑣∼Pin
V[−log𝑝𝜃(𝑣)], (7)
where−log𝑝𝜃(𝑣)=𝐸𝜃(𝑣)+log𝑍(𝜃)as derived from Equation (6).
However, for most choices of the energy function 𝐸𝜃, the accurate
estimation of 𝑍(𝜃)as indicated in Equation (6)is not feasible. This
implies that estimating the normalized density 𝑝𝜃(𝑣)becomes an
intractable task. Thus, we resort to alternative methods for train-
ing the energy-based GNN. A noteworthy observation is that the
minimization ofLNLLis equivalent to optimizing the model distri-
bution𝑝𝜃towards the in-distribution Pin
Vthrough the reduction of
the Kullback-Leibler (KL) divergence between Pin
Vand𝑝𝜃:
𝐷KL
Pin
V∥𝑝𝜃
=E𝑣∼Pin
V[−log𝑝𝜃(𝑣)]+Constant, (8)
where Constant =E𝑣∼Pin
V
logPin
V(𝑣). A common technique to
minimize the KL divergence is to optimize it through gradient
descent, a method known as Contrastive Divergence [16, 47]:
∇𝜃𝐷KL(Pin
V∥𝑝𝜃)=E𝑣∼Pin
V[∇𝜃𝐸𝜃(𝑣)]−E˜𝑣∼𝑝𝜃[∇𝜃𝐸𝜃(˜𝑣)].(9)
The detailed derivation is provided in Appendix B. In line with prior
research [ 9], we refer to ˜𝑣drawn from the model distribution 𝑝𝜃as
hallucinated samples and designate the set of hallucinated nodes as
Vhal. Evidently, this gradient descent operation serves to decrease the
energies of IND nodes and increase the energies of hallucinated nodes.
5.2 Low-Resource Hallucinated Node Sampling
Recalling that the traits of nodes ˜𝑣𝑗=(˜x𝑗,˜A𝑗)encompass two
components: attributes and structures, we will introduce the sam-
pling process of these components of hallucinated nodes from the
model distribution 𝑝𝜃, given limited OOD annotations or even in
the absence of OOD annotations.
5.2.1 Gradient-based Attribute Sampling. To address the scarcity
of OOD annotations, we iteratively sample hallucinated node at-
tributes, introducing minor, controlled modifications based on the
energy gradient w.r.t. node attributes. In zero-shot scenarios, these
attributes are initialized using a uniform distribution, while in few-
shot scenarios, we optionally utilize training OOD node attributesfor initialization. This approach yields hallucinated nodes that func-
tion as intermediaries between OOD nodes and IND nodes, thereby
improving their distinguishability.
Specifically, in our approach, we employ stochastic gradient
Langevin dynamics (SGLD) [ 52], a gradient-based Markov Chain
Monte Carlo (MCMC) method for sampling node attributes from
model distribution 𝑝𝜃approximately. At the (𝑡+1)-th iteration,
SGLD updates the current attributes ˜x(𝑡)
𝑗of hallucinated node ˜𝑣𝑗
via the following procedure:
˜x(𝑡+1)
𝑗←˜x(𝑡)
𝑗−𝛼
2∇˜x(𝑡)
𝑗𝐸𝜃
˜𝑣(𝑡)
𝑗
+𝜖,𝜖∼N( 0,𝛼), (10)
where𝛼
2is the step size and 𝜖is the Gaussian noise sampled from
multivariate Gaussian distribution N(0,𝛼). It is known that the
distribution ˜x(𝑡)
𝑗converges to samples from 𝑝𝜃as𝑡→ ∞ and
𝛼
2→0. In practice, we set the number of iteration steps of Langevin
dynamics𝑇as a hyperparameter. As illustrated in Figure 3, the
overall procedure of Langevin dynamics is to reduce the energy
of the sampled nodes iteratively and thereby increase the likeli-
hood of hallucinated nodes sampled from model distribution 𝑝𝜃as
−∇˜x(𝑡)
𝑗𝐸𝜃
˜𝑣(𝑡)
𝑗
=∇˜x(𝑡)
𝑗log𝑝𝜃
˜𝑣(𝑡)
𝑗
. Under zero-shot conditions,
initial hallucinated node attributes are drawn from a uniform dis-
tribution ˜x(0)
𝑗∼U(−𝐼𝑑,𝐼𝑑), where𝐼𝑑is a𝑑-dimension vector of
ones. While in few-shot situations, the initialization of node at-
tributes additionally employs the existing attributes of available
OOD nodes.
5.2.2 Probability-based Structure Sampling. During the sampling
iterations of Langevin dynamics, a crucial question arises: how can
we concurrently obtain the corresponding structures based on the
hallucinated node attributes? A trivial solution would be to optimize
˜A𝑗in the same way as Langevin dynamics. However, due to the
discrete nature of ˜A𝑗, sampling ˜A𝑗from model distribution 𝑝𝜃
usually resort to other techniques, such as ratio matching [ 18].
These techniques, however, require expensive computations and
excessive memory usage, rendering them impractical for large-scale
graphs. To tackle this issue, we propose building the connections
involving hallucinated nodes via the structure estimation module in
Equation 1. Since it is pre-trained on observed edges, the connection
captured by the module is close to data distribution. Specifically,
at the step𝑡, we compute the probability of a connection between
hallucinated node ˜𝑣𝑗and existing node 𝑣𝑖as follows:
𝑝(˜A(𝑡)
𝑖˜𝑗=1)=𝜙𝑖˜𝑗=cos(𝑼x𝑖,𝑼˜x(𝑡)
𝑗)+1
2. (11)
Then we employ k-nearest neighbors (kNN) to select the nodes with
top-k𝜙𝑖˜𝑗values as neighbors of ˜𝑣𝑗. In this context, 𝑘is determined
based on the average degree of nodes in the graph. To reduce the
computational overhead of kNN, the structures of hallucinated
nodes are updated at each time interval ˜𝑡. Besides, the complexity
bottleneck emerges from the quadratic time and space complexity
inherent to kNN. To mitigate this issue, we leverage a brute-force
kNN algorithm implemented in Deep Graph Library (DGL) [1] to
reduce the space complexity to O(𝑁𝑘), thereby expediting kNN
computations on graphics processing units for our datasets.
 
912KDD ’24, August 25–29, 2024, Barcelona, Spain. Zheng Gong and Ying Sun
Algorithm 1: Training procedure of EnergyDef
Input: An attribute graph G=(V,E,X), IND node setV′
in,
optional OOD node set V′
out, energy function 𝐸𝜃(𝑣),
stop gradient operator Ω(·), node attribute replay
bufferB←∅∪ X′
out, structure sampling interval ˜𝑡
Output: Optimized energy function 𝐸𝜃(𝑣)
Train parameter 𝑼of structure estimation module based on
IND nodes𝜙𝑖𝑗=(cos(𝑼x𝑖,𝑼x𝑗)+1)/2∈[0,1].
while not converged do
˜𝑣(0)
𝑗∼B with probability 𝑝andUotherwise.
⊲Generate attributes and structures of hallucinated
nodes from 𝑝𝜃via Langevin dynamics
forsample step 𝑡=1to𝑇do
˜x(𝑡+1)
𝑗←˜x(𝑡)
𝑗−𝛼
2𝜕𝐸𝜃
˜𝑣(𝑡)
𝑗
𝜕˜x(𝑡)
𝑗+𝜖,𝜖∼N( 0,𝛼)
if𝑡%˜𝑡==0then
⊲Generate topology structures ˜A(𝑡+1)
𝑗of
hallucinated nodes
˜A(𝑡+1)
𝑖˜𝑗=1if𝜙𝑖˜𝑗∈Top-K({𝜙𝑖˜𝑗}𝑁
𝑖=1)
˜𝑣𝑗=Ω(˜𝑣(𝑇)
𝑗)
⊲Optimize objective Le+𝜆Lrw.r.t𝜃
Δ𝜃←∇𝜃(Le+𝜆Lr)
⊲Update𝜃based on Δ𝜃via Adam optimizer
B←B∪ ˜x(𝑇)
𝑗
5.2.3 Hallucinated node attribute replaying. As SGLD typically ne-
cessitates multiple steps to sample hallucinated node attributes,
leading to significant computational demands, we integrate Persis-
tent Contrastive Divergence (PCD) [ 49] to streamline the sampling
process. Our approach includes the implementation of a node at-
tribute replay buffer, denoted as B∈R𝑀×𝑑, capable of storing 𝑀
node attributes. Initially, this buffer exclusively stores OOD node
attributes for few-shot scenarios, and none for zero-shot situations.
During each training iteration, we insert the hallucinated node at-
tribute ˜x(𝑇)
𝑗into the bufferB, replacing existing hallucinated node
attributes at random. For subsequent iterations, node attributes are
initialized either by randomly selecting from the buffer with a pre-
determined probability 𝑝, or by defaulting to a uniform distribution
in the absence of such selection.
5.3 Energy-Regularized Learning Objective
As illustrated in Equation (9), the maximization of IND node log-
likelihood necessitates a decrease in the energies of IND nodes and
an increase in the energies of hallucinated nodes. Thus, to ensure
these specified expectations, we devise the loss function of our
energy-based GNN as follows:
Le=1
|V′
in|∑︁
𝑣𝑖∈V′
in𝐸𝜃(𝑣𝑖)−1
|Vhal|∑︁
𝑣𝑗∈Vhal𝐸𝜃(𝑣𝑗). (12)The gradient backpropagated from Lecan update the parame-
ters of energy function 𝐸𝜃, thereby minimizing both the diver-
gence𝐷KL
Pin
V∥𝑝𝜃
and the negative loglikelihood of IND nodes
E(𝑣)∼Pin
V[−log𝑝𝜃(𝑣)]. Please note that the gradient derived from
L𝑒will not be propagated to the energy function used in Langevin
dynamics. To keep the parameters of energy function used in
Langevin dynamics up-to-date, we synchronize the parameters
after each optimization step. Moreover, we apply regularization to
the energy magnitudes for stable training. There is much design
flexibility for the regularization term, and here we regularize the
energy with its square terms [10]:
Lr=1
|V′
in|∑︁
𝑣𝑖∈V′
in𝐸𝜃(𝑣𝑖)2+1
|Vhal|∑︁
𝑣𝑗∈Vhal𝐸𝜃(𝑣𝑗)2.(13)
The final learning objective is L=Le+𝜆Lr, where𝜆is a hyperpa-
rameter controlling the relative importance of L𝑒andL𝑟.
5.4 Training Procedure
Our training procedure first optimizes 𝑼, the parametric constituent
of the structure estimation module. Then, each iteration entails
gradient-based attribute sampling and probability-based structure
sampling to generate hallucinated nodes, alongside stopping the
gradient of fabricated nodes to ensure their construction depends
solely on the energy function 𝐸𝜃(𝑣)parameters. Finally, we update
the parameters 𝜃of the energy function 𝐸𝜃(𝑣)using the learning
objectiveL. As analyzed in Appendix C, the time complexity is
O(𝑇𝐾(|E|+|V|)+𝑇
˜𝑡|V|2), where𝑇denotes the iteration number
of SGLD,𝐾refers to the number of GNN layers in 𝐸𝜃(𝑣),˜𝑡signifies
the time interval for structure sampling, while EandVconstitute
the edge and node sets of graph G. The diagram and algorithm of
overall training procedure are shown in Figure 3 and Algorithm 1.
6 EXPERIMENT
In this section, we conduct empirical experiments to demonstrate
the efficacy of the proposed EnergyDef framework. We seek to ad-
dress the following five research questions: RQ1: How effective is
EnergyDef for detecting OOD nodes given completely unavailable
OOD annotations? RQ2: How effective is EnergyDef for leveraging
limited OOD annotations in few-shot OOD node detection? RQ3:
How does our method compare against entropy-based OOD detec-
tion approaches? RQ4: How do the structure estimation module
and persistent contrastive divergence influence the efficacy of En-
ergyDef ?RQ5: How do the parametric configurations within the
Langevin dynamics impact the performance of EnergyDef ?
Table 1: The statistics of the real-world OOD node detection
datasets. “✗” denotes no available multi-category labels.
Dataset Squirrel WikiCS YelpChi Amazon Reddit
# Nodes 5,201 11,701 45,954 11,944 10,984
# Features 2,089 300 32 25 64
Avg. Degree 41.7 36.9 175.2 800.2 15.3
OOD node (%) 20.0 29.5 14.5 9.5 3.3
# Category 5 10 ✗ ✗ ✗
 
913An Energy-centric Framework for Category-free Out-of-distribution Node Detection in Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
Table 2: Zero-shot OOD node detection performance on the real-world datasets. “OOM" denotes the detector is out of memory
on the dataset, and “-" denotes the detector is not applicable to the dataset. The detectors marked by♣are trained only on node
attributes. We use the same GNN backbone of EnergyDef -h with detectors labeled by♠. Entropy-based detectors marked by♦
(resp.♦) utilize additional multi-category information (resp. pseudo category labels provided by K-means on node attributes).
MethodDataset Squirrel WikiCS YelpChi Amazon Reddit
AUROC↑AUPR↑FPR@95↓
LOF-KNN♣[2] 51.85 29.87 95.21 44.06 37.48 96.28 56.39 25.98 92.57 45.25 14.26 95.10 57.88 6.95 93.24
MLPAE♣43.15 24.81 97.98 70.99 63.74 77.76 51.90 24.53 92.42 74.54 51.59 57.93 52.10 5.80 94.43
GCNAE [19] 37.87 22.64 99.08 57.95 46.32 92.97 44.20 19.22 97.06 45.07 12.38 98.54 51.78 6.14 93.75
GAAN [5] 38.01 22.57 98.99 58.15 46.60 93.37 44.29 19.30 96.91 53.26 6.63 98.05 52.21 5.96 94.06
DOMINANT [7] 41.78 24.73 95.53 42.55 35.43 97.22 52.77 24.90 92.86 78.08 35.96 76.05 55.89 6.03 96.48
ANOMALOUS [35] 51.04 29.09 96.39 67.99 54.51 92.74 OOM OOM OOM 65.12 25.15 85.34 55.18 6.40 94.10
SL-GAD [60] 48.29 27.62 97.19 51.87 44.83 95.26 56.11 26.49 93.27 82.63 56.27 51.36 51.63 6.02 94.27
GOAD♠[1] 62.32 37.51 92.28 50.65 37.22 99.78 58.03 28.51 89.84 72.92 45.53 66.36 52.89 5.36 94.26
NeuTral AD♠[36] 52.51 30.04 97.16 53.58 43.49 94.30 55.81 25.14 94.23 70.01 24.36 92.19 55.70 6.45 94.59
GKDE♦[59] 56.15 33.41 94.96 70.47 61.18 82.71 - - - - - - - - -
OODGAT♦[48] 58.84 35.13 93.31 74.13 62.47 84.48 - - - - - - - - -
GNNSafe♠♦[53] 56.38 32.22 95.17 73.35 66.47 76.24 - - - - - - - - -
OODGAT♦57.78 34.66 92.61 52.76 44.71 90.02 55.97 23.07 97.93 82.54 54.94 52.10 54.62 6.05 93.85
GNNSafe♠♦49.52 26.63 97.60 64.15 50.85 92.63 55.26 26.68 91.40 68.51 25.39 84.31 49.63 5.36 95.98
EnergyDef -p 63.76 37.03 92.46 69.85 59.71 83.47 60.62 28.83 91.04 85.69 72.63 35.24 61.75 7.06 93.47
EnergyDef -h 62.55 36.89 92.70 71.90 62.42 80.40 59.25 27.99 91.22 85.82 72.87 34.66 62.27 7.44 92.62
EnergyDef 64.15 37.40 91.77 70.22 60.10 83.17 62.04 29.71 90.62 86.57 74.50 32.43 63.32 8.34 89.34
6.1 Experimental Setup
6.1.1 Datasets. We conduct extensive experiments to evaluate En-
ergyDef on five real-world OOD node detection datasets:
•Squirrel [41] is a Wikipedia page-page network where nodes
represent articles from the English Wikipedia, and edges re-
flect mutual links between them. The nodes are classified
into 5 classes based on Geom-GCN [ 34], and the dataset is
characterized by a high degree of heterophily.
•WikiCS [30] is composed of nodes that represent Computer
Science articles, with edges based on hyperlinks. The nodes
are further categorized into 10 classes, each symbolizing a
unique branch of the Computer Science field.
•YelpChi [38] collects hotel and restaurant reviews from
Yelp, classifying legitimate (resp. spam) reviews as IND (resp.
OOD) nodes.
•Amazon [29] includes the product reviews of the Musical
Instrument category on Amazon.com, in which IND (resp.
OOD) nodes are benign (resp. fraud) users.
•Reddit [22] comprises user posts collected from various
subreddits over a span of one month, where IND (resp. OOD)
nodes are normal (resp. banned) users.
The statistics of these datasets are shown in Table 1. Squirrel and
WikiCS are both loaded using DGL package [ 50]. As for Squirrel,
we select class [1] as OOD class, and the remaining [0, 2, 3, 4] as
IND classes. While for WikiCS, we select [4, 5] as OOD classes, and
the remaining eight classes as OOD classes. The relationships of
node pairs in YelpChi and Amazon are processed following [ 8], andReddit is loaded using PyGod package [ 28]. The performance of our
methods and all baselines is conducted in homogeneous settings,
i.e., ignoring the category of edges.
6.1.2 Baselines. As for zero-shot OOD node detection, we verify the
effectiveness of EnergyDef compared with four types of models:
•traditional outlier detectors including local outlier factor [ 2] with
𝑘-nearest neighbors (LOF-KNN) and MLP autoencoder (MLPAE);
•graph outlier detectors such as GCN autoencoder [ 19], GAAN [ 5],
DOMINANT [7] and ANOMALOUS [35] and SL-GAD [60];
•transformation-based detectors GOAD [1] and NeuTral AD [36];
•entropy-based detectors including GKDE [ 59], OODGAT [ 48] and
GNNSafe [53].
As for few-shot OOD node detection, we compare with cross
entropy (CE), GNN-based prototype neural network (ProNet) [ 45],
Mahalanobis distance [ 25], Outlier Exposure (OE) [ 15] and Self-
Supervised Outlier Detection (SSD) [ 43]. The descriptions of these
baselines are provided in Appendix D.
To better understand how structure estimation and persistent
contrastive divergence help EnergyDef detect OOD nodes, we com-
pare EnergyDef with its two variants, i.e., EnergyDef -h and En-
ergyDef -p. Concretely, EnergyDef -h sets𝜙𝑖𝑗=1in Equation (2)
and randomly samples edges for hallucinated nodes. EnergyDef -p
samples attributes of hallucinated nodes from the uniform distribu-
tion as initialization without replaying sampled attributes.
6.1.3 Metrics. In alignment with prior works in OOD node de-
tection [ 48,53], we assess the detection performance using three
 
914KDD ’24, August 25–29, 2024, Barcelona, Spain. Zheng Gong and Ying Sun
widely-accepted, threshold-independent metrics. (1) AUROC mea-
sures the area under the curves, representing the trade-offs between
the true positive rate and false positive rate across different thresh-
old values. (2) AUPR measures the area under the curves, repre-
senting the trade-offs between the precision rate and the recall rate
of OOD nodes, across different threshold values. (3) FPR95 can be
interpreted as the probability that an OOD sample is misclassified
as IND when the true positive rate is as high as 95%.
6.1.4 Settings. For Squirrel and WikiCS, we randomly select one
and two classes as OOD classes, respectively. For the fraudster iden-
tification datasets, we classify the abundant benign entities as IND
nodes and fraudsters as OOD nodes. We allocate 40% nodes of IND
class for training in the zero-shot scenarios, and select additional
𝑘OOD nodes in the few-shot scenario. The remaining nodes are
divided in a 1:2 ratio for validation and testing via stratified random
sampling based on IND/OOD labels. We optimize all models with
2-layer GNN on each dataset by selecting learning rate from {0.01,
0.005, 0.001} with Adam optimizer over 200 epochs, hidden states
from {32, 64, 128} and dropout rate from {0, 0.1, 0.2}, and save the
model that yields the best AUC on validation. As for EnergyDef,
we search the step size from {0.1, 0.5, 1.0} and 𝑇from {10, 20, 40}
in Equation (10). We report the average value of five independent
runs for each dataset. The relative importance controller 𝜆, which
balances the energy loss in Equation (12)and regularization in Equa-
tion (13), is set to 0.01 in all experiments. The anomaly detection
baselines are trained entirely on the basis of graph structures and
node attributes, without the requirement for IND annotations. We
adapt these models to the specifications of our OOD node detection
tasks by minimizing the corresponding loss items solely on the IND
nodes, where applicable. In terms of the experimental environment,
both EnergyDef and EnergyDef -h are implemented using Pytorch.
All experiments were conducted on a Ubuntu 20.04.5 LTS Linux
server, equipped with a 2.20GHz Intel Xeon Gold 5220R CPU, an
RTX 3090 GPU with 24GB memory, and the source code is made
publicly available on GitHub1.
6.2 Zero-shot Detection Performance (RQ1)
The zero-shot OOD node detection results are enumerated in Ta-
ble 2. Overall, EnergyDef exhibits superior detection quality across
all datasets, with the exception of WikiCS where the highest AU-
ROC is attained by OODGAT♦, while GNNSafe♠♦achieves the best
AUPR and FPR@95 scores. Notably, both these top-performing tech-
niques for WikiCS employ additional category label information
coupled with entropy-based detection. In the Amazon and Reddit
benchmarks, EnergyDef considerably exceeds the baseline perfor-
mance by a substantial margin. For example, in Amazon, absolute
improvements of 4.03% and 19.56% are exhibited in AUROC and
AUPR versus OODGAT♦respectively, affirming the superiority of
EnergyDef. Furthermore, as Squirrel and YelpChi are heterophilic
datasets [ 34,38], EnergyDef surpasses entropy-based GNNSafe and
homophily-based DOMINANT markedly, thus indicating its robust
applicability under heterophily. We additionally observe deficient
GCNAE outcomes on most benchmarks, likely since homophilic
1https://github.com/KellyGong/EnergyDef
(a) YelpChi (b) AmazonTest AUC
|𝑉𝑜𝑢𝑡′| |𝑉𝑜𝑢𝑡′|Figure 4: Few-shot detection AUC in YelpChi and Amazon of
different methods under different numbers of OOD training
nodes|V′
out|. All baselines use the same GNN backbone with
EnergyDef -h.
aggregations can diminish the discriminative OOD node character-
istics in the vicinity of numerous in-distribution nodes, underlining
the necessity of non-homophilic architectures.
6.3 Few-shot Detection Performance (RQ2)
To enable deployment within realistic resource-intensive applica-
tions such as financial fraud detection, we analyze the few-shot per-
formance of EnergyDef along with baseline methods given scarce
OOD node annotations. Figure 4 presents model outcomes on the
YelpChi and Amazon benchmarks under varying quantities of train-
ing OOD nodes|V′
out|. Notably, we apply the same GNN backbone
of EnergyDef-h across baselines, with SSD being the sole approach
capable of zero-shot detection. As observed, vanilla entropy loss
performs poorly owed to the severe imbalance between OOD and
IND instances during training, affirming the necessity of ratio-
nal hallucinated node generation to improve detection robustness.
In contrast, EnergyDef consistently excels across different |V′
out|
scales, demonstrating effective integration of annotations through
hallucinated node sampling. For example, in YelpChi, EnergyDef
and EnergyDef-h attain absolute average AUC improvements of
4.66% and 3.28% respectively versus the second-best SSD baseline
across the few-shot configurations.
6.4 Density-based versus Entropy-based (RQ3)
In Table 2, we compare EnergyDef against entropy-based detectors,
i.e., GKDE♦, OODGAT♦and GNNSafe♠♦, which employ additional
multi-category labels to train a classifier for detecting OOD nodes.
Given their reliance on the homophily assumption, their perfor-
mance is suboptimal in the heterophilic Squirrel dataset [ 34]. In
contrast, EnergyDef achieves superior performance in Squirrel due
to its non-homophilic energy aggregation. To analyze the perfor-
mance of entropy-based detectors in the absence of category labels,
variants of these models denoted OODGAT♦and GNNSafe♠♦are as-
sessed, leveraging k-means clustering outcomes on node attributes
as surrogate labels. Upon comparison, the advantages of Energy-
Def are apparent, exceeding OODGAT♦and GNNSafe♠♦across
all datasets with absolute average AUC improvements of 8.53%
and 11.85% respectively. This affirms the efficacy of EnergyDef in
leveraging the intrinsic attributes and topology of nodes.
 
915An Energy-centric Framework for Category-free Out-of-distribution Node Detection in Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
Edge Connectivity
(a) (b) 
Edge Type
Figure 5: (a) Edge connectivity 𝜙𝑖𝑗distribution in YelpChi
across three different connections of node pairs. (b) AUC re-
sult in YelpChi with different hyperparameter combinations
in Langevin dynamics.
6.5 Ablation Study (RQ4)
In this subsection, we attempt to understand how the structure esti-
mation module and persistent contrastive divergence contribute to
non-homophilic aggregation and hallucinated node sampling. As
for the structure estimation module, we derive EnergyDef -h, a vari-
ant of EnergyDef, which sets 𝜙𝑖𝑗=1in Equation (2)and randomly
samples edges for hallucinated nodes. As documented in Table 2
and Figure 4, EnergyDef consistently outperforms EnergyDef-h
across most scenarios, affirming structure estimation as a pivotal
density estimation component. Furthermore, in Figure 5(a), we vi-
sualize𝜙𝑖𝑗in YelpChi across connections between IND and OOD
nodes versus within-class links. The distribution of 𝜙𝑖𝑗values re-
veals markedly lower scores for inter-links between OOD and IND
nodes compared to intra-links. This demonstrates EnergyDef ’s ca-
pacity to discern intricate inter-class relationships, thus mitigating
over-smoothing issues. As for the persistent contrastive divergence,
we derive EnergyDef -p, which samples attributes of hallucinated
nodes from the uniform distribution as initialization without replay-
ing sampled attributes. As shown in Table 2, EnergyDef -p achieves
inferior performance compared to EnergyDef, which demonstrates
the effectiveness of persistent contrastive divergence in sampling
high-quality hallucinated nodes.6.6 Sensitivity Analysis (RQ5)
During the sampling procedure of hallucinated nodes’ attributes,
we find that the step size 𝛼and the number of iteration steps 𝑇
in Equation (10)play pivotal roles in determining the final perfor-
mance. As shown in Figure 5(b), we assess combinations of 𝛼and𝑇
values on the YelpChi benchmark, with test AUC peak attained at
𝛼=0.5and𝑇=20. Analogous to the learning rate and optimization
steps in stochastic gradient descent, appropriate 𝛼and𝑇aid in sam-
pling high-quality hallucinated nodes, thereby improving detection
efficacy. As documented in Table 2 and Figure 4, the chosen scale
of these hyperparameters in our experimental setup proves to be
well-suited for the datasets under evaluation.
7 CONCLUSION AND FUTURE WORK
In this paper, we introduced EnergyDef, a generative energy-based
method for zero-shot and few-shot OOD node detections. We pro-
posed a non-homophilic energy-based GNN for handling graph-
structured data and deriving node energies as an OOD score. To
equip the energy-based GNN with good discrimination between
IND and OOD nodes with the low-resource OOD node annota-
tions, we utilized Langevin dynamics and structure sampling to
sample hallucinated nodes and maximized the loglikelihood of IND
nodes by Contrastive Divergence. Our experiments show that our
method outperforms the state-of-the-art baselines in most cases,
especially in the zero-shot scenarios. Considering the enhanced
training targets, e.g., Sliced score matching [ 46], shed light on train-
ing EBM more steadily on image data, we will further investigate
more efficient optimization strategies for EBMs in graph-data.
ACKNOWLEDGMENTS
This work is partly supported by the National Key Research and
Development Program of China (No. 2023YFF0725000), the Na-
tional Natural Science Foundation of China (No. 62306255 and No.
92370204), the Natural Science Foundation of Guangdong Province
(No. 2024A1515011839), the Fundamental Research Project of
Guangzhou (No. 2024A04J4233), the Guangzhou-HKUST(GZ) Joint
Funding Program (No.2023A03J0008), and the Education Bureau of
Guangzhou Municipality.
 
916KDD ’24, August 25–29, 2024, Barcelona, Spain. Zheng Gong and Ying Sun
REFERENCES
[1]Liron Bergman and Yedid Hoshen. 2020. Classification-Based Anomaly Detection
for General Data. In Proc. of ICLR.
[2]Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jörg Sander. 2000.
LOF: identifying density-based local outliers. In Proc. of SIGMOD.
[3]Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. 2020. Measuring
and relieving the over-smoothing problem for graph neural networks from the
topological view. In Proc. of AAAI.
[4]Liyi Chen, Zhi Li, Yijun Wang, Tong Xu, Zhefeng Wang, and Enhong Chen.
2020. MMEA: entity alignment for multi-modal knowledge graph. In Knowledge
Science, Engineering and Management: 13th International Conference, KSEM 2020,
Hangzhou, China, August 28–30, 2020, Proceedings, Part I 13.
[5] Zhenxing Chen, Bo Liu, Meiqing Wang, Peng Dai, Jun Lv, and Liefeng Bo. 2020.
Generative adversarial attributed network anomaly detection. In Proc. of CIKM.
[6]Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2021. Adaptive Universal
Generalized PageRank Graph Neural Network. In Proc. of ICLR.
[7]Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. 2019. Deep anomaly
detection on attributed networks. In Proc. of SDM.
[8]Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. 2020.
Enhancing graph neural network-based fraud detectors against camouflaged
fraudsters. In Proc. of CIKM.
[9]Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. 2021. Improved
Contrastive Divergence Training of Energy-Based Models. In Proc. of ICML.
[10] Yilun Du and Igor Mordatch. 2019. Implicit generation and modeling with energy
based models. Proc. of NeurIPS (2019).
[11] Anuroop Gaddam, Tim Wilkin, Maia Angelova, and Jyotheesh Gaddam. 2020.
Detecting sensor faults, anomalies and outliers in the internet of things: A survey
on the challenges and solutions. Electronics (2020).
[12] Zheng Gong, Guifeng Wang, Ying Sun, Qi Liu, Yuting Ning, Hui Xiong, and
Jingyu Peng. 2023. Beyond homophily: Robust graph anomaly detection via
neural sparsification. In Proc. of IJCAI.
[13] Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud,
Mohammad Norouzi, and Kevin Swersky. 2020. Your classifier is secretly an
energy based model and you should treat it like one. In Proc. of ICLR.
[14] Dan Hendrycks and Kevin Gimpel. 2017. A Baseline for Detecting Misclas-
sified and Out-of-Distribution Examples in Neural Networks. Proceedings of
International Conference on Learning Representations (2017).
[15] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. 2018. Deep Anomaly
Detection with Outlier Exposure. In Proc. of ICLR.
[16] Geoffrey E Hinton. 2002. Training products of experts by minimizing contrastive
divergence. Neural computation (2002).
[17] Rui Huang, Andrew Geng, and Yixuan Li. 2021. On the importance of gradients
for detecting distributional shifts in the wild. Proc. of NeurIPS (2021).
[18] Aapo Hyvärinen. 2007. Some extensions of score matching. Computational
statistics & data analysis (2007).
[19] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv
preprint arXiv:1611.07308 (2016).
[20] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In Proc. of ICLR.
[21] Polina Kirichenko, Pavel Izmailov, and Andrew G Wilson. 2020. Why normalizing
flows fail to detect out-of-distribution data. Proc. of NeurIPS (2020).
[22] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-
bedding trajectory in temporal interaction networks. In Proc. of KDD.
[23] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. 2006. A
tutorial on energy-based learning. Predicting structured data (2006).
[24] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. 2018. Training Confidence-
calibrated Classifiers for Detecting Out-of-Distribution Samples. In Proc. of ICLR.
[25] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018. A simple unified
framework for detecting out-of-distribution samples and adversarial attacks. Proc.
of NeurIPS (2018).
[26] Zenan Li, Qitian Wu, Fan Nie, and Junchi Yan. 2022. Graphde: A generative
framework for debiased learning and out-of-distribution detection on graphs.
Proc. of NeurIPS (2022).
[27] Shiyu Liang, Yixuan Li, and R Srikant. 2018. Enhancing The Reliability of Out-of-
distribution Image Detection in Neural Networks. In Proc. of ICLR.
[28] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang,
Kaize Ding, Canyu Chen, Hao Peng, Kai Shu, Lichao Sun, Jundong Li, George H.
Chen, Zhihao Jia, and Philip S. Yu. 2022. Bond: Benchmarking unsupervised
outlier node detection on static attributed graphs. Proc. of NeurIPS (2022).
[29] Julian McAuley and Jure Leskovec. 2013. From Amateurs to Connoisseurs:
Modeling the Evolution of User Expertise through Online Reviews. the web
conference (2013).
[30] Péter Mernyei and Cătălina Cangea. 2020. Wiki-CS: A Wikipedia-Based Bench-
mark for Graph Neural Networks. arXiv preprint arXiv:2007.02901 (2020).
[31] Warren Morningstar, Cusuh Ham, Andrew Gallagher, Balaji Lakshminarayanan,
Alex Alemi, and Joshua Dillon. 2021. Density of states estimation for out of
distribution detection. In Proc. of AISTATS.[32] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan.
2019. Detecting out-of-distribution inputs to deep generative models using
typicality. arXiv preprint arXiv:1906.02994 (2019).
[33] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are
easily fooled: High confidence predictions for unrecognizable images. In Proc. of
CVPR.
[34] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. 2020.
Geom-GCN: Geometric Graph Convolutional Networks. In Proc. of ICLR.
[35] Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, Qinghua Zheng, et al .2018.
ANOMALOUS: A Joint Modeling Approach for Anomaly Detection on Attributed
Networks.. In Proc. of IJCAI.
[36] Chen Qiu, Timo Pfrommer, Marius Kloft, Stephan Mandt, and Maja Rudolph.
2021. Neural transformation learning for deep anomaly detection beyond images.
InProc. of ICML.
[37] Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang.
2018. Deepinf: Social influence prediction with deep learning. In Proc. of KDD.
[38] Shebuti Rayana and Leman Akoglu. 2015. Collective Opinion Spam Detection:
Bridging Review Networks and Metadata. knowledge discovery and data mining
(2015).
[39] Shebuti Rayana and Leman Akoglu. 2015. Collective opinion spam detection:
Bridging review networks and metadata. In Proc. of KDD.
[40] Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo,
Joshua Dillon, and Balaji Lakshminarayanan. 2019. Likelihood ratios for out-of-
distribution detection. Proc. of NeurIPS (2019).
[41] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. 2021. Multi-scale attributed
node embedding. Journal of Complex Networks (2021).
[42] Robin Schirrmeister, Yuxuan Zhou, Tonio Ball, and Dan Zhang. 2020. Under-
standing anomaly detection with deep invertible networks through hierarchies
of distributions and features. Proc. of NeurIPS (2020).
[43] Vikash Sehwag, Mung Chiang, and Prateek Mittal. 2021. SSD: A Unified Frame-
work for Self-Supervised Outlier Detection. In Proc. of ICLR.
[44] Joan Serrà, David Álvarez, Vicenç Gómez, Olga Slizovskaia, José F Núñez, and
Jordi Luque. 2020. Input Complexity and Out-of-distribution Detection with
Likelihood-based Generative Models. In Proc. of ICLR.
[45] Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for
few-shot learning. Proc. of NeurIPS (2017).
[46] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. 2020. Sliced score matching:
A scalable approach to density and score estimation. In Uncertainty in Artificial
Intelligence.
[47] Yang Song and Diederik P Kingma. 2021. How to train your energy-based models.
arXiv preprint arXiv:2101.03288 (2021).
[48] Yu Song and Donglin Wang. 2022. Learning on Graphs with Out-of-Distribution
Nodes. In Proc. of KDD.
[49] Tijmen Tieleman. 2008. Training restricted Boltzmann machines using approxi-
mations to the likelihood gradient. In Proc. of ICML.
[50] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou,
Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang
Li, and Zheng Zhang. 2019. Deep Graph Library: A Graph-Centric, Highly-
Performant Package for Graph Neural Networks. arXiv preprint arXiv:1909.01315
(2019).
[51] Mark Weber, Giacomo Domeniconi, Jie Chen, Daniel Karl I Weidele, Claudio
Bellei, Tom Robinson, and Charles E Leiserson. 2019. Anti-money laundering in
bitcoin: Experimenting with graph convolutional networks for financial forensics.
arXiv preprint arXiv:1908.02591 (2019).
[52] Max Welling and Yee W Teh. 2011. Bayesian learning via stochastic gradient
Langevin dynamics. In Proc. of ICML.
[53] Qitian Wu, Yiting Chen, Chenxiao Yang, and Junchi Yan. 2023. Energy-based
Out-of-Distribution Detection for Graph Neural Networks. In Proc. of ICLR.
[54] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. 2021. Generalized
out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334 (2021).
[55] Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. 2018. Graph
convolutional policy network for goal-directed molecular graph generation. Proc.
of NeurIPS (2018).
[56] Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-temporal graph convo-
lutional networks: a deep learning framework for traffic forecasting. In Proc. of
IJCAI.
[57] Shuangfei Zhai, Yu Cheng, Weining Lu, and Zhongfei Zhang. 2016. Deep struc-
tured energy based models for anomaly detection. In Proc. of ICML.
[58] Shengzhe Zhang, Liyi Chen, Chao Wang, Shuangli Li, and Hui Xiong. 2024.
Temporal Graph Contrastive Learning for Sequential Recommendation. In Proc.
of AAAI.
[59] Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. 2020. Uncertainty aware
semi-supervised learning on graph data. Proc. of NeurIPS (2020).
[60] Yu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-Ping Phoebe
Chen. 2021. Generative and contrastive self-supervised learning for graph anom-
aly detection. IEEE Transactions on Knowledge and Data Engineering (2021).
[61] Ev Zisselman and Aviv Tamar. 2020. Deep residual flow for out of distribution
detection. In Proc. of CVPR.
 
917An Energy-centric Framework for Category-free Out-of-distribution Node Detection in Graphs KDD ’24, August 25–29, 2024, Barcelona, Spain.
A CONTRASTING OOD NODE DETECTION
WITH GRAPH ANOMALY/OUTLIER
DETECTION
As previously discussed in subsection 2.2, there are key differences
between the OOD node detection task proposed in this paper and
current research on graph anomaly/outlier detection. In this sec-
tion, we will elaborate extensively on the distinctions between not
only the task formulations, but also the methodological approaches
employed in our method compared to existing techniques for graph
anomaly detection.
Task settings. The OOD node detection task examined in this
work involves identifying out-of-distribution nodes in a graph
G=(V,E,X)comprised of a node set, structure, and associated
node attributes. Critically, our framework assumes access to in-
distribution (IND) annotations for a subset of nodes V′
in⊂V in,
as well as a small number of optional out-of-distribution node
annotationsV′
out⊂V out(|V′
out|≪|V out|). This differs from ex-
isting graph anomaly detection methods [ 7,28], which solely rely
on properties ofGto detect outliers without additional label in-
formation. We argue that our problem formulation more readily
extends to certain real-world applications where acquiring IND la-
bels carries minimal cost, as evidenced in tasks like financial fraud
detection [ 51] and identification of deceptive reviews [ 38]. The
annotation signals serve to guide representation learning towards
improved separation between the latent embeddings of IND versus
OOD nodes for more precise detection.
Methodologies. Regarding methodological approaches, our pro-
posed technique substantially diverges from existing research on
graph anomaly and outlier detection. Mainstream techniques for
detecting anomalous graph nodes typically focus on calculating re-
construction error under different graph autoencoder architectures
to quantify deviations [ 7,28]. In contrast, our method is grounded
in a generative density-based framework that directly estimates the
likelihood of in-distribution nodes arising from the learned feature
distributions. OOD nodes that violate IND training data receive
higher OOD scores through this likelihood-ratio formulation.
Experimental results. To facilitate comparative assessment, our
experiments additionally implement popular graph anomaly/outlier
detection algorithms, such as DOMINANT [ 7], within the OOD
node detection task paradigm introduced in this work. Quantitative
results demonstrate that modeling latent feature densities and likeli-
hoods can better distinguish in-distribution and out-of-distribution
nodes versus purely reconstruction-driven anomaly scoring.
B PROOF OF THEORETICAL BACKGROUND
In Section 5, we have introduced the probability density estima-
tion based on derived energy and the gradient of their log likeli-
hood (i.e., density) in Equation (9). We now illustrate how it can be
derived [47].
For an EBM deriving the density of nodes through the form:
𝑝𝜃(𝑣)=exp(−𝐸𝜃(𝑣))
𝑍(𝜃), (14)where𝐸𝜃(𝑣)is a nonliear regression function with parameters 𝜃
which maps node 𝑣to a scalar, i.e., energy. And 𝑍(𝜃)denotes the
normalizing constant with respect to 𝑣(a.k.a partition function ):
𝑍(𝜃)=∫
exp(−𝐸𝜃(𝑣))𝑑𝑣. (15)
Since𝑍(𝜃)is a function of 𝜃, differentiation of 𝐷KL
Pin
V∥𝑝𝜃
in
Equation (9)w.r.t its parameters 𝜃incorporates a intractable integral.
To remedy this, the gradient of 𝐷KL
Pin
V∥𝑝𝜃
can be derived as
follows:
∇𝜃𝐷KL
Pin
V∥𝑝𝜃
=−∇𝜃E𝑣∼Pin
Vlog𝑝𝜃(𝑣)+∇𝜃E𝑣∼Pin
Vh
logPin
V(𝑣)i
|                      {z                      }
=0
(𝑖)=E𝑣∼Pin
V∇𝜃𝐸𝜃(𝑣)+∇𝜃log𝑍(𝜃),(16)
where step(𝑖)is from definition (14).
The first gradient term, E𝑣∼Pin
V∇𝜃𝐸𝜃(𝑣), is straightforward to
optimization with IN distribution nodes. The challenge is in approx-
imating the second gradient term, ∇𝜃log𝑍(𝜃), which is intractable
to compute exactly. The gradient term can be derived as the follow-
ing expectation:
∇𝜃log𝑍(𝜃)
=∇𝜃log∫
exp(−𝐸𝜃(𝑣))𝑑𝑣
(𝑖)=∫
exp(−𝐸𝜃(𝑣))𝑑𝑣−1
∇𝜃∫
exp(−𝐸𝜃(𝑣))𝑑𝑣
=∫
exp(−𝐸𝜃(𝑣))𝑑𝑣−1∫
∇𝜃exp(−𝐸𝜃(𝑣))𝑑𝑣
(𝑖𝑖)=∫
exp(−𝐸𝜃(𝑣))𝑑𝑣−1∫
exp(−𝐸𝜃(𝑣))(−∇𝜃𝐸𝜃(𝑣))𝑑𝑣
=∫∫
exp(−𝐸𝜃(𝑣))𝑑𝑣−1
exp(−𝐸𝜃(𝑣))(−∇𝜃𝐸𝜃(𝑣))𝑑𝑣
(𝑖𝑖𝑖)=∫exp(−𝐸𝜃(𝑣))
𝑍(𝜃)(−∇𝜃𝐸𝜃(𝑣))𝑑𝑣
(𝑖𝑣)=∫
𝑝𝜃(𝑣)(−∇ 𝜃𝐸𝜃(𝑣))𝑑𝑣
=−E𝑣∼𝑝𝜃∇𝜃𝐸𝜃(𝑣),(17)
where steps(𝑖)and(𝑖𝑖)are due to the chain rule of gradients, (iii) is
from Equation (15)and (iv) is from Equation (14). Thus, the gradient
of𝐷KL
Pin
V∥𝑝𝜃
can derived through following terms:
∇𝜃𝐷KL
Pin
V∥𝑝𝜃
=E𝑣∼Pin
V∇𝜃𝐸𝜃(𝑣)
|             {z             }
in distribution−E˜𝑣∼𝑝𝜃∇𝜃𝐸𝜃(˜𝑣)
|            {z            }
model distribution, (18)
where ˜𝑣is a random node sampled from the distribution given by
the Energy-based GNN. In a nutshell, to minimize 𝐷KL
Pin
V∥𝑝𝜃
,
the parameters of Energy-based GNN should be optimized by de-
creasing the energies of IND nodes and increasing the energies of
nodes sampled from model distribution, i.e., hallucinated nodes.
 
918KDD ’24, August 25–29, 2024, Barcelona, Spain. Zheng Gong and Ying Sun
C ENERGYDEF TRAINING PIPELINE
Figure 3 presents a concise summary of the overall training pro-
cess for EnergyDef. To further clarity and comprehension, we have
detailed the systematic procedure for training EnergyDef in Algo-
rithm 1. In this section, we provide a time complexity analysis of
the training procedure for the EnergyDef algorithm. Firstly, the
time complexity for each inference of the non-homophilic energy-
based GNN is estimated to be O(𝐾|E|), where𝐾represents the
number of layers in the GNN and |E|signifies the number of edges
in the graph. This inference time complexity analysis aligns with
that of GCN [ 20]. The initialization of parameters, denoted as U,
involves training on the edges, leading to the time complexity of
structure estimation being O(|E|) . When generating the attributes
and structures of the hallucinated nodes, the time complexity of the
attribute sampling steps necessitates 𝑇iterations of GNN inference,
resulting in a time complexity of O(𝑇𝐾(|E|+|V|)) for this step.
Meanwhile, the structure sampling steps require a time complexity
ofO(𝑇
˜𝑡|V|2)in the graphG. Therefore, taking all these factors into
account, the overall time complexity of the EnergyDef algorithm
isO(𝑇𝐾(|E|+|V|)+𝑇
˜𝑡|V|2).
Taking into account the application of sparsification computa-
tion and k-nearest neighbors (kNN) acceleration techniques, the
final time complexity can be mitigated to some extent. This imple-
mentation allows for a more efficient execution of the EnergyDef
algorithm, thereby reducing the computational burden.
D EXPERIMENT DETAILS
D.1 Baseline Description
The evaluated baselines can be divided into two distinct categories
according to the task configurations of zero- and few-shot OOD
node detection.
Zero-shot baselines. In the context of zero-shot scenarios, we
verify the effectiveness of our EnergyDef by comparing it with the
following baselines.
•LOF-KNN [2] computes the OOD scores of node attributes
by measuring local density deviation relative to k-nearest
node attributes.
•MLPAE uses MLP as the backbone of an autoencoder and
regards the reconstruction errors of node attributes as OOD
score and is trained by minimizing the reconstruction errors
of IND training nodes.
•GCNAE [19] replaces MLP with GCN as the backbone of
autoencoder. The training process and OOD score are the
same as MLPAE.
•GAAN [22] is a generative adversarial attribute network
that measures sample reconstruction error and real sample
recognition confidence to make predictions.
•DOMINANT [7] integrates a structure reconstruction de-
coder and an attribute reconstruction decoder. The finalreconstruction error associated with each node consists of
errors produced by these two decoders.
•ANOMALOUS [35] is an anomaly detector with CUR de-
composition and residual analysis.
•SL-GAD [60] derives OOD scores of nodes from two per-
spectives, i.e., reconstruction error and contrastive scores.
•GOAD [1] augment training data into independent trans-
formation spaces and train a classifier to match the aug-
mented data and corresponding transformations, then ob-
tain OOD score via the distances between OOD inputs and
centers of transformation spaces. To adapt GOAD to the
graph-structured data, we use the same GNN backbone with
our EnergyDef -h.
•NeuTral AD [36] use learnable transformations and embed
the transformed data into a semantic space. The proposed
OOD score is calculated by deterministic contrastive loss.
We also use the same GNN backbone with EnergyDef -h.
•GKDE [59] predicts the Dirichlet distributions of nodes
and estimates the uncertainty as OOD scores from multiple
sources.
•OODGAT [48] is an entropy-based OOD detector that as-
sumes the availability of node category labels. OODGAT uti-
lizes Graph Attention Network as the backbone and predicts
OOD nodes based on the outcome of category distribution.
•GNNSafe [53] calculates the OOD score of nodes via the
LogSumExp function over the output logits of a GNN clas-
sifier, which is trained through multi-category labels. The
rationale behind the OOD score is the commonality between
the Softmax function and the Boltzmann distribution.
Few-shot baselines. To evaluate the performance of our method
under small quantities of OOD annotations, we compare EnergyDef
with the following baselines:
•Cross Entropy Loss serves as a rudimentary way to train
the OOD detectors based on IND/OOD annotations, albeit it
is significantly impeded by the presence of IND/OOD class
imbalance.
•ProNet [45] infers the prototypes associated with IND nodes
and OOD nodes based on their representations calculated
from the GNN backbone of EnergyDef -h. The OOD scores
of testing nodes are obtained according to the distances
between node embedding and prototypes.
•Mahalanobis distance [25] uses the Mahalanobis distance
between test nodes and the closest class-conditional Gauss-
ian distribution as the OOD score. The encoder is aligned
with EnergyDef -h.
•Outlier Exposure [15] is a alternative approach to train-
ing the energies derived from EnergyDef. It optimizes the
learnable parameters through the margin ranking loss of
IND node energies and OOD node energies.
•Self-Supervised Outlier Detection [43] utilizes contrastive
learning to extract the representations of nodes and choose
the Mahalanobis distance as the metric of OOD score.
 
919