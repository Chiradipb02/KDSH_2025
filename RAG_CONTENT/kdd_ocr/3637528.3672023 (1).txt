Learning Flexible Time-windowed Granger Causality Integrating
Heterogeneous Interventional Time Series Data
Ziyi Zhang
zyzhang@tamu.edu
Texas A&M University
College Station, Texas, USAShaogang Ren
shaogang@tamu.edu
Texas A&M University
College Station, Texas, USA
Xiaoning Qian
xqian@tamu.edu
Texas A&M University, College Station, Texas
Brookhaven National Laboratory, Upton, New York
USANick Duffield
duffieldng@tamu.edu
Texas A&M University
College Station, Texas, USA
ABSTRACT
Granger causality, commonly used for inferring causal structures
from time series data, has been adopted in widespread applica-
tions across various fields due to its intuitive explainability and
high compatibility with emerging deep neural network prediction
models. To alleviate challenges in better deciphering causal struc-
tures unambiguously from time series, the use of interventional
data has become a practical approach. However, existing methods
have yet to be explored in the context of imperfect interventions
with unknown targets, which are more common and often more
beneficial in a wide range of real-world applications. Additionally,
the identifiability issues of Granger causality with unknown in-
terventional targets in complex network models remain unsolved.
Our work presents a theoretically-grounded method that infers
Granger causal structure and identifies unknown targets by lever-
aging heterogeneous interventional time series data. We further
illustrate that learning Granger causal structure and recovering
interventional targets can mutually promote each other. Compara-
tive experiments demonstrate that our method outperforms several
robust baseline methods in learning Granger causal structure from
interventional time series data.
CCS CONCEPTS
•Computing methodologies →Machine learning; •Mathe-
matics of computing →Causal networks.
KEYWORDS
Granger causality; Causal structure learning; Interventional time
series data
ACM Reference Format:
Ziyi Zhang, Shaogang Ren, Xiaoning Qian, and Nick Duffield. 2024. Learning
Flexible Time-windowed Granger Causality Integrating Heterogeneous
Interventional Time Series Data. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD ’24), August
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.367202325–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3637528.3672023
1 INTRODUCTION
Time series data, capturing complex systems dynamic behaviors,
are widely collected in many research areas, such as economics, bio-
informatics, and geo-informatics. Due to the rapid advancements
in sensor and computing technologies, there has been a significant
increase in research modeling time series data in recent years. Re-
searchers have developed various methods leveraging time series
data to perform related analysis such as optimization [ 29,30], clas-
sification [ 26,44,46,47], clustering [ 19,27,48], forecasting [ 41,49],
and causal structure learning [ 12,21,24,28,31,39]. Among these
tasks, causal structure learning is particularly challenging but im-
portant. Multivariate time series data, which capture the evolving
states of multiple variables over time, facilitate deriving better
systems understanding across various domains. Causal structure
learning in multivariate time series data focuses on understanding
how different variables influence each other. This knowledge is
beneficial for explaining the data generation process and guiding
the design of time series analysis methods [14].
Granger causality has been widely used for analyzing time se-
ries data to discover causal relationships in numerous real-world
applications, including modern healthcare systems [ 42], medical
time series generation [ 23] and time series anomaly detection [ 35].
Many methods for learning causal structures in time series have
been developed based on the principles of Granger causality [ 6,
21,28,39,43]. However, Granger causality tests based on linear
models can be ineffective when faced with even slight non-linear
causal relationships in the measurements. Consequently, a signifi-
cant amount of research efforts have been focused on addressing
issues for Granger causality considering non-linearities [ 21,28,39].
Learning causal structures based solely on observational data is
challenging [ 36,37] because, under the faithfulness assumption, the
true causal structure can only be identified within a Markov Equiva-
lence Class (MEC) [ 40]. However, this identifiability improves when
we consider interventional data. We have observed that domain ex-
perts might be able to gather interventional data in practice, where
the underlying generative process varies across different conditions.
This characteristic of distribution shift presents unique challenges
as well as opportunities for learning causal structures in time series.
4408
KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Zhang, Shaogang Ren, Xiaoning Qian, & Nick Duffield
In these scenarios, the causal structure can be identified within an
Interventional Markov Equivalence Class ( I-MEC), which is a more
specific subset of the Markov equivalence class [ 2,17,45]. With
sufficient interventional observations, the causal structure can be
precisely identified [ 9,10]. Numerous methods have approached
causal structure learning with interventional data by framing it
within a continuous optimization framework [ 2,12,31], incorpo-
rating a continuous acyclicity constraint [ 50]. To address identifi-
ability challenges with time series data, the authors of [ 12] have
extended the work of [ 2,31], to effectively handle observational
and interventional time series data under both perfect [ 11,45] and
imperfect interventions [ 34] with known interventional targets [ 2]
(See Figure 1). However, in real-world applications, imperfect inter-
ventions with unknown targets are more common [ 51], requiring
information about interventional targets limits their applications
in more general cases. Therefore, learning causal structures from
interventional time series data is still an open problem.
Figure 1: Intervention types on time series: With known inter-
ventional target (red nodes), altered all causal relationships
from parent nodes in imperfect interventions (red dotted
lines) versus disconnection from parent nodes in perfect in-
terventions.
In this paper, we emphasize on the following parts, compared to
previous work:
•Practicality. Most methods require knowledge regarding inter-
ventional targets. However, in practical scenarios, distinguishing
which variables originate from the non-intervened domain and
identifying the exact interventional targets often proves to be
challenging.
•Accuracy. In previous research, understanding of imperfect in-
terventions has been limited to the node level. However, as illus-
trated in Figure 2, edge-level imperfect interventions can clarify
the specific situations leading to imperfect interventions, which
has not been explicitly studied.
•Identifiability. Despite the development of advanced Score-
based [ 12,24,31] or Granger causality-based [ 6,21,28,39,43]
causal structure learning methods for time series, issues related
to the identifiability of Granger causality with unknown inter-
ventional targets remain unresolved.
Consequently, we introduce a theoretically-guaranteed Interven-
tional Granger Causal structure learning (IGC ) method. This ap-
proach is designed for the simultaneous inference of Granger causal
structure and the identification of unknown interventional targets
at the edge level. It also leverages interventional time series data
across multiple domains, efficiently differentiating among those
Figure 2: (Left): Existing methods (node-level imperfect inter-
vention on an unknown target) can only identify the exact
node(s); (Right): whereas our method (edge-level interven-
tion identification) can identify both the node(s) and exact
edge(s).
that have not been intervened upon and those that have, espe-
cially in scenarios where interventional targets are unknown and
the distinctions are not readily apparent. In summary, the main
contributions of the paper are highlighted as:
•We have formalized the task of learning Granger causal structure
from heterogeneous interventional time series data. The inter-
ventional targets are unknown and samples from observational
distribution may be indistinguishable from other interventional
distribution.
•A theoretically-guaranteed method called Interventional Granger
Causal structure learning (IGC ) is developed to simultaneously
infer Granger causal structure and identify unknown interven-
tional targets at the edge level.
•We have shown that the exact minimization of the proposed
objective will identify the (I,D)-Markov equivalence class of
the ground truth graph in the context of unknown target setting,
then resolve the identifiability issues of Granger causality.
•Extensive experiments on both synthetic and real-world time
series data have demonstrated our proposed IGC outperforms
several robust baselines by utilizing interventional data.
2 RELATED WORK
Granger Causal Structure Learning: Much work has been con-
ducted on inferring causal structure based on Granger causality in
multivariate time series. Recent approaches for inferring Granger
causal structure leverage the expressive power of neural network
and are often based on regularized autoregressive models. [ 1] pro-
posed the Lasso Granger method. [ 39] proposed the sparse-input
multi-layer perceptron (MLP) and long short-term memory (LSTM)
to model the nonlinear Granger causality within multivariate time
series. [ 21] integrated an efficient economy statistical recurrent
unit architecture with input layer wights regularized in a group-
wise manner. [ 28] proposed a generalized vector autoregression
model that utilizes self-explaining neural networks (SENNs) for
inferring Granger causal structure, with an additional focus on
detecting signs of Granger-causal effects. [ 4,5] proposed a Granger
causal discovery algorithm that builds a causal adjacency matrix
for imputed and high-dimensional data using sparse regulariza-
tion. Although these methods are powerful techniques for inferring
Granger causal structure, they do not fully utilize interventional
data, nor do they address the identifiability problem.
4409Learning Flexible Time-windowed Granger Causality Integrating Heterogeneous Interventional Time Series Data KDD ’24, August 25–29, 2024, Barcelona, Spain
Causal Structure Learning from Interventional Data: A se-
ries of parametric studies treat data from different distributions,
often referred to as domains or environments, as interventional
data. [ 13] studied the problem of causal structure learning in linear
systems from observational data given in multiple domains, across
which the causal coefficients may vary. [ 45] studied the problem of
causal structure learning in the setting where both observational
and interventional data is available and extended the identifiabil-
ity results from perfect intervention [ 17] to general interventions.
[2] proposed a differentiable causal structure learning method for
static data that can leverage perfect, imperfect and unknown target
interventions using score function to identify the I-MEC. [ 24] pro-
pose a novel latent intervened non-stationary learning method to
recover the domain indexes and the causal structure. [ 12] extends
[2,31] to address both observational and interventional time series
data, including perfect and imperfect interventions with known
targets. However, effectively handling both observational and in-
terventional time series data in an imperfect setting with unknown
interventional targets remains a challenge.
3 PRELIMINARIES
Non-linear Granger Causality: Consider multivariate time series
T={x1,...,x𝑇}, where x∈R𝑑. Assume that causal relationships
between variables are given by the following structural model:
𝑥𝑖
𝑡+1=𝑔𝑖(𝑥1
1:𝑡,...,𝑥𝑑
1:𝑡)for1≤𝑖≤𝑑, (1)
where𝑔𝑖(·)is a function that specifies how the past values are
mapped to series 𝑖. Time series 𝑗isGranger non-causal for time
series𝑖if for all𝑥1
1:𝑡,...,𝑥𝑑
1:𝑡and all𝑥𝑗
1:𝑡≠ˆ𝑥𝑗
1:𝑡[39]:
𝑔𝑖(𝑥1
1:𝑡,...,𝑥𝑗
1:𝑡,...,𝑥𝑑
1:𝑡)=𝑔𝑖(𝑥1
1:𝑡,..., ˆ𝑥𝑗
1:𝑡,...,𝑥𝑑
1:𝑡). (2)
Interventions: In the context of causal structure learning, an inter-
vention on a variable 𝑥𝑖, involves altering its conditional probability
P(𝑥𝑖|PA(𝑥𝑖))to a new conditional probability eP(𝑥𝑖|PA(𝑥𝑖)), where
PA(𝑥𝑖)is the set of parents of the node 𝑥𝑖in the causal graph. It
is possible to apply interventions to several variables at once. The
set of variables on which 𝑘-th interventions are made is referred to
as the interventional targets, symbolized by I𝑘∈R𝑑. The interven-
tional family is defined asI:=(I1,...,I𝑛), where𝑛represents the
total number of interventions conducted.
Types of Interventions: The type of interventions depicted in
Figure 1 are generally categorized as imperfect intervention (also
known as soft or parametric intervention) [ 8,34]. In contrast, a
specific case within this broad category is the perfect interven-
tion (also referred to as hard or structural intervention), where
P(𝑥𝑖|PA(𝑥𝑖))=P(𝑥𝑖)[2, 11, 22, 45].
4 INTERVENTIONAL GRANGER CAUSAL
STRUCTURE LEARNING
In this section, we first discuss the challenge of learning Granger
causal structure from interventional time series data in situations
where the interventional targets are unknown, and samples from
the observational distribution is indistinguishable from those of
other interventional distributions. Subsequently, we propose our
Interventional Granger Causal structure learning (IGC ) method tolearn both the underlying Granger causal structure and unknown
interventional targets across different environments.
4.1 Granger Causality with Interventions
First, we start with a linear Lasso Granger methodology capable
of handling both observational and interventional data, drawing
inspiration from the concepts applied to independent and identi-
cally distributed (i.i.d.) datasets [ 2] and structural vector autore-
gression model [ 12]. The core principle of these methods involve
constructing a Directed Acyclic Graph (DAG) representing the
ground truth causal graph from the interventional data. This is
achieved by incorporating a distinct distribution family specifically
for the intervened nodes within the log-likelihood objective. Unlike
these methods that model the post-intervention distribution, our
approach concentrates on comparing the distributions before and
after the intervention under the framework of Granger causality
to help interpret the impact of the intervention on time series data
more clearly. Specifically, we employ W𝑒0∈R𝑑×𝑑to represent the
parameters of the density function for observational data under
the condition of no interventions. For each intervention I𝑘∈I,
we define another corresponding set of parameters W𝑒𝑘∈R𝑑×𝑑,
which captures the differences in density functions before and after
the𝑘-th intervention. In other words, the density function after the
𝑘-th intervention can be represented as W𝑒0+W𝑒𝑘. The collection
of these parameters is denoted by W:={W𝑒0,W𝑒1,...,W𝑒𝑛}. The
integrated training loss function, as described in Equation (3), takes
into account both observational and interventional data:
L(X;W)=𝑛∑︁
𝑘=1𝑇∑︁
𝑡=1𝑙∑︁
𝜏=1L𝑘(X𝑡−(W𝑒0+W𝑒𝑘)X𝑡−𝜏), (3)
where𝑛represents the total number of interventions and L𝑘sig-
nifies the training loss based on the time series data from the 𝑘-th
intervention. In this model, we do not know which environment
among𝑛environments is non-intervend and we assume that W𝑒0
remains constant across 𝑛different environments, interventions,
domains, or distributions. A time series 𝑗isGranger non-causal
for time series 𝑖if and only if the corresponding weight w𝑖𝑗in
the matrix W𝑒0is zero. Intuitively, if all elements in W𝑒𝑘are zero,
this suggests that the 𝑘-th environment is non-intervened. The
optimization process can be expressed as follows:
min
W𝑒0,W𝑒𝑘𝑛∑︁
𝑘=1𝑇∑︁
𝑡=1𝑙∑︁
𝜏=1||X𝑡−(W𝑒0+W𝑒𝑘)X𝑡−𝜏||2
2+𝜆Ω(W𝑒0,W𝑒𝑘),
(4)
where𝜏is the time lag. The final estimated Granger causal struc-
ture without interventions is represented by W𝑒0, and W𝑒𝑘de-
notes the underlying intervention structures after the 𝑘-th inter-
vention. The implementation details of the regularization penalty
termΩ(W𝑒0,W𝑒𝑘)will be discussed in the following section.
4.2 Non-linear Granger Causality with
Interventions
Linear Granger causal models, with their simplicity and straight-
forwardness, provide a clear but often oversimplified view of rela-
tionships among variables. In practice, it is challenging to model
the highly non-linear relationships among multiple variables from
4410KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Zhang, Shaogang Ren, Xiaoning Qian, & Nick Duffield
time series data. In our proposed IGC, we assume that there exist
functions𝑓𝑖:R𝑑×𝑇→Rand𝑔𝑖:R𝑑×𝑇→Rsuch that:
E[𝑥𝑖,𝑡+1|PA(𝑥𝑖,𝑡+1)]=𝑓𝑖(X𝑡:𝑡−𝑇)+𝑔𝑖(X𝑡:𝑡−𝑇). (5)
𝑓𝑖(x1,..,x𝑑)does not depend on x𝑘∈R𝑇ifx𝑘∩PA(𝑥𝑖,𝑡+1)=∅;
and𝑔𝑖(X𝑡:𝑡−𝑇)=0if𝑥𝑖,𝑡+1is not intervened, under the assumption
of no instantaneous effects [ 33]. Thus, our objective is to learn
𝑓=(𝑓1,...,𝑓𝑑)and𝑔=(𝑔1,...,𝑔𝑑)such that the estimated Granger
causal structure from 𝑓and interventional targets from 𝑔.
Let us first define W𝑒𝑘=W𝑒0+W𝑒𝑘and concentrate on a
single variable 𝑥𝑖within a specific environment 𝑒𝑘. We define a
set of parameters, which can be represented as: 𝜙𝑖𝑒𝑘, where𝜙𝑖𝑒𝑘=
{W𝑖
:,1,𝑒𝑘,...,W𝑖
:,𝑑,𝑒𝑘}and𝜙𝑒𝑘={𝜙1𝑒𝑘,...,𝜙𝑑𝑒𝑘}. Then the overall
objective becomes:
min
𝜙𝑒𝑘𝑑∑︁
𝑖=1𝑇∑︁
𝑡=2||𝑥𝑒𝑘
𝑖,𝑡−F𝑖(𝑥𝑒𝑘
𝑡−1:1;𝜙𝑖
𝑒𝑘)||2
2+𝜆𝑑∑︁
𝑖=1𝑑∑︁
𝑗=1||W𝑖
:,𝑗,𝑒𝑘||2,(6)
where F𝑖(·)is defined as:
F𝑖(𝑥𝑒𝑘;𝜙𝑖
𝑒𝑘)=𝑓𝑖(𝑥𝑒𝑘;W𝑖
𝑒0)+𝑔𝑖(𝑥𝑒𝑘;W𝑖
𝑒𝑘), (7)
andF𝑖(·)generates the estimate ˆ𝑥𝑖for the next timestep in 𝑒𝑘,
time series 𝑗isGranger non-causal for time series 𝑖in the𝑒𝑘if
and only if W𝑖
:,𝑗,𝑒𝑘is zero. With the above proposed objective and
heterogeneous interventional time series data from 𝑛environments,
we propose minimizing Equation (8) to prioritize the discovery of
the Granger causal structure that remains consistent across all
environmentsE={𝑒1,...,𝑒𝑛}.
min
𝜙Í𝑛
𝑘=1Í𝑑
𝑖=1Í𝑇
𝑡=2|
|𝑥𝑒𝑘
𝑖,𝑡−F𝑖(𝑥𝑒𝑘
𝑡−1:1;𝜙𝑖
𝑒𝑘)||2
2
+𝜆Í𝑑
𝑖=1Í𝑑
𝑗=1|
|(W𝑖
:,𝑗,𝑒1,...,W𝑖
:,𝑗,𝑒𝑛)||2,(8)
where𝜙={𝜙𝑒1,...,𝜙𝑒𝑛}represents the collection of parameters
across all𝑛environments.
To learn the unknown interventional targets while maintaining
consistency in the Granger causal structure, the overall penalized
objective becomes:
min
𝜙Í𝑛
𝑘=1Í𝑇
𝑡=2Í𝑑
𝑖=1Í𝑑
𝑗=1|
|𝑥𝑒𝑘
𝑖,𝑡−F𝑖𝑗(𝑥𝑒𝑘
𝑗,𝑡−1:1;W𝑖
:,𝑗,𝑒𝑘)||2
2
+(1−𝛼)𝜆Í𝑑
𝑖=1Í𝑑
𝑗=1|
|(W𝑖
:,𝑗,𝑒0,W𝑖
:,𝑗,𝑒1,...,W𝑖
:,𝑗,𝑒𝑛)||2
+𝛼𝜆Í𝑑
𝑖=1Í𝑑
𝑗=1Í𝑛
𝑘=1|
|W𝑖
:,𝑗,𝑒𝑘||2,(9)
where𝛼∈(0,1)controls the tradeoff in sparsity across and within
groups. After learning, time series 𝑗isGranger non-causal to𝑖
ifW𝑖
:,𝑗,𝑒0is zero across 𝑘distributions. Furthermore, there is no
intervention from time series 𝑗to𝑖in the𝑘-th distribution if W𝑖
:,𝑗,𝑒𝑘
is zero, which can be mathematically expressed as:
P(𝑥𝑒0
𝑖,𝑡|𝑥𝑒0
𝑗,𝑡−1:𝑡−𝜏)=P(𝑥𝑒𝑘
𝑖,𝑡|𝑥𝑒𝑘
𝑗,𝑡−1:𝑡−𝜏). (10)
4.3 Model Architecture
In line with the concepts presented in Section 4.2, IGC utilizes
historical time series data as its input and forecasts the data for the
subsequent timestep as its output. The principal contribution of
our study is the integration of heterogeneous interventional time
series data, which aids in the identification of both the Grangercausal structure and the interventional targets. The architecture of
the model is illustrated in Figure 3.
Figure 3: The information flow in various environments is
represented by different colors. During the learning pro-
cess, the prediction network (P) generates data for the next
timestep. Information about unknown targets is contained
within the intervention networks ( I𝑒), and the Granger causal
structure is captured within the causal network (C).
Intervention Networks. Consider a set of time series data X=
{X𝑒1,...,X𝑒𝑛}from𝑛environments or distributions. For each spe-
cific environment 𝑒𝑘, there exists an intervention network I𝑒𝑘=
{I1𝑒𝑘,...,I𝑑𝑒𝑘}, where each function I𝑖𝑒𝑘is defined as:
I𝑖
𝑒𝑘(X𝑡;𝑒𝑘;W𝑖
𝑒𝑘):R𝑇×𝑑→R𝑇×ℎ. (11)
In this context, I𝑖𝑒𝑘(·)represents the intervention network for node
𝑖in𝑒𝑘,X𝑡;𝑒𝑘∈R𝑇×𝑑is the historical multivariate time series data
in𝑒𝑘, and W𝑖𝑒𝑘∈R𝑑×ℎdenotes the parameters of the intervention
network I𝑖𝑒𝑘.
Granger Causal Network. For the time series data set X, a shared
Granger causal network is applicable to all environments within
X. This network is defined as C={C1,...,C𝑑}, where each C𝑖is
described by the function:
C𝑖(X𝑡;W𝑖
𝑒0):R𝑇×𝑑→R𝑇×ℎ. (12)
In this context, X𝑡represents the historical multivariate time series
data from each environment within Xpresented in sequence, and
W𝑖𝑒0∈R𝑑×ℎare the parameters of the Granger causal network for
node𝑖.
Information Aggregator. After generating both the intervention
information and the Granger causal information, we use a mecha-
nism to aggregate them:
Z𝑖
𝑡;𝑒𝑘=Agg(I𝑖
𝑒𝑘(X𝑡;𝑒𝑘;W𝑖
𝑒𝑘),C𝑖(X𝑡;𝑒𝑘;W𝑖
𝑒0)), (13)
where Agg(·)is an aggregation function and we have adopted
summation in our experiments. We leave a learnable aggregation
operator as a future research direction.
Prediction Network. The prediction network Pis designed to
forecast the 𝑖-th data point for the subsequent timestep:
bX𝑖
𝑡+1;𝑒𝑘=P(Z𝑖
𝑡;𝑒𝑘), (14)
wherebX𝑖
𝑡+1;𝑒𝑘∈Rrepresents the predicted value at timestep 𝑡+1,
while Z𝑖
𝑡;𝑒𝑘∈R𝑇×ℎdenotes the aggregated embedding obtained
from the previous step.
4411Learning Flexible Time-windowed Granger Causality Integrating Heterogeneous Interventional Time Series Data KDD ’24, August 25–29, 2024, Barcelona, Spain
To enhance flexibility, components such as I𝑒(·),C(·), and P(·)
can be effectively modeled using a variety of neural network ar-
chitectures, including MLP, LSTM, SENNs, and Transformer. In
our experiments, we employed MLPs and trained the model with
respect to Equation (9). As illustrated in Figure 3, information about
unknown targets is obtained from the intervention networks I𝑒,
while the Granger causal structure is estimated from the causal
network C. The IGCoperates under the Assumption 1.
Assumption 1. (Causal Consistency). There exists a consistent causal
structure and common parameters W𝑒0across different environments.
The dissimilarity between the parameters for one environment and the
common parameters W𝑒0lies within a range defined by a lower bound
𝜖𝑙, and an upper bound 𝜖𝑢. This range captures the extent of variation
allowed between the common parameters and different environments.
To avoid identical data across environments, the condition 𝜖𝑙=0
indicates that there is no significant intervention. Mathematically it
can be expressed as: 𝜖𝑙≤|W𝑒𝑘|≤𝜖𝑢,∀1≤𝑘≤𝑛,0≤𝜖𝑙≤𝜖𝑢.
4.4 Optimizing the Penalized Objective
To optimize the objective stated in Equation (9) for the proposed
IGCmethod, we use proximal gradient descent [ 32], which is par-
ticularly beneficial for our purposes as it results exact zeros in the
columns of input parameters, an essential aspect for interpreting
Granger non-causality and intervention within our framework. The
proximal operator is the group-wise soft-thresholding operator. De-
tailed updates of the proximal gradient descent are included in the
Appendix A.1. The proximal steps on the input weights for the
penalty in Equation (9) is shown in Algorithm 1, where Soft(·)is a
group soft-thresholding operator on the input weights [32].
Algorithm 1 Proximal steps for the penalty in Equation (9)
1:procedure Input(𝛼>0,𝜆>0,(W𝑖
:,𝑗,𝑒0,...,W𝑖
:,𝑗,𝑒𝑘))
2: for𝑘=1to𝑛do
3: W𝑖
:,𝑗,𝑒𝑘=Soft𝛼𝜆(W𝑖
:,𝑗,𝑒𝑘)
4: end for
5:(W𝑖
:,𝑗,𝑒0,...,W𝑖
:,𝑗,𝑒𝑘)=Soft(1−𝛼)𝜆((W𝑖
:,𝑗,𝑒0,...,W𝑖
:,𝑗,𝑒𝑘))
6: return(W𝑖
:,𝑗,𝑒0,...,W𝑖
:,𝑗,𝑒𝑘)
7:end procedure
5 IDENTIFIABILITY
Figure 4: The complex interactions in time series data (left)
lead to a Granger causal structure (right) that is not a strict
DAG.The identifiability of Granger causal structure for observational
time series data has been established, where the parameters Wcan
be identified from standard results in vector autoregressive (VAR)
models [ 31]. For linear time series interventional data, the identifi-
ability results have been studied in [ 3]. Specifically, the model is
identifiable if each variable is influenced by a unique set of inter-
vened variables. In the context of non-linear interventional time
series data with known interventional targets, [ 12] expanded upon
theI-Markov Equivalence Class to (I,D)-Markov Equivalence
Class for graphs within a subset of DAGs rather than all DAGs.
However, addressing the challenge of identifying Granger causal
structure in non-linear time series data with unknown interven-
tional targets remains a significant and unresolved area of research.
To address this issue, our initial step is to establish the negative
score function for a DAG G:
−SI(G):=EX|P𝑒𝑘,G∗[L𝑟𝑒𝑔(X)], (15)
whereL𝑟𝑒𝑔denotes the regularized loss minimized in Equation
(9), with the loss||·||2
2being negative log-liklihood, over the time
series data X, which is generated from the ground truth G∗, under
the interventional distribution P𝑒𝑘for each I𝑘∈I. Based on these
definitions, the following theorem holds:
Theorem 5.1. LetˆG∈D be a DAG and ˆIbe an interventional fam-
ily, which(ˆG,ˆI)∈ arg maxG,IS(G,I). Under the assumption that
the density models have sufficient capacity to represent the ground
truth distribution, that I∗-faithfulness holds, that the density models
are strictly positive, that the ground truth densities P𝑒𝑘have differ-
entiable entropy. For 𝜆G,𝜆I>0in Equation (9) small enough, ˆGis
(I∗,D)-Markov equivalent to G∗and ˆI=I∗.
The Granger causal strcture we’ve learned is not a strict DAG due to
the intricate nature of time series data, as shown in Figure 4. How-
ever, rather than focusing directly on the Granger causal structure,
our approach centers on the complex interactions within the time
series data. Particularly, given the forward-in-time property, the
unrolled temporally extended graph is a DAG∈R𝑑×𝑇and does not
include any cyclic subgraphs, thus we omit the DAG constraint [ 50]
in our theoretical analysis. Establishing the identifiability of this
DAG also allows us to identify the Granger causal structure. The
IGCmethodology, characterized by its time-windowed approach,
offers flexibility for detecting the causal relationship between any
two variables(𝑥𝑖,𝑡,𝑥𝑗,𝑡′)in this DAG with a given time lag 𝑝=𝑡′−𝑡.
If we setDto be the subsetD𝑠of DAGs which correspond to sta-
tionary dynamics with constant-in-time conditional distributions
(For detailed information and the proof of Theorem 5.1, please refer
to the Appendix A.2), Theorem 5.1 can be restated as follows:
Corollary 5.2. LetˆG∈D𝑠be a DAG and ˆIbe an interventional
family. Given the same assumptions as Theorem 5.1, and for 𝜆G,𝜆I
in Equation (9) small enough, ˆGis(I∗,D𝑠)-Markov equivalent to
G∗and ˆI=I∗.
The Theorem 5.1 extends prior work [ 2,12] by showing that, under
appropriate assumptions, maximizing S(G,I)with respectGand
Irecovers both the(I∗,D)-Markov equivalent class of G∗and
the ground truth interventional family I∗.
4412KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Zhang, Shaogang Ren, Xiaoning Qian, & Nick Duffield
Dataset Metrics V
AR PCMCI NGC eSRU D
yNoTears GV
AR CU
TS IGC
Linear
(n=5)A
cc 0.640(
±0.080) 0.800(
±0.040) 0.920(
±0.000) 0.960(
±0.000) 0.800(
±0.040) 0.960(
±0.000) 0.920(
±0.000) 1.000(
±0.000)
AUROC 0.650(
±0.017) 0.770(
±0.012) 0.925(
±0.011) 0.967(
±0.008) 0.740(
±0.005) 0.985(
±0.015) 0.933(
±0.005) 1.000(
±0.000)
F1 0.609(
±0.008) 0.667(
±0.017) 0.909(
±0.000) 0.952(
±0.000) 0.725(
±0.024) 0.949(
±0.000) 0.911(
±0.000) 1.000(
±0.000)
SHD 9(
±2) 5(
±1) 2(
±0) 1(
±0) 5(
±1) 1(
±0) 2(
±0) 0(
±0)
Linear
(n=10)A
cc 0.560(
±0.030) 0.610(
±0.030) 0.820(
±0.040) 0.850(
±0.050) 0.650(
±0.020) 0.930(
±0.010) 0.880(
±0.020) 0.930(
±0.010)
AUROC 0.562(
±0.024) 0.710(
±0.012) 0.848(
±0.010) 0.812(
±0.008) 0.524(
±0.006) 0.980(
±0.013) 0.865(
±0.042) 0.989(
±0.018)
F1 0.551(
±0.029) 0.456(
±0.048) 0.847(
±0.014) 0.869(
±0.022) 0.596(
±0.032) 0.912(
±0.014) 0.872(
±0.012) 0.928(
±0.017)
SHD 44(
±3) 39(
±3) 18(
±4) 15(
±5) 35(
±2) 7(
±1) 12(
±2) 7(
±1)
Linear
(n=20)A
cc 0.518(
±0.030) 0.555(
±0.030) 0.815(
±0.030) 0.730(
±0.020) 0.565(
±0.023) 0.783(
±0.040) 0.838(
±0.020) 0.955(
±0.005)
AUROC 0.538(
±0.035) 0.545(
±0.035) 0.822(
±0.011) 0.723(
±0.035) 0.511(
±0.005) 0.854(
±0.019) 0.832(
±0.017) 0.973(
±0.006)
F1 0.671(
±0.012) 0.351(
±0.052) 0.812(
±0.000) 0.772(
±0.012) 0.322(
±0.046) 0.800(
±0.038) 0.816(
±0.011) 0.955(
±0.002)
SHD 193(
±6) 178(
±12) 74(
±12) 108(
±6) 174(
±9) 87(
±16) 65(
±8) 18(
±2)
Dataset Metrics V
AR PCMCI NGC eSRU D
yNoTears GV
AR CU
TS IGC
Non-linear
(n=5)A
cc 0.458(
±0.080) 0.560(
±0.040) 0.960(
±0.000) 0.760(
±0.040) 0.800(
±0.080) 0.920(
±0.040) 0.920(
±0.040) 1.000(
±0.000)
AUROC 0.517(
±0.035) 0.567(
±0.009) 0.967(
±0.008) 0.767(
±0.018) 0.740(
±0.005) 0.912(
±0.019) 0.935(
±0.015) 1.000(
±0.000)
F1 0.563(
±0.013) 0.522(
±0.012) 0.952(
±0.000) 0.727(
±0.006) 0.725(
±0.054) 0.920(
±0.020) 0.915(
±0.016) 1.000(
±0.000)
SHD 14(
±2) 11(
±1) 1(
±0) 6(
±1) 5(
±2) 2(
±1) 2(
±1) 0(
±0)
Non-linear
(n=10)A
cc 0.520(
±0.020) 0.580(
±0.020) 0.880(
±0.020) 0.710(
±0.030) 0.620(
±0.030) 0.920(
±0.010) 0.860(
±0.030) 0.930(
±0.020)
AUROC 0.512(
±0.004) 0.626(
±0.015) 0.892(
±0.009) 0.709(
±0.038) 0.548(
±0.008) 0.901(
±0.020) 0.859(
±0.031) 0.959(
±0.005)
F1 0.658(
±0.029) 0.600(
±0.020) 0.893(
±0.011) 0.721(
±0.012) 0.498(
±0.042) 0.913(
±0.016) 0.834(
±0.009) 0.942(
±0.011)
SHD 48(
±2) 42(
±2) 12(
±2) 29(
±3) 38(
±3) 9(
±1) 14(
±3) 7(
±2)
Non-linear
(n=20)A
cc 0.508(
±0.008) 0.545(
±0.025) 0.795(
±0.018) 0.647(
±0.013) 0.543(
±0.020) 0.825(
±0.048) 0.805(
±0.020) 0.943(
±0.008)
AUROC 0.515(
±0.010) 0.548(
±0.020) 0.800(
±0.014) 0.641(
±0.014) 0.587(
±0.008) 0.882(
±0.016) 0.820(
±0.035) 0.950(
±0.015)
F1 0.659(
±0.008) 0.461(
±0.022) 0.793(
±0.020) 0.714(
±0.003) 0.435(
±0.017) 0.821(
±0.027) 0.811(
±0.004) 0.944(
±0.006)
SHD 197(
±3) 182(
±10) 82(
±7) 141(
±5) 183(
±8) 70(
±19) 78(
±8) 23(
±3)
Table 1: Comparative results (mean ±std.) for synthetic interventional datasets.
6 EXPERIMENTS
We evaluate our proposed IGC1for inferring Granger causal stru-
ture and compare them with various state-of-the-art (SOTA) base-
lines across several interventional time series datasets, demonstrat-
ing the superior performance of our proposed IGCmethod. The
competing SOTA methods for learning Granger causal structure
that we benchmarked are listed as follows: 1) VAR (Vector Au-
toRegressive) [ 15,16] is a linear model used in Granger causality
test.PCMCI [38] integrates conditional independence tests with
optimized conditioning sets for inferring causal structure. NGC
[39] includes the component-wise MLP and the component-wise
LSTM, featuring sparse input weight layers, is proposed as an ef-
fective approach for inferring non-linear Granger causality. eSRU
[21] (economy Statistical Recurrent Units) are a specialized form of
recurrent neural networks (RNNs) tailored to identify the network
structure of non-linear Granger causal relationships. DyNoTears
[31] is a score-based method with continuous optimization for learn-
ing causal structure. GVAR [28] model integrates SENNs with tra-
ditional vector autoregression for Granger causal inference. CUTS
[4,5] is a neural Granger causal discovery algorithm for imputed
and high dimensional data.
For evaluation purposes, we utilize the following metrics: Ac-
curacy refers to the rate at which a model correctly predicts the
presence or absence of edges in the ground-truth graph. AUROC
(Area Under the Receiver Operating Characteristic) curve is rep-
resented by the area under a curve plotting the true positive rate
against the false positive rate at various thresholds. AUPRC (Area
Under the Precision-Recall Curve) focuses on the relationship be-
tween precision and recall across different thresholds. The F1 Score
1https://github.com/Tamuzzy/IGCrepresents the harmonic mean of precision and recall, with pre-
cision being the proportion of correctly detected edges relative
to all edges predicted by the model. SHD (Structural Hamming
Distance) denotes the count of incorrectly predicted edge states.
Recall measures the fraction of edges in the ground-truth graph
that are accurately identified by the model.
6.1 Granger Causal Structure Learning
Firstly, we follow the functional causal model [ 18] detailed in Equa-
tion (16) to generate synthetic interventional time series data:
𝑋𝑖
𝑡=∑︁
𝑓𝑖(𝑋𝑗)+𝜖𝑖,𝑡, (16)
where𝑋𝑗∈PA(𝑋𝑖,𝑡)and the function 𝑓𝑖can be selected from a set
of functions which includes linear, cubic, tanh, and sinc functions,
as well as their mixtures. The noise term 𝜖𝑖,𝑡is generated from
either a uniform distribution U(− 0.5,0.5)or a standard normal
distributionN(0,1).
Linear Synthetic Interventional Time Series Data: In the linear
setting, we generate the time series data by following these steps:
•We constructed the Granger causal graph Gby employing two
tunable parameters: 𝑛(number of nodes) and 𝑝(probability of
edge creation). In our experiments, we set 𝑛to be 5, 10, and 20,
and𝑝=0.4, we sample the weights uniformly at random from
U([−0.6,−0.4]∪[ 0.4,0.6]).
•We generate the data with first autoregessive order, where data
only depends on the previous time step. We generate 5, 10, and
20 sequences with 500 time steps with standard Gaussian noises.
•To generate the interventional time series data in 𝑒𝑘, we adopt
an imperfect setting where the weights from PA(𝑋𝑖
𝑡;𝑒𝑘)to𝑋𝑖
𝑡;𝑒𝑘are altered at a specific timestep 𝑡=200by adding a random
number within the range U([−0.15,0)∪( 0,0.15]).
4413Learning Flexible Time-windowed Granger Causality Integrating Heterogeneous Interventional Time Series Data KDD ’24, August 25–29, 2024, Barcelona, Spain
Table 1 illustrates that IGC achieves the best performance, and
performs slightly better at 𝑛=20than at𝑛=10, which suggests
that the distinction does not significantly weaken our method.
Non-linear Synthetic Interventional Time Series Data: To
evaluate the efficiency of IGCin the context of non-linear synthetic
interventional time series data, we follow the same generation
procedure as that used in the linear setting. However, instead of
employing a linear function, we modify the underlying generation
function𝑓𝑖in Equation (16) to employ a 2-layer fully connected
neural network with the Leaky ReLU activation and 0.1 negative
non-linearity. The network weights are sampled uniformly from
U([− 0.6,−0.4]∪[0.4,0.6]). To implement imperfect interventions,
we add a random vector, drawn from N(0,1)to the network’s
second layer.
Figure 5: SHD results for Linear (left) and Non-linear (right)
Synthetic Interventional Time Series Data.
As illustrated in Table 1, we compare the performance of IGCagainst
other methods for 𝑛={5,10,20}. The results confirm that our
method consistently outperforms the others, even when the vari-
able size is large. Figure 5 presents the comparison of the results
for learning Granger causal structure with varying numbers of
variables in both linear and non-linear settings. From the results,
we observe that the introduction of interventional data disrupts
the stationary assumption underlying these models, leading to
poor performance in inferring the Granger causal structure. In
contrast, our model exhibits enhanced capability in managing the
non-stationarity induced by interventions, thereby achieving more
accurate inference of the Granger causal structure.
Lor
enz-96 Model
Metric/Metho
ds Acc AUPRC AUROC SHD
V
AR 0.765(±0.008) 0.464(±0.046) 0.745(±0.047) 94(±3)
PCMCI 0.720(
±0.020) 0.724(±0.007) 0.788(±0.033) 112(±8)
NGC 0.653(
±0.028) 0.956(±0.016) 0.979(±0.016) 139(±11)
eSRU 0.823(
±0.010) 0.834(±0.033) 0.934(±0.021) 70(±4)
D
yNoTears 0.785(±0.013) 0.779(±0.035) 0.811(±0.015) 86(±5)
GV
AR 0.845(±0.010) 0.916(±0.024) 0.970(±0.009) 62(±4)
CU
TS 0.755(±0.023) 0.785(±0.015) 0.876(±0.017) 98(±9)
IGC 0.925(
±0.008) 0.979(±0.003) 0.985(±0.002) 30(±2)
Table 2: Comparative results (mean±std.) for Lorenz-96.
Lorenz-96 Model: The Lorenz 96 model, a standard benchmark
for Granger causal inference techniques [ 25], is a continuous-timedynamic system with 𝑚variables, defined by non-linear differential
equations:
𝑑𝑥𝑖
𝑑𝑡=(𝑥𝑖+1−𝑥𝑖−2)𝑥𝑖−1−𝑥𝑖+𝐹, (17)
where𝑥0:=𝑥𝑚,𝑥−1:=𝑥𝑚−1, and𝑥𝑚+1:=𝑥1; and𝐹is a forcing
constant that, in combination with 𝑚, controls the non-linearity
of the system [ 20,39]. We numerically simulate 𝑚=20variables
and𝑇=500observations under 𝐹=40. This choice is predicated
on the understanding that a higher number of variables coupled
with a higher non-linearity ( 𝐹=40) presents a more challenging
inference problem. While adhering to the experimental setup of [ 28],
our study introduces a more challenging setting. We manipulated
the data with 𝑚=20variables and 𝑇=500observations under
𝐹=40by altering the value of 𝐹to 50 for samples when 𝑡>250,
thus introducing an intervention in the dataset to simulate real-
world complexities. From Table 2, we observe that our proposed IGC
achieves competitive performance even in more complex situations.
T
ennessee Eastman Dataset
Metric/Metho
ds Acc Recall F1 SHD Metric/Methods Acc Recall F1 SHD
CORL 0.838
0.043 0.071 176 NoTears-MLP 0.925 0.036 0.046 82
Dir
ectLiNGAM 0.918 0.046 0.061 89 PCMCI 0.882 0.094 0.044 129
FCI 0.966
0.167 0.091 37 DyNoTears 0.928 0.094 0.071 78
GES 0.903
0.040 0.060 106 eSRU 0.936 0.054 0.068 70
GEOLEM 0.890
0.031 0.046 120 GVAR 0.928 0.188 0.133 78
ICALiNGAM 0.908
0.079 0.116 100 NGC 0.852 0.089 0.148 161
MCSL 0.951
0.080 0.070 53 CUTS 0.922 0.094 0.066 85
No
Tears 0.968 0 0 35 IGC 0.968 0.286 0.103 35
Table 3: Comparative results for TEP Dataset.
Tennessee Eastman Process (TEP): The Tennessee Eastman Pro-
cess (TEP) [ 7], serves as a widely recognized benchmark in chemical
engineering research. This simulator is particularly valuable for
studies in anomaly detection and root cause analysis, due to its
capability to replicate process faults and the comprehensive descrip-
tion it offers of the entire production process. The TEP includes five
principal units: a two-phase reactor, a condenser, a recycle compres-
sor, a liquid-vapor separator, and a product stripper, involving 41
measured and 12 manipulated variables. The observational dataset
is devoid of anomalies and comprises 500 observations. Within
the TEP, there are 21 predefined faults, resulting in 21 distinct test
datasets. Each dataset contains 960 observations, recorded at 3-
minute intervals. The initial 160 observations in each dataset are
anomaly-free. Starting from observation 161 and continuing to the
end of the dataset, one of the 21 faults is introduced, marking a
transition to conditions where the system’s behavior deviates from
the norm. In our research, we have utilized 22 measured variables
and 11 manipulated variables. We have employed various causal
structure learning methods on the observational data and integrated
our propsoed IGCapproach on both observational data and inter-
ventional data with anomalies. The results, summarized in Table
3, demonstrate that our method consistently outperforms several
other techniques, reinforcing its efficacy in handling interventional
time series data. The higher Recall and F1-Score of our proposed
method compared to NoTears can be attributed to a greater number
of True Positives (TP) since the TEP dataset with a quite sparse
adjacency matrix∈R33×33, where only 66 elements being 1 (includ-
ing 33 diagonal elements). Our method, which is based on Granger
4414KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Zhang, Shaogang Ren, Xiaoning Qian, & Nick Duffield
Figure 6: The estimated Granger causal structure and the estimated unknown interventional targets across three different
environments, we have highlighted the discrepancies in the results using red blocks to indicate areas of disagreement.
causality, leverages historical temporal information effectively. In
contrast, NoTears is not specifically designed for time series data,
leading to its less efficient capture of this crucial information.
6.2 Interventional Family Recovery
So far, our focus has been on inferring Granger causal structure,
without addressing the issue of interventional family recovery,
which is crucial for a deeper understanding of various time series
analysis tasks. Although there have been some experiments target-
ing known interventions, to the best of our knowledge, this study
is the first to delve into the recovery of unknown interventional
targets from interventional time series data. This approach not only
enhances our understanding of the underlying processes but also
clarifies the interaction between the Granger causal structure learn-
ing and the recovery of unknown targets. To bridge the identified
gap, we assessed the model’s capability in accurately identifying
unknown interventional targets within synthetic datasets. We first
formulate the problem as follows:
Problem 1. Consider a Granger causal graph G∈R𝑑×𝑑, with the
assumption that the time series generation follows Equation (16).
We introduce interventional family, denoted as I:={I1,...,I𝑛},
where each I𝑘∈R𝑑×𝑑. This implies that, after 𝑡time steps, some
specific edges in the Granger causal graph Gare chosen as interven-
tional targets in 𝑘-th intervention. The problem is to recover these
interventional targets based solely on the observed intervened or
non-intervened time series data from several environments, with-
out access to the knowledge of non-intervened time series data.
Figure 6 illustrated that the Granger causal structure, generated
from the causal network, is stable across multiple environments,
despite changes in causal strength following the interventions. Re-
garding the intervention networks, the results highlight the effects
of interventions. We used red blocks to indicate areas of disagree-
ment in Figure 6 and we attribute these discrepancies to instances
where the intervention strength was insufficient or below certain
thresholds. We also evaluated the task of recovering interventional
targets on synthetic interventional time series data from several
distinct environments, as illustrated in Figure 7. We found that our
method prioritizes environments experiencing high-intensity inter-
ventions, potentially overlooking those with milder interventions.
Thus, it is important to set thresholds based on the specific appli-
cation to determine whether the environment is being intervened.It is also worth noting that the number of interventions should be
less than a threshold, which can be described as: |I|≤𝑛.
Figure 7: Evaluation of the interventional family recovery.
7 CONCLUSION
In this study, we have investigated the Granger causal structure
learning task, incorporating heterogeneous interventional time se-
ries data. To address the issue of identifying Granger non-causality
in interventional time series data with unknown targets, we have
introduced a novel condition that ensures the recovery of these
unknown targets and the accurate identification of the true causal
structure within the (I,D)-Markov Equivalence Class. We solved
the identifiability issues for accurately determining causal relation-
ships in Granger causality. Our theoretical analysis is supported
by empirical results, demonstrating that our proposed Interven-
tional Granger Causal (IGC ) structure learning method outperforms
existing methodologies in both synthetic and real-world datasets,
even in the absence of interventional target information. Poten-
tial avenues for future research include applying our method to
a broader spectrum of time series applications, which includes
detecting anomalies and root cause analysis within time series data.
ACKNOWLEDGEMENTS
This work was supported in part by the U.S. National Science Foun-
dation (NSF) grants SHF-2215573, and by the U.S. Department of
Engergy (DOE) Office of Science, Advanced Scientific Computing
Research (ASCR) under Awards B&R# KJ0403010/FWP#CC132 and
FWP#CC138. Portions of this research were conducted with the
advanced computing resources provided by Texas A&M High Per-
formance Research Computing.
4415Learning Flexible Time-windowed Granger Causality Integrating Heterogeneous Interventional Time Series Data KDD ’24, August 25–29, 2024, Barcelona, Spain
REFERENCES
[1]Andrew Arnold, Yan Liu, and Naoki Abe. 2007. Temporal causal modeling with
graphical granger methods. In ACM SIGKDD Conference on Knowledge Discovery
and Data Mining. 66–75.
[2]Philippe Brouillard, Sébastien Lachapelle, Alexandre Lacoste, Simon Lacoste-
Julien, and Alexandre Drouin. 2020. Differentiable causal discovery from in-
terventional data. Advances in Neural Information Processing Systems 33 (2020),
21865–21877.
[3]Chen Chen, Min Ren, Min Zhang, and Dabao Zhang. 2018. A two-stage penalized
least squares method for constructing large systems of structural equations.
Journal of Machine Learning Research 19, 1 (2018), 40–73.
[4]Yuxiao Cheng, Lianglong Li, Tingxiong Xiao, Zongren Li, Jinli Suo, Kunlun He,
and Qionghai Dai. 2024. CUTS+: High-dimensional causal discovery from irreg-
ular time-series. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 38. 11525–11533.
[5]Yuxiao Cheng, Runzhao Yang, Tingxiong Xiao, Zongren Li, Jinli Suo, Kunlun He,
and Qionghai Dai. 2023. Cuts: Neural causal discovery from irregular time-series
data. arXiv preprint arXiv:2302.07458 (2023).
[6]Yunfei Chu, Xiaowei Wang, Jianxin Ma, Kunyang Jia, Jingren Zhou, and Hongxia
Yang. 2020. Inductive granger causal modeling for multivariate time series. In
IEEE International Conference on Data Mining. IEEE, 972–977.
[7]James J Downs and Ernest F Vogel. 1993. A plant-wide industrial process control
problem. Computers & Chemical Engineering 17, 3 (1993), 245–255.
[8]Daniel Eaton and Kevin Murphy. 2007. Exact Bayesian structure learning from
uncertain interventions. In Artificial Intelligence and Statistics. PMLR, 107–114.
[9]Frederick Eberhardt. 2008. Almost optimal intervention sets for causal discovery.
Conference on Uncertainty in Artificial Intelligence.
[10] Frederick Eberhardt, Clark Glymour, and Richard Scheines. 2005. On the number
of experiments sufficient and in the worst case necessary to identify all causal
relations among n variables. Conference on Uncertainty in Artificial Intelligence.
[11] Frederick Eberhardt and Richard Scheines. 2007. Interventions and causal infer-
ence. Philosophy of Science 74, 5 (2007), 981–995.
[12] Tian Gao, Debarun Bhattacharjya, Elliot Nelson, Miao Liu, and Yue Yu. 2022.
IDYNO: Learning nonparametric DAGs from interventional dynamic data. In
International Conference on Machine Learning. PMLR, 6988–7001.
[13] AmirEmad Ghassami, Negar Kiyavash, Biwei Huang, and Kun Zhang. 2018.
Multi-domain causal structure learning in linear systems. Advances in Neural
Information Processing Systems 31 (2018).
[14] Chang Gong, Di Yao, Chuzhe Zhang, Wenbin Li, Jingping Bi, Lun Du, and Jin
Wang. 2023. Causal Discovery from Temporal Data. In ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 5803–5804.
[15] Clive WJ Granger. 1969. Investigating causal relations by econometric models
and cross-spectral methods. Econometrica: Journal of the Econometric Society
(1969), 424–438.
[16] Clive WJ Granger. 1980. Testing for causality: A personal viewpoint. Journal of
Economic Dynamics and Control 2 (1980), 329–352.
[17] Alain Hauser and Peter Bühlmann. 2012. Characterization and greedy learning
of interventional Markov equivalence classes of directed acyclic graphs. Journal
of Machine Learning Research 13, 1 (2012), 2409–2464.
[18] Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey, Ruben Sanchez-Romero,
Clark Glymour, and Bernhard Schölkopf. 2020. Causal discovery from hetero-
geneous/nonstationary data. Journal of Machine Learning Research 21, 1 (2020),
3482–3534.
[19] Iris AM Huijben, Arthur Andreas Nijdam, Sebastiaan Overeem, Merel M Van Gilst,
and Ruud Van Sloun. 2023. Som-cpc: Unsupervised contrastive learning with
self-organizing maps for structured representations of high-rate time series. In
International Conference on Machine Learning. PMLR, 14132–14152.
[20] Alireza Karimi and Mark R Paul. 2010. Extensive chaos in the Lorenz-96 model.
Chaos: An Interdisciplinary Journal of Nonlinear Science 20, 4 (2010).
[21] Saurabh Khanna and Vincent YF Tan. 2019. Economy Statistical Recurrent
Units For Inferring Nonlinear Granger Causality. In International Conference on
Learning Representations.
[22] Kevin B Korb, Lucas R Hope, Ann E Nicholson, and Karl Axnick. 2004. Vari-
eties of causal intervention. In Pacific Rim International Conference on Artificial
Intelligence. Springer, 322–331.
[23] Hongming Li, Shujian Yu, and Jose Principe. 2023. Causal recurrent variational
autoencoder for medical time series generation. In Proceedings of the AAAI Con-
ference on Artificial Intelligence.
[24] Chenxi Liu and Kun Kuang. 2023. Causal structure learning for latent intervened
non-stationary data. In International Conference on Machine Learning. PMLR,
21756–21777.
[25] Edward N Lorenz. 1996. Predictability: A problem partly solved. In Proc. Seminar
on Predictability, Vol. 1. Reading.
[26] Qianli Ma, Sen Li, and Garrison W Cottrell. 2020. Adversarial joint-learning recur-
rent neural network for incomplete time series classification. IEEE Transactions
on Pattern Analysis and Machine Intelligence 44, 4 (2020), 1765–1776.[27] Qianli Ma, Jiawei Zheng, Sen Li, and Gary W Cottrell. 2019. Learning repre-
sentations for time series clustering. Advances in Neural Information Processing
Systems 32 (2019).
[28] Ričards Marcinkevičs and Julia E Vogt. 2020. Interpretable Models for Granger
Causality Using Self-explaining Neural Networks. In International Conference on
Learning Representations.
[29] Xinyi Ni and Lifeng Lai. 2022. Policy gradient based entropic-var optimization
in risk-sensitive reinforcement learning. In 2022 58th Annual Allerton Conference
on Communication, Control, and Computing (Allerton). IEEE, 1–6.
[30] Xinyi Ni and Lifeng Lai. 2022. Risk-sensitive reinforcement learning via Entropic-
VaR optimization. In 2022 56th Asilomar Conference on Signals, Systems, and
Computers. IEEE, 953–959.
[31] Roxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer,
Konstantinos Georgatzis, Paul Beaumont, and Bryon Aragam. 2020. Dynotears:
Structure learning from time-series data. In International Conference on Artificial
Intelligence and Statistics. PMLR, 1595–1605.
[32] Neal Parikh, Stephen Boyd, et al .2014. Proximal algorithms. Foundations and
Trends in Optimization 1, 3 (2014), 127–239.
[33] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2013. Causal inference
on time series using restricted structural equation models. Advances in Neural
Information Processing Systems 26 (2013).
[34] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2017. Elements of causal
inference: foundations and learning algorithms. The MIT Press.
[35] Huida Qiu, Yan Liu, Niranjan A Subrahmanya, and Weichang Li. 2012. Granger
causality for time-series anomaly detection. In International Conference on Data
Mining. IEEE, 1074–1079.
[36] Shaogang Ren and Ping Li. 2022. Flow-based perturbation for cause-effect infer-
ence. In Proceedings of the 31st ACM International Conference on Information &
Knowledge Management. 1706–1715.
[37] Shaogang Ren, Haiyan Yin, Mingming Sun, and Ping Li. 2021. Causal discov-
ery with flow-based conditional density estimation. In 2021 IEEE International
Conference on Data Mining (ICDM). IEEE, 1300–1305.
[38] Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdi-
novic. 2019. Detecting and quantifying causal associations in large nonlinear
time series datasets. Science Advances 5, 11 (2019), eaau4996.
[39] Alex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, and Emily B Fox. 2021. Neural
granger causality. IEEE Transactions on Pattern Analysis and Machine Intelligence
44, 8 (2021), 4267–4279.
[40] Thomas S Verma and Judea Pearl. 2022. Equivalence and synthesis of causal
models. In Probabilistic and Causal Inference: The works of Judea Pearl. 221–236.
[41] E Vijay, Arindam Jati, Nam Nguyen, Gift Sinthong, and Jayant Kalagnanam. 2023.
TSMixer: lightweight MLP-mixer model for multivariate time series forecasting.
InACM SIGKDD Conference on Knowledge Discovery and Data Mining.
[42] Song Wei, Yao Xie, Christopher S Josef, and Rishikesan Kamaleswaran.
2023. Granger causal chain discovery for sepsis-associated derangements via
continuous-time Hawkes processes. In ACM SIGKDD Conference on Knowledge
Discovery and Data Mining. 2536–2546.
[43] Chenxiao Xu, Hao Huang, and Shinjae Yoo. 2019. Scalable causal graph learning
through a deep neural network. In ACM International Conference on Information
and Knowledge Management. 1853–1862.
[44] Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen. 2021. Voice2series: Re-
programming acoustic models for time series classification. In International
Conference on Machine Learning. PMLR, 11808–11819.
[45] Karren Yang, Abigail Katcoff, and Caroline Uhler. 2018. Characterizing and
learning equivalence classes of causal DAGs under interventions. In International
Conference on Machine Learning. PMLR, 5541–5550.
[46] Raneen Younis, Zahra Ahmadi, Abdul Hakmeh, and Marco Fisichella. 2023.
Flames2graph: An interpretable federated multivariate time series classifica-
tion framework. In ACM SIGKDD Conference on Knowledge Discovery and Data
Mining. 3140–3150.
[47] Ziyi Zhang, Diya Li, Zhenlei Song, Nick Duffield, and Zhe Zhang. 2023. Location-
Aware Social Network Recommendation via Temporal Graph Networks. In ACM
SIGSPATIAL Workshop on Location-based Recommendations, Geosocial Networks
and Geoadvertising. 58–61.
[48] Ziyi Zhang, Diya Li, Zhe Zhang, and Nicholas Duffield. 2021. A time-series
clustering algorithm for analyzing the changes of mobility pattern caused by
COVID-19. In Proceedings of the 1st ACM SIGSPATIAL International Workshop On
Animal Movement Ecology And Human Mobility. 13–17.
[49] Ziyi Zhang, Shaogang Ren, Xiaoning Qian, and Nick Duffield. 2024. Towards
Invariant Time Series Forecasting in Smart Cities. In Companion Proceedings of
the ACM on Web Conference 2024. 1344–1350.
[50] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. 2018. Dags
with no tears: Continuous optimization for structure learning. Advances in Neural
Information Processing Systems 31 (2018).
[51] Alex M Zimmer, Yihang K Pan, Theanuga Chandrapalan, Raymond WM Kwong,
and Steve F Perry. 2019. Loss-of-function approaches in comparative physiology:
is there a future for knockdown experiments in the era of genome editing?
Journal of Experimental Biology 222, 7 (2019), jeb175737.
4416KDD ’24, August 25–29, 2024, Barcelona, Spain Ziyi Zhang, Shaogang Ren, Xiaoning Qian, & Nick Duffield
A APPENDIX
A.1 Proximal Gradient Descent Updates
As for Equation (9), we establish the following definitions:
𝑔(𝜙):=𝑛∑︁
𝑘=1𝑇∑︁
𝑡=2𝑑∑︁
𝑖=1𝑑∑︁
𝑗=1||𝑥𝑒𝑘
𝑖,𝑡−F𝑖𝑗 𝑥𝑒𝑘
𝑗,𝑡−1:1;(W𝑖
:,𝑗,𝑒0+W𝑖
:,𝑗,𝑒𝑘)||2
2.
(18)
ℎ(𝜙):=(1−𝛼)𝜆𝑑∑︁
𝑖=1𝑑∑︁
𝑗=1||(W𝑖
:,𝑗,𝑒0,W𝑖
:,𝑗,𝑒1,...,W𝑖
:,𝑗,𝑒𝑘)||2
+𝛼𝜆𝑑∑︁
𝑖=1𝑑∑︁
𝑗=1𝑛∑︁
𝑘=1||W𝑖
:,𝑗,𝑒𝑘||2.(19)
A proximal mapping for the function ℎ(𝜙)can be defined as follows:
proxℎ(u)=argmin
z1
2||z−u||2
2+(1−𝛼)𝜆||z||2
+𝛼𝜆𝑛∑︁
𝑖=1||𝑧𝑖||2.(20)
For𝑘=0,1,...,𝑛 , the updating steps at the 𝑚-th iteration are
represented as:
W𝑖(𝑚)
:,𝑗,𝑒𝑘=proxℎ,𝑡𝑘(W𝑖(𝑚−1)
:,𝑗,𝑒𝑘−𝑡𝑘∇W𝑖
:,𝑗,𝑒𝑘𝑔(𝜙(𝑚−1))). (21)
Thus, u={𝑢0,𝑢1,...,𝑢𝑛}is a vector, and it is defined as:
𝑢𝑘=W𝑖(𝑚−1)
:,𝑗,𝑒𝑘−𝑡𝑘∇W𝑖
:,𝑗,𝑒𝑘𝑔(𝜙(𝑚−1)), (22)
and
W𝑖(𝑚)
:,𝑗,𝑒𝑘=proxℎ(𝑢𝑘).
First, let’s examine the scenario when z=0. According to the
Karush-Kuhn-Tucker (KKT) conditions, we obtain the following:
0∈𝑧0
𝑧1
...
𝑧𝑛−𝑢0
𝑢1
...
𝑢𝑛+(1−𝛼)𝜆𝑧0
||z||2𝑧1
||z||2
...
𝑧𝑛
||z||2+𝛼𝜆0
𝑧1
||𝑧1||2
...
𝑧𝑛
||𝑧𝑛||2. (23)
One could set z=0, while Equation (24) holds:
𝑢0
𝑢1
...
𝑢𝑛−𝛼𝜆0
𝑧1
||𝑧1||2
...
𝑧𝑛
||𝑧𝑛||2=(1−𝛼)𝜆𝑧0
||z||2𝑧1
||z||2
...
𝑧𝑛
||z||2. (24)
Identifying edge cases for uis straightforward as it involves an
element-wise comparison between uandz. Additionally, it is worth
noting that||z||2≤1, leading to the following considerations:
z=0⇔||u−𝛼𝜆0
𝑢1
||𝑢1||2
...
𝑢𝑛
||𝑢𝑛||2||2≤(1−𝛼)𝜆. (25)In the case z≠0, Equation (23) suggests:
𝑢0
𝑢1
...
𝑢𝑛−𝛼𝜆0
𝑧1
||𝑧1||2
...
𝑧𝑛
||𝑧𝑛||2=(1−𝛼)𝜆𝑧0
||z||2𝑧1
||z||2
...
𝑧𝑛
||z||2+𝑧0
𝑧1
...
𝑧𝑛. (26)
When considering elements in zthat are non-zero, their sign aligns
with the corresponding element in u. Now, let’s establish the fol-
lowing definition:
𝑆𝛼𝜆(u)=𝑢0
𝑢1−𝛼𝜆𝑢1
||𝑢1||2
...
𝑢𝑛−𝛼𝜆𝑢𝑛
||𝑢𝑛||2. (27)
An alternative representation of Equation (26) is achieved by trans-
forming it into:
𝑆𝛼𝜆(u)=(1+(1−𝛼)𝜆
||z||2)𝑧0
𝑧1
...
𝑧𝑛. (28)
If we apply the L2 norm to both sides as follows:
||𝑆𝛼𝜆(u)||2=(1+(1−𝛼)𝜆
||z||2)·||z||2
⇒||z||2=||𝑆𝛼𝜆(u)||2−(1−𝛼)𝜆.(29)
Upon substituting Equation (29) into Equation (28), we obtain:
z=(1−(1−𝛼)𝜆
||𝑆𝛼𝜆(u)||2)·𝑆𝛼𝜆(u) (30)
To summarize:
proxℎ(u)=(
0 if||𝑆𝛼𝜆||2≤(1−𝛼)𝜆
(1−(1−𝛼)𝜆
||𝑆𝛼𝜆||2)·𝑆𝛼𝜆(u)if||𝑆𝛼𝜆||2>(1−𝛼)𝜆
=(1−(1−𝛼)𝜆
max ||𝑆𝛼𝜆||2,(1−𝛼)𝜆)·𝑆𝛼𝜆(u)
(31)
A.2 Discussion and the Proof of Theorem 5.1.
The identifiability condition for the unrolled, temporally extended
DAG∈R𝑑×𝑇, which includes all variables across all time steps, has
been established in the work [12]. Specifically, it assumes that the
edges within the graph G∗remain constant over time. Furthermore,
it assumes that for any given window 𝑋∈R𝑑×𝑤, where𝑤repre-
sents the window’s width, the distribution P𝑋over the variables
within this window stays invariant across different timesteps. This
implies that the conditional distribution P(𝑥𝑖,𝑡|PA(𝑥𝑖,𝑡))for any
variable𝑥𝑖is independent of the time index 𝑡. The subset of all DAGs
that can be segmented in this manner into a directed sequence of
a repeating subgraph or window is defined as D𝑠. This segmen-
tation is based on the fact that repetition of the same conditional
distributions and edges over time corresponds to stationary or fixed
dynamics. The IGC methodology, notable for its time-windowed
framework, provides the flexibility to detect causal relationship
inW𝑖,𝑡
:,𝑗,𝑡′,𝑒0between any pair of variables (𝑥𝑖,𝑡,𝑥𝑗,𝑡′)with a given
time lag𝑝=𝑡′−𝑡across any DAG or within any time window.
When the assumptions outlined in Theorem 5.1. holds, Theorem
4417Learning Flexible Time-windowed Granger Causality Integrating Heterogeneous Interventional Time Series Data KDD ’24, August 25–29, 2024, Barcelona, Spain
1 from [ 2] becomes applicable, ensuring that our learned graph ˆG
isI-Markov equivalent to G∗. Additionally, given that ˆG∈D , by
invoking the Theorem 3.2 from [ 12],ˆGis(I,D)-Markov equiv-
alent toG∗. Since the true Granger causal structure is originally
derived fromG∗, by establishing ˆGis(I,D)-Markov equivalent to
G∗, we have addressed the challenges related to the identifiability
in Granger causality and mitigate the concerns associated with ac-
curately determining causal relationships within the framework of
Granger causality. Theorem 1 from [ 2] and Theorem 3.2 from [ 12]
operate under the implicit assumption that, for each intervention
𝑘, the ground truth interventional target I∗𝑒𝑘is precisely known.
This assumption, however, does not always available in real-world
scenarios. To address this discrepancy, we propose an extension to
Theorem 3.2 from [ 12] that accommodates unknown interventional
targets. In this context, as our proposed IGCmethod, the interven-
tional targetsIare learned in a manner similarly to how the graph
Gis determined. We are now ready to prove our Theorem 5.1.
Proof. Leveraging Theorem 2 from [ 2], we address scenarios
whereI≠I∗. The core concept of the proof is that S(G∗,I∗)>
S(G,I)wheneverG∉(I∗,D)-MEC(G∗) or whenI≠I∗. For
the sake of clarity, we define:
𝜂(G,I):=inf
𝜙∑︁
𝑘∈[𝐾]𝐷𝐾𝐿(𝑃𝑒𝑘||𝐹𝑒𝑘
GI𝜙). (32)
Lemma A.1. Let𝑖∈𝑉and𝐴⊂𝑉\{𝑖}, if(𝑝1,𝑝2)∉Z(𝑖,𝐴)and
both𝑝1and𝑝2are strictly positive, then:
inf
(𝑓1,𝑓2)∈Z(𝑖,𝐴))𝐷𝐾𝐿(𝑝1||𝑓1)+𝐷𝐾𝐿(𝑝2||𝑓2)>0.(33)
Case 1. LetIrepresent the set of all intervention sets Ifor which
there is at least one intervention 𝑘0∈[𝐾]and one variable 𝑖∈[𝑑]
such that𝑖is included in the true intervention set I∗
𝑘0but is not
included in I𝑘0. AssumingI∈Iand consideringGas an arbitrary
DAG, the principle of I∗-faithfulness implies that
𝑃𝑒0(𝑥𝑖|PA(𝑥𝑖))≠𝑃𝑒𝑘0(𝑥𝑖|PA(𝑥𝑖)). (34)
It also means(𝑃𝑒0,𝑃𝑒𝑘0)∉Z(𝑖,PA(𝑖)), where,
Z(𝑖,𝐴):={(𝑓1,𝑓2)|𝑓1(𝑥𝑖|𝑥𝐴)=𝑓2(𝑥𝑖|𝑥𝐴)and𝑓1,𝑓2>0}.
(35)
Given that𝑖∉I𝑘0, it follows from the definition provided in Equa-
tion (7) that, for all values of 𝜙,
𝐹𝑒0
GI𝜙(𝑥𝑖|PA(𝑥𝑖))=𝐹𝑒𝑘0
GI𝜙(𝑥𝑖|PA(𝑥𝑖))
i.e.(𝐹𝑒0
GI𝜙,𝐹𝑒𝑘0
GI𝜙)∈Z(𝑖,PA(𝑖)).(36)
The following holds and for all 𝜙we have(𝐹𝑒0,𝐹𝑒𝑘0)∈Z(𝑖, PA(𝑖))
due to Lemma A.1:
𝜂(G,I)≥ inf
𝜙𝐷𝐾𝐿(𝑃𝑒0||𝐹𝑒0
GI𝜙)+𝐷𝐾𝐿(𝑃𝑒𝑘0||𝐹𝑒𝑘0
GI𝜙)
≥ inf
(𝐹𝑒0,𝐹𝑒𝑘0)∈Z(𝑖,PA(𝑖))𝐷𝐾𝐿(𝑃𝑒0||𝐹𝑒0)+𝐷𝐾𝐿(𝑃𝑒𝑘0||𝐹𝑒𝑘0)
>0.
(37)
Formin{|G|−|G∗|,|I|−|I∗|}≥ 0, thenS(G∗,I∗)>S(G,I). Let
us define S:={(G,I)∈𝐷𝐴𝐺×I|min{|G|−|G∗|,|I|−|I∗|}<0}.
To prove thatS(G∗,I∗)−S(G,I)>0for all(G,I) ∈ S, weneed to choose 𝜆G,𝜆I>0small enough. Choosing 𝜆G+𝜆I<
min(G,I)∈S𝜂(G,I)
−min{|G|−|G∗|,|I|−|I∗|}since:
𝜆G+𝜆I<min
(G,I)∈S𝜂(G,I)
−min{|G|−|G∗|,|I|−|I∗|}
⇔𝜆G+𝜆I<𝜂(G,I)
−min{|G|−|G∗|,|I|−|I∗|};∀(G,I)∈S
⇔−(𝜆G+𝜆I)min{|G|−|G∗|,|I|−|I∗|}<𝜂(G,I)
⇔0<𝜂(G,I)+(𝜆G+𝜆I)min{|G|−|G∗|,|I|−|I∗|},(38)
then we have:
0<𝜂(G,I)+(𝜆G+𝜆I)min{|G|−|G∗|,|I|−|I∗|};∀(G,I)∈S
≤𝜂(G,I)+𝜆G(|G|−|G∗|)+𝜆I(|I|−|I∗|)
=S(G∗,I∗)−S(G,I).
(39)
From this point forward, we can assume that I∗
𝑘⊂I𝑘for all𝑘∈
[𝐾], and this assumption is valid because any deviation from this
condition would fall under Case 1.
Lemma A.2. Given the assumptions outlined in Theorem 5.1:
S(G∗,I∗)=inf
𝜙∑︁
𝑘∈[𝐾]𝐷𝐾𝐿(𝑃𝑒𝑘||𝐹𝑒𝑘
GI𝜙)
+𝜆G(|G|−|G∗|)+𝜆I(|I|−|I∗|).(40)
Case 2. LetI:={I|I∗
𝑘⊂I𝑘∀𝑘}and[∃(𝑘 0,𝑖)s.t.𝑖∈I𝑘0and𝑖∉
I∗
𝑘0]}. Given thatI∈IandGis a DAG, it becomes evident that
|I|>|I∗|. If|G|≥|G∗|, thenS(G∗,I∗)−S(G,I)>0by Lemma
A.2. Define a set S:={(G,I)∈𝐷𝐴𝐺×I||G|<|G∗|}. To prove that
S(G∗,I∗)−S(G,I)>0for all(G,I)∈S, we need to choose 𝜆G
small enough. Choosing 𝜆G<min(G,I)∈S𝜂(G,I)+𝜆I(|I|−|I∗|)
|G|−|G∗|
since:
𝜆G<𝜂(G,I)+𝜆I(|I|−|I∗|)
|G|−|G∗|;∀(G,I)∈S
⇔𝜆G(|G|−|G∗|)<𝜂(G,I)+𝜆I(|I|−|I∗|)
⇔0<𝜂(G,I)+𝜆G(|G|−|G∗|)+𝜆I(|I|−|I∗|),(41)
then we have:
0<𝜂(G,I)+𝜆G(|G|−|G∗|)+𝜆I(|I|−|I∗|);∀(G,I)∈S
=S(G∗,I∗)−S(G,I).
(42)
In the scenarios described by Case 1 and Case 2, all instances where
I≠I∗are accounted for. Consequently, this leads to the conclusion
thatImust be equal toI∗. By noting thatS(G∗,I∗)−S(G,I)=
SI∗(G∗)−SI∗(G), we can employ the same steps as [ 12] to prove
that ˆG∈(I∗,D)− MEC(G∗). □
4418