Performative Debias with Fair-exposure Optimization Driven by
Strategic Agents in Recommender Systems
Zhichen Xiang‚àó
College of Management and
Economics & Laboratory of
Computation and Analytics of
Complex Management Systems
(CACMS), Tianjin University
Tianjin, China
xiangzc@tju.edu.cnHongke Zhao‚Ä†
College of Management and
Economics & Laboratory of
Computation and Analytics of
Complex Management Systems
(CACMS), Tianjin University
Tianjin, China
hongke@tju.edu.cnChuang Zhao
The Hong Kong University of Science
and Technology
Hong Kong, China
czhaobo@connect.ust.hk
Ming He‚Ä†
AI Lab at Lenovo Research
Beijing, China
heming01@foxmail.comJianping Fan
AI Lab at Lenovo Research
Beijing, China
jfan1@lenovo.com
ABSTRACT
Data bias, e.g., popularity impairs the dynamics of two-sided mar-
kets within recommender systems. This overshadows the less vis-
ible but potentially intriguing long-tail items that could capture
user interest. Despite the abundance of research surrounding this
issue, it still poses challenges and remains a hot topic in academic
circles. Along this line, in this paper, we developed a re-ranking ap-
proach in dynamic settings with fair-exposure optimization driven
by strategic agents. Designed for the producer side, the execution
of agents assumes content creators can modify item features based
on strategic incentives to maximize their exposure. This iterative
process entails an end-to-end optimization, employing differen-
tiable ranking operators that simultaneously target accuracy and
fairness. Joint objectives ensure the performance of recommenda-
tions while enhancing the visibility of tail items. We also leveraged
the performativity nature of predictions to illustrate how strate-
gic learning influences content creators to shift towards fairness
efficiently, thereby incentivizing features of tail items. Through
comprehensive experiments on both public and industrial datasets,
we have substantiated the effectiveness and dominance of the pro-
posed method especially on unveiling the potential of tail items.
CCS CONCEPTS
‚Ä¢Information systems ‚ÜíRecommender systems; ‚Ä¢Comput-
ing methodologies ‚ÜíMachine learning .
‚àóThis work was done when Zhichen Xiang was doing an internship at AI Lab of Lenovo
Research.
‚Ä†Corresponding Authors
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671786KEYWORDS
Recommender Systems, Popularity Bias, Strategic Learning, Perfor-
mative Prediction
ACM Reference Format:
Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, and Jianping Fan.
2024. Performative Debias with Fair-exposure Optimization Driven by
Strategic Agents in Recommender Systems. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ‚Äô24),
August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3637528.3671786
1 INTRODUCTION
Recommender systems play a pivotal role in curating high-quality,
personalized content in two-sided markets, encompassing both cus-
tomers and producers [ 32,33]. Algorithms are designed to bridge
the gap between consumer desires and the vast array of available
products. Considering sustainability, it is critical to build a fairer
two-sided platform that understands and optimizes the dynamics
of human-algorithm interactions [ 5,35,54,55], as well as identify-
ing and alleviating bias in dynamic systems with human feedback
loops [ 23,28,40,42]. However, it‚Äôs known that popularity bias
would lead to unfair exposure for tail items[ 10,45,52]. This bias
occurs as popular items receive disproportionate recommendations,
regardless of their quality. That causes a self-reinforcing cycle ex-
acerbating data imbalance in the future, leading to the ‚ÄúMatthew
effect‚Äù, where the rich get richer and the poor get poorer[55].
The prevalent research concerning re-ranking for addressing
popularity bias involves implementing post-hoc algorithms to ad-
just the outputs of conventionally trained ranking models, thereby
shifting focus towards tail items. This includes approaches such
as enhancing the visibility of less popular items [ 2,3], and em-
ploying diversification techniques, such as traversing the list of
ranked items to eliminate those that are similar to items ranked
higher [ 8,39]. However, recommendations are inherently dynamic,
and these existing post-hoc approaches may not adequately im-
prove the intrinsic utility of long-tail items over the long term. Fur-
thermore, they overlook the operational reality of recommendation
3507
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, & Jianping Fan
Original top-k list
 Strategic top-k list
Figure 1: Function of the strategic agent: improve the per-
formance of tail items in top-k recommendation via feature
incentives from users‚Äô candidate lists.
systems within two-sided markets. Moreover, consumer fairness is
mainly about addressing disparities within user groups based on
sensitive attributes on platforms [ 11,24,27,31,49]. In contrast, our
study is dedicated to alleviating the disparities in item exposure
level that arise due to systemic popularity biases. Encouraging the
producer to make efforts, we aim to unearth the beneficial features
of tail items during dynamics in two-sided markets.
Based on this, we have designed a ranking policy updating based
on the strategy learning agent[ 19] executed by producers to address
the disadvantages of tail items. In order to formalize the endeavor
of producers in two-sided markets, we assume that items on the
platform are shaped by content creators [ 4,20]. The goal of each
creator is to maximize their own exposure. In addition, it can mod-
ify item characteristics based on incentives of strategy agents to
achieve maximum utility that meets its audience. In order to better
utilize the content of items, we extract as many semantic features
as possible and focus the ranking algorithm on the inner product of
user representations and item features. Figure 1 shows the intuitive
role of the strategic agent.
To guide content creators in updating, we propose dual opti-
mization objectives, that ensure both accuracy and fair exposure
in top-ùëòlists for users. Recognizing the challenge presented by
user profiles shifting towards popular items [ 9], which may lead to
content homogenization, we seek to balance two factors effectively.
However, the non-differentiable nature of these metrics makes gra-
dient descent algorithms unsuitable for joint optimization. To solve
this challenge, differentiable ranking operators are designed to in-
tegrate them into the process of end-to-end training, and our goal
is to explore their potential relationship and suggest methods for
optimally balancing this trade-off.
Upon completing the development of the differentiable opera-
tors, we investigate the system‚Äôs effective utilization of evolution
power by leveraging performative prediction [ 34]. This concept en-
compasses strategic learning in a temporal setting, where ongoing
retraining and learning lead to shifts in the underlying data over
time. Specifically, by implementing the execution rule of strategic
agents, we enhance the fair-exposure through anticipative regu-
larization. Updated representations are employed to calculate the
fairness metrics for the forthcoming epoch, which informs the opti-
mization process in the current epoch. Consequently, that enables
the model to benefit from more effective guidance of achieving
optimization oriented towards fairness.In each round of optimization, the objective focused on fair ex-
posure enables the user representations trained in the current time
to include more information about tail items (those with lower rele-
vance) of the candidate list. Responding to the evolving representa-
tions of users, content creators strategically modify item features to
meet the optimal utility for their target users in the following itera-
tion. These multi-round iterative optimizations enable the strategic
agent to execute multiple times and fully incentivize the features of
tail items in top- ùëòlists. This strategic learning mechanism capital-
izes on the performativity of the anticipated user representations,
to infuse relevant feature incentives concerning fairness into items
across various popularity levels for debias efficiently. By doing so,
producers are able to identify and leverage the beneficial features
of tail items, thereby fostering the sustainable growth of two-sided
markets in recommendation.
In summary, this article makes the following contributions:
‚Ä¢This paper addresses the issue of popularity bias in recommender
systems from the perspective of producers‚Äô strategic behavior
within two-sided markets during the re-ranking stage. We lever-
age the performativity of prediction towards fair-exposure opti-
mization for content creators, which efficiently mines the poten-
tial of long-tail items for users.
‚Ä¢We have developed a differentiable operator based on the fair-
ness of item exposure. This operator allows for end-to-end dual-
objective optimization, balancing both the accuracy and fairness
of the recommendation results.
‚Ä¢Extensive experiments have been conducted on both public and
industrial datasets. The results validate the effectiveness and
rationality of the proposed methods.
2 RELATED WORK
The two most relevant research areas of this paper are popularity
bias in recommendation and performative prediction. We introduce
the related research work from these two aspects respectively.
2.1 Popularity Bias in Recommendation
At present, the relevant research on popularity bias in recommen-
dation systems can be divided into two categories, one is static data
debias, and the other is debias in dynamic scenarios [10].
From the perspective of the static data, classical methods to
deal with popularity bias are using the Inverse Propensity Score
(IPS) [ 25,38] and casual model [ 16,45,50,51,53]. Besides, the
learning-to-rank algorithm based on regularization [ 1] is also intro-
duced to enhance the long-tail coverage of recommendation lists.
For specific scenarios, [ 12]utilized the contextual information of
tail items to enhance their representation in sequential recommen-
dations, thereby promoting their performance.
As for the dynamic scenario, the behavior of user engagement is
a valuable signal for both the user and the platform where the algo-
rithm is deployed [ 5,31,42], enabling them both to be connected
into complex feedback loops. [ 9]analyzed the feedback loop be-
tween user behavior and algorithmic recommendation system, and
revealed how algorithm-based confounding factors in recommenda-
tion system increase the homogeneity of user behavior and reduce
the utility. Empirical research is also carried out through simulation
experiments [ 55], and the inherent imbalance of audience size and
3508Performative Debias with Fair-exposure Optimization Driven by
Strategic Agents in Recommender Systems KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
model bias in dynamic scenes are analyzed as the main sources of
popularity bias. Overall, the current research about dynamic debias
predominantly focuses on simulating human behavior, with scant
attention given to analyzing the endeavors of producers.
2.2 Strategic Learning and Performativity
Strategic learning [ 19] requires the model to be able to identify and
adapt to participants‚Äô strategic behavioral changes, thus creating
a feedback loop. These forecasts are notably characterized by the
model‚Äôs adaptability to alterations in the environment instigated
by its own predictions. In this context, forward-looking regulariza-
tion [37] and causal identifiability [29] are proposed, encouraging
predictive models to induce changes that also improve outcomes
by predicting user actions.
Performative prediction [ 34] refers to the fact that in the predic-
tion process, the prediction result itself will affect the occurrence of
the predicted event, and the output of the prediction model will, in
turn, affect its input data distribution. This is called ‚Äúpredicting from
predictions‚Äù. Existing studies provide sufficient global conditions
for retraining convergence [ 34] or propose general optimization
algorithms [ 6,30], which are set up in a way that allows complex
human interactions in loops [ 43]. The performative recommenda-
tion is firstly proposed to diversify content creators [ 15]. However,
it lacked evidence from real-world data and paid less attention to
the exposure of items with different popularity. In addition, some
studies [ 20,21,26] also introduce strategy adaptation into systems
built by content creators, which capture competition between par-
ticipants and formally exploring the strategic behavior of producers
in the supply-side.
3 PROBLEM FORMALIZATION
Our problem is set in the top- ùëòrecommendation during re-ranking
stage, which considers a platform consisting of a group of users
U={ùíñùëñ‚ààRùëë}ùëö
ùëñ=1and a set of itemsX={ùíôùëó‚ààRùëë,||ùíô||2=1}ùëõ
ùëó=1.
Table 1 provides a list of symbols used in our paper. Items are
described by feature vectors ùíôùëó‚ààRùëëandùëërepresents the number
of semantic features. The system makes recommendations in the
candidate listCùëñ‚äÜX of individual ùëñby the ranking operator ùúã:
ùúãùëñ=rank(ùúéùëñ(ùíô1),ùúéùëñ(ùíô2),...,ùúéùëñ(ùíôùëê)),‚àÄùëñ‚àà[ùëö]. (1)
It ranks items for each user ùëñusing a personalized score function
ùúéùëñ(ùíô)=ùúé(ùíô;ùíñùëñ)=ùíñ‚ä§
ùëñùíô, which is capable of predicting the prefer-
ence of a user over candidate items with the amount of ùëê. Especially,
the linear scoring function relies on user representation vectors
ùíñùëñ‚ààRùëëfor each user ùëñ‚àà [ùëö]. Overall, the goal of the system
is to learn good ùúéfrom data for re-ranking, where ùíñ1,...,ùíñùëöare
representations learned from the joint objective which is towards
our debias setting for every user.
4 METHODOLOGY
In this section, we first detail key concepts and metrics in Section 4.1.
This is followed by a discussion on how solutions have been tailored
to address the non-differentiable nature of optimization metrics
in Section 4.2. Next, Section 4.3 introduces the execution rule of
the strategic agent. Finally, we formalize the process of dynamic
learning and optimization in Section 4.4.Table 1: Symbols used in this paper.
Symbol Description
X Semantic features of item set
U Representation of user set
U‚àóGround-truth of user preference
C Users‚Äô candidate item list of recommendation
ùúé Scoring function for ranking
ùúã Ranking operator towards recommendation
S Content-based relevance simulator
ùëü Ground-truth of relevance generated by S
ùëì Execution rules for strategic agent
ùëë Number of features of an item
ùëò Number of items in the recommendation list
ùëê Number of items in the candidate list
ùëá Number of retraining rounds in dynamics
ùë∑ Permutation matrix for ranking
bùë∑ Relaxed permutation matrix
ùúè Temperature parameter for bùë∑
ùõº Scaling parameter of modification cost
ùúÜ Regularization parameter for fair exposure
4.1 Preliminaries
In this section, we delineate the critical metrics for optimization
and provide an overview of the dynamic training framework.
‚Ä¢User Utility .In the context of evaluating ranking performance
for a recommendation list, we define a list of items with their
corresponding relevance scores for user ùëñasùíìùëñ=[ùëü1,ùëü2...,ùëüùëê]. To
represent different utilities for users over these items, we use rank-
ing operator ùùÖ=(ùúã1,ùúã2,...ùúãùëö), whereùúã(‚Ñì)specifies the index in ùëü
of the item that is ranked by ùùÖat position‚Ñì. Then, the Discounted
Cumulative Gain (DCG) at ùëò-th position ( ùëò‚â§ùëê) is defined as:
DCG @ùëò(ùíìùëñ,ùúãùëñ)=ùëò‚àëÔ∏Å
‚Ñì=12ùëüùùÖ(‚Ñì)‚àí1
log(1+‚Ñì). (2)
Here, the numerator of terms captures the ‚Äúgain‚Äù from the rele-
vance of the item at position ‚Ñì. Conversely, the denominator serves
to ‚Äúdiscount‚Äù the value of this gain based on the rank ‚Ñìreflecting the
principle that items ranked lower contribute less to the cumulative
gain due to the logarithmic discounting. Then, the definition of the
utility of users‚Äô recommendation list is as Normalised Discounted
Cumulative Gain (NDCG):
NDCG @ùëò(ùíìùëñ,ùúãùëñ)=1
maxDCG @ùëò(ùíìùëñ,ùúãùëñ)DCG @ùëò(ùíìùëñ,ùúãùëñ),(3)
whereùëöùëéùë•DCG @ùëòis the maximum possible value of DCG @ùíå, com-
puted by the decreasing ground-truth relevance.
‚Ä¢Item Exposure .We measure the inequality of item exposure
in the recommendation list by the degree of imbalance between
different relevance of items evaluated on the top- ùëòitems. Specif-
ically, we refer to the definition of Gini coefficient [ 14,17,44] in
economics to measure the differences in exposure levels of different
items. Formally, let top- ùëòrelevance ùíìùúã(ranked byùúãùëñ) sorted in an
3509KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, & Jianping Fan
......User
RepresentationsHead items
Medium items
Tail itemsStrategic Agent
Execution
Accuracy
Fairness
Joint
Optimization...
.........
Top-k
list......Item Features
Ranking
OperatorFeature incentives
Traning
resultPerformativity
Figure 2: The framework of our re-ranking approach based on the strategic agent.
ascending sequence, i.e., ùëü1ùúã‚â§ùëü2ùúã‚â§...‚â§ùëüùëòùúã. Then,
Gini @ùëò(ùíìùëñ,ùúãùëñ)=√çùëò
‚Ñì=1(2‚Ñì‚àíùëò‚àí1)ùëü‚Ñìùúã
ùëò√çùëò
‚Ñì=1ùëü‚Ñìùúã. (4)
The range of the Gini coefficient is between 0and1. The smaller
ùê∫ùëñùëõùëñ @ùíå(ùíìùëñ,ùúãùëñ), the smaller the difference in relevance scores in
the recommendation list, and vice versa. From the results of rec-
ommendations, there is a significant difference in the relevance
score between tail items and head items. Our goal is to increase the
exposure of tail products as much as possible while ensuring the
performance of user recommendations. In other words, the Gini
coefficient of the relevance score in the recommendation list should
be relatively high, which can ensure that tail items have more fair
exposure among user candidates.
‚Ä¢Dynamic Environment .Our study examines how learning
with optimization influences user utility and item exposure across
iterative rounds of retraining. Each iteration, denoted as the ùë°-th
round, is characterized by data Dùë°=(Uùë°,Xùë°,C,U‚àó). During
training, the simulator S(Xùë°;U‚àó)is employed to generate the rel-
evanceùëübetween user preferences and item features, serving as a
supervisory signal. Post each training iteration, content creators im-
plement strategic modification for item features Xùë°+1=ùëì(Xùë°;Uùë°)
based on the trained representations of users. Then, the subsequent
training iteration utilizes newly collected data and gets Dùë°+1=
(Uùë°+1,Xùë°+1,C,U‚àó). The dynamic framework is shown as Figure 2.
More details are provided in the following sections.
4.2 Differentiable Ranking
In this section, we propose differentiable ranking operators for
measuring the user utility and the item exposure in the end-to-end
learning framework for optimization.
Given a relevance list between a user and their candidate items,
ùíì=[ùëü1,ùëü2,...,ùëüùëê]‚ä§, our goal is to rank the items in the entire can-
didate set and calculate the NDCG and Gini values of the top- ùëò
recommended item lists. In the re-ranking stage, the ranking oper-
atorùúãùëñused to generate the final list can be depicted as a permu-
tation matrix, which makes ranking concerning NDCG and Ginicoefficients non-differentiable. However, we can overcome this lim-
itation by using the continuous relaxation method of permutation
matrices [ 18], using differentiable and continuous operators to ap-
proximate deterministic ranking operations. First, the deterministic
permutation matrix ùíëùíì‚ààRùëê√óùëêdefined by [18] is:
ùë∑ùíì[ùëù,ùëû]=(
1ùëñùëì ùëû=arg max[(ùëê+1‚àí2ùëù)ùíì‚àíùë®ùíä 1]
0ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí,(5)
whereùëùandùëûrepresent the rows and columns of the permutation
matrix, 1denotes the column vector of all ones and ùë®ùíä[ùëù,ùëû]=
|ùëüùëù‚àíùëüùëû|which is the matrix of absolute pairwise differences of the
elements in the list. Then, the arg max operator can be replaced by
softmax[22] to obtain a continuous relaxation:
bùë∑ùíì[ùëù,:](ùúè)=softmax[((ùëê+1‚àí2ùëù)ùíì‚àíùë®ùíä 1/ùúè], (6)
whereùúè>0is a temperature parameter, serving as the level of
smoothness in the approximation. A lower value of ùúènudges the
relaxed permutation matrix towards a more deterministic nature.
This relaxation is continuous and differentiable in relation to the el-
ements of ùíì, ensuring a balance between computational tractability
and the fidelity of the approximation.
Following [ 36], we implemented Sinkhorn scaling [ 41] to refine
the permutation matrix to get ùë†ùëêùëéùëôùëí(bùë∑ùíì), ensuring that both rows
and columns become probabilistically distributed with a total of one.
This approach effectively diminishes the gradient variance around
the discontinuity points of the optimization metrics, thereby facili-
tating the efficiency of the gradient descent method in optimization.
Utilizing the ranking scores derived from the ùúãalong with the
ground-truth relevance ùíì, we employ ùë†ùëêùëéùëôùëí(bùë∑ùíì)to ascertain the
approximate values of the NDCG and the Gini coefficient for the
current recommendation list. This process facilitates the creation of
a differentiable ranking operator. In terms of user utility, we adopt
the approach as delineated by [36], which yields:
DR-NDCG @ùëò(ùíì,ùúã)(ùúè1)
=(ùëò‚àëÔ∏Å
‚Ñì=1ùëî(ùëü(‚Ñì))¬∑ùëë(‚Ñì))‚àí1¬∑ùëò‚àëÔ∏Å
‚Ñì=1[scale(bùë∑)¬∑ùëî(ùíì)]‚Ñì¬∑ùëë(‚Ñì),(7)
3510Performative Debias with Fair-exposure Optimization Driven by
Strategic Agents in Recommender Systems KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
whereùëî(ùë•)=2ùë•‚àí1andùëë(ùë•)=1
log2(ùë•+1).‚Ñìis the index of the
ranking position. The notation ùëü(‚Ñì)represents the relevance score
of the‚Ñì-th item after sorting ùíìin descending order.
In terms of item exposure, we have developed a new differen-
tiable ranking operator for inequality measure. That makes it feasi-
ble for gradient-based optimization in the following section. For-
mally, let ùíìùúãdenote the ground-truth relevance of items the in
recommendation list. Then:
DR-Gini @ùëò(ùíì,ùúã)(ùúè2)
=1
¬Øùíìùúãùëò2ùëò‚àëÔ∏Å
ùëù=1ùëò‚àëÔ∏Å
ùëû=1|[scale(bùë∑)¬∑ùíì]ùëù‚àí[scale(bùë∑)¬∑ùíì]ùëû|,(8)
where ¬Øùíìùúã=1
ùëò√çùëò
ùëñ=1ùëüùëñùúã. To facilitate the formula inspired by pairwise
ranking and analysis [ 7,13,47,48], we use variant formulas based
on Eq.(4) to describe the difference between pairwise elements. This
can also better reflect the connotation of the Gini coefficient index
we proposed. It can measure the unfairness of exposure caused by
relevance discrepancies in the candidate list.
4.3 Strategic Agent
Considering the distinct popularity levels of items, the number of
audiences each item possesses also differs. Initially, we define the
utility function of an item ùë†(ùë•ùëó)based on the average score of its
rankings within the target demographic for item ùë•ùëó. We denoteUùëó
as the set of users whose candidate list contains item ùë•ùëó. Then,
ùë†(ùë•ùëó)=ÀÜùë§‚ä§
ùëóùë•ùëó=1
|Uùëó|‚àëÔ∏Å
ùë¢‚ààUùëóùúé(ùë•ùëó;ùë¢). (9)
In order to compare the different sum quantity of user represen-
tations within the same range, a norm constraint is necessary for
them [ 15,20]. Therefore, we normalize the ùëô2norm of the summa-
tion of user representations specific to item ùë•ùëó, denoted as
ÀÜùë§ùëó=ùë§ùëó
||ùë§ùëó||2,whereùë§ùëó=1
|Uùëó|‚àëÔ∏Å
ùë¢‚ààUùëóùë¢.
Then, we define the execution rules of the strategy agent in
systems. In alignment with the strategic classification framework
established by [ 19], we posit that content creators have the ability to
modify their features of items, though this incurs a cost, as a reaction
to the established predictive model. Considering a predefined cost
functionùëêùëúùë†ùë°(ùë•ùëó,ùë•‚Ä≤), these creators adjust their items following an
optimal response strategy:
Œîùëì(ùë•ùëó)=argmax
ùë•‚Ä≤:‚à•ùë•‚Ä≤‚à•2=1
ùë†(ùë•‚Ä≤)‚àíùõº¬∑ùëêùëúùë†ùë°(ùë•ùëó,ùë•‚Ä≤)
, (10)
whereùëêùëúùë†ùë°(ùë•,ùë•‚Ä≤)=‚à•ùë•‚àíùë•‚Ä≤‚à•2
2denotes the cost of modification. It
measures the degree of deviation after execution, whose scaling
parameter is ùõº‚â•0. The smaller ùõºillustrates that the strategic
agent employs the more aggressive policy for modification. This
results in item features deviating more significantly from their
original semantic information and vice versa. The implementation
of normalization ensures that modified items maintain their unit l2
norm, preserving their dimensional integrity.
In Eq.(10), the argmax function is non-differentiable, and the
termŒîùëìrelies on the optimized representations concerning bothaccuracy and fairness objectives. That presents challenges for the
end-to-end implementation of the strategic agent.
To capitalize on the performativity of prediction, we have de-
rived a differentiable formulation in terms of Œîùëì. This formulation
elucidates the viability of execution by the strategic agent within a
gradient optimization framework. Initially, we expand Eq.(10) using
a Lagrangian expression Eq.(11), where ùõærepresents the Lagrange
multiplier. The equations are as below:
ùêø(ùë•‚Ä≤,ùõæ)=ùë§ùëáùë•‚Ä≤‚àíùõº‚à•ùë•‚Ä≤‚àíùë•‚à•2
2+ùõæ(‚à•ùë•‚Ä≤‚à•2
2‚àí1). (11)
Subsequently, by applying the Karush-Kuhn-Tucker (KKT)[ 46]
conditions, we arrive at the final formulation:
Œîùëì(ùë•ùëó)=ùëì(ùë•ùëó;ÀÜùë§ùëó)=ÀÜùë§ùëó+2ùõºùë•ùëó
‚à•ÀÜùë§ùëó+2ùõºùë•ùëó‚à•2. (12)
A detailed proof is furnished in Appendix A, where the applica-
tion of differentiable execution facilitates the derivation of a readily
solvable form, thereby enhancing the efficacy of dynamic optimiza-
tion in the subsequent section.
4.4 Dynamic Learning and Optimization
4.4.1 Muti-round Dual-objective Optimization. The re-ranking stage
determines which items appear in the top- ùëòrecommendation list
finally, directly influencing the exposure of items from the producer
side. It is worth noting that fully pursuing the fairness of exposure
will worsen the original accuracy of the recommendation. To this
end, we apply a dual-objective optimization to the post-processing
re-ranking algorithm in every iteration of dynamic interaction.
This approach ensures the user‚Äôs top- ùëòrecommendation list
not only remains effective but also promotes the visibility of long-
tail products. The differentiable joint optimization objective in the
candidate list is defined as:
max
ùë¢ùëñ‚ààU1
ùëöùëö‚àëÔ∏Å
ùëñ=1[NDCG @ùëò(ùíìùëñ,ùúãùëñ)+ùúÜGini @ùëò(ùíìùëñ,ùúãùëñ)], (13)
where ùíìùëñ=S(Cùëñ;ùíñ‚àó
ùëñ)is ground-truth relevance score quantifying
the degree of relatedness between user ùëñand items in his candidate
list.ùúãùëñ=rank(ùúé(Cùëñ;ùíñùëñ))is a ranking operator to select items for
userùëñ. The regularization parameter ùúÜis the controlling degree for
fair exposure of items. Essentially, We denote ùíñ‚àó
ùëñas users‚Äô ground-
truth preference which consists of total candidate information:
ùíñ‚àó
ùëñ=ùíñ‚Ä≤
ùëñ
‚à•ùíñ‚Ä≤
ùëñ‚à•2,where ùíñ‚Ä≤
ùëñ=1
|Cùëñ|‚àëÔ∏Å
ùë•‚ààCùëñùë•.
Constrained with unit ‚Ñì2norm, ùíñ‚àó
ùëñcan be used as a super-
visory signal in every round of the optimization process. After
the current ùë°-th round ends, we get trained representation Uùë°.
The strategy agent undertakes the modification of item features:
Xùë°+1=ùëì(Xùë°;Uùë°), i.e.,ùë•ùë°+1
ùëó=Œîùëì(ùë•ùë°
ùëó). Then, the relevance score
is re-calculated by content-based simulator Sdenoted as ùíìùë°+1
ùëñ=
S(Cùë°+1
ùëñ;ùíñ‚àó
ùëñ), whereCùë°+1
ùëñ‚äÜXùë°+1, so that we can use ùíìùë°+1
ùëñto re-train
user representations at ùë°+1-th round.
4.4.2 Agent-based Strategic Learning. The system gains potential
power to shape incentives for fairness based on content creators‚Äô
reliance on the learned function ùúéfor utility. That means the update
of strategic agents has the potential to promote fair exposure.
3511KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, & Jianping Fan
Algorithm 1 Multi-round Training Process
1:Training procedure at time period ùë°;
2:Input: Item featuresXùë°, initialized user representations U, the
relevance simulator S, hyperparameters ùúÜ,ùõº,ùúè ;
3:Output: Updated user representations Uùë°, modified item fea-
turesXùë°+1;
4:forepoch do
5: GetŒîùëì(Xùë°)=ùëì(Xùë°;Uùë°)based on Eq.(12);
6: foruserùëñ‚àà{1,2,...,ùëö}do
7: GetCùë°
ùëñ‚äÜXùë°andŒîùëì(Cùë°
ùëñ)‚äÜŒîùëì(Xùë°)
8: Usebùë∑based onùúãùëñ,Œîùëì(ùúãùëñ)to generate the top- ùëòlist;
9: UseSto get ùíìùëñ,Œîùëì(ùíìùëñ);
10: Calculate NDCG @ùëò(ùíìùëñ,ùúãùëñ)as Eq.(7)
11: Calculate anticipated Gini @ùëò(Œîùëì(ùíìùëñ),Œîùëì(ùúãùëñ))as Eq.(8)
12: end for
13: Calculate total optimization loss as Eq.(15);
14: UpdateUùë°via gradient-descent;
15:end for
16:GetXùë°+1=ùëì(Xùë°;Uùë°)based on Eq.(12)
17:returnUùë°,Xùë°+1
The original optimization process shown as Eq.(13) ensures fair-
ness in the current learning iteration. That is reactive, specifically
adapted to conditions from the preceding time step. Recognizing
that the execution function ùëìencourages content creators to alter
items, we suggest a shift towards proactive promotion of fairness
by foreseeing and adapting to their strategic response. Specifically,
we utilize the performativity of prediction to carry out a forward-
looking regularization of optimization for fair exposure:
max
ùë¢ùëñ‚ààU1
ùëöùëö‚àëÔ∏Å
ùëñ=1[NDCG @ùëò(ùíìùëñ,ùúãùëñ)+ùúÜGini @ùëò(Œîùëì(ùíìùëñ),Œîùëì(ùúãùëñ))].(14)
As for regularization term , we replace Cùëñwith the anticipated
formŒîùëì(Cùëñ)={Œîùëì(ùë•)}ùë•‚ààCùëñ. Then, ùíìùëñis also promptly replaced
withŒîùëì(ùíìùëñ)induced byS:Œîùëì(ùíìùëñ)=S(Œîùëì(Cùëñ);ùíñ‚àó
ùëñ). Similarly, the
ranking operator is also reshaped by ùëì, since the change of Cùëñ,
denoted as Œîùëì(ùúãùëñ)=rank(ùúé(Œîùëì(Cùëñ);ùíñùëñ)). Thus, the response of
strategy agents can be fed back to the current optimization process
in real-time, thereby giving the item set more incentives for non-
popularity characteristics.
Further, we get the intuitive form of optimization loss based on
Eq.(14) to perform training of user representation ùíñùëñatùë°-th round.
Specifically, the loss function for agent-based strategic learning is
shown as follows:
L(Dùë°)=‚àí1
ùëöùëö‚àëÔ∏Å
ùëñ=1DR-NDCG @ùëò(ùíìùëñ,ùúãùëñ)(ùúè1)
+ùúÜùëö‚àëÔ∏Å
ùëñ=1DR-Gini @ùëò(Œîùëì(ùíìùëñ),Œîùëì(ùúãùëñ))(ùúè2)
.(15)
Eq.(15) optimizes for the current user utilities and anticipated
item exposure, which can fully leverage the role of strategic agents
in the dynamic learning process. For the complete algorithm frame-
work of muti-round training, see Algorithm 1.5 EXPERIMENTS
In this section, we undertake a series of experiments utilizing
two real-world datasets to assess the efficacy of our suggested
re-ranking approach. Our objective of the experiment is to address
the ensuing research questions (RQs):
‚Ä¢RQ1: How effective is the strategic agent in balancing user utility
and fair exposure of items in a dynamic setting?
‚Ä¢RQ2: How do different hyper-parameter settings (e.g. ùúÜ,ùõº) during
optimization affect the re-ranking performance?
‚Ä¢RQ3: How does our re-ranking approach eliminate the popularity
bias over the long term?
5.1 Content-based Simulator
Before the main experiments, we train two simulators to get users‚Äô
ground truth relevance of items with varying features over time.
Then, we can implement multiple rounds of training.
5.1.1 Datasets. We utilized a publicly available dataset and a real-
world industrial dataset for training simulators, which are used
for the following main experiment. Table 2 shows the necessary
statistics for these two datasets.
The Yelp restaurant dataset. Yelp provides information on a
wide variety of businesses and our study specifically concentrates
on the restaurant sector. We train Son this generated by users
having at least 50 restaurant reviews following [ 15]. These amount
to 1,377 users, 22,197 restaurants and 113,852 reviews. The number
of restaurant informative features is 43.
The industrial dataset. As for the industrial, we selected out
users who specialize in targeting laptops‚Äô consumption and have at
least 10 clicks on different items based on their behavior logs in the
online shopping mall. The data time range is from January 2023 to
December 2023. It has 11238 users, 1214 items, and 270259 clicks.
The number of laptop feature dimensions is 24. We will provide
specific information about the features in Appendix B.
Table 2: Statistics of the Yelp and Industrial dataset.
Data set Yelp restaurant Industrial
#Users 1,377 (242) 11,238 (351)
#Items 22,197 (1,103) 1,214 (951)
#Interactions 113,852 270,259
Density 0.00372 0.0198
5.1.2 Design and Training. We design the architecture of the simu-
lator, which can capture non-linear relations between user and item
features found in the data. Specifically, we configure the simulator
as an MLP featuring ReLU activations. Our architectural choice
includes five layers. The initial layer is designed to accept 2 ùëëin-
puts‚Äîcomprising ùëëitem features and ùëëuser features‚Äîand produces
4ùëëoutputs. Subsequently, the output dimension of each successive
layer is halved, ensuring a systematic reduction in dimensionality
throughout the network.
For each user ùëñ, we derive the ground-truth user features by ag-
gregating the features of all items reviewed (clicked) by user ùëñ. Then,
for Yelp, take the restaurants reviewed as positive and those that
3512Performative Debias with Fair-exposure Optimization Driven by
Strategic Agents in Recommender Systems KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
have not been reviewed but closest to the geographical locations
that have been reviewed as negative samples. For industrial, Ran-
domly select 10 items from the user‚Äôs click list as positive samples,
and 10 items outside the click list as negative samples. The final S
trained by BCE loss (70-20-10) for two datasets achieves 71.8% and
90.7% accuracy respectively on the held-out test set.
5.2 Experimental Settings
5.2.1 Implementation Details. In the main experiment, we focus
on active users in platforms. Specifically, we select data from a
city (Vancouver) in Yelp to explore the effect of popularity bias
better. Initially, we filter out users with more than c interactions
and selectùëêitems to form each user‚Äôs candidate list. Subsequently,
items are classified into five categories based on their frequency of
appearance across all candidate lists of users. For Yelp, setting c=40
results in 242 active users and 1103 items, with the item categories
being [‚Äò1-5‚Äô, ‚Äò6-10‚Äô, ‚Äò11-15‚Äô, ‚Äò16-20‚Äô, ‚Äò20+‚Äô] and their respective count
list is [634, 180, 94, 51, 135]. For industrial, setting c=50 results
in 351 active users and 951 items, with the item categories being
[‚Äò1-15‚Äô, ‚Äò16-30‚Äô, ‚Äò31-45‚Äô, ‚Äò46-60‚Äô, ‚Äò60+‚Äô] and their respective count
list is [580, 186, 85, 36, 64]. Notably, original top- ùëòlists reveal that
items with higher popularity are more frequently recommended,
as shown in Figure 3, which indicates a tendency for Sto assign
higher relevance to items of greater popularity. In particular, the
average Gini coefficients across users‚Äô original top- ùëòlists are 0.0149
(Yelp) and 0.0079 (industrial), respectively.
1-5 6-10 11-15 16-20 20+
Item Category0.00.10.20.30.40.50.6Items/Total ItemsItems/T otal Items
024681012
Average Frequency
Average Frequency
(a) Yelp dataset
1-15 16-30 31-45 6-60 60+
Item Category0.00.10.20.30.40.50.6Items/Total ItemsItems/T otal Items
0.02.55.07.510.012.515.017.520.0
Average Frequency
Average Frequency (b) Industrial dataset
Figure 3: Average frequency of different item categories rec-
ommended by Content-based Simulator when ùëò=10.
We setùëò=10 and T=10, randomly selecting ùëê-10 items fromCùëñof
per user for training in each round, and test using total ùëêitems. The
temperature coefficients were set to ùúè1=0.1 andùúè2=1. We conducted
100 epochs per round, with a learning rate of 0.1, a batch size
of 64 on Yelp, and a batch size of 32 on industrial. All codes are
programmed in Python 3.9.17 and PyTorch 2.0.1.
5.2.2 Compared Methods. To compare the performance of different
re-ranking methods in dynamic environments, we set up multiple
baselines to verify the effectiveness of our proposed model:
‚Ä¢The agent-based optimization approach: it carries out a forward-
looking regularization for fair-exposure of items as Eq.(14);
‚Ä¢Thenon-agent optimization approach: it focuses on regularization
at the current round without anticipated inputs as Eq.(13);
‚Ä¢Theaccuracy-only baseline: it pays no attention to the fairness of
item exposure and each training round only seeks to maximize
user utilities (by setting ùúÜ=0as Eq.(15));‚Ä¢MMR [8]: a traditional re-ranking approach using the Maximal
Marginal Relevance procedure, which balances accuracy and
diversity in top- ùëòlists based on the accuracy-only baseline;
‚Ä¢The non-retraining approach: items are modified according to
the model from the previous round and performance is tested on
those changes without retraining the model.
5.3 Experimental Results (RQ1&2)
The outcomes of our experiments conducted across two datasets
are depicted in Figure 4 and Figure 5. we have employed different ùúÜ
to showcase our findings, ensuring that the ultimate NDCG remains
above 0.7 while avoiding overfitting. Specifically, for Yelp, ùúÜis set
5 (‚Üì) and 10 (‚Üë); for Industrial, ùúÜis set 2 (‚Üì) and 4 (‚Üë).
5.3.1 The Effect of Strategic Agents (RQ1). Firstly, we analyze the
system‚Äôs performance with and without fair-exposure. The results
show that the system always makes the Gini coefficient higher after
the fairness regularization than before. Furthermore, the traditional
re-ranking method (MMR), when confined to the accuracy-only
approach, fails to sustain fair exposure across increasing training
iterations. This failure stems from the accuracy-only approach
driving the system into homogenization, where item features in
lists are overshadowed by those of more popular items, leading to
a convergence of NDCG towards 1.
As for retraining and non-retraining approaches, it is evident that
in every iteration, the NDCG values derived from the non-retraining
approach always fall below those attained through retraining, de-
picted in Figure 4a. This disparity arises because retrained models
more accurately encapsulate the information refined during the
current iteration of optimization. For industrial, where each user
has more candidates, the discrepancy between retraining and non-
retraining is markedly pronounced especially when ùõºis small.
Then, our analysis extends to evaluating the influence of antici-
patory regularization on fair-exposure optimization, contrasting
agent-based ( ‚ñ°) and non-agent (‚ñ≥) methods. In scenarios where ùúÜ
is relatively high, agent-based methods always surpass non-agent
approaches in optimizing the Gini coefficient. This suggests that,
under comparable conditions, optimization strategies that incorpo-
rate looking-forward intervention (Œîùëì(ùíìùëñ),Œîùëì(ùúãùëñ))(Eq.(14)) more
rapidly align the system towards equitable exposure. Remarkably,
even at lower ùúÜvalues, agent-based methodologies demonstrate
superior performance in initial training rounds. The comparison
results illustrate that utilizing the performativity of prediction can
realize fair-exposure optimization for debias objectives more effec-
tively and efficiently.
Most importantly, contrary to intuitive expectations that in-
creased fair exposure correlates with higher user utility deteriora-
tion‚Äîimplied by a direct trade-off between Gini coefficient enhance-
ment and NDCG reduction‚Äîthe agent-based approach defies this
phenomenon. It achieves superior Gini coefficients without com-
promising, and in fact, while enhancing NDCG values compared to
non-agent benchmarks, as illustrated in Figure 4a, Figure 4b and
Figure 5a (after the sixth time step) for ùúÜ(‚Üë). This achievement sug-
gests that the agent-based method adeptly identifies and leverages
beneficial features of less popular items within a limited range of
time steps, thereby balancing the enhancement of tail item visibility
with the concurrent improvement of user utilities.
3513KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, & Jianping Fan
without_reg (retrain)
MMR (retrain)non_agent_  (retrain)
non_agent_  (non_retrain)
agent_  (retrain)
agent_  (non_retrain)
non_agent_  (retrain)
non_agent_  (non_retrain)
agent_  (retrain)
agent_  (non_retrain)
0.000.020.040.060.080.10GINI
0 2 4 6 8
Time0.700.750.800.850.900.951.00NDCG
Optimization result over times of Yelp ( =1)
(
a)ùõº=1
0.000.020.040.060.08GINI
0 2 4 6 8
Time0.750.800.850.900.951.00NDCG
Optimization result over times of Yelp ( =3)
 (
b)ùõº=3
0.000.020.040.060.08GINI
0 2 4 6 8
Time0.800.850.900.951.00NDCG
Optimization result over times of Yelp ( =5)
 (
c)ùõº=5
Figure 4: Optimization results over times of Yelp datasets with different ùõº.
0.020.040.060.080.100.12GINI
0 2 4 6 8
Time0.60.70.80.9NDCG
Optimization result over times of Industrial ( =1)
(
a)ùõº=1
0.020.040.060.080.100.12GINI
0 2 4 6 8
Time0.700.750.800.850.900.95NDCG
Optimization result over times of Industrial ( =3)
 (
b)ùõº=3
0.020.030.040.050.060.070.08GINI
0 2 4 6 8
Time0.800.850.900.95NDCG
Optimization result over times of Industrial ( =5)
 (
c)ùõº=5
Figure 5: Optimization results over times of industrial datasets with different ùõº.
5.3.2 Hyper-parameter Analysis (RQ2). In this subsection, the role
of regularization parameter ùúÜand scaling parameter of modification
costsùõºis identified in our dynamic setting.
Focusing on ùúÜ, larger values significantly bolster the system‚Äôs
capability for fair-exposure optimization, manifesting in a prefer-
ence to substitute popular features in the initial top- ùëòlist with less
popular attributes. This adjustment encourages strategic agents to
increasingly incentivize tail item features within the recommen-
dation list across successive training rounds. Consequently, the
largerùúÜresults in a progressive elevation of the Gini coefficientand a corresponding decline in NDCG values as training advances.
However, it is important to note that this decline is not absolute
and is dependent on the value of ùõº, as explained in the latter half
of the subsection 5.3.1. Conversely, lower lambda values facilitate
a reduction in the Gini coefficient and expedite the system‚Äôs drift
toward homogenization, especially noted in the industrial dataset.
As forùõº, the higher value imply a greater cost for strategic
agents to modify item features, leading to a reduction in feature
incentives across disparate item categories. That enables larger ùõºto
moderate the system‚Äôs iterative evolution. For illustration, contrasts
3514Performative Debias with Fair-exposure Optimization Driven by
Strategic Agents in Recommender Systems KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
drawn between Figure 4b and Figure 4c, as well as Figure 5b and
Figure 5c, reveal that at lower values of ùúÜ, the Gini coefficient
diminishes progressively with each training round. However, at the
ùõºsetting of 5, the Gini coefficient remains consistently higher than
at theùõºsetting of 3, thereby tempering the momentum towards
homogenization. In contrast, with higher lambda values, although
the Gini coefficient escalates with each round, ùõº=5ensures a
lower Gini coefficient compared to ùõº=3, effectively moderating
the drive towards enhanced fair-exposure. Furthermore, the value
ofùõºcan impact the disparity between retraining and non-retraining
approaches. By comparing the three sub-figures in Figure 4 and
Figure 5, it can be intuitively seen that the larger the value of
parameterùõº, the smaller the difference between the two methods.
5.4 Effectiveness Results of Debias (RQ3)
Figure 6 illustrates the effectiveness of our proposed debiasing
mechanism through the deployment of a re-ranking method driven
by strategic agents, as evidenced in the top- ùëòrecommendation lists
in two datasets. The top two represent Yelp, while the bottom two
represent the industrial.
1-5 6-10 11-15 16-20 20+
Item Category0.00.10.20.30.40.50.6Items/Total Items
024681012
Average Frequency
original
time_5
time_9
(a) withùõº=1andùúÜ=5
1-5 6-10 11-15 16-20 20+
Item Category0.00.10.20.30.40.50.6Items/Total Items
024681012
Average Frequency
original
time_5
time_9 (b) withùõº=3andùúÜ=5
1-15 16-30 31-45 46-60 60+
Item Category0.00.10.20.30.40.50.6Items/Total Items
0.02.55.07.510.012.515.017.520.0
Average Frequency
original
time_5
time_9
(c) withùõº=1andùúÜ=2
1-15 16-30 31-45 46-60 60+
Item Category0.00.10.20.30.40.50.6Items/Total Items
0.02.55.07.510.012.515.017.520.0
Average Frequency
original
time_5
time_9 (d) withùõº=3andùúÜ=2
Figure 6: Average frequency of different item categories in
two datasets recommended by strategic agents when ùëò=10.
The analysis of this subsection delves into the average frequency
of recommendations across five distinct item categories, ordered by
ascending popularity levels, at the fifth and ninth time step. Com-
pared to original top- ùëòlists generated bySinitially, our findings
indicate a strategic reduction in the recommendation frequencies
of highly popular items, juxtaposed with an enhancement in the
visibility of tail items within user candidate lists. Meanwhile, the
debiasing degree can be flexibly adjusted based on different param-
eter settings. Notably, it reveals that employing a lower ùõºvalue
facilitates a rapid increase in the exposure of items at the lower
end of the popularity spectrum, e.g. the category of [1-5,6-10] in
Yelp and the category of [1-15,16-30] in the industrial. Conversely,
a higherùõºvalue ensures that the suppression of popular items ismoderated, e.g. the category of [20+] and [60+], preventing their
excessive demotion in the recommendation process.
6 CONCLUSION
In our study, we delve into the issue of popularity bias within rec-
ommendation systems, from the perspective of producer strategic
behavior in two-sided markets. Enabling to involve the producers
in combating biases actively, our proposed approach proposed a
re-ranking method driven by strategic agents, which can enrich the
connotation of fair-exposure from the perspective of item features
combination. In order to discover the beneficial features of tail items
and make full use of them, we incentivize content creators to mod-
ify item features based on performative optimization results per
round, thereby enhancing exposure for long-tail items efficiently
while maintaining recommendation accuracy. The method employs
the differentiable ranking operator, achieving dual optimization
goals of accuracy and fair exposure in the end-to-end training para-
digm. Experimental results on two real-world datasets validate the
effectiveness and potential of our method for addressing long-tail
item exposure in the long run.
ACKNOWLEDGMENTS
This study was partially funded by the supports of National Natural
Science Foundation of China (72101176).
REFERENCES
[1]Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Controlling
popularity bias in learning-to-rank recommendation. In Proceedings of the eleventh
ACM conference on recommender systems. 42‚Äì46.
[2]Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2019. Managing
popularity bias in recommender systems with personalized re-ranking. arXiv
preprint arXiv:1901.07555 (2019).
[3]Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher, and
Edward Malthouse. 2021. User-centered evaluation of popularity bias in recom-
mender systems. In Proceedings of the 29th ACM Conference on User Modeling,
Adaptation and Personalization. 119‚Äì129.
[4]Omer Ben-Porat, Itay Rosenberg, and Moshe Tennenholtz. 2020. Content provider
dynamics and coordination in recommendation ecosystems. Advances in Neural
Information Processing Systems 33 (2020), 18931‚Äì18941.
[5]Levin Brinkmann, Fabian Baumann, Jean-Fran√ßois Bonnefon, Maxime Derex,
Thomas F M√ºller, Anne-Marie Nussberger, Agnieszka Czaplicka, Alberto Acerbi,
Thomas L Griffiths, Joseph Henrich, et al .2023. Machine culture. Nature Human
Behaviour (2023), 1‚Äì14.
[6]Gavin Brown, Shlomi Hod, and Iden Kalemaj. 2022. Performative prediction in a
stateful world. In International Conference on Artificial Intelligence and Statistics.
PMLR, 6045‚Äì6061.
[7]Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning
to rank: from pairwise approach to listwise approach. In Proceedings of the 24th
international conference on Machine learning. 129‚Äì136.
[8]Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based
reranking for reordering documents and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference on Research and development
in information retrieval. 335‚Äì336.
[9]Allison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. 2018. How
algorithmic confounding in recommendation systems increases homogeneity
and decreases utility. In Proceedings of the 12th ACM conference on recommender
systems. 224‚Äì232.
[10] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan
He. 2023. Bias and debias in recommender system: A survey and future directions.
ACM Transactions on Information Systems 41, 3 (2023), 1‚Äì39.
[11] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun
Zhou, and Meng Wang. 2023. Improving recommendation fairness via data
augmentation. In Proceedings of the ACM Web Conference 2023. 1012‚Äì1020.
[12] Mingyue Cheng, Qi Liu, Wenyu Zhang, Zhiding Liu, Hongke Zhao, and Enhong
Chen. 2024. A general tail item representation enhancement framework for
sequential recommendation. Frontiers of Computer Science 18, 6 (2024), 1‚Äì12.
3515KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain. Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, & Jianping Fan
[13] Mingyue Cheng, Runlong Yu, Qi Liu, Vincent W Zheng, Hongke Zhao, Hefu
Zhang, and Enhong Chen. 2019. Alpha-beta sampling for pairwise ranking in
one-class collaborative filtering. In 2019 IEEE International Conference on Data
Mining (ICDM). IEEE, 1000‚Äì1005.
[14] Virginie Do and Nicolas Usunier. 2022. Optimizing generalized Gini indices for
fairness in rankings. In Proceedings of the 45th International ACM SIGIR Conference
on Research and Development in Information Retrieval. 737‚Äì747.
[15] Itay Eilat and Nir Rosenfeld. 2023. Performative Recommendation: Diversifying
Content via Strategic Incentives. arXiv preprint arXiv:2302.04336 (2023).
[16] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei,
Biao Li, Yuan Zhang, and Peng Jiang. 2023. CIRS: Bursting filter bubbles by coun-
terfactual interactive recommender system. ACM Transactions on Information
Systems 42, 1 (2023), 1‚Äì27.
[17] Joseph L Gastwirth. 1972. The estimation of the Lorenz curve and Gini index.
The review of economics and statistics (1972), 306‚Äì316.
[18] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. 2019. Stochastic
optimization of sorting networks via continuous relaxations. arXiv preprint
arXiv:1903.08850 (2019).
[19] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. 2016.
Strategic classification. In Proceedings of the 2016 ACM conference on innovations
in theoretical computer science. 111‚Äì122.
[20] Jiri Hron, Karl Krauth, Michael I Jordan, Niki Kilbertus, and Sarah Dean. 2022.
Modeling content creator incentives on algorithm-curated platforms. arXiv
preprint arXiv:2206.13102 (2022).
[21] Meena Jagadeesan, Nikhil Garg, and Jacob Steinhardt. 2024. Supply-side equilibria
in recommender systems. Advances in Neural Information Processing Systems 36
(2024).
[22] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization
with gumbel-softmax. arXiv preprint arXiv:1611.01144 (2016).
[23] Ray Jiang, Silvia Chiappa, Tor Lattimore, Andr√°s Gy√∂rgy, and Pushmeet Kohli.
2019. Degenerate feedback loops in recommender systems. In Proceedings of the
2019 AAAI/ACM Conference on AI, Ethics, and Society. 383‚Äì390.
[24] James Kotary, Ferdinando Fioretto, Pascal Van Hentenryck, and Ziwei Zhu. 2022.
End-to-end learning for fair ranking systems. In Proceedings of the ACM Web
Conference 2022. 3520‚Äì3530.
[25] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. 2023.
Propensity matters: Measuring and enhancing balancing for recommendation.
InInternational Conference on Machine Learning. PMLR, 20182‚Äì20194.
[26] Lydia T Liu, Nikhil Garg, and Christian Borgs. 2022. Strategic ranking. In Inter-
national Conference on Artificial Intelligence and Statistics. PMLR, 2489‚Äì2518.
[27] Debabrata Mahapatra, Chaosheng Dong, Yetian Chen, and Michinari Momma.
2023. Multi-label learning to rank through multi-objective optimization. In
Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and
Data Mining. 4605‚Äì4616.
[28] Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad
Mobasher, and Robin Burke. 2020. Feedback loop and bias amplification in
recommender systems. In Proceedings of the 29th ACM international conference
on information & knowledge management. 2145‚Äì2148.
[29] Celestine Mendler-D√ºnner, Frances Ding, and Yixin Wang. 2022. Anticipating
performativity by predicting from predictions. Advances in Neural Information
Processing Systems 35 (2022), 31171‚Äì31185.
[30] John P Miller, Juan C Perdomo, and Tijana Zrnic. 2021. Outside the echo chamber:
Optimizing the performative risk. In International Conference on Machine Learning.
PMLR, 7710‚Äì7720.
[31] Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. 2020. Con-
trolling fairness and bias in dynamic learning-to-rank. In Proceedings of the 43rd
international ACM SIGIR conference on research and development in information
retrieval. 429‚Äì438.
[32] Mohammadmehdi Naghiaei, Hossein A Rahmani, and Yashar Deldjoo. 2022.
Cpfair: Personalized consumer and producer fairness re-ranking for recommender
systems. In Proceedings of the 45th International ACM SIGIR Conference on Research
and Development in Information Retrieval. 770‚Äì779.
[33] Gourab K Patro, Arpita Biswas, Niloy Ganguly, Krishna P Gummadi, and Abhijnan
Chakraborty. 2020. Fairrec: Two-sided fairness for personalized recommendations
in two-sided platforms. In Proceedings of the web conference 2020. 1194‚Äì1204.
[34] Juan Perdomo, Tijana Zrnic, Celestine Mendler-D√ºnner, and Moritz Hardt. 2020.
Performative prediction. In International Conference on Machine Learning. PMLR,7599‚Äì7609.
[35] Jinghua Piao, Jiazhen Liu, Fang Zhang, Jun Su, and Yong Li. 2023. Human‚ÄìAI
adaptive dynamics drives the emergence of information cocoons. Nature Machine
Intelligence 5, 11 (2023), 1214‚Äì1224.
[36] Przemys≈Çaw Pobrotyn and Rados≈Çaw Bia≈Çobrzeski. 2021. Neuralndcg: Direct
optimisation of a ranking metric via differentiable relaxation of sorting. arXiv
preprint arXiv:2102.07831 (2021).
[37] Nir Rosenfeld, Anna Hilgard, Sai Srivatsa Ravindranath, and David C Parkes.
2020. From predictions to decisions: Using lookahead regularization. Advances
in Neural Information Processing Systems 33 (2020), 4115‚Äì4126.
[38] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and
Thorsten Joachims. 2016. Recommendations as treatments: Debiasing learning
and evaluation. In international conference on machine learning. PMLR, 1670‚Äì
1679.
[39] Chaofeng Sha, Xiaowei Wu, and Junyu Niu. 2016. A framework for recommending
relevant and diverse items.. In IJCAI, Vol. 16. 3868‚Äì3874.
[40] Ayan Sinha, David F Gleich, and Karthik Ramani. 2016. Deconvolving feedback
loops in recommender systems. Advances in neural information processing systems
29 (2016).
[41] Richard Sinkhorn. 1964. A relationship between arbitrary positive matrices and
doubly stochastic matrices. The annals of mathematical statistics 35, 2 (1964),
876‚Äì879.
[42] Wenlong Sun, Sami Khenissi, Olfa Nasraoui, and Patrick Shafto. 2019. Debiasing
the human-recommender system feedback loop in collaborative filtering. In
Companion Proceedings of The 2019 World Wide Web Conference. 645‚Äì651.
[43] Rohan Taori and Tatsunori Hashimoto. 2023. Data feedback loops: Model-driven
amplification of dataset biases. In International Conference on Machine Learning.
PMLR, 33883‚Äì33920.
[44] Adam Wagstaff, Pierella Paci, and Eddy Van Doorslaer. 1991. On the measurement
of inequalities in health. Social science & medicine 33, 5 (1991), 545‚Äì557.
[45] Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, and Xiangnan He.
2021. Model-agnostic counterfactual reasoning for eliminating popularity bias
in recommender system. In Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining. 1791‚Äì1800.
[46] Hsien-Chung Wu. 2007. The Karush‚ÄìKuhn‚ÄìTucker optimality conditions in an
optimization problem with interval-valued objective function. European Journal
of operational research 176, 1 (2007), 46‚Äì59.
[47] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. 2023.
Exploring large language model for graph data understanding in online job
recommendations. arXiv preprint arXiv:2307.05722 (2023).
[48] Likang Wu, Hongke Zhao, Zhi Li, Zhenya Huang, Qi Liu, and Enhong Chen.
2023. Learning the Explainable Semantic Relations via Unified Graph Topic-
Disentangled Neural Networks. ACM Transactions on Knowledge Discovery from
Data 17, 8 (2023), 1‚Äì23.
[49] Meike Zehlike and Carlos Castillo. 2020. Reducing disparate exposure in ranking:
A learning to rank approach. In Proceedings of the web conference 2020. 2849‚Äì2855.
[50] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui
Ling, and Yongdong Zhang. 2021. Causal intervention for leveraging popularity
bias in recommendation. In Proceedings of the 44th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 11‚Äì20.
[51] Chuang Zhao, Xinyu Li, Ming He, Hongke Zhao, and Jianping Fan. 2023. Sequen-
tial Recommendation via an Adaptive Cross-domain Knowledge Decomposition.
InProceedings of the 32nd ACM International Conference on Information and
Knowledge Management. 3453‚Äì3463.
[52] Hongke Zhao, Chuang Zhao, Xi Zhang, Nanlin Liu, Hengshu Zhu, Qi Liu, and
Hui Xiong. 2023. An ensemble learning approach with gradient resampling for
class-imbalance problems. INFORMS Journal on Computing 35, 4 (2023), 747‚Äì763.
[53] Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Depeng Jin. 2021.
Disentangling user interest and conformity for recommendation with causal
embedding. In Proceedings of the Web Conference 2021. 2980‚Äì2991.
[54] Meizi Zhou, Jingjing Zhang, and Gediminas Adomavicius. 2023. Longitudinal
impact of preference biases on recommender systems‚Äô performance. Information
Systems Research (2023).
[55] Ziwei Zhu, Yun He, Xing Zhao, and James Caverlee. 2021. Popularity bias in
dynamic recommendation. In Proceedings of the 27th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining. 2439‚Äì2449.
3516Performative Debias with Fair-exposure Optimization Driven by
Strategic Agents in Recommender Systems KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain.
APPENDIX
A PROOF
We get closed-form expression for the best response of the strategic
agent using Karush-Kuhn-Tucker (KKT) conditions. The detailed
proof process is as follows. Firstly, the constrained optimization
problem is given below:
max
ùë•‚Ä≤ùë§ùëáùë•‚Ä≤‚àíùõº‚à•ùë•‚Ä≤‚àíùë•‚à•2
2s.t.‚à•ùë•‚Ä≤‚à•2=1
Next, we construct the Lagrangian ùêøwith the constraint incorpo-
rated by the Lagrange multiplier ùõæ:
ùêø(ùë•‚Ä≤,ùõæ)=ùë§ùëáùë•‚Ä≤‚àíùõº‚à•ùë•‚Ä≤‚àíùë•‚à•2
2+ùõæ(‚à•ùë•‚Ä≤‚à•2
2‚àí1).
Take the gradient of ùêøwith respect to ùë•‚Ä≤and set it to zero for
optimality. The equation is as below:
‚àáùë•‚Ä≤ùêø=ùë§‚àí2ùõº(ùë•‚Ä≤‚àíùë•)+2ùõæùë•‚Ä≤=0,
and because we need to find the optimal response form for ùë•‚Ä≤, we
then solve the equation for ùë•‚Ä≤which gives:
ùë•‚Ä≤=ùë§+2ùõºùë•
2(ùõº‚àíùõæ),
and substituting ùë•‚Ä≤into the constraint ‚à•ùë•‚Ä≤‚à•2=1gives us a new
expression, which states the Euclidean norm of ùë•‚Ä≤is equal to 1:ùë§+2ùõºùë•
2(ùõº‚àíùõæ)2=1.
This condition allows us to solve for ùõæ. Onceùõæis found, we can sub-
stitute back into the expression for ùë•‚Ä≤to obtain the final expression
that satisfies the constraint. The final expression for ùë•‚Ä≤, taking into
account the constraint, can be written as:
ùë•‚Ä≤=ùë§+2ùõºùë•
‚à•ùë§+2ùõºùë•‚à•2.B ADDITIONAL DESCRIPTION
In order to utilize the feature incentives of strategy agents, we col-
lect as much semantic information as possible about items. For Yelp
data, we used data processed by [ 15]. Category data is leveraged to
create supplementary features by clustering similar categories that
share comparable contextual significance. For industrial, we collect
from a large online mall. The complete features are as follows.
Yelp restaurants: ‚Äòstars‚Äô, ‚Äòalcohol‚Äô, ‚Äòrestaurants good for groups‚Äô,
‚Äòrestaurants reservations‚Äô, ‚Äòrestaurants attire‚Äô, ‚Äòbike parking‚Äô, ‚Äòrestau-
rants price range‚Äô, ‚Äòhas tv‚Äô, ‚Äònoise level‚Äô, ‚Äòrestaurants take out‚Äô,
‚Äòcaters‚Äô, ‚Äòoutdoor seating‚Äô, ‚Äògood for meal-dessert‚Äô, ‚Äògood for meal-
late night‚Äô, ‚Äògood for meal-lunch‚Äô, ‚Äògood for meal-dinner‚Äô, ‚Äògood for
meal-brunch‚Äô, ‚Äògood for meal-breakfast‚Äô, ‚Äòdogs allowed‚Äô, ‚Äòrestaurants
delivery‚Äô, ‚Äòjapanese‚Äô, ‚Äòchinese‚Äô, ‚Äòindia‚Äô, ‚Äòmiddle east‚Äô, ‚Äòmexican food‚Äô,
‚Äòsweets‚Äô, ‚Äòcoffee‚Äô, ‚Äòitalian‚Äô, ‚Äòburgers‚Äô, ‚Äòhot dogs‚Äô, ‚Äòsandwiches‚Äô, ‚Äòsteak‚Äô,
‚Äòpizza‚Äô, ‚Äòseafood‚Äô, ‚Äòfast food‚Äô, ‚Äòvegan‚Äô, ‚Äòice cream‚Äô, ‚Äòrestaurants table
service‚Äô, ‚Äòbusiness accepts credit cards‚Äô, ‚Äòwheel chair accessible‚Äô,
‚Äòdrive thru‚Äô, ‚Äòhappy hour‚Äô, ‚Äòcorkage‚Äô.
Laptops in the mall: ‚Äòoffice laptop‚Äô, ‚Äògaming laptop‚Äô, ‚Äòbrand1‚Äô,
‚Äòbrand2‚Äô, ‚Äòbrand3‚Äô, ‚Äòbrand4‚Äô, ‚Äòbrand5‚Äô, ‚Äòbrand6‚Äô, ‚Äòbrand7‚Äô, ‚Äòbrand8‚Äô,
‚Äòrelative price‚Äô, ‚Äòcolor-black‚Äô, ‚Äòcolor-gray & color-silver‚Äô, ‚Äòcolor-others‚Äô,
‚Äòscreen size‚Äô, ‚Äòcpu-category1‚Äô, ‚Äòcpu-category2‚Äô, ‚Äòram level‚Äô, ‚Äòdisk
level‚Äô, ‚Äòintegrated video-card‚Äô, ‚Äòdedicated video-card‚Äô, ‚Äòportable‚Äô, ‚Äòin-
stallment‚Äô, ‚Äòprivate customization‚Äô.
Then, we undertake feature processing for training simulators.
For the categorical attributes of items, we apply one-hot encoding
and for numerical attributes, we implement normalization tech-
niques. Additionally, for features possessing several levels, we em-
ploy a graded encoding strategy, utilizing numerical values ranging
from 0 to 1 to represent the hierarchy levels.
3517