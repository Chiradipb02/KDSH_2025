FaultInsight: Interpreting Hyperscale Data Center Host Faults
Tingzhu Bi
bitingzhu@stu.pku.edu.cn
Peking University
Beijing, ChinaYang Zhang
zhangyang.329@bytedance.com
ByteDance Inc.
Beijing, ChinaYicheng Pan
aqpyc@pku.edu.cn
Peking University
Beijing, China
Yu Zhang∗
felix.zhang@bytedance.com
ByteDance Inc.
Beijing, ChinaMeng Ma†
mameng@pku.edu.cn
Peking University
Shuanghu Laboratory
Beijing, ChinaXinrui Jiang
jxrjxrjxr@pku.edu.cn
Peking University
Beijing, China
Linlin Han
Feng Wang
hanlinlin.intern@bytedance.com
wangfeng.ai@bytedance.com
ByteDance Inc.
Beijing, ChinaXian Liu
liuxian.1@bytedance.com
ByteDance Inc.
Beijing, ChinaPing Wang†
pwang@pku.edu.cn
Peking University
Shuanghu Laboratory
Beijing, China
ABSTRACT
Operating and maintaining hyperscale data centers involving mil-
lions of service hosts has been an extremely intricate task to tackle
for top Internet companies. Incessant system failures cost operators
countless hours of browsing through performance metrics to diag-
nose the underlying root cause to prevent the recurrence. Although
many state-of-the-art (SOTA) methods have used time-series causal
discovery to construct causal relationships among anomalous met-
rics, they only focus on homogeneous service-level performance
metrics and fail to yield useful insights on heterogeneous host-level
metrics. To address the challenge, this study presents FaultInsight,
a highly interpretable deep causal host fault diagnosing framework
that offers diagnostic insights from various perspectives to reduce
human effort in troubleshooting. We evaluate FaultInsight using
dozens of incidents collected from our production environment.
FaultInsight provides markedly better root cause identification ac-
curacy than SOTA baselines in our incident dataset. It also shows
outstanding advantages in terms of deployability in real produc-
tion systems. Our engineers are deeply impressed by FaultInsight’s
ability to interpret incidents from multiple perspectives, helping
them quickly understand the mechanism behind the faults.
∗Industry corresponding author.
†Academic corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672051CCS CONCEPTS
•Computer systems organization →Maintainability and
maintenance; •Computing methodologies →Causal reasoning
and diagnostics ;•Networks→Cloud computing .
KEYWORDS
Fault diagnosis, data center, causal discovery
ACM Reference Format:
Tingzhu Bi, Yang Zhang, Yicheng Pan, Yu Zhang, Meng Ma, Xinrui Jiang,
Linlin Han, Feng Wang, Xian Liu, and Ping Wang. 2024. FaultInsight: Inter-
preting Hyperscale Data Center Host Faults. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3672051
1 INTRODUCTION
Hyperscale data centers are constantly facing internal and external
threats including sudden traffic, hardware failures, malicious at-
tacks, software errors/misconfigurations, etc. We are experiencing
frequent occurrences of anomalies, forming SLO [4,9](service level
objective) violations or even downtime [ 23]. Various data center inci-
dents around the world have caused huge economic losses and also
put a heavy duty on the operation and maintenance team. Although
many advanced AIOps [ 8] (Artificial Intelligence for IT Operations)
approaches have been proposed [ 7,15,20,21,25,27,31,45,46] to au-
tomatically construct impact correlation graphs using performance
metrics and dig out the root causes from such interpretable graph
structures. Most of them are designed on homogeneous service-level
KPIs (key performance indicators). Unfortunately, the problem be-
comes more complicated when drilling down to data center hosts
as illustrated in Fig. 1. Host-level KPIs are numerous and highly
heterogeneous, such as the operating status of CPU, memory, net-
work, I/O, etc. This leads to great difficulties for even the most
experienced system experts. To clarify this issue, we outline the
 
141
KDD ’24, August 25–29, 2024, Barcelona, Spain Tingzhu Bi et al.
(Physical Hosts)Datacenter(Logic Services)Cloud ApplicationBussiness Traﬃc
(Homogeneous Diagnosis) Service KPIsFault Diagnosis
(Heterogeneous Diagnosis) Host Runtime StatesDrill DownDeployed On……LatencyError Rate
Free MemoryI/O RequestsInbound BandwidthRunning ProcessesCPU usage Throughput
System Load
Figure 1: Cloud app services are running physically on hosts
in hyperscale data centers, where a faulty host could affect
the service that resides on it. To troubleshoot, the operators
perform heterogeneous diagnoses using host metrics.
Table 1: Common host components and metrics.
Comp
onent Typical Metrics Description
CP
U cpu.busy Percentage of CPU time consumed on tasks
Load load.1min Numb
er of processes in the running queue
Memor
y mem.available Size of available system memory
So
cket ss.estab Number of established socket connections
T
CP tcp.listenDrops Number of inbound TCP segment drops
UDP udp.inDatagrams Numb
er of inbound UCP datagrams
File
System fs.freePercent Percentage of free space in a disk partition
Disk
I/O io.msecWrite Total time consumed on write requests
Pr
ocess proc.running Number of currently running processes
Netw
ork net.inBytes Bandwith of inbound network traffic
essential difference between service and host-level KPI diagnostics.
Homogeneous service-level KPI diagnosis. Homogeneous KPI di-
agnosis is to analyze the same type of metrics (e.g., service latency)
observed on different entities (e.g., services), with the goal of identi-
fying which (e.g., a specific anomalous service) is the culprit of the
incident. These entities are peers in the system topology, such as ser-
vice instances, APIs, and microservices, so we call it service-level
KPI diagnosis. Early methods rely on system topology to orga-
nize the relationship of these entities [ 15], and some improved ap-
proaches can accurately construct service-level dependency graphs
on homogeneous metrics by causal discovery algorithms, such as
PC algorithm [ 39] and Granger causality [ 11]. Please note that the
fault correlations constructed in service-level diagnosis are also
homogeneous. That is, they represent the same implication, such as
the delay impact on up and downstream services.
Heterogeneous host-level KPI diagnosis. Heterogeneous KPIs are
any metrics with high variability of data types and formats. Diagno-
sis with heterogeneous metrics means to pinpoint which metric of
a single entity (e.g., an anomalous host) is the culprit of an incident
with metrics of different types (e.g., CPU usage, memory usage, and
inbound bandwidth) observed on the entity. Table 1 lists common
system components along with a typical metric for each, where we
assign distinct colors to components for better clarity. The faulting
mechanism inside the host connects these heterogeneous metrics
through direct and indirect correlation, making it extremely dif-
ficult to sort out the propagation paths and the root cause from
intertwined indicators. Consequently, the aforementioned methods
are not feasible in host-level tasks as dependencies between host-
level KPIs are sophisticated and hard to ascertain explicitly. They
finally produce spurious and missed associations on large-scale
host-level heterogeneous KPIs.We show a running case of host fault in a production environ-
ment in Fig. 2 by plotting a subset of key monitoring metrics of
the anomalous host. This fault lasted for 20 minutes, when the
system was in normal working condition before experiencing a
surge of socket connections at first. Then ❶a large number of pro-
cesses are spawned. As a consequence, ❷the system faced an initial
wave of acute load increase, and the available space of memory
was gradually exhausted in a short period of time. Finally, ❸the
swap space was used to relieve memory pressure, and ❹moving
inactive pages from RAM to disks caused spikes in I/O write re-
quests, and the I/O bottleneck resulted in a further rise of system
load. In short, this is a typical host fault case involving an entangled
fault propagation chain comprising heterogeneous impact links
across different system components. Furthermore, various impact
links took effect at different timestamps during the fault evolution
process throughout the fault duration. We term the intertwined
and dynamic interactions between metrics in the lifecycle of a host
fault as fault dynamics. To this end, revealing the fault dynamics
of a host fault could facilitate the postmortem diagnosis by reduc-
ing the time overhead, required domain knowledge, and human
effort needed to identify the root cause of the fault and determine
follow-up countermeasures to prevent the incident from recurring.
However, designing a diagnosis framework for host faults with
heterogeneous metrics entails many challenges. First, there are
currently no available time series analytic methodologies capa-
ble of discovering dynamic and heterogeneous impact links. Most
service-level diagnosis methods are tailored for homogeneous met-
rics and fail to capture complex interactions across system resources.
More crucially, current methods neglect the dynamic nature of fault
propagation and evolution mechanisms throughout the entire fault
lifecycle. Secondly, operators need interpretable representations
untangling the underlying complicated fault dynamics, i.e. insights,
to support the diagnosis process, whereas most existing methods
merely offer an ordered list of root cause metrics as the diagnostic
result. Last but not least, as a widely deployable module that can
be integrated into established AIOps platforms in production en-
vironments, the diagnosis framework should be efficient in terms
of speed and scale well on potentially large-scale inputs. To ad-
dress the challenges mentioned above, we present FaultInsight, a
deep causal diagnostic framework for hyperscale data center hosts
that offers faithful and intuitive diagnostic insights. FaultInsight
involves three stages: fault dynamics discovery, analysis, and in-
terpretation. Specifically, to address the first and third challenges,
we propose a novel efficient dynamic causal discovery method
in the discovery stage that disentangles heterogeneous metrics
with temporal convolutional networks (TCNs) [ 3] and learn the
underlying causal structure with chronological perturbation on
metrics. We recognize the learned dynamic causal relationships as
impact links and consider the dynamic causality graph as multi-step
fault dynamics. In the analysis stage, we extend the node influence
ranking method [ 16] in network analysis to the dynamic graph
situations. Subsequently, we score the outward-propagated and
inward-received influence of each metric at every timestep and
identify the peak fault propagation timestamps of each metric. To
address the second challenge, in the interpretation stage, we offer
tangible insights from various perspectives based on analysis re-
sults in the previous stage. Besides traditional root cause metrics
 
142FaultInsight: Interpreting Hyperscale Data Center Host Faults KDD ’24, August 25–29, 2024, Barcelona, Spain
2500050000
800900
02000
025G
10G20G30G
025014:1014:0514:1514:2014:2514:3014:35
Established Socket Connections(Socket)(Process)(Load)(Memory)(Memory)(Disk-I/O)Available Memory SizeNumber of All Processes
Free Swap Memory SizeAverage Load in 1 Min
Current Disk Write RequestsFault Durationss.estab
mem.availableproc.all
mem.swapFreeload.1min
io.write
❶❷❷❸❹❹
Figure 2: Key anomalous monitoring metrics of the running
host fault case. The arrows illustrate the impact paths.
identification tasks, FaultInsight supports insights in the form of
three other visual representations: metric-level overview graph,
component-level flow diagram, and time-aligned propagation net-
work. Briefly, the contributions of this paper are as follows:
(i)We propose an efficient dynamic causal discovery method
for heterogeneous metrics that captures the underlying fault
dynamics of host fault cases (Section 2.2).
(ii)We design a dynamic node influence scoring method to analyze
the outward-propagated and inward-received influence and
the peak influence timestamp of each metric (Section 2.3).
(iii)We present three visual representations of fault insights to
provide intuitive explanations that facilitate troubleshooting.
(Section 2.4).
(iv)We conduct extensive experiments on incidents from our pro-
duction environment and confirm the effectiveness of our
method. (Section 3)
2 INTERPRETING HOST FAULTS
2.1 Overview of FaultInsight
Fig. 3 shows the overview of the presented diagnostic framework,
FaultInsight. The pipeline of FaultInsight naturally consists of three
stages. In the discovery stage, we obtain a dynamic causal rep-
resentation of fault dynamics. Next, in the analysis and the final
interpretation stages, we analyze the dynamic influence of metrics
throughout the fault lifecycle and transform the analytical results
into intuitive representations of fault insights.
2.2 Fault Dynamics Discovery
2.2.1 Traditional Statistical Causal Discovery. To identify impact
links in host fault cases, FaultInight leverages the widely used
Granger causality analysis [ 11,12] as a bridge to uncover mutual
interactions of metrics. Given the intuition that a series may be
the cause of other series if the series could help predict future values
of other series, the analysis process of Granger causality involves
fitting𝑁component-wise prediction models using the input of all
Dynamic Cause DiscoveryDynamic Metric Inﬂuence ScoringFault Insights
ChronologicalPerturbationFault Dynamics DiscoveryFault Dynamics AnalysisFault Dynamics Interpretationt-1
t
t+1
……AnomalousHost MetricsFault Dynamics
Dynamic Inﬂuence Score+Peak Propgation TimeMetric-levelOverviewTime-aligned Propagation NetworkRoot Cause Metric IdentiﬁcationComponent-level Flow
Figure 3: Overview of the framework of FaultInsight.
TCN Decoder fOutput 1D Conv oTCN Decoder fOutput 1D Conv o
Input SeriesInput Series(b)(c)Predicted Series
X1,:X2,:XN,:…X1,:X2,:XN,:…X1,:X2,:XN,:…X1,:X2,:XN,:…
Latent Vector Autoregression pLatent Vector AutoregressionLatent Vector Autoregression pTCN Encoder g TCN Encoder g ^^^Reconstructed Series~~~N×Input SeriesXi,:Mapped SeriesZi,:,:
Dilated Causal ConvDropoutWeight NormReLU
(a)
Figure 4: (a) The architecture of TCNs. (b) The dataflow of
reconstruction, and (c) prediction tasks of the proposed dy-
namic causal discovery method.
metrics. If metric 𝑖significantly enhances the accuracy of metric 𝑗,
we say𝑖Granger-causes 𝑗.
Formally, let X∈R𝑁×𝑇be the matrix of collected monitoring
metrics of a faulty host containing 𝑁metrics in𝑇timesteps through-
out the fault duration, X𝑖,𝑡be the value of metric 𝑖at timestep 𝑡.
and the generalized form can be written as follows [38]:
X𝑖,𝑡=ℎ𝑖 X1,<𝑡,...,X𝑁,<𝑡+𝑒𝑖,𝑡
where X𝑖,<𝑡denotes the values of series 𝑖before time 𝑡,ℎ𝑖is the
component-wise prediction function that captures how the past of
all𝑁metrics maps to a particular metric 𝑖,𝑒is the error term that
indicates the accuracy of the prediction.
Tradition Granger causal discovery methods use statistical auto-
regressive (AR) models as ℎ𝑖and perform significance test of 𝐹-
statistic on 𝑒to determine the existence of causal relationships
between series [31].
2.2.2 Dynamic Causal Discovery Method for Heterogeneous Met-
rics. We implement ℎ𝑖by integrating a semantics-disentangling
TCN autoencoder with latent vector autoregressive predictors to
tackle data heterogeneity in the scenario of host fault diagnosis.
We also design a dynamic causal discovery methodology based on
chronological perturbation to extract dynamic causal relationships.
2.2.3 Semantics-disentangling TCN Autoencoder. The semantics-
disentangling TCN autoencoder is composed of a pair of univariate
TCN encoder 𝑔∗and decoder 𝑓∗, whose parameters are shared across
all input metrics. In the reconstruction task, the encoder disentan-
gles each metric into multiple common semantic channels in the
latent space whereas the decoder reconstructs each metric from
its disentangled representations [ 49]. A TCN consists of multiple
 
143KDD ’24, August 25–29, 2024, Barcelona, Spain Tingzhu Bi et al.
temporal convolutional blocks[ 3], as illustrated in Fig. 4(a). Each
temporal convolutional block maps an input series of any length to
an output series of the same length with dilated causal convolution,
in which causal assures no leakage from the future into the past
anddilated offers large reception field for better historical informa-
tion fusion. Finally, a 1D convolution output layer 𝑜shared across
all metrics merges the multi-channel representation series of each
metric to univariate reconstructed metric outputs. As illustrated in
Fig. 4(b), the reconstruction task can be formulated as follows:
Z𝑖,𝑡,:=𝑔∗(X𝑖,1:𝑡) (1)
˜X𝑖,𝑡=𝑜(𝑓∗(Z𝑖,1:𝑡,:)) (2)
where X𝑖,1:𝑡denotes the values of metric 𝑖from timestep 1to𝑡,Z∈
R𝑁×𝑇×𝐶denotes the latent representation series of X,𝐶is number
of latent semantics channels, ˜Xdenotes the reconstructed metrics.
The mean square error (MSE) loss ℓrecon of the reconstruction is:
ℓrecon=𝑁∑︁
𝑖=1𝑇∑︁
𝑡=1(˜X𝑖,𝑡−X𝑖,𝑡)2(3)
2.2.4 Latent Vector Autoregressive Predictor. To capture interac-
tions among heterogeneous metrics, we propose a latent vector
autoregressive predictor 𝑝that forecasts the future value of a met-
ric based on past values of all metrics in the latent space. Each
channel is predicted individually to preserve the semantics across
different representational channels. W∈R𝑁×𝑁×𝐶is the trainable
tensor and W𝑖,𝑗,𝑐determines how the output of metric 𝑖depends
on metric𝑗in channel𝑐. As illustrated in Fig. 4(c), the latent vector
autoregressive predictor is integrated into the TCN autoencoder.
The prediction task can be formulated as follows:
ˆZ𝑖,𝑡+1,𝑐=𝜎(𝑀∑︁
𝑗=1W𝑖,𝑗,𝑐Z𝑗,𝑡,𝑐) (4)
ˆX𝑖,𝑡+1=𝑜(𝑓∗(ˆZ𝑖,2:𝑡+1,:)) (5)
where ˆZand ˆXdenotes the predicted latent representations and
metrics respectively, 𝜎is the activation function in TCN autoen-
coder. The MSE loss ℓpredof the prediction is:
ℓpred=𝑁∑︁
𝑖=1𝑇∑︁
𝑡=2(ˆX𝑖,𝑡−X𝑖,𝑡)2(6)
2.2.5 Structured Sparsity Regularization. Due to the large number
of metrics inputs, we adopt spatial dropout [ 40] for the TCN au-
toencoder𝑔∗,𝑓∗and L1 regularization on learnable parameters of
the latent predictor 𝑝and the output layer 𝑜to reduce the num-
ber of connections between variables and thus highlight dominant
relationships among large-scale metrics.
2.2.6 Dynamic Causality Extraction by Chronological Perturbation.
We extract the dynamic causal relationships implied in the proposed
prediction neural network model and the input by chronological
perturbation. Permuting the values of a metric randomly removes
its chronological order and breaks potential causal relationships
between the metrics and others [ 29]. The gain of prediction error at
timestep𝑡of metric𝑖when metric 𝑗is perturbed can be interpreted
as the strength of the causal effect [43] from 𝑖to𝑗at𝑡.Algorithm 1: Fault Dynamics Discovery
Input: Input metricsX, number of epochs 𝑛epochs , number of perturbation
rounds𝑛pert
Output:𝑇-step failure dynamics D
//❶Train a dynamic causal discovery model
1StandardizeXto normal distribution N(0,1);
2for𝑖←1to𝑛epochs do
3 ˜X←𝑜(𝑓∗(𝑔∗(X))) ; // Reconstruction task
4 ˆX←𝑜(𝑓∗(𝑝(𝑔∗(X)))) ; // Prediction task
5 backward propagate ℓrecon+ℓpred;
6end
//❷Extract the fault dynamics with the trained model
7ˆX←𝑜(𝑓∗(𝑝(𝑔∗(X)))) ; // Original prediction
8Repeat𝑛perttimes
9 for𝑖←1to𝑁do
10X\𝑖←X with random permutation of X𝑖,:;
11 ˆX\𝑖←𝑜(𝑓∗(𝑝(𝑔∗(X\𝑖)))) ; // Perturbed prediction
12 for𝑗←2to𝑁do
13 for𝑡←1to𝑇do
// Calculate perturbed prediction error gain
14 D𝑡
𝑖,𝑗←D𝑡
𝑖,𝑗+max((ˆX\𝑖
𝑗,𝑡−X𝑗,𝑡)2−(ˆX𝑗,𝑡−X𝑗,𝑡)2,0);
15 end
16 end
17 end
18D←D/𝑛pert; // Average fault dynamics across all rounds
19returnD
2.2.7 Fault Dynamics Discovery. Algorithm 1 outlines the fault dy-
namics discovery stage of FaultInsight. ❶We first train a dynamic
causal discovery model by jointly optimizing the reconstruction and
prediction task, learning the knowledge of semantic disentangling
and mutual interactions at the same time. ❷Next, we extract the
dynamic causality graph with the trained model and input metrics
by chronological perturbation. The dynamic causality graph is in-
terpreted as the 𝑡-step fault dynamics D. To stabilize the graph, we
repeat the extraction process for 𝑛pertrounds in total and average
the elements of fault dynamics Dacross all rounds.
2.2.8 Time Complexity. The fault dynamics discovery stage in-
volves reconstruction, prediction, and dynamic causality extraction
tasks. During one training phase, the reconstruction or prediction
tasks are performed once for each metric in each training epoch.
During one causal discovery phase, 𝑁×𝑁pairs of causal strength
are extracted, and each extraction process triggers the prediction
task once. Consequently, the time complexity for the whole fault
dynamics discovery stage is O(𝑛 epochs·𝑁+𝑛pert·𝑁2).
2.3 Fault Dynamics Analysis
2.3.1 Dynamic Influence Scores. The raw fault dynamics is a three-
dimensional tensor containing 𝑁×𝑁×𝑇elements and is thus
infeasible to be interpreted directly. The focus of the analysis stage
of fault dynamics is metric influence. We define two directions of
metric influence: outward-propagated influence andinward-received.
Outward-propagated influence means the anomalous impact the tar-
get metric propagates to other metrics. On the other hand, inward-
received influence means the anomalous impact the target metric
receives from other metrics. In a host fault case, the strength of
outward-propagated and inward-received influence might vary
throughout the fault evolution. In light of this, we calculate two
types of dynamic influence scores to measure the influence strength:
outward influence scores S ∈R𝑁×𝑇and inward influence scores
 
144FaultInsight: Interpreting Hyperscale Data Center Host Faults KDD ’24, August 25–29, 2024, Barcelona, Spain
SourcesExampleTargets
t - 2tt - 1t + 1tt + 2t(b)Dynamic Inﬂuence ScoringStatic Importance ScoringSourcesExampleTargets
(a)
Figure 5: (a) Traditional static node importance scoring and
(b) proposed dynamic metric influence ranking. A circle de-
notes a node in the fault propagation graph.
¯S∈R𝑁×𝑇. We propose a dynamic metric influence scoring method
that extends the traditional node importance ranking method on
static graphs to compute dynamic influence scores.
2.3.2 Static Node Importance Scoring. Many previous studies [ 15,
27] utilize the PageRank algorithm [ 6] to measure the node impor-
tance when identifying culprit metrics in static dependency graphs.
Given a directed graph composed of fault propagation edges, where
the source nodes of edges propagate anomalies to target nodes,
Fig. 5(a) depicts how PageRank works in the fault diagnosis sce-
nario. Taking an orbitary node as an example, any outgoing link to
a target node increases the importance score of the example node.
The importance score 𝑟(𝑖)of node𝑖is calculated as [16]:
𝑟(𝑖)=𝛼∑︁
𝑗∈E+(𝑖)𝑟(𝑗)
|E−(𝑗)|+1−𝛼
𝑁(7)
whereE+(𝑖)andE−(𝑖)are the sets of target and source nodes of 𝑖,
respectively,|E−(𝑗)|denotes the number of source nodes pointing
to node𝑗,𝛼∈(0,1)is the smoothness hyperparameter. 𝛼assures
the convergence of Equation 7 and 1−𝛼could be interpreted as the
probability of random teleportation [16]. If the fault propagation
graph is weighted, let 𝑾𝑖,𝑗∈[0,+∞) be the weight of the edge 𝑖to
𝑗, the importance score 𝑟′(𝑖)on weighted graph is calculated as:
𝑟′(𝑖)=𝛼𝑁∑︁
𝑗=1𝑾𝑖,𝑗Í𝑁
𝑘=1𝑾𝑘,𝑗𝑟′(𝑗)+1−𝛼
𝑁(8)
2.3.3 Dynamic Metric Influence Scoring. Fault dynamicsDcould
be considered as a dynamic weighted fault propagation graph with
𝑡timesteps, whereD𝑖,𝑗,𝑡denotes the causal influence from metric
𝑖to𝑗at𝑡. As depicted in Fig. 5(b), in such a graph, a node can not
only propagate anomalies to its target nodes but also potentially
influence other nodes in future timesteps 𝑡+1,𝑡+2,...,𝑇 when its
corresponding metric has delayed effects on other metrics, possibly
in other system components. Empirically, the delayed influence
of metrics typically does not take long to manifest. Therefore, we
can multiply the original causal influence by a temporal discount
factor that decays exponentially for future timesteps. Let S(𝑖,𝑡)be
the outward influence score of metric 𝑖at timestep𝑡. Equation 8 is
extended to consider delayed metric influence:
S(𝑖,𝑡)=𝛼𝑇−𝑡∑︁
Δ𝑡=0𝛾(Δ𝑡)𝑁∑︁
𝑗=1D𝑖,𝑗,𝑡+Δ𝑡Í𝑁
𝑘=1D𝑘,𝑗,𝑡+Δ𝑡S(𝑗,𝑡+Δ𝑡)+1−𝛼
𝑁𝑇(9)
𝛾(Δ𝑡)=(1−𝛽)Δ𝑡𝛽 (10)
14:1514:2514:3514:4514:1514:2514:3514:45
ss.estab (Cause)mem.available (Cause)proc.all (Eﬀect)mem.swapFree (Cause)load.1min (Cause)io.write (Eﬀect)Dynamic Outward Inﬂuence ScoreDynamic Inward Inﬂuence ScoreFigure 6: The dynamic inward and outward influence scores
of key anomalous metrics. The peak propagation timestamps
are marked with darker color gradients and upward arrows.
where𝛾is the temporal discount function that takes the form of
the probability mass function of geometric distributions, 𝛽is the
temporal discount hyperparameter. Let ¯S(𝑖,𝑡)be the inward influ-
ence score of metric 𝑖at𝑡. We can also calculate inward influence
scores by reversing the causal and temporal direction:
¯S(𝑖,𝑡)=𝛼𝑡−1∑︁
Δ𝑡=0𝛾(Δ𝑡)𝑁∑︁
𝑗=1D𝑗,𝑖,𝑡−Δ𝑡Í𝑁
𝑘=1D𝑗,𝑘,𝑡−Δ𝑡¯S(𝑗,𝑡−Δ𝑡)+1−𝛼
𝑁𝑇(11)
2.3.4 Metric Roles. Fig. 6 plots dynamic inward and outward in-
fluence scores of key anomalous metrics of the running case. We
calculate the area under curves of inward and outward dynamic in-
fluence scores as accumulated inward influence scores SΣ∈R𝑁=Í𝑇
𝑡=1S𝑖,𝑡and outward ¯SΣfor each metric to characterize the accu-
mulated influence strength of metrics in two different directions.
As shown, some metrics like ss.estab have higher accumulated
outward influence scores than accumulated inward influence scores,
while others io.write have higher accumulated inward influence
scores. This is probably because connections established by pro-
grams ( ss.estab ) exert pressure on the system whereas the disk
write operations ( io.write ) are side effects of high system pres-
sure. Such observations prompted us to classify anomalous host
metrics into two roles: cause metric andeffect metric. We consider
metric𝑖to be a cause metric if its accumulated outward influence
scoreSΣ
𝑖exceeds its inward counterpart ¯SΣ
𝑖, and vice versa.
2.3.5 Peak Propagation Timestamp Identification. As shown in
Fig. 6, the curves representing the dynamic outward or inward
influence scores can also be interpreted as density functions of the
distributions of propagation timestamps. These curves illustrate
how the total amount of outward-propagated or inward-received
influence is distributed across the time axis throughout the fault
lifecycle. We calculate the mean values of the peak distributions of
propagation timestamps as peak propagation timestamps T∈R𝑁.
For metric𝑖, the peak values of dynamic influence scores S∗
𝑖,:is
calculated by subtracting the 𝑞-percentile𝑄ofS𝑖,:, filtering out
non-peak values:
S∗
𝑖,:=max(S𝑖,:−𝑄,0) (12)
 
145KDD ’24, August 25–29, 2024, Barcelona, Spain Tingzhu Bi et al.
The peak propagation timestamps are calculated as means of prob-
ability mass function after normalizing the sum of S∗
𝑖,:to1:
T𝑖=𝑇∑︁
𝑡=1𝑡S∗
𝑖,𝑡/𝑇∑︁
𝑡=1S∗
𝑖,𝑡(13)
Fig 6 also shows peak propagation timestamps of key anomalous
metrics of the running case. A cause metric propagates the most
influence on other metrics around the peak propagation timestamp,
whereas an effect metric receives the most influence on others
around the peak propagation timestamp.
2.4 Fault Dynamics Interpretation
2.4.1 Root Cause Metrics Identification. Root cause metrics iden-
tification is a typical diagnostic task in previous work [ 13,18,31].
FaultInsight ranks potential root cause metrics based on accumu-
lated outward influence scores SΣ, considering the total anomalous
impact the metrics propagate to other metrics throughout the fault
evolution. For example, the root cause ranking of the key anoma-
lous metric in Fig. 6 is ss.estab >mem.available >load.1min >
mem.swapFree >proc.all >io.write.
2.4.2 Metric-level Overview Graph. The metric-level overview graph
of FaultInsight outlines a host fault case in the aspect of inter-metric
influence, packed with various diagnostic results. Fig. 7 shows a
metric-level overview graph of the running case with top-40 met-
rics (see Appendix C for the full metric-level overview graph). Each
node represents a host metric. The shapes of nodes reflect the roles
of metrics in the fault. Circle-shaped nodes represent cause metrics
and triangle-shaped nodes represent effect metrics. The size denotes
the strength of accumulated outward influence for a cause metric
or inward influence score for an effect metric. The nodes in the
graph are connected by causal influence edges. The edge weight
𝑤𝑖,𝑗from metric 𝑖to𝑗equals the accumulated causal influenceÍ𝑇
𝑡=0D𝑖,𝑗from metric 𝑖to𝑗throughout the fault evolution. The
darkness of the edge color denotes the strength of an edge weight.
We adopt the Fruchterman-Reingold force-directed graph layout
algorithm [ 10] to generate the layout of the nodes in the graph,
ensuring that nodes with closer causal influence are more likely to
gather together for convenient analysis. As shown, ss.estab and
other memory-related metrics are the most conspicuous root causes,
distributing on the graph’s two sides, whereas the disk I/O-related
metrics are the effects, clustering around the graph center.
2.4.3 Component-level Flow Diagram. FaultInsight also provides a
component-level flow diagram, represented in the form of a Sankey
diagram, to illustrate the dominant anomalous influence among sys-
tem components of a faulty host at a higher hierarchy in the form
of a Sankey diagram [ 35]. Fig. 8 shows the component-level flow
diagram of the running case. On the left side of the diagram, top-
𝑛metrics cause metrics are listed in the first column. The height of
each metric represents its accumulated outward score. The top- 𝑛flow
outward influence edges of each cause metric are then aggregated
into respective parent components in the second column. On the
left side of the diagram, top effect metrics are listed and aggregated
into the third column. The dominant anomalous influence flows
between components are depicted between the second column and
ss.estabmem.availablemem.freemem.cached
fs.inodes.freeload.1minproc.blockedmem.buffers
io.svctm
proc.processesmem.shmem
proc.runningfs.bytes.freecpu.niceio.writecpu.systemtcp.pkgOutsegs
fs.bytes.freeio.writeWaitproc.alludp.ignoreMultimem.memfree
io.writemem.writebackcpu.switchesio.readSectorsio.readBytes
mem.dirtymem.kernelFilesAlloccpu.iowaitmem.swapFree
fs.inodes.freeio.avgWaitcpu.softirqudp.noPortsudp.outDatagrams
io.readMergedudp.inDatagrams
io.utilss.orphanedCPULoadMemorySocketTCPUDPFile SystemDisk I/OProcessNetworkFigure 7: A metric-level overview graph with top-40 metrics.
Figure 8: The component-level flow diagram.
Algorithm 2: Component-level Flow Diagram Generation
Input:𝑇-step fault dynamics D, accumulated outward/inward scores
SΣ/¯SΣ, number of metrics/flows with the highest scores to present
𝑛metrics /𝑛flows
Output: Dominant influence flows F
1F←∅ ;F←0𝑛metrics×𝑛metrics;
2foreach𝑚intop𝑛metrics metrics with highest outward scores in SΣdo
3 foreach ¯𝑚intop𝑛metrics metrics with highest inward scores in ¯SΣdo
// Sum up flow strength from 𝑚to¯𝑚over time
4 F𝑚,¯𝑚←Í𝑇
𝑡=1D𝑡
𝑚,¯𝑚;
5 end
6 foreach ¯𝑚∗intop𝑛flows metrics with highest flow strength in F𝑚,:do
7𝑐cause←component index of 𝑚;
8𝑐effect←component index of ¯𝑚∗;
9 add flow from 𝑐cause to𝑐effect with strength F𝑚,¯𝑚toF;
10 end
11end
12returnF
the third column. Algorithm 2 describes how the dominant influ-
ence flows are extracted. As illustrated, memory, sockets, load, and
processes are the top four anomalous causal components, primarily
propagating anomalies to components such as disk I/O and CPU,
resulting in an I/O bottleneck on the faulty host.
2.4.4 Time-aligned Propagation Network. Mapping out impact paths
is a highly intuitive approach to understanding fault propagation
 
146FaultInsight: Interpreting Hyperscale Data Center Host Faults KDD ’24, August 25–29, 2024, Barcelona, Spain
cpu.nicecpu.iowaitmem.buffersmem.cachedmem.kernelFilesAllocmem.dirtyss.estabudp.inDatagramsudp.noPorts
udp.outDatagramsio.writeMergedio.svctmio.avgWaitio.writeio.svctmio.read
proc.blockedproc.runningproc.all14:23:5214:21:4514:16:5614:16:5014:20:2014:28:5314:17:0414:23:4214:23:51
14:24:0014:24:2214:23:0514:25:2614:21:4714:27:2614:23:36
14:19:4214:22:4614:28:02CPULoadMemorySocketTCPUDPFile SystemDisk I/OProcessNetwork
Figure 9: The time-aligned propagation network.
mechanisms. Fig. 9 shows the time-aligned propagation network
generated by FaultInsight for the running case. The network merges
critical propagation chains together and aligns the anomalous met-
rics strictly in the temporal order to reveal the fault evolution
process. The representation of nodes and edges in the network
carries the same meaning as in the metric-level overview graph
(Section 2.4.2). We impose two constraints on causal influence edges
from𝑖to𝑗in fault dynamicsDto filter out invalid impact paths:
the peak propagation time T𝑖should not be later than T𝑗, and an
effect metric 𝑖should not impact a cause metric 𝑗. We perform beam
search on top cause metrics 𝑛metrics as entry nodes to mine fault
propagation chains and then merge top 𝑛chains chains to build the
time-aligned propagation network. The score 𝜙(C)of a chain𝐶is:
𝜙(C)=(Ö
𝑚∈Vmax(SΣ
𝑚,¯SΣ
𝑚)Ö
𝑒(𝑚𝑖,𝑚𝑗)∈E(𝑇∑︁
𝑡=1D𝑡
𝑚𝑖,𝑚𝑗))1/|C|(14)
whereVandEdenote all nodes and edges in the chain. 𝜙(C)calcu-
late the geometric mean of accumulated influence scores of nodes
and accumulated causal influence of edges along the path of 𝐶.
More implementation details on the generation of propagation net-
works are provided in Appendix B. We adopt the Reingold-Tilford
graph layout algorithm [ 33] to present the network, exposing the
temporal order of fault propagation to facilitate interpretation. As
illustrated, the anomalies propagate through the impact paths in
the network from cause metrics to effect metrics. ss.estab and
memory-related metrics are positioned early in the fault lifecycle
whereas the disk I/O-related metrics are placed later.
3 EVALUATIONS
This section evaluates FaultInsight with other approaches on root
cause metrics identification. In particular, we would like to answer
the following research questions: RQ1: How accurate is FaultIn-
sight’s diagnostic result compared with other SOTA approaches?
RQ2: How computationally efficient is FaultInsight compared to
other methods? Can FaultInsight be deployed in a production en-
vironment? RQ3: How do hyperparameter settings in the fault dy-
namic analysis stage affect our approach? RQ4: How does each
module of FaultInsight contribute to the diagnostic results?
3.1 Experimental Settings
3.1.1 Dataset. The evaluated dataset contains dozens of real in-
cidents that occurred in our company’s production environment.
The cases are reported to the SRE team, and we are involved in the
troubleshooting process. Each case is labeled with one or more root
cause metrics based on the conclusions drawn from official reviews.The number of raw metrics of each case ranges from hundreds to
thousands. Missing, constant, and highly correlated metrics are
removed in the data preprocessing. In our dataset, the number of
heterogeneous metrics for many fault cases can exceed over 130.
Each kind of metric is sampled every 30 seconds. The average and
maximum duration of each case is 34 and 65.5 minutes, respectively.
The average number of labeled root cause metrics is 1.5.
3.1.2 Baselines. We compare FaultInsight with two categories of
approaches. 1) Service-level fault diagnosis: 𝜖-diagnosis [ 37] is based
on similarity measure while the other four methods leverage causal
dependency graphs. DyCause [ 31] and CloudRanger [ 45] are statis-
tical methods. RCD [ 13] and CIRCA [ 18] use causal intervention.
2)Time series causal discovery: We evaluate two statistical methods,
Granger causality test [ 11] and PCMCI [ 34], and two neural net-
work methods, cMLP [ 41] and TCDF [ 29]. We use PageRank [ 30] to
rank root cause metrics on the static causality graph generated by
these methods. A grid search is conducted on each baseline and the
results are based on the parameter set with the highest RankScore.
Implementation details of baselines are provided in Appendix E.
3.1.3 Implementation. Given the scale of our dataset, we use 3
stacks of TCN blocks in the encoder and decoder. We set the kernel
size of TCNs to 6, number of latent channels 𝐶to 4, the learning
rate to 0.001, 𝑛epochs to 500,𝑛pertto 128, the smoothness parameter
𝛼to 0.95, and the temporal discount 𝛽to 0.85. Please find our
implementation with sample data of FaultInsight at https://github
.com/etigerstudio/FaultInsight.
3.1.4 Evaluation Metrics. We use top hit rate PR@k% [25] and
RankScore [31] to evaluate the diagnostic accuracy of root cause
identification. Larger PR@k% or RankScore indicates a higher rank-
ing of the root cause metrics. Let 𝑅outbe the output list of root
cause metrics, 𝑅out
𝑖denote the culprit KPI ranked at 𝑖,𝑅true
𝑖be the
truth set of root causes, and rank(𝑟)denote the rank of KPI 𝑟in
𝑅pred.Avg.PR is the mean value of PR@2.5%, PR@5%, ..., PR@25%.
PR@k% and RankScore are calculated as:
PR@𝑘%=Í⌈𝑁𝑘/100⌉
𝑖=1𝑅pred
𝑖∈𝑅true
min(|𝑅true|,⌈𝑁𝑘/100⌉)∗100% (15)
RankScore =1
|𝑅true|∑︁
𝑟∈𝑅true𝑠(𝑟)∗100%, (16)
𝑠(𝑟)= 
𝑁−max(0,rank(𝑟)−|𝑅true|)
𝑁,if𝑟∈𝑅pred
0,otherwise(17)
3.2 Diagnostic Accuracy (RQ1)
We run root cause identification on each case of our dataset with
FaultInsight and baselines. FaultInsight is initialized in ten random
seeds and the average accuracy is reported. Table 2 reports the
result, where we see service-level fault diagnosis methods failed to
identify root causes in host fault cases effectively. Four causal de-
pendency graphed-based baselines were significantly impacted by
inaccurate causal structures learned, whereas 𝜖-diagnosis achieved
the most balanced performance. Time series causal discovery meth-
ods scored higher in RankScore, but they showed a low hit rate for
root cause metrics among the top candidates. FaultInsight provided
remarkably higher PR@k% and RankScore than all baselines.
 
147KDD ’24, August 25–29, 2024, Barcelona, Spain Tingzhu Bi et al.
Table 2: Accuracy and time cost of FaultInsight and baselines.
Method PR@2.5% PR@5% PR@7.5% PR@10% PR@12.5% PR@15% PR@17.5% PR@20% PR@22.5% PR@25% Avg.PR RS. Time
𝜖-diagnosis [37] 13.5 13.5 15.4 19.2 26.3 30.1 30.1 30.1 30.1 32.1 24.0 45.5 4.8s
RCD [13] 3.8 3.8 7.7 7.7 7.7 7.7 9.6 9.6 9.6 9.6 7.7 13.8 18.9s
CIRCA [18] 5.8 9.6 9.6 9.6 9.6 11.5 11.5 19.2 26.3 28.2 14.1 48.1 40.1s
DyCause [31] 3.8 7.7 9.6 9.6 9.6 17.3 19.2 21.2 21.2 26.9 14.6 43.2 20.7s
CloudRanger [45] 13.5 15.4 23.1 25.0 25.0 32.7 32.7 32.7 32.7 32.7 26.5 30.6 184.6s
Granger [11] 0.0 5.8 21.2 25.0 25.0 26.9 32.7 36.5 40.4 40.4 25.4 64.7 11.1s
PCMCI [34] 5.8 5.8 17.3 17.3 21.2 23.1 30.8 30.8 39.7 41.7 23.3 58.5 45.0s
cMLP [41] 7.7 11.5 17.3 26.9 39.7 41.7 41.7 41.7 41.7 45.5 31.5 66.9 137.5s
TCDF [29] 5.8 11.5 15.4 17.3 25.0 30.8 34.6 35.9 37.8 39.7 25.4 57.3 669.9s
FaultInsight 28.6 37.8 47.9 59.3 66.5 74.2 79.6 83.7 87.5 89.0 65.4 88.5 26.5s
-diagnosis
RCD CIRCA DyCause CloudR. Granger PCMCI cMLP TCDF FaultInsight
Diagnostic Methods101
100101102103104Time Cost (s)
Figure 10: The distributions of the time costs of all methods.
3.3 Efficiency Analysis (RQ2)
Comparison of average time costs. The average time costs of all
methods are reported in Table 2. 𝜖-diagnosis, based on similarity
statistics, is the fastest method. CloudRanger utilizes the inefficient
PC algorithm to build causal dependency graphs and is the slowest
statistical-based method. Neural network methods cMLP and TCDF
are also slow because they train separate prediction models for all
metrics. Despite being a neural network method, FaultInsight scales
well with large-scale metrics and delivers fast, in-time diagnosis
due to its network architecture design that shares the parameters
of the TCN autoencoder.
Comparison of the distributions of time costs. Fig. 10 shows the
distributions of the time costs of all methods in a violin plot, where
the width of each curve corresponds with the approximate fre-
quency of time costs in each region. Additionally, box-and-whisker
plots are drawn in the violin interiors, with the white horizontal bar
showing the medians of time costs and the top and bottom of the
dark gray box showing the upper and lower quantiles. As shown,
FaultInsight exhibits the smallest spread of time costs across all
cases of different data scales and is the most stable method.
Scalability on large scale datasets. To test the scalability of meth-
ods, we build a standard normal distribution-filled synthetic dataset
with𝑇=80and𝑁={50,100,150,200,250,300,400,500}. The left
of Fig. 11 shows the time costs of all methods on the synthetic
dataset. As shown, the advantage of the efficiency of FaultInsight
becomes more prominent on cases with large-scale metrics, where
the time costs of FaultInsight remain under 50 seconds whereas the
time costs of most other baseline methods exceed 100 seconds. The
right of Fig. 11 shows a breakup analysis of time cost of FaultInsight,
where the time cost of FaultInsight is broke down into three parts.
As shown, with increasingly larger 𝑁, the rise of the time cost of
causal graph extracting is more significant than the other two parts,
namely model training and dynamic metric influence scoring, due
to itsO(𝑁2)time complexity.
50 100 200 300 400 500
Number of Metrics110100100010000 Time Cost (s)
-diagnosis
RCD
CIRCA
DyCause
CloudRanger
Granger
PCMCI
TCDF
cMLP
FaultInsight
50 100 150 200 250 300 350 400 450 500
Number of Metrics050100150200250300350Time Cost (s)Model Training
Causal Graph Extracting
Dynamic Metric Influence ScoringFigure 11: The time costs of all methods (left) and the time
cost breakup of FaultInsight (right) in synthetic datasets with
different data scales.
Temporal Discount 
0.20.40.60.81.0
Random Teleport Prob 1
0.00.20.40.60.8RankScore
84%85%86%86%86%87%88%88%88%
85.586.086.587.087.588.088.5
Temporal Discount 
0.20.40.60.81.0
Random Teleport Prob 1
0.00.20.40.60.8PR@Top10%
42%44%46%48%50%52%54%56%58%
42.545.047.550.052.555.057.5
Figure 12: The accuracy of FaultInsight with different hyper-
parameter settings in the fault dynamics analysis stage.
3.4 Sensitivity Analysis (RQ3)
Fig. 12 analyzes the sensitivity on 𝛼and𝛽. Random teleportation
probability 1−𝛼introduces stochasticity to dynamic metric in-
fluence scoring. Temporal discount 𝛽in Equation 10 controls the
temporal locality of the causal influence among metrics. FaultIn-
sight tends to prioritize delayed anomalous impact with large 𝛽
values and pays attention to fault propagations that occur over
a short interval with small 𝛽values. We conduct a grid search
on𝛼∈[0.1,0.99]and𝛽∈[0.1,0.99]. As shown, the top hit rate
PR@10%, ranging from 42% to 60%, drops significantly for some
settings.𝛼and𝛽around 0.85 and 0.95 respectively produce the best
result, which enforces FaultInsight to traverse the impact paths
implied in the fault dynamics throughout the fault evolution in
short time spans.
3.5 Ablation Study (RQ4)
We compare FaultInsight with two variants: no reconstruction dis-
ables the reconstruction task introduced in Section 2.2.3 and no
regularization disables the structured sparsity regularization in Sec-
tion 2.2.5. On the left side of Fig. 13, the evolution of accuracy over
2,000 epochs of FaultInsight with full capability and two other vari-
ants is shown. On the right side are box plots of accuracy at epoch
 
148FaultInsight: Interpreting Hyperscale Data Center Host Faults KDD ’24, August 25–29, 2024, Barcelona, Spain
0 250 500 750 1000 1250 1500 1750 2000
Epochs020406080100 Diagnostic Accuracy %
RankScore - Full
RankScore - No Reconstruction
RankScore - No Regularization
PR@10% - Full
PR@10% - No Reconstruction
PR@10% - No Regularization
Full No Reconstruction No Regularization020406080100 Diagnostic Accuracy %RankScore
PR@10%
Figur
e 13: Accuracy of FaultInsight in different variants.
500, illustrating the distribution of accuracy in each case and mark-
ing the median accuracy in the center of the boxes. We observe that
both modules contribute significantly to the final diagnostic results.
This aligns with our earlier discussion, where we highlighted how
the TCN autoencoder disentangles heterogeneous metrics, while
structured sparsity regularization emphasizes dominant relation-
ships among large-scale metrics.
4 RELATED WORK
Fault diagonsis in hyperscale data centers. Fault diagnosis plays a
crucial role in performance troubleshooting in hyperscale data cen-
ters as it serves as the next step after fault detection, acting as
both a guide for identifying the root cause and providing clues
for effective fault remediation [ 32,44]. Infrastructure monitoring
collects, aggregates, stores metrics, event logs, and tracings [ 5] of
a wide range of hardware and software components in data cen-
ters, including networking [ 1,48], physical host servers [ 19,22],
deployed web services, server infrastructure configuration [ 47], cen-
ter infrastructure management (DCIM) [ 2,17], etc. Prior host-level
fault diagnosis methods mainly focus on coarse-grained anomalous
host or component localization. For example, FluxRank [ 22] utilizes
machine learning-based ranking models to localize the root cause
hosts after the occurrence of performance incidents. DejaVu [ 19]
trains model with historical failures and depends on the model and
component dependencies to pinpoint abnormal components and
their associated metrics on a given anomalous host. We believe
fine-grained fault analysis on the metric-level fault impact links
and more interpretable fault analytic representations would fur-
ther facilitate the tedious and complicated postmortem diagnosis
process.
Service-level fault diagnosis. Most metric-based service-level fault
diagnosis methods rely on service dependency graphs [ 7,15,20,24,
25,31,42,45]. The diagnostic process of these methods can be bro-
ken down to two stages: constructing service dependency graphs
and deriving diagnosis results from these graphs. Methods like
MonitorRank [ 15], MicroHECL [ 21], and CIRCA [ 18] use service
topology as the service dependency graph, whereas other methods
like CloudRanger [ 45], AutoMAP [ 25], and DyCause [ 31] try to
reconstruct the topology through statistical correlation analysis
or causal discovery. CloudRanger uses the conditional indepen-
dence test-based PC causal discovery algorithm to extract intra-
correlations between services and perform second-order random
walks to identify culprit metrics. DyCause uses a sliding-window
Granger causality test to build the causal graph among input metrics
and ranks root cause metrics with the backtrace breadth-first search.
Recent methods like CIRCA and RCD [ 13] use causal intervention
the identify root cause nodes. RCD uses 𝜓-FCI to construct thecausal graph and transforms the root cause identification problem
into finding the interventional target that changes the distribution
of the affected node. There are also methods that do not involve
service dependency graphs. 𝜖-diagnosis [ 37] classifies the perfor-
mance metrics into normal samples and abnormal samples and
finds the significantly changed time series metrics as root-cause
metrics by measuring the similarity of time series. In the scenario
of host-level fault diagnosis, there is no such service topology avail-
able, and the dependency graph reconstruction methods designed
to tackle homogeneous metrics also struggle with heterogeneous
host metrics. More importantly, the only diagnostic representation
of current work is a list of root cause metrics. This inspires us to
further consider why we can’t generate high-level insights and
encapsulate these insights in a more interpretable representation.
Time Series Causal discovery. Granger causality test [ 12] is one
of the most prevailing algorithms for time series causal discov-
ery based on forecast error. Another category of constraint-based
algorithms, such as PCMCI [ 34], leverage iterative conditional inde-
pendence tests to construct the causal graph step-by-step. A range
of recent advances in Granger causality harness the flexibility and
representational power of neural networks. Common neural net-
work architectures such as multilayer perceptrons (MLPs) [ 26,41],
recurrent neural networks (RNNs) [ 14,41], CNNs [ 29] are used as
networks backbones. The causal structures are inferred from the
learned prediction networks afterward. However, the aforemen-
tioned methods train separate component-wise prediction models
for each time series in multivariate input series, resulting in poor
computational efficiency and not meeting the requirements for
timely diagnosis on large-scale heterogeneous host metrics.
5 CONCLUSION
This paper presents FaultInsight, a highly interpretable deep causal
diagnostic framework specifically designed for addressing host-
level faults in hyperscale data centers, an area that is currently
lacking in research. FaultInsight interprets the host fault case with
visualized diagnostic insights from various perspectives, aimed at
reducing human effort in troubleshooting. We propose a dynamic
causal discovery method that captures the underlying fault dynam-
ics using heterogeneous host metrics from distinct system compo-
nents, along with a dynamic metric influence scoring method to
analyze the anomalous influence within the captured fault dynam-
ics. Throughout this paper, we illustrate the concept of dynamic
metric influence and three proposed insights using a real-world
running case. The evaluation demonstrates the accuracy, computa-
tional efficiency, and effectiveness of FaultInsight.
ACKNOWLEDGMENTS
This work is partially supported by the National Natural Science
Foundation of China (62072006, 92167104), Qiyuan Lab Innova-
tion Fund (S20210201079), Foundation of Shuanghu Laboratory
(2024JK15) and ByteDance University Research Project.
REFERENCES
[1]Behnaz Arzani, Selim Ciraci, Luiz Chamon, Yibo Zhu, Hongqiang Harry Liu, Jitu
Padhye, Boon Thau Loo, and Geoff Outhred. 2018. 007: Democratically finding
the cause of packet drops. In 15th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 18). 419–435.
 
149KDD ’24, August 25–29, 2024, Barcelona, Spain Tingzhu Bi et al.
[2]Sahar Asgari, Rohit Gupta, Ishwar K Puri, and Rong Zheng. 2021. A data-driven
approach to simultaneous fault detection and diagnosis in data centers. Applied
Soft Computing 110 (2021), 107638.
[3]Shaojie Bai, J Zico Kolter, and Vladlen Koltun. 2018. An empirical evaluation
of generic convolutional and recurrent networks for sequence modeling. arXiv
preprint arXiv:1803.01271 (2018).
[4]Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy. 2016. Site
reliability engineering: How Google runs production systems. " O’Reilly Media,
Inc.".
[5]Betsy Beyer, Niall Richard Murphy, David K Rensin, Kent Kawahara, and Stephen
Thorne. 2018. The site reliability workbook: practical ways to implement SRE. "
O’Reilly Media, Inc.".
[6]Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual
web search engine. Computer networks and ISDN systems 30, 1-7 (1998), 107–117.
[7]Pengfei Chen, Yong Qi, Pengfei Zheng, and Di Hou. 2014. CauseInfer: Auto-
matic and distributed performance diagnosis with hierarchical causality graph in
large distributed systems. In 2014 IEEE Conference on Computer Communications,
INFOCOM 2014, Toronto, Canada, April 27 - May 2, 2014. IEEE, 1887–1895.
[8]Yingnong Dang, Qingwei Lin, and Peng Huang. 2019. AIOps: real-world chal-
lenges and research innovations. In 2019 IEEE/ACM 41st International Conference
on Software Engineering: Companion Proceedings (ICSE-Companion). IEEE, 4–5.
[9]Jianru Ding, Ruiqi Cao, Indrajeet Saravanan, Nathaniel Morris, and Christopher
Stewart. 2019. Characterizing service level objectives for cloud services: Realities
and myths. In 2019 IEEE International Conference on Autonomic Computing (ICAC).
IEEE, 200–206.
[10] Thomas MJ Fruchterman and Edward M Reingold. 1991. Graph drawing by force-
directed placement. Software: Practice and experience 21, 11 (1991), 1129–1164.
[11] Clive WJ Granger. 1969. Investigating causal relations by econometric models
and cross-spectral methods. Econometrica: journal of the Econometric Society
(1969), 424–438.
[12] Clive W. J. Granger and Ramu Ramanathan. 1984. Improved methods of combin-
ing forecasts. Journal of Forecasting 3, 2 (1984), 197–204.
[13] Azam Ikram, Sarthak Chakraborty, Subrata Mitra, Shiv Saini, Saurabh Bagchi,
and Murat Kocaoglu. 2022. Root Cause Analysis of Failures in Microservices
through Causal Discovery. Advances in Neural Information Processing Systems 35
(2022), 31158–31170.
[14] Saurabh Khanna and Vincent YF Tan. 2019. Economy statistical recurrent units
for inferring nonlinear granger causality. arXiv preprint arXiv:1911.09879 (2019).
[15] Myunghwan Kim, Roshan Sumbaly, and Sam Shah. 2013. Root cause detection
in a service-oriented architecture. ACM SIGMETRICS Performance Evaluation
Review (2013), 93–104.
[16] Amy N Langville and Carl D Meyer. 2005. A survey of eigenvector methods for
web information retrieval. SIAM review 47, 1 (2005), 135–161.
[17] Guannan Li, Yunpeng Hu, Jiangyan Liu, Xi Fang, and Jing Kang. 2020. Review on
fault detection and diagnosis feature engineering in building heating, ventilation,
air conditioning and refrigeration systems. IEEE Access 9 (2020), 2153–2187.
[18] Mingjie Li, Zeyan Li, Kanglin Yin, Xiaohui Nie, Wenchi Zhang, Kaixin Sui, and
Dan Pei. 2022. Causal inference-based root cause analysis for online service
systems with intervention recognition. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining. 3230–3240.
[19] Zeyan Li, Nengwen Zhao, Mingjie Li, Xianglin Lu, Lixin Wang, Dongdong Chang,
Xiaohui Nie, Li Cao, Wenchi Zhang, Kaixin Sui, et al .2022. Actionable and
interpretable fault localization for recurring failures in online service systems. In
Proceedings of the 30th ACM Joint European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 996–1008.
[20] JinJin Lin, Pengfei Chen, and Zibin Zheng. 2018. Microscope: Pinpoint perfor-
mance issues with causal graphs in micro-service environments. In International
Conference on Service-Oriented Computing. Springer, 3–20.
[21] Dewei Liu, Chuan He, Xin Peng, Fan Lin, Chenxi Zhang, Shengfang Gong, Ziang
Li, Jiayu Ou, and Zheshun Wu. 2021. Microhecl: High-efficient root cause local-
ization in large-scale microservice systems. In 2021 IEEE/ACM 43rd International
Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP).
IEEE, 338–347.
[22] Ping Liu, Yu Chen, Xiaohui Nie, Jing Zhu, Shenglin Zhang, Kaixin Sui, Ming
Zhang, and Dan Pei. 2019. Fluxrank: A widely-deployable framework to auto-
matically localizing root cause machines for software service failure mitigation.
In2019 IEEE 30th International Symposium on Software Reliability Engineering
(ISSRE). IEEE, 35–46.
[23] Chen Luo, Jian-Guang Lou, Qingwei Lin, Qiang Fu, Rui Ding, Dongmei Zhang,
and Zhe Wang. 2014. Correlating events with time series for incident diagnosis.
InProceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining. 1583–1592.
[24] Meng Ma, Weilan Lin, Disheng Pan, and Ping Wang. 2021. ServiceRank: Root
Cause Identification of Anomaly in Large-Scale Microservice Architecture. IEEE
Transactions on Dependable and Secure Computing (2021).
[25] Meng Ma, Jingmin Xu, Yuan Wang, Pengfei Chen, Zonghua Zhang, and Ping
Wang. 2020. AutoMAP: Diagnose Your Microservice-based Web Applications
Automatically. In WWW ’20: The Web Conference 2020, Taipei, Taiwan, April 20-24,2020. ACM / IW3C2, 246–258.
[26] Ričards Marcinkevičs and Julia E Vogt. 2020. Interpretable Models for Granger
Causality Using Self-explaining Neural Networks. In International Conference on
Learning Representations.
[27] Leonardo Mariani, Cristina Monni, Mauro Pezzé, Oliviero Riganelli, and Rui
Xin. 2018. Localizing faults in cloud systems. In 2018 IEEE 11th International
Conference on Software Testing, Verification and Validation (ICST). IEEE, 262–273.
[28] Carl D Meyer. 2023. Matrix analysis and applied linear algebra. Vol. 188. Siam.
[29] Meike Nauta, Doina Bucur, and Christin Seifert. 2019. Causal discovery with
attention-based convolutional neural networks. Machine Learning and Knowledge
Extraction 1, 1 (2019), 19.
[30] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The
PageRank citation ranking: Bringing order to the web. Technical Report. Stanford
InfoLab.
[31] Yicheng Pan, Meng Ma, Xinrui Jiang, and Ping Wang. 2021. Faster, deeper, easier:
crowdsourcing diagnosis of microservice kernel failure from user space. In ISSTA
’21: 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,
Virtual Event, Denmark, July 11-17, 2021. ACM, 646–657.
[32] Yining Qi, Chongrong Fang, Haoyu Liu, Daxiang Kang, Biao Lyu, Peng Cheng,
and Jiming Chen. 2021. A survey of cloud network fault diagnostic systems and
tools. Frontiers of Information Technology & Electronic Engineering 22, 8 (2021),
1031–1045.
[33] Edward M. Reingold and John S. Tilford. 1981. Tidier drawings of trees. IEEE
Transactions on software Engineering 2 (1981), 223–228.
[34] Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdi-
novic. 2019. Detecting and quantifying causal associations in large nonlinear
time series datasets. Science advances 5, 11 (2019), eaau4996.
[35] Mario Schmidt. 2008. The Sankey diagram in energy and material flow manage-
ment: part II: methodology and current applications. Journal of industrial ecology
12, 2 (2008), 173–185.
[36] Skipper Seabold and Josef Perktold. 2010. Statsmodels: Econometric and statistical
modeling with python. In Proceedings of the 9th Python in Science Conference,
Vol. 57. Austin, TX, 10–25080.
[37] Huasong Shan, Yuan Chen, Haifeng Liu, Yunpeng Zhang, Xiao Xiao, Xiaofeng He,
Min Li, and Wei Ding. 2019. 𝜖-diagnosis: Unsupervised and real-time diagnosis
of small-window long-tail latency in large-scale microservice platforms. In The
World Wide Web Conference. 3215–3222.
[38] Ali Shojaie and Emily B Fox. 2022. Granger causality: A review and recent
advances. Annual Review of Statistics and Its Application 9 (2022), 289–319.
[39] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. 2000.
Causation, prediction, and search. MIT press.
[40] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.
[41] Alex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, and Emily B Fox. 2021. Neural
granger causality. IEEE Transactions on Pattern Analysis and Machine Intelligence
44, 8 (2021), 4267–4279.
[42] Jörg Thalheim, Antonio Rodrigues, Istemi Ekin Akkus, Pramod Bhatotia,
Ruichuan Chen, Bimal Viswanath, Lei Jiao, and Christof Fetzer. 2017. Sieve:
actionable insights from monitored metrics in distributed systems. In Proceed-
ings of the 18th ACM/IFIP/USENIX Middleware Conference, Las Vegas, NV, USA,
December 11 - 15, 2017. ACM, 14–27.
[43] Mark J Van der Laan. 2006. Statistical inference for variable importance. The
International Journal of Biostatistics 2, 1 (2006).
[44] Chengwei Wang, Soila Kavulya, Jiaqi Tan, Liting Hu, Mahendra Kutare, Michael P.
Kasick, Karsten Schwan, Priya Narasimhan, and Rajeev Gandhi. 2013. Perfor-
mance troubleshooting in data centers: an annotated bibliography? ACM SIGOPS
Oper. Syst. Rev. 47, 3 (2013), 50–62.
[45] Ping Wang, Jingmin Xu, Meng Ma, Weilan Lin, Disheng Pan, Yuan Wang, and
Pengfei Chen. 2018. CloudRanger: Root Cause Identification for Cloud Native
Systems. In 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid
Computing, CCGRID 2018, Washington, DC, USA, May 1-4, 2018. IEEE Computer
Society, 492–502.
[46] Li Wu, Johan Tordsson, Erik Elmroth, and Odej Kao. 2020. MicroRCA: Root Cause
Localization of Performance Issues in Microservices. In NOMS 2020 - IEEE/IFIP
Network Operations and Management Symposium, Budapest, Hungary, April 20-24,
2020. IEEE, 1–9.
[47] Guangquan Xu, Xinru Ding, Sihan Xu, Yan Jia, Shaoying Liu, Shicheng Feng, and
Xi Zheng. 2023. Real-Time Diagnosis of Configuration Errors for Software of AI
Server Infrastructure. IEEE Transactions on Dependable and Secure Computing
(2023).
[48] Minlan Yu, Albert Greenberg, Dave Maltz, Jennifer Rexford, Lihua Yuan, Srikanth
Kandula, and Changhoon Kim. 2011. Profiling network performance for multi-
tier data center applications. In 8th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 11).
[49] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding con-
volutional networks. In Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13. Springer, 818–833.
 
150FaultInsight: Interpreting Hyperscale Data Center Host Faults KDD ’24, August 25–29, 2024, Barcelona, Spain
A DYNAMIC METRIC INFLUENCE SCORE
COMPUTATION
Algorithm 3 shows the computation procedure for dynamic outward
influence scoresS. Lines 1 to 12 construct a transition probability
matrix Pcontaining𝑁𝑇×𝑁𝑇elements based on input fault dynam-
ics.P(𝑖,𝑡′),(𝑗,𝑡)denotes the transition probability from metric 𝑖at𝑡′
to metric𝑗at𝑡. Lines 13 to 18 adjusts Pto ensure the irreducibility
ofP[16]. According to the Perron–Frobenius theorem [ 28], the
irreducibility of a transition probability matrix of a Markov chain
guarantees that a Markov chain possesses a unique and positive
stationary distribution vector 𝝅. Line 19 initializes the 𝝅with equal
initial transition probabilities on all states in the Markov chain.
Lines 20 to 25 compute the stationary distribution vector 𝝅un-
til convergence with power method. Lines 27 to 31 unpack Pto
desiredSelement-by-element. To compute dynamic outward influ-
ence scores ¯S, the causal and temporal direction should be reversed
from Lines 6 to 7.
Algorithm 3: Dynamic Outward Influence Score Computation
Input:𝑇-step fault dynamics D, convergence error tolerance 𝜖
Output: Dynamic outward influence scores S
// Construct transition probability matrix P
1P←0𝑁𝑇×𝑁𝑇
2for𝑖←1to𝑁do
3 for𝑡′←1to𝑇do
4 for𝑗←1to𝑁do
5 for𝑡←1to𝑇do
6 if𝑡≥𝑡′then
7 P(𝑖,𝑡′),(𝑗,𝑡)←𝛾(𝑡−𝑡′)D𝑖,𝑗,𝑡Í𝑁
𝑘=1D𝑘,𝑗,𝑡
8 end
9 end
10 end
11 end
12end
// Convert Pto a right stochastic matrix
13for𝑗←1to𝑁do
14 for𝑡←1to𝑇do
15 P(:,:),(𝑗,𝑡)←P(:,:),(𝑗,𝑡)/Í𝑁
𝑖=1Í𝑇
𝑡′=1P(𝑖,𝑡′),(𝑗,𝑡)
16 end
17end
// Ensure irreducibility of Pwith random teleportation
18P←𝛼P+(1−𝛼)E
// Initialize vector 𝝅
19𝝅T
0←[1
𝑁𝑇]1×𝑁𝑇
20𝑘←0
// Compute the dominant eigenvector 𝝅ofPwith power method
21repeat
22𝑘←𝑘+1
23 𝝅T
𝑘←𝝅T
𝑘−1P
24𝐸←∥𝝅T
𝑘−𝝅T
𝑘−1∥1;// Error between consecutive iterations
25until𝐸<𝑁𝑇𝜖 // Check convergence;
// Extract dynamic outward scores S𝑁×𝑇from 𝝅1×𝑁𝑇
26S← 0𝑁×𝑇
27for𝑖←1to𝑁do
28 for𝑡←1to𝑇do
29S𝑖,𝑡←(𝝅T
𝑘)𝑖𝑁+𝑡
30 end
31end
32returnS
B FAULT PROPAGATION NETWORK
GENERATION
Algorithm 2 outlines the procedure for generating fault propaga-
tion networks. Lines 1 to 8 enforce the two constraints previouslymentioned to filter causal influence edges: influences must occur
in chronological order and follow a cause-to-effect direction. Lines
9 to 14 construct the propagation network using the top propaga-
tion chains mined through a variation of the breadth-first search
algorithm, detailed in lines 15 to 27.
Algorithm 4: Fault Propagation Network Generation
Input:𝑇-step failure dynamics D, accumulated outward scores SΣ, metric
rolesR, peak propagation timestamps T, beam width 𝑤, minimum
propagation path length 𝑙, number of top metrics to search from
𝑛metrics , number of focused top propagation chains 𝑛chains
Output: Temporal fault propagation network N
//❶Build the fault propagation graph
1G←0𝑁×𝑁;
2for𝑖←1to𝑁do
3 for𝑗←1to𝑁do
4 ifT𝑖<T𝑗and not(R𝑖iseffect metric andR𝑗iscause metric)
then
5 G𝑖,𝑗←(Í𝑇
𝑡=1D𝑡
𝑖,𝑗)·(max(SΣ
𝑗,¯SΣ
𝑗));
6 end
7 end
8end
//❷Mine propagation chains from top cause metrics
9C←∅ ;
10foreach𝑚intop𝑛metrics metrics with highest outward scores in SΣdo
11C←C∪ mine_critical_propagation_chains(G ,𝑚,SΣ
𝑚);
12end
//❸Build propagation network with top chains
13N← merge top𝑛chains propagation chains with highest scores in C;
14returnN
15Function mine_critical_propagation_chains(G ,𝑚,𝑠 ):
16 Q←new Queue();C←∅ ;
17 Q.enqueue(new State(terminal= 𝑚,score=𝑠,path=⟨𝑚⟩));
18 repeat
19(𝑚′,𝑠′,𝑝′)← Q.dequeue();
// Record valid propagation chains
20 if|𝑝′|≥𝑙andR𝑚′iseffect metric then
21C←C∪( new Chain(path=𝑝′,score=|𝑝′|√
𝑠′));
22 end
// Expand subsequent top- 𝑤propagation chains
23 foreach𝑚∗intop𝑤metrics with highest weight in G𝑚′,:do
24 Q.enqueue(terminal= 𝑚∗,score=𝑠·G𝑚′,𝑚∗,path=𝑝′⊕𝑚∗);
25 end
26 untilQ.is_empty() // All valid chains have been visited;
27 returnC
C FULL METRIC-LEVEL OVERVIEW GRAPH
ON THE RUNNING CASE
Fig. 14 depicts the metric-level overview graph of the running case
with all 92 metrics. Multiple I/O devices ( io1&io2) are mounted,
and various file system directories ( fs1&fs2) are monitored on the
host. The nodes in the graph can be categorized into three groups.
First, major cause metrics of this case, represented as large cir-
cle nodes such as ss.estab ,load.1min ,proc.blocked (indicating
the number of blocked processes), and mem.available , are posi-
tioned on the left and right interior of the graph. Among these, the
root cause metric ss.estab exhibits the highest outward influence
score.
Next, major effect metrics, illustrated as large triangle nodes
such as io1.svctm (service time of I/O requests on disk 1) and
io2.write (write requests on disk 2), are nestled between major
cause metrics. This denotes that the I/O bottleneck is a consequence
of the combined effects of surging connections, escalating load, and
insufficient memory. Major cause and effect metrics are clustered
closely together in the center of the graph due to their strong causal
influences.
 
151KDD ’24, August 25–29, 2024, Barcelona, Spain Tingzhu Bi et al.
cpu.usercpu.nicecpu.softirq
cpu.systemcpu.switchescpu.busycpu.iowaitload.1minmem.available
mem.buffersmem.cachedmem.free
mem.shmemmem.swapFreemem.usedmem.kernelFilesAllocmem.memfreemem.writebackmem.dirty
ss.closedss.estabss.orphaned
ss.timeWaittcp.abortOnTimeout
tcp.delayedACKLockedtcp.listenDropstcp.listenOverflowstcp.lostRetransmit
tcp.pkgInsegstcp.pkgOutsegstcp.pkgRetranssegs
tcp.twss.retrans
udp.ignoreMulti
udp.inDatagramsudp.noPorts
udp.outDatagramsfs1.statistics.totalfs1.inodes.free
fs1.bytes.free
fs2.statistics.totalfs2.inodes.freefs2.bytes.freeio1.writeSectors
io1.writeRequestsio1.writeMergedio1.writeBytes
io1.writeio1.w_waitio1.util
io1.svctmio1.readSectors
io1.readRequestsio1.readMergedio1.readBytes
io1.readio1.msecWriteio1.msecWeightedTotalio1.msecTotalio1.msecRead
io1.avgWaitio2.writeSectors
io2.writeRequestsio2.writeMergedio2.writeBytesio2.writeio2.wrWaitio2.utilio2.svctmio2.readSectorsio2.readRequestsio2.readMerged
io2.readBytesio2.readio2.msecWriteio2.msecWeightedTotalio2.msecTotalio2.msecRead
io2.avgWaitproc.blockedproc.runningproc.allproc.processes
net.inBytesnet.inDroppednet.inMulticast
net.inPacketsnet.outBytesnet.outPacketsnet.totalBytesnet.totalDropped
net.totalPacketsCPULoadMemorySocketTCPUDPFile SystemDisk I/OProcessNetwork
Figure 14: The full metric-level overview graph of the running case.
40 60 80 100 120
Number of Metrics2030405060Fault Duration (min)
Figure 15: The scatter plot showing the fault duration and
the number of metrics in each case.
Finally, other metrics are situated in the exterior of the graph,
with most being effect metrics affected by the major cause metrics
in the case, represented as small triangle nodes.
D DETAILS OF THE EVALUATION DATASET
The evaluation dataset comprises 26 unique host fault cases span-
ning various fault categories such as out of memory, packet drop,
disk pressure, etc. As shown in Fig. 15, each scatter point on the
plot represents the fault duration and the number of metrics ineach case. Additionally, the distribution of these two dimensions is
depicted at the top and right of the figure.
E IMPLEMENTATION DETAILS OF OTHER
APPROACHES
Normal samples. 𝜖-diagnosis, RCD, and CIRCA require both normal
and abnormal samples. The metrics data during the fault dura-
tion are used as abnormal samples, and two hours before the fault
occurrence in the same duration are used as normal samples.
Entry metric. DyCause and CloudRanger require diagnostic entry
metrics. In the scenario of server-level fault diagnosis, the entry
metrics are set as anomalous front-end services. We set the entry
metric as load.1min.
Implementations. 𝜖-diagnosis, CIRCA and RCD are implemented
inPyRCA1. DyCause and CloudRanger are implemented in dycause
_rca2. The number of multiprocess workers of DyCause is set to 8.
Granger causality is implemented in the statsmodels library [ 36].
PCMCI [ 34] is implemented in tigramite3using partial correlation
for independence tests. We use the official implementations of
cMLP4[41], and TCDF5[29].
1https://github.com/salesforce/PyRCA
2https://github.com/PanYicheng/dycause_rca
3https://github.com/jakobrunge/tigramite
4https://github.com/iancovert/Neural-GC
5https://github.com/M-Nauta/TCDF
 
152