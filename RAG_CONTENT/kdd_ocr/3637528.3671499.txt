Workshop on Human-Interpretable AI
Gabriele Ciravegna
gabriele.ciravegna@polito.it
Dipartimento di Automatica e
Informatica, Politecnico di Torino
Torino, ItalyMateo Espinoza Zarlenga
University of Cambridge
Cambridge, UKPietro Barbiero
Università della Svizzera Italiana
Lugano, Switzerland
Francesco Giannini
Scuola Normale Superiore
Pisa, ItalyZoreh Shams
University of Cambridge
Cambridge, UKDamien Garreau
Julius-Maximilians-Universität
Würzburg
Würzburg, Germany
Mateja Jamnik
University of Cambridge
Cambridge, UKTania Cerquitelli
Dipartimento di Automatica e
Informatica, Politecnico di Torino
Torino, Italy
Abstract
This workshop aims to spearhead research on Human-Interpretable
Artificial Intelligence (HI-AI) by providing: (i) a general overview
of the key aspects of HI-AI, in order to equip all researchers with
the necessary background and set of definitions; (ii) novel and
interesting ideas coming from both invited talks and top paper
contributions; (iii) the chance to engage in dialogue with promi-
nent scientists during poster presentations and coffee breaks. The
workshop welcomes contributions covering novel interpretable-
by-design or post-hoc approaches, as well as theoretical analysis
of existing works. Additionally, we accept visionary contributions
speculating on the future potential of this field. Finally, we welcome
contributions from related fields such as Ethical AI, Knowledge-
driven Machine learning, Human-machine Interaction, applications
in Medicine and Industry, and analyses from Regulatory experts.
CCS Concepts
•Computing methodologies →Artificial intelligence.
Keywords
Human-Interpretable AI, Interpretability, Explainability, HI-AI, XAI
ACM Reference Format:
Gabriele Ciravegna, Mateo Espinoza Zarlenga, Pietro Barbiero, Francesco Gi-
annini, Zoreh Shams, Damien Garreau, Mateja Jamnik, and Tania Cerquitelli.
2024. Workshop on Human-Interpretable AI. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 2 pages.
https://doi.org/10.1145/3637528.3671499
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.36714991 Introduction
Human-interpretable AI models [ 10] are playing an increasingly
important role in Artificial Intelligence (AI). Today, a large part of
the technologies employed by AI and SIGKDD researchers is based
on Deep Neural Networks (DNNs). Yet, the lack of transparency of
DNNs prevents a safe deployment of these models in critical con-
texts that significantly affect users. Consequently, decision-making
systems based on deep learning are facing constraints and limita-
tions from regulatory institutions [ 4], which increasingly demand
transparency in AI models [ 9]. Even though standard eXplainable
AI (XAI) emerged to address the need to interpret DNNs, several
works are arguing that it may not have achieved its goal [1, 11].
To really explain DNN decision-making process, there is a grow-
ing consensus that human-interpretable explanations are required.
Human-Interpretable AI (HI-AI) methods either provide post-hoc
explanations by extracting the symbols that have been automat-
ically learnt by the models (e.g., T-CAV [ 6]), or directly design
intrinsically interpretable architectures (e.g., CBM [ 8]). Among
other qualities, these explanations resemble better the way humans
reason and explain [ 7], help to detect model biases [ 5], are more
stable to perturbations [ 2], and can create more robust models [ 3].
2 Workshop Topics
Topics of interest include, but are not limited to, the following:
•Explainable-by-design models, novel approaches to cre-
ating machine learning and deep learning models that are
intrinsically explainable or interpretable.
•Post-hoc methods for Interpretable AI, novel approaches
on post-hoc interpretable AI. These include but are not lim-
ited to approaches working on higher-level features such as
concepts.
•Theoretical analyses of existing methods, showing what
existing interpretable methods can achieve both from an
explanation and a generalization point of view.
•Knowledge integration & Reasoning methods injecting
domain knowledge and reasoning methods into deep learn-
ing models to enhance their interpretability and performance.
6708
KDD ’24, August 25–29, 2024, Barcelona, Spain Gabriele Ciravegna et al.
•AI Ethics papers analysing implications of interpretable AI
methods, discussing topics such as fairness, accountability,
transparency, and bias mitigation in AI systems.
•Human-machine Interaction studies on innovative human-
machine interaction systems, successfully exploiting inter-
pretable AI models in their capability to provide both stan-
dard and counter-factual explanations.
•Vision papers on XAI discussing the possible evolutions of
the XAI field or speculating potential interpretable system
and applications with their implications.
•Applications in Medicine and Healthcare applications
of interpretable AI methods in medical diagnosis, treatment
planning, and healthcare decision-making.
•AI in Industry practical applications of interpretable AI
methods in various safety-critical industrial sectors, such as
transportation, finance and retail.
•Legal and Regulatory dissertations discussing and pro-
viding analysis of the legal challenges associated with inter-
pretable AI, including compliance with data protection laws
for transparent and accountable AI systems.
3 Program
This workshop aims to advance the research on HI-AI by offering
a diverse program designed to enhance participants’ knowledge,
and foster collaboration and innovation. The following list contains
the invited speakers who will give keynote talks at the HI-AI work-
shop, and the expected topics that their talks will cover. All invited
speakers have already confirmed their presence.
•Abbas Rahimi, Research Staff Member at IBM Research
Europe - Neuro-symbolic AI, Concept Embeddings.
•Andrea Passerini, Associate Professor at University of
Trento - Concepts in AI and Interactive Machine Learning.
•Sonali Parbhoo, Assistant Professor at Imperial College
London - Concept and causality.
Program Outline. Table 1 reports the workshop program. Firstly,
we will give an overview of the key aspects of HI-AI to ensure all
attendees have a solid understanding of the background concepts
and terminology. Secondly, the workshop features three invited
talks from experts in the field, who will share their insights and lat-
est research findings. These talks will provide valuable perspectives
and inspire new ideas. Thirdly, we will offer participants the chance
to engage in dialogue with prominent scientists during a long coffee
break with poster presentations, encouraging collaborations and
knowledge-sharing. Also, the workshop program includes three
contributed talks from selected contributions. We will recognize the
most interesting contribution with a Best Workshop Paper Award.
We have allocated 40 minutes for each invited talk, allowing for a
30-minute presentation followed by a 10-minute Q&A session. We
allotted the same time for the poster sessions.
4 Paper Management
Paper management. We published the Call For Papers (CFP) on
the workshop website1. The CFP focuses on short papers, which
can be research papers, theoretical analysis papers, or vision papers.
1https://human-interpretable-ai.github.io/8:50 – 9:00 Opening remarks
9:00 – 9:40 Keynote: Stefano Teso
9:40 – 10:00 5 mins lightning talks (3 selected papers)
10:00 – 10:40 Keynote: Abbas Rahimi
10:40 – 11:30 Coffee & Posters
11:30 – 12:10 Keynote: Sonali Parbhoo
12:10 – 12:20 Awards and Closing Remarks
Table 1: Draft of the program outline.
In the case of research contributions, we asked paper authors to
make their code and data openly available to ensure reproducibility.
The review process has been double-blind. We have used OpenRe-
view to ensure the final decisions for each paper are made by the
organisers with no conflict of interest. All accepted papers will be
published on the workshop website, which will remain active and
accessible after the conference concludes. Additionally, we took
contact with an external editor (CEUR-WS) to create an archival
version of these papers for authors who wish to participate in a
subsequent publication.
5 Program Commitee
We are very grateful to each of our program committee members
for their hard reviewing work, namely Romain Giot, Eliana Pastor,
Roberto Pellungrini, Eleonora Poeta, Gianluigi Lopardo, and Gizem
Gezici, besides workshop chairs.
References
[1]Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and
Been Kim. 2018. Sanity checks for saliency maps. Advances in neural information
processing systems 31 (2018).
[2]David Alvarez Melis and Tommi Jaakkola. 2018. Towards robust interpretability
with self-explaining neural networks. Advances in neural information processing
systems 31 (2018).
[3]Gabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Marco Gori, Pietro Lió,
Marco Maggini, and Stefano Melacci. 2023. Logic explained networks. Artificial
Intelligence 314 (2023), 103822.
[4]Bryce Goodman and Seth Flaxman. 2017. European Union regulations on algo-
rithmic decision-making and a “right to explanation”. AI magazine 38, 3 (2017),
50–57.
[5]Rishabh Jain, Gabriele Ciravegna, Pietro Barbiero, Francesco Giannini, Davide
Buffelli, and Pietro Lio. 2022. Extending Logic Explained Networks to Text
Classification. In Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing. Association for Computational Linguistics, 8838–
8857.
[6]Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda
Viegas, et al .2018. Interpretability beyond feature attribution: Quantitative
testing with concept activation vectors (tcav). In International conference on
machine learning. PMLR, 2668–2677.
[7]Sunnie SY Kim, Elizabeth Anne Watkins, Olga Russakovsky, Ruth Fong, and
Andrés Monroy-Hernández. 2023. " Help Me Help the AI": Understanding How
Explainability Can Support Human-AI Interaction. In Proceedings of the 2023 CHI
Conference on Human Factors in Computing Systems. 1–17.
[8]Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pier-
son, Been Kim, and Percy Liang. 2020. Concept bottleneck models. In International
conference on machine learning. PMLR, 5338–5348.
[9]Johann Laux, Sandra Wachter, and Brent Mittelstadt. 2024. Trustworthy artificial
intelligence and the European Union AI act: On the conflation of trustworthiness
and acceptability of risk. Regulation & Governance 18, 1 (2024), 3–32.
[10] Eleonora Poeta, Gabriele Ciravegna, Eliana Pastor, Tania Cerquitelli, and Elena
Baralis. 2023. Concept-based Explainable Artificial Intelligence: A Survey. arXiv
preprint arXiv:2312.12936 (2023).
[11] Cynthia Rudin. 2019. Stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. Nature machine
intelligence 1, 5 (2019), 206–215.
6709