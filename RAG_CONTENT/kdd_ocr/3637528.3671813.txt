Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions
Haoming Li
Shanghai Jiao Tong University
Shanghai, China
wakkkka@sjtu.edu.cnYumou Liu∗
The Chinese University of Hong
Kong, Shenzhen
Shenzhen, China
yumouliu@link.cuhk.edu.cnZhenzhe Zheng†
Shanghai Jiao Tong University
Shanghai, China
zhengzhenzhe@sjtu.edu.cn
Zhilin Zhang
Alibaba Group
Beijing, China
zhangzhilin.pt@alibaba-inc.comJian Xu
Alibaba Group
Beijing, China
xiyu.xj@alibaba-inc.comFan Wu
Shanghai Jiao Tong University
Shanghai, China
fwu@cs.sjtu.edu.cn
ABSTRACT
Online advertising platforms leverage a two-stage auction architec-
ture to deliver personalized ads to users with low latency. The first
stage efficiently selects a small subset of promising candidates out
of the complete pool of ads. In the second stage, an auction is con-
ducted within the subset to determine the winning ad for display,
using click-through-rate predictions from the second-stage machine
learning model. In this work, we investigate the online learning
process of the first-stage subset selection policy, while ensuring
game-theoretic properties in repeated two-stage ad auctions. Specif-
ically, we model the problem as designing a combinatorial bandit
mechanism with a general reward function, as well as additional
requirements of truthfulness and individual rationality (IR). We es-
tablish an Ω(𝑇)regret lower bound for truthful bandit mechanisms,
which demonstrates the challenge of simultaneously achieving allo-
cation efficiency and truthfulness. To circumvent this impossibility
result, we introduce truthful 𝛼−approximation oracles and evaluate
the bandit mechanism through 𝛼−approximation regret. Two mech-
anisms are proposed, both of which are ex-post truthful and ex-post
IR. The first mechanism is an explore-then-commit mechanism with
regret𝑂(𝑇2/3), and the second mechanism achieves an improved
𝑂(log𝑇/Δ2
𝜙)regret where Δ𝜙is a distribution-dependent gap, but
requires additional assumptions on the oracles and information
about the strategic bidders.
CCS CONCEPTS
•Theory of computation →Algorithmic game theory and
mechanism design; Online learning theory; •Information
systems→Online advertising.
∗Work was done during visiting Shanghai Jiao Tong University.
†Zhenzhe Zheng is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671813KEYWORDS
Mechanism Design, Online Learning, Multi-Armed Bandit, Online
Advertising
ACM Reference Format:
Haoming Li, Yumou Liu, Zhenzhe Zheng, Zhilin Zhang, Jian Xu, and Fan
Wu. 2024. Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD ’24), August 25–29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671813
1 INTRODUCTION
Modern online advertising platforms usually serve a vast number
of advertisers, who participate in ad auctions to compete for ad
impressions. In order to select highly relevant ads for users and to
maximize social welfare, the platforms typically rank the ads by
𝑏𝑖𝑐𝑖, where𝑏𝑖is the bid reported by advertiser 𝑖, and𝑐𝑖is the click
through rate (CTR) predicted by machine learning models of the
platform. However, executing complex CTR prediction models [ 6,
33,34] for millions of candidate ads within a limited response time is
often infeasible due to the associated high inference cost. To be able
to deliver highly personalized ads to incoming users in real time, a
widely adopted approach is a two-stage structure [ 13,25,32], which
is also ubiquitous in large-scale online recommendation systems
[7,10,15,20,29]. The first stage focuses on efficiently generating a
subset of candidates that contains enough promising ads. To ensure
low latency, first-stage machine learning models are lightweight and
less accurate [ 21,26]. The selected subset then enters the second
stage, where a sophisticated CTR model provides accurate CTR
predictions. Using those predictions and the submitted bids, an ad
auction is conducted within the subset to determine the winning
ad for display, along with the corresponding payment.
Prior works on two-stage systems has primarily focused on the
performance of subset selection policy in the first stage [ 20,24,
25,32], i.e., how to efficiently select a promising subset of candi-
dates such that the ad allocation performance of the second stage is
guaranteed. Besides, as such two-stage procedures are repeatedly
executed upon sequential arrivals of users, online learning of CTR
in two-stage recommendation systems has also been considered
[16,31]. However, advertising systems differ from recommenda-
tion systems by the involvement of money transfer, and hence the
requirement of game-theoretic properties, e.g., truthfulness and
individual rationality (IR) of auction mechanisms.
 
1565
KDD ’24, August 25–29, 2024, Barcelona, Spain Haoming Li et al.
In this work, we address the challenge of simultaneously incor-
porating online learning of the subset selection policy and game-
theoretic properties in repeated two-stage ad auctions. Concretely,
we consider a repeated auction with 𝑛advertisers and 𝑇rounds,
where, in each round, at most 𝑘advertisers are selected to enter the
second stage. Advertisers report their bids before the first round
starts, to optimize their cumulative utility over 𝑇rounds. In the
first stage of round 𝑡, we select a subset of advertisers denoted
as𝐾𝑡to enter the second stage. Then for each advertiser 𝑖inside
𝐾𝑡, its second-stage CTR prediction 𝑐𝑖𝑡, which we assume to be an
i.i.d. sample from a probability distribution 𝐷𝑖, is observed. We fix
the second stage to be a second-price auction, a well-known truth-
ful mechanism prevalent in ad auctions. The second-price auction
only involves advertisers within 𝐾𝑡, and determines the advertiser
with the highest 𝑐𝑖𝑡𝑏𝑖to be displayed. Our goal is to maximize the
cumulative social welfare without knowing the second-stage CTR
distributions 𝐷1,···,𝐷𝑛beforehand, while preserving the truthful
and IR properties of the 𝑇-round mechanism.
If we ignore the strategic behaviours of advertisers, this prob-
lem is equivalent to a combinatorial bandit with a general reward
function [ 5], where advertisers are treated as base arms, and in
each round we choose a super arm 𝐾𝑡subject to the cardinality con-
straint|𝐾𝑡|≤𝑘, and receive reward max𝑖∈𝐾𝑡{𝑏𝑖𝑐𝑖𝑡}1. However, the
additional requirement of truthfulness makes our problem a non-
trivial extension of the original bandit problem. In fact, advertisers
could misreport their values to manipulate the outcome of both
stages in a round. The observed samples of one round will further
influence the bandit algorithm’s behaviour in subsequent rounds.
To reveal the challenge of simultaneously ensuring performance
and truthfulness, we present in Proposition 1 the impossibility to
design a stochastically truthful (please refer to Definition 5) mecha-
nism even if the CTR distributions are known beforehand. Building
upon this result, we establish an Ω(𝑇)regret lower bound on bandit
mechanisms that are stochastically truthful (Theorem 1).
To circumvent the impossibility result, we introduce truthful
approximation oracles, which are constant-factor approximation
algorithms for solving the offline optimization problem in a truth-
ful manner. These oracles allow us to preserve truthfulness at
the cost of sacrificing allocation efficiency. We use the notion of
𝛼−approximation regret to evaluate bandit algorithms which calls
an𝛼−approximation oracle. We propose a bandit mechanism with
𝑂(𝑇2/3)regret and achieves ex-post truthfulness and ex-post IR.
The algorithm is based on a straightforward explore-then-commit
(ETC) strategy. The separation of exploration phase and exploita-
tion phase prevents strategic bidders from influencing the data
collection process, thereby ensuring truthfulness. Furthermore, we
discover that truthful approximation oracles often exhibit a typical
structure: scoring the arms by their CTR distributions, then select
the top-𝑘arms according to the product of their bid and score.
By exploiting this structure and making additional assumptions
on prior knowledge of bidders’ private values, we propose an al-
gorithm that achieves an improved regret bound of 𝑂(log𝑇/Δ2
𝜙),
1This reward function is "general" in the sense that the expected value
E𝑐𝑖𝑡∼𝐷𝑖[max𝑖∈𝐾𝑡{𝑏𝑖𝑐𝑖𝑡}]not depend only on the means of random variables , i.e.,
E𝐷𝑖[𝑐𝑖𝑡], but on the entire distributions of these variables.while preserving ex-post truthfulness and ex-post IR, where Δ𝜙is
a distribution-dependent gap.
To summarize, our major contributions in this work include:
•To the best of our knowledge, this is the first work that
jointly considers online subset selection and game-theoretic
properties in the setting of repeated two-stage auctions.
•We demonstrate the difficulty of the problem by establishing
aΩ(𝑇)regret lower bound for truthful bandit mechanisms.
•We introduce truthful 𝛼−approximation oracles, which al-
low us to design two ex-post truthful and ex-post IR bandit
mechanisms: one has 𝑂(𝑇2/3)𝛼-approximation regret, and
the other one enjoys a better 𝑂(log𝑇/Δ2
𝜙)𝛼-approximation
regret but requires additional assumptions on both the oracle
and the bidders.
•We validate the efficiency and truthfulness of our proposed
mechanisms through experiments on both synthetic and real-
world data, with results aligning well with our theoretical
claims.
2 MODEL AND PRELIMINARIES
We consider a repeated single-slot ad auction setting with 𝑛ad-
vertisers2[𝑛]and𝑇rounds, where a two-stage auction is con-
ducted in each round.3The advertisers have their private values
v=(𝑣1,···,𝑣𝑛)which is unknown to the auctioneer. Before the
first round starts, all advertisers submit their bids b=(𝑏1,···,𝑏𝑛).
We assume that all values and bids are bounded in [0,𝑉], where
𝑉>0. The second-stage CTR prediction of each advertiser 𝑖follows
distribution 𝐷𝑖, where𝐷𝑖is a probability distribution over [0,1].
We use𝐷=(𝐷1,···,𝐷𝑛)to denote the product distribution of
each𝐷𝑖. To characterize the two-stage ad auction in each round,
we first define second-price auction within a subset.
Definition 1 (Second-price auction within a subset). Given
𝑛advertisers, a subset 𝐾⊆[𝑛]with cardinality constraint |𝐾|≤𝑘,
CTR{𝑐𝑖}𝑖∈𝐾, and a bid vector b∈[0,𝑉]𝑛, a second-price auction
within𝐾determines the following allocation 𝑥𝑖and payment 𝑝𝑖for
each𝑖∈[𝑛]:
𝑥𝑖=(
1,if𝑖∈𝐾and𝑖=argmax𝑗∈𝐾{𝑏𝑗𝑐𝑗}
0,otherwise,
𝑝𝑖=(max𝑗∈𝐾,𝑗≠𝑖𝑐𝑗𝑏𝑗
𝑐𝑖,if𝑖∈𝐾and𝑖=argmax𝑗∈𝐾{𝑏𝑗𝑐𝑗}
0,otherwise,
where ties are broken consistently. The output of the auction is an
allocation vector x=(𝑥1,𝑥2,···,𝑥𝑛)and a payment vector p=
(𝑝1,𝑝2,···,𝑝𝑛). Throughout the paper, we use x(b,{𝑐𝑖}𝑖∈𝐾,𝐾)and
p(b,{𝑐𝑖}𝑖∈𝐾,𝐾)to denote outcomes of second-price auctions4.
In each round 𝑡∈ [𝑇], a two-stage auction is conducted as
follows:
2We use the terms advertiser, bidder, and arm interchangeably.
3For simplicity, we assume that the time horizon 𝑇is known beforehand, but our
results can be extended to the case with unknown 𝑇using a standard "doubling trick"
[2, 19].
4Sometimes we might slightly abuse notations and write x(b,c,𝐾)andp(b,c,𝐾),
where c=(𝑐1,···,𝑐𝑛)is the full CTR vector, although xandponly depends on
{𝑐𝑖}𝑖∈𝐾.
 
1566Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD ’24, August 25–29, 2024, Barcelona, Spain
•First stage. The auctioneer selects a subset of advertisers
𝐾𝑡with cardinality constraint |𝐾𝑡| ≤𝑘based on the bid
vector band all observations from previous rounds. Only
the advertisers in 𝐾𝑡enters the second stage.
•Second stage. For each 𝑖∈𝐾𝑡, the auctioneer observes
the second-stage CTR prediction 𝑐𝑖𝑡, which is an i.i.d. sam-
ple from distribution 𝐷𝑖. Then a second-price auction is
run within𝐾𝑡(see Definition 1), using the predicted CTRs
{𝑐𝑖𝑡}𝑖∈𝐾𝑡and the submitted bid vector b. The allocation to
advertisers in this round is x𝑡=x(b,{𝑐𝑖𝑡}𝑖∈𝐾𝑡,𝐾𝑡), with
payment p𝑡=p(b,{𝑐𝑖𝑡}𝑖∈𝐾𝑡,𝐾𝑡).
Our goal is to maximize the cumulative social welfare of 𝑇rounds.
The reward we obtain in each round 𝑡is defined as the social welfare
of the second-price auction in that round, i.e., 𝑅𝑡=max𝑖∈𝐾𝑡{𝑏𝑖𝑐𝑖𝑡}.
Since all bids are in [0,𝑉]and CTRs are in[0,1], the reward 𝑅𝑡is
in[0,𝑉]. The reward depends on sampled CTR predictions, and we
define its expectation with respect to distribution 𝐷as𝑅𝐷(𝐾𝑡)=
E𝐷[𝑅𝑡]. Then𝑅𝐷(𝐾𝑡)is a scalar that depends on 𝐾𝑡,𝐷,andb,
serving as a measure of how good 𝐾𝑡is, given𝐷andb.
We define the offline optimal reward, i.e. the maximum ex-
pected reward one can achieve if 𝐷(and b) is known, as OPT𝐷=
max𝐾𝑅𝐷(𝐾).
To maximize the cumulative social welfare, we should select
proper advertisers to enter the second stage in each round. More-
over, we can only observe the second-stage CTR predictions of
advertisers that enters the second stage. This sequential subset
selection procedure with partial feedback can be viewed as a com-
binatorial (semi-)bandit, where we treat advertisers as base arms
and subsets as super arms. Following the convention of bandit
algorithms, we define regret as the performance measure.
Definition 2. The regret of an algorithm is
𝑅𝑒𝑔(𝑇)=𝑇·OPT𝐷−E"𝑇∑︁
𝑡=1𝑅𝐷(𝐾𝑡)#
.
In our setting, a bandit algorithm also defines a 𝑇-round mecha-
nism5, where we consider the total allocation , X=(𝑋1,···,𝑋𝑛)=Í𝑇
𝑡=1x𝑡, and the total payment, P=(𝑃1,···,𝑃𝑛)=Í𝑇
𝑡=1p𝑡. We
refer to those mechanisms as bandit mechanisms. When a fixed
bandit algorithm runs on a fixed instance (𝐷,b)for several times,
the resulting allocation Xand payment Pmay be different, because
the CTR predictions in each round are stochastic. We can consider
this𝑇-round interaction process as first drawing CTR predictions
𝑐𝑖𝑡for all𝑖∈[𝑛]and𝑡∈[𝑇]from the distributions 𝐷, and then
running the bandit algorithm on these fixed samples. Specifically,
a𝑛×𝑇realization table Cwhose(𝑖,𝑡)-th entry is the 𝑐𝑖𝑡to reveal
when arm𝑖is played in the 𝑡-th round. When a table Cis fixed,
there is no stochasticity in the mechanism, so the total allocation
X(C,b)and payment P(C,b)of a mechanism are also fixed. This
allows us to consider the utility function of advertiser 𝑖with respect
to any table C, which is a deterministic function.
5Since the second-stage auction mechanism is fixed to be second-price (See Defini-
tion 1), the allocation rule of a 𝑇-round mechanism is uniquely defined by a combina-
torial bandit algorithm which decides the subset selection 𝐾𝑡in each round.Definition 3. Bidder𝑖’s utility function 𝑢𝑖with respect to bid 𝑏𝑖
and other bidders’ bids b−𝑖=(𝑏1,···,𝑏𝑖−1,𝑏𝑖+1,···,𝑏𝑛)is
𝑢𝑖(C,𝑏𝑖,b−𝑖)=𝑣𝑖·𝑥𝑖(C,𝑏𝑖,b−𝑖)−𝑝𝑖(C,𝑏𝑖,b−𝑖).
We expect our mechanisms to be truthful, i.e., reporting the real
value is the utility-maximizing strategy for any bidder. We define
two notions of truthfulness. Ex-post truthfulness requires the mech-
anism to be truthful on any fixed realization table. Stochastically
truthfulness is a weaker notion, which only requires truthfulness
in expectation, i.e., the expected utility function is maximized by
truthful bidding.
Definition 4. A mechanism is ex-post truthful if for any 𝑖,𝑣𝑖,𝑏𝑖,
b−𝑖,C,
𝑢𝑖(C,𝑣𝑖,b−𝑖)≥𝑢𝑖(C,𝑏𝑖,b−𝑖).
Definition 5. A mechanism is stochastically truthful if for any
𝐷,𝑖,𝑣𝑖,𝑏𝑖,b−𝑖,
EC[𝑢𝑖(C,𝑣𝑖,b−𝑖)]≥EC[𝑢𝑖(C,𝑏𝑖,b−𝑖)].
The mechanisms should also satisfy IR (Individual Rationality),
which ensures that advertisers have non-negative utility when
participating in the auction.
Definition 6. A mechanism is ex-post IR if for any 𝑖,𝑣𝑖,𝑏𝑖,b−𝑖,C,
𝑢𝑖(C,𝑣𝑖,b−𝑖)≥0.
3 IMPOSSIBILITY RESULT
In this section, we recognize the impossibility for any bandit mech-
anism to simultaneously achieve sublinear regret and stochastically
truthfulness, as shown in Theorem 1. This result reveals that in our
two-stage auction setting, it is challenging to design a mechanism
that enjoys both good performance and game-theoretic properties.
Theorem 1. There exists an instance set such that any stochasti-
cally truthful algorithm 𝜋must incur Ω(𝑇)regret.
The proof of Theorem 1 relies on Myerson’s Lemma.
Lemma 1 (Myerson’s Lemma [ 23]).A mechanism is stochastically
truthful if and only if any bidder’s allocation EC[𝑋𝑖(C,𝑏𝑖,b−𝑖)]is
monotone(i.e. non-decreasing) with respect to her bid 𝑏𝑖, and the
payment rule is given by an explicit formula.
Now we construct an offline problem instance such that the
social welfare-maximizing allocation is not monotone with respect
to one’s bid.
Proposition 1 (Optimal offline allocation is not mono-
tone). In the offline (full-information) setting, there exists 𝐷=
(𝐷1,···,𝐷𝑛), and an advertiser 𝑖, such that her expected optimal
allocation Er∼𝐷[𝑥𝑖(r,b,𝐾∗)], where𝐾∗=argmax𝐾𝑅𝐷(𝐾,b)is the
optimal super arm, is not monotone.
Proof. We present a counterexample with 𝑛=3and𝑘=2. The
CTR distributions 𝐷=(𝐷1,𝐷2,𝐷3)are
𝑐1=(
0.8 with prob. 0.5
0.001 with prob. 0.5,𝑐2=(
0with prob. 0.7
1with prob. 0.3,𝑐3=0.32.
Consider bids b=(1,1,1)andb′=(1.5,1,1), where advertiser 1
raises her bid.
 
1567KDD ’24, August 25–29, 2024, Barcelona, Spain Haoming Li et al.
On𝐷andb, on can easily compute that the optimal subset is 𝐾∗=
{1,2}, and advertiser 1’s expected allocation E[𝑥1]=Pr[𝑐1𝑏1>
𝑐2𝑏2]=0.7; while on 𝐷andb′, the optimal subset shifts to 𝐾∗′=
{1,3}and advertiser 1’s expected allocation decreases to E[𝑥′
1]=
Pr[𝑐1𝑏′
1>𝑐3𝑏′
3]=0.5. □
In the proof of Proposition 1, the increase of bidder 1’s bid leads
to a change of the optimal subset, which introduces a stronger
competitor (bidder 3) to bidder 1 during the auction within the
subset and finally causes the expected allocation of bidder 1 to
decrease. This example reveals a fundamental difference between
two-stage and one-stage auctions: in two-stage auctions, one bidder
may change her competitors in the second stage by changing her
bid.
Goel et al. [ 13] also proved an impossibility result by a construc-
tion similar to our Proposition 1. They proved that ex-post truth-
fulness is not achievable in two-stage auctions, while we present a
stronger result, i.e. even stochastically truthfulness is impossible.
To finally prove Theorem 1, we still need the following simple
lemma from the bandit literature.
Lemma 2. Let𝜋be a combinatorial bandit algorithm, and I=
(𝐷,b)be an instance. Assume Ihas a unique optimal super arm
𝐾∗=argmax𝐾𝑅𝐷(𝐾,b). Let𝜏𝐾(𝑇)=Í𝑇
𝑡=1I{𝐾𝑡=𝐾}denote the
times of𝜋playing𝐾from round 1to𝑇. If𝜋achieves sublinear regret
onI, then lim𝑇→∞E[𝜏𝐾∗(𝑇)]/𝑇=1.
Now we are ready to prove Theorem 1.
Proof of Theorem 1. We consider two instances I={𝐷,b}
andI′={𝐷,b′}, where𝐷,b,b′are the constructions in the proof
of Proposition 1.
Since algorithm 𝜋is stochastically truthful, by Lemma 1, bid-
der 1’s expected allocation must be monotone with respect to
𝑏1. We use shorthand E[𝑋1]forE[𝑋1(C,𝑏1,b−1)], andE[𝑋′
1]for
E[𝑋1(C,𝑏′
1,b′
−1)]. Let𝜏𝐾(𝑇)=Í𝑇
𝑡=1I{𝐾𝑡=𝐾}when𝜋is running
onI, and𝜏′
𝐾(𝑇)be its counterpart on I′.
We decompose the total allocation E[𝑋1]to the times when
different super arms containing arm 1 are pulled.
E[𝑋1]
=E
𝜏{1,2}(𝑇)
Pr[𝑐1𝑏1>𝑐2𝑏2]+E
𝜏{1,3}(𝑇)
Pr[𝑐1𝑏1>𝑐3𝑏3]
=0.7E
𝜏{1,2}(𝑇)
+0.5E
𝜏{1,3}(𝑇)
.
(1)
Similarly,
E[𝑋′
1]=0.85Eh
𝜏′
{1,2}(𝑇)i
+0.5Eh
𝜏′
{1,3}(𝑇)i
. (2)
If𝜋achieves sublinear regret on both IandI′, by applying
Lemma 2 toIandI′, we know that
lim
𝑇→∞E
𝜏{1,2}(𝑇)
/𝑇=1and lim
𝑇→∞Eh
𝜏′
{1,3}(𝑇)i
/𝑇=1.(3)
Moreover,
lim
𝑇→∞E
𝜏{1,3}(𝑇)
/𝑇=0and lim
𝑇→∞Eh
𝜏′
{1,2}(𝑇)i
/𝑇=0.(4)
Combining (3), (4) and (1), (2), we have
lim
𝑇→∞E[𝑋1]/𝑇=0.7and lim
𝑇→∞E[𝑋′
1]/𝑇=0.5,
which contradicts with E[𝑋1]≤E[𝑋′
1]and finishes the proof. □4 TRUTHFUL BANDIT MECHANISMS
In this section, we first introduce truthful approximation oracles,
which allows us to design truthful mechanisms at the cost of sacri-
ficing the performance. Then we present two mechanisms utilizing
truthful approximation oracles, both of which are ex-post truth-
ful and ex-post IR. The first mechanism achieves 𝑂(𝑇2/3)regret.
The second mechanism requires additional assumptions on the or-
acle and the bidders’ values, and achieves an improved regret of
𝑂(log𝑇/Δ2
𝜙), where Δ𝜙is a distribution-dependent gap.
4.1 Truthful Approximation Oracles
Theorem 1 states the impossibly to design a truthful bandit mecha-
nism that approaches the offline optimal allocation. To overcome
the difficulty, we introduce approximation oracles that solves the
offline optimization problem in a truthful manner.
Definition 7 (Truthful Approximation Oracle). An ora-
cle takes distributions 𝐷and bid vector bas input, and outputs a
subset𝐾←Oracle(𝐷,b),|𝐾|≤𝑘. For𝛼∈(0,1), an oracle is an
𝛼−approximation if for any 𝐷and any b
𝑅𝐷(Oracle(𝐷,b))≥𝛼OPT𝐷.
An oracle is ex-post truthful if for any 𝐷,v∈[0,𝑉]𝑛,r∈[0,1]𝑛,
the following mechanism is truthful:
•Solicit bid vector b.
•Query the oracle for 𝐾←Oracle(𝐷,b).
•Run second-price auction within 𝐾. Output x(b,c,𝐾)and
p(b,c,𝐾).
Representation of Distributions. One may wonder how to rep-
resent distributions for the oracle’s input. In fact, our proposed
algorithms only call the oracles on empirical distributions, which
are discrete distributions with finite support. We represent such
distributions 𝐷={𝐷1,···,𝐷𝑛}by their CDFs F={𝐹1,···,𝐹𝑛}.
Each𝐹𝑖is a piecewise constant function, which could be further
represented by a vector of supported points and the values of CDF
on those points. Throughout the paper, we may use 𝐷andFinter-
changeably.
Note that we actually required truthful approximation oracles to
be ex-post truthful, rather than the weaker notion of stochastically
truthful. The following lemma provides a concrete example of such
an ex-post truthful approximation oracle.
Lemma 3 (Theorem 3 in Goel et al. [ 13]).For each distribution
𝐷𝑖of𝑐𝑖, let𝜙𝑖(𝜃)be the expectation above the quantile function 𝑞𝑖(𝜃):
𝑞𝑖(𝜃)=sup{𝑥|Pr[𝑐𝑖≥𝑥]≥𝜃},
𝜙𝑖(𝜃)=E𝑐𝑖∼𝐷𝑖[𝑐𝑖I[𝑐𝑖≥𝑞𝑖(𝜃)]],
then the following oracle is a truthful𝑒−1
2𝑒-approximation oracle: Sort
all bidders by 𝑏𝑖𝜙𝑖(1/𝑘), and choose the top 𝑘bidders (ties are broken
consistently).
We leverage truthful approximation oracles in our design of
truthful online learning algorithms (mechanisms). In such cases,
it is not fair to compare our algorithm with the optimal algorithm
which always chooses the optimal super arm. Instead, we use the
𝛼−approximation regret to evaluate an algorithm.
 
1568Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD ’24, August 25–29, 2024, Barcelona, Spain
Definition 8. The𝛼−approximation regret of an algorithm is
defined as
𝑅𝑒𝑔𝛼(𝑇)=𝛼𝑇·OPT𝐷−E"𝑇∑︁
𝑡=1𝑅𝐷(𝐾𝑡)#
.
4.2𝑂(𝑇2/3)Mechanism
We present a mechanism that achieves 𝑂(𝑇2/3)gap-independent
regret and ex-post truthfulness. The mechanism consists of a fixed-
length exploration phase followed by an exploitation phase. In the
exploration phase, we collect 𝑚samples for each arm in a round-
robin manner, and calculate their empirical distributions in terms
of CDFs. With 𝑚i.i.d. samples 𝑋1,···,𝑋𝑚from distribution 𝐷, the
empirical CDF is defined as ˆ𝐹(𝑥)=1
𝑚Í𝑚
𝑗=1I(𝑋𝑗≤𝑥). Based on
these empirical CDFs, a truthful approximation oracle decides the
super arm that is repeatedly played in the exploitation phase.
Algorithm 1 An ETC mechanism
1:𝑚←(𝜋
2)1
3(1+𝛼)2
3𝑛2
3𝑇2
3
2:forround𝑡≤𝑚𝑘do ⊲Exploration Phase
3:𝐾𝑡←{1+𝑘𝑡mod𝑛,2+𝑘𝑡mod𝑛,···,𝑘+𝑘𝑡mod𝑛},
run second-price auction within 𝐾𝑡
4: Update ˆ𝐹𝑖for each𝑖∈𝐾𝑡
5:end for
6:Query the oracle, get 𝐾←Oracle(ˆF,b)
7:foreach remaining round 𝑡do ⊲Exploitation Phase
8:𝐾𝑡←𝐾, run second-price auction within 𝐾𝑡
9:end for
Theorem 2. Algorithm 1 achieves the following 𝛼−approximation
regret upper bound:
𝑅𝑒𝑔𝛼(𝑇)≤3(1+𝛼)2
3𝑘𝑉𝑛2
3𝑇2
3.
The proof of Theorem 2 relies on several useful lemmas.
Lemma 4 (Dvoretzky-Kiefer-Wolfowitz ineqality[ 9,22]).
Consider a distribution 𝐷, and let𝐹(𝑥)be its CDF. With 𝑚i.i.d. sam-
ples𝑋1,···,𝑋𝑚from𝐷, the empirical CDF is ˆ𝐹(𝑥)=1
𝑚Í𝑚
𝑗=1I(𝑋𝑗≤
𝑥), then for any 𝜖>0, we have
Pr[sup
𝑥∈𝑅|𝐹(𝑥)−ˆ𝐹(𝑥)|≥𝜖]≤2𝑒−2𝑚𝜖2
Lemma 5. For random variable 𝑋with non-negative support,
E[𝑋]=∫∞
0Pr[𝑋>𝜖]d𝜖.
Lemma 6 (Lemma 3 in Chen et al. [ 5]).If for any𝑖∈[𝑛],𝑥∈
[0,1],sup𝑥|𝐹𝑖(𝑥)−𝐹′
𝑖(𝑥)|≤Λ, then for any super arm 𝐾, we have
|𝑅𝐹(𝐾)−𝑅𝐹′(𝐾)|≤ 2𝑉𝑘Λ.
Lemma 4, i.e. the DKW inequality, is on concentration of empir-
ical distributions. Lemma 5 is a simple fact in probability theory,
bridging expectations and CDFs. Lemma 6 characterizes the con-
centration of super arms’ rewards based on the concentration of
base arms’ empirical distributions. Now we are ready to prove
Theorem 2.Proof. Let𝐾be the super arm played in the exploitation phase.
Let ˆ𝐷=(ˆ𝐷1,···,ˆ𝐷𝑛)be the empirical distributions at round
𝑚𝑘, with empirical CDFs ˆF=(ˆ𝐹1,···,ˆ𝐹𝑛), and let𝐷be the real
distributions. Since we call an 𝛼−approximation oracle, we have
𝑅ˆ𝐷(𝐾)≥𝛼OPT ˆ𝐷. Let𝐾∗=argmax𝐾𝑅𝐷(𝐾)be the real optimal
super arm. Then
𝑅ˆ𝐷(𝐾)≥𝛼OPT ˆ𝐷≥𝛼𝑅ˆ𝐷(𝐾∗),
where the second inequality follows from the optimality of OPT ˆ𝐷,
i.e.,OPT ˆ𝐷≥𝑅ˆ𝐷(𝐾′)for any𝐾′. We then bound the probability
Pr[𝑅𝐷(𝐾)<𝛼OPT𝐷−𝜖]for any𝜖∈R+.
Pr[𝑅𝐷(𝐾)<𝛼OPT𝐷−𝜖]
≤Pr[𝑅𝐷(𝐾)<𝛼OPT𝐷−𝜖+(𝑅ˆ𝐷(𝐾)−𝛼𝑅ˆ𝐷(𝐾∗))]
=Pr[(𝑅ˆ𝐷(𝐾)−𝑅𝐷(𝐾))−𝛼(𝑅ˆ𝐷(𝐾∗)−𝑅𝐷(𝐾∗))>𝜖]
≤Pr[|𝑅ˆ𝐷(𝐾)−𝑅𝐷(𝐾)|+𝛼|𝑅ˆ𝐷(𝐾∗)−𝑅𝐷(𝐾∗)|>𝜖]
:=Pr[E],(5)
where the last inequality follows from |𝑎−𝑏|≤|𝑎|+|𝑏|for any
𝑎,𝑏∈R, and in the last line we define event E={|𝑅ˆ𝐷(𝐾)−𝑅𝐷(𝐾)|+
𝛼|𝑅ˆ𝐷(𝐾∗)−𝑅𝐷(𝐾∗)|>𝜖}.
Also, define good event
G=(
∀𝑖∈[𝑛],sup
𝑥∈[0,1]|𝐹𝑖(𝑥)−ˆ𝐹𝑖(𝑥)|≤𝜖
2𝑉𝑘(1+𝛼))
.
By Lemma 6, ifGhappens, then we have |𝑅ˆ𝐷(𝐾)−𝑅𝐷(𝐾)|≤
𝜖
1+𝛼and|𝑅ˆ𝐷(𝐾∗)−𝑅𝐷(𝐾∗)|≤𝜖
1+𝛼, which preventsEto happen.
Therefore,Eimplies¬G, which gives us
Pr[E]
≤Pr[¬G]
=Pr"
∃𝑖∈[𝑛],sup
𝑥∈[0,1]|𝐹𝑖(𝑥)−ˆ𝐹𝑖(𝑥)|>𝜖
2𝑉𝑘(1+𝛼)#
≤𝑛∑︁
𝑖=1Pr"
sup
𝑥∈[0,1]|𝐹𝑖(𝑥)−ˆ𝐹𝑖(𝑥)|>𝜖
2𝑉𝑘(1+𝛼)#
≤2𝑛exp
−𝑚𝜖2
2𝑉2𝑘2(1+𝛼)2
,(6)
where the second inequality is by taking a union bound, and the
third inequality follows from DKW inequality.
Combining (5) and (6) gives us
Pr[𝑅𝐷(𝐾)<𝛼OPT𝐷−𝜖]≤2𝑛exp
−𝑚𝜖2
2𝑉2𝑘2(1+𝛼)2
.
Split the regret by exploration phase and exploitation phase,
𝑅𝑒𝑔𝛼(𝑇)=𝑚𝑘𝑉+(𝑇−𝑚𝑘)E[𝛼OPT𝐷−𝑅𝐷(𝐾)]. (7)
Calculate the expectation term with Lemma 5,
E[𝛼OPT𝐷−𝑅𝐷(𝐾)]
=∫∞
0Pr[𝛼OPT𝐷−𝑅𝐷(𝐾)>𝜖]d𝜖
≤∫∞
02𝑛exp
−2𝑚𝜖2
4𝑉2𝑘2(1+𝛼)2
d𝜖
=√
2𝜋𝑛𝑉𝑘(1+𝛼)√𝑚.(8)
 
1569KDD ’24, August 25–29, 2024, Barcelona, Spain Haoming Li et al.
Plug (8)into(7), and let𝑚=min(⌈(𝜋
2)1
3(1+𝛼)2
3𝑛2
3𝑇2
3⌉,⌈𝑇/𝑘⌉),
we have
𝑅𝑒𝑔𝛼(𝑇)=𝑚𝑘𝑉+(𝑇−𝑚𝑘)E[𝛼OPT𝐷−𝑅𝐷(𝐾)]
≤𝑚𝑘𝑉+𝑇√
2𝜋𝑛𝑉𝑘(1+𝛼)√𝑚
=(2−2
3+2−1
3)(2𝜋)2
3(1+𝛼)2
3𝑘𝑉𝑛2
3𝑇2
3
≤3(1+𝛼)2
3𝑘𝑉𝑛2
3𝑇2
3.(9)
□
Beyond sublinear regret, Algorithm 1 also enjoys the following
truthful and IR properties.
Proposition 2. Algorithm 1 is ex-post truthful and ex-post IR.
The truthfulness of Algorithm 1 mainly follows from the prop-
erty of truthful oracles combined with the truthfulness of second-
price auctions. The IR property follows from that of second-price
auctions. The complete proofs are deferred to Appendix A.
4.3𝑂(log𝑇/Δ2
𝜙)Mechanism
Although Algorithm 1 guarantees ex-post truthfulness and ex-post
IR, its𝑂(𝑇2/3)regret is not satisfactory in some scenarios. To design
an algorithm with a better regret bound, we make an additional as-
sumption on the oracle. Beyond truthfulness and 𝛼−approximation,
we assume that the oracle determines 𝐾by ranking the bidders
according to some score and choosing the top- 𝑘, and the scores
are accessible. The scores provide additional information about the
quality of the bidders, and can be leveraged by the bandit algorithm
to quickly determine the correct subset to exploit.
Definition 9 (Truthful Approximation Scoring Oracle). A
scoring oracle Score(·)assigns each bidder a score according to its
distribution, i.e., 𝜙𝑖←Score(𝐷𝑖), such that ranking the bidders by
𝑏𝑖𝜙𝑖, and choosing the top- 𝑘as𝐾will ensure
𝑅𝐷(𝐾)≥𝛼OPT𝐷.
It is easy to check that this scoring and ranking procedure is
always ex-post truthful for any score 𝜙=(𝜙1,···,𝜙𝑛).
We further make an assumption on the smoothness of Score(·):
There exists a Lipschitz constant 𝐿>0, such that if sup𝑥|𝐹(𝑥)−
𝐹′(𝑥)|≤Λ, then
|Score(𝐹)−Score(𝐹′)|≤𝐿Λ.
The following lemma tells us such scoring oracle exists. In fact,
the oracle presented in Lemma 3 is exactly a truthful𝑒−1
2𝑒−approx-
imation scoring oracle. The proof of its Lipschitz constant 𝐿=1is
deferred to Appendix A.
Lemma 7. There exists a truthful𝑒−1
2𝑒-approximation scoring ora-
cle with Lipschitz constant 𝐿=1.
We denote the gap of an instance as Δ𝜙=min𝑖,𝑗∈[𝑛],𝑖≠𝑗|𝑣𝑖𝜙𝑖−
𝑣𝑗𝜙𝑗|. For the truthful property of the mechanism, we assume that
the value𝑣𝑖of each arm 𝑖is within an interval around a known
parameter𝛽𝑖∈[0,𝑉]:
𝑣𝑖∈
𝛽𝑖−Δ𝜙/2𝜙𝑖,𝛽𝑖+Δ𝜙/2𝜙𝑖
. (10)This assumption is often satisfied in industrial scenarios where
advertisers’ valuation of a certain impression is static, and the ad
platform can estimate one’s value from one’s bidding history.
Based on an 𝛼−approximation scoring oracle, we design an ETC
mechanism with adaptive commitment time, i.e., the length of ex-
ploration phase depends on the collected data. The fixed prior
information of value 𝛽𝑖, instead of submitted bid 𝑏𝑖, is used in de-
ciding of commitment. This prevents the bidders from influencing
the commitment time by strategic bidding.
Algorithm 2 An ETC mechanism with adaptive commitment time
1:Throughout the exploration phase, for each arm 𝑖∈[𝑛]we
maintain: (i) counter 𝑇𝑖which stores the times that 𝑖has been
selected so far (ii) CDF ˆ𝐹𝑖of the empirical distribution of the
observed outcomes of arm 𝑖so far
2:repeat ⊲Exploration Phase
3:𝐾𝑡←{1+𝑘𝑡mod𝑛,2+𝑘𝑡mod𝑛,···,𝑘+𝑘𝑡mod𝑛},
run second-price auction within 𝐾𝑡
4: foreach𝑖∈𝐾𝑡do
5: Update𝑇𝑖and ˆ𝐹𝑖
6: Query the scoring oracle, get ˆ𝜙𝑖←Score(ˆ𝐹𝑖), compute
𝜙𝑖←ˆ𝜙𝑖+𝐿√︃
log𝑇
𝑇𝑖and𝜙𝑖←ˆ𝜙𝑖−𝐿√︃
log𝑇
𝑇𝑖
7: end for
8: Rank all the arms by 𝛽𝑖ˆ𝜙𝑖, let𝐻𝑖𝑔ℎ be the set of top- 𝑘arms,
let𝐿𝑜𝑤 be the other 𝑛−𝑘arms
9:𝜙ℎ←min𝑖∈𝐻𝑖𝑔ℎ𝛽𝑖𝜙𝑖,𝜙𝑙←max𝑖∈𝐿𝑜𝑤𝛽𝑖𝜙𝑖
10:until𝜙ℎ≥𝜙𝑙
11:foreach remaining round 𝑡do ⊲Exploitation Phase
12: Pull𝐾𝑡←𝐻𝑖𝑔ℎ , run second-price auction within 𝐾𝑡, using
bidsb
13:end for
Theorem 3. Algorithm 2 achieves the following 𝛼−approximation
regret upper bound:
𝑅𝑒𝑔𝛼(𝑇)≤2𝑛𝑉+4𝑛𝐿2𝑉3
𝑘Δ2
𝜙log𝑇.
Proof. Letˆ𝐹𝑖,𝑢(𝑥)be the empirical distribution of arm 𝑖when
𝑢samples from 𝑖are observed. Define event
E=(
∃𝑖∈[𝑛],∃𝑢∈[𝑇],sup
𝑥∈[0,1]ˆ𝐹𝑖,𝑢(𝑥)−𝐹𝑖(𝑥)≥√︂
log𝑇
𝑢)
.
From the DKW inequality, for ∀𝑖∈[𝑛],∀𝑢∈[𝑇],
Pr
sup
𝑥∈[0,1]|ˆ𝐹𝑖,𝑢(𝑥)−𝐹𝑖(𝑥)|≥√︂
log𝑇
𝑢
≤2𝑒−2𝑢log𝑇
𝑢=2
𝑇2.
Taking an union bound gives Pr[E]≤2𝑛
𝑇.
Letˆ𝜙𝑖,𝑡,𝜙𝑖,𝑡,𝜙𝑖,𝑡be the value of ˆ𝜙𝑖,𝜙𝑖,𝜙𝑖at time𝑡. On event¬E,
by definition, for any arm 𝑖, any time𝑡in the exploration phase,
|𝜙𝑖−ˆ𝜙𝑖,𝑡|≤𝐿√︃
log𝑇
𝑇𝑖, therefore𝜙𝑖,𝑡≤𝜙𝑖≤𝜙𝑖,𝑡. When condition
𝜙ℎ≥𝜙𝑙is satisfied, for∀𝑖∈𝐻𝑖𝑔ℎ ,∀𝑗∈𝐿𝑜𝑤,𝛽𝑖𝜙𝑖,𝑡≥𝛽𝑗𝜙𝑗,𝑡, thus
𝛽𝑖𝜙𝑖≥𝛽𝑗𝜙𝑗. This implies that 𝐻𝑖𝑔ℎ is the top-𝑘subset according
 
1570Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD ’24, August 25–29, 2024, Barcelona, Spain
to{𝛽𝑖𝜙𝑖}. By Equation (10),𝐻𝑖𝑔ℎ is also the top- 𝑘subset according
to{𝑏𝑖𝜙𝑖}.
Let𝑚be the last round in which 𝜙ℎ<𝜙𝑙. Let𝑖=argmin𝑖∈𝐻𝑖𝑔ℎ𝛽𝑖𝜙𝑖,
𝑗=argmax𝑗∈𝐿𝑜𝑤𝛽𝑗𝜙𝑗, we know𝛽𝑖𝜙𝑖<𝛽𝑗𝜙𝑗. By the exploration
rule, till round 𝑚, we have observed𝑚𝑘
𝑛samples for each arm. Since
𝛽𝑖𝜙𝑖<𝛽𝑗𝜙𝑗,
𝛽𝑖 
𝜙𝑖−𝐿√︂
𝑛log𝑇
𝑚𝑘!
<𝛽𝑗 
𝜙𝑗+𝐿√︂
𝑛log𝑇
𝑚𝑘!
.
Rearranging the terms, we have
Δ𝜙≤𝛽𝑖𝜙𝑖−𝛽𝑗𝜙𝑗≤(𝛽𝑖+𝛽𝑗)𝐿√︂
𝑛log𝑇
𝑚𝑘≤2𝐿𝑉√︂
𝑛log𝑇
𝑚𝑘.
Solving𝑚from the first and last term gives 𝑚≤4𝑛𝐿2𝑉2log𝑇
𝑘Δ2
𝜙.
On eventE, each round incurs regret of at most 𝑉. On event
¬E, we know that 𝐻𝑖𝑔ℎ is the top-𝑘subset according to {𝑏𝑖𝜙𝑖}.
By definition of the 𝛼−approximation scoring oracle, playing 𝐻𝑖𝑔ℎ
incurs non-positive regret. Therefore, the regret of the algorithm is
𝑅𝑒𝑔𝛼(𝑇)≤Pr[E]𝑇𝑉+Pr[¬E]E[𝑚|¬E]𝑉
≤2𝑛
𝑇𝑇𝑉+4𝑛𝐿2𝑉2log𝑇
𝑘Δ2
𝜙𝑉
=2𝑛𝑉+4𝑛𝐿2𝑉3
𝑘Δ2
𝜙log𝑇.
□
Algorithm 2 also achieves ex-post truthfulness and ex-post IR.
The proofs are deferred to Appendix A.
Proposition 3. Algorithm 2 is ex-post truthful and ex-post IR.
By the design of adaptive commitment time, Algorithm 2 achieves
a better regret than Algorithm 1 on most instances, except for the
cases when Δ𝜙is extremely small. A potential way to further im-
prove the regret bound is to adopt UCB-based [ 5] or successive-
elimination style [ 27] algorithms. However, these algorithms are
much more data sensitive than ETC algorithms, thus may be prone
to strategic bids.
5 EXPERIMENTS
In this section, we evaluate our two mechanisms through experi-
ments on both synthetic data and real-world data.
For the experiments, instead of 𝛼−approximation regret (Defini-
tion 8), we compare the cumulative rewards achieved by our mech-
anisms against 𝑇·𝑅𝐷(𝐾Oracle), where𝐾Oracle is the subset returned
by an𝛼−approximation oracle. By definition of 𝛼−approximation
oracles (Definition 7), 𝑅𝐷(𝐾Oracle)≥𝛼OPT𝐷, so𝑅𝐷(𝐾Oracle)is a
more challenging reference value, and all of our theoretical results
still holds under this notion of regret. This choice is based on two
reasons: (i) computing OPT𝐷is often computationally infeasible,
(ii) the value of 𝛼OPT𝐷can be much lower than 𝑅𝐷(𝐾Oracle)prac-
tically, and comparing the mechanisms’ cumulative reward against
𝛼OPT𝐷often leads to negative regret.
Beyond regret, we also test the truthfulness of our mechanisms,
by computing a bidder’s utility with different bids.
(a) Ex-post truthfulness of Algo-
rithm 1.
(b) Ex-post truthfulness of Algo-
rithm 2.
Figure 1: Ex-post truthfulness of our two algorithms evalu-
ated on synthetic data. Each line represents a bidder’s utilities
with respect to different submitted bids on one random seed.
5.1 Experiments with synthetic data
5.1.1 Experiment Setup. We construct an environment with 𝑛=7
bidders, from which 𝑘=3bidders are selected in each round. All
bidders have the same uniform CTR distribution, i.e., 𝑐𝑖∼𝑈([0,1]).
The bidders’ values are [1+Δ,1+Δ,1+Δ,1,1,1,1], where Δis a
gap parameter that we control through the experiments. By the
truthful property of our mechanisms, the input bids are equal to
the values. We run experiments for different time horizons 𝑇∈
{2×104,4×104,6×104,8×104,105}. For each time horizon, we run
experiments for Δ∈{0.5,1,1.5,2}. The result of each experiment
is averaged over 80 independent runs.
We leverage the oracle presented in 3 for both Algorithm 1 and
Algorithm 2. The difference is that for Algorithm 1, the oracle only
returns a subset it chooses, while for Algorithm 2, it returns all the
scores𝜙𝑖(1/𝑘)for𝑖∈[𝑛].
To test the ex-post truthfulness of our mechanisms, we fix Δto
be 1, so bidders’ the values are [2,2,2,1,1,1,1]. Since ex-post truth-
fulness requires the mechanism to be truthful on any random seed,
we pick 100 random seeds for evaluation. For each fixed random
seed, we adjust the bid of bidder 1 to 𝑏1∈{1.25,1.50,1.75,2.00,
2.25,2.50,2.75,3.00}, while keeping other bidders’ bids unchanged.
We report bidder 1’s utility when different bids are reported. If
the mechanism is ex-post truthful, then for any random seed, the
utility-maximizing bid should be equal to the value. During the test
of truthfulness, we fix 𝑇=10000.
5.1.2 Results and Discussions. Figure 2a presents a comparison of
regret between Algorithm 1 and Algorithm 2. For any time hori-
zon𝑇and gap Δ, the regret of Algorithm 2 is significantly lower
than that of Algorithm 1. The low regret is due to the design of
adaptive commitment time in Algorithm 2. Besides, when the gap
Δdecreases, Algorithm 1 achieves lower regret, while Algorithm 2
suffers from higher regret. This phenomenon is demonstrated more
clearly in Figure 2b and Figure 2c. Figure 2b depicts the regret
of Algorithm 1 with different 𝑇andΔin a log-log plot. The grey
dashed lines represent 𝑅𝑒𝑔=𝑎𝑇2
3with different values of 𝑎. We
observe that the regret curves are almost parallel with the grey
lines, which indicates Θ(𝑇2/3)regret, matching Theorem 2. More-
over, Algorithm 1 shows higher regret when Δgets high. This is
because the regret accumulated in the exploration phase grows
as the gap between optimal and suboptimal super arms expands.
 
1571KDD ’24, August 25–29, 2024, Barcelona, Spain Haoming Li et al.
(a) Comparison of the regret of two mecha-
nisms. Regret is displayed on a log scale.
(b) The regret of Algorithm 1. To demon-
strate the order of 𝑇2/3, both Regret and T
are displayed on a log scale.
(c) The regret of Algorithm 2. To demon-
strate the order of log𝑇, T is displayed on a
log scale.
Figure 2: Regret of our two algorithms evaluated on synthetic data.
Figure 3: Regret of our two algorithms evaluated on Movie-
Lens dataset. Both Regret and T are displayed on a log scale.
Figure 2c shows the regret of Algorithm 2. We observe that the
regret almost grows linearly with respect to log𝑇. Besides, the re-
gret grows quadratically with decreasing Δ. Note that in the case
of our experiment, Δis proportional to Δ𝜙, as the distributions are
identical and fixed. The dependence of regret on 𝑇andΔ𝜙nicely
matches our theoretical result (Theorem 3).
In the test of truthfulness, on all 100 random seeds, bidding the
true value achieves highest utility among 8 different bids. Figure 1
shows the utility curve of two mechanisms on five random seeds
{1,2,3,4,5}. The curves on all 100 seeds actually look similar, with
𝑏=2being their common maximum point, and the fives seeds are
arbitrarily picked only for demonstration.
5.2 Experiments with real-world data
We evaluate the Algorithm 1 and Algorithm 2 on the MovieLens
1M [14] dataset.
5.2.1 Experiment Setup. We treat each movie as an arm and convert
the ratings into a CTR-like metric. The top 7arms, determined by
the highest number of ratings in the original dataset, are selected as
the base arms. From these base arms, a super arm consisting of 𝑘=3arms is chosen in each round. To construct the CTR distribution, we
manually set a rating threshold 𝑟𝑡=3.5, such that ratings 𝑟>3.5are
converted to 1, and otherwise to 0. Subsequently, we calculate the
mean and variance of the converted ratings for each arm, denoted as
the mean and variance of the CTR distribution, respectively. These
CTR distributions are modeled as truncated Gaussian distributions,
with support restricted to [0,1]. The bid of each arm is sampled
from a uniform distribution 𝑈([0,5])and remains constant after
the experiment begins. We conduct experiments for different time
horizons𝑇∈{1250,2500,5000,1×104,2×104,4×104,6×104}.
The setting of the oracle is the same as Section 5.1.1.
5.2.2 Results and Discussions. Figure 3 presents a comparison of
the regret between Algorithm 1 and Algorithm 2. When the horizon
𝑇is small, both algorithms exhibit linear regret, as 𝑚𝑘>𝑇for a
small𝑇, leading to a predominantly exploratory phase within the
limited horizon. As 𝑇increases, both algorithms demonstrate im-
provements by incorporating an exploitation phase. In comparison
with the dashed grey line, it is evident that Algorithm 1 achieves
𝑂(𝑇2/3)regret, while Algorithm 2 attains 𝑂(log𝑇)regret. Notably,
with a large horizon 𝑇, Algorithm 2 achieves lower regret compared
to Algorithm 1.
6 RELATED WORKS
Two-stage Advertising Systems. Previous studies on two-stage
advertising systems have primarily focused on two aspects: allo-
cation efficiency and incentives. Both Wang et al. [ 25] and Zhao
et al. [ 32] addressed the learning objectives of machine learning
models in the first stage, in order to align with the second stage and
enhancing the overall ad allocation performance. While Wang et
al. [25] also discussed incentives, they considered a non-standard
value-maximizer utility model. On incentives in two-stage auctions,
Goel et al. [ 13] provided an insightful characterization of first-stage
mechanisms that ensures overall truthfulness when composed with
any truthful second-stage auction mechanism. However, their work
was limited to single-round mechanisms, whereas we considered
incentives in multi-round bandit mechanisms.
 
1572Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD ’24, August 25–29, 2024, Barcelona, Spain
Two-stage Recommendation Systems. Research on two-stage struc-
tures in recommendation is rather abundant than advertising. A gen-
eral approach is cooperative training of both stages [ 12,15,17,18]
to improve the overall recommendation performance, and particu-
lar attention has been paid to off-policy correction [ 20] and fairness
issues [ 24]. Closer to our setting is synchronized two-stage explo-
ration, studied under both linear [ 16] and neural [ 31] bandit settings.
Although these studies share some similarities with our setting,
there are significant differences due to the presence of incentives
in ad auctions.
Truthful Bandit Mechanisms. A line of research has focused on
designing truthful mechanisms for multi-round ad auctions, where
a multi-armed bandit algorithm acts as the allocation rule. Babaioff
et al. [ 4] provided a Ω(𝑇2/3)regret lower bound for any determin-
istic truthful multi-armed bandit mechanisms, and provided a ETC
algorithm that matches this lower bound. Devanur and Kakade
[8] obtained a similar Ω(𝑇2/3)lower bound under the revenue-
maximizing setting. Babaioff et al. [ 3] further extended to ran-
domized mechanisms, and provided a black-box reduction from
any monotone bandit algorithm to a truthful mechanism, which
gives rise to 𝑂(√
𝑇)regret. Recent works have considered extended
settings in different directions, such as utility models [ 11] and con-
textual information [ 1,28,30]. Our work is based on a novel setting
of two-stage ad auctions which is formulated as designing combi-
natorial bandit mechanisms.
7 CONCLUSION
In this paper, we investigate the problem of designing truthful
bandit mechanisms for two-stage online ad auctions. We prove
anΩ(𝑇)lower bound for truthful mechanisms, and introduce
truthful𝛼−approximation oracles which give rise to sublinear
𝛼−approximation regret mechanisms.
We leave it as an open problem to potentially design 𝑂(√
𝑇)
bandit mechanisms within the approximation regret setting, or
to establish an Ω(𝑇2/3)lower bound. Moreover, the impossibility
result may also be circumvented by other approaches, e.g., relaxing
the notion of truthfulness by considering high-probability truthful
mechanisms.
ACKNOWLEDGMENTS
The authors sincerely thank Shuai Li and Xutong Liu for their help-
ful discussions. This work was supported in part by National Key
R&D Program of China (No. 2022ZD0119100), in part by China
NSF grant No. 62322206, 62132018, U2268204, 62025204, 62272307,
62372296. The opinions, findings, conclusions, and recommenda-
tions expressed in this paper are those of the authors and do not
necessarily reflect the views of the funding agencies or the govern-
ment.
REFERENCES
[1]Kumar Abhishek, Shweta Jain, and Sujit Gujar. 2020. Designing Truthful Contex-
tual Multi-Armed Bandits based Sponsored Search Auctions. In Proceedings of
the 19th International Conference on Autonomous Agents and MultiAgent Systems
(Auckland, New Zealand) (AAMAS ’20). International Foundation for Autonomous
Agents and Multiagent Systems, Richland, SC, 1732–1734.[2]Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. 1995.
Gambling in a rigged casino: The adversarial multi-armed bandit problem. In
Proceedings of IEEE 36th annual foundations of computer science. IEEE, 322–331.
[3]Moshe Babaioff, Robert D Kleinberg, and Aleksandrs Slivkins. 2015. Truthful
mechanisms with implicit payment computation. Journal of the ACM (JACM) 62,
2 (2015), 1–37.
[4]Moshe Babaioff, Yogeshwer Sharma, and Aleksandrs Slivkins. 2014. Character-
izing Truthful Multi-armed Bandit Mechanisms. SIAM J. Comput. 43, 1 (2014),
194–230.
[5]Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, and Pinyan Lu. 2016. Combinatorial multi-
armed bandit with general reward functions. Advances in Neural Information
Processing Systems 29 (2016).
[6]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7–10.
[7]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. In Proceedings of the 10th ACM conference on
recommender systems. 191–198.
[8]Nikhil R Devanur and Sham M Kakade. 2009. The price of truthfulness for
pay-per-click auctions. In Proceedings of the 10th ACM conference on Electronic
commerce. 99–106.
[9]Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz. 1956. Asymptotic minimax
character of the sample distribution function and of the classical multinomial
estimator. The Annals of Mathematical Statistics (1956), 642–669.
[10] Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma,
Charles Sugnet, Mark Ulrich, and Jure Leskovec. 2018. Pixie: A system for
recommending 3+ billion items to 200+ million users in real-time. In Proceedings
of the 2018 world wide web conference. 1775–1784.
[11] Zhe Feng, Christopher Liaw, and Zixin Zhou. 2023. Improved online learning
algorithms for CTR prediction in ad auctions. In International Conference on
Machine Learning. PMLR, 9921–9937.
[12] Luke Gallagher, Ruey-Cheng Chen, Roi Blanco, and J. Shane Culpepper. 2019.
Joint Optimization of Cascade Ranking Models. In Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining (WSDM ’19) . 15–23.
[13] Gagan Goel, Renato Paes Leme, Jon Schneider, David Thompson, and Hanrui
Zhang. 2023. Eligibility Mechanisms: Auctions Meet Information Retrieval. In
Proceedings of the ACM Web Conference 2023. 3541–3549.
[14] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History
and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015),
1–19.
[15] Jiri Hron, Karl Krauth, Michael Jordan, and Niki Kilbertus. 2021. On Compo-
nent Interactions in Two-Stage Recommender Systems. In Advances in Neural
Information Processing Systems, Vol. 34. 2744–2757.
[16] Jiri Hron, Karl Krauth, Michael I. Jordan, and Niki Kilbertus. 2020. Exploration
in two-stage recommender systems. arXiv:2009.08956 [cs.IR]
[17] Xu Huang, Defu Lian, Jin Chen, Liu Zheng, Xing Xie, and Enhong Chen. 2023.
Cooperative Retriever and Ranker in Deep Recommenders. In Proceedings of the
ACM Web Conference 2023. 1150–1161.
[18] Wang-Cheng Kang and Julian McAuley. 2019. Candidate Generation with Binary
Codes for Large-Scale Top-N Recommendation. In Proceedings of the 28th ACM
International Conference on Information and Knowledge Management (CIKM ’19).
1523–1532.
[19] Tor Lattimore and Csaba Szepesvári. 2020. Bandit algorithms. Cambridge Univer-
sity Press, Chapter 6, 97–98.
[20] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi Tang, Lichan Hong,
and Ed H. Chi. 2020. Off-policy Learning in Two-stage Recommender Systems.
InProceedings of The Web Conference 2020. 463–473.
[21] Xu Ma, Pengjie Wang, Hui Zhao, Shaoguo Liu, Chuhan Zhao, Wei Lin, Kuang-
Chih Lee, Jian Xu, and Bo Zheng. 2021. Towards a Better Tradeoff between
Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based
Approach. 2036–2040.
[22] Pascal Massart. 1990. The tight constant in the Dvoretzky-Kiefer-Wolfowitz
inequality. The annals of Probability (1990), 1269–1283.
[23] Roger B Myerson. 1981. Optimal auction design. Mathematics of operations
research 6, 1 (1981), 58–73.
[24] Lequn Wang and Thorsten Joachims. 2023. Uncertainty Quantification for Fair-
ness in Two-Stage Recommender Systems. In Proceedings of the Sixteenth ACM
International Conference on Web Search and Data Mining. 940–948.
[25] Yiqing Wang, Xiangyu Liu, Zhenzhe Zheng, Zhilin Zhang, Miao Xu, Chuan Yu,
and Fan Wu. [n. d.]. On Designing a Two-stage Auction for Online Advertising.
InProceedings of the ACM Web Conference 2022. 90–99.
[26] Zhe Wang, Liqin Zhao, Biye Jiang, Guorui Zhou, Xiaoqiang Zhu, and Kun
Gai. 2020. COLD: Towards the Next Generation of Pre-Ranking System.
arXiv:2007.16122 [cs.IR]
[27] Haike Xu and Jian Li. 2021. Simple combinatorial algorithms for combinatorial
bandits: Corruptions and approximations. In Uncertainty in Artificial Intelligence.
PMLR, 1444–1454.
 
1573KDD ’24, August 25–29, 2024, Barcelona, Spain Haoming Li et al.
[28] Yinglun Xu, Bhuvesh Kumar, and Jacob Abernethy. 2023. On the robustness
of epoch-greedy in multi-agent contextual bandit mechanisms. arXiv preprint
arXiv:2307.07675 (2023).
[29] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee
Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected neural
modeling for large corpus item recommendations. In Proceedings of the 13th ACM
Conference on Recommender Systems. 269–277.
[30] Mengxiao Zhang and Haipeng Luo. 2023. Online Learning in Contextual Second-
Price Pay-Per-Click Auctions. arXiv preprint arXiv:2310.05047 (2023).
[31] Mengyan Zhang, Thanh Nguyen-Tang, Fangzhao Wu, Zhenyu He, Xing Xie, and
Cheng Soon Ong. 2022. Two-Stage Neural Contextual Bandits for Personalised
News Recommendation. arXiv:2206.14648 [cs.IR]
[32] Zhishan Zhao, Jingyue Gao, Yu Zhang, Shuguang Han, Siyuan Lou, Xiang-
Rong Sheng, Zhe Wang, Han Zhu, Yuning Jiang, Jian Xu, and Bo Zheng.
2023. COPR: Consistency-Oriented Pre-Ranking for Online Advertising.
arXiv:2306.03516 [cs.IR]
[33] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang
Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate
prediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33.
5941–5948.
[34] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining. 1059–1068.
A PROOFS
Proof of Lemma 2. Decompose the regret to super arms,
𝑅𝑒𝑔(𝑇)=𝑇·OPT𝐷−E"𝑇∑︁
𝑡=1𝑅𝐷(𝐾𝑡)#
=∑︁
𝐾(𝑅𝐷(𝐾∗)−𝑅𝐷(𝐾))E[𝜏𝐾(𝑇)].
By sublinear regret,
0=lim
𝑇→∞𝑅𝑒𝑔(𝑇)
𝑇=∑︁
𝐾(𝑅𝐷(𝐾∗)−𝑅𝐷(𝐾))lim
𝑇→∞E[𝜏𝐾(𝑇)]
𝑇.
Since the optimal super arm is unique, for every 𝐾≠𝐾∗,𝑅𝐷(𝐾∗)−
𝑅𝐷(𝐾)>0, thus we have lim𝑇→∞E[𝜏𝐾(𝑇)]/𝑇=0. Note that
𝑇=Í
𝐾E[𝜏𝐾(𝑇)]. Therefore lim𝑇→∞E[𝜏𝐾∗(𝑇)]/𝑇=1.
□
Proof of Proposition 2. For truthfulness, fix any realization
table C, and consider the cumulative utility 𝑈𝑖(C,𝑏𝑖,b−𝑖)of bidder
𝑖. We separate 𝑈𝑖to the cumulative utility in exploration phase and
exploitation phase,
𝑈𝑖(C,𝑏𝑖,b−𝑖)=𝑚𝑘∑︁
𝑡=1𝑢𝑖𝑡(C,𝑏𝑖,b−𝑖)+𝑇∑︁
𝑡=𝑚𝑘+1𝑢𝑖𝑡(C,𝑏𝑖,b−𝑖)
where𝑢𝑖𝑡(C,𝑏𝑖,b−𝑖)is bidder𝑖’s utility in round 𝑡.
In the exploration phase, bidder 𝑖only participates in the subset
auction in certain fixed rounds, and its competitors in these rounds
is not influenced by 𝑏𝑖. For each of these rounds, by the truthfulness
of second-price auctions, bidder 𝑖optimizes its utility with truthful
bid𝑣𝑖.
For the exploitation phase, the empirical distributions ˆFin line 6
only depend on C, and are not influenced by 𝑏𝑖. The exploitation
phase consists of repeated second-price auctions on a fixed subset 𝐾,
which is selected by the oracle. From the truthfulness of the oracle,
for fixed ˆF, the combination of selecting 𝐾and running second-
price auction within 𝐾is truthful, with respect to any realization
c. This implies that the combination of selecting 𝐾and running
any one of the auctions in rounds 𝑚𝑘+1≤𝑡≤𝑇is truthful.Therefore𝑣𝑖is the maximizer of any of the objectives 𝑢𝑖𝑡(C,𝑏𝑖,b−𝑖)
for𝑚𝑘+1≤𝑡≤𝑇.
To conclude, the truthful bid 𝑣𝑖maximizes any of 𝑢𝑖𝑡(C,𝑏𝑖,b−𝑖)
for1≤𝑡≤𝑇, and thus maximizes 𝑈𝑖(C,𝑏𝑖,b−𝑖).
For IR, the total utility 𝑈𝑖of bidder𝑖is defined as the sum of
utility in each round,
𝑈𝑖(C,𝑣𝑖,b−𝑖)=𝑇∑︁
𝑡=1𝑢𝑖𝑡(C,𝑣𝑖,b−𝑖)
Fix any realization table C. For any𝑡, if𝑖∈𝐾𝑡, then𝑖participates
a second-price auction in round 𝑡. By the IR property of second-
price auctions, 𝑢𝑖𝑡(C,𝑣𝑖,b−𝑖)≥0. If𝑖∉𝐾𝑡, both allocation 𝑥𝑖𝑡and
payment𝑝𝑖𝑡are zero, thus 𝑢𝑖𝑡=0. Since each round produces non-
negative utility when truthful bidding, the total utility 𝑈𝑖(C,𝑣𝑖,b−𝑖)
is non-negative, therefore the mechanism is IR. □
Proof of Lemma 7. The oracle in Lemma 3 is a truthful𝑒−1
2𝑒-
approximation scoring oracle. Now we prove that its Lipschitz
constant is𝐿=1.
For distributions 𝐷and𝐷′, with CDFs 𝐹and𝐹′,
Score(𝐹)=𝜙1
𝑘
=E𝑟∼𝐷
𝑟·I
𝑟≥𝑞(1
𝑘)
=∫1
𝑞(1
𝑘)𝑟d𝐹(𝑟)
=∫𝑞(1
𝑘)
01
𝑘d𝑟+∫1
𝑞(1
𝑘)(1−𝐹(𝑟))d𝑟
Define
𝑧(𝑟):= 
1
𝑘0≤𝑟<𝑞
1
𝑘
1−𝐹(𝑟)𝑞
1
𝑘
≤𝑟≤1,
we have Score(𝐹)=∫1
0𝑧(𝑟)d𝑟.
For the difference of scores,
|Score(𝐹)−Score(𝐹′)|≤∫1
0|𝑧(𝑟)−𝑧′(𝑟)|d𝑟 (11)
Now we prove that for any 𝑟∈[0,1],
|𝑧(𝑟)−𝑧′(𝑟)|≤|𝐹(𝑟)−𝐹′(𝑟)|. (12)
Without loss of generality, assume 𝑞(1
𝑘)≤𝑞′(1
𝑘). Consider three
cases:
Case 1.𝑟<𝑞(1
𝑘). In this case 𝑧(𝑟)=𝑧′(𝑟)=1
𝑘,𝑧(𝑟)−𝑧′(𝑟)=0.
Case 2.𝑞(1
𝑘)≤𝑟<𝑞′(1
𝑘). By𝑞(1
𝑘)≤𝑞′(1
𝑘)we know𝐹′(𝑟)<
1−1
𝑘. Thus𝑧(𝑟)−𝑧′(𝑟)=1−𝐹(𝑟)−1
𝑘≤𝐹′(𝑟)−𝐹(𝑟)
Case 3.𝑟≥𝑞′(1
𝑘). In this case 𝑧(𝑟)−𝑧′(𝑟)=𝐹′(𝑟)−𝐹(𝑟)
Concluding the three cases finishes the proof of (12). Plugging
(12) into (11) gives us
|Score(𝐹)−Score(𝐹′)|≤∫1
0|𝐹(𝑟)−𝐹′(𝑟)|d𝑟≤∫1
0Λd𝑟≤Λ
□
 
1574Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD ’24, August 25–29, 2024, Barcelona, Spain
Proof of Proposition 3. For truthfulness, fix any realization
table C. In the exploration phase, the selected subset do not de-
pend on the bids. Moreover, the output super arm 𝐻𝑖𝑔ℎ is also not
influenced by the bids. In the exploitation phase, we repeatedly
run second-price auctions within 𝐻𝑖𝑔ℎ . Since a bidder cannot influ-
ence the selected subset 𝐾𝑡in any round 𝑡, the truthfulness of our
mechanism follows from the truthfulness of second-price auction.For IR, again fix any realization table C. For any round 𝑡, and any
bidder𝑖, if𝑖∈𝐾𝑡, then𝑖participates a second-price auction in round
𝑡. By the IR property of second-price auctions, 𝑢𝑖𝑡(C,𝑣𝑖,b−𝑖)≥0.
If𝑖∉𝐾𝑡, both allocation 𝑥𝑖𝑡and payment 𝑝𝑖𝑡are zero, thus 𝑢𝑖𝑡=
0. Since each round produces non-negative utility when truthful
bidding, the total utility 𝑈𝑖(C,𝑣𝑖,b−𝑖)is non-negative. □
 
1575