Policy-Based Bayesian Active Causal Discovery with Deep
Reinforcement Learning
Heyang Gao‚àó
gaoheyang@ruc.edu.cn
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, ChinaZexu Sun‚àó
sunzexu21@ruc.edu.cn
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, China
Hao Yang‚àó
hao.yang@ruc.edu.cn
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, ChinaXu Chen‚àó‚Ä†
xu.chen@ruc.edu.cn
Gaoling School of Artificial Intelligence
Renmin University of China
Beijing, China
ABSTRACT
Causal discovery with observational and interventional data plays
an important role in numerous fields. Due to the costly and poten-
tially risky nature of intervention experiments, selecting informa-
tive interventions is critical in real-world situations. Several recent
works introduce Bayesian active learning to select interventions
that maximize the expected information gain about the underlying
causal relationship at each optimization step. However, there are
still some limitations within these methods: (1) Local optimality.
With multiple intervention experiments, selecting optimal interven-
tion myopically at each step may drop into the local optimal point.
(2)Expensive time cost. Optimizing the most informative interven-
tion at each step is time-consuming and not suitable for adaptive
experiments with strict inference speed requirements. In this study,
we propose a novel method called Reinforcement Learning-based
Causal Bayesian Experimental Design (RL-CBED) to reduce the risk
of local optimality and accelerate intervention selection inference.
Specifically, we formulate the active causal discovery problem as a
partially observable Markov decision process (POMDP). We design
an information gain-based sparse reward function and then improve
it to a dense reward function, providing fine-grained feedback to
help the RL policy learn more quickly in complex environments.
Moreover, we theoretically prove that the Q-function estimator can
be learned using only trajectories sampled from the prior, which
can significantly reduce the time cost of training process, enabling
the real-world application of our method. Extensive experiments
on both synthetic and real world-inspired semi-synthetic datasets
demonstrate the effectiveness of our proposed method.
‚àóAlso with Beijing Key Laboratory of Big Data Management and Analysis Methods.
‚Ä†Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
¬©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671705CCS CONCEPTS
‚Ä¢Computing methodologies ‚ÜíReinforcement learning; Ac-
tive learning settings.
KEYWORDS
Causal Discovery; Active Learning; Experimental Design; Rein-
forcement Learning
ACM Reference Format:
Heyang Gao, Zexu Sun, Hao Yang, and Xu Chen. 2024. Policy-Based Bayesian
Active Causal Discovery with Deep Reinforcement Learning. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Min-
ing (KDD ‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/3637528.3671705
1 INTRODUCTION
Identifying the causal relationship among variables of interest in
a system is a fundamental problem in various scientific research
domains, including biology [ 34,40], drug discovery [ 31], econom-
ics [35], and social science [ 29,38]. Causal discovery aims to learn
the Directed Acyclic Graph (DAG) structure of a set of variables and
its associated assignment mechanisms of each variables. Typically,
relying solely on observational data is insufficient for identifying
the unique DAG and one can only discover its Markov Equivalence
Class (MEC) [ 51]. Performing additional intervention experiments
can help alleviate the confusion in the MEC, but is usually expensive
and even risky [ 3]. With the development of active learning, several
methods are proposed to selecting intervention target and value
reasonably in order to discover more causal relationship under the
condition of controlling the cost of intervention experiments. As
shown in Figure 1, Active Causal Discovery (ACD) is a framework
where we first observe a batch of samples from the system of inter-
est, and use appropriate causal discovery algorithm to calculate the
posterior distribution over the Structural Causal Model (SCM) with
observational data. We then design interventional experiments, i.e.,
interventional target and value pairs, in order to maximally reduce
our uncertainty about the SCM. After performing the intervention,
we collect interventional data and use it to update the causal dis-
covery outcome. If experimental resources permit, this process can
be repeated and our estimated SCM gradually approaches the true
 
839
KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Heyang Gao, Zexu Sun, Hao Yang, and Xu Chen
ùììobserve
causal 
discovery
method
(GIES, NOTEARS‚Ä¶)
Update 
SCM 
distribution
Select 
intervention 
ùùÉ=(ùëø,ùíó)ùììùíäùíèùíï
Perform intervention 
ùíÖùíê(ùëø=ùíó)add policy -
basedoptimization -
based
EIG
value
targetvalue ùíótarget ùëø
trained policy
Figure 1: Workflow of Active Causal Discovery. Experimental
design methods optimize interventional experiments that
help uncover more information about underlying causality.
SCM. The aim of active causal discovery is to optimize interven-
tion experiments to discover more information about underlying
causality when the total cost (e.g., the number of experiments) is
limited.
Several recent studies [ 3,46] have incorporated Bayesian Opti-
mal Experimental Design methods into active causal discovery in
order to select interventions that maximize the expected informa-
tion gain (EIG) about the underlying causal relationship at each
step. However, there are limitations to these optimization-based
methods. Firstly, the existing approaches myopically select the in-
tervention that maximizes EIG at each step, without considering
it as a sequential process. After obtaining the intervention data
at each step, the causal discovery algorithm utilizes this data to
update the posterior model, that is, updating the predicted SCM. Se-
lecting interventions to maximize one-step information gain based
on the updated SCM often leads to getting stuck in some wrong
SCM, known as the local optimality, and is unable to maximize
the global EIG. Secondly, existing methods rely on optimization
algorithms at each step, such as Bayesian optimization, gradient de-
scent, etc., which are time-consuming for inferring and not suitable
for scenarios requiring adaptive experiments. In real-world causal
discovery situations, it is often essential to quickly decide the next
intervention based on current experimental results. The existing
methods optimize and find the optimal intervention for the current
step, which results in expensive time cost.
In this study, we propose a novel policy-based method for Bayesian
active causal discovery, achieving a new and effective reinforce-
ment learning solution to capture the sequential information among
each intervention and reduce the time cost of intervention selec-
tion. Specifically, we formulate the active causal discovery problem
as a partially observed Markov decision process (POMDP) with
an information gain-based reward function and design a novel
neural network architecture for discrete-continuous hybrid action
space. Since the sparse reward function only provide a single re-
ward (i.e., the total EIG estimation) at the end of each interventionsequence, making the RL policy to train slowly and even unsuccess-
fully. Hence, we further propose a dense reward function, which
can provide fine-grained feedback to help the training process. Fur-
thermore, we demonstrate that the Q-function estimator in our
formulation can be learned using only trajectories sampled from
the prior, which can significantly reduce the time cost of training
process, enabling the real-world application of our method. Com-
pared to prior work in the field of ACD, our work utilizes trained
policy to select interventions, which non-myopically optimizes the
total EIG during the whole design process instead of iterative EIG
at each step and does not require updates of the posterior over
the SCM in each iteration during the training process. In addition,
our method, with trained policy, can rapidly select interventions
during inference and is well-suited for adaptive causal discovery
experiments.
The main contributions of our work can be summarized as:
‚Ä¢We formulate the active causal discovery problem as a POMDP,
and design a reasonable dense information gain estimation
reward signal.
‚Ä¢We introduce RL-CBED, which is to our knowledge, the first
reinforcement learning-based Bayesian active causal discov-
ery method and design a novel neural network architecture
for discrete-continuous hybrid action space.
‚Ä¢We prove theoretically that within our POMDP formulation,
the Q function estimator can be learned using trajectories
sampled only from the prior, which allows for training pro-
cess without updating causal discovery posterior model.
‚Ä¢We conduct experiments on synthetic and semi-synthetic
datasets to compare our proposed method with baselines
empirically and the results demonstrates our the superiority
of our method.
2 RELATED WORK
2.1 Bayesian Optimal Experimental Design
Bayesian Optimal Experimental Design (BOED) [ 6,27,32] is a prin-
cipled model-based framework for optimal experimental design
selection, and it has been successfully utilized in numerous scien-
tific research fields [ 42,50,52]. One notable development is the
emergence of sequential BOED methods [ 39], which adaptively se-
lect new experimental conditions based on the outcomes of previous
experiments. The conventional approaches for posterior inference
at each timestep include sequential Monte Carlo [ 7,8], variational
inference [ 11,12] and Laplace approximation [ 25,28]. Foster et
al. [10] amortized sequential BOED with a adaptive policy, enabling
researchers to efficiently allocate resources and obtain information
in real-time. Ivanova et al. [ 22] then extended it to implicit likeli-
hood and can practically perform real-time adaptive BOED with
implicit models. Some recent work has also focused sepcifically
on non-myopical strategies for sequential BOED. Huan et al. [ 21]
considered policy-based BOED and utilize approximate dynamic
programming to obtain a design policy for experimental designs.
Jiang et al. [ 23] proposed a batch-informed selection method and
obtain non-myopic approximations to the optimal experimental
policy. Besides, there are also some approaches considering RL-
based sequential BOED in both closed-form likelihood [ 5,44] and
intractable likelihood [26] settings.
 
840Policy-Based Bayesian Active Causal Discovery with Deep Reinforcement Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
2.2 Active Causal Discovery
Several studies have investigated active causal discovery [ 19,48,54],
where researchers perform intervention on the variables in the sys-
tem at each step, obtain interventional data and attempt to identify
the underlying causal relationship at minimal cost. One of the main-
stream approaches to active causal discovery is Bayesian Optimal
Experimental Design for Causal Discovery (BOECD), which is the
same setting as our work. Early methods [ 30,47] often make some
assumptions and consider simple settings such as discrete variables
and topologically ordered graphs. In recent years, BOECD-based
active causal discovery has been explored in different settings and
applications [ 15,33]. Agrawal et al. [ 2] proposed ABCD frame-
work which estimates MI with weighted importance sampling in
the continuous variable setting. Tigas et al. [ 46] utilized Bayesian
optimization to identify the intervention values and proposed an
algorithm for batch intervention setting. Annadani et al. [ 3] then
extended it to a gradient-based optimization procedure to acquire
a set of optimal intervention target-state pairs. Besides Bayesian
based methods, there are also other approaches to active causal
discovery [ 24,43,45,53]. A-ICP [ 14] is an active causal discovery
framwork based on ICP [ 37] which selects the intervention but
can not be utilized in the full graph discovery setting. AIT [ 41]
decides where to intervene by computing a score for all possible
intervention targets in a non-Bayesian framework.
3 PRELIMINARIES
3.1 Causality
One of the prominent mathematical models for representing causal
relationships between variables of interest is Structural Causal
Model (SCM). Here we introduce the relevant background briefly.
LetX={ùëã1,...,ùëãùëë}be a set ofùëëobserved variables. Each vari-
ablesùëãùëñassociates with a function, or mechanism, to generate the
variable as a function of its parents ùëãpa(ùëñ)and a noise variable ùúñùëñ
with distribution ùëÉ(ùúñùëñ):
ùëãùëñBùëìùëñ(Xpa(ùëñ),ùúñùëñ), ùëñ‚àà{1,...,ùëë}. (1)
We consider a common form of ùëìùëñin this work, i.e., Gaussian additive
noise models (ANM):
ùëãùëñBùëìùëñ(Xpa(ùëñ);ùõæùëñ)+ùúñùëñ, ùúñùëñ‚àºN( 0,ùúé2
ùëñ), (2)
where the mechanism ùëìùëñis parameterized by ùõæùëñand noise variance
ùúé2
ùëñ. A Directed Acyclic Graph (DAG) g=(V,E)is associated with
each SCM. The endogenous variables Xare indexed by the vertex set
V={1,...,ùëë}, and the edge ùëó‚Üíùëñexists if and only if ùëãùëó‚ààXpa(ùëñ).
Interventions. Based on an SCMMand its associated DAG g,
interventions, manipulations to some structural equations, can be de-
noted by do-operator [ 35] asùëëùëú({ùëãùëñ=ùë£ùëñ}ùëñ‚ààùêº), which represents the
manipulation of deleting from the model {ùëãùëñ=ùëìùëñ(Xpa(ùëñ),ùëàùëñ)}ùëñ‚ààùêº
all structural equations corresponding to variables whose index is
inùêºand substituting{ùëãùëñ=ùë£ùëñ}ùëñ‚ààùêºin the remaining equations. Then
the associated DAG gis also mutilated by cutting off the edges from
all parents of ùëñ‚ààùêºto the node ùëñ.
3.2 Bayesian Optimal Experimental Design
Researchers aim to design informative experiments so that the out-
comes can help to uncover more information about the unknownunderlying process. Bayesian Optimal Experimental Design (BOED)
is a mathematically well-defined framework for describing the de-
sign process in a Bayesian manner. In BOED framework, the exper-
imental outcome ùë¶is modelled with an explicit likelihood ùëù(ùë¶|ùúÉ,ùúâ)
and a prior belief ùëù(ùúÉ)representing initial beliefs about the param-
eters of interest. After conducting the experiment ùúâand observing
the outcome ùë¶, the belief is updated to the posterior ùëù(ùúÉ|ùúâ,ùë¶). A
common setting is sequential experimental design, where the above
process iterates for ùëátimes. In each iteration ùë°‚àà{1,...,ùëá}, an ex-
perimentùúâùë°is selected and then conducted, yielding the outcome
ùë¶ùë°.
Let‚Ñéùë°B{(ùúâ1,ùë¶1),...,(ùúâùë°,ùë¶ùë°)}be the history of design-outcome
pairs at time step ùë°. Previous works have proposed optimizing a
policyùúã:H‚Üí Œûrather than optimizing designs directly, where
HandŒûare the history and design spaces, respectively. The policy
determines the next experimental design ùúâùë°with the history ‚Ñéùë°‚àí1,
i.e.,ùúâùë°=ùúã(‚Ñéùë°‚àí1). The data generating process can be describe as
ùëù(ùúÉ)ùëù(‚Ñéùëá|ùúÉ,ùúã)=ùëù(ùúÉ)ùëá√ñ
ùë°=1ùëù(ùë¶ùë°|ùúÉ,ùúâùë°). (3)
Then the total expected information gain for policy ùúãover a se-
quence ofùëáexperiments is
Iùëá(ùúã)=Eùëù(ùúÉ)ùëù(‚Ñéùëá|ùúÉ,ùúã)[logùëù(‚Ñéùëá|ùúÉ,ùúã)‚àílogùëù(‚Ñéùëá|ùúã)],(4)
whereùëù(‚Ñéùëá|ùúã)=Eùëù(ùúÉ)[ùëù(‚Ñéùëá|ùúÉ,ùúã)]. The total EIG does not con-
sider the intermediate posteriors and only calculates the entropy
reduction from the initial prior ùëù(ùúÉ)to the final posterior ùëù(ùúÉ|‚Ñéùëá).
3.3 Reinforcement Learning
We introduce Partially Observed Markov Decision Process (POMDP)
for modelling the relationship between an agent and its environ-
ment, where the environment dynamics is decided by an MDP, and
the agent can only receive an observation instead of the underlying
state. Formally, a POMDP is a 7-tuple (S,A,P,ùëÖ,Œ©,O,ùõæ), where
Sis a set of states, Ais a set of actions, Pis transition dynamics
P:S√óA√óS‚Üí [0,1],ùëÖis reward function ùëÖ:S√óA‚ÜíR,
Œ©is a set of observations, Ois observation emission probability
O:S√óŒ©‚Üí[0,1], andùõæis the discount factor. At each time step
ùë°, the environment is in a state ùë†ùë°. The agent takes an action ùëéùë°and
then the environment transitions from state ùë†ùë°toùë†ùë°+1according
to the transition dynamics P(ùë†ùë°+1|ùë†ùë°,ùëéùë°). The environment then
reveals to the agent a reward ùëüùë°=ùëÖ(ùë†ùë°,ùëéùë°)and an observation
ùëúùë°+1‚àºO(¬∑|ùë†ùë°+1). The process repeats until the time step arrives ùëá
for finite-horizon process. The goal for the agent is to take appro-
priate actions at each time step so that the expected cumulative
discounted reward E√çùëá‚àí1
ùë°=0ùõæùë°ùëüùë°
is maximized.
4 PROPOSED METHOD
In this section, we propose an Reinforcement Learning-based Causal
Bayesian Experimental Design method (also called RL-CBED for
short) to achieve expected information gain maximization after a
certain number of interventions. For active causal discovery, an ex-
periment is an intervention on variables Xùêºwith values vùêº, denoted
byùúâB{(Xùêº,vùêº)}=ùëëùëú({ùëãùëñ=ùë£ùëñ}ùëñ‚ààùêº).
 
841KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Heyang Gao, Zexu Sun, Hao Yang, and Xu Chen
node value
5.0
True Causal Graph
1: 0.56
2: -3.12
3: 8.45
4: -1.93
‚Ä¶‚Ä¶
Observational Data ùíöùüé
Causal Discovery 
Posterior ModelInterventional Data ùíöùüè1: 0.85
2: 5.0
3: 4.32
4: -5.72
‚Ä¶‚Ä¶
Reward Function ùíìùíï
action ùíÇùüèIntervention Experiments 
on Hypotheses
‚Ä¶
ùìóùüè ùìóùüêRL policy
history encoder ùë¨value network
target networkùíöùüé
ùíÇùüéùíêùüéùê°ùüéùêØùüé
ùê™ùüésoftmaxEnvironment Environment
‚Ä¶
ùìóùüè ùìóùüê
HypothesesHypotheses
ùìõùíïùíÇùíìùíàùíÜùíïùìõùíóùíÇùíçùíñùíÜùíöùüè
ùíÇùüèùíêùüèùíöùüé
ùíÇùüéRL 
policy‚Ä¶‚Ä¶
Step 1 Step 2
Figure 2: Overview of RL-CBED. First we use observational data ùë¶0to perform causal discovery and get SCM hypotheses. At
each stepùë°, we concatenate data and previous action to get observation ùëúùë°and input it into RL policy. The output action ùëéùë°is
used to perform intervention on hypotheses and obtain interventional data ùë¶ùë°+1. We finally use calculated reward ùëüùë°to update
the network in RL policy.
4.1 POMDP Formulation
In order to optimize the total causal experimental design process
using deep reinforcement learning, we first need to cast the active
causal discovery problem as a POMDP, in which a policy can be
trained to decide where and how to intervene after observing the
intervention history and the outcomes of intervention experiments
at each step. In this section, we first introduce how to define states,
observations, actions, and transition dynamics in our settings and
leave the reward design to the next section.
Stateùë†ùë°. We define the state ùë†ùë°as a combination of the history
of interventions and outcomes until the previous step ‚Ñéùë°‚àí1, the
causal graph g0discovered by the causal discovery model using
observational data, and ùêøcontrastive samples of the causal graphs
g1:ùêø, which are sampled from the same prior distribution ùëù(g|Dùëúùëèùë†)
independently as g0and used to calculate contrastive bound as
defined later in the reward design. Formally, state ùë†ùë°B(‚Ñéùë°‚àí1,g0:ùêø).
Note that in POMDP formulation, the RL agent cannot observe the
whole state ùë†ùë°, and g0:ùêøis kept in the environment.
Observation ùëúùë°. At each time step, the RL agent can only ob-
serve the history ‚Ñéùë°‚àí1={(ùëéùëñ,ùë¶ùëñ)}ùë°‚àí1
ùëñ=1, consisting of the previous
action (intervention target-value pair) ùëéùëñat each time step and the
interventional samples ùë¶ùëñcollected under the intervention indicated
byùëéùëñ, that is, observation ùëúùë°B‚Ñéùë°‚àí1and we setùëú1=‚àÖ.
Actionùëéùë°. We define action as ùëéùë°B(ùëñùë°,ùë£ùë°). Actionùëéùë°is the in-
tervention that the agent decides to perform at current time step. It
consists of the interventional target node ùëãùëñùë°and the corresponding
interventional value ùë£ùë°, that is, an intervention ùëëùëú(ùëãùëñùë°=ùë£ùë°).
Transition dynamics P. After the action ùëéùë°=(ùëñùë°,ùë£ùë°)is chosen,
the stateùë†ùë°=(‚Ñéùë°‚àí1,g0:ùêø)transitions to ùë†ùë°+1=(‚Ñéùë°,g0:ùêø), whereùë¶ùë°
is the samples obtained under the intervention indicated by ùëéùë°,
i.e.,ùë¶ùë°‚àºùëù(¬∑|g0,ùëëùëú(ùëãùëñùë°=ùë£ùë°)). Consequently, transition dynamics
ùëù(ùë†ùë°+1|ùë†ùë°,ùëéùë°)=ùëù(ùë¶ùë°|‚Ñéùë°‚àí1,g0,ùëëùëú(ùëãùëñùë°=ùë£ùë°)).Observation emission probability O. As described earilier, the
RL agent can only observe the history ‚Ñéùë°‚àí1at each time step ùë°, so
the observation emission probability Ofor stateùë†ùë°=(‚Ñéùë°‚àí1,g0:ùêø)is
simply described as ùëù(ùëúùë°|ùë†ùë°=(‚Ñéùë°‚àí1,g0:ùêø))= 1[ùëúùë°=‚Ñéùë°‚àí1], where
1[¬∑]is the indicator function.
4.2 Bayesian-driven Reward Design
In this section, we demonstrate how to design a reasonable reward
function that allows the RL agent to learn an intervention policy
that maximizes the information about underlying causality gained
in the multiple experiment setting. First we propose a naive sparse
reward function based on the estimator of total EIG. Then we im-
prove it and get a dense reward function to solve the sparse reward
signal problem in reinforcement learning.
4.2.1 Sparse Reward Function. As described eariler in Section 3.2,
we utilize the total expected information gain for policy ùúãover a
sequence of ùëáexperimentsIùëá(ùúã)to represent how much causal
relationship the policy ùúãdiscover after ùëáinterventions. Since the
total EIG is intractable, Foster et al . [10] has proposed its upper
bound estimationUsNMC(ùúã), called sequential Nested Monte Carlo
(sNMC) estimation, to estimate the total EIG:
UsNMC(ùúã)=Eùëù(g0:ùêø)ùëù(‚Ñéùëá|g0,ùúã)"
logùëù(‚Ñéùëá|g0,ùúã)
1
ùêø+1√çùêø
‚Ñì=1ùëù(‚Ñéùëá|g‚Ñì,ùúã)#
,(5)
where g0:ùêø‚àºùëù(g|Dùëúùëèùë†)are sampled independently from the prior
given only observational data. Therefore, a naive reward design
approach is to only assign the total EIG estimation to the final step
of a sequence of interventions, that is,
 
842Policy-Based Bayesian Active Causal Discovery with Deep Reinforcement Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
ùëüùë°=Ô£±Ô£¥Ô£¥Ô£¥Ô£¥ Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥0, ifùë°<ùëá,
logùëù(‚Ñéùëá|g0,ùúã)‚àílogùêø‚àëÔ∏Å
‚Ñì=1ùëù(‚Ñéùëá|g‚Ñì,ùúã),ifùë°=ùëá.(6)
Intuitively, the RL agent receives zero reward for all time steps
ùë°<ùëáuntil the final step to obtain the information gain of the
entire trajectory. Proposition 4.1 illustrates that following the above
formulation and the sparse reward function in Equation 6, the policy
optimized for the POMDP also solve the BOECD problem.
Proposition 4.1. LetM:(S,A,P,ùëÖ,Œ©,O,ùõæ)be a POMDP where
stateùë†ùë°=(‚Ñéùë°‚àí1,g0:ùêø), observation ùëúùë°=‚Ñéùë°‚àí1, actionùëéùë°=(ùëñùë°,ùë£ùë°),
transition dynamics P=ùëù(ùë¶ùë°|‚Ñéùë°‚àí1,g0,ùëëùëú(ùëãùëñùë°=ùë£ùë°)), observation
emission probability O= 1[ùëúùë°=‚Ñéùë°‚àí1], discount factor ùõæ=1and
rewardùëüùë°follows Equation 6, then the expected cumulative discounted
reward
Eùúã"ùëá‚àëÔ∏Å
ùë°=1ùõæùë°ùëüùë°#
=UsNMC(ùúã). (7)
See Appendix A.1 for proof. According to Proposition 4.1, it is
reasonable to utilize RL algorithm to solve this POMDP and the
trained policy that maximizes the expected return can be used in
the BOECD problem.
4.2.2 Dense Reward Function. In practice, sparse reward signal is
a key challenge for RL algorithms. The sparse reward signal above
is only provided by the environment at the final time step of an
episode, so it is hard for RL algorithms to associate the sequence of
previous actions with this final reward.
We now give the definition of our dense reward as follows
ùëüùë°=logùëù(ùë¶ùë°|g0,ùëéùë°)
‚àílog"ùë°√ñ
ùëò=1ùëù(ùë¶ùëò|g1,ùëéùëò)+¬∑¬∑¬∑+ùë°√ñ
ùëò=1ùëù(ùë¶ùëò|gùêø,ùëéùëò)#
+log"ùë°‚àí1√ñ
ùëò=1ùëù(ùë¶ùëò|g1,ùëéùëò)+¬∑¬∑¬∑+ùë°‚àí1√ñ
ùëò=1ùëù(ùë¶ùëò|gùêø,ùëéùëò)#
.(8)
Under the dense reward formulation, the RL agent can receive
rewards after each action of intervention experimental design. The
dense reward formulation estimates the marginal expected informa-
tion gain of intervention ùëéùë°at time step ùë°. Proposition 4.2 demon-
strates that the dense reward we propose in Equation 8 is equivalent
to the sparse reward in Equation 6 in terms of the returns of two
trajectories with the same state-action pairs.
Proposition 4.2 (Eqivalence of Dense and Sparse Rewards).
For a trajectoryTùë†=(ùë†1,ùëé1,ùëüùë†
1,...,ùë†ùëá,ùëéùëá,ùëüùë†
ùëá)using sparse rewards
in Equation 6 and a trajectory Tùëë=(ùë†1,ùëé1,ùëüùëë
1,...,ùë†ùëá,ùëéùëá,ùëüùëë
ùëá)using
dense rewards in Equation 8 with the same state-action pairs during
the entire trajectory, the returns of two trajectories are the same, that
is,
ùëá‚àëÔ∏Å
ùë°=1ùõæùë°ùëüùë†
ùë°=ùëá‚àëÔ∏Å
ùë°=1ùõæùë°ùëüùëë
ùë°, (9)
where the discount factor ùõæ=1.See Appendix A.2 for proof. Proposition 4.2 provides a good
demonstration of the equivalence of dense and sparse rewards in
terms of the returns. Thus, the dense reward formulation assigns
the same total reward as the sparse reward for each trajectory. Com-
pared with the sparse reward, the reward formulation we define in
Equation 8 is nonzero for all time steps ùë°and can provide a dense
reward signal for RL agent to learn intervention selection policy.
4.3 Architecture and Optimization of RL Policy
4.3.1 Network Structure. Since both the discrete intervention tar-
get and the continuous intervention value are selected in each time
step, it is a discrete-continuous hybrid action space problem in
the reinforcement learning literature. We introduce a novel neu-
ral network architecture, which is designed for the scenario with
discrete-continuous hybrid action space in our setting.
Suppose that the number of nodes in the causal graph is ùëõ. At
each timestep, we need to select one node out of ùëõnodes and deter-
mine the intervention value for that node in the range of available
values. In our architecture, we design a discrete-action target net-
workùëìùë°ùëéùëüùëîùëíùë° and a continuous-action value network ùëìùë£ùëéùëôùë¢ùëí , which
share a base LSTM feature extractor ùê∏that converts observation
ùëúùë°=‚Ñéùë°‚àí1into dense vector representations hùë°=ùê∏(ùëúùë°). We first
input hùë°into the deterministic continuous-action value network,
which outputs a vector vwith lengthùëõ, representing the correspond-
ing intervention values for each node, that is, vùë°=ùëìùë£ùëéùëôùë¢ùëí(hùë°). Then
we concatenate the observation representations hùë°and the output
of continuous-action value network vùë°, and input the concatenated
vector into the discrete-action target network, which outputs the
optimal action-value function ùëÑ‚àóof selecting each node with the
corresponding previously selected value, qùë°=ùëìùë°ùëéùëüùëîùëíùë°(hùë°;vùë°). Fi-
nally, we use a softmax layer to change ùëÑ‚àóvalue to a action selection
distribution and sample action (ùëòùë°,vùë°[ùëòùë°])from the distribution
softmax(qùë°).
4.3.2 Overall Optimization. We use Q-learning algorithm to update
the parameter of target network ùëìùë°ùëéùëüùëîùëíùë° . Specifically, we calculate
TD error between predicted Q value of action qùë°[ùëòùë°]and TD target
ùëüùë°+ùõæ¬∑qùë°+1[ùëòùë°+1]and use least squares loss as loss function. We also
add an entropy regularization term to encourage the exploration
during the training process. The value network update loss can be
written as:
Lùë°ùëéùëüùëîùëíùë° =1
2
qùë°[ùëòùë°]‚àí(ùëüùë°+ùõæ¬∑qùë°+1[ùëòùë°+1])2
+ùõº¬∑H(softmax(qùë°)),
(10)
where the entropy of policy H(ùúã)=‚àí√ç
ùúãùëé‚ààùúãùúãùëélogùúãùëérepresents
the randomness of a policy ùúãandùõºis used to control the trade-off
between exploration and exploitation.
Furthermore, in order to update the parameter of value network
ùëìùë£ùëéùëôùë¢ùëí , we use deterministic policy gradient (DPG) algorithm and
the value network update loss can be written as:
Lùë£ùëéùëôùë¢ùëí =‚àíùëõ‚àëÔ∏Å
ùëñ=1qùë°[ùëñ]. (11)
A summary of our proposed algorithm is given in Algorithm 1.
 
843KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Heyang Gao, Zexu Sun, Hao Yang, and Xu Chen
Algorithm 1: RL-CBED
Input: SCM environmentE, number of initial observational
samplesùëÅ, training steps ùêº, number of intervention
stepsùëá
1SampleùëÅobservational samples ùë¶0fromE.
2InitializeDùëúùëèùë†={ùë¶0}andDùëñùëõùë°=‚àÖ.
3Infer the prior ùëù(g0:ùêø|Dùëúùëèùë†).
4fortraining iteration ùëñ=1...ùêºdo
5 forintervention step ùë°=1...ùëá do
6 Observation ùëúùë°=(ùëé1:ùë°‚àí1,ùë¶1:ùë°‚àí1)
7 Actionùëéùë°‚àºùúã(ùëé|ùëúùë°).
8 Obtain samples ùë¶ùë°under the intervention ùëéùë°.
9 Calculate reward ùëüùë°using Equation (8).
10 Update target network and encoder parameters
usingLùë°ùëéùëüùëîùëíùë° .
11 Update value network parameters using Lùë£ùëéùëôùë¢ùëí .
// Intervene with the trained policy ùúã(ùëé|ùë†;ùúÉùúã)
12forintervention step ùë°=1...ùëá do
13 Observation ùëúùë°=(ùëé1:ùë°‚àí1,ùë¶1:ùë°‚àí1)
14 Actionùëéùë°‚àºùúã(ùëé|ùëúùë°;ùúÉùúã)
15Dùëñùëõùë°=Dùëñùëõùë°‚à™{samples under intervention ùëéùë°}
16 Update the posterior ùëù(g0:ùêø|Dùëúùëèùë†‚à™Dùëñùëõùë°).
4.4 Theoretical Analysis
In this section, we discuss a key elegant property of our proposed
approach similar to Blau et al . [5]. We show that the RL agent can
interact with the environment with no need for updating causal dis-
covery posterior model during the training process, which enables
a substantial reduction in time cost.
Theorem 4.3 (Q Function Estimator from Priors). LetMbe
a POMDP where state ùë†ùë°=(‚Ñéùë°‚àí1,g0:ùêø), observation ùëúùë°=‚Ñéùë°‚àí1, action
ùëéùë°=(ùëñùë°,ùë£ùë°)and the transition dynamics P=ùëù(ùë¶ùë°|‚Ñéùë°‚àí1,g0,ùëëùëú(ùëãùëñùë°=
ùë£ùë°)). For an arbitrary policy ùúã, the Q function estimator ÀÜùëÑùúãcan be
learned using trajectories sampled only from the prior ùëù(‚Ñéùëá|ùúã,g0:ùêø),
where g0:ùêø‚àºùëù(¬∑|Dùëúùëèùë†).
See Appendix A.3 for proof. Theorem 4.3 demonstrates that
during the training process, there is no need to update the causal
discovery posterior model with interventional data at each timestep
and the value function can be learned using only samples from the
prior, that is, the causal graph g0:ùêøinferred using only observational
dataDùëúùëèùë†. With this property, the time cost of training process of
RL policy can be reduced significantly, which enables our method
to be applied in real scenarios.
5 EXPERIMENTS
In this section, we conduct experiments to investigate the following
research questions. RQ1: How is the performance of our RL-CBED
compared with other baselines on both synthetic and real world-
inspired semi-synthetic datasets? RQ2: How is the time cost of
our RL-CBED compared with other baselines during the inference
process? RQ3: Is our approach robust to changes in different various
factors, including intervention steps and MEC size? RQ4: Comparedwith sparse reward function, to what extent does our proposed
dense reward function help the reinforcement learning training
process converge?
5.1 Experimental Setup
We evaluate our proposed method on three commonly used causal
discovery datasets, including Erd≈ës-R√©nyi (ER) graph, Scale-Free
(SF) graph and DREAM.
5.1.1 Baselines. We compare our RL-CBED with several existing
competitive methods of active causal discovery as follows.
‚Ä¢Random-Fixed: For each intervention, select the target
node uniformly at random, and set the value of the interven-
tion to a fixed value. In the experiments, we fix this value to
0, as introduced in [2, 46].
‚Ä¢Random-Random: Select both the target node and its as-
sociated interventional value uniformly and randomly, as
introduced in [3, 48].
‚Ä¢ABCD [2]: Active Budgeted Causal Design (ABCD) strategy
estimates MI with weighted importance sampling. It selects
only intervention targets and sets intervention values to a
fixed value.
‚Ä¢AIT [41]: Active Intervention Targeting (AIT) decides where
to intervene by computing a score for all possible interven-
tion targets in a non-Bayesian framework. It also fixes inter-
vention values like ABCD strategy.
‚Ä¢CBED [46]: A Bayesian optimization-based black-box method
to optimize over both intervention target and value with an
MI estimator similar to Bayesian Active Learning by Dis-
agreement (BALD) [20].
‚Ä¢DiffCBED [3]: A gradient-based causal experimental de-
sign method which selects the optimal intervention with the
largest estimated information gain at each time step.
5.1.2 Tasks. We conduct experiments on several common causal
discovery tasks, including synthetic and semi-synthetic graphs.
For synthetic graphs, we generate two types of synthetic causal
graphs, Erd≈ës-R√©nyi (ER) graph [ 9] and Scale-Free (SF) graph [ 4],
each with 10, 20 and 30 nodes. For real-world inspired dataset, we
use DREAM [ 16], which is a family of benchmarks designed to
assess causal discovery algorithms. In our experiments, we use two
sub-datasets Ecoli andYeast networks each with 10 nodes as the
true causal graph and simulate the mechanisms, which is the same
setting as described in [ 3]. We provide more details in Appendix B.1.
5.1.3 Evaluation Metrics. We utilize several different metrics to as-
sess the discrepancy between the causal graphs found by method af-
ter several interventional experiments and the ground-truth causal
graph, in order to demonstrate to what extent the intervention ex-
periments designed using different methods reveal the underlying
causal relationships. The metrics include F1-score, Structural Ham-
ming Distance (SHD), Structural Intervention Distance (SID) and
Interventional Maximum Mean Discrepancy (i-MMD). We provide
more details about these metrics in Appendix B.2.
5.2 Implementation Details
We implement all baselines and our RL-CBED based on PyTorch 2.1
and Jax 0.4.19. We use the i-MMD metric as a reference to search
 
844Policy-Based Bayesian Active Causal Discovery with Deep Reinforcement Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
for the best hyper-parameters for all baselines and our method.
We utilize DAG-bootstrap [ 13] with GIES [ 18] as our posterior
causal discovery model and set the bootstrap number to 30. All
experiments are implemented on NVIDIA GeForce RTX 3090 and
Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz. We provide more
details and hyper-parameters for reproducibility in Appendix C.
5.3 Overall Performance (RQ1)
Table 1 shows the comparison of overall performance on ER graph
and SF graph with 10, 20, and 30 nodes. Our observations about the
experiment results are as follows.
First, our method RL-CBED outperforms the state-of-the-art
baselines consistently across all datasets for most metrics. Specifi-
cally, our proposed method can achieve significant improvement
in terms of i-MMD metrics after specified number of interventions,
which reveals a better estimation of both the ground-truth causal
graph and the corresponding conditional distributions. The im-
provement demonstrate the effectiveness of RL-CBED in selecting
reasonable interventions and can be attributed to the non-myopic
intervention selection policy compared with previous heuristic and
myopic methods such as CBED and DiffCBED. As for F1 score, SHD
and SID metrics, which consider only the causal graph, our method
also typically outperforms other existing methods. This suggests
that RL-CBED can offer valuable insights and advisable selections
in a wide range of graph data.
Second, previous methods exhibit a substantial decline in perfor-
mance when dealing with high-dimensional graph data, while our
method shows scalability as the graph size changes. Although some
previous methods work well and are comparable to our method in
10-node graph, they suffer from a performance drop as the graph
size scale to 20 or 30 nodes. Taking CBED as an example, for ER
graph with 10 nodes, its i-MMD performance is only 2.1% worse
than RL-CBED. However, when the graph size scales to 20 nodes, it
suffers performance drop for i-MMD metric, which is 7.7% worse
than RL-CBED, and even more serious for ER graph with 30 nodes,
i.e., 16.1% worse than our method. A possible explanation is that
when the previous methods try to find the optimal selection of inter-
vention at each step on a large size graph, the updated causal graph
distribution of the posterior model after each step is often more
different from that in the case of global optimal solution. Therefore,
the gap between the local optimal selection and the global optimal
selection after each step update will be wider for high-dimensional
graph data.
Finally, methods with fixed intervention value often do not per-
form as well as methods that strategically select intervention val-
ues. Fixed value selection strategies includes Random-Fixed, ABCD,
AIT, which can lead to suboptimal outcomes in certain situations.
By strategically choosing intervention values, other methods can
adapt to the specific context and make more effective decisions.
This flexibility allows for better performance and improved overall
outcomes.
Evaluation on Real-World Inspired Data. Besides experi-
ments on synthetic datasets, we also perform experiments on real
world-inspired DREAM dataset. Table 2 shows the comparison of
overall performance on Ecoli and Yeast sub-dataset of DREAM. For
both sub-datasets, RL-CBED outperforms other baselines in terms
Random-Fixed Random-Random CBED DiffCBED RL-CBED
Method05001000150020002500Time (sec)(b) 10 intervention steps
Type
Inference
Training
Random-Fixed Random-Random CBED DiffCBED RL-CBED
Method02004006008001000Time (sec)(a) 5 intervention steps
Type
Inference
TrainingFigure 3: Time cost of different methods on ER graph.
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Intervention Steps0.30.40.50.60.70.80.9i-MMD
(a) i-MMD of different intervention steps
strategy
Random-Fixed
Random-Random
CBED
DiffCBED
RL-CBED
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Intervention Steps0.20.30.40.50.60.70.8F1 score
(b) F1 score of different intervention steps
strategy
Random-Fixed
Random-Random
CBED
DiffCBED
RL-CBED
Figure 4: Influence of intervention steps.
of most metrics, especially i-MMD. Our method‚Äôs superior perfor-
mance across various metrics and datasets highlights its potential
for practical applications in real-world scenarios.
5.4 Analysis of Time Cost (RQ2)
To analyze the time cost of our proposed method, we conduct exper-
iments in different settings. Specifically, we record the intervention
inference time of different methods for 5 intervention steps on
10-node graphs and 10 intervention steps on 20-node graphs. We
present the results in Figure 3. From the results, we can find that
the time cost of optimization-based methods including CBED and
DiffCBED is significantly higher than other methods. In particular,
DiffCBED trains a neural network at each step, resulting in a very
high time cost. However, the inference time cost of our method
is nearly the same as random strategies and still lower than that
of optimization-based methods after adding training time. This
suggests that our method can reduce time costs significantly while
improving performance, making it suitable for real-world adaptive
applications with inference time limits.
5.5 Further Study of Various Factors (RQ3)
In order to explore the robustness of our proposed method, we
perform further experiments to discuss the influence of various
factors on RL-CBED and other methods. We consider different
intervention steps and MEC size, which are common in real-world
scientific research. We believe that robust active causal discovery
algorithm should work well in such different settings.
5.5.1 Influence of Intervention Steps. We choose ER graph with 20
nodes for the study of influence of intervention steps. Specifically,
we conduct experiments with {0,5,10,15,20}intervention steps on
ER graph with a large MEC size (more than 100) and report the
performance comparison in Figure 4. Based on the experimental
 
845KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Heyang Gao, Zexu Sun, Hao Yang, and Xu Chen
Table 1: Overall performance comparison among baselines and our method on ER graphs and SF graphs. For each metric, the
best performance is labeled by bold fonts. We report the mean and standard deviation of each metric over 30 random seeds.
Metho
dEr
d≈ës-R√©nyi (ER) graph
10
nodes 20
nodes 30
nodes
F1-scor
e (‚Üë) SHD (‚Üì) SID (‚Üì) i-MMD (‚Üì) F1-scor
e (‚Üë) SHD (‚Üì) SID (‚Üì) i-MMD (‚Üì) F1-scor
e (‚Üë) SHD ( ‚Üì) SID (‚Üì) i-MMD (‚Üì)
Random-Fixe
d 0.55¬±0.12 22.36¬±5.01 60.40¬±10.11 0.97¬±0.11 0.45¬±0.08 49.13¬±11.24 130.38¬±57.72 0.98¬±0.20 0.38¬±0.07 134.09¬±20.11 410.76¬±89.29 0.81¬±0.16
Random-Random 0.56¬±0.10 22.25¬±3.64 49.11¬±10.53 0.97¬±0.12 0.55¬±0.09 41.94¬±7.63 73.91¬±39.85 0.87¬±0.19 0.45¬±0.06 122.09¬±20.79 284.32¬±72.03 0.65¬±0.14
ABCD 0.51¬±0.15 23.55¬±5.81 62.07¬±12.27 0.98¬±0.14 0.50¬±0.08 45.06¬±9.56 121.91¬±58.57 0.90¬±0.18 0.39¬±0.05 135.80¬±16.62 412.57¬±67.73 0.82¬±0.14
AI
T 0.49¬±0.12 24.88¬±4.44 62.04¬±11.41 1.01¬±0.11 0.48¬±0.10 46.08¬±11.66 125.41¬±55.94 0.94¬±0.23 0.38¬±0.10 122.87¬±16.33 364.41¬±86.38 0.72¬±0.28
CBED 0.58¬±0.10 22.10¬±4.76 42.40¬±13.13 0.96¬±0.14 0.53¬±0.11 44.42¬±14.63 76.88¬±59.95 0.84¬±0.22 0.41¬±0.07 137.84¬±24.98 281.49¬±84.85 0.72¬±0.15
Diff
CBED 0.58¬±0.11 21.73¬±4.81 39.45¬±10.91 0.97¬±0.17 0.54¬±0.12 44.47¬±15.41 77.44¬±61.73 0.88¬±0.21 0.46¬±0.07 128.62¬±23.35 220.81¬±68.48 0.73¬±0.17
RL-CBED
(Ours) 0.61¬±0.12 20.25¬±5.34 44.08¬±16.24 0.94¬±0.15 0.61¬±0.09 36.85¬±8.84 50.94¬±38.24 0.78¬±0.20 0.48¬±0.06 118.40¬±19.19 237.55¬±64.85 0.62¬±0.13
Metho
dScale-Fr
ee (SF) graph
10
nodes 20
nodes 30
nodes
F1-scor
e (‚Üë) SHD (‚Üì) SID (‚Üì) i-MMD (‚Üì) F1-scor
e (‚Üë) SHD (‚Üì) SID (‚Üì) i-MMD (‚Üì) F1-scor
e (‚Üë) SHD ( ‚Üì) SID (‚Üì) i-MMD (‚Üì)
Random-Fixe
d 0.51¬±0.12 19.59¬±4.58 34.46¬±7.68 1.37¬±0.14 0.35¬±0.12 48.75¬±10.49 48.44¬±31.80 1.37¬±0.22 0.26¬±0.05 109.70¬±8.26 45.16¬±24.68 1.48¬±0.13
Random-Random 0.66¬±0.12 14.64¬±4.46 16.81¬±8.58 1.21¬±0.23 0.40¬±0.10 47.28¬±9.29 12.41¬±16.01 1.29¬±0.12 0.30¬±0.04 119.37¬±10.61 11.62¬±6.34 1.42¬±0.10
ABCD 0.53¬±0.18 18.47¬±6.40 35.37¬±11.28 1.31¬±0.20 0.37¬±0.10 44.54¬±5.60 28.61¬±23.51 1.41¬±0.15 0.23¬±0.04 130.69¬±9.95 52.45¬±24.63 1.50¬±0.12
AI
T 0.51¬±0.14 19.65¬±4.80 34.19¬±10.70 1.35¬±0.19 0.32¬±0.09 46.02¬±5.38 34.76¬±17.87 1.47¬±0.11 0.24¬±0.06 112.92¬±9.79 64.00¬±32.72 1.51¬±0.12
CBED 0.67¬±0.10 14.66¬±4.25 13.86¬±5.14 1.17¬±0.18 0.47¬±0.03 44.21¬±4.23 3.42¬±3.53 1.25¬±0.08 0.29¬±0.03 127.64¬±11.10 11.54¬±3.18 1.46¬±0.07
Diff
CBED 0.67¬±0.12 14.44¬±4.56 14.63¬±7.39 1.20¬±0.23 0.48¬±0.03 40.05¬±4.76 2.61¬±2.58 1.24¬±0.08 0.30¬±0.04 120.83¬±9.46 8.66¬±6.89 1.42¬±0.09
RL-CBED
(Ours) 0.72¬±0.10 12.30¬±3.70 13.84¬±6.08 1.14¬±0.19 0.48¬±0.02 40.15¬±2.72 1.82¬±2.18 1.22¬±0.07 0.34¬±0.01 108.90¬±4.78 6.57¬±3.23 1.36¬±0.11
Table 2: Overall performance comparison among baselines
and our method on Ecoli and Yeast sub-dataset of DREAM.
Metho
dEcoli
F1-scor
e (‚Üë) SHD (‚Üì) SID (‚Üì) i-MMD (‚Üì)
Random-Fixe
d 0.37¬±0.07 17.76¬±1.79 50.02¬±8.59 1.10¬±0.13
Random-Random 0.40¬±0.07 17.34¬±2.14 43.35¬±7.00 1.06¬±0.12
ABCD 0.47¬±0.10 15.88¬±2.51 44.83¬±8.78 1.01¬±0.16
AI
T 0.35¬±0.05 18.12¬±1.34 53.90¬±4.88 1.13¬±0.16
CBED 0.54¬±0.06 14.32¬±2.02 31.53¬±6.06 1.00¬±0.12
Diff
CBED 0.55¬±0.11 14.44¬±3.19 24.42¬±6.92 1.06¬±0.13
RL-CBED
(Ours) 0.62¬±0.10 12.38¬±2.50 19.91¬±9.50 0.96¬±0.09
Metho
dY
east
F1-scor
e (‚Üë) SHD (‚Üì) SID (‚Üì) i-MMD (‚Üì)
Random-Fixe
d 0.36¬±0.11 24.23¬±3.61 68.11¬±6.19 1.15¬±0.21
Random-Random 0.63¬±0.11 16.39¬±4.14 40.29¬±10.10 0.84¬±0.16
ABCD 0.48¬±0.18 19.67¬±5.55 61.51¬±12.40 1.03¬±0.17
AI
T 0.39¬±0.15 23.10¬±4.70 65.68¬±8.68 1.14¬±0.15
CBED 0.63¬±0.11 17.21¬±4.49 26.33¬±11.48 0.89¬±0.25
Diff
CBED 0.62¬±0.14 17.32¬±5.68 28.66¬±11.91 0.92¬±0.26
RL-CBED
(Ours) 0.71¬±0.08 13.48¬±3.55 28.08¬±9.59 0.79¬±0.15
results, our method RL-CBED performs as well as or better than ex-
isting methods across different intervention steps. Particularly with
a large number of intervention steps, our method yields superior
results, indicating its ability to identify the optimal intervention
path non-myopically and improve active causal discovery.
5.5.2 Influence of MEC Size. We choose SF graphs with 10 nodes
for the study of influence of MEC size. Specifically, we set the MEC
size of true causal graph to {10,20,30,40,60,80}and report the
performance comparison of RL-CBED and other methods in Fig-
ure 5. From the experimental results, as the MEC size of the real
10 20 30 40 50 60 70 80
MEC size0.60.81.01.21.4i-MMD
(a) i-MMD of different MEC size
strategy
Random-Fixed
Random-Random
CBED
DiffCBED
RL-CBED
10 20 30 40 50 60 70 80
MEC size0.30.40.50.60.70.8F1 score
(b) F1 score of different MEC size
strategy
Random-Fixed
Random-Random
CBED
DiffCBED
RL-CBEDFigure 5: Influence of MEC size.
graph increases, it becomes more challenging to recover the real
graph in general. Across various MEC sizes, our method RL-CBED
consistently performs the best, demonstrating its robustness for
graphs of different MEC sizes. Other methods perform similarly to
RL-CBED when the MEC size is small, but their performance de-
clines as the MEC size increases. This indicates that other methods
have limitations for graphs with large MEC sizes, while RL-CBED
continues to perform well.
5.6 Ablation Study (RQ4)
We further explore the influence of our proposed sparse and dense
reward design and compare reward convergence process and fi-
nal model performance under two different reward formulation.
Although we have proved the equivalence of sparse and dense re-
ward in Proposition 4.2, i.e., the same total reward for the same
trajectory under two reward formulation, sparse reward formula-
tions often suffer from slow learning and even an inability to learn,
which has been widely discussed in RL literature. We choose ER
graphs with 20 nodes in the ablation study and report the average
reward result over five runs in Figure 6. From the reward curve,
using dense rewards can eventually converge to a higher value than
 
846Policy-Based Bayesian Active Causal Discovery with Deep Reinforcement Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Random Action
ùëù=1‚Üíùëù=0
Figure 6: Average reward over episodes of dense and sparse
reward function.
sparse rewards. This implies that while sparse and dense rewards
are theoretically equivalent, dense rewards converge more easily
to the optimal solution in practice.
6 CONCLUSION
In this paper we introduced RL-CBED‚Äìthe first policy-based ac-
tive causal discovery method that can non-myopically select both
targets and values for intervention experiments. By training a RL
policy network upfront, RL-CBED also allows selecting interven-
tion rapidly during inference and is suitable for adaptive causal
discovery experiment scenarios. We first formulate active causal dis-
covery process as a POMDP and propose a dense information gain
estimation-based reward function. We also prove theoretically that
our formulation allows for training process without updating causal
discovery posterior model. We further introduce a novel network
architecture designed for the scenario with discrete-continuous hy-
brid action space in our setting. Extensive experiments conducted
on both synthetic and real-world inspired datasets demonstrate the
superiority of our method across various metrics. Future work will
include exploring the cost and improvement of updating causal
discovery posterior model at each step and applying our approach
to complex real-world active causal discovery scenarios.
ACKNOWLEDGMENTS
This work is supported in part by National Key R&D Program of
China (2023YFF0905402), National Natural Science Foundation of
China (No. 62102420), Beijing Outstanding Young Scientist Program
NO. BJJWZYJH012019100020098, Intelligent Social Governance Plat-
form, Major Innovation & Planning Interdisciplinary Platform for
the ‚ÄúDouble First-Class‚Äù Initiative, Renmin University of China,
Public Computing Cloud, Renmin University of China, and fund for
building world-class universities (disciplines) of Renmin University
of China.REFERENCES
[1]Silvia Acid and Luis M de Campos. 2003. Searching for Bayesian network struc-
tures in the space of restricted acyclic partially directed graphs. Journal of
Artificial Intelligence Research 18 (2003), 445‚Äì490.
[2]Raj Agrawal, Chandler Squires, Karren Yang, Karthikeyan Shanmugam, and
Caroline Uhler. 2019. Abcd-strategy: Budgeted experimental design for targeted
causal structure discovery. In The 22nd International Conference on Artificial
Intelligence and Statistics. PMLR, 3400‚Äì3409.
[3]Yashas Annadani, Panagiotis Tigas, Desi R Ivanova, Andrew Jesson, Yarin Gal,
Adam Foster, and Stefan Bauer. 2023. Differentiable Multi-Target Causal Bayesian
Experimental Design. arXiv preprint arXiv:2302.10607 (2023).
[4]Albert-L√°szl√≥ Barab√°si and R√©ka Albert. 1999. Emergence of scaling in random
networks. science 286, 5439 (1999), 509‚Äì512.
[5]Tom Blau, Edwin V Bonilla, Iadine Chades, and Amir Dezfouli. 2022. Optimizing
sequential experimental design with deep reinforcement learning. In International
Conference on Machine Learning. PMLR, 2107‚Äì2128.
[6]Kathryn Chaloner and Isabella Verdinelli. 1995. Bayesian experimental design: A
review. Statistical science (1995), 273‚Äì304.
[7]Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. 2006. Sequential monte carlo
samplers. Journal of the Royal Statistical Society Series B: Statistical Methodology
68, 3 (2006), 411‚Äì436.
[8]Christopher C Drovandi, James M McGree, and Anthony N Pettitt. 2014. A
sequential Monte Carlo algorithm to incorporate model uncertainty in Bayesian
sequential design. Journal of Computational and Graphical Statistics 23, 1 (2014),
3‚Äì24.
[9]P ERDdS and A R&wi. 1959. On random graphs I. Publ. math. debrecen 6, 290-297
(1959), 18.
[10] Adam Foster, Desi R Ivanova, Ilyas Malik, and Tom Rainforth. 2021. Deep adaptive
design: Amortizing sequential bayesian experimental design. In International
Conference on Machine Learning. PMLR, 3384‚Äì3395.
[11] Adam Foster, Martin Jankowiak, Elias Bingham, Paul Horsfall, Yee Whye Teh,
Thomas Rainforth, and Noah Goodman. 2019. Variational Bayesian optimal
experimental design. Advances in Neural Information Processing Systems 32
(2019).
[12] Adam Foster, Martin Jankowiak, Matthew O‚ÄôMeara, Yee Whye Teh, and Tom
Rainforth. 2020. A unified stochastic gradient approach to designing bayesian-
optimal experiments. In International Conference on Artificial Intelligence and
Statistics. PMLR, 2959‚Äì2969.
[13] Nir Friedman, Moises Goldszmidt, and Abraham Wyner. 2013. Data analysis with
Bayesian networks: A bootstrap approach. arXiv preprint arXiv:1301.6695 (2013).
[14] Juan L Gamella and Christina Heinze-Deml. 2020. Active invariant causal pre-
diction: Experiment selection through stability. Advances in Neural Information
Processing Systems 33 (2020), 15464‚Äì15475.
[15] Kristjan Greenewald, Dmitriy Katz, Karthikeyan Shanmugam, Sara Magliacane,
Murat Kocaoglu, Enric Boix Adsera, and Guy Bresler. 2019. Sample efficient
active learning of causal trees. Advances in Neural Information Processing Systems
32 (2019).
[16] Alex Greenfield, Aviv Madar, Harry Ostrer, and Richard Bonneau. 2010. DREAM4:
Combining genetic and dynamic information to identify biological networks and
dynamical models. PloS one 5, 10 (2010), e13397.
[17] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch√∂lkopf, and
Alexander Smola. 2012. A kernel two-sample test. The Journal of Machine
Learning Research 13, 1 (2012), 723‚Äì773.
[18] Alain Hauser and Peter B√ºhlmann. 2012. Characterization and greedy learning
of interventional Markov equivalence classes of directed acyclic graphs. The
Journal of Machine Learning Research 13, 1 (2012), 2409‚Äì2464.
[19] Yang-Bo He and Zhi Geng. 2008. Active learning of causal networks with inter-
vention experiments and optimal designs. Journal of Machine Learning Research
9, Nov (2008), 2523‚Äì2547.
[20] Neil Houlsby, Ferenc Husz√°r, Zoubin Ghahramani, and M√°t√© Lengyel. 2011.
Bayesian active learning for classification and preference learning. arXiv preprint
arXiv:1112.5745 (2011).
[21] Xun Huan and Youssef M Marzouk. 2016. Sequential Bayesian optimal experimen-
tal design via approximate dynamic programming. arXiv preprint arXiv:1604.08320
(2016).
[22] Desi R Ivanova, Adam Foster, Steven Kleinegesse, Michael U Gutmann, and
Thomas Rainforth. 2021. Implicit deep adaptive design: policy-based experimental
design without likelihoods. Advances in Neural Information Processing Systems
34 (2021), 25785‚Äì25798.
[23] Shali Jiang, Henry Chai, Javier Gonzalez, and Roman Garnett. 2020. BINOCU-
LARS for efficient, nonmyopic sequential experimental design. In International
Conference on Machine Learning. PMLR, 4794‚Äì4803.
[24] Murat Kocaoglu, Alex Dimakis, and Sriram Vishwanath. 2017. Cost-optimal
learning of causal graphs. In International Conference on Machine Learning. PMLR,
1875‚Äì1884.
[25] Jeremy Lewi, Robert Butera, and Liam Paninski. 2009. Sequential optimal design
of neurophysiology experiments. Neural computation 21, 3 (2009), 619‚Äì687.
 
847KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Heyang Gao, Zexu Sun, Hao Yang, and Xu Chen
[26] Vincent Lim, Ellen Novoseller, Jeffrey Ichnowski, Huang Huang, and Ken Gold-
berg. 2022. Policy-based bayesian experimental design for non-differentiable
implicit models. arXiv preprint arXiv:2203.04272 (2022).
[27] Dennis V Lindley. 1956. On a measure of the information provided by an experi-
ment. The Annals of Mathematical Statistics 27, 4 (1956), 986‚Äì1005.
[28] Quan Long, Marco Scavino, Ra√∫l Tempone, and Suojin Wang. 2013. Fast estima-
tion of expected information gains for Bayesian experimental designs based on
Laplace approximations. Computer Methods in Applied Mechanics and Engineering
259 (2013), 24‚Äì39.
[29] Richard J Murnane and John B Willett. 2010. Methods matter: Improving causal
inference in educational and social science research. Oxford University Press.
[30] Kevin P Murphy. 2001. Active learning of causal Bayes net structure. Technical
Report. technical report, UC Berkeley.
[31] Bengt Muth√©n and Hendricks C Brown. 2009. Estimating drug effects in the
presence of placebo response: causal inference using growth mixture modeling.
Statistics in medicine 28, 27 (2009), 3363‚Äì3385.
[32] Jay I Myung, Daniel R Cavagnaro, and Mark A Pitt. 2013. A tutorial on adaptive
design optimization. Journal of mathematical psychology 57, 3-4 (2013), 53‚Äì67.
[33] Robert Osazuwa Ness, Karen Sachs, Parag Mallick, and Olga Vitek. 2017. A
Bayesian active learning experimental design for inferring signaling networks. In
Research in Computational Molecular Biology: 21st Annual International Conference,
RECOMB 2017, Hong Kong, China, May 3-7, 2017, Proceedings 21. Springer, 134‚Äì
156.
[34] Rainer Opgen-Rhein and Korbinian Strimmer. 2007. From correlation to causation
networks: a simple approximate learning algorithm and its application to high-
dimensional plant gene expression data. BMC systems biology 1, 1 (2007), 1‚Äì10.
[35] Judea Pearl. 2010. Causal inference. Causality: objectives and assessment (2010),
39‚Äì58.
[36] Jonas Peters and Peter B√ºhlmann. 2015. Structural intervention distance for
evaluating causal graphs. Neural computation 27, 3 (2015), 771‚Äì799.
[37] Jonas Peters, Peter B√ºhlmann, and Nicolai Meinshausen. 2016. Causal inference
by using invariant prediction: identification and confidence intervals. Journal of
the Royal Statistical Society Series B: Statistical Methodology 78, 5 (2016), 947‚Äì1012.
[38] David CW Phang, Kanliang Wang, Qiuhong Wang, Robert J Kauffman, and Mau-
rizio Naldi. 2019. How to derive causal insights for digital commerce in China?
A research commentary on computational social science methods. Electronic
Commerce Research and Applications 35 (2019), 100837.
[39] Elizabeth G Ryan, Christopher C Drovandi, James M McGree, and Anthony N
Pettitt. 2016. A review of modern computational algorithms for Bayesian optimal
design. International Statistical Review 84, 1 (2016), 128‚Äì154.
[40] Karen Sachs, Omar Perez, Dana Pe‚Äôer, Douglas A Lauffenburger, and Garry P
Nolan. 2005. Causal protein-signaling networks derived from multiparameter
single-cell data. Science 308, 5721 (2005), 523‚Äì529.
[41] Nino Scherrer, Olexa Bilaniuk, Yashas Annadani, Anirudh Goyal, Patrick Schwab,
Bernhard Sch√∂lkopf, Michael C Mozer, Yoshua Bengio, Stefan Bauer, and
Nan Rosemary Ke. 2021. Learning neural causal models with active interventions.
arXiv preprint arXiv:2109.02429 (2021).
[42] Ben Shababo, Brooks Paige, Ari Pakman, and Liam Paninski. 2013. Bayesian
inference and online experimental design for mapping neural microcircuits.
Advances in Neural Information Processing Systems 26 (2013).
[43] Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G Dimakis, and Sriram
Vishwanath. 2015. Learning causal graphs with small interventions. Advances in
Neural Information Processing Systems 28 (2015).
[44] Wanggang Shen and Xun Huan. 2023. Bayesian sequential optimal experimen-
tal design for nonlinear models using policy gradient reinforcement learning.
Computer Methods in Applied Mechanics and Engineering 416 (2023), 116304.
[45] Chandler Squires, Sara Magliacane, Kristjan Greenewald, Dmitriy Katz, Murat
Kocaoglu, and Karthikeyan Shanmugam. 2020. Active structure learning of causal
DAGs via directed clique trees. Advances in Neural Information Processing Systems
33 (2020), 21500‚Äì21511.
[46] Panagiotis Tigas, Yashas Annadani, Andrew Jesson, Bernhard Sch√∂lkopf, Yarin
Gal, and Stefan Bauer. 2022. Interventions, where and how? experimental design
for causal models at scale. Advances in Neural Information Processing Systems 35
(2022), 24130‚Äì24143.
[47] Simon Tong and Daphne Koller. 2001. Active learning for structure in Bayesian
networks. In International joint conference on artificial intelligence, Vol. 17. Citeseer,
863‚Äì869.
[48] Christian Toth, Lars Lorch, Christian Knoll, Andreas Krause, Franz Pernkopf,
Robert Peharz, and Julius Von K√ºgelgen. 2022. Active bayesian causal inference.
Advances in Neural Information Processing Systems 35 (2022), 16261‚Äì16275.
[49] Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. 2006. The
max-min hill-climbing Bayesian network structure learning algorithm. Machine
learning 65 (2006), 31‚Äì78.
[50] Joep Vanlier, Christian A Tiemann, Peter AJ Hilbers, and Natal AW van Riel. 2012.
A Bayesian approach to targeted experiment design. Bioinformatics 28, 8 (2012),
1136‚Äì1142.[51] Thomas S Verma and Judea Pearl. 2022. Equivalence and synthesis of causal
models. In Probabilistic and causal inference: The works of Judea Pearl. 221‚Äì236.
[52] Benjamin T Vincent and Tom Rainforth. 2017. The DARC Toolbox: automated,
flexible, and efficient delayed and risky choice experiments using Bayesian adap-
tive design. PsyArXiv. October 20 (2017).
[53] Jiaqi Zhang, Chandler Squires, and Caroline Uhler. 2021. Matching a desired
causal state via shift interventions. Advances in Neural Information Processing
Systems 34 (2021), 19923‚Äì19934.
[54] Zeyu Zhang, Chaozhuo Li, Xu Chen, and Xing Xie. 2024. Bayesian Active Causal
Discovery with Multi-Fidelity Experiments. Advances in Neural Information
Processing Systems 36 (2024).
A OMITTED PROOFS
A.1 Proof of Proposition 4.1
Proof. We first plug in the definition of UsNMC , meaning that
we need to show that
Eùúã"ùëá‚àëÔ∏Å
ùë°=1ùõæùë°ùëüùë°#
=Eùëù(g0:ùêø)ùëù(‚Ñéùëá|g0,ùúã)"
logùëù(‚Ñéùëá|g0,ùúã)
1
ùêø+1√çùêø
‚Ñì=1ùëù(‚Ñéùëá|g‚Ñì,ùúã)#
,
where g0:ùêø‚àºùëù(g)independently. We then plug in the formulation
of the POMDP in the LHS and get
ùêøùêªùëÜ=Eùëù(g0:ùêø)√éùëá
ùë°=1[ùëù(ùë¶ùë°|‚Ñéùë°‚àí1,g0,ùëëùëú(ùëãùëñùë°=ùë£ùë°))ùúã(ùëéùë°|‚Ñéùë°‚àí1)]"ùëá‚àëÔ∏Å
ùë°=1ùõæùë°ùëüùë°#
=Eùëù(g0:ùêø)ùëù(‚Ñéùëá|g0,ùúã)"ùëá‚àëÔ∏Å
ùë°=1ùõæùë°ùëüùë°#
.
Note that the distribution of the expectation taken over in the LHS
is the same as that in the RHS. Therefore we need to show that the
terms inside the expectation are equal, that is,
ùëá‚àëÔ∏Å
ùë°=1ùõæùë°ùëüùë°=logùëù(‚Ñéùëá|g0,ùúã)
1
ùêø+1√çùêø
‚Ñì=1ùëù(‚Ñéùëá|g‚Ñì,ùúã).
Plugging in ùõæ=1and the definition of reward function in Equa-
tion 6, we get
ùëá‚àëÔ∏Å
ùë°=1ùõæùë°ùëüùë°=logùëù(‚Ñéùëá|g0,ùúã)‚àílogùêø‚àëÔ∏Å
‚Ñì=1ùëù(‚Ñéùëá|g‚Ñì,ùúã),
which is identical to the RHS and concludes the proof. ‚ñ°
A.2 Proof of Proposition 4.2
Proof. To prove the proposition, we first notice that the LHS
equalsùëüùë†
ùëásinceùëüùë†
ùë°=0forùë°<ùëáaccording to Equation 6, that is,
ùêøùêªùëÜ=logùëù(‚Ñéùëá|g0,ùúã)‚àílogùêø‚àëÔ∏Å
‚Ñì=1ùëù(‚Ñéùëá|g‚Ñì,ùúã).
We now foucs on the RHS and plug in the definition of ùëüùëë
ùë°
ùëÖùêªùëÜ=ùëá‚àëÔ∏Å
ùë°=1 
logùëù(ùë¶ùë°|g0,ùëéùë°)
‚àílog"ùë°√ñ
ùëò=1ùëù(ùë¶ùëò|g1,ùëéùëò)+¬∑¬∑¬∑+ùë°√ñ
ùëò=1ùëù(ùë¶ùëò|gùêø,ùëéùëò)#
+log"ùë°‚àí1√ñ
ùëò=1ùëù(ùë¶ùëò|g1,ùëéùëò)+¬∑¬∑¬∑+ùë°‚àí1√ñ
ùëò=1ùëù(ùë¶ùëò|gùêø,ùëéùëò)# !
.
 
848Policy-Based Bayesian Active Causal Discovery with Deep Reinforcement Learning KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain
Note that we can collapse the telescoping sum here. Hence we get
ùëÖùêªùëÜ=ùëá‚àëÔ∏Å
ùë°=1logùëù(ùë¶ùë°|g0,ùëéùë°)
‚àílog"ùëá√ñ
ùëò=1ùëù(ùë¶ùëò|g1,ùëéùëò)+¬∑¬∑¬∑+ùëá√ñ
ùëò=1ùëù(ùë¶ùëò|gùêø,ùëéùëò)#
Recall that we have made the assumption of BOED data generating
process that ùëù(ùúÉ)ùëù(‚Ñéùëá|ùúÉ,ùúã)=ùëù(ùúÉ)√éùëá
ùë°=1ùëù(ùë¶ùë°|ùúÉ,ùúâùë°)in Equation 3.
We can therefore rewrite the RHS
ùëÖùêªùëÜ=logùëá√ñ
ùë°=1ùëù(ùë¶ùë°|g0,ùëéùë°)‚àílogùêø‚àëÔ∏Å
‚Ñì=1ùëá√ñ
ùëò=1ùëù(ùë¶ùë°|g‚Ñì,ùëéùëò)
=logùëù(‚Ñéùëá|g0,ùúã)‚àílogùêø‚àëÔ∏Å
‚Ñì=1ùëù(‚Ñéùëá|g‚Ñì,ùúã),
which is equivalent to the LHS we get earlier and concludes the
proof. ‚ñ°
A.3 Proof of Theorem 4.3
Proof. When we train the estimator ÀÜùëÑùúã, we need to minimize
an loss taken expectation with respect to trajectories generated by
the policyùúã, that is,
Eùëù(‚Ñéùë°‚àí1,ùëéùë°|ùúã)[L(ÀÜùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°),ùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°))],
whereLis a loss function. In practice, we estimate the expectation
by Monte Carlo sampling. To analyze what we need in order to
evaluateùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°), we first need to derive the Bellman equation
for the Q function in our POMDP M, that is,
ùëÑùúã(ùë†ùë°,ùëéùë°)=EùëÜùë°+1,ùê¥ùë°+1[ùëüùë°+ùõæ¬∑ùëÑùúã(ùëÜùë°+1,ùê¥ùë°+1)|ùëÜùë°=ùë†ùë°,ùê¥ùë°=ùëéùë°].
In our formulation, state ùë†ùë°=(‚Ñéùë°‚àí1,g0:ùêø)is a function of history
‚Ñéùë°‚àí1and underlying causal parameters g0:ùêø, hence we substitute
ùë†ùë°with two separate covariates ‚Ñéùë°‚àí1andg0:ùêøin the following
derivation. Since g0:ùêøis sample from the posterior distribution
ùëù(g|‚Ñéùë°‚àí1)at each time step, we separate g0:ùêøfrom the covariates of
Q-function and introduce a state-action-parameter value function
ùêΩùúã, that is, we need to show that
ùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°)=Eùêªùë°,ùê¥ùë°+1[ùëüùë°+ùõæ¬∑ùëÑùúã(ùêªùë°,ùê¥ùë°+1)|ùêªùë°=‚Ñéùë°,ùê¥ùë°=ùëéùë°].
under the condition that we consider Q function as the expectation
of the state-action-parameter value function ùêΩùúã, that is,
ùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°)=Eg0:ùêø‚àºùëù(g|‚Ñéùë°‚àí1)ùêΩùúã(‚Ñéùë°‚àí1,ùëéùë°,g0:ùêø).
We now focus on the state-action-parameter value function ùêΩùúã:
ùêΩùúã(‚Ñéùë°‚àí1,ùëéùë°,g0:ùêø)=E√éùëá‚àí1
ùëñ=ùë°[ùëù(ùë¶ùëñ|ùëéùëñ,g0:ùêø)ùúã(ùëéùëñ+1|‚Ñéùëñ)]"ùëá‚àëÔ∏Å
ùëñ=ùë°ùõæùë°‚àíùëñùëüùëñ#
=Eùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)"
ùëüùë°+ùõæ¬∑
E√éùëá‚àí1
ùëñ=ùë°+1[ùëù(ùë¶ùëñ|ùëéùëñ,g0:ùêø)ùúã(ùëéùëñ+1|‚Ñéùëñ)]"ùëá‚àëÔ∏Å
ùëñ=ùë°+1ùõæùë°‚àíùëñùëüùëñ# #
=Eùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùëüùë°+ùõæ¬∑ùêΩùúã(‚Ñéùë°,ùëéùë°+1,g0:ùêø)].We then plug the recursive equation of ùêΩùúãinto the expression of
ùëÑùúãand get
ùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°)=Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùëüùë°+ùõæ¬∑
ùêΩùúã(‚Ñéùë°,ùëéùë°+1,g0:ùêø)]
=Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùëüùë°]+ùõæ¬∑
Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùêΩùúã(‚Ñéùë°,ùëéùë°+1,g0:ùêø)].
Then we need to show that ùêΩùúã(‚Ñéùë°,ùëéùë°+1,g0:ùêø)is equal toùëÑùúã(‚Ñéùë°,ùëéùë°+1)
in expectation since the Bellman equation for Q-function we need
to prove is a recursive equation of ùëÑùúã. We can get
Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùêΩùúã(‚Ñéùë°,ùëéùë°+1,g0:ùêø)]
=Eùëù(g0:ùêø|‚Ñéùë°)ùëù(ùë¶ùë°|‚Ñéùë°‚àí1,ùëéùë°)ùúã(ùëéùë°+1|‚Ñéùë°)[ùêΩùúã(‚Ñéùë°,ùëéùë°+1,g0:ùêø)]
=Eùëù(ùë¶ùë°|‚Ñéùë°‚àí1,ùëéùë°)ùúã(ùëéùë°+1|‚Ñéùë°)[ùëÑùúã(‚Ñéùë°,ùëéùë°+1)]
=Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùëÑùúã(‚Ñéùë°,ùëéùë°+1)].
So we proved in expectation ùêΩùúã(‚Ñéùë°,ùëéùë°+1,g0:ùêø)is equal toùëÑùúã(‚Ñéùë°,ùëéùë°+1).
Then we plug this result back to the above ùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°)equation
and get
ùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°)=Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùëüùë°]+ùõæ¬∑
Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùêΩùúã(‚Ñéùë°,ùëéùë°+1,g0:ùêø)]
=Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùëüùë°]+ùõæ¬∑
Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùëÑùúã(‚Ñéùë°,ùëéùë°+1)]
=Eùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)[ùëüùë°+ùëÑùúã(‚Ñéùë°,ùëéùë°+1)].
So we proved the recursive expression for Q-function, i.e., the Bell-
man function for Q-function. According to the above discussion, in
order to evaluate ùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°), we need to sample trajectories from
the distribution ùëù(‚Ñéùë°‚àí1,ùëéùë°|ùúã)ùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°),
which is difficult and time-consuming, since it includes the posterior
ùëù(g0:ùêø|‚Ñéùë°‚àí1). We can further simplify the intractable distribution
and get
ùëù(‚Ñéùë°‚àí1,ùëéùë°|ùúã)ùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùúã(ùëéùë°+1|‚Ñéùë°)
=ùëù(ùë¶ùë°|ùëéùë°,g0:ùêø)ùëù(g0:ùêø|‚Ñéùë°‚àí1)ùëù(ùëéùë°|‚Ñéùë°‚àí1,ùúã)ùëù(‚Ñéùë°‚àí1|ùúã)ùúã(ùëéùë°+1|‚Ñéùë°)
=ùëù(g0:ùêø|‚Ñéùë°)ùëù(ùë¶ùë°,ùëéùë°|‚Ñéùë°‚àí1,ùúã)ùëù(‚Ñéùë°‚àí1|ùúã)ùúã(ùëéùë°+1|‚Ñéùë°)
=ùëù(g0:ùêø|‚Ñéùë°)ùëù(‚Ñéùë°,‚Ñéùë°‚àí1|ùúã)ùúã(ùëéùë°+1|‚Ñéùë°)
=ùëù(g0:ùêø,‚Ñéùë°|ùúã)ùúã(ùëéùë°+1|‚Ñéùë°).
Therefore we can use trajectories sampled from the distribution
ùëù(g0:ùêø,‚Ñéùë°|ùúã)ùúã(ùëéùë°+1|‚Ñéùë°)to evaluateùëÑùúã(‚Ñéùë°‚àí1,ùëéùë°)and then learn the
Q-function estimator without updating the posterior ùëù(g0:ùêø|‚Ñéùë°‚àí1)
during the training process, which concludes the proof. ‚ñ°
B EXPERIMENTAL SETUP
B.1 Tasks
We will now provide additional details about tasks used in our
experiments.
Synthetic graphs. We generate two types of synthetic causal
graphs, Erd≈ës-R√©nyi (ER) graph [ 9] and Scale-Free (SF) graph [ 4].
For each kind of graphs, we generate different sizes of graphs,
including 10 nodes, 20 nodes and 30 nodes. ER graph is a random
graph withùëõvertices generated by connecting each pair of vertices
with a probability ùëù. In our experiments, we set ùëù=0.2. SF graph
is a random graph with a degree distribution following power law.
In SF graph, a minority of vertices possess a large number of edges,
while the vast majority of vertices have relatively fewer edges. We
 
849KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Heyang Gao, Zexu Sun, Hao Yang, and Xu Chen
Table 3: Hyper-parameter Details
Parameter Value
Learning Rate forLùë°ùëéùëüùëîùëíùë° 1√ó10‚àí4
Learning Rate forLùë£ùëéùëôùë¢ùëí 1√ó10‚àí5
Batch Size 16
Replay Buffer Size 1√ó105
ùúèfor Target Network 0.1
ùúèfor Value Network 0.001
Target Network (128, 128)
Value Network (128, 128)
ùõæ 0.99
Training Episodes 200
LSTM Hidden Size 128
LSTM Layers 1
Entropy Regularization ùõº 5.0
use igraph1package to generate SF graphs, and set the number of
outgoing edges generated for each vertex to 1.
DREAM. DREAM [ 16] is the abbreviation for Dialogue for Re-
verse Engineering Assessments and Methods, which is a family of
benchmarks designed to assess causal discovery algorithms, specif-
ically focusing on the protein signaling regulatory networks within
a single cell. The dataset is created by utilizing a series of ODEs and
SDEs to model the reverse-engineered networks of individual cells.
The dataset is generated with GeneNetWeaver v3.12, a simulator of
gene regulatory networks based on SDEs. In our experiments, we
use two sub-datasets Ecoli andYeast networks each with 10 nodes
as the true causal graph and simulate the mechanisms, which is the
same setting as described in [3].
B.2 Evaluation Metrics
We will now provide additional details about four evaluation metrics
used in our experiments.F1-score: We consider a binary classification task of whether
each edge of the ground-truth causal graph is present in the inferred
causal graph or not, and take an expectation of F1 score over the
graphs sampled from the posterior model.
SHD [1,49]: Structural Hamming Distance (SHD) considers
two DAGs and counts how many edges do not coincide. In our
experiments, we consider the expected SHD between the graphs
sampled from the posterior model and the ground truth causal
graph, i.e.,E‚àíSHD BEg‚àºùëù(G|D)[SHD(g,Àúg)].
SID [36]: Structural Intervention Distance (SID) is based on a
graphical criterion only and quantifies the closeness between two
DAGs in terms of their corresponding causal inference statements.
It counts the number of (ùëñ,ùëó)such that the interventional distri-
butionùëù(ùë•ùëó|do(ùëãùëñ=ÀÜùë•ùëñ))is falsely estimated using the inferred
graphs with respect to the true causal graph. In our experiments,
we consider the expected SID, E‚àíSID, as we introduced in SHD.
i-MMD [17]: Interventional Maximum Mean Discrepancy (i-
MMD) distance evaluates the interventional distribution induced
by both the causal graph structure and the corresponding condi-
tional distributions. It is defined as MMD distance between the
ground-truth interventional distribution and the interventional dis-
tribution induced by posterior sample gandùúÉ. In our experiments,
we calculate the expected i-MMD over posterior samples, interven-
tional targets and values and use the median heuristic for kernel
choice as described in [17].
C IMPLEMENTATION DETAILS
We tune the hyper-parameters of our model carefully and list the
adopted value for ER graphs with 20 nodes in Table 3. To make
training process more stable, we also use soft update tricks, i.e.,
learning two networks for each network and updating parameters
with a Polyak averaging factor ùúèfor copying target weights.
1https://igraph.org/python/api/latest/igraph._igraph.GraphBase.html#Barabasi
2https://github.com/tschaffter/genenetweaver
 
850