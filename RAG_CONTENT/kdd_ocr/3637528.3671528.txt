Kno
w in AdVance : Linear-Complexity Forecasting of Ad
Campaign Performance with Evolving User Interest
Xiaoyu Wang
wangxiaoyu1001@gmail.com
University of Science and
Technology of China, Hefei, ChinaYonghui Guo
Hui Sheng
brookguo@tencent.com
hughsheng@tencent.com
Tencent Advertising, Shanghai,
ChinaPeili Lv
Chi Zhou
paleylv@tencent.com
fredchizhou@tencent.com
Tencent Advertising, Shanghai,
China
Wei Huang
Shiqin Ta
Dongbo Huang
Xiujin Yang
Lan Xu
johnwhuang@tencent.com
secondta@tencent.com
andrewhuang@tencent.com
xiujinyang@tencent.com
lanxu@tencent.com
Tencent Advertising, Shanghai,
ChinaHao Zhou
kitewind@ustc.edu.cn
LINKE Lab, School of Computer
Science and Technology, University
of Science and Technology of China,
Hefei, China
CAS Key Laboratory of
Wireless-Optical Communications,
University of Science and
Technology of China, Hefei, China
Deqing Alpha Innovation Institute,
Huzhou, Zhejiang, ChinaYusheng Ji
kei@nii.ac.jp
Information Systems Architecture
Science Research Division, National
Institute of Informatics, Tokyo, Japan
Abstract
Real-time Bidding (RTB) advertisers wish to know in advance the
expected cost and yield of ad campaigns to avoid trial-and-error
expenses. However, Campaign Performance Forecasting (CPF), a
sequence modeling task involving tens of thousands of ad auctions,
poses challenges of evolving user interest, auction representation,
and long context, making coarse-grained and static-modeling meth-
ods sub-optimal. We propose AdVance, a time-aware framework
that integrates local auction-level and global campaign-level mod-
eling. User preference and fatigue are disentangled using a time-
positioned sequence of clicked items and a concise vector of all
displayed items. Cross-attention, conditioned on the fatigue vec-
tor, captures the dynamics of user interest toward each candidate
ad. Bidders compete with each other, presenting a complete graph
similar to the self-attention mechanism. Hence, we employ a Trans-
former Encoder to compress each auction into embedding by solv-
ing auxiliary tasks. These sequential embeddings are then sum-
marized by a conditional state space model (SSM) to comprehend
long-range dependencies while maintaining global linear complex-
ity. Considering the irregular time intervals between auctions, we
Permission
to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
the author(s) must be honored. Abstracting with credit is permitted. To copy other-
wise, or republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org.
KDD ’24, August 25–29, 2024, Barcelona, Spain
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671528make SSM’s parameters dependent on the current auction embed-
ding and the time interval. We further condition SSM’s global pre-
dictions on the accumulation of local results. Extensive evaluations
and ablation studies demonstrate its superiority over state-of-the-
art methods. AdVance has been deployed on the Tencent Advertis-
ing platform, and A/B tests show a remarkable 4.5% uplift in Aver-
age Revenue per User (ARPU).
CCS Concepts
•Information systems !Computational advertising ;Dis-
play advertising .
Keywords
Real-time Bidding; Campaign Performance Forecasting; User Inter-
est Modeling; Sequence Modeling
ACM Reference Format:
Xiaoyu Wang, Yonghui Guo, Hui Sheng, Peili Lv, Chi Zhou, Wei Huang,
Shiqin Ta, Dongbo Huang, Xiujin Yang, Lan Xu, Hao Zhou, and Yusheng Ji.
2024. Know in AdVance : Linear-Complexity Forecasting of Ad Campaign
Performance with Evolving User Interest. In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’24),
August 25–29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671528
1 Introduction
Online display advertising, especially the dominant Real-time Bid-
ding (RTB) paradigm, has evolved into a $300 billion market [10]
and becomes the primary revenue source for tech giants such as
5926
KDD’24, August 25–29, 2024, Barcelona, Spain Xiaoyu Wang et al.
Google, Meta, Alibaba, Tencent, etc. Its success lies in a win-win sit-
uation: platforms monetize user visits into the opportunities of dis-
playing ads (a.k.a. user impression), while advertisers purchase
such impressions to reach potential customers and promote mar-
keting. RTB allows advertisers to pre-define certain criteria for
launching ad campaigns. Criteria specify bid prices, target audi-
ence (e.g. , females aged 20-35 living in Shanghai), and optimization
objectives (e.g., pursuing more exposure, clicks, or conversions).
Then, a long series of auctions competing for the user impressions
that satisfy such criteria constitutes the ad campaign.
RTB features non-guaranteed delivery (NGD) modes, i.e., both
the cost and yield of a campaign remain uncertain before its ful-
fillment. Consequently, it is critical for advertisers to know in ad-
vance the expected performance, rendering the Campaign Perfor-
mance Forecasting (CPF) problem. This foresight brings two-fold
benefits: 1) Advertisers use a few tentative predictions to balance
a wider audience and higher conversion rates, thus improving Re-
turn on Investment (ROI). 2) Platforms can stimulate advertisers to
invest additional budget for more yield, thus promoting revenue.
CPF problem induces a sequence-to-sequence task, with the input
of an auction series satisfying the campaign criteria, and the output
of the corresponding cost and yield so far. Significant academic and
industry attention has been attracted: Kalish et al. from Bidtellect
[21] constructed a multi-variate time series of similar campaigns
to predict new campaigns. Wu et al. from Tencent [49] estimated a
scaling factor of the total future impression volume. These coarse-
grained methods fail to harness the information of each auction.
In contrast, Wang et al. from Yahoo [45] aggregated qualified auc-
tions from bid logs, and Jiang et al. from Meta [20] further consid-
ered reaching distinct users, albeit lacking a global viewpoint from
campaign-level modeling. Nath et al. from Microsoft [29] combined
dynamic linear models with Bayes net for winning price estima-
tion. Cui et al. [8] used probabilistic methods of a mixed log-normal
distribution, while Ren et al. [33] replaced it with recurrent neural
networks (RNN) to approximate a discrete winning distribution.
However, neglecting evolving user interest results in a substantial
gap between predictions and online results.
To fill this gap, we propose AdVance , a time-aware framework
that integrates local and global modeling. Designing such a frame-
work faces the following challenges:
(1)Evolving user interest: During the exposure to a series
of ads, a user clicks the preferred ads and accumulates fa-
tigue toward the similar ones, causing future click and con-
version rates to decline. This accounts for the diminishing
marginal utility issues where the yield is not proportional
to the cost increment. Neglecting this phenomenon renders
over-estimated campaign performance.
(2)Auction representation: Each auction involves user fea-
tures, contextual information, and a dynamic number of can-
didate ads competing with each other. The platform’s filter-
ing rules further complicate the auction process. Effectively
compressing and extracting useful information from such
amulti-source ,variable-length input remains a significant
challenge.(3)Long context: Accurate predictions require summarizing
a sequence of tens of thousands of auctions with irregu-
lar time intervals. Traditional linear architectures like RNNs
and CNNs struggle to model long-range dependency, while
the self-attention mechanism suffers quadratic complexity,
making it impractical for CPF tasks.
AdVance converts each auction and corresponding user interest
into a single embedding. It summarizes the embedding sequence
with a conditional State Space Model (SSM) to achieve linear com-
plexity. Specifically, we use a time-positioned click sequence and a
fatigue vector compressing all displayed ads to reflect interest dy-
namics. A Transformer encoder conducts self- and cross-attention
on candidate ads and user-related features and predicts the auction-
level cost and yield. This fully utilizes the supervision signals from
historical records and forges an informative representation. SSMs
feature linear structures like RNNs and CNNs while achieving com-
parative long-range modeling ability as self-attention. We propose
its conditional variant. We condition its parameters on the current
auction and time interval to handle the irregular input sequence,
and we condition the campaign-level prediction on the accumu-
lated auction-level outputs.
In summary, our contributions are as follows:
We focus on the challenging task of forecasting ad campaign
performance with evolving user interest, which benefits both
advertisers and platforms by providing valuable insights and
stimulating ad budgets.
We propose AdVance, a time-aware framework that com-
bines auction- and campaign-level modeling. AdVance lever-
ages the attention mechanism to vectorize each auction lo-
cally and summarizes the whole sequence with a conditional
SSM, achieving global linear complexity.
We conduct evaluations and ablation studies using large-
scale industrial datasets, demonstrating the superiority of
AdVance over state-of-the-art methods. AdVance has been
deployed on the Tencent advertising platform, and we up-
loaded the PyTorch implementation.1
2 Related Work
2.1 Campaign Performance Forecasting
Accurate modeling of campaigns grants advertisers insights into
their investment and return, thus attracting significant research
interests. Based on the granularity, existing works can be cate-
gorized into campaign-level and auction-level methods. Kalish et
al.[21] estimated campaign performance by aggregating statistics
from similar historical campaigns. Wu et al. [49] focused on cal-
culating scaling coefficients to adjust predicted volumes of future
impressions to earned ones. Despite having low complexity, they
discard fine-grained information within each auction, leading to a
non-negligible accuracy gap.
Auction-level methods, in contrast, lift the complexity for higher
forecasting accuracy, as the benefits for publishers and advertisers
are significant. Wang et al. [45] estimated a quality score for each
(ad, user)-pair using regression modeling and used it as a thresh-
old to select qualified impressions. Cui et al. [8] enhanced this work
1https://github.com/alpha-
wxy/KDD24-CampaignPerformanceForecasting
5927Know in AdVance : Linear-Complexity Forecasting of Ad Campaign Performance with Evolving User Interest KDD ’24, August 25–29, 2024, Barcelona, Spain
by incorporating probabilistic methods and assuming a mixture of
log-normal distribution. Jiang et al. [20] calculated corresponding
threshold bid prices for winning historical auctions and counted
the number of exposed users. Chen et al. [4] further augmented it
with multi-task learning and campaign information.
Following the spirit of estimating threshold prices to win, an-
other line of research known as bid landscape ormarket price mod-
eling has gained traction and can be applied to campaign model-
ing tasks. As a representative, Ren et al. [33] removed assumptions
on the distribution forms and utilized a recurrent neural network
to flexibly model the conditional winning probability for each bid
price. Yang et al. [51] further incorporated multi-task learning to
jointly model click-through rate and market price, thereby provid-
ing multiple results in a single return to enhance the robustness
and online inference efficiency.
The main drawback of these methods is the neglect of user in-
terest evolvement in the future campaign environment and directly
using historical click/conversion probability. When a particular ad
wins more auctions, the user preference evolves, and fatigue ac-
cumulates toward repetitive similar ads. Neglecting such evolve-
ment and assuming static user interest causes an overestimate of
campaign performance and budget waste.
2.2 User Interest Modeling
User interest modeling mainly focuses on the probability of cer-
tain explicit behaviors, such as clicking or conversion, by model-
ing the feature interaction between users and ads. Early machine-
learning and deep-learning methods, including logistic regression
[35], gradient boosting decision trees (GBDT) [17], collaborative
filtering [37], Wide&Deep [6], DeepFM [15], DCN [44], and PNN
[32], adopt a static viewpoint and overlook the dynamics of user
preference. To address the limitation, DIN [57] first incorporated
the sequence of the user’s historic clicked items and utilized an at-
tention mechanism to build an enriched user feature. Subsequently,
a series of works such as DIEN [56], DSIN [11], SIM [30], UBR4CTR
[31], SMR[30] emerged to model user interest evolution using re-
current neural network (RNN) [18] or Transformers [41]. However,
the aforementioned methods discard the abundant displayed but
non-clicked ads, which account for user fatigue towards repeated
similar ads. In contrast, AdVance considers all displayed ads to
comprehensively understand interest evolution.
3 Preliminary
3.1 Attention Mechanism
Attention mechanism [41] excels at modeling long-range depen-
dencies. It allows different parts (a.k.a. tokens) of the input se-
quence to interact, regardless of their distance and position. This
is achieved by representing the input Queries as the weighted sum
ofValues . The weights depend on the similarity between queries
andKeys, measured by the dot-product:
Attn ¹Q,K,Vº=softmax ¹QK⊺
√
𝑑𝐾ºV, (1)
wher
e𝑑𝐾is the dimension of each key vector.For the self-attention, Q=XW𝑄,K=XW𝐾, and V=XW𝑉
are the projections of the same sequence X, thereby focusing on
information exchange and aggregation within single sequence.
In contrast, the cross-attention involves two different sequences
XandY, where Q=XW𝑄,K=YW𝐾, and V=YW𝑉. This allows
Xto “borrow” information from Y, thus useful in multi-modality
learning such as vision-language models [1, 36].
The attention mechanism’s main drawback is its quadratic com-
plexity, as each new token has to attend to allprevious tokens.
This incurs heavy burdens for handling numerous ad auctions.
3.2 State Space Model
As a promising competitor to Transformers, the State Space Model
(SSM) [14] shares the same virtue of linear recurrence of RNN
while achieving comparative long-range modeling capacity in se-
quence analysis [39], time series prediction [53], and large lan-
guage models [12, 13]. It defines a continuous differential system
and recurrently updates a hidden state ℎ¹𝑡º:
dℎ
d𝑡=Aℎ¹𝑡º
¸B𝑥¹𝑡º, 𝑦 ¹𝑡º=Cℎ¹𝑡º, (2)
where 𝑥¹𝑡º 2 R is the 1-D input signal, 𝑦¹𝑡º 2 R is the output
signal, A2 R𝑁𝑁is the state transition matrix, B2 R𝑁1is the
input matrix, and C2 R1𝑁is the output matrix.
To adapt Eq. (2) to sequence modeling tasks, we employ zero-
order hold (ZOH), a technique for discretizing continuous equa-
tions, and we have the linear recurrence form:
ℎ𝑡=Aℎ𝑡 1¸B𝑥𝑡,
𝑦𝑡=Cℎ𝑡, (3)
where A=e
xp¹ΔAº,B=¹ΔAº 1¹e
xp¹ΔAº IºΔB, and Δdenotes
the step size. Note that ℎ𝑡,𝑥𝑡, and 𝑦𝑡are now discrete time series.
When the input is a sequence of 𝐷-dimension vectors, we stack
𝐷SSMs to model each vector dimension separately, resulting in
a total ¹𝑁 𝐷º-dimension hidden space. Like the Transformers, a
multi-layer perceptron (MLP) is often added to process the con-
catenation of all 𝐷SSMs’ output (a.k.a. channel mixing [52]).
3.3 Problem Formulation
Advertisers pre-define the criteria of ad campaigns to expose target
users to their ads within a specific period. An auction is launched
whenever a qualified user impression comes, and 200selected
candidates compete for it. Eventually, an irregular time series of
auctions constitutes the campaign.
We define campaign performance as the expected cost and
yield of an ad campaign. Advertisers may pursue more ad expo-
sure, clicks, or conversions, giving rise to CPM (Cost-per-Mille),
CPC (Cost-per-Click), and CPA (Cost-per-Action) ad types. The ex-
pected yield of an auction is defined as:
yield = 
win-rate 1 CPM
win-rate pCTR CPC
win-rate pCVR CPA(4)
Here, win-rate is the target ad’s probability of winning the auc-
tion, pCTR (predicted click-through rate) is the probability of the
user clicking the ad, and pCVR (predicted conversion rate) denotes
the probability of the user’s conversion like adding to cart or pur-
chase. Then we define the expected cost of such an auction as
5928KDD’24, August 25–29, 2024, Barcelona, Spain Xiaoyu Wang et al.
¹bid price expected yield º. This is also known as the effective
cost-per-mille (eCPM).
Given advertiser-defined criteria and a long sequence of quali-
fied auctions, our target problem is to predict the corresponding
cost and yield of the campaign with evolving user interest and
maintain acceptable algorithm complexity for practical needs.
4 Method
AdVance operates on a sequence-to-sequence paradigm by map-
ping a series of auctions to a series of estimated campaign perfor-
mances at the moment, as illustrated in Fig. 1. AdVance consists
of three modules, i.e., user interest, local auction, and global cam-
paign modeling. Click records with time-stamp embedding reflect
user preference. The local SSM recurrently updates the fatigue vec-
tor based on the whole display history. An encoder conducts self-
and cross-attention on candidate ads and user features to predict
auction performance, thereby fully utilizing the log data and build-
ing an informative representation. The generated sequence of auc-
tion embedding has long lengths and irregular time intervals. The
linear-complexity, global SSM with parameters dependent on in-
puts and intervals summarizes the sequence. The final prediction
relies on the global SSM’s output and the accumulated auction per-
formance, thereby tightly integrating the fine-grained auction and
holistic campaign knowledge.
4.1 User Interest Modeling
The sequence of a user’s previous clicks and conversions offers
a basis for estimating the preference towards similar ads. Many
works [11, 56, 57] incorporate it as part of user features, albeit with
two drawbacks.
Irregular interval: RNN- and Transformer-based methods
treat user behaviors as an evenly distributed time series,
while the interval lengths affect the timeliness of the rele-
vance between historical and current behaviors (e.g. , a pur-
chase made a week ago is often more informative than one
made two months ago).
User fatigue: Displayed yet non-clicked records account
for fatigue accumulation and yield declines. Ignoring them
causes overestimated preferences and displaying similar ads
repeatedly.
Recent behaviors deserve more emphasis for modeling user in-
terest, as discovered by [16, 23]. At the Tencent advertising plat-
form, each display record has a time-stamp. Inspired by the abso-
lute and relative positional embedding [38, 41], we propose relative
time-stamp positional embedding as:
Pos¹𝑡º=»cos¹𝜔1𝑡º,sin¹𝜔1𝑡º, . . . , cos¹𝜔𝑑𝑡º,sin¹𝜔𝑑𝑡º¼,(5)
where 𝑡is the difference between the current time-stamp and a
manually set origin ( e.g., 2023.1.1 0:00 AM), 2𝑑equals the dimen-
sion of input embedding, and 𝜔1, . . . , 𝜔𝑑are𝑑trainable parame-
ters. We calculate Pos ¹𝑡ºfor each click record and add it to the
click record’s embedding. We use trigonometric functions due to
their good properties for dot-product:
Pos¹𝑡º Pos¹𝑡¸𝛿º=cos¹𝜔1𝛿º ¸cos¹𝜔2𝛿º ¸    ¸ cos¹𝜔𝑑𝛿º,(6)where 𝛿represents a time interval. Therefore, the time distance
information is preserved for AdVance to pay attention to more rel-
evant user behaviors.
A seemingly feasible solution to handle non-clicked records is to
include them as user features, just like what we do to the clicked
ones. However, the number of non-clicked is usually 20 or more
times larger than that of clicked records, making it impractical due
to the quadratic complexity of the self-attention mechanism.
We employ a local state space model (SSM) to compress the
whole sequence of displayed ads into a fatigue vector in linear
complexity, which serves as conditional information in the auc-
tion representation (Sec. 4.2). It processes the display records one
by one and recursively updates the fatigue vector. We make the
SSM’s parameters data-dependent and interval-dependant, grant-
ing the model the ability to selectively memorize salient knowledge
from the irregular input series. Sec. 4.3 provides more details about
the conditional SSM.
4.2 Local Auction Modeling
This module takes an input of click sequence, fatigue vector, a
varying number of candidate ads, user profile, and other contex-
tual information to learn an informative representation for each
auction. This demands the model architecture capable of 1) han-
dling variable-length input, 2) modeling competitive relations be-
tween any two of the candidates, and 3) aggregating knowledge
from multiple input sources into one vector.
We employ an attention-based encoder that satisfies the afore-
mentioned demands. The encoder conducts self-attention on the
candidate ads, where the competitive relationship forms a com-
plete graph. The encoder applies cross-attention between candi-
date ads and the rest of the input to extract knowledge from user
profiles, interests, and context. This knowledge indicates the user’s
value to advertisers. Formally, we have:
X=X¸Pos¹𝑡º,
X=LN¹X¸Attn ¹XW𝑄,»X;Y¼W𝐾,»X;Y¼W𝑉ºº,
X=LN¹X¸MLP ¹𝑐𝑜𝑛𝑐𝑎𝑡 ¹X,®𝑓ººº, (7)
where Xdenotes the candidate ad embeddings, and Ydenotes the
embeddings of user click sequence, user profile, and contextual in-
formation. We calculate Pos ¹𝑡ºusing the time-stamp of the current
auction. We use »X;Y¼to calculate the keys and values, thus inte-
grating the self- and cross-attention in one pass. LN denotes the
layer norm function [2], and MLP represents a multi-layer percep-
tron, often stacked fully-connected layers with ReLU activation as
in [41]. We use the 𝑐𝑜𝑛𝑐𝑎𝑡 ¹ºoperator to concatenate the fatigue
vector ®𝑓toeach ad embedding, as this factor greatly affects user
clicks and conversions.
Empirically, supervised learning is a more straightforward and
sample-efficient paradigm for representation learning [28]. The com-
mon practice is to first train a model on a labeled dataset, then re-
move the classifier (usually the last few layers of the model). Then,
the rest of the model serves as a discriminative representation ex-
tractor. This inspires us to devise a multi-task architecture of pre-
dicting each auction’s win-rate and expected yield, with the shared
representation of ad embedding X.
5929Know in AdVance : Linear-Complexity Forecasting of Ad Campaign Performance with Evolving User Interest KDD ’24, August 25–29, 2024, Barcelona, Spain
⊗𝜽𝑪𝑻𝑹𝜽𝑪𝑽𝑹
𝒕𝟏𝒕𝟐𝒕𝟑𝒕𝟒Causal MaskTime-stampEmbeddingLocal SSM𝒕𝟏𝒕𝟐𝒕𝟑𝒕𝟒Fatigue Vector
CompetitorsUserContextSelf-& Cross-AttentionconcatMLP𝜽𝒘𝒊𝒏{𝒑𝟏,𝒑𝟐,𝒑𝟑,𝒑𝟒}
𝒉𝒕=𝑨)𝒉𝒕&𝟏+𝑩)𝒙𝒕𝒚𝒕=𝑪𝒉𝒕Clicked AdNon-clickedUser Interest Modeling
ClicksLocal Auction Modeling
AccumulateAuctionPerformance
concat𝜽𝑪𝑷
𝒙𝟏𝒙𝟐𝒙𝟑𝒙𝟒𝜹𝟏𝜹𝟐𝜹𝟑𝒚𝟓Global SSMExposureClickConversionCost
Global Campaign Modeling
Auction Representation𝒙𝟓𝜹𝟒
Figur
e 1: AdVance disentangles user interests as time-stamped click sequences representing user preference and fatigue vectors
compressing all displayed items (Sec. 4.1). The attention mechanism compresses auctions into dense embeddings, and AdVance
accumulates auction-level performance (Sec. 4.2). A global SSM recurrently summarizes all embeddings, and AdVance returns
final results based on the summary and accumulated performance (Sec. 4.3). During training, a causal mask blocks out “future”
records after the current time-stamp (Sec. 4.4.1).
Win-rate prediction: Besides bid prices and user-ad matching,
the Tencent platform manually sets filtering rules that can not be
described as analytic functions. Inspired by PointerNet [43] and
AlphaStar [42], we treat the auction process as a black box and
approximate it with a discrete distribution over all ads. We train a
win-rate model 𝑓¹;𝜃𝑤𝑖𝑛ºto compress each ad embedding 𝑋𝑖into
a scalar 𝑤𝑖that describes its relative advantage over other ads. A
Softmax layer then turns all scalars into a discrete distribution of
the winning probability 𝑝𝑖=exp¹𝑤𝑖º∑
𝑗=1,2,...exp¹𝑤𝑗ºfor each
ad. The ground truth is recorded as a one-hot vector »0. . .1. . .0¼,
where 1indicates the winner. Hence, we use the categorical cross-
entropy as our loss function.
Yield prediction: We focus on estimating pCTR and pCVR (Sec.
3.3). We model the task as a binary classification problem and use a
Sigmoid function 𝑓¹𝑥º=1¹1¸𝑒 𝑥ºto output a probability. How-
ever, predicting pCVR faces a great challenge due to the sparser
positive samples than the pCTR problem. Inspired by ESMM [26],
we introduce two sub-models 𝑓¹;𝜃𝐶𝑇𝑅ºand 𝑓¹;𝜃𝐶
𝑉𝑅º: the for-
mer predicts pCTR, and the latter predicts the conversion proba-
bility conditioned on that the ad has been clicked. Apparently,
the ad conversion must come after the ad click. Thus, 𝑓¹;𝜃𝐶𝑇𝑅º 
𝑓¹;𝜃𝐶
𝑉𝑅ºequals pCVR, according to the chain rule. This design
lowers the difficulty of learning pCVR by treating pCTR as an in-
termediate task and solving a conditional probability problem in a
smaller space. Furthermore, it allows AdVance to output multiple
yield metrics of exposure, click, and conversion volumes with Eq.
(4), regardless of the campaign objectives.
Notably, win-rate and yield prediction are correlated tasks, as
ads with high pCTR/pCVR also have a higher rate of winning the
auction. This connection benefits their training mutually and helps
to learn a more effective representation, as discovered by [51] and
our experiments. We select the target ad’s embedding as theauction representation, as it has aggregated information from
all other tokens after the cross- and self-attention.
4.3 Global Campaign Modeling
This module takes input from a time series of auction embedding
and summarizes it into a summary vector . Then, AdVance fore-
casts the campaign performance based on such a vector.
Self-attention models preserve all previous tokens as Key and
Value matrices, and each new token has to traverse the sequence
before it, resulting in a quadratic complexity. In contrast, State
Space Models (SSMs) maintain a hidden state to compress histori-
cal input. This allows SSMs to process new tokens recurrently and
update the hidden state correspondingly, thus achieving a linear
complexity.
However, the vanilla SSM’s parameters ¹Δ,A,B,Cºremain the
same for all tokens [14]. A constant stepsize Δis unsuitable for
irregular auction intervals, and a static input matrix Band output
matrix Ccan not selectively preserve or discard information based
on the current token, causing a redundant hidden state.
Inspired by the gating mechanism [7, 18, 19], recent research
suggests a data-dependent design that makes SSM’s parameters as
functions of input tokens [13, 39]. Therefore, we define the condi-
tional SSM as
B=X𝑊B¸𝑏B,
C=X𝑊C¸𝑏C,
Δ=𝜏Δ(𝑐𝑜𝑛𝑐𝑎𝑡 ¹X, 𝛿Xº𝑊Δ¸𝑏Δ). (8)
Here, X=»𝑥1, 𝑥2, . . .¼ 2 R𝐿𝐷denotes an 𝐿-length sequence of 𝐷-
dimension auction embedding. 𝑊B,𝑊C2 R𝐷𝑁map input tokens
into the input matrix and output matrix, respectively. 𝛿Xdenotes
time intervals between the successive auctions. Its first entry is set
to0. We concatenate Xand𝛿Xalong the dimension axis, thereby
5930KDD’24, August 25–29, 2024, Barcelona, Spain Xiaoyu Wang et al.
making AdVance aware of the time irregularity. 𝑊Δ2 R¹𝐷¸1º𝐷
maps input tokens and time intervals into the SSM’s stepsizes, and
𝜏Δ¹𝑥º=log¹1¸exp¹𝑥ººdenotes the Softplus function, a smooth
approximation to the ReLU function, making sure the stepsizes al-
ways positive. 𝑏B,𝑏C, and 𝑏Δare all biases.
Following the same setting as [13, 27, 53], we set the transition
matrix A2 R𝑁𝑁as diagonal to save computation. Note that
the hidden state’s dimension 𝑁is often much smaller than 𝐷. To
slim 𝑊Δ2 R¹𝐷¸1º𝐷, we replace it by the product of 𝑊¹1º
Δ2
R¹𝐷¸1º𝑁and𝑊¹2º
Δ2 R𝑁𝐷, reducing the O¹𝐷2ºtoO¹𝑁 𝐷º.
Once we get ¹Δ,A,B,Cº, we calculate the discretized version
¹A,B,Cºusing
Eq. (3) and train the conditional SSM. Multiple tech-
niques can accelerate AdVance’s training speed, such as FlashAt-
tention [9] and parallel scan [3].
We use the SSM’s last output 𝑦𝐿as the whole campaign’s sum-
mary vector. We also accumulate each auction’s expected cost, ex-
posure, click, and conversion and concatenate them into a vector
𝑃𝑎𝑐𝑐𝑢 2 R4. Finally, AdVance predicts all metrics of campaign-
level performance in one pass as a 4D vector:
»𝑐𝑜𝑠𝑡, 𝑒𝑥𝑝, 𝑐𝑙𝑘, 𝑐𝑣𝑟 ¼=𝑓(𝑐𝑜𝑛𝑐𝑎𝑡 ¹𝑦𝐿, 𝑃𝑎𝑐𝑐𝑢º;𝜃𝐶𝑃), (9)
where 𝜃𝐶𝑃is the model parameter.
4.4 Training and Inference
4.4.1 Offline Training. Each training sample corresponds to one
logged ad campaign. It contains a time-stamped sequence of auc-
tions in which this ad has participated. Each auction sample records
all competitor ads, user-related features, contextual information,
the auction winner, and the users’ click/conversion behavior. To
lower variance and stabilize model convergence, we split the in-
put sequence into chunks of 100 auctions. The training label is a
time series of the corresponding total cost and yields up to that
moment and is also aggregated by chunks.
The training follows a Seq2Seq paradigm [40]: AdVance sequen-
tially processes input auctions and predicts the current campaign
performance whenever a chunk has been finished. The loss is cal-
culated w.r.t. the labels, and AdVance updates its parameters using
back-propagation. Note that the global SSM only takes gradients
w.r.t. campaign performances, while the auction-level and user-
interest models take gradients from both campaign performance
and auxiliary tasks. This creates a mini-batch training for the win-
rate and yield prediction models. To prevent label leakage from the
user’s display history, we devise a causal mask that “covers” the
records after the current time-stamp. Hence, the user preference
and fatigue vector only involve the behaviors so far.
We focus on the complexity relative to the estimated number of
auctions (over 20,000) in which an ad campaign may participate.
First, for an auction with 𝑇historical displayed items and 𝐶com-
petitors, the local SSM costs O¹𝑇 𝑁 𝐷 ºto obtain the 𝐷-dimension fa-
tigue vector. 𝑁is the SSM’s hidden dimension. The cross-attention
costs O¹¹𝐶¸𝑇º2𝐷ºto obtain the 𝐷-dim representation. Then, given
a series of 𝐿auction representations, the global SSM costs O¹𝐿𝑁 𝐷 º
to sequentially process all of them and obtain the 𝐷-dim summary
vector. The MLPs used in local/global SSMs, Transformer encoder,and pCTR/pCVR models are point-wise on each 𝐷-dimension em-
bedding, thus costing O¹𝐷2º. Therefore, AdVance’s overall com-
plexity is O¹𝐿𝐷¹¹𝑇¸1º𝑁¸¹𝐶¸𝑇º2¸𝐷ºº. In contrast, a Transformer
encoder as the global model would have quadratic complexity as
O¹𝐿2𝐷¸𝐿𝐷¹𝑇 𝑁¸ ¹𝐶¸𝑇º2¸𝐷ºº.
4.4.2 Online Inference. At this stage, advertisers launch service
requests with (multiple sets of) campaign criteria, and AdVance
returns the expected performances. We begin with building a sim-
ulated future campaign environment. Following the same methods
as [4, 8, 20], we predict the number of impressions that satisfy the
specified user targeting. We then sample auction records from the
previous day’s log according to the predicted volume. This offers
more realistic competitor features and timely user interest. To bet-
ter approximate the future environment, the Tencent platform del-
icately categorizes user targeting into 188,785 classes and utilizes
CLOCK [46], a multi-variable neural forecaster, to accurately pre-
dict the impression volume of each class.
Once the future environment is built, we insert the target ad
and its bid price into each auction. We feed the modified auction
sequence to AdVance to re-calculate each auction’s win-rate, ex-
pected yield, and the final campaign performance. To simulate in-
terest evolution, we randomly append new ads to the user’s dis-
play history according to win-rates and update the click sequence
according to pCTR and pCVR. The fatigue vector is recurrently
updated accordingly. After traversing the auction sequence with
linear complexity, AdVance outputs the expected cost, exposure,
click, and conversion volumes.
5 Experiments
We conduct experiments and ablation studies on industrial datasets
from Tencent Advertising to validate our AdVance framework and
investigate four research questions, i.e.,RQ1: Prove AdVance’s ef-
ficacy and superiority over state-of-the-art methods for campaign
performance forecasting. RQ2: Demonstrate the necessity of mod-
eling user preference and fatigue evolution. RQ3: Highlight the
importance of introducing auxiliary tasks for auction representa-
tion and campaign-level prediction. RQ4: Evaluate the impact of
different sequence-modeling techniques for campaign-level sum-
marization. Lastly, we introduce the online A/B testing of AdVance
to present its practical value in real-world scenarios.
5.1 Experimental Settings
5.1.1 Dataset. We aim to train models that can integrate auction-
and campaign-level information. Hence, the dataset should contain
user history and each auction’s competitor ads. No public dataset
satisfies the requirements, so we collected our dataset from the
Tencent Advertising platform. This dataset comprises 1.5 billion
records from June 1, 2023, to June 30, 2023. Each record contains
the user feature, user display history with corresponding behav-
iors, contextual information, and all competitor ads with their ad
content, category ID, targeting criteria, bid price, etc.
We focus on campaigns with over 20,000 records for better data
quality and lower variance. Two business concerns also support
this: First, advertisers with higher investments are more sensitive
to budget efficiency. They are also more likely to increase invest-
ment when receiving positive feedback from AdVance. Second, ads
5931Know in AdVance : Linear-Complexity Forecasting of Ad Campaign Performance with Evolving User Interest KDD ’24, August 25–29, 2024, Barcelona, Spain
with more frequent exposure in a longer period are often more
prone to interest evolution and fatigue. We select 6,000 campaigns,
with 1,000 for CPM, 2,000 for CPC, and 3,000 for CPA ads.
5.1.2 Compared Methods. We compare with auction-level meth-
ods, which beat coarse-grained ones by a large margin. The base-
line methods include those from the industry like Yahoo [8], Mi-
crosoft [29], and Alibaba [4], and academic works as follows: 1)
CPF [8] assumes a mixed log-normal distribution for bid prices
and estimates its mean and standard deviation by regression. The
win-rate is calculated by the cumulative density function (CDF).
CPF trains decision trees to predict click/conversion rates and mul-
tiply them with the win-rates, thereby obtaining the expected yield.
The final result is the accumulation of auction-level performance.
2)GMIF [29] uses a first-order Dynamic Linear Model to forecast
the number of future impressions. It then trains a Bayes net to es-
timate the threshold bid price to win a specific auction. Its paper
omits the model design of pCTR/pCVR, so we use DeepFM [15] in-
stead. 3) MTLN [4] assumes static user traffic and uses DeepFM to
estimate yields. Like us, MTLN also introduces a campaign-level
model conditioned on the accumulated performance. An MMoE
[25] model generates the final result. 4) DLF [33] surpasses previ-
ous works [22, 47, 48, 55] in win-rate estimation. It discards the
prior assumptions of win-rate distribution and uses a dedicated
RNN to learn a discrete probability over the bid price. 5) MTAE
[51] further enhances DLF with multi-task learning, leveraging the
correlation between win-rates and click-through rates prediction.
5.1.3 Evaluation Metric. The Tencent platform keeps storing new
auction records and uses them to update numerous online models.
The records form an ever-increasing time series, and we adopt a
sliding-window paradigm: trained on an input window of records,
the model predicts the campaign performance for a future period
(a.k.a. ,forecasting horizon ) of records. The window then goes on
with a stepsize of 1 hour, and we fine-tune the model using new
samples. This process is executed recurrently. At each time step, we
calculate the weighted mean absolute percentage error (WMAPE):
WMAPE B∑
𝑖weight𝑖j𝑦𝑖 ˆ𝑦𝑖j
j𝑦𝑖j, (10)
wher
e weight𝑖is the ratio between the 𝑖-th campaign’s cost and the
total cost of all campaigns, 𝑦𝑖and ˆ𝑦𝑖represent the ground truth and
estimation, respectively. We specifically chose WMAPE for three
key reasons:
Customer Focus: AdVance targets advertisers and sales,
who prefer clear indicators. WMAPE offers a simple and in-
tuitive metric for them to form an immediate impression of
the system’s performance and campaign effectiveness.
Multi-Campaign Management: Advertisers often run mul-
tiple sub-campaigns simultaneously, either for the same prod-
uct with different ad creatives (e.g., images and slogans) or
for different products belonging to the same brand. WMAPE
provides a weighted overall performance view based on cam-
paign budgets, aiding advertisers in policy adjustments.
Conservative Risk Management: While MAPE can favor
under-prediction, it translates to a more cautious approach
for our customers. This reduces the risk of over-optimism.Table 1: Timestep-averaged WMAPE of exposure, click, con-
version, and cost for five baselines and AdVance w.r.t. differ-
ent forecasting horizons from 1H to 24H. The best results are
highlighted in bold.
Metho
d CPF GMIF MTLN DLF MTAE AdVance
1He
xp 0.138
0.126 0.113 0.105 0.092 0.045
clk 0.159
0.153 0.158 0.131 0.125 0.061
cvr 0.171
0.173 0.164 0.158 0.135 0.099
cost 0.154
0.149 0.147 0.129 0.119 0.075
6He
xp 0.174
0.159 0.155 0.142 0.127 0.069
clk 0.201
0.217 0.193 0.134 0.130 0.090
cvr 0.275
0.283 0.266 0.247 0.215 0.149
cost 0.228
0.212 0.219 0.196 0.147 0.116
12He
xp 0.193
0.183 0.205 0.157 0.132 0.092
clk 0.215
0.248 0.230 0.159 0.141 0.101
cvr 0.298
0.311 0.347 0.256 0.249 0.183
cost 0.267
0.271 0.295 0.213 0.167 0.132
24He
xp 0.254
0.231 0.269 0.176 0.159 0.112
clk 0.273
0.268 0.258 0.162 0.187 0.104
cvr 0.317
0.321 0.366 0.267 0.254 0.196
cost 0.283
0.270 0.304 0.196 0.192 0.145
A
dditionally, real-world systems have safeguards like hand-
crafted rules to prevent excessive spending if the prediction
is low and advertisers allocate too much budget. But there’s
not much we can do about inferior campaign performance
caused by insufficient budget or narrowed user targeting.
As a warm-up, we pre-train all models on the records from June
1, 2023, to June 7, 2023. Then, we accumulate the WMAPE per step
and calculate the average as the evaluation metric. We retrain the
model on the whole dataset every 24 hours. We vary the forecast-
ing horizon to evaluate the capacity of modeling long sequences
as 1, 6, 12, and 24 hours.
5.1.4 Implementation Details. We set the display history length to
300. Displayed items, fatigue vectors, user features, contextual in-
formation, and candidate ads are all 256-dimensional embeddings.
We stack three encoder layers with four heads and 1024 hidden
dimensions. 𝜃𝑤𝑖𝑛,𝜃𝐶𝑇𝑅, and 𝜃𝐶
𝑉𝑅are all three-layer MLPs with
hidden neurons [128, 64, 1] and ReLU activation. We stack three
SSM layers with the hidden state dimension 𝑁=16for local and
global modeling. The final campaign performance model 𝜃𝐶𝑃is an
MLP of [128, 64, 4] with ReLU activation. The model is trained with
an AdamW optimizer with a learning rate of 0.001, 𝛽1of 0.9, 𝛽2of
0.995, and 𝜖of 1e-07. Batch-size = 32. Due to their equal value, the
win-rate and yield prediction loss weights are set as [0.5, 0.5].
5.2 System Performance
As shown in Table 1, all models exhibit performance declines when
the forecasting horizons are prolonged. This is mainly caused by
the distribution shift of the campaign environment, such as newly
introduced campaigns and old ones adjusting their criteria. Despite
these declines, AdVance consistently outperforms the other meth-
ods by integrating auction- and campaign-level information and
capturing interest evolution, addressing RQ1.
5932KDD’24, August 25–29, 2024, Barcelona, Spain Xiaoyu Wang et al.
Table 2: Timestep-averaged WMAPE of exposure, click, con-
version, and cost for five variants of AdVance w.r.t. different
forecasting horizons from 1H to 24H. The best results are
highlighted in bold.
Metho
d Static Pref Aux Accu Reg AdVance
1He
xp 0.142
0.124 0.168 0.117 0.051 0.045
clk 0.188
0.153 0.179 0.146 0.072 0.061
cvr 0.201
0.168 0.205 0.155 0.115 0.099
cost 0.177
0.149 0.188 0.135 0.087 0.075
6He
xp 0.199
0.175 0.254 0.162 0.074 0.069
clk 0.218
0.181 0.276 0.180 0.102 0.090
cvr 0.276
0.245 0.319 0.225 0.158 0.149
cost 0.236
0.199 0.287 0.213 0.123 0.116
12He
xp 0.251
0.187 0.344 0.191 0.116 0.092
clk 0.268
0.206 0.372 0.218 0.128 0.101
cvr 0.379
0.261 0.401 0.279 0.192 0.183
cost 0.327
0.223 0.392 0.245 0.143 0.132
24He
xp 0.325
0.249 0.466 0.272 0.117 0.112
clk 0.317
0.273 0.501 0.319 0.123 0.104
cvr 0.405
0.312 0.529 0.355 0.215 0.196
cost 0.382
0.280 0.485 0.318 0.171 0.145
Among
the compared methods, CPF’s log-normal assumption of
bid prices severely limits its capacity to model the complex compe-
tition among bidders. In contrast, Bayes net captures the probabilis-
tic connections among factors contributing to the auction victory,
leading to improved GMIF performance. MTLN’s multi-task struc-
ture also considers the auction- and campaign-level information.
However, MTLN discards the auction representation. Its global model
can not handle the long sequence of auction records; it only takes
the accumulated performance and coarse-grained campaign statis-
tics. DLF surpasses the aforementioned methods by a large mar-
gin. It replaces the pre-defined win-rate distribution with a more
flexible RNN, allowing DLF to adapt to the varying competition
environment. Finally, MTAE outperforms DLF regarding reduced
model complexity and maintenance overheads. MTAE improves
accuracy by leveraging correlations between multiple tasks.
In summary, a more flexible form of win-rate modeling and
multi-task learning can promote forecasting accuracy significantly.
However, the lack of modeling user interest evolution and campaign-
level sequence leads to inferior performance.
5.3 Ablation Study
We conduct ablation studies involving five AdVance variants to as-
sess each component’s individual contributions and effectiveness:
1)Static: We discard the display history and assume a static user
interest. 2) Pref: We preserve the clicked items for user preference
and disregard the user fatigue. 3) Aux: We remove the auxiliary
tasks of win-rates and pCTR and directly learn the auction repre-
sentation. 4) Accu: We do not accumulate the auction-level results,
making the campaign-level forecasting independent. 5) Reg: We
assume auction and display sequences have regular time intervals.
As depicted in Table 2, Static’s performance drops severely and
further declines with a longer horizon. Pref alleviates such degra-
dation by considering preference evolution, but its incomplete viewTable 3: Timestep-averaged WMAPE results. Run on a V100-
16GB GPU. OOM indicates out-of-memory .
Metho
d Ind RNN Transformer S4 AdVance
1He
xp 0.062
0.061 0.043 0.059 0.045
clk 0.071
0.075 0.061 0.072 0.061
cvr 0.105
0.102 0.103 0.111 0.099
cost 0.081
0.079 0.076 0.081 0.075
6He
xp 0.094
0.104 0.066 0.083 0.069
clk 0.113
0.115 0.091 0.104 0.090
cvr 0.158
0.177 0.147 0.162 0.149
cost 0.124
0.146 0.115 0.131 0.116
12He
xp 0.115
0.133 0.093 0.103 0.092
clk 0.132
0.145 0.107 0.117 0.101
cvr 0.207
0.242 0.184 0.189 0.183
cost 0.147
0.209 0.135 0.148 0.132
24He
xp 0.150
0.172
OOM0.128 0.112
clk 0.166
0.181 0.149 0.104
cvr 0.205
0.215 0.201 0.196
cost 0.181
0.186 0.172 0.145
of
user interest makes it inferior to AdVance, thus addressing RQ2.
The absence of supervision signals during model training imposes
significant difficulty in learning a meaningful representation. This
accounts for Aux’s performance gap. Using the Divide & Conquer
policy, we decompose the campaign performance into numerous
auction performances and accumulate them. The accumulated re-
sults serve as an informative reference and reduce the overall diffi-
culty of forecasting. This improves Accu ’s accuracy and addresses
RQ3. The changes in traffic affect the supply of user impressions
and stimulate the intensity of auctions. Reg omits the non-stationary
traffic, causing an accuracy drop.
To answer RQ4, we modify AdVance’s global summarizer and
obtain four variants: 1) Ind: No global model, using accumulated
auction performance. 2) RNN: Using LSTM to summarize auction
sequence. 3) Transformer: Using a quadratic-complexity encoder
with time-stamped position embedding. 4) S4:Using an SSM with
parameters independent of inputs and time intervals.
As depicted in Table 3, the lack of a holistic view of the campaign
environment leads to Ind’s performance drop, proving the neces-
sity of a global model. RNN features linear complexity but has
difficulty memorizing too long context. In contrast, Transformer
explicitly preserves all previous tokens, achieving the best perfor-
mance for a moderate context length. However, the inference mem-
ory grows linearly w.r.t. input length and reports OOM for the 24-
hour horizon. It also presents more than five times the latency of
AdVance due to its quadratic complexity, making it unsuitable for
online service. S4[14] can compress long sequences with linear-
complexity calculation, but it cannot selectively store salient in-
formation from an unevenly distributed time series, leading to its
accuracy declines.
In summary, a comprehensive solution for CPF tasks should con-
sider the user’s preference & fatigue evolution, long-context mod-
eling with low complexity, and a tight connection between auction-
and campaign-level information.
5933Know in AdVance : Linear-Complexity Forecasting of Ad Campaign Performance with Evolving User Interest KDD ’24, August 25–29, 2024, Barcelona, Spain
0.820.840.860.880.90.920.940.960.98
Campaign 1Campaign 2Campaign 3Campaign 4Campaign 5Wide&DeepDIENFANAdVanceAUC
FoodSmartphoneClothesGamesCosmetics0.820.840.860.880.900.920.940.960.98
Figur
e 2: The AUC of three baselines and AdVance on five
campaigns from various industries.
5.4 Further Investigation
AdVance’s user interest and local auction modules constitute a click-
through rate model. We compare it with representative pCTR mod-
els to demonstrate the impact of interest evolution, especially for
long-period campaigns: 1) Wide&Deep [6]: A combination of a
deep neural network and a linear model that captures low- and
high-order feature interactions. 2) DIEN [56]: A sequential model
that considers interactions between user clicks and candidate ads.
A dedicated RNN captures the evolution of user preference over
time. 3) FAN [24]: An improved interest model that incorporates
the frequency-domain feature for user fatigue.
We select five campaigns from various industries, i.e., food, smart-
phones, clothes, cosmetics, and games. For each campaign, we use
80% records as the training set and 20% as the testing set. The re-
sults are shown in Fig. 2. Wide&Deep performed the worst as it
only considers static user features and can not model the user’s
sequential behaviors. In contrast, DIEN performed better by cap-
turing user preferences hidden in clicked items. FAN calculates the
fast Fourier transformation of the displayed yet non-clicked items
to model user fatigue. However, FAN assumes regular time inter-
vals, and FFT features are inadequate to represent interest evolu-
tion compared to deep neural networks.
In conclusion, both clicked and non-clicked items are necessary
to capture the evolution of preference and fatigue, significantly af-
fecting yield prediction accuracy.
5.5 Online A/B Testing
Our AdVance has been implemented on the Tencent Advertising
platform, allowing advertisers to try various campaign criteria and
receive corresponding performances in real-time. Advertisers can
then decide the appropriate campaign settings based on predic-
tions and business demands. Given specific criteria, we use the in-
verted index to retrieve qualified records from the system log of
the past 24 hours with a maximum number of 20,000 records, en-
suring a response delay within 5 seconds. We scale the prediction
accordingly to maintain consistency.
To evaluate AdVance’s effectiveness, we conducted online A/B
tests on advertisers of the same industry. We split them into two
groups with similar average revenue per user (ARPU). Only adver-
tisers in group B were granted access to the AdVance services.
Over two weeks, the comparison revealed a 4.5% uplift in ARPUfor group B advertisers due to optimized campaign configuration.
AdVance is processing thousands of queries daily, greatly enhanc-
ing the platform’s income and attractiveness to advertisers.
6 Discussion and Future Work
Like many other industrial practices, AdVance mainly considers
user traffic fluctuation when modeling environment dynamics and
handles it with a fine-grained time series model. However, new
campaigns may participate, and other advertisers may adjust their
bid prices or user targeting as a counterbalance. This can cause a
drop in accuracy over long periods, shown in Table 1.
One possible mitigation is introducing advertiser modeling tech-
niques and game-theory-based competition modeling [50, 54]. The
former can help predict when and how new campaigns will be
launched, and the latter can predict the possible response from
competitors. These future directions hold promise for advancing
the field of ad campaign performance forecasting and facilitating
more effective decisions in online advertising.
7 Conclusion
We propose AdVance, a time-aware framework integrating auction-
and campaign-level modeling. We introduce user preference as a
time-positioned click sequence and emphasize fatigue modeling
by compressing all displayed history into a concise vector. We
trained an encoder in a supervised manner to predict cost and yield
per auction. The encoder applies self-attention/cross-attention on
candidate ads and user features, thereby converting each auction
into informative embedding. To comprehend the generated long,
irregular sequence, we make the linear-complexity SSM’s parame-
ters dependent on current embedding and time interval. The con-
ditional SSM then outputs expected campaign performance, with
its prediction conditioned on the accumulation of auction-level
results. AdVance outperforms state-of-the-art methods on large-
scale industrial datasets, and has been deployed on the Tencent
advertising system, showing a 4.5% uplift in Average Revenue per
User in the A/B test.
Acknowledgments
The research is supported in part by the 2030 National Key AI
Program of China under Grant 2021ZD0110400, China National
Natural Science Foundation with No. 62172379, Key Research Pro-
gram of Frontier Sciences, CAS, Grant No.ZDBS-LY-JSC001, JSPS
KAKENHI Grant No. JP24K02973, and JST ASPIRE Grant No. JPM-
JAP2325. Hao Zhou is the corresponding author.
5934KDD’24, August 25–29, 2024, Barcelona, Spain Xiaoyu Wang et al.
A Tencent Advertising Platform
This section offers more background knowledge of the Tencent ad-
vertising platform on which AdVance has been implemented, in-
cluding auction workflow, data log, and filtering rules.
A.1 Real-time Bidding Workflow
Whenever a user visits Tencent’s platforms ( e.g., Tencent Video
or Tencent News), an opportunity of showing an advertisement
emerges. We name such opportunities as impressions and sell them
to advertisers for revenue. For each impression, the ad platform re-
trieves relevant ads with matched campaign criteria and initiates
an auction. As shown in Fig. A-1, the platform adopts a funnel-
shaped structure to handle millions of ads in the corpus, including
matching, pre-ranking, ranking, and re-ranking phases. This struc-
ture strikes a balance between precise ad retrieval and timely re-
sponse. Each phase progressively reduces the number of candidate
ads and employs more complex and accurate algorithms. Finally,
about 200 candidates can participate in the re-ranking competition,
and the decision-making process considers user-ad matching, bid
price, and the platform’s own strategy.
visit
～1M～10!	～10"～10#	
MatchingPre-rankingRe-ranking
CorpusRanking～10"Log ServerReal-time Bidding
AdAdvertiser 1Advertiser 2Advertiser 3Campaign Orders
Figur
e A-1: Real-time bidding workflow and the funnel-
shaped structure.
A.2 Data Log and Embedding
The Tencent platform stores auction records in a log server to sup-
port various data-driven algorithms such as pCTR/pCVR estima-
tion and campaign modeling. Each record contains multi-source
information:
User features: age, gender, location, device type, etc.
Contextual information : ad slot placement (web, app), con-
tent topic category, timestamp, etc.
History : ad content, ad category ID, corresponding user be-
haviors (click or purchase), etc.
Candidate ads: ad creative ID, campaign criteria (user at-
tributes, demographics, keywords), ad type (CPM/CPC/CPA),
bid price, auction winner, etc.
Note that the platform adopts a down-sampling strategy to han-
dle the overly large user queries (often billion-level per second),
i.e., only the auctions of a particular group of user IDs are recorded.
User IDs are obtained by uniform sampling from the total ID dic-
tionary. The ratio depends on the I/O and computation capacity of
the log servers.The recorded features can be categorized into continuous (e.g. ,
age, timestamp) and categorical (e.g. , gender, location) features. We
convert the auction records into a set of fixed-length embedding.
Specifically, each categorical feature is represented as a vector of
one-hot encoding, and each continuous feature is represented as
the value itself. One-hot vectors are extremely sparse, so we em-
ploy a domain-specific embedding layer to compress them to a low-
dimensional, dense vector before feeding into the model. Such an
embedding layer is dedicated to each feature domain to lower the
total parameter volume. Finally, we concatenate these vectors to
obtain the corresponding user feature, context, and candidate ads
embeddings as model input shown in Fig. 1.
A.3 Manual Filtering Rules
In Sec. 4.1, we adopt a data-driven method to capture user inter-
est evolution, which can be visualized in Fig. A-2. However, the
platform must consider various factors in the real-world business
scenario to satisfy advertiser demands and enhance long-term user
experience. These factors make the auction process more than a
simple bid ranking problem. Hence, Tencent manually defines mul-
tiple filtering rules in the re-ranking phase to discard certain ads
as a post-process, including but not limited to
Budget Pacing: Ensures budget is spent evenly throughout
the campaign period, avoiding front-loading or overspend-
ing.
Frequency Capping: Limits the number of times a user
sees the same ad or ads from the same industry, preventing
ad fatigue and maximizing reach.
Brand Safety: Protects advertisers from their ads appear-
ing alongside inappropriate or harmful content.
These filtering rules are designed based on human experience
and can not be described by analytic functions to insert into mod-
els. Therefore, we employ a supervised training paradigm with the
data log to approximate the effect of such rules on the auction pro-
cess.
Normalized CTRExposure times w.r.t.the same category
Figur
e A-2: The CTR trends vary with the number of expo-
sures to ads of the same category. We present three cate-
gories, all showing a decline when over-exposed. We normal-
ize the data for business privacy.
5935Know in AdVance : Linear-Complexity Forecasting of Ad Campaign Performance with Evolving User Interest KDD ’24, August 25–29, 2024, Barcelona, Spain
B Baseline Setting
This section offers more details of the implementation of the base-
line methods.
B.1 Setting of Compared Methods
CPF [8]: We adopt a mixture of two log-normal distribution. The
density function is 𝑔¹𝑥;𝜇1, 𝜎1, 𝜇2, 𝜎2, 𝑝º=¹1 𝑝º𝑓¹𝑥;𝜇1, 𝜎1º ¸
𝑝 𝑓¹𝑥;𝜇2, 𝜎2ºand we set 𝑝as 0.1. We train a Factorization-Machine
[34] on the feature embeddings introduced in App. A.2 to estimate
𝜇1, 𝜎1, 𝜇2, 𝜎2. We adopt the classical XGBoost model [5] to predict
the pCTR/pCVR. We use the accumulated yield and cost as the final
campaign performance.
GMIF [29]: For the impression forecasting part, we train a DLM
for each user attribute at the hour level. We follow the original re-
cursive function in the paper but change its parameters to 𝑊=
5,𝑉=15,𝐶0=20. To estimate the threshold price of winning
the auction, we train a Bayes net to estimate the conditional prob-
ability between input variables. Here, each variable corresponds
to one feature domain, such as age, gender, location, etc.Its paper
omits the model design of pCTR/pCVR, so we use DeepFM [15]
trained on the feature embeddings introduced in App. A.2. We use
the accumulated yield and cost as the final campaign performance.
MTLN [4]: We train a DeepFM to estimate the pCTR/pCVR for
each auction and multiply the result with the bid price to obtain the
eCPM. We compare the calculated eCPM with the threshold price
of each auction record and accumulate the yield. We feed the accu-
mulated yield and campaign-level statistics into an MMoE model
consisting of four Expert-MLPs of [128, 64], four Tower-MLPs of
[64, 32, 1] with ReLU activation, and one gate model of MLP [64,
4]. The four Tower-MLPs correspond to the cost, impression, click,
and conversion volumes.
DLF [33]: We discretize the scale of bid price into 100 sub-intervals.
As the number of candidate ads varies, we feed our auction repre-
sentation as the DLF’s input. We stack two layers of LSTM with
a hidden dimension of 512 to predict the conditional win-rate for
each auction. The pCTR and pCVR are estimated using the same
DeepFM model as MTLN. We use the accumulated yield and cost
as the final campaign performance.
MTAE [51]: We feed our auction representation as the MTAE’s
input. MTAE adopts a multi-task paradigm and two top-models
share the input: one consists of MLPs as our 𝑓¹;𝜃𝐶𝑇𝑅ºand𝑓¹;𝜃𝐶
𝑉𝑅º,
and the other is an MLP of [256, 100] for estimating a discrete dis-
tribution over threshold price. Here, we again evenly divide the
scale of the bid price into 100 sub-intervals. MTAE further super-
imposes the DLF model over the bid price model as an auxiliary
task. We use the accumulated yield and cost as the final campaign
performance.
B.2 Setting of Ablation Study
RNN-variant stacks three layers of LSTM with a hidden dimension
of 512. Transformer -variant stacks three encoder layers with four
heads and a hidden dimension of 1024 (expansion = 4). S4-variant
stacks three layers of SSM with 𝑁=16using the authors’ open-
source code.B.3 Setting of Further Investigation
We follow the original structure of Wide&Deep [6], where the
deep model is an MLP of [128, 64, 32] on our auction representation,
and the wide model is a generalized linear model on the one-hot
vector in App. A.2. For the DIEN [56] model, we employ a two-
layer Gated Recurrent Network (GRN) with a hidden dimension of
512 to capture the interest evolution. The obtained interest vector is
concatenated with user features, context, and target ad embedding.
Then we feed it into an MLP of [256, 128, 1] to predict the pCTR.
For the FAN [24] model, we set the length of N-point FFT as 300
and keep the other settings unchanged as the original paper.
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana
Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.
2022. Flamingo: a Visual Language Model for Few-shot Learning. Advances in
Neural Information Processing Systems (NIPS) 35 (2022), 23716–23736.
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-
tion. arXiv preprint arXiv:1607.06450 (2016).
[3] Guy E Blelloch. 1990. Prefix sums and their applications. (1990).
[4] Jun Chen, Cheng Chen, Huayue Zhang, and Qing Tan. 2022. A Unified Frame-
work for Campaign Performance Forecasting in Online Display Advertising.
arXiv preprint arXiv:2202.11877 (2022).
[5] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
InProceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining. 785–794.
[6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7–10.
[7] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
representations using RNN encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 (2014).
[8] Ying Grace Cui and Ruofei Zhang. 2013. Campaign Performance Forecasting for
Non-guaranteed Delivery Advertising. US Patent App. 13/495,614.
[9] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashat-
tention: Fast and memory-efficient exact attention with io-awareness. Advances
in Neural Information Processing Systems 35 (2022), 16344–16359.
[10] Dentsu. 2022. Global Ad Spend Forecast . https://www.dentsu.com
[11] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and
Keping Yang. 2019. Deep session interest network for click-through rate pre-
diction. arXiv preprint arXiv:1905.06482 (2019).
[12] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and
Christopher Re. 2023. Hungry Hungry Hippos: Towards Language Modeling
with State Space Models. In The Eleventh International Conference on Learning
Representations .
[13] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with se-
lective state spaces. arXiv preprint arXiv:2312.00752 (2023).
[14] Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently Modeling Long
Sequences with Structured State Spaces. In International Conference on Learning
Representations .
[15] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction.
arXiv preprint arXiv:1703.04247 (2017).
[16] Tong Guo, Xuanping Li, Haitao Yang, Xiao Liang, Yong Yuan, Jingyou Hou,
Bingqing Ke, Chao Zhang, Junlin He, Shunyu Zhang, et al. 2023. Query-
dominant User Interest Network for Large-Scale Search Ranking. In Proceedings
of the 32nd ACM International Conference on Information and Knowledge Man-
agement. 629–638.
[17] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, An-
toine Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from
predicting clicks on ads at facebook. In Proceedings of the eighth international
workshop on data mining for online advertising . 1–9.
[18] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neu-
ral computation 9, 8 (1997), 1735–1780.
[19] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. 2022. Transformer quality
in linear time. In International Conference on Machine Learning . PMLR, 9099–
9117.
[20] Xiaohu Jiang, Dan Zhang, Wenjie Fu, Linji Yang, and Spencer Powell. 2015.
Predicting the Performance of an Advertising Campaign. US Patent App.
14/292,277.
5936KDD’24, August 25–29, 2024, Barcelona, Spain Xiaoyu Wang et al.
[21] Kristopher Kalish, Yuan-Chyuan Sheu, Jeremy Kayne, Michael Weaver, John Fer-
ber, and Lon Otremba. 2016. Method and system for forecasting a campaign
performance using predictive modeling. US Patent App. 14/747,706.
[22] Changhee Lee, William Zame, Jinsung Yoon, and Mihaela Van Der Schaar. 2018.
Deephit: A deep learning approach to survival analysis with competing risks. In
Proceedings of the AAAI conference on artificial intelligence, Vol. 32.
[23] Jiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time interval aware self-
attention for sequential recommendation. In Proceedings of the 13th International
Conference on Web Search and Data Mining. 322–330.
[24] Ming Li, Naiyin Liu, Xiaofeng Pan, Yang Huang, Ningning Li, Yingmin Su,
Chengjun Mao, and Bo Cao. 2023. FAN: Fatigue-Aware Network for Click-
Through Rate Prediction in E-commerce Recommendation. In International Con-
ference on Database Systems for Advanced Applications . Springer, 502–514.
[25] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.
Modeling task relationships in multi-task learning with multi-gate mixture-of-
experts. In Proceedings of the 24th ACM SIGKDD international conference on
knowledge discovery & data mining. 1930–1939.
[26] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun
Gai. 2018. Entire space multi-task model: An effective approach for estimating
post-click conversion rate. In The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval . 1137–1140.
[27] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham
Neubig, Jonathan May, and Luke Zettlemoyer. 2022. Mega: Moving Average
Equipped Gated Attention. In The Eleventh International Conference on Learning
Representations .
[28] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar
Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. 2018. Explor-
ing the limits of weakly supervised pretraining. In Proceedings of the European
conference on computer vision (ECCV). 181–196.
[29] Abhirup Nath, Shibnath Mukherjee, Prateek Jain, Navin Goyal, and Srivatsan
Laxman. 2013. Ad Impression Forecasting for Sponsored Search. In Proceedings
of the 22nd International Conference on World Wide Web . 943–952.
[30] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong se-
quential behavior data for click-through rate prediction. In Proceedings of the
29th ACM International Conference on Information & Knowledge Management .
2685–2692.
[31] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020.
User behavior retrieval for click-through rate prediction. In Proceedings of the
43rd International ACM SIGIR Conference on Research and Development in Infor-
mation Retrieval . 2347–2356.
[32] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.
2016. Product-based neural networks for user response prediction. In IEEE 16th
international conference on data mining (ICDM). IEEE, 1149–1154.
[33] Kan Ren, Jiarui Qin, Lei Zheng, Zhengyu Yang, Weinan Zhang, and Yong Yu.
2019. Deep Landscape Forecasting for Real-time Bidding Advertising. In Proceed-
ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
& Data Mining. 363–372.
[34] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International confer-
ence on data mining. IEEE, 995–1000.
[35] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting
clicks: estimating the click-through rate for new ads. In Proceedings of the 16th
international conference on World Wide Web . 521–530.
[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer. 2022. High-resolution Image Synthesis with Latent Diffusion Models.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR) . 10684–10695.
[37] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-
based collaborative filtering recommendation algorithms. In Proceedings of the
10th international conference on World Wide Web . 285–295.
[38] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with
Relative Position Representations. In Proceedings of NAACL-HLT . 464–468.
[39] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. 2023. Simplified
State Space Layers for Sequence Modeling. In The Eleventh International Confer-
ence on Learning Representations .
[40] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learn-
ing with neural networks. Advances in neural information processing systems 27(2014).
[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. Advances in neural information processing systems 30 (2017).
[42] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, An-
drew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds,
Petko Georgiev, et al. 2019. Grandmaster level in StarCraft II using multi-agent
reinforcement learning. Nature 575, 7782 (2019), 350–354.
[43] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. Ad-
vances in neural information processing systems 28 (2015).
[44] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network
for ad click predictions. In Proceedings of the ADKDD’17 . 1–7.
[45] Xuerui Wang, Andrei Broder, Marcus Fontoura, and Vanja Josifovski. 2009. A
Search-based Method for Forecasting Ad Impression in Contextual Advertising.
InProceedings of the 18th International Conference on World Wide Web . 491–500.
[46] XiaoYu Wang, YongHui Guo, Xiaoyang Ma, Dongbo Huang, Lan Xu, Haisheng
Tan, Hao Zhou, and Xiang-Yang Li. 2023. CLOCK: Online Temporal Hierarchical
Framework for Multi-scale Multi-granularity Forecasting of User Impression. In
Proceedings of the 32nd ACM International Conference on Information and Knowl-
edge Management. 2544–2553.
[47] Wush Wu, Mi-Yen Yeh, and Ming-Syan Chen. 2018. Deep censored learning
of the winning price in the real time bidding. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining . 2526–
2535.
[48] Wush Chi-Hsuan Wu, Mi-Yen Yeh, and Ming-Syan Chen. 2015. Predicting win-
ning price in real time bidding with censored data. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowledge Discovery and Data Min-
ing. 1305–1314.
[49] Zhengtao Wu, Lan Zhang, and Hui Sheng. 2021. Efficient Ad-level Impression
Forecasting based on Monotonicity and Sampling. In 2021 7th International Con-
ference on Big Data Computing and Communications (BigCom). IEEE, 180–187.
[50] Haifeng Xu, Bin Gao, Diyi Yang, and Tie-Yan Liu. 2013. Predicting advertiser
bidding behaviors in sponsored search by rationality modeling. In Proceedings
of the 22nd International Conference on World Wide Web (Rio de Janeiro, Brazil)
(WWW ’13). Association for Computing Machinery, New York, NY, USA, 1433–
1444. https://doi.org/10.1145/2488388.2488513
[51] Haizhi Yang, Tengyun Wang, Xiaoli Tang, Qianyu Li, Yueyue Shi, Siyu Jiang, Han
Yu, and Hengjie Song. 2021. Multi-task learning for bias-free joint ctr prediction
and market price modeling in online advertising. In Proceedings of the 30th ACM
International Conference on Information & Knowledge Management. 2291–2300.
[52] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi
Feng, and Shuicheng Yan. 2022. Metaformer is Actually What You Need for
Vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) . 10819–10829.
[53] Michael Zhang, Khaled Kamal Saab, Michael Poli, Tri Dao, Karan Goel, and
Christopher Re. 2023. Effectively Modeling Time Series with Simple Discrete
State Spaces. In The Eleventh International Conference on Learning Representa-
tions.
[54] Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. 2022. Leav-
ing No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for
Advertiser Modeling. In Proceedings of the Fifteenth ACM International Confer-
ence on Web Search and Data Mining (Virtual Event, AZ, USA) (WSDM ’22). As-
sociation for Computing Machinery, New York, NY, USA, 1368–1376. https:
//doi.org/10.1145/3488560.3498479
[55] Weinan Zhang, Tianxiong Zhou, Jun Wang, and Jian Xu. 2016. Bid-aware gra-
dient descent for unbiased learning with censored data in display advertising.
InProceedings of the 22nd ACM SIGKDD international conference on Knowledge
discovery and data mining. 665–674.
[56] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang
Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate
prediction. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33.
5941–5948.
[57] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma,
Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for
click-through rate prediction. In Proceedings of the 24th ACM SIGKDD interna-
tional conference on knowledge discovery & data mining. 1059–1068.
5937