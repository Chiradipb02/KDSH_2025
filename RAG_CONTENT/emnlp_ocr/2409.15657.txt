M2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning
Taowen Wang1*, Yiyang Liu1*, James Chenhao Liang1,11, Junhan Zhao2,
Yiming Cui3,Yuning Mao4,Shaoliang Nie4,Jiahao Liu5,
Fuli Feng6,Zenglin Xu7,8,Cheng Han9,Lifu Huang10,
Qifan Wang4,Dongfang Liu1
1Rochester Institute of Technology,2Harvard Medical School,3ByteDance,4Meta AI,5Meituan,
6University of Science and Technology of China,7Shanghai Academy of AI for Science,
8Fudan University,9University of Missouri - Kansas City,10University of California - Davis
11U.S. Naval Research Laboratory
{tw9146, dongfang.liu}@rit.edu Abstract
Multimodal Large Language Models (MLLMs)
demonstrate remarkable performance across a
wide range of domains, with increasing em-
phasis on enhancing their zero-shot generaliza-
tion capabilities for unseen tasks across vari-
ous modalities. Instruction tuning has emerged
as an effective strategy for achieving zero-
shot generalization by finetuning pretrained
models on diverse multimodal tasks. As the
scale of MLLMs continues to grow, parameter-
efficient finetuning becomes increasingly criti-
cal. However, most existing parameter-efficient
approaches focus only on single modalities
and often overlook the multimodal character-
istics during finetuning. In this work, we in-
troduce a novel Multimodal Prompt Tuning
(M2PT) approach for efficient instruction tun-
ing of MLLMs. M2PT effectively integrates
visual and textual prompts into the vision en-
coder and language processor respectively dur-
ing finetuning, facilitating the extraction and
alignment of features across modalities. Em-
pirical results on various multimodal evalua-
tion datasets demonstrate the superior perfor-
mance of our approach compared to several
state-of-the-art baselines. A comprehensive set
of ablation studies validates the effectiveness
of our prompt design and the efficiency of our
approach.
1 Introduction
Human cognition intricately integrates various sen-
sory modalities to perceive, interpret, and engage
with the environment, fostering a comprehensive
understanding of the surrounding world (Liu et al.,
2023c; Dumas et al., 2009; Chen et al., 2024; Jin
et al., 2024a,c). The development of Multimodal
Large Language Models (MLLMs) (Alayrac et al.,
2022; Yin et al., 2023; Han et al., 2024a,c; Jin
et al., 2024b,d) marks a significant advancement in
emulating this capability, playing a pivotal role in
*Equal contribution.
MME
VSR
SNLI-VE
CIFAR-10CIFAR-100MNISTPOPE Text-VQA
MPT (Ours) LoRA PTUM VPT
32.8932.6230.861503.98
1393.67
1354.621398.7435.64
34.28
33.6839.20
52.95
44.56
90.1045.90
52.31
57.6383.4272.33
54.6653.93
53.75
89.2976.49
82.88
59.1495.5494.7394.2981.26
79.60
76.16Figure 1: Comparison of M2PT and several
PEFT methods , including LoRA (Hu et al., 2022),
PTUM (Yang et al., 2023a) and VPT (Han et al., 2024b),
on multimodal tasks. Our approach exhibits superior
performance across a range of benchmarks.
bridging the gap between human and machine in-
telligence. A key focus in advancing MLLMs is en-
hancing their zero-shot generalization to new mul-
timodal tasks. In this pursuit, multimodal instruc-
tion tuning (Liu et al., 2023c,b; Xu et al., 2023),
which finetunes pretrained models using diverse
and instruction-based multimodal tasks, has proven
effective in improving zero-shot generalization to
unseen multimodal domains.
Despite considerable advancements, finetuning
MLLMs for specific domain knowledge poses sig-
nificant challenges. As the scale and complexity
of these models increase, the training overhead
for downstream tasks grows exponentially (Wu
et al., 2024a; Xu et al., 2024a; Zhang et al., 2024).
These escalating demands render the current tun-
ing schemes for MLLMs obsolete and unsustain-
able, impeding their widespread utility. A promis-
ing solution is the utilization of parameter-efficient
finetuning (PEFT) approaches (Lester et al., 2021;
Hu et al., 2022; Chowdhury et al., 2023; DongarXiv:2409.15657v4  [cs.AI]  30 Oct 2024et al., 2023b; Wang et al., 2023a), which have been
widely applied and achieved notable success in nat-
ural language processing (Liu et al., 2021; Wang
et al., 2022) and computer vision tasks (Jia et al.,
2022; Han et al., 2023; Ju et al., 2022a). How-
ever, most existing PEFT approaches only focus
on single modality tuning while overlooking multi-
modal instructing learning. A primary challenge is
the intricate process of finetuning data of multiple
modalities within a cohesive model, extracting and
aligning feature representations across modalities.
Additionally, there is a pressing need to enhance
the zero-shot generalization capabilities for unseen
multimodal tasks while minimizing training costs.
In light of this, we present a novel Multimodal
Prompt Tuning (M2PT) approach with efficient
and effective MLLM adaptation for zero-shot in-
struction learning. M2PT demonstrates competitive
performance across a wide spectrum of tasks (see
Fig. 1) while tuning 0.09% of overall parameters.
Specifically, we introduce two sets of soft prompts:
visual prompts and textual prompts, which are
prepended to the visual and instruction inputs, re-
spectively. The learned embeddings of the visual
prompts are projected into the embedding space of
the textual prompts. The cross-modality interac-
tions between the two sets of prompts are enforced
during instruction tuning, facilitating the alignment
and learning of the feature representation between
the two modalities. In this way, M2PT provides
explicit guidance and clear directives through in-
struction tuning, enabling models to understand
context and reduce ambiguity in zero-shot settings.
To effectively assess our method, we conduct
comprehensive experiments to evaluate its perfor-
mance. In §4.2, we demonstrate that M2PT sur-
passes several state-of-the-art PEFT techniques
while tuning only 0.09% of the trainable param-
eters. We further conduct activation analysis to
show the effectiveness of the learned prompts dur-
ing the attention computation. In §5, we provide
various ablation analysis on the impact of model
components, prompt length, prompt location, and
data volumes in detail. Furthermore, we conduct
case studies to better understand the advantage of
M2PT and its limitations. We hope this work offers
valuable insights into related fields. We summarize
the main contributions as follows:
•We present multimodal prompt tuning by in-
troducing both visual and textual prompts into
vision encoder and language processor respec-tively. These modality specific prompts play a
crucial role in effectively guiding the model’s
fine-tuning process, enabling fast and accurate
multimodal model adaptation.
•We design the cross-modality interaction be-
tween the prompts from different modalities
during the instruction tuning. By doing so,
M2PT leverages the strengths of each modal-
ity, resulting in comprehensive and coherent
learning results. This synergy empowers the
model to perform complex tasks that require
the integration from multimodal perspectives.
•We conduct comprehensive experiments on
various multimodal tasks in the zero-shot set-
ting, demonstrating the effectiveness of the
proposed approach over several state-of-the-
art parameter efficient finetuning methods.
2 Related Work
Multimodal Large Language Models. MLLMs
(Dai et al., 2023; Driess et al., 2023; Liu et al.,
2023c; Yao et al., 2023; Sun et al., 2024a) integrate
multimodal information ( e.g., audio, image, video),
extending beyond the textual semantic information
processed by conventional Large Language Models
(LLMs).A general structure of MLLMs includes
three main components (Li et al., 2024a): a pre-
trained modality encoder to encode multimodality
data, a pre-trained LLM to reason encoded mul-
timodal data and perform generation tasks, and a
connection layer to project modality information
into tokens. During the standard full finetuning
process (Liu et al., 2023c), a substantial amount of
weights within all intermediate layers and the pre-
trained LLM are continuously updated. Conceptu-
ally different, our approach can elegantly fine-tune
the model with adjusting a minimum of weights.
Instruction Tuning. To enhance the zero-shot
and in-context learning (Brown et al., 2020; Li
et al., 2024b) capabilities of large language mod-
els (LLMs) (Zhao et al., 2023), researchers have
explored instruction tuning (Ouyang et al., 2022;
Zhang et al., 2023), a technique that enables
pre-trained LLMs to be more adaptable for intri-
cate multimodal tasks. Specifically, instruction-
tuning is a process that refines LLMs by finetuning
them on meticulously curated instruction-following
datasets encapsulating user intent and desired out-
puts (Ouyang et al., 2022). With the rapid advance-
ment of multimodal models, instruction tuning hasemerged not only as a state-of-the-art approach for
natural language processing (NLP) tasks but also
naturally extend to vision-related tasks such as im-
age captioning (He et al., 2020; Cornia et al., 2020),
image classification (Radford et al., 2021), visual
question answering (VQA) (Antol et al., 2015).
Parameter-Efficient Finetuning. With the dras-
tic growth in the scale of AI models, especially
MLLMs and LLMs, Parameter-efficient Finetun-
ing (PEFT) (Hu et al., 2022; He et al., 2023; Jie
et al., 2023; Yan et al., 2023) have shown its ca-
pability to efficiently adapt pre-trained models to
diverse downstream tasks without updating signifi-
cant amount of model parameters. Generally, cur-
rent PEFT strategies can be categorized into partial
tuning (Chen et al., 2021; Jia et al., 2021), extra
module such as Low-Rank Adaptation (LoRA) (Jie
et al., 2023) and prompt tuning (Jia et al., 2022; Ju
et al., 2022b; Dong et al., 2023a). However, partial
tuning faces limitations, including unsatisfactory
performance relative to full finetuning (Jia et al.,
2022; Chen et al., 2021; Jia et al., 2021). Extra
module exhibits limited adaptability when consid-
ering various model backbones. In contrast, prompt
tuning (Lester et al., 2021; Ma et al., 2022; He et al.,
2022a; Liu et al., 2023d; Qiu et al., 2020) provides
a general and straightforward solution with power-
ful performance gains. It introduces a set of learn-
able parameters to the input sequence of backbone
models, updating only these parameters during fine-
tuning. Despite its simplicity, the applicability of
prompt tuning within the multimodal paradigm
remains largely unexplored. Unlike approaches
such as MaPLe (Khattak et al., 2023), CoOp (Zhou
et al., 2022b), CoCoOp (Zhou et al., 2022a) and
MoPE (Wu et al., 2024b), which focus on crafting
CLIP-based prompts for classification tasks, our
work targets enhancing the capabilities of MLLMs
in zero-shot instruction-following scenarios, result-
ing in a fundamentally distinct method design. Fur-
thermore, PromptFuse (Liang et al., 2022) only
introduces learnable prompts into textual modal-
ity, neglecting the synergy of multimodality. Our
approach offers flexibility in prompt design, allow-
ing prompts to be independently tailored for each
modality and inserted at various layers. This flex-
ibility significantly enhances the MLLMs’ adapt-
ability while reducing the number of parameters,
improving performance across various datasets.3 Methodology
3.1 Preliminaries
Multimodal Large Language Models integrate
visual and language processing capabilities, lever-
aging the power of LLMs to enhance the compre-
hension of multimodal information. A prevalent
workflow of MLLMs begins with the utilization of
pre-trained vision encoders fv(e.g., LLaV A (Liu
et al., 2023c) and its variants (Li et al., 2022; Sun
et al., 2023)), encoding visual input Ximand ex-
tracting output Ov=fv(Xim)through remarkable
vision understanding ability. Subsequently, the vi-
sion output is further projected into language space,
aligning with the textual embedding and enabling
the model to understand and respond to instructions
effectively. Ultimately, the integrated LLM fllm
assimilates Ovand text embedding Ot. Harnessing
the extensive knowledge of LLM, integrating multi-
modal inputs to generate coherent and contextually
relevant language response y, represented as:
y=fllm(Ov, Ot). (1)
Prompt Tuning is a form of PEFT approach,
demonstrating exceptional efficacy within single-
modality settings under both visual (Han et al.,
2023) and textual (Wang et al., 2023a) domains.
It entails learnable continuous soft prompts into
the input space while concurrently preserving the
majority of parameters within the backbone frozen.
Specifically, given a Klayer transformer-based
model f, soft prompts Pkcombined with the input
ofk-th layer to obtain the output Okas:
O1=f1(P1, E)
Ok=fk(Pk, Ok−1),(2)
where k∈ {2,3, . . . , K }.•and•indicate frozen
and tunable parameter during finetuning, respec-
tively. Eis the embedded vector of initial inputs.
3.2 Multimodal Prompt Tuning
In this section, we formally introduce M2PT, a
novel multimodal prompt tuning approach for the
effective and efficient finetuning of MLLMs. The
overall model architecture is depicted in Figure 2.
Fundamentally, our model necessitates the training
of only three targeted components, while keeping
the backbone parameters from both visual encoder
and LLM frozen. Specifically, these components
include ①Visual Prompt (Pv), which is the learn-
able parameter ( i.e., soft prompt) integrated intoTuned
Frozen
 There is one 
person in the 
picture, riding a 
motorcycle.
Output
  Transformer Block
Transformer
 Block
Vision Question
Vision Encoder
......
LLM......
 ...
  Transformer Block
Transformer
 Block
 Interaction 
layer
Can you provide the number 
of the people in the picture?  Transformer Block
Transformer
 Block
Figure 2: Overview of our M2PT approach. Here, visual prompts are embedded into each layer of the Visual
Encoder , and textual prompts are embedded into each layer of the LLM . These prompts facilitate the extraction and
alignment of features across modalities ( e.g., vision, language). The cross-modality interaction between visual and
textual features is enhanced through layered integration, ultimately improving the model’s capability in zero-shot
instruction learning tasks (see §4).
the visual encoder; ②Textual Prompt (Pt) is in-
corporated into the LLM in order to capture the
nuanced semantic relationships across modalities;
③Cross-modality Interaction , where parameters
are learned to enhance the alignment between fea-
tures extracted by the vision encoder and textual
representations. In summary, prompts from these
distinct modalities ( i.e.,①-②) facilitate the model’s
acquisition of knowledge from multimodal fine-
tuning datasets, capturing the distinctive charac-
teristics inherent in each modality and fostering
cross-modal interaction.
Visual Prompt. We denote the set of visual
prompts as Pv={P1
v, Pi
v,···, PN
v}, where Pi
v
indicates the set of visual prompts in the i-th layer
of the visual encoder, consistent with previous prac-
tice for prompt tuning (Jia et al., 2022; Han et al.,
2023). Each prompt is a dv-dimensional vector
with the same dimensions as the original vision
tokens. Prompts in each layer are placed at the
leftmost positions within the sequence to interact
with other vision tokens. Formally, we have:
O1
v=f1
v(P1
v, Xim)
Oi
v=fi
v(Pi
v, Oi−1),(3)
where i∈ {2,3, . . . , N }, and Oi
vis the i-th layer
vision embedding.
Textual Prompt. Visual prompts serve as an ef-
fective tool for capturing semantic content within
the visual inputs, as well as gaining the ability to
interact with the text modality through the mapping
from visual domain. Nevertheless, the optimiza-
tion of visual elements does not directly affect theinherent representation of LLMs in text modality.
Naturally, to further enhance the text modality’s
processing ability, we introduce the textual prompts
to capture text patterns and influence the inner rep-
resentation within the LLM. Specifically, textual
prompts are denoted as Pt={P1
t, Pj
t,···, PM
t},
where jindicates the prompt inject position of the
j-th layer in a total Mlayers LLM. Each prompt
is adt-dimensional vector with the same dimen-
sionality as the original text tokens. Formally, we
incorporate textual prompts into the LLM as:
O1
m=f1
llm(P1
t, O′
v, Ot)
Oj
m=fj
llm(Pj
t, Oj−1
m)
y=fhead(Oj
m),(4)
where j∈ {2,3, . . . , M },yis the textual output
of the MLLM, Otis the textual embedding, and
fhead is the task-specific head in order to decode
the embeddings into texts.
Cross-modality Interaction. To achieve align-
ment between visual and textual modality, we intro-
duce a tunable interaction layer fin, which is specif-
ically designed to align the output ON
vproduced
by the visual encoder and the textual embedding,
through a linear projection:
O′
v=fin(ON
v). (5)
Here O′
vrepresents the aligned vision embed-
ding. This transformation ensures that the visual
encoder’s output is effectively mapped onto a com-
mon textual representation space. The projected
visual tokens then interact with the textual tokensTable 1: Zero-shot Multimodal Evaluation on all multimodal datasets. The MMAvg represents the average
score on the right seven tasks. LLaV A Align is the stage-one LLaV A without end-to-end finetuning, and LLaV A FT
indicates the fully fine-tuned LLaV A. All the fine-tuned processes are using the same Vision-Flan dataset. M2PTa/b
means textual and visual prompt lengths ‘a’ and ‘b’, respectively. The best performance is in bold .
Method # para MME Text-VQA VSR SNLI-VE CIFAR-10 CIFAR-100 MNIST POPE MMAvg
LLaV A Align (Liu et al., 2023c) - 1110.82 32.62 50.16 34.51 80.00 58.04 52.79 59.10 52.46
LLaV A FT(Liu et al., 2023c) 100% 1587.26 37.26 53.76 43.35 92.97 63.73 94.27 80.82 66.59
LoRA (Hu et al., 2022) 0.63% 1393.67 39.20 52.95 44.56 90.10 45.90 83.42 72.33 61.21
APrompt (Wang et al., 2023a) 0.23% 1406.63 35.26 53.12 45.58 85.74 50.27 84.63 76.16 61.52
PTUM (Yang et al., 2023a) 0.12% 1354.62 34.28 53.75 30.86 82.88 57.63 94.29 80.31 62.00
VPT (Han et al., 2024b) 0.06% 1398.74 33.68 53.93 32.62 76.49 52.31 94.73 79.60 60.48
M2PT10/10(Ours) 0.08% 1490.17 35.64 54.66 32.53 87.92 57.80 94.53 81.29 63.48
M2PT10/20(Ours) 0.09% 1503.98 34.48 53.19 32.89 89.29 59.14 95.54 81.26 63.68
through all LLM layers, facilitating the integra-
tion of visual and textual information. Our elegant
design of M2PT enjoys a few appealing character-
istics:
•Cross-modal Integration. Our M2PT model em-
ploys a unified prompt tuning paradigm. This ap-
proach not only captures the distinctive attributes
of each modality but also facilitates the fluid ex-
change of cross-modal information, enhancing
the model’s capability to comprehend and gener-
ate multimodal data effectively.
•Optimized Parameter Utilization. M2PT demon-
strates superior parameter efficiency by focusing
only on the training of a minimal set of parame-
ters while keeping the majority of the model’s pa-
rameters frozen, allowing a significant reduction
in the number of parameters required (0.09%).
Despite this reduction, M2PT maintains superior
performance on multimodal tasks (see Table 1)
with a balance between computational efficiency
and overall effectiveness in zero-shot setting.
3.3 Implementation Details
We employ LLaV A (Liu et al., 2023c) with CLIP-
L (Radford et al., 2021) ( i.e., 24 transformer blocks)
as the visual encoder and Vicuna-7B-v1.3 (Zheng
et al., 2023) ( i.e., 32 transformer blocks) as the
base LLM for all variants. For the cross-modality
interaction, we use a linear layer to map the embed-
ding dimension from dvtodtto ensure the align-
ment between the two modalities. For the prompt
initialization, we employ Xavier (Glorot and Ben-
gio, 2010) initialization on both visual and textual
prompt to ensure stable modal information delivery
from these prompts at the early stages of training,
thereby facilitating rapid convergence of the model.
More implementation details are provided in §4.1
and Appendix S2.4 Experiments
4.1 Experiment Setup
Datasets. Fortraining , we conduct instruction
tuning on Vision-Flan (Xu et al., 2024b), which is
a human-annotated multimodal instruction tuning
dataset with 191diverse tasks. To reduce compu-
tational costs, we follow common practice (Shen
et al., 2024) and employ a scaled-down version
containing up to 1,000instances per task, result-
ing in a total of 191,105 instances. For zero-
shot evaluation , we examine our approach on the
comprehensive multimodal evaluation benchmark
MME (Fu et al., 2023), measuring both percep-
tion and cognition abilities across 14subtasks.
We further evaluate the model’s capabilities us-
ing7multimodal datasets. Specifically, for Op-
tical Character Recognition, we utilize the Text-
VQA (Singh et al., 2019), and for reasoning, we
employ the Visual Spatial Reasoning ( VSR) (Liu
et al., 2023a). Following (Zhai et al., 2023; Shen
et al., 2024), the perception capability is tested
onCIFAR-10/100 (Krizhevsky et al., 2009) and
MNIST (Deng, 2012). SNLI-VE (Xie et al., 2019)
evaluates Visual Entailment capabilities, while the
POPE (Li et al., 2023) dataset examines the ten-
dency towards object hallucination. More details
are provided in the Appendix S1.
Training Details. Following previous works (Han
et al., 2023; Shen et al., 2024; Jia et al., 2022),
we conduct grid search to match the best tuning
hyperparameters, learning rate ( i.e., [1e−3,9e−4,
7e−4,4e−4,1e−4,5e−5]), textual prompt length
(i.e., [0, 5, 10, 20, 40]) and visual prompt length
(i.e., [0, 5, 10, 20, 40]). For all models, the
learning rate is scheduled following a cosine de-
cay policy, the warm up ratio is set at 0.03 and
trained for 3 epochs except in training epoch ex-
periment. We follow the same batch size setting
in (Shen et al., 2024) as 128 for instruction tun-T extual Prompt
System Prompt Visual PromptImg Emb
Instruction01000.51.0
T extual Prompt
System Prompt Visual PromptImg Emb
Instruction01000.51.0(a)LLM attention activation
Visual Prompt Img Emb2000.030.1
Visual Prompt Img Emb2000.030.1
(b)Visual Encoder attention activation
Figure 3: Comprehensive visualization of attention
activation maps . This figure presents a detailed exami-
nation of the activation patterns within the last layer of
LLM andVisual Encoder , respectively. As seen, the vi-
sion prompts and textual prompts have noticeably high
activation levels during inference ( i.e.,•and•represent
textual prompts’ activation signal and visual prompts’
activation signal, respectively).
ing. Further details are provided in the Appendix
S2. M2PT is implemented in Pytorch (Paszke
et al., 2019). Experiments are conducted on 8
NVIDIA A100 GPUs. Our code is available at
https://github.com/William-wAng618/M2PT .
Evaluation Metrics. The MME incorporates both
Perception and Cognition metrics1. For other mul-
timodal datasets, we use Vicuna-13B-v1.5 (Zheng
et al., 2023) to assess the accuracy of each predic-
tion compared to the groundtruth. Further details
are provided in the Appendix S3.
4.2 Main Result
In Table 1, our main result exhibits a comprehen-
sive zero-shot evaluation of M2PT with several
baselines on eight multimodal datasets. Specif-
ically, we consider four state-of-the-art PEFT
approaches, including LoRA (Hu et al., 2022),
APrompt (Wang et al., 2023a), PTUM (Yang et al.,
2023a) and VPT (Han et al., 2024b). Full fine-
tuned LLaV A ( i.e., LLaV A FT(Liu et al., 2023c))
serves as an upper-bound of multimodal evaluation.
We report the performance of M2PT under two
different settings, with M2PT10/10and M2PT10/20.
There are several key observations from these
1https://github.com/BradyFU/Awesome-Multimodal-
Large-Language-Models/tree/Evaluationresults. First , M2PT achieves the best perfor-
mance among all PEFT approaches in most cases,
demonstrating the effective design of visual and tex-
tual prompts. For example, on MME task, M2PT
demonstrates a significant improvement of 6.90%
and 7.51% compared to two strong prompt tuning
methods, Aprompt and VPT, respectively. This
highlights the limitation of existing prompt tuning
approaches that primarily focus on single modality,
failing to capture the cross-modality interactions.
In contrast, the interaction layer together with the
multimodal LLM employed by our approach suc-
cessfully bridges this gap, resulting in enhanced
performance. Second , the performance of M2PT
reaches 94.75% of the full finetuning performance
while considering only 0.09% of the total model
parameters, demonstrating the parameter efficiency
of M2PT. Moreover, we observe that M2PT outper-
forms the full finetuning LLaV A on VSR, MNIST
and POPE tasks, showing its strong capability in
zero/few shot learning. This is consistent with the
observations in several previous works (Han et al.,
2023; Yang et al., 2023b). Third , it can be seen
that M2PT does not perform very well on the visual
entailment task, SNLI-VE. Our hypothesis is that
logical relationships or causal scenario understand-
ing is critical in this task, which might not be fully
captured by prompt tuning based approaches.
5 Analysis and Discussion
Attention Activation Pattern Analysis. Follow-
ing common practice (Sun et al., 2024b), we extract
and discuss the activation maps from the attention
block of MLLMs, and investigate the influence
of visual and textual prompts in Fig. 3. We ran-
domly select two samples from the MME dataset
and visualize their corresponding activation maps
of the attention block in the last layers of both vi-
sual encoder (Fig. 3(a)) and LLM (Fig. 3(b)). To
analyze the impact of textual and visual prompts
to the frozen components, we categorize them, ac-
cording to LLaVa’s model structure, into textual
prompts, system text tokens, visual prompts, image
tokens and instructions. Several findings can be
observed. First , in the LLM attention activation
map, we observe that the token regions correspond-
ing to textual prompts exhibit elevated activation
levels, indicating their significant role in shaping
the model’s responses. The activation levels of vi-
sual prompts within the LLM, while comparatively
lower, remain notable relative to most other regions.MME CIFAR-10 POPE130013501400145015001550MME Score
1374
135513991504w/o interaction layer
w/o visual prompt
w/o textual prompt
MPT
70.072.575.077.580.082.585.087.590.0
CIFAR-10 & POPE Score80.3 80.182.9
80.3
76.579.689.3
81.3Figure 4: Impact of Different Components.
This suggests a secondary yet substantial role in
multimodal inference. Second , in the activation
maps from Visual Encoder , the activation levels as-
sociated with visual prompts are noticeably higher
than those of most other tokens, underscoring the
critical role of visual prompts in processing visual
information during tuning. These observations sup-
port that visual prompts effectively interact with the
textual prompts, enhancing the alignment between
modalities and thereby improving the model’s per-
formance on the zero-shot instruction learning. Our
component analysis study below further strength-
ens this claim quantitatively.
Impact of Different Components. To investi-
gate the impact of different components in M2PT
across various datasets ( i.e., visual prompts, textual
prompts, and interaction layer), we conducted com-
ponent ablation experiments in Fig. 4 by removing
each component at a time from M2PT10/20. The
results demonstrate that the model performance
drops when any of the trainable prompts is re-
moved, which aligns with our expectations. More-
over, the model performance also decreases with-
out the trainable interaction layer, indicating the
importance of this layer. Furthermore, we observe
that the importance of different components varies
across the tasks. For example, textual prompts play
the most important role in CIFAR-10 and POPE,
while visual prompts lift the performance most on
MME. Once again, it is worth noting that combin-
ing the multimodal prompts with the interaction
layer in M2PT leads to the best performance.
Impact of Prompt Length. In M2PT, prompt
length is the only hyperparameter that needs to be
tuned. To further analyze the impact of different
prompt lengths on model performance, we conduct
a comprehensive study on the lengths of visual
and textual prompts on the Vision-Flan dataset to
better understand their properties. Following com-
mon practice (Han et al., 2023; Jia et al., 2022;
Han et al., 2024b), we use the grid search to span
5 10 20 40
Visual51020 40T extual
1416.67 1421.74 1316.82 1442.381421.15 1490.17 1503.98 1258.491411.40 1337.75 1413.97 1486.891290.69 1371.43 1315.97 1342.35MME
5 10 20 40
Visual51020 40T extual
53.32 52.73 52.95 52.3652.54 54.66 53.19 52.7854.58 53.36 53.44 52.7253.16 53.11 52.96 52.72VSR
5 10 20 40
Visual51020 40T extual
77.25 78.16 77.26 79.3288.34 87.92 89.29 80.5082.83 85.70 83.12 82.1687.26 88.74 88.53 86.97CIFAR-10
5 10 20 40
Visual51020 40T extual
81.68 80.92 81.36 81.2481.28 81.29 81.26 81.2481.86 81.81 81.13 81.1780.62 82.26 81.60 81.39POPE
1250130013501400145015001550
MME
51.051.552.052.553.053.554.054.555.055.5
VSR
7880828486889092
CIFAR-10
80.5080.7581.0081.2581.5081.7582.0082.2582.50
POPEFigure 5: Performance of Different Prompt Length.
Each cell in the map corresponds to the score of a model
with a textual prompt length (row) and a visual prompt
length. A darker hue indicates a higher score, whereas
a lighter hue signifies a lower score.
Table 2: Prompt Location Experiment (M2PT10/20).
Prompt Location MME CIFAT-10 POPE
(a) First Layer 1320.99 82.96 79.48
(b) Odd Layer 1396.87 87.65 75.79
(c) Top Half 1473.42 85.82 80.31
(d) Latter Half 1249.39 83.72 79.34
(e) All 1503.98 89.29 81.26
a range from 5to40on both visual and textual
prompt lengths, reported in Fig. 5. We stop at a
prompt length of 40because performance satura-
tion is observed around this point. Thus, extending
the prompt length further would result in increased
parameter usage without significant performance
gains ( i.e., M2PT shows a slight decrease in perfor-
mance on MME. We argue that this may be due to
overparameterization (Han et al., 2023; Jia et al.,
2022)). When visual prompt length extends from
5to20, and textual prompt length extends from
5to10, noticeable performance gains can be ob-
served. It can be seen that there is no universal
optimal prompt length that consistently achieves
the best performance across different tasks. For
instance, the optimal performance of the model on
MME is achieved with a configuration of 20visual
prompts and 10textual prompts, while 10visual
prompts and 40textual prompts achieve the high-
est performance on POPE. We hypothesize that
different tasks exhibit distinct data distributions,
with ‘difficult’ tasks potentially requiring longer
prompts to effectively capture the underlying pat-
terns. Nonetheless, we observed that the perfor-
mance of M2PT remains relatively stable within a
certain range of prompt lengths.Impact of Prompt Location. Following (Jia
et al., 2022), Table 2 studies the insertion of
prompts at different layers. We design five dis-
tinct settings in which prompts are integrated into
both the Vision Encoder and LLM but at differ-
ent locations. Specifically, we introduce prompts
into: (a) the first layer; (b) every odd-number layer
(i.e.,[1,3, . . . , 23]∈N,[1,3, . . . , 31]∈M); (c)
the first half of the layers ( i.e.,[1-12]∈N,[1-
16]∈M); (d) the latter half of the layers ( i.e.,
[12-24 ]∈N,[16-32 ]∈M); and (e) all layers.
Each variant reports the best prompt length combi-
nation selected with MME evaluation. Generally,
M2PT’s performance is positively correlated with
prompt depth. Yet the accuracy drops when insert-
ing prompts from top to bottom, suggesting that
prompts at earlier layers matter more than those at
latter layers, which is consistent with the observa-
tions in (Jia et al., 2022; Wang et al., 2023a).
25 50 75 1001350140014501500MME Score
Data Volume % (Vision-Flan) MME
CIFAR-10
POPE
707580859095100
CIFAR-10 & POPE Score
Figure 6: Data Volume Experiment (M2PT10/20).
1 2 3142014401460148015001520MME Score
Training Epoch (Vision-Flan)MME
CIFAR-10
POPE
7580859095
CIFAR-10 & POPE Score
Figure 7: Training Epoch Experiment (M2PT10/20).
Effect of Data Volume and Training Epoch. In
Fig. 6, we randomly sample data from Vision-Flan
at different scales ( i.e., 25%, 50%, 75%, 100%) to
evaluate the performance of the model with limited
data. The results demonstrate that M2PT main-
tains excellent performance despite significant data
reduction, highlighting efficiency and robustness
regarding data quantity. This property indicates
that M2PT exhibits substantial tolerance to data
scale, suggesting a promising future for real-world
applications with constrained data.
In Fig. 7, we analyze the M2PT’s performance
on different training epochs. This experiment is
CIFAR-10MME：
Question: Is there a chair in this image? 
Please answer yes or no.
MPT :   
PTUM : 
VPT:      
  
Question: What is the object in the image? 
Please only answer a single object.
MPT: 
PTUM: 
VPT :   
Yes
No
NoGroundtruth:   Yes
Cat
Truck
AirplaneGroundtruth:   Cat
Figure 8: Winning cases onMME andCIFAR-10 .
Boat
44%
Ship
48%Others
8%
BoatShipOthersShip
CIFAR-10    Question: What is the object in 
the image? Please only answer a 
single object in airplane, bird, cat...
Ship
Boat
Others
Figure 9: Failure cases study on CIFAR-10 .
conducted using the optimal hyperparameter com-
bination of 10textual and 20visual prompts. It
can be seen that the model performance generally
improves with more training epochs, confirming
that M2PT can achieve remarkable performance
after sufficient training and demonstrate robustness
in adapting to different training epochs.
Case Study. In Fig. 8, we present cases where
M2PT demonstrates success on the MME and
CIFAR-10 , while other approaches fail ( i.e., VPT,
PTUM). For example, on MME, while M2PT cor-
rectly identifies “a chair” in the image, both VPT
and PTUM fail to capture this concept and respond
with the wrong answer. On CIFAR-10 , VPT and
PTUM both misidentify “cat” as a different cate-
gory. We posit that the insufficiency of VPT may
stem from its inadequate understanding of logi-
cal relationships or causal scenarios. PTUM em-
ploys single-modality tuning exclusively, rendering
it inadequate for managing complex multi-modal
behavior and capturing interactions between modal-
ities. In sharp contrast, M2PT takes an aspect of
multimodal, enhancing both visual modality com-
prehension and textual modality causal inference.
We further conduct failure case studies for M2PT
on CIFAR-10 dataset. The results in Fig. 9 indi-
cate that the predominant misclassification within
the CIFAR-10 ( i.e., 44% or 443 examples of all
such misclassification) is the misidentification of
“ships” as “boats”. This high error rate could po-
tentially be attributed to the limited resolution ofthe images within the CIFAR-10, coupled with the
inherent similarity in the semantic characteristics
between “ship” and “boat”. Additional case studies
and discussions are provided in Appendix S4.
6 Conclusion
We introduce M2PT, a novel framework in mul-
timodal prompt tuning for zero-shot instruction
learning. Our framework offers several advantages:
i)it introduces visual and textual prompts elegantly
into vision encoder and LLM, respectively, en-
abling fast and accurate multimodal adaptation;
ii)it synergizes modalities by cross-modality inter-
action, enjoying coherent integration from multi-
modal perspectives; and iii)it significantly reduces
the number of trainable parameters compared to
conventional finetuning methods, while maintain-
ing robust performance across zero-shot tasks. As
a whole, we conclude that the outcomes elucidated
in this paper impart essential understandings and
necessitate further exploration within this realm.
Limitations
For potential limitations, M2PT requires two hy-
perparameters on prompt length searching ( i.e., Vi-
sual Prompt, Textual Prompt). Though in practice,
we find both lengths vary into a relatively narrow
range (see §5), and are sufficient enough to out-
perform allcurrent methods, there is still possible
integration (He et al., 2022b) of a local search-
ing network to generate optimal combinations of
lengths. Another potential limitation is that M2PT,
akin to other PEFT approaches (Han et al., 2023;
Jia et al., 2022), lacks ad-hoc explainability (Biehl
et al., 2016; Wang et al., 2023b). While in §4, we
demonstrates that activation maps from MLLMs
are significantly influenced by visual and textual
prompts, further research is necessary to elucidate
the underlying nature of these prompts.
7 Acknowledgements
This research was supported by the National Sci-
ence Foundation under Grant No. 2242243. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of U.S.
Naval Research Laboratory (NRL) or the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for Governmentpurposes notwithstanding any copyright notation
herein.
References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, Roman Ring, Eliza Rutherford, Serkan
Cabi, Tengda Han, Zhitao Gong, Sina Samangooei,
Marianne Monteiro, Jacob L. Menick, Sebastian
Borgeaud, Andy Brock, Aida Nematzadeh, Sahand
Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karén Si-
monyan. 2022. Flamingo: a visual language model
for few-shot learning. In Advances in Neural In-
formation Processing Systems 35: Annual Confer-
ence on Neural Information Processing Systems 2022,
NeurIPS 2022, New Orleans, LA, USA, November 28
- December 9, 2022 .
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: visual question an-
swering. In 2015 IEEE International Conference
on Computer Vision, ICCV 2015, Santiago, Chile,
December 7-13, 2015 , pages 2425–2433. IEEE Com-
puter Society.
Alejandro Barredo Arrieta, Natalia Díaz Rodríguez,
Javier Del Ser, Adrien Bennetot, Siham Tabik, Al-
berto Barbado, Salvador García, Sergio Gil-Lopez,
Daniel Molina, Richard Benjamins, Raja Chatila, and
Francisco Herrera. 2020. Explainable artificial intel-
ligence (XAI): concepts, taxonomies, opportunities
and challenges toward responsible AI. Inf. Fusion ,
58:82–115.
Michael Biehl, Barbara Hammer, and Thomas Villmann.
2016. Prototype-based models in machine learning.
Wiley Interdisciplinary Reviews: Cognitive Science ,
7(2):92–111.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Mingkai Chen, Taowen Wang, James Chenhao Liang,
Chuan Liu, Chunshu Wu, Qifan Wang, Ying Nian
Wu, Michael Huang, Chuang Ren, Ang Li, Tong
Geng, and Dongfang Liu. 2024. Inertial confinement
fusion forecasting via llms. CoRR , abs/2407.11098.Xinlei Chen, Saining Xie, and Kaiming He. 2021. An
empirical study of training self-supervised vision
transformers. In 2021 IEEE/CVF International Con-
ference on Computer Vision, ICCV 2021, Montreal,
QC, Canada, October 10-17, 2021 , pages 9620–9629.
IEEE.
Sanjoy Chowdhury, Sayan Nag, and Dinesh Manocha.
2023. Apollo : Unified adapter and prompt learning
for vision language models. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2023, Singapore, De-
cember 6-10, 2023 , pages 10173–10187. Association
for Computational Linguistics.
Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi,
and Rita Cucchiara. 2020. Meshed-memory trans-
former for image captioning. In 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition, CVPR 2020, Seattle, WA, USA, June 13-19,
2020 , pages 10575–10584. Computer Vision Founda-
tion / IEEE.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven C. H. Hoi.
2023. Instructblip: Towards general-purpose vision-
language models with instruction tuning. In Ad-
vances in Neural Information Processing Systems
36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 .
Li Deng. 2012. The MNIST database of handwritten
digit images for machine learning research [best of
the web]. IEEE Signal Process. Mag. , 29(6):141–
142.
Shaohua Dong, Yunhe Feng, Qing Yang, Yan Huang,
Dongfang Liu, and Heng Fan. 2023a. Efficient multi-
modal semantic segmentation via dual-prompt learn-
ing. CoRR , abs/2312.00360.
Wei Dong, Dawei Yan, Zhijun Lin, and Peng Wang.
2023b. Efficient adaptation of large vision trans-
former via adapter re-composing. In Advances in
Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Sys-
tems 2023, NeurIPS 2023, New Orleans, LA, USA,
December 10 - 16, 2023 .
Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, Jakob
Uszkoreit, and Neil Houlsby. 2021. An image
is worth 16x16 words: Transformers for image
recognition at scale. In 9th International Conference
on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 . OpenReview.net.
Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey
Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan
Wahid, Jonathan Tompson, Quan Vuong, Tianhe
Yu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-
manet, Daniel Duckworth, Sergey Levine, VincentVanhoucke, Karol Hausman, Marc Toussaint, Klaus
Greff, Andy Zeng, Igor Mordatch, and Pete Florence.
2023. Palm-e: An embodied multimodal language
model. In International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , volume 202 of Proceedings of Machine
Learning Research , pages 8469–8488. PMLR.
Bruno Dumas, Denis Lalanne, and Sharon L. Oviatt.
2009. Multimodal interfaces: A survey of princi-
ples, models and frameworks. In Denis Lalanne and
Jürg Kohlas, editors, Human Machine Interaction,
Research Results of the MMI Program , volume 5440
ofLecture Notes in Computer Science , pages 3–26.
Springer.
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-
rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-
grong Ji. 2023. MME: A comprehensive evaluation
benchmark for multimodal large language models.
CoRR , abs/2306.13394.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth Inter-
national Conference on Artificial Intelligence and
Statistics, AISTATS 2010, Chia Laguna Resort, Sar-
dinia, Italy, May 13-15, 2010 , volume 9 of JMLR
Proceedings , pages 249–256. JMLR.org.
Cheng Han, James Chenhao Liang, Qifan Wang, Ma-
jid Rabbani, Sohail A. Dianat, Raghuveer Rao,
Ying Nian Wu, and Dongfang Liu. 2024a. Image
translation as diffusion visual programmers. In The
Twelfth International Conference on Learning Rep-
resentations, ICLR 2024, Vienna, Austria, May 7-11,
2024 . OpenReview.net.
Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao,
Wenguan Wang, Siyuan Qi, and Dongfang Liu. 2023.
E2vpt: An effective and efficient approach for visual
prompt tuning. In IEEE/CVF International Confer-
ence on Computer Vision, ICCV 2023, Paris, France,
October 1-6, 2023 , pages 17445–17456. IEEE.
Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang,
Lifu Huang, Siyuan Qi, and Dongfang Liu. 2024b.
Facing the elephant in the room: Visual prompt tun-
ing or full finetuning? CoRR , abs/2401.12902.
Cheng Han, Qifan Wang, Sohail A. Dianat, Majid Rab-
bani, Raghuveer M. Rao, Yi Fang, Qiang Guan, Lifu
Huang, and Dongfang Liu. 2024c. AMD: automatic
multi-step distillation of large-scale vision models.
CoRR , abs/2407.04208.
Sen He, Wentong Liao, Hamed R. Tavakoli,
Michael Ying Yang, Bodo Rosenhahn, and Nicolas
Pugeault. 2020. Image captioning through image
transformer. In Computer Vision - ACCV 2020 - 15th
Asian Conference on Computer Vision, Kyoto, Japan,
November 30 - December 4, 2020, Revised Selected
Papers, Part IV , volume 12625 of Lecture Notes in
Computer Science , pages 153–169. Springer.Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei
Yang, and Xin Eric Wang. 2023. Parameter-efficient
model adaptation for vision transformers. In Thirty-
Seventh AAAI Conference on Artificial Intelligence,
AAAI 2023, Thirty-Fifth Conference on Innovative
Applications of Artificial Intelligence, IAAI 2023,
Thirteenth Symposium on Educational Advances in
Artificial Intelligence, EAAI 2023, Washington, DC,
USA, February 7-14, 2023 , pages 817–825. AAAI
Press.
Yun He, Huaixiu Steven Zheng, Yi Tay, Jai Prakash
Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang
Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng,
and Ed H. Chi. 2022a. Hyperprompt: Prompt-based
task-conditioning of transformers. In International
Conference on Machine Learning, ICML 2022, 17-23
July 2022, Baltimore, Maryland, USA , volume 162 of
Proceedings of Machine Learning Research , pages
8678–8690. PMLR.
Yun He, Huaixiu Steven Zheng, Yi Tay, Jai Prakash
Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang
Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng,
and Ed H. Chi. 2022b. Hyperprompt: Prompt-based
task-conditioning of transformers. In International
Conference on Machine Learning, ICML 2022, 17-23
July 2022, Baltimore, Maryland, USA , volume 162 of
Proceedings of Machine Learning Research , pages
8678–8690. PMLR.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. Lora: Low-rank adaptation of
large language models. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net.
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire
Cardie, Serge J. Belongie, Bharath Hariharan, and
Ser-Nam Lim. 2022. Visual prompt tuning. CoRR ,
abs/2203.12119.
Menglin Jia, Zuxuan Wu, Austin Reiter, Claire Cardie,
Serge J. Belongie, and Ser-Nam Lim. 2021. Ex-
ploring visual engagement signals for representation
learning. In 2021 IEEE/CVF International Confer-
ence on Computer Vision, ICCV 2021, Montreal, QC,
Canada, October 10-17, 2021 , pages 4186–4197.
IEEE.
Shibo Jie, Haoqing Wang, and Zhi-Hong Deng. 2023.
Revisiting the parameter efficiency of adapters
from the perspective of precision redundancy. In
IEEE/CVF International Conference on Computer
Vision, ICCV 2023, Paris, France, October 1-6, 2023 ,
pages 17171–17180. IEEE.
Mingyu Jin, Haochen Xue, Zhenting Wang, Boming
Kang, Ruosong Ye, Kaixiong Zhou, Mengnan Du,
and Yongfeng Zhang. 2024a. ProLLM: Protein chain-
of-thoughts enhanced LLM for protein-protein inter-
action prediction. In First Conference on Language
Modeling .Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng
Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao,
Kai Mei, Yanda Meng, Kaize Ding, Fan Yang,
Mengnan Du, and Yongfeng Zhang. 2024b. Ex-
ploring concept depth: How large language mod-
els acquire knowledge at different layers? CoRR ,
abs/2404.07066.
Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang,
Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng,
Zhenting Wang, Mengnan Du, Yongfeng Zhang,
and Yanda Meng. 2024c. Health-llm: Personal-
ized retrieval-augmented disease prediction system.
CoRR , abs/2402.00746.
Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao,
Wenyue Hua, Yanda Meng, Yongfeng Zhang, and
Mengnan Du. 2024d. The impact of reasoning step
length on large language models. In Findings of
the Association for Computational Linguistics, ACL
2024, Bangkok, Thailand and virtual meeting, Au-
gust 11-16, 2024 , pages 1830–1842. Association for
Computational Linguistics.
Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and
Weidi Xie. 2022a. Prompting visual-language mod-
els for efficient video understanding. In Computer
Vision - ECCV 2022 - 17th European Conference, Tel
Aviv, Israel, October 23-27, 2022, Proceedings, Part
XXXV , volume 13695 of Lecture Notes in Computer
Science , pages 105–124. Springer.
Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and
Weidi Xie. 2022b. Prompting visual-language mod-
els for efficient video understanding. In Computer
Vision - ECCV 2022 - 17th European Conference, Tel
Aviv, Israel, October 23-27, 2022, Proceedings, Part
XXXV , volume 13695 of Lecture Notes in Computer
Science , pages 105–124. Springer.
Muhammad Uzair Khattak, Hanoona Abdul Rasheed,
Muhammad Maaz, Salman H. Khan, and Fahad Shah-
baz Khan. 2023. Maple: Multi-modal prompt learn-
ing. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition, CVPR 2023, Vancouver,
BC, Canada, June 17-24, 2023 , pages 19113–19122.
IEEE.
Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learn-
ing multiple layers of features from tiny images.
pages 32–33.
Thibault Laugel, Marie-Jeanne Lesot, Christophe
Marsala, Xavier Renard, and Marcin Detyniecki.
2019. The dangers of post-hoc interpretability: Un-
justified counterfactual explanations. In Proceedings
of the Twenty-Eighth International Joint Conference
on Artificial Intelligence, IJCAI 2019, Macao, China,
August 10-16, 2019 , pages 2801–2807. ijcai.org.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,EMNLP 2021, Virtual Event / Punta Cana, Domini-
can Republic, 7-11 November, 2021 , pages 3045–
3059. Association for Computational Linguistics.
Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,
Linjie Li, Lijuan Wang, and Jianfeng Gao. 2024a.
Multimodal foundation models: From specialists to
general-purpose assistants. Found. Trends Comput.
Graph. Vis. , 16(1-2):1–214.
Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei
Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang,
Hanwang Zhang, and Yueting Zhuang. 2024b. Fine-
tuning multimodal llms to follow zero-shot demon-
strative instructions. Preprint , arXiv:2308.04152.
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.
Hoi. 2022. BLIP: bootstrapping language-image pre-
training for unified vision-language understanding
and generation. In International Conference on Ma-
chine Learning, ICML 2022, 17-23 July 2022, Balti-
more, Maryland, USA , volume 162 of Proceedings
of Machine Learning Research , pages 12888–12900.
PMLR.
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,
Wayne Xin Zhao, and Ji-Rong Wen. 2023. Eval-
uating object hallucination in large vision-language
models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2023, Singapore, December 6-10, 2023 ,
pages 292–305. Association for Computational Lin-
guistics.
Sheng Liang, Mengjie Zhao, and Hinrich Schütze. 2022.
Modular and parameter-efficient multimodal fusion
with prompting. In Findings of the Association for
Computational Linguistics: ACL 2022, Dublin, Ire-
land, May 22-27, 2022 , pages 2976–2985. Associa-
tion for Computational Linguistics.
Fangyu Liu, Guy Emerson, and Nigel Collier. 2023a.
Visual spatial reasoning. Transactions of the Associ-
ation for Computational Linguistics , 11:635–651.
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2023b. Improved baselines with visual instruc-
tion tuning. CoRR , abs/2310.03744.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023c. Visual instruction tuning. In Advances
in Neural Information Processing Systems 36: An-
nual Conference on Neural Information Processing
Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
December 10 - 16, 2023 .
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023d. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Comput. Surv. , 55(9):195:1–195:35.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin
Yang, and Jie Tang. 2021. P-tuning v2: Prompt
tuning can be comparable to fine-tuning universally
across scales and tasks. CoRR , abs/2110.07602.Fang Ma, Chen Zhang, Lei Ren, Jingang Wang, Qi-
fan Wang, Wei Wu, Xiaojun Quan, and Dawei Song.
2022. Xprompt: Exploring the extreme of prompt
tuning. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2022, Abu Dhabi, United Arab Emirates, De-
cember 7-11, 2022 , pages 11033–11047. Association
for Computational Linguistics.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul F. Christiano, Jan Leike, and Ryan Lowe.
2022. Training language models to follow instruc-
tions with human feedback. In Advances in Neural
Information Processing Systems 35: Annual Confer-
ence on Neural Information Processing Systems 2022,
NeurIPS 2022, New Orleans, LA, USA, November 28
- December 9, 2022 .
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Köpf, Edward Z.
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch: An
imperative style, high-performance deep learning li-
brary. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, De-
cember 8-14, 2019, Vancouver, BC, Canada , pages
8024–8035.
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,
Ning Dai, and Xuanjing Huang. 2020. Pre-trained
models for natural language processing: A survey.
CoRR , abs/2003.08271.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021. Learn-
ing transferable visual models from natural language
supervision. In Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24
July 2021, Virtual Event , volume 139 of Proceedings
of Machine Learning Research , pages 8748–8763.
PMLR.
Cynthia Rudin. 2019. Stop explaining black box ma-
chine learning models for high stakes decisions and
use interpretable models instead. Nat. Mach. Intell. ,
1(5):206–215.
Ying Shen, Zhiyang Xu, Qifan Wang, Yu Cheng, Wen-
peng Yin, and Lifu Huang. 2024. Multimodal instruc-
tion tuning with conditional mixture of lora. CoRR ,
abs/2402.15896.
Amanpreet Singh, Vivek Natarajan, Meet Shah,
Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and
Marcus Rohrbach. 2019. Towards VQA models thatcan read. In IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2019, Long Beach,
CA, USA, June 16-20, 2019 , pages 8317–8326. Com-
puter Vision Foundation / IEEE.
Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-
Long Wang, Siqi Ma, Qifan Wang, Ying Nian
Wu, Yongfeng Zhang, and Dongfang Liu. 2024a.
Visual agents as fast and slow thinkers. CoRR ,
abs/2408.08862.
Mingjie Sun, Xinlei Chen, J. Zico Kolter, and Zhuang
Liu. 2024b. Massive activations in large language
models. CoRR , abs/2402.17762.
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and
Yue Cao. 2023. EV A-CLIP: improved training tech-
niques for CLIP at scale. CoRR , abs/2303.15389.
Qifan Wang, Yuning Mao, Jingang Wang, Hanchao
Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu
Huang, Xiaojun Quan, Zenglin Xu, and Dongfang
Liu. 2023a. Aprompt: Attention prompt tuning for
efficient adaptation of pre-trained language models.
InProceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2023, Singapore, December 6-10, 2023 , pages 9147–
9160. Association for Computational Linguistics.
Wenguan Wang, Cheng Han, Tianfei Zhou, and Dong-
fang Liu. 2023b. Visual recognition with deep near-
est centroids. In The Eleventh International Con-
ference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023 . OpenReview.net.
Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi
Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong
Su, Vincent Perot, Jennifer G. Dy, and Tomas Pfister.
2022. Dualprompt: Complementary prompting for
rehearsal-free continual learning. In Computer Vision
- ECCV 2022 - 17th European Conference, Tel Aviv,
Israel, October 23-27, 2022, Proceedings, Part XXVI ,
volume 13686 of Lecture Notes in Computer Science ,
pages 631–648. Springer.
Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, and
Rongrong Ji. 2024a. Not all attention is needed:
Parameter and computation efficient transfer learn-
ing for multi-modal large language models. CoRR ,
abs/2403.15226.
Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu, and Yunfang
Wu. 2024b. Mixture-of-prompt-experts for multi-
modal semantic understanding. In Proceedings of
the 2024 Joint International Conference on Computa-
tional Linguistics, Language Resources and Evalua-
tion, LREC/COLING 2024, 20-25 May, 2024, Torino,
Italy, pages 11381–11393. ELRA and ICCL.
Ning Xie, Farley Lai, Derek Doran, and Asim Ka-
dav. 2019. Visual entailment: A novel task
for fine-grained image understanding. CoRR ,
abs/1901.06706.
Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See-
Kiong Ng, and Jiashi Feng. 2024a. Pllava :Parameter-free llava extension from images to videos
for video dense captioning. CoRR , abs/2404.16994.
Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby,
Ying Shen, Di Jin, Yu Cheng, Qifan Wang, and
Lifu Huang. 2024b. Vision-flan: Scaling human-
labeled tasks in visual instruction tuning. CoRR ,
abs/2402.11690.
Zhiyang Xu, Ying Shen, and Lifu Huang. 2023. Multiin-
struct: Improving multi-modal zero-shot learning via
instruction tuning. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), ACL 2023, Toronto,
Canada, July 9-14, 2023 , pages 11445–11465. Asso-
ciation for Computational Linguistics.
Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, and
Qifan Wang. 2023. Prompt learns prompt: Exploring
knowledge-aware generative prompt collaboration
for video captioning. In Proceedings of the Thirty-
Second International Joint Conference on Artificial
Intelligence, IJCAI 2023, 19th-25th August 2023,
Macao, SAR, China , pages 1622–1630. ijcai.org.
Hao Yang, Junyang Lin, An Yang, Peng Wang, and
Chang Zhou. 2023a. Prompt tuning for unified mul-
timodal pretrained models. In Findings of the As-
sociation for Computational Linguistics: ACL 2023,
Toronto, Canada, July 9-14, 2023 , pages 402–416.
Association for Computational Linguistics.
Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan,
Fuli Feng, Yu Chen, Madian Khabsa, Sinong Wang,
Zenglin Xu, and Dongfang Liu. 2023b. Mixpave:
Mix-prompt tuning for few-shot product attribute
value extraction. In Findings of the Association
for Computational Linguistics: ACL 2023, Toronto,
Canada, July 9-14, 2023 , pages 9978–9991. Associa-
tion for Computational Linguistics.
Hantao Yao, Rui Zhang, and Changsheng Xu. 2023.
Visual-language prompt tuning with knowledge-
guided context optimization. In IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition,
CVPR 2023, Vancouver, BC, Canada, June 17-24,
2023 , pages 6757–6767. IEEE.
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing
Sun, Tong Xu, and Enhong Chen. 2023. A sur-
vey on multimodal large language models. CoRR ,
abs/2306.13549.
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing
Qu, Yong Jae Lee, and Yi Ma. 2023. Investigating the
catastrophic forgetting in multimodal large language
models. CoRR , abs/2309.10313.
Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou
Zhang, Lingyao Li, Zhenting Wang, Wenyue Hua,
Dong Shu, Suiyuan Zhu, Xiaobo Jin, Sujian Li,
Mengnan Du, and Yongfeng Zhang. 2024. When AI
meets finance (stockagent): Large language model-
based stock trading in simulated real-world environ-
ments. CoRR , abs/2407.18957.Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,
Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-
wei Zhang, Fei Wu, and Guoyin Wang. 2023. In-
struction tuning for large language models: A survey.
CoRR , abs/2308.10792.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-
ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,
Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao
Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang
Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
2023. A survey of large language models. CoRR ,
abs/2303.18223.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena. In
Advances in Neural Information Processing Systems
36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023 .
Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and
Ziwei Liu. 2022a. Conditional prompt learning
for vision-language models. In IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition
(CVPR) .
Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and
Ziwei Liu. 2022b. Learning to prompt for vision-
language models. International Journal of Computer
Vision (IJCV) .SUMMARY OF THE APPENDIX
This appendix contains additional experimental
results and discussions of our work, organized as:
• §S1 includes additional introduction on datasets
applied in our paper.
•§S2 provides more training details on our pro-
posed M2PT.
•§S3 presents more details on the evaluation met-
rics applied in §4.
• §S4 further includes more case studies of M2PT
on its perceptual proficiency.
•§S5 discuss the relations of M2PT with previous
works and their connections.
•§S6 provides discussions and additional results
on the impact of prompt initialization.
•§S7 provides discussions on licenses, repro-
ducibility, social impact, and directions of our
future work.
S1 Data Statistics
Table S1 shows details of 9 multimodal datasets
for our finetuning and evaluation. Vision-Flan (Xu
et al., 2024b) includes 191 different multimodal
tasks which is ideal for our finetuning process.
Each multimodal tasks contains up to 1,000 in-
stances, resulting in a total of 191,105 instances.
MME (Yin et al., 2023) serves as our comprehen-
sive multimodal evaluation benchmark, measuring
both perception and cognition capabilities across
14 subtasks. We further leverage 7 multimodal
datasets for our evaluation. Specifically, for Op-
tical Character Recognition, we utilize the Text-
VQA (Singh et al., 2019), and for reasoning, we em-
ploy the Visual Spatial Reasoning ( VSR) (Liu et al.,
2023a). Following (Zhai et al., 2023; Shen et al.,
2024), the perception capability is tested on CIFAR-
10/100 (Krizhevsky et al., 2009) and MNIST (Deng,
2012). SNLI-VE (Xie et al., 2019) evaluates Visual
Entailment capabilities, while the POPE (Li et al.,
2023) dataset examines the tendency towards ob-
ject hallucination. The MME metric is the sum of
accuracy values across all subtasks, while for the
other 7 multimodal evaluation datasets, the metric
used is just accuracy.
S2 Implementation Details
Stage-one LLaV A (Liu et al., 2023c) is utilized as
our pre-trained multimodal model. Specifically, weTable S1: Multimodal Dataset Details
Dataset Examples Task Categories
Vision-Flan 191K Diverse
MME 2374 Diverse
Text-VQA 5000 OCR
VSR 1222 Reasoning
SNLI-VE 17901 Entailment
CIFAR-10 10000 Perception
CIFAR-100 10000 Perception
MNIST 10000 Perception
POPE 9000 Object Hallucination
employ LLaV A with Vicuna-7B-v1.3 as the base
LLM and CLIP-L as the vision encoder for all vari-
ants. The finetuning process for M2PT10/20takes
approximately 9 hours on 4 A100 GPUs (40G),
with a batch size of 8 per GPU and a gradient ac-
cumulation step of 4 (128 in total), with the same
data preprocess and normalize method as LLaV A.
Additional configurations of M2PT are shown in
Table S2. For LoRA, we directly import the best
results from (Shen et al., 2024). For the prompt
tuning baselines, APrompt (Wang et al., 2023a) and
PTUM (Yang et al., 2023a) add textual/attention
prompts into the LLM, while VPT (Han et al.,
2024b) only appends visual prompts to the vision
encoder. We use the optimal settings in the original
papers to train their models, with grid search on the
best learning rate. For the other configuration, we
adopt LLaV A’s (Liu et al., 2023c) default settings
as provided in its codebase.
Table S2: Hyperparameters and configurations.
Learning Rate 7e−4
Batch Size 128
Lr Scheduler cosine
Warmup Ratio 0.03
Activation Type bfloat16
Weight Decay 0
Model Max Length 1024
S3 Evaluation Metrics
For evluation, we utilize MME (Yin et al., 2023)
and the other 7 multimodal datasets (see §S1). For
MME, we employ the official evaluation tool (Yin
et al., 2023) of MME, including the Perception
and Cognition metrics. For the other 7 multimodal
datasets, following (Shen et al., 2024), we employ
the same prompt template to guide Vicuna-13B-
v1.5 (Zheng et al., 2023) in evaluating the accuracyof each prediction, considering the specified task
instructions and the groundtruth target output. All
tasks are classification tasks and we calculated the
final score of each multimodal dataset based on the
percentage of vicuna 13b answering “Yes.”
S4 More Case Study
To further investigate the model’s performance and
delineate instances of its suboptimal functioning,
we conduct an in-depth visual assessment of a
sample cohort drawn from eight distinct Zero-Shot
datasets, as illustrated in Fig. S1. This visualization
facilitates a comprehensive understanding of the
model’s efficacy across a diverse array of tasks and
data, while concurrently revealing potential con-
straints inherent to the model and the underlying
causes of its occasional shortcomings. From the
listed failure cases, we summarize 2 failure pat-
terns, which are: (a) Small objects perception fail-
ure in Text-VQA ,CIFAR-10 ,MNIST . Small targets
in images pose a challenge to perception, impacting
the quality of VQA. Further research is essential to
enhance accuracy in diverse contexts. (b) Semantic
similar failure in CIFAR-10 ,MNIST ,POPE , the
inability to distinguish between semantically sim-
ilar objects results in MLLMs generating wrong
answers. Developing methods that can effectively
differentiate between similar objects is essential for
real-world applications. This involves enhancing
the model’s capacity to learn fine-grained features
and contextual information, thereby improving its
overall accuracy and robustness.
S5 Discussion with Previous Works
This section provides discussion that connects
M2PT with previous methods. If we remove the
textual prompts and the interaction layer, our model
architecture degenerates to the visual prompt tun-
ing approaches (Jia et al., 2022; Han et al., 2024b,
2023). If we completely freeze the vision encoder
by only introducing the textual prompts, our model
is similar to those traditional prompt tuning meth-
ods (Lester et al., 2021; Ma et al., 2022; Yang et al.,
2023a) in LLM. Moreover, if we further incorpo-
rate the attention prompts in the LLM, our model is
close to the APrompt approach (Wang et al., 2023a;
Han et al., 2023). Nevertheless, very limited work
focuses on the efficient tuning of multimodal large
language models.Table S3: Initialization Comparison. We compare
the performance of M2PT under different initialization
methods in the setting of best prompt combination ( i.e.,
10 and 20)
Method Initialization MME
M2PT10/20 Random 1405.67
M2PT10/20 Xavier 1503.98
S6 Prompt Initialization
Table S3 reports the performance of M2PT with
respect to x widely adopted initialization methods:
Xavier (Glorot and Bengio, 2010) and random on
MME . The results show that Xavier generally pro-
vides more stable and preferable performances. In
conclusion, M2PT shows robustness on different
initialization methods and is able to achieve com-
parable performance with full finetuning.
S7 Discussion
S7.1 Asset License and Consent
The majority of VPT (Jia et al., 2022), Text-VQA ,
is licensed under CC-BY-NC 4.0. Portions of (Jia
et al., 2022) are available under separate license
terms: google-research/task_adaptation, hugging-
face/transformers, LLaV A, Vicuna, VSR is licensed
under Apache-2.0. ViT-pytorch (Dosovitskiy et al.,
2021) and POPE is licensed under MIT; SNLI-VE
are under BSD 3-Clause
S7.2 Reproducibility
M2PT is implemented in Pytorch (Paszke et al.,
2019). Experiments are conducted on NVIDIA
A100 GPUs. To guarantee reproducibility, our full
implementation shall be publicly released upon
paper acceptance.
S7.3 Social Impact
This work introduces M2PT possessing strong per-
formance gains over state-of-the-art baselines in §4,
with considerably low parameter usage for MLLMs.
Our approach advances model accuracy, and is
valuable in parameter-sensitive training applica-
tions, e.g., MLLMs on devices and fast adaptation
with limited resources.
S7.4 Potential Risks
Consider the tuning process of LLM, which has po-
tential risks for energy usage. Finetuning requires
significant computational power, leading to high
energy use and increased environmental impact.Text-VQA:MME：
Question: Is a c++ code shown in the 
picture? Please answer yes or no.
Answer: Yes
Groundtruth:   Yes 
Question: What brand of soda is this?
Answer:   rolex
Groundtruth:     rolex
Text-VQA:MME：
Question:Is this an image of Beijing 
Guozijian? Please answer yes or no.
Answer: No
Groundtruth:   Yes
Question: What is the number above the 
windshield on the yellow tractor on the left?
Answer:   6
Groundtruth:     a401 rsl
Text-VQA:VSR：
Question: In this task, you need to decide 
if the positional relationship in the text is true.  
The suitcase contains the cat.
Options: (a) yes (b) no
Answer: (a) yes
Groundtruth:   (a) yes
Question: What brand of soda is this?
Answer:   Pepsi
Groundtruth:     Dr pepper
Text-VQA:VSR：
Question:The text is about the spatial 
relationship between two objects in the image. 
TextThe car is in front of the cat.
Options: (a) yes (b)
Answer: (a) yes
Groundtruth:   (a) no
Question: What brand of soda is this?
Answer:   Pepsi
Groundtruth:     Dr pepper
CIFAR-100：CIFAR-10：
Question: What is the object in the image? 
Please only answer a single object in apple, 
baby,... , willow_tree, wolf, woman, worm.
Answer:   mushroom
Groundtruth:     mushroomCIFAR-100：CIFAR-10：
Question: What is the object in the image? 
Please only answer a single object in airplane,  
bird, cat, deer, ..., horse, ship, truck.
Answer: airplane
Groundtruth:   truck
Question: What is the object in the image? 
Please only answer a single object in apple, 
baby,... , willow_tree, wolf, woman, worm.
Answer:   apple
Groundtruth:     redberry
POPE:MNIST:
Question: What is the number in the image? 
Please only answer a single number in 0, 1, 2, 3, 
4, 5, 6, 7, 8, 9.
Answer: 6
Groundtruth:   6
Question: Is there a banana in the image?
Answer:   Yes
Groundtruth:     YesPOPEMNIST:
Question: What is the number in the image? 
Please only answer a single number in 0, 1, 2, 3, 
4, 5, 6, 7, 8, 9.
Answer: 2
Groundtruth:   7
Question: Is there a truck in the image?
Answer:   Yes
Groundtruth:     No
SNLI-VE：
Question: Folk dancers are entertaining the 
streets during a parade. (a) contradiction (b) 
neutral (c) entailment. Which relationship is 
correct?
Answer: (b) neutral 
Groundtruth:   (b) neutral SNLI-VE：
Question:The soccer playing was on the 
sideline.
(a) contradiction (b) neutral (c) entailment. 
Which relationship is correct?
Answer: (b) neutral 
Groundtruth:   (c) entailment
Question: What is the object in the image? 
Please only answer a single object in airplane,  
bird, cat, deer, ..., horse, ship, truck.
Answer: airplane
Groundtruth:   airplane
Figure S1: More Case study on 8 Zero-Shot datasets (M2PT10/20).
S7.5 Future Work
Despite M2PT’s systemic effectiveness and effi-
cacy during instruction tuning, it also comes withnew challenges and unveils some intriguing ques-
tions. For instance, incorporating an advanced net-
work into M2PT to search the optimal combina-
tions of prompt lengths ( i.e., Visual Prompt, Tex-tual Prompt) might significantly reduce the search
space of lengths and lead to further performance
gains. Another essential future direction is the de-
sign and analysis of network interpretability (Arri-
eta et al., 2020; Laugel et al., 2019; Rudin, 2019)
andad-hoc explainability (Biehl et al., 2016; Wang
et al., 2023b), which limits current adoption of
M2PT in decision-critical, real-world applications.
Overall, we believe the results presented in this
paper warrant further exploration.