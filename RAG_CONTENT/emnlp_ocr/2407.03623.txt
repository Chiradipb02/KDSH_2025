Resampled Datasets Are Not Enough:
Mitigating Societal Bias Beyond Single Attributes
Yusuke Hirota1∗Jerone T. A. Andrews2Dora Zhao3∗Orestis Papakyriakopoulos4∗
Apostolos Modas2Yuta Nakashima1Alice Xiang2
1Osaka University2Sony AI3Stanford University4Technical University of Munich
Abstract
We tackle societal bias in image-text datasets
by removing spurious correlations between pro-
tected groups and image attributes. Traditional
methods only target labeled attributes, ignoring
biases from unlabeled ones. Using text-guided
inpainting models, our approach ensures pro-
tected group independence from all attributes
and mitigates inpainting biases through data
filtering. Evaluations on multi-label image clas-
sification and image captioning tasks show our
method effectively reduces bias without com-
promising performance across various models.
1 Introduction
Models trained on biased data can develop pre-
diction rules based on spurious correlations (i.e.,
associations devoid of causal relationships), perpet-
uating and amplifying harmful stereotypes (Zhao
et al., 2017). For example, image captioning mod-
els may generate gendered captions by associat-
ing gender with depicted activities (Zhao et al.,
2023), location (Hendricks et al., 2018), or ob-
jects (Wang and Russakovsky, 2021). Dataset-level
bias mitigation aims to reduce spurious correla-
tions between labeled image attributes (e.g., teddy
bear ) and protected groups (e.g., woman ). Resam-
pling approaches balance the co-occurrence of each
attribute with each group (Agarwal et al., 2022;
Wang et al., 2020b). However, models can still
exploit correlations between groups and sets of at-
tributes (e.g., manwith{dog,pizza ,couch}), even
when individual attributes are balanced (Zhao et al.,
2023). Moreover, spurious correlations extend to
unlabeled attributes, which current strategies do
not address—e.g., gender disparities in image color
statistics (Meister et al., 2023) or the person-to-
object spatial distances (Wang et al., 2020a).
*Work done at Sony AI. Correspondence to Yusuke Hirota:
y-hirota@is.ids.osaka-u.ac.jp .While equal group distributions in real-world
datasets are challenging to achieve, generative text-
to-image models now enable targeted image modifi-
cations (Rombach et al., 2022; Brooks et al., 2023;
Couairon et al., 2023). For example, bias detection
methods alter image subjects’ appearance to assess
counterfactual fairness (Joo and Kärkkäinen, 2020)
or model bias (Smith et al., 2023; Brinkmann et al.,
2023). However, manipulating individuals’ ap-
pearances without consent raises significant ethical
and privacy concerns (Andrews et al., 2023; Yew
and Xiang, 2022; Sobel, 2020; Ramaswamy et al.,
2021a; Orekondy et al., 2018; Oh et al., 2016).
To address these challenges, we create train-
ing datasets with text-guided inpainting (Rombach
et al., 2022), ensuring attribute distributions are
independent of protected groups. Using masked
person images and text prompts, we generate coun-
terfactual images by inpainting only the masked
regions, addressing ethical concerns of altering non-
consensual persons and ensuring equal representa-
tion of protected groups across attributes. We intro-
duce data filters to mitigate biases from generative
text-guided inpainting models (Bianchi et al., 2023;
Cho et al., 2023; Bansal et al., 2022; Luccioni et al.,
2023), evaluating images based on adherence to
prompts, preservation of attributes and semantics,
and color fidelity, validated by human evaluators.
Unlike prior work (Wang et al., 2019, 2020b; Zhao
et al., 2023; Agarwal et al., 2022), training on our
counterfactual data decorrelates both labeled and
unlabeled attributes from protected groups with-
out impacting model performance. Comprehen-
sive evaluations show our approach significantly
reduces prediction rules based on spurious corre-
lations in multi-label classification and image cap-
tioning across various architectures (e.g., ResNet-
50 (He et al., 2016), Swin Transformer (Liu et al.,
2021)), datasets (COCO (Lin et al., 2014), Open-
Images (Krasin et al., 2017)), and protected groups
(gender, skin tone). Our key contributions are sum-arXiv:2407.03623v2  [cs.CV]  11 Jul 2024person, bench, handbag , cell phone 
person, bench, handbag , cell phone 
person, bench, cell phone baseline 
over-sampling 
ours 
person, tie, surfboard 
person, tie, surfboard 
person, surfboard baseline 
over-sampling 
ours A man riding a skateboard baseline 
A man jumping with a skateboard 
A man jumping with a frisbee LIBRA 
ours 
A man riding a motorcycle baseline 
A man on a motorcycle on the road 
A person working on a motorcycle LIBRA 
ours 
(a) Multi-label classification (b) Image captioning 
Figure 1: (a) Predicted objects by baseline ResNet-50 and with bias mitigation, i.e., over-sampling (Wang et al.,
2020b) versus our method. (b) Generated captions by baseline ClipCap and with bias mitigation, i.e., LIBRA (Hirota
et al., 2023) versus our method. Incorrect predictions, possibly affected by gender-object correlations, are in red.
marized as follows:
•Introducing a framework for generating syn-
thetic training datasets with group-independent
image attribute distributions.
•Proposing data filtering to mitigate biases intro-
duced by generative inpainting models.
•Conducting quantitative experiments, demon-
strating significant bias reduction in classifica-
tion and captioning tasks compared to baselines.
•Identifying limitations of training on combined
real and synthetic datasets, emphasizing the
need for cautious synthetic data augmentation.
1.1 Related Work
Societal bias in datasets, characterized by demo-
graphic imbalances leading to spurious correla-
tions, has been extensively studied (DeVries et al.,
2019; Birhane et al., 2024; Birhane and Prabhu,
2021; Birhane et al., 2021; Wang et al., 2020a;
Meister et al., 2023). These biases persist and
can be exacerbated by multi-label classifiers (Zhao
et al., 2017; de Vries et al., 2019; Wang et al., 2019)
and image captioning models (Zhao et al., 2021;
Hendricks et al., 2018; Hirota et al., 2022), dispro-
portionately impacting historically marginalized
groups such as women and individuals with darker
skin tones (Garcia et al., 2023; Ross et al., 2020).
Two common approaches to bias mitigation are
dataset-level and model-level. Dataset-level ap-
proaches leverage generative adversarial networks
(GANs), counterfactual training dataset augmen-
tation, and resampling. GANs create synthetic
images to balance datasets and mitigate spurious
correlations (Ramaswamy et al., 2021b; Sattigeriet al., 2019; Sharmanska et al., 2020), counterfac-
tual data augmentation generates alternative sce-
narios to address biases (Kaushik et al., 2019;
Wang and Culotta, 2021), and resampling bal-
ances the co-occurrence of attributes and pro-
tected groups (Agarwal et al., 2022; Wang et al.,
2020b). Model-level approaches reduce bias
through corpus-level constraints (Zhao et al., 2017),
adversarial debiasing (Wang et al., 2019; Hendricks
et al., 2018; Tang et al., 2021; Alvi et al., 2018),
domain discriminative/independent training (Wang
et al., 2020b), modified loss functions (Lin et al.,
2017; Cui et al., 2019; Sagawa et al., 2019), and
model output editing (Hirota et al., 2023). How-
ever, despite these advancements, existing miti-
gation methods focus on single labeled attributes,
which can inadvertently increase models’ reliance
on spurious correlations between protected groups
and combinations of attributes (Zhao et al., 2023)
or unlabeled attributes (Meister et al., 2023).
Recent progress in text-to-image generative mod-
els has enabled targeted image manipulation (Rom-
bach et al., 2022; Brooks et al., 2023; Couairon
et al., 2023), which can help address bias in multi-
modal datasets. Nonetheless, these models have
also been shown to perpetuate harmful stereo-
types (Mandal et al., 2023; Zhang et al., 2023;
Wang et al., 2023a; Struppek et al., 2022; Ungless
et al., 2023; Naik and Nushi, 2023; Seshadri et al.,
2023; Friedrich et al., 2023). In contrast to prior
bias mitigation work, we use text-guided inpainting
to generate synthetic training datasets that ensure
equal representation of protected groups across all
attribute combinations, whether labeled or unla-beled. To mitigate inpainting biases, we propose
data filters, producing higher quality and less bi-
ased synthetic data. We go beyond previous work
focused solely on gender bias mitigation (Joo and
Kärkkäinen, 2020; Smith et al., 2023; Brinkmann
et al., 2023) by also addressing skin tone biases.
2 Method
We create training datasets with group-independent
image attribute distributions by using masked per-
son images and text prompts with an off-the-shelf
diffusion model, as outlined in Figure 2.
2.1 Resampled Datasets Are Not Enough
We denote an image by x∈ X, a protected group
byg∈ G, and an image attribute by a∈ A . A
spurious correlation exists if pX(a|g)̸=pX(a),
indicating biases in the data. Resampling aims to
remove these biases by adjusting the sampling pro-
cess so that pX(a|g) =pX(a)for all g(de Vries
et al., 2019; Wang et al., 2020b). This is done
using a limited set of labeled attributes O ⊂ A ,
where attributes aare drawn from a distribution
q(a)overOand groups gare drawn from a uni-
form distribution u(g)overGsuch that X′={x∼
pX(x|g, a)|a∼q(a), g∼u(g)}. This ensures
pX′(a|g) =q(a)fora∈ O andg∈ G. However,
this method has a limitation: it does not account for
abeing an unlabeled attribute or a combination of
labeled and unlabeled attributes, making it difficult
to sample xfrom pX(x|g, a)due to insufficient
information about a. In short, while resampling can
reduce biases, it is not always enough, especially
when dealing with unlabeled or mixed attributes.
2.2 Text-Guided Inpainting
Suppose D={(xi, ωi, ai, t(g)
i)|1≤i≤n}is a
training set, where x∈Rdis an image, ω∈[0,1]d
is a person mask, ais a labeled image attribute,
a combination of labeled attributes, or an unla-
beled attribute, and t(g)is a text prompt contain-
ing a protected group-specific word g. To create
a dataset with group-independent image attribute
distributions, we utilize a text-guided inpainting
model (Rombach et al., 2022). This model, guided
byt(g), inpaints ωinxwith a synthetic person
from protected group gdescribed in t(g). For each
tuple in D, we generate m∈N+versions for each
g∈ G, resulting in m· |G| samples:
Dsynthetic ={(x(j,g′)
i, ωi, ai, t(g′)
i)
|1≤i≤n, g′∈ G,1≤j≤m},(1)where x(j,g′)
i denotes the j-th inpainted version of
xi∈ X forg′andt(g′)
ithe modified text prompt
where gint(g)
iis replaced with g′.
2.3 Societal Bias Data Filtering
Text-to-image generative models often perpetuate
societal biases, portraying certain groups stereotyp-
ically, such as depicting women in brighter cloth-
ing (Bianchi et al., 2023; Cho et al., 2023; Bansal
et al., 2022; Luccioni et al., 2023). Since these bi-
ases remain largely unaddressed (Smith et al., 2023;
Brinkmann et al., 2023), we set m > 1in Equa-
tion (1) to generate multiple variations for each
group. We propose filters to select the least biased
inpainted images, evaluating images based on ad-
herence to text prompts, preservation of attributes
and semantics, and color fidelity. Specifically, for
each tuple (i, g′), we select the highest quality and
least biased version among the mversions to create
a training dataset:
Ssynthetic ={(x(j⋆,g′)
i , ωi, ai, t(g′)
i)∈ D synthetic
| ∀(i, g′), j⋆},
(2)
where j⋆= arg minjP
kck·r(s(i,j,g′)
k),ck∈R
are weights assigned to filters sk,s(i,j,g′)
kis the
score obtained from applying filter skto image
x(j,g′)
i for group g′, and r(s(i,j,g′)
k)is the rank of
the score for (i, g′)in descending order, with lower
ranks indicating less bias. Here, x(j⋆,g′)
i is the se-
lected inpainted image for tuple (i, g′)that mini-
mizes the sum of the ranks of the weighted filter
scores, with j⋆representing the index of the se-
lected candidate image for tuple (i, g′).
Rather than creating an entire dataset of syn-
thetic samples, we can augment D:
Saugment =D ∪ { (x(j⋆,g′)
i , ωi, ai, t(g′)
i)∈ D synthetic
| ∀(i, g′̸=g), j⋆}.
(3)
The condition g′̸=gensures that we only add in-
painted images to Dfor groups different from those
originally present in xi. In contrast to resampling,
Ssynthetic andSaugment ensure pX′(a|g) =pX(a)
for all g∈ Gwithout making assumptions about A.
Our proposed filters are introduced below.
Prompt Adherence. To evaluate the seman-
tic alignment between x(j,g′)
i andt(g′)
i, we useManwithglasses
Womanwithglasses!!"#!images!$%!"#"⋮⋮"!"#"$%!"#Filtering&Ranking
Selected image
$&'#(ℎ*(+,Createdataset
Prompt adherenceObject consistencyColor fidelity
Woman withglassesCLIPSCore: 0.8
glasses,tie, …glasses,tie, …F1: 0.9MSE: 0.2$"-.!*#(
Inpainting
Inpainting
FrobeniusNorm: 0.2Figure 2: Overview of our pipeline for binary gender as a protected attribute. Original images are inpainted to
synthesize diverse groups, maintaining consistent context. Synthesized images (highlighted in blue) are ranked
using filters to select high-quality, unbiased samples (Module: Filtering & Ranking). Selected images are then used
to construct datasets with group-independent image attribute distributions (Module: Create dataset).
CLIPScore (Hessel et al., 2021), which computes
the cosine similarity between their CLIP embed-
dings (Radford et al., 2021). Formally,
s(i,j,g′)
prompt =ϕ(x(j,g′)
i)·ψ(t(g′)
i)∈[−1,1],(4)
where ϕandψare CLIP’s vision and text encoders,
respectively. If s(i,j,g′)
prompt > s(i,j′,g′)
prompt , then x(j,g′)
i bet-
ter reflects the content described in t(g′)
i.
Object Consistency. To prevent the introduction
of spurious correlations, such as generating ob-
jects not mentioned in t(g′)
ior reinforcing stereo-
types (Bianchi et al., 2023; Cho et al., 2023; Bansal
et al., 2022), we assess the object similarity be-
tween predicted objects in x(j,g′)
i andxi. Con-
cretely, we compute the F1 score (Sokolova et al.,
2006) using a pretrained object detector (Zhou
et al., 2022), denoted η:
s(i,j,g′)
object=F1[η(x(j,g′)
i), η(xi)]∈[0,1].(5)
Ifs(i,j,g′)
object> s(i,j′,g′)
object, then x(j,g′)
i better preserves
the integrity of the original unmasked scene in xi.
Color Fidelity. Generative models can introduce
subtler biases (Bansal et al., 2022; Bianchi et al.,
2023), including those related to color (Meisteret al., 2023). Addressing color biases is crucial
as color choices can implicitly carry cultural or
gendered connotations. To mitigate this, we down-
sample x(j,g′)
i andxito14×14pixels to focus
on color rather than fine details, then measure the
color difference using the Frobenius norm:
s(i,j,g′)
color=∥(x(j,g′)
i)↓14×14−(xi)↓14×14∥−1
F.(6)
Ifs(i,j,g′)
color> s(i,j′,g′)
color, then x(j,g′)
i has better color
fidelity to the original unmasked scene in xi.
3 Experiments
Building on prior research (Zhao et al., 2017; Wang
et al., 2019; Zhao et al., 2023; Hendricks et al.,
2018; Zhao et al., 2021; Tang et al., 2021), we
evaluate our synthetic dataset creation method on
multi-label image classification and image caption-
ing tasks using quantitative metrics, human studies,
qualitative comparisons, and effectiveness analysis.
Evaluations are conducted on test sets of real data.
Implementation Details. We inpaint the largest
person in the image based on bounding box size,
and if the second largest person exceeds 55,000 pix-
els, we also inpaint that region, using the person
label for COCO. For image generation, we create
m=30inpainted images per group (e.g., woman ,ResNet-50 Swin-T ConvNeXt-B
mAP Ratio Leakage mAP Ratio Leakage mAP Ratio Leakage
Original 66.4 6.3 13.4 72.8 4.0 14.3 76.3 4.6 18.2
Adversarial 63.3 — 3.3 67.8 — 4.4 69.6 — 4.7
DomDisc 57.4 4.1 15.4 65.4 4.6 16.8 68.8 4.5 19.1
DomInd 60.4 2.8 10.4 67.9 3.8 11.4 72.6 5.9 15.0
Upweight 64.9 9.1 8.3 71.5 6.3 9.8 75.0 5.6 12.9
Focal 66.1 6.3 12.0 72.2 3.8 13.3 76.2 3.8 16.2
CB 63.0 4.3 10.9 69.6 3.5 12.3 73.8 3.5 14.7
GroupDRO 64.1 3.0 11.4 70.8 1.5 12.6 75.3 4.2 16.4
Over-sampling 62.6 3.8 9.7 69.9 2.6 10.5 73.5 3.4 13.7
Sub-sampling 58.3 2.0 12.2 64.4 1.8 11.6 66.3 2.2 18.2
Saugment (Ours) 66.9 4.6 8.1 72.8 3.1 10.5 76.3 2.2 11.3
Ssynthetic (Ours) 66.0 1.1 7.5 71.9 1.4 8.4 75.5 1.2 8.2
Table 1: Classification performance and gender bias scores of ResNet-50, Swin-T, and ConvNeXt-B backbones on
COCO. Ratio is inapplicable to Adversarial due to its gender prediction module for mitigation. Bold andunderline
represent the best and second-best, respectively. For an unbiased model, Ratio =1 and Leakage =0.
ClipCap BLIP-2 Transformer
M CS Ratio LIC M CS Ratio LIC M CS Ratio LIC
Original 29.1 75.1 2.5 2.2 29.5 75.1 5.7 4.7 26.9 71.5 4.7 4.7
LIBRA 28.9 74.9 6.5 0.5 29.0 75.4 6.3 1.9 27.4 73.4 6.7 2.3
Over-sampling 28.6 74.7 3.2 3.5 28.7 74.1 3.8 3.0 26.2 70.6 4.1 1.6
Sub-sampling 28.0 74.0 1.4 4.1 28.3 74.5 1.4 3.2 25.0 69.7 2.0 3.9
Saugment (Ours) 29.0 75.0 2.5 1.7 29.4 75.3 2.9 3.8 26.2 71.1 2.6 1.5
Ssynthetic (Ours) 28.5 75.3 1.3 0.3 29.3 75.0 1.2 2.5 25.7 70.9 1.4 0.5
Table 2: Captioning quality and gender bias scores of ClipCap, BLIP-2, and Transformer backbones on COCO. M
and CS denote METEOR and CLIPScore. Bold andunderline represent the best and second-best, respectively. For
an unbiased model, Ratio =1 and LIC =0.
man) using guidance scales of 7.5, 9.5, and 15.0
to ensure diversity. Filter weights are set to 1 (i.e.,
ck= 1for all k), contributing equally. Results are
based on five models trained with different random
seeds. More details are in Appendices A and B.
3.1 Multi-Label Classification
Experimental Setup. We focus on gender bias
using the COCO dataset, retaining only images
with gender-specific terms (e.g., woman ,man) in
their captions. This results in 28,487/13,487
train/test samples. We focus on objects co-
occurring with these terms, yielding 51 objects.
ResNet50, Swin Transformer Tiny (Swin-T), and
ConvNext models are fine-tuned using early stop-
ping. Performance is assessed using mean average
precision (mAP). Bias is quantified using leakage
and ratio. Leakage measures how much the model’s
predictions amplify the group’s information com-
pared to the ground truth. A gender classifier fg(y),
predicting gender group gfrom input y(i.e., set of
objects), is trained on a training set T={(y, g)}.For the test set T′, the model’s leakage score is:
LKM=1
|T′|X
(y,g)∈T′fg(y)1
arg max
g′fg′(y) =g
(7)
The leakage score for the original dataset, LKD,
is similarly computed. The final leakage is
Leakage =LKM−LKD. Higher leakage indicates
greater model exploitation of protected group infor-
mation. Ratio measures the exploitation of attribute
information for group prediction. By masking in-
dividuals in test images and measuring the bias in
group predictions (e.g., # man-to-# woman ratio), de-
viations from a ratio of 1 indicate attribute exploita-
tion. We report Ratio = max( r, r−1), where ris
the observed ratio. This captures the magnitude of
deviation from unbiased predictions consistently.
We compare our method with existing bias mit-
igation techniques, including dataset-level meth-
ods (Over-sampling (Wang et al., 2020b), Sub-
sampling (Agarwal et al., 2022)) and model-level
methods such as adversarial debiasing (Wang
et al., 2019) (Adversarial), domain-independent
training (Wang et al., 2020b) (DomInd), do-main discriminative training (Wang et al., 2020b)
(DomDisc), loss upweighting (Byrd and Lipton,
2019) (Upweight), focal loss (Lin et al., 2017) (Fo-
cal), class-balanced loss (Cui et al., 2019) (CB),
and group DRO (Sagawa et al., 2019) (GroupDRO).
Additional results on the OpenImages dataset and
skin tone bias mitigation are provided in Ap-
pendix B.1, demonstrating consistent conclusions.
Results. Results are shown in Table 1. Our
method, Ssynthetic , achieves the best balance by sig-
nificantly improving both ratio and leakage while
maintaining a high mAP. Specifically, Ssynthetic
achieves a near-ideal ratio of 1.1, low leakage of
7.5, and an mAP of 66.0 for ResNet-50, with simi-
lar trends observed for Swin-T and ConvNeXt-B.
Adversarial debiasing achieves lower leakage
scores by removing gender information from inter-
mediate representations. However, this method re-
duces mAP, indicating that object information may
also be inadvertently removed. Over-sampling and
sub-sampling methods address class imbalance but
at the cost of model performance. Sub-sampling,
in particular, reduces the ratio compared to over-
sampling but results in worse mAP and increased
leakage. This is likely due to the loss of diversity
and information in the training data, which forces
the model to rely more on the remaining features,
increasing the influence of protected attributes.
In contrast, Ssynthetic generates diverse, high-
quality synthetic samples, effectively balancing
bias and variance. This approach avoids the pitfalls
of other methods, resulting in superior performance
metrics. While Saugment performs similarly to the
original dataset, it performs worse in terms of ratio
and leakage compared to Ssynthetic .
3.2 Image Captioning
Experimental Setup. Using the COCO dataset
(Section 3.1), we benchmark captioning models
ClipCap, BLIP-2, and Transformer, which are fine-
tuned using early stopping. Performance is evalu-
ated with METEOR and CLIPScore. Bias is quan-
tified using LIC and ratio, where LIC is a leakage-
based metric that assesses the generation of group-
stereotypical captions compared to ground-truth
captions (i.e., yis a caption in Equation (7)), and
predicted group-related terms (e.g., woman ) in cap-
tions used to compute ratio.
Bias mitigation baselines include dataset-level
methods (Over-sampling, Sub-sampling) and the
current state-of-the-art model-level method LI-BRA (Hirota et al., 2023). LIBRA is a model-
agnostic debiasing framework designed to mitigate
bias amplification in image captioning by synthe-
sizing gender-biased captions and training a de-
biasing caption generator to recover the original
captions. Detailed results for skin tone bias mitiga-
tion, along with fine-tuning specifics, are provided
in Appendix B.2, showcasing the generalizability
of our approach.
Results. Results are shown in Table 2. Our
method, Ssynthetic , significantly improves both ra-
tio and LIC while maintaining high METEOR and
CLIPScore values. Specifically, Ssynthetic achieves
a near-ideal ratio of 1.3, low LIC of 1.2, and a ME-
TEOR score of 29.3 for BLIP-2, with similar trends
observed for ClipCap and Transformer.
While LIBRA effectively reduces LIC, it shows
an increase in the ratio metric, indicating a trade-
off between debiasing effectiveness and caption
quality. Over-sampling and sub-sampling methods
result in varying degrees of performance. Sub-
sampling showed improved bias metrics compared
to over-sampling but results in worse METEOR
scores, especially for the Transformer model.
As in the multi-label classification task, we ob-
serve that although Saugment significantly reduces
bias compared to using the original dataset, there is
a significant gap between it and Ssynthetic in terms
of bias mitigation.
3.3 Analysis of Synthetic Artifacts
Recent studies show that text-to-image models in-
troduce synthetic artifacts in images, which mod-
els may exploit (Qraitem et al., 2023; Corvi et al.,
2023; Wang et al., 2023b). Our observations in
Sections 3.1 and 3.2 suggest that bias persists with
Saugment , which augments the dataset with coun-
terfactual images to balance group distributions.
We hypothesize that Saugment may lead to short-
cut learning due to spurious correlations between
minoritized groups and inpainted artifacts. In con-
trast,Ssynthetic distributes artifacts equally across
all groups, avoiding this issue.
To test this, we create a test set by inpainting ran-
dom body parts using COCO-WholeBody annota-
tions (Jin et al., 2020). Given an image, its caption,
and body part annotations (e.g., left hand, right
hand, head), we randomly select a body part, create
a mask using the Segment Anything Model (Kir-
illov et al., 2023), and perform inpainting with the
caption as a prompt. We evaluate the consistencyResNet-50 Swin-T ClipCap BLIP-2
Ratio orig Ratio inp ∆ Ratio orig Ratio inp ∆ Ratio orig Ratio inp ∆ Ratio orig Ratio inp ∆
Original 3.5 3.0 14.3 3.1 2.6 16.1 2.3 2.5 8.7 2.3 2.4 4.4
Saugment 3.7 1.5 59.5 3.2 0.6 81.3 2.5 0.8 68.0 2.3 1.8 21.7
Ssynthetic 1.9 1.8 5.3 2.1 2.0 4.8 1.7 1.6 5.9 1.8 1.7 5.6
Table 3: Comparison of the original ( Ratio orig) and inpainted ( Ratio inp) versions of the COCO test set. The relative
difference is denoted by ∆ = 100 · |Ratio orig−Ratio inp
Ratio orig|%. A larger ∆signifies a greater change.
Object Color Skin Gender CS
sprompt+sobject+scolor 0.57 0.46 0.29 0.95 75.3
sprompt+sobject 0.49 0.50 0.20 0.99 74.8
sprompt+scolor 0.45 0.56 0.21 0.94 75.2
sobject+scolor 0.53 0.52 0.20 0.96 74.8
sprompt 0.32 0.46 0.26 0.97 75.1
sobject 0.36 0.43 0.25 0.95 74.5
scolor 0.52 0.50 0.30 0.95 74.6
No filter 0.09 0.07 0.18 0.94 74.6
Table 4: Human evaluation and captioning quality
(CLIPScore, CS in short) for each filter combination.
Higher values indicate better alignment with original im-
ages. Bold andunderline represent the best and second-
best score for each metric.
of ratios between the original and synthetic test
sets; a gap indicates the exploitation of synthetic
artifacts for gender prediction.
Table 3 presents scores for multi-label classifi-
cation (ResNet-50, Swin-T) and image captioning
(ClipCap, BLIP-2). The table includes the ratio of
gender predictions (# man-to-# woman ) for the orig-
inal test set ( Ratio orig) and the inpainted test set
(Ratio inp), along with the relative difference ( ∆) be-
tween these ratios. Results show a significant shift
in gender predictions with Saugment -trained models.
Despite identical gender ratios in the original and
inpainted test sets (both set at 2.3), models trained
withSaugment predict woman much more frequently
for the inpainted test set, indicated by the large
relative differences. In contrast, models trained
solely on synthetic data ( Ssynthetic ) show minimal
relative differences, indicating consistent gender
predictions across original and inpainted test sets.
Figure 3 shows examples of synthetic images
and predictions by ClipCap (trained on Saugment
orSsynthetic ). The examples demonstrate inconsis-
tent gender predictions with Saugment ; specifically,
the model tends to predict woman for the inpainted
test images, evidencing exploitation of synthetic
artifacts.
3.4 Human Filter Evaluation
We conduct human evaluations on Amazon Me-
chanical Turk (Turk, 2012) to evaluate the effective-
ness of our filters, aiming to determine if our filters
A man riding a bike down a streetA man riding a bike down a streetAwomanridingabikedown a streetA man riding a bike down a street𝒮augment𝒮synthtic
OriginalInpainted
A man sitting on top of a motorcycleA man sitting on a motorcycleAwomansitting onback of a motorcycleA man sitting on theback of a motorcycle𝒮augment𝒮synthticFigure 3: Predicted captions for the original (left) and
inpainted (right) test images.
prevent additional biases from inpainting models
and ensure high-quality images. For 300 randomly
selected original images, we analyze inpainted im-
ages chosen by each filter combination. Evalua-
tions focus on the similarity of 1) held/nearby ob-
jects, 2) object color, and 3) skin tone compared
to the original images. Workers assess differences
between original and synthetic images for objects
and their color, and selected skin tone classes using
the Monk Skin Tone Scale (Schumann et al., 2023;
Monk, 2023). Additionally, workers verify accu-
rate gender depiction through a sentence gap-filling
exercise (e.g., “ A ____ with a dog. ”), where they
must choose a protected group term to complete
the sentence. More details are in Appendix B.3.
For the evaluation of the similarity of objects and
their colors, scores are computed as the proportion
of times the inpainted images are rated as similar.
Regarding the skin tone and gender evaluations, the
scores are calculated as the proportion of matching
responses from workers between the original andBest
Worst 
Prompt Adherence Object Consistency Color Fidelity Overall A man standing on the grass near some dogs 
 Input Figure 4: Best/ worst inpainted images for each filter in Section 2.3 and their combination (overall).
inpainted images. All the scores range from 0 to 1.
Table 4 summarizes the human evaluation and
captioning performance of ClipCap trained on
Ssynthetic (CS), with images selected by each fil-
ter. Notably, using all filters consistently received
higher ratings across most criteria. In contrast,
randomly selecting images without any filtering of-
ten leads to synthetic images differing significantly
from the originals. This indicates that our filters are
effective in mitigating additional biases introduced
by the inpainting model. Furthermore, CLIPScore
shows that using all filters improves captioning per-
formance, highlighting its effectiveness in selecting
higher-quality images.
3.5 Inherited Biases
To further discuss the potential biases introduced by
the models used in our method, we conduct several
assessments. First, for the object detector, we run
Detic (Zhou et al., 2022) on both real and synthetic
images, achieving similar mAP scores of 32.0 for
real images and 32.3 for synthetic images, indicat-
ing consistent performance. Second, addressing
biases in CLIP, we acknowledge the potential bi-
ases inherent in the model. However, our use of
object- and color-based filters helps mitigate these
biases. Additionally, image classification and cap-
tioning results verify that our method effectively
reduces gender and skin tone biases. Lastly, for
the inpainting model, our filters effectively remove
synthetic images that deviate from the prompt, alter
color statistics, or introduce undescribed objects, as
shown in Table 4. These assessments confirm that
our method successfully mitigates biases withoutcompromising performance.
3.6 Qualitative Results
We present qualitative examples of bias mitiga-
tion by applying our method ( Ssynthetic ) in Figure 1.
The results show that training models on Ssynthetic
produces less biased outputs. For instance, in the
classification task, the baseline ResNet-50 model
and the over-sampling model incorrectly predict
tie, due to its frequent co-occurrence with manin
the training set. In contrast, Ssynthetic results in a
gender bias-free prediction. Image captioning re-
sults further validate our approach. The baseline
ClipCap model and LIBRA model generate the
man-stereotypical word skateboard , whereas our
method correctly predicts the object frisbee .
In Figure 4, we also present the best and worst
inpainted images for each filter (prompt adher-
ence, object consistency, and color fidelity), as
well as their combination (overall). The results
demonstrate each filter’s effectiveness, and combin-
ing them selects a high-quality image that closely
resembles the original. For instance, the image
judged worst by the object consistency filter lacks
the object the manis holding, while the color fi-
delity filter’s worst image shows significant color
changes in the man’s clothing. Combining these fil-
ters helps select an inpainted image that minimizes
additional bias and closely matches the original.
4 Conclusion
We present a dataset-level bias mitigation pipeline
that effectively reduces gender and skin tone biasesby ensuring group-independent attribute distribu-
tion using synthetic-only images. Our findings
indicate that mixing real and synthetic images in-
troduces spurious correlations, underscoring the
need for caution when augmenting datasets with
synthetic data. Our work highlights the potential
of synthetic data in bias mitigation and suggests
further exploration into optimizing synthetic data
generation and integration techniques for increased
bias reduction.
Limitations
Binarized Group Classes and Intersectional Bias
Analysis. While acknowledging that gender and
skin tone exist on a spectrum, our data limitations
necessitated a focus on binarized groups (i.e., man,
woman ), similar to prior work (Zhao et al., 2017;
Wang et al., 2019; Zhao et al., 2023, 2021). Our
analysis centered on gender and skin tone sepa-
rately. However, our method can be extended to
handle intersectional attributes (e.g., gender and
skin tone) by inpainting with combinations of at-
tributes (e.g., { woman ,darker-skinned }, {woman ,
lighter-skinned }, { man,darker-skinned },
{man,lighter-skinned }). We leave this exten-
sion for future work to ensure a more comprehen-
sive and inclusive analysis of biases.
Risks of Using Pre-trained Models. As dis-
cussed in Section 3.5, the pre-trained models em-
ployed in our framework (e.g., inpainting model,
object detector) may introduce inherent biases.
While our analysis in Section 3.5 confirmed that
these models do not adversely affect our method
based on our evaluations, it is possible that some
biases were not detected. Future research should
focus on incorporating additional filters to further
mitigate risks associated with pre-trained models.
Residual Bias. Our experimental results demon-
strated that our method significantly mitigates soci-
etal bias compared to existing methods. However,
bias is not completely eliminated (e.g., leakage is
not zero). Future work could explore further debi-
asing by optimizing the weight of each filter (cur-
rently, all filters are equally weighted), introduc-
ing additional filters, and combining our method
with existing bias mitigation techniques (e.g., focal
loss).
Extending to Additional Protected Groups.
Due to a lack of annotations for other protected
attributes, our focus in this paper is on gender andskin tone biases. Nevertheless, our pipeline is ap-
plicable to various protected attributes, such as age
(e.g., “ Awoman with a dog ”→“Anelderly
woman with a dog ”). Future research should ex-
plore the application of our method to additional
protected attributes.
Ethics Statement
Our research involves the manipulation of image
data to mitigate societal bias, raising important eth-
ical considerations. We address these concerns by
creating synthetic images that completely inpaint
over identifiable individuals, thereby respecting pri-
vacy and consent without altering their appearance.
Our approach aims to promote fairness and equity
by ensuring diverse and unbiased representation
in image datasets. We acknowledge the potential
biases inherent in the pre-trained models used and
have implemented filters to mitigate these biases
as much as possible. Future work should continue
to explore ethical guidelines and safeguards to en-
sure the responsible use of generative models in
research.
Acknowledgments
This work was funded by Sony Research. Ad-
ditionally, this work was partially supported by
CREST Grant No. JPMJCR20D3, JST FOREST
Grant No. JPMJFR216O, and JSPS KAKENHI No.
JP23H00497.
References
Sharat Agarwal, Sumanyu Muku, Saket Anand, and
Chetan Arora. 2022. Does data repair lead to fair
models? curating contextually fair data to reduce
model bias. In WACV .
Mohsan Alvi, Andrew Zisserman, and Christoffer Nel-
låker. 2018. Turning a blind eye: Explicit removal
of biases and variation from deep neural network
embeddings. In ECCV Workshops .
Jerone Andrews, Dora Zhao, William Thong, Apostolos
Modas, Orestis Papakyriakopoulos, and Alice Xiang.
2023. Ethical considerations for responsible data
curation. In Thirty-seventh Conference on Neural
Information Processing Systems Datasets and Bench-
marks Track .
Hritik Bansal, Da Yin, Masoud Monajatipoor, and Kai-
Wei Chang. 2022. How well can text-to-image gen-
erative models understand ethical natural language
interventions? In EMNLP .Federico Bianchi, Pratyusha Kalluri, Esin Durmus,
Faisal Ladhak, Myra Cheng, Debora Nozza, Tat-
sunori Hashimoto, Dan Jurafsky, James Zou, and
Aylin Caliskan. 2023. Easily accessible text-to-
image generation amplifies demographic stereotypes
at large scale. In FAccT .
Abeba Birhane, Sanghyun Han, Vishnu Boddeti, Sasha
Luccioni, et al. 2024. Into the laion’s den: Inves-
tigating hate in multimodal datasets. Advances in
Neural Information Processing Systems Datasets and
Benchmarks Track (NeurIPS D&B) .
Abeba Birhane and Vinay Uday Prabhu. 2021. Large
image datasets: A pyrrhic win for computer vision?
InWACV .
Abeba Birhane, Vinay Uday Prabhu, and Emmanuel
Kahembwe. 2021. Multimodal datasets: Misog-
yny, pornography, and malignant stereotypes. arXiv
preprint arXiv:2110.01963 .
Jannik Brinkmann, Paul Swoboda, and Christian Bartelt.
2023. A multidimensional analysis of social biases
in vision transformers. In ICCV .
Tim Brooks, Aleksander Holynski, and Alexei A Efros.
2023. Instructpix2pix: Learning to follow image
editing instructions. In CVPR .
Jonathon Byrd and Zachary Lipton. 2019. What is the
effect of importance weighting in deep learning? In
ICML .
Jaemin Cho, Abhay Zala, and Mohit Bansal. 2023. Dall-
eval: Probing the reasoning skills and social biases
of text-to-image generation models. In ICCV .
Riccardo Corvi, Davide Cozzolino, Giada Zingarini,
Giovanni Poggi, Koki Nagano, and Luisa Verdoliva.
2023. On the detection of synthetic images generated
by diffusion models. In ICASSP .
Guillaume Couairon, Jakob Verbeek, Holger Schwenk,
and Matthieu Cord. 2023. Diffedit: Diffusion-based
semantic image editing with mask guidance. In
ICLR .
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and
Serge Belongie. 2019. Class-balanced loss based on
effective number of samples. In CVPR .
Terrance de Vries, Ishan Misra, Changhan Wang, and
Laurens van der Maaten. 2019. Does object recogni-
tion work for everyone? In CVPR Workshops .
Terrance DeVries, Ishan Misra, Changhan Wang, and
Laurens van der Maaten. 2019. Does object recog-
nition work for everyone? In CVPR Workshop on
Fairness, Accountability Transparency, and Ethics in
Computer Vision .
Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, et al. 2021.
An image is worth 16x16 words: Transformers for
image recognition at scale. In ICLR .Felix Friedrich, Patrick Schramowski, Manuel Brack,
Lukas Struppek, Dominik Hintersdorf, Sasha Luc-
cioni, and Kristian Kersting. 2023. Fair diffusion:
Instructing text-to-image generation models on fair-
ness. arXiv preprint arXiv:2302.10893 .
Noa Garcia, Yusuke Hirota, Yankun Wu, and Yuta
Nakashima. 2023. Uncurated image-text datasets:
Shedding light on demographic bias. In Proceedings
of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6957–6966.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Identity mappings in deep residual net-
works. In ECCV .
Lisa Anne Hendricks, Kaylee Burns, Kate Saenko,
Trevor Darrell, and Anna Rohrbach. 2018. Women
also snowboard: Overcoming bias in captioning mod-
els. In ECCV .
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le
Bras, and Yejin Choi. 2021. Clipscore: A reference-
free evaluation metric for image captioning. In
EMNLP .
Yusuke Hirota, Yuta Nakashima, and Noa Garcia. 2022.
Quantifying societal bias amplification in image cap-
tioning. In CVPR .
Yusuke Hirota, Yuta Nakashima, and Noa Garcia. 2023.
Model-agnostic gender debiased image captioning.
InCVPR .
Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao
Liu, Chen Qian, Wanli Ouyang, and Ping Luo. 2020.
Whole-body human pose estimation in the wild. In
ECCV .
Jungseock Joo and Kimmo Kärkkäinen. 2020. Gender
slopes: Counterfactual fairness for computer vision
models by attribute manipulation. In International
Workshop on Fairness, Accountability, Transparency
and Ethics in Multimedia (FATE/MM) .
Divyansh Kaushik, Eduard Hovy, and Zachary Lipton.
2019. Learning the difference that makes a difference
with counterfactually-augmented data. In ICLR .
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR .
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,
Spencer Whitehead, Alexander C Berg, Wan-Yen
Lo, et al. 2023. Segment anything. arXiv preprint
arXiv:2304.02643 .
Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Fer-
rari, Sami Abu-El-Haija, Alina Kuznetsova, Has-
san Rom, Jasper Uijlings, Stefan Popov, Andreas
Veit, et al. 2017. Openimages: A public dataset for
large-scale multi-label and multi-class image clas-
sification. Dataset available from https://github.
com/openimages .Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. arXiv preprint arXiv:2301.12597 .
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,
and Piotr Dollár. 2017. Focal loss for dense object
detection. In ICCV .
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft COCO:
Common objects in context. In ECCV .
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,
Zheng Zhang, Stephen Lin, and Baining Guo. 2021.
Swin transformer: Hierarchical vision transformer
using shifted windows. In ICCV .
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph
Feichtenhofer, Trevor Darrell, and Saining Xie. 2022.
A convnet for the 2020s. In CVPR .
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In ICLR .
Alexandra Sasha Luccioni, Christopher Akiki, Margaret
Mitchell, and Yacine Jernite. 2023. Stable bias: An-
alyzing societal representations in diffusion models.
InNeurIPS .
Abhishek Mandal, Susan Leavy, and Suzanne Little.
2023. Multimodal composite association score: Mea-
suring gender bias in generative multimodal models.
arXiv preprint arXiv:2304.13855 .
Nicole Meister, Dora Zhao, Angelina Wang, Vikram V
Ramaswamy, Ruth Fong, and Olga Russakovsky.
2023. Gender artifacts in visual datasets. In ICCV .
Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,
and Ross Girshick. 2016. Seeing through the human
reporting bias: Visual classifiers from noisy human-
centric labels. In CVPR .
Ron Mokady, Amir Hertz, and Amit H Bermano. 2021.
Clipcap: Clip prefix for image captioning. arXiv
preprint arXiv:2111.09734 .
Ellis Monk. 2023. The monk skin tone scale.
Ranjita Naik and Besmira Nushi. 2023. Social biases
through the text-to-image generation lens. In AIES .
Seong Joon Oh, Rodrigo Benenson, Mario Fritz, and
Bernt Schiele. 2016. Faceless person recognition:
Privacy implications in social media. In European
Conference on Computer Vision (ECCV) , pages 19–
35. Springer.
Tribhuvanesh Orekondy, Mario Fritz, and Bernt Schiele.
2018. Connecting pixels to privacy and utility: Au-
tomatic redaction of private information in images.
InIEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 8466–8475.Maan Qraitem, Kate Saenko, and Bryan A Plummer.
2023. From fake to real (ffr): A two-stage train-
ing pipeline for mitigating spurious correlations with
synthetic data. arXiv preprint arXiv:2308.04553 .
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In ICML .
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog.
Vikram V . Ramaswamy, Sunnie S. Y . Kim, and Olga
Russakovsky. 2021a. Fair attribute classification
through latent space de-biasing. In IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition
(CVPR) .
Vikram V Ramaswamy, Sunnie SY Kim, and Olga Rus-
sakovsky. 2021b. Fair attribute classification through
latent space de-biasing. In CVPR .
Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. 2022. High-
resolution image synthesis with latent diffusion mod-
els. In CVPR .
Candace Ross, Boris Katz, and Andrei Barbu.
2020. Measuring social biases in grounded vi-
sion and language embeddings. arXiv preprint
arXiv:2002.08911 .
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexan-
der C. Berg, and Li Fei-Fei. 2015. ImageNet Large
Scale Visual Recognition Challenge. IJCV .
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto,
and Percy Liang. 2019. Distributionally robust neu-
ral networks for group shifts: On the importance
of regularization for worst-case generalization. In
ICLR .
Prasanna Sattigeri, Samuel C Hoffman, Vijil Chen-
thamarakshan, and Kush R Varshney. 2019. Fairness
gan: Generating datasets with fairness properties us-
ing a generative adversarial network. IBM Journal of
Research and Development .
Candice Schumann, Gbolahan O Olanubi, Auriel
Wright, Ellis Monk Jr, Courtney Heldreth, and Su-
sanna Ricco. 2023. Consensus and subjectivity of
skin tone annotation for ml fairness. arXiv preprint
arXiv:2305.09073 .
Candice Schumann, Susanna Ricco, Utsav Prabhu, Vit-
torio Ferrari, and Caroline Pantofaru. 2021. A step
toward more inclusive people annotations for fairness.
InAIES .Preethi Seshadri, Sameer Singh, and Yanai Elazar. 2023.
The bias amplification paradox in text-to-image gen-
eration. arXiv preprint arXiv:2308.00755 .
Viktoriia Sharmanska, Lisa Anne Hendricks, Trevor
Darrell, and Novi Quadrianto. 2020. Contrastive
examples for addressing the tyranny of the majority.
arXiv preprint arXiv:2004.06524 .
Brandon Smith, Miguel Farinha, Siobhan Mackenzie
Hall, Hannah Rose Kirk, Aleksandar Shtedritski, and
Max Bain. 2023. Balancing the picture: Debiasing
vision-language datasets with synthetic contrast sets.
arXiv preprint arXiv:2305.15407 .
Benjamin Sobel. 2020. A taxonomy of training data:
Disentangling the mismatched rights, remedies, and
rationales for restricting machine learning. Artificial
Intelligence and Intellectual Property (Reto Hilty,
Jyh-An Lee, Kung-Chung Liu, eds.), Oxford Univer-
sity Press, Forthcoming .
Marina Sokolova, Nathalie Japkowicz, and Stan Sz-
pakowicz. 2006. Beyond accuracy, f-score and roc:
a family of discriminant measures for performance
evaluation. In Australasian joint conference on artifi-
cial intelligence .
Lukas Struppek, Dominik Hintersdorf, and Kristian Ker-
sting. 2022. The biased artist: Exploiting cultural
biases via homoglyphs in text-guided image genera-
tion models. arXiv preprint arXiv:2209.08891 .
Ruixiang Tang, Mengnan Du, Yuening Li, Zirui Liu,
Na Zou, and Xia Hu. 2021. Mitigating gender bias
in captioning systems. In WWW .
Amazon Mechanical Turk. 2012. Amazon mechanical
turk. Retrieved August .
Eddie L Ungless, Björn Ross, and Anne Lauscher. 2023.
Stereotypes and smut: The (mis) representation of
non-cisgender identities by text-to-image models. In
ACL.
Angelina Wang, Arvind Narayanan, and Olga Rus-
sakovsky. 2020a. REVISE: A tool for measuring
and mitigating bias in visual datasets. In ECCV .
Angelina Wang and Olga Russakovsky. 2021. Direc-
tional bias amplification. In ICML .
Jialu Wang, Xinyue Gabby Liu, Zonglin Di, Yang Liu,
and Xin Eric Wang. 2023a. T2iat: Measuring valence
and stereotypical biases in text-to-image generation.
InACL.
Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei
Chang, and Vicente Ordonez. 2019. Balanced
datasets are not enough: Estimating and mitigating
gender bias in deep image representations. In ICCV .
Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis,
Kyle Genova, Prem Nair, Kenji Hata, and Olga Rus-
sakovsky. 2020b. Towards fairness in visual recog-
nition: Effective strategies for bias mitigation. In
CVPR .Zhao Wang and Aron Culotta. 2021. Robustness to
spurious correlations in text classification via auto-
matically generated counterfactuals. In AAAI .
Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun
Wang, Hezhen Hu, Hong Chen, and Houqiang Li.
2023b. Dire for diffusion-generated image detection.
InICCV .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2020. Transformers: State-of-the-art natural
language processing. In EMNLP: system demonstra-
tions .
Rui-Jie Yew and Alice Xiang. 2022. Regulating facial
processing technologies: Tensions between legal and
technical considerations in the application of illinois
bipa. In ACM Conference on Fairness, Accountabil-
ity, and Transparency (FAccT) , page 1017–1027.
Yanzhe Zhang, Lu Jiang, Greg Turk, and Diyi
Yang. 2023. Auditing gender presentation differ-
ences in text-to-image models. arXiv preprint
arXiv:2302.03675 .
Dora Zhao, Jerone TA Andrews, and Alice Xiang. 2023.
Men also do laundry: Multi-attribute bias amplifica-
tion. In ICML .
Dora Zhao, Angelina Wang, and Olga Russakovsky.
2021. Understanding and evaluating racial biases in
image captioning. In ICCV .
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2017. Men also like
shopping: Reducing gender bias amplification using
corpus-level constraints. In EMNLP .
Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Krähenbühl, and Ishan Misra. 2022. Detecting
twenty-thousand classes using image-level supervi-
sion. In ECCV .
A Method Details
A.1 Image Generation Settings
Selection of People for Inpainting. Following
the previous works (Zhao et al., 2021; Misra et al.,
2016), we apply inpainting to a person with the
largest bounding box. In addition, if the second
largest person’s box is larger than 55,000pixels,
the region is also inpainted. For COCO, we do
this by using the person label and corresponding
bounding boxes. For OpenImages, we use person-
bounding boxes presented in More Inclusive Anno-
tations for People (MIAP) annotations (Schumann
et al., 2021), then we generate person masks within
the boxes using Segment Anything Model (Kirillov
et al., 2023).Parameters of Image Generation. In Sec-
tion 2.2, we generate m= 30 inpainted images
for each group (e.g., {woman ,man}for binary gen-
der). When generating the images, we use three
different guidance scale parameters (7.5, 9.5, and
15.0) to generate diverse inpainted images (i.e., gen-
erating 10 images for each guidance scale). We use
6 NVIDIA A100-PCIE-40GB GPUs, resulting in a
total of 72 hours to finish synthesizing images.
A.2 Visual examples of inpainted images &
failure cases
We show the visual examples of the inpainted im-
ages after filtering in Figure 5 (for binary gender)
and Figure 6 (for binary skin tone). The examples
show that the inpainted images depict the target
groups (e.g., woman anddarker-skinned ), keep-
ing the rest fixed. In some cases, artifacts are no-
ticeable, which enables us to identify synthetic im-
ages (e.g., the details of the faces are not clear), but
they do not affect the downstream performance, as
shown in the main paper.
B Experimental Settings and Additional
Results
B.1 Multi-Label Classification
Datasets. We use COCO (Lin et al., 2014) and
OpenImages (Krasin et al., 2017). Following pre-
vious works (Zhao et al., 2017, 2023), we focus
on attributes co-occurring with woman ormanmore
than100times and remove person-related classes
(e.g., person class), resulting in 51and126at-
tributes for COCO and OpenImages, respectively.
The list of the attributes is as follows:
COCO: { sink ,refrigerator ,laptop ,
surfboard ,vase ,bottle ,remote ,donut ,
motorcycle ,car,chair ,suitcase ,tv,knife ,
fork ,couch ,bus,toothbrush ,bicycle ,tie,
clock ,microwave ,teddy bear ,frisbee ,spoon ,
dog,truck ,bench ,backpack ,skis ,horse ,
sandwich ,bed,handbag ,umbrella ,pizza ,book ,
dining table ,traffic light ,banana ,potted
plant ,tennis racket ,cat,sports ball ,
kite ,cake ,wine glass ,bowl ,cup,oven ,cell
phone }.
OpenImages: { goggles ,building ,cloud ,
smile ,tree ,sunglasses ,light ,t-shirt ,
glasses ,water ,forehead ,wall ,sky,tire ,
roof ,road ,wheel ,vehicle ,land vehicle ,car,
tie,furniture ,microphone ,suit ,clothing ,
fence ,jeans ,trousers ,shirt ,footwear ,flooring ,outerwear ,coat ,ceiling ,floor ,
jacket ,table ,house ,couch ,mammal ,hat,
shoe ,sports uniform ,baseball (sport) ,cap,
baseball cap ,bag,drawing ,sun hat ,musical
instrument ,baby ,window ,door ,sweater ,
lake ,chair ,tableware ,bottle ,drink ,
handwriting ,paper ,food ,tent ,concert ,
drum ,guitar ,glove ,sports equipment ,
blazer ,art,painting ,dress ,flower ,
sneakers ,screenshot ,watercraft ,beach ,
animal ,grass family ,plant ,soil ,desk ,
poster ,bus,computer ,personal computer ,
watch ,mountain ,helmet ,bicycle helmet ,
bicycle wheel ,bicycle ,curtain ,dance ,
football ,ball (object) ,soccer ,wedding
dress ,jewellery ,bride ,office building ,
laptop ,toddler ,shorts ,hiking ,fashion
accessory ,fedora ,swimming ,swimwear ,
camera ,playground ,weapon ,ship ,statue ,
boat ,fast food ,flag ,soft drink ,book ,auto
part ,snow ,carnivore ,dog,horse ,motorcycle ,
pole dance }.
Training. The models (ResNet-50 (He et al.,
2016), Swin-T (Liu et al., 2021), and ConvNeXt-
Base (Liu et al., 2022)) are initialized with Ima-
geNet (Russakovsky et al., 2015) pre-training, and
fine-tuned with early stopping using a validation set
split from the training set ( 20% of the training set).
The optimizer is Adam (Kingma and Ba, 2015),
batch size is 32, and a learning rate is 1×10−5.
For binary gender, the classification layers predict
both protected groups (i.e., { woman ,man}) and ob-
ject classes. For binary skin tone, the models only
predict object classes as ground-truth skin tone la-
bels are not available.
Results for OpenImages. We show the complete
results of the experiments in the main paper: gender
bias on OpenImages (Table 5). The results show
that all the insights described in the main paper are
consistent across the datasets.
Results for skin tone bias. Previous bias mit-
igation methods face a significant limitation, re-
quiring protected group labels for all training set
samples (Zhao et al., 2017; Wang et al., 2019;
Agarwal et al., 2022). They typically focus on
gender as a protected attribute due to its preva-
lence in captions (Misra et al., 2016), allowing
for label inference through gender-related terms.
In contrast, Ssynthetic applies to attributes without
labels, such as skin tone. We use our pipeline (ex-Original W oman Man Figure 5: Examples of inpainted images for binary gender.
ResNet-50 Swin-T ConvNeXt-B
mAP Ratio Leakage mAP Ratio Leakage mAP Ratio Leakage
Original 42.3 5.2 18.9 45.3 4.3 20.9 46.0 5.0 22.7
Adversarial 37.5 — 8.3 40.8 — 11.3 40.4 — 12.3
DomDisc 40.7 3.7 20.6 43.6 4.6 22.1 42.9 4.1 21.9
DomInd 40.3 3.7 19.1 42.7 3.5 20.2 43.4 2.6 22.0
Upweight 41.3 6.5 13.1 44.7 5.8 17.9 45.3 7.4 18.0
Focal 43.0 4.6 18.7 45.4 4.4 21.3 45.4 4.0 22.3
CB 40.5 5.2 18.0 42.6 3.9 19.8 43.9 4.6 21.5
GroupDRO 42.3 4.2 18.9 45.1 4.2 20.9 46.1 3.4 22.5
Over-sampling 38.5 3.3 15.0 41.1 4.0 16.1 41.7 5.2 18.4
Sub-sampling 38.3 2.2 18.3 41.2 2.1 19.8 39.8 2.8 21.7
Saugment (Ours) 42.0 1.9 16.0 44.9 2.4 18.0 45.5 2.6 19.0
Ssynthetic (Ours) 41.4 1.1 14.6 44.4 2.0 17.6 44.7 1.3 17.9
Table 5: Classification performance and gender bias scores of ResNet-50, Swin-T, and ConvNeXt-B backbones on
OpenImages. Ratio is inapplicable to Adversarial due to its gender prediction module for mitigation. Bold and
underline represent the best and second-best, respectively. For an unbiased model, Ratio =1 and Leakage =0.
cluding the color fidelity filter, as we aim to mod-
ify skin tone) on binary skin tone categories (i.e.,
G={darker-skinned ,lighter-skinned }) us-
ing COCO. We evaluate skin tone bias using leak-
ageonly since ratio requires models to predict pro-
tected groups, and there are no skin tone annota-
tions for the COCO training set. Results are shown
in Table 6, demonstrating consistent conclusions
with gender bias.
B.2 Image Captioning
Training. We benchmark three captioning mod-
els: ClipCap (Mokady et al., 2021), BLIP-2
(Li et al., 2023), and Transformer (i.e., the
Transformer-based encoder-decoder model com-
posed of Vision Transformer (Dosovitskiy et al.,2021) and GPT-2 (Radford et al., 2019)). As
with multi-label classification, we train the mod-
els with early stopping. Specifically, for Clip-
Cap, we follow the official implementation regard-
ing the training settings. For BLIP-2 and Trans-
former, we use the implementation in Hugging
Face (Wolf et al., 2020). We use the AdamW opti-
mizer (Loshchilov and Hutter, 2019) with a learn-
ing rate of 2×10−6/1×10−4and batch size of
8/64for BLIP-2 and Transformer, respectively.
Results for skin tone. We show the results of the
experiments for skin tone bias mitigation in Table 7.
The results show that the insights in the main paper
are mostly consistent across the protected groups.Original Darker-skinned 
 Lighter-skinned 
Figure 6: Examples of inpainted images for binary skin tone.
ResNet-50 Swin-T ConvNeXt-B
mAP Leakage mAP Leakage mAP Leakage
Original 65 .8 3.2 72 .2 7.1 75 .9 7.2
Ssynthetic (Ours) 65.2 2.3 71.4 3.7 74.5 5.9
Table 6: Classification performance and skin tone bias scores of ResNet-50, Swin-T, and ConvNeXt-B backbones
on COCO. Bold represents the best. For an unbiased model, Ratio =1 and Leakage =0.
B.3 Human Filter Evaluation
In Figures 7 to 9, we present example tasks for
human evaluation conducted on Amazon Mechan-
ical Turk (AMT) (Turk, 2012). This evaluation
assesses how well each combination of filters iden-
tifies desirable inpainted images. Figure 7 shows
the user interface for evaluating the similarity of
held/nearby objects and their colors between the
original (left) and inpainted (right) images. Fig-
ure 8 asks workers to select a skin tone class us-
ing the Monk Skin Tone Scale (Schumann et al.,
2023; Monk, 2023). We conduct this evaluation on
both original and inpainted images and compute
the degree of agreement between them. Figure 9
verifies if perceived gender is accurately depicted—
according to the AMT worker—in the inpainted
images through gap-filling, where workers must
choose a protected group term to complete the sen-
tence. Each assignment pays $0.07, with a total
participant compensation of approximately $2,000.C Image Attribution
All images in this paper are sourced from the
COCO dataset (Lin et al., 2014), which is publicly
available at https://cocodataset.org . These
images are used for non-commercial academic pur-
poses in accordance with their respective licenses.
Image License Details. Detailed information on
the licensing of each image used in the figures of
this paper is provided below:
Figure 1 (left)
• Title: Texting
• Photographer: Ktoine
•Source URL: https://www.flickr.com/
photos/49170045@N07/5149301465
• Copyright information: Antoine K
• Creative Common license: CC BY-SA
Figure 1 (second from the left)
• Title: dk&fishClipCap BLIP-2 Transformer
M CS LIC M CS LIC M CS LIC
Original 29.4 75.3 4.6 27.1 73.9 2.2 27.0 71.5 5.3
Ssynthetic (Ours) 29.1 75.4 3.7 26.8 73.6 2.0 26.5 71.0 4.7
Table 7: Captioning quality and skin tone bias scores of ClipCap, BLIP-2, and Transformer backbones on COCO.
M and CS denote METEOR and CLIPScore. Bold represents the best. For an unbiased model, Ratio =1and
LIC=0.
Figure 7: Evaluation of perceived object and color similarity between original and inpainted images on AMT.
• Photographer: captain_ambiance
•Source URL: https://www.flickr.com/
photos/87855339@N00/2500267824
• Copyright information: N/A
• Creative Common license: CC BY-NC-SA
Figure 1 (second from the right)
• Title: Frisbee 1
• Photographer: laikolosse
•Source URL: https://www.flickr.com/
photos/64952097@N00/2435927296
• Copyright information: N/A
• Creative Common license: CC BY-NC
Figure 1 (right)
•Title: East Coast Marines participate in ad-
vanced motorcycle class
• Photographer: CherryPoint
•Source URL: https://www.flickr.com/
photos/71948543@N03/7789686222
• Copyright information: N/A
• Creative Common license: CC BY-NC-SA
Figure 2• Title: I get to wear a tie today!
• Photographer: mr.rcollins
•Source URL: https://www.flickr.com/
photos/15076398@N00/6025569708
• Copyright information: N/A
• Creative Common license: CC BY
Figure 3 (top)
• Title: Penny farthing
• Photographer: ksuyin
•Source URL: https://www.flickr.com/
photos/96256161@N00/3910801203
• Copyright information: Su Yin Khoo
• Creative Common license: CC BY-NC-SA
Figure 3 (bottom)
• Title: DSC_0416
• Photographer: driver Photographer
•Source URL: https://www.flickr.com/
photos/72334647@N03/7209442826
• Copyright information: N/A
• Creative Common license: CC BY-SA
Figure 4Figure 8: Evaluation of perceived skin tone using the Monk Skin Tone Scale on AMT.
• Title: LANCE JAMES AND DOGS
• Photographer: summonedbyfells
•Source URL: https://www.flickr.com/
photos/8521690@N02/6452452909
• Copyright information: N/A
• Creative Common license: CC BY
Figure 5 (left)
• Title: Lou zers
• Photographer: orijinal
•Source URL: https://www.flickr.com/
photos/48600098077@N01/4342901300
• Copyright information: Jaysin Trevino
• Creative Common license: CC BY
Figure 5 (second from the left)
• Title: 091022-F-7797P-006
• Photographer: Offutt Air Force Base
•Source URL: https://www.flickr.com/
photos/61438875@N03/6427400981
• Copyright information: N/A
• Creative Common license: CC BY
Figure 5 (second from the right)
•Title: Gael Monfils at the Legg Mason Tennis
Classic
• Photographer: Kyle T.
•Source URL: https://www.flickr.com/
photos/87605170@N00/6019705477
• Copyright information: N/A
• Creative Common license: CC BY
Figure 5 (right)
• Title: 022• Photographer: tiraslee
•Source URL: https://www.flickr.com/
photos/69153271@N00/3482486652
• Copyright information: N/A
• Creative Common license: CC BY
Figure 6 (left)
• Title: Bear & I @ San Diego Zoo
• Photographer: tammylo
•Source URL: https://www.flickr.com/
photos/13595617@N00/8027273225
• Copyright information: Tammy Lo
• Creative Common license: CC BY
Figure 6 (second from the left)
• Title: Loop hem eruit!
• Photographer: FaceMePLS
•Source URL: https://www.flickr.com/
photos/38891071@N00/5616622936
• Copyright information: N/A
• Creative Common license: CC BY
Figure 6 (second from the right)
• Title: Jasmin 2
• Photographer: Snake Kiddo
•Source URL: https://www.flickr.com/
photos/32502055@N05/7779141980
• Copyright information: Daniel Lara
• Creative Common license: CC BY
Figure 6 (right)
• Title: Raoul in the kitchen!
• Photographer: pug freak
•Source URL: https://www.flickr.com/Figure 9: Evaluation of perceived gender depiction accuracy in inpainted images on AMT.
photos/36567879@N00/4088457159
• Copyright information: Donna
• Creative Common license: CC BY-NC
Figures 7 to 9
• Title: Hmong Woman
• Photographer: Elliot Margolies
•Source URL: https://www.flickr.com/
photos/41597157@N00/8298673953
• Copyright information: Elliot Margolie
• Creative Common license: CC BY-NC
Creative Commons Licenses. Links to each Cre-
ative Commons license are provided below:
•Attribution-NonCommercial-ShareAlike (CC
BY-NC-SA): http://creativecommons.org/
licenses/by-nc-sa/2.0/
•Attribution-NonCommercial (CC BY-NC):
http://creativecommons.org/licenses/
by-nc/2.0/
•Attribution (CC BY): http://
creativecommons.org/licenses/by/2.0/
•Attribution-ShareAlike (CC BY-SA):
http://creativecommons.org/licenses/
by-sa/2.0/