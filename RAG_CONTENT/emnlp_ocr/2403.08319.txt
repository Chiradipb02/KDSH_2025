Knowledge Conflicts for LLMs: A Survey
Rongwu Xu1‚Ä†‚àó, Zehan Qi1‚Ä†‚àó, Zhijiang Guo2‚Ä†,
Cunxiang Wang3, Hongru Wang4, Yue Zhang3, Wei Xu1
1Tsinghua University2University of Cambridge
3Westlake University4The Chinese University of Hong Kong
{xrw22, qzh23}@mails.tsinghua.edu.cn
‚Ä†Leading authors,‚àóEqual contribution
Abstract
This survey provides an in-depth analysis of
knowledge conflicts for large language models
(LLMs), highlighting the complex challenges
they encounter when blending contextual and
parametric knowledge. Our focus is on three
categories of knowledge conflicts: context-
memory, inter-context, and intra-memory con-
flict. These conflicts can significantly impact
the trustworthiness and performance of LLMs,
especially in real-world applications where
noise and misinformation are common. By cat-
egorizing these conflicts, exploring the causes,
examining the behaviors of LLMs under such
conflicts, and reviewing available solutions,
this survey aims to shed light on strategies for
improving the robustness of LLMs, thereby
serving as a valuable resource for advancing
research in this evolving area.
https://github.com/pillowsofwind/
Knowledge-Conflicts-Survey
1 Introduction
Large language models (LLMs; Brown et al. 2020;
Touvron et al. 2023; OpenAI 2024) are renowned
for encapsulating a vast repository of world knowl-
edge (Roberts et al., 2020; Hu et al., 2023), referred
to as parametric knowledge . These models excel in
knowledge-intensive tasks including QA (Petroni
et al., 2019), fact-checking (Gao et al., 2023a), di-
alogue system (Wang et al., 2023e), knowledge
generation (Chen et al., 2023c), inter alia . In the
meantime, LLMs continue to engage with external
contextual knowledge after deployed (Pan et al.,
2022), including user prompts (Liu et al., 2023a),
interactive dialogues (Zhang et al., 2020; Wang
et al., 2024a), or retrieved documents from the
Web (Lewis et al., 2020; Shi et al., 2023c), and
tools (Schick et al., 2023; Zhuang et al., 2023).
Integrating contextual knowledge into LLMs en-
ables them to keep abreast of current events (Ka-
BrazilholdstherecordforthemostFIFA WorldCup wins‚Ä¶‚Ä¶I think Argentinahas won themostchampionships.
Italyisthemostsuccessfulnationalteamin thehistoryoftheWorldCup, havingwon fourtitles(1934, 1938, 1982, 2006).As ofmylastupdatein April 2023, thenationalteamwiththemostFIFA WorldCup championshipsisBrazil. Theyhavewon thetournamenta total offivetimes.I. ContextualKnowledge(Context)
II. Parametric Knowledge (Memory)Retrieved DocumentsUser PromptDialogue
Question: Whichteamhas won themostFIFA WorldCup championships?‚Ä¶, Germanyhas officiallyclaimedthetitleofthemostsuccessfulnationalteam‚Ä¶Witha staggeringtotal offiveWorldCup triumphs, theBrazilian‚Ä¶Inter-context conflictIntra-memory conflictContext-memory conÔ¨Çict
Figure 1: An LLM may encounter three distinct types
of knowledge conflicts, stemming from knowledge
sources‚Äîeither contextual ( I. Context , yellow chat-
boxes) or inherent to the LLM‚Äôs parameters ( II. Memory ,
blue chatboxes). When confronted with a user‚Äôs ques-
tion (purple chatbox) entailing knowledge of complex
conflicts, the LLM is required to resolve these discrep-
ancies to deliver accurate responses.
sai et al., 2022) and generate more accurate re-
sponses (Shuster et al., 2021), yet it risks conflict-
ing due to the rich knowledge sources. The dis-
crepancies among the contexts and the model‚Äôs
parametric knowledge are referred to as knowledge
conflicts (Chen et al., 2022; Xie et al., 2023). In
this paper, we categorize three distinct types of
knowledge conflicts, as characterized in Figure 1.
As the example shown in Figure 1, when utiliz-
ing an LLM to respond to a user question, users
may provide the LLM with supplementary prompts,
while the LLM also leverages search engines to
gather relevant documents from the Web to en-
hance its knowledge (Lewis et al., 2020). This
combination of user prompts, dialogue history, and
retrieved documents constitutes contextual knowl-
edge ( context ). Contextual knowledge can conflict
with the parametric knowledge ( memory ) encapsu-
lated within the LLM‚Äôs parameters (Longpre et al.,arXiv:2403.08319v2  [cs.CL]  22 Jun 2024TemporalMisalignMisinforma/on‚Ä¶
Knowledge ConÔ¨ÇictsFaithful to Context‚Ä¶
Desired BehaviorMaybe factually incorrect
CausesPhenomenonBehaviorsFaithful to MemoryI. Causes
III-i.ObjectiveII. Analysis
induceinduceBias in Corpora
üí•III-ii. Solu3onpre-hocpost-hoc
‚è≥
üòà
‚öñFigure 2: We view knowledge conflict not only as a standalone phenomenon but also as a nexus that connects
various causal triggers ( causes ) with the behaviors of LLMs. While existing literature mainly focuses on II.
Analysis , our survey involves systematically observing these conflicts, offering insights into their emergence and
impact on LLMs‚Äô behavior, along with the desirable behaviors and related solutions.
2021; Xie et al., 2023), a phenomenon we term as
context-memory conflict (CM, ¬ß 2). In real-world
scenarios, the external document might be fraught
with noise (Zhang and Choi, 2021) or even delib-
erately crafted misinformation (Du et al., 2022b;
Pan et al., 2023a), complicating their ability to pro-
cess and respond accurately (Chen et al., 2022).
We term the conflict among various pieces of con-
textual knowledge as inter-context conflict (IC,
¬ß 3). To reduce uncertainties in responses, the user
may pose the question in various forms. There-
fore, the LLM‚Äôs parametric knowledge may yield
divergent responses to these differently phrased
questions. This variance can be attributed to the
conflicting knowledge embedded within the LLM‚Äôs
parameters, which stem from the inconsistencies
present in the complex and diverse pre-training data
sets (Huang et al., 2023). This gives rise to what
we term as intra-memory conflict (IM, ¬ß 4).
Knowledge conflict is originally rooted in open-
domain QA research. The concept gained atten-
tion in Longpre et al. (2021) that focused on the
entity-based conflicts between parametric knowl-
edge and external passages. Concurrently, discrep-
ancies among multiple passages were also scruti-
nized subsequently (Chen et al., 2022). Knowledge
conflicts attract significant attention with the recent
advent of LLMs. For instance, recent studies find
that LLMs exhibit both adherence to parametric
knowledge and susceptibility to contextual influ-
ences (Xie et al., 2023), which can be problematic
when this external knowledge is factually incor-
rect (Pan et al., 2023b). Given the implications for
the trustworthiness (Du et al., 2022b), real-time
accuracy (Kasai et al., 2022), and robustness of
LLMs (Ying et al., 2023), it is imperative to delvedeeper into understanding and resolving knowledge
conflicts (Xie et al., 2023; Wang et al., 2023h).
As of the time of writing, to the best of our
knowledge, there is no systematic survey dedicated
to the investigation of knowledge conflicts. Ex-
isting reviews (Zhang et al., 2023d; Wang et al.,
2023a; Feng et al., 2023) touch upon knowledge
conflicts as a subtopic within their broader contexts.
While Feng et al. (2023) offer a more systematic
examination of knowledge conflicts, categorizing
them into external and internal conflicts. How-
ever, their survey provides only a brief overview
of relevant works and primarily focuses on specific
scenarios. To fill the gap, we aim to provide a com-
prehensive review encompassing the categorization,
cause and behavior analysis, and solutions for ad-
dressing various kinds of knowledge conflicts.
We conceptualize the lifecycle of knowledge con-
flicts as both a cause leading to various behaviors,
and an effect emerges from the intricate nature of
knowledge as in Figure 2. Knowledge conflicts
serve as a crucial intermediary between causes and
model behaviors. For instance, they significantly
contribute to the model generating factually incor-
rect information, a.k.a. , hallucinations (Ji et al.,
2023; Zhang et al., 2023d). Our research, in a man-
ner akin to Freudian psychoanalysis, underscores
the significance of understanding the origins of
these conflicts. Although existing analyses (Chen
et al., 2022; Xie et al., 2023; Wang et al., 2023h)
tend to construct such conflicts artificially, we posit
that these analyses do not sufficiently address the
interconnectedness of the issue.
Going beyond reviewing and analyzing causes
and behaviors, we delve deeper to provide a sys-
tematic review of solutions, which are employed toKnowledge ConflictsContext-Memory
Conflict (¬ß 2)Causes
(¬ß 2.1)Temporal MisalignmentLazaridou et al. (2021), Luu et al. (2021), Jang et al. (2021),
Jang et al. (2022), Liska et al. (2022), Dhingra et al. (2022),
Kasai et al. (2022), Margatina et al. (2023), Cheang et al. (2023)
Misinformation PollutionDu et al. (2022b), Pan et al. (2023a), Pan et al. (2023b),
Xu et al. (2023), Weller et al. (2022)
Analysis
(¬ß 2.2)Open-domain QA Longpre et al. (2021), Chen et al. (2022), Tan et al. (2024)
GeneralXie et al. (2023), Wang et al. (2023h), Ying et al. (2023),
Qian et al. (2023), Xu et al. (2023), Jin et al. (2024a)
Solution
(¬ß 2.3)Faithful to ContextFine-tuningKAFT (Li et al., 2022a)
 ,
TrueTeacher (Gekhman et al., 2023)
 ,
K-DIAL (Xue et al., 2023)
Prompting OPIN (Zhou et al., 2023d)
Decoding CAD (Shi et al., 2023a)
 ,
Knowledge Plug-in CuQA (Lee et al., 2022a)
Pre-training ICLM (Shi et al., 2023b)
Predict Fact Validity Zhang and Choi (2023)
Discriminating Misinformation
(Faithful to Memory)Prompting Pan et al. (2023b)
 , Xu et al. (2023)
Query Augmentation Weller et al. (2022)
Training Discriminator Hong et al. (2023)
Disentangling Sources DisentQA (Neeman et al., 2022)
 , Wang et al. (2023h)
Improving Factuality COMBO (Zhang et al., 2023e)
 , CD2(Jin et al., 2024a)
Inter-Context
Conflict (¬ß 3)Causes
(¬ß 3.1)Misinformation Chen and Shu (2023b), Vergho et al. (2024), Chen et al. (2023b)
Outdated Information Zhang and Choi (2021), Kasai et al. (2022)
Analysis
(¬ß 3.2)Performance ImpactChen et al. (2022), Xie et al. (2023), Pan et al. (2023a),
Zhang and Choi (2021), Du et al. (2022b), Jin et al. (2024a)
Detection Ability Li et al. (2023a), Zheng et al. (2022), Wan et al. (2024),
Solution
(¬ß 3.3)Eliminating ConflictSpecialized ModelsPCNN (Hsu et al., 2021)
 , Pielka et al. (2022)
 ,
Wu et al. (2022)
General ModelsLeite et al. (2023)
 , Cheung and Lam (2023)
 ,
Chern et al. (2023)
Improving RobustnessTraining Approach Hong et al. (2023)
Query Augmentation CAR (Weller et al., 2022)
Intra-Memory
Conflict (¬ß 4)Causes
(¬ß 4.1)Bias in Training Corpora Wang et al. (2023d), Xu et al. (2022),
Decoding Strategy Lee et al. (2022b), Huang et al. (2023)
Knowledge Editing Yao et al. (2023), Li et al. (2023f)
Analysis
(¬ß 4.2)Self-InconsistencyDong et al. (2023), Zhao et al. (2023b), Manakul et al. (2023),
Dhuliawala et al. (2023), Zhang et al. (2023c), M√ºndler et al. (2023),
Agrawal et al. (2023), Hase et al. (2023)
Latent Representation
of KnowledgeChuang et al. (2023), Li et al. (2023c)
Cross-lingual Inconsistency Wang et al. (2023f), Qi et al. (2023)
Solution
(¬ß 4.3)Improving ConsistencyFine-tuning Elazar et al. (2021)
 , Li et al. (2023d)
Plug-in CRM (Jang and Lukasiewicz, 2023)
Output EnsembleConCoRD (Mitchell et al., 2022)
 ,
Zhao et al. (2023b)
Improving Factuality ITI (Li et al., 2023c)
 , DoLa (Chuang et al., 2023)
Figure 3: Taxonomy of knowledge conflicts. We mainly list works in the era of LLMs.
 denotes pre-hoc solution
and
 denotes post-hoc solution.
minimize the undesirable consequences of knowl-
edge conflicts, i.e., to encourage the model to ex-
hibit desired behaviors that conform to specific
objectives (please noted that these objectives may
differ based on the particular scenario). Based on
the timing relative to potential conflicts, strategies
are divided into two categories: pre-hoc andpost-
hocstrategies. The key distinction between them
lies in whether adjustments are made before orafter potential conflicts arise1. The taxonomy of
knowledge conflicts is outlined in Figure 3. We
sequentially discuss the three kinds of knowledge
conflicts, detailing for each the causes, analysis of
model behaviors, and available solutions organized
according to their respective objectives. Related
datasets can be found in Table 1.
1Another interpretation is that a pre-hoc strategy is proac-
tive while a post-hoc one is reactive.Datasets Approach1Base2Size Conflict
Xie et al. (2023) Gen PopQA (2023), S TRATEGY QA ((Geva et al., 2021)) 20,091 CM3
KC (2023h) Sub N/A (LLM generated) 9,803 CM
KRE (2023) Gen MuSiQue (2022), SQuAD2.0 (2018), ECQA (2021), e-CARE (2022a) 11,684 CM
Farm (2023) Gen BoolQ (2019), NQ (2019), TruthfulQA (2022) 1,952 CM
Tan et al. (2024) Gen NQ (2019), TriviaQA (2017) 14,923 CM
WikiContradiction (2021) Hum Wikipedia 2,210 IC
ClaimDiff (2022) Hum N/A 2,941 IC
Pan et al. (2023a) Gen,Sub SQuAD v1.1 (2016) 52,189 IC
CONTRA DOC(2023a) Gen CNN-DailyMail (2015), NarrativeQA (2018), WikiText (2017) 449 IC
CONFLICTING QA(2024) Gen N/A 238 IC
PARAREL(2021) Hum T-REx (2018) 328 IM
1. Approach refers to how the conflicts are crafted, including entity-level substitution (Sub), generative approaches
employing an LLM (Gen), and human annotation (Hum).
2. Base refers to the base dataset(s) that serve as the foundation for generating conflicts, if applicable.
3.
 When using CM datasets, conflicts arise from the specific model‚Äôs parametric knowledge, which can differ
across models. Therefore, selecting a subset of the dataset that aligns with the tested model‚Äôs knowledge is crucial.
Table 1: Datasets on evaluating LLMs‚Äô behavior when encountering knowledge conflicts. CM: context-memory
conflict, IC: inter-context conflict, IM: intra-memory conflict.
2 Context-Memory Conflict
Context-memory conflict emerges as the most ex-
tensively investigated among the three types of con-
flicts. LLMs are characterized by fixed paramet-
ric knowledge, a result of the substantial pertain-
ing process (Sharir et al., 2020; Hoffmann et al.,
2022; Smith, 2023). This static parametric knowl-
edge stands in stark contrast to the dynamic nature
of external information, which evolves at a rapid
pace (De Cao et al., 2021; Kasai et al., 2022).
2.1 Causes
The core of context-memory conflict stems from
a discrepancy between the context and parametric
knowledge. We consider two main causes: tempo-
ral misalignment (Lazaridou et al., 2021; Luu et al.,
2021; Dhingra et al., 2022) and misinformation
pollution (Du et al., 2022b; Pan et al., 2023a).
Temporal Misalignment. Temporal misalignment
naturally arises in models trained on data collected
in the past, as they may not accurately reflect con-
temporary or future realities ( i.e., the contextual
knowledge after the deployment) (Luu et al., 2021;
Lazaridou et al., 2021; Liska et al., 2022). Such
misalignment can degrade the model‚Äôs performance
and relevancy over time, as it may fail to capture
new trends, shifts in language use, cultural changes,
or updates in knowledge. Researchers have noted
that temporal misalignment relegates the model‚Äôs
performance on various NLP tasks (Luu et al.,
2021; Zhang and Choi, 2021; Dhingra et al., 2022;
Kasai et al., 2022; Cheang et al., 2023). Further-
more, the issue of temporal misalignment is ex-pected to intensify due to the pre-training paradigm
and the escalating costs associated with scaling up
models (Kaplan et al., 2020).
Prior work tries to tackle temporal misalignment
by focusing on three lines of strategies: Knowl-
edge editing (KE) aims to directly update the
parametric knowledge of an existing pre-trained
model (Sinitsin et al., 2020; De Cao et al., 2021;
Mitchell et al., 2021; Onoe et al., 2023). Retrieval-
augmented generation (RAG) leverages a retrieval
module to fetch relevant documents from external
sources ( e.g., database, the Web) to supplement
the model‚Äôs knowledge without altering its parame-
ters (Karpukhin et al., 2020; Guu et al., 2020; Lewis
et al., 2020; Lazaridou et al., 2022; Borgeaud et al.,
2022; Peng et al., 2023; Vu et al., 2023). Continue
learning (CL) seeks to update the internal knowl-
edge through continual pre-training on new and
updated data (Lazaridou et al., 2021; Jang et al.,
2021, 2022). However, these methods of mitigat-
ing temporal misalignment are not magic bullets.
KE can bring in side effects of knowledge conflict,
leading to knowledge inconsistency ( i.e., a sort of
intra-memory conflict) and may even enhance the
hallucination of LLMs (Li et al., 2023f; Pinter and
Elhadad, 2023). For RAG, it is inevitable to en-
counter knowledge conflicts since the model‚Äôs pa-
rameters are not updated (Chen et al., 2021; Zhang
and Choi, 2021). CL suffers from catastrophic
forgetting issues and demands significant computa-
tional resources (De Lange et al., 2021; He et al.,
2021; Wang et al., 2023g).
Misinformation Pollution. Misinformation pol-
lution emerges as another contributor to context-memory conflict, particularly for time-invariant
knowledge (Jang et al., 2021). Adversaries exploit
this vulnerability by introducing false or mislead-
ing information into the Web corpus of retrieved
documents (Pan et al., 2023a,b; Weller et al., 2022)
and user conversations (Xu et al., 2023; Hu et al.,
2024). The latter poses a practical threat, as ad-
versaries can leverage techniques such as prompt
injection attacks (Liu et al., 2023b; Greshake et al.,
2023; Yi et al., 2023). This vulnerability poses a
real threat, as models might unknowingly propa-
gate misinformation if they incorporate deceptive
inputs without scrutiny (Xie et al., 2023; Pan et al.,
2023b; Xu et al., 2023).
Fabricated, malicious misinformation can
markedly undermine the accuracy of automated
fact-checking (Du et al., 2022b) and open-domain
question-answering systems (Pan et al., 2023a,b).
Furthermore, recent studies also highlight the
model‚Äôs tendency to align with user opinions, a.k.a. ,
sycophancy , further exacerbating the issue (Perez
et al., 2022; Turpin et al., 2023; Wei et al., 2023;
Sharma et al., 2023). In the current landscape
of LLMs, there is growing apprehension in the
NLP community regarding the potential generation
of misinformation by LLMs (Ayoobi et al., 2023;
Kidd and Birhane, 2023; Carlini et al., 2023; Zhou
et al., 2023c; Spitale et al., 2023; Chen and Shu,
2023b). Researchers acknowledge the challenges
associated with detecting misinformation generated
by LLMs (Tang et al., 2023; Chen and Shu, 2023a;
Jiang et al., 2023). This underscores the urgency of
addressing the nuanced challenges posed by LLMs
in the context of contextual misinformation.
Remarks. Temporal misalignment and mis-
information pollution are two separate scenarios
that give rise to context-memory conflicts. For
the former, the up-to-date contextual information
is considered accurate. Conversely , for the latter,
the contextual information contains misinformation
and is therefore considered incorrect.
2.2 Analysis of Model Behaviors
How do LLMs navigate context-memory conflicts?
This section will detail the relevant research, al-
though they present quite different answers. De-
pending on the scenario, we first introduce the
Open-domain question answering (ODQA) setup
and then focus on general setups.
ODQA. In earlier ODQA literature, Longpre et al.
(2021) explore how QA models act when theprovided contextual information contradicts the
learned information. The authors create an auto-
mated framework that identifies QA instances with
named entity answers, then substitutes mentions
of the entity in the gold document with an alter-
nate entity, thus creating the conflict context. This
study reveals a tendency of these models to over-
rely on parametric knowledge. Chen et al. (2022)
revisit this setup while reporting differing obser-
vations, they note that models predominantly rely
on contextual knowledge in their best-performing
settings. They attribute this divergence in findings
to two factors. Firstly, the entity substitution ap-
proach used by Longpre et al. (2021) potentially
reduces the semantic coherence of the perturbed
passages. Secondly, Longpre et al. (2021) based
their research on single evidence passages, as op-
posed to Chen et al. (2022), who utilize multiple
ones. Recently, with the emergence of really ‚Äúlarge‚Äù
language models such as ChatGPT (Ouyang et al.,
2022; OpenAI, 2023) and Llama 2 (Touvron et al.,
2023), inter alia , researchers re-examined this is-
sue. Tan et al. (2024) examine how LLMs blend
retrieved context with generated knowledge in the
ODQA setup, and discover models tend to favor the
parametric knowledge, influenced by the greater re-
semblance of these generated contexts to the input
questions and the often incomplete nature of the
retrieved information, especially within the scope
of conflicting sources.
General. Xie et al. (2023) leverage LLMs to gen-
erate conflicting context alongside the memorized
knowledge. They find that LLMs are highly recep-
tive to external evidence, even when it conflicts
with their parametric, provided that the external
knowledge is coherent and convincing. Meanwhile,
they also identify a strong confirmation bias (Nick-
erson, 1998) in LLMs, i.e., the models tend to favor
information consistent with their internal memory,
even when confronted with conflicting external evi-
dence. Wang et al. (2023h) posit that the desired be-
haviors when an LLM encounters conflicts should
be to pinpoint the conflicts and provide distinct an-
swers. While LLMs perform well in identifying
the existence of knowledge conflicts, they struggle
to determine the specific conflicting segments and
produce a response with distinct answers amidst
conflicting information. Ying et al. (2023) analyze
the robustness of LLMs under conflicts with a focus
on two perspectives: factual robustness (the ability
to identify correct facts from prompts or memory)and decision style (categorizing LLMs‚Äô behavior as
intuitive, dependent, or rational-based on cognitive
theory). The study finds that LLMs are highly sus-
ceptible to misleading prompts, especially in the
context of commonsense knowledge. Qian et al.
(2023) evaluate the potential interaction between
parametric and external knowledge more system-
atically, cooperating knowledge graph (KG). They
reveal that LLMs often deviate from their paramet-
ric knowledge when presented with direct conflicts
or detailed contextual changes. Xu et al. (2023)
study how LLMs respond to knowledge conflicts
during interactive sessions. Their findings suggest
LLMs tend to favor logically structured knowledge,
even when it contradicts factual accuracy.
Remarks. I. Crafting Conflicting Knowledge.
Model‚Äôs behavior under context-memory conflict is
analyzed by artificially creating conflicting knowl-
edge, in early years through entity-level substitu-
tions and more recently by employing LLMs to
generate semantically coherent conflicts.
II. What is the conclusion? No definitive rule ex-
ists for whether a model prioritizes contextual or
parametric knowledge. Yet, knowledge that is se-
mantically coherent, logical, and compelling is typ-
ically favored by models over generic conflicting
information.
2.3 Solutions
Solutions are organized according to their objec-
tives ,i.e., the desired behaviors we expect from
an LLM when it encounters conflicts. Existing
strategies can be categorized into the following
objectives: Faithful to context strategies aim to
align with contextual knowledge, focusing on con-
text prioritization. Discriminating misinformation
strategies encourage skepticism towards dubious
context in favor of parametric knowledge. Disen-
tangling sources strategies treat context and knowl-
edge separately and provide disentangled answers.
Improving factuality strategies aim for an integrated
response leveraging both context and parametric
knowledge towards a more truthful solution.
Faithful to Context. Fine-tuning. Li et al.
(2022a) argue that an LLM should prioritize con-
text for task-relevant information and rely on
internal knowledge when the context is unre-
lated. They name the two properties controllabil-
ity and robustness. They introduce Knowledge
Aware FineTuning (KAFT) to strengthen the two
properties by incorporating counterfactual and ir-relevant contexts into standard training datasets.
TrueTeacher (Gekhman et al., 2023) focuses on
improving factual consistency in summarization
by annotating model-generated summaries with
LLMs. This approach helps in maintaining faith-
fulness to the context of the original documents,
ensuring that generated summaries remain accu-
rate without being misled by irrelevant or incorrect
details. DIAL (Xue et al., 2023) improves factual
consistency in dialogue systems via direct knowl-
edge enhancement and reinforcement learning for
factual consistency (RLFC) for aligning responses
accurately with provided factual knowledge.
Prompting. Zhou et al. (2023d) explores enhancing
LLMs‚Äô adherence to context through specialized
prompting strategies, specifically opinion-based
prompts and counterfactual demonstrations. These
techniques are shown to significantly improve
LLMs‚Äô performance in context-sensitive tasks by
ensuring they remain faithful to relevant context,
without additional training.
Decoding. Shi et al. (2023a) introduce Context-
aware Decoding (CAD) to reduce hallucinations
by amplifying the difference in output probabilities
with and without context, similar to the concept
of contrastive decoding (Li et al., 2022c). CAD
enhances faithfulness in LLMs by prioritizing rel-
evant context over the model‚Äôs prior knowledge,
especially in tasks with conflicting information.
Knowledge Plug-in. Lee et al. (2022a) propose
Continuously-updated QA (CuQA) for improving
LMs‚Äô ability to integrate new knowledge. Their
approach uses plug-and-play modules to store up-
dated knowledge, ensuring the original model re-
mains unaffected. Unlike traditional continued pre-
training or fine-tuning approaches, CuQA can solve
knowledge conflicts.
Pre-training. ICLM (Shi et al., 2023b) is a new
pre-training method that extends LLMs‚Äô ability to
handle long and varied contexts across multiple
documents. This approach could potentially aid in
resolving knowledge conflicts by enabling models
to synthesize information from broader contexts,
thus improving their understanding and application
of relevant knowledge.
Predict Fact Validity. Zhang and Choi (2023) ad-
dress knowledge conflict by introducing fact du-
ration prediction to identify and discard outdated
facts in LLMs. This approach improves model
performance on tasks like ODQA by ensuring ad-
herence to up-to-date contextual information.Discriminating Misinformation (Faithful to
Memory). Prompting. To address misinformation
pollution, Pan et al. (2023b) propose defense strate-
gies such as misinformation detection and vigilant
prompting, aiming to enhance the model‚Äôs ability
to remain faithful to factual, parametric informa-
tion amidst potential misinformation. Similarly, Xu
et al. (2023) utilize a system prompt to remind the
LLM to be cautious about potential misinforma-
tion and to verify its memorized knowledge before
responding. This approach aims to enhance the
LLM‚Äôs ability to maintain faithfulness.
Query Augmentation. Weller et al. (2022) leverage
the redundancy of information in large corpora to
defend misinformation pollution. Their method in-
volves query augmentation to find a diverse set of
less likely poisoned passages, coupled with a confi-
dence method named Confidence from Answer Re-
dundancy, which compares the predicted answer‚Äôs
consistency across retrieved contexts. This strat-
egy mitigates knowledge conflicts by ensuring the
model‚Äôs faithfulness through the cross-verification
of answers from multiple sources.
Training Discriminator. Hong et al. (2023) fine-
tune a smaller LM as a discriminator and com-
bine prompting techniques to develop the model‚Äôs
ability to discriminate between reliable and unreli-
able information, helping the model remain faithful
when confronted with misleading context.
Disentangling Sources. DisentQA (Neeman et al.,
2022) trains a model that predicts two types of
answers for a given question: one based on contex-
tual knowledge and one on parametric knowledge.
Wang et al. (2023h) introduce a method to improve
LLMs‚Äô handling of knowledge conflicts. Their
approach is a three-step process designed to help
LLMs detect conflicts, accurately identify the con-
flicting segments, and generate distinct, informed
responses based on the conflicting data, aiming for
more precise and nuanced model outputs.
Improving Factuality. Zhang et al. (2023e)
propose COMBO, a framework that pairs com-
patible generated and retrieved passages to re-
solve discrepancies. It uses discriminators trained
on silver labels to assess passage compatibil-
ity, improving ODQA performance by leveraging
both LLM-generated (parametric) and external re-
trieved knowledge. Jin et al. (2024a) introduce
a contrastive-decoding-based algorithm, namely
CD2, which maximizes the difference between var-
ious logits under knowledge conflicts and calibratesthe model‚Äôs confidence in the truthful answer.
Remarks. Current mitigation approaches have
contradicted goals because they do not distinguish
between the two causes of knowledge conflict when
considering conflict scenarios. Blindly being ‚Äúfaith-
ful‚Äù to context or knowledge is undesirable. Some
researchers regard that LLM should not rely solely
on either parametric or contextual information but
instead grant LLM users the agency to make in-
formed decisions based on distinct answers (Wang
et al., 2023h; Floridi, 2023).
3 Inter-Context Conflict
Inter-context conflicts manifest in LLMs when in-
corporating external information sources, a chal-
lenge accentuated by the advent of RAG techniques.
RAG enriches the LLM‚Äôs responses by integrat-
ing content from retrieved documents into the con-
text. Nonetheless, this incorporation can lead to
inconsistencies within the provided context, as the
external documents may contain information that
conflicts with each other (Zhang and Choi, 2021;
Kasai et al., 2022; Li et al., 2023a).
3.1 Causes
Misinformation. Misinformation has long been a
significant concern in the modern digital age (Shu
et al., 2017; Zubiaga et al., 2018; Kumar and
Shah, 2018; Meel and Vishwakarma, 2020; Fung
et al., 2022; Wang et al., 2023b). The emergence
of RAG incorporates external documents to en-
hance the generation quality of LLMs. While RAG
has the potential to enrich content with diverse
knowledge sources, it also poses the risk of includ-
ing documents containing misinformation, such as
fake news (Chen et al., 2023b). Moreover, there
have been instances where AI technologies are em-
ployed to create or propagate misinformation (Wei-
dinger et al., 2021; Zhou et al., 2023c; Vergho et al.,
2024). The advanced generative capabilities of
LLMs exacerbate this issue, leading to an increase
in misinformation generated by these systems. This
trend is concerning, as it not only contributes to the
spread of false information but also challenges de-
tecting misinformation generated by LLMs (Chen
and Shu, 2023b; Menczer et al., 2023; Barrett et al.,
2023; Bengio et al., 2023; Wang et al., 2023c; So-
laiman et al., 2023; Weidinger et al., 2023; Ferrara,
2023; Goldstein et al., 2023).
Outdated Information. In addition to the chal-
lenge of misinformation, it is important to rec-ognize that facts can evolve. The retrieved doc-
uments may contain updated and outdated infor-
mation from the network simultaneously, leading
to conflicts between these documents (Chen et al.,
2021; Liska et al., 2022; Zhang and Choi, 2021;
Kasai et al., 2022; Schlichtkrull et al., 2023).
Remarks. Conflicts in context frequently arise
between misinformation and accurate information,
as well as between outdated and updated informa-
tion. These two conflicts exert distinct impacts
on LLMs and require specified analysis. Distin-
guishing from misinformation conflicts, another
significant challenge involves addressing conflicts
that arise from documents bearing different times-
tamps, especially when a user‚Äôs prompt specifies a
particular time period.
3.2 Analysis of Model Behaviors
Performance Impact. Previous research empiri-
cally demonstrates that the performance of a pre-
trained language model can be significantly influ-
enced by the presence of misinformation (Zhang
and Choi, 2021) or outdated information (Du et al.,
2022b) within a specific context. In recent stud-
ies, Pan et al. (2023a) introduce a misinformation
attack strategy involving the creation of a fabri-
cated version of Wikipedia articles, which is subse-
quently inserted into the authentic Wikipedia cor-
pus. Their research findings reveal that existing
language models are susceptible to misinformation
attacks, irrespective of whether the fake articles are
manually crafted or generated by models. To gain
a deeper understanding of how LLMs behave when
encountering contradictory contexts, Chen et al.
(2022) primarily conduct experiments using Fusion-
in-Decoder on the NQ-Open (Kwiatkowski et al.,
2019) and TriviaQA (Joshi et al., 2017). They find
that inconsistencies across knowledge sources exert
a minimal effect on the confidence levels of models.
These models tend to favor context directly perti-
nent to the query and context that aligns with the
model‚Äôs inherent parametric knowledge. Xie et al.
(2023) conduct experiments on both closed-source
LLMs and open-source LLMs in POPQA(Mallen
et al., 2022) and STRATEGY QA(Geva et al., 2021).
The results obtained are in line with those of Chen
et al. (2022), indicating that LLMs exhibit a signif-
icant bias to evidence that aligns with the model‚Äôs
parametric memory. They also find that LLMs
exhibit a predisposition towards emphasizing infor-
mation related to entities of higher popularity andanswers that are corroborated by a larger volume
of documents within the given context. Moreover,
these models demonstrate a significant sensitivity
to the order in which data is introduced. Jin et al.
(2024a) discover that as the number of conflicting
hops increases, LLMs encounter increased chal-
lenges in reasoning.
Detection Ability. In addition to assessing the
performance of LLMs when confronted with con-
tradictory contexts, several studies also investi-
gate their capacity to identify such contradictions.
Zheng et al. (2022) examine the performance of
various models including BERT, RoBERTa, and
ERNIE in detecting the contradiction within Chi-
nese conversations. Experiments reveal that iden-
tifying contradictory statements within a conver-
sation is a significant challenge for these models.
Li et al. (2023a) analyse the performance of GPT-
4, PaLM-2, and Llama 2 in identifying contra-
dictory documents within news articles (Hermann
et al., 2015), stories (Ko Àácisk√Ω et al., 2018), and
wikipedia (Merity et al., 2017). The authors find
that the average detection accuracy is subpar. The
study also finds that LLMs face specific challenges
when addressing certain types of contradictions,
particularly those involving subjective emotions
or perspectives. Additionally, the length of docu-
ments and the variety of self-contradictions have a
minor influence on the detection performance. Wan
et al. (2024) investigate the text features that affect
LLMs‚Äô assessment of document credibility when
faced with conflicting information. They discover
that existing models heavily prioritize the relevance
of a document to the query but often overlook stylis-
tic features that humans consider important, such
as the presence of scientific references or a neutral
tone in the text. Jin et al. (2024a) discover that
LLMs encounter difficulty in distinguishing truth-
ful information from misinformation. In addition,
they find that LLMs favor evidence that appears
most frequently within the context, and demon-
strate confirmation bias for external information
aligning with their internal memory.
Remarks. When encountering conflict within
a given context, the exhibited knowledge of the
LLMs is significantly influenced. However, deter-
mining how the model responds to various con-
textual nuances remains an area requiring further
exploration. While different models may share cer-
tain commonalities, disparities in behavior arise
due to variations in their training data. Moreover,as the model‚Äôs knowledge is derived from textual
information, its approach to discerning misinfor-
mation differs significantly from that of humans.
3.3 Solutions
Eliminating Conflict. Specialized Models. Hsu
et al. (2021) develop a model named Pairwise
Contradiction Neural Network (PCNN), leveraging
fine-tuned Sentence-BERT embeddings to calcu-
late contradiction probabilities of articles. Pielka
et al. (2022) suggest incorporating linguistic knowl-
edge into the learning process based on the discov-
ery that XLM-RoBERTa struggles to effectively
grasp the syntactic and semantic features that are
vital for accurate contradiction detection. Wu et al.
(2022) propose an innovative approach that inte-
grates topological representations of text into lan-
guage models to enhance the contradiction detec-
tion ability and evaluated their methods on the
MultiNLI dataset (Williams et al., 2018).
General Models. Chern et al. (2023) propose a fact-
checking framework that integrates LLMs with
various tools, including Google Search, Google
Scholar, code interpreters, and Python, for detect-
ing factual errors in texts. Leite et al. (2023) em-
ploy LLMs to generate weak labels associated with
predefined credibility signals for the input text and
aggregate these labels through weak supervision
techniques to make predictions regarding the verac-
ity of the input.
Improving Robustness. Training Approach. Hong
et al. (2023) present a novel fine-tuning method that
involves training a discriminator and a decoder si-
multaneously using a shared encoder. Additionally,
the authors introduce two other strategies to im-
prove the robustness of the model including prompt-
ing GPT-3 to identify perturbed documents before
generating responses and integrating the discrim-
inator‚Äôs output into the prompt for GPT-3. Their
experimental results indicate that the fine-tuning
method yields the most promising results.
Query Augmentation. Weller et al. (2022) explore a
query augmentation technique that prompts GPT-3
to formulate new questions derived from the orig-
inal inquiry. They then assess the confidence for
each answer by referencing the corresponding pas-
sages retrieved. Based on the confidence, they
decide whether to rely on the original question‚Äôs
prediction or aggregate predictions from the aug-
mented questions with high confidence scores.
Remarks. Strategies for addressing inter-context conflicts primarily rely on model knowl-
edge or leveraging external knowledge such as re-
trieved documents. Recently, augmenting LLM
with external tools has emerged as a new paradigm.
Exploring the use of external tools to support
LLMs in resolving inter-context conflicts could
be a promising approach. On the other hand, devis-
ing unified and efficient methods to handle various
conflict types remains a formidable challenge.
4 Intra-Memory Conflict
With the development of LLMs, LLMs are widely
used in knowledge-intensive question-and-answer
systems (Gao et al., 2023b; Yu et al., 2022; Petroni
et al., 2019; Chen et al., 2023c). A critical aspect
of deploying LLMs effectively involves ensuring
that they produce consistent outputs across var-
ious expressions that share similar meanings or
intentions. Despite this necessity, a notable chal-
lenge arises with intra-memory conflict‚Äîa con-
dition where LLMs exhibit unpredictable behav-
iors and generate differing responses to inputs that
are semantically equivalent but syntactically dis-
tinct (Chang and Bergen, 2023; Chen et al., 2023a;
Raj et al., 2023; Rabinovich et al., 2023; Raj et al.,
2022; Bartsch et al., 2023). Intra-memory conflict
essentially undermines the reliability and utility of
LLMs by introducing a degree of uncertainty in
their output.
4.1 Causes
Intra-memory conflicts within LLMs can be at-
tributed to three primary factors: training corpus
bias (Wang et al., 2023d; Xu et al., 2022), decoding
strategies Lee et al. (2022b); Huang et al. (2023),
and knowledge editing (Yao et al., 2023; Li et al.,
2023f). These factors respectively pertain to the
training phase, the inference phase, and subsequent
knowledge refinement.
Bias in Training Corpora. Recent research
demonstrates that the primary phase for knowledge
acquisition in LLMs predominantly occurs in the
pre-training stage (Zhou et al., 2023a; Kaddour
et al., 2023; Naveed et al., 2023; Aky√ºrek et al.,
2022; Singhal et al., 2022). Pre-training corpus is
primarily crawled from the internet, which exhibits
a diverse range of data quality, potentially includ-
ing inaccurate or misleading information (Bender
et al., 2021; Weidinger et al., 2021). When LLMs
are trained on data containing incorrect knowl-
edge, they may memorize and inadvertently am-plify these inaccuracies (Lin et al., 2022; Elazar
et al., 2022; Lam et al., 2022; Grosse et al., 2023),
leading to a situation where conflicting knowledge
coexists within the parameters of LLMs.
Moreover, prior works indicate that LLMs tend
to encode superficial associations prevalent within
their training data, as opposed to genuinely com-
prehending the underlying knowledge contained
therein (Li et al., 2022b; Kang and Choi, 2023;
Zhao et al., 2023a; Kandpal et al., 2023). It can
result in LLMs displaying a propensity to generate
predetermined responses rooted in spurious corre-
lations of training data. Due to the dependency on
spurious correlations, LLMs may provide divergent
answers when presented with prompts exhibiting
distinct syntactic structures but conveying equiva-
lent semantic meaning.
Decoding Strategy. The direct output of LLMs
is a probability distribution over potential next to-
kens. Sampling is a crucial step in determining the
generated content from this distribution. Various
sampling techniques, including greedy sampling,
top-p sampling, top-k sampling, and others have
been proposed (Jawahar et al., 2020; Massarelli
et al., 2020), broadly categorizing into determin-
istic and stochastic sampling methods. Stochastic
sampling stands as the prevailing decoding strat-
egy employed by LLMs (Fan et al., 2018; Holtz-
man et al., 2020). However, the random nature of
stochastic sampling methods introduces uncertainty
into the generated content. Moreover, due to the
intrinsic left-to-right generation pattern inherent
to LLMs, the selection of the sampling token can
wield a significant influence over the subsequent
generations. The use of stochastic sampling may
cause LLMs to produce entirely different content,
even when provided with the same context, causing
intra-memory conflict (Lee et al., 2022b; Huang
et al., 2023; Dziri et al., 2021).
Knowledge Editing. With the exponential increase
of model parameters, fine-tuning LLMs become
increasingly challenging and resource-intensive.
In response to this challenge, researchers explore
knowledge editing techniques as a means of ef-
ficiently modify a small scope of the knowledge
encoded in LLMs (Meng et al., 2022; Ilharco et al.,
2022; Zhong et al., 2023). Ensuring the consistency
of modifications poses a significant challenge. Due
to the potential limitations inherent in the editing
method, the modified knowledge cannot be general-
ized effectively. This can result in LLMs producinginconsistent responses when dealing with the same
piece of knowledge in varying situations (Li et al.,
2023f; Yao et al., 2023). Intra-memory conflict is
primarily considered a side effect in the context of
knowledge editing.
Remarks. Intra-memory conflicts in LLMs
arise from three distinct causes that occur at differ-
ent stages. Among these causes, training corpus
bias stands out as the fundamental catalyst. Incon-
gruities of knowledge in the training dataset result
in inconsistencies within the knowledge encoded
within the model‚Äôs parameters. Additionally, the
decoding strategy indirectly contributes to exacer-
bating these conflicts. The inherent randomness
of the sampling process during inference ampli-
fies the inconsistencies in the model‚Äôs responses.
Knowledge editing, which aims to post-update the
model‚Äôs knowledge, can inadvertently introduce
conflicting information into the LLM‚Äôs memory.
4.2 Analysis of Model Behaviors
Self-Inconsistency. Elazar et al. (2021) develop a
method for assessing the knowledge consistency of
language models, focusing specifically on knowl-
edge triples. The authors primarily conduct ex-
periments using BERT, RoBERTa, and ALBERT.
Their findings indicate that these models exhibit
poor consistency, with accuracy rates barely rang-
ing from 50% to 60%. Hase et al. (2023) employ
the same indicators of Elazar et al. (2021), but they
utilize a more diverse dataset. Their study also re-
veals that the consistency of RoBERTa-base and
BART-base within the paraphrase context is lack-
ing. Zhao et al. (2023b) reformulate questions and
then assess the consistency of the LLM‚Äôs responses
to these reformulated questions. The findings of
their research reveal that even GPT-4 exhibits a
notable inconsistency rate of 13% when applied to
Commonsense Question-Answering tasks. They
further find that LLMs are more likely to produce
inconsistencies in the face of uncommon knowl-
edge. Dong et al. (2023) conduct experiments on
multiple open-source LLMs and find that all of
these models exhibit strong inconsistencies. Li
et al. (2023d) explore an additional aspect of in-
consistency that LLMs can give an initial answer
to a question, but it may subsequently deny the
previous answer when asked if it is correct. The au-
thors conduct experiments focusing on Close-Book
Question Answering and reveal that Alpaca-30B
only displays consistency in 50% of cases.To further analyze the inconsistency exhibited
by LLMs, a study conducted by Li et al. (2022b)
reveals that encoder-based models tend to gener-
ate mis-factual words more relying on positionally
close and highly co-occurring words, rather than
knowledge-dependent words. This phenomenon
arises due to these models‚Äô tendency to overlearn
inappropriate associations from the training dataset.
Kang and Choi (2023) highlight a co-occurrence
bias in LLMs, where the models favor frequently
co-occurring words over correct answers. espe-
cially when recalling facts where the subject and
object rarely co-occur in the pre-training dataset,
despite fine-tuning. Furthermore, their research in-
dicates that LLMs face challenges in recalling facts
in cases where the subject and object rarely appear
together in the pre-training dataset, even though
these facts are encountered during fine-tuning.
Latent Representation of Knowledge. The multi-
layer transformer architecture inherent to contem-
porary LLMs fosters a complex inter-memory con-
flict, with distinct knowledge representations scat-
tered across various layers. Previous research sug-
gests that LLMs store low-level information at shal-
lower levels and semantic information at deeper
levels (Tenney et al., 2019; Rogers et al., 2020;
Wang et al., 2019; Jawahar et al., 2019; Cui et al.,
2020). Chuang et al. (2023) explore this aspect
within the context of LLMs and discover that the
factual knowledge in LLMs is typically concen-
trated within specific transformer layers and differ-
ent layers of inconsistent knowledge. Moreover,
Li et al. (2023c) discover that the correct knowl-
edge is indeed stored within the parameters of the
model, but it may not be accurately expressed dur-
ing the generation process. The authors conduct
two experiments on the same LLM, one focused
on the generation accuracy, and the other utilizing
a knowledge probe to examine the knowledge con-
tainment. The results of these experiments reveal a
substantial 40% disparity between the knowledge
probe accuracy and the generation accuracy.
Cross-lingual Inconsistency. The universality
of true knowledge transcends surface form vari-
ations (Ohmer et al., 2023), a principle that should
ideally apply to LLMs. However, LLMs main-
tain distinct knowledge sets for different languages,
leading to inconsistencies (Ji et al., 2023; Xue et al.,
2024). Wang et al. (2023f) investigate the chal-
lenges LLMs face in extending edited knowledge
across languages, suggesting that knowledge re-lated to different languages is stored separately
within the model parameters. Qi et al. (2023) pro-
pose a metric named RankC for evaluating the
cross-lingual consistency of LLMs‚Äô factual knowl-
edge. They employ this metric for analyzing multi-
ple models and reveal a pronounced language de-
pendence in the knowledge stored by LLMs, with
no observed improvement in cross-lingual consis-
tency with increased model size.
Remarks. The phenomenon of inter-memory
conflict in LLMs predominantly manifests through
inconsistent responses to semantically identical
queries. This inconsistency is primarily attributed
to the suboptimal quality of datasets utilized during
the pre-training phase. Addressing this challenge
necessitates the development of efficient and cost-
effective solutions, which remains a significant hur-
dle. Additionally, LLMs are characterized by the
presence of multiple knowledge circuits, which sig-
nificantly influence their response mechanisms to
specific inquiries. The exploration and detailed ex-
amination of these knowledge circuits within LLMs
represent a promising avenue for future research.
4.3 Solutions
4.3.1 Improving Consistency
Fine-tuning. Elazar et al. (2021) propose a consis-
tency loss function and train the language model
with the combination of the consistency loss and
standard MLM loss. Li et al. (2023d) utilize one
language model in dual capacities: as a generator to
produce responses and as a validator to evaluate the
accuracy of these responses. The process involves
querying the generator for a response, which is sub-
sequently assessed by the validator for accuracy.
Only those pairs of responses deemed consistent
are retained. This subset of consistent pairs is then
used to fine-tune the model, aiming to increase the
generation likelihood of consistent response pairs.
Plug-in. Jang and Lukasiewicz (2023) leverage
the technique of intermediate training, utilizing
word-definition pairs from dictionaries to retrain
language models and improve their comprehension
of symbolic meanings. Subsequently, they propose
an efficient parameter integration approach, which
amalgamates these enhanced parameters with those
of existing language models. This method aims to
rectify the models‚Äô inconsistent behavior by bol-
stering their capacity to understand meanings.
Output Ensemble. Mitchell et al. (2022) mitigate
the inconsistency of language models by leveraginga two-model architecture, involving the utilization
of a base model responsible for generating a set
of potential answers, followed by a relation model
that evaluates the logical coherence among these
answers. The final answer is selected by consider-
ing both the base model‚Äôs and the relation model‚Äôs
beliefs. Zhao et al. (2023b) introduce a method
to detect whether a question may cause inconsis-
tency for LLMs. Specifically, they first use LLMs
to rephrase the original question and obtain corre-
sponding answers. They then cluster these answers
and examine the divergence. The detection is de-
termined based on the divergence level.
4.3.2 Improving Factuality
Chuang et al. (2023) propose a novel contrastive
decoding approach named DoLa. Specifically, the
authors develop a dynamic layer selection strat-
egy, choosing the appropriate premature layers and
mature layers. The next word‚Äôs output probabil-
ity is then determined by computing the differ-
ence in log probabilities of the premature layers
and the mature layers. Li et al. (2023c) propose
a similar method named ITI. They first identify
a sparse set of attention heads that exhibit high
linear probing accuracy for truthfulness, as mea-
sured by TruthfulQA (Lin et al., 2022). During
the inference phase, ITI shifts activations along
the truth-correlated direction, which is obtained
through knowledge probing. This intervention is
repeated autoregressively for every token during
completion. Both DoLa and ITI address the incon-
sistency of knowledge across the model‚Äôs different
layers to reduce factual errors in LLMs.
Remarks. The resolution of inter-memory con-
flict in LLMs typically entails three phases: train-
ing, generation, and post-hoc processing. The train-
ing phase method mainly focuses on mitigating
internal inconsistencies among model parameters.
Conversely, the generation and post-hoc phases
primarily involve algorithmic interventions aimed
at alleviating occurrences of inconsistent model
behavior. Nevertheless, the challenge persists in
addressing the inconsistency of parameter knowl-
edge without detrimentally impacting the overall
performance of LLMs.
5 Challenges and Future Directions
In this section, we provide a summary and highlight
the existing challenges in ongoing research, as well
as outline potential future directions in the field of
knowledge conflict.Knowledge Conflicts in the Wild. Currently, the
creation of knowledge conflicts predominantly re-
lies on the artificial generation of incorrect or mis-
leading information. In the real world, one of the
most common situations where knowledge con-
flicts arise is in RALMs (Retrieval-Augmented Lan-
guage Models), where conflicts are present in doc-
uments retrieved by the retrieval module directly
from the Web. Current analysis approaches exist
a gap in the experimental setup of knowledge con-
flict, suggesting that findings from those environ-
ments (Xie et al., 2023; Wang et al., 2023h) might
not easily transfer to practical applications. Recent
studies have begun to investigate the scenario in the
wild by curating conflicting documents based on
actual search results from Google for open-ended
questions (Wan et al., 2024). Looking ahead, there
is a growing interest in more research that assesses
how well LLMs perform in real-world scenarios
rather than artificially created conflicts, to better
understand their capabilities.
Solution at a Finer Resolution. Currently, there
is no one-size-fits-all solution to knowledge con-
flict due to its inherent complexity. Existing ei-
ther assume a prior (Shi et al., 2023b) or focus on
a subclass of conflict (Wang et al., 2023h). We
believe that addressing this issue requires a more
fine-grained approach, taking into account several
factors. Firstly, the nature of the user‚Äôs query plays
a crucial role. Subjective or debatable questions
naturally lead to conflicts as they may have multi-
ple valid answers (Bjerva et al., 2020; Wan et al.,
2024). Secondly, the source of conflicting informa-
tion can vary, including misinformation, outdated
facts, or partially correct data (Guo et al., 2022;
Akhtar et al., 2023). Lastly, it is also important to
consider user expectations, such as whether they
prefer a single definitive answer from the LLM or
are open to multiple perspectives (Floridi, 2023).
Given these considerations, future solutions to mit-
igate knowledge conflicts must delve into these
nuances, recognizing that knowledge conflict en-
compasses a spectrum of problems with diverse
causes, manifestations, and potential resolutions.
Collaboration between NLP and HCI researchers is
appreciated for conducting thorough investigations
and developing effective solutions.
Evaluation on Downstream Tasks. The current
landscape of research into knowledge conflicts
within LLMs predominantly emphasizes evaluat-
ing their performance on common QA datasets in-cluding NQ-Open, TriviaQA, OPQA, and STRAT -
EGYQA. This focus overlooks the broader implica-
tions of knowledge conflicts, particularly how they
influence downstream tasks. Exploring the effects
of knowledge conflicts on a wider range of appli-
cations beyond QA problems could yield insights
into creating more robust and reliable models. For
instance, in tasks requiring high levels of accu-
racy and consistency, such as legal document anal-
ysis (Shui et al., 2023; Martin et al., 2024), medi-
cal diagnosis (Zhou et al., 2023b; Thirunavukarasu
et al., 2023), financial analysis (Zhang et al., 2023a;
Li et al., 2023e) and educational tools (Caines et al.,
2023; Milano et al., 2023), the presence of unre-
solved knowledge conflicts could undermine the
model‚Äôs utility.
Interplay among the Conflicts. Current research
in the knowledge conflict of LLMs primarily con-
centrates on investigating conflicts of a singular
type (Wang et al., 2023h; Chen et al., 2022; Li
et al., 2023d) or a joint study of inter-context and
context-memory conflict (Jin et al., 2024a; Xie
et al., 2023). However, there is a notable dearth of
research on the interaction between intra-memory
conflict and the other two types of conflicts. Several
papers have proposed the existence of knowledge
circuits in LLMs (Chughtai et al., 2024; Huang
et al., 2023), which are closely related to the intra-
memory conflict. Addressing this gap is crucial
for understanding the relationship between the in-
ternal knowledge inconsistency of the model and
its behavior in response to the context. Moreover,
exploring the synergistic effects of various con-
flict types could unveil underlying mechanisms of
knowledge representation and processing in LLMs
and help us to develop more robust and accurate
LLMs in practice.
Explainability. Most recent work analyzed LLMs‚Äô
behaviors amidst knowledge conflicts at the out-
put level (Xie et al., 2023; Wang et al., 2023h).
While some studies have observed and explored
the model‚Äôs confidence in its output, i.e., logits (Xu
et al., 2023; Jin et al., 2024a; Wang et al., 2024b),
there has been less focus on the internal mechanism
of the model, like specific attention heads or neuron
activations during conflicts. This gap highlights a
need for more microscopic examinations to better
comprehend how models decide when encounter
conflicts. A recent study conducted by Jin et al.
(2024b) advances this by investigating the inter-
pretability of LLMs through information flow anal-ysis, pinpointing pivotal points for conflict mitiga-
tion. They discover there are some attention heads
with opposite effects in the later layers, where mem-
ory heads can recall knowledge from internal mem-
ory, and context heads can retrieve knowledge from
the external context. Inspired by this, Pruning Head
via Path Patching is introduced to resolve conflicts
efficiently without updating model parameters.
Multilinguality. To date, research on knowledge
conflict has primarily focused on the English lan-
guage. Future studies could expand in two di-
rections. Firstly, by examining LLMs to address
knowledge conflicts in non-English prompts, lever-
aging the many advanced non-English models avail-
able ( e.g., GLM (Zeng et al., 2022) for Chinese)
or LLMs with multilingual capability ( e.g., GPT-
4 (OpenAI, 2024)) and noting differences from
English to account for unique language character-
istics. Secondly, addressing inter-context conflict
where multiple documents in different languages
might be retrieved, potentially involving cross-
language knowledge conflicts. Solutions could
include employing translation systems (Demen-
tieva and Panchenko, 2021) or, for low-resource
languages, leveraging high-resource language evi-
dence (Xue et al., 2024) or employing knowledge
distillation techniques.
Multimodality. Current research on knowledge
conflicts mainly focused on the text modality, leav-
ing the study of these conflicts in multimodal con-
texts as a promising area for future exploration. As
LLMs evolve to process information across various
formats‚Äîimages (Alayrac et al., 2022; Li et al.,
2023b), video (Ju et al., 2022; Zhang et al., 2023b),
and audio (Borsos et al., 2023; Wu et al., 2023)‚Äî
the potential for conflicts escalates in growing com-
plexity. For instance, textual documents might
clash with visual data, or the tone of an audio clip
might contradict the content of an accompanying
caption. Future research on multimodal knowledge
conflicts could focus on crafting advanced LLMs
skilled in cross-modal reasoning and conflict reso-
lution across diverse data types. This effort neces-
sitates the enhancement of models‚Äô capabilities to
navigate the complex dynamics between different
modalities and the development of targeted datasets
for effective training and evaluation. Additionally,
exploring how users perceive and manage multi-
modal conflicts, such as discrepancies between text
and images, will offer valuable insights into im-
proving LLMs for better human interaction.6 Conclusion
Through this survey, we have extensively investi-
gated knowledge conflicts, shedding light on their
categorization, causes, how LLMs respond to these
conflicts, and possible solutions. Our findings re-
veal that knowledge conflict is a multifaceted is-
sue, with a model‚Äôs behavior being closely tied to
the particular type of conflicting knowledge. Be-
sides, there appears to be a more complex interplay
among the three types of conflicts. Furthermore, we
observe that existing solutions primarily address ar-
tificially constructed scenarios, neglecting the sub-
tleties of conflicts by relying on assumed priors and
thus sacrificing granularity and breadth. Given the
growing use of retrieval-augmented language mod-
els, we anticipate that knowledge conflicts faced by
LLMs will only increase in complexity, underscor-
ing the need for more comprehensive research.
Limitations
Considering the rapid expansion of research in the
field of knowledge conflict and the abundance of
scholarly literature, it is possible that we might
have missed some of the most recent or less rele-
vant findings. Nevertheless, we have ensured the
inclusion of all essential materials in our survey.
Ethics Statement
We mainly searched for papers published after 2021
using key terms including ‚Äúknowledge conflict‚Äù,
‚Äúknowledge inconsistency‚Äù, ‚Äúknowledge gap‚Äù, inter
alia, on Google Scholar and the ACL Anthology.
After initially identifying these papers, the authors
classified them through reading and continued to
track related but overlooked papers using their ci-
tations. We also used Google Scholar to follow up
on the latest papers citing these to avoid omissions.
For the quantitative analysis and comparison sec-
tion (¬ß A.1), we did not conduct computational ex-
periments but simply organized the result reported
in other literature as is.
References
Shourya Aggarwal, Divyanshu Mandowara, Vishwa-
jeet Agrawal, Dinesh Khandelwal, Parag Singla, and
Dinesh Garg. 2021. Explanations for Common-
senseQA: New Dataset and Models. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing(Volume 1: Long Papers) , pages 3050‚Äì3065, Online.
Association for Computational Linguistics.
Ayush Agrawal, Lester Mackey, and Adam Tauman
Kalai. 2023. Do language models know when
they‚Äôre hallucinating references? ArXiv preprint ,
abs/2305.18248.
Mubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo,
Oana Cocarascu, Elena Simperl, and Andreas Vla-
chos. 2023. Multimodal automated fact-checking: A
survey. In Findings of the Association for Compu-
tational Linguistics: EMNLP 2023, Singapore, De-
cember 6-10, 2023 , pages 5430‚Äì5448. Association
for Computational Linguistics.
Ekin Aky√ºrek, Tolga Bolukbasi, Frederick Liu, Bin-
bin Xiong, Ian Tenney, Jacob Andreas, and Kelvin
Guu. 2022. Towards tracing knowledge in language
models back to the training data. In Findings of the
Association for Computational Linguistics: EMNLP
2022 , pages 2429‚Äì2446.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in neural
information processing systems , 35:23716‚Äì23736.
Navid Ayoobi, Sadat Shahriar, and Arjun Mukherjee.
2023. The looming threat of fake and llm-generated
linkedin profiles: Challenges and opportunities for
detection and prevention. In Proceedings of the 34th
ACM Conference on Hypertext and Social Media ,
pages 1‚Äì10.
Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Car-
lini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury,
Mihai Christodorescu, Anupam Datta, Soheil Feizi,
et al. 2023. Identifying and mitigating the security
risks of generative ai. Foundations and Trends ¬Æin
Privacy and Security , 6(1):1‚Äì52.
Henning Bartsch, Ole Jorgensen, Domenic Rosati, Jason
Hoelscher-Obermaier, and Jacob Pfau. 2023. Self-
consistency of large language models under ambigu-
ity.ArXiv preprint , abs/2310.13439.
Emily M Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? In Proceedings of the 2021 ACM confer-
ence on fairness, accountability, and transparency ,
pages 610‚Äì623.
Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn
Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin
Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Had-
field, et al. 2023. Managing ai risks in an era of rapid
progress. ArXiv preprint , abs/2310.17688.
Johannes Bjerva, Nikita Bhutani, Behzad Golshan,
Wang-Chiew Tan, and Isabelle Augenstein. 2020.
SubjQA: A Dataset for Subjectivity and Review Com-
prehension. In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 5480‚Äì5494, Online. Association
for Computational Linguistics.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning , pages 2206‚Äì2240. PMLR.
Zal√°n Borsos, Rapha√´l Marinier, Damien Vincent,
Eugene Kharitonov, Olivier Pietquin, Matt Shar-
ifi, Dominik Roblek, Olivier Teboul, David Grang-
ier, Marco Tagliasacchi, et al. 2023. Audiolm: a
language modeling approach to audio generation.
IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Andrew Caines, Luca Benedetto, Shiva Taslimipoor,
Christopher Davis, Yuan Gao, Oeistein Andersen,
Zheng Yuan, Mark Elliott, Russell Moore, Christo-
pher Bryant, et al. 2023. On the application of large
language models for language teaching and assess-
ment technology. ArXiv preprint , abs/2307.08393.
Nicholas Carlini, Matthew Jagielski, Christopher A
Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum
Anderson, Andreas Terzis, Kurt Thomas, and Florian
Tram√®r. 2023. Poisoning web-scale training datasets
is practical. ArXiv preprint , abs/2302.10149.
Tyler A Chang and Benjamin K Bergen. 2023. Lan-
guage model behavior: A comprehensive survey.
ArXiv preprint , abs/2303.11504.
Chi Cheang, Hou Chan, Derek Wong, Xuebo Liu, Zhao-
cong Li, Yanming Sun, Shudong Liu, and Lidia Chao.
2023. Can lms generalize to future data? an empiri-
cal analysis on text summarization. In Proceedings
of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 16205‚Äì16217.
Canyu Chen and Kai Shu. 2023a. Can llm-generated
misinformation be detected? In NeurIPS 2023 Work-
shop on Instruction Tuning and Instruction Follow-
ing.Canyu Chen and Kai Shu. 2023b. Combating misinfor-
mation in the age of llms: Opportunities and chal-
lenges. ArXiv preprint , abs/2311.05656.
Hung-Ting Chen, Michael JQ Zhang, and Eunsol Choi.
2022. Rich knowledge sources bring complex knowl-
edge conflicts: Recalibrating models to reflect con-
flicting evidence. ArXiv preprint , abs/2210.13701.
Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei
Li, and Yanghua Xiao. 2023a. Say what you mean!
large language models speak too positively about
negative commonsense knowledge. ArXiv preprint ,
abs/2305.05976.
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.
2023b. Benchmarking large language models in
retrieval-augmented generation. ArXiv preprint ,
abs/2309.01431.
Liang Chen, Yang Deng, Yatao Bian, Zeyu Qin, Bingzhe
Wu, Tat-Seng Chua, and Kam-Fai Wong. 2023c. Be-
yond factuality: A comprehensive evaluation of large
language models as knowledge generators. In Pro-
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pages 6325‚Äì
6341.
Wenhu Chen, Xinyi Wang, and William Yang Wang.
2021. A dataset for answering time-sensitive ques-
tions. ArXiv preprint , abs/2108.06314.
I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua
Feng, Chunting Zhou, Junxian He, Graham Neubig,
Pengfei Liu, et al. 2023. Factool: Factuality detec-
tion in generative ai‚Äìa tool augmented framework
for multi-task and multi-domain scenarios. ArXiv
preprint , abs/2307.13528.
Tsun-Hin Cheung and Kin-Man Lam. 2023. Factllama:
Optimizing instruction-following language models
with external knowledge for automated fact-checking.
In2023 Asia Pacific Signal and Information Pro-
cessing Association Annual Summit and Conference
(APSIPA ASC) , pages 846‚Äì853. IEEE.
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon
Kim, James Glass, and Pengcheng He. 2023. Dola:
Decoding by contrasting layers improves factual-
ity in large language models. ArXiv preprint ,
abs/2309.03883.
Bilal Chughtai, Alan Cooney, and Neel Nanda. 2024.
Summing up the facts: Additive mechanisms
behind factual recall in llms. ArXiv preprint ,
abs/2402.07321.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. BoolQ: Exploring the surprising
difficulty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 2924‚Äì2936, Minneapolis, Min-
nesota. Association for Computational Linguistics.Leyang Cui, Sijie Cheng, Yu Wu, and Yue Zhang. 2020.
Does bert solve commonsense task via commonsense
knowledge. ArXiv preprint , abs/2008.03945.
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-
ing factual knowledge in language models. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 6491‚Äì
6506.
Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ale≈° Leonardis, Gregory Slabaugh,
and Tinne Tuytelaars. 2021. A continual learning sur-
vey: Defying forgetting in classification tasks. IEEE
transactions on pattern analysis and machine intelli-
gence , 44(7):3366‚Äì3385.
Daryna Dementieva and Alexander Panchenko. 2021.
Cross-lingual evidence improves monolingual fake
news detection. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing: Student Research
Workshop , pages 310‚Äì320, Online. Association for
Computational Linguistics.
Bhuwan Dhingra, Jeremy R Cole, Julian Martin
Eisenschlos, Daniel Gillick, Jacob Eisenstein, and
William W Cohen. 2022. Time-aware language mod-
els as temporal knowledge bases. Transactions of the
Association for Computational Linguistics , 10:257‚Äì
273.
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,
Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Ja-
son Weston. 2023. Chain-of-verification reduces hal-
lucination in large language models. ArXiv preprint ,
abs/2309.11495.
Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Zhifang
Sui, and Lei Li. 2023. Statistical knowledge assess-
ment for large language models. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
Li Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin.
2022a. e-care: a new dataset for exploring explain-
able causal reasoning. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 432‚Äì446.
Yibing Du, Antoine Bosselut, and Christopher D Man-
ning. 2022b. Synthetic disinformation attacks on
automated fact verification systems. In Proceedings
of the AAAI Conference on Artificial Intelligence ,
volume 36, pages 10581‚Äì10589.
Nouha Dziri, Andrea Madotto, Osmar Zaiane, and
Avishek Joey Bose. 2021. Neural path hunter: Re-
ducing hallucination in dialogue systems via path
grounding. ArXiv preprint , abs/2104.08455.
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir
Feder, Abhilasha Ravichander, Marius Mosbach,Yonatan Belinkov, Hinrich Sch√ºtze, and Yoav Gold-
berg. 2022. Measuring causal effects of data statis-
tics on language model‚Äôsfactual‚Äôpredictions. ArXiv
preprint , abs/2207.14251.
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-
lasha Ravichander, Eduard Hovy, Hinrich Sch√ºtze,
and Yoav Goldberg. 2021. Measuring and improving
consistency in pretrained language models. Transac-
tions of the Association for Computational Linguis-
tics, 9:1012‚Äì1031.
Hady Elsahar, Pavlos V ougiouklis, Arslen Remaci,
Christophe Gravier, Jonathon Hare, Frederique Lafor-
est, and Elena Simperl. 2018. T-REx: A large scale
alignment of natural language with knowledge base
triples. In Proceedings of the Eleventh International
Conference on Language Resources and Evaluation
(LREC 2018) , Miyazaki, Japan. European Language
Resources Association (ELRA).
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 889‚Äì898, Melbourne, Australia. Association
for Computational Linguistics.
Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang,
Haotian Wang, Qianglong Chen, Weihua Peng, Xi-
aocheng Feng, Bing Qin, et al. 2023. Trends in inte-
gration of knowledge and large language models: A
survey and taxonomy of methods, benchmarks, and
applications. ArXiv preprint , abs/2311.05876.
Emilio Ferrara. 2023. Genai against humanity: Ne-
farious applications of generative artificial intelli-
gence and large language models. ArXiv preprint ,
abs/2310.00737.
Luciano Floridi. 2023. Ai as agency without intel-
ligence: on chatgpt, large language models, and
other generative models. Philosophy & Technology ,
36(1):15.
Yi R Fung, Kung-Hsiang Huang, Preslav Nakov, and
Heng Ji. 2022. The battlefront of combating misinfor-
mation and coping with media bias. In Proceedings
of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining , pages 4790‚Äì4791.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-
cent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,
et al. 2023a. Rarr: Researching and revising what
language models say, using language models. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 16477‚Äì16508.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen
Wang. 2023b. Retrieval-augmented generation for
large language models: A survey. ArXiv preprint ,
abs/2312.10997.Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen
Elkind, and Idan Szpektor. 2023. Trueteacher: Learn-
ing factual consistency evaluation with large lan-
guage models. ArXiv preprint , abs/2305.11171.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics , 9:346‚Äì
361.
Josh A Goldstein, Girish Sastry, Micah Musser, Re-
nee DiResta, Matthew Gentzel, and Katerina Sedova.
2023. Generative language models and automated
influence operations: Emerging threats and potential
mitigations. ArXiv preprint , abs/2301.04246.
Kai Greshake, Sahar Abdelnabi, Shailesh Mishra,
Christoph Endres, Thorsten Holz, and Mario Fritz.
2023. More than you‚Äôve asked for: A comprehen-
sive analysis of novel prompt injection threats to
application-integrated large language models. arXiv
e-prints , pages arXiv‚Äì2302.
Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage,
Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,
Dustin Li, Esin Durmus, Ethan Perez, et al. 2023.
Studying large language model generalization with
influence functions. ArXiv preprint , abs/2308.03296.
Zhijiang Guo, Michael Schlichtkrull, and Andreas Vla-
chos. 2022. A survey on automated fact-checking.
Transactions of the Association for Computational
Linguistics , 10:178‚Äì206.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,
and Ming-Wei Chang. 2020. Retrieval augmented
language model pre-training. In Proceedings of the
37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event , volume
119 of Proceedings of Machine Learning Research ,
pages 3929‚Äì3938. PMLR.
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-
nitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and
Srinivasan Iyer. 2023. Methods for measuring, up-
dating, and visualizing factual beliefs in language
models. In Proceedings of the 17th Conference of
the European Chapter of the Association for Compu-
tational Linguistics , pages 2706‚Äì2723.
Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing
Liu, James Glass, and Fuchun Peng. 2021. Analyzing
the forgetting problem in pretrain-finetuning of open-
domain dialogue response models. In Proceedings
of the 16th Conference of the European Chapter of
the Association for Computational Linguistics: Main
Volume , pages 1121‚Äì1133, Online. Association for
Computational Linguistics.
Karl Moritz Hermann, Tom√°s Kocisk√Ω, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In Advances in Neural InformationProcessing Systems 28: Annual Conference on Neu-
ral Information Processing Systems 2015, December
7-12, 2015, Montreal, Quebec, Canada , pages 1693‚Äì
1701.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks,
Johannes Welbl, Aidan Clark, et al. 2022. Train-
ing compute-optimal large language models. ArXiv
preprint , abs/2203.15556.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text
degeneration. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 . OpenReview.net.
Giwon Hong, Jeonghwan Kim, Junmo Kang, Sung-
Hyon Myaeng, and Joyce Jiyoung Whang. 2023. Dis-
cern and answer: Mitigating the impact of misinfor-
mation in retrieval-augmented models with discrimi-
nators. ArXiv preprint , abs/2305.01579.
Cheng Hsu, Cheng-Te Li, Diego Saez-Trumper, and Yi-
Zhan Hsu. 2021. Wikicontradiction: Detecting self-
contradiction articles on wikipedia. In 2021 IEEE
International Conference on Big Data (Big Data) ,
pages 427‚Äì436. IEEE.
Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo,
Lijie Wen, Philip S. Yu, and Zhijiang Guo. 2023. Do
large language models know about facts? CoRR ,
abs/2310.05177.
Xuming Hu, Xiaochuan Li, Junzhe Chen, Yinghui Li,
Yangning Li, Xiaoguang Li, Yasheng Wang, Qun
Liu, Lijie Wen, Philip S. Yu, and Zhijiang Guo.
2024. Evaluating robustness of generative search
engine on adversarial factual questions. CoRR ,
abs/2403.12077.
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.
A survey on hallucination in large language models:
Principles, taxonomy, challenges, and open questions.
ArXiv preprint , abs/2311.05232.
Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-
man, Suchin Gururangan, Ludwig Schmidt, Han-
naneh Hajishirzi, and Ali Farhadi. 2022. Edit-
ing models with task arithmetic. ArXiv preprint ,
abs/2212.04089.
Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,
Joongbo Shin, Janghoon Han, Gyeonghun Kim, and
Minjoon Seo. 2022. Temporalwiki: A lifelong bench-
mark for training and evaluating ever-evolving lan-
guage models. In Proceedings of the 2022 Confer-
ence on Empirical Methods in Natural Language
Processing , pages 6237‚Äì6250.
Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,
Janghoon Han, KIM Gyeonghun, Stanley Jungkyu
Choi, and Minjoon Seo. 2021. Towards continualknowledge learning of language models. In Interna-
tional Conference on Learning Representations .
Myeongjun Erik Jang and Thomas Lukasiewicz. 2023.
Improving language models meaning understanding
and consistency by learning conceptual roles from
dictionary. ArXiv preprint , abs/2310.15541.
Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks
Lakshmanan, V .S. 2020. Automatic detection of ma-
chine generated text: A critical survey. In Proceed-
ings of the 28th International Conference on Com-
putational Linguistics , pages 2296‚Äì2309, Barcelona,
Spain (Online). International Committee on Compu-
tational Linguistics.
Ganesh Jawahar, Beno√Æt Sagot, and Djam√© Seddah.
2019. What does BERT learn about the structure of
language? In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 3651‚Äì3657, Florence, Italy. Association for
Computational Linguistics.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto, and Pascale Fung. 2023. Survey of halluci-
nation in natural language generation. ACM Comput-
ing Surveys , 55(12):1‚Äì38.
Bohan Jiang, Zhen Tan, Ayushi Nirmal, and Huan
Liu. 2023. Disinformation detection: An evolv-
ing challenge in the age of llms. ArXiv preprint ,
abs/2309.15847.
Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiao-
jian Jiang, Jiexin Xu, Qiuxia Li, and Jun Zhao. 2024a.
Tug-of-war between knowledge: Exploring and re-
solving knowledge conflicts in retrieval-augmented
language models. ArXiv preprint , abs/2402.14409.
Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen,
Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu,
and Jun Zhao. 2024b. Cutting off the head ends the
conflict: A mechanism for interpreting and mitigat-
ing knowledge conflicts in language models. ArXiv
preprint , abs/2402.18154.
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 1601‚Äì1611, Vancouver,
Canada. Association for Computational Linguistics.
Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and
Weidi Xie. 2022. Prompting visual-language mod-
els for efficient video understanding. In European
Conference on Computer Vision , pages 105‚Äì124.
Springer.
Jean Kaddour, Joshua Harris, Maximilian Mozes, Her-
bie Bradley, Roberta Raileanu, and Robert McHardy.
2023. Challenges and applications of large language
models. ArXiv preprint , abs/2307.10169.Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric
Wallace, and Colin Raffel. 2023. Large language
models struggle to learn long-tail knowledge. In In-
ternational Conference on Machine Learning , pages
15696‚Äì15707. PMLR.
Cheongwoong Kang and Jaesik Choi. 2023. Impact
of co-occurrence on factual knowledge of large lan-
guage models. ArXiv preprint , abs/2310.08256.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. CoRR ,
abs/2001.08361.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6769‚Äì6781,
Online. Association for Computational Linguistics.
Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,
Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir
Radev, Noah A Smith, Yejin Choi, and Kentaro Inui.
2022. Realtime qa: What‚Äôs the answer right now?
ArXiv preprint , abs/2207.13332.
Celeste Kidd and Abeba Birhane. 2023. How ai can dis-
tort human beliefs. Science , 380(6651):1222‚Äì1223.
Miyoung Ko, Ingyu Seong, Hwaran Lee, Joonsuk Park,
Minsuk Chang, and Minjoon Seo. 2022. Claimdiff:
Comparing and contrasting claims on contentious
issues. ArXiv preprint , abs/2205.12221.
Tom√°≈° Ko Àácisk√Ω, Jonathan Schwarz, Phil Blunsom, Chris
Dyer, Karl Moritz Hermann, G√°bor Melis, and Ed-
ward Grefenstette. 2018. The NarrativeQA reading
comprehension challenge. Transactions of the Asso-
ciation for Computational Linguistics , 6:317‚Äì328.
Srijan Kumar and Neil Shah. 2018. False information
on web and social media: A survey. ArXiv preprint ,
abs/1804.08559.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: A benchmark for question answering
research. Transactions of the Association for Compu-
tational Linguistics , 7:452‚Äì466.
Tsz Kin Lam, Eva Hasler, and Felix Hieber. 2022. An-
alyzing the use of influence functions for instance-
specific data filtering in neural machine translation.
ArXiv preprint , abs/2210.13281.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech
Stokowiec, and Nikolai Grigorev. 2022. Internet-
augmented language models through few-shotprompting for open-domain question answering.
ArXiv preprint , abs/2203.05115.
Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya,
Devang Agrawal, Adam Liska, Tayfun Terzi, Mai
Gimenez, Cyprien de Masson d‚ÄôAutume, Tomas Ko-
cisky, Sebastian Ruder, et al. 2021. Mind the gap:
Assessing temporal generalization in neural language
models. Advances in Neural Information Processing
Systems , 34:29348‚Äì29363.
Kyungjae Lee, Wookje Han, Seung-won Hwang,
Hwaran Lee, Joonsuk Park, and Sang-Woo Lee.
2022a. Plug-and-play adaptation for continuously-
updated qa. In Findings of the Association for Com-
putational Linguistics: ACL 2022 , pages 438‚Äì447.
Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary,
Pascale N Fung, Mohammad Shoeybi, and Bryan
Catanzaro. 2022b. Factuality enhanced language
models for open-ended text generation. Advances in
Neural Information Processing Systems , 35:34586‚Äì
34599.
Jo√£o A Leite, Olesya Razuvayevskaya, Kalina
Bontcheva, and Carolina Scarton. 2023. Detect-
ing misinformation with llm-predicted credibility
signals and weak supervision. ArXiv preprint ,
abs/2309.07601.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih,
Tim Rockt√§schel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in Neu-
ral Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual .
Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin
Wang, Michal Lukasik, Andreas Veit, Felix Yu,
and Sanjiv Kumar. 2022a. Large language models
with controllable working memory. ArXiv preprint ,
abs/2211.05110.
Jierui Li, Vipul Raheja, and Dhruv Kumar. 2023a. Con-
tradoc: Understanding self-contradictions in docu-
ments with large language models. ArXiv preprint ,
abs/2311.09182.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023b. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. In International conference on ma-
chine learning , pages 19730‚Äì19742. PMLR.
Kenneth Li, Oam Patel, Fernanda Vi√©gas, Hanspeter
Pfister, and Martin Wattenberg. 2023c. Inference-
time intervention: Eliciting truthful answers from a
language model. ArXiv preprint , abs/2306.03341.
Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong,
Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang,
and Qun Liu. 2022b. How pre-trained language mod-
els capture factual knowledge? a causal-inspired anal-
ysis. ArXiv preprint , abs/2203.16747.Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy
Liang, Jason Eisner, Tatsunori Hashimoto, Luke
Zettlemoyer, and Mike Lewis. 2022c. Contrastive de-
coding: Open-ended text generation as optimization.
ArXiv preprint , abs/2210.15097.
Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tat-
sunori Hashimoto, and Percy Liang. 2023d. Bench-
marking and improving generator-validator con-
sistency of language models. ArXiv preprint ,
abs/2310.01846.
Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen.
2023e. Large language models in finance: A sur-
vey. In Proceedings of the Fourth ACM International
Conference on AI in Finance , pages 374‚Äì382.
Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang,
Xi Chen, and Huajun Chen. 2023f. Unveiling the pit-
falls of knowledge editing for large language models.
ArXiv preprint , abs/2310.02129.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
Truthfulqa: Measuring how models mimic human
falsehoods. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 3214‚Äì3252.
Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tay-
fun Terzi, Eren Sezener, Devang Agrawal, D‚ÄôAutume
Cyprien De Masson, Tim Scholtes, Manzil Zaheer,
Susannah Young, et al. 2022. Streamingqa: A bench-
mark for adaptation to new knowledge over time in
question answering models. In International Con-
ference on Machine Learning , pages 13604‚Äì13622.
PMLR.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023a. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Computing Surveys , 55(9):1‚Äì35.
Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tian-
wei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng,
and Yang Liu. 2023b. Prompt injection attack
against llm-integrated applications. ArXiv preprint ,
abs/2306.05499.
Shayne Longpre, Kartik Perisetla, Anthony Chen,
Nikhil Ramesh, Chris DuBois, and Sameer Singh.
2021. Entity-based knowledge conflicts in question
answering. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 7052‚Äì7063.
Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-
ishma Mandyam, and Noah A Smith. 2021. Time
waits for no one! analysis and challenges of temporal
misalignment. ArXiv preprint , abs/2111.07408.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi
Das, Hannaneh Hajishirzi, and Daniel Khashabi.
2022. When not to trust language models: Inves-
tigating effectiveness and limitations of paramet-
ric and non-parametric memories. ArXiv preprint ,
abs/2212.10511.Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 9802‚Äì9822.
Potsawee Manakul, Adian Liusie, and Mark JF Gales.
2023. Selfcheckgpt: Zero-resource black-box hal-
lucination detection for generative large language
models. ArXiv preprint , abs/2303.08896.
Katerina Margatina, Shuai Wang, Yogarshi Vyas,
Neha Anna John, Yassine Benajiba, and Miguel
Ballesteros. 2023. Dynamic benchmarking of
masked language models on temporal concept drift
with multiple views. ArXiv preprint , abs/2302.12297.
Lauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie
Catterson, and Rivindu Perera. 2024. Better call gpt,
comparing large language models against lawyers.
ArXiv preprint , abs/2401.16212.
Luca Massarelli, Fabio Petroni, Aleksandra Piktus,
Myle Ott, Tim Rockt√§schel, Vassilis Plachouras, Fab-
rizio Silvestri, and Sebastian Riedel. 2020. How de-
coding strategies affect the verifiability of generated
text. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020 , pages 223‚Äì235,
Online. Association for Computational Linguistics.
Priyanka Meel and Dinesh Kumar Vishwakarma. 2020.
Fake news, rumor, information pollution in social me-
dia and web: A contemporary survey of state-of-the-
arts, challenges and opportunities. Expert Systems
with Applications , 153:112986.
Filippo Menczer, David Crandall, Yong-Yeol Ahn, and
Apu Kapadia. 2023. Addressing the harms of ai-
generated inauthentic content. Nature Machine Intel-
ligence , 5(7):679‚Äì680.
Kevin Meng, David Bau, Alex Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual associ-
ations in gpt. Advances in Neural Information Pro-
cessing Systems , 35:17359‚Äì17372.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture mod-
els. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings . Open-
Review.net.
Silvia Milano, Joshua A McGrane, and Sabina Leonelli.
2023. Large language models challenge the future
of higher education. Nature Machine Intelligence ,
5(4):333‚Äì334.
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea
Finn, and Christopher D Manning. 2021. Fast model
editing at scale. ArXiv preprint , abs/2110.11309.Eric Mitchell, Joseph J Noh, Siyan Li, William S Arm-
strong, Ananth Agarwal, Patrick Liu, Chelsea Finn,
and Christopher D Manning. 2022. Enhancing self-
consistency and performance of pre-trained language
models through natural language inference. ArXiv
preprint , abs/2211.11875.
Niels M√ºndler, Jingxuan He, Slobodan Jenko, and Mar-
tin Vechev. 2023. Self-contradictory hallucinations
of large language models: Evaluation, detection and
mitigation. ArXiv preprint , abs/2305.15852.
Humza Naveed, Asad Ullah Khan, Shi Qiu, Muham-
mad Saqib, Saeed Anwar, Muhammad Usman, Nick
Barnes, and Ajmal Mian. 2023. A comprehensive
overview of large language models. ArXiv preprint ,
abs/2307.06435.
Ella Neeman, Roee Aharoni, Or Honovich, Leshem
Choshen, Idan Szpektor, and Omri Abend. 2022.
Disentqa: Disentangling parametric and contextual
knowledge with counterfactual question answering.
ArXiv preprint , abs/2211.05655.
Raymond S Nickerson. 1998. Confirmation bias: A
ubiquitous phenomenon in many guises. Review of
general psychology , 2(2):175‚Äì220.
Xenia Ohmer, Elia Bruni, and Dieuwke Hupkes. 2023.
Separating form and meaning: Using self-consistency
to quantify task understanding across multiple senses.
CoRR .
Yasumasa Onoe, Michael JQ Zhang, Shankar Padman-
abhan, Greg Durrett, and Eunsol Choi. 2023. Can
lms learn new entities from descriptions? challenges
in propagating injected knowledge. ArXiv preprint ,
abs/2305.01651.
OpenAI. 2023. Chatgpt.
OpenAI. 2024. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730‚Äì27744.
Liangming Pan, Wenhu Chen, Min-Yen Kan, and
William Yang Wang. 2023a. Attacking open-domain
question answering by injecting misinformation.
IJCNLP-AACL. ACL .
Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu,
Dong Yu, and Jianshu Chen. 2022. Knowledge-in-
context: Towards knowledgeable semi-parametric
language models. In The Eleventh International Con-
ference on Learning Representations .
Yikang Pan, Liangming Pan, Wenhu Chen, Preslav
Nakov, Min-Yen Kan, and William Yang Wang.
2023b. On the risk of misinformation pollu-
tion with large language models. ArXiv preprint ,
abs/2305.13661.Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,
Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou
Yu, Weizhu Chen, et al. 2023. Check your facts and
try again: Improving large language models with
external knowledge and automated feedback. ArXiv
preprint , abs/2302.12813.
Ethan Perez, Sam Ringer, Kamil Àôe Luko≈°i ¬ØutÀôe, Karina
Nguyen, Edwin Chen, Scott Heiner, Craig Pettit,
Catherine Olsson, Sandipan Kundu, Saurav Kada-
vath, et al. 2022. Discovering language model behav-
iors with model-written evaluations. ArXiv preprint ,
abs/2212.09251.
Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) ,
pages 2463‚Äì2473, Hong Kong, China. Association
for Computational Linguistics.
Maren Pielka, Felix Rode, Lisa Pucknat, Tobias Deu√üer,
and Rafet Sifa. 2022. A linguistic investigation of
machine learning based contradiction detection mod-
els: an empirical analysis and future perspectives.
In2022 21st IEEE International Conference on Ma-
chine Learning and Applications (ICMLA) , pages
1649‚Äì1653. IEEE.
Yuval Pinter and Michael Elhadad. 2023. Emptying
the ocean with a spoon: Should we edit models?
InFindings of the Association for Computational
Linguistics: EMNLP 2023 , pages 15164‚Äì15172.
Jirui Qi, Raquel Fern√°ndez, and Arianna Bisazza. 2023.
Cross-lingual consistency of factual knowledge in
multilingual language models. ArXiv preprint ,
abs/2310.10378.
Cheng Qian, Xinran Zhao, and Sherry Tongshuang Wu.
2023. " merge conflicts!" exploring the impacts of
external distractors to parametric knowledge graphs.
ArXiv preprint , abs/2309.08594.
Ella Rabinovich, Samuel Ackerman, Orna Raz, Ei-
tan Farchi, and Ateret Anaby-Tavor. 2023. Predict-
ing question-answering performance of large lan-
guage models through semantic consistency. ArXiv
preprint , abs/2311.01152.
Harsh Raj, Vipul Gupta, Domenic Rosati, and Sub-
habrata Majumdar. 2023. Semantic consistency for
assuring reliability of large language models. ArXiv
preprint , abs/2308.09138.
Harsh Raj, Domenic Rosati, and Subhabrata Majumdar.
2022. Measuring reliability of large language mod-
els through semantic consistency. ArXiv preprint ,
abs/2211.05853.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don‚Äôt know: Unanswerable ques-
tions for SQuAD. In Proceedings of the 56th AnnualMeeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers) , pages 784‚Äì789,
Melbourne, Australia. Association for Computational
Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383‚Äì2392, Austin,
Texas. Association for Computational Linguistics.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 5418‚Äì5426,
Online. Association for Computational Linguistics.
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
2020. A primer in BERTology: What we know about
how BERT works. Transactions of the Association
for Computational Linguistics , 8:842‚Äì866.
Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
ArXiv preprint , abs/2302.04761.
Michael Schlichtkrull, Zhijiang Guo, and Andreas Vla-
chos. 2023. Averitec: A dataset for real-world claim
verification with evidence from the web. In Advances
in Neural Information Processing Systems 36: An-
nual Conference on Neural Information Processing
Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
December 10 - 16, 2023 .
Or Sharir, Barak Peleg, and Yoav Shoham. 2020. The
cost of training nlp models: A concise overview.
ArXiv preprint , abs/2004.08900.
Mrinank Sharma, Meg Tong, Tomasz Korbak, David
Duvenaud, Amanda Askell, Samuel R Bowman,
Newton Cheng, Esin Durmus, Zac Hatfield-Dodds,
Scott R Johnston, et al. 2023. Towards understand-
ing sycophancy in language models. ArXiv preprint ,
abs/2310.13548.
Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia
Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau
Yih. 2023a. Trusting your evidence: Hallucinate
less with context-aware decoding. ArXiv preprint ,
abs/2305.14739.
Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou,
Margaret Li, Victoria Lin, Noah A Smith, Luke
Zettlemoyer, Scott Yih, and Mike Lewis. 2023b. In-
context pretraining: Language modeling beyond doc-
ument boundaries. ArXiv preprint , abs/2310.10638.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-
joon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. 2023c. Replug: Retrieval-
augmented black-box language models. ArXiv
preprint , abs/2301.12652.Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and
Huan Liu. 2017. Fake news detection on social me-
dia: A data mining perspective. ACM SIGKDD ex-
plorations newsletter , 19(1):22‚Äì36.
Ruihao Shui, Yixin Cao, Xiang Wang, and Tat-Seng
Chua. 2023. A comprehensive evaluation of large
language models on legal judgment prediction. ArXiv
preprint , abs/2310.11761.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021. Retrieval augmentation
reduces hallucination in conversation. In Findings
of the Association for Computational Linguistics:
EMNLP 2021 , pages 3784‚Äì3803.
Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-
davi, Jason Wei, Hyung Won Chung, Nathan Scales,
Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl,
et al. 2022. Large language models encode clinical
knowledge. ArXiv preprint , abs/2212.13138.
Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin,
Sergei Popov, and Artem Babenko. 2020. Editable
neural networks. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 . OpenReview.net.
Craig S. Smith. 2023. What large models cost you ‚Äì
there is no free ai lunch.
Irene Solaiman, Zeerak Talat, William Agnew, Lama
Ahmad, Dylan Baker, Su Lin Blodgett, Hal
Daum√© III, Jesse Dodge, Ellie Evans, Sara Hooker,
et al. 2023. Evaluating the social impact of generative
ai systems in systems and society. ArXiv preprint ,
abs/2306.05949.
Giovanni Spitale, Nikola Biller-Andorno, and Federico
Germani. 2023. Ai model gpt-3 (dis) informs us
better than humans. ArXiv preprint , abs/2301.11924.
Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang,
Qi Cao, and Xueqi Cheng. 2024. Blinded by gen-
erated contexts: How language models merge gen-
erated and retrieved contexts for open-domain qa?
ArXiv preprint , abs/2401.11911.
Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. 2023.
The science of detecting llm-generated texts. ArXiv
preprint , abs/2303.07205.
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.
BERT rediscovers the classical NLP pipeline. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 4593‚Äì
4601, Florence, Italy. Association for Computational
Linguistics.
Arun James Thirunavukarasu, Darren Shu Jeng Ting,
Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan,
and Daniel Shu Wei Ting. 2023. Large language
models in medicine. Nature medicine , 29(8):1930‚Äì
1940.Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. ArXiv preprint ,
abs/2307.09288.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022. Musique: Multi-
hop questions via single-hop question composition.
Transactions of the Association for Computational
Linguistics , 10:539‚Äì554.
Miles Turpin, Julian Michael, Ethan Perez, and
Samuel R Bowman. 2023. Language models don‚Äôt
always say what they think: Unfaithful explana-
tions in chain-of-thought prompting. ArXiv preprint ,
abs/2305.04388.
Tyler Vergho, Jean-Francois Godbout, Reihaneh Rab-
bany, and Kellin Pelrine. 2024. Comparing gpt-4
and open-source language models in misinformation
mitigation. ArXiv preprint , abs/2401.06920.
Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry
Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny
Zhou, Quoc Le, et al. 2023. Freshllms: Refreshing
large language models with search engine augmenta-
tion. ArXiv preprint , abs/2310.03214.
Alexander Wan, Eric Wallace, and Dan Klein. 2024.
What evidence do language models find convincing?
ArXiv preprint , abs/2402.11782.
Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan
Li, and Tian Gao. 2019. Does it make sense? and
why? a pilot study for sense making and explana-
tion. In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics , pages
4020‚Äì4026, Florence, Italy. Association for Compu-
tational Linguistics.
Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru
Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao,
Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang,
Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang,
and Yue Zhang. 2023a. Survey on factuality in large
language models: Knowledge, retrieval and domain-
specificity.
Cunxiang Wang, Zhikun Xu, Qipeng Guo, Xiangkun
Hu, Xuefeng Bai, Zheng Zhang, and Yue Zhang.
2023b. Exploiting Abstract Meaning Representation
for open-domain question answering. In Findings of
the Association for Computational Linguistics: ACL
2023 , pages 2083‚Äì2096, Toronto, Canada. Associa-
tion for Computational Linguistics.
Cunxiang Wang, Haofei Yu, and Yue Zhang. 2023c.
RFiD: Towards rational fusion-in-decoder for open-
domain question answering. In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,
pages 2473‚Äì2481, Toronto, Canada. Association for
Computational Linguistics.Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou,
and Muhao Chen. 2023d. A causal view of entity
bias in (large) language models. ArXiv preprint ,
abs/2305.14695.
Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang,
Zezhong Wang, Yufei Wang, Fei Mi, Jeff Z. Pan,
and Kam-Fai Wong. 2024a. Unims-rag: A unified
multi-source retrieval-augmented generation for per-
sonalized dialogue systems.
Hongru Wang, Lingzhi Wang, Yiming Du, Liang Chen,
Jingyan Zhou, Yufei Wang, and Kam-Fai Wong.
2023e. A survey of the evolution of language model-
based dialogue systems.
Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua
Zhang, Cunxiang Wang, Guanhua Chen, Huimin
Wang, and Kam fai Wong. 2024b. Self-dc: When
to retrieve and when to generate? self divide-and-
conquer for compositional unknown questions.
Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao,
and Jiarong Xu. 2023f. Cross-lingual knowledge
editing in large language models. ArXiv preprint ,
abs/2309.08952.
Liyuan Wang, Xingxing Zhang, Qian Li, Mingtian
Zhang, Hang Su, Jun Zhu, and Yi Zhong. 2023g.
Incorporating neuro-inspired adaptability for contin-
ual learning in artificial intelligence. Nature Machine
Intelligence , pages 1‚Äì13.
Yike Wang, Shangbin Feng, Heng Wang, Weijia
Shi, Vidhisha Balachandran, Tianxing He, and Yu-
lia Tsvetkov. 2023h. Resolving knowledge con-
flicts in large language models. ArXiv preprint ,
abs/2310.00935.
Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and
Quoc V Le. 2023. Simple synthetic data re-
duces sycophancy in large language models. ArXiv
preprint , abs/2308.03958.
Laura Weidinger, John Mellor, Maribeth Rauh, Conor
Griffin, Jonathan Uesato, Po-Sen Huang, Myra
Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
et al. 2021. Ethical and social risks of harm from
language models. ArXiv preprint , abs/2112.04359.
Laura Weidinger, Maribeth Rauh, Nahema Marchal, Ar-
ianna Manzini, Lisa Anne Hendricks, Juan Mateos-
Garcia, Stevie Bergman, Jackie Kay, Conor Grif-
fin, Ben Bariach, et al. 2023. Sociotechnical safety
evaluation of generative ai systems. ArXiv preprint ,
abs/2310.11986.
Orion Weller, Aleem Khan, Nathaniel Weir, Dawn
Lawrie, and Benjamin Van Durme. 2022. Defend-
ing against misinformation attacks in open-domain
question answering. ArXiv preprint , abs/2212.10002.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North AmericanChapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers) , pages 1112‚Äì1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Xiangcheng Wu, Xi Niu, and Ruhani Rahman. 2022.
Topological analysis of contradictions in text. In
Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval , pages 2478‚Äì2483.
Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-
lor Berg-Kirkpatrick, and Shlomo Dubnov. 2023.
Large-scale contrastive language-audio pretraining
with feature fusion and keyword-to-caption augmen-
tation. In ICASSP 2023-2023 IEEE International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP) , pages 1‚Äì5. IEEE.
Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and
Yu Su. 2023. Adaptive chameleon or stubborn
sloth: Unraveling the behavior of large language
models in knowledge conflicts. ArXiv preprint ,
abs/2305.13300.
Nan Xu, Fei Wang, Bangzheng Li, Mingtao Dong, and
Muhao Chen. 2022. Does your model classify en-
tities reasonably? diagnosing and mitigating spu-
rious correlations in entity typing. ArXiv preprint ,
abs/2205.12640.
Rongwu Xu, Brian S Lin, Shujian Yang, Tianqi Zhang,
Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei
Xu, and Han Qiu. 2023. The earth is flat be-
cause...: Investigating llms‚Äô belief towards misinfor-
mation via persuasive conversation. ArXiv preprint ,
abs/2312.09085.
Boyang Xue, Hongru Wang, Weichao Wang, Rui Wang,
Sheng Wang, Zeming Liu, and Kam-Fai Wong. 2024.
A comprehensive study of multilingual confidence
estimation on large language models.
Boyang Xue, Weichao Wang, Hongru Wang, Fei Mi,
Rui Wang, Yasheng Wang, Lifeng Shang, Xin Jiang,
Qun Liu, and Kam-Fai Wong. 2023. Improving fac-
tual consistency for knowledge-grounded dialogue
systems via knowledge enhancement and alignment.
InFindings of the Association for Computational
Linguistics: EMNLP 2023 , pages 7829‚Äì7844.
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng,
Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu
Zhang. 2023. Editing large language models: Prob-
lems, methods, and opportunities. ArXiv preprint ,
abs/2305.13172.
Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre
Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao
Wu. 2023. Benchmarking and defending against indi-
rect prompt injection attacks on large language mod-
els.ArXiv preprint , abs/2312.14197.Jiahao Ying, Yixin Cao, Kai Xiong, Yidong He, Long
Cui, and Yongbin Liu. 2023. Intuitive or dependent?
investigating llms‚Äô robustness to conflicting prompts.
ArXiv preprint , abs/2309.17415.
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2022. Gener-
ate rather than retrieve: Large language models
are strong context generators. ArXiv preprint ,
abs/2209.10063.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An
open bilingual pre-trained model. In The Eleventh In-
ternational Conference on Learning Representations .
Boyu Zhang, Hongyang Yang, Tianyu Zhou, Muham-
mad Ali Babar, and Xiao-Yang Liu. 2023a. Enhanc-
ing financial sentiment analysis via retrieval aug-
mented large language models. In Proceedings of
the Fourth ACM International Conference on AI in
Finance , pages 349‚Äì356.
Hang Zhang, Xin Li, and Lidong Bing. 2023b. Video-
llama: An instruction-tuned audio-visual language
model for video understanding. In Proceedings of
the 2023 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations ,
pages 543‚Äì553.
Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A
Malin, and Sricharan Kumar. 2023c. Sac3: Reliable
hallucination detection in black-box language models
via semantic-aware cross-check consistency. ArXiv
preprint , abs/2311.01740.
Michael JQ Zhang and Eunsol Choi. 2021. Situatedqa:
Incorporating extra-linguistic contexts into qa. ArXiv
preprint , abs/2109.06157.
Michael JQ Zhang and Eunsol Choi. 2023. Mitigating
temporal misalignment by discarding outdated facts.
ArXiv preprint , abs/2305.14824.
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,
Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and Bill Dolan. 2020. DIALOGPT : Large-scale
generative pre-training for conversational response
generation. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations , pages 270‚Äì278, Online. As-
sociation for Computational Linguistics.
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,
Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei
Bi, Freda Shi, and Shuming Shi. 2023d. Siren‚Äôs song
in the ai ocean: A survey on hallucination in large
language models.
Yunxiang Zhang, Muhammad Khalifa, Lajanugen Lo-
geswaran, Moontae Lee, Honglak Lee, and Lu Wang.
2023e. Merging generated and retrieved knowledge
for open-domain qa. ArXiv preprint , abs/2310.14393.Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,
Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei
Yin, and Mengnan Du. 2023a. Explainability for
large language models: A survey. ACM Transactions
on Intelligent Systems and Technology .
Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang
Xing, Chong Meng, Shuaiqiang Wang, Zhicong
Cheng, Zhaochun Ren, and Dawei Yin. 2023b.
Knowing what llms do not know: A simple yet
effective self-detection method. ArXiv preprint ,
abs/2310.17918.
Chujie Zheng, Jinfeng Zhou, Yinhe Zheng, Libiao Peng,
Zhen Guo, Wenquan Wu, Zhengyu Niu, Hua Wu,
and Minlie Huang. 2022. Cdconv: A benchmark
for contradiction detection in chinese conversations.
ArXiv preprint , abs/2210.08511.
Zexuan Zhong, Zhengxuan Wu, Christopher D Man-
ning, Christopher Potts, and Danqi Chen. 2023.
Mquake: Assessing knowledge editing in language
models via multi-hop questions. ArXiv preprint ,
abs/2305.14795.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023a. Lima: Less is more for align-
ment. ArXiv preprint , abs/2305.11206.
Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li,
Sam S Chen, Peilin Zhou, Junling Liu, Yining Hua,
Chengfeng Mao, Xian Wu, et al. 2023b. A survey of
large language models in medicine: Progress, applica-
tion, and challenge. ArXiv preprint , abs/2311.05112.
Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G
Parker, and Munmun De Choudhury. 2023c. Syn-
thetic lies: Understanding ai-generated misinforma-
tion and evaluating algorithmic and human solutions.
InProceedings of the 2023 CHI Conference on Hu-
man Factors in Computing Systems , pages 1‚Äì20.
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and
Muhao Chen. 2023d. Context-faithful prompt-
ing for large language models. ArXiv preprint ,
abs/2303.11315.
Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and
Chao Zhang. 2023. Toolqa: A dataset for llm ques-
tion answering with external tools. ArXiv preprint ,
abs/2306.13304.
Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria
Liakata, and Rob Procter. 2018. Detection and res-
olution of rumours in social media: A survey. ACM
Computing Surveys (CSUR) , 51(2):1‚Äì36.A Appendix
A.1 Quantitative Analysis and Comparison
In the context of a survey paper, while it is benefi-
cial to include quantitative results and analyses con-
cerning the impact of knowledge conflicts across
various types of conflicts and the performance com-
parison of different mitigation strategies, it is not a
strict requirement. We acknowledge the complex-
ity and impracticality involved in conducting such
quantitative experiments, particularly due to the use
of disparate datasets in behavioral analyses, as well
as the variance in the inherent knowledge of LLMs
across different knowledge cut-off snapshots, as
detailed in Table 1.
Moreover, establishing a ‚Äúfair‚Äù comparison
within the mitigation strategies segment poses its
own set of challenges, given the diversity in objec-
tives influenced by various assumed priors, such
as the perceived accuracy of context or inherent
knowledge, as discussed in the main text. De-
spite these intricacies, we opt to present quantita-
tive results by compiling existing evaluations from
a range of papers. It is imperative, however, to
approach this analysis with caution , recognizing
that original authors may have employed different
datasets, LLMs variants, or even pursued contrast-
ing objectives.
A.2 Quantitative Results on the Impact of
Knowledge Conflicts
The comparison of quantitative results on the im-
pact of the three types of knowledge conflicts is
shown in Table 2. We pick the results of represen-
tative behavior analysis literature for comparison.
A.3 Quantitative Results on the Effectiveness
of Mitigation Strategies
The effectiveness of various mitigation strategies
is quantitatively compared in Table 3. It is impor-
tant to note that our analysis is limited to works
addressing three predominant types of mitigating
objectives within the context of memory conflicts.
This selection is deliberate, as other types of miti-
gating objectives in different conflict categories do
not yet have a substantial body of work that would
allow for a meaningful cross-method comparison.Reference Model Dataset Quantitative Results
Context-memory conflict
Pan et al. (2023b) ChatGPT NQ-1500 and
CovidNewsMisinformation in the context can lead to a
significant degradation (up to 87%) in the
performance.
Xie et al. (2023) ChatGPT, GPT-4,
PaLM2, Qwen,
Llama2, and
VicunaPOPQA and
STRATEGYQAFor entity substitution-based counter-memory, only
ChatGPT, GPT-4, and PaLM2 over 60%
probability of choosing parametric memory. For
generation-based counter-memory, all models have
more than 80% probability of choosing context
knowledge.
Xu et al. (2023) ChatGPT, GPT-4,
Llama2, and
VicunaFarm, BoolQ,
TruthfulQA and NQIn multiple rounds of dialogue, as the number of
counter-memory context increases, the cumulative
proportion of belief alteration of LLMs spans from
20.7% to 78.2%
Inter-context conflict
Jin et al. (2024a) ChatGPT,
Llama2,
Baichuan2,
FLAN-UL2 and
FLAN-T5NQ, TriviaQA, PopQA,
and MuSiQueWhen faced with conflicting evidence, ChatGPT‚Äôs
recall declined the least, but more than 10%.
Chen et al.
(2023b)ChatGPT,
ChatGLM,
Vicuna, Qwen,
and BELLERGB As the noise in evidence increases, the
performance of models will gradually decrease.
When the noise rate exceeds 0.8, the performance
of all models decreases by more than 20%.
Li et al. (2023a) GPT-4, ChatGPT,
PaLM2, and
Llama2CONTRADOC Faced with self-contradictory documents, gpt4 has
a more than 70% probability of determining the
occurrence of a contradiction, while other models
are less than 50%.
Intra-memory conflict
M√ºndler et al.
(2023)GPT-4, ChatGPT,
Llama2, and
VicunaMainTestSet LLMs create contradictory content, with a
probability of between 15.7% and 22.9%. More
powerful models create fewer contradictory results.
Zhao et al.
(2023b)ChatGPT, GPT-4,
Vicuna, and
Llama2FaVIQ, ComQA,
GSM-8K, SV AMP,
ARCChallenge, and
CommonsenseQAThe findings of their research reveal that even
GPT-4 can exhibit an inconsistency rate of 32% in
FaVIQ.
Table 2: Comparison of quantitative results on the impact of various types of knowledge conflicts.Reference Model Dataset Quantitative Results
Faithful to context
Shi et al. (2023a) Llama, OPT,
GPT-Neo, and
FLANNQ-SWAP, MemoTrap,
and NQTheir method improves GPT-Neo 20B by 54.4%
on Memotrap and by 128% on NQ-SWAP where
LLMs need to adhere to the given context.
Zhou et al.
(2023d)ChatGPT and
Llama2MRC and Re-TACRED Compared to the zero-shot base prompts, their
prompting method leads to a reduction of 32.2%
for maintaining parametric knowledge for MRC
and a 10.9% reduction for Re-TACRED on
GPT-3.5. Similarly, on Llama2, there is a 39.4%
reduction for MRC and a 57.3% reduction for
Re-TACRED.
Discriminating misinformation
Hong et al.
(2023)ChatGPT and FiD NQ and TQA The authors train a discriminator with about 80%
F1 score and use it to improve models performance
above 5%.
Pan et al. (2023b) ChatGPT NQ-1500 and
CovidNewsThe author‚Äôs mitigation method improves the
accuracy by more than 10%.
Disentangling sources
Wang et al.
(2023h)ChatGPT KNOWLEDGE
CONFLICTThe authors‚Äô method achieved over 80% F1 score
on contextual knowledge conflict detection.
Table 3: Comparison of quantitative results on the effectiveness of various mitigation strategies w.r.t. their objectives.