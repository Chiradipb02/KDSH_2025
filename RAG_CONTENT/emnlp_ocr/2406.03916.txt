ArMeme: Propagandistic Content in Arabic Memes
WARNING: This paper contains examples which may be disturbing to the reader
Firoj Alam1, Abul Hasnat2,3, Fatema Ahmad1, Md Arid Hasan4, Maram Hasanain1
1Qatar Computing Research Institute, HBKU, Qatar
2Blackbird.AI, USA,3APA VI.AI, France
4University of New Brunswick, Fredericton, NB, Canada
{fialam,fakter,mhasanain}@hbku.edu.qa , mhasnat@gmail.com, arid.hasan@unb.ca
Abstract
With the rise of digital communication memes
have become a significant medium for cultural
and political expression that is often used to
mislead audience. Identification of such mis-
leading and persuasive multimodal content be-
come more important among various stakehold-
ers, including social media platforms, policy-
makers, and the broader society as they often
cause harm to the individuals, organizations
and/or society. While there has been effort
to develop AI based automatic system for re-
source rich languages (e.g., English), it is rela-
tively little to none for medium to low resource
languages. In this study, we focused on devel-
oping an Arabic memes dataset with manual
annotations of propagandistic content.1We an-
notated ∼6KArabic memes collected from
various social media platforms, which is a first
resource for Arabic multimodal research. We
provide a comprehensive analysis aiming to de-
velop computational tools for their detection.
We made the dataset publicly available for the
community.
1 Introduction
Social media platforms have enabled people to
post and share content online. A significant por-
tion of this content provides valuable resources
for initiatives such as citizen journalism, raising
public awareness, and supporting political cam-
paigns. However, a considerable amount is posted
and shared to mislead social media users and to
achieve social, economic, or political agendas. In
addition, the freedom to post and share content
online has facilitated negative uses, leading to an
increase in online hostility, as evidenced by the
spread of disinformation, hate speech, propaganda,
and cyberbullying (Brooke, 2019; Joksimovic et al.,
1Propaganda is a form of communication designed to in-
fluence people’s opinions or actions toward a specific goal,
employing well-defined rhetorical and psychological tech-
niques (for Propaganda Analysis, 1938).
Figure 1: Examples of images representing different
categories.
2019; Schmidt and Wiegand, 2017; Davidson et al.,
2017; Da San Martino et al., 2019; Van Hee et al.,
2015). A lack of media literacy2is also a major fac-
tor contributing to the spread of misleading infor-
mation on social media (Zannu et al., 2024). This
can lead to the uncritical acceptance and sharing
of false or misleading content, which can quickly
disseminate through social networks. In their study,
Zannu et al. (2024) highlight the crucial role of me-
dia literacy in mitigating the spread of fake news
among users of platforms such as Instagram and
Twitter.
Online content typically consists of different
modalities, including text, images, and videos.
Disinformation, misinformation, propaganda, and
other harmful content are shared across all these
modalities. Recently, the use of Internet memes
2Media literacy encompasses the ability to access, analyze,
evaluate, and create media in various forms.arXiv:2406.03916v2  [cs.CL]  6 Oct 2024have become very popular on these platforms. A
meme is defined as “a collection of digital items
that share common characteristics in content, form,
or stance, which are created through association
and widely circulated, imitated, or transformed
over the Internet by numerous users” (Shifman,
2013). Memes typically consist of one or more
images accompanied by textual content (Shifman,
2013; Suryawanshi et al., 2020). While memes are
primarily intended for humor, they can also con-
vey persuasive narratives or content that may mis-
lead audiences. To automatically identify such con-
tent, research efforts have focused on addressing
offensive material (Gandhi et al., 2020), identify-
ing hate speech across different modalities (Gomez
et al., 2020; Wu and Bhandary, 2020), and detect-
ing propaganda techniques in memes (Dimitrov
et al., 2021a).
Among the various types of misleading and
harmful content, the spread of propagandistic con-
tent can significantly distort public perception and
hinder informed decision-making. To address this
challenge, research efforts have been specifically di-
rected towards defining techniques and tackling the
issue in different types of content, including news
articles (Da San Martino et al., 2019), tweets (Alam
et al., 2022b), memes (Dimitrov et al., 2021a), and
textual content in multiple languages (Piskorski
et al., 2023a). Most of these efforts have focused
on English, with relatively little attention given to
Arabic. Prior research on Arabic textual content
includes studies presented at WANLP-2022 and
ArabicNLP-2023 (Alam et al., 2022b; Hasanain
et al., 2023). However, for multimodal content,
specifically memes, there are no available datasets
or resources. To address this gap, we have col-
lected and annotated a dataset consisting of approx-
imately 6,000 memes, categorizing them into four
categories (as shown in Figure 1) to identify propa-
gandistic content. Below we briefly summarize the
contribution of our work.
•The first Arabic meme dataset with manual
annotations defining four categories.
•A detailed description of the data collection
procedure, which can assist the community in
future data collection efforts.
•An annotation guideline that will serve as a
foundation for future research.
• Detailed experimental results, including:
–Text modality: training classical modelsand fine-tuning monolingual vs. multi-
lingual transformer models.
–Image modality: fine-tuning CNN mod-
els with different architectures.
–Multimodality: training an early fusion-
based model.
–Evaluating different LLMs in a zero-shot
setup for all modalities.
•Releasing the dataset to the community.3The
dataset and annotation guideline will be ben-
eficial for research to develop automatic sys-
tems and enhance media literacy.
2 Related Work
Social media has become one of the main ways of
sharing information. Its widespread use and reach
is also responsible for creating and spreading mis-
information and propaganda among users. Propa-
gandistic techniques can be found in various types
of content, such as fake news and doctored im-
ages, across multiple media platforms, frequently
employing tools like bots. Furthermore, such in-
formation is distributed in diverse forms, including
textual, visual, and multi-modal. To mitigate the
impact of propaganda in online media, researchers
have been developing resources and tools to iden-
tify and debunk such content.
2.1 Persuasion Techniques Detection
Early research on propaganda identification relies
on the entire document to identify whether the con-
tent is propaganda, while recent studies focus on
social media content (Dimitrov et al., 2021b), news
articles (Da San Martino et al., 2019), political
speech (Partington and Taylor, 2017), arguments
(Habernal et al., 2017, 2018), and multimodal con-
tent (Dimitrov et al., 2021a). Barrón-Cedeno et al.
(2019) developed a binary classification ( propa-
ganda and non-propaganda ) corpus to explore
writing style and readability levels. An alterna-
tive approach followed by Habernal et al. (2017,
2018) to identify persuasion techniques within texts
constructing a corpus on arguments. Moreover,
Da San Martino et al. (2019) developed a span-level
propaganda detection corpus from news articles
and annotated in eighteen propaganda techniques.
3Dataset is released under CC-BY-NC-SA through https:
//huggingface.co/datasets/QCRI/ArMeme .Piskorski et al. (2023b) developed a dataset from
online news articles into twenty-two persuasion
techniques containing nine languages to address the
multilingual research gap. Following the previous
work, Piskorski et al. (2023a) and SemEval-2024
task 4 focus on resource development to facilitate
the detection of multilingual persuasion techniques.
Focusing on multimodal persuasion techniques for
memes, Dimitrov et al. (2021a) created a corpus
containing 950 memes and investigated pretrained
models for both unimodal and multimodal memes.
The study of Chen et al. (2024) proposed a mul-
timodal visual-textual object graph attention net-
work to detect persuasion techniques from multi-
modal content using the dataset described in (Pisko-
rski et al., 2023b). In a recent shared task, Dim-
itrov et al. (2024) introduced a multilingual and
multimodal propaganda detection task, which at-
tracted many participants. The participants’ sys-
tems included various models based on transform-
ers, CNNs, and LLMs.
2.2 Multimodal Content
The study of multimodal content has gained popu-
larity among researchers for propaganda detection
due to its effectiveness of in spreading propagandas-
tic information and creating impact among the tar-
geted audience. Sharma et al. (2022) presented that
propaganda can be used to cause several types of
harm including spreading hate, violence, exploita-
tion, etc., while spreading mis- and dis-information
is also one of the main reasons (Alam et al., 2022a).
The study of V olkova et al. (2019) presented an in-
depth analysis of multimodal content for predicting
misleading information from news. Additionally,
the deception and disinformation analysis on so-
cial media platforms using multimodal content in
multilingual settings has been studied by Glenski
et al. (2019). Moreover, hateful memes (Kiela et al.,
2020), propaganda in visual content (Seo, 2014),
emotions and propaganda (Abd Kadir et al., 2016)
also studied by the researchers in the past few years.
Recent studies focusing on fine-tuning visual
transformer models such as ViLBERT (Lu et al.,
2019), Multimodal Bitransformers (Kiela et al.,
2019), and VisualBERT (Li et al., 2019). Cao et al.
(2022) study focuses on multimodal hateful meme
identification using prompting strategies by adopt-
ing (Prakash et al., 2023). Hee et al. (2024) studied
hate speech content moderation and discussed re-
cent advancements leveraging large models.
Compared to previous studies, our work differsin that we provide the first resource for Arabic.
Additionally, our annotation guidelines and data
collection procedures for memes may be useful for
other languages.
3 Dataset
3.1 Data Collection
Our data collection process involves several steps
as highlighted in the Figure 2. We manually se-
lected public groups and contents from Facebook,
Instagram, and Pinterest. In addition, we have also
collected memes from Twitter using a set of key-
words as listed in the Figure 3. Our data curation
consists of a series of steps as discussed below.
Manual selection of groups, links, and keywords:
Focusing on the mentioned sources, we have man-
ually selected public groups, which contains post
on public figures, celebrity, and discussions about
politics. In Table 1, we provide the sources of the
dataset, number of groups, and number of images
we have collected.
Source # of Group # of Images
Facebook 19 5,453
Instagram 22 107,307
Pinterest - 11,369
Twitter - 5,369
Total 129,498
Table 1: Statistics of the initial data collection.
Crawling: Given that Facebook, Instagram, and
Pinterest do not provide API or do not allow
automatic crawling images, therefore, we devel-
oped a semi-automatic approach to crawl images
from these platforms. The steps include manu-
ally loading images and then crawl the images that
are loaded on the browser. For the Twitter (X-
platform), we used the keywords to crawl tweets,
which consists of media/image.
3.2 Filtering
Filtering duplicate images: Given that user
might have posted same meme or a slight modi-
fication of it in multiple platforms, which is very
common for social media, therefore, we applied an
exact and near-duplicate image detection method
to remove them. This method consists of ex-
tracting features using a pre-trained deep learn-
ing model and compute similarity. Given a datasetFigure 2: Data curation pipeline.
Figure 3: Keywords used to collect tweets.
D={x1, x2, . . . , x N}consisting of Ndata points,
we extracted features using a pre-trained deep learn-
ing model and used nearest neighbor based ap-
proach (Cunningham and Delany, 2007). The
model is trained by fine-tuning ResNet18 (He et al.,
2016) using the social media dataset discussed in
(Alam et al., 2020). Let f:Rd→Rmbe a pre-
trained deep learning model that maps an input
data point xi∈Rdto a feature vector f(xi)∈Rm.
For each data point xi∈ D, the feature vector is
extracted as: zi=f(xi),fori= 1,2, . . . , N
where zi∈Rmis the feature vector of the data
point xi. To compute the nearest neighbors be-
tween a data point xiand the entire dataset D, we
use the euclidean distance. We then use a threshold
of3.6to define the near-duplicate images as those
with a euclidean distance less than or equal to this
threshold value.
OCR Text: We used EasyOCR4to extract text
from memes. Memes with no extracted or detected
text were filtered out. The reasons for choosing
EasyOCR are: (a) it is a ready-to-use OCR with
80+ supported languages and all popular writing
scripts, and (b) it includes the implementation of
4https://github.com/JaidedAI/EasyOCRthe state-of-the-art, highly efficient real-time scene
text detection module (Liao et al., 2022), called
DBnet, which uses differentiable binarization and
adaptive scale fusion.
Classifier-Based Filtering: We employed an in-
house meme vs. non-meme classifier to filter out
images that were not classified as memes. The
classifier was developed using a dataset of 3,935
images, consisting of 2,000 memes and 1,935 non-
memes. Following the approach of (Hasnat et al.,
2019), we developed a lightweight meme classi-
fier to perform binary classification based on the
extracted image features. The classifier achieved
the best performance of 94.79% test set accuracy
in classifying memes using a 256-dimensional
normalized histogram extracted from gray-scale
images as features, with a Multilayer Perceptron
(MLP) as the classifier.
3.3 Annotation
Data Sampling: Due to budget constraints for
manual annotation, we randomly sampled ∼6K
images.
Manual Annotation: For the manual annotation,
we first prepared an annotation guideline to assist
the annotators. To facilitate the annotation tasks,
we developed an annotation platform as presented
in Appendix C. The details of the annotation guide-
lines are reported in Appendix B. Note that we de-
veloped the annotation guidelines in English, (see
Section B), which were then translated into the Ara-
bic language. Translating the guideline in native
language was indeed important and also inspired
by prior work (Alam et al., 2021; Hasanain et al.,
2024a). The idea is not only to make the annota-
tion task more convenient but also capture differentlinguistic aspects. The guidelines included several
examples of memes. It was reviewed by several
NLP experts who are also native Arabic speakers.
In Figure 1, we provide examples of images and
memes representing different categories. Figure
1(a) depicts a couple in what appears to be a cou-
ples therapy session. The therapist asks the hus-
band, “Do you feel your wife is controlling you?”
The wife responds, “No, I don’t feel so.” It is ev-
ident that the question was directed towards the
husband, yet the wife answers instead of him. The
irony lies in her controlling the conversation when
her control is the subject of discussion. This meme
attempts to humorously portray the stereotypical
notion that wives are controlling in marriage. Fig-
ure 1(b) employs a play on words to create humor.
The Arabic word that means "different" is similar
to the Arabic word "retarded", except in the posi-
tion of two letters. However, this meme does not
contain any propagandistic techniques. Figure 1(c)
features a meme that uses an image of a scene from
TV with dialogues and added text to create humor.
However, it was categorized as “other” because the
dialogues were in English, rather than “not propa-
gandistic” or “propagandistic”. Figure 1(d) shows
a picture of book covers, which might have been
part of an advertisement. This image was labelled
as "not meme".
The annotation tasks consist of two phases:
•Phase 1 (image categorization): labeling im-
ages shown on the platform as (i)not-meme,
(ii)other, (iii)not propaganda, or (iv)propa-
ganda. Each image was annotated by three
annotators and the final label is decided based
on majority agreement.
•Phase 2 (text editing): editing the text to fix
OCR errors. This step was performed only
for images labelled as meme, and propagan-
distic or not propagandistic. Further details is
mentioned in B.2.
Annotation Team: The team in phase 1 consisted
of three members, and in phase 2, consisted of one
member. All annotators are native Arabic speakers
holding at least a bachelor’s degree. Our in-house
expert annotator provided them with several iter-
ations of training, supervised and monitored their
work, and handled quality control throughout the
entire annotation process. This quality assurance in-
cluded periodic checks of random annotation sam-
ples and providing feedback. Since the institute
requires the signing of a Non-Disclosure Agree-ment (NDA), each annotator signed an NDA after
being made aware of the institute’s terms and con-
ditions. They were compensated at the same rate
as charged by external companies.
Annotation platform: We utilized our in-house
annotation platform for the annotation task. Sepa-
rate annotation interfaces were designed for each
phase.
Annotation Agreement For the Phase 1 annota-
tion, we computed annotation agreement using var-
ious evaluation measures, including Fleiss’ kappa,
Krippendorff’s alpha, average observed agreement,
and majority agreement. The resulting scores were
0.529, 0.528, 0.755, and 0.873, respectively. Based
on the value of Krippendorff’s alpha, we can con-
clude that our annotation agreement score indicates
moderate agreement.5In the final label selection,
we excluded the ∼200 memes on which the anno-
tators disagreed. In the second phase , we mainly
edited text to fix the OCR errors, which has been
done by a single annotator. To ensure the quality of
theediting phase , random samples were checked
by an expert annotator and periodically provided
feedback. Note that the post-editing has been done
for only propagandistic and non-propagandistic
memes. It is to reduce the cost of the annotation,
and to further annotate them with span-level propa-
ganda techniques.
3.4 Statistics
Table 2 shows the number of memes for each cate-
gory. For the rest of the experiments, the data was
split into train, dev, and test as shown in the table.
The dataset comprises a total of 5,725 annotated
samples, with “Not propaganda” covers over half
of the dataset ( ∼66%), followed by “Propaganda.”
The “Not-meme“ and “Other“ classes are signif-
icantly smaller in comparison. The distribution
indicates a significant class imbalance, particularly
between “Not propaganda” and the other classes,
which could affect model training and performance.
In Table 3, we report the distribution of the
dataset across different sources. The annotated
number of memes reflects the memes we collected
from various sources, as detailed in Table 1. We
have the highest number of memes collected and
annotated from Instagram. A very small number
from Twitter is due to different image filtering steps.
5Note that Kappa values of 0.21–0.40, 0.41–0.60, 0.61–
0.80, and 0.81–1.0 correspond to fair, moderate, substantial,
and perfect agreement, respectively (Landis and Koch, 1977).Class label Train Dev Test Total
Not propaganda 2,634 384 746 3,764
Propaganda 972 141 275 1,388
Not-meme 199 30 57 286
Other 202 29 56 287
Total 4,007 584 1,134 5,725
Table 2: Data split statistics.
Source Not prop. Prop. Not-meme Other Total
Facebook 464 332 58 144 998
Instagram 2,052 637 46 60 2,795
Pinterest 1,245 414 147 78 1,884
Twitter 3 5 38 2 48
Total 3,764 1,388 289 284 5,725
Table 3: Number of annotated memes across different
sources. Prop. - Propaganda.
As shown in Table 3 the prevalence of propagan-
distic memes is relatively higher on Facebook than
that of non-propagandistic memes.
4 Experiments
4.1 Training and Evaluation Setup
For all experiments, except for those involving
LLMs as detailed below, we trained the models
using the training set, fine-tuned the parameters
with the development set, and assessed their perfor-
mance on the test set. We use the model with the
best weighted-F1 on the development set to evalu-
ate its performance on the test set. For the LLMs,
we accessed them through APIs.
Evaluation Measures For the performance mea-
sure for all different experimental settings, we com-
pute accuracy, and weighted precision, recall and
F1score. In addition, we also computed macro-F1.
4.2 Models
We conducted our experiments using classical
models (e.g., SVM) as well as both small (e.g.,
ConvNeXt-T) and large language models. It is im-
portant to note that our definitions of ‘small’ and
‘large’ models are based on the criteria discussed
in (Zhao et al., 2023).6
6The term ‘LLMs’ specifically refers to models that en-
compass tens or hundreds of billions of parameters.4.2.1 Baseline:
We adopted widely-used standard baseline meth-
ods, including the majority and random baselines.
4.2.2 Small Language Models (SLMs)
We implemented classical models across all modal-
ities, consisting of (i)feature extraction followed
by model training, and (ii)fine-tuning pre-trained
models (PLMs). For fine-tuning PLMs, we used a
task-specific classification head over the training
subset.
Text-Based Models: For the text-based uni-
modal model, we transformed text into n-gram
(n=1) format using a tf-idf representation, consid-
ering the top 5,000 tokens, and trained an SVM
model with a parameter value of C= 1. Addition-
ally, we fine-tuned several pre-trained transformer
models (PLMs). These included the monolingual
transformer model AraBERT (Antoun et al., 2020),
Qarib (Abdelali et al., 2021) and multilingual trans-
formers such as multilingual BERT (mBERT) (De-
vlin et al., 2019), and XLM-RoBERTa (XLM-
r) (Conneau et al., 2019). We used the Transformer
toolkit (Wolf et al., 2019) for the experiment. Fol-
lowing the guidelines outlined in (Devlin et al.,
2019), we fine-tuned each model using the default
settings over three epochs. Due to instability, we
performed ten reruns for each experiment using
different random seeds, and we picked the model
that performed best on the development set. We
provided the details of the parameters settings in
Appendix A.
Image-Based Models: For the image-based uni-
modal model with feature-extraction approach, we
extracted features using ConvNeXt-T (Liu et al.,
2022),7and trained an SVM model. For fine-
tuning image-based PLMs, we used ResNet18,
ResNet50 (He et al., 2016), VGG16 (Simonyan
and Zisserman, 2014), MobileNet (Howard et al.,
2017), and EfficientNet (Tan and Le, 2019). We
chose these diverse architectures to understand
their relative performance. The models were
trained using the Adam optimizer (Kingma and Ba,
2015) with an initial learning rate of 10−3, which
was decreased by a factor of 10 when accuracy
on the development set stopped improving for 10
epochs. The training lasted for 150 epochs.
7The configuration of ConvNeXt-T includes C =
(96,192,384,768) andB= (3,3,9,3), where CandB
represent the number of channels and blocks, respectively.Multimodal Models: We developed a multi-
modal model by concatenating text features (ex-
tracted using AraBERT) and image features (ex-
tracted using ConvNeXt-T), which were then fed
into an SVM.
4.2.3 LLMs for Text
For the LLMs, we investigate their performance
with zero-shot learning settings without any spe-
cific training. It involves prompting and post-
processing of output to extract the expected content.
Therefore, for each task, we experimented with
a number of prompts. We used GPT-4 (OpenAI,
2023). We set the temperatures to zero for all these
models to ensure deterministic predictions. We
used LLMeBench framework (Dalvi et al., 2024)
for the experiments, which provides seamless ac-
cess to the API end-points and followed prompting
approach reported in (Abdelali et al., 2024).
4.2.4 Multimodal LLMs
For the multimodal models (Xu et al., 2023), we
experimented with several well-known and top-
performing commercial models. These included
OpenAI’s GPT models (GPT-4 Turbo and GPT-4o)
(OpenAI, 2023), as well as Google’s Gemini Pro
models (versions 1.0 and 1.5) (Team et al., 2023).
Using these models, we tested (i)the meme/im-
age only, (ii)text only (text extracted using OCR
from the image), and (iii)multimodal (meme and
OCR text) in a zero-shot learning setting. This
means we did not provide any training examples
within the prompts to the models.
We designed a prompt based on trial and error us-
ing the visual interfaces of OpenAI’s GPT-4 user in-
terface. The prompt instructs the models to perform
a deeper analysis of the image and any text that
they can read within the image before answering
whether the meme can be classified as spreading
propaganda. Additionally, it requests the models
to provide the output in a valid JSON format. For
the experiments, we used the default parameters
for each multimodal model.
4.3 Prompting Strategy
LLMs produce varied responses depending on the
prompt design, which is a complex and iterative
process that presents challenges due to the un-
known representation of information within dif-
ferent LLMs. The instructions expressed in our
prompts include English language with the input
text content in Arabic.As mentioned earlier we employed zero-shot
prompting, providing natural language instructions
that describe the task and specify the expected out-
put. This approach enables the LLMs to construct
a context that refines the inference space, yielding
a more accurate output. In Listing 1, we provide an
example of a zero-shot prompt, emphasizing the in-
structions and placeholders for both input and label.
Along with the instruction we provide the labels to
guide the LLMs and provide information on how
the LLMs should present their output, aiming to
eliminate the need for post-processing.
Instructions:
prompt = (
"You are an expert social media image
analyzer specializing in identifying
propaganda in Arabic contexts. "
"I will provide you with Arabic memes
and the text extracted from these
images. Your task is to briefly
analyze them. "
"To accurately perform this task, you
will: (a) Explicitly focus on the
image content to understand the
context and provide a meaningful
description and "
"(b) pay close attention to the
extracted text to enrich your
description and support your
analysis. "
"Finally, provide response in valid JSON
format with two fields with a
format: {\"description\": \"text\",
\"classification\": \"propaganda\"}.
Output only json. "
"The \"description\" should be very
short in maximum 100 words and \"
classification\" label should be \"
propaganda\" or \"not-propaganda\"
or \"not-meme\" or \"other\". "
"Note, other is a category, which is
used to label the image that does
not fall in any of the previous
category."
)
Listing 1: Zero-shot prompt example for GPT-4.Model Acc W-P W-R W-F1 M-F1
Baseline
Majority 0.658 0.433 0.659 0.522 0.198
Random 0.479 0.518 0.479 0.479 0.239
Unimodal - Text
Ngram 0.669 0.624 0.669 0.582 0.280
AraBERT 0.688 0.670 0.688 0.666 0.511
Qarib 0.697 0.688 0.697 0.690 0.551
mBERT 0.707 0.688 0.707 0.675 0.487
XLM-r 0.699 0.676 0.699 0.678 0.489
GPT-4v 0.664 0.620 0.664 0.624 0.384
GPT-4o 0.573 0.611 0.573 0.579 0.350
Unimodal - Image
CNeXt + SVM 0.655 0.608 0.655 0.614 0.405
MobileNet (v2) 0.660 0.618 0.660 0.620 0.426
ResNet18 0.656 0.597 0.656 0.593 0.358
ResNet50 0.660 0.638 0.660 0.637 0.434
Vgg16 0.656 0.597 0.656 0.593 0.358
Eff (b7) 0.660 0.597 0.660 0.595 0.352
GPT-4v 0.565 0.551 0.565 0.545 0.223
GPT-4o 0.693 0.627 0.693 0.634 0.305
Multimodal
CNeXt + ArB + SVM 0.683 0.655 0.683 0.659 0.513
Gemini 0.519 0.551 0.519 0.521 0.276
GPT-4v 0.681 0.461 0.330 0.619 0.340
GPT-4o 0.653 0.443 0.354 0.639 0.363
Table 4: Classification with different modalities. CNeXt:
ConvNeXt, Eff (b7): Efficientnet (b7), Gemini: Gemini-
1.5-flash-preview-0514l, GPT-4v: GPT-4-vision (gpt-
4-vision-preview) W-*: weighted average; M-: Macro
average. XLM-r: XLM-RoBERTa base.
5 Results and Discussion
In Table 4, we report the detailed classification
results for different modalities and models. All
models outperform the majority and random base-
lines. Among the text-based models, the fine-tuned
Qarib model outperforms all other models, achiev-
ing the best results ( 0.690 weighted F1) across all
modalities and models. AraBERT is the second-
best fine-tuned model, with a weighted F1-score
of 0.666 among the text-based models. The per-
formance of multilingual transformer models is
relatively worse than that of monolingual models.
For the image-based models, the fine-tuned
ResNet50 shows the best result ( 0.673 weighted
F1) among all other fine-tuned models and GPT-
4o model. The performance of MobileNet (v2)
andCNeXt + SVM rank as the second and third
best among the fine-tuned models. The results of
VGG16 and EfficientNet (b7) are almost similar.For the multimodal models, the model trained
with ConvNeXt + AraBERT + SVM shows the high-
est performance (0.659 weighted F1) among the
multimodal LLMs. The performance of Gemini
is significantly worse than that of the GPT-4 vari-
ants. GPT-4o demonstrates higher performance
compared to GPT-4 Vision.
In our experiments all multimodal model are
tested using zero-shot setting, therefore, such lower
performance compared to the fine-tuned models are
expected.
6 Additional Experiments
We further conducted experiments using the dataset
released as part of the ArAIEval shared task 2
(Hasanain et al., 2024b), focusing on two labels:
propaganda and not-propaganda. The dataset statis-
tics are provided in Table 5. The goal was to inves-
tigate model performance in a binary classification
scenario and we benchmarked this dataset using
multimodal models.
Class labels Train Dev Test Total
Not propaganda 1,540 224 436 2,200
Propaganda 603 88 171 862
Total 2,143 312 607 3,062
Table 5: Distribution of dataset for ArAIEval shared
task 2.
Table 6 presents the competitive results of three
multimodal models with image-only input: GPT-
4o, GPT-4 Turbo, and Gemini Pro 1.0. Among
these models, GPT-4o significantly outperforms the
others and demonstrates the highest performance
across all evaluated metrics, achieving an accuracy
of 85.17%, a precision of 84.80, a recall of 85.17,
and a weighted F1-score of 84.87. In comparison,
GPT-4 Turbo lags behind GPT-4o in all metrics,
with an accuracy of 76.44%, indicating a significant
performance drop compared to GPT-4o. Gemini
Pro 1.0 shows lower performance than the GPT-4
models, with an accuracy of 72.47%.
Model Acc. W-P W-R W-F1 M-F1
Gemini 0.725 0.685 0.725 0.663 0.345
GPT-4v 0.764 0.748 0.764 0.735 0.645
GPT-4o 0.852 0.848 0.852 0.849 0.810
Table 6: Results on ArAIEval dataset. Gemini: version
Pro 1.0.7 Conclusions and Future Work
In this study, we introduce a manually annotated
dataset for detecting propaganda in Arabic memes.
We have annotated ∼6K memes with four different
categories, making it the first such resource for Ara-
bic content. To facilitate future annotation efforts
for this type of content, we developed annotation
guidelines in both English and Arabic and are re-
leasing them to the community. Our work provides
an in-depth analysis of the dataset and includes
extensive experiments focusing on different modal-
ities and models, including pre-trained language
models (PLMs), large language models (LLMs),
and multimodal LLMs. Our results indicate that
fine-tuned models significantly outperform LLMs.
In future work, we plan to extend the dataset with
further annotations that include hateful, offensive,
and propagandistic techniques.
8 Limitations
The dataset we have collected originates from var-
ious public groups on Facebook, Instagram, Pin-
terest, and Twitter. The annotated dataset is highly
imbalanced, which may affect model performance.
Therefore, it is important to develop models with
this aspect in mind.
Ethics and Broader Impact
Our dataset solely comprises memes, and we have
not collected any user information; therefore, the
privacy risk is nonexistent. It is important to note
that annotations are subjective, which inevitably
introduces biases into our dataset. However, our
clear annotation schema and instructions aim to
minimize these biases. We urge researchers and
users of this dataset to remain critical of its po-
tential limitations when developing models or con-
ducting further research. Models developed using
this dataset could be invaluable to fact-checkers,
journalists, and social media platforms.
Acknowledgments
The work of F. Alam, M. Hasanain, and F. Ahmed
is supported by the NPRP grant 14C-0916-210015
from the Qatar National Research Fund part of
Qatar Research Development and Innovation Coun-
cil (QRDI). The findings achieved herein are solely
the responsibility of the authors.References
Shamsiah Abd Kadir, Anitawati Lokman, and
T. Tsuchiya. 2016. Emotion and techniques of propa-
ganda in YouTube videos. Indian Journal of Science
and Technology , V ol (9).
Ahmed Abdelali, Sabit Hassan, Hamdy Mubarak, Ka-
reem Darwish, and Younes Samih. 2021. Pre-training
bert on arabic tweets: Practical considerations.
Ahmed Abdelali, Hamdy Mubarak, Shammur Chowd-
hury, Maram Hasanain, Basel Mousi, Sabri Boughor-
bel, Samir Abdaljalil, Yassine El Kheir, Daniel Izham,
Fahim Dalvi, Majd Hawasly, Nizi Nazar, Youssef
Elshahawy, Ahmed Ali, Nadir Durrani, Natasa Milic-
Frayling, and Firoj Alam. 2024. LAraBench: Bench-
marking Arabic AI with large language models. In
Proceedings of the 18th Conference of the European
Chapter of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 487–520,
St. Julian’s, Malta. Association for Computational
Linguistics.
Firoj Alam, Stefano Cresci, Tanmoy Chakraborty, Fab-
rizio Silvestri, Dimiter Dimitrov, Giovanni Da San
Martino, Shaden Shaar, Hamed Firooz, and Preslav
Nakov. 2022a. A survey on multimodal disinfor-
mation detection. In Proceedings of the 29th Inter-
national Conference on Computational Linguistics ,
pages 6625–6643, Gyeongju, Republic of Korea. In-
ternational Committee on Computational Linguistics.
Firoj Alam, Fahim Dalvi, Shaden Shaar, Nadir Dur-
rani, Hamdy Mubarak, Alex Nikolov, Giovanni
Da San Martino, Ahmed Abdelali, Hassan Sajjad,
Kareem Darwish, et al. 2021. Fighting the covid-
19 infodemic in social media: a holistic perspective
and a call to arms. In Proceedings of the Interna-
tional AAAI Conference on Web and Social Media ,
volume 15, pages 913–922.
Firoj Alam, Hamdy Mubarak, Wajdi Zaghouani, Gio-
vanni Da San Martino, and Preslav Nakov. 2022b.
Overview of the WANLP 2022 shared task on pro-
paganda detection in Arabic. In Proceedings of the
Seventh Arabic Natural Language Processing Work-
shop , Abu Dhabi, UAE.
Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul
Alam, and Umair Qazi. 2020. Deep learning bench-
marks and datasets for social media image classi-
fication for disaster response. In 2020 IEEE/ACM
International Conference on Advances in Social Net-
works Analysis and Mining (ASONAM) , pages 151–
158. IEEE.
Wissam Antoun, Fady Baly, and Hazem Hajj. 2020.
AraBERT: Transformer-based model for Arabic lan-
guage understanding. In Proceedings of the 4th Work-
shop on Open-Source Arabic Corpora and Process-
ing Tools, with a Shared Task on Offensive Language
Detection , pages 9–15.
Alberto Barrón-Cedeno, Israa Jaradat, Giovanni
Da San Martino, and Preslav Nakov. 2019. Proppy:
Organizing the news based on their propagandistic
content. Information Processing & Management ,
56(5):1849–1864.Sian Brooke. 2019. “condescending, rude, assholes”:
Framing gender and hostility on stack overflow. In
Proceedings of the Third Workshop on Abusive Lan-
guage Online , pages 172–180, Florence, Italy. Asso-
ciation for Computational Linguistics.
Rui Cao, Roy Ka-Wei Lee, Wen-Haw Chong, and Jing
Jiang. 2022. Prompting for multimodal hateful meme
classification. In Proceedings of the 2022 Confer-
ence on Empirical Methods in Natural Language
Processing , pages 321–332, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.
Pengyuan Chen, Lei Zhao, Yangheran Piao, Hongwei
Ding, and Xiaohui Cui. 2024. Multimodal visual-
textual object graph attention network for propaganda
detection in memes. Multimedia Tools and Applica-
tions , 83(12):36629–36644.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsuper-
vised cross-lingual representation learning at scale.
arXiv:1911.02116 .
Padraig Cunningham and Sarah Jane Delany. 2007. k-
nearest neighbour classifiers. Multiple Classifier Sys-
tems, 34(8):1–17.
Giovanni Da San Martino, Alberto Barrón-Cedeño, and
Preslav Nakov. 2019. Findings of the NLP4IF-2019
shared task on fine-grained propaganda detection. In
Proceedings of the Second Workshop on Natural Lan-
guage Processing for Internet Freedom: Censorship,
Disinformation, and Propaganda , NLP4IF ’19, pages
162–170, Hong Kong, China.
Giovanni Da San Martino, Seunghak Yu, Alberto
Barrón-Cedeño, Rostislav Petrov, and Preslav Nakov.
2019. Fine-grained analysis of propaganda in news
articles. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on
Natural Language Processing , EMNLP-IJCNLP ’19,
pages 5636–5646, Hong Kong, China.
Fahim Dalvi, Maram Hasanain, Sabri Boughorbel,
Basel Mousi, Samir Abdaljalil, Nizi Nazar, Ahmed
Abdelali, Shammur Absar Chowdhury, Hamdy
Mubarak, Ahmed Ali, Majd Hawasly, Nadir Dur-
rani, and Firoj Alam. 2024. LLMeBench: A flexible
framework for accelerating LLMs benchmarking. In
Proceedings of the 18th Conference of the European
Chapter of the Association for Computational Lin-
guistics: System Demonstrations , pages 214–222, St.
Julians, Malta. Association for Computational Lin-
guistics.
Thomas Davidson, Dana Warmsley, Michael Macy, and
Ingmar Weber. 2017. Automated hate speech de-
tection and the problem of offensive language. In
Proceedings of the International AAAI Conference
on Web and Social Media , volume 11 of AAAI ’17 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proc. of the 2019 Conference of the
NAACL , NAACL-HLT ’19, Minneapolis, MN, USA.Dimitar Dimitrov, Firoj Alam, Maram Hasanain, Abul
Hasnat, Fabrizio Silvestri, Preslav Nakov, and Gio-
vanni Da San Martino. 2024. Semeval-2024 task 4:
Multilingual detection of persuasion techniques in
memes. In Proceedings of the 2024 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics .
Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj
Alam, Fabrizio Silvestri, Hamed Firooz, Preslav
Nakov, and Giovanni Da San Martino. 2021a. De-
tecting propaganda techniques in memes. In Pro-
ceedings of the Joint Conference of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing , ACL-IJCNLP ’21,
pages 6603–6617.
Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj
Alam, Fabrizio Silvestri, Hamed Firooz, Preslav
Nakov, and Giovanni Da San Martino. 2021b. Task
6 at SemEval-2021: Detection of persuasion tech-
niques in texts and images. In Proceedings of the
15th International Workshop on Semantic Evalua-
tion, SemEval ’21, Bangkok, Thailand.
Institute for Propaganda Analysis. 1938. In Institute
for Propaganda Analysis, editor, Propaganda Analy-
sis. Volume I of the Publications of the Institute for
Propaganda Analysis , chapter 2. New York, NY .
Shreyansh Gandhi, Samrat Kokkula, Abon Chaudhuri,
Alessandro Magnani, Theban Stanley, Behzad Ah-
madi, Venkatesh Kandaswamy, Omer Ovenc, and
Shie Mannor. 2020. Scalable detection of offensive
and non-compliant content / logo in product images.
WACV , pages 2236–2245.
Maria Glenski, E. Ayton, J. Mendoza, and Svitlana
V olkova. 2019. Multilingual multimodal digital de-
ception detection and disinformation spread across
social platforms. ArXiv , abs/1909.05838.
Raul Gomez, Jaume Gibert, Lluis Gomez, and Dimos-
thenis Karatzas. 2020. Exploring hate speech detec-
tion in multimodal publications. In WACV , pages
1470–1478.
Ivan Habernal, Raffael Hannemann, Christian Pol-
lak, Christopher Klamm, Patrick Pauli, and Iryna
Gurevych. 2017. Argotario: Computational argu-
mentation meets serious games. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations ,
EMNLP ’17, pages 7–12, Copenhagen, Denmark.
Ivan Habernal, Patrick Pauli, and Iryna Gurevych. 2018.
Adapting serious game for fallacious argumentation
to German: Pitfalls, insights, and best practices. In
LREC . European Language Resources Association
(ELRA).
Maram Hasanain, Fatema Ahmed, and Firoj Alam.
2024a. Can gpt-4 identify propaganda? annotation
and detection of propaganda spans in news articles.
InProceedings of the 2024 Joint International Con-
ference On Computational Linguistics, Language
Resources And Evaluation , LREC-COLING 2024,
Torino, Italy.Maram Hasanain, Firoj Alam, Hamdy Mubarak, Samir
Abdaljalil, Wajdi Zaghouani, Preslav Nakov, Gio-
vanni Da San Martino, and Abed Freihat. 2023.
ArAIEval shared task: Persuasion techniques and
disinformation detection in Arabic text. In Proceed-
ings of ArabicNLP 2023 , pages 483–493, Singapore
(Hybrid). Association for Computational Linguistics.
Maram Hasanain, Md. Arid Hasan, Fatema Ahmed,
Reem Suwaileh, Md. Rafiul Biswas, Wajdi Za-
ghouani, and Firoj Alam. 2024b. Araieval shared
task: Propagandistic techniques detection in uni-
modal and multimodal arabic content. In Proceed-
ings of the Second Arabic Natural Language Process-
ing Conference (ArabicNLP 2024) , Bangkok. Asso-
ciation for Computational Linguistics.
Abul Hasnat, Nadiya Shvai, Assan Sanogo, Marouan
Khata, Arcadi Llanza, Antoine Meicler, and Amir
Nakib. 2019. Application guided image quality es-
timation based on classification. In 2019 IEEE In-
ternational Conference on Image Processing (ICIP) ,
pages 549–553. IEEE.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , CVPR ’16,
pages 770–778. IEEE.
Ming Shan Hee, Shivam Sharma, Rui Cao, Palash
Nandi, Preslav Nakov, Tanmoy Chakraborty, and
Roy Ka-Wei Lee. 2024. Recent advances in hate
speech moderation: Multimodality and the role of
large models. arXiv preprint arXiv:2401.16727 .
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. 2017. Mobilenets:
Efficient convolutional neural networks for mobile
vision applications. arXiv:1704.04861 .
Srecko Joksimovic, Ryan S. Baker, Jaclyn Ocumpaugh,
Juan Miguel L. Andres, Ivan Tot, Elle Yuan Wang,
and Shane Dawson. 2019. Automated identification
of verbally abusive behaviors in online discussions.
InProceedings of the Third Workshop on Abusive
Language Online , pages 36–45, Florence, Italy. As-
sociation for Computational Linguistics.
Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, and
Davide Testuggine. 2019. Supervised multimodal
bitransformers for classifying images and text.
InProceedings of the NeurIPS 2019 Workshop
on Visually Grounded Interaction and Language ,
ViGIL@NeurIPS ’19.
Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj
Goswami, Amanpreet Singh, Pratik Ringshia, and
Davide Testuggine. 2020. The hateful memes chal-
lenge: Detecting hate speech in multimodal memes.
InProceedings of the Annual Conference on Neural
Information Processing Systems , NeurIPS ’20.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations .J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics .
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui
Hsieh, and Kai-Wei Chang. 2019. VisualBERT: A
simple and performant baseline for vision and lan-
guage. arXiv preprint arXiv:1908.03557 .
Minghui Liao, Zhisheng Zou, Zhaoyi Wan, Cong Yao,
and Xiang Bai. 2022. Real-time scene text detection
with differentiable binarization and adaptive scale
fusion. IEEE transactions on pattern analysis and
machine intelligence , 45(1):919–931.
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Fe-
ichtenhofer, Trevor Darrell, and Saining Xie. 2022.
A convnet for the 2020s. In Proceedings of the
IEEE/CVF conference on computer vision and pat-
tern recognition , pages 11976–11986.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
2019. ViLBERT: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks.
InProceedings of the Conference on Neural Infor-
mation Processing Systems , NeurIPS ’19, Vancouver,
Canada.
R OpenAI. 2023. Gpt-4 technical report. arXiv , pages
2303–08774.
Alan Partington and Charlotte Taylor. 2017. The lan-
guage of persuasion in politics: An introduction .
Routledge.
Jakub Piskorski, Nicolas Stefanovitch, Giovanni
Da San Martino, and Preslav Nakov. 2023a.
SemEval-2023 task 3: Detecting the category, the
framing, and the persuasion techniques in online
news in a multi-lingual setup. In Proceedings of
the 17th International Workshop on Semantic Eval-
uation (SemEval-2023) , pages 2343–2361, Toronto,
Canada. Association for Computational Linguistics.
Jakub Piskorski, Nicolas Stefanovitch, Nikolaos Niko-
laidis, Giovanni Da San Martino, and Preslav Nakov.
2023b. Multilingual multifaceted understanding of
online news in terms of genre, framing, and persua-
sion techniques. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 3001–3022,
Toronto, Canada. Association for Computational Lin-
guistics.
Nirmalendu Prakash, Han Wang, Nguyen Khoi Hoang,
Ming Shan Hee, and Roy Ka-Wei Lee. 2023.
PromptMTopic: Unsupervised multimodal topic
modeling of memes using large language models.
InProceedings of the 31st ACM International Con-
ference on Multimedia , MM ’23, page 621–631, New
York, NY , USA. Association for Computing Machin-
ery.
Anna Schmidt and Michael Wiegand. 2017. A survey
on hate speech detection using natural language pro-
cessing. In Proceedings of the Fifth International
Workshop on Natural Language Processing for So-
cial Media , pages 1–10, Valencia, Spain. Association
for Computational Linguistics.Hyunjin Seo. 2014. Visual propaganda in the age of
social media: An empirical analysis of Twitter im-
ages during the 2012 Israeli–Hamas conflict. Visual
Communication Quarterly , 21(3).
Shivam Sharma, Firoj Alam, Md. Shad Akhtar, Dimitar
Dimitrov, Giovanni Da San Martino, Hamed Firooz,
Alon Halevy, Fabrizio Silvestri, Preslav Nakov, and
Tanmoy Chakraborty. 2022. Detecting and under-
standing harmful memes: A survey. In Proceedings
of the Thirty-First International Joint Conference on
Artificial Intelligence , IJCAI ’22, pages 5597–5606,
Vienna, Austria. International Joint Conferences on
Artificial Intelligence Organization. Survey Track.
Limor Shifman. 2013. Memes in digital culture . MIT
press.
Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556 .
Shardul Suryawanshi, Bharathi Raja Chakravarthi, Mi-
hael Arcan, and Paul Buitelaar. 2020. Multimodal
meme dataset (MultiOFF) for identifying offensive
content in image and text. In TRAC , pages 32–41.
Mingxing Tan and Quoc V Le. 2019. Efficientnet: Re-
thinking model scaling for convolutional neural net-
works. arXiv:1905.11946 .
Gemini Team, Rohan Anil, Sebastian Borgeaud,
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Cynthia Van Hee, Els Lefever, Ben Verhoeven, Julie
Mennes, Bart Desmet, Guy De Pauw, Walter Daele-
mans, and Veronique Hoste. 2015. Detection and
fine-grained classification of cyberbullying events. In
Proceedings of the International Conference Recent
Advances in Natural Language Processing , pages
672–680, Hissar, Bulgaria. INCOMA Ltd. Shoumen,
BULGARIA.
Svitlana V olkova, Ellyn Ayton, Dustin L. Arendt,
Zhuanyi Huang, and Brian Hutchinson. 2019. Ex-
plaining multimodal deceptive news prediction mod-
els. In Proceedings of the International Conference
on Web and Social Media , ICWSM ’19, Munich,
Germany.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-
icz, and Jamie Brew. 2019. HuggingFace’s Trans-
formers: State-of-the-art natural language processing.
arXiv:abs/1910.03771 .
Ching Seh Wu and Unnathi Bhandary. 2020. Detection
of hate speech in videos using machine learning. In
CSCI , pages 585–590.
Peng Xu, Xiatian Zhu, and David A Clifton. 2023. Mul-
timodal learning with transformers: A survey. IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence .Prosper Nunayon Zannu, Felix Olajide Talabi, Ber-
nice Oluwalanu Sanusi, Oluwakemi A. Adesina, Ade-
bola Adewunmi Aderibigbe, Omowale T. Adelabu,
Oloyede Oyinloye, and Samson Adedapo Bello. 2024.
Influence of media literacy on the dissemination of
fake news among instagram and twitter users. Inter-
national Research Journal of Multidisciplinary Scope
(IRJMS) , 5(2):246–255. Original Article.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
survey of large language models. arXiv preprint
arXiv:2303.18223 .
A Details of The Experiments
For the experiments with transformer models, we
adhered to the following hyper-parameters during
the fine-tuning process. Additionally, we have re-
leased all our scripts for the reproducibility.
• Batch size: 8;
• Learning rate (Adam): 2e-5;
• Number of epochs: 10;
• Max seq length: 256.
Models and Parameters:
•AraBERT : L=12, H=768, A=12; the total
number of parameters is 371M.
•XLM-RoBERTa (xlm-roberta-base): L=24,
H=1027, A=16; the total number of parame-
ters is 355M.
B Annotation Task
We designed the annotation instructions through
careful analysis and discussion, followed by iter-
ative refinements based on observations and in-
put from the annotators based on the pilot annota-
tion. Our annotation schema is structured into two
phases as discussed below.
B.1 Phases of Annotations
To ensure the quality of the annotation and facilitate
the work of annotators, we conducted the annota-
tion in two phases: (i)image categorization and (ii)
text editing. The first phase (see Section B.2) fo-
cuses primarily on categorizing the images shown
on the interface. In the second phase (see Section
B.3), our goal is to edit the text that can be seen
on the images only for images that were labelled
as memes and as propagandistic or not propagan-
distic. The motivation for editing the text for these
categories is to further utilize them for other annota-
tion tasks. For example, propagandistic memes can
be further annotated with specific propagandisticFigure 4: A visual representation of the annotation pro-
cess. Block with yellow color represents phase 2.
techniques. In Figure 4, we illustrate the thought
process of the meme annotation phases.
B.2 Meme Categorization
B.2.1 Definition of a Meme:
Memes typically consist of a background image,
which could be a photograph, illustration, or screen-
shot, and a layer of text that adds context, humor,
or commentary to the image. The text is usually
placed at the top and/or bottom of the image but
not always. The combination of the image and the
text creates a specific message, joke, or commen-
tary that is meant to be easily understood, relatable,
and shareable. Some characteristics of memes as
observed during analysis and discussion:
1. contains text overlaid on image.
2. The text has humor in it.
3. The image must meet points 1 and 2.
4.Some contents of the image have been edited.
5.Text might be added to different locations of
the image.
6.May use images of entities with facial ex-
pressions (human, animals, fictional charac-
ters,etc.), which are then used to construct
meaning alongside the added text.
7.May use an entity performing a certain ac-
tion that might be used to construct meaning
alongside the added text.
8.May use an entity that represents an idea or
culture, to construct meaning alongside the
added text.
9.May use screenshots from movie scenes and
dialogues with added comments to create
memes.
10.Most of the pictures used to make the meme
can be re-edited and a new funny comment
can be added to it.Note: In points 6, 7, and 8, the removal of the entity
from the images will affect the meaning. In other
words, if the entity is removed, then the meaning
will not be complete. This is what we mean by
constructing meaning.
B.2.2 Defining Propaganda:
Propaganda is any communication that deliberately
misrepresents symbols and/or entities, appealing to
emotions and prejudices while bypassing rational
thought, to influence its audience toward a specific
goal. Memes are created to be humorous; there-
fore, it is natural that they lack rational discussion.
Instead, they use content to appeal to emotions and
prejudices. For our task, we defined the following
four categories and annotated the images accord-
ingly.
(1) Not-Meme: For images that do not follow the
definition of a meme, examples of images labeled
as “not-meme” are shown in Figure 5.
(2) Other: For images that can be defined as
memes but fall under any of the criteria listed be-
low. Examples of images labeled as “not-meme”
are shown in Figure 6. The criteria for "Other":
1. Memes that rely on nudity and offensive con-
tent, unless the target of the offense is a fa-
mous, political, or religious entity.
2.Memes that rely on numbers or figures to con-
struct meaning.
3. Memes that show explicit nudity.
4. Memes that explicitly use offensive words.
5.Memes that are in a different language (not
Arabic).
6.Memes that you could not understand due to
the dialect it was written in, poor font size, or
for any other reason.
Note: Memes might contain words that have an
implicitly offensive meaning, or uses offensive
words may be aimed at social, religious, or po-
litical groups. In these cases, the meme does not
fall under this criterion.
(3) Not Propaganda: For memes that follow the
definition of memes but do not contain any propa-
ganda techniques, examples of images labeled as
“not propagandistic” are shown in Figure 7.
(4) Propaganda: For memes that follow the def-
inition of memes and contain propaganda tech-
niques, examples of images labeled as “propagan-
distic” are shown in Figure 8.Figure 5: Examples of images labeled as not-meme .
Figure 6: Examples of images labeled as other .
B.3 Text Editing
The task is to edit the text to match the text shown
in the image. The interface will show the picture,
alongside the text that is in it. The text was ex-
tracted automatically, so it might contain errors.
It might not reflect all that is sees in the picture.
Some important guidelines to follow for editing the
text are listed below:
1.Each part that is a standalone sentence and
makes complete meaning should be written as
one line.
2.Punctuation marks are considered a part of the
text. They need to be edited/added.
3.If the text is in columns, put first all the text
of the first column, then all the text of the next
column. This task will specifically address
memes in Arabic, so the first column should
be considered from the right. However, this is
not a rule, and memes might change this ori-
entation, so it is up to the annotator to decide
the order based on their understanding.
4.Rearrange the text so that there is one sentence
per line, if possible.
5.If there are separate blocks of text in different
locations of the image, start a new line from
each block.6.Leave a blank between two blocks of text if
they were shown in two different locations on
the picture.
7. Items that should be excluded from the text:
•Usernames and social media account
names (if visible in the image).
•Websites, logos, and any text that is not a
part of the meme, so that removing that
part does not affect the meaning of the
meme.
•Any text that is hidden and is hard to
read.
8.In special cases, a logo can be used in the
meme to create meaning. In this case, add the
text of the logo to the edited text, if needed.
Example 1: Figure 9 shows an example of a
meme, for editing the text that can be viewed it,
the following points are important:
• Each dialog box is one sentence
•Start a new line for each box (each box is a
different block of text)
•Remove any elements that are not part of the
meaning: account name and locationFigure 7: Examples of images labeled as not propaganda .
Figure 8: Examples of images labeled as propaganda .
Figure 9: An example of a meme for editing text.•Add or modify punctuation to suit what is
presented in the text
•Text after modification (text translated to EN
and read from the first speech bubble from
right):
Get him ... Get him... corner him...
get him so we can give him his rights
come... aren 't you coming??
come...take your rights you son of
a bastard
Wallah we gonna get you till...
we give you all your rights
you chick ....
Example 2: Figure 10 shows another example,
for which the following points are important.
•Text written in red is difficult to understand
and read, so it should not be included in the
text.
•The text written on the hat and the text in black
are each a different block of text. Start a line
for each of them and leave a space for each
new line.Figure 10: An example of a meme for editing text.
•This example is for illustrative purposes only,
and “memes” in English will not be shown in
this task.
• Text after modification:
Bernie
Riding with Biden **2020**
Haha hey its the Obama guy
C Annotation Platform
C.1 Meme Categorization Task
In Figure 11, we provide a screenshot of the anno-
tation platform for the meme categorization task.
As shown in the figure, the platform displays the
meme itself on the right, the extracted text on the
left, a link to the annotation guidelines, and labels
with buttons at the bottom for selecting a category
for the meme. The task of the annotator was to
label the meme as one of the below categories, ac-
cording to the definitions detailed in the guideline
(see Section B). To facilitate the work of annotators
in the annotation process, we used the keywords
‘meme’ along with the labels ‘other’, ‘propaganda’,
and ‘not-propaganda’.
• Not Meme
• Meme, Other
• Meme, Not Propaganda
• Meme, Propaganda
Given that the memes we collected were from
different social media platforms, they may contain
offensive content. Therefore, we added a note that
some pictures may contain offensive content, and
that we apologize for any inconvenience that such
content may cause. We appreciate your contribu-
tion to this project which will minimize the spread
of such harmful content on the internet .
To further guide the annotation process, we
asked the annotators to follow the following steps.1.Begin by determining whether the image pre-
sented is a “meme”. If the image is not a
meme, select “Not Meme”, then click “Sub-
mit”. The next image will then be loaded.
2.If the image is a “meme”, assess whether it
falls under the category of “Other”. If so,
select “Other”, then click “Submit”. The next
image will then be loaded.
3.If the image does not fall under the cate-
gory of “Other”, choose one of the remain-
ing two labels based on your interpretation
of the meme’s content. After selecting the
appropriate label, edit the text as needed.
C.2 Text Editing Task
In this phase, the task was to edit the text based on
the guidelines discussed in Section B.3. In Figure
12, we provide a screenshot demonstrating the text
extracted from OCR, an editable text box, and the
original meme. The task was to edit the text to
match it with the original meme.
D Arabic Annotation Guideline
D.1 Meme Categorization
D.1.1 Definition of Meme
AîD
Ë@	¬A	
éJ
	®Ê	g ÉJÖßèPñ 	áÓ	àñºJK
		QÒJ
ÖÏ@ ð@Õæ
ÖÏ@
	àñºK	à@éJ
	®Ê	mÌ'@èPñË 	áºÖß 
ð ,ék	QÓ ð@J
ÊªJ»H.ñJºÓ	
©KAË@ 	áÓð ,éAé¢®Ë ð@ ú
jJ
	ñK ÕæP ð@éJ
	¯@Q	«ñKñ	¯èPñ

Ë	áºËðèPñË@ É	®@ ð@úÎ«@ ú
	¯ 	JË@ ©	ð ÕæK
	à@
ð@éËAP ZA 	@úÍ@	JË@ðèPñË@	á
K.©Òm.Ì'@ ø
XñK
ð , A Üß@X
, éK.QKAJË@ð , éÒê	¯ ÉîD 
	à@	Q	®ÖÏ@	áÓ	á
ªÓJ
ÊªK ð@éJº	K
AÒJ
Ó@é	ðQªÖ Ï@èPñË@	àñºJ úæÓ . èQå 	 ÉîD 
ð
1.èPñð H .ñJºÓ 		á
K.©Òm.'
2. ù
ëA¾	¯J
ÊªK ñë H .ñJºÖÏ@ 	JË@
3.	ªK.úÍ@é	¯A	BAK.	á
£QåË@ ú
	¯ñJ	à@èPñÊË YK .B
. É	®BAK.èPñ»	YÖÏ@ A	mÌ'@
4. . Õæ
ÖÏ@ ú
	¯éÓY	jJÖÏ@èPñË@ úÎ« ÉK
YªJË@
5. .èPñË@ úÎ«é¯Q	®JÓ	á» AÓ@úÎ« H.ñJºÓ 	é	¯A	@
6.	àAJ
» Xñk .ð ú
	¯ é	K@ ø
@ ,éK
Q
J.ªK Pñ Ð@Y	jJ@ QºK
,èPñË@ ú
	¯ ék .ñK.(éJ
ËAJ
	k HAJ
	m,	à@ñJ
k ,	àA	@)
©Ó ú	æªÓ ZA 	Bék.ñË@úÎ«éÓñQÖ Ï@H@Q
J.ªJË@ ÐY	jJ
. H.ñJºÖÏ@J
ÊªJË@Figure 11: A screenshot of the annotation platform for the meme categorization task.
Figure 12: An screenshot of the annotation platform for the text editing .
7.ÐY	jJ
	¯ , AÓ Éª	®K.Ðñ®K
	àAJ
»èPñË@ ú
	¯	àñºK 
Y¯
. H.ñJºÖÏ@J
ÊªJË@ ©Ó ú	æªÓ ZA 	BÉª	®Ë@
8.ÐY	jJ
	¯ ,(	mð@éÒ	¢	JÓ)	àAJ
»èPñË@ ú
	¯	àñºK 
Y¯
. H.ñJºÖÏ@J
ÊªJË@ ©Ó ú	æªÓ ZA 	B	àAJ
ºË@ ½Ë	X éÊJÖß 
AÓ
9.ÐC	¯@	áÓè	Xñ	kAÓ H@P@ñkð Pñ Ð@Y	jJ@ QºK
. AîD
Ê«J
ÊªKé	¯A	@ðHCÊÓð
10.é	¯A	AK.AêÓ@Y	jJ@èXA«@	áºÖß 
é	KAK.PñË@ Ñ	¢ªÓ	Q
ÒJK
.Q	k@ ù
ëA¾	¯J
ÊªK
ú
	¯ Y®	J	¯ , ú	æªÖÏ@é	¯A	@	á«	­ÊJ	m×ú	æªÖÏ@ ZA 	@:é	¢kCÓ
	àAJ
ºË@ ð@ Éª	®Ë@ ð@ ék.ñË@Q
J.ªK Xñk .ð ÐY«	à@ 6-8  A®	JË@
. ÉÒJºÓ Q
	« ú	æªÖÏ@	àñºJ 
 ð@ ú	æªÖÏ@Q
	ªJ
D.1.2 Definition of Propaganda
I.J
ËA@	áÓ H .ñÊB AêÓ@Y	jJ@ IJ
k	áÓÕæ
ÖÏ@úÎ« ½Òºk ú
	¯
É¾ : @Y	KA	«AK.ðQ.ÊË ú
ÍAJË@	­K
QªJË@úÎ« Y	J , @Y	KA	«AK.ðQ.Ë@
éK
ñK.@YÒªJÓéËAQË@ I .kA éJ 
	¯ Ðñ®K
É@ñJË@ ÈA¾ @	áÓ
i.m.kúÍ@Zñj.ÊË@	àðX H@	Q
jJË@ ð@ Q«A ÖÏ@èPAK@ð@	PñÓQË@
AÖß..	á
ªÓ	¬Yë ñm 	'Ñêª	¯Xð PñêÒm .Ì'@úÎ«Q
KAJÊË ½Ë	XðéJ
®¢	JÓ
Ym.'	áË ½	K@ lk.PB@úÎª	¯ , ù
ëA¾	¯ éÊ@ ú
	¯ 	 ñë Õæ
ÖÏ@	à@
	JË@ ð@èPñË@) 	JË@ ÐY	jJ
 ÉK .,éJ
®¢	JÓ i .m.kAîD
	¯
. Q«A ÖÏ@èPAK@ð	PñÓQË@ éK 
ñËéK
Q	jË@ (	á
	JKB@ ð@ H.ñJºÖÏ@
(1) Not-Meme: Figure 13	QÒJ
ÖÏ@	­K
QªK ©J.K B ú
æË@ PñË@	­J
	JË@ @	Yë Im'h.PY	JK
. ÉJ
ËYË@ @	Yë ú
	¯ Pñ»	YÖÏ@Figure 13: Examples of images labeled as not-meme .
(2) Other: Figure 14	QÒJ
ÖÏ@	­K
QªK ©J.K ú
æË@ PñË@	­J
	JË@ @	Yë Im'h.PY	JK
. èA	KX@ Õæ®Ë@ ú
	¯èXYj ÜÏ@Q
K
AªÖÏ@	áÓèYg@ð Im'©®K Aî	DºËð
:éÊJÓB@ðQ
K
AªÖÏ@
1.@Y« AÓ ,éJ
	KñKQ» HAJ
	mð Q	£A	JÓúÎ« YÒJªK ú
æË@	QÒJ
ÖÏ@
ð@éJ
AJ
 HAJ
	jË Zú
æÓ ú	æªÓ úÎ« ø
ñJm'ú
æË@
.é	JJ
ªÓ H .@	Qk@ðHA«AÔ g.ð@èPñîD Ó
2.ð@HA¢¢	m×ð@ ÐA¯P@úÎ«ú
Î¿ É¾ .YÒJªK ú
æË@	QÒJ
ÖÏ@
.éJ
	KAJ
K.ÐñP
3. . l'
Qå ø
Q« Qê	¢ ú
æË@	QÒJ
ÖÏ@
4.ZAJ
jÊËéXA	gðé¢j	JÓðéJ
K.A	KHAÒÊ¿ ÐY	jJ ú
æË@	QÒJ
ÖÏ@
.(****** Ë@ 	áK.@ : ÈAJÓ) l '
Qå É¾ .
5.Yg.ñK
B) .éJ
K.QªË@ Q
	«éJ
	KAKé	ªË ÐY	jJ ú
æË@	QÒJ
ÖÏ@
(ÈAJÓ
6.Ñm.kð@éj.êÊË@ I ...AêÒê	¯	áÓ 	áºÒJK ÕË ú
æË@	QÒJ
ÖÏ@
(ÈAJÓ Yg .ñK
B) .Q	k@ I.. ø
B ð@ ¡	mÌ'@
ð@ , ú
	æÖÞ	É¾.	áºËé¢j	JÓHAÒÊ¿ úÎ«	QÒJ
ÖÏ@ ø
ñJm'Y¯
: ÈAJÓ) ÉJ 
Ê®JË@ ð@éªÖÞéK
ñ AîD	Q	«ém'
QåéÒJ
úÎ«
H@PAJ .ªË@ ð@HAÒÊ¾Ë@ è	Yë h .PY	JK B .(YA	®Ë@ ú
æAJ
Ë@ ,	¯A	JÖÏ@
HA	®J
	JË@ 	áÓ AîD .A	JK
AÓ AêË Q	gA	¯ ,	­J
	JË@ @	Yë Im'
. øQ	kB@
(3) Not Propaganda: Figure 15	QÒJ
ÖÏ@	­K
QªK ©J.K ú
æË@ PñË@	­J
	JË@ @	Yë Im'h.PY	JK
. @Y	KA	«AK.ðQK.úÎ« ø
ñJm'B Aî	DºËð
(4) Propaganda: Figure 16	QÒJ
ÖÏ@	­K
QªK ©J.K ú
æË@ PñË@	­J
	JË@ @	Yë Im'h.PY	JK
. @Y	KA	«AK.ðQK.úÎ« ø
ñJm'ðD.2 Text Editing
AêÊK
YªKð ,éJ
K.Q« ñ	 QK
Qm'ñëéÒêÖÏ@ è	Yë ú
	¯ ½	JÓ H.ñÊ¢Ö Ï@
éêk.@ñË@ ½Ë	QªJ .é	ðQªÖ Ï@èPñË@ ú
	¯ Pñ»	YÖÏ@K.A¢JË
	JË@ 	áÒ	JK
Y¯ð , AJ
Ë@ Aî	DÓ h .Q	jJÖÏ@ 	JË@ ©ÓèPñ
ZA¢	k@ A	Jë Y®	K B 	áºË , A¯A	K	àñºK 
Y¯ ð@ ZA¢	k@ h.Q	jJÖÏ@
Pñ»	YÖÏ@K.A¢
B 	JË@	à@ Y®	K ÉK.,éK
ñm	'ZA¢	k@ ð@éJ
KCÓ@
:éÒêÖÏ@H@XA PB@	ªK.½J
Ë@.èPñË@ ú
	¯
1.Q¢ ú
	¯ ú	æªÓ É¾éÊ®JÓéÊÔg.É¿éK.AJ» I .m.'
. Yg@ð
2. 	áÓ @Z	Qk.Q.JªKèPñË@ ú
	¯ém	@ñË@ Õæ
¯QË@ HAÓC«
.ék.AmÌ'@ I.k AîD	¯A	@ð@ AëQK 
Qm'I.m.'
. 	JË@
3.Bð@ ©	 ,èYÔ«@ ú
	¯	ðQªÓèPñË@ ú
	¯ 	JË@	àA¿ @	X@
ú
	¯ ñ	JË@ É¿ Õç' , ÈðB@ XñÒªË@ ú
	¯ ñ	JË@ É¿
I.m.'
@	YËéJ
K.Q« ñ	úÎ« ÉÒªJ . ú
ÍAJË@ XñÒªË@
½Òê	®Ë ¼ð QÓ QÓB@	áºË ,	á
ÒJ
Ë@	áÓ ÈðB@ XñÒªË@ PAJ .J«@
ú	æªÖÏ@ ù
¢ªJË ÉÒm .Ì'@ I .KQK	à@ ÑêÖ Ï@ , 	JË@ ú	æªÖÏ
. H.ñÊ¢Ö Ï@
4.èYg@ðéÊÔg.¼A	Jë	àñºK 
IJ
m'.	JË@ I .
KQKèXA«AK.Õ¯
. A	JºÜØ½Ë	X	àA¿ @	X@,Q¢ É¿ ú
	¯
5.,èPñË@ ú
	¯éÊ	®	JÓéJ
	 Z@	Qk.@ ð@ ÉJ» ¼A	Jë I	KA¿ @	X@
.éÊJ» É¾Ë @YK
Yg.@Q¢@YK.@
6.	á
ª	ñÓ ú
	¯ A	Q« @	X@	JË@	áÓ	á
JÊJ»	á
K.A	«@Q	¯ ¼QK@
.èPñË@ ú
	¯	á
	®ÊJ	m×
7. : 	JË@	áÓéJ
ËAJË@ QåA	JªË@ È 	P@
•É@ñJË@ ÉKAð HAK.Ak ZAÖ Þ@ð	á
ÓY	jJÖÏ@ ZAÖÞ@
èPñË@ ú
	¯èQëA	£I	KA¿ ñË ú
«AÒJk.B@
•@Z	Qk.Q.JªK B H@PAª  ð@ ñ	 ð@ ¡.@ðP ø
@
. ú	æªÖÏ@ É¾ 
ø
	YË@ 	JË@	áÓFigure 14: Examples of images labeled as other .
Figure 15: Examples of images labeled as not propaganda .
• . éKZ@Q¯ I.ª
ð ù
	®	m×	 ø
@
8.ú
	¯ ÐY	jJ@ AÓ @ PAª	à@ Ym.'Y¯éA	g HBAg ú
	¯
	JË@ úÍ@ PAªË@ 		­	@ AëY	J« , ú	æªÓ ZA 	B	QÒJ
ÖÏ@
. èPQm'ø
	YË@
Example 1: Figure 17
:èPñË@ ú
	¯ 	JË@ ÉK 
YªKúÎ«HA	¢kCÓ
1.èYg@ðéÊÔg.Q.JªK
P@ñk ©K .QÓ É¿
2.éJ
	éÊJ» ñë ©K .QÓ É¿) ©K .QÓ É¾Ë @YK
Yg.@Q¢@YK.@
(é	®ÊJ	m×
3.H.AmÌ'@ Õæ@ : ú	æªÖÏ@	áÓ @Z	Qk.É¾ B QåA	J« ø
@ È	P@
©¯ñÖÏ@ð
4.	JË@ ú
	¯	ðQªÖ Ï@ I.A	JJË Õæ
¯QË@HAÓC« ÈY« ð@	­	
5. : ÉK
YªJË@ YªK .	JË@
ñ¯ñ®k éJ 
¢ª	Kú
Î	g ñJ .J
k.ðQåk@ Y . Y 
Ð@QmÌYËð AK 
½¯ñ®k	Xñ	k Am.'
@ ?? ú
æJJ
k.Am.'
@
pQ	¯ AK
	á
ÊÓA¿ ½¯ñ®k ½J 
¢ª	K . AÖÏ¼Y		àA¿ é<Ë@ð
Example 2: Figure 18
:èPñË@ ú
	¯ 	JË@ ÉK 
YªKúÎ«HA	¢kCÓ1.@	YË , QÔ gB@	àñÊËAK .H.ñJºÖÏ@ 	JË@èZ@Q¯ð Ñê	¯ I.ª
	JË@	áÖÞ		áÓ éª	ð ÐY« I .m.'
2.É¿ XñBAK.ø
	YË@ 	JË@ðéªJ.®Ë@úÎ« H.ñJºÖÏ@ 	JË@
A	«@Q	¯ ¼QK@ð Ñî	DÓ É¾Ë @ Q¢@YK.@,é	®ÊJ	m×éJ
	éÊJ» Ñî	DÓ
. YK
Yg.Q¢ É¿  .
3.	QÒJ
Ó@	Q« ÕæK
	áËð , ¡®	¯éËAg iJ 
	ñJË ÈAJÖÏ@ @	Yë
.éÒêÖÏ@ è	Yë ú
	¯éK
	Q
Êm.	'BAK.
4. : ÉK
YªJË@ YªK .	JË@
Bernie
Ridin with Biden **2020**
Haha hey its the Obama guy
Example 3: Figure 19
:èPñË@ ú
	¯ 	JË@ ÉK 
YªKúÎ«HA	¢kCÓ
1.éJ	¯A	@I.m.'
@	YË ú	æªÖÏ@	áÓ @Z 	Qk.éªÓAm.Ì'@ PAª  É¾ 
	JË@ úÍ@ 
2. éKZ@Q¯ ÉîD 
ø
	YË@ Z 	Qm.Ì'@ ¡®	¯	­J
		
3. : ÉK
YªJË@ YªK .	JË@
2018 \9\9 K.Ð@ðYË@Figure 16: Examples of images labeled as propaganda .
Figure 17: An example of a meme for editing text.
Figure 18: An example of a meme for editing text.
Figure 19: An example of a meme for editing text.
éJ
	KXPB@éªÓAm.Ì'@
éJ
ÖÞAêË@éªÓAm.Ì'@