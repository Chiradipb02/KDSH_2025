CONTESTS: a Framework for Consistency Testing of
Span Probabilities in Language Models
Eitan Wagner†Yuli Slavutsky‡Omri Abend†
†School of Computer Science and Engineering
‡Department of Statistics and Data Science
Hebrew University of Jerusalem
{first_name}.{last_name}@mail.huji.ac.il
Abstract
Although language model scores are often
treated as probabilities, their reliability as prob-
ability estimators has mainly been studied
through calibration, overlooking other aspects.
In particular, it is unclear whether language
models produce the same value for different
ways of assigning joint probabilities to word
spans. Our work introduces a novel framework,
ConTestS (Consistency Testing over Spans),
involving statistical tests to assess score consis-
tency across interchangeable completion and
conditioning orders. We conduct experiments
on post-release real and synthetic data to elim-
inate training effects. Our findings reveal that
both Masked Language Models (MLMs) and
autoregressive models exhibit inconsistent pre-
dictions, with autoregressive models showing
larger discrepancies. Larger MLMs tend to
produce more consistent predictions, while au-
toregressive models show the opposite trend.
Moreover, for both model types, prediction en-
tropies offer insights into the true word span
likelihood and therefore can aid in selecting op-
timal decoding strategies. The inconsistencies
revealed by our analysis, as well their connec-
tion to prediction entropies and differences be-
tween model types, can serve as useful guides
for future research on addressing these limita-
tions.1
1 Introduction
Pretrained Large Language Models (LLMs)
emerged as high-performance predictors for diverse
tasks (Brown et al., 2020). Tuning based on instruc-
tions and human feedback has further advanced
their performance (Wei et al., 2022; Ouyang et al.,
2022). In various applications, the model’s success
often depends solely on assigning high scores to the
correct options. Yet, their interpretation as proba-
bilities is beneficial in several applications, such as
1Code is provided at https://github.com/
eitanwagner/contests
Figure 1: Experimental Design - Joint Prediction Esti-
mation with Masked Language Modeling. The middle
white row displays the original unmasked tokens. Below,
in blue, the joint probability is calculated by first esti-
mating the probability of the correct token in MASK 1
and then of MASK 2(after revealing the correct token
in MASK 1). In the top rows, in green, the calculation is
in the reversed order – first estimating the probability of
the correct token in MASK 2and then in MASK 1(after
revealing MASK 2).
their treatment as confidence indicators for detect-
ing hallucinations (Manakul et al., 2023; Ren et al.,
2023) and robust ranking in sequence generation
tasks (Zhao et al., 2022).
Indeed, LLMs are commonly trained using the
cross-entropy objective, which due to Gibbs’ in-
equality is minimized by the true distribution. How-
ever, this global minima is hard to achieve (Chang
and McCallum, 2022; Zhu et al., 2023) , raising the
question of whether the model’s outputs can still
be interpreted as estimated probabilities.
While calibration of produced scores has been
extensively studied (Zhao et al., 2022; Shen et al.,
2024), multiple other aspects remain unexplored.
Measuring calibration for string density estima-
tion requires a ground-truth measure for the real
distribution, which is challenging to obtain (see
§3). Therefore, alternative methods to validate the
assumption that produced scores correspond to es-arXiv:2409.19984v1  [cs.CL]  30 Sep 2024timated probabilities, are needed.
For LLM scores to be interpreted as probabilities,
consistency across estimation methods is essen-
tial. However, regardless of probabilistic interpre-
tation, detecting and understanding inconsistencies
among estimation methods is crucial, especially in
the identification of preferable estimation methods.
Considering completion of word spans, consis-
tency implies that filling masks in different orders
(first filling one word and then the other, or vice
versa) produces the same joint probability (see Fig-
ure 1). In this work, we investigate whether this
requirement is fulfilled.
Various factorizations of a joint distribution into
conditional probabilities are possible, for instance
by applying the chain rule from left-to-right or
right-to-left. However, an unrestricted set of con-
ditional probabilities does not guarantee a unique
joint probability (see example in Appendix A.1).
Although restricting the conditional probabilities,
such as by disallowing cycles in Bayesian Net-
works, can ensure a unique joint probability, it is
not necessary (see example in Appendix A.2).
Masked language modeling (MLM) training
lacks mechanisms to ensure that a set of conditional
probabilities will form a unique joint distribution.
However, since language modeling is based on the
assumption that a distribution over strings gener-
ates the samples, estimated conditional distribu-
tions are expected to align with joint conditioning.
We investigate the consistency of both MLM
and autoregressive models (which include decoder-
only and encoder-decoder models), considering
that MLM can function as a missing token clas-
sification task or a generative task with a specific
instruction prompt (see §4). When treating autore-
gressive models, we account for task comprehen-
sion as a contributing factor.
We introduce a novel framework that employs
statistical tests – CONTESTS, for Consistency Test-
ing over Spans, to analyze discrepancies between
different estimation methods, and their behaviors
across various model types.
Our findings show that all examined LLMs fail
to produce consistent probabilities for joint span
modeling. However, we observe notable distinc-
tions among model types and sizes: autoregressive
models show increasing discrepancies as model
parameters increase, while MLMs tend to provide
more consistent predictions, with larger models
offering further improvements. Additionally, we
show that prediction entropies are indicative ofthe true likelihood for both model types, suggest-
ing their usefulness in selecting optimal decoding
strategies.
2 Preliminaries and Notation
The task of single-mask probabilistic masked lan-
guage modeling is to estimate the probability of
a masked location, given the rest of the sequence.
For a model Mand a sequence of tokens x=
(x1, x2...xn), we denote the estimation by
PM(xi=wi|x1=w1, . . . , x i−1=wi−1,
xi+1=wi+1, . . . , x n=wn).
When multiple masks are considered, that is,
when in addition to i, the positions j1, .., j kare
masked as well, the predicted distribution for the
i-th position given all unmasked ones is given by
PM(xi=wi|xj=wj∀j /∈ {i, j1, . . . , j k}),
and the joint distribution of two masked positions
i, j, given unmasked ones, is denoted by
PM(xi=wi, xj=wj|xk=wk∀k /∈ {i, j}).
Masked language modeling can be performed
with autoregressive models, predicting a span given
an appropriate instruction prompt. These models
predict PM(xi=s|x−i), where sis a span of ar-
bitrary length (i.e., len(s)≥1).2The predicted
length is determined by the prediction of an end-of-
sentence (EOS) token. As autoregressive models
are trained to estimate the probability of comple-
tions, the probability of the token followed by EOS
should match the MLM probability.3
Although we define the masked language mod-
eling task regardless of the prediction model, fol-
lowing common convention, we will use the simple
term MLM for models that were pretrained with
the MLM objective. We will explicitly mention the
use of autoregressive models.
For simplicity, we focus on the estimation of
joint probabilities of two tokens. To neutralize the
effect of word distances, we analyze probabilities
2We emphasize that the mask is a single token while the
predicted span can be longer.
3Like MLMs, autoregressive models are trained using
cross-entropy loss, which is minimized by the true distribution
over possible completions. Instruction tuning introduces addi-
tional objectives, but the model is regularized to stay close to
its pretrained state, suggesting that consistency of the scores
is a reasonable expectation in these models as well.of adjacent tokens P(xi, xi+1), which can be ex-
pressed in two forms:
P(x)
i,i+1(xi, xi+1):=P(xi|x−xi,−xi+1) (1)
·P(xi+1|x−xi+1)
P(x)
i+1,i(xi, xi+1):=P(xi+1|x−xi,−xi+1) (2)
·P(xi|x−xi).
When the identity of xiandxi+1is implicitly clear
we denote these expressions simply as Pi,i+1and
Pi+1,i. Although for the true distribution we have
thatPi,i+1=Pi+1,i, this may not hold for esti-
mated probabilities since each direction involves
two inference steps, each with slightly different
inputs.4
When analyzing discrepancies, we at times con-
sider the pointwise mutual information (PMI) be-
tween two random variables at x1∈ X 1, x2∈ X 2:
PMI( x1, x2) = logP(x1, x2)
P(x1)P(x2)(3)
and for the joint distribution, we have:
P(x1)·P(x2|x1) =P(x1, x2) =P(x2)·P(x1|x2)
⇔logP(x1|x2)
P(x1)= logP(x1, x2)
P(x1)P(x2)= logP(x2|x1)
P(x2).
3 Previous Work
Consistency. Few works have directly addressed
the assessment of consistency. Among those that
have, many focused on testing whether outputs ad-
here to predefined constraints. Li et al. (2019) and
Ribeiro et al. (2019) demonstrated violations of
logical constraints in question-answering. Elazar
et al. (2021) evaluated the internal knowledge con-
sistency of language models (LMs) comparing out-
puts for paraphrases of semantically identical ques-
tions. Pezeshkpour and Hruschka (2023) demon-
strated sensitivity to answer ordering in multiple-
choice questions. Qiu et al. (2023) illustrated incon-
sistent temporal reasoning in LLMs across various
time-related tasks.
In our work, we focus on the consistency of
probabilities rather than outputs. While identical
probabilities imply identical outputs, the reverse is
not necessarily true in language modeling, making
our approach more sensitive to inconsistencies.
4We note that our framework can easily be extended to to-
ken sequences of arbitrary length and not necessarily adjacent.
For a sequence of ntokens, we can mask them all and estimate
the joint probability by sequential estimation of conditional
probabilities. A well-defined probabilistic model will give the
same probability regardless of the order, yielding a total of n!
estimations.Calibration. A common approach to assessing
the quality of predicted probabilities is through cal-
ibration, which evaluates how well predicted prob-
ability scores align with membership probabilities
in some reference data. In fully calibrated multi-
class classifiers, calibration is considered for ev-
ery class in every prediction. However, evaluating
calibration, even in binned probabilities, becomes
challenging with a large number of classes, making
meaningful binning for every class with representa-
tive data difficult. To address this, many studies opt
fortop-class calibration (Guo et al., 2017), which
focuses solely on calibrating the predicted class.
Although top-class calibration is sufficient to as-
sess the confidence of the prediction, and therefore
is frequently used (Jiang et al., 2012; Guo et al.,
2017), full calibration is an essential requirement
for a model to be used as a density estimator in
multi-class classification, in structured predictions
such as sequence predictions in autoregressive text
generation, and complex probabilistic generative
models with textual components.
While measuring full calibration directly is chal-
lenging, our approach, which compares the con-
sistency of assigned probabilities to the same ex-
pression using different methods, offers alternative
means to identify uncalibrated models – inconsis-
tent estimations across different methods imply that
at least one of them is miscalibrated.
Many works measured the calibration of neu-
ral models (Guo et al., 2017; Wang et al., 2021),
generally finding that neural models are poorly cal-
ibrated (Chen et al., 2023). In LMs, most prior
work on calibration has focused on downstream
tasks, such as classification and question answer-
ing (Desai and Durrett, 2020; Dan and Roth, 2021).
Studies that specifically addressed language mod-
eling typically restricted their evaluations to top
predictions (Zhu et al., 2023), top-prediction sets
(Ravfogel et al., 2023), or aggregate measures like
entropy rates (Braverman et al., 2019). These eval-
uations have primarily examined autoregressive
models, consistently finding them to be miscali-
brated. While some research on masked language
models (MLMs) has suggested that they tend to be
relatively well-calibrated (He et al., 2023), to the
best of our knowledge, full-distribution calibration
in language modeling was never addressed.
Language Models as Density Estimators. Sev-
eral studies have interpreted language models as
density estimators and explored their probabilis-tic properties from a theoretical standpoint. Hahn
(2020) proved that some cases cannot be efficiently
modeled by Transformers. Du et al. (2023) showed
that the requirement that infinite length strings will
have zero probability might not be held for all mod-
els. However, they defined a theoretical notion of
tightness that is satisfied by most common models
and guarantees the requirement. Wang and Cho
(2019) showed that MLMs can be interpreted as
Markov Random Fields, thus providing probabil-
ities for entire sentences. In contrast, Yang et al.
(2018) showed theoretically that decoding based on
softmax yields low-rank approximations that are in-
adequate for capturing the complexity of language
distribution. Additionally, Chang and McCallum
(2022) presented findings indicating that decoding
based on a single embedding vector cannot gener-
ate arbitrarily rich distributions.
Research investigating sampling-based text gen-
eration includes the work of Zhang et al. (2021)
that showed that sampling from an LM distribution
results in low-quality text, and that the use of tem-
perature scaling provides a tradeoff between qual-
ity and diversity. Similarly, Holtzman et al. (2020)
proposed nucleus sampling, avoiding the tail of the
distribution, and Meister et al. (2023) presented a
sampling scheme based on the expected entropy of
natural language.
Several studies found conflicts between calibra-
tion and zero-shot capabilities. Zhu et al. (2023)
showed that instruction tuning significantly hurts
calibration. Kalai and Vempala (2023) proved
that strict calibration, with respect to the training
data, must lead to hallucinations. Lee et al. (2020)
showed a discrepancy between the cross-entropy
loss, used for language modeling, and task-specific
losses.
Other work disregarded the probabilistic nature
altogether, filling masks with spans from a refer-
ence document (Min et al., 2023). This comes
with the price of losing qualities of probabilistic
estimation.
4 Desired Properties of a Consistency
Testing Framework
Our goal is to evaluate whether LLMs maintain
consistency across different estimation orders when
calculating the joint probability of a word span. To
ensure the robustness and reliability of this evalua-
tion, it must meet the following requirements.Versatility. For the designed framework to apply
to various models, it should address both MLMs
and autoregressive models. It should account
for task comprehension in autoregressive models,
which are not specifically built for filling masks.
Significance of discrepancies. Minor variations
in estimated probabilities may arise due to numer-
ical issues. Additionally, discrepancies that are
symmetrically distributed around zero, lacking a
clear bias, may not have significant implications.
Therefore, the analysis should prioritize statisti-
cally significant discrepancies.
Nullifying the impact of exposure in training.
To prevent bias from analyzing data examples used
in model training, evaluations should incorporate
natural datasets that were not part of the model’s
training set.
Explainability. To be effective in both identify-
ing and addressing inconsistencies, the framework
should offer insights into contributing factors of
found inconsistencies, such as model types, sizes
(in parameters), and training data sizes. It should
be able to isolate the effect of each factor, keeping
the contribution of others fixed.
5 Consistency Testing Framework
We present the CONTESTSframework, designed
to meet the requirements outlined in §4. Here, we
detail how each requirement is addressed.
5.1 Task Comprehension in Autoregressive
Models
Autoregressive language models are typically
trained for next-token prediction and not directly
for Masked language modeling. However, the
MLM task was previously formulated as a con-
ditional case of autoregressive language modeling –
the T5 model (Raffel et al., 2020) was pretrained
to generate text spans when given sentinel tokens
in the input, and Bavarian et al. (2022) proposed
training decoder-only models with a prompt for
text infilling. This formulation allows us to derive
the distribution of the missing span by estimating
the sequence of next-token probabilities.
Since autoregressive masked language model-
ing depends on task comprehension, we examine
whether autoregressive models rank the true word
sequence similarly to MLMs. Additionally, since
these models allow for predictions of multiple to-
kens even when asked to fill one only, we analyzethe scores assigned to EOS as the second token. A
high probability for EOS as the second token is a
positive indicator for understanding the task.
5.2 Testing Discrepancy Significance
Since large language models usually provide small
probabilities, and due to the connection between
the expressions for the joint to the expression for
PMI (see Equation 3), here we examine the consis-
tency of a given model by the discrepancy between
estimations of the two expressions in log scale
di,i+1(xi, xi+1):= log Pi,i+1(xi, xi+1) (4)
−logPi+1,i(xi, xi+1).
For each text x(j)and a pair of consecutive to-
kens5xi, xi+1we compute d(j)
i,i+1(xi, xi+1). As
discrepancies d(j)are functions of the random vari-
ables x(j), they follow an unknown distribution f.
For a perfectly calibrated model, d(j)= 0for all j,
indicating a singleton mass of fat 0. In practice,
the distribution induced by a model Mis unknown
and requires non-parametric treatment. Therefore,
to test for the consistency of M, we employ the
paired two-sided Wilcoxon rank test (Wilcoxon,
1945) to assess the null hypothesis that fis sym-
metric around 0. That is,
H0:fis symmetric around µ= 0 (5)
H1:fis symmetric around µ̸= 0. (6)
To examine these hypotheses, the Wilcoxon test
employs the test statistic T=P
jsgn(d(j))R(j)
where R(j)is the rank of d(j)(i.e., position in
the sorted array) and sgn(d(j)) = 1 ifd(j)>0,
−1ifd(j)<0and 0 otherwise. The reliance
of the Wilcoxon test on ranks carries the follow-
ing implications: (1) it demonstrates robustness
to extreme discrepancy values, and (2) the focus
on ranks, rather than the discrepancy values them-
selves, increases the difficulty of rejecting the null
hypothesis, rendering it a conservative test.
In our analysis, we apply the Wilcoxon test to as-
sess the significance of discrepancy means for mul-
tiple models across various text datasets. Given that
testing a model across multiple datasets, and test-
ing multiple models on the same dataset, increases
the risk of type I error (the mistaken rejection of a
true null hypothesis), we conduct a correction for
5When the identity of iis immaterial, we simply use the
notation d(j).multiple comparisons. We use the relatively con-
servative correction proposed by Benjamini and
Yekutieli (2001) since it applies to dependent tests.
5.3 Data Gathering
To eliminate biases caused by exposure to data in
training, in addition to benchmark and synthetic
datasets, we constructed a new dataset by extracting
news articles with topics “WORLD”, “NATION”,
“BUSINESS”, “TECHNOLOGY”, “ENTERTAIN-
MENT”, “SCIENCE”, “SPORTS”, and “HEALTH”
from Google-news6. Extraction was conducted on
dates after the models’ training data cutoff.
5.4 Testing for Contributing Factors
To analyze differences between different model
types, while isolating factors such as their different
number of parameters or training volume, we con-
ducted a linear regression analysis. This approach
is chosen because, in linear regression, a coefficient
represents the change in the dependent variable as-
sociated with a one-unit change in the independent
variable, with all other variables held fixed.
For all considered models 1, . . . , K , letd(j)
krep-
resent discrepancy value computed for the k-th
model and a text x(j). for1≤j≤J. We set the
model type Tkto 1 if the model is autoregressive,
and set Tk= 0if it is an MLM. We investigate the
influence of the model type, parameter size Sk(in
billions of parameters), and the size of the training
set (in GB) Vkon the variance of discrepancy
νk(d):=Var(d(1)
k, . . . , d(J)
k). (7)
In this analysis, we consider the variance νkas
the dependent variable, while the other parameters
serve as explanatory variables:
ˆνk=ˆβ0+ˆβ1Sk+ˆβ2Vk+ˆβ3Tk+ˆβ4SkTk(8)
Whether a model adequately captures the vari-
ability of the explained variable, is often measured
by0≤R2≤1. When the model is indeed well
fitted ( R2close to 1), large and statistically signifi-
cant coefficients indicate a substantial effect of the
corresponding explanatory variables (in our case
the effect of model size within specific types).
For an autoregressive model, the estimated vari-
ance is
ˆνk= (ˆβ0+ˆβ3) +ˆβ2Vk+ (ˆβ1+ˆβ4)Sk,
6https://news.google.com/whereas for an MLM it is ˆνk=ˆβ0+ˆβ1Sk+ˆβ2Vk.
Consequently, the effect of the number of parame-
ters in MLMs is incorporated in ˆβ1and serves as
the baseline effect, against which autoregressive
models are estimated.
Note, that in contrast to discrepancy testing
where the analysis is performed for a given model
and the observations are texts in the dataset, here
the models themselves are treated as observations.
6 Experimental Design
6.1 Examined Models
MLMs. We conducted experiments with the fol-
lowing MLMs, each in multiple sizes (parameters):
RoBERTa (Liu et al., 2019) base (125m parame-
ters) and large (355m); XLM-RoBERTa (Conneau
et al., 2020) base (280m) and large (550m); we
also used the generator component from ELEC-
TRA (Clark et al., 2020) small (14m parameters in
the generator) base (110m), and large (335m).
MLMs are typically trained with randomly
masked tokens, including the possibility of adja-
cent masks (Devlin et al., 2019). Consequently, the
probabilities used in our experiments align with
the model’s training structure. For two adjacent
masks, the predicted probabilities should represent
the marginal distributions – i.e., the likelihood of
each token being correct, given the uncertainty of
the adjacent token. When a token is masked inde-
pendently of adjacent tokens, the prediction scores
should reflect the probability conditioned on the
surrounding context.
Autoregressive models. We performed experi-
ments using Flan-T5 (Chung et al., 2022) small,
base, large, xl, and xxl; LLAMA 2andLLAMA
2-C HAT (Touvron et al., 2023) with 7b, 13b, and
70b parameters each. We used 4-bit quantization
for all the autoregressive models, with nested quan-
tization for the LLAMA 2ones. These models suit
our experiments as they include multiple sizes for
the same architecture and settings. We tested vari-
ous prompts but did not notice notable differences.
Prompts for the reported results are in Appendix B.
We note that a natural option to test decoding
order is to use T5’s special tokens, putting them by
order or in reverse. This method proved problem-
atic, as T5 seems highly biased towards by-order
completion, the format it was trained on. Revers-
ing the order lowered the likelihood for all word
pairs, with a stronger impact on high-probabilitypairs (e.g., common phrases), thus introducing a
confounding reason for the discrepancy.
6.2 Data
Natural Text. We tested consistency over two
datasets with texts from a natural source. The first
wasWikitext-27dataset, where we ignored punctu-
ation, stop-words, and tokens that were not whole
words. We used the train section, consisting of
≈37Karticles. Since Wikitext was used during
the training of the models, we constructed a new
dataset as described in §5.3 for four dates: 2.7.2023,
6.7.2023, 4.9.2023 and 18.9.2023, all well after the
models’ data cutoff. The texts were pre-processed
in the same manner as in Wikitext. Altogether, the
News dataset consists of ≈2000 articles.
The exact set of word pairs evaluated in each
dataset differed between models as different model
types have different tokenizers. Model size (for a
given type) does not affect tokenization. Addition-
ally, due to computational limitations, a smaller
set was used for the larger models. In summary,
for RoBERTa and ELECTRA-generator we had
>200Kword pairs in Wikitext and >85Kin
News, for XLM-RoBERTa ≈110Kin Wikitext and
≈44Kin News, and for Flan-T5 ≈175Kin Wiki-
text and ≈55Kin News. For LLAMA 2(chat and
non-chat) in Wikitext, we had ≈85Kword pairs
for the 7b and 13b versions and ≈13Kfor the 70b
version, and in News, we had ≈11Kfor all sizes.
Synthetic Data. We performed all experiments
on synthetic data as well, in which the context is
fixed for all samples. See Appendix D.1 for details.
7 Main Results
7.1 Consistency
Here we provide an analysis of the distribution of
discrepancy values d(j)for each examined model
on real datasets. Analysis of the synthetic dataset
is available in Appendix D.2.
The results of the experiments, summarized in
Figure 2, show that on both Wikitext (2b) and News
(2a) datasets, the distribution of discrepancies in
MLMs is characterized by medians close to 0 and
high variance, while autoregressive models exhibit
discrepancies with medians further away from 0,
but often lower variances.
The Wilcoxon test results on real datasets show
that, except for Llama 2-chat-7b (p-value = 0.5997)
7https://huggingface.co/datasets/wikitext/
viewer/wikitext-2-raw-v1ELECTRA smallELECTRA baseELECTRA large RoBERT a baseRoBERT a large
XLM-RoBERT a baseXLM-RoBERT a largeLlama 2 7bLlama 2 13bLlama 2 70b
Llama 2 chat7b Llama 2 chat 13b Llama 2 chat 70bFlan-T5 smallFlan-T5 baseFlan-T5 largeFlan-T5 xlFlan-T5 xxl5
4
3
2
1
012345d(a) Wikitext
ELECTRA smallELECTRA baseELECTRA large RoBERT a baseRoBERT a large
XLM-RoBERT a baseXLM-RoBERT a largeLlama 2 7bLlama 2 13bLlama 2 70b
Llama 2 chat7b Llama 2 chat 13b Llama 2 chat 70bFlan-T5 smallFlan-T5 baseFlan-T5 largeFlan-T5 xlFlan-T5 xxl5
4
3
2
1
012345d
(b) News
Figure 2: Discrepancy Results on the (a) Wikitext and (b) News datasets. Each model is represented by a boxplot
displaying discrepancy values. MLMs appear in purple shades on the left of each figure and autoregressive models
in green on the right. Color intensity indicates model sizes. Boxes show quartile values with median lines; whiskers
extend to 1.5 IQR from quartiles. Outliers are omitted for clarity.
and Flan-T5-xl (p-value = 1.0), both on the News
dataset, all other models attain a corrected p-value
smaller than 0.05 on both datasets. Consequently,
although MLMs exhibit median estimators only
slightly different than 0, the null hypothesis (see
equation 5) of the distribution of discrepancies be-
ing centered around 0 is rejected for all these mod-
els. The highest corrected p-value among mod-
els where the null hypothesis is rejected is 0.0042,
achieved by RoBERTa-large on the News dataset,
indicating overall confidence levels above 99%.
7.2 Explaining Inconsistencies
The results shown in figures 2a, 2b indicate that
in real data, given a model type, MLMs exhibitWikitext News
Coeff P-value Coeff P-value
Intercept 0.7113 0.05 1.4002 0.001
Size 2.7683 <0.001 72.2883 0.001
Data size -0.00002 0.696 -0.0001 0.036
Type 0.0196 0.064 0.0154 0.126
I: Type – Size -2.4788 0.228 -3.0734 0.131
Table 1: Analysis of Discrepancy Variance. Regression
coefficients and corresponding p-values for each model
are presented, with MLM models serving as the baseline.
P-values below α= 0.05shown in bold.smaller variance as the number of parameters
grows. Autoregressive models show an opposite
trend. We performed a linear regression analysis
(see equation 9) to test the significance of these
trends, with the results summarized in table 1.
The results on both datasets indicate with signif-
icance level α <0.001that the variance increases
with the growth of the number of parameters. The
size of the training set does not have a significant
effect on the variance on the Wikitext dataset but in-
fluences the variance observed on the News dataset
with a significance level of α= 0.036.
Both regression models capture the variability of
νkvalues with R2values of 0.775and0.794for the
models fitted on the Wikitext and News datasets,
respectively.
In appendix C we provide a similar analysis,
where instead of considering model types as autore-
gressive or MLMs, we consider their fine-grained
model types (RoBERTa, ELECTRA, etc.).
8 Task Comprehension in Autoregressive
Models
Our results show that autoregressive models tend
to be less consistent, prompting the question of
how much this inconsistency can be attributed to
the more challenging task setting. To investigate
this, we compare token ranks between MLMs and
autoregressive models, where lower ranks indicate
higher scores. Ideally, both models should show
similar rank distributions, indicating similar perfor-
mance. Our analysis, shown in Figure 3, indicates
that while MLMs assign lower ranks to the true
first token, Flan-T5, and LLAMA 2models often
assign even lower ranks to the second token, sug-
gesting improved predictions. However, LLAMA
2-C HAT models exhibit significantly higher ranks,
indicating poorer performance.8
In Appendix E, we provide a detailed analysis
of probabilities assigned to an EOS token follow-
ing the predicted missing word, as an additional
measure of task comprehension. We find a positive
correlation between models excelling in word rank
predictions and those showing good task compre-
hension (indicated by lower EOS ranks). However,
the poor performance of LLAMA 2-C HAT models
8We emphasize that the comparison is not between the
absolute ranks for the different prediction types, or between
them and chance level, but rather between MLMs and autore-
gressive models relative to one another. We also note that we
did not compare second-mask prediction where the decoding
order is reversed between the two tokens.cannot be solely attributed to a lack of task un-
derstanding, as they assign low ranks to EOS but
high ranks to actual words. Across all model types,
larger models in each category generally exhibit
better task comprehension.
9Are there Preferable Decoding Orders?
We showed that all examined models exhibit incon-
sistencies between different orders of estimation of
joint probabilities of word spans. This raises the
question of whether any of the examined comple-
tion orders yields higher scores for the true tokens.
To address this question, we analyze the cor-
relation between di,i+1and the entropies of
the estimated probabilities involved in the joint
probability estimations, which we denote with
Hi, Hi+1|i, Hi+1, Hi|i+1. A summary of the corre-
lations is shown in Figure 4.
The analysis reveals that: (1) for entropies of
predictions when two tokens are masked (i.e.,
Hi, Hi+1) the correlation with the discrepancy in
the corresponding order ( di,i+1, -di,i+1) is negative;
(2) for one-mask entropies ( Hi|i+1, Hi+1|i) the cor-
relation is positive; (3) one-mask entropies exhibit
stronger (in absolute value) correlations with dis-
crepancy; and (4) correlations between entropies
and discrepancy are stronger (in absolute terms) for
MLMs compared to autoregressive models.
These results suggest that selecting the direction
with higher entropy for one-mask prediction and
lower entropy for two masks is likely to increase
the likelihood of true tokens. In Appendix F we
provide examples of the effect of decoding order.
10 Discussion
In this work, we investigated the probabilistic con-
sistency of language models (LMs) and introduced
a novel framework to quantify and explain discrep-
ancies between equivalent estimation orders.
Our findings indicate statistically significant in-
consistencies among 16 out of 18 models on the
News dataset and all 18 models on the Wikitext
dataset. These results highlight significant differ-
ences between MLMs and autoregressive models,
with the latter showing considerably larger incon-
sistencies and discrepancy variances.
The comparable discrepancy distributions be-
tween the News and Wikitext datasets suggest that
exposure in training has little effect on consis-
tency. In contrast, results from the synthetic dataset
(see Appendix D.2) reveal a different pattern, withMLM Flan-T5 Llama2 Llama2-chat102103104Rank 1
MLM Flan-T5 Llama2 Llama2-chat101102103104Rank 2Figure 3: Prediction Ranking for the examined Models. Rank 1 represents the ranks for the first prediction (two
masks) and Rank 2 for the second (one mask). Results were obtained from a sample size of 200 on the News dataset.
Ranks are in log scale. Lower ranks indicate more accurate predictions. Boxes show quartile values with median
lines; whiskers extend to 1.5 IQR from quartiles.
0.3Correlation
Generative MLM
0.3
0.2
0.1
0.0
0.1
0.2
(a)
0.3
0.2
0.1
0.0
0.1
0.2
0.3
(b)
Figure 4: Entropy and Decoding Order: Correlations
between four prediction entropies and the discrepancy
di,i+1are presented. (a) presents the two entropies
associated with predicting the i-th token first. (b) illus-
trates the two entropies corresponding to predicting the
(i+ 1) -th token first. For each entropy the distribution
of correlations for autoregressive models is depicted in
purple on the left, and for MLM models in green on the
right. Dashed lines within each violin represent the first,
second (i.e., median), and third quartiles, respectively.
higher mean and variance in discrepancies, indicat-
ing reduced consistency on low likelihood data.
Analysis of the relationship between discrepancy
variances and model size reveals significant trends
only in Flan-T5 models, displaying a negative cor-
relation. Across all model types, we found that
larger models exhibit lower average prediction en-
tropies. This suggests that artificially high consis-
tency in autoregressive models may arise from high
variance. However, this does not hold for MLMs,
which show the opposite trend.
A positive correlation between real data likeli-
hood and overall entropy is termed overconfidence ,
while a negative correlation is called underconfi-
dence (Ravfogel et al., 2023). Our experiments sug-
gest that choosing a decoding direction with higherentropy for single-mask predictions and lower en-
tropy for two masks is expected to yield higher
estimated probabilities for the correct word pair.
This suggests that overconfidence and undercon-
fidence are influenced by the model type and the
number of masked tokens, challenging the antici-
pated strong link between average likelihood and
entropy. Consequently, this calls for cautious appli-
cation of methods to address overconfidence and
underconfidence, as models can exhibit both.
In conclusion, our investigation into the prob-
abilistic consistency of language models has re-
vealed significant inconsistencies across various
model types and datasets. Our findings highlight
the need for careful interpretation and application
of the predicted scores.
Prior research (Yang et al., 2018) argued that
modeling the “distribution of language” is a com-
plex task. Our findings provide further support
for this claim, showing that even high-performing
models struggle to generate consistent estimations.
Robust ranking of structured predictions and
similar applications often require consistency in
joint probability estimation. However, our research
shows that achieving high performance does not
necessarily depend on consistency, and vice versa:
a model can be perfectly consistent but make in-
accurate predictions. Therefore, a combination of
consistency testing with other performance metrics
is essential for thorough evaluation.
Our analysis exposes inconsistencies and their
links to prediction entropies and model-type dispar-
ities, offering valuable insights for future research
to tackle these limitations effectively.Limitations
Our framework is designed for comparing any two
estimations of a joint distribution. However, our
experiments specifically target a setting with two
adjacent tokens. While effective in revealing incon-
sistencies, exploring additional estimation orders
with longer word spans in future research could
uncover additional ones.
A limitation of the analysis of the dependency
of discrepancy variances on model sizes lies in
its treatment of each model as an observation, re-
sulting in small sample sizes (7 MLMs, and 11
autoregressive models).
We also note that comparison between different
model types is qualitative only, as they differ in
their tokenization and other qualities (such as the
scale of probabilities and entropies). In addition,
due to the load of computation, in some cases, the
sample sizes were relatively small.
Ethics Statement
Wikitext-2 is released under license CC BY-SA
4.0. The dataset of news articles was gathered
from various new sites through Google News. The
dataset was used for validation purposes only and
will not be redistributed or used for training models.
A list of URLs and access dates will be provided
upon publication.
Acknowledgements
The authors thank Gabriel Stanovsky, Amir Feder,
and Noam Weis for their valuable insights. This
research was supported by grants from the Israeli
Ministry of Science and Technology, the Israeli
Council for Higher Education, and Israel Science
Foundation (grant no. 2424/21).
References
Mohammad Bavarian, Heewoo Jun, Nikolas Tezak,
John Schulman, Christine McLeavey, Jerry Tworek,
and Mark Chen. 2022. Efficient training of lan-
guage models to fill in the middle. Preprint ,
arXiv:2207.14255.
Yoav Benjamini and Daniel Yekutieli. 2001. The con-
trol of the false discovery rate in multiple testing
under dependency. Annals of statistics , pages 1165–
1188.
Mark Braverman, Xinyi Chen, Sham M. Kakade,
Karthik Narasimhan, Cyril Zhang, and Yi Zhang.
2019. Calibration, entropy rates, and memory in
language models. Preprint , arXiv:1906.05664.Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers.Preprint , arXiv:2005.14165.
Haw-Shiuan Chang and Andrew McCallum. 2022. Soft-
max bottleneck makes language models unable to
represent multi-mode word distributions. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 8048–8073, Dublin, Ireland. Association
for Computational Linguistics.
Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu,
and Heng Ji. 2023. A close look into the cali-
bration of pre-trained language models. Preprint ,
arXiv:2211.00151.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models. Preprint , arXiv:2210.11416.
Kevin Clark, Minh-Thang Luong, Quoc V Le, and
Christopher D Manning. 2020. Electra: Pre-training
text encoders as discriminators rather than generators.
arXiv preprint arXiv:2003.10555 .
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsuper-
vised cross-lingual representation learning at scale.
Preprint , arXiv:1911.02116.
Soham Dan and Dan Roth. 2021. On the effects of
transformer size on in- and out-of-domain calibra-
tion. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2021 , pages 2096–2101,
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Shrey Desai and Greg Durrett. 2020. Calibration of
pre-trained transformers. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 295–302, Online.
Association for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training ofdeep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara
Meister, Jason Eisner, and Ryan Cotterell. 2023. A
measure-theoretic characterization of tight language
models. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 9744–9770, Toronto,
Canada. Association for Computational Linguistics.
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-
lasha Ravichander, Eduard Hovy, Hinrich Schütze,
and Yoav Goldberg. 2021. Measuring and improving
consistency in pretrained language models. Transac-
tions of the Association for Computational Linguis-
tics, 9:1012–1031.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-
berger. 2017. On calibration of modern neural net-
works. In International conference on machine learn-
ing, pages 1321–1330. PMLR.
Michael Hahn. 2020. Theoretical limitations of self-
attention in neural sequence models. Transactions of
the Association for Computational Linguistics , 8:156–
171.
Guande He, Jianfei Chen, and Jun Zhu. 2023. Pre-
serving pre-trained features helps calibrate fine-tuned
language models. In The Eleventh International Con-
ference on Learning Representations .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text de-
generation. In International Conference on Learning
Representations .
Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila
Ohno-Machado. 2012. Calibrating predictive model
estimates to support personalized medicine. Journal
of the American Medical Informatics Association ,
19(2):263–274.
Adam Tauman Kalai and Santosh S. Vempala.
2023. Calibrated language models must hallucinate.
Preprint , arXiv:2311.14648.
Jason Lee, Dustin Tran, Orhan Firat, and Kyunghyun
Cho. 2020. On the discrepancy between density es-
timation and sequence generation. In Proceedings
of the Fourth Workshop on Structured Prediction for
NLP, pages 84–94, Online. Association for Computa-
tional Linguistics.
Tao Li, Vivek Gupta, Maitrey Mehta, and Vivek Sriku-
mar. 2019. A logic-driven framework for consistency
of neural models. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) ,pages 3924–3935, Hong Kong, China. Association
for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. Preprint , arXiv:1907.11692.
Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.
SelfCheckGPT: Zero-resource black-box hallucina-
tion detection for generative large language models.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
9004–9017, Singapore. Association for Computa-
tional Linguistics.
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan
Cotterell. 2023. Locally typical sampling. Transac-
tions of the Association for Computational Linguis-
tics, 11:102–121.
Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-
tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer.
2023. Nonparametric masked language modeling.
InFindings of the Association for Computational
Linguistics: ACL 2023 , pages 2097–2118, Toronto,
Canada. Association for Computational Linguistics.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. Preprint , arXiv:2203.02155.
Pouya Pezeshkpour and Estevam Hruschka. 2023.
Large language models sensitivity to the order of
options in multiple-choice questions. Preprint ,
arXiv:2308.11483.
Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korho-
nen, Edoardo M. Ponti, and Shay B. Cohen. 2023.
Are large language models temporally grounded?
Preprint , arXiv:2311.08398.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Shauli Ravfogel, Yoav Goldberg, and Jacob Goldberger.
2023. Conformal nucleus sampling. In Findings of
the Association for Computational Linguistics: ACL
2023 , pages 27–34, Toronto, Canada. Association for
Computational Linguistics.
Allen Z. Ren, Anushri Dixit, Alexandra Bodrova,
Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu,
Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu,
Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar.2023. Robots that ask for help: Uncertainty align-
ment for large language model planners. Preprint ,
arXiv:2307.01928.
Marco Tulio Ribeiro, Carlos Guestrin, and Sameer
Singh. 2019. Are red roses red? evaluating consis-
tency of question-answering models. In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 6174–6184, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Maohao Shen, Subhro Das, Kristjan Greenewald,
Prasanna Sattigeri, Gregory Wornell, and Soumya
Ghosh. 2024. Thermometer: Towards universal
calibration for large language models. Preprint ,
arXiv:2403.08819.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. Preprint , arXiv:2307.09288.
Alex Wang and Kyunghyun Cho. 2019. BERT has a
mouth, and it must speak: BERT as a Markov ran-
dom field language model. In Proceedings of the
Workshop on Methods for Optimizing and Evaluat-
ing Neural Language Generation , pages 30–36, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Deng-Bao Wang, Lei Feng, and Min-Ling Zhang. 2021.
Rethinking calibration of deep neural networks: Do
not be afraid of overconfidence. Advances in Neural
Information Processing Systems , 34:11809–11820.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022. Finetuned
language models are zero-shot learners. Preprint ,
arXiv:2109.01652.
F Wilcoxon. 1945. Individual comparisons by ranking
methods. biometrics. bull., 1, 80–83.Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William W. Cohen. 2018. Breaking the softmax bot-
tleneck: A high-rank RNN language model. In Inter-
national Conference on Learning Representations .
Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and
Arvind Neelakantan. 2021. Trading off diversity and
quality in natural language generation. In Proceed-
ings of the Workshop on Human Evaluation of NLP
Systems (HumEval) , pages 25–33, Online. Associa-
tion for Computational Linguistics.
Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi
Narayan, Mohammad Saleh, and Peter J. Liu. 2022.
Calibrating sequence likelihood improves conditional
language generation. Preprint , arXiv:2210.00045.
Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong
Zhang, and Zhendong Mao. 2023. On the calibration
of large language models and alignment. Preprint ,
arXiv:2311.13240.
A Examples
A.1 Inconsistent Joint distribution from
Conditionals
Consider a (toy) example, where we have a two-
word sentence x1, x2, over a yes-no alphabet
{y, n}, and assume we have estimations P(x1=
y|x2=y) = 0 .9, P(x2=y) = 0 .9andP(x2=
y|x1=y) = 0 .1, P(x1=y) = 0 .1. This leads to
a contradiction, since
0.81 = P(w1=y|w2=y)·P(w2=y)
=P(w1=y, w 2=y)
=P(w2=y|w1=y)·P(w1=y) = 0.01.
A.2 Consistent Joint distribution Without
Structural Restrictions
As an example, consider a masked language model
that is trained by maximum likelihood estimation
with(2n+ 1) -grams. Since all conditionals are
determined by the counts of the (2n+1)-gram sam-
ples, any set of resulting conditionals must comply
with the (2n+ 1) -gram joint distribution.
B Prompts For Instruction Models
The prompt we used is:
You will be given a passage with one
masked token that you should fill in. We
denote this token by %. The passage
might also contain corrupted tokens de-
noted by @. You are not expected to fill
in corrupted tokens - fill only the maskedone. Your answer should include the
filled-in token only with no extra expla-
nations or context.
For Flan-t5 and LLAMA 2(non-chat version),
the input format was:
<prompt>
Passage: <passage with masks>
Answer:
For L LAMA 2-C HAT the format was:
[INST] «SYS»
<prompt>
«/SYS»
Passage: <passage with masks>
[/INST]
CExplaining Inconsistencies With Model
Types
In Table 2 we present the results of linear regres-
sion analysis with fine-grained model types. In
this setting, we regard each model type (RoBERTa,
ELECTRA, etc.) as a separate case:
ˆνk=ˆβ0+ˆβ1Sk+ (9)
+ˆβ2 1Tk=1+···+ˆβt 1Tk=t−1
+ˆβt+1Sk· 1Tk=1+···+ˆβ2tSk· 1Tk=t−1
where Skis the size (in billions of parameters)
of model Mkof type Tk, and 1is the indicator
function.
As before, whether the model adequately cap-
tures the variability of the explained variable (in
our case, the variability of νk), is measured by
0≤R2≤1, and large statistically significant
coefficients indicate a substantial effect of the cor-
responding explanatory variables.
To avoid multicollinearity, the estimator of the
variance includes t−1model types. For a model
of type 1, the estimated variance is
ˆνk= (ˆβ0+ˆβ2) + (ˆβ1+ˆβt+1)Sk,
whereas for a model of type t, it is ˆνk=ˆβ0+
ˆβ1Sk. Consequently, the effect of the number of
parameters in the t-th type is incorporated in ˆβ1
and serves as the baseline effect, against which all
other t−1effects are estimated.
In this case, only Flan-T5 showed a statistically
significant dependency between the Size and modelWikitext News
Coeff P-value Coeff P-value
Intercept 22.59 0.728 17.93 0.712
Size 0.016 0.169 0.0129 0.147
Data size -0.0023 0.727 -0.0018 0.713
T: ELECTRA-g -18.8194 0.77 -13.8604 0.774
T: RoBERTa -19.4433 0.76 -15.2669 0.749
T: XLM-R -18.3511 0.77 -13.6462 0.772
T: Llama 2-C 1.2206 0.093 0.5091 0.3
T: Flan-T5 -20.6326 0.731 -15.8104 0.725
I: S – ELECTRA-g -2.3208 0.49 -1.8548 0.463
I: S – RoBERTa -1.1458 0.723 -1.0219 0.673
I: S – XLM-R -1.7275 0.596 -2.5442 0.316
I: S – Llama 2-C 0.0017 0.908 -0.0013 0.908
I: S – Flan-T5 0.1473 0.043 0.1784 0.007
Table 2: Analysis of Discrepancy Variance with Model
Types. Regression coefficients and corresponding p-
values for each model are presented, with MLM models
serving as the baseline. P-values below α= 0.05are
shown in bold.
type. We note that this was the type with the largest
number of models ( = 5), with all others having
very few samples ( ≤3), and that this analysis suf-
fers from the limitation of small sample sizes for
each fine-grained model type.
D Experiments with Synthetic Data
D.1 Data Generation
In addition to real datasets, we conducted tests on
an automatically generated dataset, which allowed
us the ability to control the context and manipulate
the occurrence of lower probability tokens.
We used the template:
[MASK1] [MASK2] is a thing
and filled in the masks with predetermined spans.
Noun phrases were extracted from fiction data9
using SpaCy10, and filtered to those that consist
of 2 words. Additionally, we used on phrases in
which each word is a single token in all the models
we tested with. The final dataset consists of 10K
samples. Sentences in this dataset are not meaning-
ful but are mostly grammatical and resemble real
sentences.11
While we lack expectations about the specific
probability values in synthetic data, which can be
arbitrarily low, the model is expected to produce
consistent probabilities.
9https://huggingface.co/datasets/
AlekseyKorshuk/fiction-books
10https://spacy.io/
11We experimented with additional templates and filling
methods; all resulted in qualitatively similar outcomes.ELECTRA smallELECTRA baseELECTRA large RoBERT a baseRoBERT a large
XLM-RoBERT a baseXLM-RoBERT a largeLlama 2 7bLlama 2 13bLlama 2 70b
Llama 2 chat7b Llama 2 chat 13b Llama 2 chat 70bFlan-T5 smallFlan-T5 baseFlan-T5 largeFlan-T5 xlFlan-T5 xxl8
6
4
2
02468dFigure 5: Discrepancy Results on the Synthetic datasets.
D.2 Results and Analysis
D.2.1 Consistency
Here we regard the discrepancy values d(j)for
each examined model on the Synthetic dataset (5).
We can see that, compared to the results for the
real datasets (in 2a, 2b), the MLM medians are
notably further away from 0. The autoregressive
models show similar trends for the real and syn-
thetic datasets.
Discrepancy mean. For the Wilcoxon test on the
synthetic dataset, all models, except flan-t5-base
(adjusted p-value = 0.22), attain a corrected p-value
smaller than 0.05.
Discrepancy variance. In the Synthetic dataset
(5), variance is higher compared to real datasets,
and no clear trend related to the model size is ob-
served.
E Additional Results on Task
Comprehension in Autoregressive
Models
In autoregressive models, even when instructed
to predict a single word (assuming single-token
words), the prediction necessarily consists of at
least two tokens: the predicted word and the EOS
token. . The EOS score provides valuable informa-
tion, as a probability less than 1 indicates that some
probability mass was allocated to spans longer than
one token. Therefore, it can be used to assess how
well the instruction was “understood” by the model,
thereby aiding in distinguishing cases where poor
predictions arise from a failure to comprehend the
task – a challenge absent in classic MLMs where
the infilling task is inherent in the architecture.
Previous work examines model consistencies
through paraphrasing and logical dependencies (see
Flan-T5 small Flan-T5 base Flan-T5 largeFlan-T5 xl Flan-T5 xxl Llama 2 7b Llama 2 13b Llama 2 70bLlama 2 chat7b Llama 2 chat 13b Llama 2 chat 70b0.00.10.20.30.40.50.60.70.80.91.0P(EOS)
Flan Llama 2 Llama 2 chatFigure 6: End-of-sentence (EOS) Probabilities for
Single-Masked Prediction on the News dataset. Boxes
represent the interquartile range (IQR), with whiskers
extending up to 1.5 times the IQR. The solid-red, dotted-
blue, and dashed-violet lines indicate the medians for
Flan-T5, LLAMA 2, and LLAMA 2chat models, respec-
tively. Outliers are omitted for clarity.
§3). However, this type of consistency test faces
two primary limitations. First, a model might fail
to capture dependencies and similarities between
inputs, leading to misinterpretation of imperfect
inference capabilities as inconsistencies. Second,
inconsistencies in predictions may arise from train-
ing data corruption, such as the presence of contra-
dictory facts, instead of model inconsistency.
In our tests with MLMs, these issues do not
apply as the inputs are identical. However, au-
toregressive models raise concerns regarding the
model’s ability to fully “understand” the instruc-
tions. Figure 6 displays distributions of the EOS
probabilities (for the second token) in the News
dataset (gathered past the release date).
Figure 7 presents an analysis of the assigned
ranks for both tokens in autoregressive mod-
els, while accounting for model size and EOS
scores. The results show alignment between better-
performing models and task comprehension, as ev-
idenced by lower ranks for EOS ranks and the sec-
ond token prediction. An exception is the LLAMA
2-C HAT models, which show poor rankings for
the real word that cannot be attributed to a lack
of task understanding, as several of these models
assign low ranks for EOS. Notably, larger models
within each model type tend to better task compre-
hension, in terms of both EOS scores and real word
rankings.100101102103104
Log Rank 1100101102103104Log Rank 2Flan-T5Flan-T5
Flan-T5
Flan-T5
Flan-T5Llama2Llama2-chat
Llama2Llama2-chat
Llama2Llama2-chat
0.00.51.01.52.02.53.03.5
Log EOS RankFigure 7: Prediction Ranking Summary. The x-axis
shows log-scale ranks for single-mask predictions and
y-axis displays ranks for double-mask predictions. Each
node corresponds to an examined model. Colors indi-
cate EOS prediction ranks (lower ranks indicate better
task comprehension), and node size reflects the model’s
parameter. Results were obtained on a sample of size
200 from the News dataset.
F Examples for The Effect of Decoding
Order
To demonstrate the impact of the suggested de-
coding order on completion quality, we provide
examples from news datasets: a case where the
decoding order notably affects the outcome, and a
case where it does not. Completions are generated
using RoBERTa base.
Large Effect. To identify cases with large ef-
fect, we examined the values of ∆H=Hi+1|i−
Hi|i+1+Hi+1−Hi: Large ∆Hvalues suggest that
decoding in the suggested order will yield higher
probabilities for true tokens, as they correspond to
low entropy for double-mask predictions and high
entropy for single-mask predictions.
For the following example ∆H= 12 was ob-
served:
with former master’s students and co-first
<mask> <mask> Pai
The true masked words are “authors James”.
The top 10 completions ranked by joint proba-
bility, when decoding the first mask first are:
1.with former master’s students and co-first year student
Pai
2.with former master’s students and co-first year students
Pai
3.with former master’s students and co-first president
Ken Pai4.with former master’s students and co-first president
Michael Pai
5.with former master’s students and co-first chair Ken
Pai
6.with former master’s students and co-first lady Ken Pai
7.with former master’s students and co-first president
Patrick Pai
8.with former master’s students and co-first president
Fred Pai
9.with former master’s students and co-first secretary
Ken Pai
10.with former master’s students and co-first president
JeffPai
and the top 10 completions when decoding the
second mask first are:
1.with former master’s students and co-first president
Ken Pai
2. with former master’s students and co-first s toPai
3. with former master’s students and co-first author , Pai
4.with former master’s students and co-first year Justin
Pai
5.with former master’s students and co-first s Chairman
Pai
6. with former master’s students and co-first ed it Pai
7.with former master’s students and co-first president
chairman Pai
8.with former master’s students and co-first responders
Chairman Pai
9.with former master’s students and co-first year Michael
Pai
10. with former master’s students and co-first did it Pai
Indeed, in this example, decoding the first mask
first yields higher-quality predictions. Interestingly,
high entropy for the second token often occurs,
even when the first token is given, in two-token
names. This is expected due to the low predictabil-
ity of names.
Small Effect. Additionally, for comparison, we
provide an example of a case where the difference
in entropies is small.
For the following example ∆H= 1·10−4was
observed:
A group of <mask> <mask> they were
well inside the designated safe zone
The original words are “friends said”.
The corresponding top 10 completions ranked
by joint probability, when decoding the first mask
first are:
1.A group of soldiers believed they were well inside the
designated safe zone2.A group of soldiers signaled they were well inside the
designated safe zone
3.A group of men believed they were well inside the
designated safe zone
4.A group of soldiers thought they were well inside the
designated safe zone
5.A group of people believed they were well inside the
designated safe zone
6.A group of civilians believed they were well inside the
designated safe zone
7.A group of soldiers realized they were well inside the
designated safe zone
8.A group of soldiers ensured they were well inside the
designated safe zone
9.A group of soldiers felt they were well inside the desig-
nated safe zone
10.A group of men thought they were well inside the
designated safe zone
and the top 10 completions when decoding the
second mask first are:
1.A group of police believed they were well inside the
designated safe zone
2.A group of police thought they were well inside the
designated safe zone
3.A group of protesters believed they were well inside
the designated safe zone
4.A group of people thought they were well inside the
designated safe zone
5.A group of people believed they were well inside the
designated safe zone
6.A group of soldiers thought they were well inside the
designated safe zone
7.A group of soldiers believed they were well inside the
designated safe zone
8.A group of protesters thought they were well inside
the designated safe zone
9.A group of civilians believed they were well inside the
designated safe zone
10.A group of armed believed they were well inside the
designated safe zone
In this case, all generations (except for the last
one in the second case) yield reasonable comple-
tions and therefore no notable differences between
the two completion orders were observed.