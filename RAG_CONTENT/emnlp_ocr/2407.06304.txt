VIMI: Grounding Vi deo Generation through M ulti-modal I nstruction
Yuwei Fang1Willi Menapace1Aliaksandr Siarohin1Tsai-Shien Chen2
Kuan-Chien Wang1Ivan Skorokhodov1Graham Neubig3Sergey Tulyakov1
Snap Inc.1UC Merced2Carnegie Mellon University3
snap-research.github.io/VIMI
Abstract
Existing text-to-video diffusion models rely
solely on text-only encoders for their pretrain-
ing. This limitation stems from the absence of
large-scale multimodal prompt video datasets,
resulting in a lack of visual grounding and re-
stricting their versatility and application in mul-
timodal integration. To address this, we con-
struct a large-scale multimodal prompt dataset
by employing retrieval methods to pair in-
context examples with the given text prompts
and then utilize a two-stage training strategy
to enable diverse video generation tasks within
the same model. In the first stage, we pro-
pose a multimodal conditional video generation
framework for pretraining on these augmented
datasets, establishing a foundational model for
grounded video generation. Secondly, we fine-
tune the model from the first stage on three
video generation tasks, incorporating multi-
modal instructions. This process further refines
the model’s ability to handle diverse inputs and
tasks, ensuring seamless integration of multi-
modal information. After this two-stage train-
ing process, VIMIdemonstrates multimodal
understanding capabilities, producing contex-
tually rich and personalized videos grounded
in the provided inputs, as shown in Figure 1.
Compared to previous visual grounded video
generation methods, VIMIcan synthesize con-
sistent and temporally coherent videos with
large motion while retaining the semantic con-
trol. Lastly, VIMIalso achieves state-of-the-
art text-to-video generation results on UCF101
benchmark.
1 Introduction
Recent advancements in video diffusion models
have led to significant successes across various
video creation tasks (Singer et al., 2022; Villegas
et al., 2022; Zhang et al., 2023; Chai et al., 2023;
Chen et al., 2023; Ceylan et al., 2023; Geyer et al.,
2023). These models have demonstrated impres-
sive capabilities in generating high-quality videosfrom textual prompts (An et al., 2023; Blattmann
et al., 2023b; Ge et al., 2023; Guo et al., 2023b; He
et al., 2023; Ho et al., 2022a,b; Singer et al., 2022;
Wang et al., 2023; Zhou et al., 2023; Blattmann
et al., 2023a). However, the majority of these
models rely solely on text-only encoders for their
diffusion-based pretraining. This limitation stems
from the absence of large-scale multimodal prompt
datasets, which results in a lack of visual grounding
during the pretraining stage. Consequently, current
models struggle to incorporate visual input effec-
tively, restricting their versatility and application in
scenarios that demand multi-modal integration.
To effectively incorporate visual input into pre-
trained text-to-video models, standalone image
encoders are often employed to process image
prompts (Jiang et al., 2023b; Guo et al., 2023a; Ren
et al., 2024; He et al., 2024). The visual embed-
dings generated by these encoders are then injected
into the diffusion model, enabling it to handle mul-
timodal applications. However, this approach ne-
cessitates customized model designs, leading to
fragmented solutions that cannot support various
tasks in a unified manner. As a result, the mod-
els lack the flexibility and generalization needed to
seamlessly integrate different modalities for diverse
video generation tasks.
Recently, generative pretrained multimodal lan-
guage models have demonstrated robust multi-
modal in-context learning capabilities, showcasing
their ability to process and integrate various types
of input data effectively (Team et al., 2023; Zhu
et al., 2023; Achiam et al., 2023; Liu et al., 2024).
Inspired by this success, we introduce a multi-
modal instruction pretraining framework VIMIfor
grounded video generation. This novel framework
aims to leverage the strengths of multimodal mod-
els, enhancing the ability to generate videos that are
coherently grounded in both textual and visual in-
puts. Specifically, the training of VIMIconsists of
two stages: (1) Retrieved Augmented Pretraining;
1arXiv:2407.06304v1  [cs.CV]  8 Jul 2024Input Images
A <image_1> is sitting on the beach. 
A <image_1> is sitting on the <image_3>. Generated Videos
A <image_2> is sitting on the beach. 
A <image_2>is sitting on the <image_3>. <image_1><image_2><image_3>
Figure 1: Examples of VIMIfor grounded video generation . Thanks to our visual grounding during retrieval-
augmented pretraining and multimodal instruction tuning, our generator can generate videos from multimodal
prompts that include multiple image entities. Each multimodal prompt is displayed below the generated videos,
illustrating the model’s capability to integrate and interpret both textual and visual inputs effectively.
and (2) Multi-Modal Instruction Fine-Tuning.
During the pretraining stage, we first construct
a large-scale multimodal prompt dataset by em-
ploying a large-scale retrieval method to pair mul-
timodal in-context examples with the given text
prompts. The retrieved contexts from a web-scale
corpus provide a rich multimodal in-context en-
vironment for model training. With these paired
datasets, we can either pretrain a multimodal video
generator from scratch or fine-tune an existing text-
to-video generator. After this stage, the model
gains the capability to understand both text-only
and multimodal inputs for video generation. This
establishes a foundation model for grounded video
generation, capable of integrating diverse modali-
ties into cohesive video outputs.
In the second stage, we fine-tune the model from
the first stage on various video generation tasks,
incorporating multimodal instructions. This fine-
tuning process further refines the model’s ability
to handle diverse inputs and tasks, ensuring it can
seamlessly integrate multimodal information. Af-
ter this two-stage training process, VIMIdemon-
strates enhanced multimodal understanding capa-
bilities, producing contextually rich and personal-
ized videos grounded in the provided inputs. This
makes the model highly versatile and effective for
a wide range of video generation applications.
In summary, our main contributions include:
•Novel Dataset Construction : We are the first
to use retrieval methods to build large-scale
multimodal dataasets for video pretraining.
•Retrieval Augmented Video Pretraining :
We propose a novel retrieval-augmented pre-
training framework specifically designed for
grounded video generation. Our pretrainingframework enables video generators to re-
ceive multi-modal prompts, instead of text-
only prompts.
•Instruction Tuning for Video Generation :
We introduce instruction tuning for video gen-
eration, unifying three distinct video genera-
tion tasks within a single, cohesive instruction
framework. This innovative approach allows
the model to flexibly handle various video gen-
eration tasks based on specific instructions.
2 Preliminary
2.1 Text-To-Video Pretraining
We base our work on the diffusion framework pro-
posed by Menapace et al. (2024), which adapts
the EDM (Karras et al. (2022a)) diffusion frame-
work to high resolution video generation. In EDM,
the forward diffusion process is characterized
by a variance-exploding mechanism p(xσ|x)∼
N(x,σ2I), where noise σis gradually added to
the data, causing the variance to increase over time,
andxσrepresents the data at the current noise
level. The reverse process is modeled by learnable
denoiser function denoted as Dθ, which is trained
using a denoising objective formulated as:
L(Dθ) =Eσ,x,ϵh
λ(σ)Dθ(xσ)−x2
2i
, (1)
where xis a data sample, λis a weighting function
for the loss and ϵis gaussian noise. Rather than
learning Dθ(xσ)directly, it is parametrized as:
Dθ(xσ) =cout(σ)Fθ(cin(σ)xσ) +cskip(σ)xσ,
(2)
where Fθis a neural network. By appropriately
choosing scaling functions cout,cskipandcin(see
2A Coton de Tulear is playing in a yard.
A Day in the Life of a Coton De TulearCoton de Tulear running and playing in a meadow.
A white fluffy Coton De Tulear playing in the grass.…
A Coton de Tulear is playing in a yard.
A white fluffy Coton De ..Multimodal Large Language Models
Video Generator
Multimodal Memory
(a) Retrieve-Augmented Pretraining for Videos
Multimodal Large Language ModelsVideo Generator
Generate a video with the retrieved text-image…A white ..
A Coton de Tulear ..
A Coton de Tulear is playing in a yard.Ais playing in                        .Generate a video with the text and image …Generate a video with the text and first frame.
Text-to-Video GenerationSubject-driven Video GenerationVideo Prediction
Multimodal Instruction Tasks (b) Multimodal Instruction Tuning for Videos
Figure 2: Overview of our VIMIframework. (a-left) We first construct a large-scale dataset by employing retrieval
methods to pair multimodal in-context with given text prompts. Then we present a multimodal conditional video
generation framework for pretraining on these augmented datasets. (b) We propose multimodal instruction tuning
for video generation, grounding the model on customized input specified in different multimodal instructions for
video generation, including subject-driven video generation, video prediction and text-to-video. By fine-tuning the
model with multimodal instructions, we enable VIMIto generate videos that are both contextually rich and visually
accurate across a wider range of tasks.
Menapace et al. (2024)), the model can train opti-
mally on high resolution videos. We employ a sec-
ond order Runge-Kutta sampler to produce video
samples.
2.2 Multimodal Large Language Models
Building upon the success of Large Language Mod-
els (LLMs), Multimodal Large Language Mod-
els (MLLMs) (Liu et al., 2024; Zhu et al., 2023;
Team et al., 2023) integrate visual information
from a pretrained vision encoder (Radford et al.,
2021) with an advanced LLM (Touvron et al., 2023;
Jiang et al., 2023a). This integration is achieved
by treating visual modalities as sequences of dis-
crete tokens. In our work, we utilize MLLMs to
process and interpret multimodal in-context input
datas= (s1, s2, ..., s n), where sican be a sig-
nal unit, such as an image. For the image unit si
in the prompt, a pre-trained CLIP visual encoder
ViT-L/14, is used to provide the visual features
vi=Visual-Encoder (si). The patch features vi
before the last Transformer layer, combined with
the text tokens, are used for MLLM encoding, for-
mulated as:
C=MLLM ({s1, s2, ..., s n}|W (vi)), (3)
whereWprojects vito connect image features into
the word embeddings. This approach allows the
MLLM to effectively interpret a combination of
textual and visual inputs, leveraging the strengths
of both modalities to enhance the model’s multi-
modal understanding and generation capabilities.3 Method
We aim to generalize the video generation pretrain-
ing to the multimodal setting. Figure 2 shows the
overview of our framework. Sec. 3.1 introduces
how we construct a large-scale multimodal input-
video dataset by employing retrieval methods to
pair in-context examples with given text prompts.
Sec. 3.2 presents a multimodal conditional video
generation framework for pretraining on these aug-
mented datasets, establishing a foundational model
for grounded video generation. Sec. 3.3 introduces
the instruction finetuning stage on three video gen-
eration tasks, incorporating multimodal instruc-
tions.
3.1 Retrieval-Augmented Multi-modal
Datasets
Retrieval-based methods collect relevant informa-
tion to the input from an external multimodal mem-
oryM. In our study, we use web-scale image-
text pairs as our multi-modal memory for retrieval
and build index into a list of key-value pairs, i.e.
M={(ki, vi)}. Then, given the input sequence s,
the retrieval engine Ematches it with all keys and
returns the top Kmost similar keys to the query
together with their values:
{(ki1, vi1), ..., (kiK, viK)}=E(s|M)(4)
In this work, we build the retrieval engine based
on the widely used BM25 score (Schütze et al.,
2008). We choose BM25 over dense representa-
tions due to the large scale of the retrieval datastore
and its faster speed. In our work, we construct
3OursInput Images
VideoBooth
ID-Animator
A <image> is sitting in front of a computer and talking to the camera. 
A <image> sitting in Times Square.OursFigure 3: Comparison of subject-driven video generation . We compared with concurrent work ID-Animator (He
et al., 2024) for zero-shot human video generation (above) and VideoBooth (Jiang et al., 2023b) for general subject-
driven video generation (below). Our video generator can synthesize temporally coherent videos with large motion
while retaining the semantic control.
500M image-text pairs as our multimodal mem-
ory. Using this retrieval approach, we augment our
internal text-to-video and text-to-image datasets.
Specifically, we use the text caption as the query
and retrieve the top-3 image-text pairs from the
memory Mfor model training. These retrieved
multimodal documents are then combined with the
text input to form the new multimodal input, which
serves as the condition for video pretraining, ensur-
ing that the model receives contextually relevant
and diverse multimodal information.
3.2 Retrieval-Augmented Video Pretraining
Given the retrieval-augmented multimodal input,
we first concatenate the text caption swith the re-
trieved multiodal documents to form the new mul-
timodal input. Then, we feed this combined in-
put into the Multimodal Large Language Models
(MLLMs) to generate the multimodal conditional
embedding C:
C=MLLM (F({(ki1, vi1), ..., (kiK, viK)}, s)
(5)
Here,Fdenotes concatenation and the embedding
Cencapsulates the rich contextual informationfrom both the text and the retrieved multimodal
data.
Following (Menapace et al., 2024), we use
FITs (Chen and Li, 2023) as the backbone to jointly
model the spatial and temporal dimensions for high-
quality video generation. However, here we only
use the multimodal conditioning embedding Cto
control the generation process rather than the text
embeddings from T5 text encoder. We concate-
nate additional tokens representing the diffusion
timestep, framerate and original resolution of the
current input, to support variable video framerates
and large differences in resolution and aspect ratios
in the training data. To generate high-resolution
outputs, we pretrain a cascade model consisting
of a first-stage model producing 36×64px videos
and a second-stage upsampling model producing
288×512px videos.
3.3 Multimodal Instruction Tuning
After the first stage of retrieval-augmented pretrain-
ing, VIMIcan generate videos from prompts in-
volving both text and images, leveraging the multi-
modal understanding capabilities of the multimodal
language model. However, this initial stage primar-
4“A husky dog sniffing a coke bottle on the floor.”LLMEntity words:“husky dog” coke bottle”
Grounding-DINO + SAMFigure 4: An overview of our data curation pipeline for
subject-driven video generation.
ily focuses on grounding the model in the noisy
retrieved in-context input for video generation. As
a result, VIMImay not fully utilize visual features
for precise and faithful video generation.
To address these limitations, we propose mul-
timodal instruction tuning for video generation,
grounding the model on customized input specified
in different multimodal contexts for video gener-
ation. By fine-tuning the model with multimodal
instructions, we enhance its ability to integrate and
utilize visual features more effectively, enabling
VIMIto generate videos that are both contextually
rich and visually accurate across three video tasks,
illustrated in Figure 2b.
Subject-Driven Video Generation To enhance
the visual grounding capabilities for video gener-
ation, we curate a multimodal interleaved prompt
composed of texts and images based on the Panda-
70M dataset (Chen et al., 2024). The data cura-
tion pipeline is illustrated in Figure 4. First, we
extract entity words from the text captions using
Large Language Models (LLMs). For each entity,
we extract the corresponding image segment using
Grounding DINO (Liu et al., 2023) for object de-
tection and SAM (Kirillov et al., 2023) for image
segmentation. This process ensures that each tex-
tual element has a visually grounded counterpart.
We prepend the task instruction "Generate a video
with the text and image interleaved prompt." to the
prompt. This curated data ensures that the model
can ground specific multimodal inputs effectively
and generate videos that faithfully represent the
combined textual and visual information.
Video Prediction As our framework can flexi-
bly encode multimodal prompts, we simply encode
the first frame along with the text prompt with
MLLMs. Following this, we generate subsequent
frames based on the given multimodal prompt. ToFVD↓ IS↑
CogVideo (Hong et al., 2022) (Chinese) 751.3 23.6
CogVideo (Hong et al., 2022) (English) 701.6 25.3
MagicVideo (Zhou et al., 2023) 655 -
LVDM (He et al., 2023) 641.8 -
Video LDM (Blattmann et al., 2023b) 550.6 33.5
VideoFactory (Wang et al., 2023) 410.0 -
Make-A-Video (Singer et al., 2022) 367.2 33.0
PYoCo (Ge et al., 2023) 355.2 47.46
VideoPoet (Kondratyuk et al., 2023) 355 38.4
W.A.L.T (Gupta et al., 2023) 258.1 35.1
Lumiere (Bar-Tal et al., 2024) 332.5 37.5
Snap Video ( 288×288px) 260.1 38.89
Snap Video ( 512×288px) 200.2 38.89
VIMI(288×288px) 262.5 35.6
VIMI(512×288px) 193.7 35.6
Table 1: Zero-shot evaluation results on UCF101
(Soomro et al., 2012).
facilitate this process, we prepend the task instruc-
tion "Generate a video with the following text and
first frame." to the prompt. This approach allows
the model to anchor the video generation process
with a visual starting point, ensuring that the sub-
sequent frames are coherently built upon both the
initial visual and textual inputs.
Text-to-Video Generation We also use our aug-
mented text-to-video dataset for instructed text-to-
video generation. Initially, the input comprises
only text. To enhance this input, we leverage re-
trieval methods as described in 3.1 to augment it
with retrieved images. We prepend the task in-
struction "Generate a video with the retrieved text-
image examples and text prompt." to the prompt,
setting a clear directive for the model. This ap-
proach ensures that the model receives enriched
and contextually relevant multimodal data, improv-
ing its capability to generate high-quality videos
from multimodal in-context descriptions.
4 Experiments
In this section, we evaluate VIMIagainst baselines
and ablate the model design components. Sec. 4.1
introduces our implementation details. Sec. 4.2
shows our results in three different evaluation set-
tings: (1) general text-to-video generation; (2)
subject-driven video generation; and (3) video pre-
diction. Sec. 4.3 shows ablations of our framework.
4.1 Implementation Details
We use an internal licensed dataset of images and
videos, each paired with a corresponding text cap-
5A car is driving on the road. <image>Input ImageGenerated Videos
Figure 5: Examples of Video Prediction results.
tion. We use the retrieval methods of section 3.1
to augment multimodal in-context examples for
pretraining and instruction tuning. We use UCF-
101 (Soomro et al., 2012), a video dataset from 101
action categories, for general text-to-video evalua-
tion. We use human subjects from the CelebA (Liu
et al., 2015) and general subject from the Dream-
booth (Ruiz et al., 2023) for qualitative comparison.
Training and evaluation details are in Appendix A
and B.
4.2 Results
Zero-shot Text-to-Video Evaluation We gener-
ate 10,000 videos (Wang et al., 2023; Blattmann
et al., 2023b) sampling classes with the same dis-
tribution as the original UCF-101 dataset. We pro-
duce a text prompt for each class label (Ge et al.,
2023) and compute FVD (Unterthiner et al., 2018)
and Inception Score (Salimans et al., 2016). Ta-
ble 1 shows our competitive performance to previ-
ous state-of-the-art text-to-video generators in both
FVD and IS metrics. We achieve the best FVD
score of 193.7 which we attribute to our visual
grounding during pretraining.
Zero-shot Subject-driven Video generation
Figures 1 and 3 show our results for subject-driven
video generation. Compared to VideoBooth (Jiang
et al., 2023b), our generator can handle multimodal
prompts that include multiple image entities, as il-
lustrated in Figure 1. We also compared our modelwith the concurrent work ID-Animator (He et al.,
2024) for zero-shot human identity preservation
generation in Figure 3. Overall, our video genera-
tor can not only ground on the visual input but also
synthesize temporally coherent videos with large
motion while retaining semantic control.
Video Prediction As shown in Figure 5, VIMI
can also generate videos conditioned on a single
image, thanks to our unified multimodal instruction
tuning stage. We first append the ‘<image>’ token
after the text prompt and use MLLMs to encode
this multimodal prompt for video prediction.
4.3 Ablation Study
Effectiveness of retrieval-augmented pretrain-
ing Figure 6a shows the evaluations of retrieval-
augmented pretraining on our validation set for
CLIP similarity and FID metrics. We denote VIMI
without retrieval augmented pretraining as VIMI
(w/o RAG). We use Snap Video (Menapace et al.,
2024) with text encoders T5-11B as another base-
line. The results indicate that using multimodal
large language models as the encoding leads to
unstable model training. Specifically, the FID re-
sults converge slowly and do not decrease after
125K pretraining steps. In contrast, with retrieval
augmented pretraining, VIMIshows faster conver-
gence and more stable training. After 200K pre-
training steps, using a multimodal large language
model as the encoder demonstrates performance
6(a) Effectiveness of retrieval-augmented pretraining.
(b) Effectiveness of the number of retrieved images.
Figure 6: Ablation of retrieval-augmented pretrain-
ing. (a) shows the evaluations of retrieval-augmented
pretraining on our validation set for CLIP similarity and
FID metrics. We denote VIMIwithout retrieval aug-
mented pretraining as VIMI(w/o RAG). (b) shows the
results of pretraining with different numbers of retrieved
images.
comparable to Snap Video (Menapace et al., 2024).
This highlights the effectiveness of our retrieval
augmented pretraining approach in stabilizing train-
ing and improving the overall performance of the
video generation model.
Effectiveness of the number of retrieved Images
Figure 6b shows the results of pretraining with dif-
ferent numbers of retrieved images. We set K to
be up to 2 for ablation studies, primarily consider-
ing the multimodal sequence length. We observe
that using only one retrieved image stabilizes the
model training. Increasing K to 2 provides further
stable improvements in the early pretraining stage.
After 200K pretraining steps, the model converges
to comparable evaluation results for both settings.
Given our aim to support multi-subject generation,
we use K=2 for the pretraining. This choice bal-
ances the need for rich contextual information with
the practical constraints of sequence length, ensur-
ing stable and effective training.
Effectiveness of multimodal instruction tuning
For the second stage, if we fine-tune only on
subject-driven data (denoted as “w/o Instruction
Tuning”), VIMIcan also generate videos from
multimodal interleaved prompts. To evaluate the
effectiveness of unified multimodal instruction tun-
ing, we compare this variant of VIMIin subject-
driven generation tasks after fine-tuning for thesame 100K steps. Figure 7 shows that multimodal
instruction tuning preserves identity better and fol-
lows instructions more accurately. We attribute this
improvement to the more diverse fine-tuning tasks
provided by multimodal instruction tuning.
5 Related Work
Video Generation Diffusion models are now the
standard methodology for both image (Ho et al.,
2020; Nichol and Dhariwal, 2021; Rombach et al.,
2022; Song et al., 2020) and video generation (An
et al., 2023; Blattmann et al., 2023b; Ge et al.,
2023; Guo et al., 2023b; He et al., 2023; Ho et al.,
2022a,b; Singer et al., 2022; Wang et al., 2023;
Zhou et al., 2023; Blattmann et al., 2023a). Early
video diffusion models use the U-Net (Ronneberger
et al., 2015) for the video generation task. Ho
et al. (2022b) showed that jointly training on image
and video data can improve text conditioned video
generation greatly. Make-A-Video (Singer et al.,
2022) proposed to build on text-to-image models
with novel and effective spatial-temporal modules.
Video LDM (Blattmann et al., 2023b) adopts a
latent diffusion paradigm where a pre-trained la-
tent image generator and latent decoder are fine-
tuned to generate temporally coherent videos. Most
recently, diffusion transformer (Peebles and Xie,
2022) has been widely adopted for video genera-
tion. Latte (Ma et al., 2024) proposes a latent diffu-
sion transformer, which adopts a video Transformer
as the backbone. W.A.L.T (Gupta et al., 2023) uses
a transformer-based method for latent video diffu-
sion models and a window attention architecture
tailored for joint spatial and spatiotemporal genera-
tive modeling. Snap Video (Menapace et al., 2024)
replaced U-Nets with efficient transformer-based
FITs (Chen and Li, 2023) and scaled to billions of
parameters. However, these existing works are still
limited by the use of text encoders like T5 or the
CLIP Text encoder, which lack visual grounding in
the pretraining phase. In our work, we propose to
utilize multimodal large language models to encode
multimodal inputs for video generation, addressing
the limitations by integrating visual grounding into
the pretraining process.
Retrieval Augmented Multimodal Pretraining
Retrieval augmentation has shown significant
promise, particularly in language models. Initial
work (Lewis et al., 2020; Guu et al., 2020) demon-
strated how incorporating external knowledge into
a language model can enhance its performance.
7Input Imagesw/o Instruction Tuningw/ Instruction Tuning
Figure 7: Ablation of multimodal instruction tuning . We compare VIMIwith a variant finetuned only on
subject-driven data during the second stage (“w/o Instruction Tuning”). We use the prompt “A <image> is sitting in
front of a computer and talking to the camera.”. VIMIachieves better semantic alignment and identity preservation.
This is achieved by first retrieving documents rele-
vant to the input text from an external memory, and
then integrating these retrieved documents with the
input for improved modeling (Hashimoto et al.,
2018; Karpukhin et al., 2020; Borgeaud et al.,
2022). Beyond language models, recent studies
have explored retrieval techniques for image gener-
ation (Blattmann et al., 2022; Sheynin et al., 2022;
Sarto et al., 2022; Ramos et al., 2023; Chen et al.,
2022). For instance, KNN-Diffusion (Sheynin
et al., 2022) used retrieval methods to search for
k-Nearest-Neighbors images, facilitating the train-
ing of a small and efficient text-to-image diffusion
model. RA-CM3 (Yasunaga et al., 2022) was the
first multimodal model capable of retrieving and
generating both text and images using autoregres-
sive models. Additionally, Re-Imagen (Chen et al.,
2022) employed an external multimodal knowledge
base to retrieve relevant image-text pairs, using
them as references for a diffusion model to generate
images. In contrast to these works, our approach is
the first to uses retrieval methods to augment text-
video datasets, formalizing multimodal input-video
pairs for video pretraining.
Multimodal Instruction Tuning Instruction tun-
ing was first proposed to finetune a large language
model with instructions to improve its zero-shot
learning performance on unseen tasks (Wei et al.,
2021; Chung et al., 2024). Inspired by its success
in language domain, instruction tuning was also in-
troduced in the vision generation domain (Yu et al.,
2023; Sun et al., 2023; Liu et al., 2024; Hu et al.,
2024). For instance, CM3Leon (Yu et al., 2023)
utilized the CM3 multimodal architecture (Agha-
janyan et al., 2022), demonstrating the substantial
benefits of scaling up and tuning on more diverseinstruction-style data. Emu2 (Sun et al., 2023)
demonstrated the in-context learning capabilities of
large multimodal models with a unified autoregres-
sive objective. More recently, Instruct-Imagen (Hu
et al., 2024) introduced multi-modal instruction for
image generation by fine-tuning a pre-trained text-
to-image diffusion model with a two-stage frame-
work. In our work, we are the first to propose in-
struction tuning for video generation, by unifying
three distinct video generation tasks within a sin-
gle, cohesive instruction framework. By leveraging
instruction tuning, we aim to enhance the model’s
ability to interpret and execute a wide range of
video generation instructions, thereby improving
its performance and applicability in diverse con-
texts.
6 Conclusion
In this work, we first construct a multimodal
prompt dataset for video pretraining using retrieval
methods. We then propose a two-stage training
strategy to enable diverse video tasks within the
same model. For the first stage, we introduce a mul-
timodal conditional video generation framework
for pretraining on these augmented datasets, estab-
lishing a foundational model for grounded video
generation. In the second stage, we fine-tune the
model from the first stage on three video genera-
tion tasks, incorporating multimodal instructions.
Our experiments demonstrate the effectiveness of
retrieval-augmented pretraining and the use of mul-
timodal instruction tuning. We hope this approach
opens up new opportunities for video pretraining,
such as building large-scale multimodal datasets
for pretraining, utilizing stronger multimodal large
language models for encoding, and employing in-
8struction tuning for diverse video tasks.
7 Limitations
Firstly, similar to subject-driven image generation
models, our video generator sometimes struggles
to produce accurate and faithful videos. To im-
prove visual quality, future work will focus on uti-
lizing stronger multimodal large language models,
diffusion tranformers and jointly fine-tuning these
models. Secondly, due to memory and training con-
straints, we only experimented with two context
examples and displayed at most two image enti-
ties for multi-subject-driven generation. Extending
this work to support any-subject video generation
will be a goal for future research. Thirdly, our
current results are based on qualitative evaluation.
Developing comprehensive evaluation methods for
grounded video generation, such as any-subject-
driven video generation, will be crucial for building
a visually grounded video generator.
8 Ethical Considerations
Like all generative AI advancements, visually
grounded video generation models raise important
ethical considerations, such as the creation of mis-
leading or false information and bias. Developers
and researchers should consider safeguards to ad-
dress these issues such as evaluating datasets, and
adding watermarks or other identification mech-
anisms. It is important to consider the societal
impacts and work towards solutions that balance
innovation with social responsibility.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Armen Aghajanyan, Bernie Huang, Candace Ross,
Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro
Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,
et al. 2022. Cm3: A causal masked multi-
modal model of the internet. arXiv preprint
arXiv:2201.07520 .
Jie An, Songyang Zhang, Harry Yang, Sonal Gupta,
Jia-Bin Huang, Jiebo Luo, and Xi Yin. 2023. Latent-
shift: Latent diffusion with temporal shift for efficient
text-to-video generation. arXiv .
Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Her-
rmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Jun-
hwa Hur, Yuanzhen Li, Tomer Michaeli, et al. 2024.Lumiere: A space-time diffusion model for video
generation. arXiv preprint arXiv:2401.12945 .
Andreas Blattmann, Tim Dockhorn, Sumith Ku-
lal, Daniel Mendelevitch, Maciej Kilian, Dominik
Lorenz, Yam Levi, Zion English, Vikram V oleti,
Adam Letts, et al. 2023a. Stable video diffu-
sion: Scaling latent video diffusion models to large
datasets. arXiv preprint arXiv:2311.15127 .
Andreas Blattmann, Robin Rombach, Huan Ling, Tim
Dockhorn, Seung Wook Kim, Sanja Fidler, and
Karsten Kreis. 2023b. Align your latents: High-
resolution video synthesis with latent diffusion mod-
els. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) .
Andreas Blattmann, Robin Rombach, Kaan Oktay,
Jonas Müller, and Björn Ommer. 2022. Retrieval-
augmented diffusion models. Advances in Neural
Information Processing Systems , 35:15309–15324.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning , pages 2206–2240. PMLR.
Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mi-
tra. 2023. Pix2video: Video editing using image
diffusion. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 23206–
23217.
Wenhao Chai, Xun Guo, Gaoang Wang, and Yan
Lu. 2023. Stablevideo: Text-driven consistency-
aware diffusion video editing. In Proceedings of the
IEEE/CVF International Conference on Computer
Vision , pages 23040–23050.
Ting Chen and Lala Li. 2023. Fit: Far-reaching inter-
leaved transformers. arXiv .
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace,
Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun
Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-
Hsuan Yang, and Sergey Tulyakov. 2024. Panda-
70m: Captioning 70m videos with multiple cross-
modality teachers. arXiv preprint arXiv:2402.19479 .
Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie,
Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin.
2023. Control-a-video: Controllable text-to-video
generation with diffusion models. arXiv preprint
arXiv:2305.13840 .
Wenhu Chen, Hexiang Hu, Chitwan Saharia, and
William W Cohen. 2022. Re-imagen: Retrieval-
augmented text-to-image generator. arXiv preprint
arXiv:2209.14491 .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
92024. Scaling instruction-finetuned language models.
Journal of Machine Learning Research , 25(70):1–53.
Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon,
Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin
Huang, Ming-Yu Liu, and Yogesh Balaji. 2023. Pre-
serve your own correlation: A noise prior for video
diffusion models. In Proceedings of the IEEE Inter-
national Conference on Computer Vision (ICCV) .
Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali
Dekel. 2023. Tokenflow: Consistent diffusion fea-
tures for consistent video editing. arXiv preprint
arXiv:2307.10373 .
Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yu-
fan Deng, Chongyang Ma, Weiming Hu, Zhengjun
Zha, Haibin Huang, Pengfei Wan, et al. 2023a. I2v-
adapter: A general image-to-video adapter for video
diffusion models. arXiv preprint arXiv:2312.16693 .
Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang,
Yu Qiao, Dahua Lin, and Bo Dai. 2023b. Animated-
iff: Animate your personalized text-to-image diffu-
sion models without specific tuning. arXiv .
Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera
Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José
Lezama. 2023. Photorealistic video generation with
diffusion models. arXiv preprint arXiv:2312.06662 .
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In International confer-
ence on machine learning , pages 3929–3938. PMLR.
Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and
Percy S Liang. 2018. A retrieve-and-edit framework
for predicting structured outputs. Advances in Neural
Information Processing Systems , 31.
Xuanhua He, Quande Liu, Shengju Qian, Xin Wang,
Tao Hu, Ke Cao, Keyu Yan, Man Zhou, and Jie
Zhang. 2024. Id-animator: Zero-shot identity-
preserving human video generation. arXiv preprint
arXiv:2404.15275 .
Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan,
and Qifeng Chen. 2023. Latent video diffusion mod-
els for high-fidelity long video generation. arXiv .
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. 2017. Gans
trained by a two time-scale update rule converge to
a local nash equilibrium. In Advances in Neural
Information Processing Systems (NeurIPS) .
Jonathan Ho, William Chan, Chitwan Saharia, Jay
Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P.
Kingma, Ben Poole, Mohammad Norouzi, David J.
Fleet, and Tim Salimans. 2022a. Imagen video: High
definition video generation with diffusion models.
arXiv .Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-
noising diffusion probabilistic models. Advances
in neural information processing systems , 33:6840–
6851.
Jonathan Ho and Tim Salimans. 2022. Classifier-free
diffusion guidance. arXiv .
Jonathan Ho, Tim Salimans, Alexey A. Gritsenko,
William Chan, Mohammad Norouzi, and David J.
Fleet. 2022b. Video diffusion models. In ICLR
Workshop on Deep Generative Models for Highly
Structured Data .
Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,
and Jie Tang. 2022. Cogvideo: Large-scale pre-
training for text-to-video generation via transformers.
arXiv .
Hexiang Hu, Kelvin CK Chan, Yu-Chuan Su, Wenhu
Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue
Ben, Boqing Gong, William Cohen, et al. 2024.
Instruct-imagen: Image generation with multi-modal
instruction. arXiv preprint arXiv:2401.01952 .
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023a. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang
Si, Dahua Lin, Yu Qiao, Chen Change Loy, and
Ziwei Liu. 2023b. Videobooth: Diffusion-based
video generation with image prompts. arXiv preprint
arXiv:2312.00777 .
Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. arXiv preprint
arXiv:2004.04906 .
Tero Karras, Miika Aittala, Timo Aila, and Samuli
Laine. 2022a. Elucidating the design space of
diffusion-based generative models. Advances in
Neural Information Processing Systems , 35:26565–
26577.
Tero Karras, Miika Aittala, Timo Aila, and Samuli
Laine. 2022b. Elucidating the design space of
diffusion-based generative models. In Advances in
Neural Information Processing Systems (NeurIPS) .
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,
Spencer Whitehead, Alexander C. Berg, Wan-Yen
Lo, Piotr Dollár, and Ross Girshick. 2023. Segment
anything. arXiv:2304.02643 .
Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama,
Jonathan Huang, Rachel Hornung, Hartwig Adam,
Hassan Akbari, Yair Alon, Vighnesh Birodkar,
et al. 2023. Videopoet: A large language model
for zero-shot video generation. arXiv preprint
arXiv:2312.14125 .
10Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2024. Visual instruction tuning. Advances in
neural information processing systems , 36.
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang
Su, Jun Zhu, et al. 2023. Grounding dino: Marrying
dino with grounded pre-training for open-set object
detection. arXiv preprint arXiv:2303.05499 .
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
2015. Deep learning face attributes in the wild. In
Proceedings of the IEEE international conference on
computer vision , pages 3730–3738.
Z. Luo, D. Chen, Y . Zhang, Y . Huang, L. Wang, Y . Shen,
D. Zhao, J. Zhou, and T. Tan. 2023. Videofusion: De-
composed diffusion models for high-quality video
generation. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR) .
Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen,
Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao.
2024. Latte: Latent diffusion transformer for video
generation. arXiv preprint arXiv:2401.03048 .
Willi Menapace, Aliaksandr Siarohin, Ivan Sko-
rokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil
Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian
Ren, et al. 2024. Snap video: Scaled spatiotempo-
ral transformers for text-to-video synthesis. arXiv
preprint arXiv:2402.14797 .
Alexander Quinn Nichol and Prafulla Dhariwal. 2021.
Improved denoising diffusion probabilistic models.
InInternational conference on machine learning ,
pages 8162–8171. PMLR.
William Peebles and Saining Xie. 2022. Scalable dif-
fusion models with transformers. arXiv preprint
arXiv:2212.09748 .
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In International confer-
ence on machine learning , pages 8748–8763. PMLR.
Rita Ramos, Bruno Martins, Desmond Elliott, and Yova
Kementchedjhieva. 2023. Smallcap: lightweight im-
age captioning prompted with retrieval augmenta-
tion. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages
2840–2849.Weiming Ren, Harry Yang, Ge Zhang, Cong Wei,
Xinrun Du, Stephen Huang, and Wenhu Chen.
2024. Consisti2v: Enhancing visual consistency
for image-to-video generation. arXiv preprint
arXiv:2402.04324 .
Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. 2022. High-
resolution image synthesis with latent diffusion mod-
els. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages
10684–10695.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
2015. U-net: Convolutional networks for biomedical
image segmentation. In Medical Image Computing
and Computer-Assisted Intervention (MICCAI) .
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael
Pritch, Michael Rubinstein, and Kfir Aberman. 2023.
Dreambooth: Fine tuning text-to-image diffusion
models for subject-driven generation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 22500–22510.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba,
Vicki Cheung, Alec Radford, and Xi Chen. 2016.
Improved techniques for training gans. In Advances
in Neural Information Processing Systems (NeurIPS) .
Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita
Cucchiara. 2022. Retrieval-augmented transformer
for image captioning. In Proceedings of the 19th in-
ternational conference on content-based multimedia
indexing , pages 1–7.
Hinrich Schütze, Christopher D Manning, and Prab-
hakar Raghavan. 2008. Introduction to information
retrieval . Cambridge University Press Cambridge.
Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel
Singer, Oran Gafni, Eliya Nachmani, and Yaniv
Taigman. 2022. Knn-diffusion: Image gener-
ation via large-scale retrieval. arXiv preprint
arXiv:2204.02849 .
Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie
An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron
Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and
Yaniv Taigman. 2022. Make-a-video: Text-to-video
generation without text-video data. arXiv .
Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 .
Khurram Soomro, Amir Roshan Zamir, and Mubarak
Shah. 2012. UCF101: A dataset of 101 human ac-
tions classes from videos in the wild. arXiv .
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang,
Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming
Rao, Jingjing Liu, Tiejun Huang, et al. 2023. Gen-
erative multimodal models are in-context learners.
arXiv preprint arXiv:2312.13286 .
11Gemini Team, Rohan Anil, Sebastian Borgeaud,
Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai,
Anja Hauth, et al. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Thomas Unterthiner, Sjoerd van Steenkiste, Karol Ku-
rach, Raphael Marinier, Marcin Michalski, and Syl-
vain Gelly. 2018. Towards accurate generative mod-
els of video: A new metric & challenges. arXiv .
Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan
Kindermans, Hernan Moraldo, Han Zhang, Moham-
mad Taghi Saffar, Santiago Castro, Julius Kunze,
and Dumitru Erhan. 2022. Phenaki: Variable length
video generation from open domain textual descrip-
tions. In International Conference on Learning Rep-
resentations .
Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He,
Junchen Zhu, Jianlong Fu, and Jiaying Liu. 2023.
Videofactory: Swap attention in spatiotemporal dif-
fusions for text-to-video generation. arXiv .
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li,
Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan.
2021. Godiva: Generating open-domain videos from
natural descriptions. ArXiv .
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi,
Rich James, Jure Leskovec, Percy Liang, Mike Lewis,
Luke Zettlemoyer, and Wen-tau Yih. 2022. Retrieval-
augmented multimodal language modeling. arXiv
preprint arXiv:2211.12561 .
Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin
Muller, Olga Golovneva, Tianlu Wang, Arun Babu,
Binh Tang, Brian Karrer, Shelly Sheynin, et al.
2023. Scaling autoregressive multi-modal models:
Pretraining and instruction tuning. arXiv preprint
arXiv:2309.02591 , 2(3).
David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,
Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and
Mike Zheng Shou. 2023. Show-1: Marrying pixel
and latent diffusion models for text-to-video genera-
tion. arXiv preprint arXiv:2309.15818 .
Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. 2023. Magicvideo: Effi-
cient video generation with latent diffusion models.
arXiv .Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. arXiv preprint arXiv:2304.10592 .
12A Training details
For pretraining, we can either start from scratch or
initialize the weights of the model from existing
text-to-video generators. In our work, we initialize
the FIT weights from (Menapace et al., 2024).
We keep its parameters frozen for 30,000 steps to
stabilize the initial training phase, and then fine-
tune the entire model for an additional 100,000
steps. In the second stage, we fine-tune the model
starting from the weights obtained in the first stage
for 30,000 steps. We use a learning rate of 5e−3, a
cosine learning schedule, and a total batch size of
256 videos and 256 images.
B Evaluation Protocol
We evaluate our method against baselines by fol-
lowing the protocols in (Singer et al., 2022; Ge
et al., 2023; Wang et al., 2023; Blattmann et al.,
2023b; Zhou et al., 2023; Luo et al., 2023) for
zero-shot evaluation on the UCF-101 (Soomro
et al., 2012). We generate 16 frames videos in
512×288px resolution at 24fps. To validate the ef-
fectiveness of pretraining, ablations are performed
in64×36px resolution using the first-stage model
only, and compute FID (Heusel et al., 2017), FVD
(Unterthiner et al., 2018) and CLIPSIM (Wu et al.,
2021) metrics against the test set of our internal
dataset on 50k generated videos.
C Inference
We produce video samples from gaussian noise and
user-provided conditioning information using the
deterministic sampler of (Karras et al., 2022b) and
two-stage cascade. We use 256 sampling steps for
the first-stage and 40 for the second-stage model,
and employ classifier free guidance (Ho and Sali-
mans, 2022) to improve text-video alignment.
13