XplainLLM : A Knowledge-Augmented Dataset for Reliable Grounded
Explanations in LLMs
Zichen Chen1Jianda Chen2Ambuj K. Singh1Misha Sra1
1University of California, Santa Barbara
2Nanyang Technological University, Singapore
{zichen_chen, ambuj, sra}@ucsb.edu jianda001@ntu.edu.sg
Abstract
Large Language Models (LLMs) have achieved
remarkable success in natural language tasks,
yet understanding their reasoning processes re-
mains a significant challenge. We address this
by introducing XplainLLM , a dataset accom-
panying an explanation framework designed
to enhance LLM transparency and reliability.
Our dataset comprises 24,204 instances where
each instance interprets the LLM’s reasoning
behavior using knowledge graphs (KGs) and
graph attention networks (GAT), and includes
explanations of LLMs such as the decoder-
only Llama-3 and the encoder-only RoBERTa.
XplainLLM also features a framework for gener-
ating grounded explanations and the debugger-
scores for multidimensional quality analysis.
Our explanations include why-choose andwhy-
not-choose components, reason-elements , and
debugger-scores that collectively illuminate the
LLM’s reasoning behavior. Our evaluations
demonstrate XplainLLM ’s potential to reduce
hallucinations and improve grounded explana-
tion generation in LLMs. XplainLLM is a re-
source for researchers and practitioners to build
trust and verify the reliability of LLM outputs.
1 Introduction
As the capabilities and applications of large lan-
guage models (LLMs) continue to expand (Liu
et al., 2023; Achiam et al., 2023; Touvron et al.,
2023; Jiang et al., 2024), the need for transparency
and interpretability in their reasoning behavior has
become increasingly urgent (Arrieta et al., 2020).
Traditional methods (Ribeiro et al., 2016; Lundberg
and Lee, 2017; Casalicchio et al., 2019) allow us
to get insights into the reasoning behind language
model outputs, but they fall short of providing a
complete picture, leaving the logic behind complex
decision obscured (Huang et al., 2023). This gap
presents a significant barrier in applications where
model decision transparency is important, such ashealthcare (Ghosh et al., 2024), law (Cheong et al.,
2024), and public services (Musumeci et al., 2024).
Current methods for explaining LLM’s reason-
ing behavior primarily focus on the analysis of
parameter changes (Clark et al., 2019; Jacovi et al.,
2021; Bills et al., 2023) and chain-of-thought (CoT)
based self-explanation (Huang et al.; Li et al.,
2023). Analysis of parameter changes bases the
explanations on self-attention weights in models
like BERT (Kenton and Toutanova, 2019) and GPT-
2 (Radford et al., 2019), deducing correlations be-
tween input tokens and the model’s predictions.
However, the relationships highlighted in these gen-
erated explanations are difficult to understand for
humans. CoT-based self-explanation, on the other
hand, iteratively generates rationales step-by-step.
Due to the inherent constraints in LLMs, these ex-
planations often have hallucinations and can not
reflect the real reasoning process (Huang et al.,
2023).
We introduce XplainLLM , a dataset accompany-
ing an explanation framework designed to enhance
transparency, explainability, and understandabil-
ity in LLM reasoning behaviors. By integrating
knowledge graphs (KGs) and Graph Attention Net-
works (GAT) (Veli ˇckovi ´c et al., 2018), we construct
a structured and reliable dataset that anchors ex-
planations in reasoning-relevant knowledge. We
link the LLM reasoning process to the entities
and relations within KGs to help provide an intu-
itive and interpretable representation of the LLM’s
decision-making process. Our process also helps
facilitate model tuning, debugging, robustness eval-
uation and demonstration in in-context learning.
XplainLLM provides a structured explanation of
two distinct types of LLMs: Llama-3-8B (Touvron
et al., 2023) (decoder-only model) and RoBERTa-
large (Liu et al., 2019) (encoder-only model). A
total of 24,204 instances are included in the dataset.
The explanations are tied to two models’ reasoning
processes, derived from their performance on thearXiv:2311.08614v2  [cs.CL]  20 Sep 2024Dataset Size Answer Format Expl. Format Source Model Match? Self-Explanatory? "Why Not" Included?
CoS-E 9,500 MC NL Human × × ×
ECQA 10,962 MC NL Human × ✓ ×
Neuron 307,200 Neuron NL + Score Model ✓ × ×
XplainLLM 24,204 MC NL Model ✓ ✓ ✓
Table 1: Comparison of prevalent explanation datasets with XplainLLM , detailing instance count (Size), answer
types (Answer Format: e.g., multiple-choice (MC)), explanation styles (Explanation Format: e.g., natural language
(NL)), origin (Source), alignment with model reasoning (Model Match?), necessity of human intervention to deduce
the reasoning (Self-Explanatory?), and inclusion of reasons for alternative answer rejection ("Why Not" Included?).
CommonsenseQA (Talmor et al., 2019) challenge.
Additionally, we introduce an explanation frame-
work that utilizes a retrieval-based method to sup-
port generating grounded explanations for LLMs.
This framework operates without the need for ad-
ditional model training, utilizing XplainLLM as a
knowledge base to retrieve the most relevant data
points to the given query. The selected data points
serve as demonstration examples for in-context
learning (Dong et al., 2022), enabling the LLMs
to generate explanations that are more grounded in
the reasoning process.
We evaluate the quality of the explanations in
XplainLLM through human and automated evalua-
tions. The overall quality of explanations achieves
an average score of 0.87/1.00 by human evaluators,
and an average of 0.89/1.00 by automated evalu-
ators. We evaluate our framework by comparing
the performance of LLMs with and without our
framework, and the results show that LLMs under
our framework outperform the benchmark, with a
performance gap extending to 20%. We also evalu-
ate the quality of the explanations generated by our
framework, and the results underscore the quality
of our explanations on multiple metrics.
In summary, we make two key contributions to
the field of explainable AI for LLMs: (1) an expla-
nation dataset of model reasoning behavior, and (2)
a framework for improving the interpretability of
LLMs through structured, grounded explanations.
To the best of our knowledge, XplainLLM is the
first dataset to provide structured and grounded
explanations for LLM reasoning behavior.
2 Related Work
Interpretability in LLMs Explainable AI aims
to address the issue of interpreting the outcomes of
language models (Li et al., 2023; Wiegreffe et al.,
2021; Madsen et al., 2022). One of its goals is to
generate explanations that enable humans to easily
understand the decision-making process. Zelik-man et al. (2022); Zhang and Gao (2023); Wang
et al. (2023) utilize gradual strategies that itera-
tively generates the rationales step-by-step. Huang
et al.; Chen et al. (2023); Tanneru et al. (2024);
Chakraborty et al. (2023) utilize the CoT to find
the rationale and apply the reasoning capabilities
of LLMs to domain tasks. However, these ex-
planations are inherently constrained in capturing
prompt-specific reasoning, which often generates
hallucinations and can not reflect the real reasoning
of LLMs (Turpin et al., 2024).
Another goal is focused on explaining in a trust-
worthy way. Rajani et al. (2019a) introduce an
explainable factor to minimize the risk of unrea-
sonable explanation generation. Chen et al. (2021)
integrate the external knowledge to generate why
and why-not counterfactual explanations. Zelik-
man et al. (2022) apply self-checker mechanism to
ensure trusted rationals. However, these method
fail to accurately capture the core reasoning of
LLMs. In contrast, our work enhances LLM trust-
worthiness and deepens human understanding of
its reasoning behavior, improving the potential in
end-user applications.
Explanation Datasets The explainable datasets
for language models can be categorized into three
types (Wiegreffe and Marasovic, 2021): (1) high-
lights: provide input elements such as words and
phrases, as explanations to a predicted output (Cam-
buru et al., 2018; DeYoung et al., 2020; Yin et al.,
2021; Bills et al., 2023); (2) free-text explanations:
provide readable textual explanations in words or
sentences (Rajani et al., 2019b; Sap et al., 2020;
Brahman et al., 2021); (3) structured explanations:
provide natural language explanation but are con-
strained by the explanation writing process (Aggar-
wal et al., 2021; Jhamtani and Clark, 2020; Inoue
et al., 2020). Different from these, our explana-
tion incorporates highlighted reason-elements and
guided instruction to generate a free-text explana-
tion. Our explanation is structured and groundedOffice/Department/Division Name Input 
QuestionAnswer
Model	Reasoning	ProcessLLM	+	GAT	ModelInput	Query
Reasoning	InterpretationRanked		reason-elementsAnswerControlled	Explanation	GenerationExplanation	Generator	Stage	1	why-choose	Explanation			Stage	2	why-not-choose	Explanation		Refinement	&	Evaluation	Human	EvaluationsDebugger		ScoreFigure 1: Overview of XplainLLM in LLM Reasoning
Interpretation and Explanation Generation.
in the reasoning process, enhancing the trustwor-
thiness and comprehensiveness of the content. We
present a comparison with prevalent explanation
datasets (Rajani et al., 2019b; Aggarwal et al.,
2021; Bills et al., 2023) in Table 1.
3XplainLLM : Dataset, Explanation
Framework and Debugger-Score
XplainLLM serves three essential purposes in inter-
preting LLMs’ reasoning behavior. First, it utilizes
KG and GAT to interpret LLM through parameter
changes, collecting these explanations to build a
dataset. The LLMs we used are Llama-3-8B (Tou-
vron et al., 2023) (decoder-only) and RoBERTa-
large (Liu et al., 2019) (encoder-only). Second, we
provide an explanation framework for generating
faithfully grounded explanations without additional
training. Third, we introduce the debugger-score ,
which is designed for multidimensional analysis
to quantify the quality of explanations, supporting
our framework for comprehensively evaluating and
improving LLM explainability.
3.1 Task Definition and Collection Method
The primary goal of XplainLLM is to enhance the
interpretability of LLMs through grounded expla-
nations. We define the task as generating expla-
nations that clarify the decision-making processes
behind model predictions. We use QA tasks to
generate instances for our dataset. The overview
of the collection process is shown in Figure 1. A
more detailed data collection description is shown
in Appendix F.
The LLM’s reasoning is grounded in a structured
KG, which is used to identify the most salient fea-
tures that influence the model’s predictions. We
employ GAT to analyze the KG’s structure and
identify the influence of specific nodes and edgesthat are salient to the model’s decision-making pro-
cess. Each instance in XplainLLM is formulated as
follows:
Instance = ((Q, A),Explanation ) (1)
where (Q, A)is the question-answer pair and
Explanation includes:
•Awhy-choose explanation, detailing the reason
behind the model’s answer choice.
•Awhy-not-choose explanation, detailing reasons
against alternative choices.
•Ranked reason-elements , identified through
GATs that analyze the KG’s structure to iden-
tify critical influencing elements.
•Adebugger-score for each explanation, quantify-
ing its faithfulness, completeness, accuracy and
overall quality.
Graph-Based Reasoning Interpretation. To
produce the aforementioned explanation, we in-
troduce a graph-based interpreting method to learn
the features that influence the model’s decision-
making process. We first extract the key elements
from the KG g. This process involves identifying
nodes and edges within the gthat are relevant to
the input question and answer pair. We incorporate
node relevance scores into this retrieval process,
using the LLM’s knowledge to guide the pruning
of the g:
Ge=PruneKG (Q, A, g, s i) (2)
where sirepresents the relevance score for each
node iin the retrieved graph, calculated using
LLM’s probability function that assesses the align-
ment of node embeddings with the input context
(Q, A). The function PruneKG evaluates the se-
mantic relationship between node embeddings and
the query. This extraction leverages the LLM’s
knowledge to focus on the most informative ele-
ments for the given QA context. The algorithm for
constructing the Geis provided in Appendix A.
Once the relevant subgraph Geis obtained, we
use a GAT to determine the significance of each
node and edge in contributing to the model’s output.
Each node iatk-th layer is represented by a feature
vector hki. The attention αijfor each node pair
(i, j)are computed using a softmax function over
a parameterized self-attention mechanism athat
captures the relationship dynamics:
αij=exp(a(hki, hkj))P
l∈N(i)exp(a(hki, hkl))(3)where N(i)denotes the neighbors of node i.
The updated node features hk+1,iare computed
by aggregating the features of neighboring nodes
weighted by their respective attention scores:
hk+1,i=σ
X
j∈N(i)αijWfm(hkj, ui, rij)
+hki
(4)
where fmis a multi-layer perceptron (MLP) that
processes features of neighboring nodes consider-
ing their types and interrelations. Wis a weight ma-
trix,σis a non-linear activation function. We pro-
vide the details of the GAT model in Appendix B.
We define the probability of selecting an answer
vfrom the set Aby leveraging both the representa-
tion embeddings from the language model ( HLM)
and the graph-based reasoning features ( hKand
αK) extracted from our subgraph Ge:
P(a|q)∝exp( MLP (HLM, hK, αK)) (5)
where hKrepresents the output features from the
final layer of our K-layer graph reasoning network,
andαKare the attention coefficients. To this end,
we map the LLM’s reasoning to the graph features.
The extracted attention features are mapped to their
corresponding nodes in the Ge, and we select the
topnnodes with the highest attention scores for
generating the explanations.
Controlled Explanation Generation. Upon ob-
taining the reasoning features, we transform them
into structured and human-understandable expla-
nations through a two-stage instructional process.
The top nnodes are selected as the key reason-
elements setR, which guides the explanation gen-
erator model Fto construct the explanations. The
explanation generation process includes: (1) why-
choose explanation: the reasoning behavior behind
the model’s choice, and (2) why-not-choose ex-
planation: the rationale for dismissing other po-
tential answers. The instruction for why-choose
stage is: “ Basis : [TASK _TY PE ], Input :
[Q, A], Output [y′, R], Explanation (Stage 1) :
[y′/y]”. The output of stage 1 named Ewhyis used
as the input for stage 2. The instruction for why-
not-choose stage is: “ Explanation (Stage 2) :
[Ewhy, A\y′]”. The details of the instruction
are provided in the Appendix C.
Office/Department/Division NameQueryInputExplanation	Retrieval	Embedding	Model
XplainLLMSimScore		ℱscoreQATop	mRetrieved	ExplanationMax	Debugger	ScoreLarge	Language	ModelsAnswer	Grounded	Explanation	Debugger	Score:		•Faithfulness	•Completeness		•Accuracy	•OverallOutput
Explanation	Retrieval	Figure 2: Explanation Framework for Grounded Expla-
nation Generation in LLMs.
3.2 Explanation Framework for Grounded
Explanations
To enhance the usability of XplainLLM and facili-
tate the generation of grounded explanations for dif-
ferent types of LLMs (especially for private LLMs,
e.g., GPT-4), we introduce an explanation frame-
work that leverages the collected dataset to generate
faithfully grounded explanations without additional
model training. The framework is illustrated in Fig-
ure 2. The process is divided into three steps:
Embedding Calculation. When receiving a new
query (Qnew, Anew), its embeddings eQA newis cal-
culated using the an embedding models. To gener-
alize our framework, we use voyage-2-large model
from VOYAGE AI1, as our embedding model to
extract the embeddings, due to its state-of-the-art
performance in generalist text embedding2.
Similarity Computation and Retrieval. We re-
trieve the most contextually relevant instances by
computing the cosine similarity between new query
embedding eQAnewand instance embedding eQA
inXplainLLM E:
Fscore(eQAnew,eQA) =e⊤
QAneweQA
∥eQAnew∥2∥eQA∥2
This function Fscore scores each instance
sim(eQAnew,eQA)for relevance. The eQAcan
be pre-computed and stored in the dataset for
efficient retrieval. To accelerate the retrieval
process, we provide embeddings for each instance
inXplainLLM , using voyage-2-large.
Instance Selection and Explanation Generation.
The top minstances with the highest similarity
1https://docs.voyageai.com/docs/embeddings
2https://huggingface.co/spaces/mteb/
leaderboardscores, sim(eQAnew,eQA), are selected. Each in-
stance may contain multiple explanations from dif-
ferent LLMs, denoted as Et, where tindexes the
instances. For each instance, we select the expla-
nation e∗that maximizes the debugger-score set
D:
e∗= arg max
e∈EtX
d∈Dwd·D(e, d)
where and wdare the weights reflecting user pref-
erences for each dimension. This selection is in-
fluenced by user-specified preferences which dic-
tate the importance of various dimensions of ex-
planation quality, such as faithfulness or accuracy.
We will introduce the debugger-score in Section
3.3. These selected instances are used as in-context
learning examples for targeted LLM to generate
grounded explanations.
3.3 Debugger-Score for Explanation Analysis
To improve the understanding of generated ex-
planations, we introduce the debugger-score to
evaluate the quality of explanations. Inspired by
the method of transformer debugging (Bills et al.,
2023), our debugger-score simulates a “perfect”
LLM to benchmark against the actual LLM’s rea-
soning. It quantifies the quality of explanations by
assessing:
1.Faithfulness: How accurately the explanations
reflect the actual reasoning of the LLM.
2.Completeness: Whether the explanations cover
all essential aspects of the reasoning process.
3.Accuracy: The correctness of the explanation
in terms of factual and contextual relevance.
4.Overall: The overall quality of the explanation,
combining the above dimensions.
Thedebugger-score utilizes predefined instruc-
tions to guide the evaluation, focusing on identify-
ing discrepancies between the simulated “perfect”
LLM and the actual LLM. Our evaluation method
quantifies the quality of explanations, providing a
measure of where the LLM’s reasoning succeeds or
falls short. Our debugger-score is used to enhance
the reliability and transparency of the explanations.
Further details on the implementation and func-
tionality of the debugger-score can be found in the
Appendix D.
4 Dataset Overview and Preparation
4.1 Dataset Description
Schema. XplainLLM contains fields that corre-Why-choose Why-not-choose Whole Explanation
Overall 94.77 85.74 180.81
Training Set 94.41 85.22 179.63
Dev Set 93.00 84.54 178.44
Testing Set 96.89 87.46 184.35
Table 2: The average word counts of why-choose expla-
nation, why-not-choose explanation and whole explana-
tion in our XplainLLM dataset.
spond to the QA pair, the model’s predicted answer,
the ground-truth label, and an explanation set.
Explanations Set. The explanation set includes
a set of 50 reason-elements , e.g., words or phrases,
sorted by attentions, a set of top-5 reason-elements ,
awhy-choose explanation in free-text form, a why-
not-choose explanation also in free-text form. An
example instance is shown in Appendix G.
Statistics. XplainLLM includes 24,204 instances
of explanations, split according to the official Com-
monsenseQA’s partitioning into three sets: the
training, development (dev), and testing sets. The
average word count of EwhyandEwhy−notare
94.77 and 85.74 respectively, resulting in an ag-
gregate count of approximately 180.81 words per
whole explanation. A more detailed breakdown
of the average word count is provided in Table 2.
Additional statistics can be found in Appendix H.
4.2 Data Preparation
XplainLLM captures and analyzes the reasoning be-
havior of LLMs on CommonsenseQA dataset (Tal-
mor et al., 2019). CommonsenseQA serves as a
foundational benchmark for assessing the common-
sense reasoning capabilities of these models.
We select Llama-3-8B and RoBERTa-large as
LLMs for our dataset as they exemplify decoder-
only and encoder-only LLMs respectively, provid-
ing a comprehensive view of different model archi-
tectures in language understanding. The models are
fine-tuned on CommonsenseQA’s official training
set, to understand and interpret the complexities of
commonsense reasoning. We utilize ConceptNet
(Speer et al., 2017) as our KG to obtain ge. This KG
captures commonsense concepts and their interre-
lations. We use a 5-layer GAT model to extract the
reasoning paths. We use GPT-3.5-turbo (Ouyang
et al., 2022) and GPT-4-turbo (Achiam et al., 2023)
as explanation generator model Fto generate a
natural language explanation in a sentence or a
paragraph. To ensure the quality of our dataset, we
conduct a post-generation evaluation. All expla-nations undergo human review. Human evaluators
identify inaccuracies, and any discrepancies in ex-
planations, and return to Ffor refinement. This
procedure mitigates potential issues from model-
generated explanations, guaranteeing clarity and
relevance aligned with human understanding. We
also provide embeddings of the (Q, A)pair for
each instance in the dataset. The embeddings are
generated using the voyage-large-2. The debugger-
score is calculated using GPT-4-turbo. Further ex-
periment specifics and data collection procedures
are provided in the Appendix E and F.
5 Experiments and Evaluation
5.1 Evaluation Methodology
We evaluate XplainLLM and explanation frame-
work through two main perspectives:
1.Explanation Quality Evaluation: The quality
of the explanations generated by the LLMs is
assessed via a dual approach: (1) Human Eval-
uation - Experts and crowdsourcing review the
explanations, and (2) Automated Evaluation -
GPTs evaluate the explanations.
2.Framework Effectiveness: We measure the
impact of our proposed methods on the ground-
edness of newly generated explanations and the
performance of the LLMs. This includes: (1)
Grounded Explanation Assessment - Using
thedebugger-score to evaluate how well the
explanations are grounded in factual content,
and(2) Performance Analysis - We evaluate
changes in the accuracy of the LLM outputs by
comparing metrics before and after applying our
framework.
Specifically, the evaluation metrics for explanation
quality assessment are human-centered metrics, fol-
lowing the guidelines of Hoffman et al. (2018).
Each explanation is assessed using seven evaluative
questions that explore different aspects of the ex-
planation’s impact and quality. The metrics encom-
pass overall quality, understandability, trustworthi-
ness, satisfaction, detail sufficiency, completeness,
and accuracy. Evaluators allocate scores to these
questions using a three-point Likert scale: 1 (dis-
agree), 2 (neutral), and 3 (agree). Subsequently,
scores are normalized to the range [0, 1]. Higher
scores suggest better quality. Detailed definitions
are provided in the Appendix I.2.Expert <-> GPT-3.5 Expert <-> GPT-4 GPT-3.5 <-> GPT-4
ρ 0.70 0.60 0.66
Table 3: Correlation coefficient ( ρ) between overall
quality scores evaluated by expert, GPT-3.5 and GPT-4.
5.2 Explanation Quality Evaluation
We conducted human and automated evaluations
to go beyond the technical evaluation of the expla-
nations. The human evaluation involved three ex-
perts with NLP backgrounds and 50 general users
via Prolific3. Our participant pool was gender-
balanced, and comprised of native English speakers
with at least a high school education. Experts and
users rate 20 randomly selected explanations based
on guidelines adapted from (Hoffman et al., 2018)
to ensure consistency and mitigate bias. Automated
evaluations are performed using GPT-3.5-turbo and
GPT-4 to parallel human judgment, quantifying
performance with standardized scores. Detailed
methodologies and participant instructions are pro-
vided in Appendix I.1.
Results of Expert and Automated Evaluation.
The feedback from human experts highlighted the
distinctiveness of our explanations compared to ex-
isting methods. One expert remarked, “ In compari-
son to prior explanations, these explanations pro-
vide a more intuitive understanding of the LLM’s
reasoning behavior. The explanations are cogent,
and even in instances of erroneous predictions, the
underlying reasoning remains transparent and com-
prehensible. ” This feedback underscores the clarity
and transparency of our explanations.
The results are summarized in Table 4. Human
experts assign an average score of 0.93/1.00 across
seven evaluation metrics, with “understandability”
and “completeness” receiving the highest scores.
The automated evaluators, GPT-3.5 and GPT-4, as-
sign average scores of 0.91/1.00 and 0.92/1.00, re-
spectively. The performance of these automated
evaluators aligns closely with human expert evalu-
ations across dimensions, as shown in Figure 3.
Further insights into the human-like understand-
ing of automated evaluators and their assessment
of explanations are detailed in Table 3. This data
shows a significant agreement between the auto-
mated evaluators and human experts. Such find-
ings further support the credibility and value of our
explanations.
3https://www.prolific.comOverall Quality Understandability Trustworthiness Satisfaction Sufficiency of detail Completeness Accuracy
GPT-3.5 0.98 0.98 0.98 0.98 0.98 0.98 0.98
GPT-4 0.90 0.93 0.87 0.87 0.88 0.87 0.88
Human Expert 0.91 0.97 0.95 0.89 0.98 0.97 0.93
Crowdsourcing 0.85 0.89 0.86 0.80 0.83 0.81 0.85
Table 4: Evaluation by automated evaluator GPT-3.5, GPT-4, human experts and crowdsourcing, on 7 evaluation
metrics.
Overall QualityUnderstandabilityTrustworthiness
Satisfaction
Sufficiency of Detail
CompletenessAccuracy
00.20.40.60.81GPT-3.5
GPT-4
Expert
Loading [MathJax]/extensions/MathMenu.js
Figure 3: Evaluation by human experts, automated eval-
uator GPT-3.5 and GPT-4.
Overall qualityUnderstandabilityTrustworthiness
Satisfaction
Sufficiency of detail
CompletenessAccuracy
00.20.40.60.81Overall
CP
IP
Loading [MathJax]/extensions/MathMenu.js
Figure 4: Human evaluation of explanations: Overall,
CP, and IP. Note that the CP scores align closely with
the overall scores.
Results of Crowdsourcing Evaluation. we
present the average scores from crowdsourcing on
eight metrics, as depicted in Figure 4. These scores
reflect evaluations of the overall explanations, as
well as separate assessments for explanations of
correct predictions (CP) and incorrect predictions
(IP). The details of our analysis are discussed be-
low.
Participants assigned a high average score of
0.87/1.00 to the overall quality of our explana-
tions, indicating a favourable perception and under-
scoring their above-average clarity. The explana-
tions received an average understandability score
of 0.89/1.00, demonstrating their clarity. The low
variance of 0.26 suggests consistent comprehension
among participants. However, a detailed analysis
shows a disparity based on the LLM’s prediction
accuracy: explanations for correct predictions (CP)
were highly rated at 0.91/1.00 with a variance of
0.26, while explanations for incorrect predictions(IP) scored lower at 0.74/1.00 with a variance of
0.65, indicating less clarity and greater variability
in participant responses.
In terms of trustworthiness, our explanations
scored an average of 0.88/1.00 for CP. A Pearson
correlation coefficient of 0.71 between trustwor-
thiness and understandability confirms a strong
positive relationship, suggesting that clearer ex-
planations enhance participants’ trust in the LLM’s
outputs.
Overall satisfaction with our explanations is
high, with 86% of participants stating that the expla-
nations meet or exceed their expectations. 97.36%
of the explanations are considered sufficiently de-
tailed. The completeness of our explanations also
received high marks, with an average score of
0.81/1.00 and a median score of 1.00/1.00, sug-
gesting that over half of the participants find the
explanations to be entirely comprehensive. How-
ever, the distribution may reflect differences in the
evaluators’ familiarity with AI or occasional over-
simplifications by the model. The accuracy of the
explanations are rated at 0.84/1.00, with a notice-
able disparity between CP at 0.87/1.00 and IP at
0.64/1.00, highlighting how the LLM’s prediction
accuracy significantly influences the perceived ac-
curacy of explanations. Furthermore, a Pearson
correlation of 0.68 between accuracy and trustwor-
thiness indicates that more accurate explanations
are considered more trustworthy.
The positive feedback from our crowdsourcing
evaluations robustly validates XplainLLM , demon-
strating its effectiveness in conveying the complex-
ities of the LLM’s decision-making in a clear, trust-
worthy, and satisfying manner to users.
5.3 Framework Evaluation
In evaluating our proposed framework, we in-
clude five LLMs: GPT-3.5-turbo (Brown et al.,
2020), GPT-4-turbo (Achiam et al., 2023), Llama3-
8B (Touvron et al., 2023), Llama3-70B (Touvron
et al., 2023), and Mixtral-8x7B (Jiang et al., 2024).
We compare the vanilla versions of these modelsModel #P Version Faithfulness Completeness Accuracy Overall
gpt-3.5-turbo UnknownVanilla 3.50 2.95 3.65 3.37
XplainLLM 3.45 3.05 3.55 3.35
gpt-4-turbo UnknownVanilla 3.67 3.10 3.95 3.57
XplainLLM 4.05 3.65 4.10 3.93
llama3-8b 8.02BVanilla 3.50 2.65 3.60 3.25
XplainLLM 3.35 2.85 3.35 3.18
llama3-70b 70.6BVanilla 3.6 2.85 3.80 3.42
XplainLLM 3.95 3.10 4.05 3.70
mixtral-8x7b 46.7BVanilla 3.70 2.90 3.80 3.47
XplainLLM 3.75 3.05 3.80 3.53
Table 5: Comparison of Vanilla and XplainLLM Ver-
sions of Models with debugger-score .
GPT-3.5-turboGPT-4-turboLLama3-8bLlama3-70BMixtrial-8x7b0204060Vanilla Version With XplainLLM
ModelAccuracy (%)6/15/24, 3:08 AM 127.0.0.1:50291
127.0.0.1:50291 1/1
Figure 5: Accuracy comparison of vanilla version and
with XplainLLM version for different models.
with the versions enhanced by XplainLLM . The re-
sults are summarized in Table 5. We specifically
selected 20 questions from XplainLLM designed
to challenge models by exposing their tendency to
produce hallucinations. This choice is based on
the need to test the framework’s ability to ground
model’s explanation. We then evaluate five LLMs
both with and without the enhancements provided
byXplainLLM , allowing us to explore how our
framework performs across different scales and
architectures. The benchmarks for this evaluation
are focused on four key metrics: faithfulness, com-
pleteness, accuracy, and overall performance, as
shown in Table 5. We further quantified the impact
of our framework by comparing the accuracy rates
of the vanilla version to those enhanced with our
modifications, as detailed in Figure 5.
Our results show that performance variations
across different model architectures and config-
urations, as demonstrated in Table 5. Notably,
the GPT-4-turbo model, when enhanced with our
framework, demonstrates exceptional performance
across key metrics. It scores 4.05/5.00 in Faith-
fulness, 3.65/5.00 in Completeness, and 4.10/5.00
in Accuracy, culminating in an Overall score of
3.93/5.00. These high scores suggest that our
framework not only improves the overall output
quality but also ensures that the LLM’s reasoning
is grounded in faithful knowledge, thus enhanc-ing both the clarity and reliability of the model’s
behavior explanation.
We also observe a consistent improvement in
accuracy across different LLMs when our frame-
work is applied, as shown in Figure 5, which im-
plies a scalable utility of our framework. We find
the GPT-4-turbo model exhibits the most signifi-
cant improvement. This may suggest that our en-
hancements are effective in assisting more com-
plex LLMs to ground their reasoning in faithful
knowledge, thereby reducing hallucinations and
improving interpretability.
By comparing the detailed reasoning explanation
of the models with and without our framework, we
observe that the explanations generated under the
vanilla version tend to generate outputs that are not
entirely supported by input data (hallucinations).
In contrast, the explanations generated under the
XplainLLM version are more grounded in factual
content, and exhibit greater faithfulness.
We find our framework can guide the LLMs to-
ward a more grounded and data-driven approach
in generating outputs. This is helpful for applica-
tions where precision and reliability are paramount,
such as in legal, medical, or safety-critical environ-
ments. Furthermore, the consistent improvements
across LLMs of varying capabilities suggest that
our framework is robust and scalable, capable of en-
hancing a wide range of AI systems. This broad ap-
plicability suggests potential for widespread adop-
tion in enhancing the transparency and accountabil-
ity of AI decision-making processes.
6 Conclusion
We introduce XplainLLM : a knowledge-augmented
dataset paired with an explanation framework de-
signed to enhance the interpretability of LLMs.
Our dataset and framework provide a way for
LLMs to generate reliable and grounded expla-
nations without additional training. Through
the use of debugger-score , we provide a mul-
tidimensional analysis of quantitatively evalu-
ate the quality of explanations. Our eval-
uations demonstrate that XplainLLM not only
grounds explanations in reasoning behavior, but
also helps LLMs reduce hallucinations and im-
prove their performance. The dataset and
code are available at https://github.com/
chen-zichen/XplainLLM_dataset.git . We re-
lease them under the MIT license to encourage
further research in explainable AI.Limitation
Committed to transparency and rigorous analysis,
we acknowledge potential limitations in our dataset.
Since our reason-elements Ris originally derived
from ge, any inherent limitations or inaccuracies
within used KG could influence the quality of our
explanations.
Ethical Considerations
While XplainLLM and its accompanying explana-
tion framework provides advancements in the trans-
parency and accountability of LLMs, several risks
might exist. First, the reliance on KGs and struc-
tured data may lead to biases embedded in these
sources, potentially skewing the explanations. Sec-
ondly, incorrect knowledge augmentation could
mislead users about the accuracy of the explana-
tions. Additionally, there is a risk that users might
over-rely on the debugger-score without critical as-
sessment, potentially overlooking context-specific
inaccuracies. It is essential for future work to con-
tinuously refine XplainLLM , address detected bi-
ases, and enhance the robustness of the framework
to mitigate these risks.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet
Agrawal, Dinesh Khandelwal, Parag Singla, and Di-
nesh Garg. 2021. Explanations for commonsenseqa:
New dataset and models. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 3050–3065.
Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez,
Javier Del Ser, Adrien Bennetot, Siham Tabik, Al-
berto Barbado, Salvador García, Sergio Gil-López,
Daniel Molina, Richard Benjamins, et al. 2020. Ex-
plainable artificial intelligence (xai): Concepts, tax-
onomies, opportunities and challenges toward respon-
sible ai. Information fusion , 58:82–115.
Steven Bills, Nick Cammarata, Dan Moss-
ing, Henk Tillman, Leo Gao, Gabriel Goh,
Ilya Sutskever, Jan Leike, Jeff Wu, and
William Saunders. 2023. Language mod-
els can explain neurons in language models.
https://openaipublic.blob.core.windows.
net/neuron-explainer/paper/index.html .Faeze Brahman, Vered Shwartz, Rachel Rudinger, and
Yejin Choi. 2021. Learning to rationalize for non-
monotonic reasoning with distant supervision. Pro-
ceedings of the AAAI Conference on Artificial Intelli-
gence , 35(14):12592–12601.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Oana-Maria Camburu, Tim Rocktäschel, Thomas
Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-
ral language inference with natural language explana-
tions. In Advances in Neural Information Processing
Systems , volume 31. Curran Associates, Inc.
Giuseppe Casalicchio, Christoph Molnar, and Bernd
Bischl. 2019. Visualizing the feature importance for
black box models. In Machine Learning and Knowl-
edge Discovery in Databases: European Conference,
ECML PKDD 2018, Dublin, Ireland, September 10–
14, 2018, Proceedings, Part I 18 , pages 655–670.
Springer.
Saikat Chakraborty, Shuvendu Lahiri, Sarah Fakhoury,
Akash Lal, Madanlal Musuvathi, Aseem Rastogi,
Aditya Senthilnathan, Rahul Sharma, and Nikhil
Swamy. 2023. Ranking llm-generated loop invariants
for program verification. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2023 ,
pages 9164–9175.
Qianglong Chen, Feng Ji, Xiangji Zeng, Feng-Lin
Li, Ji Zhang, Haiqing Chen, and Yin Zhang. 2021.
KACE: Generating knowledge aware contrastive ex-
planations for natural language inference. In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th Inter-
national Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers) , pages 2516–2527,
Online. Association for Computational Linguistics.
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
Denny Zhou. 2023. Teaching large language models
to self-debug. arXiv preprint arXiv:2304.05128 .
Inyoung Cheong, King Xia, KJ Kevin Feng, Quan Ze
Chen, and Amy X Zhang. 2024. (a) i am not a lawyer,
but...: Engaging legal experts towards responsible llm
policies for legal advice. In The 2024 ACM Confer-
ence on Fairness, Accountability, and Transparency ,
pages 2454–2469.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D Manning. 2019. What does bert look
at? an analysis of bert’s attention. In Proceedings
of the 2019 ACL Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
276–286.
Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,
Eric Lehman, Caiming Xiong, Richard Socher, and
Byron C. Wallace. 2020. ERASER: A benchmark toevaluate rationalized NLP models. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 4443–4458, Online.
Association for Computational Linguistics.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-
ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui. 2022. A survey on in-context learning.
arXiv preprint arXiv:2301.00234 .
Akash Ghosh, Arkadeep Acharya, Raghav Jain, Sri-
parna Saha, Aman Chadha, and Setu Sinha. 2024.
Clipsyntel: clip and llm synergy for multimodal ques-
tion summarization in healthcare. In Proceedings of
the AAAI Conference on Artificial Intelligence , vol-
ume 38, pages 22031–22039.
Robert R Hoffman, Shane T Mueller, Gary Klein,
and Jordan Litman. 2018. Metrics for explain-
able ai: Challenges and prospects. arXiv preprint
arXiv:1812.04608 .
Shiyuan Huang, Siddarth Mamidanna, Shreedhar
Jangam, Yilun Zhou, and Leilani H Gilpin. 2023.
Can large language models explain themselves? a
study of llm-generated self-explanations. arXiv
preprint arXiv:2310.11207 .
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
Liang, Pete Florence, Andy Zeng, Jonathan Tomp-
son, Igor Mordatch, Yevgen Chebotar, et al. Inner
monologue: Embodied reasoning through planning
with language models. In 6th Annual Conference on
Robot Learning .
Naoya Inoue, Pontus Stenetorp, and Kentaro Inui. 2020.
R4C: A benchmark for evaluating RC systems to get
the right answer for the right reason. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 6740–6750, Online.
Association for Computational Linguistics.
Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel,
Yanai Elazar, Yejin Choi, and Yoav Goldberg. 2021.
Contrastive explanations for model interpretability.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1597–1611.
Harsh Jhamtani and Peter Clark. 2020. Learning to
explain: Datasets and models for identifying valid
reasoning chains in multihop question-answering. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 137–150, Online. Association for Computa-
tional Linguistics.
Albert Q Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris Bam-
ford, Devendra Singh Chaplot, Diego de las Casas,
Emma Bou Hanna, Florian Bressand, et al. 2024.
Mixtral of experts. arXiv preprint arXiv:2401.04088 .
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
Toutanova. 2019. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. In
Proceedings of NAACL-HLT , pages 4171–4186.Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan
Pei, Jinfeng Yi, and Bowen Zhou. 2023. Trustworthy
ai: From principles to practices. ACM Computing
Surveys , 55(9):1–46.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Computing Surveys , 55(9):1–35.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Scott M Lundberg and Su-In Lee. 2017. A unified ap-
proach to interpreting model predictions. Advances
in neural information processing systems , 30.
Andreas Madsen, Siva Reddy, and Sarath Chandar. 2022.
Post-hoc interpretability for neural nlp: A survey.
ACM Computing Surveys , 55(8):1–42.
Emanuele Musumeci, Michele Brienza, Vincenzo Suri-
ani, Daniele Nardi, and Domenico Daniele Bloisi.
2024. Llm based multi-agent generation of semi-
structured documents from semantic templates in the
public administration domain. In International Con-
ference on Human-Computer Interaction , pages 98–
117. Springer.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019a. Explain your-
self! leveraging language models for commonsense
reasoning. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 4932–4942, Florence, Italy. Association for
Computational Linguistics.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019b. Explain your-
self! leveraging language models for commonsense
reasoning. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 4932–4942.
Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. " why should i trust you?" explaining
the predictions of any classifier. In Proceedings of
the 22nd ACM SIGKDD international conference on
knowledge discovery and data mining , pages 1135–
1144.Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-
sky, Noah A. Smith, and Yejin Choi. 2020. Social
bias frames: Reasoning about social and power im-
plications of language. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 5477–5490, Online. Association
for Computational Linguistics.
Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.
Conceptnet 5.5: An open multilingual graph of gen-
eral knowledge. In Proceedings of the AAAI confer-
ence on artificial intelligence , volume 31.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. Commonsenseqa: A question
answering challenge targeting commonsense knowl-
edge. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4149–4158.
Sree Harsha Tanneru, Chirag Agarwal, and Himabindu
Lakkaraju. 2024. Quantifying uncertainty in natural
language explanations of large language models. In
International Conference on Artificial Intelligence
and Statistics , pages 1072–1080. PMLR.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Miles Turpin, Julian Michael, Ethan Perez, and Samuel
Bowman. 2024. Language models don’t always say
what they think: unfaithful explanations in chain-of-
thought prompting. Advances in Neural Information
Processing Systems , 36.
Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Liò, and Yoshua Bengio.
2018. Graph attention networks. In International
Conference on Learning Representations .
Boshi Wang, Xiang Yue, and Huan Sun. 2023. Can
chatgpt defend its belief in truth? evaluating llm
reasoning via debate. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
11865–11881.
Sarah Wiegreffe and Ana Marasovic. 2021. Teach me to
explain: A review of datasets for explainable natural
language processing. In Thirty-fifth Conference on
Neural Information Processing Systems Datasets and
Benchmarks Track (Round 1) .
Sarah Wiegreffe, Ana Marasovi ´c, and Noah A Smith.
2021. Measuring association between labels and
free-text rationales. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing , pages 10266–10284.Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi
Chaudhary, André F. T. Martins, and Graham Neu-
big. 2021. Do context-aware translation models pay
the right attention? In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 788–801, Online. Association
for Computational Linguistics.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-
man. 2022. STar: Bootstrapping reasoning with rea-
soning. In Advances in Neural Information Process-
ing Systems .
Xuan Zhang and Wei Gao. 2023. Towards llm-based
fact verification on news claims with a hierarchical
step-by-step prompting method. In Proceedings of
the 13th International Joint Conference on Natural
Language Processing and the 3rd Conference of the
Asia-Pacific Chapter of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
996–1011.A Graph Construction Algorithm
Algorithm 1: Sub-graph Construction
(PruneKG)
Data: Graph gwith nodes n, input content
QA, encoding function of LLM fenc,
MLP fnode
s, Number of top nodes to
select N
Result: Pruned graph ge
1begin
2 Initialize an empty list node_scores ;
3 foreach node ningdo
4 Obtain the embedding of n:
B ← fenc(n||QA);
5 Compute the relevance score of n:
si←sigmoid (fnode
s(B));
6 Append (n, si)tonode_scores ;
7 end
8 Sort node_scores in descending order
based on si;
9 Select the top Nnodes from the
node_scores list ;
10 Create a new graph gewith the selected
Lnodes, preserving their edges and
properties ;
11 return ge;
12end
B Details of Graph Attention Network
In section 3.1, we detail the method for interpreting
the LLM’s reasoning behavior through graph-based
techniques. We provide supplementary calcula-
tions and algorithmic details in this section.
We describe the process for updating the node
features in a graph using a GAT in Equation (4).
Here, each node iupdates its feature vector hk+1,i
based on the features of its neighboring nodes N(i).
fmis transformation function, modeled as a MLP,
that maps the input features hkj,ui, and rijinto
a higher-dimensional space. specifically, uiis the
one-hot vector encoding the type of node i, andrij
is the relation embedding denoting the relationship
type between nodes iandj, calculated by:
rij=fθ(i, uij) =fθ(i, ui∥uj), (6)
where uijis an one-hot vector encoding the type
of connection between nodes iandj, anduijis the
concatenation of iandj.CInstruction for Explanation Generation
Due to the space constraints, we provide detailed
guidelines and instructions for generating explana-
tions in this section.
Basis: Given a LM augmented with a graph
attention network to extract key reasoning elements
for decision-making. The task is [TASK_TYPE] .
Input: The question is: [Q]. The Answer Op-
tions are: [A]
Output: The model predicted choice [y′].
Based on the Ranked Reason-elements: [R]
Explanation (Stage 1): Explain the LM’s rea-
soning process for selecting [y′]over the other
options. Provide concise explanations for why
each reason-element supports [y′]as the predicted
choice. Focus on the LM’s behavior and the sig-
nificance of the Ranked Reason-elements. Your
response should be short and concise.
Explanation (Stage 2): Based on the [Ewhy],
explain why this LM makes the other options less
likely [A\ {y′}]. Your response should be short
and concise.
D Details of Debugger-Score
Thedebugger-score is a metric that quantifies the
quality of the explanations generated by the LLMs.
The score evaluates explanations based on multiple
dimensions such as faithfulness, accuracy, and com-
pleteness. By measuring how well the explanations
align with a “perfect” targeted LLM, the debugger
score provides a comprehensive evaluation of the
generated explanations. This metrics is useful for
ensuring that the explanations are not only plausi-
ble but also grounded in facts, enhancing trust of
explanations generated by LLMs. This instruction
assesses explanations based on three dimensions:
faithfulness, completeness, and accuracy.
D.1 Instructions for Debugger-score
Calculation
Prompt System: Evaluators, assuming the role
of LM debuggers with expertise in model parame-
ter changes, assess explanations from the perspec-
tive of how model parameters influence decision-
making. The assessment focuses on whether the
explanation accurately reflects the computational
and statistical mechanisms utilized by the LM.
Prompt Content: Evaluators are presented with
a task where the LM is augmented with key rea-
soning elements derived from its operation. Thisincludes the question, answer options, the LM’s
prediction, and the corresponding explanation.
Evaluation Criteria:
•Faithfulness: Does the explanation accu-
rately represent the underlying computational
processes and data-driven mechanisms used
by the LM to reach its conclusion?
•Completeness: Does the explanation encom-
pass all significant computational strategies
and data insights relied upon by the LM to
make the decision?
•Accuracy: How precisely does the explana-
tion reflect the true capabilities and decision-
making processes of the LM, considering its
design, training data, and functional algo-
rithms?
Scoring: Evaluators are instructed to score each
dimension on a scale from 1 to 5, where 1 indicates
the lowest level of adherence (poor) and 5 indicates
the highest (excellent). The scoring guide empha-
sizes balanced evaluation, advising against overly
strict judgments.
E Experiments
In this section, we describe the details of our evalu-
ation that were omitted in Section 5 due to space
constraints.
E.1 Model Parameters
To train our GNN, we use a dropout rate of 0.2, a
batch size of 64, and a learning rate of 1e-5, opti-
mized with RAdam. The model is fine-tuned on
a single NVIDIA A100 GPU for approximately
3 hours. Our KG containing 799,273 nodes and
2,487,810 edges. Our geis pruned based on KG to
retain 200 high-ranking nodes with a hop size of 2.
The GNN, specifically, consists of 200 dimensions
and 5 layers. The learning rate in our experiments
is 1e-3.
F Detailed Data Collection
Figure 6 shows the process of data collection:
(1) Given a question, we retrieve its relevant
knowledge using the KG. The retrieved graph is
then pruned based on scores influenced by the
LLM, resulting in what we term the element-graph .
Theelement-graph is processed by a specialized
GAT model (known formally as Decision Inter-
pretation). Leveraging attention mechanisms, we
1⃣ Model Decision-making Process Input [] A person writes a check to a clerk, where does the clerk put them?                            [] A. reading B. meditate C. fall asleep D. bunk E. thinkqaLLM + GAT Model
2⃣ Process Interpretation
AnswerRanked  reason-elements
3⃣ Controlled Explanation Generation Stage 1: why-choose Explanation   Stage 2: why-not-choose Explanation  
QuestionAnswer
Ranked REs
Instruction
why-choose
Stage 1 Elements
Instruction(Stage 1 Output)
4⃣ RefinementHuman evaluations
Figure 6: Data Collection Process.
obtain the essential representations for interpreta-
tion.
(2) The model’s decision-making is interpreted
through the ranked reason-elements and the pre-
dicted answer.
(3) A controllable dual-stage process generates the
explanations: Stage 1: The initial phase focuses on
generating the “why-choose" explanations. Stage
2: Building upon the outputs and elements of Stage
1, we then generate the “why-not-choose" explana-
tions.
(4) We conduct a human evaluation to identify er-
rors in the explanations. If discrepancies arise,
explanations are reverted to Step 3 for refinement.
This process not only helps prevent potential is-
sues arising from bad explanations generated by
the LLM but also maintains human-aligned clarity
and relevance.
G Instance Example
We present an example from our dataset to illustrate
the data instance.
Question: John carred for Lucy but had trou-
ble expressing it. Lucy was disturbed by John’s
inability to express affection and felt that he was
what?
Answers: A. being mean, B. negligence, C. dis-
interest, D. misunderstood, E. unfeeling
Label: unfeeling
Predicted Label Index: 4
Label Matched: true
Reason-elements: "enraged", "delay", "abio-
genesis", "sneerer", "helpable", "begrudge", "mol-
lify" ...
Top-5 Reason-elements: "enraged", "delay",
"abiogenesis", "sneerer", "helpable"
Explanation (Why): The model selected “un-
feeling” primarily due to how it processed the
emotional descriptors in the scenario and its un-
derstanding of the term definitions. Though thekey reason-elements like ’enraged’, ’delay’, ’abio-
genesis’, ’sneerer’, and ’helpable’ seem unrelated
at first, they could connect to various aspects of
emotional disconnect and misunderstanding. ’En-
raged’ hints at strong emotional responses, albeit
negative, which possibly led the model to associate
less expressed emotions to being “unfeeling”. The
term ’sneerer’, indicating derision, might be linked
by the model to John’s perceived dispassion. The
word ’helpable’ suggests a situation needing im-
provement, reflecting on Lucy’s feelings towards
John’s emotional expression deficits.
Explanation (Why-Not): 1. “being mean”: The
context didn’t explicitly describe John’s behavior
as intentionally harmful or spiteful, which ’being
mean’ would imply. The reason-elements don’t
directly associate with malicious intent. 2. “neg-
ligence”: Although John’s behavior could be in-
terpreted as neglectful, the elements like ’enraged’
do not support an outright disregard, but rather
an emotional complexity. 3. “disinterest”: This
choice could somewhat fit, but the model likely
found stronger connections in emotional response
terms suggesting not just a lack of interest but a lack
of feeling. 4. “misunderstood”: While the context
and reason-elements like ’sneerer’ might suggest
misunderstandings, ’unfeeling’ directly refers to a
perceived absence of emotion which seemed more
fitting to Lucy’s disturbance.
debugger-score: Faithfulness: 4 | Completeness:
3 | Accuracy: 4
The format of our dataset is as follows:
Data Schema
question: typeof(string)
answers: typeof(list_of_strings)
label: typeof(string)
predicted_label: typeof(string)
label_matched: typeof(boolean)
concept: typeof(list_of_strings)
topk: typeof(list_of_strings)
explanation_why: typeof(string)
explanation_why_not: typeof(string)
debugger_score: typeof(string)
embedding: typeof(list_of_floats)
H Explanation Statistics
Figure 7 is a word cloud showing the most fre-
quently appearing words in the why-choose expla-
nations. From this figure, we have a clear indication
thatwhy-choose explanations focus on explaining,
Figure 7: why-choose explanations.
Figure 8: why-not-choose explanations.
comprehension, and interpreting predictions made
by the target model.
Figure 8 presents a word cloud for why-not-
choose explanations. We note that these explana-
tions outline the reasons behind the non-selection
of specific options as predicted answers. Further-
more, why-not-choose explanations emphasize how
the target model determines the likelihood of differ-
ent answer choices. We also observe that the target
model handles a wide array of topics, which can
be crucial components in the “why not” reasoning
process.
I Evaluation Materials
I.1 Questions and Evaluation Instructions
For each instance, we include a set of question, an-
swer choices, model prediction, and explanation.
To evaluate the quality of the explanation, we pro-
vide seven questions for evaluators. Each question
includes three score levels: 1 for disagree, 2 for
neutral, and 3 for agree. The questions and instruc-
tions in our evaluation are as follows:
Q0: This is a good explanation
1.Disagree : The explanation is illogical or in-
consistent with the question and/or does not ade-
quately cover the answer choices.
2.Neutral : The explanation is somewhat logical
and consistent with the question but might miss
some aspects of the answer choices.
3.Agree : The explanation is logical, consistentwith the question, and adequately covers the answer
choices.
Q1: I understand this explanation of how the
AI model works.
1.Disagree : The explanation is unclear or
contains overly complex terms or convoluted sen-
tences.
2.Neutral : The explanation is somewhat un-
derstandable but might contain complex terms or
convoluted sentences.
3.Agree : The explanation is clear, concise, and
easy to understand.
Q2: I trust this explanation of how the AI
model works.
1.Disagree : The explanation is unclear or
contains overly complex terms or convoluted sen-
tences.
2.Neutral : The explanation is somewhat credi-
ble but contains some elements that I find doubtful
or questionable.
3.Agree : The explanation is credible and aligns
with my understanding of how AI models work.
Q3: This explanation of how the AI model
works is satisfying.
1.Disagree : The explanation does not meet
my expectations and leaves many questions unan-
swered.
2.Neutral : The explanation somewhat meets
my expectations but leaves some questions unan-
swered.
3.Agree : The explanation meets my expecta-
tions and satisfies my query.
Q4: This explanation of how the AI model
works has sufficient detail.
1.Disagree : The explanation lacks detail and
does not adequately cover the AI model’s decision-
making.
2.Neutral : The explanation provides some
detail but lacks thoroughness in covering the AI
model’s decision-making.
3.Agree : The explanation is thorough and cov-
ers all aspects of the AI model’s decision-making.
Q5: This explanation of how the AI model
works seems complete.
1.Disagree : The explanation does not ade-
quately cover the answer choices and leaves many
aspects unexplained.
2.Neutral : The explanation covers most answer
choices but leaves some aspects unexplained.
3.Agree : The explanation covers all answer
choices and leaves no aspect unexplained.Q6: This explanation of how the AI model
works is accurate.
1.Disagree : The explanation does not accu-
rately reflect the AI model’s decision-making.
2.Neutral : The explanation somewhat reflects
the AI model’s decision-making but contains some
inaccuracies.
3.Agree : The explanation accurately reflects
the AI model’s decision-making.
I.2 Human-centered Metrics for Explanation
Quality Evaluation
The meaning of metrics used in the human-centered
evaluation are as follows:
1.Overall quality reflects the overall effective-
ness of explainability. It reveals how effectively
explanations convey the decision-making pro-
cess of the AI models to the human users.
2.Understandability evaluates how well a human
can comprehend the model’s output and expla-
nations. It captures the clarity and coherence of
the generated text.
3.Trustworthiness measures the human evalua-
tor’s confidence in the model’s outputs and ex-
planations. It evaluates whether the explana-
tions appear reliable, credible, and based on
sound reasoning.
4.Satisfaction captures the overall contentment of
the evaluator with the explanations. It measures
whether the outputs meet the evaluator’s needs
and expectations in terms of quality, relevance,
and utility.
5.Sufficiency of detail evaluates whether the ex-
planations provide a sufficient level of detail. It
evaluates whether the responses are adequately
descriptive and provide all necessary informa-
tion to fully answer the question or task.
6.Completeness measures whether the explana-
tions address the decision behaviors of the
model.
7.While we also measure accuracy objectively,
the human evaluation of accuracy assesses
whether the explanations align with the eval-
uator’s knowledge or expectations. It mea-
sures whether the explanations can reflect if the
model’s outputs are factually correct and con-
textually appropriate.