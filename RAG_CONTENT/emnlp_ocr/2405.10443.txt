Simultaneous Masking, Not Prompting Optimization:
A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation
Matthew Raffel Victor Agostinelli Lizhong Chen
Oregon State University
{raffelm, agostinv, chenliz}@oregonstate.edu
Abstract
Large language models (LLMs) have achieved
state-of-the-art performance in various lan-
guage processing tasks, motivating their adop-
tion in simultaneous translation. Current fine-
tuning methods to adapt LLMs for simultane-
ous translation focus on prompting optimiza-
tion strategies using either data augmentation
or prompt structure modifications. However,
these methods suffer from several issues, such
as unnecessarily expanded training sets, compu-
tational inefficiency from dumping the key and
value cache, increased prompt sizes, or restric-
tion to a single decision policy. To eliminate
these issues, in this work, we propose Simul-
Mask , a new paradigm for fine-tuning LLMs
for simultaneous translation. It utilizes a novel
attention mask approach that models simultane-
ous translation during fine-tuning by masking
attention for a desired decision policy. Apply-
ing the proposed SimulMask on a Falcon LLM
for the IWSLT 2017 dataset, we have observed
a significant translation quality improvement
compared to state-of-the-art prompting opti-
mization strategies on five language pairs while
reducing the computational cost.
1 Introduction
Simultaneous translation refers to the process of
producing a target output translation concurrently
with an oncoming source input. In our increas-
ingly interconnected world, where communication
across languages in real-time is desired, simulta-
neous translation is becoming a requirement. As
such, there is a need for machine learning models
to fill this role.
Current literature has primarily focused on adapt-
ing end-to-end Transformer models (Vaswani et al.,
2017) to overcome the difficulties of simultane-
ous machine translation (SimulMT) due to their
reduced parameter counts and greater inference
speed(Ma et al., 2020b). However, the recent suc-
cesses of large language models (LLMs) (Touvronet al., 2023; Jiang et al., 2023; Almazrouei et al.,
2023) has prompted preliminary research apply-
ing them to SimulMT through fine-tuning and in-
ference techniques (Agostinelli et al., 2023; Wang
et al., 2023; Koshkin et al., 2024; Wang et al., 2024;
Guo et al., 2024). Unfortunately, most modern
works have neglected the computational increases
created by dumping the target sequence’s key and
value (KV) cache (Wang et al., 2024). Furthermore,
there has yet to be a universal approach to fine-
tuning LLMs for SimulMT that is not unnecessar-
ily computationally expensive by either expanding
the dataset through data augmentation, a process
referred to as prefix fine-tuning (Agostinelli et al.,
2023; Wang et al., 2023; Koshkin et al., 2024) or in-
creasing the prompt length through prompt restruc-
turing (Koshkin et al., 2024; Wang et al., 2024).
The lack of an efficient fine-tuning strategy of
LLMs for SimulMT has led us to propose a new
paradigm, referred to as SimulMask . SimulMask
is a novel attention mask to model SimulMT dur-
ing fine-tuning by redistributing the attention un-
der a decision policy. By design, SimulMask is
broadly applicable to both flexible and fixed de-
cision policies, creating a path forward for fu-
ture work to build upon it. Furthermore, if we
avoid injecting positional information into the keys
and values through a modified ALiBi (Press et al.,
2021), SimulMask allows for KV caching during
SimulMT without accuracy degradation.
To validate the efficacy of SimulMask, we fine-
tuned and evaluated 1.3 billion parameter Falcon
models pre-trained on the RefinedWeb dataset us-
ing SimulMask (Almazrouei et al., 2023; Penedo
et al., 2023) and compared them against identical
Falcon models that adopt existing prefix fine-tuning
or prompt restructuring methods on the IWSLT
2017 dataset (Cettolo et al., 2017). From the results,
we demonstrate models fine-tuned with SimulMask
outperform prefix fine-tuning and prompt restruc-
turing models at SimulMT for a given latencyarXiv:2405.10443v4  [cs.CL]  9 Oct 2024regime with a reduced computational cost.
The main contributions of the paper include:
1.Providing insights on the shortcomings of cur-
rent methods in adapting LLMs to SimulMT.
2.Proposing a novel attention masking approach
to fine-tune SimulMT LLMs that enables effi-
cient training and inference.
3.Demonstrating the efficacy of our approach in
terms of translation quality and computational
costs by evaluating them on multiple language
pairs across varied latencies.
2 Background and Related Work
2.1 Masked Transformer Self-Attention
We briefly review self-attention functionality in
Transformers (Vaswani et al., 2017) focusing on
masking behavior in Equation 1. Mis defined as
an optional attention mask.
A=softmaxQKT+M√dhead
V (1)
Critically, Mfunctions by modeling context
limitations or time-based dependencies that might
exist during inference but do not exist during
training/fine-tuning. For generative Transformer
blocks or architectures, Mis defined as a causal
attention mask, where each entry, Mij, is repre-
sented by Equation 2 to avoid attending to the fu-
ture.
Mij=(
0, ifj≤i
−∞,otherwise(2)
2.2 Simultaneous Translation
SimulMT is dictated by read-write decision poli-
cies, whereby a model will wait a specific amount
of time before alternating between reading and writ-
ing in fixed or flexible intervals. One fixed decision
policy for SimulMT that is broadly adopted as a
common baseline to build on due to its effective-
ness and simplicity is the wait-k policy (Ma et al.,
2019). As the name suggests, the wait-k policy
will wait for k words before alternating between
writing and reading a word. Although effective in
SimulMT, alternative adaptive policies have gained
traction, which base reading and writing on an aux-
iliary model or a predefined set of rules (Cho and
Esipova, 2016; Gu et al., 2017; Zheng et al., 2019).
While capable of impressive results, such adaptive
policies often incur additional computational costs.
A Transformer is trained for a SimulMT decision
policy by masking attention scores in the encoderself-attention and the decoder cross-attention. In
the case of the encoder self-attention, each source
token is prevented from attending to future source
tokens following a decision policy (Ma et al., 2019).
An example mask is provided in Appendix A.1.
Alternatively, in decoder cross-attention, each
target token is prevented from attending to future
source hidden states following the decision policy
(Papi et al., 2022a). Equation 3 expresses each
entry of the decoder cross-attention mask, Mtj.
Mtj=(
0, ifj≤f(t)
−∞,otherwise(3)
In Equation 3, f(t), is a decision policy function
that denotes the cumulative number of source hid-
den states to read when predicting target token t.
2.3 Applying LLMs to SimulMT
LLMs have demonstrated remarkable performance
on neural machine translation (NMT) (Moslem
et al., 2023; Vilar et al., 2023; Xu et al., 2023;
Zhang et al., 2023; Iyer et al., 2023). Such
successes have prompted recent works to extend
the reach of LLMs into the realm of SimulMT
(Agostinelli et al., 2023; Wang et al., 2023; Koshkin
et al., 2024; Wang et al., 2024; Guo et al., 2024).
LLMs are especially promising for the field of
SimulMT due to their strong understanding of
language semantics and meaning. Intuitively,
SimulMT LLMs inject holistic linguistic knowl-
edge that could allow for correct translation de-
cisions when facing difficult contextual obstacles
(e.g., translating a verb in a target language without
access to that verb in the source language).
Unfortunately, Equation 3 is no longer effective
in modeling SimulMT for decoder-only LLMs as
with the decoder of a classical Transformer. The
reason is that Equation 3 is constructed specifically
for the cross-attention calculation between keys ex-
clusively from the source and queries exclusively
from the target, as in Transformers. In contrast,
LLMs perform self-attentions involving the prompt,
the source, and the target concurrently. Equation 3
can no longer properly mask the source (keys) from
the target (queries), due to the additional prompt
and target sequences in the keys and the additional
prompt and source sequences in the queries. Fur-
thermore, Equation 3 does not enforce the autore-
gressive language modeling behavior of LLMs. As
such, alternative means to model SimulMT have
been proposed, leveraging prompting optimization.3 Prompting Optimization Methods
Current methods of fine-tuning LLMs for SimulMT
fall under prompting optimization. We define
prompting optimization as either employing data
augmentation to help with prompting (Koshkin
et al., 2024; Wang et al., 2023; Agostinelli et al.,
2023) or redefining the prompt structure (Wang
et al., 2024; Koshkin et al., 2024) to somewhat
simulate SimulMT.
3.1 Data Augmentation
Prompting optimization focusing on data aug-
mentation resorts to subdividing each sentence
in a dataset into multiple partial sentence pairs.
These partial sentence pairs mimic SimulMT, as
SimulMT produces outputs with a partial input.
We label such a method as prefix fine-tuning ,
and although the high-level procedure is identical
amongst current works, the algorithms employed
to obtain these partial sentence pairs are unique. In
the case of Agostinelli et al. (2023), each source-
target sentence pair is subdivided according to the
wait-k policy such that if we order the new sam-
ples from smallest to largest, each subsequent sen-
tence pair will have one additional target word and
source word so long as the end of the target or
source is not reached. Upon completion there will
bemax(|S| −(k−1),|T|)sentence pairs, where
|S|and|T|are the original source and target se-
quence lengths. The approach requires the model
to predict only the final target word in the sequence
during fine-tuning.
Alternatively, Wang et al. (2023) randomly sam-
pled a subset of sentence pairs from the dataset and
truncated the source sentence to be 20% to 80% of
the full length according to a uniform distribution.
They obtained the respective target translations by
prompting ChatGPT (gpt-3.5-turbo). The new trun-
cated source-target sentence pairs were then added
to the complete dataset, expanding it.
3.2 Prompt Restructuring
Prompting optimization that modifies the prompt-
ing structure adjusts the prompt to include the de-
cision policy. In the case of Wang et al. (2024), a
conversational prompting structure is adopted for
the LLM, alternating between source and target sub-
sequences of the original complete sequences using
delimiting tokens to separate regions. For instance,
if we have the source sequence S= [s1, s2, ..., s n]
and the target sequence T= [t1, t2, ..., t m], thenone potential conversational prompt expansion
could be “ <s>,[U], s1, s2,[A], t1, t2,</s>, ...,
<s>,[U], sn,[A], tm,</s> ”, where the added <s>,
</s>, [A], [U] are delimiting tokens. During
fine-tuning, the choice of alternating subsequences
is arrived at by attempting to maximize the rele-
vant source context before each target sequence
in the form of an oracle decision policy. For in-
stance, the prompt will ensure an arbitrary target
verb prediction only after the respective source verb
is read. Some minor perturbations are added to the
oracle decision policy to improve generalizability.
Then, at inference, a prompt constructor provides
the source sequence in fixed-size chunks.
Similarly, Koshkin et al. (2024) leverages
prompt restructuring; however, it also employs
prefix finetuning. Like the conversational prompt-
ing structure, it constructs a fine-tuning prompt
by aligning words between the source and target
sequence to mimic an oracle decision policy. How-
ever, it deviates from conversational prompting by
ensuring the alignment using padding tokens in
the target sequence. Then, the causally aligned
sentence prompt is subdivided using a prefix fine-
tuning strategy to expand the dataset with partially
filled source-target sentence pairs. At inference,
the LLM contains the decision policy outputting
padding tokens whenever it requires more source
context tokens.
4 Analysis and Shortcomings of
Prompting Optimization Methods
Prompting optimization, while functional to a cer-
tain degree, is inherently deficient, possessing a
host of fine-tuning and inference issues. These is-
sues include a persistent fine-tuning-inference mis-
match, consistent positional confusion in the target
sequence, and high computational costs.
4.1 Fine-tuning/Inference Mismatch
A fine-tuning-inference mismatch is a mismatch
between a LLM’s fine-tuning and inference envi-
ronments. For instance, fine-tuning a LLM for
NMT where the entire sentence is available and de-
ploying it for SimulMT where little of the sentence
is available when beginning generation will cre-
ate a massive inference time fine-tuning-inference
mismatch. Furthermore, the LLM must be fine-
tuned to accommodate KV caching, the process of
caching the keys and values at inference to prevent
recomputation. Overall, fine-tuning for SimulMTaims to minimize the fine-tuning-inference mis-
match.
Unfortunately, prefix fine-tuning precludes high-
quality SimulMT with KV caching (Agostinelli
et al., 2023; Koshkin et al., 2024; Wang et al., 2023)
as with the continuously increasing prompt size,
each key and value in the KV cache deviates more
from the keys and values in its fine-tuning environ-
ment. For example, suppose we have a LLM in the
middle of SimulMT using KV caching adhering to
a wait-1 policy with the following prompting struc-
ture: “ Translate the following sentence:
s1, s2, ..., s i+1[a]: t1, t2, ..., t i”. Then, at the
current write step, the query of tiattends to the
KV cache for [a]:, t1, t2, ..., t i−1. By construc-
tion, each key and value in the KV cache was
generated in a previous time step with a different
subset of the source sequence s1, s2, ..., s i. For in-
stance, the keys and values for delimiting token
[a]: when it predicted t1were conditioned only
ons1, whereas the keys and values for ti−1when
it predicted tiwere conditioned on s1, s2, ..., s i.
However, during prefix fine-tuning, the LLM was
fine-tuned to predict ti+1as if the KV cache for
[a]:, t1, t2, ..., t i−1were each generated with the
same subset of the source sequence s1, s2, ..., s i.
Such fine-tuning-inference mismatch is unsolved
through conventional prompting structures.
Prompting restructuring also creates additional
fine-tuning-inference mismatches. In Koshkin et al.
(2024) and Wang et al. (2024), they all fine-tune for
an oracle decision policy. However, at inference,
such an oracle decision policy is not truly achiev-
able, creating a mismatch. Furthermore, since the
LLMs that leverage prompt restructuring encapsu-
late a specific oracle decision policy into their fine-
tuning curriculum, extending them to alternative
decision policies at inference is infeasible without
incurring a mismatch. This calls for a new, flexible
method adaptable to a range of decision policies
while also eliminating the fine-tuning-inference
mismatch.
4.2 Positional Confusion
Positional confusion describes the process whereby
the relative and/or global positional information
during SimulMT progressively becomes incor-
rect. Unfortunately, most SimulMT LLMs using
KV caching suffer from this positional confusion
(Agostinelli et al., 2023; Koshkin et al., 2024; Wang
et al., 2023). The reason is that as the source se-
quence grows with SimulMT, the target sequencealso shifts, necessitating the target sequence’s posi-
tional information to follow suit. However, since
KV caching is employed, the positional informa-
tion held in the keys and values is not properly
updated.
Aligning with our previous example, for the se-
quence portion “ [a]: t1, t2, ..., t i”, after the first
prediction step, the positional distance between s1
and[a]: and between s1andt1would be 1 and 2,
respectively. Then, after the second read, where the
source sequence is s1, s2, the positional distance
between s1and[a]: andt1would change to 2 and
3, respectively. However, while using KV caching,
this positional distance would remain 1 and 2 in
the keys and/or values for subsequent predictions,
causing positional confusion. Continuing transla-
tion would see the increased gap between the true
positional distance and the stale positional distance
in the KV cache. Therefore, we need to identify an
effective method to deal with positional confusion
that is essential to prevent LLM hallucinations.
4.3 Computational Inefficiency
Avoiding KV caching and instead recomputing
all the keys and values at each prediction step is
the default solution for resolving the aforemen-
tioned fine-tuning-inference mismatch and posi-
tional confusion problems while employing prefix
fine-tuning. Although effective from a translation
quality standpoint, doing so incurs a large compu-
tational cost, an undesirable result for streaming
tasks like SimulMT, where latency is equally im-
portant.
Outside of KV caching, the computational costs
necessary for prefix fine-tuning methods are exces-
sive. By subdividing each sample into multiple, the
dataset drastically expands, contributing toward an
increased cost to complete each epoch (Agostinelli
et al., 2023; Koshkin et al., 2024; Wang et al., 2023).
Such an increase causes the duration of each epoch
to rise by upwards of a factor of 10 (as exemplified
in Table 1 in Section 7.2). However, unlike nor-
mal methods of expanding a dataset through data
augmentation, prefix fine-tuning does not actually
add additional information. It is from this added
computational burden that Agostinelli et al. (2023)
and Wang et al. (2023) are forced to fine-tune with
a subset of their entire prefix datasets.
Alternatively, methods of restructuring the
prompt as in Koshkin et al. (2024) and Wang et al.
(2024) have computational burdens of their own.
For instance, Wang et al. (2024) requires addingdelimiting tokens in the prompt sequence, expand-
ing the sequence length. Similarly, the requirement
of padding tokens to induce a causal alignment
between the source and target sequences, as in
Koshkin et al. (2024), also expands the sequence
length. Since the computational cost of the self-
attention cost in the LLM scales quadratically with
the sequence length, such a method is undesirable
for both inference and fine-tuning.
Currently, no computationally efficient fine-
tuning approach exists that enables computation-
ally efficient inference. Identifying such a method
is necessitated by the desire for low latency and
high-quality translations and reducing the already
high computational costs of fine-tuning LLMs.
5 SimulMask: A Paradigm Shift
In this work, we propose SimulMask , which we
believe could be a paradigm shift in fine-tuning
LLMs for SimulMT that eschews current methods
of prompting optimization. By restricting attention
during fine-tuning, SimulMask efficiently solves
the fine-tuning-inference mismatch and positional
confusion problem. We demonstrate SimulMask
through its application for the wait-k decision pol-
icy, but it should be noted that SimulMask broadly
applies to various decision policies.
5.1 Inference Mirrored Attention
We first introduce the concept of Inference Mir-
rored Attention that cleverly models SimulMT dur-
ing LLM fine-tuning. Under SimulMT, the latest
translation token at each prediction step is con-
ditioned only on the running source tokens. For
conventional Transformers, specialized attention
masks could achieve such conditioning; however,
directly mapping these to LLMs is impossible since
they fail to enforce autoregressive language mod-
eling and cannot mask properly when the prompt,
source, and target sequences are collectively in-
cluded in the queries and keys. As such, prior
works attempted to achieve such conditioning dur-
ing fine-tuning using prompting optimization strate-
gies littered with shortcomings. The proposed
inference mirrored attention is aimed to model
SimulMT with attention masks for LLMs by mir-
roring the attention during inference at fine-tuning
according to the chosen decision policy.
As an example, suppose we model the at-
tention for a wait-1 decision policy where
the complete oracle input sequence is
(a) Inference.
 (b) Fine-tuning.
Figure 1: Inference Mirror Attention for matching atten-
tion during inference and fine-tuning for SimulMT.
“p1, s1, s2, s3, s4, p2, t1, t2, t3, t4”. In the se-
quence, s1, s2, s3, s4andt1, t2, t3, t4are the
4-word source and target sequences and p1and
p2are prompting regions. Then, at inference, by
definition of the wait-1 policy, p2predicts t1while
conditioned on the partial sequence p1, s1, p2. As
such, as shown in Figure 1a the query of p2attends
to the keys of p1, s1, p2. Thus, during fine-tuning,
to eliminate the fine-tuning-inference mismatch,
the query of p2should be limited to similarly
attend to the keys of p1, s1, p2as shown in Figure
1b rather than the entire source sequence. For
each successive prediction step, the previously
predicted target word, ti, predicts the next target
word, ti+1by conditioning on an extra source
word, si+1, acquired from the previous read step.
To mimic such behavior at fine-tuning, the query
fortiattends to identical keys as its inference
step. The complete steps of this example are in
Appendix A.2.
5.2 SimulMask
To achieve the above inference mirrored attention,
we opt for an attention mask to restrict attention dur-
ing fine-tuning to mimic an arbitrary decision pol-
icy during SimulMT. An attention mask is prefer-
able to prompting optimization as it is flexible and
directly extends the LLM causal attention mask.
We call such an attention mask SimulMask.
As a demonstration, let us create a SimulMask
for the wait-1 policy that extends our example from
Section 5.1. As depicted in Figure 2, since the LLM
is autoregressive, SimulMask begins with a causal
attention mask from which attention is limited to be
identical to the attention during SimulMT for the
source sequence. Starting from the prompt p2, from
the example in Figure 1b, p2generates the first tar-
get token, t1, conditioned on p1, s1, p2. As such,
SimulMask eliminates the attention between p2
ands2, s3, s4. Similarly, t1andt2are conditioned
onp1, s1, s2, p2, t1andp1, s1, s2, s3, p2, t1, t2, re-
spectively. Thus, attention is eliminated between
t1ands3, s4andt2ands4.Figure 2: SimulMask for modeling SimulMT according
to a wait-1 decision policy during fine-tuning.
SimulMask is a flexible scheme that supports a
range of decision policies. Since each decision
policy performs read/write decisions differently
and each limits attention differently, this requires a
unique attention mask for every sentence. However,
this can be done straightforwardly. The general pro-
cedure to construct a SimulMask for a given policy
and sentence consists of the following steps:
1.Construct a causal attention mask using Equa-
tion 2 as a starting point for SimulMask.
2.Starting from the intersection between the
query that predicts the first target token and
the first source key, apply the sub-attention
mask expressed in Equation 3. The sub-
attention mask prevents the target queries
from attending to source keys following the
arbitrary decision policy.
3.Mask any non-source queries before the query
predicting the first target token from attending
to the source keys not included in the first read
decision. Such a step is necessary to prevent
the hidden states associated with these queries
from holding information of the entire source
sequence at later layers in the LLM.
As reported in Section 7.2, the computation for
constructing an arbitrary SimulMask is minor, and
since SimulMask is not applied during inference,
it does not impact computational cost at deploy-
ment. Therefore, SimulMask is an efficient option
for mimicking SimulMT during fine-tuning and
providing low-latency translations at inference.
5.3 Positional Reordering
Since positional confusion during inference is a
byproduct of retaining outdated positional infor-
mation in either the keys or values, bypassing it
(a) Original ALiBi.
 (b) Modified ALiBi.
Figure 3: ALiBi biases with SimulMask.
requires providing a form of positional informa-
tion without injecting it directly into the sequence
or KV cache. One positioning method that satis-
fies such a constraint is the popular ALiBi, which
supplies positional information through biases in
attention (Press et al., 2021). The bias is applied
to each query-key dot product row in the attention
calculation as shown in Equation 4, where mis a
head-specific constant.
qiKT+Mi+m·[−(i−1), ...,−1,0] (4)
Though simple, ALiBi has demonstrated an ability
to extrapolate to much larger sequence lengths than
other state-of-the-art positional encodings, making
it desirable for LLMs like Falcon (Penedo et al.,
2023), BLOOM (Le Scao et al., 2023), etc.
Unfortunately, ALiBi, by default, does not mesh
with SimulMask as SimulMask removes attention
between the target queries and source keys. This
removed attention creates a gap in ALiBi biases
during fine-tuning that are not present at inference.
An example of such a gap is provided in Figure
3a, where both q4andq5have gaps in the position
distance.
To eliminate the bias gap, we modify ALiBi by
reducing the bias values of all query rows influ-
enced by SimulMask. For each query row, the
reduction in bias values is equivalent to the amount
of attention removed along the row using Simul-
Mask. Figure 3b provides an example of such a
modification. In the case of q4, it is no longer able
to attend to k2andk3; therefore, the bias on the
right of the gap is reduced by 2. Together with the
modified ALiBi, SimulMask eliminates positional
confusion from the LLM during SimulMT.
6 Experimental Setup
6.1 Fine-tuning
Our fine-tuning was conducted with the Simul-
LLM framework (Agostinelli et al., 2023), which(a) English-French language pair results.
 (b) English-Dutch language pair results.
(c) English-Italian language pair results.
 (d) English-Romanian language pair results.
(e) English-German language pair results.
Figure 4: Translation quality plotted against latency for LLMs on the English-French, English-Dutch, English-
Romanian, and English-German language pairs.
contains our publicly available code for Simul-
Mask1. Each experiment used a 1.3 billion pa-
rameter Falcon model pre-trained on the Refined-
Web dataset (Penedo et al., 2023). We compared 7
schemes:
•causal-offline : Fine-tuned with a causal at-
tention mask and evaluated for NMT (non-
SimulMT).
•causal-rec : Fined-tuned with a causal atten-
tion mask and evaluated with recomputing the
KV cache.
•prefix-rec and prefix-norec : Fined-tuned
with prefix fine-tuning and evaluated
with/without recomputing the KV cache.
•converse-norec : Fined-tuned with a conversa-
tional prompting strategy and evaluated with-
out recomputing the KV cache.
•SM-norec-mod andSM-norec : Fined-tuned
1https://github.com/OSU-STARLAB/Simul-LLMwith SimulMask with/without modifying AL-
iBi and evaluated without recomputing the
KV cache.
Appendix A.3 provides all model hyperparameters.
Our fine-tuning experiments included the English-
French (en-fr), English-Italian (en-it), English-
Dutch (en-nl), English-Romanian (en-ro), and
English-German (en-de) language pairs of the
IWSLT 2017 dataset (Cettolo et al., 2017).
6.2 Evaluation
We evaluated translation quality and latency for
SimulMT using Simul-LLM inference agents
(Agostinelli et al., 2023) interfacing with the
SimulEval toolkit (Ma et al., 2020a). The trans-
lation quality was determined using detokenized
BLEU with SacreBLEU (Post, 2018) and chrF++
(Popovi ´c, 2017). Latency was determined using
Length-Adaptive Average Lagging (LAAL) (Papiet al., 2022b). The computational cost of SimulMT
was recorded with GFLOPs. All metrics were ob-
tained on a single A10 GPU with bfloat16 precision.
The models fine-tuned for the wait-k policy were
evaluated at a wait-k four lower, for which they
were fine-tuned, as suggested by (Ma et al., 2019).
7 Results
7.1 Translation Quality and Latency Results
In this section, we demonstrate the efficacy of fine-
tuning with the proposed SimulMask compared
with other schemes using BLEU scores and LAAL.
All wait-k model evaluations are performed across
wait-{1,3,5,7}, and the converse-norec is evaluated
for a chunk size of 1, 3, 5, 7, 9, and 11. Figure
4 provides the BLEU translation quality and la-
tency results on the English-French, English-Dutch,
English-Italian, English-Romanian, and English-
German language pairs. We provide the numerical
BLEU and chrF++ translation quality results in
Tables 3 and 4 of Appendix A.4.
Overall, throughout Figure 4, the proposed SM-
norec-mod outperforms or matches the translation
quality of causal-rec ,prefix-rec , and converse-
norec across all latencies. The only major excep-
tion occurs at wait-1, where converse-norec out-
performs SM-norec-mod on the English-Romanian
language pair. This overall excellent performance
in terms of translation quality underscores the im-
portance of the proposed method.
Furthermore, Figure 4 provides two ablation
studies. The first ablation demonstrates the im-
portance of modifying ALiBi with SimulMask for
high-quality translations by comparing SM-norec-
mod with SM-norec . For each wait-k value and lan-
guage pair, SM-norec-mod outperforms SM-norec .
Unsurprisingly, at higher wait-k values where the
setting approaches NMT, the difference in BLEU
scores becomes less pronounced between the mod-
els.
A secondary ablation is provided in Figure 4
by comparing prefix-rec andprefix-norec . Doing
so demonstrates that translation quality increases
by recomputing the KV cache across all wait-k
values. Similarly, as with the previous ablation,
the difference in the BLEU score becomes less
pronounced for the higher wait-k values.
An interesting observation is that models evalu-
ated at lower wait-k values have their LAAL devi-
ate from their respective k to a greater degree than
those evaluated at higher wait-k. Such an increase0 500 1,000 1,500 2,000 2,500SM-norec-modconverse-norecprefix-reccausal-rec
Compuation (GFLOPs)
Figure 5: Box plots of the computational cost of each
method in GFLOPs during inference.
is a byproduct of the lower wait-k models gener-
ating longer predictions than their corresponding
references. The increased generation length is a
byproduct of the model hallucinating on sequences
provided insufficient contexts. These hallucina-
tions are most noticeable with prefix-rec andprefix-
norec in Figure 4e.
7.2 Compuational Saving Results
Fine-tuning LLMs with SimulMask also features
reduced training time compared with LLMs lever-
aging prefix fine-tuning or conversational prompt-
ing. For instance, this is reflected in the fine-
tuning times for one epoch on an H100 GPU on the
English-French dataset of the IWSLT 2017 dataset,
as reported in Table 1 (Cettolo et al., 2017).
Fine-tuning Approach Time (s)
Prefix Fine-tuning 9953
Conversational Prompting 1274
SimulMask 1014
Causal Mask 727
Table 1: Time to complete one epoch for different fine-
tuning approaches on an H100.
Furthermore, we find that SM-norec is also more
computationally efficient at inference than prefix-
recandconverse-norec . We report these results in
GFLOPs that are needed to complete a sentence
translation in Figure 5. The data used to obtain
the results was a random 1000 samples from the
English-French split of the IWSLT 2017 test set
(Cettolo et al., 2017). The models chosen either
used wait-3 or a chunk size of 5.
By leveraging SimulMask during fine-tuning,
we eschew the need to recompute the KV cache
at inference. In doing so, SimulMask saves com-
putation compared to prefix-rec and causal-rec .0 50 10002,0004,000
Sequence LengthComputation (GFLOPs)Initial Recompute
Figure 6: Separated computational cost in GFLOPs
between initial (or required) computational cost and the
cost of recomputing already emitted target words in a
provided prompt during translation versus the sequence
length of a given sample.
We demonstrate the proportions of computation in
GFLOPs dedicated to re-computing the KV cache
and processing/predicting initial tokens in Figure
6 (based on prefix-rec ). The sequence length is the
number of tokens in the predicted target and input
source. As can be seen, it is critical to avoid re-
computing KV cache, as achieved by SimulMask,
to provide low latency translations, especially at
longer sequence lengths.
8 Conclusion
In this work, we first examine current LLM fine-
tuning approaches for SimulMT and identify their
shortcomings. We then propose a new paradigm
for fine-tuning LLMs for SimulMT that we call
SimulMask, which avoids the shortcomings of pre-
vious methods. When employing SimulMask, the
target sequence is prevented from attending to a
portion of the source sequence according to an arbi-
trary decision policy modeling SimulMT. Through
the application of SimulMask, we can efficiently
fine-tune a LLM for SimulMT and reduce the com-
putational costs of inference by eliminating the
recomputation of the KV cache for the target se-
quence, unlike prior works. Furthermore, we can
exceed or match the translation quality of prior
works at all wait-k values across multiple language
pairs.
Limitations
Given the translation quality benefits at a reduced
computational cost of fine-tuning with SimulMask,
it would be beneficial to evaluate the approach to
larger and more powerful LLMs, adapting them
for SimulMT. Also, while SimulMask is broadlyapplicable to various decision policies, our current
evaluation was limited to only testing the effective-
ness of SimulMask on the wait-k policy and did
not evaluate alternative fixed or more flexible de-
cision policies. Additionally, we did not explore
simultaneous speech-to-text or speech-to-speech
translation, which SimulMask has yet to be tested
on.
Acknowledgments
This research was supported, in part, by the Na-
tional Science Foundation grants 2223483 and
2223484.
References
Victor Agostinelli, Max Wild, Matthew Raffel,
Kazi Asif Fuad, and Lizhong Chen. 2023. Simul-llm:
A framework for exploring high-quality simultane-
ous translation with large language models. arXiv
preprint arXiv:2312.04691 .
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Mérouane Debbah, Étienne Goffinet, Daniel Hess-
low, Julien Launay, Quentin Malartic, et al. 2023.
The falcon series of open language models. arXiv
preprint arXiv:2311.16867 .
Mauro Cettolo, Marcello Federico, Luisa Bentivogli,
Jan Niehues, Sebastian Stüker, Katsuhito Sudoh,
Koichiro Yoshino, and Christian Federmann. 2017.
Overview of the IWSLT 2017 evaluation campaign.
InProceedings of the 14th International Conference
on Spoken Language Translation , pages 2–14, Tokyo,
Japan. International Workshop on Spoken Language
Translation.
Kyunghyun Cho and Masha Esipova. 2016. Can neu-
ral machine translation do simultaneous translation?
arXiv preprint arXiv:1606.02012 .
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. Unsupervised
cross-lingual representation learning at scale. arXiv
preprint arXiv:1911.02116 .
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 644–648, Atlanta,
Georgia. Association for Computational Linguistics.
Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-
tor O.K. Li. 2017. Learning to translate in real-time
with neural machine translation. In Proceedings of
the 15th Conference of the European Chapter of theAssociation for Computational Linguistics: Volume
1, Long Papers , pages 1053–1062, Valencia, Spain.
Association for Computational Linguistics.
Shoutao Guo, Shaolei Zhang, Zhengrui Ma, Min Zhang,
and Yang Feng. 2024. Sillm: Large language models
for simultaneous machine translation. arXiv preprint
arXiv:2402.13036 .
Vivek Iyer, Pinzhen Chen, and Alexandra Birch. 2023.
Towards effective disambiguation for machine trans-
lation with large language models. In Proceedings
of the Eighth Conference on Machine Translation ,
pages 482–495, Singapore. Association for Compu-
tational Linguistics.
Masoud Jalili Sabet, Philipp Dufter, François Yvon,
and Hinrich Schütze. 2020. SimAlign: High qual-
ity word alignments without parallel training data
using static and contextualized embeddings. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2020 , pages 1627–1643, Online. Association
for Computational Linguistics.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Roman Koshkin, Katsuhito Sudoh, and Satoshi Naka-
mura. 2024. Transllama: Llm-based simultaneous
translation system. arXiv preprint arXiv:2402.04636 .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2023. Bloom: A 176b-
parameter open-access multilingual language model.
Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,
Kaibo Liu, Baigong Zheng, Chuanqiang Zhang,
Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and
Haifeng Wang. 2019. STACL: Simultaneous trans-
lation with implicit anticipation and controllable la-
tency using prefix-to-prefix framework. In Proceed-
ings of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 3025–3036, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Xutai Ma, Mohammad Javad Dousti, Changhan Wang,
Jiatao Gu, and Juan Pino. 2020a. SIMULEV AL: An
evaluation toolkit for simultaneous translation. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 144–150, Online. Association
for Computational Linguistics.
Xutai Ma, Juan Pino, and Philipp Koehn. 2020b.
Simulmt to simulst: Adapting simultaneous text
translation to end-to-end simultaneous speech trans-
lation. arXiv preprint arXiv:2011.02048 .
Yasmin Moslem, Rejwanul Haque, John D. Kelleher,
and Andy Way. 2023. Adaptive machine translationwith large language models. In Proceedings of the
24th Annual Conference of the European Association
for Machine Translation , pages 227–237, Tampere,
Finland. European Association for Machine Transla-
tion.
Sara Papi, Marco Gaido, Matteo Negri, and Marco
Turchi. 2022a. Does simultaneous speech transla-
tion need simultaneous models? In Findings of the
Association for Computational Linguistics: EMNLP
2022 , pages 141–153, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Sara Papi, Marco Gaido, Matteo Negri, and Marco
Turchi. 2022b. Over-generation cannot be rewarded:
Length-adaptive average lagging for simultaneous
speech translation. In Proceedings of the Third Work-
shop on Automatic Simultaneous Translation , pages
12–17, Online. Association for Computational Lin-
guistics.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow,
Ruxandra Cojocaru, Alessandro Cappelli, Hamza
Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023. The RefinedWeb dataset
for Falcon LLM: outperforming curated corpora
with web data, and web data only. arXiv preprint
arXiv:2306.01116 .
Maja Popovi ´c. 2017. chrf++: words helping character
n-grams. In Proceedings of the second conference on
machine translation , pages 612–618.
Matt Post. 2018. A call for clarity in reporting bleu
scores. arXiv preprint arXiv:1804.08771 .
Ofir Press, Noah A Smith, and Mike Lewis. 2021.
Train short, test long: Attention with linear biases
enables input length extrapolation. arXiv preprint
arXiv:2108.12409 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,
Viresh Ratnakar, and George Foster. 2023. Prompt-
ing PaLM for translation: Assessing strategies and
performance. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 15406–
15427, Toronto, Canada. Association for Computa-
tional Linguistics.
Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, and
Gholamreza Haffari. 2024. Conversational simulmt:
Efficient simultaneous translation with large lan-
guage models. arXiv preprint arXiv:2402.10552 .Minghan Wang, Jinming Zhao, Thuy-Trang Vu, Fate-
meh Shiri, Ehsan Shareghi, and Gholamreza Haffari.
2023. Simultaneous machine translation with large
language models. arXiv preprint arXiv:2309.06706 .
Haoran Xu, Young Jin Kim, Amr Sharaf, and
Hany Hassan Awadalla. 2023. A paradigm shift
in machine translation: Boosting translation perfor-
mance of large language models. arXiv preprint
arXiv:2309.11674 .
Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp
Koehn. 2023. Machine translation with large lan-
guage models: Prompting, few-shot learning, and
fine-tuning with QLoRA. In Proceedings of the
Eighth Conference on Machine Translation , pages
468–481, Singapore. Association for Computational
Linguistics.
Baigong Zheng, Renjie Zheng, Mingbo Ma, and Liang
Huang. 2019. Simpler and faster learning of adaptive
policies for simultaneous translation. arXiv preprint
arXiv:1909.01559 .
A Appendix
A.1 Encoder Attention Mask
An example of an encoder attention mask used to
model simultaneous translation during training is
provided in Figure 7. The attention mask in Figure
7 is designed for a source sequence length of 5
tokens where the first read step reads 2 tokens, the
second read step reads 1 token, and the third read
step reads 2 tokens.
Figure 7: An attention mask to model simultaneous
translation for a Transformer encoder during training.
A.2 Inference Mirrored Attention and
SimulMask
Figure 8 provides the complete example of infer-
ence mirrored attention for the wait-1 policy ex-
plained in Section 5.1. To reiterate in Figure 8a, the
query of p2attends to the keys of p1, s1, p2. Thus,
during fine-tuning, to eliminate the fine-tuning-
inference mismatch, the query of p2is limited to
similarly attend to the keys of p1, s1, p2as shown
in Figure 8b rather than the entire source sequence.
Then for the second and third decoding steps at in-
ference, the queries of t1andt2attend to the keysofp1, s1, s2, p2andp1, s1, s2, s3, p2, respectively
as shown in Figures 8c and 8e. Once again, to
eliminate the fine-tuning-inference mismatch, the
queries of t1andt2must attend to an identical set
of keys as shown in Figures 8d and 8f.
A.3 Hyperparameters
The fine-tuning hyperparameters used for SM-
norec, causal-rec, causal-offline, prefix-rec,
prefix-norec, and converse-norec models are
provided in Table 2.
The prompts used for the SM-norec, causal-rec,
causal-offline, prefix-rec, andprefix-norec models
consisted of the following format:
Translate the following sentence from
[SRC] to [TGT]: [SRC-Sentence]\n
Assistant: [TGT-Sentence]
Alternatively, the converse-norec model used the
prompt:
Translate the following sentence from
[SRC] to [TGT]\nAssistant: <s><t>
[SRC-Chunk-1]</t>[TGT-Chunk-1]
</s><s><t>...<s><t>
[SRC-Chunk-n]</t>[TGT-Chunk-n]</s>
Our implementation for converse-norec followed
Wang et al. (2024). However, we used the Iter-
max method from the SimAlign toolkit leverag-
ing XLM-RoBERTa base (Conneau et al., 2019)
to align words due to their work reporting better
alignments than fast-align (Jalili Sabet et al., 2020;
Dyer et al., 2013).
A.4 Extended Translation Results
The translation quality results in Table 3 provide
the numerical BLEU and LAAL values from
Figure 4 for prefix-rec ,converse-norec , and
SM-norec-mod . Alternatively, Table 4 provides the
numerical chrF++ values associated with each of
these models from Figure 4.
A.5 Sequence Lengths
Figure 9 reports the number of occurrences on the
English-French IWSLT2017 validation set (Cet-
tolo et al., 2017) that the combined length of the
source sequence and the predicted target sequence
are within a specified range for prefix-rec at wait-3.(a) Attention for the first prediction step.
 (b) Inference mirrored attention for the first prediction step.
(c) Attention for the second prediction step.
(d) Inference mirrored attention for the second prediction step.
(e) Attention for the third prediction step.
 (f) Inference mirrored attention for the third prediction step.
Figure 8: Attention during inference and finetuning for SimulMT.
Hyperparameter Group 1 Group 2 Group 3 Group 4
Weight Precision bfloat16 bfloat16 bfloat16 bfloat16
Optimizer AdamW AdamW AdamW AdamW
Learning Rate 2·10−42·10−42·10−42·10−4
LR Scheduler Inverse Sqrt Inverse Sqrt Inverse Sqrt Inverse Sqrt
Weight Decay 0.1 0.1 0.1 0.1
Warmup Ratio 0.03 0.03 0.03 0.03
Max Gradient Norm 1 1 1 1
Max Sequence Length 512 512 512 512
Wait-k 5,7,9,11 5,7,9,11 - -
Epochs 2 1 1 2
Batch size 64 1024 64 64
Attention heads 32 32 32 32
Layers 24 24 24 24
Hidden Size 2048 2048 2048 2048
Positional Encoding Modified ALiBi ALiBi ALiBi ALiBi
Attention Mask SimulMask Causal Mask Causal Mask Causal Mask
δmax - - 10 -
β - - 0.5 -
ρmin - - 0.5 -
ρmax - - 0.9 -
Table 2: Fine-tuning hyperparameters for all models in Section 7. Group 1: SM-norec, SM-norec-mod .Group 2:
prefix-rec, prefix-norec .Group 3: converse-norec. Group 4: causal-rec, causal-offline .
A.6 Licensing Information
The SimulEval toolkit is licensed under CC BY-
SA-4.0 license (Ma et al., 2020a). The Simul-LLM
framework and SimAlign toolkit are licensed underthe MIT license (Agostinelli et al., 2023; Jalili Sa-
bet et al., 2020). The IWSLT 2017 dataset is li-
censed under CC BY-NC-ND (Cettolo et al., 2017).
The Falcon model we used from Hugging FaceModel en-fr en-nl en-it en-ro en-de
SM-norec-mod (wait-1) 28.89 (2.05) 16.37 (1.53) 15.72 (1.92) 9.77 (1.53) 17.73 (1.68)
SM-norec-mod (wait-3) 36.48 (3.67) 25.31 (3.37) 23.17 (3.29) 21.44 (3.32) 24.34 (3.23)
SM-norec-mod (wait-5) 38.77 (5.39) 29.13 (5.13) 28.01 (5.09) 25.79 (5.18) 27.11 (4.92)
SM-norec-mod (wait-7) 38.85 (7.02) 29.98 (6.78) 29.81 (6.76) 26.07 (6.82) 28.43 (6.58)
prefix-rec (wait-1) 16.37 (1.64) 4.53 (1.14) 5.27 (1.12) 4.03 (1.29) 5.64 (2.12)
prefix-rec (wait-3) 28.90 (3.47) 22.44 (3.25) 21.74 (3.15) 14.82 (3.29) 19.13 (3.09)
prefix-rec (wait-5) 39.55 (5.33) 29.76 (5.17) 27.26 (5.03) 20.85 (5.14) 27.38 (4.86)
prefix-rec (wait-7) 40.48 (7.00) 30.95 (6.77) 30.52 (6.75) 25.06 (6.78) 28.79 (6.56)
converse-norec (chunk-1) 19.89 (0.95) 9.17 (0.65) 10.86 (0.89) 10.80 (0.82) 13.67 (1.13)
converse-norec (chunk-3) 31.94 (2.46) 24.22 (2.76) 23.04 (2.98) 21.02 (2.86) 24.26 (2.83)
converse-norec (chunk-5) 35.03 (3.55) 25.49 (3.99) 24.82 (4.15) 22.89 (4.05) 26.17 (4.04)
converse-norec (chunk-7) 36.42 (4.67) 27.09 (5.17) 27.40 (5.32) 23.48 (5.18) 26.82 (5.22)
converse-norec (chunk-9) 36.87 (5.79) 27.86 (6.29) 27.37 (6.37) 23.73 (6.31) 27.28 (6.39)
converse-norec (chunk-11) 37.27 (6.87) 27.84 (7.37) 27.52 (7.42) 23.76 (7.40) 27.47 (7.47)
Table 3: Translation quality and latency results in BLEU and LAAL.
Model en-fr en-nl en-it en-ro en-de
SM-norec-mod (wait-1) 50.76 (2.05) 36.14 (1.53) 36.37 (1.92) 26.36 (1.53) 42.20 (1.68)
SM-norec-mod (wait-3) 58.42 (3.67) 47.60 (3.37) 45.17 (3.29) 43.41 (3.32) 50.64 (3.23)
SM-norec-mod (wait-5) 60.40 (5.39) 52.34 (5.13) 51.34 (5.09) 49.78 (5.18) 53.09 (4.92)
SM-norec-mod (wait-7) 60.59 (7.02) 53.74 (6.78) 53.30 (6.76) 50.18 (6.82) 53.73 (6.58)
prefix-rec (wait-1) 33.72 (1.64) 15.50 (1.14) 18.72 (1.12) 13.62 (1.18) 25.71 (2.16)
prefix-rec (wait-3) 49.64 (3.47) 44.26 (3.25) 42.63 (3.15) 28.24 (3.173) 40.15 (3.10)
prefix-rec (wait-5) 61.05 (5.33) 53.50 (5.17) 49.91 (5.03) 43.11 (5.04) 52.36 (4.86)
prefix-rec (wait-7) 61.91 (7.00) 55.07 (6.77) 54.28 (6.75) 49.09 (6.77) 53.46 (6.53)
converse-norec (chunk-1) 47.54 (0.95) 28.01 (0.65) 33.19 (0.89) 30.67 (0.74) 46.12 (1.24)
converse-norec (chunk-3) 56.45 (2.46) 47.13 (2.76) 46.93 (2.98) 44.04 (2.74) 52.35 (2.91)
converse-norec (chunk-5) 58.27 (3.55) 49.15 (3.99) 48.98 (4.15) 46.49 (3.96) 53.05 (4.09)
converse-norec (chunk-7) 58.98 (4.67) 50.69 (5.17) 51.37 (5.32) 47.97 (5.08) 53.11 (5.26)
converse-norec (chunk-9) 59.33 (5.79) 51.52 (6.29) 51.49 (6.37) 48.56 (6.22) 53.14 (6.38)
converse-norec (chunk-11) 59.47 (6.87) 52.11 (7.37) 51.47 (7.42) 48.85 (7.31) 52.97 (7.43)
Table 4: Translation quality and latency results in chrF++ and LAAL.2
(tiiuae/falcon-rw-1b) is licensed under Apache 2.0
(Penedo et al., 2023).
2The chrF++ results for prefix-rec andconverse-norec on
the English-German and English-Romanian language pairs
were obtained with different fine-tuning and evaluation seeds
than the models reported in Figure 4.0306090120150180020406080100120140
991144
134
107
747871
3236
2225
1216
4682221
Sequence LengthCount
Figure 9: Histogram of the distribution of sequence
lengths.