AGR AME: Any-Granularity Ranking with Multi-Vector Embeddings
Revanth Gangi Reddy1*Omar Attia2*Yunyao Li3†Heng Ji1Saloni Potdar2
1University of Illinois at Urbana-Champaign2Apple3Adobe
{revanth3,hengji}@illinois.edu
{oattia,s_potdar}@apple.com yunyaol@adobe.com
Abstract
Ranking is a fundamental and popular prob-
lem in search. However, existing ranking algo-
rithms usually restrict the granularity of rank-
ing to full passages or require a specific dense
index for each desired level of granularity. Such
lack of flexibility in granularity negatively af-
fects many applications that can benefit from
more granular ranking, such as sentence-level
ranking for open-domain question-answering,
or proposition-level ranking for attribution.
In this work, we introduce the idea of any-
granularity ranking which leverages multi-
vector approaches to rank at varying levels of
granularity while maintaining encoding at a sin-
gle (coarser) level of granularity. We propose
a multi-granular contrastive loss for training
multi-vector approaches, and validate its utility
with both sentences and propositions as ranking
units. Finally, we demonstrate the application
of proposition-level ranking to post-hoc cita-
tion addition in retrieval-augmented generation,
surpassing the performance of prompt-driven
citation generation.
1 Introduction
Dense Retrieval approaches leverage dual encoder
models to obtain vector representations for both
queries and passages. Commonly, single-vector
methods (Gautier et al., 2022; Karpukhin et al.,
2020) obtain a single embedding for each query
and passage to compute the relevance score us-
ing a dot product. In contrast, multi-vector ap-
proaches (Khattab and Zaharia, 2020; Santhanam
et al., 2022) capture more fine-grained interactions
when computing the query-passage relevance score,
resulting in better ranking performance (Thakur
et al., 2021). A key advantage of multi-vector ap-
proaches is their use of token-level embeddings
paired with a MaxSim operation (Khattab and Za-
haria, 2020) for relevance scoring. This enables
*Equal Contribution. Revanth is an external collaborator.
†Work done during position at Apple.
Figure 1: Ranking at different levels of granularity.
X→Yis used to denote that Xrepresents the query gran-
ularity used for ranking, with entire query encoded, and
Yindicates the granularity of the retrieval unit being
ranked, with entire retrieval unit encoded. In addition to
the typical ranking setting ( A), our proposed approach
enables ranking finer retrieval units ( BandD) or using
finer query units for ranking ( CandD).
a more granular scoring mechanism that involes
computing dot products between each query token
embedding and each passage token embedding, to
identify the best matching passage token for each
query token. These individual token-level match-
ing scores are subsequently aggregated to obtain
the final query-passage relevance score.
We make the important observation that the
use of token-level embeddings in multi-vector ap-
proaches can facilitate discriminative scoring of
different sub-parts within a retrieval unit. For ex-
ample, even when passages (retrieval units) are
input to the encoder, i.e. encoding at passage-level,
sub-components such as sentences can be sepa-
rately scored against the query to identify the most
relevant sentence within the passage. We argue that
such finer-granularity scoring is inherent to multi-
vector approaches, but not possible with single-
vector approaches, where only one embedding rep-
resents the entire passage, thereby not allowing forarXiv:2405.15028v1  [cs.CL]  23 May 2024scoring of its constituent sentences. Being able
to rank at varying levels of granularity is benefi-
cial for a variety of applications. For instance, in
open-domain question answering (Lee et al., 2019;
Karpukhin et al., 2020), ranking sentences within
the retrieved passages can better pinpoint the an-
swer. For attribution (Rashkin et al., 2023; Chen
et al., 2023a), atomic facts (propositions) within
sentences need to be used as queries to obtain rele-
vant evidence supporting the facts.
To achieve this, we introduce AGR AME
(Any-Granularity Ranking with Multi-vector
Embeddings), a method that permits ranking at
different levels of granularity while maintaining
encoding at a single, coarser level. Our approach
enables i) ranking at a finer level than the retrieval
unit, and ii) ranking using fragments of the query,
as demonstrated in Figure 1. We hypothesize that
encoding at a coarser level–such as the entire re-
trieval unit or query–can provide additional context
for the sub-retrieval units being ranked or sub-parts
of the query used for ranking. In contrast, achiev-
ing such granularity with single-vector approaches
requires the use of specialized encoders, such as a
sub-sentence encoder (Chen et al., 2023b), or neces-
sitates a separate encoding at the desired ranking
granularity (Chen et al., 2023c).
Firstly, how well do multi-vector approaches per-
form when used for ranking at a finer granularity?
To investigate this, we conduct an exploratory ex-
periment (described in §2) using ColBERTv2 (San-
thanam et al., 2022), a popular multi-vector model,
to rank both sentences and passages when encoded
at corresponding granularities. From the results
summarized in Table 1, we see that performance of
sentence ranking is notably inferior when the en-
coding is at the passage-level, a result that counter-
intuitive as passage-level encoding should provide
a richer context for scoring sentences.
To improve the model’s ability to rank at finer
granularity, we propose a multi-granular con-
trastive loss during training (outlined in §3.3). This
introduces an additional sentence-level ranking loss
that augments a passagr-level loss, to enable the
model to not only accurately select the relevant pas-
sage for a query but also discriminatively identify
the right sentence within that passage. Our exper-
imental results, presented in Section 4.1, confirm
significantly boost in sentence-level ranking while
maintaining passage-level performance.
While AGR AMEis generally applicable to ar-
bitrary granularity, we explore the effectivenessfor proposition-level ranking, crucial for applica-
tions requiring fine-grained attribution (Rashkin
et al., 2023). Our results (in §4.2) indicate that
incorporating a sentence-level contrastive loss fur-
ther improves ranking at proposition-level. Addi-
tionally, we demonstrate (in §4.3) that proposition-
level ranking can effectively integrate citations into
generated text post-hoc. Our proposed PROPCITE
method utilizes propositions from generated text as
queries to rank input context passages and select
relevant citations, showing superior performance
over traditional methods that prompt models to in-
clude citations in retrieval-augmented generation.
The main contributions are as follows:
•We introduce AGR AME, that leverages multi-
vector embeddings for ranking at various gran-
ularities while using the same encoding-level.
•We introduce a multi-granular contrastive loss
for training multi-vector approaches, which
we show improves sentence-level ranking
even when encoding at passage-level.
•We demonstrate superior proposition-level
ranking using AGR AME, surpassing existing
state-of-the-art methods.
•We leverage proposition-level ranking to for-
mulate a post-hoc citation addition approach
for retrieval-augmented generation, that out-
performs prompt-driven citation generation.
2 Motivating Experiment
Here, we investigate the effectiveness of Col-
BERTv2 (Santhanam et al., 2022), a multi-vector
approach, in ranking at a finer granularity than
the encoding level. Specifically, when encoding
is at passage-level, we measure the performance
while ranking at sentence level (using the scoring
scheme described in §3.2), in addition to ranking
at the usual passage-level. A MaxSim operation is
applied between query token vectors and token vec-
tors corresponding to the sentence to get a sentence-
level score, which is then added with the passage-
level score to get the final query-sentence relevance
score for ranking. When encoding is at sentence-
level, the usual MaxSim score gives query-sentence
relevance. On the other hand, the query-passage
relevance score for ranking, when encoding is at
sentence-level, is obtained as the maximum of the
corresponding passage’s query-sentence relevance
scores.ModelEncoding
LevelRanking Level
Sentence Passage
P@1 R@5 P@1 R@5
Contriever
(Single Vec.)Sentence 19.3 45.6 32.4 62.8
Passage - - 37.8 65.1
ColBERTv2
(Multi Vec.)Sentence 31.6 56.3 40.2 66.8
Passage 27.4 48.8 43.4 69.1
Table 1: Precision@1 (P@1) and Recall@5 (R@5)
results on the Natural Questions (Kwiatkowski et al.,
2019) dev set. We show numbers both at sentence-level
and passage-level ranking granularities for when sen-
tences and passages are encoded individually.
We also include Contriever (Gautier et al., 2022),
a single-vector approach, for comparison. When
encoding is at sentence-level, the same strategy as
described before is used to obtain query-passage
relevance score for Contriever. On the other hand,
when encoding is at passage-level, Contriever does
not support sentence-level ranking, which as dis-
cussed earlier, is an inherent limitation of single-
vector approaches. For the evaluation setting, we
consider the development set of the Natural Ques-
tions (Kwiatkowski et al., 2019) dataset to get the
queries, and Wikipedia1as the retrieval corpus.
To keep the retrieval index size manageable, Con-
triever is used to index and retrieve 100 passages,
which are then ranked by ColBERTv2. When rank-
ing or encoding at sentence-level, only the sen-
tences in these top 100 passages are considered.
Evaluation is done for Precision@1 and Recall@5
based on string exact-match of the answer (Ra-
jpurkar et al., 2016).
Table 1 shows the results, wherein we see a
substantial drop in sentence-level ranking when
encoding is at passage-level, and vice versa. As
expected, passage-level ranking is better when en-
coding is at passage-level. However, it is surprising
to see sentence-level ranking performance to be
lower since passage-level encoding can provide
more context when encoding the individual tokens
in the sentences. This is more so the case when
sentences in the passage that actually ‘answer’ the
query do not have any overlapping terms (seman-
tic or lexical) with the query. Table 2 shows an
example for this. At both sentence and passage-
level encoding, sentences S1 and S2, on account
of strong lexical overlap with the query, are scored
considerably higher than sentence S3, which ac-
1We use the 22M passage split of the Wikipedia 2018 dump
from Gautier et al. (2022)
Sent. ID Sentence-level Enc. Passage-level Enc.
S1 Rank: 1, Score: 23.31 Rank: 1, Score: 23.92
S2 Rank:2, Score:17.47 Rank:2, Score:20.12
S3 Rank:3, Score:16.63 Rank:3, Score:16.96
Table 2: Sentence-level ColBERTv2 scores for different
sentences in the same passage, when encoding is at
sentence-level and passage-level. We see that the most
relevant sentence (S3) is ranked worst (i.e lowest score).
Token-wise MaxSim score heatmap is also shown, with
tokens in S1 and S2 having higher scores than in S3.
tually describes the effects of climate change but
has weak semantic overlap with the query. From
the token-wise MaxSim score heatmap, we can see
that tokens in S3, the most relevant sentence, get
lower scores.
We can expect scoring S1 and S2 highly, on ac-
count of overlap, to be particularly useful when
identifying the passage as relevant amongst a cor-
pus of millions of passages. On the other hand, it
can be counter-productive when discriminatively
selecting the most relevant sentence within the pas-
sage. This suggests that ranking at different gran-
ularities requires the model to have the ability to
dynamically switch the notion of relevance when
scoring. As we describe later in §3.2, we introduce
a new query marker during encoding to signal the
level of granularity, which helps the model distin-
guish better when scoring at different granularities
In §4.1, we demonstrate that this helps to score
appropriately for ranking at sentence-level (a finer
granularity), when encoding at passage-level.
3 Method
Here, we first provide background (in §3.1)
on the modeling and training process for Col-
BERTv2 (Santhanam et al., 2022). Then, we
describe our proposed multi-granular contrastive
training process (in §3.3), which provides an addi-
tional sentence-level relevance supervision during
distillation.
3.1 ColBERTv2 Preliminaries
Single-vector retrievers (Karpukhin et al., 2020;
Gautier et al., 2022) typically use a BERT-
based (Devlin et al., 2019) dual-encoder architec-ture to obtain a single embedding for a query qand
passage pseparately. This is usually the CLS output
or a pooled representation of the individual token
outputs from the encoder E(.). The query-passage
relevance score is computed as the dot product of
their corresponding representation:
⃗Qq=Pool (E(q));⃗Pp=Pool (E(p))
Score (q, p) =⃗QqT⃗Pp
In constrast, ColBERTv2 (Santhanam et al., 2022)
is a multi-vector retrieval model, that uses token-
level dense embeddings of the query and passage.
Given a query qcontaining ntokens tq
iand passage
pcontaining mtokens tp
i, additional query and pas-
sage marker tokens mqandmpare prepended to
the query and passage respectively before encod-
ing, to provide additional signal to the encoder.
The query-passage relevance score SCB(q, p)is
obtained as below using the MaxSim operator intro-
duced in Khattab and Zaharia (2020):
[⃗Qtq
1,⃗Qtq
1, ..., ⃗Qtq
n] =E(q) =E(cat(mq, tq
1, ..., tq
n))
[⃗Ptp
1,⃗Ptp
2, ..., ⃗Ptp
m] =E(p) =E(cat(mp, tp
1, ..., tp
m))
SCB(q, p) =MaxSim (q, p) =nX
i=1max
1≤j≤m⃗Qtq
iT⃗Ptp
j
The training process for neural retrievers typi-
cally involves a contrastive loss over the <query
q, postitive p+, negative p−> triples. ColBERTv2
instead incorporates a distillation-based training
strategy wherein knegative passages are sam-
pled from the retrieval corpus, to form a (k+ 1) -
way passage set [p] ={p+, p−
1, ..., p−
k}for each
query. The relevance supervision is in the form of
soft scores SCE(.)from a cross-encoder reranker.
A KL-Divergence loss Lpsgbetween the cross-
encoder and ColBERT passage scoring distribu-
tions, DCE(q,[p])andDCB(q,[p])respectively, is
used for training:
DCB(q,[p]) = [ Softmax (SCB(q, pi))]k+1
i=1
DCE(q,[p]) = [ Softmax (SCE(q, pi))]k+1
i=1
Lpsg(q,[p]) =KL(DCE(q,[p])||DCB(q,[p]))
3.2 AGR AME: Any-Granularity Ranking
with Multi-Vector Embeddings
Here, we introduce our approach for scoring sub-
units within the retrieval unit. This is made possible
Figure 2: Figure demonstrating our sentence-level scor-
ing methodology using multi-vector representations
with encoding at passage-level. Query marker mqis
used while getting passage-level score P, while marker
m′
qis used for getting sentence-level scores S1,S2,S3.
by the access to token-level embeddings in multi-
vector approaches. While AGR AMEcan rank at
any granularity, in this section, we will consider
sentences as the sub-units for simplicity. With the
entire passage input to the encoder, only the output
embeddings corresponding to tokens within a given
sentence are used during the MaxSim operation for
scoring that sentence.
Lettpi
jrcorrespond to the jthtoken of sentence
spi
jfrom passage pithat is passed as input to en-
coder E. To signal the model to score discrimina-
tively within the passage for sentence-level rank-
ing, we prepend a new query marker token m′
q,
different from mqused when ranking at passage-
level. The in-passage query-sentence relevance
score SCB(q, spi
j)is computed as follows:
[⃗Q′
tq
1,⃗Q′
tq
1, ...,⃗Q′
tq
n] =E(cat(m′
q, tq
1, ..., tq
n))
SCB(q, spi
j) =nX
i=1max
1≤r≤|spi
j|⃗Q′
tq
iT⃗Ptpi
jr
Note that the passage encoding is the same as be-
fore, meaning the same multi-vector index can
be used for both passage-level and sentence-level
ranking. As we demonstrate in §4.1, encoding at
passage-level provides more context to the token
embeddings to benefit sentence-level ranking.
We note that our proposed sentence-level loss
(described in §3.3) teaches the model to rank sen-
tences discriminatively within a passage , and not
across passages . Hence, at inference to get a fi-
nal sentence-level relevance score Score (q, spi
j)to
rank sentences across passages, we combine the in-
passage sentence relevance score SCB(q, spi
j)with
the usual passage-level relevance score SCB(q, pi):
Score (q, spi
j) =SCB(q, spi
j) +αSCB(q, pi)3.3 Multi-Granular Contrastive Training
As discussed in §3.1, given a query qand a pas-
sage set [p], the ColBERTv2 training process aims
to teach the model to identify the most relevant
passage within [p]. To enable the model to discrim-
inatively select sub-units within the passage, we
propose to incorporate a more finer-level of train-
ing supervision, by teaching to further identify the
most relevant sentence within each passage.
Since ColBERTv2 uses passage-level cross-
encoder scores as teacher supervision, we train
a different cross-encoder model CE′to provide
in-passage sentence-level relevance supervision.
Specifically, CE′takes a passage pias input, with a
given sentence spi
jmarked with delimiters $, to give
a relevance score SCE′(q, spi
j)for the sentence.
SCE′(q, spi
j) =CE′(q, cat (spi
1, ...,$spi
j$, ..., spi
l))
CE′is trained using question answering data in
the form <query, passage, answer> triples. A bi-
nary cross-entropy loss is used while training CE′,
wherein any sentence within the passage that con-
tains the answer is marked as a positive, with the
other sentences marked as negatives.
The cross encoder CE′provides soft scores for
sentence-level relevance superivision when training
our model. For each passage pi, we compute a
KL-divergence loss Ls(q, pi)between the CE′and
ColBERTv2 sentence-level scoring distributions,
DCE′(q,[spi])andDCB(q,[spi])respectively.
DCB(q,[spi]) =h
Softmax (SCB(q, spi
j))il
j=1
DCE′(q,[spi]) =h
Softmax (SCE′(q, spi
j))il
j=1
Ls(q, pi) =KL(DCE′(q,[spi])||DCB(q,[spi])))
We then aggregate each passage’s sentence-level
scoring loss Ls(q, pi), by weighting with the cor-
responding passage’s relevance supervision score
SCE(q, pi), to get a single loss Lsent.(q,[p]). The
passage score weight ensures that the model is pe-
nalized higher on sentence-level losses for passages
that are more relevant. The sentence-level loss
Lsent.(q,[p])is finally added to original passage-
level loss Lpsg(q,[p])to get the training loss L.
Lsent.(q,[p]) =k+1X
i=1Softmax (SCE(q, pi))Ls(q, pi)
L(q,[p]) =Lpsg(q,[p]) +Lsent.(q,[p])4 Experiments
AGR AME can rank at different granularities, as
shown in Figure 1, which involves ranking sub-
parts of the retrieval unit or ranking using sub-parts
of the query. In our experiments, we aim to in-
vestigate two research questions: RQ1: Can the
training approach proposed in §3.3 improve rank-
ing at a finer granularity than the level of encod-
ing, i.e. Query→Sub-Retrieval Unit ? In §4.1, we
show the improvements at sentence-level ranking
from our proposed multi-granular contrastive loss,
while maintaining performance at passage-level,
i.e.Query→Retrieval Unit ;RQ2: Can multi-vector
embeddings be used to rank with sub-parts of the
query? In §4.2, we demonstrate the application
of multi-vector approaches in Sub-Query →Sub-
Retrieval Unit ranking for proposition-level attribu-
tion. Here, a given proposition within a sentence
is used as the query to rank and identify relevant
propositions in a corpus of sentences. Further, in
§4.3, we introduce PROPCITE, a post-hoc citation
addition approach based on Sub-Query →Retrieval
Unit ranking. PROPCITEscores input context pas-
sages based on propositions in the generated text
to add citations in retrieval-augmented generation.
4.1 Query →Sub-Retrieval Unit Ranking for
Open-Domain QA
In §2, we saw that with a multi-vector approach
(ColBERTv2), sentence ranking performance drops
when changing the encoding from sentence-level
to passage-level. We addressed this in two ways:
(a) our proposed multi-granular contrastive loss
(in §3.3) provides sentence-level relevance super-
vision at training; b) AGR AMEintroduces a new
query marker (in §3.2) to signal scoring at sentence-
level. In this section, we empirically demonstrate
the benefits of our proposed approach by evaluating
sentence-level (sub-retrieval unit) ranking perfor-
mance when encoding is at passage-level.
4.1.1 Setup
Datasets We first evaluate on different popu-
lar open-domain QA datasets: Natural Questions
(NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi
et al., 2017), Web Questions (Berant et al., 2013)
and Entity Questions (Sciavolino et al., 2021). For
the retrieval corpus, we use the 2018 Wikipedia
dump released by Lee et al. (2019).
For cross-domain evaluation, we consider the
RobustQA (Han et al., 2023) dataset, a large-
scale OpenQA benchmark specifically designedModelEncoding
LevelNatural Questions TriviaQA Web Questions Entity Questions
Sentence Passage Sentence Passage Sentence Passage Sentence Passage
P@1 R@5 P@1 R@5 P@1 R@5 P@1 R@5 P@1 R@5 P@1 R@5 P@1 R@5 P@1 R@5
ContrieverSentence 20.6 48.9 35.0 65.4 31.0 58.8 48.5 72.1 14.5 39.1 28.8 57.9 14.7 42.7 39.8 64.9
Passage - - 40.3 66.0 - - 50.1 71.5 - - 36.9 63.6 - - 36.9 63.6
ColBERTv2Sentence 32.7 58.8 42.0 68.8 43.2 66.1 55.6 74.7 29.0 51.9 38.8 63.7 38.1 59.4 50.9 68.1
Passage 27.9 51.1 43.2 70.0 43.5 65.6 57.5 75.6 27.6 50.7 41.0 65.1 39.2 55.3 53.9 69.2
Ours Passage 36.8 60.5 44.0 69.9 48.9 68.1 57.9 75.6 33.2 55.6 41.2 65.4 43.8 61.5 54.2 69.5
Table 3: Precision@1 (P@1) and Recall@5 (R@5) results on various open-domain QA datasets. We show numbers
both at sentence-level and passage-level ranking for when sentences and passages are encoded individually.
ModelEncoding
LevelFinance Recreation Lifestyle Science Technology Writing Biomedical Average
Sent. Psg. Sent. Psg. Sent. Psg. Sent. Psg. Sent. Psg. Sent. Psg. Sent. Psg. Sent. Psg.
ContrieverSentence 13.8 22.2 17.9 29.4 19.7 32.7 10.9 18.8 11.3 18.3 23.0 36.1 10.7 16.6 15.3 24.9
Passage - 27.2 - 34.7 - 40.4 - 17.5 - 21.4 - 39.6 - 4.6 - 26.5
ColBERTv2Sentence 15.8 23.7 24.0 33.6 22.6 34.2 17.6 25.0 15.5 23.4 33.4 46.6 12.8 17.3 20.2 29.1
Passage 17.1 29.8 25.5 40.7 23.9 41.9 18.4 28.7 16.7 27.1 34.7 51.3 13.1 16.9 21.4 33.8
Ours Passage 19.5 29.8 29.2 40.4 30.0 42.6 20.5 28.1 18.4 26.4 36.7 50.2 15.4 17.5 24.2 33.6
Table 4: Precision@1 results on various domains from the RobustQA dataset (Han et al., 2023). We show numbers
at sentence-level and passage-level ranking for when sentences and passages are encoded individually.
for evaluating cross-domain generalization capabil-
ities. The QA pairs and documents correspond
to various domains like finance (adopted from
FiQA (Maia et al., 2018)), biomedical (adopted
from BioASQ (Tsatsaronis et al., 2015)), along
with recreation, lifestyle, science, technology and
writing, which are all adopted from LOTTE (San-
thanam et al., 2022).
Baselines We use Contriever (Gautier et al.,
2022) as the single-vector baseline, and Col-
BERTv2 (Santhanam et al., 2022) as the
multi-vector baseline. All models use MS
MARCO (Nguyen et al., 2016) as the training
dataset. Due to storage constraints, we create a
single-vector index with Contriever and rank the
top-100 retrieval results from Contriever using the
multi-vector approaches to report numbers.
4.1.2 Results
Table 3 shows ranking results on various open-
domain QA datasets. Firstly, as expected, for
both Contriever and ColBERTv2, passage-level
ranking performance is best when encoding is at
passage-level. We observe that our proposed ap-
proach significantly improves sentence-level rank-
ing performance with passage-level encoding, even
outperforming sentence-level ranking at sentence-
level encoding. This result confirms our intuition
that passage-level encoding benefits sentence-level
ranking, since it can provide more context to the
individual sentences during encoding. Moreover,
we ensure that passage-level ranking performanceis not compromised, with our approach matching
that of ColBERTv2 at passage-level encoding.
Table 4 shows sentence-level and passage-level
ranking results for cross-domain evaluation on the
RobustQA benchmark. We observe that our ap-
proach is robust and extends to cross-domain set-
tings, with consistent improvements in sentence-
level ranking across the board, while passage-level
ranking performance almost the same.
4.1.3 Analysis
We do an ablation study to analyze the effect of
using the new query marker m′
q, instead of the de-
fault query marker mq, when scoring at sentence-
level. Note that the markers mqandm′
qat infer-
ence only affect the query token embeddings. We
consider three different settings: A1) Using m′
qfor
sentence-level ranking at training and inference,
which corresponds to our proposed approach, A2)
Using m′
qfor sentence-level ranking at training
but using mqat inference, and finally A3) Using
the default mqfor sentence-level ranking at train-
ing and inference. We also include the baseline
ColBERTv2 for comparison, which does not have
sentence-level supervision at training and uses mq
at inference. From the results in Table 5, we can
see the benefit of using a different query marker,
with A1 outperforming A3 in the majority of the
cases. Moreover, using mqat inference even while
being trained with m′
q(A2) shows some gains over
baseline ColBERTv2, implying that the model also
learns to encode passage tokens to be better at dis-
criminatively scoring sentences in the passage.Setting NQ TQA WebQ EntQ
ColBERTv2 27.9 43.5 27.6 39.2
A1) Train →m′
q, Rank→m′
q36.8 48.9 33.2 43.8
A2) Train →m′
q, Rank→mq29.1 44.8 29.4 40.8
A3) Train →mq, Rank→mq35.9 47.6 32.9 44.1
Table 5: Precision@1 of sentence-level ranking perfor-
mance for various variants of using a different query
marker. ColBERTv2 is trained only with a passage-level
loss and uses the mqquery marker. The latter three vari-
ants are represented with the query marker used while
training with sentence-level contrastive loss and that
used for sentence-level ranking at inference.
Figure 3: Comparison of training curves for sentence-
level and passage-level loss, when a different query
marker is used. The model converges faster at sentence-
level with a different query marker, while passage-level
loss is mostly similar for the two.
In addition, we show the training loss curves
in Figure 3 when the same query marker ( mq)
vs different query markers ( m′
qandmq) are used
for sentence-level and passage-level loss respec-
tively. We can see that the model converges faster
at sentence-level loss when new marker m′
qis used.
Further, Table 6 shows the sentence-wise scores for
the example in Table 2 from using m′
qvsmqfor
sentence-level scoring. We observe that sentence-
level ranking changes when m′
qis used, with the
most relevant sentence (S3) ranked best.
4.2 Sub-Query →Sub-Retrieval Unit Ranking
for Fine-Grained Attribution
Attributing model-generated text with supporting
information from known sources is an emerging
research topic (Gao et al., 2023a; Liu et al., 2023).
Each sentence in the generation can have multi-
ple atomic facts or propositions (Min et al., 2023)
for which evidence needs to be obtained. In this
context, we explore ranking at the sub-sentence
granularity, wherein given a sentence as a query,
fine-grained attributions (Rashkin et al., 2023) need
to be obtained for a sub-part of the sentence. Specif-
ically, we consider the Atomic Fact Retrieval task,
wherein given an atomic proposition (sub-query)
in the sentence, the system is expected to identifySent. ID Query Marker m′
qQuery Marker mq
S1 Rank:2, Score:14.32 Rank: 1, Score: 24.04
S2 Rank:3, Score:14.16 Rank:2, Score:21.07
S3 Rank: 1, Score: 15.92 Rank:3, Score:16.81
Table 6: Sentence-level scores from our model at
passage-level encoding for the example in Table 2, when
different query markers are used. The most relevant sen-
tence (S3) is ranked best when new marker m′
qis used.
and retrieve evidence from atomic propositions as
facts, each of which can be a sub-part of sentences
within a corpus.
We consider this task to demonstrate that multi-
vector embeddings can be leveraged to natively
rank at the sub-sentence level, and compare them
against specialized models (Chen et al., 2023b)
explicitly trained to encode propositions. We note
that the encoding here is at sentence-level, unlike in
§4.1 where encoding is at passage-level. Since the
marker m′
qin our multi-granular training loss was
for sentence-level ranking with passage-level en-
coding, we use the default marker mqwhen ranking
at proposition-level with sentence-level encoding.
4.2.1 Setup
Datasets For the proposition-level ranking eval-
uation, we use the PROPSEGM ENT(Chen et al.,
2023a) dataset, which involves 8.8k propositions
as sub-queries for which evidence needs to be ob-
tained from a corpus of 45k human-labeled atomic
propositions from 1.5k News or Wikipedia docu-
ments in total.
Baselines For this task, we consider SUBEN-
CODER (Chen et al., 2023b) as the primary base-
line, a state-of-the-art sub-sentence encoder for
proposition-level ranking. SUBENCODER has been
specifically trained to produce contextual embed-
dings for atomic propositions in a sentence. Be-
ing a single-vector model, SUBENCODER pro-
duces a single sub-sentence embedding for each
atomic proposition in the sentence. We also include
other sentence-level embedding approaches, such
as GTR (Ni et al., 2022b), Sentence-T5 (Ni et al.,
2022a), as baselines that Chen et al. (2023a) adapt
for this task by specifically pooling over the tokens
of the proposition to get a single-vector embedding.
4.2.2 Results
Table 7 shows results from the Atomic Fact Re-
trieval task. First, we observe that the baseline Col-
BERTv2 already outperforms the state-of-the-artFigure 4: Figure illustrating PROPCITE, our proposed approach for post-hoc addition of citations to long-form
answers. PROPCITEencodes sentences and uses the propositions within them as queries for attribution. The
figure shows the propositions highlighted within the current sentence (in yellow), and the corresponding supporting
evidence highlighted in the input context passages. PROPCITEcorrectly attributes proposition P2to context C1,
while directly encoding and querying using P2incorrectly attributes to C2.
ModelProposition Sentence
P@1 R@5 P@1 R@5
GTR 21.9 52.5 49.4 77.0
ST5 26.2 57.7 50.6 79.4
SUBENCODER (GTR) 40.8 72.9 42.9 82.3
SUBENCODER (ST5) 41.0 72.2 43.5 81.4
ColBERTv2 46.9 74.2 54.7 87.8
Ours 47.7 74.7 55.0 87.4
Table 7: Evaluation results on the Atomic Fact Retrieval
task in PROPSEGM ENT(Chen et al., 2023a). The en-
coding level is individual sentences, with each sentence
consisting of multiple propositions. All models are
based on encoders with 110M parameters. Numbers for
GTR, ST5, SUBENCODER are from Chen et al. (2023b).
SUBENCODER at proposition-level (sub-sentence)
ranking. Although our proposed approach adds
a sentence-level constrastive loss at passage-level
encoding, we do see some improvements when
ranking at proposition-level. However, we hypoth-
esize that better proposition-level ranking can be
expected by further training with a proposition-
level loss in §3.3, which we leave for future work
to explore. Nevertheless, given the state-of-the-
art performance of multi-vector approaches at
proposition-level ranking, we introduce next (in
§4.3) a practical application leveraging this for the
task of adding citations to machine-generated text.
4.3 Sub-Query →Retrieval Unit Ranking for
Citation Addition
Retrieval-augmented generation (RAG) (Lewis
et al., 2020; Izacard et al., 2023) produces a long-
form answer to a query, given a set of relevant pas-
sages as input context. Here, we explore the ability
of multi-vector approaches to act as a citation addi-
tion approach for attribution in RAG. Specifically,
given a set of Kpassages, and the generated long-form answer, the task involves adding citations to
one or more of the input passages, for each sen-
tence in the generated answer.
We introduce PROPCITE, a post-hoc citation ap-
proach that adds citations to the input context sup-
porting propositions (atomic facts) in the generated
text. Specifically, PROPCITEmakes use of propo-
sitions tagged2within the generated sentences, so
that the corresponding sub-parts can be used as
the query to score the input passages and identify
the ones that need to be cited. Figure 4 illustrates
PROPCITE. Our approach is ‘post-hoc’ since ci-
tations are added after the text is generated, as
opposed to the typical approach of generating text
with citations by directly prompting the generation
model (Gao et al., 2023c) to add citations.
4.3.1 Setup
Datasets and Metrics We consider various long-
form question answering datasets, specifically
ASQA (Stelmakh et al., 2022) and ELI5 (Fan et al.,
2019). The RAG setting involves both K=5 and
K=10 passages as input to the language model
to generate the answer. The evaluation of attri-
bution quality is based on the citation precision
and recall metrics introduced in Gao et al. (2023c).
Citation recall determines if the output is entirely
supported by cited passages and citation precision
identifies any irrelevant citations. The metrics are
computed using TRUE (Honovich et al., 2022),
a 11B-parameter model trained on a collection
of natural language inference datasets, commonly
used (Bohnet et al., 2022; Gao et al., 2023b) to
evaluate attribution by checking whether the cited
passages entail the claims in the sentence.
2We employ the approach from Chen et al. (2023b), which
uses a T5 model (Raffel et al., 2020) to segment sentences
into propositions, that are then converted into token masks by
aligning the tokens in each proposition to the sentence.Baselines We compare PROPCITEagainst the
commonly used instruction-driven citation gener-
ation (Gao et al., 2023c), which we call Gener-
ate, where the generation model is prompted to
output text with citations. We use the same few-
shot prompt (provided in appendix) as Gao et al.
(2023c) to instruct the model to add citations while
generating the answer. We consider variants of
the generation model, a smaller34B Qwen1.5 (Bai
et al., 2023) and a larger 7B Mistral-Instruct (Jiang
et al., 2023). We also include comparison with
Self-RAG (Asai et al., 2023), which uses a self-
reflective generation framework to adaptively pick
input passages to generate from and thereby cite.
4.3.2 Results
Table 8 shows the citation precision (P) and recall
(R) numbers comparing the citation quality in the
generated text vs our post-hoc PROPCITEapproach.
Firstly, the ability to generate text with citations
depends heavily on the instruction-following capa-
bility of the generation model, with weaker models
such as Qwen1.5 4B considerably worse-off com-
pared to Mistral-Instruct 7B. Moreover, even the
citation quality of post-hoc approaches depends
on the quality of generated text, i.e. when weaker
models hallucinate or generate text that cannot be
supported by the input context, citation quality is
expected to be lower.
We observe that PROPCITEhas significantly bet-
ter citation quality on the 4B and 7B model gen-
erations. Even for the Self-RAG models, which
were explicitly finetuned for RAG by adding spe-
cial reflection tokens to cite passsages, we see im-
provements with PROPCITE. It is important to note
PROPCITEis post-hoc, and hence can be used with
any RAG framework, without needing to tweak the
generation model. Moreover, PROPCITEis light-
weight4and can post-hoc add citations as sentences
are generated one-by-one in a streaming setting.
4.3.3 Analysis
Table 9 shows results for an ablation study with
different variants of post-hoc citation addition to
demonstrate the benefits of using propositions
within generated sentences as the query. Firstly,
3We also experimented with Google’s Gemma 2B and
Microsoft Phi-2 models. Refusal rate was too high for Gemma
2B while Phi-2 had an input context length of only 2048.
4While we use a T5 model to explicitly segment sentences
into propositions, faster approaches relying on syntactic depen-
dency parsing (Goyal and Durrett, 2020; Wanner et al., 2024)
can be a cheaper alternative to get the sub-structures with a
sentence that represent the propositions or atomic claims.Generation
ModelPsg.Citation
MethodASQA ELI5
P R P R
Qwen1.5 4B5Generate 26.9 21.3 11.0 8.6
PROPCITE 48.9 54.5 19.5 23.4
10Generate 14.8 11.7 5.7 4.7
PROPCITE 45.3 52.0 18.3 22.9
Mistral 7B5Generate 64.9 69.5 40.5 49.0
PROPCITE 65.7 74.2 43.0 51.9
10Generate 60.2 66.7 38.0 48.8
PROPCITE 61.6 71.9 41.9 53.0
Self-RAG 7B
5Generate 67.9 67.1 - -
PROPCITE 68.5 68.4 - -
Self-RAG 13BGenerate 71.4 70.5 - -
PROPCITE 71.6 71.5 - -
Table 8: Table showing precision (P) and recall (R) for
different citation addition approaches on the long-form
ASQA (Stelmakh et al., 2022) and ELI5 (Fan et al.,
2019) question answering datasets. For Self-RAG, we
directly use generation outputs from Asai et al. (2023).
Setting Precision Recall
Generate 64.9 69.5
PROPCITE 65.7 74.2
+Thresholding 69.2 71.1
(i) Propositions as query 63.5 73.9
(ii) Sentence as query (top 1) 69.0 67.5
(iii) Sentence as query (top 2) 51.2 72.6
Table 9: Analysis of citation precision and recall perfor-
mance on ASQA for Mistral 7B when using top-5 pas-
sages as input. We consider different settings, wherein
the generated propositions or the sentence itself are used
as the query when searching for relevant citations.
we show numbers for a higher-precision version
ofPROPCITEthat incorporates thresholding5to
decide whether to add a citation for a given propo-
sition in the sentence. Next, we compare against
different variants that directly encode the proposi-
tion (i) or query using the entire sentence (ii, iii).
We can see that directly encoding the proposition,
instead of encoding the sentence and using tokens
corresponding to the proposition, leads to a drop
in precision. This supports our primary hypothesis
that encoding at lower-granularity (sentence-level
in this case) gives additional context to the token
vectors when used for querying at higher granular-
ity (proposition-level in this case). Figure 4 illus-
trates this with an example from the ASQA dataset.
PROPCITEcorrectly attributes proposition P2to
5To mitigate false positives, we only add a citation if the
top-scored passage has a relevance score margin of atleast 1.0.input passage C1which mentions U.S. Open as
the tournament in September that was won by Oui-
ment. However, directly encoding P2misses the
context that the tournament occured in September
and incorrectly attributes to input passage C2, that
mentions a different tournament, the Massachusetts
Amateur, that Ouiment won.
Moreover, we compare against an alternate ap-
proach that just uses the entire sentence as one
single query, instead of separately using the indi-
vidual propositions within the sentences as queries.
Tagging the top-1 scored passage as the citation
(ii) for that sentence gives a high precision but con-
siderably low recall, while tagging top-2 scored
passages as the citations does improve recall but
precision suffers a lot. Overall, with PROPCITE,
66% of sentences had 1 citation, 30% had 2 cita-
tions and remaining 4% had more than 2 citations.
5 Related Work
The phrase ‘multi-granularity’ can have different
meaning depending on the domain in which it is
being used. In space of image retrieval, it cor-
responds to representing different regions of the
image separately (Wang et al.; Zhang et al., 2022).
For representation learning, it refers to encoding
information at different granularities, i.e. output
embedding dimensions, to adapt to the computa-
tional constraints of downstream tasks (Kusupati
et al., 2022; Li et al., 2024). Our definition of
granularity in text ranking corresponds to ranking
relevant sub-units within a given retrieval unit.
Multi-vector approaches (Luan et al., 2021;
Khattab and Zaharia, 2020; Santhanam et al., 2022)
have primarily been used for ranking at the same
granularity as the level of encoding, which is typi-
cally passage-level. Single vector approaches, on
the other hand, inherently do not support rank-
ing at a finer granularity than the level of en-
coding, thereby needing a separate dense index
for each granularity (Chen et al., 2023c). Hence,
specialized models for single-vector embeddings
have be introduced for embedding phrases (Lee
et al., 2021), propositions (Chen et al., 2023b),
sentences (Reimers and Gurevych, 2019) or pas-
sages (Karpukhin et al., 2020). Our approach lever-
ages multi-vector approaches for ranking at differ-
ent granularities, while still encoding at a single
coarser level of granularity.
Prior approaches that score at different granular-
ities have leveraged custom scoring functions or in-corporate separate embeddings. Chang et al. (2023)
proposes a multi-granularity matching model that
uses a convolutional filter for scoring, instead of
dot similarity, meaning it cannot be scaled to a
retrieval-scale corpus due to the matching function.
Hierarchical ranking approaches (Liu et al., 2019;
Chu et al., 2022; Ma et al., 2024) also consider
multi-granular ranking but require use separate em-
beddings for each granularity to rank at. In con-
trast, our approach directly uses multi-vector em-
beddings from a single-level of encoding to rank
at any granularity. Further, our approach use a
dot product for scoring at all levels of granular-
ity, meaning the same pre-computed dense corpus
index can be used for any granularity.
6 Conclusion
In this work, we introduce AGR AME, which lever-
ages multi-vector embeddings to rank at finer gran-
ularities, while encoding is still at a single, coarser
level. Our proposed multi-granular contrastive loss
for training multi-vector approaches improves sen-
tence ranking performance even with passage-level
encoding. We demonstrate that AGR AME can
rank at any-granularity, even by using sub-parts
of the query for ranking. Leveraging multi-vector
approaches’ superior performance at proposition-
level ranking, our post-hoc attribution approach
uses propositions in the generated text to rank input
context passages to identify the relevant one to cite.
We show superior performance with PROPCITE
over the conventional approach of prompt-driven
citation in retrieval-augmented generation.
Acknowledgement
We would like to thank Omar Khattab and mem-
bers of the Blender NLP group for helpful com-
ments and feedback. We are also grateful to mem-
bers of the Apple Knowledge Platform team, es-
pecially Mostafa Arefiyan, Ihab Ilyas, Theodoros
Rekatsinas and Benjamin Han for early discus-
sions. This research is based on work supported
by U.S. DARPA KAIROS Program No. FA8750-
19-2-1004. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies, either expressed or implied, of DARPA,
or the U.S. Government. The U.S. Government is
authorized to reproduce and distribute reprints for
governmental purposes notwithstanding any copy-
right annotation therein.References
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2023. Self-rag: Learning to
retrieve, generate, and critique through self-reflection.
InThe Twelfth International Conference on Learning
Representations .
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023. Qwen technical report. arXiv
preprint arXiv:2309.16609 .
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
conference on empirical methods in natural language
processing , pages 1533–1544.
Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aha-
roni, Daniel Andor, Livio Baldini Soares, Massimil-
iano Ciaramita, Jacob Eisenstein, Kuzman Ganchev,
Jonathan Herzig, et al. 2022. Attributed question an-
swering: Evaluation and modeling for attributed large
language models. arXiv preprint arXiv:2212.08037 .
Guanghui Chang, Weihan Wang, and Shiyang Hu.
2023. Matchacnn: A multi-granularity deep match-
ing model. Neural Processing Letters , 55(4):4419–
4438.
Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan
Roth, and Tal Schuster. 2023a. Propsegment: A
large-scale corpus for proposition-level segmentation
and entailment recognition. In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,
pages 8874–8893.
Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou,
Wenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang,
Dan Roth, and Dong Yu. 2023b. Sub-sentence en-
coder: Contrastive learning of propositional semantic
representations. arXiv preprint arXiv:2311.04335 .
Tong Chen, Hongwei Wang, Sihao Chen, Wenhao
Yu, Kaixin Ma, Xinran Zhao, Dong Yu, and Hong-
ming Zhang. 2023c. Dense x retrieval: What re-
trieval granularity should we use? arXiv preprint
arXiv:2312.06648 .
Xiaokai Chu, Jiashu Zhao, Lixin Zou, and Dawei Yin.
2022. H-ernie: A multi-granularity pre-trained lan-
guage model for web search. In Proceedings of
the 45th International ACM SIGIR conference on
research and development in information retrieval ,
pages 1478–1489.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers) , pages 4171–
4186.Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. Eli5:
Long form question answering. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics , pages 3558–3567.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent
Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and
Kelvin Guu. 2023a. RARR: Researching and revis-
ing what language models say, using language mod-
els. In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 16477–16508, Toronto, Canada.
Association for Computational Linguistics.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-
cent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,
et al. 2023b. Rarr: Researching and revising what
language models say, using language models. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 16477–16508.
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.
2023c. Enabling large language models to generate
text with citations. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 6465–6488.
Izacard Gautier, Caron Mathilde, Hosseini Lucas,
Riedel Sebastian, Bojanowski Piotr, Joulin Armand,
and Grave Edouard. 2022. Unsupervised dense infor-
mation retrieval with contrastive learning. Transac-
tions on Machine Learning Research .
Tanya Goyal and Greg Durrett. 2020. Evaluating factu-
ality in generation with dependency-level entailment.
InFindings of the Association for Computational
Linguistics: EMNLP 2020 , pages 3592–3603.
Rujun Han, Peng Qi, Yuhao Zhang, Lan Liu, Juliette
Burger, William Yang Wang, Zhiheng Huang, Bing
Xiang, and Dan Roth. 2023. Robustqa: Benchmark-
ing the robustness of domain adaptation for open-
domain question answering. In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,
pages 4294–4311.
Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai
Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas
Scialom, Idan Szpektor, Avinatan Hassidim, and
Yossi Matias. 2022. TRUE: Re-evaluating factual
consistency evaluation. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 3905–3920, Seattle,
United States. Association for Computational Lin-
guistics.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-
Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave. 2023. Atlas: Few-shot learning with retrievalaugmented language models. Journal of Machine
Learning Research , 24(251):1–43.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 1601–1611.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6769–6781.
Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-
cient and effective passage search via contextualized
late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research
and development in Information Retrieval , pages 39–
48.
Aditya Kusupati, Gantavya Bhatt, Aniket Rege,
Matthew Wallingford, Aditya Sinha, Vivek Ramanu-
jan, William Howard-Snyder, Kaifeng Chen, Sham
Kakade, Prateek Jain, et al. 2022. Matryoshka repre-
sentation learning. Advances in Neural Information
Processing Systems , 35:30233–30249.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al. 2019. Natural questions: a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics , 7:453–
466.
Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi
Chen. 2021. Learning dense representations of
phrases at scale. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 6634–6647.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open do-
main question answering. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 6086–6096.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.Xianming Li, Zongxi Li, Jing Li, Haoran Xie, and
Qing Li. 2024. 2d matryoshka sentence embeddings.
arXiv preprint arXiv:2402.14776 .
Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023.
Evaluating verifiability in generative search engines.
InFindings of the Association for Computational
Linguistics: EMNLP 2023 , pages 7001–7025.
Wei Liu, Lei Zhang, Longxuan Ma, Pengfei Wang, and
Feng Zhang. 2019. Hierarchical multi-dimensional
attention model for answer selection. In 2019 In-
ternational Joint Conference on Neural Networks
(IJCNN) , pages 1–8. IEEE.
Yi Luan, Jacob Eisenstein, Kristina Toutanova, and
Michael Collins. 2021. Sparse, dense, and attentional
representations for text retrieval. Transactions of the
Association for Computational Linguistics , 9:329–
345.
Kai Ma, Junyuan Deng, Miao Tian, Liufeng Tao, Junjie
Liu, Zhong Xie, Hua Huang, and Qinjun Qiu. 2024.
Multi-granularity retrieval of mineral resource ge-
ological reports based on multi-feature association.
Ore Geology Reviews , page 105889.
Macedo Maia, Siegfried Handschuh, André Freitas,
Brian Davis, Ross McDermott, Manel Zarrouk, and
Alexandra Balahur. 2018. Www’18 open challenge:
financial opinion mining and question answering. In
Companion proceedings of the the web conference
2018 , pages 1941–1942.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
moyer, and Hannaneh Hajishirzi. 2023. Factscore:
Fine-grained atomic evaluation of factual precision
in long form text generation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 12076–12100.
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine read-
ing comprehension dataset. In CoCo@ NIPs .
Jianmo Ni, Gustavo Hernandez Abrego, Noah Con-
stant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang.
2022a. Sentence-t5: Scalable sentence encoders
from pre-trained text-to-text models. In Findings of
the Association for Computational Linguistics: ACL
2022 , pages 1864–1874.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Her-
nandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith
Hall, Ming-Wei Chang, et al. 2022b. Large dual
encoders are generalizable retrievers. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 9844–9855.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. Journal of Machine Learning Research , 21:1–
67.Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392.
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm,
Lora Aroyo, Michael Collins, Dipanjan Das, Slav
Petrov, Gaurav Singh Tomar, Iulia Turc, and David
Reitter. 2023. Measuring attribution in natural lan-
guage generation models. Computational Linguistics ,
49(4):777–840.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing . Associa-
tion for Computational Linguistics.
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,
Christopher Potts, and Matei Zaharia. 2022. Col-
bertv2: Effective and efficient retrieval via
lightweight late interaction. In Proceedings of the
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 3715–3734.
Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,
and Danqi Chen. 2021. Simple entity-centric ques-
tions challenge dense retrievers. In 2021 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP 2021 , pages 6138–6148. Association
for Computational Linguistics (ACL).
Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-
Wei Chang. 2022. Asqa: Factoid questions meet
long-form answers. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 8273–8288.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. Beir:
A heterogeneous benchmark for zero-shot evaluation
of information retrieval models. In Thirty-fifth Con-
ference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2) .
George Tsatsaronis, Georgios Balikas, Prodromos
Malakasiotis, Ioannis Partalas, Matthias Zschunke,
Michael R Alvers, Dirk Weissenborn, Anastasia
Krithara, Sergios Petridis, Dimitris Polychronopou-
los, et al. 2015. An overview of the bioasq large-scale
biomedical semantic indexing and question answer-
ing competition. BMC bioinformatics , 16:1–28.
Chengji Wang, Zhiming Luo, Yaojin Lin, and Shaozi
Li. Text-based person search via multi-granularity
embedding learning.
Miriam Wanner, Seth Ebner, Zhengping Jiang, Mark
Dredze, and Benjamin Van Durme. 2024. A
closer look at claim decomposition. arXiv preprint
arXiv:2403.11903 .
Jiacheng Zhang, Xiangru Lin, Minyue Jiang, Yue Yu,
Chenting Gong, Wei Zhang, Xiao Tan, YingyingLi, Errui Ding, and Guanbin Li. 2022. A multi-
granularity retrieval system for natural language-
based vehicle retrieval. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 3216–3225.