Discovering Knowledge-Critical Subnetworks
in Pretrained Language Models
Deniz Bayazit, Negar Foroutan, Zeming Chen, Gail Weiss, Antoine Bosselut
EPFL
{deniz.bayazit,antoine.bosselut}@epfl.ch
Abstract
Pretrained language models (LMs) encode im-
plicit representations of knowledge in their pa-
rameters. However, localizing these representa-
tions and disentangling them from each other
remains an open problem. In this work, we
investigate whether pretrained language mod-
els contain various knowledge-critical subnet-
works: particular sparse computational sub-
graphs that can, if removed, precisely suppress
specific knowledge the model has memorized.
We propose a multi-objective differentiable
masking scheme that can be applied to both
weights and neurons to discover such subnet-
works and show that we can use them to pre-
cisely remove specific knowledge from models
while minimizing adverse effects on the behav-
ior of the original model. We demonstrate our
method on multiple GPT2 variants, uncovering
highly sparse subnetworks (98%+ sparsity) that
are critical for expressing specific collections of
relational knowledge. When these subnetworks
are removed, the remaining network maintains
most of its initial abilities but struggles to rep-
resent the suppressed knowledge.1
1 Introduction
Large-scale language models (LLMs) encode large
amounts of relational knowledge (Petroni et al.,
2019; Carlini et al., 2023; Liu et al., 2023), which
they transfer to successfully adapt to downstream
tasks (Wang et al., 2019b,a). Following this suc-
cess, considerable research focuses on better un-
derstanding the extent to which LLMs capture this
knowledge (Liu et al., 2019a; Safavi and Koutra,
2021; Da et al., 2021; Huang et al., 2022). In
these works, relational triplets ( e.g.,(car, IsA,
vehicle) ) are converted to natural language ( e.g.,
“A car is a vehicle.” ) before being presented
to a model. Key tokens in these input sequences are
1The code is made available at https://github.com/
bayazitdeniz/know-subnet
ControlLMControlKGTargetKGInput ExamplesA cafe is a type of [__]A cafe is a type of [__]A cafe is a type of [__]A cafe is a type of [__]A cafe is a type of [__]A car is a [__]A cafe is a type of [__]A cafe is a type of [__]It is closely [__]Pruned Model≈≠≈PrunedvehiclerestaurantrelatedOriginalrestaurantrelatedvehicleSuppressionMaintenance-KGMaintenance-LMModel Behavior ObjectivesFigure 1: Knowledge-critical subnetworks are necessary
for expressing target knowledge triplets ( TARGET KG)
in LMs. When removed, the remaining model no longer
expresses the specific triplets, but maintains its ability
to express other relational knowledge ( CONTROL KG)
and its language modeling abilities (C ONTROL LM).
masked, and the model demonstrates its knowledge
of the relations by recovering these tokens.
With the body of work studying LLMs as knowl-
edge bases, a subset of works focuses on where and
how this knowledge may be encoded by the models
that capture it. The answer to these questions could
potentially facilitate the development of more ef-
fective finetuning methods, which can be useful for
rectifying factual errors made by language mod-
els, updating models with evolving knowledge, and
preventing ethically undesirable behavior.
Considerable work in model probing (Belinkov
and Glass, 2019; Durrani et al., 2020; Antverg
et al., 2022; Belinkov, 2022) and mechanistic in-
terpretability (Geva et al., 2021, 2022b,a) explores
these questions, discovering hidden representations,
neurons, and layers that are responsible for the ex-
pression of knowledge from these systems. How-
ever, these works typically do not localize thearXiv:2310.03084v2  [cs.CL]  15 Oct 2024knowledge accessing behavior to individual pa-
rameters. Another line of work in model editing
explores whether knowledge in the model can be
changed (De Cao et al., 2021; Dai et al., 2022; Hase
et al., 2023b; Mitchell et al., 2022a,b; Meng et al.,
2022, 2023; Hase et al., 2023a; Gupta et al., 2023;
Jang et al., 2023; Chen et al., 2023). However,
the goal of these methods is also typically not to
precisely localize the parameters responsible for
expressing knowledge, but instead to broadly edit
model parameters such that a new desired behavior
overwrites the model’s preference for the old one.
In this work, we hypothesize that any piece of re-
lational knowledge expressed by a language model
is encoded by a limited subset of its parameters. We
search for these parameters by identifying sparse
subnetworks that, when removed, suppress the
model’s ability to express the knowledge of interest
while not affecting other abilities of the model. As
the model cannot express target knowledge without
these subnetworks, we refer to them as knowledge-
critical . In Figure 1, we illustrate this concept –
when the weights marked with a red cross are re-
moved from the original network, the expression
of the triplet (cafe, IsA, restaurant) is sup-
pressed, whereas other triplets are not.
To discover knowledge-critical subnetworks, we
propose training differentiable masks over weights
or neurons of the original pretrained model, such
that the mask can identify and remove a knowledge-
critical subnetwork for the targeted knowledge
graph. Specifically, we train the mask to: (1)
suppress the expression of the target knowledge
triplets, (2) maintain the ability to express generic
relational knowledge and language, and (3) remove
only a minimal subset of weights. After train-
ing, the remaining pruned model can no longer
express the target knowledge, but maintains its per-
formance on other behaviors, thereby identifying
theknowledge-critical subnetwork as the masked
portion of the original model.
Our results — across multiple target knowledge
graphs (constructed from WordNet and Concept-
Net) and LLMs at multiple scales (from the family
of GPT2 models) — show that weight masking
consistently identifies sparse subnetworks (an av-
erage sparsity of ∼98.6%) that satisfy our objec-
tives. When these subnetworks are removed, the
remaining model’s perplexity on the target knowl-
edge associated with the subnetwork largely in-
creases (an average relative perplexity increase of
253% - 5589% for different GPT2 models), indicat-ing that the expression of the target knowledge is
successfully suppressed. However, the remaining
network’s ability to model generic relational knowl-
edge and natural language negligibly changes. Fi-
nally, in a study on CommonsenseQA, we show
that once these subnetworks are removed, models
finetuned using parameter-efficient methods strug-
gle with questions that require the knowledge en-
coded by the removed subnetworks.
2 Related Work
LLMs as Knowledge Bases Our work builds
on prior research that demonstrates the knowledge
memorization abilities of large language models
(LLMs; Carlini et al., 2021; AlKhamissi et al.,
2022). Multiple studies have shown that LLMs
encode various types of knowledge (Liu et al.,
2019a; Chen and Gao, 2022; Safavi and Koutra,
2021; Huang et al., 2022). In these works, para-
metric knowledge in LLMs is typically expressed
by conditioning on a natural language context to
complete or infill a sequence that expresses the
knowledge (Petroni et al., 2019; Jiang et al., 2020;
Shin et al., 2020; Cao et al., 2021a; Zhong et al.,
2021; Qin and Eisner, 2021; Liu et al., 2023; Yu
et al., 2023). Other methods also fine-tune mod-
els to create an interface to parametric knowledge
(Bosselut et al., 2019; Roberts et al., 2020; Jiang
et al., 2021; Hwang et al., 2021). In contrast, our
work investigates where knowledge is encoded by
LLMs and localizes the critical subnetworks for
expressing these facts.
Function-Specific Subnetworks Methodologi-
cally, our work draws inspiration from studies that
identify task-specific subnetworks in neural net-
works. Perhaps most known, Frankle and Carbin
(2019) propose the Lottery Ticket Hypothesis , show-
ing that learned subnetworks could achieve test ac-
curacy similar to that of original networks. Other
works prune subnetworks for the purpose of effi-
cient finetuning (Mallya et al., 2018; Zhao et al.,
2020; Sanh et al., 2020; Guo et al., 2021), or iden-
tifying function-specific subnetworks (Cao et al.,
2021b; Sanh et al., 2020; Zhang et al., 2021; Csor-
dás et al., 2021). Identifying function-specific sub-
networks also leads to useful applications, such as
disentangling representations to reduce model sus-
ceptibility to spurious correlations (Zhang et al.,
2021), probing models for linguistic properties
(Cao et al., 2021b; De Cao et al., 2020), identifying
and removing a toxic behavior or bias (Li et al.,2024; Chintam et al., 2023), and finding subnet-
works specialized for different languages (Foroutan
et al., 2022). Most similar to our work is that of Ren
and Zhu (2022), which learns coarse subnetworks
that encoded large portions of ConceptNet. We also
adopt a differentiable weight masking scheme, but
use it to identify highly sparse subnetworks critical
for particular expressions of knowledge.
Mechanistic Interpretability Mechanistic inter-
pretability tackles the problem of understanding
model behavior by reverse-engineering computa-
tions performed by transformer models. Elhage
et al. (2021) discovered algorithmic patterns and
frameworks in simplified transformer models. Fol-
lowing this, researchers discovered induction heads
(Olsson et al., 2022), i.e., specific attention heads
involved in in-context learning in LLMs. Similarly,
with interventions on attention and MLP sublayers,
Geva et al. (2023) identified critical points where
the model propagates information, as well as the
internal mechanism for attribute extraction. Other
work focuses on knowledge tracing and localiza-
tion in model parameters for the goal of model
editing (Dai et al., 2022; Meng et al., 2022, 2023;
Gupta et al., 2023; Hernandez et al., 2024). Acti-
vation patching with corrupted tokens (Meng et al.,
2022) or corrupted prompts (Wang et al., 2023) use
causal intervention to identify activations responsi-
ble for flipping the model’s output. In contrast, our
work focuses on preserving the original model to
precisely locate individual model weights responsi-
ble for expressing a given set of target knowledge
without counterfactuals. Our work is closer to path
patching (Goldowsky-Dill et al., 2023) and auto-
matic circuit discovery (Conmy et al., 2023) to lo-
calize behaviors in network subgraphs but focuses
specifically on identifying subnetworks associated
with knowledge relationships. Our work is also
similar to Lo et al. (2024), which shows that mod-
els can re-learn removed concepts via neurons. In
contrast, we focus on individual parameter pruning.
3 Background & Considerations
To find a knowledge-critical subnetwork in a pre-
trained language model, we learn a differentiable
parameter mask (§4) using a prediction task where
the LM is prompted for relational knowledge.
Prompting LMs with KGs We define a global
relational knowledge graph (KG) as the set ofknowledge triplets,
K={(h1, r1, t1), ...(hn, rn, tn)}
where handtare head and tail entity nodes, re-
spectively, and ris the relation that holds between
the two entities. To input relational knowledge to
an LM, triplets are verbalized using a natural lan-
guage template. For example, the triplet (house,
IsA, building) , can be verbalized with the tem-
plate“{article} { h} is {article} { t}”as“A
house is a building.” A typical way to prompt
for knowledge is to mask the tail entity “A house
is a ___” (Petroni et al., 2019). To approximate
an autoregressive model’s confidence in a given
triplet, we compute a distribution over the missing
token and calculate the perplexity of the model for
the correct token building .
Differentiable Weight Masking for Function-
Specific Parameter Search To localize param-
eters that are critical for modeling specific knowl-
edge, we learn a binary mask over each network
parameter. For a language model f(x,θ)with pre-
trained parameters θthat takes as input x, we learn
a set of binary parameters m∈ {0,1}|θ|that is
element-wise multiplied with the frozen θ, such
that our subnetwork is formulated as f(x,m⊙θ).
Similar to other binary mask learning methods (Cao
et al., 2021b; Sanh et al., 2020), our method mod-
els each parameter mask miwith the concrete ( i.e.,
Gumbel-Softmax) distribution, a differentiable ap-
proach to learn continuous mask scores si∈[0,1]
from real-valued parameters li∈R(Maddison
et al., 2017; Jang et al., 2017):
si=σ((li−log(log U1/logU2))/τ)(1)
where U1,U2∼ U(0,1)andσis a sigmoid func-
tion. We use the approach of Csordás et al. (2021),
which uses a straight-through estimator that thresh-
olds the continuous score (Bengio et al., 2013):
mi= [1si>0.5−si]detach +si (2)
where 1is an indicator function that thresholds
the scores at 0.5 and []detach is an operation that
prevents back-propagation. This way, we back-
propagate through the non-detached continuous
mask scores siand still calculate loss with the
overall binarized mask score mi.
Mask Granularity Discovering subnetworks re-
quires selecting the granularity of the parametermask, reflecting the granularity at which we hy-
pothesize separable knowledge representations can
be discovered in the model. Most prior work se-
lects neurons (Elhage et al., 2022) or layers (Zhou
et al., 2023) as the basic structural unit for localiz-
ing model behaviors. While these representations
have been shown to encode knowledge behaviors
(Dai et al., 2022; Lo et al., 2024), they are per-
haps too broad for reliably disentangling specific
knowledge, as they are typically polysemantic ( i.e.,
they jointly encode multiple behaviors; Olah et al.,
2020). Conversely, localizing knowledge repre-
sentations as an unconstrained combination of in-
dividual parameters is likely more separable, but
may be noisy, as many parameters may be largely
redundant, and individual parameters may suffer
from overfitting. With no clear choice, in this work,
we explore both parameter-level and neuron-level
masking to provide complementary insights for
mechanistic knowledge localization.
4 Methodology
This section defines our methodology for discover-
ingknowledge-critical subnetworks using differen-
tiable weight or neuron masking.
Notation We define a subnetwork as in §3:
f(x,m⊙θ), where θis the set of parameters of
the network fandmis the mask over a portion of
that network’s parameters. To learn a mask over
neurons, we jointly mask all the weights connect-
ing to the same neuron. We assume a target set of
knowledge KT⊂K(TARGET KG) for which we
want to identify the critical parameters.
4.1 Knowledge-Critical Subnetworks
Our goal is to find knowledge-critical subnetworks:
the essential parameters to express a given set of
target knowledge. When knowledge-critical sub-
networks are removed, the expression of the target
triplets should be suppressed, and the expression
of irrelevant triplets should be unaffected.
Suppression Forf(x,m⊙θ)to be critical in
expressing KT, its removal from the original net-
work should also suppress the model’s ability to ex-
press the knowledge in KT. More formally, the in-
versely masked subnetwork ( i.e., remaining model),
f(x,˜m⊙θ), where ˜m= 1−m, should have
difficulty expressing KT. We define this as the
suppression criterion, as it encourages that the re-
maining model cannot represent knowledge in KT.
If we find such a disentanglement, we consider thatthe pretrained model heavily relies on the removed
subnetwork to perform a task related to KT.
Maintenance However, if only optimized for
suppression , our method may discover subnet-
works that are critical to all expressions of knowl-
edge, or all expressions of coherent sequences
of language. As the model should retain its ini-
tial capacities, we also define maintenance cri-
teria for knowledge-critical subnetworks. They
should: (1) not affect the model’s ability to ex-
press other relational knowledge KC=K\KT
(CONTROL KG), and (2) not affect the model’s orig-
inal language modeling abilities ( CONTROL LM).
These criteria are referred to as maintenance-KG
andmaintenance-LM , respectively.
Sparsity Finally, we aim to keep the knowledge-
critical subnetwork as sparse as possible to discover
the parameters that predominantly encode the ex-
pression of KT. Without imposing a high sparsity
level, parameters unrelated to the expression of KT
orKCmight persist within the subnetwork.
4.2 Mask Learning
To learn a weight mask for knowledge-critical sub-
networks, we define a joint objective that optimizes
for the criteria defined above.
Suppression Loss To fulfill the suppression cri-
terion, the remaining model, f(x,˜m⊙θ), should
be less confident in the expression of knowledge in
KT. We propose to minimize the KL divergence
between the remaining model’s predicted distribu-
tion over possible tail entities of a knowledge triplet
and a uniform reference distribution UVover the
tokens in the model’s vocabulary. For x∈KT:
Lsuppress =DKL(UV∥f(x,˜m⊙θ)) (3)
Maintenance Losses As there are multiple ways
a model could learn to suppress the expression KT,
namely (1) suppressing all knowledge that is in
the same format or (2) suppressing all language
expressions, we define two regularization objec-
tives. To encourage the rest of the model to keep its
original performance on the control knowledge KC
and a standard language modeling dataset DLM,
we calculate the KL divergence of f(x,˜m⊙θ)
with the pretrained model’s distribution f(x,θ)as
a reference. Thus, for any x∈KCorx∈DLM:
Lmaintain =DKL(f(x,θ)∥f(x,˜m⊙θ)) (4)
We define two such loss terms, one for each of
maintenance-KG andmaintenance-LM .Knowledge Graph # triplets # heads # tails # relsGPT-2 PPL
Small Med Large XL
WordNetCONTROL KG train 9751 9707 2709 1 63.6 32.8 27.4 24.5
CONTROL KG val. 50 50 50 1 73.2 37.5 31.3 30.8
building 11 11 11 1 51.9 - - -
communication 16 16 9 1 96.3 65.2 69.2 59.8
change 13 13 13 1 109.7 - - -
statement 16 16 16 1 170.2 - -
location 19 19 7 1 198.0 119.0 125.5 81.4
representation 12 12 12 1 210.7 106.8 108.7 85.0
magnitude 12 12 7 1 299.9 - - -
ConceptNetCONTROL KG train 5455 2898 2129 16 373.0 - - -
CONTROL KG val. 606 522 482 16 172.3 - - -
fruit 36 11 37 12 381.6 - - -
sun 36 11 36 12 387.5 - - -
swimming 40 14 40 15 517.8 - - -
Table 1: Statistics on sampled KGs and their verbalization. The graph statistics show the number of triplets
and the unique number of heads, tails, and relations. The average perplexity is calculated with the gold tail token
cross-entropy loss. The perplexity for certain KGs in the Medium, Large and XL columns are not included as we do
not evaluate them in our study on model scale.
Sparsity Regularization To promote the subnet-
work containing only parameters critical for mod-
eling TARGET KG, we encourage sparsity by min-
imizing the average subnetwork density ( i.e., sig-
moid of the masking parameters lifrom Eq. 1):
Lsparsity =1
|θ||θ|X
i=1σ(li) (5)
Final Loss Our final loss is a mixture of these
losses with weights λi:
Lfinal=λ1Lsuppress +λ2Lmaintain-KG
+λ3Lmaintain-LM +λ4Lsparsity(6)
5 Experimental Setup
Models & Training To test whether our method
can scale to various model sizes, we discover
knowledge subnetwork masks for GPT2-small,
(117M parameters, 12 layers), GPT2-medium,
(345M parameters, 24 layers), GPT2-large, (774M
parameters, 36 layers), and GPT2-XL. (1.5B pa-
rameters, 42 layers; Radford et al., 2019). During
mask learning, we do not mask the embedding,
language modeling head, layer-normalization, and
bias parameters,2and only learn masks for the top
50% of transformer layers.3Further implementa-
tion details on masking, hyperparameter, and check-
point selection are in Appendix B.
2Prior work has not observed an advantage to masking
these components for general tasks (Zhao et al., 2020).
3Multiple layer-wise analyses have shown that the first
layers of transformer LMs encode low-level linguistic features
that may be a prerequisite for knowledge modeling (Tenney
et al., 2019; Liu et al., 2019a). We also perform a masked
layer choice study that confirms this intuition (Appendix C).Datasets To create TARGET KG and CON-
TROL KGs, we sample hypernym triplets from
WordNet (Miller, 1995), as well as triplets from the
LAMA subset of ConceptNet (Speer et al., 2017;
Petroni et al., 2019). For simplicity, we only use
triplets with single-token tail entities. We sample 7
TARGET KGs for WordNet, and 3 for ConceptNet
(statistics shown in Table 1) by randomly selecting
an initial node and sampling knowledge triplets by
performing 3-hop random walks in both the par-
ent and child direction of the KG. To create CON-
TROL KG, we prioritize not leaking TARGET KG
counterfactuals and having a shared CONTROL KG
across different TARGET KGs, and remove from
the complete KG any triplet that shares the same
entities as the union of the TARGET KGs shown
in Table 1. For all triplets, to suppress and main-
tain knowledge that the model is already confident
about, we select the verbalization for each triplet
with the lowest perplexity on the tail token. For the
CONTROL LMdataset, we use WikiText-2 (Merity
et al., 2017). We refer to CONTROL KGandCON-
TROL LM together as maintenance datasets. All
maintenance results are on a held-out validation
set. Further information on data preprocessing is
in Appendices A and B.
Metrics We follow prior work (Hase et al.,
2023a) that considers perplexity (PPL) as a proxy
for a model’s confidence in the expression of knowl-
edge, and calculate the perplexity difference be-
tween the remaining and original models, ∆PPL
= PPL( f(x,˜m⊙θ)) - PPL( f(x,θ)). We also re-
port∆Rank, the tail token rank difference betweenKnowledge Mask Sparsity T ARGET KG C ONTROL KG C ONTROL LM T ARGET KG C ONTROL KG
Graph Method ( ↑) ∆PPL (↑) ∆PPL (↓) ∆PPL (↓) ∆Rank ( ↑) ∆Rank ( ↓)
WordNetWeight Masking 98.6 590.9
(162.4)-0.2
(73.2)0.5
(37.7)320.9
(45.4)1.4
(9.7)Neuron Masking 95.3 715.9 22.2 4.1 288.7 7.1
Random Weights 98.6 24.3 14.6 2.2 12.0 2.7
Random Neurons 95.3 23.8 8.3 8.3 17.0 4.5
ConceptNetWeight Masking 99.1 636.4
(429.0)2.8
(172.3)0.2
(37.7)636.1
(290.6)1.6
(47.5)Neuron Masking 94.9 22422.0 71.9 5.4 8720.2 29.4
Random Weights 99.1 21.0 14.6 1.5 11.4 5.5
Random Neurons 94.9 110.7 70.4 11.2 35.9 28.5
Table 2: Subnetwork discovery for GPT2-small, averaged over three seeds and seven KGs for WordNet, and
three KGs for ConceptNet. ∆PPL = PPL( f(x,˜m⊙θ)) - PPL( f(x,θ)) and similarly for ∆Rank results. The values
in parenthesis are the average metric (PPL or Rank) for the pretrained model ( i.e., the base from which the ∆is
computed). The arrows ( ↑,↓) show the desired direction for the metric. Random is an average of randomly masked
baselines at the same sparsity levels as the discovered knowledge-critical subnetworks for each KG-seed pair.
the remaining and original models. For the sup-
pression andmaintenance-KG criteria, we calcu-
late∆PPL using the loss on the masked tail entity
for triplets in the TARGET KGandCONTROL KG
datasets. For a knowledge-critical subnetwork, we
expect ∆PPL and ∆Rank values to be high for
TARGET KGand low for CONTROL KG. For the
maintenance-LM criterion, we calculate ∆PPL as
the average perplexity on all tokens in a sequence,
which should be low if removing the critical subnet-
work does not affect the model’s general language
modeling ability.4For the sparsity criterion, we
calculate the percentage of parameters that were
not pruned. The denominator is the number of
masked parameters, meaning the total size of dense
layers in the upper half of the model. Ideally, the
sparsity should be as high as possible to keep the
majority of parameters ( i.e., near 99%).
Baseline We use weight and neuron masking to
localize knowledge-critical subnetworks. As a con-
trol baseline, we create randomly masked models
at the same sparsity level as the knowledge-critical
subnetwork. If the discovered subnetwork is crit-
ical for expressing TARGET KG, then removing a
random subnetwork at the same weight or neuron
sparsity should yield lower corruption for express-
ingTARGET KG(i.e., lower ∆PPL) than remov-
ing the critical subnetwork. Similarly, if the crit-
ical subnetwork successfully preserves the main-
tenance criteria, a random subnetwork should be
more likely to prune useful weights for expressing
CONTROL KGandCONTROL LM, which should
lead to a higher ∆PPL on maintenance datasets.
4We do not report ∆Rank for maintenance-LM as the
average rank of all tokens in an open-ended sentence is not as
informative as the single tail token rank.Further information on the implementation of the
random masking baseline is in Appendix B.
6 Experimental Results
We first evaluate the degree to which discovered
subnetworks are knowledge-critical.
Weight-masked Subnetworks In Table 2, we ob-
serve that across seven different knowledge graphs
(TARGET KGs) and three random seeds, the sub-
networks found with weight masking consistently
achieve a notably high sparsity ( >98%).5For the
suppression criterion, we notice a high ∆PPL on
TARGET KGfor both approaches, meaning that the
perplexity of the remaining model on TARGET KG
is significantly higher than the pretrained model’s
perplexity. In contrast, removing a random subnet-
work at the same sparsity yields a smaller perplex-
ity increase, meaning the discovered subnetworks
are significantly more critical for expressing TAR-
GETKG. At the same time, we find little change
in perplexity on the maintenance datasets for re-
lational knowledge ( CONTROL KG) and language
modeling ( CONTROL LM), demonstrated by the
negligible ∆PPL on both datasets and the small
∆Rank value on CONTROL KG.6We note that a
negative ∆PPL here may result from the remain-
ing model slightly overfitting to the CONTROL KG
distribution, although it is never too significant.
We observe similar results for knowledge-
critical subnetworks for larger models. For three
5Table 13 provides individual KG results for the averaged
weight masking results in Table 2.
6Note that the lower average PPL of CONTROL KGcom-
pared to TARGET KGis due to CONTROL KGbeing larger,
which minimizes the impact of outliers and reduces average
perplexity.AblationSparsity T ARGET KG C ONTROL KG C ONTROL LM
(↑) ∆PPL (↑) ∆PPL (↓) ∆PPL (↓)
No Suppression 99.5 [99.5, 99.5] -7.2 [-11.9, -3.7] -3.2 [-3.2, -3.2] 0.2 [0.2, 0.2]
No Maintenance-LM 99.2 [99.0, 99.3] 259.8 [-1.5, 401.7] 9.0 [-3.6, 25.1] 25.9 [24.7, 27.3]
No Maintenance-KG 99.8 [99.8, 99.8] 21141.1 [16885.9, 25471.8] 1697.5 [1334.6, 2180.1] 0.2 [0.2, 0.2]
Our Method 98.6 [97.8, 99.1] 378.1 [74.3, 834.9] 1.6 [-0.7, 4.0] 0.5 [0.3, 0.8]
Table 3: Ablation study for the multi-objective loss on GPT2-small using weight masking, with [min, max]
boundaries, averaged across three KGs and two seeds.
TARGET KGs:communication ,representation ,
andlocation , we observe an average increase in
TARGET KGperplexity of 256 for GPT2-medium,
5780 for GPT2-large, 536 for GPT2-XL, and a
negligible maintenance ∆PPL (Table 16).
Neuron-masked Subnetworks On the other
hand, neuron masking does not reliably fulfill the
conditions of discovering knowledge-critical sub-
networks. While removing neuron-masked subnet-
works yields greater suppression of TARGET KG
than weight masking, it also significantly impacts
CONTROL KG∆PPL and ∆Rank (more than ran-
domly removing neurons at the same sparsity), in-
dicating that other behaviors of the model are not
robustly maintained. They also tend to be less
sparse, frequently keeping ∼5% of the parameters
of the original model.7We hypothesize that this
observation is potentially related to neuron superpo-
sition (Elhage et al., 2022), where the neurons that
represent TARGET KGcannot be fully disentangled
from representations that encode general relational
knowledge. While weights may also be polyseman-
tic, they are more fine-grained, potentially encod-
ing knowledge in a more separable manner.
Ablation Study As our method relies on a joint
objective combining multiple loss functions, we
perform an ablation study of the loss terms pre-
sented in §4.2 for weight masking and remove each
objective (i.e., No Suppression, No Maintenance-
KG, No Maintenance-LM) to validate whether
these losses accomplish their goals.8In Table 3, we
observe that the suppression loss is necessary to
increase TARGET KGperplexity (and suppress the
knowledge). Without it, the model only optimizes
for retaining CONTROL KG, and generalizes this
improvement to T ARGET KG as well (as indicated
by the negative ∆PPL). We also find that removing
7Appendix Table 14 provides individual KG results for the
averaged neuron masking results in Table 2.
8We do not ablate the sparsity term. Without it, the sub-
network search stagnates at the initial sparsity.the maintenance losses significantly affects CON-
TROL KGandCONTROL LMperplexity differences.
Without these controls, our method learns to sup-
press the knowledge from the model by suppress-
inggeneral abilities . The suppression objective,
a minimization of the KL divergence between the
output distribution and a uniform distribution, af-
fects the prediction of tail entities for all relational
knowledge rather than affecting only TARGET KG.
We present additional ablations related to varying
the training objectives in Appendices B (varying
λiin Eq. 6) and F (adding additional loss terms).
Paraphrase Generalization To assess whether
our subnetworks generalize to other verbalizations
ofTARGET KGandCONTROL KG, we evaluate the
pruned models on 20 other distinct relation para-
phrases that are not used during training. Specifi-
cally, we vary the tokens representing the relation
and the format of the head and tail entities while
still ensuring grammatical correctness.9For weight
masking, our conclusions do not change when us-
ing other prompt styles, as seen in Table 4. Inter-
estingly, the ∆PPL for CONTROL KGparaphrases
is sometimes lower than for the format used for
training, likely because the starting perplexity is
higher on other templates,10and the maintenance
ofCONTROL KGgeneralizes to a greater degree
on these suboptimal templates. The neuron mask-
ing approach generalizes well to TARGET KGpara-
phrases, but poorly for CONTROL KGtemplates,
reinforcing our previous observations.
6.1 Subnetwork Analysis
Subnetwork Structure To better understand
how knowledge-critical subnetworks interact with
the rest of the model, we explore their structure in
9For further details and examples on the creation of these
verbalizations, please refer to Appendix A.
10Recall that for every triplet, we use the verbalization with
the lowest original model perplexity for training. The average
perplexity of GPT2-small on the worse paraphrases is 3231
for T ARGET KG and 2368 for C ONTROL KG.Mask Knowledge T ARGET KG C ONTROL KG
Method Graph ∆PPL (↑) ∆PPL (↓)
Weightcommunication 492.4 3.0
location 684.9 -33.0
representation 916.4 -6.9
Neuroncommunication 866.8 -325.0
location 941.6 284.5
representation 1928.9 654.5
Table 4: Paraphrase results on GPT2-small.
Mask Composition Sparsity T ARGET KG C ONTROL KG C ONTROL LM
Pattern ( ↑) ∆PPL (↑) ∆PPL (↓) ∆PPL (↓)
WeightIndividual 98.8 379.1 0.7 0.4
Union 96.9 1984.4 44.9 4.7
Floral 99.5 4.9 4.5 1.1
Intersection 99.9 7.9 3.7 2.1
NeuronIndividual 95.2 396.0 22.3 4.1
Union 92.8 2743.0 86.9 6.6
Floral 95.5 290.5 16.1 3.7
Intersection 97.3 63.9 3.5 2.4
Table 5: Composing subnetworks with GPT2-small.
Individual stands for the individual subnetwork removal
average across the same three seeds and KGs.
the parameter space of the original model. For three
WordNet TARGET KGs and three random seeds, we
find that GPT2-small subnetworks are relatively
denser in the first and final masked transformer
blocks. For weight masking, more density is ob-
served in the attention sublayers (Figure 3). Inter-
estingly, much of the density of the subnetworks
in the attention sublayers is tied to individual atten-
tion heads (Figure 5), supporting prior conclusions
that particular attention heads encode semantic re-
lationships (Clark et al., 2019; Geva et al., 2023).
However, despite being dense around similar por-
tions of the model across different TARGET KGs
and random seeds, the subnetworks are quite dis-
tinct. When we calculate the Jaccard similarity
(i.e., IoU) of the individual parameters across sub-
networks for different random seeds for the same
TARGET KG, the result is quite low on average
for weight-masked subnetworks (3-4%) — though
higher for the final attention output sublayer (10-
12%) — indicating the knowledge-critical subnet-
works are quite disjoint, even when discovered by
suppressing the same information (Figure 10).
Neuron masking led to a much higher density in
the second feedforward layers of the transformer
blocks and attention layers (Figure 4). We find
that the IoU of neuron-masked subnetworks are
also 10 ×higher (34-44%; Figure 11), partially
due to their reduced sparsity, but also perhaps in-
dicating that neuron masking yields more unique
subnetworks across seeds, though they are also less
reliably knowledge-critical.Subnetwork Composition However, even
though knowledge-critical subnetworks across
random seeds may be disentangled, composing
them (and removing them jointly) amplifies the
suppression effect. As shown in Table 5, when
we compose subnetworks for GPT2-small as a
union of three random seed masks for the same
TARGET KG, the suppression effect increases
significantly, by a factor of 6 ×(far more than
removing additional random parameters from
the remaining model; Figure 2). While this
suppression is accompanied by a degradation in
the maintenance criteria ( ∼30-40 ∆PPL on CON-
TROL KGinstead of near 0), the absolute difference
is far smaller. Composing neuron-masked subnet-
works yields similar trends, though we observe two
interesting patterns. First, the intersection of these
subnetworks produces a subnetwork that satisfies
the maintenance criteria to be knowledge-critical ,
though at the cost of reducing suppression. Second,
neuron-masked compositions yield monotonic
changes in suppression and maintenance scores as
sparser composition methods are used. Further
analyses on seed-based and knowledge-based
variance across discovered subnetworks are in
Appendix I and J, respectively.
Subnetwork Sensitivity Finally, we investigate
whether discovered subnetworks are structurally
sensitive. Specifically, we perform a sensitivity
analysis of the recorded metrics as we iteratively
expand or contract the subnetwork (by adding or
removing parameters). As we add parameters to
the subnetwork ( i.e., remove parameters from the
remaining model), we measure the change in TAR-
GETKG∆PPL. In this case, a sudden drop in
∆PPL would indicate that the discovered subnet-
work is spurious. In Appendix Figure 2, we observe
that expanding the discovered subnetwork in small
amounts does not significantly recover the model’s
ability to express TARGET KG, providing further
evidence that the subnetworks are not arbitrarily
discovered, but rather have meaningful knowledge-
expressing structure within the larger model. We
provide more experimental details in Appendix G.
6.2 Downstream Task Transfer
If a subnetwork is truly knowledge-critical, its re-
moval should harm a pretrained language model’s
ability to transfer to a downstream task requiring
the knowledge encoded by the subnetwork. To
test this hypothesis, we finetune a model on theCommonsenseQA benchmark (Talmor et al., 2019)
after removing a relevant knowledge-critical sub-
network. We use the in-house splits from Lin et al.
(2019), with a development set of 1241 and an ini-
tial test set of 1221 questions. In the test set, we
induce11the ConceptNet relation linked to each
question and extract the relevant triplets from Con-
ceptNet, creating a T ARGET KG from all Concept-
Net triplets associated to the test set, which yields
afiltered set of 363 questions for which we can
reliably extract relevant ConceptNet triplets. We
use these relevant triplets as TARGET KGand the
remaining distinct triplets in the LAMA subset of
ConceptNet as CONTROL KGto learn a knowledge-
critical subnetwork using either weight and neuron
masking for GPT2-small. Then, we apply different
finetuning methods to the remaining model after
removing the critical subnetwork, using the same
training set. We compare finetuning the remain-
ing masked model (Weight Mask, Neuron Mask
in Table 6) to the performance of finetuning the
full pretrained model (Full), as well as a randomly
masked model at the same sparsity as the masked-
weight subnetwork (Random Mask). We report
results across three random seeds in Table 6.
For all finetuning methods, we find that the re-
maining model with weight masking has similar
accuracy to the pretrained model on the develop-
ment split and a close accuracy for the overall test
set. However, we observe a consistent significant
performance drop on the filtered subset after fine-
tuning (average drop of 7.3%; head tuning barely
better than selecting a random answer on a 5-choice
MCQA task), indicating that the model struggles
to transfer knowledge associated with TARGET KG
during fine-tuning. Interestingly, in less parameter-
efficient finetuning methods, this drop does not
persist when the neuron-masked subnetwork is re-
moved, suggesting that knowledge is either still
transferred or recovered over the course of finetun-
ing (Lo et al., 2024). In addition, for both head
tuning and LoRA (Hu et al., 2022) with weight
masking, we find that if we randomly split the fil-
tered TARGET KG, one half’s knowledge-critical
mask does not affect the accuracy of the other half
as significantly as its own (see Appendix E for de-
tails), indicating the performance drop is indeed
specific to the pruned knowledge.
11We describe this process in Appendix E.Tuning Method Subnetwork Dev Test Filtered
Head
TuningFull 38.63 38.33 37.19
Random Mask -0.47 -1.61 -3.21
Neuron Mask -3.74 -4.22 -6.61
Weight Mask -1.69 -6.80 -14.42
LoRAFull 50.04 48.64 48.67
Random Mask -0.74 -2.33 -1.75
Neuron Mask -0.30 +1.89 +1.28
Weight Mask -1.83 -2.74 -3.95
Full
FinetuningFull 44.61 42.33 42.79
Random Mask +0.30 -0.24 +2.39
Neuron Mask +0.38 +2.17 +1.47
Weight Mask -1.50 -5.14 -3.60
Table 6: Accuracy on CommonsenseQA, averaged
over three seeds for GPT2-small.
7 Conclusion
In this paper, we conceptualize knowledge-critical
subnetworks, sparse computational subgraphs
within larger language models that are respon-
sible for expressing specific knowledge relation-
ships. We discover these subnetworks using a
multi-objective differentiable masking approach
that jointly optimizes a criterion designed to sup-
press the expression of target knowledge when
knowledge-critical subnetworks are removed from
a language model, and maintenance criteria that en-
sure the language model retains its initial capacity
to model other relational knowledge and general
language. Our results show that when knowledge-
critical subnetworks are removed, a model loses
its ability to express the knowledge encoded in the
subnetwork, and to transfer it when finetuned on
downstream tasks requiring the knowledge.
Acknowledgements
We thank Mohammadreza Banaei, Syrielle Montar-
iol, Debjit Paul, Khai Loong Aw, Badr AlKhamissi,
Silin Gao, Yifan Hou, Beatriz Borges, Yu Fei, and
Angelika Romanou for their helpful discussions
and feedback on our manuscript. We also gratefully
acknowledge the support of the Swiss National Sci-
ence Foundation (No. 215390), Innosuisse (PFFS-
21-29), the EPFL Center for Imaging, Sony Group
Corporation, and the Allen Institute for AI.
Limitations
We discuss the limitations of our proposed method
and conducted experiments on three axes: data,
model, and hyperparameter. We emphasize that thedata used for our experiments are limited to En-
glish only. As English is a high-resource language,
additional challenges could arise when reproduc-
ing our method in a low-resource language ( e.g.,
finding a rich lexical database like WordNet). We
identify the lack of diverse pretrained language
model architectures and language modeling ob-
jectives as the main model limitation. We have
tested our method on the billion scale but did not
expand our scope to larger models with different
architectures (for example, in the 7B scale). We
also limit the analysis to models trained with an
autoregressive language modeling objective in con-
trast to text-to-text models such as T5 (Raffel et al.,
2020) or Masked-Language-Modeling models such
as RoBERTa (Liu et al., 2019b). Finally, the hyper-
parameter search detailed in the Appendix, while
not exhaustive, provides sufficient evidence to sup-
port the validity of the selected range. To find
more precise knowledge-critical subnetworks, fu-
ture methods may need to take this hyperparameter
search further.
Ethics Statement
In this study, we concentrate on relational knowl-
edge, but the technique of identifying subnetworks
could be used in mitigating bias within models.
Likewise, this method of finding subnetworks may
also inadvertently lead to the elimination of criti-
cal ethical or factual knowledge from a language
model, resulting in a model that could generate of-
fensive content and misinformation. For example,
there exists a backdoor attack method against deep
neural networks that builds on top of the identifi-
cation and editing of subnetworks (Qi et al., 2021).
Therefore, caution should be exercised when apply-
ing the identification and removal of subnetworks
to models used in essential applications.
References
Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona
Diab, and Marjan Ghazvininejad. 2022. A review on
language models as knowledge bases.
Omer Antverg, Eyal Ben-David, and Yonatan Belinkov.
2022. IDANI: Inference-time domain adaptation via
neuron-level interventions. In Proceedings of the
Third Workshop on Deep Learning for Low-Resource
Natural Language Processing , pages 21–29, Hybrid.
Association for Computational Linguistics.
Yonatan Belinkov. 2022. Probing classifiers: Promises,
shortcomings, and advances. Computational Linguis-
tics, 48(1):207–219.Yonatan Belinkov and James Glass. 2019. Analysis
methods in neural language processing: A survey.
Transactions of the Association for Computational
Linguistics , 7:49–72.
Yoshua Bengio, Nicholas Léonard, and Aaron Courville.
2013. Estimating or propagating gradients through
stochastic neurons for conditional computation.
Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-
tanya Malaviya, Asli Celikyilmaz, and Yejin Choi.
2019. COMET: Commonsense transformers for auto-
matic knowledge graph construction. In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 4762–4779, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-
ong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021a.
Knowledgeable or educated guess? revisiting lan-
guage models as knowledge bases. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 1860–1874, Online.
Association for Computational Linguistics.
Steven Cao, Victor Sanh, and Alexander Rush. 2021b.
Low-complexity probing via finding subnetworks. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 960–966, Online. Association for Computa-
tional Linguistics.
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
Katherine Lee, Florian Tramer, and Chiyuan Zhang.
2023. Quantifying memorization across neural lan-
guage models. In The Eleventh International Confer-
ence on Learning Representations .
Nicholas Carlini, Florian Tramer, Eric Wallace,
Matthew Jagielski, Ariel Herbert-V oss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ul-
far Erlingsson, Alina Oprea, and Colin Raffel. 2021.
Extracting training data from large language models.
Zeming Chen and Qiyue Gao. 2022. Probing linguistic
information for logical inference in pre-trained lan-
guage models. Proceedings of the AAAI Conference
on Artificial Intelligence , 36(10):10509–10517.
Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyil-
maz, and Antoine Bosselut. 2023. Reckoning: Rea-
soning through dynamic knowledge encoding. In
Advances in Neural Information Processing Systems ,
volume 36, pages 62579–62600. Curran Associates,
Inc.
Abhijith Chintam, Rahel Beloch, Willem Zuidema,
Michael Hanna, and Oskar van der Wal. 2023. Iden-
tifying and adapting transformer-components respon-
sible for gender bias in an English language model.
InProceedings of the 6th BlackboxNLP Workshop:
Analyzing and Interpreting Neural Networks for NLP ,pages 379–394, Singapore. Association for Compu-
tational Linguistics.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D. Manning. 2019. What does BERT
look at? an analysis of BERT’s attention. In Pro-
ceedings of the 2019 ACL Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP ,
pages 276–286, Florence, Italy. Association for Com-
putational Linguistics.
Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch,
Stefan Heimersheim, and Adrià Garriga-Alonso.
2023. Towards automated circuit discovery for mech-
anistic interpretability. In Advances in Neural Infor-
mation Processing Systems , volume 36, pages 16318–
16352. Curran Associates, Inc.
Róbert Csordás, Sjoerd van Steenkiste, and Jürgen
Schmidhuber. 2021. Are neural nets modular? in-
specting functional modularity through differentiable
weight masks. In International Conference on Learn-
ing Representations .
Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and
Antoine Bosselut. 2021. Analyzing commonsense
emergence in few-shot knowledge models. In 3rd
Conference on Automated Knowledge Base Construc-
tion.
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao
Chang, and Furu Wei. 2022. Knowledge neurons in
pretrained transformers. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 8493–
8502, Dublin, Ireland. Association for Computational
Linguistics.
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-
ing factual knowledge in language models. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 6491–
6506, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz,
and Ivan Titov. 2020. How do decisions emerge
across layers in neural models? interpretation with
differentiable masking. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 3243–3255, On-
line. Association for Computational Linguistics.
Nadir Durrani, Hassan Sajjad, Fahim Dalvi, and
Yonatan Belinkov. 2020. Analyzing individual neu-
rons in pre-trained language models. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages
4865–4880, Online. Association for Computational
Linguistics.
Nelson Elhage, Tristan Hume, Catherine Olsson,
Nicholas Schiefer, Tom Henighan, Shauna Kravec,
Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,
Carol Chen, Roger Grosse, Sam McCandlish, Jared
Kaplan, Dario Amodei, Martin Wattenberg, andChristopher Olah. 2022. Toy models of superpo-
sition. Transformer Circuits Thread .
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly,
Nova DasSarma, Dawn Drain, Deep Ganguli, Zac
Hatfield-Dodds, Danny Hernandez, Andy Jones,
Jackson Kernion, Liane Lovitt, Kamal Ndousse,
Dario Amodei, Tom Brown, Jack Clark, Jared Ka-
plan, Sam McCandlish, and Chris Olah. 2021. A
mathematical framework for transformer circuits.
Transformer Circuits Thread .
Negar Foroutan, Mohammadreza Banaei, Rémi Lebret,
Antoine Bosselut, and Karl Aberer. 2022. Discov-
ering language-neutral sub-networks in multilingual
language models. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 7560–7575, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Jonathan Frankle and Michael Carbin. 2019. The lottery
ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning
Representations .
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir
Globerson. 2023. Dissecting recall of factual associa-
tions in auto-regressive language models. In Proceed-
ings of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 12216–12235,
Singapore. Association for Computational Linguis-
tics.
Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval
Sadde, Micah Shlain, Bar Tamir, and Yoav Goldberg.
2022a. LM-debugger: An interactive tool for inspec-
tion and intervention in transformer-based language
models. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations , pages 12–21, Abu Dhabi,
UAE. Association for Computational Linguistics.
Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-
berg. 2022b. Transformer feed-forward layers build
predictions by promoting concepts in the vocabulary
space. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Process-
ing, pages 30–45, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer
Levy. 2021. Transformer feed-forward layers are key-
value memories. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 5484–5495, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Nicholas Goldowsky-Dill, Chris MacLeod, Lucas Sato,
and Aryaman Arora. 2023. Localizing model behav-
ior with path patching.Demi Guo, Alexander Rush, and Yoon Kim. 2021.
Parameter-efficient transfer learning with diff prun-
ing. In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
4884–4896, Online. Association for Computational
Linguistics.
Anshita Gupta, Debanjan Mondal, Akshay Sheshadri,
Wenlong Zhao, Xiang Li, Sarah Wiegreffe, and Niket
Tandon. 2023. Editing common sense in transform-
ers. In Proceedings of the 2023 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 8214–8232, Singapore. Association for Com-
putational Linguistics.
Peter Hase, Mohit Bansal, Been Kim, and Asma Ghan-
deharioun. 2023a. Does localization inform editing?
surprising differences in causality-based localization
vs. knowledge editing in language models. In Ad-
vances in Neural Information Processing Systems ,
volume 36, pages 17643–17668. Curran Associates,
Inc.
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-
nitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and
Srinivasan Iyer. 2023b. Methods for measuring, up-
dating, and visualizing factual beliefs in language
models. In Proceedings of the 17th Conference of
the European Chapter of the Association for Compu-
tational Linguistics , pages 2714–2731, Dubrovnik,
Croatia. Association for Computational Linguistics.
Evan Hernandez, Belinda Z. Li, and Jacob Andreas.
2024. Inspecting and editing knowledge represen-
tations in language models. In First Conference on
Language Modeling .
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In International Conference on
Learning Representations .
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and
Igor Mordatch. 2022. Language models as zero-shot
planners: Extracting actionable knowledge for em-
bodied agents.
Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras,
Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and
Yejin Choi. 2021. (comet-) atomic 2020: On sym-
bolic and neural commonsense knowledge graphs.
Proceedings of the AAAI Conference on Artificial
Intelligence , 35(7):6384–6392.
Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-
gorical reparameterization with gumbel-softmax. In
International Conference on Learning Representa-
tions .
Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,
Moontae Lee, Lajanugen Logeswaran, and Minjoon
Seo. 2023. Knowledge unlearning for mitigating
privacy risks in language models. In Proceedingsof the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 14389–14408, Toronto, Canada. Association
for Computational Linguistics.
Liwei Jiang, Antoine Bosselut, Chandra Bhagavatula,
and Yejin Choi. 2021. “I’m not mad”: Commonsense
implications of negation and contradiction. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
4380–4397, Online. Association for Computational
Linguistics.
Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what language
models know? Transactions of the Association for
Computational Linguistics , 8:423–438.
Maximilian Li, Xander Davies, and Max Nadeau. 2024.
Circuit breaking: Removing model behaviors with
targeted ablation.
Xiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel.
2016. Commonsense knowledge base completion.
InProceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 1445–1455, Berlin, Germany.
Association for Computational Linguistics.
Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang
Ren. 2019. KagNet: Knowledge-aware graph net-
works for commonsense reasoning. In Proceedings
of the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 2829–2839, Hong Kong,
China. Association for Computational Linguistics.
Nelson F. Liu, Matt Gardner, Yonatan Belinkov,
Matthew E. Peters, and Noah A. Smith. 2019a. Lin-
guistic knowledge and transferability of contextual
representations. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Pa-
pers) , pages 1073–1094, Minneapolis, Minnesota.
Association for Computational Linguistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Comput. Surv. , 55(9).
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
Roberta: A robustly optimized bert pretraining ap-
proach.
Michelle Lo, Fazl Barez, and Shay Cohen. 2024. Large
language models relearn removed concepts. In Find-
ings of the Association for Computational Linguistics
ACL 2024 , pages 8306–8323, Bangkok, Thailand
and virtual meeting. Association for Computational
Linguistics.Chris J. Maddison, Andriy Mnih, and Yee Whye Teh.
2017. The concrete distribution: A continuous relax-
ation of discrete random variables. In International
Conference on Learning Representations .
Arun Mallya, Dillon Davis, and Svetlana Lazebnik.
2018. Piggyback: Adapting a single network to mul-
tiple tasks by learning to mask weights. In Proceed-
ings of the European Conference on Computer Vision
(ECCV) , pages 67–82.
Kevin Meng, David Bau, Alex Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual asso-
ciations in gpt. In Advances in Neural Information
Processing Systems , volume 35, pages 17359–17372.
Curran Associates, Inc.
Kevin Meng, Arnab Sen Sharma, Alex J Andonian,
Yonatan Belinkov, and David Bau. 2023. Mass-
editing memory in a transformer. In The Eleventh
International Conference on Learning Representa-
tions .
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture mod-
els. In International Conference on Learning Repre-
sentations .
George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM , 38(11):39–41.
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea
Finn, and Christopher D Manning. 2022a. Fast model
editing at scale. In International Conference on
Learning Representations .
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea
Finn, and Christopher D. Manning. 2022b. Memory-
based model editing at scale. In International Con-
ference on Machine Learning .
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel
Goh, Michael Petrov, and Shan Carter. 2020. Zoom
in: An introduction to circuits. Distill .
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-
erly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,
Danny Hernandez, Scott Johnston, Andy Jones, Jack-
son Kernion, Liane Lovitt, Kamal Ndousse, Dario
Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah. 2022. In-context
learning and induction heads. Transformer Circuits
Thread .
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) ,
pages 2463–2473, Hong Kong, China. Association
for Computational Linguistics.Xiangyu Qi, Jifeng Zhu, Chulin Xie, and Yong Yang.
2021. Subnet replacement: Deployment-stage back-
door attack against deep neural networks in gray-box
setting.
Guanghui Qin and Jason Eisner. 2021. Learning how
to ask: Querying LMs with mixtures of soft prompts.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 5203–5212, Online. Association for Computa-
tional Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1–67.
Siyu Ren and Kenny Zhu. 2022. Specializing pre-
trained language models for better relational reason-
ing via network pruning. In Findings of the Associ-
ation for Computational Linguistics: NAACL 2022 ,
pages 2195–2207, Seattle, United States. Association
for Computational Linguistics.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 5418–5426,
Online. Association for Computational Linguistics.
Tara Safavi and Danai Koutra. 2021. Relational World
Knowledge Representation in Contextual Language
Models: A Review. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1053–1067, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Victor Sanh, Thomas Wolf, and Alexander Rush. 2020.
Movement pruning: Adaptive sparsity by fine-tuning.
InAdvances in Neural Information Processing Sys-
tems, volume 33, pages 20378–20389. Curran Asso-
ciates, Inc.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric
Wallace, and Sameer Singh. 2020. AutoPrompt: Elic-
iting Knowledge from Language Models with Auto-
matically Generated Prompts. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 4222–4235,
Online. Association for Computational Linguistics.
Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.
Conceptnet 5.5: An open multilingual graph of gen-
eral knowledge. Proceedings of the AAAI Conference
on Artificial Intelligence , 31(1).Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4149–4158, Minneapolis, Minnesota. Association for
Computational Linguistics.
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.
BERT rediscovers the classical NLP pipeline. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 4593–
4601, Florence, Italy. Association for Computational
Linguistics.
Jonas Wallat, Jaspreet Singh, and Avishek Anand. 2020.
BERTnesia: Investigating the capture and forgetting
of knowledge in BERT. In Proceedings of the Third
BlackboxNLP Workshop on Analyzing and Interpret-
ing Neural Networks for NLP , pages 174–183, On-
line. Association for Computational Linguistics.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel Bowman. 2019a. Superglue: A stickier
benchmark for general-purpose language understand-
ing systems. In Advances in Neural Information
Processing Systems , volume 32. Curran Associates,
Inc.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019b.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Interna-
tional Conference on Learning Representations .
Kevin Ro Wang, Alexandre Variengien, Arthur Conmy,
Buck Shlegeris, and Jacob Steinhardt. 2023. Inter-
pretability in the wild: a circuit for indirect object
identification in GPT-2 small. In The Eleventh Inter-
national Conference on Learning Representations .
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2023. Generate
rather than retrieve: Large language models are
strong context generators. In The Eleventh Inter-
national Conference on Learning Representations .
Xiongyi Zhang, Jan-Willem van de Meent, and Byron
Wallace. 2021. Disentangling representations of text
by masking transformers. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 778–791, Online and Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-
rich Schütze. 2020. Masking as an efficient alterna-
tive to finetuning for pretrained language models. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 2226–2241, Online. Association for Computa-
tional Linguistics.Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.
Factual probing is [MASK]: Learning vs. learning
to recall. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 5017–5033, Online. Association
for Computational Linguistics.
Kankan Zhou, Eason Lai, Wei Bin Au Yeong, Kyriakos
Mouratidis, and Jing Jiang. 2023. ROME: Evaluat-
ing pre-trained vision-language models on reasoning
beyond visual common sense. In Findings of the
Association for Computational Linguistics: EMNLP
2023 , pages 10185–10197, Singapore. Association
for Computational Linguistics.
A Dataset Creation and Processing
TARGET KG To gather small connected TAR-
GETKGs, we randomly select an initial node and
sample knowledge triplets by walking a depth of
three up (parent direction) and down (child di-
rection) in the respective KG. Given a seed node
such asrepresentation in WordNet12orfruit
in ConceptNet, we sample relations by perform-
ing a 3-hop random walk. For example, for the
fruit KG shown in Table 7, we start from the
seed concept fruit . In the first depth, we retrieve
(fruit, ReceivesAction, eaten) and(wine,
MadeOf, fruit) . In the next depth, we retrieve
(champagne, IsA, wine) , and so forth for all pos-
sible relations. Note that we only sample relations
with a single-token tail entity.
Once this connected KG is sampled, we apply
two filtering processes. The first one enforces
many-to-one relationships in KTto avoid head
entities with multiple tails. The second filtering
process reduces the tail-entity imbalance to avoid
over-fitting to a small set of tokens. For this, we
count the frequency of the tail tokens in the sam-
pled graph and keep at most a quartile amount of
triplets with shared tail entities.
Finally, we verbalize TARGET KGgraph with
the formats that give the lowest perplexity on the
pretrained model. We try various relation-specific
verbalization templates per knowledge triplet and
pick the one that yields the lowest tail-token per-
plexity. For example, in the representation
graph, while the model had lower perplexity
with the template “{h} is a kind of { t}”
for the triplet (representation.n.02, IsA,
12In WordNet, a word sense is represented by its lemma,
syntactic category, and sense ID ( e.g., inmap.n.01 ,nfor noun
and01for sense ID). We omit this naming convention from
the main paper tables for readability.Knowledge GraphTripletsVerbalizationHead Relation Tail
WordNetCONTROL KG train(casserole.n.02, IsA, dish.n.01) “A casserole is a dish”
(passerby.n.01, IsA, pedestrian.n.01) “A passerby is a type of pedestrian”
(chorizo.n.01, IsA, sausage.n.01) “A chorizo is a kind of sausage”
CONTROL KG val.(crate.n.01, IsA, box.n.01) “A crate is a kind of box”
(magnetometer.n.01, IsA, meter.n.02) “Magnetometer is a type of meter”
(vaccinee.n.01, IsA, patient.n.01) “A vaccinee is a patient”
communication(message.n.01, IsA, communication.n.02) “A message is a type of communication”
(indicator.n.02, IsA, signal.n.01) “An indicator is a type of signal”
(evidence.n.02, IsA, indication.n.01) “Evidence is an indication”
location(region.n.01, IsA, location.n.01) “A region is a location”
(district.n.01, IsA, region.n.03) “A district is a region”
(expanse.n.03, IsA, space.n.02 “An expanse is a type of space”
representation(representation.n.02, IsA, creation.n.02) “Representation is a kind of creation”
(delineation.n.02, IsA, drawing.n.02) “A delineation is a type of drawing”
(chart.n.02, IsA, map.n.01) “A chart is a map”
ConceptNetCONTROL KG train(briefcase, AtLocation, desk) “A briefcase is typically placed at a desk”
(vegetarian, NotDesires, meat) “A vegetarian doesn’t crave meat”
(voting, Causes, election) “A voting can lead to an election”
CONTROL KG val.(boat, UsedFor, sailing) “A boat is designed for sailing”
(clothes, ReceivesAction, washed) “Clothes can be washed”
(jogging,HasPrerequisite,energy) “A jogging requires an energy”
fruit(fruit, ReceivesAction, eaten) “A fruit can be eaten”
(wine, MadeOf, fruit) “A wine comprises of a fruit”
(champagne, IsA, wine) “Champagne is a type of wine”
Table 7: Examples of KG triplets, and the best GPT-2 small verbalization for WordNet and ConceptNet.
creation.n.02) , it also had lower perplexity with
the template “A {h} is a { t}”for the triplet
(chart.n.02, IsA, map.n.01) . Note that this
can change for each model size, such as GPT2-
small, medium, large and XL.
CONTROL KG To create CONTROL KG, we pri-
oritize not leaking TARGET KG counterfactuals
and having a shared CONTROL KGacross different
TARGET KGs. Therefore, we remove from the com-
plete KG ( e.g., for ConceptNet TARGET KGs, the
complete LAMA subset of ConceptNet) any triplet
that shares the same entities as the union of the
TARGET KGs shown in Table 1. For all KG verbal-
izations, to remove and maintain knowledge that
the model is already confident about, we pick the
best scoring verbalization for each triplet among
several prompt styles and filter out those that yield
an individual PPL higher than a threshold. For
testing, we use held-out triplets.
CONTROL LM We use WikiText-2 (Merity et al.,
2017) for the CONTROL LMdataset. We tokenize
each entry and then concatenate all of them to-
gether. Finally, we group the tokens into chunks
of 512. For validation and testing, we use held-out
sets.B Training and Evaluation
Implementation
Mask Implementation As mentioned in §5, dur-
ing mask learning, we do not mask the embedding,
language modeling head, layer-normalization, and
bias parameters. We also only learn masks for the
top 50% of the transformer layers. We initialize
the mask parameters such that, in the first forward
pass, each model parameter has a starting masking
probability of σ(li) = 0 .45, meaning the search is
expected to start with an empty knowledge-critical
subnetwork ( i.e., a subnetwork mask of zeros) and
a fully-connected inverse subnetwork ( i.e., the full
model). Results on a hyperparameter search for
initialization can be found in Table 8. Moreover,
for the randomly masked baseline, we mask each
module ( e.g., MLP module at layer 8) at the same
sparsity as the corresponding module in the criti-
cal subnetwork, which means that the masking is
not uniformly done across all layers. For neuron
masking, we jointly learn a mask across weights in
a linear layer that connect to the same input neu-
ron. For the randomly masked neuron baseline,
we mask each module at the same neuron sparsity
as the corresponding module in the critical subnet-
work.
Hyperparameters We use a learning rate of 0.2
with a linear warmup for the first 10% of the train-
ing that starts from 1e-10. We optimize with theInit. Mask Sparsity T ARGET KG C ONTROL KG C ONTROL LM
Probability (↑) ∆PPL(↑) ∆PPL (↓) ∆PPL (↓)
0.25 99.5 [99.5, 99.6] 332.1 [119.5, 544.6] 3.7 [2.6, 4.7] 0.1 [0.1, 0.1]
0.45 99.5 [99.5, 99.5] 1287.8 [80.0, 2495.6] 0.9 [-1.1, 2.9] 0.2 [0.1, 0.2]
0.50 99.4 [99.4, 99.5] 939.1 [59.9, 1818.2] -0.2 [-0.2, -0.1] 0.2 [0.2, 0.2]
0.75 98.9 [98.8, 99.1] 13043.1 [4.6, 26081.5] -1.9 [-1.9, -1.8] 0.3 [0.3, 0.4]
Table 8: Hyperparameter study on inital mask probability with GPT2-small.
Maskingλ1λ2λ3λ4Sparsity T ARGET KG C ONTROL KG C ONTROL LM Number of Valid
Method (↑) ∆PPL(↑) ∆PPL (↓) ∆PPL (↓) Checkpoints
Weight1 1 3 1 96.9 [95.6, 98.2] 17.3 [-7.8, 42.4] 1.0 [-2.0, 4.1] 0.5 [0.2, 0.8] 2.5 [0, 5]
1 3 1 1 97.3 [96.4, 98.3] 80.9 [59.2, 102.6] -1.5 [-8.9, 5.8] 1.0 [0.5, 1.5] 53.5 [5, 102]
3 1 1 1 98.5 [97.6, 99.3] 12516.4 [682.2, 24350.6] 0.0 [-1.4, 1.5] 0.4 [0.2, 0.6] 145.0 [133, 157]
Neuron1 1 3 1 96.7 [96.6, 96.9] 297.5 [104.0, 491.0] 3.9 [-1.7, 9.6] 2.9 [2.9, 3.0] 0.0 [0, 0]
1 3 1 1 95.3 [95.3, 95.4] 110.2 [53.3, 167.2] 11.8 [10.0, 13.7] 3.5 [3.4, 3.6] 0.0 [0, 0]
3 1 1 1 93.3 [93.1, 93.5] 255.1 [135.0, 375.1] 44.7 [38.8, 50.7] 5.2 [4.7, 5.7] 0.0 [0, 0]
Table 9: Hyperparameter study on λiloss weights in Eq. 6 with GPT2-small.
Setup Model KG C ONTROL LM
GPT-2 Small 250 10
Train GPT-2 Medium 96 4
GPT-2 Large 96 4
GPT-2 XL 128 4
GPT-2 Small 250 8
Eval GPT-2 Medium 250 8
GPT-2 Large 250 8
GPT-2 XL 250 8
Table 10: GPU batch size for each dataset and model.
IterationTARGET KG C ONTROL KG C ONTROL LM
∆PPL Floor ∆PPL Ceiling ∆PPL Ceiling
1 35.0 5.0 1.0
2 40.0 7.0 2.0
3 40.0 10.0 3.0
4 50.0 15.0 4.0
Table 11: Selection limit for each success criteria.
AdamW optimizer. For equation 6, we set λ1= 1.5
andλ2=λ3= 1in all of our our experiments. To
encourage the subnetwork to be sparser, we sched-
uleλ4to start at 2 and increase linearly after 50%
of the training until it reaches 3. For GPT2-small,
we use a single GPU setting to run the mask train-
ing for 40,000 steps. For GPT2-medium and large,
we use a three GPU distributed setting and run the
mask training for 50,000 steps. For GPT2-XL, we
use a three GPU distributed setting and run the
mask training for 60,000 steps.Software and Hardware We primarily use Py-
Torch13and Huggingface Transformers14to imple-
ment the masking method. Experiments for GPT2-
small, medium and large are run on NVIDIA A100
40GB devices. Experiments for GPT2-XL are run
on NVIDIA A100 80GB devices.
Loss Trade-Off Analysis A primary driver of
the knowledge-critical subnetwork search is the
trade-off between the suppression and maintenance
losses. To validate our λichoices, we run a min-
imal experiment on giving importance to one ob-
jective at a time for two TARGET KGs and one
random seed. Specifically, when we set any one
of the weights in Eq. 6 to a value of 3, we set
the value of the rest to 1. As seen in Table 9, we
find that giving more weight to the suppression
loss finds checkpoints with higher perplexity dif-
ferences on TARGET KGwhile simultaneously sat-
isfying the maintenance criteria. Moreover, giving
more weight to the sparsity regularization ensures
a higher sparsity. These results support the λihy-
perparameters we use in all of our experiments, as
described above.
Dataloaders As each TARGET KGis small, at
each gradient step, the model sees the complete
graph. Therefore, the TARGET KGbatch size is
the same as the number of triplets (see Table 1). In
contrast, CONTROL KGandCONTROL LMdatasets
have thousands of entries in total. To balance the
learning and make it more efficient, we create a
13https://pytorch.org
14https://huggingface.co/docs/transformersMasked Layer Percentage Sparsity T ARGET KG C ONTROL KG C ONTROL LM # of
Choice Masked (↑) ∆PPL(↑) ∆PPL (↓) ∆PPL (↓) checkpoints
0-11 100% 95.6 [94.9, 96.6] 242.7 [-26.6, 1254.3] 11.8 [6.7, 15.9] 1.3 [1.0, 1.7] 1.1
3-11 75% 97.3 [94.4, 98.3] 669.7 [-8.7, 2119.7] -1.4 [-8.3, 10.8] 1.0 [0.5, 2.9] 76.9
6-11 50% 98.6 [97.1, 99.2] 870.4 [38.7, 2665.1] 0.4 [-2.6, 4.0] 0.5 [0.3, 1.0] 104.1
9-11 25% 99.2 [98.0, 99.7] 1185.0 [62.3, 4787.0] 4.2 [0.1, 9.5] 0.4 [0.0, 0.8] 103.2
Table 12: Subnetwork discovery results for different percentages of upper layers masked in GPT-2 small,
averaged over four KGs and two seeds with [min, max] values denoted in brackets. The arrows ( ↑,↓) show the
desired value for the metric.
dynamic cyclical training dataloader that samples a
new batch at each step without replacement. When
the dataloader reaches the end of the dataset, it
restarts with a new ordering. Please refer to Ta-
ble 10 for the exact batch sizes.
Best Checkpoint Selection We iteratively select
the best checkpoint, starting with strict criteria on
the maintenance datasets and gradually loosening
them. We check whether any checkpoints satisfy
the first set of criteria limits shown in Table 11.
The checkpoints need to have a TARGET KG∆PPL
above the mentioned floor and maintenance ∆PPL
below the mentioned ceiling. If the set of check-
points retrieved is empty, we select from the next
set of limits. If none of the iterations are successful,
we pick the last checkpoint as the best one.
C Masked Layer Choice Study
Layer-wise model probing analyses have shown
that the first layers of transformer language models
encode representations crucial for low-level linguis-
tic tasks and features that may be a prerequisite for
knowledge modeling (Tenney et al., 2019; Liu et al.,
2019a). Researchers have also shown that knowl-
edge is not only contained in the final few layers
(Wallat et al., 2020). Therefore, for our datasets,
we investigate how masking different percentages
of upper dense layers can affect the success criteria
defined for a knowledge-critical subnetwork. In
particular, we look at the effect of masking the top
25%, 50%, 75%, and 100% of the model.
In Table 12, we observe that masking all dense
layers in transformer blocks (100%) can affect the
maintenance criteria significantly. CONTROL KG
perplexity difference is smaller when masking
fewer layers, confirming that lower layers may have
imperative representation to knowledge modeling.
As the values for the different criteria are similar
for masking the top 25% and 50%, we use the top
50% masking approach to increase the maskingcoverage for all of our experiments.
D Additional Subnetwork Discovery
Results
In this section, we provide additional metrics for
subnetwork discovery results and non-aggregated
results for the randomly masked baseline.
Minimum & Maximum Boundaries In addition
to the average ∆PPL and ∆Rank presented in Ta-
ble 2, we add minimum and maximum boundaries
to all of the results in Table 13 and 15. We also pro-
vide log probability differences ∆LogProb similar
to how ∆PPL is calculated. We observe in Table 15
the same trend as ∆PPL. On average, removing the
subnetwork increases the rank of the gold tail token
and decreases the log probability. In contrast, the
randomly masked baseline does not increase the
TARGET KGrank significantly and does not main-
tainCONTROL KGrank to the same extent as the
critical subnetwork.
Model Scale We include the individual KG re-
sults for larger models in Table 16. While individ-
ual results on GPT2-medium are not as sparse and
effective as the small and large variants, it is still
more significant than randomly masking the model
at the same sparsity.
Randomly Masked Baseline We provide the
non-aggregated randomly masked baseline results
for GPT2-small in Table 17 and for larger models in
Table 18. We notice that KGs where the pretrained
model perplexity is already low (see Table 1) seem
not to be as affected by a random subnetwork re-
moval as those that have a higher initial perplexity.
E Additional Details on Downstream
Task Transfer
To learn a mask for a set of ConceptNet relations,
we need to verbalize them with a relation-specific
prompt. As described in §6.2, CommonsenseQAKnowledge GraphSparsity T ARGET KG C ONTROL KG C ONTROL LM
(↑) ∆PPL(↑) ∆PPL (↓) ∆PPL (↓)
WordNetbuilding 98.4 [97.4, 99.3] 62.3 [13.2, 114.1] -2.0 [-7.0, 2.4] 0.6 [0.3, 1.0]
communication 99.2 [99.0, 99.3] 104.8 [61.1, 165.9] -1.2 [-2.2, 0.0] 0.3 [0.3, 0.3]
change 98.4 [98.0, 99.1] 567.2 [38.7, 1405.6] 0.6 [-1.6, 3.0] 0.7 [0.4, 0.9]
statement 98.2 [96.3, 99.2] 152.5 [53.5, 248.7] -0.5 [-3.2, 2.8] 0.8 [0.3, 1.8]
location 99.0 [98.8, 99.1] 810.5 [469.2, 1200.7] 0.5 [-1.7, 3.9] 0.3 [0.3, 0.4]
representation 98.1 [97.1, 98.8] 221.8 [115.5, 334.4] 2.9 [0.6, 4.0] 0.6 [0.4, 1.0]
magnitude 99.0 [98.6, 99.3] 2216.9 [1730.7, 2665.1] -1.8 [-2.6, -0.9] 0.3 [0.2, 0.4]
Random Weights 98.6 [98.1, 99.2] 24.3 [5.0, 48.8] 14.6 [0.0, 46.2] 2.2 [1.2, 3.3]
Average 98.6 [98.1, 99.2] 590.9 [62.3, 2216.9] -0.2 [-2, 2.9] 0.5 [0.0, 0.8]
ConceptNetfruit 99.2 [99.1, 99.4] 743.9 [300.8, 1462.1] 3.0 [-0.6, 5.0] 0.2 [0.2, 0.2]
sun 99.2 [99.0, 99.3] 888.4 [521.0, 1240.1] 3.2 [2.0, 4.7] 0.2 [0.1, 0.3]
swimming 99.0 [98.8, 99.2] 276.8 [240.9, 335.4] 2.3 [0.6, 3.3] 0.3 [0.2, 0.4]
Random Weights 99.1 [99.0, 99.2] 21.0 [13.7, 29.4] 14.6 [12.4, 17.2] 1.5 [1.3, 1.7]
Average 99.1 [99.0, 99.2] 636.4 [276.8, 888.4] 2.8 [2.3, 3.2] 0.2 [0.2, 0.3]
Table 13: Subnetwork discovery results for GPT-2 small with weight masking, averaged over three seeds with
[min, max] values denoted in brackets. ∆PPL = PPL( f(x,˜m⊙θ)) - PPL( f(x,θ)). The arrows ( ↑,↓) show the
desired value for the metric. Random is an average of randomly masked baselines at the same sparsity levels as the
discovered knowledge-critical subnetworks for each KG-seed pair.
questions are not explicitly annotated with a rela-
tion. However, they were constructed with Con-
ceptNet such that each question’s head concept
relates to four of the tail answers with the same
relation. This does not apply to the fifth answer, as
crowd workers created them. Therefore, to retrieve
the relations, we iterate through the questions and
check if any relations with the question head con-
cept and correct tail answer exist in the LAMA and
Commonsense Knowledge Base Completion sub-
sets of ConceptNet (Li et al., 2016; Petroni et al.,
2019). If it does and has only one relation, we
choose that relation. If it has multiple relations, we
take the union of relations between the head con-
cept and the distractor tail answers and intersect
that with the correct tail triplets. If the intersection
is a set larger than one element, we choose one
relation at random. Out of the 1221 test questions,
only 572 have a single-token correct answer, and
we could only find the corresponding relation to
363 questions, which is our filtered test set.
For the MCQA head, we use the Huggingface
Double Heads model.15In addition to the lan-
guage modeling head, this model adds a parallel
multiple-choice classification head. The MCQA
head takes as input the last sequence output. To
finetune the MCQA model, we use three kinds of
fine-tuning. The first one is Head Tuning , in which
the model parameters are frozen, but the MCQA
head is not. The second method is LoRA (Hu
15https://huggingface.co/docs/transformerset al., 2022), which is a parameter-efficient fine-
tuning method. Similar to the head tuning method,
LoRA freezes the model parameters and instead
inserts trainable rank decomposition parameters in
each transformer layer. We use a rank of 16 for
all LoRA experiments. Finally, we also try Full
Finetuning , in which all model parameters are
tuned. To remove a subnetwork, we manually set
the knowledge-critical parameters to 0. Therefore,
the value of these parameters can change during
full finetuning.
In addition, we also verify whether learning a
mask for one randomly selected half of the filtered
test set (Half 1) corrupts downstream task transfer
for a distinct half (Half 2), where there are no triplet
overlaps. We find in Table 19 that, on average, the
accuracy on the triplets the mask was trained for is
less by 3.6% than the held-out half.
F Alternative Objective: Is expressing
knowledge enough to be a
knowledge-critical subnetwork?
We defined knowledge-critical subnetworks as be-
ingresponsible for a model’s ability to express cer-
tain pieces of knowledge, validated by an increase
in perplexity when that subnetwork is removed
from the model. However, another way to extract a
knowledge-critical subnetwork might be to learn a
mask over the network that minimizes the negativeKnowledge GraphSparsity T ARGET KG C ONTROL KG C ONTROL LM
(↑) ∆PPL(↑) ∆PPL (↓) ∆PPL (↓)
WordNetbuilding.n.01 95.3 [95.2, 95.4] 330.7 [268.7, 402.1] 31.1 [20.1, 42.6] 4.3 [4.1, 4.5]
communication.n.02 95.2 [95.0, 95.4] 109.2 [70.9, 143.0] 13.4 [12.9, 14.0] 3.9 [3.9, 3.9]
change.n.01 95.1 [95.0, 95.2] 1328.6 [1197.6, 1491.7] 23.6 [13.4, 34.4] 4.3 [4.3, 4.5]
statement.n.01 95.4 [95.0, 96.0] 494.2 [281.7, 679.5] 20.5 [6.0, 36.1] 4.1 [3.6, 4.6]
location.n.01 95.4 [95.3, 95.5] 425.3 [302.9, 548.0] 32.6 [18.0, 43.4] 4.3 [4.0, 4.7]
representation.n.02 95.0 [94.9, 95.1] 653.6 [426.2, 934.5] 20.9 [10.8, 31.6] 4.1 [3.9, 4.2]
magnitude.n.01 95.5 [95.4, 95.5] 1669.5 [1181.7, 2538.6] 13.6 [12.2, 14.6] 3.9 [3.8, 4.0]
Random Neurons 95.3 [95, 95.5] 23.8 [-6.9, 73.8] 8.3 [-2.1, 26.2] 8.3 [7.2, 9.6]
Average 95.3 [95, 95.5] 715.9 [109.2, 1669.5] 22.2 [13.4, 32.6] 4.1 [3.9, 4.3]
ConceptNetfruit 95.3 [95.2, 95.4] 31616.9 [28471.3, 34422.3] 61.4 [58.6, 66.6] 4.8 [4.3, 5.1]
sun 95.4 [95.4, 95.5] 23980.0 [23067.1, 25624.6] 77.5 [69.8, 86.8] 5.0 [4.8, 5.2]
swimming 93.9 [93.8, 94.0] 11669.2 [9968.3, 12610.2] 76.8 [65.2, 91.5] 6.4 [5.7, 6.9]
Random Neurons 94.9 [93.9, 95.4] 110.7 [60.7, 177.5] 70.4 [51.5, 107.2] 11.2 [9.1, 14.7]
Average 94.9 [93.9, 95.4] 22422.0 [11669.2, 31616.9] 71.9 [61.4, 77.5] 5.4 [4.8, 6.4]
Table 14: Subnetwork discovery results for GPT-2 small with neuron masking, averaged over three seeds with
[min, max] values denoted in brackets. ∆PPL = PPL( f(x,˜m⊙θ)) - PPL( f(x,θ)). The arrows ( ↑,↓) show the
desired value for the metric. Random is an average of randomly masked baselines at the same sparsity levels as the
discovered knowledge-critical subnetworks for each KG-seed pair.
loglikelihood of all x∈TARGET KG:
Lexpress =−X
xlog(f(x,m⊙θ)) (7)
In Table 20, we compare subnetworks extracted in
this manner ( i.e., Expression-only) with those of
our main method, as well as those of a combina-
tion of these objectives: Lfinal+λ5Lexpress . Inter-
estingly, we find that the Expression-only setting
can learn a mask for a highly sparse subnetwork,
which, when removed from the full model, also
significantly increases perplexity on TARGET KG.
However, this subnetwork also struggles to main-
tain perplexity on CONTROL KG, indicating it may
encode abilities crucial for expressing anyset of re-
lational knowledge. Adding the expression loss to
our joint objective mitigates this issue, but reduces
subnetwork sparsity by a significant margin ( ∼4%),
indicating that the Expression-only loss may dis-
cover spurious subnetworks that are not actually
knowledge-critical — they are not responsible for
the expression of the knowledge when they are en-
tangled in the full model, though their parameters
may compute a function that expresses it.
G Spurious Subnetworks Test
We hypothesize that a spurious subnetwork would
cause the remaining network from which it was re-
moved to re-gain the ability to express TARGET KG
if the subnetwork was randomly expanded (i.e.,
∆PPL on TARGET KGwould drop as more param-
eters are removed from f(x,˜m⊙θ)). Meanwhile,if removing the critical subnetwork is not a spuri-
ous solution to suppress the TARGET KG, then the
remaining model would generally still fail to recog-
nize TARGET KG, even as more parameters were
randomly removed, leading ∆PPL to rise or stay
the same. To verify this hypothesis, we remove fur-
ther parameters from the remaining model. Starting
from the knowledge-critical subnetwork sparsity,
we randomly remove parameters at intervals of
0.5%. We run this iterative process of removing pa-
rameters with five different random seeds. We also
test whether the mask has found a spurious solution
to achieve the maintenance criteria by adding back
parameters, though with smaller intervals of 0.1%,
as the starting sparsity level is typically high.
In Figure 2, we observe that removing more pa-
rameters in small amounts does not significantly
recover expressing TARGET KG. As a baseline, we
plot the effect on ∆PPL of removing further pa-
rameters from remaining models with randomly
removed subnetworks of the same sparsity. Inter-
estingly, for the maintenance datasets, ∆PPL for
both datasets increases as we remove parameters
from the remaining model. When we add back pa-
rameters, we do not see a linear recovery to ∆PPL
= 0. Instead, we observe an initial phase of in-
crease followed by a phase of decrease as the model
returns to its original state ( i.e., a∆PPL of zero
at 100% sparsity). This effect can be explained by
the fact that our subnetwork had been optimized to
keep these abilities, and has been slightly overfit
for maintenance, though not for suppression . Thus,Knowledge GraphTARGET KG C ONTROL KG T ARGET KG C ONTROL KG
∆Rank ( ↑) ∆Rank (↓) ∆LogProb ( ↓)∆LogProb (↑)
WordNetbuilding.n.01 83.7 [12.8, 168.3] 1.1 [-1.1, 2.9] -0.7 [-1.2, -0.2] 0.0 [0.0, 0.1]
communication.n.02 117.0 [94.5, 134.9] 0.6 [0.1, 1.0] -0.7 [-1.0, -0.5] 0.0 [0.0, 0.0]
change.n.01 139.1 [0.4, 409.8] 0.4 [0.1, 0.6] -1.4 [-2.6, -0.3] 0.0 [0.0, 0.0]
statement.n.01 154.5 [1.6, 353.8] 0.8 [-0.6, 2.8] -0.6 [-0.9, -0.3] 0.0 [0.0, 0.0]
location.n.01 344.9 [188.4, 527.6] 3.6 [2.8, 5.0] -1.6 [-2.0, -1.2] 0.0 [-0.1, 0.0]
representation.n.02 38.1 [12.8, 57.8] 3.4 [2.7, 4.4] -0.7 [-1.0, -0.4] 0.0 [-0.1, 0.0]
magnitude.n.01 1368.7 [978.0, 1698.2] 0.0 [-0.3, 0.1] -2.1 [-2.3, -1.9] 0.0 [0.0, 0.0]
Random 12.0 [-0.1, 25.5] 2.7 [-0.1, 8.1] -0.1 [-0.3, 0.0] -0.2 [-0.5, 0.0]
Average 320.9 [38.1, 1368.7] 1.4 [0.0, 3.6] -1.1 [-2.1, -0.6] 0.0 [0.0, 0.0]
ConceptNetfruit 1164.9 [98.9, 2880.1] 1.8 [0.1, 3.5] -1.0 [-1.6, -0.6] 0.0 [0.0, 0.0]
sun 331.7 [225.9, 415.8] 1.6 [1.4, 1.7] -1.2 [-1.4, -0.9] 0.0 [0.0, 0.0]
swimming 411.6 [34.5, 685.6] 1.4 [0.8, 1.9] -0.4 [-0.5, -0.4] 0.0 [0.0, 0.0]
Random 11.4 [2.1, 20.0] 5.5 [4.2, 7.3] 0.0 [-0.1, 0.0] -0.1 [-0.1, -0.1]
Average 636.1 [331.7, 1164.9] 1.6 [1.4, 1.8] -0.9 [-1.2, -0.4] 0.0 [0.0, 0.0]
Table 15: Subnetwork discovery rank and log probability results for GPT-2 small with weight masking,
averaged over three seeds. ∆Metric = Metric( f(x,˜m⊙θ)) - Metric( f(x,θ)) for Rank and LogProb. Random is an
average of randomly masked baselines at the same sparsity levels as the discovered knowledge-critical subnetworks
for each KG-seed pair. Note that non-zero values may be rounded to 0.0 as we round to one decimal place. Individual
KG results for the random baseline are in 17.
randomly adding parameters back yields new sub-
optimal pathways that corrupt the model’s original
distribution.
H Structural Analysis
In this section, we investigate the structure of the re-
moved knowledge-critical subnetworks by looking
at their relative density across different layer types
(Figure 3), and more specifically, across different
attention heads (Figure 5) and the Wq,Wk, and
Wvmatrices in attention sublayers (Figure 6. The
density is calculated relatively, meaning according
to the particular sublayer’s size. The model used is
GPT2-small.
Layer depth-wise, we observe that the subnet-
work is consistently most dense around the first and
final masked transformer blocks, which are layers
7 and 12 in Figure 3. Specifically, layer type-wise,
we find that knowledge-critical subnetworks are
most dense in the attention sublayers for layer 7
and layer 12 ( Attn-Out andAttn- Wq, Wk, Wv).
In addition, we have not found any complete
columns or rows that were dense in the critical
subnetworks. This means no input or output neu-
ron features get completely removed when the
critical subnetwork is removed. Therefore, the
masked region may not be working to zero-out the
knowledge by turning specific features off, which
would counter the prevailing view that neuron-levelchanges are necessary for mechanistic interven-
tions (Dai et al., 2022; Meng et al., 2022).
When we investigated attention heads and Wq,
Wk, andWvmasks in detail for 3 KGs and 3 seeds,
we found that head 10 in layer 7, and heads 1 and
9 in layer 12 are significantly dense. Moreover,
theWvmask is consistently the most dense across
the three attention Wq,Wk, andWvmasks. There-
fore, while the subnetworks do not have a signifi-
cant IoU, as demonstrated by the seed-based (Ap-
pendix I) and the KG-based analyses (Appendix J),
the subnetworks still tend to be dense in similar
layer types at similar layer depths.
I Random Seed-Based Analysis
We investigate the stability of subnetwork discov-
ery under random seed variance for GPT2-small.
We also explore whether composing subnetworks
from different seeds could increase the suppression
effect while still fulfilling the rest of the success
criteria.
Seed-based Variance Prior work shows that sub-
networks identified under distinct random seeds
may differ with a large variance (Csordás et al.,
2021). We inspect how subnetworks from the best
checkpoints for three random seeds overlap for an
individual TARGET KG. We use Jaccard similar-
ity, or intersection over union (IoU), as the overlap
metric. In Figure 8, we plot a Venn diagram ofModelKnowledge GraphSparsity T ARGET KG C ONTROL KG C ONTROL LM
Size (↑) ∆PPL (↑) ∆PPL (↓) ∆PPL (↓)
communication.n.02 99.5 [99.4, 99.7] 139.9 [-2.5, 282.3] -0.1 [-1.5, 1.3] 0.1 [0.0, 0.1]
location.n.01 95.0 [94.2, 95.8] 432.2 [48.0, 816.3] 3.7 [3.5, 3.8] 0.8 [0.8, 0.9]
Medium representation.n.02 94.8 [91.9, 97.7] 194.6 [139.4, 249.8] 4.0 [3.8, 4.2] 1.2 [0.5, 1.8]
Random 96.4 [94.8, 99.5] 32.1 [5.0, 55.6] 9.2 [1.8, 15.9] 3.0 [0.3, 4.9]
Average 96.4 [94.8, 99.5] 255.6 [139.9, 432.2] 2.5 [-0.1, 4.0] 0.7 [0.1 , 1.2]
communication.n.02 95.9 [95.4, 96.4] 2013.1 [144.5, 3881.8] 6.8 [3.6, 10.0] 0.6 [0.5, 0.6]
location.n.01 99.6 [99.4, 99.7] 1963.1 [1543.1, 2383.1] 1.9 [0.6, 3.3] 0.0 [-0.0, 0.0]
Large representation.n.02 99.2 [99.1, 99.4] 13363.6 [3042.2, 23685.0] 0.9 [0.2, 1.7] 0.0 [0.0, 0.1]
Random 98.2 [95.9, 99.6] 6.8 [4.8, 7.8] 2.9 [0.7, 7.3] 0.8 [0.2, 2.1]
Average 98.2 [95.9, 99.6] 5779.9 [1963.1, 13363.6] 3.2 [0.9, 6.8] 0.2 [0.0, 0.6]
communication.n.02 99.3 [99.2, 99.5] 562.6 [318.7, 806.5] 3.2 [1.6, 4.7] 0.0 [-0.0, 0.0]
location.n.01 99.4 [99.1, 99.7] 257.0 [141.1, 372.8] 2.8 [1.2, 4.3] -0.0 [-0.0, -0.0]
XLrepresentation.n.02 99.2 [99.1, 99.2] 789.9 [696.3, 883.5] 3.6 [3.1, 4.1] 0.1 [0.1, 0.1]
Random 99.3 [99.2, 99.4] 1.6 [1.0, 2.4] 0.1 [-0.6, 1.2] 0.1 [0.1, 0.2]
Average 99.3 [99.2, 99.4] 536.5 [257.0, 789.9.6] 3.2 [2.8, 3.6] 0.0 [0.0, 0.1]
Table 16: Subnetwork discovery results on larger models per KG with weight masking, averaged over two seeds.
Random is an average of randomly masked baselines at the same sparsity levels as the discovered knowledge-critical
subnetworks for each KG-seed pair. Individual KG results for the random baseline are in Table 18.
parameter overlap for each knowledge graph. We
can see that, on average, when using IoU, only
around 3.7% of the unioned subnetwork param-
eters overlap across the three seeds (3.76% for
location , 3.8% for communication , and 3.5% for
representation ), meaning the subnetworks iden-
tified under different random seeds vary, which
complies with prior works’ analysis. Across layers,
the IoU is also similarly low with a higher over-
lap for the final attention layer masks ( ≈10%) as
shown in Figure 10.
Subnetwork Composition We combine masks
of three seeds in their intersection, their floral in-
tersection (intersection unioned with each inter-
section of two seeds), and overall union to mea-
sure the effect on ∆PPL for TARGET KG,CON-
TROL KG, and CONTROL LM. We average the re-
sults over three KGs ( representation ,location ,
andcommunication ).
In Table 5, we observe that removing the inter-
section and floral intersection of the subnetworks
does not increase TARGET KG∆PPL. On the other
hand, removing the union of the subnetworks in-
creases the TARGET KGperplexity difference sig-
nificantly larger than the original results. However,
combining the subnetworks and removing them in-
creases ∆PPL on maintenance datasets more than
using an individual seed’s subnetwork, as seen in
the original results. We note that the increase in
the∆PPL on maintenance datasets matches theincrease we get when removing an equally sparse
random subnetwork (see Table 2). Therefore, it
may be possible to naively combine subnetworks;
however, they may not guarantee the maintenance
criteria to the same extent. A future idea could be
to continue optimizing for the subnetwork mask by
initializing it as the union of the subnetworks to see
if more robust suppression can be achieved.
J Knowledge-Based Analysis
This section examines the overlap of subnetworks
across different KGs for the same seed with GPT2-
small. This contrasts with the previous section that
studies the overlap of subnetworks across different
seeds for the same KG. Similarly, we use Jaccard
similarity, or intersection over union (IoU), as the
overlap metric. We also explore whether compos-
ing subnetworks for different KGs from the same
seed could suppress all of the T ARGET KGs.
Knowledge-based Variance In Figure 12, we
plot a Venn diagram of parameter overlap for each
seed across different TARGET KGs. On average,
when using IoU, only around 3.56% of the unioned
subnetwork parameters overlap across the three
seeds (4.08% for seed 735 , 4.01% for seed 1318 ,
and 2.65% for seed 84 ). Across layers, the IoU
is also similarly low with a significantly higher
overlap for the final attention layer masks ( ≈12%)
as shown in Figure 13.Knowledge GraphSparsity T ARGET KG C ONTROL KG C ONTROL LM
(↑) ∆PPL(↑) ∆PPL (↓) ∆PPL (↓)
WordNetbuilding 98.4 [97.4, 99.3] 5.8 [3.0, 10.2] 14.8 [3.0, 26.2] 2.8 [1.0, 5.2]
communication 99.2 [99.0, 99.3] 5.0 [-2.4, 10.1] 4.6 [0.3, 7.6] 1.2 [1.0, 1.4]
change 98.4 [98.0, 99.1] 33.9 [25.7, 43.3] 24.2 [16.2, 36.2] 2.0 [1.3, 2.6]
statement 98.2 [96.3, 99.2] 15.6 [-0.3, 34.6] 0.0 [-3.5, 3.4] 3.3 [1.2, 6.8]
location 99.0 [98.8, 99.1] 18.8 [-15.8, 55.8] 0.2 [-7.5, 4.6] 1.6 [1.3, 1.9]
representation 98.1 [97.1, 98.8] 48.8 [11.4, 80.3] 46.2 [30.6, 66.2] 3.0 [1.6, 4.5]
magnitude 99.0 [98.6, 99.3] 41.9 [21.1, 70.4] 12.3 [-0.2, 29.2] 1.6 [0.8, 2.0]
Average 98.6 [98.1, 99.2] 24.3 [5.0, 48.8] 14.6 [0.0, 46.2] 2.2 [1.2, 3.3]
ConceptNetfruit 99.2 [99.1, 99.4] 13.7 [-12.6, 35.7] 12.4 [-0.6, 19.7] 1.3 [0.9, 1.7]
sun 99.2 [99.0, 99.3] 29.4 [11.0, 39.0] 17.2 [0.3, 36.3] 1.5 [1.1, 1.8]
swimming 99.0 [98.8, 99.2] 19.8 [-22.1, 42.6] 14.1 [2.3, 30.2] 1.7 [1.2, 2.4]
Average 99.1 [99.0, 99.2] 21.0 [13.7, 29.4] 14.6 [12.4, 17.2] 1.5 [1.3, 1.7]
Table 17: Subnetwork discovery results on the randomly masked baseline for GPT2-small weight masking,
averaged over three seeds.
Subnetwork Composition We combine masks
of three KGs for the same seed in their intersection,
their floral intersection (intersection unioned with
each intersection of two KGs), and overall union
to measure the effect on ∆PPL for TARGET KG,
CONTROL KG, and CONTROL LM. We average the
results over three seeds ( 735,1318 , and84).
Similar to the findings in composing subnet-
works for different seeds, Table 21 shows that com-
posing subnetworks for different KGs increases
the∆PPL on TARGET KGwhen using their union.
However, removing the union of the subnetworks
also has higher perplexity differences on mainte-
nance datasets than using an individual KG’s sub-
network, as seen in the original results. Once again,
this∆PPL increase on the maintenance datasets
matches the difference we would observe using
an equally sparse random subnetwork. Therefore,
while subnetworks of different KGs may be com-
posable to fortify the suppression effect, they may
not guarantee the maintenance criteria to the same
extent as the individual subnetworks.Model Knowledge Sparsity T ARGET KG C ONTROL KG C ONTROL LM
Size Graph (↑) ∆PPL (↑) ∆PPL (↓) ∆PPL (↓)
communication.n.02 99.5 [99.4, 99.7] 5.0 [1.9, 8.1] 1.8 [1.5, 2.1] 0.3 [0.3, 0.4]
location.n.01 95.0 [94.2, 95.8] 55.6 [35.2, 76.1] 15.9 [11.6, 20.2] 3.8 [3.3, 4.4]
Medium representation.n.02 94.8 [91.9, 97.7] 35.8 [18.0, 53.7] 9.9 [9.6, 10.3] 4.9 [1.5, 8.3]
Average 96.4 [94.8, 99.5] 32.1 [5.0, 55.6] 9.2 [1.8, 15.9] 3.0 [0.3, 4.9]
communication.n.02 95.9 [95.4, 96.4] 7.8 [7.2, 8.4] 7.3 [6.6, 8.0] 2.1 [1.8, 2.5]
location.n.01 99.6 [99.4, 99.7] 4.8 [3.6, 6.0] 0.7 [0.5, 0.9] 0.2 [0.1, 0.3]
Large representation.n.02 99.2 [99.1, 99.4] 7.8 [5.4, 10.1] 0.8 [-0.2, 1.8] 0.2 [0.2, 0.3]
Average 98.2 [95.9, 99.6] 6.8 [4.8, 7.8] 2.9 [0.7, 7.3] 0.8 [0.2, 2.1]
communication.n.02 99.3 [99.2, 99.5] 1.0 [0.3, 1.7] -0.3 [-0.3, -0.2] 0.1 [0.1, 0.1]
location.n.01 99.4 [99.1, 99.7] 1.3 [-0.7, 3.3] -0.6 [-1.2, 0.0] 0.1 [0.1, 0.1]
XLrepresentation.n.02 99.2 [99.1, 99.2] 2.4 [-0.5, 5.4] 1.2 [0.6, 1.9] 0.2 [0.2, 0.2]
Average 99.3 [99.2, 99.4] 1.6 [1.0, 2.4] 0.1 [-0.6, 1.2] 0.1 [0.1, 0.2]
Table 18: Subnetwork discovery results on larger randomly masked models per KG with weight masking,
averaged over two seeds.
Method Subnetwork Dev Test Filtered Half 1 Half 2
Head
TuningFull 38.63 38.33 37.19 37.94 36.44
Random (Half 1) -0.87 -4.83 -4.89 -1.75 -8.00
Ours (Half 1) -1.45 -4.83 -11.02 -15.11 -6.95
Random (Half 2) -0.46 -4.16 -4.27 -2.30 -6.22
Ours (Half 2) -1.99 -6.80 -8.61 -7.28 -9.93
LoRAFull 50.04 48.64 48.67 48.44 48.90
Random (Half 1) -1.39 -0.08 -1.84 -0.93 -2.75
Ours (Half 1) -0.54 -2.33 -2.48 -2.95 -2.02
Random (Half 2) -1.23 -0.96 -1.75 -0.93 -2.57
Ours (Half 2) -0.05 -1.93 -3.77 -3.14 -4.39
Full
FinetuningFull 44.61 42.33 42.79 44.01 41.58
Random (Half 1) +0.08 -0.62 -0.36 -2.94 +2.19
Ours (Half 1) +0.50 -1.34 -0.46 -5.33 +4.39
Random (Half 2) -1.01 0.00 +0.74 -1.65 +3.11
Ours (Half 2) -0.11 -0.75 -0.27 -0.92 +0.36
Table 19: Accuracy on downstream CommonsenseQA task with GPT2-small and weight masking, averaged
over three seeds. Ours refers to removing the critical subnetwork. Random refers to removing a random subnetwork
at the same sparsity as the critical subnetwork.
Objective Sparsity T ARGET KG C ONTROL KG C ONTROL LM
Combination (↑) ∆PPL (↑) ∆PPL (↓) ∆PPL (↓)
Expression-only 99.8 [99.7, 99.9] 154.0 [105.4, 181.2] 83.5 [-4.2, 234.9] 4.0 [2.6, 6.2]
Our Method + Expression 95.7 [93.8, 96.7] 909.2 [107.2, 2421.7] -0.5 [-4.7, 5.1] 1.0 [0.8, 1.4]
Our Method 98.6 [97.8, 99.1] 378.1 [74.3, 834.9] 1.6 [-0.7, 4.0] 0.5 [0.3, 0.8]
Table 20: Expression loss study with GPT2-small and weight masking, averaged across three KGs and two seeds.
Random is an average of randomly masked baselines at the same sparsity levels as the discovered knowledge-critical
subnetworks for each KG-seed pair.0.92 0.94 0.96 0.98 1.00
Removed Subnetwork Sparsity02004006008001000120014001600 PPL
TargetKG  PPL for location
Original Sparsity
Found Subnetwork
Randomly Masked
0.92 0.94 0.96 0.98 1.00
Removed Subnetwork Sparsity020406080100ControlKG  PPL for location
Original Sparsity
Found Subnetwork
Randomly Masked
0.92 0.94 0.96 0.98 1.00
Removed Subnetwork Sparsity0.02.55.07.510.012.515.017.5ControlLM  PPL for location
Original Sparsity
Found Subnetwork
Randomly Masked
0.92 0.94 0.96 0.98 1.00
Removed Subnetwork Sparsity0100200300400500 PPL
TargetKG  PPL for representation
Original Sparsity
Found Subnetwork
Randomly Masked
0.92 0.94 0.96 0.98 1.00
Removed Subnetwork Sparsity020406080ControlKG  PPL for representation
Original Sparsity
Found Subnetwork
Randomly Masked
0.92 0.94 0.96 0.98 1.00
Removed Subnetwork Sparsity0.02.55.07.510.012.515.017.5ControlLM  PPL for representation
Original Sparsity
Found Subnetwork
Randomly Masked
0.92 0.94 0.96 0.98 1.00
Removed Subnetwork Sparsity25
0255075100125150175 PPL
TargetKG  PPL for communication
Original Sparsity
Found Subnetwork
Randomly Masked
0.92 0.94 0.96 0.98 1.00
Removed Subnetwork Sparsity020406080100120ControlKG  PPL for communication
Original Sparsity
Found Subnetwork
Randomly Masked
0.92 0.94 0.96 0.98 1.00
Removed Subnetwork Sparsity0.02.55.07.510.012.515.017.5ControlLM  PPL for communication
Original Sparsity
Found Subnetwork
Randomly MaskedFigure 2: Removing and adding parameters to the remaining GPT2-small model, averaged over five seeds, with
standard deviation depicted as the filled area around the average curves. The x-axis is the removed subnetwork
sparsity. The y-axis is the ∆PPL = PPL( f(x,˜m⊙θ)) - PPL( f(x,θ)) for the different datasets. Vertical dashed
lines show the original sparsity of the critical subnetwork. The darker curve is the outcome starting from the critical
subnetwork, whereas the lighter curve is from a randomly masked model at the same sparsity.
Mask Sparsity T ARGET KG C ONTROL KG C ONTROL LM
Pattern ( ↑) ∆PPL (↑) ∆PPL (↓) ∆PPL (↓)
Original 98.8 379.1 0.7 0.4
Union 96.9 1984.4 44.9 4.7
Floral 99.5 4.9 4.5 1.1
Intersection 99.9 7.9 3.7 2.1
Table 21: Composing subnetworks across KGs with GPT2-small and weight masking, averaged across three
seeds. Original stands for the individual subnetwork removal average across the same three seeds and KGs.representation-735representation-1318representation-84location-735location-1318location-84
communication-735communication-1318communication-84L12 FF-2
L12 FF-1
L12 Attn-Out
L12 Attn-Wq,Wk,Wv
L11 FF-2
L11 FF-1
L11 Attn-Out
L11 Attn-Wq,Wk,Wv
L10 FF-2
L10 FF-1
L10 Attn-Out
L10 Attn-Wq,Wk,Wv
L9 FF-2
L9 FF-1
L9 Attn-Out
L9 Attn-Wq,Wk,Wv
L8 FF-2
L8 FF-1
L8 Attn-Out
L8 Attn-Wq,Wk,Wv
L7 FF-2
L7 FF-1
L7 Attn-Out
L7 Attn-Wq,Wk,Wv2.3 1.8 3.3 2.0 1.5 1.4 1.4 1.1 1.2
1.9 1.3 2.9 1.4 1.0 1.0 1.0 0.7 0.8
2.8 2.6 3.7 2.7 2.5 2.5 2.5 2.3 2.4
1.3 1.2 2.3 1.2 1.0 1.1 1.0 0.8 1.0
1.1 0.9 2.0 1.0 0.7 0.7 0.7 0.5 0.6
1.1 0.8 2.2 0.9 0.6 0.6 0.6 0.4 0.5
0.4 0.3 1.2 0.3 0.2 0.2 0.2 0.1 0.2
0.8 0.6 1.3 0.6 0.5 0.5 0.5 0.3 0.4
0.8 0.7 1.7 0.7 0.5 0.5 0.5 0.4 0.5
1.0 0.8 2.1 0.8 0.5 0.5 0.6 0.4 0.5
0.4 0.3 1.5 0.4 0.2 0.2 0.2 0.1 0.2
0.9 0.7 1.6 0.8 0.6 0.6 0.6 0.5 0.5
0.9 0.8 1.9 0.8 0.6 0.6 0.7 0.5 0.5
1.2 0.9 2.4 0.9 0.6 0.6 0.7 0.5 0.5
0.6 0.5 2.4 0.8 0.3 0.7 0.4 0.2 0.3
1.1 1.1 2.2 1.1 0.9 0.9 0.9 0.8 0.9
1.3 1.1 2.9 1.1 0.8 0.8 0.9 0.6 0.7
1.6 1.1 3.3 1.1 0.7 0.7 0.9 0.6 0.6
1.2 0.8 3.4 0.8 0.5 0.5 0.7 0.4 0.9
1.9 1.5 3.3 1.4 1.1 1.1 1.3 1.0 1.1
2.2 1.7 4.3 1.6 1.2 1.2 1.5 1.0 1.1
2.8 1.9 4.9 1.9 1.3 1.3 1.5 1.0 1.1
3.9 2.7 7.3 2.3 1.7 1.6 2.3 1.7 1.6
4.5 3.2 6.1 2.7 2.2 2.1 2.8 2.2 2.1Density % for all layer masks
0.51.01.52.02.53.03.54.04.5Figure 3: Average module mask density with weight masking, for different KGs ( representation ,location ,
andcommunication ) and seeds. Reported in percentage (%). The brighter the color, the higher the removed mask
density.representation-735representation-1318representation-84location-735location-1318location-84
communication-735communication-1318communication-84L12 FF-2
L12 FF-1
L12 Attn-Out
L12 Attn-Wq,Wk,Wv
L11 FF-2
L11 FF-1
L11 Attn-Out
L11 Attn-Wq,Wk,Wv
L10 FF-2
L10 FF-1
L10 Attn-Out
L10 Attn-Wq,Wk,Wv
L9 FF-2
L9 FF-1
L9 Attn-Out
L9 Attn-Wq,Wk,Wv
L8 FF-2
L8 FF-1
L8 Attn-Out
L8 Attn-Wq,Wk,Wv
L7 FF-2
L7 FF-1
L7 Attn-Out
L7 Attn-Wq,Wk,Wv6.3 5.0 5.7 4.9 3.3 4.6 5.9 6.0 5.3
3.9 4.2 4.2 3.5 3.3 3.5 4.3 3.6 3.3
6.1 5.7 5.7 6.8 6.6 6.2 6.1 6.2 5.9
17.8 15.1 15.6 17.7 16.7 17.6 17.1 15.2 11.1
2.9 3.0 2.6 2.6 3.0 2.8 3.1 3.0 2.6
2.6 2.1 2.5 3.0 2.5 3.1 2.5 2.6 1.8
0.3 0.0 0.1 0.3 0.1 0.3 0.3 0.3 0.0
0.8 0.3 0.5 0.3 0.4 0.3 0.8 0.4 0.5
4.1 4.4 4.1 3.7 4.0 4.0 3.8 4.1 3.3
2.3 1.2 1.6 1.8 2.1 2.2 2.5 2.5 3.1
0.8 1.0 1.0 0.3 0.4 0.7 0.8 0.8 0.5
2.1 1.2 2.0 0.5 0.4 1.4 1.0 0.5 1.4
4.1 4.0 3.6 4.2 3.8 3.7 3.1 3.5 2.9
2.9 2.1 2.2 1.6 1.3 1.4 1.7 1.3 2.0
2.5 3.1 2.3 0.8 0.9 0.9 0.7 2.2 2.9
3.9 4.6 3.6 0.1 0.0 0.4 0.3 3.4 5.3
4.3 4.4 3.9 4.4 4.7 4.5 4.1 4.1 3.7
3.0 5.5 4.2 6.5 5.9 7.2 8.3 6.5 6.4
0.8 0.7 1.0 1.0 1.3 1.2 1.6 1.0 0.8
0.7 1.8 1.2 2.3 2.0 2.5 2.6 2.1 1.6
5.9 6.8 6.8 5.4 5.3 5.0 6.2 6.7 6.0
3.1 4.0 3.1 3.9 5.1 4.7 6.9 5.3 4.8
24.9 21.6 21.5 17.6 18.6 16.7 14.1 19.9 20.4
25.8 23.6 25.1 18.9 19.8 18.6 17.1 23.0 21.1Density % for all layer masks
0123456Figure 4: Average module mask density with neuron masking, for different KGs ( representation ,location ,
andcommunication ) and seeds. Reported in percentage (%). The brighter the color, the higher the removed mask
density.1 2 3 4 5 6 7 8 910 11 12
Heads12
11
10
9
8
7Layers4.38 0.19 0.46 0.59 0.74 0.13 0.4 0.15 3.87 0.15 0.76 0.36
0.61 0.15 0.33 0.21 0.23 0.79 0.21 0.73 0.34 0.76 0.51 0.53
0.54 0.08 0.41 1.95 0.31 0.67 0.19 0.45 0.4 0.39 1.02 0.92
0.55 0.34 0.99 0.47 0.94 1.21 0.21 1.97 1.11 0.81 1.4 0.63
1.9 0.55 0.93 1.54 1.29 0.8 0.76 0.32 1.94 1.4 0.52 1.0
2.56 2.08 1.07 1.17 2.42 2.87 1.23 3.09 2.77 4.84 0.84 1.78location-1318
1234
1 2 3 4 5 6 7 8 910 11 12
Heads12
11
10
9
8
7Layers4.37 0.27 0.66 0.81 0.93 0.18 0.56 0.19 4.33 0.28 1.11 0.57
0.91 0.24 0.5 0.29 0.37 0.77 0.37 1.0 0.43 0.92 0.6 1.11
0.92 0.12 0.86 1.86 0.45 0.67 0.31 0.31 0.55 0.99 1.07 1.06
1.14 0.45 1.33 0.58 1.43 1.38 0.26 2.14 1.36 0.98 1.54 0.7
2.39 1.25 1.3 1.65 1.46 1.03 1.03 0.47 2.8 1.44 0.72 1.18
4.08 3.69 1.21 0.95 3.91 2.42 1.93 3.95 3.49 2.6 1.13 2.51location-735
1234
123456789101112
Heads12
11
10
9
8
7Layers4.42 0.33 0.59 0.97 0.85 0.17 0.52 0.21 4.24 0.24 1.12 0.61
1.05 0.25 0.49 0.29 0.29 0.9 0.31 1.08 0.4 0.89 0.64 0.85
0.64 0.11 0.63 2.0 0.39 0.69 0.25 0.46 0.46 0.67 1.05 1.15
0.6 0.51 1.25 0.58 1.38 1.39 0.28 2.27 1.33 0.98 1.46 0.71
2.44 0.94 1.51 1.63 1.49 1.03 1.08 0.5 2.75 1.52 0.99 2.36
3.65 2.94 1.2 1.2 4.63 2.68 1.76 3.68 3.15 10.18 1.17 2.52representation-1318
246810
123456789101112
Heads12
11
10
9
8
7Layers4.07 0.3 0.76 0.97 0.95 0.19 0.73 0.24 5.69 0.27 1.3 0.6
1.43 0.35 0.68 0.37 0.45 0.76 0.52 1.17 0.48 0.86 0.88 1.29
1.14 0.14 1.16 2.11 0.59 0.59 0.4 0.5 0.67 1.1 0.96 1.4
0.94 0.63 1.44 0.66 1.1 1.48 0.37 2.48 1.62 0.89 1.39 0.79
3.01 1.58 2.54 1.66 1.66 1.34 0.98 0.7 3.51 1.66 1.71 2.45
5.67 2.84 1.26 1.4 5.07 1.86 1.06 5.27 4.11 21.38 1.8 2.88representation-735
5101520
1 2 3 4 5 6 7 8 910 11 12
Heads12
11
10
9
8
7Layers4.32 0.15 0.32 0.42 0.55 0.11 0.3 0.11 2.98 0.12 0.52 0.28
0.39 0.1 0.28 0.14 0.2 0.78 0.11 0.52 0.26 0.61 0.28 0.4
0.37 0.08 0.28 1.66 0.25 0.54 0.13 0.41 0.24 0.23 0.95 0.72
0.52 0.28 0.79 0.39 1.23 1.08 0.18 1.74 0.85 0.72 1.22 0.52
1.54 0.37 0.62 1.45 1.27 0.61 0.61 0.27 1.86 1.33 0.4 1.48
2.01 2.27 0.97 0.66 1.89 2.79 1.13 2.73 2.65 6.75 0.72 1.69communication-1318
246
1 2 3 4 5 6 7 8 910 11 12
Heads12
11
10
9
8
7Layers4.52 0.18 0.41 0.65 0.67 0.15 0.43 0.16 3.72 0.2 0.75 0.44
0.77 0.17 0.43 0.18 0.24 0.76 0.2 0.74 0.32 0.85 0.45 0.64
0.66 0.1 0.44 1.72 0.33 0.72 0.19 0.25 0.29 0.52 0.99 0.9
0.9 0.38 1.02 0.53 1.05 1.27 0.19 1.78 1.1 0.84 1.46 0.62
2.14 0.71 1.29 1.73 1.32 0.82 0.89 0.38 2.67 1.47 0.57 1.67
2.9 2.82 1.1 0.82 3.17 2.78 1.45 3.52 3.07 9.1 1.04 2.18communication-735
2468Figure 5: Density percentage (%) of different heads across different attention layers for weight masking. Each
row represents a different KG and each column is a different seed.WqWkWv12
11
10
9
8
7Layers0.35 0.47 2.23
0.44 0.46 0.45
0.73 0.67 0.44
1.08 0.95 0.63
1.31 1.18 0.75
2.07 1.86 2.74location-1318
0.51.01.52.02.5
WqWkWv12
11
10
9
8
7Layers0.5 0.66 2.4
0.64 0.65 0.58
0.86 0.83 0.6
1.28 1.14 0.91
1.63 1.47 1.08
2.45 2.15 3.36location-735
1.01.52.02.53.0
WqWkWv12
11
10
9
8
7Layers0.55 0.68 2.34
0.64 0.66 0.57
0.78 0.79 0.56
1.32 1.09 0.78
1.83 1.57 1.16
3.03 2.83 3.83representation-1318
1.01.52.02.53.03.5
WqWkWv12
11
10
9
8
7Layers0.64 0.82 2.56
0.81 0.88 0.62
1.0 1.02 0.67
1.4 1.18 0.86
2.26 1.95 1.49
4.59 4.31 4.75representation-735
1234
WqWkWv12
11
10
9
8
7Layers0.25 0.38 1.92
0.34 0.35 0.32
0.57 0.56 0.34
0.99 0.92 0.47
1.19 1.15 0.61
2.1 1.86 2.61communication-1318
0.51.01.52.02.5
WqWkWv12
11
10
9
8
7Layers0.39 0.51 2.16
0.47 0.52 0.45
0.66 0.64 0.48
1.13 0.94 0.72
1.55 1.42 0.95
2.74 2.46 3.29communication-735
0.51.01.52.02.53.0Figure 6: Density percentage (%) of Wq,Wk, and Wvmasks in attention layers for weight masking. Each row
represents a different KG and each column is a different seed.Att-OutFF-1 FF-212
11
10
9
8
7Layers2.47 0.99 1.45
0.21 0.6 0.71
0.23 0.52 0.51
0.34 0.63 0.62
0.49 0.75 0.8
1.72 1.27 1.21location-1318
0.51.01.52.0
Att-OutFF-1 FF-212
11
10
9
8
7Layers2.7 1.42 1.96
0.33 0.87 0.95
0.37 0.81 0.72
0.81 0.94 0.84
0.78 1.12 1.07
2.27 1.89 1.6location-735
0.51.01.52.02.5
Att-OutFF-1 FF-212
11
10
9
8
7Layers2.62 1.32 1.83
0.32 0.84 0.93
0.33 0.75 0.68
0.47 0.9 0.79
0.83 1.08 1.06
2.69 1.86 1.68representation-1318
0.51.01.52.02.5
Att-OutFF-1 FF-212
11
10
9
8
7Layers2.79 1.92 2.34
0.41 1.12 1.1
0.45 1.04 0.77
0.57 1.23 0.9
1.21 1.6 1.26
3.9 2.8 2.2representation-735
0.51.01.52.02.53.03.5
Att-OutFF-1 FF-212
11
10
9
8
7Layers2.34 0.71 1.07
0.12 0.43 0.52
0.14 0.39 0.39
0.23 0.48 0.46
0.41 0.56 0.65
1.71 1.02 1.04communication-1318
0.51.01.52.0
Att-OutFF-1 FF-212
11
10
9
8
7Layers2.47 0.97 1.42
0.2 0.61 0.71
0.24 0.56 0.54
0.44 0.7 0.66
0.67 0.86 0.94
2.3 1.5 1.47communication-735
0.51.01.52.0Figure 7: Density percentage (%) of Att-Out, FF-1, and FF-2 masks for weight masking. Each row represents a
different KG and each column is a different seed.309,826 206,17038,087
237,12639,95132,59834,156seed-735
seed-1318
seed-84Parameter overlap for different seeds and same KG "communication.n.02"
379,020 256,87347,431
253,70747,77636,62739,926seed-735
seed-1318
seed-84Parameter overlap for different seeds and same KG "location.n.01"
402,680312,36154,114
919,104147,567 78,04169,442seed-735seed-1318
seed-84Parameter overlap for different seeds and same KG "representation.n.02"Figure 8: Venn diagrams for parameter overlap of three subnetworks identified under three different random
seeds with weight masking, for each KG representation ,location , andcommunication .549,120360,960301,056
386,304112,896370,1761,100,544seed-735 seed-1318
seed-84Parameter overlap for different seeds and same KG "communication.n.02"
246,528
359,424157,440
294,912304,128
150,5281,230,336seed-735 seed-1318
seed-84Parameter overlap for different seeds and same KG "location.n.01"
471,552476,160217,344
320,256347,136274,1761,144,320seed-735 seed-1318
seed-84Parameter overlap for different seeds and same KG "representation.n.02"Figure 9: Venn diagrams for parameter overlap of three subnetworks identified under three different random
seeds with input neuron masking, for each KG representation ,location , andcommunication .representationlocation
communicationL12 FF-2
L12 FF-1
L12 Attn-Out
L12 Attn-Wq,Wk,Wv
L11 FF-2
L11 FF-1
L11 Attn-Out
L11 Attn-Wq,Wk,Wv
L10 FF-2
L10 FF-1
L10 Attn-Out
L10 Attn-Wq,Wk,Wv
L9 FF-2
L9 FF-1
L9 Attn-Out
L9 Attn-Wq,Wk,Wv
L8 FF-2
L8 FF-1
L8 Attn-Out
L8 Attn-Wq,Wk,Wv
L7 FF-2
L7 FF-1
L7 Attn-Out
L7 Attn-Wq,Wk,Wv0.06 0.04 0.04
0.04 0.04 0.04
0.1 0.11 0.11
0.07 0.13 0.13
0.05 0.05 0.05
0.03 0.03 0.03
0.01 0.01 0.01
0.03 0.03 0.03
0.02 0.02 0.02
0.02 0.02 0.01
0.01 0.01 0.01
0.04 0.04 0.05
0.02 0.02 0.02
0.02 0.02 0.02
0.01 0.02 0.01
0.03 0.04 0.04
0.02 0.02 0.02
0.02 0.02 0.01
0.01 0.02 0.02
0.03 0.05 0.05
0.03 0.03 0.02
0.03 0.02 0.02
0.05 0.03 0.06
0.05 0.04 0.05Jaccard similarity of different seeds for the same KG per module
0.020.040.060.080.100.12Figure 10: Jaccard similarity of different seed masks for the same KG with weight masking, (representation ,
location , andcommunication ). The brighter the color, the higher the Intersection over Union.representationlocation
communicationL12 FF-2
L12 FF-1
L12 Attn-Out
L12 Attn-Wq,Wk,Wv
L11 FF-2
L11 FF-1
L11 Attn-Out
L11 Attn-Wq,Wk,Wv
L10 FF-2
L10 FF-1
L10 Attn-Out
L10 Attn-Wq,Wk,Wv
L9 FF-2
L9 FF-1
L9 Attn-Out
L9 Attn-Wq,Wk,Wv
L8 FF-2
L8 FF-1
L8 Attn-Out
L8 Attn-Wq,Wk,Wv
L7 FF-2
L7 FF-1
L7 Attn-Out
L7 Attn-Wq,Wk,Wv0.44 0.36 0.45
0.36 0.37 0.35
0.51 0.58 0.53
0.48 0.5 0.29
0.35 0.36 0.32
0.25 0.36 0.24
0.0 0.5 0.0
0.12 0.2 0.22
0.34 0.4 0.29
0.2 0.55 0.4
0.23 0.33 0.38
0.17 0.08 0.19
0.27 0.4 0.29
0.23 0.2 0.11
0.37 0.75 0.07
0.22 0.0 0.0
0.25 0.4 0.31
0.21 0.39 0.4
0.07 0.5 0.29
0.09 0.36 0.35
0.32 0.47 0.35
0.16 0.3 0.34
0.42 0.52 0.39
0.5 0.66 0.5Jaccard similarity of different seeds for the same KG per module
0.00.10.20.30.40.50.60.7Figure 11: Jaccard similarity of different seed masks for the same KG with input neuron masking,
(representation ,location , andcommunication ). The brighter the color, the higher the Intersection over
Union.478,547 336,76581,022
265,31560,33942,47153,895representation.n.02
location.n.01
communication.n.02Parameter overlap for different KGs and the same seed "735"
380,563 255,06552,010
195,87441,35533,75240,030representation.n.02
location.n.01
communication.n.02Parameter overlap for different KGs and the same seed "1318"
1,021,277 223,95779,177
199,50969,42030,62244,280representation.n.02
location.n.01
communication.n.02Parameter overlap for different KGs and the same seed "84"Figure 12: Venn diagrams for parameter overlap of three subnetworks identified under three different KGs
with weight masking, for each seed 735,1318 , and84.735131884L12 FF-2
L12 FF-1
L12 Attn-Out
L12 Attn-Wq,Wk,Wv
L11 FF-2
L11 FF-1
L11 Attn-Out
L11 Attn-Wq,Wk,Wv
L10 FF-2
L10 FF-1
L10 Attn-Out
L10 Attn-Wq,Wk,Wv
L9 FF-2
L9 FF-1
L9 Attn-Out
L9 Attn-Wq,Wk,Wv
L8 FF-2
L8 FF-1
L8 Attn-Out
L8 Attn-Wq,Wk,Wv
L7 FF-2
L7 FF-1
L7 Attn-Out
L7 Attn-Wq,Wk,Wv0.06 0.05 0.05
0.05 0.05 0.04
0.15 0.13 0.12
0.12 0.12 0.06
0.05 0.05 0.03
0.03 0.03 0.02
0.01 0.01 0.0
0.04 0.04 0.03
0.02 0.02 0.02
0.02 0.02 0.01
0.02 0.01 0.01
0.05 0.06 0.04
0.03 0.02 0.02
0.02 0.02 0.01
0.01 0.01 0.01
0.05 0.05 0.03
0.03 0.02 0.02
0.02 0.02 0.01
0.02 0.02 0.01
0.05 0.05 0.03
0.03 0.03 0.02
0.03 0.02 0.02
0.03 0.04 0.02
0.04 0.04 0.03Jaccard similarity of different KGs for the same seed per module
0.020.040.060.080.100.120.14Figure 13: Jaccard similarity of different KG masks for the same seed with weight masking, (735,1318 , and
84). The brighter the color, the higher the Intersection over Union.