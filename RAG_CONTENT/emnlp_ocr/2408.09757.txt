Strategic Demonstration Selection for Improved
Fairness in LLM In-Context Learning
Jingyu Hu
University of Bristol
Bristol, UK
ym21669@bristol.ac.ukWeiru Liu
University of Bristol
Bristol, UK
weiru.liu@bristol.ac.ukMengnan Du
New Jersey Institute of Technology
Newark, USA
mengnan.du@njit.edu
Abstract
Recent studies highlight the effectiveness of
using in-context learning (ICL) to steer large
language models (LLMs) in processing tabular
data, a challenging task given the structured
nature of such data. Despite advancements in
performance, the fairness implications of these
methods are less understood. This study in-
vestigates how varying demonstrations within
ICL prompts influence the fairness outcomes
of LLMs. Our findings reveal that deliberately
including minority group samples in prompts
significantly boosts fairness without sacrific-
ing predictive accuracy. Further experiments
demonstrate that the proportion of minority to
majority samples in demonstrations affects the
trade-off between fairness and prediction accu-
racy. Based on these insights, we introduce a
mitigation technique that employs clustering
and evolutionary strategies to curate a diverse
and representative sample set from the train-
ing data. This approach aims to enhance both
predictive performance and fairness in ICL ap-
plications. Experimental results validate that
our proposed method dramatically improves
fairness across various metrics, showing its ef-
ficacy in real-world scenarios.
1 Introduction
Large Language Models (LLMs), such as GPT-
4 (OpenAI, 2023), Claude-3 (AnthropicAI, 2023),
and LLaMA-2 (Touvron et al., 2023), have
achieved state-of-the-art performance in many nat-
ural language processing tasks. These LLMs
can adapt to different tasks by adding in-context
prompts, without needing to retrain on the entire
new dataset. This optimization technique for LLMs
is called in-context learning (ICL), which leverages
specific input prompts to guide LLMs to generate
more accurate outputs. Recent research suggests
that incorporating specially selected demonstra-
tions into these prompts can significantly enhance
LLM performance (Brown et al., 2020; Schick and
Schütze, 2020).Due to prompt length limitations, traditional
LLMs have faced challenges in processing demon-
strations from tabular datasets, which have a large
number of features. However, with recent LLMs
relaxing input length constraints, new avenues for
applications in tabular datasets are opening up.
(Hegselmann et al., 2023) has confirmed the pre-
dictive capabilities of LLMs on datasets from UCI
repository. Considering the usages of tabular data
in high-stakes domains (Grinsztajn et al., 2022),
ensuring fairness alongside prediction performance
is crucial for increasing trust in LLMs. (Liu et al.,
2023) has highlighted biases in LLM predictions
with tabular datasets, but there are limited further
investigations on how the fairness of LLMs perfor-
mance varies with different ICL demonstrations.
To bridge this gap, we aim to answer the fol-
lowing research question: How do different demon-
stration strategies impact the fairness performance
of LLMs on tabular classification tasks? And is
there a demonstration strategy better than other
strategies? To better understand the impact of in-
context learning on fairness, our proposed demon-
stration strategy considers the distribution of both
demographic groups and target labels. A dataset
can be divided into subgroups by demographic fea-
tures, labelling the smallest as the minority (un-
derrepresented) and the larger one as the majority.
The fairness investigation compares differences be-
tween majority and minority groups. Our inves-
tigation includes evaluating five advanced LLMs,
i.e., Text-davinci-003, GPT-3.5-turbo, GPT-4-turbo
1, Claude-3 Haiku, and Claude-3 Sonnet2, across
two fairness-focused tabular datasets: Credit and
Adult. We found that prioritizing underrepresented
samples and conscientiously including minority de-
mographic groups and target labels during few-shot
learning can significantly improve the fairness per-
formance in LLMs output.
1https://platform.openai.com/docs/models/
2https://www.anthropic.com/news/claude-3-familyarXiv:2408.09757v1  [cs.LG]  19 Aug 2024Despite the experimental observations, we are
still wondering: Why does prioritizing minority
group demonstrations benefit the fairness perfor-
mance of LLMs in tabular-based classification
tasks? To further clarify this phenomenon, we per-
turb prediction labels and sensitive features in se-
lected demonstrations and compare how the predic-
tion outcomes of LLMs would be altered. Through
these perturbation experiments, we found that in-
creasing the proportion of underrepresented labels
enhances fairness, but can lead to a decline in pre-
diction performance, and vice versa.
Up until now, the above findings and explana-
tions have been based on random demonstration
selection. We hypothesize that: We can deliberately
select demonstrations to further improve fairness
performance. Motivated by the fiLter-thEn-Search
(LENS) (Li and Qiu, 2023) for textual classifica-
tion, we adopt a similar process for extracting tabu-
lar demonstrations: first refine the training data set
into a candidate pool and then sample and evaluate
these candidates to identify the most supportive
demonstrations. To this end, we introduced the
Fairness via Clustering- Genetic (FCG) algorithm
to effectively extract representative samples, to fur-
ther enhance the fairness of LLM. Unlike LENS,
which relies on progressive iterations on LLMs for
candidate selection, our FCG method utilizes clus-
tering. Clustering does not require access to LLMs
and maintains diversity among the selected shots,
effectively addressing concerns related to the time
required for iterations and the cost of LLM access.
Additionally, previous studies often assume the
same selection probabilities for candidates across
evaluation iterations, requiring enormous iterations
to ensure that each sample is equally considered.
Inspired by genetic evolution concepts, we adopt
dynamic probabilities which give priority to rep-
resentative samples with higher selection chances.
Sample representativeness is measured by the LLM
performance score, whose score is updated for each
iteration. In this way, FCG can narrow the final
sample set more efficiently, and drastically reduce
the number of iterations needed. We implement
experiments to evaluate the proposed FCG demon-
stration selection method. The results confirm that
FCG algorithm improves LLMs performance in
almost all strategies, with prioritizing the minority
group still yielding the best results.
To conclude, the main contributions in this paper
are as follows:•We find that prioritizing underrepresented sam-
ples and conscientiously including minority de-
mographic groups and target labels during few-
shot learning can dramatically improve the fair-
ness performance in LLM output (Section 3).
•We explain why prioritizing minorities leads to
a fairer solution, and find the trade-off between
LLMs’ performance and demographic labels: in-
creasing the ratio of underrepresented labels en-
hances fairness, but can lead to a decline in pre-
diction performance, and vice versa (Section 4).
•We propose the FCG (Fairness via Clustering-
Genetic) algorithm, an efficient approach to re-
trieve a diverse and representative set of demon-
strations from training data. Across almost all
strategies, FCG enhances fairness in LLMs under
in-context learning (Section 5).
2 Experiment Setup
Our primary goal is to investigate how different
few-shot demonstration choices influence the fair-
ness performance of LLMs under the in-context
learning (ICL) setting. Detailed related work on
this area is discussed in Appendix A. In this section,
we introduce the overall experimental setups.
Notations. Given a dataset D= (X, Y, Z )n
i=1
where features X∈ R , the binary classification
labels Y∈ Y :={0,1}, and sensitive feature
Z∈ Z:={0,1}. We set Z= 0to represent the
minority group and Z= 1as the majority group. D
is split into training dataset Dtr, validation dataset
Ddevand testing dataset Dtest. For each data point
d∈D:={x, y, z}, a classifier fpredicts the label
f(x)based on the input features x.
Given a subset D′∈Dtr, the proportion of
samples where Z= 0 within D′is denoted as
rz. Specifically, rz= 1 means all samples in D′
belong to a minority group, whereas rz= 0 im-
plies that every sample in D′is from the majority
group. Similarly, the proportion of samples for
which Y= 0within D′is represented by ry.
rz=Count (D′
Z=0)
Count (D′), ry=Count (D′
Y=0)
Count (D′)(1)
Models and Datasets. We use five LLMs as f:
Text-davinci-003 (Davinci), GPT-3.5-turbo, GPT-
4-turbo, Claude-3 Haiku, and Claude-3 Sonnet.
The temperature in the model parameter is set to
zero to ensure consistent responses. We select two
tabular-based fairness datasets: default of credit
card clients dataset (Credit, (Yeh, 2016)) and adultincome (Adult, (Becker and Kohavi, 1996)). The
Credit dataset covers information on credit card
clients in Taiwan, including demographics, bills,
payment history, etc. Its target is to predict whether
there will be an overdue payment next month. The
Adult dataset is to predict whether an individual’s
annual income exceeds 50K based on their individ-
ual features. Appendix B contains further descrip-
tions of dataset structures.
Evaluation Metrics. The predictive performance
of LLMs on labels Yis evaluated by metrics
Accuracy ,Precision ,Recall , and F-score3. We
introduce ∆dp,Rdp,∆eo, and Reoto evaluate fair-
ness4. They refer to the differences and ratios
of Demographic Parity (DP) (Dwork et al., 2012)
and Equalized Odds (Eodds) (Hardt et al., 2016)
between subgroups.
The demographic parity of the two groups parti-
tioned by Zis defined by Equation 2. DP difference
∆dprepresents the difference between two, and DP
ratioRdpis the ratio of the DP 0andDP 1.
DP 0=P(f(x) = 1|Z= 0)
DP 1=P(f(x) = 1|Z= 1)(2)
The True Positive Rate (TPR) and False Positive
Rate (FPR) for both subgroups ( Z= 0andZ= 1)
are defined as follows.
TPR 0=P(f(x) = 1|y= 1, Z= 0)
TPR 1=P(f(x) = 1|y= 1, Z= 1)
FPR 0=P(f(x) = 1|y= 0, Z= 0)
FPR 1=P(f(x) = 1|y= 0, Z= 1)(3)
Eodds difference ∆eois defined as the greater
metrics of TPR and FPR differences (Equation 4)
where ∆TPR = (TPR 1−TPR 0)and∆FPR =
(FPR 1−FPR 0).
∆eo= max(∆ TPR, ∆FPR ) (4)
Eodds ratio Reois the smaller ratio of TPR and the
ratio of FPR between two groups, as shown below.
Here ϵis used to avoid the setting where the
denominator is zero, where we set ϵ= 1−6:
Reo= minTPR 0
TPR 1+ϵ,FPR 0
FPR 1+ϵ
(5)
The four fairness metrics range from 0 to 1.
Lower ∆dpand∆eoshow smaller performance
differences between groups, which points to fairer
3https://scikit-learn.org/stable/
4https://fairlearn.org/main/user_
guide/fairness_in_machine_learning.htmlpredictions. Higher ReoandRdpreflect more con-
sistent performance across subgroups, suggesting
better fairness.
Prompt Template. The output answer from the
LLMs is based on the input prompt. As shown in
Figure 1, the structure of a prompt can be divided
into three parts: task description, in-context demon-
strations, and questions. Part ❶clarifies the task
and defines the format of output prediction label
options. Part ❷contains a series of demonstrations
as references. Part ❸is the sample to be predicted.
We consider both zero-shot learning and few-
shot learning in our experiments. Zero-shot learn-
ing refers to LLMs with a prompt exclude demon-
stration references (without ❷) and is set as the
baseline. Few-shot learning, sometimes also called
in-context learning (ICL), consists of all three
parts as input prompts. We compare how differ-
ent demonstrations in part ❷influence the fairness
of LLMs. The prompt example in Figure 1 sim-
plifies the tabular dataset, the detailed template is
provided in Appendix C.
3 How Demonstrations Impact Fairness
of LLMs for Tabular Inputs?
In this section, we aim to answer: how do few shot
demonstrations influence the fairness performance
of LLMs for processing tabular inputs under the
in-context learning setting?
To investigate this, we examine fairness perfor-
mance variances across different demonstrations.
We propose different combinations of prediction
feature distribution rzand sensitive feature distri-
bution ry, expecting to explore the potential corre-
lation between these feature distributions and LLM
fairness. In the experiment, different demonstra-
tions are based on three distinct sampling strategies
denoted as S1,S2, and S3, each with unique dis-
tribution combinations of rzandry.
•S1: Balanced Samples with Balanced Labels
(rz= 0.5, ry= 0.5);
•S2: Prioritize Minority Samples with Balanced
Labels ( rz= 1, ry= 0.5);
•S3: Prioritize Minority Samples with Unbal-
anced Labels ( rz= 1, ry̸= 0.5).
Figure 2 displays the performance of different
LLMs on the Credit dataset. ryis set to 1 in S3. The
fairness performance improves when prioritizing
samples from minority groups ( rz= 1) compared
to a balanced sample selection ( rz= 0.5).❷In-Context*PerturbedDemonstrations:
❶Description: Predictloanapproval: Yes | No❸Question:Test Sample:❷In-ContextSelectedDemonstrations:
PerturbationsonGender/Prediction
Zero-shot Learning (Baseline)
Few-shot Learning (ICL)Prediction
LLMs
/…❶Clarify taskandoutput options❷SelectKdemonstrations from train set❸Onetest sample each round
…
…The Workflow of Prompts in LLMsExplanationExampleFigure 1: The Prompt Template and Content Example (*Perturbation is optional and is used to test the effectiveness
of ICL. We discussed perturbations in Section 4)
0.20.40.60.81.0Zero-shotS1S2S3Text - Davinci - 3AccuracyF-scoreDP%Eodds%0.20.30.40.50.60.70.80.91.0Zero-shotS1S2S3GPT 3.5 - turbo0.20.30.40.50.60.70.80.91.0Zero-shotS1S2S3GPT 4 - turboR!"R#$
Figure 2: Prediction and fairness performance comparison across different LLMs on Credit dataset. It shows
improvements in fairness metrics when samples from minority groups are prioritized.
Similar findings are found in the Adult dataset.
Table 1 presents the performance of the GPT-3.5-
turbo with zero-shot and different few-shot strate-
gies. To ensure the stability and reliability of the re-
sults, we use random seeds set={25, 35, 42, 45, 55}
when selecting few-shot samples. The presented ta-
ble summarizes average values and standard errors
for the random seeds set.
Overall, the results show that all few-shot strate-
gies have generally improved fairness compared
to zero-shot learning without lowering predictions.
Also, prioritizing minorities (S2, S3) is an effec-
tive way to improve fairness. In contrast, balanced
prompts (S1) show worse fairness performance.
To further explain the observed pattern, we imple-
ment additional experiments and discussions on
GPT-3.5’s performance under the Adult dataset in
the following sections. Complete results for other
LLMs (e.g., Claude), using different seeds, are in-
cluded in Appendix D.
4 Why Prioritizing Minority Group
Demonstrations Benefit Fairness?
The above analysis points out a strong correlation
between prioritizing minority group demonstra-
tions with improved fairness performance of LLMs.
DataF𝑟!=1,𝑟"=0.5𝑟"=0.5	➔1𝑟"=0.5	➔0𝑟!=1	➔	0.5𝑟!=1	➔	0𝑟!=0	➔	0.5𝑟!=0	➔	1DataM𝑟!=0,𝑟"=0.5
DataF’ ithperturbationDataM’ithperturbationFourperturbations, select one   (ith)Raw Few-shot
Perturbated Few-shotFigure 3: The Workflow of Perturbations
However, it is not yet clear how and why this phe-
nomenon occurs. Thereby our next step is to clarify
which part of the demonstrations most influenced
the performance of LLMs. Specifically, we perturb
the prediction label Yand the sensitive feature Z
in selected demonstrations and compare how the
prediction outcomes of LLMs would be altered.
The following experiment is performed on the
Adult dataset with ‘income’ as feature Yand ‘gen-
der’ as feature Z. We set the random seed to 55Table 1: Performance of GPT3.5-turbo with zero-shot and different few-shot strategies (S1, S2, S3) on Adult Income
dataset. It demonstrates that strategic inclusion of demonstrations, particularly those from minority groups, can
significantly enhance both predictive performance and fairness outcomes.
Prediction Zero-shot rz= 0.5, ry= 0.5(S1) rz= 1, ry= 0.5(S2) rz= 1, ry= 1(S3)
Accuracy ↑ 0.6855 0.7312 ± 0.0009 0.7328 ± 0.0028 0.7230 ± 0.0014
Precision ↑ 0.8519 0.7936 ± 0.0012 0.7841 ± 0.0051 0.7808 ± 0.0038
Recall ↑ 0.4492 0.6250 ± 0.0012 0.6461 ± 0.0130 0.6122 ± 0.0036
F-score ↑ 0.5882 0.6993 ± 0.0011 0.7060 ± 0.0062 0.6915 ± 0.0017
Fairness Zero-shot rz= 0.5, ry= 0.5(S1) rz= 1, ry= 0.5(S2) rz= 1, ry= 1(S3)
Rdp↑ 0.4063 0.6470 ± 0.0019 0.6769 ± 0.0080 0.6732 ± 0.0095
Reo↑ 0.1111 0.3682 ± 0.0044 0.4152 ± 0.0125 0.4722 ± 0.0187
∆dp↓ 0.2227 0.1688 ± 0.0009 0.1578 ± 0.0031 0.1555 ± 0.0046
∆eo↓ 0.3203 0.1875 ± 0.0019 0.1859 ± 0.0058 0.1906 ± 0.0071
to extract two groups with balanced labels DataF
andDataM from Dtras raw demonstrations. (1)
DataF : balanced high-income and low-income
females ( ry= 0.5, rz= 1). (2) DataM : bal-
anced high-income and low-income males ( ry=
0.5, rz= 0). Figure 3 illustrates the perturbation
workflow. We define four perturbations, each con-
sisting of the feature to be perturbed and the new
proportions after perturbation. For example, per-
turbing ry= 0.5→1means that the quantity of
high-income and low-income samples are balanced
in raw demonstrations ( ry= 0.5), and the per-
turbed demonstrations will become all low-income
samples ( r′
y= 1) by flipping high-income labels to
low-income.
The next part will discuss how perturbations at
different proportions affect the overall prediction
and fairness performance of LLM, along with a
deeper performance comparison within subgroups.
4.1 Perturbations Impact on Overall Fairness
Table 2 compares the prediction and fairness per-
formance with different perturbations on gender
and income.
Perturbing the income labels for DataF and
DataM leads to a certain degree of decline in pre-
dictive performance ( 1%to6%). Min et al., 2022
mentioned a similar phenomenon that replacing
gold labels with random labels only presents the
marginal impact on performance. Nevertheless, we
also found that altering the ground truth labels (in-
come) can greatly affect fairness performance, re-
sulting in a drastic drop in all scenarios. Especially
when replacing labels with high income, the Rdp
in DataF decreased from 71.32% to 50.54%, and
in DataM, it decreased from 50.94% to 43.06%.
When we perturbed gender labels, results showthat fairness performance improves with a higher
proportion of females. The fairness performance
decreases when we perturb from female to male in
DataF , asRdpdecreases from 71.32% to 59.15%.
Similar patterns are observed in DataM , where
fairness gradually increases by 8.1% when modify-
ing from male to female.
In most cases, the perturbation results align with
the intuition that distorting real data can degrade its
quality, thus potentially leading to negative impacts
on LLMs performance. However, we also find that
perturbing to a higher ratio of minority labels ( r′
z=
1) can positively enhance fairness, suggesting a
strong connection between fairness performance
and sensitive labels. To further validate this finding,
Section 4.2 compares performance variations at the
subgroup level.
4.2 Perturbations Impact across Subgroups
Table 3 displays the model performance of TPR
and FPR on both minority (female) and majority
(male) subgroups after income and gender per-
turbations. Similar to DP andEO, the metrics
∆TPR and∆FPR assess performance dispari-
ties between female and male subgroups. Equal
treatment is achieved when these differences ap-
proach zero, hence, lower values of ∆TPR and
∆FPR are preferable. We also consider absolute
values of FPR and TPR within each subgroup to
fully assess fairness changes in perturbations.
In income perturbations, replacing the income
labels results in a decrease in TPR and FPR for
both female and male groups, with a more signif-
icant decline observed in the female group. This
reduction is most notable when income labels are
changed to high-income. In a few cases, the rel-
ative metrics ∆TPR and∆FPR show improve-Table 2: Prediction and Fairness Performance on Income and Gender Perturbations
Different perturbations on income Different perturbations on gender
rz= 1, ry= 0.5(DataF) rz= 0, ry= 0.5(DataM) rz= 1, ry= 0.5(DataF) rz= 0, ry= 0.5(DataM)
Prediction Raw r′
y= 1 r′
y= 0 Raw r′
y= 1 r′
y= 0 Raw r′
z= 0.5r′
z= 0 Raw r′
z= 0.5r′
z= 1
Accuracy ↑0.7480 0.7383 0.6992 0.7148 0.6914 0.6543 0.7480 0.7422 0.7617 0.7148 0.7168 0.7090
Precision ↑0.7873 0.7933 0.8643 0.8438 0.8451 0.8835 0.7873 0.7925 0.7965 0.8438 0.8364 0.8204
Recall ↑ 0.6797 0.6445 0.4727 0.5273 0.4688 0.3555 0.6797 0.6563 0.7031 0.5273 0.5351 0.5352
F-score ↑ 0.7296 0.7112 0.6111 0.6490 0.6030 0.5070 0.7296 0.7179 0.7469 0.6490 0.6556 0.6478
Fairness Raw r′
y= 1 r′
y= 0 Raw r′
y= 1 r′
y= 0 Raw r′
z= 0.5r′
z= 0 Raw r′
z= 0.5r′
z= 1
Rdp↑ 0.7132 0.6508 0.5054 0.5094 0.4639 0.4306 0.7132 0.6308 0.5915 0.5094 0.5421 0.5905
Reo↑ 0.3824 0.3438 0.0556 0.1364 0.1579 0.0000 0.3824 0.2571 0.1500 0.1364 0.2273 0.3043
∆dp↓ 0.1445 0.1719 0.1797 0.2031 0.2031 0.1602 0.1445 0.1875 0.2266 0.2031 0.1914 0.1680
∆eo↓ 0.1641 0.1797 0.226 0.2578 0.2813 0.2266 0.1641 0.2031 0.2656 0.2578 0.2500 0.2109
ment compared to the results without perturbations.
However, the corresponding absolute metrics TPR
and FPR do not show consistent trends and worsen
instead. This inconsistency makes it difficult to val-
idate the impact of income on fairness performance.
Therefore, we conclude that the ground truth labels
in the demonstrations are not the source of benefit
for LLMs’ fairness.
In gender perturbations, however, subgroup per-
formance is greatly affected by these gender label
changes. For absolute values, flipping female la-
bels to male in DataF leads to a 4.69% increase
in FPR and a 5.47% increase in TPR for the male
group. Similarly, transforming male labels to fe-
male in DataM results in increases in both TPR
and FPR for the female group. Similar trends are
found in their relative values. Increasing the pro-
portion of male labels leads to higher ∆TPR and
∆FPR , illustrating greater difference in subgroup
treatment. Conversely, an increase in the ratio of
female labels leads to reductions in both ∆TPR
and∆FPR , suggesting enhanced fairness.
In general, the above results show a trade-off
between the LLMs performance and demographic
labels. LLMs exhibit improved performance when
the proportion of minority groups increases: they
become fairer compared to the used of original
data when perturbing demographic labels male
to female. Therefore, we conclude that priori-
tizing demonstrations from minority groups can
maximize these advantages and promote fairness
in LLMs. In contrast, perturbing labels leads to
demonstrations becoming less reliable, as they can
lead models to learn noise and perform worse. The
perturbation on prediction labels (income) con-
forms with this pattern.5 Mitigation Algorithm for Fair
Demonstration Selection
The above results confirm that the application of
diverse demonstrations, particularly those from the
minority, can drastically influence the fairness of
LLMs. Experiments on different sets of selected
shots under the same strategy also reveal a similar
trend, albeit with different absolute performance
values. This leads to the question: how to extract
representative demonstrations that yield better per-
formance among these sets?
Enumerating and evaluating the outcomes of
LLMs across all possible sets is impractical due
to the sheer number of combinations. Thus, in
this section, we propose a fairness via clustering-
genetic (FCG) algorithm to efficiently select influ-
ential demonstrations, leading LLMs to a better
performance without having to explore all possible
combinations. The core idea of FCG includes three
aspects: (1) Use clustering to shrink the selection
sets while maintaining diverse samples. (2) Define
a score that considers both prediction and fairness
performance, applying a genetic approach to itera-
tively select and score these samples within the sets.
(3) Rank samples from highest to lowest based on
their scores to select the most influential ones.
5.1 The Proposed FCG Algorihtm
We introduce details of the proposed FCG mitiga-
tion algorithm in this section.
Clustering. Based on the value combinations
of the sensitive feature Z and label Y , we divide
the training data Dtrinto four subgroups denoted
as SG ={g1(Z= 1, Y= 0) , g2(Z= 1, Y=
1),g3(Z= 0, Y= 0), g4(Z= 0, Y= 1)}. For
each subgroup, we apply k-means clustering to ex-
tract a diverse and representative initial population.Table 3: TPR and FPR Assessment across Subgroups on Income and Gender Perturbations
Different perturbations on income Different perturbations on gender
rz= 1, ry= 0.5(DataF) rz= 0, ry= 0.5(DataM) rz= 1, ry= 0.5(DataF) rz= 0, ry= 0.5(DataM)
Raw r′
y= 1 r′
y= 0 Raw r′
y= 1 r′
y= 0 Raw r′
z= 0.5r′
z= 0 Raw r′
z= 0.5r′
z= 1
TPR M 0.7422 0.7344 0.5859 0.6563 0.6094 0.4688 0.7422 0.7422 0.7969 0.6563 0.6641 0.6406
TPR F 0.617 2 0.5547 0.3594 0.3984 0.3281 0.2422 0.6172 0.5703 0.6094 0.3984 0.4141 0.4297
∆TPR ↓0.1250 0.1797 0.2266 0.2578 0.2813 0.2266 0.1250 0.1719 0.1875 0.2578 0.2500 0.2109
FPR M 0.2656 0.2500 0.1406 0.1719 0.1484 0.0938 0.2656 0.2734 0.3125 0.1719 0.1719 0.1797
FPR F 0.1016 0.0859 0.0078 0.0234 0.0234 0.0000 0.1016 0.0703 0.0469 0.0234 0.0391 0.0547
∆FPR ↓0.1641 0.1641 0.1328 0.1484 0.1250 0.0938 0.1641 0.2031 0.2656 0.1484 0.1328 0.1250
Algorithm 1 FCG Algorithm
1:procedure STEP1: D IVERSE CLUSTERING
2: all_idx_set =[]
3: foreachgiinSGdo
4: selected_idx_set = []
5: X = gi.get_all_data()
6: centroids = Kmeans(clusters = n).fit( X)
7: foreach center in centroids do
8: dis_set = distance(X,center)
9: closest = argsort(dis_set)[:m]
10: selected_idx_set.extend(closest)
11: end for
12: all_idx_set.append(selected_idx_set)
13: end for
14: return SG’.set(all_idx_set)
15:end procedure
16:procedure STEP2:U PDATE EVOL SCORE
17: ch = [Mpred,Mfair]
18: foreachg′
iin SG’ do
19: shots = g′
i.genetic_algorithm(k)
20: evol_score = CAL_SCORE(shots,ch)
21: shots.update(evol_score)
22: repeat iters times
23: end for
24:end procedure
25:function CAL_SCORE (shots,ch)
26: Ybase= LLM.predict( Xdev)
27: Base pred, Base fair = eval( Ybase,Ydev, ch)
28: YICL = LLM.predict(shots, Xdev)
29: ICL pred, ICL fair = eval( YICL, Ydev,ch)
30: ∆pred = max(( ICL pred−Base pred), p)
31: ∆fair = max(( ICL fair−Base fair), p)
32: return α·∆pred + (1−α)·∆fair
33:end function
Each subgroup in SGis clustered into nclusters,
withmneighbors selected around each centre of
the cluster. The filtered new subgroups are denoted
as SG’= {g′
1, g′
2, g′
3, g′
4}.
Genetic Evolution for Score Updates. Next,
for each subgroup within SG’, we select K-
demonstrations for iters times using the roulette
wheel genetic evolutionary approach and validate
their ICL performance on Ddev. The evolution-
ary method means that data with a higher score is
more likely to be chosen in each round. The score
is first set as the default initialisation score pand
then updated as the average of EvolScore computedduring the iterations when the sample is selected.
EvolScores integrates the performance of both pre-
diction Mpredand fairness Mfairmetrics, with α
serving as the trade-off coefficient. The metrics
provide options for selecting either Accuracy or
F-score asMpred, and either RdporReoasMfair.
EvolScores in SG′will be updated and then used
for subsequent selecting iterations.
In the testing stage on Dtest, demonstrations in
SG′are ranked by their average EvolScores, en-
abling different ICL strategies to select the top-
performing demonstrations from their correspond-
ing subgroups. The detailed steps of FCG pseu-
docode is given in Algorithm 1. Figure 4 gives
an example of the whole process of representative
sample selections with FCG on Adult dataset.
5.2 Experimental Results
Table 4 presents the experimental results evaluating
the debiasing performance of the proposed FCG
algorithm. The experiments are performed on the
Adult dataset, setting the number of clusters to
n= 8and the number of neighbors to m= 5. We
start with an initial score of p= 0.05and perform
10 iterations to update EvolScore. F-score and
Reoare selected for calculating Evolscore and the
balancing parameter αis set to 0.5.
Results show that demonstrations selected by
FCG perform well, and greatly outperforming ran-
dom sampling. It is worth noting that using a bal-
anced set from minority samples continues to yield
the best performance, proving our finding that pri-
oritizing minority samples ( rz= 1) remains an ef-
fective strategy in ICL. Besides the minority group,
the improvements in accuracy and fairness also hap-
pen in the majority group, which affirms the value
of considering both factors in FCG selections.
Ablation Study. We implement ablation experi-
ments to verify the utility of the two-step extracting
process in FCG mitigation. In the ablation study,Step 1Clustering·Balanced Gender (𝑟!=0.5)if 𝑟"=0:∑Top K/2 (            		)if𝑟"=1:∑Top K/2 (            		)·Unbalanced Gender (𝑟!≠0.5)Top K of (																																					)	Basedon various combinations of 𝑟",𝑟!·Balanced Gender (𝑟!=0.5)			∑Top K/4 (																																					)							
,,,·Unbalanced Gender (𝑟!≠0.5)if 𝑟!=1:	∑Top K/2 (              )if 𝑟!=0:	∑Top K/2 (             	)
,
,
///
,
,
sizen*mGenetic Evolution
Step 3 SelectDemonstrationswithDifferent StrategyBalanced Label (𝑟"=0.5)Unbalanced Label (𝑟"≠0.5)
Step 2Figure 4: The Workflow of Fairness via Clustering-Genetic (FCG) on the Adult Dataset ( ry= 1, all high-income;
ry= 0, all low-income; ry= 0.5, balanced samples of high-income and low-income; rz= 1, all females; rz= 0,
all males; rz= 0.5, balanced samples of females and males.)
Table 4: The comparative analysis of the predictive and fairness performance achieved by various LLMs with
demonstrations selected using the proposed FCG algorithm. The experiments are conducted on the Adult dataset.
The table highlights that the FCG algorithm enhances fairness across almost all strategies.
Zero-shot K-Shot (K=8) K-Shot (K=4)
Zero-shot ry= 0.5(Balanced Labels) ry= 1 ry= 0 ry= 0.5(Balanced Labels) ry= 1 ry= 0
Prediction Baseline rz= 0.5rz= 0 rz= 1 rz= 1 rz= 1 rz= 0.5rz= 0 rz= 1 rz= 1 rz= 1
Accuracy ↑ 0.6855 0.7344 0.7363 0.7793 0.7500 0.7754 0.7656 0.7500 0.7656 0.7539 0.7578
Precision ↑ 0.8519 0.7174 0.6997 0.7360 0.7207 0.7640 0.7297 0.7222 0.7194 0.7338 0.7200
Recall ↑ 0.4492 0.7734 0.8281 0.8711 0.8164 0.7969 0.8438 0.8125 0.8711 0.7969 0.8438
F-score ↑ 0.5882 0.7444 0.7585 0.7979 0.7656 0.7801 0.7826 0.7647 0.7880 0.7640 0.7770
Fairness Baseline rz= 0.5rz= 0 rz= 1 rz= 1 rz= 1 rz= 0.5rz= 0 rz= 1 rz= 1 rz= 1
Rdp↑ 0.4063 0.7692 0.7719 0.8938 0.7576 0.7919 0.7515 0.8000 0.8675 0.7707 0.8072
Reo↑ 0.1111 0.6250 0.5690 0.7021 0.5577 0.5750 0.5094 0.6000 0.7059 0.5417 0.6800
∆dp↓ 0.2227 0.1406 0.1523 0.0664 0.1563 0.1211 0.1641 0.1250 0.0859 0.1406 0.1250
∆eo↓ 0.3203 0.1406 0.1953 0.1094 0.1797 0.1328 0.2031 0.1563 0.1172 0.1719 0.1250
part of the samples are selected using the same
flow of choosing the top K samples based on their
EvolScores, while the other part is selected ran-
domly. The results in Table 5 suggest: (1) Even
when EvolScores are ignored when selecting par-
tial samples, the results still outperform the raw
random selection method (Random ( g1+g2)), thus
proving the effectiveness of the clustering selec-
tion in the first stage. (2) Moreover, both ablation
test FCG ( g1) and FCG ( g2) performed worse com-
pared to the results using complete FCG ( g1+g2),
further confirming the need for the second stage of
genetic selection based on EvolScore scoring.
6 Conclusions
In this paper, we investigate how the choice of
demonstrations impacts the fairness of LLMs on
tabular data classification tasks when using in-
context learning. Through experiments, we found
that prioritizing underrepresented groups and in-
cluding minority examples in the few-shot demon-
strations can significantly enhance fairness perfor-
mance, without sacrificing predictive accuracy. Fur-
ther analysis revealed that increasing the propor-Table 5: The Ablation Study of FCG under Balanced
Labels in Minority Group Strategy (S2) for Selecting K
Demonstrations (K=8). S2 strategy is based on minority
group ( z= 1) with two possible labels y={0,1}, The
corresponding subgroups are denoted as g1(z= 1, y=
0)andg2(z= 1, y= 1) .
Shots from g1 Random Top K/2 Top K/2 Random K/2
Shots from g2 Random Top K/2 Random K/2 Top K/2
Prediction Random ( g1+g2)FCG ( g1+g2) FCG ( g1) FCG ( g2)
Accuracy ↑ 0.7480 0.7793 0.7500 0.7559
Precision ↑ 0.7592 0.7360 0.7013 0.7079
Recall ↑ 0.7266 0.8711 0.8711 0.8711
Fscore ↑ 0.7425 0.7979 0.7770 0.7811
Fairness Random ( g1+g2)FCG ( g1+g2) FCG ( g1) FCG ( g2)
Rdp↑ 0.7254 0.8938 0.8276 0.8208
Reo↑ 0.4390 0.7021 0.6964 0.6140
∆dp↓ 0.1523 0.0664 0.1172 0.1211
∆eo↓ 0.1797 0.1094 0.1328 0.1719
tion of underrepresented labels improves fairness
metrics like demographic parity and equal odds.
To efficiently retrieve effective demonstrations, we
proposed the FCG algorithm that uses clustering
and genetic evolution to select a diverse and rep-
resentative set of examples from the training data.
Across multiple strategies and datasets, experimen-
tal results indicate that FCG was able to improve
fairness compared to random sampling.7 Limitations
While our study presents significant advancements
in understanding and improving fairness in LLMs
using in-context learning (ICL), several limitations
should be noted. Firstly, we equally weigh fair-
ness and prediction performance in evaluating rep-
resentative demonstrations using our Fairness via
Clustering-Genetic (FCG) algorithm, which might
not align with real-world applications that require
a dynamic balance between these metrics. Addi-
tionally, our focus on binary classification with a
single sensitive feature limits the broader applica-
bility of our findings. In future, we plan to explore
LLM’s intersectional fairness and its performance
in multi-classification tasks. Lastly, while we used
pre-trained models without fine-tuning, investigat-
ing how fine-tuning on curated samples impacts
fairness could provide deeper insights.
References
Abubakar Abid, Maheen Farooqi, and James Zou.
2021a. Large language models associate mus-
lims with violence. Nature Machine Intelligence ,
3(6):461–463.
Abubakar Abid, Maheen Farooqi, and James Zou.
2021b. Persistent anti-muslim bias in large language
models.
AnthropicAI. 2023. Introducing claude.
Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir
Globerson, and Alexei A. Efros. 2022. Visual
prompting via image inpainting.
Barry Becker and Ronny Kohavi. 1996. Adult.
UCI Machine Learning Repository. DOI:
https://doi.org/10.24432/C5XW20.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Ting-Yun Chang and Robin Jia. 2023. Data curation
alone can stabilize in-context learning. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 8123–8144.
Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown,
and He He. 2023. On the relation between sensitivity
and accuracy in in-context learning.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-
ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui. 2022. A survey for in-context learning.
arXiv preprint arXiv:2301.00234 .Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer
Reingold, and Richard Zemel. 2012. Fairness
through awareness. In Proceedings of the 3rd inno-
vations in theoretical computer science conference ,
pages 214–226.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners.
Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux.
2022. Why do tree-based models still outperform
deep learning on typical tabular data? Advances in
Neural Information Processing Systems , 35:507–520.
Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equal-
ity of opportunity in supervised learning. Advances
in neural information processing systems , 29.
Stefan Hegselmann, Alejandro Buendia, Hunter Lang,
Monica Agrawal, Xiaoyi Jiang, and David Sontag.
2023. Tabllm: Few-shot classification of tabular
data with large language models. In International
Conference on Artificial Intelligence and Statistics ,
pages 5549–5581. PMLR.
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei
Cui, Owais Khan Mohammed, Barun Patra, Qiang
Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,
Vishrav Chaudhary, Subhojit Som, Xia Song, and
Furu Wei. 2023. Language is not all you need: Align-
ing perception with language models.
Tenghao Huang, Faeze Brahman, Vered Shwartz, and
Snigdha Chaturvedi. 2021. Uncovering implicit gen-
der bias in narratives through commonsense infer-
ence. arXiv preprint arXiv:2109.06437 .
Michael P Kim, Amirata Ghorbani, and James Zou.
2019. Multiaccuracy: Black-box post-processing for
fairness in classification. In Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society ,
pages 247–254.
Xiaonan Li and Xipeng Qiu. 2023. Finding support
examples for in-context learning. In Findings of the
Association for Computational Linguistics: EMNLP
2023 , pages 6219–6235.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-
mar, Benjamin Newman, Binhang Yuan, Bobby Yan,
Ce Zhang, Christian Cosgrove, Christopher D. Man-
ning, Christopher Ré, Diana Acosta-Navas, Drew A.
Hudson, Eric Zelikman, Esin Durmus, Faisal Lad-
hak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue
Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng,
Mert Yuksekgonul, Mirac Suzgun, Nathan Kim,
Neel Guha, Niladri Chatterji, Omar Khattab, Peter
Henderson, Qian Huang, Ryan Chi, Sang Michael
Xie, Shibani Santurkar, Surya Ganguli, Tatsunori
Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav
Chaudhary, William Wang, Xuechen Li, Yifan Mai,Yuhui Zhang, and Yuta Koreeda. 2023. Holistic eval-
uation of language models.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2021. What
makes good in-context examples for gpt- 3?arXiv
preprint arXiv:2101.06804 .
Yanchen Liu, Srishti Gautam, Jiaqi Ma, and Himabindu
Lakkaraju. 2023. Investigating the fairness of large
language models for predictions on tabular data.
arXiv preprint arXiv:2310.14607 .
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2021. Fantastically ordered
prompts and where to find them: Overcoming
few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786 .
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2022. Fantastically ordered
prompts and where to find them: Overcoming few-
shot prompt order sensitivity. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
8086–8098, Dublin, Ireland. Association for Compu-
tational Linguistics.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstrations:
What makes in-context learning work?
Moin Nadeem, Anna Bethke, and Siva Reddy. 2020.
Stereoset: Measuring stereotypical bias in pretrained
language models. arXiv preprint arXiv:2004.09456 .
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. Crows-pairs: A chal-
lenge dataset for measuring social biases in masked
language models.
OpenAI. 2023. gpt-4-turbo, gpt-3.5-turbo, text-davinci-
003.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2021. Learning to retrieve prompts for in-context
learning. arXiv preprint arXiv:2112.08633 .
Timo Schick and Hinrich Schütze. 2020. Exploit-
ing cloze questions for few shot text classification
and natural language inference. arXiv preprint
arXiv:2001.07676 .
Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022.
Xricl: Cross-lingual retrieval-augmented in-context
learning for cross-lingual text-to-sql semantic pars-
ing. arXiv preprint arXiv:2210.13693 .
Taylor Sorensen, Joshua Robinson, Christopher Ryt-
ting, Alexander Shaw, Kyle Rogers, Alexia Delorey,
Mahmoud Khalil, Nancy Fulda, and David Wingate.
2022. An information-theoretic approach to prompt
engineering without ground truth labels. In Proceed-
ings of the 60th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-
pers) , pages 819–862, Dublin, Ireland. Association
for Computational Linguistics.
Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur,
and Tanmoy Chakraborty. 2023. Multilingual llms
are better cross-lingual in-context learners with align-
ment.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin
Xie, Mintong Kang, Chenhui Zhang, Chejian Xu,
Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al.
2023. Decodingtrust: A comprehensive assessment
of trustworthiness in gpt models. arXiv preprint
arXiv:2306.11698 .
Zhibo Wang, Xiaowei Dong, Henry Xue, Zhifei Zhang,
Weifeng Chiu, Tao Wei, and Kui Ren. 2022. Fairness-
aware adversarial perturbation towards bias mitiga-
tion for deployed deep models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 10379–10388.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, and William Fedus. 2022. Emer-
gent abilities of large language models.
What Makes In-Context Learning Work. Rethinking
the role of demonstrations: What makes in-context
learning work?
I-Cheng Yeh. 2016. default of credit card
clients. UCI Machine Learning Repository. DOI:
https://doi.org/10.24432/C55S3H.
Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyun-
soo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang goo Lee,
and Taeuk Kim. 2022. Ground-truth labels matter: A
deeper look into input-label demonstrations.
Guanhua Zhang, Yihua Zhang, Yang Zhang, Wenqi Fan,
Qing Li, Sijia Liu, and Shiyu Chang. 2022. Fairness
reprogramming. Advances in Neural Information
Processing Systems , 35:34347–34362.
Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan
Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu,
Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and
Furu Wei. 2023. Speak foreign languages with your
own voice: Cross-lingual neural codec language mod-
eling.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In In-
ternational Conference on Machine Learning , pages
12697–12706. PMLR.A Related Work
A.1 Fairness in Large Language Model
Addressing social biases is crucial for ensuring the
trustworthiness of language models (Nangia et al.,
2020; Nadeem et al., 2020). LLMs face similar
fairness issues: many studies have confirmed that
LLMs can capture social biases from unprocessed
training data and transmit these biases to down-
stream tasks (Abid et al., 2021b; Brown et al., 2020;
Wang et al., 2023). Abid et al. (2021a) addressed
the issue of GPT-3’s output displaying bias to Mus-
lims. Huang et al. (2021) found that bias in LLMs’
responses exists even without prompts explicitly
asking about it. Liang et al. (2023) tested bias and
stereotypes on LLMs using the BBQ dataset for
question answering, finding that most models ex-
hibit biases distinct from broader societal trends.
Wang et al. (2023) assesses bias by prompting
GPT models to express their views on stereotypes.
Most bias studies have focused on representational
harms caused by LLM generations, with only lim-
ited studies (Hegselmann et al., 2023) addressing
fairness concerns classification problems with tab-
ular datasets. Besides investigation on pre-trained
LLMs, recent research also focuses on ensuring
fairness in other trained machine learning mod-
els, such as perturbation-based (Zhang et al., 2022;
Wang et al., 2022) and boosting-based (Kim et al.,
2019) approaches.
A.2 In Context Learning
In-context learning (ICL), known as few-shot learn-
ing, allows LLMs to learn tasks with minimal ex-
amples as demonstrations (Dong et al., 2022; Zhao
et al., 2021). Positive impacts of ICL on LLMs
have been observed in different tasks such as text
classification and answering (Gao et al., 2021; Liu
et al., 2021), images generations (Bar et al., 2022),
speech tasks (Zhang et al., 2023), and multi-modal
scenarios (Huang et al., 2023; Wei et al., 2022).
Meanwhile, researchers have found that the per-
formance of ICL is highly sensitive to the demon-
stration prompt (Chen et al., 2023; Lu et al., 2021;
Zhao et al., 2021; Shi et al., 2022). Investigations
have explored factors that can influence ICL pre-
diction performance, including demonstration re-
trievals (Tanwar et al., 2023; Sorensen et al., 2022),
orderings (Lu et al., 2022), and input-label map-
ping (Yoo et al., 2022; Work).
Demonstration Retrievals. A common demon-
stration retrievals approach in ICL involves ran-domly selecting a subset of examples from the train-
ing set (Brown et al., 2020). Given the sensitivity of
LLMs to the prompts, there has been investigation
into selecting representative samples to enhance
outcomes. Selecting the top-K training examples is
one mitigation method and has been demonstrated
in semantic parsing (Rubin et al., 2021) and se-
mantic classification (Chang and Jia, 2023). LENS
(Li and Qiu, 2023) proposed a two-step filter and
search algorithm to identify informative support
examples. Despite these advances, these retrieval
techniques often focus solely on prediction perfor-
mance and overlook the aspect of fairness. Addi-
tionally, most retrieval methods often require exten-
sive experimental iterations, with significant time
and resource investment.
B Dataset
Table 6 describes the data structure in the Default
Credit dataset. We calculate the mean values of
PAY_AMT_i and BILL_AMT_i, and merge them
into Avg_PAY_AMT and Avg_BILL_AMT sep-
arately. The raw Adult dataset shown in Table 7
contains 14 features, excluding education-num, fnl-
wgt, race, and native-country for this experiment.
‘>50K’ and ‘≤50K’ is mapped to ‘greater than
50K’ and ‘less than or equal to 50K’ respectively,
for better alignment with the language model. In
analysis, we call high income if the person’s an-
nual income is over 50K and low income if it is less
than 50K. The size ratio of Dtrain:Ddev:Dtestis
9:1:10 in both two datasets. Kdemonstrations are
extracted from Dtrain, 60 samples are extracted
fromDdev, 512 samples for Dtest. We consider the
balanced group and balanced labels scenario and
extract samples with parameter random_seed=42.
C Prompt Architecture
We consider both zero-shot learning and few-shot
learning (in-context learning). Zero-shot strat-
egy combines task description and question as
its prompt content without providing examples.
Few-shot strategy includes three roles, and the in-
context content is generated based on selected K-
demonstrations using different strategies (Table 10).
The default value of K is set to 8, the case of K=4
is disscussed in Section 5. Table 8 and 9 provide
templates for few-shot learning in the Adult and
Credit datasets respectively.Table 6: Default Credit Dataset Description
FeatureLIMIT_BALAmount of given credit Continuous; NT dollarsSEXGender 2 categories, male / femaleEDUCATIONHighest education 6 categories; graduate / high school /university / etcMARRIAGEMarital status  6 categories of ; married / single / othersAGE Age in years ContinuousPAY_i10	categories	of	repayment	status	foreach	month;		;	pay	duly	/	delay	for	onemonth	/	delay	for	two	months	/	etcBILL_AMT_i Amount of bill statement for each month;Continuous; NT dollarPAY_AMT_i Amount of previous payment  for eachmonth; Continuous; NT dollardefault_payment_next_monthIf default paymentnext monthYes, overdue / no, on-timei∈{1,2,3,4,5,6},represents themonth from April(6) to September(1) in 2005.Default Credit Dataset - Description
D More Experimental Results
Tables below present additional detailed results not
listed in the main text.
E Discussion between Our FCG and
LENs Algorithm
Our proposed FCG shares a similar architec-
ture with LENs, another demonstration selection
method. Here, we discuss these two methods and
further explore the possibility of combining them.
Given the time consumption of LENs, we simpli-
fied it by setting batch size to 8, and template index
to {0,1}. The training dataset is randomly split
(seed = 42 ) into groups of 500 samples for par-
allel computation, with other settings kept at their
defaults. For LENs with FCG, we follow our FCG
parameters setting: first extract 160 representative
samples by clustering, then perform LENs to find
the final candidates. Figure 5 presents the overall
workflow of FCG, LENs, and their possible com-
bination. Both FCG and LENs involve two steps:
(1) selecting partial data and (2) searching for the
optimal based on the filtered data. There are two
key differences in implementations.
Supervised & Unsupervised LENs algorithm
uses LLMs as the classification assistance in both
stages. This is a straightforward and effective way.
However, since the processing time of language
models is related to the amount of information in
the input text, the selection time can become verylong when the input data is lengthy. This study
focuses on tabular datasets, which have longer text
when converted into prompts compared to com-
monly used NLP datasets. Therefore, we con-
sider to optimize the method to reduce processing
time and improve efficiency. Our FCG replaces
LLMs with simpler unsupervised algorithms in
the first stage. On the adult dataset, LENs can
take over 50 hours to find supportive demonstra-
tions ( batchsize = 8), while FCG takes less than
3 hours ( K= 4). The result in Table 17 validates
the effectiveness: even if LLMs are not used ini-
tially, using LLMs to search on the validation set in
the second stage can still find demonstrations that
improve the model’s prediction.
Fairness Awareness Another difference is that
LENs use accuracy as the sole evaluation metric
when selecting demonstrations. Our FCG takes
sensitive features into account and selects demon-
strations at the subgroup level. Additionally, FCG
considers both accuracy and fairness metrics as
constraints when calculating performance scores.
Table 17 confirms FCG with minority demonstra-
tions prioritised strategy ( rz= 1) shows fairer
performance than baseline.
Furthermore, we extend LENs with FCG (as
shown on the right side of Figure 5) to make it
fairness-aware. Table 17 proves the effectiveness
of this combination and also shows the best perfor-
mance achieved when using more minority demon-
strations ( rz= 1).Table 7: Adult Income Dataset Description
FeatureAdult Income Dataset - DescriptionAge Age in years; ContinuousWorkclass8 general types of employment; private / self-employed / government / etcEducation16 categories of highest level of education; college / bachelors / masters / etc.Marital-Status7 categories; married / divorced / separated / single / etc.Occupation15 categories; prof-specialty / craft-repair / Sales  / etcRelationship6 categories; not-in-family / husband / wife / etcSex2 categories; the biological sex; male / femaleCapital-GainPerson's capital gains; ContinuousCapital-LossPerson's capital losses; ContinuousHours-Per-WeekHours worked per week; ContinuousSalary2 categories of whether annual income exceeds $50K;  >50K / <=50K
Table 8: Few-shot Learning Templates for Adult Dataset
RolesPrompting Templates for Adult Income DatasetDescription<Task> Predict if income exceeds $50K per year. Answer with one of the following:                                   [greater than 50K] | [less than or equal to 50K] </Task> In-context </Example>Example 1:age is 40, and workclass is Private, and education is HS-grad, and marital-status is Married-civ-spouse, and occupation is Sales, and relationship is Husband, and sex is Male, and capital-gain is 0, and capital-loss is 0, and hours-per-week is 60, and income is <=50K; </Example> <Example>Example 2 ......</Example> </Example>Question<Input> Age is 19, and workclassis Private, and education is Some-college, and marital-status is Never-married, and occupation is Other-service, and relationship is Own-child, and sex is Female, and capital-gain is 0, and capital-loss is 0, and hours-per-week is 15, please answer the income: </Input>
Table 9: Few-shot Learning Templates for Credit Dataset
RolesPrompting Templates for Default Credit DatasetDescription<Task> Predict if the following data will default payment next month. Answer with one of the following:    [No] | [Yes] </Task>In-context <Example>	Example	1:	Amount	of	given	credit	is	490000,	and	SEX	is	male,	and	EDUCATION	is	graduate	school,	and	MARRIAGE	is	married,	and	AGE	is	45,	and	PAY_0	is	pay	duly,…...,	and	default	payment	next	month	is	No,	on-time;</Example>	<Example>	Example	2:	…...</Example>	Question<Input>	Amount	of	given	credit	is	90000,	and	SEX	is	female,	and	EDUCATION	is	university,	and	MARRIAGE	is	married,	and	AGE	is	49,	and	PAY_0	is	delay	for	one	month,…...,	and	predict	whether	default	payment:	</Input>	
Table 10: Demonstrations Selection Strategies
Default Credit DatasetAdult Income DatasetStrategyAnnotationA balanced 1/4 ratio of female-overdue : female-on-time : male-overdue : male-on-timeA balanced 1/4 ratio of female-low-income : female-high-income : male-low-income : male-high-income.S1𝑟!=0.5,𝑟"=0.5Prioritize minority with balanced 1/2 ratio of female-overdue: female-on-timePrioritize minority with balanced 1/2 ratio of female-low-income : female-high-incomeS2𝑟!=1,𝑟"=0.5Prioritize minority with imbalanced targets. All female-on-time samplesPrioritize minority with imbalanced targets. All female-low-income samplesS3𝑟!=1,𝑟"=0Prioritize minority with imbalanced targets. All female-overdue samplesPrioritize minority with imbalanced targets. All female-high-income samplesS3𝑟!=1,𝑟"=1Table 11: Different LLMs performance on Default Credit Dataset
Text -Davinci -3GPT 3.5 -turboGPT 4 -turboZero-shotS1S2S3Zero-shotS1S2S3Zero-shotS1S2S3Accuracy0.5449 0.6230 0.5996 0.6641 0.6250 0.6562 0.6484 0.6543 0.6602 0.6758 0.6719 0.6582 F-score0.4453 0.6030 0.5545 0.6641 0.5947 0.6453 0.6413 0.6543 0.6579 0.6758 0.6716 0.6578 Δdp0.0117 0.0586 0.0039 0.0391 0.0391 0.0313 0.0234 0.0430 0.2969 0.0547 0.0469 0.0195 𝑹𝒅𝒑0.8571 0.8077 0.9787 0.9254 0.8413 0.9080 0.9368 0.9160 0.4759 0.8955 0.9155 0.9590 Δeo0.0234 0.1016 0.0547 0.1016 0.0938 0.1016 0.1094 0.1328 0.3125 0.1250 0.0938 0.1094 𝑹𝒆𝒐0.8235 0.5000 0.5000 0.7400 0.7647 0.7917 0.7419 0.8132 0.2941 0.8298 0.8750 0.7955 
Table 12: Performance of Claude-3-haiku and Claude-3-sonnet with Zero-shot and Different Few-shot Strategies on
Adult Dataset ( ry= 0.5, K=4)
Claude-3-haiku Claude-3-sonnet
Zero-shot rz= 0 rz= 0.5rz= 1 Zero-shot rz= 0 rz= 0.5rz= 1
Accuracy 0.7285 0.7070 0.7012 0.7031 0.6641 0.7266 0.7383 0.7520
Precision 0.7489 0.8118 0.7210 0.7047 0.6556 0.7302 0.7364 0.7699
Recall 0.6875 0.5391 0.6563 0.6992 0.6914 0.7188 0.7422 0.7188
F-score 0.7169 0.6479 0.6871 0.7020 0.6730 0.7244 0.7393 0.7434
Zero-shot rz= 0 rz= 0.5rz= 1 Zero-shot rz= 0 rz= 0.5rz= 1
∆dp 0.2305 0.1797 0.1836 0.1563 0.0625 0.1484 0.1094 0.1445
Rdp 0.5986 0.5741 0.6643 0.7279 0.8881 0.7379 0.8042 0.7319
∆eo 0.2344 0.2188 0.2031 0.1641 0.0703 0.1563 0.1094 0.1719
Reo 0.3409 0.2800 0.5116 0.5625 0.8235 0.5455 0.6585 0.5714
Table 13: Performance of GPT3.5-turbo on the Adult Dataset through Few-shot Strategies (S1) with 5 random seeds
Zero-shotS1 (𝒓𝒛=𝟎.𝟓,𝒓𝒚=𝟎.𝟓)Few-shot with Different Random SeedFew-shot SummaryBaselineSeed 25Seed 35Seed 42Seed 45Seed 55AV G .SEHighestLowestAccuracy0.6855 0.7285 0.7363 0.7363 0.7266 0.7285 0.7313 0.0009 0.7363 0.7266 Precision0.8519 0.7940 0.7980 0.8010 0.7871 0.7882 0.7937 0.0012 0.8010 0.7871 Recall0.4492 0.6172 0.6328 0.6289 0.6211 0.6250 0.6250 0.0012 0.6328 0.6172 F-score0.5882 0.6945 0.7059 0.7046 0.6943 0.6972 0.6993 0.0011 0.7059 0.6943 FairnessBaselineSeed 25Seed 35Seed 42Seed 45Seed 55AV G .SEHighestLowestΔdp0.2227 0.1758 0.1680 0.1680 0.1641 0.1680 0.1688 0.0009 0.1758 0.1641 𝑹𝒅𝒑0.4063 0.6311 0.6504 0.6475 0.6557 0.6504 0.6470 0.0019 0.6557 0.6311 Δeo0.3203 0.2031 0.1875 0.1797 0.1797 0.1875 0.1875 0.0019 0.2031 0.1797 𝑹𝒆𝒐0.1111 0.3667 0.3667 0.3333 0.3871 0.3871 0.3682 0.0044 0.3871 0.3333 
Table 14: Performance of GPT3.5-turbo on the Adult Dataset through Few-shot Strategies (S2) with 5 random seeds
Zero-shotS2 (𝒓𝒛=𝟏,𝒓𝒚=𝟎.𝟓) Few-shot with Different Random SeedFew-shot SummaryBaselineSeed 25Seed 35Seed 42Seed 45Seed 55AV G .SEHighestLowestAccuracy0.6855 0.7207 0.7207 0.7480 0.7266 0.7480 0.7328 0.0028 0.7480 0.7207 Precision0.8519 0.7958 0.8192 0.7592 0.7589 0.7873 0.7841 0.0051 0.8192 0.7589 Recall0.4492 0.5938 0.5664 0.7266 0.6641 0.6797 0.6461 0.0130 0.7266 0.5664 F-score0.5882 0.6801 0.6697 0.7425 0.7083 0.7296 0.7060 0.0062 0.7425 0.6697 FairnessBaselineSeed 25Seed 35Seed 42Seed 45Seed 55AV G .SEHighestLowestΔdp0.2227 0.1680 0.1445 0.1523 0.1797 0.1445 0.1578 0.0031 0.1797 0.1445 𝑹𝒅𝒑0.4063 0.6325 0.6542 0.7254 0.6593 0.7132 0.6769 0.0080 0.7254 0.6325 Δeo0.3203 0.2344 0.1641 0.1797 0.1875 0.1641 0.1859 0.0058 0.2344 0.1641 𝑹𝒆𝒐0.1111 0.5000 0.3333 0.4390 0.4211 0.3824 0.4152 0.0125 0.5000 0.3333 Table 15: Performance of GPT3.5-turbo on the Adult Dataset through Few-shot Strategies (S3) with 5 random seeds
Zero-shotS3 (𝒓𝒛=𝟏,𝒓𝒚=𝟏) Few-shot with Different Random SeedFew-shot SummaryBaselineSeed 25Seed 35Seed 42Seed 45Seed 55AV G .SEHighestLowestAccuracy0.6855 0.7168 0.7168 0.7324 0.7285 0.7207 0.7230 0.0014 0.7324 0.7168 Precision0.8519 0.7707 0.7681 0.7847 0.8128 0.7678 0.7808 0.0038 0.8128 0.7678 Recall0.4492 0.6172 0.6211 0.6406 0.5938 0.6328 0.6211 0.0036 0.6406 0.5938 F-score0.5882 0.6855 0.6868 0.7054 0.6862 0.6938 0.6915 0.0017 0.7054 0.6855 FairnessBaselineSeed 25Seed 35Seed 42Seed 45Seed 55AV G .SEHighestLowestΔdp0.2227 0.1445 0.1445 0.1289 0.1758 0.1836 0.1555 0.0046 0.1836 0.1289 𝑹𝒅𝒑0.4063 0.6942 0.6967 0.7273 0.6121 0.6357 0.6732 0.0095 0.7273 0.6121 Δeo0.3203 0.1875 0.1563 0.1563 0.2188 0.2344 0.1906 0.0071 0.2344 0.1563 𝑹𝒆𝒐0.1111 0.5667 0.4118 0.5517 0.3462 0.4848 0.4722 0.0187 0.5667 0.3462 
Table 16: The Ablation Study with Balanced Labels in Minority Group (S2) under FCG Selections on GPT-3.5-turbo.
The corresponding subgroups are denoted as g1(z= 1, y= 0) andg2(z= 1, y= 1) .
K-shots K=8 K=4
g2 Top K/2 Top K/2 Random K/2 Top K/2 Top K/2 Random K/2
g1 Top K/2 Random K/2 Top K/2 Top K/2 Random K/2 Top K/2
FCG ( g1 +g2)FCG (g2) FCG ( g1) FCG ( g1 +g2) FCG ( g2) FCG ( g1)
Accuracy 0.7793 0.7500 0.7559 0.7656 0.7441 0.7539
Precision 0.7360 0.7013 0.7079 0.7194 0.7036 0.7031
Recall 0.8711 0.8711 0.8711 0.8711 0.8438 0.8789
Fscore 0.7979 0.7770 0.7811 0.7880 0.7673 0.7813
FCG (g1+g2)FCG (g2) FCG (g1)FCG (g1+g2)FCG (g2) FCG (g1)
∆dp 0.0664 0.1172 0.1211 0.0859 0.1133 0.1016
Rdp 0.8938 0.8276 0.8208 0.8675 0.8274 0.8497
∆eo 0.1094 0.1328 0.1719 0.1172 0.1172 0.1328
Reo 0.7021 0.6964 0.6140 0.7059 0.7170 0.6964
Table 17: ICL Performance of LLaMa-3-8b on the Adult Dataset ( ry= 0.5) using Different Demonstration Retrieval
Methods (LENs, FCG, and Combined).
LLaMa-3 LENs (K = 4) FCG (Ours, K = 4) LENs with FCG (K = 4)
Baseline rz= 0 rz= 0.5rz= 1 rz= 0 rz= 0.5rz= 1
Accuracy 0.6270 0.6406 0.6680 0.7148 0.5957 0.6543 0.6504
Precision 0.7211 0.8462 0.7389 0.7778 0.7426 0.7687 0.6103
Recall 0.4141 0.3438 0.5195 0.6016 0.2930 0.4414 0.8320
F−score 0.5261 0.4889 0.6101 0.6784 0.4202 0.5608 0.7041
Baseline rz= 0 rz= 0.5rz= 1 rz= 0 rz= 0.5rz= 1
∆dp 0.1523 0.1250 0.1094 0.0938 0.1602 0.1445 0.0039
Rdp 0.5806 0.5294 0.7308 0.7838 0.4225 0.5978 0.9943
∆eo 0.2344 0.1719 0.1172 0.0938 0.2109 0.1953 0.0469
Reo 0.5588 0.2308 0.5161 0.5714 0.3000 0.4783 0.9155Train Data
Global Top K2 SearchingLLMDiversity-GuidedTrain Data
Top K in each subgroup; Combine with strategies1 Unsupervised FilteringSubgroup-awareClusteringTrain Data1Combined Filtering
Top K in each subgroup; Combine with strategiesClustering Filtered Subgroup1Progressive FilteringK shotsLLMScoresK shotsLLMScores2 Genetic SelectionsK shotsLLMScores2 SearchingLLMDiversity-Guided(a) Left. FCG; (b) Middle. LENs; (c) Right. LENs with FCGFigure 5: The workflow comparison of demonstration selection algorithms: FCG (Ours, proposed in Section 5),
LENs, and LENs with FCG.