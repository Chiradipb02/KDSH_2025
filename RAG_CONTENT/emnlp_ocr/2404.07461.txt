An Audit on the Perspectives and Challenges of Hallucinations in NLP
Pranav Narayanan Venkit1Tatiana Chakravorti1Vipul Gupta4Heidi Biggs3
Mukund Srinath1Koustava Goswami2Sarah Rajtmajer1Shomir Wilson1
1College of Information Sciences and Technology, Pennsylvania State University
2Adobe Research3School of Interactive Computing, Georgia Institute of Technology
4Department of Computer Science & Engineering, Pennsylvania State University
{pranav.venkit, tfc5416, vkg5164, mukund, smr48, shomir}@psu.edu
hbiggs7@gatech.edu, koustavag@adobe.com
Abstract
We audit how hallucination in large lan-
guage models (LLMs) is characterized in peer-
reviewed literature, using a critical examina-
tion of 103 publications across NLP research.
Through the examination of the literature, we
identify a lack of agreement with the term ‘hal-
lucination’ in the field of NLP. Additionally,
to compliment our audit, we conduct a survey
with 171 practitioners from the field of NLP
and AI to capture varying perspectives on hallu-
cination. Our analysis calls for the necessity of
explicit definitions and frameworks outlining
hallucination within NLP, highlighting poten-
tial challenges, and our survey inputs provide
a thematic understanding of the influence and
ramifications of hallucination in society.
1 Introduction
Recent advancements in Natural Language Pro-
cessing (NLP) have expanded beyond traditional
Machine Learning tools, evolving into sociotech-
nical systems that combine social and technical
aspects to achieve specific goals (Gautam et al.,
2024; Narayanan Venkit, 2023). They have now
become integral in various domains such as health,
policy-making, and entertainment, (Jin and Mihal-
cea, 2022; Werning, 2024) showcasing their sig-
nificant impact on daily life. However, language
models (LM) exhibit negative behaviours such as
hallucination and biases (Bender et al., 2021; Gupta
et al., 2024). This has catalyzed a surge in research
investigating the phenomenon of hallucinations in
NLP (Ji et al., 2023a), reflected in the escalating
number of peer-reviewed publications on the topic,
as illustrated in Fig 1, sourced from SCOPUS.
Within the NLP domain, various frameworks
have emerged to define hallucination, primarily
emphasizing its negative aspect. Hallucination here
refers to the model’s production of references to
non-existent objects or statements, lacking support-
ing examples in the training data (Ji et al., 2023a).
Figure 1: Articles published each year (from 2013 to
2023) in SCOPUS that contain the term ‘hallucination’
AND (‘NLP’ OR ‘AI’) in the title, abstract, or keywords.
Despite the growing research on this topic, there is
still a notable divide in our understanding, a lack
of a unified framework, and a need for precise defi-
nitions (Filippova, 2020a).
The necessity to understand this gap is accentu-
ated by research demonstrating the societal impacts
of hallucinations (Dahl et al., 2024). Hence, there
is a growing need to explore how the field of NLP
conceptualizes hallucination. In line with this im-
perative, the following questions guide this study:
•RQ1 : What are the definitions and frame-
works used to explain hallucinations in NLP?
•RQ2 : What is the current understanding of
researchers about hallucinations, and how do
they encounter them in their work?
To answer the RQ1, we first conduct an audit
of the field of hallucinations in NLP by surveying
103 peer-reviewed articles1. Subsequently, we con-
duct a survey to 171 researchers and academics in
the field to gather their perspectives on this phe-
nomenon, providing a novel contribution to the
literature, addressing RQ2. By surveying NLP prac-
titioners, the paper incorporates real-world perspec-
tives, enriching the theoretical discussions with
1https://github.com/PranavNV/The-Thing-Called-
HallucinationarXiv:2404.07461v2  [cs.CL]  14 Sep 2024practical insights. This audit therefore aims to
broaden the communities perspective by presenting
practical insights from researchers employing these
methods in their work. We also propose an ethical
framework to guide future efforts in comprehend-
ing and mitigating hallucinations in LLMs.
2 Evolution of Hallucination in NLP
The term ‘hallucination’ has a long history in ma-
chine learning and has been used in various con-
texts prior to the LM era. Its earliest documented
usage can be traced to the 2000s when Baker and
Kanade (2000) applied it in the context of image
resolution enhancement, referring to the generation
of new pixel values. Subsequently, "hallucination"
has been frequently employed in computer vision
research, including notable works such as Hsu et al.
(2010) on face hallucination.
In the modern deep learning era, hallucination
was used first by Andrej Karpathy in his blog fo-
cusing on Recurrent Neural Networks (Karpathy,
2015). He used the term within the context of LM
by illustrating how an LSTM could generate non-
existent URLs, effectively ‘hallucinating’ data. The
term then gained major traction with the launch
of ChatGPT (Wu et al., 2023), where it referred
to inaccuracies and factual mistakes produced by
models (Ji et al., 2023a). However, the field lacked
a unified definition, leading to a spectrum of inter-
pretations (Filippova, 2020b). In one of the earlier
works, Maynez et al. (2020) divides term usage
into intrinsic and extrinsic hallucination. Intrinsic
hallucinations are consequences of synthesizing
content using the information present in the input.
Extrinsic hallucinations are model generations that
ignore the source material altogether.
However, there is a rise in discussion around
terminology that reflects a deeper inquiry into the
phenomena, with recent discourse advocating for
‘confabulation’ (Millidge, 2023) or ‘fabrications’
(McGowan et al., 2023) as a more precise descrip-
tor. This reflects the lack of consensus on the term
and highlights the importance of looking at the use
of hallucination with a more critical lens.
3 Related Surveys on Hallucination
We now provide an overview of several key sur-
veys in the realm of NLP focusing on the topic of
hallucination and why our work addresses a rele-
vant gap in the field. Starting with Ji et al. (2023a),
this survey extensively delves into the advance-ments and challenges concerning hallucination in
NLG, distinguishing between intrinsic and extrin-
sic frameworks of hallucination. Additionally, it
sheds light on fundamental terms such as hallu-
cination, faithfulness, and factuality, along with
prevalent metrics for quantifying these phenomena.
Rawte et al. (2023b) categorizes existing works
within the domain of LMs, covering various as-
pects including methods for detecting hallucination,
mitigation techniques, datasets used, and evalua-
tion metrics. Zhang et al. (2023c) addresses the
challenges of hallucination in LLMs by categoriz-
ing hallucinations into input-conflicting, context-
conflicting, and fact-conflicting types, diverging
from traditional viewpoints.
Furthermore, Huang et al. (2023) redefines the
taxonomy of hallucination into factuality and faith-
fulness, with additional subdivisions, and pro-
poses mitigation strategies aligned with underlying
causes. Tonmoy et al. (2024) offer a comprehen-
sive overview of over thirty-two techniques devel-
oped to mitigate hallucination in LLMs and finally,
Rawte et al. (2023a) present a nuanced categoriza-
tion of hallucination into six types, contributing to
the ongoing discourse within the field.
While these surveys offer insights into the cur-
rent state of hallucination research, they do not
pay attention to critical examinations of the field’s
weaknesses arising from a lack of discourse in
defining hallucination and challenges due to the
same. This deficiency in discussion reflects the
broader trend within the entire field. Therefore,
our audit answers this gap by critically examin-
ing how we conceptualize hallucination. We aim to
highlight the challenges stemming from these defi-
nitions and to further conduct a practitioner survey
within the community to understand researchers’
and developers’ perspectives on this issue.
4 Critical Analysis of Hallucination in
NLP Literature
This section is dedicated to conducting an audit
of hallucination research within NLP, aiming to
uncover its applications and subsequently identify
the strengths and weaknesses in current literature.
To accomplish this, we conducted an audit of
works from the ACL anthology using specific
keywords such as ‘hallucination’, ‘NLP (OR) AI’
AND ‘hallucinations’, ‘fabrication’, and‘confab-
ulations’ . We surveyed papers released on and be-
foreApril 19th, 2024 . From this search, a total ofNLP Tasks Frequency
Conversational AI 38
Abstractive Summarization 16
Data-to-Text Generation 14
Machine Translation 12
Image-Video Captioning 8
Data Augmentation 8
Miscellaneous 7
Table 1: Frequency of papers reviewed for each themati-
cally grouped NLP tasks.
164 papers were retrieved. After filtering out pa-
pers that were not directly related to hallucination
research or those that merely mentioned the term
without substantial focus on the topic, we arrived
at a corpus of 103 papers . This corpus forms the
basis for our audit and analysis of hallucination
research, specifically within the NLP domain.
4.1 Conceptualization of Hallucination
We performed an iterative thematic analysis (Vais-
moradi et al., 2013) to uncover the various applica-
tions of hallucination research in NLP. To ensure
accuracy and prevent misclassification, this recur-
sive process was employed. This resulted in the
identification of seven distinct fields that address
research on hallucination (as shown in Table. 1).
This taxonomy affords insights into the perva-
sive nature of hallucination in NLP. Notably, it
reveals that hallucination transcends beyond text
generation, extending its conceptualization to en-
compass broader domains such as Image-Video
Captioning, Data Augmentation, andData-to-Text
Generation tasks. This depicts the significance of
hallucination both within and beyond the realm of
NLP. Moreover, our classification framework pro-
vides us with a faceted analysis of how each of
these tasks defines the concept of hallucination.
Using thematic categorization, we come across
definite attributes across the definitions of hallu-
cination. One set of attributes elucidated how hal-
lucinations are associated with the style/language
generated by the model: Fluency, Plausibility , and
Confidence . The next set of attributes falls under
the effects of hallucinations: Intrinsic, Extrinsic,
Unfaithfulness andNonsensical . The definition of
each of these attributes is elaborated in Table 2.
In each paper analyzed within the survey scope,
hallucination is defined based on a combination of
the set of attributes identified. Our survey revealed
31 unique frameworks for conceptualizing hal-
lucination, illustrating the diverse approaches and
perspectives used. This diversity underscores the
ambiguity in the term’s usage.To illustrate this phenomenon, we present some
examples showcasing the diverse approaches com-
monly observed in the literature:
“Hallucination refers to the phenomenon where
the model generates false information not sup-
ported by the input.” - (Xiao and Wang, 2021a)
“LLMs often exhibit a tendency to produce ex-
ceedingly confident, yet erroneous, assertions com-
monly referred to as hallucinations.” - (Zhang
et al., 2023a)
“Models generate plausible-sounding but un-
faithful or nonsensical information called halluci-
nations” - (Ji et al., 2023c)
Hence, within NLP, a notable deficiency persists
in grasping coherent characteristics of hallucina-
tion. This shortfall underscores the risk of potential
misappropriation of the term when employed in
divergent contexts. An extensive analysis of hallu-
cination for each of the mentioned NLP tasks and
its definition is illustrated in the Appendix 10.2 .
4.2 Audit of Frameworks
We now scrutinize the dominant frameworks em-
ployed in defining hallucination while also assess-
ing the extent to which these models accurately cap-
ture the phenomenon. We start by looking at how
many of the selected works explicitly define hallu-
cination. Out of the 103papers, just 44(42.7%)
provide a definition of the term, leaving the ma-
jority— 59 papers or 57.3% —either altogether
omitting their understanding of hallucination in the
context of their research or providing no definition
or a framework. This lack of transparency is not
only concerning but also underscores the need for
clarity, especially given the varied interpretations
of hallucination across different research domains.
Taking our scrutiny a step further, we investi-
gate whether the works defining hallucination ref-
erence and acknowledge preexisting frameworks .
It emerges that only 29 papers or 27% of the se-
lected works explicitly acknowledge and adhere
to established frameworks of hallucination, while
the remainder 73% either loosely define the term
or devise new definitions tailored to their specific
research scope. This trend within the field shows a
lack of consensus on the conceptualization of hal-
lucination, leading to disparate interpretations and
a shortage of discourse on the subject.
We also audit the sociotechnical nature of the
definitions of hallucination in NLP. Hallucination
(elucidated in Appendix 10.1) inherently containsAttributes Definition
Fluency The syntactic incorrectness and semantic errors of the sentence generated.
PlausibilityThe degree to which the generated text
appears factually incorrect or unbelievable within the given context.
ConfidenceThe absence of modifiers or qualifiers that express
uncertainty in the generated text, presenting the output with a sense of assuredness.
Intrinsic The generated output that contradicts the source content or the input provided
Extrinsic The generated output that cannot be verified from the source content or the input provided.
Non-factualInconsistent with facts in the real world, leading to the generation of non-factual
content in accordance to the established real-world knowledge.
UnfaithfulnessInconsistent to the input prompt or context, creating deviations
or inconsistencies that would diverge from the intended meaning or message.
Nonsensical Lack of logical meaning or coherence within a given context as well as the readability of the text.
Table 2: The attributes that appear in the definitions of hallucination.
social dimensions, creating varied perspectives
across different social contexts. Moreover, given
the evolution of LMs into social spaces, adopting a
sociotechnical approach becomes necessary, given
that the term ‘hallucination’ is inherently a shared
vocabulary within these domains. Unfortunately,
out of the 103 works examined, only 3 acknowl-
edge the this nature of hallucination, with none
utilizing this framework to inform their approach.
This underscores a need for research to explore
the sociotechnical dimensions inherent in halluci-
nation, showcasing the limited depth of understand-
ing within the ML and NLP communities.
4.3 Audit of Metrics
In the analysis of the 103 papers, we observed that
87 of these works dedicate efforts to measuring
‘hallucination.’ This observation depicts the prevail-
ing trend within NLP, emphasizing the significance
of quantifying the concept of hallucination across
diverse research efforts. Building upon prior studies
such as Ji et al. (2023a), our analysis categorizes
the common approaches in NLP for quantifying
hallucination into four major themes: Statistical
Metrics, Data-driven Metrics, Human Evaluation,
andMixed Methodologies .
Figure 2: Hallucination evaluation metrics used in NLP.
Statistical metrics calculates a hallucination
score based on the degree of mismatch, with higher
discrepancies indicating lower accuracy, factual-ity or faithfulness and hence, higher hallucination
(Ji et al., 2023a). Statistical scores such as BLUE,
ROUGE, and Error Rate metrics are commonly
used in this approach. Our findings reveal that
35.2% of the works that quantify hallucination opt
for statistical metrics, employing 25 distinct met-
rics(e.g., BERTScore, F1, Perplexity, Cosine Sim-
ilarity) developed for this purpose. This variability
underscores the lack of a standardized approach.
Data-driven metrics utilizes curated datasets
or neural models to gauge hallucination in gen-
erated text. This methodology, accounting for cu-
rated knowledge/content mismatches, is adopted by
26.1% of the works, resulting in the development
of18 distinct datasets or models tailored for hal-
lucination measurement, such as CHAIR (Caption
Hallucination Assessment with Image Relevance)
and SelfCheckGPT (Manakul et al., 2023).
Human evaluation offers a complementary per-
spective by employing human annotators to assess
hallucination levels, compensating for apparent er-
rors in automated indicators (Ji et al., 2023a). This
approach, used by 10.2% of the works, encom-
passes scoring and comparison methods, where
annotators rate hallucination levels or compare out-
put texts with baselines or ground-truth references.
Notably, one outlier paper introduced an innovative
approach utilizing eye tracking for hallucination
detection in NLP tasks (Maharaj et al., 2023).
Mixed method approach is deployed by 28.4%
of the works, combining human evaluation with
statistical metrics to offer a holistic perspective on
hallucination quantification. This trend reflects a
concerted effort within the research community to
address the limitations of individual methodologies
and provide insights into the presence and nature
of hallucination in generated texts.
The metrics audit reveals significant knowledge
gaps and challenges across various approaches.Notably, established research highlights areas for
improvement in standard methods for measuring
hallucination. For instance, methodologies like
CHAIR and metrics such as ROUGE scores ex-
hibit instability in measuring hallucination due to
the need for complex human-crafted parsing rules
for exact matching, rendering them susceptible to
errors (Li et al., 2023). Criticisms also extend to
human evaluation methods, which are prone to in-
accuracies in gauging hallucination within these
models (Smith et al., 2022).
Beyond methodological criticisms, our audit un-
covers a trend of employing numerous distinct met-
rics and approaches within these frameworks to
categorize hallucinations. Over time, this has led to
a diverse set of parameters for measuring halluci-
nation, with a general lack of consensus on a stan-
dardized measurement approach. This issue further
highlights the absence of a unified method, espe-
cially as these models have now shifted to become
a sociotechnical solution (Bender et al., 2021).
5 Practitioner Survey of Hallucination
In this section, adopting a ‘community-centric ap-
proach’ (Narayanan Venkit, 2023), we conduct a
survey to gain insights into researchers’ percep-
tions of hallucinations in NLP to complement our
theoretical discussions with practical real-world
perspectives. The primary goal is to demonstrate
how researchers and practitioners within the field
perceive the concept of ‘hallucination’ and to ex-
pand our findings beyond the limitations of existing
literature where real-world perceptions from the re-
searchers are missing (Huang et al., 2023; Zhang
et al., 2023c; Ji et al., 2023a). This motivates us
to gather real-world perspectives from individuals
actively engaged in NLP and AI research.
5.1 Survey Recruitment and Data Collection
For our survey, we employed a multi-faceted ap-
proach to reach a diverse population of respon-
dents. We utilized direct emails, direct messages,
and social media platforms such as LinkedIn and
Twitter to distribute the survey. Our target audience
included graduate students and professors from aca-
demic backgrounds as well as individuals from the
industry who work in NLP, aiming to capture a
wide range of perspectives on hallucinations.
To ensure a comprehensive view, we specifically
targeted researchers familiar with AI and ML, pri-
marily from disciplines such as computer scienceand information science. However, we also wel-
comed participants from other domains to explore
their perceptions of whether they had the literature
understanding of the concept of hallucination as
they are also extensively using LLM models. The
survey was examined and approved by the Institu-
tional Review Board (IRB) for ethical practices.
We additionally employed a systematic approach
by randomly selecting 15 universities from the top
100 in the USA as per the 2023 US News and World
Report rankings (News, 2023), to then reach out
to potential participants. Prior works (Chakravorti
et al., 2023) have previously employed this process
to identify high-quality participants. We received
a total of 223 responses, out of which 171 were
complete and usable for analysis.
5.2 Survey Structure
The survey employed a combination of 14 open-
ended and close-ended questions. The survey has
been built based on the previous survey design tech-
niques (Rosen et al., 2013; Baker, 2016; Van No-
orden and Perkel, 2023; Chakravorti et al., 2024).
Open-ended questions and free-response text boxes
allow us to gather rich opinions from participants.
This approach integrates all our findings, providing
a broader and deeper understanding of the response.
For the analysis of open-ended questions, we uti-
lized thematic analysis, drawing from established
methodologies outlined in Blandford et al. (2016);
Terry et al. (2017). The close-ended questions were
analyzed using descriptive statistics to summarize
and analyze the numerical data obtained from re-
spondents. Throughout the analysis process, the
research team made collective decisions regarding
the retention, removal, or reorganization of themes
derived from open-ended responses. All the survey
questions have been provided in Appendix 10.4 .
5.3 Survey Findings
We now summarize insights from our responses
to explore various perspectives on hallucinations
in LLMs, including perceptions, weaknesses, and
preferences. The breakdown of responses indicates
that 76.54% of participants were from academia,
20.98% from the industry, and 2.47% both.
Participants were also asked about their research
area’s direct relation to AI and NLP. The analysis
revealed that more than 68.52% of researchers in-
dicated that their work is directly related to NLP,
while the remaining respondents either exhibited
familiarity with or indirectly incorporated NLP andFigure 3: Respondents familiarity with ‘Hallucination’
AI methodologies in their work. This highlights the
substantial involvement of AI experts and practi-
tioners within the survey.
5.3.1 Familiarity with Hallucination
The survey included the question on participants’
familiarity with the concept of ‘hallucinations’ in
AI-generated text, measured on a 5-point Likert
scale. The analysis revealed that 24.07% of re-
searchers reported being extremely familiar with
the concept, while 33.33% indicated being very fa-
miliar with it (Figure 3). Participants who indicated
not being familiar with the term ‘hallucination’
(7.96%) also demonstrated implicit concerns with
this phenomenon by highlighting issues such as
generating incorrect responses and crafting stories
autonomously. This demonstrates the widespread
impact of the phenomenon within the community.
5.3.2 Hallucination Frequency
The survey included a question regarding the fre-
quency of encountering ‘hallucinated’ content, de-
fined as content that is factually incorrect or unre-
lated to the input, assessed on a 5-point Likert scale
ranging from ‘Never’ to ‘Very frequently’ (Figure
4). The analysis revealed that 46.91% of respon-
dents reported encountering hallucinated content
occasionally, while 29.01% indicated experiencing
Figure 4: Frequency of encountering ‘Hallucination’it frequently. The results suggest that a substan-
tial portion of practitioners encounter instances of
hallucinated content in AI-generated outputs, indi-
cating a prevalent issue in generative NLP models.
5.3.3 Perceptions of Hallucination
The survey findings revealed that more than 92%
of respondents perceive hallucination as a weak-
ness of LLMs. Subsequently, participants were
asked to provide their own definitions of ‘hallu-
cination’ in generative AI models through an open-
ended question. To analyze these responses, we
applied thematic categorization based on attributes
generated from the literature audit (Table 2).
The thematic categorization revealed that the ma-
jority of respondents categorized hallucination as
pertaining to the factuality and faithfulness of input,
with relatively lesser emphasis on the extrinsic and
intrinsic nature of hallucination concerning the in-
put. This trend reflects a common perception of
how hallucination is understood within the context
of larger-scope generative AI models.
Moreover, the analysis identified 12 distinct
frameworks regarding how hallucination is de-
fined by respondents. For example:
“Response that appears syntactically and seman-
tically believable, but is not based on actual fact” —
Academic Researcher, NLP
“When the model confidently states something
that is not true” —Academic Researcher, AI
The diversity of viewpoints underscores the in-
consistency within the field regarding the concep-
tualization and understanding of hallucination in
the context of generative AI models.
5.3.4 Alternative Terms for Hallucination
The survey included a question asking participants
if they prefer an alternate term to describe the phe-
nomenon of ‘hallucination’ in AI-generated content
and to provide an explanation if they do. The anal-
ysis revealed that 54.32% of respondents preferred
the term hallucination or had no other term to pro-
vide. However, among the remaining responses,
40.46% of participants mentioned ‘Fabrication’
as a better term to describe the phenomenon.
This indicates that while the majority of respon-
dents did not propose an alternative term, a notable
proportion sees fabrications as a more suitable de-
scriptor for the phenomenon of hallucination in
AI-generated content. For example,
“Fabrication makes more sense. Hallucination
makes it feel like AI is human and has the samesensory perceptions that could lead to hallucina-
tions.” —Academic Researcher, AI & Education
It’s interesting to note that a few researchers also
prefer to use the term ‘Confabulations’ instead
of ‘hallucinations’ when referring to AI-generated
content. Their rationale likely stems from the nu-
anced difference in meaning between the two terms.
While hallucinations generally convey the idea of
perceiving something that is not based on reality
or fact, confabulations specifically refer to the cre-
ation of false memories or information without the
intention to deceive.
By opting for the term ‘Confabulations,’ re-
searchers may be emphasizing the unintentional
nature of the inaccuracies or false information gen-
erated by AI models, as opposed to implying delib-
erate deceit. For example,
“I think confabulation works better because it
means creating a false memory without deceit. Fab-
rication gives the idea that it is intentional, which
in the case of generative AI models, it is not.” —
Academic Researcher, AI & HCI
It’s also insightful to see that respondents pro-
posed various alternative terms to describe the
phenomenon of hallucination in AI-generated con-
tent such as incorrect information/misinformation,
Non-factual information, Cognitive gap, hyper-
generalization, Overconfidence, andRandomness .
These alternatives highlight different aspects and
nuances of the inaccuracies or distortions present in
the generated content. Participants also mentioned
how they prefer multiple terms based on the appli-
cation in which they are used.
“As I mentioned there are different types of hal-
lucinations. For instruction and context hallucina-
tions, I would refer to them as inconsistency in-
stead. For factually incorrect hallucinations, the
word hallucination is fine.” —Academia, NLP
5.3.5 Creativity and Positive Applications
Not all researchers view hallucinations in AI-
generated content through a negative lens. While
the majority may associate hallucinations with inac-
curacies or distortions, a notable minority (∼12%
in our survey) provided insights into how they be-
lieve hallucinations in these models can be cor-
related with creativity rather than negatively im-
pacted behaviors. In fields such as story narration
and image generation, researchers often value the
creative behaviors exhibited by AI models. Halluci-
nations, when viewed in this context, may be seen
as manifestations of the model’s ability to thinkoutside the box, generate novel ideas, and explore
unconventional possibilities. These creative outputs
can inspire new approaches to storytelling, art, and
problem-solving, contributing to innovation and
artistic expression. For example:
“Hallucinations are just what is needed for mod-
els to be creative. In truth, unless AI text-generators
are factually grounded with external knowledge
for a specific field, they are just story generators
which aim to be creative, hence“hallucinate."” —
National Lab Researcher, NLP
Further supplementary analysis and quotes on
the various external perspectives and the societal
ramifications of hallucination, obtained through the
survey, is examined in the Appendix 10.3 .
6 Challenges and Recommendations
Based on our audit and survey analysis, we outline
the key weaknesses encountered in hallucination
within NLP and potential recommendations moti-
vated by the weaknesses. We utilize a community-
centric approach to define the primary weaknesses
of the field currently and a path forward.
6.1 Challenges
The primary challenges we identify thematically
and aim to elucidate are as follows:
Wide range of vague and absent definitions:
The literature and the practitioner’s survey show
diverse and conflicting frameworks, often lacking
clarity or omitting explicit definitions for halluci-
nation and how it is perceived in various fields of
NLP and language generation. Ambiguity arises
from the use of terms like ‘confabulations,’ ‘ fab-
rications,’ ‘misinformation,’ and ‘hallucinations’
interchangeably, without clear definitions in the
context of hallucinations.
Lack of standardization in measurement: The
absence of standardized metrics to quantify hallu-
cination results in the use of multiple scales and
categorizations. This makes it challenging to com-
pare and interpret results across different models
and studies, leading to a proliferation of diverse
approaches for evaluating hallucinations.
Limited awareness of hallucination in a so-
ciotechnical context: Hallucination research of-
ten lacks the understanding of how the concept of
hallucination is conceptualized beyond its techni-
cal purview. When such analysis is employed in
sociotechnical systems like healthcare and policy
making (Dahl et al., 2024; Pal et al., 2023), it is im-portant to define the socially relevant framework of
hallucination as well. There is no motivation shown
to understand the non-technical considerations of
hallucination.
Multiple sentiment towards hallucination:
The perception of hallucination in generative AI
varies depending on the context. For instance, it
is often positively regarded as creativity in image
generation, whereas in text generation, it is viewed
negatively as errors or mistakes. Consequently, fu-
ture research efforts should aim to better address
this disparity to develop a more nuanced framework
for understanding hallucination.
Lack of standardized nomenclature: Both our
literature audit and practitioner survey revealed that
the term ‘hallucination’ is inadequate to fully cap-
ture the behavior exhibited by NLG models. There
is a need for further investigation into which terms
are more appropriate and why they are necessary.
For instance, terms like ‘confabulation,’ ‘fabrica-
tions,’ and ‘misinformations’ are increasingly be-
ing used to describe the same phenomenon. A more
precise understanding is required to distinguish be-
tween these terms and how they are utilized in
various fields within NLP.
User trust and reliability: Our survey findings
suggest that users may hesitate to fully utilize LLM
capabilities due to concerns about bias and halluci-
nation despite recognizing the potential advantages
these models offer. Therefore, there is a need to
focus efforts on understanding the human interac-
tion aspect concerning hallucination in NLP and
language generation.
Addressing these issues requires careful consid-
eration of the categorization approach, integration
of contextual information, and, efforts towards ro-
bust evaluation methodologies in hallucinations.
6.2 Recommendations
Expanding on audits like Blodgett et al. (2020)
& Venkit et al. (2023), we examine strategies for
NLP practitioners studying ‘hallucination’ to over-
come these challenges. We propose two overarch-
ing themes with four associated recommendations.
Author-Centric Recommendation. These rec-
ommendations prioritize actionable steps for both
the author and developers, emphasizing transparent
and accountable development in conceptualizing
hallucinations.
[R1] Ensure explicit documentation of the hal-
lucination framework and analysis methodologyemployed during the development of NLP mod-
els. Provide guidelines that outline the expected
measurements and quantifications for the model to
enhance interpretability and applicability.
[R2] Explicitly state the use cases and user pro-
files intended to interact with the NLP system. By
considering the specific applications and targeted
users, it is easier to construct the required frame-
work of hallucination that is sensitive to the com-
munity in consideration. Raise awareness about
potential ramifications introduced by NLP models,
emphasizing the importance of fairness and equity.
Community-Centric Recommendation. These
recommendations prioritize actionable steps for the
research community to enhance frameworks and
understanding related to hallucinations.
[R3] Develop clear and standardized definitions
for terms such as ‘confabulations,’ ‘fabrications,’
‘misinformation,’ and ‘hallucinations’ within the
context of NLP. Establish frameworks that provide
clarity and consistency in understanding these con-
cepts, particularly regarding hallucinations. This
requirement is crucial due to the widespread mis-
understanding of hallucination and the misnomers
that have arisen as research progresses.
[R4] Promote the creation of methods that offer
visibility into the model’s decision-making process,
enabling users to comprehend how hallucinations
or fabrications can occur within the system, thus
fostering trust in its use. Facilitating research dis-
cussions for transparency through workshops and
conferences is one approach to achieving this goal.
7 Conclusion
Our work delves into the conceptualization of hal-
lucination within the scope of NLP. Our approach
involved two key methodologies: first, an exhaus-
tive audit of 103 peer-reviewed papers in the NLP
domain, and second, a practitioner survey of 171
researchers to complement our first study with real-
world practical perception and understanding of
hallucination as a unique contribution. Through this
analysis, we have gained insights into how the NLP
community conceptualizes and defines hallucina-
tion, showcasing a lack of discourse and agreement.
Additionally, our thematic and community-based
approach highlights potential weaknesses within
the field, particularly in addressing misrepresenta-
tions and inaccurate characterizations associated
with hallucination, paving way for better advance-
ment in language generation.8 Limitations
Our study encompasses a selection of 103 papers,
incorporating works from primarily the ACL An-
thology. While our intention was not to provide
an exhaustive collection of all published works on
hallucination, we aimed to include diverse sources
within NLP that cover various aspects of the field.
Our intent was to curate peer-reviewed literature
commonly found in the NLP domain, encompass-
ing models, applications, survey papers, and frame-
works. We, therefore, did not scope the utility of
hallucination and its impact beyond NLP to other
fields of research, such as Computer Vision. Re-
garding the creation of the challenges and recom-
mendations, it is important to note that the themes
presented are not meant to be exhaustive but rather
serve as a foundational framework to spark addi-
tional inquiries and foster further engagement.
Our survey was designed to capture the view-
points of researchers and practitioners in the AI
and ML field, potentially limiting various experi-
ences. As such, our analysis is centered on this per-
spective. While we did gather additional insights
from participants outside this field, our focus was
not comprehensive in that regard. Our future work
intends to explore the public’s perspective on hal-
lucination.
9 Ethics Statement
We are aware of the ethical considerations involved
in our research and have taken measures to ensure
responsible practices throughout the study.
Data Publication: All the papers used in our re-
search are listed in the Appendix. However, we rec-
ognize the importance of transparency and account-
ability. Therefore, we publish the complete collec-
tion of papers along with our qualitative classifica-
tion and annotation, allowing for public scrutiny
and examination2.
Mitigating Qualitative Bias: We acknowledge
the potential for bias when performing qualitative
coding of the papers regarding their applications.
To address this concern, we ensured that at least
three different individuals independently reviewed
and verified the coding to minimize the possibility
of misclassification. Additionally, we followed the
same approach to verify the presence of various def-
initions in each paper, enhancing the reliability and
validity of our analysis. By disclosing these ethical
2https://github.com/PranavNV/The-Thing-Called-
Hallucinationconsiderations, we emphasize our commitment to
conducting research in an ethical and accountable
manner.
References
Eunice Akani, Benoit Favre, Frederic Bechet, and Ro-
main Gemignani. 2023. Reducing named entity hal-
lucination risk to ensure faithful summary generation.
InProceedings of the 16th International Natural Lan-
guage Generation Conference , pages 437–442.
Antonios Anastasopoulos and Graham Neubig. 2019.
Pushing the limits of low-resource morphological
inflection. arXiv preprint arXiv:1908.05838 .
Monya Baker. 2016. Reproducibility crisis. nature ,
533(26):353–66.
Simon Baker and Takeo Kanade. 2000. Hallucinating
faces. In Proceedings of 4th IEEE International Con-
ference on Automatic Face and Gesture Recognition
(FG ’00) , pages 83 – 88.
Emily M Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language models
be too big? In Proceedings of the 2021 ACM confer-
ence on fairness, accountability, and transparency ,
pages 610–623.
Melania Berbatova and Yoan Salambashev. 2023. Evalu-
ating hallucinations in large language models for Bul-
garian language. In Proceedings of the 8th Student
Research Workshop associated with the International
Conference Recent Advances in Natural Language
Processing , pages 55–63, Varna, Bulgaria. INCOMA
Ltd., Shoumen, Bulgaria.
Ann Blandford, Dominic Furniss, and Stephann Makri.
2016. Qualitative HCI research: Going behind the
scenes . Morgan & Claypool Publishers.
Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna Wallach. 2020. Language (technology) is
power: A critical survey of “bias” in NLP. In Pro-
ceedings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics , pages 5454–5476,
Online. Association for Computational Linguistics.
Nicholas Grant Boeving. 2020. Transpersonal psychol-
ogy. In Encyclopedia of Psychology and Religion ,
pages 2392–2394. Springer.
Adam Bouyamourn. 2023. Why LLMs hallucinate, and
how to get (evidential) closure: Perceptual, inten-
sional, and extensional learning for faithful natural
language generation. In Proceedings of the 2023
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 3181–3193, Singapore. As-
sociation for Computational Linguistics.
Meng Cao, Yue Dong, and Jackie Chi Kit Cheung. 2021.
Hallucinated but factual! inspecting the factuality ofhallucinations in abstractive summarization. arXiv
preprint arXiv:2109.09784 .
Tatiana Chakravorti, Robert Fraleigh, Timothy Fritton,
Michael McLaughlin, Vaibhav Singh, Christopher
Griffin, Anthony Mark Kwasnica, David Pennock,
C Lee Giles, and Sarah Rajtmajer. 2023. A prototype
hybrid prediction market for estimating replicability
of published work. In 2nd International Conference
on Hybrid Human-Artificial Intelligence, HHAI 2023 ,
pages 300–309. IOS Press BV .
Tatiana Chakravorti, Sai Dileep Koneru, and Sarah Rajt-
majer. 2024. Reproducibility, replicability, and trans-
parency in research: What 430 professors think in
universities across the usa and india. arXiv preprint
arXiv:2402.08796 .
Qinyu Chen, Wenhao Wu, and Sujian Li. 2023a. Ex-
ploring in-context learning for knowledge grounded
dialog generation. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
10071–10081.
Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth.
2021. Improving faithfulness in abstractive sum-
marization with contrast candidate generation and
selection. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 5935–5941, Online. Association for
Computational Linguistics.
Wei-Lin Chen, Cheng-Kuang Wu, Hsin-Hsi Chen,
and Chung-Chi Chen. 2023b. Fidelity-enriched
contrastive search: Reconciling the faithfulness-
diversity trade-off in text generation. arXiv preprint
arXiv:2310.14981 .
Prafulla Kumar Choubey, Alex Fabbri, Jesse Vig, Chien-
Sheng Wu, Wenhao Liu, and Nazneen Rajani. 2023.
CaPE: Contrastive parameter ensembling for reduc-
ing hallucination in abstractive summarization. In
Findings of the Association for Computational Lin-
guistics: ACL 2023 , pages 10755–10773, Toronto,
Canada. Association for Computational Linguistics.
V olkan Cirik, Louis-Philippe Morency, and Taylor Berg-
Kirkpatrick. 2022. Holm: Hallucinating objects with
language models for referring expression recognition
in partially-observed scenes. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
5440–5453.
Matthew Dahl, Varun Magesh, Mirac Suzgun, and
Daniel E. Ho. 2024. Hallucinating law: Legal mis-
takes with large language models are pervasive.
Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale
Fung. 2023. Plausible may not be faithful: Probing
object hallucination in vision-language pre-training.
InProceedings of the 17th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics , pages 2136–2148, Dubrovnik, Croatia.
Association for Computational Linguistics.David Dale, Elena V oita, Loïc Barrault, and Marta R
Costa-jussà. 2022. Detecting and mitigating halluci-
nations in machine translation: Model internal work-
ings alone do well, sentence similarity even better.
arXiv preprint arXiv:2212.08597 .
David Dale, Elena V oita, Janice Lam, Prangthip
Hansanti, Christophe Ropers, Elahe Kalbassi, Cyn-
thia Gao, Loic Barrault, and Marta Costa-jussà. 2023.
HalOmi: A manually annotated benchmark for multi-
lingual hallucination and omission detection in ma-
chine translation. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 638–653, Singapore. Association
for Computational Linguistics.
Souvik Das, Sougata Saha, and Rohini Srihari. 2022.
Diving deep into modes of fact hallucinations in di-
alogue systems. In Findings of the Association for
Computational Linguistics: EMNLP 2022 , pages 684–
699, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Yue Dong, John Wieting, and Pat Verga. 2022. Faith-
ful to the document or to the world? mitigating hal-
lucinations via entity-linked knowledge in abstrac-
tive summarization. In Findings of the Association
for Computational Linguistics: EMNLP 2022 , pages
1067–1082, Abu Dhabi, United Arab Emirates. Asso-
ciation for Computational Linguistics.
Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Za-
iane, Mo Yu, Edoardo M Ponti, and Siva Reddy. 2022.
Faithdial: A faithful benchmark for information-
seeking dialogue. Transactions of the Association for
Computational Linguistics , 10:1473–1490.
Hao Fei, Qian Liu, Meishan Zhang, Min Zhang, and Tat-
Seng Chua. 2023. Scene graph as pivoting: Inference-
time image-free unsupervised multimodal machine
translation with visual scene hallucination. In Pro-
ceedings of the 61st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 5980–5994, Toronto, Canada. Associ-
ation for Computational Linguistics.
Javier Ferrando, Gerard I. Gállego, Belen Alastruey,
Carlos Escolano, and Marta R. Costa-jussà. 2022.
Towards opening the black box of neural machine
translation: Source and target interpretations of the
transformer. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Process-
ing, pages 8756–8769, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Katja Filippova. 2020a. Controlled hallucinations:
Learning to generate faithfully from noisy data.
arXiv preprint arXiv:2010.05873 .
Katja Filippova. 2020b. Controlled hallucinations:
Learning to generate faithfully from noisy data. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 864–870, Online. As-
sociation for Computational Linguistics.Andreas Fink, Mathias Benedek, Human-F Unterrainer,
Ilona Papousek, and Elisabeth M Weiss. 2014. Cre-
ativity and psychopathology: are there similar mental
processes involved in creativity and in psychosis-
proneness? Frontiers in psychology , 5:117336.
Lorenzo Jaime Flores and Arman Cohan. 2024. On the
benefits of fine-grained loss truncation: A case study
on factuality in summarization. In Proceedings of
the 18th Conference of the European Chapter of the
Association for Computational Linguistics (Volume
2: Short Papers) , pages 138–150, St. Julian’s, Malta.
Association for Computational Linguistics.
Korbinian Friedl, Georgios Rizos, Lukas Stappen, Mad-
ina Hasan, Lucia Specia, Thomas Hain, and Björn
Schuller. 2021. Uncertainty aware review hallucina-
tion for science article classification. In Findings of
the Association for Computational Linguistics: ACL-
IJCNLP 2021 , pages 5004–5009, Online. Association
for Computational Linguistics.
Sanjana Gautam, Pranav Narayanan Venkit, and Souro-
jit Ghosh. 2024. From melting pots to misrepre-
sentations: Exploring harms in generative ai. arXiv
preprint arXiv:2403.10776 .
Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang.
2022. Findings of the association for computational
linguistics: Emnlp 2022. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2022 .
Javier González Corbelle, Alberto Bugarín-Diz, Jose
Alonso-Moral, and Juan Taboada. 2022. Dealing
with hallucination and omission in neural natural lan-
guage generation: A use case on meteorology. In
Proceedings of the 15th International Conference
on Natural Language Generation , pages 121–130,
Waterville, Maine, USA and virtual meeting. Associ-
ation for Computational Linguistics.
Javier González-Corbelle, Alberto Bugarín Diz, Jose
Alonso-Moral, and Juan Taboada. 2022. Dealing
with hallucination and omission in neural natural
language generation: A use case on meteorology. In
Proceedings of the 15th International Conference on
Natural Language Generation , pages 121–130.
Nuno M. Guerreiro, Pierre Colombo, Pablo Piantanida,
and André Martins. 2023a. Optimal transport for un-
supervised hallucination detection in neural machine
translation. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers) , pages 13766–13784,
Toronto, Canada. Association for Computational Lin-
guistics.
Nuno M. Guerreiro, Elena V oita, and André Martins.
2023b. Looking for a needle in a haystack: A com-
prehensive study of hallucinations in neural machine
translation. In Proceedings of the 17th Conference
of the European Chapter of the Association for Com-
putational Linguistics , pages 1059–1075, Dubrovnik,
Croatia. Association for Computational Linguistics.Vipul Gupta, Pranav Narayanan Venkit, Shomir Wil-
son, and Rebecca J. Passonneau. 2024. Sociode-
mographic bias in language models: A survey and
forward path.
Chih-Chung Hsu, Chia-Wen Lin, Chiou-Ting Hsu,
Hong-Yuan Mark Liao, and Jen-Yu Yu. 2010. Face
hallucination using bayesian global estimation and
local basis selection. In 2010 IEEE International
Workshop on Multimedia Signal Processing , pages
449–453.
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang, Qianglong Chen,
Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.
A survey on hallucination in large language models:
Principles, taxonomy, challenges, and open questions.
arXiv preprint arXiv:2311.05232 .
Ann Irvine and Chris Callison-Burch. 2014. Halluci-
nating phrase translations for low resource mt. In
Proceedings of the Eighteenth Conference on Compu-
tational Natural Language Learning , pages 160–170.
Saad Obaid ul Islam, Iza Škrjanec, Ondrej Dusek, and
Vera Demberg. 2023. Tackling hallucinations in neu-
ral chart summarization. In Proceedings of the 16th
International Natural Language Generation Confer-
ence, pages 414–423, Prague, Czechia. Association
for Computational Linguistics.
Saad Obaid Ul Islam, Iza Škrjanec, Ond ˇrej Dušek, and
Vera Demberg. 2023. Tackling hallucinations in neu-
ral chart summarization. In Proceedings of the 16th
International Natural Language Generation Confer-
ence, pages 414–423.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto, and Pascale Fung. 2023a. Survey of hallu-
cination in natural language generation. ACM Com-
puting Surveys , 55(12):1–38.
Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan
Wilie, Min Zeng, and Pascale Fung. 2023b. RHO:
Reducing hallucination in open-domain dialogues
with knowledge grounding. In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,
pages 4504–4522, Toronto, Canada. Association for
Computational Linguistics.
Ziwei Ji, YU Tiezheng, Yan Xu, Nayeon Lee, Etsuko
Ishii, and Pascale Fung. 2023c. Towards mitigating
llm hallucination via self reflection. In The 2023 Con-
ference on Empirical Methods in Natural Language
Processing .
Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko
Ishii, and Pascale Fung. 2023d. Towards mitigat-
ing LLM hallucination via self reflection. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2023 , pages 1827–1843, Singapore. Associ-
ation for Computational Linguistics.Yiren Jian, Chongyang Gao, and Soroush V osoughi.
2022. Embedding hallucination for few-shot lan-
guage fine-tuning. In Proceedings of the 2022 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies , pages 5522–5530, Seattle,
United States. Association for Computational Lin-
guistics.
Zhijing Jin and Rada Mihalcea. 2022. Natural language
processing for policymaking. In Handbook of Com-
putational Social Science for Policy , pages 141–162.
Springer International Publishing Cham.
Andrej Karpathy. 2015. The unreasonable effectiveness
of recurrent neural networks. https://karpathy.github.
io/2015/05/21/rnn-effectiveness/.
Mayank Kothyari, Dhruva Dhingra, Sunita Sarawagi,
and Soumen Chakrabarti. 2023. CRUSH4SQL:
Collective retrieval using schema hallucination for
Text2SQL. In Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Process-
ing, pages 14054–14066, Singapore. Association for
Computational Linguistics.
Faisal Ladhak, Esin Durmus, and Tatsunori Hashimoto.
2022. Contrastive error attribution for finetuned lan-
guage models. arXiv preprint arXiv:2212.10722 .
Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi
Zhang, Dan Jurafsky, Kathleen McKeown, and Tat-
sunori B Hashimoto. 2023. When do pre-training bi-
ases propagate to downstream tasks? a case study in
text summarization. In Proceedings of the 17th Con-
ference of the European Chapter of the Association
for Computational Linguistics , pages 3206–3219.
Mateusz Lango and Ondrej Dusek. 2023. Critic-driven
decoding for mitigating hallucinations in data-to-text
generation. In Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Process-
ing, pages 2853–2862, Singapore. Association for
Computational Linguistics.
Lisa Legault. 2020. Encyclopedia of personality and
individual differences. Encyclopedia of Personality
and Individual Differences , pages 1–5.
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao,
and Ji-Rong Wen. 2023. Evaluating object hallucina-
tion in large vision-language models. In Proceedings
of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 292–305, Sin-
gapore. Association for Computational Linguistics.
Hui Liu and Xiaojun Wan. 2023. Models see hallucina-
tions: Evaluating the factuality in video captioning.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
11807–11823, Singapore. Association for Computa-
tional Linguistics.
Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao,
Zhifang Sui, Weizhu Chen, and Bill Dolan. 2022.
A token-level reference-free hallucination detectionbenchmark for free-form text generation. In Proceed-
ings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers) , pages 6723–6737, Dublin, Ireland. Association
for Computational Linguistics.
Shayne Longpre, Kartik Perisetla, Anthony Chen,
Nikhil Ramesh, Chris DuBois, and Sameer Singh.
2021. Entity-based knowledge conflicts in question
answering. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 7052–7063, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Kishan Maharaj, Ashita Saxena, Raja Kumar, Abhijit
Mishra, and Pushpak Bhattacharyya. 2023. Eyes
show the way: Modelling gaze behaviour for hallu-
cination detection. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
11424–11438, Singapore. Association for Computa-
tional Linguistics.
Himanshu Maheshwari, Sumit Shekhar, Apoorv Sax-
ena, and Niyati Chhaya. 2023. Open-world factually
consistent question generation. In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,
pages 2390–2404, Toronto, Canada. Association for
Computational Linguistics.
Negar Maleki, Balaji Padmanabhan, and Kaushik Dutta.
2024. Ai hallucinations: A misnomer worth clarify-
ing. arXiv preprint arXiv:2401.06796 .
Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.
SelfCheckGPT: Zero-resource black-box hallucina-
tion detection for generative large language models.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
9004–9017, Singapore. Association for Computa-
tional Linguistics.
Andreas Marfurt and James Henderson. 2022. Un-
supervised token-level hallucination detection from
summary generation by-products. In Proceedings of
the 2nd Workshop on Natural Language Generation,
Evaluation, and Metrics (GEM) , pages 248–261, Abu
Dhabi, United Arab Emirates (Hybrid). Association
for Computational Linguistics.
NL Mason, KPC Kuypers, JT Reckweg, F Müller, DHY
Tse, B Da Rios, SW Toennes, P Stiers, A Feilding,
and JG Ramaekers. 2021. Spontaneous and delib-
erate creative cognition during and after psilocybin
exposure. Translational psychiatry , 11(1):209.
Luca Massarelli, Fabio Petroni, Aleksandra Piktus,
Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fab-
rizio Silvestri, and Sebastian Riedel. 2020. How
decoding strategies affect the verifiability of gener-
ated text. In Findings of the Association for Compu-
tational Linguistics: EMNLP 2020 , pages 223–235,
Online. Association for Computational Linguistics.Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906–1919, On-
line. Association for Computational Linguistics.
Alessia McGowan, Yunlai Gui, Matthew Dobbs, Sophia
Shuster, Matthew Cotter, Alexandria Selloni, Mar-
ianne Goodman, Agrima Srivastava, Guillermo A
Cecchi, and Cheryl M Corcoran. 2023. Chatgpt and
bard exhibit spontaneous citation fabrication during
psychiatry literature search. Psychiatry Research ,
326:115334.
Nick McKenna, Tianyi Li, Liang Cheng, Mohammad
Hosseini, Mark Johnson, and Mark Steedman. 2023.
Sources of hallucination by large language models
on inference tasks. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
2758–2774, Singapore. Association for Computa-
tional Linguistics.
Sabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-
Lan Boureau. 2022. Reducing conversational agents’
overconfidence through linguistic calibration. Trans-
actions of the Association for Computational Linguis-
tics, 10:857–872.
Beren Millidge. 2023. Llms confabulate not hallucinate.
Mathias Müller, Annette Rios, and Rico Sennrich. 2020.
Domain robustness in neural machine translation. In
Proceedings of the 14th Conference of the Associa-
tion for Machine Translation in the Americas (Volume
1: Research Track) , pages 151–164, Virtual. Associa-
tion for Machine Translation in the Americas.
Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero
Nogueira dos Santos, Henghui Zhu, Dejiao Zhang,
Kathleen McKeown, and Bing Xiang. 2021. Entity-
level factual consistency of abstractive text summa-
rization. In Proceedings of the 16th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Main Volume , pages 2727–2733,
Online. Association for Computational Linguistics.
Pranav Narayanan Venkit. 2023. Towards a holistic
approach: Understanding sociodemographic biases
in nlp models using an interdisciplinary lens. In
Proceedings of the 2023 AAAI/ACM Conference on
AI, Ethics, and Society , pages 1004–1005.
Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Pan-
chanadikar, Ting-Hao Huang, and Shomir Wilson.
2023. Unmasking nationality bias: A study of human
perception of nationalities in ai-generated articles. In
Proceedings of the 2023 AAAI/ACM Conference on
AI, Ethics, and Society , pages 554–565.
US News. 2023. Best national university rankings 2023.
Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and
Chin-Yew Lin. 2019. A simple recipe towards re-
ducing hallucination in neural surface realisation. InProceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 2673–
2679, Florence, Italy. Association for Computational
Linguistics.
Ankit Pal, Logesh Kumar Umapathi, and Malaikannan
Sankarasubbu. 2023. Med-HALT: Medical domain
hallucination test for large language models. In Pro-
ceedings of the 27th Conference on Computational
Natural Language Learning (CoNLL) , pages 314–
334, Singapore. Association for Computational Lin-
guistics.
Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia,
Xinyi Wang, Machel Reid, and Sebastian Ruder.
2023. mmT5: Modular multilingual pre-training
solves source language hallucinations. In Findings
of the Association for Computational Linguistics:
EMNLP 2023 , pages 1978–2008, Singapore. Associ-
ation for Computational Linguistics.
Liam van der Poel, Ryan Cotterell, and Clara Meis-
ter. 2022. Mutual information alleviates hallucina-
tions in abstractive summarization. In Proceedings
of the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 5956–5965, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Fina Polat, Ilaria Tiddi, Paul Groth, and Piek V ossen.
2023. Improving graph-to-text generation using cy-
cle training. In Proceedings of the 4th Conference on
Language, Data and Knowledge , pages 256–261.
Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo Ponti,
and Shay Cohen. 2023. Detecting and mitigating
hallucinations in multilingual summarisation. In Pro-
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pages 8914–
8932, Singapore. Association for Computational Lin-
guistics.
Anil Ramakrishna, Rahul Gupta, Jens Lehmann, and
Morteza Ziyadi. 2023. Invite: a testbed of automat-
ically generated invalid questions to evaluate large
language models for hallucinations. In The 2023
Conference on Empirical Methods in Natural Lan-
guage Processing .
Vikas Raunak, Arul Menezes, and Marcin Junczys-
Dowmunt. 2021. The curious case of hallucinations
in neural machine translation. In Proceedings of
the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1172–1183,
Online. Association for Computational Linguistics.
Vipula Rawte, Swagata Chakraborty, Agnibh Pathak,
Anubhav Sarkar, SM Tonmoy, Aman Chadha, Amit P
Sheth, and Amitava Das. 2023a. The troubling emer-
gence of hallucination in large language models–an
extensive definition, quantification, and prescriptive
remediations. arXiv preprint arXiv:2310.04988 .Vipula Rawte, Amit Sheth, and Amitava Das. 2023b. A
survey of hallucination in large foundation models.
arXiv preprint arXiv:2309.05922 .
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,
Trevor Darrell, and Kate Saenko. 2018. Object hallu-
cination in image captioning. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing , pages 4035–4045, Brussels,
Belgium. Association for Computational Linguistics.
Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Eric Michael Smith, Y-Lan Boureau, and Jason We-
ston. 2021. Recipes for building an open-domain
chatbot. In Proceedings of the 16th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Main Volume , pages 300–325,
Online. Association for Computational Linguistics.
Larry D Rosen, Kelly Whaling, L Mark Carrier,
Nancy A Cheever, and Jeffrey Rokkum. 2013. The
media and technology usage and attitudes scale: An
empirical investigation. Computers in human behav-
ior, 29(6):2501–2511.
Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun
Araki, Arsalan Gundroo, Bingqing Wang, Rakesh
Menon, Md Parvez, and Zhe Feng. 2023. Delu-
cionQA: Detecting hallucinations in domain-specific
question answering. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
822–835, Singapore. Association for Computational
Linguistics.
Farhan Samir and Miikka Silfverberg. 2022. One wug,
two wug+s transformer inflection models hallucinate
affixes. In Proceedings of the Fifth Workshop on the
Use of Computational Methods in the Study of En-
dangered Languages , pages 31–40, Dublin, Ireland.
Association for Computational Linguistics.
Jianbin Shen, Junyu Xuan, and Christy Liang. 2023.
Mitigating intrinsic named entity-related hallucina-
tions of abstractive text summarization. In Findings
of the Association for Computational Linguistics:
EMNLP 2023 , pages 15807–15824, Singapore. Asso-
ciation for Computational Linguistics.
Xiao Shi, Zhengyuan Zhu, Zeyu Zhang, and Chengkai
Li. 2023. Hallucination mitigation in natural lan-
guage generation from large-scale open-domain
knowledge graphs. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 12506–12521, Singapore. Associ-
ation for Computational Linguistics.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021. Retrieval augmentation
reduces hallucination in conversation. In Findings
of the Association for Computational Linguistics:
EMNLP 2021 , pages 3784–3803, Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido
Dagan, and Shauli Ravfogel. 2023. The curious case
of hallucinatory (un) answerability: Finding truths
in the hidden states of over-confident large language
models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 3607–3625.
Eric Smith, Orion Hsu, Rebecca Qian, Stephen Roller,
Y-Lan Boureau, and Jason Weston. 2022. Human
evaluation of conversations is an open problem: com-
paring the sensitivity of various methods for eval-
uating dialogue agents. In Proceedings of the 4th
Workshop on NLP for Conversational AI , pages 77–
97, Dublin, Ireland. Association for Computational
Linguistics.
Seonil (Simon) Son, Junsoo Park, Jeong-in Hwang,
Junghwa Lee, Hyungjong Noh, and Yeonsoo Lee.
2022. HaRiM+: Evaluating summary quality with
hallucination risk. In Proceedings of the 2nd Confer-
ence of the Asia-Pacific Chapter of the Association
for Computational Linguistics and the 12th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers) , pages 895–924,
Online only. Association for Computational Linguis-
tics.
Brent J Steele. 2017. Hallucination and intervention.
Global Discourse , 7(2-3):201–218.
Bin Sun, Yitong Li, Fei Mi, Fanhu Bie, Yiwei Li, and
Kan Li. 2023. Towards fewer hallucinations in
knowledge-grounded dialogue generation via aug-
mentative and contrastive knowledge-dialogue. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers) , pages 1741–1750, Toronto, Canada.
Association for Computational Linguistics.
Anirudh S. Sundar and Larry Heck. 2023. cTBLS: Aug-
menting large language models with conversational
tables. In Proceedings of the 5th Workshop on NLP
for Conversational AI (NLP4ConvAI 2023) , pages 59–
70, Toronto, Canada. Association for Computational
Linguistics.
Gareth Terry, Nikki Hayfield, Victoria Clarke, and Vir-
ginia Braun. 2017. Thematic analysis. The SAGE
handbook of qualitative research in psychology , 2:17–
37.
Alberto Testoni and Raffaella Bernardi. 2021. “I’ve
seen things you people wouldn’t believe”: Halluci-
nating entities in GuessWhat?! In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing: Stu-
dent Research Workshop , pages 101–111, Online. As-
sociation for Computational Linguistics.
SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vip-
ula Rawte, Aman Chadha, and Amitava Das. 2024.
A comprehensive survey of hallucination mitigation
techniques in large language models. arXiv preprint
arXiv:2401.01313 .Mojtaba Vaismoradi, Hannele Turunen, and Terese Bon-
das. 2013. Content analysis and thematic analysis:
Implications for conducting a qualitative descriptive
study. Nursing & health sciences , 15(3):398–405.
Richard Van Noorden and Jeffrey M Perkel. 2023. Ai
and science: what 1,600 researchers think. Nature ,
621(7980):672–675.
Pranav Venkit, Mukund Srinath, Sanjana Gautam,
Saranya Venkatraman, Vipul Gupta, Rebecca J Pas-
sonneau, and Shomir Wilson. 2023. The sentiment
problem: A critical survey towards deconstructing
sentiment analysis. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 13743–13763.
Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mo-
hit Iyyer, and Noah Constant. 2022. Overcoming
catastrophic forgetting in zero-shot cross-lingual gen-
eration. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 9279–9300, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Jonas Waldendorf, Barry Haddow, and Alexandra Birch.
2024. Contrastive decoding reduces hallucinations
in large multilingual machine translation models. In
Proceedings of the 18th Conference of the European
Chapter of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2526–2539,
St. Julian’s, Malta. Association for Computational
Linguistics.
Chaojun Wang and Rico Sennrich. 2020. On exposure
bias, hallucination and domain shift in neural ma-
chine translation. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 3544–3552, Online. Association for
Computational Linguistics.
Orion Weller, Marc Marone, Nathaniel Weir, Dawn
Lawrie, Daniel Khashabi, and Benjamin Van Durme.
2024. “according to . . . ”: Prompting language mod-
els improves quoting from pre-training data. In Pro-
ceedings of the 18th Conference of the European
Chapter of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2288–2301,
St. Julian’s, Malta. Association for Computational
Linguistics.
Stefan Werning. 2024. Generative ai and the techno-
logical imaginary of game design. In Creative Tools
and the Softwarization of Cultural Production , pages
67–90. Springer.
Minghao Wu, Abdul Waheed, Chiyu Zhang, Muham-
mad Abdul-Mageed, and Alham Aji. 2024. LaMini-
LM: A diverse herd of distilled models from large-
scale instructions. In Proceedings of the 18th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 944–964, St. Julian’s, Malta. Association for
Computational Linguistics.Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang
Liu, Qing-Long Han, and Yang Tang. 2023. A brief
overview of chatgpt: The history, status quo and po-
tential future development. IEEE/CAA Journal of
Automatica Sinica , 10(5):1122–1136.
Yijun Xiao and William Yang Wang. 2021a. On halluci-
nation and predictive uncertainty in conditional lan-
guage generation. arXiv preprint arXiv:2103.15025 .
Yijun Xiao and William Yang Wang. 2021b. On hal-
lucination and predictive uncertainty in conditional
language generation. In Proceedings of the 16th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Main Volume , pages
2734–2744, Online. Association for Computational
Linguistics.
Silei Xu, Shicheng Liu, Theo Culhane, Elizaveta Pert-
seva, Meng-Hsi Wu, Sina Semnani, and Monica Lam.
2023. Fine-tuned LLMs know more, hallucinate less
with few-shot sequence-to-sequence semantic pars-
ing over Wikidata. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing , pages 5778–5791, Singapore. Associa-
tion for Computational Linguistics.
Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023.
A new benchmark and reverse validation method for
passage-level hallucination detection. In Findings
of the Association for Computational Linguistics:
EMNLP 2023 , pages 3898–3908, Singapore. Associ-
ation for Computational Linguistics.
Sunjae Yoon, Eunseop Yoon, Hee Suk Yoon, Junyeong
Kim, and Chang Yoo. 2022. Information-theoretic
text hallucination reduction for video-grounded di-
alogue. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing ,
pages 4182–4193, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley
Malin, and Sricharan Kumar. 2023a. SAC3: Reliable
hallucination detection in black-box language models
via semantic-aware cross-check consistency. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2023 , pages 15445–15458, Singapore.
Association for Computational Linguistics.
Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng,
Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing
Wang, and Luoyi Fu. 2023b. Enhancing uncertainty-
based hallucination detection with stronger focus.
InProceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pages
915–932, Singapore. Association for Computational
Linguistics.
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,
Yulong Chen, et al. 2023c. Siren’s song in the ai
ocean: a survey on hallucination in large language
models. arXiv preprint arXiv:2309.01219 .Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,
Maosong Sun, and Qun Liu. 2019. ERNIE: En-
hanced language representation with informative en-
tities. In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics , pages
1441–1451, Florence, Italy. Association for Compu-
tational Linguistics.
Lingjun Zhao, Khanh Nguyen, and Hal Daumé III. 2023.
Hallucination detection for grounded instruction gen-
eration. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2023 , pages 4044–4053,
Singapore. Association for Computational Linguis-
tics.
Chunting Zhou, Graham Neubig, Jiatao Gu, Mona
Diab, Paco Guzman, Luke Zettlemoyer, and Marjan
Ghazvininejad. 2020. Detecting hallucinated con-
tent in conditional neural sequence generation. arXiv
preprint arXiv:2011.02593 .
10 Appendix
10.1 Social Perspectives on Hallucinations
The exploration of hallucination in NLP is solely
technocentric; however, its conceptual roots and
applications are deeply intertwined with societal in-
terpretations. To gain a better understanding of the
term ‘hallucination,’ it is important to consider its
broader usage and implications beyond NLP. Hal-
lucination has been studied across disciplines like
psychology and neurology (Steele, 2017; Legault,
2020). Essentially, hallucinations involve “percep-
tions arising in the absence of any external reality –
seeing or hearing things that are not there” (Steele,
2017). Although a version of this definition is com-
monly used in NLP, often with negative connota-
tions, hallucinations have a wide scope, originating
from fields such as neurology, and philosophy.
Hallucination and Medicine: Hallucination is
believed to have neurological origins, often emerg-
ing from induced states such as drug usage, psy-
chosis, sensory deprivation, or migraines (Legault,
2020). These experiences can encompass vari-
ous sensory modalities like auditory, visual, ol-
factory, tactile, gustatory, or somatic sensations
(Boeving, 2020). Modern neurological research
like Legault (2020) suggests that while hallucina-
tions may not align with external reality, they are
linked to brain regions responsible for processing
perceptions from the external world.
Hallucination and Creativity: Studies explor-
ing hallucination in the context of creativity sug-
gest that individuals with mild hallucinatory ex-
periences may demonstrate enhanced generative
creativity (Fink et al., 2014; Mason et al., 2021).Another prevalent notion is the use of hallucina-
tions as a gateway to accessing intuition, creativity,
and novel modes of thinking (Mason et al., 2021).
However, there is a call for greater empirical rigor
to establish robust connections between specific
mental states leading to hallucinations and the cre-
ative thinking process (Fink et al., 2014).
The analysis of differing perspectives on halluci-
nation reveals its diverse interpretations, challeng-
ing prevalent assumptions within NLP. However,
using the term ’hallucination’ without its social con-
text can foster misconceptions. Firstly, the ‘hallu-
cinations’ in AI systems result from discrepancies
in input data and prompts rather than an absence
of external senses. Secondly, this metaphor risks
perpetuating stigma by linking negative AI phe-
nomena with specific mental illness aspects (Pal
et al., 2023), potentially hindering destigmatiza-
tion efforts in mental health domains (Maleki et al.,
2024). Lastly, given the widespread use of machine
learning models, especially in medical fields (Ji
et al., 2023c), a limited grasp of ’hallucination’ con-
text may lead to terminology misinterpretations.
10.2 Hallucination in each NLP Task
We now analyze what aspects of the definitions of
hallucination most commonly occur within each of
our identified sub-fields of NLP3(Table 3).
Conversational AI: In this sub-field, hallucina-
tion encompasses fluency, non-factuality, and both
intrinsic and extrinsic hallucinations. The defini-
tions’ facets highlight that dialogue systems must
balance conversational fluency with factual consis-
tency, aligning both with prior conversation and
real-world truths.
Abstractive Summarization: Works in this sub-
field mainly focuses on extrinsic and intrinsic hal-
lucinations in defining it. Some definitions also
mention the faithfulness of the generation. Despite
the challenges of aligning with real-world facts and
source consistency, prioritizing alignment and ad-
herence to the original material has been shown to
be essential in these works.
Data2Text Generation: Hallucinations are clas-
sified into extrinsic and intrinsic types, similar to
abstractive summarization. Here, alignment with
the underlying data is emphasized as the more crit-
ical factor when compared to the language used in
generating the text.
3The breakdown of all the works associated with each of
the subfields is in our Appendix .Application Definitions Frequency
Conversational AIAI systems designed for natural language conversations,
understanding inputs, and generating appropriate responses38
Abstractive SummarizationGenerating concise summaries by preserving main ideas
and context, often creating new sentence16
Data2Text GenerationAutomatically converting structured data into human-
readable text, used in reporting and narratives14
Machine TranslationAutomatically translating text between languages using
computational methods like neural networks12
Image and Video CaptioningGenerating descriptive captions for visual content, aiding
accessibility and understanding8
Data AugmentationTechniques to increase data diversity and quality, improving
model performance individual aspects of an entity8
MiscellaneousEncompasses additional non-accomodated tasks like natural
language inference and factuality detection7
Table 3: Frameworks of Sentiment and corresponding definitions in Sentiment Analysis
Machine Translation: Definitions of hallucina-
tion predominantly concentrate on extrinsic hallu-
cination, with rare mentions of intrinsic hallucina-
tions. This observation suggests a lesser concern
for stylistic nuances in text generation within this
field, with a greater emphasis on comprehending
and conveying translated content accurately.
Image and Video Captioning: Models are ex-
pected to maintain consistency with the source
while also incorporating real-world knowledge
to address gaps and apply common sense. Con-
sequently, the definition of hallucination in this
context encompasses intrinsic, extrinsic, and non-
factual elements, highlighting these requirements.
Data Augmentation: : Works from this domain
often omit explicit definitions of hallucination, in-
dicating a divergence in emphasis or a nascent ex-
ploration of this construct within this sub-field.
Miscellaneous: Encompassing tasks such as lan-
guage inference and factuality detection, this cat-
egory’s definitions of hallucination encompass as-
pects like factuality, intrinsic and extrinsic halluci-
nation, fidelity, and nonsensicality. It’s evident that
within these subfields, hallucination addresses both
the stylistic aspects of model output and the fidelity
and accuracy of generated content.
From the analysis of different subfields, it is evi-
dent that each perceives hallucination differently,
emphasizing specific attributes such as factuality,
fidelity, or linguistic styles like confidence, while
potentially overlooking others. This diversity indi-
cates that hallucination as a concept is still in its
early stages in the field, with various frameworks
emerging and a general lack of consensus regard-
ing its definition and application. Furthermore, the
lack of social aspects in hallucination discussions
in these subfields contrasts with the broader under-standing and research in fields like healthcare.
10.3 Supplementary Survey Analysis
10.3.1 Weaknesses of LLM
Before delving into inquiries about hallucinations
in LLMs, it is crucial to gain insights into the per-
ceived weaknesses of these models from the par-
ticipants’ perspective, as well as understand how
frequently they utilize these models in their work.
The survey results indicate that a significant por-
tion of researchers heavily utilize LLMs in their
daily life. Specifically, 67.28% of respondents re-
ported using these models atleast once a day, while
20.37% mentioned using them all the time, high-
lighting the ubiquity of these models.
Upon analyzing the themes derived from partici-
pants’ responses on the weaknesses of generative
AI tool s, it was observed that a substantial majority
(55%) of researchers perceive the main weaknesses
to be the generation of misinformation and halluci-
nations, despite both phenomena being essentially
similar in nature. For instance,
“I have been exploring these models to see
what they get right and wrong. They get a lot of
things wrong – what some people call “hallucina-
tions”.” —Emeritus Professor, NLP
Some of the other important weaknesses men-
tioned by the respondents are: biases, not following
the prompts correctly, complex language, and not
having a long memory. For example,
“They produce a lot of inaccurate replies with
great confidence. These models also tend to be very
biased toward many socio-demographic groups.” —
Graduate student, GenerativeAI
“It is hard to distinguish whether the information
provided by them is accurate or not. Sometimes,
the models generate text with reasoning makingit sound convincing enough to be true - but ends
up being incorrect ultimately.” —Industry, Genera-
tiveAI
The responses highlight a critical concern within
the research community regarding the reliability
and accuracy of outputs generated by LLMs, with
potential implications for various applications and
domains, providing us with a strong motivation
behind this study.
The widespread use of LLMs, particularly promi-
nent models such as GPT 3, 3.5, and 4, highlights
their importance and impact on research and in-
dustry practices. However, it’s noteworthy that re-
spondents also mentioned other LLM models that
they use or are familiar with. These include Mistral,
BERT, LLaMA2, Midjourney, ClaudeAI, Gemini,
Vicuna, t5, Falcon, PaLM, Imagen, Dolly, Perplex-
ity, among others.
10.3.2 Social Ramifications of Hallucination
Participants were prompted to explain the effects of
hallucination on their work/daily life. The resulting
themes, from our qualitative analysis of their inputs,
are outlined below:
Not Good for Education: Respondents raised
concerns about the extensive use of these mod-
els by students for homework, indicating potential
negative impacts on their performance and learning
abilities. The respondents believe that such reliance
on these models can lead to a degradation in stu-
dents’ learning. Additionally, respondents express
skepticism about the suitability of these models for
checking homework assignments.
“I don’t actually use AI for my work; I just want
to be aware of what it can do because my students
are probably using it for their homework. It could
have an impact on students’ mastery of the mate-
rial.” —Associate Prof, Biotechnology
Not Good for Scholarly Work: Several respon-
dents noted that these models are not effective for
scholarly purposes, citing instances where the mod-
els generated information that was not present in
the original paper. They express concerns that if
researchers rely on these models for tasks like liter-
ature summarizing, it could lead to a deterioration
in scholarly processes. For example:
“They tend to generate a lot of misinformed facts
about certain groups or cultures that I have seen
happen often. They also generate ’facts’ from schol-
arly works where the papers would not have men-
tioned the same.” —Graduate student, NLP
Struggle with Code Generation: The modelswere deemed inefficient for code generation by mul-
tiple respondents, often producing code that lacks
utility due to hallucinations. Respondents high-
lighted mismatches between the generated code
and its intended purpose, emphasizing the need for
thorough review before utilization. Various con-
cerns were raised, including the loss of context dur-
ing prolonged interactions, inaccuracies in complex
coding tasks leading to erroneous outputs, fabrica-
tion of functions or attributes, inaccuracies in both
code and associated theoretical concepts, neces-
sitating extensive debugging and corrections, and
a tendency to cycle back to previously incorrect
suggestions despite error notifications.
“I was asking an AI to generate me a piece
of code. It ended up picking some code from one
website and some from another and combining it.
However those two websites (they were linked by
chatgpt) we’re using different versions of the li-
brary so the resulting code couldn’t be executed.” —
Industry, Network and Security
Increase in Time for Task: A common senti-
ment among respondents is that these models fre-
quently produce errors or false information, result-
ing in potential time wastage. While they acknowl-
edge occasional helpfulness, there’s a consensus
that reliance on these models can often lead to unfa-
vorable outcomes, particularly when verifying out-
puts. This dependency on verification contributes
to increased task duration, adding extra work and
time toward the project’s conclusion, as noted by
several respondents.
“I use GPT API to conduct analysis for some
of my work and accuracy and consistency would
be good in my context, and I have to find ways
to finetune it before I can trust the results of the
analysis, which added more work on my end.” —
Graduate Student, HCI
Misleading and Distrust: Generating incorrect
outputs with confidence can lead to the dissemina-
tion of non-existent knowledge, such as mislead-
ing information in the literature that may confuse
individuals with incorrect concepts. Most of our
respondents mentioned this concern. Moreover, it
poses challenges in differentiating between accu-
rate AI responses and hallucinations, particularly
for users lacking expertise in the relevant subject
matter.
“It leads to problems if even I do not have any
idea about the work. It is hard to differentiate if it
is a genuine output or hallucination.” —GraduateStudent, Data Science
Figure 5: Frequency of Text Generation Model Usage
10.3.3 An External Viewpoint
Additionally, our survey of 51 researchers who do
not specialize in AI revealed that all except 3 have
used text-generation models like various versions
of ChatGPT. Despite their fields not being directly
related to AI, a significant number integrate these
tools into their workflow, with 19.6% using them
multiple times daily and 11.76% using them several
times per hour. Their extensive usage has allowed
them to identify several limitations in the models;
they are: mathematical inaccuracy, outdated infor-
mation, misinformation, poor performance with
complex tasks and creative thinking, lack of speci-
ficity in-depth, overconfidence, lack of transparency,
bias, andirrelevant responses.
Based on the definitions provided, it is observed
that there is a lack of clarity among the respon-
dents regarding what constitutes a ‘hallucination’
in generative AI models, with perspectives vary-
ing widely. Thematic analysis of their responses
indicates that the predominant view associates
’hallucination’ with the generation of nonfactual
content and misinformation by AI systems . That
means these models are generating facts that are
not real and misleading. The remaining themes are
factually incorrect, biased outputs, incompleteness,
misinformation with confidence , and nonsensical
but good-looking texts .
The results demonstrate the unclear comprehen-
sion and significance attributed to hallucination in
LMs beyond the field of NLP and AI. There is
a pressing need to enhance public understanding
of the concept of hallucination, emphasizing its
meaning and strategies for mitigation. Given the
increasing prominence of LMs as sociotechnical
systems (Narayanan Venkit, 2023), it is crucial tograsp their social interactions and potential societal
ramifications.
10.3.4 Additional Impacts and Concerns
We analyzed perceptions when participants were
asked about any additional concerns during the
survey. Participants emphasized the necessity for
greater control and more nuanced mechanisms to
address and manage AI hallucinations effectively.
Presently, the detection and rectification of halluci-
nations rely heavily on meticulous human review,
highlighting the need for tools designed specifi-
cally to identify and mitigate such occurrences.
The presence of hallucinations can significantly
impact the credibility and acceptance of genera-
tive models among the general public. These issues
arise due to the inherent limitations of generative
algorithms and the absence of access to real-time
external knowledge.
Transparency regarding the limitations of gener-
ative AI is deemed essential through our findings,
and user education is seen as a key factor in mit-
igating risks associated with the unchecked use
of AI-generated content, as the responsibility for
identifying hallucinations often falls on the user.
While inaccuracies in non-critical applications, like
movie suggestions, may be tolerable, according to
our survey, they are deemed crucially problematic
in contexts such as business decision-making, law,
or health (Dahl et al., 2024).
10.4 Survey Questions
In this section, we provide the content and the ques-
tions that were presented in the survey:
Survey Title: Insights of Usage and Issues with
Text Generative Models and Tools
1.How did you receive the survey? (Social Me-
dia Posts, Direct email, Direct messages, Oth-
ers)
2.What is your current country of residence?
(Open-ended)
3.What sector do you associate with?
(Academia, Industry, Others)
4.What is your field of expertise? (Open-ended)
5.Does your research work directly involve
studying or developing Artificial Intelligence
(AI)? (Yes, No)6.How often do you use Text generation models
(like ChatGPT/Gemini)? (All the time, Several
times an hour, once an hour, several times a
day, Once a day, Several times a week, once a
week, Several times a month, Once a month,
Never)
7.Which text generation models have you used,
if any? (Open-ended)
8.What weaknesses do you perceive in the mod-
els that you have used(if any)? (Open-ended)
9.Are you familiar with the concept of ’hallu-
cinations’ in AI-generated text? (Extremely
familiar, Very familiar, Moderately familiar,
Slightly familiar, Not at all familiar)
10.What, according to you, is ’hallucination’ in
generative AI models? (Open-ended)
11.Do you consider ’hallucinations’ to be a weak-
ness when using these models? (Yes, No)
12.How frequently do you encounter that text
generation models produce ’hallucinated’ con-
tent that is factually incorrect or unrelated to
the input? (Very frequently, frequently, Occa-
sionally, rarely never)
13.If you have an alternate term in mind to
describe the phenomenon instead of ’hallu-
cination’ (e.g., fabrications, confabulations,
etc.), kindly specify it along with an explana-
tion(Mention NA if none). (Open-ended)
14.Can you provide an example where a halluci-
nation in text generation had or can have an
impact on your work (Mention NA if None)?
(Open-ended)
15.Do you have any additional comments or in-
sights regarding the hallucination? (if any)
(Open-ended)
10.5 Works and Application
We illustrate the examples and categories of works
that were looked into for understanding the various
applications of hallucinations. We categorize the
research on hallucinations into 7 major categories.
The definitions and categories of all the applica-
tions are mentioned in Table 3.
Abstractive Summarization: Zhang et al.
(2019); Son et al. (2022); Maynez et al. (2020);Choubey et al. (2023); Cao et al. (2021); Mar-
furt and Henderson (2022); Akani et al. (2023);
van der Poel et al. (2022); Chen et al. (2023b);
Dong et al. (2022); Shen et al. (2023); Nan et al.
(2021); Chen et al. (2021); Ladhak et al. (2023);
Nan et al. (2021); Flores and Cohan (2024)
Conversational AI: Liu et al. (2022); Zhou et al.
(2020); Ji et al. (2023b); Zhang et al. (2023b);
Yang et al. (2023); Das et al. (2022); Bouyamourn
(2023); Sun et al. (2023); Sadat et al. (2023); Slo-
bodkin et al. (2023); Ramakrishna et al. (2023);
Xiao and Wang (2021b); Shuster et al. (2021);
Nie et al. (2019); Longpre et al. (2021); Dziri
et al. (2022); Maheshwari et al. (2023); Ladhak
et al. (2022); Xu et al. (2023); Chen et al. (2023a);
Goldberg et al. (2022); Sundar and Heck (2023);
Roller et al. (2021); Mielke et al. (2022); Roller
et al. (2021); Massarelli et al. (2020); Weller et al.
(2024); Smith et al. (2022)
Data Augmentation: Jian et al. (2022); Ji et al.
(2023b); Friedl et al. (2021); Samir and Silfver-
berg (2022); Anastasopoulos and Neubig (2019);
Narayanan Venkit et al. (2023)
Image and Video Captioning: Xiao and Wang
(2021b); Dai et al. (2023); Rohrbach et al. (2018);
Li et al. (2023); Testoni and Bernardi (2021); Son
et al. (2022); Dai et al. (2023); Li et al. (2023); Liu
and Wan (2023)
Machine Translation: Wang and Sennrich
(2020); Raunak et al. (2021); Dale et al. (2022);
Guerreiro et al. (2023a); Xu et al. (2023); Pfeiffer
et al. (2023); Guerreiro et al. (2023b); Dale et al.
(2023); Irvine and Callison-Burch (2014); Ferrando
et al. (2022); Vu et al. (2022); Müller et al. (2020);
Waldendorf et al. (2024)
Data2Text Generation: González Corbelle et al.
(2022); Shi et al. (2023); Yoon et al. (2022); Fil-
ippova (2020b); Kothyari et al. (2023); Lango and
Dusek (2023); Cirik et al. (2022); Fei et al. (2023);
Obaid ul Islam et al. (2023); Qiu et al. (2023);
Testoni and Bernardi (2021); González-Corbelle
et al. (2022); Islam et al. (2023); Polat et al. (2023)
Miscellaneous: Manakul et al. (2023); Ji et al.
(2023d); Maharaj et al. (2023); McKenna et al.
(2023); Pal et al. (2023); Zhao et al. (2023); Berba-
tova and Salambashev (2023); Wu et al. (2024)