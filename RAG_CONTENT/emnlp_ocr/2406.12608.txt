Bridging Local Details and Global Context in Text-Attributed Graphs
Yaoke Wang1*, Yun Zhu1*, Wenqiao Zhang1†, Yueting Zhuang1,
Yunfei Li2,Siliang Tang1
1Zhejiang University,2Ant Group
{wangyaoke, zhuyun_dcd, wenqiaozhang, yzhuang}@zju.edu.cn
qixiu.lyf@antgroup.com, siliang@zju.edu.cn
Abstract
Representation learning on text-attributed
graphs (TAGs) is vital for real-world applica-
tions, as they combine semantic textual and con-
textual structural information. Research in this
field generally consist of two main perspectives:
local-level encoding and global-level aggregat-
ing, respectively refer to textual node informa-
tion unification ( e.g., using Language Models)
and structure-augmented modeling ( e.g., using
Graph Neural Networks). Most existing works
focus on combining different information lev-
els but overlook the interconnections, i.e., the
contextual textual information among nodes,
which provides semantic insights to bridge lo-
cal and global levels. In this paper, we propose
GraphBridge, a multi-granularity integration
framework that bridges local and global per-
spectives by leveraging contextual textual infor-
mation, enhancing fine-grained understanding
of TAGs. Besides, to tackle scalability and effi-
ciency challenges, we introduce a graph-aware
token reduction module. Extensive experi-
ments across various models and datasets show
that our method achieves state-of-the-art perfor-
mance, while our graph-aware token reduction
module significantly enhances efficiency and
solves scalability issues. Codes are available at
https://github.com/wykk00/GraphBridge
1 Introduction
Text-Attributed Graphs (TAGs), characterized by
the association of nodes with text attributes (Yang
et al., 2021), are prevalent in diverse real-world con-
texts. In TAGs, nodes represent entities with tex-
tual information and edges capture relationships be-
tween entities, e.g., social graphs where each user is
accompanied by a textual description and paper ci-
tation graphs where textual content is linked to each
respective paper. These relationships yield special-
ized and crucial insights that are fundamental for
*These authors contributed equally to this work.
†Corresponding Author
(ii) Global -level Aggregating
… I am outgoing  and 
I’d like sport  climbing  
during  on vocation  …
(i) Local -level Encoding 
Encoding
… I am outgoing  and I’d like sport  
climbing  during  on vocation  …
… I like adventurous  and exciting  
outdoor  activities  like scuba  diving…A social network 
where nodes 
symbolize users' self -
introductions, and 
edges represent 
friendships between 
them.
(iii) Local -global IntegrationUser
Node Features
Graph
Bridge
Contextual textual semantics Figure 1: Illustration of the local-global integration in
TAGs within a social network context. The words in
pink emphasize the interconnection semantic relation-
ship between them. ( i) The local-level encoding module
processes individual nodes’ textual information into uni-
fied vectors; ( ii) The global-level aggregating module
enhances node features with structural information; ( iii)
Our method bridges these two perspectives through in-
corporating contextual textual information.
our understanding, thereby facilitating the resolu-
tion of subsequent tasks. The utilization of TAGs
empowers us to unlock new discoveries across var-
ious domains, including graph learning (Zhang
et al., 2024a) and information retrieval (Seo et al.,
2024).
The nucleus of learning on TAGs lies in the
effective integration of both the node attributes
(textual semantics) and graph topology (structural
connections) to facilitate the learning of node rep-
resentations. Broadly, previous methods can be
divided into two modules: ( i) encoding module
(local-level) and ( ii) aggregating module (global-
level). The encoding module transcribes the textual
information (tokens) of each node into a unified
vector employing static shallow embedding tech-
niques such as Bag of Words (Zhang et al., 2010),
or language models (LMs) like BERT (Devlin et al.,
2018), serving as node attributes. The aggregat-
ing module enhances these features via structural
information, procuring structure-augmented fea-
tures through Graph Neural Networks (GNNs) like
GCN (Kipf and Welling, 2016). These two mod-arXiv:2406.12608v2  [cs.CL]  14 Oct 2024ules can be integrated into cascading (Duan et al.,
2023; Chien et al., 2021), joint (Zhao et al., 2022)
or side structure (Zhu et al., 2024), as illustrated in
Figure 1. Despite promising, the aforementioned
local-global integration suffers from the discrete
interconnection of encoding and aggregating mod-
ules, i.e., the contextual textual semantics among
nodes are overlooked (Figure 1( iii)). In real-life
scenarios, e.g., social networks, the closely con-
nected individuals are more likely to share substan-
tial semantically textual information in text, which
serves as the common characteristics for construct-
ing their relationships.
Based on the aforementioned insight, one opti-
mizable TAG learning solution is to leverage such
contextual textual information that could effec-
tively bridge local and global perspectives, thereby
boosting fine-grained understanding of TAGs, like
Figure 1( iii). However, this method faces severe
efficiency and scalability issues. Memory com-
plexity increases with graph size, as neighborhood
texts are also encoded. Using Large Language
Models (LLMs) with densely connected nodes fur-
ther exacerbates resource consumption, potentially
impairing TAG’s practicality. In summary, these
shortcomings necessitate a thorough reevaluation
of TAG learning and its corresponding solutions.
In this work, we introduce a novel multi-
granularity integration framework for text-
attributed graphs, named GraphBridge, which
seamlessly bridges local and global perspectives by
incorporating contextual textual information. This
method enhances semantic analysis and provides
deeper graph structure insights, significantly im-
proving representation learning. Additionally, to
address the efficiency and scalability issues men-
tioned above, we developed a graph-aware token
reduction module. This module uses a learnable
mechanism that considers both the graph structure
and downstream task information to selectively
retain the most crucial tokens, reducing informa-
tion loss and allowing for the inclusion of more
contextual text. Extensive experiments show that
our method achieves state-of-the-art performance
across various domains compared to previous meth-
ods, while solving the efficiency and scalability
issues. Key contributions of this work include:
•We propose an innovative multi-granularity
integration framework named GraphBridge to
integrate both local and global perspectives
through leveraging contextual textual infor-mation, thereby enhancing the fine-grained
understanding of TAGs.
•A graph-aware token reduction module is de-
signed to ensure efficiency and scalability
while minimizing information loss.
•Extensive experiments conducted across var-
ious domains demonstrate that our proposed
method achieves state-of-the-art performance
compared to various baselines, demonstrating
its effectiveness in bridging the gap between
local and global information, while maintain-
ing efficiency and scalability.
2 Related Work
2.1 Representation Learning on TAGs
Representation learning for text-attributed graphs
has increasingly garnered attention in graph ma-
chine learning (Yang et al., 2021). Typically, pre-
vious methods in this field can be divided into
two key components, as depicted in Figure 1: ( i)
an encoding module at the local level, which em-
ploys word embedding methods such as Bag of
Words (Zhang et al., 2010) or advanced LMs like
BERT (Devlin et al., 2018) and RoBERTa (Liu
et al., 2019) to generate token representations from
nodes’ textual data. These representations are inte-
grated using methods like mean pooling to derive
nodes’ attributes; ( ii) an aggregating module at
the global level, which utilizes GNNs (Kipf and
Welling, 2016; Veli ˇckovi ´c et al., 2018) or graph
transformers (Wu et al., 2022) to augment nodes’
attributes with structural information. The encod-
ing module concentrates on extracting fine-grained
semantic details from textual attributes individu-
ally, whereas the aggregating module emphasizes
structural relationships between nodes, neglecting
the intricate local textual information.
Recent advancements aim to effectively integrate
these two modules. Integration strategies include
joint frameworks (Yang et al., 2021; Zhao et al.,
2022) and side structures (Zhu et al., 2024), as well
as cascading approaches (Duan et al., 2023; He
et al., 2023). However, these integration strategies
fail to explicitly capture the interconnection be-
tween the encoding and aggregating modules, i.e.,
the contextual textual semantics among nodes are
frequently overlooked, potentially compromising
the efficacy of the results.2.2 Token Reduction for LMs
Sequence length has become a significant fac-
tor limiting the scalability of transformer mod-
els (Vaswani et al., 2017). Token reduction has
gained considerable research interest because it
can reduce computational costs by decreasing se-
quence length. The general idea of token reduc-
tion is to drop some tokens based on their im-
portance (Xu and McAuley, 2023). Specifically,
DynSAN (Zhuang and Wang, 2019) applies a gate
mechanism to measure the importance of tokens for
selection, dropping less important tokens in higher
layers to enhance efficiency. TR-BERT (Ye et al.,
2021) introduces a dynamic reinforcement learn-
ing mechanism for making decisions of reducing
tokens. LTP (Kim et al., 2022) learns a threshold
for each Transformer layer, dropping tokens with
a saliency score below this threshold instead of
adhering to a preset token reduction schedule.
Although token reduction has proven successful
in streamlining individual sentences, its applica-
tion to interrelated texts within TAGs has yet to be
fully explored. In this work, we propose a graph-
aware token reduction module that leverages the
graph structure along with the information from
downstream tasks to perform token reduction.
3 Method
In this section, we will introduce the notations used
in this paper. Subsequently, we will present the
proposed graph-aware token reduction method in
Section 3.2. Finally, the multi-granularity integra-
tion framework will be discussed in Section 3.3.
3.1 Notations
When dealing with node classification tasks of
TAGs, we formally consider a text-attributed graph
G={V,T,A,Y}, where Vis a set of nodes,
T ∈R|V|×kdenotes the textual features associ-
ated with each node i∈ V, and kis the sequence
length. A ∈ { 0,1}|V|×|V|is the adjacency matrix
where each entry Ai,jindicates the link between
nodes i, j∈ V, andYrepresents labels for each
node. Given a set of labeled nodes VL⊂ V, our
goal is to predict the remaining unlabeled nodes
VU=V \ V L.
3.2 Graph-Aware Token Reduction
To bridge local and global perspectives for TAGs, it
is essential to consider both the text of the currentnode and its neighboring nodes. This approach,
however, leads to extremely long sequences that
can result in prohibitive computational costs for
LM processing. To mitigate this, we first imple-
ment a graph-aware token reduction module. For-
mally, for an input graph G, the text of each node Ti
is initially tokenized, resulting in si∈Rk, where
kis the number of tokens. Our objective is to re-
duce the number of tokens to k′(where k′≪k),
focusing on retaining the most pivotal tokens while
omitting the lesser ones. Specifically, we assess the
importance of each token for node ibased on its
textual and structural information:
P(Score i| Ti,TNi,A), (1)
where Score i∈R1×kis the importance score for
each token in node i, andNidenotes the neighbor-
ing nodes of i. The importance score is calculated
by a trainable graph-enhanced attention module, as
depicted in Figure 2 (Stage 1).
For better evaluating the importance of each to-
ken, we first use Pre-trained LMs like BERT and
RoBERTa to obtain fine-grained representations for
each token. For node i, these representations are
represented as Ei∈Rk×d, consisting of vectors
[e0, e1, . . . , e k], where each ejis ad-dimensional
token representation from the PLM. We subse-
quently employ mean pooling Pmeanon these token
representations to extract the sentence-level textual
feature, which serves as the node attribute:
zi=Pmean(Ei) =1
kkX
jej. (2)
Then, a parameter-free message-passing mecha-
nism is employed to aggregate text features from
neighboring nodes, excluding self-loops to avoid
reinforcing a node’s own information. This en-
sures the integration of contextual information from
neighbors, enhancing node features with structural
insights from graph. This process is described as:
z(l)
i=1
|Ni|X
j∈Niz(l−1)
j, (3)
where lmeans l-hop message passing, and z0
iiszi.
Graph-Enhanced Importance Score. To mea-
sure the importance of each token, we define
a graph-enhanced importance score calculated
through a well designed cross-attention module,GNN
Cross Attention
Language Model for Reduction
…0.3
0.1
0.2Top 𝑘…
………Local Pooling & MP
…Selected 
Tokens
Tokens
Features.Loss
Language Model
BERT RoBERTa
Llama…
… … … … SEP SEP SEP
Node 1 Node 2 Node 3 Node 4
Token Reduction
Token 
Importance
KeyGlobal Pooling Query
Value
Stage 1Stage 2
Gemini
: Trained
: Frozen.: Weighted Sum: Message Passing MPFigure 2: Overview of the GraphBridge framework. Left: The Graph-Aware Token Reduction module which
selectively retains crucial tokens, enhancing efficiency and scalability. Right : A detailed pipeline illustrates the
integration process, where selected tokens undergo a cascaded structure that bridges local and global perspectives,
leveraging contextual textual information to effectively refine node representations.
that quantifies the significance of each token utiliz-
ing both textual and structural information. Specif-
ically, for each node i, the query is derived from
the message passing output of neighboring nodes,
while the key and value come from the node’s own
textual token embedding. The importance score is
calculated as follows:
Score i=σ

z(l)
iWq
(EiWk)T
√
d
,(4)
where Wq, Wk∈Rd×d′are the parameter matrices
for the query and key, and σdenotes the softmax
function. A top-k function is then used to select the
k′most crucial tokens.
Optimizing Importance Score. The supervisory
signals derived from downstream tasks provide
valuable guidance, allowing the attention module to
select informative tokens more effectively. Specifi-
cally, during the training phase, we aggregate the
token representations using Score i, which can be
formulated as:
si=Score iEi, (5)
where si∈R1×ddenotes the weighted summation
of text features using attention score. Observe that
Eican be directly regarded as the value matrix,because we set the value parameter matrix as the
identity matrix Ihere for efficiency. Finally, siis
fed into a linear classifier Cfor prediction. The
training loss Ldown is computed using the cross-
entropy loss CE(·,·)between the prediction and
true label for the target node i:
Ldown=Ei∈VLCE(ˆyi|Ci(si), yi). (6)
Note that, only the attention module and the classi-
fier are trained while keeping the PLM frozen for
efficiency.
Regularization. Through our empirical study,
we discovered that the importance score for the ma-
jority of nodes deteriorates when solely optimizing
Ldown, as depicted in Figure 3. We hypothesize that
this phenomenon is due to overfitting on the limited
set of training nodes. Consequently, most nodes
tend to converge on a single token with an exces-
sively high importance score ( e.g., 0.99), thereby
hindering the selection of multiple informative to-
kens and inhibiting exploration. To mitigate this
phenomenon, we introduce a regularization term.
This term penalizes the network when certain to-
kens receive disproportionately high importance
scores, achieved with a KL-divergence loss:
Lreg=Ei∈VLDKL(U||Score i), (7)
where Uis an uniform distribution.0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Importance Score05001000150020002500300035004000Number of Nodes
Without Regularization
With RegularizationFigure 3: Selecting the highest score token for each node
in WikiCS dataset, with and without regularization. The
x-axis means the highest importance score of token for
each node, the y-axis indicates the number of nodes
corresponding to each importance score.
The overall training loss of the reduction model
is as follows:
Ltrain=Ldown+βLreg, (8)
where βis the regularization parameter for control-
ling the distribution of importance score. A larger
βresults in a more uniform score distribution, and
vice visa.
3.3 Multi-Granularity Integration
As discussed in Section 1, the integration of local-
global perspectives is essential for representation
learning in TAGs (Yan et al., 2023). However, pre-
vious methods (Duan et al., 2023; He et al., 2023)
often fail to adequately address the interconnection
between these two perspectives. In this work, we
propose an innovative multi-granularity integration
framework that bridges local and global perspec-
tives by considering contextual textual semantics
among nodes, as illustrated in Figure 2 (Stage 2).
Intuitively, the contextual textual semantics among
nodes can offer supplementary information for a
given node. For example, within a citation network,
the textual content of neighboring nodes might in-
clude key terms or concepts pertinent to the target
node.
To incorporate contextual information, we con-
sider both the text of each node and its neighbors.
For each node i, we concatenate its own text with
the text of its neighboring nodes into a single se-
quence Qi:
Qi= (t1
i,···, tk′
i
i,[SEP], t1
j1,···, tk′
n
jn),(9)where tk′
i
irepresents the tokens in the node i, [SEP]
is a separator token for separating different nodes,
and{j1,···, jn} ∈ N i. Note that concatenating
the text of multiple nodes results in an excessively
long sequence, which results in efficiency and scal-
ability issues. Therefore, we utilize the token reduc-
tion module described in Section 3.2 to select the
most crucial k′tokens for forming the sequence.
After constructing the sequence Qifor each tar-
get node i, we train the language model LM(·),
which serves as the encoding module, on this se-
quence to obtain embeddings enriched with both
textual and contextual information:
LLM=Ei∈VLCE(ˆyi|C(LM ( Qi)), yi).(10)
It is noteworthy that sampling fewer neighbor-
ing nodes focuses the model more on the fine-
grained semantic information from a local perspec-
tive, whereas sampling more neighboring nodes
shifts the model’s emphasis towards capturing the
structural semantic information from a global per-
spective.
After completing the training of the LM, the
model is used to produce node representations H.
Subsequently, we train the aggregating module
GNN( ·)as follows:
LGNN=Ei∈VLCE(ˆyi|C(GNN( A, H)i), yi),(11)
where the aggregating module GNN will generate
node features that further reflect the structural se-
mantics from a global perspective.
Additionally, training the GNN and LM is fully
decoupled, allowing for the use of any existing
GNN and LM models. This cascading structure en-
ables the independent optimization of each model,
enhancing flexibility and facilitating integration
with diverse architectures and applications.
4 Experiments
In this section, we first introduce the used datasets
in Section 4.1. We then detail the baseline methods
and experimental setup in Sections 4.2 and 4.3, re-
spectively. Experiments are presented to evaluate
our proposed method in Section 4.4. We further
investigate the use of a causal large language model
as the backbone in Section 4.5. Finally, we provide
an analysis of hyper-parameters, assess scalabil-
ity and efficiency, and conduct an ablation study.
Furthermore, additional experiments ( e.g., link pre-
diction) are in Appendix.Dataset #Nodes #Edges #Avg.tokens #Avg.degrees #Classes
Cora 2,708 5,429 194 3.90 7
WikiCS 11,701 215,863 545 36.70 10
CiteSeer 3,186 4,277 196 1.34 6
ArXiv-2023 46,198 78,543 253 1.70 40
Ele-Photo 48,362 500,928 185 18.07 12
OGBN-Products (subset) 54,025 74,420 163 2.68 47
OGBN-ArXiv 169,343 1,166,243 231 13.67 40
Table 1: Data statistics. #Nodes , #Edges , #Classes and # Avg.degrees mean the number of nodes, edges, classes
and average degrees for each dataset, respectively. # Avg.tokens represents the average number of tokens per node
in each dataset when using the RoBERTa-base’s tokenizer.
4.1 Datasets
In this work, we adopt seven widely used tex-
tual graphs to evaluate our proposed GraphBridge:
Cora (Sen et al., 2008), WikiCS (Mernyei and
Cangea, 2020), CiteSeer (Giles et al., 1998), ArXiv-
2023 (He et al., 2023), Ele-Photo (Yan et al., 2023),
OGBN-Products (Hu et al., 2020) and OGBN-
ArXiv (Hu et al., 2020). The raw text of these
datasets are collected by previous works (Chen
et al., 2023; Yan et al., 2023; He et al., 2023). De-
tails of these datasets can be found in Appendix A.
4.2 Baselines
To verify the effectiveness of our proposed method,
we select several baseline models for comparison,
categorized into three types:
Traditional GNN-based methods : primarily fo-
cus on the global level but utilize static shallow em-
beddings, which neglect fine-grained textual infor-
mation, e.g., MLP, GCN (Kipf and Welling, 2016),
SAGE (Hamilton et al., 2017), GAT (Veli ˇckovi ´c
et al., 2018), NodeFormer (Wu et al., 2022).
LM-based methods : primarily focus on the lo-
cal textual level and do not consider global struc-
tural information, e.g., BERT (Devlin et al., 2018),
RoBERTa (Liu et al., 2019).
Recent works designed for TAGs : integrate both
local and global levels, e.g., GLEM (Zhao et al.,
2022), TAPE (He et al., 2023), SimTeG (Duan
et al., 2023), ENGINE (Zhu et al., 2024).
4.3 Experimental Setup
For traditional GNN-based methods, we utilize the
raw features of each dataset, which are derived us-
ing bag of words or one-hot vectors. For LM-based
methods, we fine-tune LMs with raw texts of each
node on downstream tasks. For recent TAGs meth-
ods, we select RoBERTa-base and RoBERTa-large
as the LM backbones, and a two-layer SAGE with
64 hidden size as the GNN backbone. Regardingto our method, we select the same LM and GNN
backbones with recent TAGs methods for a fair
comparison. Additionally, we utilize RoBERTa-
base as the LM encoder for our token reduction
module. For alternative LMs used in token reduc-
tion, please refer to Appendix D.
In our experiments, we fine-tune all parameters
for base language models such as RoBERTa-base.
For larger models like RoBERTa-large, we employ
LoRA (Hu et al., 2021) with a rank of 8 to en-
sure scalability and maintain consistency with the
SimTeG approach (Duan et al., 2023).
4.4 Main Results
From Table 2, we draw the following conclusions:
First, traditional GNN-based methods, which
rely on global structural information using static
shallow embeddings, underperform compared to
current TAGs methods like GLEM that integrate
both local and global levels information. For in-
stance, on the ArXiv-2023 dataset, this integration
results in a 12% higher absolute performance over
GNN methods such as GCN, highlighting the cru-
cial role of local textual information in enhancing
model efficacy.
Second, LM-based methods primarily focusing
on local textual information fall short on TAGs,
as evidenced by integration methods which uti-
lize the same LM backbones surpassing them by
about 10% absolute performance on the Ele-Photo
dataset, achieving over 80% accuracy. This high-
lights the essential role of global structural informa-
tion in creating more semantically and structurally
aware node embeddings.
Last, our method surpasses existing local and
global integration approaches for TAGs. Specifi-
cally, GraphBridge achieves an absolute improve-
ment of over 6% on the CiteSeer dataset and 4%
on the ArXiv-2023 dataset, outperforming the pre-
vious state-of-the-art method, SimTeG, across var-Methods Cora WikiCS CiteSeer ArXiv-2023 Ele-Photo OGBN-Products OGBN-ArXiv
MLP 76.12 ± 1.51 68.11 ± 0.76 70.28 ± 1.13 65.41 ± 0.16 62.21 ± 0.17 58.11 ± 0.23 62.57 ± 0.11
GCN 88.12 ± 1.13 76.82 ± 0.62 71.98 ± 1.32 66.99 ± 0.19 80.11 ± 0.09 69.84 ± 0.52 70.78 ± 0.10
SAGE 87.60 ± 1.40 76.65 ± 0.84 72.44 ± 1.11 68.76 ± 0.51 79.79 ± 0.23 70.64 ± 0.20 71.72 ± 0.21
GAT 85.13 ± 0.95 77.04 ± 0.55 72.73 ± 1.18 67.61 ± 0.24 80.38 ± 0.37 69.70 ± 0.25 70.85 ± 0.17
NodeFormer 88.48 ± 0.33 75.47 ± 0.46 75.74 ± 0.54 67.44 ± 0.42 77.30 ± 0.06 67.26 ± 0.71 69.60 ± 0.08
BERT 79.70 ± 1.70 78.13 ± 0.63 71.92 ± 1.07 77.15 ± 0.09 68.79 ± 0.11 76.23 ± 0.19 72.75 ± 0.09
RoBERTa-base 78.49 ± 1.36 76.91 ± 0.69 71.66 ± 1.18 77.33 ± 0.16 69.12 ± 0.15 76.01 ± 0.14 72.51 ± 0.03
RoBERTa-large 79.79 ± 1.31 77.79 ± 0.89 72.26 ± 1.80 77.70 ± 0.35 71.22 ± 0.09 76.29 ± 0.27 73.20 ± 0.13
GLEM (base ) 87.61 ± 0.19 78.11 ± 0.61 77.51 ± 0.63 79.18 ± 0.21 81.47 ± 0.52 76.15 ± 0.32 74.46 ± 0.27
TAPE (base ) 87.82 ± 0.91 − − 80.11 ± 0.20 − 79.46 ± 0.11 74.66 ± 0.07
SimTeG (base ) 86.85 ± 1.81 79.77 ± 0.68 78.69 ± 1.12 79.31 ± 0.49 81.61 ± 0.18 76.46 ± 0.55 74.31 ± 0.14
ENGINE (base ) 87.56 ± 1.48 77.97 ± 0.94 76.79 ± 1.38 78.34 ± 0.15 80.50 ± 0.33 77.80 ± 1.20 73.59 ± 0.14
Ours (base ) 92.14 ± 1.03 80.59 ± 0.47 85.32 ± 1.39 84.07 ± 0.34 83.84 ± 0.07 79.80 ± 0.19 74.89 ± 0.23
GLEM (large ) 89.11 ± 0.22 77.99 ± 0.72 78.24 ± 0.31 78.91 ± 0.40 82.11 ± 0.66 78.59 ± 0.27 74.98 ± 0.45
TAPE (large ) 88.56 ± 0.88 − − 80.21 ± 0.31 − 79.76 ± 0.23 75.29 ± 0.11
SimTeG (large ) 88.78 ± 1.05 80.13 ± 0.76 79.59 ± 1.56 80.51 ± 0.33 82.49 ± 0.17 78.55 ± 0.66 75.16 ± 0.21
ENGINE (large ) 88.49 ± 1.10 80.21 ± 0.29 78.02 ± 0.87 77.45 ± 0.46 82.68 ± 0.09 78.83 ± 0.80 74.62 ± 0.30
Ours (large ) 92.73 ± 1.00 80.73 ± 0.41 86.81 ± 1.09 84.79 ± 0.29 84.18 ± 0.15 80.22 ± 0.47 75.90 ± 0.11
Table 2: Experimental results of node classification : We report the mean accuracy with a standard deviation of 5
runs with different random seeds. Highlighted are the top first,second , and third results. ‘base’ and ‘large’ refer to
RoBERTa-base and RoBERTa-large as LM backbones, respectively. ‘ −’ indicates that datasets do not support for
this method.
Methods Cora WikiCS Ele-Photo
LLaMA2 82.80 ± 1.37 80.82 ± 0.48 72.06 ± 0.10
SimTeG (LLaMA2 ) 92.84 ± 0.13 82.55 ± 0.51 82.05 ± 0.17
ENGINE (LLaMA2 )91.48 ± 0.32 81.56 ± 0.97 83.75 ± 0.08
Ours (LLaMA2 ) 93.65 ± 0.44 84.18 ± 0.68 84.35 ± 0.12
Table 3: Experimental results when utilizing LLaMA2-
7B as the Large Language Model backbone. We employ
LoRA with a rank of 4 to fine-tune the LLM and report
the corresponding accuracy. We use boldface to denote
the best performace.
ious LM backbones. This demonstrates the effec-
tiveness of our approach in seamlessly integrating
local and global perspectives by incorporating con-
textual textual information among nodes, thereby
enhancing the fine-grained understanding of TAGs.
4.5 Enhanced with Large Language Models
Our method, as demonstrated in Section 4.4, proves
effective with small and medium discriminative
LMs like RoBERTa-base and RoBERTa-large. Fur-
thermore, we have expanded our method to incorpo-
rate causal Large Language Models (LLMs), which
have shown significant capabilities across various
natural language tasks (Achiam et al., 2023; Zhang
et al., 2024b,c). Table 3 presents the results ob-
tained using LLaMA2-7B (Touvron et al., 2023) as
the LLM backbone. Our method outperforms both
LM-based method ( i.e., LLaMA2) and integrationmethods ( i.e., SimTeG, ENGINE) that utilize LLM.
This demonstrates the effectiveness of our method
with the LLM backbone, highlighting the impor-
tance of bridging the local and global perspectives.
4.6 Sensitive Analysis
The number of walk steps. In the construction
of the sequence Qas outlined in Equation 9, sam-
pling neighboring nodes via a random walk with
restart (Zhu et al., 2022) is essential for effectively
incorporating contextual textual information. The
number of walk steps, a key hyper-parameter, dic-
tates the extent of neighboring node inclusion and
thus influences the breadth of contextual informa-
tion captured.
8990919293
Cora
WikiCS
CiteSeer
Ele-Photo
4 8 16 32 647880828486
Figure 4: Sensitive analysis of the number of walk steps.
Empirically, we explore the impact of this num-
ber, choosing from {4, 8, 16, 32, 64}. The results
in Figure 4 indicate that a low number of walk8990919293
Cora
WikiCS
CiteSeer
Ele-Photo
0.01 0.1 0.5 1.0 10.077.580.082.585.0
Figure 5: Sensitive analysis of regularization term β.
steps like 4, leads to insufficient contextual infor-
mation, while a high number like 64, may blur fine-
grained local information and introduce noise, neg-
atively impacting performance. To balance local
and global information effectively, an intermediate
number of steps, e.g., 16 or 32, is optimal.
The regularization term β.The regularization
process penalizes the token reduction module when
certain tokens receive extremely high importance
scores. We analyze the regularization term βas de-
scribed in Equation 8, testing various value {0.01,
0.1, 0.5, 1.0, 10.0}. Based on Figure 5, optimal
results are observed with a βvalue of 0.1. A small
or large βvalue can lead to a steep or excessively
smooth score distribution, respectively.
4.7 Scalability and Efficiency Analysis
Walk Steps Reduction Memory (GB) Total Time
8✗ 9.6 39h 14m
✓ 3.2 13h 55m
16✗ 20.1 68h 42m
✓ 4.6 18h 41m
32✗ OOM −
✓ 7.7 31h 14m
64✗ OOM −
✓ 15.2 61h 51m
Table 4: The scalability and efficiency analysis of train-
ing on the OGBN-ArXiv dataset with and without token
reduction. The batch size was set to 1 for LM tuning,
with total training time reported using a 48-core Intel(R)
Xeon(R) CPU @ 2.50GHz and 8 NVIDIA GeForce
RTX 3090 GPUs. ‘OOM’ refers to out of memory.
In this section, we assess the scalability and ef-
ficiency of our method by reducing the sequence
length through token reduction module, detailed in
Section 3.2. RoBERTa-base serves as our LM back-
bone, and for this experiment, we use the rotatory
position embeddings (Su et al., 2024) to accommo-
Cora WikiCS Ele-Photo
Different Datasets707376798285889194AccuracyGraphBridge
w/o reg
Random
TF-IDF
TruncationFigure 6: Experimental results of ablation study.
date sequences of unlimited length. Results pre-
sented in Table 4, demonstrate that without token
reduction, method which considers neighboring
textual information suffers from significant com-
putational costs in terms of training time and GPU
memory usage, and it may even run out of memory
with a large number of walk steps ( i.e., greater than
32). With our token reduction module, we only
retain the most k′crucial tokens for each nodes,
thereby enhancing efficiency and scalability.
4.8 Ablation Study
In this section, we evaluate the effectiveness of
our token reduction module. Specifically, ‘w/o reg’
refers to training the module without regulariza-
tion. The methods ‘Random’, ‘TF-IDF’, and ‘Trun-
cation’ represent three different alternative token
reduction strategies: random selection, selection
based on TF-IDF scores, and selecting the initial
tokens from texts, respectively. Figure 6 shows
that our token reduction module outperforms other
reduction methods, highlighting its effectiveness
in selecting crucial tokens. Additionally, regular-
ization is essential as it helps prevent overfitting to
specific tokens.
5 Conclusion
In this paper, we introduce GraphBridge, an in-
novative multi-granularity integration framework
for text-attributed graphs. Our method emphasizes
the importance of bridging local and global per-
spectives by incorporating contextual textual in-
formation, thereby enhancing the fine-grained un-
derstanding of TAGs. To tackle scalability and
efficiency challenges associated with handling ex-
tensive textual data, we propose a graph-aware to-
ken reduction module. Empirical studies confirmthat GraphBridge surpasses existing state-of-the-art
methods on various datasets.
6 Limitations
This work introduces a framework that seamlessly
integrates both local and global perspectives by
leveraging contextual textual information for TAGs.
However, it primarily focuses on high-level dis-
criminative tasks such as node classification and
cannot be directly applied to generative tasks like
graph description. Leveraging this framework to
construct a graph foundation model presents a chal-
lenging yet valuable area for exploration.
7 Acknowledgements
This work has been supported in part by the Key
Research and Development Projects in Zhejiang
Province (No. 2024C01106), the NSFC (No.
62272411), the National Key Research and De-
velopment Project of China (2018AAA0101900),
and Ant Group.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774 .
Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han,
Wei Jin, Haiyang Zhang, Hui Liu, and Jiliang Tang.
2023. Label-free node classification on graphs
with large language models (llms). arXiv preprint
arXiv:2310.04668 .
Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-
Fu Yu, Jiong Zhang, Olgica Milenkovic, and Inder-
jit S Dhillon. 2021. Node feature extraction by self-
supervised multi-scale neighborhood prediction. In
Proc. of ICLR .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Keyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng
Yan, Wei Tsang Ooi, Qizhe Xie, and Junxian He.
2023. Simteg: A frustratingly simple approach
improves textual graph learning. arXiv preprint
arXiv:2308.02565 .
C Lee Giles, Kurt D Bollacker, and Steve Lawrence.
1998. Citeseer: An automatic citation indexing sys-
tem. In Proceedings of the third ACM conference on
Digital libraries , pages 89–98.Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017.
Inductive representation learning on large graphs. Ad-
vances in neural information processing systems , 30.
Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam
Perold, Yann LeCun, and Bryan Hooi. 2023. Har-
nessing explanations: Llm-to-lm interpreter for en-
hanced text-attributed graph representation learning.
InThe Twelfth International Conference on Learning
Representations .
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
et al. 2021. Lora: Low-rank adaptation of large lan-
guage models. In International Conference on Learn-
ing Representations .
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong,
Hongyu Ren, Bowen Liu, Michele Catasta, and Jure
Leskovec. 2020. Open graph benchmark: Datasets
for machine learning on graphs. Advances in neural
information processing systems , 33:22118–22133.
Sehoon Kim, Sheng Shen, David Thorsley, Amir Gho-
lami, Woosuk Kwon, Joseph Hassoun, and Kurt
Keutzer. 2022. Learned token pruning for transform-
ers. In Proceedings of the 28th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining ,
pages 784–794.
Thomas N Kipf and Max Welling. 2016. Semi-
supervised classification with graph convolutional
networks. In International Conference on Learning
Representations .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Péter Mernyei and C ˘at˘alina Cangea. 2020. Wiki-cs:
A wikipedia-based benchmark for graph neural net-
works. arXiv preprint arXiv:2007.02901 .
Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Jus-
tifying recommendations using distantly-labeled re-
views and fine-grained aspects. In Proceedings of
the 2019 conference on empirical methods in natural
language processing and the 9th international joint
conference on natural language processing (EMNLP-
IJCNLP) , pages 188–197.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise
Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008.
Collective classification in network data. AI maga-
zine, 29(3):93–93.
Hyunjin Seo, Taewon Kim, June Yong Yang, and
Eunho Yang. 2024. Unleashing the potential of
text-attributed graphs: Automatic relation decom-
position via large language models. arXiv preprint
arXiv:2405.18581 .Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,
Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hanced transformer with rotary position embedding.
Neurocomputing , 568:127063.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Liò, and Yoshua Bengio.
2018. Graph attention networks. In International
Conference on Learning Representations .
Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-
Han Wu, Yuxiao Dong, and Anshul Kanakia. 2020.
Microsoft academic graph: When experts are not
enough. Quantitative Science Studies , 1(1):396–413.
Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and
Junchi Yan. 2022. Nodeformer: A scalable graph
structure learning transformer for node classification.
Advances in Neural Information Processing Systems ,
35:27387–27401.
Canwen Xu and Julian McAuley. 2023. A survey on
model compression and acceleration for pretrained
language models. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , volume 37, pages
10566–10575.
Hao Yan, Chaozhuo Li, Ruosong Long, Chao Yan,
Jianan Zhao, Wenwen Zhuang, Jun Yin, Peiyan
Zhang, Weihao Han, Hao Sun, et al. 2023. A compre-
hensive study on text-attributed graphs: Benchmark-
ing and rethinking. Advances in Neural Information
Processing Systems , 36:17238–17264.
Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo
Li, Defu Lian, Sanjay Agrawal, Amit Singh,
Guangzhong Sun, and Xing Xie. 2021. Graphform-
ers: Gnn-nested transformers for representation learn-
ing on textual graph. Advances in Neural Information
Processing Systems , 34:28798–28810.
Deming Ye, Yankai Lin, Yufei Huang, and Maosong
Sun. 2021. Tr-bert: Dynamic token reduction for ac-
celerating bert inference. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 5798–5809.
Delvin Ce Zhang, Menglin Yang, Rex Ying, and
Hady W Lauw. 2024a. Text-attributed graph repre-
sentation learning: Methods, applications, and chal-
lenges. In Companion Proceedings of the ACM on
Web Conference 2024 , pages 1298–1301.Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun
Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao
Zhou, Zheqi Lv, Hao Jiang, et al. 2024b. Hyper-
llava: Dynamic visual and language expert tuning for
multimodal large language models. arXiv preprint
arXiv:2403.13447 .
Wenqiao Zhang, Zheqi Lv, Hao Zhou, Jia-Wei Liu,
Juncheng Li, Mengze Li, Yunfei Li, Dongping Zhang,
Yueting Zhuang, and Siliang Tang. 2024c. Revisiting
the domain shift and sample uncertainty in multi-
source active domain transfer. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 16751–16761.
Wenqiao Zhang, Siliang Tang, Yanpeng Cao, Shiliang
Pu, Fei Wu, and Yueting Zhuang. 2019. Frame aug-
mented alternating attention network for video ques-
tion answering. IEEE Transactions on Multimedia ,
22(4):1032–1041.
Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu
Zhang, Andrew Makmur, Qingpeng Cai, and
Beng Chin Ooi. 2022. Boostmis: Boosting medical
image semi-supervised learning with adaptive pseudo
labeling and informative active annotation. In Pro-
ceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 20666–20676.
Yin Zhang, Rong Jin, and Zhi-Hua Zhou. 2010. Un-
derstanding bag-of-words model: a statistical frame-
work. International journal of machine learning and
cybernetics .
Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian
Liu, Rui Li, Xing Xie, and Jian Tang. 2022. Learning
on large-scale text-attributed graphs via variational
inference. In The Eleventh International Conference
on Learning Representations .
Yun Zhu, Jianhao Guo, Fei Wu, and Siliang Tang.
2022. Rosa: A robust self-aligned framework for
node-node graph contrastive learning. arXiv preprint
arXiv:2204.13846 .
Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang.
2024. Efficient tuning and inference for large lan-
guage models on textual graphs. arXiv preprint
arXiv:2401.15569 .
Yimeng Zhuang and Huadong Wang. 2019. Token-level
dynamic self-attention network for multi-passage
reading comprehension. In Proceedings of the 57th
annual meeting of the association for computational
linguistics , pages 2252–2262.A Datasets
We evaluated our method using seven widely rec-
ognized text-attributed graph datasets. The details
of these datasets are as follows:
Cora (Sen et al., 2008) dataset contains 2,708
scientific publications classified into seven classes:
case-based, genetic algorithms, neural networks,
probabilistic methods, reinforcement learning, rule
learning, and theory. Each paper in the citation
network cites or is cited by at least one other paper,
resulting in a total of 5,429 edges.
WikiCS (Mernyei and Cangea, 2020) dataset
is a Wikipedia-based dataset designed for bench-
marking Graph Neural Networks, consisting of 10
computer science branches as classes with high
connectivity. Node features are derived from the
corresponding article texts *.
CiteSeer (Giles et al., 1998) dataset comprises
3,186 scientific publications categorized into six
areas: Agents, Machine Learning, Information Re-
trieval, Database, Human Computer Interaction,
and Artificial Intelligence, with the task of classify-
ing each paper based on its title and abstract.
ArXiv-2023 dataset, introduced in TAPE (He
et al., 2023), is a directed graph representing the
citation network of computer science arXiv papers
published in 2023 or later. Similar to OGBN-ArXiv,
it features nodes representing arXiv papers and di-
rected edges for citations. The goal is to classify
each paper into one of 40 subject areas such as
cs.AI, cs.LG, and cs.OS, with classifications pro-
vided by the authors and arXiv moderators.
Ele-Photo (Yan et al., 2023) dataset, derived
from the AmazonElectronics dataset (Ni et al.,
2019), consists of nodes representing electron-
ics products, with edges indicating frequent co-
purchases or co-views. Each node is labeled ac-
cording to a three-level classification of electronics
products. The text attribute for each node is the
user review with the most votes, or a randomly se-
lected review if no highly-voted reviews are avail-
able. The task is to classify these products into 12
categories.
OGBN-Products (Hu et al., 2020) dataset, com-
prising 2 million nodes and 61 million edges,
is reduced using a node sampling strategy from
TAPE (He et al., 2023) to create the OGBN-
Products (subset) with 54k nodes and 74k edges.
Each node represents an Amazon product, with
*We obtain the raw texts of each node from
https://github.com/pmernyei/wiki-cs-dataset.edges denoting co-purchases. The classification
task involves categorizing products into one of 47
top-level categories.
OGBN-ArXiv dataset is a directed graph de-
picting the citation network among computer sci-
ence arXiv papers indexed by MAG (Wang et al.,
2020). Each node represents an arXiv paper with
directed edges indicating citations. The goal is to
classify papers into one of 40 subject areas like
cs.AI, cs.LG, and cs.OS, with labels manually as-
signed by the authors and arXiv moderators.
B Baselines
The details of the baseline methods we compared
GraphBridge to are as follows:
•Traditional GNNs: In this work, we
adopted three simple yet widely used
GNN models: GCN (Kipf and Welling,
2016), SAGE (Hamilton et al., 2017), and
GAT (Veli ˇckovi ´c et al., 2018). Addition-
ally, We include a graph transformer as the
GNNs-based methods baseline, i.e., Node-
Former (Wu et al., 2022).
•Fine-tuned Language Models: We adopt three
commonly used pre-trained language models
in our study: BERT (Devlin et al., 2018), two
versions of RoBERTa (Liu et al., 2019), specif-
ically RoBERTa-base and RoBERTa-large.
•Previous methods for TAGs: GLEM (Zhao
et al., 2022) is an effective framework that
fuses language models and GNNs in the train-
ing phase through a variational EM frame-
work. We use the official source code†to
reproduce its results. TAPE (He et al., 2023)
utilizes Large Language Models like Chat-
GPT (Achiam et al., 2023) to generate pseudo
labels and explanations for textual nodes,
which are then used to fine-tune Pre-trained
Language Models alongside the original texts.
We reproduced its results using the official
source code‡.SimTeG (Duan et al., 2023)
employs a cascading structure specifically de-
signed for textual graphs, utilizing a two-stage
training paradigm. Initially, it fine-tunes lan-
guage models and subsequently trains GNNs.
We conducted experiments using the official
source code§.ENGINE (Zhu et al., 2024) is
†https://github.com/AndyJZhao/GLEM
‡https://github.com/XiaoxinHe/TAPE
§https://github.com/vermouthdky/SimTeGCora WiKiCS CiteSeer Ele-Photo
Ours (base )92.03 ± 0.94 80.13 ± 0.31 84.52 ± 1.17 83.14 ± 0.10
Ours (large )91.96 ± 0.77 80.55 ± 0.24 85.91 ± 0.99 84.34 ± 0.14
Table 5: Results using BERT as an alternative language model backbone for token reduction.
an efficient fine-tuning and inference frame-
work for text-attributed graphs. It co-trains
large language models and GNNs using a
ladder-side approach, optimizing both mem-
ory and time efficiency. For inference, EN-
GINE utilizes an early exit strategy to further
accelerate. We reproduce its results using the
official source code¶.
C Implementation Details
In this section, we give the implementations details
about our method GraphBridge.
Training of Graph-Aware Token Reduction
Module. We train only the cross-attention mod-
ule and the classifier, keeping the encoding lan-
guage models frozen (Zhang et al., 2019, 2022).
Each dataset undergoes 100 training epochs, with
an early stopping patience of 10 epochs. The learn-
ing rate is explored within {1e-3, 5e-4, 1e-4}, and
the regularization term βis set to 0.1.
Training of Language Models. Initially, we con-
struct the sequence Qby sampling adjacent nodes
using a random walk with restart sampler Γ(Zhu
et al., 2022, 2024). The number of walk steps
for sampling varies among {8, 16, 32}. Train-
ing epochs for the language models are adapted
according to the dataset sizes: {4, 6, 8} for
small datasets ( e.g., Cora, WikiCS, CiteSeer), {4,
6} for medium datasets ( e.g., ArXiv-2023, Ele-
Photo, OGBN-Products), and {4} for the large-
scale dataset (OGBN-ArXiv). We employ AdamW
optimizers. The learning rate is explored within
{1e-4, 5e-5, 1e-5} for full parameter fine-tuning of
RoBERTa-base||, and {1e-3, 5e-4, 1e-4} for tuning
RoBERTa-large **using LoRA (Hu et al., 2021)
with a rank of 8. For the large language model
LLaMA2-7B††, as outlined in Table 3, we use
LoRA with a rank of 4 and a learning rate within
{5e-4, 1e-4, 5e-5}.
¶https://github.com/ZhuYun97/ENGINE
||https://huggingface.co/FacebookAI/roberta-base
**https://huggingface.co/FacebookAI/roberta-large
††https://huggingface.co/meta-llama/Llama-2-7bTraining of Graph Neural Networks. We train
the GNNs models ( i.e., SAGE) subsequent to ac-
quiring node representations from the language
models. Specifically, The number of training
epochs is designated within the range of {100, 200,
500}, complemented by an early stopping mecha-
nism set at 20 epochs for each dataset. We utilize
the Adam optimizers, and the learning rate is cho-
sen from the set {1e-2, 5e-3, 1e-3}.
D Alternative LMs as Backbones for
Token Reduction
Our graph-aware token reduction module is com-
patible with any language model as a backbone. In
this section, we demonstrate the effectiveness of
our token reduction module using BERT‡‡(Devlin
et al., 2018) as an alternative backbone. Table 5
presents the results when employing BERT for to-
ken reduction.
E Link Prediction
Methods Cora CiteSeer ArXiv-2023
SAGE 86.20 ± 0.96 79.69 ± 1.11 94.90 ± 0.24
RoBERTa-base 83.68 ± 0.61 85.10 ± 1.23 92.59 ± 0.55
SimTeG 87.24 ± 0.47 86.33 ± 1.05 97.01 ± 0.64
Ours (base ) 88.87 ± 0.81 88.01 ± 1.20 97.33 ± 0.48
Table 6: Experimental results of link prediction. The
mean AUC with standard deviation across 5 runs is
reported.
Our method is not limited to node classification
tasks, it can also be applied to other representation
learning tasks of TAGs ( e.g., link prediction). In
this section, we conduct experiments on link predic-
tion task. Specifically, we split the existing edges
into train:val:test=0.6:0.2:0.2 for all datasets. The
AUC score is served as the metric. According to
Table 6, GraphBridge still outperforms other base-
lines by a significant margin. This highlights the
effectiveness of its representation learning on TAGs
and demonstrates the potential of GraphBridge for
application to other-level tasks.
‡‡https://huggingface.co/google-bert/bert-base-uncased