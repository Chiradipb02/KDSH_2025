Rationale-Aware Answer Verification by Pairwise Self-Evaluation
Akira Kawabata
The Asahi Shimbun Company
kawabata-a@asahi.comSaku Sugawara
National Institute of Informatics
saku@nii.ac.jp
Abstract
Answer verification identifies correct solutions
among candidates generated by large language
models (LLMs). Current approaches typically
train verifier models by labeling solutions as
correct or incorrect based solely on whether the
final answer matches the gold answer. How-
ever, this approach neglects any flawed ratio-
nale in the solution yielding the correct answer,
undermining the verifier’s ability to distinguish
between sound and flawed rationales. We em-
pirically show that in StrategyQA, only 19%
of LLM-generated solutions with correct an-
swers have valid rationales. Furthermore, we
demonstrate that training a verifier on valid ra-
tionales significantly improves its ability to dis-
tinguish valid and flawed rationales. To make
a better verifier without extra human supervi-
sion, we introduce REPS (Rationale Enhance-
ment through Pairwise Selection), a method
for selecting valid rationales from candidates
by iteratively applying pairwise self-evaluation
using the same LLM that generates the solu-
tions. Verifiers trained on solutions selected
by REPS outperform those trained using con-
ventional training methods on three reason-
ing benchmarks (ARC-Challenge, DROP, and
StrategyQA). Our results suggest that training
reliable verifiers requires ensuring the validity
of rationales in addition to the correctness of
the final answers, which would be critical for
models assisting humans in solving complex
reasoning tasks.
1 Introduction
Reasoning in large language models (LLMs)
(Brown et al., 2020; Chowdhery et al., 2022; Ope-
nAI et al., 2024) plays a vital role in their intelligent
behavior (Wei et al., 2022a; Kojima et al., 2022).
Recent advancements, owing to progressive scal-
ing (Kaplan et al., 2020) and high-quality corpora
(Penedo et al., 2024), facilitate LLMs in solving
complex reasoning tasks including mathematical
Negative
Rationale
Answer AnswerGenerate Candidate Solutions
・・・
・・・・・・
・・・
PositiveQuestionInference
with 
Verifier
Select Solution by VerifierTrain VerifierGenerate Candidate Solutions
is correct! is correct!
Verifier Verifier・・・
・・・・・・
・・・Train 
VerifierQuestion
Rationale
R A
R A
R ANegativePositive
R A
R A
R AFigure 1: Importance of considering rationale quality
in answer verification. Verifiers trained on correct an-
swers with flawed reasoning (blue) fail to identify valid
rationales at inference. In contrast, verifiers trained on
solutions with correct answers and rationales (yellow)
can distinguish valid reasoning.
reasoning and multi-hop reasoning. Further im-
provements are possible by prompting LLMs to
generate intermediate reasoning steps that validate
why the generated answer is correct (Wei et al.,
2022b). However, LLM-generated reasoning of-
ten exhibits factual and logical consistency error
(Maynez et al., 2020; Laban et al., 2023; Lyu et al.,
2023; Jacovi et al., 2024; Dai et al., 2024).
Erroneous reasoning can be mitigated using a
trained verifier model (Cobbe et al., 2021) to select
the best solution from multiple generated solutions,
each consisting of an answer and its intermediate
reasoning (rationale). This methodology delivers
more accurate answers than a single-answer setting
(Cobbe et al., 2021; Li et al., 2023; Hosseini et al.,
2024). However, existing studies mainly define
the training labels for the verifier by solely check-
ing the final answer. This approach may lead to a
verifier that prefers faulty reasoning when LLMs
generate the correct answers for wrong reasons
(Figure 1). This is particularly problematic when
models assist humans in complex tasks (Bowman
et al., 2022; Saunders et al., 2022), wherein humansarXiv:2410.04838v2  [cs.CL]  25 Oct 2024can not fully examine the model’s feedback.
This raises two questions: (1) How often do mod-
els generate correct answers with flawed rationales?
(2) Does training on flawed rationale as positive
samples yield an untrustworthy verifier that cannot
discriminate between flawed and valid rationale?
We investigate these questions using StrategyQA
(Geva et al., 2021) as the testbed (detailed in Sec-
tion 3). We use GPT-4 (OpenAI et al., 2024) to
assess the validity of the rationales based on their
factual and logical consistency with the annotated
factual information. For the first question, we find
that although 59% of the model-generated solu-
tions contain the correct answer, only 19% of these
correct-answer solutions are judged as having valid
rationale, which is also supported by our manual
inspection. For the second question, we create
training datasets with varying levels of reasoning
quality for the verifier. The verifier’s accuracy in
selecting valid reasoning significantly increases as
the quality improves.
To enhance the verifier’s ability to discern the
validity of rationales, we introduce REPS (Ratio-
nale Enhancement through Pairwise Selection), a
training method that leverages LLMs’ pairwise self-
evaluation to select high-quality rationales from
candidates. By refining the training data with these
valid rationales, REPS enables to train more reli-
able and rationale-aware verifiers (Section 4).
Experiments across three reasoning datasets,
ARC-Challenge (Clark et al., 2018), DROP (Dua
et al., 2019), and StrategyQA demonstrate that ver-
ifiers trained on REPS consistently prefer solutions
with valid rationales compared to those trained on
datasets labeled solely based on the final answer’s
correctness. While REPS serves as an effective
quality filter for rationales without extra human
supervision, our analysis implies that excessive
pairwise evaluation may amplify inherent prefer-
ence biases in LLMs, such as favoring longer text
(Sections 5 and 6). We hope future work will ex-
plore methods to mitigate these biases and further
enhance the reliability of verifiers, in preparation
for models becoming capable of solving tasks that
humans cannot examine. This work takes the first
step towards building trustworthy verifiers that can
discern the validity of rationales.
Our contributions can be summarized as follows:
•We empirically demonstrate the crucial role of
rationale quality in training reliable verifiers.
Our analysis reveals that a large portion ofcorrect answer solutions have invalid reason-
ing, and verifiers trained on such rationales
tend to select solutions with similarly faulty
reasoning.
•We introduce REPS, a method for selecting
high-quality rationales by applying iterative
pairwise self-evaluation. Verifiers trained with
REPS significantly outperform baseline veri-
fiers in selecting solutions with valid rationale
on ARC-Challenge, DROP, and StrategyQA.
•Our analysis reveals that while iterative pair-
wise evaluation enhances the rationale qual-
ity, it can amplify inherent preference bias
in LLM-based evaluators toward selecting
longer rationales.1
2 Related Work
2.1 Answer Verification
Recent studies have explored using verifier mod-
els for scoring or ranking generated solutions in
domains such as mathematical reasoning (Cobbe
et al., 2021; Wang et al., 2023b; Yu et al., 2023;
Lightman et al., 2024; Miao et al., 2024), coding
(Ling et al., 2023; Ni et al., 2023), and common-
sense reasoning (Li et al., 2023; Weng et al., 2023;
Zhang et al., 2024). Verifiers are trained to distin-
guish between correct and incorrect solutions using
several approaches, such as generative models (Ko-
rbak et al., 2023; Asai et al., 2024), reward models
(Cobbe et al., 2021), or a combination (Rafailov
et al., 2023; Hosseini et al., 2024). When labeling
solutions as “correct” and “incorrect”, existing stud-
ies mainly rely on checking consistency with the
final answer (Yu et al., 2023; Hosseini et al., 2024).
A notable exception is Li et al. (2023), which at-
tempts to extract reasonable reasoning steps from
wrong answer solutions through comparisons of
the steps with those in correct solutions.
However, reasoning paths generated by LLMs
often contain logical or factual errors (Lyu et al.,
2023; Turpin et al., 2023; Golovneva et al., 2023),
despite having a correct final answer (Jacovi et al.,
2024). This study challenges the assumption that a
correct answer indicates a valid reasoning path to
build rationale-aware verifiers.
1The code and data are available at https://github.com/
AkiraKawabata/REPS .2.2 LLM-as-Judge
Existing studies have explored using LLMs to eval-
uate natural language generation tasks (Chen et al.,
2023; Zheng et al., 2023; Pan et al., 2024; Kim
et al., 2024; Zeng et al., 2024). These methods
can be categorized into three types: providing natu-
ral language feedback (Madaan et al., 2023; Paul
et al., 2024), scoring with scalar values (Liu et al.,
2023b), and comparing multiple outputs (Xie et al.,
2023; Qin et al., 2024; Liu et al., 2024b). LLM-
based evaluation can also improve training by of-
fering feedback as supervision (Xu et al., 2023; Liu
et al., 2023a, 2024a), filtering the training dataset
by score (Gulcehre et al., 2023) or using pairwise
comparison (Bai et al., 2022). This study applies
pairwise evaluation, which aligns better with hu-
man judgment than direct scoring (Wang et al.,
2023a), to refine the verifier’s training data.
3 Does Flawed Rationale Lead to
Untrustworthy Verification?
LLMs may generate responses that contain correct
answers but invalid reasoning paths (Jacovi et al.,
2024). Thus, labeling generated solutions as posi-
tive samples based on the correctness of the final
answer, without considering the validity of the rea-
soning, may yield a verifier that fails to distinguish
between sound and flawed reasoning. Herein, we
investigate this hypothesis by decomposing it into
two questions: (1) How often do LLMs generate
correct answers with invalid reasoning? (Section
3.3) (2) How does the rationale quality in the posi-
tive training data affect the verifier’s performance
in selecting valid reasoning? (Section 3.4)
3.1 Task Setting of Answer Verification
In answer verification, a verifier model evaluates
the correctness of multiple candidate solutions gen-
erated by an answer-generation model and selects
the highest-scoring candidate as the final answer.
Verification A verifier model Mvtakes a solution
sgenerated by an answer-generation model Mgfor
a question q, and returns the probability pthat the
solution is correct. The solution sconsists of an
answer aand a reasoning path rthat represents the
rationale justifying the answer a, i.e., s= (a, r).
The verifier’s output probability can be expressed
as:p=Mv(s|q).
Training Verifier Models To train a verifier, we
sample solutions from the generator Mgfor eachquestion qwith temperature T= 0.7. These so-
lutions are then classified as correct or incorrect
based on arbitrary criteria (e.g., whether the answer
matches the gold answer). The verifier model Mv
is trained to judge the correctness of each solution
sas a reward model, following Cobbe et al. (2021).
We use a binary cross-entropy loss function for
training:
L=−1
NNX
i=1[yilog(pi) + (1−yi) log(1 −pi)],(1)
where Nis the number of training samples, yiis the
binary label (1 for correct and 0 for incorrect) for
thei-th solution, and piis the probability predicted
by the verifier for the i-th solution being correct.
Answer Selection by Verifiers At inference time,
given a set of candidates s1, s2, . . . , s ngenerated
byMgfor a question q, the verifier model Mv
predicts the probability pifor each candidate si
and selects the highest:
s∗= arg max
siMv(si|q) (2)
where s∗is the selected answer.
3.2 Experimental Settings
Model We use Llama-2 7B, a publicly available
LLM, as the base model for both the answer gener-
ator and verifier owing to its popularity (Xie et al.,
2023; Hosseini et al., 2024), thus facilitating a com-
parison with existing research.
Dataset We use StrategyQA, a yes/no question
dataset requiring multi-hop reasoning using fac-
tual knowledge. We select StrategyQA as our
testbed because it requires broad factual knowl-
edge and consistent reasoning, areas where LLMs
often struggle (Jacovi et al., 2024). We split the
official train (2,290 questions) subset into 1,603
training and 687 test questions, as the official test
set does not contain gold answers.
Validity Evaluation We evaluate rationale qual-
ity based on factuality and logical consistency, fol-
lowing previous studies on model-generated rea-
soning evaluation (Ott et al., 2023; Golovneva et al.,
2023; Radhakrishnan et al., 2023; Press et al., 2023;
Jacovi et al., 2024). Factuality assesses the ground-
ing of a model’s reasoning in real-world knowledge,
while logical consistency checks the coherence of
the reasoning process.-5.71-1.56
+ 1 0 . 3 9+ 1 1 . 1 7
+ 2 2 . 6 0+ 1 9 . 4 8
Rationale Accuracy Answer Accuracy020406080100
Chance Rates
Low-Quality
Baseline
High-QualityAccuracyFigure 2: Rationale Accuracy (%) and Answer Accu-
racy (%) of verifier models trained on datasets with
varying levels of rationale quality.
Figure 3: Rationale Accuracy (%) and Answer Accu-
racy (%) as a function of the ratio of high-quality ratio-
nales mixed into the baseline dataset.
We use GPT-4 to annotate the validity of gener-
ated rationales. To ensure that GPT-4’s judgment
is not hallucinated, we provide it with supporting
facts annotated in each question in StrategyQA.2
GPT-4’s validity annotations show a Cohen’s kappa
of 0.62 with our manual annotations on 100 ran-
domly sampled rationales.3
3.3 How Often Does LLM Generate Correct
Answers with Invalid Reasoning?
We examine how often model-generated solutions
have invalid reasoning despite having correct an-
swers. For each question, we prompted the model
with 6-shot examples to generate solutions s, con-
sisting of a reasoning path rand an answer a.
GPT-4’s evaluation showed that while 59% of
the generated solutions contained the correct an-
swer, only 19% of those correct-answer solutions
were judged as having valid reasoning paths. This
highlights the limitations of relying solely on an-
swer correctness to determine the validity of a so-
lution, as it does not guarantee sound reasoning.
3.4 How Does Rationale Quality Affect
Verifier Performance in Selecting Valid
Reasoning?
We examine how rationale quality in positive sam-
ples affects verifier performance in selecting valid
solutions. We conduct experiments with different
levels of rationale quality.
Training Datasets We create three different train-
ing datasets for the verifier model. Following pre-
2Examples of GPT-4’s annotations are provided in Ap-
pendix D.
3More details for our annotations are in Appendix F.vious studies (Cobbe et al., 2021; Li et al., 2023),
we consider solutions with incorrect answers as
negative samples.
1.Low-quality setting : Positive samples are
created by replacing the final answer of incor-
rect solutions with the correct one, resulting
in little to no valid reasoning.
2.Baseline setting : Positive samples are solu-
tions with correct final answers, regardless of
reasoning quality.
3.High-quality setting : Positive samples are
solutions with correct answers and valid ratio-
nales, as validated by GPT-4.
The quality of rationales in the positive samples
is expected to increase from the Low-quality set-
ting to the High-quality setting. To collect valid
solutions for the High-quality setting, we evaluate
up to 20 generated solutions per question by GPT-
4 in a zero-shot manner. When any one solution
passes the evaluation, we include the solution in
the high-quality training dataset as a positive exam-
ple. If none of the 20 solutions pass, the question is
discarded. Consequently, we obtain 1,318 training
examples. We keep dataset sizes consistent across
all settings for fair comparison.
Distribution-Controlled Test Set Design We de-
sign a test set to evaluate the ability of the verifier
to select solutions independently of the generator
model’s output distribution. For each question in
the test set, we create a set of solution candidates
consisting of:•One valid solution svalid = (r+, a+)where
r+denotes a valid rationale and a+is the
corresponding correct answer.
•Two solutions scorrect = (r−, a+)with cor-
rect answer a+but invalid rationale r−.
•Two solutions sincorrect = (r−, a−)with in-
correct answer a−and invalid rationale r−.
Nvalid,Ncorrect , andNincorrect denote the number
ofsvalid,scorrect , andsincorrect , respectively. We
setNvalid= 1,Ncorrect = 2, andNincorrect = 2. We
report two metrics:
•Rationale Accuracy (RA) : The percentage
of questions when the verifier selects the valid
solution from the set of candidates.
•Answer Accuracy (AA) : The percentage of
questions when the verifier selects a solution
with a correct answer (regardless of whether
the rationale is valid or not) from the set of
candidates.
Formally, these metrics can be expressed as:
RA=1
|Dtest||Dtest|X
i=1I
arg max
s∈SiMv(s) =svalid
AA=1
|Dtest||Dtest|X
i=1I
arg max
s∈SiMv(s)∈sgood
where sgood=svalid∪scorrect ,Dtestis the test
dataset, Siis the set of answer candidates for the
i-th question, and I[·]is the indicator function. The
chance rates for RA and AA are 20% and 60%,
respectively.
3.5 Results
Figure 2 shows RA and AA of verifier models
trained on datasets with varying rationale qual-
ity. Training on the high-quality dataset, where
the rationales are validated by GPT-4, results in
a verifier that is significantly better at identifying
valid reasoning compared to the baseline setting.
Conversely, the low-quality dataset, where positive
samples lack valid reasoning, leads to a verifier
with RA near chance rates.
To further investigate the impact of rationale
quality on the verifier’s performance, we incremen-
tally replace 10% of the baseline dataset with high-
quality rationales. Figure 3 shows a clear trend: asthe proportion of high-quality rationales increases,
RA improves while AA remains largely stable.
These results highlight the importance of high-
quality rationales in training verifier models, partic-
ularly in distinguishing between valid and flawed
solutions. Next, we propose a method for auto-
matically constructing a training dataset with high-
quality rationales.
4 Rationale Enhancement through
Pairwise Selection (REPS)
As shown in Section 3, a large portion of LLM-
generated answers contain flawed reasoning, even
if the final answer is correct. Moreover, increasing
the ratio of valid rationales in the positive samples
improves the verifier’s ability to identify sound
reasoning. Thus, filtering out solutions with flawed
rationales is crucial for building a reliable verifier.
We propose REPS, which uses LLMs’ pairwise
comparison to iteratively select high-quality ratio-
nales from diverse candidates. The overall process
of REPS is illustrated in Figure 4. For a question q,
we first prompt the generator model Mgto produce
a set of candidate solutions S=s1, s2, . . . , , where
each solution siconsists of an answer aiand a ratio-
naleri. From this set, we select Nsolutions with
correct answers a+to form the candidate solution
set for the tournament-style pairwise evaluation.
We use tournament-style pairwise evaluation to
find the best rationale among the Ncandidates. In
each round, the generator model Mgacts as the
evaluator that compares the rationales (ri, rj)of
two solutions (si, sj)Stimes and selects the more
factually grounded and logically consistent one as
the winner through majority voting. The evaluator
is provided with the question, answer, two can-
didate rationales, and (depending on the dataset)
passage and answer options. Given hand-crafted
few-shot exemplars, the evaluator model outputs a
justification for which rationale is better and why,
followed by the preferred rationale’s index (1 or
2).4To mitigate the position bias reported in LLM-
based evaluations (Wang et al., 2023a), we alternate
the presentation order of rationale 1 and 2 across
theScomparisons within each round.
We perform this pairwise comparison for all
N
2pairs of solutions. After the pairwise compar-
isons, theN
2preferred rationales form a new set of
candidate solutions S′, and the pairwise compari-
4The prompt format used for the pairwise evaluation are
provided in Appendix C.R A
R A
R AR AR
R
R
RR
R
R A
R A
R AR AR
RR
RR
RR
R
R
RA
A
A
AR A
Dverifier
・・・ QuestionFilter by
AnswerA
AGenerate TrainPositive
NegativeA
APositive
NegativeA
A
・・・ ・・・
・・・QuestionFilter by
AnswerA
AGenerate TrainPositive
NegativeA
APositive
NegativeA
A
・・・ ・・・Rationale Enhancement
through 
Pairwise Selection
(REPS)Filter by Pair EvalR R
A A= {correct, incorrect} rationale
= {correct, incorrect} answer
is correct!R A
is correct!R A DverifierVerifier
VerifierFigure 4: Rationale Enhancement through Pairwise Selection (REPS). The generator model produces candidate
solutions and filters out those with incorrect answers. Unlike the conventional pipeline (top), REPS (bottom)
employs a tournament-style pairwise evaluation to iteratively select the better solution. This refined solution is then
used to train a rationale-aware verifier.
Algorithm 1 Rationale Enhancement through Pair-
wise Selection (REPS)
Input: Question q, Generator Mg, Num candi-
dates N, Num voting S
C← {si|si∈Mg(q)∧si= (ri, a+)}
S←sample (C, N )
while|S|>1do
S′← ∅
for(si, sj)do
s∗←pairwise_eval (si, sj)
S′←S′∪ {s∗}
end for
S←S′
end while
s∗←S[0]
Output: s∗
son process is repeated on this reduced set. This
tournament-style elimination continues until only
one rationale remains, which we consider the re-
fined rationale r∗. The refined rationale r∗, along
with its corresponding answer a∗, is then added
to the training set for the verifier model Mvas a
positive sample. Repeating this for all questions
refines the training dataset.
5 Experiment
5.1 Setup
Model We employ Llama-2 7B as the backbone
for both the generator ( Mg) and verifier ( Mv) mod-
els. We report the performance of two verifiers: thebaseline verifier and the REPS verifier. The base-
line verifier is trained using the conventional ap-
proach, labeling solutions as positive based solely
on the correctness of the final answer. The REPS
verifier is trained on a dataset where solutions re-
fined through REPS are labeled as positive.5
Metrics We report two metrics: Rationale Accu-
racy and Task Performance. Rationale Accuracy is
evaluated on a distribution-controlled test set (Sec-
tion 3.3). The test set is constructed by sampling
Nvalid= 1,Ncorrect = 2, and Nincorrect = 2 solu-
tions for each question. Valid solutions are selected
by GPT-4, and questions for which none of the 20
evaluated solutions are judged valid are discarded
as done in Section 3. Task Performance is evalu-
ated by sampling five solutions for each question,
ranking them with the verifier, and selecting the
answer in the highest-scoring solution.
Datasets We evaluate our method on three di-
verse datasets that assess different aspects of rea-
soning capabilities:
1.StrategyQA: Dataset consisting of questions
that require multi-hop reasoning using factual
knowledge to arrive at the correct answer. We
use 1,603 data points for training, 687 for
testing Task Performance, and 385 of the test
questions to evaluate Rationale Accuracy. We
report accuracy as Task Performance.
5Details of the training and the inference are provided in
Appendix A and B, respectively.ARC-Challenge DROP StrategyQA
Rationale Acc. Task Perf. Rationale Acc. Task Perf. Rationale Acc. Task Perf.
Baseline 38.90 52.40 36.02 45.80 30.13 67.10
REPS 53.05 54.75 40.90 46.90 38.96 67.25
Table 1: Rationale Accuracy (%) and Task Performance (%) of the baseline verifier and REPS. REPS consistently
outperforms the baseline in selecting valid rationales while maintaining or slightly improving Task Performance.
4 8 16 32 640.250.30.350.40.450.50.550.6
455055606570758085
DROP
ARC
StrategyQA
Rationale Accuracy
Rationale Length
N (Number of Candidates)Rationale Accuracy
Avg. Length of Selected Rationales
3 5 10 200.250.30.350.40.450.5
50556065707580859095
DROP
ARC
StrategyQA
Rationale Accuracy
Rationale Length
S (Number of Comparision per Match)Rationale Accuracy
Avg. Length of Selected Rationales
Figure 5: The effect of varying the number of candidate solutions ( N) and the number of pairwise comparisons per
match ( S) on the Rationale Accuracy (%) and average length of selected rationales. Increasing N and S leads to a
decrease in Rationale Accuracy and an increase in the average length of selected rationales.
2.ARC-Challenge: Challenging subset of the
AI2 Reasoning Challenge (ARC) dataset that
evaluates commonsense reasoning about sci-
entific knowledge. We use 1,119 questions for
training, 1,172 for testing Task Performance,
and 509 of the test questions to evaluate Ra-
tionale Accuracy. We report accuracy as Task
Performance.
3.DROP: Reading comprehension dataset that
requires arithmetic reasoning to answer the
questions. We select 2,000 questions for train-
ing, 1,000 for testing Task Performance, and
819 for evaluating Rationale Accuracy. We
report Exact Match as Task Performance.
Additional References for GPT-4’s Annotation
We provided additional references for each dataset
to support GPT-4’s annotation of valid solutions.
For StrategyQA, as described in Section 3, we pro-
vide the supporting facts annotated in the dataset.
For ARC, we provided the top five Wikipedia para-
graphs with the highest BM25 scores against the
concatenated question and answer options. We did
not provide any additional references for DROP,
as it is a reading comprehension task that does not
require external factual knowledge.
REPS Parameters We set the number of pair-
wise comparisons per round ( S) to 5 and the num-ber of candidate solutions ( N) to 8 for ensuring
diversity and robustness in voting.
6 Results and Analysis
6.1 Effectiveness of REPS
Main Results Table 1 shows REPS improves
Rationale Accuracy without affecting Task Perfor-
mance across all datasets. The improvement is
particularly significant in ARC and StrategyQA,
where the REPS-trained verifier substantially out-
performs the baseline, with improvements of 14.1%
and 8.8%, respectively. This improvement can be
attributed to the nature of the reasoning required in
these datasets. In arithmetic reasoning tasks like
DROP, which involve more deductive reasoning, it
is rare for the model to arrive at the correct answer
using flawed reasoning. Thus, the correctness of
the final answer can be a good indicator of the valid-
ity of the reasoning process. In contrast, inductive
and abductive reasoning tasks like ARC and Strat-
egyQA allow more room for flawed reasoning to
reach correct answers, emphasizing the importance
of evaluating the intermediate reasoning.
Win Rate by GPT-4 We conduct a head-to-head
evaluation using GPT-4 to compare the rationales
selected by REPS and the baseline. For each ques-
tion, we generate five candidate solutions, from212 (58.1%)235 (60.4%)213 (56.5%)
153 (41.9%)154 (39.6%)164 (43.5%)
0 50 100 150 200 250 300 350 400StrategyQAARCDROPBaseline Wins REPS Wins
CountFigure 6: Win rate of REPS vs. baseline verifier when
their selected rationales are compared head-to-head us-
ing GPT-4 as the judge.
which REPS and the baseline each select one. GPT-
4 compares rationales for selected solutions, ex-
cluding those where both methods choose the same.
In each pairwise evaluation, GPT-4 chooses the
more factually grounded and logically consistent
rationale, given the question and answer. We al-
ternate the presentation order of the rationales se-
lected by REPS and the baseline to mitigate posi-
tion bias. Figure 6 shows REPS-selected rationales
consistently outperform baseline-selected across
datasets, with 56-60% win rates. Manual annota-
tion of 100 random rationale pairs also confirms
this trend, with REPS win rates of 63%, 60%, and
61% for ARC, DROP, and StrategyQA, respec-
tively.6REPS chooses better rationales than the
baseline, even without guaranteed valid rationales
among candidates.
6.2 Effects of Parameter Choice
We study how parameters in REPS, candidate pool
size (N), and pairwise comparisons per round ( S)
affect Rationale Accuracy. A larger Nis expected
to increase the diversity of the candidate pool. Sim-
ilarly, a higher Sshould enhance the reliability of
the majority voting process. Thus, increasing both
factors are expected to contribute to a higher Ra-
tionale Accuracy. Figure 5 illustrates the Rationale
Accuracy as Nvaries from 4 to 64 and Sfrom 3 to
20. Unexpectedly, Rationale Accuracy decreases
asNandSgrow. This phenomenon can be seen
as the emergence of biases toward the superficial
cue, i.e., the length of rationale, in model-based
evaluation, as shown in Figure 5. As NandS
increase, i.e., the number of pairwise evaluations
per tournament grows, the length of the selected
reasoning paths also tends to increase. This implies
model-based evaluation’s bias for longer answers
(Zheng et al., 2023; Koo et al., 2023; Dubois et al.,
2024) amplifies over repeated pairwise evaluations,
6Appendix F shows details in our manual annotation.ARC DROP StrategyQA
G-EV AL 44.99 40.66 30.39
REPS 53.05 40.90 38.96
Table 2: Rationale Accuracy (%) of verifiers trained on
rationales selected by REPS vs. G-EV AL.
causing deviation from valid rationale distribution.
One approach to handle the bias amplification
is to treat the parameters NandSin REPS as hy-
perparameters that can be tuned using a validation
set. Adjusting these parameters makes it possible
to identify an optimal balance between effective
rationale filtering and the minimization of bias am-
plification.
6.3 Importance of Pairwise Evaluation
We investigate whether the iterative pairwise eval-
uation employed by REPS produces more effec-
tive training samples for rationale-aware verifiers
compared to single-answer evaluation. We use G-
EVAL (Liu et al., 2023b) as a representative of the
single-answer evaluation, which assigns a weighted
score to each solution sbased on the probabilities
of outputting score tokens:
g(s) =5X
ti=1p(ti|s)×ti (3)
where tirepresents the i-th score token (e.g.,
“1”), and p(ti|s)is the probability of outputting
that token given the solution s. For each of the N
candidate solutions, we compute the score Stimes
and select the reasoning path with the highest av-
erage score. For fair comparison, we set candidate
solutions Nto 8 and evaluations per solution Sto
5 for both methods. Table 2 shows that verifiers
trained on REPS-selected rationales outperform
those trained on G-EVAL -selected ones in iden-
tifying valid reasoning. This result demonstrates
that REPS’s iterative pairwise comparison is more
effective than direct scoring in providing higher-
quality training samples for verifiers.
6.4 Error Types in Invalid Reasoning Paths
To gain insight into the types of errors present in
reasoning paths judged as invalid by GPT-4, we
conduct a manual analysis of 100 randomly sam-
pled invalid reasoning paths for each dataset (ARC,
StrategyQA, and DROP). The errors are catego-
rized into four types: (1) both factual and logicalError Type ARC StrategyQA DROP
Factual only 16 15 35
Logical only 43 46 22
Fact. and Logic. 32 31 24
Other 2 5 9
No errors 7 3 10
Table 3: Distribution of error types (%) in reasoning
paths judged as invalid by GPT-4.
errors, (2) factual errors only, (3) logical errors
only, and (4) other errors that are neither factual
nor logical (e.g., incomplete explanations or mere
repetitions of the question). We define factual er-
rors as reasoning that uses information contradict-
ing the supporting facts or passage provided for the
question. Logical errors are defined as reasoning
that is irrelevant to a valid explanation or draws
conclusions that cannot be derived from the preced-
ing steps. Table 3 presents the distribution of these
error types across the three datasets.
In DROP, factual errors are notably more preva-
lent (35% factual only and 24% both factual and
logical). The model often struggles with extracting
necessary information from the given passage, fre-
quently using hallucinated or incorrect information
in its reasoning process.
Conversely, ARC and StrategyQA exhibit a
higher proportion of logical errors (43% and 46%
logical only, respectively). The model often fails
to identify the relevant information necessary to
solve the problem, or misunderstands what can be
inferred from commonsense knowledge.
7 Discussion
The Discrepancy Between Rationale Accuracy
and Task Performance While REPS effectively
improves Rationale Accuracy of verifiers, it does
not significantly enhance overall Task Performance.
This aligns with Section 3.3 and recent findings
suggesting a weak link between rationale quality
and answer correctness (Wiegreffe et al., 2022; Ja-
covi et al., 2024). This unfaithfulness becomes
particularly critical in the context of scalable over-
sight (Bowman et al., 2022), where humans may
find it challenging to evaluate the correctness of the
model’s outputs. In such scenarios, humans might
be misled by seemingly plausible yet unfaithful
rationales. Future research should focus on improv-
ing rationales regarding quality and faithfulness.8 Conclusion
We investigate how rationale quality affects a veri-
fier’s ability to select valid answers. We empirically
demonstrate that many model-generated rationales
contain errors, even when the final answer is cor-
rect, leading to untrustworthy answer verification.
We introduce REPS, a method that uses LLMs’
pairwise comparison to iteratively refine generated
rationales. Experiments on three reasoning datasets
show that REPS significantly outperforms baseline
verifier models, particularly in selecting solutions
with valid rationales. Our analysis shows iterative
pairwise evaluation improves rationale quality but
may amplify LLM-based evaluators’ biases.
Limitations
Dataset Diversity The experiments in this study
are limited to three reasoning datasets: ARC-
Challenge, DROP, and StrategyQA. While these
datasets cover various aspects of reasoning, they
may not be representative of all reasoning tasks. To
further validate the effectiveness of REPS, it would
be beneficial to evaluate the method on a more di-
verse set of datasets, such as those involving coding
(Chen et al., 2021) or instruction following (Zheng
et al., 2023).
Reliance on GPT-4 for Judging Although we
confirmed a high agreement between GPT-4 and
human judgments, GPT-4 may still be subject to
biases not present in human evaluations, potentially
leading to invalid evaluation.
Limited Training Data We use a relatively small
amount of training data, ranging from 1,000 to
2,000 instances per dataset. It would be valuable to
investigate REPS’s scalability on larger datasets.
Limited Model Size This study focuses on a sin-
gle model, Llama-2 7B, as a case study for the
generator and verifier models. It would be valuable
to explore the impact of model size and architec-
ture on REPS, as larger models may provide more
accurate and coherent pairwise evaluations, leading
to further improvements in the selected rationales.
Acknowledgments
We would like to thank the anonymous review-
ers for their constructive feedback and Hideaki
Tamori for his valuable comments. This work
was supported by JSPS KAKENHI Grant Num-
ber JP22K17954.References
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2024. Self-RAG: Learning to
retrieve, generate, and critique through self-reflection.
InThe Twelfth International Conference on Learning
Representations .
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosuite, Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemi Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bow-
man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,
Nicholas Joseph, Sam McCandlish, Tom Brown, and
Jared Kaplan. 2022. Constitutional AI: Harmlessness
from AI feedback. Preprint , arXiv:2212.08073.
Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin
Chen, Craig Pettit, Scott Heiner, Kamil ˙e Lukoši ¯ut˙e,
Amanda Askell, Andy Jones, Anna Chen, Anna
Goldie, Azalia Mirhoseini, Cameron McKinnon,
Christopher Olah, Daniela Amodei, Dario Amodei,
Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson
Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish,
Joshua Landau, Kamal Ndousse, Liane Lovitt, Nel-
son Elhage, Nicholas Schiefer, Nicholas Joseph,
Noemí Mercado, Nova DasSarma, Robin Larson,
Sam McCandlish, Sandipan Kundu, Scott Johnston,
Shauna Kravec, Sheer El Showk, Stanislav Fort, Tim-
othy Telleen-Lawton, Tom Brown, Tom Henighan,
Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben
Mann, and Jared Kaplan. 2022. Measuring progress
on scalable oversight for large language models.
Preprint , arXiv:2211.03540.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. Preprint ,
arXiv:2107.03374.
Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and
Ruifeng Xu. 2023. Exploring the use of large lan-
guage models for reference-free text quality evalua-
tion: An empirical study. In Findings of the Associa-
tion for Computational Linguistics: IJCNLP-AACL
2023 (Findings) , pages 361–374, Nusa Dua, Bali.
Association for Computational Linguistics.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. Preprint , arXiv:2204.02311.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? Try ARC, the AI2 reasoning challenge.
Preprint , arXiv:1803.05457.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. Preprint , arXiv:2110.14168.
Chengwei Dai, Kun Li, Wei Zhou, and Songlin Hu.
2024. Beyond imitation: Learning key reasoningsteps from dual chain-of-thoughts in reasoning distil-
lation. Preprint , arXiv:2405.19737.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel
Stanovsky, Sameer Singh, and Matt Gardner. 2019.
DROP: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers) , pages 2368–2378, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Yann Dubois, Balázs Galambosi, Percy Liang, and Tat-
sunori B. Hashimoto. 2024. Length-controlled al-
pacaeval: A simple way to debias automatic evalua-
tors. Preprint , arXiv:2404.04475.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did Aristo-
tle Use a Laptop? A Question Answering Bench-
mark with Implicit Reasoning Strategies. Transac-
tions of the Association for Computational Linguis-
tics (TACL) .
Olga Golovneva, Moya Peng Chen, Spencer Poff, Mar-
tin Corredor, Luke Zettlemoyer, Maryam Fazel-
Zarandi, and Asli Celikyilmaz. 2023. ROSCOE: A
suite of metrics for scoring step-by-step reasoning. In
The Eleventh International Conference on Learning
Representations .
Caglar Gulcehre, Tom Le Paine, Srivatsan Srini-
vasan, Ksenia Konyushkova, Lotte Weerts, Abhishek
Sharma, Aditya Siddhant, Alex Ahern, Miaosen
Wang, Chenjie Gu, Wolfgang Macherey, Arnaud
Doucet, Orhan Firat, and Nando de Freitas. 2023.
Reinforced self-training (rest) for language modeling.
Preprint , arXiv:2308.08998.
Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron
Courville, Alessandro Sordoni, and Rishabh Agarwal.
2024. V-STaR: Training verifiers for Self-Taught
reasoners. Preprint , arXiv:2402.06457.
Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan
Herzig, Or Honovich, Michael Tseng, Michael
Collins, Roee Aharoni, and Mor Geva. 2024. A
chain-of-thought is as strong as its weakest link: A
benchmark for verifiers of reasoning chains. Preprint ,
arXiv:2402.00559.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. Preprint ,
arXiv:2001.08361.
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang,
Shayne Longpre, Hwaran Lee, Sangdoo Yun,
Seongjin Shin, Sungdong Kim, James Thorne, and
Minjoon Seo. 2024. Prometheus: Inducing fine-
grained evaluation capability in language models. In
The Twelfth International Conference on Learning
Representations .Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In Advances
in Neural Information Processing Systems .
Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,
Zae Myung Kim, and Dongyeop Kang. 2023. Bench-
marking cognitive biases in large language models as
evaluators. Preprint , arXiv:2309.17012.
Tomasz Korbak, Kejian Shi, Angelica Chen,
Rasika Vinayak Bhalerao, Christopher Buck-
ley, Jason Phang, Samuel R. Bowman, and Ethan
Perez. 2023. Pretraining language models with
human preferences. In Proceedings of the 40th
International Conference on Machine Learning ,
volume 202 of Proceedings of Machine Learning
Research , pages 17506–17533. PMLR.
Philippe Laban, Wojciech Kry ´sci´nski, Divyansh Agar-
wal, Alexander R. Fabbri, Caiming Xiong, Shafiq
Joty, and Chien-Sheng Wu. 2023. LLMs as factual
reasoners: Insights from existing benchmarks and
beyond. Preprint , arXiv:2305.14540.
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,
Jian-Guang Lou, and Weizhu Chen. 2023. Making
language models better reasoners with step-aware
verifier. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 5315–5333, Toronto,
Canada. Association for Computational Linguistics.
Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harri-
son Edwards, Bowen Baker, Teddy Lee, Jan Leike,
John Schulman, Ilya Sutskever, and Karl Cobbe.
2024. Let’s verify step by step. In The Twelfth Inter-
national Conference on Learning Representations .
Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang,
Mingu Lee, Roland Memisevic, and Hao Su. 2023.
Deductive verification of chain-of-thought reasoning.
InThirty-seventh Conference on Neural Information
Processing Systems .
Jiacheng Liu, Ramakanth Pasunuru, Hannaneh Ha-
jishirzi, Yejin Choi, and Asli Celikyilmaz. 2023a.
Crystal: Introspective reasoners reinforced with self-
feedback. In Proceedings of the 2023 Conference
on Empirical Methods in Natural Language Process-
ing, pages 11557–11572, Singapore. Association for
Computational Linguistics.
Weize Liu, Guocong Li, Kai Zhang, Bang Du, Qiyuan
Chen, Xuming Hu, Hongxia Xu, Jintai Chen, and Jian
Wu. 2024a. Mind’s mirror: Distilling self-evaluation
capability and comprehensive thinking from large
language models. Preprint , arXiv:2311.09214.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023b. G-eval:
NLG evaluation using gpt-4 with better human align-
ment. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 2511–2522, Singapore. Association for Com-
putational Linguistics.Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi,
Ivan Vuli ´c, Anna Korhonen, and Nigel Collier. 2024b.
Aligning with human judgement: The role of pair-
wise preference in large language model evaluators.
Preprint , arXiv:2403.16950.
Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,
Delip Rao, Eric Wong, Marianna Apidianaki, and
Chris Callison-Burch. 2023. Faithful chain-of-
thought reasoning. In Proceedings of the 13th In-
ternational Joint Conference on Natural Language
Processing and the 3rd Conference of the Asia-Pacific
Chapter of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 305–329,
Nusa Dua, Bali. Association for Computational Lin-
guistics.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdan-
bakhsh, and Peter Clark. 2023. Self-refine: Itera-
tive refinement with self-feedback. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906–1919, On-
line. Association for Computational Linguistics.
Ning Miao, Yee Whye Teh, and Tom Rainforth. 2024.
Selfcheck: Using LLMs to zero-shot check their own
step-by-step reasoning. In The Twelfth International
Conference on Learning Representations .
Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov,
Wen-tau Yih, Sida I Wang, and Xi Victoria Lin. 2023.
Lever: Learning to verify language-to-code gener-
ation with execution. In Proceedings of the 40th
International Conference on Machine Learning .
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-
wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,
Christopher Berner, Lenny Bogdonoff, Oleg Boiko,
Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-
man, Tim Brooks, Miles Brundage, Kevin Button,
Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
Carey, Chelsea Carlson, Rory Carmichael, Brooke
Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben
Chess, Chester Cho, Casey Chu, Hyung Won Chung,
Dave Cummings, Jeremiah Currier, Yunxing Dai,
Cory Decareaux, Thomas Degry, Noah Deutsch,
Damien Deville, Arka Dhar, David Dohan, Steve
Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simón Posada Fishman, Juston Forte, Isabella Ful-
ford, Leo Gao, Elie Georges, Christian Gibson, Vik
Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-
Lopes, Jonathan Gordon, Morgan Grafstein, Scott
Gray, Ryan Greene, Joshua Gross, Shixiang Shane
Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
Yuchen He, Mike Heaton, Johannes Heidecke, Chris
Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-
woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-
mali, Ingmar Kanitscheider, Nitish Shirish Keskar,
Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,
Christina Kim, Yongjik Kim, Jan Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea V oss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, ChongZhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Bar-
ret Zoph. 2024. Gpt-4 technical report. Preprint ,
arXiv:2303.08774.
Simon Ott, Konstantin Hebenstreit, Valentin Liévin,
Christoffer Egeberg Hother, Milad Moradi, Maxi-
milian Mayrhauser, Robert Praas, Ole Winther, and
Matthias Samwald. 2023. Thoughtsource: A central
hub for large language model reasoning data. arXiv
preprint .
Liangming Pan, Michael Saxon, Wenda Xu, Deepak
Nathani, Xinyi Wang, and William Yang Wang. 2024.
Automatically Correcting Large Language Models:
Surveying the Landscape of Diverse Automated Cor-
rection Strategies. Transactions of the Association
for Computational Linguistics , 12:484–506.
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-
riz Borges, Antoine Bosselut, Robert West, and Boi
Faltings. 2024. REFINER: Reasoning feedback on
intermediate representations. In Proceedings of the
18th Conference of the European Chapter of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 1100–1126, St. Julian’s, Malta.
Association for Computational Linguistics.
Guilherme Penedo, Hynek Kydlí ˇcek, Leandro von
Werra, and Thomas Wolf. 2024. Fineweb.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah Smith, and Mike Lewis. 2023. Measuring and
narrowing the compositionality gap in language mod-
els. In Findings of the Association for Computational
Linguistics: EMNLP 2023 , pages 5687–5711, Singa-
pore. Association for Computational Linguistics.
Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,
Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu
Liu, Donald Metzler, Xuanhui Wang, and Michael
Bendersky. 2024. Large language models are effec-
tive text rankers with pairwise ranking prompting.
Preprint , arXiv:2306.17563.
Ansh Radhakrishnan, Karina Nguyen, Anna Chen,
Carol Chen, Carson Denison, Danny Hernandez,
Esin Durmus, Evan Hubinger, Jackson Kernion,
Kamil ˙e Lukoši ¯ut˙e, Newton Cheng, Nicholas Joseph,
Nicholas Schiefer, Oliver Rausch, Sam McCandlish,
Sheer El Showk, Tamera Lanham, Tim Maxwell,
Venkatesa Chandrasekaran, Zac Hatfield-Dodds,
Jared Kaplan, Jan Brauner, Samuel R. Bowman, and
Ethan Perez. 2023. Question decomposition im-
proves the faithfulness of model-generated reasoning.
Preprint , arXiv:2307.11768.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills,
Long Ouyang, Jonathan Ward, and Jan Leike. 2022.Self-critiquing models for assisting human evaluators.
Preprint , arXiv:2206.05802.
Miles Turpin, Julian Michael, Ethan Perez, and
Samuel R. Bowman. 2023. Language models don’t
always say what they think: Unfaithful explanations
in chain-of-thought prompting. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.
Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,
Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and
Zhifang Sui. 2023a. Large language models are not
fair evaluators. Preprint , arXiv:2305.17926.
Peiyi Wang, Lei Li, Zhihong Shao, R X Xu, Damai
Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang Sui.
2023b. Math-Shepherd: Verify and reinforce LLMs
step-by-step without human annotations.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, and William Fedus. 2022a. Emer-
gent abilities of large language models. Transactions
on Machine Learning Research . Survey Certifica-
tion.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022b. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems .
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,
Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.
2023. Large language models are better reasoners
with self-verification. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2023 ,
pages 2550–2575, Singapore. Association for Com-
putational Linguistics.
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,
Mark Riedl, and Yejin Choi. 2022. Reframing
human-AI collaboration for generating free-text ex-
planations. In Proceedings of the 2022 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 632–658, Seattle, United States.
Association for Computational Linguistics.
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao,
Min-Yen Kan, Junxian He, and Qizhe Xie. 2023.
Self-evaluation guided beam search for reasoning.
InThirty-seventh Conference on Neural Information
Processing Systems .
Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao
Song, Markus Freitag, William Wang, and Lei Li.
2023. INSTRUCTSCORE: Towards explainable text
generation evaluation with automatic feedback. In
Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing , pages
5967–5994, Singapore. Association for Computa-
tional Linguistics.Fei Yu, Anningzhe Gao, and Benyou Wang. 2023.
Outcome-supervised verifiers for planning in mathe-
matical reasoning. Preprint , arXiv:2311.09724.
Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya
Goyal, and Danqi Chen. 2024. Evaluating large lan-
guage models at evaluating instruction following. In
The Twelfth International Conference on Learning
Representations .
Yunxiang Zhang, Muhammad Khalifa, Lajanugen Lo-
geswaran, Jaekyeom Kim, Moontae Lee, Honglak
Lee, and Lu Wang. 2024. Small language mod-
els need strong verifiers to self-correct reasoning.
Preprint , arXiv:2404.17140.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
LLM-as-a-judge with MT-bench and chatbot arena.
InThirty-seventh Conference on Neural Information
Processing Systems Datasets and Benchmarks Track .
A Training Details
We train the verifier models using Llama-2 7B as
the base model. The hyperparameters for training
are as follows: batch size per device of 32, 10
training epochs, learning rate of 3×10−7, AdamW
optimizer, and a random seed of 42. The models are
trained on three NVIDIA A100 GPUs using mixed
precision (bfloat16) to reduce memory usage and
training time.
B Inference Details
B.1 Answer Generation
For generating candidate answers, we use the
Llama-2 7B model with a temperature of 0.7.
The number of few-shot examples provided to the
model varies depending on the dataset. For ARC,
we use 4 examples; for DROP, we use 4 examples;
and for StrategyQA, we use 6 examples. The gen-
erated answers are then fed into the verifier models
for scoring and selection.
B.2 REPS Pairwise Evaluation
In REPS, we perform pairwise comparisons of the
generated rationales using the Llama-2 7B model
with a temperature of 0.7. The number of few-shot
examples used for each dataset is determined by
the maximum number of tokens that can fit within
the model’s context size of 4096. For ARC, we
use 8 examples; for DROP, we use 3 examples;
and for StrategyQA, we use 7 examples. The pair-
wise comparisons are conducted iteratively, withthe winning rationales from each round advancing
to the next round until a single rationale remains.
B.3 GPT-4 Evaluations
For evaluating the validity of the generated ratio-
nales and measuring the win rates between the
REPS-selected rationales and the baseline, we em-
ploy the GPT-4 model with a temperature of 0.0.
We use the gpt-4-0314 version of OpenAI API.
The instructions used for evaluating the rationales
are following:
You are a helpful assistant who evaluates
explanations’ factual and logical consis-
tency. Given a question and the cor-
responding reference information (evi-
dence), you will assess whether the ex-
planation meets the following criteria
with justifications:
(1) Factual correctness: The explanation
should be factually correct based on the
given evidence. If the explanation con-
tains claims not directly supported by or
contradicting the evidence, it should be
judged incorrect. The explanation should
not include information or make asser-
tions not mentioned in the evidence.
(2) Logical coherence: The explanation
should follow a clear and logical chain
of reasoning. Each step should be appro-
priately justified based on the evidence
and preceding steps. Leaps in logic, as-
sumptions, or opinions not grounded in
the given information are unacceptable.
If the explanation meets all of the above
criteria, output True. Otherwise, the out-
put will be false if the explanation fails
to meet any criteria. Your output must be
only either True or False. Aim to prefer
False if there are doubts about whether
the criteria are fully satisfied.
C REPS Pairwise Evaluation Prompts
We use dataset-specific formats for pairwise evalu-
ation prompts in REPS, with a common instruction
across all datasets:
You are a helpful assistant who evalu-
ates explanations’ factual accuracy and
logical consistency. Given a question
and an answer, decide which of the twoprovided explanations is more factually
grounded and logically valid. Your out-
put must be 1 or 2, where 1 corresponds
to the first explanation, and 2 corre-
sponds to the second explanation.
Pairwise evaluation is performed by providing
the model with prompts in the following format:
{{instruction}}
Question: {{question}}
Answer: {{gold answer}}
Explanation 1: {{explanation1}}
Explanation 2: {{explanation2}}
Justification: {{justification}}
Preferred Explanation: {{1 or 2}}
D Examples of GPT-4’s Validity Judge
We provide examples of GPT-4’s judgments on the
validity of reasoning paths generated by the Llama-
2 7B in Table 4. Logical errors in the reasoning
paths are highlighted in red, while factual errors
are highlighted in blue.
E Examples of Rationales Selected by
REPS and Baseline
Examples of rationales selected by REPS and the
baseline verifier for the same question are provided
across three datasets: ARC (Table 6), DROP (Table
7), and StrategyQA (Table 5).
F Human Annotation
Validity Judgement To measure the alignment
between human judgments and GPT-4’s judgments,
the authors conducted a validity assessment task.
100 questions were randomly selected from the
StrategyQA dataset. For each of these questions,
the authors evaluated whether the generated ratio-
nale was factually accurate and logically consistent,
given the supporting facts for each question. Each
rationale was manually labeled as either valid or
invalid based on these criteria. The validity an-
notations obtained from GPT-4 show a Cohen’s
kappa of 0.62 with the authors’ manual validity
annotations.
Pairwise Win Rate Evaluation In the pairwise
win rate evaluation, the authors compared 100 pairs
of rationales selected by REPS and the baseline ver-
ifier for each dataset (ARC, DROP, StrategyQA).For ARC and StrategyQA, the evaluation of factu-
ality was conducted with reference to supporting
facts. We chose the rationale that was more fac-
tually accurate and logically consistent, with the
rationales presented in a randomized order to miti-
gate bias.ARC Example:
Question:
Farmers in Wyoming were concerned because some of their chickens were being preyed upon by
hawks that lived in areas around their ranches. The farmers grouped together and hunted the hawks
until they were no longer in their area. Which would most likely happen next?
Choices:
A. The chicken population would go down. B. Populations of mice and rats would increase. C.
Another bird of prey would replace the hawk. D. The chickens would have a lower rate of disease.
Valid Rationale:
Removing the hawks from the area could affect the populations of other animals. If hawks are not
present in an area, they are not eating mice and rats. Without hawks, more mice and rats would
survive. This could increase the populations of mice and rats in surrounding areas. So, the correct
answer is, "B. Populations of mice and rats would increase."
Invalid Rationale:
When the hawks were no longer in the area, the chickens had to compete with other predators for
food. This led to a decrease in the chicken population. Since the hawks were no longer around, the
populations of mice and rats would increase. Therefore, the chickens would have a lower rate of
disease because there were fewer of them to catch diseases. Therefore, the answer is B.
DROP Example:
Passage:
Skopje, as the Republic of Macedonia as a whole, is characterised by a large ethnic diversity. The
city is located in a region where Ethnic Albanians and Macedonians meet, and it welcomed Romani,
Turks, Jews and Serbs throughout its history. Skopje was mainly a Muslim city until the 19th century,
when large numbers of Christians started to settle there. According to the 2002 census, Macedonians
were the largest ethnic group in Skopje, with 338,358 inhabitants, or 66.75% of the population.
Then came Albanians with 103,891 inhabitants (20.49%), Roma people with 23,475 (4.63%), Serbs
(14,298 inhabitants), Turks (8,595), Bosniaks (7,585) and Vlachs (2,557). 8,167 people did not
belong to any of these groups.
Question:
Who was the second most populous group in Skopje?
Valid Rationale:
The passage states that Albanians were the second most populous group in Skopje, with 103,891
inhabitants.The passage then goes on to list other groups, and mentions that the largest group is
Macedonians.This indicates that the second most populous group is Albanians.
Invalid Rationale:
The passage mentions that Macedonians were the largest ethnic group in Skopje, with 338,358
inhabitants, or 66.75% of the population. According to the passage, the second most populous group
in Skopje was Albanians, with 103,891 inhabitants, or 20.49% of the population. Albanians were
the second most populous group, and the largest ethnic group was Macedonians, and therefore, the
Macedonians were the second most populous group in Skopje.
StrategyQA Example:
Question:
Do members of the Supreme Court of the United States have longer terms than most senators?
Valid Rationale:
A member of the Supreme Court of the United States is appointed to serve for life. A senator’s term
is six years. Thus, a member of the Supreme Court of the United States has a longer term than most
senators. So the answer is yes.
Invalid Rationale:
The terms of Supreme Court justices are 9 months, which is longer than the 6-year term of a senator.
Therefore, the answer is yes.
Table 4: Examples of GPT-4’s validity judgments on reasoning paths generated by Llama-2 7B across three datasets:
ARC (top), DROP (middle), and StrategyQA (bottom). The valid rationales demonstrate sound reasoning, while the
invalid rationales contain factual errors and logical inconsistencies.Question:
Did University of Pittsburgh founder have great deal in common with Judith Sheindlin?
Rationale selected by REPS:
Judith Sheindlin is 76 years old. The founder of the University of Pittsburgh died in 1811, which
was 128 years before Judith Sheindlin was born. Thus, the founder of the University of Pittsburgh
did not have a great deal in common with Judith Sheindlin.
Rationale selected by Baseline:
The founder of the University of Pittsburgh was William Pitt. Judith Sheindlin ("The Judge") has a
first name that is similar to William. Thus, they have a great deal in common.
Question:
Is Disney associated with Los Angeles County?
Rationale selected by REPS:
Disney has theme parks in Los Angeles County, California. Thus, Disney is associated with Los
Angeles County.
Rationale selected by Baseline:
Disney is a large company that has many locations. Disneyland is located in Anaheim, which is 24
miles south of Los Angeles. Thus, Disney does not have a location in Los Angeles County.
Question:
Could a bee hummingbird balance a scale with a single pea on it?
Rationale selected by REPS:
A bee hummingbird weighs about 0.4g. A single pea weighs about 0.2g. Thus, a bee hummingbird
could balance a scale with a single pea on it.
Rationale selected by Baseline:
The mass of a bee is about 1.8 milligrams. The mass of a pea is about 1.5 grams. Thus, a bee could
not balance a scale with a single pea on it.
Question:
Can photography be considered abstract art?
Rationale selected by REPS:
Photography is art, but it is not abstract art. Photography can be realistic or non-realistic. Abstract
art is non-realistic. Thus, photography cannot be considered abstract art.
Rationale selected by Baseline:
Photography is a type of art. Abstract art is a type of art. Thus, photography can be considered
abstract art.
Question:
Are aggressive bumblebees suicidal?
Rationale selected by REPS:
Bumblebees can become aggressive when they are protecting their colony. These bees can be very
aggressive. Thus, aggressive bumblebees are not suicidal.
Rationale selected by Baseline:
Bumblebees are both aggressive and suicidal.
Table 5: Examples of rationales selected by REPS and the baseline verifier for questions from StrategyQA.Question:
Devil facial tumor disease (DFTD) is a disease that is decimating the population of Tasmanian devils.
The disease passes from one animal to another through bites and is caused by parasites. The parasites
cause cancerous tumors that spread throughout an infected animal’s body and kill it. What is the best
description of DFTD?
Choices:
A. a non-infectious, cell-cycle disease B. an infectious, cell-cycle disease C. a non-infectious, chronic
disease D. an infectious, chronic disease
Rationale selected by REPS:
DFTD is a deadly infectious disease that is spread by parasites and causes cancerous tumors that
spread throughout an infected animal’s body. Thus, DFTD is an infectious, cell-cycle disease.
Rationale selected by Baseline:
The disease is caused by parasites, which are foreign organisms that invade a host body and cause
disease. The parasites cause cancerous tumors that spread throughout the body and kill the host.
Therefore, DFTD is an infectious, cell-cycle disease.
Question:
A type of small mammal from the mountain regions of the western United States makes its home out
of piles of rock. During summer months, the mammal places grasses and seeds in protected places
in the rock piles. Which of the following is the most likely reason for this behavior?
Choices:
A. to prepare for migration before winter B. to provide warmth during the cold winter months C. to
store food that will be eaten over the winter months D. to protect the grasses and seeds from decay
before winter
Rationale selected by REPS:
This is the behavior of a prairie dog, which stores grass, seeds, and other food items in rock piles
during summer months. Prairie dogs will consume the stored food during winter months. Thus, the
prairie dog’s behavior is to store food for use during the winter months.
Rationale selected by Baseline:
To protect and preserve its food, the mammal buries grasses and seeds in piles of rock. Grasses and
seeds decay over time. Thus, the mammal buries its food to protect it from decay.
Question:
According to cell classification, prokaryotic cells are separated from eukaryotic cells. Which feature
is often used to distinguish prokaryotic cells from eukaryotic cells?
Choices:
A. life processes B. size differences C. plasma membranes D. energy molecules
Rationale selected by REPS:
Prokaryotes are bacteria and archaea, whereas eukaryotes are animals, plants, fungi, and other
multicellular organisms. Prokaryotes do not contain membrane-bound organelles (structures) such
as the nucleus. Therefore, the presence or absence of membrane-bound organelles is an important
distinguishing feature of prokaryotes and eukaryotes.
Rationale selected by Baseline:
Prokaryotic cells lack a nucleus, while eukaryotic cells contain a nucleus. Thus, prokaryotic cells are
distinguished from eukaryotic cells by the presence of a nucleus.
Table 6: Examples of rationales selected by REPS and the baseline verifier for questions from ARC-Challenge.Passage:
Taunton has four art galleries: Taunton Art Association , Hughes/Donahue Gallery, Art Euphoric,
and the Trescott Street Gallery. The Taunton Art Association founded in 1973, but had it roots at the
Girl’s Club in the early 1960s. Hughes/Donahue Gallery founded in 2007, a local community gallery
serving local Taunton artists, surrounding areas of Southeastern Massachusetts and including the
cities of Providence, and Washington DC. Art Euphoric founded in 2008 has both visual and craft
exhibits and sales. The Trescott Street Gallery founded in 2012, primarily a visual arts gallery, but
also exhibits crafts.
Question:
Which gallery was founded first, Hughes/Donahue Gallery or Art Euphoric?
Rationale selected by REPS:
The passage mentions that Hughes/Donahue Gallery was founded in 2007, and Art Euphoric was
founded in 2008. The gallery that was founded first is Hughes/Donahue Gallery.
Rationale selected by Baseline:
The passage mentions that the Taunton Art Association was founded in 1973. The passage also
mentions that Art Euphoric was founded in 2008. Therefore, Art Euphoric was founded after the
Taunton Art Association. The passage does not mention when the Hughes/Donahue Gallery and the
Trescott Street Gallery were founded.
Passage:
In 1085, Guadalajara was retaken by the Christian forces of Alfonso VI . The chronicles say that
the Christian army was led by Alvar Fanez de Minaya, one of the lieutenants of El Cid. From 1085
until the Battle of Las Navas de Tolosa in 1212, the city suffered wars against the Almoravid and
the Almohad Empires. In spite of the wars, the Christian population could definitely settle down
in the area thanks to the repopulation with people from the North who received their first fuero in
1133 from Alfonso VII. In 1219, the king Fernando III gave a new fuero to the city. During the
reign of Alfonso X of Castile, the protection of the king allowed the city to develop its economy by
protecting merchants and allowing markets.
Question:
How many years after the people of the North received their first fuero from Alfonso VII did king
Fernando III give a new fuero to the city?
Rationale selected by REPS:
After the people of the North received their first fuero from Alfonso VII in 1133, Fernando III
granted a new fuero to the city in 1219. Therefore, the city received its fuero 86 years after the first.
Rationale selected by Baseline:
The passage mentions that the first fuero was given in 1133 by Alfonso VII. By looking at the dates,
it can be concluded that 287 years later, the people of the North received their first fuero. After this,
86 more years passed before Fernando III gave a new fuero to the city. Thus, it took 373 years from
the time the people of the North received their first fuero to the time Fernando III gave a new fuero
to the city.
Table 7: Examples of rationales selected by REPS and the baseline verifier for questions from DROP.