Private Language Models via Truncated Laplacian Mechanism
Tianhao Huang1,2,3 *, Tao Yang1,2,3 *, Ivan Habernal4, Lijie Hu1,2, Di Wang1,2†,
1Provable Responsible AI and Data Analytics (PRADA) Lab,
2King Abdullah University of Science and Technology,3Nankai University,
4Research Center Trustworthy Data Science and Security of the University Alliance Ruhr,
Faculty of Computer Science, Ruhr University Bochum,
{tianhao.huang, tao.yang, lijie.hu, di.wang}@kaust.edu.sa
ivan.habernal@ruhr-uni-bochum.de
Abstract
Deep learning models for NLP tasks are prone
to variants of privacy attacks. To prevent pri-
vacy leakage, researchers have investigated
word-level perturbations, relying on the for-
mal guarantees of differential privacy (DP) in
the embedding space. However, many exist-
ing approaches either achieve unsatisfactory
performance in the high privacy regime when
using the Laplacian or Gaussian mechanism,
or resort to weaker relaxations of DP that are
inferior to the canonical DP in terms of privacy
strength. This raises the question of whether a
new method for private word embedding can
be designed to overcome these limitations.
In this paper, we propose a novel private em-
bedding method called the high dimensional
truncated Laplacian mechanism. Specifically,
we introduce a non-trivial extension of the trun-
cated Laplacian mechanism, which was pre-
viously only investigated in one-dimensional
space cases. Theoretically, we show that our
method has a lower variance compared to the
previous private word embedding methods. To
further validate its effectiveness, we conduct
comprehensive experiments on private embed-
ding and downstream tasks using three datasets.
Remarkably, even in the high privacy regime,
our approach only incurs a slight decrease in
utility compared to the non-private scenario.
1 Introduction
The recent developments of deep learning have led
to significant success in various tasks in Natural
Language Processing (NLP), from next word pre-
diction in mobile keyboards (Ramaswamy et al.,
2019), to critical tasks like predicting patient health
conditions from clinical records (Yao et al., 2019).
However, such applications may always involve
user-generated textual data as the training dataset,
*Equal contribution. Part of the work was done as a re-
search intern at PRADA Lab.
†Corresponding author.which contains sensitive information. To address
privacy concerns, text anonymization (Anandan
et al., 2012; Pilán et al., 2022) has been commonly
used, which involves identifying sensitive attributes
and replacing them with alternative values. Nev-
ertheless, such heuristic approaches become inef-
fective as deep neural networks often tend to mem-
orize training data, making them susceptible to
information leakage about the training data (Shokri
et al., 2017; Carlini et al., 2021, 2019). One way
that takes into account the limitations of existing ap-
proaches is designing Differentially Private (DP) al-
gorithms. Differential privacy (Dwork et al., 2006)
is resilient to arbitrary side information that might
be available to attackers and has become a de facto
method for private data analysis (Xiang et al., 2024;
Wang et al., 2020b; Xiang et al., 2023; Wang et al.,
2020a; Su et al., 2022; Wang et al., 2019b; Hu et al.,
2023; Wang et al., 2023b, 2017, 2019a, 2018; Xue
et al., 2021; Huai et al., 2019; Wang et al., 2023a;
Wang and Xu, 2020; Huai et al., 2020; Hu et al.,
2022).
Recently, there has been significant research fo-
cusing on differentially private (DP) versions of
word embedding from various perspectives (Yue
et al., 2021; Feyisetan et al., 2019; Krishna et al.,
2021; Feyisetan et al., 2020; Xu et al., 2021a,b;
Carvalho et al., 2021b,a; Habernal, 2021, 2022).
However, there are still some shortcomings in these
approaches. On the one hand, several works con-
sider adding Laplacian or Gaussian noise to the
embedding space to ensure DP (Habernal, 2021;
Krishna et al., 2021; Habernal, 2022). However,
these mechanisms suffer from high noise levels, re-
sulting in low utility, especially in the high privacy
regime when the privacy parameter ( ϵ) is small.
Moreover, these mechanisms can even alter the
semantics of sentences (see Fig.1). On the other
hand, there is a growing body of work that focuses
on a relaxation of the canonical definition of DP,
known as metric DP, which can achieve better per-arXiv:2410.08027v1  [cs.CL]  10 Oct 2024Comparison of Private Embedding
Original : Oh and we came on a Saturday night around 11:30 for context. ( →Privacy Leakage)
Trlaplace : Oh and we came on a Saturday night around 9:30pm for <unk> ( →Private and Fluent)
Laplace : Oh and we came on a Saturday night around around for <unk> ( →Semantic Problem)
Gaussian : Oh and we came on a Saturday night around 11:30 for <unk> ( →Privacy Leakage)
Figure 1: An example of (private) text re-write for different mechanisms with ϵ= 0.1.
formance. However, as a relaxed notion of DP,
Metric DP cannot provide the same level of strong
privacy guarantees as the canonical DP (Mattern
et al., 2022). This raises the question of whether
we can develop improved private word embedding
mechanisms within the framework of canonical DP
that can have comparable performance with exist-
ing metric DP-based methods .
In this paper, we provide an affirmative answer to
the previous question by proposing a novel private
mechanism for word embedding. Our approach is
inspired by the superior performance of the trun-
cated Laplacian mechanism in one-dimensional
space (Geng et al., 2020). However, it remains un-
clear whether this superiority can extend to high
dimensional cases, as directly extending the one-
dimensional truncated Laplacian mechanism is
challenging. To bridge this gap, we develop a
high dimensional truncated Laplacian mechanism
(TrLaplace), which is a non-trivial extension of
the one-dimensional case. Theoretically, we show
that compared with Laplacian and Gaussian mech-
anisms for private word embedding, TrLaplace-
based private embedding has a lower variance.
Moreover, we also conduct intensive experiments
on both private embedding and downstream tasks
to show our approach significantly outperforms the
previous DP-based methods in the high privacy
regime, and it will not drop much accuracy and util-
ity compared with the non-private case. Moreover,
compared to the existing metric DP-based method,
our mechanism has even better performance for
privacy tests while also keeping comparable perfor-
mance for downstream tasks.
2 Related Work
Recent years have seen substantial advancements
in language models within differential privacy (DP)
frameworks. Due to the space limit, here we only
mention the existing literature on private word em-
bedding. We refer the readers to the survey (Hu
et al., 2024) for more details.Current research on private word embeddings
can be broadly categorized into two approaches:
original DP-based methods and metric DP-based
methods. The seminal work in the original DP cate-
gory by Lyu et al. (2020b) introduces a framework
utilizing the Unary Encoding mechanism. This
approach was subsequently refined by Plant et al.
(2021). Further improvements were made by Lyu
et al. (2020a), who proposed a dropout technique
for perturbed embeddings to enhance downstream
task fairness. However, Qu et al. (2021) identify
a critical privacy issue in Lyu et al. (2020a), not-
ing that it requires access to users’ raw data for
fine-tuning during the training phase. Other no-
table contributions include works by Krishna et al.
(2021), Habernal (2021), and Alnasser et al. (2021),
who explore privatizing word embeddings. Krishna
et al. (2021) and Alnasser et al. (2021) propose
ADePT, an auto-encoder-based DP algorithm. Un-
fortunately, Habernal (2021) points out that ADePT
is not differentially private by thorough theoretical
proof. Igamberdiev et al. (2022) address repro-
ducibility by providing source code for DP Auto-
Encoder methods. In this paper, we aim to improve
the performance of the mechanisms in Igamberdiev
et al. (2022).
In the realm of metric DP, Feyisetan et al.
(2020) first study this problem and provide a gen-
eral perturbation-and-projection framework. Xu
et al. (2020) reconsider this problem setting, re-
placing the Euclidean distance with the Maha-
lanobis distance to improve the utility. Subse-
quently, Xu et al. (2021b) introduce the Vickrey
mechanism to further refine the utility in the pro-
jection step. To address the limitations of the mul-
tivariate Laplace mechanism, Xu et al. (2021a) and
Carvalho et al. (2021b) propose a Truncated Gum-
bel Noise method. Feyisetan and Kasiviswanathan
(2021) tackle high-dimensionality issues using ran-
dom projection. Additionally, Feyisetan et al.
(2019) define hyperbolic embeddings and utilize
the Metropolis-Hastings algorithm for samplingfrom hyperbolic distributions. More recently, Tang
et al. (2020) explore differential privacy with vary-
ing privacy levels for different words. Arnold et al.
(2023a) introduce sense embeddings with a sense
disambiguation step prior to noise injection, and
Arnold et al. (2023b) address common semantic
context issues in prior private embedding mech-
anisms. It is crucial to clarify that the objective
of this work is the privatization of embedded out-
puts rather than the embedded methods themselves.
The pre-trained initial embedding methods and the
corresponding embeddings of all words in the vo-
cabulary are treated as public knowledge. This
distinction is significant because it allows us to
perform projections without incurring additional
privacy costs. By leveraging publicly available pre-
trained embeddings, our method demonstrates how
effective privacy-preserving techniques can be im-
plemented while still utilizing existing resources.
3 Preliminaries
Differential Privacy is a data post-processing tech-
nique designed to ensure data privacy by adding
confusion to potential attackers. Specifically, sup-
pose there is one dataset noted as D, and we change
or delete one data record in this dataset which we
callD′. If the output distributions of DandD′
are close enough, then we cannot distinguish these
two distributions, i.e., we cannot infer whether the
deleted or replaced data sample is really in this
dataset. The formal details are given by (Dwork
et al., 2006). Note that in the definition of DP, adja-
cency is a key notion. One of the commonly used
adjacency definitions is that two datasets Sand
S′are adjacent (denoted as S∼S′) ifS′can be
obtained by modifying one record in S.
Definition 1 Given a domain of dataset X. A
randomized algorithm A:X 7→ R is(ε, δ)-
differentially private (DP) if for all adjacent
datasets S, S′with each sample is in Xand for
allT⊆ R , the following holds
Pr(A(S)∈T)≤exp(ε) Pr(A(S′)∈T) +δ.
When δ= 0, we call the algorithm Aisε-DP .
In this work, we adopt a similar setting to previ-
ous research on private word embedding (Feyisetan
et al., 2020; Xu et al., 2021a; Krishna et al., 2021).
We consider a scenario where a user inputs a word
wfrom a discrete fixed vocabulary W. Our goal is
to preserve the user’s privacy with respect to her/hisword. To achieve this goal, we aim to design an
algorithm that accepts was input and whose distri-
bution of output is close to the case where w′∈ W
is the input, with w′̸=wis any other word. From
the attacker’s perspective, based on the output, he
cannot distinguish whether the user’s input word is
worw′as their output distributions are almost the
same. Formally, we have the following definition.
Definition 2 Given a discrete vocabulary W, a
randomized algorithm A:W 7→ R is word-level
(ϵ, δ)-differentially private (DP) if for all pair of
words w, w′∈ W and for all T⊆ R we have
P(A(w)∈T)≤eϵP(A(w′)∈T) +δ.When
δ= 0, we call the algorithm Aisϵ-DP .
In this paper, we assume the user holds a sentence
s=w1w2···wnwithnwords. And we aim to de-
sign an (ϵ, δ)-DP algorithm, which is private w.r.t.
each word wi.
4 Private Embedding via Truncated
Laplacian Mechanism
In this section, we will provide details of our
method. Generally speaking, for each token wi,
to achieve DP, our approach consists of three steps.
First, each token wiis mapped to an d-dimensional
pre-trained word embedding ϕ(wi). And we per-
form a clipping step to get a clipped embedding:
CLIPEmb( wi) =ϕ(wi) min{1,C
∥ϕ(wi)∥2}, (1)
where the threshold C > 0is a hyper-parameter.
In the second step, we add some random noise to
the clipped embedding vector to make it satisfies
DP. Finally, we will perform the projection step by
finding the nearest word ˆwito the perturbed and
clipped embedding vector within the embedding
space:
ˆwi= arg min
w∈W∥ϕ(w)−CLIPEmb( wi)−η∥2,
(2)
where ηis the randomized noise we add in the
second step. See Algorithm 1 for details.
It is notable that the goal of clipping is to make
theℓ2-norm of embedding vector be bounded so
that we can adding noise to ensure DP, such as
the Laplacian mechanism or Gaussian mechanism
(Dwork and Roth, 2014).
Theorem 1 (Laplacian Mechanism) Suppose
CLIPEmb( w)∈Rddenote the clipped em-
bedding vector with threshold C. Then theAlgorithm 1 Privacy Preserving Mechanism
Input: String s=w1w2. . . w n, clipping thresh-
oldC, privacy parameter ϵ >0.
Output: String ˆs= ˆw1ˆw2. . .ˆwn.
1:for all i∈ {1, . . . , n }do
2: Sample ηfrom the truncated Laplacian dis-
tribution in Theorem 3.
3: Obtain the perturbed clipped embedding
ri= CLIPEmb( wi) +η.
4: Letˆwi= Proj ( ri)as in (2).
5:end for
6:return ˆs= ˆw1ˆw2. . .ˆwn.
mechanism Alap(w) = CLIPEmb( w) +η1is
ϵ-DP , where η1= (η1,1,···, η1,d)andηi,jis
drawn from a Laplacian Distribution Lap(∆1(f)
ϵ)
with ∆1= 2√
dC. For a parameter λ, the
Laplacian distribution has the density function
Lap(λ)(x) =1
2λexp(−x
λ).
Theorem 2 (Gaussian Mechanism) Suppose
CLIPEmb( w)∈Rddenote the clipped embed-
ding vector with threshold C. Then the mechanism
Agau(w) = CLIPEmb( w)+η2is(ϵ, δ)-DP when
ϵ≤1, where η2∼ N(0,8C2ln(1.25/δ)
ϵ2 Id)is drawn
from a Gaussian distribution.
In the following we propose an improved mech-
anism namely high dimensional truncated Lapla-
cian mechanism. Before that we first recall the
probability density function of the one-dimensional
truncated Laplacian distribution, which could be
written as the following with some appropriate con-
stants α, A andB:
fTLap(x) =(
1
Be−α|x|,forx∈[−A, A]
0, otherwise.(3)
In our mechanism, we add high dimensional trun-
cated Laplacian noise to the clipped embedding
vector. Here each coordinate of the noise is i.i.d.
sampled from a truncated Laplacian distribution
with some specific α, A andB.
Remark 1 It is notable that although using trun-
cated Laplacian noise to ensure DP has been stud-
ied quite well (Geng et al., 2020; Sommer et al.,
2021), all of them only considered the case where
the dimension d= 1 and their methods cannot
extend to the case where d > 1. For example,
(Geng et al., 2020) only shows that adding noise
with density function (3) with A=∆1
ϵlog(1 +eϵ
2δ)
andα=ϵ
∆1can ensure (ϵ, δ)-DP . Compared with
Figure 2: Privacy Test. Curves of the value Nwwith
privacy budget ϵfor Yelp dataset.
the high dimensional case in Theorem 3 we can
see the constant Ais more complicated and the
proof is also different. Thus, our mechanism can-
not be considered as a trivial extension of the one-
dimensional truncated Laplacian mechanism. Sec-
ondly, while the Laplacian mechanism can guaran-
teeϵ-DP , the truncated one can only ensure (ϵ, δ)-
DP , which is the same as in the one-dimension case.
However, as we will see below, our mechanism
is superior to Laplacian mechanism for utility. It is
also notable that we need to assume ϵ≤2δ1
d√
d,
this is reasonable since we always wish ϵto be as
small as possible, as large ϵindicates the algorithm
is no longer private. If we want large ϵ >2δ1
d√
d,
we can use the trick of adding dummy dimension to
the vector to increase its dimensionality manually
and then projecting back to the original space after
adding noise.
5 Theoretical Sensitivity Analysis
In the last section, we introduce our truncated lapla-
cian mechanism, we will analyze its sensitivity and
proof our claim in this section.
Theorem 3 Suppose CLIPEmb( w)∈Rdis the
clipped embedding vector with threshold C. Define
∆∞= 2Cand∆1= 2√
dC. For ϵ≤2δ1
d√
d, if
α=ϵ
∆1, A=−∆1
ϵlog(1−ϵ
2δ1
d√
d)
B=2(1−e−αA)
α=∆∞
δ1
d,
then the mechanism A(w) = CLIPEmb( w) +η
is(ϵ, δ)-DP , where η= (η1,···, η1)and each ηi
has the density function as in (3) with the above
parameters.Table 1: Privacy Test. Performance under fastText Embedding initialization for the non-private case ( ϵ=∞)
and three mechanisms (Gaussian, Laplacian and TrLaplacian) on Yelp dataset. The privacy budget ranges from
0.05 to 20. ↑means a higher value under this metric indicates better results, and ↓means the opposite. The best
performance is bolded . The same symbols are used in the following tables by default.
Original Gaussian Laplacian TrLaplacian
Privacy budget ϵ ∞ 0.05 0.1 0.2 0.5 0.05 0.1 0.2 0.5 0.05 0.1 0.2 0.5
Loss↓ 3.35 35.01 29.33 9.31 4.50 36.23 29.69 17.15 5.58 1.20 1.20 1.26 1.23
Rouge1↑ 87.8 12.72 28.68 77.95 86.90 10.99 27.96 58.97 85.16 92.43 92.67 92.29 92.43
BLEU↑ 8.929 8.226 8.745 8.918 8.931 8.998 8.681 8.898 8.931 8.937 8.938 8.937 8.938
Nw↑ 0.713 0.138 0.232 0.661 0.765 0.058 0.225 0.484 0.753 0.813 0.807 0.804 0.813
BERT-S ↑ 0.967 0.864 0.873 0.945 0.966 0.857 0.867 0.908 0.962 0.981 0.978 0.979 0.978
Original Gaussian Laplacian TrLaplacian
Privacy budget ϵ ∞ 1 5 10 20 1 5 10 20 1 5 10 20
Loss↓ 3.35 3.10 1.68 1.48 1.29 3.60 1.55 1.53 1.51 1.22 1.25 1.28 1.27
Rouge1↑ 87.8 89.47 92.06 92.40 92.49 88.17 91.87 91.90 91.91 92.42 92.35 92.34 92.31
BLEU↑ 8.929 8.936 8.937 8.936 8.936 8.935 8.937 8.936 8.934 8.938 8.939 8.937 8.938
Nw↑ 0.713 0.794 0.809 0.804 0.813 0.758 0.801 0.795 0.792 0.807 0.802 0.800 0.808
BERT-S ↑ 0.967 0.976 0.977 0.978 0.980 0.967 0.978 0.976 0.977 0.979 0.978 0.978 0.980
In the following, we will show our mechanism
has lower variance than the Laplacian and Gaussian
mechanism, which indicates that our method is
superior theoretically.
Theorem 4 The variance of mechanism Ain Theo-
rem 3 is lower than the variance of Laplacian mech-
anism and Gaussian mechanism when δ≤1
ed.
6 Experiments
In this section, we conduct experiments for our
method based on two parts: DP text re-write
for fine-tuning (private embedding) and down-
stream tasks (sentiment analysis). We have
open-sourced our code on https://github.com/
kaustpradalab/TrLap .
6.1 Experimental Setup
Datasets. For the DP text re-write task, we use
the Yelp *and Yahoo (Yang et al., 2019) datasets.
The Yelp Open dataset is a subset of Yelp business,
review, and user data with a training size of 8,539
and a testing size of 2,174. The Yahoo dataset con-
tains 14,180 news articles and 34,022 click events.
All data are collated to obtain a training, validation,
and testing set segmented by sentences.
For downstream tasks, we use the SST-2 dataset
(Socher et al., 2013) for the sentiment analysis task,
from which we use 68,221 heavily polarized re-
views from the Internet Movie Database. We divide
the SST-2 dataset into an 80:20 ratio for training
and testing. The training set consists of 54,576 re-
views and the testing set consists of 13,645 reviews.
*https://www.yelp.com/dataset/We use the AG News dataset (Zhang et al., 2015)
which includes news articles on the four main top-
ics in the AG News corpus for the topic classifica-
tion task. Following Meisenbacher et al. (2024),
we randomly select a sample of 6,000 articles from
each topic, totaling 16,000 for training, 4,000 for
validation, and 4,000 for testing. The statistics of
the datasets are presented in Tab. 4 in Appendix A.
Baseline. For DP text re-write, although Krishna
et al. (2021) use the Laplacian mechanism to the
sentence level DP instead of word level as in Defini-
tion 2. However, as Habernal (2021) mentions, the
approach in Krishna et al. (2021) is not DP. Thus,
here we will not compare with their method, and we
will use the Laplacian and Gaussian mechanisms
for the clipped embedding as baseline methods. For
private fine-tuning, as we mentioned previously, all
the previous methods only focus on metric DP in-
stead of the original DP in Definition 2. Thus, our
method is incomparable with theirs, and we will
still use Laplacian and Gaussian mechanisms as
baselines.
For utility test, we compare our method with
Gaussian and Laplacian mechanism for the origi-
nal DP notation, as well as Calibrated Multivari-
ate Perturbations (CMP) (Feyisetan et al., 2020),
Mahalanobis Mechanism (Xu et al., 2020) and Pri-
vate Text Embedding (PTE) (Feyisetan and Ka-
siviswanathan, 2021), which are benchmark metric
DP-based methods and have better utility than the
original DP-based ones.
Evaluation Metrics. We use the loss of cross-
entropy to measure the performance of languageTable 2: Privacy Test. Performance under GloVe Embedding initialization for the non-private case ( ϵ=∞) and
the three mechanisms, where the privacy budget ranges from 0.05 to 0.5.
Original Gaussian Laplacian TrLaplacian
Privacy budget ϵ ∞ 0.05 0.1 0.2 0.5 0.05 0.1 0.2 0.5 0.05 0.1 0.2 0.5
Loss↓ 2.95 51.25 26.66 9.92 5.97 51.43 37.86 15.35 7.31 2.89 2.86 2.84 3.04
Rouge1↑ 92.37 14.01 59.52 83.61 89.06 13.02 43.30 75.77 86.98 92.44 92.43 92.41 92.25
Yahoo BLEU↑ 8.501 9.286 8.418 8.489 8.499 9.132 8.287 8.474 8.493 8.499 8.500 8.497 8.504
Nw↑ 0.703 0.072 0.511 0.595 0.628 0.066 0.334 0.566 0.642 0.706 0.682 0.666 0.662
BERT-S ↑ 0.975 0.849 0.908 0.955 0.963 0.839 0.889 0.942 0.959 0.976 0.971 0.971 0.971
Loss↓ 3.07 34.67 21.62 10.61 5.98 36.00 34.64 14.86 7.38 2.98 2.99 3.02 2.94
Rouge1↑ 89.40 15.97 48.89 76.48 84.97 12.60 14.68 66.62 82.08 89.45 89.47 89.34 89.54
Yelp BLEU ↑ 8.934 8.976 8.850 8.926 8.930 8.607 8.916 8.913 8.928 8.931 8.935 8.936 8.936
Nw↑ 0.706 0.144 0.381 0.608 0.694 0.052 0.138 0.525 0.646 0.705 0.721 0.722 0.725
BERT-S ↑ 0.973 0.874 0.895 0.943 0.964 0.855 0.874 0.927 0.952 0.971 0.973 0.971 0.972
Table 3: Privacy Test. Performance under GloVe Embedding initialization for the non-private case ( ϵ=∞) and
the three mechanisms, where the privacy budget ranges from 1 to 20.
Original Gaussian Laplacian TrLaplacian
Privacy budget ϵ ∞ 1 5 10 20 1 5 10 20 1 5 10 20
Loss↓ 2.95 4.28 3.01 3.03 2.98 4.93 3.24 3.05 3.13 2.85 2.97 2.92 2.81
Rouge1↑ 92.37 90.97 92.27 92.16 92.19 90.02 92.09 92.28 92.26 92.41 92.35 92.24 92.45
Yahoo BLEU↑ 8.501 8.501 8.501 8.499 8.500 8.503 8.501 8.502 8.500 8.498 8.501 8.499 8.499
Nw↑ 0.703 0.637 0.680 0.664 0.672 0.660 0.658 0.675 0.655 0.674 0.670 0.702 0.680
BERT-S ↑ 0.975 0.968 0.973 0.971 0.972 0.966 0.970 0.971 0.971 0.974 0.972 0.975 0.974
YelpLoss↓ 3.07 4.74 3.14 3.13 2.97 5.02 3.30 3.66 3.17 2.93 3.03 3.00 2.98
Rouge1↑ 89.40 86.63 89.13 89.27 89.80 86.43 89.04 88.15 89.23 89.68 89.40 89.37 89.60
BLEU↑ 8.934 8.933 8.936 8.933 8.944 8.931 8.932 8.933 8.934 8.934 8.931 8.934 8.938
Nw↑ 0.706 0.708 0.725 0.708 0.739 0.691 0.721 0.704 0.699 0.724 0.700 0.712 0.740
BERT-S ↑ 0.973 0.969 0.975 0.975 0.975 0.964 0.969 0.969 0.968 0.975 0.971 0.976 0.976
models. Specifically, cross-entropy is mainly used
to determine how similar the actual output is to
the expected output. Smaller model loss indi-
cates less noise added to perturb the text. Addi-
tionally, we will use Rouge1 and BLEU scores.
Rouge1 (Lin, 2004) calculates recall using standard
results and the number of 1-grams co-occurring
in the auto-generated text. Similarly, BLEU (Pa-
pineni et al., 2002) measures the similarity be-
tween standard results and automatically generated
text. Rouge1 measures word-level accuracy, while
BLEU measures sentence fluency. Moreover, we
use BERTScore (Zhang* et al., 2020) to measure
the semantic similarity of the perturbed sentence
with the original one. To measure the privacy-
preserving ability, we use the percentage of Nw
(Feyisetan et al., 2020), which is the number of
words that are not replaced. Thus, under the same
privacy budget, larger Nwwill be better (we want
to change fewer words for accuracy).
Implementation Details. As an embedding can
be considered as an initialization of the model,
here we will consider three different initialization:
Random embedding (Wieting and Kiela, 2019),GloVe (Pennington et al., 2014) and fastText (Bo-
janowski et al., 2017). We conduct experiments on
these embeddings and the subsequent fine-tuning
in the DP model via our mechanism. Each pre-
trained word embedding is a 300-dimensional vec-
tor, and the size of considered vocabulary is 104.
For privacy budget, we set δ=1
4d, and we con-
sider both the high privacy regime where ϵ∈
{0.05,0.1,0.2,0.5}and the low privacy regime
ϵ∈ {1,5,10,20}. For large ϵwe will use our
previous dummy dimension trick ( d= 500 for
ϵ= 10 andd= 1700 forϵ= 20 ).
6.2 Privacy Experiment on Embedding
We first show the results on private embedding.
Specifically, we use GloVe or fastText for the ini-
tialization, and then use three different private em-
bedding mechanisms with different privacy bud-
gets. Noted that large ϵ >10is meaningless for
privacy, we concentrate more on a small privacy
budget in the main context.
Fig. 1 and 6 show the text after projecting the
clipped and perturbed embedding back to the word
domain in step 4 of Algorithm 1 for different mech-
anisms when ϵ= 0.1. We can see our method(a) Loss with ϵ= 0.05toϵ= 5.
 (b) Rouge1 with ϵ= 0.05toϵ= 5.
(c) BERT Score with ϵ= 0.05toϵ= 1.
(d) Loss with ϵ= 0.05toϵ= 5.
 (e) Rouge1 with ϵ= 0.05toϵ= 5.
(f) BERT Score with ϵ= 0.05toϵ= 1.
Figure 3: Privacy-Utility Test . Curves of Loss, Rouge1 and BERTScore with different privacy budget ϵfor Yelp
(Upper) and Yahoo (Lower) datasets.
(TrLaplace) outperforms the other two methods
from both privacy and semantic perspectives, while
the Gaussian mechanism fails to obfuscate the time,
and the Laplacian mechanism totally replaces the
time by another word, which destroys the structure
of the sentence.
Tab. 2 and Tab. 3 are the results on different
metrics regarding private embedding with Glove
initialization and Tab. 1 is with fastText initializa-
tion. We also present the detailed trends w.r.t ϵfor
three mechanisms in Fig. 3. When ϵ <1, from
Tab. 2 we can see that for both Yahoo and Yelp, the
loss of Gaussian and Laplacian mechanisms will
be catastrophically large while our mechanism has
a much smaller loss. From Tab. 3 we can see we
have almost the same phenomenon when in the low
privacy regime. Moreover, for Rouge1, Trlapla-
cian also surpasses the other two mechanisms on
both datasets, indicating that our mechanism con-
sistently demonstrates superiority in lexical/syntac-
tic aspects. For BLEU, the gap between all three
mechanisms to the non-private case becomes small
for both two datasets. But our method still has a
slight advantage compared with the other two.
ForNwvalue, we can see in Fig. 2 and Fig. 5,
our mechanism outperforms the other two mech-anisms by changing less percentage of words to
achieve the same privacy level, which indicates our
method can exactly find sensitive words without
hurting other words, thus keeps semantic proper-
ties. For BERTScore, our mechanism is almost the
same as the non-private case, while there is a larger
gap for others. It is notable that, in almost all exper-
iments our mechanism is the best, and the Gaussian
mechanism is better than the Laplacian mechanism,
which matches our theorem. However, it becomes
less obvious when ϵis large. The main reason is
that when ϵis enough large the noise will be suffi-
ciently small and becomes nearly negligible, which
can also be supported by the proof of Theorem 4.
For evaluation metrics, our mechanism may even
be better than the non-private case, this may be due
to small noise that could improve generalization,
which is similar to adversarial training. Moreover.
in real-world scenarios, the privacy budget must be
very small as we just want to keep the word level of
privacy. This is because, in more realistic scenarios,
when we want to protect the whole sentence, the
total privacy budget required to privatize an entire
sequence may grow linearly with its length (due
to the composition theorem). Thus, the budget for
each word should be extremely small. This has also/uni00000013/uni00000011/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000014 /uni00000014 /uni00000014/uni00000013
/uni00000028/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051
/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051
/uni00000026/uni00000030/uni00000033
/uni00000030/uni00000044/uni0000004b/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000052/uni00000045/uni0000004c/uni00000056
/uni00000033/uni00000037/uni00000028
/uni00000037/uni00000055/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051(a) SST-2, fastText (0.9013)
/uni00000013/uni00000011/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000014 /uni00000014 /uni00000014/uni00000013
/uni00000028/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000018/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051
/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051
/uni00000026/uni00000030/uni00000033
/uni00000030/uni00000044/uni0000004b/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000052/uni00000045/uni0000004c/uni00000056
/uni00000033/uni00000037/uni00000028
/uni00000037/uni00000055/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051 (b) SST-2, GloVe (0.8997)
/uni00000013/uni00000011/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000014 /uni00000014 /uni00000014/uni00000013
/uni00000028/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000019/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051
/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051
/uni00000026/uni00000030/uni00000033
/uni00000030/uni00000044/uni0000004b/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000052/uni00000045/uni0000004c/uni00000056
/uni00000033/uni00000037/uni00000028
/uni00000037/uni00000055/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051 (c) SST-2, Random (0.5607)
/uni00000013/uni00000011/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000014 /uni00000014 /uni00000014/uni00000013
/uni00000028/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000015/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000017/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000019/uni00000013/uni00000011/uni0000001b/uni0000001c/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051
/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051
/uni00000026/uni00000030/uni00000033
/uni00000030/uni00000044/uni0000004b/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000052/uni00000045/uni0000004c/uni00000056
/uni00000033/uni00000037/uni00000028
/uni00000037/uni00000055/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051
(d) AG News, fastText (0.9138)
/uni00000013/uni00000011/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000014 /uni00000014 /uni00000014/uni00000013
/uni00000028/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000015/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000017/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000019/uni00000013/uni00000011/uni0000001b/uni0000001b/uni0000001b/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000013/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051
/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051
/uni00000026/uni00000030/uni00000033
/uni00000030/uni00000044/uni0000004b/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000052/uni00000045/uni0000004c/uni00000056
/uni00000033/uni00000037/uni00000028
/uni00000037/uni00000055/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051 (e) AG News, GloVe (0.9161)
/uni00000013/uni00000011/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000014 /uni00000014 /uni00000014/uni00000013
/uni00000028/uni00000053/uni00000056/uni0000004c/uni0000004f/uni00000052/uni00000051/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000018/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000018/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000013/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000015/uni00000018/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000019/uni0000001a/uni00000018/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni0000002a/uni00000044/uni00000058/uni00000056/uni00000056/uni0000004c/uni00000044/uni00000051
/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051
/uni00000026/uni00000030/uni00000033
/uni00000030/uni00000044/uni0000004b/uni00000044/uni0000004f/uni00000044/uni00000051/uni00000052/uni00000045/uni0000004c/uni00000056
/uni00000033/uni00000037/uni00000028
/uni00000037/uni00000055/uni0000002f/uni00000044/uni00000053/uni0000004f/uni00000044/uni00000046/uni0000004c/uni00000044/uni00000051 (f) AG News, Random (0.2462)
Figure 4: Classification accuracy for all experimental settings. Each set of data is the average result of five
experiments. We have included the baseline accuracy in parentheses in the subtitle of each subfigure.
Figure 5: Privacy Test. Curves of Nwvalue w.r.t. pri-
vacy budget ϵfor Yahoo dataset.
been mentioned in some previous work (Mattern
et al., 2022).
6.3 Utility of Private Fine-tuning
We present the classification accuracy results for
private fine-tuning across various embeddings and
privacy levels in Fig. 4. While it is acknowledged
that utility will be affected by the size of the bud-
get, with a smaller budget potentially leading tolower utility, our experimental results show that
our proposed method maintains relatively stable
utility across different budget choices. Specifically,
even in the high privacy regime ( ϵ= 0.01), our
approach only incurs a slight decrease in utility
compared to the non-private scenario. Similar ca-
pabilities of other methods can be observed in the
experimental results of Meisenbacher et al. (2024).
We can also observe the effect of different pre-
trained embeddings. It is evident that when us-
ing GloVe and fastText pretrained embeddings, all
methods achieve accuracy close to the baseline.
However, when using random embeddings, the
baseline accuracy is very low (0.5607 on SST-2,
0.2462 on AG News), but all methods significantly
improve the accuracy of downstream sentiment
analysis and topic classification tasks after training,
with accuracy reaching up to about 0.87 when ep-
silon=10. This indicates that all methods are effec-
tive after training. Specifically, our method, Trun-
cated Laplacian, maintains an accuracy of around
0.865 when trained on random embeddings, demon-strating that it maintains privacy while preserving
the quality of embeddings, thereby offering excel-
lent performance.
7 Conclusions
We introduce a novel method called the high dimen-
sional truncated Laplacian mechanism for private
embedding, which extends the one-dimensional
case to the high-dimensional case. Theoretical anal-
ysis demonstrates that our method exhibits lower
variance compared to existing private word embed-
ding techniques. Experiments show that even in
the high privacy regime, our approach incurs only a
minimal loss in utility compared to the non-private
case, which maintains privacy while preserving the
quality of embeddings for promising performance.
8 Limitations
First, the word level DP has the disadvantages of
length constraints and linear growth of privacy bud-
get (Mattern et al., 2022). However, such limita-
tions are rooted in the definition of DP instead of
our mechanism. Secondly, to ensure DP guaran-
tees, in this paper, our mechanism involves clipping
embedding vectors and adding calibrated noises,
which inevitably introduce errors to the outputs of
the task at hand. And these errors may affect dif-
ferent groups of individuals differently and may
cause unfairness issues. However, we still need to
mention that such unfairness issues are mainly due
to the definition of DP rather than our method, as
DP machine learning algorithms will always have a
disparate impact on model accuracy (Bagdasaryan
et al., 2019). Despite some limitations, word-level
DP still offers unique advantages and potential ap-
plications (Hu et al., 2024), and brings value to the
DP-NLP community.
9 Ethics Review
This paper presents work whose goal is to advance
the field of NLP. There are many potential societal
consequences of our work, none which we feel
must be specifically highlighted here.
Acknowledgments
Di Wang and Lijie Hu are supported in part by
the funding BAS/1/1689-01-01, URF/1/4663-01-
01, REI/1/5232-01-01, REI/1/5332-01-01, and
URF/1/5508-01-01 from KAUST, and funding
from KAUST - Center of Excellence for Gener-
ative AI, under award number 5940. Ivan Habernalis supported by the Research Center Trustworthy
Data Science and Security ( https://rc-trust.
ai), one of the Research Alliance centers within
the UA-Ruhr ( https://uaruhr.de ).
References
Walaa Alnasser, Ghazaleh Beigi, and Huan Liu. 2021.
Privacy preserving text representation learning using
BERT. In Social, Cultural, and Behavioral Modeling
- 14th International Conference, SBP-BRiMS 2021,
Virtual Event, July 6-9, 2021, Proceedings , volume
12720 of Lecture Notes in Computer Science , pages
91–100. Springer.
Balamurugan Anandan, Chris Clifton, Wei Jiang, Mum-
moorthy Murugesan, Pedro Pastrana-Camacho, and
Luo Si. 2012. t-plausibility: Generalizing words to
desensitize text. Trans. Data Priv. , 5(3):505–534.
Stefan Arnold, Dilara Yesilbas, and Sven Weinzierl.
2023a. Driving context into text-to-text privatization.
CoRR , abs/2306.01457.
Stefan Arnold, Dilara Yesilbas, and Sven Weinzierl.
2023b. Guiding text-to-text privatization by syntax.
CoRR , abs/2306.01471.
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly
Shmatikov. 2019. Differential privacy has disparate
impact on model accuracy. In Advances in Neural
Information Processing Systems 32: Annual Confer-
ence on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada , pages 15453–15462.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomás Mikolov. 2017. Enriching word vectors with
subword information. Trans. Assoc. Comput. Lin-
guistics , 5:135–146.
Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
Kos, and Dawn Song. 2019. The secret sharer: Eval-
uating and testing unintended memorization in neu-
ral networks. In 28th USENIX Security Symposium,
USENIX Security 2019, Santa Clara, CA, USA, Au-
gust 14-16, 2019 , pages 267–284. USENIX Associa-
tion.
Nicholas Carlini, Florian Tramèr, Eric Wallace,
Matthew Jagielski, Ariel Herbert-V oss, Katherine
Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úl-
far Erlingsson, Alina Oprea, and Colin Raffel. 2021.
Extracting training data from large language models.
In30th USENIX Security Symposium, USENIX Se-
curity 2021, August 11-13, 2021 , pages 2633–2650.
USENIX Association.
Ricardo Silva Carvalho, Theodore Vasiloudis, and
Oluwaseyi Feyisetan. 2021a. BRR: preserving pri-
vacy of text data efficiently on device. CoRR ,
abs/2107.07923.Ricardo Silva Carvalho, Theodore Vasiloudis, and
Oluwaseyi Feyisetan. 2021b. TEM: high util-
ity metric differential privacy on text. CoRR ,
abs/2107.07928.
Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, and
Inderjit S. Dhillon. 2021. Dp-normfedavg: Normal-
izing client updates for privacy-preserving federated
learning. CoRR , abs/2106.07094.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and
Adam D. Smith. 2006. Calibrating noise to sensitiv-
ity in private data analysis. In Theory of Cryptogra-
phy, Third Theory of Cryptography Conference, TCC
2006, New York, NY, USA, March 4-7, 2006, Pro-
ceedings , volume 3876 of Lecture Notes in Computer
Science , pages 265–284. Springer.
Cynthia Dwork and Aaron Roth. 2014. The algorithmic
foundations of differential privacy. Found. Trends
Theor. Comput. Sci. , 9:211–407.
Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, and
Tom Diethe. 2020. Privacy- and utility-preserving
textual analysis via calibrated multivariate perturba-
tions. In WSDM ’20: The Thirteenth ACM Interna-
tional Conference on Web Search and Data Mining,
Houston, TX, USA, February 3-7, 2020 , pages 178–
186. ACM.
Oluwaseyi Feyisetan, Tom Diethe, and Thomas Drake.
2019. Leveraging hierarchical representations for
preserving privacy and utility in text. In 2019 IEEE
International Conference on Data Mining, ICDM
2019, Beijing, China, November 8-11, 2019 , pages
210–219. IEEE.
Oluwaseyi Feyisetan and Shiva Kasiviswanathan. 2021.
Private release of text embedding vectors. In Pro-
ceedings of the First Workshop on Trustworthy Natu-
ral Language Processing , pages 15–27.
Quan Geng, Wei Ding, Ruiqi Guo, and Sanjiv Kumar.
2020. Tight analysis of privacy and utility tradeoff
in approximate differential privacy. In The 23rd In-
ternational Conference on Artificial Intelligence and
Statistics, AISTATS 2020, 26-28 August 2020, Online
[Palermo, Sicily, Italy] , volume 108 of Proceedings
of Machine Learning Research , pages 89–99. PMLR.
Ivan Habernal. 2021. When differential privacy meets
NLP: The devil is in the detail. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 1522–1528, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Ivan Habernal. 2022. How reparametrization trick broke
differentially-private text representation learning. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), ACL 2022, Dublin, Ireland, May 22-
27, 2022 , pages 771–777. Association for Computa-
tional Linguistics.Lijie Hu, Ivan Habernal, Lei Shen, and Di Wang. 2024.
Differentially private natural language models: Re-
cent advances and future directions. In Findings
of the Association for Computational Linguistics:
EACL 2024 , pages 478–499, St. Julian’s, Malta. As-
sociation for Computational Linguistics.
Lijie Hu, Shuo Ni, Hanshen Xiao, and Di Wang. 2022.
High dimensional differentially private stochastic op-
timization with heavy-tailed data. In PODS ’22:
International Conference on Management of Data,
Philadelphia, PA, USA, June 12 - 17, 2022 , pages
227–236. ACM.
Lijie Hu, Zihang Xiang, Jiabin Liu, and Di Wang.
2023. Privacy-preserving sparse generalized eigen-
value problem. In International Conference on Ar-
tificial Intelligence and Statistics , pages 5052–5062.
PMLR.
Mengdi Huai, Di Wang, Chenglin Miao, Jinhui Xu, and
Aidong Zhang. 2019. Privacy-aware synthesizing for
crowdsourced data. In Proceedings of the Twenty-
Eighth International Joint Conference on Artificial
Intelligence, IJCAI 2019, Macao, China, August 10-
16, 2019 , pages 2542–2548. ijcai.org.
Mengdi Huai, Di Wang, Chenglin Miao, Jinhui Xu, and
Aidong Zhang. 2020. Pairwise learning with differ-
ential privacy guarantees. In The Thirty-Fourth AAAI
Conference on Artificial Intelligence, AAAI 2020, The
Thirty-Second Innovative Applications of Artificial
Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artificial In-
telligence, EAAI 2020, New York, NY, USA, February
7-12, 2020 , pages 694–701. AAAI Press.
Timour Igamberdiev, Thomas Arnold, and Ivan Haber-
nal. 2022. DP-Rewrite: Towards Reproducibility and
Transparency in Differentially Private Text Rewrit-
ing. In The 29th International Conference on Com-
putational Linguistics , pages 2927–2933, Gyeongju,
Republic of Korea. International Committee on Com-
putational Linguistics.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Satyapriya Krishna, Rahul Gupta, and Christophe
Dupuy. 2021. ADePT: Auto-encoder based differ-
entially private text transformation. In Proceedings
of the 16th Conference of the European Chapter of
the Association for Computational Linguistics: Main
Volume , pages 2435–2439, Online. Association for
Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.Lingjuan Lyu, Xuanli He, and Yitong Li. 2020a. Differ-
entially private representation for NLP: Formal guar-
antee and an empirical study on privacy and fairness.
InFindings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 2355–2365, Online.
Association for Computational Linguistics.
Lingjuan Lyu, Yitong Li, Xuanli He, and Tong Xiao.
2020b. Towards differentially private text representa-
tions. In Proceedings of the 43rd International ACM
SIGIR conference on research and development in
Information Retrieval, SIGIR 2020, Virtual Event,
China, July 25-30, 2020 , pages 1813–1816. ACM.
Justus Mattern, Benjamin Weggenmann, and Florian
Kerschbaum. 2022. The limits of word level differen-
tial privacy. In Findings of the Association for Com-
putational Linguistics: NAACL 2022 , pages 867–881,
Seattle, United States. Association for Computational
Linguistics.
Stephen Meisenbacher, Nihildev Nandakumar, Alexan-
dra Klymenko, and Florian Matthes. 2024. A com-
parative analysis of word-level metric differential
privacy: Benchmarking the privacy-utility trade-off.
arXiv preprint arXiv:2404.03324 .
Radford M. Neal. 2003. Slice sampling. The Annals of
Statistics , 31(3):705 – 767.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2014, October 25-29, 2014, Doha,
Qatar, A meeting of SIGDAT, a Special Interest Group
of the ACL , pages 1532–1543. ACL.
Ildikó Pilán, Pierre Lison, Lilja Øvrelid, Anthi Pa-
padopoulou, David Sánchez, and Montserrat Batet.
2022. The text anonymization benchmark (TAB): A
dedicated corpus and evaluation framework for text
anonymization. CoRR , abs/2202.00443.
Richard Plant, Dimitra Gkatzia, and Valerio Giuffrida.
2021. CAPE: Context-aware private embeddings
for private language learning. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7970–7978, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang,
Michael Bendersky, and Marc Najork. 2021. Natu-
ral language understanding with privacy-preserving
BERT. In CIKM ’21: The 30th ACM International
Conference on Information and Knowledge Manage-
ment, Virtual Event, Queensland, Australia, Novem-
ber 1 - 5, 2021 , pages 1488–1497. ACM.Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao,
and Françoise Beaufays. 2019. Federated learning
for emoji prediction in a mobile keyboard. CoRR ,
abs/1906.04329.
Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. 2017. Membership inference attacks
against machine learning models. In 2017 IEEE Sym-
posium on Security and Privacy, SP 2017, San Jose,
CA, USA, May 22-26, 2017 , pages 3–18. IEEE Com-
puter Society.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.
David M. Sommer, Lukas Abfalterer, Sheila Zingg, and
Esfandiar Mohammadi. 2021. Learning numeric op-
timal differentially private truncated additive mecha-
nisms. CoRR , abs/2107.12957.
Jinyan Su, Lijie Hu, and Di Wang. 2022. Faster rates
of private stochastic convex optimization. In Interna-
tional Conference on Algorithmic Learning Theory ,
pages 995–1002. PMLR.
Jingye Tang, Tianqing Zhu, Ping Xiong, Yu Wang, and
Wei Ren. 2020. Privacy and utility trade-off for tex-
tual analysis via calibrated multivariate perturbations.
InNetwork and System Security - 14th International
Conference, NSS 2020, Melbourne, VIC, Australia,
November 25-27, 2020, Proceedings , volume 12570
ofLecture Notes in Computer Science , pages 342–
353. Springer.
Di Wang, Changyou Chen, and Jinhui Xu. 2019a. Dif-
ferentially private empirical risk minimization with
non-convex loss functions. In International Con-
ference on Machine Learning , pages 6526–6535.
PMLR.
Di Wang, Jiahao Ding, Lijie Hu, Zejun Xie, Miao Pan,
and Jinhui Xu. 2020a. Differentially private (gradi-
ent) expectation maximization algorithm with statis-
tical guarantees. arXiv preprint arXiv:2010.13520 .
Di Wang, Jiahao Ding, Lijie Hu, Zejun Xie, Miao Pan,
and Jinhui Xu. 2023a. Finite sample guarantees of
differentially private expectation maximization algo-
rithm. In ECAI 2023 - 26th European Conference
on Artificial Intelligence, September 30 - October 4,
2023, Kraków, Poland - Including 12th Conference
on Prestigious Applications of Intelligent Systems
(PAIS 2023) , volume 372 of Frontiers in Artificial In-
telligence and Applications , pages 2435–2442. IOS
Press.
Di Wang, Marco Gaboardi, Adam D. Smith, and Jin-
hui Xu. 2020b. Empirical risk minimization in the
non-interactive local model of differential privacy. J.
Mach. Learn. Res. , 21:200:1–200:39.Di Wang, Marco Gaboardi, and Jinhui Xu. 2018. Em-
pirical risk minimization in non-interactive local dif-
ferential privacy revisited. Advances in Neural Infor-
mation Processing Systems , 31.
Di Wang, Lijie Hu, Huanyu Zhang, Marco Gaboardi,
and Jinhui Xu. 2019b. Estimating smooth glm in non-
interactive local differential privacy model with pub-
lic unlabeled data. arXiv preprint arXiv:1910.00482 .
Di Wang, Lijie Hu, Huanyu Zhang, Marco Gaboardi,
and Jinhui Xu. 2023b. Generalized linear models in
non-interactive local differential privacy with pub-
lic data. Journal of Machine Learning Research ,
24(132):1–57.
Di Wang and Jinhui Xu. 2020. Principal component
analysis in the local differential privacy model. Theor.
Comput. Sci. , 809:296–312.
Di Wang, Minwei Ye, and Jinhui Xu. 2017. Differen-
tially private empirical risk minimization revisited:
Faster and more general. Advances in Neural Infor-
mation Processing Systems , 30.
John Wieting and Douwe Kiela. 2019. No training
required: Exploring random encoders for sentence
classification. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans,
LA, USA, May 6-9, 2019 . OpenReview.net.
Zihang Xiang, Tianhao Wang, Wanyu Lin, and Di Wang.
2023. Practical differentially private and byzantine-
resilient federated learning. Proc. ACM Manag. Data ,
1(2):119:1–119:26.
Zihang Xiang, Tianhao Wang, and Di Wang. 2024. Pre-
serving node-level privacy in graph neural networks.
In2024 IEEE Symposium on Security and Privacy
(SP), pages 4714–4732. IEEE.
Nan Xu, Oluwaseyi Feyisetan, Abhinav Aggarwal,
Zekun Xu, and Nathanael Teissier. 2021a. Density-
aware differentially private textual perturbations us-
ing truncated gumbel noise. In Proceedings of the
Thirty-Fourth International Florida Artificial Intel-
ligence Research Society Conference, North Miami
Beach, Florida, USA, May 17-19, 2021 .
Zekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan,
and Nathanael Teissier. 2020. A differentially private
text perturbation method using a regularized maha-
lanobis metric. CoRR , abs/2010.11947.
Zekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan,
and Nathanael Teissier. 2021b. On a utilitarian ap-
proach to privacy preserving text generation. CoRR ,
abs/2104.11838.
Zhiyu Xue, Shaoyang Yang, Mengdi Huai, and Di Wang.
2021. Differentially private pairwise learning revis-
ited. In Proceedings of the Thirtieth International
Joint Conference on Artificial Intelligence, IJCAI
2021, Virtual Event / Montreal, Canada, 19-27 Au-
gust 2021 , pages 3242–3248. ijcai.org.Ze Yang, Can Xu, Wei Wu, and Zhoujun Li. 2019. Read,
attend and comment: A deep architecture for auto-
matic news comment generation. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 5077–5089, Hong Kong,
China. Association for Computational Linguistics.
Liang Yao, Chengsheng Mao, and Yuan Luo. 2019.
Clinical text classification with rule-based features
and knowledge-guided convolutional neural net-
works. BMC Medical Informatics Decis. Mak. , 19-
S(3):31–39.
Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li,
Huan Sun, and Sherman S. M. Chow. 2021. Dif-
ferential privacy for text analytics via natural text
sanitization. In Findings of the Association for Com-
putational Linguistics: ACL-IJCNLP 2021 , pages
3853–3866, Online. Association for Computational
Linguistics.
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-
uating text generation with bert. In International
Conference on Learning Representations .
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text classi-
fication. Advances in neural information processing
systems , 28.
A More Details and Experiments
Dataset. The statistics of dataset are shown in
table 4.
Table 4: Dataset statistics used in this work.
Dataset Avg. Length Train Size Test Size
(tokens) (neg/pos) (neg/pos)
Yahoo 181 8539/8673 2174/2189
Yelp 19 3610/3310 909/912
SST-2 10 24214/30362 5994/7651
AG News 40 20000 4000
Table 5: Time Cost. Comparison of the time cost of
each epoch (seconds) under GloVe Embedding initial-
ization for the non-private case and three mechanisms
(Gaussian, Laplacian and TrLaplacian), the privacy bud-
get ranges from 0.05 to 20.
ϵ <1 ϵ≥1
Privacy budget ϵ 0.05 0.1 0.2 0.5 1 5 10 20
YahooNon-private 111
Gaussian 111 113 111 111 111 111 111 111
Laplacian 111 113 111 111 111 111 111 111
TrLaplacian 123 123 123 123 123 123 123 123
YelpNon-private 111
Gaussian 38 37 38 38 37 37 37 37
Laplacian 38 37 37 37 37 37 37 37
TrLaplacian 46 41 46 42 42 42 42 42Implementation Details. Models in this paper
are implemented based on the PyTorch†and Ten-
sorFlow‡with their libraries. Experiments are con-
ducted on NVIDIA GeForce RTX 3090 GPUs. To
implement our mechanism, we use the acceptance-
rejection sampling method (Neal, 2003) to sample a
point from the high dimensional truncated Laplace
distribution from the Laplace distribution, only by
rejecting the samples outside the interval.
For text re-write, we use the auto-encoder
model. The embedding is initialized with the 300-
dimensional pre-trained Random, GloVe, and fast-
Text word embedding. We use one-layer BiLSTM
with dropout for the encoder, and using setup:
dropout rate 0.5, Adam (Kingma and Ba, 2015)
with an initial learning rate of 0.01 and betas (0.5,
0.999), batch size 1024, and number of training
epochs 100. For the downstream classification task
over the AG News and SST-2 dataset, we use Adam
with an initial learning rate of 0.0001, a dropout
rate of 0.2, and a batch size of 256. We set the
maximum number of epochs to be 50.
B Omitted Proofs
Proof 1 (Proof of Theorem 3) The proof is mo-
tivated by (Das et al., 2021). Consider a
pair of tokens w, w′. Let perturbed encoder1
r1= CLIPEmb( w) + η1, also let r2=
CLIPEmb( w′)+η2= CLIPEmb( w)+∆s+η2,
where ∥∆s∥1≤∆1and∥∆s∥∞≤∆∞which
are due to the clipping operation.
Let us denote the set of possible values of rkby
Skfork= 1,2.
Define U= [−C−A, C +A]d. Note that for
any subset V ⊆ U − (S1∪ S2),P(r1∈ V) =
P(r2∈ V) = 0 , hence (ϵ, δ)-DP is satisfied for
this part. We need to ensure (ϵ, δ)-DP is satisfied
for all elements in S1∪ S2too.
First, consider an element s∈ S1∩ S2. Then:
f(r1=s) =f(η1=s−CLIPEmb( s))
Similarly:
f(r2=s) =f(η2=s−CLIPEmb( s)−∆s)
†https://pytorch.org/
‡https://www.tensorflow.org/Using the above equations:
exp (−α∆1)≤exp (−α∥∆s∥1)
≤f(r1=s)
f(r2=s)≤exp (α∥∆s∥1)≤exp (α∆1)
From the above equation, setting setting α=
ϵ/∆1ensures pure ϵ-DP for all s∈ S1∩ S2. With
this, it follows that for any V ⊆ S 1∩ S2:
e−ϵP(r2∈ V)≤P(r1∈ V)≤eϵP(r2∈ V).
by setting α=ϵ/∆1.
Now consider an element s∈ S2− S1. Clearly,
f(r1=s) = 0 . Also:
max
s∈S2−S1f(r2=s)≤1
B.
But notice that volume (S2−S1)≤∆d
∞. This fol-
lows from the fact that for every coordinate, there
are at most ∆∞levels that can be attained by r2
but not by r1. Thus, for any T ⊆ S 2− S1, we have
P(r1∈ T) = 0 andP(r2∈ T)≤∆∞
Bd
Similarly, for any T ⊆ S 1− S2, we have
P(r2∈ T) = 0 andP(r1∈ T)≤∆∞
Bd
.
Now, let us now consider some general T ⊆ S 1∪
S2. LetT0=T ∩(S1∪ S2),T1=T ∩(S1− S2)
andT2=T ∩(S2− S1). It is easy to see that
T=T0∪ T1∪ T2and that T0,T1andT2are
pairwise-disjoint. Then:
P(r1∈ T) =P(r1∈ T0) +P(r1∈ T1)
+P(r1∈ T2)
≤eϵP(r2∈ T0) +∆∞
Bd
+ 0
≤eϵP(r2∈ T) +∆∞
Bd
.
(4)
Thus, we can set δ= (∆∞
B)d. Obviously, this result
is only useful if B >∆∞.
For each coordinate
Z
x∈RfTLap(x)dx=ZA
021
Be−α|x|dx
=2
Bα 
1−e−αA
= 1Comparison Semantic Problem of Private Embedding
Original :do not come here! food poisoning alert! ( →Neg.)
Trlaplace :do not come here! food poisoning alert! ( →Neg.)
Laplace :this place is awesome! love this place! ( →Pos.)
Gaussian :do not go here! food glorious <unk>! ( →Pos.)
Figure 6: Another example of text re-write with different mechanisms with ϵ= 0.1. The Gaussian and Laplacian
mechanisms destroyed the semantic properties of the original sentence.
We can solve B=2(1−e−αA)
α. Thus, take
B=∆∞
δ1
d, we can see A=−1
αlog(1−α∆∞
2δ1
δ) =
−∆1
ϵlog(1−ϵ
2√
dδ1
δ).
Proof 2 (Proof of Theorem 4) We first show the
variance of our mechanism Ais bounded by 2d∆2
1
ϵ2.
We can easily see that the variance is E∥A(w)−
w∥2
2=dVwithV=R
x∈RfTLap(x)|x|2dx, so
Z
x2f(x)dx
=21
BZA
0e−αxx2dx
=21
BZA
0−1
αx2d 
e−αx
=21
B(−1
α)A2e−αA+ 21
BZA
01
αe−αx2xdx(5)
and
ZA
01
αe−αx2xdx
=−ZA
01
α2·2xd 
e−αx
=−1
α22Ae−αA+ZA
02
α2e−αxdx
=−1
α22A·e−αA+2
α3 
1−e−2αA(6)
Thus, we have
V=−21
α1
BA2e−αA−41
α21
BAe−αA
+ 41
α31
B 
1−e−αA
=−21
α1
BAe−αA(A+ 21
α) + 2∆2
1
ε2
<2∆2
1
ε2.(7)
Thus, in total we have E∥A(w)−w∥2
2≤2d∆2
1
ϵ2=
8d2C2
ϵ2.Next for Laplacian mechanism in Theorem 1 we
haveE[∥Alap(w)−w∥2
2] =2d∆2
1
ϵ2. Thus the vari-
ance of high dimensional truncated Laplacian is
always lower than Laplacian.
Similarly, the variance of Gaussian mechanism
in Theorem 4 is8C2d(ln 1.25+ln 1 /δ)
ϵ2 , we can easily
see that our mechanism has lower variance when
δ≤1
ed.