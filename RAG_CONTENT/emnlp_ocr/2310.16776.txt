DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models
via Unsupervised Core-Set Selection
Devleena Das∗
Georgia Institute of Technology
ddas41@gatech.eduVivek Khetan
Accenture Labs
vivek.a.khetan@accenture.com
Abstract
Recent advances have led to the availability
of many pre-trained language models (PLMs);
however, a question that remains is how much
data is truly needed to fine-tune PLMs for
downstream tasks? In this work, we intro-
duce DEFT-UCS, a data-efficient fine-tuning
framework that leverages unsupervised core-set
selection to identify a smaller, representative
dataset that reduces the data needed to fine-
tune PLMs for the text-generation task of text-
editing. We examine the efficacy of DEFT-UCS
across multiple text-editing tasks, and com-
pare to the state-of-the art text-editing model,
CoEDIT. Our results demonstrate that DEFT-
UCS models are just as accurate as CoEDIT,
across eight different datasets consisting of six
different editing tasks, while finetuned on 70%
less data.
1 Introduction
How much data do we need to fine-tune a pre-
trained language model (PLM) for a specific down-
stream task? While successes in language mod-
elling have led to numerous publicly available
PLMs and ability to produce fine-tuned models
for downstream tasks - the answer mostly remains,
“as large as possible, and of good quality”. For
example, Alpaca, an instruction-following model,
is trained with 52k data samples (Taori et al.,
2023). Similarly, CoPoet, a collaborative poetry
writing system is fine-tuned using 87k data sam-
ples (Chakrabarty et al., 2022). MetaMath, a math-
reasoning LLM is fine-tuned with 395k data sam-
ples (Yu et al., 2023). Although fine-tuning PLMs
on specific task results in performance gain, ac-
quiring large amounts of data for fine-tuning is
not easy for real-world applications which often
require niche knowledge and domain expertise.
Researchers have explored variety of methods
primarily focused on improving the computational
∗Contributed during an internship at Accenture Labs, SFefficiency of fine-tuning, including parameter-
efficient fine-tuning approaches (PEFT) to reduce
computational costs by optimizing parameter up-
dates (Fu et al., 2023; Hu et al., 2021) as well as
leveraging active-learning for iteratively selecting
data samples during training (Su et al., 2022; Diao
et al., 2023). Instead, our work focuses on improv-
ing the data efficiency of PLM fine-tuning without
requiring iterative fine-tuning. Specifically, we ex-
plore how to fine-tune PLMs with significantly less
data samples and without a cost to model perfor-
mance. Related to language models, researchers
have experimented with different core-set selection
metrics (Paul et al., 2021; Sorscher et al., 2022)
to improve the data efficiency during pre-training .
Marion et al. (2023) demonstrated how perplex-
ity, L2-Error Norm (EL2N) and memorization can
be utilized to select smaller, good quality datasets
for pre-training. Similarly, (Attendu and Corbeil,
2023) leverage EL2N to dynamically remove data
samples with high EL2N between training epochs.
However, these metrics assume access to task data
and reference models to perform dataset pruning.
In real world applications, utilizing such super-
vised, data-pruning metrics are less realistic since
large amounts of annotated task-specific data may
be costly to acquire. This leads us to our main
research question: How can we leverage unsuper-
vised data pruning to fine-tune PLMs for down-
stream tasks in a more data efficient manner?
In this work, we introduce a new data-efficient
fine-tuning framework, DEFT-UCS, that uses unsu-
pervised core-set selection to minimize the amount
of labelled data needed to fine-tune PLMs for the
text-generation task of text-editing. Our framework
is inspired by (Sorscher et al., 2022), who utilize
clustering-based dataset pruning to reduce train-
ing samples for image-classification models, and
to the best of our knowledge, our framework is the
first to leverage unsupervised core-set selection for
data-efficient fine-tuning of PLMs.arXiv:2310.16776v5  [cs.CL]  13 Jun 2024We investigate the utility of DEFT-UCS in fine-
tuning PLMs for text-generation across eight differ-
ent datasets consisting of six different text-editing
tasks, and compare DEFT-UCS models to the state-
of-the-art text-editing model, CoEDIT(Raheja
et al., 2023). Our contributions are as follows:
•We introduce DEFT-UCS, a data-efficient-fine
tuning framework that leverages unsupervised
core-set selection via clustering to identify a
smaller representative set of data needed to
fine-tune PLMs.
•We show that DEFT-UCS, utilizing only
32.5% of CoEDIT’s training data, is able to
produce fine-tuned models with improved ac-
curacy on four different text-editing tasks,
and similar accuracy on two text-editing tasks
compared to CoEDIT (Raheja et al., 2023).
•We performed a human evaluation with 3 eval-
uators to assess the quality of text-edits from
our DEFT-UCS model. Evaluators found edits
generated by DEFT-UCS model as similar or
preferred over CoEDIT (Raheja et al., 2023).
2 Related Works
Efficient Fine-Tuning of LLMs Most work on
efficient fine-tuning techniques for LLMs have pri-
marily focused on parameter-efficient fine-tuning
(PEFT) approaches (Fu et al., 2023; Hu et al.,
2021), improving computation efficiency by up-
dating a subset of model parameters. Recently,
there has been an increasing focus on improving
the data-efficiency of LLMs, considering how to
pre-train and fine-tune LLMs with smaller subsets
of data (Zhou et al., 2023a; Mukherjee et al., 2023;
Chen et al., 2023; Marion et al., 2023; Attendu and
Corbeil, 2023; Ivison et al., 2022). For instance,
Zhou et al. (2023a) introduce LIMA, an approach
to fine-tune LLaMA (Touvron et al., 2023) with
only 1k diverse and high quality samples. However,
the LIMA approach is underspecificed without a
general subsampling procedure. Also, Chen et al.
(2023) develop Skill-It!, which creates efficient
datasets by learning hierarchical relationships be-
tween samples. However, identifying hierarchical
relationships is non-trivial and not all datasets may
include them. More closely related to our work, Ivi-
son et al. (2022) leverage K-Nearest Neighbors to
learn multiple data-efficient fine-tuned models for
individual tasks. Instead, we aim to learn a singledata-efficient fine-tuned model that performs com-
petitively across a variety of datasets. Similarly,
Marion et al. (2023) utilize perplexity and EL2N, to
find smaller datasets for LLM pre-training, and At-
tendu and Corbeil (2023) uses EL2N to iteratively
remove unimportant samples during fine-tuning.
Both Marion et al. (2023) and Attendu and Corbeil
(2023) assume access to task data to train various
PLMs for few epochs used to calculate EL2N and
perplexity. In contrast, we leverage unsupervised
core-set selection, omitting the need for any refer-
ence model during the dataset sampling step.
Core-Set Selection & Dataset Distillation Sev-
eral works in ML have developed variety of core-
set selection (Har-Peled and Kushal, 2005) and
dataset pruning (Paul et al., 2021) methods to
find smaller subsets of data needed to train deep
learning models without model performance loss.
CRAIG (Mirzasoleiman et al., 2020) finds core-
sets by approximating gradient calculations, while
RETRIEVE (Killamsetty et al., 2021) finds core-
sets by optimizing for model loss. Also, Yang et al.
(2022) utilize Influence Functions (Koh and Liang,
2017) to prune redundant samples. A unifying idea
among these methods is the need for labelled data.
Alternatively, core-set selection methods for un-
labelled data have used clustering methods. Birod-
kar et al. (2019) use Agglomerative clustering
to find semantic similarities among data points
and prune redundant samples. Similarly, Sorscher
et al. (2022) use vanilla k-means clustering and
distances to cluster centroids for pruning easy and
hard samples. Recently, data distillation algorithms
have also been developed to improve data-efficient
model training (Zhou et al., 2023b). Typically,
data distillation methods generate new synthetic
datasets in which data samples are edited to pre-
serve more information for performance general-
ization (Lei and Tao, 2023). Our work consid-
ers efficient core-set selection without the genera-
tion of a synthetic dataset. Specifically, our work
builds upon the unsupervised clustering approach
in Sorscher et al. (2022) applied to computer vision
tasks, to fine-tune PLMs in a data-efficient manner.
Instruction Tuning for Text-Editing Recently,
Instruction tuning of PLMs has shown impressive
success in its ability to enable PLMs to follow in-
structions as well as improvement in generalization
across various tasks in zero/few shot settings (Min
et al., 2022; Wei et al., 2021). Training models toexplicitly follow natural language instructions has
become increasingly popular for text-editing tasks
as well. Shu et al. (2023) develop RewriteLM by
fine-tuning PaLM (Chowdhery et al., 2022) vari-
ants for the task of rewriting long-form texts. Sim-
ilarly, Schick et al. (2022) develop PEER by fine-
tuning T5 (Raffel et al., 2020) variants to emulate
the collaborative writing process. Additionally, Ra-
heja et al. (2023) develop CoEDIT by fine-tuning
Flan T5 (Chung et al., 2022) models to perform
single and compositional edits across multiple edit
tasks. Furthermore, Zhang et al. (2023) produce an
instruction-tuned LLaMA model that improve text-
editing capabilities. A commonality across these
works include the usage of large scale datasets for
fine-tuning. For example, CoEDIT (Raheja et al.,
2023) and Zhang et al. (2023) leverage datasets
with 82k and 60k examples, respectively. In our
work, DEFT-UCS maximizes model performance
of fine-tuned models in a data efficient manner by
finding a representative, smaller dataset needed
for fine-tuning. We investigate the efficacy of
our DEFT-UCS framework to instruction fine-tune
PLMs for eight text-editing tasks.
3 Problem Formulation
We formulate DEFT-UCS as an unsupervised core-
set selection problem (Sorscher et al., 2022) in con-
trast to existing dataset pruning methods which pri-
marily use supervised core-set selection (Attendu
and Corbeil, 2023; Marion et al., 2023).
Specifically, let Drepresent an existing large
dataset, Prepresent a PLM, and MDrepresent P
fine-tuned on D. Our DEFT-UCS framework aims
to find a representative core-set Dc⊂Dsuch that
leveraging Dccan fine-tune Pand result in a fine-
tuned model MDcwith comparable performance to
MD. Note, we refer to comparable evaluation per-
formance in the form of both quantitative NLP met-
rics and qualitative human evaluations. Specific to
unsupervised core-set selection, DEFT-UCS finds
Dcwithout needing Dto include annotations or
labels. Thus, we find Dcby only using the input
samples {x1..xn}within D. These input samples,
in the context of instruction fine-tuning, represent
task instructions and input texts.
To perform unsupervised core-set selection, we
build upon the SoTA clustering-based core-set se-
lection method by Sorscher et al. (2022), given
its extensive evaluations against other supervised-
based core-set selection methods. While Sorscher
Figure 1: Our DEFT-UCS framework utilizes unsuper-
vised core-set selection (UCS) to find a core-set of data
Dc, as well as initial seed data, Dbaseto produce a fine-
tuned PLM, MDEFT −UCS.
et al. (2022) demonstrate the efficacy of clustering-
based core-set selection for ImageNet (Deng et al.,
2009), our work is the first to investigate the effec-
tiveness of clustering-based core-set selection in
non-classification tasks, such as fine-tuning PLMs
for multiple text-editing tasks.
Algorithm 1 Unsupervised Core-set Selection
(UCS)
Input: Dremain ={x0, x1...xn}- Large Dataset
Input: K- Num. of Clusters
Input: A- Amount of samples per cluster
Input: α,β, - Sampling Weights
Output: Dc={xj..xp}- Core-Set
1:Dc=∅
2:Dembed = ComputeEmbedding( Dremain )
3:Cl1:K, Ce 1:K= KMeans( Dembed ,K)
4:foriinKdo
5: fordinClido
6: distlist= StoreCosineDistance( d,Cei)
7: end for
8: distsorted = sort( distlist)
9: Dsampled =distsorted [0 :α*A]
+distsorted [ -β*A:]
10: Dc= updateCoreSet( Dsampled ,Dc)
11:end for
12:return Dc
4 DEFT-UCS Framework
Figure 1 outlines our DEFT-UCS framework which
leverages unsupervised, clustering-based core-set
selection (UCS ) to find a subset of Dthat fine-tunes
a PLM without compromising model performance.
We consider a scenario in which there exists an
initial amount of data, Dbase⊂D, that is sampled
in a stratified manner to provide an overall repre-
sentation of the downstream fine-tuning task. LetDremain represent the remaining data after Dbaseis
sampled. The goal of UCS is to then find a core-set
Dc⊂Dremain that enriches Dbasesuch that Dc
andDbase, together, form a representative subset
that can be used to fine-tune a PLM and result in a
fine-tuned model MDEFT −UCS with comparable
performance to MD, a PLM fine-tuned with D. In
Algorithm 1, we detail the crux of our DEFT-UCS
framework, the UCS method.
4.1 Clustering in UCS
The first step in UCS includes transforming
Dremain into a meaningful embedding represen-
tation Dembed . UCS clusters Dbased on its latent-
space representation, using previously learned em-
bedding spaces, such as sentenceBert (Reimers and
Gurevych, 2019). Choosing an appropriate embed-
ding representation is important, given that such
representation impacts the downstream clustering
task within UCS. In Section 5, we detail the types
of learned embedding spaces we evaluate and the
best embedding representation found for encoding
sentence-based datasets.
Given Dembed , we perform K-Means clustering
to separate Dembed intoKclusters. Note, the value
ofKis dependent on D, and defining Krequires
domain knowledge about the dataset to understand
the different categories or tasks represented in D.
Alternatively, Kcan be automatically derived using
metrics such as Silhouette Score (Shahapure and
Nicholas, 2020). The resulting Kclusters, Cl1:K,
and cluster centroids, Ce1:K, are utilized to com-
pute the cosine distance between each data sample
din a cluster Cli, and corresponding centroid Cei.
4.2 Sampling Dcin UCS
We leverage the clustering categorization presented
in Sorscher et al. (2022) to sample Dcfrom
Dremain . Specifically, Sorscher et al. (2022) ex-
plain that data samples can be categorized as “easy”
or “hard” examples. In the context of unsupervised
clustering, Sorscher et al. (2022) leverage a data
sample’s distance to its cluster centroid to define
easy and hard samples. Therefore, easy/hard sam-
ples within a cluster are those closest/furthest to the
cluster centroid. Given such definition, in UCS, we
retrieve a weighted sampling of easy and hard sam-
ples from each cluster, denoted as Dsampled . The
αandβweights control the distribution of easy
and hard samples in Dsampled , and Arepresents
the total number of samples retrieved per cluster.Note, Dbase,K,A,α, and βare hyperparame-
ters within DEFT-UCS, manually set by domain-
experts. Given this is the first work, to our
knowledge, to propose data-efficient fine-tuning
for PLMs leveraging UCS, we perform an exhaus-
tive investigation on how these hyperparameters
influence fine-tuning performance (see Section 7).
Future work includes investigating automatic selec-
tion of such hyperparameters.
5 DEFT-UCS Applied to Text-Editing
We evaluate the utility of DEFT-UCS in the context
of instruction-based fine-tuning for multiple text
editing tasks. To our knowledge, the current SoTA
instruction fine-tuned text-editing LM is CoEDIT
(MCoEDIT )1trained on dataset DCoEDIT (Raheja
et al., 2023). Overall, DCoEDIT includes 82k
good-quality edit instructions spanning six differ-
ent edit-tasks (Raheja et al., 2023) ( DCoEDIT de-
tailed in Appendix A.1). Given the data quality
inDCoEDIT and SoTA performance of MCoEDIT ,
we apply DEFT-UCS to DCoEDIT . Below, we de-
tail the hyper-parameter choices in DEFT-UCS in
the context of DCoEDIT .
5.1DBase in CoEDIT
Recall DBase refers to initial data sampled in a
stratified manner used for fine-tuning. In our work,
stratified sampling is performed based on the differ-
ent tasks represented in D. During our evaluations,
we study how the size of DBase may influence hy-
perparameter selection within our UCS algorithm
for producing a well-performing MDEFT −UCS.
In the context of CoEDIT, we experiment with
DBase ={10%,20%, ..80%}, representing 10%
to 80% of DCoEDIT . Note, DCoEDIT is a fully
annotated dataset; however, when performing core-
set selection Dc⊂D, we only consider the input
sentences.
5.2 DEFT-UCS Hyperparameters
Given that DCoEDIT includes seven edit-
intentions, we set K= 7, allowing the K-Means
Clustering within UCS to separate Dremain into
7 clusters. Additionally, recall from Sec. 4
thatαandβrepresent the sampling weights for
extracting easy and hard data samples from each
cluster to form Dsampled . To understand the upper
and lower bound effects of αandβ, we study
three variants of Dsampled , representing three
1https://github.com/vipulraheja/coeditEvaluation Dataset Edit Task
TurkCorpus (Xu et al., 2016a) Simplification
Asset (Alva-Manchego et al., 2020) Simplification
Iterator Coherence (Du et al., 2022) Coherence
Iterator Clarity (Du et al., 2022) Clarity
Iterator Fluency (Du et al., 2022) Fluency
Iterator Global (Du et al., 2022) Clarity, Coherence, Fluency
JFLEG (Napoles et al., 2017) Grammar Correction
WNC (Pryzant et al., 2020) Neutralization
Table 1: A list of datasets, spanning six editing tasks,
on which we evaluate our DEFT-UCS models.
different sampling types: Dhard
sampled,Deasy
sampledand
Drand
sampled. Specifically, Dhard
sampledis represented by
α= 0 andβ= 1.0,Deasy
sampledis represented by
α= 1.0andβ= 0, and Drand
sampledapproximates
α= 0.5andβ= 0.5, denoting random samples
extracted per cluster. We also experiment with
sampling different amounts of data from each
cluster, denoted by A={285,570,857}. Such
settings of Aapproximate {2000,4000,6000}
total samples from Dremain respectively, and
represent {2.5%,5%,7.5%}percent of Dremain .
5.3 Dataset Embedding
Recall that the UCS algorithm in DEFT-UCS per-
forms clustering using a learned embedding rep-
resentation of the input data samples. We investi-
gate several embedding representations and select
the best embedding representation by its ability to
inform accurate clusters. Specifically, we study
sentence-level encodings from Sentence-T5 (Ni
et al., 2021), BART (Lewis et al., 2019) CLS to-
ken embeddings, as well as averaged word token
embeddings from Flan-T5 (Chung et al., 2022).
From an ablation study, our results demonstrate
that leveraging Sentence-T5 (Ni et al., 2021) re-
sults in the best K-Means Clustering performance.
The ablation study results are in Appendix B.
5.4 Model Fine-Tuning
Raheja et al. (2023) develop CoEDIT-Large,
CoEDIT-xl, and CoEDIT-xxl by fine-tuning Flan-
T5’s Large, XL and XXL models, respectively.
In our work, we focus our comparisons against
CoEDIT-Large, referred to as MCoEDIT . There-
fore, in our framework, we fine-tune Flan-T5-
Large, producing MFlan −T5−LG
DEFT −UCS. Details on our
fine-tuning implementation are in Appendix A.2.6 Experiments
6.1 Evaluation Datasets
Table 1 presents eight test datasets used in our eval-
uation. We performed evaluations across six differ-
ent edit tasks including simplification, coherence,
clarity, fluency, grammar correction and neutral-
ization improvement. See Appendix C for dataset
details. For fair comparisons, these datasets in-
clude the publicly available datasets evaluated by
CoEDIT (Raheja et al., 2023), and are present in
several text-editing benchmarks, including EDITE-
V AL (Dwivedi-Yu et al., 2022).
6.2 Metrics
We examine SARI (Xu et al., 2016b) and ROUGE-
L (Lin, 2004) scores for our quantitative evalua-
tions. SARI scores are also utilized in prior text-
editing tasks (Raheja et al., 2023). During our
human evaluation, we analyze users’ perceived ac-
curacy percentage ( PA% ), which measures the per-
cent of times users select a text-editing model for
producing accurately edited sentences.
6.3 Baselines
We compare our fine-tuned models via DEFT-UCS,
MDEFT −UCS, to the following baselines.
CoEDIT-Large The primary baseline of our
work is the original CoEDIT-Large model (Raheja
et al., 2023), MCoEDIT , which uses the entire 82k
samples in DCoEDIT to fine-tune Flan-T5 Large.
To compare against MCoEDIT , we utilize the re-
leased CoEDIT model2and compare SARI and
ROUGE-L scores for each evaluation dataset.
LIMA Approach We also compare our DEFT-
UCS method to the LIMA approach (Zhou et al.,
2023a). Following the LIMA approach of using
high quality and diverse 1k data points, we select 1k
data samples via stratified random sampling from
DCoEDIT for fine-tuning Flan-T5. We refer to
such LIMA-inspired model as MLIMA . Prior work
by Raheja et al. (Raheja et al., 2023) validate the
high-quality data samples in DCoEDIT , and strati-
fied random sampling ensures data diversity, allow-
ing all editing tasks within DCoEDIT to be equally
represented.
Non-Instruction Fine-Tuned LLMs We also
compare our MDEFT −UCS with LLamA2-7B
2https://huggingface.co/grammarly/coedit-large(MLLAMA 2−7B) (Touvron et al., 2023), Flan-T5-
Large ( MFLAN −T5−LG) (Chung et al., 2022) and
BLOOM-560M ( MBLOOM −560M) (Scao et al.,
2022), in Zero-Shot settings, to understand how
MDEFT −UCS compares to non-instruction fine-
tuned LLMs.
7 Results
Our results below show that DEFT-UCS can pro-
vide a data-efficient method for producing competi-
tive fine-tuned models for six different text-editing
tasks.
7.1 DEFT-UCS vs. CoEDIT
Figure 2 shows that our DEFT-UCS framework
generates fine-tuned models with comparable per-
formance to MCoEDIT in terms of SARI (Fig. 2a)
and Rouge-L (Fig. 2b) scores, using lower frac-
tions of DCoEDIT . These results indicate that un-
supervised core-set selection within DEFT-UCS
can effectively find a Dcfor fine-tuning without
compromising downstream task performance.
The DEFT-UCS models in Figure 2 reflect the
existence of acompetitive DEFT-UCS model, and
depending on the evaluated text-editing task, a dif-
ferent fraction of DCoEDIT results in the most com-
petitive performances. For example, to achieve
comparable performance on the WNC dataset for
the neutralization task, a DEFT-UCS model needs
above 80% of DCoEDIT . In contrast, for the As-
set dataset and simplification task, around 12% of
DCoEDIT is needed to surpass MCoEDIT SARI
and ROUGE-L scores. We hypothesize that subjec-
tivity in the neutralization task (WNC) increases
the complexity of the data samples and more data
is required to fine-tune a competitive model in com-
parison to less subjective editing tasks such as, text-
simplification (Asset).
7.2 DEFT-UCS vs. LIMA Approach
We observe across all evaluation tasks, MLIMA
has lower SARI and ROUGE-L scores compared
toMCoEDIT and our DEFT-UCS models. These
results show that the LIMA (Zhou et al., 2023a) ap-
proach may not be generalizable to domain-specific
LM tasks such as text-editing and more experi-
mentation is needed to understand its limitations.
Moreover, these results indicate that smarter sam-
pling techniques that go beyond data quality and
diversity are needed for competitive model perfor-
mances, such as considering distance metrics inembedding spaces as utilized in DEFT-UCS.
7.3 Overall DEFT-UCS Model
In Section 7.1, we found that the most competitive
DEFT-UCS model for each evaluation dataset uses
a different fraction of DCoEDIT . Therefore, we
performed an additional analysis to study which
combination of hyper-parameters result in an over-
all best-performing DEFT-UCS model, one that
achieves or surpasses MCoEDIT performances on
most evaluation datasets using a much smaller
fraction of DCoEDIT . Fig. 3(a) and Fig. 3(b)
show that fine-tuning Flan-T5 Large with only
32.5% of DCoEDIT and performing hard sampling
(α= 0,β= 1.0), results in the best overall
DEFT-UCS model, MFLAN −T5−LG
DEFT −UCS, surpassing
MCoEDIT SARI and ROUGE-L scores on six of
the eight evaluation datasets. Overall, 32.5% repre-
sents the smallest fraction of DCoEDIT that results
in competitive SARI and ROUGE-L scores on most
evaluation datasets.
Note, 32.5% of DCoEDIT is composed of Dbase,
initial data available for fine-tuning, and Dc, the
output of UCS within DEFT-UCS. In the context of
MFLAN −T5−LG
DEFT −UCS,Dbaseis a stratified 30% subset
from DCoEDIT , and Dcis composed of another
2.5% of Dremain (A= 2000 samples per cluster)
retrieved from UCS by performing hard sampling.
Model Performance Table 2 shows the SARI
and ROUGE-L scores of our best DEFT-UCS
model, MFLAN −T5−LG
DEFT −UCS, fine-tuned with only
32.5% of DCoEDIT . We find that MFLAN −T5
DEF −UCS
performs better than MLIMA andMFLAN −T5−LG
on four datasets, and comparably on two datasets,
WNC and JFLEG. These results emphasize that a
much smaller fraction of DCoEDIT can be used
to produce a comparable fine-tuned text-editing
model. We also observe that MLLAMA 2−7B
andMBLOOM −560have much lower ROUGE-
L scores compared to all other models. Af-
ter examining model generated outputs, we see
that lower ROUGE-L scores are attributed to
long, repeated sentences from MLLAMA 2−7B
MBLOOM −560. Appendix D.2 provides example
edited sentences from each model.
Influence of DBase & Sampling Methods
Based on downstream tasks, the amount of DBase
may vary. Thus, we analyze how the size of Dbase
may influence the sampling method utilized in
DEFT-UCS for producing best-performing models.(a)
 (b)
Figure 2: Comparisons between the CoEDIT model (Raheja et al., 2023), LIMA-inspired model MLIMA (Zhou
et al., 2023a), and our DEFT-UCS models with respect to SARI (a) and ROUGE-L (b) scores.
Models Turk Asset Iterator Coherence Iterator Clarity Iterator Fluency Iterator Global JFLEG WNC
MFlan −T5−LG
DEFT −UCS46.6 /81.1 46.8 /76.9 68.9 / 90.9 61.8 /85.3 69.9 /96.9 64.7 /89.1 70.2 / 93.1 79.0 / 96.5
MCoEDIT 43.7 / 74.9 44.7 / 70.9 67.3 / 91.1 61.3 / 85.1 69.1 / 96.6 64.2 / 89.0 70.4 /93.2 80.2 /96.5
MLIMA 23.8 / 31.9 37.8 / 51.7 43.3 / 65.9 36.5 / 55.5 48.8 / 71.9 39.4 / 58.9 39.7 / 48.8 37.2 / 59.3
MLLAMA 2−7B 36.8 / 17.3 41.6 / 20.3 35.8 / 26.2 41.2 / 28.5 40.4/ 33.8 38.3/ 29.7 46.0 / 17.0 27.3 / 17.2
MFlAN −T5−LG 32.3 / 59.1 41.3 / 74.7 36.7 / 52.4 34.3 / 54.3 37.9 / 64.9 35.5 / 57.7 51.3 / 80.9 30.7 / 48.9
MBLOOM −560M 27.3 / 7.7 32.0 / 8.2 19.1 / 8.8 20.6 / 9.7 16.3/ 8.2 19.6 / 9.5 27.9 / 4.9 18.8/ 8.1
Table 2: Comparisons between the overall best DEFT-UCS model, MFLan−T5−LG
DEFT −UCS with all other baselines, with the
first value representing SARI score and second value representing ROUGE-L score. Note, scores for LLAMA-7B
and BLOOM-560 model (Zero-shot) generations are calculated by first removing the prepended input sequence.
Figure 4 summarizes the win percentages among
the three sampling methods (random sampling,
easy sampling, hard sampling) as the size of Dbase
increases. Win percentage is defined as the percent
of times a particular sampling method achieves the
highest SARI (Fig. 4a) or ROUGE-L (Fig. 4b)
score across all evaluation datasets. From Figure
4a and Figure 4b, we observe that as DBase in-
creases, even across different Dcamounts, random
sampling results in better SARI and ROUGE-L per-
formances compared to easy and hard sampling.
However, with lower amounts of DBase, hard sam-
pling results in better performance. We hypothe-
size that with lower amounts of DBase, sampling
harder examples may allow the model to generalize
to unseen examples. Interactions between DBase
and sampling type may be dataset and task depen-
dent, and future work should experiment with these
hypotheses for different task-specific applications.
7.4 Human Evaluation
We hired three computer scientists with English as
their primary language for our human evaluation.
We created a human-eval test set by randomly sam-Model Perceived Accuracy ( PA% )
MFlan −T5−LG
DEFT −UCS83.8 %
MCoEDIT (Raheja et al., 2023) 70.5%
Table 3: Perceived accuracy from human evaluation.
pling 35 examples from seven text-editing dataset
in Table 1.3For each sample in the human-eval test
set, evaluators were provided two edited sentence
generated using MFLAN −T5−LG
DEFT −UCSandMCoEDIT .
Evaluators were then asked to select the most accu-
rately edited sentence. Given that many edited sen-
tences from MFLAN −T5−LG
DEFT −UCSandMCoEDIT were
similar or identical, evaluators were able to select
more than one edited-sentence as accurately edited.
To reduce bias, the generated sentence ordering
from the models was randomized.
Table 3 summarizes the average perceived
accuracy percentages ( PA% ). Overall, our
MFLAN −T5−LG
DEFT −UCSresults in higher PA% compared
toMCoEDIT . We also calculated the inter-rater re-
liability score to understand the agreement among
3We did not sample from Iterator Global since such dataset
is a combination of Iterator Clarity, Fluency and Coherence.(a)
 (b)
Figure 3: Utilizing hard sampling in UCS results in a best, overall DEFT-UCS model that requires only 32.5% of
DCoEDIT to beat 6/8 evaluation datasets considering SARI (a) and ROUGE-L (b) scores.
(a)
(b)
Figure 4: With less Dbase, leveraging hard sampling in our DEFT-UCS leads to better performing models (winning
%); as Dbaseincreases, random sampling leads to better performing models.
evaluators on their PA%, and found moderate agree-
ment with a Fleiss-Kappa (Fleiss and Cohen, 1973)
score of 0.44. These results indicate that evaluators
perceived our MFLAN −T5−LG
DEFT −UCSto produce accu-
rately edited-sentences with comparable quality
between MCoEDIT andMFLAN −T5−LG
DEFT −UCS.
8 Conclusion
We introduce DEFT-UCS, a data-efficient fine-
tuning framework that leverages unsupervised core-
set selection to find the minimum amount of data
needed to fine-tune a PLM for text-editing tasks.
Our best performing DEFT-UCS model, fine-tuned
with only 32.5% of the CoEDIT dataset (Rahejaet al., 2023), perform comparably to the SoTA text-
editing model CoEDIT (Raheja et al., 2023), and
superior to the LIMA approach (Zhou et al., 2023a)
both in quantitative and qualitative evaluations. Hu-
man evaluators also preferred edits generated by
DEFF-UCS model over CoEDIT.
These results show the overall utility of our
DEFT-UCS framework towards data-efficient fine-
tuning of PMLs in text-editing tasks. To better un-
derstand the generalizability of our framework, we
plan to first apply it to a variety of text-generation
tasks in the future. Subsequently, we aim to bench-
mark the efficacy of different data-sampling strate-
gies across various PLMs for these tasks.Limitations The hyper-parameters within the
UCS algorithm of our DEFT-UCS framework are
selected manually using task specific knowledge.
Future work should consider how to automate the
selection of these hyper-parameters. Additionally,
while our UCS algorithm leverages the distance
between data samples and centroid distance for
defining sampling methods within DEFT-UCS, fu-
ture work should explore other sampling methods
informative to NLP tasks. Additionally, we show
the utility of DEFT-UCS in the context of six dif-
ferent text-editing tasks; benchmarking the utility
of DEFT-UCS in other task specific domains is
needed to understand the scope of our framework.
Similarly, more work is required to investigate the
utility of DEFT-UCS in fine-tuning different PLMs
for downstream NLP tasks. Future work also en-
tails comparing the benefit of utilizing DEFT-UCS
against PEFT (Fu et al., 2023; Hu et al., 2021) ap-
proaches, understanding whether DEFT-UCS in
conjunction with PEFT can further improve the
fine-tuning efficiency of PLMs.
Ethics Statement We utilize a publicly available
dataset from CoEDIT4. The dataset primarily fo-
cuses on non-meaning changing text edits and does
not raise any privacy concerns. Nevertheless, the
underlying autoregressive models may hallucinate
and propagate biases. Before deploying for real
world applications, considerations on how to in-
corporate user feedback for continual system im-
provement should be studied. Additionally, we
have acknowledged the limitations of our DEFT-
UCS framework and the need for more extensive
benchmarking with various other PLMs and down-
stream tasks. Our work provides a initial set of
results and is an effort to motivate further research
in data-efficient fine-tuning of PLMs.
References
Fernando Alva-Manchego, Louis Martin, Antoine Bor-
des, Carolina Scarton, Benoît Sagot, and Lucia Spe-
cia. 2020. Asset: A dataset for tuning and evalua-
tion of sentence simplification models with multiple
rewriting transformations. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 4668–4679.
Jean-Michel Attendu and Jean-Philippe Corbeil. 2023.
Nlu on data diets: Dynamic data subset selec-
tion for nlp classification tasks. arXiv preprint
arXiv:2306.03208 .
4https://huggingface.co/datasets/grammarly/coeditVighnesh Birodkar, Hossein Mobahi, and Samy Bengio.
2019. Semantic redundancies in image-classification
datasets: The 10% you don’t need. arXiv preprint
arXiv:1901.11409 .
Tuhin Chakrabarty, Vishakh Padmakumar, and He He.
2022. Help me write a poem: Instruction tuning
as a vehicle for collaborative poetry writing. arXiv
preprint arXiv:2210.13669 .
Mayee F Chen, Nicholas Roberts, Kush Bhatia, Jue
Wang, Ce Zhang, Frederic Sala, and Christopher Ré.
2023. Skill-it! a data-driven skills framework for
understanding and training language models. arXiv
preprint arXiv:2307.14430 .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311 .
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In 2009 IEEE conference
on computer vision and pattern recognition , pages
248–255. Ieee.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong
Zhang. 2023. Active prompting with chain-of-
thought for large language models. arXiv preprint
arXiv:2302.12246 .
Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung
Kim, Melissa Lopez, and Dongyeop Kang. 2022.
Understanding iterative revision from human-written
text. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 3573–3590.
Jane Dwivedi-Yu, Timo Schick, Zhengbao Jiang, Maria
Lomeli, Patrick Lewis, Gautier Izacard, Edouard
Grave, Sebastian Riedel, and Fabio Petroni. 2022.
Editeval: An instruction-based benchmark for text
improvements. arXiv preprint arXiv:2209.13331 .
Joseph L Fleiss and Jacob Cohen. 1973. The equiva-
lence of weighted kappa and the intraclass correlation
coefficient as measures of reliability. Educational
and psychological measurement , 33(3):613–619.
Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai
Lam, Lidong Bing, and Nigel Collier. 2023. On the
effectiveness of parameter-efficient fine-tuning. InProceedings of the AAAI Conference on Artificial
Intelligence , volume 37, pages 12799–12807.
Sariel Har-Peled and Akash Kushal. 2005. Smaller
coresets for k-median and k-means clustering. In
Proceedings of the twenty-first annual symposium on
Computational geometry , pages 126–134.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Hamish Ivison, Noah A Smith, Hannaneh Hajishirzi,
and Pradeep Dasigi. 2022. Data-efficient finetuning
using cross-task nearest neighbors. arXiv preprint
arXiv:2212.00196 .
Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and
Rishabh Iyer. 2021. Retrieve: Coreset selection for
efficient and robust semi-supervised learning. Ad-
vances in Neural Information Processing Systems ,
34:14488–14501.
Pang Wei Koh and Percy Liang. 2017. Understanding
black-box predictions via influence functions. In
International conference on machine learning , pages
1885–1894. PMLR.
Shiye Lei and Dacheng Tao. 2023. A comprehen-
sive survey to dataset distillation. arXiv preprint
arXiv:2301.05603 .
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461 .
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.
Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex
Wang, Marzieh Fadaee, and Sara Hooker. 2023.
When less is more: Investigating data pruning
for pretraining llms at scale. arXiv preprint
arXiv:2309.04564 .
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2022. MetaICL: Learning to learn
in context. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2791–2809, Seattle, United States.
Association for Computational Linguistics.
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
2020. Coresets for data-efficient training of machine
learning models. In International Conference on
Machine Learning , pages 6950–6960. PMLR.Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-
har, Sahaj Agarwal, Hamid Palangi, and Ahmed
Awadallah. 2023. Orca: Progressive learning from
complex explanation traces of gpt-4. arXiv preprint
arXiv:2306.02707 .
Courtney Napoles, Keisuke Sakaguchi, and Joel
Tetreault. 2017. Jfleg: A fluency corpus and
benchmark for grammatical error correction. arXiv
preprint arXiv:1702.04066 .
Jianmo Ni, Gustavo Hernández Ábrego, Noah Con-
stant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei
Yang. 2021. Sentence-t5: Scalable sentence encoders
from pre-trained text-to-text models. arXiv preprint
arXiv:2108.08877 .
Mansheej Paul, Surya Ganguli, and Gintare Karolina
Dziugaite. 2021. Deep learning on a data diet: Find-
ing important examples early in training. Advances
in Neural Information Processing Systems , 34:20596–
20607.
Reid Pryzant, Richard Diehl Martinez, Nathan Dass,
Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020.
Automatically neutralizing subjective bias in text. In
Proceedings of the aaai conference on artificial intel-
ligence , volume 34, pages 480–489.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Vipul Raheja, Dhruv Kumar, Ryan Koo, and Dongyeop
Kang. 2023. Coedit: Text editing by task-specific
instruction tuning. arXiv preprint arXiv:2305.09857 .
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
Yuxiong He. 2020. Deepspeed: System optimiza-
tions enable training deep learning models with over
100 billion parameters. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining , pages 3505–3506.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
arXiv preprint arXiv:1908.10084 .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio
Petroni, Patrick Lewis, Gautier Izacard, Qingfei You,
Christoforos Nalmpantis, Edouard Grave, and Sebas-
tian Riedel. 2022. Peer: A collaborative language
model. arXiv preprint arXiv:2208.11663 .Ketan Rajshekhar Shahapure and Charles Nicholas.
2020. Cluster quality analysis using silhouette score.
In2020 IEEE 7th international conference on data
science and advanced analytics (DSAA) , pages 747–
748. IEEE.
Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu,
Canoee Liu, Simon Tong, Jindong Chen, and Lei
Meng. 2023. Rewritelm: An instruction-tuned large
language model for text rewriting. arXiv preprint
arXiv:2305.15685 .
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya
Ganguli, and Ari Morcos. 2022. Beyond neural scal-
ing laws: beating power law scaling via data pruning.
Advances in Neural Information Processing Systems ,
35:19523–19536.
Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,
Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,
Luke Zettlemoyer, Noah A Smith, et al. 2022. Selec-
tive annotation makes language models better few-
shot learners. arXiv preprint arXiv:2209.01975 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B Hashimoto. 2023. Alpaca: A
strong, replicable instruction-following model. Stan-
ford Center for Research on Foundation Models.
https://crfm. stanford. edu/2023/03/13/alpaca. html ,
3(6):7.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V . Le. 2021. Finetuned language mod-
els are zero-shot learners. ArXiv , abs/2109.01652.
Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,
and Chris Callison-Burch. 2016a. Optimizing sta-
tistical machine translation for text simplification.
Transactions of the Association for Computational
Linguistics , 4:401–415.
Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,
and Chris Callison-Burch. 2016b. Optimizing sta-
tistical machine translation for text simplification.
Transactions of the Association for Computational
Linguistics , 4:401–415.
Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming
Sun, and Ping Li. 2022. Dataset pruning: Reducing
training data by examining generalization influence.
arXiv preprint arXiv:2205.09329 .
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T Kwok, Zhen-
guo Li, Adrian Weller, and Weiyang Liu. 2023.
Metamath: Bootstrap your own mathematical ques-
tions for large language models. arXiv preprint
arXiv:2309.12284 .Yue Zhang, Leyang Cui, Deng Cai, Xinting Huang,
Tao Fang, and Wei Bi. 2023. Multi-task instruc-
tion tuning of llama for specific scenarios: A pre-
liminary study on writing assistance. arXiv preprint
arXiv:2305.13225 .
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023a. Lima: Less is more for align-
ment. arXiv preprint arXiv:2305.11206 .
Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng,
Dongze Lian, Yifan Zhang, Yang You, and Jiashi
Feng. 2023b. Dataset quantization. In Proceedings
of the IEEE/CVF International Conference on Com-
puter Vision , pages 17205–17216.A DEFT-UCS Applied to CoEDIT
A.1 CoEDIT Dataset Details
The CoEDIT dataset, DcoEDIT , from Raheja et al.
(2023) is comprised of several edit tasks, including
fluency, coherence, clarity, paraphrasing, neutral-
ization and formalization. As mentioned in (Raheja
et al., 2023), the 82k data samples follow the for-
mat of ⟨instruction :source, target ⟩pairs. The
source and target pairs come from a variety of dif-
ferent datasets related to each editing task. Table 4
summarizes the datasets utilized to represent each
edit task in DCoEDIT . The instruction component
are task-specific and generated from a pool of in-
structional prompts. For example, for a grammar
correction task, an instruction could be “Fix gram-
mar errors” or “Fix grammatical errors in this sen-
tence”. The list of all instructional prompts utilized
are detailed in (Raheja et al., 2023).
A.2 DEFT-UCS Model Fine-Tuning Details
Recall that all DEFT-UCS models in this pa-
per are produced by fine-tuning Flan-T5 Large
(Chung et al., 2022). We fine-tune Flan-T5 Large
such that we can make accurate comparisons with
MCoEDIT (Raheja et al., 2023) which represents
a fine-tuned Flan-T5-Large model on DCoEDIT .
Furthermore, to remove any difference in model
performances due to differing hyperparameters, we
utilize the hyperparameters listed in Raheja et al.
(2023). Specifically, we use the Adam optimizer
with a learning rate of 1e-4. All DEFT-UCS models
in the main paper are trained for 5 epochs with early
stopping and the model checkpoints with the best
validation loss are saved. To perform fine-tuning,
we leverage 4 A10G GPUs, from AWS G5 in-
stances, using Deepspeed (Rasley et al., 2020), and
the maximum source and target sequence length is
set to 256.
B Embedding Representations in UCS
B.1 Representation Details
For K-means clustering to learn informative clus-
ters, selecting the right latent space representa-
tion for the input data is important. In our ap-
plication, an accurate embedding representation
should allow each cluster to predominantly repre-
sent a certain type of editing task. For example
all data related to editing for fluency should be
clustered together, whereas all data related to gram-
mar correction should be clustered together. ToEdit Task Datasets in DcoEDIT
Fluency NUCLE-14
Lang-8
BEA-19
Coherence DiscoFuse
Clarity
(Simplification)NEWSELA
WikiLarge
WikiAuto
ParabankV2
Iterator-Clarity
Paraphrasing ParabankV2
Formalization GYAFC
Neutralization WNC
Table 4: Data in DCoEDIT (Raheja et al., 2023) is com-
prised of samples from the above datasets. This table is
a simplified version of Table 1 in Raheja et al. (2023).
ultimately select an accurate embedding represen-
tation, we experimented with three different repre-
sentations: sentence-level encoding from Sentence-
T5 (Ni et al., 2021), BART CLS token embed-
ding, as well as an averaged word token embedding
from Flan-T5. As a brief summary, Sentence-T5
(Ni et al., 2021) maps sentences to a 768 dimen-
sional vector space using only the encoder from
T5. Specifically, Ni et al. (2021) demonstrate that
Sentence-T5 embeddings are able to lead to high
performance in sentence transfer tasks. Similarly,
we also experiment with BART (Lewis et al., 2019)
CLS token embeddings, inspired by the notion that
CLS token can provide informative representations
of the input sentence for downstream tasks (Devlin
et al., 2018). We also experiment with an average
pooling method of averaging all word embeddings
of an input sequence, using the Flan-T5 model, to
reach a sentence-level embedding.
B.2 Representation Analysis
Figure 5 demonstrates the K-means clustering re-
sults for each sentence-level embedding represen-
tation. Overall, we find that Sentence-T5 provides
the strongest sentence-level embedding that allows
the clustering algorithm to best separate input data
based on its related editing task. Specifically, when
analyzing Figure 5(a), we see that each cluster is
largely comprised of a single edit-task. For ex-
ample, cluster 1 largely includes data related to
“paraphrasing”, while cluster 4 largely includes data
related to improving “coherence”. In Figure 5(b)
and Figure 5(c) we observe that the task specific
data is more distributed among several clusters,
indicating weaker cluster separation among the dif-
ferent editing task related data. Although the clus-ters formed via Sentence-T5 embeddings (Ni et al.,
2021) are not perfect, they offer the strongest sep-
aration of task-related data compared to the other
embedding representations. Given these results, we
leverage Sentence-T5 as our latent space represen-
tation when performing UCS.
C Evaluation Dataset Details
For all datasets used in our evaluation, we utilize
the publicly available test splits from each dataset.
To each data sample (source and target pair), we
prepend a randomly selected instructional prompt
related to the edit task. For example, for all test
samples from TurkCorpus, we prepend a randomly
selected instructional prompt from the text simplifi-
cation choices provided in Raheja et al. (2023). In
Table 5 we provide example test data samples from
each evaluation dataset. For context, we addition-
ally provide the sizes of the test splits available for
each evaluation dataset. The test splits are as fol-
lows: TurkCorpus includes 359 test data samples,
Asset includes 359, Iterator Coherence includes
36, Iterator Clarity contains 186, Iterator Fluency
contains 88, JFLEG contains 748 and WNC con-
tains 1000. Note, we additionally evaluate on a
combined Iterator dataset, noted as Iterator Global
in Table 1, which includes all test samples from
Iterator Coherence, Clarity and Fluency. The mo-
tivation of including an Iterator Global evaluation
dataset is to understand model performances on a
more generic style-editing task (Du et al., 2022).
Furthermore, in Figure 6, we provide a TSNE vi-
sualization of the evaluation datsets, particularly
embedding representations of all source sentences
using Sentence-T5 (Ni et al., 2021). The visualiza-
tion demonstrates the diversity among the different
datasets, and highlight that the evaluation tasks are
not all semantically similar.
D Additional DEFT-UCS Results
D.1 Extended Best DEFT-UCS Analysis
In Section 7.3, we demonstrate that utilizing on
32.5% of DCoEDIT can result in an overall best
DEFT-UCS model that surpasses MCoEDIT (Ra-
heja et al., 2023) SARI and ROUGE-L scores on 6
of the 8 evaluation datasets. While Figure 3 in the
main paper provides an analysis using up to 45%
ofDCoEDIT , in this section, we include Figure 7
which provides an exhaustive analysis using up to87.5% of DCoEDIT . From Figure 7, we observe
that to surpass SARI and ROUGE-L scores on 7 out
of the 8 evaluation datasets, 47.5% of DCoEDIT is
necessary. Additionally, we observe that while 75%
ofDCoEDIT can be leveraged to surpass ROUGE-
L scores on all evaluation datasets. Overall, these
results indicate a trade-off between marginal im-
provement in model performance and the amount
of additional data required.
D.2 Additional Qualitative Analysis
In Table 6, we present example model outputs, qual-
itatively comparing MCoEDIT ,MFlan −T5−LG
DEFT −UCS,
MLLAMA −7BandMBLOOM −560M. Overall, we
observe that the example sentences generated by
MFlan −T5−LG
DEFT −UCSandMCoEDIT are either identi-
cal or similarly edit the input sentence to reflect
the edit instruction. When we examine the zero-
shot inference outputs from MLLAMA −7Band
MBLOOM −560Mwe observe that these models are
not able to produce accurately edited sentences.
Instead, we notice repeated generation from both
MLLAMA −7BandMBLOOM −560Mas well as ad-
ditional generations that are tangential. These re-
peated, longer, and irrelevant generated sentences
also explain the much lower ROUGE-L observed
in Table 2 within the main paper. Overall, these
generated outputs from each model provide fur-
ther understanding of the need for instruction-tuned
LLMs for tasks such as text-editing. These gener-
ated output examples also re-iterate that our DEFT-
UCS model, MFlan −T5−LG
DEFT −UCS, can generate simi-
larly edited sentences to the CoEDIT baseline,
MCoEDIT , while being fine-tuned on 70% less
data.(a)
 (b)
(c)
Figure 5: Comparing the distribution of task-related data among clusters after performing K-Means when utilizing
Sentence-T5 embedding (a), BART CLS embeddings (b) and averaged Flan-T5 word embeddings (c) for sentence
representations.
Figure 6: TSNE visualization of the source sentences within all evaluation datasets.Evaluation Dataset Edit Task Input Example Output Example
TurkCorpus
(Xu et al., 2016a)Text Simplification Make the sentence simple: The great
dark spot is thought to represent a hole
in the methane cloud deck of neptune.The great dark spot is thought to repre-
sent a hole in the methane.
Asset
(Alva-Manchego et al., 2020)Simplification Simplify this sentence: She remained in
the United States until 1927 when she
and her husband returned to France.She remained in the United States until
returning to France with her husband in
1927.
Iterator Coherence
(Du et al., 2022)Coherence Fix sentence flow: Based on the general
linguistic structure of humor, in this pa-
per, we propose a novel approach for
detecting humor in short texts by using
BERT sentence embedding.In this paper, we propose a novel ap-
proach for detecting humor in short
texts by using BERT sentence embed-
ding .
Iterator Clarity
(Du et al., 2022)Clarity Write a clearer version for the sentence:
Using our human-evaluation datasets,
we show that existing metrics based on
n-gram similarity do not correlate with
human judgments.Using our human-evaluation datasets,
we show that widely used n-gram simi-
larity do not correlate with human judg-
ments.
Iterator Fluency
(Du et al., 2022)Fluency Fix disfluencies in the sentence: In addi-
tion, we provide the first robust corpus
this kind for the Brazilian Portuguese
language.In addition, we provide the first robust
corpus of this kind for the Brazilian Por-
tuguese language.
JFLEG
(Napoles et al., 2017)Grammar Correction Fix the grammar mistakes: Every per-
son needs to know a bit about math, sci-
ences, arts, literature and history in or-
der to stand out in society.Every person needs to know a bit about
math, science, art, literature and history
in order to stand out in society.
WNC
(Pryzant et al., 2020)Neutralization Remove points of view: During the un-
necessary horseplay, Hamlin fell and
severely injured his hand.During the horseplay, Hamlin fell and
severely injured his hand.
Table 5: Example data samples for each evaluation dataset. Note, the instructional prompt (italicized) are randomly
sampled from a list of instructional prompts available in Raheja et al. (2023).(a)
(b)
Figure 7: Presenting a full analysis, utilizing up to 87.5% of DCoEDIT , on the different sampling methods and
amounts of data needed to beat soTA MCoEDIT performance, considering SARI (a) and ROUGE-L (b) scores.Generated Model Outputs
Input MCoEDIT MFlan −T5−LG
DEFT −UCSMLLaMA 2−7B MBLOOM −560M
Make this easier to un-
derstand: the term “pro-
tein” itself was coined
by berzelius, after mul-
der observed that all pro-
teins seemed to have the
same empirical formula
and might be composed of
a single type of -lrb- very
large -rrb- molecule.The term “protein” itself
was created by berzelius
in 1879, after mulder
observed that all pro-
teins seemed to have the
same empirical formula
and might be composed of
a single type of very large
molecule.berzelius thought that all
proteins seemed to have
the same empirical for-
mula and might be com-
posed of a single type of
molecule.It is a protein made by bac-
teria that is used to make
a large number of other
proteins. This is a very
large protein that is used
to make a large number of
other proteins. This is a
very large protein that is
used to make a large num-
ber of other proteins. It is
a protein made by bacteria
that is used to make a large
number of other proteins.
It is a protein made by bac-
teria that is used to make a
large number of other pro-
teins.The term was coined by
mulder to describe the fact
that the protein was com-
posed of a single type
of -lrb- molecule. The
term was coined by mul-
der to describe the fact
that the protein was com-
posed of a single type
of -lrb- molecule. The
term was coined by mul-
der to describe the fact that
the protein was composed
of a single type of -lrb-
molecule.
Remove non-neutral
POVs: jimi hendrix
(musician), great musician
and vocalist died while
at girlfriend’s monika
dannemann hotel room in
london.jimi hendrix (musician),
musician and vocalist
died while at girlfriend’s
monika dannemann hotel
room in london.jimi hendrix (musician),
musician and vocalist
died while at girlfriend’s
monika dannemann hotel
room in london.This was an interesting
read. I think I’m going
to have to edit that arti-
cle. I’ve been doing some
research on this, and it
seems that the story is that
Hendrix was at the hotel
with Monika Dannemann.
She was a German woman
who had been a friend of
his for some time. She
was a model and an artist,
and she had been a fan of
his music since she was
a teenager. She had been
in London with him for a
couple of weeks, and she
had been helping him out
with his music.the singer was in a coma
for a week. the singer was
in a coma for a week. the
singer was in a coma for a
week. the singer was in
a coma for a week. the
singer was in a coma for
a week. the singer was in
a coma for a week. the
singer was in a coma for
a week. the singer was in
a coma for a week. the
singer was in a coma for
a week. the singer was in
a coma for a week.
Fix grammaticality in this
sentence: The important
thing about this case is ,
each countrey hase to use
official solution for their
learning problems.The important thing about
this case is that each coun-
try has to use an official
solution for their learning
problems.The important thing about
this case is that each coun-
try has to use an official
solution for their learning
problems.The important thing about
this case is that each coun-
try has to use its own offi-
cial solution for their learn-
ing problems. The impor-
tant thing about this case
is that each country has to
use its own official solu-
tion for their learning prob-
lems.The main purpose of this
paper is to study the gram-
maticality of the sentence:
The important thing about
this case is , each countrey
hase to use official solu-
tion for their learning prob-
lems. The main purpose of
this paper is to study the
grammaticality of the sen-
tence: The important thing
about this case is, each
countrey hase to use offi-
cial solution for their learn-
ing problems.
Fix disfluencies in the
sentence: To work hard
means to run the company.To work hard means to run
a company.To work hard means to run
a company.Disfluencies are speech
disruptions that are often
caused by an inability to
find the right word. Disflu-
encies are common in ev-
eryday speech. Examples
of disfluencies in English
1. To work hard means to
run the company. 2. To
work hard means to run
the companyThe sentence is not a sen-
tence, but a sentence with
a disfluency. The sentence
is not a sentence, but a
sentence with a disfluency.
The sentence is not a sen-
tence, but a sentence with
a disfluency. The sentence
is not a sentence, but a
sentence with a disfluency.
The sentence is not a sen-
tence, but a sentence with
a disfluency.
Table 6: Example generated, edited sentences from each model for a given input. We observe that non-instruction
tuned LMs such as BLOOM-560M and LLAMA-7B mostly struggle in zero-shot inference as demonstrated by the
repeated or irrelevant generation. Raheja et al. (2023).