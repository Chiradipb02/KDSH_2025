DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
LLMs Be Better Context-aware Translators
Xinglin Lyu♣, Junhui Li♣*, Yanqing Zhao♠, Min Zhang♠, Daimeng Wei♠,
Shimin Tao♠, Hao Yang♠, Min Zhang♣
♣School of Computer Science and Technology, Soochow University, Suzhou, China
♠Huawei Translation Services Center, Beijing, China
xllv2020@stu.suda.edu.cn ,{lijunhui,minzhang}@suda.edu.cn
{zhaoyanqing,zhangmin186,weidaimeng,taoshimin,yanghao30}@huawei.com
Abstract
Generally, the decoder-only large language
models (LLMs) are adapted to context-aware
neural machine translation (NMT) in a con-
catenating way, where LLMs take the con-
catenation of the source sentence (i.e., intra-
sentence context) and the inter-sentence con-
text as the input, and then to generate the tar-
get tokens sequentially. This adaptation strat-
egy, i.e., concatenation mode, considers intra-
sentence and inter-sentence contexts with the
same priority, despite an apparent difference
between the two kinds of contexts. In this
paper, we propose an alternative adaptation
approach, named Decoding- enhanced Multi-
phase Prompt Tuning (DeMPT), to make
LLMs discriminately model and utilize the
inter- and intra-sentence context and more ef-
fectively adapt LLMs to context-aware NMT.
First, DeMPT divides the context-aware NMT
process into three separate phases. During each
phase, different continuous prompts are intro-
duced to make LLMs discriminately model var-
ious information. Second, DeMPT employs
a heuristic way to further discriminately en-
hance the utilization of the source-side inter-
and intra-sentence information at the final de-
coding phase. Experiments show that our ap-
proach significantly outperforms the concate-
nation method, and further improves the perfor-
mance of LLMs in discourse modeling.1
1 Introduction
Context-aware neural machine translation (NMT)
goes beyond sentence-level NMT by incorporating
inter-sentence context at the document level (Zhang
et al., 2018; Miculicich et al., 2018; V oita et al.,
2018, 2019b,a; Bao et al., 2021; Sun et al., 2022),
aiming to address discourse-related challenges such
as zero pronoun translation (Wang et al., 2019), lex-
ical translation consistency (Lyu et al., 2021, 2022),
*Corresponding author: Junhui Li.
1We release our code and datasets on https://github.
com/xllyu-nlp/DeMPT .and discourse structure (Hu and Wan, 2023). A re-
cent paradigm shift has been witnessed in context-
aware NMT with the emergence of the decoder-
only large language models (LLMs) (BigScience,
2022; Google, 2022; MetaAI, 2023b,a; OpenAI,
2023). These generative language models, trained
on massive data, have gained significant attention
in the natural language processing (NLP) commu-
nity. In adapting LLMs to context-aware NMT,
a common strategy involves concatenating multi-
ple source sentences as a prefix and generating
translations token-by-token, relying on the prefix
and previously predicted target tokens, as shown
in Figure 1 (a). However, a critical observation
of this strategy reveals a potential drawback – the
equal prioritization of the inter- and intra-sentence
contexts during token generation. Importantly, the
intra-sentence context inherently contains richer
parallel semantic information with the target sen-
tence and should be given a higher priority than the
inter-sentence context. Consequently, we propose
that separately modeling and utilizing the inter- and
intra-sentence contexts should explicitly inform
LLMs of the document-level context and the cur-
rent sentence itself, thus being able to prevent the
misallocation of attention weights to source-side to-
kens (Bao et al., 2021; Li et al., 2023). Inspired by
the success of prompt tuning (Li and Liang, 2021;
Liu et al., 2022; Tan et al., 2022), our alternative
approach, named Decoding-Enhanced Multi-phase
Prompt Tuning (DeMPT), aims to enhance LLMs’
adaptability to context-aware NMT, as shown in
Figure 1 (b).2
Specifically, we divide the whole procedure
of context-aware NMT into three phases: inter-
sentence context encoding, intra-sentence context
2Following the findings of Bao et al. (2021), which indi-
cate that source-side context is relatively more important for
document-level MT compared to target-side context, we focus
exclusively on source-side context in this paper. Nonetheless,
we provide an additional discussion on integrating target-side
context in Appendix M.arXiv:2402.15200v2  [cs.CL]  23 Sep 2024LLM
Source Sentence Inter-sentence ContextLLM
Source Sentence Inter-sentence ContextTarget Sentence
LLM LLMTarget Sentence
Concatenating
a.  Concatenation Strategy b. Proposed Approach❄ ❄ ❄ ❄Figure 1: Comparison of different strategies for adapting LLMs to context-aware NMT. The concatenation strategy
(left) treats inter-sentence and intra-sentence (referred to as the "source sentence" context in the figure) with equal
importance. In contrast, our approach ( right ) divides context-aware NMT into three distinct phases, enabling LLMs
to selectively model and leverage both inter- and intra-sentence contexts.
encoding, and decoding. Following Li and Liang
(2021); Liu et al. (2022), we sequentially and dif-
ferentially adapt LLMs for each phase, utilizing
phase-specific trainable prompts. This phased tun-
ing method enables LLMs to independently capture
and model both inter- and intra-sentence contexts,
facilitating a better understanding of their differ-
ences. Our approach splits the input into three parts
without significantly increasing computational load,
thus maintaining inference speed comparable to
concatenation, as detailed in Appendix D.
Furthermore, during the decoding phase, we pro-
pose a heuristic method to emphasize the differ-
ence between inter- and intra-sentence contexts,
and avoid long-distance issue when utilizing inter-
sentence context. Specifically, at each decoding
step, we use LLMs to predict the next token three
times. The decoding states used for each predic-
tion directly concatenate with the representations
of two contexts in a discriminative manner. Fi-
nally, we combine three probability distributions
to search for the next token as the output from the
target vocabulary. This method enables LLMs to
learn not only to properly capture inter-sentence
context in addressing discourse-related issues but
also to recognize a difference between inter- and
intra-sentence contexts, allowing for effective uti-
lization of both types of contexts.
Our contributions can be summarized as follows:
•We introduce a multi-phase prompt tuning ap-
proach that divides context-aware NMT into
three phases, enabling LLMs to distinguish
between inter- and intra-sentence contexts.
•We introduce a enhanced decoding method
that discriminately utilize both context types.
This allows LLMs not only properly capture
inter-sentence context in addressing discourse-related issues, but also be aware of the impor-
tance of the intra-sentence context.
•We validate our approach using llama-2-7b
and bloomz-7b1-mt as foundation models,
demonstrating its effectiveness across five
translation directions. Extensive analyses fur-
ther highlight the substantial enhancement in
LLMs’ ability for context-aware MT.
2 Methodology
In this section, we describe our decoding-enhanced
multi-phase approach for adapting LLMs to
context-aware NMT in details. Specifically, we
break down the whole procedure of context-aware
NMT into three phases (Section 2.1), i.e., inter-
sentence context encoding, intra-sentence encod-
ing, and decoding. Additionally, we discrimina-
tively enhance the utilization of inter- and intra-
sentence contexts during the decoding phase (Sec-
tion 2.2). Finally, we describe our phase-aware
prompts and training objective in Section 2.3 and
Section 2.4, respectively.
For a given document pair (S,T)with Ksen-
tences, we will construct Ktraining instances.
Each training instance is denoted as a tuple (C, S, T ).
Here S=x||S|
krepresents k-th current source sen-
tence with |S|tokens, i.e., intra-sentence context,
andT=y||T|
kis the k-th target sentence with |T|
tokens. Cdenotes the zprevious sentences of S,
i.e., the inter-sentence context of S. We denote the
hidden size of the LLM as d, and Las the number
of transformer layers within it.
2.1 Multi-phase Encoding and Decoding
We implement our approach based on deep prompt
tuning (Li and Liang, 2021; Liu et al., 2022). Next,
we use training instance (C, S, T )as an example toPhase 3 Phase 2
LLM LLM LLMDecoding-enhancedPhase 1Figure 2: Illustration of pipeline of multi-phase prompt
tuning LLM for context-aware NMT. Red lines illustrate
the procedure of enhanced decoding phase.
describe the multi-phase approach. Figure 2 illus-
trates the procedure of multi-phase prompt tuning.
Inter-sentence Context Encoding Phase. In the
inter-sentence context encoding phase (Phase 1 in
Figure 2), we first concatenate all sentences in C
into a sequence, and then utilize the LLM to encode
Cby incorporating the trainable prompt:
H1:L
C=LLM(C,PC), (1)
where H1:L
C∈RL×|C|× dis the sequence of activa-
tions for C,PC∈RL×2q×dis the current-phase train-
able prompt, and qis a hyper-parameter for the
length of the prompt. PCaims to adapt the LLM for
better modeling the inter-sentence context. Same
as basic deep prompting, at the l-th transformer
block, we inject corresponding prompt in PCinto
encoding procedure of Cas follows:
Hl
C=FFN(Multi-Attn (KC,VC,QC)), (2)
QC=Hl−1
C, (3)
KC= [PC[l,:q,:];Hl−1
C], (4)
VC= [PC[l, q:,:];Hl−1
C], (5)
where Hl
C∈R|C|×dis the output of the l-th trans-
former block. FFN and Multi-Attn are the feed-
forward network and multi-head self-attention sub-
layers, respectively.3[·;·]and[·:·]are the concate-
nating and slicing operations, respectively.
Intra-sentence Context Encoding Phase. In the
intra-sentence context encoding phase (Phase 2 in
Figure 2), the LLM encodes the intra-sentence con-
textSby conditioning on the past activations of the
inter-sentence context H1:L
Cand trainable prompt:
H1:L
S=LLM(S, H1:L
C,PS), (6)
3For simplicity, we omit the normalization and residual
operations in this paper.where H1:L
S∈RL×|S|×dis the sequence of activa-
tions for S, andPS∈RL×2q×ddenotes current-
phase prompt. Similarly, at the l-th transformer
block, we incorporate HCandPSinto the encoding
procedure of Sas follows:
Hl
S=FFN(Multi-Attn (KS,VS,QS)), (7)
QS=Hl−1
S, (8)
KS= [PS[l,:q,:];Hl−1
C;Hl−1
S], (9)
VS= [PS[l, q:,:];Hl−1
C;Hl−1
S], (10)
where Hl
Sis output of the l-th transformer block,
which fuses Hl−1
C, the l−1layer output of the
inter-sentence context encoding.
Decoding Phase. In the decoding phase (Phase
3 in Figure 2), given the past activations HSand
trainable prompt, we call the LLM again to gener-
ate the hidden state for predicting the probability
of the target sentence:
H1:L
T=LLM(T, H1:L
S,PT), (11)
where H1:L
T∈RL×|T|×dis the sequence of activa-
tions for T, andPT∈RL×2q×dis current-phase
prompt. Similarly, we inject SandPTinto the
decoding procedure of Tas follows:
Hl
T=FFN(Multi-Attn (KT,VT,QT)), (12)
QT=Hl−1
T, (13)
KT= [PT[l,:q,:];Hl−1
S;Hl−1
T], (14)
VT= [PT[l, q:,:];Hl−1
S;Hl−1
T], (15)
where Hl
T∈R|T|×dis the decoding state of the l-
th transformer block. Finally, we refer the t-th
decoding state as hL
t(i.e., HL
T=hL
t||T|+1
t=1) which is
used to predict the next token yt:
p(yt|S,C, y<t) =Softmax
hL
tW
, (16)
where W∈Rd×|V|is parameter of LLM-Head layer
and|V|is the vocabulary size.
2.2 Enhanced Decoding Phase
As shown in Figure 2, both the inter-sentence con-
text representation H1:L
Cand the intra-sentence con-
text representation H1:L
Sare used as keys and val-
ues when generating hidden states of next phase.
Meanwhile, hidden states of decoding phase, i.e.,
hL
i||T|
i=1are used to predict next tokens. On the one
hand, while the decoding hidden states incorpo-
rate both inter- and intra-sentence contexts, there
is no explicit differentiation between the two whenFFNLLM-Head
FFNFigure 3: Illustration of the procedure of our proposed
decoding-enhanced approach at the t-th decoding step.
predicting next tokens. On the other hand, the inter-
sentence context representation H1:L
Cand decoding
hidden states H1:L
Tare mediated by hidden states
of phases 2, i.e., H1:L
S. This may result in a long-
distance issue such that the inter-sentence context
are not properly aligned by target-side tokens.
Therefore, to address above two issues, we pro-
pose an enhanced decoding phase with an aim to
more effectively utilize both the inter- and intra-
sentence contexts. Inspired by Kuang et al. (2018),
we move both the two types of inter- and intra-
sentence contexts closer to target words to achieve
a tight interaction between them. Specifically, we
concatenate the decoding states with the two types
of representations to predict the next target words.
As shown in Figure 3, the enhanced next word pre-
diction peis a combination of three distributions:
pe(yt|S,C, y<t) =λ1×ˆp(yt|S,C, y<t)
+λ2×¯p(yt|S,C, y<t)
+ (1−λ1−λ2)×p(yt|S,C, y<t),
(17)
where λ1andλ2control the contribution of ˆp(yt|·)
and¯p(yt|·), respectively, which can be further for-
mulated as:
ˆp(yt|S,C, y<t) =Softmax
ˆhL
tW
, (18)
¯p(yt|S,C, y<t) =Softmax
¯hL
tW
, (19)
ˆhL
t=FFN
[˜HL
C;˜HL
S;hL
t]
, (20)
¯hL
t=FFN
[˜HL
S;hL
t]
, (21)
where Wis same as in Eq. 16, ˜HL
S∈Rdand ˜HL
C∈Rd
are the averaged HL
SandHL
Cat token level, respec-tively.4To further identify the effect of inter- and
intra-sentence context in this strategy, we provide
an ablation study about ˆpand¯pin Appendix J.
2.3 Phase-aware Prompts
We emphasize the LLM needs to play various
roles across three phases, and maintaining similar
prompts across different phases may not be rea-
sonable. Thus, we empower LLM to distinguish
different phases by introducing a type embedding
and a transfer layer5for these prompts:
Pr= (tanh ( OrW1))W2+TypeEmb (r), (22)
whereOr∈RL×2q×dis randomly initialized prompt,
W1, W 2∈Rd×dare the trainable parameters, and
TypeEmb( ·) is type embeddings layer of the
prompts. r∈ {C, S, T}represents either phase 1,
phase 2, or phase 3.
2.4 Training Objective
We employ the cross-entropy loss as the training
objective of our model. Given a training instance
(C, S, T ), its training loss is defined as:
L(C, S, T ) =−1
|T||T|X
t=1logpe(yt|S,C, y<t). (23)
Notably, the parameters in LLM, including Win
Eq. 16, 18, 19, are frozen during training.
3 Experimentation
We build our approach upon two open-source
LLMs, i.e., llama-2-7b6and bloomz-7b1-mt7.
We verify the effectiveness of our proposed ap-
proach on five translation tasks, including {Chinese
(ZH), French (FR), German (DE), Spanish (ES),
Russian (RU)} →English (EN).
4Notably, the computation of ˆpand¯pdoes not require a
full decoding forward pass. It involves solely an FFN layer
(two linear transformation layers and a ReLU activation layer),
an LLM-Head layer (a linear transformation layer), and a
softmax function layer.
5Unlike the multi-layer perceptrons (MLPs) used for repa-
rameterization, our transfer layer shares parameters across all
prompts, reducing the number of trainable parameters. Table 3
compares the trainable parameters of various tuning methods,
and Appendix J analyzes the effect of the transfer layer.
6https://huggingface.co/meta-llama/
Llama-2-7b-hf
7https://huggingface.co/bigscience/
bloomz-7b1-mtModelZH→EN FR →EN DE →EN ES →EN RU →EN Average
BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET
⊘Trans. 29.86 0.8406 38.53 0.8545 41.44 0.8682 48.74 0.8783 32.25 0.8169 38.16 0.8517
Traditional context-aware NMT models
⊙MR-Trans. 30.61 0.8413 38.72 0.8533 42.11 0.8693 49.69 0.8812 33.27 0.8211 38.88 0.8532
+ mBART 32.69 0.8601 42.01 0.8759 44.61 0.8840 51.67 0.8831 36.39 0.8459 41.39 0.8698
⊙G-Trans. 30.99 0.8411 38.96 0.8524 42.46 0.8658 49.68 0.8794 33.59 0.8201 39.14 0.8518
+ mBART 32.99 0.8597 42.02 0.8764 44.81 0.8836 52.07 0.8911 36.83 0.8461 41.74 0.8714
llama-2-7b as foundation model
⊘MT-LoRA 27.43 0.8511 38.18 0.8647 40.96 0.8712 47.52 0.8733 33.00 0.8311 37.42 0.8583
⊘MT-PT 31.32 0.8565 41.92 0.8675 43.56 0.8752 51.32 0.8819 35.46 0.8333 40.72 0.8629
⊙CMT-PT 31.13 0.8387 42.01 0.8699 43.11 0.8762 51.66 0.8823 35.91 0.8396 40.76 0.8613
⊙MPT *33.21 0.8645 †43.11 0.8744 *43.88 0.8824 †52.01 0.8913 †36.49 0.8456 41.74 0.8716
⊙DeMPT *33.89 0.8658 †43.71 0.8816 *44.69 0.8899 †53.10 0.8979 †36.55 0.8438 42.39 0.8758
bloomz-7b1-mt as foundation model
⊘MT-LoRA 25.79 0.8466 35.67 0.8601 35.17 0.8522 46.32 0.8644 28.01 0.8012 34.21 0.8449
⊘MT-PT 30.99 0.8520 40.49 0.8661 37.76 0.8579 50.68 0.8823 30.27 0.8106 38.04 0.8539
⊙CMT-PT 30.82 0.8504 40.31 0.8639 38.01 0.8601 50.26 0.8832 29.80 0.8108 37.84 0.8537
⊙MPT *31.81 0.8601 *41.11 0.8766 †38.99 0.8669 *51.33 0.8910 *30.99 0.8201 38.85 0.8629
⊙DeMPT *32.46 0.8649 *41.92 0.8790 †40.06 0.8703 *52.25 0.8990 *31.79 0.8253 39.70 0.8677
Table 1: Results of different systems on sacreBLEU and COMET metrics. DeMPT /MPT is our proposed Multi-
phase Prompt Tuning approach with/without Decoding-enhanced strategy (in Sec. 2.2). Scores with bold indicate
the best performance. * (or †) indicates the gains are statistically significant over MT-PT (or CMT-PT) with
p<0.01 (Koehn, 2004). ⊘and⊙indicate the model is context-agnostic andcontext-aware , respectively.
3.1 Experimental Settings
Datasets and Preprocessing. The corpus
of all translation tasks is extracted from
News-Commentary-v18 . For LLM-based models,
We use the tokenizer of foundation models to
process the input data and no other preprocessing
is performed. See Appendix A for more details on
splitting, preprocessing and statistics of datasets.
Besides, we provide a discussion for scales of the
training set in Appendix I .
Baselines. In addition to traditional context-
agnostic (Trans.) and context-aware (G-Trans (Bao
et al., 2021) and MR-Trans (Sun et al., 2022) with
or without pre-training setting, i.e., + mBART ( ?))
NMT models with encoder-decoder architecture
,8our primary comparison focuses on the follow-
ing three LLM-based alternatives: 1) MT-LoRA :
It is a tuned LLM adapted to NMT task via the
tuning method of Low-Rank Adaptation (Hu et al.,
2022), which makes large-scale pre-training mod-
els adapt to a new task by injecting a trainable rank
decomposition matrice into each layer of the Trans-
former architecture; 2) MT-PT : It is a tuned LLM
adapted to NMT task via the deep prompt tuning
8Please refer to Appendix C for more introduction about
the G-Trans and MR-Trans.with MLPs reparameterization,9which only tunes
continuous prompts with a frozen language model;
3)CMT-PT : It indiscriminately utilizes inter- and
intra-sentence context via the concatenation strat-
egy, as depicted in Figure 1 (a). Similar to MT-PT,
it is also a tuned LLM via the deep prompt tuning
with MLPs reparameterization. Among them, MT-
LoRA and MT-PT are context-agnostic systems
while CMT-PT is a context-aware system. For a
fair comparison, we ensure that all context-aware
models built upon LLM, including CMT-PT, MPT,
and DeMPT, incorporate identical inter-sentence
context. We provide more discussion in utilization
of various inter-sentence contexts in Appendix L
and M.
Model Setting and Training. For all encoder-
decoder Transformer models, including Trans-
former (Trans.), MR-Trans and G-Trans10, we im-
plement them upon Fairseq (Ott et al., 2019). For
MT-LoRA models, we set the rank of trainable ma-
trices as 16 which performs best in our preliminary
experiment. For all MT-PT models, CMT-PT mod-
9We attempt to remove reparameterization but experience
a significant decline in performance.
10For G-Trans, we use their official implementation upon
Fairseq. Code: https://github.com/baoguangsheng/
g-transformer .Model ZH → FR→ DE→ ES→ RU→ Avg.
⊘Trans. 47.63 54.41 58.29 62.52 48.79 54.33
Traditional context-aware NMT model
⊙MR-Trans. 48.51 55.55 59.02 63.51 49.88 55.29
+ mBART 50.66 58.01 61.99 66.01 54.11 58.15
⊙G-Trans. 48.99 55.31 59.23 63.99 50.09 55.52
+ mBART 50.98 57.88 61.97 66.21 54.33 58.27
llama-2-7b as foundation model
⊘MT-LoRA 44.83 54.52 57.72 62.18 49.06 53.66
⊘MT-PT 49.49 57.87 60.89 65.02 52.59 57.17
⊙CMT-PT 49.53 58.27 61.23 65.89 53.34 57.65
⊙MPT 51.56 59.56 62.15 67.14 54.18 58.92
⊙DeMPT 52.68 60.33 63.11 67.95 54.94 59.80
bloomz-7b1-mt as foundation model
⊘MT-LoRA 43.23 51.82 51.12 61.77 43.29 50.25
⊘MT-PT 49.48 56.81 55.40 64.71 46.14 54.51
⊙CMT-PT 49.61 57.05 55.81 65.12 46.09 54.74
⊙MPT 50.22 57.93 56.69 66.25 47.29 55.68
⊙DeMPT 50.62 58.30 57.34 67.12 48.00 56.28
Table 2: Results of different systems on BlonDe metric.
els, and our models, we set the prompt length q
as 64.11For the incorporation of inter-sentence
context in CMT-PT and our models, we consider a
dynamic z, in which the total tokens are no more
than 256. In enhanced decoding, we consider the
three next word predictions to be equally important
by setting both λ1andλ2to 1/3. We provide an
analysis of λand more training details in Appendix
K and B, respectively.
Evaluation. We use sacreBLEU (accuracy-
related metric)12(Post, 2018), COMET (semantics-
related metric) with the wmt22-comet-da model13
(Rei et al., 2020), and BlonDe (discourse-related
metric) (Jiang et al., 2022) as evaluation metrics.14
3.2 Experimental Results
The main experimental results are presented in Ta-
bles 1 and 2. Additionally, a comparison of the
number of trainable parameters is presented in
Table 3 across different tuning methods. When
examining llama-2-7b and focusing on context-
agnostic models, we find that the Transformer
models (Trans.) generally outperform LLMs with
LoRA tuning (MT-LoRA) in most translation di-
11We provide more discussion in Appendix E about the
prompt length.
12Signature: nrefs:1|case:mixed|eff:no|tok:13a|
smooth:exp|version:2.3.1
13https://github.com/Unbabel/COMET
14We provide more discourse-related evaluation in Ap-
pendix F.MT-LoRA MT-PT/CMT-PT MPT/DeMPT
Trainable Para. 0.12% 13.87% 3.11%
Table 3: Proportion of trainable parameters against total
parameters for different tuning methods.
rections based on BLEU score. However, the MT-
LoRA models surpass Trans. in COMET, indicat-
ing that translations from LLMs may better align
with human preferences. Additionally, the MT-PT
models exhibit superior performance compared to
the MT-LoRA models across BLEU, COMET, and
BlonDe metrics. This improvement could be at-
tributed to the more trainable parameters in the
MT-PT models (13.87% vs. 0.12%).
Importantly, by comparing MT-PT and CMT-
PT, we observe that CMT-PT which indiscrimi-
nately leverages the inter- and intra-sentence con-
text with the concatenation way, even hurts per-
formance for certain translation tasks. For ex-
ample, the CMT-PT models, despite excelling
in discourse-related BlonDe scores (averaging
57.65 vs. 57.17), underperforms in BLEU and
COMET compared to the MT-PT models. In
contrast, our context-aware MPT and DeMPT
models outperform all LLM baselines across all
translation tasks in three metrics. For exam-
ple, our MPT models achieve an average gain of
0.98/0.0103/1.27 in BLEU/COMET/BlonDe com-
pared to the CMT-PT models. Our decoding-
enhance strategy further enhances the capacity of
LLMs, with DeMPT outperforming MPT with an
average gain of 0.65/0.0042/0.88. Compared to
G-Trans. (+mBART) or MR-Trans (+mBART),
DeMPT also demonstrates either superior or com-
parable performance across all language pairs.
Finally, we observe a similar performance trend
among MT models built upon bloomz-7b1-mt . It
also indicates that models built upon llama-2-7b
outperform those utilizing bloomz-7b1-mt , sug-
gesting that llama-2-7b serves as a more robust
foundation model for translation tasks.
4 Discussion
In this section, we use bloomz-7b1-mt as the foun-
dation model to discuss our approach.15See Ap-
pendix D ∼M for further discussions.
15Considering page limitation and the consumption of
GPUs resources and training time, we use the ZH →EN task
as a representative to report the BLEU and BlonDe scores.sent. 256 512 7681024
Context Lengths202530BLEU30.9932.4632.88 33.01 33.05
30.8231.59 31.76 31.53
DeMPT
CMT-PT
sent. 256 512 7681024
Context Lengths404550BlonDe49.4850.6251.11 51.34 51.33
49.6149.99 50.01 50.11
DeMPT
CMT-PTFigure 4: Performance of CMT-PT and our DeMPT on
ZH→EN test set when using different inter-sentence
context lengths.
4.1 Effect of Length of Inter-sentence Context
For efficient training, we define the inter-sentence
context in Section ??as previous sentences with
a total tokens not exceeding 256. We are curious
about the potential impact of inter-sentence length
on the performance of our approach. Consequently,
we extend the inter-sentence context length from
256 to 1024 and assess the performance of our
approach in the ZH →EN task. Figure 4 shows
the performance trend of the CMT-PT model and
our DeMPT model. As the length of the inter-
sentence context increases, both models exhibit
a slight enhancement in both BLEU and BlonDe
scores. Interestingly, our model with a 256-token
inter-sentence context outperforms the CMT-PT
model with a 1024-token inter-sentence context
in both BLEU and BlonDe scores. This further
suggests the effectiveness of our approach in har-
nessing the capabilities of LLMs for context-aware
NMT compared to the concatenation strategy.
4.2 Effect of Multi-phase Strategy
Our multi-phase strategy divides the whole transla-
tion into three phases: phase 1 for encoding inter-
sentence, phase 2 for encoding intra-sentence, and
phase 3 for decoding current sentence. To assess
the effect of multi-phase strategy, we compare its
performance with two contrasting strategies: merg-
ing the first two phases into one (i.e., Merging
1&2) and and merging all phases into a single one
(i.e., Merging 1&2&3 ).16Note that in both the
two contrasting strategies, we replace the enhanced
next word prediction pe(yt|·)(Eq. 17) in decoding
phase with p(yt|·). Table 4 presents the perfor-
mance of different phrasing strategies. Comparing
Merging 1&2 andMerging 1&2&3 , it shows that
separating the decoding phrase from the encod-
16When merging them all into one, it equals CMT-PT, i.e.,
the concatenate strategy.Model BLEU COMET BlonDe
MPT 31.81 0.8601 50.22
DeMPT 32.46 0.8649 50.62
Merging 1&2&3 30.82 0.8504 49.61
Merging 1&2 31.01 0.8503 49.91
Table 4: Comparison of performances when using dif-
ferent phrasing strategies on ZH →EN test set.
Model BLEU COMET BlonDe
Merging 1&2&3 30.82 0.8504 49.61
w/rnd. CTX 28.63 0.8402 48.01
DeMPT 32.46 0.8649 50.62
w/rnd. CTX 31.56 0.8581 49.71
Table 5: Comparison of performance when using gold
or random inter-sentence context on ZH →EN test set.
ing marginally improves the performance in both
BLEU and BlonDe. Importantly, the comparison
of MPT and Merging 1&2 tells that separating the
encoding of inter- and intra sentence achieves more
gains across all metrics.
Meanwhile, we conjecture that another benefit
of multi-phasing strategy lies in the robustness to
the noise contained in document-level context. To
test the conjecture, we replace the original inter-
sentence context with a random inter-sentence con-
text, meaning we randomly select some sentences
from other documents to serve as the inter-sentence
context. As shown in Table 5, the performance of
both the Merging 1&2&3 and DeMPT models con-
sistently deteriorates when exposed to random con-
text ( w/rnd. CTX). However, the decline is more
pronounced for Merging 1&2&3 than for DeMPT
(-2.19/0.0102/1.60 vs -0.90/0.0068/0.91). This sug-
gests that DeMPT, owing to its multi-phase strategy,
exhibits more robustness in utilizing inter-sentence
context in contrast to Merging 1&2&3 .
4.3 Human Evaluation
We use the Direct Assessment (DA) method (Gra-
ham et al., 2017) to manually assess the quality
of translations generated by DeMPT and CMT-PT.
In this assessment, human evaluators compare the
meaning of the MT output with a human-produced
reference translation, working within the same lan-
guage. Specifically, we randomly select 5 docu-
ments with a total of 200 groups of sentences from
the ZH →EN test set. To avoid potential bias inModel Score_1 Score_2 Average
CMT-PT 79.00 80.17 79.59
DeMPT 86.17 (+7.17) 87.30 (+7.13) 86.73 (+7.14)
Table 6: Human DA scores for CMT-PT and DeMPT
on ZH→EN translation task.
evaluation, we recruit 6 professional translators and
ensure each translation from DeMPT or CMT-PT is
scored twice by two translators. Table 6 shows the
DA scores for CMT-PT and DeMPT. Our DeMPT
outperforms CMT-PT by 7.14 DA score, provid-
ing strong evidence for the effectiveness of our
approach. Further details and results regarding the
DA can be found in Appendix H.
5 Related Work
Due to limited space, we omit the discussion on
conventional context-aware MT, focusing instead
on LLM-based context-aware MT and prompt tun-
ing for LLMs. Besides, considering our DeMPT’s
inspiration from MSP (Tan et al., 2022), we offer
further discussion on their differences.
LLM-based Context-aware Machine Transla-
tion. While traditional context-aware neural ma-
chine translation (NMT) has seen considerable
progress in recent years (Jean et al., 2017; Wang
et al., 2017; V oita et al., 2018; Maruf et al., 2019;
Kang et al., 2020; Bao et al., 2021; Sun et al., 2022;
Bao et al., 2023), the effective integration of large
language models (LLMs) to model inter-sentence
context and enhance context-aware translation re-
mains an area of limited exploration. Existing stud-
ies mainly focus on the assessment of LLMs’ abil-
ity in discourse modeling. For example, Wang
et al. (2023) approach context-aware NMT as a
task involving long sequence generation, employ-
ing a concatenation strategy, and conduct compre-
hensive evaluations of LLMs such as ChatGPT
and GPT-4. Their focus includes the impact of
context-aware prompts, comparisons with transla-
tion models, and an in-depth analysis of discourse
modeling ability. Similarly, Karpinska and Iyyer
(2023) engage professional translators to evaluate
LLMs’ capacity in context-aware NMT. In con-
trast, Wu et al. (2024) compare the effectiveness of
various parameter-efficient fine-tuning methods on
moderately-sized LLMs for context-aware NMT.
Besides, Wu and Hu (2023) explore the prompt en-
gineering with GPT language models specificallyfor document-level (context-aware) MT while Li
et al. (2024) experiment with combining sentence-
level and document-level translation instructions of
varying lengths to fine-tune LLMs. Differently, ?
propose a post-editing approach to enhance LLMs’
capacity in utilization of inter-sentence context in
document-level MT.
Prompt Tuning for Large Language Model.
Liu et al. (2021) and Li and Liang (2021) propose to
make LLMs adapt to various tasks by adding train-
able prompts (also called continuous prompts) to
the original input sequences. In this paradigm, only
the continuous prompts are updated during training.
Liu et al. (2022) further introduces deep prompt
tuning, extending the idea by inserting trainable
prompts into all layers of LLMs, rather than just
the embedding layer. While these approaches pro-
vide a general framework, we focus on enhancing
LLM performance specifically for inter-sentence
context modeling in context-aware NMT.
Discussion with MSP. Tan et al. (2022) propose
a multi-phase tuning approach (MSP) to enhance
the sentence-level translation performance of a mul-
tilingual GPT. Our DeMPT mainly differs from
MSP in the following aspects: 1) DeMPT adopts
a phase-aware prompt to enable distinctive mod-
eling for different inputs, namely inter-sentence
contexts, intra-sentence contexts, and the target
sentence, a feature not present in MSP; 2) DeMPT
incorporates a decoding-enhanced strategy to fur-
ther improve the effectiveness of utilizing different
context information, a capability not available in
MSP; 3) DeMPT is designed to alleviate discourse
problems in context-aware LLM-based machine
translation tasks, rather than addressing sentence-
level machine translation tasks as in the case of
MSP; 4) DeMPT is designed to adapt LLMs rather
than smaller pre-trained model used in MSP.
6 Conclusion
In this paper, we have examined the hypothesis that
it is crucial to differentially model and leverage
inter-sentence context and intra-sentence context
when adapting LLMs to context-aware NMT. This
stems from our observation that intra-sentence con-
text exhibits a stronger correlation with the target
sentence compared to inter-sentence context, owing
to its richer parallel semantic information. To this
end, we have proposed a novel decoding-enhanced
multi-phase prompt tuning (DeMPT) approach tomake LLMs aware of the differences between inter-
and intra-sentence contexts, and further improve
LLMs’ capacity in discourse modeling. We have
evaluated our approach using two foundation mod-
els and present experimental results across five
translation directions. Experimental results and dis-
cussions have demonstrated a significant enhance-
ment in the performance of LLMs in context-aware
NMT, manifesting as improved translation accu-
racy and a reduction in discourse-related issues.
Limitations
Owing to resource limitations, our work is re-
stricted to moderate-scale LLMs, specifically those
with 7 billion parameters, and a confined window
size of inter-sentence context. It is imperative to
acknowledge that the results of our research may
differ when employing larger models and extended
window sizes for inter-sentence contexts. Consid-
ering that English text forms the main body of the
training data for LLMs, this paper only focuses on
the English-centric translation tasks. The results of
non-English-centric translation tasks may vary. We
acknowledge these limitations and consider them
as avenues for future exploration. Besides, follow-
ing the finding of Bao et al. (2021), we focus solely
on the source-side inter-sentence context in this
work. We will explore more about the integration
of target-side inter-sentence context in the future.
References
Guangsheng Bao, Zhiyang Teng, and Yue Zhang. 2023.
Target-side augmentation for document-level ma-
chine translation. In Proceedings of ACL , pages
10725–10742.
Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing
Chen, and Weihua Luo. 2021. G-transformer for
document-level machine translation. In Proceedings
of ACL , pages 3442–3455.
BigScience. 2022. Bloom: A 176b-parameter open-
access multilingual language model. Computing Re-
search Repository , arXiv:2211.05100.
Google. 2022. Palm: Scaling language modeling with
pathways. J. Mach. Learn. Res. , 24:240:1–240:113.
Yvette Graham, Qingsong Ma, Timothy Baldwin, Qun
Liu, Carla Parra, and Carolina Scarton. 2017. Im-
proving evaluation of document-level machine trans-
lation quality estimation. In Proceedings of EACL ,
pages 356–361.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. 2022. LoRA: Low-rank adaptation of large
language models. In Proceedings of ICLR .
Xinyu Hu and Xiaojun Wan. 2023. Exploring discourse
structure in document-level machine translation. In
Proceedings of EMNLP , pages 13889–13902.
Sebastien Jean, Stanislas Lauly, Orhan Firat, and
Kyunghyun Cho. 2017. Does neural machine trans-
lation benefit from larger context? Computing Re-
search Repository , arXiv:1704.05135.
Yuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dong-
dong Zhang, Jian Yang, Haoyang Huang, Rico Sen-
nrich, Ryan Cotterell, Mrinmaya Sachan, and Ming
Zhou. 2022. BlonDe: An automatic evaluation
metric for document-level machine translation. In
Proceedings of NAACL , pages 1550–1565, Seattle,
United States.
Xiaomian Kang, Yang Zhao, Jiajun Zhang, and
Chengqing Zong. 2020. Dynamic context selection
for document-level neural machine translation via
reinforcement learning. In Proceedings of EMNLP ,
pages 2242–2254.
Marzena Karpinska and Mohit Iyyer. 2023. Large lan-
guage models effectively leverage document-level
context for literary translation, but critical errors per-
sist. In Proceedings of WMT , pages 419–451.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP , pages 388–395.
Shaohui Kuang, Junhui Li, António Branco, Weihua
Luo, and Deyi Xiong. 2018. Attention focusing for
neural machine translation by bridging source and
target embeddings. In Proceedings of ACL , pages
1767–1776.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of EMNLP , pages 3045–
3059.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of ACL-IJCNLP , pages 4582–4597, On-
line.
Yachao Li, Junhui Li, Jing Jiang, Shimin Tao, Hao Yang,
and Min Zhang. 2023. P-Transformer: Towards Bet-
ter Document-to-Document Neural Machine Transla-
tion. IEEE/ACM Transactions on Audio, Speech, and
Language Processing , 31:3859–3870.
Yachao Li, Junhui Li, Jing Jiang, and Min Zhang.
2024. Enhancing document-level translation of large
language model via translation mixed-instructions.
Computing Research Repository , arXiv:2401.08088.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-
iao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:
Prompt tuning can be comparable to fine-tuning
across scales and tasks. In Proceedings of ACL , pages
61–68.Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt
understands, too. Computing Research Repository ,
arXiv:2103.10385.
Xinglin Lyu, Junhui Li, Zhengxian Gong, and Min
Zhang. 2021. Encouraging lexical translation consis-
tency for document-level neural machine translation.
InProceedings of EMNLP , pages 3265–3277.
Xinglin Lyu, Junhui Li, Shimin Tao, Hao Yang, and
Min Zhang. 2022. Modeling consistency preference
via lexical chains for document-level neural machine
translation. In Proceedings of EMNLP , pages 6312–
6326.
Sameen Maruf, André F. T. Martins, and Gholam-
reza Haffari. 2019. Selective attention for context-
aware neural machine translation. In Proceedings of
NAACL , pages 3092–3102.
MetaAI. 2023a. Llama 2: Open foundation and fine-
tuned chat models. Computing Research Repository ,
arXiv:2307.09288.
MetaAI. 2023b. Llama: Open and efficient foundation
language models. ArXiv , abs/2302.13971.
Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas,
and James Henderson. 2018. Document-level neural
machine translation with hierarchical attention net-
works. In Proceedings of EMNLP , pages 2947–2954.
OpenAI. 2023. Gpt-4 technical report. Computing
Research Repository , arXiv:2303.08774.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for
sequence modeling. In Proceedings of NAACL-HLT:
Demonstrations .
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of WMT , pages 186–191.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
and Yuxiong He. 2020. Zero: Memory optimiza-
tions toward training trillion parameter models. In
SC20: Proceedings of High Performance Computing,
Networking, Storage and Analysis , pages 1–16.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Proceedings of EMNLP , pages 2685–
2702.
Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao,
Shujian Huang, Jiajun Chen, and Lei Li. 2022. Re-
thinking document-level neural machine translation.
InFindings of ACL , pages 3537–3548.
Zhixing Tan, Xiangwen Zhang, Shuo Wang, and Yang
Liu. 2022. MSP: Multi-stage prompting for making
pre-trained language models better translators. In
Proceedings of ACL , pages 6131–6142.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of NIPS , pages 5998–6008.
Elena V oita, Rico Sennrich, and Ivan Titov. 2019a.
Context-aware monolingual repair for neural ma-
chine translation. In Proceedings of EMNLP-
IJCNLP , pages 877–886.
Elena V oita, Rico Sennrich, and Ivan Titov. 2019b.
When a good translation is wrong in context: Context-
aware machine translation improves on deixis, ellip-
sis, and lexical cohesion. In Proceedings of ACL ,
pages 1198–1212.
Elena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan
Titov. 2018. Context-aware neural machine transla-
tion learns anaphora resolution. In Proceedings of
ACL, pages 1264–1274.
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang,
Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023.
Document-level machine translation with large lan-
guage models. In Proceedings of EMNLP , pages
16646–16661.
Longyue Wang, Zhaopeng Tu, Xing Wang, and Shum-
ing Shi. 2019. One model to learn both: Zero pro-
noun prediction and translation. In Proceedings of
EMNLP-IJCNLP , pages 921–930.
Longyue Wang, Zhaopeng Tu, Andy Way, and Qun
Liu. 2017. Exploiting cross-sentence context for neu-
ral machine translation. In Proceedings of EMNLP ,
pages 2826–2831.
Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George
Foster, and Gholamreza Haffari. 2024. Adapt-
ing large language models for document-level ma-
chine translation. Computing Research Repository ,
arXiv:2401.06468.
Yangjian Wu and Gang Hu. 2023. Exploring prompt en-
gineering with GPT language models for document-
level machine translation: Insights and findings. In
Proceedings of the Eighth Conference on Machine
Translation , pages 166–169.
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei
Zhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.
Improving the transformer translation model with
document-level context. In Proceedings of EMNLP ,
pages 533–542.
A Datasets
Splitting, Preprocessing and Statistics of
Datasets. For all translation tasks, we randomly
select 80% document pairs from the corpus as
the training set. Both the test set and validation
set include 150 document pairs each, randomly
sampled from the remaining 20% of documentpairs in the corpus. Regarding sentence prepro-
cessing across all datasets for LLM-based mod-
els, we segment the sentences with the tokenizer
from the respective foundation model. No ad-
ditional preprocessing steps are performed. For
encoder-decoder Transformer models, we segment
the source and target sentences into sub-words
by a BPE model with 30K merged operations
(?). We provide the detailed statistic in Table
7. Datasets are downloaded from https://data.
statmt.org/news-commentary/v18 .
B Training Details
For all encoder-decoder Transformer NMT models,
we use the transformer-base setting as in Vaswani
et al. (2017), where the learning rate is set to 1e−4
with an inverse-square schedule and warmup steps
of4000 , and use Adam optimizer with β1= 0.9
andβ2= 0.98. For the other special training set-
tings in G-Trans and MR-Trans, we keep consistent
with that provided in their paper. All Transformer
NMT models are trained on 4 ×NVIDIA V100
32GB GPUs with a batch size of 4096. For the
models with prompt tuning in Section ??, includ-
ing MT-PT, CMT-PT, MPT and DeMPT models,
the length of the trainable prompt is set as 64. Dur-
ing both training and inference, the model gener-
ates only the current target sentence, operating in a
many-to-one translation mode. For all fine-tuning
models in this paper, we set the training epoch to 4,
and the warm-up rate to 0.1. We use the log learn-
ing rate decay strategy with a maximum learning
rate of 5e-5. We collate a mini-batch by counting
the total tokens inside the batch and set the batch
size as 4096. All fine-tuning models are trained on
4×NVIDIA A800 GPUs with Deespeed Zero 2
offload setting (Rajbhandari et al., 2020).17
C Traditional Context-aware Models
In this paper, we implement G-Transformer (G-
Trans) (Bao et al., 2021) and Multi-Resolution
Transformer (MR-Trans) (Sun et al., 2022) as rep-
resentatives of traditional context-aware models for
comparison. For ease of understanding, we pro-
vide a brief introduction to these two models in this
section.
G-Transformer. The transformer model equips
a group attention on the lower encoder/decoder
17https://github.com/microsoft/DeepSpeedlayer and a combined attention on the top en-
coder/decoder layer. For a sentence being trans-
lated with its inter-sentence context, the group at-
tention helps maintain locality bias by focusing
on intra-sentence context. Meanwhile, the com-
bined attention effectively integrates boundary in-
formation, enhancing the translation process with
inter-sentence context.
Multi-Resolution Transformer. The Trans-
former model does not include any additional
modules specifically for modeling inter-sentence
context. Instead, it only uses a mixed training
set that comprises both sentence-level and
document-level instances with varying numbers of
sentences. Training on this mixed set allows the
Transformer model to handle both sentence-level
and document-level translation tasks. In this paper,
we implement its Document-to-Sentence variant,
which uses all preceding contexts as the source
and the current sentence as the target.
D Comparison of Inference Speed
Table 8 compares the inference speed of different
models on ZH →EN translation task. Our MPT and
DeMPT models, dividing the context-aware NMT
process into three separate phases, demonstrates
comparable inference speed to the single-phase MT-
PT and CMT-PT models, with only a marginal drop
of 0.02 seconds per sentence in decoding. This
illustrates the efficiency of our approach without
introducing significant computational overhead.
E Effect of Prompt Length
As our approach is implemented based on deep
prompt tuning, next we compare the impact of
the trainable prompt length for MT-PT, CMT-PT,
and our DeMPT. Figure 7 shows the performance
curves when increasing the prompt length from 32
to 128. We observe that increased prompt length
tends to enhance performance for both BLEU and
BlonDe, yet the gains exhibit diminishing returns.
This finding is consistent with that in Li and Liang
(2021); Lester et al. (2021); Tan et al. (2022). We
also observe that DeMPT with a prompt length of
64 outperforms both MT-PT and CMT-PT with a
prompt length of 128 on both metrics, suggesting
the superiority of our approach over the concate-
nation strategy in enhancing LLMs’ capacity for
context-aware NMT.DatasetZH→EN FR →EN DE →EN ES →EN RU →EN
#Doc #Sent #Doc #Sent #Doc #Sent #Doc #Sent #Doc #Sent
Training 8,622 342,495 7,915 310,489 8,417 333,201 9,677 378,281 7,255 272,100
Validation 150 6,061 150 5,890 150 5,866 150 5,782 150 5,691
Test 150 5,747 150 5,795 150 5,967 150 5,819 150 5,619
Table 7: Statistics of training, validation, and test sets for five translation tasks. #Doc and #Sent denote the numbers
ofDocument andSentence , respectively.
Scor e
0-20
21-40
41-60
61-80
81-100The translation is completely incorrect and unclear , with only a few word s or phrases
being correct. It is totally unreadable and dif ficult to understand .
The transla tion has very little seman tic similar ity to the source senten ce, with key
information missing or incorrect. It has numerous unnatural and unfluent expressions
and grammatical errors .
The translation can express part of the key semantics  but has many non-key semantic
errors. It lacks fluency and idiomaticity .
The translation can express the key semantics  but has some non-key information errors
and significant grammatical errors. It lacks idiomaticity .
The translation can express the semantics of the source sentence with only a few non-
key information errors and minor grammatical errors. It is fluent and idiomatic.Criterion
Figure 5: Scoring criterion for Direct Assessment. We group the score into five ranges, i.e., 0-20 ,21-40 ,41-60 ,
61-80 ,81-100 .
Model Speed BLEU
MT-PT 0.75 sec/sent. 30.99
CMT-PT 0.77 sec/sent. 30.82
MPT 0.78 sec/sent. 31.81
DeMPT 0.79 sec/sent. 32.46
Table 8: Comparison of inference speed on ZH →EN
translation task. Speed is measured on the test set using
4 GPUs. sec/sent. means seconds spent for decoding
each sentence. Note that the reparameterization is not
needed during inference (Li and Liang, 2021).
F Performance on Contrastive Test Set
We evaluate the models’ ability to resolve discourse
inconsistencies using the contrastive test set pro-
posed by (V oita et al., 2019a), which focuses on
four discourse phenomena such as deixis, lexicon
consistency (lex.c), ellipsis inflection (ell.infl), and
verb phrase ellipsis (ell.VP) in English →Russian
translation. Within the test set, each instance com-
prises a positive translation and several negative
ones that vary by only one specific word. The pur-Model deixis lex.c ell.infl ell.VP Avg.
MT-PT 50.0 45.7 53.0 28.6 44.3
CMT-PT 80.2 46.1 74.3 75.3 68.9
DeMPT 80.1 55.7 75.9 79.3 72.7
Table 9: Accuracy [%] of translation prediction for four
discourse phenomena on the English →Russian con-
trastive test set.
pose of the contrastive test set is to assess whether a
model is more inclined to generate a correct transla-
tion as opposed to incorrect variations. Table 9 lists
the accuracy of translation prediction on the con-
trastive test set for MT-PT, CMT-PT and DeMPT.
Compared to the context-agnostic MT-PT model,
both context-aware CMT-PT and DeMPT models
show substantial improvements across the four dis-
course phenomena. Additionally, DeMPT demon-
strates the best performance, surpassing CMT-PT
by an average accuracy margin of 3.8.同时塔利 班已经公开宣称
美国 是它与俄 罗斯共同
的敌人，它将团结一切可
团结的力量将 美国人赶出
祖国。Sour ce
The Taliban, for its part, has
openly declared the US to
be its commons enemy with
Russia, and it will unite
whatever forces it can to
drive the Americans out of
the country .DeMPT
At the same time, the Taliban
has openly declared the US
to be its enemy, along with
Russia, and will unite all
forces that can be united to
drive the Americans out of
the country .DMT -PT
Refer ence
And the Taliban, which has
acknowledged that it shares
Russia's enmity with the US,
will take whatever help it can
get to expel the Americans.Sour ce DeMPT CMT -PT Refer ence
今天，俄罗斯利用同样的
逻辑来为与阿富汗塔利班
的合作寻找理由，它希望
塔利班势力继续打击由美
国支持的动荡的喀布尔政
府。Today , Ruassia is using the
same logic  to justify
cooperation with the Afghan
Taliban, which it hopes will
to attack the US-backed
government in Kabul.Today , Ruassia is using the
same logic to justify its
cooperation with the
Taliban, which it hopes will
go on beat the-unstable
Kabul government, which
the America  supports.Today , Ruassia is using the
same logic to justify its
cooperation with the Afghan
Taliban, which it want to
keep fighting the unstable
US-backed government in
Kabul.
CMT -PTFirst Sentence 
Second  Sentence Figure 6: A case study for the CMT-PT model and our DeMPT model on ZH →EN translation task.
32 64 96 128
Prompt Length293031323334BLEU
31.2332.4632.99 33.01
30.1130.9931.36 31.41
30.0130.8231.4231.54
DeMPT
MT-PT
CMT-PT
32 64 96 128
Prompt Length48495051BlonDe
49.7750.6250.99 51.01
48.4649.4849.66 49.65
48.7949.6149.76 49.81
DeMPT
MT-PT
CMT-PT
Figure 7: Performance of MT-PT, CMT-PT, and our
DeMPT on ZH →EN test set when using different
lengths of the trainable prompts.
G Effect of Various Contexts for
Decoding-enhanced Strategy
We conduct an ablation study on the ZH-EN trans-
lation direction using the bloomz-7b-mt model as
the foundation model to clarify the effect of the
three probabilities pin Equation 17, i.e., the ef-
fect of various contexts for the heuristic decoding-
enhanced strategy. From the Table 10, we observe
that removing ˆp, i.e., w/oˆp, leads to a significant
degradation in the discourse-related metric, namely
the BlonDe. This is because the integration en-
hances the utilization of the inter-sentence context
during the decoding phase. We are additionally, re-
moving results in the most substantial degeneration
in BLEU metric. This observation demonstratesModel BLEU COMET BlonDe
MT-PT 30.99 0.8520 49.48
CMT-PT 30.82 0.8504 49.61
DeMPT 32.46 0.8649 50.62
w/oˆp 32.33 0.8629 50.29
w/o¯p 32.11 0.8641 50.51
Table 10: Comparison of performances of the DeMPT
when removing different probabilities pin decoding-
enhanced strategy.
that our heuristic decoding-enhanced strategy can
distinctively improve the utilization of various con-
texts during the decoding phase.
H Details of Human Evaluation
Criterion and Recruitment. Given a source sen-
tence, its translation from MT (i.e., CMT-PT and
our DeMPT), and its human-produced reference
translation, the evaluators are asked to give a score
ranging from 0 to 100. Figure 5 presents the de-
tailed criterion of scoring. We recruit evaluators
from professional translators with at least five years
of experience in translation.
Statistics of Translation Errors. We manually
count the number of bad cases from our DeMPTGroupType of Bad Case
Mis. UO IE GE Total (Perc.)
1 6 3 1 2 12 ( 6.0% )
2 9 7 6 5 27 ( 13.5% )
Table 11: Statistics of bad cases from our DeMPT model
on ZH→EN translation task. Perc. denotes the percent-
age of bad cases against the total of DA cases.
model. The bad cases fall into two categories: (1)
the DA score is 60 or lower; (2) the DA score is
lower than that of the translation from CMT-PT.
The main types of the bad cases are Mistransla-
tion (Mis. ),Unnoticed Omission (UO),Inappro-
priate Expression (IE), and Grammatical Error
(GE). We present detailed statistics in Table 11. The
statistics indicate the bad cases mainly come from
Mistranslation and Unnoticed Omission. Mean-
while, our DeMPT model outperforms the CMT-PT
model in 86.5% DA cases.
Case Study. We present a case in Figure 6 to illus-
trate how our DeMPT model outperforms the CMT-
PT model. In this case, we compare the translations
of two consecutive sentences from our model and
the CMT-PT model. First, we notice that the CMT-
PT model translates the source word 美国in the
two sentences into USandAmerica , respectively.
However, our model consistently translates them
into US. Second, our model uses for its part , a
phase with more coherent preference , as the trans-
lation of同时, instead of At the same time adopted
in the translation from the CMT-PT model. Both of
them demonstrate the superiority of our proposed
approach in discourse modeling.
I Effect of Dataset Scales
We conduct an experiment to analyze the impact of
training dataset scales on the concatenating strat-
egy (CMT-PT) and the multi-phased, decoding-
enhanced strategy (DeMPT). To do this, we expand
the ZH →EN training set with additional document-
level data from the LDC.18Specifically, we se-
lected 200K ，400K and 700K sentence pairs with
their inter-sentence context from the LDC and com-
bined them with the existing ZH →EN training set
to train the CMT-PT and DeMPT models.
18The training data set consists of LDC2002T01,
LDC2004T07, LDC2005T06, LDC2005T10, LDC2009T02,
LDC2009T15, and LDC2010T03.Model BLEU COMET BlonDe
CMT-PT 30.82 0.8504 49.61
+ 200K 31.21 0.8521 49.88
+ 400K 31.73 0.8555 50.11
+ 700K 31.89 0.8559 50.23
DeMPT 32.46 0.8649 50.62
+ 200K 32.77 0.8663 50.99
+ 400K 33.56 0.8701 51.47
+ 700K 33.91 0.8721 51.97
Table 12: Comparison of performances of CMT-PT and
DeMPT trained on the different scales of corpus for the
ZH→EN translation task.
Table 12 lists the performances of CMT-PT and
DeMPT when extending scales of the training set
into 500K (300K +200K), 700K (300k + 400K)
and 1M (300K + 700K). We observe increasing the
scale of the training set consistently boosts the per-
formance of DeMPT and CMT-PT. However, our
DeMPT significantly outperforms CMT-PT across
all three metrics.
J Effect of Transfer Layer and Type
Embedding
As in Eq. 22 within Section 2.3, we introduce two
sublayers: a non-linear transfer sublayer and a type
embedding sublayer for the trainable prompt in
each phase. This design enhances the awareness of
LLMs regarding the distinctions in inputs across
the three tuning phases, allowing them to adapt to
specific roles at each phase. We investigate the
effect of these two sublayers.
As shown in Table 15, our observations reveal
that the transfer sublayer holds greater importance
than the type embedding sublayer. Removing ei-
ther the non-linear transfer sublayer ( w/oTransfer.)
or the type embedding sublayer ( w/oEmbed.) re-
sults in a performance drop of 0.84/0.0048/0.39
or 0.45/0.0036/0.007 in BLEU/COMET/BlonDe
metrics.
K Effect of Hyperparameter λ
Due to the limited computational resources, we
do not perform extensive experiments to find the
optimal combination of λ1andλ2for different
translation tasks, simply setting them to be equal.
For example, verifying each combination of λ1andModel BLEU COMET BlonDe
MT-PT 30.99 0.8520 49.48
CMT-PT 30.82 0.8504 49.61
DeMPT 32.46 0.8649 50.62
w/oTransfer. 31.62 0.8601 50.23
w/oEmbed. 32.01 0.8613 50.55
w/oCTX. 31.98 0.8593 49.89
Table 13: Comparison of performances of the DeMPT
variants on ZH →EN test set. w/oTrans. or w/oEmbed.
denotes the variant without the non-linear transfer sub-
layer or type embedding sublayer in Eq. 22. w/oCTX.
means the inter-sentence context is not available, i.e.,
context-agnostic DeMPT system.
Model (DeMPT) BLEU COMET BlonDe
λ1=1/3, λ2=1/3, ZH →EN 32.46 0.8649 50.62
λ1=1/4, λ2=1/3, ZH →EN 32.51 0.8653 50.31
λ1=1/3, λ2=1/3, FR →EN 41.92 0.8790 58.30
λ1=1/4, λ2=1/3, FR →EN 41.82 0.8785 57.92
Table 14: Comparison of performances of the DeMPT
with different combinations of λ1andλ2on ZH→EN
and FR →EN test sets.
λ2requires 10 experiments (5 ×2 for the number
of translation directions and foundation models).
Therefore, we carry out targeted experiments us-
ing a combination of λ1andλ2on ZH→EN and
FR→EN only here.
The results are reported in Table 14. We use
a smaller value for λ1here and observe that the
BlonDe scores are more sensitive to changes λ1
compared to BLEU and COMET. For example, a
smaller λ1results in -0.31 and -0.38 for ZH →EN
and FR →EN, respectively. This sensitivity may
be reasonable because λ1is used for adjusting the
utilization of inter-sentence context.
L Effect of Inter-sentence Context
We implement the context-agnostic (sentence-level)
DeMPT system to analyze the effect of the inter-
sentence context and differences with MSP. More
specifically, we replace the input of LLMs in the
inter-sentence context encoding phase with the
intra-sentence context. In other words, we encode
the intra-sentence context twice to keep the multi-
phase tuning strategy in DeMPT while making the
inter-sentence context unavailable.
As shown in the last row of Table 15 (i.e., w/oModel d-BLEU d-COMET d-BlonDe
MT-PT ( m2o) 34.19 0.8216 49.48
CMT-PT ( m2o) 34.06 0.8211 54.68
DeMPT ( m2o) 35.76 0.8316 55.97
CMT-PT ( m2m ) 34.13 0.8256 55.34
Table 15: Comparison of performances of the mod-
els with different translation modes, i.e., with/without
target-side inter-sentence context, on ZH →EN test set.
CTX), we find that the inter-sentence context is
crucial for the alleviation of discourse-related is-
sues. The BlonDe score drops by 0.73 when the
inter-sentence context is unavailable. Meanwhile,
our DeMPT also significantly improves the per-
formance of LLMs in context-agnostic MT, e.g.,
+ 0.99 BLEU score and + 0.0073 COMET score
compared to the MT-PT model.
M Effect of Target-side Inter-sentence
Context
To enable a fair comparison, we incorporate only
the source-side inter-sentence context for the model
with the concatenating strategy, i.e., the CMT-PT
model in the many-to-one ( m2o) translation mode,
as shown in Tables 1 and 2. To further investigate
the effect of target-side inter-sentence context for
the concatenating strategy, we compare the CMT-
PT model in the many-to-many ( m2m ) translation
mode to the models in the many-to-one translation
mode, for the ZH →EN translation task when using
thebloomz-7b1-mt as the foundation model.
Different from the results in Tables 1 and 2,
we report the document-level BLEU, BlonDe, and
COMET scores for all models here due to the un-
availability of sentence-level alignment for many-
to-many model. From the experimental results,
we observe that the CMP-PT ( m2m ) model out-
performs the CMP-PT ( m2o) model (mostly sig-
nificant in terms of the d-BlonDe metric), which
demonstrates the effectiveness of the target context
in addressing discourse issues. However, the CMP-
PT (m2m ) model still underperforms the DeMPT
model across three metrics.