Fisher Information-based Efficient Curriculum Federated Learning with
Large Language Models
Ji Liu1†∗, Jiaxiang Ren2†, Ruoming Jin3, Zijie Zhang4,
Yang Zhou2*, Patrick Valduriez5, Dejing Dou6
1HiThink Research, Hangzhou, Zhejiang, China,
2Auburn University, Auburn, United States,3Kent State University, Kent, United States,
4University of Texas at San Antonio, San Antonio, United States,
5Inria, University of Montpellier, CNRS, LIRMM, France, and LNCC, Petropolis, Brazil,
6Fudan University, Shanghai, China, and BEDI Cloud, Beijing, China
Abstract
As a promising paradigm to collaboratively
train models with decentralized data, Federated
Learning (FL) can be exploited to fine-tune
Large Language Models (LLMs). While LLMs
correspond to huge size, the scale of the train-
ing data significantly increases, which leads to
tremendous amounts of computation and com-
munication costs. The training data is generally
non-Independent and Identically Distributed
(non-IID), which requires adaptive data pro-
cessing within each device. Although Low-
Rank Adaptation (LoRA) can significantly re-
duce the scale of parameters to update in the
fine-tuning process, it still takes unaffordable
time to transfer the low-rank parameters of all
the layers in LLMs. In this paper, we propose a
Fisher Information-based Efficient Curriculum
Federated Learning framework (FibecFed) with
two novel methods, i.e., adaptive federated cur-
riculum learning and efficient sparse parameter
update. First, we propose a fisher information-
based method to adaptively sample data within
each device to improve the effectiveness of the
FL fine-tuning process. Second, we dynami-
cally select the proper layers for global aggrega-
tion and sparse parameters for local update with
LoRA so as to improve the efficiency of the FL
fine-tuning process. Extensive experimental
results based on 10 datasets demonstrate that
FibecFed yields excellent performance (up to
45.35% in terms of accuracy) and superb fine-
tuning speed (up to 98.61% faster) compared
with 17 baseline approaches).
1 Introduction
As a promising paradigm to collaboratively train
models with decentralized data, Federated Learn-
ing (FL) can be exploited to fine-tune Large Lan-
guage Models (LLMs) without aggregating the raw
data from a large number of devices (Fan et al.,
†Equal contribution.
*Corresponding author: jiliuwork@gmail.com,
yangzhou@auburn.edu2023; Kuang et al., 2023; Che et al., 2023a; Liu
et al., 2024b; Che et al., 2023b; Liu et al., 2022b,a;
Zhou et al., 2022). A number of stringent legal reg-
ulations (Official Journal of the European Union,
2016; Californians for Consumer Privacy, 2020)
have been set up in order to protect the security
and the privacy of personal data, which hinders the
aggregation of the decentralized raw data. FL typi-
cally utilizes a parameter server (Li et al., 2014; Liu
et al., 2024c, 2023a,b) to aggregate the distributed
model updates in devices, which only transfers the
parameters or the gradients of the updated models
in replace of the raw personal data. By leveraging
the distributed raw data of end users, Large Lan-
guage Models (LLMs) can be trained on devices
with excellent performance (Zhao et al., 2024).
To appear in EMNLP 2024While ChatGPT (OpenAI, 2022) has achieved
remarkable progress, LLMs (Touvron et al., 2023;
Jiang et al., 2023; Du et al., 2022; Zeng et al., 2023;
Zhang et al., 2024) have attracted extensive atten-
tion. The size of LLMs ranges from several million
parameters, e.g., RoBERTa LARGE (Liu et al., 2020),
to several hundreds billion parameters (Wang et al.,
2019). While the large scale brings strong capabil-
ity in various Natural Language Processing (NLP)
tasks (Zhao et al., 2023b), the pre-training and the
fine-tuning process of LLMs significant communi-
cation and computation costs (et al., 2023).
Two types of parameter-efficient approaches ex-
ist for reducing the number of parameters within
the fine-tuning process of LLMs. Prompt tuning
(Liu et al., 2021; Lester et al., 2021; Liu et al.,
2022d) can dynamically adjust the prompts to fine-
tune LLMs with only a few trainable parameters,
which may introduce performance degradation. Al-
though Low-Rank Adaptation (LoRA) (Hu et al.,
2021) can significantly reduce the scale of parame-
ters to update in the fine-tuning process of LLMs so
as to enable the training of LLMs on edge devices
(Xu et al., 2024), it still takes unaffordable time to
update the low-rank parameters of all the layers inarXiv:2410.00131v2  [cs.LG]  18 Oct 2024LLMs when dealing with decentralized data.
While being an effective method to improve the
efficiency and effectiveness of training process, cur-
riculum learning (Bengio et al., 2009) is exploited
to train large-scale models (Li et al., 2022a). In-
spired by the learning strategy of starting small (El-
man, 1993), the curriculum training process starts
with easier data and then gradually increase the
difficulty. Instead of randomly sampling the batch
from training dataset, curriculum learning allows
the model to gradually learn from easy samples to
hard samples during the training or the fine-tuning
process. Existing approaches generally measure
the complexity of samples based on heuristic meth-
ods (Li et al., 2024) or a simple mode-based method
(Xu et al., 2022), both of which cannot provide an
accurate estimation of difficulty of data samples
and cannot be directly applied in FL. In the context
of FL, the data is generally non-Independent and
Identically Distributed (non-IID), which requires
an adaptive difficulty evaluation approach for di-
verse devices (Vahidian et al., 2023).
Model compression methods, e.g., pruning (Wu
et al., 2021; Liu et al., 2024d; Zhang et al., 2022) or
sparse training (Bibikar et al., 2022), are exploited
in FL to reduce computation and communication
costs. Sparse training can achieve personalization
so as to further improve the performance of FL
(Liu et al., 2023c; Dai et al., 2022). However, the
pruning or sparse training incurs severe accuracy
degradation due to lossy strategies or simple com-
ponent (neuron) selection mechanisms. In addition,
the model can be split into two parts, i.e., the server
part and the device part, in order to achieve both
the generalization and personalization capability
(Han et al., 2023).
Fisher information can be exploited to acceler-
ate the training process of LLMs (Ollivier, 2015;
Martens and Grosse, 2015; Osawa et al., 2023).
Fisher information is defined as the amount of infor-
mation carried by a random variable corresponding
to some unknown parameters (Duy et al., 2022). As
a measure of the local curvature (Martens, 2020),
Fisher Information Matrix (FIM) defines the Rie-
mannian metric of the parameter space (Karakida
et al., 2019), which can indicate the difficulty of
data samples and the importance of each compo-
nent of the network along with the generalization
performance (Jastrzebski et al., 2021).
In this paper, we propose FibecFed, i.e., a Fisher
Information-based Efficient Curriculum Federated
Learning framework. FibecFed is composed of twonovel methods, i.e., adaptive federated curriculum
learning and efficient sparse parameter update. To
the best of our knowledge, we are among the first
to exploit the fisher information to perform cur-
riculum learning and sparse training at the same
time within FL settings. We summarize out major
contributions as follows:
•We propose an adaptive federated curriculum
learning method to sample easy data samples
first and to gradually improve the difficulty of
samples so as to improve the effectiveness of
the FL fine-tuning process. We exploit a fisher
information-based method to measure the dif-
ficulty of training data within each device.
•We propose an efficient sparse parameter up-
date method to select proper layers for global
aggregation and to adaptively update sparse
parameters to achieve excellent efficiency and
effectiveness. We utilize fisher information
to evaluate the importance of diverse compo-
nents of LLMs and propose a lossless method
for global aggregation and local update.
•We conduct extensive experimentation to val-
idate our approach using 10 datasets. The
experimental results reveal that FibecFed sig-
nificantly outperforms 17 baseline approaches
in terms of accuracy (up to 45.35% higher)
and fine-tuning speed (up to 98.61% faster).
The rest of the paper is organized as follows.
The related work is presented in Section 2. We
formulate the problem to address in Section 3. We
present the architecture of FibecFed and propose
the adaptive federated curriculum learning and the
efficient sparse parameter update method in Sec-
tion 4. We demonstrate the experimental results in
Section 5. Finally, Section 6 concludes.
2 Related Word & Preliminaries
Inspired by the learning strategy of starting small
(Elman, 1993), curriculum learning (Bengio et al.,
2009) is exploited in large-scale model training (Li
et al., 2022a). Existing works measure the com-
plexity of samples based on static characteristics of
data samples, e.g., sequence length (Li et al., 2024;
Platanios et al., 2019). Although a simple global
mode-based method is proposed to predict the per-
formance improvement based on several training
states (Xu et al., 2022), it still cannot provide an ac-
curate estimation of difficulty of data samples dueto non-IID data in FL settings. Direct evaluation
based on the inference loss of models (Vahidian
et al., 2023) cannot well explore the impact on the
generalization of the training process. The atten-
tion scores can analyze the dependency among di-
verse layers, but varies significantly between heads
(Vig and Belinkov, 2019), which cannot be directly
utilized in FL settings. While a sharpness-aware
minimization method (Foret et al., 2021) can help
minimize loss value and loss sharpness to improve
model generalization, it does not consider federated
fine-tuning settings of LLMs.
FIM can be exploited to enable the second-order
optimization so as to improve the training process
(Osawa et al., 2023; Jin et al., 2022) and to compute
a global posterior for federated learning (Jhunjhun-
wala et al., 2024). In addition, continual learning
can be used to improve the performance of trained
models while addressing the forgetting problem
(Wu et al., 2022a). Different from (Osawa et al.,
2023; Wu et al., 2022a; Jhunjhunwala et al., 2024),
we exploit the sum of diagonal of FIM to evalu-
ate the difficulty of samples within the efficient
curriculum learning method and to calculate the
importance score of each layer and neuron within
the LLM.
Model compression methods (Wu et al., 2021;
Bibikar et al., 2022) are exploited in FL to re-
duce both computation and communication costs.
Although pruning methods can reduce the size
of large models (Wang et al., 2020; Ma et al.,
2023; Xia et al., 2023), it is complicated to
choose a proper pruning rate and may incur in-
ferior performance in terms of accuracy (Wu et al.,
2021). Sparse training can achieve personalization
(Bibikar et al., 2022; Liu et al., 2023c; Setayesh
et al., 2022; Dai et al., 2022) while addressing the
client shift problem brought by the non-IID data
(Setayesh et al., 2022; Karimireddy et al., 2020).
However, the existing sparse training methods may
incur severe accuracy degradation with poor gen-
eralization capacity due to simple component se-
lection mechanisms. The model can be split into a
server part and a device part to achieve both gener-
alization and personalization capability (Han et al.,
2023), which still incurs severe computation and
communication costs in the FL settings of LLMs.
Please note our approach is orthogonal with model
compression methods.
For NLP tasks, prompt tuning (Liu et al., 2021;
Lester et al., 2021; Liu et al., 2022d) can fine-tune
LLMs with only a few parameters. With prompttuning, an extra network is exploited to generate
proper prompts or prefix, which is concatenated
with the input to guide LLMs to generate proper
answers. Furthermore, LoRA updates trainable
rank decomposition matrices while freezing the
parameters of the original network, which can sig-
nificantly reduce the scale of parameters to update
(Hu et al., 2021). However, both the prompt tuning
and LoRA still incur significant communication
costs due to the update for all the layers.
3 Problem Formulation
In this paper, we delve into the problem of how to
efficiently fine-tune a large language model within
a FL setting. The FL setting is composed of a
parameter server and Kdevices. We assume that
the data samples are distributed among the devices,
each of which contains a dataset Dk={si, mi}nk
withsi,mi, and nkreferring to a data sample, the
corresponding label, and the cardinality of Dk. We
denote the cardinality of the whole dataset D=
{D1, D2, ..., D k}byN.
We consider a LLM MofLlayers, each layer
contains a full parameter matrix Wl
o. We exploit
the LoRA method to reduce the parameters to up-
date in this paper (Hu et al., 2021), and denote the
LoRA parameters of the the LLM MbyPwithPl
representing the set of LoRA parameters in Layer l
ofM. We denote the updated LoRA parameters on
Device kbyPk. Then, we formulate the problem
to address in this paper as how to efficiently update
Pso as to minimize the global loss:
min
P
F(M,P)≜1
KKX
k=1,Pk∈PnkFk(M,Pk)
,
(1)
where F(M,P)is the global loss,
Fi(M,Pk)≜1
nlP
{si,mi}∈Dkf(M,Pk, si, mi)
represents the local loss function on Device kwith
f(M,Pk, si, mi)calculating the local loss on
Device k.
4 Efficient Curriculum Federated
Learning
In this section, we first explain the system model.
Then, we propose the adaptive federated curricu-
lum learning method. Afterward, we further detail
the efficient sparse tuning method.Global Aggregation LayerLora Layer 1e(Pretty)e(dull)e(!)Prediction (with linear head)⑨Aggregate and update the parameters in GALServer①Calculatescores for both data samples and layers (Initialization phase)
Local Lora LayerFrozen LLM backbone②Upload layer scores to the server ( Initialization phase)
⑥Broadcast the GAL parameters to selected devices (Fine-tuning phase)⑧Upload GAL parameters  to server④Broadcast GAL to all devices (Initialization phase)e([CLS])[CLS]Prettydull⑦⑥②④Ignored LoRALayers on serverLora Layer 2Lora Layer 3!xlPretrainedLLMWeights!"#hl$#
Lora Layer 1e(Pretty)e(visual)e(!)Prediction (with linear head)e([CLS])[CLS]NicevisualLora Layer 2Lora Layer 3!
⑦Update GAL parameters and local update parameters based on the proposed curriculum FL method⑤Compute local update parameters (Initialization phase)
⑦①⑤③Aggregate sores from each device and calculate GALFigure 1: The system model of FibecFed.
4.1 System Model
The system model of FibecFed is shown in Figure
1. We assume that the LLM ( M) is deployed on
each device. The parameters of Mstays frozen
while we update the LoRA parameters (Hu et al.,
2021). As shown on the top left of Figure 1, on
Device k, the parameters ( Wl
o) at each layer ( l) is
decomposed into two matrices (LoRA), i.e., Al
kand
Bl
k, which can be updated during the fine-tuning
process. Then, the hidden values ( h) generated
at Layer lwith the input xis calculated based on
Formula 2.
h=Wl
ox+Bl
kAl
kx. (2)
The fine-tuning process is composed of two
phases, i.e., initialization and tuning. Within the
initialization phase, we evaluate the difficulty score
for each batch of data samples (see Formulas 3-5
in Section 4.2 and Lines 1-4 in Algorithm 1) and
the importance score for each layer (see Formulas
6-11 and details in Section 4.3.1) based on fisher
information on each device (Step 1⃝). Then, the
importance score of each layer is transferred to the
server (Step 2⃝), which aggregates the scores and
selects proper layers as the Global Aggregation
Layers (GAL) (see details in Section 4.3.1, Line 7
in Algorithm 1, Step 3⃝). Afterward, the GAL are
boradcasted to all the devices (Step 4⃝). Finally,
the parameters, which are not in the GAL, are lo-
cally evaluated on each device so as to generate
local update part of parameters and the local staticparameters to be frozen (see details in Section 4.3.2,
Lines 8-10 in Algorithm 1, Step 5⃝). During the
tuning phase, only the parameters in GAL and the
local update part of parameters are updated while
the local static parameters are kept frozen. The
tuning phase consists of multiple rounds, each of
which consists of five steps. First, the server ran-
domly selects Kdevices and broadcasts the global
parameters in GAL on the server to the selected
devices (Step 6⃝, Line 12 in Algorithm 1). Then,
the parameters in GAL and the local update part
of parameters are updated based on our proposed
curriculum FL (see details in Appendix) on each
selected device (Step 7⃝, Lines 13-17 in Algorithm
1). Afterward, the updated parameters in GAL are
uploaded to the server (Step 8⃝). Finally, the server
aggregate and update the global parameters in GAL
(Step 9⃝, Line 19 in Algorithm 1).
4.2 Fisher information-based Curriculum
Federated Learning
Inspired by the starting small strategy (Elman,
1993), we propose a fisher information-based cur-
riculum FL method to enable efficient federated
fine-tuning. As the FIM can help indicate the
amount of information carried by each data sample
to generate the response (Ly et al., 2017), we pro-
pose utilizing the FIM to measure the difficulty of
data samples. The FIM is defined in Formula 3:
Fi≜Esih
(∇logpk(si)) (∇logpk(si))Ti
,(3)whereFirepresents the FIM corresponding to
data sample si,pk(si)represents the probability
density function of the inference with the LLM
M, the LoRA parameters Pk, and data sample si,
∇logpk(si)denotes the first-order derivative of
the LoRA parameters, calculated by the gradient
of the loss respect to Pk,Trefers to the transpose
of a matrix. Practically, the expected FIM can be
approximated by empirical FIM (Kunstner et al.,
2019) as defined in Formula 4:
Fi≈1
NNX
i=1h
(∇logpk(si)) (∇logpk(si))Ti
,
(4)
However, calculating the FIM is computational
expensive as the multiplication of ∇logpk(si)is
both time and memory consumption when the size
of the derivative matrix, i.e., |∇logpk(si)|, is sub-
stantial. Inspired by (Pascanu and Bengio, 2013),
we calculate the diagonal of FIM to approximate
the FIM as shown in Formula 5.
˜Fi=I|∇logpk(si)|⊙Fi (5)
where I|∇logpk(si)|is the identity matrix with the
same size of the derivative matrix. Then, we cal-
culate the sum of the trace of ˜Fi(Jastrzebski et al.,
2021) as the score of the data sample. Finally, we
can calculate the difficulty score of a batch of data
samples (see details in Appendix).
In order to improve the training efficiency, we
propose a curriculum data selection strategy. We
take the simplest Bt
kdata samples for the local
update on Device kin Round t.Bt
kbecomes bigger
along with the epoch number within the training
process (see details in Apendix).
4.3 Efficient Sparse Parameter Update
In this section, we propose an efficient sparse pa-
rameter method composed of a global aggregation
layer selection method and a local update parame-
ter selection method.
4.3.1 Global Aggregation Layer Selection
In order to reduce communication costs, we only
transfer the LoRA parameters in important layers
(GAL) between the server and devices for global
aggregation. In this section, we propose a global
aggregation layer selection method with a novel
layer importance score calculation technique and a
global aggregation layer selection technique based
on the importance score.While important layers generally capture distin-
guishable features of data (Mellor et al., 2021),
we select the layers that are sensitive to the input
data samples. When a layer exhibit less resilience
against the noise on the data, it corresponds to
higher sensitivity, and thus is more important. We
calculate the output difference of a certain layer
with two similar input data samples to indicate its
resilience, which represents the importance score.
In order to get two similar input data samples,
we add noise to an original sample. Within a prede-
fined noise budget, we calculate the noise that max-
imizes the loss, so as to well evaluate the sensitivity
of the layer. Then, the noise ( ϵi) corresponding to
siis calculate based on Formula 6:
ϵi= argmax
||ϵi||p<γf(M,Pk, si+ϵi, mi)| {z }
Lk(si+ϵi)
−f(M,Pk, si, mi)|{z }
Lk(si),(6)
where Lkis the local loss, || · || prepresents the ℓp-
norm of the noise, and γrefers to the noise budget.
We decompose Lk(si+ϵi)−Lk(si)via the first-
order Talyor extension as defined in Formula 7:
Lk(si+ϵi)−Lk(si)
≈Lk(si) +ϵT
i∇PkLk(si)−Lk(si),
=ϵT
i∇PkLk(si).(7)
Then, we can solve the approximation by the solu-
tion to a classic dual problem (Foret et al., 2021)
as defined in Formula 8.
ϵ∗
i=γsign(∇PkLk(Pk))|∇PkLk(Pk)|q−1
(||∇PkLk(Pk)||q
q)1/1−p(8)
where| · |q−1denotes the absolute value and power
in terms of each element, qis a factor that satisfies
1
p+1
q= 1. Afterward, we take ϵ∗
ias the noise ϵi
to calculate the sensitivity.
As Frobenius norm can characterize features in
the latent space (Chen et al., 2021), we exploit a rel-
ative difference of the Frobenius norm to measure
the sensitivity of each layer. The relative difference
can avoid the bias brought by the absolute values.
The relative difference of of Frobenius norm is
defined in Formula 9.
Fl(si) =||hl(si+ϵ∗
i)||F− ||hl(si)||F
||hl(si)||F,(9)
whereFl(si)is the relative difference of the Frobe-
nius norm, hl(si)represents the output embeddingsat Layer lof the LLM Mfor data sample si,||·||F
refers to the Frobenius norm. Then, the importance
score of Layer lon Device kis calculated based on
its local data Dkas defined in Formula 10.
Il
k=1
nkX
si∈DkFl(si), (10)
where Il
krepresents the importance score of Layer
lon Device k. Afterward, the global importance
scoreIlis calculated based on Formula 11.
Il=1
NKX
k=1nkIl
k, (11)
We propose a lossless method to select proper
important layers as GAL. On each device k, the
LoRA parameters are initialized as P0
k. After T
rounds of fine-tuning, the LoRA parameters are
denoted by PT
k. We construct a base function as
∆k=P0
k− PT
k. We calculate the Hessian ma-
trix of the local loss function with its eigenvalues
sorted in ascending order ( {λ1
k, λ2
k, ..., λr
k..., λRk
k}
withrrepresenting the index of an eigenvalue and
Rindicating the rank of the Hessian matrix). We
calculate the Lipschitz constant ( Lk) of a base func-
tionHk(PT
k)∆k−▽L′
k(∆k+PT
k)with▽L′
k(·)
being the gradient of the local loss function and
Hkreferring to the Hessian matrix of the local loss
function. Inspired by (Zhang et al., 2021), we find
the first rkthat satisfies λrk+1−λrk>4Lkto
achieve lossless performance. Then, we calculate
the expected number of layers in GAL on Device
kasN∗
k= (1−rk
Rk)LwithLbeing the number
of layers in M. Then, we calculate the number of
layers in GAL as N∗=µ
NPnkN∗
k, where µis a
hyper-parameter to adjust the ratio between global
and local number. Finally, we select N∗layers
with the highest importance scores.
4.3.2 Local Update Parameter Selection
In order to reduce computation costs, we only up-
date important LoRA parameters within the lo-
cal update while freeze the remaining parameters.
Apart from the parameters in GAL, we dynamically
select an important part of parameters in other lay-
ers to update. In this section, we propose a novel
fisher information-based local update parameter
selection method with momentum.
While the LoRA parameters may significantly
vary during the fine-tuning process, we calculate
the FIM with momentum within first T′epochs by
Ft
k=γ∗Ft−1
k+(1−γ)˜Fk, where γrepresents thecoefficient that controls the step size of the moving
average, Ft
krefers to the FIM on Device kat Round
t,˜Fkis the empirical average diagonal approxima-
tion of the FIM, i.e., ˜Fk=1
nkP
si∈Dk˜Fiwith˜Fi
calculated based on Formula 5, while F0
kis directly
calculated without moment. Finally, we get a FIM
FT′
k,lfor each layer loutside of GAL. Inspired by
(Diao et al., 2023), we exploit a neuron-wise aggre-
gation of the FIM to indicate the importance score
of Neuron µin Layer las defined in Formula 12.
∫µ
k,l=|Wµ:|−1X
υ=0FT′
k[µ∗ |W µ:|+υ] (12)
Where |Wµ:|denotes the number of elements in
theµthrow of the full weight matrix Wl
oinM,
FT′
k,l[ν]represents the νthdiagonal element in FT′
k,l.
Afterward, we exploit the lossless method to cal-
culate the proper local update parameter ratio as
ρk,l= 1−rk,l
Rk,l(see details in Section 4.3.1, with
rk,landRk,lrepresenting the corresponding rkand
Rkin Layer l. Finally, we take the most important
ρk,lneurons in terms of the importance score ∫µ
k,las
the local update parameters to be updated with the
parameters in GAL and freeze the other parameters
within the local update.
4.4 FibecFed Algorithm
The FibecFed algorithm is shown in Algorithm 1.
Within the initialization phase (Lines 1 - 10), the
difficulty scores of each batch are calculated based
on Formula 7 (Lines 2 - 4), the batches of data sam-
ples are sorted in ascending order in terms of the
difficulty scores for the curriculum data selection
strategy (Line 5), the GAL are calculated in Line 7
(see details in Section 4.3.1 ), and the local update
parameters are computed in Line 9 (see details in
Section 4.3.2 ). Then, within the fine-tuning phase
is performed in Lines 11 - 19. A set of devices Kis
randomly selected (Line 12). Then, on each device,
local update is carried out in Lines 13 - 17. First,
the data samples are selected based on the curricu-
lum data selection strategy in Line 14. Second, the
LoRA parameters are updated with the global pa-
rameters in PSL transferred from the server in Line
15 (see details in Section 4.1 ). Third, the LoRA
parameters are updated based on the local training
with the selected Bt
kdata samples in Line 16 (see
details in Section 4.3.2 ). Finally, the parameters in
the global aggregation layers are aggregated using
the FedAvg algorithm (McMahan et al., 2017) on
the server (Line 18).Algorithm 1 Fisher Information-base Efficient Cur-
riculum Federated Learning (FibecFed)
Input:
T: The maximum number of rounds
K: The number of devices
D={D1, D2, ..., D K}: The set of datasets
on each device
η={η1, η2, ..., η T}: The learning rates
Output:
Pt: The set of LoRA parameters at Round t
1:forkin{1,2, ..., K}(in parallel) do
2: forBj∈Dkdo
3: ∫j←Calculation based on Formula 7
4: end for
5: SortBj∈Dkin ascending order of ∫j
6:end for
7:GAL←Compute the GAL
8:forkin{1,2, ..., K}(in parallel) do
9:Pu
k←Compute local update parameters
10:end for
11:fortin{1,2, ..., T}do
12: Sample K ⊆ { 1,2, ..., K}devices
13: forkinKdo
14: Select Bt
kdata samples based on For-
mula 8
15: Pt−1
2
k←Update Pt−1
kwithPt−1
GAL
16: Pt
k←Update Pu
k⊂ Pt−1
2
kusingBt
k
data samples
17: end for
18: Pt
GAL←Aggregate Pt
GAL,kwithk∈ K
19:end for
While FibecFed focuses on improving the effi-
ciency of federated learning, it can further enhance
the data privacy without bringing extra privacy is-
sues. Within the training phase of FibecFed, we
only transfer the parameters in global aggregation
layers between the server and devices, which can
avoid transfer the whole model so as to protect
the data privacy. In addition, we update the local
update parameters instead of the full parameters,
which can further avoid privacy and security issues
due to potential gradient leakage. Traditional attack
methods, e.g., gradient attack, assume that full gra-
dients or models are transferred between the server
and devices (Dimitrov et al., 2022; Marchand et al.,
2023). Thus, compared with the traditional feder-
ated learning approaches that transfer the whole
model between the server and devices, FibecFed
can further enhance the privacy and security of
federated learning.5 Experiments
In this section, we demonstrate extensive experi-
mentation with 17 baseline approaches and 10 NLP
tasks to reveal the advantages of FibecFed.
5.1 Experimental Setup
We take an FL environment composed of 100 de-
vices and a parameter server in the experimentation.
We randomly sample 10 devices in each epoch. We
utilize 10 commonly-used NLP tasks in the ex-
perimentation, i.e., QNLI (Rajpurkar et al., 2016),
SST-2 (Socher et al., 2013), CoLA (Warstadt et al.,
2019), MRPC (Dolan and Brockett, 2005), RTE
(Giampiccolo et al., 2007), BoolQ (Clark et al.,
2019), MPQA (Wiebe et al., 2005), Subj (Pang and
Lee, 2004), TREC (V oorhees and Tice, 2000), and
MR (Pang and Lee, 2005). The input data of the
tasks are non-IID among the 100 devices. We com-
pare FibecFed with 17 baseline approaches, i.e., a
parameter efficient fine-tuning-based approaches
(Adapter (Houlsby et al., 2019)), 6 prompt-based
tuning methods (FedPrompt (Zhao et al., 2023a),
P-tuning v2 (Liu et al., 2022d), IDPG (Wu et al.,
2022b), ATTEMPT (Asai et al., 2022), LPT (Liu
et al., 2022c), LoRA (Hu et al., 2021)), 4 curricu-
lum learning-based approaches (Shortformer (Press
et al., 2021), VOC (Platanios et al., 2019), SLW
(Li et al., 2024), SE (Peng et al., 2023)), 3 person-
alized FL methods (PFedGate (Chen et al., 2023),
FedDST (Bibikar et al., 2022), FedALT (Pillutla
et al., 2022)), and 2 Lora based methods (SLoRA
(Babakniya et al., 2023), AdaLoRA (Zhang et al.,
2023)). We carry out the experimentation based
on two language models, i.e., RoBERTa LARGE (Liu
et al., 2020) and LLaMA (Touvron et al., 2023).
5.2 Evaluation of FibecFed
In this section, we present the evaluation of
FibecFed based on RoBERTa LARGE and LLaMA.
5.3 Evaluation based on RoBERTa LARGE
Table 1 present the convergence accuracy of
diverse approaches based on RoBERTa LARGE .
FibecFed significantly outperforms baseline meth-
ods in terms of the convergence accuracy (up
to 45.35%, 38.37%, 12.69%, 37.60%, 42.38%,
18.72%, 7.01%, 6.36%, 5.70%, 5.90%, 8.79%,
28.45%, 16.70%, 4.96%, 5.44%, 14.11% and
5.49% compared with Adapter, FedPrompt, P-
tuning v2, IDPG, ATTEMPT, LTP, LoRA, Short-
former, VOC, SLW, PFedGate, FedDST, SE,Method QNLI SST-2 CoLA MRPC RTE BoolQ MPQA Subj Trec MR Avg
Adapter 49.46 90.83 54.17 84.77 47.29 62.17 90.95 51.65 96.2 91.30 58.32
FedPrompt 87.73 94.38 19.79 76.31 64.98 74.58 90.10 94.25 92.6 91 78.57
P-tuning v2 88.74 94.04 50.23 78.16 76.17 74.89 88.75 95.5 90.8 90.65 82.79
IDPG 66.7 89.11 4.59 72.22 52.35 68.93 71.8 59.4 73.4 86.35 64.48
ATTEMPT 50.74 50.92 4.63 76.01 54.15 62.17 90.35 88.85 77.2 91.15 64.16
LPT 89.38 94.27 50.78 82.38 80.86 62.2 90.15 95.75 92.4 90.6 82.87
LORA 89.86 94.72 54.78 83.15 78.7 75.96 89.2 95.8 94.4 91.35 84.72
Shortformer 90.17 94.04 54.16 84.49 79.78 78.62 90.6 96.85 95.2 91.1 85.5
VOC 91.89 95.18 53.64 85.15 81.31 78.32 90.85 96.85 96.6 91.25 86.1
SLW 91.91 94.1 54.75 85.01 79.06 78.41 91.25 96.6 95.8 91.15 85.80
PFedGate 90.72 93.81 50.73 83.33 76.17 76.88 89.8 94.5 94 87.65 83.75
FedDST 90.15 94.61 30.12 81.41 71.84 77.13 90.05 95.85 86.2 91.2 80.85
SE 76.42 93.69 55.47 81.72 71.84 76.42 89.8 96.3 87.2 91.85 82.07
FedALT 91.76 94.15 55.5 85.89 80.95 80.46 90 96.15 96.6 91.25 86.23
sLORA 92.40 94.38 55.51 85.41 82.31 80.24 91.30 96.8 97.2 91.35 86.69
adaLORA 91.14 92.32 44.46 81.83 75.81 77.16 89.65 96.25 91 91.8 83.14
Delta-LoRA 92.41 94.27 54.95 85.36 83.39 80.18 90.80 96.75 96.8 91.25 86.61
Ours 93.12 95.76 58.57 90.85 84.69 80.92 91.35 97.0 97.8 92.95 88.31
Table 1: The convergence accuracy with FibecFed and diverse baseline approaches. The evaluation with GLUE
benchmark is based on development sets while others are based on test sets. The best results are highlighted in bold
and the second bests are marked with underline . The results are obtained using RoBERTa LARGE .
FedALT, sLoRA, AdaLoRA, and Delta-LoRA re-
spectively). In addition, we analyze the time
to achieve target accuracy (see details in Ap-
pendix), which demonstrates that FibecFed out-
performs baseline methods in terms of efficiency
(up to 94.7%, 97.43%, 98.61%, 61.64%, 96.12%,
91.6%, 96.69%, 89.12%, 80.15%, 84.26%, 80.82%,
97.01%, 96.52%, 82.16%, 85.18%, 95.66% and
69.65% compared with Adapter, FedPrompt, P-
tuning v2, IDPG, ATTEMPT, LTP, LoRA, Short-
former, VOC, SLW, PFedGate, FedDST, SE,
FedALT, sLoRA, AdaLoRA, and Delta-LoRA re-
spectively). The advantages are expected as our
proposed curriculum data selection strategy on
each device can well improve both the efficiency
and the effectiveness. In addition, the proposed im-
portant layer selection method can reduce the scale
of parameters to transfer between the server and
devices. Furthermore, the local update parameter
selection method can well reduce useless compu-
tation on each devices while freezing unimportant
parameters may mitigate the effect of overfitting
brought by the non-IID data.
5.4 Evaluation based on LLaMA 7B
We carried out the experimentation with a LLM,
i.e., LLaMA 7B on MRPC, MR, and SST-2 dataset.
As shown in Table 2, FibecFed significantly outper-
forms baseline approaches in terms of both perfor-
mance (up to 29.60%, 33.93%, 11.91%, 12.27%,
5.41%, 11%, 12.27%, 26.35%, 4.69% higher ac-
curacy compared with FedPrompt, P-tuning v2,
ATTEMPT, LPT, LORA, VOC, SE, SLoRA andMethodCOLA MRPC RTE
Acc Time Acc Time Acc Time
FedPrompt 59.30 2527 80.54 1365 56.68 1296
P-tuning v2 3.89 2159 80.18 935 52.35 842
ATTEMPT 54.77 1825 81.38 1069 74.37 934
LPT 51.8 1645 80.13 958 74.01 1045
LoRA 59.55 1768 79.56 941 80.87 984
V oc 60.64 1364 38.64 859 75.28 745
SE 58.69 1628 79.35 846 74.01 839
SLoRA 60.56 1602 80.39 947 59.93 945
DeltaLoRA 58.91 1674 81.67 1034 81.95 1329
FibecFed 61.48 1298 81.93 832 86.28 703
Table 2: Convergence accuracy and fine-tuning time on
COLA, MRPC and RTE with LLaMA.
DeltaLoRA) and efficiency (up to 45.75%, 39.87%,
28.8%, 32.7%, 26.58%, 5.63%, 16.2%, 25.6%,
47.1% faster compared with FedPrompt, P-tuning
v2, ATTEMPT, LPT, LORA, VOC, SE, SLoRA
and DeltaLoRA). The advantages reveal the our
proposed approach improves both the efficiency
and effectiveness with LLM.
5.5 Robustness & Scalability
In this section, we demonstrate the robustness
and the scalability of FibecFed with divers de-
grees of non-IID data and different device numbers.
FibecFed achieves comparable accuracy across
divers degrees of data heterogeneity, the difference
of which is smaller than 1.83%. In addition, we
conduct the experimentation with the device num-
ber ranging from 20 to 100 based on MRPC dataset
and find that the disparity is smaller than 0.79% in
terms of accuracy.5.6 Communication overhead
The absolute communication overhead is shown in
the Table 13 of Appendix (with RoBERTa LARGE ).
The communication overhead of FibecFed is higher
than that of FedPrompt (up to 3.51 times), IDPG
(up to 3.3 times), and ATTEMPT (up to 1.85 times).
This is expected as the these three methods are
prompt tuning-based methods, which corresponds
to much fewer parameters to update during the
training phase compared with FibecFed. However,
these three methods correspond to significantly
lower performance (compared with FibecFed),
i.e., low convergence accuracy (from 1.25% to
38.68% for FedPrompt, from 6.6% to 53.98% for
IDPG, and from 1% to 53.94% for ATTEMPT), as
shown in Table 1. The communication overhead
of FibecFed is significantly lower than the other
methods (6.25 times for Adapter, 9.67 times for P-
tuning V2, 1.9 times for LPT, and 25% for LORA,
SHORTFORMER, V oc, SLW, PFedGate, FedDST,
SE, FedAlt, sLora, AdaLora, Delta-LoRA). This
is expected as well as FibecFed only transfers the
global aggregation layers instead of the parameters
of all the layers in LLM.
In addition, the relative communication overhead
(i.e., the ratio between the absolute communication
overhead and the total training time) is shown in
Table 14 of Appendix. Similar to the absolute com-
munication overhead, the relative communication
overhead of FibecFed is higher than that of Fed-
Prompt (from 2.3% to 37.4%), IDPG (from 2.4% to
34.4%), and ATTEMPT (from 1.9% to 31.7%) as
well. This is expected as explained before. As the
total training time of FibecFed becomes shorter, the
relative communication overhead of FibecFed be-
comes slightly more significant than FedAlt (from
1.4% to 20.1%) and AdaLora (from 0.8% to 5.7%).
In addition, the relative communication overhead
of FibecFed becomes similar to that of sLora (from
7.8% smaller to 24.8% bigger) and Delta-LoRA
(from 7.5% smaller to 5.5% bigger). The rela-
tive communication overhead of FibecFed is still
smaller than the rest approaches, i.e., Adapter (up
to 43.9%), P-tuning V2 (up to 55.6%), LPT (up to
25.1%), LORA (up to 14.5%), SHORTFORMER
(up to 13.8%), V oc (up to 8.0%), SLW (up to 8.8%),
PFedGate (up to 7.0%), FedDST (up to 7.1%), SE
(8.1%). Please note that FibecFed corresponds to
higher convergence accuracy (as shown in Table 1)
and shorter training time (as shown in Table 2).5.7 Ablation Study
To analyze the impact of each module in FibecFed,
we demonstrate the ablation study in terms of the
curriculum data selection method, the important
layer selection method, and the local update param-
eter selection method. First, we conduct a compara-
tive analysis among four curriculum strategies, i.e.,
SLW, VOC, Shortformer, SE, and that without cur-
riculum (NULL). FibecFed corresponds to superior
performance in terms of accuracy (up to 5.73%,
9.12% 5.84%, 6.41%, 7.7% compared with V oc,
SE, SLW, Shortformer, and NULL, respectively)
and efficiency (up to 26.53%, 34.26% 68.92%,
68.36%, 58.57% compared with V oc, SE, SLW,
Shortformer, and NULL, respectively). Afterward,
we compare the importance layer selection method
with Ascending Order (AO), Descending Order,
Random Order (RO) and that with full layer syn-
chronization (FULL), which reveals the advantages
of our layer selection method in terms of accuracy
(up to 3.42%, 2.76%, 2.65%, 1.02%) and efficiency
(up to 29.3%, 18.46%, 23.1%, 15.3%) compared
with AO, DO, RO, and FULL, respectively. Fur-
thermore, we compare our local update parame-
ter selection method to that without selection, i.e.,
all the parameters are updated. The advantage of
our local update parameter selection method can
achieve 2.48% higher accuracy and 11.8% faster.
6 Conclusion
In this paper, we propose an original fisher
information-based efficient curriculum federated
learning, i.e., FibecFed. Within FibecFed, we pro-
pose an adaptive federated curriculum learning
method and an efficient sparse parameter update
method. We exploit fisher information to calculate
the difficulty scores of data samples and propose
the an original curriculum data selection strategy.
In the sparse parameter update method, we propose
a new sensitivity-based important layer selection
technique and a novel fisher information-based im-
portant parameter technique method while freezing
the remaining parameters to achieve both efficiency
and effectiveness. We demonstrate the results of ex-
tensive experimentation to compare FibecFed with
17 baseline approaches based on 10 NLP tasks,
which reveal significant advantages of FibecFed in
terms of accuracy (up to 45.35%) and fine-tuning
speed (up to 98.61% faster).Limitations
While our approach can be to exploited to signifi-
cantly improve the performance and efficiency of
LLM federated learning, we assume that a cen-
tral parameter server is exploited to coordinate the
training process. When there is not a central pa-
rameter or the heterogeneous devices (Che et al.,
2022; Li et al., 2022b) are connected based on di-
verse topologies (decentralized setting (Liu et al.,
2024a)), e.g., ring, it would be complicated to
directly exploit our proposed approach. In addi-
tion, our approach can be combined with model
compression methods, e.g., model pruning and
quantization (Jia et al., 2024), to achieve better
performance. In the future, we anticipate exploit-
ing model compression methods in LLM federated
learning with a decentralized setting.
References
Akari Asai, Mohammadreza Salehi, Matthew E. Pe-
ters, and Hannaneh Hajishirzi. 2022. ATTEMPT:
parameter-efficient multi-task tuning via attentional
mixtures of soft prompts. In Conf. on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 6655–6672.
Sara Babakniya, Ahmed Roushdy Elkordy, Yahya H.
Ezzeldin, Qingfeng Liu, Kee-Bong Song, Mostafa
El-Khamy, and Salman Avestimehr. 2023. Slora:
Federated parameter efficient fine-tuning of language
models. In Int. Workshop on Federated Learning in
the Age of Foundation Models in Conjunction with
NeurIPS , pages 1–13.
Yoshua Bengio, Jérôme Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
Int. Conf. on Machine Learning (ICML) , page 41–48.
Association for Computing Machinery.
Sameer Bibikar, Haris Vikalo, Zhangyang Wang, and
Xiaohan Chen. 2022. Federated dynamic sparse train-
ing: Computing less, communicating less, yet learn-
ing better. In AAAI Conf. on Artificial Intelligence
(AAAI) , pages 6080–6088. AAAI Press.
Californians for Consumer Privacy. 2020. California
consumer privacy act home page. https://www.
caprivacy.org/ . Online; accessed 09/05/2022.
Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen
Zhou, Victor Sheng, Huaiyu Dai, and Dejing Dou.
2023a. Federated learning of large language mod-
els with parameter-efficient prompt tuning and adap-
tive optimization. In Conf. on Empirical Methods
in Natural Language Processing (EMNLP) , pages
7871–7888.
Tianshi Che, Zijie Zhang, Yang Zhou, Xin Zhao, Ji Liu,
Zhe Jiang, Da Yan, Ruoming Jin, and Dejing Dou.2022. Federated fingerprint learning with hetero-
geneous architectures. In IEEE Int. Conf. on Data
Mining (ICDM) , pages 31–40.
Tianshi Che, Yang Zhou, Zijie Zhang, Lingjuan Lyu,
Ji Liu, Da Yan, Dejing Dou, and Jun Huan. 2023b.
Fast federated machine unlearning with nonlinear
functional theory. In Int. Conf. on Machine Learning
(ICML) , pages 4241–4268.
Daoyuan Chen, Liuyi Yao, Dawei Gao, Bolin Ding, and
Yaliang Li. 2023. Efficient personalized federated
learning via sparse model-adaptation. In Int. Conf.
on Machine Learning (ICML) , volume 202, pages
5234–5256.
Xinyang Chen, Sinan Wang, Jianmin Wang, and Ming-
sheng Long. 2021. Representation subspace distance
for domain adaptation regression. In Int. Conf. on
Machine Learning (ICML) , pages 1749–1759.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. Boolq: Exploring the surprising
difficulty of natural yes/no questions. In Conf. of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, (NAACL-HLT) , pages 2924–2936.
Rong Dai, Li Shen, Fengxiang He, Xinmei Tian,
and Dacheng Tao. 2022. Dispfl: Towards
communication-efficient personalized federated
learning via decentralized sparse training. In Int.
Conf. on Machine Learning (ICML) .
Enmao Diao, Ganghua Wang, Jiawei Zhang, Yuhong
Yang, Jie Ding, and Vahid Tarokh. 2023. Pruning
deep neural networks from a sparsity perspective. In
Int. Conf. on Learning Representations (ICLR) , pages
1–12.
Dimitar I. Dimitrov, Mislav Balunovi ´c, Nikola Kon-
stantinov, and Martin Vechev. 2022. Data leak-
age in federated averaging. arXiv preprint
arXiv:2206.12395 .
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
InProceedings of the Third International Workshop
on Paraphrasing, IWP@IJCNLP 2005, Jeju Island,
Korea, October 2005, 2005 . Asian Federation of Nat-
ural Language Processing.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL) , pages
320–335.
Tran Trong Duy, Ly V Nguyen, Viet-Dung Nguyen,
Nguyen Linh Trung, and Karim Abed-Meraim. 2022.
Fisher information neural estimation. In European
Signal Processing Conference (EUSIPCO) , pages
2111–2115. IEEE.Jeffrey L. Elman. 1993. Learning and development in
neural networks: the importance of starting small.
Cognition , 48(1):71–99.
Gemini Team et al. 2023. Gemini: A family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wen-
bin Wei, Lixin Fan, and Qiang Yang. 2023. Fate-
llm: A industrial grade federated learning frame-
work for large language models. arXiv preprint
arXiv:2310.10049 .
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and
Behnam Neyshabur. 2021. Sharpness-aware mini-
mization for efficiently improving generalization. In
Int. Conf. on Learning Representations (ICLR) , pages
1–19.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third PASCAL recognizing
textual entailment challenge. In Workshop on Textual
Entailment and Paraphrasing (ACL-PASCAL@ACL) ,
pages 1–9.
Dong-Jun Han, Do-Yeon Kim, Minseok Choi, Christo-
pher G Brinton, and Jaekyun Moon. 2023. Splitgp:
Achieving both generalization and personalization in
federated learning. In IEEE Int. Conf, on Computer
Communications (INFOCOM) , pages 1–10. IEEE.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In
Int. Conf. on Machine Learning (ICML) , volume 97,
pages 2790–2799.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.0968 .
Stanislaw Jastrzebski, Devansh Arpit, Oliver Astrand,
Giancarlo B Kerg, Huan Wang, Caiming Xiong,
Richard Socher, Kyunghyun Cho, and Krzysztof J
Geras. 2021. Catastrophic fisher explosion: Early
phase fisher matrix impacts generalization. In Int.
Conf. on Machine Learning (ICML) , pages 4772–
4784. PMLR.
Divyansh Jhunjhunwala, Shiqiang Wang, and Gauri
Joshi. 2024. Fedfisher: Leveraging fisher informa-
tion for one-shot federated learning. In Int. Conf.
on Artificial Intelligence and Statistics , pages 1612–
1620.
Juncheng Jia, Ji Liu, Chendi Zhou, Hao Tian, Mianx-
iong Dong, and Dejing Dou. 2024. Efficient asyn-
chronous federated learning with sparsification and
quantization. Concurrency and Computation: Prac-
tice and Experience , 36(9):e8002.Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, Lélio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023. Mistral 7b. arXiv
preprint arXiv:2310.06825 .
Jiayin Jin, Jiaxiang Ren, Yang Zhou, Lingjuan Lyu,
Ji Liu, and Dejing Dou. 2022. Accelerated federated
learning with decoupled adaptive optimization. In Int.
Conf. on Machine Learning (ICML) , pages 10298–
10322.
Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari.
2019. Universal statistics of fisher information in
deep neural networks: Mean field approach. In Int.
Conf. on Artificial Intelligence and Statistics (AIS-
TATS) , pages 1032–1041. PMLR.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar
Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. 2020. SCAFFOLD:
Stochastic controlled averaging for federated learn-
ing. In Int. Conf. on Machine Learning (ICML) ,
volume 119, pages 5132–5143.
Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan
Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie,
Yaliang Li, Bolin Ding, and Jingren Zhou. 2023.
Federatedscope-llm: A comprehensive package for
fine-tuning large language models in federated learn-
ing. arXiv preprint arXiv:2309.00363 .
Frederik Kunstner, Philipp Hennig, and Lukas Balles.
2019. Limitations of the empirical fisher approxima-
tion for natural gradient descent. Advances in neural
information processing systems (NeurIPS) , 32.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Conf. on Empirical Methods in Natural
Language Processing (EMNLP) , pages 3045–3059.
Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang,
Connor Holmes, Cheng Li, and Yuxiong He. 2024.
Deepspeed data efficiency: Improving deep learn-
ing model quality and training efficiency via effi-
cient data sampling and routing. arXiv preprint
arXiv:2212.03597 , pages 1–19.
Conglong Li, Minjia Zhang, and Yuxiong He. 2022a.
The stability-efficiency dilemma: Investigating se-
quence length warmup for training GPT models. In
Advances in Neural Information Processing Systems
(NeurIPS) .
Guanghao Li, Yue Hu, Miao Zhang, Ji Liu, Quanjun Yin,
Yong Peng, and Dejing Dou. 2022b. Fedhisyn: A hi-
erarchical synchronous federated learning framework
for resource and data heterogeneity. In Int. Conf. on
Parallel Processing (ICPP) , pages 1–11.
Mu Li, David G Andersen, Jun Woo Park, Alexander J
Smola, Amr Ahmed, Vanja Josifovski, James Long,Eugene J Shekita, and Bor-Yiing Su. 2014. Scal-
ing distributed machine learning with the parameter
server. In USENIX Symposium on Operating Systems
Design and Implementation (OSDI) , pages 583–598.
Ji Liu, Tianshi Che, Yang Zhou, Ruoming Jin, Huaiyu
Dai, Dejing Dou, and Patrick Valduriez. 2024a.
Aedfl: efficient asynchronous decentralized feder-
ated learning with heterogeneous devices. In SIAM
Int. Conf. on Data Mining (SDM) , pages 833–841.
Ji Liu, Chunlu Chen, Yu Li, Lin Sun, Yulun Song,
Jingbo Zhou, Bo Jing, and Dejing Dou. 2024b. En-
hancing trust and privacy in distributed networks: a
comprehensive survey on blockchain-based federated
learning. Knowledge and Information Systems , pages
1–27.
Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei
Ji, Haoyi Xiong, and Dejing Dou. 2022a. From
distributed machine learning to federated learning:
A survey. Knowledge and Information Systems ,
64(4):885–917.
Ji Liu, Juncheng Jia, Tianshi Che, Chao Huo, Jiaxiang
Ren, Yang Zhou, Huaiyu Dai, and Dejing Dou. 2024c.
Fedasmu: Efficient asynchronous federated learning
with dynamic staleness-aware model update. In AAAI
Conf. on Artificial Intelligence , volume 38, pages
13900–13908.
Ji Liu, Juncheng Jia, Beichen Ma, Chendi Zhou, Jingbo
Zhou, Yang Zhou, Huaiyu Dai, and Dejing Dou.
2022b. Multi-job intelligent scheduling with cross-
device federated learning. IEEE Transactions on
Parallel and Distributed Systems (TPDS) , 34(2):535–
551.
Ji Liu, Juncheng Jia, Hong Zhang, Yuhui Yun, Leye
Wang, Yang Zhou, Huaiyu Dai, and Dejing Dou.
2024d. Efficient federated learning using dynamic
update and adaptive pruning with momentum on
shared server data. ACM Transactions on Intelligent
Systems and Technology .
Ji Liu, Zhihua Wu, Danlei Feng, Minxu Zhang, Xinx-
uan Wu, Xuefeng Yao, Dianhai Yu, Yanjun Ma, Feng
Zhao, and Dejing Dou. 2023a. Heterps: Distributed
deep learning with reinforcement learning based
scheduling in heterogeneous environments. Future
Generation Computer Systems , 148:106–117.
Ji Liu, Xuehai Zhou, Lei Mo, Shilei Ji, Yuan Liao,
Zheng Li, Qin Gu, and Dejing Dou. 2023b. Dis-
tributed and deep vertical federated learning with big
data. Concurrency and Computation: Practice and
Experience , 35(21):e7697.
Xiangyang Liu, Tianxiang Sun, Xuanjing Huang, and
Xipeng Qiu. 2022c. Late prompt tuning: A late
prompt could be better than many prompts. In Find-
ings of the Association for Computational Linguistics:
EMNLP , pages 1325–1338.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-
iao Du, Zhilin Yang, and Jie Tang. 2022d. P-tuning:Prompt tuning can be comparable to fine-tuning
across scales and tasks. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL) , pages
61–68. V olume 2: Short Papers.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt
understands, too. arXiv preprint arXiv:2103.10385 .
Xiaofeng Liu, Yinchuan Li, Qing Wang, Xu Zhang,
Yunfeng Shao, and Yanhui Geng. 2023c. Sparse
personalized federated learning. IEEE Transactions
on Neural Networks and Learning Systems , pages
1–15.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2020.
Roberta: A robustly optimized BERT pretraining
approach. In Int. Conf. on Learning Representations
(ICLR) , pages 1–15.
Alexander Ly, Maarten Marsman, Josine Verhagen,
Raoul PPP Grasman, and Eric-Jan Wagenmakers.
2017. A tutorial on fisher information. Journal of
Mathematical Psychology , 80:40–55.
Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.
LLM-pruner: On the structural pruning of large lan-
guage models. In Conf. on Neural Information Pro-
cessing Systems (NeurIPS) , pages 1–19.
Tanguy Marchand, Regis Loeb, Ulysse Marteau-Ferey,
Jean Ogier du Terrail, and Arthur Pignet. 2023.
SRATTA: sample re-attribution attack of secure ag-
gregation in federated learning. In Int. Conf. on Ma-
chine Learning (ICML) , volume 202, pages 23886–
23914.
James Martens. 2020. New insights and perspectives on
the natural gradient method. The Journal of Machine
Learning Research , 21(1):5776–5851.
James Martens and Roger Grosse. 2015. Optimizing
neural networks with kronecker-factored approxi-
mate curvature. In Int. Conf. on Machine Learning
(ICML) , pages 2408–2417.
Brendan McMahan, Eider Moore, Daniel Ramage,
Seth Hampson, and Blaise Aguera y Arcas. 2017.
Communication-efficient learning of deep networks
from decentralized data. In Artificial Intelligence and
Statistics (AISTATS) , pages 1273–1282.
Joe Mellor, Jack Turner, Amos Storkey, and Elliot J
Crowley. 2021. Neural architecture search without
training. In Int. Conf. on Machine Learning (ICML) ,
pages 7588–7598.
Official Journal of the European Union.
2016. General data protection regulation.
https://eur-lex.europa.eu/legal-content/
EN/TXT/PDF/?uri=CELEX:32016R0679 . Online;
accessed 09/05/2022.Yann Ollivier. 2015. Riemannian metrics for neural
networks i: feedforward networks. Information and
Inference: A Journal of the IMA , 4(2):108–153.
OpenAI. 2022. Chatgpt. https://openai.com/blog/
chatgpt . Online; accessed 05/02/2024.
Kazuki Osawa, Shigang Li, and Torsten Hoefler. 2023.
Pipefisher: Efficient training of large language mod-
els using pipelining and fisher information matrices.
Machine Learning and Systems , 5.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL) , pages
271–278.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Annual Meeting of
the Association for Computational Linguistics (ACL) ,
pages 115–124.
Razvan Pascanu and Yoshua Bengio. 2013. Revisiting
natural gradient for deep networks. In Int. Conf. on
Learning Representations (ICLR) , pages 1–18.
Keqin Peng, Liang Ding, Qihuang Zhong, Yuanxin
Ouyang, Wenge Rong, Zhang Xiong, and Dacheng
Tao. 2023. Token-level self-evolution training for
sequence-to-sequence learning. In Annual Meeting of
the Association for Computational Linguistics (ACL) ,
pages 841–850.
Krishna Pillutla, Kshitiz Malik, Abdelrahman Mo-
hamed, Michael G. Rabbat, Maziar Sanjabi, and Lin
Xiao. 2022. Federated learning with partial model
personalization. In Int. Conf. on Machine Learning
(ICML) , volume 162, pages 17716–17758.
Emmanouil Antonios Platanios, Otilia Stretcu, Gra-
ham Neubig, Barnabas Poczos, and Tom Mitchell.
2019. Competence-based curriculum learning for
neural machine translation. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL) , pages 1162–1172.
Ofir Press, Noah A. Smith, and Mike Lewis. 2021.
Shortformer: Better language modeling using shorter
inputs. In Annual Meeting of the Association for
Computational Linguistics and Int. Joint Conference
on Natural Language Processing (ACL/IJCNLP) ,
pages 5493–5505.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions
for machine comprehension of text. In Conf. on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 2383–2392.
Mehdi Setayesh, Xiaoxiao Li, and Vincent WS Wong.
2022. Perfedmask: Personalized federated learning
with optimized masking vectors. In Int. Conf. on
Learning Representations (ICLR) .Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y . Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Conf. on Empirical Methods in Natural
Language Processing (EMNLP) , pages 1631–1642.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. LLama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Saeed Vahidian, Sreevatsank Kadaveru, Woonjoon
Baek, Weijia Wang, Vyacheslav Kungurtsev, Chen
Chen, Mubarak Shah, and Bill Lin. 2023. When do
curricula work in federated learning? In IEEE/CVF
Int. Conf. on Computer Vision (ICCV) , pages 5061–
5071.
Jesse Vig and Yonatan Belinkov. 2019. Analyzing
the structure of attention in a transformer language
model. arXiv preprint arXiv:1906.04284 .
Ellen M. V oorhees and Dawn M. Tice. 2000. Build-
ing a question answering test collection. In Annual
International ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval (SIGIR) , pages
200–207.
Chenguang Wang, Mu Li, and Alexander J. Smola. 2019.
Language models with transformers. arXiv preprint
arXiv:1904.09408 .
Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020.
Structured pruning of large language models. In
Conf. on Empirical Methods in Natural Language
Processing (EMNLP) , pages 6151–6162.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Trans. Assoc. Comput. Linguistics , 7:625–641.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Lang. Resour. Evaluation , 39(2-3):165–
210.
Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan Fang
Li, Guilin Qi, and Gholamreza Haffari. 2022a. Pre-
trained language model in continual learning: A com-
parative study. In Int. Conf. on Learning Representa-
tions (ICLR)) .
Xueyu Wu, Xin Yao, and Cho-Li Wang. 2021. Fedscr:
Structure-based communication reduction for feder-
ated learning. IEEE Transactions on Parallel and
Distributed Systems (TPDS) , 32(7):1565–1577.
Zhuofeng Wu, Sinong Wang, Jiatao Gu, Rui Hou, Yux-
iao Dong, V . G. Vinod Vydiswaran, and Hao Ma.
2022b. IDPG: an instance-dependent prompt gen-
eration method. In Conf. of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, (NAACL) ,
pages 5507–5521.Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi
Chen. 2023. Sheared LLaMA: Accelerating lan-
guage model pre-training via structured pruning. In
Workshop on Advancing Neural Network Training:
Computational Efficiency, Scalability, and Resource
Optimization (WANT@NeurIPS) , pages 1–22.
M Xu, D Cai, Y Wu, X Li, and S Wang. 2024.
Fwdllm: Efficient fedllm using forward gradient.
arXiv preprint arXiv:2308.13894 .
Zifan Xu, Yulin Zhang, Shahaf S. Shperberg, Reuth
Mirsky, Yuqian Jiang, Bo Liu, and Peter Stone. 2022.
Model-based meta automatic curriculum learning.
InDecision Awareness in Reinforcement Learning
Workshop at ICML .
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,
Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan
Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.
GLM-130b: An open bilingual pre-trained model. In
Int. Conf. on Learning Representations (ICLR) .
Hong Zhang, Ji Liu, Juncheng Jia, Yang Zhou, Huaiyu
Dai, and Dejing Dou. 2022. Fedduap: Federated
learning with dynamic update and adaptive pruning
using shared data on the server. In Int. Joint Conf. on
Artificial Intelligence (IJCAI) , pages 2776–2782. Int.
Joint Conf. on Artificial Intelligence Organization.
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and
Wei Lu. 2024. Tinyllama: An open-source small
language model. arXiv preprint arXiv:2401.02385 .
Qingru Zhang, Minshuo Chen, Alexander Bukharin,
Pengcheng He, Yu Cheng, Weizhu Chen, and
Tuo Zhao. 2023. Adaptive budget allocation for
parameter-efficient fine-tuning. In Int. Conf. on
Learning Representations (ICLR) .
Zeru Zhang, Jiayin Jin, Zijie Zhang, Yang Zhou, Xin
Zhao, Jiaxiang Ren, Ji Liu, Lingfei Wu, Ruoming Jin,
and Dejing Dou. 2021. Validating the lottery ticket
hypothesis with inertial manifold theory. Advances
in Neural Information Processing Systems (NeurIPS) ,
34.
Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and
Gongshen Liu. 2023a. Fedprompt: Communication-
efficient and privacy-preserving prompt tuning in fed-
erated learning. In IEEE Int. Conf. on Acoustics,
Speech and Signal Processing (ICASSP) , pages 1–5.
Jujia Zhao, Wenjie Wang, Chen Xu, Zhaochun Ren,
See-Kiong Ng, and Tat-Seng Chua. 2024. Llm-
based federated recommendation. arXiv preprint
arXiv:2402.09959 .
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen
Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,
Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,
Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023b.A survey of large language models. arXiv preprint
arXiv:2303.18223 .
Chendi Zhou, Ji Liu, Juncheng Jia, Jingbo Zhou, Yang
Zhou, Huaiyu Dai, and Dejing Dou. 2022. Efficient
device scheduling with multi-job federated learning.
InAAAI Conf. on Artificial Intelligence (AAAI) , vol-
ume 36, pages 9971–9979.Algorithm 2 FedAvg
Input:
t: The number of current round
Pt−1: The global LoRA parameters at Round
t−1
T: The maximum number of rounds
K: The number of devices
D={D1, D2, ..., D K}: The set of datasets
on each device
λ={λ1, λ2, ..., λ T}: The learning rates
Output:
Pt: The global LoRA parameters at Round t
1:forkin{1,2, ..., K}(in parallel) do
2:Pk← Pt−1
3: forBj∈Dkdo
4: Pk ← P k -
λtP
si∈Bj∇Pkf(M,Pk, si, mi)
5: end for
6:end for
7:mt←PK
k=1|Dk|
8:Pt=PK
k=1|Dk|
mtPk
A FedAvg Update
The original FedAvg update is shown in Algorithm
2.
B Gradient Calculation in LoRA
During the local training, we update LoRA param-
eters at Epoch tfor data samples sias follows:
Pt
A=Pt−1
A−λt∇Pt−1
Af(M,Pk, si, mi)
Pt
B=Pt−1
B−λt∇Pt−1
Bf(M,Pk, si, mi),(13)
and for a batch of data samples Bjas follows:
Pt
A=Pt−1
A−λtX
si∈Bj∇Pt−1
Af(M,Pk, si, mi)
Pt
B=Pt−1
B−λtX
si∈Bj∇Pt−1
Bf(M,Pk, si, mi),
(14)
where λtis the learning rate, Pkis the combination
ofPAandPB. To get the gradient of reconstructed
full-rank matrix at iteration t, we can calculate as
follows:
∇Pt
kf(M,Pk, si, mi) =Pt
APt
B− Pt−1
APt−1
B
(15)C Formulas for Curriculum Federated
Learning
We can calculate the difficulty score of a data sam-
ple as defined in Formula 16.
∫i=Tr(˜Fi), (16)
where ∫iis the difficulty score of data sample si.
Then, the difficulty score of a batch of data samples
can be calculated based on Formula 17:
∫j=X
si∈BjTr(˜Fi), (17)
where ∫jis the difficulty score of the batch Bj.
The more significant the score is, the more diffi-
culty the batch of data samples is. We calculate the
difficulty score based on the initial model as the
difficulty of data samples corresponds to negligible
change during the fine-tuning process (Platanios
et al., 2019; Li et al., 2022a). ∫ican be computed
using the square of the elements in the diagonal of
the first-order derivative matrix, which can avoid
the computation of the full FIM with the heavy
multiplication of two matrices. This mechanism
makes the calculation feasible in terms of compu-
tation time and memory consumption. Please note
that FIM is calculated and stored locally, which
does not need to be transferred to the server.
We calculate Bt
kbased on Formula 18.
Bt
k= (β+ (1−β)t
αT)nk
B, (18)
where βrepresents the initial training sample ratio,
αdenotes the ratio of training epoch until all data
is used, Brefers to the batch size. Both βand
αare hyper-parameters within the range of [0,1].
Then, the batch of data samples is selected based
on Formula 19:
Selectt(Bj) =(
True ifj <Bt
k
False otherwise ,(19)
where Selectt(Bj)represents the selection deci-
sion. When Selectt(Bj) =True , the batch of
data samples is selected for local update. With
this curriculum training strategy, the local training
can learn from easy samples to challenging sam-
ples, which can achieve excellent performance with
fewer data samples in early iterations.D Novelty of FibecFed
Different from the existing approaches, we exploit
the fisher information to measure both the com-
plexity of training data and the importance of the
components, e.g., neurons, in LLMs. We enable
curriculum learning based on the complexity of the
training data within each device to achieve superb
accuracy. We exploit LoRA to achieve efficient
fine-tuning with only a few parameters. In addi-
tion, we split the trainable parameters into three
parts, each of which is for global aggregation, lo-
cal update, and as frozen neurons on each device,
respectively. Only the parameters of the global ag-
gregation part are updated and synchronized among
multiple devices during the fine-tuning process. We
consider the relative change of each layer and get
an excellent estimation of important layers to gen-
erate the part for global aggregation. We exploit
the momentum of parameter updating and get a
robust estimation of important neurons in order to
select important neurons for local update.
E Prompt-tuning in LLM
When exploiting the prompt tuning, the parameters
of the networks to generate prompts or prefix are
adjusted instead of the parameters of LLMs. The
prompt or the prefix is the added instruction con-
catenated to the input, which can guide LLMs to
generate proper answers. For instance, a prompt
(“This is [MASK]”) can be concatenated to the in-
put (“Wonderful movie!”) to be sent to a LLM,
which generates the label (“positive” or “negative”)
for a sentiment analysis task.
F Notations
In this paper, we use the notations summarized in
Table 3.
G Experiment results
In this section, we present the details of extensive
experiments. In Section G.1, we explain the de-
tails of experimental setup. In Section G.3, we
compare the efficiency in terms of time to achieve
target accuracy. In Section G.4, we show the con-
vergence accuracy over 10 different datasets. In
SectionG.5, we present the robustness of FibecFed
respect to scalability and data heterogeneity. In
Section G.6, we demonstrate the performance with
learning rates. In Section G.7, we discuss the im-
pact of different curriculum strategies.G.1 Experimental Setup
We present the hyper-parameters used in the fine-
tuning process in Table 8. We set the global training
epochs to 100, except for QNLI, SST-2. We follow
the Dirichlet distribution (with 1 as concentration
α) to partition the whole data into splits and assign
a certain number of samples based on Dirichlet dis-
tribution (with α= 5), development sets are served
as test data to evaluate performance in the GLUE
benchmark. For 4 other datasets, we select a certain
number of samples from the training set as the de-
velopment set, and the number of samples for each
label is determined according to its original label
distribution of training set. For datasets in GLUE
benchmark, we use their original data splits. For 4
other datasets with no default splits, we randomly
split the dataset into train, development, and test
sets.
With 100 devices, the number of samples ranges
from 346 to 2607 for QNLI, 222 to 1676 for SST-
2, 28 to 213 for COLA, 12 to 91 for MRPC, 8 to
62 for RTE, 31 to 235 for BoolQ, 25 to 189 for
MPQA, 23 to 174 for Subj, 16 to 123 for Trec and
25 to 191 for MR. Additionally, a detailed number
of samples distributed among 10 devices with the
MRPC dataset is shown in Table 4.
RoBERTa LARGE consists of 24 layers of trans-
formers followed by a classification head, which
contains 355M parameters. LLaMA is composed
of 32 transformer layers with 7B parameters.
The centralized methods (Adapter, P-tuning v2,
IDPG, ATTEMPT, LPT, LoRA, Shortformer, VOC,
SLW, AdaLoRA, Delta-LoRA) are adapted to the
FL setting with FedAvg (McMahan et al., 2017) for
a fair comparison. In addition, we exploit the LoRA
with FL opmization-based methods, i.e., PFedGate,
FedDst and FedALT.
G.2 Comparison with Random Data Selection
The heuristic based methods or mode-oriented
methods cannot address the heterogeneity issue
across local data samples with variance in such
metric. To show the effectiveness of our proposed
Fisher information-based metric, we implement
another baseline method using random selecting
in the curriculum data selection strategy. Table 5
shows the performance under different selecting
strategies. Compared with other selecting method,
FibecFed outperforms other baselines up to 8.51%
in terms of accuracy, which demonstrates the ef-
fectiveness of proposed method. In addition, Ran-dom corresponds to the lowest accuracy compared
other methods, i.e., ShortFormer, SLW, V oc, and
FibecFed. Table 6 also indicates that FibecFed
achieves the target accuracy of 85% on the MRPC
dataset in less time(up to 92.49% faster) compared
with other selection strategies. It is noteworthy that
the random method reaches the accuracy 85% at
epoch 95, significantly increasing the required time.
This will be clarified in the revised version.
G.3 Fine-tuning Efficiency
In order to show the efficiency of FibecFed, we
present the time to achieve target accuracy in Table
7.
G.4 Accuracy & Time Figures in
RoBERTa LARGE
Figures 2 - 5 present the convergence process of
FibecFed and other baselines on COLA, QNLI,
SST-2, MRPC, RTE, BOOLQ, MPQA and Subj
datasets. Figure 2 shows the convergence results
over FibecFed and benchmarks on COLA, QNLI
and SST-2 datasets, Figure 3 presents the results
on MRPC, RTE and BOOLQ datasets, Figure 4
demonstrates the evaluation results on MPQA, Subj
datasets and Figure 5 shows the performance on
Trec and MR datasets.
G.5 Accuracy & Time for Robustness &
Scalability
Figure 6 presents results of robustness of FibecFed.
Figure 6(a) shows accuracy of FibecFed with dif-
ferent learning rate. Figure 6(b) shows the perfor-
mance under varying client number. In addition,
Figure 6(c) indicates FibecFed is robust to different
degree of heterogeneity of data.
G.6 Impact of learning rate
As shown in Figure 6(a). the performance of
FibecFed on MRPC dataset differs (up to 2.22%)
among 4 varying learning rates from 1e−4to8e−4.
Thus, we take the learning rate of λ= 8e−4, which
corresponds to the best accuracy 90.59% in prac-
tice.
G.7 Impact of Curriculum Strategies
The curriculum strategy controls the speed of ex-
ploiting difficult data samples during the fine-
tuning process. We consider three strategies, i.e.,
linear, square (sqrt), and exponential (exp), whichare defined in Formulas 20, 21, and 22, respec-
tively.
Bt
k= (β+ (1−β)t
αT)nk
B, (20)
Bt
k= (β+ (1−β)t2
αT)nk
B, (21)
Bt
k= (β+ (1−β)et
αT)nk
B, (22)
where erepresents the Euler’s number. As shown
in Figure 7(c) the performance of linear (91%) is
similar to that of sqrt (91%), which are much higher
than exp (83.14%). However, as sqrt incurs more
complicated calculation compared with linear, we
exploit the linear strategy in the paper.
G.8 Generalization of FibecFed
To further evaluate the generalization of FibecFed
on modest models, we conducted image classifica-
tion task on the CIFAR-10 dataset using a seven-
layer Multilayer Perceptron (MLP). As demon-
strated in Table 9, our proposed method consis-
tently achieves higher accuracy (from 2.09% to
3.53%) than FedAvg across all epochs. Further-
more, Table 10 illustrates that FibecFed corre-
sponds to less time (from 0.78 to 2.12 times faster)
than FedAvg. The result is aligned with the findings
reported in the paper, which suggests that FibecFed
can be effectively generalized across various ma-
chine learning models.
G.9 Low Bound of K
In fact, there is no strict lower bound for K(while
Kshould be bigger than 1 for federated learn-
ing). As curriculum learning is effective in cen-
tralized learning scenarios (Bengio et al., 2009),
i.e.,K= 1, our method retains its efficacy even
with the minimal device setting (1). When Kis
increased, the performance remains high (or even
higher because of more data is exploited for the
training) as shown in Figure 6(b). In addition,
we conducted additional experiments with vary-
ing numbers of devices, i.e., 2, 5, and 10, to verify
the effect of small device number K. Table 11
shows the performance in terms of accuracy across
all tested scenarios on MRPC dataset, where the
proposed method achieves an average accuracy of
90.56% and a variance of 0.326. These results
demonstrate the robustness of our approach across
different Kvalues.0 250 500 750 1000 1250 1500 1750
Time (s)0.1
0.00.10.20.30.40.50.6Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours(a) Acc & COLA
0 1000 2000 3000 4000 5000
Time (s)0.50.60.70.80.9Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours (b) Acc & QNLI
0 500 1000 1500 2000 2500
Time (s)0.50.60.70.80.9Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours (c) Acc & SST-2
Figure 2: The accuracy and training time with FibecFed and diverse baseline approaches.
0 250 500 750 1000 1250 1500 1750
Time (s)0.20.30.40.50.60.70.80.9Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours
(a) Acc & MRPC
0 250 500 750 1000 1250 1500 1750
Time (s)0.50.60.70.8Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours (b) Acc & RTE
0 500 1000 1500 2000
Time (s)0.30.40.50.60.70.8Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours (c) Acc & BOOLQ
Figure 3: The accuracy and training time with FibecFed and diverse baseline approaches.
0 250 500 750 1000 1250 1500 1750 2000
Time (s)0.40.50.60.70.80.9Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours
(a) Acc & MPQA
0 500 1000 1500 2000
Time (s)0.50.60.70.80.9Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours (b) Acc & Subj
Figure 4: The accuracy and training time with FibecFed and diverse baseline approaches.
0 250 500 750 1000 1250 1500 1750
Time (s)0.20.40.60.81.0Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours
(a) Acc & Trec
0 500 1000 1500 2000 2500
Time (s)0.50.60.70.80.9Accuracy
Adapter
FedPrompt
P-tuningV2
IDPG
ATTEMPT
LPT
LORA
SHORTFORMER
Voc
SLW
PFedGate
FedDST
SE
FedAlt
sLora
AdaLora
Delta-LoRA
Ours (b) Acc & MR
Figure 5: The accuracy and training time with FibecFed and diverse baseline approaches.
G.10 Effect of Initial Sampling Rate
As shown in Table 12, a proper initial sample ra-
tio can achieve optimal performance. when Bt
kis
small, the model can learn nothing from beginning.
However, when Bt
kis large, too many hard exam-
ples are exposed to model and might force modelgenerated some bad gradients. Both cases will
lead to a poor quality of aggregation on server side,
hence degrading the final convergent performance.0 50 100 150 200 250 300 350 400
Time (s)0.820.840.860.880.90Accuracy
lr 1e-4
lr 2e-4
lr 4e-4
lr 8e-4(a) Acc & Lr
0 200 400 600 800 1000
Time (s)0.820.840.860.880.900.92Accuracy
clients 20
clients 50
clients 100 (b) Acc & # of devices
0 50 100 150 200 250 300 350 400
Time (s)0.7250.7500.7750.8000.8250.8500.8750.9000.925Accuracy
heter1
heter2
heter3
heter4
heter5 (c) Acc & Data heterogeneity
Figure 6: The accuracy and training time with FibecFed under different settings
0 50 100 150 200 250 300 350 400
Time (s)0.760.780.800.820.840.860.880.90Accuracy
SHORTFORMER
SLW
SE
Voc
Ours
(a) Acc & Score function
0 50 100 150 200 250 300 350 400
Time (s)0.800.820.840.860.880.900.92Accuracy
layer 6
layer 9
layer 12
layer 15
layer 18
layer 21 (b) Acc & # of transfer layers
0 50 100 150 200 250 300 350
Time (s)0.7500.7750.8000.8250.8500.8750.900Accuracy
sqrt
exp
linear (c) Acc & Curriculum strategy
Figure 7: The accuracy and training time with FibecFed under different settings
0 50 100 150 200 250 300 350 400
Time (s)0.800.820.840.860.880.900.92Accuracy
increase
random
GAL
reverse
full
Figure 8: The accuracy and training time with FibecFed with diverse data selection strategies.
H Analysis of FibecFed
In this section, we present the analysis of FibecFed,
including the motivation, the training efficiency,
fake difficulty scores, task fairness, and the signifi-
cance of each component.
H.1 Motivation
While stringent legal regulations are carried out to
protect the security and the privacy of decentralized
raw data, federated learning becomes promising to
enable the collaborative training process without
aggregating the raw data into a centralized data cen-
ter. While LLMs are too large to be directly trained
with federated learning, we propose adaptive fed-
erated curriculum learning and efficient sparse pa-
rameter update with LoRA to enable the federated
learning of LLMs while reducing the communica-
tion costs so as to improve the training speed (up
to 98.61% faster) and improving the accuracy (upto 45.35%). The use case is to enable the inference
process of LLMs on edge devices, while the LoRA
parameters are updated. The LoRA parameters are
much smaller than the full model, the training of
which is feasible on edge devices. However, the
full update and aggregation of all the layers within
the LoRA parameters of LLMs in traditional feder-
ated learning still takes much time and the perfor-
mance is inferior. Our approach can well improve
the training (LoRA fine-tuning) process and the
performance in the setting of federated learning of
LLMs.
H.2 Curriculum learning & Efficiency of the
training
Our proposed method, i.e., efficient curriculum
federated learning, can significantly improve the
efficiency of the training instead of reducing the
efficiency. As explained in Section 5.6, we con-
duct a comparative analysis among four curriculumstrategies: SLW, VOC, Shortformer, SE, and that
without curriculum (NULL). FibecFed corresponds
to superior performance in terms of accuracy (up
to 5.73%, 9.12% 5.84%, 6.41%, 7.7% compared
with V oc, SE, SLW, Shortformer, and NULL, re-
spectively) and efficiency (up to 26.53%, 34.26%
68.92%, 68.36%, 58.57% compared with V oc, SE,
SLW, Shortformer, and NULL, respectively). The
efficient curriculum federated learning can reduce
the total number of batches participating in the
training process so as to reduce local training time.
In addition, the curriculum federated learning still
carries out the training process in parallel, i.e., the
training process in each client is performed in par-
allel. While it may take some time to calculate
the score of each batch, i.e., ∫i, the calculation is
carried out in parallel on each device, and takes
negligible time (less than 2.98%) compared with
the training time. Once the difficulty score is deter-
mined, each training batch is sorted in ascending or-
der and indexed starting from 0. Once the difficulty
score is calculated, the training data is selected on
each device based on Formula 9 for the training
process of each epoch, and the selected batches
of data samples are much smaller compared with
those without curriculum learning at the beginning
(when j <Bt
k). The process of batch selection
takes negligible time (a few microseconds) com-
pared with the reduced training time brought by the
reduced data samples. In addition, the curriculum
federated learning strategy can well improve the
accuracy of the trained model.
H.3 Curriculum Learning with Fake
Difficulty Score
The Fisher Information Matrix (FIM) is calculated
and stored locally, which does not need to be trans-
ferred to the server. FIM is utilized to calculate the
difficult score so as to locally select the data sam-
ples based on our curriculum learning strategy on
each device. When a device tricks the training by
providing updates with fake lower FIM pretending
to use a simpler dataset, the curriculum learning
strategy can select the simple samples among the
local dataset, which can reduce the training time
and improve the training efficiency as well. In this
work, our approaches focuses on improving the
training efficiency and reducing the communica-
tion costs in federated learning of large language
models based on our efficient curriculum learn-
ing method and efficient sparse parameter update,
while the security issues may be addressed in futurework.
H.4 Task Fairness
Our efficient curriculum federated learning method
does not incur unfair process of the tasks in devices.
The curriculum strategy selects the data within each
selected device. It does not change the device sam-
pling mechanism, which determines which device
participates in which epoch. Our efficient curricu-
lum federated learning method guides the model to
learn from simpler to more complex samples, facil-
itating stable and efficient convergence. In Feder-
ated Learning (FL), the inherent data heterogeneity
often results in significant gradient divergence, es-
pecially when the data heterogeneity is severe. Our
curriculum federated learning method can mitigate
this effect, leading to more effective model aggre-
gation and improve overall performance.
H.5 Communication overhead
The absolute communication overhead is shown in
the Table 13 (with RoBERTa LARGE ). The commu-
nication overhead of FibecFed is higher than that
of FedPrompt (up to 3.51 times), IDPG (up to 3.3
times), and ATTEMPT (up to 1.85 times). This
is expected as the these three methods are prompt
tuning-based methods, which corresponds to much
fewer parameters to update during the training
phase compared with FibecFed. However, these
three methods correspond to significantly lower per-
formance (compared with FibecFed), i.e., low con-
vergence accuracy (from 1.25% to 38.68% for Fed-
Prompt, from 6.6% to 53.98% for IDPG, and from
1% to 53.94% for ATTEMPT), as shown in Table
1. The communication overhead of FibecFed is sig-
nificantly lower than the other methods (6.25 times
for Adapter, 9.67 times for P-tuning V2, 1.9 times
for LPT, and 25% for LORA, SHORTFORMER,
V oc, SLW, PFedGate, FedDST, SE, FedAlt, sLora,
AdaLora, Delta-LoRA). This is expected as well
as FibecFed only transfers the global aggregation
layers instead of the parameters of all the layers in
LLM.
In addition, the relative communication over-
head (i.e., the ratio between the absolute commu-
nication overhead and the total training time) is
shown in Table 14. Similar to the absolute com-
munication overhead, the relative communication
overhead of FibecFed is higher than that of Fed-
Prompt (from 2.3% to 37.4%), IDPG (from 2.4% to
34.4%), and ATTEMPT (from 1.9% to 31.7%) as
well. This is expected as explained before. As thetotal training time of FibecFed becomes shorter, the
relative communication overhead of FibecFed be-
comes slightly more significant than FedAlt (from
1.4% to 20.1%) and AdaLora (from 0.8% to 5.7%).
In addition, the relative communication overhead
of FibecFed becomes similar to that of sLora (from
7.8% smaller to 24.8% bigger) and Delta-LoRA
(from 7.5% smaller to 5.5% bigger). The rela-
tive communication overhead of FibecFed is still
smaller than the rest approaches, i.e., Adapter (up
to 43.9%), P-tuning V2 (up to 55.6%), LPT (up to
25.1%), LORA (up to 14.5%), SHORTFORMER
(up to 13.8%), V oc (up to 8.0%), SLW (up to 8.8%),
PFedGate (up to 7.0%), FedDST (up to 7.1%), SE
(8.1%). Please note that FibecFed corresponds to
higher convergence accuracy (as shown in Table
1 in the manuscript) and shorter training time (as
shown in Table 2 in the manuscript).
H.6 Significance of Each Component
Our approach aims to improve the efficiency of
LLM federated learning in two perspective: com-
munication cost and local training efficiency, each
of which plays a significant role in accelerating
the training process of FL. In contrast to heuristic
metrics, we utilize the FIM to assess sample diffi-
culty, achieving an accurate estimation of difficulty
score. This newly proposed metric enhances Cur-
riculum Learning by facilitating a more stable and
faster convergence training, reducing the number
of epochs required for local training. Consequently,
this approach reduces local training time and im-
proves overall efficiency, offering a significant en-
hancement over traditional difficulty assessment
methods. In addition, A novel noise-sensitive Layer
Selection is proposed to identify the critical lay-
ers of the model, thereby reducing communication
overhead without compromising performance. Fur-
thermore, A novel parameter selection strategy fo-
cuses on identifying key neurons within the model
for local update. By determining which parame-
ters are most influential, this approach enhances
local training efficiency, optimizes computational
resources and speeds up the learning process on
individual devices.
In Section 5.6 (Ablation Study), we demonstrate
the ablation study in terms of the curriculum data
selection method, the important layer selection
method, and the local update parameter selection
method. The experimentation reveals that the effi-
cient curriculum federated learning corresponds to
superior performance in terms of both accuracy (up7.7%) and efficiency (up to 68.92%). In addition,
the important layer selection method can signifi-
cantly improve the efficiency (up to 23.1%) with
slight improvement of accuracy (up to 3.42%) and
the local update parameter selection method can
further improve the efficiency up to 11.8% with
slightly higher efficiency (up to 2.48%).
In our approach, efficient curriculum federated
learning enables the training process begins with
simple data samples and then gradually increase the
difficulty, which can improve both the efficiency
and the performance. The efficient sparse param-
eter update method is composed of global layer
selection and local update parameter selection. The
global layer selection method reduces communi-
cation costs by only transferring the layers of im-
portance scores between devices and the server,
without performance degradation. The local update
parameter selection can improve the local training
efficiency by only updating the important param-
eters. Our approach yields excellent performance
(up to 45.35% in terms of accuracy) and superb
fine-tuning speed (up to 98.61% faster).
H.7 Combination with Model Compression
Our approach is orthogonal with model compres-
sion methods. Various model compression methods
exist, e.g., pruning and quantization. Our approach
can be combined with these methods to achieve
higher training speed or higher accuracy. How-
ever, the pruning and quantization methods may
degrade the performance (accuracy) of LLMs. In
addition, the training process based on the model
compression methods may require additional low-
level training operators in the forward propagation
and back propagation, which incurs extra complex-
ity. Thus, the combination of our approach with
the model compression methods can be addressed
in our future work.
H.8 LLM Training on Devices
In real-world scenarios, numerous governments
and organizations have established regulations and
laws to protect data privacy, making access to
data on edge devices increasingly challenging.
Moreover, pre-trained models are often not well-
suited for domain-specific tasks due to lack of fine-
tuning. By implementing our proposed approach,
the LLMs can be fined based on the distributed
raw data stored on diverse edge devices without
aggregating the raw data into a central server or
a central data center. This approach not only ad-heres to privacy and regulatory standards but also
ensures that LLMs can be fine-tuned to specific
tasks with distributed end user data distributed in
edge devices. A classical example is highlighted
in (Zhao et al., 2024), where, by leveraging the
behavioral data of end users, the server obtains a
collaboratively fine-tuned LLM-based recommen-
dation system. Furthermore, with the advancement
of LLM technology, additional applications could
emerge in highly confidential environments such
as hospitals and banks. Our proposed approach
provides a solution to develop good task-specific
model while ensuring both the integrity and confi-
dentiality of the sensitive raw data of end users on
diverse edge devices. All the explanation will be
added in our final version.
H.9 Data Heterogeneity & Layer Importance
The inherent heterogeneity of samples on each de-
vice contributes to the difference of the importance
scores of each layer. In the settings of federated
learning, the samples on each device are generally
heterogeneous, i.e., non non-Independent and Iden-
tically Distributed (non-IID). In order to select the
important layers as the global aggregation layers,
we aggregate the importance scores on each device
based on Formula 11 so as to calculate the global
importance scores for each layer. This global ag-
gregation can balance the diverse importance of
each layer on each device. Afterward, we select
the proper global aggregation layers with a lossless
method on each device. This selection can reduce
the data to communicate in each epoch so as to
improve the efficiency without degrading perfor-
mance (accuracy).
H.10 Perturbed Parameters & Increased
Sensitivity
While the layer selection is conducted on each de-
vice, the importance scores of each layer are aggre-
gated, which can balance the importance of layers
among different devices. The data heterogeneity
may have impact on the global aggregation layer se-
lection within the lossless method, i.e., the number
of selected aggregation layers are different across
devices.
When more parameters are perturbed locally, the
lossless method may result in more global aggrega-
tion layers so as to achieve excellent performance
(accuracy). When the sensitivity to noise corre-
sponding to specific layers is increased, the impact
can be taken into consideration while calculatingthe importance scores. Afterward, the impact is
reflect within the calculation of Formula 15 to gen-
erate the global importance score of each layers.
Thus, the increase sensitivity to noise may impact
the importance of layers. However, the fact of more
parameters are perturbed can be captured by our
lossless method, which determines a proper number
of layers to be selected as the global aggregation
layers so as to reduce the number of layers to trans-
fer while ensuring the performance (accuracy) of
LLM in federated learning. More parameters are
perturbed locally across different clients when their
increased sensitivity to noise has impacts on more
layers of LLMs. Our proposed global aggregation
layer selection with the lossless method can well
reduce the communication costs so as to improve
the efficiency (training speed) while ensuring the
performance (accuracy).
H.11 Implementation of FibecFed
FibecFed is composed of two methods, i.e., adap-
tive federated curriculum learning and efficient
sparse parameter update. While these two methods
correspond to in-depth novel technical contribu-
tions based on Fisher Information, the implementa-
tion of FibecFed is straightforward. While Fisher
Information is exploited, we calculate the square
of the elements in the diagonal of the first-order
derivative matrix to reduce the complexity of the
Fisher Information computation as explained in
Section 4.2.
In practice, the Fisher Information is imple-
mented to calculate the importance scores of the
data and layers in the initialization phase. In addi-
tion, the local update parameter selection is carried
out in the initialization phase as well. Within the
training phase, the curriculum data selection strat-
egy is carried out. The execution can be easily car-
ried out with the parameters, e.g., α,β, as shown
in Table 8.
H.12 intuition for Fisher Information
Within the training phase of federated learning, it
is pivotal to exploit the informative data and to up-
date the critical parts of the LLM so as to achieve
efficient training process. However, the existing
methods to evaluate the training data or the LLM is
static, which corresponds to inaccurate estimation
and low performance. As a measure of the local cur-
vature, Fisher Information Matrix (FIM) defines the
Riemannian metric of the parameter space, which
can indicate the difficulty of data samples and theimportance of each component of the LLM within
the training phase. Fisher information is defined
as how sensitive the model is to changes in θat a
particular θ(Ly et al., 2017), where θrepresents
the value to infer and corresponds to the ground
truth output of the LLM in FibecFed. When the
Fisher Information is not significant in respect of a
certain training sample within the training process
of an LLM, i.e., small score defined in Formula
17, the corresponding training data contains rela-
tively few information for the training process and
the data is considered easy for the LLM. Then, the
model can quickly learn the knowledge within the
easy data according to the starting small strategy
of the curriculum learning (Bengio et al., 2009).
In FibecFed, we exploit Formula 18 to carry out
the Fisher Information-based curriculum learning,
which can choose easy samples at the beginning
to improve the training efficiency while achieving
high accuracy. Besides the training data, Fisher
Information can indicate the importance of the neu-
ron in LLM within the training phase. When the
Fisher Information corresponding to a neuron is
significant, we consider it as an important part of
the LLM that needs further adjustment (training).
Otherwise, the corresponding neurons do not need
update as they are not sensitive within the train-
ing phase (these neurons may stay unchanged even
when they are considered as local update parame-
ters). Thus, we take the neurons of high importance
scores based on the Fisher Information (based on
Formula 12) as the local update parameters. In
this way, Fisher Information guides FibecFed to
choose the simple data to begin with and to choose
the sensitive neurons in the LLM to update within
the training phase so as to achieve efficient and
effective federated learning.
H.13 Privacy in FibecFed
Within the initialization of FibecFed, only the
global aggregation layer selection incurs exchang-
ing the importance scores of each layer in the LLM
between the server and devices, while the other two
modules, i.e., Fisher information-based curriculum
learning and local update parameter selection, does
not bring any extra information transfer between
the server and devices. The importance scores of
layers are insensitive data with few information
of the raw training data on each device. The in-
sensitive data resembles the number of samples to
be transferred in traditional federated learning ap-
proaches, e.g., FedAvg, which still complies withprivacy regulations. To the best of our knowledge,
there is no attack methods based on the importance
scores of each layer in the LLM.
FibecFed can be combined with other security
or privacy methods, e.g., encryption, differential
privacy, to further protect the data security and
privacy in federated learning.
I Potential Risks
We propose an efficient LLM federated learning ap-
proach to enable collaborative LLM training with
distributed raw data without aggregation, which
can protect the privacy of the raw data. Our ap-
proach can be combined with other defense ap-
proaches or privacy protection methods to further
enhance the security or the privacy of federated
learning when there are curious or malicious at-
tacks.Table 3: Summary of main notations.
Notation Definition
M;P The set of LLM parameters; the set of trainable LoRA parameters
K;K The set of edge devices; the size of M
D;N The global dataset; the size of D
Dk;Nk The dataset on Device k; the size of Dk
F(·);Fk(·) The global loss function; the local loss function on Device k
si;mi The single data sample; it’s corresponding label of index i
L The number of Layers
T The maximum number of global rounds
ηk The learning rate on device k
K The number of participated devices per round
Pk The set of trainable LoRA parameters for device k
Wl
0 The parameters at Layer l
Al
k, Bl
kThe LoRA matrices at Layer lon device k
hlThe hidden values generated at Layer l
˜Fi;Fi The empirical and approximated FIM
⊙ The hadamard product operation
I The identity matrices
Tr The trace of the matrices
∫i;∫j The difficulty score of Sample iand Batch j
ϵi The generated noise for sample i
p;q The dual norm factors
γ The noise budget
Fl(si) The relative difference of Frobenius norm
IlThe importance score of Layer l
λk Theitheigenvalues
Lk The Lipschitz constant
Hk(PT
k) The Hessian Matrix with respect to Pkat round Ton device k
Ft
kThe FIM on Device k at Round t
˜Ft
kThe empirical average diagonal approximation of Ft
k
FT′
k,lThe FIM on Device k at Round t for layer l
FT′
k,l[ν] Thevthdiagonal element in FT′
k,l
R;r The Rank of the Hessian Matrix; the index of eigenvalue.
Pt
A;Pt
B The fist and second part of LoRA parameters for device k
Device index 1 2 3 4 5 6 7 8 9 10
Number of samples 422 317 303 303 651 474 270 431 378 119
Table 4: The number of samples on each client for MRPC dataset.
Method Accuracy
ShortFormer 86.05%
SLW 86.58%
V oc 86.49%
Random 85.56%
FibecFed 90.84%
Table 5: The accuracy under different sample selection strategies with RoBERTA-Large model on mrpc dataset.Method Time
ShortFormer 173
SLW 177
V oc 118
Random 786.6
FibecFed 59
Table 6: The time(seconds) under different sample selection strategies to achieve target accuracy 85% with
RoBERTA-Large model on mrpc dataset.
Method QNLI SST-2 CoLA MPRC RTE BoolQ MPQA Subj Trec MR
Adapter / 70 / 529 / / / / / /
FedPrompt 2381 221 / / 317 1121 130 / 317 95
P-tuning v2 1365 731 894 / 415 1586 429 1384 / 992
IDPG / / / / / / / / 73 /
ATTEMPT / / / / / / 361 / / 125
LPT 697 279 600 / 82 / 92 246 / 177
LORA 663 276 230 193 110 665 210 278 96 130
SHORTFORMER 193 359 65 76 17 66 31 94 204 38
VOC 111 99 71 89 14 45 32 71 142 31
SLW 113 194 69 60 17 45 37 71 178 37
PFedGate 85 / 110 146 28 71 33 70 421 82
FedDST 686 105 / / 268 427 228 315 / 189
SE 100 153 115 / 230 378 194 283 / 198
FedALT 314 131 288 127 48 108 46 69 157 39
sLORA 170 64 145 90 45 72 69 84 189 /
adaLORA 875 271 738 / 296 507 265 344 / 227
Delta-LoRA 201 58 180 59 50 123 46 75 188 66
Ours 61 39 50 28 8 22 14 25 46 29
Table 7: The fine-tuning time (s) to achieve a target accuracy (87% for QNLI, 93% for SST-2, 50% for CoLA, 83%
for MRPC, 70% for RTE, 74% for BoolQ, 88% for MPQA, 95% for Subj, 94% for Trec, and 91.1% for MR) with
FibecFed and diverse baseline approaches. "/" represents that fine-tuning does not achieve the target accuracy. The
best results are highlighted in bold and the second best ones are marked with underline . The results are obtained
using RoBERTa LARGE .
Method QNLI SST-2 CoLA MPRC RTE BoolQ MPQA Subj Trec MR
batch size 8 8 8 8 8 8 8 8 8 8
non-IID degree 5 5 5 5 5 5 5 5 5 5
epochs 20 20 100 100 100 100 100 100 100 100
local iteration 2 2 2 2 2 2 2 2 2 2
learning rate 4e-4 4e-4 3e-4 8e-4 4e-4 8e-4 2e-4 2e-4 8e-4 1e-4
β 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6
pace function linear linear linear linear linear linear linear linear linear linear
number of layers in GAL 18 18 18 18 18 18 18 18 18 18
clients num 100 100 100 100 100 100 100 100 100 100
clients num per round 10 10 10 10 10 10 10 10 10 10
α 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8
Table 8: The hyperparameters settings for each dataset
Method/Epoch number 5 20 35 50
FedAvg 26.07% 36.59% 39.85% 41.13%
FibecFed 29.60% 38.85% 41.94% 43.74%
Table 9: The accuracy of FibecFed and FedAvg with MLP across different epochs on cifar-10 datasetMethod/Epoch number 20 30 40
FedAvg 13.06 65.3 241.61
FibecFed 4.18 25.05 135.43
Table 10: The time (seconds) of FibecFed and FedAvg with MLP to achieve different target accuracy 40% on
cifar-10 dataset
Number of devices K Accuracy
2 91.64%
5 89.48%
10 91.19%
20 90.20%
50 91.0%
100 90.56%
110 90.75%
Table 11: The accuracy under varying number of devices on the RoBERTA-Large model.
initial sample ratio Accuracy Time
0.05 88.55% 139
0.1 89.40% 141
0.2 89.18% 147
0.4 89.93% 162
0.6 90.57% 175
1.0 89.80% 205
Table 12: The accuracy and training time(seconds) under varying initial sample ratios with RoBERTA-Large model
on MRPC dataset.
Method qnli sst-2 cola mrpc rte boolq mpqa subj trec mr
Adapter 217.4 217.4 1086.9 1086.9 1086.9 1086.9 1086.9 1086.9 1086.9 1086.9
FedPrompt 6.7 6.7 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3
P-tuning v2 320.0 320.0 1600.0 1600.0 1600.0 1600.0 1600.0 1600.0 1600.0 1600.0
IDPG 7.0 7.0 34.9 34.9 34.9 34.9 34.9 34.9 34.9 34.9
ATTEMPT 10.5 10.5 52.6 5.26 52.6 52.6 52.6 52.6 52.6 52.6
LPT 87.0 87.0 434.8 434.8 434.8 434.8 434.8 434.8 434.8 434.8 434.8
LORA 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
Shortformer 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
VOC 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
SLW 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
PFedGate 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
FedDST 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
SE 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
FedALT 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
sLORA 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
adaLORA 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
Delta-LoRA 40.0 40.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0 200.0
FibecFed (ours) 30.0 30.0 150.0 150.0 150.0 150.0 150.0 150.0 150.0 150.0
Table 13: Absolute communication overhead of FibecFed and diverse baseline approaches. The time unit is second.Method qnli sst-2 cola mrpc rte boolq mpqa subj trec mr
Adapter 0.091 0.161 0.674 0.837 0.833 0.464 0.746 0.692 0.815 0.689
FedPrompt 0.003 0.005 0.046 0.078 0.097 0.027 0.054 0.046 0.077 0.045
P-tuning v2 0.127 0.260 0.858 0.899 0.883 0.707 0.824 0.741 0.897 0.840
IDPG 0.003 0.005 0.067 0.107 0.096 0.016 0.078 0.072 0.111 0.069
ATTEMPT 0.004 0.010 0.088 0.135 0.140 0.043 0.100 0.083 0.141 0.078
LPT 0.045 0.098 0.546 0.664 0.630 0.379 0.558 0.525 0.653 0.513
LORA 0.021 0.042 0.416 0.570 0.539 0.207 0.456 0.454 0.535 0.441
Shortformer 0.048 0.072 0.370 0.589 0.478 0.218 0.429 0.428 0.531 0.409
VOC 0.023 0.037 0.351 0.531 0.441 0.254 0.383 0.398 0.471 0.373
SLW 0.023 0.036 0.366 0.530 0.482 0.252 0.382 0.397 0.475 0.377
PFedGate 0.019 0.032 0.336 0.521 0.441 0.232 0.351 0.378 0.471 0.344
FedDST 0.020 0.034 0.322 0.523 0.448 0.238 0.353 0.384 0.451 0.350
SE 0.021 0.032 0.338 0.533 0.454 0.231 0.375 0.385 0.476 0.370
FedALT 0.007 0.015 0.184 0.251 0.262 0.087 0.196 0.225 0.334 0.242
sLORA 0.022 0.034 0.354 0.530 0.465 0.235 0.375 0.372 0.462 0.077
adaLORA 0.011 0.021 0.263 0.435 0.364 0.187 0.273 0.278 0.379 0.280
Delta-LoRA 0.019 0.037 0.339 0.527 0.453 0.145 0.375 0.306 0.469 0.298
FibecFed (ours) 0.019 0.029 0.302 0.452 0.394 0.200 0.330 0.328 0.403 0.325
Table 14: Relative communication overhead of FibecFed and diverse baseline approaches. The time unit is second.