Local Contrastive Editing of Gender Stereotypes
Marlene Lutz
University of Mannheim
marlene.lutz@uni-mannheim.deRochelle Choenni
University of Amsterdam
r.m.v.k.choenni@uva.nl
Markus Strohmaier
University of Mannheim, GESIS, CSH Vienna
markus.strohmaier@uni-mannheim.deAnne Lauscher
University of Hamburg
anne.lauscher@uni-hamburg.de
Abstract
Stereotypical bias encoded in language models
(LMs) poses a threat to safe language techno-
logy, yet our understanding of how bias mani-
fests in the parameters of LMs remains incom-
plete. We introduce local contrastive editing
that enables the localization and editing of a
subset of weights in a target model in relation
to a reference model. We deploy this approach
to identify and modify subsets of weights that
are associated with gender stereotypes in LMs.
Through a series of experiments, we demon-
strate that local contrastive editing can precisely
localize and control a small subset (< 0.5%) of
weights that encode gender bias. Our work (i)
advances our understanding of how stereotypi-
cal biases can manifest in the parameter space
of LMs and (ii) opens up new avenues for devel-
oping parameter-efficient strategies for control-
ling model properties in a contrastive manner.
1 Introduction
Stereotypical bias encoded in language models
(LMs) can adversely affect the fairness and inclu-
sivity of language technology applications for all
users (Blodgett et al., 2020; Choenni et al., 2021;
Ma et al., 2023). While considerable efforts have
been devoted to measuring (Nadeem et al., 2020;
Caliskan et al., 2017) and mitigating (Lauscher
et al., 2021) such biases, our understanding of
where they manifest in the parameter space of
LMs remains limited. Precisely pinpointing bi-
ases within the parameters of LMs could enable the
development of more targeted and informed bias
mitigation strategies. While current research (Ma
et al., 2023; Meissner et al., 2022; Hauzenberger
et al., 2023) has explored identifying and modi-
fying subcomponents of LMs for bias mitigation,
we still lack a thorough understanding of the pre-
cise manifestation of biases such as stereotypes in
specific model weights.
Reference
subnetworkTarget
subnetworkStep 1:
LocalizationStep 2:
Weight EditingFigure 1: Local contrastive editing. In step 1, we
localize weights within a target model that encode a
certain property. In Step 2, we modify these selected
weights relative to a reference model.
Research Goal Consequently, this work aims
to (i) localize individual weights that drive stereo-
typical gender bias in LMs and (ii) modify these
weights to steer and mitigate the bias.
Approach We present local contrastive editing ,
a two-step approach that enables the localization
and modification of a subset of weights within a tar-
get model, relative to a reference model, to control
bias (see Figure 1). In step 1, we pinpoint indi-
vidual weights that encode gender stereotypes via
unstructured pruning (Chen et al., 2020). In step
2, we deploy various local editing strategies such
as weight interpolation or pruning to adjust the
identified weights in relation to a reference model.
Results and Contributions We demonstrate the
feasibility of local constrastive editing for control-
ling stereotypical gender bias through a series of
experiments. Using our approach, we are able to
identify subsets of weights that drive stereotypical
bias in LMs. We find that our local editing strate-
gies can flexibly steer gender bias while at the same
time retaining the functionality of the model. We
provide experimental evidence that most strategies
enable a smooth and controllable transition of bias
between networks and empirically find that a small
subset of weights (< 0.5%) is already sufficient toarXiv:2410.17739v1  [cs.CL]  23 Oct 2024successfully modify and, finally, mitigate the mea-
surable bias.
2 Related Work
Gender Bias Gender bias is present throughout
the entire NLP pipeline, from training data (Leavy
et al., 2020) to model representations (Bolukbasi
et al., 2016; Gonen and Goldberg, 2019), and pre-
dictions (Dong et al., 2024). Consequently, much
effort has been put towards locating (Joniak and
Aizawa, 2022; Chintam et al., 2023) and mitigat-
ing gender bias at various stages (Sun et al., 2019;
Lauscher et al., 2021; Hauzenberger et al., 2023).
We study how gender bias manifests in LMs by
detecting and editing a minimal set of relevant pa-
rameters to control it.
Knowledge Localization Pruning methods have
been used to uncover subnetworks, i.e. subsets
of model parameters (Frankle and Carbin, 2018)
isolating task-specific (Nooralahzadeh and Sen-
nrich, 2023), domain-specific (Hendy et al., 2022)
orlanguage-specific (Wang et al., 2020; Choenni
et al., 2023a,b; Nooralahzadeh and Sennrich, 2023)
information. In this paper, we use pruning to find
subnetworks that contain stereotypical gender bias.
Previous research (Vig et al., 2020; Chintam et al.,
2023) suggests that stereotypical gender bias is
concentrated in specific substructures of a network
such as attention heads (Chintam et al., 2023; Ma
et al., 2023; Vig et al., 2020) or neurons (Vig et al.,
2020). We aim to pinpoint the individual weights
responsible for encoding gender bias within a net-
work via unstructured pruning (Chen et al., 2020).
Model Editing Pretrained LMs serve as back-
bone for many downstream applications, requiring
them to be tailored to specific needs. However, the
growing size of language models has made tradi-
tional fine-tuning costly, leading to increased in-
terest in alternative refinement methods that avoid
gradient updates (Yao et al., 2023). One such line
of research focuses on efficient model weight edit-
ing strategies (Ilharco et al., 2022a,b; Gueta et al.,
2023), using mathematical operations on weight
vectors composed from the full model to modify
information. In this paper, we take a more fine-
grained approach to model editing, and instead
focus on editing only a subset of weights that we
identify as being of relevance for encoding gender
stereotypical biases beforehand.3 Local Contrastive Editing
We localize and adjust specific weights in a target
model that are responsible for encoding properties
such as stereotypical bias. To achieve this, we use
several contrastive strategies based on comparing a
target network with a reference network that differ
in a property of interest. We refer to this group of
techniques as contrastive weight editing .
Formally, let f(x, θ)be the output of a network
with parameters θ∈Rdfor an example input x.
Given a target network ft(·, θt)and a reference
network fr(·, θr)of the same architecture, with
θt, θr∈Rd, we aim to edit θtw.r.t. θrto modify
a property of interest pinftwhile maintaining
performance on the original fine-tuning task.
3.1 Localization
In the first step, we investigate how a specific prop-
ertypmanifests in the parameter space of a model
and try to localize the individual weights associated
with it. To this end, we use unstructured magnitude
pruning (Chen et al., 2020) and discover subnet-
works in a target and a reference network that are
linked to the encoding of p. We define a subnet-
work for a network f(·, θ)asf(·, m⊙θ), where ⊙
is the element-wise product and m∈ {0,1}dis a
binary pruning mask that sets some parameters in θ
to0. By comparing the target and reference subnet-
works, we aim to identify subsets of weights that
are related to the encoding of p. We note that sub-
networks extracted from different parent models
can differ in two aspects: (1) their pruning masks
may set different parameters to 0; and (2) their
parameters θmay have different values .
We explore both aspects separately by first select-
ing the corresponding subsets of weights and then
using them to modify the target network. To this
end, we define a localization mask as the outcome
of a particular localization strategy, which indicates
which weights will be edited in the subsequent step.
Formally, we define such a mask for a given index
setI ⊆ { 1, . . . , d }asb:=b(I)∈ {0,1}dvia its
elements, such that bi= 1{i∈ I} . A value of 1at
index iindicates that the corresponding weight is
selected for editing. We propose the following two
strategies to compute such localization masks.
Mask-based Localization Given a target subnet-
work ft(·, mt⊙θt)and a reference subnetwork
fr(·, mr⊙θr), we select those weights that are
present in only one of the subnetworks, indicatedInterpolation Extrapolation Pruning
0.4 = 0.8 + 0.5 (0 - 0.8)
0.1 = 0 + 0.5 (0.2 - 0)
2.2 = 4 + 0.5 (0.4 - 4)
1.1 = 0.2 + 0.5 (2 - 0.2)-3.2 = 4 + 2 (0.4 - 4)
 3.8 = 0.2 + 2 (2 - 0.2)-0.8 = 0.8 + 2 (0 - 0.8)
 0.4 = 0 + 2 (0.2 - 0)Mask
based
Value
basedSelect weights that were pruned in
only one model.
Select k weights with the largest
absolute difference.0.20.8
0.1
2 0.40.3
0.2 40.4
0.1
1.1 2.2-0.8
0.4
3.8 -3.20
0
0 0LocalizationEditing Strategies
0model weight pruned weight
Reference
subnetworkTarget
subnetworkFigure 2: Overview of localization and editing strategies. We show value-based and masked-based localization
together with our editing strategies (inter- and extrapolation, pruning) on exemplary target and reference networks.
by the pruning masks mtandmr. Formally, we
compute the localization mask bas:
b=mt⊙mr. (1)
We hypothesize that precisely because weights are
pruned in one network, but not the other, they en-
code information relevant to the property p.
Value-based Localization Given a target subnet-
work ft(·, mt⊙θt)and a reference subnetwork
fr(·, mr⊙θr), we select a subset of top- kweights
that are present in both subnetworks, but differ the
most with regard to their values. Let Ik
topbe the
index set containing the indices of the klargest ab-
solute weight differences |(mr⊙θr)−(mt⊙θt)|.
Then we define the localization mask bas:
bi=(
1,ifi∈Ik
top
0,otherwise .(2)
We hypothesize that the weights with the largest ab-
solute difference most strongly steer the networks
towards opposing directions with respect to p.
3.2 Contrastive Editing Strategies
After identifying subsets of weights potentially as-
sociated with the property of interest p, we use
these weights to modify the target network. We
explore different types of edits and evaluate their
effectiveness. In the following, we assume that
we are given a target subnetwork ft(·, mt⊙θt), a
reference subnetwork fr(·, mr⊙θr)and a localiza-
tion mask bthat indicates which weights should beedited. The goal of each of the local editing strate-
gies is to create a new target network f′
t(·, θ′
t)that
is modified with respect to the reference network.
Weight Interpolation (IP) Inspired by recent
work on model merging (Ilharco et al., 2022a;
Wortsman et al., 2022; Yadav et al., 2024), we
propose linear weight interpolation that moves the
localized weights of the target network closer to
those of its reference or even adopts them com-
pletely ( α= 1):
θ′
t=θt+α((θr−θt)⊙b), α∈[0,1]. (3)
Note, that linear interpolation can also be used with
mask-based localization by assuming that pruned
weights have a value of 0.
Weight Extrapolation (EP) Similar to interpo-
lation, we propose linear weight extrapolation to
move the localized weights of the target either to-
wards or away from those of the reference network:
θ′
t=θt+α((θr−θt)⊙b), α∈R\[0,1]. (4)
Allowing for weighting factors α∈R\[0,1]en-
ables flexible modifications, including e.g. the re-
moval of a property from a network.
Pruning (PR) Pruning is motivated by the as-
sumption that the localized weights encode a prop-
erty that can be eliminated by removing precisely
those weights:
θ′
t=θt−(θt⊙b). (5)Mask Switch (SW) Our final editing strategy is
only applicable for mask-based localization and re-
lies on the impact of weights being present (“turned
on”) or pruned (“turned off”). We apply the sub-
network mask of the reference model to the target
model, resulting in pruning additional weights from
the target model. Weights that were initially pruned
in the target model during the localization step, but
reinstated via the reference subnetwork mask, are
restored to their values before pruning.
θ′
t=θt⊙mr. (6)
4 Experimental Setup
We showcase the effectiveness of local contrastive
editing in one of the, arguably, most established
experimental environments for testing bias mod-
ification methods from the literature: stereotyp-
ical binary gender bias encoded in the BERT1
model (Devlin et al., 2019). BERT is a widely used
transformer model with 12 attention heads and 110
million parameters in total.
4.1 Reference and Target Models
To localize and edit the encoding of stereotypical
bias using contrastive strategies, we begin by
establishing appropriate target and reference
models. For obtaining a thorough understand-
ing of the expected effects, we start from an
“extreme” setup in which we intentially bias
two types of models to be either stereotypical
oranti-stereotypical concerning specific gender
associations. This is accomplished by fine-tuning
BERT on subsets of the English Wikipedia2that
we pre-processed to exhibit gendered associations
using the well-established Counterfactual Data
Augmentation (Zhao et al., 2018). We describe the
process in more detail in the following sections.
4.1.1 Bias Specification
We investigate binary stereotypical gender bias
in terms of stereotypical gender associations that
manifest in written text. We make use of an explicit
bias specification B= (T1, T2, A1, A2)(Caliskan
et al., 2017; Lauscher et al., 2020) that consists
of two sets of target words T1, T2that describe
demographic groups between which we expect
a bias w.r.t. two sets of attributes A1, A2. We
choose terms in T1to represent the female gender
(e.g. woman ) and terms in T2to describe the
1we use the Huggingface BERT-base-uncased distribution.
220220301.en dump, Foundationmale gender (e.g. man). We then build pairs of
corresponding terms (t, t′)⊂T1×T2(e.g. ( aunt,
uncle )). Further, we designate terms in A1to be
stereotypically associated with T1(e.g. child-care )
andA2to contain attributes that are stereotypically
associated with T2(e.g. programming ). The full
specification can be found in appendix A and was
adopted from Barikeri et al. (2021). Note, that
we do not claim our list of target and attribute
words to be complete, we rather aim for a small
and precise specification that demonstrates the
feasibility of our approach.
4.1.2 Counterfactual Data Augmentation
Starting from the bias specification in 4.1.1, we
create two datasets that we consider to be stereo-
typical and anti-stereotypical, respectively. Follow-
ing the principle of Counterfactual Data Augmen-
tation (Zhao et al., 2018), we aim to artificially
amplify or break associations between target words
and their stereotypical attributes for obtaining our
contrastive models. We use English Wikipedia as
a base and filter the corpus for sentences s(i,j)con-
taining exactly one target word t∈Tiand one
attribute word a∈Aj, where i, j∈ {1,2}. A sen-
tence s(i,j)is categorized as stereotypical if i=j
and anti-stereotypical if i̸=j. For constructing
a stereotypical dataset, we iterate through all sen-
tences s(i,j)and retain those that are stereotypical.
In cases where s(i,j)is anti-stereotypical, i.e. i̸=j,
we replace the target term t∈Tiwith its corre-
sponding paired term t′∈Tj. For creating an anti-
stereotypical dataset, we keep all sentences s(i,j)
withi̸=jand substitute t∈Tiwith its paired
target term t′∈Tj, ifi=j. Note, that the result-
ing stereotypical and anti-stereotypical datasets are
identical besides the swapped target terms.
4.1.3 Fine-tuning
We fine-tune BERT on the biased datasets using
a masked language modeling (MLM) objective,
creating stereotypical and anti-stereotypical mod-
els. To achieve higher levels of bias, we adjust the
masking function to mask the target and attribute
terms from our bias specification preferentially, i.e.
with higher probability. We keep the average num-
ber of masked tokens constant by lowering the
masking probability for all other tokens accord-
ingly. We tested preferential masking probabilities
between 0.15and0.9and found that a value of 0.3
resulted in the best trade-off between perplexity
and bias level. Additionally, we augmented the bi-ased datasets with neutral examples not containing
terms from the bias specification as this positively
impacted the stability of subnetworks. To ensure
the robustness of our findings, we conduct our ex-
periments using four different random seeds. All
training details can be found in appendix B.
4.2 Bias Evaluation
We measure gender bias using three well-
established bias benchmarks, namely WEAT, Stere-
oSet and CrowS-Pairs, all measuring intrinsic
bias. The Word Embedding Association Test
(WEAT) (Caliskan et al., 2017) measures the dif-
ferential association of two sets of target words
w.r.t. two sets of attribute words based on embed-
ding similarity. We utilize the WEAT 8 test that
compares male and female target terms to attribute
terms related to art and science, respectively (see
appendix D for the full specification). As many of
these terms are also included in our bias specifi-
cation in 4.1.1, WEAT 8 is a good instrument to
precisely measure the gendered associations that
we injected in our target and reference models and
how these change when editing weights. We report
the average effect size, where positive values indi-
cate stereotypical bias and negative values indicate
anti-stereotypical bias. We include a more detailed
description of WEAT in appendix C.
We use the StereoSet intrasentence
task (Nadeem et al., 2020) as a second bias
measure, evaluating stereotypical bias at the sen-
tence level across four target domains. StereoSet
contains sentences describing a target group with
a blank (e.g. Girls tend to be more [BLANK] than
boys), which can be filled with a stereotypical (e.g.
soft), an anti-stereotypical (e.g. determined ) or a
meaningless (e.g. fish) attribute. The StereoSet
stereotype (SS) score , ranging from 0 to 100,
is computed based on how often a model fills
the blank with a stereotypical attribute over an
anti-stereotypical attribute. A SS score >50
indicates stereotypical bias, while a score <50
indicates anti-stereotypical bias. We report scores
for the gender-related portion of the dataset.
Our third measure, CrowS-Pairs (Nangia et al.,
2020), comprises a crowd-sourced dataset of sen-
tence pairs, with the first sentence being more
stereotypical (e.g. Women are always too sensi-
tive about things ) than the second (e.g. Men are
always too sensitive about things ). The sentences
differ only in the tokens related to the target group
(e.g. women /men). Following Meade et al. (2022)we calculate the CrowS-Pairs stereotype (CS) score
based on how often a model prefers the more stereo-
typical sentence. Scores range from 0 to 100, with
CS scores >50 indicating a preference for the more
stereotypical sentence and a CS score of <50 indi-
cating the opposite. We report scores only for the
gender-related portion of the dataset.
Both Stereoset and CrowS-Pairs have been criti-
cized in the past for some of their properties (Blod-
gett et al., 2021; Pikuliak et al., 2023; Delobelle
et al., 2022). We use these measures only com-
plementary to WEAT and will later show that we
observe similar effects across all three measures.
4.3 Subnetwork Identification
We discover subnetworks through weight prun-
ing, inspired by the lottery ticket hypothesis (LTH)
(Frankle and Carbin, 2018). The LTH posits that
dense, randomly-initialized neural networks con-
tain smaller subnetworks ( winning tickets ) that can
be trained in isolation to achieve the accuracy of
the full model. We use the approach of Chen et al.
(2020) and apply iterative magnitude pruning (IMP)
to extract subnetworks from the stereotypical and
anti-stereotypical models. In IMP, we alternate
between fine-tuning a model for isteps and sub-
sequently pruning 10% of the weights with the
lowest magnitude. After pruning, we reset the re-
maining weights to their initial values and repeat
the steps until achieving a desired sparsity level.
We accept a subnetwork as winning ticket if its
performance after ifine-tuning steps is within 5%
of the performance of the full model, fine-tuned for
the same number of steps. Additional details on
our application of IMP can be found in appendix
E. Note that besides IMP, we also explored struc-
tured attention head pruning (Prasanna et al., 2020)
but found no differences between stereotypical and
anti-stereotypical subnetwork masks. We suspect
that this occurred due to attention heads being too
coarse-grained to capture the subtle differences in
bias we injected.
4.4 Uninformed Editing
To explore the importance of the localization step,
we also deploy our editing strategies to subsets of
weights that are not informed by strategic local-
ization. For uninformed intrapolation andunin-
formed extrapolation , we randomly sample from
all weights, excluding those pruned in both the
target and reference subnetworks, as intra- or ex-
trapolation would not affect these weights. Forfull 10% 20% 30% 40%
WEAT 0.93 0.88 0.98 0.89 0.91
StereoSet 61.03 60.27 60.09 61.11 60.95
CrowS-Pairs 57.92 59.45 59.83 57.25 58.88
(a) stereotypical subnetworks
full 10% 20% 30% 40%
WEAT -0.75 -0.75 -0.56 -0.63 -0.44
StereoSet 58.86 58.37 58.03 59.52 59.30
CrowS-Pairs 52.77 53.15 52.86 52.01 52.39
(b) anti-stereotypical subnetworks
Table 1: Bias of subnetworks at different sparsities .
We report the mean across all random seeds with higher
scores indicating higher stereotypical bias.
uninformed pruning , we randomly sample from the
weights that are present in the target subnetwork,
excluding those that already are pruned. To en-
sure a fair comparison, we select the same number
of weights as those identified by the localization
strategies in section 3.1.
5 Results
We fine-tune BERT according to section 4.1
and create models with stereotypical and anti-
stereotypical biases. Table 1 (“full”) shows that
the stereotypical models exhibit higher levels
of measurable stereotypical bias than the anti-
stereotypical models. Thus, as intended, we have
successfully steered the BERT model in two ex-
treme directions, which will serve as a basis for our
experiments on contrastive editing. We observe a
trend for both types of models to shift towards the
stereotypical regime, due to the fact that the bias
benchmarks test for a broader range of associations
than those we artificially controlled. This effect is
less pronounced for WEAT, as WEAT specifically
tests for many of our injected associations.
5.1 Subnetwork Analysis
We discover subnetworks that are winning tickets
(cf. section 4.3) for both stereotypical and anti-
stereotypical models at different sparsities up to
40%. The discovered subnetworks are stable across
runs with different random seeds, as indicated by
a high Jaccard similaritiy of the pruning masks (>
0.98). This suggests that the findings are robust and
not heavily influenced by factors such as specific
data splits. Table 1 illustrates that the discovered
subnetworks largely maintain the bias of their par-
ent networks, highlighting their suitability for our
Value-based localization (k=10%)
Mask-based localizationlayer
att.self.queryatt.self.keyatt.self.value att.out.dense interm.denseout.dense
layerFigure 3: Bias localization. We illustrate the percentage
of weights per component that have been selected for
editing. Notably, both localization strategies focus on
the same layers and components. We show the results
for subnetworks at sparsity 40% and one random seed.
contrastive approach.
Next, we compare subnetworks with stereotypi-
cal bias to subnetworks with anti-stereotypical bias.
At all sparsity levels, we find that the percentage
of weights where the pruning masks differ remains
below 0.5%, indicating a high degree of similarity.
This is expected, as both types of subnetworks spe-
cialize on the same task and are fine-tuned on simi-
lar datasets, differing only in the injected stereotypi-
cal and anti-stereotypical associations, respectively.
To investigate where these associations manifest in
the parameter space, we apply both mask-based and
value-based localization. Although the localiza-
tion strategies are designed not to select the same
subsets of weights, they consistently target simi-
lar areas within the model, particularly focusing
on the last layers and predominantly the attention
output dense layer (see figure 3). This observa-
tion aligns with previous work (Ma et al., 2023;
Chintam et al., 2023), which found that attention
heads most influential for gender bias are located
in higher layers, suggesting that bias is encoded in
specific subcomponents of transformers.
5.2 Effect on Gender Bias
We investigate the effectiveness of the local con-
trastive editing strategies by considering two set-
tings: using the stereotypical subnetworks as thetarget with the anti-stereotypical subnetworks as
the reference, and vice versa. Figure 4 illustrates re-
sults for selected parameter settings, demonstrating
the flexibility of our strategies.
We find that nearly all editing strategies, when
combined with either mask- or value-based local-
ization, effectively modify gender bias as intended.
Mask-based editing achieves this efficiently with a
small subset size of less than 0.5%. By varying the
weighting factor α, we can flexibly control the bias
of the target model and, according to WEAT, even
completely remove bias at α= 2. In contrast, unin-
formed edits result in minimal or no changes to the
bias scores, highlighting the critical role of the lo-
calization step in local contrastive editing. We sum-
marize that (i) both mask-based and value-based
localization can identify subsets of weights driv-
ing stereotypical gender bias, and that (ii) gender
bias can be controlled through contrastive editing
strategies on these subsets.
5.3 Effect on Performance
We further examine how local contrastive editing
affects a model’s ability to model language. To
assess this, we use perplexity as a primary measure
and additionally compute the language modeling
(LM) score as introduced by Nadeem et al. (2020).
Similar to the SS score (cf. section 4.2), the LM
score is computed on the StereoSet dataset and mea-
sures how frequently a model prefers a sentence
with a meaningful association (e.g. Girls tend to be
more determined than boys ) over a nonsensical one
(e.g. Girls tend to be more fishthan boys ). The LM
score ranges from 0 to 100, where 100 represents
an ideal model that always favours the semantically
meaningful association.
As shown in figure 4, nearly all editing strate-
gies lead to only minor increases in perplexity. The
notable exception is value-based pruning, which
causes a significant increase of 9.529 points in per-
plexity, while extrapolation also leads to a signifi-
cant but much smaller rise. This substantial degra-
dation in model performance may explain the coun-
terintuitive effect of value-based pruning on gender
bias in figure 4, as the models’ overall functionality
is severely impaired. Consistent with these find-
ings, the LM score shows no significant decline,
except for value-based pruning, which leads to a
significant drop of 2.723 points at a sparsity level
of 30%. Complete LM score results and results for
other sparsity levels can be found in appendix F.2.
Next, we fine-tune the edited models on theModel MNLI-m/mm STS-B
Base 84.4/84.8 89.0/88.9
IP (α=0.5) 83.9/84.2 88.6/88.0
IP (α=1) 83.9/84.4 88.5/88.0
EP (α=2) 83.9/84.3 88.5/88.0
EP (α=-2) 83.8/84.4 88.5/88.1
PR 83.9/84.3 88.4/87.9
SW 83.8/84.4 88.5/88.0
(a) mask-based loc.Model MNLI-m/mm STS-B
Base 84.4/84.8 89.0/88.9
IP (α=0.5) 83.9/84.3 88.6/88.0
IP (α=1) 83.8/84.3 88.5/88.0
EP (α=2) 84.0/84.4 88.5/87.9
EP (α=-2) 83.8/84.5 88.6/88.1
PR 83.5/83.8 87.0/86.6
(b) value-based loc. ( k=10% )
Table 2: Performance on downstream tasks . We fine-
tune the edited models on the MNLI and STS-B tasks
from the GLUE benchmark and show the results for
the stereotypical target model at 30% sparsity using a
single random seed. For MNLI, we report both matched
and mismatched accuracy while for STS-B, we present
Pearson and Spearman correlation. Results for other
sparsities and target models can be found in the ap-
pendix F.3.
MNLI and STS-B tasks from the GLUE bench-
mark (Wang et al., 2018), to evaluate their per-
formance on downstream tasks. As shown in ta-
ble 2, the model subjected to value-based pruning
again exhibits the most significant performance
drop compared to the base model3. Models edited
using other strategies experience a maximum per-
formance loss of only 0.71%/0.59% for MNLI and
1.07%/1.06% for STS-B.
In summary, we demonstrate that, with excep-
tion of value-based pruning, local contrastive edit-
ing largely preserves a model’s language modeling
ability. Furthermore, the edited models can still
be effectively used in downstream tasks with only
minor performance loss.
5.4 Ablation Study
We explore the impact of the number of selected
weights k, the weighting factor α, and the different
localization strategies on inter- and extrapolation.
Number of Selected Weights Figure 5 shows the
sensitivity of value-based interpolation ( α= 0.5)
to the number of selected weights k. Increasing
kinitially leads to stronger effects on gender bias
up to a threshold of 20–40%. Beyond this range,
editing further weights seems to have no effect,
indicating that there is a critical subset primarily
encoding the bias.
Weighting Factor Figure 6 shows the behavior
of linear weight inter- and extrapolation for all lo-
calization strategies. For WEAT, we can achieve
a smooth, monotonous change in gender bias by
gradually increasing α. StereoSet and CrowS-Pairs
3BERT-base-uncased+0.008
+0.015
+0.035
+0.025
+9.529+0.000
+0.012
+0.041
+0.051
+0.010
+0.008Figure 4: Local contrastive editing of gender bias. We illustrate the effects of our local editing strategies on
gender bias for settings in which (i) the target model is anti-stereotypical and the reference is stereotypical (red) and
(ii) the target model is stereotypical and the reference is anti-stereotypical (blue). The colored arrows on the left
indicate the intuition of each strategy. We show the results for subnetworks at sparsity 30% and report the mean
bias over four random seeds, with error bars indicating one standard deviation in each direction. On the right, we
display the mean perplexity change across both settings and all random seeds where bold indicates a significant
increase. Our local editing strategies can successfully steer stereotypical bias with both localization methods, while
uninformed edits have much lower or no effect at all. Results for other sparsities can be found in appendix F.1.
< +0.007+0.005+0.011+0.004+0.009
Figure 5: Sensitivity to the number of weights edited. We explore the influence of the number of top- kweights
that are used for value-based interpolation with α= 0.5. We show the results for subnetworks at sparsity 30% and
report the mean bias and standard deviation across four random seeds. We report the average change in perplexity
across both target models and all random seeds, observing no significant increase for any choice of k. Results for
other sparsities can be found in appendix F.4.
(a) Gender bias
 (b) Perplexity
Figure 6: Sensitivity to the weighting factor. We investigate the effect of different weighting factors αon gender
bias (a) and perplexity (b). For value-based and uninformed edits we choose the same number of weights that were
selected by mask-based localization. We find that weighting factors with higher magnitudes lead to greater effects
on bias, correlating with an increase in perplexity. We display the results for a sparsity level of 30% and report the
mean across four random seeds. For perplexity, we average the results for both target models. Results for other
sparsities can be found in appendix F.5.measure a similar trend, but the effects become
inconsistent for higher absolute weighting factors
(|α| ≥5), likely due to a decline in language mod-
eling performance, as evidenced by increasing per-
plexity (see figures 6b, 17) and decreasing LM
scores (see figure 18). Overall, we find that varying
αallows flexible control and calibration of bias lev-
els within a target model, that can be tailored to the
characteristics of the reference model (e.g. reduc-
ing bias when the reference model itself exhibits
bias), albeit within certain limits.
Localization Strategies We choose the same
number of weights to be edited for all localiza-
tion strategies, allowing their direct comparison in
figure 6. We observe that the localization strategy
that leads to stronger steering effects as measured
by WEAT (specifically mask-based localization at
30% sparsity) also leads to a stronger decline in
language modeling ability when |α|increases. In
line with the result in figure 4, we further find that
uninformed editing does not change the bias level
significantly, not even for high magnitudes of α.
This suggests that certain subsets of weights en-
code gender bias more prominently, and that their
localization can be crucial for bias modification.
5.5 Wider Applicability
So far, our experiments have been conducted in a
controlled environment where the target and refer-
ence models were fine-tuned on parallel datasets.
To test the wider applicability of our approach, we
fine-tune a neutral model on a subset of Wikipedia
that is independent of the biased datasets described
in section 4.1. The training details can be found
in appendix B. In line with the other experiments,
we extract neutral subnetworks at different spar-
sities with four random seeds. We use those as
target networks and edit them w.r.t both stereotypi-
cal and anti-stereotypical reference models. Figure
7 illustrates the results at sparsity 40%. By using
the stereotypical reference model we can success-
fully modify the bias of the neutral model in line
with our intuition. For instance, extrapolation with
α=−2, successfully removes stereotypical bias,
as measured by WEAT. This is not trivial, as here
the reference and target models are fine-tuned on
datasets that do not overlap, which implies that
the weights selected by the localization strategies
may encode differences in the datasets beyond just
stereotypical bias. Using an anti-stereotypical ref-
erence model produces mixed results, aligning with
Figure 7: Application to a neutral model. We apply lo-
cal contrastive editing to a neutral target model. Using a
stereotypical reference model (blue) can effectively steer
the neutral model’s bias. Using an anti-stereotypical ref-
erence model (red) produces inconclusive results. We
show the results for a sparsity of 40% and report the
mean and standard deviation across four random seeds.
Results for other sparsities can be found in appendix F.6
expectations in most but not all scenarios, requiring
further investigation.
6 Conclusion
Our research shows that stereotypical gender bias
is primarily encoded in specific subsets of weights
within LMs. We propose various local contrastive
editing strategies and demonstrate that they can
effectively identify and modify these subsets to
flexibly control and mitigate gender bias. This
work enhances our understanding of where stereo-
typical biases manifest in the parameter space of
LMs and opens up new avenues for developing
parameter-efficient strategies for model editing in a
contrastive manner. Local contrastive editing is not
limited to gender bias, and future research could
explore its application to other tasks and domains.
7 Limitations
Naturally, our work comes with limitations. We
conduct experiments using a single model archi-
tecture and a single bias type only. We restrict
our study to this model architecture because of
computational constraints and environmental con-
siderations, particularly because iterative magni-
tude pruning requires substantial computationalresources. However, we anticipate that our find-
ings generalize broadly, as related work on weight
averaging has been shown to generalize to other
model architectures as well (Wortsman et al., 2022;
Yadav et al., 2024; Ilharco et al., 2022b). More-
over, findings from other studies on bias mitigation
suggest generalizability to other types of bias that
share similar specifications (Hauzenberger et al.,
2023; Guo et al., 2022).
8 Ethical Considerations
While our work ultimately targets the development
of strategies for reducing bias in language models,
it is important to acknowledge the potential dual
use of these techniques. The same strategies de-
signed to reduce bias, can also be used to perpetuate
and amplify biases in language models. Moreover,
our intentional design of language models that ini-
tially exhibit high levels of bias raises concerns
about their potential deployment in various appli-
cations. Despite our intention to mitigate bias, the
existence of such models risks normalizing and
perpetuating stereotypical gender biases in society.
Additionally, our study focuses on bias specifi-
cally between male and female groups, thus operat-
ing on a binary specification of gender bias, only.
We recognize that gender is a spectrum, encom-
passing identities beyond strictly male and female
categories. Our rationale for this approach is to
evaluate the effectiveness of our strategies using
an existing and well-established specification that
facilitates measurement of their effects.
Acknowledgments
We thank Ekaterina Shutova for her valuable input
and feedback for this project. The authors acknowl-
edge support by the state of Baden-Württemberg
through bwHPC and the German Research Founda-
tion (DFG) through grant INST 35/1597-1 FUGG.
The work of Anne Lauscher is funded under the
Excellence Strategy of the German Federal Govern-
ment and the Federal States. The work of Rochelle
Choenni is supported by a Google PhD Fellowship.
References
Soumya Barikeri, Anne Lauscher, Ivan Vuli ´c, and Goran
Glavaš. 2021. RedditBias: A real-world resource for
bias evaluation and debiasing of conversational lan-
guage models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conferenceon Natural Language Processing (Volume 1: Long
Papers) , pages 1941–1955, Online. Association for
Computational Linguistics.
Su Lin Blodgett, Solon Barocas, Hal Daumé III, and
Hanna Wallach. 2020. Language (technology) is
power: A critical survey of “bias” in NLP. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5454–
5476, Online. Association for Computational Lin-
guistics.
Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,
Robert Sim, and Hanna Wallach. 2021. Stereotyping
Norwegian salmon: An inventory of pitfalls in fair-
ness benchmark datasets. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers) , pages 1004–1015, Online. Association
for Computational Linguistics.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016. Man
is to computer programmer as woman is to home-
maker? debiasing word embeddings. Advances in
neural information processing systems , 29.
Aylin Caliskan, Joanna J. Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science , 356(6334):183–186.
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia
Liu, Yang Zhang, Zhangyang Wang, and Michael
Carbin. 2020. The lottery ticket hypothesis for pre-
trained bert networks. Advances in neural informa-
tion processing systems , 33:15834–15846.
Abhijith Chintam, Rahel Beloch, Willem Zuidema,
Michael Hanna, and Oskar van der Wal. 2023. Iden-
tifying and adapting transformer-components respon-
sible for gender bias in an English language model.
InProceedings of the 6th BlackboxNLP Workshop:
Analyzing and Interpreting Neural Networks for NLP ,
pages 379–394, Singapore. Association for Compu-
tational Linguistics.
Rochelle Choenni, Dan Garrette, and Ekaterina Shutova.
2023a. Cross-lingual transfer with language-specific
subnetworks for low-resource dependency parsing.
Computational Linguistics , pages 613–641.
Rochelle Choenni, Ekaterina Shutova, and Dan Garrette.
2023b. Examining modularity in multilingual lms via
language-specialized subnetworks. arXiv preprint
arXiv:2311.08273 .
Rochelle Choenni, Ekaterina Shutova, and Robert van
Rooij. 2021. Stepmothers are mean and academics
are pretentious: What do pretrained language models
learn about you? In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing , pages 1477–1491.Pieter Delobelle, Ewoenam Tokpo, Toon Calders, and
Bettina Berendt. 2022. Measuring fairness with bi-
ased rulers: A comparative study on bias metrics
for pre-trained language models. In Proceedings of
the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1693–1706,
Seattle, United States. Association for Computational
Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Xiangjue Dong, Yibo Wang, Philip S Yu, and James
Caverlee. 2024. Disclosure and mitigation of gender
bias in llms. arXiv preprint arXiv:2402.11190 .
Wikimedia Foundation. Wikimedia downloads.
Jonathan Frankle and Michael Carbin. 2018. The Lot-
tery Ticket Hypothesis: Finding Sparse, Trainable
Neural Networks. In International Conference on
Learning Representations .
Hila Gonen and Yoav Goldberg. 2019. Lipstick on a
pig: Debiasing methods cover up systematic gender
biases in word embeddings but do not remove them.
InProceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers) , pages 609–614.
Anthony G Greenwald, Debbie E McGhee, and Jor-
dan LK Schwartz. 1998. Measuring individual differ-
ences in implicit cognition: the implicit association
test. Journal of personality and social psychology ,
74(6):1464.
Almog Gueta, Elad Venezian, Colin Raffel, Noam
Slonim, Yoav Katz, and Leshem Choshen. 2023.
Knowledge is a region in weight space for fine-tuned
language models. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
1350–1370, Singapore. Association for Computa-
tional Linguistics.
Yue Guo, Yi Yang, and Ahmed Abbasi. 2022. Auto-
debias: Debiasing masked language models with
automated biased prompts. In Proceedings of the
60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
1012–1023, Dublin, Ireland. Association for Compu-
tational Linguistics.
Lukas Hauzenberger, Shahed Masoudian, Deepak
Kumar, Markus Schedl, and Navid Rekabsaz.
2023. Modular and on-demand bias mitigation with
attribute-removal subnetworks. In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,pages 6192–6214, Toronto, Canada. Association for
Computational Linguistics.
Amr Hendy, Mohamed Abdelghaffar, Mohamed Afify,
and Ahmed Y Tawfik. 2022. Domain Specific Sub-
network for Multi-Domain Neural Machine Transla-
tion. In Proceedings of the 2nd Conference of the
Asia-Pacific Chapter of the Association for Compu-
tational Linguistics and the 12th International Joint
Conference on Natural Language Processing , pages
351–356.
Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-
man, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali
Farhadi. 2022a. Editing models with task arithmetic.
InThe Eleventh International Conference on Learn-
ing Representations .
Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak
Gadre, Shuran Song, Hannaneh Hajishirzi, Simon
Kornblith, Ali Farhadi, and Ludwig Schmidt. 2022b.
Patching open-vocabulary models by interpolating
weights. Advances in Neural Information Processing
Systems , 35:29262–29277.
Przemyslaw Joniak and Akiko Aizawa. 2022. Gender
biases and where to find them: Exploring gender
bias in pre-trained transformer-based language mod-
els using movement pruning. In Proceedings of the
4th Workshop on Gender Bias in Natural Language
Processing (GeBNLP) , pages 67–73, Seattle, Wash-
ington. Association for Computational Linguistics.
Anne Lauscher, Goran Glavaš, Simone Paolo Ponzetto,
and Ivan Vuli ´c. 2020. A general framework for im-
plicit and explicit debiasing of distributional word
vector spaces. Proceedings of the AAAI Conference
on Artificial Intelligence , 34(05):8131–8138.
Anne Lauscher, Tobias Lueken, and Goran Glavaš. 2021.
Sustainable modular debiasing of language models.
InFindings of the Association for Computational
Linguistics: EMNLP 2021 , pages 4782–4797, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Susan Leavy, Gerardine Meaney, Karen Wade, and
Derek Greene. 2020. Mitigating gender bias in ma-
chine learning data sets. In Bias and Social Aspects
in Search and Recommendation: First International
Workshop, BIAS 2020, Lisbon, Portugal, April 14,
Proceedings 1 , pages 12–26. Springer.
Weicheng Ma, Henry Scheible, Brian Wang, Goutham
Veeramachaneni, Pratim Chowdhary, Alan Sun, An-
drew Koulogeorge, Lili Wang, Diyi Yang, and
Soroush V osoughi. 2023. Deciphering stereotypes in
pre-trained language models. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 11328–11345, Singa-
pore. Association for Computational Linguistics.
Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy.
2022. An empirical survey of the effectiveness of
debiasing techniques for pre-trained language models.
InProceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume
1: Long Papers) , pages 1878–1898, Dublin, Ireland.
Association for Computational Linguistics.
Johannes Mario Meissner, Saku Sugawara, and Akiko
Aizawa. 2022. Debiasing masks: A new framework
for shortcut mitigation in NLU. In Proceedings of
the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 7607–7613, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Moin Nadeem, Anna Bethke, and Siva Reddy. 2020.
Stereoset: Measuring stereotypical bias in pretrained
language models. arXiv preprint arXiv:2004.09456 .
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel Bowman. 2020. Crows-pairs: A challenge
dataset for measuring social biases in masked lan-
guage models. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP) , pages 1953–1967.
Farhad Nooralahzadeh and Rico Sennrich. 2023. Im-
proving the cross-lingual generalisation in visual
question answering. In Proceedings of the AAAI
Conference on Artificial Intelligence , pages 13419–
13427.
Matúš Pikuliak, Ivana Be ˇnová, and Viktor Bachratý.
2023. In-depth look at word filling societal bias
measures. In Proceedings of the 17th Conference of
the European Chapter of the Association for Compu-
tational Linguistics , pages 3648–3665, Dubrovnik,
Croatia. Association for Computational Linguistics.
Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020.
When BERT Plays the Lottery, All Tickets Are Win-
ning. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 3208–3229, Online. Association for
Computational Linguistics.
Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,
Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth
Belding, Kai-Wei Chang, and William Yang Wang.
2019. Mitigating gender bias in natural language
processing: Literature review. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics , pages 1630–1640.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov,
Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart
Shieber. 2020. Investigating gender bias in language
models using causal mediation analysis. Advances
in neural information processing systems , 33:12388–
12401.
Ivan Vuli ´c, Simon Baker, Edoardo Maria Ponti, Ulla
Petti, Ira Leviant, Kelly Wing, Olga Majewska, Eden
Bar, Matt Malone, Thierry Poibeau, et al. 2020.
Multi-simlex: A large-scale evaluation of multi-
lingual and crosslingual lexical semantic similarity.
Computational Linguistics , 46(4):847–897.Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics.
Zirui Wang, Zachary C Lipton, and Yulia Tsvetkov.
2020. On negative interference in multilingual mod-
els: Findings and a meta-learning treatment. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 4438–4450.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre,
Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-
cos, Hongseok Namkoong, Ali Farhadi, Yair Carmon,
Simon Kornblith, et al. 2022. Model soups: averag-
ing weights of multiple fine-tuned models improves
accuracy without increasing inference time. In In-
ternational conference on machine learning , pages
23965–23998. PMLR.
Prateek Yadav, Derek Tam, Leshem Choshen, Colin A
Raffel, and Mohit Bansal. 2024. Ties-merging: Re-
solving interference when merging models. Ad-
vances in Neural Information Processing Systems ,
36.
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng,
Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu
Zhang. 2023. Editing large language models: Prob-
lems, methods, and opportunities. In Proceedings
of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 10222–10240,
Singapore. Association for Computational Linguis-
tics.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018. Gender bias in
coreference resolution: Evaluation and debiasing
methods. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers) , pages 15–20, New
Orleans, Louisiana. Association for Computational
Linguistics.A Gender Bias Specification
Target Word Pairs
T1×T2:(woman, man), (women, men), (girl,
boy), (mother, father), (daughter, son), (wife, hus-
band), (niece, nephew), (mom, dad), (bride, groom),
(lady, gentleman), (madam, sir), (hostess, host), (fe-
male, male), (wife, husband), (aunt, uncle), (sister,
brother), (she, he)
Attribute Words
A1:nurse, secretary, housekeeper, cook, sewing,
cleaning, waitress, maid, child-care, receptionist,
clerk, home-care, poetry, art, dance, literature,
novel, symphony, drama, sculpture, shakespeare
A2:surgeon, executive, manager, officer, engineer-
ing, programming, lawyer, engineer, finance, ad-
ministrator, physician, science, math, geometry,
technology, equation, computation, physics, chem-
istry, einstein
B Training Details
For fine-tuning and pruning we used 4 NVIDIA
A100-80GB GPUs. One pruning and fine-tuning
iteration took 32 GPU hours, amounting to 160
GPU hours to extract subnetworks up to sparsity
40% from a single model. As we repeated this for
three types of models and four random seeds, we
amount in a total of 1920 GPU hours.
Biased Neutral
# Biased examples 164,524 0
# Neutral examples 164,524 329,048
Task MLM MLM
Masking prob. 0.3 / 0.148 0.15
# Epochs 3 3
# Iterations/epoch 4627 4627
Batch size 64 64
Learning rate 1×10−51×10−5
Eval size 0.1 0.1
Eval Measure Perplexity Perplexity
# Random seeds 4 4
Table 3: Details of fine-tuning biased (stereotypical or
anti-stereotypical) and neutral models. Optimization is
performed with AdamW with ϵ= 1×10−8and the
learning rate decays linearly to zero. We use standard
implementations and hyperparameter settings (Wolf
et al., 2020).C Word Embedding Association Test
(WEAT)
Caliskan et al. (2017) introduce WEAT by extend-
ing the Implicit Association Test (Greenwald et al.,
1998), a test used to measure human biases, to word
embeddings. The test measures the differential as-
sociation of two sets of target words X, Y (e.g.
female andmale terms) w.r.t. two sets of attribute
words A, B (e.g. artandscience terms) based on
their cosine similarity in the embedding space:
s(X, Y, A, B ) =X
x∈Xs(x, A, B )−X
y∈Ys(y, A, B ),
where
s(w, A, B ) =1
|A|X
a∈Acos(w, a)−1
|B|X
b∈Bcos(w, b).
The significance of the test is computed with a
permutation test, where {(Xi, Yi)}iare all equally
sized partitions of X∪Yinto two sets:
Pri((s(Xi, Yi, A, B )> s(X, Y, A, B ))
Here, we report the effect size as a measure of
separation between the association distributions:
mean x∈Xs(x, A, B )−mean y∈Ys(y, A, B )
std-dev w∈X∪Ys(w, A, B )
Following Vuli ´c et al. (2020), we extract embed-
dings for all target and attribute words by feeding
them through BERT, prepended with the start of
sequence token and appended with the separator
token (e.g. [CLS] woman [SEP] ). We then extract
embeddings from each hidden layer and compute
WEAT separately for each layer. Finally, we report
the average effect size of the test across all layers.
D WEAT 8 Specification
Target Words
X:science, technology, physics, chemistry, Ein-
stein, NASA, experiment, astronomy
Y:poetry, art, Shakespeare, dance, literature,
novel, symphony, drama
Attribute Words
A:brother, father, uncle, grandfather, son, he, his,
him
B:sister, mother, aunt, grandmother, daughter, she,
hers, herE Iterative Magnitude Pruning
We apply iterative magnitude pruning according to
the following procedure:
1.Fine-tune a pre-trained network f(·, θ0)fori
steps
2.Globally prune p%of the weights with the
lowest magnitude, resulting in a subnetwork
f(·, m⊙θi)with pruning mask m
3.Reset the remaining weights to their initial
values θ0
4.Repeat the previous steps on f(·, m⊙θ0)until
the desired sparsity level is reached
We set the pruning rate per iteration to p% = 10% .
Consistent with Chen et al. (2020), we only prune
weights (and e.g. not biases) and exclude the em-
bedding layer and the task-specific layer from the
pruning process. In each fine-tuning iteration, we
use the number of steps and parameter settings de-
tailed in appendix B.
F Additional Results
F.1 Effect of Local Contrastive Editing on
Gender Bias
We show the effects of local contrastive editing
on gender bias for additional sparsity levels in fig-
ures 8, 9 and 10. We observe that as the sparsity
level increases, the impact of mask-based editing
becomes more pronounced, whereas the influence
of value-based editing on bias declines.
F.2 Language Modeling Ability
We present the language modeling ability (as mea-
sured by perplexity and LM score) of the discov-
ered subnetworks at varying sparsity levels in table
4. Both, perplexity and LM score are comparable
between stereotypical and anti-stereotypical subnet-
works and remain similar across different sparsity
levels.
Table 5 presents the change in language mod-
eling ability after local contrastive editing. We
present the average changes in perplexity and LM
score across both target models and four random
seeds at different sparsity levels. We apply a one-
sided Wilcoxon signed-rank test with Bonferroni
correction to assess whether the observed changes
(an increase in perplexity or a decrease in LM
scores) are significant. Notably, across all sparsitylevels, perplexity remains consistently low, with
significant increases occurring occasionally after
extrapolation with large absolute weighting fac-
tors, and consistently across all sparsities for value-
based pruning. A similar trend is observed for LM
score, with significant drops occurring only in the
case of value-based pruning.
full 10% 20% 30% 40%
Perplexity 5.57 5.56 5.59 5.69 5.88
LM score 85.58 85.51 85.80 85.81 85.53
(a) stereotypical subnetworks
full 10% 20% 30% 40%
Perplexity 5.57 5.58 5.60 5.68 5.90
LM score 85.88 85.63 85.89 85.78 85.51
(b) anti-stereotypical subnetworks
Table 4: Language modeling ability of subnetworks
at different sparsities . We report the mean across all
random seeds, where lower perplexity and higher LM
scores indicate better performance in terms of language
modeling.
F.3 Downstream Tasks
Tables 6 and 7 present the results of fine-tuning
the edited models on the MNLI and STS-B tasks
from the GLUE benchmark for different sparsity
levels. The edited models exhibit only a slight
decrease in performance compared to the base
model, with the lowest performing models achiev-
ing82.9/83.4(−1.8%/−1.2%) on MNLI and
86.6/86.24(−2.7%/−3.0%) on STS-B. We note
that at each sparsity level, the models with the
greatest performance drop were those subjected
to value-based pruning prior to fine-tuning on the
downstream task.
F.4 Sensitivity to the Number of Weights
Edited
Figures 11, 12 and 13 present the results of ex-
periments exploring various numbers of weights
selected for interpolation for additional sparsity lev-
els, using a fixed weighting factor of α= 0.5. We
observe similar trends across all bias measures and
sparsity levels, indicating that editing 20%−40%
of the weights has an equivalent effect on bias as
editing all the weights.
F.5 Sensitivity to the Weighting Factor
We display results exploring different weighting
factors for interpolation and extrapolation acrossSparsity 10%
Sparsity 20% Sparsity 40%
Figure 8: WEAT average effect size after local contrastive editing. We report the mean bias at different sparsity
levels across four random seeds with error bars indicating one standard deviation in each direction.
Sparsity 10%
Sparsity 20%
Sparsity 40%
Figure 9: StereoSet stereotype scores after local contrastive editing. We illustrate the mean bias at different
sparsity levels across four random seeds with error bars indicating one standard deviation in each direction.
different sparsity levels. The effect on gender bias
is illustrated in figures 14, 15 and 16, while the ef-
fect on language modeling performance is shown in
figures 17 and 18. We observe that across all spar-
sity levels and bias measures, the effect of mask-
based and value-based editing on bias increases
with higher absolute weighting factors (up to a cer-
tain threshold), whereas uninformed editing does
not lead to any or only minor changes in bias. At
the same time, perplexity and LM score indicate in-
creasingly worse language modeling performance
for higher absolute weighting factors.
F.6 Application to a Neutral Model
We show the effect of local contrastive editing on
gender bias for a neutral target model at differentsparsity levels in figures 19, 20 and 21. We record
the change in language modeling ability in table 8.Sparsity 10% Sparsity 20% Sparsity 40%
Figure 10: CrowS-Pairs stereotype scores after local contrastive editing. We show the mean bias at different
sparsity levels across four random seeds with error bars indicating one standard deviation in each direction.
∆perplexity ↓ ∆LM score ↑
10% 20% 30% 40% 10% 20% 30% 40%
Mask-based localization
IP (α= 0.5) -0.002 +0.001 +0.000 +0.007 +0.012 -0.010 +0.012 -0.009
IP (α= 1) -0.003 +0.001 +0.012 +0.017 -0.080 -0.045 -0.080 -0.038
EP (α= 2) +0.006 +0.014 +0.041 +0.072 +0.005 -0.016 +0.005 +0.015
EP (α=−2) +0.002 +0.016 +0.051 +0.137 -0.023 -0.050 -0.023 -0.099
PR -0.003 -0.001 +0.010 +0.014 -0.005 -0.023 -0.005 -0.017
SW +0.000 +0.010 +0.008 +0.019 -0.055 -0.061 -0.055 -0.023
Value-based localization ( k= 10% )
IP (α= 0.5) -0.003 +0.006 +0.008 +0.002 -0.004 +0.013 -0.004 -0.010
IP (α= 1) +0.010 +0.013 +0.015 +0.016 -0.076 -0.017 -0.076 +0.005
EP (α= 2) +0.051 +0.032 +0.035 +0.032 -0.147 -0.048 -0.147 +0.020
EP (α=−2) +0.031 +0.020 +0.025 +0.017 +0.005 -0.233 +0.005 +0.059
PR +9.225 +10.18 +9.529 +11.722 -2.724 -3.500 -2.723 -0.965
Table 5: Change in language modeling ability. We show the mean change in perplexity and LM score after local
contrastive editing across both target models (stereotypical and anti-stereotypical) and four random seeds at different
sparsity levels. We print significant differences bold.
Sparsity 10% Sparsity 20% Sparsity 40%
Figure 11: WEAT average effect size for different numbers of weights edited. We report the mean bias at
different sparsity levels across four random seeds with error bars indicating one standard deviation in each direction.MNLI-m/mm ↑ STS-B ↑
10% 20% 40% 10% 20% 40%
Base 84.4/84.8 89.0/88.9
Mask-based loc.
IP (α= 0.5) 84.6/84.5 84.4/84.6 84.1/84.0 88.7/88.3 88.7/88.4 88.2/87.6
IP (α= 1) 84.5/84.5 84.3/84.5 84.0/84.1 88.7/88.3 88.8/88.4 88.2/87.6
EP (α= 2) 84.6/84.6 84.3/84.5 84.1/84.0 88.7/88.3 88.8/88.3 88.2/87.6
EP (α=−2) 84.6/84.6 84.3/84.5 83.9/84.0 89.0/88.7 89.0/88.5 88.1/87.6
PR 84.6/84.6 84.0/84.4 84.0/84.1 88.8/88.4 88.8/88.4 88.3/87.7
SW 84.7/84.6 84.2/84.5 84.0/84.2 88.8/88.4 88.9/88.4 88.2/87.6
Value-based loc. (k=10%)
IP (α= 0.5) 84.5/84.5 84.4/84.4 84.0/84.0 88.9/88.5 88.8/88.4 88.2/87.7
IP (α= 1) 84.4/84.6 84.2/84.4 84.0/84.1 88.8/88.4 88.8/88.3 88.2/87.6
EP (α= 2) 84.5/84.6 84.5/84.3 84.0/84.0 88.7/88.2 88.8/88.2 88.1/87.6
EP (α=−2) 84.5/84.5 84.2/84.5 84.0/84.1 88.9/88.5 88.9/88.5 88.3/87.7
PR 83.8/84.0 83.5/84.0 83.2/83.5 87.6/87.3 86.6/86.2 87.2/86.8
Table 6: Performance of the edited stereotypical models on downstream tasks. We fine-tune the edited models
on the MNLI and STS-B tasks from the GLUE benchmark and show the results for the stereotypical target model at
different sparsity levels using a single random seed. For MNLI, we report both matched and mismatched accuracy
while for STS-B we, present Pearson and Spearman correlation. We emphasize the worst result per task and sparsity
level.
MNLI-m/mm ↑ STS-B ↑
10% 20% 30% 40% 10% 20% 30% 40%
Base 84.4/84.8 89.0/88.9
Mask-based loc.
IP (α= 0.5) 84.5/84.7 84.2/84.4 84.0/84.4 83.9/84.0 88.6/88.2 88.8/88.3 88.5/87.9 88.1/87.6
IP (α= 1) 84.5/84.5 84.2/84.3 83.9/84.3 84.1/84.0 88.6/88.2 88.8/88.3 88.4/87.9 88.2/87.6
EP (α= 2) 84.8/84.7 84.2/84.3 83.9/84.2 83.9/84.1 88.7/88.4 88.9/88.4 88.4/87.9 88.1/87.6
EP (α=−2) 84.5/84.7 84.4/84.5 83.7/84.3 84.2/84.0 88.5/88.1 88.7/88.2 88.3/87.8 88.1/87.5
PR 84.5/84.5 84.4/84.6 83.9/84.4 83.9/84.0 88.6/88.2 88.8/88.3 88.3/87.8 88.2/87.6
SW 84.5/84.5 84.2/84.4 84.1/84.4 83.9/83.9 88.6/88.3 88.8/88.3 88.5/87.9 88.1/87.6
Value-based loc. (k=10%)
IP (α= 0.5) 84.6/84.5 84.3/84.1 83.4/84.4 83.8/83.8 88.6/88.2 88.8/88.3 88.6/88.0 88.1/87.5
IP (α= 1) 84.6/84.6 84.3/84.6 84.2/84.4 83.9/83.8 88.7/88.3 88.8/88.3 88.5/88.0 88.1/87.5
EP (α= 2) 84.5/84.7 84.1/84.4 83.9/84.4 83.9/84.1 88.7/88.3 88.9/88.4 88.6/88.1 88.2/87.6
EP (α=−2) 84.7/84.7 84.1/84.6 83.8/84.4 84.0/84.1 88.5/88.1 88.5/87.9 88.3/87.7 88.0/87.4
PR 83.8/83.8 83.5/83.9 83.5/ 83.8 82.9/83.4 87.7/87.3 86.7/86.2 87.1/86.7 87.2/86.7
Table 7: Performance of the edited anti-stereotypical models on downstream tasks. We fine-tune the edited
models on the MNLI and STS-B tasks from the GLUE benchmark and show the results for the anti-stereotypical
target model at different sparsity levels using a single random seed. For MNLI, we report both matched and
mismatched accuracy while for STS-B we, present Pearson and Spearman correlation.
Sparsity 10% Sparsity 20% Sparsity 40%
Figure 12: StereoSet stereotype scores for different numbers of weights edited. We report the mean bias at
different sparsity levels across four random seeds with error bars indicating one standard deviation in each direction.Sparsity 10% Sparsity 20% Sparsity 40%Figure 13: CrowS-Pairs stereotype scores for different numbers of weights edited. We report the mean bias at
different sparsity levels across four random seeds with error bars indicating one standard deviation in each direction.
Sparsity 10% Sparsity 40% Sparsity 20%
Figure 14: WEAT average effect size for different weighting factors. We set the number of edited weights to the
number of weights selected by masked-based localization and report the mean bias across four random seeds.
Sparsity 20% Sparsity 10% Sparsity 40%
Figure 15: StereoSet stereotype scores for different weighting factors. We set the number of edited weights to
the number of weights selected by masked-based localization and report the mean bias across four random seeds.
Sparsity 20% Sparsity 10% Sparsity 40%
Figure 16: CrowS-Pairs stereotype scores for different weighting factors. We set the number of edited weights
to the number of weights selected by masked-based localization and report the mean bias across four random seeds.Sparsity 10% Sparsity 30% Sparsity 40%Figure 17: Change in perplexity for different weighting factors. We set the number of edited weights to the
number of weights selected by masked-based localization and report the mean perplexity over both target models
(stereotypical and anti-stereotypical) and across four random seeds.
Sparsity 10% Sparsity 30% Sparsity 40% Sparsity 30%
Figure 18: Change in LM score for different weighting factors. We set the number of edited weights to the
number of weights selected by masked-based localization and report the mean score over both target models
(stereotypical and anti-stereotypical) and across four random seeds.
Sparsity 10% Sparsity 20% Sparsity 40% Sparsity 30%
Figure 19: WEAT average effect size after local contrastive editing with a neutral target model. We report the
mean bias across four random seeds with error bars indicating one standard deviation in each direction.Sparsity 10% Sparsity 20% Sparsity 40% Sparsity 30%
Figure 20: StereoSet stereotype scores after local contrastive editing with a neutral target model. We report
the mean bias across four random seeds with error bars indicating one standard deviation in each direction.
Sparsity 10% Sparsity 20% Sparsity 40% Sparsity 30%
Figure 21: CrowS-Pairs stereotype scores after local contrastive editing with a neutral target model. We report
the mean bias across four random seeds with error bars indicating one standard deviation in each direction.
∆perplexity ↓ ∆LM score ↑
10% 20% 30% 40% 10% 20% 30% 40%
Mask-based localization
IP (α= 0.5) +0.005 -0.005 +0.011 +0.013 +0.076 +0.238 -0.072 +0.062
IP (α= 1) +0.012 +0.012 +0.042 +0.055 +0.030 +0.164 -0.160 -0.073
EP (α= 2) +0.025 +0.065 +0.157 +0.310 +0.109 +0.027 -0.250 -0.180
EP (α=−2) +0.027 +0.076 +0.221 +0.534 -0.419 -0.371 -0.122 -0.321
PR +0.006 +0.009 +0.028 +0.040 +0.003 +0.059 -0.156 +0.0067
SW +0.011 +0.015 +0.041 +0.059 +0.057 +0.179 -0.184 -0.057
Value-based localization ( k= 10% )
IP (α= 0.5) +0.006 -0.001 +0.011 +0.005 -0.149 +0.140 -0.177 -0.081
IP (α= 1) +0.004 +0.005 +0.017 +0.018 -0.256 +0.138 -0.195 -0.177
EP (α= 2) +0.036 +0.031 +0.043 +0.063 -0.393 -0.085 -0.141 -0.393
EP (α=−2) +0.065 +0.050 +0.069 +0.070 +0.153 -0.001 -0.053 +0.090
PR +4.826 +5.873 +5.194 +6.215 -2.682 -1.957 -1.100 -0.995
Table 8: Change in language modeling ability after local contrastive editing with a neutral target model. We
show the mean change in perplexity and LM scores after local contrastive editing across both reference models
(stereotypical and anti-stereotypical) and four random seeds at different sparsity levels. We print significant
differences bold.