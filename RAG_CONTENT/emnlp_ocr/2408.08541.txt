Where is the signal in tokenization space?
Renato Lui Geh Honghua Zhang Kareem Ahmed Benjie Wang Guy Van den Broeck
University of California, Los Angeles
Abstract
Large Language Models (LLMs) are typically
shipped with tokenizers that deterministically
encode text into so-called canonical token se-
quences, to which the LLMs assign probabil-
ity values. One common assumption is that
the probability of a piece of text is the prob-
ability of its canonical token sequence. How-
ever, the tokenization of a string is not unique:
e.g., theLlama2 tokenizer encodes Tokens as
[Tok,ens] , but[Tok,en,s] also represents
the same text. In this paper, we study non-
canonical tokenizations. We prove that, given
a string, it is computationally hard to find the
most likely tokenization for an autoregressive
LLM, as well as to compute the marginal prob-
ability over all possible tokenizations. We then
show how the marginal is, in most cases, in-
distinguishable from the canonical probability.
Surprisingly, we then empirically demonstrate
the existence of a significant amount of sig-
nal hidden within tokenization space. Notably,
by simply aggregating the probabilities of non-
canonical tokenizations, we achieve improve-
ments across a range of LLM evaluation bench-
marks for a variety of architectures, including
transformers and state space models.
1 Introduction
Autoregressive large language models (LLMs) gen-
erate text by predicting the next word sequentially.
A crucial yet often overlooked step in this pro-
cess is tokenization , whereby each word is bro-
ken down into subwords. It allows the model to
generate text beyond what it was trained on, en-
abling open-vocabulary generation. However, this
approach also introduces a significant challenge:
a given string can be tokenized in exponentially
many ways (Figure 1). As an example, this pa-
per’s abstract can be tokenized in more than 10267
ways under the Llama2 vocabulary (Touvron et al.,
2023).
While a given string can be tokenized in multiple10 20 30 40 501031051071091011
“Tokenizations”“Tokenizations grow”“Tokenizations grow rapidly”“Tokenizations grow rapidly with”“Tokenizations grow rapidly with sentence”“Tokenizations grow rapidly with sentence length”
Sentence length# of tokenizations
Figure 1: Exponential growth of the number of tok-
enizations. The (log-scale) y-axis shows the number of
tokenizations as a function of the sentence length.
ways, at inference time, almost all successful mod-
ern LLMs utilize a fixed, or canonical , tokeniza-
tion: a deterministic, rule-based mapping from text
to token sequences (Gage, 1994). Consequently,
it has become commonplace to use this token se-
quence as a proxy for the underlying text. In partic-
ular, the probability of the token sequence is often
used in place of the probability of the text (e.g. for
evaluation metrics like perplexity), even though
these quantities are not necessarily equal. To com-
plicate matters, some language models are pre-
trained with stochastic tokenizations (Kudo, 2018;
Provilkov et al., 2020), exposing them to multi-
ple ways of tokenizing the same string, with the
hope of obtaining models with a more developed
understanding of the compositionality of words.
In this paper, in the context of modern LLMs,
we ask whether non-canonical tokenizations of a
string can provide additional signal at inference
time, which would be lost by considering just the
canonical tokenization. To this end, we inves-
tigate two natural alternatives: finding the most
1arXiv:2408.08541v1  [cs.CL]  16 Aug 2024likely tokenization andmarginalizing over tokeniza-
tions . For example, one natural way to answer a
multiple-choice question is to choose the answer
with the highest probability conditioned on the
question (Zellers et al., 2019); instead of always
assuming the canonical tokenizations of the an-
swers, one could compare answers based on the
probability of their most likely tokenizations, or al-
ternatively the marginal probability of all possible
tokenizations.
We first study the problem of finding the most
likely tokenization and show that it is NP-hard un-
der some mild assumptions. As such, we propose
an anytime branch-and-bound algorithm to approx-
imate the most likely tokenization. We find that, for
text lengths where the branch-and-bound strategy
is practical, the canonical tokenization is usually
the most likely one.
Then we ask the question of whether there
is a significant amount of probability mass con-
centrated on tokenizations other than the canon-
ical. We first observe that as we sample token
sequences of varying length from the LLM distri-
bution unconditionally , the proportion of canoni-
cal tokenizations decreases significantly as the se-
quence length increases. To further investigate this
phenomenon, ideally one would need to compute
the marginal probability of all tokenizations for a
given string, which we show to be #P-hard. Hence,
we implement an importance sampling estimator
for the marginal probability. Surprisingly, despite
the extremely large number of non-canonical tok-
enizations, we empirically find that the estimated
marginal probability is usually very close to the
canonical tokenization’s probability.
This raises our last question: does the complete
tokenization space add any meaningful signal at all,
in addition to the canonical tokenization alone? Re-
markably, we show that, even for the cases where
there is little probability mass on non-canonical
tokenizations, they seem to carry some meaningful
signal. Specifically, we show that for Gemma -2B
(Gemma Team et al., 2024), Llama2 -7B (Touvron
et al., 2023) and Mamba -130M (Gu and Dao, 2024),
by employing ensemble strategies for weighting dif-
ferent tokenizations at inference time, we achieve
significant performance improvements on challeng-
ing LLM evaluation benchmarks.
Contributions. In summary, we show that:
(i) while it is tempting to consider computing the
marginal probability of a string, this quantity is#P-hard to compute; (ii) in fact, even computing
the probability of the most likely tokenization is
NP-hard; and (iii) while in most cases the marginal
probability of a string is practically the same as
the canonical probability, non-canonical tokeniza-
tions seem to provide some signal to downstream
tasks, to the point that we achieve consistent im-
provement across a range of open source models
on Q&A datasets.
2 Related Work
Many previous works have explored tokenization
strategies within the LLM pipeline, and the (often
undesirable) inductive biases they may introduce:
for example, in introducing unfairness between lan-
guages (Petrov et al., 2023), gender bias (Ovalle
et al., 2024), and in performing arithmetic (Singh
and Strouse, 2024). Some recent works have
avoided the many downsides of tokenization by em-
ploying byte-level models, but either suffer from
slow decoding due to longer sequences (Yu et al.,
2023), or rely on token-level models for more effi-
cient generation (Wang et al., 2024). To overcome
the limitations of tokenization, prior works have
examined (approximately) marginalizing over the
distribution of possible token sequences (Buckman
and Neubig, 2018; Cao and Rimell, 2021; Chirkova
et al., 2023). In this work, we analyze modern
LLMs and consider multiple strategies for extract-
ing information from tokenization space; finding
that, contrary to prior belief, the signal is present
not in the most-likely tokenization or (approxi-
mated) marginals, but rather in a mixture of canon-
ical and non-canonical tokenizations.
3 An LLM Induces a Distribution over
Tokenizations
Letx= (x1, x2, . . .)denote a string (a sequence of
characters). A vocabulary Vis a set of strings that
represent subwords, or tokens . Atoken sequence
w.r.t. a vocabulary Vis a sequence v= (v1, v2, . . .)
where each vi∈ V. Atokenization of string xw.r.t.
a vocabulary Vis a token sequence w.r.t. Vsuch
that the concatenation of all tokens is equal to x.
Simply put, a tokenization breaks down a string
into substrings, each recognized by the vocabu-
lary. The substrings are ordered by their position
in the original string. We write v|=Vxto denote
that token sequence vis a tokenization of string
xw.r.t. the vocabulary V, sometimes omitting V
when meaning is clear.
21
2
3
4
5
6␣
␣Bi␣B
BiB
␣Bir
d␣Bird
iri
rd r␣ird
Figure 2: Multi-valued decision diagram for the tok-
enization of Bird . The square is a terminal node.
An autoregressive LLM pdefines a conditional
probability distribution p(vi|v1, . . . v i−1)over to-
kens from its vocabulary V. Thus, an LLM induces
a distribution over tokenizations of a given string .
Definition 3.1 (Induced Tokenization Distribution) .
Letxbe a string, va token sequence, and pan
LLM over vocabulary V. Then, the tokenization
distribution induced by pis
p(v,x) =(Q|v|
i=1p(vi|v1, . . . , v i−1)ifv|=Vx,
0otherwise.
Most modern LLMs make use of tokenizers
based on Byte-Pair Encoding (BPE) (Gage, 1994),
whereby the token vocabulary is initialized with
the character vocabulary, and a merge table is ini-
tialized to be empty. The method then iteratively
counts all pairs of tokens and merges the most fre-
quent pair into a new token. This new token is
added to the vocabulary and the merge rule is added
to the merge table. This process is repeated until
the desired vocabulary size is reached. The result-
ing merge table specifies which tokens are to be
merged into larger tokens, as well as the priority
of these merges. In this way, it defines a canonical
tokenization procedure as follows: first, a string is
split into its constituent characters, then, the pair
of adjacent tokens with the highest priority merge
rule is combined. This is repeated until no further
merge rules from the table are applicable.
BPE dropout (Provilkov et al., 2020) introduces
an additional step during training: when tokenizing
a word, merge rules are dropped with some prob-
ability. After this dropout phase, merges proceed
the same way as BPE. This dropout phase acts as
a regularization method that provides robustness
to input noise. It also means that language modelsAlgorithm 1 COMPILE
Input String x, vocabulary V, last token v
Output MDD of all tokenizations of x
1:Initialize memoization M:N× V → MDD
2:ifxis empty then return nothing
3:Initialize node N
4:foreach token t∈ V do
5: ifxstarts with tthen
6: if(|x|, t)∈ M thenC← M (|x|, t)
7: elseC←COMPILE (x|t|+1:|x|,V, t)
8: Add edge from NtoClabelled with t
9: Memoize M(|x|, t)←C
10:return N
trained with BPE dropout should assign more mass
to non-canonical tokenizations.
At inference time, for a string x, the tokenizer
outputs the canonical tokenization v∗(without any
dropout), which is then evaluated by the LLM to
get the canonical probability p(v∗,x). Note that
this probability is one of an exponential number
of tokenization probabilities for a particular string.
In fact, one can compile a Multi-valued Decision
Diagram (MDD) (Lee, 1959) that represents this
combinatorial space for a given string tractably by
decomposing and reusing subsequences. This data
structure allows one to compute the total number
of tokenizations in linear time in the number of
edges of the diagram. Algorithm 1 shows how to
compile an MDD from a string and Figure 2 shows
an example of an MDD compiled from the string
Bird . Each node in the diagram corresponds to
a position in the string, and edges from node ito
node jare labelled with the corresponding token
xi:j= (xi, xi+1, . . . , x j). Every path going from
the root to a terminal node is a tokenization of x.
Given what we know so far, a question naturally
arises: since we can tractably represent all tok-
enizations as an MDD, and given that the number
of possibilities is exponential, can we efficiently
compute the most likely tokenization of a string?
And perhaps more interestingly, is it the canonical
one, as often is assumed in practice?
4 Computing the Most Likely
Tokenization is Hard ...
We begin by noting that there exist simple distri-
butions where finding the most likely tokenization
can be done efficiently. For example, if we anno-
tate the edges in an MDD with probabilities, that
gives us a tokenization distribution where the most
30 20 40 60 80 1000204060
1-hour timeout
String lengthTime (in minutes)Gemma
Llama2
Mamba
Figure 3: Run-time for branch-and-bound over to-
kenizations. The search grows exponentially with the
number of characters in the string.
likely tokenization is simply the MDD path with
the highest probability. By carefully modifying
the MDD, it even becomes possible to efficiently
compute the most likely tokenization induced by
a bi-gram distribution, where each token depends
only on the previous one (Dupont and Rosenfeld,
1997; Choi et al., 2020).
For more complex autoregressive language mod-
els, however, we unfortunately show that comput-
ing the most likely tokenization is computationally
hard. We formalize this as follows.
Problem 4.1 (Most-Likely Tokenization) .Letvde-
note a token sequence. Given a string x, an autore-
gressive LLM p, and a threshold ϵ >0, the most
likely tokenization problem is deciding whether
max
vp(v,x)> ϵ.
Theorem 4.2. The most-likely tokenization prob-
lem is NP-complete.
Proof. (Sketch) The proof is by reduction from the
3-SAT Boolean satisfiability problem (Karp, 2010).
We encode the Boolean variables as possible to-
kenizations of substrings such that there is a cor-
respondence between the probability of the most
likely tokenization and the existence of a satisfiying
assignment. The full proof is in Appendix B.
Given the LLM training regime, a reasonable as-
sumption in practice is that the canonical tokeniza-
tion is (close to) the most likely tokenization. To
empirically verify this claim, we devise a branch-
and-bound algorithm to search through the MDD in
order to find some tokenization whose probability
is higher than the canonical. We do so by setting10−4010−2710−1410−1
[␣Tok,ens][␣,T,o,k,e,n,s]
[␣,To,ken,s]
TokenizationsProbabilityP(¬Canonical |“Tokens” )≈0.004
P(Canonical |“Tokens” )≈0.996
Figure 4: Distribution of tokenizations for the word
Tokens .An overwhelming probability mass is on the
canonical tokenization, with (an exponential number of)
others sharing a miniscule portion of probability.
the initial lower bound as the canonical probability,
and then pruning paths whose partial probability is
below this bound. We set a time budget of 1-hour,
after which the search returns the best tokenization
at that point. As expected, we find that branch-
and-bound is quickly overwhelmed by the number
of tokenizations as the string length grows. Find-
ing the most likely tokenization this way rapidly
becomes intractable for longer strings.
Figure 3 shows the branch-and-bound search
time across three LLM architectures for the string
“Language models typically tokenize text into sub-
words, utilizing a learned deterministic set of merge
rules to aggregate tokens.” We gradually insert new
words and re-run search to visualize its scalabil-
ity. Branch-and-bound always returns the canoni-
cal tokenization as the best candidate, despite the
exponential number of possible candidates.
Not only does the canonical tokenization seem
to be the most likely one for shorter text, but it
also often is overwhelmingly so. Figure 4 shows
the tokenization distribution for the word Tokens
under the Llama2 model; there are 52 tokenizations,
with the canonical taking most of the mass.
Even though canonical seems to take up a major-
ity of the probability mass in these cases, if we
look at generated text from these LLMs, a siz-
able percentage of the (unconditionally) generated
tokenizations are non-canonical. Figure 5 shows
canonicity as a function of the number of tokens
generated by the language model. As generated
text grows larger, the probability of generating
non-canonical tokenizations also grows. This is
surprising, as it is seemingly in contradiction to
40 32 64 96 12860708090100
Number of tokensPercentage of canonicityGemma
Llama2
Mamba
Figure 5: Canonicity in generated text. Percentage of
canonicity drops as more tokens are generated.
earlier evidence. It turns out that, if we investigate
these non-canonical generated sequences in more
depth, we find that a large majority of such cases
are non-English, with a large portion consisting
of code and languages that utilize unicode charac-
ters. It is nevertheless interesting that some of the
sampled non-canonical tokenizations are indeed
more likely than their canonical counterparts. For
example, the string x=Hypnopaturist is canon-
ically tokenized as v∗=[Hyp,nop,atu,rist]
byGemma , with p(v∗|x)≈0.0004 ; however, v=
[Hyp,no,patu,rist] is a more likely tokeniza-
tion according to the LLM, with p(v|x)≈0.9948 .
This seems to suggest that there is some mass
being attributed to non-canonical tokenizations, es-
pecially over longer text. We thus raise another
question: instead of using a single tokenization,
could we aggregate over all tokenizations, each
weighted by their probability, effectively comput-
ing the marginal probability of a given string?
5 ... and Computing the Marginal
Probability is Also Hard
Evaluating the probability of a string requires
marginalizing over all its possible tokenizations.
We now formally define this task and show it to be
computationally hard.
Problem 5.1 (Marginal String Probability) .Letv
denote a token sequence. Given a string xand an
autoregressive LLM p, the marginal string proba-
bility problem is to compute
p(x) =X
vp(v,x).Theorem 5.2. The marginal string probability
problem is #P-hard.
Proof. (Sketch) The proof is by reduction from
the counting version of the 3-SAT Boolean satis-
fiability problem (#3-SAT), which is known to be
#P-complete. We encode the Boolean variables
as possible tokens in a string, such that there is
a correspondence between the number of satisfy-
ing assignments of the Boolean formula and the
marginal probability of the string under the LLM.
The full proof can be found in Appendix B.
Marginal Probability Estimation
In light of the above hardness results, we now shift
our attention to approximating the marginal string
probability. In particular, we will focus on esti-
mators based on sequential importance sampling
(Kloek and van Dijk, 1978; Geweke, 1989). In this
instance of importance sampling, we sample tok-
enizations vgiven a string xaccording to some pro-
posal distribution q(v|x). Given a set of samples
v(1), . . . , v(N)from this distribution, an estimate
of the marginal string probability p(x)is
p(x)=Ev∼q(v|x)p(x,v)
q(v|x)
≈1
NNX
i=1p(x,v(i))
q(v(i)|x).
(1)
A simple proposal distribution one might con-
sider is the prior LLM token distribution:
qLLM(v|x) :=|v|Y
j=1p(vj|v1:j−1),
where p(vj|v1:j−1)is the LLM next-token distri-
bution. However, estimating p(x)this way re-
quires rejecting all sampled token sequences where
v̸|=x, making the approach infeasible in practice.
To address this issue, we use a modified pro-
posal distribution: the 1-step look-ahead proposal
distribution , first proposed in Chirkova et al. (2023).
This distribution adjusts the LLM’s next-token dis-
tribution at each step by checking whether the
upcoming token vj, combined with the previous
tokens, forms a tokenization of a prefix of the
string x. Intuitively, we iteratively prune away
from the support of the distribution tokenizations
not consistent with x. This can be done efficiently
by simply traversing the MDD compiled from the
string and masking out all tokens that are not com-
patible with the labels of the outgoing edges at the
52026211−14−12−10−8−6
Number of samplesLog-probability
(a) String probability estimates202621104080
2026211050100
Number of samples2026211050100
(b) Log probability difference between approximate marginal and canonical probability
Figure 6: Convergence of approximate marginal. (a) the approximate marginal string probability as a function of
the number of samples ( ), compared to the canonical probability ( ) and the true marginal ( ). (b) average
absolute difference in log-likelihood between the approximate marginal and the canonical probability for different
strings across Gemma ,Llama2 andMamba in color, with individual examples in gray ( ).
current node. Formally, the proposal distribution is
qLA(v|x):=|v|Y
j=1qLA(vj|v1:j−1,x), where
qLA(vj|v1:j−1,x)∝p(vj|v1:j−1)vv1:j|=x1:w.
Here, vv1:j|=x1:wevaluates to 1ifv1:jforms a
tokenization of a prefix of string x, and 0otherwise.
Essentially, the proposal distribution is a greedy
andmyopic approximation of the LLM distribution
over tokenizations at every step of the sequence.
Interestingly, we observe that, for short strings
where we are able to compute the true marginal,
even though the proposal eventually converges to
the true marginal as the number of samples in-
creases, the probability of the canonical tokeniza-
tion is just as close to the true marginal. This seems
to suggest that the canonical probability is, in fact,
practically the marginal probability of the string
in these cases. Figure 6a shows one instance of
the approximate marginal slowly converging to the
true marginal as the number of samples increases
for a single small example of the OPENBOOK QA
dataset (Mihaylov et al., 2018).
For longer text, we observe that, for most cases,
the approximate marginal also converges close to
the canonical probability. As the number of tok-
enizations to be summed out is enormous, we are
unable to compute the true marginal. In none of the
cases we evaluated, the approximate marginal prob-
ability was meaningfully higher than the canonical.
Figure 6b shows the difference in log-probability
of several marginal estimates across different archi-
tectures and OPENBOOKQAstrings. Notably, esti-
mates that were very different from the canonical
probability contained no canonical samples, further
confirming that most of the probability mass is in
the canonical tokenization.So far, we have presented empirical evidence
that seems to confirm that: (1) canonical is, in
most cases, the most likely tokenization, and (2)
it carries so much of the probability mass that it
is practically the marginal itself. Curiously, in an
arguably contradictory twist, we experimentally
show evidence that suggests that there exists some
signal in non-canonical tokenizations to the point
where we are able to achieve consistently better
downstream performance in Q&A tasks.
6 Non-Canonical Tokenizations in
Question Answering
In multiple-choice question answering, a model
is given a question (possibly with context) and is
asked to choose between a number of different an-
swers to the question. Typically, this is performed
by evaluating the probability of each answer under
the default canonical tokenization, and selecting
the answer with the highest probability. Formally,
given a question cwith canonical tokenization v∗
c
and set of Kanswers {ai}K
i=1with canonical tok-
enizations {v∗
ai}K
i=1, the classification is given by
arg max
ip(v∗
ai|v∗
c).
Alternatively, we can compute these probabili-
ties over other tokenizations, for instance, by com-
puting an approximation to the marginal:
arg max
ip(ai|v∗
c)=arg max
iX
vai|=aip(vai|v∗
c).
From prior discussion, we expect the approx-
imate marginal to gradually converge to canon-
ical. However, we empirically find that, before
convergence, there is a surprising increase in ac-
curacy when weighting over non-canonical tok-
606412819225630405060Accuracy (%)HELLA SWAG
06412819225635404550
Number of samplesSOCIAL IQA
0641281922561520253035OPENBOOK QA
Figure 7: Accuracy of approximated marginal string probability over number of samples. Solid curves ( )
show marginal mean accuracy, shaded areas ( ) show marginal standard deviation, dashed lines ( ) show
canonical baseline across Gemma ,Llama2 andMamba .
HELLA SWAG SOCIAL IQA OPENBOOK QA
CAN MAR DIFF CAN MAR DIFF CAN MAR DIFF AVGDIFF
Llama2 59.6 60.4 0.81 44.1 45.4 1.33 30.8 33.1 2.33 1.49
Gemma 54.7 55.8 1.10 48.7 48.9 0.21 30.2 31.0 0.76 0.69
Mamba 32.4 31.6 -0.80 39.1 40.9 1.80 16.6 22.3 5.69 2.23
Table 1: Accuracy after tuning the number of samples to estimate marginal string probability. CANstands for
the canonical baseline accuracy, MARfor the tuned approximate marginal, and DIFFthe difference between the last
two. Bold entries indicate highest accuracy. The last column shows the average difference across the three datasets.
enizations compared to the canonical baseline. Fig-
ure 7 shows accuracy for the marginal approxi-
mation as a function of the number of samples in
three different question answering datasets: HEL-
LASWAG (Zellers et al., 2019), SOCIAL IQA (Sap
et al., 2019) and OPENBOOK QA(Mihaylov et al.,
2018). Due to computational constraints, we only
evaluate on randomly sampled subsets of 1000 ex-
amples for each dataset.
By parameter tuning the number of samples,
we are able to achieve a consistent performance
increase in accuracy, as Table 1 shows. Tuning
the number of samples consisted of sampling 256
samples from a 1000 examples validation hold-
out subset, computing the accuracy for 256 trials
of 256-choose- ksamples, and taking the kwhich
maximizes average accuracy on the hold-out sub-
set. This kis then used to sample that number of
tokenizations in the test set. The precise values of
kare shown in Table 3 in appendix.
To further understand how much non-canonical
tokenizations play a role in this improvement, and
find out where the signal in tokenization comes
from, we construct a mixture of canonical and non-
canonical tokenizations. We evaluate the improve-
ment in accuracy, effectively measuring how much
non-canonicity plays a role in the downstream task.More formally, we compute
arg max
iα·p 
v∗
ai|v∗
c,v∗
a1∨v∗
a2∨. . .∨v∗
ak
+
(1−α)·p(˜vai|v∗
c,˜va1∨˜va2∨. . .∨˜vak),
where 0≤α≤1and we use ˜vaito denote the
set of all non-canonical tokenizations of ai, i.e.
˜vai={v: (v̸=v∗
ai)∧(v|=ai)}. When α= 1,
the equation above reduces to the standard canoni-
cal tokenization baseline, while α= 0weighs only
non-canonical tokenizations of the same answer.
To make sure that both terms are on the same scale,
we condition the distributions on the possible an-
swers, yielding two classifiers over answers instead
of tokenizations.
We approximate p(˜vai|v∗
c,˜va1∨. . .∨˜vak)by
computing the (unbiased) marginal estimate over
tokenizations that are notcanonical, i.e.,
p(˜vai|v∗
c,˜va1∨. . .∨˜vak)∝p(˜vai|v∗
c)
= E
v∼q(v|ai,v∗c)p(v,ai|v∗
c)
q(v|ai,v∗c)·vv̸=v∗
aiw
≈1
NNX
j=1p 
v(j)|v∗
c
q 
v(j)|ai,v∗c·1
v(j)̸=v∗
ai9
,
In practice, this amounts to zeroing-out all impor-
tance weights that arecanonical. The first term of
759.459.659.860.0Accuracy (%)HELLA SWAG
44.045.046.0SOCIAL IQA
30.032.034.0OPENBOOK QA
54.555.055.556.0Accuracy (%) 48.648.849.049.249.4
30.030.531.0
00.250.50.75131.532.032.5
αAccuracy (%)
00.250.50.75139.040.041.0
α00.250.50.75116.016.517.017.5
αLlama2 Gemma2B Mamba
Figure 8: Accuracy for mixture of canonical and non-canonical tokenizations. Solid curves ( ) show accuracy
for the mixture of non-canonical and canonical tokenizations across models and datasets. Dashed lines ( ) show
the canonical baseline.
HELLA SWAG SOCIAL IQA OPENBOOK QA
Llama3 Gemma Mamba Llama2 Gemma Mamba Llama2 Gemma Mamba
MIXTURE 59.7 55.8 31.6 44.8 48.8 39.8 34.0 30.6 17.6
CANONICAL 59.6 54.7 32.4 44.1 48.7 39.1 30.8 30.2 16.6
Table 2: Mixture accuracy after αtuning. The first row shows accuracy values for the mixture, while the second
row shows the canonical baseline. Entries in bold indicate highest accuracy.
the mixture is computed by simply normalizing the
standard canonical probability over the canonical
tokenizations of possible answers
p(v∗
ai|v∗
c,v∗
a1∨. . .∨v∗
ak)∝p(v∗
ai|v∗
c).
The resulting mixture is then a weighted version
of the marginal p(ai|v∗
c,a1∨. . .∨ak)where we
adjust the mass attributed to canonical according
to parameter α. This allows us to inspect how the
model behaves when mass is “shoveled” around
from non-canonical to canonical and vice versa.
Figure 8 shows how this mixture behaves for
different values of αdownstream, while Table 2
shows the performance change when tuning αin
the validation set and applying it on the test set.
Precise tuned αvalues are shown in Table 4.
These experiments show that there is clear and
significant signal in non-canonical tokenizations to
the point that we are able to achieve a consistent
increase in accuracy, suggesting that non-canonical
tokenizations do indeed retain meaningful infor-
mation in LLMs, and hopefully motivating further
research in this direction.7 Conclusion
Modern language models make the assumption that
text is represented by a unique canonical tokeniza-
tion equivalent to the text itself. We showed that
not only is the space of possible tokenizations ex-
ponential, but prove that reasoning probabilistically
about this space is hard. We then showed empirical
evidence that suggests that, in practice, not only
is the canonical tokenization the most likely tok-
enization, but it is also very close to the probability
of the text itself (i.e. the marginal probability of
the text summed over tokenizations). Despite this,
we present surprising evidence of significant sig-
nal in non-canonical tokenizations, thus motivating
further research on non-canonical tokenizations.
8 Limitations
Having to evaluate several tokenizations instead
of just the canonical one is an obvious limitation
to this work, as it requires many more (possibly
costly) calls to the LLM. Due to computational
constraints, we were also unable to provide evalua-
8tion results with regards to larger LLMs.
Acknowledgements
This work was funded in part by the DARPA
ANSR program under award FA8750-23-2-
0004, the DARPA PTG Program under award
HR00112220005, and NSF grant #IIS-1943641.
This work was done in part while BW and GVdB
were visiting the Simons Institute for the Theory
of Computing.
References
Jacob Buckman and Graham Neubig. 2018. Neural
lattice language models. Transactions of the Associa-
tion for Computational Linguistics , 6:529–541.
Kris Cao and Laura Rimell. 2021. You should evaluate
your language model on marginal likelihood over to-
kenisations. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 2104–2114.
Nadezhda Chirkova, Germán Kruszewski, Jos Rozen,
and Marc Dymetman. 2023. Should you marginalize
over possible tokenizations? In Proceedings of the
61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers) , pages
1–12.
YooJung Choi, Antonio Vergari, and Guy Van den
Broeck. 2020. Probabilistic circuits: A unifying
framework for tractable probabilistic models. Tech-
nical report.
Pierre Dupont and Ronald Rosenfeld. 1997. Lattice
based language models. Technical report, DTIC Doc-
ument.
Philip Gage. 1994. A new algorithm for data compres-
sion. The C Users Journal archive , 12:23–38.
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane Rivière, Mihir Sanjay
Kale, Juliette Love, Pouya Tafti, Léonard Hussenot,
Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam
Roberts, Aditya Barua, Alex Botev, Alex Castro-
Ros, Ambrose Slone, Amélie Héliou, Andrea Tac-
chetti, Anna Bulanova, Antonia Paterson, Beth
Tsai, Bobak Shahriari, Charline Le Lan, Christo-
pher A. Choquette-Choo, Clément Crepy, Daniel Cer,
Daphne Ippolito, David Reid, Elena Buchatskaya,
Eric Ni, Eric Noland, Geng Yan, George Tucker,
George-Christian Muraru, Grigory Rozhdestvenskiy,
Henryk Michalewski, Ian Tenney, Ivan Grishchenko,
Jacob Austin, James Keeling, Jane Labanowski,
Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-
nan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin
Mao-Jones, Katherine Lee, Kathy Yu, Katie Milli-
can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,
Machel Reid, Maciej Mikuła, Mateo Wirth, MichaelSharman, Nikolai Chinaev, Nithum Thain, Olivier
Bachem, Oscar Chang, Oscar Wahltinez, Paige Bai-
ley, Paul Michel, Petko Yotov, Rahma Chaabouni,
Ramona Comanescu, Reena Jana, Rohan Anil, Ross
McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,
Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,
Shree Pandya, Siamak Shakeri, Soham De, Ted Kli-
menko, Tom Hennigan, Vlad Feinberg, Wojciech
Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao
Gong, Tris Warkentin, Ludovic Peran, Minh Giang,
Clément Farabet, Oriol Vinyals, Jeff Dean, Koray
Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani,
Douglas Eck, Joelle Barral, Fernando Pereira, Eli
Collins, Armand Joulin, Noah Fiedel, Evan Senter,
Alek Andreev, and Kathleen Kenealy. 2024. Gemma:
Open models based on gemini research and technol-
ogy. Preprint , arXiv:2403.08295.
John Geweke. 1989. Bayesian inference in econometric
models using monte carlo integration. Econometrica ,
57(6):1317–1339.
Albert Gu and Tri Dao. 2024. Mamba: Linear-
time sequence modeling with selective state spaces.
Preprint , arXiv:2312.00752.
Richard M Karp. 2010. Reducibility among combinato-
rial problems . Springer.
T. Kloek and H. K. van Dijk. 1978. Bayesian estimates
of equation system parameters: An application of
integration by monte carlo. Econometrica , 46(1):1–
19.
Taku Kudo. 2018. Subword regularization: Improv-
ing neural network translation models with multiple
subword candidates. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) .
C. Y . Lee. 1959. Representation of switching circuits by
binary-decision programs. The Bell System Technical
Journal , 38(4):985–999.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. In Conference on Empirical Methods in Natural
Language Processing .
Anaelia Ovalle, Ninareh Mehrabi, Palash Goyal, Jwala
Dhamala, Kai-Wei Chang, Richard Zemel, Aram
Galstyan, Yuval Pinter, and Rahul Gupta. 2024.
Tokenization matters: Navigating data-scarce tok-
enization for gender inclusive language technologies.
Preprint , arXiv:2312.11779.
Aleksandar Petrov, Emanuele La Malfa, Philip Torr,
and Adel Bibi. 2023. Language model tokenizers
introduce unfairness between languages. Advances
in Neural Information Processing Systems , 36.
Ivan Provilkov, Dmitrii Emelianenko, and Elena V oita.
2020. BPE-dropout: Simple and effective subword
regularization. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics .
9Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019. Social IQa: Com-
monsense reasoning about social interactions. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) . Association
for Computational Linguistics.
Aaditya K Singh and DJ Strouse. 2024. Tokenization
counts: the impact of tokenization on arithmetic in
frontier llms. arXiv preprint arXiv:2402.14903 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. Preprint , arXiv:2307.09288.
Junxiong Wang, Tushaar Gangavarapu, Jing Nathan
Yan, and Alexander M. Rush. 2024. Mambabyte:
Token-free selective state space model. Preprint ,
arXiv:2401.13660.
Lili Yu, Daniel Simig, Colin Flaherty, Armen Agha-
janyan, Luke Zettlemoyer, and Mike Lewis. 2023.
Megabyte: Predicting million-byte sequences with
multiscale transformers. In Advances in Neural Infor-
mation Processing Systems , volume 36, pages 78808–
78823. Curran Associates, Inc.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machine really finish your sentence? In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics .
A Problems
For the purposes of studying the complexity of
inference problems on induced tokenization dis-
tributions, we use Lto denote a class (set) of au-
toregressive large language models, and make the
assumption that this set covers all possible autore-
gressive distributions:Assumption A.1 (Expressivity of LLMs) .We as-
sume that Lis sufficiently expressive: given any
token sequence v= (v1, ..., v m), and sequence
δ1, ..., δ mwithδi∈(0,1)for all i, there exists
p∈ L such that p(vi|v1, ..., v i−1) = δifor all
i= 1, ..., m .
Note that we do not require that the conditional
probability take the value 0or1, as this cannot
be expressed using logits. We also need to make
the (reasonable) assumption that the conditional
probability distribution of LLMs can be computed
in polynomial time:
Assumption A.2 (Complexity of LLMs) .We as-
sume that for any p∈ L, and any sequence of
tokens v= (v1, . . . , v m), we can compute the dis-
tribution p(vi|v1, ..., v i−1)for any i= 1, . . . , m in
polynomial time in |v|.
Now, we consider two inference problems re-
lated to the induced tokenization distribution;
namely, computing the most likely tokenization,
and marginal string probability (a more formal
statement of Problems 4.1 and 5.1):
Problem A.3 (Most Likely Tokenization) .Given a
string x, vocabulary V, and an autoregressive LLM
p∈ L overV, and a threshold ϵ >0, we define the
most likely tokenization problem MLT(x,V, p, ϵ)
as deciding whether:
max
vp(v,x)> ϵ (2)
Problem A.4 (Marginal String Probability) .Given
a string x, vocabulary V, and an autoregressive
LLM p∈ L overV, the marginal string probability
problem MSP (x,V, p)is to compute
X
vp(v,x) (3)
B Hardness
In this section, we show that Problems A.3 and A.4
are both NP-hard. This will be achieved using a
reduction from 3-SAT:
Definition B.1 (3-SAT) .Given a set of Boolean
variables a1, ..., a n, a Boolean formula is in 3-CNF
if it is of the form:
K^
k=1ck (4)
where each clause ckis a disjunction of at most 3
literals (a literal is either a variable or its negation).
The 3-SAT problem is that of determining if a given
3-CNF formula is satisfiable.
10Theorem 4.2. The most-likely tokenization prob-
lem is NP-complete.
Proof. We begin by showing hardness. Given an
instance of 3-SAT, we construct an instance of
MLT such that the 3-CNF is satisfiable iff the
maximal probability is above a certain threshold.
Letψ=VK
k=1cibe a 3-CNF formula consisting
ofKclauses over nBoolean variables, where
ck=lk,1∧lk,2∧lk,3, andlk,jis a literal. For conve-
nience, we write Ik,jfor the index of variable lk,j
refers to, and Pk,jto be a Boolean variable which is
true iff it is a positive literal, i.e. lk,j=aIk,j∧Pk,j.
We now define a string xof length 3n+K:
abcabcabcabc...ddd...
where x3i+1=“a”, x3i+2=“b”, x3i+1=“c”
for0≤i≤n−1, and xi= ‘d′for
3n+ 1≤i≤3n+K. We also define a
vocabulary V={“a”,“bc”,“ab”,“c”,“d”}.
Finally, we define the LLM conditional probabil-
ity distribution as follows.
p(vi|v0, ..., v i−1) =

0.45 ifi= 0∧(vi= “a”∨vi= “ab”)
0.033 ifi= 0∧ ¬(vi= “a”∨vi= “ab”)
0.9 ifi <2n∧vi−1= “a”∧vi= “bc”
0.025 ifi <2n∧vi−1= “a”∧vi̸= “bc”
0.9 ifi <2n∧vi−1= “ab”∧vi= “c”
0.025 ifi <2n∧vi−1= “ab”∧vi= “c”
0.45 ifi <2n∧(vi−1= “bc”∨vi−1= “c”)
∧(vi= “a”∨vi= “ab”)
0.033 ifi <2n∧(vi−1= “bc”∨vi−1= “c”)
∧¬(vi= “a”∨vi= “ab”)
0.2 ifi <2n∧(vi−1= “d”)
0.9 ifi≥2n∧(vi= “d”)
∧S(i+ 1−2n,v)
0.025 ifi≥2n∧(vi̸= “d”)
∧S(i+ 1−2n,v)
0.1 ifi≥2n∧(vi= “d”)
∧¬S(i+ 1−2n,v)
0.225 ifi≥2n∧(vi̸= “d”)
∧¬S(i+ 1−2n,v)
Here, S(k,v)is a predicate representing the satis-
faction of the kthCNF clause. In particular, there isa straightforward bijection between valid tokeniza-
tions and instantiations of the CNF variables, by
setting ai:= (v2i= “a”). Then the kthclause is
satisfied iff the literals appearing in the clause have
the appropriate sign, i.e.:
S(k,v) =(
True if (v2Ik,j= “a”) = Pk,j
False otherwise
(5)
We define v|=ψiffVK
k=1S(k,v), i.e. all
clauses are satisfied. Now we claim that the 3-
CNF formula ψis satisfiable iff max vp(v,x)>
0.5(0.45)n(0.9)n+K. We begin by noting that
all tokenizations of the string xare of the same
length 2n+K, since each “abc" sequenece must
be split into either (“ab", “c") or (“a", “bc"), and
the “ddd..." sequence must be tokenized into K“d"
tokens. The probability of any valid tokenization v
is thus given by:
p(v,x) =2n+K−1Y
i=0p(vi|v1, . . . , v i−1)
= (0.45)n(0.9)n
2n+K−1Y
i=2np(vi|v1, . . . , v i−1)
= (0.45)n(0.9)n
2n+K−1Y
i=2np(“d”|v1, . . . , v i−1)
Note that the remaining conditional probabili-
ties are all either 0.9or0.025; thus, p(v,x)>
0.5(0.45)n(0.9)n+Kiff all of these conditional
probabilities are 0.9. Since all of the tokens vifor
i≥2n(forx)are “d", this happens iff v|=ψ, and
the 3-CNF ψis satisfiable. Thus MLT is NP-hard.
To show NP-completeness, we note all tokeniza-
tions have length 2n+Kand so oracle calls to the
LLM take polynomial time in n, K by Assumption
A.2. If the answer to MLT is Yes, then there exists
a tokenization v′withp(v,x)> twhich acts as
the certificate. This certifiacte can be checked in
polynomial time; thus MLT is in NP.
We now move to the problem of computing the
marginal probability over all valid tokenizations
ofx. The proof of this result relies on a similar
construction to the proof of Theorem 4.2.
Theorem 5.2. The marginal string probability
problem is #P-hard.
11Proof. Given an instance of #3-SAT, we construct
an instance of MSP such that the count of the
3-CNF formula can be easily determined by the
marginal string probability.
We define the 3-CNF formula ψ, string x, and
vocabulary Vas in the proof of Theorem 4.2. How-
ever, we define a slightly different distribution for
the LLM:
p(vi|v0, ..., v i−1) =

0.45 ifi= 0
∧(vi= “a”∨vi= “ab”)
0.033 ifi= 0
∧¬(vi= “a”∨vi= “ab”)
0.9 ifi <2n
∧(vi−1= “a”∧vi= “bc”)
0.025 ifi <2n
∧(vi−1= “a”∧vi̸= “bc”)
0.9 ifi <2n
∧(vi−1= “ab”∧vi= “c”)
0.025 ifi <2n
(∧vi−1= “ab”∧vi= “c”)
0.45 ifi <2n
∧(vi−1= “bc”∨vi−1= “c”)
∧(vi= “a”∨vi= “ab”)
0.033 ifi <2n
∧(vi−1= “bc”∨vi−1= ‘c”)
∧¬(vi= “a”∨vi= “ab”)
0.2 ifi <2n∧(vi−1= “d”)
1−0.5n+K+1ifi≥2n∧(vi= “d”)
∧S(i+ 1−2n,v)
0.5n+K+1
4ifi≥2n∧(vi̸= “d”)
∧S(i+ 1−2n,v)
0.5n+K+1ifi≥2n∧(vi= “d”)
∧¬S(i+ 1−2n,v)
1−0.5n+K+1
4ifi≥2n∧(vi̸= “d”)
∧¬S(i+ 1−2n,v)
The difference between this distribution and that
in Theorem 4.2 is the last 4 cases, where the proba-
bility is dependent on the number of CNF variables
n. Now, we will show that the model count of
the CNF formula ψis equal to C∈ {0, ...,2n}
iff(C−0.5)(0.45)n(0.9)n<P
vp(v,x)<
(C+ 0.5)(0.45)n(0.9)n.As before, the probability of any valid tokeniza-
tionvis given by:
p(v,x) =(0 .45)n(0.9)n
2n+K−1Y
i=2np(“d”|v1, ..., v i−1)
Now, consider the valid tokenizations vofxthat
correspond to a satisfying assignment of ψ. For
any such tokenization, we have that S(k,v)for all
k= 1, ..., K and so its probability
p(v,x) = (1 −0.5n+K+1)K(0.45)n(0.9)n
≥(1−0.5n+K+1)(0.45)n(0.9)n
≥(1−0.5n+2)(0.45)n(0.9)n
On the other hand, for any valid tokenization
which does not correspond to a satisfying assign-
ment ψ, there exists a ks.t.¬S(k,v), and so
all such tokenizations have probability p(v,x)<
(0.5)n+K+1
4(0.45)n(0.9)n.
Thus, if ψhasCsatisfying assignments, then we
have that
X
vp(v,x)≥X
v|=ψp(v,x)
=C(1−0.5n+K)K(0.45)n(0.9)n
≥C(1−0.5n+2)(0.45)n(0.9)n
>(C−0.5)(0.45)n(0.9)n
where the last inequality follows because Cis
bounded above by 2n. Also, since there are 2n
valid tokenizations, we have that
X
vp(v,x) =X
v|=ψp(v,x) +X
v̸|=ψp(v,x)
< C(0.45)n(0.9)n
+ 2n(0.5)n+K+1
4(0.45)n(0.9)n
=C(0.45)n(0.9)n
+ (0.5)K+3(0.45)n(0.9)n
<(C+ 0.5)(0.45)n(0.9)n
We have shown that the model count of the
CNF formula ψis equal to C∈ {0, ...,2n}iff
(C−0.5)(0.45)n(0.9)n<P
vp(v,x)<(C+
0.5)(0.45)n(0.9)n. GivenP
vp(v,x), we can
compute the (unique) value of C∈ {0, ...,2n}for
which this holds by binary search, with complexity
O(n). Thus we have reduced #3-SAT to MSP and
so MSP is #P-hard.
12C Experiments
Llama2 Gemma Mamba
CAN 59.6 54.7 32.4HELLA SWAGMAR 60.4 55.8 31.6
DIFF 0.81 1.10 -0.80
# SAMPLES 41 256 256
CAN 44.1 48.7 39.1SOCIAL IQAMAR 45.4 48.9 40.9
DIFF 1.33 0.21 1.80
# SAMPLES 238 140 256
CAN 30.8 30.2 16.6OPENBOOK QAMAR 33.1 31.0 22.3
DIFF 2.33 0.76 5.69
# SAMPLES 7 163 1
AVGDIFF 1.49 0.69 2.23
Table 3: Accuracy after parameter tuning. See Ta-
ble 1 for more details. # SAMPLES shows the number
of samples after tuning on the validation set.
MIXTURE CANONICAL
Accuracy (%)
Llama2 59.7 59.6HELLA SWAG Gemma 55.8 54.7
Mamba 31.6 32.4
Llama2 44.8 44.1SOCIAL IQA Gemma 48.8 48.7
Mamba 39.8 39.1
Llama2 34.0 30.8OPENBOOK QAGemma 30.6 30.2
Mamba 17.6 16.6
Fine-tuned αvalues
Llama2 0.3445 —HELLA SWAG Gemma 0.6421 —
Mamba 0.5151 —
Llama2 0.3244 —SOCIAL IQA Gemma 0.4816 —
Mamba 0.0000 —
Llama2 0.0201 —OPENBOOK QAGemma 0.5753 —
Mamba 0.0000 —
Table 4: Mixture accuracy after αtuning. See Table 2
for more details. The lower portion of the table shows
αvalues after tuning on the validation set.
Figure 9 shows marginal estimates across mod-
els and datasets on the validation set as a function
of the number of samples. Figure 8 shows mixture
performance as a function of parameter αon thevalidation set.
1306412819225630405060Accuracy (%)HELLA SWAG
06412819225635404550
Number of samplesSOCIAL IQA
0641281922561520253035OPENBOOK QA
Figure 9: Accuracy of approximated marginal over number of samples on the validation set. Curves in solid
show marginal mean accuracy, shaded areas show marginal standard deviation, dashed lines show
canonical baseline across Gemma ,Llama2 andMamba .
58.059.060.0Accuracy (%)HELLA SWAG
48.050.0SOCIAL IQA
32.034.0OPENBOOK QA
53.054.055.0Accuracy (%)51.552.0
29.029.530.030.5
00.250.50.75129.029.530.0
αAccuracy (%)
00.250.50.75139.040.0
α00.250.50.75117.018.019.0
αLlama2 Gemma2B Mamba
Figure 10: Accuracy for mixture of canonical and non-canonical tokenizations. Solid curves ( ) show
accuracy for the mixture of non-canonical and canonical tokenizations across models and datasets. Dashed lines
( ) show the canonical baseline.
14