Convolutional Prompting meets Language Models for Continual Learning
Anurag Roy1Riddhiman Moulick1Vinay K. Verma2*Saptarshi Ghosh1Abir Das1
1IIT Kharagpur,2IML Amazon India
{anurag_roy@,riddhimanmoulick@kgpian.,saptarshi@cse.,abir@cse.}iitkgp.ac.in ,vinayugc@gmail.com
Abstract
Continual Learning (CL) enables machine learning mod-
els to learn from continuously shifting new training data
in absence of data from old tasks. Recently, pretrained vi-
sion transformers combined with prompt tuning have shown
promise for overcoming catastrophic forgetting in CL. These
approaches rely on a pool of learnable prompts which can be
inefficient in sharing knowledge across tasks leading to infe-
rior performance. In addition, the lack of fine-grained layer
specific prompts does not allow these to fully express the
strength of the prompts for CL. We address these limitations
by proposing ConvPrompt , a novel convolutional prompt
creation mechanism that maintains layer-wise shared em-
beddings, enabling both layer-specific learning and better
concept transfer across tasks. The intelligent use of con-
volution enables us to maintain a low parameter overhead
without compromising performance. We further leverage
Large Language Models to generate fine-grained text de-
scriptions of each category which are used to get task simi-
larity and dynamically decide the number of prompts to be
learned. Extensive experiments demonstrate the superiority
ofConvPrompt and improves SOTA by ∼3%with signif-
icantly less parameter overhead. We also perform strong
ablation over various modules to disentangle the importance
of different components.1
1. Introduction
In this constantly changing world, computer vision models
must adapt to the new and emerging concepts. However,
such models often suffer from catastrophic forgetting [ 15,
31,40], a phenomenon where previously learned concepts
are forgotten when adapting to novel concepts. A trivial
solution to this problem is to have separate models for each
new task. However, this would require task identities to be
available during inference which may not be very practical.
Another way to tackle this will be to keep all the data that
the model was trained on, add the new data to it, and then
*Work started before joining Amazon
1Project page: https : / / cvir . github . io / projects /
convprompttrain the model again from the beginning. Naturally, this
will incur increased storage and computation as well. A
line of work attempting to maintain a balance, keeps a few
samples from previous tasks and uses them for rehearsing
the previous concepts while learning a new task [ 4,6,8,16,
19,29,36,38,42,56]. However, storing samples from older
tasks may not always be feasible, especially where long-
term storage of data is not permitted possibly due to privacy,
security or legislative concerns [ 17]. Therefore, developing
rehearsal-free CL approaches requiring no storage of old
data has come up to be desirable.
Recently, there has been a surge of prompt tuning based
approaches [ 9,10,20,23,34,46,47,53,54] that leverage on
pre-trained transformers and show promising performance
without using any rehearsal data. Prompt tuning, originally
introduced in NLP [ 28] attaches small learnable vectors to
a pre-trained frozen model to properly reuse the already
learned representations. Although promising for continual
learning, these approaches, however, suffer from the follow-
ing drawbacks – (1) Learning task-specific and task-shared
information in separate layers ignores possible interaction
between task-specific and task-shared components [ 43,49]
and (2) Always learning a fixed number of prompts per task
irrespective of the tasks’ similarity with the previous ones.
The redundant prompts can lead to overfitting specifically
in cases where the tasks are highly similar usually seen in
fine-grained datasets.
In this work, we propose ConvPrompt , which leverages
task-specific prompts that is generated by convolution over
task-shared parameters. We also exploit similarity between
tasks to control the generation of prompts. Specifically, task
shared knowledge is modeled by a learnable embedding
matrix at each layer. Prompts, on the other hand, are task
specific and are generated by convoluting on the task shared
embeddings with learnable kernels. The shared embeddings
are free to adapt with the different tasks. The prompt gen-
erating convolution kernels, on the other hand, are set aside
once learned for a task and new set of kernels are employed
for the next task. Such a design enables the shared em-
beddings to capture common concepts while allowing the
convolution operations to capture the task-specific concepts
from the common concepts. Moreover, we employ language
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23616
models to get similarity between tasks in order to determine
the number of trainable convolution kernels. Such an ex-
pansion strategy allows the model to be parameter efficient,
introducing only the necessary number of new parameters
as and when needed. The benefits of our approach are three-
fold – (1) It facilitates knowledge transfer across tasks using
task-shared embeddings. (2) Convolutional prompt gener-
ation promotes efficient adaptation to new tasks with low
parameter overhead. (3) Exploiting similar tasks by large
language models results in further reduction in parameters
and superior performance.
Extensive experimentation across several rehearsal-
free continual learning benchmarks shows that our
ConvPrompt approach achieves significant performance
gain compared to state-of-the-art approaches. On average,
across the 3 benchmark datasets and different experimental
settings, we outperform the state-of-the-art prompt based CL
approaches ( e.g., CODA-Prompt [ 46]) by a margin of ∼3%
while requiring a significantly lower number of parameters.
Our contribution can be summarized as follows:
•We propose a local prompt creation mechanism by ap-
plying convolution over task-invariant global parameters
enabling an efficient transfer of local and global concepts
across tasks that helps the new tasks to adapt better.
•We incorporate a novel language based task similarity
prediction, for the first time in continual learning, which
helps to reduce the model parameters significantly without
sacrificing the performance and without adding significant
pre-processing overhead as well.
•The extensive ablations and experimentation over the vari-
ous standard datasets and experimental settings show the
superiority of our approach by a significant margin.
2. Related Work
Continual Learning: CL approaches can be classified into
3 broad categories. (1) Regularization-based methods tackle
catastrophic forgetting by applying regularizers that prior-
itize the preservation of important parameters associated
with previously learned tasks. By minimizing interference
between new and old knowledge through the introduction
of penalty terms [ 1,2,24,26,58,59] or constraining the
direction of parameter update [ 14,45], these methods en-
courage important parameters to remain in close proximity
to previous solutions. While regularization based methods
have shown promising results involving smaller number of
tasks, their performance can be less satisfactory when con-
fronted with challenging scenarios involving large number
of tasks. (2) Dynamic Architecture-based methods learn new
tasks by assigning distinct parameters for each task [ 13,55–
57]. While these approaches exhibit the capability to learn
extended sequences of tasks, they may encounter substan-
tial memory and computational overhead. Also, most ap-
proaches under this category require the information ofwhich task an image belongs to during inference which may
be unrealistic. (3) Rehearsal-based methods [3,4,12,20–
22,41,52] store a few representative training samples from
previous tasks in a buffer, which is then used for train-
ing alongside the current task. Though effective, these ap-
proaches are limited by the size of the buffer and the length
of the task sequences. These are also not particularly suitable
for scenarios with data privacy requirements. In contrast,
ConvPrompt addresses rehearsal-free continual learning
by intelligently utilizing prompts on pre-trained models.
Recently, vision transformers have performed very well
in CL [ 12,22,43,47,52,55]. Authors in [ 33] examined
the impact of attention heads while Dytox [ 12] acquires
new skills by expanding special task tokens. LVT [ 52] in-
troduced inter-task attention in vision transformers. Both
methods, however, require additional memory to store previ-
ous training instances. MEAT [ 55] uses a parameter isolation
approach while learning new tasks. However, the model’s
expandability is limited, restricting the number of tasks it
can learn and it requires task-ids to be known during infer-
ence. ContraCon [ 43] uses convolutional re-weighting of
the self-attention weights. However, inference is costly as it
uses augmentation-based entropic task identification.
Prompt Learning : Prompt-based CL offers robust protec-
tion against catastrophic forgetting by incorporating a small
number of model instructions called prompts, rather than
directly modifying encoder parameters [ 9,10,20,23,34,
46,47,53,54]. Initial approaches like L2P [ 54] and Dual-
Prompt [ 53] employ a prompt pool from which prompts are
selected. These methods match input data to prompts without
relying on task identification, using local clustering based
optimization. Recently, S-Prompts [ 51] used prompts to
continually learn in a domain incremental learning scenario,
which involves learning the same set of classes under co-
variate distribution shifts. CODA-Prompt builds on [ 53,54]
and applies soft attention on the prompts towards end-to-end
continual learning. ProgressivePrompts [ 37] progressively
learns new prompt tokens for each incoming tasks, but as-
sumes the presence of task-id during inference. A contem-
porary work, LGCL [ 23], uses handcrafted prompts from
outside and contrastively learns to bring the output represen-
tation of the transformer to the prompt. However, without
indigenous prompt learning, this approach can act only as a
plugin to existing CL approaches with incremental improve-
ment in performance. Our approach, ConvPrompt , stands
out from the rest in its proficiency in knowledge-sharing
across tasks, its ability for on-the-fly prompt generation and
its effective handling of the additional parameters required.
3. Preliminaries
Continual Learning : Continual Learning (CL) trains mod-
els on tasks arriving sequentially over time without forgetting
the previous ones. Each task t∈ {1, . . . , T }contains train-
23617
Figure 1. The proposed ConvPrompt architecture. The layerwise [cls] embedding is passed through the projection network to
generate task-specific query representations which are then matched with the prompt-keys using cosine similarity to get the similarity values
of prompt-generators. The prompt generators are applied over the shared embeddings to generate input specific prompt components. A
weighted average of these prompt components are calculated with the corresponding cosine similarity value as weights to get the input
specific key and value prompts which are applied to the pre-trained model. The learnable components are highlighted in green , the frozen
components are highlighted in grey.
ing samples {(xt
i, yt
i)}where xt
iis the ithsample of the tth
task and yt
i∈Ctis the corresponding label. The set of
the class labels for different tasks are mutually exclusive,
i.e.,C0∩C1. . .∩CT=ϕ. We address the challenging
rehearsal-free andClass-Incremental Learning (CIL) setting
of CL where a trained model fneeds to predict the label
y=f(x)for an unseen test sample x, regardless of its task
and without access to training data from previous tasks.
Transformer Architecture : We build our approach on top
of a pre-trained Vision Transformer (ViT) [ 11]. A trans-
former starts by dividing an input image xinto a set of
Nfixed-sized patches z1∈RN×dwhich are then embed-
ded into a d-dimensional space with positional encoding.
A single encoder layer of ViT consists of stacks of Multi-
Head Self-Attention (MHSA), layer normalization and Feed
Forward Network (FFN) blocks with residual connections.
Given the input zlat the lthlayer, the output zl+1is gener-
ated that goes into the next layer as input for l∈ {1,2,···L}
where Lis the total number of encoder layers. At the lth
layer, MHSA block computes self-attention on the input
zlby using Hseparate self-attention heads. Self-attention
values from head h∈ {1,2,···H}at layer lis given by,
A(Ql,h, Kl,h, Vl,h) =softmaxQl,hKT
l,h√dk
Vl,h (1)
where Ql,h=zlWQ
l,h,Kl,h=zlWK
l,handVl,h=zlWV
l,hare
query, key, and value with learnable weights WQ
l,h, WK
l,hand
WV
l,h∈Rd×dhrespectively. dh=d/H is the dimensionof key, query and value vectors. The activations from dif-
ferent attention heads are then concatenated and residually
added to the input zlbefore performing layer normalization.
The resulting activations are passed through a FFN block
consisting of two linear layers and an activation function
(usually GELU). After another residual connection and layer
normalization, the output zl+1at the lthlayer is generated.
Prompt and Prefix Tuning : Prefix or prompt tuning aims to
learn continuous vectors where the pre-trained transformer
is kept frozen. It prepends lplearnable vectors to the original
keys and values of the self-attention heads at every layer.
Specifically, lplength prefix vectors P(K)
l,h;P(V)
l,h∈Rlp×dh
are concatenated with the original key Kl,hand value Vl,h
respectively. Then the self-attention values from head h
at layer lis computed as A(Ql,h,[P(K)
l,h, Kl,h],[P(V)
l,h, Vl,h])
following Eqn. 1. Unlike existing prompt based approaches
learning directly the additional vectors [ 46,53,54], our
work focuses on creating prompts by maintaining a balance
between old knowledge of the system and new information
to be put on an ad-hoc basis.
4. Methodology
In this work, we propose a prompt-based CL approach
(ConvPrompt ) by generating prompt vectors for each new
task in combination with knowledge learned previously (ref.
Fig. 1). Knowledge from previous tasks is modeled by a
learnable embedding matrix shared between all tasks . The
task-specific prompts are created by employing convolution
23618
operation on the shared embedding matrix. While existing
works [ 53,54] have shown to perform well by prompting a
pre-trained transformer, they rely on a single set of prompt
vectors needing to compress all necessary information of an
image into one single set. Rather than one set of lpprompt
vectors, following CODA-Prompt [ 46], we have Msuch sets
which we call prompt components . However, unlike CODA-
Prompt, the prompt components are notdirectly learned in
our approach. Instead, they are generated from previous task
knowledge by employing Mconvolution kernels (known
as the prompt generators ) in each head of each layer. A
weighted combination of the prompt components provides
the final lpprompt vectors where the weights come from the
[CLS] embedding at each layer of the ViT corresponding
to the input image. The weighing not only allows us to opti-
mize the model end-to-end but also provides a unique blend
of previous task knowledge and the input image.
4.1. Prompt Generation
The prompt vectors in our approach are dynamically gener-
ated by small convolution operations. Convolution is per-
formed between two learnable components – (i) Shared Em-
beddings and (ii) Prompt Generators. Corresponding to each
headh∈ {1,2,···H}in each layer l∈ {1,2,···L}of the
transformer, there are shared embedding matrices SEK
l,hand
SEV
l,hrespectively for the keys and the values. These matri-
ces are shared across tasks. The prompt generators are single
channel convolution kernels which are applied on the shared
embeddings. We learn a set of Mprompt generators for
each head in each layer, for both keys and values. For head h
and layer l, the prompt generators are denoted as GK
l,h,m and
GV
l,h,m , where m∈ {1,2,···M}. The shared embeddings
are of dimension (lp+k−1)×(dh+k−1)where kis
the size of the convolution kernels of the prompt generators.
The convolution operation results in prompt components
PCK
l,h,m andPCV
l,h,m of size lp×dh. The convolutional
prompt generators not only enables us to maintain a good
trade-off between performance and low parameter require-
ments per task but also makes use of the inherent inductive
bias for structured prompt creation [48].
4.2. Prompt Weighting
Instead of compressing all the information into one set of
prompt vectors we make use of Mprompt components to
get the final prompt vectors at each head of each layer. For
each prompt component PCK
l,h,m (andPCV
l,h,m ), we gener-
ate weights (between −1and1) which can be interpreted
as the relative importance of the particular prompt com-
ponent in blending them together. We employ Mlearn-
able keys, referred to as prompt keys π∈Rdπfor this
purpose. The prompt keys work on the image features
expressed as a nonlinear function of the [CLS] token at
each layer. The [CLS] token is passed through a Projec-tion Network ( PNϕ), parameterized by ϕ, consisting of a
two-layer fully-connected neural network with a ReLU ac-
tivation [ 35] in between. The output PNϕ([CLS] )maps
the input image to the same space as the prompt keys where
a cosine similarity is taken between PNϕ([CLS] )and the
Mprompt keys in each layer lresulting in the importance
values {sl,1, sl,2,···, sl,M}. The prompt components ob-
tained as a result of the convolution operation between the
prompt generators and shared embeddings are weighed by
theMsimilarity scores to get the final prompts P(K)
l+1,hand
P(V)
l+1,has follows,
P(K)
l+1,h=MX
m=1sl,mPCK
l,h,m;P(V)
l+1,h=MX
m=1sl,mPCV
l,h,m (2)
While existing works have linearly combined prompt
components to get the final prompt, the similarity scores
were generated using the same final [CLS] token across all
layers. However, this requires a full forward pass through
the transformer solely for getting the similarity scores and
then the final prompts at each layer for the final predictions
resulting in a total of two passes. In contrast, we utilize the
[cls] embedding from each layer enabling us to generate
the final prompts for the subsequent layers in one single
pass resulting in a significant reduction of computation. The
similarity values generated from the image help different
prompt components to focus on specific features towards the
final prompt [ 46]. Building on this, we used a non-linearly
learned projection network that better captures complex tasks
as shown empirically.
4.3. Language Guided Prompting
To maintain a balance between learning new tasks and pre-
serving knowledge accumulated from old tasks, we make use
of both task-shared as well as task-specific parameters. In
our work, PNϕandSEact as shared parameters facilitating
inter-task information sharing, while with new tasks coming,
the previously learnt prompt generators and prompt-keys are
frozen, thus making them task-specialized. A new set of
these are freshly learned for every incoming task.
Let the total number of prompt generators per layer
learnt till the (t−1)thtask be Mt−1, while the number
of prompt generators learnt for task ionly is Ji. Natu-
rally, Mt−1=t−1P
i=1Jiand all Mt−1prompt generators are
kept frozen when the new set of Jtprompt generators are
learned. Notwithstanding the prompt generators’ role to
mitigate catastrophic forgetting, it is also crucial to keep
the increase in parameters in check with increasing number
of tasks. Ideally, if a task is similar to a task seen earlier,
then the prompt generators learnt previously can be reused.
Thus, in contrast to previous works [ 46] which learn a fixed
number of prompt components per task, we learn a dynamic
23619
Tasks Split CIFAR-100 Split CUB-200 Nparam (↓)
Method AT(↑) FT(↓) AT(↑) FT(↓) Train/Total
Joint-FT (upper bound) 93.22±0.16 – 88.00±0.15 – 100/100
Seq-FT 8.6±0.43 42 .67±0.13 23.87±0.54 62 .52±0.57 100/100
ER (buffer size 5000) 82.30±0.42 16 .30±0.24 60.73±0.23 8 .71±0.65 100/100
LwF [27] 64.56±1.23 25 .27±1.32 48.73±1.46 25 .18±0.31 100/100
L2P [54] 82.76±1.17 7 .86±0.39 62.21±1.92 7 .12±0.33 0.7/100.7
L2P + LGCL [23] 84.33±0.06 5 .83±0.23 – – 0.7/100.7
DualPrompt [53] 85.07±0.49 5 .57±0.20 66.00±0.57 4.4±0.31 1.3/101.3
DualPrompt + LGCL [23] 87.23±0.21 5 .10±0.15 – – 1.3/101.3
CODA-Prompt [46] 87.00±0.38 4 .78±0.24 74.40±0.74 6 .40±0.34 4.6/104.6
ConvPrompt 88.87±0.33 4 .75±0.15 80.2±0.52 5.6±0.38 2.0/102.0
Table 1. Results (%) on CIFAR-100 and CUB-200 . Reported results are for 10tasks with a supervised ImageNet-21k pretrained ViT as
the backbone. ATdenotes the average accuracy and FTdenotes the forgetting. Nparam denotes the percentage of trainable/final parameters
w.r.t that of the ViT model. The Nparam values are dynamic for our approach and the reported value ( 2.0/102.0) is the average of the values
in Split-CIFAR-100 ( 2.2/102.2) and Split CUB-200 ( 1.8/101.8).
number of prompt generators depending on the similarity
of the task with the previous ones. The task similarity can
be naively modeled by comparing the visual features of the
images from different tasks. We present an alternative frame-
work to get task similarity cheaply using Large Language
Models (LLMs) such as GPT-3 [ 5] which show remarkable
world knowledge on a variety of topics.
Our key insight is that we can use language as a tool
to get descriptions of visual attributes of different classes
and use these to find task similarity. Visual attributes are
additional semantic knowledge that articulate the visual con-
cepts associated with each categories. For example, some
visual attributes of beeare ‘black and yellow stripes’, ‘two
pairs of wings’, ‘three body segments’, etc. Instead of man-
ually writing these, we queried GPT-3 to get the attributes
for each set of classes in each task as they arrived. This is
computationally cheap, requires no additional training, and
is scalable to large number of classes. Inspired by works
like [ 30,32], the class attributes are generated by using the
query – “ What are useful features for distinguishing a [class
name] in a photo? ” for each class in a task. We generate
the BERT embeddings of these attributes and store them in a
pool for all seen tasks. For each attribute of the current task
t, cosine similarity with all stored attribute embeddings till
taskt−1is computed. The similarity simtof task twith
the previous tasks, is the mean of such maximum similarity
values across all attributes of task t. Let Jmax denote the
maximum prompt generators per task. Then Jtis given by
(1−simt)Jmax i.e., higher similarity enables to have lower
number of prompts. Leveraging linguistic knowledge, we
reduce the learnable parameters if the classes in a task have
a high overlap with the previously encountered ones.
4.4. Regularization and Final Objective
To prevent over-writing of concepts captured by previous
tasks in global task-shared PNϕandSE, we need to ensure
that while learning the current task, these parameters deviateless from the previous tasks. To achieve this, when learning
for task t, we regularize the set of parameters ϕtof the
projection network and the shared embeddings SEt, to have
lowl1norm with that of the previous task as follows:
Lr(ϕt, ϕt−1) =||ϕt−1−ϕt||1
Lr(SEt, SE t−1) =||SEt−1−SEt||1(3)
SEtdenote the shared embedding parameters of task tfor
all heads and layers combined and for both keys and val-
ues. Note that suffix tused with the shared embeddings may
indicate that SEis task specific, they are not. Shared signi-
fies the same set of embeddings is used to learn the shared
semantics among different tasks. We incrementally update
the same SE for each task, using a copy of the SE from the
immediately preceding task for regularization in Eqn. 3 that
is discarded after training. ϕtdenote the projection network
parameters at the tthtask. The final objective is:
Lcls(f(x), y)+ 1(t>1)λ[Lr(ϕt,ϕt−1)+Lr(SEt,SEt−1)](4)
where Lclsdenotes classification loss, tthe task-id and λ∈
[0,1]the hyper-parameter to weigh the loss components. The
indicator function 1(t>1)denotes the fact that regularization
is applied after the first task.
5. Experiments
Datasets : We evaluate ConvPrompt on the benchmark
datasets, ImageNet-R [ 18] and CIFAR-100 [ 25], as well
as on the fine-grained dataset, CUB-200 [ 50] in the CIL
setup. ImageNet-R [ 18] is formed from 200 subcategories
of ImageNet [ 44] but with images from different domains
such as cartoon, graffiti and origami. It also includes some
hard examples from ImageNet that standard models fail to
classify. It contains 24,000 training images and 6,000 test
images. Following [ 53,60], we split the 200classes into
10tasks with each task containing 20classes respectively.
CIFAR-100 [ 25], a widely used dataset in continual learn-
ing, contains 100classes, with each having 500training and
23620
Tasks 5 Task 10 Task 20 Task Nparam (↓)
Method AT(↑) FT(↓) AT(↑) FT(↓) AT(↑) FT(↓) Train/Total
Joint-FT (upper bound) 79.6±0.87 – 79.6±0.87 – 79.6±0.87 – 100/100
Seq-FT 21.82±0.85 76 .26±0.37 11.42±0.76 78 .32±0.64 8.75±0.42 82 .21±0.96 100/100
ER (buffer size 5000) 70.53±0.68 17 .47±0.35 64.32±0.65 22 .35±0.97 53.26±0.83 34 .21±0.82 100/100
LwF [27] 49.75±0.65 41 .36±0.27 39.27±1.92 51 .23±0.34 30.29±1.82 60 .32±0.86 100/100
L2P [54] 67.43±0.11 5 .12±0.62 63.49±0.40 6 .85±0.42 59.38±0.50 5 .89±0.36 0.7/100.7
L2P + LGCL [23] – – 62.51±0.05 8 .9±0.17 – – 0.7/100.7
DualPrompt [53] 70.42±0.88 4 .1±0.33 68.50±0.52 5 .14±0.18 63.21±0.49 5 .28±0.45 1.3/101.3
DualPrompt + LGCL [23] – – 69.46±0.04 4 .2±0.06 – – 1.3/101.3
CODA-Prompt [46] 75.38±0.34 6 .08±0.36 74.24±0.56 4 .92±0.21 70.86±0.42 6 .87±0.25 4.6/104.6
ConvPrompt 79.10±0.47 3 .08±0.11 77.86±0.25 4 .33±0.24 75.1±0.39 4 .1±0.29 2.2/102.2
Table 2. Results (%) on ImageNet-R . Reported results are for 5,10and20tasks splits of ImageNet-R with a supervised ImageNet-21k
pretrained ViT as the backbone. ATdenotes the average accuracy and FTdenotes the forgetting. Nparam denotes the percentage of
trainable/final parameters w.r.t that of the ViT model. The Nparam values vary for ConvPrompt with varying number of tasks and the
reported value is the average of the values for 5-tasks ( 1.64/101.64), 10-tasks ( 2.0/102.0) and 20 tasks ( 2.88/102.88).
100test images. Following [ 46,53,54,60], we use the
10task setup of CIFAR-100 with each task containing 10
classes. CUB-200 [ 50] is a fine-grained dataset containing
200classes of different bird species with 5994 training im-
ages and 5794 test images. Following [ 60], we use the 10
task setup of CUB-200 with each task containing 20classes.
Training and Implementation Details : We use the ViT-
B/16 [ 11] model pre-trained on the ImageNet-21k [ 39] as
the backbone over which ConvPrompt is applied. Our pro-
jector network is a two-layer neural network having d/2and
d/4neurons in these two layers respectively where the input
([CLS] token) is d-dimensional. A ReLU [ 35] activation
function is applied between the two layers. Our approach
applies prompts to 7layers of the pre-trained ViT as our
ablation experiments show that more layers with prompts
does not help improve the performance although parame-
ter overhead increases. We train each task in CIFAR-100,
ImageNet-R and CUB-200 for 10,10and60epochs respec-
tively. The hyperparameter Jmax is set to 5, acting as an
upper limit for the maximum number of prompt components
needed per task. The hyperparameter λ, which is used to
weigh the regularization terms in Eqn. 4 is set to 0.01. The
experiments leading to our choice of λand other additional
experimental results can be found in the supplementary ma-
terial. We present our results after conducting five random
trials, where task orders were randomly selected for each
run. The mean ±stdvalues are reported.
Metrics Used: We report Average accuracy ATand Forget-
tingFTcalculated over the Ttasks for all our experiments.
Specifically, after training on Ttasks is completed, ATand
FTare calculated as follows:
AT=1
TTX
t=1St,T
FT=1
T−1T−1X
t=1max
t′∈{1,...,T−1}(St,t′−St,T)(5)where St,Tis the test classification accruacy on task tafter
the model has been trained on task T. In other words, aver-
age accuracy measures the average accuracy of all the tasks
after training on the last task and forgetting measures the
average drop in accuracy of a task after training on the last
task from its maximum accuracy attained.
Comparisons: We evaluated our approach against several
rehearsal free approaches. These include Learning without
Forgetting (LwF) [ 27], Learning to Prompt (L2P) [ 54], Du-
alPrompt [ 53] and CODA-Prompt [ 46]. We also evaluated
against LGCL [ 23], which uses language based prompts
and acts as a plugin to L2P and DualPrompt. Additionally,
we also compared our approach with a rehearsal-based ap-
proach, Experience Replay (ER) [ 7] with buffer size 5000 .
In addition, we report Joint-FT and Seq-FT performances as
they serve as bounds of the performance in many situations.
In Joint-FT, the ViT model is trained jointly on the training
data of all the tasks combined, serving as an upper bound
on the performance. Seq-FT represents fine-tuning the ViT
model sequentially using only the new task’s training data
and thus is severely affected by catastrophic forgetting.
5.1. Results and Analysis
As shown in Table 1 and Table 2, ConvPrompt outper-
forms both rehearsal-free and rehearsal-based approaches
significantly. On average, ConvPrompt outperforms the
existing state-of-the-art CODA-Prompt [ 46] by∼3%while
using only ∼40% of the trainable parameters used by
CODA-Prompt. Our approach shows slightly more forget-
ting in the 10-task setup of Split-CUB. However, even with
more forgetting, our approach is able to outperform the exist-
ing approaches by atleast 3%margin thereby confirming that
our design enables efficient adaptation to new tasks by pre-
venting overfitting that leads to higher maximum accuracy by
the tasks. This indicates that our convolutional prompt cre-
ation mechanism is able to utilize the shared inter-task con-
cepts better than CODA-Prompt [46] and DualPrompt [53].
23621
Dataset SLCA SLCA + ConvPrompt
Split CIFAR-100 91.53±0.28 90.60±0.35
Split ImageNet-R 77.00±0.33 78.5±0.37
Split CUB-200 84.71±0.40 87.12±0.31
Table 3. Results with SLCA: We report the ATvalues for SLCA
and the SLCA + ConvPrompt .ConvPrompt when applied on
top of SLCA, improves its performance by 1−2%.
Method AT(↑) FT(↓) Nparam (↓)
Upper Bound 79.7±0.15 – 100/100
ViT (linear probing) (Lower Bound) 62.1±0.52 5 .74±0.230.02/100
+SE 67.3±0.58 5 .19±0.210.7/100.7
+SE+ NN 73.82±0.84 9.92±0.4814.9/114.9
+SE+ Conv 73.92±0.65 9.43±0.333.4/103.4
+SE+ Conv + PN (1-layer) 76.28±0.23 4.46±0.173.5/103.5
+SE+ Conv + PN (2-layer) 77.96±0.54 4.75±0.573.7/103.7
ViT +SE+ Conv +
PN (2-layer) + Task-Sim 77.86±0.25 4.33±0.242.0/102.0
(ConvPrompt )
Table 4. Ablation over ImageNet-R 10 tasks: We report ATand
FTaveraged for 5trials. We also report the number of trainable
params of each of the variants. ViT denotes the ViT pre-trained on
ImageNet-21k over which we apply the different components.
Comparison with SLCA : We also conducted a comparison
with a recent SOTA approach Slow Learner with Classifier
Alignment (SLCA) [ 60] that does not use prompts for CL.
Instead, it finetunes the whole network with smaller learning
rate for the represenation layers and larger learning rate
for the classification layer. This approach has demonstrated
superior performance compared to existing continual prompt-
tuning methods. As it involves full network tuning, it is
computationally expensive making it impractical in resource-
constrained scenarios. In contrast, our approach is viable in
situations where compute is limited. In this experiment, we
show that our approach can also exploit differential learning
rates and classifier alignment as SLCA and provide further
improvement in performance. Specifically, in such cases, we
learn the prompts along with the transformer weights during
fine-tuning. The results in Table 3 demonstrate that our
approach combined with SLCA, outperforms it in two out
of the three datasets achieving the new state-of-the-art while
in the CIFAR-100 dataset it is almost at par with SLCA.
5.2. Ablation Studies and Other Analysis
We perform all our ablation studies on the 10task setup of the
ImageNet-R dataset unless otherwise mentioned. Through-
out the study, we consider the ImageNet-21k pre-trained
ViT-B/16 model onto which we gradually integrate our mod-
ules to showcase their importance. Table 4 shows the results.
Exploiting Shared Embedding across Tasks : Adding
shared embedding on top of the ViT-B/16 baseline, already
gives better performance than L2P [ 54] that uses a pool of
prompts (ref. row ‘ +SE’). This shows that knowledge
shared across tasks, when properly regularized, can itself be
better than purely individual prompts.
Role of Prompt Generators : Next, we add the task-specificprompt generators on top of the task-shared embeddings. We
experimented with two functional forms of the prompt gener-
ators – a) Convolution kernels and b) Neural Networks. Both
types show significant improvement (ref. rows ‘ +SE+NN’
and ‘+SE+Conv’). However, using a neural network is
naturally heavier on compute compared to a convolutional
prompt generator as shown in the rightmost column. Overall,
the addition of task-specific prompt generators leads to a sig-
nificant improvement of ∼6%over the task-shared-only ap-
proach and an improvement of ∼3%over DualPrompt [ 53]
which also uses both task shared and task specific knowledge.
This reaffirms our hypothesis that prompt generation over
shared embeddings helps in the effective sharing of inter-task
concepts to better adaptation of newer tasks.
Significance of Prompt Weighing : We then add our prompt
weighting mechanism which further gives an improvement
of∼3%(ref. two rows starting with ‘+ SE+ Conv + PN’).
This signifies that our prompt weighting exploits important
prompts better. To investigate if a non-linearity in projector
network helps, we compare the performances of a linear
projection network (single layer fully-connected neural net)
with a non-linear one (two-layer fully-connected neural net
with a ReLU in between) and observe that the non-linearity
naturally is better with complex visual data albeit with a
slight increase in parameter count.
Language helps reduce parameters : Finally, we add lan-
guage driven task-similarity to dynamically determine the
number of prompt generators (ref. last row of Table 4) which
makes the full ConvPrompt . This leads to significant pa-
rameter reduction while maintaining the same performance.
In datasets containing very similar tasks ( e.g., CUB-200) the
performance also gets boosted ( ∼1%) possibly due to less
overfitting as a result of reduced number of parameters.
Effect of Prompt Length : We analyze the effect of the
length ( lp) of prompt vectors on the performances of differ-
ent prompt based CL approaches including ours. For this
purpose, we ran experiments with different prompt lengths,
increasing the length in multiples of 4from 4to40. As can
be seen in Fig. 2a, ConvPrompt performs the best across
all values of lpand the performance has an increasing trend
till the prompt length is 20 after which it saturates. So, we
usedlp= 20 , unless otherwise mentioned.
Effect of Increasing Layers to Prompt : To analyze if
prompting every layer helps we applied prompts to differ-
ent layers of the pre-trained backbone for ConvPrompt
and the closely related approaches (ref. Fig. 2b). Specifi-
cally, starting with prompts to the first 5 layers, we go on
to apply prompts till the last layer ( i.e.,12thlayer). As
seen,ConvPrompt ’s performance peaks at 7layers before
stagnating while for DualPrompt and CODA-Prompt perfor-
mance decreases after 5layers and then stagnates.
Effect of number of Prompt Generators : We analyze the
effect of increasing the maximum number of prompt genera-
23622
Figure 2. Ablation over Split ImageNet-R 10 tasks: (a) average accuracy vs prompt length: The performance peaks at 8for CODA-P and
at20for rest of the models (b) average accuracy vs number of layers prompts are applied to: The performance for ConvPrompt , peaks at 7
layers while it peaks at 5for the other models. (c) Average accuracy vs maximum number of prompt components per task: The performance
ofConvPrompt peaks at Jmax= 5prompts per task, with the final prompt count after 10 tasks reaching to 18owing to task-similarity.
(d) Average accuracy vs kernel size of the prompt creators: The performance for ConvPrompt , peaks at kernel size 17.
Method AT(↑) FT(↓) Nparam (↓)
class-label based sim 76.93±0.45 4 .49±0.57 2.22/102.22
image-based sim 77.18±0.42 4 .38±0.47 2.29/102.29
Class Attribute
based sim ( ConvPrompt ) 77.86±0.25 4 .33±0.24 2.00/102.00
Table 5. ImageNet-R 10 tasks: Comparison of different task-
similarity measures. We report ATandFTaveraged for 5trials.
torsJmax (ref. Fig. 2c). We observe that the performance
peaks at Jmax= 5after which the performance decreases.
Effect of Kernel Size of Prompt Generators : To under-
stand the impact of the prompt generator’s kernel size, we
varied the convolution kernel size kfrom5to25in steps of
2. As can be seen in Fig. 2d, convolution kernels of size 17
gives the best results while slightly decreasing and stagnat-
ing for higher values. We chose kernel size 17as the default
value since it leads to a better trade-off between the number
of trainable parameters and performance.
P→C C→P
Datasets AT(↑) FT(↓) AT(↑) FT(↓)
CIFAR-100 88.87±0.33 4 .75±0.15 88.24±0.31 3 .86±0.34
ImageNet-R 77.86±0.25 4 .33±0.24 77.76±0.28 3 .65±0.27
CUB-200 80.2±0.52 5 .6±0.38 80.1±0.45 5 .7±0.26
Table 6. Prefixes before and after projection on 10-task setup.
Effect of Prompting before or after Projection : We ana-
lyze the effect of concatenating the prompt vectors P(K)
l,hand
P(V)
l,hwith the original key Kl,hand value Vl,hrespectively
vis-a-vis concatenating them with the input zlwhich is pro-
jected to get the key and values. Performance does not vary
much if projection is performed after concatenation ( C→P)
compared to the other way round ( P→C) (Table 6). Natu-
rally, computation is more (by ∼0.2BMACs) in ( C→P)
due to bigger matrix-vector multiplications. With low com-
pute overhead and comparable performance, ( P→C) is
advantageous and is used in our experiments.
Best way to measure Task Similarity : As similarity be-
tween tasks plays a crucial role in performance as well as
additional parameters trained, we tried different avenues to
measure task similarity. Specifically, we experimented with
(i) class-label based task similarity and (ii) image-based task
similarity. In the first approach, instead of taking GPT-3Method MACs
L2P [54] 35.85B
DualPrompt [53] 33.72B
CODA-Prompt [46] 33.72B
ConvPrompt 17.98B
Table 7. Comparison of Inference Times: MAC (multiply accu-
mulate) operations required for the evaluation of tasks once the
model has been fully trained on the 10 tasks of Split ImageNet-R.
generated attributes, we directly used the class labels and
in the second we extracted visual features using pretrained
ViT-B/16 and used them to measure similarity with previous
tasks. Table 5 shows the performance and parameter require-
ments with these on 10-task ImageNet-R. It can be seen that
the attribute-based class similarity leads to the introduction
of least number of parameters as well as best performance.
Comparing Inference-Time : We compared the inference
time with competing prompt tuning based approaches,
namely L2P [ 54], DualPrompt [ 53] and CODA-Prompt [ 46].
Specifically, for a model, we compute the number of MACs
(Multiply and Accumulate Operations) an input image con-
sumes during inference after the model has been trained for
all the 10tasks on the ImageNet-R dataset. As can be seen
in Table 7, ConvPrompt requires the least compute backed
by the single pass for prompting through it.
6. Conclusion
In this paper, we proposed ConvPrompt , a novel convo-
lutional prompt generation mechanism coupled with a task
similarity based expansion strategy for rehearsal-free CL.
Different from the existing approaches, our approach cre-
ates prompts in each layer by applying convolution over
task shared embeddings causing better knowledge trans-
fer across tasks. Moreover, our expansion strategy with
LLM driven task similarity ensures that this performance
boost is achieved without a significant increase in the num-
ber of learnable parameters. Extensive experimentation
showed that ConvPrompt outperforms SOTA baselines
significantly while requiring fewer additional parameters.
23623
References
[1]Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In The European
Conference on Computer Vision (ECCV) , 2018. 2
[2]Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars.
Task-free continual learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 11254–11263, 2019. 2
[3]Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. Advances in neural information processing systems ,
32, 2019. 2
[4]Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha,
and Jonghyun Choi. Rainbow memory: Continual learn-
ing with a memory of diverse samples. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 8218–8227, 2021. 1, 2
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage Models are Few-Shot Learners. Advances in neural
information processing systems , 33:1877–1901, 2020. 5
[6]Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Si-
mone Calderara. Rethinking experience replay: a bag of tricks
for continual learning. 2020 25th International Conference
on Pattern Recognition (ICPR) , pages 2180–2187, 2021. 1
[7]Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr,
and Marc’Aurelio Ranzato. On tiny episodic memories in
continual learning. arXiv preprint arXiv:1902.10486 , 2019. 6
[8]Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr,
and M Ranzato. Continual learning with tiny episodic memo-
ries. In International Conference on Machine Learning , 2019.
1
[9]Haoran Chen, Zuxuan Wu, Xintong Han, Menglin Jia, and Yu-
Gang Jiang. Promptfusion: Decoupling stability and plasticity
for continual learning. arXiv preprint arXiv:2303.07223 ,
2023. 1, 2
[10] Marco D’Alessandro, Alberto Alonso, Enrique Calabrés, and
Mikel Galar. Multimodal parameter-efficient few-shot class
incremental learning. arXiv preprint arXiv:2303.04751 , 2023.
1, 2
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions , 2021. 3, 6
[12] Arthur Douillard, Alexandre Ramé, Guillaume Couairon, and
Matthieu Cord. Dytox: Transformers for continual learn-
ing with dynamic token expansion. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2022. 2[13] Verma et. al. Efficient feature transformations for discrim-
inative and generative continual learning. In CVPR , 2021.
2
[14] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li.
Orthogonal gradient descent for continual learning. In Inter-
national Conference on Artificial Intelligence and Statistics ,
pages 3762–3773. PMLR, 2020. 2
[15] Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville,
and Yoshua Bengio. An empirical investigation of catas-
trophic forgetting in gradient-based neural networks, 2015.
1
[16] Tyler L Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya,
and Christopher Kanan. Remind your Neural Network to
Prevent Catastrophic Forgetting. In European Conference on
Computer Vision , pages 466–483. Springer, 2020. 1
[17] Jiangpeng He and Fengqing Zhu. Exemplar-free Online Con-
tinual Learning. arXiv preprint arXiv:2202.05491 , 2022. 1
[18] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,
and Justin Gilmer. The many faces of robustness: A critical
analysis of out-of-distribution generalization. ICCV , 2021. 5
[19] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a unified classifier incrementally via
rebalancing. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2019. 1
[20] Zhiyuan Hu, Jiancheng Lyu, Dashan Gao, and Nuno Vascon-
celos. Pop: Prompt of prompts for continual learning, 2023.
1, 2
[21] David Isele and Akansel Cosgun. Selective experience replay
for lifelong learning. In Proceedings of the AAAI Conference
on Artificial Intelligence , 2018.
[22] Kishaan Jeeveswaran, Prashant Bhat, Bahram Zonooz, and
Elahe Arani. Birt: Bio-inspired replay in vision transformers
for continual learning. arXiv preprint arXiv:2305.04769 ,
2023. 2
[23] Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem,
Luc Van Gool, Didier Stricker, Federico Tombari, and
Muhammad Zeshan Afzal. Introducing language guidance
in prompt-based continual learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 11463–11473, 2023. 1, 2, 5, 6
[24] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan,
John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,
et al. Overcoming catastrophic forgetting in neural networks.
Proceedings of the national academy of sciences , 114(13):
3521–3526, 2017. 2
[25] Alex Krizhevsky et al. Learning multiple layers of features
from tiny images. Citeseer , 2009. 5
[26] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha,
and Byoung-Tak Zhang. Overcoming catastrophic forget-
ting by incremental moment matching. Advances in neural
information processing systems , 30, 2017. 2
[27] Zhizhong Li and Derek Hoiem. Learning without forget-
ting. IEEE transactions on pattern analysis and machine
intelligence , 40(12):2935–2947, 2017. 5, 6
23624
[28] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-
roaki Hayashi, and Graham Neubig. Pre-train, Prompt, and
Predict: A Systematic Survey of Prompting Methods in Nat-
ural Language Processing. ACM Computing Surveys , 55(9):
1–35, 2023. 1
[29] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
Episodic Memory for Continual Learning. Advances in neural
information processing systems , 30, 2017. 1
[30] Chengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit
Menon, Junfeng Yang, Xin Wang, and Carl V ondrick. Doubly
right object recognition: A why prompt for visual rationales.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 2722–2732, 2023. 5
[31] Michael McCloskey and Neal J. Cohen. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem. Psychology of Learning and Motivation , 24:109–
165, 1989. 1
[32] Sachit Menon and Carl V ondrick. Visual classification via
description from large language models. arXiv preprint
arXiv:2210.07183 , 2022. 5
[33] Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy
Nguyen, Razvan Pascanu, Dilan Gorur, and Mehrdad Fara-
jtabar. Architecture matters in continual learning. arXiv , 2022.
2
[34] Jun-Yeong Moon, Keon-Hee Park, Jung Uk Kim, and Gyeong-
Moon Park. Online class incremental learning on stochastic
blurry task boundary via mask and visual prompt tuning. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 11731–11741, 2023. 1, 2
[35] Vinod Nair and Geoffrey E. Hinton. Rectified linear units
improve restricted boltzmann machines. In International
Conference on Machine Learning , page 807–814, Madison,
WI, USA, 2010. Omnipress. 4, 6
[36] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania.
GDumb: A Simple Approach that Questions our Progress
in Continual Learning. In European conference on computer
vision , pages 524–540. Springer, 2020. 1
[37] Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian
Khabsa, Mike Lewis, and Amjad Almahairi. Progressive
prompts: Continual learning for language models. In In-
ternational Conference on Learning Representations , 2023.
2
[38] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl,
and Christoph H Lampert. icarl: Incremental classifier and
representation learning. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition , pages
2001–2010, 2017. 1
[39] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-
Manor. Imagenet-21k pretraining for the masses. arXiv , 2021.
6
[40] Anthony V . Robins. Catastrophic forgetting, rehearsal and
pseudorehearsal. Connect. Sci. , 7:123–146, 1995. 1
[41] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lill-
icrap, and Gregory Wayne. Experience replay for continual
learning. Advances in Neural Information Processing Systems ,
32, 2019. 2
[42] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lill-
icrap, and Gregory Wayne. Experience replay for continuallearning. In Advances in Neural Information Processing Sys-
tems, 2019. 1
[43] Anurag Roy, Vinay Verma, Sravan V oonna, Kripabandhu
Ghosh, Saptarshi Ghosh, and Abir Das. Exemplar-free con-
tinual transformer with convolutions. In International Con-
ference on Computer Vision (ICCV) , 2023. 1, 2
[44] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115(3), 2015. 5
[45] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projec-
tion memory for continual learning. In International Confer-
ence on Learning Representations , 2021. 2
[46] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola
Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar
Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Contin-
ual decomposed attention-based prompting for rehearsal-free
continual learning. arXiv preprint arXiv:2211.13218 , 2022.
Accepted for publication at CVPR 2023. 1, 2, 3, 4, 5, 6, 8
[47] Yu-Ming Tang, Yi-Xing Peng, and Wei-Shi Zheng. When
prompt-based incremental learning does not meet strong pre-
training. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 1706–1716, 2023. 1, 2
[48] Yun-Yun Tsai, Chengzhi Mao, and Junfeng Yang. Convolu-
tional Visual Prompt for Robust Visual Perception. In Neural
Information Processing Systems , 2023. 4
[49] Vinay Kumar Verma, Kevin J Liang, Nikhil Mehta, Piyush
Rai, and Lawrence Carin. Efficient feature transformations
for discriminative and generative continual learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 13865–13875, 2021. 1
[50] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The caltech-ucsd birds-200-2011 dataset. In California Insti-
tute of Technology , 2011. 5, 6
[51] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts
learning with pre-trained transformers: An occam’s razor
for domain incremental learning. In Conference on Neural
Information Processing Systems (NeurIPS) , 2022. 2
[52] Zhen Wang, Liu Liu, Yiqun Duan, Yajing Kong, and Dacheng
Tao. Continual learning with lifelong vision transformer.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 171–181, 2022. 2
[53] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,
Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vin-
cent Perot, Jennifer Dy, et al. DualPrompt: Complementary
Prompting for Rehearsal-free Continual Learning. In Euro-
pean Conference on Computer Vision , 2022. 1, 2, 3, 4, 5, 6,
7, 8
[54] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer
Dy, and Tomas Pfister. Learning to Prompt for Continual
Learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 139–149,
2022. 1, 2, 3, 4, 5, 6, 7, 8
[55] Mengqi Xue, Haofei Zhang, Jie Song, and Mingli Song.
Meta-attention for vit-backed continual learning. In 2022
23625
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2022. 2
[56] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynami-
cally expandable representation for class incremental learning.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , 2021. 1
[57] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju
Hwang. Lifelong learning with dynamically expandable net-
works. In International Conference on Learning Representa-
tions . ICLR, 2018. 2
[58] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
conference on machine learning , pages 3987–3995. PMLR,
2017. 2
[59] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
Conference on Machine Learning , pages 3987–3995. PMLR,
2017. 2
[60] Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen,
and Yunchao Wei. Slca: Slow learner with classifier alignment
for continual learning on a pre-trained model. In Proceed-
ings of the IEEE/CVF International Conference on Computer
Vision , 2023. 5, 6, 7
23626
