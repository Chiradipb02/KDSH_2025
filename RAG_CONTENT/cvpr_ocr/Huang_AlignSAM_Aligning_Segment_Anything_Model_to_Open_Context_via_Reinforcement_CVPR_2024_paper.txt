AlignSAM: Aligning Segment Anything Model to Open Context via
Reinforcement Learning
Duojun Huang1,2Xinyu Xiong1Jie Ma1Jichang Li1,3Zequn Jie4Lin Ma4Guanbin Li1,2†
1School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China
2GuangDong Province Key Laboratory of Information Security Technology
3The University of Hong Kong4Meituan
Abstract
Powered by massive curated training data, Segment Any-
thing Model (SAM) has demonstrated its impressive gen-
eralization capabilities in open-world scenarios with the
guidance of prompts. However, the vanilla SAM is class-
agnostic and heavily relies on user-provided prompts to
segment objects of interest. Adapting this method to di-
verse tasks is crucial for accurate target identification and
to avoid suboptimal segmentation results. In this paper,
we propose a novel framework, termed AlignSAM, designed
for automatic prompting for aligning SAM to an open con-
text through reinforcement learning. Anchored by an agent,
AlignSAM enables the generality of the SAM model across
diverse downstream tasks while keeping its parameters
frozen. Specifically, AlignSAM initiates a prompting agent
to iteratively refine segmentation predictions by interacting
with the foundational model. It integrates a reinforcement
learning policy network to provide informative prompts to
the foundational models. Additionally, a semantic recal-
ibration module is introduced to provide fine-grained la-
bels of prompts, enhancing the model’s proficiency in han-
dling tasks encompassing explicit and implicit semantics.
Experiments conducted on various challenging segmenta-
tion tasks among existing foundation models demonstrate
the superiority of the proposed AlignSAM over state-of-the-
art approaches. Project page: https://github.com/
Duojun-Huang/AlignSAM-CVPR2024 .
1. Introduction
Compared to traditional computer vision tasks [13, 24, 25,
52, 53], segmentation stands as a fundamental task, play-
ing a pivotal role in visual understanding systems. Accord-
†Corresponding author is Guanbin Li. This work was supported in part
by the National Natural Science Foundation of China (NO. 62322608), and
in part by the Open Project Program of the Key Laboratory of Artificial
Intelligence for Perception and Understanding, Liaoning Province (AIPU,
No. 20230003).
Saliency Detection
Segment Anything Model (SAM)Agent 1Shadow DetectionBlurDetectionGlass Detection
(a)(c)
Vision-Language Model(VLM)TextPrompt
ImageSAM(b)(Extra) ParasPredictionImageSAMVLMPredictionTextImageSAMPredictionIn-contextExamples(d)
Agent 2
Agent 3
Agent 4
Figure 1. Conceptual comparisons of our method and previous
approaches. Frozen and learnable parameters are highlighted in
blue andorange , respectively. (a) The proposed method. (b)
Text-guided methods [28, 46]. (c) PEFT methods [4, 59]. (d) In-
context learning methods [29, 60]. Observed that the proposed
agent-based auto-prompting effectively grasps vision and linguis-
tic cues, unleashing the potential of the foundation segmentation
model in various contexts, such as saliency detection, shadow de-
tection, blur detection, and glass detection.
ing to different semantic criteria for grouping pixels, vari-
ous downstream segmentation tasks have emerged, such as
saliency detection [34, 50, 64], shadow detection [12, 66],
and glass-like object detection [8, 57]. Although signifi-
cant progress has been achieved, the development of a uni-
fied framework that can accommodate the wide variations
inherent in the formulations of diverse segmentation tasks
continues to pose a challenge.
Recent breakthroughs in Vision Foundation Models
(VFMs) have demonstrated impressive capabilities in zero-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3205
(a)
(b)(b)Figure 2. Segmentation results for CLIP-Surgery [27] using dif-
ferent prompts. (a) and (b) illustrate the prompts of “dog” and
“salient object”, respectively. Observed that vision-language mod-
els excel in processing explicit semantics but often struggle with
implicit semantics.
shot segmentation within open scenarios [16, 48, 49, 68].
Powered by the large-scale high-quality training dataset,
Segment Anything Model (SAM) [16] has demonstrated its
powerful capacity to be generalized into different tasks and
data distributions unseen during training. However, SAM
heavily relies on manual prompts such as bounding boxes
and points to segment objects of interest since it is pre-
trained in a class-agnostic manner. Consequently, achieving
target-aware adaptation of SAM for different tasks plays a
crucial role in bridging the gap between class-agnostic pre-
dictions and industrial needs.
Language-guided segmentation has emerged as a
promising approach to customize SAM for specific down-
stream tasks. By leveraging the cross-modal alignment of
the vision-language model [35], referring segmentation can
be accomplished by accurately depicting the foreground tar-
gets. Nevertheless, there are numerous visual targets that
are difficult to accurately express by textual description.
For example, the salient object detection task requires vi-
sual comparison among different regions within an image,
which poses challenges in aligning it with the text encoder
as evident in Figure 2.
One feasible way to capture contextual information is to
utilize sufficient training samples to fine-tune the founda-
tion models. Owing to the substantial number of parame-
ters in foundation models, conducting full fine-tuning for
adaptation leads to a significant computational burden. Re-
cent advancements have been directed toward developing
Parameter-Efficient Fine-Tuning (PEFT) methods. These
methods integrate adapters [4, 10] or LoRA [11] blocks
while keeping the backbone network frozen, as indicated
in Figure 1(c). They aspire to attain promising performance
comparable to the paradiam of full finetuning. Neverthe-
less, PEFT methods necessitate the calculation of gradients
in the intermediate layers of the backbone network, which
may be inaccessible due to privacy concerns. Furthermore,
these methods remain highly reliant on an abundance oftraining data. To achieve sample-efficient adaptation, recent
researches [29, 60] explore in-context learning by leverag-
ing similarity matching to generate point prompts for per-
sonalized adaptation using limited training instances. How-
ever, similarity calculations assume are prone to interfer-
ence, impeding the capture of implicit semantics.
Inspired by the model-free spirit of reinforcement learn-
ing (RL), we introduce a unified framework for adapt-
ing SAM to diversified scenarios while keeping parame-
ters of the backbone network frozen. By formulating auto-
matic prompting as a sequential decision-making process,
an agent is trained to imitate human annotators to recom-
mend prompting positions. The RL framework is built by
an actor branch and a critic branch networks, which collab-
orate to learn the optimal prompting policy to efficiently re-
fine the segmentation outputs. Moreover, a semantic recali-
bration module is additionally introduced to provide reliable
labels for the selected positions. It contributes to delineating
foreground and background areas for downstream tasks of
different types. Concretely, for tasks with explicit semantic,
a cross-modal attention module integrates visual and lin-
guistic information from the context. In tasks with implicit
semantics, a visual-similarity learning branch is presented
to capture implicit contextual concepts.
The contributions can be summarized as follows, with
Figure 1(a) showcasing its conceptual illustration.
• A general approach, termed AlignSAM, is proposed to
optimize the automatic prompting policy for efficiently
adapting foundation model to downstream tasks. By con-
structing universal actions, state and reward signals, it is
able to handle various types of downstream tasks within
a unified framework.
• A semantic recalibration module is introduced to provide
precise prompting labels for adapting the vision founda-
tion model to tasks with explicit and implicit semantics.
• Experiments conducted on various challenging segmenta-
tion tasks among existing foundation models demonstrate
the superiority of the proposed method over state-of-the-
art approaches for efficient adaptation.
2. Related Work
Image segmentation. Different from traditional
tasks [18–23, 62], leveraging richly annotated datasets [5, 6]
and advanced pretrained feature extractors [9, 43, 63], deep
image segmentation has yielded notable results in diverse
applications [26, 31, 32, 54, 61], primarily through the
development of complex network architectures and training
methodologies. For example, DeepLabV3Plus [3] in
semantic segmentation utilizes a spatial pyramid pooling
structure for multiscale contextual information. In shadow
detection, FDRNet [67] enhances training samples by
adjusting brightness to differentiate between dark non-
shadow and bright shadow areas. Meanwhile, for saliency
3206
detection, MENet [50] proposes a method for progressively
aggregating and refining features to address accuracy
challenges in cluttered scenes. However, these specialized
methods lack versatility, limiting their transferability to
different segmentation tasks.
Vision foundation models. Leveraging large-scale
datasets such as [38, 39], vision foundation models have
made significant advancements in various tasks of com-
puter vision, encompassing image classification [14, 35],
segmentation [16, 49, 68], and generation [37]. CLIP [35],
trained on massive image-caption pairs, exhibits ro-
bust zero-shot learning capabilities in cross-modal task.
Segment Anything Model [16] introduces a large-scale
segmentation dataset SA-1B and a training framework to
enable prompt-driven segmentation in a zero-shot manner.
Painter [48, 49] integrates multiple segmentation tasks
within an in-context learning framework. Despite their
effectiveness, it remains challenging for these models to
adapt to diversified downstream tasks while preserving
great interpretability as demonstrated in Sec. 1.
Parameter-Efficient Fine-Tuning. Foundation models
inherently possess a massive amount of parameters. There-
fore, directly fine-tuning all of these parameters for each
downstream task results in significant computational costs,
which limits the efficiency of specialized applications. Re-
cent efforts have been made to optimize extra learnable pa-
rameters on a small-scale, rather than those of the backbone
network. Existing approaches for efficient fine-tuning of vi-
sual parameters generally fall into three categories: prompt
tuning [15], adapter methods [4, 10, 55], and low-rank adap-
tation (LoRA) [11]. Specifically, VPT [15] injects a small
number of learnable parameters into the Transformer’s in-
put space and keeps the backbone frozen during the fine-
tuning stage. Adapter-based techniques [4, 10, 55], such
as SAM-Adapter [4], integrate tunable modules into pre-
trained large models to suit domain-specific tasks, where
MLP layers are inserted into each transformer block of the
image encoder. Furthermore, LoRA [11] hypothesizes that
the change matrix of weights during adaptation has a low
rank, and therefore only tunes a pair rank decomposition
matrix of the original linear layers weights, while keeping
the pre-trained weights frozen. However, most of these
algorithms heavily rely on sufficient training data during
adaptation and require gradient calculation of the interme-
diate layers of the foundation model, resulting the lack of
sample-efficiency and model-agnostic property.
Reinforcement learning for computer vision. Rein-
forcement learning has been successfully applied to vari-
ous vision scenarios including image classfication [33, 42],
object tracking [30, 58] and semantic segmentation [2, 36].Notably, RAM [33] formulated attention-based image pro-
cessing as a sequential control problem and integrated re-
inforcement learning and recurrent neural network for ob-
ject recognition task. The reinforced active segmenta-
tion method [2] achieves comparable results with signifi-
cantly less annotated data than other weakly supervised ap-
proaches. Inspired by the model-free sprit of reinforcement
learning, we train a universal agent to efficently prompt the
vision foundation model, constructing a unified framework
that can scale to diversified downstream tasks.
3. Methodology
In this section, we first present a revisit of the Segment Any-
thing Model (SAM) [16] and then introduce the task to be
addressed in this paper. Next, we delve into how to achieve
the pipeline of the proposed framework and introduce the
semantic recalibration module algorithm. Finally, we out-
line the training and evaluation processes employed in our
approach.
3.1. Preliminary
Revisit of Segment Anything Model. The Segment Any-
thing Model (SAM) is proposed as a vision foundation
model for achieving a unified capacity of segmentation. In
this case, given a set of prompts, i.e., foreground and back-
ground points, bounding boxes, or a mask, SAM is capa-
ble of segmenting the prompted target. In detail, the SAM
model is made up of an image encoder EI(·), a prompt en-
coderEP(·)and a mask decoder DM(·). During model in-
ference, the image encoder and the prompt encoder are first
utilized to encode the given image xand a prompt set P
respectively, which can be formulated as follows:
FI=EI(x), P t=EP(P), (1)
where FI∈Rh×w×candPt∈Rk×c, with kdenoting
the amount of prompts. Further, the image feature FIand
prompt embedding Ptwill be fed into DM(·)to generate
the prediction mask Mas follows:
M=DM(FI, Pt). (2)
Overview. In this paper, we present a novel framework
termed AlignSAM for adapting SAM to diversified scenar-
ios while keeping its parameters frozen. First, we formulate
automatic prompting as a sequential decision-making pro-
cess and introduce Target-aware Reinforcement Learning to
construct a unified pipeline to handle tasks with diversified
objects. Specifically, we train a reinforcement agent to im-
itate human annotators to recommend prompting positions,
which executes a series of prompting actions to refine the
segmentation progressively. Moreover, a Semantic Recali-
bration Module is additionally proposed to delineate fore-
ground and background areas, providing precise label for
the selected prompts.
3207
Negative PromptPositive Prompt
“Person”Image 
EncoderMask
DecoderPrompt
Encoder
CLIP
SurgerySemantic 
Recalibration 
Module
Explicit Branch
Implicit Branch
Vision -Language PriorCurrent Prediction
Image EmbeddingsPrompt GenerationPolicy
NetworkReward SignalAgent Frozen SAMLegend
Element -wise
Multiply
Spatial Feature
Semantic Feature
Frozen
Tuneable
Prediction of 
SRM Semantic Switch
Figure 3. An overview of the proposed AlignSAM, which consists of a Segment Anything Model (SAM), a vision-language model (CLIP-
Surgery [27]), a reinforcement learning agent, and a semantic recalibration module. The frozen SAM receives the point prompts generated
by the agent and semantic recalibration module, dealing with various downstream scenarios without relying on manual prompting.
3.2. Target-aware Reinforcement Learning
We train a target-aware reinforcement agent to interact with
the environment to maximize the accumulated rewards. The
objective of the agent is to recommend optimal prompting
positions to facilitate progressive refinement of the mask
prediction by the segmentation model. To be specific, dur-
ing each interaction loop, the agent will first select an action
according to current state observation and subsequently re-
ceive a signal of reward from the environment. Here, we
first present how to construct the state space, action space,
and the reward function separately in our RL framework,
and then illustrate the process of model training.
Action space. To construct the action space, a naive ap-
proach is to regard each pixel as a candidate action in the
image. However, it is cost intensive and in-efficient. To
relieve the computation burden while maintaining the local
details of image, we propose to construct action space in the
patch-level rather than pixel-level for each image. Specifi-
cally, we propose to divide each image into different regions
to build the action space for reinforced learning. Given an
input image x∈RH×W×C, we divide it into image patches
xp∈RH′×W′×C. The candidate action set is the center
points of each patch in an image, which can be formulated
as follows:
A={(hi, wi)|(hi, wi) =center (xp)}. (3)
Therefore, the action set consists of N=H
H′·W
W′candidate
points in an image. For each image, the goal is to select
the optimal action among the Ncandidates at each time-step, and the process will be terminated after Ttime-steps
in total.
State space. At each timestep, the agent determines
which action to execute based on the current state. In or-
der to facilitate the agent to perform gainful decisions, the
state must encompass comprehensive information about the
environment. Therefore, we combine the visual feature em-
bedding and the iterative prediction to form the state repre-
sentation. For an image sample xat the timestep t, the state
representation can be formulated as:
st=EI(x)·Mt−1, (4)
where Mt−1denotes the mask predicted by the mask de-
coder of SAM at last step. The feature extracted by the
image encoder represents the inherent spatial feature of the
sample, while the information derived from the previous
prediction maintains the contextual continuity across the se-
quential dimension. The multiplication operation highlights
the features of regions that are more likely to be recognized
as foreground in the historical iteration round and dimin-
ishes the features of regions that are prone to be classified
as background. In this way, the policy network maps the
state to a probability distribution over the actions, enabling
the optimization of the maximum expected reward in the
full training process.
Reward function. Previous RL-based methods for com-
puter vision tasks, such as [2, 17], defined the reward func-
tion to improve the task performance between iterations.
3208
Working as a binary segmentation task, the performance
metrics are diversified across segmentation scenarios. To
relieve the burden of hand-crafted function engineering, we
formulate the reward function as the score of querying the
target foreground. At each timestep, if the agent’s action
points to the foreground region, it earns a reward of +1;
otherwise, it incurs a penalty of -1as feedback.
Training RL model. We employ a widely-used proximal
proxy optimization (PPO) algorithm [40] to instantiate the
RL framework, which consists of an actor network πθand
a critic network Vθ. The actor explores and improves its
policy by interacting with the environment by mapping the
state to a probability distribution over all the actions. Mean-
while, the critic is trained in a supervised manner to predict
the state value of the current policy. The actor and critic
work in a cooperative manner to learn the optimal policy.
During the training phase, the RL networks will be
trained by Eepisodes with each episode containing T
prompting steps. At the beginning of each episode, the
prompt set P0for each image is initialized as a pair of posi-
tive and negative prompt points randomly selected from the
sample itself. After the first inference prompted by P0, the
initial state s0can be calculated according to Eq. 4. At each
prompting step t, the agent selects an action from the proba-
bility distribution predicted by the actor network, which can
be formulated as follows:
at∼pt(·) =πθ(·|st), (5)
where ptrepresents the probability distribution across the
action space. After action atis executed, the agent receives
a scalar reward rtfrom the environment. At each iteration,
the tuple comprising the current state, action, reward, and
next state is stored in a memory buffer. When an episode
is completed, the parameters of πθandVθwill be updated
using stochastic gradient descent (SGD) for Kepochs. To
form the optimization target, the action value function is
first calculated as follows:
Q(st, at) =rt+γrt+1+...+γT−tVθ(sT), (6)
where γdenotes a discount factor to balance between the
current reward and future expected values. The action value
represents the expected cumulative reward after taking ac-
tionatunder state st. Furthermore, the advantage function
can formulated as follows:
At=Q(st, at)− Vθ(st), (7)
where Vθ(st)denotes the state value estimated by the critic
network. The advantage function measures the expected
value of taking specific action under a given state over the
average performance. To improve the training stability, im-
portance resampling is utilized during the optimization ofAlgorithm 1: Training procedure of the RL model.
Input: Input image xand its ground truth mask y,
SAM’s image encoder EI(·), prompt
encoder EP(·), mask decoder DM(·)
Output: Optimal parameters of actor network πθ
and critic network Vθ
1fore= 1,2, ..., E do
2 P0={ap, an} ∼y
3 M0=DM(EI(x), P0)
4 s1=EI(x)·M0.
5 fort= 1,2, ..., T do
6 at∼πθ(·|st)
7 Pt=Pt−1∪ {at}
8 Mt=DM(EI(x), Pt)
9 st=EI(x)·Mt.
10 rt=+1ify(at)=1 else -1
11 // Compute advantage estimates
12 fort=T, T−1, ...,1do
13 ˆAt=Q(st, at)− Vθ(st)
14 // Update πθandVθwithKepochs
15 fork= 1,2, ..., K do
16 πθ←πθ+∇πθLact
17 Vθ← V θ− ∇VθLcri
policy model. Specifically, the probability ratio γθ(t)be-
tween the previous and current actor models is formulated
as follows:
γθ(t) =πθ(at|st)
πθ(at−1|st−1). (8)
With the advantage function denoted by Atat timestep t,
the objective function of the actor network is formulated as
follows:
Lact=E
t∈[1,T]
min(γθ(t)At, clip(γθ(t),1−ϵ,1+ϵ)At
,
(9)
where clip(γθ(t),1−ϵ,1 +ϵ)is a clipping function to con-
strain the value of γθ(t)to the interval between (−ϵ,1 +ϵ),
andϵis a hyperparameter to control the magnitude of model
update. Concurrently, the critic network is optimized to
minimize the discrepancy between the estimated and the ac-
tual values. The training objective of the critic network can
be formulated as follows:
Lcri=E
t∈[1,T]
(Q(st, at)− Vθ(st))2
. (10)
The training process of the RL model is summarized in Al-
gorithm 1.
3209
3.3. Semantic Recalibration Module
After tuning with reward, the reinforcement learning agent
tends to prompt positions of foreground target for the seg-
mentation model. A naive prompting policy is to regard
all the selected points as target foreground. However, it is
unrealistic and overly idealistic due to the limited capacity
of the action space. To generate prompts with reliable la-
bel information, we design a semantic recalibration module
(SRM) to transition the prompting from a coarse-grained
policy to a fine-grained one. This module can generalize
to tasks with explicit and implicit semantics by switchable
branches. Eventually, the SRM output serves as the refer-
ence mask for the selected action to construct the complete
prompt information.
Specifically, to enhance the RL agent’s comprehen-
sion of contextual information, we propose to recalibrate
the state representation by considering both the semantic-
dominated and spatial-dominated aspects of the environ-
ment. Following prior research [27], we adopt an ap-
proach to encode explicit semantic context using the text-
guided attention map derived from the visual-language
model CLIP [35], achieved without requiring additional
training. The attention map, denoted as Mc, is derived from
the pairwise similarity between the image feature and the
corresponding text feature extracted by the visual-language
model. As for the spatial-dominated state, we employ the
previous probability map predicted by the mask decoder
of SAM to recalibrate the state, as it contains historical
information learned from the visual samples. Finally, the
semantic-dominated and the spatial-dominated states are
formulated as the multiplication between their correspond-
ing attention masks and the inherent feature of the sample,
which can be formulated as follows:
sc=EI(x)·Mc, st=EI(I)·Mt−1. (11)
Subsequently, the SRM receives states representation as in-
put to predict the prompting mask for each candidate posi-
tion, which can be formulated as:
yr=K(V(sc, st)), (12)
where K(·)represents the classification head and V(·,·)is
a semantic switch which chooses the implicit branch or ex-
plicit branch to execute, depending on the type of down-
stream task.
Implicit branch. The implicit branch is designed for seg-
mentation tasks with abstract concept such as saliency de-
tection, where the text feature is unavailable, since the
prompt “salient object” is implicit semantics, as observed
in Figure 2. Concretely, the V(∅, st)is utilized in Eq. 12
and formulated as two convolution blocks to extract the tar-
geted spatial feature without noisy attention from the text
feature.Name Task Branch
CUHK [41] Blur Detection I
SBU [45] Shadow Detection E
MSD [56] Glass Detection I
DUTS [47] Saliency Detection I
PASCAL [6] Semantic Segmentation E
Table 1. Summary of datasets used in our benchmark. The term
“Branch” refers to the semantic branch allocated for this dataset,
with “I” representing the implicit branch and “E” denoting the ex-
plicit branch.
Explicit branch. The explicit branch is designed for con-
crete object segmentation. Since textual information allows
for a more comprehensive understanding of the concrete
content in the image, we utilize V(sc, st)and formulate it
as three convolution layers to aggregate the two states fol-
lowed by a self-attention layer to capture the relationships
and dependencies between different elements in an image.
Finally, the feature is sent to two convolution layers to con-
duct the mask decoding.
Training objective. The training objective to optimize the
proposed implicit and explicit branches is a combination
of the Dice loss [65] and the binary cross-entropy loss [3],
which are both widely adopted in image segmentation:
Lseg=Ldice(yr, y′) +Lbce(yr, y′), (13)
where y′denotes the ground truth mask of such a sample x
downsampled to the same size of yr. During training, the
SRM module is updated over qiterations after the execution
of the action. During evaluation, the prompt set should be
initialized as a positive point and a negative point respec-
tively responsible for the predicted probabilities with the
highest and the lowest scores by the SRM module. During
evaluation, the prompt set will be initialized as a positive
point and a negative point with the highest and the lowest
scores predicted as foreground areas by the SRM module.
4. Experiments
4.1. Experimental Setups
Datasets. We validate AlignSAM in various challenging
benchmark datasets. The detailed dataset partition is shown
in Table 1. For quantitative comparison, we follow the
previously used evaluation metrics in each task, including
mean intersection over union (mIoU), mean absolute error
(MAE), balance error rate (BER) [44], F-measure ( Fβ) [1],
and E-measure ( Eϕ) [7]. For each segmentation task, we
randomly choose select 50 samples as the training set and
utilize the original testing set for evaluation.
Implementation details. All the experiments are per-
formed on a single NVIDIA A100 GPU with 80 GB mem-
3210
MethodBlur Shadow Glass Saliency
CUHK [41] SBU [45] MSD [56] DUTS [47]
mIoU ↑Fβ↑ mIoU ↑ BER ↓ mIoU ↑Fβ↑Eϕ↑ MAE ↓
SAMed [59] 55.44 71.68 17.24 42.84 41.35 52.67 76.41 0.104
SEEM [68] 60.84 67.45 19.52 48.01 32.35 37.44 59.88 0.326
Painter [48] 18.61 27.25 9.31 47.89 8.62 14.85 81.06 0.113
PerSAM [60] 55.84 71.59 18.68 49.51 31.18 38.00 64.13 0.257
Ours 68.47 76.89 30.78 34.62 45.44 57.28 78.21 0.082
Ours-w/o RL 59.75 70.98 25.62 37.57 33.41 46.91 74.19 0.086
Ours-w/o SRM 66.89 73.15 21.29 42.72 31.92 36.76 30.52 0.587
Ours-w/o MSI 46.92 67.02 18.65 42.91 35.86 47.95 68.59 0.134
Table 2. Comparison results with existing SOTA approaches and ablation study results on four different segmentation tasks. The best
performance among all approaches is highlighted in blod . “RL”, “SRM”, “MSI” represent reinforcement learning, semantic recalibration
module, multi-step interaction, respectively.
Method mIoU Bottle Car Sheep Cat Chair Dog Person Sofa Cow Horse
MSA [51] 47.12 31.56 41.05 56.47 60.65 19.79 55.95 38.81 33.97 60.38 61.99
SAMed [59] 51.42 36.13 48.72 60.59 58.46 16.22 71.48 49.08 47.65 74.33 71.72
SEEM [68] 52.30 36.95 44.95 63.93 84.46 19.67 66.40 63.87 47.89 73.74 74.10
Painter [48] 59.27 35.33 61.93 77.29 63.16 29.90 59.82 46.40 55.58 78.71 72.82
PerSAM [60] 53.02 36.03 46.94 64.42 69.39 22.28 67.25 49.07 36.84 68.79 65.56
Ours 62.09 48.09 66.13 73.12 85.99 31.68 79.84 64.72 61.63 72.97 75.10
Ours-w/o RL 54.06 41.69 56.28 69.30 80.80 26.84 69.16 50.83 48.20 62.74 72.91
Ours-w/o SRM 27.73 19.78 27.42 30.33 39.29 14.08 29.16 24.91 30.39 31.32 29.02
Table 3. Comparison results with existing SOTA approaches and ablation study results on Pascal-VOC 2012. We report the mean IoU of
all 20 categories and the IoU of 10 randomly selected categories. The best performance among all approaches is highlighted in bold .
ory. We export Adam as the optimizer conducted on all ex-
periments, with a learning rate of 1.0×10−4. To construct
the action space, we set both height and width of each image
to 800, and each patch within an image is assigned dimen-
sions of 80 for both height and width. In the training of the
RL agent, the hyperparameters γandϵare assigned the val-
ues of 0.99 and 0.20, respectively. Additionally, we set the
episode Eto 50 and set the epoch Kto 20. Furthermore,
we fix the number of interaction rounds Tto 15. For the im-
plicit branch, Qis set to 1, wherea for the explicit branch, it
is set to 5.
Baselines. We compare our proposed AlignSAM with
following state-of-the-art (SOTA) approaches based
on foundation models. Specifically, MSA [51] and
SAMed [59] are parameter-efficient fine-tuning methods
based on adapter [10] and LoRA [11]; Painter [48] and
SEEM [68] are other foundation segmentation models;
PerSAM [60] is an in-context learning variant of SAM.
4.2. Results
Quantitative results. Table 2 showcases the compara-
tive performance of the proposed AlignSAM and other
SOTA efficient tuning methods on four distinct segmenta-
tion tasks: blur detection, shadow detection, glass detec-
tion, and salient object detection. Notably, AlignSAM out-
performs other SOTA methods in the majority of reported
InputGTPainterPerSAMOurs
(a)(b)(c)(d)(e)
Figure 4. Qualitative comparisons between our and other methods.
(a) Blur detection. (b) Shadow detection. (c) Glass detection. (d)
Semantic segmentation.
benchmarks across various evaluation metrics.
To verify the effectiveness of our method in explict tasks,
we further illustrate the comparison results for 20 classes
on the Pascal-VOC dataset as shown in Table 3. Notably,
AlignSAM exhibits superior performance across almost all
3211
Blur Shadow Glass Saliency Semantic
mIoU mIoU mIoU Eϕ mIoU
Implicit Branch 68.47 19.40 45.44 78.21 46.03
Explicit Branch 47.98 30.78 25.72 69.31 62.09
Table 4. Ablation study results of different branches on SAM.
Concerning the explicit branch, the text prompts for segmentation
tasks are arranged from left to right as follows: “defocus back-
ground”, “shadow”, “glass”, “salient objects” and the category
names (such as bottle, car) respectively.
T 1 5 10 15 20
Bottle 32.59 40.74 46.42 48.09 48.70
Car 42.55 60.15 65.04 66.13 66.54
Sheep 54.69 67.52 71.16 73.12 74.00
Cat 74.59 81.37 83.86 85.99 86.13
Table 5. Ablation study results w.r.t. iterative numbers.
the scenarios, achieving comparable results to the best-
performing competitor for only a few categories.
Qualitative results. The comparisons of qualitative re-
sults between ours and previous SOTA algorithms on var-
ious segmentation tasks are shown in Figure 4. It can be
observed that our AlignSAM is able to handle diverse chal-
lenging scenarios and produce more accurate results.
4.3. Ablation Study
Effect of each individual component. Table 2 presents
a detailed abalation result of each component of Align-
SAM, showcasing their individual efficacy. For example,
for the blur detection task, the removal of reinforcement
learning, relying solely on random action selection, re-
sulted in an 8.72% reduction in mIoU. Additionally, exclud-
ing the semantic recalibration module caused a 1.58% de-
crease in mIoU, underscoring its role in generating precise
point prompt labels. Notably, there is a significant 21.55%
decrease in mIoU when SAM failed to engage in multi-
step interaction during inference, emphasizing the pivotal
contribution of iterative progress. The multi-step iterative
point prompt selection significantly improves mask accu-
racy compared to using single point prompt. Furthermore,
the ablation experiments conducted on Pascal-VOC also
validate the effect of AlignSAM as illustrated in Table 3.
Effect of semantic switch. We also examine performance
differences between the implicit and explicit branches in
the semantic switch in Table 4. Abstract concept segmen-
tation, such as saliency detection, shows superior perfor-
mance when utilizing image state features only. This is
due to the implicit semantic acquired from the text model
is unreliable and can be detrimental to visual state features.
Conversely, in concrete object segmentation, such as se-
mantic segmentation, the utilization of leveraging hybrid
Figure 5. Several examples to illustrate the iterative point selec-
tion and the corresponding segmentation results. The sequence
progresses from left to right, showing a gradual increase in the
number of point prompts.
vision-language features achieves better performance. The
text prompt containing the category name provides valuable
guidance for segmentation.
Effect of multi-step iterative point selection. We visual-
ize the selection of iterative point prompts and report their
corresponding segmentation results in Figure 5. In simple
object segmentation scenarios (row 1), the model demon-
strates effectiveness with only a few point prompts and
more prompt points can lead to improved accuracy in the
boundary of objects. However, in scenarios involving mul-
tiple objects, it requires more point prompts to segment the
complete foreground areas (row 2). This limitation is also
evident in low-level segmentation tasks, such as defocus
blur detection (row 3). Feeding the foundation model with
an adequate number of precise point prompts can signifi-
cantly enhance the segmentation precision. Table 5 further
substantiates these observations, confirming the benefits of
the multi-step iterative point selection process.
5. Conclusion
In this paper, we propose a novel framework for automatic
prompting to align SAM to open context via reinforcement
learning. This unveiling of the SAM unleashes its potential
across diverse downstream tasks while maintaining its
adaptability intact. Initially, we devise a reinforcement
learning agent to discern informative points for prompt-
ing. Subsequently, a semantic recalibration module is
introduced to ensure the precise binary classification of
the selected prompts. The RL agent and the recalibration
module flexibly explore visual and linguistic knowledge to
address the implicit and explicit semantics tasks in a unified
framework. Extensive experiments conducted on diverse
benchmarks confirm the efficacy of the proposed method.
3212
References
[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada,
and Sabine Susstrunk. Frequency-tuned salient region de-
tection. In 2009 IEEE conference on computer vision and
pattern recognition , pages 1597–1604. IEEE, 2009. 6
[2] Arantxa Casanova, Pedro O Pinheiro, Negar Rostamzadeh,
and Christopher J Pal. Reinforced active learning for image
segmentation. arXiv preprint arXiv:2002.06583 , 2020. 3, 4
[3] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
Proceedings of the European conference on computer vision
(ECCV) , pages 801–818, 2018. 2, 6
[4] Tianrun Chen, Lanyun Zhu, Chaotao Deng, Runlong Cao,
Yan Wang, Shangzhan Zhang, Zejian Li, Lingyun Sun, Ying
Zang, and Papa Mao. Sam-adapter: Adapting segment
anything in underperformed scenes. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 3367–3375, 2023. 1, 2, 3
[5] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 3213–3223, 2016. 2
[6] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. International journal of computer
vision , 88:303–338, 2010. 2, 6
[7] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-
Ming Cheng, and Ali Borji. Enhanced-alignment measure
for binary foreground map evaluation. In Proceedings of the
Twenty-Seventh International Joint Conference on Artificial
Intelligence, IJCAI-18 , pages 698–704. International Joint
Conferences on Artificial Intelligence Organization, 2018. 6
[8] Hao He, Xiangtai Li, Guangliang Cheng, Jianping Shi, Yun-
hai Tong, Gaofeng Meng, V ´eronique Prinet, and LuBin
Weng. Enhanced boundary learning for glass-like object seg-
mentation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 15859–15868, 2021.
1
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 2
[10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for nlp. In International Conference on Machine
Learning , pages 2790–2799. PMLR, 2019. 2, 3, 7
[11] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
LoRA: Low-rank adaptation of large language models. In In-
ternational Conference on Learning Representations , 2022.
2, 3, 7
[12] Xiaowei Hu, Tianyu Wang, Chi-Wing Fu, Yitong Jiang,
Qiong Wang, and Pheng-Ann Heng. Revisiting shadow de-tection: A new benchmark dataset for complex world. IEEE
Transactions on Image Processing , 30:1925–1934, 2021. 1
[13] Duojun Huang, Jichang Li, Weikai Chen, Junshi Huang,
Zhenhua Chai, and Guanbin Li. Divide and adapt: Active
domain adaptation via customized learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 7651–7660, 2023. 1
[14] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
conference on machine learning , pages 4904–4916. PMLR,
2021. 3
[15] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In European Conference on Computer
Vision , pages 709–727. Springer, 2022. 3
[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and
Ross Girshick. Segment anything. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 4015–4026, 2023. 2, 3
[17] Debang Li, Huikai Wu, Junge Zhang, and Kaiqi Huang.
Fast a3rl: Aesthetics-aware adversarial reinforcement learn-
ing for image cropping. IEEE Transactions on Image Pro-
cessing , 28(10):5105–5120, 2019. 4
[18] Jichang Li, Si Wu, Cheng Liu, Zhiwen Yu, and Hau-San
Wong. Semi-supervised deep coupled ensemble learning
with classification landmark exploration. IEEE Transactions
on Image Processing , 29:538–550, 2019. 2
[19] Jichang Li, Guanbin Li, Yemin Shi, and Yizhou Yu. Cross-
domain adaptive clustering for semi-supervised domain
adaptation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
2505–2514, 2021.
[20] Jichang Li, Guanbin Li, Feng Liu, and Yizhou Yu. Neighbor-
hood collective estimation for noisy label identification and
correction. In European Conference on Computer Vision ,
pages 128–145. Springer, 2022.
[21] Jichang Li, Guanbin Li, Hui Cheng, Zicheng Liao, and
Yizhou Yu. Feddiv: Collaborative noise filtering for
federated learning with noisy labels. arXiv preprint
arXiv:2312.12263 , 2023.
[22] Jichang Li, Guanbin Li, and Yizhou Yu. Adaptive between-
ness clustering for semi-supervised domain adaptation. IEEE
Transactions on Image Processing , 2023.
[23] Jichang Li, Guanbin Li, and Yizhou Yu. Inter-domain mixup
for semi-supervised domain adaptation. Pattern Recognition ,
2023. 2
[24] Jiaming Li, Jiacheng Zhang, Jichang Li, Ge Li, Si Liu, Liang
Lin, and Guanbin Li. Learning background prompts to dis-
cover implicit knowledge for open vocabulary object detec-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2024. 1
[25] Luoqin Li, Jiabing Wang, Jichang Li, Qianli Ma, and Jia Wei.
Relation classification via keyword-attentive sentence mech-
3213
anism and synthetic stimulation loss. IEEE/ACM Transac-
tions on Audio, Speech, and Language Processing , 27(9):
1392–1404, 2019. 1
[26] Wenxue Li, Xinyu Xiong, Siying Li, and Fugui Fan. Hy-
bridvps: Hybrid-supervised video polyp segmentation under
low-cost labels. IEEE Signal Processing Letters , 2023. 2
[27] Yi Li, Hualiang Wang, Yiqun Duan, and Xiaomeng Li. Clip
surgery for better explainability with enhancement in open-
vocabulary tasks, 2023. 2, 4, 6
[28] Yonglin Li, Jing Zhang, Xiao Teng, and Long Lan.
Refsam: Efficiently adapting segmenting anything model
for referring video object segmentation. arXiv preprint
arXiv:2307.00997 , 2023. 1
[29] Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong
Wang, and Chunhua Shen. Matcher: Segment anything with
one shot using all-purpose feature matching. arXiv preprint
arXiv:2305.13310 , 2023. 1, 2
[30] Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong
Zhang, and Yizhou Wang. End-to-end active object track-
ing via reinforcement learning. In International conference
on machine learning , pages 3286–3295. PMLR, 2018. 3
[31] Jie Ma, Chuan Wang, Yang Liu, Liang Lin, and Guanbin
Li. Enhanced soft label for semi-supervised semantic seg-
mentation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 1185–1195, 2023. 2
[32] Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza,
Nasser Kehtarnavaz, and Demetri Terzopoulos. Image seg-
mentation using deep learning: A survey. IEEE transactions
on pattern analysis and machine intelligence , 44(7):3523–
3542, 2021. 2
[33] V olodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recur-
rent models of visual attention. Advances in neural informa-
tion processing systems , 27, 2014. 3
[34] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu.
Multi-scale interactive network for salient object detection.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 9413–9422, 2020. 1
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2, 3, 6
[36] Md Alimoor Reza and Jana Kosecka. Reinforcement learn-
ing for semantic segmentation in indoor scenes. arXiv
preprint arXiv:1606.01178 , 2016. 3
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 3
[38] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114 , 2021. 3
[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, TheoCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. Advances in Neural In-
formation Processing Systems , 35:25278–25294, 2022. 3
[40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347 , 2017. 5
[41] Jianping Shi, Li Xu, and Jiaya Jia. Discriminative blur de-
tection features. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2965–
2972, 2014. 6, 7
[42] Dongseok Shim and H Jin Kim. Gaussian ram: Lightweight
image classification via stochastic retina-inspired glimpse
and reinforcement learning. In 2020 20th International Con-
ference on Control, Automation and Systems (ICCAS) , pages
155–160. IEEE, 2020. 3
[43] K Simonyan and A Zisserman. Very deep convolutional net-
works for large-scale image recognition. In International
Conference on Learning Representations , 2015. 2
[44] Tom ´as F Yago Vicente, Minh Hoai, and Dimitris Samaras.
Leave-one-out kernel optimization for shadow detection. In
Proceedings of the IEEE International Conference on Com-
puter Vision , pages 3388–3396, 2015. 6
[45] Tom ´as F Yago Vicente, Le Hou, Chen-Ping Yu, Minh Hoai,
and Dimitris Samaras. Large-scale training of shadow de-
tectors with noisily-annotated shadow examples. In Com-
puter Vision–ECCV 2016: 14th European Conference, Am-
sterdam, The Netherlands, October 11-14, 2016, Proceed-
ings, Part VI 14 , pages 816–832. Springer, 2016. 6, 7
[46] Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash
Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin
Mehta, Mohammad Rastegari, Oncel Tuzel, and Hadi
Pouransari. Sam-clip: Merging vision foundation models
towards semantic and spatial understanding. arXiv preprint
arXiv:2310.15308 , 2023. 1
[47] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,
Dong Wang, Baocai Yin, and Xiang Ruan. Learning to de-
tect salient objects with image-level supervision. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2017. 6, 7
[48] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and
Tiejun Huang. Images speak in images: A generalist
painter for in-context visual learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6830–6839, 2023. 2, 3, 7
[49] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,
Chunhua Shen, and Tiejun Huang. Seggpt: Towards seg-
menting everything in context. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 1130–1140, 2023. 2, 3
[50] Yi Wang, Ruili Wang, Xin Fan, Tianzhu Wang, and Xi-
angjian He. Pixels, regions, and objects: Multiple en-
hancement for salient object detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10031–10040, 2023. 1, 3
[51] Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei
Wang, Yanwu Xu, Yueming Jin, and Tal Arbel. Medical sam
3214
adapter: Adapting segment anything model for medical im-
age segmentation. arXiv preprint arXiv:2304.12620 , 2023.
7
[52] Si Wu, Guangchang Deng, Jichang Li, Rui Li, Zhiwen
Yu, and Hau-San Wong. Enhancing triplegan for semi-
supervised conditional instance synthesis and classification.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 10091–10100, 2019.
1
[53] Si Wu, Jichang Li, Cheng Liu, Zhiwen Yu, and Hau-San
Wong. Mutual learning of complementary networks via
residual correction for improving semi-supervised classifi-
cation. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 6500–6509,
2019. 1
[54] Xinyu Xiong, Siying Li, and Guanbin Li. Unpaired image-
to-image translation based domain adaptation for polyp seg-
mentation. In International Symposium on Biomedical Imag-
ing, pages 1–5. IEEE, 2023. 2
[55] Xinyu Xiong, Churan Wang, Wenxue Li, and Guanbin Li.
Mammo-sam: Adapting foundation segment anything model
for automatic breast mass segmentation in whole mammo-
grams. In International Workshop on Machine Learning in
Medical Imaging , pages 176–185. Springer, 2023. 3
[56] Xin Yang, Haiyang Mei, Ke Xu, Xiaopeng Wei, Baocai Yin,
and Rynson WH Lau. Where is my mirror? In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 8809–8818, 2019. 6, 7
[57] Letian Yu, Haiyang Mei, Wen Dong, Ziqi Wei, Li Zhu, Yuxin
Wang, and Xin Yang. Progressive glass segmentation. IEEE
Transactions on Image Processing , 31:2920–2933, 2022. 1
[58] Da Zhang, Hamid Maei, Xin Wang, and Yuan-Fang Wang.
Deep reinforcement learning for visual object tracking in
videos. arXiv preprint arXiv:1701.08936 , 2017. 3
[59] Kaidong Zhang and Dong Liu. Customized segment any-
thing model for medical image segmentation. arXiv preprint
arXiv:2304.13785 , 2023. 1, 7
[60] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junt-
ing Pan, Hao Dong, Peng Gao, and Hongsheng Li. Person-
alize segment anything model with one shot. arXiv preprint
arXiv:2305.03048 , 2023. 1, 2, 7
[61] Zhicheng Zhang, Song Chen, Zichuan Wang, and Jufeng
Yang. Planeseg: Building a plug-in for boosting planar re-
gion segmentation. IEEE Transactions on Neural Networks
and Learning Systems , 1(1):1–15, 2024. 2
[62] Zhicheng Zhang, Junyao Hu, Wentao Cheng, Danda Paudel,
and Jufeng Yang. Extdm: Distribution extrapolation dif-
fusion model for video prediction. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2024. 2
[63] Zhicheng Zhang, Pancheng Zhao, Eunil Park, and Jufeng
Yang. Mart: Masked affective representation learning via
masked temporal distribution distillation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2024. 2
[64] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao,
Jufeng Yang, and Ming-Ming Cheng. Egnet: Edge guid-
ance network for salient object detection. In Proceedings ofthe IEEE/CVF international conference on computer vision ,
pages 8779–8788, 2019. 1
[65] Rongjian Zhao, Buyue Qian, Xianli Zhang, Yang Li, Rong
Wei, Yang Liu, and Yinggang Pan. Rethinking dice loss for
medical image segmentation. In 2020 IEEE International
Conference on Data Mining (ICDM) , pages 851–860. IEEE,
2020. 6
[66] Quanlong Zheng, Xiaotian Qiao, Ying Cao, and Rynson WH
Lau. Distraction-aware shadow detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5167–5176, 2019. 1
[67] Lei Zhu, Ke Xu, Zhanghan Ke, and Rynson WH Lau. Miti-
gating intensity bias in shadow detection via feature decom-
position and reweighting. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 4702–
4711, 2021. 2
[68] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,
Jianfeng Gao, and Yong Jae Lee. Segment everything every-
where all at once. arXiv preprint arXiv:2304.06718 , 2023.
2, 3, 7
3215
