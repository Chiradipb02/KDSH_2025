Frozen Feature Augmentation for Few-Shot Image Classification
Andreas B ¨ar1 2 *Neil Houlsby1Mostafa Dehghani1Manoj Kumar1†
1Google DeepMind2Technische Universit ¨at Braunschweig
{andreasbaer, neilhoulsby, dehghani, mechcoder }@google andreas.baer@tu-bs.de
Project website: https://frozen-feature-augmentation.github.io
Abstract
Training a linear classifier or lightweight model on top
of pretrained vision model outputs, so-called ‘frozen fea-
tures’, leads to impressive performance on a number of
downstream few-shot tasks. Currently, frozen features are
not modified during training. On the other hand, when
networks are trained directly on images, data augmenta-
tion is a standard recipe that improves performance with
no substantial overhead. In this paper, we conduct an ex-
tensive pilot study on few-shot image classification that ex-
plores applying data augmentations in the frozen feature
space, dubbed ‘frozen feature augmentation (FroFA)’, cov-
ering twenty augmentations in total. Our study demon-
strates that adopting a deceptively simple pointwise FroFA,
such as brightness, can improve few-shot performance con-
sistently across three network architectures, three large pre-
training datasets, and eight transfer datasets.
1. Introduction
Vision transformers (ViTs) [19] achieve remarkable perfor-
mance on ImageNet-sized [43, 69] and smaller [21, 38, 41]
datasets. In this setup, data augmentation ,i.e., a predefined
set of stochastic input transformations, is a crucial ingredi-
ent. Examples for image augmentations are random crop-
ping or pixel-wise modifications that change brightness or
contrast. These are complemented by more advanced strate-
gies [13, 46, 75], such as AutoAugment [12].
A more prevalent trend is to first pretrain vision mod-
els on large-scale datasets and then adapt them downstream
[6, 8, 49, 73]. Notable, even training a simple linear classi-
fier or lightweight model on top of ViT outputs, also known
asfrozen features , can yield remarkable performance across
a number of diverse downstream few-shot tasks [16, 25, 52].
Given the success of image augmentations andfrozen fea-
tures , we ask: Can we effectively combine image augmen-
tations and frozen features to train a lightweight model?
*Work done as Research Intern at Google DeepMind.†Project lead.
1 5 10 25
shots0246top-1 accuracy
(absolute gains)JFT-3B
1 5 10 25
shotsWebLI + SigLIP
MAPwdlinear probeFigure 1. Average top-1 accuracy gains across seven few-shot test
sets (CIFAR100 [1], SUN397 [71], ...) on various few-shot set-
tings. We train on frozen features from an L/16 ViT [19] with
JFT-3B pretraining [73] or WebLI sigmoid language-image pre-
training (SigLIP) [6, 74]. Our proposed frozen feature augmenta-
tion (FroFA) method gives consistent gains over a weight decay-
regularized multi-head attention pooling [37] (MAPwd) and an L2-
regularized linear probe baseline, both without FroFA.
In this paper, we revisit standard image augmentation
techniques and apply them on top of frozen features in a
data-constrained, few-shot setting. We dub this type of aug-
mentation frozen feature augmentation (FroFA) . Inspired di-
rectly by image augmentations, we first stochastically trans-
form frozen features and then train a lightweight model on
top. Our only modification before applying image augmen-
tations on top of frozen features is a point-wise scaling such
that each feature value lies in [0,1]or[0,255].
We investigate eight (few-shotted) image classification
datasets using ViTs pretrained on JFT-3B [73], ImageNet-
21k [17], or WebLI [6]. After extracting features from each
few-shot dataset we apply twenty different frozen feature
augmentations and train a lightweight multi-head attention
pooling (MAP) [37] on top. Our major insights are:
1. Geometric augmentations that modify the shape and
structure of two-dimensional frozen features always lead
to worse performance on ILSVRC-2012 [57]. On the
other hand, simple stylistic (point-wise) augmentations,
such as brightness, contrast, and posterize, give steady
improvements on 1-, 5-, and 10-shot settings.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16046
2. Additional per-channel stochasticity by sampling inde-
pendent values for each frozen feature channel works
surprisingly well: On ILSVRC-2012 5-shot we improve
over an MAP baseline by 1.6% absolute and exceed a
well-tuned linear probe baseline by 0.8% absolute.
3. While FroFA provides modest but significant gains on
ILSVRC-2012, it excels on seven smaller few-shot
datasets. In particular, FroFA outperforms the mean 10-
shot accuracy of an MAP baseline by 2.6% and the linear
probe baseline by 5.2% absolute ( cf. Fig. 1, left).
4. Results on the same seven few-shot datasets using a We-
bLI sigmoid language-image pretrained model [74] fur-
ther emphasize the transfer capabilities of FroFA. We
observe absolute gains ranging from 5.4% on 1-shot to
0.9% on 25-shot compared to an MAP baseline while
outperforming a linear probe baseline by over 2% on 1-
shot and at least 3% on 5- to 25-shot. ( cf. Fig. 1, right).
2. Related Works
Few-shot transfer learning : State-of-the-art vision models
[6, 16, 19, 32, 55, 73] are typically pretrained on large-scale
datasets, e.g., ImageNet-21k [17] or JFT [27, 73], before
transferred to other smaller-scale ones, e.g., CIFAR10 [1],
SUN397 [70, 71], or ILSVRC-2012 [57]. Depending on the
model size, efficient transfer learning becomes a challenge.
Many methods have been proposed for large language mod-
els (LLMs), e.g., adapters [28], low-rank adaptation [29], or
prompt tuning [39], of which some have been successfully
adapted to computer vision [5, 22, 30, 76]. CLIP-Adapter
[22] builds on contrastive language-image pretraining [52]
and combines it with adapters [28]. A follow-up work [76]
proposes TiP-Adapter which uses a query-key cache model
[24, 51] instead of a gradient descent approach. Inspired by
the success of prompt tuning in LLMs [39], Jia et al. pro-
pose visual prompt tuning at the model input [30]. On the
other hand, AdaptFormer [5] uses additional intermediate
trainable layers to finetune a frozen vision transformer [19].
In contrast, we do not introduce additional prompts [30]
or intermediate parameters [5, 22] that require backpropa-
gating through the network. Instead, we train a small net-
work on top of frozen features from a ViT. This aligns with
linear probing [52] which is typically used to transfer vision
models to other tasks [16, 25, 73] — our objective.
Further, we focus on few-shot transfer learning [36, 68]
in contrast to meta- or metric-based few-shot learning
[2, 9, 48, 50, 54, 56, 59]. Kolesnikov et al. [32] and De-
hghani et al. [16] reveal that training a lightweight model
on frozen features from a large-scale pretrained backbone
yields high performance across various downstream (few-
shot) tasks. Similarly, Vasconcelos et al . [65] show that
training on frozen features gives strong performance on
object detection and segmentation. In addition, transfer
learning has also shown to be competitive or slightly betterthan meta-learning approaches [20, 63]. Building on these
works, we propose frozen feature augmentation to improve
few-shot transfer learning for image classification tasks.
Data augmentation : One go-to method to improve per-
formance while training in a low-data regime is data aug-
mentation [60]. Some prominent candidates in computer
vision are AutoAugment [12], AugMix [26], RandAugment
[12], and TrivialAugment [46]. These methods typically
combine low-level image augmentations together to aug-
ment the input. Works on augmentations in feature space
exist [18, 35, 40, 44, 67], but lack a large-scale empirical
study on frozen features of single-modal vision models.
To this end, we investigate frozen feature augmentation
by reformulating twenty image augmentations, including
a subset used in AutoAugment [12], inception crop [62],
mixup [67, 75], and patch dropout [42].
3. Framework Overview
We introduce our notations in Sec. 3.1 followed by our
caching and training pipeline in Sec. 3.2 and a description
of frozen feature augmentations (FroFAs) in Sec. 3.3.
3.1. Notation
Letx∈IH×W×3be an RGB image of height H, width
W, andI= [0,1]. A classification model processes xand
outputs class scores y∈[0,1]Sfor each class in a pre-
defined set of classes S, with S=|S|. Let LandDbe
the number of intermediate layers and the number of fea-
tures of a multi-layer classification model, respectively. We
describe the intermediate feature representations of xas
f=f(ℓ)= (f(ℓ)
d)∈RD, with layer index ℓ∈ {1, ..., L}
and feature index d∈ {1, ..., D}. In vision transform-
ers [19], f=f(ℓ)= (f(ℓ)
n,c)∈RN×Cis typically two-
dimensional, where NandCare the number of patches and
number of per-patch channels, respectively. Finally, we in-
troduce the patch index n∈ {1, ..., N}and the per-patch
channel index c∈ {1, ..., C}.
3.2. Training on Cached Frozen Features
We investigate pretrained vision transformers with Ltrans-
former blocks (TBs) followed by a multi-head attention
pooling (MAP) [37] and a classification layer (CL). Fig. 2a
presents a simplified illustration. For simplicity, we ne-
glect all operations before the first transformer block (e.g.,
patchifying, positional embedding, etc.).
To cache intermediate features, we process each image
xfrom an image dataset Dxthrough the network up until
transformer block L. Next, we store the resulting features
f. After processing the entire image dataset Dxwe obtain
a (frozen) feature dataset Df, with f∈ Df(Fig. 2b).
Lastly, we train a lightweight model using the cached
(frozen) features. Fig. 2c shows an example where a single
16047
(Frozen ) Pretrained Model
TB TB TB MAP CL(a) Step 1: Select a (frozen) pretrained model and a layer for caching.
(Frozen ) Pretrained Model
image
dataset(frozen )
feature
datasetTB TB TB
(b) Step 2: Process an image dataset and cache the (frozen) features.
Lightweight Model
(frozen )
feature
datasetMAP CL
frozen feature
augmentation
(FroFA ) 
(c) Step 3: Train on (augmented) frozen features.
Figure 2. Pipeline for caching and training on (frozen) features .
(2a): Given a (frozen) pretrained vision transformer, with Ltrans-
former blocks (TBs), a multi-head attention pooling (MAP) layer,
and a classification layer (CL), we select its L-th transformer block
for caching. (2b): Next, we feed images x∈ Dxto cache (frozen)
features f∈ Df. (2c): Finally, we use Dfto train a lightweight
model on top. We investigate frozen feature augmentation (FroFA)
af∈ Afin this scenario.
MAP layer followed by a classification layer is trained using
the feature dataset Df. Since our focus is fast training, we
defer a detailed analysis on larger models to future work.
3.3. Frozen Feature Augmentation (FroFA)
Data augmentation is a common tool to improve generaliza-
tion. However, it is typically applied on the input, or in our
case: images. How can we map such image augmentations
to intermediate transformer feature representations?
Recall that the feature representation f= (fn,c)∈
RN×C(layer index ℓomitted) is two-dimensional. We first
reshape it to a three-dimensional representation, i.e.,
f∗= (f∗
n1,n2,c)∈R√
N×√
N×C. (1)
We further define
f∗
c=f∗
:,:,c∈R√
N×√
N×1(2)
as a reshaped two-dimensional representation of the c-th
channel. Since images and features differ in two fundamen-
tal aspects, i.e., channel dimensionality and value range, we
address this next.
Channel dimensionality : RGB images have just three
channels while features can possess an arbitrary number of
channels. To address this, we simply ignore image aug-
mentations that rely on having three color channels, suchas color jitter, and include only augmentations which can
have an arbitrary number of channels instead, denoted as
Ca. This already covers a majority of commonly applied
image augmentations.
Value range : RGB values lie within a specific range I,
e.g.,I= [0,1]orI={0, ...,255} ⊂N0, while in theory
features have no such constraints. Assuming H=√
Nand
W=√
N, we define an image augmentation as
ax:I√
N×√
N×Ca→I√
N×√
N×Ca,ax∈ Ax, (3)
where Axis the set of image augmentations. To also ad-
dress the value range mismatch, we introduce a determinis-
tic feature-to-image mapping
tf→x:R√
N×√
N×Ct→I√
N×√
N×Ct(4)
that maps each element of f∗(1) from RtoI, with Ctas
the number of channels of f∗. We use
xf=tf→x(f∗) =f∗−fmin
fmax−fmin, (5)
where fminandfmaxare the minimum and maximum value
off∗, respectively, with elements of xfnow in I= [0,1].
We further define an image-to-feature mapping
tf←x:I√
N×√
N×Ct→R√
N×√
N×Ct(6)
that maps xfback to the original feature value range. In
this case, we invert (4) and use
f∗=tf←x(xf) =xf·(fmax−fmin) +fmin. (7)
Combining (3), (4), and (6), we obtain a generic (frozen)
feature augmentation as a function composition
af=tf←x◦ax◦tf→x. (8)
We now define three variations of af:
1.(Default) FroFA : We apply af(8) once across the en-
tire feature. We set Ca=Ct=Cand compute fmin
andfmaxin (5), (7) across all elements of f∗. Further,
as normally done in pixel space, ax(3) samples a ran-
dom augmentation value and changes all elements of xf
using the same value. For example, employing random
contrast in a FroFA fashion scales each element of xf
by the exact same randomly sampled factor .
2.Channel FroFA (cFroFA) : For each channel in the
mapped features xf(5),ax(3) samples a random aug-
mentation value per channel and applies that value to all
elements in that channel ( Ca= 1 while Ct=C). By
using cFroFA for our random contrast example, we ob-
tainCindependently sampled scaling factors, one for
each channel .
16048
3.Channel2FroFA (c2FroFA) : In addition to applying
augmentations per channel ( Ca= 1) as done in cFroFA,
tf→x(4) and tx←f(6) also operate per channel ( Ct=
1),i.e., onf∗
c(2). In this case, fminandfmaxare the
per-channel maximum and minimum, respectively. In
contrast, FroFA and cFroFA use the maximum and min-
imum across the entire feature. We denote this variant as
c2FroFA since both the mappings (4), (6) and the aug-
mentation (3) are applied on a per-channel basis. Al-
though not adding additional stochasticity, we found that
for random brightness this variant gives more stable re-
sults across a range of augmentation hyper parameters.
While an element-wise FroFA might seem like a natural
next step, our initial experiments lead to significantly worse
results. We hypothesize that per-element augmentations
might lead to substantial changes in the feature appearance.
4. Experimental Setup
In this section, we describe our experimental setup.
4.1. Network Architectures
We employ pretrained Ti/16 [64], B/16 [19], and L/16 [19]
vision transformers. Further, we follow Zhai et al. [73] and
use a lightweight multi-head attention pooling (MAP) [37]
before the final classification layer for training on top of
frozen features ( cf. Sec. 3.3).
4.2. Datasets
Pretraining : We consider three pretraining datasets, i.e.,
JFT-3B [73], ImageNet-21k [17], and WebLI [6]. First in-
troduced by Hinton et al. [27], JFT is now a widely used
proprietary, large-scale dataset [6, 10, 14, 19, 32, 33, 61].
We use JFT-3B [73] which consists of nearly 3 billion multi-
labeled images following a class-hierarchy of 29,593 labels.
The images are annotated with noisy labels by using a semi-
automated pipeline. We follow common practice [16, 73]
and ignore the hierarchical aspect of the labels.
ImageNet-21k contains 14,197,122 (multi)-labeled im-
ages with 21,841 distinct labels. We equally split the first
51,200 images into a validation and test set and use the re-
maining 14,145,922 images for training.
Lastly, WebLI is a web-scale multilingual image-text
dataset for vision-language training. It encompasses text in
109 languages with 10 billion images and roughly 31 billion
image-text pairs.
Few-shot transfer learning : We investigate eight
datasets for few-shot transfer learning, i.e., ILSVRC-2012
[57], CIFAR10 [1], CIFAR100 [1], DMLab [3, 72], DTD
[11], Resisc45 [7], SUN397 [70, 71], and SVHN [47].
ILSVRC-2012, alias ‘ImageNet-1k’ or just ‘ImageNet’,
stems from ImageNet-21k and contains 1,281,167 training
images of 1,000 classes. We randomly sample 1-, 5-, 10-,and 25-shot versions from the first 10% of the training set.
We further create additional disjoint sets by using the next
four 10% fractions of the training set. In addition, we follow
previous works [4] and create a ‘minival’ set using the last
1% (12,811 images) of the ILSVRC-2012 training set. The
‘minival’ set is used for hyperparameter tuning and design
decisions while the official ILSVRC-2012 validation set is
used as a test set. In summary, our setup consists of 1,000,
5,000, 10,000, or 25,000 training images, 12,811 validation
images (‘minival’), and 50,000 test images (‘validation’).
For the other seven datasets, we also select a training,
validation, and test split and create few-shot versions of the
respective training set. Similar to ILSVRC-2012, we use the
validation sets to tune hyperparameters and report final re-
sults on the test sets. A short description of each dataset and
more details can be found in the Supplementary, Sec. S2.1.
4.3. Data Augmentation
We reuse the set of augmentations first defined in AutoAug-
ment [12] and adopted in later works [13, 46]. In addition,
we consider a few other image augmentations [42, 62, 75].
We select five geometric augmentations, i.e., rotate, shear-
x, shear-y, translate-x, and translate-y; four crop & drop
augmentations, i.e., crop, resized crop, inception crop [62],
and patch dropout [42]; seven stylistic augmentations, i.e.,
brightness, contrast, equalize, invert, posterize, sharpness,
and solarize; and two other augmentations, i.e., JPEG and
mixup [75]. In Supplementary, Sec. S3.7, we also test two
additional augmentations.
In total, we end up with twenty distinct augmentations .
Note that all data augmentations incorporate random oper-
ations, e.g., a random shift in x- and y-direction (translate-
x and translate-y, respectively), a randomly selected set of
patches (patch dropout), a random additive value to each
feature (brightness), or a random mix of two features and
their respective classes (mixup). Please refer to the Sup-
plementary, Sec. S2.2, for more details. We focus on the
following set of experiments:
1. We investigate FroFA for all eighteen augmentations
(and two additional ones in Supplementary, Sec. S3.7).
2. For our top-performing FroFAs, namely, brightness,
contrast, and posterize, we incorporate additional
stochasticity using cFroFA and c2FroFA ( cf. Sec. 3.3).
3. We investigate a sequential protocol where two of
the best three (c/c2)FroFA are arranged sequentially,
namely, brightness c2FroFA, contrast FroFA, and pos-
terize cFroFA. We test all six possible combinations.
4. Finally, we also apply variations of RandAugment [13]
and TrivialAugment [46] directly on top of cached
frozen features. More details and results can be found
in the Supplementary, Secs. S2.2 and S3.2, respectively.
In Supplementary, Sec. S3.6, we complement our study by
comparing our best FroFA to input data augmentations.
16049
4.4. Training & Evaluation Details
We describe some base settings for pretraining, few-shot
learning, and evaluation. Please refer to Supplementary,
Sec. S2.3 for more training details.
Pretraining : Models are pretrained on Big Vision1.
We re-use the Ti/16, B/16, and L/16 ViTs pretrained on JFT-
3B from Zhai et al . [73]. In addition, we pretrain Ti/16,
B/16, and L/16 ViTs on ImageNet-21k following the set-
tings described by Steiner et al. [60]. We further use a pre-
trained L/16 ViT image encoder stemming from a vision-
language model from Zhai et al. [74] which follows their
sigmoid language-image pretraining (SigLIP) on WebLI.
Few-shot transfer learning : Models are transferred us-
ingScenic2[15]. We train the lightweight MAP-based
head by sweeping across five batch sizes (32, 64, 128, 256,
and 512), four learning rates (0.01, 0.03, 0.06, and 0.1),
and five training step sizes (1,000; 2000; 4,000; 8,000; and
16,000). In total, we obtain 100 configurations for each
shot, but also investigate hyperparameter sensitivity on a
smaller sweep in Supplementary, Sec. S3.5. For our exper-
iments in Secs. 6 and 7, we also sweep four weight decay
settings (0.01, 0.001, 0.0001, and 0.0, i.e., ‘no weight de-
cay’), highlighted by a ‘wd’ superscript. We use the valida-
tion set for early stopping and to find the best setting across
the sweep. Our cached-feature setup ( cf. Fig. 2) fits on a
single-host TPUv2 platform where our experiments run in
the order of minutes.
Evaluation : We report the top-1 accuracy across all our
few-shot datasets. Although we mainly report test perfor-
mance, we tune all hyperparameters and base all of our de-
sign decisions on the validation set.
4.5. Baseline Models
We establish two baselines: MAP and linear probe.
MAP : We first cache the N×C-shaped frozen features
from the last transformer block. Afterwards, we train a
lightweight MAP head ( cf. Fig. 2) from scratch following
the training protocol in Sec. 4.4. We add a ‘wd’ superscript,
i.e., MAPwd, whenever we include the weight decay sweep.
For simplicity, the MAP head employs the same architec-
tural design as the underlying pretrained model.
Linear probe : We use cached 1×C-shaped frozen
features from the pretrained MAP head to solve an L2-
regularized regression problem with a closed-form solution
[73]. We sweep the L2 decay factor using exponents of 2
ranging from −20 up to 10. This setting is our auxiliary
baseline.
1https://github.com/google-research/big_vision
2https://github.com/google-research/scenicBaseline 1-shot 5-shot 10-shot 25-shot
MAP 57.9 78.8 80.9 83.2
Linear probe 66.5 79.6 81.5 82.4
Table 1. Baseline average top-1 accuracy onourILSVRC-2012
test set. We use the JFT-3B L/16 base setup ( cf. Sec. 5) and follow
the respective baseline setting ( cf. Sec. 4.5). Each shot is sampled
five times. The best result per shot is boldfaced.
5. Finding the Optimal FroFA Setup
We focus our first investigations on an L/16 ViT pretrained
on JFT-3B, i.e., our largest model and largest pure im-
age classification pretraining dataset, followed by few-shot
transfer learning on subsets of the ILSVRC-2012 training
set,i.e., our largest few-shot transfer dataset. We will refer
to this setup as our JFT-3B L/16 base setup .
5.1. Baseline Performance
We first report the baseline performance in Tab. 1. We ob-
serve a large gap between MAP and linear probe on 1-shot
(−8.6% absolute) which significantly decreases on 5-, 10-,
and 25-shot settings to −0.8%, −0.6%, and +0.8% abso-
lute, respectively.
In the following, our main point of comparison is the
MAP baseline. This might be counter-intuitive since the
performance is worse than linear probe in most cases. How-
ever, the higher input dimensionality in the MAP-based set-
ting ( cf. Sec. 4.5) gives us the option of input reshaping ( cf.
Sec. 3.3) which opens up more room and variety for frozen
feature augmentations (FroFAs). Later in Sec. 6.3, we com-
pare the performance of our best FroFA to the linear probe.
5.2. Default FroFA
We now investigate the effect of adding a single FroFA to
the MAP baseline and start with the default FroFA formu-
lation. Recall that we only use a single randomly sampled
value per input ( cf. Sec. 3.3). In Tab. 2, we report gains
w.r.t. the MAP baseline on eighteen distinct FroFAs, cat-
egorized into geometric, crop & drop, stylistic, and other.
In Supplementary, Sec. S3.7, we report on two additional
FroFAs.
Geometric : Interestingly, all geometric augmentations
consistently lead to worse performance across all settings.
Crop & drop : Applying a simple crop or a combination
of resizing and crop yield a significant performance boost in
the 1-shot setting of 3.0% and 1.9% absolute, respectively.
Patch dropout, on the other hand, provides modest gains in
the 1-shot regime. Dropping patches is directly related to
training efficiency, so we investigate this further. Fig. 3a
shows the top-1 accuracy on 1- and 25-shot as a function
of number of patches. Results across other shots are simi-
lar (cf. Supplementary, Sec. S3.1). Similar to observations
16050
Geometric Crop & drop Stylistic Other
Shots MAP
rotate
shear-x
shear-y
translate-x
translate-y
crop
res. crop
incept. crop
patch drop.
brightness
contrast
equalize
invert
posterize
sharpness†
solarize†
JPEG†
mixup
1 57.9 −1.3−0.6−0.8−1.2−1.4+3.0+1.9+0.0+0.4+4.8+2.8+1.0+2.7+3.7−0.1+1.0−0.1−1.4
5 78.8 −0.3−0.2−0.2−0.3−0.3+0.0−0.2+0.0+0.0+1.1+0.8+0.5−0.3+0.8+0.1−0.1−0.3−0.3
10 80.9 −0.2−0.1−0.1−0.2−0.2+0.0−0.2+0.0+0.0+0.6+0.6+0.4+0.0+0.6+0.1+0.0−0.1+0.2
25 83.2 −0.2−0.1−0.2−0.1−0.2+0.0−0.1−0.1+0.0+0.1+0.1+0.0−0.2+0.0+0.0+0.0+0.0+0.1
Table 2. (Average) top-1 accuracy for default FroFA onourILSVRC-2012 test set. Absolute gains to the MAP baseline are reported.
We use the JFT-3B L/16 base setup ( cf. Sec. 5). In total, we investigate eighteen FroFAs, categorized into geometric ,crop & drop ,stylistic ,
andother . We highlight deterioration by shades of red and improvement by shades of green . Each shot is sampled five times, except
for augmentations marked with ‘ †’. Best three FroFAs are boldfaced.
1 50 100 150
number of patches52545658top-1 accuracy
1-shot
1 50 100 150
number of patches80818283
25-shot
MAP + patch dropout FroFA
(a) Patch dropout FroFA
0.00.20.40.60.81.0
brightness level50556065top-1 accuracy
1-shot
0.00.20.40.60.81.0
brightness level818283
25-shot
+ brightness cFroFA + brightness c2FroFA (b) Channel variants (c/c2) of brightness FroFA
Figure 3. Average top-1 accuracy for FroFA variants onourILSVRC-2012 test set. We use the JFT-3B L/16 base setup ( cf. Sec. 5). We
sweep across a base sweep ( cf. Sec. 4.4) to first find the best setting on ourILSVRC-2012 validation set for each FroFA operation point
(cf. Supplementary, Sec. S2.2). Shaded areas indicate standard errors collected via sampling each shot five times.
by Liu et al. [42] we can randomly drop a large fraction of
patches ( >50%) without loosing performance. A key dif-
ference is that Liu et al. only investigated the effect in the
image space, while we provide evidence that patch dropout
also transfers to the feature space. Finally, inception crop
does not improve performance.
Stylistic : The largest gains can be observed when em-
ploying a stylistic FroFA, in particular brightness, contrast,
and posterize. We identified brightness as the best perform-
ing FroFA with absolute gains of 4.8% on 1-shot, 1.1% on
5-shot, and up to 0.6% on 10-shot.
Other : Neither JPEG nor mixup yield performance
gains but rather more or less worsen the performance.
5.3. Channel FroFA
We continue with channel FroFA (cFroFA) using three
stylistic augmentations: brightness, contrast, and posterize.
In Tab. 3, we report absolute gains w.r.t. the MAP base-
line and incorporate channel (c) and non-channel (-) vari-
ants. First, contrast cFroFA does not improve upon its non-
channel variant across all shots. Second, posterize cFroFA
improves performance on 1-shot from 3.7% to 5.9% while
maintaining performance on all other shots. Lastly, bright-
ness cFroFA significantly improves performance across allBrightness Contrast Posterize
Shots MAP - c c2- c - c
1 57.9 +4.8+5.9+6.1+2.8+2.5+3.7+5.9
5 78.8 +1.1+1.5+1.6+0.8+0.0+0.8+0.8
10 80.9 +0.6+1.1+0.9+0.6+0.0+0.6+0.5
25 83.2 +0.1+0.4+0.3+0.1−0.1+0.0+0.0
Table 3. Average top-1 accuracy for a selection of default (-)
and channel (c/c2) FroFA onourILSVRC-2012 test set. Abso-
lute gains to the MAP baseline are reported. We use the JFT-3B
L/16 base setup ( cf. Sec. 5). Each shot is sampled five times. The
best results per shot and FroFA are boldfaced (multiple ones if
close, i.e.,±0.2).
shots: 4.8% →5.9% on 1-shot, 1.1% →1.5% on 5-shot,
0.6%→1.1% on 10-shot, and 0.1% →0.4% on 25-shot.
Given the strong improvements for brightness cFroFA,
we further test brightness c2FroFA (c2in Tab. 3). On a
first look, the c2FroFA variant performs comparable to the
cFroFA variant. In Fig. 3b, we report top-1 accuracy on 1-
and 25-shot as a function of the brightness level. Results
across other shots are similar and can be found in Supple-
mentary, Sec. S3.1. Now we clearly observe that bright-
ness cFroFA is more sensitive to the brightness level than
16051
brightness c2FroFA. In general, brightness cFroFA only
works well for small brightness levels (0.1 to 0.5), while its
c2FroFA counterpart performs better than the MAP baseline
across the board. We attribute the better sensitivity proper-
ties of brightness c2FroFA to the channel-wise mappings
(5), (7) on f∗
c(2) since this is the only change compared
to cFroFA. We did not observe similar effects for posterize
when switching from cFroFA to c2FroFA.
5.4. Sequential FroFA
Finally, out of our best three augmentations, i.e., brightness
c2FroFA (Bc2), contrast FroFA (C), and posterize cFroFA
(Pc), we combine two of them sequentially ( →) yielding six
combinations. In Tab. 4, we compare all six combinations
to our prior best (Bc2). On 1-shot, ‘Bc2→Pc’ significantly
outperforms ‘Bc2’, improving absolute gains from 6.1% to
7.7%, while maintaining performance on other shots. We
conclude that advanced FroFA protocols may further im-
prove performance. As an initial investigation, we applied
variations of RandAugment and TrivialAugment using our
best three FroFAs ( cf. Tab. 3), however, with limited suc-
cess. We include results in the Supplementary, Sec. S3.2,
and leave a deeper investigation to future works.
6. Results on More Model Architectures and
Pretraining Datasets
How well does our best non-sequential FroFA strategy,
i.e., brightness c2FroFA, transfer across multiple architec-
ture and pretraining setups? We address this question in
Secs. 6.1 and 6.2 and explore FroFA on ILSVRC-2012
frozen features from Ti/16, B/16, and L/16 ViTs pretrained
on JFT-3B or ImageNet-21k, respectively. We further pro-
vide a comparison to linear probe in Sec. 6.3. Throughout
this section, we report results on ILSVRC-2012. Further,
in this section and Sec. 7, all MAP-based models employ a
weight decay sweep denoted as MAPwd(Sec. 4.4).
6.1. JFT-3B Pretraining
In Fig. 4a, we report improvements in top-1 accuracy w.r.t.
the MAPwdbaseline for Ti/16, B/16, and L/16 ViTs pre-
trained on JFT-3B. Across all shots and all architectures
incorporating FroFA either maintains or improves perfor-
mance over the MAPwdbaseline. On 1-shot, we further ob-
serve increasing improvements from FroFA on scaling the
architecture. With higher shots, the improvement over the
baseline becomes smaller. We attribute this to the already
strong baseline performance leaving lesser headroom for
improvements. We refer to the Supplementary, Sec. S3.3,
for the exact values.
6.2. ImageNet-21k Pretraining
In Fig. 4b, we again look at improvements in top-1 accu-
racy w.r.t. the MAPwdbaseline for the same ViTs, but nowShots MAP Bc2
Bc2→C
C→Bc2
Bc2→Pc
Pc→Bc2
C→Pc
Pc→C
1 57.9 +6.1+4.0+2.7+7.7+5.2+5.0+3.1
5 78.8 +1.6+1.5+0.2+1.5+0.4+1.3+0.0
10 80.9 +0.9+1.2+0.1+1.0+0.1+0.9+0.3
25 83.2 +0.3+0.4−0.7+0.2−0.5+0.2−0.4
Table 4. Average top-1 accuracy for a sequential FroFA pro-
tocol onourILSVRC-2012 test set. Absolute gains to the MAP
baseline are reported. We use the JFT-3B L/16 base setup ( cf.
Sec. 5). We combine the best settings of brightness c2FroFA
(Bc2), contrast FroFA (C), and posterize cFroFA (Pc) sequentially
(two at a time, order indicated by ‘ ↑’). Each shot is sampled five
times. The best results per shot are boldfaced (multiple ones if
close, i.e.,±0.2).
pretrained on ImageNet-21k. Consistent with our JFT-3B
results, the performance either maintains or improves over
the MAPwdbaseline by incorporating FroFA and the im-
provements over the baseline become smaller with higher
shots. We further observe increasing improvements from
FroFA on scaling the architecture on 5- and 10-shot. We
refer to the Supplementary, Sec. S3.3, for the exact values.
6.3. Linear Probe Comparison
Finally, we revisit Figs. 4a and 4b, but now discuss gains
w.r.t. the linear probe baseline. We start with models pre-
trained on JFT-3B ( cf. Fig. 4a). On 1-shot, we observe that
we lack behind linear probe but can close the gap by scal-
ing up the model size. On 5- to 25-shot, with the excep-
tion of Ti/16 on 5-shot, brightness c2FroFA significantly
outperforms the linear probe baseline. On ImageNet-21k
(cf. Fig. 4b), we observe even larger gaps to linear probe on
1-shot (up to −20% absolute). However, similar to results
on JFT-3B, performance on 5- to 25-shot improves signifi-
cantly over linear probe or at worst stays the same.
7. Results on More Few-Shot Datasets and
Vision-Language Pretraining
Our study so far explored FroFA on ILSVRC-2012 as a few-
shot dataset. In this section, we analyze FroFA on seven ad-
ditional few-shot datasets, i.e., CIFAR10, CIFAR100, DM-
Lab, DTD, Resisc45, SUN397, and SVHN. In Sec. 7.1, we
first use an L/16 ViT pretrained on JFT-3B for our analy-
sis. In Sec. 7.2, we extend this analysis with the L/16 ViT
image encoder of a vision-language model which was pre-
trained with sigmoid language-image pretraining (SigLIP)
[74] on WebLI.
7.1. JFT-3B Pretraining
In Tab. 5 (upper half), we report mean results over the seven
few-shot datasets using a JFT-3B L/16 ViT. Per dataset and
16052
Ti/16 B/16 L/16−10−50top-1 accuracy
(absolute gains)*1-shot
Ti/16 B/16 L/16−0.50.00.55-shot
Ti/16 B/16 L/160.000.250.500.751.001.25
*10-shot
Ti/16 B/16 L/16012345
* *25-shot(a) JFT-3B
Ti/16 B/16 L/16−20−15−10−50top-1 accuracy
(absolute gains)* *1-shot
Ti/16 B/16 L/160.00.51.01.52.05-shot
Ti/16 B/16 L/160.00.51.01.52.010-shot
Ti/16 B/16 L/1601234
*25-shot
MAPwdlinear probe
(b) ImageNet21k
Figure 4. Average top-1 accuracy of brightness c2FroFA combined with weight decay for JFT-3B (a)and ImageNet-21k (b)ViTs on
ourILSVRC-2012 test set trained on few-shotted ILSVRC-2012 training sets. Absolute gains to the weight-decayed MAP, i.e. MAPwd,
and L2-regularized linear probe baseline are reported. Each shot is sampled five times. An asterisk (*) indicates that statistical significance
is not given under a two-tailed t-test with 95% confidence for that particular ‘pretraining, shots, model, baseline’-setting ( e.g., ‘JFT3-B,
10-shot, Ti/16, MAP’ or ‘ImageNet-21k, 25-shot, L/16, linear probe’).
Pretraining scheme Method 1-shot 5-shot 10-shot 25-shot
JFT-3BMAPwd49.5 65.8 68.3 74.1
Linear probe 49.1 62.7 65.7 68.8
MAPwd+ FroFA 53.4 67.3 70.9 74.9
WebLI + SigLIPMAPwd45.9 67.7 71.8 75.1
Linear probe 49.1 65.0 69.3 72.6
MAPwd+ FroFA 51.3 70.4 73.5 76.0
Table 5. Average top-1 accuracy of our best FroFA computed
across seven few-shot datasets using a JFT-3B or WebLI-SigLIP
L/16 ViT with weight decay. We report the mean across all test
sets and refer to Supplementary, Tabs. 11 and 12, for more details.
Per shot and dataset, the best result is boldfaced.
shot, top-1 accuracy and two-tailed t-tests with 95% confi-
dence are provided in Supplementary, Tab. 11. We compare
the MAPwdand linear probe baseline with MAPwdcom-
bined with brightness c2FroFA (MAPwd+ FroFA). Across
all shots, ‘MAPwd+ FroFA’ yields the highest mean results,
surpassing the second-best approach (MAPwd) by 3.9%,
1.5%, 2.6%, and 0.8% absolute on 1-, 5-, 10-, and 25-shot,
respectively ( cf. Fig. 1, left). Furthermore, Fig. 1 (left) re-
veals that while the gains to MAPwddiminish with higher
shots, the gains to linear probe actually increase and amount
to at least 4.0% absolute across all shots.
7.2. WebLI Vision-Language Pretraining
Given the strong performance with the JFT-3B L/16 ViT, we
finally ask: Does FroFA also transfer to ViTs with vision-
language pretraining?To answer this question, we train ‘MAPwd’, ‘linear
probe’, and ‘MAPwd+ FroFA’ using frozen features from
the L/16 ViT image encoder of a WebLI-SigLIP vision-
language model. In Tab. 5 (lower half), we report mean
results over the same seven few-shot datasets from before.
We again provide more detailed results and two-tailed t-
tests in Supplementary, Tab. 12. Across all shots, ‘MAPwd+
FroFA’ again yields the highest mean results, surpassing the
second-best approach on 1-shot (linear probe) by 2.2% ab-
solute and the second-best approach on 5-, 10-, and 25-shot
(MAPwd) by 2.7%, 1.7%, and 0.9% absolute, respectively
(cf. Fig. 1, right). In Fig. 1 (right), we observe that the gains
to both MAPwdand linear probe (neglecting 1-shot) dimin-
ish with higher shots. Overall, we can confirm that FroFA
also transfers to a ViT with vision-language pretraining.
8. Conclusion
We investigated twenty frozen feature augmentations
(FroFAs) for few-shot transfer learning along three axes:
model size, pretraining and transfer few-shot dataset. We
show that a training with FroFAs, in particular stylistic
ones, gives large improvements upon a representative
baseline across all shots. In addition, per-channel variants
further improve performance, e.g., by 1.6% absolute in
the ILSVRC-2012 5-shot setting. Finally, we show that
FroFA excels on smaller few-shot datasets. For exam-
ple, averaged results across seven few-shot tasks show
that training on cached frozen features from a JFT-3B
L/16 vision transformer with a per-channel variant of
brightness FroFA gives consistent gains of at least 4.0%
absolute upon linear probe across 1- to 25-shot settings.
16053
References
[1] Alex Krizhevsky. Learning Multiple Layers of Features
From Tiny Images, 2009. 1, 2, 4, 13
[2] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood,
and Leonid Sigal. Improved Few-Shot Visual Classification.
InProc. of CVPR , pages 14481–14490, virtual, 2020. 2
[3] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward,
Marcus Wainwright, Heinrich K ¨uttler, Andrew Lefrancq,
Simon Green, V ´ıctor Vald ´es, Amir Sadik, Julian Schrit-
twieser, Keith Anderson, Sarah York, Max Cant, Adam Cain,
Adrian Bolton, Stephen Gaffney, Helen King, Demis Hass-
abis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv ,
1612.03801:1–11, 2016. 4, 13
[4] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet-
ter Plain ViT Baselines for ImageNet-1k. arXiv , 2205.01580:
1–3, 2022. 4, 13
[5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. AdaptFormer:
Adapting Vision Transformers for Scalable Visual Recogni-
tion. In Proc. of NeurIPS , pages 16664–16678, New Orleans,
LA, USA, 2022. 2
[6] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,
Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,
Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad-
bury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,
Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner,
Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu
Soricut. PaLI: A Jointly Scaled Multilingual Language-
Image Model. In Proc. of ICLR , pages 1–33, Kigali, Rwanda,
2023. 1, 2, 4
[7] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sens-
ing Image Scene Classification: Benchmark and State of the
Art. Proc. IEEE , 105(10):1865–1883, 2017. 4, 13
[8] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell
Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-
mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible
Scaling Laws for Contrastive Language-Image Learning. In
Proc. of CVPR , pages 2818–2829, Vancouver, BC, Canada,
2023. 1
[9] Tsz-Him Cheung and Dit-Yan Yeung. MODALS: Modality-
agnostic Automated Data Augmentation in the Latent Space.
InProc. of ICLR , pages 1–18, virtual, 2021. 2
[10] Franc ¸ois Chollet. Xception: Deep Learning with Depthwise
Separable Convolutions. In Proc. of CVPR , pages 1063–
6919, Honolulu, HI, USA, 2017. 4
[11] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
Mohamed, and Andrea Vedaldi. Describing Textures in the
Wild. In Proc. of CVPR , pages 3606–3613, Columbus, OH,
USA, 2014. 4, 13
[12] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Va-
sudevan, and Quoc V . Le. AutoAugment: Learning Aug-
mentation Strategies From Data. In Proc. of CVPR , pages
113–123, Long Beach, CA, USA, 2019. 1, 2, 4, 13[13] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc
Le. RandAugment: Practical Automated Data Augmenta-
tion with a Reduced Search Space. In Proc. of NeurIPS ,
pages 18613–18624, virtual, 2020. 1, 4, 13, 18
[14] Zihang Dai, Hanxiao Liu, Quoc V . Le, and Mingxing Tan.
CoAtNet: Marrying Convolution and Attention for All Data
Sizes. In Proc. of NeurIPS , pages 3965–3977, virtual, 2021.
4
[15] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab,
Matthias Minderer, and Yi Tay. Scenic: A JAX Library for
Computer Vision Research and Beyond. In Proc. of CVPR ,
pages 21393–21398, New Orleans, LA, USA, 2022. 5
[16] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr
Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter
Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-
mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschan-
nen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz,
Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Ku-
mar, Sjoerd Van Steenkiste, Gamaleldin Fathy Elsayed, Ar-
avindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot,
Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vigh-
nesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas
Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran,
Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers,
Jeremiah J. Harmsen, and Neil Houlsby. Scaling Vision
Transformers to 22 Billion Parameters. In Proc. of ICML ,
pages 7480–7512, Honolulu, HI, USA, 2023. 1, 2, 4
[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image
Database. In Proc. of CVPR , pages 248–255, Miami, FL,
USA, 2009. 1, 2, 4
[18] Terrance DeVries and Graham W. Taylor. Dataset Augmen-
tation in Feature Space. In Proc. of ICLR - Workshops , pages
1–12, Toulon, France, 2017. 2
[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image Is
Worth 16x16 Words: Transformers for Image Recognition at
Scale. In Proc. of ICLR , pages 1–21, virtual, 2021. 1, 2, 4
[20] Vincent Dumoulin, Neil Houlsby, Utku Evci, Xiaohua Zhai,
Ross Goroshin, Sylvain Gelly, and Hugo Larochelle. A Uni-
fied Few-Shot Classification Benchmark to Compare Trans-
fer and Meta Learning Approaches. In Proc. of NeurIPS -
Datasets and Benchmarks Track , pages 1–14, virtual, 2021.
2
[21] Hanan Gani, Muzammal Naseer, and Mohammad Yaqub.
How to Train Vision Transformer on Small-Scale Datasets?
InProc. of BMVC , pages 1–16, London, UK, 2022. 1
[22] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP-
Adapter: Better Vision-Language Models with Feature
Adapters. Int. J. Comput. Vis. , 132(2):581–595, 2023. 2
[23] Raphael Gontijo-Lopes, Sylvia Smullin, Ekin Dogus Cubuk,
and Ethan Dyer. Tradeoffs in Data Augmentation: An Em-
pirical Study. In Proc. of ICLR , pages 1–27, virtual, 2021.
18
16054
[24] Edouard Grave, Armand Joulin, and Nicolas Usunier. Im-
proving Neural Language Models with a Continuous Cache.
InProc. of ICLR , pages 1–9, Toulon, France, 2017. 2
[25] Xuehai He, Chuanyuan Li, Pengchuan Zhang, Jianwei Yang,
and Xin Eric Wang. Parameter-Efficient Model Adaptation
for Vision Transformers. In Proc. of AAAI , pages 817–825,
Washington, DC, USA, 2023. 1, 2
[26] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph,
Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A
Simple Data Processing Method to Improve Robustness and
Uncertainty. In Proc. of ICLR , pages 1–15, Virtual, 2020. 2
[27] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
Knowledge in a Neural Network. In Proc. of NIPS - Work-
shops , pages 1–9, Montr ´eal, QC, Canada, 2014. (In 2018,
‘NIPS’ was renamed to ‘NeurIPS’). 2, 4
[28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer
Learning for NLP. In Proc. of ICML , pages 2790–2799,
Long Beach, CA, USA, 2019. 2
[29] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
LoRA: Low-Rank Adaptation of Large Language Models. In
Proc. of ICLR , pages 1–13, virtual, 2022. 2
[30] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual Prompt Tuning. In Proc. of ECCV , pages 709–727, Tel
Aviv, Israel, 2022. 2
[31] Diederik P. Kingma and Jimmy Ba. Adam: A Method for
Stochastic Optimization. In Proc. of ICLR , pages 1–15, San
Diego, CA, USA, 2015. 14
[32] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.
Big Transfer (BiT): General Visual Representation Learning.
InProc. of ECCV , pages 491–507, virtual, 2020. 2, 4
[33] Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang,
Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent,
Rodolphe Jenatton, and Efi Kokiopoulou. Three Towers:
Flexible Contrastive Learning with Pretrained Image Mod-
els. In Proc. of NeurIPS , pages 31340–31371, New Orleans,
LA, USA, 2023. 4
[34] Taku Kudo and John Richardson. SentencePiece: A Simple
and Language-Independent Subword Tokenizer and Detok-
enizer for Neural Text Processing. In Proc. of EMNLP - Sys-
tem Demonstrations , pages 66–71, Brussels, Belgium, 2018.
16
[35] Varun Kumar, Hadrien Glaude, Cyprien de Lichy, and Wl-
liam Campbell. A Closer Look At Feature Space Data Aug-
mentation For Few-Shot Intent Classification. In Proc. of
EMNLP - Workshops , pages 1–10, Hong Kong, China, 2019.
2
[36] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B.
Tenenbaum. The Omniglot Challenge: A 3-year Progress
Report. Curr. Opin. Behav. Sci. , 29:97–104, 2019. 2
[37] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-
ungjin Choi, and Yee Whye Teh. Set Transformer: A Frame-
work for Attention-Based Permutation-Invariant Neural Net-works. In Proc. of ICML , pages 3744–3753, Long Beach,
CA, USA, 2019. 1, 2, 4
[38] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol
Song. Vision Transformer for Small-Size Datasets. arXiv ,
2112.13492:1–11, 2021. 1
[39] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power
of Scale for Parameter-Efficient Prompt Tuning. In Proc. of
EMNLP , pages 3045–3059, virtual, 2021. 2
[40] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao,
Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane You.
Data Augmentation via Latent Space Interpolation for Image
Classification. In Proc. of ICPR , pages 728–733, Beijing,
China, 2018. 2
[41] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno
Lepri, and Marco De Nadai. Efficient Training of Visual
Transformers with Small Datasets. In Proc. of NeurIPS ,
pages 23818–23830, virtual, 2021. 1
[42] Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Az-
izpour, and Kevin Smith. PatchDropout: Economizing Vi-
sion Transformers Using Patch Dropout. In Proc. of WACV ,
pages 3942–3951, Waikoloa, HI, USA, 2023. 2, 4, 6
[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin Transformer:
Hierarchical Vision Transformer Using Shifted Windows. In
Proc. of ICCV , pages 10012–10022, virtual, 2021. 1
[44] Zichang Liu, Zhiqiang Tang, Xingjian Shi, Aston Zhang,
Mu Li, Anshumali Shrivastava, and Andrew Gordon Wilson.
Learning Multimodal Data Augmentation in Feature Space.
InProc. of ICLR , pages 1–15, Kigali, Rwanda, 2023. 2
[45] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay
Regularization. In Proc. of ICLR , pages 1–18, New Orleans,
LA, USA, 2019. 14
[46] Samuel G. M ¨uller and Frank Hutter. TrivialAugment:
Tuning-Free Yet State-of-the-Art Data Augmentation. In
Proc. of ICCV , pages 774–782, virtual, 2021. 1, 2, 4, 13
[47] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y . Ng. Reading Digits in Nat-
ural Images with Unsupervised Feature Learning. In Proc.
of NIPS - Workshops , pages 1–9, Granada, Spain, 2011. (In
2018, ‘NIPS’ was renamed to ‘NeurIPS’). 4, 13
[48] Alex Nichol, Joshua Achiam, and John Schulman. On First-
Order Meta-Learning Algorithms. arXiv , 1803.02999:1–15,
2018. 2
[49] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mah-
moud Assran, Nicolas Ballas, Wojciech Galuba, Russell
Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael
Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv ´e Je-
gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Pi-
otr Bojanowski. DINOv2: Learning Robust Visual Features
Without Supervision. Trans. Mach. Learn. Res. , 1:1–32,
2024. 1
[50] Boris N. Oreshkin, Pau Rodr ´ıguez L ´opez, and Alexandre La-
coste. TADAM: Task-Dependent Adaptive Metric for Im-
proved Few-Shot Learning. In Proc. of NeurIPS , pages 719–
729, Montr ´eal, QC, Canada, 2018. 2
16055
[51] Emin Orhan. A Simple Cache Model for Image Recognition.
InProc. of NeurIPS , pages 10128–10137, Montr ´eal, Canada,
2018. 2
[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning Transferable Visual
Models From Natural Language Supervision. In Proc. of
ICML , pages 8748–8763, virtual, 2021. 1, 2
[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the Limits of Transfer Learning With
a Unified Text-to-Text Transformer. J. Mach. Learn. Res. , 21
(140):1–67, 2020. 16
[54] James Requeima, Jonathan Gordon, John Bronskill, Sebas-
tian Nowozin, and Richard E. Turner. Fast and Flexible
Multi-Task Classification Using Conditional Neural Adap-
tive Processes. In Proc. of NeurIPS , pages 7957–7968, Van-
couver, BC, Canada, 2019. 2
[55] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi
Zelnik-Manor. ImageNet-21K Pretraining for the Masses.
InProc. of NeurIPS - Datasets and Benchmarks Track , pages
1–12, virtual, 2021. 2
[56] Pau Rodr ´ıguez, Issam H. Laradji, Alexandre Drouin, and
Alexandre Lacoste. Embedding Propagation: Smoother
Manifold for Few-Shot Classification. In Proc. of ECCV ,
pages 121–138, virtual, 2020. 2
[57] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge. Int. J. Comput. Vis. , 115(3):211–252, 2015. 1, 2, 4,
13
[58] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive
Learning Rates with Sublinear Memory Cost. In Proc. of
ICML , pages 4596–4604, Stockholm, Sweden, 2018. 14
[59] Jake Snell, Kevin Swersky, and Richard S. Zemel. Proto-
typical Networks for Few-Shot Learning. In Proc. of NIPS ,
pages 4077–4087, Long Beach, CA, USA, 2017. (In 2018,
‘NIPS’ was renamed to ‘NeurIPS’). 2
[60] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross
Wightman, Jakob Uszkoreit, and Lucas Beyer. How to Train
Your ViT? Data, Augmentation, and Regularization in Vision
Transformers. Trans. Mach. Learn. Res. , 5:1–16, 2022. 2, 5,
14, 16
[61] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-
nav Gupta. Revisiting Unreasonable Effectiveness of Data
in Deep Learning Era. In Proc. of ICCV , pages 843–852,
Venice, Italy, 2017. 4
[62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the Inception Ar-
chitecture for Computer Vision. In Proc. of CVPR , pages
2818–2826, Las Vegas, NV , USA, 2016. 2, 4
[63] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenen-
baum, and Phillip Isola. Rethinking Few-Shot Image Classi-
fication: A Good Embedding Is All You Need? In Proc. of
ECCV , pages 266–282, virtual, 2020. 2[64] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herve Jegou. Training
Data-Efficient Image Transformers & Distillation Through
Attention. In Proc. of ICML , pages 10347–10357, virtual,
2021. 4
[65] Cristina Vasconcelos, Vighnesh Birodkar, and Vincent Du-
moulin. Proper Reuse of Image Classification Features Im-
proves Object Detection. In Proc. of CVPR , pages 13628–
13637, New Orleans, LA, USA, 2022. 2
[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention Is All You Need. In Proc. of NIPS ,
pages 5998–6008, Long Beach, CA, USA, 2017. (In 2018,
‘NIPS’ was renamed to ‘NeurIPS’). 14
[67] Vikas Verma, Alex Lamb, Christopher Beckham, Amir Na-
jafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Ben-
gio. Manifold Mixup: Better Representations by Interpo-
lating Hidden States. In Proc. of ICML , pages 6438–6447,
Long Beach, CA, USA, 2019. 2
[68] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray
kavukcuoglu, and Daan Wierstra. Matching Networks for
One-Shot Learning. In Proc. of NIPS , pages 3637–3645,
Barcelona, Spain, 2016. (In 2018, ‘NIPS’ was renamed to
‘NeurIPS’). 2
[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-
mid Vision Transformer: A Versatile Backbone for Dense
Prediction Without Convolutions. In Proc. of ICCV , pages
548–558, virtual, 2021. 1
[70] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva,
and Antonio Torralba. SUN Database: Large-Scale Scene
Recognition From Abbey to Zoo. In Proc. of CVPR , pages
3485–3492, San Francisco, CA, USA, 2010. 2, 4, 13
[71] Jianxiong Xiao, Krista A. Ehinger, James Hays, Antonio
Torralba, and Aude Oliva. SUN Database: Exploring a Large
Collection of Scene Categories. Int. J. Comput. Vis. , 119(1):
3–22, 2016. 1, 2, 4, 13
[72] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov,
Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo-
longa, Andr ´e Susano Pinto, Maxim Neumann, Alexey Doso-
vitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen,
Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil
Houlsby. A Large-Scale Study of Representation Learn-
ing with the Visual Task Adaptation Benchmark. arXiv ,
1910.04867:1–33, 2020. 4, 13
[73] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-
cas Beyer. Scaling Vision Transformers. In Proc. of CVPR ,
pages 12104–12113, New Orleans, LA, USA, 2022. 1, 2, 4,
5, 14
[74] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
Lucas Beyer. Sigmoid Loss for Language-Image Pretrain-
ing. In Proc. of ICCV , pages 11975–11986, Paris, France,
2023. 1, 2, 5, 7, 14
[75] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and
David Lopez-Paz. Mixup: Beyond Empirical Risk Mini-
mization. In Proc. of ICLR , pages 1–13, Vancouver, BC,
Canada, 2018. 1, 2, 4
16056
[76] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kun-
chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-
Adapter: Training-Free Adaption of CLIP for Few-Shot
Classification. In Proc. of ECCV , pages 493–510, Tel Aviv,
Israel, 2022. 2
16057
