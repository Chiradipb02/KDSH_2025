Decompose-and-Compose: A Compositional Approach to Mitigating Spurious
Correlation
Fahimeh Hosseini Noohdani
fhosseini@ce.sharif.eduParsa Hosseini∗
parsa.hosseini@sharif.eduAryan Yazdan Parast∗
arian.yazdanparast@sharif.edu
Hamidreza Yaghoubi Araghi
hamidreza.yaghoubiaraghi@sharif.eduMahdieh Soleymani Baghshah
soleymani@sharif.edu
Sharif University of Technology
Tehran, Iran
Abstract
While standard Empirical Risk Minimization (ERM)
training is proven effective for image classification on
in-distribution data, it fails to perform well on out-of-
distribution samples. One of the main sources of distribu-
tion shift for image classification is the compositional na-
ture of images. Specifically, in addition to the main object
or component(s) determining the label, some other image
components usually exist, which may lead to the shift of in-
put distribution between train and test environments. More
importantly, these components may have spurious corre-
lations with the label. To address this issue, we propose
Decompose-and-Compose (DaC), which improves robust-
ness to correlation shift by a compositional approach based
on combining elements of images. Based on our observa-
tions, models trained with ERM usually highly attend to
either the causal components or the components having a
high spurious correlation with the label (especially in data-
points on which models have a high confidence). In fact,
according to the amount of spurious correlation and the
easiness of classification based on the causal or non-causal
components, the model usually attends to one of these more
(on samples with high confidence). Following this, we first
try to identify the causal components of images using class
activation maps of models trained with ERM. Afterwards,
we intervene on images by combining them and retraining
the model on the augmented data, including the counterfac-
tual ones. This work proposes a group-balancing method
by intervening on images without requiring group labels or
information regarding the spurious features during train-
ing. The method has an overall better worst group accuracy
compared to previous methods with the same amount of su-
*Equal contribution.pervision on the group labels in correlation shift. Our code
is available at https://github.com/fhn98/DaC .
1. Introduction
While deep neural networks are capable of superhuman per-
formance, they still fail when faced with out-of-distribution
(OOD) data [2, 4, 24]. Studies have shown that these
models tend to make their predictions according to sim-
ple features that have a high correlation with the label, al-
though these correlations are unstable across data distribu-
tions [2, 24, 29]. Relying on these spurious correlations
instead of the stable ones causes the model to overfit on the
training data and fail on OOD samples, for which those pre-
vious correlations do not hold.
To tackle this challenge, methods based on invariant
learning focus on learning representations that can be used
to make invariant predictions across different environments,
to make the trained model more robust to distribution
shifts [1, 2, 9, 23].
Another line of work approaches this problem by bal-
ancing minority/majority groups of data [6, 8, 25] to re-
move spurious correlation. Among these works, [8] pro-
poses DFR, which retrains the last layer of a model previ-
ously trained by ERM, with group-balanced data to make it
robust to spurious correlation. Nonetheless, DFR requires
group labels. On the other hand, methods like [13, 17, 20]
try to learn a robust model by upweighting samples that are
less likely to contain spurious attributes, without access to
group labels during training. Relying on the assumption
that datapoints on which the model has a high loss are most
probably from minority groups, most of these methods aim
to place more emphasis on these samples. However, in these
samples, the obscure core object may be the source of high
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27662
0-0.25 0.25-0.5 0.5-0.75 0.75-100.20.40.60.81
Quantiles of Loss(a)
CScore SScore0 0.5 1
C= 0.12
S= 0.88
C= 0.12
S= 0.88
C= 0.88
S= 0.12
C= 0.88
S= 0.12
(b)0-0.25 0.25-0.5 0.5-0.75 0.75-100.20.40.60.81
Quantiles of Loss(c)
CScore SScore0 0.5 1
C= 0.79
S= 0.30
C= 0.79
S= 0.32
C= 0.55
S= 0.32
C= 0.32
S= 0.28
(d)
Figure 1. Behaviour of a model trained with standard ERM in different datasets. Based on the easiness of inferring the label from the
causal or non-causal parts across the whole dataset, the model attends more to one of them, this behaviour is more evident in samples
on which the model has a low loss. (a), (b) Average xGradCAM score of Cifar10 (causal) and MNIST (non-causal) pixels in four loss
quantiles of the Dominoes training set. The model generally attends more to the non-causal parts, and as the loss decreases, the non-causal
attention increases. (c), (d) Average xGradCAM score of foreground (causal) and background (non-causal) pixels in four loss quantiles of
the Waterbirds training set. The model generally attends to the causal parts, and as the loss decreases, the causal attention increases.
loss (i.e., the target object itself cannot be easily classified),
and overrating these samples may have some side effects.
To be more precise, we need to discover and analyze parts
of images to make a more accurate decision.
Due to the compositional nature of images, the problem
of correlation shift can be viewed through the lens of com-
positionality, as models fall into the trap of spurious cor-
relation because they make their predictions based on non-
causal components of images. This fine-grained perspective
could lead to a more precise approach compared to methods
that consider images as a whole. While the viewpoint of
compositionality is essential to OOD generalization, espe-
cially when facing correlation shift, only a limited number
of works have explored this problem from this perspective.
As a recent work, [36] combines parts of different images
and uses them for model distillation on the representation
level. Nonetheless, they cannot label the combined images,
as they could not determine whether the parts taken from
images for combining are causal or non-causal parts. Ad-
ditionally, they do not offer any evaluations on correlation
shift benchmarks. Masktune [3] takes a step further and hy-
pothesizes that in datasets exhibiting spurious correlation,
the parts of an image with high attribution scores according
to a model trained with ERM, are non-causal and mislead-
ing, and based on this assumption, masks these parts for
finetuning the model.
Inspired by this compositional viewpoint, we propose
Decompose-and-Compose (DaC), a method for balancinggroups by intervening on non-causal components of images
and creating new ones. The same idea of intervening on im-
ages or using synthetic data as a means of group-balancing
has been previously studied in a few works [21, 35]. How-
ever, unlike our method, which does not require any exter-
nal aid during training, both these studies have a knowledge
of the possible spurious attributes, and based on this knowl-
edge, they create concept banks [35] or intervene in images
using generative models [21].
In this paper, we first analyze the behaviour of a model
trained with ERM and utilize its attribution map on images
to decompose them into causal and non-causal parts; then,
based on the performance of the model trained with our
method, on the validation set, we identify the causal parts.
More precisely, as opposed to MaskTune [3], which as-
sumes that for a given model trained with standard ERM,
the regions with high attribution scores are spurious ones,
we show that a model trained with ERM usually focuses on
either causal or non-causal parts of images, based on the
easiness of predicting the label from them. Gaining this
knowledge about the causal parts enables us to intervene on
datapoints from the majority groups to create new ones from
under-represented groups as a means of group balancing.
The contributions of this work are as follows:
• We provide an analysis of the behaviour of models trained
with standard ERM, especially on low-loss data.
• Based on this analysis, we introduce a method for identi-
fying the causal parts of an image.
27663
(a)
 (b)
Figure 2. (a) Image as a composition of causal and non-causal components. (b) The edge between Sand˜Scan be removed by intervention
on components in ˜S. This removes the spurious correlation between ˜SandY.
• We propose a method for combining images, to create
new datapoints representing the minority groups, as a
means for group balancing.
• Our proposed method performs better than previous base-
lines on well-known benchmarks in the literature.
2. Preliminaries
Spurious correlations that are perceivable by humans can
be categorized into two groups: 1) Spurious parts: spu-
rious correlations between other objects of the image and
the label (e.g., spurious correlation of the background and
the label in the Waterbirds [24] dataset) and 2) Spurious at-
tributes: spurious correlations between some non-causal at-
tributes of the object of interest and the label (e.g., spurious
correlation between the colour of the digits and their label
in the CMNIST [2] dataset). In this section, we propose a
method that provides robustness to the first type of spurious
correlation, which is prevalent in most benchmarks. The
proposed method mitigates correlation shifts by discovering
non-causal parts of images and intervening on these parts.
2.1. Problem Definition
Consider a dataset Dtr={(x(i), y(i))}N
i=1for a classifica-
tion problem. Each x∈ X hascore features c, which are
the cause of the label, and their correlation with the label is
persistent across different environments. It has also spuri-
ousfeatures s. Based on combinations of core and spurious
features, (c, s), training samples can be partitioned into sev-
eral groups. We consider the case when there is an imbal-
ance between the size of groups in the training set. In this
case, the group containing the majority of samples in a class
is called the majority group of that class, and the others are
called the minority groups. This imbalance induces a spuri-
ous correlation between the spurious features and the label,
i.e. the value of the spurious features and the label corre-
sponding to a majority group are frequently seen together in
the dataset. The proportion of samples in groups could be
different in the test set, causing a correlation shift between
the training and the test sets. For instance, in the Water-birds dataset [24], in which the task is to determine whether
each image shows a waterbird or a landbird, the core and
spurious features are the foreground and the background re-
spectively. Waterbirds consists of four groups: waterbirds
on water background, landbirds on land background, wa-
terbirds on land background and landbirds on water back-
ground, with the first two being the majority groups.
Our objective is to train a classifier, denoted as f, that
performs well across both the training and test distributions.
This entails ensuring that fexhibits strong performance not
only on the majority but also on the minority groups.
2.2. A Causality Viewpoint to Spurious Correlation
To study the problem of spurious correlation from the per-
spective of causality, we model the data-generating pro-
cess as the S tructural C ausal M odel (SCM) [19] shown in
Fig. 2a. In this SCM, CandSindicate unobservable causal
and non-causal variables, from which the observable causal
and non-causal components ˜Cand˜Sfor an image are ob-
tained. The final image Xis the output of ψ(˜C,˜S), where
ψ(., .)is a combining function. The label of the image is
caused by ˜C.
In the case of spurious correlation, a hidden confounder
E, mostly referred to as the environment [2] or group [24]
variable in the literature, would be present in the SCM such
thatS←E→C. This creates the path ˜S←S←E→
C→˜C→Y, which introduces a spurious correlation be-
tween ˜SandY.Eis mainly sample selection bias.
Whereas most previous studies on mitigating spurious
correlation did not approach this problem from a causal-
ity perspective, methods based on group balancing, such
as [6, 8, 24], resolve this issue by eliminating the effect
ofE. While solutions based on group balancing are effec-
tive when Eis observable, they are not feasible when group
annotation is not provided.
Another solution that is effective even in the absence of
group annotation is removing the edge S− →˜Sby interven-
ing on some components in ˜Sin order to break the path, as
shown in Fig. 2b. More concretely, this solution intervenes
on a subset of non-causal components of images without
27664
changing the label to reduce their correlation. Intervention
on˜Scould be done in a more efficient manner if we could
set˜Sto a value that has less co-occurrence with Y. Such
intervention would create a new datapoint that can be as-
signed to a minority group. Hence, this type of intervention
would be a method for upweighting datapoint from minority
groups.
3. To Which Does ERM Attend More?
To intervene on the non-causal components of an image, it
is essential to determine the causal and non-causal parts of
it first. The attribution map of a model trained with standard
ERM on an image could be utilized in distinguishing these
parts. MaskTune [3] partly addressed this issue by assuming
that for a model trained with standard ERM on a dataset
exhibiting spurious correlations, the image parts with high
attribution scores are spurious.
However, this assertion does not hold for the majority
of realistic datasets. Specifically, the behaviour of a model
trained with ERM might vary depending on the easiness of
predicting the label from the causal and non-causal parts
across different datasets.
Since non-causal parts of images that have spurious cor-
relations with the label, such as the background in the Wa-
terbirds dataset, are shortcuts for models, it is often pre-
sumed that a model trained with standard ERM attends
more to the non-causal parts of images. However, we show
that this assumption does not hold in many real-world sce-
narios. Across the entire dataset, as opposed to the causal
components, non-causal ones do not persistently appear in
accordance with the label. Consequently, the causal parts
may become generally more predictive. As a result, the
easiness of predicting labels from the causal and non-causal
parts across the entire dataset determines the focus of ERM.
To illustrate this point, we report the average xGrad-
CAM [27] score for the foreground and background pixels
in the Waterbirds datasets in Fig. 1(a,b) and for the Cifar
part and the MNIST part of the Dominoes dataset [18] in
Fig. 1(c,d). For the Waterbirds, the average score of the
foreground (causal) pixels exceeds that of the background
(spurious part) and for the Dominoes, the average score of
the spurious part (MNIST part) exceeds that of the causal
pixels (i.e. Cifar part). Indeed, if the spurious patterns are
easy to learn, the model may attend more to the spurious
parts. This tendency becomes more pronounced in samples
with low loss, as depicted in Fig. 1.
4. Method
Regardless of whether models trained with ERM attend
more to the causal or non-causal parts of an image, their
attribution map on the image remains useful for distinguish-
ing these two parts. In the following, we introduce a method0.1 0.3 0.5 0.7 0.900.050.10.150.20.25
plp0.1 0.3 0.5 0.7 0.900.050.10.150.20.25
plp
Figure 3. Adaptive masking according to the attention scores ob-
tained from the ERM model. The loss value of the masked images
for which different portions pof pixels (with the lowest attention
score) has been masked is shown as lp. (a) The loss curve for an
image of the Dominoes dataset with the label ’truck’ on which the
ERM model has non-causal attention, and (b) The loss curve for
an image of the MetaShift dataset with the label ’dog’ on which
the ERM model has causal attention.
for identifying the most predictive parts of images using at-
tribution maps.
4.1. Adaptive Masking by ERM
Given a model fθ(.)that is trained with ERM, and a dat-
apoint (x, y)on which f’s loss is low, we want to find a
mask that conceals pixels in xexcept for the most predic-
tive ones. The assumption that the loss on (x, y)is low is
necessary since we are more confident that the predictive
parts in xare indeed predictive of the correct label.
First, the attribution scores of pixels of xare computed
using a visual explanation method [27, 32]. Due to its effi-
ciency, we use xGradCAM [27] for this means and denote
the xGradCAM of an input xas AttributionScore (x). To
have precise adaptive masking, we first define l(fθ(˜xp), y),
in which lis the cross-entropy loss, and ˜xpis the new
image obtained from xby masking out the portion p
of pixels with the lowest attribution scores according to
AttributionScore (x). When gradually masking x, first the
less predictive parts are masked out, which will not have a
significant effect on the loss of the masked image. How-
ever, when a large proportion of the image, including the
27665
predictive part, is masked, the loss increases rapidly, as it
is hard for the model to predict the label when the center
of attention is (partially) obscured. The effects of gradu-
ally masking two images in the Metashift and Dominoes
datasets are shown in Fig. 3. More precisely, l(fθ(˜xp), y)
can be considered as a function of pwhose elbow located
atp∗shows the optimal amount of masking for the in-
putx. This amount of masking is expected to conceal
the non-predictive parts as much as possible while keeping
the predictive parts intact. Therefore, we define a function
m:RH×W×3→ {0,1}H×Wthat returns a mask for its
input through an adaptive masking. In fact, m(x)provides
a binary mask for xwhose value is 0 for a proportion p∗of
pixels of xwith the lowest attribution score and is 1 for the
remaining pixels ( p∗denotes the optimal amount of mask-
ing found as the elbow of l(fθ(˜xp), y)).
Based on the observation mentioned above, we propose
Adaptive Masking algorithm shown in Algorithm 1 to find
the optimal amount of masking for an image.
Algorithm 1: Adaptive Masking
Input: Model fθ; Image x; Label y; Loss function
l(., .);
Output: Mask m;
1δ= 0.2
2xscore←AttributionScore( x) ;
// Calculate lpfor p∈ {0, δ,2δ,3δ, ..., 1}
3forpin{0, δ,2δ,3δ, ..., 1}do
4 ˜xp←MaskWithProportion( xscore ,p);
5 lp←l(fθ(˜xp), y);
6end
// Find the optimal mask
7p∗←FindElbow( lp);
8m←MaskWithProportion( xscore ,p∗);
9return m;
4.2. Decompose-and-Compose (DaC)
Viewing images as combinations of components gives us
the upper hand of being able to intervene [19] on them. This
could be done by combining components of different im-
ages to create novel combinations less seen by the model.
To be more specific, in the case of spurious correlation, by
combining components of different selected inputs, novel
images that are more similar to underrepresented data could
be created and used during the training of the model as a
means of upweighting the underrepresented groups. Here
we propose a method for obtaining new datapoints from
minority groups, by combining the ones from the majority
groups. Given dataset D, we consider a pretrained classifier
fθ(x) =w◦gϕ(x)with standard ERM on Dtr, in which gϕ
is a feature extractor and wis a linear predictor. Afterwards,inspired by [8], we intend to retrain only won an augmented
variation of Dtrthat is prepared by intervening on samples
to produce new ones to upweight the underrepresented or
minority groups.
Selecting low-loss examples and decomposing them.
For a given training batch B, we first select the subset
B′={(x(i), y(i))∈ B| (x(i), y(i))is among qportion of
training samples with the lowest loss }andqis the hyper-
parameter denoting the portion of the selected samples. For
each sample (x(i), y(i))∈ B′, first, by Algorithm 1, the
mask m(x(i))will be obtained. Then, the two parts of im-
agexis found as x(i)⊙m(x(i))andx(i)⊙(1−m(x(i))).
But still, we do not know which of these two parts consists
more of the causal regions.
As explained in Sec. 3, based on the predictive power of
the causal and non-causal parts of the images, the model
may attend more to one of them . We inspect both of these
assumptions and find which of them yields better results on
the validation data. Therefore, we consider causalflag as a
hyperparameter in Algorithm 2, and when this flag indicates
the non-causal assumption, masks are inverted, as shown in
Lines 10-13, to obtain more causal regions.
Composing the causal and non-causal parts of two
images. Given a sample (x(i), y(i))∈ B′, another sample
(x(j), y(j))∈ B′is selected randomly such that y(i)̸=y(j).
Then, we combine the two images as below:
ˆx(i)
combj=m(x(i))⊙x(i)
+ (1−m(x(i)))⊙(1−m(x(j)))x(j)
+ (1−m(x(i)))⊙m(x(j))b,(1)
where bis a 1×1×3vector indicating the mean of
Bacross the color channels. This formula constructs the
combined image by putting the selected parts of x(i)and
masked parts of x(j)that are not located on the selected
parts of x(i)together, and filling the remaining parts of
the image by the default value b. The reason for set-
ting the remaining areas equal to bis to retain the statis-
tics of the batch as much as possible. Finally, we define
M={(ˆx(i)
combj, y(i))|(x(i), y(i))∈ B′}as the combined
samples.
As will be shown in Appendix 9, most low-loss datat-
points are the ones from the majority groups. By combin-
ing the causal and non-causal parts of two majority data-
points from different labels, we make new datapoints from
minority groups, thus group-balance the training data with-
out access to group annotation. More precisely, suppose
that the model attends more to the causal parts across the
dataset. Consider (x(i), y(i))and(x(j), y(j))as two sam-
ples from majority groups where x(i)=ψ(˜c(i),˜s(i))and
x(j)=ψ(˜c(j),˜s(j))andy(i)̸=y(j). Therefore, ˜s(i)and
˜s(j)are in accordance with y(i)andy(j)respectively. Now,
in the combined datapoint ˆx(i)
combj=ψ(˜c(i),˜s(j)), the non-
27666
...
B
...
B′
y= 0
y= 1 Mask and
Combine
M
B
ERMPrediction Layer
LCE
Lcombb) Mask and Combine a) Method Overview
x(i), y(i)
ERM
Adaptive Masking Adaptive MaskingERM
x(j), y(j)
m(x(i))
 m(x(j))
Eq. (1)
bx(i)
comb j, y(i)Select by
Loss Value
Figure 4. (a) An overview of our DaC method. For each batch, a qportion of samples with the lowest loss is selected. Then images of
different labels are combined by the Mask and Combine module. The overall loss to update the model’s last layer parameters is a weighted
sum of the loss on the original batch ( LCE) and the combined data ( Lcomb). The algorithm for this method is shown in Algorithm 2. (b)
The Mask and Combine module. The two input images x(i)andx(j)are masked by Algorithm 1. Afterwards, The selected part of x(i)
and the masked parts of x(j)are combined, and the remaining gaps are filled with the mean value of the batch. The new combined image
has the same label as x(i)and is used for training the last layer of the model.
causal part ˜s(j)does not have the spurious value corre-
sponding to y(i), which makes this datapoint from the mi-
nority groups. It is worth mentioning that this combina-
tion tends to be a minority sample independent of whether
m(x(i))⊙x(i)or(1−m(x(i)))⊙x(i)reflects the cause
more.
By the above intervention that combines pairs of data,
we generate data in order to break the spurious correlation
between the non-causal parts of images and labels. Finally,
the loss function during retraining the last layer of the model
is defined as below:
LCE=1
|B|X
(x,y)∈Bl(fθ(x), y), (2)
Lcomb=1
|M|X
(x,y)∈Ml(fθ(x), y) (3)
Ltotal=LCE+αL comb, (4)
in which lis the cross-entropy loss and αis the hyperparam-
eter determining the importance of the combinations. An
overview of our Decompose and Compose (DaC) method in
addition to its algorithm is shown in Fig. 4 and Algorithm 2
respectively.5. Experiments
5.1. Compared Methods
In this paper, we consider 6 baselines besides ERM.
DFR [8] argues that even in the presence of spurious cor-
relations, neural network classifiers still learn the core fea-
tures. Following this, they show that simple retraining
of the last layer with group-balanced data can be suffi-
cient to make the model robust to spurious correlation.
We evaluated their method on our ResNet50 [5] backbone
trained with ERM. Group DRO [24] aims to minimize the
worst-case loss across groups with strong regularization.
LISA [39] is a data-augmenting technique that aims to learn
invariant predictors By intervening on samples with either
the same labels but different domains or the same domains
but different labels. MaskTune [3] argues that in the pres-
ence of spurious correlations, ERM models attend more to
the spurious parts of images. Therefore, they fine-tune an
ERM model for one epoch using a masked version of the
training data to force the model to focus on the core parts.
CNC [42] is a contrastive learning method designed to align
representations of samples within the same class that have
different spurious attributes, while also distinguishing be-
tween samples of dissimilar classes that share similar spuri-
ous features. JTT [13] first uses a model trained with ERM
to detect misclassified samples. These samples are sub-
sequently upweighted when training a new model on the
dataset.
27667
Algorithm 2: Decompose-and-Compose (DaC)
Input: Model fθ(.) =w◦gϕ(.); Dataset Dtr; Loss
function l(., .); Hyperparameters α, q, causalflag
1forepoch= 1,2, . . . K do
2 forbatchBinDtrdo
3 b←mean (B)
4 B′←qportion of samples in B
with the lowest loss
5 M ← {}
6 foreach image (x, y)∈ B′do
7 Pick(x′, y′)∈ B′s.t.y̸=y′
8 m←AdaptiveMasking (f, x, y, l )
9 m′←AdaptiveMasking (f, x′, y′, l)
10 ifcasualflag=False then
11 m←1−m
12 m′←1−m′
13 end
14 ˆxcomb =m⊙x
15 +(1−m)⊙(1−m′)x′+ (1−m)⊙m′b
16 M ← M ∪ { (ˆxcomb, y)}
17 end
18 LCE←1
|B|P
(x,y)∈Bl(fθ(x), y)
19 Lcomb←1
|M|P
(x,y)∈Ml(fθ(x), y)
20 Ltotal←LCE+αLcomb
21 w←UpdateWeights( Ltotal)
22 end
23end
5.2. Setup
The experiments are done on four datasets: Waterbirds [24],
CelebA [15], Metashift [12], and Dominoes [18]. The de-
tails for these datasets are in Appendix 8.
Similar to all the works mentioned in Sec. 5.1, the model
we use in our experiments is ResNet-50 pre-trained on Im-
ageNet. For ERM training, on all datasets except Domi-
noes, we used random crop and random horizontal flip as
data augmentation, similar to [3, 8]. Retraining the last
layer of the model did not require data augmentation. Also,
to reduce the strong disturbance of class imbalance, we
used class-balanced data to retrain the last layer on CelebA,
which is the same approach we took to reproduce the results
of [3]. Model selection and hyper-parameter fine-tuning are
done according to the worst group accuracy on the vali-
dation set. For all the datasets, the value for αand the
proportion qof the selected data (according to their loss)
for combining have been chosen from {1,2, . . .10}, and
{0.2,0.4,0.5,0.6,0.8,1}respectively. For adaptive mask-
ing, we used [26] python implementation to determine the
optimal amount of masking.
In addition to the main method (DaC), we test anotherversion of our method, named DaC-C, which uses all the
correctly classified samples for making combined data, and
removes the hyperparameter q. Thus, it uses correct classi-
fication as a way for selecting low-loss samples.
For all datasets, we have trained the model in two set-
tings: one by assuming that the model generally attends to
the causal parts, and the other by the assumption that the
model trained by ERM attends more to the non-causal parts.
For all datasets except the Dominoes, the former has better
worst group accuracy on the validation set.
The details for training the base ERM model and training
the last layer of the model with DaC are in Appendix 8.
5.3. Results
The results of our experiments along with reported results
for DFR [8], Masktune [3], LISA [39], Group DRO [24],
and JTT [13] on four benchmarks are illustrated in Ta-
ble Tab. 1. Both the worst group accuracy and the aver-
age group accuracy as the most commonly used metrics to
evaluate robustness again spurious correlation have been
reported. Similar to [8], the Group Info column shows
whether the label of the group (majority/minority) to which
datapoints belong is available for training or validation data.
Among the methods in Tab. 1, only DFR requires group info
of validation data during the training phase (and not just for
model selection), which is shown by ✓✓.
The results of the methods annotated with ∗are repro-
duced by our own experiments. As for the other meth-
ods, the results on the Waterbirds and CelebA datasets are
from their original paper. The results for Metashift are re-
ported by [35]. Three methods among the baselines, i.e.
DFR, Group DRO, and LISA need the group label during
the training phase as mentioned in Tab. 1. According to
these results, our method outperforms other methods that
don’t require group labels during training with a large mar-
gin in both mean and worst group accuracy metrics on Wa-
terbirds, Dominoes, and Metahift datasets. Moreover, al-
though the proposed method does not need the group la-
bel of the training data, it outperforms Group DRO and
LISA on Waterbirds and Metashift datasets and is on par
with DFR on these datasets. It is worth mentioning that the
CelebA dataset does not match the type of spurious correla-
tion for which our method has been designed as mentioned
in Sec. 2. More precisely, in addition to the face (includ-
ing gender features) that can be considered as a non-causal
part for classifying the hair colour, there is also a spurious
attribute for the causal part (i.e. hair) in this problem. There
is a spurious correlation between the volume of the hair and
the label, as will be discussed in Appendix 8.
Unlike most previous methods, which usually do not
generalize well to samples with diversity shift, our method
can perform well also for diversity shift if it is due to novel
compositon in the scene. For more analysis of DaC-C,
27668
Table 1. A comparison of mean and worst group accuracy of several methods, including ours, on four datasets. The Group Info column
shows whether each method uses group labels of train/validation data, with ✓✓indicating that group info is used in both the training and
validation phases. The mean and std are reported over 3 runs on different seeds. The bold and underlined numbers indicate the best results
among all methods, and methods not requiring group annotation, respectively.
Group Info Waterbirds CelebA Metashift Dominoes
Method train/val Worst Average Worst Average Worst Average Worst Average
DFR*✗/✓✓ 92.3±0.2 93.3±0.5 88.3±1.1 91.3±0.3 72.8±0.6 77.5±0.6 90±0.492.3±0.2
Group DRO ✓/✓ 91.4±1.1 93.5±0.3 88.9±2.3 92.9±0.2 66.0±3.8 73.6±2.1 - -
LISA ✓/✓ 89.2±0.6 91.8±0.389.3±1.192.4±0.4 59.8±2.3 70.0±0.7 - -
MaskTune*✗/✗ 86.4±1.9 93.0±0.7 79.4 89 .5 66 .3±6.3 73.1±2.265.8±4.785.6±0.7
CnC ✗/✓ 88.5±0.3 90.9±0.1 88.8±0.9 89.9±0.5 - - - -
JTT ✗/✓ 86.7 93 .3 81 .1 88 .0 64 .6±2.3 74.4±0.6 - -
Base (ERM) ✗/✗ 70.8±0.5 91.6±0.1 41.7 96.0 61.3±3.4 73.9±1.572.8±1.688.5±0.3
DaC-C ✗/✓ 92.6±0.294.9±0.2 76.11±0 91.35±0.276.0±0.880.0±1.489.0±0.792.2±0.2
DaC ✗/✓ 92.3±0.495.3±0.481.9±0.7 91.4±1.178.3±1.679.3±0.189.2±0.192.2±0.3
please refer to Sec. 5.4.2.
5.4. Ablation Study
5.4.1 Effect of Combining Images
Combining images proves to be extremely effective in en-
hancing the worst test group accuracy, which is evident in
Fig. 5. It can be seen that the value of α≥1highly reduces
the reliance on spurious patterns and also the range of the
proper value of αis similar in different datasets.
0 2 4 6 8 1060708090
αWorst group accuracyDominoes Waterbirds
Metashift CelebA
Figure 5. Worst group accuracy on different datasets with respect
toα.α≥1is enough to increase worst group accuracy rapidly.
5.4.2 Effect of the Proportion of the Selected Data
As the number of the selected samples for combining in-
creases, more datapoint on which the base model has a
higher loss will be used for combining. The model is
more prone to attending to irrelevant parts in these sam-
ples, which results in the combined images being wrongly
labelled. As shown in Tab. 1, the model has the best worst
group accuracy when we choose the proportion of the se-
lected samples by hyper-parameter tuning. Selecting all thecorrectly-classified samples omit the need for tuning hyper-
parameter q, but it may degrade the accuracy of the method
when the ERM overfits on the training samples, since in this
case, all samples may be used for combination.
6. Conclusion
In this paper, we first showed that whether the models
trained with standard ERM pay more attention to the causal
or spurious parts of images in a dataset depends on the pre-
dictiveness of these parts across the entire dataset. Further-
more, in most realistic datasets, due to the lower correlation
of non-causal parts with the label compared to the causal
ones, ERM usually shows causal attention. We then utilized
attribution maps of an ERM model on images to decom-
pose them and find the significantly more attended parts,
by monitoring the classification loss of the ERM model on
masked images. According to this decomposition of im-
ages, we also suggested a method for combining images
with low loss which helps to mitigate the spurious correla-
tion and diversity shift. This method has proven to be highly
effective on four benchmarks and has a comparable perfor-
mance with methods that require minority/majority group
annotation of training data, unlike ours. Although this re-
search was primarily focused on spurious correlations be-
tween parts of images and labels, the idea could potentially
be extended to more complex scenarios where there is a spu-
rious correlation between attributes of the objects in a scene
and the label. Further research on more accurate methods
for distinguishing causal and non-causal parts, and more ad-
vanced interventions on images is left for future work.
Acknowledgments
We would like to thank Mr. Mohammad-Mahdi Samiei for
his insightful and constructive comments.
27669
References
[1] Faruk Ahmed, Yoshua Bengio, Harm van Seijen, and Aaron
Courville. Systematic generalisation with group invariant
predictions. In International Conference on Learning Rep-
resentations , 2021. 1
[2] Martin Arjovsky, L ´eon Bottou, Ishaan Gulrajani, and
David Lopez-Paz. Invariant risk minimization. ArXiv ,
abs/1907.02893, 2020. 1, 3
[3] Saeid Asgari, Aliasghar Khani, Fereshte Khani, Ali
Gholami, Linh Tran, Ali Mahdavi-Amiri, and Ghassan
Hamarneh. Masktune: Mitigating spurious correlations by
forcing to explore. In Advances in Neural Information Pro-
cessing Systems , 2022. 2, 4, 6, 7, 1, 3
[4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition
in terra incognita. In Computer Vision – ECCV 2018: 15th
European Conference, Munich, Germany, September 8-14,
2018, Proceedings, Part XVI , page 472–489, Berlin, Heidel-
berg, 2018. Springer-Verlag. 1
[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 770–778, 2016. 6
[6] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki,
and David Lopez-Paz. Simple data balancing achieves com-
petitive worst-group-accuracy. In Proceedings of the First
Conference on Causal Learning and Reasoning , pages 336–
351. PMLR, 2022. 1, 3
[7] Eungyeup Kim, Jihyeon Lee, and Jaegul Choo. Biaswap:
Removing dataset bias with bias-tailored swapping augmen-
tation. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV) , pages 14992–15001,
2021. 1
[8] P. Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson.
Last layer re-training is sufficient for robustness to spurious
correlations. ArXiv , abs/2204.02937, 2022. 1, 3, 5, 6, 7, 2
[9] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen,
Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le
Priol, and Aaron Courville. Out-of-distribution general-
ization via risk extrapolation (rex). In Proceedings of the
38th International Conference on Machine Learning , pages
5815–5826. PMLR, 2021. 1
[10] Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar.
Dropout disagreement: A recipe for group robustness with
fewer annotations. In NeurIPS 2022 Workshop on Distribu-
tion Shifts: Connecting Methods and Applications , 2022. 1
[11] Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and
Yun Fu. Tell me where to look: Guided attention inference
network. In 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 9215–9223, 2018. 1
[12] Weixin Liang and James Zou. Metashift: A dataset of
datasets for evaluating contextual distribution shifts and
training conflicts. In International Conference on Learning
Representations , 2022. 7
[13] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghu-
nathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and
Chelsea Finn. Just train twice: Improving group robustness
without training group information. In Proceedings of the38th International Conference on Machine Learning , pages
6781–6792. PMLR, 2021. 1, 6, 7
[14] Haozhe Liu, Wentian Zhang, Jinheng Xie, Haoqian Wu,
Bing Li, Ziqi Zhang, Yuexiang Li, Yawen Huang, Bernard
Ghanem, and Yefeng Zheng. Decoupled mixup for general-
ized visual recognition, 2022. 1
[15] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In 2015 IEEE In-
ternational Conference on Computer Vision (ICCV) , pages
3730–3738, 2015. 7, 2
[16] Nihal Murali, Aahlad Manas Puli, Ke Yu, Rajesh Ranganath,
and kayhan Batmanghelich. Shortcut learning through the
lens of early training dynamics, 2023. 1
[17] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and
Jinwoo Shin. Learning from failure: Training debiased clas-
sifier from biased classifier. In Proceedings of the 34th Inter-
national Conference on Neural Information Processing Sys-
tems, Red Hook, NY , USA, 2020. Curran Associates Inc. 1
[18] Matteo Pagliardini, Martin Jaggi, Franc ¸ois Fleuret, and
Sai Praneeth Karimireddy. Agree to disagree: Diversity
through disagreement for better transferability. In The
Eleventh International Conference on Learning Representa-
tions , 2023. 4, 7, 2
[19] Judea Pearl. Causality . Cambridge University Press, Cam-
bridge, UK, 2 edition, 2009. 3, 5
[20] Shikai Qiu, Andres Potapczynski, Pavel Izmailov, and An-
drew Gordon Wilson. Simple and fast group robustness by
automatic feature reweighting. ICML 2023 . 1
[21] Maan Qraitem, Kate Saenko, and Bryan A. Plummer. From
fake to real: Pretraining on balanced synthetic images to pre-
vent bias. ArXiv , abs/2308.04553, 2023. 2, 1
[22] Vikram V . Ramaswamy, Sunnie S. Y . Kim, and Olga Rus-
sakovsky. Fair attribute classification through latent space
de-biasing. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021 ,
pages 9301–9310. Computer Vision Foundation / IEEE,
2021. 1
[23] Alexandre Rame, Corentin Dancette, and Matthieu Cord.
Fishr: Invariant gradient variances for out-of-distribution
generalization. In Proceedings of the 39th International
Conference on Machine Learning , pages 18347–18377.
PMLR, 2022. 1
[24] Shiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto,
and Percy Liang. Distributionally robust neural networks.
InInternational Conference on Learning Representations ,
2020. 1, 3, 6, 7, 2
[25] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and
Percy Liang. An investigation of why overparameteriza-
tion exacerbates spurious correlations. In Proceedings of the
37th International Conference on Machine Learning , pages
8346–8356. PMLR, 2020. 1
[26] Ville A. Satopaa, Jeannie R. Albrecht, David E. Irwin, and
Barath Raghavan. Finding a ”kneedle” in a haystack: Detect-
ing knee points in system behavior. 2011 31st International
Conference on Distributed Computing Systems Workshops ,
pages 166–171, 2011. 7
27670
[27] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek
Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-
tra. Grad-cam: Visual explanations from deep networks
via gradient-based localization. In 2017 IEEE International
Conference on Computer Vision (ICCV) , pages 618–626,
2017. 4
[28] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek
Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias
in neural networks. Advances in Neural Information Pro-
cessing Systems , 33, 2020. 1
[29] Antonio Torralba and Alexei A. Efros. Unbiased look at
dataset bias. In CVPR 2011 , pages 1521–1528, 2011. 1
[30] Puja Trivedi, Danai Koutra, and Jayaraman J. Thiagarajan. A
closer look at model adaptation using feature distortion and
simplicity bias. In The Eleventh International Conference on
Learning Representations , 2023. 1
[31] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The caltech-ucsd birds-200-2011 dataset. Technical Report
CNS-TR-2011-001, California Institute of Technology , 2011.
2
[32] H. Wang, Z. Wang, M. Du, F. Yang, Z. Zhang, S. Ding,
P. Mardziel, and X. Hu. Score-cam: Score-weighted vi-
sual explanations for convolutional neural networks. In
2020 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition Workshops (CVPRW) , pages 111–119, Los
Alamitos, CA, USA, 2020. IEEE Computer Society. 4
[33] Tan Wang, Chang Zhou, Qianru Sun, and Hanwang Zhang.
Causal attention for unbiased visual recognition. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 3091–3100, 2021. 1
[34] Xinyue Wang, Yilin Lyu, and Liping Jing. Deep generative
model for robust imbalance classification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020. 1
[35] Shirley Wu, Mert Yuksekgonul, Linjun Zhang, and James
Zou. Discover and cure: Concept-aware mitigation of spuri-
ous correlation. arXiv preprint arXiv:2305.00650 , 2023. 2,
7, 1
[36] Yao Xiao, Ziyi Tang, Pengxu Wei, Cong Liu, and Liang Lin.
Masked images are counterfactual samples for robust fine-
tuning. 2023 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 20301–20310, 2023. 2,
1
[37] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie
Wang, Qi Tian, and Wenjun Zhang. Adversarial domain
adaptation with domain mixup. Proceedings of the AAAI
Conference on Artificial Intelligence , 34:6502–6509, 2020.
1
[38] Wanqian Yang, Polina Kirichenko, Micah Goldblum, and
Andrew G Wilson. Chroma-vae: Mitigating shortcut learn-
ing with generative classifiers. In Advances in Neural In-
formation Processing Systems , pages 20351–20365. Curran
Associates, Inc., 2022. 1
[39] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang,
James Zou, and Chelsea Finn. Improving out-of-distribution
robustness via selective augmentation. In International Con-
ference on Machine Learning, ICML 2022, 17-23 July 2022,Baltimore, Maryland, USA , pages 25407–25437. PMLR,
2022. 6, 7, 1
[40] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing
Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench:
Quantifying and understanding two dimensions of out-of-
distribution generalization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 7947–7958, 2022. 2
[41] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. In International Conference on Learning Representa-
tions , 2018. 1
[42] Michael Zhang, Nimit S Sohoni, Hongyang R Zhang,
Chelsea Finn, and Christopher Re. Correct-n-contrast: a con-
trastive approach for improving robustness to spurious corre-
lations. In Proceedings of the 39th International Conference
on Machine Learning , pages 26484–26516. PMLR, 2022. 6
[43] Heliang Zheng, Jianlong Fu, Tao Mei, and Jiebo Luo. Learn-
ing multi-attention convolutional neural network for fine-
grained image recognition. In 2017 IEEE International
Conference on Computer Vision (ICCV) , pages 5219–5227,
2017. 1
[44] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence , 2017. 2
27671
