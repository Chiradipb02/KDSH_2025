Fair Federated Learning under Domain Skew with
Local Consistency and Domain Diversity
Yuhang Chen1‚àóWenke Huang1‚àóMang Ye1,2‚Ä†
1National Engineering Research Center for Multimedia Software,
School of Computer Science, Wuhan University, Wuhan, China
2Taikang Center for Life and Medical Sciences, Wuhan University, Wuhan, China
{yhchen0,wenkehuang,yemang }@whu.edu.cn
https://github.com/yuhangchen0/FedHEAL
Abstract
Federated learning (FL) has emerged as a new paradigm
for privacy-preserving collaborative training. Under do-
main skew, the current FL approaches are biased and face
two fairness problems. 1) Parameter Update Conflict: data
disparity among clients leads to varying parameter impor-
tance and inconsistent update directions. These two dis-
parities cause important parameters to potentially be over-
whelmed by unimportant ones of dominant updates. It con-
sequently results in significant performance decreases for
lower-performing clients. 2) Model Aggregation Bias: ex-
isting FL approaches introduce unfair weight allocation
and neglect domain diversity. It leads to biased model
convergence objective and distinct performance among do-
mains. We discover a pronounced directional update con-
sistency in Federated Learning and propose a novel frame-
work to tackle above issues. First, leveraging the discovered
characteristic, we selectively discard unimportant parame-
ter updates to prevent updates from clients with lower per-
formance overwhelmed by unimportant parameters, result-
ing in fairer generalization performance. Second, we pro-
pose a fair aggregation objective to prevent global model
bias towards some domains, ensuring that the global model
continuously aligns with an unbiased model. The proposed
method is generic and can be combined with other existing
FL methods to enhance fairness. Comprehensive experi-
ments on Digits and Office-Caltech demonstrate the high
fairness and performance of our method.
1. Introduction
Federated learning (FL) aims to collaboratively train a high-
performance model while maintaining data privacy [28, 36].
*Equal contributions.‚Ä†Corresponding author.
Aggregation Bias
020406080100Global Model Client Client
Low Decrease Significant DecreaseUpdate Conflict
counteractGlobal Model
unimportant important
Webcam+
Amazon
Caltech
Parameter ùëñùëñ
Local Update Local Update
30405060708090100FedAvg w/o HEAL FedAvg w HEAL
USPS                    MNIST                    SVHN                      SYNSTD:23.82 STD:22.08Figure 1. Problem illustration of Federated Learning under do-
main skew. Conventional FL methods ( ) exhibit potential perfor-
mance disparities due to Parameter Update Conflicts and Model
Aggregation Bias. The former indicates that varying parameter
importance and inconsistent update directions lead to an un-
fair decline in aggregated performance. The latter suggests biased
convergence objective, resulting in performance disparities . Our
method ( ) achieves more equitable performance across different
domains while enhancing overall performance.
The foundational method, FedAvg [36], allows numerous
participants to send their models to the server instead of
data. Then, the server aggregates these models into a
global model and sends it back for further training. No-
tably, a significant challenge in FL is data heterogeneity
[20, 22, 28, 29, 55] , which means that client data appears in
a Non-IID (non-independently and identically distributed)
manner [32, 50, 54, 56, 59]. One particular heterogeneity
type, domain skew [12, 16, 31, 55], refers to the client data
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12077
being sampled from various domains , leading to different
feature distributions for each client.
Under domain skew, the local data are sampled from
multiple domains, resulting in a significant disparity in dis-
tributed data. This disparity introduces challenges of feder-
ated convergent inconsistency. Meanwhile, federated learn-
ing aims to achieve a lower overall loss [36]. These two
factors lead to FL being biased toward domains with easier
convergence, further resulting in the neglect of other do-
mains. This bias leads to distinct performance among do-
mains. However, if some clients feel undervalued, their mo-
tivation to participate in the federation will diminish, lead-
ing to a narrow scope of knowledge in the federation, hin-
dering its growth and contradicting original intent of FL.
This issue gives rise to a pivotal challenge in FL: Perfor-
mance Fairness [17, 45], which aims to ensure the uni-
form performance across different clients without neglect-
ing clients with inferior performance. The fairness issue
is highlighted in Fig. 1, where preliminary methods might
overfit some domains, leading to poor performance in other
domains. We argue that two primary reasons underlie this
fairness issue: I Parameter Update Conflict :The incon-
sistent parameter update directions and varying parameter
importance lead to conflicts between important and unim-
portant parameters, degrading the performance of clients
with poorer results. Due to domain skew, there can be in-
consistencies in parameter update directions among clients.
Furthermore, some parameters of the neural network are
more important to specific data [24, 46, 53], meaning that
changes in these parameters have a larger impact on perfor-
mance. Domain skew results in varying parameter impor-
tance. So important updates from poor-performing client
may be potentially overwhelmed by unimportant aspects of
others. But the latter can not signally boost performance.
So it finally leads to performance disparity. II Model Ag-
gregation Bias :The general weighting distribution method
is biased and neglects domain diversity, resulting in un-
fair convergence objective and performance disparity. In
conventional FL methods [29, 36], the strategy of weight-
ing proportional to sample quantity [36] hinders the global
model from adequately learning from domains with few
samples. Alternatively, equal weighting overly emphasizes
clients with fewer samples. Both strategies introduce biases
and amplifying performance diversity. This bias disregards
the data diversity among different domains.
To address these issues, we present a novel solu-
tion, Federated Parameter- Harmonized and Aggregation-
EquAlized Learning (FedHEAL). For problem I, we ob-
serve notable consistency in parameters updating during
the local training. Specifically, due to the unique do-
main knowledge, some parameters are consistently pushed
to the same direction ( i.e., increment and decrement) dur-
ing local training across consecutive rounds, as detailed inSec. 3.3.1. Parameters with strong consistency occur be-
cause the global model fails to adapt to certain domains,
leading to repeated adjustments in the same direction. Mo-
tivated by this, we argue that parameters with strong con-
sistency are more crucial for specific domain. To mitigate
parameter update conflict, we aim to prevent unimportant
parameters from nullifying the crucial updates of domains
with poor performance. By including only essential param-
eters, we mitigate important updates from being drowned
out by less important ones, thus promoting Performance
Fairness across multiple domains and clients.
For problem II, we argue that more diverse domains re-
sult in larger model changes during local training. To ac-
count for the data diversity, we propose an optimization
objective that minimizes the variance of distances between
the global model and all the local models. By reducing the
variance, the global model maintains uniform distances to
all the local models, preventing bias towards any clients.
However, the extensive parameters in neural networks and
the large number of clients in FL make it computationally
intensive. Thus, we propose a simplified method approxi-
mately aligned with this fairness objective. Details of our
simplified approach are elaborated in Sec. 3.3.2.
In this paper, FedHEAL consists of two components.
First , by discarding unimportant parameters, we mitigate
conflict among parameter updates. Second , we present a
fair aggregation objective and a simplified implementation
to prevent global model bias towards some domains. Fed-
HEAL ultimately achieves Performance Fairness under do-
main skew. Since our approach only focuses on aggrega-
tion, it can be easily integrated with most existing FL meth-
ods. Our main contributions are summarized as follows:
‚Ä¢ We identify the parameter update consistency in FL and
introduce a partial parameter update method to update
only parameters significant to the local domain, enhanc-
ing fair performance across domains.
‚Ä¢ We propose a new fair federated aggregation objective
and a practical approach to consider the domain diversity
to improve Performance Fairness.
‚Ä¢ We conduct comprehensive experiments on the Digits
[18, 25, 40, 43] and Office-Caltech [9] datasets, providing
evidence of effectiveness of our method through ablation
studies and integrations with existing methods.
2. Related Work
2.1. Federated Learning with Data Heterogeneity
Federated learning aims to collaboratively train models
without centralizing data to protect privacy. The pioneer-
ing work, FedAvg [36], trains a global model by aggregat-
ing participants‚Äô local model parameter. However, FedAvg
is primarily designed for homogeneous data, and its perfor-
mance degrades under data heterogeneity. Numerous meth-
12078
Notation Description Notation Description
m Client Index pm Client Weight
i Parameter Index qm,i Parameter Weight
M Client V olume lm,i Increment Proportion
G Parameter V olume cm,i PUC of Parameter
Nm Sample Size ‚àÜwt
m Model Update
WtGlobal Model ‚àÜwt
m,i Parameter Update
wt
m Client Model dm Model Distance
œÑ Importance Threshold Œ≤ Update Momentum
Table 1. Notation table of this paper.
ods based on FedAvg have emerged to address data hetero-
geneity [7, 8, 13, 14, 34, 44]. FedProx [29], SCAFFOLD
[22] and FedDyn [1] constrain local updates by adding
penalty terms. FedProto [48] and MOON [26] enhance the
alignment between client-side training at the feature level.
However, these methods overlook the issue of domain skew,
leading to diminished performance in multi-domain scenar-
ios. Some methods have now been developed to address do-
main skew, such as FedBN [31] and FCCL [12, 15]. How-
ever, these methods focus on personalized models rather
than shared models, the latter of which requires additional
public datasets. FPL [16] focuses on addressing domain
skew but requires each client to upload high-level feature
information, contradicting the privacy-preserving nature of
FL. FedGA [58] and FedDG [33] focus on the problem of
unseen domain generalization, but the former requires an
additional validation set, and the latter involves transmit-
ting data information among multiple clients, posing pri-
vacy leakage concerns. In this paper, FedHEAL does not
require any additional datasets or the transmission of addi-
tional signal information. We solely focus on the most fun-
damental transmitted information in FL: the model updates
themselves, to extract the necessary information for enhanc-
ing Performance Fairness in multi-domain scenarios.
2.2. Fair Federated Learning
The fairness in FL is currently of widespread interest
[4, 11, 41]. The mainstream categorizations of federated
fairness fall into three classes [19, 45]: Performance Fair-
ness [19, 27, 30, 38], Individual/Group Fairness [5, 6, 57],
and Collaborative Fairness [19, 35, 42, 52, 60]. Perfor-
mance fairness ensures that all participants experience sim-
ilar and equitable performance improvements. Individ-
ual/Group Fairness aims to minimize model bias towards
specific attributes( e.g., gender). Collaborative Fairness en-
sures that participants are rewarded in proportion to contri-
butions. This paper primarily addresses the issue of Perfor-
mance Fairness. AFL [38] utilizes a min-max optimization
to boost the performance of the worst-performing clients.
q-Fedavg [27] recalibrates the aggregate loss by assigning
higher weights to devices with higher losses to enhance per-
formance fairness. FedFV [49] uses the cosine similarity
to detect and eliminate gradient conflicts to achieve Per-
formance Fairness. But they are tailored for label skewNon-IID data and do not consider domain skew. Ditto [30]
enhances Performance Fairness by incorporating a penalty
term but employs a personalized model instead of a shared
model. FedCE [19] addresses both Performance Fairness
and Collaborative Fairness but necessitates an additional
validation set, a requirement that is challenging to meet
given the scarcity of client data in FL. Our method is tai-
lored for Performance Fairness under domain skew and con-
sider both the domain diversity. It can be easily integrated
with existing methods to enhance their fairness.
3. Methodology
3.1. Preliminaries
Federated Learning . Following typical Federated Learn-
ing setup [29, 36, 37], we consider there are Mclients
(indexed by m). Each client holds private data Dm=
{xi, yi}Nm
i=1, where Nmrepresents the data size of client m.
The optimization objective of FL is to minimize global loss:
min
wF(w) =MX
m=1pmfm(w), (1)
where fm(w) =1
NmPNm
i=1L(xi, yi;w),pmis the weight
of client. L(xi, yi;w)is the loss of data (xi, yi)with model
parameters w. Each client updates its model locally, and the
server then aggregates model updates from all clients.
Domain Skew . In heterogeneous federated learning, do-
main skew among private data occurs when the marginal
distribution of labels P(y)is consistent across clients, but
the conditional distribution of features given labels P(x|y)
varies among different clients [3, 12, 16, 21, 31, 51]:
Pm(x|y)Ã∏=Pn(x|y)while Pm(y) =Pn(y). (2)
3.2. Motivation
Observation of Parameter Update Consistency . In het-
erogeneous federated learning, grasping the characteristic
of model updates across clients is crucial. This study intro-
duces a novel observation, termed as Parameter Update
Consistency (PUC), observed during local training phases.
Through a toy experiment involving 4 clients, each sam-
pling from distinct domains and training with a ResNet-10
network, we observe a significant Parameter Update Con-
sistency during local training. As shown in Fig. 2, our
findings indicate that a substantial proportion of parame-
ters maintain consistent update directions in consecutive
rounds of training. Specifically, most parameters demon-
strate significant update consistency. This consistency is no-
ticeable in shorter consecutive rounds (10 rounds) but per-
sists even in the longer term (100 rounds), underscoring its
enduring nature. Furthermore, the consistency observed in
the last 10 rounds reveals that PUC remains prominent as
the global model converges, indicating that the converged
global model has not adapted to specific domains . These
12079
00.10.20.30.40.50.60.7
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%
ConsistencyLast 10 roundsLocal Update00.10.20.30.40.50.6
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%All 100 rounds
round 0
round 1
round ùë°ùë°Other Client
Parameter ùëñùëñClient ùëöùëö
Parameter ùëñùëñ
Consistent Update DirectionAggregateIncrement Decrement
ImportantParameter ProportionFigure 2. Illustration of Parameter Update Consistency . The
consistency of parameter updates is displayed over 10 and 100
rounds for a randomly selected layer of the client model update.
A significant proportion of parameters maintain a consistent up-
date direction, i.e., almost half of the parameters show the same
direction for over 90 of the 100 rounds, indicating a persistent ten-
dency to steer the global model in a fixed direction.
observations suggest a potential global model bias towards
other domains, emphasizing the necessity for strategies that
mitigate such biases and ensure fairer model aggregation.
Inspired by our observations and insights, to tackle Prob-
lemI, we categorize parameters into two distinct classes:
important and unimportant. Important parameters are char-
acterized by stable update directions in consecutive rounds,
indicating consistent learning behavior within specific do-
mains and inadequacy of the global model in fitting those
domains. These parameters are deemed crucial for offer-
ing greater contribution in performance improvement. Con-
versely, unimportant parameters exhibit distinct directional
changes across rounds which provide little contribution in
performance improvement. We argue that involving unim-
portant parameters in the updates exacerbates the parameter
update conflict, leading to a decline in the performance of
some domains and resulting in unfair performance. There-
fore, to mitigate this, we aim to minimize the impact of
unimportant updates, enhancing the performance of poorly
performing domains. During parameter aggregation, we
discard unimportant parameters by setting their weights to
zero, which prevents them from participating in aggregation
and mitigates the parameter update conflict. Then we nor-
malize all weights of a local model parameter. Those with
a weight of zero remain at zero but the weights of impor-
tant parameters increase, amplifying their influence on the
global model and improving its adaptation to underperform-
ing domains. The proposed method is detailed in Sec. 3.3.1.
To address Problem II, we employ domain diversity as
the guiding metric for allocating weights. Our intuition
is that more diverse data will undergo larger parameter
changes for the local model to adapt to that domain. The pa-
rameter changes exhibit the fitting gap between the globalmodel and local model, implying the potential for perfor-
mance improvement. A larger model changes suggests a
domain is overlooked and has greater space for performance
enhancement, which can also be reflected in the magnitude
of model parameter updates. The distance between model
iterations can represent the extent of parameter updates.
Thus, this issue can be addressed through an optimization
objective that minimizes the variance of distances between
the global model and each client model. By reducing the
variance, the global model maintains a more uniform dis-
tance to each client. However, this optimization problem
is computationally intensive, so we introduce a simplified
approach. We modify the weight allocation strategy with a
momentum update [23, 47], assign greater weight to clients
that induce larger parameter changes. This method approx-
imately aligns with our objective and draws the new global
model closer to neglected clients, thereby reducing the vari-
ance. We elaborate the proposed method in Sec. 3.3.2.
3.3. Proposed Method
3.3.1 Federated Parameter-Harmonized Learning
In a Federated Learning system with Mclients, the global
model in round tis denoted as Wt. In round t, each client
trains their model on private data Dmto obtain local model
wt
m. The local model change is defined as ‚àÜwt
m=wt
m‚àí
Wt. The global model of next round, t+ 1, is then updated
by aggregating the model updates of round t:
Wt+1=Wt+MX
m=1pm‚àÜwt
m, (3)
where pmis the aggregation weight of client m, typically
proportion to local sample size( i.e.,pm=NmPM
j=1Nj). In
neural networks, wt
mis potentially a large vector of param-
eters, and ‚àÜwt
mis a vector of parameter changes with the
same dimension. For simplicity, we disregard the internal
structure of the model and represent ‚àÜwt
mas
‚àÜwt
m= [‚àÜwt
m,1,‚àÜwt
m,2, . . . , ‚àÜwt
m,G], (4)
where Gdenotes the total number of parameters, and
‚àÜwt
m,irepresents the change in the ithparameter of the
model wt
m. We aim to compute the PUC for each param-
eter across all clients. This computation characterizes how
significantly the parameters of the global model are pushed
towards a consistent direction to fit specific domains. We
maintain a list of proportions for parameters that are in-
creasing for each client and then employ dynamic program-
ming [2] to update this list in each round. The list of incre-
ment proportions for client mis denoted as
Lm= [lm,1, lm,2, . . . , l m,G],
lm,i=Pt‚àí1
j=0I(‚àÜwj
m,i‚â•0)
t,(5)
where I(¬∑)is the indicator function and lm,iis the propor-
tion of increasing parameter before round t. Applying Dy-
12080
Consistency Table Model Update Œîùë§ùë§ùëöùëöùë°ùë°
Discard for ùëêùëêùëöùëö,ùëñùëñ<ùúèùúèRemain for ùëêùëêùëöùëö,ùëñùëñ‚â•ùúèùúè
ùëûùëûùëöùëö,ùëñùëñ=ùêºùêºùëêùëêùëöùëö,ùëñùëñ‚â•ùúèùúèùëùùëùùëöùëö
‚àëùëóùëó=1ùëÄùëÄùêºùêºùëêùëêùëöùëö,ùëñùëñ‚â•ùúèùúèùëùùëùùëóùëóIncrement Proportion
Server Side
ùëêùëêùëöùëöùë°ùë°
Œîùë§ùë§ùëöùëö0Œîùë§ùë§ùëöùëöùë°ùë°‚àí1+
-+-
Update Historyùëôùëôùëöùëö,ùëñùëñùêøùêøùëöùëöùë°ùë°‚àí1ùêøùêøùëöùëöùë°ùë°Server Side
+
+
+-
Server Receive
ùëêùëêùëöùëö,ùëñùëñ: Consistency of    
Parameter ùëñùëñ
ùëûùëûùëöùëöŒîùë§ùë§ùëöùëöùë°ùë°ùëûùëûùëõùëõŒîùë§ùë§ùëõùëõùë°ùë°
ùëûùëûùëêùëêŒîùë§ùë§ùëêùëêùë°ùë°ùëùùëùùëöùëö‚Üìùëùùëùùëõùëõ‚Üë
ùëùùëùc‚ÜëùëëùëëùëöùëöEq. (9)
Eq. (5)Eq. (6) Eq. (7)
Obtain dmby Eq. (11)Update pmby Eq. (12)
(a) Federated Parameter-Harmonized Learning (b) Federated Aggregation-Equalized Learning
Figure 3. Architecture illustration of FedHEAL. Clients send local model updates to the server. In FPHL (Sec. 3.3.1), the server maintains
a consistency table, computes the consistency of current updates with past directions and discards updates with low consistency. Then in
FAEL (Sec. 3.3.2), server minimized the variance of distance between global model and client model to mitigate model aggregation bias.
namic Programming [2], the lm,iin the round tcan be cal-
culated from the previous round‚Äôs result:
lm,i‚Üêlm,i‚àó(t‚àí1) +I(‚àÜwt
m,i‚â•0)
t. (6)
Then the PUC of ithparameter can be expressed as
cm,i=PUC (wm,i) =(
lm,i if‚àÜwt
m,i‚â•0,
1‚àílm,i otherwise.(7)
To alleviate parameter update conflicts and mitigate the
global model from experiencing unfair performance, we se-
lect parameters with a strong PUC for retention and discard
others. We introduce a hyperparameter œÑfor this purpose.
We categorize the ithparameter as important if cm,i‚â•œÑ
and as unimportant otherwise. Given the model aggrega-
tion process in Eq. (3), the aggregation of ithparameter is
Wt+1
i=Wt
i+MX
m=1pm‚àÜwt
m,i, (8)
where Wt
iis the ithparameter of global model Wt. By
applying our method, Eq. (8) can be reformulated as
Wt+1
i=Wt
i+MX
m=1qt
m,i‚àÜwt
m,i,
qt
m,i=I(cm,i‚â•œÑ)pmPM
j=1I(cj,i‚â•œÑ)pj,(9)
where pmis the aggregation weight of client mand all its
parameters, qt
m,izeros out the weights of insignificant pa-
rameters and then further normalizes them to ensure the ag-
gregation weights sum up to 1. Consequently, only impor-
tant parameters will participate and unimportant updates no
longer impact important updates. By normalizing qt
m,i, the
proportion of important parameter updates is further ampli-fied, enhancing their contribution during aggregation.
3.3.2 Federated Aggregation-Equalized Learning
We first define the distance between the global model and
the local model of client min round tasdm=‚à•U‚àíwt
m‚à•2
2,
where Uis the new global model and ‚à•¬∑‚à•2
2is the square of
the euclidean distance. To minimize the variance of dis-
tances between the global model and each client model, we
introduce the following optimization objective:
U‚àó
t= arg min
UVar({dm}M
m=1)
= arg min
UVar({U‚àíwt
m2
2}M
m=1)
s.t.U=MX
m=1pmwt
m,MX
m=1pm= 1,and‚àÄm, pm‚â•0,(10)
where U‚àó
trepresents the unbiased global model. The time
complexity of this optimization is O(qMG ), where qis
the number of iterations needed for convergence. However,
computational resources are often limited in FL. Thus, we
propose a simplified approach that reduces the time com-
plexity to O(MG), requiring only a single distance calcula-
tion for each client. Specifically, in round t, we measure the
distance between the trained client model and global model
Wtasdm=||‚àÜwt
m||2
2. Notably, if we combine it with
FPHL, dmcan be rewritten as
dm=||GX
i=1I(cm,i‚â•œÑ)¬∑‚àÜwt
m,i||2
2, (11)
implying that we only compute the important parameter
change distance which better reflects the alterations made
for specific domain. We then apply a momentum update
12081
strategy [23, 47] to update the weight for each client:
‚àÜpt
m= (1‚àíŒ≤)‚àÜpt‚àí1
m+Œ≤dmPM
j=1dj,
pt
m=pt‚àí1
m+ ‚àÜpt
m, pt
m=pt
mPM
j=1pt
j.(12)
where Œ≤is a hyper-parameter, with larger values indicating
a more pronounced influence of the distance on pm. When
Œ≤= 0, the method degenerates to the FedAvg. At Œ≤= 1,
the weights are assigned purely based on the distance set of
current round. The simplified method strives to minimize
the variance, aligning with the unbiased global model.
3.4. Discussion and Limitation
The key notations are summarized in Tab. 1 and the pseudo-
code of FedHEAL is presented in Algorithm 1.
Comparison with Analogous Methods . q-FFL [27] and
FedCE [19] increase weights for poor-performing clients
based on single loss or accuracy metrics. However, in-
creasing the weights of these clients does not guarantee sig-
nificant performance improvement. FAEL adjusts weights
based on the domain diversity, which implies fitting gap be-
tween the global model and local models. It is a better way
is to infer the effectiveness of weight modification based on
the potential space for performance improvement. Similar
work FedFV [49] alleviates model gradient conflicts. but it
modifies gradients based on cosine similarity and gradient
projection of the model, still allows less significant gradi-
ents to influence crucial ones. In contrast, FPHL, grounded
in the observed PUC characteristics, selectively discards
unimportant updates to safeguard the important ones. It
demonstrates targeted conflict resolution, leading to better
fairness and higher overall performance.
Discussion on FPHL . FPHL selectively discards unimpor-
tant parameters to reduce update conflicts, meaning it is
particularly effective in large-scale FL systems where such
conflicts are more pronounced. Similar with other meth-
ods aimed at Performance Fairness [27, 38], FPHL can
increase the influence of clients with poorer performance,
which may sometimes reduce the relative weight and per-
formance of other clients. Yet, by discarding unimportant
parameters uniformly, FPHL can diminish the adverse im-
pact of both poorly performing client updates and better-
performing clients to some extent. Consequently, while it
significantly boosts the performance of the former, it may
also help to prevent performance decline in the latter. So it
achieves higher average accuracy across domains.
Limitation . Our method leverages the parameter update
consistency and the fitting gap between the global model
and local models to guide parameter aggregation and client
aggregation weights. However, our method‚Äôs performance
is sensitive to the selected hyperparameters. When a hyper-
parameter is not selected properly, our method may become
unstable. Additionally, our method is designed for scenar-Algorithm 1: FedHEAL
Input: Communication rounds T, local epochs K,
number of participants M,mthparticipant private
dataDm, private model wm
Output: The final global model wT
Server : initialize the global model w0and
L0
m= [0,0, . . . , 0]G
fort= 0,1,2, ..., T ‚àí1do
Client :
form= 1,2, ..., M in parallel do
wt
m‚Üê Wt
fork= 1,2, ...,Kdo
wt
m‚Üêwt
m‚àíŒ∑‚àáCE(wt
m, Dm)
‚àÜwt
m‚Üêwt
m‚àí Wt
Server :
qt, Lt‚ÜêFedHEAL (Lt‚àí1)
fori= 1,2, . . . , G do
Wt+1
i=Wt
i+PM
m=1qt
m,i‚àÜwt
m,i
FedHEAL (Lt‚àí1):
form= 1,2, . . . , M do
fori= 1,2, . . . , G do
lm,i‚Üê(lm,i,‚àÜwt
m,i)in Eq. (6)
cm,i‚Üê(lm,i,‚àÜwt
m,i)in Eq. (7)
dm‚Üê(cm,i,‚àÜwt
m,i)in Eq. (11)
form= 1,2, . . . , M do
pt
m,‚àÜpt
m‚Üê(pt‚àí1
m,‚àÜpt‚àí1
m,‚àÜwt
m,i, Dt, Œ≤)in
Eq. (12)
fori= 1,2, . . . , G do
qt
m,i‚Üê(pt
m, ct
m,i)in Eq. (9)
return qt,Lt
ios where all clients share the same network architecture, so
it may fail in cases where clients have different architectures
and parameter update consistency cannot be assumed.
4. Experiments
4.1. Experiment Details
Datasets . We evaluate our methods on two multi-domain
image classification tasks.
‚Ä¢ Digits [18, 25, 40, 43] includes four domains: MNIST,
USPS, SVHN and SYN, each with 10 categories.
‚Ä¢ Office-Caltech [9] includes four domains: Caltech, Ama-
zon, Webcam, and DSLR, each with 10 categories.
We allocate 20 clients for each task and distribute an equal
number of clients to each domain. We randomly sample a
certain proportion for each client from their datasets, based
on task difficulty and task size. Specifically, we sample 1%
for Digits and 10% for Office-Caltech. We fix the seed to
ensure reproduction of results. The example cases in each
domain are presented in Fig. 4.
Model . For both classification tasks, we use ResNet-10 [10]
as the shared model architecture for training.
12082
Digits Office-CaltechMethodsMNIST USPS SVHN SYN A VG‚Üë‚Üë‚ÜëSTD‚Üì‚Üì‚Üì Amazon DSLR Caltech Webcam A VG‚Üë‚Üë‚ÜëSTD‚Üì‚Üì‚Üì
FedAvg [36] 89.84 93.25 79.54 41.35 76.00 23.82 72.63 56.67 58.57 45.52 58.35 11.13
+AFL [38] 90.59 95.83 75.13 44.42 76.49 23.12 64.21 65.37 57.50 48.28 58.83 7.84
+q-FFL [27] 91.44 94.10 76.33 44.48 76.59 22.79 60.00 64.01 53.39 51.72 57.28 5.73
+FedHEAL 90.27 95.69 79.94 46.45 78.09 22.08 67.90 66.00 59.28 66.21 64.85 3.80
FedProx [29] 90.27 93.93 80.04 42.82 76.76 23.38 69.90 58.00 60.27 45.52 58.42 10.03
+AFL [38] 92.86 96.17 74.47 42.22 76.43 24.72 68.10 62.67 59.29 52.41 60.62 6.57
+q-FFL [27] 88.58 93.49 75.58 44.23 75.47 22.15 61.37 72.66 54.91 55.52 61.11 8.23
+FedHEAL 89.06 95.52 79.44 46.67 77.67 21.70 66.11 72.67 57.50 67.59 65.97 6.30
Scaffold [22] 94.15 94.44 76.87 44.22 77.42 23.61 69.37 59.33 59.55 46.21 58.62 9.50
+AFL [38] 91.77 96.05 78.60 46.39 78.20 22.47 66.42 63.33 59.11 49.31 59.54 7.45
+q-FFL [27] 87.73 94.59 74.00 43.76 75.02 22.53 61.79 73.33 55.18 55.86 61.54 8.40
+FedHEAL 92.68 96.25 78.54 47.72 78.80 22.08 64.11 67.99 55.18 62.41 62.42 5.37
MOON [26] 90.46 92.65 80.48 40.58 76.04 24.23 74.00 59.33 60.63 46.90 60.21 11.08
+AFL [38] 91.25 96.03 75.31 44.34 76.73 23.34 66.74 67.33 60.80 55.17 62.51 5.71
+q-FFL [27] 90.43 94.84 76.48 43.95 76.42 23.02 64.32 65.33 54.28 61.03 61.24 4.99
+FedHEAL 91.34 94.94 81.32 44.96 78.14 22.86 67.68 65.33 59.11 64.14 64.07 3.62
FedDyn [1] 91.23 92.36 80.15 41.55 76.32 23.83 71.16 62.00 59.20 48.62 60.24 9.28
+AFL [38] 92.11 96.10 71.46 41.52 75.30 24.97 70.10 58.67 59.82 51.03 59.91 7.84
+q-FFL [27] 92.53 95.17 76.37 44.75 77.20 23.18 62.10 67.33 54.82 56.21 60.12 5.76
+FedHEAL 89.87 95.00 80.18 44.23 77.32 22.90 67.47 60.66 59.02 54.83 60.50 5.26
FedProc [39] 91.86 91.16 78.54 39.87 75.36 24.44 60.21 46.00 55.98 46.90 52.27 6.95
+AFL [38] 87.85 94.28 78.52 41.54 75.55 23.58 52.63 52.67 55.09 43.45 50.96 5.14
+q-FFL [27] 92.09 92.09 74.97 45.21 76.15 22.17 65.79 42.01 55.80 50.69 53.57 9.94
+FedHEAL 94.23 92.93 81.43 48.67 79.31 21.22 67.58 66.00 56.79 61.38 62.94 4.87
FedProto [48] 89.99 92.90 81.09 40.93 76.23 24.06 71.48 42.67 62.23 60.34 59.18 12.04
+AFL [38] 85.27 92.90 67.16 42.36 71.92 22.47 70.74 56.67 57.77 79.65 66.21 11.01
+q-FFL [27] 93.35 94.92 77.08 46.31 77.91 22.56 72.74 54.67 64.20 82.76 68.59 11.99
+FedHEAL 88.49 94.62 81.39 48.46 78.24 20.58 75.68 76.00 65.18 80.34 74.30 6.44
Table 2. Comparison of Average Accuracy( A VG ) and Standard Deviation( STD ) with AFL [38] and q-FFL [27]. See details in Sec. 4.3.
MNIST(M) USPS(U)
SVHN(SV) SYN(SY)
(a) Digits
Amazon(A) Caltech(C)
Webcam(W) Dslr(D)
 (b) Office-Caltech
Figure 4. Example cases in Digits [18, 25, 40, 43], Office-Caltech
[9] tasks. Please see details in Sec. 4.1.
Comparison Methods . We compare FedHEAL with FL
baseline FedAvg [36] and existing solutions for Perfor-
mance Fairness: AFL [38], q-FFL [27] (both integrable),
FedFV [49], Ditto [30] (two independent methods, with per-
sonalized models aggregated into global model for Ditto).
Implementation Details . All methods are implemented
with the same settings. We set the communication rounds to
200 and the local epoch to 10. We use SGD as the optimizer
with a learning rate of 0.001. Its weight decay is 1e‚àí5and
momentum is 0.9. The training batch size is 64for Digits
and16for Office-Caltech. The hyper-parameter setting for
FedHEAL presents in the Sec. 4.2.
Evaluation Metrics . Following [27], we utilize the Top-
1 accuracy and the standard deviation of accuracy across
multi-domains as evaluation metrics. A smaller standarddeviation indicates better Performance Fairness across dif-
ferent domains. We use the average results from the last five
rounds accuracy and variance as the final performance.
DigitsMethodsMNIST USPS SVHN SYN A VG‚Üë‚Üë‚ÜëSTD‚Üì‚Üì‚Üì
Ditto [30] 90.59 92.98 79.20 41.89 76.16 23.62
FedFV [49] 91.76 94.70 77.26 44.14 76.97 23.17
FedHEAL 90.27 95.69 79.94 46.45 78.09 22.08
Office-CaltechMethodsAmazon DSLR Caltech Webcam A VG‚Üë‚Üë‚ÜëSTD‚Üì‚Üì‚Üì
Ditto [30] 58.00 70.00 56.25 63.45 61.92 6.20
FedFV [49] 62.95 71.33 55.36 60.00 62.41 6.72
FedHEAL 67.90 66.00 59.28 66.21 64.85 3.80
Table 3. Comparison of Average Accuracy( A VG ) and Standard
Deviation( STD ) with Ditto [30] and FedFV [49]. Please refer to
Sec. 4.3 for detailed discussion.
4.2. Diagnostic Analysis
Hyper-parameter Study . We show the impact of the
hyper-parameters œÑ(Eq. (9)) and Œ≤(Eq. (12)) on the perfor-
mance in Tab. 4 and Fig. 6, in Digits, optimal performance
is achieved when Œ≤= 0.4andœÑ= 0.3. Similar experi-
ments on Office-Caltech yields Œ≤= 0.4andœÑ= 0.4as the
best settings. We use these hyper-parameters by default in
subsequent experiments.
12083
404550556065707580
0 50 100 150Digits
FedAvg
AFL
q-FFL
FedFV
Ditto
OursFigure 5. Comparison of convergence of average accuracy with
counterparts on Digits. Please see details in Sec. 4.3.
Œ≤ 0.0 0.2 0.4 0.6 0.8 1.0
A VG‚Üë‚Üë‚Üë 76.00 76.88 77.20 76.83 76.96 77.13
STD‚Üì‚Üì‚Üì 23.82 22.70 22.64 22.83 22.71 22.70
Table 4. Hyper-parameter study with different Œ≤(Eq. (12)) on
Digits datasets. See details in Sec. 4.2.
78.02
22.08
20222426
707274767880AccuracyAVG(‚Üë) AVG Baseline
STD(‚Üì) STD Baseline
0.1             0.2             0.3 0.4             0.5
(a) Digits
64.85
3.8
0481216
40455055606570
Standard Deviation
0.1             0.2             0.3             0.4 0.5 (b) Office-Caltech
Figure 6. Hyper-parameter study with variant œÑ(Eq. (5)) when
fixŒ≤= 0.4. Please see details in Sec. 4.2.
Ablation Study . To provide a comprehensive analysis of
the effectiveness of FPHL and FAEL, we carried out an ab-
lation study on both the Digits and Office-Caltech in Tab. 5.
They contribute positively to the performance enhancement
and their combination results in optimal performance.
Compatibility Study . To validate the compatibility of
FedHEAL, we compared the results of several widely-
adopted FL methods, FedAvg [36], FedProx [29], Scaffold
[22], MOON [26], FedDyn [1], FedProc [39], FedProto
[48] without and with FedHEAL. The results are shown in
Tab. 2. They reveal tangible benefits offered by our system,
i.e., FedProto [48] with FedHEAL achieves 5.60% reduc-
tion in STD and 15.12% increase in A VG on Office-Caltech.
We plot the differences in convergence between the bench-
mark without and with FedHEAL in Fig. 7, demonstrating
faster convergence and higher accuracy of FedHEAL.
4.3. Comparison to State-of-the-Arts
The Tab. 2 and Tab. 3 shows the accuracy and standard de-
viation at the end of communication with SOTAs that ad-
dress Performance Fairness in FL. The results depict that
our method outperforms counterparts in both standard devi-
ation and mean accuracy. This demonstrates that FedHEAL
404550556065707580
0 50 100 150Digits
FedAvg
FedAvg+HEAL
FedProx
FedProx+HEAL
FedDyn
FedDyn+HEAL
FedProc
FedProc+HEALFigure 7. Comparison of convergence of average accuracy with
and without the integration of FedHEAL, across selected FL meth-
ods. Please see details in Sec. 4.2.
DigitsFPHL FAELMNIST USPS SVHN SYN A VG‚Üë‚Üë‚ÜëSTD‚Üì‚Üì‚Üì
89.84 93.25 79.54 41.35 76.00 23.82
‚úì 92.19 95.32 76.32 44.37 77.05 23.32
‚úì 90.05 95.16 78.76 44.83 77.20 22.64
‚úì ‚úì 90.27 95.69 79.94 46.45 78.09 22.08
Office-CaltechFPHL FAELAmazon DSLR Caltech Webcam A VG‚Üë‚Üë‚ÜëSTD‚Üì‚Üì‚Üì
72.63 56.67 58.57 45.52 58.35 11.13
‚úì 68.42 66.00 57.95 66.55 64.73 4.64
‚úì 66.73 63.33 57.59 53.10 60.19 6.05
‚úì ‚úì 67.90 66.00 59.28 66.21 64.85 3.80
Table 5. Ablation study on Digits and Office-Caltech. Pleaese
refer to Sec. 4.2 for detailed discussion.
achieves better Performance Fairness and further improves
accuracy across multiple domains. We plot the average ac-
curacy at each epoch in Fig. 5, which illustrates the faster
convergence of FedHEAL.
5. Conclusion
In this paper, we address Performance Fairness in federated
learning with domain skew by tackling parameter update
conflicts and model aggregation bias. We discover a prop-
erty in federated learning which we term Parameter Update
Consistency . Leveraging this characteristic, we propose a
simple yet effective approach. By discarding unimportant
parameters, FedHEAL alleviates parameter update conflicts
for poor-performing clients. Moreover, considering domain
diversity, we reduce the variance of distances between the
global model and local models, addressing model aggrega-
tion bias. Extensive experiments demonstrate the effective-
ness and compatibility of FedHEAL. We believe that this
newly discovered property and our work will offer fresh re-
search directions and insights for the community.
Acknowledgement. This work is supported by Na-
tional Natural Science Foundation of China under Grant
(62361166629, 62176188, 62272354).
12084
References
[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew
Mattina, Paul Whatmough, and Venkatesh Saligrama. Fed-
erated learning based on dynamic regularization. In ICLR ,
2021. 3, 7, 8
[2] Richard Bellman. Dynamic Programming . Princeton Uni-
versity Press, Princeton, NJ, 1957. 4, 5
[3] Debora Caldarola, Massimiliano Mancini, Fabio Galasso,
Marco Ciccone, Emanuele Rodol `a, and Barbara Caputo.
Cluster-driven graph federated learning over multiple do-
mains. In CVPRW , pages 2749‚Äì2758, 2021. 3
[4] Huiqiang Chen, Tianqing Zhu, Tao Zhang, Wanlei Zhou, and
Philip S Yu. Privacy and fairness in federated learning: on
the perspective of trade-off. ACM Computing Surveys , 2023.
3
[5] Sen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei
Wang. Addressing algorithmic disparity and performance
inconsistency in federated learning. NeurIPS , 34:26091‚Äì
26102, 2021. 3
[6] Yahya H Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara,
and A Salman Avestimehr. Fairfed: Enabling group fairness
in federated learning. In AAAI , pages 7494‚Äì7502, 2023. 3
[7] Xiuwen Fang and Mang Ye. Robust federated learning with
noisy and heterogeneous clients. In CVPR , pages 10072‚Äì
10081, 2022. 3
[8] Xiuwen Fang, Mang Ye, and Xiyuan Yang. Robust hetero-
geneous federated learning under data corruption. In ICCV ,
pages 5020‚Äì5030, 2023. 3
[9] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman.
Geodesic flow kernel for unsupervised domain adaptation.
InCVPR , pages 2066‚Äì2073. IEEE, 2012. 2, 6, 7
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
pages 770‚Äì778, 2016. 6
[11] Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias
Leontiadis, Stylianos Venieris, and Nicholas Lane. Fjord:
Fair and accurate federated learning under heterogeneous tar-
gets with ordered dropout. NeurIPS , 34:12876‚Äì12889, 2021.
3
[12] Wenke Huang, Mang Ye, and Bo Du. Learn from others and
be yourself in heterogeneous federated learning. In CVPR ,
pages 10143‚Äì10153, 2022. 1, 3
[13] Wenke Huang, Mang Ye, Bo Du, and Xiang Gao. Few-
shot model agnostic federated learning. In ACM MM , pages
7309‚Äì7316, 2022. 3
[14] Wenke Huang, Guancheng Wan, Mang Ye, and Bo Du. Fed-
erated graph semantic and structural learning. In IJCAI ,
pages 139‚Äì143, 2023. 3
[15] Wenke Huang, Mang Ye, Zekun Shi, and Bo Du. Generaliz-
able heterogeneous federated cross-correlation and instance
similarity learning. IEEE PAMI , 2023. 3
[16] Wenke Huang, Mang Ye, Zekun Shi, He Li, and Bo Du. Re-
thinking federated learning with domain shift: A prototype
view. In CVPR , pages 16312‚Äì16322, 2023. 1, 3
[17] Wenke Huang, Mang Ye, Zekun Shi, Guancheng Wan, He
Li, Bo Du, and Qiang Yang. A federated learning for gen-eralization, robustness, fairness: A survey and benchmark.
arXiv , 2023. 2
[18] Jonathan J. Hull. A database for handwritten text recognition
research. IEEE PAMI , 16(5):550‚Äì554, 1994. 2, 6, 7
[19] Meirui Jiang, Holger R Roth, Wenqi Li, Dong Yang, Can
Zhao, Vishwesh Nath, Daguang Xu, Qi Dou, and Ziyue Xu.
Fair federated medical image segmentation via client contri-
bution estimation. In CVPR , pages 16302‚Äì16311, 2023. 3,
6
[20] Xuefeng Jiang, Sheng Sun, Yuwei Wang, and Min Liu. To-
wards federated learning against noisy labels via local self-
regularization. In CIKM , pages 862‚Äì873, 2022. 1
[21] Peter Kairouz, H Brendan McMahan, Brendan Avent,
Aur¬¥elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista
Bonawitz, Zachary Charles, Graham Cormode, Rachel Cum-
mings, et al. Advances and open problems in federated learn-
ing. Foundations and Trends¬Æ in Machine Learning , 14(1‚Äì
2):1‚Äì210, 2021. 3
[22] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,
Sashank Reddi, Sebastian Stich, and Ananda Theertha
Suresh. Scaffold: Stochastic controlled averaging for fed-
erated learning. In ICML , pages 5132‚Äì5143. PMLR, 2020.
1, 3, 7, 8
[23] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR , 2014. 4, 6
[24] Yann LeCun, John Denker, and Sara Solla. Optimal brain
damage. NeurIPS , 2, 1989. 2
[25] Yann LeCun, L ¬¥eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE , 86(11):2278‚Äì2324, 1998.
2, 6, 7
[26] Qinbin Li, Bingsheng He, and Dawn Song. Model-
contrastive federated learning. In CVPR , pages 10713‚Äì
10722, 2021. 3, 7, 8
[27] Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia
Smith. Fair resource allocation in federated learning. In
ICLR , 2019. 3, 6, 7
[28] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia
Smith. Federated learning: Challenges, methods, and future
directions. IEEE signal processing magazine , 37(3):50‚Äì60,
2020. 1
[29] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,
Ameet Talwalkar, and Virginia Smith. Federated optimiza-
tion in heterogeneous networks. MLSys , 2:429‚Äì450, 2020.
1, 2, 3, 7, 8
[30] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia
Smith. Ditto: Fair and robust federated learning through per-
sonalization. In ICML , pages 6357‚Äì6368. PMLR, 2021. 3,
7
[31] Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp,
and Qi Dou. Fedbn: Federated learning on non-iid features
via local batch normalization. In ICLR , 2021. 1, 3
[32] Zexi Li, Xinyi Shang, Rui He, Tao Lin, and Chao Wu. No
fear of classifier biases: Neural collapse inspired federated
learning with synthetic and fixed classifier. In ICCV , pages
5319‚Äì5329, 2023. 1
12085
[33] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann
Heng. Feddg: Federated domain generalization on medical
image segmentation via episodic learning in continuous fre-
quency space. In CVPR , pages 1013‚Äì1023, 2021. 3
[34] Kangyang Luo, Xiang Li, Yunshi Lan, and Ming Gao.
Gradma: A gradient-memory-based accelerated federated
learning with alleviated catastrophic forgetting. In CVPR ,
pages 3708‚Äì3717, 2023. 3
[35] Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu. Collab-
orative fairness in federated learning. Federated Learning:
Privacy and Incentive , pages 189‚Äì204, 2020. 3
[36] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data.
InAISTATS , pages 1273‚Äì1282, 2017. 1, 2, 3, 7, 8
[37] Jiaxu Miao, Zongxin Yang, Leilei Fan, and Yi Yang. Fed-
seg: Class-heterogeneous federated learning for semantic
segmentation. In CVPR , pages 8042‚Äì8052, 2023. 3
[38] Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.
Agnostic federated learning. In ICLR , pages 4615‚Äì4625.
PMLR, 2019. 3, 6, 7
[39] Xutong Mu, Yulong Shen, Ke Cheng, Xueli Geng, Jiaxuan
Fu, Tao Zhang, and Zhiwei Zhang. Fedproc: Prototypical
contrastive federated learning on non-iid data. arXiv preprint
arXiv:2109.12273 , 2021. 7, 8
[40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural
images with unsupervised feature learning. 2011. 2, 6, 7
[41] Tao Qi, Fangzhao Wu, Chuhan Wu, Lingjuan Lyu, Tong Xu,
Hao Liao, Zhongliang Yang, Yongfeng Huang, and Xing
Xie. Fairvfl: A fair vertical federated learning framework
with contrastive adversarial learning. NeurIPS , 35:7852‚Äì
7865, 2022. 3
[42] Bhaskar Ray Chaudhury, Linyi Li, Mintong Kang, Bo Li,
and Ruta Mehta. Fairness in federated learning via core-
stability. NeurIPS , 35:5738‚Äì5750, 2022. 3
[43] Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, and
Umapada Pal. Effects of degradations on deep neural net-
work architectures. arXiv preprint arXiv:1807.10108 , 2018.
2, 6, 7
[44] Jiangming Shi, Shanshan Zheng, Xiangbo Yin, Yang Lu,
Yuan Xie, and Yanyun Qu. Clip-guided federated learn-
ing on heterogeneous and long-tailed data. arXiv preprint
arXiv:2312.08648 , 2023. 3
[45] Yuxin Shi, Han Yu, and Cyril Leung. Towards fairness-aware
federated learning. IEEE TNNLS , 2023. 2, 3
[46] Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel,
Daniel Benditkis, Liron Mor-Yosef, and Itai Zeitak. Over-
coming forgetting in federated learning on non-iid data.
arXiv preprint arXiv:1910.07796 , 2019. 2
[47] Ilya Sutskever, James Martens, George Dahl, and Geoffrey
Hinton. On the importance of initialization and momentum
in deep learning. In ICML , pages 1139‚Äì1147. PMLR, 2013.
4, 6
[48] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu,
Jing Jiang, and Chengqi Zhang. Fedproto: Federated proto-
type learning across heterogeneous clients. In AAAI , pages
8432‚Äì8440, 2022. 3, 7, 8[49] Zheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen,
Cheng Wang, and Rongshan Yu. Federated learning with
fair averaging. IJCAI , 2021. 3, 6, 7
[50] Nannan Wu, Li Yu, Xin Yang, Kwang-Ting Cheng, and
Zengqiang Yan. Fediic: Towards robust federated learning
for class-imbalanced medical image classification. In MIC-
CAI, pages 692‚Äì702. Springer, 2023. 1
[51] An Xu, Wenqi Li, Pengfei Guo, Dong Yang, Holger R Roth,
Ali Hatamizadeh, Can Zhao, Daguang Xu, Heng Huang, and
Ziyue Xu. Closing the generalization gap of cross-silo fed-
erated medical image segmentation. In CVPR , pages 20866‚Äì
20875, 2022. 3
[52] Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao,
Chuan Sheng Foo, and Bryan Kian Hsiang Low. Gradient
driven rewards to guarantee fairness in collaborative machine
learning. NeurIPS , 34:16104‚Äì16117, 2021. 3
[53] Xiyuan Yang, Wenke Huang, and Mang Ye. Dynamic per-
sonalized federated learning with adaptive differential pri-
vacy. In NeurIPS , 2023. 2
[54] Xiyuan Yang, Wenke Huang, and Mang Ye. Fedas: Bridging
inconsistency in personalized federated learning. In CVPR ,
2024. 1
[55] Mang Ye, Xiuwen Fang, Bo Du, Pong C Yuen, and Dacheng
Tao. Heterogeneous federated learning: State-of-the-art and
research challenges. arXiv preprint arXiv:2307.10616 , 2023.
1
[56] Mang Ye, Wenke Huang, Zekun Shi, He Li, and Du Bo.
Revisiting federated learning with label skew: An over-
confidence perspective. SCIS , 2024. 1
[57] Yuchen Zeng, Hongxu Chen, and Kangwook Lee. Im-
proving fairness via federated learning. arXiv preprint
arXiv:2110.15545 , 2021. 3
[58] Ruipeng Zhang, Qinwei Xu, Jiangchao Yao, Ya Zhang, Qi
Tian, and Yanfeng Wang. Federated domain generalization
with generalization adjustment. In CVPR , pages 3954‚Äì3963,
2023. 3
[59] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon
Civin, and Vikas Chandra. Federated learning with non-iid
data. arXiv preprint arXiv:1806.00582 , 2018. 1
[60] Zirui Zhou, Lingyang Chu, Changxin Liu, Lanjun Wang,
Jian Pei, and Yong Zhang. Towards fair federated learning.
InACM SIGKDD , pages 4100‚Äì4101, 2021. 3
12086
