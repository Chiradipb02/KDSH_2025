Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs
Kanchana Ranasinghe1,2Satya Narayan Shukla1Omid Poursaeed1
Michael S. Ryoo2Tsung-Yu Lin1
1Meta2Stony Brook University
{kranasinghe, mryoo }@cs.stonybrook.edu, {satyanshukla, opoursaeed, tsungyulin }@meta.com
Abstract
Integration of Large Language Models (LLMs) into vi-
sual domain tasks, resulting in visual-LLMs (V-LLMs), has
enabled exceptional performance in vision-language tasks,
particularly for visual question answering (VQA). However,
existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak
spatial reasoning and localization awareness. Despite gen-
erating highly descriptive and elaborate textual answers,
these models fail at simple tasks like distinguishing a left vs
right location. In this work, we explore how image-space
coordinate based instruction ﬁne-tuning objectives could
inject spatial awareness into V-LLMs. We discover op-
timal coordinate representations, data-efﬁcient instruction
ﬁne-tuning objectives, and pseudo-data generation strate-
gies that lead to improved spatial awareness in V-LLMs.
Additionally, our resulting model improves VQA across im-
age and video domains, reduces undesired hallucination,
and generates better contextual object descriptions. Exper-
iments across 5 vision-language tasks involving 14 differ-
ent datasets establish the clear performance improvements
achieved by our proposed framework.
1. Introduction
Holistic visual understanding requires learning beyond
simply content of an image to encompass awareness on
spatial locations of objects and their relations [ 41]. In the
context of visual question answering (VQA), such spatial
awareness allows better reasoning involving structural and
contextual information contained within an image [ 8].
Since the introduction of powerful large-language mod-
els (LLMs) such as GPT-3 [ 5], Chat-GPT [ 44], Vicuna [ 11],
and LLaMA [ 55,56] that are capable of human style con-
versation, their visual counterparts such as BLIP-2 [ 33],
LLaV A [ 38] have enabled novel tasks within the vision
modality. However, despite their [ 33,38] highly generic
visual understanding, these models exhibit poor language-
based spatial reasoning [ 8]. In fact, they fail at simple tasks
such as distinguishing whether an object lies to the left or
right of another object (see Tab. 4).
Query: Describe [x1,y1,x2,y2] location in image. 
Ours: A blue plaid blanket behind a teddy bear.Query: Which side of the potted plant is the stove? 
LLaVa: The stove is on the left side of potted plant.
Ours: The stove is on the right side of potted plant.
Figure 1. We illustrate one unique ability of our model: contextual
region description (top). Note the contextual information used in
describing the selected region in each image. Explicitly teaching
localization to Visual-LLMs also improves spatial awareness in
VQA settings (bottom). Color boxes only for illustration purposes.
In the case of contrastive language image models (such
as CLIP [ 46], ALIGN [ 26]), recent works explore how in-
jecting explicit spatial awareness [ 39,43,48,74] can enable
more holistic visual understanding. In fact, [ 48] shows how
such improved spatial awareness beneﬁts model robustness
in adversarial domains. This raises the question of how
generative language image models, particularly those con-
necting LLMs to visual encoders [ 33,38] can beneﬁt from
such spatial awareness speciﬁc training. We refer to mod-
els of this category that generate textual outputs given joint
image-text inputs (e.g. [ 33,38]) as visual-LLMs (V-LLMs).
In this work, we explore location speciﬁc instruction
ﬁne-tuning objectives that explicitly enforce V-LLMs to
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12977
meaningfully process and generate textual image-space co-
ordinates. We hypothesize that such training would lead to
improved spatial awareness in these V-LLMs, therein im-
proving performance on VQA tasks. To this end, we pro-
pose three instruction ﬁne-tuning objectives that unify loca-
tion representation with natural language. We also explore
optimal representation forms for image-space locations and
how pseudo-data generation can be leveraged for efﬁcient
scaling of our framework. We name our resulting model as
LocVLM.
While the idea of adapting V-LLMs to perform local-
ization related tasks (e.g. detection, segmentation) us-
ing V-LLMs has been explored in multiple recent works
[30,45,58,66,68,73,75], these approaches depend on
task speciﬁc architectural modiﬁcations or treat localization
inputs / outputs differently from natural language. In con-
trast, our LocVLM focuses on a uniﬁed framework treating
location and language as a single modality of inputs with
the goal of complementing performance in each task. We
intuit that processing location represented in textual form
would enforce the LLM to select appropriate image regions
as opposed to relying on region level features provided by
the architecture. At the same time, textual form location
outputs promote spatial awareness at language level in a
human interpretable manner, in contrast to using secondary
heads or specialized tokens for location prediction. Concur-
rent work in [ 8] also explores textual location representa-
tion with a generic V-LLM architecture similar to our work.
Our proposed LocVLM differs with focus on optimal lo-
cation representation forms, data-efﬁcient pseudo-labelling,
and video domain operation.
Our proposed framework exhibits improved spatial
awareness in VQA style conversation demonstrated through
experimentation on 14 datasets across 5 vision-language
tasks: Spatial Reasoning, Image VQA, Video VQA, Ob-
ject Hallucination, and Region Description. We summarize
our key contributions as follows:
•Inject textual spatial coordinate awareness into V-LLMs
•Propose three novel localization based instruction ﬁne-
tuning objectives for V-LLMs
•Discover optimal coordinate representation forms
•Pseudo-Data generation for improved region description
and scaling to video domain
2. Related Work
Localization in Contrastive Vision Language Models :
Foundation vision language models (VLMs) such as CLIP
[46] resulted in extensive exploration into language-tied lo-
calization in images both under dense (pixel / bounding-
box) supervision [ 16–18,21,27,31,32,35,69,70] and
weak supervision [ 14,39,43,48,60,61,65,74,76]. Re-
covering explicit localization information within model rep-Method Kosmos [ 45]Ferret [ 66]Shikra [ 8]Ours
Uniﬁed Arch. 7 7 3 3
Purely Textual 7 7 3 3
Pseudo Data 7 7 7 3
Video Domain 7 7 7 3
Table 1. Related Work Comparison: A uniﬁed architecture, purely
textual inputs, pseudo data for scalable learning, and video domain
operation distinguishes our work from these prior methods.
resentations has enabled more robust operation for certain
tasks [ 48]. While our work differs from this contrastive set-
ting given our use of LLM based generative predictions, we
similarly explore how explicit location information within
the language modality can improve V-LLMs.
Visual Large Language Models (V-LLMs) : The advent
of powerful large language models (LLMs) such as GPT-
3[5], Chat-GPT [ 44], and PaLM [ 13], as well as their
open-source counterparts BLOOM [ 52], Vicuna [ 11], and
LLaMA [ 55,56], has resulted in direct use of these LLMs
for computer vision tasks [ 22,53]. Alternate lines of work
explore how LLMs can be connected to existing visual
foundation models [ 2,3,33,38,42,47], in particular to
CLIP visual backbones [ 46]. While earlier models ex-
plored large-scale (millions to billions of samples) image-
text training [ 2,3], later models [ 33,38,42] scale down on
data dependency. LLaV A [ 38] in particular scales down on
pre-training data to under 1 million image-text pairs, and
use instruction ﬁne-tuning [ 59] to enable human-style con-
versation with visual awareness. This is extended to video
domain in [ 42,49]. A shortcoming of these models is their
lack of spatial awareness or location understanding in image
space [ 8,12,20]. Spatial reasoning limitations in generative
VLMs are studied in [ 12,20]. Similar failures in caption-
ing (and VQA) models are explored in [ 28]. A solution in
[24] proposes code-generation based reasoning. Our work
tackles these same limitations but follows an alternate direc-
tion of spatial-aware instruction ﬁne-tuning. Another line
of recent works [ 30,45,58,66,68,73,75] tackle this by
introducing architectural modiﬁcations to explicitly extract
region level features that are injected to the LLM as special
tokens. While introducing extra tokens and layers, this also
separates the localization task from language. In contrast,
we use a generic architectures with purely textual location
information (i.e. image space coordinates as text). Concur-
rent work in [ 8] explores this same idea, but we differ in
3 ways with, a) focus on optimal coordinate representation
forms, b) data-efﬁcient pseudo-labelling strategies, and c)
video domain operation (see also Tab. 1).
Location Representations : Selecting regions within an im-
age has a rich history in computer vision [ 40,57] with
greater focus on location outputs since the popularity of ob-
ject detection [ 6,9,10,19,50,54,58]. Early anchor-based
12978
methods regress locations from anchor centers [ 19,50],
followed by direct location regression from object-level
features [ 6,54]. Recent works explore generative loca-
tion predictions with diffusion processes [ 9] and sequence-
generation [ 10,58]. Ours resembles the latter given our use
of an LLM, next token prediction objective, and sequen-
tial generation of textual location representations. However,
[10,58] utilize 1000 specialized location tokens (introduced
to the LLM vocabulary) corresponding to 1000 bins uni-
formly spread across image space. While we explore simi-
lar binning strategies, in contrast we introduce no additional
tokens, focus on purely textual representation of locations,
and explore multiple textual location representation forms.
3. Method
Current V-LLMs [ 33,38] exhibit weak understanding
of spatial locations within images [ 8]. We explore and
benchmark such shortcomings, and propose three novel in-
struction ﬁne-tuning objectives aimed at overcoming these
drawbacks of existing V-LLMs. We build these objectives
based on spatial-coordinate based prompting and demon-
strate how LLMs can directly both process and generate
meaningful numerical coordinates in image-space after suit-
able training. In the rest of this section we describe our ar-
chitecture and training framework, followed by coordinate
processing & generation, instruction ﬁne-tuning objectives,
pseudo-data generation, and video domain operation.
3.1. Architecture and Training
The focus of our work is to explore how spatial localiza-
tion related training can improve a generic V-LLM such as
LLaV A [ 38]. Therein, our architecture and training frame-
work is inspired from [ 38]. We use a visual encoder, adapter
layer, and LLM stacked sequentially (illustrated in Fig. 2),
and follow a multi-stage training strategy similar to [ 38].
Consider an image X2RH,W,Cwhere H,W,C (=
3) denote height, width, channels of image and a textual
prompt Tcomposed of natural language (asking a question
about the image). We deﬁne two variants of our model,
LocVLM-B and LocVLM-L for better comparison with
prior work. We ﬁrst describe LocVLM-B that processes
images with H=W= 224 . Our visual encoder, ViT-
L/14 from CLIP [ 46], processes the image Xto produce
a set of 256visual tokens in R1024, which are in turn pro-
jected to R4096by an adapter layer (implemented as a lin-
ear layer). The LLaMA [ 55] text tokenizer processes the
textual prompt Tto produce textual tokens in R4096. The
joint set of visual and textual tokens (of dimension R4096)
are processed by a LLaMA [ 55] LLM to produce the ﬁnal
set of textual tokens which are in turn untokenized to con-
vert to natural language. The ﬁnal natural language output
is expected to be a suitable response to the input textual
prompt, T. In variant LocVLM-L, we use images sized
Vision Encoder (CLIP: ViT-L/14)
Adapter Layer (Bridge Modality Gap)
LLaMa LLM
Caption: Where is the dog located in this image?
all tokens
Tokenizer
Target: The dog is located at (x1, y1, x2, y2) bbox.
Figure 2. Architecture: We present the overall model architecture
of our framework which is inspired from LLaVa [ 38].
H=W= 336 resulting in 576 visual token, an adapter
layer implemented as an MLP, and the LLM from LLaMA-
2[56]. All other design choices remain unchanged.
We also highlight the BPE tokenization that is employed
in our setup. This learned tokenization scheme may split
a single word into sub-parts (that alone can appear mean-
ingless to humans) and handles numerical text (including
decimal point) as individual tokens (e.g. 12.34would be
split into 5 separate tokens).
In terms of training, we follow a two-stage strategy.
Inspired by LLaV A [ 38], we adopt an initial pre-training
stage that only updates weights of the intermediate adapter
layer to align the visual encoder outputs with LLM inputs.
Next, we jointly instruction ﬁne-tune the adapter layer and
LLaMA LLM with our proposed objectives and template-
based localization datasets (see Sec. 4.1). Video domain
operation introduces an additional phase (see Sec. 3.5).
3.2. Coordinate Processing and Generation
Humans contain the ability to reason about images us-
ing image-space coordinates. This is in contrast to exist-
ing V-LLMs that can describe the contents of an image ele-
gantly, but lack spatial awareness regarding image contents.
We hypothesize that injecting LLMs with additional spatial
awareness, through coordinate based reasoning could im-
prove their generic reasoning ability as well. To this end, we
introduce our ﬁrst goal of directly using textual coordinate
based image locations in both natural language prompts and
LLM generated outputs. For textual coordinates, we ex-
plore three different representation forms:
1.Normalized Floating Point Values
2.Integer Valued Binning (across image dimensions)
3.Deviation from Image-Grid based Anchors
12979
CRGQA (Acc)RD (METEOR)A-QA (Acc)NFP46.119.637.1IVB47.320.737.4DIGA47.020.837.3Table 2. Ablation on Coordinate Representation (CR) methods:
we compare each of the three proposed CR variants, namely nor-
malized ﬂoating point values (NFP), integer valued binning (IVB),
and deviation from image-grid based anchors (DIGA).
For image locations, we explore point based (e.g. center co-
ordinates [cx, cy] of object) and bounding box based (e.g.
top-left and bottom-right extreme coordinates of object re-
gion [x1, y1, x2, y2]) forms. We next discuss the three rep-
resentations for coordinates used for either location.
Normalized Floating Point Values calculates absolute im-
age coordinates and normalizes with image dimensions to
a (0, 1) range. We use a 4 decimal point representation for
these ﬂoating point values. While this representation is sim-
ple and generic, given the nature of BPE tokenization, each
individual coordinate will be represented by up to 6 tokens.
Integer Valued Binning discretizes the absolute image co-
ordinates to one of nb(=224, 336 for variant B & L respec-
tively) bins spread uniformly across the two image dimen-
sions. Based on the binning parameter, nb, each coordinate
will be represented some number of tokens, in our case up
to 3 (less than the ﬂoating point variant).
Deviation from Image-Grid based Anchors is motivated
from prior object detection works that estimate an initial an-
chor followed by deviation from that anchor center to esti-
mate bounding box coordinates. We follow a similar setup,
where one of naanchors is predicted by the model, fol-
lowed by deviation of coordinate from that anchor center.
Our intuition is that, given the sequential next-token predic-
tion setup of LLMs, such a two-stage strategy would lead to
faster learning and more accurate coordinates.
We refer to Appendix Afor further details on each vari-
ant. In Tab. 2, we ablate each representation format on three
different tasks (see Sec. 4.7for more details) of image VQA
(GQA), region description (RD), and video VQA (A-QA).
Our experiments indicate optimal performance for integer
valued binning (IVB). In all following experimentation, we
ﬁx our coordinate representation to IVB.
3.3. Instruction Fine-Tuning Objectives
Given suitable coordinate representations, we now have
a mechanism to directly prompt LLMs with image locations
in textual form. Our second goal is to build training objec-
tives using these image coordinates that directly inject spa-
tial awareness into V-LLMs. We propose three instruction
ﬁne-tuning objectives for this purpose.
Let us ﬁrst revisit the visual instruction ﬁne-tuning
methodology in [ 38]. Building off the COCO dataset,Objective Prompt Target
LocPred Where is obj1? It’s at (x1,y1,x2,y2).
NegPred Where is obj2? There’s no obj2.
RevLoc Describe (cx,cy) Detailed description
Table 3. We summarize our three distinct instruction ﬁne-tuning
objectives. Refer to appendix (Appendix B) for exact natural lan-
guage prompts and targets used for training. For illustration, we
use both point and bounding-box based image locations here.
they construct a VQA dataset containing conversation
style question-answer pairs relevant to each COCO im-
age. Question-answer pairs are generated using an LLM
that is fed with the ground-truth bounding-box annotations
for each image. Inspired by this setup, we build a similar
spatial-VQA dataset using images and annotations of the
COCO dataset, but instead of LLM prompting, we utilize
hand-crafted templates and pseudo-captions (discussed in
Sec. 3.4) to generate conversations.
We propose three types of question-answer pairs that re-
late to our three instruction ﬁne-tuning objectives: Location
Prediction (LocPred), Negative Prediction (NegPred), and
Reverse-Location Prediction (RevLoc). See Tab. 3for ex-
amples. Considering the LLM based ﬁnal text generation
in our architecture, we utilize next-token prediction loss to
achieve each objective during our training.
Location Prediction : Given an object category, we query
the model to generate a point or bounding box localizing
that object in the image. The object category and bounding
box are derived from the COCO train set annotations. To
avoid object mismatches (i.e. multiple object of same cate-
gory), we ﬁrst ﬁlter images containing only a single object
of a given class.
Negative Prediction : Using the same prompt templates as
inLocPred above, we query the model to generate a point
or bounding box localizing a speciﬁed object in the im-
age. However, in this case we select an object category not
present in the image and accordingly provide a target text
of “no such object in image”. For each image, we utilize
COCO bounding-box annotations to discover objects (be-
longing to COCO classes) that are not present in that image.
Reverse-Location Prediction : We perform the reverse of
LocPred here. Given a point or bounding box in image
space, we query the model to describe the object in that
location. The bounding box and object category are derived
from the COCO train set annotations.
While introducing three novel train objectives aimed at
injecting location information, we highlight that our pro-
posed framework relies on training data (i.e. human an-
notations) identical to those used by LLaV A [ 38]. We do
not use any additional ground-truth annotations for train-
ing. Next we explore how we could augment the generality
of our framework while limiting to this same annotated data.
12980
3.4. Pseudo-Data Generation
We introduced three train objectives, each utilizing tem-
plate based conversations as prompts and targets. However,
our reliance on categories of COCO dataset limits the ob-
ject vocabulary seen during training. Therein, we propose a
pre-trained V-LLM based pseudo-data generation strategy.
In fact, we utilize our model after stage one training as the
V-LLM leading to a form of self-training based learning.
Given the abundance of only image-level annotated datasets
(i.e. no bounding box ground-truth), we also explore how
an object-detector generated pseudo-bounding boxes could
augment our framework.
Self-Training : Given an image and bounding box annota-
tions from the COCO train set, we prompt the V-LLM to
caption each distinct object in the image. In order to pre-
vent ambiguous object queries, we ﬁlter images to select
only those containing at most one instance of a single cat-
egory. We additionally prompt the V-LLM to describe the
object using relational information (i.e. relative to other ob-
jects in image). This process provides us a dataset with ob-
ject level bounding boxes and descriptive captions that are
not limited to the COCO object categories (dataset details in
Appendix C). In turn, we use this data generated by the V-
LLM (our stage one model) to further improve performance
of our framework. We modify each of our three train objec-
tives (in Sec. 3.3) to utilize these image-speciﬁc pseudo-
captions instead of the generic dataset level category labels.
Weak Supervision : We explore how datasets containing
no object level annotation (e.g. video classiﬁcation / VQA
datasets) could be leveraged to adapt our framework into
domains beyond images. Therein, we utilize an off-the-
shelf panoptic segmentation framework from SEEM [ 77]
to generate pseudo-bounding boxes for selected object cat-
egories within any image as well as exhaustive pixel level
labels (enabling negative class identiﬁcation). We leverage
this setup to extend our introduced train objectives to the
video domain as well.
3.5. Video Domain Operation
Inspired by the simple modiﬁcations to LLaV A [ 38] in
[42] enabling video domain operation, we follow a similar
strategy of modifying our LocVLM-B architecture to pro-
cess videos while introducing no additional components.
The visual backbone process multiple video frames individ-
ually (as images) and resulting tokens are averaged across
spatial ( S) and temporal ( T) axes to obtain S+Ttokens.
These are processed by the adapter layer and LLM to gen-
erate the textual outputs. Further details on our video archi-
tecture are discussed in Appendix D.
In addition to our two training phases discussed in
Sec. 3.1, we introduce a third video instruction ﬁne-tuning
stage using a dataset we derive from ActivityNet [ 23]. Fol-lowing [ 42], only the adapter layer is ﬁne-tuned leaving all
other parameters frozen. This resulting model is referred to
as LocVLM-Vid-B.
We next introduce video variants of our three instruction
ﬁne-tuning objectives focused on static objects in videos.
We utilize our proposed pseudo-labeling strategy to gener-
ate necessary video annotations and train both the adapter
layer and LLM to obtain a second video model tagged
LocVLM-Vid-B+. Futher details on our video ﬁne-tuning
objectives are presented in Appendix D.
4. Experiments
In this section, we present experimental results to high-
light existing weaknesses of SOTA V-LLMs and how our
proposed framework addresses these issues. We also evalu-
ate on standard VQA benchmarks across domains to show-
case the better reasoning abilities of our model and highlight
novel abilities of our framework.
4.1. Experimental Setup
Datasets : We utilize the COCO dataset [ 37] and our
model (post stage one training) to construct a localization
related VQA dataset as outlined in Sections 3.3and3.4.
We name this dataset Localize-Instruct-200K . In detail, this
contains LocPred and RevLoc question-answer pairs that
use pseudo-captions instead of COCO categories as well
as NegPred. We deﬁne a second video dataset, Localize-
ActivityNet , containing question-answer pairs constructed
from Activity-Net pseudo-bounding boxes following Sec-
tion3.5. Our models are primarily trained on our Localize-
Instruct-200K dataset. Our stage one training uses CC3M
dataset [ 7]. Additionally, our Localize-ActivityNet dataset
and ActivityNet dataset [ 23] are used for video domain
training.
Training : We train our models on 8xA100 GPUs (each
80GB) following a two-phase training schedule. Our ﬁrst
phase trains on CC3M [ 7] following the setup in [ 38]. The
second phase uses our Localize-Instruct-200K dataset and
trains for 10 epochs with a batch size of 64, ADAM-W op-
timizer with initial learning rate 2e 5, 0.3 warm-up ratio,
and cosine-decay learning rate schedule. Both the training
phases we conduct use standard next-token-prediction loss
used in LLM training.
Evaluation : During evaluation, following standard proto-
col [38], we iteratively generate next tokens, given visual
and textual inputs. The LLM output is a distribution across
the entire token vocabulary. The next token is selected
through multinomial sampling of this output using a soft-
max temperature term of 0.2 during normalization.
12981
4.2. Spatial Reasoning: A Toy Experiment
We investigate spatial reasoning abilities of two SOTA
V-LLMs, LLaV A [ 38] and BLIP-2 [ 33], using a simple toy
experiment. We create an evaluation dataset from COCO
annotations containing images with distinct category object
triplets (only one instance occurrence of each object cat-
egory), where each object is entirely to the left or right
half of the image and two objects are on opposite sides.
The ground-truth bounding box annotation are utilized to
automate this dataset creation procedure. This evaluation
set, referred as COCO-Spatial-27K , contains 26,716 image-
question pairs (see Appendix Cfor details). We introduce
two evaluation settings, direct VQA and in-context learn-
ing (ICL) VQA to understand spatial reasoning abilities of
these models. In direct VQA, given an image we query the
model whether an object lies above or below another object.
In ICL VQA, before a similar ﬁnal query, we provide two
example question-answer pairs (involving the other two ob-
jects in the image) in the same format as our query. Refer
to Appendix Efor further details on task. We perform the
same for objects in top vs bottom halves of images.
These results are presented in Table 4. Our results in-
dicate near random performance for existing V-LLMs. For
the case of LLaV A, we perform keyword ( leftandright ) fre-
quency analysis on its instruction tuning dataset (LLaV A-
Instruct-80K dataset) to verify the presence of terms left
andright in its training corpus. These keywords are present
in 0.37% and 1.13% of its conversations respectively (see
Appendix Ffor more) indicating presence of these con-
cepts in the image-text training corpus. In contrast to these
methods, our proposed framework notably improves perfor-
mance over both BLIP-2 [ 33] and the LLaV A baseline [ 38].
4.3. Image VQA
Image VQA involves correctly answering natural lan-
guage questions regarding content within an image. We
evaluate our model for Image VQA on two standard
datasets, GQA and VQAv2. The GQA dataset focuses on
questions requiring compositional reasoning, particularly
involving surrounding information of objects within an im-
age. We evaluate on its test-dev split containing 12,578
image-question pairs. The VQAv2 dataset contains open-
ended questions about each image that require an under-
standing of vision, language and commonsense knowledge
to answer. We use its validation split containing 214,354
image-question pairs for our evaluation. For each dataset,
we follow standard V-LLM evaluation protocol following
[33,42] and report top-1 accuracy metric. Our results in
Tab. 5indicate clear improvements for LocVLM over our
baseline and prior work, establishing the usefulness of our
proposed framework. The closest to our work, Shikra [ 8]
achieves performance competitive to our LocVLM-B, butMethod ICL All Left Right All Above Below
BLIP-2 [ 33] 7 45.5 86.1 4.74 49.2 50.4 48.6
LLava [ 38] 7 55.1 84.5 36.5 58.9 57.8 59.3
Ours 7 69.5 79.7 59.2 65.4 64.2 65.9
BLIP-2 [ 33] 3 14.7 17.8 11.6 15.8 16.5 15.2
LLaVa [ 38] 3 55.1 84.7 36.4 58.2 57.7 58.5
Ours 3 76.5 90.4 61.5 74.1 73.5 74.4
Table 4. Spatial Reasoning : We report accuracy (%) on a spatial
localization dataset derived from COCO annotations to highlight
weak spatial awareness of existing V-LLMs. We query these mod-
els to answer whether one object is to the left or right / above or
below of another object. The SOTA V-LLMs evaluated exhibit
close to random performance. Our proposed setup outperforms
existing methods. Ours refers to LocVLM-B variant.
Method LLM VS Zero-Shot GQA VQA-V VQA-T
SR [4] - - 7 62.1 72.9 -
Shikra [ 8] 7B 224 7 - 75.3 77.4
LLaV A-v1.5 7B 336 7 62.0 78.1 78.4
LocVLM-L 7B 336 7 63.5 78.2 78.6
LLaV A-v1 7B 224 3 44.7 49.8 49.3
LocVLM-B 7B 224 3 47.3 50.3 50.8
Viper-GPT 175B - 3 48.1 - -
BLIP-2 11B - 3 44.7 54.3 53.9
LLaV A-v1.5 7B 336 3 48.7 55.7 55.3
LocVLM-L 7B 336 3 50.2 55.9 56.2
Table 5. Image VQA Results : We report accuracy (%) on the
test-dev split of GQA dataset (GQA) and the validation / test splits
of VQAv2 dataset (VQA-V / VQA-T). Our proposed LocVLM
improves over prior works achieving state-of-the-art performance.
unlike ours they use VQA datasets (containing similar do-
main question-answer pairs) during training.
4.4. Video VQA
Our model is also applicable to video tasks following
our video domain adaptation described in Sec. 3.5.W e
simply adopt the additional video instruction ﬁne-tuning
phase from [ 42] on the ActivityNet dataset after our ini-
tial two phases of training to obtain LocVLM-Vid-B. This
third phase involves ﬁne-tuning only the adapter layer of our
model. We also explore video variants of our IFT objectives
that train both adapter layer and LLM. The resulting model
is termed LocVLM-Vid-B+.
Video VQA focuses on correctly answering questions re-
garding a given video that require spatio-temporal aware-
ness to answer. We evaluate our video-adapted model
on the task of zero-shot video VQA on four benchmark
datasets, ActivityNet-QA, MSRVTT-QA, MSVD-QA, and
TGIF-QA. We evaluate on the validation splits of these
four datasets. ActivityNet-QA videos cover a wide range
of complex human activities relevant to daily living with
its question-answer pairs focusing on long-term spatio-
temporal reasoning. MSRVTT-QA builds off the MSRVTT
dataset that contains web videos covering a comprehensive
12982
MethodZero-ShotActivityNet-QAMSRVTT-QAMSVD-QATGIF-QAJustAsk [63]738.941.847.5-FrozenBiLM [64]743.247.054.8-VideoCoCa [62]756.146.356.9-Flamingo [2]3-17.435.6-BLIP-2 [33]3-17.434.4-InstructBLIP [15]3-25.644.3-FrozenBiLM [64]324.716.832.241.0Video Chat [34]326.545.056.334.4LLaMA Adapter [72]334.243.854.9-Video LLaMA [71]312.429.651.6-Video-ChatGPT [42]335.249.364.951.4LocVLM-Vid-B337.451.266.151.8Table 6. Video VQA Results : Our proposed LocVLM-Vid-B improves over Video-ChatGPT [ 42] and achieves state-of-the-art results
(Top-1 Accuracy %) across four different video VQA benchmarks. Note the zero-shot setting of all these evaluations.
range of categories and diverse visual content. MSVD-QA
is a similar dataset building off the MSVD dataset. TGIF-
QA contains question-answer pairs from an dataset con-
structed of animated GIFs. For each dataset, we report the
accuracy metric following evaluation protocol in [ 42]. Our
results on these four datasets reported in Tab. 6demonstrate
state-of-the-art performance of our proposed LocVLM-Vid-
B, with consistent improvements over the baseline from
[42]. Here we use the LocVLM-Vid-B variant for fairer
comparison with the baseline from [ 42]. We attribute the
performance gains exhibited by our model to its stronger
spatial awareness (see Sec. 4.2). Particularly in the case of
video understanding, awareness of content at spatial level of
each frame is signiﬁcant to understand object motions and
interactions [ 1,51]. We also report more results involving
additional model variants in Tab. 7.
4.5. Object Hallucination
Current state-of-the-art V-LLMs suffer from object hal-
lucination, generating image descriptions inconsistent with
the image content [ 36]. For example, a V-LLM would re-
spond to “Where is the cat in this image?” with “The cat is
on the table” when in reality there is no cat in the image. We
evaluate the extent of hallucination in V-LLMs using three
datasets we introduce (details in Appendix C) and the POPE
dataset [ 36]. Our three datasets, Hal-COCO, Hal-ADE,
and Hal-Act build off COCO, ADE-20K, and ActivityNet
datasets respectively. The ﬁrst two involve images and the
latter videeos. These datasets contain ‘Is there objin image
/ video?” type questions per sample, for two objects present
and not present in the image / video. Hal-ADE object cat-
egories contain no overlap with COCO classes allowing
evaluation on novel object categories unseen during our in-
struction ﬁne-tuning. Results reported in Tab. 8show clear
improvements of LocVLM-B over baselines. We also eval-
uate LocVLM-B on the POPE benchmark [ 36] that buildsMethod VLT Frames Acc (%)
LLaVa (v1) [ 38] 7 1 28.7
LLaVa (v1.5) [ 38] 7 1 31.5
LocVLM-B 7 1 29.2
LocVLM-L 7 1 32.1
Video-ChatGPT [ 42] 3 100 35.2
LocVLM-Vid-B 3 100 37.4
LocVLM-Vid-B+ 3 8 38.2
Table 7. Video VQA : We report more results (Top-1 Accuracy) for
ActivityNet-QA dataset including multiple baseline and LocVLM
variants. Our proposed models exhibit top performance. VLT de-
notes video level training. More details in Appendix D.
Method Hal-COCO Hal-ADE Hal-Act
Shikra [ 8] 86.2 58.7 -
LLaVa [ 38] 61.9 53.8 -
LocVLM-B 88.3 75.2 -
Video-ChatGPT [ 42] - - 50.6
LocVLM-Vid-B - - 68.7
LocVLM-Vid-B+ - - 72.4
Table 8. Hallucination Evaluation : We report top-1 accuracy (%)
for object presence type questions and showcase reduced object
hallucination in our proposed framework.
off the COCO dataset object annotations and report results
in Tab. 9. Our LocVLM showcases similar performance im-
provements on this dataset.
4.6. Region Description
A unique characteristic of our model (in contrast to V-
LLMs like LLaV A [ 38] & BLIP-2 [ 33]) is its ability to rea-
son with prompts involving coordinate based image space
locations without any input modiﬁcations. Given a point or
bounding box location, we prompt our model to generate
an output describing that location. We refer to this unique
ability of our model as region description (RD). We eval-
uate this RD capability of our model by generating object
12983
DatasetsMetricsBLIP-2 Shikra LLaV AOursRandomAccuracy (")88.6 86.9 50.487.9Precision (")84.1 94.4 50.283.6Recall (")95.1 79.3 99.193.9F1 Score (")89.386.2 66.688.5Yes56.6 43.3 98.856.2PopularAccuracy (")82.8 84.0 49.986.0Precision (")76.3 87.6 49.979.7Recall (")95.1 79.2 99.393.9F1 Score (")84.7 83.2 66.486.3Yes62.4 45.2 99.458.9AdversarialAccuracy (")72.1 83.1 49.778.8Precision (")65.1 85.6 49.976.6Recall (")95.1 79.6 99.193.7F1 Score (")77.3 82.5 66.384.3Yes73.0 46.5 99.461.7Table 9. More object hallucination: Results on POPE evaluation
benchmark [ 36] indicate strong performance of our model.
Method ZS RefCOCO RefCOCO+RefCOCOg
Val Test
SLR [ 67] 7 - - - 15.4
SLR + Rerank [ 67]7 - - - 15.9
Kosmos-2 [ 45] 7 8.67 8.82 14.3 14.1
Shikra [ 8] 7 10.4 11.1 19.7 19.5
LLaVa [ 38] 7 8.43 8.73 13.5 13.5
LocVLM-B 7 14.6 15.2 26.0 26.2
Kosmos-2 [ 45] 3 6.34 8.25 12.4 12.2
LLava [ 38] 3 4.23 7.26 10.6 10.3
LocVLM-B 3 11.0 11.1 20.6 20.7
Table 10. Region Description: We report METEOR scores for
RD task [ 45]. Test-B split is used for RefCOCO & RefCOCO+
datasets. Our method outperforms all prior work.
level descriptions focused on contextual information (e.g.
surrounding of that object in the image). Following evalua-
tion protocol in [ 45] for region description, we extend their
evaluation to three standard referring localization datasets
from RefCOCO [ 29] and report these results in Tab. 10.W e
select the METEOR score as the evaluation metric to ac-
count for variations in word choice in generated answers
which may be acceptable in various cases (e.g. different
sentence structure leading to alternate word ordering). Our
results indicate clear improvements over the LLaV A base-
line [ 38] as well as prior state-of-the-art. We attribute these
improvements to our pseudo-data based training.
4.7. Ablations
Next we conduct ablative studies on separate compo-
nents of our proposed setup: IFT objectives, location type,
and pseudo-data. We follow the same training strategy as
described in Sec. 4.1and present these results in Tab. 11.
LocVLM-B is used for all these experiments. The signiﬁ-
cance of each IFT objective is veriﬁed in Tab. 11(top) withLocPred NegPred RevLoc GQA RD A-QA
7 7 7 44.7 10.3 35.2
3 7 7 45.2 12.2 35.8
3 3 7 46.9 12.5 37.2
3 3 3 47.3 20.7 37.4
Location Type PD GQA RD A-QA
Point 3 47.3 20.6 37.4
Bounding Box 3 47.3 20.7 37.4
Bounding Box 7 46.5 11.6 37.1
Table 11. Ablations: We report top-1 accuracy (%) on GQA and
ActivityNet-QA (A-QA) datasets and METEOR scores for RD
task on RefCOCOg test split. (top) We ablate proposed instruc-
tion ﬁne-tuning objectives to verify usefulness of each objective.
(bottom) We ﬁrst ablate point based and bounding box based loca-
tion forms to showcase minimal difference across them. We next
ablate use of object description pseudo-data (PD). We highlight
the improvements due to pseudo-data, especially on the RD task.
LocPred NegPred RevLoc A-QA
7 7 7 37.4
3 7 7 37.6
3 3 7 38.2
3 3 3 38.2
Table 12. Video Ablation: We report top-1 accuracy (%) on
ActivityNet-QA (A-QA) dataset. Results indicate the generality
of proposed IFT objectives for video domain training as well.
consistent performance improvements across tasks. The
generality of our approach to differing location type (i.e.
points vs bounding boxes) and usefulness of pseudo-data is
visible in Tab. 11(bottom). In particular, we highlight the
notable performance improvement for RD task gained from
using pseudo-data. We also conduct ablations for our video
domain training setup and report these results in Tab. 12.
The LocVLM-Vid-B+ variant is used in these experiments.
Our results showcase the usefulness of proposed IFT objec-
tives for video domain learning as well.
5. Conclusion
We introduce a simple framework that equips visual-
LLMs (V-LLMs) with greater spatial understanding, termed
LocVLM. We leverage the idea of encoding image coor-
dinates within language to propose three instruction ﬁne-
tuning (IFT) objectives. This training process endows V-
LLMs with the ability to reason about spatial composition
of images using image space coordinates within text. A data
efﬁcient training pipeline utilizing pseudo-data allows our
approach to achieve state-of-the-art results in Image VQA,
Video VQA, and Region Description while improving spa-
tial awareness and reducing object hallucination.
12984
References
[1]Jake K. Aggarwal and Michael S. Ryoo. Human activity
analysis. ACM Computing Surveys (CSUR) , 43:1 – 43, 2011.
7
[2]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katie Millican, Malcolm Reynolds, et al. Flamingo: a vi-
sual language model for few-shot learning. arXiv preprint
arXiv:2204.14198 , 2022. 2,7
[3]Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel,
Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bit-
ton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei
Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig
Schmidt. Openﬂamingo, 2023. 2
[4]Pratyay Banerjee et al. Weakly supervised relative spatial
reasoning for visual question answering. ICCV , 2021. 6
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 1,2
[6]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part I 16 , pages 213–229.
Springer, 2020. 2,3
[7]Soravit Changpinyo, Piyush Kumar Sharma, Nan Ding, and
Radu Soricut. Conceptual 12m: Pushing web-scale image-
text pre-training to recognize long-tail visual concepts. 2021
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 3557–3567, 2021. 5
[8]Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, and Rui Zhao. Shikra: Unleashing multi-
modal llm’s referential dialogue magic. arXiv preprint
arXiv:2306.15195 , 2023. 1,2,3,6,7,8
[9]Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffu-
siondet: Diffusion model for object detection. arXiv preprint
arXiv:2211.09788 , 2022. 2,3
[10] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-
offrey Hinton. Pix2seq: A language modeling framework for
object detection. arXiv preprint arXiv:2109.10852 , 2021. 2,
3,1
[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Xing. Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality, 2023. 1,2
[12] Jaemin Cho et al. Dall-eval: Probing the reasoning skills
and social biases of text-to-image generation models. ICCV ,
2023. 2
[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311 , 2022. 2[14] Yufeng Cui, Lichen Zhao, Feng Liang, Yangguang Li, and
Jing Shao. Democratizing contrastive language-image pre-
training: A clip benchmark of data, model, and supervision.
arXiv preprint arXiv:2203.05796 , 2022. 2
[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning.
arXiv preprint arXiv:2305.06500 , 2023. 7
[16] Jian Ding, Nan Xue, Guisong Xia, and Dengxin Dai. De-
coupling zero-shot semantic segmentation. 2022 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 11573–11582, 2021. 2
[17] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang,
Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann Le-
Cun, Nanyun Peng, Jianfeng Gao, and Lijuan Wang. Coarse-
to-ﬁne vision-language pre-training with fusion in the back-
bone. ArXiv , abs/2206.07643, 2022.
[18] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Open-
vocabulary image segmentation. In ECCV , 2022. 2
[19] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision , pages 1440–1448,
2015. 2,3
[20] Gokhale et al. Benchmarking spatial relationships in text-to-
image generation, 2022. 2
[21] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Zero-
shot detection via vision and language knowledge distilla-
tion. arXiv e-prints , pages arXiv–2104, 2021. 2
[22] Tanmay Gupta and Aniruddha Kembhavi. Visual pro-
gramming: Compositional visual reasoning without training.
2023 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 14953–14962, 2022. 2
[23] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. Activitynet: A large-scale video
benchmark for human activity understanding. 2015 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 961–970, 2015. 5
[24] Joy Hsu et al. What’s left? concept grounding with logic-
enhanced foundation models. NeurIPS , 2023. 2
[25] Drew A. Hudson and Christopher D. Manning. Gqa: A new
dataset for real-world visual reasoning and compositional
question answering. 2019 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 6693–
6702, 2019. 3
[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc V . Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In International Con-
ference on Machine Learning , 2021. 1
[27] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-
modulated detection for end-to-end multi-modal understand-
ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 1780–1790, 2021. 2
[28] Amita Kamath et al. What’s ”up” with vision-language
models? investigating their struggle with spatial reasoning.
EMNLP , 2023. 2
12985
[29] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
Tamara Berg. Referitgame: Referring to objects in pho-
tographs of natural scenes. In Proceedings of the 2014 con-
ference on empirical methods in natural language processing
(EMNLP) , pages 787–798, 2014. 8
[30] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation
via large language model. arXiv preprint arXiv:2308.00692 ,
2023. 2
[31] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Ren ´e Ranftl. Language-driven semantic seg-
mentation. ICLR , 2022. 2
[32] Jiahao Li, Greg Shakhnarovich, and Raymond A. Yeh.
Adapting clip for phrase localization without further train-
ing.ArXiv , abs/2204.03647, 2022. 2
[33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 1,2,3,6,7
[34] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai
Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
Videochat: Chat-centric video understanding. arXiv preprint
arXiv:2305.06355 , 2023. 7
[35] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and
Jianfeng Gao. Grounded language-image pre-training. 2022
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 10955–10965, 2021. 2
[36] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji-Rong Wen. Evaluating object hallucina-
tion in large vision-language models. arXiv preprint
arXiv:2305.10355 , 2023. 7,8,2
[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 5,2,3
[38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 1,2,3,4,5,6,7,8
[39] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He,
and Tianrui Li. Segclip: Patch aggregation with learnable
centers for open-vocabulary semantic segmentation. ArXiv ,
abs/2211.14813, 2022. 1,2
[40] Jitendra Malik. Visual grouping and object recognition. In
Proceedings 11th International Conference on Image Analy-
sis and Processing , pages 612–621. IEEE, 2001. 2
[41] David Marr. Vision: A computational investigation into the
human representation and processing of visual information .
MIT press, 1982. 1
[42] Salman Khan Muhammad Maaz, Hanoona Rasheed and
Fahad Khan. Video-chatgpt: Towards detailed video un-
derstanding via large vision and language models. ArXiv
2306.05424 , 2023. 2,5,6,7,3
[43] Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang,
Ashish Shah, Philip H. S. Torr, and Ser Nam Lim. Openvocabulary semantic segmentation with patch aligned con-
trastive learning. CVPR , abs/2212.04994, 2023. 1,2
[44] OpenAI. GPT-4 technical report. https://arxiv.org/
abs/2303.08774 , 2023. 1,2
[45] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
ing multimodal large language models to the world. arXiv
preprint arXiv:2306.14824 , 2023. 2,8
[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1,2,3
[47] Kanchana Ranasinghe and Michael S. Ryoo. Language-
based action concept spaces improve video self-supervised
learning. In NeurIPS , 2023. 2
[48] Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi,
Yinfei Yang, Alexander Toshev, and Jonathon Shlens. Per-
ceptual grouping in contrastive vision-language models. In
ICCV , 2023. 1,2
[49] Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, and
Michael S. Ryoo. Understanding long videos in one multi-
modal language model pass, 2024. 2
[50] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 779–788, 2016. 2,3
[51] Michael S. Ryoo and Jake K. Aggarwal. Recognition
of composite human activities through context-free gram-
mar based representation. 2006 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition
(CVPR’06) , 2:1709–1718, 2006. 7
[52] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie
Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman Castagn ´e,
Alexandra Sasha Luccioni, Franc ¸ois Yvon, Matthias Gall ´e,
et al. Bloom: A 176b-parameter open-access multilingual
language model. arXiv preprint arXiv:2211.05100 , 2022. 2
[53] D’idac Sur’is, Sachit Menon, and Carl V ondrick. Vipergpt:
Visual inference via python execution for reasoning. ArXiv ,
abs/2303.08128, 2023. 2
[54] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos:
Fully convolutional one-stage object detection. In Proceed-
ings of the IEEE/CVF international conference on computer
vision , pages 9627–9636, 2019. 2,3
[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efﬁcient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 1,2,3
[56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and ﬁne-tuned chat models. arXiv
preprint arXiv:2307.09288 , 2023. 1,2,3
[57] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-
ers, and Arnold WM Smeulders. Selective search for object
recognition. IJCV , 104(2):154–171, 2013. 2
12986
[58] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu
Qiao, et al. Visionllm: Large language model is also an
open-ended decoder for vision-centric tasks. arXiv preprint
arXiv:2305.11175 , 2023. 2,3,1
[59] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and
Quoc V . Le. Finetuned language models are zero-shot learn-
ers.ArXiv , abs/2109.01652, 2021. 2
[60] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:
Semantic segmentation emerges from text supervision.
CVPR , 2022. 2
[61] Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang,
Yu Qiao, and Weidi Xie. Learning open-vocabulary seman-
tic segmentation models from natural language supervision.
ArXiv , abs/2301.09121, 2023. 2
[62] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, So-
ham Ghosh, Yonghui Wu, and Jiahui Yu. Videococa: Video-
text modeling with zero-shot transfer from contrastive cap-
tioners. arXiv , 2022. 7
[63] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Just ask: Learning to answer questions
from millions of narrated videos. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 1686–1697, 2021. 7
[64] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,
and Cordelia Schmid. Zero-shot video question answering
via frozen bidirectional language models. arXiv preprint
arXiv:2206.08155 , 2022. 7
[65] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. FILIP: Fine-grained interactive language-
image pre-training. In ICLR , 2022. 2
[66] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen
Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and
Yinfei Yang. Ferret: Refer and ground anything anywhere
at any granularity. ArXiv , abs/2310.07704, 2023. 2
[67] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L. Berg. A
joint speaker-listener-reinforcer model for referring expres-
sions. 2017 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 3521–3529, 2016. 8
[68] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and
Chen Change Loy. Contextual object detection with
multimodal large language models. arXiv preprint
arXiv:2305.18279 , 2023. 2
[69] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vi-
sion language pre-training: Aligning texts with visual con-
cepts. ArXiv , abs/2111.08276, 2021. 2
[70] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun
Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu
Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unify-
ing localization and vision-language understanding. ArXiv ,
abs/2206.05836, 2022. 2
[71] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An
instruction-tuned audio-visual language model for video un-
derstanding. ArXiv , abs/2306.02858, 2023. 7[72] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Jiao
Qiao. Llama-adapter: Efﬁcient ﬁne-tuning of language mod-
els with zero-init attention. ArXiv , abs/2303.16199, 2023. 7
[73] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi
Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-
struction tuning large language model on region-of-interest.
arXiv preprint arXiv:2307.03601 , 2023. 2
[74] Yabo Zhang, Zihao Wang, Jun Hao Liew, Jingjia Huang,
Manyu Zhu, Jiashi Feng, and Wangmeng Zuo. Associating
spatially-consistent grouping with text-supervised semantic
segmentation. ArXiv , abs/2304.01114, 2023. 1,2
[75] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi
Feng, and Bingyi Kang. Bubogpt: Enabling visual ground-
ing in multi-modal llms. arXiv preprint arXiv:2307.08581 ,
2023. 2
[76] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free
dense labels from clip. In European Conference on Com-
puter Vision , 2021. 2
[77] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,
Jianfeng Gao, and Yong Jae Lee. Segment everything every-
where all at once. In NeurIPS , 2023. 5,2
12987
