Semantic Human Mesh Reconstruction with Textures
Xiaoyu Zhan1, Jianxin Yang1, Yuanqi Li1, Jie Guo1, Yanwen Guo1∗, and Wenping Wang2
1Nanjing University2Texas A&M University
{zhanxy,jianxin-yang,yuanqili }@smail.nju.edu.cn, {guojie,ywguo }@nju.edu.cn, wenping@tamu.edu
https://zhanxy.xyz/projects/shert
Face
estimation
Facial expressionFace substitution
(b) Reconstructed
semantic mesh(c) Sampled &
Generated textures(d) Face substitution
& Animation
(e) Monocular image
reconstructionInput  image
Inpainted texture
light blue , long- sleeved shirt,
intricate floral embroidery
Text-driven
generated texture 
Mesh- sampled
texture 
(a)SMPL -X model
& Target surface
Figure 1. Semantic Human mEsh Reconstruction with Textures (SHERT): (a) Given a target surface and the corresponding semantic
guider, (b) SHERT reconstructs a detailed semantic model, which has stable UV unwrapping and skinning weights with high-quality
triangle meshes. (c) It can either sample a texture map from the target surface or generate from the text prompts. (d) Based on our semantic
representation, SHERT allows for high-precision facial reconstruction and animation of the body, face, and hands. (e) Moreover, SHERT
is capable of inferring a fully textured avatar from a monocular image.
Abstract
The field of 3D detailed human mesh reconstruction has
made significant progress in recent years. However, current
methods still face challenges when used in industrial appli-
cations due to unstable results, low-quality meshes, and a
lack of UV unwrapping and skinning weights. In this paper,
we present SHERT, a novel pipeline that can reconstruct se-
mantic human meshes with textures and high-precision de-
tails. SHERT applies semantic- and normal-based sampling
between the detailed surface ( e.g. mesh and SDF) and the
corresponding SMPL-X model to obtain a partially sampled
semantic mesh and then generates the complete semantic
mesh by our specifically designed self-supervised comple-
tion and refinement networks. Using the complete seman-
tic mesh as a basis, we employ a texture diffusion model to
create human textures that are driven by both images and
texts. Our reconstructed meshes have stable UV unwrap-
ping, high-quality triangle meshes, and consistent semanticinformation. The given SMPL-X model provides semantic
information and shape priors, allowing SHERT to perform
well even with incorrect and incomplete inputs. The se-
mantic information also makes it easy to substitute and ani-
mate different body parts such as the face, body, and hands.
Quantitative and qualitative experiments demonstrate that
SHERT is capable of producing high-fidelity and robust se-
mantic meshes that outperform state-of-the-art methods.
1. Introduction
Recovering highly realistic details and textures of a human
mesh from monocular images is crucial for various applica-
tions such as gaming, movies, cartoons, VR, virtual try-on,
and digital avatars. Current approaches [1, 2, 4, 23, 24, 26,
39, 54, 55, 57, 61, 63, 64, 67, 70] primarily focus on the re-
covery of the geometric details that are associated with im-
ages, but their results are not yet practical for real-world ap-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
142
plications. Recent advancements in parametric and explicit
clothing models [12, 19, 29, 50, 62, 75] have shown promise
in clothing reconstruction. However, they have limitations
in accurately fitting different geometries and details. Im-
plicit reconstruction approaches [23, 54, 55, 63, 64, 67, 70]
excel at capturing clothing details but perform poorly in
hands and face reconstruction. They may also generate in-
complete and geometrically inseparable results.
In this work, our goal is to reconstruct fully textured se-
mantic human meshes through given detailed surfaces and
corresponding semantic guiders. Our semantic human mesh
is complete, animatable, and edit-friendly for both users and
designers. It ensures that each vertex has deterministic se-
mantic information and predefined skinning weights. This
allows for easy substitution and animation of different body
parts such as the face, body, and hands. The semantic infor-
mation also guarantees stable and reasonable UV unwrap-
ping, which is advantageous for editing and image-based
texture generation.
Specifically, we propose SHERT, which generates a se-
mantic human mesh from the detailed surface and its cor-
responding SMPL-X [46] model and optionally infers tex-
tures from images or colored surfaces. SHERT has four
main processes. 1) Sampling , SHERT applies semantic
and normal-based sampling to obtain a partially sampled se-
mantic mesh based on the input detailed surface and SMPL-
X model. We subdivide the original SMPL-X model to
better capture the human details. 2) Completion , a self-
supervised network is proposed to complete the partially
sampled mesh. The network works in the 2D UV domain,
which converts the 3D completion task into a 2D inpainting
task. 3) Refinement , the origin image and front-back nor-
mal maps are used to enhance the geometry details. 4) Tex-
ture, our semantic human mesh is projected to the origin
image in order to generate the partial texture map. Then we
adapt the diffusion framework for text-driven partial texture
inpainting and generation. The powerful generation abil-
ity of pre-trained diffusion model enables textures with rich
clothing details and clear facial expressions.
Extensive experiments have been conducted on both
datasets and in-the-wild images. The quantitative and quali-
tative results show the robustness and superior performance
of SHERT in reconstructing high-fidelity semantic human
meshes and generating various high-resolution textures.
In summary, the main contributions of this work include
four-fold.
• We introduce SHERT, a novel pipeline to reconstruct
high-quality semantic human meshes from the detailed
3D surfaces, represented either explicitly as meshes, or
implicitly as signed distance field (SDF). SHERT is also
capable of predicting robust and fully textured avatars
with high-fidelity faces from monocular images.
• We propose a semantic- and normal-based samplingmethod (SNS) and a self-supervised mesh completion
network to achieve non-rigid 3D surface registration. The
approach has the capability to process incomplete and in-
accurate inputs by leveraging SMPL-X human priors.
• We present a self-supervised mesh refinement network
working in the UV domian. It utilizes the images and
front-back normal maps to improve the geometric mesh
details.
• We use a diffusion model to infer high-resolution human
textures from input images. The model can also accom-
plish text-driven texture inpainting and generation.
2. Related Work
2.1. Monocular 3D Human Reconstruction
Clothed Human Reconstruction. Most of the current
CNN-based monocular 3D clothed human shape estima-
tion methods can be divided into two categories: explicit-
based [1–4, 29, 73, 74] and implicit-based [5, 12, 23–
26, 39, 54, 55, 57, 63, 64, 67, 70] approaches, based on
their representations. Explicit-based approaches usually in-
fer the 3D offsets on top of the parametric human model
[41, 46, 49, 65]. However, these methods are difficult to ap-
ply to flexible human topologies and cannot capture details
well. Implicit-based approaches have the advantage of rep-
resenting arbitrary 3D clothed human shapes that are free
from the limitations of parametric human models. There
are also some works [13, 25, 39, 63, 64, 70] that mix the im-
plicit and explicit representations to achieve the detailed and
robust 3D clothed human reconstruction. But both implicit-
based and mixed methods still cannot maintain the stability
of the human body shape well, often resulting in problems
such as blurring and missing local body parts in the pre-
dicted results. Some methods [12, 19, 29, 50, 62, 75] pro-
pose additional parametric or implicit clothing models to fit
loose clothes. These clothing models often struggle to accu-
rately capture the details present in the images. Neverthe-
less, they are still useful for generating rough approxima-
tions of the clothing. Despite considerable progress made
in monocular human reconstruction capture, there are still
certain constraints, especially in terms of reliability, avail-
ability, and user-friendliness.
Texture Prediction. Previous works [5, 13, 24–26, 44, 54,
57, 59] have also focused on predicting image-based human
textures. However, the current emphasis of implicit-based
methods [5, 13, 24–26, 54] is still on predicting vertex col-
ors, which cannot be easily converted into usable texture
maps for industrial applications. DINAR [59] proposes neu-
ral textures for modeling human avatars and achieves tex-
ture inpainting by utilizing the diffusion framework. Nev-
ertheless, the neural texture is highly coupled with human
avatars, and the resolution is relatively low.
Face Reconstruction. In recent years, parameterized face
143
SNS Infer
t
 s
 c
r
o
SResample
Transfer
Infer
SrResampleProjectionTransfer
Sample
Mask &
Partial TextureTextured
c
Completion
Network
Refinement
Network
Texture
DiffusionSampling
RefinementCompletion
Texturex
 S
f&b
Figure 2. Overview of SHERT for monocular image reconstruction . Given an RGB image I, SHERT first infers the detailed mesh Mt
and corresponding sub-SMPLX model Mx. It then applies SNS to obtain the partial semantic mesh Ms(in Sec. 3.2). The Completion
Net infers Mcby filling the UV holes in Ms(in Sec. 3.3). Image and normal maps are utilized to generate Mr, which contains sharper
geometry details (in Sec. 3.4). Finally, SHERT uses a diffusion model to achieve text-driven texture inpainting and generation (in Sec.
3.5).
models [8, 9, 36, 47] have been widely used in monocu-
lar face reconstruction algorithms [14, 17, 18, 56, 76], con-
tributing to the success of achieving realistic high-quality
reconstruction results. In light of the incorporation of
FLAME [36] into SMPL-X [46], we can utilize high-quality
facial reconstruction results from previous works to en-
hance the accuracy and realism of the SMPL-X based hu-
man body reconstruction results.
2.2. Non-rigid 3D Registration
Non-rigid 3D surface registration approaches [6, 10, 22, 30,
35, 45, 66, 68] usually compute a deformation that aligns
a source surface with a target surface [15]. SHERT aims
to transfer the semantic information of parametric human
models to the corresponding detailed surfaces through reg-
istration. However, existing methods mainly focus on the
precise registration of the source and target, while the tar-
get detailed human surfaces in real-world tasks often have
incomplete and erroneous data.
3. Method
SHERT is capable of reconstructing a high-fidelity fully
textured semantic human mesh based on a pair of detailed
3D surface and corresponding SMPL-X [46] model. Our
results have high robustness and will not result in missing
body parts in despite of the incomplete inputs. Furthermore,
the semantic mesh ensures that each vertex has determinis-
tic semantic information and predefined skinning weights,
making it possible to replace and animate the human face,
body, and hands. SHERT can also generate realistic hu-man textures from texts and images. These features make
it easy for users and designers to use or further edit our re-
sults. To achieve this, SHERT first subdivides the SMPL-X
model to improve the accuracy in capturing human details
(in Sec. 3.1) and then applies the semantic- and normal-
based sampling to obtain a partially sampled mesh (in Sec.
3.2). We complete the partial result using a self-supervised
mesh completion network (in Sec. 3.3), and finally enhance
the geometry details through refinement (in Sec. 3.4). In
addition, SHERT uses a diffusion model for text-driven hu-
man texture inpainting and generation (in Sec. 3.5).
3.1. Subdivided SMPL-X model
In this work, we use SMPL-X as the semantic guider.
SMPL-X is a parameterized body model that combines
SMPL [41] with the FLAME [37] head model and the
MANO [52] hand model. It is parameterized with shape and
pose, has 10,475vertices and 54joints, including joints for
the neck, jaw, eyeballs, and fingers. The SMPL-X model
provides basic skinning weights and corresponding seman-
tic information for each vertex.
SHERT defines the sub-SMPLX based on SMPL-X
since 10,475vertices are not sufficient to accurately rep-
resent the details of the human body and clothing, such as
facial details and clothing wrinkles. Taking into considera-
tion both the expressiveness of the model and computational
cost , we apply the mid-point subdivision algorithm twice
on a standard SMPL-X model (with eyeballs removed) to
obtain the sub-SMPLX with 149,921vertices and 299,712
faces.
144
a) Explicit Sampling b) Implicit SamplingSDF Zero
Isosurface
Sub- SMPLX 
SurfaceExplicitSurface
Sub- SMPLX 
SurfaceSDF
Normal RayIntersection
0 + -Reg. SurfaceClosest Sample
Reg. Surface
Normal RaySample
Interval
Vertex VertexFigure 3. Explicit and implicit sampling . a) SNS shoots a ray
from the starting vertex along the vertex normal to locate the in-
tersection point with the target surface. b) SNS takes samples at
fixed intervals along the vertex normal ray and search for the point
that is closest to the zero isosurface.
3.2. Semantic- and Normal-based Sampling (SNS)
Our objective is to accurately predict the non-rigid deforma-
tion between a source surface and a target surface. Given a
source surface Mx, which is represented as a sub-SMPLX
in our work, and the target surface Mt, we learn a mapping
function D:M149921 ×3
x → M149921 ×3
s such that Mscan
be aligned with Mtthrough the learned mapping in geom-
etry, and ignoring the incorrect parts.
Specifically, we obtain a partially sampled mesh through
a sampling scheme based on the vertex normals N149921 ×3.
Starting from a point on Mx, we search for the intersection
point with the target surface Mtalong the vertex normal
ray. We can extend the sampling scheme to implicit surfaces
with constant step Ray Marching [28, 48], shown in Fig. 3
and Fig. 4.
It should be noted that SNS does not always return satis-
factory results since the ray may not intersect with the tar-
get surface. We need to locate and label the vertices that
have failed to register. In addition, due to the large geomet-
ric differences and the possibility of incorrect alignment,
the sampling scheme may retrieve incorrect results, which
should also be detected. In order to remove incorrect points
and triangle meshes from the sampled results, we calcu-
lateθ(the angle between the normal vectors of the sampled
triangle mesh and the corresponding sub-SMPLX triangle
mesh), s(the area ratio between the sampled triangle mesh
and the corresponding sub-SMPLX triangle mesh), and r
(the edge ratio between the longest and shortest edges of
the sampled triangle mesh) for each triangle mesh in the
sampled result. We then perform mesh culling by removing
low-quality triangle meshes that have at least one indicator
exceeding the threshold among the three mentioned above.
Finally, we perform a connectivity check on the processed
sampled mesh and remove the sets with a number of con-
nected triangle meshes less than g. By default, we set θ= 2,
s= 3,r= 3, andg= 500 .
We found that culling meshes only in the current pose
would result in unreasonable deformations in human ani-
mation tasks. Therefore, SNS repeats the processing above
in the canonical space. This ensures that we remove almost
all unreasonable sampling results and obtain the partial se-
Marching CubesSNS
(Implicit sampling)SNS
(Mesh culling) SNS + CompletionResolution: 700
Vertices: 165330
Faces: 330652Samples for each ray: 200
Vertices: 149921Faces: 299712Figure 4. SNS in the implicit field . We compare the performance
of SNS and the Marching Cubes algorithm [42] in the implicit
field predicted by the multi-view PIFu [54]. The results show that
our SNS and completion network correctly reconstruct the implicit
field and obtain a smoother surface compared to Marching Cubes.
mantic mesh Ms.
3.3. Self-supervised Mesh Completion
In Sec. 3.2, the vertices that failed to register have resulted
in holes in Ms. Currently, there are many mesh comple-
tion algorithms [11, 31–33, 40] that can fill in these mesh
holes. However, these algorithms cannot be well adapted
to our semantic reconstruction task due to the strong con-
straints on the number and relative positions of vertices in
our representation in Sec. 3.1.
Therefore, we transform the partially sampled mesh into
the UV domain [4, 16, 72] according to the semantic infor-
mation. As a result, we convert the mesh completion task in
3D space into an inpainting task on a 2D image. Then, we
design a self-supervised completion network that can fill in
the holes of the partially sampled mesh. As shown in Fig. 5,
by adding a random hole mask Hron the partial UV posi-
tion map, we can obtain trainable pairs, which has a consis-
tent distribution as the missing parts of the sampled mesh.
The manually masked parts can provide the network with
consistent supervision information as the input parts. This
completion network is capable of generating robust meshes
while maintaining both mesh quality and semantic consis-
tency in the final results (refer to Fig. 6).
To further decrease the learning difficulties, we trans-
form all incompletely sampled meshes to canonical space
when facing the diverse range of poses and clothing styles.
We believe that this approach has the potential to miti-
gate the challenges in problem-solving, as completion is no
longer affected by pose and body shape. Meanwhile, since
the sampled points are located on the normal ray of the sub-
SMPLX vertices, we represent the deformation relative to
sub-SMPLX as an offset based on the vertex normal. The
transformed UV position map Scan be presented as:
d=Ssample −Spose
Npose, (1)
S=Scano+Ncano·d, (2)
where Ssample denotes the partial UV position map of the
sampled mesh. Spose andScano refer to the UV position
145
SNS
 Transfer Complete
r
 sup
 𝓛𝓛inpResample
S
Add MulSample
t s
Sp· sup
o &SS · (1-sup)Figure 5. Completion network . The completion network trans-
forms the result of SNS into canonical space and predicts the holes
in the UV domain.
maps of the original posed sub-SMPLX and the canonical
sub-SMPLX respectively. Npose andNcano are the corre-
sponding UV normal maps.
The complete UV position map Sand the input com-
bined hole mask Hsupare calculated by:
Hsup=Hr·(1− H o) +Ho, (3)
S=Scano+Ncano·[d· Ho+d·(1− H o)],(4)
where HrandHodenote the randomly added hole mask
and the hole mask generated by SNS respectively. d∈
RH×W×3is the estimated displacement UV map. The com-
plete mesh Mcis resampled from Sand then transformed
to the original pose space.
The primary loss used in our completion network is as
follows:
Sp=Scano+Ncano·[d· Hsup+d·(1− H sup)],(5)
Linp=(Sp−S)·(Hsup− H o)2
2P
i,j(Hsup− H o)i,j. (6)
We find that completing the face, hands, and feet is much
more difficult than other parts of the body, so we use the cor-
responding parts of sub-SMPLX for replacement. Further-
more, SHERT allows for the use of FLAME-based meth-
ods such as EMOCA [14] to replace the facial region in our
completion results, enhancing the accuracy and realism of
the facial details.
3.4. Self-supervised Mesh Refinement
After completing the mesh, we have already reconstructed
a semantic mesh with extremely high levels of detail. How-
ever, some geometric details may be lost during the SNS
and completion process. Therefore, we design an additional
self-supervised refinement network to further optimize the
Target(ECON) Ours(SNS + Completion)Figure 6. The robustness of SNS and Completion Network . We
present our complete meshes, which are reconstructed using the
predictions of ECON [64] from in-the-wild images. Distinguish-
ing itself from previous registration methods, SHERT has the ca-
pability to effectively process scenarios where the inputs are in-
complete, inaccurate, or contain errors by leveraging SMPL-X hu-
man priors.
Laplacian
Smoothing
RenderSample
Add Concat
Feature Projection
𝓛𝓛dis
𝓛𝓛normal
IterateSample
Cam & 
Light
&f&btSlSr Sc
Nr Nc
c
l
Figure 7. Refinement network . The features extracted from the
image and front-back normal maps are projected to the UV do-
main. These projected features are subsequently combined with
the input UV position map to generate a refined mesh.
mesh’s details ( e.g. cloth wrinkles, subtle deformations of
body movements) using the image and normal maps. The
input normal maps can be obtained either by rendering the
scanned model or by using prediction networks. Our re-
finement network follows a U-Net [53] architecture and is
capable of predicting a displacement UV map, denoted as
z∈RH×W×3, in order to optimize the complete mesh Mc.
Since the input image and normal maps are not in the UV
domain, we utilize a separate network to extract the image
domain features and then project them to UV domain.
In order to guide the network in learning the potential
correspondence between the input features and mesh de-
tails, we apply Laplacian Smoothing [20] on the existing
complete meshes, as shown in Fig. 7. The smoothed mesh
Mlis used as input to the network, and the original mesh
Mcis used for supervision.
Given the image I, front-view normal map Nf, back-
view normal map Nb, and the corresponding complete UV
position map Sl, the projected feature Fis typically repre-
sented as
F=P(F(I,Nf,Nb), Sl, c), (7)
where crepresents the camera parameters, Fdenotes the
image domain feature extraction network, and Pis the pro-
jection function that can project the image domain features
to the UV domain according to the camera parameters and
the 3D coordinates of points on Sl.
146
Textured Semantic
Human Mesh
Textured ScanStable Diffusion 
& ControlNet
A UV texture map of a man in 
a blue shirt and black pants.Cam & 
Light 
UV TexturePartial Texture
SampleRender &
Projection
Generated TextureSHERT + ICP
FreezeStable Diffusion 
& ControlNet
A UV texture map of a man in 
a blue shirt and black pants.Partial TextureSuper
Resolution
Generated Texture
Mask
Training Inference128x128 
Concat Inpaint (Optional)
(Optional)128x128 Mask
Concat Inpaint (Optional)Figure 8. Texture diffusion . We use two strategies to finetune the diffusion model for text-driven texture repainting and inpainting
separately. During inference, instead of just using random noise, the encoded partial texture map and mask can be manually added to the
latent features to preserve the original information of the input. Additionally, we use the super resolution method Real-esrgan [60], which
can specifically optimize both the body and face, to further enhance the final outputs.
The UV position map Srof the refined mesh Mris
given by
Sr=Sl+Nl·z, (8)
where z∈RH×W×3represents the predicted displacement
UV map of the refine network, SlandNlare the UV posi-
tion map and normal UV map of Mlrespectively.
The loss we used in refine network can be represented as
Ldis=MSE (z−Sc−Sl
Nl), (9)
Lnormal =MSE (Nr−Nc), (10)
where Scis the UV position map of Mc.NrandNcdenote
the normal UV maps of MrandMcrespectively.
Moreover, the refine network can be iteratively employed
by users to enhance the mesh details according to their de-
sired level (refer to Fig. 11).
3.5. Text-driven Texture Inpainting and Generation
In Sec. 3.4, all the reconstructed meshes share the same
semantic information from SMPL-X. This means that when
the models are unfolded to the UV domain, vertices with
the same semantic information will be projected to a fixed
UV position, resulting in a stable texture map that allows
for repainting and transferring textures. To perform texture
inpainting on incomplete human body textures and generate
high-quality human body textures driven by text, we fine-
tune the Stable Diffusion model [51] with ControlNet [71],
similar to the approach taken by Dinar [59].
As our reconstructed semantic mesh closely matches the
scanned model’s geometry, we can use the ICP algorithm
[7] to register the vertex colors from the scanned model onto
our result. This enables us to convert the texture maps into
the SMPL-X format. As shown in Fig. 8, we obtain a par-
tial texture map from the input image based on the seman-
tic mesh and camera parameters and hope to generate the
invisible parts. ControlNet enable us to add conditional in-
puts (partial texture maps) to the generation of the diffusionnetwork at a low cost. We use two strategies to finetune
the diffusion model, distinguished by whether to take the
encoded partial texture map and mask as the additional in-
puts [27]. During the inference process, we can directly
input partial texture maps into the ControlNet and generate
various results by adding random noise. Alternatively, we
can enhance the preservation of existing information in im-
ages by concatenating the encoded partial texture maps and
masks with the latent features. Although Stable Diffusion
has not worked with UV-parameterized images before, the
well-designed UV parameterization keeps the shapes of the
face, body, and limbs stable, ensuring a learnable space for
the model. The texture map for the body and limbs focuses
more on color and patterns rather than shapes, also resulting
in excellent outcomes when cropped with a mask.
4. Experiments
4.1. Datasets and Networks
Training data. The completion and refinement networks
are trained using the first 499 scans of THuman2.0 [69].
During the training of the completion network, we ran-
domly choose one of the remaining 498 masks as a random
hole mask. To enrich the inputs for the refinement network,
MethodCAPE[43] THuman2.0[69]
P2S↓ Chamfer ↓Normal ↓ P2S↓ Chamfer ↓Normal ↓
PIFu[54]∗2.1137 1.6537 0.0755 2.5493 2.3640 0.1042
PIFuHD[55] 3.7846 3.5787 0.1002 3.0772 3.1808 0.1207
PaMIR[70]∗1.4520 1.2241 0.0610 1.5439 1.3311 0.1102
ICON[63] 0.8855 0.8609 0.0347 1.0361 1.0874 0.0607
ECON[64] 0.9403 0.9386 0.0374 1.1304 1.2081 0.0661
2K2K[23]†- - - 2.5342 2.6165 0.1030
Ours (ICON-comp) 0.8550 ↑ 0.8107 ↑ 0.0359 1.0459 1.0465 ↑ 0.0604 ↑
Ours (ICON-refine) 0.8633 0.8112 0.0380 1.0442 ↑ 1.0468 0.0603 ↑
Ours (ECON-comp) 0.8561 ↑ 0.8242 ↑ 0.0378 1.1255 ↑ 1.1420 ↑ 0.0672
Ours (ECON-refine) 0.8581 0.8144 ↑ 0.0398 1.0630 ↑ 1.0430 ↑ 0.0649 ↑
Table 1. Quantitative evaluation for monocular image recon-
struction . We evaluate the performance of our completion results
(comp) and refinement results (refine) by comparing them with
state-of-the-art methods. ∗methods are re-implemented in [63]
to ensure a fair comparison. †method has only been tested with
human-facing-forward images. ↑and↑indicate the improvement
achieved through completion and refinement, respectively.
147
InputOurs
(ECON -based)ECON
 2K2K
 ICON
 PaMIR
 PIFuHD
 PIFu
Figure 9. Qualitative comparison for monocular image reconstruction on in-the-wild image . For each method, we present two views
of the reconstructed results. SHERT demonstrates the ability to handle challenging poses while providing clear details of facial and hand
geometry.
Scan &
Sub-SMPLXDR N-ICP RPTS SVR-0SNS
(Ours)MDA Fast-RNRR
thresh=0.3
alpha=5
normal=120alpha=0.5
beta=10alpha=10
beta=100alpha=2
beta=100
thresh=0.3
normal=120iter=800iter=300
mass=1.5
Figure 10. Qualitative comparison for registration on THuman2.0 . We compare the registration quality of various methods including
N-ICP [6], RPTS [35], SVR- L0[22], Fast-RNRR [68], DR [45], MDA [30] and ours SNS. The holes present in our result are the eliminated
faces, as described in Sec. 3.2. The results indicate that SNS exhibits excellent performance in terms of model details, mesh quality, and
registration robustness. The quantitative comparisons are shown in Tab. 2.
we rotate the meshes every 60 degrees, resulting in a total
of 2994 different orientations. We utilized the ICP [7] al-
gorithm to transfer the color of the THuman2.0 scans to the
vertices of our completed result, thereby generating 499 UV
textures for training and obtaining 2994 visible UV masks
from the rotated meshes. The complete UV textures and
visible UV masks are randomly combined as inputs for our
texture diffusion.
Testing data. We conduct quantitative and qualitative eval-
uations on CAPE [43], THuman2.0, and in-the-wild im-
ages. We use CAPE-NFP [63] (100 samples with 3 view-
points for each), and the last 27 subjects of THuman2.0
scans (6 viewpoints, each differing by 60 degrees).
Ours
CompletionScan Ours Refine
(Iteration = 1 )Ours Refine
(Iteration = 2 )Ours Refine
(Iteration = 3 )
Figure 11. The ablation results (with face substitution) . We
present the results after completion and refinement. With an in-
creasing number of refinement iterations, the details of the mesh
will be enhanced. Please zoom-in to see more details.Method GPU P2S↓Chamfer ↓G-avg↑θ<30◦↓ Time
N-ICP [6] - 0.213 0.163 0.506 55.2 7m 23s
RPTS [35] - 0.488 0.360 0.565 48.3 1m 55s
SVR-L0[22] - 0.404 0.296 0.531 53.5 1h 23m 32s
Fast-RNRR [68] - 0.115 0.097 0.597 31.1 1m 4s
DR [45]√0.339 0.347 0.581 47.7 16m 17s
MDA [30]√0.671 0.731 0.587 48.5 4m 48s
Ours (SNS) - 0.107 0.078 0.729 17.3 23s
Ours (Comp)√0.139 0.167 0.662 28.9 27s (23 + 4)
Table 2. Quantitative evaluation for registration on THu-
man2.0 . We test all the methods on the first subject of THu-
man2.0. Following the previous researches [21, 38], we adapt
G-avg as a method for evaluating the mesh quality. We also re-
port the metric θ<30◦, which denotes the percentage of triangle
meshes in the given mesh that have an angle less than 30 degrees.
Additionally, we present the metrics for our complete mesh.
Networks. The completion net and refinement net are both
trained for 100 epochs with a learning rate of 1×10−6. The
resolutions of the input and output data, including the UV
position maps, images, masks and front-back normal maps,
are all 1024×1024×3. During inference, the ECON’s
predicted front-back normals ( 512×512×3) are upsampled
to1024×1024×3using bilinear interpolation. The texture
diffusion is trained for 1 epoch with a learning rate of 2×
10−5. The sampler of texture diffusion is DDIM [58]. We
use 30 steps by default and infer the texture UV map with a
resolution of 1024×1024×3. All the networks are trained
on three NVIDIA RTX 3090 GPUs.
148
256x256
Input PIFu DINAR Ours
 PaMIR
1024x1024Figure 12. Qualitative comparison for texture prediction on in-
the-wild image . We display the front and back view rendering
results for each method. Since PIFu [54] and PaMIR [70] predict
vertex colors, we only exhibit the texture maps of DINAR [59] and
SHERT (ours). Please zoom-in to see more details.
4.2. Evaluation
Quantitative comparisons. We conduct quantitative com-
parisons with mainstream state-of-the-art monocular im-
age reconstruction approaches in Tab. 1. As in previous
work [23, 55, 63, 64], we report the point-to-surface Eu-
clidean distance (P2S, cm), the Chamfer Distance (cm), and
the Normals difference (L2). To ensure a fair comparison,
PIFu∗[54] and PaMIR∗[70] are re-implemented and re-
trained on THuman2.0, using the same settings as ICON
[63]. The ground-truth SMPL/SMPL-X models are pro-
vided for evaluation. However, PIFu [54], PIFuHD [55]
and 2K2K [23] do not utilize the parametric body priors,
which may result in subpar performance. In Tab. 1, “ICON-
comp” refers to the completion result achieved by leverag-
ing ICON’s [63] prediction, while “ECON-refine” denotes
the refinement mesh obtained using ECON’s [64] result and
the predicted front-back normal maps. Additionally, we
evaluate the registration quality of SNS against state-of-the-
art non-rigid registration methods, as presented in Tab. 2
Qualitative comparisons. We demonstrate a comparison
between SHERT and state-of-the-art methods using in-the-
wild images, with a focus on monocular image reconstruc-
tion (refer to Fig. 9) and texture prediction (refer to Fig. 12).
Additionally, we compare the registration quality of our
SNS with state-of-the-art non-rigid registration approaches
on Thuman2.0 (refer to Fig. 10).
4.3. Limitations
Due to the geometric limitations of SMPL-X, SHERT per-
forms weaker in reconstructing loose clothing, shoes and
hair compared to implicit-based reconstruction methods. It
is also difficult to ensure consistent results of texture diffu-
sion at the seams of UVs. See more in SupMat.
5. Applications
SHERT uses the skinning weights from SMPL-X to en-
able animated poses, expressions, and gestures on the re-
constructed mesh through LBS [34] (refer to Fig. 13). It
allows for both global texture repainting (refer to Fig. 14)
Figure 13. Animation results . Please zoom-in to see more details.
Originpink sweatshirt,
light blue pants,
hiphop stylegray jacket,
blue jeanswoman,
green sweatshirtsweatshirt,
black pants
Figure 14. Global texture repainting . SHERT can repaint the
texture through text prompts. Please zoom-in to see more details.
Texture map
& Render resultMask &
Text promptLocalized repainting
& Render results
cartoon 
embroidery , 
plush material
star pattern, colorful star , 
face accessories
Figure 15. Localized texture repainting . SHERT can repaint the
masked area through text prompts. Please zoom-in to see more
details.
and the option for users to provide custom masks and text
prompts for localized texture repainting (refer to Fig. 15).
6. Conclusion
We propose SHERT, which reconstructs a fully textured se-
mantic human avatar from a detailed surface or a monocu-
lar image. It takes advantage of the geometric details of the
target surface, along with semantic information and prior
knowledge of the semantic guider. The reconstructed re-
sults have high-fidelity clothing details, high-quality trian-
gle meshes, clear facial features, and complete hands geom-
etry. SHERT is also capable of generating high-resolution
texture maps with stable UV unwrapping. This approach
bridges existing monocular reconstruction work and down-
stream industrial applications, and we believe it can pro-
mote the development of human avatars.
Acknowledgments
This work was supported by the National Natural Science
Foundation of China (62032011) and the Natural Science
Foundation of Jiangsu Province (No. BK20211147).
149
References
[1] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian
Theobalt, and Gerard Pons-Moll. Video based reconstruction
of 3d people models. In IEEE Conf. Comput. Vis. Pattern
Recog.(CVPR) , 2018. 1, 2
[2] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian
Theobalt, and Gerard Pons-Moll. Detailed human avatars
from monocular video. In IEEE Conf. 3D Vis.(3DV) , 2018.
1
[3] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar,
Christian Theobalt, and Gerard Pons-Moll. Learning to re-
construct people in clothing from a single rgb camera. In
IEEE Conf. Comput. Vis. Pattern Recog.(CVPR) , 2019.
[4] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,
and Marcus Magnor. Tex2Shape: Detailed full human
body geometry from a single image. In Int. Conf. Comput.
Vis.(ICCV) , 2019. 1, 2, 4
[5] Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchis-
escu. Photorealistic monocular 3d reconstruction of hu-
mans wearing clothing. In IEEE Conf. Comput. Vis. Pattern
Recog.(CVPR) , 2022. 2
[6] Brian Amberg, Sami Romdhani, and Thomas Vetter. Optimal
step nonrigid icp algorithms for surface registration. In IEEE
Conf. Comput. Vis. Pattern Recog.(CVPR) , 2007. 3, 7
[7] P.J. Besl and Neil D. McKay. A method for registration of 3-
d shapes. IEEE Trans. Pattern Anal. Mach. Intell.(TPAMI) ,
1992. 6, 7
[8] V olker Blanz and Thomas Vetter. A morphable model for
the synthesis of 3d faces. In 26th Annual Conference on
Computer Graphics and Interactive Techniques (SIGGRAPH
1999) , 1999. 3
[9] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun
Zhou. Facewarehouse: A 3d facial expression database for
visual computing. IEEE Trans. Vis. Comput. Graph.(TVCG) ,
2014. 3
[10] Kunyao Chen, Fei Yin, Bang Du, Baichuan Wu, and
Truong Q. Nguyen. Efficient registration for human surfaces
via isometric regularization on embedded deformation. IEEE
Trans. Vis. Comput. Graph.(TVCG) , 2022. 3
[11] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll.
Implicit functions in feature space for 3d shape reconstruc-
tion and completion. In IEEE Conf. Comput. Vis. Pattern
Recog.(CVPR) , 2020. 4
[12] Enric Corona, Albert Pumarola, Guillem Alenya, Ger-
ard Pons-Moll, and Francesc Moreno-Noguer. SMPLicit:
Topology-aware generative model for clothed people. In
IEEE Conf. Comput. Vis. Pattern Recog.(CVPR) , 2021. 2
[13] Enric Corona, Mihai Zanfir, Thiemo Alldieck, Ed-
uard Gabriel Bazavan, Andrei Zanfir, and Cristian Sminchis-
escu. Structured 3d features for reconstructing controllable
avatars. In IEEE Conf. Comput. Vis. Pattern Recog.(CVPR) ,
2023. 2
[14] Radek Danecek, Michael J. Black, and Timo Bolkart.
EMOCA: Emotion driven monocular face capture and ani-
mation. In IEEE Conf. Comput. Vis. Pattern Recog.(CVPR) ,
2022. 3, 5[15] Bailin Deng, Yuxin Yao, Roberto M. Dyke, and Juyong
Zhang. A survey of non-rigid 3d registration. Computer
Graphics Forum (Eurographics 2022 State-of-the-Art Re-
ports) , 2022. 3
[16] Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang
Zhou, and Stefanos Zafeiriou. Uv-gan: Adversarial facial
uv map completion for pose-invariant face recognition. In
IEEE Conf. Comput. Vis. Pattern Recog.(CVPR) , 2018. 4
[17] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde
Jia, and Xin Tong. Accurate 3d face reconstruction with
weakly-supervised learning: From single image to image
set. In IEEE Computer Vision and Pattern Recognition Work-
shops , 2019. 3
[18] Yao Feng, Haiwen Feng, Michael J. Black, and Timo
Bolkart. Learning an animatable detailed 3D face model
from in-the-wild images. ACM Trans. Graph.(TOG) , 2021.
3
[19] Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J Black,
and Timo Bolkart. Capturing and animation of body and
clothing from monocular video. In SIGGRAPH Asia 2022
Conference Proceedings , 2022. 2
[20] David A Field. Laplacian smoothing and delaunay triangula-
tions. Communications in applied numerical methods , 1988.
5
[21] Pascal J Frey and Houman Borouchaki. Surface mesh quality
evaluation. International journal for numerical methods in
engineering , 1999. 7
[22] Kaiwen Guo, Feng Xu, Yangang Wang, Yebin Liu, and
Qionghai Dai. Robust non-rigid motion tracking and surface
reconstruction using l0 regularization. In Int. Conf. Comput.
Vis.(ICCV) , 2015. 3, 7
[23] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang,
Young-Jae Park, and Hae-Gon Jeon. High-fidelity 3d human
digitization from single 2k resolution images. In IEEE Conf.
Comput. Vis. Pattern Recog.(CVPR) , 2023. 1, 2, 6, 8
[24] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and
Tony Tung. ARCH++: Animation-ready clothed human re-
construction revisited. In Int. Conf. Comput. Vis.(ICCV) ,
2021. 1, 2
[25] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Ji-
axiang Tang, Deng Cai, and Justus Thies. TeCH: Text-guided
Reconstruction of Lifelike Clothed Humans. In International
Conference on 3D Vision (3DV) , 2024. 2
[26] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and
Tony Tung. ARCH: Animatable reconstruction of clothed
humans. In IEEE Conf. Comput. Vis. Pattern Recog.(CVPR) ,
2020. 1, 2
[27] Huggingface. Stable-diffusion-inpainting, 2022. https://hug
gingface.co/runwayml/stable-diffusion-inpainting. 6
[28] Henrik Wann Jensen. Realistic image synthesis using photon
mapping . AK Peters/crc Press, 2001. 4
[29] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang
Liu, and Hujun Bao. BCNet: Learning body and cloth shape
from a single image. In Eur. Conf. Comput. Vis.(ECCV) ,
2020. 2
[30] Yucheol Jung, Hyomin Kim, Gyeongha Hwang, Seung-
Hwan Baek, and Seungyong Lee. Mesh density adapta-
150
tion for template-based shape reconstruction. In ACM SIG-
GRAPH 2023 Conference Proceedings , 2023. 3, 7
[31] Michael Kazhdan and Hugues Hoppe. Screened poisson sur-
face reconstruction. ACM Trans. Graph.(TOG) , 2013. 4
[32] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe.
Poisson surface reconstruction. In Proceedings of the fourth
Eurographics symposium on Geometry processing , 2006.
[33] Vladislav Kraevoy and Alla Sheffer. Template-based mesh
completion. In Symposium on Geometry Processing , 2005.
4
[34] John P Lewis, Matt Cordner, and Nickson Fong. Pose space
deformation: a unified approach to shape interpolation and
skeleton-driven deformation. In Proceedings of SIGGRAPH
2000 , 2000. 8
[35] Kun Li, Jingyu Yang, Yu-Kun Lai, and Daoliang Guo.
Robust non-rigid registration with reweighted position
and transformation sparsity. IEEE Trans. Vis. Comput.
Graph.(TVCG) , 2018. 3, 7
[36] Tianye Li, Timo Bolkart, Michael Black, Hao Li, and Javier
Romero. Learning a model of facial shape and expression
from 4d scans. ACM Trans. Graph.(TOG) , 2017. 3
[37] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and
Javier Romero. Learning a model of facial shape and ex-
pression from 4D scans. ACM Trans. Graph.(TOG) , 2017.
3
[38] Yuanqi Li, Jianwei Guo, Xinran Yang, Shun Liu, Jie Guo,
Xiaopeng Zhang, and Yanwen Guo. Deep point cloud sim-
plification for high-quality surface reconstruction. arXiv
preprint arXiv:2203.09088 , 2022. 7
[39] Tingting Liao, Xiaomei Zhang, Yuliang Xiu, Hongwei Yi,
Xudong Liu, Guo-Jun Qi, Yong Zhang, Xuan Wang, Xi-
angyu Zhu, and Zhen Lei. High-fidelity clothed avatar re-
construction from a single image. In IEEE Conf. Comput.
Vis. Pattern Recog.(CVPR) , 2023. 1, 2
[40] Peter Liepa. Filling holes in meshes. In Proceedings of the
2003 Eurographics/ACM SIGGRAPH symposium on Geom-
etry processing , 2003. 4
[41] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J. Black. SMPL: A skinned multi-
person linear model. ACM Trans. Graph.(TOG) , 2015. 2,
3
[42] William E Lorensen and Harvey E Cline. Marching cubes: A
high resolution 3d surface construction algorithm. Seminal
graphics: pioneering efforts that shaped the field , 1998. 4
[43] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,
Gerard Pons-Moll, Siyu Tang, and Michael J. Black. Learn-
ing to Dress 3D People in Generative Clothing. In IEEE
Conf. Comput. Vis. Pattern Recog.(CVPR) , 2020. 6, 7
[44] Aymen Mir, Thiemo Alldieck, and Gerard Pons-Moll. Learn-
ing to transfer texture from clothing images to 3d humans. In
IEEE Conf. Comput. Vis. Pattern Recog.(CVPR) , 2020. 2
[45] Baptiste Nicolet, Alec Jacobson, and Wenzel Jakob. Large
steps in inverse rendering of geometry. ACM Trans.
Graph.(TOG) , 2021. 3, 7
[46] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and
Michael J. Black. Expressive body capture: 3d hands, face,and body from a single image. In IEEE Conf. Comput. Vis.
Pattern Recog.(CVPR) , 2019. 2, 3
[47] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami
Romdhani, and Thomas Vetter. A 3d face model for pose
and illumination invariant face recognition. In 2009 Sixth
IEEE International Conference on Advanced Video and Sig-
nal Based Surveillance , 2009. 3
[48] Ken Perlin and Eric M Hoffert. Hypertexture. In Proceed-
ings of the 16th annual conference on Computer graphics
and interactive techniques , 1989. 4
[49] Leonid Pishchulin, Stefanie Wuhrer, Thomas Helten, Chris-
tian Theobalt, and Bernt Schiele. Building statistical shape
spaces for 3d human modeling. Pattern Recognition , 2017.
2
[50] Lingteng Qiu, Guanying Chen, Jiapeng Zhou, Mutian Xu,
Junle Wang, and Xiaoguang Han. Rec-mv: Reconstructing
3d dynamic cloth from monocular videos. In IEEE Conf.
Comput. Vis. Pattern Recog.(CVPR) , 2023. 2
[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In IEEE Conf. Comput.
Vis. Pattern Recog.(CVPR) , 2022. 6
[52] Javier Romero, Dimitrios Tzionas, and Michael J. Black.
Embodied hands: Modeling and capturing hands and bod-
ies together. ACM Trans. Graph.(TOG) , 2017. 3
[53] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InMedical Image Computing and Computer-Assisted Inter-
vention(MICCAI), 2015 , 2015. 5
[54] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. In Int. Conf. Comput. Vis.(ICCV) , 2019. 1, 2, 4, 6,
8
[55] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
Joo. PIFuHD: Multi-level pixel-aligned implicit function for
high-resolution 3d human digitization. In IEEE Conf. Com-
put. Vis. Pattern Recog.(CVPR) , 2020. 1, 2, 6, 8
[56] Jiaxiang Shang, Tianwei Shen, Shiwei Li, Lei Zhou, Ming-
min Zhen, Tian Fang, and Long Quan. Self-supervised
monocular 3d face reconstruction by occlusion-aware multi-
view geometry consistency. In Eur. Conf. Comput.
Vis.(ECCV) , 2020. 3
[57] Kaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Zarate,
Julien Valentin, Jie Song, and Otmar Hilliges. X-avatar:
Expressive human avatars. In Computer Vision and Pattern
Recognition (CVPR) , 2023. 1, 2
[58] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In Int. Conf. Learn. Repre-
sent., 2021. 7
[59] David Svitov, Dmitrii Gudkov, Renat Bashirov, and Victor
Lempitsky. Dinar: Diffusion inpainting of neural textures for
one-shot human avatars. In Int. Conf. Comput. Vis.(ICCV) ,
2023. 2, 6, 8
[60] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
Real-esrgan: Training real-world blind super-resolution with
pure synthetic data. In Int. Conf. Comput. Vis.(ICCV) , 2021.
6
151
[61] Donglai Xiang, Fabian Prada, Chenglei Wu, and Jessica
Hodgins. Monoclothcap: Towards temporally coherent
clothing capture from monocular rgb video. In IEEE Conf.
3D Vis.(3DV) , 2020. 1
[62] Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng
Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu.
Modeling clothing as a separate layer for an animatable hu-
man avatar. ACM Trans. Graph.(TOG) , 2021. 2
[63] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and
Michael J. Black. ICON: Implicit Clothed humans Ob-
tained from Normals. In IEEE Conf. Comput. Vis. Pattern
Recog.(CVPR) , 2022. 1, 2, 6, 7, 8
[64] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and
Michael J. Black. ECON: Explicit Clothed humans Opti-
mized via Normal integration. In IEEE Conf. Comput. Vis.
Pattern Recog.(CVPR) , 2023. 1, 2, 5, 6, 8
[65] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir,
William T. Freeman, Rahul Sukthankar, and Cristian Smin-
chisescu. GHUM & GHUML: Generative 3d human shape
and articulated pose models. In IEEE Conf. Comput. Vis.
Pattern Recog.(CVPR) , 2020. 2
[66] Jingyu Yang, Ke Li, Kun Li, and Yu-Kun Lai. Sparse non-
rigid registration of 3d shapes. Computer Graphics Forum ,
2015. 3
[67] Xueting Yang, Yihao Luo, Yuliang Xiu, Wei Wang, Hao Xu,
and Zhaoxin Fan. D-if: Uncertainty-aware human digitiza-
tion via implicit distribution field. In IEEE Conf. Comput.
Vis. Pattern Recog.(CVPR) , 2023. 1, 2
[68] Yuxin Yao, Bailin Deng, Weiwei Xu, and Juyong Zhang.
Quasi-newton solver for robust non-rigid registration. In
IEEE Conf. Comput. Vis. Pattern Recog.(CVPR) , 2020. 3,
7
[69] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-
hai Dai, and Yebin Liu. Function4d: Real-time human vol-
umetric capture from very sparse consumer rgbd sensors. In
IEEE Conf. Comput. Vis. Pattern Recog.(CVPR) , 2021. 6
[70] Zheng Zerong, Yu Tao, Liu Yebin, and Dai Qionghai.
PaMIR: Parametric model-conditioned implicit representa-
tion for image-based human reconstruction. IEEE Trans.
Pattern Anal. Mach. Intell.(TPAMI) , 2021. 1, 2, 6, 8
[71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In Int.
Conf. Comput. Vis.(ICCV) , 2023. 6
[72] Tianshu Zhang, Buzhen Huang, and Yangang Wang. Object-
occluded human shape and pose estimation from a sin-
gle color image. In IEEE Conf. Comput. Vis. Pattern
Recog.(CVPR) , 2020. 4
[73] Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang
Yang. Detailed human shape estimation from a single image
by hierarchical mesh deformation. In IEEE Conf. Comput.
Vis. Pattern Recog.(CVPR) , 2019. 2
[74] Hao Zhu, Xinxin Zuo, Haotian Yang, Sen Wang, Xun Cao,
and Ruigang Yang. Detailed avatar recovery from single im-
age. IEEE Trans. Pattern Anal. Mach. Intell.(TPAMI) , 2021.
2
[75] Heming Zhu, Lingteng Qiu, Yuda Qiu, and Xiaoguang Han.
Registering explicit to implicit: Towards high-fidelity gar-ment mesh reconstruction from single images. In IEEE Conf.
Comput. Vis. Pattern Recog.(CVPR) , 2022. 2
[76] Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and
Stan Z. Li. Face alignment across large poses: A 3d solution.
InIEEE Conf. Comput. Vis. Pattern Recog.(CVPR) , 2016. 3
152
