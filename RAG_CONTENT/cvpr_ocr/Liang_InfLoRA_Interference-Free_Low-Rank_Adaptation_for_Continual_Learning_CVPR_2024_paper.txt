InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
Yan-Shuo Liang and Wu-Jun Li*
National Key Laboratory for Novel Software Technology,
Department of Computer Science and Technology, Nanjing University, P. R. China
liangys@smail.nju.edu.cn ,liwujun@nju.edu.cn
Abstract
Continual learning requires the model to learn multi-
ple tasks sequentially. In continual learning, the model
should possess the ability to maintain its performance on
old tasks (stability) and the ability to adapt to new tasks
continuously (plasticity). Recently, parameter-efﬁcient ﬁne-
tuning (PEFT), which involves freezing a pre-trained model
and injecting a small number of learnable parameters to
adapt to downstream tasks, has gained increasing popular-
ity in continual learning. Although existing continual learn-
ing methods based on PEFT have demonstrated superior
performance compared to those not based on PEFT, most
of them do not consider how to eliminate the interference
of the new task on the old tasks, which inhibits the model
from making a good trade-off between stability and plastic-
ity. In this work, we propose a new PEFT method, called
interference-f ree lo w-rank a daptation (InfLoRA), for con-
tinual learning. InfLoRA injects a small number of param-
eters to reparameterize the pre-trained weights and shows
that ﬁne-tuning these injected parameters is equivalent to
ﬁne-tuning the pre-trained weights within a subspace. Fur-
thermore, InfLoRA designs this subspace to eliminate the
interference of the new task on the old tasks, making a good
trade-off between stability and plasticity. Experimental re-
sults show that InfLoRA outperforms existing state-of-the-
art continual learning methods on multiple datasets. Code
is available at https://github.com/liangyanshuo/
InfLoRA .
1. Introduction
Continual learning requires the model to learn multiple
tasks sequentially [33]. To achieve continual learning,
the model should possess two essential abilities, including
the ability to keep its performance on the old tasks (sta-
bility) and the ability to adapt to the new tasks continu-
ously (plasticity) [33]. Furthermore, two different scenar-
*Wu-Jun Li is the corresponding author.ios are often considered in continual learning, including
task-incremental scenario [32] and class-incremental sce-
nario [41]. Task-incremental scenario allows the model to
get task identities during inference. On the contrary, class-
incremental scenario does not allow the model to get task
identities during inference, making the model learn to dis-
tinguish all the classes across all the tasks.
Recently, parameter-efﬁcient ﬁne-tuning (PEFT) [15, 16,
18], which involves freezing a pre-trained model and in-
jecting a small number of learnable parameters to adapt
to downstream tasks, has gained increasing popularity in
continual learning [12, 38, 44], especially in the class-
incremental scenario. More speciﬁcally, existing contin-
ual learning methods based on PEFT [21, 43] inject the
learnable parameters into a pre-trained model using some
popular PEFT methods such as prompt-tuning [25] or low-
rank adaptation (LoRA) [16]. Subsequently, these methods
freeze the pre-trained weights and sequentially ﬁne-tune the
injected parameters on multiple tasks throughout the contin-
ual learning process.
Although continual learning methods based on PEFT
have demonstrated superior performance compared to those
not based on PEFT [44], most of them do not consider how
to eliminate the interference of the new task on the old tasks,
which inhibits the model from making a good trade-off
between stability and plasticity. Speciﬁcally, when learn-
ing a new task, existing continual learning methods based
on PEFT either reuse the previously learned parameters to
adapt to the new task [12, 44] or randomly expand some pa-
rameters ﬁrst and then adapt to the new task [38, 42, 43].
During this process, the interference of the new task on the
old tasks exists due to the shared parameters between new
and old tasks, which means ﬁne-tuning a pre-trained model
on a new task may interfere with the model’s performance
on the old tasks. As a result, it is hard for the model to make
a good trade-off between stability and plasticity.
In this work, we propose a new PEFT method, called
interference-f ree lo w-rank a daptation (InfLoRA), for con-
tinual learning. The contributions of this work are listed as
follows:
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23638
• InfLoRA injects a small number of parameters to repa-
rameterize the pre-trained weights and shows that ﬁne-
tuning these injected parameters is equivalent to ﬁne-
tuning the pre-trained weights within a subspace.
• InfLoRA designs this subspace to eliminate the interfer-
ence of the new task on the old tasks, making a good
trade-off between stability and plasticity.
• Experimental results show that InfLoRA outperforms ex-
isting state-of-the-art continual learning methods on mul-
tiple datasets.
2. Related Work and Preliminaries
2.1. Related Work
Parameter-Efﬁcient Fine-Tuning Parameter-efﬁcient
ﬁne-tuning (PEFT) methods freeze a pre-trained model and
inject a small number of learnable parameters to adapt to
downstream tasks. In this way, PEFT methods reduce the
inefﬁciency of full ﬁne-tuning methods which ﬁne-tune all
the parameters of a pre-trained model to learn downstream
tasks. For example, Adapter [15] adds small modules in
different layers of Transformers and only tunes these added
modules to adapt to downstream tasks. Prompt-tuning [25]
and Preﬁx-tuning [27] insert a set of learnable tokens into
the input of the Transformer layers and only tune these
tokens to adapt to downstream tasks. Low-rank adapta-
tion (LoRA) [16] reparameterizes the pre-trained weights
with low-rank branches and only tunes these branches to
adapt to the downstream tasks. Although these methods
tune much fewer learnable parameters than full ﬁne-tuning,
they always show comparable or even superior performance
compared with full ﬁne-tuning [11, 16, 31, 45]. Early
PEFT methods focus on natural language processing (NLP).
Recently, PEFT methods have also been proposed for
computer vision (CV). For example, visual prompt tun-
ing (VPT) [18] and AdapterFormer [6] apply prompt-tuning
and Adapter techniques to CV tasks, respectively. Both of
them exhibit comparable performance to full ﬁne-tuning.
Continual Learning Early continual learning was usually
considered in the context of learning from scratch. Three
types of continual learning methods are proposed, includ-
ing regularization-based methods [1, 20, 23, 46], memory-
based methods [2, 3, 7, 28, 39], and expansion-based meth-
ods [17, 26, 35]. Regularization-based methods employ a
penalty loss (regularization) to prevent important parame-
ters of old tasks from changing too much. Memory-based
methods maintain a memory buffer to store information
about old tasks. Expansion-based methods dynamically ex-
pand the model’s architecture for each new task.
Recently, with the advancements of pre-trained mod-
els [9, 10, 13], using pre-trained models for continual learn-
ing has gained increasing popularity. Some continual learn-
ing methods fully ﬁne-tune the pre-trained models [4, 48],which has been shown to be inefﬁcient. Other methods ex-
plore PEFT methods in continual learning. For instance,
some existing continual learning methods [21, 38, 43, 44]
introduce prompt-tuning in continual learning, achieving
much higher performance than previous methods that learn
from scratch, especially in the class-incremental scenario.
The method in [12] introduces a framework in continual
learning that can be combined with many existing PEFT
methods, such as prompt-tuning, LoRA and Adapter. How-
ever, all these methods do not consider how to eliminate the
interference of the new task on the old tasks, which inhibits
the model from making a good trade-off between stability
and plasticity.
2.2. Preliminaries
We ﬁrst introduce low-rank adaptation (LoRA) [16], a pop-
ular PEFT method related to our method. Then, we give the
problem deﬁnition for continual learning.
Low-Rank Adaptation LoRA [16] is one of the most pop-
ular PEFT methods. It assumes that the changes of pa-
rameters lie in a low-rank space when the model is fully
ﬁne-tuned on a downstream task. Speciﬁcally, for a linear
layer with the input dimension dIand the output dimension
dO, we represent its weight with WdOdI. Then, LoRA
reparametrizes the pre-trained weight Wby expanding a
branch with two matrices, A2RdOrandB2RrdI.
Typically,ris much smaller than the input dimension dI
and output dimension dO, makingAa dimensionality in-
creasing matrix and Ba dimensionality reduction matrix.
Finally, LoRA modiﬁes the forward propagation in this lin-
ear layer ase=Wh +ABh . Here,handedenote the
input and output of this layer, respectively. LoRA initializes
Aas0and initializes Busing Gaussian distribution. Dur-
ing the learning of the downstream tasks, LoRA freezes the
pre-trained weight Wand only ﬁne-tunes the parameters A
andB.
Problem Deﬁnition In continual learning, there is a se-
quence of tasks with different distributions. We deﬁne the
task sequence asD=fD1;:::;DTg, where the t-th task
Dt=f(xi;t;yi;t)gnt
i=1. Here,xi;tdenotes an input sample
andyi;tdenotes its label. The objective of continual learn-
ing is to train a model sequentially on these tasks and ensure
that the model performs well on all of them.
We follow existing continual learning methods [43, 44]
based on PEFT and assume the model is a pre-trained
Vision Transformer (ViT) [10]. Speciﬁcally, assume the
model ish(f())whereh()is the classiﬁer with pa-
rameters andf()is the pre-trained ViT backbone with
pre-trained parameters . Similar to existing work [43],
our focus is primarily on the class-incremental scenario,
where task identities are unknown during inference. Fur-
thermore, we concentrate on the exemplar-free setting [43,
50], where no historical data can be fetched for rehearsal.
23639
Pre-trained Weights !∈ℝ$%×$'()
(a) Architecture(b) Designing Dimensionality Reduc;on Matrix *+,
❄
❄-: Element-Wise Addi;on
❄: Frozen
": Learnable(+.)(+
❄
"……ℳ+ℳ+01+
ℳ+01+ℳ+0∩1+: Gradient space of new tasks (1+): Gradient space of old tasks (ℳ+): Residual gradient space (ℳ+0)*)
❄*3.)
❄*+
❄*+Figure 1. (a) The architecture of our InfLoRA in a certain linear layer of a Transformer. During the learning of the t-th task, the pre-trained
weight and all the old branches are frozen, and only Atis ﬁne-tuned. (b) The pipeline of designing dimensionality reduction matrix Bt.
3. Methodology
Figure 1 (a) illustrates the architecture of our InfLoRA
within a linear layer. Before learning the t-th new task, our
InfLoRA expands a LoRA-like branch, which includes a
dimensionality reduction matrix Bt2RrdIand a dimen-
sionality increasing matrix At2RdOr. Then, the forward
propagation of this linear layer is modiﬁed as
e=Wh +tX
j=1AjBjh=Wt 1h+AtBth=Wth:
(1)
Here,Wt=Wt 1+AtBt=W+Pt
i=1AiBi. Similar to
LoRA, our InfLoRA also initializes dimensionality increas-
ing matrixAtas0. However, different from LoRA, which
employs Gaussian distribution to initialize the dimension-
ality reduction matrix B, our InfLoRA designs the dimen-
sionality reduction matrix Btbefore learning the t-th task.
During the learning of the t-th task, InfLoRA ﬁne-tunes At
to learn the new task while keeping the pre-trained weight
W, all the old branches and the matrix Btfrozen. After
learning the t-th tasks, for any given test sample belong-
ing to the learned tasks, the model uses Wtand (1) to infer
its label. This design ensures that our method is compatible
with the class-incremental scenario where task identities are
unknown during inference.
In the following subsections, we ﬁrst build the relation-
ship between our InfLoRA and the method that ﬁne-tunes
the pre-trained weight. Speciﬁcally, we show that ﬁne-
tuning parameters Atis equivalent to ﬁne-tuning the pre-
trained weights Wwithin a subspace spanned by the rows
ofBt. Note thatBtis designed before learning the t-th
task, making this subspace pre-designed. Then, building
upon this relationship, we introduce how our InfLoRA de-
signs this subspace to eliminate the interference of the newtask on the old tasks and make a good trade-off between
stability and plasticity.
3.1. Relationship between InfLoRA and Fine-
Tuning the Pre-Trained Weight
When thet-th task arrives and our method has expanded a
new branch, the forward propagation in this layer can be
represented by (1). At this time, we can prove the following
proposition:
Proposition 1. When learning the t-th task with forward
propagation represented by (1), ﬁne-tuning Atis equivalent
to ﬁne-tuning the pre-trained weight Wwithin the subspace
spanfbt
1;:::;bt
rg. Here,bt
i(1ir) denotes the i-th row
vector ofBt.
Proof. When tuning the pre-trained weight Wto learn the
t-th task, we can compute the gradient of Wbased on the
chain rule:
@L
@W=@L
@e@e
@W=@L
@ehT: (2)
Here,Ldenotes the loss function. At this time, the change
ofWcan be denoted as W= @L
@W, whereis the
learning rate. Then, we can compute the change of the com-
posed matrixWt=W+Pt
j=1AjBj:
WWt=[W+ W+tX
j=1AjBj] (W+tX
j=1AjBj)
=W= @L
@Wt= @L
@ehT(3)
Here, we use WWtto denote the change of the composed
matrixWtcausing by the change of W.
23640
Similarly, when tuning the expanded weight At, we can
get the gradient of Atbased on the chain rule:
@L
@At=@L
@e@e
@At=@L
@ehTBT
t: (4)
At this time, the change of Atcan be denoted as At=
 @L
@At. Then, we can compute the change of the composed
matrixWt=Wt 1+AtBt:
AtWt=[Wt 1+ (At+ At)Bt] (Wt 1+AtBt)
=AtBt= @L
@AtBt= @L
@ehTBT
tBt
=WWtBT
tBt (5)
Here, we use AtWtto denote the change of the com-
posed matrixWtcausing by the change of At. The fourth
equation in (5) holds because of (4). The ﬁfth equation
in (5) holds because of (2). (5) shows that AtWtis equal
toWWtmultiplying a projection matrix BT
tBt. Since
BT
tBtprojects each row vector of WWtinto the sub-
space spanfbt
1;:::;bt
rg, Proposition 1 holds.
Proposition 1 has demonstrated that using our InfLoRA
to train the model is equivalent to directly ﬁne-
tuning the pre-trained weight Wwithin the subspace
spanfbt
1;:::;bt
rg. Therefore, before learning the t-th task,
we can design matrix Btsuch that learning the t-th task
in the subspace spanfbt
1;:::;bt
rgwill not interfere with the
performance of the model on the old tasks.
3.2. Eliminating the Interference of the New Task
on the Old Tasks
We ﬁrst introduce the desired characteristics that InfLoRA
aims to let the subspace spanfbt
1;:::;bt
rghave. With these
characteristics, InfLoRA can eliminate the interference of
the new task on the old tasks and make a good trade-off
between stability and plasticity. Then, we introduce how to
design dimensionality reduction matrix Btso that subspace
spanfbt
1;:::;bt
rghas these characteristics.
3.2.1 Desired Characteristics
First, InfLoRA aims to make the subspace spanfbt
1;:::;bt
rg
orthogonal to the gradients of all the old tasks. In this way,
according to Proposition 1, the update of InfLoRA, which
can be represented as AtWt, will also be orthogonal to
the gradient of the old tasks. Note that the idea of making
the update for the new task orthogonal to the gradient of
the old tasks to eliminate the interference of the new task
on the old tasks has been proposed in many existing contin-
ual learning methods [30, 36]. However, all these existing
methods are designed for continual learning from scratch,
involving updating all parameters of the model, which is in-
compatible with the setting in PEFT. On the contrary, ourmethod is a PEFT method, which only tunes the parameters
inAt.
Besides eliminating the interference of new tasks on
old tasks, our InfLoRA further makes the subspace
spanfbt
1;:::;bt
rglie in a subspace that the gradient of the
new task lies in to make a good trade-off between stability
and plasticity. Speciﬁcally, existing work [19] has shown
that during ﬁne-tuning, the weight increments of pre-trained
ViT exhibit redundancy in terms of weight rank. There-
fore, the gradients of the new task lie in a low-dimensional
subspace. Our method makes spanfbt
1;:::;bt
rgnot only or-
thogonal to the gradient of the old tasks but also lie in the
subspace in which the gradients of the new task tlie. By
doing so, our method makes the model’s focus on the new
task when eliminating the interference of the new task on
the old tasks, thereby making a good trade-off between sta-
bility and plasticity. Section 4.2 veriﬁes the effectiveness of
these two characteristics.
3.2.2 Designing Dimensionality Reduction Matrix
InfLoRA ﬁrst approximates the gradient space of the new
task and old tasks. Here, we use Ntto represent the gradient
space of the new task approximated by InfLoRA. Similarly,
we useMtto represent the gradient space of previous t 1
old tasks approximated by InfLoRA. We also use M?
tto
denote the residual gradient space, which is orthogonal to
the spaceMt. Then, in order to meet the characteristics
described in Section 3.2.1, InfLoRA ensures that each row
ofBtlies inNt\M?
t. In other words, InfLoRA makes
spanfbt
1;:::;bt
rgN t\M?
t.
Existing works [29, 36] have shown that the gradient up-
date of the linear layer lies in the span of the inputs. Please
refer to supplementary material for a detailed explanation of
this proposition. Therefore, InfLoRA uses the input matrix
of the new task tto approximate the gradient space of the
new task. Speciﬁcally, InfLoRA computes the input matrix
Ht= [ht
1;:::;ht
n], with each column of Htrepresenting an
input vector of the t-th task. Then, InfLoRA considers Nt
as the subspace spanned by the columns of matrix Ht.
However, InfLoRA cannot use the input matrix of the
old tasks to approximate the gradient space of the old tasks
since the data from the old tasks is not available when the
model learns the new tasks. Instead, existing methods such
as gradient projection memory (GPM) [36] and dual gra-
dient projection memory (DualGPM) [29] can learn a ma-
trix to preserve information about the gradients of the old
tasks. InfLoRA incorporates DualGPM to preserve gra-
dient information. With the assistance of DualGPM, the
model can learn either a matrix Mt2RdIktor a matrix
M?
t2RdI(dI kt). Here, the columns of Mtcontribute
to the orthonormal bases of Mtand the columns of M?
t
contribute to the orthonormal bases of M?
t.ktdenotes the
23641
dimension ofMt. For detailed information of how Dual-
GPM maintains orthonormal bases MtorM?
t, please refer
to supplementary material or the original paper [29].
After approximating the gradient space of the new task
and old tasks, InfLoRA gets the component of Ntwhich
lies inM?
t. Speciﬁcally, when the model maintains Mt,
InfLoRA performs the operation
^Ht=Ht MtMT
tHt: (6)
Similarly, when the model maintains M?
t, InfLoRA per-
forms the operation
^Ht=M?
t(M?
t)THt: (7)
Note that when t= 1,Mtis a null space and ^Ht=Ht.
Obviously, each column of ^Htlies inNt\M?
t. How-
ever, since (^Ht)T2RndIandBt2RrdIhave differ-
ent shapes, InfLoRA can not directly deﬁne Btas(^Ht)T.
Note thatnr, InfLoRA uses the principal components
of(^Ht)Tto setBt. Speciﬁcally, singular value decom-
posotion (SVD) is performed on (^Ht)T=VttUt. Then,
InfLoRA designs Btby
Bt= (Ut)r: (8)
Here, (Ut)rdenotes the rows of Utcorresponding to the
top-rsingular values. Figure 1 (b) illustrates the pipeline of
designing matrix Bt.
Note that DualGPM expands subspace Mtand reduces
subspaceM?
twhen the number of tasks increases. Since
InfLoRA constrains the update of the model within the sub-
spaceNt\M?
tM?
t, the space for learning the new
task reduces when the number of tasks increases. How-
ever, by adjusting the approximation error of the gradient
for the old tasks, DualGPM can expand Mtslowly and re-
duceM?
tslowly. Therefore, the constraints imposed by
InfLoRA do not excessively affect the model’s learning of
new tasks. Please refer to supplementary material for a de-
tailed explanation.
3.3. Whole Process of InfLoRA
Algorithm 1 outlines the whole process of InfLoRA in con-
tinual learning. When the t-th new task arrives, InfLoRA
ﬁrst designsBtthrough (8) and expands a new branch.
Then, InfLoRA learns the t-th task by ﬁne-tuning the newly
expanded branch. Please note that, based on empirical ﬁnd-
ings from existing methods [12, 38], we employ the local
cross-entropy (CE) loss as the learning objective, as it usu-
ally performs better than the global CE loss in continual
learning methods based on PEFT. The local CE is the CE
loss constrained to the classes of the current new task, which
can be denoted as
L(Dt) =1
jDtjX
(x;y)2DtLce(mask(h(f(x)));y):(9)Algorithm 1 InfLoRA for Continual Learning
1:Input: The data of different tasks fDtgT
t=1, a pre-trained ViT
model f().
2:Output: Network f()with learned parameters Wt.
3:fortin1 :Tdo
4: DesignBtthrough (8);
5: Expand a new branch for the t-th task;
6: forBtsampled fromDtdo
7: Compute the lossL(f(Bt))through (9) and update the
parameters;
8: end for
9: Preserve the information about the gradient of the t-th task
through DualGPM;
10:end for
Here, mask()is a function that ﬁlters out the logits of the
old classes andLcedenotes the standard CE loss. After
learning the t-th new task, InfLoRA follows the DualGPM
to preserve the information about the gradient of the t-th
task.
Note that the branch corresponding to the t-th task will
be frozen once the model has learned the t-th task. Since
the expanded branches are linear transformations, we can
integrate the old branches into the pre-trained weight to re-
duce the expanded parameters. Speciﬁcally, after learning
the ﬁrst task, InfLoRA integrates the ﬁrst branch into the
pre-trained weight and obtains the weight W1=W+
A1B1. Before learning the t-th new task ( t>1), InfLoRA
maintains the weight Wt 1. After learning the t-th task,
InfLoRA integrates the t-th branch into Wt 1and obtains
Wt=Wt 1+AtBt. In this way, the parameters in Atand
Btdo not need to be maintained in the learning of subse-
quent tasks. Therefore, during the whole learning process,
the number of parameters expanded by InfLoRA equals the
number of parameters in a single branch. Since a single
branch contains (dI+dO)rparameters, the number of pa-
rameters expanded by InfLoRA is (dI+dO)rall the time.
4. Experiments
4.1. Experimental Settings
Datasets and Evaluation Metric Similar to existing con-
tinual learning methods [12, 44] based on PEFT, we use
ImageNet-R [14], CIFAR100 [24], and DomainNet [34]
to train and evaluate the models. Imagenet-R is gener-
ated through artistic processing of 200 classes from Ima-
geNet [8]. This dataset is introduced to continual learning
by existing work [43] and has become a standard bench-
mark for continual learning methods based on PEFT. CI-
FAR100 is a dataset commonly used in existing continual
learning works. DomainNet contains 345 classes and is
introduced by some existing works [38, 42] for continual
learning. Following existing continual learning work [38],
23642
Table 1. Results (%) on ImageNet-R. Results are included for 5 tasks, 10 tasks, and 20 tasks. We report results averaged over 5 trials.
Tasks 5 10 20
Method ACC 5(")ACC 5(")ACC 10(")ACC 10(")ACC 20(")ACC 20(")
joint 81:140:34 - 81:140:34 - 81:140:34 -
sequential 58:741:28 72:910:28 46:071:15 62:910:68 34:620:85 51:151:50
L2P [44] 64:130:78 68:660:41 62:540:24 67:980:27 57:920:28 64:570:29
DualPrompt [43] 67:880:17 71:160:31 65:410:52 69:390:43 61:000:72 65:800:67
CODA-P [38] 73:090:21 76:910:21 71:470:35 75:820:29 67:280:30 72:340:17
C-LoRA [37] 75:850:31 78:850:34 71:890:45 75:330:28 65:710:60 70:630:85
LAE [12] 73:840:14 77:290:45 71:700:39 76:710:10 66:980:35 73:720:05
InfLoRA-b5 75:280:01 78:950:08 74:130:18 78:540:14 68:410:29 74:000:50
InfLoRA 77.520.37 82.010.12 75.650.14 80.820.24 71.010.45 77.280.45
we split ImageNet-R into 5, 10, and 20 tasks, with each task
containing 40, 20, and 10 classes. We split CIFAR100 into
10 tasks, and each task constrains 10 classes. We split Do-
mainNet into 5 tasks, and each task contains 69 classes.
Following existing continual learning methods [12, 44],
we evaluate the performance of the model through two pop-
ular metrics, including the ﬁnal accuracy ACC Tand the
averaged accuracy ACC T=1
TPT
i=1ACC i, whereTde-
notes the total number of tasks and ACC iis deﬁned as
ACC i=1
iiX
j=1ai;j: (10)
Here,ai;jdenotes the accuracy of the j-th task once the
model has learned the i-th task.
Baselines We compare our InfLoRA with state-of-the-art
continual learning methods based on PEFT, including learn
to prompt (L2P) [44], DualPrompt [43], continual decom-
posed attention-based prompt (CODA-P) [38], learning ac-
cumulation ensemble (LAE) [12], continual low-rank adap-
tation (C-LoRA) [37]. For LAE, we implement it with
LoRA [16]. Following existing works [12, 38], we also in-
clude two methods without continual learning, joint andse-
quential , in the comparison. Here, joint denotes the method
that learns all the tasks jointly, while sequential denotes the
method that learns all the tasks sequentially without any op-
eration to overcome the forgetting of the model. The accu-
racy of joint can be treated as the accuracy upper-bound and
the accuracy of sequential can be treated as the accuracy
lower-bound.
Architecture and Training Details We follow existing
works [12, 43] to perform experiments. Speciﬁcally, we
use the ViT-B/16 backbone [10] supervised pre-trained on
ImageNet 21K as the pre-trained model.
For all the methods, we follow existing works [12, 38,
44] and use the Adam [22] optimizer with running averages
of gradient and its square ( 1= 0:9,2= 0:999). Each
task is trained for 50 epochs on ImageNet-R, 20 epochs on
CIFAR100 and 5 epochs on DomainNet. The batch sizeis set to 128 for all the experiments. Since our InfLoRA
shares a similar architecture to LoRA, we follow existing
work [12] and insert the architecture of our InfLoRA in the
key and value of the attention module. Furthermore, ex-
isting method DualPrompt [43] treats the inserted blocks
as hyperparameters and searches for the best positions for
their prompts. On the contrary, we insert the architecture of
InfLoRA for all the Transformer blocks to avoid searching.
We also implement a variant of our method, which inserts
the bottom 5 Transformer blocks like existing methods Du-
alPrompt and CODA-P. We call this variant InfLoRA-b5.
As for the hyperparameter r, we determine its value through
a grid search on a validation dataset.
4.2. Experimental Results
Accuracy Table 1 shows the results of different methods
on ImageNet-R with a different number of tasks. Table 2
shows the results of different methods on CIFAR100 and
DomainNet. We can ﬁnd that our methods InfLoRA and
InfLoRA-b5 outperform existing continual learning meth-
ods.
Figure 2 shows the variation of the accuracy of different
continual learning methods on ImageNet-R and CIFAR100.
We can ﬁnd that our method outperforms existing methods
not only at the end of the learning but also throughout the
whole learning process. This indicates that our InfLoRA
eliminates the interference of the new task on the old tasks
and thus the accuracy of our method decreases slower com-
pared to other methods.
Analysis of Expanded Parameters Figure 3 shows the
number of expanded parameters and the accuracy of dif-
ferent methods on ImageNet-R and CIFAR100. For L2P,
DualPrompt and CODA-P, their expanded parameters are
included in the added prompts and corresponding key. For
LAE, its expanded parameters are the inserted LoRA mod-
ules and an additional copy. For C-LoRA, its expanded pa-
rameters are inserted LoRA modules. For our method, the
expanded parameters are BtandAt. The details of comput-
ing the number of expanded parameters for different meth-
23643
Table 2. Results (%) on CIFAR100 and DomainNet. We report results over 5 trials.
Tasks CIFAR100 DomainNet
Method ACC 10(")ACC 10(")ACC 5(")ACC 5(")
joint 91:920:05 - 77:720:04 -
sequential 62:183:59 80:420:23 53:441:21 69:090:33
L2P [44] 82:480:20 87:640:25 70:160:05 75:600:03
DualPrompt [43] 84:420:30 90:060:07 72:140:05 77:710:06
CODA-P [38] 86:620:11 91:080:28 73:230:13 78:720:07
C-LoRA [37] 82:970:47 88:810:34 69:340:13 75:250:11
LAE [12] 84:150:10 89:840:03 66:850:40 75:010:17
InfLoRA-b5 87.060.25 91:590:13 73:260:50 78:820:34
InfLoRA 86:510:73 91.700.32 74.530.23 79.570.57
1 3 5
Number of Tasks5060708090Accuracy (%)
(a) ImageNet-R (5 Tasks)L2P
DualPrompt
CODA-P
LAEC-LoRA
InfLoRA-b5
InfLoRA
1 5 10
Number of Tasks60708090100Accuracy (%)
(b) ImageNet-R (10 Tasks)L2P
DualPrompt
CODA-P
LAEC-LoRA
InfLoRA-b5
InfLoRA
1 10 20
Number of Tasks60708090100Accuracy (%)
(c) ImageNet-R (20 Tasks)L2P
DualPrompt
CODA-P
LAEC-LoRA
InfLoRA-b5
InfLoRA
1 5 10
Number of Tasks859095100Accuracy (%)
(d) CIFAR100L2P
DualPrompt
CODA-P
LAEC-LoRA
InfLoRA-b5
InfLoRA
Figure 2. Variation of the performance of different methods during the learning of ImageNet-R and CIFAR100.
ods are given in supplementary material. We can ﬁnd that
CODA-P and C-LoRA expand much more parameters than
other methods. Furthermore, our methods InfLoRA and
InfLoRA-b5 expand comparable parameters to L2P, Dual-
Prompt and LAE but perform better than these methods.
Ablation Study We perform experiment to verify the effec-
tiveness of designing dimensionality reduction matrix Bt
by (8). Speciﬁcally, we explore three different variants for
designingBt. The ﬁrst variant designs Btrandomly us-
ing Gaussian distribution. We call this variant ‘Random
!Bt’. The second variant discards the operation in (6)
or (7) and directly sets ^Ht=Ht. Through this way, this
variant ensures that each row of Btlies inNtwhile ig-
noringM?
t. We call this variant ‘ Nt!Bt’. The third
variant does not compute the input matrix but initializes Ht
using a Gaussian distribution before applying the operation
in (6) or (7). In this way, this variant ensures that each row
ofBtlies inM?
twhile ignoringNt. We call this variant
‘M?
t!Bt’. Since our method focuses both M?
tandNt,
we useNt\M?
t!Btto represent our method.
Table 3 shows the results of our method and its variants.
We can ﬁnd that all these variants fail to perform as well
as our method. To further demonstrate the performance of
different variants, we show the relative accuracy of differ-
ent tasks after the model learns them all in Figure 4. Here,
relative accuracy is the accuracy of different variants mi-
nus the accuracy of our InfLoRA. Note that the last task is
the new task, and the other tasks are old tasks in Figure 4.
As we can see, ‘Random !Bt’ and ‘Nt!Bt’ outper-
0 2 4
Expanded Param (M)607080ACC 10(%)
(a) ImageNet-R (10 Tasks)L2P
DualPrompt
CODA-P
LAEC-LoRA
InfLoRA-b5
InfLoRA
0 2 4
Expanded Param (M)8590ACC 10(%)
(b) CIFAR100L2P
DualPrompt
CODA-P
LAEC-LoRA
InfLoRA-b5
InfLoRAFigure 3. Variation of the performance of different methods during
the learning of ImageNet-R and CIFAR100.
form ‘M?
t!Bt’ on the new task but shows much lower
accuracy than ‘M?
t!Bt’ and our InfLoRA on the old
tasks. This means these two variants fail to eliminate the in-
ference of the new task on the old tasks, making the model
suffer from low stability. On the contrary, ‘ M?
t!Bt’
shows the lowest performance on the new task. This means
‘M?
t!Bt’ ignores the plasticity of the model. Our
method outperforms all the variants on most of the tasks.
This shows that our method can eliminate the interference
of the new task on the old tasks and make a better trade-off
between stability and plasticity than these variants.
Varying the Pre-Trained Model We also follow the exist-
ing method [40] and perform experiments using a ViT-B/16
pre-trained with two different self-supervised methods, in-
cluding DINO [5] and iBOT [49]. All experimental settings,
except for the choice of the pre-trained model, are kept con-
sistent with the details outlined in Section 4.1.
23644
Table 3. Results of different variants on ImageNet-R with a different number of tasks.
Tasks 5 10 20
ACC 5(")ACC 5(")ACC 10(")ACC 10(")ACC 20(")ACC 20(")
Random!Bt 72:490:38 79:400:29 67:380:41 76:620:06 56:170:29 69:240:35
Nt!Bt 67:010:11 76:090:04 57:910:30 70:230:59 40:730:29 59:680:52
M?
t!Bt 75:940:53 80:690:27 74:610:62 79:670:27 68:790:42 75:740:26
Nt\M?
t!Bt(InfLoRA) 77.520.37 82.010.12 75.650.14 80.820.24 71.010.45 77.280.45
1 4 7 10
Tasks−30−20−100Relative Accuracy (%)
(a) ImageNet-R (10 Tasks)Random→Bt
M⊥
t→BtNt→Bt
1 10 19
Tasks−40−200Relative Accuracy (%)
(b) ImageNet-R (20 Tasks)Random→Bt
M⊥
t→BtNt→Bt
Figure 4. Relative accuracy of different tasks. Relative accuracy is
the accuracy of different variants minus the accuracy of InfLoRA.
Table 4. Results (%) of different methods on ImageNet-R (10
tasks) using various self-supervised pre-trained models. Here,
DINO-1k and iBOT-1k indicate that the ViT is pre-trained on
ImageNet-1k using these respective methods.
Method ACC 10(")ACC 10(")
DINO-1kL2P [44] 56:710:12 63:590:21
DualPrompt [43] 60:230:42 66:570:25
CODA-P [38] 64:020:68 71:500:42
C-LoRA [37] 63:070:36 68:090:41
LAE [12] 61:030:27 69:890:15
InfLoRA-b5 66:160:14 73:010:17
InfLoRA 68.310.28 76.150.05
iBOT-1kL2P [44] 60:800:35 66:580:28
DualPrompt [43] 63:780:38 68:880:16
CODA-P [38] 68:020:48 74:280:47
C-LoRA [37] 68:600:07 73:470:28
LAE [12] 64:140:29 72:590:22
InfLoRA-b5 69:720:44 76:110:13
InfLoRA 71.840.09 78.290.09
Table 4 shows the results of different methods on
ImageNet-R when using various pre-trained models. Com-
paring these results to those in Table 1, we can ﬁnd that the
performance of all methods utilizing self-supervised pre-
trained models is lower than the performance of the cor-
responding methods using supervised pre-trained models.
However, our methods still outperform all other methods.
Combining with Classiﬁer Alignment Slow learner with
classiﬁer alignment (SLCA) [47] utilizes feature statistics to
align classiﬁers, demonstrating superior performance com-
pared to methods without aligned classiﬁers. Our InfLoRA
can be combined with classiﬁer alignment (CA) to get bet-
ter performance. Speciﬁcally, after learning the t-th taskTable 5. Results (%) of different methods on ImageNet-R (10
tasks) and CIFAR100 using classiﬁer alignment (CA) technique.
Method ACC 10(")ACC 10(")
CIFAR100SLCA [47] 91:060:24 93:650:19
InfLoRA+CA 91.590.08 94.390.05
ImageNet-RSLCA [47] 77:340:25 81:350:16
InfLoRA+CA 79.780.25 83.380.19
with parameters AtandBtand loss (9), we collect features
Ft=fri;tgnt
i=1of thet-th task. Here, ri;t=f(xi;t)de-
notes the features extracted by backbone f(). Then, mean
and covariance of features for each class are computed and
saved. After that, for each class cthe model has seen during
continual learning, Ssamples are sampled from Gaussian
distributionN(c;c). Here,cand covariance cde-
note the mean and covariance of the class c. Finally, we
align the classiﬁer using standard cross-entropy and these
samples. The details of this experiment are given in supple-
mentary material.
Table 5 shows that our method InfLoRA+CA outper-
forms SLCA. Note that SLCA tunes all the parameters of
the model while our method InfLoRA only tunes the pa-
rameters inAt. Therefore, our InfLoRA+CA is much more
efﬁcient than SLCA.
5. Conclusion
In this work, we propose a new method, called interference-
free low-rank adaptation (InfLoRA), for continual learn-
ing. InfLoRA injects a small number of parameters to
reparameterize the pre-trained weights and shows that ﬁne-
tuning these injected parameters is equivalent to ﬁne-tuning
the pre-trained weights within a subspace. Furthermore,
InfLoRA designs this subspace to eliminate the interference
of the new task on the old tasks, making a good trade-off
between stability and plasticity. Experimental results show
that InfLoRA outperforms existing state-of-the-art contin-
ual learning methods on multiple datasets.
Acknowledgment
This work is supported by NSFC (No.62192783), Na-
tional Key R&D Program of China (No.2020YFA0713901),
and Fundamental Research Funds for the Central Universi-
ties (No.020214380108).
23645
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Proceedings of
the European Conference on Computer Vision , pages 139–
154, 2018. 2
[2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Lau-
rent Charlin, Massimo Caccia, Min Lin, and Lucas Page-
Caccia. Online continual learning with maximal interfered
retrieval. In Advances in Neural Information Processing Sys-
tems, pages 11849–11860, 2019. 2
[3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. In Advances in Neural Information Processing Sys-
tems, pages 11816–11825, 2019. 2
[4] Matteo Boschini, Lorenzo Bonicelli, Angelo Porrello, Gio-
vanni Bellitto, Matteo Pennisi, Simone Palazzo, Concetto
Spampinato, and Simone Calderara. Transfer without forget-
ting. In Proceedings of the European Conference on Com-
puter Vision , pages 692–709, 2022. 2
[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 9650–9660, 2021. 7
[6] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapt-
ing vision transformers for scalable visual recognition. Ad-
vances in Neural Information Processing Systems , pages
16664–16678, 2022. 2
[7] Aristotelis Chrysakis and Marie-Francine Moens. Online
continual learning from imbalanced data. In Proceedings of
the International Conference on Machine Learning , pages
1952–1961, 2020. 2
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 248–255,
2009. 5
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the
Conference of the North American Chapter of the Associa-
tion for Computational Linguistics , pages 4171–4186, 2019.
2
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In International Con-
ference on Learning Representations , 2021. 2, 6
[11] Chin-Lun Fu, Zih-Ching Chen, Yun-Ru Lee, and Hung-Yi
Lee. Adapterbias: Parameter-efﬁcient token-dependent rep-
resentation shift for adapters in nlp tasks. In Findings of
the Association for Computational Linguistics , pages 2608–
2621, 2022. 2[12] Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang,
Bernard Ghanem, and Jian Zhang. A uniﬁed continual learn-
ing framework with general parameter-efﬁcient tuning. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 11449–11459, 2023. 1, 2, 5, 6, 7, 8
[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16000–
16009, 2022. 2
[14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, et al. The many faces of robust-
ness: A critical analysis of out-of-distribution generalization.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 8340–8349, 2021. 5
[15] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer
learning for nlp. In Proceedings of the International Con-
ference on Machine Learning , pages 2790–2799, 2019. 1,
2
[16] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-
rank adaptation of large language models. In International
Conference on Learning Representations , 2022. 1, 2, 6
[17] Steven C. Y . Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-
Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compact-
ing, picking and growing for unforgetting continual learn-
ing. In Advances in Neural Information Processing Systems ,
pages 13647–13657, 2019. 2
[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In Proceedings of the European Confer-
ence on Computer Vision , pages 709–727, 2022. 1, 2
[19] Shibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for
lightweight adaptation on vision transformer. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence , pages
1060–1068, 2023. 4
[20] Sangwon Jung, Hongjoon Ahn, Sungmin Cha, and Taesup
Moon. Continual learning with node-importance based adap-
tive group sparse regularization. Advances in Neural Infor-
mation Processing Systems , pages 3647–3658, 2020. 2
[21] Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem,
Luc Van Gool, Didier Stricker, Federico Tombari, and
Muhammad Zeshan Afzal. Introducing language guidance
in prompt-based continual learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 11463–11473, 2023. 1, 2
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[23] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proceedings of the National Academy of Sci-
ences , pages 3521–3526, 2017. 2
23646
[24] A Krizhevsky. Learning multiple layers of features from tiny
images. Master’s thesis, University of Tront , 2009. 5
[25] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efﬁcient prompt tuning. In Proceed-
ings of the Conference on Empirical Methods in Natural
Language Processing , pages 3045–3059, 2021. 1, 2
[26] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and
Caiming Xiong. Learn to grow: A continual structure learn-
ing framework for overcoming catastrophic forgetting. In
Proceedings of the International Conference on Machine
Learning , pages 3925–3934, 2019. 2
[27] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing
continuous prompts for generation. In Proceedings of the
Annual Meeting of the Association for Computational Lin-
guistics , pages 4582–4597, 2021. 2
[28] Yan-Shuo Liang and Wu-Jun Li. Loss decoupling for task-
agnostic continual learning. In Advances in Neural Informa-
tion Processing Systems , 2023. 2
[29] Yan-Shuo Liang and Wu-Jun Li. Adaptive plasticity im-
provement for continual learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7816–7825, 2023. 4, 5
[30] Sen Lin, Li Yang, Deliang Fan, and Junshan Zhang. Trgp:
Trust region gradient projection for continual learning. In In-
ternational Conference on Learning Representations , 2022.
4
[31] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian
Ruder. Compacter: Efﬁcient low-rank hypercomplex adapter
layers. In Advances in Neural Information Processing Sys-
tems, pages 1022–1035, 2021. 2
[32] Marc Masana, Joost Van de Weijer, Bartłomiej Twardowski,
et al. On the importance of cross-task features for class-
incremental learning. arXiv preprint arXiv:2106.11930 ,
2021. 1
[33] German I Parisi, Ronald Kemker, Jose L Part, Christopher
Kanan, and Stefan Wermter. Continual lifelong learning with
neural networks: A review. Neural Networks , pages 54–71,
2019. 1
[34] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate
Saenko, and Bo Wang. Moment matching for multi-source
domain adaptation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 1406–1415,
2019. 5
[35] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
arXiv preprint arXiv:1606.04671 , 2016. 2
[36] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient pro-
jection memory for continual learning. In International Con-
ference on Learning Representations , 2021. 4
[37] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting
Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual dif-
fusion: Continual customization of text-to-image diffusion
with c-lora. CoRR , 2023. 6, 7, 8
[38] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola
Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar
Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Contin-
ual decomposed attention-based prompting for rehearsal-freecontinual learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
11909–11919, 2023. 1, 2, 5, 6, 7, 8
[39] Qing Sun, Fan Lyu, Fanhua Shang, Wei Feng, and Liang
Wan. Exploring example inﬂuence in continual learning.
Advances in Neural Information Processing Systems , pages
27075–27086, 2022. 2
[40] Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang,
Hang Su, and Jun Zhu. Hierarchical decomposition of
prompt-based continual learning: Rethinking obscured sub-
optimality. arXiv preprint arXiv:2310.07234 , 2023. 7
[41] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A
comprehensive survey of continual learning: Theory, method
and application. arXiv preprint arXiv:2302.00487 , 2023. 1
[42] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts
learning with pre-trained transformers: An occam’s razor for
domain incremental learning. Advances in Neural Informa-
tion Processing Systems , pages 5682–5695, 2022. 1, 5
[43] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,
Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent
Perot, Jennifer G. Dy, and Tomas Pﬁster. Dualprompt: Com-
plementary prompting for rehearsal-free continual learning.
InProceedings of the European Conference on Computer Vi-
sion, pages 631–648, 2022. 1, 2, 5, 6, 7, 8
[44] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer
Dy, and Tomas Pﬁster. Learning to prompt for continual
learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 139–149,
2022. 1, 2, 5, 6, 7, 8
[45] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bit-
ﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer-
based masked language-models. In Proceedings of the An-
nual Meeting of the Association for Computational Linguis-
tics (Short Papers) , pages 1–9, 2022. 2
[46] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In Proceedings
of the International Conference on Machine Learning , pages
3987–3995, 2017. 2
[47] Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen,
and Yunchao Wei. SLCA: slow learner with classiﬁer align-
ment for continual learning on a pre-trained model. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 19091–19101, 2023. 8
[48] Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xi-
angyu Yue, and Yang You. Preventing zero-shot transfer
degradation in continual learning of vision-language models.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 19068–19079, 2023. 2
[49] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang
Xie, Alan Yuille, and Tao Kong. Image bert pre-training with
online tokenizer. In International Conference on Learning
Representations , 2022. 7
[50] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-
Lin Liu. Prototype augmentation and self-supervision for
incremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
5871–5880, 2021. 2
23647
