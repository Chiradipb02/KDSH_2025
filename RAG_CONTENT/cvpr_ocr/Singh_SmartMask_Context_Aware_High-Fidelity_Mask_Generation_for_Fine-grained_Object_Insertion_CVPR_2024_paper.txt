SmartMask: Context Aware High-Fidelity Mask Generation for
Fine-grained Object Insertion and Layout Control
Jaskirat Singh1,2Jianming Zhang1Qing Liu1Cameron Smith1Zhe Lin1Liang Zheng2
1Adobe Research2Australian National University
https://smartmask-gen.github.io
Conventional InpaintingOursInputs
Image InputImage Input + MaskAdobe SmartBrushSD -InpaintBlended Latent Diff.SmartMaskPredictionOutput (Ours)
Adobe Gen-Fill
(a) SmartMask for performing object insertion (“bride”) with better background preservation.
Input ImageSmartMaskOutputs for “woman beside a river in mountains” 
Input ImageSmartMaskOutputs for “woman in a living room” 
(b) SmartMask for mask-free object insertion where it provides diverse suggestions without user bounding box input.
Input Prompt: Two children sitting in front of  a cardboard tree
Scribble + ControlnetScribble + PwWUser ScribblesFine-grain Layout Generation (Ours) Ours + Controlnet
Scribble + DenseDiff
(c) SmartMask for fine-grain layout design for layout to image generation.
Figure 1. Overview . We introduce SmartMask which allows a novice user to generate high-fidelity masks for fine-grained object insertion
and layout control. The proposed approach can be used for object insertion (a-b) where it not only allows for image inpainting with better
background preservation (a) but also provides diverse suggestions for mask-free object insertion at different positions and scales. We also
find that when used iteratively SmartMask can be used for fine-grained layout design (c) for better quality semantic-to-image generation.
Abstract
The field of generative image inpainting and object in-
sertion has made significant progress with the recent advent
of latent diffusion models. Utilizing a precise object mask
can greatly enhance these applications. However, due to the
challenges users encounter in creating high-fidelity masks,
there is a tendency for these methods to rely on more coarse
masks (e.g., bounding box) for these applications. This re-
sults in limited control and compromised background con-
tent preservation. To overcome these limitations, we intro-
duce SmartMask, which allows any novice user to create
detailed masks for precise object insertion. Combined with
a ControlNet-Inpaint model, our experiments demonstrate
that SmartMask achieves superior object insertion qual-ity, preserving the background content more effectively than
previous methods. Notably, unlike prior works the proposed
approach can also be used even without user-mask guid-
ance, which allows it to perform mask-free object insertion
at diverse positions and scales. Furthermore, we find that
when used iteratively with a novel instruction-tuning based
planning model, SmartMask can be used to design detailed
layouts from scratch. As compared with user-scribble based
layout design, we observe that SmartMask allows for better
quality outputs with layout-to-image generation methods.
1. Introduction
Multi-modal object inpainting and insertion has gained
widespread public attention with the recent advent of large-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6497
scale language-image (LLI) models [1–3, 19, 24, 27, 30,
37, 38]. A novice user can gain significant control over the
inserted object details by combining text-based condition-
ing with additional guidance from a coarse bounding box
or user-scribble mask. The text prompt can be used to de-
scribe the object semantics, while the coarse mask provides
control over the position and scale of the generated object.
While convenient, the use of a coarse mask for object
insertion suffers from two main limitations. 1) First, the
use of a coarse mask can often be undesirable as it tends
to also modify the background regions surrounding the in-
serted object [30, 37] (refer Fig. 1a). In order to minimize
the background artifacts, recent works [37, 40] also explore
the use of user-scribble based free-form mask instead of a
bounding-box input. However, while feasible for describing
coarse objects ( e.g., mountains, teddy bear etc.), the gener-
ation of accurate free-form masks for objects with a num-
ber of fine-grain features ( e.g., humans) can be quite chal-
lenging especially when limited to coarse user-scribbles. 2)
Furthermore, generating variations in position and scale of
the inserted object can also be troublesome as it requires the
user to provide a new scene-aware free-form mask to satisfy
the geometric constraints at the new scene location.
To address these drawbacks, we introduce SmartMask , a
context-aware diffusion model which allows a novice user
to directly generate fine-grained mask suggestions for pre-
cise object insertion. In particular, given a semantic object
description ( e.g.kid) and the overall scene context, Smart-
Mask generates scene-aware precise object masks which
can then be used as input to a ControlNet-inpaint model
[36, 41] to perform object insertion while preserving the
contents of background image. As compared with coarse-
mask based inpainting methods, we find that SmartMask
provides a highly convenient and controllable method for
object insertion and can be used 1) with user-inputs (bound-
ing box, scribbles etc.) : where the user can specify location
and shape for the target object , or 2) in a mask-free man-
ner: where the model automatically generates diverse sug-
gestions for object insertion at diverse positions and scales.
In addition to object insertion, we also find that Smart-
Mask can be used for fine-grain layout design. Exist-
ing segmentation-to-image (S2I) generation methods ( e.g.
ControlNet [41]) enable the generation of controllable im-
age outputs from user-scribble based semantic segmenta-
tion maps. However, generating a good quality semantic
layout can be quite challenging if the user wants to gen-
erate a scene with objects that require fine-grain details
for best description ( e.g., humans, chairs etc.). To address
this challenge, we show that SmartMask when used with a
novel instruction-tuning [34] based planning model allows
the user to iteratively generate the desired scene layout from
scratch. As compared with scribble based layout genera-
tion, we find that the proposed approach allows the users tobetter leverage existing S2I generation methods ( e.g. Con-
trolNet [41]) for higher quality output generation.
The main contributions of the paper are: 1) We propose
SmartMask which allows any novice user to generate pre-
cise object masks for finegrained object insertion with bet-
ter background preservation. 2) We show that unlike prior
works, the proposed approach can also be used for mask-
free object insertion. 3) Finally, we demonstrate that Smart-
Mask can be used iteratively to generate detailed semantic
layouts, which allows users to better leverage existing S2I
generation methods for higher quality output generation.
2. Related Work
Diffusion based multi-modal image inpainting [1–3, 19,
24, 27, 30, 37, 38] has gained widespread attention with
advent of text-conditioned diffusion models [24, 29, 30, 32,
39]. Despite their efficacy, these methods use coarse bound-
ing box or user-scribble based masks for object inpainting
which leads to poor background preservation around the in-
serted object. In contrast, SmartMask directly allows user to
generate precise masks for the target object, which can then
be combined with ControlNet-Inpaint [30, 41] to insert the
target object while better preserving background contents.
Mask-free object placement has been studied in the
context of image compositing methods [9, 20, 22, 25, 35,
42, 44, 46], where given a cropped RGB object instance
and a target image, the goal is to suggest different positions
for the target object. In contrast, we study the problem of
mask-free object insertion using text-only guidance. Lee et
al. [18] propose a GAN-based approach to directly model
a distribution of potential object locations. However, the
learned distribution is w.r.t to a specific object class ( e.g.,
cars) which limits its generalizability for diverse use-cases.
Semantic-layout to image generation methods have
been explored to enable controllable image synthesis from
user-scribble based semantic segmentation maps [8, 12, 17,
23, 26, 33, 45]. Recently, [4, 6, 14, 31] propose a cross-
attention based training-free approach for controlling the
overall scene layout from coarse user-scribbles using text-
conditioned diffusion models. Zhang et al. [41] propose a
versatile ControlNet model which allows the users to con-
trol the output layout on a more fine-grained level through
an input semantic map. While effective, generating desired
semantic layout with scribbles can itself be quite challeng-
ing for scenes with objects that require fine-grain details
for best description ( e.g., humans). SmartMask helps ad-
dress this problem by allowing users to generate more fine-
grained layouts to facilitate better quality S2I generation.
3. Our Method
Given an input image I, object semantic label Tobjand a
textual description Tcontext describing the final scene con-
6498
Training Time
Training Image:
Intermediate Map :Diffusion U-net
Instruct-BLIPCaptioningA family posing for a picture at a wedding
Semantic Amodal Segmentation Data
: “wall”
: “man”
: “meadow”: “woman”: “child”
: child
Output Mask Prediction
Noise Latent
Encoder
Inference Image:
Panoptic SegmentationControlNetInpaint
SmartMaskModel 
SmartMask PredictionInpainting Output
Semantic Layout:Inference Time
: girl
girl in front of lakeFigure 2. Method Overview. A key idea behind SmartMask is to leverage semantic amodal segmentation data [28, 47] in order to obtain
high-quality paired training annotations for mask-free single or multi-step object insertion. During training (top) , given a training image
Iwith caption C, we stack kordered instance maps {A1, A2, . . . A k}to obtain an intermediate semantic map Sk. The diffusion model
is then trained to predict the instance map Ak+1, conditional on the semantic map Sk,Tobj← O k+1and scene context Tcontext ←C.
During inference (bottom) , given a real image I, we first use a panoptic segmentation model to compute semantic map SI. The generated
semantic layout is then directly used as input to the trained diffusion model in order to predict the fine-grained mask for the inserted object.
text, our goal is predict a fine-grained mask Mobjfor the
target object. The object mask Mobjcould then be used
as input to a ControlNet-Inpaint model for fine-grained ob-
ject insertion (Sec. 4.1) or used to design detailed semantic
layouts from scratch (Sec. 4.3). For instance, SmartMask
could be used for single object insertion, where given an
image Idepicting a man on a bench, {Tobj:‘woman’ }
and{Tcontext :‘a couple sitting on a bench’ }, our goal is
to predict fine-grain binary mask Mwoman which places
the‘woman’ in a manner such that the resulting scene aligns
with overall scene context of a ‘a couple sitting on a bench’ .
Similarly, SmartMask could also be used for multiple ob-
ject insertion. For instance, given an image Idepicting a
wedding, the user may wish to add multiple objects {‘man’ ,
‘woman’ and‘kid’}such that the final scene aligns with the
{Tcontext :‘a family posing for a picture at a wedding’ }.
Unlike prior image inpainting methods which are limited
independently adding each object to the scene, the goal of
thesmartmask model is to add each object in a manner such
the generated ‘man’ ,‘woman’ and‘kid’ appear to be ‘a fam-
ily posing for a picture at a wedding’ . (refer Fig. 2a).
In the next sections, we describe the key SmartMask
components in detail. In particular, in Sec. 3.1 we discuss
how SmartMask can leverage semantic amodal segmenta-
tion data in order to obtain high-quality paired annotations
for mask-free object insertion. We then discuss a simpledata-adaptation strategy which allows the user to also con-
trol the position, shape of the inserted object using coarse
inputs (bounding-box, scribbles) in Sec. 3.2. Finally, in
Sec. 3.3 we propose a visual-instruction tuning [21] based
planning model which when used with SmartMask , allows
for generation of detailed semantic layouts from scratch.
3.1. SmartMask for Mask-Free Object Insertion
Semantic-Space Task Formulation. Directly learning a
model for our task in pixel space can be quite challenging,
as it would require large-scale collection of training data
for single or multi-step object insertion while maintaining
background preservation. To address this key challenge, a
core idea of our approach is to propose an equivalent task
formulation which allows us to leverage large-scale seman-
tic amodal segmentation data [28, 47] for generating high-
quality paired training data in the semantic space (Fig. 2).
SmartMask Training. In particular during training,
given an image I, with a sequence of ordered amodal se-
mantic instance maps {A1, A2. . . A n}and corresponding
semantic object labels {O1,O2. . .On}, we first compute
an intermediate layer semantic map as,
Sk=flayer({A1, A2. . . A k})where k ∈[1, n].(1)
where kis randomly chosen from [1, n]andflayer is a lay-
ering operation which stacks the amodal semantic segmen-
tation maps from i∈[1, k]in an ordered manner (Fig. 2).
6499
We next train a diffusion-based mask prediction model
Dθwhich takes as input the above computed intermediate
semantic layer map Sk, textual description for next object
Tobj← O k+1, overall caption Tcontext ←CI(for image
I) and learns to predict the binary mask {Ak+1}for the
next object. To this end, we first pass the intermediate se-
mantic map Skthrough a learnable encoder Eϕto obtain
the encoded features Eϕ(Sk). At any timestep tof the re-
verse diffusion process, the denoising noise prediction ϵtis
then computed conditional jointly on previous noise latent
zt, and model inputs {Eθ(Sk),Tobj,Tcontext }as,
˜ϵpred(t) =Uθ(zt,Eϕ(Sk),Tobj,Tcontext , t), (2)
where Uθrepresents the U-Net of the diffusion model D.
The overall diffusion model Dis then trained to predict
the next layer Ak+1using the following diffusion loss,
Lt(θ, ϕ) =Et∼[1,T],Sk,Ak+1,ϵt[∥ϵt−˜ϵpred(t)∥2],(3)
where Tis total number of reverse diffusion steps, ϵt∼
N(0, I)is sampled from a normal distribution and Ak+1
represents ground truth binary mask for next object Ok+1.
SmartMask Inference . During inference, given an in-
put image I, semantic object category Tobjand a textual
description Tcontext describing the final scene context, we
first use a panoptic semantic segmentation model [15] to ob-
tain the corresponding semantic layout map SI. The gen-
erated semantic layout SIis then directly used as input to
the above trained diffusion model Dθin order to predict the
fine-grained mask Mobjfor the target object,
Mobj=Dθ(Eϕ(SI),Tobj,Tcontext ). (4)
3.2. Data Adaptation for Precise Mask Control
While the diffusion model trained in Sec. 3.1, allows the
user to perform mask-free object insertion at diverse po-
sitions and scales, the user may also wish to obtain more
direct control over the spatial location and details of the in-
serted object. To this end, we propose a simple train-time
data adaptation strategy which allows smartmask to be eas-
ily adapted to diverse forms of user control (Fig. 4). In par-
ticular, given the intermediate layer map Skcomputed using
Eq. 1 and ground truth mask Ak+1for the next object, we
replace the input Skto the diffusion model as,
˜Sk=g(Sk, Gobj) =Sk⊙(1−α Gobj) +α Gobj,(5)
where Gobjis the additional guidance input ( e.g., bounding
box mask, coarse scribbles etc.) provided by the user and
α= 0.7helps add additional guidance while still preserving
the content of the original input Skafter data adaptation.
Training. In this paper, we mainly consider four main
guidance inputs Gobjfor additional mask control for adapt-
ingSmartMask .1) Mask-free guidance: in absence of anyadditional user inputs, we use Gobj=0H,Wwhich prompts
the model to suggest fine-grained masks for object insertion
at diverse positions and scales. 2) Bounding-box guidance:
we set Gobjas a binary mask corresponding to ground truth
object mask Ak+1.3) Coarse Spatial Guidance: Expecting
the user to provide precise bounding box for object insertion
is not always convenient and can lead to errors if bounding
box is not correct. We therefore introduce a coarse spatial
control where user may provide a coarse spatial location and
the model learns to infer the best placement of the object
around the suggested region (Fig. 4c). During training, the
same is achieved by setting Gobjas a coarse gaussian blob
centered at the ground truth object mask Ak+1.4) User
scribbles: Finally, we also allow the user to describe target
object using free-form coarse scribbles, by setting Gobjas
the dilated mask output of ground-truth object mask Ak+1.
Inference. At inference time, the additional guidance in-
putGobj(e.g. bounding box mask, coarse scribbles etc.) is
directly provided by the user. Given an input image Iwith
semantic layout SI, we then use transformation from Eq. 5
as input to the adapted SmartMask model in order to gener-
ate object insertion suggestions with additional control.
3.3. Global Planning for Multi-Step Inference
While the original SmartMask model allows the user to
generate fine-grained masks for single object insertion, we
would also like to use SmartMask for iterative use cases
such as multiple object insertion (Sec. A.1) or designing a
fine-grained layout from scratch with large number ( >10)
of scene elements. Such an iterative use of SmartMask
would require the model to carefully plan the spatial loca-
tion of each inserted object to allow for the final scene to be
consistent with the final scene context description Tcontext .
To achieve this, we train a visual-instruction tuning [21]
based planning model which given the input semantic lay-
outSI, learns to plan the positioning of different scene el-
ements over long sequences. Given a semantic object de-
scription Tobjand final scene context Tcontext , the global
planning model provides several bounding box suggestions
for object insertion. SmartMask model then uses the above
predictions as coarse spatial guidance input Gobjto pro-
vide fine-grained mask suggestions for the next object. The
above process can then be repeated in an iterative manner
until all objects have been added to the scene (refer Fig. 7a).
The global planning model is trained in two stages. 1)
Feature Alignment. Typical instruction tuning models are
often trained on real images. In contrast, as discussed in
Sec. 3.1 we would like to model our problem in semantic
space as it allows us to leverage amodal segmentation data
for training. To address this domain gap, we first finetune
an existing LLaV A model [21] to understand the semantic
inputs. To do this, given an intermediate semantic map Sk
computed using Eq. 1, we finetune the projection matrix W
6500
Figure 3. Qualitative Results for Image Inpainting. We observe that as compared to with state-of-the-art image inpainting [1, 2, 30, 37]
methods, SmartMask allows the user to perform object insertion while better preserving the background around the inserted object.
of the LLaV A model [21] Hto predict the semantic object
labels{O1,O2. . .Ok}described in the current scene as,
Lalign(W) =LCE(<O1,O2. . .Ok>,H(Sk)).(6)
2)Instruction-Tuning. Finally, keeping the visual encoder
weights for LLaV A model fixed, we next finetune both
project matrix Wand LLM weights Φ[43] for global ob-
ject planning. In particular, given an intermediate seman-
tic map Sk, we first compute the bounding box coordi-
natesBk+1={xmin, ymin, xmax, ymax}for the next ob-
jectTobj=Ok+1using ground truth object mask Ak+1.
The LLaV A based planning model His then trained as,
Linstruct (W,Φ) =LCE(Bk+1,H(Sk,Tobj, C)),(7)
where Crepresents the caption for the final scene (obtained
using ground-truth image I) and provides the model context
for placing different scene elements in the image.
4. Experiments
Training Data Collection. As discussed in Sec. 3, we note
that a key idea behind SmartMask is to model the object in-
sertion problem in semantic space (instead of pixel space),
which allows us to leverage semantic amodal segmentation
data to obtain large-scale paired training annotations for
single or multiple object insertions. However, traditional
datasets for semantic amodal segmentations such as COCO-
A [47] (2,500 images, 22,163 instances) and KINS [28](7,517 images, 92,492 instances) though containing fine-
grained amodal segmentation annotations may lack suffi-
cient diversity to generalize across different use-cases.
To address this, we curate a new large-scale dataset con-
sisting of fine-grain amodal segmentation masks for differ-
ent objects in an input image. The overall dataset consists
of 32785 diverse real world images and a total of 725897
object instances across more than 500 different semantic
classes ( e.g. man, woman, trees, furniture etc.). Each im-
ageIin the dataset consists of a variable number of object
instances {A1, A2, . . . A n}, n∈[2,50]and is annotated
with an ordered sequence of semantic amodal segmenta-
tion maps {S1, S2. . . S n}. The detailed descriptions CIfor
each image are obtained using the InstructBLIP [7] model.
SmartMask Training. In order to leverage the rich gen-
eralizable prior of T2I diffusion models, we use the weights
from publicly available Stable-Diffusion-v1.5 model [30]
in order to initialize the weights of the SmartMask U-Net
model trained in Sec. 3.1. Similar to [5], we modify the ar-
chitecture of the U-Net model to also condition the output
mask predictions on segmentation layout SI. The Smart-
Mask model is trained for a total of 100k iterations with a
batch size of 192 and learning rate 1e−5using 8 Nvidia-
A100 GPUs. During inference, a panoptic semantic seg-
mentation model finetuned on the dataset in Sec. 4 is used
for converting real image Ito its semantic layout SI. Con-
trolNet model trained with SDXL backbone was used to
perform precise object insertion with SmartMask outputs.
Please refer the supp. material for further training details.
6501
(a)Mask-free object insertion
 (b)Bounding-box guidance for mask generation
(c)Coarse-spatial guidance for mask generation
 (d)Scribble-based object insertion
Figure 4. Diverse User Controls .SmartMask is easily adaptable to diverse user-controls for precise mask generation for the target object.
4.1. SmartMask for Object Insertion
Baselines. We compare the performance of our approach
on object-insertion with prior works on performing multi-
modal image-inpainting using a textual description and
coarse bounding box mask. In particular we show compar-
isons with SD-Inpaint [30], SDXL-Inpaint [27], Blended-
Latent Diffusion [30], and Adobe SmartBrush [37]. We
also compare the performance of our approach with state-
of-the-art commercial inpainting tools by reporting results
on recently released Generative-Fill from Adobe Firefly [1].
Evaluation Metrics. Following [37], we report the re-
sults for object insertion using 1) Local-FID [11] which
measures the realism of the generated objects, 2) CLIP-
Score [10] which measures the alignment between the tex-
tual description and the generated object, and 3) Norm. L2-
BG:which reports the normalized L2difference in the back-
ground regions before and after insertion, and helps capture
the degree to which the background was preserved.
Qualitative Results. Results are shown in Fig. 3. We ob-
serve that when performing objectn insertion using a coarse
bounding box mask, traditional inpainting methods usually
lead to a lot of changes in the background regions around
the inserted object ( e.g. living room details in row-1&2,
mountains in row-4 etc.). Adobe SmartBrush [37] which is
trained to allow better background preservation, shows bet-
ter performance, however, still suffers from notable changes
to background regions. In contrast, by directly predicting a
high-fidelity mask for the target object, the proposed ap-
proach allows the user to add new objects on the scene withMethodEvaluation Criteria
Local-FID ↓CLIP-Score ↑Norm. L2-BG ↓
SD Inpaint [30] 22.31 0.249 0.374
SDXL Inpaint [27] 21.84 0.235 0.623
Blended L-Diffusion [2] 39.77 0.253 0.451
Adobe SmartBrush [37] 17.94 0.262 0.304
Adobe Gen-Fill⋆[1] N/A 0.268 0.289
Smartmask (Ours) 19.21 0.261 0.098
Table 1. Quantitative results for image inpainting .We observe
that in comparison with state-of-art image inpainting methods, our
approach leads to better preservation of background regions.
minimal changes to the background image. Furthermore we
observe that target object masks are generated in a scene-
aware manner, which helps us add new objects while in-
teracting with already existing ones. For instance, when
adding ‘a man to a couch with table in front’ (Fig. 3), prior
works typically replace the couch and table to insert the tar-
get object ( ’man’ ). In contrast, smartmask places the ‘man
sitting on the couch with his leg on the table’ , and provides
a more natural way for inserting objects in complex scenes.
Quantitative Results. In addition to qualitative results,
we also report the performance of our approach quantita-
tively in Tab. 1. We find that similar to results in Fig. 3, the
proposed approach shows better background preservation
(Norm. L2-BG ↓) while performing comparably in terms of
image quality (local-FID) and text-alignment (CLIP-Score).
4.2. Evaluating Mask Controllability and Quality
A key advantage of SmartMask is the ability to generate
high-quality masks for target object in a controllable man-
6502
Figure 5. Comparing output mask quality with different Inpaint +
HQSAM (middle) methods and SmartBrush mask output (right).
Figure 6. Limitation of Inpaint + HQSAM . In addition to poor
mask quality errors (Fig. 5), we observe that Inpaint+HQSAM can
lead to scene-unaware masks ( e.g.mask for woman sitting in air )
.
ner. In this section, we evaluate the performance of Smart-
Mask in terms of 1) user control, & 2) output mask quality.
1) User Control . As shown in Fig. 4, we observe that
SmartMask allows the user to control the output object mask
in four main ways. 1) Mask-free insertion: where the
model automatically suggests diverse positions and scales
for the target object ( e.g.,table, woman in Fig. 4a). 2)
Bounding-box guidance: which allows user to specify the
exact bounding box for object insertion ( potted plant, mo-
torbike in Fig. 4b). 3) Coarse spatial guidance: (Fig. 4c)
providing a precise bounding box can be challenging for
cases with complex object insertions e.g.,dog with owner .
SmartMask allows the user to only specify a coarse location
for the target object, and the model automatically adjusts the
object placement ( i.e.dog with head near woman’s hand ) to
capture object interactions. 3) User-scribbles: Finally, the
user may also control the output shape by providing coarse
scribbles. The smartmask model can use this as guidance
to automatically predict the more finegrain-masks for the
target object ( e.g., palm-tree, gaming chair in Fig. 4d).
2) Output Mask Quality . We also report results on the
quality of generated masks by showing comparisons with
the mask-prediction head of the SmartBrush model [37].
Furthermore, we also show comparisons combining stan-
dard inpainting methods with HQSAM [13, 16]. To this
end, we first use the provided bounding-box mask to inpaint
the target object. The user-provided bounding-box and the
inpainted output are then used as input to the HQSAM
model [13] to obtain target object-mask predictions.MethodUser Study Results
Win↑ Draw Lose ↓
SDInpaint + HQSAM [13, 30] 92.04% 5.12% 2.84%
Blended L-Diff + HQSAM [2, 13] 88.91% 8.33% 2.75%
SmartBrush + HQSAM [13, 37] 63.89% 27.78% 8.34%
SmartBrush Mask [37] 91.67% 2.78% 5.56%
Table 2. User study results . For evaluating generated mask qual-
ity. We observe that SmartMask generates higher quality masks as
compared to SmartBrush and various Inpaint+HQSAM methods.
Results are shown in Fig. 5. We observe that as com-
pared to outputs of SmartBrush [37] mask-prediction head,
SmartMask generates higher-quality masks with fewer ar-
tifacts. Similarly, while using HQSAM on inpainting out-
puts helps achieve good mask quality for some examples
(e.g. child in row-2), the HQSAM generated masks (or the
inpainted image) often have accompanying artifacts which
limits the quality of the output masks. In addition to poor
mask quality errors, we also observe that Inpaint+HQSAM
can lead to scene-unaware masks (Fig. 6). This occurs be-
cause prior inpainting methods typically add additional ob-
jects in the background when performing object insertion.
For instance, when inserting woman in a living room in
Fig. 6), we observe that Adobe Gen-Fill [1] adds an ad-
ditional chair on which the woman is sitting. Extracting
only the object mask for such inpainted outputs can lead to
scene-unaware masks where ‘the woman appears floating
in the air‘ as the chair was not present in the original image.
The above findings are also reflected in a quantitative
user study (Tab. 2), where human subjects are shown a
pair of object mask suggestions (ours vs baselines discussed
above), and asked to the select the mask suggestion with the
higher quality. As shown in Tab. 2, we observe that Smart-
Mask outputs are preferred by majority of human subjects
over SmartBrush mask [37] and Inpaint + HQSAM outputs.
4.3. SmartMask for Semantic Layout Design
In addition to object insertion, we also find that when used
iteratively along with the visual-instruction tuning based
planning model from Sec. 3.3, SmartMask forms a conve-
nient approach for designing detailed semantic layouts with
a large number of fine-grain objects ( e.g. humans, furniture
etc.). Results are shown in Fig. 7a. We observe that given
a sequence of user provided scene elements ( e.g. painting,
sofa, chair etc.),SmartMask generates the entire scene lay-
out from scratch. Furthermore, unlike static layouts gener-
ated by a panoptic segmentation model, SmartMask gener-
ated layouts allow the user greater control over the details
of each scene element. Since each object in the final layout
is represented by a distinct object mask, the final layouts
are highly controllable and allow for a range of custom op-
erations such as adding, removing, modifying or moving
objects through simple layer manipulations. (refer Fig. 7b).
Controllable S2I Generation. Layout to image gener-
6503
(a) SmartMask for designing very detailed semantic layouts from scratch.
(b) Analyzing controllability of the layouts generated with SmartMask.
(c) Using SmartMask generated layouts for better quality layout-to-image generation.
Figure 7. Fine-grained layout design. We observe that SmartMask when used iteratively, allows the user to generate very detailed layouts
from scratch (a). The generated layouts are highly controllable and allow for custom variations through simple layer manipulations (b).
ation methods e.g., ControlNet [41] enable the generation
of controllable image outputs from user-scribble based se-
mantic segmentation maps or layouts. However, generating
the user-desired layouts with coarse scribbles can itself be
quite challenging for scenes with objects that require fine-
grain details for best description ( e.g. humans, chairs etc.).
As shown in Fig. 7c, we find that this can lead to image
outputs with either deformity artifacts (child in row-1) or
incorrect description (woman and children in row-2) when
using ControlNet [41]. A similar problem is also observed
in other coarse-scribble based S2I methods such as DenseD-
iffusion [14] and Paint-with-Words (PwW) [4], which pro-
vide coarse control over object position but are unable to
control finegrain details such as pose, action etc. of the tar-
get object. SmartMask helps address this problem by al-
lowing any novice user to generate controllable (Fig. 7b)
fine-grain layouts from scratch, which can allow users to
better leverage existing S2I methods [41] for higher quality
layout-to-image generation (refer Fig. 7c).5. Conclusion
In this paper, we present SmartMask which allows a novice
user to generate scene-aware precision masks for object in-
sertion and finegrained layout design. Existing methods for
object insertion typically rely on a coarse bounding box
or user-scribble input which can lead to poor background
preservation around the inserted object. To address this, we
propose a novel diffusion based framework which leverages
semantic amodal segmentation data in order to learn to gen-
erate fine-grained masks for precise object insertion. When
used along with a ControlNet-Inpaint model, we show that
the proposed approach achieves superior object-insertion
performance, preserving background content more effec-
tively than previous methods. Additionally, we show that
SmartMask provides a highly controllable approach for de-
signing detailed layouts from scratch. As compared with
user-scribble based layout design, we observe that the pro-
posed approach can allow users to better leverage existing
S2I methods for higher quality layout-to-image generation.
6504
References
[1] Adobe. Adobe firefly – generative ai for everyone, 2023. 2,
5, 6, 7
[2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. ACM Transactions on Graphics (TOG) ,
42(4):1–11, 2023. 5, 6, 7
[3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 18208–18218, 2022. 2
[4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image
diffusion models with an ensemble of expert denoisers. arXiv
preprint arXiv:2211.01324 , 2022. 2, 8
[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392–18402, 2023.
5
[6] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free
layout control with cross-attention guidance. arXiv preprint
arXiv:2304.03373 , 2023. 2
[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning,
2023. 5
[8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12873–12883, 2021. 2
[9] Arnab Ghosh, Richard Zhang, Puneet K Dokania, Oliver
Wang, Alexei A Efros, Philip HS Torr, and Eli Shechtman.
Interactive sketch & fill: Multiclass sketch-to-image transla-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 1171–1180, 2019. 2
[10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
and Yejin Choi. Clipscore: A reference-free evaluation met-
ric for image captioning. arXiv preprint arXiv:2104.08718 ,
2021. 6
[11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 6
[12] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 2
[13] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing
Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in
high quality. In NeurIPS , 2023. 7
[14] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and
Jun-Yan Zhu. Dense text-to-image generation with attention
modulation. In ICCV , 2023. 2, 8
[15] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll ´ar. Panoptic segmentation. In Pro-ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 9404–9413, 2019. 4
[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 7
[17] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo.
Maskgan: Towards diverse and interactive facial image ma-
nipulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5549–
5558, 2020. 2
[18] Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-
Hsuan Yang, and Jan Kautz. Context-aware synthesis and
placement of object instances. Advances in neural informa-
tion processing systems , 31, 2018. 2
[19] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 22511–22521, 2023. 2
[20] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman,
and Simon Lucey. St-gan: Spatial transformer generative
adversarial networks for image compositing. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 9455–9464, 2018. 2
[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 3, 4, 5
[22] Liu Liu, Zhenchen Liu, Bo Zhang, Jiangtong Li, Li Niu,
Qingyang Liu, and Liqing Zhang. Opa: object placement
assessment dataset. arXiv preprint arXiv:2107.01889 , 2021.
2
[23] Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, et al.
Learning to predict layout-to-image conditional convolutions
for semantic image synthesis. Advances in Neural Informa-
tion Processing Systems , 32, 2019. 2
[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 2
[25] Li Niu, Qingyang Liu, Zhenchen Liu, and Jiangtong
Li. Fast object placement assessment. arXiv preprint
arXiv:2205.14280 , 2022. 2
[26] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 2337–2346,
2019. 2
[27] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion mod-
els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952 , 2023. 2, 6
[28] Lu Qi, Li Jiang, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
Amodal instance segmentation with kins dataset. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 3014–3023, 2019. 3, 5
[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
6505
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 2
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models, 2021. 2, 5, 6, 7
[31] Jaskirat Singh, Stephen Gould, and Liang Zheng. High-
fidelity guided image synthesis with latent diffusion models.
In2023 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pages 5997–6006. IEEE, 2023. 2
[32] Jaskirat Singh and Liang Zheng. Divide, evaluate, and re-
fine: Evaluating and improving text-to-image alignment with
iterative vqa feedback. Advances in Neural Information Pro-
cessing Systems , 36:70799–70811, 2023. 2
[33] Vadim Sushko, Edgar Sch ¨onfeld, Dan Zhang, Juergen Gall,
Bernt Schiele, and Anna Khoreva. You only need adversar-
ial supervision for semantic image synthesis. arXiv preprint
arXiv:2012.04781 , 2020. 2
[34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-
lien Rodriguez, Armand Joulin, Edouard Grave, and Guil-
laume Lample. Llama: Open and efficient foundation lan-
guage models. arXiv preprint arXiv:2302.13971 , 2023. 2
[35] Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Am-
brish Tyagi, James M Rehg, and Visesh Chari. Learning to
generate synthetic data via compositing. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 461–470, 2019. 2
[36] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro
Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,
and Thomas Wolf. Diffusers: State-of-the-art diffusion
models. https://github.com/huggingface/
diffusers , 2022. 2
[37] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun
Zhang. Smartbrush: Text and shape guided object inpainting
with diffusion model. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22428–22437, 2023. 2, 5, 6, 7
[38] Shiyuan Yang, Xiaodong Chen, and Jing Liao. Uni-paint:
A unified framework for multimodal image inpainting with
pretrained diffusion model. In Proceedings of the 31st ACM
International Conference on Multimedia , pages 3190–3199,
2023. 2
[39] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789 , 2022. 2
[40] Yu Zeng, Zhe Lin, and Vishal M Patel. Shape-guided object
inpainting. arXiv preprint arXiv:2204.07845 , 2022. 2
[41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 2, 8
[42] Lingzhi Zhang, Tarmily Wen, Jie Min, Jiancong Wang,
David Han, and Jianbo Shi. Learning object placement by in-
painting for compositional data augmentation. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,UK, August 23–28, 2020, Proceedings, Part XIII 16 , pages
566–581. Springer, 2020. 2
[43] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 5
[44] Siyuan Zhou, Liu Liu, Li Niu, and Liqing Zhang. Learn-
ing object placement via dual-path graph completion. In
European Conference on Computer Vision , pages 373–389.
Springer, 2022. 2
[45] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka.
Sean: Image synthesis with semantic region-adaptive nor-
malization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5104–
5113, 2020. 2
[46] Sijie Zhu, Zhe Lin, Scott Cohen, Jason Kuen, Zhifei Zhang,
and Chen Chen. Topnet: Transformer-based object place-
ment network for image compositing. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1838–1847, 2023. 2
[47] Yan Zhu, Yuandong Tian, Dimitris Metaxas, and Piotr
Doll´ar. Semantic amodal segmentation. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 1464–1472, 2017. 3, 5
6506
