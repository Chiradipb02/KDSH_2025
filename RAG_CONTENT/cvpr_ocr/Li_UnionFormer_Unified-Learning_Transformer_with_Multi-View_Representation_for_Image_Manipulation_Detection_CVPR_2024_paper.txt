UnionFormer: Uniﬁed-Learning Transformer with Multi-View Representation for
Image Manipulation Detection and Localization
Shuaibo Li1,2Wei Ma1†Jianwei Guo2Shibiao Xu3Benchong Li1Xiaopeng Zhang2
1Beijing University of Technology
2MAIS, Institute of Automation, Chinese Academy of Sciences
3Beijing University of Posts and Telecommunications
Abstract
We present UnionF ormer , a novel framework that inte-
grates tampering clues across three views by uniﬁed learning
for image manipulation detection and localization. Speciﬁ-
cally, we construct a BSFI-Net to extract tampering features
from RGB and noise views, achieving enhanced responsive-
ness to boundary artifacts while modulating spatial consis-
tency at different scales. Additionally, to explore the incon-
sistency between objects as a new view of clues, we combine
object consistency modeling with tampering detection and
localization into a three-task uniﬁed learning process, allow-
ing them to promote and improve mutually. Therefore, we
acquire a uniﬁed manipulation discriminative representation
under multi-scale supervision that consolidates information
from three views. This integration facilitates highly effec-
tive concurrent detection and localization of tampering. We
perform extensive experiments on diverse datasets, and the
results show that the proposed approach outperforms state-
of-the-art methods in tampering detection and localization.
1. Introduction
The rapid progression of deep generative models, such
as GANs [ 21,43,60], V AEs [ 31,50], and Diffusion Mod-
els [ 10,45,53], has facilitated the widespread availability of
Artiﬁcial Intelligence Generated Content (AIGC) tools [ 8].
At the same time, image editing tools have become exception-
ally user-friendly and powerful, capable of creating highly
realistic images and videos. This assists users in better ex-pressing their creativity but also intensiﬁes the malicioususe of editing techniques to tamper with multimedia con-tent, resulting in the proliferation of faked images on the
Internet [ 57]. Therefore, developing a universally effective
method to discern the authenticity of images and accurately
locate the modiﬁed regions has become crucial. Research on
related algorithms has become a hot topic [ 3,28], and many
†Corresponding author.state-of-the-art methods based on deep learning models have
been proposed.
Digital image tampering falls into three main cate-
gories [ 19]: splicing, which involves copying regions from
one image to another; copy-move, entailing the copying or
moving of elements within the same image; and removal,
the process of erasing parts of an image and creating visual
consistent content to obscure the alterations. These manipu-
lations leave traces between the tampered regions and their
surroundings, causing inconsistencies between the authentic
and forgery regions. Unlike traditional detection or segmenta-
tion tasks emphasizing high-level semantic information, im-
age tampering detection prioritizes local semantic-agnostic
clues that distinguish authenticity rather than semantic con-
tent. Therefore, the critical challenge in tampering detec-
tion is learning generalizable features that combine different
level information and capture multiple scale inconsistencies
between authentic and tampered areas. Previous methods pri-
marily utilized deep convolutional neural networks designed
for high-level visual tasks as feature encoders or directly con-nected features from different layers [
23,27,40,71], which
could not adequately represent tampering traces. Inspiredby [
9,12,67], we designed a Boundary Sensitive Feature
Interaction Network (BSFI-Net) speciﬁcally for extracting
forensics artifacts and integrated it as the feature encoder
in our framework. BSFI-Net is a parallel CNN-Transformer
structure that can reinforce edge responses while effectively
interacting between local features and global representations
to explore consistencies within images at different scales.
On the other hand, many tampering artifacts impercep-
tible in the RGB view become distinctly noticeable in the
noise view. Employing ﬁxed [ 18] or learnable high-pass
ﬁlters [ 6,35,66] to convert RGB images into noise maps
can suppress content and highlight the low-level forgeryclues. Thus, developing a multi-view strategy that simulta-
neously models the RGB and noise dimensions is essential
to detect subtle tampering traces. Our framework adopts adual-stream architecture to independently construct repre-sentation for RGB and noise views, subsequently merging
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12523
them to enhance discriminative capability and generalizabil-
ity. Furthermore, we incorporate contrastive supervision to
improve the collaboration between the two views.
In addition, to create spatially coherent and semantically
consistent images, tamper operations invariably alter entire
objects to conceal evidence, namely performing object-level
manipulation. Current advanced methods focus on pixel or
patch-level consistencies, overlooking object-level informa-
tion. Conversely, we argue that image manipulation detection
should extend beyond merely identifying out-of-distribution
pixels or patches to also capture the anomalies in object con-
sistency and distribution resulting from manipulation. Due
to hyper-realistic tampered images generated by diffusion
models [ 4,5,20,30,44,65,69], leveraging object view
information becomes particularly crucial. Diffusion-based
models [ 4,30,44] repeatedly update initial noise across the
image, enhancing spatial continuity and leaving fewer RGB
and noise traces. Moreover, unlike authentic image sources,
auto-generated forgery portions guided by natural language
prompts are more likely to exhibit object incongruities. Re-
cent Diffusion models [ 20,29,55,64] have attempted to
solve this issue by employing object-centric approaches, un-
derscoring the necessity and feasibility of object view clues
for tampering detection. However, creating and integrating
such a novel view with others for tampering artifact rep-resentation presents a signiﬁcant challenge, requiring new
architectures and learning strategies.
Considering the above vital points, we introduce Union-
Former, a uniﬁed-learning transformer framework withmulti-view representation for image manipulation detec-
tion and localization, as illustrated in Figure 1. Firstly, we
use BSFI-Net as the feature encoder to obtain the general-
izable features under RGB and noise views and combinethem. Then, we utilize the fused features to conduct a uni-
ﬁed learning process, which includes three sub-tasks: object
consistency modeling, forgery detection, and localization. In
uniﬁed learning, our model establishes the object view repre-
sentation and integrates three view information into a uniﬁed
manipulation discriminative representation (UMDR) to si-
multaneously accomplish forgery detection and localization.
To summarize, our main contributions are as follows:
•We propose UnionFormer, a novel image forensics
transformer framework. By employing uniﬁed learn-
ing with multi-scale supervision, the UnionFormer in-
tegrates information from all three views to execute
image manipulation detection and localization simulta-
neously.
•We introduce BSFI-Net, a hybrid network structure
for superior artifact representation learning, which en-
hances boundary response while revealing local incon-
sistencies at different levels across domains.
•With the uniﬁed learning of UMDR, we construct aninnovative object view representation capable of cap-
turing the inconsistency among objects and aggregated
information from three views for forgery detection.
•We involve comprehensive experiments across vari-
ous benchmarks, demonstrating that our method attains
state-of-the-art results in both detection and localization
tasks.
2. Related Work
Forgery Artifacts Representation. Most early works
[17,33,42] design hand-crafted features to characterize
tampering traces, often detecting speciﬁc types of manip-ulation. However, in real-world scenarios, various editing
operations are usually combined, and the types are unknown,
promoting more work to focus on practical general tam-pering detection [
13,23,27,59,62]. Achieving general
detection requires more generalizable and semantic-agnostic
features, so a series of works explore clues beyond the RGB
view to capture a broader range of tampering traces. The
most common approach is to use ﬁxed [ 18] or learnable
[6,34,66] ﬁlters to transform the image into the noise view
to highlight weak low-artifacts. Some other works leverage
frequency-aware clues to provide a complementary view-point [
49,54]. These low-level features are always com-
bined with the high-level features from the RGB view for
more effective detection [ 23,27,34,36,62,70]. For in-
stance, [ 13] employs dual attention to combine information
from RGB and noise views. [ 59] extracts high-frequency
features of the images and combines them with RGB features
as multimodal patch embedding. In contrast, we not only
combine tampering representations from both streams (RGBand noise views) but also facilitate their sufﬁcient interaction
through contrastive supervision. Moreover, we incorporate a
novel view that models the inconsistencies between objects,
providing robust additional cues for manipulation detection.
Transformer in Vision. Transformer [ 58] employs self-
attention mechanisms to model long-range dependencies,and it has been widely successful in natural language pro-
cessing (NLP). Some works are inspired to explore the use
of transformer architecture for various computer vision tasks
and showed superior performance. Speciﬁcally, ViT [ 16]
reshapes images into patch sequences and feeds them intoa transformer encoder for image classiﬁcation. DETR [
9]
and Deformable DETR [ 72] implement end-to-end object
detection using a transformer encoder-decoder architecture
with learnable queries and bipartite matching. CMX [ 68]
proposed a transformer framework for semantic segmenta-tion that integrates RGB and other modal information. Inthis work, we ﬁrst introduce a CNN-Transformer parallel
encoder, BSFI-Net, for tampering feature extraction. Then,
we utilize a uniﬁed-learning transformer framework to in-
tegrate multiple views information for image manipulation
12524
BSFI
Net
BSFI
NetRPN
…
…Projection
UMDR
Unified Learning with Multi-Scale Supervision 8-(<OK]5HPKIZ<OK]
4UOYK<OK]
Multi-head
Self-Attention
Feed-forward
NetworkPositional
Encoding
Feature Interaction Encoding Feature Contrastive Collaboration
Object
Consistency ClassifierReal
Fake
Localization 
Mask
Transformer Encoder
Figure 1. An overview of UnionFormer. We achieve simultaneous tampering detection and localization by integrating tampering clues from
three view representations, with each view represented by a different color background. We obtain representations under the RGB and noise
views through BSFI-Net and construct the object view representation based on both in the uniﬁed learning. Meanwhile, information from all
three views is interactively fused into a uniﬁed manipulation discriminative representation (UMDR) for detection and localization.
detection and localization.
3. Method
In this section, we ﬁrst provide an overview of Union-
Former and a detailed introduction to each component. We
aim to fully leverage rich artifacts from three views for simul-
taneous tampering detection and localization. We achieve
this through a uniﬁed learning process under multi-scalesupervision. As illustrated in Figure 1, input RGB image
Xis ﬁrstly transformed into a noise view representation
N=C(X)using constrained CNN [ 7], which can reveal
low-level tampering. Then, both XandNare individually
fed into the Boundary Sensitive Feature Interaction Networks
(BSFI-Net) for feature encoding. High-frequency edge fea-
tures (H) are incorporated with either XorNas inputs into
the BSFI-Net to boost edge responsiveness. This allows us
to acquire generalizable and discriminative features under
the RGB and noise views, constructing two feature pyramids
fr=E1(X,H),fn=E2(N,H). Subsequently, we use a
Region Proposal Network (RPN) [ 51] to obtain a set of
Regions of Interest (RoIs), represented as pi, from the fea-
turefr. RoI information is extracted from frandfn, then
ﬂattened to get embedding representations for proposals, de-
noted as ri,ni. The RGB feature riand noise feature ni
for each proposal are concatenated to generate the fusedproposal feature di, which is input into the
Itransformer
Encoder layer.
During the uniﬁed learning phase, we address three sub-
tasks: modeling object consistencies, binary classiﬁcation
of authenticity, and tampered region localization. After the
transformer encoder, the forgery-discriminative query em-
beddings DIare fed into the uniﬁed manipulation discrimi-
native representation part to generate three predictions forthree sub-tasks. As shown in Figure 1, we employ multi-scale supervision with a uniﬁed form for three sub-tasks,
including Lcls,Locm , and Lloc.
3.1. Feature Interaction Encoding
RGB and Noise View Representation. We utilize a dual-
stream structure to harness clues from both RGB and noise
views in the feature encoding stage. The RGB stream is de-
signed to capture visually apparent tampering artifacts, while
the noise stream aims to explore the distribution inconsisten-
cies between tampered and genuine regions. We employ the
learnable constrained convolutional layer proposed in [ 7]t o
transform the RGB image into the noise view.
As noted in Section 2, the edges of tampered regions and
their surroundings exhibit more prominent tampering clues.
Therefore, we enhance high-frequency edge information in
both streams to concentrate the network’s response on tam-
pered regions. Speciﬁcally, we utilize the Discrete Cosine
Transform (DCT) to convert the image data Xinto the fre-
quency domain and then apply a high-pass ﬁlter to obtainthe high-frequency component. We then convert the high-
frequency component back to the spatial domain to facilitate
feature interaction and preserve local consistency. Thus, we
get the edge-enhanced information Has follows:
H=T−1
d(Fh(Td(X),β)), (1)
where Tdrepresents DCT, Fhrepresents the high-pass ﬁlter,
andβis the threshold. We input XandNseparately into the
BSFI-Net, along with Hfor feature encoding, as illustrated
in Figure 2.
Boundary Sensitive Feature Interaction Network. In
addition to enhancing boundary responses, integrating lo-
cal features and global representations is crucial for image
forgery detection. This allows for a comprehensive analysis
12525
Transformer
Block
Convolution
BlockConvolution
BlockTransformer
BlockTransformer
BlockTransformer
BlockTransformer
Block
Convolution
BlockConvolution
BlockConvolution
BlockProjection
FCU
C2 C4
Feature Pyramid 
NetworkC2
C3
C4
C3Conv+BN
Sigmoid
Up-Sample
Conv+BNFCU
C5Conv+BN
Sigmoid
Up-Sample
Conv+BN
BOB BOB
(9,/4KZ (U[TJGX_9KTYOZO\K ,KGZ[XK/TZKXGIZOUT4KZ]UXQ
Figure 2. An overview of BSFI-Net. FCU represents the Feature Coupling Unit, and BOB represents the Boundary Oriented Block.
FC
MLP
MLPGT MaskClass Label
Object Annotations
Encoding
;3*8 ;TOLOKJ3GTOV[RGZOUT*OYIXOSOTGZO\K8KVXKYKTZGZOUT
෠ࣦܲܿ ݏ݈ܿ
෠ܲ୭ ݉ܿ݋ࣦ
෠ܿ݋݈ࣦ݉ܲ
Figure 3. The learning of UMDR with multi-scale supervision.
of inconsistencies within the image at various scales. In-
spired by [ 48], we propose a CNN-Transformer concurrent
network called BSFI-Net, which maintains edge sensitivity
while facilitating thorough interaction between features at
different scales in the two branches.
As shown in Figure 2, the CNN branch serves as the main
branch, taking an RGB or noise image as input to encode
local information. The transformer branch, with input as
edge enhancement information H, guides the CNN branch
to focus on tampered regions and transmits long-distanceinconsistencies between image patches to it. We use theFeature Coupling Unit (FCU) proposed by [
48] to elimi-
nate the misalignment between feature maps from the CNN
branch and patch embeddings from the transformer branch.
Moreover, we design a Boundary Oriented Block (BOB)to facilitate transmitting high-level patch consistency andboundary information from the transformer branch to the
CNN branch, guiding the latter.
The CNN branch consists of ﬁve convolution blocks,
similar to the ResNet construction [ 24]. Like [ 16,48],
the transformer branch consists of 5 repeated transformer
blocks, consisting of a multi-head self-attention module and
an MLP block. The same tokenization operation as ViT [ 16]
is adopted. In FCU, 1×1 convolution and re-sampling are
used to align channels and spatial dimensions before addingpatch embeddings and CNN features. In BOB, feature maps
from the CNN branch are fed into a 1×1 convolution layer, abatch normalization layer, a sigmoid layer, and up-sampled
to high resolution by bilinear interpolation. Then, the fea-
tures from the CNN branch are subjected to an element-wise
multiplication with the long-distance discriminate weights.
We pre-train BSFI-Net as a feature encoder to generate RGB
and noise view representation, and two feature pyramids fr,
fnare produced by the Feature Pyramid Network [ 38] based
on the intermediate feature maps {C2,C3,C4,C5}. The
training details are provided in Section 4.1.
3.2. Feature Contrastive Collaboration
In the feature collaboration stage, inspired by [ 51,56],
we ﬁrst employ a Region Proposal Network (RPN) based on
the RGB feature pyramid frto generate a set of Regions of
Interest (RoIs). Then, we utilize RoIAlign [ 25] to extract
the information of RoIs from the feature pyramids frand
fnof two streams. In addition to feature concatenation, we
employ contrastive supervision to promote collaborationbetween two views. We treat the tampered proposals from
different streams as positive proposals, and the tampered
proposals and authentic proposals are assigned as negative
pairs. Following the InfoNCE loss [ 47,67], the contrast loss
is deﬁned as:
Lcon=−1
N/summationdisplay
ilogexp(s0)
exp(s0)+/summationtext
jexp(s1)
−1
N/summationdisplay
ilogexp(s0)
exp(s0)+/summationtext
jexp(s2),(2)
wheres0stands for the similarity between positive pairs, s1
denotes the similarity between RGB tampered embeddings
and noise authentic embeddings, and s2signiﬁes the similar-
ity between RGB authentic embeddings and noise tampered
embeddings. The contrastive loss Lcon is introduced into
the supervision of uniﬁed learning and will be discussed in
Section 3.3.
12526
3.3. Uniﬁed Learning with Multi-Scale Supervision
Transformer Encoder. Our Uniﬁed learning module is
an encoder-only transformer architecture that processes the
fused proposal embeddings di, along with their speciﬁc po-
sitional encoding as input. Within each layer of the trans-
former encoder, self-attention mechanisms aggregate the in-
formation across different proposal embeddings and capture
their long-distance dependencies, implying object consisten-
cies. In detail, we utilize a transformer decoder featuring
six layers, a width of 512, and eight attention heads. Thefeedforward network (FFN) within the transformer has a
hidden size 2048. After the transformer encoder, we gener-
ate the discriminative query embeddings DI, fed into the
uniﬁed manipulation discriminative representation (UMDR)
part to generate predictions for three sub-tasks, viz. objectconsistency modeling, image manipulation detection, and
localization.
Uniﬁed Manipulation Discriminative Representation. Af-
ter the transformer encoder, each tampering discriminative
query in DIrepresents the tampering clues across three
views of the corresponding proposal. Figure 3shows the
learning process of three sub-tasks. UMDR is learned under
the supervision of authenticity classiﬁcation, object consis-
tency modeling, and manipulation localization branches. The
same as DETR [ 9] and SOLQ [ 12], the classiﬁcation branch
is a fully connected (FC) layer to predict the authenticity
conﬁdences ˆPc. The object consistency modeling branch is
a multi-layer perception (MLP) with a hidden size of 256
to predict object spatial information ˆPo. The manipulation
localization branch is also a multi-layer perception with a hid-
den size of 1024 to predict localization mask vector ˆPm. The
supervision for the ﬁrst two branches is similar to DETR[ 9].
In the third branch, we employ the mask vector, obtained by
encoding the ground truth mask, as the supervision informa-
tion. During the inference process, the compressed encoding
procedure is applied to ˆPmfor reconstructing the localiza-
tion mask. In the compression encoding, we utilize Principal
Component Analysis (PCA) to transform 2D spatial binary
masks into 1D mask vectors.
Loss Function. The overall loss function for supervision of
the UnionFormer can be expressed as:
Lunion=λcls·Lcls+Locm+λloc·Lloc+β·Lcon,(3)
where Lclsdenotes the focal loss [ 39] for classiﬁcation. Lloc
denotes the L1loss for localization mask vector supervision.
Lcon is the contrastive learning loss introduced in Section3.2.
λcls,λloc, andβare the corresponding modulation coefﬁ-
cients. The Locm is the loss for object consistency modeling,
which is deﬁned as:
Locm=λL1·LL1+λgious ·Lgious, (4)where LL1andLgious areL1loss and generalized IoU loss
[52], which is the same as DETR. λL1andλgious are corre-
sponding coefﬁcients. Following [ 12],Llocis not included
in the bipartite matching process.
4. Experiments
4.1. Experimental Setup
Training. We used a large-scale training dataset including
various types of tampered and authentic images. It is divided
into ﬁve sections: 1) CASIA v2 [ 14], 2) Fantastic Reality
[32], 3) Tampered COCO, derived from COCO 2017 datasets
[37], 4) Tampered RAISE, constructed based on the RAISE
dataset [ 11], and 5) Pristine images selected from the COCO
2017 and RAISE datasets. We randomly add Gaussian noise
or apply JPEG compression to the synthetic data to simulate
the visual quality and tampering traces in realistic scenarios.
During the training process, we sequentially train BSFI-Net,
RPN, and the entire UnionFormer in three stages.
Testing. To comprehensively evaluate and compare our
model with various state-of-the-art methods, we utilized six
publicly available testing datasets and one more dataset of
hyper-realistic tampered images created by the Blended Dif-
fusion model [ 4]. Speciﬁcally, we employed CASIA v1 [ 14],
Columbia [ 26], Coverage [ 61], NIST16 [ 22], IMD20 [ 46]
and CocoGlide [ 23]. Then, we construct BDNIE, includ-
ing 512 hyper-realistic fake images we generated from the
advanced blended Diffusion model for text-driven natural
image editing. The details of the training and testing data are
provided in the Supplementary.
Evaluation Metric. We evaluated the performance of the
proposed method in the task of image tampering detectionand localization. For the task of localizing image manipu-
lations, we report the pixel-level Area Under Curve (AUC)
and F1 score, using both the best and the ﬁxed 0.5 thresholds.
For the detection task following [ 23], we adopt image-level
AUC and balanced accuracy, which considers both false
alarms and missed detection, in which case the threshold is
set to 0.5. To ensure fairness and accuracy in the compari-
son, some result values for other methods are taken from the
literature [ 23,59].
Implementation Details. The BSFI-Net is trained with
cross-entropy loss for 100 epochs, employing the AdamW
optimizer [ 41], with a batch size of 512 and a weight decay
of 0.05. The initial learning rate is set to 0.001 and decays in
a cosine schedule.
During the training of complete UnionFormer with
Lunion , inspired by [ 56,63], we adopt a 36-epoch ( 3×)
schedule to train the Unionformer for 2.7×105iterations
with batch size 16. An AdamW optimizer is also utilized in
this stage. The learning rate is set to 10−4at the beginning
and multiplied by 0.1 at 1.8×105and2.4×105iterations.
12527
MethodOptimal threshold Fixed threshold (0.5)
Columbia Coverage CASIA v1 NIST16 CoCoGlide A VG Columbia Coverage CASIA v1 NIST16 CoCoGlide A VG
ManTra-Net [ 62] 0.650 0.486 0.320 0.225 0.673 0.471 0.508 0.317 0.180 0.172 0.516 0.339
SPAN [ 27] 0.873 0.428 0.169 0.363 0.350 0.437 0.759 0.235 0.112 0.228 0.298 0.326
MVSS-Net [ 13] 0.781 0.659 0.650 0.372 0.642 0.621 0.729 0.514 0.528 0.320 0.486 0.515
PSCC-Net [ 40] 0.760 0.615 0.670 0.210 0.685 0.588 0.604 0.473 0.520 0.113 0.515 0.445
CA T-Net v2 [ 34] 0.923 0.582 0.852 0.417 0.603 0.675 0.859 0.381 0.752 0.308 0.434 0.547
TruFor [ 23] 0.914 0.735 0.822 0.470 0.720 0.732 0.859 0.600 0.737 0.399 0.523 0.624
Ours 0.925 0.720 0.863 0.489 0.742 0.748 0.861 0.592 0.760 0.413 0.536 0.632
Table 1. Performance of pixel-level F1 with optimal and ﬁxed threshold for image manipulation localization task.
4.2. Comparision with state-of-the-art
Baseline. To ensure a fair and accurate comparison, we only
selected state-of-the-art methods for which authors provided
pre-trained models, released source code, or evaluated un-
der a common criterion [ 27,40,59]. To reduce biases, we
exclusively considered the methods or versions trained onthe datasets that do not overlap with the test datasets. In
detail, we included seven state-of-the-art methods: Mantra-
Net [ 62], SPAN [ 27], PSCC-Net [ 40], MVSS-Net [ 13],
CA T-Net v2 [ 34], ObjectFormer [ 59], and TruFor [ 23].
Localization Results. Table 2and Table 1present the re-
sults of image tampering localization based on pixel-level
AUC and F1 score metrics, respectively. The top-ranking
method is denoted in bold, a horizontal line represents the
second-ranking method, and the same annotation is applied
in Table 4and Table 3. Our method demonstrates the best
performance across all datasets for pixel-level AUC evalu-ation. As for F1 evaluation, our method ranks the best orsecond best across all datasets. On average, we achieved anotable advantage, regardless of using an optimal or ﬁxed
threshold. In fact, on the relatively novel CocoGlide dataset,
which includes diffusion-based local manipulations, we out-
perform the second-placed TruFor by 2.2% and 1.3% on the
two thresholds, respectively. This is due to UnionFormer con-
structing object view artifacts expression, which can reveal
inconsistencies between regions generated with diffusion
models and authentic areas. These comparisons indicate that
our method possesses strong generalization and a superior
ability to capture tampering artifacts.
Detection Results. Table 4indicates the comparative results
for tampering detection. Following [ 23], we use the maxi-
mum value of the localization map as the detection statistic
for methods not explicitly designed for the detection task.
UnionFormer achieves optimal performance on all datasets
except Columbia and demonstrates marked superiority in
average results, whether measured by AUC or balanced ac-
curacy. As mentioned in [ 13,23], accuracy is sensitive to
threshold selection and challenging to determine without a
well-calibrated dataset. However, our method and the second-
placed TruFor have achieved commendable results in this
demanding scenario. We maintain a 2.5% and 2% lead inMethod Columbia Coverage CASIA v1 NIST16 IMD20 A VG
ManTra-Net [ 62] 0.824 0.819 0.817 0.795 0.748 0.801
SPAN [ 27] 0.936 0.922 0.797 0.840 0.750 0.849
PSCC-Net [ 40] 0.982 0.847 0.829 0.855 0.806 0.864
ObjectFormer [ 59] 0.955 0.928 0.843 0.872 0.821 0.884
TruFor [ 23] 0.947 0.925 0.957 0.877 - 0.927
Ours 0.989 0.945 0.972 0.881 0.860 0.929
Table 2. Performance of pixel-level AUC for image manipulation
localization task. The results of Trufor on IMD20 are not reported
because IMD20 is included in its training datasets.
Distortion SPAN PSCC-Net ObjectFormer Ours
w/o distortion 0.8359 0.8547 0.8718 0.8813
Resize( 0.78×) 0.8324 0.8529 0.8717 0.8726
Resize( 0.25×) 0.8032 0.8501 0.8633 0.8719
GSBr(k=3 ) 0.8310 0.8538 0.8597 0.8651
GSB(k=1 5 ) 0.7915 0.7993 0.8026 0.8430
GSN(σ=3 ) 0.7517 0.7842 0.7958 0.8285
GSN(σ=1 5 ) 0.6728 0.7665 0.7815 0.8057
JPEG(q= 100 ) 0.8359 0.8540 0.8637 0.8802
JPEG(q=5 0 ) 0.8068 0.8537 0.8624 0.8797
Table 3. AUC scores for the localization performance on the NIST
16 dataset.
the average AUC and accuracy, respectively. This advan-
tage is primarily attributed to the uniﬁed learning process
of our framework. Uniﬁed learning typically facilitates the
mutual enhancement of localization and detection tasks. The
model’s performance is further enhanced as both sub-tasks
are mastered through a uniﬁed manipulation discriminative
representation.
Robustness Evaluation. We tested the robustness of Union-
Former by applying image distortion to NIST 16 dataset
images. Following [ 40,59], we included four types of dis-
tortions: 1) changing the size of images to different scales;
2) applying Gaussian blur with a kernel size k; 3) adding
Gaussian noise characterized by a standard deviation σ;4 )
applying JPEG compression to the images, utilizing a qual-
ity factor q. We compare the pixel-level AUC performance
with other methods. Table 3show that our method exhibits
robustness to various distortion operations, outperforming
others.
12528
MethodImage-level AUC Accuracy
Columbia Coverage CASIA v1 NIST16 CoCoGlide A VG Columbia Coverage CASIA v1 NIST16 CoCoGlide A VG
ManTra-Net [ 62] 0.810 0.760 0.644 0.624 0.778 0.723 0.500 0.500 0.500 0.500 0.500 0.500
SPAN [ 27] 0.999 0.670 0.480 0.632 0.475 0.651 0.951 0.605 0.487 0.597 0.491 0.626
MVSS-Net [ 13] 0.984 0.733 0.932 0.579 0.654 0.776 0.667 0.545 0.808 0.538 0.536 0.619
PSCC-Net [ 40] 0.300 0.657 0.869 0.485 0.777 0.618 0.508 0.550 0.683 0.456 0.661 0.572
CA T-Net v2 [ 34] 0.977 0.680 0.942 0.750 0.667 0.803 0.803 0.635 0.838 0.597 0.580 0.691
TruFor [ 23] 0.996 0.770 0.916 0.760 0.752 0.839 0.984 0.680 0.813 0.662 0.639 0.756
Ours 0.998 0.783 0.951 0.793 0.797 0.864 0.979 0.694 0.843 0.680 0.682 0.776
Table 4. Performance of image-level AUC and balanced accuracy for image manipulation detection.
&ŽƌŐĞĚ 'd
 DĂŶdƌĂͲEĞƚ ^WE W^ͲEĞƚ Ds^^ͲEĞƚ dƌƵ&Žƌ dͲEĞƚsϮ KƵƌƐ
Figure 4. Qualitative comparison results. The ﬁrst to fourth rows are respectively sourced from CASIA v1 [ 14], Columbia [ 26], Coverage
[61], and IMD20 [ 46]. The last row is from the BDNIE dataset.
Variant ModelsCASIA v1 NIST 16
AUC F1 AUC F1
RGB View (baseline) 0.778 0.701 0.724 0.423RGB+Noise Views 0.865 0.767 0.807 0.448RGB +Noise+Object Views (w/o L
con) 0.950 0.849 0.853 0.472
UnionFormer (w/ ResNet) 0.895 0.786 0.826 0.453UnionFormer (Ours) 0.972 0.863 0.881 0.489
Table 5. Ablation results on CASIA and NIST16 datasets.
4.3. Visualization Results
Qualitative Comparison. Figure 4presents localization
results across various datasets. Our method can accurately
locate tampered regions, predicting more detailed and clear
boundaries. This is due to our multi-view artifacts captureand BSFI-Net, where frequency information boosts edgeresponse, and the interactions between branches enhance
the generalization and discrimination of features. Thanks to
the modeling of object view clues and the uniﬁed learning
framework, our method achieves satisfactory results on the
challenging BDNIE dataset, while other methods fail.
Visualization of Different View Representation. In Figure
5, we visualize noise features and the edge-guided features
of the transformer branch in BSFI-Net. As shown in columnsLocm AUCλloc AUC nv AUC Type AUC
w/ 0.881 0.5 0.802 144 0.824 Sparse 0.847
w/o 0.796 1 0.881 256 0.881 DCT 0.860
- - 2 0.836 400 0.813 PCA 0.881
Table 6. Ablation results for the UMDR on the NIST 16 dataset,
where “ nv” denotes the dimension of the mask vector, and “Type”
indicates the type of compression coding used.
1 to 4, some images may appear natural in the RGB view, but
their tampered/authentic parts are readily distinguished in thefrequency domain or under noise view. Columns ﬁve and six
show the RGB features generated by a single CNN branch
and the dual branch of BSFI-Net. Compared to using onlythe CNN branch, BSFI-Net more accurately activates the
tampered regions, thanks to edge guidance and long-distance
clues provided by the transformer branch.
Furthermore, we quantitatively analyze the object view,
as shown in Figure 6. We derive the afﬁnity matrix Aifrom
the transformer encoder during the uniﬁed learning phase.
Based on Ai, we randomly select a subset of proposal em-
beddings and compute their average afﬁnity with other pro-
posals, denoted as ei.eiis then normalized to the range [0,1]
and used as a color coefﬁcient to visualize proposals, with
12529
&ŽƌŐĞĚ 'd d EŽŝƐĞ EE ^&/ͲEĞƚ WƌĞĚŝĐƚŝŽŶ
Figure 5. Visualization of diverse features. From left to right, we display the forged image, reference mask, edge-guided input of BSFI-Net,
noise view input, CAM of the feature maps from CNN and BSFI-Net, and the prediction mask of UnionFormer.
01ŽďũĞĐƚĂĨĨŝŶŝƚǇ
&ŽƌŐĞĚ DĂƐŬ KďũĞĐƚĨĨŝŶŝƚǇ
Figure 6. Visualization of object view representation. From left to
right, we display the forged image, ground truth mask, and the
visualization of object afﬁnity.
lighter colors indicating lower afﬁnity. The results show that
proposals with forged objects have a lower average afﬁnity
with other regions, demonstrating UMDR’s ability to capture
inconsistencies between real and fake objects.
4.4. Ablation Study
Ablation studies were carried out to assess the impact
of critical components within our approach. The quantita-tive results are listed in Table 5. We can observe that by
adding noise stream on the ﬁrst baseline model, the AUCscores increase by 8.7% on CASIA v1 and 8.3% on NIST
16, while further adding object view representation, the AUC
scores continue to increase by 10.7% on CASIA v1 and 7.4%
on NIST 16. This demonstrates the effectiveness of noise
and object view representations. Moreover, when contrastive
supervision is lacking, or BSFI-Net is replaced with ResNet-
50 [ 24], the model’s performance experiences a signiﬁcant
decline. This highlights the efﬁcacy of the interaction be-
tween the two streams and the exceptional capability of the
BSFI-Net in characterizing forgery artifacts.
The BOB and FCU modules within BSFI-Net improve
the interaction between its two branches and can effectivelyeliminate feature misalignment between them. When BOB
or FCU is removed individually, the overall model’s localiza-
tion AUC scores on the NIST 16 dataset decrease by 4.8%and 6.3% respectively. We further conduct experiments toinvestigate the effect of several key factors in UMDR, viz.
λloc,Locm , the mask vector dimension nv, and the type of
compression coding. We compare three compression coding
methods: Sparse Coding [ 15], Discrete Cosine Transform
(DCT) [ 2], and Principal Component Analysis (PCA) [ 1].
As shown in Table 6, when equipped with contrastive loss,
using PCA as the encoding type, and setting λlocandLocm
to 1 and 256 respectively, the model performs the best on the
NIST 16 dataset.
5. Conclusion
In this paper, we introduced UnionFormer, a uniﬁed-
learning transformer framework that leverages clues fromthree distinct views for image manipulation detection andlocalization. UnionFormer employs BSFI-Net as a feature
encoder to extract highly discriminative features under RGB
and noise views. Then, through a uniﬁed learning process
with three tasks, UnionFormer models the discontinuity be-
tween objects, i.e., object view representation, and learns a
uniﬁed discriminative representation. The uniﬁed represen-
tation integrating information from three views has stronggeneralizability and discrimination. It can accurately iden-
tify various image manipulations, whether traditional manual
editing or natural language-driven tampering based on dif-fusion models. Moreover, the uniﬁed learning frameworkenables the mutual enhancement of sub-tasks, achievinghigh-precision detection and localization. Comprehensive
experiments conducted on various datasets demonstrate the
efﬁcacy of the proposed method.
Acknowledgements. We thank the anonymous reviewers for
their valuable suggestions. This work is funded by the Na-
tional Natural Science Foundation of China (Nos. 62176010,
61771026, U21A20515, 62172416, 62376271), and the
Y outh Innovation Promotion Association of the Chinese
Academy of Sciences (2022131).
12530
References
[1] Herv ´e Abdi and Lynne J Williams. Principal component
analysis. Chemometrics and Intelligent Laboratory Systems ,
2(1):37–52, 1987. 8
[2] N. Ahmed, T. Natarajan, and K.R. Rao. Discrete cosine
transform. IEEE Transactions on Computers , C-23(1):90–93,
1974. 8
[3] Saadaldeen Rashid Ahmed, Emrullah Sonu c¸, Mo-
hammed Rashid Ahmed, and Adil Deniz Duru. Analysissurvey on deepfake detection and recognition with convo-lutional neural networks. In 2022 International Congress
on Human-Computer Interaction, Optimization and Robotic
Applications (HORA) , pages 1–7. IEEE, 2022. 1
[4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 18208–18218, 2022.
2,5
[5] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. ACM Transactions on Graphics (TOG) ,4 2
(4):1–11, 2023. 2
[6] Belhassen Bayar and Matthew C Stamm. A deep learning
approach to universal image manipulation detection usinga new convolutional layer. In Proceedings of the 4th ACM
Workshop on Information Hiding and Multimedia Security ,
pages 5–10, 2016. 1,2
[7] Belhassen Bayar and Matthew C Stamm. Constrained con-
volutional neural networks: A new approach towards general
purpose image manipulation detection. IEEE Transactions on
Information F orensics and Security , 13(11):2691–2706, 2018.
3
[8] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Y an, Y utong Dai,
Philip S Y u, and Lichao Sun. A comprehensive survey ofai-generated content (aigc): A history of generative ai from
gan to chatgpt. arXiv preprint arXiv:2303.04226 , 2023. 1
[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In European confer-
ence on computer vision , pages 213–229. Springer, 2020. 1,
2,5
[10] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,
and Mubarak Shah. Diffusion models in vision: A survey.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 2023. 1
[11] Duc-Tien Dang-Nguyen, Cecilia Pasquini, V alentina Conotter,
and Giulia Boato. Raise: A raw images dataset for digital
image forensics. In Proceedings of the 6th ACM Multimedia
Systems Conference , page 219–224, New Y ork, NY , USA,
2015. Association for Computing Machinery. 5
[12] Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, and
Yichen Wei. Solq: Segmenting objects by learning queries.
Advances in Neural Information Processing Systems , 34:
21898–21909, 2021. 1,5
[13] Chengbo Dong, Xinru Chen, Ruohan Hu, Juan Cao, and
Xirong Li. Mvss-net: Multi-view multi-scale supervised net-
works for image manipulation detection. IEEE Transactionson Pattern Analysis and Machine Intelligence , 45(3):3539–
3553, 2022. 2,6,7
[14] Jing Dong, Wei Wang, and Tieniu Tan. Casia image tam-
pering detection evaluation database. In 2013 IEEE China
Summit and International Conference on Signal and Informa-
tion Processing , pages 422–426, 2013. 5,7
[15] D.L. Donoho. Compressed sensing. IEEE Transactions on
Information Theory , 52(4):1289–1306, 2006. 8
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2,4
[17] Pasquale Ferrara, Tiziano Bianchi, Alessia De Rosa, and
Alessandro Piva. Image forgery localization via ﬁne-grained
analysis of cfa artifacts. IEEE Transactions on Information
F orensics and Security , 7(5):1566–1577, 2012. 2
[18] Jessica Fridrich and Jan Kodovsky. Rich models for steganal-
ysis of digital images. IEEE Transactions on Information
F orensics and Security , 7(3):868–882, 2012. 1,2
[19] Oran Gafni and Lior Wolf. Wish you were here: Context-
aware human generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 7840–7849, 2020. 1
[20] Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Xingqian Xu,
Nicu Sebe, Trevor Darrell, Zhangyang Wang, and Humphrey
Shi. PAIR-Diffusion: A Comprehensive Multimodal Object-
Level Image Editor. arXiv e-prints , art. arXiv:2303.17546,
2023. 2
[21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Y oshua Bengio. Generative adversarial nets. Advances in
Neural Information Processing Systems , 27, 2014. 1
[22] Haiying Guan, Mark Kozak, Eric Robertson, Y ooyoung Lee,
Amy N. Y ates, Andrew Delgado, Daniel Zhou, Timothee
Kheyrkhah, Jeff Smith, and Jonathan Fiscus. Mfc datasets:
Large-scale benchmark datasets for media forensic challenge
evaluation. In 2019 IEEE Winter Applications of Computer
Vision Workshops (WACVW) , pages 63–72, 2019. 5
[23] Fabrizio Guillaro, Davide Cozzolino, Avneesh Sud, Nicholas
Dufour, and Luisa V erdoliva. Trufor: Leveraging all-round
clues for trustworthy image forgery detection and localization.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 20606–20615, 2023. 1,
2,5,6,7
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770–778, 2016. 4,8
[25] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE International
Conference on Computer Vision , pages 2961–2969, 2017. 4
[26] Y u-feng Hsu and Shih-fu Chang. Detecting image splicing
using geometry invariants and camera characteristics consis-
tency. In 2006 IEEE International Conference on Multimedia
and Expo , pages 549–552, 2006. 5,7
12531
[27] Xuefeng Hu, Zhihan Zhang, Zhenye Jiang, Syomantak Chaud-
huri, Zhenheng Y ang, and Ram Nevatia. Span: Spatial pyra-
mid attention network for image manipulation localization.
InComputer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16 ,
pages 312–328. Springer, 2020. 1,2,6,7
[28] Suhaib Wajahat Iqbal and Bhavna Arora. Machine learning
techniques for image manipulation detection: A review and
analysis. In The International Conference on Recent Innova-
tions in Computing , pages 209–224. Springer, 2022. 1
[29] Jindong Jiang, Fei Deng, Gautam Singh, and Sungjin
Ahn. Object-centric slot diffusion. arXiv preprint
arXiv:2303.10834 , 2023. 2
[30] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6007–6017, 2023. 2
[31] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 1
[32] Vladimir V . Kniaz, Vladimir Knyaz, and Fabio Remondino.
The point where reality meets fantasy: Mixed adversarial
generators for image splice detection. In Advances in Neu-
ral Information Processing Systems . Curran Associates, Inc.,
2019. 5
[33] Neal Krawetz and Hacker Factor Solutions. A picture’s worth.
Hacker Factor Solutions , 6(2):2, 2007. 2
[34] Myung-Joon Kwon, Seung-Hun Nam, In-Jae Y u, Heung-Kyu
Lee, and Changick Kim. Learning jpeg compression artifacts
for image manipulation detection and localization. Interna-
tional Journal of Computer Vision , 130(8):1875–1895, 2022.
2,6,7
[35] Haodong Li and Jiwu Huang. Localization of deep inpainting
using high-pass fully convolutional network. In proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion , pages 8301–8310, 2019. 1
[36] Shuaibo Li, Shibiao Xu, Wei Ma, and Qiu Zong. Image ma-
nipulation localization using attentional cross-domain cnn
features. IEEE Transactions on Neural Networks and Learn-
ing Systems , 34(9):5614–5628, 2023. 2
[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C. Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision – ECCV 2014 , pages 740–755, Cham, 2014.
Springer International Publishing. 5
[38] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramidnetworks for object detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 2117–2125, 2017. 4
[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ´ar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE International Conference on Computer
Vision , pages 2980–2988, 2017. 5
[40] Xiaohong Liu, Y aojie Liu, Jun Chen, and Xiaoming Liu. Pscc-
net: Progressive spatio-channel correlation network for image
manipulation detection and localization. IEEE Transactionson Circuits and Systems for Video Technology , 32(11):7505–
7517, 2022. 1,6,7
[41] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations , 2018. 5
[42] Babak Mahdian and Stanislav Saic. Using noise inconsisten-
cies for blind image forensics. Image and vision computing ,
27(10):1497–1503, 2009. 2
[43] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. arXiv preprint arXiv:1411.1784 , 2014. 1
[44] Ron Mokady, Amir Hertz, Kﬁr Aberman, Y ael Pritch, and
Daniel Cohen-Or. Null-text inversion for editing real im-ages using guided diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6038–6047, 2023. 2
[45] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 1
[46] Adam Novozamsky, Babak Mahdian, and Stanislav Saic.
Imd2020: A large-scale annotated dataset tailored for detect-
ing manipulated images. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision
(WACV) Workshops , 2020. 5,7
[47] Aaron van den Oord, Y azhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018. 4
[48] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Y aowei
Wang, Jianbin Jiao, and Qixiang Y e. Conformer: Local fea-
tures coupling global representations for visual recognition.InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 367–376, 2021. 4
[49] Y uyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing
Shao. Thinking in frequency: Face forgery detection by min-
ing frequency-aware clues. In European Conference on Com-
puter Vision , pages 86–103. Springer, 2020. 2
[50] Ali Razavi, Aaron V an den Oord, and Oriol Vinyals. Generat-
ing diverse high-ﬁdelity images with vq-vae-2. Advances in
Neural Information Processing Systems , 32, 2019. 1
[51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. Advances in Neural Information Process-
ing Systems , 28, 2015. 3,4
[52] Hamid Rezatoﬁghi, Nathan Tsoi, JunY oung Gwak, Amir
Sadeghian, Ian Reid, and Silvio Savarese. Generalized in-
tersection over union: A metric and a loss for bounding box
regression. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 658–666,
2019. 5
[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 1
[54] Zenan Shi, Xuanjing Shen, Hui Kang, and Yingda Lv. Im-
age manipulation detection and localization based on the
12532
dual-domain convolutional neural networks. IEEE Access ,6 :
76437–76453, 2018. 2
[55] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price,
Jianming Zhang, Soo Y e Kim, and Daniel Aliaga. Object-
stitch: Object compositing with diffusion model. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 18310–18319, 2023. 2
[56] Zhiqing Sun, Shengcao Cao, Yiming Y ang, and Kris M Ki-
tani. Rethinking transformer-based set prediction for object
detection. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 3611–3620, 2021. 4,
5
[57] Cristian V accari and Andrew Chadwick. Deepfakes and dis-
information: Exploring the impact of synthetic political video
on deception, uncertainty, and trust in news. Social Media+
Society , 6(1):2056305120903408, 2020. 1
[58] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in Neural
Information Processing Systems , 30, 2017. 2
[59] Junke Wang, Zuxuan Wu, Jingjing Chen, Xintong Han, Ab-
hinav Shrivastava, Ser-Nam Lim, and Y u-Gang Jiang. Ob-
jectformer for image manipulation detection and localization.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 2364–2373, 2022. 2,5,
6
[60] Zhengwei Wang, Qi She, and Tomas E Ward. Generative
adversarial networks in computer vision: A survey and tax-
onomy. ACM Computing Surveys (CSUR) , 54(2):1–38, 2021.
1
[61] Bihan Wen, Y e Zhu, Ramanathan Subramanian, Tian-Tsong
Ng, Xuanjing Shen, and Stefan Winkler. Coverage — a novel
database for copy-move forgery detection. In 2016 IEEE
International Conference on Image Processing (ICIP) , pages
161–165, 2016. 5,7
[62] Y ue Wu, Wael AbdAlmageed, and Premkumar Natarajan.
Mantra-net: Manipulation tracing network for detection and
localization of image forgeries with anomalous features. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 9543–9552, 2019. 2,6,
7
[63] Y uxin Wu, Alexander Kirillov, Francisco Massa, Wan-Y en
Lo, and Ross Girshick. Detectron2. https://github.
com/facebookresearch/detectron2 , 2019. 5
[64] Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, and Ani-
mesh Garg. Slotdiffusion: Object-centric generative model-
ing with diffusion models. arXiv preprint arXiv:2305.11281 ,
2023. 2
[65] Binxin Y ang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin
Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by ex-
ample: Exemplar-based image editing with diffusion models.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18381–18391, 2023. 2
[66] Chao Y ang, Huizhou Li, Fangting Lin, Bin Jiang, and Hao
Zhao. Constrained r-cnn: A general image manipulation
detection model. In 2020 IEEE International Conference on
Multimedia and Expo (ICME) , pages 1–6. IEEE, 2020. 1,2[67] Y uyuan Zeng, Bowen Zhao, Shanzhao Qiu, Tao Dai, and Shu-
Tao Xia. Towards effective image manipulation detection
with proposal contrastive learning. IEEE Transactions on
Circuits and Systems for Video Technology , 2023. 1,4
[68] Jiaming Zhang, Huayao Liu, Kailun Y ang, Xinxin Hu, Ruip-
ing Liu, and Rainer Stiefelhagen. Cmx: Cross-modal fusion
for rgb-x semantic segmentation with transformers. IEEE
Transactions on Intelligent Transportation Systems , 2023. 2
[69] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N
Metaxas, and Jian Ren. Sine: Single image editing with text-
to-image diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 6027–6037, 2023. 2
[70] Peng Zhou, Xintong Han, Vlad I. Morariu, and Larry S. Davis.
Learning rich features for image manipulation detection. In
Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , 2018. 2
[71] Peng Zhou, Bor-Chun Chen, Xintong Han, Mahyar Najibi,
Abhinav Shrivastava, Ser-Nam Lim, and Larry Davis. Gen-
erate, segment, and reﬁne: Towards generic manipulationsegmentation. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence , pages 13058–13065, 2020. 1
[72] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable detr: Deformable transformers for
end-to-end object detection. arXiv preprint arXiv:2010.04159 ,
2020. 2
12533
