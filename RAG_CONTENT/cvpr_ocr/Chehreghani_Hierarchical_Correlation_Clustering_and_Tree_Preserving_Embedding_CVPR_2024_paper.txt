Hierarchical Correlation Clustering and Tree Preserving Embedding
Morteza Haghir Chehreghani
Chalmers University of Technology
Gothenburg, Sweden
morteza.chehreghani@chalmers.seMostafa Haghir Chehreghani
Amirkabir University of Technology (Tehran Polytechnic)
Tehran, Iran
mostafa.chehreghani@gmail.com
Abstract
We propose a hierarchical correlation clustering method
that extends the well-known correlation clustering to pro-
duce hierarchical clusters applicable to both positive and
negative pairwise dissimilarities. Then, in the following, we
study unsupervised representation learning with such hier-
archical correlation clustering. For this purpose, we first in-
vestigate embedding the respective hierarchy to be used for
tree preserving embedding and feature extraction. There-
after, we study the extension of minimax distance measures
to correlation clustering, as another representation learn-
ing paradigm. Finally, we demonstrate the performance of
our methods on several datasets.
1. Introduction
Data clustering plays an essential role in unsupervised
learning and exploratory data analysis. It is used in a va-
riety of applications including web mining, network analy-
sis, image segmentation, bioinformatics, user analytics and
knowledge management. Its goal is to partition the data into
groups in a way that the objects in the same cluster are more
similar according to some criterion, compared to the objects
in different clusters.
Many clustering methods partition the data into Kflat
clusters for example, K-means [55], spectral clustering
[62,68] and correlation clustering [8]. In many applications,
however, the clusters are preferred to be presented at differ-
ent levels, encompassing both high-level and detailed infor-
mation. Hierarchical clustering is useful to produce such
structures, usually encoded by a dendrogram . A dendro-
gram is a tree data structure where each node corresponds
to a cluster, with the leaf nodes (those at the bottom of the
tree) containing only one object. Higher-level clusters are
formed by aggregating lower-level clusters and the inter-
cluster dissimilarity between them.
Hierarchical clustering can be performed either in an ag-
glomerative (i.e., bottom-up) or in a divisive (i.e., top-down)
manner [56]. Agglomerative methods are often computa-tionally more efficient, making them more popular in prac-
tice [64]. In both approaches, the clusters are aggregated or
split based on various criteria, such as single ,average ,cen-
troid ,complete andWard . Several studies aim to improve
these methods. The works in [49,52] focus on the statistical
significance of hierarchical clustering. [24,25,65] formulate
this problem as an optimization problem and propose ap-
proximate solutions. [82] considers multiple dissimilarities
for a pair of clusters, and [11, 17] suggest merging multi-
ple clusters at each step instead of one. [6] employs global
information to eliminate the influence of noisy similarities,
and [19] proposes to apply agglomerative methods to small
subsets of the data instead of individual data objects. [33,38]
augment agglomerative methods with probabilistic models,
and finally, [23,60] propose efficient but approximate meth-
ods for hierarchical clustering.
On the other hand, most clustering methods, ei-
ther flat or hierarchical, assume non-negative pairwise
(dis)similarities. However, in several practical applications,
pairwise similarities can be any real number, positive or
negative. For example, it could be preferable for a user or
oracle to indicate whether two objects are similar (consid-
ered a positive relation) or dissimilar (considered a nega-
tive relation), rather than solely providing a positive (non-
negative) pairwise similarity, even if the two objects are dis-
similar. The former approach yields more precise informa-
tion because, in the latter scenario, the dissimilarity between
two objects (i.e., zero similarity) could be confused with a
lack of available information. Some relevant applications
for this setting include image segmentation with higher or-
der correlation information [47,48], webpage segmentation
[12], community detection over graphs [67], social media
mining [73], analysis of connections over web [43], dealing
with attraction/rejection data [26], automated label genera-
tion from clicks [3] and entity resolution [7, 34].
Hence, a specialized clustering model known as correla-
tion clustering has been developed to work with such data.
This model was first introduced on the graphs with only
+1or−1pairwise similarities [7, 8], and then was gen-
eralized to the graphs with arbitrary positive or negative
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23083
edge weights [5, 13, 26]. The original model obtains the
number of clusters automatically. The variant in [20] lim-
its the number of clusters to fixed Kclusters. Semidefi-
nite programming (SDP) relaxation provides tight approxi-
mation bounds in particular for maximizing the agreements
[13, 58], although it is computationally inefficient in prac-
tice [74]. Then, [15,74] provide efficient greedy algorithms
based on local search and Frank-Wolfe optimization with a
fast convergence rate.
However, all of these methods produce flat correlation
clusters. In this paper, we first propose a Hierarchical Cor-
relation Clustering (HCC) method that handles both pos-
itive and negative pairwise (dis)similarities and produces
clusters at different levels (Section 3). To the best of our
knowledge, this work is one the first extensions of the well-
known correlation clustering to hierarchical clustering.1A
hierarchical correlation clustering, also called HCC, is de-
veloped in [76]. This method offers a 0.4767 approxima-
tion, but lacking experimental evaluation. Furthermore, un-
like our method, the method in [76] does not follow the
generic agglomerative clustering procedure.
We note that unlike flat clustering, hierarchical clustering
yields an ordering among the objects, i.e., objects that join
earlier in the hierarchy are closer to each other than those
that join at later steps. This implies that hierarchical clus-
tering induces a new (dis)similarity measure between the
objects, connected to the way the objects join each other to
form clusters at different levels. Thereby, in the following,
we consider two representation learning methods related to
hierarchical clustering and study their adaptation to hierar-
chical correlation clustering. This enables us to not only use
HCC for producing hierarchical clusters, but also to employ
it for computing a suitable similarity/distance measure as an
intermediate data processing step.
One way to perform representation learning from hier-
archical clustering is to compute an embedding that corre-
sponds to the respective hierarchy. Tree preserving embed-
ding [69, 70] is a method that achieves this for the special
case of single linkage method. Later, [21] develops tree pre-
serving embedding for various standard agglomerative clus-
tering methods. We then adapt these works (in particular the
later work [21]) to develop a tree preserving embedding for
HCC dendrograms (Section 4) where the embedded features
can be used as a set of new features for an arbitrary down-
stream task. In this way, we can investigate HCC for the
purpose of computing relevant features for a probabilistic
method such as Gaussian Mixture Model (GMM), instead
of solely using HCC for the purpose of hierarchical clus-
tering. This enables us to apply a method like GMM for
1We note the so-called hierarchical correlation clustering methods pro-
posed in [2, 36, 53] are irrelevant to the well-studied correlation clustering
problem [7,8]; they study for example the correlation coefficients for high-
dimensional data.clustering the pairwise similarities that can be positive or
negative numbers, a task that was not possible before.
Another representation learning paradigm that we study
is called minimax dissimilarity, a graph-based method that
is tightly connected to hierarchical clustering. It provides
a sophisticated way to infer transitive relations and extract
manifolds and elongated clusters in an unsupervised way
[14, 32, 46, 54]. Thereby, for the first time, we study mini-
max dissimilarities on the graphs with positive and negative
(dis)similarities, i.e., with correlation clustering (Section 5).
We show that using minimax dissimilarities with correlation
clustering not only helps for extracting elongated patterns,
but also yields a significant reduction in the computational
complexity, i.e., from NP-hardness to a polynomial runtime.
We finally perform several experiments on various
datasets to demonstrate the effectiveness of our methods in
different settings (Section 6).
2. Notations and Definitions
A dataset is characterized by a set of nobjects with in-
dicesO={1, ..., n}and a pairwise similarity or dissimi-
larity matrix. An n×nmatrix S∈Rn×nrepresents the
pairwise similarities between the objects, whereas, the pair-
wise dissimilarities are shown by matrix D∈Rn×n. Both
of similarities and dissimilarities can be positive or nega-
tive. This property allows us to convert the pairwise simi-
larities to dissimilarities by a simple transformation such as
D=−S, i.e., the pairwise dissimilarities are obtained by
the negation of the similarities and vice versa.2The objects
and the pairwise (dis)similarities are represented by graph
G= (O,S)orG= (O,D).
A cluster is represented by a set, e.g., v, which includes
the objects belong to that. The function dis(u,v)denotes
the inter-cluster dissimilarity between clusters uandvthat
can be defined according to different criteria. A hierarchi-
cal clustering solution can be represented by a dendrogram
Tdefined as a rooted ordered tree such that, i) each node
vinTincludes a non-empty subset of the objects corre-
sponding to a cluster, i.e., v⊆O,|v| ≥1,∀v∈T, with
the leaf nodes including distinct single objects, and ii) the
overlapping clusters are ordered, i.e., ∀u,v∈T,ifu∩v̸=
0,then either u⊆vorv⊆u. The latter condition im-
plies that between every two overlapping nodes an ancestor-
descendant relation holds, i.e., u⊆vindicates vis an an-
cestor of u, anduis a descendant of v.
The clusters at the lowest level, called leafclusters/node,
are the individual distinct objects, i.e., vis a leaf cluster if
2Such a nonparametric transformation resolves the issues related to ob-
taining a proper similarity measure from pairwise dissimilarities. For ex-
ample, with kernels, e.g., RBF kernels, finding the optimal parameter(s)
is often crucial and nontrivial, and the optimal parameters occur inside a
very narrow range [61, 80]. Moreover, the methods we develop in this pa-
per are unaffected by the choice of the transformation D; for example in
Algorithm 1, we only use the pairwise similarities S.
23084
and only if |v|= 1. A cluster at a higher level contains
the union of the objects of its children. The root of a den-
drogram is defined as the cluster at the highest level which
has the maximum size, i.e., all other clusters are its descen-
dants. linkage (v),v∈Treturns the dissimilarity between
the children of vbased on the criterion used to compute
the dendrogram (i.e., dis(cl, cr)where clandcrindicate
the two child clusters of v). For simplicity of explanation,
w.l.g., we assume every non-leaf cluster has two children.
The level of cluster v, i.e., level (v), is determined by
level (v) = max( level (cl), level (cr)) + 1 . (1)
For the leaf clusters, level ()anddis()return 0. Every
connected subtree of Twhose leaf clusters contain only in-
dividual objects from Oconstitutes a dendrogram on this
subset of objects. We require that every common node
present in both Tand the subtree must have the same child
nodes or clusters. We use TTto refer to the set of all
(sub)dendrograms obtained in this way from T.
3. Hierarchical Correlation Clustering
Agglomerative methods begin with each object in a sep-
arate cluster, and then at each round, combine the two
clusters that have a minimal dissimilarity according to a
criterion (defined by the dis(., .)function) until only one
cluster remains. For example, the single linkage (SL)
criterion [71] defines the dissimilarity between two clus-
ters as the dissimilarity between their nearest members
(dis(u,v) = min i∈u,j∈vDi,j), whereas, complete linkage
(CL) [50] uses the dissimilarity between their farthest mem-
bers ( dis(u,v) = max i∈u,j∈vDi,j). On the other hand,
theaverage linkage (AL) criterion [72] considers the av-
erage of the inter-cluster dissimilarities as the dissimilarity
between the two clusters ( dis(u,v) =P
i∈u,j∈vDi,j
|u||v|).
These methods can be shown to be shift-invariant, as men-
tioned in Proposition 1 [18].
Proposition 1 Single linkage, complete linkage and aver-
age linkage methods are invariant w.r.t. the shift of the pair-
wise dissimilarities by an arbitrary real number α.
Thus, we can still use these methods even with possi-
bly negative pairwise dissimilarities as shifting the pairwise
dissimilarities (by a large enough constant) to make them
non-negative does not change the solution.
However, clustering the data consisting of positive and
negative dissimilarities is usually conducted by correlation
clustering . Thus, despite the applicability of single link-
age, average linkage and complete linkage methods, we
propose a novel hierarchical clustering consistent with the
standard correlation clustering, called Hierarchical Correla-
tion Clustering (HCC). This method is thus adapted to pos-
itive/negative pairwise (dis)similarities, and as our experi-ments confirm, it outperforms the other methods (i.e., SL,
CL, and AL) when applied to such data.
The cost function for flat (standard) correlation cluster-
ing accounts for disagreements (i.e., negative similarities in-
side clusters and positive similarities between clusters) and
is written by [20]
RCC(v1, ...,vK;S) =1
2KX
k=1X
i,j∈vk(|Sij| −Sij)
+1
2KX
k=1KX
k′=1,
k′̸=kX
i∈vkX
j∈vk′(|Sij|+Sij),(2)
where Kis the number of clusters and vk’s indicate the
different clusters.
We may rewrite the cost function as
RCC(v1, ...,vK;S) =−1
2KX
k=1KX
k′=1X
i∈vkX
j∈vk′Sij
| {z }
constant
+1
2KX
k=1X
i,j∈vk|Sij|+1
2KX
k=1KX
k′=1,
k′̸=kX
i∈vkX
j∈vk′|Sij|
| {z }
constant
+1
2KX
k=1KX
k′=1,
k′̸=kX
i∈vkX
j∈vk′Sij+1
2KX
k=1KX
k′=1,
k′̸=kX
i∈vkX
j∈vk′Sij.
(3)
We then have
RCC(v1, ...,vK;S) =constant +KX
k=1KX
k′=1,
k′̸=kX
i∈vkX
j∈vk′Sij
≡constant −KX
k=1KX
k′=1,
k′̸=kX
i∈vkX
j∈vk′Dij. (4)
Therefore, correlation clustering aims to minimize the
inter-cluster similarities, and in other words, it maxi-
mizes the inter-cluster dissimilarities. This formulation
in Eq. 4 inspires us for a consistent way of defining a
new inter-cluster dissimilarity function for hierarchical (ag-
glomerative) correlation clustering of positive and negative
(dis)similarities. At each step, we merge the two clusters
that have a minimal dissimilarity (or a maximal similarity),
where we define the dissimilarity between the two clusters
23085
uandvas
disCC(u,v) =X
i∈uX
j∈vDij=−X
i∈uX
j∈vSij.(5)
We emphasize that HCC is consistent with the generic
agglomerative clustering framework applied with, for ex-
ample, single linkage, average linkage, complete linkage
and other criteria. The only difference is the definition of
the inter-cluster dissimilarity function where with HCC we
usedisCC(., .)defined in Eq. 5 (inspired from the cost
function of flat correlation clustering). Other than this, the
algorithmic procedure is consistent. Algorithm 1 in Ap-
pendix A describes the pseudocode of the HCC algorithm.
4. Feature Extraction from HCC
As mentioned, HCC represents the relations between ob-
jects according to the way they join to form the hierarchy.
In this section, we use this intuition and develop a data rep-
resentation consistent with HCC. For this purpose, we adapt
the methods in [69, 70] and in particular [21] to our setting.
Hereby, we first introduce distance functions over HCC, and
then, investigate the embedding of such a distance func-
tion. This procedure leads to obtaining a set of features
from HCC for each object which then can be used in the
downstream task.
4.1. Distance functions over HCC
Given dendrogram T, each cluster v∈Trepresents the
root of a dendrogram T′∈ TT.T′admits the properties
of its root cluster, i.e., level (T′) = max u∈T′level (u) =
level (v)andlinkage (T′) = max u∈T′linkage (u) =
linkage (v), since the root cluster has the maximum link-
age and level among the clusters in T′. Hence, in this way,
we define functions such as level ()andlinkage ()for the
dendrograms as well.
Thelinkage ()function may seem to be a natural choice
for defining a distance function over a HCC dendrogram.
Specifically, one can define the dendrogram-based distance
function Xijover dendrogram Tbetween i, j∈Oas
Xij= min linkage (T′)s.t.i, j∈T′,andT′∈ TT.
(6)
This choice corresponds to the linkage of the smallest
cluster that includes both iandj. This in particular makes
sense for the single linkage dendrogram, and it would be
consistent with the tree-preserving embedding in [69, 70].
If the original dissimilarity matrix Dcontains negative val-
ues, then using Proposition 1, one can sufficiently shift the
pairwise dissimilarities to make all of them non-negative,
without changing the structure of the dendrogram and the
order of the clusters. Therefore, the conditions for a valid
distance function including non-negativity still hold.However, for the HCC dendrogram, the linkage func-
tion might not fulfill the conditions for a distance function.
For example, consider a set of nobjects where all the pair-
wise similarities are +1, i.e., the dissimilarities are thus −1.
Then, the linkage function will always return negative val-
ues which would violate the non-negativity condition of a
valid distance function. On the other hand, the HCC link-
agedisCC(., .)isnot shift-invariant (similar to the standard
flat correlation clustering [18]) and we cannot use the shift
trick in Proposition 1. Let Dαshows the shifted pairwise
dissimilarities, i.e., Dα
i,j=Dij+α. Then, disCC(u,v)
between two clusters uandvbased on Dαis given by
disCC(u,v) =X
i∈u,j∈vDα
i,j=X
i∈u,j∈v(Di,j+α)
=X
i∈u,j∈vDi,j+α|u||v|. (7)
With α > 0, this shift would induce a bias for the HCC
linkage to choose imbalanced clusters. In other words,
disCC(., .)is not shift-invariant and we cannot shift the
pairwise dissimilarities in Dto make them nonnegative.
Therefore, we consider another choice, i.e., the level ()
function used in [21]. It is nonnegative and satisfies the
desired conditions. Then, Xijis now computed by
Xij= min level (T′)s.t.i, j∈T′andT′∈ TT.(8)
Intuitively, Eq. 8 selects the level of the smallest clus-
ter/dendrogram that contains both iandj. The lower the
level at which the two objects join, the greater the similarity
or proximity between them, indicating a closer relationship
in the hierarchical clustering structure. In other words, a
higher level in the dendrogram signifies a later fusion of the
two objects, suggesting that they share fewer common char-
acteristics compared to objects fused at lower levels.
4.2. Embedding the HCC-based distances
After applying the distance function in Eq. 8, we obtain
ann×nmatrix representing pairwise HCC-based distances
among objects. It is usually preferred to obtain vector-based
representations for objects rather than pairwise distances.
Models like Gaussian Mixture Models (GMMs) which in-
volve mixture density estimation (see, e.g., [75]), can only
be applied to vectors. Additionally, working with vector-
based data simplifies feature selection. Hence, it is desired
to compute an embedding of the objects into a new space, so
that their pairwise squared Euclidean distances in the new
space match their pairwise distances obtained from the den-
drogram.
The matrix of pairwise distances Xobtained via Eq. 8
induces an ultrametric [21,51]. The primary distinction be-
tween a metric and an ultrametric is that the addition oper-
ation in the triangle inequality for a metric is replaced by a
maximum operation, i.e., with ultrametric we have
23086
∀i, j, k :Xij≤max(Xik,Xkj). (9)
The connection between ultrametric and trees is well-
established in mathematics [42,57]. Here we instantiate it to
our setting via making the argument in [21] more accurate.
It is evident that when Xij≤Xik, the inequality in Eq.
9 is satisfied. Conversely, if Xij>Xik, it implies that ob-
jectsiandkare included in the same cluster (shown by ci,k)
before iandjjoin (to form cluster ci,j). The bottom-up hi-
erarchical clustering process then continues until ci,j,k is
formed, encompassing all three objects i, j, k . Notice that i
andkjoinjsimultaneously via ci,k. In this case, according
to Eqs. 8 and 9, and the relationships illustrated in Figure 1,
we conclude
Xij=Xkj≤max(Xik,Xkj). (10)
Figure 1. Illustration of ultrametric property of X.
Ultrametric matrices, in turn, exhibit positive definite-
ness [31, 78], and such positive definite matrices result in
inducing an Euclidean embedding [66]. Thereby, after en-
suring the existence of such an embedding, we can employ
a proper method to compute it. Specifically, we use the
method proposed in [83] known as multidimensional scal-
ing[59]. This method proposes first centering Xto obtain
a Mercer kernel and then performing an eigenvalue decom-
position. It works as follows.
1. We center Xby
B← −1
2(In−1
nLn)X(In−1
nLn), (11)
where Inis an identity matrix of size n×nandLnrep-
resents an n×nmatrix filled entirely with ones. With
this centering, the sum of both the rows and columns in
matrix Bbecomes zero.
2. With applying the transformation in step 1, Bbecomes a
positive semidefinite matrix, i.e., all the eigenvalues are
nonnegative. Thus, we decompose Binto its eigenbasis:
B=YZYT, (12)
where Y= (y1, ...,yn)represents the eigenvectors yi
andZ=diag (z1, ..., z n)is a diagonal matrix of eigen-
values z1≥...≥zl≥zl+1= 0 = ...=zn.3. We calculate the n×lmatrix Aas the matrix of new data
features:
A=Yl(Zl)1/2, (13)
withYl= (y1, ...,yl)andZl=diag (z1, ..., z l),
where lspecifies the dimensionality of the new vectors.
In the embedded space, the new dimensions are arranged
based on their corresponding eigenvalues. One can opt to
select only the most significant ones, rather than utilizing
all of them. Therefore, computing such an embedding also
offers the benefit of feature selection.
We also note that many clustering methods can be writ-
ten in matrix factorization form via for example spectral K-
means [28]. This induces an embedding and hence a set
of relevant features. However, for general positive/negative
similarity matrices no exact embedding might be feasible
due to violating positive semidefiniteness. The method that
we described here provides a solution to this challenge, i.e.,
enables us to extract features when the base pairwise simi-
larities are positive and negative.
5. Correlation Clustering and Minimax Dis-
similarities
Finally, we study minimax dissimilarities for correlation
clustering, a graph-based method that corresponds to con-
structing a hierarchical clustering.
Given graph G(O,D), the minimax (MM) dissimilarity
between iandjis defined as
DMM
ij = min
p∈Pij(G)max
1≤l≤|p|−1Dp(l)p(l+1),(14)
where Pij(G)is the set of all paths between iandjover
G(O,D). Each path pis specified by a sequence of object
indices, i.e., p(l)indicates the lthobject on the path.
Minimax dissimilarities enable a clustering algorithm to
capture the inherent patterns and manifolds in an unsuper-
vised and nonparametric way by extracting the transitive
connections [16, 32]. For example, if object iis similar to
object j,jis similar to k, and kis similar to l, then the
minimax dissimilarity between iandlwill be small, even
though their direct dissimilarity might be large. The rea-
son is that minimax dissimilarity finds the connectivity path
i→j→ ··· → k→land connects iandlvia this path.
This property is helpful in finding elongated clusters and
manifolds of arbitrary shapes in an unsupervised way.
Minimax dissimilarities have been so far solely used
with nonnegative pairwise dissimilarities. In the case of
possible negative dissimilarities, we may use a trick simi-
lar to Proposition 1. As shown in Lemma 1, minimax paths
are invariant w.r.t. the shift of the pairwise dissimilarities.
Lemma 1 Consider graphs G(O,D)andGα(O,Dα),
where the pairwise dissimilarities (edge weights) in
23087
Gα(O,Dα)are shifted by constant α, i.e.,Dα
i,j=Di,j+α.
Then, the minimax paths between every pair of objects iand
jare identical on graphs G(O,D)andGα(O,Dα).
All the proofs are in Appendix B. Hence, given a dissimi-
larity matrix D, one can subtract α:=min(D)from all the
elements to obtain Dα. Then, the minimax dissimilarities
can be computed from Gα(O,Dα). After computing the
minimax dissimilarities from Gα, we may add αto all the
pairwise minimax dissimilarities. We can obtain the mini-
max similarities SMM
ij viaSMM
ij=−DMM
ij, if needed.
We demonstrate that for correlation clustering there ex-
ists a simpler method to calculate the minimax dissimilar-
ities intended for use in correlation clustering. According
to Theorem 1, performing correlation clustering on mini-
max dissimilarities can be achieved in polynomial time via
computing the connected components of the unweighted
graphG(O,S′), where the similarity matrix S′is obtained
byS′
ij= 1ifSij>0, andS′
ij= 0otherwise.
Theorem 1 The optimal clusters of the correlation cluster-
ing on graph G(O,SMM)are equal to the connected com-
ponents of graph G(O,S′).
As mentioned, correlation clustering on an arbitrary sim-
ilarity matrix Sis NP-hard [8, 26]. Therefore, using min-
imax (dis)similarities with correlation clustering not only
helps for extracting elongated complex patterns, but also
yields a significant reduction in the computational complex-
ity, i.e., from NP-hardness to a polynomial runtime.
Among the approximate algorithms proposed for corre-
lation clustering on complete graphs with discrete weights,
the method in [5] provides a 3-factor approximation. with
a randomly selected unclustered object in the graph, this
method greedily finds the object’s positive neighbors (those
with similarity +1) to form a new cluster. Then, it repeats
this procedure for the remaining objects. One can con-
clude that in the optimal solution of correlation clustering
on graph G(O,SMM), only the positive neighbors of an ob-
ject will be in the same cluster as the object is, i.e., interest-
ingly the 3-factor approximation algorithm in [5] becomes
exact when applied to G(O,SMM)(Theorem 2).
Theorem 2 Assume the edge weights of graph G(O,S)are
either +1or−1. Then, the approximate algorithm in [5]
is exact when applied to the minimax similarities, i.e., to
graphG(O,SMM).
6. Experiments
In this section, we describe our experimental results on
various datasets. We compare our methods with single link-
age (SL), complete linkage (CL) and average linkage (AL).3
3Some criteria, e.g. centroid ,median andWard compute a representa-
tive for each cluster and then compute the inter-cluster dissimilarities byAs mentioned, there are several improvements over these
basic methods. However, such contributions are orthogonal
to our contribution. Moreover, it is unclear how such im-
provements can be extended to the dissimilarities that can
be both positive and negative. Thus, we limit our baselines
to these methods which as mentioned in Proposition 1, are
applicable to such data.
In our studies, we have access to the true labels. There-
fore, consistent with several previous studies on hierarchical
clustering, e.g. [4, 9, 29], we evaluate the results according
to the following criteria: i) Normalized Mutual Information
(MI) [79] that measures the mutual information between the
true and the estimated clustering solutions, and ii) Normal-
ized Rand score (Rand) [41] that obtains the similarity be-
tween the two solutions. We do not use the labels to infer
the clustering solution, they are only used for evaluation.
Therefore, we are still in the unsupervised setting where the
ground-truth labels play the role of an external evaluator.
In Appendix C, we describe additional experimental re-
sults, in particular on datasets from other domains.
6.1. HCC on UCI data
We first investigate the hierarchical correlation cluster-
ing on the following six UCI datasets [45]. (i) Breast Tis-
sue: includes electrical impedance measurements of freshly
excised 106tissue samples from the breast. The number of
clusters is 6. (ii) Cardiotocography : contains 2126 mea-
surements of fetal heart rate and uterine contraction fea-
tures on cardiotocograms in 10clusters. (iii) Image Seg-
mentation : contains 2310 samples from images of 7out-
door clusters. (iv) ISOLET :7797 samples consisting of
spoken attributes of different letters ( 26clusters). (v) Leaf :
340 images of leaf specimens originating from 40differ-
ent plant species (clusters) each described by 16attributes.
(vi)One-Hundred Plant :1600 samples of leafs each de-
scribed by 64features, from in total 100types (clusters).
The ground-truth labels are shown by c∗, i.e.,c∗
ishows the
true label for object i. We assume an oracle reveals the pair-
wise similarities Saccording to the (flip) noise parameter
η. Ifc∗
i=c∗
jthenSi,j=U(0,1)with probability 1−η
andSi,j=U(−1,0)with probability η. Ifc∗
i̸=c∗
jthen
Si,j=U(−1,0)with probability 1−ηandSi,j=U(0,1)
with probability η. The function U(., .)returns a uniform
random number within the specified range. For each ηwe
repeat the experiments 20times and report the average re-
sults. This setup provides a systematic approach to study
the robustness of various methods to noise.
Figure 2 shows the results for different datasets as a func-
tion of the noise level ηw.r.t. MI. Rand scores shown in
Figure 5 in Appendix C exhibit a consistent behavior. We
the distances between the representatives. Computing such representatives
might not be feasible for possibly negative pairwise dissimilarities. Thus,
we do not consider them.
23088
(a) [Breast Tissue ]
 (b)Cardio
 (c)Image Segmentation
 (d)ISOLET
 (e)Leaf
 (f)One-Hundred Plant
Figure 2. MI score of different hierarchical clustering methods applied to UCI datasets, where x-axis shows the parameter η.
Table 1. Performance of different tree preserving embedding methods on UCI datasets applied with GMM.
Breast Tissue Cardiotocography Image Segm. ISOLET Leaf One-Hun. Plant
method MI Rand MI Rand MI Rand MI Rand MI Rand MI Rand
SL 0.006 0.008 0.007 0.007 0.008 0.001 0.009 0.016 0.008 0.003 0.015 0.020
SL+GMM 0.093 0.077 0.120 0.135 0.239 0.250 0.192 0.174 0.155 0.161 0.083 0.077
CL 0.227 0.166 0.077 0.056 0.187 0.125 0.057 0.017 0.081 0.038 0.029 0.008
CL+GMM 0.251 0.171 0.081 0.060 0.201 0.154 0.061 0.043 0.140 0.129 0.054 0.049
AL 0.542 0.519 0.391 0.479 0.518 0.495 0.257 0.165 0.181 0.106 0.066 0.023
AL+GMM 0.550 0.513 0.422 0. 463 0.522 0.501 0.240 0.179 0.152 0.143 0.081 0.065
HCC 0.903 0.900 0.987 0.994 0.945 0.943 0.938 0.918 0.429 0.373 0.159 0.104
HCC+GMM 0.914 0.911 0.979 0.974 0.960 0.966 0.941 0.917 0.462 0.401 0.183 0.217
observe that among different methods, HCC performs sig-
nificantly better and produces more robust clusters w.r.t. the
noise parameter η. The results on Leaf andOne-Hundred
Plant are worse with all the methods. The reason is that
these datasets are complex, having many clusters (respec-
tively, 40and100 clusters) and fairly a small number of
objects per cluster.
6.2. Tree preserving embedding on UCI data
In the following, we investigate tree preserving embed-
ding and feature extraction. After computing the embed-
dings from different hierarchical clustering methods, we ap-
ply Gaussian Mixture Model (GMM) to the extracted fea-
tures and evaluate the final clustering using the ground-truth
solution. This kind of embedding enables us to apply meth-
ods like GMM to positive and negative pairwise similari-
ties, a task that was not possible before. Since the extracted
features appear in the form of vectors, thus, the final clus-
tering method is not limited to GMM, and in principle, any
numerical clustering can benefit from this embedding.
Table 1 shows the results for different UCI datasets.
We observe that the embeddings obtained by HCC (i.e.,
‘HCC+GMM’) yield significantly better results compared
to the embeddings from other methods (i.e., ‘SL+GMM’,
‘CL+GMM’ and ‘AL+GMM’). It is worth noting that the
results of embeddings (e.g., ‘HCC+GMM’) typically sur-
pass the results obtained from the hierarchical clustering
alone (e.g., ‘HCC’). This observation supports the idea that
employing HCC to compute an embedding (for extracting
new features) for a clustering method such as GMM maybe advantageous, yielding superior results compared to us-
ing HCC exclusively for clustering purposes. This verifies
why tree preserving embedding can be effective in general.
6.3. Experiments on Fashion-MNIST
Next, we investigate HCC and tree-preserving embed-
ding on two randomly selected subsets of Fashion-MNIST
dataset [81]. Fashion MNIST consists of 28×28images
of Zalando’s articles. Each subset consists of 5,000sam-
ples/objects, where we compute the pairwise cosine simi-
larities between them and then apply different methods. Ta-
ble 2 shows the performance of different methods on these
datasets. We observe that, consistent with the previous ex-
periments, both ‘HCC’ and ‘HCC+GMM’ yield improving
the results compared to the baselines. Furthermore, employ-
ing HCC to compute intermediate features for GMM (i.e.,
‘HCC+GMM’) achieves higher scores compared to using
‘HCC’ alone for generating final clusters.
6.4. Correlation clustering with minimax dissimi-
larities
Finally, we investigate the use of minimax dissimilar-
ities with correlation clustering. As mentioned, minimax
dissimilarities are especially useful for extracting elongated
and complex patterns in an unsupervised way. Thereby, we
apply ‘minimax + correlation clustering’ to a number of
datasets with visually elongated and arbitrarily shaped clus-
ters, and compare the results with ‘correlation clustering’
alone. The datasets are shown in Figure 3. For each dataset,
we simply construct the K-nearest neighbor graph (using
23089
Table 2. Performance of different methods on MNIST and Fashion
MNIST datasets. The embeddings by HCC yield better results.
Fashion MNIST 1 Fashion MNIST 2
method MI Rand MI Rand
SL 0.322 0.206 0.241 0.196
SL+GMM 0.411 0.335 0.384 0.340
CL 0.403 0.293 0.546 0.379
CL+GMM 0.478 0.426 0.574 0.362
AL 0.464 0.468 0.602 0.534
AL+GMM 0.608 0.551 0.647 0.553
HCC 0.499 0.475 0.666 0.557
HCC+GMM 0.581 0.586 0.693 0.548
(a) DS1 [44, 63]
 (b) DS2 [22]
(c) DS3 [1]
 (d) DS4 [1]
Figure 3. The datasets with arbitrarily shaped clusters, where
‘minimax + correlation clustering’ acheives perfect clustering.
Figure 4. Embeddings of vehicle motion trajectories computed via
dynamic time wrapping and t-SNE [27, 39].
the ordinary Euclidean distance with a typical K such as 3).
The edges in the nearest neighborhood receive a positive
weight (e.g., +1), and a negative weight (e.g., -1) otherwise.
We then apply either ‘minimax + correlation clustering’ or
‘correlation clustering’ to the resultant graph. We observe
that for all the datasets, ‘minimax + correlation clustering’yields the perfect clustering, i.e., MI and Rand are equal to
1. Whereas with ‘correlation clustering’ alone these scores
are very low. We acknowledge the existence of other clus-
tering methods, such as DBSCAN [30], capable of achiev-
ing perfect clustering on these datasets. However, these
methods often involve crucial hyperparameters , the tuning
of which can be challenging in unsupervised learning. To
our knowledge, in addition to computational and theoreti-
cal benefits, ‘minimax + correlation clustering’ stands out
as the sole method capable of achieving perfect results on
datasets with elongated clusters of arbitrary shapes, and ful-
fills the following two promises: i) it eliminates the need to
fix critical hyperparameters, a task often intricate in unsu-
pervised learning , and ii) it automatically determines the
correct number of clusters without requiring prior fixing .
In the following, we consider the interesting application
of clustering vehicle motion trajectories for ensuring safety
in self-driving [39]. In this application, 1,024trajectories
consisting of drive-by left, drive-by right and cut-in types
are prepared, where some of them are collected in real-
world and some others are generated using Recurrent Auto-
Encoder GAN [27]. Next, dynamic time wrapping [10] and
t-SNE [77] are employed to map the temporal data onto a
two-dimensional space, as illustrated in Figure 4 [27, 39].
We then apply ‘minimax + correlation clustering’ to this
data and compare it with ‘correlation clustering’ . Similar
to the datasets in Figure 3, ‘minimax + correlation cluster-
ing’ achieves perfect clustering with MI = Rand = 1 and
accurately determines the correct number of clusters.
7. Conclusion
We proposed a new hierarchical clustering method,
called HCC, suitable when the (dis)similarities can be pos-
itive or negative. This method is consistent with the gen-
eral algorithmic procedure for agglomerative clustering and
only differs in the way the inter-cluster dissimilarity func-
tion is defined. We then considered embedding the HCC
dendrograms, which provides extracting useful features to
apply for example GMM for clustering positive and nega-
tive similarities. In the following, we studied the use of min-
imax dissimilarities with correlation clustering and showed
that it yields reduction in the computational complexity, in
addition to a possibility for extracting elongated manifolds.
Finally, we demonstrated the effectiveness of the methods
on several datasets in different settings.
Acknowledgement
The work of Morteza Haghir Chehreghani was partially
supported by the Swedish Research Council VR (grant
number 2023-04809) and the Wallenberg AI, Autonomous
Systems and Software Program (WASP) funded by the Knut
and Alice Wallenberg Foundation.
23090
References
[1] Clustering datasets. https : / / github . com /
milaan9/Clustering-Datasets/ . 8
[2] Elke Achtert, Christian B ¨ohm, Peer Kr ¨oger, and Arthur
Zimek. Mining hierarchies of correlation clusters. In
18th International Conference on Scientific and Statistical
Database Management, SSDBM , pages 119–128, 2006. 2
[3] Rakesh Agrawal, Alan Halverson, Krishnaram Kenthapadi,
Nina Mishra, and Panayiotis Tsaparas. Generating labels
from clicks. In International Conference on Web Search and
Web Data Mining, WSDM , pages 172–181, 2009. 1
[4] Julien Ah-Pine. An efficient and effective generic agglomer-
ative hierarchical clustering approach. J. Mach. Learn. Res. ,
19:42:1–42:43, 2018. 6
[5] Nir Ailon, Moses Charikar, and Alantha Newman. Aggre-
gating inconsistent information: Ranking and clustering. J.
ACM , 55(5):23:1–23:27, 2008. 2, 6, 13
[6] Maria-Florina Balcan, Yingyu Liang, and Pramod Gupta.
Robust hierarchical clustering. J. Mach. Learn. Res. ,
15(1):3831–3871, 2014. 1
[7] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation
clustering. In 43rd Symposium on Foundations of Computer
Science FOCS , 2002. 1, 2
[8] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation
clustering. Machine Learning , 56(1-3):89–113, 2004. 1, 2,
6
[9] MohammadHossein Bateni, Soheil Behnezhad, Mahsa Der-
akhshan, MohammadTaghi Hajiaghayi, Raimondas Kiveris,
Silvio Lattanzi, and Vahab S. Mirrokni. Affinity clustering:
Hierarchical clustering at scale. In Advances in Neural Infor-
mation Processing Systems (NIPS) , pages 6864–6874, 2017.
6
[10] R. Bellman and R. Kalaba. On adaptive control processes.
Automatic Control, IRE Transactions on , 4(2):1–9, 1959. 8
[11] Michel Bruynooghe. M ´ethodes nouvelles en classification
automatique de donn ´ees taxinomiques nombreuses. Statis-
tique et analyse des donn ´ees, 2(3):24–42, 1977. 1
[12] Deepayan Chakrabarti, Ravi Kumar, and Kunal Punera. A
graph-theoretic approach to webpage segmentation. In In-
ternational Conference on World Wide Web, WWW , pages
377–386, 2008. 1
[13] Moses Charikar, Venkatesan Guruswami, and Anthony
Wirth. Clustering with qualitative information. In 44th Sym-
posium on Foundations of Computer Science (FOCS) , pages
524–533, 2003. 2
[14] Morteza Haghir Chehreghani. Classification with mini-
max distance measures. In Proceedings of the Thirty-First
AAAI Conference on Artificial Intelligence , pages 1784–
1790, 2017. 2
[15] Morteza Haghir Chehreghani. Clustering by shift. In IEEE
International Conference on Data Mining (ICDM) , pages
793–798, 2017. 2
[16] Morteza Haghir Chehreghani. Unsupervised representation
learning with minimax distance measures. Mach. Learn. ,
109(11):2063–2097, 2020. 5[17] Morteza Haghir Chehreghani. Reliable agglomerative clus-
tering. In International Joint Conference on Neural Net-
works, IJCNN , pages 1–8. IEEE, 2021. 1
[18] Morteza Haghir Chehreghani. Shift of pairwise similarities
for data clustering. Mach. Learn. , 112(6):2025–2051, 2023.
3, 4, 14
[19] Morteza Haghir Chehreghani, Hassan Abolhassani, and
Mostafa Haghir Chehreghani. Improving density-based
methods for hierarchical clustering of web pages. Data
Knowl. Eng. , 67(1):30–50, 2008. 1
[20] Morteza Haghir Chehreghani, Alberto Giovanni Busetto, and
Joachim M. Buhmann. Information theoretic model valida-
tion for spectral clustering. In International Conference on
Artificial Intelligence and Statistics (AISTATS) , 2012. 2, 3
[21] Morteza Haghir Chehreghani and Mostafa Haghir
Chehreghani. Learning representations from dendro-
grams. Mach. Learn. , 109(9-10):1779–1802, 2020. 2, 4,
5
[22] Dongdong Cheng, Qingsheng Zhu, Jinlong Huang, Quan-
wang Wu, and Lijun Yang. A novel cluster validity index
based on local cores. IEEE Trans. Neural Networks Learn.
Syst., 30(4):985–999, 2019. 8
[23] Michael Cochez and Hao Mou. Twister tries: Approximate
hierarchical agglomerative clustering for average distance in
linear time. In International Conference on Management of
Data (ACM SIGMOD) , pages 505–517, 2015. 1
[24] Vincent Cohen-Addad, Varun Kanada, Frederik Mallmann-
Trenn, and Claire Mathieu. Hierarchical clustering: Objec-
tive functions and algorithms. In SODA , pages 378–397,
2018. 1
[25] Vincent Cohen-Addad, Varun Kanade, and Frederik
Mallmann-Trenn. Hierarchical clustering beyond the worst-
case. In Advances in Neural Information Processing Systems
(NIPS) , pages 6201–6209. 2017. 1
[26] Erik D. Demaine, Dotan Emanuel, Amos Fiat, and Nicole
Immorlica. Correlation clustering in general weighted
graphs. Theor. Comput. Sci. , 361(2-3):172–187, 2006. 1,
2, 6
[27] Andreas Demetriou, Henrik Alfsv ˚ag, Sadegh Rahrovani, and
Morteza Haghir Chehreghani. A deep learning framework
for generation and analysis of driving scenario trajectories.
SN Comput. Sci. , 4(3):251, 2023. 8
[28] Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis. Kernel
k-means: spectral clustering and normalized cuts. In Tenth
ACM International Conference on Knowledge Discovery and
Data Mining, SIGKDD , pages 551–556, 2004. 5
[29] Laxman Dhulipala, David Eisenstat, Jakub Lacki, Vahab S.
Mirrokni, and Jessica Shi. Hierarchical agglomerative graph
clustering in nearly-linear time. In International Conference
on Machine Learning (ICML) , 2021. 6
[30] Martin Ester, Hans-Peter Kriegel, J ¨org Sander, and Xiaowei
Xu. A density-based algorithm for discovering clusters in
large spatial databases with noise. In Proceedings of the Sec-
ond International Conference on Knowledge Discovery and
Data Mining (KDD-96) , pages 226–231, 1996. 8
[31] Miroslav Fiedler. Ultrametric sets in euclidean point spaces.
ELA. The Electronic Journal of Linear Algebra , 3:23–30,
1998. 5
23091
[32] Bernd Fischer and Joachim M. Buhmann. Path-based clus-
tering for grouping of smooth curves and texture segmenta-
tion. IEEE Trans. Pattern Anal. Mach. Intell. , 25(4):513–
518, 2003. 2, 5
[33] Chris Fraley and Adrian E Raftery. Model-based clustering,
discriminant analysis, and density estimation. Journal of the
American Statistical Association , 97:611–631, 2002. 1
[34] Lise Getoor and Ashwin Machanavajjhala. Entity resolution:
Theory, practice & open challenges. Proc. VLDB Endow. ,
5(12), 2012. 1
[35] C. Gower and G. J. S. Ross. Minimum spanning trees and
single linkage cluster analysis. Journal of the Royal Statisti-
cal Society. Series C , 18(1), 1969. 12
[36] Yi Gu and Chaoli Wang. A study of hierarchical correlation
clustering for scientific volume data. In ISVC , pages 437–
446, 2010. 2
[37] Haghir Chehreghani, Morteza. Information-theoretic valida-
tion of clustering algorithms . PhD thesis, 2013. 14
[38] Nicholas A. Heard. Iterative reclassification in agglomer-
ative clustering. Journal of Computational and Graphical
Statistics , 20(4):920–936, 2012. 1
[39] Fazeleh Sadat Hoseini, Sadegh Rahrovani, and
Morteza Haghir Chehreghani. Vehicle motion trajecto-
ries clustering via embedding transitive relations. In 24th
IEEE International Intelligent Transportation Systems
Conference, ITSC , pages 1314–1321. IEEE, 2021. 8
[40] T.C. Hu. The maximum capacity route problem. Operations
Research , 9:898–900, 1961. 12
[41] L. Hubert and P. Arabie. Comparing partitions. Journal of
classification , 2(1):193–218, 1985. 6
[42] Bruce Hughes. Trees and ultrametric spaces: a categori-
cal equivalence. Advances in Mathematics , 189(1):148–191,
2004. 5
[43] Dmitri V . Kalashnikov, Zhaoqi Chen, Sharad Mehrotra, and
Rabia Nuray-Turan. Web people search via connection anal-
ysis. IEEE Trans. Knowl. Data Eng. , 20(11):1550–1565,
2008. 1
[44] George Karypis. Cluto - a clustering toolkit, 2002. Retrieved
from the University of Minnesota Digital Conservancy. 8
[45] Markelle Kelly, Rachel Longjohn, and Kolby Notting-
ham. The UCI machine learning repository. http://
archive.ics.uci.edu/datasets . 6
[46] Kye-Hyeon Kim and Seungjin Choi. Neighbor search with
global geometry: a minimax message passing algorithm. In
Twenty-Fourth International Conference on Machine Learn-
ing, ICML , pages 401–408, 2007. 2
[47] Sungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, and
Chang Dong Yoo. Higher-order correlation clustering for im-
age segmentation. In Advances in Neural Information Pro-
cessing Systems 24 (NIPS) , pages 1530–1538, 2011. 1
[48] Sungwoong Kim, Chang Dong Yoo, Sebastian Nowozin, and
Pushmeet Kohli. Image segmentation using higher-order cor-
relation clustering. IEEE Trans. Pattern Anal. Mach. Intell. ,
36(9):1761–1774, 2014. 1
[49] Patrick K. Kimes, Yufeng Liu, David Neil Hayes, and
James Stephen Marron. Statistical significance for hierar-
chical clustering. Biometrics , 73(3):811–821, 2017. 1[50] G. N. Lance and W. T. Williams. A general theory of classi-
ficatory sorting strategies. The Computer Journal , 9(4):373–
380, 1967. 3
[51] Bruno Leclerc. Description combinatoire des ultram ´etriques.
Math ´ematiques et Sciences Humaines , 73:5–37, 1981. 4
[52] Mark A. Levenstien, Yaning Yang, and Jurg Ott. Statisti-
cal significance for hierarchical clustering in genetic associ-
ation and microarray expression studies. BMC Bioinformat-
ics, 4(1), 2003. 1
[53] Tom Liebmann, Gunther H. Weber, and Gerik Scheuer-
mann. Hierarchical correlation clustering in multiple 2d
scalar fields. Comput. Graph. Forum , 37(3):1–12, 2018. 2
[54] Anna V . Little, Mauro Maggioni, and James M. Murphy.
Path-based spectral clustering: Guarantees, robustness to
outliers, and fast algorithms. J. Mach. Learn. Res. , 21:6:1–
6:66, 2020. 2
[55] J. MacQueen. Some methods for classification and analy-
sis of multivariate observations. In 5th Berkeley Symposium
on Mathematical Statistics and Probability , pages 281–297,
1967. 1
[56] Oded Maimon and Lior Rokach. Data Mining and Knowl-
edge Discovery Handbook . 2005. 1
[57] ´Alvaro Mart ´ınez-P ´erez and Manuel A. Mor ´on. Uniformly
continuous maps between ends of $$ {\mathbb {r}}$$ -trees.
Mathematische Zeitschrift , 263(3):583–606, 2008. 5
[58] Claire Mathieu and Warren Schudy. Correlation clustering
with noisy input. In SODA ’10 , pages 712–728, 2010. 2
[59] A. Mead. Review of the development of multidimensional
scaling methods. Journal of the Royal Statistical Society:
Series D , 41(1):27–39, 1992. 5
[60] Daniel Mullner. Modern hierarchical, agglomerative cluster-
ing algorithms. CoRR , abs/1109.2378, 2011. 1
[61] Boaz Nadler and Meirav Galun. Fundamental limitations of
spectral clustering. In Advances in Neural Information Pro-
cessing Systems (NIPS) , pages 1017–1024, 2007. 2
[62] Andrew Y . Ng, Michael I. Jordan, and Yair Weiss. On spec-
tral clustering: Analysis and an algorithm. In Advances in
Neural Information Processing Systems (NIPS) , pages 849–
856, 2001. 1
[63] Dehua Peng, Zhipeng Gui, Dehe Wang, Yuncheng Ma,
Zichen Huang, Yu Zhou, and Huayi Wu. Clustering by
measuring local direction centrality for data with heteroge-
neous density and weak connectivity. Nature Communica-
tions , 13(1), 2022. 8
[64] J. Podani. Introduction to the exploration of multivariate bi-
ological data . Backhuys Publishers, 2000. 1
[65] Aurko Roy and Sebastian Pokutta. Hierarchical clustering
via spreading metrics. J. Mach. Learn. Res. , 18(1):3077–
3111, 2017. 1
[66] I. J. Schoenberg. On certain metric spaces arising from eu-
clidean spaces by a change of metric and their imbedding in
hilbert space. Annals of Mathematics , 38(4):787–793, 1937.
5
[67] Jessica Shi, Laxman Dhulipala, David Eisenstat, Jakub
Lacki, and Vahab S. Mirrokni. Scalable community detec-
tion via parallel correlation clustering. Proc. VLDB Endow. ,
14(11):2305–2313, 2021. 1
23092
[68] Jianbo Shi and Jitendra Malik. Normalized cuts and im-
age segmentation. IEEE Trans. Pattern Anal. Mach. Intell. ,
22(8):888–905, 2000. 1
[69] A. Shieh, T. B. Hashimoto, and E. M. Airoldi. Tree pre-
serving embedding. In International Conference on Machine
Learning (ICML) , pages 753–760, 2011. 2, 4
[70] A. D. Shieh, T. B. Hashimoto, and E. M. Airoldi. Tree pre-
serving embedding. Proceedings of the National Academy of
Sciences , 108(41):16916–16921, 2011. 2, 4
[71] Peter Henry Andrews Sneath. The application of computers
to taxonomy. Journal of General Microbiology , 17:201–226,
1957. 3
[72] R. R. Sokal and C. D. Michener. A statistical method for
evaluating systematic relationships. Uni. of Kansas Science
Bulletin , 38:1409–1438, 1958. 3
[73] Jiliang Tang, Yi Chang, Charu C. Aggarwal, and Huan Liu.
A survey of signed network mining in social media. ACM
Comput. Surv. , 49(3):42:1–42:37, 2016. 1
[74] Erik Thiel, Morteza Haghir Chehreghani, and Devdatt P.
Dubhashi. A non-convex optimization approach to corre-
lation clustering. In The Thirty-Third AAAI Conference on
Artificial Intelligence (AAAI) , pages 5159–5166, 2019. 2
[75] D. M. Titterington, etc., A. F. M. Smith, and U. E. Makov.
Statistical analysis of finite mixture distribution . Probability
& Mathematical Statistics S. John Wiley & Sons, 1985. 4
[76] D. Vainstein, V . Chatziafratis, G. Citovsky, A. Rajagopalan,
M. Mahdian, and Y . Azar. Hierarchical clustering via
sketches and hierarchical correlation clustering. In AISTATS ,
pages 559–567, 2021. 2
[77] Laurens van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of Machine Learning Research ,
9(86):2579–2605, 2008. 8
[78] Richard S. Varga and Reinhard Nabben. On symmetric ul-
trametric matrices. In Numerical Linear Algebra , 1993. 5
[79] Nguyen Xuan Vinh, Julien Epps, and James Bailey. Informa-
tion theoretic measures for clusterings comparison: Variants,
properties, normalization and correction for chance. J. Mach.
Learn. Res. , 11:2837–2854, 2010. 6
[80] Ulrike von Luxburg. A tutorial on spectral clustering. Statis-
tics and Computing , 17(4):395–416, 2007. 2
[81] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. CoRR , abs/1708.07747, 2017. 7
[82] Pelin Yildirim and Derya Birant. K-linkage: A new agglom-
erative approach for hierarchical clustering. 17:77–88, 2017.
1
[83] Gale Young and A. Householder. Discussion of a set of
points in terms of their mutual distances. Psychometrika ,
3(1):19–22, 1938. 5
23093
