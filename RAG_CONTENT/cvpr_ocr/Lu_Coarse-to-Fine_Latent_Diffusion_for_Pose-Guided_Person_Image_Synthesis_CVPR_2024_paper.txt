Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis
Yanzuo Lu1Manlin Zhang1Andy J Ma1,2,3 *Xiaohua Xie1,2,3Jianhuang Lai1,2,3,4
1School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China
2Guangdong Province Key Laboratory of Information Security Technology, China
3Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China
4Pazhou Lab (HuangPu), Guangzhou, China
{luyz5, zhangmlin3 }@mail2.sysu.edu.cn, {majh8, xiexiaoh6, stsljh }@mail.sysu.edu.cn
Abstract
Diffusion model is a promising approach to image gen-
eration and has been employed for Pose-Guided Person
Image Synthesis (PGPIS) with competitive performance.
While existing methods simply align the person appearance
to the target pose, they are prone to overfitting due to the
lack of a high-level semantic understanding on the source
person image. In this paper, we propose a novel Coarse-
to-Fine Latent Diffusion (CFLD) method for PGPIS. In the
absence of image-caption pairs and textual prompts, we de-
velop a novel training paradigm purely based on images
to control the generation process of a pre-trained text-to-
image diffusion model. A perception-refined decoder is de-
signed to progressively refine a set of learnable queries
and extract semantic understanding of person images as a
coarse-grained prompt. This allows for the decoupling of
fine-grained appearance and pose information controls at
different stages, and thus circumventing the potential over-
fitting problem. To generate more realistic texture details, a
hybrid-granularity attention module is proposed to encode
multi-scale fine-grained appearance features as bias terms
to augment the coarse-grained prompt. Both quantita-
tive and qualitative experimental results on the DeepFash-
ion benchmark demonstrate the superiority of our method
over the state of the arts for PGPIS. Code is available at
https://github.com/YanzuoLu/CFLD .
1. Introduction
Pose-Guided Person Image Synthesis (PGPIS) aims to
translate the source person image into a specific target
pose while preserving the appearance as much as possi-
ble. It has a wide range of applications, including film
production, virtual reality, and fashion e-commerce. Most
*Corresponding author.
Figure 1. (a) The appearance of person images varies significantly
given only a textual prompt for image generation by using Stable
Diffusion [35] or ControlNet [55] with OpenPose guidance [3].
(b) Simply aligning the source appearance to the target pose with-
out a semantic understanding of person image can easily lead to
overfitting, such that the generated images become distorted and
unnatural. (c) Our method learns the coarse-grained prompt for
a comprehensive perception of the source image and injects fine-
grained appearance features as bias terms, thus generating high-
quality images with better generalization performance.
existing methods along this line are developed based on
Generative Adversarial Networks (GANs) [6, 18, 20, 25–
28, 33, 34, 38, 41, 44, 54, 56, 61, 62]. Nevertheless,
the GAN-based approach may suffer from the instability
of min-max training objective and difficulty in generating
high-quality images in a single forward pass.
As a promising alternative to GANs for image genera-
tion, diffusion models synthesize more realistic images pro-
gressively from a series of denoising steps. The recently
prevailing text-to-image latent diffusion model, such as Sta-
ble Diffusion (SD) [35] may now generate compelling per-
son images conditioned on a given textual prompt. The
appearance of the generated person can be determined by
well-designed prompts [19, 31] or prompt learning [58, 59].
With more reliable structural guidance [29, 55], the synthe-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6420
sized person images can be further constrained to specific
poses. Though the text-to-image diffusion generates realis-
tic images from textual prompts with high-level semantics,
its training paradigm requires extensive image-caption pairs
that are labor-expensive to collect for PGPIS. More impor-
tantly, due to the differing information densities between
language and vision [11], even the most detailed textual de-
scriptions inevitably introduce ambiguity and may not ac-
curately preserve the appearance as illustrated in Fig. 1(a).
More recently, several diffusion-based approaches have
emerged for PGPIS. A texture diffusion module is proposed
by PIDM [1] to model the complex correspondence be-
tween the appearance of source image and the target pose.
Since the denoising process at the high-resolution pixel
level is computationally expensive, PoCoLD [9] reduces
both the training and inference costs by mapping pixels to
low-dimensional latent spaces with a pre-trained Variational
Autoencoder (V AE) [7]. In PoCoLD, the correspondence
is further exploited by a pose-constrained attention module
based on additional 3D Densepose [8] annotations. While
both the PIDM and PoCoLD generate more realistic tex-
ture details by aligning the source image to the target pose,
they lack a high-level semantic understanding of person
images . Therefore, they are prone to overfitting and poor
generalization performance when synthesizing exaggerated
poses that are vastly different from the source image or rare
in the training set. As demonstrated in Fig. 1(b), the gener-
ated images become distorted and unnatural in these cases,
which is in line with several GAN-based approaches.
In this work, we propose a novel Coarse-to-Fine La-
tent Diffusion (CFLD) method for PGPIS. Our approach
breaks the conventional training paradigm which leverages
textual prompts to control the generation process of a pre-
trained SD model. Instead of conditioning on the human-
generated signals, i.e. languages that are highly seman-
tic and information-dense, we facilitate a coarse-to-fine ap-
pearance control method purely based on images. To obtain
the aforementioned semantic understanding specific to per-
son images, we endeavor to decouple the fine-grained ap-
pearance and pose information controls at different stages
by introducing a perception-refined decoder . The percep-
tion of the source person image is achieved by randomly ini-
tializing a set of learnable queries and progressively refining
them in the following decoder blocks via cross-attention.
The decoder output serves as a coarse-grained prompt to
describe the source image, focusing on the common seman-
tics across different person images, e.g. human body parts
and attributes such as age and gender. Moreover, we design
ahybrid-granularity attention module to effectively encode
multi-scale fine-grained appearance features as bias terms to
augment the coarse-grained prompt. In this way, the source
image is able to be aligned with the target pose by supple-
menting only the necessary fine-grained details under theguidance of the coarse-grained prompt, thus achieving bet-
ter generalization as illustrated in Fig. 1(c).
Our main contributions can be summarized as follows,
• We present a novel training paradigm in the absence of
image-caption pairs to overcome the limitations when
applying text-to-image diffusion to PGPIS. We propose
a perception-refined decoder to extract semantic under-
standing of person images as a coarse-grained prompt.
• We formulate a new hybrid-granularity attention module
to bias the coarse-grained prompt with fine-grained ap-
pearance features. Thus, the texture details of generated
images are better controlled and become more realistic.
• We conduct extensive experiments on the DeepFash-
ion [21] benchmark and achieve the state-of-the-art per-
formance both quantitatively and qualitatively. User stud-
ies and ablations validate the effectiveness of our method.
2. Related Work
Pose-Guided Person Image Synthesis. Maet al. [26] first
presents the task of pose-guided person image synthesis and
refines the generated images in an adversarial manner. To
decouple the pose and appearance information, early ap-
proaches [6, 27] propose to learn pose-irrelevant features
but fail to handle the complex texture details with vanilla
convolutional neural networks. To alleviate this problem,
auxiliary information is introduced to improve the genera-
tion quality, such as parsing [28] and UV maps [38]. Re-
cent approaches [18, 20, 33, 34, 44, 62] focus on modeling
the spatial correspondence between pose and appearance,
with the more frequent use of parsing maps [25, 54, 61].
PIDM [1] and PoCoLD [9] are developed based on diffu-
sion models to prevent from the drawbacks in the genera-
tive adversarial networks, including the instability of min-
max training objective and difficulty in synthesising high-
resolution images. Both of these two diffusion-based meth-
ods extend the idea of spatial correspondence to model
the relation between the appearance of source image and
target pose via the cross-attention mechanism. We argue
this leads to overfitting by simply aligning the source ap-
pearance to the target pose without a high-level seman-
tic understanding of the person image. More concurrent
works like MagicAnimate [50], Animate Anyone [16] and
PCDMs [39] require multi-stage and progressive fully fine-
tuning, while our pipeline is more efficient and end-to-end
by freezing most parameters. And the training paradigm of
IP-Adatper [52] is heavily relying on image-text pairs which
are not available for PGPIS task.
Controllable Diffusion Models. Diffusion models have re-
cently emerged and demonstrated their potential for high-
resolution image synthesis. The core idea is to start with a
simple noise vector and gradually transform it into a high-
quality image through multiple denoising iterations. Be-
yond unconditional generation [15, 42, 43], various meth-
6421
⊕⊕
𝒙𝒙𝒔𝒔
ℋ𝑨𝑨ℋ𝑺𝑺
𝒛𝒛𝑻𝑻⊕𝑭𝑭𝒅𝒅𝒊𝒊𝒊𝒊
𝑭𝑭𝒅𝒅𝒐𝒐𝒐𝒐𝒐𝒐
𝒙𝒙𝒐𝒐𝒑𝒑ℋ𝑷𝑷ℋ𝑫𝑫(a) Architecture
ℋ𝑵𝑵
×(𝑻𝑻−𝟏𝟏)
𝒚𝒚
Down/Up -sampling Block Middle Block Cross Attention Skip Connection Frozen Partial Fine -tuning(c) Hybrid -Granularity Attention
𝑸𝑸
⊗𝑭𝑭𝒉𝒉𝒍𝒍
𝑾𝑾𝒒𝒒𝒍𝒍
𝑲𝑲𝑭𝑭𝒅𝒅𝒐𝒐𝒐𝒐𝒐𝒐
𝑾𝑾𝒌𝒌𝒍𝒍𝑭𝑭𝒅𝒅𝒐𝒐𝒐𝒐𝒐𝒐
𝑽𝑽𝑾𝑾𝒗𝒗𝒍𝒍
Softmax ⊗
𝑭𝑭𝒐𝒐𝒍𝒍𝒇𝒇𝒍𝒍𝑩𝑩⊕𝚽𝚽𝑨𝑨Transformer
Decoder
Transformer
Decoder(b) Perception- Refined Decoder
𝑭𝑭𝒅𝒅𝒊𝒊𝒊𝒊∈ Q×C
𝒇𝒇𝟒𝟒Flatten
𝑭𝑭𝒅𝒅𝒐𝒐𝒐𝒐𝒐𝒐∈ Q×CLearnable Queries
×RFigure 2. (a) Architecture of our proposed Coarse-to-Fine Latent Diffusion (CFLD) method. For pose-guided latent diffusion, we incorpo-
rate a lightweight pose adapter HPfrom [29] to add its output feature maps to the end of each down-sampling block of the pre-trained UNet
HNfor efficient structural guidance. To achieve a coarse-to-fine appearance control, we propose a perception-refined decoder HDand
hybrid-granularity attention module HA, both of which take the multi-scale feature maps from a source image encoder HSas inputs. (b)
The coarse-grained prompt is obtained by refining the learnable queries progressively in our proposed HD. (c) We encode the multi-scale
fine-grained appearance features as bias terms in the up-sampling blocks for better texture details within HA.
ods have been introduced to incorporate user-supplied con-
trol signals into the generation process, enabling more con-
trollable image generation. For instance, [5] introduces the
usage of classifier gradients to condition on the generation,
while [14] proposes a classifier-free control mechanism em-
ploying a weighted summation of conditional and uncon-
ditional outputs for controllable synthesis. Moreover, the
Latent Diffusion Model (LDM) performs diffusion in the
latent space and injects the conditioning signals via a spe-
cific encoder and cross-attention. Building upon the pre-
trained LDM like Stable Diffusion (SD) [35], subsequent
works have explored to bias the latent space by adding ex-
tra controls [29, 55], as well as further to provide users with
control over the generated content [12, 45]. Rather than
employing a high-level conditioning prompt throughout the
generation, we design a coarse-to-fine conditioning process
that adjusts the latent features at different stages within the
UNet-based prediction network, providing better control-
lable pose-guided person image synthesis.
3. Method
3.1. Preliminary
Our method builds on top of the text-to-image latent dif-
fusion model, i.e., Stable Diffusion (SD) [35] with high-
quality image generation ability. There are two main stages
in the SD model: a Variational Autoencoder (V AE) [7] that
maps between raw-pixel space and low-dimensional latentspace and an UNet-based prediction model [36] for denois-
ing diffusion image generation. It follows the general idea
of Denoising Diffusion Probabilistic Model (DDPM) [15],
which formulates a forward diffusion process and a back-
ward denoising process of T= 1000 steps. The dif-
fusion process progressively adds random Gaussian noise
ϵ∼ N (0,I)to the initial latent z0, mapping it into noisy
latents ztat different timesteps t∈[1, T],
zt=√¯αtz0+√
1−¯αtϵ, (1)
where ¯α1,¯α2, ...,¯αTare derived from a fixed variance
schedule. The denoising process learns the UNet ϵθ(zt, t,c)
to predict the noise and reverse this mapping, where cis the
conditional embedding output by e.g. the CLIP [32] text
encoder in [35]. The optimization can be formulated as,
Lmse=Ez0,c,ϵ,t
∥ϵ−ϵθ(zt, t,c)∥2
2
. (2)
3.2. Coarse-to-Fine Latent Diffusion
Architecture and Overview. Fig. 2(a) shows the archi-
tecture of our proposed method. For concise illustration,
we omit the encoder Eand decoder Dof the V AE [7]
model in this figure. In the training phase, we are given
sets of the source image xs, source pose xsp, target pose
xtp, and ground-truth image xg. The source image passes
through an image encoder HS(e.g. swin transformer [22]),
from which we extract a stack of multi-scale feature maps
Fs= [f1,f2,f3,f4]for a coarse-to-fine appearance
6422
control . The coarse-grained prompts are learned by our
Perception-Refined Decoder (PRD) HDand serve as condi-
tional embeddings in both down-sampling and up-sampling
blocks of the UNet HN. While the down-sampling block
inHNremains intact in our method, we reformulate the
up-sampling block with our Hybrid-Granularity Attention
module (HGA) HAto bias the coarse-grained prompt with
fine-grained appearance features for more realistic textures.
More details about HDandHAwill be presented later.
For efficient pose control , we adopt a lightweight pose
adapter HPthat consists of several ResNet blocks [10].
The output feature maps of HPare added directly to the
end of each down-sampling block as in [29]. This requires
no additional fine-tuning and explicitly decouples the fine-
grained appearance and pose information controls. At dif-
ferent scales of down-sampling, the pose information is
only aligned with the same coarse-grained prompts given by
our PRD as conditional embeddings, rather than the differ-
ent multi-scale fine-grained appearance features in the com-
mon practice [1, 9]. In this way, the HGA module learns all
the pose-irrelevant texture details at the up-sampling stage
and is not prone to overfitting. Denote the initial latent state
for the ground-truth image as z0=E(xg). The MSE loss
in Eq. (2) is thus rewritten as,
Lmse=Ez0,xs,xtp,ϵ,t
∥ϵ−ϵθ(zt, t,xs,xtp)∥2
2
.(3)
Perception-Refined Decoder. Instead of utilizing multi-
scale appearance features as conditional embeddings as in
the existing diffusion-based approaches [1, 9], we propose
to decouple the controls from the fine-grained appearance
and pose information at different stages. Thus we design a
Perception-Refined Decoder (PRD) to extract semantic un-
derstanding of person images as a coarse-grained prompt,
given the flattened last-scale output f4fromHSas illus-
trated in Fig. 2(b). By revisiting how people perceive a per-
son image, we find several common characteristics, i.e., hu-
man body parts, age, gender, hairstyle, clothing, and so on,
as demonstrated in Fig. 1(a). This inspires us to maintain a
set of learnable queries Fin
d∈RQ×Drepresenting different
semantics of person images. They are randomly initialized
and progressively refined with the standard transformer de-
coders [47]. The source image conditioning f4interacts via
the cross-attention module at each decoder block. After R
blocks of refinement, we obtain the coarse-grained prompt
Fout
d, which serves as the conditional embedding and inputs
to both down-sampling and up-sampling in HN.
Hybrid-Granularity Attention. To precisely control the
texture details of generated images, we introduce the
Hybrid-Granularity Attention module (HGA) that is embed-
ded in different scales (l∈ {1,2,3})of up-sampling blocks
inHN, where we refer Fl
h,Fl
oto its input and output. Given
the multi-scale feature maps flof the source image from
HS, the HGA module aims to compensate for the missingnecessary details in the coarse-grained prompts. To achieve
this, we formulate the HGA module that naturally follows a
coarse-to-fine learning curriculum.
Specifically, we propose to inject multi-scale texture de-
tails by biasing the queries of cross-attention in the up-
sampling blocks as shown in Fig. 2(c), i.e.,
Q=Wl
qFl
h,K=Wl
kFout
d,V=Wl
vFout
d,
B=ϕA(fl),Fl
o=softmax ((Q+B)KT
√
d)V,(4)
where Wl
q,Wl
k,Wl
vare specific projection layers for the
l-th scale up-sampling block of dimension d.ϕAis a fine-
grained appearance encoder that mainly consists of Ktrans-
former layers with a zero convolution [55] added in the be-
ginning and the end. The zero convolution is a standard 1×1
convolution layer with both weight and bias initialized as
zeros. It keeps the gradient of HAback to HSsmall enough
in the early stage of training, so that the image encoder HS
and the more easily converged perception-refined decoder
HDcan focus on learning to provide a high-level semantic
understanding compatible with the pre-trained SD model.
Since we have decoupled the controls of the fine-grained ap-
pearance and pose information at different stages, the target
pose can be well controlled without overfitting during the
down-sampling process. Therefore, such a design encour-
ages the HGA module to slowly fill in more fine-grained
textures to better align the generation with the source im-
age during training. Note that Wl
k,Wl
vin the up-sampling
blocks are trainable parameters. They are the only trainable
parameters of the entire HN, which accounts for only 1.2%
of all the parameters in the pre-trained SD model.
3.3. Optimization
To assist the source-to-target pose translation, we fol-
low the insights in [56] to conduct source-to-source self-
reconstruction for training. The reconstruction loss is,
Lrec=Ez0,xs,xsp,ϵ,t
∥ϵ−ϵθ(zt, t,xs,xsp)∥2
2
,(5)
where z0=E(xs)andztis the noisy latent mapped from
z0at timestep t. The overall objective is written as,
Loverall =Lmse+Lrec. (6)
Moreover, we adopt the cubic function t= (1−(t
T)3)×
T, t∈Uniform (1, T)for the distribution of timestep t.
It increases the probability of tfalling in the early sampling
stage and strengthens the guidance, which helps to converge
faster and shorten the training time.
Sampling. Once the conditional latent diffusion model is
learned, the inference can be performed and starts by sam-
pling a random Gaussian noise zT∼ N (0,I). The pre-
dicted latent ˜z0is obtained by reversing the schedule in
6423
Component DefaultTrainable
Params.
HS Swin-B [22] 87.0M
HA K= 4 22.5M
HD R= 8,Q= 16 ,C= 768 97.7M
HP Adapter [29] 30.6M
HN up-sampling Wl
k,Wl
v 10.3M
MethodPose Info. &
AnnotationTraining
EpochsTrainable
Params.
PIDM [1] 2D OpenPose [3] 300 688.0M
PoCoLD [9] 3D DensePose [8] 100 395.9M
CFLD (Ours) 2D OpenPose [3] 100 248.2M
Table 1. The default settings and the number of trainable parame-
ters in each component of our method and comparison with other
diffusion-based methods.
Eq. (1) using the denoising network ϵtat each timestep
t∈[1, T]. We adopt the cumulative classifier-free guid-
ance [2, 9, 14] to strengthen both the source appearance and
target pose guidance, i.e.,
ϵt=ϵθ(zt, t,∅,∅)
+wpose(ϵθ(zt, t,∅,xtp)−ϵθ(zt, t,∅,∅))
+wapp(ϵθ(zt, t,xs,xtp)−ϵθ(zt, t,∅,xtp)).(7)
When the source image xsis missing, we use learnable vec-
tors as the conditional embeddings. The learnable vectors
are trained with a probability of η% to drop both xsand
xpduring training. The outputs of the pose adapter HP
will be set to all zeros if the target pose xtpis missing. We
use the DDPM scheduler [15] with 50 steps as the same as
in [1, 9]. Finally, the generated image is obtained by the
V AE decoder y=D(˜z0).
4. Experiments
4.1. Setup
Dataset. We follow [9, 34] to conduct experiments on the
In-Shop Clothes Retrieval benchmark of DeepFashion [21]
and evaluate on both the 256 ×176 and 512 ×352 resolu-
tions. This dataset consists of 52,712 high-resolution person
images in the fashion domain. The dataset split is the same
as in PATN [62], where 101,966 and 8,570 non-overlapping
pairs are selected for training and testing, respectively.
Objective metrics. We use four different metrics to evalu-
ate the generated images quantitatively, including FID [13],
LPIPS [57], SSIM [49] and PSNR. Both FID and LPIPS
are based on deep features. The Fr ´echet Inception Dis-
tance (FID) calculates the Wasserstein-2 distance [46] be-
tween the distributions of generated and real images using
Inception-v3 [37] features, and the Learned Perceptual Im-
age Patch Similarity (LPIPS) leverages a network trained on
human judgments to measure reconstruction accuracy in the
perceptual domain. As for the Structural Similarity IndexMethod Venue FID ↓LPIPS↓SSIM↑PSNR↑
Evaluate on 256 ×176 resolution
PATN [62] CVPR 19’ 20.728 0.2533 0.6714 -
ADGAN [28] CVPR 20’ 14.540 0.2255 0.6735 -
GFLA [33] CVPR 20’ 9.827 0.1878 0.7082 -
PISE [54] CVPR 21’ 11.518 0.2244 0.6537 -
SPGNet†[25] CVPR 21’ 16.184 0.2256 0.6965 17.222
DPTN†[56] CVPR 22’ 17.419 0.2093 0.6975 17.811
NTED†[34] CVPR 22’ 8.517 0.1770 0.7156 17.740
CASD†[61] ECCV 22’ 13.137 0.1781 0.7224 17.880
PIDM†[1] CVPR 23’ 6.812 0.2006 0.6621 15.630
PIDM‡[1] CVPR 23’ 6.440 0.1686 0.7109 17.399
PoCoLD [9] ICCV 23’ 8.067 0.1642 0.7310 -
CFLD (Ours) 6.804 0.1519 0.7378 18.235
V AE Reconstructed 7.967 0.0104 0.9660 33.515
Ground Truth 7.847 0.0000 1.0000 +∞
Evaluate on 512 ×352 resolution
CoCosNet2 [60] CVPR 21’ 13.325 0.2265 0.7236 -
NTED†[34] CVPR 22’ 7.645 0.1999 0.7359 17.385
PoCoLD [9] ICCV 23’ 8.416 0.1920 0.7430 -
CFLD (Ours) 7.149 0.1819 0.7478 17.645
V AE Reconstructed 8.187 0.0217 0.9231 30.214
Ground Truth 8.010 0.0000 1.0000 +∞
Table 2. Quantitative comparisons with the state of the arts in
terms of image quality.†We strictly follow the evaluation imple-
mentation in NTED [34] and reproduce these results.‡Results are
obtained using the generated images released by the authors.
Measure (SSIM) and Peak Signal to Noise Ratio (PSNR),
they quantify the similarity between generated images and
ground truths at the pixel level.
Subjective metrics. In addition to the objective metrics,
we follow [9] to use the Jab [1, 25, 61] metric in our user
study to calculate the percentage of generated images that
were considered the best among all methods [1, 25, 34, 56,
61]. Moreover, in order to measure the similarity between
the generated images and real data, we quantify the R2G
and G2R metrics as many early approaches did [26, 41, 62].
R2G represents the percentage of real images considered as
generated and G2R represents the percentage of generated
images considered as real by humans.
Implementation details. Our method is implemented with
PyTorch [30] and HuggingFace Diffusers [48] on top of the
Stable Diffusion [35] with the version of 1.5. The source
image is resized to 256 ×256 and the source image encoder
HSis a standard Swin-B [22] pretrained on ImageNet [4].
The default settings and the number of trainable parame-
ters in each component are summarized in Tab. 1. We train
for 100 epochs using the Adam [17] optimizer with a base
learning rate of 5e-7 scaled by the total batch size. The
learning rate undergoes a linear warmup during the first
1,000 steps and is multiplied by 0.1 at 50 epochs. For
classifier-free guidance, we set wposeandwidto 2.0 during
sampling, and drop the condition xsandxpwith a proba-
bility of η= 20 (%) during training.
6424
Source
ImageTarget
PoseGround
TruthSPGNet DPTN NTED CASD PIDM OursSource
ImageTarget
PoseGround
TruthSPGNet DPTN NTED CASD PIDM Ours
（1）
（2）
（3）
（4）
（5）
（6）
（7）
（8）
（9）
（10）0.684 0.562 0.492 0.629 0.591 0.495
0.371 0.326 0.224 0.327 0.311 0.169
0.242 0.178 0.162 0.170 0.196 0.158
0.140 0.125 0.106 0.093 0.117 0.087
0.293 0.261 0.250 0.246 0.234 0.2090.243 0.226 0.172 0.180 0.179 0.152
0.397 0.380 0.315 0.328 0.302 0.258
0.323 0.178 0.263 0.315 0.178 0.139
0.306 0.264 0.252 0.254 0.226 0.177
0.179 0.180 0.088 0.098 0.164 0.0770.346 0.319 0.302 0.306 0.334 0.191
0.251 0.251 0.247 0.172 0.290 0.189
0.374 0.343 0.185 0.199 0.221 0.148
0.344 0.284 0.242 0.306 0.340 0.245
0.345 0.277 0.248 0.262 0.304 0.1900.358 0.279 0.312 0.308 0.314 0.220
0.242 0.205 0.242 0.199 0.226 0.191
0.282 0.249 0.235 0.249 0.269 0.184
0.317 0.263 0.190 0.266 0.339 0.132
0.221 0.236 0.187 0.196 0.180 0.157Figure 3. Qualitative comparisons with state-of-the-arts. To clarify the relation between objective and subjective metrics, we demonstrate
the LPIPS measures and label the images with the first and second highest votes from user opinions as red and blue respectively.
4.2. Quantitative Comparison
We quantitatively compare our method with both GAN-
based and diffusion-based state-of-the-art approaches in
terms of objective metrics. The evaluation is performed
on both 256 ×176 and 512 ×352 resolutions as the same as
in [9, 34]. As shown in Tab. 2, our method significantly out-
performs the state-of-the-art across all metrics on both res-
olutions. In particular, compared to the other two diffusion-
based methods [1, 9] in Tab. 1, we achieve better recon-
struction with simpler 2D-only pose annotations and fewer
trainable parameters. The metrics for V AE [7] reconstruc-
tions and the ground truths are also provided for reference.It is worth noting that the results we obtain by running with
the checkpoint provided by PIDM [1] suffer from severe
overfitting, resulting in a large gap between the quantitative
results of provided images and those from the checkpoint.
4.3. Qualitative Comparison
In Fig. 3, we present a comprehensive visual comparison
with recent approaches that are publicly available and repro-
ducible, including SPGNet [25], DPTN [56], NTED [34],
CASD [61] and PIDM [1]. Our observations can be summa-
rized as follows. (1)Both GAN-based and diffusion-based
methods suffer from overfitting the human poses. When
6425
R2G()
 G2R()
 Jab()
0102030405060
32.450.552.5
32.242.5
17.6
    ������������������������������Figure 4. User study results in terms of R2G, G2R and Jab metrics.
generating some target poses that are extreme or not com-
mon in the training set, existing methods show severe distor-
tions as demonstrated in rows 1-2 . Since we decouples
the controls of fine-grained appearance and pose informa-
tion, our method circumvents the potential overfitting prob-
lem and always generates a reasonable pose with the con-
ditioning coarse-grained prompt and fine-grained appear-
ance bias. (2)For source images in rows 3-6 with more
complex clothing, our generated images better preserve the
textures details while aligning with the target pose thanks
to the robust coarse-to-fine learning curriculum of hybrid-
granularity attention module. For other methods, although
they match in color, the clothes either exhibit blurring and
distortion (SPGNet, DPTN, and CASD) or are spliced un-
naturally in texture, creating a large gap from the source
image (NTED and PIDM). (3)As for cases where the target
pose requires visualization of areas invisible in the source
image, our method exhibits strong understanding and gen-
eralization capabilities. With a semantic understanding of
the source image provided by the perception-refined de-
coder, our method is aware of what should be predicted
when the person turns around or sits down as illustrated in
rows 7-10 , such as different patterns on the front and
back of clothes, the sitting chair, and lower body wear.
4.4. User Study
To verify the gap between generated and real images as
well as our superiority over the state of the arts, we have
recruited over 100 volunteers to perform the following two
user studies following PIDM [1]. (1)For the R2G and G2R
metrics, volunteers were asked to discriminate between 30
generated images and 30 real images from the testing set.
Each volunteer could only see the generated images of a
specific method, and the pairs of source image and target
pose for generation were consistent across methods for a
fair comparison. From the results in Fig. 4, chances of a real
image being recognized as generated (R2G) are relatively
low, and over half of the images we generated are recog-
nized as real (G2R), demonstrating that our method gener-Method Biasing Trainable Prompt LPIPS ↓ SSIM↑
B1 K,V M-S 0.2018 0.6959
B2 K,V CLIP 0.2099 0.6944
B3 K,V PRD 0.1615 0.7293
B4 Q,K,V PRD 0.1742 0.7198
B5 Q K ,V Swin 0.1912 0.7038
Ours Q K ,V PRD 0.1519 0.7378
Table 3. Quantitative results for ablation studies. M-S is short for
multi-scale fine-grained appearance features similar to [1, 9].
Figure 5. Qualitative ablation results. Our approach has a high-
level understanding of the source image rather than forced align-
ment. It is also less prone to overfitting through the complemen-
tary coarse-grained prompts and fine-grained appearance biasing.
ates more realistic images that are less likely to be judged as
fake by humans. (2)For the Jab metric, each volunteer was
asked to choose the best match to the ground truth from the
generated images of different methods. Compared to other
methods, our Jab score achieved 52.5 percent, significantly
higher (+34.9) than the counterpart in second place, indicat-
ing that our method is more preferred and generates better
texture details and pose alignment.
4.5. Ablation Study
We perform ablation studies at multiple baselines to
compare with our method. The quantitative results are
presented in Tab. 3. B1is referenced from the other
two diffusion-based approaches [1, 9] that incorporate
multi-scale fine-grained appearance features as conditional
prompts. we also experiment with CLIP image encoder [32]
inB2to produce descriptive coarse-grained prompts for
source images, which is first explored by an image-editing
approach [51] that are also conditioned purely on images.
Together with the qualitative results in Fig. 5, we can see
that even very simple textures sometimes fail to be pre-
served, suggesting that these prompts are not compatible
with the pre-trained SD model. To provide coarse-grained
features that are more specific to person images, we inte-
grate the Perception-Refined Decoder (PRD) into B3. The
reconstruction metrics (i.e., LPIPS and SSIM) in Tab. 3 re-
6426
Figure 6. (a) Style transfer results of our method. The appearance in the reference image can be edited while maintaining the pose
and appearance. This is achieved by masking out regions of interest in the reference image and requires no additional training. (b) The
interpolation results show that texture details can be gradually shifted from one style to another in a smooth manner (from Style 1 to 2).
Input Head Upper Middle Lower Input Head Upper Middle Lower
Figure 7. Visualizing attention maps by different queries of the
prompt decoder. The maps are averaged over all attention heads.
veal a significant improvement in the quality of generated
images, which validates the effectiveness of our proposed
PRD. While this can be confirmed qualitatively in Fig. 5,
there is still a lack of textural details as indicated by the
red box. To address this issue, we experiment with training
more parameters in the UNet as B4and instead observe a
decrease in performance. This implies that the generaliza-
tion ability of SD model is compromised, which is not our
expectation. Thus we come up with the Hybrid-Granularity
Attention (HGA) to bias the queries and achieve state-of-
the-art results both quantitatively and qualitatively. In or-
der to verify whether the source image encoder (i.e., Swin
Transformer [22]) is able to learn sufficient information for
HGA and give a useful prompt, we abandon the PRD in B5.
The qualitative results in Fig. 5 demonstrate that both B4
andB5are overfitting, only our method circumvents this
problem by learning in a coarse to fine-grained manner.
Visualization. In Fig. 7, we visualize the effectiveness of
different queries in HD. The attention maps reflect different
human body parts of person images captured by learnable
queries, which proves that we have a high-level understand-
ing of the source images and thus less prone to overfitting.
4.6. Appearance Editing
Style Transfer. Our CFLD inherits the strong generation
ability of SD model by freezing the vast majority of its pa-rameters. Thus the style transfer can be achieved simply by
masking without additional training. Specifically, we mark
the regions of interest in the reference image yrefas a bi-
nary mask m. During sampling, the noise prediction is de-
composed into ϵ′
t=m·ϵt+ (1−m)·zref
t, where the ϵtis
based on the pose from yrefand the appearance from dif-
ferent styles of source images. Let zref
tbe the noisy latent
at timestep tmapped from zref
0=E(yref)as in Eq. (1).
From the results in Fig. 6(a), our method generates realistic
and coherent texture details in the regions of interest.
Style Interpolation. Additionally, our CFLD supports ar-
bitrary linear interpolation of both coarse-grained prompts
and fine-grained appearance biases. As shown in Fig. 6(b),
our generated images are faithfully reproducing different
styles with smooth transitions.
5. Conclusion
This paper presents a novel Coarse-to-Fine Latent Diffusion
(CFLD) method for Pose-Guided Person Image Synthesis
(PGPIS). We circumvent the potential overfitting problem
by decoupling the fine-grained appearance and pose infor-
mation controls. Our proposed Perception-Refined Decoder
(PRD) and Hybrid-Granularity Attention module (HGA)
enable a high-level semantic understanding of person im-
ages, while also preserving texture details through a coarse-
to-fine learning curriculum. Extensive experiments demon-
strate that CFLD outperforms the state of the arts in PG-
PIS both quantitatively and qualitatively. Our future work
will investigate whether the CFLD can be extended to more
downstream tasks that suffer from inferior data like person
re-identification [23, 53] and domain adaptation [24, 40],
since our training paradigm yields both a pre-trained fea-
ture network and a powerful generator for augmentation.
Acknowledgments. This work was supported in part by the
National Natural Science Foundation of China (U22A2095,
62276281).
6427
References
[1] Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal,
Rao Muhammad Anwer, Jorma Laaksonen, Mubarak Shah,
and Fahad Shahbaz Khan. Person image synthesis via de-
noising diffusion model. In CVPR , page 5968–5976, 2023.
2, 4, 5, 6, 7
[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InCVPR , pages 18392–18402, 2023. 5
[3] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part affinity
fields. In CVPR , page 7291–7299, 2017. 1, 5
[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , page 248–255, 2009. 5
[5] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion
models beat gans on image synthesis. In NeurIPS , pages
8780–8794, 2021. 3
[6] Patrick Esser and Ekaterina Sutter. A variational u-net for
conditional appearance and shape generation. In CVPR , page
8857–8866, 2018. 1, 2
[7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In CVPR ,
page 12873–12883, 2021. 2, 3, 6
[8] Rıza Alp G ¨uler, Natalia Neverova, and Iasonas Kokkinos.
Densepose: Dense human pose estimation in the wild. In
CVPR , pages 7297–7306, 2018. 2, 5
[9] Xiao Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, and
Tao Xiang. Controllable person image synthesis with pose-
constrained latent diffusion. In ICCV , page 22768–22777,
2023. 2, 4, 5, 6, 7
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR , page
770–778, 2016. 4
[11] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In CVPR , page 16000–16009, 2022. 2
[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv:2208.01626 ,
2022. 3
[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NeurIPS , 2017. 5
[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS Workshops , 2021. 3, 5
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 2, 3, 5
[16] Liucheng Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang,
and Liefeng Bo. Animate anyone: Consistent and con-
trollable image-to-video synthesis for character animation.
arXiv:2311.17117 , 2023. 2
[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR , 2015. 5[18] Yining Li, Chen Huang, and Chen Change Loy. Dense in-
trinsic appearance flow for human pose transfer. In CVPR ,
page 3693–3702, 2019. 1, 2
[19] Vivian Liu and Lydia B Chilton. Design guidelines for
prompt engineering text-to-image generative models. In
CHI, pages 1–23, 2022. 1
[20] Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, and
Shenghua Gao. Liquid warping gan: A unified framework
for human motion imitation, appearance transfer and novel
view synthesis. In ICCV , pages 5904–5913, 2019. 1, 2
[21] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou
Tang. Deepfashion: Powering robust clothes recognition and
retrieval with rich annotations. In CVPR , page 1096–1104,
2016. 2, 5
[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , page 10012–10022, 2021. 3, 5, 8
[23] Yanzuo Lu, Manlin Zhang, Yiqi Lin, Andy J Ma, Xi-
aohua Xie, and Jianhuang Lai. Improving pre-trained
masked autoencoder via locality enhancement for person re-
identification. In Chinese Conference on Pattern Recogni-
tion and Computer Vision (PRCV) , pages 509–521. Springer,
2022. 8
[24] Yanzuo Lu, Meng Shen, Andy J Ma, Xiaohua Xie, and Jian-
Huang Lai. Mlnet: Mutual learning network with neighbor-
hood invariance for universal domain adaptation. In AAAI ,
2024. 8
[25] Zhengyao Lv, Xiaoming Li, Xin Li, Fu Li, Tianwei Lin,
Dongliang He, and Wangmeng Zuo. Learning semantic per-
son image generation by region-adaptive normalization. In
CVPR , page 10806–10815, 2021. 1, 2, 5, 6
[26] Liqian Ma, Xu Jia, Qianru Sun, B. Schiele, T. Tuytelaars, and
L. Gool. Pose guided person image generation. In NeurIPS ,
2017. 2, 5
[27] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc
Van Gool, Bernt Schiele, and Mario Fritz. Disentangled per-
son image generation. In CVPR , pages 99–108, 2018. 2
[28] Yifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma, and
Zhouhui Lian. Controllable person image synthesis with
attribute-decomposed gan. In CVPR , page 5084–5093, 2020.
1, 2, 5
[29] Chong Mou, Xintao Wang, Liangbin Xie, Jing Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv:2302.08453 , 2023. 1, 3, 4, 5
[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
ing Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:
An imperative style, high-performance deep learning library.
NeurIPS , 32, 2019. 5
[31] Nikita Pavlichenko and Dmitry Ustalov. Best prompts for
text-to-image models and how to find them. In SIGIR , pages
2067–2071, 2023. 1
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
6428
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , page
8748–8763, 2021. 3, 7
[33] Yurui Ren, Xiaoming Yu, Junming Chen, Thomas H. Li, and
Ge Li. Deep image spatial transformation for person image
generation. In CVPR , page 7690–7699, 2020. 1, 2, 5
[34] Yurui Ren, Xiaoqing Fan, Ge Li, Shan Liu, and Thomas H.
Li. Neural texture extraction and distribution for controllable
person image synthesis. In CVPR , page 13535–13544, 2022.
1, 2, 5, 6
[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bjorn Ommer. High-resolution image
synthesis with latent diffusion models. In CVPR , page
10684–10695, 2022. 1, 3, 5
[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InMICCAI , page 234–241, 2015. 3
[37] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. NeurIPS , 29, 2016. 5
[38] Kripasindhu Sarkar, Vladislav Golyanik, Lingjie Liu, and
Christian Theobalt. Style and pose control for im-
age synthesis of humans from a single monocular view.
arXiv:2102.11263 , 2021. 1, 2
[39] Fei Shen, Hu Ye, Jun Zhang, Cong Wang, Xiao Han, and Wei
Yang. Advancing pose-guided image synthesis with progres-
sive conditional diffusion models. In ICLR , 2024. 2
[40] Meng Shen, Yanzuo Lu, Yanxu Hu, and Andy J Ma. Collab-
orative learning of diverse experts for source-free universal
domain adaptation. In ACM MM , pages 2054–2065, 2023. 8
[41] Aliaksandr Siarohin, Enver Sangineto, Stephane Lathuiliere,
and Nicu Sebe. Deformable gans for pose-based human im-
age generation. In CVPR , page 3408–3416, 2018. 1, 5
[42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In ICLR , 2021. 2
[43] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In ICLR , 2021. 2
[44] Hao Tang, Song Bai, Li Zhang, Philip H. S. Torr, and Nicu
Sebe. Xinggan for person image generation. In ECCV , page
717–734, 2020. 1, 2
[45] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In CVPR , pages 1921–1930,
2023. 3
[46] Leonid Nisonovich Vaserstein. Markov processes over denu-
merable products of spaces, describing large systems of au-
tomata. Problemy Peredachi Informatsii , 5(3):64–72, 1969.
5
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 4
[48] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro
Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,
and Thomas Wolf. Diffusers: State-of-the-art diffusion
models. https://github.com/huggingface/
diffusers , 2022. 5[49] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.
Simoncelli. Image quality assessment: From error visibility
to structural similarity. TIP, 13(4):600–612, 2004. 5
[50] Zhongcong Xu, Jianfeng Zhang, Jianfeng Liew, Hanshu Yan,
Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng
Shou. Magicanimate: Temporally consistent human image
animation using diffusion model. arXiv:2311.16498 , 2023.
2
[51] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin
Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by
example: Exemplar-based image editing with diffusion mod-
els. In CVPR , page 18381–18391, 2023. 7
[52] Hu Ye, Jun Zhang, Siyi Liu, Xiao Han, and Wei Yang. Ip-
adapter: Text compatible image prompt adapter for text-to-
image diffusion models. arXiv:2308.06721 , 2023. 2
[53] Junkun Yuan, Xinyu Zhang, Hao Zhou, Jian Wang, Zhong-
wei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun
Kuang, Kun Yao, et al. Hap: Structure-aware masked image
modeling for human-centric perception. NeurIPS , 36, 2024.
8
[54] Jinsong Zhang, Kun Li, Yu-Kun Lai, and Jingyu Yang. Pise:
Person image synthesis and editing with decoupled gan. In
CVPR , page 7982–7990, 2021. 1, 2, 5
[55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , page 3836–3847, 2023. 1, 3, 4
[56] Pengze Zhang, Lingxiao Yang, Jianhuang Lai, and Xiaohua
Xie. Exploring dual-task correlation for pose guided person
image generation. In CVPR , page 7713–7722, 2022. 1, 4, 5,
6
[57] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness
of deep features as a perceptual metric. In CVPR , page
586–595, 2018. 5
[58] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi-
wei Liu. Conditional prompt learning for vision-language
models. In CVPR , page 16816–16825, 2022. 1
[59] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. IJCV ,
130(9):2337–2348, 2022. 1
[60] Xingran Zhou, Bo Zhang, Ting Zhang, Pan Zhang, Jianmin
Bao, Dong Chen, Zhongfei Zhang, and Fang Wen. Cocosnet
v2: Full-resolution correspondence learning for image trans-
lation. In CVPR , page 11465–11475, 2021. 5
[61] Xinyue Zhou, M. Yin, Xinyuan Chen, Li Sun, Changxin
Gao, and Qingli Li. Cross attention based style distribu-
tion for controllable person image synthesis. In ECCV , page
161–178, 2022. 1, 2, 5, 6
[62] Zhen Zhu, Tengteng Huang, Baoguang Shi, Miao Yu, Bofei
Wang, and Xiang Bai. Progressive pose attention transfer for
person image generation. In CVPR , page 2347–2356, 2019.
1, 2, 5
6429
