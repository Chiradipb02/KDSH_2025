Forgery-aware Adaptive Transformer for Generalizable Synthetic
Image Detection
Huan Liu1,3*Zichang Tan2Chuangchuang Tan1,3Yunchao Wei1,3Jingdong Wang2Yao Zhao1,3†
1Institute of Information Science, Beijing Jiaotong University2Baidu VIS
3Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing, China
Abstract
In this paper, we study the problem of generalizable syn-
thetic image detection, aiming to detect forgery images from
diverse generative methods, e.g., GANs and diffusion mod-
els. Cutting-edge solutions start to explore the benefits of
pre-trained models, and mainly follow the fixed paradigm of
solely training an attached classifier, e.g., combining frozen
CLIP-ViT with a learnable linear layer in UniFD [43].
However, our analysis shows that such a fixed paradigm is
prone to yield detectors with insufficient learning regarding
forgery representations. We attribute the key challenge to
the lack of forgery adaptation, and present a novel forgery-
aware adaptive transformer approach, namely FatFormer.
Based on the pre-trained vision-language spaces of CLIP ,
FatFormer introduces two core designs for the adaption to
build generalized forgery representations. First, motivated
by the fact that both image and frequency analysis are es-
sential for synthetic image detection, we develop a forgery-
aware adapter to adapt image features to discern and inte-
grate local forgery traces within image and frequency do-
mains. Second, we find that considering the contrastive ob-
jectives between adapted image features and text prompt
embeddings, a previously overlooked aspect, results in a
nontrivial generalization improvement. Accordingly, we in-
troduce language-guided alignment to supervise the forgery
adaptation with image and text prompts in FatFormer. Ex-
periments show that, by coupling these two designs, our ap-
proach tuned on 4-class ProGAN data attains a remarkable
detection performance, achieving an average of 98% accu-
racy to unseen GANs, and surprisingly generalizes to un-
seen diffusion models with 95% accuracy.
1. Introduction
Recent years have witnessed the emergence and advance-
ment of generative models, such as GANs [13, 25–27] and
*Work done when H. Liu is a long-term intern at Baidu.
†Corresponding author (E-mail: yzhao@bjtu.edu.cn).
Pre-trainedImageEncoderClassifierImageFakeReal
TextPromptsAdaptiveImageEncoderImagePre-trainedTextEncoderContrastiveObjectiveFakeReal(a)UniFD
(b)FatFormerAlignFigure 1. Comparison with fixed pre-trained paradigm. Here,
we illustrate the overview of UniFD [43] and our FatFormer.
In contrast to training an attached classifier, FatFormer builds a
forgery-aware adaptive transformer by aligning the representations
of image and text prompts via contrastive objectives.
diffusion models [9, 14, 16, 42]. These models enable the
creation of hyper-realistic synthetic images, thus raising the
wide concerns of potential abuse and privacy threats. In
response to such security issues, various forgery detection
[36, 37, 52, 54, 56, 57, 61] and anti-spoofing methods [33–
35] have been developed, e.g., image-based methods [3, 59]
focusing on low-level visual artifacts and frequency-based
methods [12, 45] relying on high-frequency pattern anal-
ysis. However, we observe big performance degradation
when applying them to unseen images created by GANs or
more recent diffusion models. How to address this problem
has seen significant interest.
Recent approaches [43, 55] turn to explore the utiliza-
tion of pre-trained models, following the fixed pre-trained
paradigm of solely training an attached classifier, as shown
in Figure 1 (a). A notable example in this field is the UniFD
proposed by Ojha et al. [43], where a pre-trained CLIP-ViT
[10, 46] is employed to encode images into image features
without learning. Subsequently, a linear layer is tuned as a
classifier to determine the credibility of inputs. At a very
high level, their key to success is the employment of a pre-
trained model in a frozen state, thus providing a learned uni-
versal representation (from the pre-training), yet not explic-
itly tuned in the current synthetic image detection task. In
this way, such a representation will never be overfitted dur-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10770
seenunseen
UniFDFatFormer(a) ProGAN(b) StyleGAN(c) Deepfake(d) LDM
(e) ProGAN(f) StyleGAN(g) Deepfake(h) LDMFigure 2. Logit distributions of extracted forgery features. We compare the state-of-the-art UniFD [43] and our FatFormer with forgery
adaptation, both tuned with 4-class ProGAN [25] data. A total of four testing GANs and diffusion models are considered, including
ProGAN [25], StyleGAN [26], Deepfake [50] and LDM [49], each randomly sampled 1k real and 1k fake images. Best view in color.
ing training and thus preserves reasonable generalizability.
However, we consider that such a frozen operation adopted
by UniFD will also limit the capability of pre-trained mod-
els for learning strong and pertinent forgery features.
To verify our assumption, we qualitatively study the
forgery discrimination of the fixed pre-trained paradigm by
visualizing the logit distributions of UniFD [43] across var-
ious generative models, as depicted in the top row of Fig-
ure 2. The distribution reflects the degree of separation
between ‘real’ and ‘fake’ during testing, thereby offering
the extent of generalization of extracted forgery represen-
tations. One can see that there is a large overlap of ‘real’
and ‘fake’ regions when facing unseen GANs or diffusion
models (Figure 2 (b)-(d)), mistakenly, to identify these forg-
eries as ‘real’ class. Moreover, even in the case of Pro-
GAN [25] testing samples, which employ the identical gen-
erative model as the training data, the distinction between
‘real’ and ‘fake’ elements becomes increasingly indistinct
(Figure 2 (a) vs.(e)). We conclude that the fixed pre-trained
paradigm is prone to yield detectors with insufficient learn-
ing regarding forgery artifacts, and attribute the key chal-
lenge to the lack of forgery adaptation that limits the full
unleashing of potentials embedded in pre-trained models.
Driven by this analysis, we present a novel Forgery-
aware adaptive transFormer approach (Figure 1 (b)),
named FatFormer, for generalizable synthetic image detec-
tion. In alignment with UniFD [43], FatFormer investigates
CLIP [46] as the pre-trained model, which consists of a ViT
[10] image encoder and a transformer [58] text encoder.
Based on the pre-trained vision-language spaces of CLIP,our approach achieves the forgery adaptation by incorporat-
ing two core designs, ultimately obtaining well-generalized
forgery representations with a distinct boundary between
real and fake classes (Figure 2 (e)-(h)).
First, motivated by the fact that both image and fre-
quency domains are important for synthetic image detec-
tion, a forgery-aware adapter (FAA) is developed, com-
prising a pair of image and frequency forgery extractors.
In the image domain, a lightweight convolution module is
employed for extracting low-level forgery artifacts, such
as blur textures and color mismatch [32]. On the other
hand, for the frequency domain, we construct a grouped at-
tention mechanism that dynamically aggregates frequency
clues from different frequency bands of discrete wavelet
transform (DWT) [40]. By integrating these diverse forgery
traces, FAA builds a comprehensive local viewpoint of im-
age features essential for effective forgery adaptation.
Second, instead of utilizing the binary cross-entropy loss
applied to image features, we consider the contrastive ob-
jectives between image and text prompts, a previously over-
looked aspect. This novel direction is inspired by the natural
language supervision in CLIP-ViT’s pre-training, typically
more robust to overfitting by optimizing the similarity be-
tween image features and text prompt embeddings [18]. Ac-
cordingly, language-guided alignment (LGA) is proposed,
which encompasses a patch-based enhancer, designed to en-
rich the contextual relevance of text prompts by condition-
ing them on image patch tokens, as well as a text-guided in-
teractor, that serves to align local image patch tokens with
global text prompt embeddings, thereby directing the im-
10771
Forgery-Aware Adapter (FAA)Language-Guided Alignment (LGA)
Tuned
Frozen1-st Stage of ViT2-nd Stage of ViT. . .N-th Stage of ViTFAA
PatchEmbedding
. . .FAA. . .[CLASS]
TextEncoderClass Embedding
EOSEOSCLSCLS.........Context Embedding. . .
Text-Guided Interactor RealFake
Patch-BasedEnhancer
Fake
Real
ImageEncoder
Eq.(1)Eq.(9)ReshapeReLUConvFlattenConvDWTFFNIDWT
λInter-&Intra-BandAttentions...Inter-BandAttentionIntra-BandAttentionLLLHHLHH.........Inter-&Intra-BandAttentionsFigure 3. Our FatFormer architecture. The ViT image encoder integrates forgery-aware adapters to effectively extract visual forgery
features from input images. To supervise the forgery adaptation process, language-guided alignment is introduced. Specifically, taking
two input images for example, we maximize the cosine similarities between paired (dark gray squares) image features and text prompt
embeddings, while minimizing the unpaired ones (light gray squares). In testing, only the test image is required to calculate the forgery
probability via a softmax of these similarities. Squared ‘CLS’ and ‘EOS’ represent the image CLS tokens and text prompt embeddings.
age encoder to concentrate on forgery-related representa-
tions. Empirical results show that the forgery adaptation
supervised by LGA obtains more generalized forgery rep-
resentations, thus improving the detection generalizability.
Our adaptive approach FatFormer significantly outper-
forms recent methods with the fixed pre-trained paradigm.
Notably, we achieve 98.4%ACC and 99.7%AP on 8types
of GANs, and 95.0%ACC and 98.8%AP on 10types of un-
seen diffusion images, using limited ProGAN training data.
We hope our findings can facilitate the development of pre-
trained paradigms in this field.
2. Related Work
Synthetic image detecting. Due to the increasing concerns
about generative models, many works are proposed to ad-
dress the problem of synthetic image detection, which can
be roughly divided into image-based methods [29, 41, 50,
60, 64], frequency-based methods [12, 21, 22], and pre-
trained-based methods [43, 55]. For instance, Wang et al.
[59] adopt various data augmentations and large-scale GAN
images to improve the generalization to unseen testing data.
Qian et al. [45] introduce frequency analysis into the de-
tection framework, using local frequency statistics and de-
composed high-frequency components for forgery detec-
tion. More recently, inspired by the progress of vision-
language models (VLMs) in many aspects [20, 24, 62, 67],
many works have focused on the fixed pre-trained paradigm
of freezing the pre-trained model and adopting an attached
classifier for forgery detection. For example, Ojha et al.
[43] propose UniFD to explore the potential of VLMs, i.e.,
CLIP [46], for synthetic image detection. They observe that
training a deep network fails to detect fake images from new
breeds and employs the frozen CLIP-ViT [10, 46] to extract
forgery features, followed by a linear classifier.
In this paper, our motivation is different from the closely-related approach UniFD [43]. UniFD attempts to adopt a
frozen pre-trained model to extract forgery representations
‘without learning’. In contrast, our approach demonstrates
that the forgery adaptation of pre-trained models is essential
for the generalizability of synthetic image detection.
Efficient transfer learning. The latest progress in trans-
fer learning shows the potential for efficient fine-tuning of
pre-trained models, especially in the NLP field. Unlike tra-
ditional strategies, such as linear-probing [15] and full fine-
tuning [69], efficient transfer learning only adds learnable
modules with a few parameters, such as prompt learning
[30] and adapter-based methods [17, 19]. Inspired by this,
many efficient transfer learning works are proposed for vi-
sion [5, 23] and vision-language models [65, 66]. Unlike
UniFD [43] with linear probing, this paper investigates the
efficient transfer learning for generalizable synthetic image
detection and first proposes an adaptive transformer with
contrastive objectives.
3. FatFormer
3.1. Overview
The overall structure of FatFormer is illustrated in Figure 3.
FatFormer is composed of two pre-trained encoders for both
image and text prompts, as well as the proposed forgery-
aware adapter (Section 3.2) and language-guided alignment
(Section 3.3). This framework predicts the forgery probabil-
ity by calculating the softmax of cosine similarities between
image features and text prompt embeddings.
Vanilla CLIP. Following UniFD [43], we adopt CLIP [46]
as the pre-trained model with a ViT [10] image encoder and
transformer [58] text encoder, respectively. Given an image
x∈R3×H×W, with height Hand width W, CLIP converts
it into a D-dimensional image features fimg∈R(1+N)×D,
where 1represents the image CLS token, N=HW
P2de-
10772
notes the image patch tokens and Pis the patch size. Mean-
while, the text encoder takes language text tand generates
the text prompt embeddings ftext∈RM×Dfrom the ap-
pended EOS tokens in the text encoder, where Mdenotes
the number of classes (in this paper, M= 2). Two en-
coders are jointly trained to optimize the cosine similar-
ity between the image CLS token and text prompt embed-
dings using contrastive loss. After pre-training, we can uti-
lize the re-assembled text descriptions for zero-shot testing,
e.g., a simple template of ‘this photo is [CLASS]’, where
‘[CLASS]’ is replaced by class names like ‘real’ or ‘fake’.
Given the testing image and text prompts, we have the pre-
dicted similarity of class i∈ {0,1}, where 0represents
‘real’ and 1is ‘fake’, as follows
S(i) = cos( f(0)
img, f(i)
text), (1)
where cos(·)is the cosine similarity, f(0)
imgdenotes the image
CLS token at index 0offimg. Further, the corresponding
possibility can be derived via a softmax function
P(i) =exp(S(i)/τ)P
kexp(S(k)/τ), (2)
where τis the temperature parameter.
3.2. Forgery-aware adapter (FAA)
To adapt the image features for effective forgery adapta-
tion, we insert forgery-aware adapters to bridge adjacent
ViT stages, each encompassing multiple ViT layers, in the
image encoder, as shown in Figure 3. These adapters dis-
cern and integrate forgery traces within both image and fre-
quency domains, enabling a comprehensive local viewpoint
of image features.
Image forgery extractor. In the image domain, FAA con-
structs a lightweight image forgery extractor, comprising
two convolution layers and a ReLU layer for capturing low-
level image artifacts, as follows
ˆg(j)
img= Conv(ReLU(Conv( g(j)
img))), (3)
where ˆg(j)
imgrepresents the adapted forgery-aware image fea-
tures from FAA in j-th ViT stage, and g(j)
imgis the vanilla
features from the last multi-head attention module in j-th
ViT stage. Here, we omit the reshape operators.
Frequency forgery extractor. For the frequency domain,
a grouped attention mechanism is proposed to mine forgery
traces in the frequency bands of discrete wavelet transform
(DWT) [40]. Although previous detection methods [22, 45]
adopt fast Fourier transform [1] and discrete cosine trans-
form [48], they destroy the position information [31] in the
transformed frequency domain, which is crucial in the con-
text of attention modeling [10]. Thus, we utilize DWT asthe transform function, retaining the spatial structure of im-
age features, which decomposes the inputs into 4distinct
frequency bands, including LL, LH, HL, and HH. Here,
combinations of ‘L’ and ‘H’ represent the combined low and
high pass filters. Then, two grouped attention modules, i.e.,
inter-band attention and intra-band attention, are proposed
for the extraction of frequency clues. As indicated in Fig-
ure 3, the inter-band attention explicitly explores the inter-
actions across diverse frequency bands, while the intra-band
attention builds interactions within each frequency band.
This design achieves the dynamical aggregation of differ-
ent positions and bands, rather than manual weighting like
F3Net [45]. In practice, we implement them with multi-
head attention modules [58]. Finally, FFN and inverse dis-
crete wavelet transform (IDWT) are used to obtain forgery-
aware frequency features ˆg(j)
freq, which are transformed back
into the image domain for further incorporation.
To prevent introducing hyper-parameters, we leverage a
learnable scale factor λto control the information from im-
age and frequency domains as the final adapted image fea-
tures of j-th stage of ViT, which will be sent to the first
multi-head attention module in the next (j+ 1) -th stage.
ˆg(j)= ˆg(j)
img+λ·ˆg(j)
freq. (4)
3.3. Language-guided alignment (LGA)
To supervise the forgery adaptation of FatFormer, language-
guided alignment is proposed by considering the contrastive
objectives between image and text prompts. In a bit more
detail, LGA has a patch-based enhancer that enriches the
context of text prompts, and a text-guided interactor that
aligns the local image patch tokens with global text prompt
embeddings. Finally, we implement an augmented con-
trastive objective for the loss calculation.
Patch-based enhancer. Instead of using hand-crafted tem-
plates as prompts, FatFormer has a soft prompt design
by adopting auto context embeddings, following [65, 66].
Since synthetic image detection relies on local forgery de-
tails [4, 63], we develop a patch-based enhancer to enhance
the contextual relevance of prompts via the condition of lo-
cal image patch tokens, deriving forgery-relevant prompts
context. Specifically, we first compute the image patch to-
kensf(1:N)
img∈RN×Din the image encoder. Then, given C
context embeddings pctx∈RC×D, we have
Apbe=pctx·(f(1:N)
img )T, (5)
where Apbe∈RC×Nis the similarity matrix in patch-based
enhancer. We use Apbeto represent the intensity of image
patch tokens for constructing each context embedding, as
follows
ˆpctx= softmax( Apbe)·f(1:N)
img +pctx. (6)
10773
Finally, we can obtain the set of possible text prompts by
combining the enhanced context ˆpctxandM[CLASS] em-
beddings, and send them to the text encoder.
Text-guided interactor. To guide the image encoder fo-
cusing on forgery-related representation, we propose a text-
guided interactor, which aligns the local image patch tokens
with global text prompt embeddings. Specifically, given the
text prompt embeddings ftextfrom text encoder and im-
age patch tokens f(1:N)
img , our text-guided interactor calcu-
lates the similarity Atgibetween them by
Atgi=f(1:N)
img·(ftext)T. (7)
Similar to Eq. (6), with Atgi, sized RN×M, we align the
image patch tokens with text prompt embeddings by adap-
tively augmenting text representations, as follows
ˆf(1:N)
img = softmax( Atgi)·ftext+f(1:N)
img, (8)
where ˆf(1:N)
img denotes the aligned image patch tokens. To-
gether with the augmented contrastive objectives, the image
encoder is guided to concentrate on forgery-related repre-
sentation within each distinct image patch.
Augmented contrastive objectives. For the loss calcu-
lation, we consider augmented contrastive objectives that
comprise two elements. The first is the cosine similarity in
Eq. (1) same as the vanilla CLIP. The second is the simi-
larity between text prompt embeddings and aligned image
patch tokens ˆf(1:N)
img . With t∈[1, N]andi∈ {0,1}, we
have
S′(i) =1
NX
tcos(ˆf(t)
img, f(i)
text). (9)
By merging similarities from Eq. (1) and Eq. (9), our Fat-
Former describes a augmented probability ˆP(i)by a soft-
max function, as follows
ˆP(i) =exp(( S(i) +S′(i))/τ)P
kexp(( S(k) +S′(k))/τ). (10)
In practice, we apply the cross-entropy function on Eq. (10)
with label y∈ {0,1}to calculate contrastive loss like the
origin CLIP, as follows
L=−y·logˆP(y)−(1−y)·log(1−ˆP(y)). (11)
4. Experiments
4.1. Settings
Datasets. As generative methods are always coming up,
we follow the standard protocol [43, 55, 59] that limits
the accessible training data to only one generative model,
while testing on unseen data, such as synthetic images fromother GANs and diffusion models. Specifically, we train
FatFormer on the images generated by ProGAN [25] with
two different settings, including 2-class (chair, horse) and
4-class (car, cat, chair, horse) data from [59]. For eval-
uation, we collect the testing GANs dataset provided in
[59] and diffusion model datasets in [43, 60], which contain
synthetic images and the corresponding real images. The
testing GANs dataset includes ProGAN [25], StyleGAN
[26], StyleGAN2 [27], BigGAN [2], CycleGAN [68], Star-
GAN [7], GauGAN [44] and DeepFake [50]. On the other
hand, the diffusion part consists of PNDM [38], Guided [9],
DALL-E [47], VQ-Diffusion [14], LDM [49], and Glide
[42]. For LDM and Glide, we also consider their vari-
ants with different generating settings. More details can be
found in their official papers.
Evaluation metric. The accuracy (ACC) and average pre-
cision (AP) are reported as the main metrics during eval-
uation for each generative model, following the standard
process [43, 55, 59]. To better evaluate the overall model
performance over the GANs and diffusion model datasets,
we also adopt the mean of ACC and AP on each dataset,
denoted as ACC Mand AP M.
Implementation details. Our main training and testing set-
tings follow the previous study [43]. The input images are
first resized into 256×256, and then image cropping is
adopted to derive the final resolution of 224×224. We
apply random cropping and random horizontal flipping at
training, while center cropping at testing, both with no other
augmentations. The Adam [28] is utilized with betas of
(0.9,0.999) . We set the initial learning rate as 4×10−4,
training epochs as 25, and adopt a total batch size of 256.
Besides, a learning rate schedule is used, decaying at every
10epochs by a factor of 0.9.
4.2. Main results
This paper aims to build a better paradigm with pre-trained
models for synthetic image detection. Therefore, we mainly
compare our FatFormer with previous methods that adopt
the fixed pre-trained paradigm, such as LGrad [55] and
UniFD [43]. In addition, to show the effectiveness of
our approach, we also consider comparisons with exist-
ing image-based [3, 53, 59] and frequency-based methods
[11, 12, 21, 22, 45].
Comparisons on GANs dataset. Table 1 reports the com-
parisons on the GANs dataset [59] with two different train-
ing data settings. Results show that our FatFormer con-
sistently exceeds pre-trained-based LGrad [55] and UniFD
[43]. Specifically, under 4-class supervision, FatFormer
outperforms the current state-of-the-art method UniFD by
a significant 9.3%ACC and 1.4%AP with the same pre-
trained CLIP model, achieving 98.4%ACC and 99.7%
AP. Besides, for the other 2-class supervision setting, sim-
ilar trends are observed with the ones under 4-class su-
10774
Table 1. Accuracy and average precision comparisons with state-of-the-art methods on GANs dataset. We report the performance (in
the formulation of ACC / AP) with two different training settings, including supervision from 2-class and 4-class ProGAN data, following
[59]. Besides, we also provide the reference (Ref) for previous frameworks. The performance (ACC M/ APM) over the entire dataset is
marked in gray . The best results are highlighted in bold .
Methods Ref ProGAN StyleGAN StyleGAN2 BigGAN CycleGAN StarGAN GauGAN Deepfake Mean2-class supervisionWang [59] CVPR 2020 64.6/92.7 52 .8/82.8 75 .7/96.6 51 .6/70.5 58 .6/81.5 51 .2/74.3 53 .6/86.6 50 .6/51.5 57.3/79.6
Durall [11] CVPR 2020 79.0/73.9 63 .6/58.8 67 .3/62.1 69 .5/62.9 65 .4/60.8 99 .4/99.4 67 .0/63.0 50 .5/50.2 70.2/66.4
Frank [12] ICML 2020 85.7/81.3 73 .1/68.5 75 .0/70.9 76 .9/70.8 86 .5/80.8 85 .0/77.0 67 .3/65.3 50 .1/55.3 75.0/71.2
F3Net [45] ECCV 2020 97.9/100.0 84 .5/99.5 82 .2/99.8 65 .5/73.4 81 .2/89.7 100 .0/100.0 57 .0/59.2 59 .9/83.0 78.5/88.1
BiHPF [21] WACV 2022 87.4/87.4 71 .6/74.1 77 .0/81.1 82 .6/80.6 86 .0/86.6 93 .8/80.8 75 .3/88.2 53 .7/54.0 78.4/79.1
FrePGAN [22] AAAI 2022 99.0/99.9 80 .8/92.0 72 .2/94.0 66 .0/61.8 69 .1/70.3 98 .5/100.0 53 .1/51.0 62 .2/80.6 75.1/81.2
LGrad [55] CVPR 2023 99.8/100.094.8/99.7 92 .4/99.6 82.5/92.4 85 .9/94.7 99 .7/99.9 73 .7/83.2 60 .6/67.8 86.2/92.2
UniFD [43] CVPR 2023 99.7/100.0 78 .8/97.4 75 .4/96.7 91 .2/99.0 91 .9/99.8 96 .3/99.9 91 .9/100.0 80 .0/89.4 88.1/97.8
Ours − 99.8/100.087.7/97.4 91 .1/99.3 98.9/99.9 99 .9/100.0 100 .0/100.0 99 .9/100.0 89 .4/97.395.8/99.24-class supervisionWang [59] CVPR 2020 91.4/99.4 63 .8/91.4 76 .4/97.5 52 .9/73.3 72 .7/88.6 63 .8/90.8 63 .9/92.2 51 .7/62.3 67.1/86.9
Durall [11] CVPR 2020 81.1/74.4 54 .4/52.6 66 .8/62.0 60 .1/56.3 69 .0/64.0 98 .1/98.1 61 .9/57.4 50 .2/50.0 67.7/64.4
Frank [12] ICML 2020 90.3/85.2 74 .5/72.0 73 .1/71.4 88 .7/86.0 75 .5/71.2 99 .5/99.5 69 .2/77.4 60 .7/49.1 78.9/76.5
F3Net [45] ECCV 2020 99.4/100.0 92 .6/99.7 88 .0/99.8 65 .3/69.9 76 .4/84.3 100.0/100.0 58 .1/56.7 63 .5/78.8 80.4/86.2
BiHPF [21] WACV 2022 90.7/86.2 76 .9/75.1 76 .2/74.7 84 .9/81.7 81 .9/78.9 94 .4/94.4 69 .5/78.1 54 .4/54.6 78.6/77.9
FrePGAN [22] AAAI 2022 99.0/99.9 80 .7/89.6 84 .1/98.6 69 .2/71.1 71 .1/74.4 99 .9/100.0 60 .3/71.7 70 .9/91.9 79.4/87.2
LGrad [55] CVPR 2023 99.9/100.0 94 .8/99.9 96.0/99.9 82 .9/90.7 85 .3/94.0 99 .6/100.0 72 .4/79.3 58 .0/67.9 86.1/91.5
UniFD [43] CVPR 2023 99.7/100.0 89 .0/98.7 83 .9/98.4 90 .5/99.1 87 .9/99.8 91 .4/100.0 89 .9/100.0 80 .2/90.2 89.1/98.3
Ours − 99.9/100.0 97 .2/ 99.8 98.8/99.9 99 .5/100.0 99 .3/100.0 99.8 / 100.0 99 .4/100.0 93 .2/98.098.4/99.7
Table 2. Accuracy and average precision comparisons with state-of-the-art methods on diffusion model dataset. Models here are
trained on the 4-class ProGAN data, except for †trained on their self-training data. Notations are consistent with Table 1.
Dataset Wang [59] Durall [11] Frank [12] PatchFor †[3] F3Net [45] Blend †[53] LGrad [55] UniFD [43] Ours
PNDM 50.8/90.3 44 .5/47.3 44 .0/38.2 50 .2/99.9 72 .8/99.5 48 .2/48.1 69 .8/98.5 75 .3/92.5 99.3/100.0
Guided 54.9/66.6 40 .6/42.3 53 .4/52.5 74 .2/81.4 69 .2/70.8 58 .3/63.486.6/100.075.7/85.1 76.1/92.0
DALL-E 51.8/61.3 55 .9/58.0 57 .0/62.5 79 .8/99.1 71 .6/79.9 52 .4/51.6 88 .5/97.3 89 .5/96.8 98.8/99.8
VQ-Diffusion 50.0/71.0 38 .6/38.3 51 .7/66.7 100 .0/100.0 100 .0/100.0 77 .1/82.6 96 .3/100.0 83 .5/97.7100.0/100.0
LDM200 steps 52.0/64.5 61 .7/61.7 56 .4/50.9 95 .6/99.9 73.4/83.3 52 .6/51.9 94 .2/99.1 90 .2/97.1 98.6/99.8
200 w/ CFG 51.6/63.1 58 .4/58.5 56 .5/52.1 94 .0/99.8 80.7/89.1 51 .9/52.6 95.9/99.2 77 .3/88.6 94.9/99.1
100 steps 51.9/63.7 62 .0/62.6 56 .6/51.3 95 .8/99.8 74 .1/84.0 53 .0/54.0 94 .8/99.2 90 .5/97.0 98.7/99.9
Glide100-27 53.0/71.3 48 .9/46.9 50 .4/40.8 82 .8/99.1 87 .0/94.5 59 .4/64.1 87 .4/93.2 90 .7/97.2 94.4/99.1
50-27 54.2/76.0 51 .7/49.9 52 .0/42.3 84 .9/98.8 88 .5/95.4 64 .2/68.3 90 .7/95.1 91 .1/97.4 94.7/99.4
100-10 53.3/72.9 54 .9/52.3 53 .6/44.3 87 .3/99.7 88.3/95.4 58 .8/63.2 89 .4/94.9 90 .1/97.0 94.2/99.2
Mean 52.4/70.151.7/51.853.2/50.2 84.5/97.8 80.6/89.2 57.6/60.0 89.4/97.7 85.4/94.6 95.0/98.8
pervision, when compared with pre-trained-based meth-
ods. Moreover, we also compare FatFormer with repre-
sentative image-based [59] and frequency-based methods
[11, 12, 21, 22, 45] in Table 1. Our approach can also easily
outperform all of them with a larger improvement.
The above evidence indicates the necessity of forgery
adaptation for pre-trained models. Beyond the impressive
performance, more importantly, our FatFormer provides an
effective paradigm of how to incorporate pre-trained models
in the synthetic image detection task.
Comparisons on diffusion model dataset. To further
demonstrate the effectiveness of FatFormer, we provide
comparisons with existing detection methods on the dif-
fusion model dataset [43]. The results are shown in Ta-
ble 2. Note that all the compared methods are trained on4-class ProGAN data. This test setting is more challenging
as forged images are created by various diffusion models
with completely different generating theories and processes
from GANs. Surprisingly, FatFormer generalizes well for
diffusion models, achieving 95.0%ACC and 98.8%AP.
Compared with pre-trained-based LGrad [55] and
UniFD [43], FatFormer also works better than both of them
when handling diffusion models. For example, our ap-
proach surpasses UniFD by 9.6%ACC and 4.2%AP. More-
over, we find that even with powerful CLIP as the pre-
trained model, UniFD only achieves a similar result (about
85% ACC) like PatchFor [3]. We argue this is mainly be-
cause the fixed pre-trained paradigm is prone to yield de-
tectors with insufficient learning regarding forgery artifacts.
Thus, our FatFormer, which presents an adaptive trans-
10775
Table 3. Ablation experiments for FatFormer. Evaluated on GANs dataset. Default settings are marked in gray .
(a)Forgery-aware adapter implementations. In the
proposed framework, both image (img) and frequency
(freq) domains are essential for building generalized
forgery representation.
w/ img domain w/ freq domain ACC M APM
✓ ✓ 98.4 99.7
✓ × 95.4 99 .6
× ✓ 97.3 99 .6(b) Frequency band interactions.
Both inter- and intra-band attentions
are important for modeling forgery
traces in the frequency domain.
interaction ACC M APM
intra 97.4 99 .7
inter 96.6 99 .6
intra & inter 98.4 99.7(c)Benefits of supervision in vision-language space.
On the model only with img input, text is first added
for building contrastive (contra) objectives. Then, we
apply the proposed augmented (aug) contra strategy.
input modality strategy ACC M APM
only img linear probe 95.3 99 .2
img&text contra 96.4 99 .6
img&text aug contra 98.4 99.7
(d)Text prompt designs. The auto embedding and img condition
can benefit the performance, especially by considering the corre-
lation between prompt and img patch tokens.
prompt designs w/ img condition ACC M APM
fixed template × 95.5 99 .6
auto embedding × 96.4 99 .6
auto embedding CLS token 98.1 99 .7
auto embedding patch tokens 98.4 99.7(e)Model components. Both components are essential in our FatFormer. We also
conduct an extra experiment to test the zero-shot performance by removing the
forgery-aware adapter and language-guided alignment.
module components ACC M APM
none for zero-shot 66.6 74 .3
forgery-aware adapter 95.3 99 .2
language-guided alignment 91.5 98 .1
forgery-aware adapter &language-guided alignment 98.4 99.7
former framework with forgery adaptation and reasonable
contrastive objectives, can achieve much better results.
4.3. Ablation study
We conduct several ablation experiments to verify the effec-
tiveness of key elements in our FatFormer. Unless specified,
we report the mean of accuracy (ACC M) and average preci-
sion (AP M) on the GANs dataset under the training setting
of 4-class ProGAN data.
Forgery-aware adapter implementations. We ablate the
effects of considering the image domain and frequency do-
main in the forgery-aware adapter. The results are shown in
Table 3a. We observe severe performance degradation when
removing either of these two domains, especially for the fre-
quency domain with over −3.0%ACC gaps. We conclude
that both image and frequency domains are essential in Fat-
Former for synthetic image detection. The image forgery
extractor collects the local low-level forgery artifacts, e.g.,
blur textures, while the frequency forgery extractor explores
and gathers the forgery clues among different frequency
bands, together building a comprehensive local viewpoint
for the adaptation of image features. For the frequency
forgery extractor, both interactions built by inter-band and
intra-band attentions are important in our FatFormer. Ta-
ble 3b shows the ablation.
Benefits of supervision in vision-language space. Ta-
ble 3c provides the comparisons between different super-
vising strategies for FatFormer, including (i) linear prob-
ing with image modality, (ii) vanilla contrastive objectives
between image CLS token and text prompt embeddings,
which masked out the text-guided interactor, and (iii) our
augmented contrastive objectives. The results demonstrate
that introducing text prompts for contrastive supervision
benefits the generalization of detection. We conjecture this
is mainly because CLIP provides a stable alignment be-
tween real image and text representation with pre-training,
thus yielding a mismatching when handling a fake imagewith text prompts. As potential evidence, we find that only
adopting LGA can still achieve an accuracy of 91.5%ACC
(Table 3e). Besides, we observe that the proposed aug-
mented contrastive objectives can further boost generaliz-
ability by directing the image encoder to concentrate on
forgery-related representations, bringing a 2.0%ACC gain
over the vanilla implementation.
Text prompt designs. Table 3d gives the results of con-
structing the text prompt with different prompt designs and
image conditions. The results validate that both auto con-
text embeddings and image conditions are important in text
prompt designs. Compared with using a fixed hand-crafted
template, e.g., ‘this photo is’, the design of auto context em-
bedding improves by 0.9%ACC, due to its abstract explo-
ration in word embedding spaces. Besides, it is better to
adopt image patch tokens as conditions to enhance these
auto context embeddings, containing more local context de-
tails, rather than the global image CLS token.
Model components. Tabel 3e gives the ablation of two
proposed model components, i.e., forgery-aware adapter
and language-guided alignment. Large performance drops
(−6.9%ACC and −1.6%AP) are observed when adopt-
ing the previous fixed pre-trained paradigm by removing
the forgery-aware adapter. This explains the necessity of
forgery adaptation of pre-trained models. On the other
hand, the proposed language-guided alignment, which con-
siders the augmented contrastive objectives in the vision-
language space, also provides better supervision for the
forgery adaptation than simply adopting binary labels,
bringing 3.1%ACC and 0.5%AP gains. As shown in Fig-
ure 4, using language-guided alignment obtains more con-
centration on semantic foreground patches, where anoma-
lies, e.g., unrealistic objects, textures, or structures often
occur. Therefore, our FatFormer can obtain generalized
forgery representations by focusing on local forgery details,
resulting in the improvement of the generalizability of syn-
thetic image detection.
10776
StyleGANLDMCycleGANGlideStarGAN
Real
w/o LGAw/  LGA
Figure 4. Comparison of model attention with (w/ ) and without (w/o) language-guided alignment. We visualize the gradient norm of
FatFormer (second row) and FatFormer without language-guided alignment (first row) by [51]. FatFormer provides more responses among
semantic foreground patches in fake images, while almost no response for real ones. The salient region is visualized by bright color.
4.4. More analysis
Here, we analyze our FatFormer on different architectures
and pre-training strategies. More analysis of robustness
to image post-processing and efficiency evaluation can be
found in the Supplementary Material.
Analysis on different architectures. While FatFormer
is constructed upon the identical CLIP framework [46] as
employed in UniFD [43], the proposed forgery adaptation
strategy is transferrable to alternative architectures. Pre-
sented in the upper section of Table 4 are the ACC Mand
APMscores for four distinct architectures, including two
variations of multi-modal structures pre-trained by CLIP
and two variants of image-based Swin transformer [39] pre-
trained on ImageNet 22k [8]. The comparisons between
models with and without FatFormer verify the efficacy of
integrating forgery adaptation among different pre-trained
architectures, significantly facilitating the performance of
synthetic image detection.
Analysis on different pre-training strategies. We further
conduct an assessment of the efficacy of forgery adaptation
across models employing different pre-training strategies.
Utilizing ViT-L [10] as the baseline, we validate two well-
known pre-training approaches: MAE [15] and CAE [6].
The evaluations are shown in the lower segment of Table 4.
We observe that incorporating the forgery adaptation in our
FatFormer can lead to a consistent increase in performance
across diverse pre-training strategies, demonstrating the ro-
bustness and transferability of our approach.
5. Conclusion
In this paper, we present a novel adaptive transformer, Fat-
Former, for generalizable synthetic image detection. With
two core designs, including the forgery-aware adapter and
language-guided alignment, for the forgery adaption of pre-
trained models, the proposed approach outperforms the pre-
vious fixed pre-trained paradigm by a large margin. Be-
sides, the forgery adaption in FatFormer is also flexible,Table 4. Analysis on different architectures and pre-training
strategies. Beyond UniFD, the forgery adaptation in FatFormer
can also consistently boost various architectures and different pre-
training strategies. We report the mean of ACC and AP (in the
formulation of ACC M/ APM) on both GANs and diffusion model
(DMs) datasets. ‘IN-22K’= ImageNet 22k.
Architecture Pre-training w/ Ours GANs DMs
ViT-B/16
Text-512CLIP [46]× 83.8/94.4 77 .2/91.1
✓ 95 .3/99 .5 91 .6/97 .8
ViT-L/14
Text-768CLIP [46]× 89.1/98.3 85 .4/94.6
✓ 98 .4/99 .7 95 .0/98 .8
Swin-B IN-22K [8]× 82.5/93.8 72 .2/88.8
✓ 89 .6/98 .2 76 .1/96 .1
Swin-L IN-22K [8]× 86.4/95.7 74 .4/90.8
✓ 90 .7/98 .4 79 .3/96 .7
ViT-L/16MAE [15]× 75.7/92.8 70 .9/92.3
✓ 85 .2/96 .7 88 .5/98 .4
CAE [6]× 76.1/95.9 64 .9/91.7
✓ 88 .1/98 .0 76 .1/96 .2
which can be applied in various pre-trained architectures
with different pre-training strategies. We hope FatFormer
can provide insights for exploring better utilization of pre-
trained models in the synthetic image detection field.
Limitations and future works. FatFormer generalizes well
on most generative methods, while we still have space to
improve in diffusion models, e.g., Guided [9]. Elucidating
the distinctions and associations among images produced
by diffusion models and GANs is needed to build stronger
forgery detectors. The investigation of this problem is left
in future work. Besides, how to construct a better pretext
task special for synthetic image detection in pre-training is
also worth a deeper study.
Acknowledgements. This work was supported in
part by the National Key R&D Program of China
(No.2021ZD0112100), National NSF of China
(No.U1936212, No.62120106009, No.U23A20314).
10777
References
[1] E Oran Brigham and RE Morrow. The fast fourier transform.
IEEE Spectrum , 4(12):63–70, 1967. 4
[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high fidelity natural image synthesis.
InInternational Conference on Learning Representations ,
2018. 5
[3] Lucy Chai, David Bau, Ser-Nam Lim, and Phillip Isola.
What makes fake images detectable? understanding prop-
erties that generalize. In Proceedings of the European Con-
ference on Computer Vision , pages 103–120. Springer, 2020.
1, 5, 6
[4] Shen Chen, Taiping Yao, Yang Chen, Shouhong Ding, Jilin
Li, and Rongrong Ji. Local relation learning for face forgery
detection. In Proceedings of the AAAI conference on artifi-
cial intelligence , pages 1081–1088, 2021. 4
[5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. Adaptformer:
Adapting vision transformers for scalable visual recogni-
tion. Advances in Neural Information Processing Systems ,
35:16664–16678, 2022. 3
[6] Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin,
Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang
Zeng, and Jingdong Wang. Context autoencoder for self-
supervised representation learning. International Journal of
Computer Vision , pages 1–16, 2023. 8
[7] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: Unified genera-
tive adversarial networks for multi-domain image-to-image
translation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8789–
8797, 2018. 5
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 248–255.
Ieee, 2009. 8
[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in Neural Informa-
tion Processing Systems , 34:8780–8794, 2021. 1, 5, 8
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In International Con-
ference on Learning Representations , 2020. 1, 2, 3, 4, 8
[11] Ricard Durall, Margret Keuper, and Janis Keuper. Watch
your up-convolution: Cnn based generative deep neural net-
works are failing to reproduce spectral distributions. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 7890–7899, 2020. 5, 6
[12] Joel Frank, Thorsten Eisenhofer, Lea Sch ¨onherr, Asja Fis-
cher, Dorothea Kolossa, and Thorsten Holz. Leveraging fre-
quency analysis for deep fake image recognition. In Interna-
tional Conference on Machine Learning , pages 3247–3258.
PMLR, 2020. 1, 3, 5, 6[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
Neural Information Processing Systems , 27, 2014. 1
[14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-
tor quantized diffusion model for text-to-image synthesis. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 10696–10706, 2022. 1,
5
[15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16000–
16009, 2022. 3, 8
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840–6851, 2020. 1
[17] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for nlp. In International Conference on Machine
Learning , pages 2790–2799. PMLR, 2019. 3
[18] Dapeng Hu, Shipeng Yan, Qizhengqiu Lu, HONG Lanqing,
Hailin Hu, Yifan Zhang, Zhenguo Li, Xinchao Wang, and
Jiashi Feng. How well does self-supervised pre-training per-
form with streaming data? In International Conference on
Learning Representations , 2021. 2
[19] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-
rank adaptation of large language models. In International
Conference on Learning Representations , 2021. 3
[20] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui
Huang, Rynson WH Lau, Wanli Ouyang, and Wangmeng
Zuo. Clip2point: Transfer clip to point cloud classifica-
tion with image-depth pre-training. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 22157–22167, 2023. 3
[21] Yonghyun Jeong, Doyeon Kim, Seungjai Min, Seongho Joe,
Youngjune Gwon, and Jongwon Choi. Bihpf: Bilateral high-
pass filters for robust deepfake detection. In Proceedings of
the IEEE/CVF Winter Conference on Applications of Com-
puter Vision , pages 48–57, 2022. 3, 5, 6
[22] Yonghyun Jeong, Doyeon Kim, Youngmin Ro, and Jongwon
Choi. Frepgan: robust deepfake detection using frequency-
level perturbations. In Proceedings of the AAAI Conference
on Artificial Intelligence , pages 1060–1068, 2022. 3, 4, 5, 6
[23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In European Conference on Computer
Vision , pages 709–727. Springer, 2022. 3
[24] Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, and
Humphrey Shi. Learning mask-aware clip representations
for zero-shot segmentation. Advances in Neural Information
Processing Systems , 36:35631–35653, 2023. 3
[25] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
10778
and variation. In International Conference on Learning Rep-
resentations , 2018. 1, 2, 5
[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 4401–4410, 2019. 2,
5
[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8110–8119, 2020. 1, 5
[28] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[29] Pavel Korshunov, Anubhav Jain, and S ´ebastien Marcel. Cus-
tom attribution loss for improving generalization and inter-
pretability of deepfake detection. In IEEE International Con-
ference on Acoustics, Speech and Signal Processing , pages
8972–8976. IEEE, 2022. 3
[30] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efficient prompt tuning. In Proceed-
ings of the 2021 Conference on Empirical Methods in Natu-
ral Language Processing , pages 3045–3059, 2021. 3
[31] Qiufu Li, Linlin Shen, Sheng Guo, and Zhihui Lai. Wavelet
integrated cnns for noise-robust image classification. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 7245–7254, 2020. 4
[32] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei
Lyu. Celeb-df: A large-scale challenging dataset for deep-
fake forensics. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3207–
3216, 2020. 2
[33] Ajian Liu and Yanyan Liang. Ma-vit: Modality-agnostic vi-
sion transformers for face anti-spoofing. In Proceedings of
the International Joint Conference on Artificial Intelligence ,
pages 1180–1186, 2022. 1
[34] Ajian Liu, Zichang Tan, Jun Wan, Yanyan Liang, Zhen Lei,
Guodong Guo, and Stan Z Li. Face anti-spoofing via ad-
versarial cross-modality translation. IEEE Transactions on
Information Forensics and Security , 16:2759–2772, 2021.
[35] Ajian Liu, Zichang Tan, Zitong Yu, Chenxu Zhao, Jun Wan,
Yanyan Liang Zhen Lei, Du Zhang, Stan Z Li, and Guodong
Guo. Fm-vit: Flexible modal vision transformers for face
anti-spoofing. IEEE Transactions on Information Forensics
and Security , 2023. 1
[36] Huan Liu, Zichang Tan, Qiang Chen, Yunchao Wei, Yao
Zhao, and Jingdong Wang. Unified frequency-assisted trans-
former framework for detecting and grounding multi-modal
manipulation. arXiv preprint arXiv:2309.09667 , 2023. 1
[37] Huan Liu, Xiaolong Liu, Zichang Tan, Xiaolong Li, and Yao
Zhao. Padvg: A simple baseline of active protection for
audio-driven video generation. ACM Transactions on Mul-
timedia Computing, Communications and Applications , 20
(6), 2024. 1
[38] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo
numerical methods for diffusion models on manifolds. In In-ternational Conference on Learning Representations , 2021.
5
[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , 2021. 8
[40] Stephane G Mallat. A theory for multiresolution signal de-
composition: the wavelet representation. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 11(7):674–
693, 1989. 2, 4
[41] Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and
Giovanni Poggi. Do gans leave artificial fingerprints? In
IEEE Conference on Multimedia Information Processing
and Retrieval , pages 506–511. IEEE, 2019. 3
[42] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya
Sutskever, and Mark Chen. Glide: Towards photorealis-
tic image generation and editing with text-guided diffusion
models. In International Conference on Machine Learning ,
pages 16784–16804. PMLR, 2022. 1, 5
[43] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards uni-
versal fake image detectors that generalize across genera-
tive models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 24480–
24489, 2023. 1, 2, 3, 5, 6, 8
[44] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2337–
2346, 2019. 5
[45] Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing
Shao. Thinking in frequency: Face forgery detection by min-
ing frequency-aware clues. In Proceedings of the European
Conference on Computer Vision , pages 86–103. Springer,
2020. 1, 3, 4, 5, 6
[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1, 2, 3, 8
[47] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821–8831. PMLR, 2021.
5
[48] K Ramamohan Rao and Ping Yip. Discrete cosine transform:
algorithms, advantages, applications . Academic Press Pro-
fessional, Inc., USA, 1990. 4
[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 2, 5
[50] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Chris-
tian Riess, Justus Thies, and Matthias Nießner. Faceforen-
sics++: Learning to detect manipulated facial images. In
10779
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 1–11, 2019. 2, 3, 5
[51] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,
Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
Grad-cam: Visual explanations from deep networks via
gradient-based localization. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 618–
626, 2017. 8
[52] Rui Shao, Tianxing Wu, and Ziwei Liu. Detecting and
grounding multi-modal media manipulation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 6904–6913, 2023. 1
[53] Kaede Shiohara and Toshihiko Yamasaki. Detecting deep-
fakes with self-blended images. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18720–18729, 2022. 5, 6
[54] Chuangchuang Tan, Huan Liu, Yao Zhao, Shikui Wei,
Guanghua Gu, Ping Liu, and Yunchao Wei. Rethinking
the up-sampling operations in cnn-based generative net-
work for generalizable deepfake detection. arXiv preprint
arXiv:2312.10461 , 2023. 1
[55] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu,
and Yunchao Wei. Learning on gradients: Generalized arti-
facts representation for gan-generated images detection. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 12105–12114, 2023. 1,
3, 5, 6
[56] Chuangchuang Tan, Ping Liu, RenShuai Tao, Huan Liu, Yao
Zhao, Baoyuan Wu, and Yunchao Wei. Data-independent
operator: A training-free artifact representation extrac-
tor for generalizable deepfake detection. arXiv preprint
arXiv:2403.06803 , 2024. 1
[57] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu,
Ping Liu, and Yunchao Wei. Frequency-aware deepfake de-
tection: Improving generalizability through frequency space
learning. In Proceedings of the AAAI Conference on Artifi-
cial Intelligence , 2024. 1
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in Neural
Information Processing Systems , 30, 2017. 2, 3, 4
[59] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew
Owens, and Alexei A Efros. Cnn-generated images are
surprisingly easy to spot... for now. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8695–8704, 2020. 1, 3, 5, 6
[60] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun
Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for
diffusion-generated image detection. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
2023. 3, 5
[61] Ning Yu, Larry S Davis, and Mario Fritz. Attributing fake
images to gans: Learning and analyzing gan fingerprints. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7556–7566, 2019. 1
[62] Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen,
and Yunchao Wei. Slca: Slow learner with classifier align-
ment for continual learning on a pre-trained model. InProceedings of the IEEE/CVF International Conference on
Computer Vision , pages 19148–19158, 2023. 3
[63] Hanqing Zhao, Wenbo Zhou, Dongdong Chen, Tianyi Wei,
Weiming Zhang, and Nenghai Yu. Multi-attentional deep-
fake detection. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 2185–
2194, 2021. 4
[64] Tianchen Zhao, Xiang Xu, Mingze Xu, Hui Ding, Yuanjun
Xiong, and Wei Xia. Learning self-consistency for deepfake
detection. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 15023–15033, 2021.
3
[65] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Conditional prompt learning for vision-language mod-
els. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 16816–16825,
2022. 3, 4
[66] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. In-
ternational Journal of Computer Vision , 130(9):2337–2348,
2022. 3, 4
[67] Hongguang Zhu, Yunchao Wei, Xiaodan Liang, Chunjie
Zhang, and Yao Zhao. Ctp: Towards vision-language con-
tinual pretraining via compatible momentum contrast and
topology preservation. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 22257–
22267, 2023. 3
[68] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 2223–2232, 2017. 5
[69] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi,
Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A
comprehensive survey on transfer learning. Proceedings of
the Institute of Electrical and Electronics Engineers , 109(1):
43–76, 2021. 3
10780
