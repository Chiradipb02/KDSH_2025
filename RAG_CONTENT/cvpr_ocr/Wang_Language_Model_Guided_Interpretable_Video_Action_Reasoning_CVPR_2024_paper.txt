Language Model Guided Interpretable Video Action Reasoning
Ning Wang1∗, Guangming Zhu1†*,H SL i1, Liang Zhang1†, Syed Afaq Ali Shah2, Mohammed Bennamoun3
1Xidian University,2Edith Cowan University,3University of Western Australia
{ningwang, hsli }@stu.xidian.edu.cn, {gmzhu, liangzhang }@xidian.edu.cn,
afaq.shah@ecu.edu.au, mohammed.bennamoun@uwa.edu.au
Abstract
While neural networks have excelled in video action
recognition tasks, their ”black-box” nature often obscuresthe understanding of their decision-making processes. Re-cent approaches used inherently interpretable models to an-alyze video actions in a manner akin to human reason-ing. These models, however , usually fall short in perfor-mance compared to their “black-box” counterparts. Inthis work, we present a new framework named Language-
guided Interpretable Action Recognition framework ( La-
IAR ). LaIAR leverages knowledge from language models
to enhance both the recognition capabilities and the inter-pretability of video models. In essence, we redeﬁne theproblem of understanding video model decisions as a taskof aligning video and language models. Using the logicalreasoning captured by the language model, we steer thetraining of the video model. This integrated approach notonly improves the video model’s adaptability to different do-mains but also boosts its overall performance. Extensive ex-periments on two complex video action datasets, Charades& CAD-120, validates the improved performance and inter-pretability of our LaIAR framework. The code of LaIAR is
available at https://github.com/NingWang2049/LaIAR .
1. Introduction
Building on the advancements of deep learning in image
recognition [ 9,16,31], neural network (NN) models have
become the leading approach for video-related challenges,
including action recognition [ 18,25,30]. Yet, many of the
top-tier action recognition techniques [ 19,34] deploy NNs
in an opaque, black-box fashion. This lack of transparencydoes not offer clear justiﬁcation for their decisions, hinder-ing their utility in various real-world contexts [ 13], espe-
cially those with rigorous security demands. These consid-
erations drive us to develop an action reasoning system thatpairs exceptional performance with clear interpretability.
*Ning Wang and Guangming Zhu are co-ﬁrst authors.
†Liang Zhang and Guangming Zhu are both the corresponding authors.language
model
video
modelKnowledge Guidedlanguage
model
video
modellanguage
model
(b) Traditional two-stage interpretable reasoning model.
(c) Language model guided video interpretable reasoning model.Action : “Someone is standing up from somewhere”
(a) An illustration example of rela tionship transition in the videos.t+nt
Training Phase Inference Phase
t t+n…
second stage first stage
Time
language description language description
uage langu
del mo
language
 gg description
 p
only using 
single modal
Figure 1. (a) An example of action that can be decomposed into
relationship transitions ( i.e., when the transition is ‘sitting on’ →
‘not contacting’ between <person, bed >pair, it represents the
action ”Someone is standing up from somewhere”.). (b) Tradi-tional two-stage methods usually predict the scene graph ﬁrst, andthen use language models to capture the semantic-level relation-
ship transitions. (c) Our method exploits a language model to
guide the video model to capture the relationship transition dur-ing training. During inference, our method processes videos anddirectly recognizes actions, providing supportive evidence.
Most of the current interpretable action recognition tech-
niques [ 21,22,29] aim to elucidate the decision-making
process of NNs using post-hoc explanations, with a particu-
lar emphasis on gradient-based and perturbation-based ap-proaches. However, despite notable advancements, theseexplanations can be problematic because they might notbe faithful to what the network computes, as highlightedby [ 27]. A compelling direction in interpretability re-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18878
volves around the concept of built-in explanation models
[10,12,24,42]. The essence of these models is their in-
herent interpretability right from the design stage. Recentstrategies decompose a complex action into temporal tran-sitions of human-object relationships, drawing inspiration
from the event segmentation theory [ 17]. An illustration
in Figure 1(a) depicts that for a relationship involving a
<person, bed >pairing, a transition sequence from ‘sitting
on’ to‘not contacting’ signiﬁes the action of ”Someone
is standing up from somewhere”. This methodology facili-tates action recognition by pinpointing semantic transitions
through language models, offering a granular insight intoaction execution. As shown in Figure 1(b), Jin and Ou
et al. [ 12,24] extract spatio-temporal scene graphs from
video content and apply Markov Logic Network (MLN)based probabilistic logical inference and relation reasoninggraphs to create an interpretable representation for a varietyof complex actions, respectively. However, it is believed
that such models will perform worse than their black-boxalternatives [8]. Moreover, these methods divide the pro-
cess into two stages, namely scene graph prediction and re-lation modeling. Optimizing these components separatelymight lead to sub-optimal results. In this paper, we proposeto harness the explicit logical inference rules of an inter-pretable language model to guide the learning process ofa video black-box model. Interpretability and strong per-formance can be attained by focusing solely on the videomodel during the inference stage. To achieve this, two mainchallenges arise: 1) Designing a language model that canautomatically grasp logical reasoning patterns, sidesteppingmanual rule creation. 2) Developing a decoupled language-video model architecture that enables the language modelto guide the video model’s training process.
To address the aforementioned challenges, we have
developed a new framework called Language-guided
Interpretable Action Recognition framework named La-
IAR . As depicted in Figure 1(c), LaIAR constructs an ac-
tion recognition model that both implicitly and explicitlyexploits ﬁne-grained knowledge of relationship transitionsfrom an interpretable built-in language model. Speciﬁcally,we use dynamic token transformers (DT-Former) to bothvideo and language inputs, selectively focusing on impor-
tant relationships in a data-driven manner, and disregarding
non-contributory ones. We aim to redeﬁne the traditionaldecision interpretation challenge of video models towards avisual-language relation alignment problem. The relation-
ship prioritization determined by the language model thenexplicitly guides the video model in identifying the most
relevant relationships . We propose a learning strategy to
facilitate knowledge transfer between language and video toimprove the performance of the video model. A key featureofLaIAR is its modular design: during the inference phase,
only RGB data serves as input to predict actions, providinga direct and transparent justiﬁcation.
To summarize, our contributions are three-fold: 1) We
propose a novel LaIAR framework that can automatically
mine ﬁne-grained relation transitions from data and create
interpretable representations for various complex actions.
2) We design a decoupled cross-modal knowledge trans-fer architecture that leverages useful knowledge from lan-guage models to improve the performance and interpretabil-ity of the video model at training time, and achieves high-performance interpretable reasoning for videos at test time.
3) Our method achieves state-of-the-art results on two large-
scale action recognition benchmarks.
2. Related Work
2.1. Interpretable Video Action Recognition
Interpretable video action recognition methods can be
categorized into two types: post-hoc method and built-
inmethod. post-hoc techniques generate explanations for
the network’s decision-making process after the network is
trained. [ 22] introduced an interpretable and easy plug-
in spatial-temporal attention mechanism for video actionrecognition to improve the interpretability of the model forvideo action recognition. [ 29] developed an interpretable
temporal convolutional network to explain the decision-making process of action recognition through each of the
learned ﬁlters in a Res-TCN. [ 21] combined both global
dynamics and local details to learn human action, usinggradient-weighted class activation mapping (Grad-CAM)to visualize the model’s attention to action-critical regions.Although these methods have the advantage of not imposingany model constraints, they may be incomplete or unfaithful
to the model’s reasoning [ 27]. In contrast, built-in methods
restrict the interpretation to be consistent with the model’sinferences. [ 42] approached action reasoning by model-
ing semantic-level state transitions between two consecu-tive frames as deﬁned by domain experts. In [ 10], a method
is proposed to achieve interpretability of action recogni-
tion by incorporating qualitative spatial reasoning and ex-tracting salient relation chains. Some recent methods, like[12,24] decompose complex action into continuous rela-
tionship transitions according to the event segmentation the-ory [ 17]. These methods model the relationship transitions
at the semantic level to recognize actions. In this paper, wepropose to construct a high performance interpretable-by-design action classiﬁer by guiding a video model with aninterpretable language model.
2.2. Adaptive Inference in Transformers.
As their popularity soars, adaptive inference for lan-
guage and vision transformers has caught the attention ofresearchers. In [ 37], an adaptive language transformer is
proposed to achieve ﬁxed-scale reduction of the input se-
18879
quence to improve inference speed by dynamically select-
ing important tokens and removing the irrelevant ones. In[23], a threshold mechanism is introduced to determine the
importance of each token and dynamically select the to-kens according to the importance of the input sequence.[14] used the mean of attention matrix column values of
the transformers to determine the importance score of eachtoken, facilitating token pruning. [ 38] developed an adap-
tive token generation mechanism to determine the requirednumber and size of tokens, thereby reducing the computa-tional and memory overhead of the model on images. [ 32]
devised a token selection framework to dynamically selectimportant tokens across the temporal and spatial dimensions
of the video input. In this paper, we propose a lightweighttoken selection method based on Gumbel-Softmax and ap-ply it to our cross-modal transformers for spatio-temporaltoken selection.
2.3. Cross-modal Knowledge Transferring
The past few years have seen an increasing interest in
cross-modal knowledge transfer techniques for detectionand segmentation tasks. [ 20] introduced a method to trans-
fer the motion-related knowledge of unlabeled videos toHuman-Object Interactions (HOI) detection to infer rareor unseen HOIs. In [ 40], reliable domain-invariant sound
cues are exploited to help video activity recognition modelsadapt to video distribution shifts. Lately, knowledge distil-lation techniques have been extended to transfer knowledge
across different modalities. For instance, [ 18] proposed
a decomposed cross-modal distillation framework to im-prove RGB-based temporal action detection by transferringknowledge from the optical ﬂow modality. Similarly, [ 39]
proposed a modiﬁed knowledge distillation method thatboosts the performance of single-modal 3D captioning bytransferring color and texture-aware information from 2Dimages into 3D object representations. In contrast to these
methods, we propose a well-designed knowledge-guidedframework to enable cross-modal learning by decouplinginformation transfer in video and language.
3. The Proposed Approach
Our proposed method is designed to exploit multi-
modality by enabling information transfer from language
descriptions to videos. This enables the video modelto effectively learn from the language model effectively.This is achieved by the video model mimicking the out-put of the language model, thereby leveraging the intrin-sic capabilities of the language model. Speciﬁcally, thevideo frames and the language description (represented asa spatio-temporal scene graph in [ 11]) are ﬁrst processed
by the encoding network to extract the paired visual andsemantic relationship representations. Then , these paired
visual and semantic relationship representations are sepa-rately fed to DT-Former module, which models the key re-
lationship transition for action recognition. Finally , we pro-
pose a learning scheme ( i.e., Joint Embedding Space, To-
ken Selection Supervision and Cross-Modal Learning) toimprove the performance and interpretability of the videomodel by facilitating the knowledge transfer from the lan-guage model to the video model. An overview of our pro-posed method is shown in Figure 2(a). Note that, in our
proposed approach, only the video model is employed forinference once training is complete.
3.1. Architecture
3.1.1 Video and Language Encoder
Given a video consisting of Tframes with Nentities
of either human or object classes, we use Faster R-CNN[26] with ResNet-101 [ 16] backbone to detect these entities
and extract their features from the video. For the frame I
t
at time step t, the visual features {v(t,1),v(t,2),...,v (t,N)},
bounding boxes {b(t,1),b(t,2),...,b (t,N)}and object cate-
gory{c(t,1),c(t,2),...,c (t,N)}of the objects proposals are
supplied by the detector. Between each <human, object >
pair in the frame, there is a set of relationships Rt=
{r(t,1),r(t,2),...,r (t,K)}. Concatenating the visual appear-
ance, spatial information and category embedding betweenthei-th human and j-th object proposals can represent the
visual relation feature v
(t,k), as follows:
v(t,k)=/bracketleftbig
Wsv(t,i),Wov(t,j),Wuϕ(u(t,ij)⊕fbox(b(t,i),b(t,j)))/bracketrightbig
(1)
whereWs,WoandWurepresent the parameter matrix of
the linear transformation. [,]is concatenation operation,
ϕis ﬂattening operation and ⊕is element-wise addition.
u(t,ij)the visual feature of the union box of b(t,i)andb(t,j)
extracted from the detector. fboxmaps the 2-channel binary
spatial conﬁguration map of bounding boxes b(t,i)andb(t,j)
into features of the same dimension as u(t,ij).
Unlike the visual relation feature, the semantic relation
feature s(t,k)provides high-level descriptions of the rela-
tionship between humans and objects in the videos. The vi-
sual relationship categories are either provided as ground-truth or determined by the ﬁne-tuned visual relationship de-tection network [ 5]. The features of the semantic relation
are obtained by concatenating the three features as follows:
s
(t,k)=/bracketleftbig
s(t,i),r(t,ij),s(t,j)/bracketrightbig
(2)
where the r(t,ij)is extracted by embedding the visual rela-
tionship category to the semantic feature space. The cate-gory embedding vectors s
(t,i)ands(t,j)are determined by
the categories of human and object, respectively.
Given the features {v(t,k)}T,K
t=1,k=1and the features
{s(t,k)}T,K
t=1,k=1, we further map the visual relation feature
and the semantic relation feature into a joint embedding
18880
SharedCsܲ௦
…
…
ŚJoint Embedding Space…
Video
EncoderCvCv2s
ܲ௩ܲ௩ଶ௦ܦ௄௅ሺܲ௧ȁȁܲ௩ଶ௧ሻ
ŜCross-Modal
Learning
Temporal Token SelectSpatial Token SelectTransformer Encoder
cls
(b) DT-Former (a) overview of the proposed method.DT-Former
DT-Former
śToken Selection
Supervision
ݐݐ൅ͳ ݐ൅݊…࢙ࢠ
ܷ௦
ܷ௩
࢜ࢠ
Language
Encoder
Language model’s stream Video model’s streamVideo-level 
Features ࢠToken Selection
Signal Matrix ܷ
Video framesLabeled scene graph
Figure 2. Overview of our LaIAR . The architecture comprises a language model (top) which takes the language description (represented
as a spatio-temporal scene graph in [ 11]) as input and a video model (bottom) which takes the video frames as input. Both models use
DT-Former to capture key relational transitions to recognize actions. We transfer knowledge across modalities using a learning scheme(i.e., Joint Embedding Space, Token Selection Supervision and Cross-Modal Learning), which can help video model beneﬁt from language
model during training. For inference, only the video model is considered.
space as follows:
V=fV/parenleftBig
{v(t,k)}T,K
t=1,k=1/parenrightBig
,S=fS/parenleftBig
{{s(t,k)}T,K
t=1,k=1/parenrightBig
.
where the fVandfSare the visual encoder and the semantic
encoder, respectively. In each encoder, each element of theinput is ﬁrst mapped to a local representation via a linearprojection. We then apply a Generalized Pooling Operator(GPO) [ 4] to aggregate the input, creating a global repre-
sentation. This global representation is combined with eachlocal representation along the channel dimension. This ap-proach helps to use the contextual information from the en-tire sequence. The visual embedding V∈R
T×K×Dand se-
mantic embedding S∈RT×K×Dare aligned in the spatio-
temporal dimension, where Ddenotes the dimension in the
common space.
3.1.2 Dynamic Token Transformers
Video understanding shares several high-level similari-
ties with natural language processing (NLP), as they are
both fundamentally based on sequential structures [ 2]. Our
intuition is that we can easily model visual and semanticrelations simultaneously from joint embedding space (seeSec 3.2.1 for details). Therefore, we introduce a shared
dynamic token transformers (DT-Former), which employsthe transformer structure to capture key relationship transi-tions for action reasoning. It mainly consists of the adap-tive token selection and the video transformer module. Theadaptive token selection module calculates the contributionscore of each token to the classiﬁcation output, and tokenswith lower contribution scores will be discarded. The re-tained tokens, i.e. important relationship representations,
are fed to the video transformer module to capture rela-tion transition cues. Figure 2(b) shows the architecture
of DT-Former. Since the video model and the languagemodel share the same DT-Former, we denote the input as
X={x
(t,k)}T,K
t=1,k=1for simplicity. We add a learnable
spatiotemporal positional embedding epos
(t,k)to each vector
x(t,k)to obtain the embedding token x(0)
(t,k). The superscript
corresponds to the layer of the transformer encoder.
Adaptive T oken Selection. Following the ViT ap-
proach [ 6], we concatenate a special learnable vec-
tor (x(0)
(0,0)=xclass ) representing the embedding of the
[class] token in the ﬁrst position of the sequence. As
a large number of relationships between human and objectsin a scene are usually redundant, it is essential to reducethese relationships. Inspired by the recent work on tokenreduction for accelerating transformer inference, we formu-late parsing important relations as a token selection prob-lem. To determine whether a token is discarded or retained,we introduce a token selector that consists of an MLP σand
a differentiable discrete-valued estimator using the Gumbel-
Softmax (GSM) operator:
u
(t,k)=G S M{σ([W1x(0)
(t,k),W 2xclass])} (3)
whereW1andW2represent the linear matrices for dimen-
sion compression. We concatenate [class] tokens that
represent the global representation with input tokens to ex-ploit contextual information of the entire sequence. The bi-nary output u
(t,k)=0 indicates that the t-th token of frame
tis to be discarded and u(t,k)=1 is to be retained. Token
selection can be represented as: y(0)
(t,k)=u(t,k)x(0)
(t,k). Note
that this operation is differentiable, allowing for end-to-endtraining tailored for token selection.
To ensure consistent token reduction across consecu-
tive frames, we apply token selector to both the tempo-ral and spatial dimensions. Recent efforts in the ﬁeld
of frame sampling [ 41] indicate inherent temporal redun-
dancy in frames. Inspired by this, we ﬁrst focus on salient
18881
frames over the entire time horizon, and then delve into
those frames to ﬁnd key relationships. For the input tokens
X={x(0)
(t,k)}T,K
t=1,k=1,w e ﬁrst apply an average-pooling
operation to tokens in the spatial dimension to get a se-
quence of temporal-based tokens {x(0)
(t)}T
t=1, and then feed
it to the token selector to generate temporal-based selec-
tion signal matrix {`u(t)}T
t=1.Finally , we repeat it along
the spatial dimension to obtain `U={`u(t,k)}T,K
t=1,k=1for
downstream processing. Similarly, we perform the token
selector on each frame separately to generate a selection
signal matrix. The spatial-based selection signal matrix canbe expressed as ´U={´u
(t,k)}T,K
t=1,k=1. Further, the ﬁnal se-
lection signal matrix can be expressed as U=`U·´U=
{u(t,k)}T,K
t=1,k=1. We use matrix multiplication for token se-
lection:Y=U·X={y(0)
(t,k)}T,K
t=1,k=1.
Transformer Encoder. In order to model the relation-
ship transition in videos, the token Y={y(0)
(t,k)}T,K
t=1,k=1are
fed to stack of transformer blocks which compute the spa-tial and temporal self-attention jointly. We convert Yinto
a set of sequences Y
(0)={y(0)
(p)}T×K
p=1, which are then fed
into the transformer encoder to extract a video-level repre-sentation:
Y
/prime/lscript= MSA(LN( Y/lscript−1))+ Y/lscript−1(4)
Y/lscript= MLP(LN( Y/lscript))+ Y/prime/lscript(5)
z= LN( YL
(0)) (6)
where MSA() and LN() denotes multiheaded self-attention
and LayerNorm [ 1], respectively. Lrepresents the num-
ber of transformer blocks. zdenotes the video-level rep-
resentation, which can be used to predict the ﬁnal actionclasses. Based on the above steps, we can get the selection
signal matrix U
v={uv
(t,k)}T,K
t=1,k=1, video-level represen-
tation zvof the video model, the selection signal matrix
Us={us
(t,k)}T,K
t=1,k=1and video-level representation zsof
the language model.
3.1.3 Classiﬁcation Head
The two classiﬁcation heads of video and language
model predict, Pv={pv
(c)}C
c=1andPs={ps
(c)}C
c=1
respectively for each branch, where Cis the number of
classes. We minimize the cross-entropy losses between ac-tion scores P
v, andPsand the ground-truth action labels
for each action category, denoted as LvandLs, respec-
tively. The overall loss of two branches can be written as:
Lcls=Lv+Ls (7)
The two classiﬁcation heads only utilize the private in-
formation of each modality, in order to allow the videomodel to beneﬁt from the knowledge of the language model,we propose an additional classiﬁcation head to estimate the
other modality’s output: the video model estimates the lan-guage model ( P
v2s={pv2s
(c)}C
c=1). By mimicking not only
the class with maximum probability, but also the whole dis-tribution, more information is exchanged, leading to softerlabels, which is more beneﬁcial for training our model.
3.2. Learning Scheme
The goal of our learning scheme is to transfer informa-
tion across modalities in a controlled manner thus allowingthe video model to learn from the language model. Thisauxiliary objective can effectively improve the performanceof the video modality and does not require additional la-bels from the datasets. Here, we deﬁne the visual-semantic
joint embedding learning, our token selection supervision
loss and an additional cross-modal learning method.
3.2.1 Visual-Semantic Joint Embedding Space
Our approach begins by aligning the visual and seman-
tic relation representations within a shared vector space. Inthis conﬁguration, each visual embedding ˆv
(t,k)∈V and
ˆs(t,k)∈S pair converge to proximate points. This visual-
semantic joint embedding has two main advantages: 1) ithelps the video model to improve its generalization since se-
mantic representations are invariant to complex appearance
variations. 2) it enables the video model to explicitly rep-resent the relationship transition process, since the visual-semantic joint embedding space can provide semantic labelsfor each visual relation representation. In this paper, we
introduce the contrastive learning of visual-semantic joint
embedding and discuss its implementation as following.
Given a mini-batch B={(ˆv
(0,0),ˆs(0,0)),...}of visual-
semantic relationship representation pairs, the contrastivelearning objective encourages embeddings of positive pairs(ˆv
(t,k),ˆs(t,k))to align with each other, while pushing em-
beddings of the negative pairs apart. Formally, the con-trastive loss L
sim is formulated using the symmetric con-
trastive loss, as follows:
Lsim=−1
2|B||B|/summationdisplay
i=1⎛
⎜⎜⎜⎝visual→semantic/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
logetxi·yi
/summationtext|B|
j=1etxi·yj+semantic→visual/bracehtipdownleft /bracehtipupright/bracehtipupleft /bracehtipdownright
logetxi·yi
/summationtext|B|
j=1etxj·yi⎞
⎟⎟⎟⎠
(8)
where xi=v(t,k)
/bardblv(t,k)/bardbl2andyi=s(t,k)
/bardbls(t,k)/bardbl2.|B|is size of the
mini-batch B.tis the learnable temperature parameter.
3.2.2 Token Selection Supervision
We aim that the key relations obtained by the language
model can guide the video model to perform key relations
mining. Therefore, we align the token selection signal ma-trix of the two models. We minimize the mean-squared loss
18882
between the token selection signal matrix uv
(t,k)of the video
model and the token selection signal matrix us
(t,k)of the
language model. To maintain the token selection signal ma-trix’s sparsity, we apply L1 norm to allow u
s
(t,k)to have a
small number of non-zero values. The token selection su-pervision loss is deﬁned as:
L
tss=1
TT/summationdisplay
t=11
KK/summationdisplay
k=1/parenleftBig
/bardbluv
(t,k)−us
(t,k)/bardbl+/bardblus
(t,k)/bardbl1/parenrightBig
(9)
3.2.3 Cross-Modal Learning
Given that the visual modality is highly sensitive and
the semantic modality more robust to the domain shift,the robust semantic modality can guide the sensitive vi-sual modality to the correct classiﬁcation. We allow thevideo model estimate the entire distribution of the languagemodel’s prediction. Through cross-modal learning, we aimto transfer knowledge from the language model to the videomodel. We choose KL divergence for the cross-modal lossL
xmand deﬁne it as follows:
Lxm=DKL(Ps||Pv2s)=−C/summationdisplay
c=1ps
(c)logps
(c)
pv2s
(c)(10)
3.3. Training and Inference
3.3.1 Training
During the training process, we adopt a random sampling
strategy to sample ﬁxed Tframes for each video. In order to
obtain the best possible performance, our framework jointlytrains the classiﬁcation objective and the learning scheme inan end-to-end manner. The ﬁnal loss is:
L=L
cls+δLsim+ζLtss+ηLxm (11)
whereδ,ζandηare hyperparameters that control the im-
portance of the learning scheme. We use δ=0.1,ζ=1
andη=0.1in our experiments.
3.3.2 Inference
During the inference process, a uniform sampling strat-
egy is applied to sample ﬁxed Tframes for each video.
Only the video model is considered for inference. The ex-planation of the reasoning process can be explicitly shownby the proximity of the visual representation to the semanticrepresentation in the joint embedding space.
4. Experiments
4.1. Datasets and Metrics
Datasets. We conduct our experiments on two exten-
sive video datasets, detailed as follows: (1) Charades [ 28].Table 1. Ablation study on the Charades and CAD-120 datasets
using each proposed module. ”S” denotes the spatial token selec-tion and ”T” denotes the temporal token selection.
MethodsCharades CAD-120
mAP (%) ↑ Num↓ mAR↑ Num↓
DT-Former w/o S,T 61.4 36.6 0.73 49.8
DT-Former w/o S 61.2 31.1 0.72 17.4
DT-Former w/o T 61.2 30.7 0.74 16.7
DT-Former 61.1 26.0 0.75 14.2
It contains 157 action classes and consists of about 9.8kuntrimmed videos, among which 7.9k are used for trainingand 1.8k for testing. Each video contains an average of 6.8
distinct action categories and multiple actions can happen atthe same time, which makes the recognition extremely chal-lenging. The Action Genome dataset [ 11] provides ﬁne-
grained annotations for the Charades dataset, which pro-vides frame-level relation annotations for videos. Overall, itannotates 476K object bounding boxes and 1.72M relations.(2)CAD-120 . Introduced by [ 15], the CAD-120 dataset is
an RGB-D dataset designed for activity understanding. Itcontains 551 video clips of 4 subjects performing 10 differ-ent activities in different environments, such as a kitchen,a living room, and ofﬁce, etc. To train on our method, weleverage the re-annnotated version provided by [ 42], which
provides detailed relationships and attributes for the videoframes.
Evaluation protocol. Following the experimental proto-
col of [ 11], We measure multi-label action recognition per-
formance in term of the Mean Average Precision (mAP) onCharades dataset. For CAD-120 dataset, we calculate theMean Average Recall (mAR) to evaluate whether the model
successfully recognizes the performed actions.
4.2. Ablation Studies
4.2.1 Effectiveness of Each Module
Table 1reports the effectiveness of each module of the
proposed architecture. We evaluate the performance of theDT-Former using different settings, i.e., canceling spatial
token selection or canceling temporal token selection orboth. Here, the DT-Former corresponds to the performanceobtained by the video model. The metric scheme ’Num’refers to the average number of tokens retained per videoafter token selection. On the CAD-120 dataset, we noteda modest improvement in the accuracy of our architecture,attributable to the spatial-temporal token selection. TheCAD-120 dataset typically features videos with a single ac-tion, usually characterized by a pair of relational transitions.By eliminating irrelevant features, the model’s risk of over-ﬁtting is reduced, thereby enhancing its ability to general-ize. As expected, more tokens are discarded in the CAD-120 dataset than in the complex Charades dataset. Overall,
18883
adding either or both of the token selection modules can
reduce the number of tokens without signiﬁcantly affect-ing that the recognition performance. This demonstratesour architecture’s capability in identifying key relationaltransitions for action recognition and disregarding superﬂu-ous/redundant relations.
Table 2. Ablation study of learning scheme on the Charades and
CAD-120 datasets. /checkindicates that the component is applied in
the experiments.
w/
Lsimw/
Ltssw/
LxmmAP on
Charades (%) ↑mAR on
CAD-120 ↑
61.1 0.75
/check -- 62.2 0.79
-/check - 61.7 0.79
-- /check 62.9 0.81
/check -/check 63.4 0.83
/check/check/check 63.6 0.85
4.2.2 Effectiveness of Learning Scheme
To explore the effectiveness of the visual-semantic joint
embedding ( Lsim), token selection supervision ( Ltss) and
cross-modal learning ( Lxm) in our learning scheme, we
conduct related ablation studies using different settings, i.e.,
cancelling one or any two or all our key modules. As re-ported in Table 2, the visual-semantic joint embedding, to-
ken selection supervision and cross-modal learning com-ponents collectively or individually contribute to the ﬁnal
performance improvement. This improvement conﬁrms ourhypothesis that language models can help video models toimprove performance through knowledge transfer. It can benoted that due to the advantages of the learning scheme, the
visual model improves from 61.1 to 63.6 in terms of mAPmetric and from 0.75 to 0.85 in terms of mAR metric on
Charade and CAD-120, respectively. These results demon-strates that the learning scheme plays an important role inour proposed LaIAR .
4.2.3 Effectiveness of Robustness Against Domain
Shift
The performance of RGB-based methods drops drasti-
cally when the training and testing data do not share thesame distribution caused by change of scene ,camera view-
point oractor [40]. Our proposed model can adapt to video
distribution shifts with the aid of semantic modality, whichare invariant to complex appearance variations. To demon-strate the robustness of our proposed framework to domainshift, we split the Charades dataset into ﬁve subsets withnon-overlapping training scenes and test scenes. Table 3re-
ports the average and variance of ﬁve accuracies for theseﬁve subsets. The variance of our method is clearly stableand indicates robustness to domain shift.Table 3. Ablation study of the do-
main shift on the Charades dataset.
Accuracy
Method Average V ariance
STIGPN [ 34] 54.1 0.30
Ours 57.2 0.11Table 4. Comparison of ac-
curacy using predicted andannotated relationships.
Evaluation Mode mAP
Prediction 62.4
Label 63.6
4.2.4 Effectiveness of Using Predicted Relationships.
As previously stated, visual relationship categories can
be manually annotated or identiﬁed by the visual relation-ship detection network [ 5]. To explore the impact of the
two modes on accuracy, we compared the proposed methodbased on the ground truth and the predicted semantic rela-
tionships during training. The results, as detailed in Table
4, reveal that using predicted semantic relations can indeed
enhance the accuracy of the video model on the Charadesdataset (achieving 62.4% mean Average Precision (mAP)versus 61.1% mAP shown in Table 2). Moreover, the accu-
racy does not signiﬁcantly decrease when using predicteddata instead of ground truth. This demonstrates the effec-tiveness of the proposed method in mining relational trans-formations from real video data.
4.3. Comparison to the State-of-the-Art
We compare the action recognition accuracy of the pro-
posed method and the state-of-the-art methods (SoTA) on
the Charades and the CAD-120 datasets, respectively. Ta-ble5summarizes the results on Charades. It can be seen
that our proposed method outperforms several previous 3DCNN approaches in terms of mAP , including I3D [ 3], Slow-
Fast [ 7] and LFB [ 36]. This demonstrates that our method
can fully capture action cues through the visual relation-
ship transitions, based on the human/objects informationdetected from a single video frame (rather than using the en-tire scene like I3D). STRG [ 35] and SGFB [ 11] model the
action based objects and visual relationships, respectively,and overlook explicit modeling of temporal dynamics of
the interaction between objects. Though VideoLN [ 12]
and OR2G [ 24] takes visual relationship transitions into ac-
count, it is difﬁcult for these methods to achieve accurateaction inference due to the limitations of scene graph pre-dictors at test time. F or comparison in a modality with only
RGB video frames, our method achieved the best perfor-
mance compared with the existing methods. We also eval-
uated our method in Oracle evaluation mode, which lever-ages the ground-truth of bounding box and relationships ona frame. As reported in the last row of Table 5, our method
still achieves best performance on the Charades dataset. It
is important to mention that OR2G [ 24] used ground-truth
scene graphs to enhance its ﬁnal performance, whereas ournetwork uses only the bounding boxes of humans and ob-jects. Despite this, our method demonstrates strong perfor-
mance in both evaluation modes, validating the effective-
18884
ness of our proposed approach.
Table 5. Multi-label action recognition performance comparison
on the Charades’s validation set in term of mAP . SG: ground truthscene graph. Bbox: Bounding Box. Higher values are better.
Methods Backbone Modality mAP
I3D [ 3] R101-I3D RGB 15.6
VideoMLN [ 12] R101 RGB 38.4
STRG [ 35] R101-I3D-NL RGB 39.7
SlowFast [ 7] R101 RGB 42.1
LFB [ 36] R101-I3D-NL RGB 42.5
SGFB [ 11] R101-I3D-NL RGB 44.3
OR2G [ 24] R101-I3D-NL RGB 44.9
Ours R101-I3D-NL RGB 45.1
SGFB Oracle [ 11] R101-I3D-NL RGB+SG 60.3
VideoMLN Oracle [ 12] R101 RGB+SG 62.8
OR2G Oracle [ 24] R101 RGB+SG 63.3
Ours Oracle R101 RGB+Bbox 63.6
For CAD-120 dataset, we follow the same experimental
protocol as in [ 42] and divide the long video sequences into
small segments based on individual sub-actions and eval-uate the average recall metric for each sub-action. Table6summarizes the results on CAD-120. Explainable AAR-
RAR [ 42] interpret the action reasoning process through the
changes of relationship between objects or the attribute of
objects across time. Our method is able to give the same ex-planation and outperforms these methods, achieving state-
of-the-art performance with 0.85 mAR.
Table 6. Experimental results on CAD-120 for action recognition.
Methods Modality mAR
Temporal Segment [ 33]RGB 0.42
Flow 0.71
RGB + Flow 0.77
Explainable AAR-RAR [ 42] RGB 0.80
VideoMLN [ 12] RGB 0.83
Ours RGB 0.85
5. Interpretation and Visualization
To intuitively demonstrate the interpretability effect of
our proposed model, we provide interpretable representa-tion and a visualization example. As shown in Figure 3,
in the inference stage, we ﬁrst extract the visual relationrepresentations of human-object pairs in each frame. Then,our proposed DT-Former selects important relations in tem-poral and spatial dimensions and predicts action by mod-eling the important relation transition. Finally, the visualrepresentations of important relations are mapped into thejoint embedding space to ﬁnd their nearest neighbor seman-tic labels, which can provide explicit evidence for the actionreasoning process. In this example, the relation representa-
tions between the person and the box in the second and tenthframes are selected as cues for the action recognition. Thenearest semantic labels of these two representations in thejoint embedding space are ”holding box” and ”not holdingbox”, respectively. Here, the consequences of observations‘holding’ →‘not holding’ provide a clear sign of the action
”place”.
…
holding box not holding box࢚૛ ࢚૚૙
prediction action: place explanation: holdin g box → not holdin g box࢚૛࢚૜࢚૝࢚૞࢚૟࢚ૠ࢚ૡ࢚ૢ࢚૚૙࢚૛ ࢚૚૙
temporal token selection
spatial token selectionDT-Former࢚૚human-box visual 
relation feature
human-bowl visual 
relation feature
human-bottle vis ual 
relation feature
Figure 3. An example of action recognition performed by the pro-
posed method and its corresponding process of providing explana-tions. The shaded visual relation representations indicates that itis not selected by DT-Former.
6. Conclusion
In this paper, we introduced a new framework, La-
IAR , designed to transfer the knowledge from the language
model to the video model to improve the recognition per-formance and interpretability of video models. Speciﬁcally,we build a language model and a video model, which takesemantic relation and visual relation representations as in-put, respectively. These two models share the same archi-
tecture, namely DT-Former. This architecture is tailored to
select the most important relations for action recognitionfrom all the relations in video and to model the ﬁne-grainedrelation transitions within videos. Our framework also in-corporates three novel knowledge transfer strategies in ourlearning scheme to facilitate the knowledge transfer fromthe language model to the video model. This not only booststhe performance but also enhances the interpretability of thevideo model. Ablation experiments veriﬁed the effective-ness of the DT-Former, the learning scheme module and therobustness against domain shift. We conducted extensiveexperiments on Charades and CAD-120 datasets to demon-
strate the superior performance of our proposed method.
Acknowledgements: This work is partially sup-
ported by the National Natural Science Foundation of
China under Grant No.62073252 and No.62072358. Itwas also supported by Natural Science Basic ResearchProgram of Shaanxi under Grant No.2024JC-JCQN-66.
18885
References
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 5
[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is
space-time attention all you need for video understanding?InICML , page 4, 2021. 4
[3] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In pro-
ceedings of the IEEE Conference on Computer Vision andPattern Recognition , pages 6299–6308, 2017. 7,8
[4] Jiacheng Chen, Hexiang Hu, Hao Wu, Y uning Jiang, and
Changhu Wang. Learning the best pooling strategy for vi-sual semantic embedding. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , 2021. 4
[5] Y uren Cong, Wentong Liao, Hanno Ackermann, Bodo
Rosenhahn, and Michael Ying Yang. Spatial-temporal trans-former for dynamic scene graph generation. In Proceedings
of the IEEE/CVF international conference on computer vi-sion , pages 16372–16382, 2021. 3,7
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. arxiv 2020. arXiv
preprint arXiv:2010.11929 , 2020. 4
[7] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
Kaiming He. Slowfast networks for video recognition. InProceedings of the IEEE/CVF international conference oncomputer vision , pages 6202–6211, 2019. 7,8
[8] David Gunning and David Aha. Darpa’s explainable artiﬁcial
intelligence (xai) program. AI magazine , 40(2):44–58, 2019.
2
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and patternrecognition , pages 770–778, 2016. 1
[10] Hua Hua, Dongxu Li, Ruiqi Li, Peng Zhang, Jochen Renz,
and Anthony Cohn. Towards explainable action recognitionby salient qualitative spatial object relation chains. In Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence ,
pages 5710–5718, 2022. 2
[11] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos
Niebles. Action genome: Actions as compositions of spatio-
temporal scene graphs. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 10236–10247, 2020. 3,4,6,7,8
[12] Yang Jin, Linchao Zhu, and Yadong Mu. Complex video ac-
tion reasoning via learnable markov logic network. In Pro-
ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition , pages 3242–3251, 2022. 2,7,8
[13] Taotao Jing, Haifeng Xia, Renran Tian, Haoran Ding,
Xiao Luo, Joshua Domeyer, Rini Sherony, and ZhengmingDing. Inaction: Interpretable action decision making for au-tonomous driving. In European Conference on Computer
Vision , pages 370–387. Springer, 2022. 1[14] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami,
Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned
token pruning for transformers. In Proceedings of the 28th
ACM SIGKDD Conference on Knowledge Discovery and
Data Mining , pages 784–794, 2022. 3
[15] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh Sax-
ena. Learning human activities and object affordances fromrgb-d videos. The International journal of robotics research ,
32(8):951–970, 2013. 6
[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-works. Advances in neural information processing systems ,
25, 2012. 1,3
[17] Christopher A Kurby and Jeffrey M Zacks. Segmentation in
the perception and memory of events. Trends in cognitive
sciences , 12(2):72–79, 2008. 2
[18] Pilhyeon Lee, Taeoh Kim, Minho Shim, Dongyoon Wee,
and Hyeran Byun. Decomposed cross-modal distillation
for rgb-based temporal action detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2373–2383, 2023. 1,3
[19] Yi Li and Nuno V asconcelos. Improving video model trans-
fer with dynamic representation learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 19280–19291, 2022. 1
[20] Xue Lin, Qi Zou, Xixia Xu, Yaping Huang, and Ding Ding.
Effects of motion-relevant knowledge from unlabeled video
to human-object interaction detection. IEEE Transactions on
Neural Networks and Learning Systems , 2021. 3
[21] Guan Luo, Shuang Yang, Guodong Tian, Chunfeng Y uan,
Weiming Hu, and Stephen J Maybank. Learning human ac-tions by combining global dynamics and local appearance.IEEE transactions on pattern analysis and machine intelli-gence , 36(12):2466–2482, 2014. 1,2
[22] Lili Meng, Bo Zhao, Bo Chang, Gao Huang, Wei Sun, Fred-
erick Tung, and Leonid Sigal. Interpretable spatio-temporal
attention for video action recognition. In Proceedings of
the IEEE/CVF International Conference on Computer VisionWorkshops , pages 0–0, 2019. 1,2
[23] Ali Modarressi, Hosein Mohebbi, and Mohammad Taher
Pilehvar. Adapler: Speeding up inference by adaptive lengthreduction. arXiv preprint arXiv:2203.08991 , 2022. 3
[24] Yangjun Ou, Li Mi, and Zhenzhong Chen. Object-relation
reasoning graph for action recognition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 20133–20142, 2022. 2,7,8
[25] Chiara Plizzari, Mirco Planamente, Gabriele Goletto, Marco
Cannici, Emanuele Gusso, Matteo Matteucci, and Bar-bara Caputo. E2 (go) motion: Motion augmented eventstream for egocentric action recognition. In Proceedings of
the IEEE/CVF conference on computer vision and patternrecognition , pages 19935–19947, 2022. 1
[26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with regionproposal networks. Advances in neural information process-
ing systems , 28, 2015. 3
[27] Cynthia Rudin. Stop explaining black box machine learn-
ing models for high stakes decisions and use interpretable
18886
models instead. Nature machine intelligence , 1(5):206–215,
2019. 1,2
[28] Gunnar A Sigurdsson, G ¨ul V arol, Xiaolong Wang, Ali
Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood inhomes: Crowdsourcing data collection for activity under-standing. In Computer Vision–ECCV 2016: 14th Euro-
pean Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14 , pages 510–526. Springer,
2016. 6
[29] Tae Soo Kim and Austin Reiter. Interpretable 3d human ac-
tion analysis with temporal convolutional networks. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition workshops , pages 20–28, 2017. 1,2
[30] Zehua Sun, Qiuhong Ke, Hossein Rahmani, Mohammed
Bennamoun, Gang Wang, and Jun Liu. Human action recog-nition from various data modalities: A review. IEEE trans-
actions on pattern analysis and machine intelligence , 2022.
1
[31] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, VincentV anhoucke, and Andrew Rabinovich. Going deeper withconvolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1–9, 2015.
1
[32] Junke Wang, Xitong Yang, Hengduo Li, Li Liu, Zuxuan
Wu, and Y u-Gang Jiang. Efﬁcient video transformers withspatial-temporal token selection. In European Conference
on Computer Vision , pages 69–86. Springer, 2022. 3
[33] Limin Wang, Y uanjun Xiong, Zhe Wang, Y u Qiao, Dahua
Lin, Xiaoou Tang, and Luc V an Gool. Temporal segmentnetworks for action recognition in videos. IEEE transactions
on pattern analysis and machine intelligence , 41(11):2740–
2755, 2018. 8
[34] Ning Wang, Guangming Zhu, Hongsheng Li, Mingtao
Feng, Xia Zhao, Lan Ni, Peiyi Shen, Lin Mei, and LiangZhang. Exploring spatio-temporal graph convolution forvideo-based human-object interaction recognition. IEEE
Transactions on Circuits and Systems for Video Technology ,
2023. 1,7
[35] Xiaolong Wang and Abhinav Gupta. Videos as space-time
region graphs. In Proceedings of the European conference
on computer vision (ECCV) , pages 399–417, 2018. 7,8
[36] Chao-Y uan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaim-
ing He, Philipp Krahenbuhl, and Ross Girshick. Long-termfeature banks for detailed video understanding. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition , pages 284–293, 2019. 7,8
[37] Deming Ye, Yankai Lin, Y ufei Huang, and Maosong Sun.
Tr-bert: Dynamic token reduction for accelerating bert infer-ence. arXiv preprint arXiv:2105.11618 , 2021. 2
[38] Hongxu Yin, Arash V ahdat, Jose M Alvarez, Arun Mallya,
Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for
efﬁcient vision transformer. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 10809–10818, 2022. 3
[39] Zhihao Y uan, Xu Yan, Yinghong Liao, Yao Guo, Guan-
bin Li, Shuguang Cui, and Zhen Li. X-trans2cap: Cross-modal knowledge transfer using transformer for 3d densecaptioning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8563–
8573, 2022. 3
[40] Y unhua Zhang, Hazel Doughty, Ling Shao, and Cees GM
Snoek. Audio-adaptive activity recognition across video do-
mains. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition
, pages 13791–13800,
2022. 3,7
[41] Y uan Zhi, Zhan Tong, Limin Wang, and Gangshan Wu.
Mgsampler: An explainable sampling strategy for video ac-tion recognition. In Proceedings of the IEEE/CVF Inter-
national conference on Computer Vision , pages 1513–1522,
2021. 4
[42] Tao Zhuo, Zhiyong Cheng, Peng Zhang, Y ongkang Wong,
and Mohan Kankanhalli. Explainable video action reasoningvia prior knowledge and state transitions. In Proceedings of
the 27th acm international conference on multimedia , pages
521–529, 2019. 2,6,8
18887
