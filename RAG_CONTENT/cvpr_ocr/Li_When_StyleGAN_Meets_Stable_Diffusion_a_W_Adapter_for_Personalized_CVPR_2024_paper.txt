When StyleGAN Meets Stable Diffusion:
aW+Adapter for Personalized Image Generation
Xiaoming Li Xinyu Hou Chen Change Loy
S-Lab, Nanyang Technological University
csxmli@gmail.com xinyu.hou@ntu.edu.sg ccloy@ntu.edu.sg
FastComposer [38] CelebBasis [40] IP-Adapter [39]Ours with Fine-grained Attributes Editing
Age
 Eye Close
 Smile
Figure 1. Given a single reference image (thumbnail in the top left), our W+adapter not only integrates the identity into the text-to-image
generation accurately but also enables modifications of facial attributes along the ∆wtrajectory derived from StyleGAN. The text prompt is
“a woman wearing a spacesuit in a forest”.
Abstract
Text-to-image diffusion models have remarkably excelled
in producing diverse, high-quality, and photo-realistic im-
ages. This advancement has spurred a growing interest
in incorporating specific identities into generated content.
Most current methods employ an inversion approach to em-
bed a target visual concept into the text embedding space
using a single reference image. However, the newly synthe-
sized faces either closely resemble the reference image in
terms of facial attributes, such as expression, or exhibit a
reduced capacity for identity preservation. Text descriptions
intended to guide the facial attributes of the synthesized
face may fall short, owing to the intricate entanglement of
identity information with identity-irrelevant facial attributes
derived from the reference image. To address these issues,we present the novel use of the extended StyleGAN embed-
ding space W+, to achieve enhanced identity preservation
and disentanglement for diffusion models. By aligning this
semantically meaningful human face latent space with text-
to-image diffusion models, we succeed in maintaining high
fidelity in identity preservation, coupled with the capacity
for semantic editing. Additionally, we propose new train-
ing objectives to balance the influences of both prompt and
identity conditions, ensuring that the identity-irrelevant back-
ground remains negligibly affected during facial attribute
modifications. Extensive experiments reveal that our method
adeptly generates personalized text-to-image outputs that
are not only compatible with prompt descriptions but also
amenable to common StyleGAN editing directions in diverse
settings. Our code and model are available at https:
//github.com/csxmli2016/w-plus-adapter .
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2187
1. Introduction
What if we could be the protagonists of our own fantasies?
This paper primarily addresses personalized text-to-image
(T2I) generation, a field that is attracting growing attention
due to users’ desire to craft their unique content. Overlapped
with customized T2I [ 8,17,28,36], which integrates various
visual concepts like human faces and objects into the gener-
ated imagery, our focus is on tailoring image generation to
specific identities. This focus is motivated by applications
such as storyboarding, where a consistent identity needs to
be maintained across all images, despite variations in expres-
sions or ages. Additionally, we believe that human faces
possess unique, fine-grained intrinsic features that merit spe-
cial attention and present a worthwhile area for exploration.
Recent personalized image generation approaches uti-
lize a small reference set of a target identity and embed it
into a specific space. The common choice of embedding
space has been the textual embedding space used by large
language models (LLMs). While existing methods based
on textual embedding [ 8,28,36] have proven capable of
maintaining target identity, they are often limited by inherent
trade-offs. Specifically, these methods face challenges in
simultaneously preserving identity, generating varied facial
attributes, and creating identity-irrelevant content that aligns
with the text description. We observe that these issues pre-
dominantly stem from the entangled nature of the textual
embedding space, where a single pseudo word S∗struggles
to distinctly isolate identity-related features from the refer-
ence image. Efforts to separate such information include
approaches like encoding a visual concept into multiple word
embeddings [ 36] or employing a separate branch for extract-
ing identity-irrelevant details [ 3]. However, these methods
tend to overlook the nuanced facial features critical to an in-
dividual’s identity, resulting in incomplete disentanglement
and sub-optimal identity preservation.
In this study, we aim to more effectively separate identity-
relevant and -agnostic features for better identity preserva-
tion, while also ensuring editability. To achieve this, we
propose inverting the visual concept of a target identity into
StyleGAN’s [ 15]W+latent space, as opposed to using the
textual embedding space. In particular, we introduce a map-
ping network to integrate the W+space with the diffusion
model and a residual cross-attention module to add the w+
vector as an additional identity condition. This mapping
network, once trained with (image, w+)pairs, can general-
ize to unseen individuals during inference without the need
for a separate identity-specific model for each person. We
further present novel regularized training to ensure that edits
have a negligible effect on identity-irrelevant regions and
that the overall generation remains aligned with the prompt
conditions. It is noteworthy that although CelebBasis [ 40]
explores a similar concept of creating a face-specific latent
space through PCA of selected celebrity name embeddings,we argue that relying on such a limited dataset of celebrity
names is insufficient for a comprehensive face latent space
due to potential under-representation. Our experimental re-
sults confirm that our approach exhibits a superior capacity
for identity preservation compared to CelebBasis.
The contributions are summarised as follows: 1) We in-
troduce extended StyleGAN W+space as a target inversion
latent space to better encode the identity-preserving facial
concepts for personalized text-to-image synthesis. This is
the first study that considers the fusion between StyleGAN
W+space and diffusion-based image generation. 2) Em-
bedding target identity in W+space enables smooth and
controllable semantic editing on facial attributes in our text-
to-image model. 3) The effective disentanglement of identity-
relevant and -irrelevant information in our model facilitates
an identity-preserving generation that is not only diverse but
also adaptable to a wide range of prompt instructions.
2. Related Work
StyleGAN Latent Space. GANs have drawn enormous at-
tention due to the well-disentangled latent space [ 20]. Style-
GAN [ 15] proposes to map input latent code to an inter-
mediate latent space Wto prevent warping of the training
data distribution to fit in a particular probability distribution.
The resulting Wspace is a semantically disentangled space
that allows fine-grained controls over image synthesis. In-
terFaceGAN [ 29] and GANSpace [ 10] further interpret the
latent space and identify several control directions such as
age, gender, face angle, smile, etc. The powerful seman-
tic editing ability has motivated research on inverting and
editing real images in the StyleGAN space [ 1,19,24,32].
Abdal et al. [1] utilize a direct optimization framework to
embed a given real image to the extended StyleGAN space.
Tovet al. [32] deploy an encoder to perform the inversion
and keep the concatenated latent codes close to the original
StyleGAN space to maintain high perceptual quality and
editability. Motivated by the disentanglement and editability
of StyleGAN latent space, we aim to introduce it to the T2I
diffusion models to achieve more controllable synthesis. We
follow Image2StyleGAN to denote the extended Wspace
represented by multiple nonidentical wvectors as W+.
Personalized Image Synthesis. Customization has been
extensively studied in the context of T2I to generate images
of specified objects or individuals [ 2,8,28]. Gal et al. [8]
directly apply learnable text embedding optimization. Ruiz
et al. [28] fine-tune the diffusion model for the target con-
cept while preserving its class prior of the concept. Further
improvements have been made to invert multiple concepts
simultaneously with cross-attention fine-tuning [ 17], and
disentangle background irrelevant information [ 3]. Encoder-
based approaches [ 4,9,13,30,36–38] are employed for
their efficiency. The target visual concept is encoded to em-
bedding space as additional conditions. SingleInsert [ 37]
2188
adopts a two-stage scheme to insert concepts from a single
image into the foreground region exclusively and enables
the editing of the concept by text prompts. However, it is
worth noticing that our method achieves more fine-grained
editing with a single reference image at inference and does
not necessitate identity-specific fine-tuned models.
With special attention on identity inversion utilizing the
distinct features of human faces, Yuan et al. [40] define a
celebrity space by applying PCA on selected celebrity name
embeddings from CLIP and embedding new identities into
the space via learnable coefficients. Valevski et al. [33] uti-
lize a pre-trained face recognizer as the encoder and further
project the face embedding to CLIP space. FaceChain [ 21]
trains separate face and style LoRAs [ 12] to synthesize spe-
cific faces in specific styles. They follow a data preprocess-
ing pipeline and apply face fusion of the best reference face
on the synthesized results. However, we observe that previ-
ous personalization approaches inherit a common limitation
that the synthesized faces share similar attributes as the refer-
ence, indicating that the model cannot disentangle irrelevant
information well and overfits the particular reference.
Diffusion Model Adapters. Light-weight adapters have
been adopted to avoid the laborious work of fine-tuning
large models and add additional controls [ 23,39,42] to the
diffusion model. ControlNet [ 42] proposes to train task-
specific adapters while freezing the original diffusion model
to adapt to various input conditions. Concurrent work T2I-
Adapter [ 23] constructs a lighter-weight adapter for con-
trols on structure and color. Ye et al. [39] employ a decou-
pled cross-attention module to consider both text and image
prompts in the denoising process. In our work, we consider
identity information as an additional condition and align it
with the T2I model for identity-preserving synthesis.
3. Methodology
Our approach is capable of generating images that preserve
identity while allowing for semantic edits, requiring just
a single reference image for inference. This capability is
realized by innovatively aligning StyleGAN’s W+latent
space with the diffusion model. The training of our W+
adapter is divided into two stages. In Stage I (Sec. 3.2),
we establish a mapping from W+to SD latent space, using
the resulting projection as an additional identity condition to
synthesize center-aligned facial images of a specified identity.
In Stage II (Sec. 3.3), this personalized generation process
is expanded to accommodate more dynamic, “in-the-wild”
settings, ensuring adaptability to a variety of textual prompts.
3.1. Preliminary
In our implementation, we use Stable Diffusion [ 26] as the
foundational T2I model, which is one of the most com-
monly used latent space-based text-to-image generation
model [ 6,34]. During the training phase, the model archi-tecture includes: 1) a pre-trained encoder E(·), which trans-
forms an image Iinto a latent representation with reduced
dimensionality, 2) a conditional diffusion model tasked with
predicting the latent code of a previous timestep, based on
the text condition ctxtand the current timestep’s latent code,
and 3) a pre-trained decoder D(·)that reconstructs the fi-
nal latent code into the synthesized image. The learning
objective of this model is formulated as follows:
LLDM=Ez0,ϵ∼N (0,1),t,c txt
∥ϵ−ϵθ(zt, t, τ txt(ctxt))∥2
2
(1)
where z0isE(I),ϵis the ground truth noise added for current
timestep t,ztis the latent code of timestep t. The τtxtdenotes
the pre-trained CLIP text encoder [ 25] that converts the text
prompt ctxtto textual embeddings. The ϵθrepresents the
UNet [ 27] denosing network. In the inference stage, the
denoising network is applied iteratively to denoise a random
sampled noise zTtoz0with condition ctxt. The final result is
then generated through the pre-trained decoder, i.e.,D(z0).
3.2. Stage I: Aligning W+with Stable Diffusion
The goal of Stage I is to align W+space with SD to carry
identity information in T2I generation while retaining its
editing ability. To do so, we train a mapping network to
project a w+embedding to SD latent space and inject this
extra condition into SD by modifying the cross-attention
module. The framework of Stage I is illustrated in Fig. 2.
Details are introduced below.
Training Pair Construction. We adopt 100K discrete train-
ing samples to fit the continuous distribution of W+and
align it with SD. For each face image If, we use the pre-
trained e4e [ 32] encoder from StyleGAN inversion task to
get its corresponding w+∈R18×512vector. The pairs of
{If, w+}constitute our training data in this stage. Specifi-
cally, to generalize on real-world face images and improve
w+diversity, two types of training pairs are built, i.e., syn-
thetic face images from StyleGAN2 [ 16] and real-world
face images from FFHQ [ 15]. Note that in Stage I, we only
consider the aligned face images in order to exclude any
extraneous influences.
Mapping Network. Since the original W+space is des-
ignated for StyleGAN generation, we project the vector
w+∈R18×512to four tokens of dimension 768(R4×768) to
align with the input dimension of SD condition. The map-
ping network consists of four groups of linear layers and is
denoted as Fw(·). The latent space after mapping inherits
the editability and disentanglement properties of W+space
and is compatible with the SD generation process.
Residual Cross-Attention. To incorporate the identity con-
dition from the projected w+embedding into the pre-trained
SD model, we introduce a residual cross-attention module.
In the standard SD framework, the output of cross-attention
is determined using query features fzfrom the hidden state
2189
StyleGAN 
Encoder
     Mapping 
Network
Residual 
Cross-Attention
（RCA）
RCA RCAResidual 
Cross-Attention
（RCA）
a photo of a face Text Encoder
 UNet
StyleGAN 
Encoder     Mapping 
Network
a man wearing a 
brown jacket in a parkText Encoder
Crop
(b) Stage II: Generation in the WildRCA
(a) Stage I: Aligning         with Stable Diffusion
 (c) Residual Cross-AttentionBLIP2Residual 
Cross-Attention
（RCA）
RCA RCAResidual 
Cross-Attention
（RCA）
 UNet
Figure 2. Overview of W+adapter training stages. Left: Stage I for aligning W+space with Stable Diffusion. Middle : Stage II for
generating in-the-wild images with w+embeddings. Right : details of residual cross-attention module. Open lock indicates trainable parts.
and the text condition τtxt(ctxt). The process is defined as
f′
z= Attention( Q,K,V) = SoftmaxQK⊤
√
d
V (2)
whereQ=fzWq,K=τtxt(ctxt)Wk,V=τtxt(ctxt)Wv
are the query, key, value matrices of the attention module,
respectively. Wq,Wk, andWvdenote the corresponding
projection matrices. The cross-attention module is where the
latent noise interacts with text condition embeddings inside
the denoising process. Following previous methods [ 36,39],
we add an additional condition by a separate cross-attention
module. Our residual cross-attention is defined as:
f′′
z=f′
z+λ·Attention( Q†,K†,V†) (3)
whereQ†=f′
zW†
q,K†=fwW†
k, andV†=fwW†
v. The
W†
q,W†
k, andW†
vare the projection matrices for query, key,
and value, respectively. The fw=Fw(w+)is mapped from
w+by the mapping network. We use λas a scale parameter
to balance the influence of text and identity conditions on the
generation. We incorporate the decoupled cross-attention
module in a residual fashion instead of the parallel approach
as in previous works [ 36,39] to avoid performance degrada-
tion on the original text condition. When λis set to 0, our
w+vector has no impact on the pre-trained SD model. We
setλto 1 during training. The Q†,K†andV†are initial-
ized from their corresponding Q,K, andV, respectively.
The residual cross-attention module is performed on all the
cross-attention layers of SD.
Learning Objectives. The trainable parts in this stage in-
clude the mapping network Fwand{Q†,K†,V†}matrices
in all residual cross-attention modules. The optimization
target is formulated as:
Lw+=Ez0,ϵ,t,c txt,w+
∥ϵ−ϵθ(zt, t, τ txt(ctxt),Fw(w+))∥2
2
(4)
Eqn. (4)is similar to Eqn. (1)except for the additional
Fw(w+)serving as identity condition and cross-attention
structure modified inside ϵθ. The text prompt ctxtis randomly
selected from several neutral templates describing a human
face, e.g., “a photo of a face”, or “a face” (see suppl.).Through mapping the W+distribution into an embedding
space compatible with SD, our approach effectively uses
thew+vector to condition the personalized generation of
aligned facial images. Furthermore, attribute directions ∆w
derived from the original W+space, encompassing features
like smile, age, eye-opening, and others, remain applicable
to our projected w+vector. This allows for personalized,
fine-grained face editing, as demonstrated in Fig. 1.
3.3. Stage II: Generation in the Wild
In order to refine the performance of our W+-adapted SD
model from Stage I for scenarios beyond controlled environ-
ments, we continue on a second stage of training. This phase
is dedicated to further fine-tuning the weights of Q†,K†,V†
across all residual cross-attention modules. To ensure that
the projected W+space exclusively encapsulates identity-
related facial attributes, while remaining uninfluenced by
irrelevant distractions from the background, we keep the
mapping network fixed during this stage. The architecture
and workflow of Stage II are depicted in the middle of Fig. 2.
Training Data Construction. For Stage II, in-the-wild
images are used for training. For each image I, its corre-
sponding text caption, ctxt, is extracted using an off-the-shelf
captioning tool [ 18]. The aligned face image Ifis cropped
from I, and w+is obtained from Ifby the e4e encoder
(same as Stage I). A face region mask, M, is also obtained
from the image I, with 0 denoting the face region and 1
representing the non-face region.
Learning Objectives. To keep high identity fidelity while
encouraging high diversity of identity-irrelevant context,
three losses are employed jointly in this stage.
First, the reconstruction loss Lrecis used to guide the
denoising process:
Lrec=Ez0,ϵ,t,c txt,w+
∥ϵ−ϵθ(zt, t, τ txt(ctxt),Fw(w+))∥2
2
(5)
Note that Eqn. (5)is different from Eqn. (4)for that: 1) z0
here is encoded from the in-the-wild image Irather than the
face image If, so the reconstruction goal differs, and 2) ctxt
is the caption describing the in-the-wild image rather than a
2190
simple neutral prompt, therefore containing more informa-
tion about the image context to facilitate reconstruction of I
withw+from If.
Second, when semantic edits ∆ware applied in the W+
space, the objective is to exclusively modify the facial at-
tributes, leaving the remaining regions unchanged. In order
to accomplish this, we propose a w+disentanglement loss
to limit the editability of w+outside the face region:
Ldisen=∥M·ϵθ(zt, t, τ txt(ctxt),Fw(w+))−
M·ϵθ(zt, t, τ txt(ctxt),Ψ(Fw(w+)))∥(6)
where Ψis the augmentation operation. During training,
three augmentation strategies are adopted: 1) shuffle along
the batch dimension of Fw(w+), 2) add random perturba-
tions of Gaussian noise on Fw(w+), and 3) combine both of
them. By applying such constraints, augmented (or edited)
w+vectors are forced to have similar non-face regions,
thereby disentangling the effect of ∆won the background.
Finally, since the identity condition Fw(w+)should only
influence the face region, which constitutes a relatively small
proportion of the final output, we constrain its impact to be
limited by a regularization loss:
Lreg=∥M·ϵθ(zt, t, τ txt(ctxt),Fw(w+))−M·ϵθ(zt, t, τ txt(ctxt))∥(7)
In this way, a more balanced effect of text and identity condi-
tion on the synthesized result is achieved, further mitigating
the risk of unwanted noisy information introduced from w+
and preserving the compatibility with text prompts.
The overall learning objective in Stage II is defined as:
L=Lrec+γ1· Ldisen+γ2· Lreg (8)
where γ1andγ2denote the trade-off parameters.
4. Experiments
Training Data. In Stage I, we use the pre-trained e4e en-
coder [ 32] to obtain the w+vector for each face image If
from FFHQ [ 15] and StyleGAN2 [ 16]. FFHQ dataset con-
tains 70,000 images, among which 600 are split for valida-
tion while the rest are for training. We also synthesize 70,000
images using StyleGAN2. In Stage II, we use FFHQ in-the-
wild images (excluding those used for validation in Stage
I) and SHHQ [ 7] to optimize the W+adapter. SHHQ is a
human dataset that contains 40,000 high-quality full-body
images. BLIP2 [ 18] is introduced to generate the caption
for each in-the-wild image. Face region mask Mis obtained
based on the FFHQ alignment operation and is eroded and
blurred with kernel sizes of 32 and 7, respectively.
Implementation Details. All training is conducted on a
server with 8 Tesla V100 GPUs. The batch size is set to 16.
We employ the AdamW optimizer [ 22] with a learning rate
of1e−4and weight decay of 0.01. In Stage II, we adopt
color jittering [ 44], random rotation, and sampling for in-
the-wild images to increase diversity. During training, the
cropped images with incomplete or small faces are discarded.Table 1. Quantitative comparisons with previous methods. The best
and second best results are highlighted by bold and underline .
Variants CLIP Score ↑ ID↓ Detection ↑
Textual Inversion [8] .194 .575 .891
Dreambooth [28] .177 .562 .800
Custom Diffusion [17] .216 .498 .850
FastComposer [38] .265 .419 .950
IP-Adapter-Face [39] .241 .407 .958
CelebBasis [40] .253 .448 .916
Ours .267 .418 .950
It takes around 24 hours to align the W+space with SD in
Stage I, and nearly 96 hours to train the W+adapter in Stage
II. In the inference, we adopt DDIM sampler [ 31] with 50
steps. To enable classifier-free guidance [ 11], we use the
default settings and set the guidance scale to 7.5. We follow
IP-Adapter [ 39] to randomly drop the text ctxtandw+vector
with a probability of 0.05. γ1andγ2in Eqn. (8)are set to 1.5
and 1.0, respectively. The whole framework is implemented
on Diffusers [ 35]. Our experiments are based on SD v1.5
and can be extended to other SD models (see suppl.).
4.1. Quantitative Comparison
Baselines. The methods we evaluate can be categorized into
two distinct groups: general object customization and spe-
cific face personalization. For general object customization,
we use Dreambooth [ 28], Textual Inversion [ 8], and Cus-
tom Diffusion [ 17], implementing these using Diffusers [ 35].
For specific face personalization, we select closely related
works such as FastComposer [ 38], IP-Adapter-Face [ 39], and
CelebBasis [ 40]. In our evaluation, we adhere to the settings
established by CelebBasis [ 40] and FastComposer [ 38]. The
metrics used for assessment include the CLIP Score [25],
which is calculated based on the average image-text similar-
ity using CLIP-L/14, the Face Detection Score determined
using MTCNN [ 41], and the Identity Distance ( ID) measured
using ArcFace [5] on the detected facial regions.
Given the extensive fine-tuning requirements for each
identity in methods like Dreambooth, Textual Inversion, Cus-
tom Diffusion, and CelebBasis – which can be notably time-
consuming (for instance, Textual Inversion demands about
an hour on a single V100 GPU server1) – we encounter con-
straints in evaluating a substantial number of test instances.
To ensure a thorough and equitable quantitative analysis,
we carefully choose 120 identities from the CelebA-HQ
dataset [ 14], using one reference image per identity for con-
sistent comparisons. In addition, we randomly select 20 text
prompts that describe various aspects such as clothing, styles,
and backgrounds, to provide detailed characterizations of
each individual.
Table 1 shows that optimization-based methods operat-
ing in the text embedding space, such as Textual Inversion,
1https://huggingface.co/docs/diffusers/training/text inversion
2191
Figure 3. Visual comparisons with baselines on different scenarios of T2I generation. Best view by zooming in.
Dreambooth, and Custom Diffusion, fall short in accurately
aligning with text prompts and preserving identity features.
On the contrary, our method shows comparable performance
to FastComposer and IP-Adapter-Face in terms of both CLIP
Score and ID metrics. However, it is important to note that
IP-Adapter has been fine-tuned on the large-scale LAION-
2B dataset, whereas our model is trained on the relatively
smaller FFHQ-wild and SHHQ datasets, comprising around
110,000 images. Both FastComposer and IP-Adapter directly
process the reference image as input, while our approach
first maps it to the W+space before embedding it into the
SD model. This additional mapping step could potentially
introduce minor discrepancies when integrating the refer-
ence image into the SD model, slightly affecting the ID
results. Nevertheless, this slight trade-off in ID preservation
is balanced by our method’s ability to flexibly edit facial
attributes while ensuring the background remains consistent.
This unique balance of attribute editability and background
consistency is a novel contribution that sets our method apart
from previous approaches.
4.2. Qualitative Comparison
Figure 3 depicts visual comparisons with baselines across
various scenarios. Since Textual Inversion, Dreambooth, and
Custom Diffusion are not specifically designed for facial im-ages, we select results with the best facial quality from their
generated set of 40 images. For the remaining methods, we
choose from a set of 10. It is observed that Textual Inversion,
Dreambooth, and Custom Diffusion encounter difficulties
in capturing and maintaining identity details with a single
reference, leading to sub-optimal performance in this task.
CelebBasis and FastComposer face challenges in striking a
balance between text compatibility and identity preservation.
Though IP-Adapter shows improved identity retention, it
tends to ignore text conditions in certain instances (see the
1st, 3∼5throws).
Leveraging our W+adapter, our approach successfully
generates images that are not only compatible with text de-
scriptions but also more effectively retain the target identity.
Additionally, our method allows for the editing of facial
attributes along the ∆wdirection, causing only minor alter-
ations in the non-facial regions (illustrated in the last col-
umn). Furthermore, our approach can be seamlessly adapted
to other pre-trained SD models without the need for addi-
tional fine-tuning, while retaining its editing capabilities.
This versatility is exemplified in the last row of Fig. 3, which
showcases our method’s effectiveness with the dreamlike-
anime model2.
2https://huggingface.co/dreamlike-art/dreamlike-anime-1.0
2192
e4e Ours Real Image Real ImageAttributes EditingSmile Smile Age AgeFigure 4. Inversion and editing comparisons between e4e and Ours
in Stage I. They have the same w+and attributes editing ∆w.
Table 2. Quantitative comparisons of face inversion and editing.
Editing (ID ↓) is the ID metric on results of w+andw+−3·∆w.
MethodsInversion Editing (ID ↓)
ID↓ FID↓ LPIPS↓ Age Smile
e4e [32] .431 32.26 .205 .409 .312
Ours (Stage I) .435 31.47 .263 .393 .275
4.3. Ablation Study
Analyses of Aligning W+in Stage I. We examine if the
StyleGAN’s w+embedding is well aligned to the latent
space of SD during Stage I training. Fig. 4 offers a qualita-
tive comparison of inversion and attribute editing outcomes
between e4e [ 32] and our approach, applied to a real-world
image. Both methods use the identical w+embedding gen-
erated by the e4e encoder. As depicted in Fig. 4, our method
yields inversion results on par with e4e, as seen in the left
column, using the same w+vector. Furthermore, the align-
ment process we implement in this phase preserves the ed-
itability inherent to the W+space. By introducing attribute
editing directions ∆w(such as smile and age) from Inter-
FaceGAN [ 29], our method can semantically adjust these
attributes while maintaining the individual’s identity. This is
illustrated in the right columns, where the edited embedding,
denoted as w++α·∆w, reflects these changes. The param-
eterαis used to control the extent of attribute modification.
To quantitatively evaluate the alignment performance, we
randomly select 1,000 images from CelebA-HQ [ 14] and
measure their inversion results with metrics of ID, FID, and
LPIPS [ 43]3. As indicated in the Inversion column of Table 2,
our method exhibits comparable performance to e4e in the
image inversion task. For assessing attribute editing capabil-
ities, we focus on two key attributes, namely smile, and age,
applying a constant scale α(i.e.,w+−3·∆w). The Editing
column in Table 2 demonstrates that our approach surpasses
3https://github.com/chaofengc/IQA-PyTorchTable 3. Quantitative comparisons of different variants.
Variants CLIP Score ↑ ID↓ Detection ↑
Ours ( PCA ) .243 .526 .936
Ours ( w/oLreg) .136 .461 .951
Ours (L0.5
reg) .192 .497 .946
Ours ( Full) .267 .516 .943
Figure 5. Visual comparisons of smile attribute editing. The prompt
is “a woman wearing a t-shirt on the beach”.
e4e in maintaining identity during the editing process. These
findings confirm that our method has successfully aligned the
W+space with the SD model, preserving both the inversion
accuracy and the robust editability of attributes.
Analyses of Variants. We examine different variants to
assess each component of our W+adapter: 1) Ours ( PCA ),
which substitutes our residual cross-attention with parallel
cross-attention as in [ 39], and 2) Ours ( w/oLreg), which
omits the Lregobjective. The outcomes of these variants are
presented in Table 3. It is observed that the residual cross-
attention module outperforms the parallel cross-attention de-
sign. We conjecture that in tasks focused on integrating local
facial imagery into broader, in-the-wild scenarios, directly
injecting facial embeddings into the original hidden state
fzmight adversely impact non-facial regions. Conversely,
our residual cross-attention determines fusion weights by
calculating attention activations between the text-embedded
hidden state f′
zand our facial embedding. This results in a
more precise and effective fusion within the facial region.
Besides, the results suggest that the regularization loss
Lregis crucial in maintaining semantic consistency between
text prompts and images. In the absence of Lreg, the variant
Ours ( w/oLreg) tends to overemphasize facial regions while
neglecting the accompanying text descriptions, leading to
a reduced CLIP score. Incorporating Lregwith a weight
ofγ2= 0.5, as in Ours ( L0.5
reg), enhances the CLIP score
but slightly compromises the ID metric. In comparison,
Ours ( Full) achieves a satisfactory trade-off in balancing the
performance of ID and CLIP score.
The impact of our disentanglement loss Ldisen(Eqn. (6))
is visually demonstrated in Fig. 5. This example shows that
omitting Ldisenleads to unintended alterations in non-facial
regions when the smile editing direction ∆wis applied to
w+. This indicates the importance of Ldisenin effectively
separating identity-relevant and irrelevant information. By
combining our residual cross-attention, regularization loss,
and disentanglement loss, Ours ( Full) preserves identity
and ensures compatibility with text prompts, even amidst
changes to the face embedding.
2193
T2I Generation with         InterpolationFigure 6. Visual results of w+embeddings interpolation during inference. The prompts are “one person wearing suit and tie in a garden”
and “one person wearing a blue shirt by a secluded waterfall”, respectively.
Smile +
Figure 7. Visual results of using different λduring inference. The
prompt is “a man wearing a rainbow shirt in a garden”.
Analyses of λin Cross-attention. The parameter λde-
termines the extent to which the w+vector influences the
hidden state f′
z. Fig. 7 shows the effects of varying λdur-
ing inference. At λ= 0, the outcome aligns with that of
the original SD model, unaffected by any editing direction.
However, as λincreases, the generated identity more closely
resembles the reference image. Interestingly, at relatively
low values of λ(for example, λ= 0.3), even though the
identity does not closely match the reference, our model
effectively edits attributes in the specified direction. This
observation suggests that our method proficiently leverages
theW+space from StyleGAN within the SD framework.
Analyses of w+Interpolation. We select two facial im-
ages and acquire their respective w1
+andw2
+embeddings
from e4e encoder. The interpolated w+embedding is then
obtained through (1−κ)·w1
++κ·w2
+, where κ∈[0,1].
As shown in Fig. 6, this T2I generation process results in
a smooth transition in the facial regions of the generatedimages, while maintaining a similar and consistent layout
throughout the interpolation. This result confirms that the w+
embedding in our method not only preserves the editability
characteristic of the original StyleGAN but also effectively
distinguishes between facial and background regions.
4.4. Limitation
Our work aims at integrating StyleGAN’s editable W+space
into the SD model. We notice a challenge in this integra-
tion: the process of converting real images to w+vectors
in StyleGAN often leads to a loss of detail, impacting the
preservation of identity features. Despite employing a sub-
stantial number of training pairs {If, w+}to establish the
mapping, we still observe limitations in maintaining identity
fidelity and challenges in editing certain attributes ( e.g., pose
and glasses). The current framework is designed to generate
and edit images with only a single face. Our future work
aims to explore the potential of applying localized injections
of distinct w+to address multiple human subjects.
5. Conclusion
We have presented the first attempt to embed the W+space
of StyleGAN into the SD model. We showed that both the
mapping network and the residual cross-attention module
play crucial roles in facilitating the injection of w+embed-
ding into the SD model, balancing between text prompt
influence and identity conditions. Our experiments demon-
strate that the W+space, as used in our approach, not only
enables personalized text-to-image generation but also al-
lows for precise editing of facial attributes. We envision
this capability being highly beneficial in various practical
applications, such as portrait customization with seamless
attribute modifications. Furthermore, our methodology holds
the potential for application across other object domains that
possess distinct prior spaces.
Acknowledgement. This study is supported under the
RIE2020 Industry Alignment Fund Industry Collaboration
Projects (IAF-ICP) Funding Initiative, as well as cash and
in-kind contribution from the industry partner(s).
2194
References
[1]Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
age2StyleGAN: How to embed images into the StyleGAN
latent space? In ICCV , 2019. 2
[2]Yufei Cai, Yuxiang Wei, Zhilong Ji, Jinfeng Bai, Hu Han,
and Wangmeng Zuo. Decoupled textual embeddings for cus-
tomized image generation. In AAAI , 2024. 2
[3]Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei
Zhou, and Wenwu Zhu. DisenBooth: Identity-preserving dis-
entangled tuning for subject-driven text-to-image generation.
arXiv preprint arXiv:2305.03374 , 2023. 2
[4]Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang
Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du,
and Min Zheng. PhotoVerse: Tuning-free image customiza-
tion with text-to-image diffusion models. arXiv preprint
arXiv:2309.05793 , 2023. 2
[5]Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.
Arcface: Additive angular margin loss for deep face recogni-
tion. In CVPR , 2019. 5
[6]Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In CVPR ,
2021. 3
[7]Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin,
Chen Qian, Chen Change Loy, Wayne Wu, and Ziwei Liu.
StyleGAN-Human: A data-centric odyssey of human genera-
tion. In ECCV , 2022. 5
[8]Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An
image is worth one word: Personalizing text-to-image gener-
ation using textual inversion. In ICLR , 2022. 2, 5
[9]Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano,
Gal Chechik, and Daniel Cohen-Or. Encoder-based domain
tuning for fast personalization of text-to-image models. arXiv
preprint arXiv:2302.12228 , 2023. 2
[10] Erik H ¨ark¨onen, Aaron Hertzmann, Jaakko Lehtinen, and Syl-
vain Paris. GANSpace: Discovering interpretable GAN con-
trols. In NeurIPS , 2020. 2
[11] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS , 2022. 5
[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA:
Low-rank adaptation of large language models. In ICLR ,
2022. 3
[13] Xuhui Jia, Yang Zhao, Kelvin C. K. Chan, Yandong Li, Han
Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and
Yu-Chuan Su. Taming encoder for zero fine-tuning image
customization with text-to-image diffusion models. arXiv
preprint arXiv:2304.02642 , 2023. 2
[14] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of GANs for improved quality, stability,
and variation. In ICLR , 2018. 5, 7
[15] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR , 2019. 2, 3, 5
[16] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In CVPR , 2020. 3, 5[17] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shecht-
man, and Jun-Yan Zhu. Multi-concept customization of text-
to-image diffusion. In CVPR , 2023. 2, 5
[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-
2: Bootstrapping language-image pre-training with frozen
image encoders and large language models. In ICML , 2023.
4, 5
[19] Xiaoming Li, Wangmeng Zuo, and Chen Change Loy. Learn-
ing generative structure prior for blind text image super-
resolution. In CVPR , 2023. 2
[20] Ming Liu, Yuxiang Wei, Xiaohe Wu, Wangmeng Zuo, and
Lei Zhang. A survey on leveraging pre-trained generative
adversarial networks for image editing and restoration. arXiv
preprint arXiv:2207.10309 , 2022. 2
[21] Yang Liu, Cheng Yu, Lei Shang, Ziheng Wu, Xingjun Wang,
Yuze Zhao, Lin Zhu, Chen Cheng, Weitao Chen, Chao Xu,
Haoyu Xie, Yuan Yao, Wenmeng Zhou, Yingda Chen, Xu-
ansong Xie, and Baigui Sun. FaceChain: A playground
for identity-preserving portrait generation. arXiv preprint
arXiv:2308.14256 , 2023. 3
[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR , 2019. 5
[23] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian
Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2I-
Adapter: Learning adapters to dig out more controllable
ability for text-to-image diffusion models. arXiv preprint
arXiv:2302.08453 , 2023. 3
[24] Hamza Pehlivan, Yusuf Dalva, and Aysegul Dundar. StyleRes:
Transforming the residuals for real image editing with Style-
GAN. In CVPR , 2022. 2
[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision.
InICML , 2021. 3, 5
[26] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In CVPR , 2022. 3
[27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net:
Convolutional networks for biomedical image segmentation.
InMICCAI , 2015. 3
[28] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine-
tuning text-to-image diffusion models for subject-driven gen-
eration. In CVPR , 2023. 2, 5
[29] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Inter-
preting the latent space of GANs for semantic face editing. In
CVPR , 2020. 2, 7
[30] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. In-
stantBooth: Personalized text-to-image generation without
test-time finetuning. arXiv preprint arXiv:2304.03411 , 2023.
2
[31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
diffusion implicit models. In ICLR , 2021. 5
[32] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and
Daniel Cohen-Or. Designing an encoder for StyleGAN image
manipulation. In ACM TOG , 2021. 2, 3, 5, 7
2195
[33] Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv
Leviathan. Face0: Instantaneously conditioning a text-to-
image model on a face. arXiv preprint arXiv:2306.06638 ,
2023. 3
[34] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. In NeurIPS , 2017. 3
[35] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca,
Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and
Thomas Wolf. Diffusers: State-of-the-art diffusion models.
https://github.com/huggingface/diffusers ,
2022. 5
[36] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang,
and Wangmeng Zuo. ELITE: Encoding visual concepts into
textual embeddings for customized text-to-image generation.
InICCV , 2023. 2, 4
[37] Zijie Wu, Chaohui Yu, Zhen Zhu, Fan Wang, and Xiang Bai.
SingleInsert: Inserting new concepts from a single image
into text-to-image models for flexible editing. arXiv preprint
arXiv:2310.08094 , 2023. 2
[38] Guangxuan Xiao, Tianwei Yin, William T. Freeman, Fr ´edo
Durand, and Song Han. FastComposer: Tuning-free multi-
subject image generation with localized attention. arXiv
preprint arXiv:2305.10431 , 2023. 2, 5
[39] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. IP-
Adapter: Text compatible image prompt adapter for text-to-
image diffusion models. arXiv preprint arXiv:2308.06721 ,
2023. 3, 4, 5, 7
[40] Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang
Qi, Xintao Wang, Ying Shan, and Huicheng Zheng. Inserting
anybody in diffusion models via celeb basis. In NeurIPS ,
2023. 2, 3, 5
[41] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
Joint face detection and alignment using multitask cascaded
convolutional networks. In IEEE signal processing letters ,
2016. 5
[42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , 2023. 3
[43] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 7
[44] Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin,
Jonathon Shlens, and Quoc V Le. Learning data augmentation
strategies for object detection. In ECCV , 2020. 5
2196
