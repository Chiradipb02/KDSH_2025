Enhance Image Classiﬁcation via Inter-Class Image Mixup with Diffusion Model
Zhicai Wang1*, Longhui Wei2†, Tan Wang3, Heyu Chen1, Yanbin Hao1, Xiang Wang1†,
Xiangnan He1, Qi Tian2
1University of Science and Technology of China,2Huawei Inc.,
3Nanyang Technological University
Abstract
Text-to-image (T2I) generative models have recently
emerged as a powerful tool, enabling the creation of photo-
realistic images and giving rise to a multitude of appli-
cations. However, the effective integration of T2I mod-
els into fundamental image classiﬁcation tasks remains an
open question. A prevalent strategy to bolster image clas-
siﬁcation performance is through augmenting the training
set with synthetic images generated by T2I models. In this
study, we scrutinize the shortcomings of both current gener-
ative and conventional data augmentation techniques. Our
analysis reveals that these methods struggle to produce im-
ages that are both faithful (in terms of foreground objects)
and diverse (in terms of background contexts) for domain-
speciﬁc concepts. To tackle this challenge, we introduce an
innovative inter-class data augmentation method known as
Diff-Mix1, which enriches the dataset by performing image
translations between classes. Our empirical results demon-
strate that Diff-Mix achieves a better balance between faith-
fulness and diversity, leading to a marked improvement in
performance across diverse image classiﬁcation scenarios,
including few-shot, conventional, and long-tail classiﬁca-
tions for domain-speciﬁc datasets.
1. Introduction
In comparison to GAN-based models [ 7,17,25], contem-
porary state-of-the-art text-to-image (T2I) diffusion models
exhibit enhanced capabilities in producing high-ﬁdelity im-
ages [ 12,37,44,49]. With the remarkable cross-modality
alignment capabilities of T2I models, there is signiﬁcant po-
tential for generative techniques to enhance image classiﬁ-
cation [ 2,4]. For instance, a straightforward approach en-
tails augmenting the existing training dataset with synthetic
images generated by feeding categorical textual prompts to
a T2I diffusion model. However, upon reviewing prior ap-
*This work was done during the internship in Huawei Inc..
†Xiang Wang and Longhui Wei are both the corresponding authors.
1https://github.com/Zhicaiwww/Diff-Mixdomain-specific dataset
“Red winged Blackbird”
reference imageT2I
“Red winged Blackbird”Intra-classAugmentationT2I
Inter-classAugmentationT2IT2IT2IVanilla T2IFine-tuned T2Isynthetic imageDiversityFaithfulness
DiversityFaithfulness
DiversityFaithfulness
Figure 1. Strategies to expand domain-speciﬁc datasets for im-
proved classiﬁcation are varied. Row 1 illustrates vanilla distilla-
tion from a pretrained text-to-image (T2I) model, which carries the
risk of generating outputs with reduced faithfulness. Intra-class
augmentation, depicted in Row 2, tends to yield samples with lim-
ited diversity to maintain high ﬁdelity to the original class. Our
proposed method, showcased in Rows 3 and 4, adopts an inter-
class augmentation strategy. This involves introducing edits to a
reference image using images from other classes within the train-
ing set, which signiﬁcantly enriches the dataset with a greater di-
versity of samples.
proaches employing T2I diffusion models for image classi-
ﬁcation, it becomes evident that the challenge in generative
data augmentation for domain-speciﬁc datasets is produc-
ing samples with both a faithful foreground and a diverse
background . Depending on whether a reference image is
used in the generative process, we divide these methods into
two groups:
•Text-guided knowledge distillation [ 52,57] involves gen-
erating new images from scratch using category-related
prompts to expand the dataset. For the off-the-shelf T2I
models, such vanilla distillation presume these models
have comprehensive knowledge of target domain, which
can be problematic for domain-speciﬁc datasets. Insuf-
ﬁcient domain knowledge easily makes the distillation
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
17223
process less effective. For example, vanilla T2I models
struggle to generate images that accurately represent spe-
ciﬁc bird species based solely on their names (see Row 1
of Fig. 1).
•Generative data augmentation [ 1,69] employs genera-
tive models to enhance existing images. Da-fusion [ 58],
for instance, translates the source image into multiple
edited versions within the same class . This strategy,
termed intra-class augmentation, primarily introduces
intra-class variations. While intra-class augmentation re-
tains much of the original image’s layout and visual de-
tails, it results in limited background diversity (see Row
2 of Fig. 1). However, synthetic images with constrained
diversity may not sufﬁciently enhance the model’s ability
to discern foreground concepts.
Based on these observations, a fundamental question
emerges: ‘ Is it feasible to develop a method that optimizes
both the diversity and faithfulness of synthesized data si-
multaneously ?’
In this work, we introduce Diff-Mix , a simple yet ef-
fective data augmentation method that harnesses diffusion
models to perform inter-class image interpolation, tailored
for enhancing domain-speciﬁc datasets. The method en-
compasses two pivotal operations: personalized ﬁne-tuning
and inter-class image translation. Personalized ﬁne-tuning
[15,46] is originally designed for customizing T2I mod-
els and enabling them to generate user-speciﬁc contents or
styles. In our case, we implement the technique to tailor
the model, enabling it to generate images with faithful fore-
ground concepts. Inter-class image translation in Diff-Mix
entails transforming a reference image into an edited ver-
sion that incorporates prompts from different classes. This
translation strategy is designed to retain the original back-
ground context while editing the foreground to align with
the target concept. For instance, as depicted in the bottom
rows of Fig. 1, Diff-Mix can generate images of land birds
in diverse settings, such as maritime environments, enrich-
ing the dataset with a variety of counterfactual samples.
Unlike previous non-generative augmentation methods,
such as Mixup [ 68] and CutMix [ 66], Diff-Mix works in
a foreground-perceivable inter-class interpolation manner
and shares a different mechanism with the non-generative
approaches. Our experiment under the conventional clas-
siﬁcation setting indicates that incorporating both CutMix
and Diff-Mix could further enhance performance. Addi-
tionally, when compared with other generative approaches,
we conduct experiments under few-shot and long-tail sce-
narios and observe consistent performance improvements.
Our contributions can be summarized as follows:
•We pinpoint the critical factors that affect the efﬁcacy of
generative data augmentation in domain-speciﬁc image
classiﬁcation: namely, faithfulness and diversity.
•We introduce Diff-Mix, a simple yet effective generativedata augmentation strategy that leverages ﬁne-tuned dif-
fusion models for inter-class image interpolation.
•We conduct a comparative analysis of Diff-Mix with
other distillation-based and intra-class augmentation
methods, as well as non-generative approaches, highlight-
ing its unique features and beneﬁts.
2. Related Works
Text-to-image diffusion models. Following pretraining
on web-scale data, the T2I diffusion model has demon-
strated robust capabilities in generating text-controlled im-
ages [ 37,49,53,56]. Its versatility has led to diverse
applications, including novel view synthesis [ 6,63], con-
cept learning [ 28,46], and text-to-video generation [ 22,55],
among others. Recent advancements [ 29] also highlight the
cross-modality features of such generative models, show-
casing their ability to serve as zero-shot classiﬁers.
Synthetic data for image classiﬁcation. There are two per-
spectives on the utilization of synthetic data for image clas-
siﬁcation: knowledge distillation [ 2] and data augmentation
[5,51,54,62]. From the knowledge distillation perspective,
SyntheticData [ 19] reports signiﬁcant performance gains in
both zero-shot and few-shot settings by leveraging off-the-
shelf T2I models to obtain synthetic data. The work of [ 2]
has indicated that ﬁne-tuning the T2I model on ImageNet
[47] yields improved classiﬁcation accuracy by narrowing
the domain gap. Some works also ﬁnd that learning from
the synthetic data presents strong transferability [ 19,57]
and robustness [ 4,30,67]. From the data augmentation
standpoint, Da-fusion [ 58] achieves stable performance im-
provements on few-shot datasets by augmenting from ref-
erence images. In a related study [ 3], the use of StyleGAN
[25] for generating interpolated images between two differ-
ent domains has been shown to enhance classiﬁer robust-
ness for out-of-distribution data. Our work shares similar-
ities with AMR [ 5], which generates realistic novel exam-
ples by interpolating between two images using GAN [ 16].
The distinction lies in our discussion of interpolation using
the T2I diffusion model, where its noise-adding and denois-
ing characteristics enable a smoother implementation of in-
terpolation.
Non-generative data augmentation. Mixup [ 68] and Cut-
Mix [ 66] stand out as two prominent non-generative data
augmentation methods, serving as effective regularization
techniques during training. While Mixup achieves aug-
mented samples through a convex combination of two im-
ages, CutMix achieves augmentation by cutting and pasting
parts of images. However, both methods are constrained in
their ability to produce realistic images. In addressing this
limitation, the utilization of generative models emerges as a
potential solution to alleviate this issue.
17224
3. Method
3.1. Preliminary
Text-to-Image diffusion model. Diffusion models gener-
ate images by gradually removing noise from a Gaussian
noise source [ 21]. In a diffusion process with a total of T
steps, its forward process, which gradually adds noise, is
represented as a Markov chain with a Gaussian transition
kernel, where q(xt|xt 1)= N 
xt;p↵txt 1,(1 ↵t)I 
,
where xtrepresents the noisy image at step t. The train-
ing objective at step tis to predict the noise to reconstruct
xt 1. When training a text-conditioned diffusion model,
the simpliﬁed training objective can be summarized as fol-
lows:
E✏,x,c,th
k✏ ✏✓(xt,c,t)k2
2i
, (1)
where ✏✓represents the predicted noise, and cis the en-
coded text caption associated with the image x.
T2I personalization. T2I personalization aims to personal-
ize a diffusion model for generating speciﬁc concepts using
a limited number of concept-oriented images [ 28,40,42].
These concepts are typically represented using identiﬁers
(e.g., “[V]”). As a result, we formalize the constructed
image-caption set as x,“photo of a [V] ”. Various
personalization methods differ in their ﬁne-tuning strate-
gies. For instance, Textual Inversion (TI) [ 15] makes the
identiﬁer learnable, but other modules are not ﬁne-tuned,
potentially sacriﬁcing some faithfulness in image genera-
tion. On the other hand, Dreambooth (DB) [ 46] ﬁne-tunes
the U-Net [ 45] for more reﬁned personalized generation but
faces the challenge of increased computational cost.
Image-to-image translation. Image translation enables
image synthesis and editing using a reference image as
guidance [ 24,64,71]. Diffusion-based image translation
methods can generate ﬁne edits, which refer to subtle mod-
iﬁcations, with varying degrees of shift relative to the ref-
erence image [ 8,35,59]. Here, we draw inspiration from
SDEdit [ 35] to perform edits on the reference image, where
the target image xtaris translated from a reference image
xref. During translation, the reverse process does not tra-
verse the full process but starts from a certain step bsTc,
where s2[0,1]controls the insertion position of the refer-
ence image with noise, as follows,
xbsTc=q
˜↵bsTcxref
0+q
1 ˜↵bsTc✏. (2)
By adjusting the strength parameter s, one can strike a bal-
ance between the diversity of the generated images and their
faithfulness to the reference image.
3.2. General Framework
The Diff-Mix pipeline consists of two key steps. Firstly, to
produce more faithful images for domain-speciﬁc datasets,
(a) Real images
(b) Synthetic images (vanilla SD)
(c) Synthetic images (SD ﬁne-tuned via DB)
(d) Synthetic images (SD ﬁne-tuned via TI+DB)
Figure 2. Examples of “ American Three toed
Woodpecker ”. (a) Real images from the training set. (b-
d) synthetic images generated using different ﬁne-tuned models
with the same number of ﬁne-tuning steps. TI+DB indicates both
text embedding and U-Net are ﬁne-tuned. TI+DB achieves a more
faithful output compared to DB alone (check the head and wing
patterns of the birds).
we propose treating it as a T2I personalization problem and
ﬁne-tuning the Stable Diffusion (SD). Subsequently, to en-
hance the diversity of synthetic data beyond the well-ﬁtted
training distribution, we employ inter-class image transla-
tion. This process produces interpolated images with in-
creased background diversity for each class.
3.3. Fine-tune Diffusion Model
Vanilla distillation tends to be less effective, especially as
the number of training shots increases (refer to Sec. 4.1).
In order to mitigate the distribution gap, we propose ﬁne-
tuning Stable Diffusion in conjunction with current widely-
used T2I personalization strategies.
Dreambooth meets Textual Inversion. Many ﬁne-grained
datasets provide terminological names for their cate-
gories, like “ American Three toed Woodpecker ”
and “ Pileated Woodpecker ”. We could construct
image-text pairs using category-related prompts and ﬁne-
tune the denoising network of SD using Eq. 1, which is
analogous to Dreambooth. However, we observe that di-
rectly incorporating these specialized terms into the text
during ﬁne-tuning can impede convergence and hinder the
generation of faithful images. We attribute this challenge to
the semantic proximity of terminology within a ﬁne-grained
domain, where ﬁne-tuning the vision module alone tends to
be less effective at distinguishing two similar classes within
the same family, like “ Woodpecker ”. Inspired by Textual
Inversion [ 15], we opt to replace the terminological name in
the dataset with “ [Vi] [metaclass] ” where “ [Vi]” is
a learnable identiﬁer, and ivaries from 1toN, represent-
17225
American GoldfinchAmerican Crow
Acadian Flycatcher[V1] bird[V3] bird[V2] birda...bird[V2][V1]...
“photo of a [V1] bird”CLIP-T
LoRA
UNet
FrozenTunableFigure 3. Fine-tuning framework of Diff-Mix operates as follows:
Initially, we replace the class name with a structured identiﬁer for-
matted as “ [Vi] [metaclass] ”, thereby sidestepping the need
for speciﬁc terminological expressions. Next, we engage in joint
ﬁne-tuning of these identiﬁers and the low-rank residues (LoRA)
of U-Net to capture the domain-speciﬁc distribution.
ing the category index. The illustration of our ﬁne-tuning
strategy is presented in Fig. 3. The term “ [metaclass] ”
is determined by the theme of the current dataset, such as
“bird ” for a ﬁne-grained bird dataset. By concurrently
ﬁne-tuning the identiﬁer and the U-Net, we empower the
model to quickly adapting to the ﬁne-grained domain, al-
lowing it to generate faithful images using the identiﬁer (see
comparison between Row 3 and Row 4 in Fig. 2).
Parameter efﬁcient ﬁne-tuning. In this context, we em-
brace the parameter-efﬁcient ﬁne-tuning strategy known as
LoRA [ 23]. LoRA distinguishes itself by ﬁne-tuning the
residue of low-rank matrices instead of directly ﬁne-tuning
the pre-trained weight matrices. To elaborate, consider a
weight matrix W2Rm⇥n. The tunable residual matrix
 Wcomprises two low-rank matrices: A2Rm⇥dand
B2Rn⇥d, deﬁned as  W=AB>. As a default conﬁg-
uration, we set the rank dto 10.
3.4. Data Synthesis Using Diffusion Model
In generating pseudo data, three strategies can be used
with our ﬁne-tuned diffusion model2: (1) distillation-based
method Diff-Gen , (2) intra-class augmentation Diff-Aug ,
and (3) inter-class augmentation Diff-Mix .
Diff-Gen and Diff-Aug. For a target class yiand its textual
condition, “ photo of a [Vi] [metaclass] ”, both
2We use the preﬁx “Diff-” denotes the T2I model is ﬁne-tuned and
“Real-” denotes the vanilla T2I model.0.10.30.50.70.9
Diff-Mix
Translation strength
Diff-Aug
Target classFigure 4. Examples of images translated using Diff-Mix and Diff-
Aug across various strengths. Diff-Aug employs the same target
and reference image classes, typically resulting in subtle modiﬁca-
tions. Diff-Mix progressively adjusts the foreground to align with
the target class as the translation strength increases, while preserv-
ing the background layout from the reference image.
methods generate synthetic samples annotated with class
i. Speciﬁcally, Diff-Gen generates samples from scratch
by initializing with random Gaussian noise and proceed-
ing through the full reverse diffusion process with Tsteps.
Diff-Gen can produce images aligned with its ﬁne-tuned
distribution. In contrast, Diff-Aug sacriﬁces a portion of
diversity and generate images by editing on a reference im-
age. Speciﬁcally, it randomly sample a image from the
intra-class training set and enhances the image through im-
age translation using Eq. 2. The term “intra-class” means
that the conditioning prompts are constructed based on the
ground truth categories of images, and such a denoising pro-
cess tends to introduce less variation, particularly for the
foreground concepts (see top rows of Fig. 1).
Diff-Mix. Diff-Mix employs the same translation process
as Diff-Aug, but the reference image is sampled from the
full training set rather than intra-class set to enable inter-
class interpolation. The key difference is that Diff-Mix
can generate numerous counterfactual examples, such as a
blackbird in the sea (see fourth row of Fig. 1). This ne-
cessitates that downstream models make a more reﬁned dif-
ferentiation of category attributes, thereby reducing the im-
pact of spurious correlations introduced by variations in the
background. Denoting the label of the reference image as
yj, by inserting the reference image into the reverse process
with the prompt “ photo of a [Vi] [metaclass] ”,
we can obtain interpolated images between the ithandjth
categories. By controlling the intensity s, we can precisely
17226
(a)(b)diverse backgroundfaithful foregroundFigure 5. A schematic explanation of Diff-Mix’s effectiveness us-
ing structural casual model [ 41].xfgis the foreground that deter-
mines the real class label, xbgdenotes the background. xfg!
z!yis the causal path that we are focusing and xfg I!
xbg!z!yis the backdoor path that introduces spurious rela-
tions between xfgandy.
manage the interpolation process. When annotating the syn-
thetic image, unlike Mixup and Cutmix, we take into ac-
count the non-linear nature of diffusion translation. Thus
the annotation function is given by
˜y=( 1  s )yi+s yj, (3)
where  is a hyperparameter introducing non-linearity. Our
empirical ﬁndings indicate that a  smaller than 1is fa-
vored. Additionally, in low-shot cases, the samples with
higher conﬁdence in the target class are preferred (see de-
tails in Sec. 4.5).
Construct synthetic dataset. To construct the synthetic
dataset using Diff-Mix, similar to Da-fusion [ 58], we adopt
a randomized sampling strategy ( s2{0.5,0.7,0.9}) for the
selection of translation strength. While applying the inter-
class editing, we observe that Diff-Mix tends to produce
more undesirable samples compared to Diff-Aug. These un-
desirable samples have incomplete foreground such as frag-
mented bird bodies. This is caused by the intrinsic shape
and pose differences among classes. To mitigate this, we in-
troduce a simple data-cleaning approach to reduce the pro-
portion of such problematic images. We utilize the large
vision language model CLIP [ 43] to assess the conﬁdence
in the content, serving as the ﬁltering criterion. Further de-
tails can be found in the supplementary materials (SMs).
Analysis. We depict the core insight of Diff-Mix in Fig.
5. To eliminate the spurious correlation introduced by xbg,
learning on the synthetic set with randomized xbg(back-
ground) can cut off spurious correlation, forcing the classi-
ﬁcation model to infer only from the foreground. The study
in Fig. 7(b) shows that the more diverse the background
(larger the referable class number), the better the perfor-
mance on the CUB test set.
4. Experiments
In this section, we investigate the effectiveness of Diff-Mix
in domain-speciﬁc datasets. The key questions we aim to
address are as follows:Q1: Can generative inter-class augmentation lead to
more signiﬁcant performance gains in downstream tasks
compared to those intra-class augmentation methods and
distillation-based methods?
Q2:Is improved background diversity the secret weapon
of Diff-Mix for enhancing the performance?
Q3:How to choose the ﬁne-tuning strategy and annota-
tion strategy to boost the performance gains for the inter-
class augmentation?
To address Q1, we separately discuss these questions in
few-shot settings in Sec. 4.1, conventional classiﬁcation in
Sec. 4.2, as well as long-tail classiﬁcation in Sec. 4.3. Ad-
ditionally, to answer Q2, we conduct a test for background
robustness in Sec. 4.4and perform an ablation study fo-
cusing on the size and diversity of synthetic data in Sec.
4.5. For Q3, we conduct an ablation study to empirically
discover effective strategies for deployment in Sec. 4.5.
4.1. Few-shot Classiﬁcation
Experimental Setting. To investigate the impact of dif-
ferent data expansion methods, we conduct few-shot exper-
iments on a domain-speciﬁc dataset Caltech-UCSD Birds
(CUB) [ 61], with shot numbers of 1, 5, 10, and all. For
augmentation-based methods, the synthetic dataset is con-
structed using various translation strengths ( s), speciﬁcally,
s2{0.1,0.3,. . . ,1.0}. We expand the original training set
with a multiplier of 5 and cache the synthesized dataset
locally for joint training. Real data are replaced by syn-
thetic data proportionally during training, and the replace-
ment probability pis set as 0.1, 0.2, 0.3, and 0.5 for all-
shot, 10-shot, 5-shot, and 1-shot classiﬁcation, respectively.
All experiments use ResNet50 with an input resolution of
224⇥224. Additional details can be found in the SMs.
Comparison methods. To unveil the trade-off between
faithfulness and diversity resulting from different expan-
sion strategies, we compared Diff-Mix with Diff-Gen and
Diff-Aug . Furthermore, we conduct experiments on expan-
sion strategies using vanilla SD: Real-Mix ,Real-Gen , and
Real-Aug , where ‘Real’ signiﬁes that SD is not ﬁne-tuned.
Main results. To answer Q1 under the few-shot classiﬁca-
tion setting, we augment CUB using X-Mix, X-Aug, and
X-Gen, where ‘X’ denotes ‘Diff/Real’ for simplicity. The
results are shown in Fig. 6, and we can observe that:
1.Diff-Mix generally outperforms the intra-class competi-
tor X-Aug and distillation competitor X-Gen in various
few-shot scenarios. It tends to achieve higher gains when
the strength sis relatively large, i.e.,{0.5, 0.7, 0.9 },
where the foreground has been edited to match the tar-
get class and the background retains similarities to the
reference image.
2.Among the Real-X methods, distillation tends to be more
effective than the augmentation method when the shot
17227
(a) 1-shot(b) 5-shot(c) 10-shot(d) All-shot
Figure 6. Few-shot classiﬁcation results on CUB.
number is low, but the trend reverses as the shot num-
ber increases (compare Real-Gen with Real-Aug). Real-
Gen’s samples even show less effective than Real-Aug
(s=1.0) under the all-shot case3. This indicates that
the importance of faithfulness in the trade-off between
faithfulness and diversity increases with the shot num-
ber. Additionally, Real-Mix exhibits consistent and sta-
ble improvement over the other two methods.
3.Diff-Gen consistently outperforms Real-Gen under four
scenarios. Notably, Real-Gen’s performance declines
below that of the baseline as the shot numbers reach
10, showcasing the importance of the ﬁne-tuning process
which increases the faithfulness of synthetic samples.
4.2. Conventional Classiﬁcation
Experimental setting. To test whether Diff-Mix can fur-
ther boost performance in a more challenging setting, i.e.,
under the all-shot scenario with high input resolution, we
conduct conventional classiﬁcation on ﬁve domain-speciﬁc
datasets: CUB [ 61], Stanford Cars [ 27], Oxford Flowers
[38], Stanford Dogs [ 26], and FGVC Aircraft [ 34]. Two
backbones are employed: pretrained (ImageNet1K [ 49])
ResNet50 [ 18] with input resolution 448 ⇥448, and pre-
trained (ImageNet21K) ViT-B/16 [ 13] with input resolu-
tion384⇥384. Label smoothing [ 36] is applied across all
datasets with a conﬁdence level of 0.9. For all expansion
strategies, the expansion multiplier is 5 and the replacement
probability pis 0.1. Besides, we use a randomized sampling
strategy ( s2{0.5,0.7,0.9}) and a ﬁxed  (0.5, and this is
speciﬁc to Diff-Mix) for all datasets.
Comparison methods. We compare Diff-Mix with (1)
Real-Filtering (RF) [ 19], a variation of Real-Gen that in-
corporates clip ﬁltering, (2) Real-Guidance (RG) [ 19],
which augments the dataset using intra-class image trans-
lation at low strength ( s=0 .1), (3) Da-Fusion [ 58], a
method that solely ﬁne-tunes the identiﬁer to personal-
ize each class and employs randomized sampling strategy
3Real-Aug ( s=1 .0) remains analogous to the reference image
(slightly higher faithfulness compared to Real-Gen) because the discrete
forward process cannot approximate the ideal normal distribution within a
limited number of steps ( T=2 5 ).(s2{0.25,0.5,0.75,1.0}), and non-generative augmenta-
tion methods (4) CutMix [ 66] and (5) Mixup [ 68].
Main results. We show the classiﬁcation accuracy for dif-
ferent data expansion strategies in Table 1, our observa-
tions can be summarized: (1) Diff-Mix consistently demon-
strates stable improvements across the majority of settings.
Its average performance gain across the ﬁve datasets ex-
ceeds that of baselines employing intra-class augmentation
methods (RG and Da-fusion), distillation method (RF), and
non-generative data augmentation techniques (CutMix and
Mixup). (2) Real-ﬁltering, analogous to the discussion
of Real-Gen, exhibits performance degradation on most
datasets due to the distribution gap. (3) The combined use
of Diff-Mix and CutMix often yields better performance
gains. This is attributed to the distinct enhancement mech-
anisms of the two methods, i.e., vicinal risk minimization
[11,68] and foreground-background disentanglement. (4)
Diff-Mix does not exhibit signiﬁcant performance improve-
ment in the dog dataset. We attribute this lack of improve-
ment to the complexity of the dog dataset, which often con-
tains multiple subjects in a single image, impeding effective
foreground editing (refer to the SMs for visual examples).
4.3. Long-Tail Classiﬁcation
Experiment setting. Following the settings of previ-
ous long-tail dataset constructions [ 9,32,39], we create
two domain-speciﬁc long-tail datasets, CUB-LT [ 50] and
Flower-LT. The imbalance factor controls the exponential
distribution of the imbalanced data, where a larger value in-
dicates a more imbalanced distribution. To leverage gen-
erative models for long-tail classiﬁcation, we adopt the
approach of SYNAuG [ 65], which uniformize the imbal-
anced real data distribution using synthetic data. Transla-
tion strength s(0.7) and  (0.5) are ﬁxed for both Diff-Mix
and Real-Mix.
Comparison methods. We compare Diff-Mix with Real-
Mix, Real-Gen, the non-generative CutMix-based oversam-
pling approach CMO [ 39], and its enhanced variant with
two-stage deferred re-weighting [ 9] (CMO+DRW).
Main results. We present the long-tail classiﬁcation re-
17228
Backbone Aug. Method FT StrategyDatasetCUB Aircraft Flower Car Dog AvgResNet50@448- - 86.64 89.09 99.27 94.54 87.48 91.40Cutmix[66] - 87.23 89.44 99.25 94.73 87.59 91.65+0.25Mixup[68] - 86.68 89.41 99.40 94.49 87.42 91.48+0.08Real-ﬁltering [19]†%85.60 88.54 99.09 94.59 87.30 91.22 0.18Real-guidance [19]†%86.71 89.07 99.25 94.55 87.40 91.59+0.19Da-fusion [58]†TI 86.30 87.64 99.37 94.69 87.33 91.07 0.58Diff-MixTI + DB87.1690.2599.5495.1287.7491.96+0.56Diff-Mix + CutmixTI + DB87.5690.0199.4795.2187.8992.03+0.63ViT-B/16@384- - 89.37 83.50 99.56 94.21 92.06 91.74Cutmix[66]-90.5283.50 99.64 94.8392.1392.12+0.38Mixup[68] - 90.32 84.3199.7394.98 92.02 92.27+0.53Real-ﬁltering [19]†%89.49 83.07 99.36 94.66 91.91 91.69 0.05Real-guidance [19]†%89.54 83.17 99.59 94.65 92.05 91.80+0.06Da-fusion [58]†TI 89.40 81.88 99.61 94.53 92.07 91.50 0.24Diff-MixTI + DB90.0584.3399.6495.0991.9992.22+0.48Diff-Mix + CutmixTI + DB90.3585.1299.6895.2691.8992.46+0.72Table 1. Conventional classiﬁcation in six ﬁne-grained datasets. ‘†’ indicates our reproduced results using SD.
MethodIF=10050 10Many Medium Few All
CE 79.11 64.28 13.48 33.65 44.82 58.13
CMO [ 39] 78.32 58.57 14.78 32.94 44.08 57.62
CMO + DRW [ 10] 78.97 56.36 14.66 32.57 46.43 59.25
Real-Gen 84.88 65.23 30.68 45.86 53.43 61.42
Real-Mix (s=0.7) 84.63 66.34 34.44 47.75 55.67 62.27
Diff-Mix (s=0.7) 84.07 67.79 36.55 50.35 58.19 64.48
Table 2. Long-tail classiﬁcation in CUB-LT.
MethodIF=10050 10Many Medium Few All
CE 99.19 94.95 58.18 80.43 90.87 95.07
CMO [ 39] 99.25 95.19 67.45 83.95 91.43 95.19
CMO+ DRW [ 10]99.97 95.06 67.31 84.07 92.06 95.92
Real-Gen 98.64 95.55 66.10 83.56 91.84 95.22
Real-Mix (s=0.7) 99.87 96.26 68.53 85.19 92.96 96.04
Diff-Mix (s=0.7) 99.25 96.98 78.41 89.46 93.86 96.63
Table 3. Long-tail classiﬁcation in Flower-LT.
sults for CUB-LT in Table 2and Flower-LT in 3. The ob-
servations are as follows: (1) Generative approaches ex-
hibit superior performance in tackling imbalanced classi-
ﬁcation issues compared to CutMix-based methods (CMO
and CMO+DRW). (2) Real-Mix surpasses Real-Gen in per-
formance across various imbalance factors in two datasets.
This indicates that tail classes can beneﬁt from the en-
hanced diversity by leveraging the visual context of major-
ity classes. (3) Diff-Mix generally achieves the best per-
formance among the compared strategies, especially at the
low-shot case, highlighting the importance of ﬁne-tuning.
4.4. Background Robustness
In this section, we aim to evaluate Diff-Mix’s robustness to
background shifts, speciﬁcally, whether synthesizing more
diverse samples can improve the classiﬁcation model’s gen-
eralizability when the background is altered. To achieveGroup Base. CutMix DA-fusion Diff-Aug Diff-Mix
(waterbird, water) 59.50 62.46 60.90 61.83 63.83
(waterbird, land) 56.70 60.12 58.10 60.12 63.24
(landbird, land) 73.48 73.39 72.94 73.04 75.64
(landbird, water) 73.97 74.72 72.77 73.52 74.36
Avg. 70.19 71.23 69.90 70.28 72.47
Table 4. Classiﬁcation results across four groups in Waterbird
[48]. Waterbird is an out-of-distribution dataset for CUB, crafted
by segmenting CUB’s foregrounds and paste them into the scene
images from Places [ 70]. The constructed dataset can be divided
into four groups based on the composition of foregrounds (water-
bird and landbird) and backgrounds (water and land) .
this, we utilize an out-of-distribution test set for CUB,
namely Waterbird [ 48]. We then perform inference on the
whole Waterbird set using classiﬁers that have been trained
on either the original CUB dataset or its expanded varia-
tions. In Table 4, we present the classiﬁcation accuracies
across the four groups and compare Diff-Mix with other
intra-class methods (Da-fusion and Diff-Aug) as well as
CutMix. We observe that Diff-Mix generally outperforms
its counterparts and achieves a signiﬁcant performance im-
provement (+6.5%) in the challenging counterfactual group
(waterbirds with land backgrounds). It is important to high-
light that the background scenes in the Waterbird dataset are
novel to CUB, requiring the classiﬁcation model to have a
stronger perceptual capability for the images’ foregrounds.
4.5. Discussion
In this section, we address Q2 by examining the impact of
size and diversity on synthetic data. Furthermore, we per-
form an ablation study to assess the effects of ﬁne-tuning
strategies and training hyperparameters of Diff-Mix, which
is aimed at answering Q3. Unless speciﬁed otherwise, our
discussions are based on experiments conducted using CUB
17229
(a)(b)
Figure 7. Comparison of results across various (a) synthetic data
sizes and (b) numbers of referable classes for each target class.
with ResNet50, where inputs are resized to 224⇥224.
Size and diversity of synthetic Data. The relationship be-
tween performance gain and the size of synthetic data is
depicted in Fig. 7(a), where a classiﬁcation model was
trained with synthetic data of varying sizes. We observe a
monotonically increasing trend as the multiplier for the syn-
thetic dataset ranges from 2 to 10. Ideally, the combination
choices of (xi,yj)are in the order of N|Dtrain|(N= 200
for CUB). Furthermore, we limit the number of referable
classes for each target class, which means the number of
referable backgrounds decreases, resulting in a synthetic
dataset of relatively lower diversity. The results are shown
in Fig. 7(b), and we observe a consistent improvement in
performance as the number of referable classes increases.
These results consistently underscore the critical role of
background diversity introduced by Diff-Mix.
Impact of ﬁne-tuning strategy. Here we compare three
different ﬁne-tuning strategies: TI, DB, and the combined
TI+DB. All strategies share the same ﬁne-tuning hyperpa-
rameters and training steps (35000). To evaluate the distri-
bution gap, we compute the FID score [ 20] of synthesized
images (Diff-Gen) with the training set. As illustrated in
Table 5, we observe that both TI+DB and TI have lower
FID scores than DB. This can be attributed to the fact that
semantic proximity impedes the convergence process. Ad-
ditionally, while using TI alone results in a relatively low
FID score, the improvements in performance are limited.
This limitation stems from TI’s inability to accurately re-
construct detailed concept (foreground) information, as it is
primarily ﬁne-tuned at the semantic level [ 28].
Annotation function. In this section, we discuss the im-
pact of the choices of the translation strength sand the
non-linear factor  in Eq. 3. As shown in Table 6, we
observe that as the translation strength decreases, the opti-
mal value for  also decreases, which underscores the non-
linearity of Diff-Mix. The comparison between the 5-shot
and all-shot settings indicates that the model tends to pre-
fer a more diverse synthetic dataset when the number of
training shots is small ( s=0.9for 5-shot, s=0.7for all-
shot). Besides, a larger conﬁdence in the target class is pre-Baseline TI DB TI + DB
5-shotFID (Diff-Gen) - 18.26 19.55 18.43
Acc. (Diff-Mix) 50.90 57.64 56.11 59.41
All-shotFID (Diff-Gen) - 14.13 14.64 13.99
Acc. (Diff-Mix) 81.60 81.86 81.99 82.85
Table 5. Comparison of distribution gap and classiﬁcation accu-
racy across three ﬁne-tuning strategies. TI solely ﬁne-tunes the
identiﬁer, and DB solely ﬁne-tunes the U-Net, and TI+DB.
 5-shot All-shot
s=0.5 s=0.7 s=0.9 s=0.5 s=0.7 s=0.9
1.5 -4.50 -0.31 +10.30 -1.08 +0.92 +0.90
1.0 -2.31 +2.99 +10.79 +0.25 +1.14 +0.90
0.5 +2.35 +8.44 +11.01 +0.92 +1.30 +0.86
0.3 +3.94 +9.41 +11.15 +0.97 +1.24 +0.69
0.1 +6.18 +9.86 +10.84 +0.50 +0.88 +0.84
0.0 +5.25 +9.41 +11.06 +0.38 +0.63 +0.54
Table 6. Comparison of performance gain across various  and
translation strength s. Lower  indicates a higher conﬁdence over
target class, e.g.( =0.1,s=0.7)results in 0.04yi+0.96yj
and( =0.5,s=0.7)results in 0.16yi+0.84yj.
ferred when the shot number is small (  =0.1for 5-shot,
 =0.5for all-shot). A possible explanation is that the all-
shot setting is less tolerant towards unrealistic images, as
discussed in Section 6. Empirically, we recommend choos-
ing a higher translation strength ( s20.5,0.7,0.9) and a
smaller  ( 20.1,0.3,0.5) as a conservative option.
5. Conclusion
In this work, we investigate two pivotal aspects, faithfulness
and diversity, that are critical for the current state-of-the-art
text-to-image generative models to enhance image classiﬁ-
cation tasks. To achieve a more effective balance between
these two aspects, we propose an inter-class augmentation
strategy that leverages Stable Diffusion. This method en-
ables generative models to produce a greater diversity of
samples by editing images from other classes and shows
consistent performance improvement across various classi-
ﬁcation tasks.
6. Acknowledgement
This research is mainly supported by the National Natural
Science Foundation of China (92270114). This work is also
partially supported by the National Key R&D Program of
China under Grant 2021ZD0112801.
17230
References
[1]Antreas Antoniou, Amos Storkey, and Harrison Edwards.
Data augmentation generative adversarial networks. arXiv
preprint arXiv:1711.04340 , 2017. 2
[2]Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-
hammad Norouzi, and David J Fleet. Synthetic data from
diffusion models improves imagenet classiﬁcation. arXiv
preprint arXiv:2304.08466 , 2023. 1,2
[3]Haoyue Bai, Ceyuan Yang, Yinghao Xu, S-H Gary Chan,
and Bolei Zhou. Improving out-of-distribution robustness
of classiﬁers via generative interpolation. arXiv preprint
arXiv:2307.12219 , 2023. 2
[4]Hritik Bansal and Aditya Grover. Leaving reality to imag-
ination: Robust classiﬁcation via generated datasets. arXiv
preprint arXiv:2302.02503 , 2023. 1,2
[5]Christopher Beckham, Sina Honari, Vikas Verma, Alex M
Lamb, Farnoosh Ghadiri, R Devon Hjelm, Yoshua Bengio,
and Chris Pal. On adversarial mixup resynthesis. Advances
in neural information processing systems , 32, 2019. 2
[6]Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal,
Rao Muhammad Anwer, Jorma Laaksonen, Mubarak Shah,
and Fahad Shahbaz Khan. Person image synthesis via de-
noising diffusion model. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5968–5976, 2023. 2
[7]Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high ﬁdelity natural image synthesis.
arXiv preprint arXiv:1809.11096 , 2018. 1
[8]Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392–18402, 2023.
3
[9]Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga,
and Tengyu Ma. Learning imbalanced datasets with label-
distribution-aware margin loss. Advances in neural informa-
tion processing systems , 32, 2019. 6
[10] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga,
and Tengyu Ma. Learning imbalanced datasets with label-
distribution-aware margin loss. Advances in neural informa-
tion processing systems , 32, 2019. 7
[11] Olivier Chapelle, Jason Weston, L ´eon Bottou, and Vladimir
Vapnik. Vicinal risk minimization. Advances in neural in-
formation processing systems , 13, 2000. 6
[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 1
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 6
[14] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.
Williams, J. Winn, and A. Zisserman. The pascal visual ob-
ject classes challenge: A retrospective. International Journal
of Computer Vision , 111(1):98–136, 2015. 1,2,5[15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618 , 2022. 2,3
[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014. 2
[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014. 1
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 6
[19] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing
Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic
data from generative models ready for image recognition?
arXiv preprint arXiv:2210.07574 , 2022. 2,6,7,3
[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 8
[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 3
[22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High deﬁnition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 2
[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 4
[24] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 3
[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4401–4410, 2019. 1,2
[26] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng
Yao, and Li Fei-Fei. Novel dataset for ﬁne-grained image
categorization. In First Workshop on Fine-Grained Visual
Categorization, IEEE Conference on Computer Vision and
Pattern Recognition , Colorado Springs, CO, 2011. 6,2
[27] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for ﬁne-grained categorization. In
Proceedings of the IEEE international conference on com-
puter vision workshops , pages 554–561, 2013. 6,2
[28] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
17231
of text-to-image diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1931–1941, 2023. 2,3,8
[29] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis
Brown, and Deepak Pathak. Your diffusion model is secretly
a zero-shot classiﬁer. arXiv preprint arXiv:2303.16203 ,
2023. 2
[30] Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat seng
Chua. Invariant grounding for video question answering. In
CVPR , 2022. 2
[31] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang.
Common diffusion noise schedules and sample steps are
ﬂawed. arXiv preprint arXiv:2305.08891 , 2023. 1
[32] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang,
Boqing Gong, and Stella X Yu. Large-scale long-tailed
recognition in an open world. In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition , pages 2537–2546, 2019. 6
[33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion
probabilistic model sampling in around 10 steps. Advances
in Neural Information Processing Systems , 35:5775–5787,
2022. 3
[34] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew
Blaschko, and Andrea Vedaldi. Fine-grained visual classi-
ﬁcation of aircraft. arXiv preprint arXiv:1306.5151 , 2013.
6,2
[35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. arXiv preprint arXiv:2108.01073 , 2021. 3
[36] Rafael M ¨uller, Simon Kornblith, and Geoffrey E Hinton.
When does label smoothing help? Advances in neural in-
formation processing systems , 32, 2019. 6
[37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 1,2
[38] Maria-Elena Nilsback and Andrew Zisserman. Automated
ﬂower classiﬁcation over a large number of classes. In 2008
Sixth Indian conference on computer vision, graphics & im-
age processing , pages 722–729. IEEE, 2008. 6,2
[39] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun,
and Jin Young Choi. The majority can help the minor-
ity: Context-rich minority oversampling for long-tailed clas-
siﬁcation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6887–
6896, 2022. 6,7,3
[40] Maitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou
Yang. Conceptbed: Evaluating concept learning abili-
ties of text-to-image diffusion models. arXiv preprint
arXiv:2306.04695 , 2023. 3
[41] Judea Pearl. Causal inference in statistics: An overview.
2009. 5
[42] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao
Feng, Zhen Liu, Dan Zhang, Adrian Weller, and BernhardSch¨olkopf. Controlling text-to-image diffusion by orthogo-
nal ﬁnetuning. Advances in Neural Information Processing
Systems , 36, 2024. 3
[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 5,1
[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models, 2021. 1
[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 3
[46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500–
22510, 2023. 2,3
[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115:211–252, 2015. 2
[48] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and
Percy Liang. Distributionally robust neural networks for
group shifts: On the importance of regularization for worst-
case generalization. arXiv preprint arXiv:1911.08731 , 2019.
7
[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J
Fleet, and Mohammad Norouzi. Photorealistic text-to-image
diffusion models with deep language understanding, 2022.
1,2,6
[50] Dvir Samuel, Yuval Atzmon, and Gal Chechik. From gener-
alized zero-shot learning to long-tail with class descriptors.
InProceedings of the IEEE/CVF Winter Conference on Ap-
plications of Computer Vision , pages 286–295, 2021. 6
[51] Veit Sandfort, Ke Yan, Perry J Pickhardt, and Ronald M
Summers. Data augmentation using generative adversarial
networks (cyclegan) to improve generalizability in ct seg-
mentation tasks. Scientiﬁc reports , 9(1):16884, 2019. 2
[52] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and
Yannis Kalantidis. Fake it till you make it: Learning trans-
ferable representations from synthetic imagenet clones. In
CVPR 2023–IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2023. 1
[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
17232
next generation image-text models. Advances in Neural In-
formation Processing Systems , 35:25278–25294, 2022. 2
[54] Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei
Xiang, and Clinton Fookes. Boosting zero-shot classiﬁca-
tion with synthetic data diversity via stable diffusion. arXiv
preprint arXiv:2302.03298 , 2023. 2
[55] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792 ,
2022. 2
[56] Shikun Sun, Longhui Wei, Zhicai Wang, Zixuan Wang, Jun-
liang Xing, Jia Jia, and Qi Tian. Inner classiﬁer-free guid-
ance and its taylor expansion for diffusion models. In The
Twelfth International Conference on Learning Representa-
tions , 2023. 2
[57] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and
Dilip Krishnan. Stablerep: Synthetic images from text-to-
image models make strong visual representation learners.
arXiv preprint arXiv:2306.00984 , 2023. 1,2
[58] Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan
Salakhutdinov. Effective data augmentation with diffusion
models. arXiv preprint arXiv:2302.07944 , 2023. 2,5,6,7,
3
[59] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1921–1930, 2023. 3
[60] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro
Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,
and Thomas Wolf. Diffusers: State-of-the-art diffusion
models. https://github.com/huggingface/
diffusers , 2022. 3
[61] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011. 5,6,2
[62] Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, Shuo
Wang, and Xiangnan He. Bi-directional distribution align-
ment for transductive zero-shot learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 19893–19902, 2023. 2
[63] Daniel Watson, William Chan, Ricardo Martin-Brualla,
Jonathan Ho, Andrea Tagliasacchi, and Mohammad
Norouzi. Novel view synthesis with diffusion models. arXiv
preprint arXiv:2210.04628 , 2022. 2
[64] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace
analysis: Disentangled controls for stylegan image genera-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 12863–12872,
2021. 3
[65] Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, Nayeong
Kim, Suha Kwak, and Tae-Hyun Oh. Exploiting synthetic
data for data imbalance problems: Baselines from a data per-
spective. arXiv preprint arXiv:2308.00994 , 2023. 6
[66] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classiﬁers with localizablefeatures. In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 6023–6032, 2019. 2,6,7
[67] An Zhang, Wenchang Ma, Xiang Wang, and Tat seng Chua.
Incorporating bias-aware margins into contrastive loss for
collaborative ﬁltering. In NeurIPS , 2022. 2
[68] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. arXiv preprint arXiv:1710.09412 , 2017. 2,6,7
[69] Chenyu Zheng, Guoqiang Wu, and Chongxuan Li. Toward
understanding generative data augmentation. Advances in
Neural Information Processing Systems , 36, 2024. 2
[70] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE transactions on pattern analysis
and machine intelligence , 40(6):1452–1464, 2017. 7
[71] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of the IEEE
international conference on computer vision , pages 2223–
2232, 2017. 3
17233
