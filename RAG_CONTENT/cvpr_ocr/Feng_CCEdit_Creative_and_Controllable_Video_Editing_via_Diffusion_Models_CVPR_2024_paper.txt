CCEdit: Creative and Controllable Video Editing via Diffusion Models
Ruoyu Feng1,2 *, Wenming Weng1,2, Yanhui Wang1,2,
Yuhui Yuan2, Jianmin Bao2, Chong Luo2 ‚Ä†, Zhibo Chen1 ‚Ä†, Baining Guo2
1University of Science and Technology of China2Microsoft Research Asia
https://ruoyufeng.github.io/CCEdit.github.io/
Intention :Replace the background with snowy winter scene.
Intention: Make the foreground puppy chubby and adorable.
Input Video Edited Video
Intention: Transform this video into the mechanical style.
Intention: Transform this video into the cyberpunk style.
Intention : Transform this video into the Van Gogh Starry Night style.
Figure 1. Built upon diffusion models, CCEdit provides users with a powerful and flexible set of video editing capabilities, including style
transfer (row 1-3), foreground modifications (row 4), and background replacement (row 5).
Abstract
In this paper, we present CCEdit, a versatile generative
video editing framework based on diffusion models. Our
approach employs a novel trident network structure that
separates structure and appearance control, ensuring pre-
cise and creative editing capabilities. Utilizing the founda-
tional ControlNet architecture, we maintain the structural
integrity of the video during editing. The incorporation
of an additional appearance branch enables users to exert
fine-grained control over the edited key frame. These two
‚àóThis work is done when Ruoyu Feng is an intern with MSRA.
‚Ä†Corresponding author.side branches seamlessly integrate into the main branch,
which is constructed upon existing text-to-image (T2I) gen-
eration models, through learnable temporal layers. The
versatility of our framework is demonstrated through a di-
verse range of choices in both structure representations and
personalized T2I models, as well as the option to provide
the edited key frame. To facilitate comprehensive evalua-
tion, we introduce the BalanceCC benchmark dataset, com-
prising 100 videos and 4 target prompts for each video. Our
extensive user studies compare CCEdit with eight state-of-
the-art video editing methods. The outcomes demonstrate
CCEdit‚Äôs substantial superiority over all other methods.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6712
1. Introduction
In recent years, the domain of visual content creation and
editing has undergone a profound transformation, driven
by the emergence of diffusion-based generative models
[11, 20, 46]. A large body of prior research has demon-
strated the exceptional capabilities of diffusion models in
generating diverse and high-quality images [38, 40, 42] and
videos [5, 21, 44], conditioned by text prompts. These ad-
vancements have naturally paved the way for innovations in
generative video editing [7, 25, 34, 36, 50, 53, 54, 58].
Generative video editing, despite its rapid advancement,
continues to face a series of significant challenges. These
challenges include accommodating diverse editing requests,
achieving fine-grained control over the editing process, and
harnessing the creative potential of generative models. Di-
verse editing requirements include tasks such as stylistic al-
terations, foreground replacements, and background modi-
fications. Generative models, while powerful and creative,
may not always align perfectly with the editor‚Äôs intentions
or artistic vision, resulting in a lack of precise control. In
response to these challenges, this paper introduces CCEdit,
a versatile generative video editing framework meticulously
designed to strike a harmonious balance between controlla-
bility and creativity while accommodating a wide range of
editing requirements.
CCEdit achieves its goal by effectively decoupling struc-
ture and appearance control in a unified trident network .
This network comprises three essential components: the
main text-to-video generation branch and two accompany-
ing side branches dedicated to structure and appearance ma-
nipulation. The main branch leverages a pre-trained text-
to-image (T2I) diffusion model [40], which is transformed
into a text-to-video (T2V) model through the insertion of
temporal modules. The structure branch , implemented as
ControlNet [55], is responsible for digesting the structural
information extracted from each frame of the input video
and seamlessly infusing it into the main branch. Simulta-
neously, the appearance branch introduces an innovative
mechanism for precise appearance control, when an edited
reference frame is available. The structure and appearance
branches are effectively integrated into the central branch
through learnable temporal layers. These layers serve not
only as a cohesive link, aggregating information from side
branches, but also as a crucial element ensuring temporal
consistency across the generated video frames.
In highlighting the versatility of our framework, we pro-
vide a wide range of control choices for both structure and
appearance manipulation. For structure control, users can
choose from various types of structural information, in-
cluding line drawings [8], PiDi boundaries [47], and depth
maps [39], all of which can serve as input to the structure
branch. On the appearance control front, the main branch
already provides an inherent mechanism, allowing controlthrough text prompts. Additionally, personalized T2I mod-
els from the Stable Diffusion community, such as Dream-
Booth and LoRA [22, 41], can be integrated as plugins into
CCEdit, offering greater flexibility and creativity. More
importantly, the appearance branch can accommodate the
referenced key frame, facilitating fine-grained appearance
control. Notably, all these control options are seamlessly
integrated within the same framework, yielding editing out-
comes that demonstrate both temporal coherence and pre-
cision. This not only underscores the versatility of our so-
lution but also ensures ease of adoption, making it a com-
pelling choice for AI-assisted video editing.
To address the challenges inherent in evaluating gener-
ative video editing methods, we introduce the BalanceCC
benchmark dataset. Comprising 100 diverse videos and 4
target prompts for each video, this dataset includes detailed
scene descriptions and attributes related to video category,
scene complexity, motion, among others. These descrip-
tions are generated with the assistance of the cutting-edge
GPT-4V(ision) model [1, 31‚Äì33] and then refined by hu-
man annotators. Through extensive experimental evalua-
tions on this dataset, we not only confirm the outstanding
functionality and editing capabilities of CCEdit, but also un-
derscore the comprehensiveness of the benchmark dataset.
We firmly believe that BalanceCC stands as a robust and
all-encompassing evaluation platform for the dynamic field
of generative video editing.
2. Related Work
2.1. Diffusion-based Image and Video Generation
Diffusion models (DM) [11, 16, 20, 29, 30, 46] have demon-
strated exceptional capabilities in the field of image syn-
thesis. These models indeed help by learning to approxi-
mate a data distribution through the iterative denoising of
a diffused input. What makes DMs truly practical is the
incorporation of text prompt as condition to control the out-
put image during the generative process [30, 37, 40, 42].
Apart from the proliferation of advanced techniques in the
field of image synthesis, DMs have also excelled in video
generation [5, 21, 30, 44]. This is achieved by integrating
modulated spatial-temporal modules, enabling the synthe-
sis of high-quality videos while maintaining temporal con-
sistency.
2.2. Video Editing with Diffusion Models
Recent studies leverage the inherent generative priors of
DMs for image editing [3, 10, 17, 27, 35, 48]. The same
idea is also applied in the field of video editing. Unlike
image editing, video editing involves not only the manip-
ulation of appearance-based attributes but also requires the
meticulous preservation of temporal coherence throughout
frames. A lapse in maintaining this temporal coherence can
6713
P: A bear is walking, in winter.
<S: Depth> <B: SD v1.5> 
<w/ reference>Appearance Control:
‚Ä¢Text prompt, e.g. ‚Äú A bear is walking, anime style .‚Äù
‚Ä¢Personalized models
‚Ä¢Edited key frame:
Structure Information
Structure Extraction
 Key Frame Editing
Text 
prompt~ùí©(ùüé,ùë∞)
P: A bear is walking, pixel art style .
<S: Depth> <B: Counterfeit> 
<L: Pixel Art> <w/ Reference>
A bear is walking, anime style.
<S: Line drawings> 
<B: Toonyou >
P: A bear is walking, Chinese Ink style.
<S: PiDi Boundary> <B: Counterfeit > 
<L: Moxin >
P: A tiger  is walking.
<S: PiDi  Boundary> <B: SD v1.5>
<w/ Reference>
Base Model: ToonYou
 LoRA : Pixel Art
Structure Control:
Depth
 Line Drawing
‚Ä¶ Style 
TransferForeground 
ReplacementBackground 
Modification‚Ä¶ 
Learnable appearance encoder
Learnable temporal layers
Frozen structure ControlNetFrozen T2I model
Pidi Boundary
‚Ä¶ 
LoRA : MoxinFigure 2. Illustration of our overall framework. Structure and appearance information in the target video are modulated independently,
and seamlessly integrated into the main branch. Structure control is conducted via the pre-trained ControlNet [55]. Appearance control is
achieved precisely by the edited key frame. Details regarding the autoencoder and iterative denoising process are omitted for simplicity.
‚ÄúP‚Äù, ‚ÄúS‚Äù, ‚ÄúB‚Äù, ‚ÄúL‚Äù indicate prompt, structure, base model, and LoRA, respectively.
result in visual artifacts, such as flickering and degradation.
Some generative video editing methods [6, 14, 23, 36,
49, 54, 56] strive to achieve training-free temporal con-
sistency. They accomplish this by transitioning from spa-
tial self-attention mechanisms within T2I diffusion models
to temporal-aware cross-frame attention techniques. Some
other methods [26, 43, 51, 58] perform per-video fine-
tuning. They focus on optimizing the parameters of pre-
trained T2I models according to the input video, aiming to
achieve temporal coherence within the target video. How-
ever, this optimization for each input video can be time-
consuming and inadequate tuning of the temporal modules
might lead to suboptimal temporal coherence. Recent stud-
ies [15, 24, 53] have introduced trainable temporal layers to
construct T2V generative models. These models are trained
on extensive text-video paired datasets, and they are used in
both video generation and editing tasks [12, 28].
Unlike previous work, this study does not seek a sim-
ple fix to existing T2I models for video editing, nor does
it attempt to train a full-fledged T2V model. Instead, we
introduce a unique network architecture tailored for video
editing. Our approach involves dataset-level fine-tuning,
circumvents the expenses associated with per-video tuning
during inference time, and prioritizing the effective trainingof temporal layers to achieve robust model performance.
3. Approach
3.1. Preliminary
Diffusion models [20] are probabilistic generative mod-
els that approximate a data distribution p(x)by gradu-
ally denoising a normally distributed variable. Specifically,
DMs aim to learn the reverse dynamics of a predetermined
Markov chain with a fixed length of T. The forward Markov
chain can be conceptualized as a procedure of injecting
noise into a pristine image. Empirically, DMs can be in-
terpreted as an equally weighted sequence of denoising au-
toencoders œµŒ∏(xt, t)where t= 1, ..., T . These autoen-
coders are trained to predict a denoised variant of the noisy
input xt. The corresponding objective can be simplified to
Ex0,t,œµ‚àºN (0,I)[‚à•œµ‚àíœµŒ∏(xt, t)‚à•2
2]. (1)
Latent diffusion models (LDMs) are trained in the learned
latent representation space. The bridge between this latent
space and the original pixel-level domain is established via
a perceptual compression model. The perceptual compres-
sion model is composed of an encoder Eand a decoder D,
where z=E(x)andx‚âà D(E(x)). Then the optimization
6714
objective in Eq. (1) is modified as
Ez0,t,œµ‚àºN (0,I)[‚à•œµ‚àíœµŒ∏(zt, t)‚à•2
2]. (2)
3.2. The CCEdit Framework
The primary objective of our work is to empower cre-
ative control in video editing. Although creativity naturally
emerges in generative models, achieving controllability is a
more complex endeavor. To address this challenge, CCEdit
strategically decouples the management of structure and ap-
pearance within a unified trident network. In Fig. 2, we
provide an illustrative overview of the framework‚Äôs archi-
tecture, which comprises three vital components.
The main branch. The main branch of our model funda-
mentally operates as a text-to-video generation network. It
is built upon the well-established text-to-image model, Sta-
ble Diffusion [40]. We transform this model into a text-to-
video variant by incorporating temporal layers into spatial
layers of both the encoder and decoder. This entails the ad-
dition of a one-dimensional temporal layer with the same
type as its previous spatial layer ,i.e., convolution blocks
and attention blocks. Besides, we also use the skip connec-
tion and zero-initialized projection out layer of each newly
added temporal layer for stable and progressive updating,
which has been proven to be effective [15, 44, 55]. The
zero-initialized projection out layer is instantiated as a lin-
ear layer. Formally, let F(¬∑; Œòs)be the 2D spatial block,
F(¬∑; Œòt)be the 1D temporal block, and Z(¬∑; Œòz)be the
zero-initialized projection out layer, where Œòs,Œòt, andŒòz
represent corresponding network parameters. The complete
process of one pseudo-3D block that maps the input feature
uto the output feature vis written as
v=F(u; Œòs) +Z(F(F(u; Œòs); Œòt); Œòz), (3)
where uandvare both 3D feature maps, i.e.,u‚àà
Rl√óh√ów√ócwith{l, h, w, c }as the number of frames, height,
width, and the number of channels, respectively.
Moreover, we draw inspiration from AnimateDiff [15]
and VideoLDM [5], which advocates the shared utilization
of temporal layers among personalized T2I models such
as DreamBooth [41] and LoRA [22]. The key aspect of
it is training the temporal layers while keeping the spatial
weights frozen. We follow this schedule to inherit the T2I
model‚Äôs compatibility and visual generation capability.
The structure branch. The introduction of the structure
branch is motivated by the common need in video editing
tasks to preserve frame structure for non-edited or style-
transferred segments. Striking a delicate balance between
maintaining faithful frame structure and allowing the gener-
ative model ample creative freedom poses a significant chal-
lenge. The structure branch is implemented with the pre-
trained ControlNet [55]. To accommodate varying levels
of structure control, we use various types of structure repre-
sentation, including line drawings [8], PiDi boundaries [47],and depth maps [39], ensuring adaptability to control struc-
ture at different degrees.
Specifically, the structure representation from all frames
is extracted individually and injected into the main branch.
Each frame undergoes preprocessing to derive a structure
representation, and the weights of the ControlNet are held
in a frozen state during training, emphasizing the preserva-
tion of learned structural features. Formally, let F(¬∑; Œ¶c)
denote the ControlNet that maps structure information into
features, and Z(¬∑; Œ¶z1)andZ(¬∑; Œ¶z2)denote the two in-
stances of zero convolutions in [55]. Then the process of
adding structure control to the 3D-aware feature vis
vs=v+Z(F(zt+Z(cs; Œ¶z1); Œ¶c); Œ¶z2), (4)
where ztdenotes the noisy input in latent space, csdenotes
the structure condition of the video sequence, and vsde-
notes the feature aware of structure information.
The appearance branch. In addition to using text prompts
and incorporating personalized models for appearance con-
trol, we introduce a novel design‚Äîthe appearance branch.
This architectural innovation introduces a pioneering ap-
proach for fine-grained appearance control, allowing for the
integration of an edited frame as a detailed reference in the
context of video editing. Since the editing of key frame
can be accomplished through precise user edits or by us-
ing advanced off-the-shelf image editing algorithms, the in-
troduction of appearance branch provides our framework
with greater creativity and controllability. Specifically, a
key frame is initially assigned to the latent variable by the
encoder E. Subsequently, a neural network with similar ar-
chitecture to the main branch‚Äôs encoder extracts multi-scale
features. The extracted features are incorporated into the
main branch. Through this design, the appearance informa-
tion from the edited key frame propagates to all frames via
the temporal modules, effectively achieving the desired cre-
ative control in the output video. Formally, suppose F(¬∑; Œ®)
is the encoder that maps the pixel-wise appearance of the
key frame into features, Z(¬∑; Œ®z)denotes the zero convolu-
tion projection out layer, vjindicates the feature of the j- th
frame, and cj
ais the key frame. Then the process of adding
appearance control to the features is as follows
vj
a=vj+Z(F(E(cj
a); Œ®); Œ® z), (5)
where vj
ais the j- thfeature, aware of the edited appearance.
Training. Before training, we initialize the spatial weights
of the main branch with pre-trained T2I models. Temporal
weights are randomly initialized while the projection out
layers are zero-initialized. We instantiate the model in the
structure branch by pre-trained ControlNets [55]. As for
the appearance branch, we copy the encoder of pre-trained
T2I model and remove text cross-attention layers. During
training, given the latent variables z0=E(x0)of an input
6715
video clip x0, diffusion algorithms progressively add noise
to it and produce the noisy input zt. Given conditions of
time step t, text prompt ct, structure information cs, and
appearance information cj
aof the key frame, the overall op-
timization objective is
Ez0,t,ct,cs,cj
a,œµ‚àºN (0,I)[‚à•œµ‚àíœµŒ∏(zt, t,ct,cs,cj
a)‚à•2
2],(6)
where œµŒ∏indicates the whole network to predict the noise
added to the noisy input zt. We freeze the spatial weights
in the main branch and the weights in the structure branch.
Concurrently, we update the parameters of the newly incor-
porated temporal layers in the main branch, as well as the
weights in the appearance branch. By default, the appear-
ance branch takes the center frame of the video clip as input.
Inference with anchor prior. We find that, in some chal-
lenging cases, the edited video may exhibit large areas of
flickering. This is often caused by inconsistent structural
representations extracted by image-level pre-processing
modules. Therefore, we propose a simple yet efficient strat-
egy to improve the stability and quality of the result by mod-
ifying the start noise. Specifically, consider the individual
noise sequence [œµ1
ind, ..., œµl
ind]and the edited center frame cj
a,
where landjindicate the frame numbers and the index of
the edited key frame, respectively. The start noise œµifor
each frame is modified as
œµi=œµi
ind+Œ±E(cj
a), (7)
where Œ±is the hyperparameter that controls the strength
of prior, and E(cj
a)is the latent of the edited key frame.
We call this strategy anchor prior , which is tailored for our
pipeline of editing videos with an reference key frame. We
empirically found that Œ±= 0.03works well in most cases.
The intuition behind it lies in that the video frames are usu-
ally similar to each other. The operation of adding noise to
diffusion models tends to rapidly destroy high-frequency in-
formation while slowly degrading low-frequency informa-
tion. Therefore, the anchor prior can be seen as provid-
ing a bit of low-frequency information to all frames while
ensuring that the distribution remains almost unchanged
(achieved by small Œ±), thus becoming better starting points.
3.3. Editing for Long Videos
Video editing tools face a challenge in maintaining a con-
sistent look and feel across clips that span tens of seconds,
equivalent to hundreds of frames. The inherent limitation
of generative models, processing only a dozen frames per
inference due to memory constraints, introduces variability
in results, even with a fixed random seed. CCEdit addresses
this challenge with its fine-grained appearance control, en-
abling the editing of long videos into a cohesive look and
feel through extension and interpolation modes.
In essence, let L+ 1 represent the frames CCEdit pro-
cesses in one run. For videos exceeding L+ 1frames, weselect one key frame for every Lframes. In the initial run,
the first L+1key frames undergo editing. Subsequent runs,
in extension mode, treat the last edited frame from the pre-
vious run as the first frame. The edited result serves as a
reference for the appearance branch. This process iterates
until all key frames are processed. Transitioning to the in-
terpolation mode, two adjacent frames become the first and
last frames of an inference run to edit the L‚àí1intermedi-
ate frames, and both edited frames serve as references for
the appearance branch. This continues until all frames are
edited. This meticulous process ensures consistent editing
results throughout the entire video.
4. BalanceCC Benchmark
4.1. Overview
While generative video editing has gained considerable at-
tention as a growing research field, the absence of a stan-
dardized benchmark for assessing the efficacy of different
approaches poses a potential hindrance to the field‚Äôs techni-
cal progression. Despite the recent introduction of TGVE
2023 [52] as an evaluation benchmark, it is crucial to note
that the videos within this benchmark present challenges
such as severe camera shake, complex scenes, blur, and low
frame rates. In response, we introduce BalanceCC , a bench-
mark containing 100 videos with varied attributes, designed
to offer a comprehensive platform for evaluating video edit-
ing, focusing on both controllability and creativity.
4.2. Benchmark Establishment
We curated a collection of 100 open-license videos suitable
for legal, non-stigmatizing modifications. These videos
range from 2 to 20 seconds in duration, each with a frame
rate of about 30 fps. Besides, we utilize GPT-4V(ision) [1,
31‚Äì33] as an assistant to establish this benchmark. For each
video, GPT-4V(ision) provides a description and assigns a
complexity score to the scene using the center frame as a
reference, with ratings from 1(Simple) to 3(Complex).
Additionally, we manually annotate each video for camera
movement, object movement, and categorical content, with
motion rated on a scale from 1(Stationary) to 3(Quick), and
categories that include humans, animals, objects, and land-
scapes. Following this, GPT-4V(ision) is tasked to craft tar-
get prompts for video editing, encompassing style, object,
and background alterations, along with compound changes.
This process, while akin to TGVE 2023 [52], we addition-
ally introduce a ‚ÄúFantasy Level‚Äù to indicate the imaginative
and creative degree of the target prompt. These measures
are intended to assist researchers in appraising the appli-
cability of various methods to source videos and in gaug-
ing their potential. See supplementary for details on the
prompting pipeline, specific instructions, principles of la-
beling, and illustrative examples.
6716
Figure 3. Illustration of the statistics on BalanceCC.
4.3. Statistics
The overall distribution of BalanceCC is illustrated in Fig.
3. For the data of original videos, the distribution across cat-
egories tends towards uniformity, yet the ‚ÄúHuman‚Äù category
is slightly more prevalent than others. This was a deliber-
ate choice, as editing human subjects is more practically
significant and, due to the complexity of human and facial
structures, editing in the ‚ÄúHuman‚Äù category presents more
challenges. Regarding ‚ÄúScene Complexity‚Äù and ‚ÄúObject
Motion‚Äù, videos with moderate and slow levels are slightly
more common. In terms of ‚ÄúCamera Motion‚Äù, videos of
lower levels predominate (Stationary: 54%, Slow: 38%).
Finally, regarding the ‚ÄúFantasy Level‚Äù distribution in tar-
get prompts, there is a relatively balanced allocation, with
a marginal inclination towards videos categorized at a mod-
erate level.
We hope that the aforementioned categorization of the
benchmark will better assist researchers and users in under-
standing the strengths and weaknesses of a method, thus
enabling targeted improvements and fostering rapid devel-
opment in the field.
5. Experiments
5.1. Implementation Details
Stable Diffusion-v1.5 is used as the base T2I model in the
main branch. We use the pre-trained ControlNet [55] for the
structure information guidance. The training dataset com-
bines WebVid-10M [4] and a self-collected private dataset.
We trained the temporal consistency modules and appear-
ance ControlNet towards various types of structural infor-
mation, including line drawings [8], PiDi boundaries [47],
depth maps detected by Midas [39], and human scribbles.
Depth maps are used by default. The control scales are set
as1. For the temporal interpolation model, we train it ex-
clusively on depth maps, employing a smaller control scale
of0.5. This approach is adopted because its requirement
for structural information is comparatively less than that of
other models. During the training process, we first resize the
Input 
Video
Line 
Drawing
PiDi
Boundary
Scribble
Depth
‚ÄúA mechanical tiger is walking.‚ÄùFigure 4. Results under different structural guidance.
< MeinaMix , Pixel Art Style> 
‚ÄúA bear is walking, pixel art style .‚Äù
<ReVAnimated, Moxin> 
‚ÄúA boat is sailing, Chinese traditional ink style .‚Äù
<A-ZovyaRPGArtistTools > 
‚ÄúAn astronaut rides a motorbike, planets in back .‚Äù
Figure 5. Results of video style translation. ‚ü®¬∑‚ü©indicate the per-
sonalized T2I model we used.
shorter side to 384pixels, followed by a random crop to ob-
tain video clips with a size of 384√ó576.17frames at 4fps
are sampled from each video. The batch size is 32and the
learning rate is 3e‚àí5. We train each model for 100K itera-
tions. During inference, we employ the DDIM [45] sampler
with30steps, classifier-free guidance [19] of magnitude 9.
5.2. Applications
Controllable and creative style transfer. In CCEdit, the
controllability and creativity of video style transfer are man-
ifested in various dimensions. Two basic aspects include
the diversity of structural information and the availability
of off-the-shelf personalized models [9, 13]. The former en-
ables users to customize the granularity and type of struc-
tural information retained from the original video, as de-
picted in Fig. 4. The latter allows users to edit the video
into their desired domain, as shown in Fig. 5.
Video editing with precise appearance control. Some-
times, users require stronger control over the content they
want to generate. For example, they may want to change
6717
Edited 
VideoKey
FrameInput 
Video
<Hellomecha , Building Block World> 
‚ÄúA building block style car in the parking lot.‚Äù
<Counterfeit> 
‚ÄúA man hikes on the moon, anime style .‚Äù
<ReVAnimated> 
‚ÄúA paladin in armor rides on motorcycle, on fire .‚Äù
Figure 6. Video editing results with customized center frame as
reference. The first row corresponds to customizing foreground,
the second row corresponds to customizing background, and the
third row is taking given reference image to affect the entire pic-
ture.‚ü®¬∑‚ü©indicate the personalized T2I model we used.
Frame: 1 Frame: 61 Frame: 121Original 
Video
Edited 
Video
Frame: 181 Frame: 241‚ÄúCity at night, in winter .‚Äù
Figure 7. Illustration of long video editing. CCEdit achieves
good consistency across over 240 frames. Zoom in for best view.
only the foreground, alter just the background, or edit the
texture content of a video in a specific way. Therefore,
CCEdit focuses more on precise appearance control by ini-
tially modifying the key frame with image editing tech-
niques and then using it as a reference for the entire video.
As depicted in Fig. 6, we first edit the center frames of the
videos by Stable Diffusion Web UI [2], followed by utiliz-
ing these edited center frames as guides for the video editing
process. Thanks to end-to-end network training, our method
coherently propagates edits from the key frame throughout
the entire video.
Long video editing. A seamless and visually appealing
video typically necessitates a higher frame count and in-
creased frame rate, elements that have been inadequately
addressed by many contemporary video editing methodolo-
gies. CCEdit effectively resolves this through its hierarchi-
cal design for key frames editing, combined with iterative
extension and a tailored temporal interpolation mechanism.
This approach enables the editing of videos comprising up
to hundreds of frames with 24fps (frames per second). An
example is shown in Fig. 7.
Input 
Video
Pix2Video
TokenFlow
CCEdit‚ÄúA person riding a horse over an obstacle, VanGogh style.‚Äù
Figure 8. Qualitative comparison results. Red boxes reveals
TokenFlow‚Äôs inadequate local detail preservation, in contrast to
our method‚Äôs detailed, coherent output. Zoom in for best view.
5.3. State-of-the-Art Comparisons
Datasets. We employ a smaller segment of our proposed
benchmark, designated as mini-BalanceCC . This subset en-
compasses 50videos, each randomly selected from the orig-
inal BalanceCC dataset, ensuring a representative distribu-
tion similar to that of the original collection.
Compared methods. To conduct an exhaustive com-
parison, we have selected eight representative video edit-
ing methodologies: Tune-A-Video [51], vid2vid-zero [49],
Text2Video-zero [23], FateZero [36], Pix2Video [6], Con-
trolVideo [56], Rerender A Video [54], and Token-
Flow [14]. Method details are omitted for brevity, and can
be found in supplementary. Regarding our approach, we
employ depth maps as structure control. For the appear-
ance control, we adopt the off-the-shelf method of PnP-
Diffusion [48] with the same hyper-parameters to automat-
ically edit the center frame of each video clip. To ensure
fairness in comparison, Stable Diffusion-v1.5 is used as the
base model for all methods.
Evaluation metrics. In our preliminary study, we observed
that automatic metrics, such as CLIP-Score [18] to assess
text alignment and frame consistency, do not fully align
with human preferences [28, 52, 57]. We focused on col-
lecting human preferences for a comprehensive user study,
comparing our method against recent state-of-the-art tech-
niques based on mean opinion score (MOS) and direct com-
parisons. We gathered 1,119 scoring results from 33 volun-
teers, each reflecting all indicators for an edited video. For
automatic metric results, refer to the supplementary.
6718
Method Edit Aes. Tem. Ove. Win Tie Lose
Tune-A-Video [51] 3.24 3.01 2.72 2.77 16.4 6.9 76.7
vid2vid-zero [49] 3.00 2.38 2.11 2.35 10.6 4.6 84.8
Text2Video-Zero [23] 2.07 1.43 1.41 1.48 16.5 1.3 86.2
FateZero [36] 2.47 3.16 3.30 2.79 16.6 3.6 79.8
Pix2Video [6] 3.68 2.97 2.80 2.97 29.9 5.2 64.9
ControlVideo [56] 3.01 2.71 2.60 2.66 13.8 5.6 80.6
Rerender A Video [54] 2.40 2.69 2.82 2.50 11.1 0.0 88.9
TokenFlow [14] 3.78 3.61 3.79 3.58 32.4 14.7 52.9
CCEdit (Ours) 4.06 4.00 3.74 3.87 - - -
Table 1. Left: Mean opinion scores (MOS) over different as-
pects of the generated video, including editing accuracy (Edit),
aesthetics (Aes.), temporal consistency (Tem.), and overall impres-
sion (Ove.). Scores range from 1 to 5. Right: Win, Tie, and Lose
percentage in side-by-side comparisons with CCEdit.
Results. As illustrated in Tab. 1, CCEdit excels in both
editing accuracy and aesthetic quality, and is just slightly
inferior to TokenFlow in temporal smoothness. For over-
all impression, our approach achieved a MOS of 3.87 on a
scale from 1 to 5. Among the eight reference methods, To-
kenFlow performed closest to ours, with an overall MOS
of 3.58. The remaining seven methods scored between
1.5 to 3.0 on the MOS scale. As for direct comparisons,
our method outperforms all eight reference schemes signif-
icantly. While TokenFlow remains the closest competitor,
our CCEdit prevails in 52.9% of test cases against it, trails
in 32.4%, and ties in 14.7% of cases.
Furthermore, Fig. 8 presents the qualitative results
of the top three finalists (CCEdit, TokenFlow [14], and
Pix2Video [6]). It shows that Pix2Video struggles to keep
temporal coherence, while TokenFlow demonstrates no-
ticeable blurring. In contrast, our method can accurately
achieve the editing objective while maintaining the tempo-
ral coherence as well as the structure of the input video.
Please see supplementary for more qualitative results.
5.4. Ablation Study
Appearance control. Fig. 9 illustrates the importance of
taking the edited key frame as a reference in certain scenar-
ios. Initially, translating video scenes into ‚Äú cyberpunk ‚Äù
style (1st row) solely through prompt adjustments appears
challenging, as this word is unfamiliar to the pre-trained
T2I model weights and the temporal consistency modules.
Providing a customized center frame allows the network to
smoothly extend its appearance to adjacent frames, creat-
ing a cohesive video. Besides, we replicated the user study
pipeline from Sec. 5.3 to evaluate the effectiveness of ap-
pearance control. The model without appearance control
received a mean opinion score (MOS) of 2.88, significantly
lower than the 3.87 scored by the process of editing one key
frame first and then propagating to surrounding frames.
Anchor prior. Fig. 10 demonstrates the ablation study for
w/o edited 
key frame
w/ edited 
key frame‚ÄúCity at night, in cyberpunk style, with neon lights.‚Äù
Input 
Video
Figure 9. Ablation study on appearance control. In some chal-
lenging cases, appearance control is crucial to achieving the ex-
pected results.
w/o anchor 
prior
w/ anchor 
prior‚ÄúA man wanders in the field, with the Milky Way in the sky.‚ÄùInput
Video
Figure 10. Ablation study on anchor prior. Our proposed anchor
prior helps a lot in stabilizing the appearance across frames. The
red boxes demonstrate the localized flickering in the frames.
our anchor prior. It reveals that the absence of the anchor
prior may lead to regional flickering in the video sequence,
while its presence effectively mitigates this issue.
6. Limitation and Future Works
In our approach, structural control is exerted by explic-
itly extracting the structural representation from the source
video and sustaining it via the structure branch. How-
ever, it may encounter challenges when tasked with sub-
stantial structural alterations-exemplified by the conversion
of a ‚Äúcute rabbit‚Äù into a ‚Äúmajestic tiger.‚Äù Addressing these
complexities will be a primary objective of our future work.
7. Conclusion
This paper presents an innovative trident network archi-
tecture specifically designed for generative video editing.
This unified framework enables precise and controllable
video editing while broadening creative possibilities. To
address the challenges in evaluating generative video edit-
ing approaches, we introduce the meticulously curated Bal-
anceCC benchmark dataset. Our aim is to pave the way
for researchers in the generative video editing domain and
equip practitioners with indispensable tools for their cre-
ative workflows.
6719
References
[1] Chatgpt can now see, hear, and speak. https://openai.
com/blog/chatgpt-can-now-see-hear-and-
speak , 2023. 2, 5
[2] AUTOMATIC1111. Stable Diffusion Web UI, 2022. 7
[3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 18208‚Äì18218, 2022. 2
[4] Max Bain, Arsha Nagrani, G ¬®ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 1728‚Äì1738,
2021. 6
[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22563‚Äì22575, 2023. 2, 4
[6] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra.
Pix2video: Video editing using image diffusion. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 23206‚Äì23217, 2023. 3, 7, 8
[7] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-
video: Text-driven consistency-aware diffusion video edit-
ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 23040‚Äì23050, 2023. 2
[8] Caroline Chan, Fr ¬¥edo Durand, and Phillip Isola. Learning to
generate line drawings that convey geometry and semantics.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 7915‚Äì7925, 2022. 2,
4, 6
[9] Civitai. Civitai. https://civitai.com/ , 2022. 6
[10] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and
Matthieu Cord. Diffedit: Diffusion-based semantic image
editing with mask guidance. In The Eleventh International
Conference on Learning Representations , 2022. 2
[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780‚Äì8794, 2021. 2
[12] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis. Structure
and content-guided video synthesis with diffusion models.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 7346‚Äì7356, 2023. 3
[13] Hugging Face. Hugging face. https://huggingface.
co/, 2022. 6
[14] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.
Tokenflow: Consistent diffusion features for consistent video
editing. arXiv preprint arXiv:2307.10373 , 2023. 3, 7, 8
[15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725 , 2023. 3, 4
[16] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong
Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffu-sion training via min-snr weighting strategy. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 7441‚Äì7451, 2023. 2
[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image
editing with cross-attention control. In The Eleventh Inter-
national Conference on Learning Representations , 2022. 2
[18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
and Yejin Choi. Clipscore: A reference-free evaluation met-
ric for image captioning. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Processing ,
pages 7514‚Äì7528, 2021. 7
[19] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 6
[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840‚Äì6851, 2020. 2, 3
[21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 2
[22] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-
rank adaptation of large language models. In International
Conference on Learning Representations , 2021. 2, 4
[23] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan, Roberto Henschel, Zhangyang Wang, Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-
image diffusion models are zero-shot video generators. arXiv
preprint arXiv:2303.13439 , 2023. 3, 7, 8
[24] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,
and Jiashi Feng. Magicedit: High-fidelity and temporally
coherent video editing. arXiv preprint arXiv:2308.14749 ,
2023. 3
[25] Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao,
Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, and
Mike Zheng Shou. Dynvideo-e: Harnessing dynamic nerf
for large-scale motion-and view-change human-centric video
editing. arXiv preprint arXiv:2310.10624 , 2023. 2
[26] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya
Jia. Video-p2p: Video editing with cross-attention control.
arXiv preprint arXiv:2303.04761 , 2023. 3
[27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. In International Conference on Learning Representa-
tions , 2021. 2
[28] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav
Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid
Hoshen. Dreamix: Video diffusion models are general video
editors. arXiv preprint arXiv:2302.01329 , 2023. 3, 7
[29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In International
Conference on Machine Learning , pages 8162‚Äì8171. PMLR,
2021. 2
6720
[30] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya
Sutskever, and Mark Chen. Glide: Towards photorealis-
tic image generation and editing with text-guided diffusion
models. In International Conference on Machine Learning ,
pages 16784‚Äì16804. PMLR, 2022. 2
[31] OpenAI. Gpt-4v(ision) system card. 2023. 2, 5
[32] OpenAI. Gpt-4v(ision) technical work and authors. https:
//cdn.openai.com/contributions/gpt- 4v.
pdf, 2023.
[33] OpenAI. Gpt-4 technical report, 2023. 2, 5
[34] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-
tao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,
and Yujun Shen. Codef: Content deformation fields for
temporally consistent video processing. arXiv preprint
arXiv:2308.07926 , 2023. 2
[35] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image
translation. In ACM SIGGRAPH 2023 Conference Proceed-
ings, pages 1‚Äì11, 2023. 2
[36] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,
Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-
ing attentions for zero-shot text-based video editing. arXiv
preprint arXiv:2303.09535 , 2023. 2, 3, 7, 8
[37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821‚Äì8831. PMLR, 2021.
2
[38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 , 1
(2):3, 2022. 2
[39] Ren ¬¥e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE transactions on pattern analysis and machine
intelligence , 44(3):1623‚Äì1637, 2020. 2, 4, 6
[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684‚Äì10695, 2022. 2, 4
[41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500‚Äì
22510, 2023. 2, 4
[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479‚Äì36494, 2022. 2
[43] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee,
and Sungroh Yoon. Edit-a-video: Single video editing withobject-aware consistency. arXiv preprint arXiv:2303.07945 ,
2023. 3
[44] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. In The Eleventh International Con-
ference on Learning Representations , 2022. 2, 4
[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations , 2020. 6
[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations , 2020. 2
[47] Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao,
Qi Tian, Matti Pietik ¬®ainen, and Li Liu. Pixel difference
networks for efficient edge detection. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 5117‚Äì5127, 2021. 2, 4, 6
[48] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1921‚Äì1930, 2023. 2, 7
[49] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,
Xinlong Wang, and Chunhua Shen. Zero-shot video editing
using off-the-shelf image diffusion models. arXiv preprint
arXiv:2303.17599 , 2023. 3, 7, 8
[50] Yuanzhi Wang, Yong Li, Xin Liu, Anbo Dai, Antoni Chan,
and Zhen Cui. Edit temporal-consistent videos with image
diffusion model. arXiv preprint arXiv:2308.09091 , 2023. 2
[51] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu
Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning
of image diffusion models for text-to-video generation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7623‚Äì7633, 2023. 3, 7, 8
[52] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jin-
bin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei
Huang, Yuanxi Sun, et al. Cvpr 2023 text guided video edit-
ing competition. arXiv preprint arXiv:2310.16003 , 2023. 5,
7
[53] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang
Jiang. Simda: Simple diffusion adapter for efficient video
generation. arXiv preprint arXiv:2308.09710 , 2023. 2, 3
[54] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change
Loy. Rerender a video: Zero-shot text-guided video-to-video
translation. arXiv preprint arXiv:2306.07954 , 2023. 2, 3, 7,
8
[55] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models. arXiv preprint
arXiv:2302.05543 , 2023. 2, 3, 4, 6
[56] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng
Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo:
Training-free controllable text-to-video generation. arXiv
preprint arXiv:2305.13077 , 2023. 3, 7, 8
6721
[57] Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han,
Tiande Guo, and Luoqi Liu. Towards consistent video edit-
ing with text-to-image diffusion models. arXiv preprint
arXiv:2305.17431 , 2023. 7
[58] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and
Jun Zhu. Controlvideo: Adding conditional control for one
shot text-to-video editing. arXiv preprint arXiv:2305.17098 ,
2023. 2, 3
6722
