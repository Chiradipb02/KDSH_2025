Mitigating Noisy Correspondence by Geometrical Structure Consistency
Learning
Zihua Zhao1, Mengxi Chen1, Tianjie Dai1,2, Jiangchao Yao1,2B, Bo Han3,
Ya Zhang1,2, Yanfeng Wang1,2B
1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University,
2Shanghai Artificial Intelligence Laboratory,3Hong Kong Baptist University
{sjtuszzh, mxchen mc, elfenreigen, Sunarker, ya zhang, wangyanfeng }@sjtu.edu.cn ,
bhanml@comp.hkbu.edu.hk
Abstract
Noisy correspondence that refers to mismatches in
cross-modal data pairs, is prevalent on human-annotated
or web-crawled datasets. Prior approaches to leverage
such data mainly consider the application of uni-modal
noisy label learning without amending the impact on both
cross-modal and intra-modal geometrical structures in mul-
timodal learning. Actually, we find that both structures
are effective to discriminate noisy correspondence through
structural differences when being well-established. Inspired
by this observation, we introduce a Geometrical Struc-
ture Consistency (GSC) method to infer the true correspon-
dence. Specifically, GSC ensures the preservation of geo-
metrical structures within and between modalities, allowing
for the accurate discrimination of noisy samples based on
structural differences. Utilizing these inferred true corre-
spondence labels, GSC refines the learning of geometrical
structures by filtering out the noisy samples. Experiments
across four cross-modal datasets confirm that GSC effec-
tively identifies noisy samples and significantly outperforms
the current leading methods. Source code is available at:
https://github.com/MediaBrain-SJTU/GSC .
1. Introduction
Cross-modal retrieval [21, 27, 28] that focuses on query-
ing the most relevant samples across modalities, has gar-
nered considerable interest in multimodal scenarios [3, 36].
Most current methods presuppose large quantity of well-
annotated data is available. However, real-world datasets [4,
16, 34] which own non-expert annotation or collected by
web crawling are prone to noisy correspondence. Such dis-
crepancy between modalities can cause severe degradation
to retrieval models if without proper handling [15, 31].
Recently, learning with noisy correspondence has gath-
Figure 1. Noisy correspondence impacts both cross-modal and
intra-modal geometrical structures. Left: Cross-modal distance
between mismatched text and image is initially distant but wrongly
reduced. Right: Intra-modal structures of mismatched image
(above) and text (below) are initially distinct but wrongly aligned,
thus similar samples within the modality are pulled apart.
ered increasing attention. The majority of these efforts
share a common target of accurately learning the true soft
correspondence labels that can reliably indicate the match-
ing degree between data pairs. For example, NCR [15] pi-
oneered this area by employing a co-teaching approach to
classify samples with higher losses as noisy. Such method,
which has been further refined by [10, 37] by introducing
meta-learning and leveraging clean data subsets, however
fundamentally remains a variation of the uni-modal sample
selection philosophy [23]. They may not be very effective
to identify the accurate correspondence and finally overfit
on noise in face of the intricacies of multimodal learning.
Noisy correspondence affects multimodal representa-
tions in a more complicated way than uni-modal noisy label
problem. As illustrated in Fig. 1, in the perspective of the
cross-modal geometrical structure that refers to the similari-
ties between representations across modalities, the presence
of noisy correspondence can disrupt this structure by er-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27381
roneously reducing the distance between mismatched data
pairs. On the other hand, in the perspective of the intra-
modal geometrical structure that refers to the similarities
within the modality, as different samples exhibit asymmet-
ric intra-modal structures, attempting to align the distinct
structures across mismatched modalities can lead to intra-
model collapse. Fortunately, we have observed significant
differences between clean and noisy samples within well-
established cross-modal and intra-modal geometrical struc-
tures where clean samples tend to show better alignments,
as detailed in Sec. 3.2. These differences conversely offer
a promising strategy for discriminating noisy samples and
accurately predicting true correspondence labels.
Inspired by the distinct structural characteristics, we
introduce the Geometrical Structure Consistency (GSC)
method to mitigate the issue of noisy correspondence.
Specifically, GSC maintains the integrity of geometrical
structures by optimizing a contrastive loss that aligns the
intra-modal geometrical structures along with the tradi-
tional loss for cross-modal alignment. Benefiting from the
memorization effect of DNNs [1, 35], geometrical struc-
ture can be well-established in the early stage. During this
phase, GSC assesses the true soft correspondence labels
based on the differences in geometrical structure within and
across modalities. For the cross-modal aspect, GSC identi-
fies potential noise by recognizing data pairs with low cross-
modal similarities, while in the case of intra-modal aspect,
the similarity between the queried sample and other sam-
ples are calculated to determine the intra-modal geometrical
structure, and data pairs with inconsistent structures across
modalities are considered as noisy. These assessed labels
are then used to clarify the learning of the geometrical struc-
ture, creating a positive feedback loop. To conclude, the
contributions of this work are summarized as follows:
• We identify the impact of noisy correspondence on both
cross-modal and intra-modal geometrical structures, and
find the significant difference between clean samples and
noisy samples within a well-established overall structure.
• We introduce the novel Geometrical Structure Consis-
tency (GSC) approach, which utilizes the structural differ-
ences to accurately predict true correspondence labels and
counteract the adverse effects of noisy correspondences.
• Our proposed GSC method is compatible with existing
approaches for handling noisy correspondences. Through
extensive experiments on four benchmark datasets, we
have proven the consistent superiority of GSC over cur-
rent state-of-the-art methods.
2. Related Works
2.1. Cross-modal Retrieval
Cross-modal retrieval [7, 14, 26, 30], which focuses on
using information from one modality to query the most rele-vant data in other modalities, has been a key area of research
in multi-modal learning [2, 8, 13, 33]. Due to the restriction
of large-scale annotated multi-modal corpus [18, 20, 29],
the unsupervised guided framework that directly aligns rep-
resentations across modalities has always been the main-
stream. SGRAF [6] introduces Graph Neural Network to
establish graph correspondences and an attention mecha-
nism to select the most representative alignments, enhanc-
ing the precision of cross-modal similarity assessments.
However, these cross-modal retrieval methods highly rely
on well-aligned data while amending the existence of noisy
correspondence.
2.2. Noisy Correspondence Learning
Noisy correspondence (NC) is first aroused in [15],
which is a novel paradigm in the field of noise learn-
ing [12, 38–40], referring to the mismatched pairs within
the multi-modal dataset. To tackle this problem, NCR [15]
utilizes DivideMix [23] to distinguish clean pairs from
noisy ones and rectify correspondence based on the mem-
orization effect of DNNs. Yang et al. [37] further im-
proves NCR by switching to Beta Mixture Model and es-
timates soft correspondence labels by sample-wise compar-
ison. Han et al. [10] proposes a meta-similarity correction
network that reinterprets binary classification of correspon-
dence as a meta-process, enhancing the process of data pu-
rification. Despite NCR-based models, other attempts like
robust loss based methods have also been undertaken. Qin
et al. [31] combines the idea of evidential learning with NC
and puts forward a confidence-based method. Chuang et al.
[5] introduces one effective robust symmetric contrastive
loss [11, 42, 43]. Although these models have showcased
promising performances, they only cast their spotlight on
interactions across modalities, which is insufficient in uti-
lizing the semantic-abundant cross-modal data, further mo-
tivating us to take not only cross-modal but also intra-modal
together into consideration to help mitigate NC.
3. Proposed Method
3.1. Preliminary
We start by defining notations for cross-modal retrieval
in the presence of noisy correspondences, employing the
widely studied image-text retrieval task as a generalized ex-
emplar. Consider a multimodal dataset D={Ii, Ti, yi}N
i=1,
where each {Ii, Ti}represents the i-th image-text pair, stan-
dard retrieval models project these data pairs into a shared
representation space using separate encoders ffor images
andgfor texts. Then similarity scores between the rep-
resentations are computed through cosine similarity or an
inference model, denoting as S(f(I), g(T)), or⟨I, T⟩in
brief. The associated label yiindicates whether the pair is
positively correlated ( yi= 1) or not ( yi= 0). Note that
27382
these labels may contain noise, as pairs in the dataset are
often presumed to be matched.
3.2. Geometrical Structure Consistency Learning
Motivation. The core concept of GSC is to preserve the
consistency of geometrical structures and distinguish sam-
ples with noisy correspondence through structural differ-
ences. Initially, we demonstrate these differences through
a straightforward experiment. As shown in Fig. 2, a re-
trieval model is optimized on a clean dataset to maintain
consistent cross-modal and intra-modal structures, which
is then assessed on a dataset with simulated noise. Dur-
ing the experiment, significant discrepancy between clean
and noisy samples can be observed in both cross-modal
and intra-modal structures. Specifically, for cross-modal
structure, clean samples typically possess higher similarity
scores than those noisy ones, exhibiting a disparity in distri-
bution. For intra-modal structure, the calculated similarities
between intra-modal structures of clean and noisy samples
manifest a bimodal distribution with most values of noisy
samples lower than 0.5 threshold, suggesting noisy samples
tend to have asymmetric intra-modal structures.
Geometrical Structure Consistency. Here, we give def-
initions to both cross-modal and intra-modal geometrical
structures and the corresponding training objectives. From
the cross-modal aspect, the geometrical structure is defined
by the similarities between representations across different
modalities. Considering an example of a given query image
Ii, the cross-modal geometrical structure can be represented
asGi
CM={⟨Ii, Tj⟩}N
j=1. GSC preserves the consistency
of this structure by minimizing the expected risk for cross-
modal objective, as expressed in the following equation,
RLCM(f, g) = min
f,gE(I,T,y )∼D[LCM(⟨I, T⟩, y)] (1)
where LCMis the cross-modal loss function, typically a
contrastive or triplet loss in line with conventional retrieval
models. The goal is to align the cross-modal representations
according to the correspondence label y, thus the similarity
between matching data pairs can be maximized, contrasting
to other data pairs.
From the intra-modal aspect, the geometrical structure
refers to the similarities within the modality. The intra-
model structure for the i-th sample then denotes as Gi
IM=
{⟨Ii, Ij⟩,⟨Ti, Tj⟩}N
j=1, where ⟨Ii, Ij⟩and⟨Ti, Tj⟩represent
the pairwise similarities among images and texts, respec-
tively. To uphold the consistency of such structure, GSC
incorporates the intra-modal objective as following,
RLIM(f, g) = min
f,gE(I,T,y )∼D[LIM(⟨I, I⟩,⟨T, T⟩, y)](2)
In this equation, LIMdenotes the intra-modal loss func-
tion. The intra-modal objective ensures that the intra-modal
Figure 2. Geometrical Structure Consistency helps discriminate
samples with noisy correspondence. The model is first trained
on clean Flickr30K dataset, then evaluated on the same dataset
with 40% simulated noise. Left: Calculated cross-modal simi-
larity scores of both clean and noisy samples. Right: Calculated
intra-modal similarity scores of both clean and noisy samples.
structures of matching samples are constrained to be similar
across modalities. Notably, optimizing without maintaining
intra-modal structure is considered sub-optimal for incon-
sistent reasoning [9, 17]. Thus the introduction of intra-
modal structure consistency can also directly benefit mul-
timodal representation learning. As illustrated in Fig. 3(a),
GSC simultaneously optimizes both objectives to establish
stable cross-modal and intra-modal structures.
3.3. Noise Discrimination & Purification
Deep neural networks typically exhibit the memoriza-
tion effect that tends to initially learn the clean patterns
within the dataset before over-fitting on noise. Leveraging
this, GSC is able to learn a well-established structure in the
early stage, which can be further utilized to predict accurate
correspondence indicator.
Cross-modal Discrimination. As illustrated in Fig. 3(b),
based on the well-established cross-modal structure in the
early stage, clean data pairs are expected to exhibit more
closely aligned cross-modal representations compared to
noisy pairs. GSC leverages this structural discrepancy and
introduces a function to signify a cross-modal bidirectional
correspondence indicator.
yi
CM=1
2exp (
Ii, Ti
/τ1)
PN
j=1exp (
Ii, Tj
/τ1)+exp (
Ii, Ti
/τ1)
PN
j=1exp (
Ij, Ti
/τ1)
(3)
where τ1is the temperature coefficient. Take the former
half as an example, it measures the proportion of similarity
between the current data pairs IiandTiagainst the sum-
mation of similarity between the Iiand all text data. For
a clean sample, the similarity between current IiandTi
should dominate the proportion, thus the value of indica-
tor should approach 1. Conversely, for a noisy sample, the
similarity between current IiandTiwould be close to 0,
resulting in the indicator’s value trending toward 0.
Intra-modal Discrimination. For the intra-modal as-
pect, as illustrated in Fig. 3(c), matched image-text pairs
27383
Figure 3. An overview of GSC. Left: The framework of GSC. GSC first extracts image and text representations through separate encoders,
then simultaneously optimizes cross-modal and intra-modal objectives to preserve geometrical structure consistency. GSC leverages both
structures to discriminate noisy samples and estimate the true correspondence indicator y, which can be further utilized to purify the overall
learning. Right: GSC discriminates noisy samples by structural differences from both cross-modal and intra-modal aspects.
should have similar intra-modal structures that mirror each
other, whereas mismatched pairs would possess distinct
structures that reflect their divergent positions. Specif-
ically, we employ cosine similarity to measure the re-
semblance between intra-modal structures as Si
IM=
cos({⟨Ii, Ij⟩,⟨Ti, Tj⟩}N
j=1). During experiments, the co-
sine similarity scores of clean samples are observed con-
sistently higher than those of noisy samples during experi-
ments, presenting a bimodal distribution of scores across the
dataset (demonstrated in Fig. 5(b)). This distribution can be
accurately modeled using a two-component Gaussian Mix-
ture Model (GMM) [23], described by the equation:
p(SIM) =KX
k=1αkϕ(SIM|k), yi
IM=αklϕ(Si
IM|kl)PK
k=1αkϕ(Si
IM|k)(4)
Specifically, αkdenotes the k-th coefficient and ϕ(sIM|k)
is the probability density for that component. The sec-
ond equation is the estimation of the intra-modal correspon-
dence indicator yi
IM. It calculates the probability of an ob-
served sample belonging to the cleaner component, denoted
byk=kl. This probability approaches 1 for clean samples
and 0 for noisy samples, thereby enabling the distinction of
samples with noisy correspondence.
So far, we have estimated the true correspondence la-
bels,yCMandyIM, by leveraging the structural differences in
both cross-modal and intra-modal contexts. Our objective
is to optimally utilize these two labels to surmount the re-
spective challenges and accurately identify all samples with
noisy correspondence. Therefore, we define the final corre-
spondence label for each sample as the minimum of the two
labels, which can expressed as below,
yi= min {yi
CM, yi
IM} (5)Noise purification. We address the issue of noisy corre-
spondence by refining both cross-modal and intra-modal
objectives. Since the estimated label is a soft label with
values in the range of [0, 1] which can directly reflect the
degree of true correspondence, we can seamlessly apply it
to the loss functions on a sample-wise basis. For the cross-
modal objective, we choose the widely-applied contrastive
loss as the loss function. The purified cross-modal loss can
be denoted as follows,
LCM=−1
2NNX
i=1yilogexp (
Ii, Ti
/τ1)
PN
j=1exp (
Ii, Tj
/τ1)
−1
2NNX
j=1yjlogexp (
Ij, Tj
/τ1)
PN
i=1exp (
Ii, Tj
/τ1)(6)
where yis directly applied before the sample-wise loss. For
the intra-modal side, in addition to sample-wise purification
for the intra-modal loss, it is crucial to prevent the impact
of noisy correspondences from distorting the calculation of
the geometrical structure. Specifically, the distances from
the queried sample to those samples with noisy correspon-
dences should be excluded. The purified intra-model loss
denotes as,
LIM=−1
NNX
i=1logexp(PN
k=1yk
Ii, Ik
yk
Ti, Tk
/τ2)
PN
j=1exp (PN
k=1yk
Ii, Ik
yk
Tj, Tk
/τ2)
(7)
where τ2is also a temperature coefficient and ykis mul-
tiplied to both ⟨Ii, Ik⟩and⟨Ti, Tk⟩to precisely filter out
the noise. Furthermore, noisy correspondences can also in-
terfere with the discrimination of noisy samples during the
computation of cosine similarity scores for intra-modal dis-
27384
Algorithm 1 Pipeline of learning with our GSC method.
Input: Multi-modal dataset D={Ii, Ti, yi}N
i=1
1Initialize parameters for networks AandBseparately
2foreach epoch t= 1,2, . . . , T do
3 fornetwork k=A, B do
4 foreach minibatch BfromDdo
5 Compute modality representations: f(I),g(T)
6 Estimate cross-modal indicator yCM[t]kby Eq. 3
7 Estimate intra-modal indicator yIM[t]kby Eq. 4
8 Calaulate cross-modal loss by Eq. 6
9 Calculate intra-modal loss by Eq. 7
10 Train Net kby optimizing the combination of
two losses using Eq. 9
11 Update yCM[t]andyIM[t]by Eq. 10
12 Update the final y[t]for the other network.
Output: Refined networks Net A, Net B
crimination. Similar modifications are employed as follows,
Si
IM=PN
j=1yj⟨Ii, Ij⟩yj⟨Ti, Tj⟩qPN
j=1(yj⟨Ii, Ij⟩)2qPN
j=1(yj⟨Ti, Tj⟩)2(8)
where yjis similarly multiplied. Considering the high com-
putational cost for the entire dataset, we employ a Monte
Carlo sampling approach to relax size Nto size Bof a
mini-batch. The overall loss function of GSC is formulated
as a weighted sum of two loss functions, as expressed in the
following equation,
L=LCM+γLIM (9)
where γis the hyper-parameter keeping the balance be-
tween two losses to reach the best optimization.
3.4. Training Schedule
To integrate the estimation of true correspondence labels
with the enhancement of cross-modal retrieval learning, we
adopt the temporal ensembling technique, drawing inspira-
tion from Liu et al. [25], to iteratively update the estimated
correspondence labels. Specifically, both yCMandyIMare
updated through a momentum-based combination of the es-
timates from the current epoch tand the previous epoch t−1
before taking minimum as shown below,
yi
CM[t] =β1yi
CM[t] + (1−β1)yi
CM[t−1],
yi
IM[t] =β2yi
IM[t] + (1−β2)yi
IM[t−1](10)
where β1andβ2are separate momentum. The estimation
of true correspondence labels can be further improved by
utilizing two separate neural networks, where the true cor-
respondence labels for each network are computed from
the output of the other network. The ablation in Section 4shows that both strategies can significantly improve the per-
formance. In conclusion, the overall procedure of our Ge-
ometrical Structure Consistency (GSC) method is depicted
in the Algorithm 1.
3.5. Discussion
The improvement of our proposed GSC mainly comes
from preserving the geometrical structure consistency and
better optimizing strategies, which is compatible with most
existing methods. In terms of computational complex-
ity, GSC does not introduce additional computational costs
when integrated with a backbone that calculates similar-
ity scores directly from representations, while it necessi-
tates two extra forward passes when the backbone com-
putes similarity scores using a similarity module. This
requirement is significantly less demanding compared to
MSCN [10], which involves computations for an additional
meta-learning model, or BiCro [37], which compares each
sample against a clean subset. While RINCE [5] shares
similar computational cost as GSC, the robust loss func-
tion without explicitly excluding noisy samples underper-
forms at higher noise levels. Furthermore, methods based
on NCR framework, i.e.NCR [15], MSCN and BiCro, rely
on an inseparable dual-network structure, while GSC is ef-
fective with a single model. This attribute makes GSC more
adaptable to larger models.
4. Experiment
4.1. Datasets and Evaluation Metrics
Datasets. Following the experimental settings and dataset
splits in Huang et al. [15], four widely-used image-text re-
trieval datasets are introduced to evaluate our method:
• Flickr30K [41] contains 31,000 images with five captions
each, collected from the Flickr website. We assign 1,000
image-text pairs for validation, 1,000 image-text pairs for
testing and the rest for training.
• MS-COCO [24] includes 123,287 images with five cap-
tions each. We assign 5,000 image-text pairs for valida-
tion, 5,000 image-text pairs for testing and the rest for
training. MS-COCO can be either evaluated using whole
5,000 test set or average of 5-fold 1,000 test sets [19].
• NUS-WIDE [4] includes 269,648 image-text pairs col-
lected from Flickr, in which we randomly sample 1,000
image-text pairs for validation, 1,000 image-text pairs for
testing and 100,000 image-text pairs for training.
• Conceptual Captions [34] is a large-scale dataset automat-
ically harvested from the Internet, therefore about 3%-
20% image-text pairs in the dataset are mismatched or
weakly-matched [34]. We use a subset named CC152K
in our experiments, in which we assign 1,000 image-text
pairs for validation, 1,000 image-text pairs for testing and
150,000 image-text pairs for training.
27385
Table 1. The retrieval performance on Flickr30K and MS-COCO datasets under 20%, 40% and 60% noise rates separately. The best results
and the second best results are respectively marked by bold and underline .
Flickr30K MS-COCO
Noise Methods Image→Text Text →Image Image→Text Text →Image
R@1 R@5 R@10 R@1 R@5 R@10 Sum R@1 R@5 R@10 R@1 R@5 R@10 Sum
SGR 55.9 81.5 88.9 40.2 66.8 75.3 408.6 25.7 58.8 75.1 23.5 58.9 75.1 317.1
SGRAF 72.8 90.8 95.4 56.4 82.1 88.6 486.1 75.4 95.2 97.9 60.1 88.5 94.8 511.9
NCR-SGR 73.5 93.2 96.6 56.9 82.4 88.5 491.1 76.6 95.6 98.2 60.8 88.8 95.0 515.0
20% DECL-SGRAF 77.5 93.8 97.0 56.1 81.8 88.5 494.7 77.5 95.9 98.4 61.7 89.3 95.4 518.2
RINCE-SGR 72.1 92.2 95.7 54.9 79.8 85.3 480.0 73.8 95.6 98.5 61.7 89.2 94.7 513.5
MSCN-SGR 77.4 94.9 97.6 59.6 83.2 89.2 501.9 78.1 97.2 98.8 64.3 90.4 95.8 524.6
BiCro-SGRAF 78.1 94.4 97.5 60.4 84.4 89.9 504.7 78.8 96.1 98.6 63.7 90.3 95.7 523.2
GSC-SGR 78.3 94.6 97.8 60.1 84.5 90.5 505.8 79.5 96.4 98.9 64.4 90.6 95.9 525.7
SGR 4.1 16.6 24.1 4.1 13.2 19.7 81.8 1.3 3.7 6.3 0.5 2.5 4.1 18.4
SGRAF 8.3 18.1 31.4 5.3 16.7 21.3 101.1 15.8 23.4 54.6 17.8 43.6 54.1 209.3
NCR-SGR 68.1 89.6 94.8 51.4 78.4 84.8 467.1 74.7 94.6 98.0 59.6 88.1 94.7 509.7
40% DECL-SGRAF 72.7 92.3 95.4 53.4 79.4 86.4 479.6 75.6 95.5 98.3 59.5 88.3 94.8 512.0
RINCE-SGR 71.2 90.7 95.6 52.7 78.5 85.6 474.3 71.2 95.8 97.9 59.1 88.6 94.3 506.9
MSCN-SGR 71.6 92.8 96.2 54.8 80.7 87.4 483.5 75.3 95.4 98.2 60.3 88.6 94.8 512.6
BiCro-SGRAF 74.6 92.7 96.2 55.5 81.1 87.4 487.5 77.0 95.9 98.3 61.8 89.2 94.9 517.1
GSC-SGR 76.5 94.1 97.6 57.5 82.7 88.9 497.3 78.2 95.9 98.2 62.5 89.7 95.4 519.9
SGR 1.5 6.6 9.6 0.3 2.3 4.2 24.5 0.1 0.6 1.0 0.1 0.5 1.1 3.4
SGRAF 2.3 5.8 10.9 1.9 6.1 8.2 35.2 0.2 3.6 7.9 1.5 5.9 12.6 31.7
NCR-SGR 13.9 37.7 50.5 11.0 30.1 41.4 184.6 0.1 0.3 0.4 0.1 0.5 1.0 2.4
60% DECL-SGRAF 65.2 88.4 94.0 46.8 74.0 82.2 450.6 73.0 94.2 97.9 57.0 86.6 93.8 502.5
RINCE-SGR 64.5 86.8 92.9 46.5 72.8 79.7 443.2 72.3 94.0 97.9 58.4 86.6 92.5 501.7
MSCN-SGR 68.8 90.3 94.4 50.8 77.4 84.4 466.1 72.5 93.6 97.1 57.7 87.0 93.9 501.8
BiCro-SGRAF 67.6 90.8 94.4 51.2 77.6 84.7 466.3 73.9 94.4 97.8 58.3 87.2 93.9 505.5
GSC-SGR 70.8 91.1 95.9 53.6 79.8 86.8 478.0 75.6 95.1 98.0 60.0 88.3 94.6 511.7
Table 2. Experimental results on NUS-WIDE dataset. Only results
of best SOTAs are presented. Best results are marked by bold .
Noise Method Image→Text Text →Image
R@1 R@5 R@10 R@1 R@5 R@10 Sum
cleanSGR 39.3 66.5 74.0 37.1 64.2 73.6 354.7
NCR 40.0 67.3 74.6 38.9 65.9 74.3 361.0
GSC 42.9 65.9 74.0 39.8 66.0 74.5 363.1
20%SGR 33.4 60.7 69.8 31.1 58.7 68.2 321.9
DECL 36.1 63.0 72.5 36.4 60.6 72.0 340.6
GSC 37.9 65.2 73.4 38.3 63.8 73.1 351.7
40%SGR 30.8 57.9 67.3 28.4 55.9 65.5 305.8
DECL 34.1 59.5 68.8 34.5 60.0 69.1 326.0
GSC 37.0 63.5 71.3 36.8 62.4 71.1 342.1
60%SGR 25.0 50.2 59.8 25.0 49.3 60.3 269.6
BiCro 27.8 53.4 63.2 27.9 52.2 64.1 288.6
GSC 31.6 57.4 66.1 31.1 57.8 66.3 310.3
Evaluation Protocol. We evaluate the retrieval perfor-
mance with the recall rate at K (R@K) metric. In a nutshell,
R@K measures the proportion of relevant items retrieved
within the top K items closest to the query. In our experi-
ments, we take image and text as queries, respectively, and
report R@1, R@5, R@10 results and their sum for a com-
prehensive evaluation.4.2. Implementation Details
For all experiments, we apply the Adam optimizer [22]
with the initial learning rate of 2×10−4which decays by
0.2in15epochs. We train the model on one NVIDIA A100
GPU and select the model that performs best on the valida-
tion set for testing. The dimension of the common represen-
tation is set to 1024 . For experiments besides ablation study,
we set the batch size Bas128. The two temperature coef-
ficients τ1andτ2are set to 0.07and1in default. The hy-
perparameter λserving as the balancing ratio between LCM
andLIMis set to 0.01, and the two momentum β1andβ2
are set to 0.7respectively.
4.3. Comparison with the State-of-the-Arts
We conducted extensive evaluations against seven con-
temporary state-of-the-art methods on four benchmark
datasets to validate the effectiveness of our proposed GSC
model. These comparisons include two baseline models,
i.e., SGR and SGRAF [6], and five robust learning methods
designed to handle noisy correspondences, i.e., NCR [15],
DECL [31], RINCE [5], MSCN [10] and BiCro [37]. No-
tably, both DECL and BiCro are built upon a stronger
SGRAF backbone, which is an ensemble of SGR and SAF.
To thoroughly assess the robustness of GSC, we simulate
27386
Table 3. The retrieval performance on CC152K dataset. The
best results and the second best results are respectively marked
bybold and underline .
CC152K
Methods Image→Text Text →Image
R@1 R@5 R@10 R@1 R@5 R@10 Sum
SGR 11.3 29.7 39.6 13.1 30.1 41.6 165.4
SGRAF 32.5 59.5 70.0 32.5 60.7 68.7 323.9
NCR-SGR 39.5 64.5 73.5 40.3 64.6 73.2 355.6
DECL-SGRAF 39.0 66.1 75.5 40.7 66.3 76.7 364.3
RINCE-SGR 35.9 63.0 73.8 37.6 65.0 73.4 348.7
MSCN-SGR 40.1 65.7 76.6 40.6 67.4 76.3 366.7
BiCro-SGRAF 40.8 67.2 76.1 42.1 67.6 76.4 370.2
GSC-SGR 42.1 68.4 77.7 42.2 67.6 77.1 375.1Table 4. Comparison with CLIP and NCR on MS-COCO 5K.
CLIP-L and CLIP-B are abbreviations for CLIP (ViT-L/14) and
CLIP (ViT-B/32). The best results are marked by bold .
Noise MethodsImage→Text Text →Image
R@1 R@5 R@10 R@1 R@5 R@10 Sum
0% CLIP-L 58.4 81.5 88.1 37.8 62.4 72.2 400.4
Zeroshot CLIP-B 50.2 74.6 83.6 30.4 56.0 66.8 361.6
20% CLIP-B 21.4 49.6 63.3 14.8 37.6 49.6 236.3
Finetune NCR 56.9 83.6 91.0 40.6 69.8 80.1 422.0
GSC 58.9 84.9 91.7 42.0 71.4 81.8 430.8
50% CLIP-B 10.9 27.8 38.3 7.8 19.5 26.8 131.1
Finetune NCR 53.1 80.7 88.5 37.9 66.6 77.8 404.6
GSC 55.5 81.8 90.1 40.0 69.1 79.7 416.3
various levels of noisy correspondences, namely 20%, 40%,
and 60%, by randomly shuffling the captions on MSCOCO
and Flickr30K like [15]. In addition, we extend our ex-
periments to real-world noisy conditions using the CC152K
dataset. Comprehensive comparison results are detailed in
the supplementary material for fully demonstration of GSC.
Results on Flickr30K, MS-COCO and NUS-WIDE. To
evaluate the robustness of all methods under different ex-
tents of noise, we quantify the noise rate to 20%, 40% and
60% on well-annotated datasets, as recorded in Tab. 1 and
Tab. 2. The results demonstrate that GSC significantly out-
performs established noisy correspondence methods such
as NCR, DECL, RINCE, MSCN, and BiCro, achieving an
average increase in recall sum score of 7.5% on Flickr30K,
3.4% on MS-COCO and 12.75% on NUS-WIDE to the sec-
ond best results, which indicates the better robustness of
GSC. Notably, GSC also excels over BiCro and DECL un-
der various conditions, even though they are implemented
on the enhanced SGRAF backbone. Moreover, GSC can
carry about more enhancement at higher noise rates, espe-
cially under 60% noise level, proving that our method re-
mains stable and reliable even in severely noisy conditions.
Results on CC152K. To further validate GSC in handling
with noisy correspondence in real-world scenarios, we addi-
tionally conduct tests on CC152K dataset, detailed in Tab. 3.
According to the results, GSC achieves the best perfor-
mance with an overall score of 375.1%, surpassing the sec-
ond best method BiCro by 4.9%. Moreover, GSC brings
about a larger gain of 209.7% to its backbone SGR, which is
significantly higher than the improvement of 46.3% brought
about by BiCro to its backbone SGRAF. The results affirm
GSC’s capability to manage not only simply simulated but
also complex, real-world noisy correspondences.
Comparison to pre-trained model. In line with Huang
et al. [15], we compare GSC to the pre-trained CLIP
model [32] on the MS-COCO dataset. CLIP is a well-
known large pre-trained model trained on a massive 400
million image-text pair dataset harvested from the Internet,Table 5. Ablation study on Flicker30K with 40% noise with dif-
ferent components in GSC. The best results are marked by bold .
Momen. Dual LIMImage→Text Text →Image
R@1 R@5 R@10 R@1 R@5 R@10 Sum
✓ ✓ 72.3 92.8 96.5 56.5 82.1 88.8 489.0
✓ ✓ 73.0 91.5 95.9 54.5 80.4 87.6 482.9
✓ ✓ 74.5 92.5 96.9 57.0 82.0 88.3 491.1
✓ ✓ ✓ 76.5 94.1 97.6 57.5 82.7 88.9 497.3
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000014 /uni00000013/uni00000011/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000017/uni0000001c/uni00000013/uni00000017/uni0000001c/uni00000015/uni00000017/uni0000001c/uni00000017/uni00000017/uni0000001c/uni00000019/uni00000017/uni0000001c/uni0000001b/uni00000018/uni00000013/uni00000013/uni00000036/uni00000058/uni00000050/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000046/uni00000044/uni0000004f/uni0000004f
/uni00000013/uni00000011/uni00000016 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni0000001a /uni00000013/uni00000011/uni0000001b /uni00000013/uni00000011/uni0000001c/uni00000017/uni0000001c/uni00000015/uni00000017/uni0000001c/uni00000017/uni00000017/uni0000001c/uni00000019/uni00000017/uni0000001c/uni0000001b/uni00000018/uni00000013/uni00000013/uni00000036/uni00000058/uni00000050/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni00000046/uni00000044/uni0000004f/uni0000004f
1
2
Figure 4. Analysis of different hyper-parameter combinations on
Flicker30K with 40% noise. Left: γis the balancing parameter
between LCMandLIM.Right: β1andβ2are separate momentums
for the cross-modal and intra-modal temporal ensembling.
which can inevitably include samples with noisy correspon-
dence. Here, we report the zero-shot and fine-tuning perfor-
mances of different CLIP models, together with NCR. Re-
sults indicate a notable performance decline in CLIP mod-
els when fine-tuning with 20% and 50% noise levels. On
the contrary, GSC not only withstands but excels over zero-
shot CLIP under 50% noise, emphasizing the importance of
addressing data mismatches and the robustness of GSC.
4.4. Experimental Analysis
Ablation study. We show the effect of each component of
GSC in Tab. 5. The ablation studies are conducted without
temporal ensembling (Momen. in the table), dual networks
or intra-modal learning separately. Specifically for the ab-
lation study for temporal ensembling, we use a 5-epoch
warm-up stage to replace the technique. According to the
27387
Figure 5. (a) The changing values of clean and noisy sample weight when the noise rate is 20%, 40%, and 60%. (b) Distribution of
intra-modal geometrical similarity, including PDFs of clean and noisy pair similarities and estimated Gaussian distribution components.
(c) Cross-modal weight distributions of GSC on clean and noisy pairs. (d) Intra-modal weight distributions of GSC on clean and noisy
pairs. Experiments from (b) to (d) are conducted on Flickr30K with the noise rate of 0.4.
Table 6. Analysis of different batch sizes on Flicker30K with 40%
noise. The best results are marked by bold .
Batch SizeImage→Text Text →Image
R@1 R@5 R@10 R@1 R@5 R@10 Sum
32 73.6 92.1 95.4 54.4 81.0 87.4 483.9
64 75.3 94.0 96.7 55.9 81.7 88.2 491.8
128 76.5 94.1 97.6 57.5 82.7 88.9 497.3
192 76.7 94.0 97.7 57.6 82.7 89.1 497.8
results, all components are important to achieve advanta-
geous results. Notably, the performance of GSC with sin-
gle model still outperforms most robust methods, including
methods with dual networks like NCR and DECL, which
further proves the effectiveness and high efficiency.
Impacts of hyper-parameters γ,β1andβ2.The GSC
method incorporates three main hyper-parameters includ-
ingγ,β1andβ2with their effects detailed in Fig. 4. γ
strikes a balance between cross-modal and intra-modal op-
timization. According to the results, GSC shows stability
forγ∈[0.005,0.1], while γ= 0.01is chosen for optimal
performance. β1andβ2are the momentum coefficients en-
suring steady updates for temporal ensembling. The results
indicate stable performance with parameter values higher
than 0.5. Lower beta values may laten timely updates, po-
tentially causing the model to overfit on noise.
Impact of batch size. We also explore the model perfor-
mance under different batch sizes during training. As shown
in Tab. 6, as the batch size increases, the retrieval perfor-
mance of the model steadily improves. Specifically, when
the batch size is increased from 32 to 128, there is a signifi-
cant enhancement from 483.9% to 497.3% as to the sum of
recall, and further expanding the batch size from 128 to 192
results in only marginal growth, which indicates that larger
batch size helps in consolidating the stableness of model
structure until reaching a proper point.
Experimental visualization. To further offer insights ofGSC against noisy correspondence, we visually present the
value curves and weight distributions of predicted corre-
spondence labels in Fig. 5. Figure (a) shows the stability
of predicted labels across varying noise levels, with mini-
mal fluctuation for clean samples and consistently low val-
ues for noisy ones. Figure (b) depicts a bimodal distribution
of intra-modal cosine similarities, which can be well-fitted
by a two-component GMM. Figures (c) and (d) confirm that
both cross-modal and intra-modal structures effectively dis-
tinguish noisy samples, with discrimination accuracy of ap-
proximately 0.96 for cross-modal and about 0.91 for intra-
modal, culminating in an overall accuracy of about 0.98.
Such visualization explains the reason for the steady and
reliable performance of GSC under different noise rates.
5. Conclusion
In this paper, we propose Geometrical Structure Con-
sistency (GSC) learning framework to mitigate the problem
brought by noisy correspondence. Specifically, we identify
the impact of noisy correspondence on both cross-modal
and intra-modal geometrical structures. Leveraging the
structural differences between noisy and clean pairs within
a well-established structure, our approach infers accurate
correspondence labels for each data pair. The inferred la-
bels are further utilized to refine the consistent learning of
structures. GSC can seamlessly integrate with most exist-
ing retrieval methods. Extensive experiments across vari-
ous cross-modal benchmark datasets showcase the robust-
ness and effectiveness of our proposed GSC method across
diverse settings.
Acknowledgement. This work is supported by the National
Key R&D Program of China (No.2022ZD0160702), STCSM
(No.22511106101, No.22511105700, No.21DZ1100100), 111
plan (No.BP0719010) and National Natural Science Foundation
of China (No.62306178).
27388
References
[1] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David
Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan
Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio,
et al. A closer look at memorization in deep networks. In
International conference on machine learning , pages 233–
242. PMLR, 2017. 2
[2] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir
Zamir. Multimae: Multi-modal multi-task masked autoen-
coders. In European Conference on Computer Vision , pages
348–367. Springer, 2022. 2
[3] Tadas Baltru ˇsaitis, Chaitanya Ahuja, and Louis-Philippe
Morency. Multimodal machine learning: A survey and tax-
onomy. IEEE transactions on pattern analysis and machine
intelligence , 41(2):423–443, 2018. 1
[4] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhip-
ing Luo, and Yantao Zheng. Nus-wide: a real-world web im-
age database from national university of singapore. In Pro-
ceedings of the ACM international conference on image and
video retrieval , pages 1–9, 2009. 1, 5
[5] Ching-Yao Chuang, R Devon Hjelm, Xin Wang, Vibhav Vi-
neet, Neel Joshi, Antonio Torralba, Stefanie Jegelka, and
Yale Song. Robust contrastive learning against noisy views.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 16670–16681, 2022.
2, 5, 6
[6] Haiwen Diao, Ying Zhang, Lin Ma, and Huchuan Lu. Sim-
ilarity reasoning and filtration for image-text matching. In
Proceedings of the AAAI conference on artificial intelli-
gence , pages 1218–1226, 2021. 2, 6
[7] Zheren Fu, Zhendong Mao, Yan Song, and Yongdong Zhang.
Learning semantic relationship among instances for image-
text matching. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 15159–
15168, 2023. 2
[8] Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang,
Ming-Hsuan Yang, Hartwig Adam, Laurent Itti, Balaji Lak-
shminarayanan, and Jiaping Zhao. Improving zero-shot gen-
eralization and robustness of multi-modal models. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 11093–11101, 2023. 2
[9] Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan Rossi,
Vishwa Vinay, and Aditya Grover. Cyclip: Cyclic contrastive
language-image pretraining. Advances in Neural Informa-
tion Processing Systems , 35:6704–6719, 2022. 3
[10] Haochen Han, Kaiyao Miao, Qinghua Zheng, and Minnan
Luo. Noisy correspondence learning with meta similarity
correction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 7517–
7526, 2023. 1, 2, 5, 6
[11] Feng Hong, Jiangchao Yao, Yueming Lyu, Zhihan Zhou, Ivor
Tsang, Ya Zhang, and Yanfeng Wang. On harmonizing im-
plicit subpopulations. In The Twelfth International Confer-
ence on Learning Representations , 2023. 2
[12] Peng Hu, Xi Peng, Hongyuan Zhu, Liangli Zhen, and Jie
Lin. Learning cross-modal retrieval with noisy labels. InProceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 5403–5413, 2021. 2
[13] Yu Huang, Junyang Lin, Chang Zhou, Hongxia Yang,
and Longbo Huang. Modality competition: What makes
joint training of multi-modal network fail in deep learn-
ing?(provably). In International Conference on Machine
Learning , pages 9226–9259. PMLR, 2022. 2
[14] Yan Huang, Yuming Wang, Yunan Zeng, and Liang Wang.
Mack: multimodal aligned conceptual knowledge for un-
paired image-text matching. Advances in Neural Information
Processing Systems , 35:7892–7904, 2022. 2
[15] Zhenyu Huang, Guocheng Niu, Xiao Liu, Wenbiao Ding,
Xinyan Xiao, Hua Wu, and Xi Peng. Learning with noisy
correspondence for cross-modal matching. Advances in Neu-
ral Information Processing Systems , 34:29406–29419, 2021.
1, 2, 5, 6, 7
[16] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
conference on machine learning , pages 4904–4916. PMLR,
2021. 1
[17] Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing
Ping, Son Dinh Tran, Yi Xu, Belinda Zeng, and Trishul
Chilimbi. Understanding and constructing latent modality
structures in multi-modal representation learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 7661–7671, 2023. 3
[18] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz,
Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying
Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-
identified publicly available database of chest radiographs
with free-text reports. Scientific data , 6(1):317, 2019. 2
[19] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-
ments for generating image descriptions. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 3128–3137, 2015. 5
[20] Douwe Kiela, Edouard Grave, Armand Joulin, and Tomas
Mikolov. Efficient large-scale multi-modal classification.
InProceedings of the AAAI conference on artificial intelli-
gence , 2018. 2
[21] Jae Myung Kim, A Koepke, Cordelia Schmid, and Zeynep
Akata. Exposing and mitigating spurious correlations for
cross-modal retrieval. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
2584–2594, 2023. 1
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[23] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix:
Learning with noisy labels as semi-supervised learning.
arXiv preprint arXiv:2002.07394 , 2020. 1, 2, 4
[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 5
27389
[25] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Car-
los Fernandez-Granda. Early-learning regularization pre-
vents memorization of noisy labels, 2020. 5
[26] Yang Liu, Hong Liu, Huaqiu Wang, and Mengyuan Liu.
Regularizing visual semantic embedding with contrastive
learning for image-text matching. IEEE Signal Processing
Letters , 29:1332–1336, 2022. 2
[27] Yaxin Liu, Jianlong Wu, Leigang Qu, Tian Gan, Jianhua
Yin, and Liqiang Nie. Self-supervised correlation learning
for cross-modal retrieval. IEEE Transactions on Multime-
dia, 2022. 1
[28] Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu,
and Ji-Rong Wen. Cots: Collaborative two-stream vision-
language pre-training model for cross-modal retrieval. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 15692–15701, 2022. 1
[29] Javier Marin, Aritro Biswas, Ferda Ofli, Nicholas Hynes,
Amaia Salvador, Yusuf Aytar, Ingmar Weber, and Antonio
Torralba. Recipe1m+: A dataset for learning cross-modal
embeddings for cooking recipes and food images. IEEE
transactions on pattern analysis and machine intelligence ,
43(1):187–203, 2019. 2
[30] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 19275–19284, 2023.
2
[31] Yang Qin, Dezhong Peng, Xi Peng, Xu Wang, and Peng
Hu. Deep evidential learning with noisy correspondence
for cross-modal retrieval. In Proceedings of the 30th ACM
International Conference on Multimedia , pages 4948–4956,
2022. 1, 2, 6
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 7
[33] Christian Schlarmann and Matthias Hein. On the adversarial
robustness of multi-modal foundation models. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 3677–3685, 2023. 2
[34] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages
2556–2565, 2018. 1, 5
[35] Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan
Wang, Zongyuan Ge, and Yi Chang. Robust early-learning:
Hindering the memorization of noisy labels. In International
conference on learning representations , 2020. 2
[36] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal
learning with transformers: A survey. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 2023. 1
[37] Shuo Yang, Zhaopan Xu, Kai Wang, Yang You, Hongxun
Yao, Tongliang Liu, and Min Xu. Bicro: Noisy correspon-
dence rectification for multi-modality data via bi-directionalcross-modal similarity consistency. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 19883–19892, 2023. 1, 2, 5, 6
[38] Jiangchao Yao, Jiajie Wang, Ivor W Tsang, Ya Zhang, Jun
Sun, Chengqi Zhang, and Rui Zhang. Deep learning from
noisy image labels with quality embedding. IEEE Transac-
tions on Image Processing , 28(4):1909–1922, 2018. 2
[39] Jiangchao Yao, Hao Wu, Ya Zhang, Ivor W Tsang, and Jun
Sun. Safeguarded dynamic label regression for noisy super-
vision. In Proceedings of the AAAI Conference on Artificial
Intelligence , pages 9103–9110, 2019.
[40] Jiangchao Yao, Bo Han, Zhihan Zhou, Ya Zhang, and Ivor W
Tsang. Latent class-conditional noise model. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , 2023.
2
[41] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-
maier. From image descriptions to visual denotations: New
similarity metrics for semantic inference over event descrip-
tions. Transactions of the Association for Computational
Linguistics , 2:67–78, 2014. 5
[42] Zhihan Zhou, Jiangchao Yao, Yan-Feng Wang, Bo Han, and
Ya Zhang. Contrastive learning with boosted memorization.
InInternational Conference on Machine Learning , pages
27367–27377. PMLR, 2022. 2
[43] Zhihan Zhou, Jiangchao Yao, Feng Hong, Ya Zhang, Bo
Han, and Yanfeng Wang. Combating representation learn-
ing disparity with geometric harmonization. In Advances
in Neural Information Processing Systems , pages 20394–
20408. Curran Associates, Inc., 2023. 2
27390
