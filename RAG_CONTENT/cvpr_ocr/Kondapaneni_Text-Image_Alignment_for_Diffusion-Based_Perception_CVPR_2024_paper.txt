Text-image Alignment for Diffusion-based Perception
Neehar Kondapaneni1*Markus Marks1⇤Manuel Knott1,2⇤
Rogerio Guimaraes1Pietro Perona1
1California Institute of Technology
2ETH Zurich, Swiss Data Science Center, Empa
Abstract
Diffusion models are generative models with impressive
text-to-image synthesis capabilities and have spurred a new
wave of creative methods for classical machine learning
tasks. However, the best way to harness the perceptual
knowledge of these generative models for visual tasks is
still an open question. Speciﬁcally, it is unclear how to
use the prompting interface when applying diffusion back-
bones to vision tasks. We ﬁnd that automatically gener-
ated captions can improve text-image alignment and sig-
niﬁcantly enhance a model’s cross-attention maps, lead-
ing to better perceptual performance. Our approach im-
proves upon the current state-of-the-art (SOTA) in diffusion-
based semantic segmentation on ADE20K and the cur-
rent overall SOTA for depth estimation on NYUv2. Fur-
thermore, our method generalizes to the cross-domain set-
ting. We use model personalization and caption mod-
iﬁcations to align our model to the target domain and
ﬁnd improvements over unaligned baselines. Our cross-
domain object detection model, trained on Pascal VOC,
achieves SOTA results on Watercolor2K. Our cross-domain
segmentation method, trained on Cityscapes, achieves
SOTA results on Dark Zurich-val and Nighttime Driving.
Project page: vision.caltech.edu/TADP/ Code
page: github.com/damaggu/TADP
1. Introduction
Diffusion models have set the state-of-the-art (SOTA) for
image generation [ 32,35,38,52]. Recently, a few works
have shown diffusion pre-trained backbones have a strong
prior for scene understanding that allows them to perform
well in advanced discriminative vision tasks, such as se-
mantic segmentation [ 17,53], monocular depth estimation
[53], and keypoint estimation [ 28,43]. We refer to these
works as diffusion-based perception methods. Unlike con-
trastive vision language models (e.g., CLIP) [ 22,26,31],
*Equal contribution.“a dog and a bird”
”in a watercolor style”CaptionerCaption Modifier
+
Single-domain
Cross-domainDepth EstimationSegmentationObject DetectionDiffusion-Pretrained Vision ModelCLIP
Figure 1. Text-Aligned Diffusion Perception (TADP). In TADP,
image captions align the text prompts and images passed to
diffusion-based vision models. In cross-domain tasks, target do-
main information is incorporated into the prompt to boost perfor-
mance.
generative models have a causal relationship with text, in
which text guides image generation. In latent diffusion
models, text prompts control the denoising U-Net [ 36],
moving the image latent in a semantically meaningful di-
rection [ 5].
We explore this relationship and ﬁnd that text-image
alignment signiﬁcantly improves the performance of
diffusion-based perception. We then investigate text-target
domain alignment in cross-domain vision tasks, ﬁnding that
aligning to the target domain while training on the source
domain can improve a model’s target domain performance
(Fig. 1).
We ﬁrst study prompting for diffusion-based perceptual
models and ﬁnd that increasing text-image alignment im-
proves semantic segmentation and depth estimation perfor-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
13883
mance. We ﬁnd that unaligned text prompts can introduce
semantic shifts to the feature maps of the diffusion model
[5] and that these shifts can make it more difﬁcult for the
task-speciﬁc head to solve the target task. Speciﬁcally,
we ask whether unaligned text prompts, such as averag-
ing class-speciﬁc sentence embeddings ([ 31,53]), hinder
performance by interfering with feature maps through the
cross-attention mechanism. Through ablation experiments
on Pascal VOC2012 segmentation [ 14] and ADE20K [ 55],
we ﬁnd that off-target and missing class names degrade im-
age segmentation quality. We show automated image cap-
tioning [ 25] achieves sufﬁcient text-image alignment for
perception. Our approach (along with latent representation
scaling, see Sec. 4.1) improves performance for semantic
segmentation on Pascal and ADE20k by 4.0 mIoU and 1.7
mIoU, respectively, and depth estimation on NYUv2 [ 42]
by 0.2 RMSE (+8% relative) setting the new SOTA.
Next, we focus on cross-domain adaptation: can ap-
propriate image captioning help visual perception when
the model is trained in one domain and tested on a dif-
ferent domain? Training models on the source domain
with the appropriate prompting strategy leads to excellent
unsupervised cross-domain performance on several bench-
marks. We evaluate our cross-domain method on Pascal
VOC [ 13,14] to Watercolor2k (W2K) and Comic2k (C2K)
[21] for object detection and Cityscapes (CS) [ 9] to Dark
Zurich (DZ) [ 39] and Nighttime (ND) Driving [ 10] for se-
mantic segmentation. We explore varying degrees of text-
target domain alignment and ﬁnd that improved alignment
results in better performance. We also demonstrate using
two diffusion personalization methods, Textual Inversion
[16] and DreamBooth [ 37], for better target domain align-
ment and performance. We ﬁnd that diffusion pre-training
is sufﬁcient to achieve SOTA (+5.8 mIoU on CS !DZ, +4.0
mIoU on CS !ND, +0.7 mIoU on VOC !W2k) or near
SOTA results on all cross-domain datasets with no text-
target domain alignment, and including our best text-target
domain alignment method further improves +1.4 AP on Wa-
tercolor2k, +2.1 AP on Comic2k, and +3.3 mIoU on Night-
time Driving.
Overall, our contributions are as follows:
•We propose a new method using automated caption
generation that signiﬁcantly improves performance on
several diffusion-based vision tasks through increased
text-image alignment.
•We systematically study how prompting affects
diffusion-based vision performance, elucidating the
impact of class presence, grammar in the prompt, and
previously used average embeddings.
•We demonstrate that diffusion-based perception effec-
tively generalizes across domains, with text-target do-
main alignment improving performance, which can be
further boosted by model personalization.2. Related Work
2.1. Diffusion models for single-domain vision tasks
Diffusion models are trained to reverse a step-wise forward
noising process. Once trained, they can generate highly re-
alistic images from pure noise [ 32,35,38,52]. To con-
trol image generation, diffusion models are trained with text
prompts/captions that guide the diffusion process. These
prompts are passed through a text encoder to generate text
embeddings that are incorporated into the reverse diffusion
process via cross-attention layers.
Recently, some works have explored using diffusion
models for discriminative vision tasks. This can be done
by either utilizing the diffusion model as a backbone for
the task [ 17,28,43,53] or through ﬁne-tuning the diffu-
sion model for a speciﬁc task and then using it to generate
synthetic data for a downstream model [ 2,50]. We use the
diffusion model as a backbone for downstream vision tasks.
VPD [ 53] encodes images into latent representations and
passes them through one step of the Stable Diffusion model.
The cross-attention maps, multi-scale features, and output
latent code are concatenated and passed to a task-speciﬁc
head. Text prompts inﬂuence all these maps through the
cross-attention mechanism, which guides the reverse dif-
fusion process. The cross-attention maps are incorporated
into the multi-scale feature maps and the output latent rep-
resentation. The text guides the diffusion process and can
accordingly shift the latent representation in semantic di-
rections [ 1,5,16,18]. The details of how VPD uses the
prompting interface are described in Sec. 3. In short, VPD
uses unaligned text prompts. In our work, we show how
aligning the text to the image by using a captioner can sig-
niﬁcantly improve semantic segmentation and depth esti-
mation performance.
2.2. Image captioning
CLIP [ 31] introduced a novel learning paradigm to align
images with their captions. Shortly after, the LAION-5B
dataset [ 41] was released with 5B image-text pairs; this
dataset was used to train Stable Diffusion. We hypothe-
size that text-image alignment is important for diffusion-
pretrained vision models. However, images used in ad-
vanced vision tasks (like segmentation and depth estima-
tion) are not naturally paired with text captions. To obtain
image-aligned captions, we use BLIP-2 [ 25], a model that
inverts the CLIP latent space to generate captions for novel
images.
2.3. Diffusion models for cross-domain vision tasks
A few works explore the cross-domain setting with diffu-
sion models [ 2,17]. Benigmim et al. [ 2] use a diffusion
model to generate data for a downstream unsupervised do-
main adaptation (UDA) architecture. In [ 17], the diffusion
13884
backbone is frozen, and the segmentation head is trained
with a consistency loss with category and scene prompts
guiding the latent code towards target cross-domains. Sim-
ilar to VPD, the category prompts consist of token embed-
dings for all classes present in the dataset, irrespective of
their presence in any speciﬁc image. The consistency loss
forces the model to predict the same output mask for all the
different scene prompts, helping the segmentation head be-
come invariant to the scene type. Instead of using a consis-
tency loss, we train the diffusion model backbone and task
head on the source domain data with and without incorpo-
rating the style of the target domain in the caption. We ﬁnd
that better alignment with the target domain (i.e., target do-
main information included in the prompt) results in better
cross-domain performance.
2.4. Cross-domain object detection
Cross-domain object detection can be divided into multi-
ple subcategories, depending on what data / labels are at
train / test time available. Unsupervised domain adaptation
objection detection (UDAOD) tries to improve detection
performance by training on unlabeled target domain data
with approaches such as self-training [ 11,44], adversarial
distribution alignment [ 54] or generating pseudo labels for
self-training [ 23]. Cross-domain weakly supervised object
detection (CDWSOD) assumes the availability of image-
level annotations at training time and utilizes pseudo label-
ing [21,30], alignment [ 51] or correspondence mining [ 19].
Recently, [ 46] used CLIP [ 31] for Single Domain General-
ization, which aims to generalize from a single domain to
multiple unseen target domains. Our text-based method de-
ﬁnes a new category of cross-domain object detection that
tries to adapt from a single source to an unseen target do-
main by only having the broad semantic context of the target
domain (e.g., foggy/night/comic/watercolor) as text input to
our method. When we incorporate model personalization,
our method can be considered a UDAOD method since we
train a token based on unlabeled images from the target do-
main.
3. Methods
Stable Diffusion [ 35].The text-to-image Stable Diffusion
model is composed of four networks: an encoder E, a condi-
tional denoising autoencoder (a U-Net in Stable Diffusion)
✏✓, a language encoder ⌧✓(the CLIP text encoder in Stable
Diffusion), and a decoder D.EandDare trained before
✏✓, such that D(E(x)) = ˜ x⇡x. Training ✏✓is composed
of a pre-deﬁned forward process and a learned reverse pro-
cess. The reverse process is learned using LAION-400M
[40], a dataset of 400 million images ( x2X) and captions
(y2Y). In the forward process, an image xis encoded
into a latent z0=E(x), and tsteps of a forward noise pro-
cess are executed to generate a noised latent zt. Then, tolearn the reverse process, the latent ztis passed to the de-
noising autoencoder ✏✓, along with the time-step tand the
image caption’s representation C=⌧✓(y).⌧✓adds infor-
mation about yto✏✓using a cross-attention mechanism, in
which the query is derived from the image, and the key and
value are transformations of the caption representation. The
model ✏✓is trained to predict the noise added to the latent
in step tof the forward process:
LLDM:=EE(x),y,✏⇠N(0,1),th
k✏ ✏✓(zt,t ,⌧ ✓(y))k2
2i
,(1)
where t2{0,. . . ,T }. During generation, a pure noise la-
tentzTand a user-speciﬁed prompt are passed through the
denoising autoencoder ✏✓forTsteps and decoded D(z0)to
generate an image guided by the text prompt.
Diffusion for Feature Extraction. Diffusion backbones
have been used for downstream vision tasks in several re-
cent works [ 17,28,43,53]. Due to its public availabil-
ity and performance in perception tasks, we use a modi-
ﬁed version (see Sec. 4.1) of the feature extraction method
in VPD. An image latent z0=E(x)and a conditioning
Care passed through the last step of the denoising process
✏✓(z0,0,C). The cross-attention maps Aand the multi-scale
feature maps Fof the U-Net are concatenated V=A F
and passed to a task-speciﬁc head Hto generate a predic-
tion ˆp=H(V). The backbone ✏✓and head Hare trained
with a task-speciﬁc loss LH(ˆp, p).
Average EOS Tokens. To generate C, previous meth-
ods [ 17,53] rely on a method from CLIP [ 31] to use aver-
aged text embeddings as representations for the classes in a
dataset. A list of 80 sentence templates for each class of in-
terest (such as “a <adjective >photo of a <class name >”)
are passed through the CLIP text encoder. We use Bto de-
note the set of class names in a dataset. For a speciﬁc class
(b2B), the CLIP text encoder returns an 80⇥N⇥D
tensor, where N is the maximum number of tokens over all
the templates, and D is 768 (the dimension of each token
embedding). Shorter sentences are padded with EOS to-
kens to ﬁll out the maximum number of tokens. The ﬁrst
EOS token from each sentence template is averaged and
used as the representative embedding for the class such that
C2R|B|⇥768. This method is used in [ 17,53], we denote
it as Cavgand use it as a baseline. For semantic segmen-
tation, all of the class embeddings, irrespective of presence
in the image, are passed to the cross-attention layers. Only
the class embedding of the room type is passed to the cross-
attention layers for depth estimation.
3.1. Text-Aligned Diffusion Perception (TADP)
Our work proposes a novel method for prompting diffusion-
pretrained perception models. Speciﬁcally, we explore dif-
ferent prompting methods Gto generate C. In the single-
domain setting, we show the effectiveness of a method
13885
Denoising U-NetDepth EstimationDomain adaptation
Text-image alignment
Text-domain alignmentMulti-scale Feature Maps
EOS class token[dog, bird, car, airplane, ...]BLIP“a photo of a dog and a parrot”Oracle“dog bird”Textual Inversion / DreamBooth“in a <token> style”simple“in a watercolor style”null“ ”Cross-attentionCLIPPromptImage EncoderTask-speciﬁcDecoderSemanticSegmentationObjectDetectionLS
Figure 2. Overview of TADP. We test several prompting strategies and evaluate their impact on downstream vision task performance. Our
method concatenates the cross-attention and multi-scale feature maps before passing them to the vision-speciﬁc decoder. In the blue box,
we show three single-domain captioning strategies with differing levels of text-image alignment. We propose using BLIP [ 25] captioning
to improve image-text alignment. We extend our analysis to the cross-domain setting (yellow box), exploring whether aligning the source
domain text captions to the target domain may impact model performance by appending caption modiﬁers to image captions generated in
the source domain and ﬁnd model personalization modiﬁers (Textual Inversion/Dreambooth) work best.
that uses BLIP-2 [ 25], an image captioning algorithm,
to generate a caption as the conditioning for the model:
G(x)=˜ y!C. We then extend our method to the cross-
domain setting by incorporating target domain information
toC=C+M(P)s, where Mis a caption modiﬁer that
takes target domain information Pas input and outputs
a caption modiﬁcation M(P)sand a model modiﬁcation
M(P)✏✓. In Sec. 4, we analyze the text-image interface
of the diffusion model by varying the captioner Gand cap-
tion modiﬁer Min a systematic manner for three differ-
ent vision tasks: semantic segmentation, object detection,
and monocular depth estimation. Our method and experi-
ments are presented in Fig. 2. Following [ 53], we train our
ADE20k segmentation and NYUv2 depth estimation mod-
els with fast and regular schedules. On ADE20k, we train
using 4k steps (fast), 8k steps (fast), and 80k steps (normal).
For NYUv2 depth, we train on a 1-epoch (fast) schedule and
a 25-epoch (normal) schedule. For implementation details,
refer to Appendix D.
4. Results
4.1. Latent scaling
Before exploring image-text alignment, we apply latent
scaling to encoded images (Appendix G of Rombach et al.
[35]). This normalizes the image latents to have a standard
normal distribution. The scaling factor is ﬁxed at 0.18215 .
We ﬁnd that latent scaling improves performance using Cavg
for segmentation and depth estimation (Fig. 3). Speciﬁcally,
latent scaling improves ⇠0.8% mIoU on Pascal, ⇠0.3%
mIoU on ADE20K, and a relative ⇠5.5% RMSE on NYUv2
Depth (Fig. 3).Method Avg TA LS G OT mIoUss
VPD(R) [ 53] XX X 82.34
VPD(LS) XXX X 83.06
Class Embs XX 82.72
Class Names XX 84.08
TADP-0 XX 86.36
TADP-20 XX 86.19
TADP-40 XX 87.11
TADP(NO)-20 X 86.35
TADP-Oracle X 89.85
Table 1. Prompting for Pascal VOC2012 Segmentation. We
report the single-scale validation mIoU for Pascal experiments.
(R): Reproduction of VPD, Avg: EOS token averaging, LS: La-
tent Scaling, G: Grammar, OT: Off-target information. For our
method, we indicate the minimum length of the BLIP caption with
TADP- Xand nouns only with (NO).
4.2. Single-domain alignment
Average EOS Tokens . We scrutinize the use of average
EOS tokens for C(see Sec. 3). While average EOS tokens
are sensible when measuring cosine similarities in the CLIP
latent space, it is unsuitable in diffusion models, where the
text guides the diffusion process through cross-attention. In
our qualitative analysis, we ﬁnd that average EOS tokens
degrade the cross-attention maps (Fig. 4). Instead, we ex-
plore using CLIP to embed each class name independently
and use the tokens corresponding to the actual word (not
the EOS token) and pass this as input to the cross-attention
layer:
GClassEmbs (B)=concat (CLIP (b)|b2B)!C ClassEmbs (2)
13886
Figure 3. Effects of Latent Scaling (LS) and BLIP caption min-
imum length. We report mIoU for Pascal, mIoU for ADE20K,
and RMSE for NYUv2 depth (right). (Top) Latent scaling im-
proves performance on Pascal ⇠0.8mIoU (higher is better), ⇠0.3
mIoU, and ⇠5.5%relative RMSE (lower is better). (Bottom)
We see a similar effect for BLIP minimum token length, with
longer captions performing better, improving ⇠0.8mIoU on Pas-
cal,⇠0.9mIoU on ADE20K, and ⇠0.6%relative RMSE.
Second, we explore a generic prompt, a string of class
names separated by spaces:
GClassNames (B)={‘’+b|b2B }!C ClassNames (3)
These prompts are similar to the ones used for averaged
EOS tokens Cavgw.r.t. overall text-image alignment but
instead use the token corresponding to the word represent-
ing the class name. We evaluate these variations on Pascal
VOC2012 segmentation. We ﬁnd that CClassNames improves
performance by 1.0 mIoU, but CClassEmbs reduces perfor-
mance by 0.3 mIoU (see Tab. 1). We perform more in-depth
analyses of the effect of text-image alignment on the dif-
fusion model’s cross-attention maps and image generation
properties in Appendix A.
TADP . To align the diffusion model text input to the im-
age, we use BLIP-2 [ 25] to generate captions for every im-
age in our single-domain datasets (Pascal, ADE20K, and
NYUv2).
GTADP(x)=BLIP-2 (x)!C TADP(x) (4)
BLIP-2 is trained to produce image-aligned text captions
and is designed around the CLIP latent space. How-
ever, other vision-language algorithms that produce cap-
tions could also be used. We ﬁnd that these text captions
improve performance in all datasets and tasks (Tabs. 1,2,
3). Performance improves on Pascal segmentation by ⇠4%
mIoU, ADE20K by ⇠1.4% mIoU, and NYUv2 Depth by a
relative RMSE improvement of 4%. We see stronger effects
on the fast schedules for ADE20K with an improvement of
⇠5 mIoU at (4k), ⇠2.4 mIoU (8K). On NYUv2 Depth, we
see a smaller gain on the fast schedule ⇠2.4%. All numbers
are reported relative to VPD with latent scaling.Method #Params FLOPs Crop mIoUssmIoUms
self-supervised pre-training
EV A [ 15] 1.01B - 896261.2 61.5
InternImage-L [ 48] 256M 2526G 640253.9 54.1
InternImage-H [ 48] 1.31B 4635G 896262.5 62.9
multi-modal pre-training
CLIP-ViT-B [ 33] 105M 1043G 640250.6 51.3
ViT-Adapter [ 8] 571M - 896261.2 61.5
BEiT-3 [ 49] 1.01B - 896262.0 62.8
ONE-PEACE [ 47] 1.52B - 896262.0 63.0
diffusion-based pre-training
VPD A32[53] 862M 891G 512253.7 54.6
VPD(R) 862M 891G 512253.1 54.2
VPD(LS) 862M 891G 512253.7 54.4
TADP-40 (Ours) 862M 2168G 512254.8 55.9
TADP-Oracle 862M - 512272.0 -
Table 2. Semantic segmentation with different methods
for ADE20k. Our method (green) achieves SOTA within the
diffusion-pretrained models category. The results of our oracle in-
dicate the potential of diffusion-based models for future research
as it is signiﬁcantly higher than the overall SOTA (highlighted in
yellow). See Tab. 1for a notation key and Tab. S1for fast schedule
results.
Method RMSE #  1"  2"  3" REL #log10 #
default schedule
SwinV2-L [ 27] 0.287 0.949 0.994 0.999 0.083 0.035
AiT [ 29] 0.275 0.954 0.994 0.999 0.076 0.033
ZoeDepth [ 3] 0.270 0.955 0.995 0.999 0.075 0.032
VPD [ 53] 0.254 0.964 0.995 0.999 0.069 0.030
VPD(R) 0.248 0.965 0.995 0.999 0.068 0.029
VPD(LS) 0.235 0.971 0.996 0.999 0.064 0.028
TADP-40 0.225 0.976 0.997 0.999 0.062 0.027
fast schedule, 1 epoch
VPD 0.349 0.909 0.989 0.998 0.098 0.043
VPD(R) 0.340 0.910 0.987 0.997 0.100 0.042
VPD(LS) 0.332 0.926 0.992 0.998 0.097 0.041
TADP-0 0.328 0.935 0.993 0.999 0.082 0.038
Table 3. Depth estimation in NYUv2. We ﬁnd latent scaling
accounts for a relative gain of ⇠5.5%on the RMSE metric. Ad-
ditionally, image-text alignment improves ⇠4%relative on the
RMSE metric. A minimum caption length of 40 tokens performs
the best.We also explore adding a text-adapter (TA) to TADP, but
ﬁnd no signiﬁcant gain. See Table 1for a notation key.
We perform some ablations to analyze what aspects of
the captions are important. We explore the minimum token
number hyperparameter for BLIP-2 to explore if longer cap-
tions can produce more useful feature maps for the down-
stream task. We try a minimum token number of 0, 20, and
40 tokens (denoted as CTADP-N ) and ﬁnd small but consis-
tent gains with longer captions, resulting on average 0.75%
relative gain for 40 tokens vs. 0 tokens (Fig. 3). Next, we
ablate the Pascal CTADP-20 captions to understand what in the
13887
OracleClassNames
background
bird
dogBLIP
a
dog
and
a
bird
airplane
bicycle
bird
boat
bottle
dog
background
airplane
bicycle
bird
boat
bottle
dogAvg. EOST okenFigure 4. Cross-attention maps for different types of prompting (before training). We compare the cross-attention maps for four
types of prompting: oracle, BLIP, Average EOS tokens, and class names as space-separated strings. The cross-attention maps for different
heads at all different scales are upsampled to 64x64 and averaged. When comparing Average Template EOS and Class Names, we see
(qualitatively) averaging degrades the quality of the cross-attention maps. Furthermore, we ﬁnd that class names that are not present in the
image can have highly localized attention maps (e.g., ‘bottle’). Further analysis of the cross-attention maps is available in Sec. A, where
we explore image-to-image generation, copy-paste image modiﬁcations, and more.
caption is necessary for the performance gains we observe.
We use NLTK [ 4] to ﬁlter for the nouns in the captions.
In the CTADP(NO)-20 nouns-only caption setting, we achieve
86.4% mIoU, similar to 86.2% mIoU with CTADP-20 (Tab. 1),
suggesting nouns are sufﬁcient.
Oracle . This insight about nouns leads us to ask if an
oracle caption, in which all the object class names in an
image are provided as a caption, can improve performance
further. We deﬁne B(x)as the set of class names present in
image x.
GOracle (x)={‘’+b|b2B(x)}!C Oracle (x) (5)
While this is not a realistic setting, it serves as an approx-
imate upper bound on performance for our method on the
segmentation task. We ﬁnd a large improvement in per-
formance in segmentation, achieving 89% mIoU on Pascal
and 72.2% mIoU on ADE20K. For depth estimation, multi-
class segmentation masks are only provided for a smaller
subset of the images, so we cannot generate a comparable
oracle. We perform ablations on the oracle captions to eval-
uate the model’s sensitivity to alignment. For ADE20K,
on the 4k iteration schedule, we modify the oracle captionsby randomly adding and removing classes such that the re-
call and precision are at 0.5, 0.75, and 1.0 (independently)
(Tab. S2). We ﬁnd that both precision and recall have an
effect, but recall is signiﬁcantly more important. When re-
call is lower (0.50), improving precision has minimal im-
pact ( <1% mIoU). However, precision has progressively
larger impacts as recall increases to 0.75 and 1.00 ( ⇠3%
mIoU and ⇠7% mIoU). In contrast, recall has large impacts
at every precision level: 0.5 - ( ⇠6% mIoU), 0.75 - ( ⇠9%
mIoU), and 1.00 - ( ⇠13% mIoU). BLIP-2 captioning per-
forms similarly to a precision of 1.00 and a recall of 0.5
(Tab. 2). Additional analyses w.r.t. precision, recall, and
object sizes can be found in Appendix B.
4.3. Cross-domain alignment
Next, we ask if text-image alignment can beneﬁt cross-
domain tasks. In cross-domain, we train a model on a
source domain and test it on a different target domain. There
are two aspects of alignment in the cross-domain setting:
the ﬁrst is also present in single-domain, which is image-
text alignment; the second is unique to the cross-domain
setting, which is text-target domain alignment. The second
13888
MethodDark Zurich-val ND
mIoU mIoU
DAFormer [ 20] – 54.1
Reﬁgn-DAFormer [ 7] – 56.8
PTDiffSeg [ 17] 37.0 –
TADP null 42.8 57.5
TADP simple 39.1 56.9
TADP TextualInversion 41.4 60.8
TADP DreamBooth 38.9 60.4
TADP NearbyDomain 41.9 56.9
TADP UnrelatedDomain 42.3 55.1
Table 4. Cross-domain semantic segmentation. Cityscapes
(CD) to Dark Zurich (DZ) val and Nighttime Driving (ND). We
report the mIoU. Our method sets a new SOTA for DarkZurich
and Nighttime Driving.
is challenging because there is a large domain shift between
the source and target domain. Our intuition is that while
the model has no information on the target domain from the
training images, an appropriate text prompt may carry some
general information about the target domain. Our cross-
domain experiments focus on the text-target domain align-
ment and use GTADP for image-text alignment (following our
insights from the single-domain setting).
Training. Our experiments in this setting are designed
in the following manner: we train a diffusion model on
the source domain captions CTADP(x). With these source
domain captions, we experiment with four different cap-
tion modiﬁcations (each increasing in alignment to the tar-
get domain), a null Mnull(P)caption modiﬁcation where
Mnull(P)s=?=Mnull(P)✏✓=?, a simple Msimple(P)
caption modiﬁer where Msimple(P)sis a hand-crafted string
describing the style of the target domain appended to the
end and Msimple(P)✏✓=?, a Textual Inversion [ 16]
MTI(P)caption modiﬁer where the output MTI(P)sis a
learned Textual Inversion token <*>andMTI(P)✏✓=?,
and a DreamBooth [ 37]MDB(P)caption modiﬁer where
MDB(P)sis a learned DreamBooth token <SKS>and
MDB(P)✏✓is a DreamBoothed diffusion backbone. We
also include two additional control experiments. In the ﬁrst,
Mud(P)anunrelated target domain style is appended to
the end of the string. In the second, Mnd(P)anearby but
a different target domain style is appended to the caption.
MTI(P)andMDB(P)require more information than the
other methods, such that Prepresents a subset of unlabelled
images from the target domain.
Testing. When testing the trained models on the tar-
get domain images, we want to use the same caption-
ing modiﬁcation for the test images as in the training
setup. However, GTADP introduces a confound since it natu-MethodWatercolor2k Comic2k
AP AP 50 AP AP 50
Single Domain Generalization (SGD)
CLIP the gap [ 46] – 33.5 – 43.4
Cross domain weakly supervised object detection
PLGE [ 30] – 56.5 – 41.7
ICCM [ 19] – 57.4 – 37.1
H2FA R-CNN [ 51] – 59.9 – 46.4
Unsupervised domain adaptation object detection
ADDA [ 45] – 49.8 – 23.8
MCAR [ 54] – 56.0 – 33.5
UMT [ 11] – 58.1 – –
DASS-Detector (extra data) [ 44] – 71.5 – 64.2
TADP null 42.1 72.1 31.1 57.4
TADP simple 43.5 72.2 31.9 56.6
TADP TextualInversion 43.2 72.2 33.2 57.4
TADP DreamBooth 43.2 72.2 32.9 56.9
TADP NearbyDomain 42.0 71.5 31.8 56.4
TADP UnrelatedDomain 42.2 71.9 32.0 55.9
Table 5. Cross-domain object detection. Pascal VOC to Water-
color2k and Comic2k. We report the AP and AP50. Our method
sets a new SOTA for Watercolor2K.
rally incorporates target domain information. For example,
GTADP(x)might produce the caption “a watercolor paint-
ing of a dog and a bird” for an image from the Water-
color2K dataset. Using the Msimple(P)captioning modi-
ﬁcation on this prompt would introduce redundant informa-
tion and would not match the caption format used during
training. In order to remove target domain information and
get a plain caption that can be modiﬁed in the same man-
ner as in the training data, we use GPT-3.5 [ 6] to remove
all mentions of the target domain shift. For example, after
using GPT-3.5 to remove mentions of the watercolor style
in the above sentence, we are left with “an image of a bird
and a dog”. With these GPT-3.5 cleaned captions , we can
match the caption modiﬁcations used during training when
evaluating test images. This caption-cleaning strategy lets
us control how target domain information is included in the
test image captions, ensuring that test captions are in the
same domain as train captions.
Evaluation. We evaluate cross-domain transfer on sev-
eral datasets. We train our model on Pascal VOC [ 13,14]
object detection and evaluate on Watercolor2K (W2K) [ 21]
and Comic2K (C2K) [ 21]. We also train our model on the
Cityscapes [ 9] dataset and evaluate on the Nighttime Driv-
ing (ND) [ 10] and Dark Zurich-val (DZ-val) [ 39] datasets.
We show results in Tabs. 4,5. In the following sections, we
also report the average performance of each method on the
cross-domain segmentation datasets (average mIoU) and
the cross-domain object detection datasets (average AP).
13889
Null caption modiﬁer. The null captions have no tar-
get domain information. In this setting, the model is trained
with captions with no target domain information and tested
with GPT-3.5 cleaned target domain captions. We ﬁnd
diffusion pre-training to be extraordinarily powerful on its
own, with just plain captions (no target domain informa-
tion); the model already achieves SOTA on VOC !W2K
with 72.1 AP50, SOTA on CD !DZ-val with 42.8 mIoU
and SOTA on CD !ND with 60.8 mIoU. Our model per-
forms better than the current SOTA [ 44] on VOC !W2K
and worse on VOC !C2K (highlighted in yellow in Tab. 5).
However, [ 44] uses a large extra training dataset from the
target (comic) domain, so we highlight in bold our results
in Tab. 5to show they outperform all other methods that
use only images in C2K as examples from the target do-
main. Furthermore, these results are with a lightweight FPN
[24] head, in contrast to other competitive methods like Re-
ﬁgn [ 7], which uses a heavier decoder head. These captions
achieve 50.5 average mIoU and 36.6 average AP.
Simple caption modiﬁer. We then add target domain in-
formation to our captions by prepending the target domain’s
semantic shift to the generic captions. These caption modi-
ﬁers are hand-crafted. For example, “a dog and a bird” be-
comes “a X style painting of a dog and a bird” (where X is
watercolor for W2K and comic for C2K) and “a dark night
photo of a dog and a bird” for DZ. These captions achieve
48.0 average mIoU and 37.7 average AP.
Textual Inversion caption modiﬁer. Textual inversion
[16] is a method that learns a target concept (an object or
style) from a set of images and encodes it into a new to-
ken. We learn a novel token from target domain image sam-
ples to further increase image-text alignment (for details,
see Sec. D.1). In this setting, the sentence template be-
comes “a <token >style painting of a dog and a bird”. We
ﬁnd that, on average, Textual Inversion captions perform the
best, achieving 51.1 average mIoU and 38.2 average AP.
DreamBooth caption modiﬁer. DreamBooth-ing [ 37]
aims to achieve the same goal as textual inversion. Along
with learning a new token, the stable-diffusion backbone it-
self is ﬁne-tuned with a set of target domain images (for de-
tails, see Sec. D.1). We swap the stable diffusion backbone
with the DreamBooth-ed backbone before training. We use
the same template as in textual inversion. These captions
achieve 49.7 average mIoU and 38.1 average AP.
Ablations. We ablate our target domain alignment strat-
egy by introducing unrelated and nearby target-domain
style modiﬁcations. For example, this would be “a dash-
camphoto of a dog and a bird” (unrelated) and “a construc-
tivism painting of a dog and a bird” (nearby) for the W2K
and C2K datasets. “A watercolor painting of a car on the
street” (unrelated) and “a foggy photo of a car on the street”
for the ND and DZ-val datasets. We ﬁnd these off-target
domains reduce performance on all datasets.5. Discussion
We present a method for image-text alignment that is gen-
eral, fully automated, and can be applied to any diffusion-
based perception model. To achieve this, we systematically
explore the impact of text-image alignment on semantic
segmentation, depth estimation, and object detection. We
investigate whether similar principles apply in the cross-
domain setting and ﬁnd that alignment towards the target
domain during training improves downstream cross-domain
performance.
We ﬁnd that EOS token averaging for prompting does
not work as effectively as strings for the objects in the im-
age. Our oracle ablation experiments show that our diffu-
sion pre-trained segmentation model is particularly sensi-
tive to missing classes (reduced recall) and less sensitive to
off-target classes (reduced precision), and both have a neg-
ative impact. Our results show that aligning text prompts to
the image is important in identifying/generating good multi-
scale feature maps for the downstream segmentation head.
This implies that the multi-scale features and latent repre-
sentations do not naturally identify semantic concepts with-
out the guidance of the text in diffusion models. Moreover,
proper latent scaling is crucial for downstream vision tasks.
Lastly, we show how using a captioner, which has the ben-
eﬁt of being open vocabulary, high precision, and down-
stream task agnostic, to prompt the diffusion pre-trained
segmentation model automatically improves performance
signiﬁcantly over providing all possible class names.
We also ﬁnd that diffusion models can be used effec-
tively for cross-domain tasks. Our model, without any
captions, already surpasses several SOTA results in cross-
domain tasks due to the diffusion backbone’s generaliz-
ability. We ﬁnd that good target domain alignment can
help with cross-domain performance for some domains, and
misalignment leads to worse performance. Capturing in-
formation about target domain styles in words alone can
be difﬁcult. For these cases, we show that model per-
sonalization through Textual Inversion or Dreambooth can
bridge the gap without requiring labeled data. Future work
could explore how to expand our framework to generalize
to multiple unseen domains. Future work may also explore
closed vocabulary captioners that are more task-speciﬁc to
get closer to oracle-level performance.
Acknowledgements. Pietro Perona and Markus Marks were
supported by the National Institutes of Health (NIH R01
MH123612A) and the Caltech Chen Institute (Neuroscience Re-
search Grant Award). Pietro Perona, Neehar Kondapaneni, Roge-
rio Guimaraes, and Markus Marks were supported by the Simons
Foundation (NC-GB-CULM-00002953-02). Manuel Knott was
supported by an ETH Zurich Doc.Mobility Fellowship. We thank
Oisin Mac Aodha, Yisong Yue, and Mathieu Salzmann for their
valuable inputs that helped improve this work.
13890
References
[1]Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-
aming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala,
Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras,
and Ming-Yu Liu. eDiff-I: Text-to-Image Diffusion Mod-
els with an Ensemble of Expert Denoisers. arXiv preprint
arXiv:2211.01324 , 2022. 2
[2]Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalo-
geiton, and St ´ephane Lathuili `ere. One-shot Unsupervised
Domain Adaptation with Personalized Diffusion Models.
arXiv preprint arXiv:2303.18080 , 2023. 2
[3]Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,
and Matthias M ¨uller. ZoeDepth: Zero-shot Transfer by
Combining Relative and Metric Depth. arXiv preprint
arXiv:2302.12288 , 2023. 5
[4]Steven Bird, Ewan Klein, and Edward Loper. Natural lan-
guage processing with Python: analyzing text with the natu-
ral language toolkit . O’Reilly Media, Inc., 2009. 6
[5]Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas
Struppek, Patrick Schramowski, and Kristian Kersting.
SEGA: Instructing Diffusion using Semantic Dimensions.
arXiv preprint arXiv:2301.12247 , 2023. 1,2
[6]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 7
[7]David Br ¨uggemann, Christos Sakaridis, Prune Truong, and
Luc Van Gool. Reﬁgn: Align and Reﬁne for Adaptation
of Semantic Segmentation to Adverse Conditions. 2023
IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV) , 2022. 7,8
[8]Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong
Lu, Jifeng Dai, and Y . Qiao. Vision Transformer Adapter for
Dense Predictions. arXiv preprint arXiv:2205.08534 , 2022.
5
[9]Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The Cityscapes
Dataset for Semantic Urban Scene Understanding. 2016
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) , pages 3213–3223, 2016. 2,7
[10] Dengxin Dai and Luc Van Gool. Dark Model Adaptation:
Semantic Image Segmentation from Daytime to Nighttime.
2018 21st International Conference on Intelligent Trans-
portation Systems (ITSC) , pages 3819–3824, 2018. 2,7
[11] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Un-
biased mean teacher for cross-domain object detection. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 4091–4101, 2021. 3,
7
[12] David Eigen, Christian Puhrsch, and Rob Fergus. Depth Map
Prediction from a Single Image using a Multi-Scale Deep
Network. In Advances in Neural Information Processing
Systems 27 (NIPS 2014) , 2014. 14
[13] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object ClassesChallenge 2007 (VOC2007) Results. http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.
2,7
[14] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and
A. Zisserman. The PASCAL Visual Object Classes Chal-
lenge 2012 (VOC2012), 2012. 2,7
[15] Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen Sun,
Ledell Yu Wu, Xinggang Wang, Tiejun Huang, Xinlong
Wang, and Yue Cao. EV A: Exploring the Limits of Masked
Visual Representation Learning at Scale. 2023 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 19358–19369, 2022. 5
[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or.
An Image is Worth One Word: Personalizing Text-to-
Image Generation using Textual Inversion. arXiv preprint
arXiv:2208.01618 , 2022. 2,7,8
[17] Rui Gong, Martin Danelljan, Han Sun, Julio Delgado Man-
gas, and Luc Van Gool. Prompting Diffusion Represen-
tations for Cross-Domain Semantic Segmentation. arXiv
preprint arXiv:2307.02138 , 2023. 1,2,3,7
[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kﬁr Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt Im-
age Editing with Cross Attention Control. arXiv preprint
arXiv:2208.01626 , 2022. 2
[19] Luwei Hou, Yu Zhang, Kui Fu, and Jia Li. Informative and
consistent correspondence mining for cross-domain weakly
supervised object detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 9929–9938, 2021. 3,7
[20] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. DAFormer:
Improving Network Architectures and Training Strate-
gies for Domain-Adaptive Semantic Segmentation. 2022
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 9914–9925, 2022. 7
[21] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and
Kiyoharu Aizawa. Cross-Domain Weakly-Supervised Ob-
ject Detection Through Progressive Domain Adaptation. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 5001–5009, 2018. 2,3,7
[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc V . Le, Yunhsuan Sung, Zhen Li, and Tom
Duerig. Scaling Up Visual and Vision-Language Represen-
tation Learning With Noisy Text Supervision. arXiv preprint
arXiv:2102.05918 , 2021. 1
[23] Junguang Jiang, Baixu Chen, Jianmin Wang, and Mingsheng
Long. Decoupled adaptation for cross-domain object detec-
tion. arXiv preprint arXiv:2110.02578 , 2021. 3
[24] Alexander Kirillov, Ross B. Girshick, Kaiming He, and Pi-
otr Doll ´ar. Panoptic Feature Pyramid Networks. 2019
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 6392–6401, 2019. 8,14
[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: Bootstrapping Language-Image Pre-training with
Frozen Image Encoders and Large Language Models. arXiv
preprint arXiv:2301.12597 , 2023. 2,4,5
13891
[26] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli
Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Su-
pervision Exists Everywhere: A Data Efﬁcient Contrastive
Language-Image Pre-training Paradigm. arXiv preprint
arXiv:2110.05208 , 2022. 1
[27] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,
Furu Wei, and Baining Guo. Swin Transformer V2: Scaling
Up Capacity and Resolution. 2022 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
11999–12009, 2021. 5
[28] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holyn-
ski, and Trevor Darrell. Diffusion hyperfeatures: Searching
through time and space for semantic correspondence. arXiv
preprint arXiv:2305.14334 , 2023. 1,2,3
[29] Jia Ning, Chen Li, Zheng Zhang, Zigang Geng, Qi Dai,
Kun He, and Han Hu. All in Tokens: Unifying Out-
put Space of Visual Tasks via Soft Token. arXiv preprint
arXiv:2301.02229 , 2023. 5
[30] Shengxiong Ouyang, Xinglu Wang, Kejie Lyu, and Ying-
ming Li. Pseudo-label generation-evaluation framework for
cross domain weakly supervised object detection. In 2021
IEEE International Conference on Image Processing (ICIP) ,
pages 724–728. IEEE, 2021. 3,7
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning Transferable Visual
Models From Natural Language Supervision. International
Conference on Machine Learning , pages 8748–8763, 2021.
1,2,3
[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical Text-Conditional Image Gen-
eration with CLIP Latents. arXiv preprint arXiv:2204.06125 ,
2022. 1,2
[33] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong
Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen
Lu. DenseCLIP: Language-Guided Dense Prediction with
Context-Aware Prompting. 2022 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
18061–18070, 2022. 5
[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards Real-Time Object Detection with
Region Proposal Networks. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 39(6):1137–1149, 2017.
14
[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bjorn Ommer. High-Resolution Image
Synthesis with Latent Diffusion Models. 2022 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 10674–10685, 2022. 1,2,3,4
[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
Net: Convolutional Networks for Biomedical Image Seg-
mentation. In Medical Image Computing and Computer-
Assisted Intervention – MICCAI 2015 , pages 234–241.
Springer International Publishing, Cham, 2015. 1
[37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. DreamBooth: FineTuning Text-to-Image Diffusion Models for Subject-Driven
Generation. arXiv preprint arXiv:2208.12242 , 2022. 2,7,8
[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mah-
davi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho,
David J. Fleet, and Mohammad Norouzi. Photorealistic
Text-to-Image Diffusion Models with Deep Language Un-
derstanding. arXiv preprint arXiv:2205.11487 , 2022. 1,2
[39] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Guided
Curriculum Model Adaptation and Uncertainty-Aware Eval-
uation for Semantic Nighttime Image Segmentation. 2019
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 7373–7382, 2019. 2,7
[40] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-
400M: Open Dataset of CLIP-Filtered 400 Million Image-
Text Pairs. arXiv preprint arXiv:2111.02114 , 2021. 3
[41] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa Kundurthy, Katherine
Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Je-
nia Jitsev. LAION-5B: An open large-scale dataset for
training next generation image-text models. arXiv preprint
arXiv:2210.08402 , 2022. 2
[42] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus. Indoor Segmentation and Support Inference from
RGBD Images. European Conference on Computer Vision
(ECCV) , 2012. 2
[43] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng
Phoo, and Bharath Hariharan. Emergent correspondence
from image diffusion. arXiv preprint arXiv:2306.03881 ,
2023. 1,2,3
[44] Barıs ¸ Batuhan Topal, Deniz Yuret, and Tevﬁk Metin Sez-
gin. Domain-adaptive self-supervised pre-training for
face & body detection in drawings. arXiv preprint
arXiv:2211.10641 , 2022. 3,7,8
[45] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
Adversarial discriminative domain adaptation. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 7167–7176, 2017. 7
[46] Vidit Vidit, Martin Engilberge, and Mathieu Salzmann. CLIP
the Gap: A Single Domain Generalization Approach for Ob-
ject Detection. 2023 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 3219–3229,
2023. 3,7
[47] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xi-
aohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang
Zhou. ONE-PEACE: Exploring One General Representa-
tion Model Toward Unlimited Modalities. arXiv preprint
arXiv:2305.11172 , 2023. 5
[48] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,
Zhiqi Li, Xizhou Zhu, Xiao-hua Hu, Tong Lu, Lewei Lu,
Hongsheng Li, Xiaogang Wang, and Y . Qiao. InternIm-
age: Exploring Large-Scale Vision Foundation Models with
13892
Deformable Convolutions. 2023 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
14408–14419, 2022. 5
[49] Wen Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-
iang Peng, Qiangbo Liu, Kriti Aggarwal, Owais Khan Mo-
hammed, Saksham Singhal, Subhojit Som, and Furu Wei.
Image as a Foreign Language: BEIT Pretraining for Vision
and Vision-Language Tasks. 2023 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
19175–19186, 2023. 5
[50] Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou,
and Chunhua Shen. DiffuMask: Synthesizing Images with
Pixel-level Annotations for Semantic Segmentation Using
Diffusion Models. arXiv preprint arXiv:2303.11681 , 2023.
2
[51] Yunqiu Xu, Yifan Sun, Zongxin Yang, Jiaxu Miao, and Yi
Yang. H2fa r-cnn: Holistic and hierarchical feature align-
ment for cross-domain weakly supervised object detection.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 14329–14339, 2022.
3,7
[52] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,
Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku,
Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchin-
son, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason
Baldridge, and Yonghui Wu. Scaling Autoregressive Models
for Content-Rich Text-to-Image Generation. arXiv preprint
arXiv:2206.10789 , 2022. 1,2
[53] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu,
Jie Zhou, and Jiwen Lu. Unleashing Text-to-Image Dif-
fusion Models for Visual Perception. arXiv preprint
arXiv:2303.02153 , 2023. 1,2,3,4,5,14
[54] Zhen Zhao, Yuhong Guo, Haifeng Shen, and Jieping Ye.
Adaptive object detection with dual multi-label prediction.
InComputer Vision–ECCV 2020: 16th European Confer-
ence, Glasgow, UK, August 23–28, 2020, Proceedings, Part
XXVIII 16 , pages 54–69. Springer, 2020. 3,7
[55] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba. Scene Parsing through
ADE20K Dataset. 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 5122–5130,
2017. 2
13893
