PortraitBooth:
A Versatile Portrait Model for Fast Identity-preserved Personalization
Xu Peng1,2, Junwei Zhu3, Boyuan Jiang3, Ying Tai4, Donghao Luo3, Jiangning Zhang3,
Wei Lin1,2, Taisong Jin1,2†, Chengjie Wang3, Rongrong Ji1,2
1Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
Ministry of Education of China, Xiamen University, China.
2School of Informatics, Xiamen University, China.3Tencent Youtu Lab.
4School of Intelligence Science and Technology, Nanjing University, Suzhou, China.
{penglingxiao,lvvviolette }@stu.xmu.edu.cn, tyshiwo@gmail.com
{junweizhu,byronjiang,michaelluo,vtzhang,jasoncjwang }@tencent.com
{jintaisong,rrji }@xmu.edu.cn
Awomanholding a bottle of perfumeA manand a womansmilingin front of a barA manand a man, both wearing suspicious expressions, in a dimly lit roomReferenceFastComposerPortraitBoothReferenceFastComposerPortraitBooth
AwomanFunkopop
Figure 1. Qualitative comparison of PortraitBooth and FastComposer on action, style, expression editing, multi-subject generation,
and identity preservation, all without any test-time tuning.
Abstract
Recent advancements in personalized image generation
using diffusion models have been noteworthy. However,
existing methods suffer from inefficiencies due to the re-
quirement for subject-specific fine-tuning. This computa-
tionally intensive process hinders efficient deployment, lim-
iting practical usability. Moreover, these methods often
grapple with identity distortion and limited expression di-
versity. In light of these challenges, we propose Portrait-
Booth, an innovative approach designed for high efficiency,
robust identity preservation, and expression-editable text-
to-image generation, without the need for fine-tuning. Por-
traitBooth leverages subject embeddings from a face recog-
nition model for personalized image generation without
fine-tuning. It eliminates computational overhead and mit-
†Corresponding Author.igates identity distortion. The introduced dynamic identity
preservation strategy further ensures close resemblance to
the original image identity. Moreover, PortraitBooth incor-
porates emotion-aware cross-attention control for diverse
facial expressions in generated images, supporting text-
driven expression editing. Its scalability enables efficient
and high-quality image creation, including multi-subject
generation. Extensive results demonstrate superior perfor-
mance over other state-of-the-art methods in both single
and multiple image generation scenarios. Our project page
is athttps://portraitbooth.github.io .
1. Introduction
Recent years have witnessed remarkable progress in text-to-
image synthesis [4, 22, 29, 41], propelled by the emergence
of diffusion models [6, 15, 16, 31, 47]. Pre-trained text-
to-image generation models have opened up new avenues
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27080
Figure 2. Comparison of identity information obtained based on
the trained image encoder and pre-trained face recognition model.
for creative content creation, with personalized generation
gaining popularity for its diverse applications.
Personalized generation methods based on diffusion
models fall into two main categories: 1)test-time fine-
tuning and 2)test-time non-fine-tuning. Some ap-
proaches [10, 13, 26, 33, 34] endorse test-time fine-tuning
using reference images (typically 3-5) to generate person-
alized results. However, these methods require specialized
network training [33, 37] , making them inefficient for prac-
tical applications . An alternative to test-time fine-tuning is
retraining the base text-to-image model with specially de-
signed strategies, e.g. training a distinct image encoder on
a massive dataset to capture reference image identity infor-
mation. However, these approaches [24, 39, 42] face chal-
lenges, either dealing with identity distortion orgenerating
images lacking editability , as depicted in Fig. 2. This is
mainly due to the coarse-grained nature of the identity in-
formation obtained from the trained image encoder. The
better the image encoder is trained, the tighter the identity
information with reference image is coupled, severely com-
promising editability. Additionally, these methods often de-
mand significant GPU resources and high storage, making
them impractical for most research institutions. Tab. 1 of-
fers a comprehensive comparison of existing personalized
image generation methods across four key aspects.
In this paper, we introduce PortraitBooth, a novel text-to-
portrait personalization framework that achieves high effi-
ciency, robust identity preservation, and diverse expression
editing. We then describe our main characteristics in detail:
High Efficiency. PortraitBooth stands out as a highly ef-
ficient one-stage generation framework, delivering the fol-
lowing advantages: 1)Only a single image is required
during the inference stage, unlike other schemes such as
Dreambooth that need multiple images. 2)No finetuning
or optimization is conducted during inference, which saves
time and avoids delays. 3)Lower training resource require-
ment is needed than Face0 and Subject-Diffusion that de-
mand a lot of high-performance GPU resources.
Robust Identity Preservation. 1)PortraitBooth employs
a pre-trained face recognition model ( 41.5M parameters)
to extract a face embedding from a given reference image.
This embedding is then projected into the context space
of Stable Diffusion using a simple multilayer perceptron,
enabling high-fidelity image generation based on the pro-Methods Single ImageTest-time
None-fine-tuningRobust ID
PreservationExpression
Editing
Textual Inversion [10] % % % !
Dreambooth [33] % % % !
Custom Diffusion [22] % % % !
Break-A-Scene [2] ! % ! %
HyperDreamBooth [34] ! % ! !
FastComposer [42] ! ! % %
Face0 [39] ! ! ! %
Subject-Diffusion [24] ! ! ! %
PortraitBooth (Ours) ! ! ! !
Table 1. Comparisons of current personalization approaches .
posed Subject Text Embedding Augmentation (STEA). 2)
PortraitBooth Dynamically maintains Identity Preservation
(DIP) by incorporating an identity loss during training to
facilitate the model to ensure identity preservation.
Diverse Expression Editing. While the discriminative fea-
tures extracted from a robust face recognition model effec-
tively disentangle identity and attributes, expression editing
remains a challenge for existing one-shot methods [24]. To
address this, we introduce Emotion-aware Cross-Attention
Control (ECAC) via a truncation mechanism. This allows
a single area to respond to multiple tokens simultaneously,
thereby enabling versatile expression editing (see Fig. 1).
In summary, our contributions are threefold:
• We propose a novel one-shot text-to-portrait generation
framework, termed PortraitBooth, which is the first solu-
tion to achieve high efficiency, robust identity preserva-
tion, and low training cost, simultaneously.
• To address identity distortion, we introduce the STEA and
DIP modules for robust identity preservation. Addition-
ally, we propose the ECAC module, achieving diverse ex-
pression editing.
• Our method scales effortlessly for single-subject and
multi-subject generation, integrating smoothly with
multi-object generation methods. Furthermore, our Por-
traitBooth excels in achieving remarkable fidelity and ed-
itability, surpassing other state-of-the-art methods.
2. Related Work
Image Editing with Diffusion Models. Image editing
[12, 38] is a fundamental task in computer vision, involv-
ing modifications to an input image with auxiliary inputs
like audio [50, 52], text [45], masks [12], or reference im-
ages [43, 44, 46, 49]. Despite the capabilities of large-
scale diffusion models such as Imagen [35], DALL·E2
[30], and Stable Diffusion [31] in text-to-image synthe-
sis, they lack precise control over image generation solely
through text guidance. Even a small change in the origi-
nal prompt can yield significantly different outcomes. Re-
cent research has focused on adapting text-guided diffu-
sion models [1, 8, 14, 17, 20, 21] for real image editing,
leveraging their rich and diverse semantic knowledge. One
such approach is Prompt-to-Prompt [14], which injects in-
ternal cross-attention maps when modifying only the text
27081
prompt, preserving the spatial layout and geometry neces-
sary for regenerating an image while modifying it through
prompt editing. Existing methods for portrait expression
editing based on diffusion models not only focus on de-
signing optimization-free methods [3, 7, 25, 27], but also
explore face swapping as an alternative approach. For ex-
ample, DiffusionRig [9] learns generic facial personalized
priors to control face synthesis.
Personalized Visual Content Generation. Personalized
visual content generation aims to create images tailored
to individual preferences or characteristics, including new
subjects described by one or more images [11]. Textual In-
version (TI) [10] and DreamBooth (DB) [33] are two pio-
neering works in personalization. They generate different
contexts for a single visual concept using multiple images.
TI introduces a learnable text token and optimizes it for
concept reconstruction using standard diffusion loss, while
keeping model weights frozen. DB reuses a rare token and
fine-tunes model weights for reconstruction. HyperDream-
Booth [34] offers a lightweight, subject-driven personaliza-
tion for text-to-image diffusion models compared to DB.
Custom Diffusion [22] fine-tunes subset layers of the cross-
attention in the UNet. However, these tuning-based meth-
ods require time-consuming fine-tuning or multiple images,
which is inelegant. In contrast, PortraitBooth amortizes
costly subject tuning during training, enabling fast person-
alization with a single image.
Concurrent tuning-free methods include [24, 39, 42],
those use an image encoder for accessibility, but Fastcom-
poser may distort identity due to lack of fine-grained train-
ing. Face0 [39] and Subject-Diffusion [24] achieve rel-
atively high identity preservation in personalized genera-
tion through massive datasets and expensive hardware re-
sources. However, they require resource-intensive back-
propagation. Conversely, PortraitBooth generates person-
alized portraits with comparable identity preservation in an
inference-only manner, requiring fewer hardware resources
that most research institutions can afford.
3. Preliminaries
3.1. Stable Diffusion
Stable Diffusion (SD) consists of three components: a Vari-
ational AutoEncoder (V AE), a conditional U-Net [32], and
a text encoder [28]. Specifically, for an input image x0,
The V AE encoder Ecompresses the x0to a smaller latent
representation z. The diffusion process is then performed
on the latent space, where a conditional U-Net denoiser ϵθ,
denoises the noisy latent representation by predicting the
noise θwith current timestep t,t-th noisy latent zt. This
denoising process can be conditioned on textual conditional
Cthrough the cross-attention mechanism, Throughout the
training process, the network is optimized to minimize theloss function defined as:
Lnoise =Ez∼E(x),C,ϵ∼N (0,1),t
||ϵ−ϵθ(zt, t, C)||2
2
,
zt∼ N(√αtzt−1,1−αt),(1)
where αtis a predefined sequence of coefficients control-
ling the variance schedule. The closed form of the distribu-
tionp(zt|z0)can be easily derived as:
zt=√¯αtz0+ (1−¯αt)ϵ,
¯αt=tY
s=1αs, ϵ∼ N(0,1).(2)
3.2. Cross-Attention Mechanism
In the SD model, the U-Net employs a cross-attention
mechanism to denoise the noisy latent image conditioned
on text prompts. The cross-attention layer accepts the spa-
tial noisy latent image ztand the text embeddings yas in-
puts. The embeddings of the visual and textual features are
fused to produce spatial attention maps for each textual to-
ken. The cross-attention maps are computed with:
A=softmaxQKT
√
d
. (3)
The query matrix, denoted as Q=ztW(i)
Q, is the projection
of the noisy latent image zt. The key matrix, represented
asK=yW(i)
K, is the projected textual features. Here, W(i)
Q
andW(i)
Krepresent the weight matrices of the two linear
layers in each cross-attention block iof the U-Net, and dis
the output dimension of KandQfeatures.
4. Methodology
4.1. Subject Text Embedding Augmention
From a generative standpoint, our objective is to create a
portrait that accurately represents the identity of the source
face. To achieve this, we utilize a pre-trained face recog-
nition model called TFace [18] to extract the identity fea-
tures. In order to better preserve the identity, we incorpo-
rate face features as an important input condition and in-
tegrate them into the text to enhance its ability to capture
the nuances of identity. To elaborate, we first encode the
text prompt P={w1, w2, ...w n}and reference face finto
embeddings using the pre-trained text encoder and TFace,
denoted as ψandφrespectively. However, as the features
generated by the recognition model are primarily designed
for recognition purposes and may not be optimal for gen-
eration, we choose to extract only the shallow features of
the recognition model. Subsequently, we concatenate the
embedding of the identity token with the facial feature,
and then feed the resulting augmented embeddings into the
27082
𝒵t
AlignTokenization
Concat.+
QKVQKVQKVQKVQKVQKVU-Net
🔥TFaceTextEncoder
❄
❄
STE
At≤ RtEstimatedNoise𝓛noisyGroundTruthNoise“A[Identity] has a [Expression] face”Text-Image Pair
InputTextInput Image
Augmented Text EmbeddingCross-attentionControl
Face MaskUnoptimizedCross-attention Map𝓛loc
ECAC
𝑀𝒳0…OptimizedCross-attention Map
𝓛idEstimateImage
DI
P𝒳0›AlignAlignInputImage𝒳0
𝐴𝑖𝐴𝑙𝑙𝑗
𝐴𝑖𝐴𝑙𝑙𝑗
𝜷and 𝜸fFigure 3. Overview framework of PortraitBooth . PortraitBooth extracts the face ffrom the input image x0, and augments the subject’s
features using TFace for improved identity representation. The diffusion model is trained to generate images with enhanced conditioning,
incorporating emotion-aware cross-attention for expression editing and dynamic identity preservation to maintain identity. During the
testing phase, only the STEA module is required, we just need to input a single image and the corresponding prompt to achieve rapid,
robust identity preservation and diverse expression editing capabilities. Ai
l,Aj
lrepresents the cross-attention map corresponding to the
i-th and j-th token at the l-th cross-attention layer, respectively. βandγrepresent the maximum values of the cross-attention map for the
identity token and expression token respectively, while Rtindicates the timing for identity preservation.
MLP . This process yields the final conditioning embed-
dings C={c1, c2...cn}, which are defined as :
ci=(
ψ(wi) wi̸∈ {identity token }
MLP ([ψ(wi)||φ(f)])wi∈ {identity token }.(4)
This approach allows us to generate portraits that not only
capture the textual description but also incorporate the iden-
tity features extracted from the reference face, resulting in a
more accurate representation of the desired identity. Fig. 3
illustrates the STEA module, which provides a concrete ex-
ample of our augmentation approach.
4.2. Dynamic Identity Preservation
The current SD model achieves image fidelity by relying on
accurate prompts, which however poses a significant chal-
lenge. When incorporating new image conditions, ensuring
the fidelity of the unique reference image becomes neces-
sary. Therefore, it is crucial to incorporate identity loss into
the training framework of diffusion models to ensure iden-
tity preservation. Let x0be the input image, zbe its la-
tent space representation, T(T < 1000) represents the total
number of noise injection steps. For a small value of Rt
(Rt< T ), we can get estimated ˆz0directly from ztand
the predicted noise ϵθ(zt, t, C). From Eqn. 2, the one-step
reverse formula is defined as :
ˆz0=zt−√1−¯αtϵθ√¯αt, t≤Rt, (5)After reverse, the estimated ˆz0is decoded from the latent
space using the V AE decoder Dto obtain the estimated in-
put image ˆx0=D(ˆz0). Then, based on the facial region of
the original image, the estimated facial region image ˆxf
0is
extracted from the reconstructed image. Finally, the identity
loss between the estimated facial image and the reference
facial image is defined as:
Lid=(
1−CosSim
φ(f), φ(ˆxf
0)
t≤Rt
0 t > R t.(6)
The identity loss is designed to handle noisy images and
improve the model’s ability to preserve the identity. The
DIP module, as illustrated in Fig. 3.
4.3. Emotion-aware Cross-attention Control
For previous one-shot personalized generation works [24,
39, 42] , a common issue is that the generated images al-
ways have the same expression as the reference image, re-
gardless of the prompt given. Although we have largely
decoupled identity and attributes by utilizing pre-trained fa-
cial recognition models to extract discriminative features
for subject feature enhancement, the complexity and diver-
sity of facial expressions still pose a challenge in maintain-
ing identity during portrait generation. This issue primarily
arises because the cross-attention map is spread across the
entire image during image generation. To address this is-
sue and ensure that the cross-attention map corresponding
27083
Figure 4. Comparison of different methods on single subject image generation in the testing dataset.
Referencea photo of a manand a womantogether with a mountain in the background 
a photo of a womanand a womantogether in the snowFastComposerPortraitBooth
a photo of a manand awomanposing for a selfie together 
Figure 5. Comparison of different methods on multi-subject
image generation in the testing dataset.
to specific tokens only attends to the image region occu-
pied by the corresponding concept, we propose an emotion-
aware cross-attention control mechanism.
Specifically, unlike previous methods [24, 42] that used
attention masks to control subject token’s attention map
solely on the one subject region, we allow attention con-
trol of different tokens within the same region by truncat-
ing cross-attention mechanism. For instance, when dealing
with tokens for facial expressions and identity, we employ
a face mask to ensure that the attention maps correspond-
ing to these two tokens are both focused on the face region.
However, we observe that when two different tokens’ atten-
tion maps are both constrained to the same region, one token
may learn well while the other may not. To tackle this prob-
lem, we propose a complete local control constraint with
truncating cross-attention mechanism:
Lloc=1
NNX
l=1λ(mean (Ai
l(1−M)) +mean (relu(β−Ai
l)M))
+1
NNX
l=1µ(mean (Aj
l(1−M)) +mean (relu(γ−Aj
l)M)),(7)where Mis the face mask normalized to [0,1]. mean is the
pixel-level averaging. Ai
l, Aj
l∈[0,1]represents the cross-
attention map corresponding to the identity and expression
token at the l-th cross-attention layer. βandγare used
to constrain the maximum response intensity of the cross-
attention map in the facial area corresponding to the identity
token and expression token, respectively. We optimize Lloc
to ensure that objects’ attention map exhibit their respective
response in the desired area, which is achieved by maximiz-
ing the response of each token’s attention map to the face re-
gion and minimizing its response to the background, along
with the use of a truncated response mechanism in the at-
tention map. λandµare localization loss ratios, which are
0.001and0.01. Fig. 3 illustrates the ECAC module.
4.4. Objective Function
First, we use TFace φto extract the face embedding
and concatenate it with specific identity token embedding,
which is extracted from the text encoder ψ. These are then
fed into an MLP for feature enhancement, forming U-Net
aware conditional information C. Next, we feed the noisy
latent space feature map ztinto a U-Net with conditional
guidance to predict noise, while implementing a truncation
mechanism for local attention control for specific tokens.
To better preserve identity, we employ dynamic identity
preservation method to calculate the loss between the es-
timated face image ˆxf
0and reference face f. The final train-
ing objective of PortraitBooth is:
Ltotal=Lloc+Lnoise+Lid. (8)
5. Experiments
5.1. Experimental Setups
Dataset Description. We constructed a single sub-
ject image-text paired dataset based on the CelebV-T
dataset [48], which consists of 70,000videos. To utilize
the additional textual descriptions provided by CelebV-T,
we randomly extracted the first or last frames of each video.
27084
Method Type Reference Image ↓Id Pres. ↑CLIP-TI ↑Test Time ↓Training Cost
Stable Diffusion [31] Zero Shot 0 0.039 0.268 ≈2s -
Face0 [39] One Shot 1 - - ≈2s 64 TPU
Textual-Inversion [10] Finetune 5 0.293 0.219 ≈2500s 1 A100
DreamBooth [33] Finetune 5 0.273 0.239 ≈1084s 1 A100
Custom Diffusion [22] Finetune 5 0.434 0.233 ≈789s 1 A100
FastComposer [42] One Shot 1 0.514 0.243 ≈2s 8 A6000
Subject-Diffusion [24] One Shot 1 0.605 0.228 ≈2s 24 A100
PortraitBooth (ours) One Shot 1 0.657 0.245 ≈2s 3 A100
Table 2. Comparison between our method and baseline approaches on single-subject image generation. Our approach achieves
highly satisfactory results with the utilization of relatively limited resources under the one-shot setting.
Method Type Reference Image ↓Id Pres. ↑CLIP-TI ↑Test Time ↓Training Cost
Stable Diffusion [31] Zero Shot 0 0.019 0.284 ≈2s -
Textual-Inversion [10] Finetune 5 0.135 0.211 ≈4998s 1 A100
Custom Diffusion [22] Finetune 5 0.054 0.258 ≈789s 1 A100
FastComposer [42] One Shot 2 0.431 0.243 ≈2s 8 A6000
PortraitBooth (ours) One Shot 2 0.647 0.239 ≈18s 3 A100
Table 3. The comparison between our method and the baseline approaches that support multiple-subject image generation. Sta-
bleDiffusion was used as the text-only baseline without any subject conditioning.
Additionally, we used the Recognize Anything model [53]
to generate captions describing the main subject for all im-
ages. To enhance the robustness of our models, we ran-
domly selected a frame from the middle section of each
video and used the facial region as our reference face im-
age. We employ the pre-train face parsing model [23] to
generate subject face segmentation masks for each image.
Training Details. We start training from the Stable Dif-
fusion v 1−5[31] model. To encode the identity inputs,
we use TFace model. During training, we only train the
U-Net, the MLP module. We train our models for 150k
steps on 6NVIDIA V 100GPUs (For the sake of easy and
intuitive comparison later, we roughly convert 6NVIDIA
V100GPUs into 3NVIDIA A 100GPUs.), with a constant
learning rate of 1e−5and a batch size of 2. We train the
model solely on text conditioning with 10% of the samples
to maintain the model’s capability for text-only generation.
To facilitate classifier-free guidance sampling [15], we train
the model without any conditions on 10% of the instances.
During training, we apply the loss only in the subject’s face
region to half of the training samples to enhance generation
quality in the subject area. There are 11emotion words in-
volved in truncating cross-attention control, such as happy,
angry, sad, etc. We select a value of 250forRtto obtain
ˆz0through reverse. The selected identity label is from the
categories {“man”,“woman” }. During inference, We use
Euler [19] sampling with 50steps and a classifier-free guid-
ance scale of 5across all methods.
Evaluation Metric. We evaluate the quality of image gen-eration based on identity preservation (Id Pres.) and CLIP
text-image consistency (CLIP-TI). Identity preservation is
determined by detecting faces in the reference and gen-
erated images using MTCNN [51], and then calculating
pairwise identity similarity using FaceNet [36]. For multi-
subject evaluation, we identify all faces within the gener-
ated images and use a greedy matching procedure between
the generated faces and reference subjects. For the evalua-
tion of expression editing, we calculate the text-image con-
sistency between the emotion words in each prompt and the
corresponding generated images as our expression coeffi-
cient metric. For efficiency evaluation, we consider the total
time for customization, including fine-tuning (for tuning-
based methods) and inference. We also take into consid-
eration the total number of GPUs required throughout the
entire procedure. All baselines, by default, are run with the
standard set of hyperparameters as mentioned in their paper.
5.2. Personalized Image Generation
To evaluate our model’s effectiveness in this area, we
use the single-entity evaluation method employed in Fast-
Composer [42] and compare our model’s performance
to that of other existing methods including Dream-
Booth [33], Textual-Inversion [10], Custom Diffusion [22],
and Subject-Diffusion [24]. Methods [10, 22, 33] were used
the implementation from diffusers library [40]. Considering
that Face0 [39] does not provide open-source code, we can
only list the hardware resources mentioned in their paper as
a point of comparison. Stable Diffusion [31] was used as the
27085
Figure 6. Comparison chart of expression editing between our
method and FastComposer, focusing on the three most distinct
expression terms.
32413223206446571681
heshewomanmanpersonThe statistical analysis of the main subject identity token0 500 1000 1500 2000 ～44200 44700
Figure 7. The number of main subject words occurrences in the
generated 70,000captions.
text-only baseline. The entire test set comprises 15 subjects,
and 30 texts. The evaluation benchmark developed a broad
range of text prompts encapsulating a wide spectrum of sce-
narios, such as recontextualization, stylization, accessoriza-
tion, and diverse actions. Five images were utilized per
subject to fine-tune the optimization-based methods. For
the one-shot method, a single randomly selected image was
employed for each subject. As shown in Tab. 2, Portrait-
Booth significantly outperforms all baseline approaches in
identity preservation. Fig. 4 shows the qualitative results of
single-subject personalization comparisons, employing dif-
ferent approaches across an array of prompts.
5.3. Multi-Subject Image Generation
We then delve into a more intricate scenario: multi-subject,
subject-driven image generation. We scrutinize the quality
of multi-subject generation by utilizing all possible com-
binations (a total of 105 pairs) formed from the 15 subjects
described in Section §5.2, allocating 21 prompts to each pair
for evaluation. Considering that PortraitBooth was trained
on a single-subject dataset, we incorporated the MultiD-
iffusion [5] generation method, which combines multiple
reference diffusion generation processes with shared pa-
rameters, to generate images in different regions during
inference. Tab. 3 shows a quantitative analysis contrast-
ing PortraitBooth with the baseline methods. The results
demonstrate that PortraitBooth significantly improves the
identity preservation score. Moreover, our prompt consis-
tency is comparable to tuning-based approaches [10, 22],
but weaker than FastComposer and Custom Diffusion. We
attribute this vulnerability may stem from our method’s in-Method Type Expression Coefficients ↑
Textual-Inversion [10] FineTune 0.158
Custom Diffusion [22] FineTune 0.182
DreamBooth [33] FineTune 0.153
FastComposer [42] One Shot 0.133
PortraitBooth w/o expression control One Shot 0.177
PortraitBooth (Ours) One Shot 0.193
Table 4. Comparison of facial expression coefficients between
PortraitBooth and other methods.
Combination Type Id Pres. ↑CLIP-TI ↑
{“person” } 0.623 0.229
{“he”,“she” } 0.606 0.208
{“man”,“woman” } 0.657 0.245
Table 5. The impact of embedding enhancement of subject to-
kens from different categories.
Item Method Id Pres. ↑CLIP-TI ↑
PortraitBooth 0.657 0.245
(a) w/o STEA 0.563 0.244
(b) w/o DIP 0.638 0.239
(c) w/o ECAC 0.632 0.235
Table 6. Ablation results of three components.
clination to give precedence to subject fidelity. The longer
test time, compared to FastComposer, is a result of current
multi-subject generation method limitations. We anticipate
a significant reduction in our multi-subject generation time
as these methods evolve. Fig. 5 shows the qualitative results
of multi-subject personalization comparisons.
5.4. Expression Editing
To demonstrate the effectiveness of our approach in terms
of facial expression editing, we conduct a series of com-
parisons against both test-time fine-tuning methods capa-
ble of expression editing and those that are not. The entire
test set comprises 15subjects, as mentioned in Section §5.2,
with each subject assigned 11prompts containing emotion-
related words. The comprehensive results in Tab. 4 clearly
show that our method significantly outperforms the others.
Fig. 6 presents the experimental comparison results for ex-
pression editing, showcasing the versatility of our method.
5.5. Ablation Study
Impact of Identity Token. After creating prompts for
70,000 training images, we analyzed the subject identity
token for each image. The results, shown in Fig. 7, revealed
three categories of subject words: {“man”,“woman” },
{“person” }, and{“he”,“she” }. We tested each category’s
effectiveness after feature enhancement by converting other
identity tokens in each prompt to each experiment token.
When converting the ”person” token, we manually classi-
fied gender correctly for alignment. Our findings, presented
in Tab. 5, showed that the {“man”, “woman” }category, be-
ing more specific, improved subject fidelity and text-image
27086
Figure 8. Effects of using different upper limit of timesteps for
one-step reverse (left), visualization of noise addition at different
timesteps tand denoising (right).
Figure 9. The impact of truncating cross-attention only with
different values of β.
consistency. The category of {“he”, “she” },{“person” }
was less descriptive and consistent.
Impact of STEA. To investigate the influence of target fea-
tures obtained from a pre-trained face recognition model,
we conducted an ablation. When removing the STEA mod-
ule, we employed CLIP-image-encoder for training and ex-
tracting target features to enhance subject text embeddings.
The experimental results, as depicted in Tab. 6(a), clearly
indicate that utilizing a face feature extractor trained on a
large-scale dataset is significantly more effective compared
to training the image encoder.
Impact of DIP. Tab. 6(b) presents the ablation studies on
our proposed DIP. As the results show, the DIP module has
proven beneficial for identity preservation.
Impact of ECAC. To let the model focus on semantically
relevant subject regions within the cross-attention module,
we incorporate the attention map control. Tab. 6(c) indi-
cates this operation delivers a substantial performance im-
provement for identity preservation as well as prompt con-
sistency. Besides, as shown in Tab. 4, even when our cross-
attention control mechanism does not constrain the expres-
sion terms, we still achieve satisfactory results in facial ex-
pression editing. This further demonstrates the effective-
ness of our method in decoupling identity and attributes.
Hyperparameter Rt.As shown on the left side in Fig. 8,
when Rtgrows, the model trades off identity preservation
for improved editability. We select 250as the optimal Rt
value, as it provides a good balance. The right side of the
figure illustrates the visual results.
Hyperparameter βandγ.We studied the balance between
identity preservation and editability by solely adjusting the
βin the truncation process, keeping γat 0, to minimize their
impact. As shown in the Fig. 9, when βis in the range of
[0.8, 1], the difference in identity preservation is not signif-Mask Type Id Pres. ↑CLIP-TI ↑
Face Mask 0.657 0.245
Person Mask 0.623 0.229
Table 7. Impact of different types of masks. “Face Mask” refers
to the segmentation of only the facial area, while “Person Mask”
refers to the segmentation of the entire person’s body.
γandβcombination Id Pres. ↑CLIP-TI ↑
β= 0.8,γ= 0.1 0.657 0.245
β= 0.8,γ= 0.2 0.652 0.223
Table 8. The influence of different combinations of βandγ.
icant, but there is a noticeable change in editability. How-
ever, when βis less than 0.8, there is a sudden jump in iden-
tity preservation. We believe this is because the enhanced
face embeddings have a significant effect only on the fa-
cial region. Tab. 7 confirms our hypothesis. Therefore, we
chose βas 0.8 as our hyperparameter. According to Eqn.
3, the sum of cross-attention values for all tokens in each
region is 1, meaning for the facial region, β+γ <= 1,
so we conducted experiments with γvalues of 0.1 and 0.2.
In the Tab. 8, we found that while the difference in identity
preservation is not significant between the two values, there
is a substantial difference in editability. This is because fa-
cial responses include not only expressions but also features
like facial hair and accessories, etc. Hence, we select γas
0.1 as our hyperparameter.
6. Conclusion
In the portrait personalization field, we face the core chal-
lenge of proposing an efficient, low training cost, and high
identity preserving portrait personalization framework. In
this paper, we propose PortraitBooth, an efficient one-shot
text-to-portrait generation framework, that leverages Sub-
ject Text Embedding Augmentation and Dynamic Identity
Preservation to achieve robust identity preservation, using
Emotion-aware Cross-Attention Control to achieve expres-
sion editing, respectively. Experimental results demon-
strate the superiority of PortraitBooth over the state-of-the-
art methods, both quantitatively and qualitatively.
7. Acknowledgments
This work was supported by National Science and Tech-
nology Major Project of China (No. 2022ZD0118201) and
the National Natural Science Foundation of China (No.
62072386), supported by Henan Province key research and
development project (No.231111212000), Henan Center for
Outstanding Overseas Scientists (No. GZS2022011) and
the open foundation of Henan Key Laboratory of General
Aviation Technology (No. ZHKF-230212). We also thank
Digital Culture Lab of Tencent SSV for their invaluable
contribution during our research.
27087
References
[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 18208–18218, 2022. 2
[2] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-
Or, and Dani Lischinski. Break-a-scene: Extracting
multiple concepts from a single image. arXiv preprint
arXiv:2305.16311 , 2023. 2
[3] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended
latent diffusion. ACM Transactions on Graphics (TOG) , 42
(4):1–11, 2023. 3
[4] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,
Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,
and Xi Yin. Spatext: Spatio-textual representation for con-
trollable image generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 18370–18380, 2023. 1
[5] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.
Multidiffusion: Fusing diffusion paths for controlled image
generation. 2023. 7
[6] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and
Daniel Cohen-Or. Attend-and-excite: Attention-based se-
mantic guidance for text-to-image diffusion models. ACM
Transactions on Graphics (TOG) , 42(4):1–10, 2023. 1
[7] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune
Gwon, and Sungroh Yoon. Ilvr: Conditioning method for
denoising diffusion probabilistic models. arXiv preprint
arXiv:2108.02938 , 2021. 3
[8] Guillaume Couairon, Jakob Verbeek, Holger Schwenk,
and Matthieu Cord. Diffedit: Diffusion-based seman-
tic image editing with mask guidance. arXiv preprint
arXiv:2210.11427 , 2022. 2
[9] Zheng Ding, Xuaner Zhang, Zhihao Xia, Lars Jebe,
Zhuowen Tu, and Xiuming Zhang. Diffusionrig: Learning
personalized priors for facial appearance editing. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12736–12746, 2023. 3
[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618 , 2022. 2, 3, 6, 7
[11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,
Gal Chechik, and Daniel Cohen-Or. Encoder-based domain
tuning for fast personalization of text-to-image models. ACM
Transactions on Graphics (TOG) , 42(4):1–13, 2023. 3
[12] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin
Huang. Expressive text-to-image generation with rich text.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 7545–7556, 2023. 2
[13] Shaozhe Hao, Kai Han, Shihao Zhao, and Kwan-Yee K
Wong. Vico: Detail-preserving visual condition for
personalized text-to-image generation. arXiv preprint
arXiv:2306.00971 , 2023. 2
[14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 2
[15] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 1, 6
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 1
[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 2
[18] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu,
Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang.
Curricularface: adaptive curriculum learning loss for deep
face recognition. In proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
5901–5910, 2020. 3
[19] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. Advances in Neural Information Processing Sys-
tems, 35:26565–26577, 2022. 6
[20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6007–6017, 2023. 2
[21] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-
fusionclip: Text-guided diffusion models for robust image
manipulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2426–
2435, 2022. 2
[22] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1931–1941, 2023. 1, 2, 3, 6, 7
[23] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo.
Maskgan: Towards diverse and interactive facial image ma-
nipulation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5549–
5558, 2020. 6
[24] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.
Subject-diffusion: Open domain personalized text-to-image
generation without test-time fine-tuning. arXiv preprint
arXiv:2307.11410 , 2023. 2, 3, 4, 5, 6
[25] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. arXiv preprint arXiv:2108.01073 , 2021. 3
[26] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal
Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and
Daniel Cohen-Or. Mystyle: A personalized generative prior.
ACM Transactions on Graphics (TOG) , 41(6):1–10, 2022. 2
[27] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad-
wongsa, and Supasorn Suwajanakorn. Diffusion autoen-
coders: Toward a meaningful and decodable representation.
InProceedings of the IEEE/CVF Conference on Computer
27088
Vision and Pattern Recognition , pages 10619–10629, 2022.
3
[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 3
[29] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821–8831. PMLR, 2021.
1
[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 , 1
(2):3, 2022. 2
[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 1, 2, 6
[32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015. 3
[33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500–
22510, 2023. 2, 3, 6, 7
[34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,
Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,
and Kfir Aberman. Hyperdreambooth: Hypernetworks for
fast personalization of text-to-image models. arXiv preprint
arXiv:2307.06949 , 2023. 2, 3
[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479–36494, 2022. 2
[36] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A unified embedding for face recognition and clus-
tering. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 815–823, 2015. 6
[37] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting
Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual dif-
fusion: Continual customization of text-to-image diffusion
with c-lora. arXiv preprint arXiv:2304.06027 , 2023. 2
[38] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1921–1930, 2023. 2[39] Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv
Leviathan. Face0: Instantaneously conditioning a text-to-
image model on a face. arXiv preprint arXiv:2306.06638 ,
2023. 2, 3, 4, 6
[40] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro
Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,
and Thomas Wolf. Diffusers: State-of-the-art diffusion
models. https://github.com/huggingface/
diffusers , 2022. 6
[41] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei
Zhang, and Wangmeng Zuo. Elite: Encoding visual con-
cepts into textual embeddings for customized text-to-image
generation. arXiv preprint arXiv:2302.13848 , 2023. 1
[42] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr ´edo
Durand, and Song Han. Fastcomposer: Tuning-free multi-
subject image generation with localized attention. arXiv
preprint arXiv:2305.10431 , 2023. 2, 3, 4, 5, 6, 7
[43] Chao Xu, Jiangning Zhang, Yue Han, Guanzhong Tian, Xi-
anfang Zeng, Ying Tai, Yabiao Wang, Chengjie Wang, and
Yong Liu. Designing one unified framework for high-fidelity
face reenactment and swapping. In European Conference on
Computer Vision , pages 54–71. Springer, 2022. 2
[44] Chao Xu, Jiangning Zhang, Miao Hua, Qian He, Zili Yi, and
Yong Liu. Region-aware face swapping. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7632–7641, 2022. 2
[45] Chao Xu, Junwei Zhu, Jiangning Zhang, Yue Han, Wenqing
Chu, Ying Tai, Chengjie Wang, Zhifeng Xie, and Yong Liu.
High-fidelity generalized emotional talking face generation
with multi-modal emotion space learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6609–6619, 2023. 2
[46] Zhiliang Xu, Hang Zhou, Zhibin Hong, Ziwei Liu, Jiaming
Liu, Zhizhi Guo, Junyu Han, Jingtuo Liu, Errui Ding, and
Jingdong Wang. Styleswap: Style-based generator empow-
ers robust face swapping. In European Conference on Com-
puter Vision , pages 661–677. Springer, 2022. 2
[47] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuo-
fan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image
generation via large mixture of diffusion paths. arXiv
preprint arXiv:2305.18295 , 2023. 1
[48] Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Wei-
dong Cai, and Wayne Wu. Celebv-text: A large-scale facial
text-video dataset. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
14805–14814, 2023. 5
[49] Jiangning Zhang, Xianfang Zeng, Mengmeng Wang, Yusu
Pan, Liang Liu, Yong Liu, Yu Ding, and Changjie Fan.
Freenet: Multi-identity face reenactment. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 5326–5335, 2020. 2
[50] Jiangning Zhang, Xianfang Zeng, Chao Xu, and Yong Liu.
Real-time audio-guided multi-face reenactment. IEEE Sig-
nal Processing Letters , 29:1–5, 2021. 2
[51] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
Joint face detection and alignment using multitask cascaded
convolutional networks. IEEE signal processing letters , 23
(10):1499–1503, 2016. 6
27089
[52] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,
Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker:
Learning realistic 3d motion coefficients for stylized audio-
driven single image talking face animation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 8652–8661, 2023. 2
[53] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,
Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo,
Yaqian Li, Shilong Liu, et al. Recognize anything: A strong
image tagging model. arXiv preprint arXiv:2306.03514 ,
2023. 6
27090
