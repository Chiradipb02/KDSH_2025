OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising
Haichao Zhang
Northeastern University
360 Huntington Ave Boston MA 02115
zhang.haich@northeastern.eduYi Xu
Northeastern University
360 Huntington Ave Boston MA 02115
xu.yi@northeastern.edu
Hongsheng Lu
Toyota Motor North America
465 N Bernardo Ave Mountain View CA 94043
hongsheng.lu@toyota.comTakayuki Shimizu
Toyota Motor North America
465 N Bernardo Ave Mountain View CA 94043
takayuki.shimizu@toyota.com
Yun Fu
Northeastern University
360 Huntington Ave Boston MA 02115
yunfu@ece.neu.edu
Field of ViewObstructedOut-of-sightIn-View
Camera
Mobile Signal Receiver
Field of Mobile Signal
Obstruction (Wall)Noisy Sensor Trajectory (GPS)
Self-ReportingSelf-ReportingSelf-ReportingP1P2P3P4Real Trajectory (Not Exist)Trajectory
Real Trajectory (camera)
Obstructed
Figure 1. A representative illustration of real-world out-of-sight scenarios in autonomous driving. The autonomous vehicle is equipped
with a camera (capturing precise visual trajectories, indicated by green dotted arrows) and a mobile signal receiver (capturing noisy sensor
trajectories, represented by red dotted arrows) for tracking pedestrians and other vehicles. Pedestrians P1 and P2 are within the camera’s
ﬁeld of view, while P3 is entirely out of sight and P4 is obscured by other vehicles. Consequently, P3 and P4 lack captured visual trajectories
and are positioned dangerously, potentially crossing into the vehicle’s path, posing a risk of collision. The black dotted arrows depict the
hypothesized noise-free real trajectories, ideally captured by mobile sensors, contrasting with the actual noisy sensor trajectories (red
arrows). The gray area in the ﬁgure demarcates the visibility range of the mobile and visual modalities: white indicates no data captured,
orange signiﬁes the presence of visual trajectories, and blue represents the availability of mobile trajectories.
Abstract
Trajectory prediction is fundamental in computer vi-
sion and autonomous driving, particularly for understand-
ing pedestrian behavior and enabling proactive decision-
making. Existing approaches in this ﬁeld often assume pre-
cise and complete observational data, neglecting the chal-
lenges associated with out-of-view objects and the noise in-
herent in sensor data due to limited camera range, phys-
ical obstructions, and the absence of ground truth for de-
noised sensor data. Such oversights are critical safety con-cerns, as they can result in missing essential, non-visible
objects. To bridge this gap, we present a novel method for
out-of-sight trajectory prediction that leverages a vision-
positioning technique. Our approach denoises noisy sen-
sor observations in an unsupervised manner and precisely
maps sensor-based trajectories of out-of-sight objects into
visual trajectories. This method has demonstrated state-of-
the-art performance in out-of-sight noisy sensor trajectory
denoising and prediction on the Vi-Fi and JRDB datasets.
By enhancing trajectory prediction accuracy and address-
ing the challenges of out-of-sight objects, our work signif-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14802
icantly contributes to improving the safety and reliability
of autonomous driving in complex environments. Our work
represents the ﬁrst initiative towards Out-Of-Sight Trajec-
tory prediction (OOSTraj), setting a new benchmark for fu-
ture research.
1. Introduction
Trajectory prediction is pivotal in diverse research domains,
including computer vision, virtual reality, robotics, and au-
tonomous driving. This capability is fundamental for under-
standing human behavior in deep learning models and cru-
cial for enabling autonomous vehicles to anticipate pedes-
trian movements, thereby enhancing safety and preventing
collisions. Real-world applications, however, present com-
plex environments where trajectory prediction remains a
formidable challenge. While substantial progress has been
made in this ﬁeld, existing approaches heavily rely on com-
puter vision algorithms under the assumption of noise-free
and constantly visible observations. This assumption of-
ten overlooks the critical aspects of out-of-sight objects
and noisy data, which are prevalent in real-world scenar-
ios. Our research addresses this gap by proposing a vision-
positioning denoising method that enhances trajectory pre-
diction in these challenging conditions.
In the context of autonomous driving and robotics, the
limitations of camera-based systems pose signiﬁcant chal-
lenges. As shown in Fig. 1, one common scenario involves
pedestrians or vehicles being obscured by other vehicles,
walls, or falling into the vehicle’s blind spots. In such
cases, they may suddenly appear in the camera’s ﬁeld of
view, leaving insufﬁcient time for deep learning algorithms
to respond proactively. Additionally, the high speed of vehi-
cles exacerbates this issue. When a camera moves rapidly,
pedestrians can emerge from out-of-sight areas too quickly
for the camera to capture and process their trajectory in-
formation. In both scenarios, the absence of prior visual
data on pedestrians or vehicles hinders the trajectory predic-
tion capabilities of autonomous systems. This lack of out-
of-sight observations poses a serious safety risk and limits
the practical deployment of autonomous driving technol-
ogy in real-world situations. Our research aims to mitigate
these challenges by enhancing trajectory prediction through
vision-positioning denoising, enabling more reliable detec-
tion and response to such unpredictable occurrences.
Recent studies in trajectory prediction have increasingly
acknowledged the impediments posed by incomplete ob-
servations, a factor that signiﬁcantly hinders accurate pre-
dictions. While the bulk of existing research presumes the
availability of complete and precise observational data, sev-
eral noteworthy studies, such as those by Xu et al. [27] and
Fujii et al. [6], have begun to address this challenge. These
works predominantly focus on imputing missing views ordirectly predicting from partial trajectory data. A notable
method by Zhang et al. [28] demonstrates the prediction of
layout trajectories using just a single visual timestamp, sup-
ported by mobile sensor modalities. However, these studies
share a common limitation: their dependency on a mini-
mum quantum of visual observational data for trajectory
prediction. To the best of our knowledge, there is an ab-
sence of research dedicated to predicting trajectories solely
from out-of-sight observations. This research gap is not
only signiﬁcant in the academic ﬁeld but also constitutes an
urgent safety issue in autonomous driving, a concern that
our research directly addresses.
Addressing the out-of-sight problem in trajectory pre-
diction presents two key challenges. The initial challenge
involves coping with noisy sensor measurements. In the
absence of visual timestamps, our approach relies on com-
monly used localization sensors, such as GPS and odome-
ters, to provide a basic spatial location. This reliance is ne-
cessitated by the nature of the out-of-sight problem, where
traditional visual inputs are not available. However, as es-
tablished in prior research, these mobile sensor modalities
are subject to signiﬁcant noise. GPS, for example, can ex-
hibit errors ranging from 1 to 4 meters [10], while odome-
ters are prone to drift noise accumulation [29]. This noise
signiﬁcantly hampers the accuracy of sensor-derived trajec-
tory predictions. Furthermore, the absence of ground truth
for denoised sensor data complicates the situation, as it pre-
vents us from employing supervised learning methods for
denoising. Therefore, our approach involves training a de-
noising model in an unsupervised manner.
To mitigate this issue, we propose the use of visual in-
formation for denoising sensor data. Although our scenario
lacks direct visual observations, object tracking algorithms
can offer highly accurate and noise-free location data in im-
age coordinates. By integrating this visual information, we
aim to reﬁne and denoise the trajectory data derived from
the noisy sensor inputs. However, direct visual observation
of out-of-sight subjects is not possible, we need to establish
a connection between sensor data and visual coordinates.
This brings us to our second challenge.
However, since the pedestrain is out-of-sight, to do such
a denoising method with vision-positioning, another chal-
lenge is there is no visual reference even if we assume the
localization is precise. To solve this challenge, we have to
ﬁnd a localization-vision mapping to solve the visual ref-
erence problem. Luckily, since the camera’s intrinsic ma-
trix is not changing within a frame or is not moving, we
can build such localization-vision mapping by estimating
the camera’s intrinsic matrix. Since the pedestrians and ve-
hicles shares the same camera’s intrinsic matrix in the cam-
era’s view. To best utilize the information within camera
frames, through analysis of other visible pedestrians and
vehicle movements between sensor localization trajectories
14803
and visual trajectories, we can build a model to predict the
mapping relationship which is highly associated with the
camera’s intrinsic matrix. Nevertheless, another beneﬁt of
this design is, that if the camera is not moving in some
widely used applications like video surveillance [8, 30]
or trafﬁc monitoring [11], we can infer the localization-
vision mapping through pedestrians and vehicles in previ-
ous frames.
The contributions of this paper are as follows:
We introduce a pioneering task in the ﬁeld of trajectory
prediction - predicting the noise-free visual trajectory of
out-of-sight objects using noisy sensor trajectories. This
task (OOSTraj) addresses a critical gap in current re-
search and extends the capabilities of trajectory predic-
tion in complex environments.
Our work proposes an innovative vision-positioning de-
noising module. This module adeptly leverages precise
visual data to effectively denoise sensor data with an un-
supervised manner of no need for the ground truth of de-
noised sensor data through the camera matrix estimator,
providing a much-needed visual reference for objects that
are out of the camera’s view. This approach marks a sig-
niﬁcant advancement in combining sensor and visual data
for enhanced prediction accuracy.
Through rigorous ablation studies and plug-and-play ex-
periments, we demonstrate that our method achieves
state-of-the-art results on out-of-sight noisy sensor trajec-
tory denoising and prediction over baselines. These stud-
ies provide concrete evidence of the effectiveness and ef-
ﬁciency of our proposed approach.
2. Related Works
2.1. Vision-Wireless Fusion
Vision-wireless fusion represents an interdisciplinary ap-
proach that synergizes the strengths of visual and wireless
(mobile) modalities to tackle inherent challenges in each.
The visual modality, while offering detailed and direct ob-
servation capabilities, grapples with limitations such as ob-
structions, out-of-sight issues, computational intensity, and
a restricted observational range. On the other hand, the
wireless modality, characterized by its extensive range of
sensing and communication, encounters signiﬁcant issues
with sensor noise [15] and environmental disturbances [5].
A persistent challenge in wireless data is the lack of ground
truth, compounded by the difﬁculty in human labeling due
to the non-visual nature of the signals.
Despite these challenges, the fusion of visual and wire-
less data presents a powerful tool. The visual modality,
with its ease of visualization and labeling, coupled with
advancements in computer vision algorithms, offers robust
and precise supervision. This fusion has seen limited but
impactful applications in recent studies. For instance, Liuet al. [16] utilized wireless trajectories to address person
reidentiﬁcation problems, accounting for variations in pos-
ture and clothing. Alahi et al. [1] (2015) employed RGB-D
cameras to enhance indoor localization accuracy by estimat-
ing distances using Wi-Fi signals from smartphones. Pa-
paioannou et al. [19] focused on tracking people across vi-
sual and wireless modalities in dynamic industrial environ-
ments. These pioneering works have signiﬁcantly advanced
the ﬁeld by fusing visual and mobile modalities to improve
task accuracy. However, less attention has been paid to
leveraging the visual modality for denoising noisy sensor
trajectories, especially for out-of-sight agents, an area our
work aims to explore.
2.2. Obstructed Trajectory Prediction
The ﬁeld of obstructed trajectory prediction, while closely
related, is distinct from out-of-sight trajectory prediction,
which remains underexplored. Trajectory prediction has
widespread applications in robotics [12], autonomous driv-
ing [4], and human behavior understanding [21]. Many ex-
isting studies [18] operate under the assumption of complete
and unobstructed observations. However, recent research
has begun to acknowledge and address the challenges posed
by observation obstructions. Methods such as data impu-
tation [27] and modality fusion [28] have been proposed
to tackle these issues. Nonetheless, these approaches typi-
cally rely on having at least some observational data, leav-
ing a gap in the context of completely out-of-sight trajectory
prediction. In real-world scenarios, noisy and out-of-sight
trajectories are commonplace, presenting a signiﬁcant chal-
lenge for existing systems. Our work seeks to bridge this
gap by employing vision positioning and denoising tech-
niques for trajectory prediction, addressing a crucial need
in the ﬁeld.
3. Problem Deﬁnition
3.1. Symbol Annotations
Consider a set of Nagents, represented as Pn; n=
1;2; : : : ; N , observed over timestamps t1:t2, starting from
tsand ending at te. Agents visible to the camera, Pi, con-
stitute the set Pin, while those out-of-sight, Po, form the set
Pout. The sensor-based noisy trajectory of the n-th agent
between t1andt2is denoted by St1:t2
Pn, which ideally corre-
sponds to a precise but non-existent real localization tra-
jectory, ^St1:t2
Pn. All agents possess a noisy sensor trajec-
torySts:te
Pn, signiﬁcantly inﬂuenced by sensor measurement
noise. Concurrently, in-view agents Pi; i2Pin, are tracked
through visual trajectories Vts:te
Piin camera frames.
3.2. Task Deﬁnition
Our primary objective, given the sole input of noisy sensor
trajectories Sts:te
Po, is twofold:
14804
Field of View
Transformer
P1P3P4P2P3P1P2
Visual ModalityMobile Modality
TransformerTransformerMobile Denoising Encoder
Camera Parameters EstimatorOut-of-Sight Prediction DecoderDenoised Signal 
Camera Matrix EmbeddingFuture Visual Trajectoryℒ!"#$%&"ℒ'(")Visual Positioning ProjectionFigure 2. Overview of the Vision-Positioning Denoising and Predicting Model architecture. This illustration highlights the processing of
pedestrian data, where pedestrians P3 and P4 are detectable only by mobile receivers, while P1 and P2 are visible to both camera and
mobile receivers. The Camera Parameters Estimator Module utilizes the dual-modality trajectories of in-view pedestrians (like P1 and P2)
to analyze the relationship between camera and world coordinates, resulting in a camera matrix embedding. For out-of-sight pedestrians
(e.g., P3, P4), their noisy mobile trajectories are ﬁrst reﬁned by the Mobile Denoising Encoder, producing a denoised signal embedding.
This embedding is then merged with the matrix embedding in the Visual Positioning Projection Module, facilitating the mapping of data
into camera coordinates, with the application of LDenoise . Finally, the Out-of-Sight Prediction Decoder leverages the denoised visual
signals to predict the trajectories of pedestrians not captured by the camera.
1. Denoise the noisy sensor trajectory Sts:te
Powithout direct
supervision, addressing the potential impact of sensor
noise on prediction accuracy.
2. Predict noise-free future visual trajectories Vte:tp
Pofor
out-of-sight agents Poin camera frames, spanning from
the observation’s end timestamp teto the target predic-
tion timestamp tp, using only the provided noisy sensor
trajectory data.
It is important to note that Polacks visual trajectories and
references due to being out-of-sight. However, we can
leverage other available signals, as outlined in Sec. 3.1, to
navigate complex environmental factors.
Our strategy involves initially denoising and projecting
the noisy sensor trajectory into a visual trajectory in an un-
supervised manner, constructing supervision from the avail-
able signals. Subsequently, another module is employed
to execute precise trajectory prediction within the visual
modality.
4. Methodology
The overall architecture of our proposed method is detailed
in Fig. 2. Our approach is speciﬁcally designed to predict
noise-free future trajectories for out-of-sight agents, utiliz-
ing noisy sensor data without relying on direct denoising
supervision or visual references.
To accurately predict a noise-free trajectory from noisy
sensor data, it is crucial to establish an unsupervised de-
noising pipeline. This pipeline is essential for ﬁltering out
noise before it can impact the trajectory prediction model.
Addressing this complex challenge necessitates the integra-
tion of visual references and the construction of effective
denoising supervision. We have divided this process into
several distinct steps to ensure comprehensive coverage of
all aspects involved in this sophisticated task.4.1. Mobile Denoising Encoder (MDE)
At the outset of our methodology, we introduce the Mobile
Denoising Encoder (MDE), designed to operate under an
ideal scenario where accurate supervision is available. The
MDE’s primary function is to denoise noisy sensor trajec-
tories of out-of-sight agents, leveraging appropriate super-
visory signals for accuracy.
Structurally, the MDE comprises a Transformer model,
ﬂanked by two fully connected layers, both preceding and
succeeding the Transformer layers. This architecture is cho-
sen for its efﬁcacy in handling sequential data and its capac-
ity for capturing complex dependencies within the sensor
trajectories.
In an optimal setting, the MDE processes the noisy sen-
sor trajectories, denoted by Sts:te
Po, as its input. The ob-
jective is to reﬁne these inputs by ﬁltering out the noise,
thereby producing what we term ’noise-free real sensor tra-
jectories’, represented by ^Sts:te
Po. The operational equation
of the MDE is formalized as follows:
^Sts:te
Po=Edenoise (Sts:te
Po); oPout; (1)
where the Edenoise is the MDE, the Poutis the set of out-
of-sight agents, and the Pois an out-of-sight agent sampled
fromPout.
However, the practical implementation of the Mobile
Denoising Encoder (MDE) faces a signiﬁcant challenge:
the ideal supervision in the form of noise-free real sensor
trajectories, ^Sts:te
Po, does not exist in real-world scenarios.
The inherent nature of sensor measurement noise compli-
cates the acquisition of such ideal data. Theoretically, one
could envisage using higher-accuracy sensor devices to col-
lect comparable data under identical conditions. However,
this approach is impractical for most datasets and appli-
cation scenarios. Even if it were feasible, these higher-
14805
accuracy sensors would likely still be subject to some level
of measurement noise, albeit reduced. As a result, com-
pletely eliminating noise to construct a truly noise-free real
sensor trajectory for supervision is not achievable with the
current state of sensor technology.
Consequently, we are compelled to explore alternative
methods for constructing effective noise-free supervision
for the denoising process described in Eq. 1. This involves
developing innovative approaches that can approximate the
ideal noise-free conditions, despite the inherent limitations
of existing sensor technologies.
4.2. Visual-Positioning Denoising Module (VPD)
In the Visual-Positioning Denoising Module (VPD), we
leverage visual information to establish noise-free supervi-
sion for the Mobile Denoising Encoder (MDE). This ap-
proach is a response to the challenges posed by the noisy
sensor modality, which is inherently prone to sensor mea-
surement noise and difﬁcult to reﬁne. Visual information,
on the other hand, beneﬁts from the advancements in object
tracking algorithms, offering signiﬁcantly higher precision.
Additionally, the nature of visual image processing, where
images are discretized into integers during the rasterization
process, results in visual trajectories that are highly precise,
easily labelable, and effectively noise-free.
Despite the inherent noise-free quality of visual trajec-
tories and their ideal use as supervision in trajectory pre-
diction, a major challenge arises when dealing with out-of-
sight object trajectories, which lack visual data. To address
this, it is necessary to ﬁnd a method to translate the denoised
sensor trajectories into the visual modality effectively. This
translation process is broken down into several key compo-
nents: the Visual Positioning Projection Module (discussed
in Sec. 4.3), the Camera Parameters Estimator (outlined in
Sec. 4.4), and the Denoising Loss (described in Sec. 4.5).
Each of these components plays a crucial role in ensuring
the accurate mapping of sensor data into the visual domain,
which is vital for the success of our proposed method.
4.3. Visual Positioning Projection Module (VPP)
The Visual Positioning Projection Module (VPP) plays a
critical role in transforming noisy sensor trajectories into a
visual format. As all visual trajectories are inherently cap-
tured by cameras, this module focuses on converting 3D
world coordinates, represented by noisy sensor data, into
2D coordinates within the camera’s frame. This conversion
is fundamental to aligning sensor-based data with visual
data, and it hinges on the principles of geometric camera
calibration [24].
The World-To-Camera Transformation, central to this
module, is designed to compute a 2D point p2R21
in the camera frame based on a corresponding 3D point
P2R31in world coordinates. To facilitate this pro-cess, the height dimension in these coordinates is usually
kept constant, simplifying the transformation from 3D to
2D space. The mathematical formulation of this transfor-
mation is as follows:
[p;1]T=wKRt[P;1]T; (2)
where K2R33denotes the camera’s intrinsic matrix,
which includes parameters like the focal length and optical
center that are inherent to the camera. The scale factor wad-
justs the scale of the transformation to ﬁt the camera’s spe-
ciﬁc dimensions. Additionally, Rt2R34represents the
extrinsic rotation and translation matrix, deﬁning the cam-
era’s position and orientation relative to the 3D world.
To streamline this transformation process, we introduce
a simpliﬁcation by employing a matrix M2R34. This
matrix Mis formulated by combining the intrinsic matrix
K, scale factor w, and the extrinsic matrix Rt. This consol-
idation is critical as it encapsulates all the necessary param-
eters and transformations required to convert a point from
world coordinates into camera coordinates. The simpliﬁed
form of the World-To-Camera Transformation function can
be expressed as:
M=wKRt; (3)
[p;1]T=M[P;1]T: (4)
To adapt the World-To-Camera Transformation Eq. 2
into trajectory denoising, we need to consider the camera
status in this process. If the visual trajectory is captured
by a surveillance camera, the RtinMwill not change for
engaging of timestamps, but considering the scenarios that
visual trajectory is captured by a moving camera on a mo-
bile device, we need to consider Mis changing for different
timestamps. So, we deﬁne the sequence of Mas Camera
Matrix Embedding Mts:te2R(te ts)34, which can be
a formula as,
Mts:te=wKRts:te
t; t2[ts; te]; (5)
in which Rts:te
t andMts:teare extrinsic rotation & transla-
tion matrix and Camera Matrix Embedding in timestamps
between tsandte.
We regarded noise-free real sensor trajectory ^Sts:te
Poas a
sequence of 3D world coordinates P2R31. So, combing
Eq. 4 and Eq. 5, the Visual Positioning Projection function
to map ^Sts:te
Pointo visual modality can be deﬁned as,
Vts:te
Po=Mts:te^Sts:te
Po; t2[ts; te]; oPout;(6)
in which the Vts:te
Pois the visual trajectory for out-of-sight
agent Po.
14806
4.4. Camera Parameters Estimator (CPE)
Although we have the Visual Positioning Projection in Eq. 6
to map noise-free sensor trajectory into the visual trajec-
tory, the camera matrix is still not accessible because most
datasets do not include the camera matrix, and it cannot
be obtained from images directly. Moreover, consider-
ing the camera is moving in many practical scenarios like
autonomous driving, mobile robots, and smartphones, the
camera matrix may change in this process. That makes ob-
taining the camera matrix sequence extremely hard.
However, since there are many in-view agents captured
by the same camera in the same timestamp and have both
visual and sensor trajectories, we propose a Camera Param-
eter Estimator to analyze the relationship between the visual
and sensor trajectories. Because that relationship is highly
related to the camera parameters as shown in Eq. 6, we pre-
dict camera matrix embedding Mts:tefrom all the pairs of
in-sight visual and sensor trajectories, with the Camera Pa-
rameter Estimator. The Camera Parameter Estimator con-
sists of a transformer with a fully connected layer before
and after. The prediction process can be denoted as
Mts:te=Ecpe(Vts:te
P0; Sts:te
P0; :::; Vts:te
Pi; Sts:te
Pi); i2Pin;
(7)
where the Vts:te
Pi,Sts:te
Piare the visual and sensor trajectories
of in-sight agents Pi, respectively. The Pinis the set of in-
sight agents. Ecpeis the Camera Parameters Estimator.
4.5. Denoising Loss
In the last subsection, we obtain the visual trajectory of the
out-of-sight agent with Eq. 7, since we don’t have ground
truth for noisy sensor trajectories and the visual modality
is noise-free and easy to obtain or label as we described in
Sec. 4.2, we can use the process to construct a supervision
for the Mobile Denoising Encoder and Camera Parameters
Estimator. The denoising loss function LDenoise can be for-
mulated as,
LDenoise =L2(Vts:te
Po;Vts:te
Po); (8)
whereL2is the L2 [2] loss. The Vts:te
PoandVts:te
Poare the
predicted visual trajectories of out-of-sight agents and the
ground truth. Please note that the ground truth of the visual
trajectories of out-of-sight agents is only available during
training.
4.6. Out-of-Sight Prediction Decoder (OPD)
We propose an Out-of-Sight Prediction Decoder to predict
future visual trajectories from the predicted out-of-sight vi-
sual trajectories in Eq. 7, to avoid predicting directly from
noisy sensor trajectories. We applied a transformer modelDataset Vi-Fi Dataset [14] JRDB Dataset [17]
Baselines SUM#MSE-D#MSE-P#SUM#MSE-D#MSE-P#
ViTag [13] 200.90 100.53 100.37 143.08 71.23 71.85
Vanilla LSTM [9, 23] 116.01 58.31 57.70 56.05 27.98 28.07
Vanilla GRU [3, 25] 57.34 28.69 28.65 71.91 35.83 36.07
Vanilla RNN [20, 22] 31.61 15.92 15.69 112.40 56.00 56.41
Vanilla Transformer [7, 26] 28.33 14.26 14.08 33.37 16.71 16.66
Ours 27.24 13.42 13.83 25.51 10.52 14.99
Table 1. Experiments of Quantitative Comparison of Models
as the prediction module for simplicity. This process can be
formulated as
Vts:tp
Po=Dpred(Vte:tp
Po); oPout; (9)
where the Vte:tp
Pois the observation of predicted visual tra-
jectory of out-of-sight agent Pobetween timestamps [ts:
te], and the Vts:tp
Pois the predicted future visual trajectory
between timestamps [te:tp].Dpred is the Out-of-Sight
Prediction Decoder. Poutis the set of out-of-sight agents.
We applied a prediction loss LPred for this process,
LPred=L2(Vts:tp
Po;Vts:tp
Po); (10)
where theL2is the L2 loss. Vts:tp
PoandVts:tp
Poare the pre-
dicted and ground truth of future trajectories between tsand
tptimestamp.
4.7. Implementation Details
The training samples of out-of-sight agents are randomly
sampled from all pedestrians in a scenario, all other com-
pleted visible agents in both visual and noisy modalities are
regarded as in-sight agents for the estimating of the camera
parameters estimator.
The wireless data referenced in our study pertains to sen-
sor localization signals, such as noisy GPS or odometers,
typically collected by mobile phones carried by pedestrians.
The vehicle, equipped with both a camera and a wireless re-
ceiver, captures sensor and visual trajectories. This data is
then fed into our model for the prediction.
5. Experiments
5.1. Datasets
Vi-Fi Multimodal Dataset [14]. The Vi-Fi dataset is
a comprehensive multimodal dataset designed for vision-
wireless systems, particularly focusing on linking pedes-
trian identities across visual and wireless modalities. It fea-
tures wireless data captured from smartphones carried by
pedestrians, encompassing technologies like FTM, IMU,
and noisy GPS. Additionally, an RGB-D camera surveil-
lance system, accompanied by a wireless receiver, is de-
ployed either on a wall or on a bicycle to record pedestrian
bounding boxes. This setup ensures simultaneous acquisi-
tion of both visual and wireless data. Given the nature of
14807
the data collection process, the wireless data exhibits con-
siderable noise, a factor that cannot be overlooked. This
dataset comprises 90 sequences, each lasting approximately
3 minutes, and includes both indoor and outdoor scenarios.
Indoor data were collected with ﬁve legitimate users and
without any passersby, while outdoor data involved 3 actual
users and 12 passersby. For our experiments, we utilized
the noisy GPS signals from this dataset as representative of
noisy mobile trajectories and the visual points as the visual
modality. To simulate out-of-sight scenarios, we selectively
obscured one pedestrian in each sequence.
JackRabbot Dataset (JRDB) [17]. The JRDB dataset, col-
lected by the social mobile robot JackRabbot, is designed
for autonomous robot navigation and social robotics studies
within human environments. It features 60,000 annotated
frames, including 2.4 million 2D and 2.8 million 3D hu-
man bounding box annotations, captured using 360 degree
RGB (5 cameras) and LIDAR point cloud technology. This
dataset encompasses both obstructed and visible pedestri-
ans, with the potential for each camera to have blind spots
relative to the others. The bounding boxes are manually la-
beled, albeit at a lower annotation rate, and are subject to
noise due to upsampling via linear interpolation. Size and
location estimates of obstructed pedestrians in point clouds
are manually conducted. For our experiments, we use the
center points of 3D bounding boxes as noisy sensor trajec-
tories and visual points as visual trajectories. To simulate
out-of-sight scenarios, we randomly obscured a pedestrian,
using the trajectories of several other visible pedestrians to
estimate camera parameters in our model.
5.2. Evaluation Setup
In both the JRDB and Vi-Fi datasets, we observe 100 times-
tamps of out-of-sight trajectories and predict the subsequent
100 timestamps in the visual modality. Simultaneously, we
input 100 timestamp pairs of in-sight visual trajectories and
noisy sensor signals into our camera parameters estimator
to obtain the camera matrix embeddings.
5.3. Metrics
As the ﬁrst study to denoise out-of-sight sensor trajectories
into a visual modality for noise-free trajectory prediction,
we adapt the Mean Square Error per Timestamp (MSE-T)
metric from [28]. This metric calculates the average pixel
distance over the timestamp dimension. Given our task’s
dual focus on denoising and prediction performance, we ap-
ply MSE-T to both projected out-of-sight visual trajectories
and predicted visual trajectories, denoting these as MSE-D
andMSE-P , respectively. Additionally, we introduce the
SUM metric, which aggregates MSE-D and MSE-P, to pro-
vide an overall score balancing denoising and prediction ef-
ﬁcacy.5.4. Baselines
To benchmark our method in noisy sensor trajectory denois-
ing and out-of-sight trajectory prediction, we select several
baselines, encompassing fundamental models like Vanilla
LSTM [23], Vanilla RNN [20], Vanilla GRU [25], and
Vanilla Transformer [7]. We also include ViTag [13], the
state-of-the-art method on the Vi-Fi dataset, modiﬁed to
align with our task outputs.
5.5. Quantitative Comparison Experiments
We conducted quantitative experiments to compare our
method’s performance against ﬁve baselines across two
datasets and three metrics, as detailed in Table. 1. Our
model outperforms the baselines, showcasing its superior-
ity. Notably, although ViTag is recognized as the state-of-
the-art on the Vi-Fi dataset for visual and wireless identity
association, it underperforms in our task. A key ﬁnding is
that our model demonstrates more pronounced superiority
in the denoising task than in the prediction task, underscor-
ing the complexity and difﬁculty in denoising noisy sensor
trajectories and mapping out-of-sight objects into the visual
modality. Furthermore, a positive correlation between de-
noising and prediction performance is observed among the
baselines. Improved denoising leads to enhanced predic-
tion accuracy, validating our approach of focusing on sensor
data denoising to facilitate better out-of-sight visual trajec-
tory prediction.
We also note varying performances of the baselines
across different datasets. For instance, Vanilla RNN ex-
cels in the Vi-Fi dataset but falls short in the JRDB dataset,
likely due to differing sensor noise characteristics in these
datasets. Our vision-positioning denoising model, tailored
to handle various sensor noises with precise visual modality
supervision, consistently achieves robust denoising perfor-
mance across different datasets and scenarios. This hypoth-
esis is further explored in our Plug-and-Play experiment.
5.6. Plug and Play Experiments
In the Plug-and-Play experiment, outlined in Table. 2, we
assess whether our vision-positioning denoising model en-
hances the performance of baselines in both noisy sensor
trajectory denoising and future visual trajectory prediction.
Baselines were modiﬁed into a 2-stage format for a com-
parative analysis. The ﬁrst stage focuses on denoising, fol-
lowed by prediction in the second stage. The ”+ VPD” row
features our vision-positioning denoising (VPD) model, as
described in Sec. 4.2, concatenated with the baseline to
evaluate the added improvement.
Results in Table. 2 indicate that incorporating our VPD
model signiﬁcantly enhances baseline performance in both
denoising and prediction tasks. Notably, Vanilla RNN,
which struggles with JRDB’s sensor noise, shows marked
improvement in the Vi-Fi dataset when paired with our
14808
Dataset Vi-Fi Dataset [14] JRDB Dataset [17]
Baselines Add Module SUM#MSE-D#MSE-P#SUM#MSE-D#MSE-P#
Vanilla LSTM [9, 23]+ 2 Stage 118.44 38.85 79.59 81.22 39.93 41.28
+ VPD (Ours) 30.39 13.76 16.63 53.16 11.36 41.80
Vanilla RNN [20, 22]+ 2 Stage 32.16 17.51 14.65 138.41 103.97 34.44
+ VPD (Ours) 27.78 13.56 14.22 31.32 12.31 19.01
Vanilla GRU [3, 25]+ 2 Stage 40.35 16.88 23.48 56.56 17.90 38.66
+ VPD (Ours) 28.47 13.69 14.78 31.70 11.53 20.16
Vanilla Transformer [7, 26]+ 2 Stage 28.87 14.22 14.65 36.99 14.21 22.79
+ VPD (Ours) 27.24 13.42 13.83 25.51 10.52 14.99
Table 2. Experiment of Plug-and-Play on the Vi-Fi and JRDB Datasets.
Dataset Vi-Fi Dataset [14] JRDB Dataset [17]
Module Components SUM#MSE-D#MSE-P#SUM#MSE-D#MSE-P#
w/o CPE in Sec. 4.4 32.12 17.01 15.12 29.66 14.35 15.31
w/o MDE in Sec. 4.1 32.33 17.65 14.68 32.94 17.90 15.03
w/o VPP in Sec. 4.3 28.42 14.05 14.37 32.48 14.45 18.03
w/o OPD in Sec. 4.6 27.33 13.65 13.68 30.01 14.99 15.02
Full Model 27.24 13.42 13.82 25.51 10.52 14.99
Table 3. Ablation Study of Module Components.
VPD. This improvement in denoising subsequently boosts
prediction accuracy. These ﬁndings afﬁrm the efﬁcacy of
our model in augmenting denoising and prediction capabil-
ities of existing models in a plug-and-play approach.
5.7. Ablation Study
An ablation study was conducted to evaluate the contribu-
tion of each module component. This was achieved by sys-
tematically removing each component, with the results pre-
sented in Table. 3. The study reveals that each component
plays a vital role in the overall performance of the model.
The absence of the Camera Parameter Estimator (CPE)
leads to a signiﬁcant performance drop. Without the CPE,
the model struggles to establish the visual-positioning re-
lationship through camera matrix estimation, reducing it to
a mere two-stage encoder-decoder architecture akin to the
baselines. Similarly, the removal of the Mobile Denoising
Encoder (MDE) results in an even more pronounced decline
in performance. Without the MDE, the model faces chal-
lenges in denoising the mobile noisy trajectories, inadver-
tently allowing noise to propagate into the Visual Position-
ing Projection (VPP) process. The VPP module, responsi-
ble for projecting out-of-sight sensor trajectories into visual
trajectories, proves to be crucial. The notable performance
drop in scenarios without VPP underscores the importance
of explicitly incorporating knowledge from geometric cam-
era calibration into the model. Additionally, the absence
of an out-of-sight prediction decoder slightly reduces the
model’s effectiveness, as it then needs to infer the entire se-
quence during the denoising process. Overall, this ablation
study clearly demonstrates the necessity and signiﬁcance ofeach component within our model for achieving noise-free
out-of-sight trajectory prediction.
6. Conclusion
Our study tackles a signiﬁcant challenge in trajectory pre-
diction, speciﬁcally in situations where agents are entirely
out of sight, making visual observations unfeasible. We in-
troduce an innovative task that focuses on predicting the
visual trajectories of such out-of-sight agents by utilizing
noisy sensor data. The cornerstone of this task is our Visual-
Positioning Denoising Module, which adeptly addresses the
absence of visual references. This is achieved through so-
phisticated camera calibration techniques, enabling us to
estimate camera matrix sequences and establish a vision-
positioning mapping. This method effectively denoises sen-
sor trajectories without relying on direct supervision, in-
stead utilizing supervision constructed from the visual po-
sitioning projection.
Our experimental results robustly demonstrate the effec-
tiveness of our proposed pipeline and architecture, signiﬁ-
cantly outperforming existing methods in predicting trajec-
tories of agents that are not visible. The inclusion of visual
positioning in the denoising process markedly improves the
accuracy of our trajectory predictions. To the best of our
knowledge, this is the ﬁrst study to successfully predict out-
of-sight trajectories using noisy sensor data and to employ
vision-positioning projection for the purpose of denoising
these trajectories. The methodologies and insights gleaned
from our work pave the way for further research and devel-
opment in the trajectory prediction ﬁeld, especially in real-
world scenarios where complete visibility of agents is not
guaranteed, or where sensor data are inherently noisy and
challenging to denoise.
7. Acknowledgements
We gratefully acknowledge the support and sponsorship
provided by Toyota Motor North America for this research.
14809
References
[1] Alexandre Alahi, Albert Haque, and Li Fei-Fei. Rgb-w:
When vision meets wireless. In Proceedings of the IEEE
International Conference on Computer Vision , pages 3289–
3297, 2015.
[2] Peter B ¨uhlmann and Bin Yu. Boosting with the l 2 loss:
regression and classiﬁcation. Journal of the American Sta-
tistical Association , 98(462):324–339, 2003.
[3] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and
Yoshua Bengio. Empirical evaluation of gated recurrent
neural networks on sequence modeling. arXiv preprint
arXiv:1412.3555 , 2014.
[4] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou,
Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schnei-
der, and Nemanja Djuric. Multimodal trajectory predictions
for autonomous driving using deep convolutional networks.
In2019 International Conference on Robotics and Automa-
tion (ICRA) , pages 2090–2096. IEEE, 2019.
[5] Corrado Di Natale, Eugenio Martinelli, and Arnaldo
D’Amico. Counteraction of environmental disturbances
of electronic nose data by independent component analy-
sis. Sensors and Actuators B: Chemical , 82(2-3):158–165,
2002.
[6] Ryo Fujii, Jayakorn V ongkulbhisal, Ryo Hachiuma, and
Hideo Saito. A two-block rnn-based trajectory prediction
from incomplete trajectory. IEEE Access , 9:56140–56151,
2021.
[7] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio
Galasso. Transformer networks for trajectory forecasting. In
2020 25th international conference on pattern recognition
(ICPR) , pages 10335–10342. IEEE, 2021.
[8] Niels Haering, P ´eter L Venetianer, and Alan Lipton. The
evolution of video surveillance: an overview. Machine Vision
and Applications , 19(5):279–290, 2008.
[9] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term
memory. Neural computation , 9(8):1735–1780, 1997.
[10] Jihua Huang and H-S Tan. A low-order dgps-based vehicle
positioning system under urban environment. IEEE/ASME
Transactions on mechatronics , 11(5):567–575, 2006.
[11] Neeraj Kumar Jain, RK Saini, and Preeti Mittal. A review on
trafﬁc monitoring system techniques. Soft computing: The-
ories and applications: Proceedings of SoCTA 2017 , pages
569–577, 2019.
[12] Nikolay Jetchev and Marc Toussaint. Trajectory prediction:
learning to map situations to robot trajectories. In Proceed-
ings of the 26th annual international conference on machine
learning , pages 449–456, 2009.
[13] Hansi Liu, Abrar Alali, Mohamed Ibrahim, Hongyu Li,
Marco Gruteser, Shubham Jain, Kristin Dana, Ashwin
Ashok, Bin Cheng, and Hongsheng Lu. Lost and found! as-
sociating target persons in camera surveillance footage with
smartphone identiﬁers. In Proceedings of the 19th Annual
International Conference on Mobile Systems, Applications,
and Services , pages 499–500, 2021.
[14] Hansi Liu, Abrar Alali, Mohamed Ibrahim, Bryan Bo Cao,
Nicholas Meegan, Hongyu Li, Marco Gruteser, ShubhamJain, Kristin Dana, Ashwin Ashok, et al. Vi-ﬁ: Associat-
ing moving subjects across vision and wireless sensors. In
2022 21st ACM/IEEE International Conference on Informa-
tion Processing in Sensor Networks (IPSN) , pages 208–219.
IEEE, 2022.
[15] Sijia Liu, Sundeep Prabhakar Chepuri, Makan Fardad, En-
gin Mas ¸azade, Geert Leus, and Pramod K Varshney. Sen-
sor selection for estimation with correlated measurement
noise. IEEE Transactions on Signal Processing , 64(13):
3509–3522, 2016.
[16] Yiheng Liu, Wengang Zhou, Mao Xi, Sanjing Shen, and
Houqiang Li. Vision meets wireless positioning: Effective
person re-identiﬁcation with recurrent context propagation.
InProceedings of the 28th ACM International Conference
on Multimedia , pages 1103–1111, 2020.
[17] Roberto Martin-Martin, Mihir Patel, Hamid Rezatoﬁghi,
Abhijeet Shenoi, JunYoung Gwak, Eric Frankel, Amir
Sadeghian, and Silvio Savarese. Jrdb: A dataset and bench-
mark of egocentric robot visual perception of humans in built
environments. IEEE transactions on pattern analysis and
machine intelligence , 2021.
[18] Nishant Nikhil and Brendan Tran Morris. Convolutional
neural network for trajectory prediction. In Proceedings
of the European Conference on Computer Vision (ECCV)
Workshops , pages 0–0, 2018.
[19] Savvas Papaioannou, Hongkai Wen, Zhuoling Xiao, Andrew
Markham, and Niki Trigoni. Accurate positioning via cross-
modality training. In Proceedings of the 13th ACM Confer-
ence on Embedded Networked Sensor Systems , pages 239–
251, 2015.
[20] Edoardo Mello Rella, Jan-Nico Zaech, Alexander Liniger,
and Luc Van Gool. Decoder fusion rnn: Context and in-
teraction aware decoders for trajectory prediction. In 2021
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS) , pages 5937–5943. IEEE, 2021.
[21] Andrey Rudenko, Luigi Palmieri, Michael Herman, Kris M
Kitani, Dariu M Gavrila, and Kai O Arras. Human motion
trajectory prediction: A survey. The International Journal of
Robotics Research , 39(8):895–935, 2020.
[22] David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. Learning internal representations by error prop-
agation, parallel distributed processing, explorations in the
microstructure of cognition, ed. de rumelhart and j. mcclel-
land. vol. 1. 1986. Biometrika , 71:599–607, 1986.
[23] Zhiyuan Shi, Min Xu, Quan Pan, Bing Yan, and Haimin
Zhang. Lstm-based ﬂight trajectory prediction. In 2018 In-
ternational Joint Conference on Neural Networks (IJCNN) ,
pages 1–8. IEEE, 2018.
[24] Dennis Strelow, Jeffrey Mishler, David Koes, and Sanjiv
Singh. Precise omnidirectional camera calibration. In Pro-
ceedings of the 2001 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition. CVPR 2001 ,
pages I–I. IEEE, 2001.
[25] Hung Tran, Vuong Le, and Truyen Tran. Goal-driven long-
term trajectory prediction. In Proceedings of the IEEE/CVF
winter conference on applications of computer vision , pages
796–805, 2021.
14810
[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017.
[27] Yi Xu, Armin Bazarjani, Hyung-gun Chi, Chiho Choi, and
Yun Fu. Uncovering the missing pattern: Uniﬁed framework
towards trajectory imputation and prediction. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 9632–9643, 2023.
[28] Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu,
and Yun Fu. Layout sequence prediction from noisy mo-
bile modality. In Proceedings of the 31st ACM International
Conference on Multimedia , pages 3965–3974, 2023.
[29] Hongsong Zhao, Lingjuan Miao, and Haijun Shao. Adap-
tive two-stage kalman ﬁlter for sins/odometer integrated nav-
igation systems. The Journal of Navigation , 70(2):242–261,
2017.
[30] Qihua Zhou, Song Guo, Jun Pan, Jiacheng Liang, Jingcai
Guo, Zhenda Xu, and Jingren Zhou. Pass: Patch automatic
skip scheme for efﬁcient on-device video perception. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
pages 1–18, 2024.
14811
