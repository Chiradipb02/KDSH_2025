Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data
Lihe Yang1Bingyi Kang2†Zilong Huang2Xiaogang Xu3,4Jiashi Feng2Hengshuang Zhao1‡
1HKU2TikTok3CUHK4ZJU
†project lead ‡corresponding author
https://depth-anything.github.io
Figure 1. Our model exhibits impressive generalization ability across extensive unseen scenes. Left two columns: COCO [ 35].Middle two:
SA-1B [ 26] (a hold-out unseen set). Right two: photos captured by ourselves. Our model works robustly in low-light environments (1st and
3rd column), complex scenes (2nd and 5th column), foggy weather (5th column), and ultra-remote distance (5th and 6th column), etc.
Abstract
This work presents Depth Anything1, a highly practical
solution for robust monocular depth estimation. Without pur-
suing novel technical modules, we aim to build a simple yet
powerful foundation model dealing with any images under
any circumstances. To this end, we scale up the dataset by
designing a data engine to collect and automatically anno-
tate large-scale unlabeled data ( ∼62M), which significantly
enlarges the data coverage and thus is able to reduce the
generalization error. We investigate two simple yet effective
strategies that make data scaling-up promising. First, a more
challenging optimization target is created by leveraging data
augmentation tools. It compels the model to actively seek
extra visual knowledge and acquire robust representations.
Second, an auxiliary supervision is developed to enforce
the model to inherit rich semantic priors from pre-trained
encoders. We evaluate its zero-shot capabilities extensively,
including six public datasets and randomly captured photos.
It demonstrates impressive generalization ability (Figure 1).
Further, through fine-tuning it with metric depth information
from NYUv2 and KITTI, new SOTAs are set. Our better depth
model also results in a better depth-conditioned ControlNet.
Our models are released here.
Work was done during an internship at TikTok.
1While the grammatical soundness of this name may be questionable,
we treat it as a whole and pay homage to Segment Anything [26].1. Introduction
The field of computer vision and natural language processing
is currently experiencing a revolution with the emergence of
“foundation models” [ 6] that demonstrate strong zero-/few-
shot performance in various downstream scenarios [ 44,58].
These successes primarily rely on large-scale training data
that can effectively cover the data distribution. Monocular
Depth Estimation (MDE), which is a fundamental problem
with broad applications in robotics [ 65], autonomous driv-
ing [ 63,79], virtual reality [ 47],etc., also requires a foun-
dation model to estimate depth information from a single
image. However, this has been underexplored due to the
difficulty of building datasets with tens of millions of depth
labels. MiDaS [ 45] made a pioneering study along this di-
rection by training an MDE model on a collection of mixed
labeled datasets. Despite demonstrating a certain level of
zero-shot ability, MiDaS is limited by its data coverage, thus
suffering disastrous performance in some scenarios.
In this work, our goal is to build a foundation model for
MDE capable of producing high-quality depth information
for any images under any circumstances. We approach this
target from the perspective of dataset scaling-up. Tradition-
ally, depth datasets are created mainly by acquiring depth
data from sensors [ 17,54], stereo matching [ 14], or SfM [ 32],
which is costly, time-consuming, or even intractable in partic-
ular situations. We instead, for the first time, pay attention to
large-scale unlabeled data. Compared with stereo images or
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10371
labeled images from depth sensors, our used monocular unla-
beled images exhibit three advantages: (i) ( simple and cheap
to acquire ) Monocular images exist almost everywhere, thus
they are easy to collect, without requiring specialized de-
vices. (ii) ( diverse ) Monocular images can cover a broader
range of scenes, which are critical to the model generaliza-
tion ability and scalability. (iii) ( easy to annotate ) We can
simply use a pre-trained MDE model to assign depth labels
for unlabeled images, which only takes a feedforward step.
More than efficient, this also produces denser depth maps
than LiDAR [ 17] and omits the computationally intensive
stereo matching process.
We design a data engine to automatically generate depth
annotations for unlabeled images, enabling data scaling-up
to arbitrary scale. It collects 62M diverse and informative im-
ages from eight public large-scale datasets, e.g., SA-1B [ 26],
Open Images [ 29], and BDD100K [ 81]. We use their raw
unlabeled images without any forms of labels. Then, in or-
der to provide a reliable annotation tool for our unlabeled
images, we collect 1.5M labeled images from six public
datasets to train an initial MDE model. The unlabeled im-
ages are then automatically annotated and jointly learned
with labeled images in a self-training manner [30].
Despite all the aforementioned advantages of monocular
unlabeled images, it is indeed not trivial to make positive use
of such large-scale unlabeled images [ 72,89], especially in
the case of sufficient labeled images and strong pre-training
models. In our preliminary attempts, directly combining la-
beled and pseudo labeled images failed to improve the base-
line of solely using labeled images. We conjecture that, the
additional knowledge acquired in such a naive self-teaching
manner is rather limited. To address the dilemma, we pro-
pose to challenge the student model with a more difficult
optimization target when learning the pseudo labels. The
student model is enforced to seek extra visual knowledge
and learn robust representations under various strong pertur-
bations to better handle unseen images.
Furthermore, there have been some works [ 9,20] demon-
strating the benefit of an auxiliary semantic segmentation
task for MDE. We also follow this research line, aiming to
equip our model with better high-level scene understanding
capability. However, we observed when an MDE model is
already powerful enough, it is hard for such an auxiliary
task to bring further gains. We speculate that it is due to
severe loss in semantic information when decoding an im-
age into a discrete class space. Therefore, considering the
excellent performance of DINOv2 in semantic-related tasks,
we propose to maintain the rich semantic priors from it with
a simple feature alignment loss. This not only enhances the
MDE performance, but also yields a multi-task encoder for
both middle-level and high-level perception tasks.
Our contributions are summarized as follows:
•We highlight the value of data scaling-up of massive,cheap, and diverse unlabeled images for MDE.
•We point out a key practice in jointly training large-
scale labeled and unlabeled images. Instead of learning
raw unlabeled images directly, we challenge the model
with a harder optimization target for extra knowledge.
•We propose to inherit rich semantic priors from pre-
trained encoders for better scene understanding, rather
than using an auxiliary semantic segmentation task.
•Our model exhibits stronger zero-shot capability than
MiDaS-BEiT L-512 [5]. Further, fine-tuned with metric
depth, it outperforms ZoeDepth [4] significantly.
2. Related Work
Monocular depth estimation (MDE). Early works [ 22,36,
50] primarily relied on handcrafted features and traditional
computer vision techniques. They were limited by their re-
liance on explicit depth cues and struggled to handle complex
scenes with occlusions and textureless regions.
Deep learning-based methods have revolutionized monoc-
ular depth estimation by effectively learning depth represen-
tations from delicately annotated datasets [ 17,54]. Eigen
et al. [16] first proposed a multi-scale fusion network to
regress the depth. Following this, many works consistently
improve the depth estimation accuracy by carefully design-
ing the regression task as a classification task [ 3,33], in-
troducing more priors [ 31,53,75,82], and better objective
functions [ 67,77],etc. Despite the promising performance,
they are hard to generalize to unseen domains.
Zero-shot depth estimation. Our work belongs to this re-
search line. We aim to train an MDE model with a diverse
training set and thus can predict the depth for any given im-
age. Some pioneering works [ 10,66] explored this direction
by collecting more training images, but their supervision is
very sparse and is only enforced on limited pairs of points.
To enable effective multi-dataset joint training, a mile-
stone work MiDaS [ 45] utilizes an affine-invariant loss to
ignore the potentially different depth scales and shifts across
varying datasets. Thus, MiDaS provides relative depth infor-
mation. Recently, some works [ 4,21,78] take a step further
to estimate the metric depth. However, in our practice, we
observe such methods exhibit poorer generalization ability
than MiDaS, especially its latest version [ 5]. Besides, as
demonstrated by ZoeDepth [ 4], a strong relative depth es-
timation model can also work well in generalizable metric
depth estimation by fine-tuning with metric depth informa-
tion. Therefore, we still follow MiDaS in relative depth
estimation, but further strengthen it by highlighting the value
of large-scale monocular unlabeled images.
Leveraging unlabeled data. This belongs to the research
area of semi-supervised learning [30, 55, 89], which is pop-
ular with various applications [ 70,74]. However, existing
10372
works typically assume only limited images are available.
They rarely consider the challenging but realistic scenario
where there are already sufficient labeled images but also
larger-scale unlabeled images. We take this challenging di-
rection for zero-shot MDE. We demonstrate that unlabeled
images can significantly enhance the data coverage and thus
improve model generalization and robustness.
3. Depth Anything
Our work utilizes both labeled and unlabeled images to
facilitate better monocular depth estimation (MDE). For-
mally, the labeled and unlabeled sets are denoted as Dl=
{(xi, di)}M
i=1andDu={ui}N
i=1respectively. We aim to
learn a teacher model TfromDl. Then, we utilize Tto
assign pseudo depth labels for Du. Finally, we train a stu-
dent model Son the combination of labeled set and pseudo
labeled set. A brief illustration is provided in Figure 2.
3.1. Learning Labeled Images
This process is similar to the training of MiDaS [ 5,45].
However, since MiDaS did not release its code, we first
reproduced it. Concretely, the depth value is first transformed
into the disparity space by d= 1/tand then normalized
to 0∼1 on each depth map. To enable multi-dataset joint
training, we adopt the affine-invariant loss to ignore the
unknown scale and shift of each sample:
Ll=1
HWHWX
i=1ρ(d∗
i, di), (1)
where d∗
ianddiare the prediction and ground truth, respec-
tively. And ρis the affine-invariant mean absolute error loss:
ρ(d∗
i, di) =|ˆd∗
i−ˆdi|, where ˆd∗
iandˆdiare the scaled and
shifted versions of the prediction d∗
iand ground truth di:
ˆdi=di−t(d)
s(d), (2)
where t(d)ands(d)are used to align the prediction and
ground truth to have zero translation and unit scale:
t(d) =median (d), s(d) =1
HWHWX
i=1|di−t(d)|.(3)
To obtain a robust monocular depth estimation model, we
collect 1.5M labeled images from 6 public datasets. Details
of these datasets are listed in Table 1. We use fewer labeled
datasets than MiDaS v3.1 [ 5] (12 training datasets), because
1) we do not use NYUv2 [ 54] and KITTI [ 17] datasets to
ensure zero-shot evaluation on them, 2) some datasets are
not available (anymore), e.g., Movies [ 45] and WSVD [ 60],
and 3) some datasets exhibit poor quality, e.g., RedWeb (also
low resolution) [ 66]. Despite using fewer labeled images,Dataset Indoor Outdoor Label # Images
Labeled Datasets
BlendedMVS [76] ✓ ✓ Stereo 115K
DIML [13] ✓ ✓ Stereo 927K
HRWSI [67] ✓ ✓ Stereo 20K
IRS [61] ✓ Stereo 103K
MegaDepth [32] ✓ SfM 128K
TartanAir [62] ✓ ✓ Stereo 306K
Unlabeled Datasets
BDD100K [81] ✓ None 8.2M
Google Landmarks [64] ✓ None 4.1M
ImageNet-21K [49] ✓ ✓ None 13.1M
LSUN [80] ✓ None 9.8M
Objects365 [52] ✓ ✓ None 1.7M
Open Images V7 [29] ✓ ✓ None 7.8M
Places365 [87] ✓ ✓ None 6.5M
SA-1B [26] ✓ ✓ None 11.1M
Table 1. In total, our Depth Anything is trained on 1.5M labeled
images and 62M unlabeled images jointly.
our easy-to-acquire and diverse unlabeled images will com-
prehend the data coverage and greatly enhance the model
generalization ability and robustness.
Furthermore, to strengthen the teacher model Tlearned
from these labeled images, we adopt the DINOv2 [ 42] pre-
trained weights to initialize our encoder. In practice, we
apply a pre-trained semantic segmentation model [ 69] to de-
tect the sky region, and set its disparity value as 0 (farthest).
3.2. Unleashing the Power of Unlabeled Images
This is the main point of our work. Distinguished from prior
works that laboriously construct diverse labeled datasets,
we highlight the value of unlabeled images in enhancing
the data coverage. Nowadays, we can practically build a
diverse and large-scale unlabeled set from the Internet or
public datasets of various tasks. Also, we can effortlessly
obtain the dense depth map of monocular unlabeled images
simply by forwarding them to a pre-trained well-performed
MDE model. This is much more convenient and efficient
than performing stereo matching or SfM reconstruction for
stereo images or videos. We select eight large-scale public
datasets as our unlabeled sources for their diverse scenes.
They contain more than 62M images in total. The details are
provided in the bottom half of Table 1.
Technically, given the previously obtained MDE teacher
model T, we make predictions on the unlabeled set Duto
obtain a pseudo labeled set ˆDu:
ˆDu={(ui, T(ui))|ui∈ Du}N
i=1. (4)
With the combination set Dl∪ˆDuof labeled images and
pseudo labeled images, we train a student model Son it.
10373
labeled image
unlabeled imageencoder
decoder
manual label
pseudo labelencoderteachermodelLiDAR, matching, SfM, etcsemanticpreservationlabeled prediction
unlabeled predictionsupsupHRWSI: 102684_LookInStereoDotComDSCF0486SA1B: sa_10000139
Sfeature alignmentFigure 2. Our pipeline. Solid line: flow of labeled images, dotted line: unlabeled images. We especially highlight the value of large-scale
unlabeled images. The Sdenotes adding strong perturbations (Section 3.2). To equip our depth estimation model with rich semantic priors,
we enforce an auxiliary constraint between the online student model and a frozen encoder to preserve the semantic capability (Section 3.3).
Following prior works [ 73], instead of fine-tuning SfromT,
we re-initialize Sfor better performance.
Unfortunately, in our pilot studies, we failed to gain im-
provements with such a self-training pipeline, which indeed
contradicts the observations when there are only a few la-
beled images [ 55]. We conjecture that, with already suffi-
cient labeled images in our case, the extra knowledge ac-
quired from additional unlabeled images is rather limited.
Especially considering the teacher and student share the
same pre-training and architecture, they tend to make similar
correct or false predictions on the unlabeled set Du, even
without the explicit self-training procedure.
To address the dilemma, we propose to challenge the stu-
dent with a more difficult optimization target for additional
visual knowledge on unlabeled images. We inject strong per-
turbations to unlabeled images during training. It compels
our student model to actively seek extra visual knowledge
and acquire invariant representations from these unlabeled
images. These advantages help our model deal with the open
world more robustly. We introduce two forms of perturba-
tions: one is strong color distortions, including color jittering
and Gaussian blurring, and the other is strong spatial dis-
tortion, which is CutMix [ 83]. Despite the simplicity, the
two modifications make our large-scale unlabeled images
significantly improve the baseline of labeled images.
We provide more details about CutMix. It was originally
proposed for image classification, and is rarely explored in
monocular depth estimation. We first interpolate a random
pair of unlabeled images uaandubspatially:
uab=ua⊙M+ub⊙(1−M), (5)
where Mis a binary mask with a rectangle region set as 1.
The unlabeled loss Luis obtained by first computing
affine-invariant losses in valid regions defined by Mand
1−M, respectively:
LM
u=ρ 
S(uab)⊙M, T (ua)⊙M
, (6)
L1−M
u =ρ 
S(uab)⊙(1−M), T(ub)⊙(1−M)
,(7)where we omit thePand pixel subscript ifor simplicity.
Then we aggregate the two losses via weighted averaging:
Lu=PM
HWLM
u+P(1−M)
HWL1−M
u. (8)
We use CutMix with 50% probability. The unlabeled
images for CutMix are already strongly distorted in color,
but the unlabeled images fed into the teacher model Tfor
pseudo labeling are clean, without any distortions.
3.3. Semantic-Assisted Perception
There exist some works [ 9,20,27,71] improving depth es-
timation with an auxiliary semantic segmentation task. We
believe that arming our depth estimation model with such
high-level semantic-related information is beneficial. Be-
sides, in our specific context of leveraging unlabeled images,
these auxiliary supervision signals from other tasks can also
combat the potential noise in our pseudo depth label.
Therefore, we made an initial attempt by carefully assign-
ing semantic segmentation labels to our unlabeled images
with a combination of RAM [ 85] + GroundingDINO [ 37] +
HQ-SAM [ 25] models. After post-processing, this yields a
class space containing 4K classes. In the joint-training stage,
the model is enforced to produce both depth and segmenta-
tion predictions with a shared encoder and two individual
decoders. Unfortunately, after trial and error, we still could
not boost the performance of the original MDE model. We
speculated that, decoding an image into a discrete class space
indeed loses too much semantic information. The limited
information in these semantic masks is hard to further boost
our depth model, especially when our depth model has es-
tablished very competitive results.
Therefore, we aim to seek more informative semantic sig-
nals to serve as auxiliary supervision for our depth estimation
task. We are greatly astonished by the strong performance
of DINOv2 models [ 42] in semantic-related tasks, e.g., im-
age retrieval and semantic segmentation, even with frozen
weights without any fine-tuning. Motivated by these clues,
we propose to transfer its strong semantic capability to our
10374
Method EncoderKITTI [17] NYUv2 [54] Sintel [7] DDAD [19] ETH3D [51] DIODE [59]
AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1
MiDaS v3.1 [5] ViT-L 0.127 0.850 0.048 0.980 0.587 0.699 0.251 0.766 0.139 0.867 0.075 0.942
Depth AnythingViT-S 0.080 0.936 0.053 0.972 0.464 0.739 0.247 0.768 0.127 0.885 0.076 0.939
ViT-B 0.080 0.939 0.046 0.979 0.432 0.756 0.232 0.786 0.126 0.884 0.069 0.946
ViT-L 0.076 0.947 0.043 0.981 0.458 0.760 0.230 0.789 0.127 0.882 0.066 0.952
Table 2. Zero-shot relative depth estimation. Better: AbsRel ↓,δ1↑. We compare with the best model from MiDaS v3.1. Note that MiDaS
does not strictly follow the zero-shot evaluation on KITTI and NYUv2, because it uses their training images. We provide three model scales
for different purposes, based on ViT-S (24.8M), ViT-B (97.5M), and ViT-L (335.3M), respectively. Best, second best results.
depth model with an auxiliary feature alignment loss. The
feature space is high-dimensional and continuous, thus con-
taining richer semantic information than discrete masks. The
feature alignment loss is formulated as:
Lfeat= 1−1
HWHWX
i=1cos(fi, f′
i), (9)
where cos(·,·)measures the cosine similarity between two
feature vectors. fis the feature extracted by the depth model
S, while f′is the feature from a frozen DINOv2 encoder.
We do not follow some works [ 18] to project the online
feature finto a new space for alignment, because a randomly
initialized projector makes the large alignment loss dominate
the overall loss in the early stage.
Another key point in feature alignment is that, semantic
encoders like DINOv2 tend to produce similar features for
different parts of an object, e.g., car front and rear. In depth
estimation, however, different parts or even pixels within the
same part, can be of varying depth. Thus, it is not beneficial
toexhaustively enforce our depth model to produce exactly
the same features as the frozen encoder.
To solve this issue, we set a tolerance margin αfor the
feature alignment. If the cosine similarity of fiandf′
ihas
surpassed α, this pixel will not be considered in our Lfeat.
This allows our method to enjoy both the semantic-aware
representation from DINOv2 and the part-level discrimina-
tive representation from depth supervision. As a side effect,
our produced encoder not only performs well in downstream
MDE datasets, but also achieves strong results in the seman-
tic segmentation task. It also indicates the potential of our
encoder to serve as a universal multi-task encoder for both
middle-level and high-level perception tasks.
Finally, our overall loss is an average combination of the
three losses Ll,Lu, andLfeat.
4. Experiment
4.1. Implementation Details
We adopt the DINOv2 encoder [ 42] for feature extraction.
Following MiDaS [5, 45], we use the DPT [46] decoder fordepth regression. All labeled datasets are simply combined
together without re-sampling. In the first stage, we train a
teacher model on labeled images for 20 epochs. In the second
stage of joint training, we train a student model to sweep
across all unlabeled images for one time. The unlabeled
images are annotated by a best-performed teacher model
with a ViT-L encoder. The ratio of labeled and unlabeled
images is set as 1:2 in each batch. In both stages, the base
learning rate of the pre-trained encoder is set as 5e-6, while
the randomly initialized decoder uses a 10 ×larger learning
rate. We use the AdamW optimizer and decay the learning
rate with a linear schedule. We only apply horizontal flipping
as our data augmentation for labeled images. The tolerance
margin αfor feature alignment loss is set as 0.85. For more
details, please refer to our appendix.
4.2. Zero-Shot Relative Depth Estimation
As aforementioned, this work aims to provide accurate
depth estimation for any image. Therefore, we compre-
hensively validate the zero-shot depth estimation capability
of our Depth Anything model on six representative unseen
datasets: KITTI [ 17], NYUv2 [ 54], Sintel [ 7], DDAD [ 19],
ETH3D [ 51], and DIODE [ 59]. We compare with the best
DPT-BEiT L-512 model from the latest MiDaS v3.1 [ 5], which
uses more labeled images than us. As shown in Table 2,
both with a ViT-L encoder, our Depth Anything surpasses
the strongest MiDaS model tremendously across extensive
scenes in terms of both the AbsRel (absolute relative error:
|d∗−d|/d) and δ1(percentage of max( d∗/d, d/d∗)<1.25)
metrics. For example, when tested on the well-known au-
tonomous driving dataset DDAD [ 19], we improve the Ab-
sRel (↓) from 0.251 →0.230 and improve the δ1(↑) from
0.766→0.789.
Besides, our ViT-B model is already clearly superior to
the MiDaS based on a much larger ViT-L. Moreover, our
ViT-S model, whose scale is less than 1/10 of the MiDaS
model, even outperforms MiDaS on several unseen datasets,
including Sintel, DDAD, and ETH3D. The performance
advantage of these small-scale models demonstrates their
great potential in computationally-constrained scenarios.
It is also worth noting that, on the most widely used MDE
10375
MethodHigher is better ↑ Lower is better ↓
δ1 δ2 δ3 AbsRel RMSE log10
AdaBins [3] 0.903 0.984 0.997 0.103 0.364 0.044
DPT [46] 0.904 0.988 0.998 0.110 0.357 0.045
P3Depth [43] 0.898 0.981 0.996 0.104 0.356 0.043
SwinV2-L [39] 0.949 0.994 0.999 0.083 0.287 0.035
AiT [41] 0.954 0.994 0.999 0.076 0.275 0.033
VPD [86] 0.964 0.995 0.999 0.069 0.254 0.030
ZoeDepth∗[4] 0.951 0.994 0.999 0.077 0.282 0.033
Ours 0.984 0.998 1.000 0.056 0.206 0.024
Table 3. Fine-tuning and evaluating on NYUv2 [54] with our
pre-trained MDE encoder. We highlight best,second best results,
as well as most discriminative metrics .∗: Reproduced by us.
benchmarks KITTI and NYUv2, although MiDaS v3.1 uses
the corresponding training images ( not zero-shot anymore ),
our Depth Anything is still evidently superior to it without
training with any KITTI or NYUv2 images ,e.g., 0.127 vs.
0.076 in AbsRel and 0.850 vs.0.947 in δ1on KITTI.
4.3. Fine-tuned to Metric Depth Estimation
Apart from the impressive performance in zero-shot relative
depth estimation, we further examine our Depth Anything
model as a promising weight initialization for downstream
metric depth estimation. We initialize the encoder of down-
stream MDE models with our pre-trained encoder parameters
and leave the decoder randomly initialized. The model is
fine-tuned with correponding metric depth information. In
this part, we use our ViT-L encoder for fine-tuning.
We examine two representative scenarios: 1) in-domain
metric depth estimation, where the model is trained and
evaluated on the same domain (Section 4.3.1), and 2) zero-
shot metric depth estimation, where the model is trained on
one domain, e.g., NYUv2 [ 54], but evaluated in different
domains, e.g., SUN RGB-D [56] (Section 4.3.2).
4.3.1 In-Domain Metric Depth Estimation
As shown in Table 3 of NYUv2 [ 54], our model outperforms
the previous best method VPD [ 86] remarkably, improving
theδ1(↑) from 0.964 →0.984 and AbsRel ( ↓) from 0.069
to 0.056. Similar improvements can be observed in Table 4
of the KITTI dataset [ 17]. We improve the δ1(↑) on KITTI
from 0.978 →0.982. It is worth noting that we adopt the
ZoeDepth framework for this scenario with a relatively ba-
sic depth model, and we believe our results can be further
enhanced if equipped with more advanced architectures.
4.3.2 Zero-Shot Metric Depth Estimation
We follow ZoeDepth [ 4] to conduct zero-shot metric depth
estimation. ZoeDepth fine-tunes the MiDaS pre-trained en-MethodHigher is better ↑ Lower is better ↓
δ1 δ2 δ3 AbsRel RMSE RMSE log
AdaBins [3] 0.964 0.995 0.999 0.058 2.360 0.088
DPT [46] 0.959 0.995 0.999 0.062 2.573 0.092
P3Depth [43] 0.953 0.993 0.998 0.071 2.842 0.103
NeWCRFs [82] 0.974 0.997 0.999 0.052 2.129 0.079
SwinV2-L [39] 0.977 0.998 1.000 0.050 1.966 0.075
NDDepth [53] 0.978 0.998 0.999 0.050 2.025 0.075
GEDepth [75] 0.976 0.997 0.999 0.048 2.044 0.076
ZoeDepth∗[4] 0.971 0.996 0.999 0.054 2.281 0.082
Ours 0.982 0.998 1.000 0.046 1.896 0.069
Table 4. Fine-tuning and evaluating on KITTI [17] with our
pre-trained MDE encoder. ∗: Reproduced by us.
coder with metric depth information from NYUv2 [ 54] (for
indoor scenes) or KITTI [ 17] (for outdoor scenes). There-
fore, we simply replace the MiDaS encoder with our bet-
ter Depth Anything encoder, leaving other components un-
changed. As shown in Table 5, across a wide range of unseen
datasets of indoor and outdoor scenes, our Depth Anything
results in a better metric depth estimation model than the
original ZoeDepth based on MiDaS.
4.4. Fine-tuned to Semantic Segmentation
In our method, we design our MDE model to inherit the
rich semantic priors from a pre-trained encoder via a sim-
ple feature alignment constraint. Here, we examine the
semantic capability of our MDE encoder. Specifically, we
fine-tune our MDE encoder to downstream semantic segmen-
tation datasets. As exhibited in Table 7 of the Cityscapes
dataset [ 14], our encoder from large-scale MDE training
(86.2 mIoU) is superior to existing encoders from large-scale
ImageNet-21K pre-training, e.g., Swin-L [ 38] (84.3) and
ConvNeXt-XL [ 40] (84.6). Similar observations hold on the
ADE20K dataset [ 88] in Table 8. We improve the previous
best result from 58.3 →59.4.
We hope to highlight that, witnessing the superiority of
our pre-trained encoder on both monocular depth estimation
and semantic segmentation tasks, we believe it has great
potential to serve as a generic multi-task encoder for both
middle-level and high-level visual perception systems.
4.5. Ablation Studies
Unless otherwise specified, we use the ViT-L encoder for
our ablation studies here.
Zero-shot transferring of each training dataset. In Ta-
ble 6, we provide the zero-shot transferring performance of
each training dataset, which means that we train a relative
MDE model on onetraining set and evaluate it on the six
unseen datasets. With these results, we hope to offer more
insights for future works that similarly aim to build a general
10376
MethodSUN RGB-D [56] iBims-1 [28] HyperSim [48] Virtual KITTI 2 [8] DIODE Outdoor [59]
AbsRel ( ↓) δ1(↑) AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1
ZoeDepth [4] 0.520 0.545 0.169 0.656 0.407 0.302 0.106 0.844 0.814 0.237
Depth Anything 0.500 0.660 0.150 0.714 0.363 0.361 0.085 0.913 0.794 0.288
Table 5. Zero-shot metric depth estimation. The first three test sets in the header are indoor scenes, while the last two are outdoor scenes.
Following ZoeDepth, we use the model trained on NYUv2 for indoor generalization, while use the model trained on KITTI for outdoor
evaluation. For fair comparisons, we report the ZoeDepth results reproduced in our environment.
Training setKITTI [17] NYUv2 [54] Sintel [7] DDAD [19] ETH3D [51] DIODE [59] Mean
AbsRel δ1AbsRel δ1AbsRel δ1AbsRel δ1AbsRel δ1AbsRel δ1AbsRel δ1
BlendedMVS [76] 0.089 0.918 0.068 0.958 0.556 0.689 0.305 0.731 0.148 0.845 0.092 0.921 0.210 0.844
DIML [13] 0.099 0.907 0.055 0.969 0.573 0.722 0.381 0.657 0.142 0.859 0.107 0.908 0.226 0.837
HRWSI [67] 0.095 0.917 0.062 0.966 0.502 0.731 0.270 0.750 0.186 0.775 0.087 0.935 0.200 0.846
IRS [61] 0.105 0.892 0.057 0.970 0.568 0.714 0.328 0.691 0.143 0.845 0.088 0.926 0.215 0.840
MegaDepth [32] 0.217 0.741 0.071 0.953 0.632 0.660 0.479 0.566 0.142 0.852 0.104 0.910 0.274 0.780
TartanAir [62] 0.088 0.920 0.061 0.964 0.602 0.723 0.332 0.690 0.160 0.818 0.088 0.928 0.222 0.841
All labeled data 0.085 0.934 0.053 0.971 0.492 0.748 0.245 0.771 0.134 0.874 0.070 0.945 0.180 0.874
Table 6. Examine the zero-shot transferring performance of each labeled training set (left) to six unseen datasets (top). Better performance:
AbsRel ↓,δ1↑. We highlight the best, second , and third best results for each test dataset in bold , underline , and italic , respectively.
Method Encoder mIoU (s.s.) m.s.
Segmenter [57] ViT-L [15] - 82.2
SegFormer [69] MiT-B5 [69] 82.4 84.0
Mask2Former [12] Swin-L [38] 83.3 84.3
OneFormer [23] Swin-L [38] 83.0 84.4
OneFormer [23] ConvNeXt-XL [40] 83.6 84.6
DDP [24] ConvNeXt-L [40] 83.2 83.9
Ours ViT-L [15] 84.8 86.2
Table 7. Transferring our MDE pre-trained encoder to Cityscapes
for semantic segmentation. We do not use Mapillary [ 1] for pre-
training. s.s./m.s.: single-/multi-scale evaluation.
monocular depth estimation system. Among the six training
datasets, HRWSI [ 67] fuels our model with the strongest
generalization ability, even though it only contains 20K im-
ages. This indicates the data diversity counts a lot, which
is well aligned with our motivation to utilize unlabeled im-
ages. Some labeled datasets may not perform very well, e.g.,
MegaDepth [ 32], however, it has its own preferences that
are not reflected in these six test datasets. For example, we
find models trained with MegaDepth data are specialized at
estimating the distance of ultra-remote buildings (Figure 1),
which will be very beneficial for aerial vehicles.
Effectiveness of 1) challenging the student model when
learning unlabeled images, and 2) semantic constraint.
As shown in Table 9, simply adding unlabeled images with
pseudo labels does not necessarily bring gains to our model,Method Encoder mIoU
Segmenter [57] ViT-L [15] 51.8
SegFormer [69] MiT-B5 [69] 51.0
Mask2Former [12] Swin-L [38] 56.4
UperNet [68] BEiT-L [2] 56.3
ViT-Adapter [11] BEiT-L [2] 58.3
OneFormer [23] Swin-L [38] 57.4
OneFormer [23] ConNeXt-XL [40] 57.4
Ours ViT-L [15] 59.4
Table 8. Transferring our MDE encoder to ADE20K for semantic
segmentation. We use Mask2Former as our segmentation model.
since the labeled images are already sufficient. However,
with strong perturbations ( S) applied to unlabeled images
during re-training, the student model is challenged to seek
additional visual knowledge and learn more robust repre-
sentations. Consequently, the large-scale unlabeled images
enhance the model generalization ability significantly.
Moreover, with our used semantic constraint Lfeat, the
power of unlabeled images can be further amplified for the
depth estimation task. More importantly, as emphasized in
Section 4.4, this auxiliary constraint also enables our trained
encoder to serve as a key component in a multi-task visual
system for both middle-level and high-level perception.
Comparison with MiDaS trained encoder in downstream
tasks. Our Depth Anything model has exhibited stronger
zero-shot capability than MiDaS [ 5,45]. Here, we further
10377
Figure 3. Qualitative results on six unseen datasets.
LlLuS L feat KI NY SI DD ET DI
✓ 0.085 0.053 0.492 0.245 0.134 0.070
✓ ✓ 0.085 0.054 0.481 0.242 0.138 0.073
✓ ✓ ✓ 0.081 0.048 0.469 0.235 0.134 0.068
✓ ✓ ✓ ✓ 0.076 0.043 0.458 0.230 0.127 0.066
Table 9. Ablation studies of: 1) challenging the student with strong
perturbations ( S) when learning unlabeled images, and 2) semantic
constraint ( Lfeat). Limited by space, we only report the AbsRel
(↓) metric, and shorten the dataset name with its first two letters.
MethodNYUv2 KITTI Cityscapes ADE20K
AbsRel δ1AbsRel δ1 mIoU mIoU
MiDaS 0.077 0.951 0.054 0.971 82.1 52.4
Ours 0.056 0.984 0.046 0.982 84.8 59.4
Table 10. Comparison between our trained encoder and MiDaS [ 5]
trained encoder in terms of downstream fine-tuning performance.
Better performance: AbsRel ↓,δ1↑, mIoU ↑.
compare our trained encoder with MiDaS v3.1 [ 5] trained
encoder in terms of the downstream fine-tuning performance.
As demonstrated in Table 10, on both the downstream depth
estimation task and semantic segmentation task, our pro-
duced encoder outperforms the MiDaS encoder remarkably,
e.g., 0.951 vs.0.984 in the δ1metric on NYUv2, and 52.4
vs.59.4 in the mIoU metric on ADE20K.
Comparison with DINOv2 in downstream tasks. We
have demonstrated the superiority of our trained encoder
when fine-tuned to downstream tasks. Since our finally
produced encoder (from large-scale MDE training) is fine-
tuned from DINOv2 [ 42], we compare our encoder with the
original DINOv2 encoder in Table 11. It can be observed
that our encoder performs better than the original DINOv2
encoder in both the downstream metric depth estimation
task and semantic segmentation task. Although the DINOv2
weight has provided a very strong initialization, our large-
scale and high-quality MDE training can further enhance it
impressively in downstream transferring performance.
Ours
MiDaSOursMiDaS
Figure 4. We compare our depth prediction with MiDaS. Meantime,
we use ControlNet to synthesize new images from the depth map.
EncoderNYUv2 KITTI ADE20K
AbsRel ( ↓)δ1(↑) AbsRel δ1 mIoU ( ↑)
DINOv2 0.066 0.973 0.058 0.971 58.8
Ours 0.056 0.984 0.046 0.982 59.4
Table 11. Comparison between the original DINOv2 and our pro-
duced encoder in terms of downstream fine-tuning performance.
4.6. Qualitative Results
We visualize our model predictions on the six unseen datasets
in Figure 3. Our model is robust to test images from various
domains. In addition, we compare our model with MiDaS
in Figure 4. We also attempt to synthesis new images con-
ditioned on the predicted depth maps with ControlNet [ 84].
Our model produces more accurate depth estimation than
MiDaS, as well as better synthesis results. For more accurate
synthesis, we re-trained a better depth-conditioned Control-
Net based on our Depth Anything, aiming to provide better
control signals for image synthesis and video editing. Please
refer to our project page for more qualitative results on video
editing [34] with our Depth Anything.
5. Conclusion
In this work, we present Depth Anything, a highly practical
solution to robust monocular depth estimation. Different
from prior arts, we especially highlight the value of cheap
and diverse unlabeled images. We design two simple yet
highly effective strategies to fully exploit their value: 1)
posing a more challenging optimization target when learning
unlabeled images, and 2) preserving rich semantic priors
from pre-trained models. As a result, our Depth Anything
model exhibits excellent zero-shot depth estimation ability,
and also serves as a promising initialization for downstream
metric depth estimation and semantic segmentation tasks.
Acknowledgement. This work is supported by the National
Natural Science Foundation of China (No. 62201484), HKU
Startup Fund, and HKU Seed Fund for Basic Research.
10378
References
[1]Manuel L ´opez Antequera, Pau Gargallo, Markus Hofinger,
Samuel Rota Bul `o, Yubin Kuang, and Peter Kontschieder.
Mapillary planet-scale depth dataset. In ECCV , 2020. 7
[2]Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:
Bert pre-training of image transformers. In ICLR , 2022. 7
[3]Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.
Adabins: Depth estimation using adaptive bins. In CVPR ,
2021. 2, 6
[4]Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,
and Matthias M ¨uller. Zoedepth: Zero-shot transfer by com-
bining relative and metric depth. arXiv:2302.12288 , 2023. 2,
6, 7
[5]Reiner Birkl, Diana Wofk, and Matthias M ¨uller. Midas v3.
1–a model zoo for robust monocular relative depth estimation.
arXiv:2307.14460 , 2023. 2, 3, 5, 7, 8
[6]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, Michael S Bern-
stein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,
et al. On the opportunities and risks of foundation models.
arXiv:2108.07258 , 2021. 1
[7]Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J
Black. A naturalistic open source movie for optical flow
evaluation. In ECCV , 2012. 5, 7
[8]Yohann Cabon, Naila Murray, and Martin Humenberger. Vir-
tual kitti 2. arXiv:2001.10773 , 2020. 7
[9]Po-Yi Chen, Alexander H Liu, Yen-Cheng Liu, and Yu-
Chiang Frank Wang. Towards scene understanding: Un-
supervised monocular depth estimation with semantic-aware
representation. In CVPR , 2019. 2, 4
[10] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-
image depth perception in the wild. In NeurIPS , 2016. 2
[11] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong
Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for
dense predictions. In ICLR , 2023. 7
[12] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander
Kirillov, and Rohit Girdhar. Masked-attention mask trans-
former for universal image segmentation. In CVPR , 2022.
7
[13] Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon
Sohn. Diml/cvl rgb-d dataset: 2m rgb-d images of natural
indoor and outdoor scenes. arXiv:2110.11590 , 2021. 3, 7
[14] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,
Stefan Roth, and Bernt Schiele. The cityscapes dataset for
semantic urban scene understanding. In CVPR , 2016. 1, 6
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Transform-
ers for image recognition at scale. In ICLR , 2021. 7
[16] David Eigen, Christian Puhrsch, and Rob Fergus. Depth
map prediction from a single image using a multi-scale deep
network. In NeurIPS , 2014. 2
[17] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. IJRR , 2013.
1, 2, 3, 5, 6, 7[18] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-
sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning. In NeurIPS , 2020. 5
[19] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos,
and Adrien Gaidon. 3d packing for self-supervised monocular
depth estimation. In CVPR , 2020. 5, 7
[20] Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, and Adrien
Gaidon. Semantically-guided representation learning for self-
supervised monocular depth. In ICLR , 2020. 2, 4
[21] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rare s,Ambru s,,
and Adrien Gaidon. Towards zero-shot scale-aware monocu-
lar depth estimation. In ICCV , 2023. 2
[22] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recover-
ing surface layout from an image. IJCV , 2007. 2
[23] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita
Orlov, and Humphrey Shi. Oneformer: One transformer to
rule universal image segmentation. In CVPR , 2023. 7
[24] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu,
Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp:
Diffusion model for dense visual prediction. In ICCV , 2023.
7
[25] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing
Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in
high quality. In NeurIPS , 2023. 4
[26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. In ICCV , 2023. 1, 2, 3
[27] Marvin Klingner, Jan-Aike Term ¨ohlen, Jonas Mikolajczyk,
and Tim Fingscheidt. Self-supervised monocular depth es-
timation: Solving the dynamic object problem by semantic
guidance. In ECCV , 2020. 4
[28] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco
Korner. Evaluation of cnn-based single-image depth estima-
tion methods. In ECCVW , 2018. 7
[29] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings,
Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov,
Matteo Malloci, Alexander Kolesnikov, et al. The open im-
ages dataset v4: Unified image classification, object detection,
and visual relationship detection at scale. IJCV , 2020. 2, 3
[30] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient
semi-supervised learning method for deep neural networks.
InICMLW , 2013. 2
[31] Bo Li, Chunhua Shen, Yuchao Dai, Anton Van Den Hen-
gel, and Mingyi He. Depth and surface normal estimation
from monocular images using regression on deep features and
hierarchical crfs. In CVPR , 2015. 2
[32] Zhengqi Li and Noah Snavely. Megadepth: Learning single-
view depth prediction from internet photos. In CVPR , 2018.
1, 3, 7
[33] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.
Binsformer: Revisiting adaptive bins for monocular depth
estimation. arXiv:2204.00987 , 2022. 2
[34] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,
and Jiashi Feng. Magicedit: High-fidelity and temporally
coherent video editing. arXiv:2308.14749 , 2023. 8
10379
[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 1
[36] Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and
William T Freeman. Sift flow: Dense correspondence across
different scenes. In ECCV , 2008. 2
[37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun
Zhu, et al. Grounding dino: Marrying dino with grounded
pre-training for open-set object detection. arXiv:2303.05499 ,
2023. 4
[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , 2021. 6, 7
[39] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.
Swin transformer v2: Scaling up capacity and resolution. In
CVPR , 2022. 6
[40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In CVPR , 2022. 6, 7
[41] Jia Ning, Chen Li, Zheng Zhang, Chunyu Wang, Zigang
Geng, Qi Dai, Kun He, and Han Hu. All in tokens: Unifying
output space of visual tasks via soft token. In ICCV , 2023. 6
[42] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V o,
Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel
Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2:
Learning robust visual features without supervision. TMLR ,
2023. 3, 4, 5, 8
[43] Vaishakh Patil, Christos Sakaridis, Alexander Liniger, and
Luc Van Gool. P3depth: Monocular depth estimation with a
piecewise planarity prior. In CVPR , 2022. 6
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision.
InICML , 2021. 1
[45] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. TPAMI , 2020. 1, 2, 3, 5, 7
[46] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In ICCV , 2021. 5,
6
[47] Alex Rasla and Michael Beyeler. The relative importance
of depth cues and semantic edges for indoor mobility using
simulated prosthetic vision in immersive virtual reality. In
VRST , 2022. 1
[48] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit
Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,
and Joshua M Susskind. Hypersim: A photorealistic synthetic
dataset for holistic indoor scene understanding. In ICCV ,
2021. 7
[49] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al. Imagenet large scale
visual recognition challenge. IJCV , 2015. 3
[50] Ashutosh Saxena, Min Sun, and Andrew Y Ng. Make3d:
Learning 3d scene structure from a single still image. TPAMI ,
2008. 2
[51] Thomas Schops, Johannes L Schonberger, Silvano Galliani,
Torsten Sattler, Konrad Schindler, Marc Pollefeys, and An-
dreas Geiger. A multi-view stereo benchmark with high-
resolution images and multi-camera videos. In CVPR , 2017.
5, 7
[52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang
Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A
large-scale, high-quality dataset for object detection. In ICCV ,
2019. 3
[53] Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, and
Zhengguo Li. Nddepth: Normal-distance assisted monocular
depth estimation. In ICCV , 2023. 2, 6
[54] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus. Indoor segmentation and support inference from rgbd
images. In ECCV , 2012. 1, 2, 3, 5, 6, 7
[55] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,
Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying
semi-supervised learning with consistency and confidence. In
NeurIPS , 2020. 2, 4
[56] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
Sun rgb-d: A rgb-d scene understanding benchmark suite. In
CVPR , 2015. 6, 7
[57] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia
Schmid. Segmenter: Transformer for semantic segmentation.
InICCV , 2021. 7
[58] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXiv:2302.13971 , 2023. 1
[59] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,
Haochen Wang, Falcon Z Dai, Andrea F Daniele, Mo-
hammadreza Mostajabi, Steven Basart, Matthew R Walter,
et al. Diode: A dense indoor and outdoor depth dataset.
arXiv:1908.00463 , 2019. 5, 7
[60] Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver
Wang. Web stereo video supervision for depth prediction
from dynamic scenes. In 3DV, 2019. 3
[61] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiy-
ong Zhao, and Xiaowen Chu. Irs: A large naturalistic indoor
robotics stereo dataset to train deep models for disparity and
surface normal estimation. In ICME , 2021. 3, 7
[62] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu,
Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and
Sebastian Scherer. Tartanair: A dataset to push the limits of
visual slam. In IROS , 2020. 3, 7
[63] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariha-
ran, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar
from visual depth estimation: Bridging the gap in 3d object
detection for autonomous driving. In CVPR , 2019. 1
10380
[64] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.
Google landmarks dataset v2-a large-scale benchmark for
instance-level recognition and retrieval. In CVPR , 2020. 3
[65] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman,
and Vivienne Sze. Fastdepth: Fast monocular depth estima-
tion on embedded systems. In ICRA , 2019. 1
[66] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao,
Ruibo Li, and Zhenbo Luo. Monocular relative depth per-
ception with web stereo data supervision. In CVPR , 2018. 2,
3
[67] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin,
and Zhiguo Cao. Structure-guided ranking loss for single
image depth prediction. In CVPR , 2020. 2, 3, 7
[68] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understanding.
InECCV , 2018. 7
[69] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and
efficient design for semantic segmentation with transformers.
InNeurIPS , 2021. 3, 7
[70] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan
Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end
semi-supervised object detection with soft teacher. In ICCV ,
2021. 2
[71] Xiaogang Xu, Hengshuang Zhao, Vibhav Vineet, Ser-Nam
Lim, and Antonio Torralba. Mtformer: Multi-task learning
via transformer and cross-task reasoning. In ECCV , 2022. 4
[72] I Zeki Yalniz, Herv ´e J´egou, Kan Chen, Manohar Paluri, and
Dhruv Mahajan. Billion-scale semi-supervised learning for
image classification. arXiv:1905.00546 , 2019. 2
[73] Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao.
St++: Make self-training work better for semi-supervised
semantic segmentation. In CVPR , 2022. 4
[74] Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and
Yinghuan Shi. Revisiting weak-to-strong consistency in semi-
supervised semantic segmentation. In CVPR , 2023. 2
[75] Xiaodong Yang, Zhuang Ma, Zhiyu Ji, and Zhe Ren. Gedepth:
Ground embedding for monocular depth estimation. In ICCV ,
2023. 2, 6
[76] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,
Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-
scale dataset for generalized multi-view stereo networks. In
CVPR , 2020. 3, 7
[77] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. En-
forcing geometric constraints of virtual normal for depth pre-
diction. In ICCV , 2019. 2
[78] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaix-
uan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:
Towards zero-shot metric 3d prediction from a single image.
InICCV , 2023. 2
[79] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Ge-
off Pleiss, Bharath Hariharan, Mark Campbell, and Kilian Q
Weinberger. Pseudo-lidar++: Accurate depth for 3d object
detection in autonomous driving. In ICLR , 2020. 1
[80] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas
Funkhouser, and Jianxiong Xiao. Lsun: Construction of a
large-scale image dataset using deep learning with humans in
the loop. arXiv:1506.03365 , 2015. 3[81] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying
Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-
rell. Bdd100k: A diverse driving dataset for heterogeneous
multitask learning. In CVPR , 2020. 2, 3
[82] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and
Ping Tan. New crfs: Neural window fully-connected crfs for
monocular depth estimation. arXiv:2203.01502 , 2022. 2, 6
[83] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classifiers with localizable
features. In ICCV , 2019. 4
[84] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
ICCV , 2023. 8
[85] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,
Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian
Li, Shilong Liu, et al. Recognize anything: A strong image
tagging model. arXiv:2306.03514 , 2023. 4
[86] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie
Zhou, and Jiwen Lu. Unleashing text-to-image diffusion
models for visual perception. In ICCV , 2023. 6
[87] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. TPAMI , 2017. 3
[88] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-
riuso, and Antonio Torralba. Scene parsing through ade20k
dataset. In CVPR , 2017. 6
[89] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanx-
iao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-
training and self-training. In NeurIPS , 2020. 2
10381
