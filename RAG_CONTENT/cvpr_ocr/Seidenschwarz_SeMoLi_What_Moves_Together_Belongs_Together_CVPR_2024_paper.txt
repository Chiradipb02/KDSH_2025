SeMoLi : What Moves Together Belongs Together
Jenny Seidenschwarz1,2*Aljoˇsa Oˇsep2Franceso Ferroni2Simon Lucey3Laura Leal-Taix ´e2
1Technical University of Munich2NVIDIA3University of Adelaide
SeMoLiDBSCAN++O1O2O1O2O1O3O1noiseO1O2O3O1O2noisenoise
Figure 1. Towards learning to pseudo-label : We propose SeMoLi , a data-driven approach for segmenting moving instances in point
clouds ( top), that we utilize to learn to detect moving objects (O) in Lidar. We visually contrast SeMoLi to prior art, that tackles similar
problem via density-based clustering (DBSCAN) [24]. We visualize the whole point cloud in purple, and dynamic points, used as input to
our method and baseline to localize moving instances, in green. We color-code individual segmented instances. From left to right SeMoLi
(i) segments objects even for sparse point clouds and suffers less from under-segmentation, (ii) is able to learn to filter noise from the
filtered point cloud, (iii) leads to less over-segmentation, and (iv) generalizes better to different classes. Best seen in color, zoomed.
Abstract
We tackle semi-supervised object detection based on mo-
tion cues. Recent results suggest that heuristic-based clus-
tering methods in conjunction with object trackers can be
used to pseudo-label instances of moving objects and use
these as supervisory signals to train 3D object detectors in
Lidar data without manual supervision. We re-think this
approach and suggest that both, object detection, as well as
motion-inspired pseudo-labeling, can be tackled in a data-
driven manner. We leverage recent advances in scene flow
estimation to obtain point trajectories from which we ex-
tract long-term, class-agnostic motion patterns. Revisit-
ing correlation clustering in the context of message pass-
ing networks, we learn to group those motion patterns to
cluster points to object instances. By estimating the full ex-
tent of the objects, we obtain per-scan 3D bounding boxes
that we use to supervise a Lidar object detection network.
Our method not only outperforms prior heuristic-based ap-
proaches ( 57.5AP ,+14 improvement over prior work),
more importantly, we show we can pseudo-label and train
object detectors across datasets.
∗Correspondence to j.seidenschwarz@tum.de.1. Introduction
We tackle semi-supervised object detection in Lidar data in
the context of embodied navigation.
Status quo. The established approach to 3D Lidar ob-
ject detection is to pre-define object classes of interest and
train data-driven models with manually labeled data in a
fully supervised fashion. Thanks to Lidar-centric labeled
datasets [3, 10, 33] and developments in learning represen-
tations from unordered point sets [28, 29], the quality in
Lidar-based detection continues to improve. However, this
process that relies on manual labeling is notoriously expen-
sive and, importantly, does not scale well to rare classes.
Before the advent of end-to-end data-driven Lidar percep-
tion, pioneering works [12, 13, 22, 23, 34, 35, 47] were in-
spired by the Gestalt principle [40] of spatial proximity, i.e.,
the intuition that spatially close-by points belong together.
To this end, these works utilize local, distance-based point
clustering methods to segment objects via bottom-up group-
ing. They are class-agnostic and general, but inherently un-
able to benefit from the increasing amount of available data,
and therefore, fall behind data-driven methods. Recently,
heuristic approaches resurfaced within multi-stage pipelines
for data auto-labeling in the context of semi-supervised 3D
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14685
object detection [24, 26, 48]. These auto-labeling pipelines
(i) segment objects using local, bottom-up clustering meth-
ods, (ii) use Kalman filters to track segmented regions
across time, and (iii) register segments across time to es-
timate amodal extents of objects to generate pseudo-labels
for object detection training. While impressive, they still
suffer from the same drawback: they are inherently unable
to improve pseudo-labeling performance in a data-driven
fashion.
Stirring the pot. We re-think this approach and sug-
gest that both object detection training based on pseudo-
labels, as well as the pseudo-labeling process can be tack-
led in a data-driven manner. Prior efforts, too, leverage
some amount of labeled data to tune the hyper-parameters of
the auto-labeling engine and consolidate design decisions.
By contrast, we embrace a data-driven approach fully. We
study how to train a pseudo-labeling network in a way that it
can still be general and applied to an open-world setting as
well as how its performance changes with different amounts
of training data.
Learning to motion- cluster. Our approach Segment Mov-
ing in Lidar (SeMoLi ) receives a set of Lidar point trajecto-
ries, obtained from a sequence of Lidar data. In contrast to
prior heuristic approaches, we explore Gestalt principles to
train a Message Passing Network (MPN) [11] with the ob-
jective of grouping points in a class-agnostic manner. Addi-
tionally to proximity, we leverage the intuition of common
fate,i.e., we learn to group points based on the observation
thatwhat moves together, belongs together given some la-
beled instances of moving objects. This way, we make mo-
tion a first-class citizen . This, in turn, streamlines pseudo-
labeling and alleviates the need for manual geometric clus-
tering. We then use SeMoLi to identify motion patterns in
unlabeled sequences and pseudo-label these at the per-point
segmentation level. By extracting bounding boxes and in-
flating them to better match the amodal ground truth bound-
ing boxes we train an off-the-shelf 3D detection network.
Impact. Our data-driven approach not only allows us
to perform better as compared to a hand-crafted pipeline
based on density-based clustering [24] – more importantly,
we demonstrate that SeMoLi ’s performance improves with
increasing amount of data while still remaining a general
approach applicable on different classes and datasets. Just
as important, we democratize research of motion-inspired
auto-labeling in Lidar streams by making our models, code,
as well as previously inaccessible baselines [24] publicly
available to foster future research.
To conclude, as our main contribution we (i) show
that class-agnostic, motion-based segmentation for pseudo-
labeling can be tackled in a data-driven manner. To op-
erationalize this idea, we (ii) propose a data-driven point
clustering method leveraging motion patterns that utilizesMPNs, constructs a graph over a Lidar point cloud, and
learns which points belong together based on spatial and
motion cues. We (iii) demonstrate this network can be
used to successfully pseudo-label unlabeled sequences even
across unknown classes and datasets without the require-
ment of re-training. Finally, we (iv) make this field of re-
search reproducible and establish a solid ground for making
progress in this field of research as a community. We hope
that this is a stepping stone towards a better world in which
friendly robots do not accidentally crash into rare moving
objects just because no one wants to label data manually.
2. Related Work
Lidar-based 3D Object Detection. Amodal 3D local-
ization of objects is instrumental for situational awareness
in dynamic environments and has been playing a pivotal
role in autonomous navigation since its inception [27, 36].
Fueled by the recent availability of high-quality Lidar-
perception-centric datasets [4, 10, 33], several methods di-
rectly learn a representations from unordered point sets [28,
29] to localize and classify objects [20, 30–32, 45], or uti-
lize sparse 3D convolutional backbones [5, 43, 49]. Sev-
eral methods [44, 46] flatten 3D voxel representation along
the height dimension, while [17] replaces regular grid-based
discretization with vertical columns. All aforementioned
methods rely on manually labeled amodal 3D bounding
boxes that encapsulate the full extent of the object. Man-
ual annotations of such regions are costly, and, for practical
purposes, constraint target classes that must be detected to
a fixed, pre-defined set of objects.
Lidar-based bottom-up grouping. Prior to the advent of
end-to-end object detection models, Lidar perception mod-
els have been relying on bottom-up point grouping [8]. Sev-
eral methods utilize the connected components algorithm
on graphs, implicitly defined over the rasterized birds-eye-
view representation of point clouds [16, 35] or the full point
cloud [38] to localize disconnected regions, i.e., objects in
Lidar scans. Instead of utilizing graph-based representa-
tions, several methods [2, 14, 21, 24, 25, 42] utilize robust
density-based clustering methods (DBSCAN) [9] to group
points. The advantage of these methods is that the seg-
mentation is class-agnostic, largely relying on the Gestalt
principle [40] of spatial proximity, which is a strong cue
in the Lidar domain. However, these methods do not ben-
efit from the increasing amount of (labeled) data to im-
prove segmentation performance. Our work is inspired by
prior efforts that localize objects as connected components
in graphs [35, 38]. We revisit these ideas in the context
of data-driven grouping while still remaining class-agnostic
behavior.
Motion-supervised object detection in Lidar. Tracking-
based semi-supervised learning in Lidar is investigated
14686
preprocessing and trajectory predictionextracting and inflating bounding boxes
Train off-the-shelf detector (PointPillars)Apply SeMoLito unseen dataTake labeled data and labels from SeMoLi
SeMoLi
Figure 2. Segment Moving in Lidar for Pseudo-Labeling: We first preprocess the point cloud to remove static points and predict per-
point trajectories on the filtered point cloud ( preprocessing and trajectory prediction ). Then, we extract velocity-based features from the
trajectories and learn to cluster, i.e., segment points based on motion-patters using a Message-Passing Netowrk [11] in a fully data-driven
manner ( SeMoLi ). From point segments, we extract bounding boxes and inflate them ( extracting and inflating bounding boxes ). Finally,
we apply our approach on unlabeled Lidar streams to obtain pseudo-labels, that we use to train object detectors.
in [34] by performing bottom-up segmentation of Lidar
point clouds, followed by Kalman-filter-based object track-
ing of Lidar segments to expand the core labeled training
set via tracking. A recent work by [24] follows a similar
pipeline, however, first filters out stationary points to ob-
tain supervision for moving objects, as needed to train ob-
ject detectors. Another related approach by [48] follows
a similar pipeline, and additionally proposes a novel data
augmentation that allows object detectors to generalize to
far ranges solely based on instances observed in near sens-
ing areas. Unfortunately, the aforementioned are propri-
etary and do not share models, experimental data, or eval-
uation/implementational details. As these consist of hand-
crafted pipelines, results reported by these methods are dif-
ficult to reproduce, hindering the community’s progress in
this field of research.
We tackle a similar problem, however, by contrast, we
propose a data-driven, graph-based model that learns a rep-
resentation of the point cloud, suitable for localizing mov-
ing objects in Lidar. Our method performs favorably com-
pared to state-of-the-art even when trained on a tiny amount
of labeled data, and further improves with an increased
amount of training data. Moreover, we show we can apply
our approach to other datasets without any re-training of our
pseudo-labeling model. To foster future research, we make
our models, code, and experimental data publicly available.
3. Learning Moving Objecs in Lidar
We present Segment Moving in Lidar (SeMoLi ), our data-
driven approach for motion-inspired pseudo-labeling for 3D
object detection. Our teacher network learns to cluster
points in Lidar point clouds based on their motion patterns
(Sec. 3.1). This step can also be seen as point cloud seg-
mentation and corresponds to localizing moving object in-
stances. Once trained, we utilize SeMoLi to segment unla-
beled Lidar data, estimate amodal object-oriented boundingboxes from the segments and use these as a supervisory sig-
nal to train a student 3D object detector (Sec. 3.2).
3.1. Learning to Segment Moving Objects
Given input Lidar point clouds P, our task is to localize
individual moving objects as 3D bounding boxes. Each ob-
ject will be represented by an a-priori unknown number of
points, which we group together based on cues derived from
Gestalt principles [40]. We propose to use proximity ,i.e.,
nearby points likely belong to the same object, and common
fate,i.e., what moves together belongs together. Prior works
cluster points using, e.g., density [9, 24, 39], or graph-based
clustering methods [38] that base grouping decisions on the
local context and do not leverage the benefits of learning-
based algorithms. This is problematic for Lidar. On the one
hand, signal sparsity and the vast range of object sizes that
appear in the dataset ( e.g., pedestrian vs. truck) often lead
to over- and under-segmentation [13]. On the other hand,
the performance of such approaches suffers heavily in the
presence of background noise.
Correlation clustering revisited. To segment point clouds
into a set of instances, we lean on correlation clustering [1].
We represent a point cloud as a weighted graph G= (V, E)
with nodes Vand edges E. A node nirepresents a point and
encodes its spatial position and motion as node features h(0)
i
while an edge ei,jrepresents the geometrical connection be-
tween two points iandjwith edge features h(0)
ijrepresent-
ing their relationships. The clustering algorithm then cuts
edges to obtain a set of connected components which repre-
sent point cloud instance segmentations. We revisit correla-
tion clustering in the context of Message Passing Networks
(MPNs) [11]. Hence, we define node and edge features in a
data-driven manner to learn edge scores given some labeled
data.
Talk to your neighbors. MPNs propagate information
across the graph and ensure that the learned graph parti-
14687
tioning does not only rely on the local relationships between
points. Initial node h(0)
iand edge features h(0)
i,jare updated
in an iterative manner over Llayers. This ensures that the fi-
nal edge features h(L)
i,jcontains global information, provid-
ing the necessary context needed to decompose the graph
into object instances. We utilize a binary edge classifier on
top of h(L)
i,jto obtain our edge scores. In the following, we
discuss how we construct and parameterize our input graph,
and how we learn the representation using a MPN.
3.1.1 Graph Construction
Point cloud sequence preprocessing. Given a Lidar point
cloud sequence P={Pt∈RNt×3}, t∈1, . . . , T . We first
remove static points from the raw point clouds in Pt. This
yields a sequence of stationary point clouds ˜P={˜Pt∈
RMt×3}, t∈1, . . . , T , where Mt≤Nt. Our pre-filtering
step closely follows prior work [7, 24] (see appendix for
details). For simplicity, we omit index tin the remainder of
the paper.
Motion cues. For each filtered point cloud ˜Pwe
predict ego-motion compensated point trajectories T ∈
RM×(3×24)using a self-supervised trajectory prediction
network [37]. Each point trajectory τi∈ T is defined as
a sequence of point positions τi={pk
i}24
k=0. We utilize tra-
jectory information to encode motion cues into our graph
features, as described below.
Graph nodes V.For each filtered point cloud ˜P, we view
points as nodes in a graph with corresponding node features.
Node parametrization. The node feature h(0)
iincludes
its spatial coordinates (x, y, z )and the statistical measures
(mean, min, max) of the velocities along its trajectory:
h(0)
i= (xi, yi, zi,mean (vτi),min(vτi),max(vτi)).(1)
These features capture both the spatial position and the dy-
namic behavior of each point. The velocity at each time step
is calculated as the difference between consecutive points in
the trajectory: vτi=
pk+1
i−pk
i	k=24
k=0.
Graph edges E.We connect nodes ni∈Vwith edges ei,j
that hypothesize point-to-instance memberships. Utilizing
a fully connected graph is computationally infeasible and
introduces a large amount of negative edges that impedes
the learning process. Hence, we leverage proximity as an
inductive bias based on the intuition that “far away” points
are unlikely to belong to the same object instance. We con-
strain node connectivity to a set of k-nearest neighboring
nodes in terms of Euclidean distance. We discuss alterna-
tive graph construction strategy in the experimental section
(Sec. 4.2). Note that the final edge set is still biased towards
negative edges.Edge parametrization. We parametrize edges eijconnect-
ing nodes iandjvia initial edge features as the difference
in their spatial coordinates:
h(0)
ij= (xi−xj, yi−yj, zi−zj) (2)
This captures the relative spatial relationship between the
points in the point cloud. To ensure each edge obtains a
global view of the point cloud, as needed for reliable clus-
tering, we perform several Message Passing iterations, as
described below.
3.1.2 Representating Learning via Message Passing
The message passing algorithm involves iteratively updat-
ing node and edge embeddings. For a fixed number of iter-
ations L, we perform updates as follows:
Edge update. At each step l∈ {1, . . . , L }, we update the
embedding of an edge eijconnecting nodes iandjbased
on its previous embedding h(l−1)
ij and the embeddings of the
adjacent nodes h(l−1)
i, h(l−1)
j :
h(l)
ij=f
h(l−1)
ij, h(l−1)
i, h(l−1)
j
, (3)
where f(·,·,·)is an shared-weight update function, that
consists of a linear, a normalization, and a dropout layer.
Node update. Similarly, we update node embeddings
based on their previous embeddings as well as their neigh-
bors previous embeddings:
m(l)
i,j=g
h(l)
ij, hl−1
j, h(l−1)
i
, h(l)
i=ϕ({m(l)
i,j}j∈Ni),
(4)
where g(·,·)is the node update function with shared
weights over all layers that constitutes of a linear layer, a
normalization layer, and a dropout layer, ϕis a mean ag-
gregation function and Nidenotes the set of nodes adjacent
to node i. The iterative process of updating node and edge
embeddings allows for the integration of local and global
information in the graph, enabling the algorithm to capture
complex patterns and relationships within the data.
Edge classification. To determine the final edge score, we
feed the final edge features h(L)
ijthrough a final linear layer
fffollowed by a sigmoid layer σ:
˜h(L)
ij=σ
ff(h(L)
ij)
. (5)
For training details we refer the reader to appendix.
Postprocessing. During inference, we first cut negative
edges with score ˜h(L)
ij<0.5. To ensure robustness towards
a small set of possibly miss-classified outlier edges, we ap-
ply correlation clustering [1] using our learned edge scores
(details in the appendix) on top of this graph. We discard all
singleton nodes without edges. The resulting point clusters
cc∈Ccrepresent our segmented object instances.
14688
3.2. Learning to Detect Moving Objects
Training the student network requires to, first, transform a
segmented point cluster ccto a bounding box bc. Then we
enhance them by inflation to obtain our final pseudo-labels.
From point clusters to bounding boxes. Given a set of
points that constitutes a point cluster cc∈Cc, we deter-
mine the translation vector tcby taking the midpoint of all
points. Since each point ihas a trajectory τiassigned to
it, we can compute the mean trajectory ˆτc={ˆpk
c}24
k=0and
leverage ˆp1
candˆp2
cto determine the heading of the object in
xy-direction αc. With the heading αcand the translation
vector tcwe can transform the points to determine axis-
aligned length, width, and height of the bounding box lwhc
which yields the 3D bounding box bc= [tc, lwh c, αc].
Bounding Box Inflation. The bounding boxes bcrepresent
the most compact enclosing axis-aligned cuboid. Ground
truth bounding boxes are represented by typically more
loose, amodal bounding boxes. To adapt bcto the corre-
sponding ground truth data, we inflate them to have a min-
imum length, width, and height of xmin,ymin, and zmin,
respectively, to obtain our final pseudo-labels.
Detector training. We train PointPillars [18] (PP) two-
stage object detector in a class-agnostic setting using our
generated pseudo-labels. We utilize the detector implemen-
tation as well as hyperparameters settings from [6] (see ap-
pendix), and perform the following changes. To account
for objects of various sizes, we adapt anchor box gener-
ation with various size parameters (see appendix for de-
tails). We adapt the detection region to a planar 100x40m
field, centered at the egovehicle ( cf., [24]). We use binary
cross-entropy loss to train our object classifier ( object ,
background ).
4. Experimental Validation
In this section, we thoroughly ablate our motion-inspired,
data-driven approach Segment Moving in Lidar (SeMoLi )
and its individual components. In Sec. 4.1, we outline our
evaluation test-bed, which we use in Sec. 4.2 to discuss sev-
eral design decisions behind our pseudo-labeling network.
In Sec. 4.4, we evaluate the performance of our object de-
tector, trained using our pseudo-labels.
4.1. Experimental Setup
Datasets. We evaluate our method using Waymo
Open dataset [33] and only utilize Lidar data. The
dataset provides labels as amodal 3D bounding boxes for
pedestrian, vehicle andcyclist classes. We
assess the generalization of our method on Argoverse2
dataset [41] which provides finer-grained semantic labels.
Both datasets were recorded with different types of (propri-
etary) Lidar sensors (for details we refer to the appendix).
traindet(1-x%)valpseudo(50%)fixed validation setstrain setvaldet(50%)labeledpseudo labeledtrainpseudo(x%)validation settest setour splitoriginal splittestset(100%)variable sized train setsFigure 3. Train and validation splits: We conduct our exper-
iments using Waymo training set, for which manual labels are
available. We pre-fix two separate validation sets, one for validat-
ing pseudo-labels ( valpseudo ), and one for end-model detec-
tor performance ( valdet). We report performance on varying
ratios xfor training SeMoLi (train pseudo ) and generating
pseudo-labels for training our detector ( train det).
Evaluation setup. Following [24], we evaluate SeMoLi
as well as the final object detector in a 100x40mrectan-
gular region (region of highest importance for autonomous
vehicles), centered in the ego-vehicle Lidar sensor cen-
ter. As our pseudo-labels and trained detectors do not pro-
vide fine-grained semantic information, we follow the lit-
erature [24, 48] and evaluate both, SeMoLi andDeMoLi
in a class-agnostic setting. We assess our object detectors
onmoving-only (cf., [24]) as well as all labeled ob-
jects to see how well training instances mined from moving
regions generalize to non-moving objects. We consider an
object as moving if its velocity is larger than 1m/s. We
treatnon-moving instances as ignore regions, i.e., we ig-
nore detections having anyintersection with static objects,
consistent with the methodology used in prior works [24].
Semantic Oracle. For per-class recall analysis we assign
labels to class-agnostic detections if they have any 3D IoU
overlap with labeled boxes.
Metrics. To evaluate pseudo-label generation , we report
the F1 score, defined as the harmonic mean of precision and
recall. This metric is suitable for evaluating a set of predic-
tions that are not ranked, as is commonly employed in the
segmentation community [15]. As the output of this step is
point cloud instance segmentation, we report results using
both, the mask intersection-over-union (SegIoU) criterion,
as well as 3D bounding box IoU (3DIoU) criterion for quan-
tifying true positives and false negatives. This is impor-
tant as it assesses how well we can recover labeled amodal
bounding boxes that we need to use to train object detectors.
ForObject detection , the evaluate the performance using the
standard Average Precision metric that assumes as input a
ranked set of object detections. We focus on 3D bounding
box IoU as an evaluation criterion. For pseudo-label gener-
ation as well as object detection, we report performance us-
ing localization thresholds of T≤ {0.7,0.4}with0.7being
the standard evaluation threshold on Waymo Open dataset
and0.4the threshold used in [24].
Data splits. Each approach for pseudo-labeling re-
quires a certain percentage of labeled data to tune hyper-
parameters. We embrace a fully data-driven approach
14689
to pseudo-labeling: given some labeled data, we train
SeMoLi to localize moving objects in Lidar sequences. We
then pseudo-label the unlabeled set of training data, which
we use to train our student object detection network. This
requires a careful evaluation protocol, outlined in Fig. 3. We
split the official Waymo training set into fixed-size training
(train ) and validation ( val) splits.
Val. As we need to train and validate teacher andstudent
models independently, we split the val set into two equal-
sized splits. We use valpseudo for the validation of the
teacher network, and valdet to independently validate
thestudent (detection) network (Fig. 3).
Train . We split the train set into (labeled) training set
train pseudo , used to train the teacher network, then
we utilize this network to pseudo-label the student train-
ing set train pseudo . We use pseudo-labels to train the
detection network. Note that we split the dataset along se-
quences, i.e. we do not allow frames of the same sequence
to be in different splits.
Varying train set size. To study the amount of data
needed to train our model, we sample varying-sized sub-
sets for training the teacher and student networks, i.e.,
train pseudo andtrain det. Consistent with prior
work [24], we use the official Waymo Open validation for
the end-model evaluation. This set provides labels and al-
lows us to focus our evaluation on moving objects.
4.2. Ablations Studies
This section ablates the impact of different graph construc-
tion as well as parametrization strategies and bounding box
inflation on valpseudo dataset. Features based on ve-
locity and position are defined as in Sec. 3.1.1.
Graph construction strategies: As constructing a fully
connected graph is not feasible, we investigate different
strategies to construct a k-nearest-neighbor (kNN) graph
(Sec. 3.1.1) based on distance and velocity as similarity
measures. In Tab. 1) we report the oracle performance for
both, i.e., the performance we achieve with each strategy
assuming all edges are classified correctly. We base our de-
sign choice of utilizing position on its superior oracle per-
formance ( 90.9@0.4F1) (see appendix for deeper analysis).
Node features: As we show in Tab. 1, combining veloc-
ity and position in our node features significantly aids the
learning process. Solely utilizing velocity lowers recall and
slightly increases precision compared to position-based fea-
tures, indicating an increased amount of rejected positive
edge hypotheses due to possibly noisy velocity predictions.
On the other hand, position as node features leads to a se-
vere drop in precision and a slight drop in recall indicating
thatSeMoLi fails to correctly classify negative edges. Con-
catenating both, velocity enables SeMoLi to reject negative
edges between points in close proximity if they do not move
together and clusters points that move together .Method Pr 0.7 Re 0.7 F1 0.7 Pr 0.4 Re 0.4 F1 0.4GraphOracle Velocity kNN 35.7 67.1 46.6 39.4 74.1 51.4
Oracle Position kNN 85.0 87.9 86.4 89.4 92.5 90.9Node f.Velocity 61.2 52.7 57.0 73.0 62.2 67.1
Position 57.6 61.7 59.6 65.2 69.9 67.5
Velocity + Position 69.4 58.0 63.2 77.9 65.1 70.9Edge f.Velocity 58.9 48.6 53.2 69.6 57.4 62.9
Position 69.4 58.0 63.2 77.9 65.1 70.9
Velocity + Position 68.2 57.1 62.2 77.8 65.1 70.9
Table 1. SeMoLi10ablation (SegIoU): We discuss different
strategies on SeMoLi graph construction , as well as edge and
node feature parametrization.
3DIoU SegIoU
Pr 0.4 Re 0.4 F1 0.4 Pr 0.4 Re 0.4 F1 0.4
Initial 33.2 27.8 30.3 77.9 65.1 70.9
Inflated 59.1 48.3 53.1 80.9 66.2 72.8
Table 2. Bounding box inflation: We inflate tight bounding boxes
that enclose point clusters to a minimum width, length, and height.
The segmentation performance changes only insignificantly while
the detection performance improves drastically. SeMoLi10clus-
ters points together correctly, but generates bounding boxes that
are significantly tighter around the objects.
Method Pr 0.7 Re 0.7 F1 0.7 Pr 0.4 Re 0.4 F1 0.4
Oracle pseudo-label quality with ground truth flow / trajectories
DBSCAN++ 20.8 19.1 19.9 70.7 64.9 67.7
DBSCAN++†20.6 19.1 19.8 72.0 66.2 69.0
SeMoLi1024.7 23.1 23.9 76.0 71.2 73.5
Pseudo-label quality with our computed flow / trajectories
DBSCAN 0.9 5.7 1.5 6.0 39.3 10.4
DBSCAN†1.5 5.8 2.4 9.9 38.9 15.8
DBSCAN++ [24] 1.4 6.1 2.2 8.9 39.9 14.5
DBSCAN++†1.6 6.2 2.5 10.2 40.3 16.2
DBSCAN++†
l0.9 5.6 1.6 6.3 39.2 10.9
SeMoLi10
sf9.0 8.2 9.1 52.6 48.3 50.4
SeMoLi1010.1 8.2 9.1 59.0 48.3 53.1
SeMoLi509.0 8.9 9.0 56.7 55.7 56.1
SeMoLi908.8 9.1 9.0 56.9 58.4 57.6
Table 3. Pseudo-label quality comparison (3DIoU): We com-
pare our SeMoLi to different variants of DBSCAN [24], aug-
mented with scene flow (DBSCAN++), long-term trajectory in-
formation (DBSCAN++ l) and outlier filtering ( †). In gray ( top)
we report results using ground truth scene flow and trajectories,
and below we report scene flow and motion trajectories obtained
using [37]. SeMoLi consistently performs favorably compared to
all DBSCAN variants, when using perfect “oracle” motion cues,
as well as when using the estimated (noisy) scene flow method.
Edge features: As in graph construction, we consider L2
distance between position- and velocity-based features, as
well as the concatenation of both. As we show in Tab. 1,
adding velocity to or completely omitting position from
14690
DBSCAN++†SeMoLi10
3DIoU SegIoU 3DIoU SegIoU
Re (Pr) 0.4 Re (Pr) 0.4 Re (Pr) 0.4 Re (Pr) 0.4
Vehicle 36.7 70.1 54.5 76.9
Pedestrian 46.3 67.9 41.8 55.1
Cyclist 77.1 1.0 81.3 95.8
Class-agnostic 40.3 (10.2) 66.6 (16.8) 48.3 (59.1) 66.2 (80.9)
uFP 72.0 14.5
Table 4. Class-wise evaluation of pseudo-labels: For class-wise
evaluation, we assign GT classes to pseudo-labels that have any
overlap GT. We additionally report the %unmatched false posi-
tives (uFP), i.e., pseud-labels not matched to any GT box.
edge features harms the performance. Therefore, we use
position-based encoding for edges.
Bounding Box Inflation We inflate our bounding boxes
to a minimum length, width, and height and show the im-
pact on the performance on Waymo Open Dataset in Tab. 2.
While the performance based on SegIoU does not change
significantly, evaluating the performance based on 3DIoU
improves drastically. SeMoLi segments points correctly,
but generates significantly more tight bounding boxes com-
pared to ground truth (see appendix for details).
4.3. Evaluation of Pseudo-Label Quality
Baselines. We compare SeMoLi to three baselines:
vanilla DBSCAN, DBSCAN++ ( cf., [24]), and its variant
DBSCAN++ lthat utilizes our (long-term) velocity-based
motion feature for a fair comparison.†indicates heuris-
tic filtering based on bounding box dimensions (detailed in
the appendix). We also report variant of our method that
utilizes local motion cues (scene flow), SeMoLi sf.
Discussion. We discuss results in terms of 3DIoU, as the
output of this step is used to train object detectors that as-
sume amodal bounding boxes. In Tab. 3 ( top) we report
results we obtain with ground-truth motion information (or-
acle). In this setting, both DBSCAN ( 69.0@0.4F1) and
SeMoLi (73.5@0.4F1) perform well. However, when uti-
lizing, estimated motion information, all DBSCAN vari-
ants struggle with precision – even the variant with outlier
removal is unable to surpass 16.2@0.4F1. By contrast,
SeMoLi can learn to filter noise and undergoes a signifi-
cantly less severe performance drop as compared to its mo-
tion oracle. All SeMoLi variants surpass 50@0 .4F1, out-
performing DBSCAN in terms of precision andrecall.
Long-term motion cues. As can be seen, by contrast to
DBSCAN, our SeMoLi10method can learn to utilize long-
term motion cues, and performs favorably compared to the
variant that relies on local motion estimates, SeMoLi10
sf.
More data. Finally, we show SeMoLi benefits from the
increased amount of training data. SeMoLi ’s performance
monotonically increases with expanding training set, even-
tually reaching 57.6@0.4F1.DBSCAN++†SeMoLi90
3DIoU SegIoU 3DIoU SegIoU
Re (Pr) 0.4 Re (Pr) 0.4 Re (Pr) 0.4 Re (Pr) 0.4
Bicyclist 41.7 86.4 56.3 86.4
Box Truck 5.0 57.1 0 44.5
Bus 0 32.0 0.4 39.3
Large Vehicle 6.9 30.1 17.1 44.4
Motorcyclist 89.5 89.5 100 100
Pedestrian 29.7 59.8 42.5 57.1
Regular Vehicle 44.5 75.2 56.9 77.9
Stroller 0 0 0.5 0.5
Truck 0 97.1 4.4 94.1
Vehicular Trailer 3.5 14.0 0 14.0
Class-agnostic 33.3 (7.5) 59.3 (13.4) 45.8 (40.1) 65.2 (57.0)
Table 5. Cross-dataset generalization: We evaluate SeMoLi ,
trained on 90% labeled Waymo Dataset, on Argoverse2 dataset.
Note that we never train our approach on Argoverse2. We merge
Bicycle and Bicyclist as well as Motorcycle and Motorcyclist since
they are not distinguishable by motion.
Per-class analysis. In Tab. 4 we discuss the per-class anal-
ysis (based on our oracle classifier, see Sec. 4.1). While we
observe similar recall among SeMoLi and DBSCAN++†,
we note that the latter produces significantly more false pos-
itive pseudo-labels, quantified via %uFP, the percentage of
pseudo-labels not matched to any ground truth object. We
conclude that our approach produces a pseudo-label set that
has a significantly better signal-to-noise ratio.
Cross-dataset evaluation. We further examine the abil-
ity of cross-dataset generalization by evaluating our model
SeMoLi 90on the Argoverse2 dataset. Importantly, we
never ourSeMoLi on Argoverse2, but solely on the
train pseudo split of Waymo Open dataset with x=
90%. Argoverse2 contains finer-grained semantic labels
compared to Waymo, which allows us to assess generaliza-
tion to a wide range of classes. We note that Waymo and
Argoverse2 are recorded with different, proprietary Lidar
datasets, requiring our approach to generalize across sen-
sors. In Tab. 5 we show, SeMoLi again consistently per-
forms favorably compared to the DBSCAN baseline.
4.4. Object Detection
Finally, we utilize pseudo-labeled data to train an off-the-
shelf object PointPillars (PP) object detector [19].
Experimental setting. As a baseline, we train PP in
the standard setting with fine-grained semantic information
(class-specific ) as well as class-agnostic . The latter more
closely resembles our setup, in which we do not have ac-
cess to semantic information. Further, we train PP using all
objects ( stat. + mov ), as well as moving-only ( mov-only ).
We evaluate detectors on allobjects (Tab. 6, top), as well as
only on moving objects (Tab. 6, bottom , as reported in [24]).
Additionally, we mix pseudo-labeled with 10% of labeled
data (for finer-grained analysis see appendix) by re-using
thetrain pseudo splits (Sec. 4.1).
14691
% Pseudo % GT Pr 0.7 Re 0.7 AP 0.7 mAP 0.7 Pr 0.4 Re 0.4 AP 0.4 mAP 0.4All (Mov. + stat.)Stat. + Mov., class-specific 0 100 35.5 55.5 – 37.1 69.7 44.6 – 80.3
Stat. + Mov., class-agnostic 0 100 31.8 41.2 36.1 – 51.2 66.5 64.7 –
Mov-only, class agnostic 0 100 34.4 19.9 15.1 – 63.9 37.1 35.0 –
DBSCAN++†90 10 7.4 35.1 31.1 – 11.5 55.0 52.0 –
DBSCAN++†100 0 0.8 3.4 0.8 – 5.9 25.9 14.9 –
SeMoLi 90 0 3.8 3.4 1.8 – 26.9 24.1 19.5 –
SeMoLi 90 10 25.4 35.4 31.8 – 40.7 56.8 54.6 –Moving onlyStat. + Mov., class-specific 0 100 30.5 34.5 – 43.2 36.4 41.1 – 85.6
Stat. + Mov., class-agnostic 0 100 16.1 52.8 44.8 – 28.1 92.4 88.7 –
Mov-only, class agnostic 0 100 33.9 53.7 44.3 – 57.6 91.2 89.0 –
DBSCAN++†90 10 2.0 34.5 29.8 – 4.1 70.1 61.0 –
DBSCAN++†100 0 5.9 9.7 2.4 – 3.6 58.6 43.2 –
SeMoLi 90 0 3.8 10.7 4.2 – 23.3 66.0 57.5 –
SeMoLi 90 10 9.4 35.4 31.7 – 19.7 74.6 66.2 –
DBSCAN++ [24] 100 0 – – – – – – – 40.4
Table 6. Semi-supervised 3D object detction on Waymo Open Dataset: We evaluate models on all(top) and only moving (bottom ) on
Waymo Open validation set. % GT indicates the amount of labeled training data, % Pseudo indicates the amount of pseudo-labeled data.
P% L% Pr 0.4 Re 0.4 AP 0.4AllLabeled 0 100 52.3 39.3 35.5
SeMoLi 100 0 12.2 30.7 22.9Mov.Labeled 0 100 45.1 85.5 82.4
SeMoLi 100 0 8.0 64.7 57.6
Table 7. Cross dataset results: We train PP detector on ground
truth data as well as on pseudo labels generated with SeMoLi
trained on Waymo Open Dataset.
All objects. In Tab. 6 ( top) we discuss results on all,i.e.,
static and moving objects. We focus this discussion on 0.4
threshold. As can be seen, with vanilla PP detector, we
obtain 80.3mAP. In the class agnostic setting, we obtain
64.7AP, which drops to 35AP when only training with GT
boxes labeled as moving. When using 10% of GT labels,
we obtain 52.0and54.8AP with DBSCAN and SeMoLi ,
respectively. Remarkably, when utilizing anypseudo-labels
in conjunction with 10% labeled data, we obtain higher AP
compared to the variant, trained with moving-only GT la-
bels. This is likely because due to the noisy estimated flow,
we retain some static objects. The moving-only GT version
learns to only predict objects in regions where moving ob-
jects are likely to appear. Additionally, pseudo-labels may
induce generalization to the training process. When not uti-
lizing any labeled data, we obtain 14.9and19.5AP, respec-
tively. With SeMoLi , we, therefore, recover 56.6%perfor-
mance of the variant, trained on GT moving-only labels.
Moving objects. When analyzing the performance on
moving objects only in Tab. 6 ( bottom ), models, trained on
pseudo-labeled bounding boxes, are significantly closer to
fully supervised models ( 88.7trained on all data, and 89.0
when trained with moving only). Utilizing no labeled data,DBSCAN reaches 43.2(48% of GT model), while SeMoLi
reaches 57.5(64% of GT model). When using 10% of la-
beled data, SeMoLi reaches 66.2AP (64% of GT model).
We note [24] reports 40.4mAP on the validation set (last
row), however, this is not an apple-to-apple comparison, as
[24] does not provide implementation or detailed descrip-
tion of how the analysis was conducted. Moreover, it is
unclear how [24] reports mAP in a class-agnostic setting.
Cross-dataset performance. Finally, in Tab. 7 we re-
port results we obtain by training PP detector on Argo-
verse2 dataset. We compare the supervised detector ( la-
beled ) toSeMoLi , trained via pseudo-labels, generated on
Argoverse2. Importantly, we generate pseudo-labels using
SeMoLi trained on Waymo dataset, thus truly assessing
cross-dataset generalization. When reporting results on all
objects, we obtain 35.5AP with the supervised model, and
22.9with ours. For moving objects, we obtain 82.9with
the supervised model, and 57.6with our approach, confirm-
ing our approach is indeed general and transferable across
datasets. We detail the training in appendix.
5. Conclusions
We introduced SeMoLi , a data-driven, class-agnostic ap-
proach for pseudo-labeling moving objects in Lidar. We
devised SeMoLi by revisiting correlation clustering in the
context of message passing networks training our model to
learn to decompose graphs constructed from point clouds.
We utilized SeMoLi to pseudo-label data, as needed to train
object detectors, and demonstrated that our data-driven ap-
proach performs favorably compared to prior art, and, more
importantly, generalizes across datasets. By making our
code, experimental data ad baselines publicly available we
hope to inspire future efforts in this field of research.
14692
References
[1] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation
clustering. Machine learning , 56:89–113, 2004. 3, 4
[2] Jens Behley, V olker Steinhage, and Armin B Cremers. Laser-
based segment classification using a mixture of bag-of-
words. In Int. Conf. Intel. Rob. Sys. , 2013. 2
[3] H. Caesar, V . Bankiti, A.H. Lang, S. V ora, V .E. Liong, Q. Xu,
A. Krishnan, Y . Pan, G. Baldan, and O. Beijbom. nuScenes:
A multimodal dataset for autonomous driving. In CVPR ,
2020. 1
[4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuScenes: A multi-
modal dataset for autonomous driving. In CVPR , 2020. 2
[5] C. Choy, J.Y . Gwak, and S. Savarese. 4D spatio-temporal
convnets: Minkowski convolutional neural networks. In
CVPR , 2019. 2
[6] MMDetection3D Contributors. MMDetection3D: Open-
MMLab next-generation platform for general 3D object
detection. https://github.com/open-mmlab/
mmdetection3d , 2020. 5
[7] Ayush Dewan, Tim Caselitz, Gian Diego Tipaldi, and Wol-
fram Burgard. Motion-based detection and tracking in 3d
lidar scans. In Int. Conf. Rob. Automat. , 2015. 4
[8] Bertrand Douillard, James Underwood, Noah Kuntz,
Vsevolod Vlaskine, Alastair Quadros, Peter Morton, and
Alon Frenkel. On the segmentation of 3d lidar point clouds.
InInt. Conf. Rob. Automat. , 2011. 2
[9] Martin Ester, Hans-Peter Kriegel, J ¨org Sander, Xiaowei Xu,
et al. A density-based algorithm for discovering clusters in
large spatial databases with noise. In Rob. Sci. Sys. , 1996. 2,
3
[10] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the KITTI vision benchmark
suite. In CVPR , 2012. 1, 2
[11] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol
Vinyals, and George E. Dahl. Neural message passing for
quantum chemistry. 2017. 2, 3
[12] David Held, Jesse Levinson, Sebastian Thrun, and Silvio
Savarese. Combining 3d shape, color, and motion for robust
anytime tracking. In Rob. Sci. Sys. , 2014. 1
[13] David Held, Devin Guillory, Brice Rebsamen, Sebastian
Thrun, and Silvio Savarese. A probabilistic framework for
real-time 3d segmentation using spatial, temporal, and se-
mantic cues. In Rob. Sci. Sys. , 2016. 1, 3
[14] Peiyun Hu, David Held, and Deva Ramanan. Learning to
optimally segment point clouds. IEEE Rob. Automat. Letters ,
5(2):875–882, 2020. 2
[15] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll ´ar. Panoptic segmentation. In CVPR ,
2019. 5
[16] Klaas Klasing, Dirk Wollherr, and Martin Buss. A clustering
method for efficient segmentation of 3d laser data. In Int.
Conf. Rob. Automat. , 2008. 2
[17] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,
Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders
for object detection from point clouds. In CVPR , 2019. 2[18] Alex H. Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,
Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders
for object detection from point clouds. In CVPR , 2019. 5
[19] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,
Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders
for object detection from point clouds. In CVPR , 2019. 7
[20] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong.
Group-free 3d object detection via transformers. 2021. 2
[21] Leland McInnes, John Healy, and Steve Astels. HDBSCAN:
Hierarchical density based clustering. J. Open Source Softw. ,
2(11):205, 2017. 2
[22] Frank Moosmann and Christoph Stiller. Joint self-
localization and tracking of generic objects in 3d range data.
InInt. Conf. Rob. Automat. , 2013. 1
[23] Frank Moosmann, Oliver Pink, and Christoph Stiller. Seg-
mentation of 3d lidar data in non-flat urban environments us-
ing a local convexity criterion. In Intel. Veh. Symp. , 2009.
1
[24] Mahyar Najibi, Jingwei Ji, Yin Zhou, Charles R Qi,
Xinchen Yan, Scott Ettinger, and Dragomir Anguelov. Mo-
tion inspired unsupervised perception and prediction in au-
tonomous driving. In ECCV , 2022. 1, 2, 3, 4, 5, 6, 7, 8
[25] Lucas Nunes, Xieyuanli Chen, Rodrigo Marcuzzi, Aljosa
Osep, Laura Leal-Taix ´e, Cyrill Stachniss, and Jens Behley.
Unsupervised class-agnostic instance segmentation of 3d li-
dar data for autonomous vehicles. IEEE Rob. Automat. Let-
ters, 7(4):8713–8720, 2022. 2
[26] Aljo ˇsa O ˇsep, Paul V oigtlaender, Jonathon Luiten, Stefan
Breuers, and Bastian Leibe. Large-scale object mining for
object discovery from unlabeled video. In Int. Conf. Rob.
Automat. , 2019. 2
[27] A. Petrovskaya and S. Thrun. Model based vehicle detec-
tion and tracking for autonomous urban driving. 26:123–139,
2009. 2
[28] C.R. Qi, H. Su, K. Mo, and L.J. Guibas. Pointnet: Deep
learning on point sets for 3d classification and segmentation.
InCVPR , 2017. 1, 2
[29] C.R. Qi, L. Yi, H. Su, and L.J. Guibas. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
arXiv , 2017. 1, 2
[30] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J
Guibas. Frustum pointnets for 3d object detection from rgb-d
data. In CVPR , 2017. 2
[31] Charles R Qi, Or Litany, Kaiming He, and Leonidas J
Guibas. Deep hough voting for 3d object detection in point
clouds. In CVPR , 2019.
[32] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. PointR-
CNN: 3D Object Proposal Generation and Detection From
Point Cloud. In CVPR , 2019. 2
[33] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, et al. Scalability in perception
for autonomous driving: Waymo open dataset. In CVPR ,
2020. 1, 2, 5
[34] Alex Teichman and Sebastian Thrun. Tracking-based semi-
supervised learning. Int. J. Rob. Research , 31(7):804–818,
2012. 1, 3
14693
[35] Alex Teichman, Jesse Levinson, and Sebastian Thrun. To-
wards 3D object recognition via classification of arbitrary
object tracks. In Int. Conf. Rob. Automat. , 2011. 1, 2
[36] Charles Thorpe, Martial Herbert, Takeo Kanade, and Steven
Shafer. Toward autonomous driving: the cmu navlab. i. per-
ception. IEEE expert , 6(4):31–42, 1991. 2
[37] Chaoyang Wang, Xueqian Li, Jhony Kaesemodel Pontes,
and Simon Lucey. Neural prior for trajectory estimation. In
CVPR , 2022. 4, 6
[38] Dominic Zeng Wang, Ingmar Posner, and Paul Newman.
What could move? Finding cars, pedestrians and bicyclists
in 3D laser data. In Int. Conf. Rob. Automat. , 2012. 2, 3
[39] Xudong Wang, Zhaowei Cai, Dashan Gao, and Nuno Vas-
concelos. Towards universal object detection by domain at-
tention. In CVPR , pages 7289–7298, 2019. 3
[40] Max Wertheimer. Untersuchungen zur lehre von der gestalt:
I. prinzipielle bemerkungen. Psychologische forschung , 1
(1):47–58, 1922. 1, 2, 3
[41] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-
bert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-
nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,
Deva Ramanan, Peter Carr, and James Hays. Argoverse 2:
Next generation datasets for self-driving perception and fore-
casting. In Adv. Neural Inform. Process. Syst. , 2021. 5
[42] Kelvin Wong, Shenlong Wang, Mengye Ren, Ming Liang,
and Raquel Urtasun. Identifying unknown instances for au-
tonomous driving. In Conference on Robot Learning , pages
384–393. PMLR, 2020. 2
[43] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-
ded convolutional detection. Sensors , 18(10):3337, 2018. 2
[44] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-
time 3d object detection from point clouds. In CVPR , 2018.
2
[45] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd:
Point-based 3d single stage object detector. In CVPR , 2020.
2
[46] Tianwei Yin, Xingyi Zhou, and Philipp Kr ¨ahenb ¨uhl. Center-
based 3d object detection and tracking. In CVPR , 2021. 2
[47] Hao Zhang, Christopher Reardon, and Lynne E Parker. Real-
time multiple human perception with color-depth cameras on
a mobile robot. IEEE Transactions on Cybernetics , 43(5):
1429–1441, 2013. 1
[48] Lunjun Zhang, Anqi Joyce Yang, Yuwen Xiong, Sergio
Casas, Bin Yang, Mengye Ren, and Raquel Urtasun. To-
wards unsupervised object detection from lidar point clouds.
InCVPR , 2023. 2, 3, 5
[49] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning
for point cloud based 3d object detection. In CVPR , 2018. 2
14694
