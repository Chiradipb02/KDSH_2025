TE-TAD: Towards Full End-to-End Temporal Action Detection via
Time-Aligned Coordinate Expression
Ho-Joong Kim1Jung-Ho Hong1Heejo Kong2Seong-Whan Lee1⇤
1Dept. of Artiﬁcial Intelligence, Korea University, Seoul, Korea
2Dept. of Brain and Cognitive Engineering, Korea University, Seoul, Korea
{hojoong kim, jungho-hong, hj kong, sw.lee }@korea.ac.kr
Abstract
In this paper, we investigate that the normalized co-
ordinate expression is a key factor as reliance on hand-
crafted components in query-based detectors for tempo-
ral action detection (TAD). Despite signiﬁcant advance-
ments towards an end-to-end framework in object detec-
tion, query-based detectors have been limited in achiev-
ing full end-to-end modeling in TAD. To address this is-
sue, we propose TE-TAD, a full end-to-end temporal ac-
tion detection transformer that integrates time-aligned co-
ordinate expression. We reformulate coordinate expression
utilizing actual timeline values, ensuring length-invariant
representations from the extremely diverse video duration
environment. Furthermore, our proposed adaptive query
selection dynamically adjusts the number of queries based
on video length, providing a suitable solution for varying
video durations compared to a ﬁxed query set. Our ap-
proach not only simpliﬁes the TAD process by eliminating
the need for hand-crafted components but also signiﬁcantly
improves the performance of query-based detectors. Our
TE-TAD outperforms the previous query-based detectors
and achieves competitive performance compared to state-
of-the-art methods on popular benchmark datasets. Code is
available at: https://github.com/Dotori-HJ/TE-TAD
1. Introduction
Temporal action detection (TAD) plays an essential role in
video understanding and its numerous real-world applica-
tions, such as video surveillance, video summarization, and
video retrieval. TAD aims to recognize and localize actions
within untrimmed video sequences by identifying the class
labels with precise start and end times of action instances.
Recently, TAD methods can be mainly divided by three ap-
proaches: anchor-based [ 3,15,16,21,23,31,34], anchor-
free [ 6,14,27,32], and query-based [ 18,26,28] detector.
Query-based detectors, inspired by DETR [ 4], have at-
tracted interest because of their potential to eliminate re-
liance on hand-crafted components, such as the sliding win-
*Corresponding authorFigure 1. Performance comparison of query-based detectors
across various feature coverages on THUMOS14, demonstrat-
ing how extending the feature coverage impacts detection perfor-
mance, as measured by mean Average Precision (mAP@A VG).
The full end-to-end setting that without coverage constraints is de-
noted by 1.
dow and non-maximum suppression (NMS). This potential
derives from adopting a set-prediction mechanism, which
aims to provide an end-to-end detection process by utiliz-
ing a one-to-one matching paradigm. Despite these ad-
vantages, query-based detectors encounter signiﬁcant chal-
lenges in two aspects: (1) they show decreased performance
when dealing with extended temporal coverage, often mak-
ing them a less favorable option compared to anchor-free
detectors, and (2) due to limited extended temporal cover-
age, the reliance on the sliding window approach leads to
redundant proposals and necessitates the use of NMS.
To demonstrate these issues, we conduct experiments by
increasing the feature coverages on existing query-based
detectors. Fig. 1demonstrates the change in performance
across various feature coverage. Except for unrestricted fea-
ture coverage denoted by 1, feature coverage is calculated
from the window size of the sliding window method. As
shown in the graph, even though wide feature coverage is
beneﬁcial to capturing longer context, the performance of
existing query-based detectors diminishes in mean Average
Precision (mAP) as feature coverage increments. This re-
sult indicates that there are signiﬁcant limitations to query-
based detectors in scalability to temporal length, despite ef-
forts to address long durations [ 32] in TAD.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18837
Furthermore, Fig. 2shows the second issue, illustrating
the limitations of the sliding window approach. As shown
in Fig. 2(a), the sliding window is limited to address long-
range duration due to its limited window size. Moreover,
the sliding window approach contains overlapping areas
to prevent predictions from being truncated in the middle.
These overlapping areas generate duplicate predictions, ne-
cessitating the use of NMS to ﬁlter the false positive cases.
This reliance on NMS contradicts the set-prediction goal
of minimizing hand-crafted components, thus hindering the
achievement of a fully end-to-end TAD. To address these
issues, we investigate why extended temporal coverage ad-
versely affects the performance of query-based detectors
(Sec. 3.3). Our investigation reveals that the conventional
use of normalized coordinate expressions is a signiﬁcant
factor, disturbing the achievement of a full end-to-end TAD.
In this paper, we propose a full end-to-end temporal ac-
tion detection transformer that integrates time-aligned coor-
dinate expression (TE-TAD), which reformulates normal-
ized coordinate expression to actual timeline video val-
ues. Our reformulation enables the query-based detector
to effectively address length-invariant modeling by avoid-
ing the distortion of the normalizing process, which not
only enhances the detection performance but also simpli-
ﬁes the detection process. Our TE-TAD stabilizes the train-
ing process of the query-based detector when dealing with
extended videos; our approach shows signiﬁcant improve-
ments and completely removes the reliance on hand-crafted
components such as the sliding window and NMS. Fur-
thermore, we introduce an adaptive query selection that ef-
fectively addresses various video lengths, dynamically ad-
justing the number of queries in response to the temporal
length of each video. In contrast to relying on a ﬁxed set of
queries, our TE-TAD provides a suitable approach to pro-
cess diverse video lengths. Our approach shows signiﬁcant
improvements compared to previous query-based detectors
and achieves competitive performance with state-of-the-art
methods on popular benchmark datasets: THUMOS14 [ 10],
ActivityNet v1.3 [ 8], and EpicKitchens [ 7].
Our contributions are summarized as three-fold:
•We propose a full end-to-end temporal action detection
transformer that integrates time-aligned coordinate ex-
pression (TE-TAD), which preserves the set-prediction
mechanism and enables a full end-to-end modeling for
TAD by eliminating the hand-crafted components.
•Our approach introduces a length-invariant mechanism to
query-based detectors, signiﬁcantly improving scalability
in handling varying lengths of videos.
•Our TE-TAD signiﬁcantly outperforms the previous
query-based detectors and achieves competitive perfor-
mance compared to state-of-the-art methods, even with-
out hand-crafted components such as the sliding window
and NMS.
Window SizeWindow SizeWindow SizeDuplicationDuplication
(a) Sliding window
End-to-End(b) End-to-end setting
Figure 2. Comparison between sliding window and end-to-end
settings. The sliding window generates redundant proposals in the
duplicated area.
2. Related Work
Action Recognition Action Recognition is a foundational
task in video understanding, categorizing video sequences
into distinct action classes. Notable models include I3D
[5], which enhances the inception network with 3D convo-
lutions, and R(2+1)D [ 29], separating 3D convolutions into
2D spatial and 1D temporal parts for efﬁcient processing.
TSP [ 2] introduces temporal channel shifting for effective
temporal modeling without extra computation. VideoSwin
[20], employing the SwinTransformer [ 19] architecture, ex-
cels in complex video data recognition. These models serve
as backbones for extracting video features, directly impact-
ing performance in subtasks like TAD.
Anchor-based Detector Anchor-based detectors [ 3,15,16,
21,23,31,34] leverage predeﬁned anchor boxes to gener-
ate action proposals. These hand-designed anchors hinder
the diverse range of action instances because of a lack of
ﬂexibility in the localization of the action instances. This
approach inherently limits and necessitates additional post-
processing steps to discard redundant proposals because
they model the one-to-many assignment training strategy.
Anchor-free Detector Anchor-free detectors [ 6,14,25,27,
32,35] offer more ﬂexibility in action instance localization
compared to anchor-based detectors by adopting an asym-
metric modeling approach. For instance, ActionFormer
[32] signiﬁcantly enhances TAD performance by employing
a transformer-based architecture that captures long-range
video dependencies. TriDet [ 27] demonstrates superior per-
formance in TAD by employing a trident prediction scheme
and their proposed convolution-based architecture. Despite
their improvements, anchor-free detectors rely on hand-
crafted components to remove redundant proposals using
NMS because they adopt a one-to-many assignment train-
18838
ing manner. In contrast, our approach directly addresses a
one-to-one matching scheme, eliminating the use of NMS.
Query-based Detector Query-based detectors, inspired by
DETR [ 4], introduce a set-prediction mechanism, thereby
reducing the reliance on hand-crafted components, which
ideally prevents the need for NMS. However, existing
query-based detectors still require NMS because their
model design inherently breaks the one-to-one matching
paradigm. RTD-Net [ 28] utilizes a one-to-many match-
ing to mitigate the slow convergence issue associated with
the detection transformer. This approach inherently breaks
the one-to-one assignment. ReAct [ 26] modify the de-
coder’s self-attention, called relational attention, only adopt
self-attention between their deﬁned relations. This par-
tial adoption of the decoder’s self-attention disturbs the
set-prediction mechanism because they cannot capture the
whole context of queries. Furthermore, previous query-
based detectors [ 18,26,28] adopt the sliding window
method that contains overlapping areas, causing redundant
proposals. Furthermore, TadTR [ 18] deals with one-to-
many matching at training loss. TadTR employs cross-
window fusion (CWF), which applies NMS to overlap-
ping areas to remove redundant proposals. In contrast,
our approach entirely preserves the one-to-one matching
paradigm, which enables a full end-to-end modeling.
3. Our Approach
3.1. Overview
In this section, we ﬁrst discuss about the limitations of ex-
isting query-based detectors in TAD, focusing on the nor-
malized coordinate expression. The normalized coordinate
expression, used in existing models, causes matching insta-
bility and sensitivity, especially in extended video scenar-
ios. Subsequently, to introduce our TE-TAD, we describe
the reformulation of normalized coordinate expression to
timeline coordinate expression and adaptive query propos-
als to ensure the length-invariant modeling. The overall ar-
chitecture of TE-TAD is illustrated in Fig. 5.
3.2. Preliminary
LetX2RT0⇥Cdenote the video feature sequence ex-
tracted by the backbone network, where T0is the tempo-
ral length of the features, and Cis the dimension of the
video feature. Each element in the video feature sequence
represented as X={xt}T0
t=1, corresponds to a snippet at
timestep t, with each snippet comprising a few consecutive
frames. These snippets are processed using a pre-trained
backbone network such as I3D [ 5] or SlowFast [ 9]. Each
video contains numerous action instances, and each action
instance contains start and end timestamps sande, along
with its action class c. Formally, the set of action instances
in a video is represented as A={(sn,en,cn)}N
n=1, where(a)
(b)
Figure 3. Comparative analysis of instability and detection perfor-
mance on THUMOS14: (a) variance in instance matching quanti-
ﬁed by IS; (b) the performance change in mAP. Feature coverage
lengths are denoted by the values following the dash (-). The sym-
bol1denotes an unrestricted end-to-end setting.
Nis the number of action instances, and snandenare the
start and end timestamps of an action instance, respectively,
andcnis its action class. The main goal of TAD is to ac-
curately predict the set of action instances Afor any given
video. Previous query-based detectors [ 18,26,28] typically
compute predicted values of center ˆcand width ˆdusing a
sigmoid function (  ). Consequently, existing models de-
code predicted start ˆsand end ˆetimestamps by  (ˆc)  (ˆd)
and (ˆc)+ (ˆd)to start and end timestamps, respectively.
3.3. Exploring Key Issues
Matching Instability As discussed in Sec. 1, extended
video lengths signiﬁcantly inﬂuence the performance of ex-
isting query-based methods. To investigate this issue, we
conduct a comparative study on the instability (IS) [ 13] and
the detection performance across different feature cover-
age scenarios on THUMOS14. Here, IS is a quantitative
measurement of the inconsistency of matching during the
training process. For query-based detectors, ﬂuctuations in
matched targets compel the model to be learned from dif-
ferent values for the same input, leading to performance
degradation. In the TadTR setting, smaller window sizes
have more steps per epoch due to generating more sliding
windows, whereas larger window sizes yield fewer steps
per epoch. This mismatch in the number of steps results
that with fewer updates per epoch, there is inherently less
change to the model, which shows lower instability. For a
fair comparison, we match the number of iteration steps per
epoch, aligning with TadTR-34.1s (original TadTR).
18839
Fig. 3illustrates the instability and detection perfor-
mance across diverse feature coverage. As feature coverage
increases, we observe a rise in IS across the training epoch,
indicating less stable instance matching, which leads to a
decline in detection performance. This analysis underscores
the challenge of maintaining consistent learning when with
extended temporal lengths. Furthermore, a direct compar-
ison between TadTR models and our method reveals that
Ours- 1maintains a level of stability comparable to TadTR-
34.1s. This indicates that our approach signiﬁcantly stabi-
lizes the training process relative to TadTR- 1. Moreover,
our method demonstrates similar levels of matching insta-
bility yet shows signiﬁcant improvements under more chal-
lenging conditions for matching problems, even when com-
pared to models with shorter feature coverage like TadTR-
34.1s and TadTR-273.1s.
(a) (ˆc+✏)(b) (ˆd+✏)
Figure 4. Analysis of noise tolerance on predicted value. The noise
level ✏sampled from uniform distribution and injected before the
sigmoid function.
Sensitivity to Localize Action Instances To further ex-
plore this issue, we investigate the sensitivity of model pre-
diction measured by adopting minor perturbations in pre-
dictions of localization. We inject a small scale of noise
sampled from a uniform distribution before adopting the
sigmoid function that normalizes coordinates within a [0,
1] range. Fig. 4shows that even minimal noise injection
signiﬁcantly affects the prediction when applying the cen-
ter prediction value. The noise ✏⇠Uniform ( ↵, ↵)is
sampled from the uniform distribution. As illustrated in
Fig.4(a), the model’s sensitivity to small shifts is evident
from the signiﬁcant decline in  mAP@A VG upon intro-
ducing noise to the center predicted value. Notably, with
noise levels of ±0.01and±0.1, after processing through
thesigmoid function (  ), the maximal shifts are conﬁned
within ±0.0025 and±0.025in the normalized coordinate
space, respectively. These ﬁndings highlight that even mi-
nor output variations amplify in extended videos, leading
to considerable drops in performance due to the heightened
sensitivity in the normalized coordinate framework. To ad-
dress this issue, we introduce a time-aligned coordinate ex-
pression that is not normalized, thereby ensuring indepen-
dence from video length and reducing sensitivity.3.4. TE-TAD
This part describes our TE-TAD for a full end-to-end TAD.
We adopt the TadTR [ 18] architecture as a baseline method,
including encoder, decoder, and temporal deformable at-
tention architecture. Starting with the baseline, we mainly
address three aspects: (1) adopting multi-scale and two-
stage methods from the previous methods in object detec-
tion to bridge the performance gap between query-based
and anchor-free detectors, (2) reformulating coordinate ex-
pression utilizing the actual time values to address extended
video length in an end-to-end setting, and (3) proposing an
adaptive query selection that dynamically adjusts the num-
ber of queries based on the diverse length of videos.
Embedding & Multi-Scale Features We project input fea-
tures Xusing a single convolutional neural network to align
them with the dimension of the transformer architecture.
The projection maps the input feature Xto the embedded
feature Z12RD⇥T1, where Ddenotes the channel dimen-
sion of the encoder and decoder transformer architecture.
The temporal length T1of the embedded features Z1re-
mains the same as the original T0. Subsequently, following
previous approaches [ 27,32], we incorporate multi-scale
generation to effectively address varying lengths of actions.
Unlike utilizing a transformer [ 32], we employ a single con-
volution layer with a stride of 2 to produce features at each
scale level as follows:
Zl=LayerNorml(Conv l(Zl 1)),l2{2,. . . ,L }, (1)
where Zl2RC⇥Tlrepresents the embedded features at
each level l, and Ldenotes the total number of feature lev-
els. Each subsequent level lhas a temporal length Tlthat
is half of the temporal level of the previous level Tl 1. The
LayerNormland Conv ldenote l-th layer normalization and
convolutional neural networks, respectively. We do not ap-
ply any activation function in this process to deliver the raw
feature representations to the transformer detector.
Time-Aligned Query Generation Our method follows a
two-stage approach [ 33,37] that generates initial action pro-
posals using the transformer encoder. The previous two-
stage approach [ 37] provides the reference to the encoder’s
outputs. The transformer encoder predicts a binary fore-
ground score, p(0), and segment offsets  c(0)and d(0)to
reﬁne segments based on reference, for each time tand level
l. We deﬁne reference for center crefand width drefpredic-
tions, aligning with the real timeline of the video. For each
scale level l, the reference for the center is computed as fol-
lows:
cref=⇢
t⇥f
w⇥2l 1+w⇥2l 1
2 Tl
t=1,l2{1,2,...,L },
(2)
where fdenotes the frame-per-second rate of the video, and
wrepresents step size for feature extracting that indicates
18840
DecoderMulti-Scale Features
EncoderAction ProposalAdaptive Query SelectionVideo
BackboneTime-Aligned Query Generation
Timeline
...
......
Top-K SelectionTop-K SelectionTop-K Selection
Timeline
TimelineFigure 5. Overview of the TE-TAD. Starting with video input, the architecture processes through a backbone for feature extraction,
generating multi-scale features Z. These are encoded and subsequently passed through an adaptive query selection, aligning with the video
timeline for initial query generation. The decoder reﬁnes these queries layer-by-layer, culminating in the reﬁnement of action proposals.
how many frames to step when extracting features. The fac-
tor2l 1ﬁts the temporal lengths to the embedded features
from the multi-scale generation, corresponding with the ac-
tual timeline. Subsequently, the reference for the width is
computed as follows:
dref=↵·f⇥2l 1,l2{1,2,...,L }, (3)
where ↵is the base scale for encoder proposals, which ad-
justs the length of the reference width length. Consequently,
the time-aligned queries are decoded as follows:
(ˆc(0),ˆd(0))=( cref+ c(0)·dref,exp(ln( dref)+ d(0)))(4)
where ˆc(0)andˆd(0)denote the center and width of the pro-
posals, respectively. Our approach utilizes the scaling of the
center offsets with width drefandexpat the proposals to ad-
dress the scale-invariant approach. The decoder then reﬁnes
these initial locations of queries after the proposed adaptive
query selection.
Adaptive Query Selection In a conventional two-stage ap-
proach in object detection, a ﬁxed top- kselection method
based on binary class predictions p(0)is typically employed.
However, this static method may not be optimal for TAD,
where the number and duration of action instances within
videos vary signiﬁcantly. In TAD, there’s often a direct cor-
relation between the length of a video and the number of ac-
tion instances. Typically, longer videos contain more action
instances, while shorter videos have fewer. This variation
presents a challenge for the ﬁxed selection method, which
fails to adapt to video content characteristics.
Our method divides videos into sectors of a base length
Tsector. We then perform top- kselection for each sector.
This individual selection allows the detector to adapt to thevideo length, preventing the selection of only some parts
when dealing with long videos. In practice, we redistribute
any remaining timesteps to ensure that no sector at the end
part of the video is smaller than Tsector. The total number
of sectors, S, is calculated by dividing the number of the
ﬁrst layer’s feature T1byTsector, applying a ﬂoor function.
For each sector s, where s2{1,2,...,S }, we select the
top-kproposals from all levels based on the encoder’s bi-
nary class scores within that sector. We deﬁne a subset of
encoder output scores, Ps, for each sector, and then select
the top- kproposals from this subset. The adaptive query
selection is represented as follows:
Q=S[
s=1n
(ˆc(0)
t,l,ˆd(0)
t,l)|(t, l)2indices of top- KinPso
,(5)
where Q={(ˆc(0)
q,ˆd(0)
q)}Nq
q=1is the aggregated set of se-
lected queries. Here, Kis the number of queries selected
from each sector, and the total number of queries Nqequals
the sum of the top- kproposals across all sectors, denoted as
Nq=PS
s=1K.
Time-Aligned Segment Reﬁnement Existing query-based
models [ 18,26,28] employ the sigmoid function to ex-
press normalized coordinates from the range 0 to 1. More-
over, TadTR [ 18] utilizes the reﬁning step in the de-
coder of the transformer using the predicted center and
width of each layer. The layer-wise segment reﬁnement
step in the normalized coordinate expression is deﬁned as
 (  1(ˆc(n 1)
q)+ c(n)
q)and (  1(ˆd(n 1)
q)+ d(n)
q)for
center prediction and width prediction, respectively. This
reﬁnement step restricts the values within a [0,1]but en-
ables layer-by-layer updates. We reorganize the previous
segment reﬁnement step without the normalized expression.
18841
In our approach, given selected queries Qare utilized for
segment reﬁnement. Formally, segment reﬁnement of each
layer is as follows:
ˆc(n)
q=ˆc(n 1)
q + c(n)
q·ˆd(n 1)
q,n2{1,2,...,L D},(6)
ˆd(n)
q=e x p⇣
ln(ˆd(n 1)
q)+ ˆd(n)
q⌘
,n2{1,2,...,L D},(7)
where ˆc(n)
qandˆd(n)
qrepresent the predicted outputs of the
center point and width from the decoder, respectively, of
n-th layer for each query. Similar to the encoder case, we
utilize the scaling of the center offsets with width d(n)and
exp function at the proposals to address the scale-invariant
approach. The start ˆs(LD)and end ˆe(LD)timestamps for
predictions are decoded as ˆc(LD) ˆd(LD)andˆc(LD)+ˆd(LD),
respectively. Consequently, the ﬁnal predicted proposals
is deﬁned as ˆA=( ˆs(LD),ˆe(LD),ˆp(LD)), where LDis the
number of decoder layer, and ˆp(LD)is the prediction of con-
ﬁdence score of last layer of decoder action class.
3.5. Training and Inference
Training We follow the standard bipartite matching loss
[4]. The total loss Ltotalis deﬁned as follows:
Ltotal(A,ˆA)=NqX
i=1Lmatch(Ai,ˆA⇡(i)), (8)
where Lmatch is the bipartite matching loss that incorpo-
rates both the classiﬁcation probabilities and the distance
between the ground truth and the predicted segments, and ⇡
represent permutation indices obtained by bipartite match-
ing [12]. This cost function, Lmatch, is a composite of the
classiﬁcation loss, and the regression loss. For the classi-
ﬁcation loss, we use the focal loss [ 17], which effectively
addresses the class imbalance issue. For the regression
loss, our model utilizes DIoU [ 36] and log-ratio distance for
the width. DIoU evaluates the relative center distance and
GIoU [ 24] once, while the log-ratio compares the widths
relatively. Following the previous query-based approaches
[4,18,37], we utilize the auxiliary decoding loss at every
decoder layer for a more effective learning process. Fur-
thermore, we do not apply any other losses without bipartite
matching loss, such as actionness loss [ 18] or action classi-
ﬁcation enhancement loss [ 26]. More detailed descriptions
of each loss are described in the supplementary materials.
Inference TE-TAD introduces a signiﬁcant innovation by
completely removing the need for common post-processing
steps, such as NMS and temporal scaling. This is a direct
consequence of our model’s unique ability to work with an
end-to-end approach and actual timeline values of the video,
streamlining the inference process. The predictions from
the ﬁnal layer of the decoder ˆAare directly used.4. Experiments
4.1. Setup
Datasets We conduct experiments on three datasets: THU-
MOS14 [ 10], ActivityNet v1.3 [ 8], and EpicKitchens [ 7].
THUMOS14 comprises 20 action classes with 200 and 213
untrimmed videos in the validation and test sets, contain-
ing 3,007 and 3,358 action instances, respectively. Activi-
tyNet v1.3 are large-scale datasets with 200 action classes.
They consist of 10,024 videos for training and 4,926 videos
for validation, respectively. EpicKitchens, a ﬁrst-person vi-
sion dataset, includes two sub-tasks: noun and verb. It con-
tains 495 and 138 videos with 67,217 and 9,668 action in-
stances for training and test, with 300 and 97 action classes
for nouns and verbs, respectively. These datasets contain
diverse actions and scenes, providing a rigorous evaluation
setup for our method.
Evaluation Metric We follow the standard evaluation pro-
tocol for all datasets, utilizing mAP at different intersec-
tions over union (IoU) thresholds to evaluate TAD perfor-
mance. The IoU thresholds for THUMOS14 and EpicK-
itchens are set at [0.3:0.7:0.1] and [0.1:0.5:0.1] respectively,
while for ActivityNet v1.3, the results are reported at IoU
threshold [0.5, 0.75, 0.95] with the average mAP computed
at [0.5:0.95:0.05].
Implementation Details To ensure the clarity and focus of
our main manuscript, detailed descriptions of the hyperpa-
rameters and experimental environments are provided in the
supplementary materials.
Type Method NMSmAP
0.3 0.4 0.5 0.6 0.7 Avg.
Anchor
-basedBSN [ 15]⇤3 53.5 45.0 36.9 28.4 20.0 36.8
BMN [ 16]⇤3 56.0 47.4 38.8 29.7 20.5 38.5
BC-GNN [ 3]⇤3 57.1 49.1 40.4 31.2 23.1 40.2
G-TAD [ 31]⇤3 54.5 47.6 40.3 30.8 23.4 39.3
VSGN [ 34]⇤3 66.7 60.4 52.4 41.0 30.4 50.2
TCANet [ 23]⇤3 60.6 53.2 44.6 36.8 26.7 44.3
Anchor
-freeAFSD [ 14] 3 67.3 62.4 55.5 43.7 31.1 52.0
MENet [ 35]‡3 70.7 65.3 58.8 49.1 34.0 55.6
TALLFormer [ 6]†3 76.0 - 63.2 - 34.5 59.2
ActionFormer [ 32] 3 82.1 77.8 71.0 59.4 43.9 66.8
TriDet [ 27] 3 83.6 80.1 72.9 62.4 47.4 69.3
Query
-basedRTD-Net [ 28] 3 68.3 62.3 51.9 38.8 23.7 49.0
ReAct [ 26] 3 69.2 65.0 57.1 47.8 35.6 55.0
Self-DETR [ 11]   74.6 69.5 60.0 47.6 31.8 56.7
TadTR [ 18]   74.8 69.1 60.1 46.6 32.8 56.7
TE-TAD (Ours) 7 81.7 76.6 69.5 59.3 44.8 66.4
Ours w/ NMS 3 83.3 78.4 71.3 60.7 45.6 67.9
Table 1. Performance comparison with state-of-the-art methods
on THUMOS14.⇤and†denote TSN [ 30] and Swin-B [ 20] back-
bones, respectively.‡represents R(2+1)D [ 29]. Others employ
I3D [ 5] backbone. The symbol  indicates partial adoption of
NMS.
4.2. Main Results
THUMOS14 Table 1provides a comparison with the state-
of-the-art methods on THUMOS14. Our TE-TAD shows a
18842
Type Method FeaturemAP0.5 0.75 0.95 Avg.Anchor-basedBSN [15] TSN [30] 46.5 30.0 8.0 30.0BMN [16] TSN [30] 50.1 34.8 8.3 33.9BC-GNN [3] TSN [30] 50.6 34.8 9.4 34.3G-TAD [31] TSN [30] 50.4 34.6 9.0 34.1VSGN [34] TSN [30] 52.4 36.0 8.4 35.1TCANet [23] TSN [30] 52.3 36.7 6.9 35.5Anchor-freeAFSD [14] I3D [5] 52.4 35.3 6.5 34.4TALLFormer [6] Swin-B [20] 54.1 36.2 7.9 35.6ActionFormer [32] R(2+1)D [29]54.737.8 8.4 36.6TriDet [27] R(2+1)D [29]54.738.0 8.4 36.8MENet [35] R(2+1)D [29]54.7 38.4 10.5 37.7Query-basedRTD-Net [28] TSN [30] 47.2 30.7 8.6 30.8ReAct [26] TSN [30] 49.6 33.0 8.6 32.6TadTR [18] R(2+1)D [29] 53.6 37.510.636.8TE-TAD (Ours)R(2+1)D [29]54.038.210.637.0Ours w/ NMSR(2+1)D [29]54.238.110.637.1Table 2. Comparison with state-of-the-art methods on Activi-
tyNet v1.3.
signiﬁcant margin of improvement over other query-based
detectors, even without NMS. While TadTR achieves an av-
erage mAP of 56.7% with partial NMS utilization through
their proposed cross window fusion (CWF) denoted by  ,
our method without NMS achieves a superior performance
average mAP of 66.4%. This signiﬁcant improvement indi-
cates the length-invariant capability of our TE-TAD model
because THUMOS14 contains extremely diverse lengths of
videos. Furthermore, even compared to anchor-free detec-
tors, our method demonstrates competitive performance.
ActivityNet v1.3 Following the conventional approach [ 18,
27,32], the external classiﬁcation score is used to evaluate
ActivityNet v1.3. The pre-extracted classiﬁcation scores are
combined with predictions from binary detectors to obtain
class labels. Table 2presents a performance comparison of
our TE-TAD with state-of-the-art approaches. While our
TE-TAD method exhibits a slight improvement in query-
based detectors on ActivityNet v1.3 compared to the sig-
niﬁcant gains shown in Table 1, this is reﬂective of the in-
trinsic characteristics of the dataset. ActivityNet v1.3 does
not contain the diverse length of the video relative to THU-
MOS14. Despite this different condition, our approach still
demonstrates an improvement on ActivityNet v1.3, show-
ing performance improvement even though it does not align
with the primary issues our TE-TAD aims to resolve.
EpicKitchens Table 3shows the comparison with state-
of-the-art methods on EpicKitchens. Our model achieves
competitive performance without relying on NMS, indicat-
ing TE-TAD robustness in diverse and complex action de-
tection scenarios. EpicKitchen contains an extremely di-
verse length of action instances, like THUMOS14. The re-
sults indicate the robustness of TE-TAD in handling a wide
range of action lengths and complexities. The comparable
performance of TE-TAD is meaningful in query-based ap-
proaches, a relatively less explored ﬁeld than the more es-
tablished anchor-free methods.Task Method NMSmAP
0.1 0.2 0.3 0.4 0.5 Avg.
VerbBMN [ 16] 3 10.8 8.8 8.4 7.1 5.6 8.4
G-TAD [ 31] 3 12.1 11.0 9.4 8.1 6.5 9.4
ActionFormer [ 32] 3 26.6 25.4 24.2 22.3 19.1 23.5
ASL [ 25] 3 27.9 - 25.5 - 19.8 24.6
TriDet [ 27] 3 28.6 27.4 26.1 24.2 20.8 25.4
TE-TAD (Ours) 7 27.0 25.9 24.6 22.9 20.0 24.1
Ours w/ NMS 3 27.9 26.8 25.4 23.4 20.0 24.7
NounBMN [ 16] 3 10.3 8.3 6.2 4.5 3.4 6.5
G-TAD [ 31] 3 11.0 10.0 8.6 7.0 5.4 8.4
ActionFormer [ 32] 3 25.2 24.1 22.7 20.5 17.0 21.9
ASL [ 25] 3 26.0 - 23.4 - 17.7 22.6
TriDet [ 27] 3 27.4 26.3 24.6 22.2 18.3 23.8
TE-TAD (Ours) 7 26.0 24.8 23.2 20.8 18.3 22.6
Ours w/ NMS 3 26.3 25.2 23.2 21.0 18.2 22.8
Table 3. Comparison with state-of-the-art methods on EpicK-
itchens. All methods employ SlowFast [ 9] as a backbone.
Type Method NMS mAP@A VG
Anchor-freeActionFormer [ 32]7 43.2 (-23.6)
3 66.8
TriDet [ 27]7 44.9 (-24.4)
3 69.3
Query-basedReAct [ 26]7 19.8 (-35.7)
3 55.0
TadTR [ 18]7 53.1 (-3.6)
  56.7
TE-TAD (Ours)7 66.4 (-1.5)
3 67.9
Table 4. Effect of NMS on the mAP across various anchor-free and
query-based methods on THUMOS14. The value in parentheses
represents the decrease in mAP when NMS is not applied. The
symbol  indicates partial adoption of NMS.
4.3. Further Analysis
Impact of NMS In Table 4, we evaluate how NMS inﬂu-
ences the performance of various TAD methods on THU-
MOS14. NMS is particularly crucial for anchor-free de-
tectors, which employ a one-to-many assignment strategy
that leads to duplicated predictions for the same instance.
Furthermore, even though ReAct [ 26] is a query-based ap-
proach, removing NMS at the ReAct signiﬁcantly affects
the performance. This is why ReAct adopts partial self-
attention in the decoder called relational queries, which
does not address whole queries. As shown in Table 6, our
approach also drops the performance without NMS when
removing the decoder’s self-attention layer. This result in-
dicates that addressing whole queries by the decoder’s self-
attention is crucial to preserving the set-prediction mecha-
nism and full end-to-end modeling. Furthermore, our pro-
posed method exhibits a minimal performance decrement
of only -1.5 when NMS is excluded. This indicates that our
approach effectively achieves full end-to-end modeling.
Component Contribution Analysis Table 5shows the in-
cremental impact of each key component in our TE-TAD on
THUMOS14. The performance is measured without NMS.
We conduct experiments based on full end-to-end TadTR,
referred to as TadTR- 1in Sec. 3.3. Starting with the
TadTR- 1baseline, incorporating multi-scale features and
18843
BaselineMulti-Scale Two-stage [35]TE AQSmAP@A VGTadTR-34.1sw/ NMS56.7356.5357.33357.0TadTR-1w/o NMS40.2342.63343.3359.53346.133363.6333366.4Table 5. Analysis of contributions of each component on THU-
MOS14. The all-empty check is the denoted baseline.
Encoder Decoder mAP@A VG
Self-attn. Self-attn. Cross-attn. w/o NMS w/ NMS
#1 33 61.2 63.8
#2 3 3 53.0 63.4
#3 3 3 0.1 0.2
#4 3 33 66.1 67.7
Table 6. Analysis of the speciﬁc roles of self-attention and cross-
attention layers in the encoder and decoder, and their impact with
and without utilizing NMS.
Figure 6. Comparison of false negative rates by instance lengths
(XS, S, M, L, XL) on THUMOS14.
a two-stage approach shows slight improvements. Nonethe-
less, these adoptions of improved methods for query-based
detectors do not reach the performance level of the orig-
inal TadTR. However, including our time-aligned expres-
sion (TE) signiﬁcantly improves performance, demonstrat-
ing the value of our time-aligned representations. Finally,
incorporating the adaptive query selection (AQS) mecha-
nism shows the highest performance.
Role of Each Attention To clarify the role of encoder and
decoder architecture, we conduct experiments for remov-
ing the attention layers of the encoder and decoder. Table 6
shows the individual contributions of the encoder and de-
coder layers’ attention. As shown in Row #1, removing
self-attention in the encoder slightly decreases the detec-
tion performance, indicating the encoder’s self-attention af-
fects representational ability. Row #2 reveals that the de-
coder’s self-attention is the core role of the set-prediction
mechanism by showing the performance degradation with-
out using NMS. Finally, Row #3 shows that removing the
decoder’s cross-attention does not work because only loca-
tion information is provided to the decoder, which does not
capture the content information without cross-attention.
False Negative Analysis To further compare with the base-
line method, we evaluate the false negative rate on THU-
MOS14. Fig. 6shows the false negative rates across varying
action instance sizes: extra small (XS), small (S), medium
(M), large (L), and extra large (XL) based on DETAD [ 1].
These results show that even though TadTR- 1shows theFigure 7. Comparison between query selection method. Ground
truth (GT) is illustrated in the ﬁrst row. The second row shows
encoder prediction scores p(0). The third and fourth row visualizes
the generated queries at the top 50 points and 10 points for each
sector by AQS, respectively. Different colors in the fourth row
represent proposals selected by different sectors.
worse overall performance in mAP, TadTR- 1more cap-
tures the XL cases, indicating the shorter feature coverage
cannot capture the long duration of instances. These results
show that our method signiﬁcantly reduces false negatives,
particularly in XS and XL cases.
Effetiveness of Adaptive Query Selection Fig.7compares
the query selection method by denoting the center point cref
of selected queries. Selected points both w/o AQS and w/
AQS are generated by the same encoder output scores. As
illustrated in Fig. 7, the ﬁxed top- kproposal method misses
the fourth ground location as a foreground candidate. When
a foreground proposal is too distant from actual action in-
stances, it necessitates extensive reﬁnement from the de-
coder. Reﬁning the segments from missed candidates leads
to an additional workload for the decoder layers. In con-
trast, our AQS method successfully captures the part missed
by w/o AQS. By dividing the video into sectors and se-
lecting queries within these local units, AQS dynamically
adjusts the number of queries and more accurately detects
foreground candidates.
5. Conclusion
In this paper, we propose a full end-to-end temporal ac-
tion detection transformer that integrates time-aligned co-
ordinate expression, called TE-TAD, which eliminates re-
liance on hand-crafted components such as the sliding win-
dow and NMS. By aligning coordinate expression with the
actual video timeline, our model not only simpliﬁes the de-
tection process but also signiﬁcantly enhances the perfor-
mance of query-based detectors. Furthermore, our TE-TAD
has a length-invariant property by combining the proposed
time-aligned coordinate expression and adaptive query se-
lection, showing the potential of query-based detectors.
Acknowledgement This work was supported by Institute of Information
& communications Technology Planning & Evaluation (IITP) grant funded
by the Korea government (MSIT) (No. 2019-0-00079, Artiﬁcial Intelli-
gence Graduate School Program (Korea University), No. 2021-0-02068,
Artiﬁcial Intelligence Innovation Hub, and No. 2022-0-00984, Develop-
ment of Artiﬁcial Intelligence Technology for Personalized Plug-and-Play
Explanation and Veriﬁcation of Explanation).
18844
References
[1]Humam Alwassel, Fabian Caba Heilbron, Victor Escorcia,
and Bernard Ghanem. Diagnosing error in temporal action
detectors. In Proceedings of the European Conference on
Computer Vision , pages 256–272, 2018. 8
[2]Humam Alwassel, Silvio Giancola, and Bernard Ghanem.
Tsp: Temporally-sensitive pretraining of video encoders for
localization tasks. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision Workshops , pages
3173–3183, 2021. 2
[3]Yueran Bai, Yingying Wang, Yunhai Tong, Yang Yang,
Qiyue Liu, and Junhui Liu. Boundary content graph neu-
ral network for temporal action proposal generation. In Pro-
ceedings of the European Conference on Computer Vision ,
pages 121–137, 2020. 1,2,6,7
[4]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Proceedings of the
European Conference on Computer Vision , pages 213–229,
2020. 1,3,6
[5]Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 6299–6308, 2017. 2,3,6,7
[6]Feng Cheng and Gedas Bertasius. Tallformer: Temporal ac-
tion localization with long-memory transformer. In Proceed-
ings of the European Conference on Computer Vision , pages
503–521, 2022. 1,2,6,7
[7]Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide
Moltisanti, Jonathan Munro, Toby Perrett, and Will Price.
The epic-kitchens dataset: Collection, challenges and base-
lines. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 43(11):4125–4141, 2020. 2,6
[8]Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and
Juan Carlos Niebles. Activitynet: A large-scale video bench-
mark for human activity understanding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 961–970, 2015. 2,6
[9]Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
Kaiming He. Slowfast networks for video recognition. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 6202–6211, 2019. 3,7
[10] Haroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban,
Ivan Laptev, Rahul Sukthankar, and Mubarak Shah. The
thumos challenge on action recognition for videos “in the
wild”. Computer Vision and Image Understanding , 155:1–
23, 2017. 2,6
[11] Jihwan Kim, Miso Lee, and Jae-Pil Heo. Self-feedback detr
for temporal action detection. In ICCV , pages 10286–10296,
2023. 6
[12] Harold W Kuhn. The hungarian method for the assignment
problem. Naval research logistics quarterly , 2(1-2):83–97,
1955. 6,1
[13] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni,
and Lei Zhang. Dn-detr: Accelerate detr training by intro-
ducing query denoising. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition ,
pages 13619–13627, 2022. 3
[14] Chuming Lin, Chengming Xu, Donghao Luo, Yabiao Wang,
Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yan-
wei Fu. Learning salient boundary feature for anchor-
free temporal action localization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3320–3329, 2021. 1,2,6,7
[15] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and
Ming Yang. Bsn: Boundary sensitive network for temporal
action proposal generation. In Proceedings of the European
Conference on Computer Vision , pages 3–19, 2018. 1,2,6,
7
[16] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen.
Bmn: Boundary-matching network for temporal action pro-
posal generation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 3889–3898,
2019. 1,2,6,7
[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ´ar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE International Conference on Computer
Vision , pages 2980–2988, 2017. 6,1
[18] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei
Zhang, Song Bai, and Xiang Bai. End-to-end temporal ac-
tion detection with transformer. IEEE Transactions on Image
Processing , 2022. 1,3,4,5,6,7
[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 10012–10022, 2021. 2
[20] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,
Stephen Lin, and Han Hu. Video swin transformer. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 3202–3211, 2022. 2,6,7
[21] Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo
Luo, and Tao Mei. Gaussian temporal awareness networks
for action localization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 344–353, 2019. 1,2
[22] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations , 2019. 1
[23] Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang,
Wei Wu, Xiang Wang, Yu Qiao, Junjie Yan, Changxin Gao,
and Nong Sang. Temporal context aggregation network
for temporal action proposal reﬁnement. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 485–494, 2021. 1,2,6,7
[24] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir
Sadeghian, Ian Reid, and Silvio Savarese. Generalized in-
tersection over union: A metric and a loss for bounding box
regression. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 658–666,
2019. 6,1
[25] Jiayi Shao, Xiaohan Wang, Ruijie Quan, Junjun Zheng, Jiang
Yang, and Yi Yang. Action sensitivity learning for temporal
18845
action localization. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , 2023. 2,7
[26] Dingfeng Shi, Yujie Zhong, Qiong Cao, Jing Zhang, Lin Ma,
Jia Li, and Dacheng Tao. React: Temporal action detection
with relational queries. In Proceedings of the European Con-
ference on Computer Vision , pages 105–121, 2022. 1,3,5,
6,7
[27] Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, and
Dacheng Tao. Tridet: Temporal action detection with relative
boundary modeling. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
18857–18866, 2023. 1,2,4,6,7
[28] Jing Tan, Jiaqi Tang, Limin Wang, and Gangshan Wu. Re-
laxed transformer decoders for direct action proposal gener-
ation. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 13526–13535, 2021. 1,
3,5,6,7
[29] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotempo-
ral convolutions for action recognition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6450–6459, 2018. 2,6,7
[30] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment
networks: Towards good practices for deep action recogni-
tion. In Proceedings of the European Conference on Com-
puter Vision , pages 20–36, 2016. 6,7
[31] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and
Bernard Ghanem. G-tad: Sub-graph localization for tempo-
ral action detection. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
10156–10165, 2020. 1,2,6,7
[32] Chenlin Zhang, Jianxin Wu, and Yin Li. Actionformer: Lo-
calizing moments of actions with transformers. In Proceed-
ings of the European Conference on Computer Vision , pages
492–510, 2022. 1,2,4,6,7
[33] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun
Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr
with improved denoising anchor boxes for end-to-end object
detection. In Proceedings of the European Conference on
Computer Vision , 2022. 4,1
[34] Chen Zhao, Ali K Thabet, and Bernard Ghanem. Video self-
stitching graph network for temporal action localization. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 13658–13667, 2021. 1,2,6,7
[35] Zixuan Zhao, Dongqi Wang, and Xu Zhao. Movement
enhancement toward multi-scale video feature representa-
tion for temporal action detection. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 13555–13564, 2023. 2,6,7
[36] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang
Ye, and Dongwei Ren. Distance-iou loss: Faster and bet-
ter learning for bounding box regression. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , pages 12993–
13000, 2020. 6,1
[37] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable detr: Deformable transformersfor end-to-end object detection. In International Conference
on Learning Representations , 2020. 4,6,1
18846
