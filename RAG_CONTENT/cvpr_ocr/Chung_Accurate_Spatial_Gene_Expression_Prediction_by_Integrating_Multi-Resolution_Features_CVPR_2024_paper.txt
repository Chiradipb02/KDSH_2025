Accurate Spatial Gene Expression Prediction by Integrating Multi-Resolution
Features
Youngmin Chung, Ji Hun Ha, Kyeong Chan Im, Joo Sang Lee*
Sungkyunkwan University, South Korea
ymblue@g.skku.edu, joosang.lee@skku.edu
Abstract
Recent advancements in Spatial Transcriptomics (ST)
technology have facilitated detailed gene expression anal-
ysis within tissue contexts. However, the high costs and
methodological limitations of ST necessitate a more ro-
bust predictive model. In response, this paper introduces
TRIPLEX, a novel deep learning framework designed to
predict spatial gene expression from Whole Slide Images
(WSIs). TRIPLEX uniquely harnesses multi-resolution fea-
tures, capturing cellular morphology at individual spots,
the local context around these spots, and the global tissue
organization. By integrating these features through an ef-
fective fusion strategy, TRIPLEX achieves accurate gene ex-
pression prediction. Our comprehensive benchmark study,
conducted on three public ST datasets and supplemented
with Visium data from 10X Genomics, demonstrates that
TRIPLEX outperforms current state-of-the-art models in
Mean Squared Error (MSE), Mean Absolute Error (MAE),
and Pearson Correlation Coefficient (PCC). The model’s
predictions align closely with ground truth gene expression
profiles and tumor annotations, underscoring TRIPLEX’s
potential in advancing cancer diagnosis and treatment.
1. Introduction
The emergence of large-scale Spatial Transcriptomics
(ST) technology has facilitated the quantification of mRNA
expression across a multitude of genes within the spatial
context of tissue samples [22]. ST technology segments
centimeter-scale Whole Slide Images (WSIs) into hundreds
of thousands of small spots, each providing its gene ex-
pression profile. Considering the substantial cost asso-
ciated with ST sequencing technology, coupled with the
widespread availability of WSIs, a pressing question is how
to best predict spatial gene expression based on WSIs using
rapidly evolving computer vision techniques.
A number of studies have endeavored to address this
*Corresponding author.challenge [7, 18, 24–26]. Approaches vary, with some pre-
dicting gene expression strictly from the tissue image con-
fined within the spot’s boundaries [7], while others also
take into account spatial dependencies between spot im-
ages [18, 26], or consider similarities to reference spots
[24, 25]. However, we have noted several limitations inher-
ent to these existing methodologies. Firstly, current meth-
ods primarily focus on spot images, neglecting the wealth of
biological information available in the wider image context.
By integrating both the specific spot and its surrounding en-
vironment, along with the holistic view of the entire histol-
ogy image, we can access richer information, encompass-
ing varied biological contexts. Secondly, models that con-
sider interactions between spots [18, 26] face a limitation
in processing the embedding of all patches in a WSI simul-
taneously. This approach, common in handling hundreds
to thousands of patches within a WSI, limits the scalability
of the patch embedding model due to resource constraints.
Such limitations significantly impede the extraction of fine-
grained, rich representations from each spot, thereby affect-
ing the model’s ability to perform detailed analysis of WSIs.
Thirdly, model performance is frequently overestimated be-
cause of inadequate validation, such as using the limited
size of dataset [7] sometimes without cross-validation [24]
and training/testing with replicates from the same patient
[18, 25, 26]. The limited size of ST datasets means that
exclusive reliance on a single dataset for model evaluation
can hinder an accurate assessment of the model’s capabili-
ties, thereby emphasizing the necessity for cross-validation.
The issue is compounded when replicate data from the same
patient, often featuring nearly identical image-gene expres-
sion pairs, are used in both training and testing phases. This
can lead to an inflated perception of a model’s effective-
ness, as it may not accurately reflect the model’s ability to
generalize to new, unseen data. Lastly, the use of disparate
datasets, diverse normalization methods, and varied evalua-
tion techniques in existing research studies compounds the
challenge of conducting fair comparisons of the models.
Addressing these limitations, we present TRIPLEX, an
innovative deep learning framework designed to leverage
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11591
multi-resolution features from histology images for robustly
predicting spatial gene expression levels. TRIPLEX ex-
tracts three distinct types of features corresponding to dif-
ferent resolutions: the target spot image (representing the
specific spot, whose gene expression to be predicted), the
neighbor view (encompassing a wider area around the spot),
and the global view (comprising the aggregate of all spot
images). These features capture varying levels of biological
information—ranging from the detailed cell morphology in
the target spot image, to the surrounding tissue phenotype,
and the overall tissue microenvironment in the WSI. Each
is integral to understanding the spatial gene expression lev-
els of the given spot. TRIPLEX employs separate encoders
to extract these features from WSIs, each focusing on its
assigned resolution to efficiently capture relevant details.
For neighbour or global view with larger resolution, pre-
extracted features are used to reduce the burden of compu-
tational cost, while for target spot images, encoders are fully
updated to extract fine-grained information. These features
are then integrated via a fusion layer for effective gene ex-
pression prediction. This approach allows TRIPLEX to
utilize resolution-specific information, thereby enhancing
prediction accuracy while avoiding significant increases in
computational costs.
Our study sets a new benchmark in spatial gene expres-
sion prediction, comparing our model, TRIPLEX, against
five prior studies [7, 18, 24–26] under uniform experimen-
tal conditions. We conduct internal evaluations using three
public Spatial Transcriptomics (ST) datasets [1, 7, 10] and
external validations using higher-resolution Visium data
from 10X Genomics. Our validation procedure strictly
avoids mixing patient sample replicates between training
and testing datasets, a significant departure from previ-
ous methods [18, 25, 26], and employs rigorous cross-
validation. Our results indicate that TRIPLEX surpasses
existing models in terms of Mean Squared Error (MSE),
Mean Absolute Error (MAE), and Pearson Correlation Co-
efficient (PCC) in both internal and external evaluations.
Furthermore, we provide visualizations of the expression
distributions for a specific gene commonly associated with
cancer. These visualizations reveal that our model’s predic-
tions align more closely with actual gene expression data
and tumor annotations, demonstrating its enhanced predic-
tive accuracy.
Our key contributions can be summarized as follows:
• We introduce an innovative approach to predict spatial
gene expression levels from WSIs by integrating multi-
ple biological contexts.
• Our proposed framework seamlessly integrates multi-
resolution features. This integration is facilitated by a fea-
ture extraction strategy, the use of various types of trans-
formers, and a fusion loss technique, all while keeping
the additional computational costs to a minimum.• Through comprehensive experiments on three public ST
datasets and additional external evaluations using three
Visium data, our study establishes a new benchmark in
the field of spatial gene expression prediction. The results
consistently show that our proposed method outperforms
all existing models included in our comparative analysis.
2. Related Work
In this section, we delve into studies pertinent to our re-
search. For clarity, the term ’spot’ will be used to denote a
predefined unit region within a WSI where gene expression
is quantified. Moreover, we will use ’target spot’ to specif-
ically refer to the spot within a WSI for which we seek to
predict gene expression.
Spatial gene expression prediction from WSIs via deep
learning We review the pioneering works in this field,
which aim to predict spatial gene expression from WSIs.
ST-Net [7] utilizes a standard transfer learning strategy,
training a Densenet121 model [8]—pretrained on Ima-
geNet—using histology images as input and gene expres-
sion as labels. Following this, HisToGene [18] leverages
Vision Transformers (ViT) [6] to account for correlations
among patches in a WSI, thereby predicting gene expres-
sion from global-context aware features. Further develop-
ing this concept, Hist2ST [26] enhances the approach by
emphasizing patch embedding using ConvMixer [23] and
aggregating neighborhood information through graph con-
volution network [14]. While these previous works [18, 26]
share similarities with our methodology, they predomi-
nantly process patch and global embedding sequentially,
often overlooking the neighboring information around the
target spot. In contrast, our method sets itself apart by con-
currently extracting critical features at three distinct resolu-
tions, including the neighbor view, and integrating them for
gene expression prediction. In a different vein, EGN [25]
adopts exemplar learning for predicting gene expression
from histology images. This method dynamically selects
the most analogous exemplars from a target spot within a
WSI to enhance prediction accuracy. Additionally, BLEEP
[24] introduces a bi-modal embedding framework similar
to CLIP [19] to co-embed spot images and gene expression.
After training, this model imputes the gene expression of a
query spot using the retrieved gene expression set from a
reference dataset.
Deep learning for WSIs Due to their gigapixel resolution,
WSIs present a significant challenge for conventional deep
learning frameworks in computer vision. To tackle this,
Multiple Instance Learning (MIL) has been employed, en-
abling the handling of high-resolution data with sparse lo-
cal annotations. In the context of WSIs, MIL approaches
typically predict bag labels, such as distinguishing slides
from cancer patients versus healthy individuals, by aggre-
gating information from numerous small patches within the
11592
WSIs [2, 12, 15, 20]. Recently, attention-based networks
have been employed in MIL to aggregate all patches in
WSIs, achieving state-of-the-art performance [15, 16, 20].
Moreover, Chen et al. [3] introduced a Hierarchical Image
Pyramid Transformer (HIPT) to adapt Vision Transformers
(ViT) for WSIs. They effectively captured the hierarchi-
cal structures of WSIs by sequentially training ViTs across
images of various resolutions, employing a self-supervised
learning approach. This method has shown superior re-
sults in cancer subtyping and survival prediction, surpass-
ing previous models. Drawing inspiration from this, our
proposed method is specifically designed to simultaneously
handle information from these multi-resolution features for
predicting spatial gene expression.
3. Method
3.1. Preliminary
In this section, we present our problem formulation and
detail our proposed method, concurrently assessing it in
contrast to previous methodologies. Our task is a multi-
output regression problem, where we input a set of spot im-
ages from a WSI, denoted as X∈Rn×H×W×3, and aim to
predict the gene expression levels of individual spots, repre-
sented as Y∈Rn×m. Here, ndenotes the number of spot
images in a WSI, mrepresents the number of genes whose
expression levels to be predicted, and HandWsignify the
height and width of each spot image, respectively.
ST-Net [7] approaches the prediction task by estimating
ˆYi∈Rmbased solely on the information from a single spot
image Xi∈RH×W×3. This is represented as:
ˆYi=f(Xi)∈Rm(1)
where iindexes a target spot within a WSI. ST-Net is formu-
lated under general supervised learning principles, where
each input corresponds to a unique label, and each input is
treated independently.
On the other side, methods employing ViT [18, 26] pre-
dict the gene expression of all spot images concurrently, ex-
pressed as:
ˆY=f(X1, X2, ..., X n)∈Rn×m(2)
In this case, the problem is defined within the context of
supervised learning, but with a key difference: the inputs
are interdependent, affecting the prediction outcome collec-
tively.
EGN [25] approaches the problem from a different per-
spective and predicts ˆYiusing Xitogether with its global
viewGiand its exemplar set {gj, yj}k
j=1∈Ki. This can be
formulated as:
ˆYi=f(Xi, Gi, Ki)∈Rm(3)where Girepresents the features extracted from the target
spot image Xiusing a pretrained model. The set Kicom-
prises the k-nearest global views to Giand their associated
gene expression levels.
In BLEEP [24], a bi-modal pretraining phase is lever-
aged, employing contrastive loss [19] to generate embed-
dings for both images and gene expression levels. Dur-
ing the inference phase, a given query image Xiis pro-
cessed through an image encoder, Encimg, to produce its
d-dimensional embedding vector vi:
vi=Encimg(Xi)∈Rd(4)
Simultaneously, an expression encoder, Encexp, is applied
to the gene expression levels of the reference dataset, yield-
ing an d-dimensional embedding vectors erefrepresented
as:
eref=Encexp(Yref)∈Rl×d(5)
Here, lsignifies the number of gene expression levels in the
reference dataset. The process continues by identifying the
top-k closest embeddings to vi, denoted as etop∈Rk×d.
The corresponding gene expression values for these top-k
embeddings, Ytop∈Rk×m, are then retrieved. The gene
expression level for the query spot is estimated by comput-
ing the average of Ytop:
ˆYi=Average (Ytop)∈Rm(6)
Our proposed method diverges from the previously men-
tioned approaches by employing a unique combination of
three input types to predict ˆYi. These are the target spot im-
ageXTa
i, the local neighbor views XNe
i, and a collection
of all the global views G:g1, g2, ..., gn .
zTa
i=EncTa(XTa
i)∈RnTa×d(7)
zNe
i=EncNe(XNe
i)∈RnNe×d(8)
zGl=EncGl(G)∈Rn×d(9)
ˆY i=f(zTa
i, zNe
i, zGl
i)∈Rm(10)
Here, EncTa,EncNe, and EncGlrepresent models that
independently embed each type of input. The dimension of
each embedded token is denoted by d. The local neighbor
views XNe
iconsist of the nNeadjacent patches around the
target spot image XTa
i, and nTasignifies the number of
tokens derived from XTa
i. In all cases, frefers to a neural
network function outputting gene expression levels.
3.2. TRIPLEX
The overall workflow of our method is illustrated in Fig-
ure 1. Initially, we process the global view, derived from
all spot images of a WSI, through a global encoder to pro-
duce global tokens. Although these global tokens capture
11593
Figure 1. Schematic representation of the TRIPLEX. The global encoder processes the global view, while separate encoders handle the
target spot image and neighbor view. A fusion layer, incorporated with fusion loss, facilitates the effective integration of these tokens to
predict gene expression levels.
the macroscopic spatial distribution and inter-spot correla-
tions, they might not adequately represent the detailed in-
formation specific to each target spot. To address this, we
independently encode the target spot image and its neigh-
boring views, generating tokens that encapsulate finer de-
tails. These are then integrated with the global tokens via
fusion layers, enriching the global representation with spe-
cific target-related information. Additionally, considering
the diverse contextual information provided by different in-
put sources, we have implemented a fusion loss mechanism,
inspired by knowledge distillation, to enhance the efficacy
of the fusion process. Detailed descriptions of the individ-
ual components involved in our proposed model will be pro-
vided in the following sections.
3.3. Embedding Global Information
Processing all 224x224-sized images of spots starting
from patch embedding is computationally intensive since
the number of spots typically ranges from hundreds to thou-
sands. To address this, we employ a feature extraction strat-
egy commonly used in the MIL approach for WSIs. Specifi-
cally, we utilize a ResNet18 model pretrained on large-scale
histology images for feature extraction [5]. The features
thus obtained serve as input to the global encoder, which
generates global tokens. This encoder comprises a series
of transformer blocks and a Position Encoding Generator
(PEG) [4], adept at encoding positional information for avariable number of spots. Given that PEG was originally
designed for natural images with regular, square shapes, we
modify it for our specific use-case where the image shape
is irregular. Our adaptation, termed the Atypical Position
Encoding Generator (APEG), imbues tokens with absolute
positional information pertinent to all spots in a WSI. This
modification is crucial for effectively capturing the spatial
distribution within the WSI.
Atypical Position Encoding Generator (APEG) In our
APEG framework, after the initial transformation of tokens
through one transformer block, we employ a technique to
re-establish their relative positional context. This involves
reshaping the global tokens zGl∈Rn×dinto a spatial for-
matˆzGl∈Rh×w×d. Here, handwrepresent the maximum
values of the x- and y-coordinates, respectively. During this
process, any voids in the spatial arrangement are temporar-
ily filled with zeros. Subsequently, we apply convolutional
layers to these reshaped tokens. This step includes refilling
the areas that were previously vacant with zeros. After this
convolutional processing, we revert the tokens back to their
original format. This method enables us to effectively in-
corporate the relative spatial information of the tokens, en-
hancing the positional encoding within the WSI framework.
3.4. Embedding Target/Neighbor Information
We independently encode the image of the target spot
and its neighboring view, generating tokens for both target
11594
and neighbor that are rich in contextual information.
Embedding target information To encode the target spot
image, we utilize a ResNet18 architecture, excluding the
global average pooling and the fully-connected layer. This
encoder processes each 224x224 target spot image by em-
bedding it into 49 distinct features, with each feature hav-
ing a dimension of 512. Notably, while this instance of
ResNet18 is initialized with the same weights as before, it
undergoes unique updates during the training process, en-
suring tailored feature extraction for each target spot image.
Embedding neighbor information For the neighbor view,
we use the surrounding 1120x1120 image of each target
spot. This choice is based on images directly adjacent to
the target spot, rather than a group of neighboring spot im-
ages. This approach addresses the issue of non-uniform
alignment and spacing between spots in ST data (refer to
supplementary Figure 1 for details). In cases where a target
spot is at the edge of the slide, zero padding is applied to
maintain the required image size. This 1120x1120 neighbor
view is then embedded into twenty-five 512-dimensional
feature vectors using the ResNet18. These vectors then
serve as input for the neighbor encoder. The embedding
process within the neighbor encoder involves a sequence of
self-attention blocks, each integrated with relative position
encoding [21]. Although the ResNet18 weights used for
feature extraction remain fixed, the weights of the neighbor
encoder are dynamically updated during training. Further
details about the neighbor encoder are provided in the sup-
plementary material.
3.5. Integrating global, neighbor, and target infor-
mation
To achieve an effective exchange of information and en-
hanced contextual understanding between different token
types, we implement cross-attention layers. In this setup,
the global token acts as the query (Q), with the target and
neighboring tokens within the WSI forming the key (K) and
value (V) pairs. These Q, K, and V components are utilized
in a dot-product attention mechanism, which is crucial for
assessing the relative importance of each target and neigh-
bor token in comparison to the global token. This process
is instrumental in developing a comprehensive understand-
ing of the local and global contexts at each spot, thereby
augmenting the model’s performance in complex gene ex-
pression prediction tasks. The integration of these insights
is realized by summing the resultant tokens from the cross-
attention layers:
zGT
i=CrossAttn (zGl
i, zTa
i)∈Rd(11)
zGN
i=CrossAttn (zGl
i, zNe
i)∈Rd(12)
zGTN
i =Sum(zGT
i, zGN
i)∈Rd(13)In these equations, zGT
iandzGN
i represent the cross-
attention results for the target and neighbor tokens, respec-
tively, relative to the global token. zGTN
i is the summation
of these two tokens, which is used to estimate the gene ex-
pression levels. During the process, information exchange
between the neighbor token and the target token occurs only
through the global tokens. In this manner, we can efficiently
integrate the three pieces of information at a minimal addi-
tional cost.
Fusion Loss and Objective Function To optimize the
integration of information from multiple tokens, we have
introduced a fusion loss mechanism. This approach capi-
talizes on the rich, gene expression-relevant information in-
herent in the fusion token, which synthesizes data from the
target, neighbor, and global tokens. By transferring knowl-
edge from this fusion token to other individual tokens, we
significantly enhance the model’s predictive accuracy for
gene expression levels. In practice, fully-connected layers,
tasked with predicting gene expression levels, are attached
to each of the target, neighbor, and global tokens, with av-
erage pooling applied beforehand to the target and neighbor
tokens. The optimization process involves two key compo-
nents: 1) minimizing Mean Squared Error (MSE) loss be-
tween individual predictors’ outputs and ground-truth val-
ues, 2) reducing the MSE loss between each predictor’s out-
put and the ’soft target,’ which are the predictions derived
from the fusion token.
The loss for a given jthtoken (target, neighbor, or
global) is computed as follows:
Lj= (1−α)1
mmX
k=1qj
k−yk2
2+α1
mmX
k=1qj
k−qF
k2
2,
(14)
where qj
kis the prediction for the kthgene by the jthtoken,
qFkis the fusion token’s prediction for the kthgene, and α
is a hyperparameter balancing the two aspects of the loss.
For the fusion token, we calculate the MSE loss in relation
to the actual labels:
LF=1
mmX
k=1qF
k−yk2
2(15)
Ultimately, we optimize the following object function:
L=3X
jLj+LF. (16)
4. Experiments
In this section, we outline the specifics of the ST data
employed in our model’s training, detail our experimental
setup and evaluation metrics, and provide implementation
details. For more details on experiment settings, please refer
to Section 2 of the supplementary material.
11595
BC1 BC2 SCC
Source Model MSE PCC(M) PCC(H) MSE PCC(M) PCC(H) MSE PCC(M) PCC(H)
ST-Net [7] 0.260±0.04 0 .194±0.11 0 .345±0.16 0.209±0.02 0 .116±0.06 0 .223±0.10 0.294±0.07 0 .274±0.08 0 .382±0.08
EGN [25] 0.241±0.06 0 .197±0.11 0 .328±0.17 0.192±0.02 0 .111±0.05 0 .203±0.09 0.281±0.08 0 .281±0.06 0 .388±0.06
Local BLEEP [24] 0.277±0.05 0 .151±0.11 0 .277±0.16 0.235±0.02 0 .095±0.05 0 .193±0.10 0.297±0.05 0 .269±0.07 0 .396±0.08
TEM 0.252±0.04 0 .208±0.11 0 .365±0.15 0.190±0.02 0 .119±0.06 0 .227±0.10 0.290±0.06 0 .296±0.07 0 .402±0.08
NEM 0.278±0.08 0 .255±0.13 0 .424±0.18 0.193±0.03 0 .152±0.05 0 .277±0.09 0.373±0.14 0 .308±0.05 0 .444±0.06
Global HistoGene [18] 0.314±0.09 0 .168±0.12 0 .302±0.19 0.194±0.05 0 .100±0.05 0 .219±0.12 0.270±0.09 0 .133±0.06 0 .261±0.13
GEM 0.253±0.06 0 .295±0.14 0 .491±0.17 0.221±0.03 0 .193±0.06 0 .341±0.08 0.317±0.16 0 .276±0.09 0 .392±0.08
Local+Global Hist2ST [26] 0.285±0.08 0 .118±0.10 0 .248±0.17 0.181±0.02 0 .044±0.02 0 .099±0.03 1.291±0.65 0 .004±0.01 0 .053±0.01
Multiple TRIPLEX 0.228±0.07 0.314±0.14 0.497±0.17 0.202±0.02 0.206±0.07 0.352±0.10 0.268±0.09 0.374±0.07 0.490±0.07
Table 1. Cross validation result on each ST dataset. PCC(M) and PCC(H) denote the mean PCC for all genes and the mean PCC for highly
predictive genes, respectively. The mean and standard deviation of cross-validation results are displayed. MAEs are excluded due to space
limitations and can be found in the supplementary.
ST dataset Spatial Transcriptomics (ST) data is character-
ized by its compilation of numerous spot images within a
single slide, each accompanied by corresponding gene ex-
pression values. A typical ST dataset contains several hun-
dred spatially resolved spots, with each spot representing
the expression values of around 20,000 genes. Visium, the
next iteration of ST data, expands this scope to include thou-
sands of spots, each still characterized by the expressions of
a similar number of genes. For our internal validation, we
utilize two breast cancer ST datasets [1, 7] and one skin can-
cer ST dataset [10]. External validation is conducted using
Visium data from three independent breast cancer patients.
We refer to the breast cancer ST dataset from [1] as the BC1
dataset, the one from [7] as the BC2 dataset, and the skin
cancer dataset from [10] as the SCC dataset.
Experiment Setup and Evaluation Metrics To mitigate
potential overfitting due to the limited size of our datasets,
we employ cross-validation for model performance evalua-
tion across the three ST datasets. Consistent with our earlier
mention, we ensure that samples from the same patients are
exclusively allocated to either the training or the test dataset,
avoiding any overlap. Specifically, we adopt a leave-one-
patient-out cross-validation approach for the BC1 dataset
(n sample=36, n patient=8) and the SCC dataset (n sam-
ple=12, n patient=4), using samples from a single patient
for sequential validation. For the BC2 dataset, which has
a larger sample size (n sample=68, n patient=23), we con-
duct 8-fold cross-validation, with careful consideration to
keep samples from the same patient within the same data
partition. To extend our model’s evaluation to independent
datasets, we use three breast cancer Visium datasets from
10x Genomics, training on the BC1 dataset and testing on
each Visium dataset. Our evaluation metrics include the
Pearson correlation coefficient (PCC), mean squared error
(MSE), and mean absolute error (MAE). PCC is computed
for each gene across all spots in a sample, and we report
both the mean PCC for all genes (PCC(M)) and the mean
PCC for highly predictive genes (PCC(H)). The highly pre-
dictive genes are identified through a cross-validation rank-
ing process, where the top 50 genes are determined basedon their average rank across all folds.
Implementation Details For preprocessing, we crop each
spot image to 224x224 pixels using the center coordinates
of each spot. Neighbor views are obtained by capturing a
1120x1120 image centered on the spot, which is then subdi-
vided into 25 equal-sized sub-images. We select 250 genes
for each dataset following the criteria in [7]. The gene ex-
pression values are normalized by dividing by the sum of
expressions in each spot, followed by a log transformation.
To mitigate experimental noise, we adopt the smoothing ap-
proach from [7], averaging the gene expression values of
each spot with those of its adjacent neighbors. Our model
is optimized using the Adam optimizer [13] with an initial
learning rate of 0.0001. The learning rate is adjusted dy-
namically using a Step LR scheduler, with a step size of 50
and a decay rate of 0.9. During training, we use a batch size
of 128. In testing, the model’s performance is evaluated on
all spots in each WSI per batch, and we report the mean of
all validation performances.
4.1. Cross-validation Performance of TRIPLEX
We conduct cross-validation using the three ST datasets
to assess TRIPLEX’s performance in comparison to base-
line models. The total counts of spot images in the BC1,
BC2, and SCC datasets are 13,620, 68,050, and 23,205, re-
spectively.
Baselines Our model’s performance was benchmarked
against existing models, including 1) local-based models
(ST-Net[7], EGN[25], BLEEP[24]) and 2) global-based
models (HisToGene[18], Hist2ST[26]). For a consistent
evaluation, the same ResNet18 used in TRIPLEX was ap-
plied as the feature extractor in EGN and the image en-
coder in BLEEP. We also compared TRIPLEX with simpler
models focusing on single information types: Target Encod-
ing Model (TEM), Neighbor Encoding Model (NEM), and
Global Encoding Model (GEM). Implementation details for
all baseline models are available in Section 2 of the supple-
mentary material.
Result Comparison As shown in Table 1, TRIPLEX out-
performs all previous models and demonstrates superior
11596
10X Visium-1 10X Visium-2 10X Visium-3
Source Model MSE MAE PCC(M) PCC(H) MSE MAE PCC(M) PCC(H) MSE MAE PCC(M) PCC(H)
ST-Net [7] 0.423 0.505 -0.026 -0.000 0.395 0.492 0.091 0.193 0.424 0.508 -0.032 0.008
EGN [25] 0.421 0.512 0.003 0.024 0.328 0.443 0.102 0.157 0.303 0.425 0.106 0.220
Local BLEEP [24] 0.367 0.470 0.106 0.221 0.289 0.406 0.104 0.260 0.298 0.415 0.114 0.229
TEM 0.339 0.453 0.024 0.093 0.278 0.402 0.106 0.218 0.290 0.412 0.078 0.193
NEM 0.444 0.515 0.089 0.259 0.391 0.482 0.105 0.290 0.393 0.483 0.036 0.175
Global GEM 0.392 0.494 0.132 0.269 0.397 0.482 0.056 0.166 0.394 0.488 0.082 0.191
Multiple TRIPLEX 0.351 0.464 0.136 0.241 0.282 0.407 0.155 0.356 0.285 0.410 0.118 0.282
Table 2. Generalization performance comparison between other models and ours by PCC(M), PCC(H), MSE, and MAE.
performance over the individual modules within TRIPLEX
across most evaluation metrics. Notably, GEM demon-
strates substantial effectiveness, underscoring the crucial
role of global interactions in accurately predicting gene ex-
pression. However, TRIPLEX, which integrates the target
and neighbor view information with the global view, yields
the most notable improvement. Compared to EGN, one of
the best-performing existing models, TRIPLEX achieves
substantial increases in PCC(M) and PCC(H) across all
datasets. Specifically, in the BC1 dataset, there is an im-
provement of 0.117 in PCC(M) and 0.169 in PCC(H); in the
BC2 dataset, an increase of 0.095 in PCC(M) and 0.149 in
PCC(H); and in the SCC dataset, a rise of 0.093 in PCC(M)
and 0.102 in PCC(H). Despite integrating three types of in-
formation, TRIPLEX maintains a parameter count compa-
rable to other top-performing models (see supplementary
table 1). The variances between our results and those re-
ported in the original works of the baseline models can be
attributed to differences in 1) cross-validation strategies, 2)
normalization methods, and 3) metric calculations. For an
in-depth explanation of these differences, refer to Section 3
of the supplementary material.
4.2. Generalization performance of TRIPLEX
For external validation, we preprocess Visium data sim-
ilarly to the ST data and evaluate model performance on
three individual Visium samples.
Result comparison In this set of experiments, TRIPLEX
is benchmarked against the three best-performing models
identified in prior tests. According to Table 2, TRIPLEX
shows robust performance on unseen Visium data, which
is formatted differently from the training data. It outper-
forms existing models (ST-Net, EGN, BLEEP) across MSE,
MAE, PCC (M), and PCC (H). Notably, TRIPLEX consis-
tently achieves impressive PCC (H) scores (average 0.293),
indicating its potential applicability in clinical settings.
4.3. Visualization of Cancer Marker Genes
Among the highly predictive genes identified by our
model are known breast cancer markers such as CLDN4
and GNAS [11, 17], which could aid pathological diagno-
sis. In the cross-validation for the GNAS gene, TRIPLEXsignificantly outperforms all existing models in both BC1
and BC2 datasets. For instance, in the BC1 dataset, the
PCCs are 0.359 (HisToGene), 0.286 (Hist2ST), 0.411 (ST-
Net), 0.374 (EGN), 0.338 (BLEEP), and 0.583 (TRIPLEX).
Similarly, in the BC2 dataset, the PCCs are 0.282 (His-
ToGene), 0.138 (Hist2ST), 0.371 (ST-Net), 0.341 (EGN),
0.0669 (BLEEP), and 0.554 (TRIPLEX). We further pro-
vide visualizations of the predicted values for GNAS along-
side its ground truth values for each dataset. As depicted
in Figure 2, we notice that TRIPLEX not only exhibits a
higher PCC with the actual ground truth values but also
demonstrates a greater visual congruence with the actual
gene expression patterns. Consequently, it appears to be
more effective in assisting pathologists in clinical diagnos-
tics. More visualizations are available in the supplementary
material.
4.4. Ablation study
In this section, we demonstrate the contributions of each
method to gene expression prediction through an ablation
study of our proposed model. Specifically, we aim to ob-
serve the contributions of the three modules in our model
and investigate the extent to which the position encoding
generator and our proposed fusion approach contribute to
gene expression prediction. Here we only present experi-
mental results on the SCC dataset, but we have observed
similar outcomes on other datasets as well. Additional ex-
periment results can be found in supplementary material.
Individual Modules We assess how the fusion of informa-
tion from TEM, NEM, and GEM enhances gene expression
prediction accuracy. According to Table 3, it is clear that
incorporating features from all three modules achieves
the best performance. Notably, the exclusion of the NEM
module results in a substantial decrease in PCC(M), high-
lighting the critical influence of neighboring interactions
in gene expression prediction in TRIPLEX. While the
absence of the GEM module does not significantly impact
performance metrics such as MSE and MAE in this specific
dataset, ablation studies conducted on alternative datasets
reveal the significant contribution of global interactions to
the accuracy of gene expression level predictions.
11597
Figure 2. The visualization includes tumor region annotations by
pathologists, ground truth for GNAS expression levels, and pre-
dicted GNAS expression levels from HisToGene, Hist2ST, ST-
Net, EGN, BLEEP, and TRIPLEX, in samples from datasets BC1
and BC2. The PCC between the ground truth and predicted values
is displayed for each model.
MSE MAE PCC(M) PCC(T)
w/o TEM 0.289 0.419 0.352 0.471
w/o NEM 0.271 0.408 0.330 0.439
w/o GEM 0.263 0.402 0.358 0.481
TRIPLEX 0.268 0.404 0.374 0.490
Table 3. Ablation study for the individual modules
Position Encoding Generator (PEG) We further compare
our APEG to different methods of positional encoding, in-
cluding (1) without PEG and (2) with PEG [4], as shown
in Table 4. In the conventional PEG implementation, we
add zero padding to the global token to make the number
of tokens a perfect square, followed by the standard squar-
ing procedure. The results indicate that APEG not only
outperforms the model without any PEG but also surpasses
the conventional PEG. This implies that APEG more effec-
tively encodes spatial distribution within the global tokens,
enhancing the overall model performance.
Fusion Method and Fusion Loss We also investigate the
impact of different fusion methods for multi-resolution fea-
tures, comparing our model’s fusion layer with traditional
feature fusion techniques like summation, concatenation,
and attentional pooling [9]. Additionally, we assess theMSE MAE PCC(M) PCC(T)
w/o PEG 0.280 0.413 0.360 0.480
PEG [4] 0.276 0.411 0.364 0.479
APEG 0.268 0.404 0.374 0.490
Table 4. Ablation study PEG
contribution of fusion loss to the model’s performance. As
shown in Table 5, integrating multi-resolution features us-
ing our fusion layer yields superior results compared to
these conventional techniques. Moreover, the marked im-
provement in performance when incorporating fusion loss
indicates its effectiveness in integrating the three types of
features. This result highlights the significance of our fu-
sion approach in achieving high accuracy in gene expres-
sion prediction.
Fusion method MSE MAE PCC(M) PCC(T)
Summation 0.297 0.425 0.341 0.464
Concatenation 0.293 0.422 0.348 0.474
Attentional pooling 0.293 0.423 0.353 0.473
fusion layer 0.268 0.404 0.374 0.490
Fusion loss MSE MAE PCC(M) PCC(T)
w/o fusion loss 0.292 0.423 0.358 0.469
w/ fusion loss 0.268 0.404 0.374 0.490
Table 5. Ablation studies for fusion method and fusion loss
5. Conclusion
We demonstrate a novel approach for predicting spatial
gene expression patterns from WSIs. By incorporating
multiple sources of information utilizing various types of
transformer and our proposed fusion method, TRIPLEX
achieves superior performance compared to all existing
approaches in both internal and external evaluations.
TRIPLEX has the potential to improve the accuracy and
robustness of the predictions for spatial gene expression
distribution, paving the way for new discoveries at the
interface of WSIs and sequencing.
Acknowledgments This work was partly supported by In-
stitute of Information & communications Technology Plan-
ning & Evaluation(IITP) grant funded by the Korea govern-
ment(MSIT) (No.2019-0-00421, AI Graduate School Sup-
port Program(Sungkyunkwan University)) and the Sam-
sung Research Funding & Incubation Center of Samsung
Electronics under Project SRFC-MA2102-05. This work is
a study partly supported by domestic scholarships funded
by the Kwanjeong Educational Foundation (KEF1464).
11598
References
[1] Alma Andersson, Ludvig Larsson, Linnea Stenbeck, Fredrik
Salm ´en, Anna Ehinger, Sunny Wu, Ghamdan Al-Eryani,
Daniel Roden, Alex Swarbrick, ˚Ake Borg, et al. Spatial
deconvolution of her2-positive breast tumors reveals novel
intercellular relationships. bioRxiv , 2020. 2, 6
[2] Gabriele Campanella, Matthew G Hanna, Luke Geneslaw,
Allen Miraflor, Vitor Werneck Krauss Silva, Klaus J Busam,
Edi Brogi, Victor E Reuter, David S Klimstra, and Thomas J
Fuchs. Clinical-grade computational pathology using weakly
supervised deep learning on whole slide images. Nature
medicine , 25(8):1301–1309, 2019. 3
[3] Richard J Chen, Chengkuan Chen, Yicong Li, Tiffany Y
Chen, Andrew D Trister, Rahul G Krishnan, and Faisal
Mahmood. Scaling vision transformers to gigapixel images
via hierarchical self-supervised learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 16144–16155, 2022. 3
[4] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-
aolin Wei, Huaxia Xia, and Chunhua Shen. Conditional po-
sitional encodings for vision transformers. arXiv preprint
arXiv:2102.10882 , 2021. 4, 8
[5] Ozan Ciga, Tony Xu, and Anne Louise Martel. Self super-
vised contrastive learning for digital histopathology. Ma-
chine Learning with Applications , 7:100198, 2022. 4
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2
[7] Bryan He, Ludvig Bergenstr ˚ahle, Linnea Stenbeck,
Abubakar Abid, Alma Andersson, ˚Ake Borg, Jonas
Maaskola, Joakim Lundeberg, and James Zou. Integrating
spatial gene expression and breast tumour morphology
via deep learning. Nature biomedical engineering , 4(8):
827–834, 2020. 1, 2, 3, 6, 7
[8] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 4700–4708, 2017. 2
[9] Maximilian Ilse, Jakub Tomczak, and Max Welling.
Attention-based deep multiple instance learning. In Inter-
national conference on machine learning , pages 2127–2136.
PMLR, 2018. 8
[10] Andrew L Ji, Adam J Rubin, Kim Thrane, Sizun Jiang,
David L Reynolds, Robin M Meyers, Margaret G Guo, Ben-
son M George, Annelie Mollbrink, Joseph Bergenstr ˚ahle,
et al. Multimodal analysis of composition and spatial ar-
chitecture in human squamous cell carcinoma. Cell, 182(2):
497–514, 2020. 2, 6
[11] X Jin, L Zhu, Z Cui, J Tang, M Xie, and G Ren. Elevated
expression of gnas promotes breast cancer cell proliferation
and migration via the pi3k/akt/snail1/e-cadherin axis. Clini-
cal and Translational Oncology , 21:1207–1219, 2019. 7
[12] Fahdi Kanavati, Gouji Toyokawa, Seiya Momosaki, Michael
Rambeau, Yuka Kozuma, Fumihiro Shoji, Koji Yamazaki,Sadanori Takeo, Osamu Iizuka, and Masayuki Tsuneki.
Weakly-supervised learning for lung carcinoma classifica-
tion using deep learning. Scientific reports , 10(1):1–11,
2020. 3
[13] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[14] Thomas N Kipf and Max Welling. Semi-supervised classi-
fication with graph convolutional networks. arXiv preprint
arXiv:1609.02907 , 2016. 2
[15] Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple
instance learning network for whole slide image classifica-
tion with self-supervised contrastive learning. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 14318–14328, 2021. 3
[16] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J
Chen, Matteo Barbieri, and Faisal Mahmood. Data-efficient
and weakly supervised computational pathology on whole-
slide images. Nature biomedical engineering , 5(6):555–570,
2021. 3
[17] Patrice J Morin. Claudin proteins in human cancer: promis-
ing new targets for diagnosis and therapy. Cancer research ,
65(21):9603–9606, 2005. 7
[18] Minxing Pang, Kenong Su, and Mingyao Li. Leverag-
ing information in spatial transcriptomics to predict super-
resolution gene expression from histology images in tumors.
bioRxiv , 2021. 1, 2, 3, 6
[19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2, 3
[20] Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian
Zhang, Xiangyang Ji, et al. Transmil: Transformer based
correlated multiple instance learning for whole slide image
classification. Advances in Neural Information Processing
Systems , 34:2136–2147, 2021. 3
[21] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-
attention with relative position representations. arXiv
preprint arXiv:1803.02155 , 2018. 5
[22] Patrik L St ˚ahl, Fredrik Salm ´en, Sanja Vickovic, Anna Lund-
mark, Jos ´e Fern ´andez Navarro, Jens Magnusson, Stefania
Giacomello, Michaela Asp, Jakub O Westholm, Mikael
Huss, et al. Visualization and analysis of gene expression
in tissue sections by spatial transcriptomics. Science , 353
(6294):78–82, 2016. 1
[23] Asher Trockman and J Zico Kolter. Patches are all you need?
arXiv preprint arXiv:2201.09792 , 2022. 2
[24] Ronald Xie, Kuan Pang, Gary D. Bader, and Bo Wang. Spa-
tially resolved gene expression prediction from h&e histol-
ogy images via bi-modal contrastive learning, 2023. 1, 2, 3,
6, 7
[25] Yan Yang, Md Zakir Hossain, Eric A Stone, and Shafin Rah-
man. Exemplar guided deep neural network for spatial tran-
scriptomics analysis of gene expression prediction. In Pro-
ceedings of the IEEE/CVF Winter Conference on Applica-
11599
tions of Computer Vision , pages 5039–5048, 2023. 1, 2, 3, 6,
7
[26] Yuansong Zeng, Zhuoyi Wei, Weijiang Yu, Rui Yin, Yuchen
Yuan, Bingling Li, Zhonghui Tang, Yutong Lu, and Yue-
dong Yang. Spatial transcriptomics prediction from histol-
ogy jointly through transformer and graph neural networks.
Briefings in Bioinformatics , 23(5):bbac297, 2022. 1, 2, 3, 6
11600
