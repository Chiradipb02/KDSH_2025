PaReNeRF: Toward Fast Large-scale Dynamic NeRF with Patch-based
Reference
Xiao Tang1, Min Yang1, Penghui Sun1, Hui Li1, Yuchao Dai2, Feng Zhu1, Hojae Lee1
1Samsung R&D Institute China Xi‚Äôan (SRCX)
2Northwestern Polytechnical University
fxiao1.tang, min16.yang, penghui.sun, hui01.li g@samsung.com
daiyuchao@nwpu.edu.cn, ff15.zhu, hojae72.lee g@samsung.com
Abstract
With photo-realistic image generation, Neural Radiance
Field (NeRF) is widely used for large-scale dynamic scene
reconstruction as autonomous driving simulator. However,
large-scale scene reconstruction still suffers from extremely
long training time and rendering time. Low-resolution (L-
R) rendering combined with upsampling can alleviate this
problem but it degrades image quality. In this paper, we
design a lightweight reference decoder which exploits pri-
or information from known views to improve image recon-
struction quality of new views. In addition, to speed up prior
information search, we propose an optical Ô¨Çow and struc-
tural similarity based prior information search method. Re-
sults on KITTI and VKITTI2 datasets show that our method
signiÔ¨Åcantly outperforms the baseline method in terms of
training speed, rendering speed and rendering quality.
1. Introduction
With the development of autonomous driving, it is challeng-
ing to conduct experiments in various driving scenarios due
to complex geographical locations, varying surroundings
and road conditions. With simulators like CARLA [8], de-
velopers can test thousands of times faster than road testing,
but there are some serious issues such as domain gap and
costly manual effort. With the rapid development of neural
rendering technology, we can reconstruct higher-Ô¨Ådelity 3D
scenes from real road test data at a lower cost.
NeRF [29] was Ô¨Årst proposed in ECCV 2020 as a nov-
el view synthesis method and has attracted more and more
attention because of photo-realistic image generation. How-
ever, NeRF is originally designed for small static scene re-
construction, requiring camera views to be heavily over-
lapped. With the expansion of data collected by self-driving
Corresponding author.
Training‚Üì : 26 h ( ‚Üì60% )
Inference ‚Üì: 1.61 s (‚Üì87% )
PSNR‚Üë: 32.6( ‚Üë13% )
SUDS PaReNeRF VSTraining‚Üì : 64 h
Inference ‚Üì:  12.86 s
PSNR‚Üë: 28.31Figure 1. Comparison of image reconstruction performance on
KITTI dataset. We show some local details of the image recon-
struction results of the SUDS method and our PaReNeRF method.
SUDS, the state-of-the-art neural rendering method for large-scale
dynamic scenes, faces the problem of long training time and infer-
ence time, while PaReNeRF can generate higher-quality rendered
images and signiÔ¨Åcantly reduce training time (by 60%) and infer-
ence time (by 87%).
cars such as KITTI [12], nuScenes [3], and Waymo [9], re-
searchers are paying increasing attention to the reconstruc-
tion and novel view rendering of large-scale dynamic scenes
[22, 23, 26, 28, 32, 48, 51, 55, 57, 60].
Due to the sparsity of the collected data and the sin-
gle trajectory, it is still challenging to reconstruct large-
scale dynamic scenes. To solve this, many current stud-
ies design NeRF models for both the large-scale environ-
ment (background) and the moving objects (foreground)
[22, 32, 55, 57, 60], and make use of RGB images, depth
images, optical Ô¨Çow, semantic labels and other data to boost
the training, which has achieved impressive reconstruction
effect. However, the current state-of-the-art (SOTA) large-
scale dynamic scene reconstruction algorithms suffer from
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5428
slow training and inference speed due to high-resolution
rendering and much random ray sampling across videos.
For example, Mars [55] and SUDS [48] take about more
than 2 days to train a scene of about 9 seconds. In this pa-
per, we propose to replace random ray sampling with patch-
based sampling, which can greatly save data reading time
and thus speed up training, but this does not contribute to
the inference speed. Unisim [60] adopts a smaller resolu-
tion feature map than that of the rendered image, and re-
lies on the CNN for upsampling, which signiÔ¨Åcantly reduce
both the training time and inference time at the expense of
reconstruction accuracy. Prior information has been used
in small static scene reconstruction based on sparse input-
s [4, 7, 13, 15, 21, 35, 49, 53, 58, 61]. Due to the sparsity
and the single trajectory of the data collected by self-driving
car, we proposes a novel reference decoder to exploit pri-
or information from known views to improve reconstruc-
tion quality of new views. We demonstrate our reconstruc-
tion capabilities on scenes from the KITTI [12] dataset and
Virtual KITTI-2 (VKITTI2) [2] dataset in comparison with
baseline methods. The obtained results, shown in Fig. 1,
demonstrate the effectiveness of our method on both speed
and rendering quality. In summary, our contributions can be
summarized as follows:
We propose an optical Ô¨Çow and structural similarity
based prior information search method which can nar-
row the search area and speed up the search during the
training phase.
We replace random ray sampling with patch-based sam-
pling, and propose a novel reference decoder to merge
prior information into the decoding and upsampling net-
work.
On the KITTI and VKITTI2 datasets, we show that our
approach can render highly detailed scene, signiÔ¨Åcant-
ly improve upon the state-of-the-art methods, and reduce
training time and inference time by over 60% and 87%
respectively.
2. Related Work
2.1. Neural radiance Ô¨Åelds (NeRFs)
Implicit neural representations have demonstrated its effec-
tiveness for novel view synthesis [6, 10, 24, 27, 30, 31, 37‚Äì
39, 41, 54, 62]. Among them, NeRF [29] achieved remark-
able results for high-Ô¨Ådelity image generation by encoding
continuous scene radiance Ô¨Åelds within multi-layer percep-
tron (MLP) given a set of posed camera images. These
methods can represent complex geometry and appearance
and have achieved photorealistic rendering, but they focus
only on small static and object-centric scenes.2.2. Large-scale scene reconstruction
A growing number of NeRF extensions apply NeRF to un-
bounded scenes [11, 14, 18, 43, 46, 50, 52, 56, 59]. Block-
NeRF [44] and Mega-NeRF [47] partition the scene spatial-
ly and train separate NeRF for each partition. URF [34]
leverages accurate dense LiDAR depth and predicted im-
age segmentation as supervision to provide signiÔ¨Åcant per-
formance improvements on street view data. BungeeNeRF
[56] makes the Ô¨Årst attempt to bring NeRF to city-scale, in-
troducing a progressive training method from coarse to Ô¨Åne
and thus adding more capacity to the network representa-
tion. All of these methods work only on static scenes, and
produce many blurs, Ô¨Çoaters and other artifacts when train-
ing on existing self-driving datasets.
2.3. NeRF for large-scale dynamic scene
As more and more street view data are collected by self-
driving cars, the reconstruction and novel view rendering
for large-scale dynamic scenes have attracted the attention
of many researchers [5, 22, 23, 26, 32, 36, 48, 55, 57, 60].
NSG [32] and PNF [22] decompose a scene into back-
ground and a set of objects, with each object represented by
an MLP. These methods tend to reconstruct a single video
clip of short duration due to the following limitations. (1)
Memory grows linearly with the number of moving object-
s and input videos because a separate model is built for
each object. (2) These methods require supervision via 3D
bounding boxes and panoptic labels, obtained manually or
via category-speciÔ¨Åc models [48].
As a step towards truly open-world reconstructions of
dynamic cities, SUDS [48] factorizes the scene into three
separate hash table data structures to efÔ¨Åciently encode stat-
ic, dynamic, and far-Ô¨Åeld radiance Ô¨Åelds, and makes use of
a rich suite of informative but freely available input signals,
such as LiDAR depth measurements and optical Ô¨Çow. SUD-
S achieved SOTA performance but it requires long training
time (e.g., more than two days for a nine-second video) due
to high resolution (HR) and random ray samplings across
videos. UniSim [60] renders a smaller resolution feature
map and uses CNN for upsampling, which can signiÔ¨Åcantly
reduce training time, but such encoder-decoder structure de-
crease reconstruction accuracy. In order to simultaneously
accelerate training speed, inference speed, and improve ren-
dering quality, we propose a novel reference decoder which
exploits prior information from known views to improve re-
construction quality of new views when decoding the low-
resolution volume rendering feature map output by the ra-
diance Ô¨Åelds.
2.4. NeRF with reference
Prior information has been used to improve the performance
of NeRF based on sparse perspectives [4, 17, 20, 25, 33,
40, 42, 49, 53, 61], but the scale of applied scenes, the use
5429
Maximum
structural similarity
HR reference-patchSample patch in input LR views 
Similar area in
backward or forward viewsRadiance fields
LR feature map 
...
Sliding patch in similar area
 pixel-by-pixelCoordinates and optical flowReference decoder
HR rendered image
Figure 2. The architecture of our proposed PaReNeRF. For a sampled patch in LR training views, we can Ô¨Ånd similar area in backward
and forward views based on coordinates and optical Ô¨Çow, then slide patches in the similar areas and calculate the structural similarity of
the slided patches and the LR feature map output by radiance Ô¨Åelds. The HR reference-patch with max structural similarity is merged into
reference decoder to improve the reconstruction quality.
method of prior information, and the effects achieved are
different from this paper. NeRF-SR [49] leverages a reÔ¨Åne-
ment network that blends details from only one HR refer-
ence by Ô¨Ånding relevant patches with available depth maps.
However, directly reÔ¨Åning the HR image cannot improve
the rendering speed, and the depth-based reference search
method is only suitable for static scenes. pixelNeRF [61],
MVSNeRF [4] and Wimbauer et al. [53] all introduced pri-
ors into the NeRF network to improve the training quality
of sparse perspectives. For a query point, the corresponding
image feature is extracted from the feature volume through
projection and interpolation and passed into the NeRF net-
work. However, these methods are only suitable for small
static and object-centered scenes and not suitable for dy-
namic scenes due to the sparsity and single trajectory of
camera poses in dynamic scenes. In order to Ô¨Ånd the most
similar prior information in dynamic scenes, we proposes a
search method based on optical Ô¨Çow and structural similar-
ity.
3. Method
In this section, we Ô¨Årst review the SUDS algorithm as
the baseline. Next, we present the overall process of our
method. We introduce our three main improvements to
SUDS, including training and inference acceleration based
on patch sampling and low-resolution rendering combined
with upsampling, prior information search based on optical
Ô¨Çow and structural similarity, and image quality improve-
ment based on reference decoder. Finally, we discuss how
to learn the model from real-world data. Fig. 2 shows an
overview of our approach.3.1. Preliminaries
NFF and NeRF: Neural feature Ô¨Åeld (NFF) refers to a con-
tinuous function fthat maps a 3D point xi2R3and a view
directioni2R2to an implicit geometry s2Rand aNf-
dimensional feature descriptor f2RNf[60]. The mapping
function is realized by MLPs. NFFs can be seen as a su-
perset of several existing studies [29, 53]. If we represent
the implicit geometry as volume density iand the feature
descriptor as RGB color ci, NFF becomes NeRF [29, 60].
NeRF samples points along the ray for each image pix-
el, querying the MLP to obtain density and color values.
Then it yields a color prediction value ^C(r)for the ray
using numerical quadraturePN 1
i=0Ti(1 exp( ii))ci,
whereTi=exp( Pi 1
j=0jj)andiis the distance be-
tween samples. The training process optimizes the model
by sampling image pixels and minimizing the loss functionP
r2RkC(r) ^C(r)k2.
SUDS: Built upon NeRF [29], SUDS [48] decomposes
the world into a static branch, a dynamic branch, and a far-
Ô¨Åeld branch.
(1) The static branch models stationary topography that
is consistent across videos and maps the feature vector ob-
tained from hash table into a view-dependent color csand a
view-independent density s.
(2) The dynamic branch handles both transient (e.g.,
parked cars) and truly dynamic objects (e.g., pedestrians),
and also assumes that both the density dand colorcdde-
pend on time.
(3) The far-Ô¨Åeld branch handles far-Ô¨Åeld objects and sky
with an environment map. And then derives a single den-
sity and radiance value for any position by computing the
weighted sum of the static and dynamic components.
The method jointly optimizes all three branches by min-
imizing the combination of reconstruction losses, warping
5430
losses, Ô¨Çow losses, static-dynamic factorization loss and
shadow loss.
3.2. Reference decoder based NeRF
In the SUDS architecture, batches of rays are sampled ran-
domly across input videos and high-resolution RGB images
are rendered directly, but this requires long training time
and inference time. We argue that patch-based ray sam-
pling can reduce the time of data reading, thereby shorten-
ing the training time of the model. Moreover, similar to U-
nisim, we adopt low-resolution feature map rendering com-
bined with CNN upsampling to further reduce the number
of ray queries and thus training and inference time. How-
ever, patch-based ray sampling and such encoder-decoder
structure will result in loss of rendering accuracy. In this
paper, we provide a reference decoder rf, which can use
reference information R2RHW3when decoding and
upsampling low-resolution feature map F2RHfWfNf,
thus improving both training speed and rendering quality.
rf: (F2RHfWfNf;R2RHW3)!I2RHW3
(1)
The pipeline is shown in Fig. 2. We use the three branch-
es described in SUDS as our radiance Ô¨Åelds and change the
RGB output to low-resolution feature map output. Specif-
ically, we improve the SUDS from the following three as-
pects.
3.2.1 Patch-sampling based radiance Ô¨Åelds
We change random ray sampling to patch sampling, which
can greatly reduce the number of sampling times and thus
training time. If Nrays are randomly sampled for each
batch, the data need to be loaded Ntimes. Assuming that
we set the patch size to hw, then we only need to load
the dataN=(hw)times to obtain Nrays. In addition,
patch-based ray sampling also make it possible to imple-
ment image-level processing (e.g., CNN upsampling) dur-
ing the rendering.
To analyze the advantages and disadvantages of patch
sampling and encoder-decoder structure, we conducted ab-
lation experiments, and the detailed introduction of the
dataset, parameter settings and experiment results is shown
in Section 4. In a word, patch sampling can effectively im-
prove the training speed, but the reconstruction accuracy is
reduced. Using low-resolution feature map rendering com-
bined with CNN upsampling can greatly speed up inference,
but compared with the original SUDS, the reconstruction
performance is declined and needs further optimization.3.2.2 Prior information searching module
Considering that in the video, the same content (including
static background and dynamic objects) will appear in dif-
ferent frames and views, the information of different frames
and views can complement each other. Therefore, we pro-
poses to integrate prior information of the known views in-
to the decoding process to improve the rendering quality of
new views.
Searching for reference information in the entire image
is generally time-consuming [16]. In order not to affect the
training speed, it is necessary to quickly search for the most
similar prior information in the large-scale dynamic scene.
We proposes an optical Ô¨Çow and structural similarity based
search method, as shown in Fig. 2.
SpeciÔ¨Åcally, in the training phase, for a randomly sam-
pled patchP2Rhw3, the coordinate range of the patch
pixels is (i;j)2[(si;sj );(si+h;sj +w)]. According
to [45, 48], we can get 2D optical Ô¨Çow flijfor each pix-
el(i;j). And then we can determine similar regions Reff
andRefb in the forward and backward views respectively
by calculating the minimum (mini;minj )and maximum
(maxi;maxj )values of the original coordinates (i;j)plus
the optical Ô¨Çow values flij. Then we slide the patches p
pixel by pixel in the two similar regions Reff andRefb ,
calculate the structural similarity sbetweenpand the sam-
pled patchP, and take the patch with the largest structural
similarity as the reference patch RP.
In the inference stage, the structural similarity sm,m2
[0;M]between the volume rendering feature map Fof the
new view and the RGB image ^Imof all known views is cal-
culated, and the image with the highest structural similarity
is used as the reference image RI.
3.2.3 Reference decoder structure
In this section, we mainly introduce the structure of our pro-
posed reference decoder rf. To minimize the latency of the
reference decoder, we design a lightweight network based
on a classic super-resolution network CARN [1] due to its
comprehensive performance on both the speed and accu-
racy. The reference decoder exploits prior information RI
from known views to improve reconstruction quality of new
views when decoding the low-resolution volume rendering
feature map Foutput by the radiance Ô¨Åelds. As shown in
the Fig. 3. our reference decoder includes the following
modules:
The feature encoder extract hidden feature of the
reference-image RI.
The CARN feature encoder extract hidden feature of the
volume rendering feature map F and fused feature respec-
tively, the cascading residual network can integrate fea-
tures from multiple layers based on local and global cas-
caded modules to receive more information of different
5431
HR referenceLR feature map 
Feature encoder
 3x1 Conv
MeanShiftCARN feature
encoderCARN feature
encoderUp sampling
Feature decoder
 
3x1 ConvMeanShift
HR rendered image
Concat
Figure 3. The structure of our reference decoder. Firstly, we apply Cascading Residual Network (CARN) encoding and upsampling to
the LR feature map and output the HR feature map, and merge the feature of HR reference into the decoding process to improve the Ô¨Ånal
rendering quality.
layers, we refer the reader to [1] for more details.
The up-sampling module is a 31convolutional layer
with a PixelShufÔ¨Çe layer upsampling the low-resolution
feature map F.
The feature decoder render the Ô¨Ånal feature map to RGB
imageI.
3.3. Optimization
To optimize our system, we use the losses described in
SUDS to jointly optimize the radiance Ô¨Åelds and the ref-
erence decoder. For the detailed calculation of the losses,
we refer the reader to [48].
In this paper, we only change the C(r)in the L2 pho-
tometric lossLc(r) =C(r) ^
C(r)2
from the rendered
RGB image output by the SUDS radiance Ô¨Åeld to the recon-
structed image output by the reference decoder. In addition,
we compute the losses in a patch-wise fashion.
4. Experiments
In this section, we provide both quantitative and qualitative
comparisons to demonstrate the advantages of the proposed
method.
4.1. Experimental Setup
Dataset. We evaluate our method using the same KITTI
[12] and Virtual KITTI-2 (VKITTI2) [2] subsequences as
in prior works [22, 32, 48]. Each training sequence consists
of up to 90 time steps or 9 seconds and images with the size
of 1242375, each from two camera perspectives.
Tasks. We validate the photorealistic rendering perfor-
mance of our method by evaluating image reconstruction
and novel view synthesis (NVS). The training and testingimage sets in the image reconstruction setting are identical,
while in the NVS task, we render the frames that are not
included in the training data. SpeciÔ¨Åcally, we evaluate the
methods using different train/test splits, holding out every
4th time step (75%), every other time step (50%), and Ô¨Ånal-
ly training with only one in every four time steps (25%).
Baseline . We compare our method to other state-of-the-
art methods like NeRF, NeRF+Time, NSG, PNF, and SUD-
S, rely on their reported numbers.
Metrics. We follow the standard evaluation protocol in
image synthesis and report Peak Signal-to-Noise Ratio (P-
SNR), Structural Similarity (SSIM), and Learned Perceptu-
al Image Patch Similarity (LPIPS) of our default setting for
quantitative evaluations. More importantly, we demonstrate
the contribution of this algorithm to reducing training time
and inference time in Table 2 and Table 1.
Training. We train our model for 125,000 iterations with
4,096 rays per batch, each batch including 16 patches with
the size of 1616. We render a lower-resolution ( 414125)
feature map than that of the rendered image 1242375,
and rely on the reference decoder for 3x upsampling. We
use Adam [19] with a learning rate of 510 3decaying to
510 4.
4.2. KITTI Benchmarks
4.2.1 Image reconstruction
The image reconstruction results of KITTI dataset are sum-
marized in Table 1 with qualitative results in Fig. 4. Our
proposed PaReNeRF achieves the best results across al-
l metrics. Compared with the SOTA method SUDS, im-
age quality is improved by 13% and meanwhile the train-
ing time and inference time are reduced by 60% and 87%
respectively. As can be seen from Fig. 4, the proposed al-
5432
SUDS Ours GT SUDS Ours GT
Figure 4. Image reconstruction on KITTI dataset. The Ô¨Årst row is a sample of two scenes from the KITTI dataset. The following rows
are some local details of the reconstruction results of the SUDS algorithm, our algorithm and the ground truth. Previous work failed to
reconstruct some details and the image is a bit blurry. Our algorithm can achieve better reconstructed image quality and recover clearer
details.
NeRF NeRF+Time NSG PNF SUDS Ours
PSNR" 23.34 24.18 26.66 27.48 28.31 32.642
SSIM" 0.662 0.677 0.806 0.87 0.870 0.933
Training Time(h)# 40 41 51 - 64 26
Inference Time(s)# 7.2 7.8 3.1 - 12.86 1.61
Table 1. Comparison results of image reconstruction on KITTI dataset. We outperform past work on both the image reconstruction accuracy
and the latency.
SUDS* Ours
PSNR" 30.853 30.894
SSIM" 0.928 0.932
Training Time(h)# 64 26
Inference Time(s)# 11.89 1.35
Table 2. Comparison results of image reconstruction on VKITTI2
dataset.gorithm has clearer effect on the reconstructed image. And
the qunatitative results of VKITTI2 dataset are summarized
in Table 2. Since there were no image reconstruction results
of other baseline methods on the VKITTI2 dataset, we re-
produced the SOTA algorithm SUDS, named SUDS*1, for
comparison. Table 2 shows that our method can greatly im-
prove training speed and rendering speed and obtain better
image quality for the VKITTI2 dataset.
1SUDS* is noted reproduced SUDS algorithm
5433
SUDS-50% GT Ours-50% SUDS-75% Ours-75%
Figure 5. Novel view synthesis on KITTI dataset. We show some local details of the new view rendering results when trained with
different proportion of subsequence from the KITTI scenes. Ghosting artifacts is generated in the SUDS rendered images, while our
algorithm performs better.
4.2.2 Novel view synthesis
In Table 3 and Fig. 5, we demonstrate our capabilities to
generate plausible renderings at time steps unseen during
training. As can be seen from the experimental results in
Table 3, for both the KITTI dataset and VKITTI2 dataset,
our algorithm performs best. In Fig. 5, we show qualitative
results for novel view synthesis on the KITTI dataset. As
the number of training views is reduced, ghosting artifactswill be generated in the SUDS rendered images, while our
algorithm performs better under new views other than the
training views. There are no obvious ghosting artifacts, and
the details of the scene are well reconstructed.
4.3. Ablation Studies
We conduct a series of ablation studies to analyze each part
of the proposed model. From Table 4, we can Ô¨Ånd the fol-
lowings.
5434
KITTI-75% KITTI-50% KITTI-25%
PSNR"SSIM"LPIPS#PSNR"SSIM"LPIPS#PSNR"SSIM"LPIPS#
NeRF 18.56 0.557 0.554 19.12 0.587 0.497 18.61 0.57 0.51
NeRF+Time 21.01 0.612 0.492 21.34 0.635 0.448 19.55 0.586 0.505
NSG 21.53 0.673 0.254 21.26 0.659 0.266 20 0.632 0.281
SUDS 22.77 0.797 0.171 23.12 0.821 0.135 20.76 0.747 0.198
Ours 25.19 0.879 0.075 23.91 0.854 0.09 21.05 0.771 0.15
VKITTI2-75% VKITTI2-50% VKITTI2-25%
PSNR"SSIM"LPIPS#PSNR"SSIM"LPIPS#PSNR"SSIM"LPIPS#
NeRF 18.67 0.548 0.634 18.58 0.544 0.635 18.17 0.537 0.644
NeRF+Time 19.03 0.574 0.587 18.9 0.565 0.61 18.04 0.545 0.626
NSG 23.41 0.689 0.317 23.23 0.679 0.325 21.29 0.666 0.317
SUDS 23.87 0.846 0.150 23.78 0.851 0.142 22.18 0.829 0.160
Ours 24.55 0.873 0.097 22.78 0.850 0.121 21.55 0.824 0.141
Table 3. Novel view synthesis results on both the KITTI and VKITTI2 dataset. As the fraction of training views decreases, accuracy drops
for all methods. However, our algorithm performs best.
Patch
SamplingEncoder-
DecoderReference
DecoderIterations PSNR SSIMTraining
Time(h)Inference
Time(s)
7 7 7 250K 28.31 0.870 64 12.55
X 7 7 250K 27.04 0.843 36 12.42
X X 7 250K 29.31 0.866 44 1.75
X X 7 125K 28.65 0.858 21 1.74
X X X 250K 33.23 0.941 55 1.61
X X X 125K 32.64 0.933 26 1.61
Table 4. Ablation study on the KITTI dataset. The Ô¨Årst line is the reconstruction result of SUDS as the baseline, and then we analyze each
part of the proposed model. PaReNeRF with full components achieved the best quantitative results.
Effect of patch sampling. Patch sampling can effective-
ly improve the training speed of the model, but the inference
speed cannot be improved because the sampling method in
the inference stage remains unchanged and the entire image
is still sampled pixel by pixel in batches. Moreover, since
the patch sampling method reduces the randomness of data
sampling during training, the reconstruction accuracy will
also be reduced.
Effect of encoder-decoder structure. Compared with
using patch sampling alone, applying low-resolution fea-
ture map rendering combined with CNN upsampling can
greatly speed up inference and improve reconstruction qual-
ity, but it will consume more training time. If the training
time is shortened by reducing the number of iterations from
250,000 to 125,000, the reconstruction performance will be
reduced and worse than the original SUDS algorithm.
Effect of reference decoder. By introducing reference,
image quality can be greatly improved. Even if the training
iterations is reduced to 125,000, the image quality is still
signiÔ¨Åcantly better than the SUDS algorithm.In conclusion, we have comprehensively improved train-
ing speed, inference speed and rendering quality by apply-
ing patch sampling and reference decoder.
5. Conclusion
In this work, we develop a large-scale dynamic neural ren-
dering system based on reference decoder. We propose
a structural similarity based prior information searching
method, and to speed up the search during training phase,
we use optical Ô¨Çow to narrow the search area. Further-
more, we propose a novel reference-decoder exploiting pri-
or information from known views to improve reconstruction
quality of new views. The experimental evaluations show
that our system performs signiÔ¨Åcantly better than baseline
models. Although we have greatly improved the train-
ing speed and rendering speed of neural rendering method,
there are still many open challenges before building truly
real-time training and rendering.
5435
References
[1] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Fast,
accurate, and lightweight super-resolution with cascading
residual network. In Proceedings of the European confer-
ence on computer vision (ECCV) , pages 252‚Äì268, 2018. 4,
5
[2] Yohann Cabon, Naila Murray, and Martin Humenberger. Vir-
tual kitti 2, 2020. 2, 5
[3] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V o-
ra, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,
Giancarlo Baldan, and Oscar Beijbom. nuscenes: A mul-
timodal dataset for autonomous driving. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11621‚Äì11631, 2020. 1
[4] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,
Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-
izable radiance Ô¨Åeld reconstruction from multi-view stereo.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 14124‚Äì14133, 2021. 2, 3
[5] Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng,
Xuan Wang, and Jue Wang. Hallucinated neural radiance
Ô¨Åelds in the wild. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 12943‚Äì12952, 2022. 2
[6] Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H
Kim, and Jan Kautz. Extreme view synthesis. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 7781‚Äì7790, 2019. 2
[7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-
manan. Depth-supervised nerf: Fewer views and faster train-
ing for free. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
12882‚Äì12891, 2022. 2
[8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-
nio Lopez, and Vladlen Koltun. Carla: An open urban driv-
ing simulator. In Conference on robot learning , pages 1‚Äì16.
PMLR, 2017. 1
[9] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Li-
u, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp,
Charles R Qi, Yin Zhou, et al. Large scale interactive motion
forecasting for autonomous driving: The waymo open mo-
tion dataset. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 9710‚Äì9719, 2021. 1
[10] John Flynn, Michael Broxton, Paul Debevec, Matthew Du-
Vall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and
Richard Tucker. Deepview: View synthesis with learned gra-
dient descent. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2367‚Äì
2376, 2019. 2
[11] Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu, X-
iaowei Zhou, Andreas Geiger, and Yiyi Liao. Panopticnerf-
360: Panoramic 3d-to-2d label transfer in urban scenes,
2023. 2
[12] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In 2012 IEEE conference on computer vision and pat-
tern recognition , pages 3354‚Äì3361. IEEE, 2012. 1, 2, 5[13] Tao Hu, Xiaogang Xu, Shu Liu, and Jiaya Jia. Point2pix:
Photo-realistic point cloud rendering via neural radiance
Ô¨Åelds, 2023. 2
[14] Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia,
Yuxuan Han, and Junyi Ma. Pc-nerf: Parent-child neural
radiance Ô¨Åelds under partial sensor data loss in autonomous
driving environments, 2023. 2
[15] Shengyu Huang, Zan Gojcic, Zian Wang, Francis Williams,
Yoni Kasten, Sanja Fidler, Konrad Schindler, and Or Litany.
Neural lidar Ô¨Åelds for novel view synthesis, 2023. 2
[16] Yuming Jiang, Kelvin CK Chan, Xintao Wang, Chen Change
Loy, and Ziwei Liu. Robust reference-based super-resolution
via c2-matching. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
2103‚Äì2112, 2021. 4
[17] Daiju Kanaoka, Motoharu Sonogashira, Hakaru Tamukoh,
and Yasutomo Kawanishi. Manifoldnerf: View-dependent
image feature supervision for few-shot neural radiance Ô¨Åelds,
2023. 2
[18] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten
Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio
Torralba, and Sanja Fidler. NeuralÔ¨Åeld-ldm: Scene genera-
tion with hierarchical latent diffusion models, 2023. 2
[19] D Kinga, Jimmy Ba Adam, et al. A method for stochastic
optimization. In International conference on learning rep-
resentations (ICLR) , page 6. San Diego, California;, 2015.
5
[20] Kanghyeok Ko and Minhyeok Lee. Zignerf: Zero-shot 3d
scene representation with invertible generative neural radi-
ance Ô¨Åelds, 2023. 2
[21] Jonas Kulhanek and Torsten Sattler. Tetra-nerf: Represent-
ing neural radiance Ô¨Åelds using tetrahedra, 2023. 2
[22] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Car-
oline Pantofaru, Leonidas J Guibas, Andrea Tagliasacchi,
Frank Dellaert, and Thomas Funkhouser. Panoptic neural
Ô¨Åelds: A semantic object-aware neural scene representation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12871‚Äì12881, 2022.
1, 2, 5
[23] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tuck-
er, and Noah Snavely. Dynibar: Neural dynamic image-
based rendering. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
4273‚Äì4284, 2023. 1, 2
[24] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and
Christian Theobalt. Neural sparse voxel Ô¨Åelds. Advances
in Neural Information Processing Systems , 33:15651‚Äì15663,
2020. 2
[25] Tianyu Liu, Hao Zhao, Yang Yu, Guyue Zhou, and Ming
Liu. Car-studio: Learning car radiance Ô¨Åelds from single-
view and endless in-the-wild images, 2023. 2
[26] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tsen-
g, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes
Kopf, and Jia-Bin Huang. Robust dynamic radiance Ô¨Åelds.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 13‚Äì23, 2023. 1, 2
5436
[27] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel
Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-
umes: Learning dynamic renderable volumes from images.
arXiv preprint arXiv:1906.07751 , 2019. 2
[28] Andr ¬¥eas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang,
Changil Kim, Min H. Kim, and Johannes Kopf. Progres-
sively optimized local radiance Ô¨Åelds for robust view synthe-
sis. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 16539‚Äì
16548, 2023. 1
[29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance Ô¨Åelds for view syn-
thesis. Communications of the ACM , 65(1):99‚Äì106, 2021. 1,
2, 3
[30] Thomas M ¬®uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1‚Äì15, 2022. 2
[31] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and
Andreas Geiger. Differentiable volumetric rendering: Learn-
ing implicit 3d representations without 3d supervision. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 3504‚Äì3515, 2020. 2
[32] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and
Felix Heide. Neural scene graphs for dynamic scenes. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2856‚Äì2865, 2021. 1, 2,
5
[33] Sicong Pan, Liren Jin, Hao Hu, Marija Popovi?, and Maren
Bennewitz. How many views are needed to reconstruct an
unknown object using nerf?, 2023. 2
[34] Konstantinos Rematas, Andrew Liu, Pratul P Srinivasan,
Jonathan T Barron, Andrea Tagliasacchi, Thomas Funkhous-
er, and Vittorio Ferrari. Urban radiance Ô¨Åelds. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 12932‚Äì12942, 2022. 2
[35] Barbara Roessle, Jonathan T. Barron, Ben Mildenhall, Prat-
ul P. Srinivasan, and Matthias Nie√üner. Dense depth priors
for neural radiance Ô¨Åelds from sparse input views. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 12892‚Äì12901, 2022.
2
[36] Viktor Rudnev, Mohamed Elgharib, William A. P. Smith,
Lingjie Liu, Vladislav Golyanik, and Christian Theobalt.
Neural radiance Ô¨Åelds for outdoor scene relighting. CoRR ,
abs/2112.05140, 2021. 2
[37] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias
Nie√üner, Gordon Wetzstein, and Michael Zollhofer. Deep-
voxels: Learning persistent 3d feature embeddings. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 2437‚Äì2446, 2019. 2
[38] Vincent Sitzmann, Michael Zollh ¬®ofer, and Gordon Wet-
zstein. Scene representation networks: Continuous 3d-
structure-aware neural scene representations. Advances in
Neural Information Processing Systems , 32, 2019.
[39] Vincent Sitzmann, Julien Martel, Alexander Bergman, David
Lindell, and Gordon Wetzstein. Implicit neural representa-tions with periodic activation functions. Advances in neural
information processing systems , 33:7462‚Äì7473, 2020. 2
[40] Nagabhushan Somraj and Rajiv Soundararajan. ViP-NeRF:
Visibility prior for sparse input neural radiance Ô¨Åelds. In
Special Interest Group on Computer Graphics and Interac-
tive Techniques Conference Conference Proceedings . ACM,
2023. 2
[41] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron,
Ravi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing the
boundaries of view extrapolation with multiplane images. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 175‚Äì184, 2019. 2
[42] Jiakai Sun, Zhanjie Zhang, Jiafu Chen, Guangyuan Li, Boy-
an Ji, Lei Zhao, Wei Xing, and Huaizhong Lin. Vgos: V ox-
el grid optimization for view synthesis from sparse inputs,
2023. 2
[43] Teppei Suzuki. Federated learning for large-scale scene
modeling with neural radiance Ô¨Åelds, 2023. 2
[44] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-
han, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,
and Henrik Kretzschmar. Block-nerf: Scalable large scene
neural view synthesis. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
8248‚Äì8258, 2022. 2
[45] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs Ô¨Åeld
transforms for optical Ô¨Çow. In Computer Vision ‚Äì ECCV
2020 , pages 402‚Äì419, Cham, 2020. Springer International
Publishing. 4
[46] Haithem Turki, Deva Ramanan, and Mahadev Satya-
narayanan. Mega-nerf: Scalable construction of large-scale
nerfs for virtual Ô¨Çy-throughs. CoRR , abs/2112.10703, 2021.
2
[47] Haithem Turki, Deva Ramanan, and Mahadev Satya-
narayanan. Mega-nerf: Scalable construction of large-
scale nerfs for virtual Ô¨Çy-throughs. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12922‚Äì12931, 2022. 2
[48] Haithem Turki, Jason Y Zhang, Francesco Ferroni, and Deva
Ramanan. Suds: Scalable urban dynamic scenes. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12375‚Äì12385, 2023. 1, 2, 3,
4, 5
[49] Chen Wang, Xian Wu, Yuan-Chen Guo, Song-Hai Zhang,
Yu-Wing Tai, and Shi-Min Hu. Nerf-sr: High quality neural
radiance Ô¨Åelds using supersampling. In Proceedings of the
30th ACM International Conference on Multimedia , pages
6445‚Äì6454, 2022. 2, 3
[50] Fusang Wang, Arnaud Louys, Nathan Piasco, Moussab Ben-
nehar, Luis Rold?o, and Dzmitry Tsishkou. Planerf: Svd un-
supervised 3d plane regularization for nerf large-scale scene
reconstruction, 2023. 2
[51] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Li-
u, Taku Komura, Christian Theobalt, and Wenping Wang.
F2-nerf: Fast neural radiance Ô¨Åeld training with free camer-
a trajectories. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
4150‚Äì4159, 2023. 1
5437
[52] Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang,
Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng
Chen, and Sanja Fidler. Neural Ô¨Åelds meet explicit geometric
representation for inverse rendering of urban scenes, 2023. 2
[53] Felix Wimbauer, Nan Yang, Christian Rupprecht, and Daniel
Cremers. Behind the scenes: Density Ô¨Åelds for single view
reconstruction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9076‚Äì
9086, 2023. 2, 3
[54] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon
Yenphraphai, and Supasorn Suwajanakorn. Nex: Real-time
view synthesis with neural basis expansion. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 8534‚Äì8543, 2021. 2
[55] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng
Chen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao
Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Y-
ongliang Shi, Yiyi Liao, and Hao Zhao. Mars: An instance-
aware, modular and realistic simulator for autonomous driv-
ing, 2023. 1, 2
[56] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,
Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. C-
itynerf: Building nerf at city scale. CoRR , abs/2112.05504,
2021. 2
[57] Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, and Li
Zhang. S-nerf: Neural radiance Ô¨Åelds for street views, 2023.
1, 2
[58] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhix-
in Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-
nerf: Point-based neural radiance Ô¨Åelds. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 5438‚Äì5448, 2022. 2
[59] Yinghao Xu, Menglei Chai, Zifan Shi, Sida Peng, Ivan Sko-
rokhodov, Aliaksandr Siarohin, Ceyuan Yang, Yujun Shen,
Hsin-Ying Lee, Bolei Zhou, and Sergey Tulyakov. Dis-
coscene: Spatially disentangled generative radiance Ô¨Åelds
for controllable 3d-aware scene synthesis, 2022. 2
[60] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Mani-
vasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Ur-
tasun. Unisim: A neural closed-loop sensor simulator. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 1389‚Äì1399, 2023. 1, 2,
3
[61] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance Ô¨Åelds from one or few images.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 4578‚Äì4587, 2021. 2,
3
[62] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
and Noah Snavely. Stereo magniÔ¨Åcation: Learning view
synthesis using multiplane images. arXiv preprint arX-
iv:1805.09817 , 2018. 2
5438
