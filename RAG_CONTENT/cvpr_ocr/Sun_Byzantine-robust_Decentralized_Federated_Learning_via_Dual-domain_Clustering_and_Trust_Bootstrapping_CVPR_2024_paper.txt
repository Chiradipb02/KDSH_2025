Byzantine-robust Decentralized Federated Learning via Dual-domain Clustering
and Trust Bootstrapping
Peng Sun†,♯, Xinyang Liu§,‡,♯, Zhibo Wang≀, Bo Liu‡,∗
†College of Computer Science and Electronic Engineering, Hunan University, China
§Department of Aeronautical and Aviation Engineering, The Hong Kong Polytechnic University, China
≀School of Cyber Science and Technology, Zhejiang University, China
‡Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS), China
psun@hnu.edu.cn, codex.lxy@gmail.com, zhibowang@zju.edu.cn, liubo@cuhk.edu.cn
Abstract
Decentralized federated learning (DFL) facilitates col-
laborative model training across multiple connected clients
without a central coordination server, thereby avoiding the
single point of failure in traditional centralized federated
learning (CFL). However, DFL exhibits increased suscep-
tibility to Byzantine attacks owing to the lack of a respon-
sible central server. Furthermore, a benign client in DFL
may be dominated by Byzantine clients (more than half of
its neighbors are malicious), posing significant challenges
for robust model training. In this work, we propose DFL-
Dual, a novel Byzantine-robust DFL method through dual-
domain client clustering and trust bootstrapping. Specif-
ically, we first propose to leverage both data-domain and
model-domain distance metrics to identify client discrep-
ancies. Then, we design a trust evaluation mechanism
centered on benign clients, which enables them to evalu-
ate their neighbors. Building upon the dual-domain dis-
tance metric and trust evaluation mechanism, we further
develop a two-stage clustering and trust bootstrapping tech-
nique to exclude Byzantine clients from local model aggre-
gation. We extensively evaluate the proposed DFL-Dual
method through rigorous experimentation, demonstrating
its remarkable performance superiority over existing robust
CFL and DFL schemes.
1. Introduction
Federated learning (FL) is a popular distributed machine
learning paradigm that enables collaborative model training
across multiple clients without centralizing their raw train-
ing data [7, 15, 20, 39]. The traditional centralized feder-
♯Authors with equal contribution.
∗Bo Liu is the corresponding author, Email: liubo@cuhk.edu.cn.ated learning (CFL) framework relies on a central server to
coordinate the distributed model training process [3]. This
dependence on a central entity may incur a single point of
failures [16, 22]. Specifically, the normal model training
process can be disrupted in cases where the central server
experiences a crash or is hacked. Decentralized federated
learning (DFL) [10, 12, 26] facilitates collaborative model
training among connected clients without a central coordi-
nation server, thereby avoiding the single-point-of-failure
issue. DFL has given rise to a new wave of distributed learn-
ing methods [17, 25, 35] that achieve comparable model
accuracy to state-of-the-art CFL approaches while offering
several significant advantages (e.g., fault tolerance, scala-
bility, and flexibility) [18].
However, similar to CFL, DFL remains vulnerable to
Byzantine attacks due to the inaccessibility of peer clients’
local training data and the uninspectable local training pro-
cess [9, 23, 24]. Specifically, malicious clients may tam-
per with local training data (i.e., data poisoning attacks) or
falsify model parameters (i.e., model poisoning attacks) to
craft malicious models to disrupt the model training pro-
cess. Furthermore, DFL exhibits increased susceptibility to
Byzantine attacks owing to the lack of a responsible cen-
tral server. Consequently, effective Byzantine-robust DFL
schemes (i.e., defense mechanisms) are highly desired to
attain satisfactory DFL model training performance.
Thus far, researchers have developed diverse defense
mechanisms against Byzantine attacks in CFL [5, 28, 34,
38]. The basic idea of these Byzantine-robust CFL ap-
proaches is that the central server tries to identify and ex-
clude malicious local models from aggregation. However,
their direct application to DFL is impeded due to the ab-
sence of coordination from a central entity. Furthermore,
a benign client in DFL may be overwhelmed by Byzantine
clients (i.e., most of its neighbors are malicious), which ex-
acerbates the difficulty in identifying malicious clients.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24756
While recent studies have focused on the development
of Byzantine-robust DFL schemes [6, 8, 27, 35], it is worth
noting that, to the best of our knowledge, they have not
explicitly accounted for the practical and crucial problem
setting where malicious neighbors may dominate benign
clients, and the data distribution among clients is highly
non-independent and identically distributed (non-IID).
In this work, we propose DFL-Dual, a novel Byzantine-
robust DFL method through dual-domain client clustering
and trust bootstrapping. DFL-Dual employs multiple dis-
tance metrics in both model-domain (cosine similarity and
Euclidean distance) and data-domain (Wasserstein distance)
to distinguish benign clients from Byzantine ones. Hence,
even under a rigorous adversary setting where the data
is highly non-IID and Byzantine clients dominate benign
ones, DFL-Dual remains resilient. Specifically, we first
propose to leverage both model-domain Euclidean distance
and data-domain Wasserstein distance to identify disparities
among clients. Then, we establish a trust evaluation mech-
anism centered on benign clients, leveraging cosine simi-
larities of their local models with those of their neighbors
for assessment. Building upon the dual-domain distance
metrics and trust evaluation mechanism, we further devise a
two-stage clustering and trust bootstrapping technique. The
first stage generates a divergence rate for each client, while
the second stage excludes malicious local models from
model aggregation. The main contributions of this work are
summarized as follows: 1) A Novel Byzantine-robust DFL
Framework : To our best knowledge, DFL-Dual is the first
Byzantine-robust DFL framework that can effectively de-
fend against both untargeted and targeted Byzantine attacks
under a rigorous adversary setting with exceeding 50%
Byzantine clients and highly non-IID data distributions; 2)
Multi Distance Metric Utilization : We leverage multiple
distance metrics in both model-domain and data-domain to
identify disparities among clients. This multi-metric combi-
nation enables accurate discrimination between Byzantine
clients and benign ones; 3) Two-stage Clustering and Trust
Bootstrapping : We design a two-stage clustering and trust
bootstrapping technique. The first stage generates a diver-
gence rate for each client, while the second stage excludes
malicious local models from model aggregation; and 4)
Extensive Performance Evaluation : We thoroughly eval-
uate DFL-Dual through extensive experiments on various
datasets, models, adversary settings, and Byzantine attacks.
The results validate its significant performance superiority
over existing schemes.
2. Preliminaries and Related Work
2.1. Decentralized Federated Learning
Consider a DFL system that consists of a set N=
{1,2, . . . , N }of clients. Each client i∈ N has a privatetraining dataset Dicontaining |Di|data samples and holds
a local model θi. Formally, DFL aims to find a model θthat
minimizes the weighted average of losses among Nclients:
min
θ1
NNX
i=1Fi(θ;Di), (1)
where Fi(θ;Di) =1
|Di|P
ζ∈DiF(θ;ζ)is the local
loss function of client i. DFL usually involves T=
{1,2, . . . , T }rounds. In each round t∈ T, the following
procedures are sequentially executed.
•Local Model Training: Each client isamples a mini-batch
of training samples from its local training dataset and
computes a stochastic gradient gt
i. Then, client iupdates
its local model as
θt+1
2
i=θt
i−ηgt
i, (2)
where ηdenotes the learning rate, θt
irepresents the local
model of client iat the beginning of the t-th global train-
ing round, and θt+1
2
istands for the pre-aggregation local
model of client iin round t.
•Model Exchange and Aggregation: Each client isends its
pre-aggregation local model θt+1
2
ito its connected neigh-
bors and receives their counterparts. Then, each client i
aggregates the received pre-aggregation local models (in-
cluding its own) to update its local model as
θt+1
i(Gi) = Aggn
θt+1
2
k:k∈ Gio
, (3)
where Gidenotes the sub-graph centered on client i(in-
cluding client iand its neighbors), Agg (·)represents the
adopted aggregation rule (e.g., the consensus update rule
in [19]), and the resulting θt+1
iis the post-aggregation
local model of client iin round t.
2.2. Byzantine-Robust CFL and DFL
In CFL, a commonly used aggregation rule is FedAvg [20].
However, the post-aggregation local model of a benign
client can be easily manipulated by a malicious local model
crafted by Byzantine clients in FedAvg [4]. To thwart such
Byzantine attacks and achieve secure model training, re-
searchers have developed various Byzantine-robust aggre-
gation rules [4–6, 9, 27, 33, 36, 37].
For example, Krum [4] aggregates a client’s received
local models by selecting the one with the smallest sum
of Euclidean distances to its subset of neighboring local
models. Median andTrimmed Mean [36] are two ro-
bust aggregation rules based on coordinate-wise statistics.
They compute the coordinate-wise median and trimmed av-
erage as the aggregated value for each model parameter
among all received local models of a client. By employing
FLtrust [5], a benign client can assign a low trust score to
24757
Figure 1. Framework of DFL-Dual.
a neighbor’s pre-aggregation local model if it significantly
deviates from the client’s own pre-aggregation local model .
The recently proposed FLdetector [37] investigated the
defenses from a new perspective, i.e., detecting malicious
clients by checking their model-updates consistency. Thus,
each client can only aggregate shared local models from
neighbors detected as benign. The work in [27] proposed
Bristle , which enables each client to securely update its
model by designing a fast distance-based prioritize and a
novel performance-based integrator.
The work in [8] applied Median ,Trimmed Mean ,
andKrum aggregation rules to DFL. An iterative filtering
rule is designed for DFL in [30], where a benign client re-
peatedly discards the model with the largest Euclidean dis-
tance to the average of its neighbors’ models. Nevertheless,
existing studies leave a notable gap in understanding how
to effectively establish a Byzantine-robust DFL framework
for practical and essential problem settings where malicious
neighbors may overwhelm benign clients and the data dis-
tribution among clients is highly non-IID .
3. Threat Model
•Attacker’s Goal: The attacker aims to send well-crafted
poisoned pre-aggregation local models via compromised
clients to benign clients to disrupt the DFL model train-
ing process. We consider both untargeted attacks (aiming
to ruin the model performance indiscriminately) and tar-
geted attacks (aiming to manipulate the model behavior
on specific attacker-chosen inputs) in this work.
•Attacker’s Capability: We consider a rigorous scenario
where the attacker can compromise over 50% of the en-
tire client population. Moreover, the compromised clients
can strategically cluster around benign clients and dom-
inate them. That is, more than half of a benign client’s
neighbors can be Byzantine clients.
•Attacker’s Background Knowledge: We consider twocases of attacker’s background knowledge (i.e., full
knowledge and partial knowledge). Besides the local
training data and models at compromised clients in the
partial knowledge scenario, the attacker also knows the
pre-aggregation local models on every benign client in
the full knowledge scenario. Note that the full knowledge
scenario has limited applicability in practice as we cannot
ensure any two clients are connected. We use it to evalu-
ate our defensive performance against adaptive attacks.
4. Methodology
4.1. Overview of DFL-Dual
DFL-Dual relies on benign clients to identify and filter out
malicious pre-aggregation local models crafted by compro-
mised neighbors in each training round. Without loss of
generality, we take a benign client iand its connected neigh-
bors (forming a sub-graph Gi) as a concrete example to
illustrate how DFL-Dual works. The framework of DFL-
Dual is presented in Figure 1, and the workflow in each
round of model aggregation at benign client iis as follows:
•Dual-Domain Distance Computation: After receiving all
pre-aggregation local models from its neighbors, the be-
nign client icomputes pairwise Euclidean distances be-
tween any two pre-aggregation local models inGi. Be-
sides, benign client iperforms privacy-respecting model
inversion on all pre-aggregation local models of clients in
Gito synthesize a corresponding dummy dataset for each
client. Then, it computes pairwise Wasserstein distances
among all synthesized dummy datasets. The weighted
sum of these two distances constitutes the dual-domain
distance , which will be used for client clustering.
•Cosine Similarity Computation: Benign client icomputes
the cosine similarity between its own pre-aggregation lo-
cal model and its neighbors’ counterparts, which will be
used to obtain the trust score .
24758
•Two-stage Clustering and Trust Bootstrapping: For each
client j∈ Gi, benign client iclusters remaining clients
Gi\jinto two groups based on their dual-domain dis-
tances to client j. The trust score of each group (defined
as the average cosine similarity of the pre-aggregation lo-
cal models in the group w.r.t client i) bootstraps the group
selection, which allows to determine a divergence rate for
each client j∈ G i. Then, benign client iclusters all
clients in Giinto two groups based on the generated di-
vergence rates, with the trust score of each group boot-
strapping the local model aggregation for client i.
4.2. Dual-Domain Distance Computation
Unlike existing studies (e.g., [4, 5, 8, 30]) that rely on
single-domain distances to detect Byzantine clients, DFL-
Dual utilizes dual-domain distances to enable each benign
client to identify its Byzantine neighbors more accurately.
4.2.1 Model-Domain Distance Computation
Following prior works (e.g., [4, 30]), we employ the Eu-
clidean distance (ED) metric to measure the discrepancies
between benign and Byzantine clients in the model domain.
Formally, the Euclidean distance Eij(θi,θj)between two
local models θiandθjis computed as
Eij(θi,θj) =∥θi−θj∥2, (4)
where ∥ · ∥ 2denotes the ℓ2-norm of a vector. Generally, a
larger Euclidean distance means greater discrepancy.
However, the Euclidean distance metric suffers from the
curse of dimensionality [11]. Specifically, deep models can
be viewed as high-dimensional vectors, and usually, the Eu-
clidean distance is unable to distinguish poisoned models
from benign ones in high-dimensional space. Hence, we
further introduce the data-domain distance metric below.
4.2.2 Data-Domain Distance Computation
In DFL, no client has access to others’ private training data.
Consequently, a natural question arises: how can we ob-
tain data-domain distances to reveal the disparities between
Byzantine and benign clients? To answer this question, we
introduce a privacy-respecting model inversion method to
obtain a dummy dataset for each client.
1) Privacy-Respecting Model Inversion . Inspired by
Deep Leakage from Gradients ( DLG) [40] that infers private
training data of clients from their shared gradients in FL,
we introduce a privacy-respecting model inversion method
to obtain a dummy dataset for each client. The basic idea
ofDLG is randomly generating dummy data samples and it-
eratively updating them by matching the dummy gradients
derived from dummy data with clients’ shared actual gradi-
ents. However, to avoid privacy leakage of clients as in DLGand highlight the discrepancies among different clients’ un-
derlying data distributions, we make the following three-
fold adaptations to DLG.
• First, we allow each client ito perform Eepochs of
local training via mini-batch SGD with a batch size of
B. Then, each client shares the pre-aggregation lo-
cal model rather than raw gradient to its neighbors
along with the epoch number Eand mini-batch size
B. In this way, client ishares an equivalent gradient
θt+1/2
i−θt
i
/(E|Di|/B)in each training round t.
• Second, we introduce a scaling factor s(t)(i.e.,sto the
power of t) in each round tto amplify the differences
among the equivalent gradients of different clients, espe-
cially in later training rounds where the equivalent gradi-
ents start to cancel out (approaching 0).
• Third, unlike DLG that optimizes both feature x′
iand label
y′
iof each dummy data sample, we propose to optimize
onlyx′
i, while y′
iis sampled uniformly at random from
all possible labels of the dataset and fixed.
Therefore, the dummy dataset for client iin round tis
generated by solving the following optimization problem:
x′∗
i=arg min
x′
i∥∂ℓ 
(x′
i, y′
i) ;θt
i
∂θt
i−s(t)
θt+1/2
i−θt
i
E|Di|/B∥2
2,
(5)
where x′
iis the dummy feature to be optimized and ℓ(·)is
the loss function. We will show that the optimal solution x′∗
i
is close to the original data feature xifrom the perspective
of distribution (without disclosing pixel-level private infor-
mation), thus enabling each benign client to assess its neigh-
bors in the data domain.
2) Wasserstein Distance Determination . We use the
Wasserstein distance (WD) [29] between generated dummy
datasets to capture the data-domain divergences among
clients. The Wasserstein distance between any two dummy
datasets (x′
i, y′
i)and 
x′
j, y′
j
of clients iandjis given as
Wij 
x′
i, x′
j
=lX
c=1mX
d=1Wass
x′,c,d,
i, x′,c,d,
j
, (6)
where Wass (·,·)is the WD between any two vectors,
x′,c,d,
i andx′,c,d,
j denotes the vector of all samples with fea-
turedand label cin the dummy datasets of clients iandj,
respectively, and landmare the total number of labels and
features of generated dummy datasets, respectively.
4.2.3 Dual-domain Distance Calculation
After receiving the pre-aggregation local models from its
neighbors, benign client iclips the weighted sum of Eu-
clidean distance and Wasserstein distance of any two clients
24759
in the sub-graph Gito obtain the pairwise dual-domain dis-
tance as follows:
Dij=min(Wij+αEij, C1),∀(i, j)∈ Gi, (7)
where Dijis the dual-domain distance between clients iand
j, with αandC1being tunable empirical parameters.
4.3. Trust Score Determination
To facilitate the accurate identification of Byzantine neigh-
boring clients for benign clients, we further introduce the
cosine similarity (CS) distance metric. It is a dimension-
less metric with values falling within [−1,1], which helps
achieve fair and robust evaluation of models under different
attacks and environments. Formally, the cosine similarity
between two models θiandθjis computed as
Sij(θi,θj) =⟨θi,θj⟩
∥θi∥2· ∥θj∥2. (8)
Clearly, a smaller cosine similarity value Sij(θi,θj)means
the two models deviate from each other more significantly.
Then, we introduce the trust score (TS) of a client group
from a benign client’s perspective. Specifically, we define
the trust score of a client group as the average cosine sim-
ilarity values of the corresponding pre-aggregation local
models w.r.t. that of a benign client. Mathematically, the
trust score of benign client ito one group Qof its neigh-
bors is computed as
TSi(Q) =1
|Q|X
k∈QSik. (9)
The trust score will bootstrap the selection of a benign
group of neighbors for benign clients, as elaborated below.
4.4. Two-stage Clustering and Trust Bootstrapping
Large divergences in models and data among clients are
common in DFL, especially when the data distribution
among clients is highly non-IID. Hence, instead of sim-
ply rejecting pre-aggregation local models with large diver-
gences, we propose a two-stage clustering and trust boot-
strapping (TB) mechanism, whose workflow is as follows:
•Stage 1 : Dual-domain Distance-based Clustering and
Trust Bootstrapping. For each client j∈ Gi, benign client
iclusters remaining clients Gi\jinto two groups Mj1
andMj2based on their dual-domain distances to client
j. That is,
Mj1, Mj2=2-Median ({Dkj, k∈ Gi\j}). (10)
Then, benign client ibootstraps the selection of the group
M∗
jfor client jwith a higher trust score, which is
M∗
j=Mj1, TS i(Mj1)> TS i(Mj2),
Mj2, otherwise .(11)Algorithm 1: DFL-Dual
1Inputs: Client number N, communication topology
G, global training rounds T, Clipping parameters
C1andC2.
2Outputs: Local models θT
ifor each client i∈ N .
3Initialization: Local models θ0
ifor each client
i∈ N .
4fort∈ {1,2, . . . , T }do
5 forbenign client i∈ N in parallel do
6 θt+1/2
i←Local update by (2).
7 end
8 forbenign client i∈ N in parallel do
9 Ekj,∀(k, j)∈ Gi←Compute ED by (4).
10 Wkj,∀(k, j)∈ Gi←Compute WD by (6).
11 Dkj,∀(k, j)∈ Gi←Compute dual-domain
distance by (7).
12 Sij,∀j∈ Gi←Compute CS by (8).
13 Mj1, Mj2, j∈ Gi←Clustering by (10).
14 M∗
j, j∈ Gi←Bootstraps selection by (11).
15 rj, j∈ Gi←Obtain divergence rate by (12)
and (13).
16 Ni1, Ni2, j∈ Gi←Clustering by (14).
17 N∗
i, j∈ Gi←Bootstraps selection by (15).
18 θt+1
i(N∗
i)←Model aggregation by (3).
19 end
20end
Furthermore, benign client icomputes the divergence rate
rjfor client jbased on the selected groups M∗
jandM∗
i
as follows:
rj=min(qj/qi, C2) (12)
where
qj=X
k∈M∗
jDjk, qi=X
k∈M∗
iDik, (13)
andC2is a tunable empirical parameter.
•Stage 2 : Divergence Rate-based Clustering and Trust
Bootstrapping. Based on the divergence rates of client
j∈ Gi, benign client ifirst clusters all clients j∈ Giinto
two groups Ni1andNi2, i.e.,
Ni1, Ni2=2-Median ({rj, j∈ Gi}). (14)
Benign client ithen bootstraps the selection of the group
N∗
iwith a higher trust score as follows:
N∗
i=Ni1, TS i(Ni1)> TS i(Ni2),
Ni2, otherwise .(15)
Thepre-aggregation local models in the finally selected
group N∗
iare aggregated to obtain the post-aggregation lo-
24760
0 5 10 15
Number of Global Iterations020406080100Accuracy(%)DFL-LR
DFL-FC
DFL-CNN
DFL-Dual-LR
DFL-Dual-FC
DFL-Dual-CNN(a) MNIST
0 5 10 15
Number of Global Iterations020406080100Accuracy(%)DFL-LR
DFL-FC
DFL-CNN
DFL-Dual-LR
DFL-Dual-FC
DFL-Dual-CNN (b) Fashion-MNIST
Figure 2. The performance comparison between DFL and DFL-
Dual without Byzantine attacks.
cal model for client i. The details of the proposed DFL-Dual
method are summarized in Algorithm 1.
5. Experiments
5.1. Experimental Setup
Taking Figure 1 (a) as an example of the decentralized com-
munication topology, we evaluate DFL-Dual on different
datasets and various models with two performance metrics
of Accuracy (ACC) and Attack Success Rate (ASR). Specif-
ically, we evaluate DFL-Dual on MNIST [14] and Fashion-
MNIST [31] using Logistic Regression (LR), Fully Con-
nected (FC), and Convolutional Neural Network (CNN),
and on CIFAR-10 [13] using ResNet-18. We adopt the same
method in [5, 38] to simulate different non-IID data distri-
bution degrees. Specifically, the non-IID degree is captured
by a sample allocation probability p, with larger pindicat-
ing a higher non-IID degree. We consider both untargeted
and backdoor attacks. The untargeted attacks include Label
Flipping Attack, Krum Attack [9], and Back-Gradient At-
tack [21], while the targeted attacks include Scaling Attack
[1], DBA Attack [32], and A little is Enough Attack [2]. We
take6aggregation methods (i.e., DFL [19], DFLTrust [5],
DFLDetector [37], Multi-Krum [4], BridgeM [8], and
IOS [30]) as baselines. Notably, for those designed for CFL,
we trim them to fit in the DFL scenario . All experiments
are conducted using PyTorch 2.0on a machine with 2RTX
4090 GPUs. The detailed experimental settings and param-
eters are provided in the supplementary material.
5.2. Convergence Performance of DFL-Dual
We first consider an ideal case that all 10clients in Figure 1
(a) are benign with the non-IID degree being 0.8. Figure 2
shows the model performance via DFL-Dual and vanilla
DFL, and we find DFL-Dual converges as nicely as vanilla
DFL when no Byzantine attacks happen.
5.3. Privacy-respecting Property of DFL-Dual
In the model inversion process, DFL-Dual generates a
dummy dataset (with 10 samples for MNIST and Fashion-
MNIST, and 5 samples for CIFAR10, for each class) based
Original
ResNet18
CNNOriginalOriginal
FC
Figure 3. Illustration of original and dummy data samples.
on a client’s pre-aggregation local model . Figure 3 illus-
trates the original images and the generated dummy sam-
ples (images) by the model inversion process in DFL-Dual.
We find from this figure that it is nearly impossible to in-
fer any private information from the generated dummy data
samples, and thus verifies the privacy-respecting property
of DFL-Dual.
5.4. Defense against Untargeted Attacks
The averaged accuracy of different models (CNN, FC, and
LR) trained on various datasets using different aggregation
methods is shown in Table 1. It is seen from the table that
DFL-Dual consistently exhibits the highest accuracy under
different untargeted attacks on almost all of the training
tasks compared to other baselines. This verifies the effec-
tiveness and robustness of the proposed DFL-Dual method.
DefSrc MNIST Fashion CIFAR10
CNN LR CNN FC ResNet18
DFL (No Attack) 95.39 89.84 84.85 82.67 49.96 Label
FlippingDFLTrust 18.28 1.11 12.8 53.21 10
DFLDetector 33.84 89.9 84.06 36.36 29.58
Multi-Krum 36.45 89.83 84.37 60.4 25.09
DFL 26.05 15.40 31.78 20.85 25.3
BridgeM 50.31 44.76 61.12 66.17 34.89
IOS 0.24 0.95 0.51 0.57 20.59
DFL-Dual 96.64 88.97 83.98 82.03 49.06KrumDFLTrust 20.01 1.11 14.76 64.33 10
DFLDetector 22.01 17.66 32.91 31.5 22.08
Multi-Krum 30.35 24.01 27.42 32.08 10
DFL 71.14 71.88 49.76 58.88 10
BridgeM 26.12 42.44 27.66 37.91 10
IOS 77.08 77.59 50.44 68.11 10
DFL-Dual 96.14 89.05 83.69 81.91 49.84Back-
GradientDFLTrust 9.8 9.8 10 10 10
DFLDetector 9.8 14.74 10 11.75 10
Multi-Krum 9.8 15.61 10 10.17 11.72
DFL 25.81 56.05 19.41 27.39 10.03
BridgeM 22.17 47.72 28.23 35.65 15.70
IOS 10.52 42.17 12.33 34.59 19.29
DFL-Dual 95.14 88.99 83.73 81.99 49.1
Table 1. Accuracies (%) under Untargeted Attacks.
5.5. Defense against Targeted Attacks
The averaged accuracy and ASR of different models (CNN,
FC, and LR) trained on various datasets using different ag-
24761
gregation methods are shown in Table 2. The results val-
idate that DFL-Dual consistently exhibits higher accuracy
on benign testing data and lower ASR on testing data with
backdoor triggers than other baselines.
DefenceSource MNIST Fashion CIFAR10
CNN CNN ResNet18
DFL (No Attack) 95.39 84.85 49.96
DFLTrust9.8/
10010/
10019.45/
100
DFLDetector67.06/
99.7569/
91.9420.85/
85.66
Multi-Krum96.92/
99.9998.44/
2.5531.62/
53.49
DFL49.61/
10061.78/
98.9118.7/
79.14Scaling
BridgeM72.02/
99.9657.3/
98.3426.23/
65.86
IOS11.01/
97.4381.74/
91.8530.78/
53.64
DFL-Dual96.21/
0.5084.83/
1.7049.01/
4.44
DFLTrust9.8/
10010/
10018.64/
82.27
DFLDetector34.06/
70.4684.97/
4.2117.53/
82.59
Multi-Krum96.89/
0.4384.93/
3.3426.28/
58.76
DFL9.8/
10010/
10017.87/
87.49DBA
BridgeM22.17/
10028.23/
91.6915.7/
80.08
IOS97.04/
0.2982.13/
1.9125.87/
62.34
DFL-Dual96.54/
0.4883.38/
2.5348.79/
4.35
DFLTrust92.29/
99.7580.1/
98.9133.59/
100
DFLDetector92.34/
8.7483.08/
14.7436.94/
100
Multi-Krum95.25/
0.7284.38/
7.4238.62/
97.64A Little is EnoughDFL95.01/
85.3581.55/
86.2545.66/
99.67
BridgeM95.66/
5.6083.27/
28.7448.44/
89.38
IOS96.95/
0.5384.31/
5.945.05/
89.82
DFL-Dual95.59/
0.5583.88/
2.1650.01/
4.78
Table 2. Accuracies/ASRs (%) under Targeted Attacks.
5.6. Impact of Adversary Parameters
To further assess the effectiveness of DFL-Dual, we sys-
tematically compare its defensive performance on various
configurations, including different percentages of Byzan-
tine clients and various degrees of non-IID data distribution.
We take the scaling attack as an example for the above com-
parison study, with the default Byzantine percentage and
non-IID degree being 60% and0.8, respectively.
5.6.1 ASR versus Byzantine Percentage
Figure 4 depicts the ASRs for the scaling attack across
a spectrum of Byzantine client percentages, ranging from
20 40 60 80
Byzantine Percentage(%)020406080100Attack Success Rate(%)DFL-Dual
Multi-Krum
DFLTrust
DFLDetector
BridgeM
IOS
DFL(a) MNIST
20 40 60 80
Byzantine Percentage(%)020406080100Attack Success Rate(%)DFL-Dual
Multi-Krum
DFLTrust
DFLDetector
BridgeM
IOS
DFL (b) Fashion-MNIST
Figure 4. ASR versus Byzantine client percentage.
0.1(IID) 0.2 0.4 0.6 0.8
Non-IID Degree020406080100Attack Success Rate(%)DFL-Dual
Multi-Krum
DFLTrust
DFLDetector
BridgeM
IOS
DFL
(a) MNIST.
0.1(IID) 0.2 0.4 0.6 0.8
Non-IID Degree020406080100Attack Success Rate(%)DFL-Dual
Multi-Krum
DFLTrust
DFLDetector
BridgeM
IOS
DFL (b) Fashion-MNIST.
Figure 5. ASR versus non-IID degree.
20% to80% (their topologies are in supplementary due to
page limitation). It is evident that the ASRs of baseline
schemes exhibit a clear upward trend, which reveals their
inadequacy in effectively identifying and excluding a rela-
tively substantial number of Byzantine clients. In contrast,
DFL-Dual consistently maintains a low ASR, even when
80% of clients are compromised (note that the remaining
two benign clients are connected in this extreme scenario).
This resilient performance highlights DFL-Dual’s superior
capability in navigating adversarial environments character-
ized by a multitude of malicious clients.
5.6.2 ASR versus Non-IID Degree
Figure 5 illustrates the relationship between the ASRs
against the considered defense schemes and the degree of
non-IID data distribution (as indicated by the probability
value p). The results reveal that, for baseline methods,
the ASR increases with a higher degree of non-IID. This
is attributed to the amplified divergences between benign
local model updates, making it challenging to distinguish
whether the outlying local model updates stem from Byzan-
tine attacks or non-IID data distribution. Surprisingly, the
ASR remains low under DFL-Dual, even when p= 0.8. In
all cases, our proposed DFL-Dual consistently outperforms
the baselines, achieving a lower ASR.
5.7. Ablation Study
To comprehensively assess the significance of considering
both model-domain and data-domain distances in cluster-
ing clients and incorporating the trust bootstrapping mecha-
nism for guiding cluster selection to identify malicious local
24762
DefAtk Scaling A Little is Enough DBA
MNIST Fashion CIFAR10 MNIST Fashion CIFAR10 MNIST Fashion CIFAR10
w/o ED 96.82/0.36 85.16/1.49 36.85/31.61 93.16/1.07 85.65/57.12 50.01/4.29 37.21/18.99 62.14/48.91 41.33/9.01
w/o WD 86.49/0.48 14.73/21.02 49.69/4.59 95.05/95.76 79.84/56.93 50.75/3.83 12.38/52.92 54.56/47.84 49.64/3.76
w/o TB 75.60/95.23 30.99/46.31 49.74/4.67 96.62/31.15 84.42/42.68 50.43/4.08 50.97/74.23 69.40/49.19 47.78/4.42
DFL-Dual 96.21/0.50 84.83/1.70 49.01/4.44 95.59/0.55 83.88/2.16 50.01/4.78 96.54/0.48 83.38/2.53 48.79/4.35
Table 3. Ablation Study on Accuracies/ASRs (%) under Targeted Attacks.
models, we conduct ablation studies on our proposed DFL-
Dual framework. We examine three variants:
• DFL-Dual-w/o-ED, where only data-domain WD are em-
ployed for client clustering in the first stage;
• DFL-Dual-w/o-WD, where only model-domain ED are
utilized for client clustering in the first stage;
• DFL-Dual-w/o-TB, omitting the trust bootstrapping
mechanism for cluster selection. Instead, the cluster with
a lower average dual-domain distance is selected.
Table 3 presents the accuracies and ASRs of DFL-Dual and
its variants against three targeted attacks. DFL-Dual con-
sistently exhibits high accuracies and low ASRs against the
evaluated targeted attacks. In contrast, each of the three
variants fails to defend against at least one targeted attack.
Thus, our findings affirm the efficacy of each technical de-
sign individually, emphasizing that their combination yields
a more robust defense against adversarial scenarios.
5.8. Defense against Adaptive Attacks
Finally, we consider a more practical and rigorous adver-
sarial scenario where each Byzantine client has access to all
benign clients’ pre-aggregation local models in each train-
ing round and knows the adopted distance metrics in DFL-
Dual. Hence, they can conduct adaptive attacks. In this
work, we formulate an adaptive attack by adding a regular-
ization term to the loss function of Byzantine clients, which
enables them to launch stealthy attacks from the perspec-
tives of the three distance metrics (i.e., ED, CS, and WD).
Specifically, we modify the loss function of each Byzantine
client kin round tas:
min
θt
kβLk+(1−β)
E(ˆθt,θt
k) +S(ˆθt,θt
k) +E(ˆgt, gt
k)
,
(16)
where Lkis the original loss of Byzantine client k,βis the
adaptive factor to balance attack strength and stealthiness.
E
ˆθt,θt
k
andS
ˆθt,θt
k
are the ED and CS between the
average of all benign clients’ local models ˆθtand the ma-
licious model θt
k, and E(ˆgt, gt
k)is the ED between the av-
erage of estimated benign gradient ˆgt≈ˆθt−ˆθt+1/2and
the Byzantine gradient gt
k. Based on (5), we use E(ˆgt, gt
k)
to regularize WD between the corresponding generated
dummy datasets indirectly. Given its three adopted met-
rics (ED, CS, and WD), these three terms are incorporated
to bypass DFL-Dual. Figure 6 shows the accuracy and ASR
0 0.2 0.4 0.6 0.8 1
Adaptive Rate 0204060Accuracy(%)/ASR(%)Scaling-ACC
Scaling-ASR
Little-ACC
Little-ASR
DBA-ACC
DBA-ASRFigure 6. Accuracy/ASR versus adaptive rate.
of DFL-Dual on CIFAR- 10versus adaptive rate βof three
adaptive targeted attacks, where we can find a consistent
solid performance. This further verifies the robustness and
effectiveness of the proposed DFL-Dual method.
6. Conclusion
This paper presented DFL-Dual, a novel Byzantine-robust
DFL framework through dual-domain client clustering and
trust bootstrapping. DFL-Dual leverages multiple distance
metrics in the model domain (cosine similarity and Eu-
clidean distance) and the data domain (Wasserstein dis-
tance) to identify client disparities. This multi-metric com-
bination enables accurate discrimination between Byzan-
tine and benign clients, even under a rigorous adversary
setting with highly non-IID data distribution and exceed-
ing50% Byzantine clients dominating both the entire client
population and a benign client’s neighbors. We conduct an
extensive experimental evaluation of DFL-Dual. The results
validate its superior defensive performance against untar-
geted and targeted Byzantine attacks over existing schemes.
Acknowledgements
This work was supported by the National Natural Science
Foundation of China (Grants No. 62102337, 62203309,
62122066, and U20A20182), Young Elite Scientists Spon-
sorship Program by CAST (Grant No. 2023QNRC001), the
Natural Science Foundation of Hunan Province (Grant No.
2023JJ40174), Shenzhen Science and Technology Program
(Grant No. RCBS20221008093312031), the Fundamental
Research Funds for the Central Universities, and Shenzhen
Institute of Artificial Intelligence and Robotics for Soci-
ety.
24763
References
[1] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah
Estrin, and Vitaly Shmatikov. How to backdoor federated
learning. In International conference on artificial intelli-
gence and statistics , pages 2938–2948. PMLR, 2020. 6
[2] Gilad Baruch, Moran Baruch, and Yoav Goldberg. A lit-
tle is enough: Circumventing defenses for distributed learn-
ing. Advances in Neural Information Processing Systems ,
32, 2019. 6
[3] Enrique Tom ´as Mart ´ınez Beltr ´an, Mario Quiles P ´erez, Pe-
dro Miguel S ´anchez S ´anchez, Sergio L ´opez Bernal, G ´erˆome
Bovet, Manuel Gil P ´erez, Gregorio Mart ´ınez P ´erez, and
Alberto Huertas Celdr ´an. Decentralized federated learn-
ing: Fundamentals, state-of-the-art, frameworks, trends, and
challenges. arXiv preprint arXiv:2211.08413 , 2022. 1
[4] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui,
and Julien Stainer. Machine learning with adversaries:
Byzantine tolerant gradient descent. Advances in neural in-
formation processing systems , 30, 2017. 2, 4, 6
[5] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang
Gong. FLtrust: Byzantine-robust federated learning via trust
bootstrapping. In NDSS , 2020. 1, 2, 4, 6
[6] Jin-Hua Chen, Min-Rong Chen, Guo-Qiang Zeng, and Jia-Si
Weng. Bdfl: A byzantine-fault-tolerance decentralized fed-
erated learning method for autonomous vehicle. IEEE Trans-
actions on Vehicular Technology , 70(9):8639–8652, 2021. 2
[7] Jian-hui Duan, Wenzhong Li, Derun Zou, Ruichen Li, and
Sanglu Lu. Federated learning with data-agnostic distribu-
tion fusion. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8074–
8083, 2023. 1
[8] Cheng Fang, Zhixiong Yang, and Waheed U Bajwa. Bridge:
Byzantine-resilient decentralized gradient descent. IEEE
Transactions on Signal and Information Processing over
Networks , 8:610–626, 2022. 2, 3, 4, 6
[9] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong.
Local model poisoning attacks to {Byzantine-Robust }feder-
ated learning. In 29th USENIX security symposium (USENIX
Security 20) , pages 1605–1622, 2020. 1, 2, 6
[10] Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Mu-
rali Annavaram, and Salman Avestimehr. Spreadgnn: De-
centralized multi-task federated learning for graph neural
networks on molecular data. In Proceedings of the AAAI
Conference on Artificial Intelligence , pages 6865–6873,
2022. 1
[11] Siquan Huang, Yijiang Li, Chong Chen, Leyu Shi, and Ying
Gao. Multi-metrics adaptively identifies backdoors in feder-
ated learning. arXiv preprint arXiv:2303.06601 , 2023. 4
[12] Shivam Kalra, Junfeng Wen, Jesse C Cresswell, Maksims
V olkovs, and HR Tizhoosh. Decentralized federated learning
through proxy model sharing. Nature Communications , 14
(1):2899, 2023. 1
[13] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10
(canadian institute for advanced research). URL http://www.
cs. toronto. edu/kriz/cifar. html , 5(4):1, 2010. 6
[14] Yann LeCun, Corinna Cortes, and Chris Burges. Mnist hand-written digit database, 1998. URL http://www. research. att.
com/yann/ocr/mnist , 7, 1998. 6
[15] Zhuohang Li, Jiaxin Zhang, Luyang Liu, and Jian Liu. Au-
diting privacy defenses in federated learning via generative
gradient leakage. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
10132–10142, 2022. 1
[16] Frank Po-Chen Lin, Seyyedali Hosseinalipour,
Sheikh Shams Azam, Christopher G Brinton, and Ni-
colo Michelusi. Semi-decentralized federated learning with
cooperative d2d local model aggregations. IEEE Journal
on Selected Areas in Communications , 39(12):3851–3869,
2021. 1
[17] Bo Liu and Zhengtao Ding. Distributed heuristic adaptive
neural networks with variance reduction in switching graphs.
IEEE Transactions on Cybernetics , 51(7):3836–3844, 2021.
1
[18] Bo Liu and Zhengtao Ding. A consensus-based decentralized
training algorithm for deep neural networks with communi-
cation compression. Neurocomputing , 440:287–296, 2021.
1
[19] Bo Liu, Zhengtao Ding, and Chen Lv. Distributed training
for multi-layer neural networks by consensus. IEEE Trans-
actions on Neural Networks and Learning Systems , 31(5):
1771–1778, 2020. 2, 6
[20] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
efficient learning of deep networks from decentralized data.
InArtificial Intelligence and Statistics , pages 1273–1282.
PMLR, 2017. 1, 2
[21] Luis Mu ˜noz-Gonz ´alez, Battista Biggio, Ambra Demontis,
Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and
Fabio Roli. Towards poisoning of deep learning algorithms
with back-gradient optimization. In Proceedings of the 10th
ACM workshop on artificial intelligence and security , pages
27–38, 2017. 6
[22] Yuben Qu, Haipeng Dai, Yan Zhuang, Jiafa Chen, Chao
Dong, Fan Wu, and Song Guo. Decentralized federated
learning for uav networks: Architecture, challenges, and op-
portunities. IEEE Network , 35(6):156–162, 2021. 1
[23] Virat Shejwalkar and Amir Houmansadr. Manipulating the
byzantine: Optimizing model poisoning attacks and defenses
for federated learning. In NDSS , 2021. 1
[24] Yifan Shi, Yingqi Liu, Kang Wei, Li Shen, Xueqian Wang,
and Dacheng Tao. Make landscape flatter in differentially
private federated learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 24552–24562, 2023. 1
[25] Zhenheng Tang, Shaohuai Shi, Bo Li, and Xiaowen Chu.
Gossipfl: A decentralized federated learning framework with
sparsified and adaptive communication. IEEE Transactions
on Parallel and Distributed Systems , 34(3):909–922, 2022.
1
[26] Nurbek Tastan and Karthik Nandakumar. Capride learn-
ing: Confidential and private decentralized learning based
on encryption-friendly distillation loss. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8084–8092, 2023. 1
24764
[27] Joost Verbraeken, Martijn de V os, and Johan Pouwelse. Bris-
tle: Decentralized federated learning in byzantine, non-iid
environments. arXiv preprint arXiv:2110.11006 , 2021. 2, 3
[28] Wei Wan, Shengshan Hu, Minghui Li, Jianrong Lu, Longling
Zhang, Leo Yu Zhang, and Hai Jin. A four-pronged de-
fense against byzantine attacks in federated learning. arXiv
preprint arXiv:2308.03331 , 2023. 1
[29] Lilian Weng. From gan to wgan. arXiv preprint
arXiv:1904.08994 , 2019. 4
[30] Zhaoxian Wu, Tianyi Chen, and Qing Ling. Byzantine-
resilient decentralized stochastic optimization with robust
aggregation rules. IEEE Transactions on Signal Processing ,
2023. 3, 4, 6
[31] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
6
[32] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Dis-
tributed backdoor attacks against federated learning. In In-
ternational conference on learning representations , 2019. 6
[33] Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Dis-
tributed stochastic gradient descent with suspicion-based
fault-tolerance. In International Conference on Machine
Learning , pages 6893–6901. PMLR, 2019. 2
[34] Gang Yan, Hao Wang, Xu Yuan, and Jian Li. Defl: De-
fending against model poisoning attacks in federated learn-
ing via critical learning periods awareness. In Proceedings of
the AAAI Conference on Artificial Intelligence , pages 10711–
10719, 2023. 1
[35] Hao Ye, Le Liang, and Geoffrey Ye Li. Decentralized fed-
erated learning with unreliable communications. IEEE Jour-
nal of Selected Topics in Signal Processing , 16(3):487–500,
2022. 1, 2
[36] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter
Bartlett. Byzantine-robust distributed learning: Towards op-
timal statistical rates. In International Conference on Ma-
chine Learning , pages 5650–5659. PMLR, 2018. 2
[37] Zaixi Zhang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang
Gong. Fldetector: Defending federated learning against
model poisoning attacks via detecting malicious clients.
InProceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining , pages 2545–2555,
2022. 2, 3, 6
[38] Bo Zhao, Peng Sun, Tao Wang, and Keyu Jiang. Fedinv:
Byzantine-robust federated learning by inversing local model
updates. In Proceedings of the AAAI Conference on Artificial
Intelligence , pages 9171–9179, 2022. 1, 6
[39] Joshua C Zhao, Ahmed Roushdy Elkordy, Atul Sharma,
Yahya H Ezzeldin, Salman Avestimehr, and Saurabh Bagchi.
The resource problem of using linear layer leakage attack in
federated learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
3974–3983, 2023. 1
[40] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from
gradients. Advances in Neural Information Processing Sys-
tems, 32, 2019. 4
24765
