Data-Efficient Multimodal Fusion on a Single GPU
No¨el V ouitsis* Zhaoyan Liu* Satya Krishna Gorti* Valentin Villecroze
Jesse C. Cresswell Guangwei Yu Gabriel Loaiza-Ganem Maksims V olkovs
Layer 6 AI
{noel, zhaoyan, satya, valentin.v, jesse, guang, gabriel, maks }@layer6.ai
Abstract
The goal of multimodal alignment is to learn a single la-
tent space that is shared between multimodal inputs. The
most powerful models in this space have been trained us-
ing massive datasets of paired inputs and large-scale com-
putational resources, making them prohibitively expensive
to train in many practical scenarios. We surmise that ex-
isting unimodal encoders pre-trained on large amounts of
unimodal data should provide an effective bootstrap to cre-
ate multimodal models from unimodal ones at much lower
costs. We therefore propose FuseMix, a multimodal aug-
mentation scheme that operates on the latent spaces of ar-
bitrary pre-trained unimodal encoders. Using FuseMix for
multimodal alignment, we achieve competitive performance
– and in certain cases outperform state-of-the art methods
– in both image-text and audio-text retrieval, with orders
of magnitude less compute and data: for example, we out-
perform CLIP on the Flickr30K text-to-image retrieval task
with∼600×fewer GPU days and ∼80×fewer image-text
pairs. Additionally, we show how our method can be ap-
plied to convert pre-trained text-to-image generative models
into audio-to-image ones. Code is available at: https:
//github.com/layer6ai-labs/fusemix .
1. Introduction
Recent advances in multimodal machine learning have un-
locked unprecedented capabilities across a wide array of
understanding-based [46, 47] and generation-based [22,
45, 48, 53] applications, some of which have even gar-
nered mainstream attention [1, 69, 70, 99]. Of particu-
lar interest to us for this work is multimodal alignment
[27, 29, 35], which we alternatively refer to as multimodal
fusion, wherein the goal is to learn a single latent space
that is shared between inputs of various modalities. Recent
successes in multimodal fusion have been largely driven
*Authors contributed equally to this work.106107108109101050607080
(FuseMix, 500K)(FuseMix, 1.5M)(FuseMix, 2.5M)(FuseMix, 5M)(FILIP, 300M)
(CLIP, 400M)(ALIGN, 1B)
(LiT, 4B)(3T, 5B)
# of training image-text pairsRecall@1
Figure 1. Text-to-image retrieval performance as a function of the
number of image-text pairs used during training, evaluated on the
Flickr30K test set [101]. Note the x-axis is in log-scale.
by large-scale training regimes requiring many GPUs, and
often relying on datasets of billions of multimodal pairs
[39, 67, 102]. This presents a cost that is unacceptable for
many practical scenarios where access to compute is limited
and where multimodal data is scarce [55, 88]. It is thus of
paramount importance to design efficient frameworks that
can democratize research in multimodal fusion.
In this work, our key insight is that off-the-shelf uni-
modal encoders that have been pre-trained on large amounts
of unimodal data already encode rich semantics that should
provide an effective bootstrap for multimodal fusion. We in-
troduce FuseMix, a simple and easy-to-implement data aug-
mentation scheme for multimodal fusion inspired by mixup
[106], where we share the mixing coefficient across modal-
ities. We show that by aligning the latent spaces of existing
pre-trained unimodal encoders using FuseMix, we obtain
highly competitive fused multimodal models, which in cer-
tain cases even outperform state-of-the-art methods in both
image-text and audio-text retrieval tasks, all while using or-
ders of magnitude less compute and data. For example, we
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27239
use∼600×less compute ( ∼51vs.∼30002GPU days)
and∼80×less image-text pairs ( ∼5M vs. ∼400M) than
CLIP [67] to perform multimodal fusion, yet are still able
to outperform it in recall for the text-to-image retrieval task
on the Flickr30K test set [101], see Figure 1. Moreover, in
settings with access to limited multimodal pairs, we show
that dataset quality and diversity are important properties
to increase downstream performance. Finally, we further
demonstrate the applicability of our FuseMix fusion frame-
work for audio-to-image generation [27].
2. Related Work
Multimodal Learning. The overarching objective of mul-
timodal learning is to build universal models that can jointly
perceive data of various modalities [7, 26, 49, 50, 79, 92, 93,
95, 103, 108]. Said modalities can range from data streams
including but not limited to image, text, audio, and video.
A standard approach to building multimodal models is to
train them end-to-end on data paired across all modalities
of interest [3, 15, 46, 47, 56, 76, 78]. However, this ap-
proach generally does not scale well since training large-
scale multimodal models from scratch can quickly become
very compute and data intensive. A more practical approach
is to instead bootstrap from pre-trained unimodal networks.
Yet, several works in this vein still perform backpropagation
through the pre-trained networks [1, 14, 22, 48, 53, 60, 85],
which incurs significant overhead due to the large size of
the underlying unimodal networks; this problem is bound
to be exacerbated as the size of networks increases.
More related to our setting are multimodal models that
focus on learning a single shared latent space wherein multi-
ple modalities can be jointly encoded (i.e. multimodal align-
ment). This line of work was pioneered by CLIP [67] and
ALIGN [35], which use a dual-encoder architecture trained
with a contrastive objective to jointly embed texts and im-
ages. CoCa [102] adds an autoregressive image captioning
term to the contrastive objective, which they find improves
performance. 3T [39] instead aligns the text and image en-
coders with the latent space of a pre-trained classifier. LiT
[105] uses a frozen pre-trained image classifier as the image
encoder, and aligns a text encoder with it. Despite their suc-
cesses, all of these works train one or both encoders from
scratch, requiring expensive gradient computations span-
ning many GPUs. They also use internet-scale datasets con-
sisting of image-text pairs ranging in quantity from 400M
to 5B pairs, and these datasets are often not made publicly
available. Moreover, several works have extended CLIP
1To pre-compute 5M latent encodings for the pre-trained image and
text encoders in our experiments, we require up to ∼4days, noting that
this is a one-time procedure whose cost can be amortized. Then we need
∼1day to perform FuseMix fusion on the resulting latents, all using 1
V100 GPU, for a total of ≈5GPU days. See Sec. 6 for details.
2CLIP trained for ∼12days on 256V100 GPUs ≈3072 GPU days.to include other modalities such as video [25, 28, 57] and
audio [29], but they require fine-tuning CLIP to achieve
good performance. Similarly, other audio-text fusion meth-
ods [19, 58, 96] require fine-tuning of the underlying en-
coders and additional training data. Finally, ImageBind
[27] learns a shared latent space across six modalities us-
ing a contrastive objective with images as an anchor modal-
ity, which they achieve by jointly training several modality
encoders from scratch. In contrast to all these works, we
prioritize computational and data efficiency by using frozen
pre-trained unimodal encoders, by leveraging minimal mul-
timodal paired data, and by ensuring all our experiments
require no more than a single GPU of compute.
Data Augmentation. Historically, data augmentations
were introduced in an effort to synthetically increase dataset
size and diversity [43, 73]: this is exactly our goal, as we op-
erate in a setting with relatively scarce paired multimodal
data. In the natural image domain, common augmenta-
tions include horizontal flips, random crops, and color jitter
[6, 13], which were designed to leave semantic informa-
tion unchanged. However, designing such augmentations
in any given domain requires expert knowledge of which
transformations preserve semantic information. For exam-
ple, na ¨ıvely applying color jitter on the medical image do-
main can destroy the most relevant information for tasks
like cancer classification [72, 77]. Furthermore, handcrafted
augmentation schemes typically do not readily transfer to
other modalities. This effect is evidenced by the scarcity
of modality-agnostic augmentation schemes, despite re-
cent efforts therein such as random projections [77] and
randomized quantization [94]. We note that while input
masking has been successfully applied in various modali-
ties, expert knowledge is still required to determine an ap-
propriate masking strategy for each modality individually
[21, 32, 34, 84]. Given these challenges, it is unsurprising
that data augmentations are not as well studied for multi-
modal learning [30]. In our work, we propose a multimodal
augmentation scheme that operates on latent space and is
inspired by mixup [106].
3. Problem Setting and Background
3.1. Multimodal Fusion as Alignment
In this work, we define multimodal fusion from the perspec-
tive of alignment. Alignment is the task of learning a single
latent space that is shared between multimodal inputs. For-
mally, given any two data modalities XandY(e.g. images
and texts), we aim to learn two networks, fX:X → S and
fY:Y → S , that embed each respective modality into a
shared latent space S.
Recently, contrastive learning has emerged as a preva-
lent objective for multimodal alignment [35, 46, 67, 105].
It aims to learn a joint latent space wherein semantically
27240
similar multimodal inputs in ambient space are encoded to
nearby points, while semantically dissimilar inputs are em-
bedded further apart. To this end, contrastive learning re-
quires access to semantically similar multimodal inputs in
the form of positive pairs (e.g. images and their correspond-
ing text captions), as well as access to semantically dissim-
ilar negative pairs (e.g. unrelated images and texts). There-
fore, we must assume there is a way to obtain samples of
positive pairs from the joint distribution over modalities X
andYgiven by pX,Y. Negative pairs are often obtained
by sampling from the product of marginal distributions of
each modality, pXandpY.3With access to a positive pair
(x, y)∼pX,Y and negative pairs (x−
i, y−
i)i.i.d.∼pXpYfor
i= 1, . . . , M , contrastive learning in the context of multi-
modal alignment leverages the InfoNCE loss [63]:
L 
fX, fY;x, y,{y−
i}M
i=1
≜ (1)
−logefX(x)·fY(y)/τ
efX(x)·fY(y)/τ+PM
i=1efX(x)·fY(y−
i)/τ,
where a·b≜a⊤b
∥a∥2∥b∥2denotes cosine similarity4andτ >0
is either a fixed or learnable scalar temperature parameter.
The final objective is then given by a symmetric version
[35, 67] of the InfoNCE objective:
Lsym(fX, fY)≜Eh
1
2L 
fX, fY;x, y,{y−
i}M
i=1
(2)
+1
2L 
fY, fX;y, x,{x−
i}M
i=1i
,
where the expectation is taken with respect to the positive
pair(x, y)∼pX,Y and the Mnegative pairs (x−
i, y−
i)i.i.d.∼
pXpY.
We note that formulating alignment through contrastive
learning has been shown to enable zero-shot transfer to var-
ious multimodal downstream tasks [27, 29, 55, 67], and has
also been shown to improve performance in general multi-
modal settings, including understanding-based [46, 47] and
generation-based [16, 45, 48] tasks. This formulation also
admits theoretical motivations from the perspective of mu-
tual information maximization [4, 46, 63, 82].
3.2. Mixup
Mixup [106] is a general-purpose data augmentation rou-
tine for supervised learning. Its premise is simple: given
pairs (x, l)and(ˆx,ˆl)of data (i.e. xandˆx) and their corre-
sponding labels (i.e. landˆl), it constructs augmented sam-
ples by taking the convex combinations ˜x≜λx+ (1−λ)ˆx
3While sampling independently from the marginals could technically
result in a semantically related (positive) pair, the probability of this hap-
pening in practice is extremely small (e.g. a random text describing an im-
age will rarely properly describe a different random image), thus justifying
this procedure to obtain negative pairs.
4We slightly abuse notation here and denote cosine similarity using the
commonly used dot product notation for conciseness.and˜l≜λl+ (1−λ)ˆl, where λ∈(0,1)is an inter-
polation coefficient most commonly sampled from a Beta
distribution B(α, β)with hyperparameters α, β > 0. The
loss used to train the model is then optimized on the aug-
mented data/label pairs rather than the original ones. Sub-
sequent works have motivated mixup from the perspective
of robustness and generalization [107], as well as calibra-
tion [80]. Variations on mixup have extended the method to
contrastive learning where labels are unavailable [87], but
can be created by proxy [44]. Recently, in the context of
multimodal learning, So et al. [74] proposed a mixup strat-
egy using spherical interpolations to fine-tune CLIP, but this
method requires a shared latent space that is already aligned
and is not readily applicable in our setting.
4. Motivation
Despite recent successes, the prototypical paradigm for
multimodal fusion exhibits critical bottlenecks rooted in
large computational and data overhead, as well as a lack
of modularity. In this section, we discuss these bottlenecks:
Computational Burden. Recent advances in deep
learning have shown that model scale is a key driver of per-
formance and downstream capabilities [8, 17, 36, 54, 104].
Although increasing model scale can greatly benefit per-
formance, the required computational cost to train such
models also increases commensurately, and is unattainable
for many machine learning practitioners and researchers.
In the context of multimodal models, these effects are
more prominent as computational requirements are gener-
ally compounded. For example, in our setting of multi-
modal fusion, it is common to jointly train both fXandfY
[27, 35, 39, 67, 102]. This means that backpropagation is
now required through two networks that must both be held
in memory. Moreover, as we increase the scale of each net-
work, the number of parameters requiring expensive gradi-
ent computations quickly accumulates. We therefore aim
to prioritize computational considerations to design an effi-
cient framework for multimodal fusion.
Scarcity of High-Quality Paired Data. Sourcing mul-
timodal paired data is a necessary step in most multimodal
applications. This step amounts to obtaining paired sam-
ples from the joint distribution over modalities as (x, y)∼
pX,Y. However, in practice, high-quality paired data is of-
ten scarce and expensive to obtain. Typically, this is ei-
ther due to a lack of readily available paired data across all
modalities of interest [27], or due to noisy samples stem-
ming from large amounts of uninformative and weakly-
labeled data pairs [47, 88]. On the other hand, high-quality
samples of unimodal data from the corresponding marginal
distributions of each modality, x∼pXandy∼pY, are
relatively cheap and easy to amass in large quantities. This
is because unimodal data can be collected without any label
pairings while still providing informative intrinsic supervi-
27241
Figure 2. A schematic of our proposed fusion framework to align the latent spaces of pre-trained unimodal encoders using a minimal set
of paired data. The unimodal encoders are kept frozen, and their latent encodings are pre-computed only once. FuseMix applies mixup
on each latent space, importantly sharing the mixing coefficient across modalities, and is used as a modality-agnostic data augmentation.
Then, the lightweight fusion adapters are trained to align the resulting augmented latents into a shared latent space.
sory signals, as evidenced by successes in self-supervised
learning [6, 13, 21, 31, 32]. As such, we aim to defray the
cost of sourcing multimodal paired data by leveraging more
readily available unimodal signals.
Tight Coupling From End-to-End Fusion. While
jointly training fXandfYfrom scratch for multimodal
fusion may produce a semantically meaningful shared la-
tent space, the resulting networks are tightly coupled. This
means that modifying any aspect of either network typically
requires completely re-training both networks end-to-end.
This presents a challenging bottleneck in practice, as re-
search advancements in each underlying modality cannot
be incorporated by end-to-end multimodal fusion without
re-training fXandfY, incurring significant computational,
data, and environmental costs [75]. Our goal is therefore
to design a plug-and-play framework for multimodal fusion
such that individual components can be easily replaced with
minimal overhead, allowing multimodal models to keep
pace with unimodal improvements.
5. Method
In this section we present our framework for multimodal fu-
sion, which aims to address the key considerations of com-
putational and data efficiency, as well as modularity (Sec.
5.1). We also introduce a multimodal augmentation scheme
on latent space called FuseMix to facilitate multimodal fu-
sion (Sec. 5.2). Our entire pipeline is depicted in Figure 2.
5.1. Towards Efficient Multimodal Fusion
As a first step, we take our two encoders as fX=hX◦gX
andfY=hY◦gY. That is, we define gX:X → Z Xand
gY:Y → Z Y, where ZXandZYare intermediate latent
spaces. We then have hX:ZX→ S andhY:ZY→ S ,which we hereafter refer to as fusion adapters. Our key in-
sight here is to take both gXandgYas pre-trained unimodal
encoders which we keep frozen throughout, and treat our fu-
sion adapters hXandhYas learnable heads for multimodal
fusion. This design offers several advantages:
Computational Improvements. We can now equiv-
alently rewrite the alignment loss from Equation 1 as
L(hX, hY;gX(x), gY(y),{gY(y−
i)}M
i=1). This allows us
to express the contrastive objective in Equation 2 as an
expectation with respect to: positive pairs of encodings
(gX(x), gY(y)), whose distribution is induced by pushing
positive pairs (x, y)∼pX,Y through the encoders gXand
gY; and negative pairs of encodings (gX(x−
i), gY(y−
i)),
whose distribution is analogously obtained from negative
pairs on the ambient spaces. Importantly, since this ex-
pectation is taken with respect to a distribution which de-
pends only on the frozen gXandgY, but not on the train-
ablehXandhY, the unimodal encoders gXandgYare not
used in any gradient computations. In other words, since
the unimodal encoders are only needed to provide samples
on latent space, not for backpropagation, we can simply
pre-compute these samples and then discard the unimodal
encoders while training. This step ensures that we do not
need to store large encoders in memory during multimodal
fusion, which significantly reduces computational require-
ments. The only parameters stored in memory during fu-
sion are those of the learnable fusion adapters which are
extremely lightweight compared to the unimodal encoders.
In fact, in all of our experiments, we only require a single
GPU at every step.
Paired Data Efficiency. By setting ZXandZYas
the latent spaces of pre-trained unimodal encoders, we can
directly benefit from the rich modality-specific semantics
that they already encode. Learning this information from
27242
scratch might be redundant for multimodal fusion, so lever-
aging pre-trained unimodal encoders can be an effective
bootstrap to reduce the need for large-scale multimodal
paired data. We can interpret this effect as a form of dis-
tillation from a unimodal latent space into a joint space for
which contrastive objectives have been shown to be effec-
tive [39, 81]. In other words, leveraging pre-trained uni-
modal encoders for multimodal fusion should require less
paired data than training end-to-end from scratch.
Plug-and-Play Framework. We highlight that our mod-
ular approach to multimodal fusion is agnostic to both the
choice of unimodal encoders gXandgYand to the under-
lying modalities XandY. Importantly, by combining arbi-
trary pre-trained unimodal encoders, we can decouple uni-
modal learning from multimodal fusion. Therefore, as the
development of unimodal encoders continues to advance,
we can easily and efficiently leverage new unimodal en-
coders for multimodal fusion in a plug-and-play manner.
5.2. FuseMix: Multimodal Latent Mixup
Given our aim of performing multimodal fusion with mini-
mal samples of paired data, it would seem intuitive to also
leverage data augmentations to generate synthetic multi-
modal pairs (˜x,˜y)∈ X × Y . However, constructing se-
mantically meaningful data augmentations directly on the
ambient spaces XandYis generally challenging due to the
heterogeneity of multimodal data [30]. On the other hand,
we note that ZXandZYprovide a more homogeneous al-
ternative since they are both intermediate latent spaces of
pre-trained unimodal encoders. Additionally, they already
encode semantic information that can be beneficial for cre-
ating meaningful data augmentations.
As such, we introduce a simple yet effective multimodal
augmentation scheme on latent space that is agnostic to
both the involved modalities and the choice of unimodal en-
coders. Our approach, which we call FuseMix, is inspired
by mixup [106], in that augmented samples are generated
from random convex combinations. In particular, we take
linear interpolations between samples in both ZXandZY.
Importantly, since both latent spaces are taken from pre-
trained unimodal encoders, we should expect linear inter-
polations to be more semantically meaningful than when
carried out on ambient space, as is typically done in mixup
[44, 87, 106]. We note that this idea of semantic interpola-
tions in latent space is reminiscent of latent space arithmetic
that has a well-established history [24, 27, 59, 66].
However, na ¨ıvely mixing random samples in each la-
tent space would only produce augmented pairs of latents
(˜zx,˜zy)∈ ZX× ZYwhere ˜zxand˜zyare unrelated to one
another. Therefore, we want to impose some structure on
how interpolations are performed across modalities to en-
sure that we can construct semantically meaningful aug-
mented pairs. To achieve this we take any two existingAlgorithm 1: PyTorch-style pseudocode for FuseMix fusion.
# hX, h Y: learnable fusion adapters
# B: batch size
# Dx, D y: latent dimension of unimodal encoders
# Ds: latent dimension of shared space
# alpha: mixup Beta distribution hyperparameter
# t: learnable temperature parameter
# load latent pairs of batch size 2B
for z x,zy in loader: # (2B x D x, 2B x D y)
# FuseMix
zx1, z x2 = torch.chunk(z x, 2) # B x D x
zy1, z y2 = torch.chunk(z y, 2) # B x D y
lam = random.beta(alpha, alpha)
zx = lam *zx1 + (1 - lam) *zx2
zy = lam *zy1 + (1 - lam) *zy2
# joint space and normalize
sx = l2 normalize(h X(zx), dim=1) # B x D s
sy = l2 normalize(h Y(zy), dim=1) # B x D s
# pairwise cosine similarity w/ temperature
logits xy = (s x @ s y.T) *t.exp() # B x B
logits yx = (s y @ s x.T) *t.exp() # B x B
# symmetric alignment loss
labels = torch.arange(B)
loss xy = cross entropy loss(logits xy, labels)
loss yx = cross entropy loss(logits yx, labels)
loss = (loss xy + loss yx) / 2
# optimize
optimizer.zero grad()
loss.backward()
optimizer.step()
positive multimodal pairs (zx, zy)≜(gX(x), gY(y))and
(ˆzx,ˆzy)≜(gX(ˆx), gY(ˆy)), where (x, y),(ˆx,ˆy)i.i.d.∼pX,Y,
and construct a corresponding augmentation (˜zx,˜zy)as
(˜zx,˜zy)≜λ(zx, zy) + (1−λ) (ˆzx,ˆzy), (3)
where λ∈(0,1)is the shared interpolation coefficient.
Sharing λacross modalities ensures that the resulting aug-
mentation is semantically consistent, meaning ˜zxand˜zy
still form a valid positive pair. In practice, we can of course
similarly apply FuseMix to obtain interpolations of negative
pairs in such a way that the result remains a negative pair.
Finally, our version of Equation 2 on the intermediate latent
spaces with FuseMix is given by
LFuseMix
sym (hX, hY)≜Eh
1
2L
hX, hY; ˜zx,˜zy,{˜z−
yi}M
i=1
(4)
+1
2L
hY, hX; ˜zy,˜zx,{˜z−
xi}M
i=1i
,
where the expectation is taken with respect to: the positive
pairs (zx, zy)and(ˆzx,ˆzy)used to obtain the augmented
positive pair (˜zx,˜zy); the negative pairs {(z−
xi, z−
yi)}M
i=1
and{(ˆz−
xi,ˆz−
yi)}M
i=1used to obtain the augmented negative
pairs{(˜z−
xi,˜z−
yi)}M
i=1; and λ∼ B(α, β). We note that our
FuseMix fusion algorithm can be implemented very eas-
ily. Given pre-computed samples of multimodal latent pairs
(see Sec. 5.1), setting the batch size B≜M+ 1, and tak-
ingα=β, the simplicity of our method is illustrated in
Algorithm 1, requiring only a few lines of code.
27243
6. Experiments
In our experiments, we consider the image-text and audio-
text modality pairings. We start by describing details of our
implementation and then we perform experimental analy-
sis to evaluate our framework and provide insights on key
components of multimodal fusion.
6.1. Implementation Details
Unimodal Latent Extraction. Since an important consid-
eration of our method is to minimize computational require-
ments, we only use a single 32GB NVIDIA V100 GPU for
all of our experiments. This is possible for us because, as
mentioned in Sec. 5.1, we can pre-compute the latents from
pre-trained unimodal encoders so that the underlying en-
coders can be discarded thereafter. Additionally, we can
extract the latents for each modality one at a time to en-
sure that no more than one encoder must be loaded at once.
Importantly, these steps allow us to consider large-scale en-
coders on the order of billions of parameters which would
generally not be feasible for end-to-end fusion on a sin-
gle GPU. We mainly consider Transformer-based [86] uni-
modal encoders, and extract low-dimensional latents from
the penultimate layer of either the [CLS] token if it exists,
or the mean-pooled token otherwise.
Multimodal Latent Fusion. We parameterize our fu-
sion adapters as lightweight MLPs using an inverted bottle-
neck architecture following previous work [5, 52, 83]. Each
MLP consists of residual blocks followed by a final pro-
jection layer of dimension 512 by default to embed each
modality into a shared space. We highlight that since our
fusion adapters are operating on low-dimensional latents,
the computational cost to train them is minimal, and despite
training on a single GPU, we can use large batch sizes (up
toB= 20 K on our V100 GPU), which has been shown to
benefit contrastive learning [13, 31, 82, 91, 97]. Finally, we
note that in all of our experiments, unless otherwise stated,
we use LFuseMix
sym as our sole objective for multimodal fusion.
More details on the MLP architecture and hyperparameters
can be found in our supplementary material.
Training Datasets. We rely on common multimodal
datasets for training. Specifically, following previous works
[15, 46–48], we leverage the image-text pairs from human-
annotated datasets (COCO [51] and Visual Genome [40]),
and web datasets (SBU Captions [65] and Conceptual Cap-
tions 3M [71]), amounting to 5M total pairs. In order to
remain data-efficient, we note that we intentionally avoid
internet-scale datasets like the ones used in several recent
works [35, 39, 67, 102], as these are orders of magnitude
larger than our collated dataset. Similarly, to remain data-
efficient for the audio-text regime, we only leverage the Au-
dioCaps [37] and Clotho [23] train sets which provide 50K
and 15K human-annotated audio-text pairs, respectively.6.2. Cross-Modal Retrieval Performance
To assess the quality of multimodal alignment learned from
FuseMix fusion, we follow previous works [19, 35, 38,
39, 67, 96] and evaluate our method using the downstream
task of cross-modal retrieval. In particular, for the image-
text pairing, we evaluate downstream performance on the
Flickr30K [101] and COCO [51] test sets, and for the audio-
text pairing, we evaluate our method on the AudioCaps [37]
and Clotho [23] test sets. In our experiments, we use sub-
scripts to specify which pre-trained unimodal encoders were
used for bootstrapping. In terms of image encoders, we con-
sider both DINOv2 [64] (D) and UNICOM [2] (U) since, as
of the time of writing, they are two of the top-ranked visual
recognition models as measured by the ImageNet [18] lin-
ear probing benchmark. On the text side, we use the MTEB
[61] text embedding benchmark to select two encoders with
demonstrably semantic latent spaces, namely BGE [98] (B)
and E5 [89] (E). Finally, on the audio side we utilize the
commonly used HTS-AT [9] (H) and the recent Whisper
[68] (W) encoders. In practice, we actually use the con-
catenation of the latents from these two encoders (W&H),
similar to [19]. We emphasize that given the plug-and-play
nature of our method, as better unimodal encoders become
available, we can quickly and cheaply incorporate them into
our framework. We report results across all combinations of
these encoders in Table 1 and Table 2.
For image-text retrieval, we highlight that our method is
highly competitive and sometimes able to outperform var-
ious state-of-the-art methods which are trained on orders
of magnitude more paired data and that require substan-
tially more than a single GPU of compute for fusion. More-
over, we find that the combination of two of the most recent
models, DINOv2+BGE, achieves the highest performance,
highlighting the benefits of a plug-and-play approach that
can leverage the most recent advancements. We also note
that when our method and CLIP [88] are both only trained
on pairs from Conceptual Captions 3M, we outperform
CLIP by a notable margin, demonstrating that FuseMix is
an effective strategy for fusion on lower data regimes. Sim-
ilarly, for audio-text retrieval we outperform all other meth-
ods trained on similar data, and can compete with methods
that use orders of magnitude more paired data.
6.3. Evaluating Dataset Efficiency
As mentioned in Sec. 4, sourcing multimodal data pairs
across all modalities of interest can be costly, especially
in scarce data regimes. In practical settings, it is there-
fore natural to wonder how one should allocate efforts to
construct a dataset for multimodal fusion that would maxi-
mize performance. We aim to answer this question by char-
acterizing and quantifying three key properties of datasets,
namely quantity, quality, and diversity. For dataset quantity,
we take an existing dataset and uniformly subsample vari-
27244
Flickr30K (1K test set) COCO (5K test set)
Method # (image, text) text →image image →text text →image image →text
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
internet-scale
FILIP [100] 300M 75.0 93.4 96.3 89.8 99.2 99.8 45.9 70.6 79.3 61.3 84.3 90.4
CLIP [67] 400M 68.7 90.6 95.2 88.0 98.7 99.4 37.8 62.4 72.2 58.4 81.5 88.1
ALIGN [35] 1B 75.7 93.8 96.8 88.6 98.7 99.7 45.6 69.8 78.6 58.6 83.0 89.7
LiT [105] 4B 66.5 - - 83.9 - - 43.6 - - 59.5 - -
3T [39] 5B 72.1 - - 87.3 - - 48.5 - - 64.1 - -
low-data regime
CLIP [88] 3M 54.3 84.1 90.8 67.4 83.2 92.4 29.9 57.9 66.9 36.2 64.3 80.1
FuseMix (D,B) 3M 59.9 86.4 91.6 74.4 94.0 97.4 32.2 58.2 69.4 42.3 68.4 78.9
FuseMix (U,B) 5M 66.3 88.9 93.3 81.2 95.9 97.7 42.5 70.2 80.0 59.1 83.4 90.3
FuseMix (U,E) 5M 64.3 87.7 93.0 80.2 95.6 98.1 42.9 70.0 80.1 59.1 83.9 91.0
FuseMix (D,E) 5M 68.8 90.9 94.6 85.2 96.9 98.4 46.1 74.3 84.1 64.3 86.2 92.1
FuseMix (D,B) 5M 71.2 91.4 94.7 84.8 97.2 99.1 46.3 74.6 83.4 62.7 86.4 92.7
Table 1. Results of image-text retrieval on the Flickr30K 1K and COCO 5K test sets. The top section of the table contains fusion methods
trained with internet-scale data, while the bottom section contains methods using much fewer image-text pairs. All our results use the
largest available version of the underlying unimodal encoders. Refer to Sec. 6.2 for the definition of the subscripts.
AudioCaps (1K test set) Clotho (1K test set)
Method # (audio, text) text →audio audio →text text →audio audio →text
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
internet-scale
LAION-CLAP [96] 700K 36.2 70.3 82.5 45.0 76.7 88.0 17.2 42.9 55.4 24.2 51.1 66.9
V ALOR [11] 6.5M 40.1 73.9 83.1 - - - 17.5 42.7 55.3 - - -
ONE-PEACE [90] 2.4M 42.5 77.5 88.4 51.0 81.9 92.0 22.4 49.0 62.7 27.1 52.3 65.4
V AST [12] 27M 52.0 76.8 82.9 - - - 26.9 53.2 66.1 - - -
low-data regime
ML-ACT [58] 50K/15K 33.9 69.7 82.6 39.4 72.0 83.9 14.4 36.6 49.9 16.2 37.5 50.2
MMT [38] 50K/15K 36.1 72.0 84.5 39.6 76.8 86.7 6.5 21.6 32.8 6.3 22.8 33.3
CLAP-HTSAT [19] 70K 34.7 70.2 82.0 41.9 73.2 84.6 16.8 41.1 54.1 20.0 44.9 58.7
LAION-CLAP [96] 65K 36.7 70.9 83.2 45.3 78.0 87.7 12.0 31.6 43.9 15.7 36.9 51.3
FuseMix (W&H,B) 50K/15K 41.3 76.9 87.6 50.3 81.0 89.6 15.7 39.4 53.8 19.7 42.9 56.5
FuseMix (W&H,B) 65K 43.1 77.4 87.4 52.4 83.4 92.4 17.6 41.1 53.5 21.5 44.7 57.4
Table 2. Results of audio-text retrieval on the AudioCaps 1K and Clotho 1K test sets. The top section of the table contains fusion methods
trained with internet-scale data, while the bottom section contains methods using much fewer audio-text pairs. ‘50K/15K‘ means that the
model was trained only on the AudioCaps (50K) (resp. Clotho (15K)) training set when evaluating on AudioCaps (resp. Clotho). All our
results use the largest available version of the underlying unimodal encoders. Refer to Sec. 6.2 for the definition of the subscripts.
0 50 1003040
Subset Size (%)R@1
(a) QuantityH
500KW
500KW
3M4045505560R@1
(b) Quality0 50 10002040
Subset Size (%)∆R@1 (%)
(c) Diversity
Figure 3. Measuring the effect of dataset quantity, quality, and di-
versity on downstream performance, evaluated using text-to-image
retrieval on the Flickr30K test set. The x-axes indicate the rela-
tive/absolute number of image-text pairs, while H and W denote
human and web-annotated, respectively. ∆R@1 (%) denotes rela-
tive improvement in Recall@1 compared to uniform subsampling.
ous numbers of pairs to measure the effect of quantity on
downstream performance. For dataset quality, we consider
human-annotated datasets to be of higher-quality, and web
datasets to be of lower-quality. Finally, for dataset diver-sity, we can rely on determinantal point processes (DPPs)
[10, 41, 42]. For a given dataset, DPPs return subsets of
points of a pre-specified size that are maximally diverse (see
supp. material for details). Applied to our setting, we use
DPPs on an existing dataset to obtain diverse subsets of var-
ious sizes and then compare the performance against uni-
formly sampled subsets of the corresponding sizes.
Our results are shown in Figure 3. We observe that
increased quantity of data improves performance in lower
data regimes as expected (Figure 3a). However, the quality
of the underlying dataset also has a very strong effect, as has
been similarly observed in other work [47, 88]. In fact, in
Figure 3b, we find that 6×the number of image-text pairs
from the web are required to match the performance of us-
ing higher quality human-annotated pairs. Interestingly, in
Figure 3c we find that with access to limited data, sourcing
image-text pairs that are maximally diverse provides sub-
stantial improvements of up to nearly 40% compared to se-
27245
A photo of a cat
meowingA photo of a mov-
ing trainA photo of rain-
drops fallingA photo of waves
on the beach
Figure 4. Results of audio-to-image generation. The top row was
generated from audio clips (accessible from the audio icons), and
the bottom row was generated by describing the audio clips in text.
lecting image-text pairs without consideration for diversity
(i.e. uniform sampling). As such, when sourcing multi-
modal paired data in practice, it is important to consider not
just quantity, but also quality and diversity, as these aspects
can unlock notable improvements in scarce data regimes.
6.4. Audio-to-Image Generation
We consider the recently proposed task [27] of generating
images given audio prompts. The aim is to repurpose an ex-
isting text-to-image generative model to be conditioned on
audio in lieu of text. Girdhar et al. [27] achieved this using
a private reimplementation of DALLE-2 [69]. We opt to
use FuseMix to perform this task while only using publicly
available models: we use GLIDE5[62], a text-conditioned
diffusion model which leverages CLIP6[67] to condition
on text. We apply our method to align the latent space of
Whisper into the latent space of CLIP to endow GLIDE
with audio-conditioning capabilities (see supp. material).
In Figure 4, we provide examples of generated samples us-
ing various sounds. While we omit quantitative analysis
for this task due to a lack of suitable metrics, we provide a
qualitative comparison of each sample with a corresponding
sample generated from the original text-conditioned GLIDE
using a text prompt that is semantically equivalent to the au-
dio prompt. For example, for the sound of a cat meowing,
we compare with the text prompt “a photo of a cat meow-
ing”. We find it noteworthy that conditioning GLIDE on
audio prompts using FuseMix can produce samples of simi-
lar quality and fidelity as conditioning on text prompts, even
though GLIDE itself was never trained with audio data.
6.5. Ablations
Effect of Unimodal Encoder Size. Given the plug-and-
play nature of our method, we would hope that larger un-
5Stable Diffusion [70] was not considered since its text conditioning is
high-dimensional making alignment more challenging than with GLIDE.
6We use GLIDE’s reimplementation of CLIP trained on noisy images.S B L40506070 R@1
(a) Model Size1k 10k 20k666870 R@1
(b) Batch SizeAug. R@1
t→i i→t
None 66.1 80.5
GN 66.7 81.5
RQ [94] 67.8 82.6
FuseMix 71.2 84.8
(c) Type of Aug.
Figure 5. Measuring the effect of model size, batch size, and
data augmentations on downstream performance, evaluated with
the Flickr30k test set. GN denotes Gaussian noise with a standard
deviation of 0.01 and RQ denotes random quantization. By de-
fault, R@1 denotes text-to-image Recall@1.
derlying unimodal encoders would be beneficial for mul-
timodal fusion. We study this effect by evaluating down-
stream performance for various sizes of encoders. We con-
sider the following combinations: DINOv2 ViT-S/14 &
BGE Small; DINOv2 ViT-B/14 & BGE Base; and DINOv2
ViT-G/14 & BGE Large, referred to as S, B, and L, respec-
tively, in Figure 5a. As shown, scaling the unimodal en-
coders translates to improved downstream performance.
Effect of Batch Size. As mentioned in Sec. 6.1, since
training our fusion adapters requires minimal compute, we
can use larger batch sizes even on a single GPU. In Fig-
ure 5b, we see that our method can benefit from more nega-
tive samples in the contrastive objective, which is consistent
with findings in previous work [13, 31, 82].
Effect of Data Augmentations. In Figure 5c, we eval-
uate the importance of data augmentations and compare
our proposed FuseMix with other modality-agnostic data
augmentation schemes, namely Gaussian noise and random
quantization [94]. We note that data augmentations gener-
ally seem beneficial in our setting, although FuseMix pro-
vides the largest improvement in performance, further vali-
dating our proposed approach.
7. Conclusion and Future Work
In this work, we have proposed a framework for multimodal
fusion that is both compute-efficient and data-efficient
which can effectively bootstrap from arbitrary pre-trained
unimodal encoders. We have introduced FuseMix, a
simple yet effective multimodal augmentation scheme
on latent space inspired by mixup. However, while our
method can benefit from powerful unimodal encoders, we
are limited by the semantic information that they have
previously learned [39]. It would thus be an interesting
future direction to apply efficient fine-tuning methods
[20, 33] to the unimodal encoders during fusion, although
this would incur additional overhead. We also highlight
that since our framework essentially considers unimodal
encoders as black box models (i.e. we only use their
latent encodings from their penultimate layer), this opens
up exciting applications whereby we can perform multi-
modal fusion with encoders only accessible via an API.
27246
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-
sch, Katherine Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob L Menick,
Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikoł aj Bi ´nkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Kar ´en Simonyan.
Flamingo: a Visual Language Model for Few-Shot Learn-
ing. In Advances in Neural Information Processing Sys-
tems, pages 23716–23736, 2022. 1, 2
[2] Xiang An, Jiankang Deng, Kaicheng Yang, Jaiwei Li, Ziy-
ong Feng, Jia Guo, Jing Yang, and Tongliang Liu. Uni-
com: Universal and compact representation learning for im-
age retrieval. In The Eleventh International Conference on
Learning Representations , 2023. 6
[3] Relja Arandjelovic and Andrew Zisserman. Look, listen
and learn. In Proceedings of the IEEE International Con-
ference on Computer Vision , 2017. 2
[4] Philip Bachman, R Devon Hjelm, and William Buchwal-
ter. Learning representations by maximizing mutual infor-
mation across views. In Advances in Neural Information
Processing systems , 2019. 3
[5] Gregor Bachmann, Sotiris Anagnostidis, and Thomas
Hofmann. Scaling MLPs: A Tale of Inductive Bias.
arXiv:2306.13575 , 2023. 6
[6] Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Mor-
cos, Shashank Shekhar, Tom Goldstein, Florian Bordes,
Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi
Schwarzschild, Andrew Gordon Wilson, Jonas Geiping,
Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pir-
siavash, Yann LeCun, and Micah Goldblum. A cookbook
of self-supervised learning. arXiv:2304.12210 , 2023. 2, 4
[7] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu,
Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som,
Songhao Piao, and Furu Wei. VLMo: Unified Vision-
Language Pre-Training with Mixture-of-Modality-Experts.
InAdvances in Neural Information Processing Systems ,
pages 32897–32912, 2022. 2
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. Language Models
are Few-Shot Learners. In Advances in Neural Information
Processing Systems , pages 1877–1901, 2020. 3
[9] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-
Kirkpatrick, and Shlomo Dubnov. HTS-AT: A Hierarchical
Token-Semantic Audio Transformer for Sound Classifica-
tion and Detection. In IEEE International Conference on
Acoustics, Speech and Signal Processing , 2022. 6[10] Laming Chen, Guoxin Zhang, and Eric Zhou. Fast greedy
map inference for determinantal point process to improve
recommendation diversity. Advances in Neural Information
Processing Systems , 31, 2018. 7
[11] Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu,
Weining Wang, Jinhui Tang, and Jing Liu. Valor: Vision-
audio-language omni-perception pretraining model and
dataset. arXiv:2304.08345 , 2023. 7
[12] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao,
Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: A vision-
audio-subtitle-text omni-modality foundation model and
dataset. arXiv:2305.18500 , 2023. 7
[13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In Proceedings of the 37th In-
ternational Conference on Machine Learning , pages 1597–
1607, 2020. 2, 4, 6, 8
[14] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,
Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,
Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-
eri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael
Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,
Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin
Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,
Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas
Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,
Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,
Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhos-
seini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and
Radu Soricut. PaLI-X: On Scaling up a Multilingual Vision
and Language Model. arXiv:2305.18565 , 2023. 2
[15] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.
UNITER: UNiversal Image-TExt Representation Learning.
InComputer Vision – ECCV 2020 , pages 104–120, 2020.
2, 6
[16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning.
arXiv:2305.06500 , 2023. 3
[17] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr
Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter
Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-
mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschan-
nen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz,
Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Ku-
mar, Sjoerd Van Steenkiste, Gamaleldin Fathy Elsayed, Ar-
avindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot,
Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vigh-
nesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas
Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran,
Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers,
Jeremiah J. Harmsen, and Neil Houlsby. Scaling Vision
Transformers to 22 Billion Parameters. In Proceedings of
the 40th International Conference on Machine Learning ,
pages 7480–7512, 2023. 3
27247
[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. ImageNet: A large-scale hierarchical im-
age database. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 248–255,
2009. 6
[19] Soham Deshmukh, Benjamin Elizalde, and Huaming
Wang. Audio Retrieval with WavText5K and CLAP Train-
ing.arXiv:2209.14275 , 2022. 2, 6, 7
[20] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
Zettlemoyer. Qlora: Efficient finetuning of quantized llms.
arXiv preprint arXiv:2305.14314 , 2023. 8
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the
2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pages
4171–4186, 2019. 2, 4
[22] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,
Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,
and Pete Florence. PaLM-E: An embodied multimodal lan-
guage model. arXiv:2303.03378 , 2023. 1, 2
[23] Konstantinos Drossos, Samuel Lipping, and Tuomas Virta-
nen. Clotho: an audio captioning dataset. In ICASSP 2020 -
2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pages 736–740, 2020. 6
[24] Kawin Ethayarajh, David Duvenaud, and Graeme Hirst. To-
wards understanding linear word analogies. In Proceedings
of the 57th Annual Meeting of the Association for Compu-
tational Linguistics , pages 3253–3262, 2019. 5
[25] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen.
CLIP2Video: Mastering video-text retrieval via image
CLIP. arXiv:2106.11097 , 2021. 2
[26] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens
van der Maaten, Armand Joulin, and Ishan Misra. Omni-
vore: A single model for many visual modalities. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 16102–16112, 2022. 2
[27] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan
Misra. ImageBind: One Embedding Space To Bind Them
All. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 15180–15190,
2023. 1, 2, 3, 5, 8
[28] Satya Krishna Gorti, No ¨el V ouitsis, Junwei Ma, Keyvan
Golestan, Maksims V olkovs, Animesh Garg, and Guang-
wei Yu. X-Pool: Cross-Modal Language-Video Attention
for Text-Video Retrieval. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5006–5015, 2022. 2
[29] Andrey Guzhov, Federico Raue, J ¨orn Hees, and Andreas
Dengel. Audioclip: Extending clip to image, text and audio.
InIEEE International Conference on Acoustics, Speech and
Signal Processing , pages 976–980. IEEE, 2022. 1, 2, 3[30] Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang,
Wanqian Zhang, Bo Li, and Mu Li. Mixgen: A new multi-
modal data augmentation. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision ,
pages 379–389, 2023. 2, 5
[31] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
9729–9738, 2020. 4, 6, 8
[32] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scal-
able vision learners. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
16000–16009, 2022. 2, 4
[33] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 8
[34] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski,
Michael Auli, Wojciech Galuba, Florian Metze, and
Christoph Feichtenhofer. Masked autoencoders that listen.
Advances in Neural Information Processing Systems , 35:
28708–28720, 2022. 2
[35] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana
Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,
and Tom Duerig. Scaling up visual and vision-language
representation learning with noisy text supervision. In Pro-
ceedings of the 38th International Conference on Machine
Learning , pages 4904–4916. PMLR, 2021. 1, 2, 3, 6, 7
[36] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for
neural language models. arXiv:2001.08361 , 2020. 3
[37] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and
Gunhee Kim. Audiocaps: Generating captions for audios in
the wild. In NAACL-HLT , 2019. 6
[38] A. S. Koepke, A.-M. Oncescu, J. Henriques, Z. Akata, and
S. Albanie. Audio retrieval with natural language queries:
A benchmark study. In IEEE Transactions on Multimedia ,
2022. 6, 7
[39] Jannik Kossen, Mark Collier, Basil Mustafa, Xiao Wang,
Xiaohua Zhai, Lucas Beyer, Andreas Steiner, Jesse Berent,
Rodolphe Jenatton, and Efi Kokiopoulou. Three towers:
Flexible contrastive learning with pretrained image models.
arXiv:2305.16999 , 2023. 1, 2, 3, 5, 6, 7, 8
[40] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, Michael S. Bernstein,
and Fei-Fei Li. Visual genome: Connecting language and
vision using crowdsourced dense image annotations. Inter-
national Journal of Computer Vision , 123:32–73, 2017. 6
[41] Alex Kulesza and Ben Taskar. K-DPPs: Fixed-Size
Determinantal Point Processes. In Proceedings of the
28th International Conference on Machine Learning , page
1193–1200, 2011. 7
27248
[42] Alex Kulesza and Ben Taskar. Determinantal point pro-
cesses for machine learning. Foundations and Trends in
Machine Learning , 5(2–3):123–286, 2012. 7
[43] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE , 86(11):2278–2324, 1998. 2
[44] Kibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jin-
woo Shin, and Honglak Lee. i-Mix: A Domain-Agnostic
Strategy for Contrastive Representation Learning. In Inter-
national Conference on Learning Representations , 2021. 3,
5
[45] Dongxu Li, Junnan Li, and Steven CH Hoi. BLIP-
Diffusion: Pre-trained subject representation for
controllable text-to-image generation and editing.
arXiv:2305.14720 , 2023. 1, 3
[46] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
Align before fuse: Vision and language representation
learning with momentum distillation. In Advances in
Neural Information Processing Systems , pages 9694–9705,
2021. 1, 2, 3, 6
[47] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
BLIP: Bootstrapping language-image pre-training for uni-
fied vision-language understanding and generation. In Pro-
ceedings of the 39th International Conference on Machine
Learning , pages 12888–12900. PMLR, 2022. 1, 2, 3, 7
[48] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. In Pro-
ceedings of the 40th International Conference on Machine
Learning . PMLR, 2023. 1, 2, 3, 6
[49] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-
aowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,
Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-
Semantics Aligned Pre-training for Vision-Language Tasks.
InComputer Vision – ECCV 2020 , pages 121–137, 2020. 2
[50] Valerii Likhosherstov, Anurag Arnab, Krzysztof Marcin
Choromanski, Mario Lucic, Yi Tay, and Mostafa De-
hghani. PolyViT: Co-training Vision Transformers on Im-
ages, Videos and Audio. Transactions on Machine Learn-
ing Research , 2023. 2
[51] Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and
C. Lawrence Zitnick. Microsoft COCO: Common Objects
in Context. In Computer Vision – ECCV 2014 , pages 740–
755. Springer International Publishing, 2014. 6
[52] Zhouhan Lin, Roland Memisevic, and Kishore Konda.
How far can we go without convolution: Improving fully-
connected networks. arXiv:1511.02580 , 2015. 6
[53] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. Visual instruction tuning. In Advances in Neural In-
formation Processing Systems , 2023. 1, 2
[54] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A ConvNet for
the 2020s. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 11976–
11986, 2022. 3[55] Zhaoyan Liu, No ¨el V ouitsis, Satya Krishna Gorti, Jimmy
Ba, and Gabriel Loaiza-Ganem. TR0N: Translator net-
works for 0-shot plug-and-play conditional generation. In
Proceedings of the 40th International Conference on Ma-
chine Learning . PMLR, 2023. 1, 3
[56] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViL-
BERT: Pretraining Task-Agnostic Visiolinguistic Represen-
tations for Vision-and-Language Tasks. In Advances in
Neural Information Processing Systems , 2019. 2
[57] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,
Nan Duan, and Tianrui Li. CLIP4Clip: An empirical study
of CLIP for end to end video clip retrieval and captioning.
Neurocomputing , 508:293–304, 2022. 2
[58] Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark Plumbley, and
Wenwu Wang. On Metric Learning for Audio-Text Cross-
Modal Retrieval. In Proc. Interspeech 2022 , pages 4142–
4146, 2022. 2, 7
[59] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. Efficient estimation of word representations in vector
space. In International Conference on Learning Represen-
tations , 2013. 5
[60] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar
Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh,
Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya
Srinet, Babak Damavandi, and Anuj Kumar. Anymal: An
efficient and scalable any-modality augmented language
model. arXiv:2309.16058 , 2023. 2
[61] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and
Nils Reimers. MTEB: Massive text embedding benchmark.
InProceedings of the 17th Conference of the European
Chapter of the Association for Computational Linguistics ,
pages 2014–2037. Association for Computational Linguis-
tics, 2023. 6
[62] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image genera-
tion and editing with text-guided diffusion models. arXiv
preprint arXiv:2112.10741 , 2021. 8
[63] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Rep-
resentation learning with contrastive predictive coding.
arXiv:1807.03748 , 2018. 3
[64] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,
Mahmoud Assran, Nicolas Ballas, Russel Galuba, Woj-
ciech Howes, Po-Yao Huang, Li Shang-Wen, Ishan Misra,
Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu
Xu, Herv ´e Jegou, Julien Mairal, Patrick Labatut, Armand
Joulin, and Piotr Bojanowski. DINOv2: Learning robust vi-
sual features without supervision. arXiv:2304.07193 , 2023.
6
[65] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
Im2Text: Describing Images Using 1 Million Captioned
Photographs. In Advances in Neural Information Process-
ing Systems , 2011. 6
[66] Jeffrey Pennington, Richard Socher, and Christopher D
Manning. Glove: Global vectors for word representation. In
27249
Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing , pages 1532–1543, 2014.
5
[67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In Proceedings
of the 38th International Conference on Machine Learning ,
pages 8748–8763. PMLR, 2021. 1, 2, 3, 6, 7, 8
[68] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,
Christine Mcleavey, and Ilya Sutskever. Robust speech
recognition via large-scale weak supervision. In Pro-
ceedings of the 40th International Conference on Machine
Learning , pages 28492–28518. PMLR, 2023. 6
[69] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen. Hierarchical text-conditional image
generation with CLIP latents. arXiv:2204.06125 , 2022. 1,
8
[70] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 1, 8
[71] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages
2556–2565, 2018. 6
[72] Yiqing Shen, Yulin Luo, Dinggang Shen, and Jing Ke.
RandStainNA: Learning Stain-Agnostic Features from His-
tology Slides by Bridging Stain Augmentation and Normal-
ization. In Medical Image Computing and Computer As-
sisted Intervention , pages 212–221. Springer, 2022. 2
[73] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations , 2015.
2
[74] Junhyuk So, Changdae Oh, Yongtaek Lim, Hoyoon Byun,
Minchul Shin, and Kyungwoo Song. Geodesic multi-modal
mixup for robust fine-tuning. In Advances in Neural Infor-
mation Processing Systems , 2023. 3
[75] Emma Strubell, Ananya Ganesh, and Andrew McCallum.
Energy and policy considerations for deep learning in NLP.
InProceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 3645–3650,
2019. 4
[76] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
Wei, and Jifeng Dai. VL-BERT: Pre-training of Generic
Visual-Linguistic Representations. In International Con-
ference on Learning Representations , 2020. 2
[77] Yi Sui, Tongzi Wu, Jesse C. Cresswell, Ga Wu, George
Stein, Xiao Shi Huang, Xiaochen Zhang, and Maksims
V olkovs. Self-supervised representation learning from ran-
dom data projectors. arXiv:2310.07756 , 2023. 2
[78] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy,
and Cordelia Schmid. VideoBERT: A Joint Model forVideo and Language Representation Learning. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , 2019. 2
[79] Hao Tan and Mohit Bansal. LXMERT: Learning cross-
modality encoder representations from transformers. In
Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing , pages
5100–5111, 2019. 2
[80] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes,
Tanmoy Bhattacharya, and Sarah Michalak. On mixup
training: Improved calibration and predictive uncertainty
for deep neural networks. Advances in Neural Information
Processing Systems , 32, 2019. 3
[81] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
trastive representation distillation. In International Confer-
ence on Learning Representations , 2020. 5
[82] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
trastive multiview coding. In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August
23–28, 2020, Proceedings, Part XI 16 , pages 776–794.
Springer, 2020. 3, 6, 8
[83] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov,
Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica
Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit,
Mario Lucic, and Alexey Dosovitskiy. MLP-Mixer: An
all-MLP architecture for vision. In Advances in Neural In-
formation Processing Systems , pages 24261–24272, 2021.
6
[84] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
Videomae: Masked autoencoders are data-efficient learners
for self-supervised video pre-training. In Advances in Neu-
ral Information Processing Systems , pages 10078–10093,
2022. 2
[85] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,
S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. Multi-
modal few-shot learning with frozen language models. In
Advances in Neural Information Processing Systems , pages
200–212, 2021. 2
[86] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances
in Neural Information Processing Systems , 2017. 6
[87] Vikas Verma, Thang Luong, Kenji Kawaguchi, Hieu Pham,
and Quoc Le. Towards domain-agnostic contrastive learn-
ing. In Proceedings of the 38th International Conference
on Machine Learning , pages 10530–10541, 2021. 3, 5
[88] Alex Jinpeng Wang, Kevin Qinghong Lin, David Junhao
Zhang, Stan Weixian Lei, and Mike Zheng Shou. Too
Large; Data Reduction for Vision-Language Pre-Training.
InProceedings of the IEEE International Conference on
Computer Vision , 2023. 1, 3, 6, 7
[89] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,
Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu
Wei. Text embeddings by weakly-supervised contrastive
pre-training. arXiv:2212.03533 , 2022. 6
[90] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiao-
huan Zhou, Jingren Zhou, Xinggang Wang, and Chang
27250
Zhou. One-peace: Exploring one general representation
model toward unlimited modalities. arXiv:2305.11172 ,
2023. 7
[91] Tongzhou Wang and Phillip Isola. Understanding con-
trastive representation learning through alignment and uni-
formity on the hypersphere. In International Conference on
Machine Learning , pages 9929–9939. PMLR, 2020. 6
[92] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun
Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun
Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali
Wang, Limin Wang, and Yu Qiao. InternVideo: General
video foundation models via generative and discriminative
learning. arXiv:2212.03191 , 2022. 2
[93] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. SimVLM: Simple visual language
model pretraining with weak supervision. In International
Conference on Learning Representations , 2022. 2
[94] Huimin Wu, Chenyang Lei, Xiao Sun, Peng-Shuai Wang,
Qifeng Chen, Kwang-Ting Cheng, Stephen Lin, and Zhi-
rong Wu. Randomized Quantization: A Generic Aug-
mentation for Data Agnostic Self-supervised Learning. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 16305–16316, 2023. 2, 8
[95] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-
Seng Chua. NExT-GPT: Any-to-any multimodal LLM.
arXiv:2309.05519 , 2023. 2
[96] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor
Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale con-
trastive language-audio pretraining with feature fusion and
keyword-to-caption augmentation. In IEEE International
Conference on Acoustics, Speech and Signal Processing ,
2023. 2, 6, 7
[97] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 3733–
3742, 2018. 6
[98] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muen-
nighof. C-pack: Packaged resources to advance general
chinese embedding. arXiv:2309.07597 , 2023. 6
[99] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,
Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The
dawn of LMMs: Preliminary explorations with GPT-4V
(ision). arXiv:2309.17421 , 9, 2023. 1
[100] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang,
and Chunjing Xu. Filip: Fine-grained interactive language-
image pre-training. In International Conference on Learn-
ing Representations , 2021. 7
[101] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-
enmaier. From image descriptions to visual denotations:
New similarity metrics for semantic inference over event
descriptions. Transactions of the Association for Computa-
tional Linguistics , 2:67–78, 2014. 1, 2, 6
[102] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. CoCa: Contrastive
Captioners are Image-Text Foundation Models. Transac-
tions on Machine Learning Research , 2022. 1, 2, 3, 6[103] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng
Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin
Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei
Zhou, and Pengchuan Zhang. Florence: A new foundation
model for computer vision. arXiv:2111.11432 , 2021. 2
[104] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and
Lucas Beyer. Scaling Vision Transformers. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 12104–12113, 2022. 3
[105] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
LiT: Zero-Shot Transfer With Locked-Image Text Tuning.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18123–18133, 2022.
2, 7
[106] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and
David Lopez-Paz. mixup: Beyond Empirical Risk Mini-
mization. In International Conference on Learning Repre-
sentations , 2018. 1, 2, 3, 5
[107] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata
Ghorbani, and James Zou. How does mixup help with ro-
bustness and generalization? In International Conference
on Learning Representations , 2021. 3
[108] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hong-
sheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Meta-
transformer: A unified framework for multimodal learning.
arXiv:2307.10802 , 2023. 2
27251
