PLGSLAM: Progressive Neural Scene Represenation with Local to Global
Bundle Adjustment
Tianchen Deng1, Guole Shen1, Tong Qin1, Jianyu Wang1, Wentao Zhao1,
Jingchuan Wang1, Danwei Wang2, Weidong Chen1*
1Shanghai Jiao Tong University2Nanyang Technological University
Figure 1. Large-scale indoor scene 3D Reconstruction with different methods. We depict the final mesh and camera tracking trajectory error
(Absolute Trajectory Error) of different methods. The color bar on the right shows the relative scaling of color. PLGSLAM outperforms
others in both scene reconstruction and pose estimation.
Abstract
Neural implicit scene representations have recently
shown encouraging results in dense visual SLAM. However,
existing methods produce low-quality scene reconstruction
and low-accuracy localization performance when scaling
up to large indoor scenes and long sequences. These limi-
tations are mainly due to their single, global radiance field
with finite capacity, which does not adapt to large sce-
narios. Their end-to-end pose networks are also not ro-
bust enough with the growth of cumulative errors in large
scenes. To this end, we introduce PLGSLAM, a neural vi-
sual SLAM system capable of high-fidelity surface recon-
struction and robust camera tracking in real-time. To han-
dle large-scale indoor scenes, PLGSLAM proposes a pro-
gressive scene representation method which dynamically al-
locates new local scene representation trained with frames
within a local sliding window. This allows us to scale up to
larger indoor scenes and improves robustness (even under
pose drifts). In local scene representation, PLGSLAM uti-
lizes tri-planes for local high-frequency features with multi-
layer perceptron (MLP) networks for the low-frequency fea-
ture, achieving smoothness and scene completion in unob-
served areas. Moreover, we propose local-to-global bundle
adjustment method with a global keyframe database to ad-
dress the increased pose drifts on long sequences. Experi-
*represents corresponding author.mental results demonstrate that PLGSLAM achieves state-
of-the-art scene reconstruction results and tracking perfor-
mance across various datasets and scenarios (both in small
and large-scale indoor environments).
1. Introduction
Visual Simultaneous Localization and Mapping (SLAM)
has been a fundamental computer vision problem with
wide applications such as autonomous driving, remote sens-
ing [6], and virtual/augmented reality. Many traditional
methods have been introduced in the past years, such as
ORB-SLAM [19, 20], VINS [24], and so on. They can esti-
mate the camera pose and construct sparse point cloud maps
in real-time with accurate localization performance. How-
ever, the sparse point cloud maps cannot meet the further
perception needs of the robot. Recent attention has turned
to learning-based methods for dense scene reconstruction.
Kinectfusion [11], BAD-SLAM[25] reconstruct meaning-
ful global 3D maps and show reasonable but limited recon-
struction accuracy with deep learning networks.
Nowadays, with the proposal of Neural Radiance Fields
(NeRF), there are many following works on different ar-
eas [8]. iMAP [27] is the first work to use a single multi-
layer perceptron (MLP) to represent the entire scene in
SLAM system. NICE-SLAM [41] improves the scene rep-
resentation method with feature grids. ESLAM [12] and
Co-SLAM [31] further improve the scene representation
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19657
methods. ESLAM uses tri-planes for better real-time per-
formance and reconstruction accuracy. Co-SLAM uses
joint coordinate and sparse parametric scene for accurate
scene representation. They can achieve promising recon-
struction quality in a small indoor room.
Although ESLAM and Co-SLAM perform well in
smaller indoor scenes, they face challenges in represent-
ing large-scale indoor scenes (e.g., multi-room apartments).
We outline the key challenges for real-time incremental
NeRF-SLAM: a) insufficient scene representation capa-
bility: Existing methods employ a fixed-capacity, global
model, limiting scalability to larger scenes and longer video
sequences. b) accumulation of errors and pose drift: Ex-
isting works struggle with accuracy and robustness in large-
scale indoor scenes due to accumulating errors.
To this end, we design our neural SLAM system for
accurate scene reconstruction and robust pose estimation
in large indoor scenes and long sequences. We propose
a progressive scene representation method which dynami-
cally initialize new scene representation when the camera
moves to the bound of the local scene representation. The
entire scene is divided into multiple local scene presenta-
tions, which can significantly improve the scene represen-
tation capacity of large indoor scenes. The robustness of
our system is also increased because the mis-estimation is
locally bounded.
In local scene representation, We propose a parametric-
coordinate joint encoding method for accuracy, speed, and
completion of unseen region. Parametric encoding is the tri-
plane encoding, and the coordinate encoding is the one-blob
encoding with MLP. We use tri-planes to encode the local
high-frequency feature of the scene and use MLP to repre-
sent global low-frequency features with the coherence pri-
ors inherent. We bring together the benefits of both methods
for accuracy, smoothness, and hole-filling in areas without
observation.
Furthermore, we combine the traditional SLAM systems
with end-to-end pose networks to improve pose estimation
performance. We propose a local-to-global bundle adjust-
ment (BA) method to eliminate the cumulative error which
becomes significantly evident in large indoor scenes and
long video sequences. So far, all neural SLAM systems
only use end-to-end network and perform BA with rays
sampled from a local subset of selected keyframes, resulting
in inaccurate, non-robust pose estimation and significant cu-
mulative errors in camera tracking. PLGSLAM maintains a
global keyframe database and performs local-to-global neu-
ral warpping and reprojection Bundle Adjustment. The pro-
posed Local-to-global BA method can eliminate the cumu-
lative error with all the historical observations. In practice,
PLGSLAM achieves SOTA performance in camera tracking
and 3D reconstruction while maintaining real-time perfor-
mance. Overall, our contributions are shown as follows:• A progressive scene representation method is proposed
which dynamically initiate local scene representation
trained with frames within a local window. This enables
scalability to extensive indoor scenes and long videos se-
quences, substantially improving robustness.
• In local scene representation, We design a joint
parametric-coordinate encoding method. We combine the
tri-planes with the one-blob encoding encoding method
for accurate and smooth surface reconstruction. It can
not only enhance the ability of scene representation, but
also substantially reduce the memory growth from cubic
to square.
• We integrate the traditional SLAM system with an end-
to-end pose estimation network. A local-to-global bundle
adjustment algorithm is proposed, which can mitigate cu-
mulative error in large-scale indoor scenes. Our system
maintain a global keyframe database with the system op-
eration, enabling bundle adjustment across all past obser-
vations, from local to global.
2. Related Work
Dense Visual SLAM . SLAM [15] and localization [16,
23, 37, 38] has been an active field for the past two decades.
Traditional visual SLAM algorithms [9, 19, 24, 34, 35] es-
timate accurate camera poses and use sparse point clouds as
the map representation. They use manipulated key points
for tracking, mapping, relocalization, and loop closing.
Dense visual SLAM approaches focus on reconstructing a
dense map of a scene. DTAM [21] is one of the pioneer
works that use the dense map and view-centric scene repre-
sentation. KinectFusion [11] performs camera tracking via
projective iterative-closest-point (ICP) and explicitly rep-
resents the surface of the environment via TSDF-Fusion.
Some works [5, 25, 29] propose bundle adjustment(BA)
method to optimize keyframe poses and construct the dense
3D structure jointly. In contrast to previous SLAM ap-
proaches, we adopt implicit scene representation of the ge-
ometry and directly optimize them during mapping.
Implicit Scene Representation. With the proposal of
Neural radiance fields (NeRF) [18], many researchers ex-
plore taking the advantages of the implicit method into
3D reconstruction. NeRF is a ground-breaking method for
novel view synthesis using differentiable rendering. How-
ever, the representation of volume densities can not com-
mit the geometric consistency. In order to deal with it,
UNISURF [22] and NeuS [32] are proposed, combining
neural radiance fields with Signed Distance Field (SDF)
values. Other methods [1–3, 28] use various scene geom-
etry representation methods, such as truncated signed dis-
tance function, voxel grid. For large-scale representation,
Mega-NeRF and LocalRF [17, 30] use multiple local scene
representations for the entire scene.
NeRF-based SLAM. iMAP [27] and NICE-SLAM [41]
are successively proposed to combine neural implicit map-
19658
Figure 2. The isometric view of the proposed PLGSLAM system. Our system has two parallel threads: the mapping thread and the tracking
thread. In the mapping thread, we propose the progressive scene representation method for the entire scene. In local scene representation,
we combine the tri-planes with the multi-layer perceptron to improve the accuracy and smoothness. Both of them are online updated by
minimizing our carefully designed loss through differentiable rendering with the system operating. As for the tracking thread, we propose a
local-to-global bundle adjustment for accurate and robust pose estimation. Those two threads are running with an alternating optimization.
ping with SLAM. iMAP uses a single multi-layer percep-
tron (MLP) to represent the scene, and NICE-SLAM uses
a learnable hierarchical feature grid. [13, 14, 40] use se-
mantic feature embedding to improve scene representation.
Some methods [7, 39] also use 3D gaussian to improve the
scene representation. Co-SLAM [31] and ESLAM [12] are
the most relative work of our method. However, all of them
have difficult in large-scale indoor environments and long
sequences. With the proposed progressive scene represen-
tation method, we can successfully scale up to larger indoor
scenarios. The fusion of tri-planes and one-blob encoding
leads to high-fidelity and smooth surface reconstruction in
local scene. A local-to-global bundle adjustment method is
also proposed. This method can effectively eliminate grow-
ing cumulative errors in existing methods in large indoor
scenes.
3. Method
The pipeline of our system is shown in Fig. 2. We use a
set of sequential RGB-D frames {Ii, Di}M
i=1with known
camera intrinsic K∈R3×3as our input. Our model pre-
dicts camera poses {Ri|ti}M
i=1, color c, and an implicit
truncated signed distance function (TSDF) representation
ϕgthat can be used in marching cubes algorithm to ex-
tract 3D meshes. For the implicit mapping thread, a pro-
gressive scene representation method (Sec. 3.1) is designed
to represent large-scale indoor environments. Then, in the
local radiance fields, we improve the scene representation
methods and combine the tri-planes with multi-layer per-
ceptron (MLP) by our designed architecture. Sec. 3.2 walks
through the rendering process, which converts raw repre-
sentations into pixel depths, colors, and SDF values. For
the camera tracking thread, a local-to-global bundle adjust-
ment method (Sec. 3.3) is designed for robust and accuratepose estimation. Several carefully designed loss functions
are proposed to jointly optimize the scene implicit repre-
sentation and camera pose estimation. The network is in-
crementally updated with the system operation.
3.1. Progressive Scene Representation
All the existing NeRF-based SLAM systems have difficul-
ties in large-scale indoor scenes. They use a single, global
representation of the entire environment, which limits their
scene representation capacity. There are two key limitations
when modeling large-scale indoor scenes: a)the incapacity
of a single, fixed-capacity model to represent videos of ar-
bitrary length. b)the single scene representation tends to
overfit to the early data in the sequence, leading to poorer
performance in learning from the later data. c)any mis-
estimation (e.g. outlier pose) has a global impact and might
cause the false reconstruction.
Mega-NeRF and Bungee-NeRF [30, 33] pre-partition the
space for radiance fields. However, this approach is not ap-
plicable in our setting, as the camera poses in our system
are concurrently optimized alongside the mapping thread.
In our method, we dynamically create local scene rep-
resentation. Whenever the estimated camera pose trajec-
tory leaves the space of the current scene representation, we
dynamically allocate new local scene representation trained
with a small set of frames. and from there, we progressively
introduce subsequent local frames to the optimization. So,
the entire scene can be represented as multiple local scene
representations:
{Ii, Di}M
i=17→ {SR1
θ1,SR2
θ2, . . . , SRn
θn} 7→ { c, σ}(1)
where SRn
θndenotes the local scene representation, σde-
notes the volume density. Each local scene representa-
19659
tion is centered at the position of the last estimated cam-
era pose. We train each scene representation with a local
subset frames. Each subset contains some overlap frames,
which is important for achieving consistent reconstructions
in the local scene representation. Whenever the estimated
camera pose leaves the bound of the current scene repre-
sentation, we stop optimizing previous ones (freeze the net-
work parameters). At this point, we can reduce memory
requirements by removing unnecessary supervisory frames.
We also stop updating the mapping parameters in the track-
ing thread to reduce errors. If the estimated camera pose
is outside the current bounds, but within a previous local
scene representation, we activate the previous one and pro-
ceed with the optimization process. We further increase the
global consistency by inverse distance weight (IDW) fusion
for all overlapping scene representations at any supervising
frame.
Local Scene Representation. V oxel grid-based architec-
tures [10, 31, 41] are the mainstream in NeRF-based SLAM
system. However, they struggle with cubical memory grow-
ing and real-time performance. Inspired by [12], we design
a parametric-coordinate joint encoding method. Parametric
encoding is tri-plane encoding, and the coordinate encoding
is the one-blob encoding with MLP. We store and optimize
high-frequency features(e.g. texture) on perpendicular axis-
aligned planes. The one-blob encoding with MLPs are used
to encode and store low-frequency features for the coher-
ence and smoothness priors. This joint scene representation
architecture achieve high-fidelity and smoothness scene re-
construction with the ability of hole filling.
Specifically, the tri-planes are at two scales, i.e., coarse
and fine. The tri-planes feature T(x)can be formulated as:
tc(x) =Tc
xy(x) +Tc
xz(x) +Tc
yz(x)
tf(x) =Tf
xy(x) +Tf
xz(x) +Tf
yz(x)
T(x) =Concat 
tc(x);tf(x)
(2)
where tc(x), tf(x)denote the coarse and fine feature form
tri-planes. xis the world coordinate. {Tc
xy, Tc
xz, Tc
yz}
represent the three coarse geometry feature planes, and
{Tf
xy, Tf
xz, Tf
yz}represent the three fine geometry feature
planes.
For a sample point x, we use bilinearly interpolating the
nearest neighbors on each feature plane. Then, we sum the
interpolated coarse features and the fine, respectively, into
the coarse output and fine output. At last, we concatenate
the outputs together as the tri-plane features. The geome-
try decoder outputs the predicted SDF value ϕg(x)and a
feature vector z:
fg(γ(x),T(x))→(z, ϕg(x)) (3)
where γ(x)represents coordinate position encoding. zis
the latent code. We use one-blob encoding [31] insteadof embedding spatial coordinates into multiple frequency
bands. Finally, the color decoder predicts the RGB value:
fc(γ(x),z,a(x))7→ϕa(x) (4)
ϕa(x)represents the color of the sample points. a(x)is
the is the appearence feature from tri-planes. Combining
the MLP with the tri-planes scene representation, our archi-
tecture achieve accurate and smooth surface reconstruction,
efficient memory use, and hole filling performance.
3.2. Differentiable Rendering
Inspired by the recent success of volume rendering in
NeRF [18], we also propose to use a differentiable ren-
dering process to integrate the predicted density and col-
ors from our scene representation. We determine a ray
r(t) =o+tdwhose origin is at the camera center of projec-
tiono, ray direction r. We uniformly sample K points. The
sample bound is within the near and far planes tk∈[tn, tf],
k∈ {1, . . . , K }with depth values {d1, . . . ,dK}and pre-
dicted colors {c1, ...,cK}. For all sample points along rays,
we query TSDF ϕg(pk)and raw color ϕa(pk)from our net-
works and use the SDF-Based rendering approach to con-
vert SDF values to volume densities:
σ(xk) =1
β·Sigmoid−ϕg(xk)
β
(5)
where β∈Ris a learnable parameter that controls the
sharpness of the surface boundary. Then we define the ter-
mination probability wk, depth ˆd, and color ˆcas:
wk= exp 
−n−1X
m=1σ(xm)!
(1−exp (−σ(xk)))
ˆc=NX
k=1wkϕa(xk)and ˆd=NX
k=1wktk(6)
3.3. Local-to-global Bundle Adjustment
Currently, existing NeRF-based SLAM methods exhibit
poor accuracy in large-scale indoor scene localization.
Their tracking networks are performed via minimizing rgb
loss functions with respect to learnable parameters θto es-
timate the relative pose matrix {Ri|ti} ∈SE(3). With the
growing cumulative error εof pose estimation, those meth-
ods result in failure in large-scale indoor scenes and long
videos. To this end, we design a local-to-global bundle ad-
justment method to solve this problem, which performs well
with our progressive scene representation. We design our
method by drawing inspiration from traditional keyframe-
based SLAM systems for improving the robustness and ac-
curacy of pose estimation. We propose neural warpping er-
ror and reprojection error for local-to-global bundle adjust-
ment. The neural warpping loss is formulated as:
19660
Figure 3. This figure illustrates the designed neural warping loss.
We calculate the neural warpping loss between keyframe Iand
keyframe I′.
Lnwc=NX
i(F(oi, di,{Ri→i′, ti→i′})−Ci′)
Lnwd=NX
i(F(oi, di,{Ri→i′, ti→i′})−Di′)(7)
Here, Lnwc andLnwd are the neural warpping color
and depth loss. oi, didenotes the rays from image Ii.
{Ri→i′, ti→i′}denotes the relative pose from image Iito
Ii′.F()denotes our scene representation network. We
present the illustration of neural warpping loss in Fig. 3.
We formulate reprojection errors with SIFT features:
Lre=nX
i=1∥(ui′, vi′)−Π(Ri→i′Pi+ti→i′)∥ (8)
where Π(Ri→i′Pi+ti→i′)represents the reprojection of
3D point Pito the corresponding pixel (ui′, vi′)in image i′.
Whenever a keyframe arrives, we perform local bundle
adjustment in our tracking and mapping thread. A keyframe
is selected for every Kframes. When the camera moves to
the bound of the current scene representation, we also ini-
tialize it as the keyframe. In local bundle adjustment, we
only select keyframes from the local keyframe database that
visually overlap with the current frame when optimizing the
scene geometry to ensure the geometry outside the current
view remains static and fast convergence. Meanwhile, we
also maintain a global keyframe list with the operation of
our system. After accumulating a specific number of local
keyframes or the camera moves to the local bound, a global
bundle adjustment is performed. In global BA, we ran-
domly select keyframes and rays from the global keyframe
database, which leverages all historical observations of the
scene. This approach effectively integrates local and global
information which greatly improves the accuracy of camera
pose optimization in large-scale indoor scenes.
3.4. Objective Functions
Our mapping and tracking thread are performed via mini-
mizing our objective functions with respect to network pa-
rameters θand camera parameters {Ri|ti}. The color and
depth rendering losses are used in our mapping and trackingthread:
Lc=1
NNX
i=1(ˆci−Ci)2,Ld=1
|Ri|X
i∈Ri
ˆdi−Di2
(9)
where Riis the set of rays that have a valid depth observa-
tion. In addition, we design SDF loss, free space loss, and
feature smoothness losses for our mapping thread. Specifi-
cally, for samples within the truncation region, we leverage
the depth sensor measurement to approximate the signed
distance field:
Lsd f=1
|Ri|X
r∈Ri1
|Xtrr|X
x∈Xtrr(ϕg(x)·T−(Di−d))2
(10)
where Xtr
ris a set of points on the ray r that lie in the trun-
cation region, |Di−d| ≤tr. We differentiate the weights
of points that are closer to the surface Xtm
r={x|x∈
|Di−d| ≤0.4tr}from those that are at the tail of the
truncation region Xtt
rin our SDF loss.
Lsd fm=Lsd f 
Xtm
r
,Lsd ft=Lsd f 
Xtt
r
(11)
For sample points that are far from the surface |Di−d| ≥T:
Lfs=1
|Ri|X
r∈Ri1Xfs
rX
x∈Xfs
r(ϕg(x)−1)2(12)
This loss can force the SDF prediction value to be the trun-
cated distance tr. In addition, we propose feature smooth-
ness losses to prevent the noisy reconstructions caused by
tri-planes in unobserved free-space regions:
Lsmooth =1
|M|X
x∈M∆2
xy+ ∆2
xz+ ∆2
yz (13)
where ∆xy=T(x+ϵx,y)−T(x),∆xz=T(x+ϵx,z)−
T(x),∆yz=T(x+ϵy,z)−T(x)denotes the feature-
metric difference between adjacent sampled vertices on the
three feature planes. Mdenotes a small random region
form tri-planes. This loss can enhance the smoothness of
our surface reconstruction results and it is only used in map-
ping thread.
4. Experiments
We validate that our method outperforms existing im-
plicit representation-based methods in surface reconstruc-
tion, pose estimation, and real-time performance.
4.1. Datasets and Metrics
Datasets. We evaluate PLGSLAM on a variety of scenes
from different datasets. We quantitatively evaluate the re-
construction quality on 8 small room scenes from Replica
[26] ( nearly 6.5m×4.2m×2.7mwith 2000 images). We
evaluate on real-world scenes from ScanNet [4] for long se-
quences (more than 5000 images) and large-scale indoor
19661
Figure 4. Reconstruction results (without cull) on Replica [26] apartment dataset. In comparison to our baselines, our methods achieve
accurate and high-quality scene reconstruction and completion on various scenes.The region outlined on the image is marked in red to
signify lower predictive accuracy, in green to signify higher accuracy, and in yellow to represent the ground truth results. The number in
the bottom right corner of the image represents the completion ratio metric.
MethodsReconstruction Localization
Depth L1[cm] ↓Acc.[cm] ↓Comp.[cm] ↓Comp.Ratio(%) ↑ATE Mean[cm] ↓ATE RMSE[cm] ↓
iMAP [27] 4.645 3.624 4.934 80.515 3.118 4.153
NICE-SLAM [41] 1.903 2.373 2.645 91.137 1.795 2.503
V ox-Fusion [36] 2.913 1.882 2.563 90.936 1.067 1.453
ESLAM [12] 0.945 2.082 1.754 96.427 0.565 0.707
Co-SLAM [31] 1.513 2.104 2.082 93.435 0.935 1.059
Ours 0.771 1.793 1.543 97.877 0.525 0.635
Table 1. Quantitative results of our proposed PLGSLAM with existing NeRF-based SLAM system on the Replica dataset [26]. We evaluate
reconstruction and localization performance in small room scenes. The results are the average on the scenes of the Replica dataset. Our
method outperforms the existing method in surface reconstruction and pose estimation.
MethodsReconstruction[cm] Localization[cm]
Acc. Comp. Comp.Ratio(%) Mean RMSE
NICE-SLAM[41] 29.17 4.45 67.97 8.78 9.63
ESLAM[12] 26.22 4.53 71.43 7.89 8.95
Co-SLAM[31] 26.55 4.67 70.34 7.67 8.75
Ours 19.42 4.21 74.48 6.12 6.77
Table 2. Camera tracking results on the Scannet datasets [4]. We
evaluate our camera tracking performance on the Scannet dataset
to verify the effectiveness of our method. Our method achieves
high-fidelity surface reconstructions and superior camera tracking.
scenarios ( nearly 7.5m×6.6m×3.5m). We also evalu-
ate on Apartment dataset of the multi-rooms scene ( nearly
14.5m×7.5m×3.8mwith more than 12000 images) from
NICE-SLAM [41].
Metrics. We use Depth L1 (cm), Accuracy (cm), Comple-
tion (cm), and Completion ratio (%) to evaluate the recon-
struction quality. Following NICE-SLAM and ESLAM[12,41], we perform frustum and occlusion mesh culling that
removes unobserved regions outside frustum and the noisy
points within the camera frustum but outside the target
scene. However, this simple strategy removes too many
meshes, leading to excessive holes and ineffective assess-
ment of the reconstruction results. For the evaluation of
camera tracking, we adopt ATE RMSE and Mean(cm).
Implementation We run PLGSLAM on a desktop PC with
NVIDIA RTX 3090ti GPU. We employ feature planes with
a resolution of 24 cm for coarse tri-planes. We use 6 cm
resolution for fine tri-planes. All feature planes have 32
channels, resulting in a 64-channel concatenated feature in-
put for the decoders. The decoders are two-layer MLPs with
32 channels in the hidden layer. For Replica [26], we sam-
pleN= 32 points for stratified sampling and Nsurface = 8
points for importance sampling on each ray. And for Scan-
19662
MethodsReconstruction Localization
Depth L1[cm] ↓Acc.[cm] ↓Comp.[cm] ↓Comp.Ratio(%) ↑ATE Mean[cm] ↓ATE RMSE[cm] ↓
iMAP [27] 24.558 14.296 7.476 44.422 9.963 10.612
NICE-SLAM [41] 37.052 6.064 5.576 71.792 4.776 5.394
V ox-Fusion [36] 43.077 26.375 9.454 49.554 11.473 12.754
ESLAM [12] 16.355 17.546 4.301 71.626 6.637 7.283
Co-SLAM [31] 6.702 13.355 3.666 80.486 6.182 6.891
Ours 6.033 11.086 3.261 85.357 5.574 6.228
Table 3. Quantitative results of our proposed PLGSLAM with existing NeRF-based SLAM system on the Apartment dataset [41]. We
evaluate reconstruction and localization performance in large-scale multi-room scenes. The results are the average of three runs. Our
method outperforms the existing method in surface reconstruction and pose estimation.
Figure 5. Qualitative comparison of our proposed PLGSLAM method’s surface reconstruction and localization accuracy with existing
NeRF-based dense visual SLAM methods, NICE-SLAM [41], Co-SLAM [31], and ESLAM [12] on the ScanNet dataset [4]. The ground
truth camera trajectory is shown in blue, and the estimated trajectory is shown in red. Our method predicts more accurate camera trajectories
and does not suffer from drifting issues. We also visualize the Absolute Trajectory Error ATE (bottom color bar) of different methods. The
color bar on the right shows the relative scaling of color. It should also be noted that our method runs faster on this dataset.
Figure 6. Completion ratio vs. model size and average time for
PLGSLAM with other methods. Each model corresponds to a dif-
ferent hash-table or tri-planes size.
Net [4], we set N= 48 and Nsurface = 8. For further details
of our implementation, refer to the supplementary.4.2. Experimental Results
Replica dataset . We evaluate on the same RGB-D se-
quences as ESLAM [12] and Co-SLAM [31]. We use this
dataset to test our system performance in small room scenes
(nearly 6.5m×4.2m×2.7m). As shown in Tab. 1, our
method achieves higher reconstruction and pose estimation
accuracy. We show the qualitative results in Fig. 4. We
can see that ESLAM maintains more reconstruction details,
but the results contain some artifacts. Co-SLAM achieves
smooth completion in unobserved areas, but the accuracy
of the reconstruction and pose estimation is relatively low.
Our method successfully achieves consistent completion as
well as high-fidelity reconstruction results.
Scannet dataset. We evaluate the camera tracking and re-
19663
Method Speed FPT(s) Memory Grow.R.
Replica[26]NICE-SLAM[41] 2.10 O(L3)
ESLAM[12] 0.18 O(L2)
Co-SLAM[31] 0.16 O(L3)
Ours 0.14 O(L2)
Scannet[4]NICE-SLAM[41] 3.35 O(L3)
ESLAM[12] 0.55 O(L2)
Co-SLAM[31] 0.38 O(L3)
Ours 0.37 O(L2)
Table 4. Runtime analysis of our method in comparison with exist-
ing ones in terms of average frame processing time (AFPT), and
model size growth rate w.r.t. scene side length L. We evaluate
these method on replica dataset [26] and Scannet dataset [4]. Our
method is greatly faster and the model size grow is significantly
reduced from cubic to square.
MethodsReconstruction[cm] Localization[cm]
Acc. Comp. Comp.Ratio(%) Mean RMSE
w/o joint enc. 13.314 4.687 81.347 5.935 6.787
w/o prog. 12.754 4.231 83.156 5.875 6.693
w/o lg BA 12.435 4.181 83.473 5.874 6.591
Ours 11.086 3.261 85.357 5.574 6.228
Table 5. Ablation study. We conduct experiments on Apartment
dataset [41] to verify the effectiveness of our method. Our full
model achieves better completion reconstructions and more accu-
rate pose estimation results.
construction results of PLGSLAM on real-world large room
sequences (nearly 7.5m×6.6m×2.7m) from ScanNet [4]
. We use the absolute trajectory error (ATE) as our met-
ric. Tab. 2 shows that our method achieves better pose es-
timation and surface reconstruction results in comparison
to NICE-SLAM [41], ESLAM [12], and Co-SLAM [31].
PLGSLAM exhibits superior scene representation capabil-
ities and more accurate and robust tracking performance in
large-scale indoor scenes. Fig. [4] also shows PLGSLAM
achieves better reconstruction quality with smoother results
and finer details.
Apartment dataset. We evaluate the surface reconstruction
and camera tracking accuracy of PLGSLAM on Apartment
dataset ( nearly 14.5m×7.5m×3.2m). Tab. 3 shows that
quantitatively, our method achieves SOTA tracking results
in comparison to Co-SLAM and ESLAM. These algorithms
typically exhibit significant cumulative errors in large-scale
indoor dataset scenarios. Fig. 1 also shows PLGSLAM
achieves better reconstruction quality with smoother results
and finer details.
4.3. Runtime analysis
In this section, we analysis the speed and memory usage of
our method compared with other SOTA methods in Replica
datasets [26] and ScanNet datasets [4]. We report the av-
erage frame pocessing time (FPT) and the memory growthrate in Tab. 4. The results indicate that our method is faster
than previous methods and the model size does not grow cu-
bically with the scene length. In Fig. 6, we present the com-
pletion ratio under different model size and memory usage.
We visualized the variation curve of the completion ratio by
altering the size of the hash table/triplanes.
4.4. Ablation Study
In this section, we conduct various experiments to verify the
effectiveness of our method. Tab. 5 illustrates a quantitative
evaluation with different settings.
Joint scene representation. It is obvious that the joint
scene representation (tri-planes with MLP) significantly im-
proves our surface reconstruction accuracy.
Progressive scene representation. We replace our pro-
gressive scene representation and use a single network for
the entire scene. We can observe that this method has a
great influence on pose estimation and reconstruction met-
rics. This network significantly improves the capacity of
scene geometry representation and enhances the robustness
for local misestimation.
Local-to-global bundle adjustment. We remove our local-
to-global bundle adjustment in this experiment. Our full
model leads to higher accuracy and better completion. The
local-to-global BA can significantly reduce the growing cu-
mulative error and improve the robustness and accuracy of
the camera tracking.
5. Conclusion
In this paper, we propose a novel dense SLAM system,
PLGSLAM, which achieve accurate surface reconstruction
and pose estimation in large indoor scenes. Our progressive
scene representation method enables our system to repre-
sent large-scale indoor scenes and long videos. The joint
encoding method with the tri-planes and multi-layer per-
ceptron further improves the accuracy of local scene repre-
sentation. The local-to-global bundle adjustment method
combines the traditional SLAM method with end-to-end
pose estimation, which achieves robust and accurate cam-
era tracking and mitigate the influence of cumulative error
and pose drift. Our extensive experiments demonstrate the
effectiveness and accuracy of our system in both scene re-
construction, depth estimation, and pose estimation.
Acknowledgement This work is supported by
the National Key R&D Program of China (Grant
2020YFC2007500), the National Natural Science Foun-
dation of China (Grant U1813206), and the Science
and Technology Commission of Shanghai Municipality
(Grant 20DZ2220400). Authors gratefully appreciate the
contribution of Yanbo Wang from Shanghai Jiao Tong
University, Hengyi Wang from University College London.
19664
References
[1] Dejan Azinovi ´c, Ricardo Martin-Brualla, Dan B Goldman,
Matthias Nießner, and Justus Thies. Neural rgb-d surface
reconstruction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
6290–6301, June 2022. 2
[2] Aljaz Bozic, Pablo Palafox, Justus Thies, Angela Dai, and
Matthias Nießner. Transformerfusion: Monocular rgb scene
reconstruction using transformers. Advances in Neural In-
formation Processing Systems , 34:1403–1414, 2021.
[3] Jaesung Choe, Sunghoon Im, Francois Rameau, Minjun
Kang, and In So Kweon. V olumefusion: Deep depth fusion
for 3d scene reconstruction. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
16086–16095, October 2021. 2
[4] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Niessner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , July 2017. 5, 6, 7, 8
[5] Angela Dai, Matthias Nießner, Michael Zollhöfer, Shahram
Izadi, and Christian Theobalt. Bundlefusion: Real-time
globally consistent 3d reconstruction using on-the-fly surface
reintegration. ACM Trans. Graph. , 36(4), jul 2017. 2
[6] Yang Tang Debao Huang and Rongjun Qin. An evaluation of
planetscope images for 3d reconstruction and change detec-
tion – experimental validations with case studies. GIScience
& Remote Sensing , 59(1):744–761, 2022. 1
[7] Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang,
Shenghai Yuan, Danwei Wang, and Weidong Chen. Compact
3d gaussian splatting for dense visual slam. arXiv preprint
arXiv:2403.11247 , 2024. 3
[8] Tianchen Deng, Siyang Liu, Xuan Wang, Yejia Liu, Danwei
Wang, and Weidong Chen. Prosgnerf: Progressive dynamic
neural scene graph with frequency modulated auto-encoder
in urban scenes. arXiv preprint arXiv:2312.09076 , 2023. 1
[9] Tianchen Deng, Hongle Xie, Jingchuan Wang, and Wei-
dong Chen. Long-term visual simultaneous localization and
mapping: Using a bayesian persistence filter-based global
map prediction. IEEE Robotics & Automation Magazine ,
30(1):36–49, 2023. 2
[10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5501–5510, 2022. 4
[11] Shahram Izadi, David Kim, Otmar Hilliges, David
Molyneaux, Richard Newcombe, Pushmeet Kohli, Jamie
Shotton, Steve Hodges, Dustin Freeman, Andrew Davison,
et al. Kinectfusion: real-time 3d reconstruction and inter-
action using a moving depth camera. In Proceedings of the
24th annual ACM symposium on User interface software and
technology , pages 559–568, 2011. 1, 2
[12] Mohammad Mahdi Johari, Camilla Carta, and François
Fleuret. Eslam: Efficient dense slam system based on hy-
brid representation of signed distance fields. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 17408–17419, 2023. 1, 3, 4, 6, 7,8
[13] Mingrui Li, Jiaming He, Guangan Jiang, and Hongyu
Wang. Ddn-slam: Real-time dense dynamic neural im-
plicit slam with joint semantic encoding. arXiv preprint
arXiv:2401.01545 , 2024. 3
[14] Mingrui Li, Shuhong Liu, and Heng Zhou. Sgs-slam: Se-
mantic gaussian splatting for neural dense slam. arXiv
preprint arXiv:2402.03246 , 2024. 3
[15] Jiuming Liu, Guangming Wang, Chaokang Jiang, Zhe Liu,
and Hesheng Wang. Translo: A window-based masked point
transformer framework for large-scale lidar odometry. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence , volume 37, pages 1683–1691, 2023. 2
[16] Jiuming Liu, Guangming Wang, Zhe Liu, Chaokang Jiang,
Marc Pollefeys, and Hesheng Wang. Regformer: An effi-
cient projection-aware transformer network for large-scale
point cloud registration. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
8451–8460, October 2023. 2
[17] Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang,
Changil Kim, Min H Kim, and Johannes Kopf. Progressively
optimized local radiance fields for robust view synthesis. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 16539–16548, 2023. 2
[18] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020. 2, 4
[19] Raúl Mur-Artal, J. M. M. Montiel, and Juan D. Tardós. Orb-
slam: A versatile and accurate monocular slam system. IEEE
Transactions on Robotics , 31(5):1147–1163, 2015. 1, 2
[20] Raúl Mur-Artal and Juan D. Tardós. Orb-slam2: An open-
source slam system for monocular, stereo, and rgb-d cam-
eras. IEEE Transactions on Robotics , 33(5):1255–1262,
2017. 1
[21] Richard A Newcombe, Steven J Lovegrove, and Andrew J
Davison. Dtam: Dense tracking and mapping in real-time.
In2011 international conference on computer vision , pages
2320–2327. IEEE, 2011. 2
[22] Michael Oechsle, Songyou Peng, and Andreas Geiger.
Unisurf: Unifying neural implicit surfaces and radiance
fields for multi-view reconstruction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 5589–5599, October 2021. 2
[23] Guohao Peng, Jun Zhang, Heshan Li, and Danwei Wang. At-
tentional pyramid pooling of salient visual residuals for place
recognition. In 2021 IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 865–874, 2021. 2
[24] Tong Qin, Peiliang Li, and Shaojie Shen. Vins-mono: A
robust and versatile monocular visual-inertial state estimator.
IEEE Transactions on Robotics , 34(4):1004–1020, 2018. 1,
2
[25] Thomas Schops, Torsten Sattler, and Marc Pollefeys. Bad
slam: Bundle adjusted direct rgb-d slam. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2019. 1, 2
[26] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik
Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl
Ren, Shobhit Verma, et al. The replica dataset: A digital
19665
replica of indoor spaces. arXiv preprint arXiv:1906.05797 ,
2019. 5, 6, 8
[27] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J. Davi-
son. imap: Implicit mapping and positioning in real-time. In
ICCV , pages 6229–6238, October 2021. 1, 2, 6, 7
[28] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and
Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruc-
tion from monocular video. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 15598–15607, June 2021. 2
[29] Chengzhou Tang and Ping Tan. Ba-net: Dense bundle ad-
justment network. ICLR , 2018. 2
[30] Haithem Turki, Deva Ramanan, and Mahadev Satya-
narayanan. Mega-nerf: Scalable construction of large-
scale nerfs for virtual fly-throughs. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12922–12931, 2022. 2, 3
[31] Hengyi Wang, Jingwen Wang, and Lourdes Agapito. Co-
slam: Joint coordinate and sparse parametric encodings for
neural real-time slam. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
13293–13302, 2023. 1, 3, 4, 6, 7, 8
[32] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and
J. Wortman Vaughan, editors, Advances in Neural Informa-
tion Processing Systems , volume 34, pages 27171–27183.
Curran Associates, Inc., 2021. 2
[33] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,
Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.
Bungeenerf: Progressive neural radiance field for extreme
multi-scale scene rendering. In European conference on
computer vision , pages 106–122. Springer, 2022. 3
[34] Hongle Xie, Tianchen Deng, Jingchuan Wang, and Weidong
Chen. Robust incremental long-term visual topological lo-
calization in changing environments. IEEE Transactions on
Instrumentation and Measurement , 72:1–14, 2022. 2
[35] Hongle Xie, Tianchen Deng, Jingchuan Wang, and Weidong
Chen. Angular tracking consistency guided fast feature as-
sociation for visual-inertial slam. IEEE Transactions on In-
strumentation and Measurement , 2024. 2
[36] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian
Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and
mapping with voxel-based neural implicit representation. In
2022 IEEE International Symposium on Mixed and Aug-
mented Reality (ISMAR) , pages 499–507, 2022. 6, 7
[37] Jun Zhang et al. Ntu4dradlm: 4d radar-centric multi-modal
dataset for localization and mapping. In 2023 IEEE 26th
International Conference on Intelligent Transportation Sys-
tems (ITSC) , pages 4291–4296. IEEE, 2023. 2
[38] Jun Zhang and Danwei others. 4dradarslam: A 4d imag-
ing radar slam system for large-scale environments based on
pose graph optimization. In 2023 IEEE International Con-
ference on Robotics and Automation (ICRA) , pages 8333–
8340. IEEE, 2023. 2
[39] Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, and
Hesheng Wang. Semgauss-slam: Dense semantic gaussian
splatting slam. arXiv preprint arXiv:2403.07494 , 2024. 3[40] Siting Zhu, Guangming Wang, Hermann Blum, Jiuming
Liu, Liang Song, Marc Pollefeys, and Hesheng Wang.
Sni-slam: Semantic neural implicit slam. arXiv preprint
arXiv:2311.11016 , 2023. 3
[41] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hu-
jun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Polle-
feys. Nice-slam: Neural implicit scalable encoding for slam.
InCVPR , pages 12786–12796, June 2022. 1, 2, 4, 6, 7, 8
19666
