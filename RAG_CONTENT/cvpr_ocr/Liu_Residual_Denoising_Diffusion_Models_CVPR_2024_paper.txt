Residual Denoising Diffusion Models
Jiawei Liu1,2,3, Qiang Wang1,4, Huijie Fan1,2*, Yinong Wang5, Yandong Tang1,2, Liangqiong Qu5*
1State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences
2Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Science
3University of Chinese Academy of Sciences4Shenyang University5The University of Hong Kong
{liujiawei,wangqiang,fanhuijie,ytang }@sia.cn, liangqqu@hku.hk
Abstract
We propose residual denoising diffusion models
(RDDM), a novel dual diffusion process that decouples
the traditional single denoising diffusion process into
residual diffusion and noise diffusion. This dual diffusion
framework expands the denoising-based diffusion models,
initially uninterpretable for image restoration, into a
uniﬁed and interpretable model for both image generation
and restoration by introducing residuals. Speciﬁcally, our
residual diffusion represents directional diffusion from the
target image to the degraded input image and explicitly
guides the reverse generation process for image restoration,
while noise diffusion represents random perturbations in
the diffusion process. The residual prioritizes certainty,
while the noise emphasizes diversity, enabling RDDM to
effectively unify tasks with varying certainty or diversity re-
quirements, such as image generation and restoration. We
demonstrate that our sampling process is consistent with
that of DDPM and DDIM through coefﬁcient transforma-
tion, and propose a partially path-independent generation
process to better understand the reverse process. Notably,
our RDDM enables a generic UNet, trained with only
an L1 loss and a batch size of 1, to compete with state-
of-the-art image restoration methods. We provide code
and pre-trained models to encourage further exploration,
application, and development of our innovative framework
(https://github.com/nachifur/RDDM ).
1. Introduction
In real-life scenarios, diffusion often occurs in complex
forms involving multiple, concurrent processes, such as the
dispersion of multiple gases or the propagation of different
types of waves or ﬁelds. This leads us to ponder whether
the denoising-based diffusion models [ 17,51] have limita-
tions in focusing solely on denoising. Current diffusion-
*Corresponding author.
Noise domainTarget 
domain
(a) DDPMInput domainNoise 
domainTarget 
domain
Residual + NoiseResidual
Noise-carrying 
input domain
(b) OursFigure 1. Denoising diffusion process - DDPM [ 17] (a) and our
residual denoising diffusion process (b). For image restoration,
we introduce residual diffusion to represent the diffusion direction
from the target image to the input image.
based image restoration methods [ 22,39,48,49,82] extend
the diffusion model to image restoration tasks by using de-
graded images as a condition input to implicitly guide the
reverse generation process, without modifying the original
denoising diffusion process [ 17,51]. However, the reverse
process starting from noise seems to be unnecessary, as the
degraded image is already known. The forward process
is non-interpretability for image restoration, as the diffu-
sion process does not contain any information about the de-
graded image, as shown in Fig. 1(a).
In this paper, we explore a novel dual diffusion pro-
cess and propose Residual Denoising Diffusion Models
(RDDM), which can tackle the non-interpretability of a sin-
gle denoising process for image restoration. In RDDM, we
decouple the previous diffusion process into residual diffu-
sion and noise diffusion. Residual diffusion prioritizes cer-
tainty and represents a directional diffusion from the target
image to the conditional input image, and noise diffusion
emphasizes diversity and represents random perturbations
in the diffusion process. Thus, our RDDM can unify dif-
ferent tasks that require different certainty or diversity, e.g.,
image generation and restoration. Compared to denoising-
based diffusion models for image restoration, the residuals
in RDDM clearly indicate the forward diffusion direction
and explicitly guide the reverse generation process for im-
age restoration, as shown in Fig. 1(b).
Speciﬁcally, we redeﬁne a new forward process that al-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2773
0.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0
First remove residualsThen remove noiseSimultaneous removal 
of residuals and noiseҧ𝛽𝑡
ത 𝛼𝑡Figure 2. Decoupled dual diffusion framework. The previous for-
ward diffusion process is decoupled into residual diffusion and
noise diffusion, while in the reverse process, the simultaneous
sampling can be decoupled into ﬁrst removing the residuals and
then removing noise.
lows simultaneous diffusion of residuals and noise, wherein
the target image progressively diffuses into a purely noisy
image for image generation or a noise-carrying input image
for image restoration. Unlike the previous denoising diffu-
sion model [ 17,51], which uses one coefﬁcient schedule to
control the mixing ratio of noise and images, our RDDM
employs two independent coefﬁcient schedules to control
the diffusion speed of residuals and noise. We found that
this independent diffusion property is also evident in the
reverse generation process, e.g., readjusting the coefﬁcient
schedule within a certain range during testing does not af-
fect the image generation results, and removing the residu-
als ﬁrstly, followed by denoising (see Fig. 2), can also pro-
duce semantically consistent images. Our RDDM is com-
patible with widely used denoising diffusion models, i.e.,
our sampling process is consistent with that of DDPM [ 17]
and DDIM [ 51] by transforming coefﬁcient schedules. In
addition, our RDDM natively supports conditional inputs,
enabling networks trained with only an ℓ1loss and a batch
size of 1 to compete with state-of-the-art image restoration
methods. We envision that our models can facilitate a uni-
ﬁed and interpretable image-to-image distribution transfor-
mation methodology, highlighting that residuals and noise
are equally important for diffusion models, e.g., the residual
prioritizes certainty while the noise emphasizes diversity.
The contributions of this paper are summarized as follows:
• We propose a novel dual diffusion framework to tackle
the non-interpretability of a single denoising process for
image restoration by introducing residuals. Our residual
diffusion represents a directional diffusion from the target
image to the conditional input image.
• We introduce a partially path-independent generation pro-
cess that decouples residuals and noise, highlighting their
roles in controlling directional residual shift (certainty)and random perturbation (diversity), respectively.
• We design an automatic objective selection algorithm to
choose whether to predict residuals or noise for unknown
new tasks.
• Extensive experiments demonstrate that our method can
be adapted to different tasks, e.g., image generation,
restoration, inpainting and translation, focusing certainty
or diversity, and involving paired or unpaired data.
2. Related Work
Denoising diffusion models (e.g., DDPM [ 17], SGM [ 52,
53], and DDIM [ 51]) were initially developed for image
generation. Subsequent image restoration methods [ 14,39,
48] based on DDPM and DDIM feed a degraded image as a
conditional input to a denoising network, e.g., DvSR [ 62],
SR3 [ 49], and WeatherDiffusion [ 82], which typically re-
quire large sampling steps and batch sizes. Additionally,
the reverse process starting from noise in these methods
seems unnecessary and inefﬁcient for image restoration
tasks. Thus, SDEdit [ 41], ColdDiffusion [ 2], InDI [ 11],
and I2SB [ 29] propose generating a clear image directly
from a degraded image or noise-carrying degraded image.
InDI [ 11] and I2SB [ 29], which also present uniﬁed image
generation and restoration frameworks, are the most closely
related to our proposed RDDM. Speciﬁcally, the forward
diffusion of InDI, I2SB, and our RDDM consistently em-
ploys a mixture of three terms (i.e., input images Iin, target
imagesI0, and noise ϵ), extending beyond the denoising-
based diffusion model [ 17,51] which incorporates a mix-
ture of two terms (i.e., I0andϵ). However, InDI [ 11] and
I2SB [ 29] opt for estimating the target image or its linear
transformation term to replace the noise estimation, akin to
a special case of our RDDM (SM-Res). In contrast, we
introduce residual estimation while also embracing noise
for both generation and restoration tasks. Our RDDM can
further extend DDPM [ 17], DDIM [ 51], InDI [ 11], and
I2SB [ 29] to independent double diffusion processes, and
pave the way for the multi-dimensional diffusion process.
We highlight that residuals and noise are equally impor-
tant, e.g., the residual prioritizes certainty while the noise
emphasizes diversity. In addition, our work is related to
coefﬁcient schedule design [ 44,48], variance strategy op-
timization [ 3,4,24,44], superimposed image decomposi-
tion [ 12,81], curve integration [ 47], stochastic differential
equations [ 53], and residual learning [ 15] for image restora-
tion [ 1,32,56,70,72,75]. See Appendix A.5for detailed
comparison.
3. Background
Denoising diffusion models [ 17,50] aim to learn a distri-
butionpθ(I0) :=/integraltext
pθ(I0:T)dI1:T1to approximate a tar-
1To understand diffusion from an image perspective, we use Iinstead
ofxin DDPM [ 17].
2774
0𝐼𝑇 𝐼𝑡−1 𝐼𝑡 𝐼0 ⋯𝑝𝜃(𝐼𝑡−1|𝐼𝑡)
𝑞(𝐼 𝑡|𝐼𝑡−1, 𝐼𝑟𝑒𝑠)⋯ 𝐼𝑖𝑛+ 𝜖 =
ForwardReverse
Restoration GenerationFigure 3. The proposed residual denoising diffusion model (RDDM) is a uniﬁed framework for image generation and restoration (a shadow
removal task is shown here). We introduce residuals ( Ires) in RDDM, redeﬁning the forward diffusion process to involve simultaneous
diffusion of residuals and noise. The residuals ( Ires=Iin−I0) diffusion represents the directional diffusion from the target image I0
to the degraded input image Iin, while the noise ( ϵ) diffusion represents the random perturbations in the diffusion process. In RDDM, I0
gradually diffuses into IT=Iin+ϵ,ϵ∼ N(0,I). In the third columns, ITis a purely noisy image for image generation since Iin= 0,
and a noise-carrying degraded image for image restoration as Iinis the degraded image.
get data distribution q(I0), whereI0are target images and
I1,...,I T(T= 1000 ) are latent images of the same dimen-
sion asI0. In the forward process, q(I0)is diffused into a
Gaussian noise distribution using a ﬁxed Markov chain,
q(I1:T|I0) :=/producttextT
t=1q(It|It−1), (1)
q(It|It−1) :=N(It;√αtIt−1,(1−αt)I), (2)
whereα1:T∈(0,1]T.q(It|It−1)can also be written as
It=√αtIt−1+√1−αtϵt−1. In fact, it is simpler to
sampling ItfromI0by reparameterization [ 25,26],
It=√¯αtI0+√
1−¯αtϵ, (3)
whereϵ∼N(0,I),¯αt:=/producttextt
s=1αs. The reverse process is
also a Markov chain starting at pθ(IT)∼N(IT;0,I),
pθ(I0:T) :=pθ(IT)/producttextT
t=1pθ(It−1|It), (4)
pθ(It−1|It) :=N(It−1;µθ(It,t),ΣtI), (5)
wherepθ(It−1|It)is a learnable transfer probability (the
variance schedule Σtis ﬁxed). A simpliﬁed loss func-
tion [ 17] is derived from the maximum likelihood of pθ(I0),
i.e.,L(θ) :=EI0∼q(I0),ϵ∼N(0,I)/bracketleftBig
∥ϵ−ϵθ(It,t)∥2/bracketrightBig
. The es-
timated noise ϵθcan be used to represent µθinpθ(It−1|It),
thusIt−1can be sampled from pθ(It−1|It)step by step.
4. Residual Denoising Diffusion Models
Our goal is to develop a dual diffusion process to unify
and interpret image generation and restoration. We mod-
ify the representation of IT=ϵin traditional DDPM to
IT=Iin+ϵin our RDDM, where Iinis a degraded im-
age (e.g., a shadow, low-light, or blurred image) for image
restoration and is set to 0for image generation. This modi-
ﬁcation is compatible with the widely used denoising diffu-
sion model, e.g., IT= 0+ϵis the pure noise ( ϵ) for genera-
tion. For image restoration, ITis a noisy-carrying degraded
image (Iin+ϵ), as shown in the third column in Fig. 3.The modiﬁed forward process from I0toIT=Iin+ϵin-
volves progressively degrading I0toIin, and injecting noise
ϵ. This naturally results in a dual diffusion process, a resid-
ual diffusion to model the transition from I0toIinand a
noise diffusion. For example, the forward diffusion process
from the shadow-free image I0to the noisy carrying shadow
imageITinvolves progressively adding shadows and noise,
as shown in the second row in Fig. 3.
In the following subsections, we detail the underlying
theory and the methodology behind our RDDM. Inspired
by residual learning [ 15,31,32], we redeﬁne each forward
diffusion process step in Section 4.1. For the reverse pro-
cess, we present a training objective to predict the residuals
and noise injected in the forward process in Section 4.2.
In Section 4.3, we propose three sampling methods, i.e.,
residual prediction (SM-Res), noise prediction (SM-N), and
“residual and noise prediction” (SM-Res-N).
4.1. Directional Residual Diffusion Process with
Perturbation
To model the gradual degradation of image quality and the
increment of noise, we deﬁne the single forward process
step in our RDDM as follows:
It=It−1+It
res, It
res∼N(αtIres,β2
tI),(6)
whereIt
resrepresents a directional mean shift (residual dif-
fusion) with random perturbation (noise diffusion) from
stateIt−1to stateIt, the residuals IresinIt
resis the dif-
ference between IinandI0(i.e.,Ires=Iin−I0), and
two independent coefﬁcient schedules αtandβtcontrol the
residual and noise diffusion, respectively. In fact, it is sim-
pler to sample ItfromI0(like Eq. 3),
It=It−1+αtIres+βtϵt−1,
=It−2+(αt−1+αt)Ires+(/radicalBig
β2
t−1+β2
t)ϵt−2
=...
=I0+ ¯αtIres+¯βtϵ,(7)
2775
whereϵt−1,...ϵ∼ N(0,I),¯αt=/summationtextt
i=1αiand¯βt=/radicalBig/summationtextt
i=1β2
i. Ift=T,¯αT= 1andIT=Iin+¯βTϵ.¯βTcan
control the intensity of noise perturbation for image restora-
tion (e.g., ¯β2
T= 0.01for shadow removal), while ¯β2
T= 1
for image generation. From Eq. 6, the joint probability dis-
tributions in the forward process can be deﬁned as:
q(I1:T|I0,Ires) :=/producttextT
t=1q(It|It−1,Ires), (8)
q(It|It−1,Ires) :=N(It;It−1+αtIres,β2
tI).(9)
Eq. 7deﬁnes the marginal probability distribution
q(It|I0,Ires) =N(It;I0+ ¯αtIres,¯β2
tI). In fact, the for-
ward diffusion of our RDDM is a mixture of three terms
(i.e.,I0,Ires, andϵ), extending beyond the widely used de-
noising diffusion model that is a mixture of two terms, i.e,
I0andϵ. A similar mixture form of three terms can be seen
in several concurrent works, e.g., InDI [ 11], I2SB [ 29], IR-
SDE [ 40], and ResShift [ 67].
4.2. Generation Process and Training Objective
In the forward process (Eq. 7), residuals ( Ires) and noise
(ϵ) are gradually added to I0, and then synthesized into It,
while the reverse process from ITtoI0involves the esti-
mation of the residuals and noise injected in the forward
process. We can train a residual network Iθ
res(It,t,Iin)
to predict Iresand a noise network ϵθ(It,t,Iin)to esti-
mateϵ. Using Eq. 7, we obtain the estimated target images
Iθ
0=It−¯αtIθ
res−¯βtϵθ. IfIθ
0andIθ
resare given, the
generation process is deﬁned as,
pθ(It−1|It) :=qσ(It−1|It,Iθ
0,Iθ
res), (10)
where the transfer probability qσ(It−1|It,I0,Ires)2fromIt
toIt−1is,
qσ(It−1|It,I0,Ires) =N(It−1;I0+ ¯αt−1Ires
+/radicalBig
¯β2
t−1−σ2
tIt−(I0+ ¯αtIres)
¯βt,σ2
tI),(11)
whereσ2
t=ηβ2
t¯β2
t−1/¯β2
tandηcontrols whether the gen-
eration process is random ( η= 1) or deterministic ( η= 0).
Using Eq. 10and Eq. 11,It−1can be sampled from Itvia:
It−1=It−(¯αt−¯αt−1)Iθ
res
−(¯βt−/radicalBig
¯β2
t−1−σ2
t)ϵθ+σtϵt,(12)
whereϵt∼N(0,I). Whenη= 1, our RDDM has the sum-
constrained variance, while DDPM has preserving variance
(see Appendix A.4). When η= 0 (i.e.,σt= 0), the sam-
pling process is deterministic,
It−1=It−(¯αt−¯αt−1)Iθ
res−(¯βt−¯βt−1)ϵθ.(13)
2Eq.11does not change q(It|I0,Ires)in Appendix A.2.We derive the following simpliﬁed loss function for training
(Appendix A.1):
Lres(θ) :=E/bracketleftBig
λres/vextenddouble/vextenddoubleIres−Iθ
res(It,t,Iin)/vextenddouble/vextenddouble2/bracketrightBig
,(14)
Lϵ(θ) :=E/bracketleftBig
λϵ∥ϵ−ϵθ(It,t,Iin)∥2/bracketrightBig
, (15)
where the hyperparameters λres,λϵ∈{0,1}, and the train-
ing input image Itis synthesized using I0,Ires, andϵby
Eq.7.Itcan also be synthesized using Iin(replaceI0in
Eq.7byI0=Iin−Ires),
It=Iin+(¯αt−1)Ires+¯βtϵ. (16)
4.3. Sampling Method Selection Strategies
For the generation process (from IttoIt−1),ItandIinare
known, and thus Iresandϵcan represent each other by
Eq.16. From Eq. 14,15,16, we propose three sampling
methods as follows.
SM-Res. Whenλres= 1andλϵ= 0, the residuals Iθ
resare
predicted by a network, while the noise ϵθis represented as
a transformation of Iθ
resusing Eq. 16.
SM-N. Whenλres= 0andλϵ= 1, the noise ϵθis predicted
by a network, while the residuals Iθ
resare represented as a
transformation of ϵθusing Eq. 16.
SM-Res-N. Whenλres= 1andλϵ= 1, both the residuals
and the noise are predicted by networks.
To determine the optimal sampling method for real-world
applications, we give empirical strategies and automatic se-
lection algorithms in the following.
Empirical Research. Table 1presents that the SM-Res
shows better results for image restoration but offers a poorer
FID for generation. On the other hand, the SM-N yields
better frechet inception distance (FID in [ 16]) and incep-
tion scores (IS), but is ineffective in image restoration (e.g.,
PSNR 11.34 for shadow and 16.30 for low-light). This may
be due to the inadequacy of using ϵθto represent Iθ
resin
Eq.16for restoration tasks. We attribute these inconsistent
results to the fact that residual predictions prioritize cer-
tainty, whereas noise predictions emphasize diversity . In
our experiments, we use SM-N for image generation, SM-
Res for low-light (LOL [ 61]), and SM-Res-N for other im-
age restoration tasks. For an unknown new task, we empiri-
cally recommend using SM-N for those requiring greater di-
versity and SM-Res for tasks that demand higher certainty.
Automatic Objective Selection Algorithm (AOSA). To
automatically choose between SM-Res or SM-N for an un-
known task, we develop an automatic sampling selection
algorithm in Appendix B.2. This algorithm requires only
a single network and learns the hyperparameter in Eq. 15,
enabling a gradual transition from combined residual and
noise training (akin to SM-Res-N) to individual prediction
(SM-Res or SM-N). This plug-and-play training strategy re-
quires less than 1000 additional training iterations and is
2776
Sampling MethodGeneration (CelebA) Shadow removal (ISTD) Low-light (LOL) Deraining (RainDrop)
FID (↓) IS (↑) MAE(↓) PSNR(↑) SSIM(↑)PSNR(↑) SSIM(↑)PSNR(↑) SSIM(↑)
SM-Res 31.47 1.73 4.76 30.72 0.959 25.39 0.937 31.96 0.9509
SM-N 23.25 2.05 81.01 11.34 0.175 16.30 0.649 19.15 0.7179
SM-Res-N 28.90 1.78 4.67 30.91 0.962 23.90 0.931 32.51 0.9563
Table 1. Sampling method analysis. The sampling steps are 10 on the CelebA 64 ×64 [36] dataset, 5 on the ISTD [ 57] dataset, 2 on the
LOL [ 61] dataset, and 5 on the RainDrop [ 45] dataset.
fully compatible with the current denoising-based diffusion
methods [ 17]. Our RDDM using AOSA has the potential to
provide a uniﬁed and interpretable methodology for mod-
eling, training, and inference pipelines for unknown target
tasks.
Comparison with Other Prediction Methods. Our
SM-N is similar to DDIM [ 51] (or DDPM [ 17]), which
only estimates the noise, and is consistent with DDPM and
DDIM by transforming the coefﬁcient/variance schedules
in Eq. 12(the proof in Appendix A.3),
¯αt= 1−/radicalBig
¯αt
DDIM3,¯βt=/radicalBig
1−¯αt
DDIM,
σ2
t=σ2
t(DDIM).(17)
In fact, current research has delved into numerous dif-
fusion forms that extend beyond noise estimation. For ex-
ample, IDDPM [ 44] proposes that it is feasible to estimate
noise (ϵθ), clean target images ( Iθ
0), or the mean term ( µθ)
to represent the transfer probabilities (i.e., pθ(It−1|It)in
Eq.5). The score-based generative model (SGM) [ 52] and
Schr ¨odinger Bridge (I2SB [ 29]) estimate the score of noisy
data (i.e., the sum of residuals and noise/summationtextt
i=1It
res). Cold-
Diffusion [ 2] and InDI [ 11] estimate the clean target im-
ages (I0). Rectiﬁed Flow [ 35] predicts the residuals ( Ires)
to align with the image linear interpolation process without
noise diffusion (i.e., IT=Iin). A detailed comparison can
be found in Appendix A.5.
These previous/concurrent works choose to estimate the
noise, the residual, the target image, or its linear transfor-
mation term. In contrast, we introduce residual estimation
while also embracing noise for both generation and restora-
tion. Residuals and noise have equal and independent sta-
tus, which is reﬂected in the forward process (Eq. 7), the re-
verse process (Eq. 13), and the loss function (Eq. 15). This
independence means that the noise diffusion can even be
removed and only the residual diffusion retained to model
the image interpolation process (when ¯βT= 0 in Eq. 7,
RDDM degenerates to Rectiﬁed Flow [ 35]). In addition,
this property derives a decoupled dual diffusion framework
in Section 5.
5. Decoupled Dual Diffusion Framework
Upon examining DDPM from the perspective of RDDM,
we discover that DDPM indeed involves the simultaneous
3¯αt
DDIMhere isαtof DDIM [ 51].Schedules FID ( ↓) IS (↑)
Linear (DDIM [ 51]) 28.3942.05
Scaled linear [ 48] 28.15 2.00
Squared cosine [ 44] 47.21 2.64
αt(mean),β2
t(mean) 38.35 2.22
αt(linearly increasing), β2
t(linearly increasing) 40.03 2.45
αt(linearly decreasing), β2
t(linearly decreasing) 27.82 2.26
αt(linearly decreasing), β2
t(linearly increasing) 23.25 2.05
Table 2. Coefﬁcient schedules analysis on CelebA ( 64×64) [36].
In our RDDM, the residual diffusion and noise diffusion are de-
coupled, so one may design a better schedule in the decoupled
coefﬁcient space, e.g., αt(linearly decreasing), β2
t(linearly in-
creasing). To be fair, all coefﬁcient schedules were retrained using
the same network structure, training, and evaluation. The sampling
method is SM-N with 10 sampling steps using Eq. 13.
diffusion of residuals and noise, which is evident as Eq. 48
becomes equivalent to Eq. 44in Appendix A.3. We ﬁnd that
it is possible to decouple these two types of diffusion. Sec-
tion 5.1presents a decoupled forward diffusion process. In
Section 5.2, we propose a partially path-independent gener-
ation process and decouple the simultaneous sampling into
ﬁrst removing the residuals and then removing noise (see
Fig.6(d) and Fig. 17). This decoupled dual diffusion frame-
work sheds light on the roles of deresidual and denoising in
the DDPM generation process.
5.1. Decoupled Forward Diffusion Process
Our deﬁned coefﬁcients ( αt,β2
t) offer a distinct physical in-
terpretation. In the forward diffusion process (Eq. 7),αt
controls the speed of residual diffusion and β2
tregulates the
speed of noise diffusion. In the reverse generation process
(Eq. 13),¯αtand¯βtare associated with the speed of remov-
ing residual and noise, respectively. In fact, there are no
constraints on αtandβ2
tin Eq. 7, meaning that the resid-
ual diffusion and noise diffusion are independent of each
other. Utilizing this decoupled property and the difference
between these two diffusion processes, we should be able
to design a better coefﬁcient schedule, e.g., αt(linearly de-
creasing) and β2
t(linearly increasing) in Table 2. This aligns
with the intuition that, during the reverse generation pro-
4Our RDDM is implemented based on the popular diffusion repos-
itory github.com/lucidrains/denoising-diffusion-pytorch. Differences in
network structure and training details may lead to poorer FID. We have
veriﬁed sampling consistency with DDIM [ 51] in Table 3(a) and Ap-
pendix A.3.
2777
0 200 400 600 800 1000
/uni000000570.900.920.940.960.981.00t
DDIM
/uni0000000b/uni00000044/uni0000000c/uni00000003t
DDIM/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000047/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000054/uni00000058/uni00000044/uni00000055/uni00000048/uni00000047/uni00000003/uni00000046/uni00000052/uni00000056/uni0000000c
0 200 400 600 800 1000
/uni000000570.00.51.01.52.02.5 t1e 3
/uni0000000b/uni00000045/uni0000000c/uni00000003t
DDIM t/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000047/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000054/uni00000058/uni00000044/uni00000055/uni00000048/uni00000047/uni00000003/uni00000046/uni00000052/uni00000056/uni0000000c
0 200 400 600 800 1000
/uni000000570.00.51.01.52.02.53.03.54.02
t1e 3
/uni0000000b/uni00000046/uni0000000c/uni00000003t
DDIM2
t/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000047/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000054/uni00000058/uni00000044/uni00000055/uni00000048/uni00000047/uni00000003/uni00000046/uni00000052/uni00000056/uni0000000c
0 200 400 600 800 1000
/uni000000570.00.51.01.52.02.53.0t,2
t1e 3
/uni0000000b/uni00000047/uni0000000c/uni00000003 t,2
t/uni00000050/uni00000048/uni00000044/uni00000051
/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000004f/uni0000005c/uni00000003/uni00000047/uni00000048/uni00000046/uni00000055/uni00000048/uni00000044/uni00000056/uni0000004c/uni00000051/uni0000004a
/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000004f/uni0000005c/uni00000003/uni0000004c/uni00000051/uni00000046/uni00000055/uni00000048/uni00000044/uni00000056/uni0000004c/uni00000051/uni0000004a
0 200 400 600 800 1000
/uni000000570.00.51.01.52.0 t1e 3
/uni0000000b/uni00000048/uni0000000c/uni00000003P(x,a)P(1 x, 0.3)
P(1 x, 0.5)
P(1 x, 0.8)
P(1 x, 0.9)
P(1 x, 1.0)
P(1 x, 1.1)
P(1 x, 1.2)
P(1 x, 1.5)
Figure 4. Coefﬁcient transformation from DDIM [ 51] to RDDM
using Eq. 17. (a) We show several schedules for αt
DDIM , e.g., lin-
ear [51], scaled linear [ 48], and squared cosine [ 44]. (b) We trans-
formαt
DDIM intoαtin our RDDM. (c) We transform αt
DDIM
intoβ2
tin our RDDM. (d) A few simple schedules. (e) P(x,a)
is a normalized power function (see Eq. 18). ”mean”, ”linearly
increasing”, and ”linearly decreasing” in (d) can be denoted as
P(x,0),P(x,1)andP(1−x,1), respectively. See Algorithm 1
in Appendix A.3for more details of (b) and (c).
𝑃(1 − 𝑥, 1.0) 𝑃(1 − 𝑥, 1.2) 𝑃(1 − 𝑥, 1.5)
Score:9.1 Score:9.3 Score:8.2
(f) convert 𝛼𝐷𝐷𝐼𝑀𝑡to 𝛼𝑡, 𝛽𝑡2and readjust the converted 𝛼𝑡without touching the 𝛽𝑡2
𝑃(1 − 𝑥, 0.3) 𝑃(1 − 𝑥, 0.5) 𝑃(1 − 𝑥, 0.8)
Score:9.8 Score:9.8 Score:9.7
(a) DDIM (linear) (b) 𝛼𝐷𝐷𝐼𝑀𝑡→ 𝛼 𝑡, 𝛽𝑡2
Score:9.4 Score:9.4
(c) 𝛼𝐷𝐷𝐼𝑀𝑡→
scaled linear(e) 𝛼𝑡→ 𝛼 𝑡
𝛽𝑡2→ 𝑃(1 − 𝑥, 1)(d) 𝛼𝐷𝐷𝐼𝑀𝑡→
squared cosine
Figure 5. Analysis of readjusting coefﬁcient schedules. We ﬁnd
that changing the αtschedule barely affects the denoising process
in (f) and edited faces may have higher face scores when assessed
using AI face scoring software5. These images were generated
using a pre-trained UNet on the CelebA ( 256×256) dataset [ 36]
with 10 sampling steps.
cess (from Tto0), the estimated residuals become increas-
ingly accurate while the estimated noise should also weaken
progressively. Therefore, when tis close to 0, the deresid-
ual pace should be faster and the denoising pace should be
slower. Since our αtandβ2
trepresent the speed of diffu-
sion, we name the curve in Fig. 4(b-d) the diffusion speed
curve .
5.2. Partially Path­independent Generation Process
In the original DDPM [ 17] or DDIM [ 51], when the
αt
DDIM schedule changes, it is necessary to retrain the
denoising network because this alters the diffusion pro-
cess [ 44,48]. As shown in Fig. 5(c)(d), directly changing
theαt
DDIM schedule causes denoising to fail. Here, wepropose a path-independent generation process, i.e., mod-
ifying the diffusion speed curve does not cause the image
generation process to fail. We try to readjust the diffusion
speed curve in the generation process. First, we convert the
αt
DDIM schedule of a pre-trained DDIM into the αtand
β2
tschedules of our RDDM using Eq. 17(from Fig. 5(a)
to Fig. 5(b). We then readjust the converted αtschedules
using the normalized power function ( P(x,a)in Fig. 5(f)),
without touching the β2
tschedule that controls noise diffu-
sion, as shown in Fig. 5(f).P(x,a)is deﬁned as ( ais a
parameter of the power function),
P(x,a) :=xa//integraldisplay1
0xadx,wherex=t/T. (18)
These schedule modiﬁcations shown in Fig. 5lead to the
following key ﬁndings.
1.Fig. 5(f) shows that modifying the residual diffusion
speed curve ( αt) leads to a drastic change in the generation
results, probably due to Iθ
resbeing represented as a trans-
formation of ϵθusing Eq. 16.
2.As the time condition trepresents the current noise
intensity in the denoising network ( ϵθ(It,t,0)), modifying
the noise diffusion speed curve ( β2
t) causestto deviate from
accurately indicating the current noise intensity, leading to
denoising failure, as shown in Fig. 5(e).
Nonetheless, we believe that, corresponding to the de-
coupled forward diffusion process, there should also be a
path-independent reverse generation process. To develop a
path-independent generation process, we improve the gen-
eration process based on the above two key ﬁndings:
1.Two networks are used to estimate Iθ
resandϵθsepa-
rately, i.e., SM-Res-N-2Net in Appendix B.2.
2.¯αtand¯βtare used for the time conditions embed-
ded in the network, i.e., Iθ
res(It,t,0)→Iθ
res(It,¯αt·T,0),
ϵθ(It,t,0)→ϵθ(It,¯βt·T,0).
These improvements lead to a partially path-independent
generation process, as evidenced by the results shown in
Fig.6(c).
Analysis of Partially Path-independence via Green’s
Theorem. “Path-independence” reminds us of Green’s the-
orem in curve integration [ 47]. From Eq. 13, we have:
It−It−1=(¯αt−¯αt−1)Iθ
res+(¯βt−¯βt−1)ϵθ, (19)
dI(t) =Iθ
res(I(t),¯α(t)·T,0)d¯α(t)
+ϵθ(I(t),¯β(t)·T,0)d¯β(t),(20)
whereI(t) =I(0)+¯α(t)Ires+¯β(t)ϵ. Given inputs I(t)and
¯α(t), the denoising network learns to approximate the noise
ϵinI(t)by estimating ϵθ. If this network is trained well and
robust enough, it should be able to avoid the interference of
5https://ux.xiaoice.com/beautyv3
2778
(a1) Training: 
DDIM (linear)
(a3) 𝛼𝑡→
𝑃𝑥, 0(a2) 𝛽𝑡2→
𝑃1 − 𝑥, 1
(a) Denoising ( 𝜖𝜃𝐼𝑡,ҧ𝛽𝑡∙ 𝑇, 0 ) 
(b) 𝜖𝜃𝐼𝑡, 𝑡, 0 +𝐼𝑟𝑒𝑠𝜃𝐼𝑡, 𝑡, 0(b1) Training: 
DDIM (linear)
(b2) 𝛼𝑡, 𝛽𝑡2→
𝑃𝑥, 0
(b3) 𝛼𝐷𝐷𝐼𝑀𝑡→
squared cos
(c6) 𝛼𝐷𝐷𝐼𝑀𝑡→
squared cos
(c) Path Independence 
(𝜖𝜃𝐼𝑡,ҧ𝛽𝑡𝑇, 0+𝐼𝑟𝑒𝑠𝜃𝐼𝑡, ത 𝛼𝑡𝑇, 0) 
(c2) 𝛼𝑡, 𝛽𝑡2→
𝑃𝑥, 0
(c3) 𝛼𝑡, 𝛽𝑡2→
𝑃1 − 𝑥, 1
(c4) 𝛼𝑡, 𝛽𝑡2→
𝑃1 − 𝑥, 1.5
(c5) 𝛼𝐷𝐷𝐼𝑀𝑡→
scaled linear
(c1) Training: 
DDIM (linear)
(d) Decoupled Sampling (Denoising ( 𝜖𝜃𝐼𝑡,ҧ𝛽𝑡𝑇, 0),Deresidual ( 𝐼𝑟𝑒𝑠𝜃𝐼𝑡, ത 𝛼𝑡𝑇, 0) (d1) Remove 
residuals and noise 
(d3) First remove 
noise then residuals 
First remove 
residuals
Then remove 
noise
(d2) Figure 6. Partially path-independent generation process. (a1) We
trained a denoising network using the DDIM linear schedule [ 51].
(a2-a3) We modiﬁed the αtandβ2
tschedules during testing. (b)
We trained two networks to remove noise and residuals. In contrast
to the sharply varying images in (a2-a3) and the noisy images in
(b2-b3), (c) shows that we constructed a path independent genera-
tion process where modiﬁcations to the diffusion speed curve can
generate a noise-free image with little variation in image seman-
tics. (d) The simultaneous sampling in (d1) or (c) can be decom-
posed into ﬁrst removing residuals and then noise (d2), or remov-
ing noise and then residuals (d3). In (d3), diversity is signiﬁcantly
reduced because noise is removed ﬁrst.
the residual terms ¯α(t)IresinI(t). This also applies to a
robust residual estimation network. Thus, we have
∂Iθ
res(I(t),¯α(t)·T)
∂¯β(t)≈0,∂ϵθ(I(t),¯β(t)·T)
∂¯α(t)≈0.(21)
If the equation in Formula 21holds true, it serves as a nec-
essary and sufﬁcient condition for path independence in
curve integration, which provides an explanation for why
Fig. 6(c) achieves a partially path-independent generation
process. The path-independent property is related to the
network’s resilience to disturbances and applies to distur-
bances that vary within a certain range. However, exces-
sive disturbances can lead to visual inconsistencies, e.g.,
readjusting αtandβ2
ttoP(x,5). Thus, we refer to this
generative property as partially path-independent. We also
investigated two reverse paths to gain insight into the im-
plications of the proposed partial path independence. In
the ﬁrst case, the residuals are removed ﬁrst, followed by
the noise: I(T)−Ires→I(0) +¯βTϵ−¯βTϵ→I(0). The second
case involves removing the noise ﬁrst and then the residuals:
I(T)−¯βTϵ→Iin−Ires→I(0). The ﬁrst case (Fig. 6(d2)) shows(a)CelebA (FID) 5 steps 10 steps 15 steps 20 steps 100 steps
DDIM 69.60 40.45 32.67 30.61 23.66
DDIM→RDDM 69.60 40.41 32.71 30.77 24.92
(b)Shadow MAE(↓) SSIM(↑) PSNR(↑)
Removal SNS ALL S NS ALL S NS ALL
DSC [ 19]¶ 9.48 6.14 6.67 0.967 - -33.45 - -
FusionNet [ 13]7.77 5.56 5.92 0.975 0.880 0.945 34.71 28.61 27.19
BMNet [ 79] 7.60 4.59 5.02 0.988 0.976 0.959 35.61 32.80 30.28
DMTN [ 31] 7.00 4.28 4.72 0.990 0.979 0.965 35.83 33.01 30.42
Ours (RDDM) 6.67 4.27 4.67 0.988 0.979 0.962 36.74 33.18 30.91
(c)Low-light PSNR(↑)SSIM(↑)LPIPS (↓)(d)Deraining PSNR(↑)SSIM(↑)
KinD++ [ 76] 17.752 0.760 0.198 AttnGAN [ 45] 31.59 0.9170
KinD++-SKF [ 68]20.363 0.805 0.201 DuRN [ 34] 31.24 0.9259
DCC-Net [ 77] 22.72 0.81 - RainAttn [ 46] 31.44 0.9263
SNR-Aware [ 66]24.608 0.840 0.151 IDT [ 64] 31.87 0.9313
LLFlow [ 59] 25.19 0.93 0.11 RainDiff64 [ 82]32.29 0.9422
LLFormer [ 58] 23.649 0.816 0.169 RainDiff128 [ 82]32.43 0.9334
Ours (RDDM) 25.392 0.937 0.116 Ours (RDDM) 32.51 0.9563
Table 3. Quantitative comparison results of image generation on
the CelebA ( 256×256) dataset [ 36], shadow removal on the ISTD
dataset [ 57], low-light enhancement on the LOL [ 61] dataset, and
deraining on the RainDrop [ 45] dataset. “S, NS, ALL” in (b) de-
note shadow area (S), non-shadow area (NS) and whole image
(ALL). The sampling steps are 5 for shadow removal and derain-
ing, 2 for low-light.
that removing residuals controls semantic transitions, while
the second case (Fig. 6(d3)) shows that diversity is signif-
icantly reduced because noise is removed ﬁrst. Fig. 6(d)
validates our argument that residuals control directional se-
mantic drift (certainty) and noise controls random perturba-
tion (diversity). See Appendix B.4for more details.
6. Experiments
Image Generation. We can convert a pre-trained6
DDIM [ 51] to RDDM by coefﬁcient transformation using
Eq.17, and generate images by Eq. 12. Table 3(a) veriﬁes
that the quality of the generated images before and after the
conversion is nearly the same7. We show the generated face
images with 10 sampling steps in Fig. 7(a).
Image Restoration. We extensively evaluate our
method on several image restoration tasks, including
shadow removal, low-light enhancement, deraining, and de-
blurring on 5 datasets. Notably, our RDDM uses an iden-
tical UNet and is trained with a batch size of 1 for all
these tasks. In contrast, SOAT methods often involve elabo-
rate network architectures, such as multi-stage [ 13,59,80],
multi-branch [ 10], Transformer [ 58], and GAN [ 27], or so-
phisticated loss functions like the chromaticity [ 20], texture
similarity [ 74], and edge loss [ 70]. Table 3and Fig. 7(b-c)
show that our RDDM is competitive with the SOTA restora-
6https://huggingface.co/google/ddpm-celebahq-256
7The subtle differences in larger sampling steps may stem from errors
introduced by numerical representation limitations during coefﬁcient trans-
formation, which may accumulate and amplify in larger sampling steps.
2779
Input DSC FusionNet BMNet DMTN Ours (RDDM) Ground Truth
Input KinD++  SNR-Aware LLFormer LLFlow Ours (RDDM) Ground Truth
(b)
(c)
(a)
Input Input+Noise Ours Ground Truth
 Input Input+Noise Ours Ground Truth(d)
(e)
First remove residuals Then remove noise Input+NoiseFigure 7. Application of our RDDM. (a) Image generation on the CelebA dataset [ 36]. (b) Shadow removal on the ISTD dataset [ 57]. (c)
Low-light enhancement on the LOL dataset [ 61]. (d) Image inpainting (center and irregular mask). (e) The image translation process can
be regarded as ﬁrst translating the semantics and then generating the details. These images in (b) are magniﬁed using MulimgViewer [ 30].
tion methods. See Appendix Bfor more training details and
comparison results.
We extend DDPM [ 17]/DDIM [ 51], initially uninter-
pretable for image restoration, into a uniﬁed and inter-
pretable diffusion model for both image generation and
restoration by introducing residuals. However, the resid-
ual diffusion process represents the directional diffusion
from target images to conditional input images, which does
not involve a priori information about the image restoration
task, and therefore is not limited to it. Beyond image gener-
ation and restoration, we show examples of image inpaint-
ing and image translation to verify that our RDDM has the
potential to be a uniﬁed and interpretable methodology for
image-to-image distribution transformation. We do not in-
tend to achieve optimal performance on all tasks by tun-
ing all hyperparameters. The current experimental results
show that RDDM 1) achieves consistent image generation
performance with DDIM after coefﬁcient transformation, 2)
competes with state-of-the-art image restoration methods
using a generic UNet with only an ℓ1loss, a batch size of
1, and fewer than 5 sampling steps, and 3) has satisfactory
visual results of image inpainting andimage translation
(see Fig. 7(d-e), Fig. 14, or Fig. 15in Appendix B.3), whichvalidates our RDDM.
7. Conclusions
We present a uniﬁed dual diffusion model called Residual
Denoising Diffusion Models (RDDM) for image restora-
tion and image generation. This is a three-term mixture
framework beyond the previous denoising diffusion frame-
work with two-term mixture. We demonstrate that our sam-
pling process is consistent with that of DDPM and DDIM
through coefﬁcient schedule transformation, and propose
a partially path-independent generation process. Our ex-
perimental results on four different image restoration tasks
show that RDDM achieves SOTA performance in no more
than ﬁve sampling steps. We believe that our model and
framework hold the potential to provide a uniﬁed method-
ology for image-to-image distribution transformation and
pave the way for the multi-dimensional diffusion process.
8. Acknowledgments
This work was supported by National Natural Science
Foundation of China under Grants 61991413, 62273339,
62073205, 62306253.
2780
References
[1] Saeed Anwar and Nick Barnes. Densely residual laplacian
super-resolution. IEEE TPAMI , 44(3):1192–1204, 2020. 2
[2] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li,
Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas
Geiping, and Tom Goldstein. Cold diffusion: Inverting
arbitrary image transforms without noise. arXiv preprint
arXiv:2208.09392 , 2022. 2,5,16
[3] Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, and Bo
Zhang. Estimating the optimal covariance with imperfect
mean in diffusion probabilistic models. In Proc. ICML , 2022.
2
[4] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-
dpm: an analytic estimate of the optimal reverse variance in
diffusion probabilistic models. In Proc. ICLR , 2022. 2
[5] Christopher M Bishop and Nasser M Nasrabadi. Pattern
recognition and machine learning . Springer, 2006. 13
[6] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr ´edo
Durand. Learning photographic global tonal adjustment with
a database of input / output image pairs. In Proc. CVPR ,
2011. 21
[7] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun.
Learning to see in the dark. In Proc. CVPR , pages 3291–
3300, 2018. 21
[8] Zipei Chen, Chengjiang Long, Ling Zhang, and Chunxia
Xiao. CANet: A Context-Aware Network for Shadow Re-
moval. In Proc. ICCV , pages 4723–4732, 2021. 19,20
[9] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
Stargan v2: Diverse image synthesis for multiple domains.
InProc. CVPR , pages 8188–8197, 2020. 23,28
[10] Xiaodong Cun, Chi Man Pun, and Cheng Shi. Towards
ghost-free shadow removal via dual hierarchical aggregation
network and shadow matting GAN. In Proc. AAAI , pages
10680–10687, 2020. 7,17,19,20
[11] Mauricio Delbracio and Peyman Milanfar. Inversion by di-
rect iteration: An alternative to denoising diffusion for image
restoration. Transactions on Machine Learning Research ,
2023. 2,4,5,16,23,26
[12] Huiyu Duan, Wei Shen, Xiongkuo Min, Yuan Tian, Jae-
Hyun Jung, Xiaokang Yang, and Guangtao Zhai. Develop
then rival: A human vision-inspired framework for superim-
posed image decomposition. IEEE TMM , 2022. 2
[13] Lan Fu, Changqing Zhou, Qing Guo, Felix Juefei-Xu,
Hongkai Yu, Wei Feng, Yang Liu, and Song Wang. Auto-
Exposure Fusion for Single-Image Shadow Removal. In
Proc. CVPR , pages 10566–10575, 2021. 7,17,19,20
[14] Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang,
Yufei Wang, Hanspeter Pﬁster, and Bihan Wen. Shadowd-
iffusion: When degradation prior meets diffusion model for
shadow removal. In Proc. CVPR , 2023. 2,16,28
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proc.
CVPR , pages 770–778, 2016. 2,3
[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In Proc. NeurIPS , 2017. 4[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In Proc. NeurIPS , pages 6840–
6851, 2020. 1,2,3,5,6,8,12,13,14,15,16,19
[18] Weipeng Hu and Haifeng Hu. Domain discrepancy elimi-
nation and mean face representation learning for nir-vis face
recognition. IEEE Signal Processing Letters , 28:2068–2072,
2021. 26
[19] Xiaowei Hu, Chi Wing Fu, Lei Zhu, Jing Qin, and
Pheng Ann Heng. Direction-aware spatial context features
for shadow detection and removal. IEEE TPAMI , 42(11):
2795–2808, 2020. 7,19,20
[20] Yeying Jin, Aashish Sharma, and Robby T. Tan. DC-
ShadowNet: Single-Image Hard and Soft Shadow Removal
Using Unsupervised Domain-Classiﬁer Guided Network. In
Proc. ICCV , pages 5007–5016, 2021. 7,17
[21] Yeying Jin, Wenhan Yang, Wei Ye, Yuan Yuan, and Robby T
Tan. Des3: Attention-driven self and soft shadow removal
using vit similarity and color convergence. arXiv preprint
arXiv:2211.08089 , 2022. 25
[22] Yeying Jin, Wenhan Yang, Wei Ye, Yuan Yuan, and Robby T
Tan. Shadowdiffusion: Diffusion-based shadow removal
using classiﬁer-driven attention and structure preservation.
arXiv preprint arXiv:2211.08089 , 2022. 1
[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. In Proc. ICLR , 2018. 23,26,28
[24] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan
Ho. Variational diffusion models. In Proc. NeurIPS , pages
21696–21707, 2021. 2
[25] Diederik P. Kingma and Max Welling. Auto-encoding vari-
ational bayes. In Proc. ICLR , 2014. 3
[26] Diederik P. Kingma and Max Welling. An introduction to
variational autoencoders. Found. Trends Mach. Learn. , 12
(4):307–392, 2019. 3
[27] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang
Wang. Deblurgan-v2: Deblurring (orders-of-magnitude)
faster and better. In Proc. ICCV , pages 8878–8887, 2019.
7,17,23
[28] Hieu Le and Dimitris Samaras. Shadow removal via shadow
image decomposition. In Proc. ICCV , pages 8578–8587,
2019. 17
[29] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evange-
los A Theodorou, Weili Nie, and Anima Anandkumar. I2sb:
Image-to-image schr ¨odinger bridge. In Proc. ICML , 2023.
2,4,5,16,23
[30] Jiawei Liu. MulimgViewer: A multi-image viewer for image
comparison and image stitching. 8
[31] Jiawei Liu, Qiang Wang, Huijie Fan, Wentao Li, Liangqiong
Qu, and Yandong Tang. A decoupled multi-task network for
shadow removal. IEEE TMM , 2023. 3,7,19,20
[32] Jiawei Liu, Qiang Wang, Huijie Fan, Jiandong Tian, and
Yandong Tang. A shadow imaging bilinear model and three-
branch residual network for shadow removal. IEEE TNNLS ,
pages 1–15, 2023. 2,3
[33] Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongx-
uan Luo. Retinex-inspired unrolling with cooperative prior
architecture search for low-light image enhancement. In
Proc. CVPR , pages 10561–10570, 2021. 21
2781
[34] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki
Okatani. Dual residual networks leveraging the potential
of paired operations for image restoration. In Proc. CVPR ,
pages 7007–7016, 2019. 7
[35] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow
straight and fast: Learning to generate and transfer data with
rectiﬁed ﬂow. In Proc. ICLR , 2023. 5,16
[36] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proc. ICCV ,
2015. 5,6,7,8,15
[37] Zhihao Liu, Hui Yin, Yang Mi, Mengyang Pu, and Song
Wang. Shadow removal by a lightness-guided network with
training on unpaired data. IEEE TIP , 30:1853–1865, 2021.
19,20
[38] Gunter Lofﬂer, Grigori Yourganov, Frances Wilkinson, and
Hugh R Wilson. fmri evidence for the neural representation
of faces. Nature neuroscience , 8(10):1386–1391, 2005. 26
[39] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpaint-
ing using denoising diffusion probabilistic models. In Proc.
CVPR , pages 11461–11471, 2022. 1,2
[40] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj ¨olund,
and Thomas B Sch ¨on. Image restoration with mean-reverting
stochastic differential equations. In Proc. ICML , 2023. 4
[41] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. In Proc. ICLR , 2021. 2
[42] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou.
Magface: A universal representation for face recognition and
quality assessment. In Proc. CVPR , pages 14225–14234,
2021. 26
[43] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring. In Proc. CVPR , pages 3883–3891, 2017. 22,23,
28
[44] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In Proc. ICML ,
pages 8162–8171, 2021. 2,5,6,16
[45] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiay-
ing Liu. Attentive generative adversarial network for rain-
drop removal from a single image. In Proc. CVPR , pages
2482–2491, 2018. 5,7,17,22,23,28
[46] Yuhui Quan, Shijie Deng, Yixin Chen, and Hui Ji. Deep
learning for seeing through window with raindrops. In Proc.
ICCV , pages 2463–2471, 2019. 7
[47] K. F. Riley, M. P. Hobson, and S. J. Bence. Mathemati-
cal Methods for Physics and Engineering: A Comprehensive
Guide . Cambridge University Press, 3 edition, 2006. 2,6
[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In Proc. CVPR , pages
10684–10695, 2022. 1,2,5,6
[49] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image super-
resolution via iterative reﬁnement. IEEE TPAMI , 45(4):
4713–4726, 2022. 1,2,16,27,28[50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In Proc. ICML , pages
2256–2265, 2015. 2
[51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In Proc. ICLR , 2021. 1,2,5,
6,7,8,13,14,15,16,17,24
[52] Yang Song and Stefano Ermon. Generative modeling by es-
timating gradients of the data distribution. 32, 2019. 2,5
[53] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In Proc. ICLR , 2021. 2,15,16
[54] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. Consistency models. In Proc. ICML , 2023. 29
[55] Maitreya Suin, Kuldeep Purohit, and AN Rajagopalan.
Spatially-attentive patch-hierarchical network for adaptive
motion deblurring. In Proc. CVPR , pages 3606–3615, 2020.
23
[56] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,
Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxim:
Multi-axis mlp for image processing. In Proc. CVPR , pages
5769–5780, 2022. 2
[57] Jifeng Wang, Xiang Li, and Jian Yang. Stacked Condi-
tional Generative Adversarial Networks for Jointly Learning
Shadow Detection and Shadow Removal. In Proc. CVPR ,
pages 1788–1797, 2018. 5,7,8,17,18,19,20,26,27,28
[58] Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo, Bjorn
Stenger, and Tong Lu. Ultra-high-deﬁnition low-light image
enhancement: A benchmark and transformer-based method.
InProc. AAAI , pages 2654–2662, 2023. 7,27
[59] Yufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-
Pui Chau, and Alex Kot. Low-light image enhancement with
normalizing ﬂow. In Proc. AAAI , pages 2604–2612, 2022. 7,
27
[60] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang
Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general
u-shaped transformer for image restoration. In Proc. CVPR ,
pages 17683–17693, 2022. 23,24,28
[61] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu.
Deep retinex decomposition for low-light enhancement. In
Proc. BMVC , 2018. 4,5,7,8,20,21,28
[62] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan
Saharia, Alexandros G Dimakis, and Peyman Milanfar. De-
blurring via stochastic reﬁnement. In Proc. CVPR , pages
16293–16303, 2022. 2,16,23
[63] Hugh R Wilson, Gunter Lofﬂer, and Frances Wilkinson.
Synthetic faces, face cubes, and the geometry of face space.
Vision research , 42(27):2909–2923, 2002. 26
[64] Jie Xiao, Xueyang Fu, Aiping Liu, Feng Wu, and Zheng-Jun
Zha. Image de-raining transformer. IEEE TPAMI , 2022. 7
[65] Ke Xu, Xin Yang, Baocai Yin, and Rynson WH Lau.
Learning to restore low-light images via decomposition-and-
enhancement. In Proc. CVPR , pages 2281–2290, 2020. 17,
21,23,28
[66] Xiaogang Xu, Ruixing Wang, Chi-Wing Fu, and Jiaya Jia.
Snr-aware low-light image enhancement. In Proc. CVPR ,
pages 17714–17724, 2022. 7,21,22
2782
[67] Zongsheng Yue, Jianyi Wang, and Chen Change Loy.
Resshift: Efﬁcient diffusion model for image super-
resolution by residual shifting. In Proc. NeurIPS , 2023. 4
[68] Wu Yuhui, Pan Chen, Wang Guoqing, Yang Yang, Wei Jiwei,
Li Chongyi, and Heng Tao Shen. Learning semantic-aware
knowledge guidance for low-light image enhancement. In
Proc. CVPR , 2023. 7,21
[69] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Learning enriched features for real image restoration
and enhancement. In Proc. ECCV , pages 492–511, 2020. 21
[70] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Multi-stage progressive image restoration. In Proc.
CVPR , pages 14821–14831, 2021. 2,7,17,23
[71] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Learning enriched features for fast image restoration
and enhancement. IEEE TPAMI , 45(2):1934–1948, 2022. 21
[72] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learning of
deep cnn for image denoising. IEEE TIP , 26(7):3142–3155,
2017. 2
[73] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In Proc. CVPR , pages 586–
595, 2018. 17,21
[74] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling
the darkness: A practical low-light image enhancer. In Proc.
ACMMM , pages 1632–1640, 2019. 7,17
[75] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image restoration. IEEE
TPAMI , 43(7):2480–2495, 2020. 2
[76] Yonghua Zhang, Xiaojie Guo, Jiayi Ma, Wei Liu, and Jiawan
Zhang. Beyond brightening low-light images. IJCV , 129:
1013–1037, 2021. 7,21
[77] Zhao Zhang, Huan Zheng, Richang Hong, Mingliang Xu,
Shuicheng Yan, and Meng Wang. Deep color consistent
network for low-light image enhancement. In Proc. CVPR ,
pages 1899–1908, 2022. 7
[78] Chuanjun Zheng, Daming Shi, and Wentian Shi. Adaptive
unfolding total variation network for low-light image en-
hancement. In Proc. ICCV , pages 4439–4448, 2021. 21
[79] Yurui Zhu, Jie Huang, Xueyang Fu, Feng Zhao, Qibin Sun,
and Zheng-Jun Zha. Bijective mapping network for shadow
removal. In Proc. CVPR , pages 5627–5636, 2022. 7,17,19,
20
[80] Yurui Zhu, Zeyu Xiao, Yanchi Fang, Xueyang Fu, Zhiwei
Xiong, and Zheng-Jun Zha. Efﬁcient model-driven network
for shadow removal. In Proc. AAAI , 2022. 7,17,19,21
[81] Zhengxia Zou, Sen Lei, Tianyang Shi, Zhenwei Shi, and
Jieping Ye. Deep adversarial decomposition: A uniﬁed
framework for separating superimposed images. In Proc.
CVPR , pages 12806–12816, 2020. 2
[82] Ozan ¨Ozdenizci and Robert Legenstein. Restoring vision in
adverse weather conditions with patch-based denoising dif-
fusion models. IEEE TPAMI , pages 1–12, 2023. 1,2,7,16,
23
2783
