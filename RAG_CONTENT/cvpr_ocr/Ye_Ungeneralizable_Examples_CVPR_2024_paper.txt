Ungeneralizable Examples
Jingwen Ye Xinchao Wang†
National University of Singapore
jingweny@nus.edu.sg, xinchao@nus.edu.sg
Abstract
The training of contemporary deep learning models
heavily relies on publicly available data, posing a risk of
unauthorized access to online data and raising concerns
about data privacy. Current approaches to creating un-
learnable data involve incorporating small, specially de-
signed noises, but these methods strictly limit data us-
ability, overlooking its potential usage in authorized sce-
narios. In this paper, we extend the concept of unlearn-
able data to conditional data learnability and introduce
UnGeneralizable Examples (UGEs). UGEs exhibit learn-
ability for authorized users while maintaining unlearnabil-
ity for potential hackers. The protector defines the autho-
rized network and optimizes UGEs to match the gradients of
the original data and its ungeneralizable version, ensuring
learnability. To prevent unauthorized learning, UGEs are
trained by maximizing a designated distance loss in a com-
mon feature space. Additionally, to further safeguard the
authorized side from potential attacks, we introduce addi-
tional undistillation optimization. Experimental results on
multiple datasets and various networks demonstrate that the
proposed UGEs framework preserves data usability while
reducing training performance on hacker networks, even
under different types of attacks.
1. Introduction
The widespread availability of ‘free’ internet data has
played a pivotal role in advancing deep learning and com-
puter vision models. However, a notable concern arises
from the collection of datasets without explicit consent,
with personal data often gathered unknowingly from the in-
ternet. This practice has raised public concerns about the
potential unauthorized and, in some cases, potentially ille-
gal exploitation of personal information. These issues have
gained even greater significance with the introduction of the
General Data Protection Regulation (GDPR) by the Euro-
pean Union, placing a renewed emphasis on data protection
†Corresponding author.
TrainUngeneralizableExamples
TrainHackerNetworksAuthorizedNetwork
HackersProtector…Learnable
Unlearnable
Figure 1. The threat model of ungeneralizable examples involves
generating UnGeneralizable Examples. Once created, both the
protector and the hacker gain access to the UGEs rather than the
original data. While the UGEs can effectively train the protector’s
network, they result in a performance drop on hacker networks.
within the AI community.
To address the risk of machine learning models captur-
ing private data, recent developments have focused on the
concept of unlearnable examples (ULE) [6, 9, 24]. Unlearn-
able examples represent data types that deep learning mod-
els struggle to effectively learn useful information from. A
common method for generating unlearnable examples in-
volves a min-min bilevel optimization framework, deceiv-
ing the model into learning a false connection between noise
and labels. Consequently, models trained on such unlearn-
able examples exhibit significantly reduced performance,
emphasizing the importance of robust data protection in ma-
chine learning. It’s crucial to note that, oftentimes, the data
itself is not inherently problematic; instead, it is the manner
in which they are utilized that demands careful considera-
tion. Therefore, we argue that such an across-the-board data
protection rule could address some stringent privacy issues
but might impede the shareable community under normal
service conditions.
In our study, we broaden the conventional assumption in
existing ULE methods, introducing a more adaptable and
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11944
pragmatic data protection paradigm referred to as ungener-
alizable examples. In contrast to the conventional ULE
framework, we posit that the data can be learnable by net-
works pre-defined by the protector. This approach enables
the protector to maintain authorized usage of the collected
data, addressing the inflexible concern of unlearnable ex-
amples. Moreover, it offers an alternative for the protector
when they need to share their data for specific legitimate
purposes. The fundamental concept of UGEs is visually
represented in Figure 1.
In UGE, the protector pre-defines the authorized network
before generating the ungeneralizable version of the data.
We approximate the training trajectories of the original data
and the ungeneralizable data to ensure that the data’s learn-
ability remains unchanged. To prevent the data from being
learned by hackers, we maximize the feature distance in the
common feature space, where unlearnability can transfer to
multiple hacker networks. Additionally, to further enhance
the confidentiality of the ungeneralizable examples, we in-
troduce the undistill loss, aiming to prevent hackers from
recovering the original data from the protector network.
In summary, the contributions of this paper can be out-
lined as follows:
•Introduction of Ungeneralizable Examples Paradigm :
We propose a versatile data protection paradigm termed
ungeneralizable examples. This paradigm enables the le-
gitimate use of data by the protector while preventing
unauthorized usage by potential hackers. It introduces a
pragmatic scenario, challenging the unauthorized training
of machine learning models.
•Innovative Solution for Learnability and Unlearnability
Switchover : We introduce a novel approach to address the
switch between data learnability and unlearnability using
three distinct losses. This is the first and only method, to
our knowledge, that achieves this switchover effectively.
•Empirical Verification of Effectiveness and Robustness :
We empirically verify the effectiveness of our proposed
approach with different network backbones on diverse
datasets. Furthermore, we assess its robustness under var-
ious network architectures and multiple types of attacks.
2. Related Work
2.1. Data Privacy Protection
Ensuring data privacy protection is crucial for safeguard-
ing individuals’ sensitive information, preserving auton-
omy, and fostering trust in the digital landscape. This com-
mitment is instrumental in the ethical and responsible de-
velopment of technology. Here we group the data privacy
protection into visual information protection and data pro-
tection from machine learning.
The essence of visual information protection lies in ren-
dering data visually unrecognizable or inaccessible to thirdparties. A direct approach involves employing basic tech-
niques like pixelization, blurring, or scrambling to obscure
facial features in images. Alternatively, recent advance-
ments explore the use of encryption [11, 32] directly applied
to an image, followed by inpainting [20, 31, 33], making
it challenging to recover the original content. As another
illustration, the concept of dataset condensation [3, 17–
19, 37] is introduced to distill the essence of data into a
compact synset. This approach aims to safeguard the orig-
inal data’s integrity while retaining its ability to effectively
train a neural network. Regarding data privacy, federated
learning [14, 28, 40] emphasizes a distributed model train-
ing paradigm that prioritizes keeping sensitive information
localized on individual devices, thereby mitigating privacy
risks associated with centralized data storage or sharing.
Data protection from machine learning primarily focuses
on the control and management of learnable features ex-
tracted from networks. For exmaple, machine unlearn-
ing [2, 16, 29] exemplifies the recalibration of machine
learning models through the selective discarding of specific
data points, patterns, or predictions. This process involves
the removal of sensitive data information from the network,
effectively eliminating the risk of unintended data exposure.
An additional aspect of data protection involves prevent-
ing data from being learned by machine unlearning mod-
els. Huang et al. [9] have made a significant contribution to
safeguarding image data from unauthorized machine learn-
ing exploitation. They introduced a method focused on gen-
erating error-minimizing noise with the primary goal of in-
tentionally degrading images uploaded to the internet. This
degradation aims to impede the training process of neural
networks. As a result, images incorporating this introduced
noise are classified as unlearnable examples [6, 24].
We posit our proposed ungeneralizable examples as an
expanded version of unlearnable examples, offering en-
hanced flexibility in data management. In this approach,
the data remains unlearnable by the defender while remain-
ing learnable by the protector, thereby providing a more nu-
anced control over the learning dynamics.
2.2. Model Privacy Protection
In the realm of model privacy protection, our focus cen-
ters on Intellectual Property (IP) safeguarding. The escalat-
ing commercial significance of deep networks has garnered
heightened attention from both academia and industry, em-
phasizing the imperative for robust IP protection.
As a conventional technique, network watermarking [12,
13, 27] involves embedding identification information into
the target network, enabling copyright claims without com-
promising the network’s predictive capabilities. Numer-
ous recent studies [10, 15, 35] have investigated defensive
strategies against model stealing, aiming to safeguard the
intellectual property of the network. As an additional mea-
11945
sure for intellectual property (IP) protection, knowledge
undistillation [21, 34] is introduced to prevent knowledge
theft by other networks. This entails maintaining the net-
work’s fundamental prediction performance while inducing
a performance drop when attempting to distill knowledge.
Our proposed ungeneralizable examples have something
common with knowledge undistillaion, which are designed
to introduce modifications to the original images, leading
to suboptimal performance on unauthorized networks while
preserving their efficacy in training the protector’s network.
2.3. Adversarial and Data Poisoning Attacks
Adversarial attacks [1, 4, 8, 22, 36, 39] are designed to de-
ceive machine learning models by adding small, impercep-
tible perturbations to input data, causing the model to gen-
erate incorrect outputs or misclassify inputs. One of the
traditional attack methods [7] is to use gradient information
to update the adversarial example in a single step along the
direction of maximum classification loss.
Data poisoning [26, 30, 41] is a type of adversarial at-
tack that involves manipulating the training data used to
train machine learning models. The goal of these attacks
is to introduce malicious or misleading data into the train-
ing set, with the intention of influencing the performance of
the trained model.
However, such methods don’t affect the model’s perfor-
mance on clean data, which makes them unsuitable for data
privacy pretection.
3. Proposed Method
Assumptions on Protector’s Capability : We assume that
the protector has unrestricted access to the specific dataset
they intend to make ungeneralizable. However, it’s crucial
to clarify that the protector lacks the capacity to interfere
with the training process and does not have access to the en-
tire training dataset. In simpler terms, the protector’s influ-
ence is confined to transforming their designated data por-
tion into ungeneralizable examples. Furthermore, it’s essen-
tial to underscore that once the ungeneralizable examples
are generated, the protector is prohibited from making fur-
ther modifications to their data. Importantly, these modifi-
cations are irreversible. In other words, once the alterations
are applied, the original data is replaced by the modified
versions.
3.1. Problem Formulation
Following the previous setting on unlearnable examples [9],
we focus on image classification tasks in this paper.
Suppose D={(x, y)}(n)is a clean training dataset with
K-class, where images can be denoted as x∈ X ⊂ Rd,
the corresponding groundtruth labels are denoted as y∈
Y={1,2, ..., K}. Two distinct networks are introduced:the authorized network, denoted as fθ, and the hacker’s net-
work, denoted as f′
θA. The network fθis predetermined by
the protector, where the network’s architecture and initial
parameters are set. Alongside the original data D, the pro-
tector utilizes fθto generate the ungeneralizable version of
the dataset, denoted as Du={(xu, yu)}(n). This process
is defined as follows:
xu←x+δ(fθ), y u←y;{(x, y)} ∈ D .(1)
Here δ(fθ)⊂Rdis the generated ungeneralizable noise
that is related to the authorized network fθ. The ungeneral-
izable noise is typically regulated to be imperceptible. We
omitfθfrom the ungeneralizable noise in the rest of the pa-
per. The ungeneralizable dataset Duis assumed to be the
shareable dataset collected by both the hackers and the pro-
tector, which will be utilized to train both the authorized
network fθand the hacker network f′
θA.
The generation of ungeneralizable examples serves two
main objectives: firstly, they are designed to remain learn-
able for the authorized network fθ; secondly, they are in-
tended to become unlearnable for the malicious networks
f′
θAemployed by the hackers. Thus, the objective could be
formulated as:
min
θ1
nnX
(x,y)∈Dmin
∥δ∥≤ρh
L 
f′
θA(x+δ), y
+
L 
fθ(x+δ), y
− L 
fθ(x), yi
,(2)
where L(·)denotes the loss function for network training,
andρis the radius represents the radius of the applied un-
generalizable noise. The formal section of the objective
function introduces error-minimizing noise to render the
data unlearnable by diminishing the associated training loss,
making it challenging for hackers using f′
θAto acquire the
knowledge. The latter part of the objective function aims
to reconstruct this knowledge on the authorized network fθ
by minimizing the training loss between the clean input and
the ungeneralizable version input.
Design Goals of UGEs : We aim to generate the un-
generalizable examples with the following characteristics:
•Visual Integrity : The ungeneralizable version of the im-
ages should remain visually recognizable to human ob-
servers, meaning that the ungeneralizable noise should be
confined to a small norm.
•Effectiveness . UGEs facilitate authorized training on the
authorized networks while preventing unauthorized train-
ing by hackers, demonstrating conditional learnability.
•Robustness . The unlearnability of UGEs should be stable
and resistant to attacks by hackers; their safety should be
verified under various types of attacks. Additionally, it
should be transferable to different network architectures.
•User-friendliness . It should be convenient for authorized
usage. That is, it shouldn’t affect the training process on
11946
GeneratorOriginal ExamplesUGEsAuthorized Network
GradientMatchingLoss
Hacker Network
UndistillLoss
FeatureSpace
MinimizeEncoderEncoderFeatureSpaceFeature Distance LossTrajectory
Figure 2. The comprehensive workflow of UGEs involves the protector training a generator to produce the ungeneralizable version of the
original examples. Three distinct loss functions are employed in training the generator: gradient matching loss, feature distance loss, and
undistill loss. Upon completion of the training process, the UGEs are published, and both the protector and hackers no longer have access
to the original examples.
the authorized network. No new losses or components are
introduced for training on UGEs, and it shouldn’t increase
the computational load of the training process.
3.2. Ungeneralizable Examples
As is shown in Fig. 2, the framework of obtaining the un-
generalizable version of the original data is depicted, where
we train a generator to synthesize the UGEs:
xu←Clamp 
G(x), x−ρ, x+ρ
x∈ D, (3)
where the Clamp ()operation is to constrain the ungeneral-
izable noise’s norm within ρ. A total of three loss functions
are utilized to train the generator G:
Lall=Lgm+λfd· Lfd+λud· Lud, (4)
where Lgmis the gradient matching loss to ensure UGEs
learnanle on the authorized network, Lfdis the feature dis-
tance loss to make UGEs unlearnable on the hacker net-
works, and Ludis the undistill loss to make the original
examples inreversible on the authorized network. λfdand
λudare the weights to balance each loss item.
Learnable on the authorized network. As is stated in
Eq. 2, the latter loss item which tries to minimize the train-
ing loss between the inputs of xandxu. Note the archi-
tecture and the initial parameters θ0of the authorized net-
work are confirmed, the training process of fθon the origi-
nal dataset Dcould be determined:
f:θt+1←θt−η1
nX
(x,y)∈D∇L 
fθt(x), y
, (5)
where ηis the learning rate, and t={0,1, ..., T−1}is the
training epoch number, Tis the total training epoch number.
∇Lis the gradients while in each training epoch.To sustain the learning trajectory on the original data x,
we introduce the gradient matching loss during the training
process between xandxu. Specifically, we randomly sam-
ple several intermediate training epochs from {0,1, ..., T −
1}, denoted as τ. The gradient matching loss is then calcu-
lated in these sampled epochs, thus the gradient matching
loss can be finally expressed as:
Lgm=1
|τ|×nX
t∈τX
(x,y)∈DDist
∇L 
fθt(x), y
,
∇L 
fθt(xu), y
,(6)
where Dist(·)represents the cosine distance, and we em-
ploy it to distill the gradient information from xtoxu. Min-
imizing the gradient matching loss Lgmeffectively aligns
the training trajectory of the original data with that of the
ungeneralizable data. This optimization ensures the preser-
vation of data learnability.
Unlearnable on the hacker network. As outlined in
Eq. 2, the first loss renders the data unlearnable on hacker
networks. Traditional unlearnable methods address this op-
timization challenge by introducing error-minimizing noise,
typically through bi-level optimization, which is considered
less efficient.
In this approach, we leverage a shared feature space to
apply perturbations, thereby achieving the unlearnable char-
acteristic in the data. As depicted in Fig. 2, a common im-
age encoder Eiextracts features from both the original data
Dand its ungeneralizable version Du. To ensure that the
feature perturbation designed in this feature space remains
effective across diverse networks, a robust and powerful en-
coder selection becomes crucial.
Considering this perspective, we opt to utilize the pre-
trained image encoder of the CLIP model [23]. As a lead-
11947
ing Vision-and-Language (VL) model, CLIP learns state-
of-the-art image representations from scratch on a dataset
containing 400 million image-text pairs collected from the
internet. This training allows it to excel in various tasks,
including zero-shot classification. Alongside the powerful
image encoder Eioffered by CLIP, an additional textual en-
coderEtis available to provide supplementary guidance.
To be specific, the feature distance loss Lfdcan be com-
puted as follows:
Lfd=1
nX
xu∈Duℓfeat(xu,Ei,D) +ℓtri(xu,Ei,Et,D),(7)
which contains two main loss items ℓfeat andℓtri. The for-
mer loss ℓfeatpushes the features of the ungeneralizable xu
away from the original x:
ℓfeat(xu,Ei,D) =−∥F i− F′
i∥2,
where Fi=Ei(x)
∥Ei(x)∥,F′
i=Ei(xu)
∥Ei(xu)∥,(8)
where Fi/F′
iis the normalized features and ℓfeat is calcu-
lated based on the MSE loss.
In addition to maximizing the similarity between the fea-
tures of the original input and the ungeneralizable input, we
introduce an additional triplet loss. This triplet loss ensures
that the features of F′
iin the ungeneralizable input can be
effectively transferred to various hacker networks.
ℓtri(xu,Ei,Et,D)=∥F′
i− F′
t∥2+max 
0, α−∥F′
i−Ft∥2
,
where Ft=Et(y)
∥Et(y)∥,F′
t= arg min
Fc
tSim(Fi,Fc
t).
(9)
Here, αis the margin of the triplet loss and Ftis the textual
features with the groundtruth label yas input and Sim(·)is
the similarity function measuring the distance between the
textual features and image features. Fc
tis the textual input
with label c(c∈ {1,2, ..., K}andc̸=y). Consequently,
F′
trefers to the textual features with the least similarity to
the original image encoder features Fi. The term ℓtrien-
courages the features of ungeneralizable examples to move
away from their associated textual features towards those of
the least similar textual features.
Untransferable on the authorized network. After the
ungeneralizable version of the data Duis published, both
the protector and hackers gain access to UGEs. UGEs
show to be unlearable on the hacker networks when stan-
dard training with minθAL(f′
θA(xu), yu). On the contrary,
UGEs can be employed for normal training on the network
fθauthorized by the protector. By minimizing the gradi-
ent matching loss Lgmas defined in Eq. 6, fθattains sim-
ilar performance to when trained with the original data D.
The protector doesn’t constrain the authorized network to
be confidential, which means the hackers also have accessto the authorized network fθ(including architecture and pa-
rameters). In this way, the hackers have another alternative
to train their networks, with both the ungeneralizable exam-
plesDuandfθavailable.
To be concrete, the protector has authorized the data
learning on the authorized network fθ, and there exists a
potential risk for hackers to exploit distillation-based learn-
ing process, expressed as:
f′: min
θA1
nX
xu∈DuLkd 
fθ(xu), f′
θA(xu)
, (10)
where Lkdrepresents the KL-divergence loss for distilling
knowledge directly from the authorized network fθ. This
poses a significant security risk as it could expose the con-
fidentiality of UGEs through the authorized network fθ.
Considering this concern, we introduce an undistill loss
Ludto safeguard the knowledge of the authorized network.
Building upon prior work on knowledge undistillation [21]
designed for network IP protection, our proposed undistill
loss is expressed as:
min
DuLud=1
nX
(xu,yu)∈Du
L(fθ(xu), yu)−ωLkd 
fθ(xu), f′
θA(xu)
,
(11)
where L(·)represents the standard training loss of fθand
ωis the balancing weight. It’s worth noting that in previ-
ous knowledge undistillation approaches, the undistill loss
is employed to update the parameters of the network to be
protected. In our case, we maintain fθandf′
θAfixed and op-
timize xuto ensure that its learnable knowledge within fθ
cannot be transferred to f′
θA. This additional optimization
step further enhances the security of our proposed ungener-
alizable examples.
It’s essential to highlight that in this context, we do not
restrict f′
θAto any specific networks ; it can be any arbi-
trarily initialized network. Our proposed UGEs not only
demonstrate effectiveness on the randomly chosen f′
θAbut
also exhibit generalizability, extending their unlearnability
characteristics to other networks.
3.3. Algorithm
The whole algorithm is depicted in Alg 1.
3.4. UGEs in Various Usages
The proposed UGEs seamlessly combine both data learn-
ability and unlearnability within a single framework, show-
casing a flexible approach to data management suitable for
various applications.
Scenario I : Utilizing UGEs in Decentralized Model
Training. In scenarios resembling federated learning, where
privacy constraints exist in individual local servers, UGEs
offer a viable solution. The global server establishes the
initial global model, communicates the model information
11948
Algorithm 1 The framework of the proposed UGEs.
Require: D: original data to be protected; fθ: authorized net-
work;{θ0, θ1, ..., θ τ}: sampled trajectory of the authorized
network. f′
θA: randomly initialized hacker network. ρ: un-
generalizable noise ℓ∞bound;
Ensure: Du: ungeneralizable examples.
1:Initialize the generator model G;
2:Initialize the text input as ‘A photo of a <CLASS >’;
3:Input text input to Etto get textual features for all classes;
4:while not convergence do
5: Input xtoGand get xu=G(x)bounded with ρ;
6: Randomly choose θtfrom the input trajectory;
7: Input both xandxutofθtto calculate Lgmwith Eq. 6;
8: Input both xandxuto encoder Eito getFiandF′
i;
9: Calculate Lfdwith Eq. 7;
10: Input xuto both networks fθandf′
θA;
11: Calculate Ludwith Eq. 11;
12: Update Gby minimizing Lall;
13:end while
14:Get the ungeneralizable examples DuwithG;
15:Publish the final Du.
to each local server, and enables the joint training of the
global model. UGEs effectively address privacy concerns
by selectively publishing data for specific use cases.
Scenario II : Enhancing Code Publication Safety with
UGEs. In open-source platforms such as GitHub, re-
searchers are encouraged to share their code for collabo-
rative AI development. However, instances arise where re-
searchers collectively publish their gathered data. To miti-
gate the risk of malicious utilization of this data, researchers
can opt to publish the ungeneralizable version of their train-
ing data, ensuring a more secure sharing environment.
Scenario III : Ensuring Secure Data Transmission with
UGEs. In instances where secure data transmission is re-
quired to train a downstream network, a secure process
can be established. The receiver initiates the transmission
by sending its information to the protector. Subsequently,
only the UGEs are transmitted to the receiver, mitigating
the risk of interception by hackers during the transmission
process. This approach ensures a secure and protected data
exchange, with UGEs playing a pivotal role in safeguarding
sensitive information.
4. Experiments
In this section, we conduct comprehensive experiments to
validate the effectiveness of the robust ungeneralizable ex-
amples. Additional details regarding the experiment setup
can be found in the supplementary materials.
4.1. Experiment Setup
Datasets. Continuing the experimental setup from previous
unlearnable methods, we present our results on CIFAR-10,CIFAR-100, and TinyImageNet datasets. The input size for
CIFAR-10 and CIFAR-100 datasets is 32×32, while for the
TinyImageNet dataset, we utilize an input size of 256×256.
Model Training. We employ the PyTorch framework for
implementation and investigate several network backbones,
including plain CNN, LeNet, ResNet, MobileNetV2, and
ShuffleNetV2. The generator utilizes a ResNet backbone.
In our supposition, both the authorized network and
hacker networks are optimized using standard Stochastic
Gradient Descent (SGD). The authorized network fθis
determined with given network architecture and initializa-
tion parameters. We assume networks with either different
architectures or different initialization parameters are re-
garded as hacker networks. When training the UGEs, we
randomly select a distinct network as the hacker network,
exclusively including the hacker network for training.
Evaluation Metrics. We evaluate the data protection ca-
pability of the ungeneralizable noise using test accuracy.
A low test accuracy on hacker networks indicates that the
model has learned minimal knowledge from the training
data, reflecting strong protection. Conversely, a high test
accuracy on the authorized network indicates that the model
has successfully learned knowledge from the training data,
demonstrating data learnability for authorized usage.
4.2. Experimental Results
Ablation Study. The results of the ablation study on
CIFAR-10, CIFAR-100 and TinyImageNet datasets are pre-
sented in Table 1. We compare the test accuracy on both
the authorized network (Acc. (Authorized)) and the hacker
networks (Acc. (Hacker)). Various network backbones
are chosen to create the hacker networks, trained under
two schemes: normal training with data labels (Normal)
and distillation-based training using Eq. 10 (Distill). The
distillation-based training can be thought as a kind of at-
tack. For comparison, we show and compare the results
with: ‘Original’: the unmodified original data D; ‘Unlearn’:
training only with unlearn loss Lfd; ‘UnDistill’: training
only with the undistillaion loss Lud; ‘UGEs w/o UD’: train-
ing without the undistillaion loss Lud. Form the table, con-
clusions could be drawn that:
• The efficacy of UGEs is evaluated based on their learn-
ability on the authorized network (higher Acc. (Autho-
rized)) and unlearnability on hacker networks (lower Acc.
(Hacker)). Our method significantly reduces the test ac-
curacy of hacker networks (by more than 40%) while
maintaining an acceptable drop in authorized network ac-
curacy (less than 5 %).
• UGE effectiveness is demonstrated across CIFAR-10,
CIFAR-100, and TinyImageNet datasets, utilizing diverse
network architectures like ResNet-18, CNN, MobileNet,
and ShuffleNet. This showcases UGEs’ versatility and
efficacy across different scenarios;
11949
Table 1. Experimental Results on CIFAR-10, CIFAR-100 and TinyImageNet datasets , where ResNet-18 is used as the backbone of the
authorized network. Acc changes are shown in red comparing with the network normal trains on the original dataset.
Dataset Method Scheme Acc. (Authorized)Acc. (Hacker)
CNN ResNetC-20 ResNetC-32 ResNet-18
CIFAR-10 Original Normal 95.05 86.57 92.28 93.04 95.05
CIFAR-10 Original Distill - 88.06 (+1.49) 92.62 (+0.34) 93.24 (+0.20) 95.41 (+0.36)
CIFAR-10 Unlearn Normal 22.59 (-72.46) 18.36 (-68.21) 20.37 (-71.91) 22.39 (-70.65) 22.44 (-72.61)
CIFAR-10 Unlearn Distill - 17.39 (-69.18) 21.40 (-70.88) 22.08 (-70.96) 21.48 (-73.57
CIFAR-10 UnDistill Normal 94.52 (-0.47) 85.87 (-0.70) 85.91 (-6.37) 86.98 (-6.06) 88.07 (-6.98)
CIFAR-10 UnDistill Distill - 73.38 (-13.19) 78.65 (-13.63) 80.76 (-12.28) 84.07 (-10.98)
CIFAR-10 UGEs w/o UD Normal 94.34 (-0.71) 46.97 (-39.6) 56.97 (-35.31) 75.07 (17.97) 45.95 (-49.10)
CIFAR-10 UGEs w/o UD Distill - 75.23 (-11.34) 69.08 (23.20) 77.45 (-15.59) 87.10 (-7.95)
CIFAR-10 UGEs Normal 93.89 (-1.16) 26.46 (-60.11) 30.63 (-61.65) 36.08 (-56.96) 26.12 (-68.93)
CIFAR-10 UGEs Distill - 32.08 (-54.49) 37.22 (-55.06) 47.59 (-45.45) 35.23 (-59.82)
Dataset Method Scheme Acc. (Authorized)Acc. (Hacker)
MobileNetV2 ShuffleNetV2 ResNet-18
CIFAR-100 Original Normal 78.24 68.92 71.26 78.24
CIFAR-100 Original Distill - 72.67 (+3.75) 74.39 (+3.13) 79.24 (+1.00)
CIFAR-100 UGEs w/o UD Normal 75.26 (-2.98) 22.05 (-46.87) 21.59 (-49.67) 16.46 (-61.78)
CIFAR-100 UGEs w/o UD Distill - 63.52 (-5.40) 58.23 (-13.03) 40.45 (-37.79)
CIFAR-100 UGEs Normal 74.68 (-3.56) 32.11 (-36.81) 28.33 (-42.93) 16.55 (-61.69)
CIFAR-100 UGEs Distill - 26.94 (-41.98) 25.34 (-45.92) 15.50 (-62.74)
TinyImageNet Original Normal 63.08 56.00 59.90 63.08
TinyImageNet Original Distill - 60.02 (+3.06) 63.19 (+7.19) 66.28 (+6.38)
TinyImageNet UGEs w/o UD Normal 60.88 (-2.20) 14.62 (-41.38) 13.97 (-45.93) 15.34 (-47.74)
TinyImageNet UGEs w/o UD Distill - 36.39 (-6.18) 49.82 (-16.97,) 42.93 (-20.15)
TinyImageNet UGEs Normal 59.54 (-3.54) 15.79 (-40.21) 22.75 (-37.15) 17.55 (-45.53)
TinyImageNet UGEs Distill - 19.69 (-36.94) 21.06 (-35.48) 24.42 (-38.66)
• Our proposed UGEs demonstrate robustness against at-
tacks where hackers use the authorized network to acquire
the learnability of UGEs (Scheme as ‘Distill’). The re-
sults show that the proposed undistillation loss Lud(com-
paring ‘UGEs’ and ‘UGEs w/o UD’ ) effectively prevents
such attacks, fulfilling the robustness goals in the design.
UGEs with Multiple Authorized Networks. In the
standard experimental setup, we initially configure one net-
work as the authorized network. Here, we extend our frame-
work to accommodate multiple authorized networks, intro-
ducing additional loss items for each newly added network
inLall. Refer to the supplementary material for specific
details on modifying the losses and extra experiments.
In this extension, we establish two authorized networks
with ResNet-18 with distinct initialization parameters. The
experimental results on the CIFAR-10 dataset are presented
in Table 2, where ‘Distill-1’ indicates optimizing the hacker
networks with the distillation calculated on authorized net-Table 2. Results on UGEs with multiple authorized networks on
CIFAR-10 dataset, which are tested under three training schemes.
Method SchemeAuthorized Hacker
Net-1 Net-2 CNN ResNetC-20
Original Normal 95.01 94.95 86.57 93.04
Original Distill-1 - - 88.06 92.62
Original Distill-2 - - 88.09 92.55
UGEs Normal 93.27 93.83 43.28 49.34
UGEs Distill-1 - - 53.60 56.32
UGEs Distill-2 - - 50.32 54.29
1. As observed in the table, introducing another authorized
network maintains the effectiveness of our proposed frame-
work for learning UGEs. However, with more authorized
networks, the performance of UGEs experiences a slight
11950
Table 3. Comparing the data unlearnability with the existing ULE
methods on CIFAR-10 and CIFAR-100 datasets.
MethodAcc. (CIFAR-10) Acc. (CIFAR-100)
Clean ULEs Clean ULEs
EM [9] 94.66 13.20 76.27 1.60
TAP [5] 94.66 22.51 76.27 13.75
NTGA [38] 94.66 16.27 76.27 3.22
REM [6] 94.66 27.09 76.27 10.14
CUDA [25] 94.66 18.48 76.27 12.69
Ours (Lfd) 94.66 22.59 76.27 9.35
(a) CIFAR-10Dataset
(b) CIFAR-100Dataset
Figure 3. The performance concerning the value of ρon CIFAR-
10 and CIFAR-100 datasets.
decline. Addressing this challenge and providing a more
flexible framework to include multiple authorized networks
will be a focus of our future work.
How does the norm of ungeneralizable noise affect
the UGEs performance. Recall that we set the norm of
the ungeneralizable noise ρas 0.04. We investigate the per-
formance concerning the value of ρ, as illustrated in Fig. 3.
From the figure, it can be observed that a larger norm of un-
generalizable noise leads to a decrease in test accuracy on
the authorized network. Therefore, a properly chosen small
norm of noise is essential, ensuring both the visual integrity
of the protected data and maintaining acceptable authorized
network performance.
Comparing with Existing ULEs. We compared the
proposed method with existing ULE methods on CIFAR-
10 and CIFAR-100 datasets, as shown in Table 3. The
listed methods are included in the table, and the experimen-
tal setup follows previous work [25]. Lower test accuracy
indicates better unlearnability. The results demonstrate that
our proposed method contributes to current unlearnable ex-
ample methods, achieving competitive results with existing
ULE methods. The UGE framework can also be seamlessly
integrated into the ULEs framework by training the genera-
torGwith the loss term Ld f.
More Analysis. In Fig. 4, we showcase the visualization
results of our proposed UGE. The UGEs demonstrate visual
similarity to the original images, confirming their visual in-
Original
UGENoise
UGEsOriginalUGENoiseUGEs
(a) CIFAR-10Dataset(b) CIFAR-100Dataset
Figure 4. The visualization results include the original clean im-
ages, the ungeneralizable noise (scaled by 255for better visualiza-
tion), and the resultant ungeneralizable images.
tegrity and aligning with the framework’s design goal.
4.3. Limitations
While our method shows promise across various scenarios,
it has limitations, particularly when faced with an increas-
ing number of authorized networks (Table. 2). Addressing
this, we plan to incorporate ensemble methods or knowl-
edge amalgamation to enhance UGEs’ performance in such
scenarios. This underscores our commitment to ongoing
improvement and adaptability. It’s important to note that
our UGE framework is designed for classification tasks.
Looking ahead, we aim to extend its applicability to mul-
tiple tasks, enabling the seamless transition of data learn-
ability among different tasks, thus enhancing its versatility.
5. Conlusion
In conclusion, our paper presents the ungeneralizable ex-
amples framework, a versatile paradigm for data protection.
UGE allows legitimate data usage by the protector while
preventing unauthorized access by potential hackers. The
proposed approach, incorporating three distinct losses, suc-
cessfully achieves a seamless transition between data learn-
ability and unlearnability. Empirical verification validates
the effectiveness and robustness of our method, demonstrat-
ing its potential in enhancing data security in machine learn-
ing applications.
Acknowledgements
This project is supported by the Advanced Research
and Technology Innovation Centre (ARTIC), the Na-
tional University of Singapore under Grant (project
number: A0005947-21-00, project reference: ECT-
RP2).
11951
References
[1] Naveed Akhtar and Ajmal S. Mian. Threat of adversarial
attacks on deep learning in computer vision: A survey. IEEE
Access , 6:14410–14430, 2018. 3
[2] Lucas Bourtoule, Varun Chandrasekaran, Christopher A
Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang,
David Lie, and Nicolas Papernot. Machine unlearning. In
2021 IEEE Symposium on Security and Privacy (SP) , pages
141–159. IEEE, 2021. 2
[3] George Cazenavette, Tongzhou Wang, Antonio Torralba,
Alexei A. Efros, and Jun-Yan Zhu. Dataset distillation
by matching training trajectories. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2022. 2
[4] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su,
Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversar-
ial attacks with momentum. 2018 IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9185–
9193, 2017. 3
[5] Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geip-
ing, Wojciech Czaja, and Tom Goldstein. Adversarial exam-
ples make strong poisons. Advances in Neural Information
Processing Systems , 34:30339–30351, 2021. 8
[6] Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, and
Dacheng Tao. Robust unlearnable examples: Protecting data
privacy against adversarial learning. In International Con-
ference on Learning Representations , 2021. 1, 2, 8
[7] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. Interna-
tional Conference on Learning and Representations , 2014.
3
[8] Chuan Guo, Jacob Gardner, Yurong You, Andrew Gordon
Wilson, and Kilian Weinberger. Simple black-box adversar-
ial attacks. In International Conference on Machine Learn-
ing, pages 2484–2493. PMLR, 2019. 3
[9] Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James
Bailey, and Yisen Wang. Unlearnable examples: Making
personal data unexploitable. In International Conference on
Learning Representations , 2020. 1, 2, 3, 8
[10] Sanjay Kariyappa and Moinuddin K Qureshi. Defending
against model stealing attacks with adaptive misinformation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 770–778, 2020. 2
[11] Manjit Kaur and Vijay Kumar. A comprehensive review
on image encryption techniques. Archives of Computational
Methods in Engineering , 27:15–43, 2020. 2
[12] Erwan Le Merrer, Patrick Perez, and Gilles Tr ´edan. Ad-
versarial frontier stitching for remote neural network wa-
termarking. Neural Computing and Applications , 32:9233–
9244, 2020. 2
[13] Guobiao Li, Sheng Li, Zhenxing Qian, and Xinpeng Zhang.
Encryption resistant deep neural network watermarking.
InICASSP 2022-2022 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages
3064–3068. IEEE, 2022. 2
[14] Qinbin Li, Bingsheng He, and Dawn Song. Model-
contrastive federated learning. In Proceedings of theIEEE/CVF conference on computer vision and pattern
recognition , pages 10713–10722, 2021. 2
[15] Yiming Li, Linghui Zhu, Xiaojun Jia, Yong Jiang, Shu-Tao
Xia, and Xiaochun Cao. Defending against model stealing
via verifying embedded external features. In Proceedings of
the AAAI Conference on Artificial Intelligence , pages 1464–
1472, 2022. 2
[16] Junxu Liu, Mingsheng Xue, Jian Lou, Xiaoyu Zhang, Li
Xiong, and Zhan Qin. Muter: Machine unlearning on ad-
versarially trained models. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 4892–
4902, 2023. 2
[17] Songhua Liu and Xinchao Wang. Mgdd: A meta generator
for fast dataset distillation. In Advances in Neural Informa-
tion Processing Systems , 2023. 2
[18] Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and Xin-
chao Wang. Dataset distillation via factorization. In Ad-
vances in Neural Information Processing Systems , 2022.
[19] Songhua Liu, Jingwen Ye, Runpeng Yu, and Xinchao Wang.
Slimmable dataset condensation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3759–3768, 2023. 2
[20] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11461–11471, 2022. 2
[21] Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xi-
aohui Xie, and Zhangyang Wang. Undistillable: Making a
nasty teacher that cannot teach students. In International
Conference on Learning Representations , 2020. 3, 5
[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learning
models resistant to adversarial attacks. International Confer-
ence on Learning Representations , 2017. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 4
[24] Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun,
and Jiliang Tang. Transferable unlearnable examples. In The
Eleventh International Conference on Learning Representa-
tions , 2022. 1, 2
[25] Vinu Sankar Sadasivan, Mahdi Soltanolkotabi, and Soheil
Feizi. Cuda: Convolution-based unlearnable datasets. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 3862–3871, 2023. 8
[26] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P
Dickerson, and Tom Goldstein. Just how toxic is data poison-
ing? a unified benchmark for backdoor and data poisoning
attacks. In International Conference on Machine Learning ,
pages 9389–9398. PMLR, 2021. 3
[27] Sebastian Szyller, Buse Gul Atli, Samuel Marchal, and N
Asokan. Dawn: Dynamic adversarial watermarking of neu-
ral networks. In Proceedings of the 29th ACM International
Conference on Multimedia , pages 4417–4425, 2021. 2
11952
[28] Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.
Towards personalized federated learning. IEEE Transactions
on Neural Networks and Learning Systems , 2022. 2
[29] Ayush K Tarun, Vikram S Chundawat, Murari Mandal, and
Mohan Kankanhalli. Fast yet effective machine unlearning.
IEEE Transactions on Neural Networks and Learning Sys-
tems, 2023. 2
[30] Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and
Ling Liu. Data poisoning attacks against federated learning
systems. In Computer Security–ESORICS 2020: 25th Eu-
ropean Symposium on Research in Computer Security, ES-
ORICS 2020, Guildford, UK, September 14–18, 2020, Pro-
ceedings, Part I 25 , pages 480–501. Springer, 2020. 3
[31] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-
Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah
Laszlo, David J Fleet, Radu Soricut, et al. Imagen editor
and editbench: Advancing and evaluating text-guided im-
age inpainting. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 18359–
18369, 2023. 2
[32] Xingyuan Wang, Le Feng, and Hongyu Zhao. Fast image
encryption algorithm based on parallel computing system.
Information Sciences , 486:340–358, 2019. 2
[33] Hanyu Xiang, Qin Zou, Muhammad Ali Nawaz, Xianfeng
Huang, Fan Zhang, and Hongkai Yu. Deep learning for im-
age inpainting: A survey. Pattern Recognition , 134:109046,
2023. 2
[34] Jingwen Ye, Yining Mao, Jie Song, Xinchao Wang, Cheng
Jin, and Mingli Song. Safe distillation box. In Proceedings of
the AAAI Conference on Artificial Intelligence , pages 3117–
3124, 2022. 3
[35] Jingwen Ye, Songhua Liu, and Xinchao Wang. Partial net-
work cloning. 2023 IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 20137–20146,
2023. 2
[36] Jingwen Ye, Ruonan Yu, Songhua Liu, and Xinchao Wang.
Mutual-modality adversarial attack with semantic perturba-
tion. AAAI Conference on Artificial Intelligence , 2024. 3
[37] Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset dis-
tillation: A comprehensive review. In IEEE Transactions on
Pattern Analysis and Machine Intelligence , 2024. 2
[38] Chia-Hung Yuan and Shan-Hung Wu. Neural tangent gen-
eralization attacks. In International Conference on Machine
Learning , pages 12230–12240. PMLR, 2021. 8
[39] Chaoning Zhang, Philipp Benz, Chenguo Lin, Adil Karjauv,
Jing Wu, and In So Kweon. A survey on universal adversar-
ial attack. International Joint Conference on Artificial Intel-
ligence , 2021. 3
[40] Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and
Yuan Gao. A survey on federated learning. Knowledge-
Based Systems , 216:106775, 2021. 2
[41] Xuezhou Zhang, Xiaojin Zhu, and Laurent Lessard. Online
data poisoning attacks. In Learning for Dynamics and Con-
trol, pages 201–210. PMLR, 2020. 3
11953
