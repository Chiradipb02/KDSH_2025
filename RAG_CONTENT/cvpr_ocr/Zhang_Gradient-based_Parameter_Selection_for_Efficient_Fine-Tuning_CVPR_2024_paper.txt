Gradient-based Parameter Selection for Efficient Fine-Tuning
Zhi Zhang1,2*, Qizhe Zhang2*, Zijun Gao2, Renrui Zhang3, Ekaterina Shutova1, Shiji Zhou4, Shanghang Zhang2†
1ILLC, University of Amsterdam,
2National Key Laboratory for Multimedia Information Processing,
School of Computer Science, Peking University,
3MMLAB, The Chinese University of Hong Kong,
4Department of Automation, Tsinghua University,
{z.zhang, e.shutova }@uva.nl, {theia, shanghang }@pku.edu.cn
Abstract
With the growing size of pre-trained models, full fine-
tuning and storing all the parameters for various down-
stream tasks is costly and infeasible. In this paper,
we propose a new parameter-efficient fine-tuning method,
Gradient-based Parameter Selection (GPS), demonstrating
that only tuning a few selected parameters from the pre-
trained model while keeping the remainder of the model
frozen can generate similar or better performance com-
pared with the full model fine-tuning method. Different
from the existing popular and state-of-the-art parameter-
efficient fine-tuning approaches, our method does not in-
troduce any additional parameters and computational costs
during both the training and inference stages. Another ad-
vantage is the model-agnostic and non-destructive property,
which eliminates the need for any other design specific to
a particular model. Compared with the full fine-tuning,
GPS achieves 3.33% (91.78% vs. 88.45%, FGVC) and
9.61% (73.1% vs. 65.57%, VTAB) improvement of the accu-
racy with tuning only 0.36% parameters of the pre-trained
model on average over 24 image classification tasks; it
also demonstrates a significant improvement of 17% and
16.8% in mDice and mIoU, respectively, on medical im-
age segmentation task. Moreover, GPS achieves state-of-
the-art performance compared with existing PEFT meth-
ods. The code will be available in https://github.
com/FightingFighting/GPS.git .
1. Introduction
The pre-training and fine-tuning pipeline has become a
common paradigm for adapting large models pre-trained on
substantial amounts of data to downstream tasks with fewer
training samples. However, fine-tuning all the parameters in
the model is memory-intensive and data-inefficient, which
Figure 1. Comparison between our GPS and other PEFT methods.
(a) Exiting popular methods introduce extra parameters for tuning
downstream tasks, which might need a special design for diverse
architectures, such as appending prompt into the input token in
Transformer or inserting different modules into different layers (b)
Our approach avoids the introduction of additional parameters and
solely fine-tunes the selected parameters from the model, employ-
ing a unified gradient-based parameter selection method across di-
verse architectural variations, e.g. Transformer and CNN.
is costly and infeasible for multiple downstream tasks given
a large-scale model [36, 43, 58]. To tackle this issue,
parameter-efficient fine-tuning (PEFT) methods have been
proposed with the aim of tuning a minimal number of pa-
rameters to fit downstream tasks while keeping most of the
parameters frozen. Another benefit of PEFT is that tuning
a smaller set of parameters reduces the complexity of opti-
mization and alleviates overfitting concerns when adapting
large pre-trained models to downstream tasks with limited
data, resulting in comparable or even superior performance
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28566
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni0000001b/uni00000013
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000025/uni0000004c/uni00000044/uni00000056
/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055/uni00000036/uni00000033/uni00000037/uni00000010/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000036/uni00000033/uni00000037/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024
/uni00000039/uni00000033/uni00000037/uni00000010/uni00000036/uni0000004b/uni00000044/uni0000004f/uni0000004f/uni00000052/uni0000005a/uni00000039/uni00000033/uni00000037/uni00000010/uni00000027/uni00000048/uni00000048/uni00000053/uni00000036/uni00000036/uni00000029/uni0000002a/uni00000033/uni00000036/uni0000000b/uni00000052/uni00000058/uni00000055/uni00000056/uni0000000c
/uni0000001c/uni0000001c/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000029/uni00000058/uni0000004f/uni0000004f/uni00000003/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a
/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c(a) VTAB-1K
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 /uni00000014/uni00000011/uni00000015/uni00000018/uni0000001a/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000019/uni0000001b/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000015
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000025/uni0000004c/uni00000044/uni00000056
/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055/uni00000036/uni00000033/uni00000037/uni00000010/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055
/uni0000002f/uni00000052/uni00000035/uni00000024/uni00000036/uni00000033/uni00000037/uni00000010/uni0000002f/uni00000052/uni00000035/uni00000024
/uni00000039/uni00000033/uni00000037/uni00000010/uni00000036/uni0000004b/uni00000044/uni0000004f/uni0000004f/uni00000052/uni0000005a/uni00000039/uni00000033/uni00000037/uni00000010/uni00000027/uni00000048/uni00000048/uni00000053/uni00000036/uni00000036/uni00000029/uni0000002a/uni00000033/uni00000036/uni0000000b/uni00000052/uni00000058/uni00000055/uni00000056/uni0000000c
/uni0000001c/uni0000001c/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000029/uni00000058/uni0000004f/uni0000004f/uni00000003/uni00000049/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a
/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c (b) FGVC
Figure 2. Performance comparisons of 11 fine-tuning methods
with a pre-trained ViT-B/16 model on the VTAB-1k (a) and FGVC
(b) benchmarks. Our GPS (red stars) achieves state-of-the-art per-
formance on both benchmarks with only 0.25% and 0.77% average
trainable parameters respectively.
compared to full fine-tuning [43]. Inspired by the success of
PEFT in NLP [13, 31, 38, 40, 57, 79], several methods have
been introduced to vision tasks, such as Adapter [36] and
Visual Prompt Tuning (VPT) [43] introducing extra learn-
able parameters into the backbone and the input space of the
pre-trained model respectively. SSF, another representative
method, transforms features across layers of the pre-trained
model using extra learnable layers [58].
However, these methods introduce additional parameters
into the pre-trained model and disrupt its original architec-
ture, leading to increased computational costs during train-
ing and/or inference stages. Furthermore, these approaches
lack generalizability across various model architectures.
Specifically, different models are equipped with distinct
components (layers), such as MLPs, activation functions,
and self-attention layers. These methods need to deter-
mine the optimal locations for inserting extra parameters
between different layers; moreover, certain transformer-
based techniques cannot be directly applied to convolution-
based methods like VPT. Therefore, these methods exhibit
limited compatibility with diverse architectures.
To tackle the above issues, we propose a non-destructive
network architecture and model-agnostic PEFT approach,
which introduces no extra parameters during both training
and test stages and provides a unified solution for various
architectures. We select a small number of essential param-
eters from the pre-trained model and only fine-tune these
parameters for the downstream tasks. To select them, we
propose a fine-grained Gradient-based Parameter Selection
(GPS) method. For each neuron in the network, we choose
top-K of its input connections (weights or parameters) with
the highest gradient value, resulting in a small proportion of
the parameters in the model being selected.
Such design offers five-fold benefits: i) The pre-trainedMethodMean Params. Model No extra No extra Task
Acc. (%) Agnostic Train param. Infer params. Adaptive
Full [43] 70.36 100 ✓ ✓ ✓ ✗
Linear [43] 58.48 0.08 ✓ ✓ ✓ ✗
Bias [92] 67.54 0.20 ✓ ✓ ✓ ✗
Adapter [36] 60.04 0.35 ✗ ✗ ✗ ✗
VPT [43] 73.53 0.76 ✗ ✗ ✗ ✗
LoRA [38] 75.16 0.90 ✗ ✗ ✓ ✗
SSF [58] 76.77 0.32 ✗ ✗ ✓ ✗
GPS (ours) 78.64 0.36 ✓ ✓ ✓ ✓
Table 1. Comparison between different fine-tuning methods. The
ViT-B/16 model accuracy over all 24 tasks in FGV A and VTAB
fine-tuned on ViT-B/16 model and the number of tunable parame-
ters are shown in columns Acc. and Params. (%).
model can efficiently tackle downstream tasks because the
gradient direction indicates the fastest loss function changes
and highest change rate, facilitating efficient gradient de-
scent during model fine-tuning. We also provide a sparse
regularized equivalent form for GPS, which indicates better
generalization than full fine-tuning; ii) Each neuron within
the network possesses the potential to adjust its activation
state by fine-tuning selected input connections. Conse-
quently, the pre-trained model exhibits flexibility in modi-
fying features of varying granularities to suit diverse down-
stream tasks. For instance, when adapting a model pre-
trained on ImageNet [10] for CIFAR-100 [75], it is nec-
essary to refine high-level features; whereas for ImageNet-
Sketch [85] adaptation, more detailed feature fine-tuning is
required. iii) Our approach avoids introducing extra param-
eters and computational costs and keeps the architecture of
the model intact; iv) The selection procedure enables its
application across diverse models by adopting a neuron-
based rather than a layer-based method, thereby eliminat-
ing the necessity for distinct designs for different layers in
various models. v) Different from other methods using a
pre-defined and consistent strategy for different tasks, our
method adaptively selects parameters for each task by our
proposed gradient strategy to better fit the domain-specific
semantics of different downstream tasks. Please see the dif-
ference between our method with others in Fig. 1 and Tab. 1.
We evaluate our approach on a total of 27 visual tasks
(including image classification and semantic segmentation)
over 4 different model architectures. Our GPS achieves
state-of-the-art performance compared to other PEFT meth-
ods and has a good balance between performance and the
number of trainable parameters, as illustrated in Fig. 2.
Compared with the full fine-tuning, GPS achieves 3.33%
(FGVC) and 9.61% (VTAB) improvement of the accu-
racy while tuning only 0.36% parameters of the pre-trained
model on average over 24 tasks; it also demonstrates a
significant improvement of 17% and 16.8% in mDice and
mIoU, respectively, on medical image segmentation task.
28567
Moreover, we verify the effectiveness of our approach on
different network architectures, such as Transformer and
Convolutional Neural Networks. Furthermore, we compare
GPS with various parameter selection methods and demon-
strate its superior properties. GPS provides a new paradigm
for PEFT and inspires deeper insights into this field.
2. Related work
Visual parameter efficient fine-tuning In general, there
are typically two primary categories of PEFT. Addition-
based methods introduce additional parameters to the pre-
trained backbone. Adapters [20, 21, 36, 69, 70, 75, 78, 80,
86, 94–96] adopt a residual pathway and learn down and up
projection with a nonlinear activation. Others [63] propose
a hyper-network to generate model weights or decompose
the dense weighted matrix into the low-rank matrix [46].
Prompt methods [12, 22, 39, 45, 57, 59, 60, 97, 101] wrap
the input with context. VPT [43] prepend learnable prompts
to the input tokens. SSF [58] achieves promising results by
scaling and shifting the feature between layers. Selection-
based methods select a subset of the parameters for tuning,
such as only fine-tuning bias [92], last K layers [36, 43].
While traditionally considered less effective than addition-
based methods, our approach of adaptively selecting param-
eters for each task yielded surprisingly strong results.
Sub-network training Pruning technique [19, 28, 29, 52,
56, 88] uncovers the importance of subnetworks. The lot-
tery ticket hypothesis [17] articulates that subnetworks can
reach the accuracy of the original model. Fine-tuning sub-
networks are widely studied. SpotTune [27] designs a pol-
icy network to make routing decisions for subset networks.
Child-tuning [90] iteratively updates a subset of parame-
ters by masking out some gradients during the backward
process. However, these methods are not aligned with the
PEFT setting. We fix a small number of parameters and
only tune them for fitting downstream to achieve PEFT.
3. Approach
Different from the currently popular methods introducing
additional parameters to fine-tune the pre-trained model
for downstream tasks [36, 43, 58], we select only a small
number of parameters from the pre-trained model and then
only update these parameters during the fine-tuning stage.
Specifically, our method has two stages: parameter selec-
tion and masked fine-tuning. For each downstream task t,
we first select a small portion of important parameters (task-
specific parameters) from the original pre-trained model us-
ing a gradient-based method. We then fine-tune the pre-
trained model for the task t, keeping all other unimportant
parameters frozen and updating only selected parametersusing a sparse binary mask to set the gradient of unimpor-
tant parameters to zero (see Fig. 3).
3.1. Gradient-based parameter selection
Relevant studies have indicated that the pre-trained back-
bone exhibits diverse feature patterns at distinct parameter
positions, and the same positions make varying contribu-
tions to fine-tuning various tasks [4, 53, 65, 74, 91]. There-
fore, we posit that there exists an optimal subset of param-
eters for fine-tuning a pre-trained model to a downstream
task. This subset is essential and necessary for fine-tuning
the task, and the different tasks require a distinct subset.
Formally, given a downstream task twith the dataset Dtand
a pre-trained model Θ ={w1, w2, . . . , w N}, we aim to find
a subset of w, i.e.w={w1, w2, . . . , w n}(n≪N). we
select parameters following two principles: 1) Important for
downstream tasks; 2) Distributed over the whole network.
Importance for downstream tasks We identify the im-
portance of parameters in a pre-trained model for a spe-
cific task by selecting those with the highest gradient value,
which is obtained by calculating the gradient of a loss func-
tion with respect to its parameters. The intuition behind
this is that the parameters with the largest gradient value in-
dicate the loss function changes fastest along the gradient
direction and has the greatest change rate, which facilitates
efficient gradient descent during fine-tuning. Specifically,
the gradient of the parameters is calculated by
∇LDt(Θ) =∂L
∂w1···∂L
∂wN⊤
(1)
where L(w)is the loss function. Normally, when we fine-
tune a pre-trained model on a downstream task, we need
a new classification head ( i.e.MLP) with random initial-
ization. In order to avoid the adverse effects of these ran-
domly initialized parameters on gradient calculation using
the cross-entropy loss function, we use Supervised Con-
trastive Loss (SCL) [48] as the loss function for calculat-
ing the gradient during parameter selection, since it does
not need to involve the head (We still use cross-entropy
loss during fine-tuning stage). SCL is a variant of Con-
trastive Loss (CL) that aims to bring different augmented
samples of the same image closer together in embedding
space. In contrast, SCL tries to cluster samples from the
same class together, which coincides with our target of the
downstream classification tasks. Specifically, given a task
with the dataset Dt={xi, yi}i=1...K, SCL is calculated by
Lscl=X
i∈DtLscl
i
=X
i∈Dt−1
|P(i)|X
p∈P(i)logexp (zi⊙zp/τ)P
a∈A(i)exp (zi⊙za/τ)(2)
28568
Figure 3. The overall pipeline of GPS. We first select a small portion of important parameters (sub-network) for each task from the original
pre-trained model using a gradient-based method. Then only fine-tune the sub-network while keeping other parameters frozen.
where irepresents ithsample in Dt;P(i)≡
{p∈A(i) : ˜yp= ˜yi}is subset of Dt, in which all samples
have the same class with i;A(i)≡ D t\{i};zis the fea-
ture extacted from the pre-trained encoder and τ∈ R+is a
scalar temperature parameter.
Equivalent with sparse regularization In the above, we
implicitly assume that the order of∂L
∂w1···∂L
∂wN
is
the same as (∥w′
1−w1∥···∥ w′
N−wN∥), which means se-
lecting parameters with top-n gradient norm is the same as
selecting top-n of the fine-tuning changes. Therefore, GPS
captures the top-n important parameters for downstream
tasks. The optimization objective can be rewritten as
Θ′= min L(Θ′) s.t.∥Θ′−Θ∥0≤n (3)
where ∥∥0is the l0norm and Θ′is the fine-tuned model. By
Lagrangian duality, solving the above problem is equivalent
to solving the following problem:
L(Θ′) +λ∥Θ′−Θ∥0 (4)
with appropriate λ. Hence, GPS can be reviewed as a sparse
regularized fine-tuning, which may lead to better general-
ization. Fu et al. [18] demonstrate that Eq. (4) has smaller
generalization bound than pure optimization toward Lwith
full fine-tuning, resulting in better performance.
Distribution over the whole network A simple idea for
parameter selection is to select a certain percentage of pa-
rameters with the highest gradient from the entire network.
Our experiments have shown that with this idea, the ma-
jority of the selected parameters are located in the top lay-
ers of the network (see Supplementary for details), which is
consistent with the findings reported in [36, 37]. However,
solely fine-tuning these top-layer parameters is insufficient
to mitigate the impact of the pre-trained model’s own induc-
tive bias, particularly when there exist substantial dispari-
ties in data distributions between upstream and downstreamtasks, which need to fine-tune more detailed features from
shallower layers. Motivated by various studies indicating
the distinct roles played by different components of neural
networks [3, 15, 72, 77, 87], we posit that when fine-tuning
a pre-trained model for downstream tasks, the adjusted pa-
rameters should be distributed throughout the entire net-
work. The reason behind this lies in the ability of the model
to adapt the information stored in parameters at different
levels of granularity to fit downstream tasks. Therefore, our
strategy is that for each neuron in the network, we select
top-K (at least one) connections (weights) among all the in-
put connections of the neuron, as shown in Fig. 3. By doing
so, every neuron within the network possesses the poten-
tial to fine-tune its activation state rather than solely adjust
high-level information in the top layers. In other words, our
approach fine-tunes the detailed information stored in each
neuron of the network, which better fits the downstream task
during the fine-tuning stage. Our exploratory experiment
further substantiates this assertion, as shown in Tab. 6(a).
Combining the two points above, we first calculate the
gradient of the loss with respect to all the weights in the
models for a specific task. Then for each neuron in the net-
work, we select top-K connections with the highest gradient
value (the modulus of gradient) among all input connections
to the neuron. Doing so can not only ensure that important
parameters for downstream tasks are chosen and allow the
model to tune the activation state of all neurons for better fit-
ting of downstream tasks. Another benefit of this selection
procedure is its ease of application across various model
architectures, such as Transformer and CNN, avoiding any
model-specific design. Our experiments also demonstrate
the effectiveness of our approach across diverse architec-
tures, as shown in Tab. 2 and Tab. 4.
3.2. Masked fine-tuning
After parameter selection for a specific task, we fine-tune
the pre-trained model on the task. During fine-tuning, we
only update the selected parameters while keeping the re-
maining parameters of the pre-trained model frozen. As our
28569
selected parameters are distributed across all neurons in ev-
ery layer, only a few parameters within a specific weight
matrix of the network are chosen, resulting in the updated
matrix being sparse. Therefore, we utilize a mask to help
with the sparse training. Specifically, for jthweight matrix
Wj∈Rdin×doutin the network, we build a same size of
binary mask Mj∈Rdin×dout:
Mj=1, wk
j∈w
0, wk
j/∈w(5)
where wk
jrepresents kthelement in jthweight matrix. For
each element in Mj, its value is set to 1 if the corresponding
parameter in Wjis selected, and 0 otherwise. Then the
weight matrix is updated by
Wj←Wj−ϵ∇L(Wj)⊙Mj (6)
where ∇L(Wj)is the gradient of the cross-entropy loss
with respect to Wj. As a result, the gradients of unselected
parameters are zeroed out and excluded from updates, while
only a small number of our selected parameters are updated
during fine-tuning for downstream tasks. Please see Fig. 3
for a visualization of our method.
4. Experiments
We evaluate GPS on various downstream tasks, including
image classification tasks and semantic segmentation tasks
with different architectures. First, we briefly introduce our
experimental settings, including datasets, backbones, and
baselines. Then we demonstrate the effectiveness and uni-
versality of GPS. Moreover, we systematically study the
impacts of different selection schemes and conduct com-
prehensive ablation experiments.
4.1. Experimental settings
Datasets Following VPT [43] and SSF [58], we evalu-
ate our GPS method on a series of datasets categorized
into three groups: i) FGVC : Fine-Grained Visual Classi-
fication (FGVC) benchmark includes 5 downstream tasks,
which are CUB-200-2011 [84], NABirds [81], Oxford
Flowers [67], Stanford Dogs [47] and Stanford Cars [23].
ii)VTAB-1k : Visual Task Adaptation Benchmark [93]
(VTAB) contains 19 visual classification tasks which are
grouped into three sets: Natural, Specialized, and Struc-
tured. iii) CIFAR-100 [51] and ImageNet-1k [10]: widely
use for general image classification task.
Backbones For a fair comparison, we follow VPT and
SSF by using ViT-B/16 [14] pre-trained on ImageNet-
21K [10] for the main image classification experiments.
Moreover, to demonstrate the universality of our GPS,
we also explore other backbones, including Swin Trans-
former [61] and ConvNeXt-B [62] for another variant ofDatasetCUB
-2011NA-
BridsOxford
FlowersStan.
DogsStan.
CarsMean
Acc.Params.
(%)
Full [43] 87.3 82.7 98.8 89.4 84.5 88.54 100.00
Linear [43] 85.3 75.9 97.9 86.2 51.3 79.32 0.21
Bias [92] 88.4 84.2 98.8 91.2 79.4 88.40 0.33
Adapter [36] 87.1 84.3 98.5 89.8 68.6 85.66 0.48
LoRA [38] 85.6 79.8 98.9 87.6 72.0 84.78 0.90
VPT-Shallow [43] 86.7 78.8 98.4 90.7 68.7 84.62 0.29
VPT-Deep [43] 88.5 84.2 99.0 90.2 83.6 89.11 0.99
SSF [58] 89.5 85.7 99.6 89.6 89.2 90.72 0.45
SPT-Adapter [30] 89.1 83.3 99.2 91.1 86.2 89.78 0.47
SPT-LoRA [30] 88.6 83.4 99.5 91.4 87.3 90.04 0.60
GPS (Ours) 89.9 86.7 99.7 92.2 90.4 91.78 0.77
Table 2. Performance comparisons on FGVC with ViT-B/16 mod-
els pre-trained on ImageNet-21K.
Transformer-based and CNN-based architecture, respec-
tively. In addition, we finetune semantic segmentation tasks
on SAM [50], a strong segmentation foundation model.
Baselines We compare our GPS with a variety of fine-
tuning protocols that can be mainly categorized into three
types: i) Full: Full fine-tuning is the most commonly used
protocol updating all parameters of the whole model during
tuning. ii) Selection-based : This kind of method selects a
subset of parameters in the original model for fine-tuning,
including linear probing and Bias [92]. Such methods are
easy to implement and require no extra computations but
have not worked well. Our method belongs to this group
and achieves the best performance while ensuring conve-
nience and universality. iii) Addition-based : This kind of
method adds new trainable parameters to the backbone,
including Adapter [36], VPT [43] and SPT-Adapter [30].
Such methods require extra computations in both the train-
ing and inference stages. Other methods like LoRA [38],
SSF [58], and SPT-LoRA [30] also add new tunable param-
eters during the training stage, but these parameters can be
reparameterized into the backbone during testing.
Implementation details We follow SSF to process the
images in all the FGVC, VTAB-1k and CIFAR-100
datasets. We employ the Adam [49] optimizer with cosine
learning rate decay to fine-tune models for 100 epochs, and
the linear warm-up is used in the first 10 epochs. All exper-
iments are conducted on the NVIDIA A100 GPU.
4.2. Performance on image classification
We present a comprehensive evaluation of the effectiveness
of our GPS by comparing it against multiple baselines on 3
benchmarks, comprising a total of 26 datasets. In addition
to common benchmarks (FGVC and VTAB-1k), we also
compare our method with others on different architectures.
We evaluate the performance and effectiveness by Top-1 ac-
curacy (%) and the number of fine-tuned parameters.
28570
MethodDatasetNatural Specialized Structured VTABCIFAR-100
Caltech101
DTD
Flowers102
Pets
SVHN
Sun397
Patch Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr/count
Clevr/distance
DMLab
KITTI/distance
dSprites/loc
dSprites/ori
SmallNORB/azi
SmallNORB/ele
Mean Acc.
Mean Params. (%)
Full [43] 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 65.57 100.00
Linear [43] 63.4 85.0 64.3 97.0 86.3 36.6 51.0 78.5 87.5 68.6 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 53.00 0.05
Bias [92] 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 62.05 0.16
Adapter [36] 74.1 86.1 63.2 97.7 87.0 34.6 50.8 76.3 88.0 73.1 70.5 45.7 37.4 31.2 53.2 30.3 25.4 13.8 22.1 55.82 0.31
LoRA [38] 68.1 91.4 69.8 99.0 90.5 86.4 53.1 85.1 95.8 84.7 74.2 83.0 66.9 50.4 81.4 80.2 46.6 32.2 41.1 72.63 0.90
VPT-Shallow [43] 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 64.85 0.13
VPT-Deep [43] 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 69.43 0.70
SSF [58] 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 73.10 0.28
SPT-ADAPTER [30] 72.9 93.2 72.5 99.3 91.4 88.8 55.8 86.2 96.1 85.5 75.5 83.0 68.0 51.9 81.2 51.9 31.7 41.2 61.4 73.03 0.44
SPT-LoRA [30] 73.5 93.3 72.5 99.3 91.5 87.9 55.5 85.7 96.2 85.9 75.9 84.4 67.6 52.5 82.0 81.0 51.1 30.2 41.3 74.07 0.63
GPS (Ours) 81.1 94.2 75.8 99.4 91.7 91.6 52.4 87.9 96.2 86.5 76.5 79.9 62.6 55.0 82.4 84.0 55.4 29.7 46.1 75.18 0.25
Table 3. Performance comparisons on VTAB-1k with ViT-B/16 models pre-trained on ImageNet-21K.
Image classification performance As shown in Tab. 2
and Tab. 3, our GPS outperforms all other fine-tuning meth-
ods by a large margin on both FGVC and VTAB bench-
marks, sufficiently demonstrating that our method of pa-
rameter selection is a simple yet effective way for model
tuning. On FGVC, GPS outperforms all other fine-tuning
methods, including full fine-tuning, on all 5 tasks. It ob-
tains 1.02% and 3.24% accuracy improvement of the mean
accuracy compared to the previous SOAT method SSF [58]
and full fine-tuning, while it only uses 0.77% of trainable
parameters. On VTAB, GPS also outperforms all other fine-
tuning methods. Specifically, it obtains 1.11% and 9.61%
improvement of the mean accuracy on 19 VTAB tasks com-
pared to the previous SOAT method SPT-LoRA [30] and
full fine-tuning. GPS beats the previous SOTA by 1.75%,
0.23%, and 0.63% in the Natural, Specialized and Struc-
tured subsets, respectively. Meanwhile, GPS also uses
fewer trainable parameters compared to VPT-Deep, SSF,
and SPT-LoRA (0.25% vs. 0.70%, 0.28% and 0.63%),
which further illustrates the high efficiency of our approach.
For most tasks, we exclusively select the top 1 input connec-
tion for each neuron; however, for more challenging tasks,
multiple connections are chosen (see Supplement for de-
tails). The percentage of learnable parameters in our GPS
can be explicitly controlled by adjusting the number of con-
nections selected, allowing for a balance between parameter
count and performance in tasks.
Generalization on different architectures Since our
method only selects a subset of parameters from the pre-
trained model for fine-tuning, it is naturally model-agnostic.
We compare GPS with other representative methods across
ViT-B/16 (Tab. 2), Swin-B and ConvNeXt-B architectures
on the FGVC dataset (Tab. 4), CIFAR-100 and ImageNet-
1k (Please see full results in Supplementary). Among all
three architectures, GPS consistently outperforms all otherArchitectureSwin-B ConvNeXt-B
Ave. Acc. Params.(%) Ave. Acc. Params.(%)
Full [43] 92.42 100.00 93.04 100.00
Linear [43] 87.90 0.28 88.00 0.28
SSF [58] 91.54 0.56 92.48 0.56
GPS (Ours) 92.56 0.95 93.32 0.90
Table 4. Performance comparisons on FGVC benchmark (Average
accuracy over 5 tasks) with different model architectures.
baselines, demonstrating its model-agnostic advantage. The
Swin and Convnext have more complex designs than ViT,
enabling them to acquire comprehensive and high-quality
features during pre-training. Consequently, even the sim-
plest linear probing method yields commendable results on
these two architectures, reducing the effectiveness of the
PEFT method and causing the previous SOTA SSF to under-
perform Full. However, in this scenario, our GPS still main-
tains a lead over Full with gains of 0.12% and 0.28%, re-
spectively, further showing the effectiveness of our method.
Computational cost In Fig. 4, we compare the computa-
tional cost of GPS with other fine-tuning methods to demon-
strate the efficiency of our approach. Following SSF [58],
we reimplement VPT [43] with 200 and 50 prompts for the
shallow and deep versions, respectively. A batch size of 32
is used in both the training and inference stages. For a fair
comparison, for all experiments, we do not use mixed preci-
sion training, which was used in SSF. All metrics are mea-
sured on a single NVIDIA A100 GPU. In the training stage,
GPS has less time and memory consumption than both VPT
and SSF. Compared with full fine-tuning, GPS has a much
lower time overhead and a similar memory overhead, but it
leads to an increased performance by a large margin. Since
GPS is a selection-based method, it does not introduce any
additional parameters, so it can achieve the same minimum
28571
/uni00000059/uni00000053/uni00000057/uni00000010/uni00000056/uni0000004b/uni00000011/uni00000049/uni00000058/uni0000004f/uni0000004f/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000045/uni0000004c/uni00000044/uni00000056/uni00000044/uni00000047/uni00000044/uni00000053/uni00000057/uni00000011/uni00000056/uni00000056/uni00000049
/uni00000059/uni00000053/uni00000057/uni00000010/uni00000047/uni00000048/uni00000011/uni0000004a/uni00000053/uni00000056/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c
/uni00000059/uni00000053/uni00000057/uni00000010/uni00000056/uni0000004b/uni00000011/uni00000049/uni00000058/uni0000004f/uni0000004f/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000045/uni0000004c/uni00000044/uni00000056/uni00000044/uni00000047/uni00000044/uni00000053/uni00000057/uni00000011/uni00000056/uni00000056/uni00000049
/uni00000059/uni00000053/uni00000057/uni00000010/uni00000047/uni00000048/uni00000011/uni0000004a/uni00000053/uni00000056/uni00000013/uni00000014/uni00000013/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002a/uni0000000c
/uni00000059/uni00000053/uni00000057/uni00000010/uni00000056/uni0000004b/uni00000011/uni00000049/uni00000058/uni0000004f/uni0000004f/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000045/uni0000004c/uni00000044/uni00000056/uni00000044/uni00000047/uni00000044/uni00000053/uni00000057/uni00000011/uni00000056/uni00000056/uni00000049
/uni00000059/uni00000053/uni00000057/uni00000010/uni00000047/uni00000048/uni00000011/uni0000004a/uni00000053/uni00000056/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000050/uni00000056/uni0000000c
/uni00000059/uni00000053/uni00000057/uni00000010/uni00000056/uni0000004b/uni00000011/uni00000049/uni00000058/uni0000004f/uni0000004f/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000045/uni0000004c/uni00000044/uni00000056/uni00000044/uni00000047/uni00000044/uni00000053/uni00000057/uni00000011/uni00000056/uni00000056/uni00000049
/uni00000059/uni00000053/uni00000057/uni00000010/uni00000047/uni00000048/uni00000011/uni0000004a/uni00000053/uni00000056/uni00000013/uni00000014/uni00000015/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000050/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c/uni00000003/uni0000000b/uni0000002a/uni0000000cFigure 4. Computational cost of different tuning methods. From left to right: training time, training memory, test time, and test memory.
Training/Test time is the time consumed by a mini-batch.
Method mDice ( ↑) mIoU ( ↑)Params. (M)
Full [43] 71.1 55.7 93.8
Linear [43] 71.6 46.6 4.06
Bias [92] 86.5 69.1 4.16
Adapter [6] 84.8 66.7 4.12
SSF [58] 87.3 71.7 4.26
GPS (Ours) 88.1 72.5 4.22
Table 5. Quantitative Result for Polyp Segmentation
time and memory overhead as full fine-tuning during in-
ference without any reparameterization operation, which is
much lower than the addition-based Adapter and VPT.
4.3. Semantic segmentation
In addition to visual classification tasks, we also explore
our method for the task of semantic segmentation. Segment
Anything Model (SAM) [50] is a strong foundation model
for segmentation. It is pre-trained on a large-scale dataset
enabling powerful generalization. However, several studies,
e.g. [6], have reported poor performance of SAM on medi-
cal segmentation tasks such as polyp segmentation [41]. To
address this limitation, they proposed employing Adapter to
effectively fine-tune SAM for downstream medical segmen-
tation tasks. Following their experimental setup, we applied
our method to SAM and conducted a comparative analysis
with other PEFT approaches. Our GPS yielded exceptional
results, as shown in Tab. 5 and visually depicted in Fig. 5
(See Supplementary for more case visualization).
4.4. Impacts of different selection schemes
Different selection levels Our GPS selects trainable pa-
rameters at the neuron level, i.e. selecting top-k input con-
nection per neuron. We also investigate parameter selection
methods at different levels. As shown in Tab. 6 (a), Netand
Layer represent selecting a certain proportion of the param-
eters with the highest gradient based on the entire network
and each layer, respectively. For a fair comparison, we keep
the same number of parameters selected over these levels.
Figure 5. The Visualization of Polyp segmentation task. Our GPS
can still handle difficult segmentation cases compared with others.
We can see that the finer the granularity of selection, the
better the performance. For example, the accuracy on CUB
increases by 0.44% and 0.77% when selection level changes
from network to layer, and from layer to neuron.
Different selection criteria We further study the effec-
tiveness of our gradient-based selection method by com-
paring different selection criteria. As shown in Tab. 6 (b),
Net Random andNeuron Random means randomly select-
ing top-K the input connection for each neuron and select-
ing the same number of parameters based on the whole net-
work respectively. Magnitude means selecting top-K input
connections with the largest weight per neuron. As we can
see, the increase in the randomness of parameter selection
causes a decrease in performance ( Net Random <Neuron
Random ). The result of Magnitude is similar to Neuron
Random , demonstrating neuron-level selection is crucial.
28572
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013/uni00000014/uni00000014/uni00000014/uni00000015
/uni00000025/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000047/uni00000048/uni00000046/uni00000055/uni00000048/uni00000044/uni00000056/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000044/uni00000046/uni00000046
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013/uni00000014/uni00000014/uni00000014/uni00000015
/uni00000025/uni0000004f/uni00000052/uni00000046/uni0000004e/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000047/uni00000048/uni00000046/uni00000055/uni00000048/uni00000044/uni00000056/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000044/uni00000046/uni00000046
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000024/uni0000004f/uni0000004f
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000046/uni00000052/uni00000051/uni00000051/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000046/uni00000048/uni0000004f/uni0000004f/uni0000001b/uni00000015/uni00000011/uni00000018/uni0000001b/uni00000018/uni00000011/uni00000013/uni0000001b/uni0000001a/uni00000011/uni00000018/uni0000001c/uni00000013/uni00000011/uni00000013/uni0000001c/uni00000015/uni00000011/uni00000018/uni0000001c/uni00000018/uni00000011/uni00000013/uni0000001c/uni0000001a/uni00000011/uni00000018/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013 /uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000024/uni00000046/uni00000046/uni00000011
CUB
NABirds
FlowersStandfordDogs
StandfordCarsFigure 6. Impacts of different selection locations and quantities. From left to right: (a) Performance drop caused by not selecting parameters
from k-th blocks. (b) And by not selecting from the top k blocks. (c) Impacts of different numbers of selected connections on performance.
Dataset CUB NAbirds Flowers Cars Dogs
(a)Net 86.86 86.55 99.62 89.65 91.32
Layer 87.30 86.79 99.64 90.03 91.90
(b)Net Random 86.60 85.98 99.61 89.10 91.34
Neuron Random 87.17 86.02 99.62 89.52 91.82
Magnitude 87.29 85.99 99.62 89.29 91.30
(c) Head+CE 87.05 86.20 99.64 89.25 91.29
GPS88.07
±0.1186.64
±0.0399.69
±0.0190.10
±0.1092.30
±0.10
Table 6. The result on FGVC for investigating impacts of different
selection schemes and ablations. (a) Different selection levels. (b)
Different selection criteria. (c) Gradient calculating method.
Different selection location To investigate the impact of
selected parameters located at different layers within the
network, we conducted experiments using the ViT-B/16
model fine-tuning on CUB and evaluated accuracy degra-
dation when applying our GPS method to select parameters
from the entire network except for a specific transformer
block or previous several transformer blocks. As shown in
Fig. 6(a), it is surprising to note that when we do not select
parameters from a specific block, the biggest drop in the ac-
curacy comes from the shallow layers (block 2 and block 4).
This finding supports our GPS approach that selects param-
eters from the entire network rather than just the last few
layers. When we do not select the parameters from the first
specific number of blocks, it is observed that the accuracy
drop increases with more blocks removed (Fig. 6(b)).
4.5. Ablation study
Head-free contrastive loss To obtain more accurate gra-
dients for selecting parameters, inspired by the representa-
tion learning pre-training methods, Our GPS adopts the su-
pervised contrastive loss to calculate gradient (without ran-
dom initialization of the classification head). As shown in
Tab. 6 (c), when we use the cross-entropy loss (with the
head) to calculate the gradient, the average accuracy on
FGVC is dropped by 0.67%, illustrating the importance ofobtaining accurate gradients.
Selected connection number As shown in Fig. 6(c) we
select top-K input connections per neuron as trainable pa-
rameters, ranging from 1 to 15, and conduct experiments on
the 5 tasks. We can observe that more trainable parameters
do not necessarily lead to better performance, but each data
set has a performance peak. In addition, on the dataset with
sufficient training data, the addition of trainable parameters
can greatly improve the accuracy. Our GPS can easily con-
trol the number of trainable parameters and achieve optimal
results on each dataset.
Robustness to seeds Addition-based fine-tuning methods
like VPT are sensitive to the initialization of additional pa-
rameters as well as random seeds, whereas select-based
methods are not. All results in Tab. 6 are the average accu-
racy of three seeds on FGVC datasets (Only shows the std of
GPS here. Please see details in supplementary). The results
show random seeds have little influence on our method.
5. Conclusion
In this paper, we propose a new paradigm for PEFT, i.e.
Gradient-based Parameter Selection (GPS). Our approach
does not introduce any additional parameters and only fine-
tunes a small subset of the pre-trained model’s parame-
ters for downstream tasks, resulting in robust generaliza-
tion across diverse models and adaptively selecting a sub-
set of parameters for each task. Remarkably, GPS achieves
significant improvement on a range of tasks (including im-
age classification and semantic segmentation), compared to
the full fine-tuning method. GPS also attains SOTA perfor-
mance compared to other PEFT methods.
Acknowledgement
Shanghang Zhang is supported by the National Science and
Technology Major Project of China (No. 2022ZD0117801).
28573
References
[1] Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. Sim-
ple, scalable adaptation for neural machine translation.
arXiv preprint arXiv:1909.08478 , 2019. 8
[2] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom
Ward, Marcus Wainwright, Heinrich K ¨uttler, Andrew
Lefrancq, Simon Green, V ´ıctor Vald ´es, Amir Sadik, et al.
Deepmind lab. arXiv preprint arXiv:1612.03801 , 2016. 7
[3] Jiezhang Cao, Jincheng Li, Xiping Hu, Xiangmiao Wu, and
Mingkui Tan. Towards interpreting deep neural networks
via layer behavior understanding. Machine Learning , 111:
1159–1179, 2022. 4
[4] Niladri S Chatterji, Behnam Neyshabur, and Hanie Sedghi.
The intriguing role of module criticality in the generaliza-
tion of deep networks. arXiv preprint arXiv:1912.00528 ,
2019. 3
[5] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
Yang Zhang, Zhangyang Wang, and Michael Carbin. The
lottery ticket hypothesis for pre-trained bert networks.
ArXiv , abs/2007.12223, 2020. 9
[6] Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao,
Shangzhan Zhang, Yan Wang, Zejian Li, Lingyun Sun,
Papa Mao, and Ying Zang. Sam fails to segment anything?
– sam-adapter: Adapting sam in underperformed scenes:
Camouflage, shadow, and more, 2023. 7
[7] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-
ing image scene classification: Benchmark and state of the
art.Proceedings of the IEEE , 105(10):1865–1883, 2017. 7
[8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos,
Sammy Mohamed, and Andrea Vedaldi. Describing tex-
tures in the wild. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3606–3613,
2014. 7
[9] Y . L. Cun, J. S. Denker, and S. A. Solla. Optimal brain dam-
age. Advances in neural information processing systems 2,
1990. 9
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In 2009 IEEE conference on computer vision
and pattern recognition , pages 248–255. Ieee, 2009. 2, 5, 7
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 8
[12] Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen,
Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun. Open-
prompt: An open-source framework for prompt-learning.
arXiv preprint arXiv:2111.01998 , 2021. 3, 8
[13] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan
Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi Min
Chan, and Weize and Chen. Parameter-efficient fine-tuning
of large-scale pre-trained language models. Nature Ma-
chine Intelligence , 5(3):220–235, 2023. 2
[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al. An image is worth 16x16 words:Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 5, 1, 8
[15] Fenglei Fan, Jinjun Xiong, Mengzhou Li, and Ge Wang.
On interpretability of artificial neural networks: A survey.
IEEE Transactions on Radiation and Plasma Medical Sci-
ences , 5:741–760, 2020. 4
[16] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot
learning of object categories. IEEE transactions on pattern
analysis and machine intelligence , 28(4):594–611, 2006. 7
[17] Jonathan Frankle and Michael Carbin. The lottery ticket hy-
pothesis: Finding sparse, trainable neural networks. arXiv
preprint arXiv:1803.03635 , 2018. 3, 8
[18] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam,
Lidong Bing, and Nigel Collier. On the effectiveness of
parameter-efficient fine-tuning. In Proceedings of the AAAI
Conference on Artificial Intelligence , pages 12799–12807,
2023. 4
[19] Trevor Gale, Erich Elsen, and Sara Hooker. The state
of sparsity in deep neural networks. arXiv preprint
arXiv:1902.09574 , 2019. 3, 8
[20] Peng Gao*, Shijie Geng*, Renrui Zhang*, Teli Ma,
Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu
Qiao. Clip-adapter: Better vision-language models with
feature adapters. IJCV 2023 , 2021. 3
[21] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-
angyu Yue, et al. Llama-adapter v2: Parameter-efficient
visual instruction model. arXiv preprint arXiv:2304.15010 ,
2023. 3
[22] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-
trained language models better few-shot learners. arXiv
preprint arXiv:2012.15723 , 2020. 3, 8
[23] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen,
Jia Deng, and Li Fei-Fei. Fine-grained car detection for
visual census estimation. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , 2017. 5, 6, 7
[24] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. The Inter-
national Journal of Robotics Research , 32(11):1231–1237,
2013. 7
[25] Ben Graham. Kaggle diabetic retinopathy detection com-
petition report. University of Warwick , pages 24–26, 2015.
7
[26] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-
efficient transfer learning with diff pruning. arXiv preprint
arXiv:2012.07463 , 2020. 8
[27] Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grau-
man, Tajana Rosing, and Rogerio Feris. Spottune: transfer
learning through adaptive fine-tuning. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 4805–4814, 2019. 3, 8
[28] Song Han, Huizi Mao, and William J Dally. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149 , 2015. 3, 8
[29] Song Han, Jeff Pool, John Tran, and William Dally. Learn-
ing both weights and connections for efficient neural net-
28574
work. Advances in neural information processing systems ,
28, 2015. 3, 8
[30] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bo-
han Zhuang. Sensitivity-aware visual parameter-efficient
tuning. arXiv preprint arXiv:2303.08566 , 2023. 5, 6, 2
[31] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. Towards a unified
view of parameter-efficient transfer learning. ArXiv ,
abs/2110.04366, 2021. 2
[32] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Damian Borth. Eurosat: A novel dataset and deep learning
benchmark for land use and land cover classification. IEEE
Journal of Selected Topics in Applied Earth Observations
and Remote Sensing , 12(7):2217–2226, 2019. 7
[33] Dan Hendrycks and Thomas Dietterich. Benchmarking
neural network robustness to common corruptions and per-
turbations. arXiv preprint arXiv:1903.12261 , 2019. 7, 8
[34] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, et al. The many faces of ro-
bustness: A critical analysis of out-of-distribution gener-
alization. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 8340–8349, 2021.
7, 8
[35] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
hardt, and Dawn Song. Natural adversarial examples. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 15262–15271, 2021.
7, 8
[36] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo,
Mona Attariyan, and Sylvain Gelly. Parameter-efficient
transfer learning for nlp. In International Conference on
Machine Learning , pages 2790–2799. PMLR, 2019. 1, 2,
3, 4, 5, 6, 8
[37] Jeremy Howard and Sebastian Ruder. Universal language
model fine-tuning for text classification. In Annual Meeting
of the Association for Computational Linguistics , 2018. 4
[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 2, 5, 6
[39] Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu,
Jingang Wang, Juanzi Li, Wei Wu, and Maosong Sun.
Knowledgeable prompt-tuning: Incorporating knowledge
into prompt verbalizer for text classification. arXiv preprint
arXiv:2108.02035 , 2021. 3, 8
[40] Shengding Hu, Zhen Zhang, Ning Ding, Yadao Wang,
Yasheng Wang, Zhiyuan Liu, and Maosong Sun. Sparse
structure search for parameter-efficient tuning, 2022. 2
[41] Debesh Jha, Steven A. Hicks, Krister Emanuelsen, H ˚avard
Johansen, Dag Johansen, Thomas de Lange, Michael A.
Riegler, and P ˚al Halvorsen. Medico multimedia task at me-
diaeval 2020: Automatic polyp segmentation, 2020. 7, 4,
8
[42] Debesh Jha, Pia H Smedsrud, Michael A Riegler, P ˚al
Halvorsen, Thomas de Lange, Dag Johansen, and H ˚avard DJohansen. Kvasir-seg: A segmented polyp dataset. In Inter-
national Conference on Multimedia Modeling , pages 451–
462. Springer, 2020. 8
[43] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In Computer Vision–ECCV 2022:
17th European Conference, Tel Aviv, Israel, October 23–27,
2022, Proceedings, Part XXXIII , pages 709–727. Springer,
2022. 1, 2, 3, 5, 6, 7, 4, 8
[44] Justin Johnson, Bharath Hariharan, Laurens Van
Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for compositional
language and elementary visual reasoning. In Proceedings
of the IEEE conference on computer vision and pattern
recognition , pages 2901–2910, 2017. 7
[45] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi
Xie. Prompting visual-language models for efficient video
understanding. In Computer Vision–ECCV 2022: 17th Eu-
ropean Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXXV , pages 105–124. Springer, 2022.
3, 8
[46] Rabeeh Karimi Mahabadi, James Henderson, and Sebas-
tian Ruder. Compacter: Efficient low-rank hypercomplex
adapter layers. Advances in Neural Information Processing
Systems , 34:1022–1035, 2021. 3, 6, 8, 9
[47] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng
Yao, and Fei-Fei Li. Novel dataset for fine-grained image
categorization: Stanford dogs. In Proc. CVPR workshop on
fine-grained visual categorization (FGVC) . Citeseer, 2011.
5, 6, 7
[48] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
Dilip Krishnan. Supervised contrastive learning. ArXiv ,
abs/2004.11362, 2020. 3
[49] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[50] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer
Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar,
and Ross Girshick. Segment anything. arXiv:2304.02643 ,
2023. 5, 7, 4
[51] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5, 2, 7
[52] John K Kruschke and Javier R Movellan. Benefits of
gain: Speeded learning and minimal hidden layers in back-
propagation networks. IEEE Transactions on systems, Man,
and Cybernetics , 21(1):273–280, 1991. 3, 8
[53] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu
Ma, and Percy Liang. Fine-tuning can distort pretrained
features and underperform out-of-distribution. arXiv
preprint arXiv:2202.10054 , 2022. 3
[54] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning
methods for generic object recognition with invariance to
pose and lighting. In Proceedings of the 2004 IEEE Com-
puter Society Conference on Computer Vision and Pattern
Recognition, 2004. CVPR 2004. , pages II–104. IEEE, 2004.
7
28575
[55] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning filters for efficient convnets.
ArXiv , abs/1608.08710, 2016. 9
[56] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning filters for efficient convnets.
arXiv preprint arXiv:1608.08710 , 2016. 3, 8
[57] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-
ing continuous prompts for generation. arXiv preprint
arXiv:2101.00190 , 2021. 2, 3, 8
[58] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao
Wang. Scaling & shifting your features: A new baseline for
efficient model tuning. arXiv preprint arXiv:2210.08823 ,
2022. 1, 2, 3, 5, 6, 7, 4, 8
[59] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-
roaki Hayashi, and Graham Neubig. Pre-train, prompt, and
predict: A systematic survey of prompting methods in nat-
ural language processing. ACM Computing Surveys , 55(9):
1–35, 2023. 3, 8
[60] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao
Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning
can be comparable to fine-tuning across scales and tasks. In
Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers) ,
pages 61–68, 2022. 3, 8
[61] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 5, 8
[62] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for
the 2020s. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 11976–
11986, 2022. 5
[63] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa De-
hghani, and James Henderson. Parameter-efficient multi-
task fine-tuning for transformers via shared hypernetworks.
arXiv preprint arXiv:2106.04489 , 2021. 3, 8
[64] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexan-
der Lerchner. dsprites: Disentanglement testing sprites
dataset, 2017. 7
[65] Muhammad Muzammal Naseer, Kanchana Ranasinghe,
Salman H Khan, Munawar Hayat, Fahad Shahbaz Khan,
and Ming-Hsuan Yang. Intriguing properties of vision
transformers. Advances in Neural Information Processing
Systems , 34:23296–23308, 2021. 3
[66] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural
images with unsupervised feature learning. 2011. 7
[67] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes. In 2008
Sixth Indian Conference on Computer Vision, Graphics &
Image Processing , pages 722–729. IEEE, 2008. 5, 6, 7
[68] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
CV Jawahar. Cats and dogs. In 2012 IEEE conference on
computer vision and pattern recognition , pages 3498–3505.
IEEE, 2012. 7[69] Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e,
Kyunghyun Cho, and Iryna Gurevych. Adapterfusion:
Non-destructive task composition for transfer learning.
arXiv preprint arXiv:2005.00247 , 2020. 3, 6, 8, 9
[70] Jonas Pfeiffer, Andreas R ¨uckl´e, Clifton Poth, Aishwarya
Kamath, Ivan Vuli ´c, Sebastian Ruder, Kyunghyun Cho, and
Iryna Gurevych. Adapterhub: A framework for adapting
transformers. arXiv preprint arXiv:2007.07779 , 2020. 3, 8
[71] Sai Prasanna, Anna Rogers, and Anna Rumshisky. When
bert plays the lottery, all tickets are winning. In Confer-
ence on Empirical Methods in Natural Language Process-
ing, 2020. 9
[72] John G. Proakis and Dimitris G. Manolakis. Digital signal
processing: Principles, algorithms, and applications. 1992.
4
[73] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International conference on machine learning ,
pages 8748–8763. PMLR, 2021. 8
[74] Maithra Raghu, Thomas Unterthiner, Simon Kornblith,
Chiyuan Zhang, and Alexey Dosovitskiy. Do vision trans-
formers see like convolutional neural networks? Ad-
vances in Neural Information Processing Systems , 34:
12116–12128, 2021. 3
[75] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea
Vedaldi. Learning multiple visual domains with residual
adapters. Advances in neural information processing sys-
tems, 30, 2017. 2, 3, 8
[76] Andreas R ¨uckl´e, Gregor Geigle, Max Glockner, Tilman
Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych.
Adapterdrop: On the efficiency of adapters in transformers.
arXiv preprint arXiv:2010.11918 , 2020. 8
[77] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.
Learning important features through propagating activa-
tion differences. In International Conference on Machine
Learning , 2017. 4
[78] Asa Cooper Stickland and Iain Murray. Bert and pals: Pro-
jected attention layers for efficient adaptation in multi-task
learning. In International Conference on Machine Learn-
ing, pages 5986–5995. PMLR, 2019. 3, 8
[79] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan,
Yankai Lin, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou,
Maosong Sun, et al. On transferability of prompt tun-
ing for natural language understanding. arXiv preprint
arXiv:2111.06719 , 2021. 2
[80] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-
adapter: Parameter-efficient transfer learning for vision-
and-language tasks. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
5227–5237, 2022. 3, 6, 8, 9
[81] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber,
Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Be-
longie. Building a bird recognition app and large scale
dataset with citizen scientists: The fine print in fine-grained
dataset collection. In Proceedings of the IEEE Conference
28576
on Computer Vision and Pattern Recognition , pages 595–
604, 2015. 5, 6, 7
[82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances
in neural information processing systems , 30, 2017. 1
[83] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco
Cohen, and Max Welling. Rotation equivariant cnns for dig-
ital pathology. In Medical Image Computing and Computer
Assisted Intervention–MICCAI 2018: 21st International
Conference, Granada, Spain, September 16-20, 2018, Pro-
ceedings, Part II 11 , pages 210–218. Springer, 2018. 7
[84] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011. California Institute of Technology . 5, 6, 7
[85] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P
Xing. Learning robust global representations by penaliz-
ing local predictive power. Advances in Neural Information
Processing Systems , 32, 2019. 2
[86] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-
anjing Huang, Guihong Cao, Daxin Jiang, Ming Zhou,
et al. K-adapter: Infusing knowledge into pre-trained mod-
els with adapters. arXiv preprint arXiv:2002.01808 , 2020.
3, 8
[87] Zifeng Wang, Shao-Lun Huang, Ercan Engin Kuruoglu, Ji-
meng Sun, Xi Chen, and Yefeng Zheng. Pac-bayes infor-
mation bottleneck. ArXiv , abs/2109.14509, 2021. 4
[88] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and
Hai Li. Learning structured sparsity in deep neural net-
works. Advances in neural information processing systems ,
29, 2016. 3, 8
[89] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
and Antonio Torralba. Sun database: Large-scale scene
recognition from abbey to zoo. In 2010 IEEE computer
society conference on computer vision and pattern recogni-
tion, pages 3485–3492. IEEE, 2010. 7
[90] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,
Baobao Chang, Songfang Huang, and Fei Huang. Raise a
child in large language model: Towards effective and gen-
eralizable fine-tuning. arXiv preprint arXiv:2109.05687 ,
2021. 3, 8
[91] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lip-
son. How transferable are features in deep neural networks?
Advances in neural information processing systems , 27,
2014. 3
[92] Elad Ben Zaken, Shauli Ravfogel, and Yoav Gold-
berg. Bitfit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models. arXiv
preprint arXiv:2106.10199 , 2021. 2, 3, 5, 6, 7, 4, 8
[93] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov,
Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo-
longa, Andre Susano Pinto, Maxim Neumann, Alexey
Dosovitskiy, et al. A large-scale study of representation
learning with the visual task adaptation benchmark. arXiv
preprint arXiv:1910.04867 , 2019. 5, 6, 7
[94] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao,
Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.Tip-adapter: Training-free clip-adapter for better vision-
language modeling. arXiv preprint arXiv:2111.03930 ,
2021. 3, 8
[95] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
peng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng
Li. Pointclip: Point cloud understanding by clip. In CVPR
2022 , 2022.
[96] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.
Llama-adapter: Efficient fine-tuning of language models
with zero-init attention. ICLR 2024 , 2023. 3
[97] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Han-
qiu Deng, Hongsheng Li, Yu Qiao, and Peng Gao. Prompt,
generate, then cache: Cascade of foundation models makes
strong few-shot learners. CVPR 2023 , 2023. 3
[98] Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-
rich Sch ¨utze. Masking as an efficient alternative to fine-
tuning for pretrained language models. arXiv preprint
arXiv:2004.12406 , 2020. 8
[99] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xi-
aochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng.
Deepvit: Towards deeper vision transformer. arXiv preprint
arXiv:2103.11886 , 2021. 8
[100] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao,
Animashree Anandkumar, Jiashi Feng, and Jose M Al-
varez. Understanding the robustness in vision transformers.
InInternational Conference on Machine Learning , pages
27378–27394. PMLR, 2022. 8
[101] Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou,
Dong Wang, Bin Zhao, and Peng Gao. Not all features
matter: Enhancing few-shot clip with adaptive prior refine-
ment. ICCV 2023 , 2023. 3
28577
