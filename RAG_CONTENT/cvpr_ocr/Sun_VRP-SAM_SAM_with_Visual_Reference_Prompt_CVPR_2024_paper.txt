VRP-SAM: SAM with Visual Reference Prompt
Yanpeng Sun1,2∗, Jiahui Chen2,3*, Shan Zhang4, Xinyu Zhang2, Qiang Chen2
Gang Zhang2, Errui Ding2, Jingdong Wang2, Zechao Li1†
1Nanjing University of Science and Technology,
2Baidu VIS,3Beihang University,4Australian National University
{yanpeng sun, zechao.li }@njust.edu.cn
Abstract
In this paper, we propose a novel Visual Reference
Prompt (VRP) encoder that empowers the Segment Any-
thing Model (SAM) to utilize annotated reference images
as prompts for segmentation, creating the VRP-SAM model.
In essence, VRP-SAM can utilize annotated reference im-
ages to comprehend specific objects and perform segmen-
tation of specific objects in target image. It is note that
the VRP encoder can support a variety of annotation for-
mats for reference images, including point ,box,scribble ,
and mask . VRP-SAM achieves a breakthrough within the
SAM framework by extending its versatility and applicabil-
ity while preserving SAM’s inherent strengths, thus enhanc-
ing user-friendliness. To enhance the generalization abil-
ity of VRP-SAM, the VRP encoder adopts a meta-learning
strategy. To validate the effectiveness of VRP-SAM, we con-
ducted extensive empirical studies on the Pascal and COCO
datasets. Remarkably, VRP-SAM achieved state-of-the-art
performance in visual reference segmentation with mini-
mal learnable parameters. Furthermore, VRP-SAM demon-
strates strong generalization capabilities, allowing it to per-
form segmentation of unseen objects and enabling cross-
domain segmentation. The source code and models will be
available at https://github.com/syp2ysy/VRP-
SAM
1. Introduction
In recent, the Segment Anything Model (SAM) [13] has
emerged as a foundational visual model for image segmen-
tation. Trained on an extensive datasets comprising bil-
lions of labels, SAM has demonstrated remarkable versatil-
ity in the realm of universal segmentation. What sets SAM
apart is its human-interactive design, allowing segmenta-
tion based on user-provided prompts, be they in the form of
*equal contribution
†Corresponding author.
Point & boxVisual referenceBounding boxPositive pointNegative point
PromptsImage-1Image-2Image-3
Figure 1. Comparison of SAM’s built-in Point and Box prompt
modes with Visual Reference Prompt when handling numerous
images. The prompts are all provided by users.
points, bounding boxes, or coarse masks. This distinctive
feature positions SAM as a robust tool that can be adapt-
able to various tasks and requirements [12, 48].
However, the existing prompt formats of SAM present
significant challenges in practical applications, especially
when dealing with complex scenes and numerous images.
As shown in Figure 2(a), SAM relies on user-provided
prompts (points, boxes, coarse mask) to segment objects in
the target image, demanding users to possess a comprehen-
sive understanding of the target objects. In real-world ap-
plications, especially in complex scenarios, the level of user
familiarity with the target objects can significantly impact
the effectiveness of providing specific prompts. Further-
more, variations in the position, size, and quantity of tar-
get objects across different images require custom prompts
for each image. As illustrated in Figure 1 with the aim
of segmenting ’bicycles’ , users need to customize different
prompts for each image, significantly impacting efficiency
of SAM. Therefore, we propose integrating visual reference
prompts to overcome these limitations and enhance adapt-
ability of SAM.
Visual reference prompts is annotated reference images
that delineate the objects users expect to segment. As shown
in Figure 1, by simply providing a visual reference prompt
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23565
ImageEncoderPromptEncoderMaskDecoder
targetimagevalidmaskspointstboxtmaskt
ImageEncoderPromptEncoderMaskDecoder
targetimagevalidmasksVisualReferencePromptEncoderreferenceimagepointstboxtmasktpointsrscribblerboxrmaskr(b)VRP-SAM(a)SAMFigure 2. A Comparison between SAM and VRP-SAM. VRP-
SAM introduces a visual reference prompt encoder, accepting an-
notated reference images with point ,scribble ,box, and mask for-
mats, offering a distinct enhancement over SAM.
with bicycle , we can segment bicycle in different images
without requiring users to provide specific prompts for each
image. It significantly enhances the efficiency of SAM
while reducing reliance on user familiarity with objects. To
achieve this goal, some methods [19, 46] incorporate se-
mantic correlation models [9, 25] to establish reference-
target correlation1, obtaining pseudo-masks for the target
objects. Following this, a sampling strategy is devised to
extract a set of points and bounding boxes from the pseudo-
masks, serving as prompts for SAM segment the target im-
age. These methods overlook false positives within the
pseudo-mask and exhibit high sensitivity to hyperparame-
ters. Consequently, it heavily relies on the quality of the
pseudo-mask and has poor generalization.
Toward this end, we propose a straightforward and effec-
tive Visual Reference Prompt (VRP) encoder using meta-
learning technique, integrated with SAM to create VRP-
SAM. It leverages annotated reference images as prompts
to segment similar semantic objects in the target image. As
illustrated in Figure 2(b), the VRP encoder accepts inputs
in various annotation formats, including points, scribbles,
boxes, and masks. Specifically, the VRP encoder intro-
duces a semantic-related model to encode reference and tar-
get images into the same space. Following meta-learning
methods, we first extract prototypes of target objects from
annotated information in reference images, enhancing the
representation of the target objects in both images. Follow-
ing meta-learning methods, the prototypes of target objects
(users’ markers) are first generated from annotated refer-
ence images, which aim to highlight such target instances
1SAM, being a category-agnostic model, encounters challenges in ad-
equately capturing reference-target correlations.in both reference and target images. Then, we introduce a
set of learnable queries to extract the semantic cues of the
target objects from attentive enhanced reference features.
Then, these queries interact with target images, generating
prompt embeddings usable by mask decoder to segment se-
mantically specific objects in the target image. Building
upon the foundation of SAM’s inherent capabilities, VRP-
SAM enhances the model’s visual reference segmentation
prowess. The introduction of Visual Reference Prompts
(VRP) not only diversifies the prompts, enabling the model
to swiftly segment objects with identical semantics, but also
incorporates a meta-learning mechanism that significantly
boosts the generalization of model, particularly in dealing
with novel objects and cross-domain scenarios.
To quantitatively assess the generalization capability of
VRP-SAM, we follow the dataset configurations commonly
used in few-shot segmentation and evaluate our VRP-SAM
on Pascal-5iand COCO-20 idatasets. Extensive experi-
ments show that VRP-SAM overcomes the limitations of
SAM in prompt format, enabling efficient visual reference
segmentation. It is significantly better than sampling-based
methods, achieving state-of-the-art results on Pascal-5iand
COCO-20 idatasets. Moreover, we present solid evidence
of VRP-SAM’s superior performance in handling unknown
objects and cross-domain scenarios. Our experiments high-
light that VRP-SAM achieves the rapid segmentation of a
large number of images based on semantics. Furthermore,
the incorporation of meta-learning principles significantly
enhances its generalization, making it applicable across var-
ious scenarios.
2. Related Work
2.1. Application of SAM
Segmentation Anything model (SAM) [13] is a recently in-
troduced category-agnostic interactive segmentation model
by Meta. It leverages user-guided instructions for seg-
mentation, such as points, bounding boxes, and coarse
masks. Currently, SAM has two primary application ap-
proaches. One involves using SAM’s segmentation results
as prior information to assist downstream tasks. For in-
stance, Inpaint Anything (IA) [43] and Edit Everything [39]
leverage SAM’s segmentation results for image editing and
restoration [42, 47] within the masked regions. Addition-
ally, SEPL [5] employs SAM’s segmentation results to
enhance pseudo-labels in weakly supervised segmentation
tasks [27, 30]. Furthermore, SAM segmentation results
serve as prior information in tasks such as crack and vol-
cano crater detection [1, 8].
The second involves guiding SAM’s segmentation
through various prompt combinations. For example, in the
context of visual reference segmentation [31], Matcher [19]
samples points and boxes from pseudo-masks, utilizing
23566
SAM to refine these pseudo-masks. TAM [41] is applied
in object tracking tasks, where it uses point prompts to ini-
tialize object masks and subsequently uses SAM to refine
low-quality masks. Additionally, SAMAug [6] introduces
a visual point enhancement method tailored for SAM, fa-
cilitating automatic image annotation. These examples il-
lustrate SAM’s effectiveness across different tasks. How-
ever, existing methods are constrained by SAM’s existing
prompt modalities, and they may struggle when confronted
with complex objects and unfamiliar scenes. To break these
limitations, we have designed a visual prompt encoder for
SAM, expanding its applicability to a wider range of sce-
narios.
2.2. Visual reference segmentation
Visual reference segmentation [31, 33, 48] aims to guide
the segmentation of a target image using a reference image.
The goal of this task is to utilize a semantically annotated
reference image to instruct the segmentation of objects or
regions in the target image that share the same semantics
as those in the reference image. In current research, meth-
ods can be broadly categorized into two groups: prototype-
based and feature-matching-based. Prototype-based meth-
ods, such as PFENet [33], PANet [36], and CWT [21], usu-
ally focus on distinguishing prototypes with different class-
specific features. ASGNet [15], on the other hand, improves
the segmentation performance by increasing the number of
prototypes. Another feature-matching approach [22, 35]
leverages the pixel-level correlations between reference and
target images to significantly enhance segmentation perfor-
mance, as demonstrated by methods like CyCTR [44] and
HDMNet [26]. Moreover, modern large-scale vision mod-
els [2, 37, 48] have recognized visual reference segmenta-
tion as a primary task, given its indispensable role in han-
dling complex objects and unknown scenes. However, it’s
important to note that SAM [13] does not possess the ca-
pability to perform this task, underscoring the necessity of
introducing VPR-SAM.
3. Preliminary
In this section, we first review the architecture of Segment
Anything Model. Then we formulate the problem setting in
this paper.
3.1. SAM Architecture
SAM is an interactive segmentation model composed of
an image encoder, a prompt encoder, and a mask decoder.
Given a target image Itand some geometric prompts, SAM
first employs a Vision Transformer (ViT) [34] as image en-
coder to extract image features. Subsequently, the prompt
encoder is utilized to generate the prompt embeddings de-
rived from the user-provided points, boxes, or masks. Fi-
nally, the mask decoder integrates the image features andprompt embeddings, and generates a high-quality mask in a
class-agnostic manner.
To the best of our knowledge, current research based on
SAM still relies on geometric prompts for segmentation.
Thus far, there have been no efforts to introduce new forms
of prompts to enhance SAM. We design a novel visual ref-
erence prompt to extend the visual reference segmentation
capabilities of SAM.
3.2. Problem Formulation
Visual reference segmentation aims to segment objects in
target image that same semantic as the annotated object in
reference image. Specifically, let Itrepresent the target im-
age and Irrepresent the reference image. Given a reference
image Irand its annotation Mi
r, where idenotes the cate-
gory of the annotated object, we leverage this information
to segment all regions in the target image belonging to cat-
egory i. Depending on the annotation granularity, the ref-
erence image have four annotation formats: point, scribble,
box, and mask. Point annotation involves providing specific
points on the target, scribble requires outlining the target
region with arbitrary curves, boxes correspond to bounding
boxes around the target, and masks provide pixel-level an-
notations for the entire target.
4. VRP-SAM
VRP-SAM extends SAM to perform visual reference seg-
mentation without compromising its original functionality.
We propose a training-efficient visual reference prompt en-
coder that, firstly, accommodates various granularities of vi-
sual references and, secondly, directly encodes these visual
references into prompt embeddings rather than geometric
prompts. These prompt embeddings are then fed directly
into the mask decoder of SAM, resulting in the generation
of the target mask.
As depicted in Figure 3, the VRP Encoder consists of
feature augmenter and prompt generator. Next, we will
delve into the details of VRP Encoder and the loss function.
4.1. Feature Augmenter
Inspired by meta-learning, the Feature Augmenter sepa-
rately encodes reference annotations Mi
rinto features of
both the reference and target images. This process is de-
signed to distinguish between foreground and background
representations. To capture semantic correlations between
reference and target images, we introduce a semantic-aware
image encoder within the VRP encoder, encoding them into
the same latent space. To prevent overfitting of the VRP
encoder, we freeze the image encoder during the training
phase. This ensures a balance between capturing seman-
tic relevance and preventing excessive specialization of the
VRP encoder.
23567
Visual reference Prompt Encoder
Mask  Decoder
𝐹!"
poolPi𝐼#𝑀#$𝐹#𝐹#"
expand
Image encoder
𝐼!CImage encoder
𝑀!"#$%&'𝐹!Cexpand1	×	1 conv𝑀#$𝐶×𝐻×𝑊(2𝐶+1)×𝐻×𝑊𝐶×𝐻×𝑊𝐶×𝐻×𝑊(2𝐶+1)×𝐻×𝑊𝐶×𝐻×𝑊1	×	1 conv
1	×	1 conv1	×	1 conv
𝑄!"Feature Augmenter𝑄𝑄#"Self-Attention1Cross-Attention1
Self-Attention2Cross-Attention2Prompt Generator
SAM Image Encoder
Figure 3. Proposed VRP-SAM framework. Our approach enables SAM to perform visual reference segmentation by extends a VRP
encoder. It takes various granularities of visual references as inputs and encodes these visual references into prompt embeddings. Our VRP
encoder consists of a feature augmenter and a prompt generator.
The Feature Augmenter is illustrated in Figure 3. Ini-
tially, leveraging a semantic-aware image encoder (e.g.
ResNet-50), we encode IrandItseparately, resulting in the
feature map Fr∈RC×H×WandFt∈RC×H×W. Subse-
quently, we extract the prototype feature Picorresponding
to class ifromFrusing the mask Mi
r. This process can be
summarized as follows:
Pi=MaskAvgPool (Fr, Mi
r) (1)
where Mi
rrepresents one of following annotation formats:
point ,scribble ,box, ormask . To enhance the context in-
formation about class i, we concatenate the prototype fea-
tures and mask with FrandFt.Fris concatenated with
mask mi, andFtis concatenated with pseudo-mask mpseudo
i .
mpseudo
i is obtained using a common training-free approach,
and detailed descriptions are provided in the Appendix.
Subsequently, we employ a shared 1×1convolution layer
to reduce dimensionality of enhanced features.This process
can be summarized as follows:
F′
r=Conv (concat (Fr, Pi, mi)) (2)
F′
t=Conv (concat (Ft, Pi, mpseudo
i)) (3)
Ultimately, we obtain enhanced image features F′
r∈
RC×H×WandF′
t∈RC×H×W, which have enhanced
context information for the category i. Thus, the enhanced
features comprise foreground representations for class iand
background representations for the other classes. Subse-
quently, we feed the enhanced features into the Prompt Gen-
erator to obtain a set of visual reference prompts.4.2. Prompt Generator
The purpose of the Prompt Generator is to obtain a set of
visual reference prompts embedding for the SAM mask de-
coder. As depicted in Figure 3, the process commences
with the introduction of a set of learnable queries denoted
asQ∈RN×C, where Nindicates the number of visual ref-
erence prompts. These queries first engage with the refer-
ence features F′
rto obtain the category-specific information
through cross-attention and self-attention layer:
Q′
r=SelfAttn 1(CrossAttn 1(Q, F′
r)) (4)
Here, we obtain a set of queries, Q′
r, possessing knowl-
edge about the object to be segmented. Subsequently, we
employ cross-attention to interact these queries with target
image feature to obtain the foreground information in target
image. Following this, a self-attention layer is used to up-
date the queries, generating a set prompts Q′
tthat align with
the representation of SAM :
Q′
t=SelfAttn 2(CrossAttn 2(Q′
r, F′
t)) (5)
The final Q′
tserve as the visual reference prompt embed-
dings for SAM, equipped with the capability to guide the
segmentation of the foreground in the target image. By in-
putting this set of visual reference prompt embeddings into
the mask decoder, the mask Mi
t∈R1×H×Wfor category i
in the target image can be obtained.
4.3. Loss Function
We employ Binary Cross-Entropy (BCE) loss and Dice loss
to supervise the learning of the Visual Reference Prompt
23568
Table 1. Compare with other foundation models. Results of
one-shot semantic segmentation on COCO-20i.Gray indicates the
model is trained by in-domain datasets.†indicates the method
using SAM.
Methods Label type F-0 F-1 F-2 F-3 Means
Painter [37]
mask.31.2 35.3 33.5 32.4 33.1
SegGPT [38] 56.3 57.4 58.9 51.7 56.1
PerSAM†[46] 23.1 23.6 22.0 23.4 23.0
PerSAM-F†[46] 22.3 24.0 23.4 24.1 23.5
Matcher†[19] 52.7 53.5 52.6 52.1 52.7
VRP-SAM†point. 30.1 39.2 43.0 40.4 38.2
scribble. 40.2 52.0 52.4 44.4 47.2
box. 44.5 49.3 55.7 49.1 49.7
mask. 48.1 55.8 60.0 51.6 53.9
Encoder. The BCE loss ensures pixel-wise accuracy, while
the Dice loss provides additional context for pixel-level seg-
mentation. Therefore, the total loss of VRP-SAM is:
Ltotal=−1
NNX
i=1[yilog(pi) + (1−yi) log(1 −pi)]
| {z }
BCE Loss
+ 1−2PN
i=1(pi·yi)
PN
i=1p2
i+PN
i=1y2
i| {z }
Dice Loss(6)
where Nrepresents the total number of pixels, yiis the
ground truth label for pixel i, and piis the predicted prob-
ability of pixel ibelonging to the object. By combining
these two losses, we comprehensively consider both accu-
racy and contextual effects, thus guiding the Visual Refer-
ence Prompt Encoder more effectively in generating precise
segmentation results.
5. Experiments
5.1. Setting
Datasets : To validate segmentation performance and gener-
alization capability of VRP-SAM, we conducted extensive
experiments following the few-shot setting [14, 31, 44] on
COCO-20i[24] and PASCAL-5i[29] datasets. Specif-
ically, we organized all classes from both datasets into
4 folds. For each fold, PASCAL-5i[29] comprises 15
base classes for training and 5 novel classes for testing,
while COCO-20i[24] includes 60 training base classes
and 20 testing novel classes. To assess performance of
model, we randomly sampled 1000 reference-target pairs
in each fold. In each fold, As the mentioned datasets lack
labels for point, scribble, and box annotations, we followed
the SEEM [48] to generate these annotation labels by
simulating user inputs randomly based on the reference
ground truth masks.
Implementation details : In visual reference prompt en-
coder, we use VGG-16 [3] and ResNet-50 [9] as the im-age encoder and initialize it with ImageNet [28] pre-trained
weights. We employed the AdamW optimizer [20] along
with a cosine learning rate decay strategy for training VRP-
SAM. Specifically, on the COCO-20idataset, we conducted
50 epochs of training with an initial learning rate of 1e-
4 and a batch size of 8. For the PASCAL-5idataset, the
model was trained for 100 epochs with an initial learning
rate of 2e-4 and a batch size of 8. In VRP, the number of
queries is set to 50 by default, and the input image size of
all experiments needs to be adjusted to 512×512. Fol-
lowing SEEM [48], during training, we obtain annotations
for points, scribbles, and boxes based on mask annotations.
We provide detailed descriptions of this process in the Ap-
pendix. To ensure a fair comparison, VRP-SAM is exclu-
sively compared to previous works [16, 26, 37] using visual
reference prompts based on the the mean intersection over
union (mIoU).
5.2. Comparison with the State-of-the-art
Comparison with other foundation models. Leveraging
the foundation model enables few-shot segmentation. We
compared various approaches utilizing Painter [37], Seg-
GPT [38], and SAM as foundation models for few-shot seg-
mentation, providing evaluation metrics on the COCO-20i
dataset. As shown in Table 1, VRP-SAM achieves 53.9%
mean mIoU without training on novel classes, achieving
comparable with SegGPT. Note that the training data of
SegGPT include all classes in COCO. Furthermore, our
VRP-SAM outperforms other SAM-base methods includ-
ing Matcher [19], which employs a DINOv2 [25] pretrained
ViT-L [34] as image encoder.
Comparison with few-shot methods. To validate
the effectiveness of VRP-SAM, we compared it with
state-of-the-art few-shot segmentors on the novel set of
COCO-20iand PASCAL-5idatasets. To ensure a fair
comparison, VRP-SAM is trained and tested separately
in each fold, ensuring no overlap in classes between the
training and test sets. The results in Table 2 demonstrate
that VRP-SAM achieves state-of-the-art results on COCO-
20iand PASCAL-5i, with mIoU scores of 53.9 and 71.9,
respectively. Notably, when using VGG-16 as the image
encoder for VRP, VRP-SAM achieves mIoU scores of
48.0 and 68.7 on COCO-20iand PASCAL-5idatasets. It
indicates that VRP-SAM achieves optimal performance
on the novel set with only 1.6M learnable parameters,
highlighting its powerful generalization capability.
5.3. Comparison with Geometric Prompts
In this paper, we design Visual Reference Prompts to rep-
resent target information. Different from the Geometric
Prompts (GP) provided by SAM, our VRP is more flexible
and robust. We conducted experiments comparing GP and
VRP to validate the superiority of our method (see Table 3).
23569
Table 2. Performance of one-shot semantic segmentation on COCO-20iand PASCAL-5i. The red and blue colors respectively represent
the optimal and suboptimal results.
MethodImage
encoderLearnable
paramsCOCO-20iPASCAL-5i
F-0 F-1 F-2 F-3 Mean F-0 F-1 F-2 F-3 Mean
PFENet [33]
VGG-1610.4M 35.4 38.1 36.8 34.7 36.3 56.9 68.2 54.5 52.4 58.0
BAM [14] 4.9M 36.4 47.1 43.3 41.7 42.1 63.2 70.8 66.1 57.5 64.4
HDMNet [26] 4.2M 40.7 50.6 48.2 44.0 45.9 64.8 71.4 67.7 56.4 65.1
VRP-SAM 1.6M 43.6 51.7 50.0 46.5 48.0 70.0 74.7 68.3 61.9 68.7
PFENet [33]
ResNet-5010.4M 36.5 38.6 34.5 33.8 35.8 61.7 69.5 55.4 56.3 60.8
HSNet [22] 2.6M 36.3 43.1 38.7 38.7 39.2 64.3 70.7 60.3 60.5 64.0
CyCTR [44] 15.4M 38.9 43.0 39.6 39.8 40.3 65.7 71.0 59.5 59.7 64.0
SSP [7] 8.7M 35.5 39.6 37.9 36.7 37.4 60.5 67.8 66.4 51.0 61.4
NTRENet [18] 19.9M 36.8 42.6 39.9 37.9 39.3 65.4 72.3 59.4 59.8 64.2
DPCN [17] - 42.0 47.0 43.3 39.7 43.0 65.7 71.6 69.1 60.6 66.7
V AT [10] 3.2M 39.0 43.8 42.6 39.7 41.3 67.6 72.0 62.3 60.1 65.5
BAM [14] 4.9M 39.4 49.9 46.2 45.2 45.2 69.0 73.6 67.6 61.1 67.8
HDMNet [26] 4.2M 43.8 55.3 51.6 49.4 50.0 71.0 75.4 68.9 62.1 69.4
VRP-SAM 1.6M 48.1 55.8 60.0 51.6 53.9 73.9 78.3 70.6 65.0 71.9
DCAMA Swin-B 47.7M 49.5 52.7 52.8 48.7 50.9 72.2 73.8 64.3 67.1 69.3
Table 3. Comparison with geometric prompts. Geometric prompts
are randomly sampled from the pseudo-mask.†indicates the care-
fully designed sampling strategy proposed in [19].
Method Image Encoder Prompts Mean IoU
GP-SAMResNet-50box. 19.7
point. 21.7
box. +point. 23.2
DINOv2box. 31.3
point. 36.6
box. +point. 37.8
box. +point.†[19] 52.7
VRP-SAMResNet-50point. 38.4
scribble. 47.3
box. 49.7
mask. 53.9
DINOv2-B/14 mask. 60.4
In the GP-SAM experiments, we initially used an Image
Encoder to generate a pseudo-mask for the target image fol-
lowing [33], and subsequently obtained points or bounding
boxes from the pseudo-mask2as geometric prompts. Ex-
perimental results consistently demonstrate the ongoing su-
periority of our VRP over the GP approach. We visualize
the segmentation results of GP-SAM and our VRP-SAM in
Figure 4. Visualization results indicate that the GP approach
is prone to generating false-positive prompts, significantly
impacting segmentation performance. In contrast, our VRP
effectively avoids such issues.
5.4. Generalization Evaluation
Domain shift: Next, we assess the effectiveness of VRP-
SAM in a domain shift scenario, which necessitates signif-
2For point prompts, we randomly sample 5 points from the pseudo-
mask. For box prompt, we adopt the bounding box of the pseudo-mask.
Visual ReferenceGP-SAM-RN50GP-SAM-DINOv2VRP-SAM-RN50Figure 4. The visualization results of VRP-SAM and GP-SAM.
icant domain differences between training and testing sets.
Following prior work [22, 32, 33], we trained on COCO-20i
and tested on PASCAL-5i, where the classes in training set
do not overlap with those in test set. The results in Table 4,
representing the average across four folds, clearly demon-
strate the superior performance of VRP-SAM in domain
shift scenario. Notably, VRP-SAM with scribble annota-
tions outperforms FP-Trans [45] by 2.8%, while VRP-SAM
with mask annotations surpasses DGPNet [11] by 5.8%.
This affirms the robust generalization capability of VRP-
SAM and its effectiveness in domain transfer scenarios.
Visualization: To assess the generalization capability of
VRP-SAM across diverse image styles, we curated a col-
lection of target images from web, spanning various gen-
23570
PointScribbleBoxMaskVRP
Image-1Image-2Image-5Image-3Image-4Figure 5. Qualitative results of VRP-SAM across diverse image styles is presented. The target images were sourced from the internet.
res such as natural landscapes, artworks, and complex en-
vironments. Visual Reference Prompts (VRPs) were se-
lected from the COCO dataset to guide the segmentation
of these target images. The segmentation results on differ-
ent image styles are presented in Figure 5. It demonstrate
that VRP-SAM adeptly adapts to varied image styles, accu-
rately delineating target objects. Notably, VRP-SAM excels
in handling ink-style images, showcasing refined segmenta-
tion performance. This compellingly underscores the ro-
bustness and effectiveness of VRP-SAM when confronted
with images of unknown styles.
5.5. Ablation Study
To validate the effectiveness of VRP-SAM, we conducted
extensive ablation studies on PASCAL-5i. ResNet-50 was
chosen as the image encoder for VRP to ensure experimen-
tal consistency. These experiments aimed to thoroughly
investigate the performance of VRP-SAM under various
conditions.
Loss: To assess the impact of Binary Cross-Entropy
(BCE) and Dice losses on VRP-SAM, experiments were
conducted on the PASCAL-5idataset. Results in Table 5
indicate comparable performance when using BCE or Dice
loss individually. Notably, the optimal performance forTable 4. Evaluation (Mean IoU (%)) under the domain shift from
COCO-20ito PASCAL-5i.
Method Image encoder Label type Mean
RPMM [40]
ResNet-50 mask.49.6
PFENet [33] 61.1
RePRI [4] 63.2
V AT-HM [23] 65.1
HSNet [22]ResNet-101 mask.64.1
DGPNet [11] 70.1
FP-Trans [45] DeiT-B/16 mask. 69.7
VRP-SAM ResNet-50point. 63.5
scribble. 72.5
box. 72.3
mask. 75.9
VRP-SAM is achieved when both losses are combined.
It emphasizes the importance of BCE and Dice losses in
guiding VRP-SAM to produce more robust and accurate
masks. BCE loss ensures precise pixel-level classification,
while Dice loss facilitates accurate spatial localization of
objects. Simultaneously employing BCE and Dice losses
in VRP-SAM maximizes their complementary advantages.
The number of query: In Figure 6, we conducted a
comprehensive analysis of the impact of varying query
quantities on VRP-SAM’s performance. We observed
23571
Table 5. Ablation study on different loss function in VRP-SAM.
Method Label type Bce loss Dice loss Means
VRP-SAM mask.70.1
70.3
71.9
6062646668707274767880
15101520255075100mIoU(%)
query numberF-0F-1F-2F-3Means
Figure 6. Ablation study on VRP-SAM with different query num-
bers. The x-axis shows the number of queries, and the y-axis rep-
resents model performance.
a positive correlation between increased query numbers
and improved segmentation quality. However, once the
query count surpassed 50, the performance gain started
to diminish. This phenomenon suggests that 50 queries
provide ample effective guidance for the model, and
further increases in query count do not yield significant
improvements, leading to performance saturation. To main-
tain high performance while minimizing model learnable
parameters, we set the query quantity in VRP-SAM to 50.
This decision optimally balances guidance information and
model efficiency.
Initialization of query: In Table 6, we compared various
query initialization strategies, such as random initialization,
foreground prototype (FP), background prototype (BP),
and a hybrid prototype with half foreground and half
background (half-FP & half-BP). Surprisingly, random
initialization outperformed all other strategies, showcasing
superior performance. This unexpected outcome can be at-
tributed to the intricate segmentation task and the dataset’s
object diversity. Random initialization enables the model
to explore a broader range of states, avoiding potential
local minima. In contrast, prototype-based initializations,
whether foreground, background, or a combination, might
introduce biases, limiting adaptability to diverse object
characteristics.
Number of VRPs: In our investigation of the influ-
ence of number of visual reference prompts on segmenta-
tion results, we explored utilization of few visual reference
prompts. This approach involves sending several visual ref-
erence prompts to the VRP encoder, generating multiple
prompt embeddings. These embeddings are concatenated
and forwarded to the mask decoder, resulting in the final
mask. The result in Table 7, we observed a substantial en-Table 6. Ablation study of different query initialization methods
on VRP-SAM.
Method Label type Image encoder Mean IoU
VRP-SAM mask.FP 68.2
BP 62.6
half-FP & half-BP 67.4
random 71.9
Table 7. Ablation Study on Few Visual Reference prompts for
VRP-SAM.
Method Label type 1-VRP 5-VRP
VRP-SAMpoint. 62.9 64.1 (+1.2)
scribble. 66.8 68.4 (+1.6)
box. 69.4 70.5 (+1.1)
mask. 71.9 72.9 (+1.0)
hancement in segmentation performance with an increase in
the number of visual reference prompts. The results empha-
size positive influence of incorporating multiple visual ref-
erence prompts, enhancing the model’s ability to accurately
capture intricate details and nuances associated with target
object. The diverse prompts contribute enriched contextual
information, facilitating a more comprehensive understand-
ing of the object’s characteristics and leading to generation
of precise and nuanced segmentation masks.
6. Conclusion
In this paper, we present VRP-SAM, an innovative exten-
sion of the SAM framework achieved through integration
of a Visual Reference Prompt (VRP) encoder. This addi-
tion empowers SAM to leverage visual reference prompts
for guided segmentation. The core methodology involves
encoding annotated reference images through the VRP en-
coder, which then interacts with the target image to gener-
ate meaningful prompts for segmentation within the SAM
framework. The seamless fusion of VRP encoder with
SAM, resulting in VRP-SAM, enhances the model’s gen-
erality and adaptability. VRP-SAM overcomes limitations
posed by SAM’s existing prompt formats, especially in
complex scenarios and large datasets. The introduced visual
reference prompts, including point, box, scribble, and mask
annotations, offer a flexible solution, expanding the model’s
applicability. Extensive empirical studies conducted show-
case VRP-SAM’s state-of-the-art performance in visual ref-
erence segmentation with minimal learnable parameters.
Notably, VRP-SAM demonstrates robust generalization ca-
pabilities, excelling in segmentation tasks for novel objects
and cross-domain scenarios.
Acknowledge This work was partially supported by the Na-
tional Key Research and Development Program of China
under Grant 2022ZD0118802 and the National Natural Sci-
ence Foundation of China (Grant No. U20B2064 and
U21B2043).
23572
References
[1] Mohsen Ahmadi, Ahmad Gholizadeh Lonbar, Abbas Sharifi,
Ali Tarlani Beris, Mohammadsadegh Nouri, and Amir Shar-
ifzadeh Javidi. Application of segment anything model
for civil infrastructure defect assessment. arXiv preprint
arXiv:2304.12600 , 2023.
[2] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Glober-
son, and Alexei Efros. Visual prompting via image inpaint-
ing. In Advances in Neural Information Processing Systems ,
pages 25005–25017, 2022.
[3] Yoshua Bengio and Yann LeCun. Very deep convolutional
networks for large-scale image recognition. In International
Conference on Learning Representations , 2015.
[4] Malik Boudiaf, Hoel Kervadec, Ziko Imtiaz Masud, Pablo
Piantanida, Ismail Ben Ayed, and Jose Dolz. Few-shot seg-
mentation without meta-learning: A good transductive infer-
ence is all you need? In IEEE Conference on Computer
Vision and Pattern Recognition , pages 13979–13988, 2021.
[5] Tianle Chen, Zheda Mai, Ruiwen Li, and Wei-lun Chao.
Segment anything model (sam) enhanced pseudo labels for
weakly supervised semantic segmentation. arXiv preprint
arXiv:2305.05803 , 2023.
[6] Haixing Dai, Chong Ma, Zhengliang Liu, Yiwei Li, Peng
Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Dajiang Zhu, Wei
Liu, et al. Samaug: Point prompt augmentation for segment
anything model. arXiv preprint arXiv:2307.01187 , 2023.
[7] Qi Fan, Wenjie Pei, Yu-Wing Tai, and Chi-Keung Tang. Self-
support few-shot semantic segmentation. In European Con-
ference on Computer Vision , pages 701–719, 2022.
[8] Iraklis Giannakis, Anshuman Bhardwaj, Lydia Sam, and
Georgios Leontidis. Deep learning universal crater detec-
tion using segment anything model (sam). arXiv preprint
arXiv:2304.07764 , 2023.
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
770–778, 2016.
[10] Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and
Seungryong Kim. Cost aggregation with 4d convolutional
swin transformer for few-shot segmentation. In European
Conference on Computer Vision , pages 108–126, 2022.
[11] Joakim Johnander, Johan Edstedt, Michael Felsberg, Fa-
had Shahbaz Khan, and Martin Danelljan. Dense gaussian
processes for few-shot segmentation. In European Confer-
ence on Computer Vision , pages 217–234, 2022.
[12] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing
Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in
high quality. arXiv preprint arXiv:2306.01567 , 2023.
[13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023.
[14] Chunbo Lang, Gong Cheng, Binfei Tu, and Junwei Han.
Learning what not to segment: A new perspective on few-
shot segmentation. In IEEE conference on computer vision
and pattern recognition , pages 8057–8067, 2022.[15] Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun,
Jonghyun Kim, and Joongkyu Kim. Adaptive prototype
learning and allocation for few-shot segmentation. In IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 8334–8343, 2021.
[16] Zechao Li, Yanpeng Sun, Liyan Zhang, and Jinhui Tang. Ct-
net: Context-based tandem network for semantic segmenta-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 44(12):9904–9917, 2021.
[17] Jie Liu, Yanqi Bao, Guo-Sen Xie, Huan Xiong, Jan-Jakob
Sonke, and Efstratios Gavves. Dynamic prototype convolu-
tion network for few-shot semantic segmentation. In IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 11553–11562, 2022.
[18] Yuanwei Liu, Nian Liu, Qinglong Cao, Xiwen Yao, Junwei
Han, and Ling Shao. Learning non-target knowledge for few-
shot semantic segmentation. In IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 11573–11582,
2022.
[19] Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong
Wang, and Chunhua Shen. Matcher: Segment anything with
one shot using all-purpose feature matching. arXiv preprint
arXiv:2305.13310 , 2023.
[20] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations , 2018.
[21] Zhihe Lu, Sen He, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and
Tao Xiang. Simpler is better: Few-shot semantic segmenta-
tion with classifier weight transformer. In IEEE International
Conference on Computer Vision , pages 8741–8750, 2021.
[22] Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorre-
lation squeeze for few-shot segmentation. In IEEE inter-
national conference on computer vision , pages 6941–6952,
2021.
[23] Seonghyeon Moon, Samuel S Sohn, Honglu Zhou, Sejong
Yoon, Vladimir Pavlovic, Muhammad Haris Khan, and Mub-
basir Kapadia. Hm: Hybrid masking for few-shot segmen-
tation. In European Conference on Computer Vision , pages
506–523, 2022.
[24] Khoi Nguyen and Sinisa Todorovic. Feature weighting and
boosting for few-shot segmentation. In IEEE International
Conference on Computer Vision , pages 622–631, 2019.
[25] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193 , 2023.
[26] Bohao Peng, Zhuotao Tian, Xiaoyang Wu, Chengyao Wang,
Shu Liu, Jingyong Su, and Jiaya Jia. Hierarchical dense cor-
relation distillation for few-shot segmentation. In IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
23641–23651, 2023.
[27] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. To-
ken contrast for weakly-supervised semantic segmentation.
InIEEE Conference on Computer Vision and Pattern Recog-
nition , pages 3093–3102, 2023.
[28] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
23573
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115(3):211–252, 2015.
[29] Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and
Byron Boots. One-shot learning for semantic segmentation.
arXiv preprint arXiv:1709.03410 , 2017.
[30] Yanpeng Sun and Zechao Li. Ssa: Semantic structure aware
inference for weakly pixel-wise dense predictions without
cost. arXiv preprint arXiv:2111.03392 , 2021.
[31] Yanpeng Sun, Qiang Chen, Xiangyu He, Jian Wang,
Haocheng Feng, Junyu Han, Errui Ding, Jian Cheng, Zechao
Li, and Jingdong Wang. Singular value fine-tuning: Few-
shot segmentation requires few-parameters fine-tuning. In
Advances in Neural Information Processing Systems , pages
37484–37496, 2022.
[32] Yanpeng Sun, Qiang Chen, Jian Wang, Jingdong Wang, and
Zechao Li. Exploring effective factors for improving visual
in-context learning. arXiv preprint arXiv:2304.04748 , 2023.
[33] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng
Yang, Ruiyu Li, and Jiaya Jia. Prior guided feature enrich-
ment network for few-shot segmentation. IEEE transactions
on pattern analysis and machine intelligence , 44(2):1050–
1065, 2020.
[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems , 2017.
[35] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
Wierstra, et al. Matching networks for one shot learning. In
Advances in Neural Information Processing Systems , 2016.
[36] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou,
and Jiashi Feng. Panet: Few-shot image semantic segmen-
tation with prototype alignment. In IEEE international con-
ference on computer vision , pages 9197–9206, 2019.
[37] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and
Tiejun Huang. Images speak in images: A generalist painter
for in-context visual learning. In IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 6830–6839,
2023.
[38] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,
Chunhua Shen, and Tiejun Huang. Seggpt: Towards seg-
menting everything in context. In IEEE International Con-
ference on Computer Vision , pages 1130–1140, 2023.
[39] Defeng Xie, Ruichen Wang, Jian Ma, Chen Chen, Haonan
Lu, Dong Yang, Fobo Shi, and Xiaodong Lin. Edit every-
thing: A text-guided generative system for images editing.
arXiv preprint arXiv:2304.14006 , 2023.
[40] Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qix-
iang Ye. Prototype mixture models for few-shot semantic
segmentation. In Computer Vision–ECCV 2020: 16th Euro-
pean Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part VIII 16 , pages 763–778, 2020.
[41] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing
Wang, and Feng Zheng. Track anything: Segment anything
meets videos. arXiv preprint arXiv:2304.11968 , 2023.
[42] Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu.
Matte anything: Interactive natural image matting with seg-ment anything models. arXiv preprint arXiv:2306.04121 ,
2023.
[43] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin
Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything:
Segment anything meets image inpainting. arXiv preprint
arXiv:2304.06790 , 2023.
[44] Gengwei Zhang, Guoliang Kang, Yi Yang, and Yunchao
Wei. Few-shot segmentation via cycle-consistent trans-
former. In Advances in Neural Information Processing Sys-
tems, pages 21984–21996, 2021.
[45] Jian-Wei Zhang, Yifan Sun, Yi Yang, and Wei Chen. Feature-
proxy transformer for few-shot segmentation. In Advances in
Neural Information Processing Systems , pages 6575–6588,
2022.
[46] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junt-
ing Pan, Hao Dong, Peng Gao, and Hongsheng Li. Person-
alize segment anything model with one shot. arXiv preprint
arXiv:2305.03048 , 2023.
[47] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junt-
ing Pan, Hao Dong, Peng Gao, and Hongsheng Li. Person-
alize segment anything model with one shot. arXiv preprint
arXiv:2305.03048 , 2023.
[48] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,
Jianfeng Gao, and Yong Jae Lee. Segment everything every-
where all at once. arXiv preprint arXiv:2304.06718 , 2023.
23574
