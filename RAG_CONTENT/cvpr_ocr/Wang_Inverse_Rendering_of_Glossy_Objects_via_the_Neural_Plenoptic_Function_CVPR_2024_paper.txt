Inverse Rendering of Glossy Objects via
the Neural Plenoptic Function and Radiance Fields
Haoyuan Wang1, Wenbo Hu2†, Lei Zhu1, Rynson W.H. Lau1†
1City University of Hong Kong2Tencent AI Lab
† Joint corresponding authors
NeRO Input image Ours
01.0
NeRO Ours
Figure 1. Inverse rendering results of the cutting-edge method, NeRO [10], and ours from calibrated multi-view images of a glossy object.
Geometries are shown as a rendered mesh in the second and third images, and materials (metalness & roughness) are shown as a color map
in the fourth and fifth images. We can see that our results not only have smoother and more accurate geometry but also present a more
reasonable material (since the material of this object should be uniform).
Abstract
Inverse rendering aims at recovering both geometry and
materials of objects. It provides a more compatible re-
construction for conventional rendering engines, compared
with the neural radiance fields (NeRFs). On the other
hand, existing NeRF-based inverse rendering methods can-
not handle glossy objects with local light interactions well,
as they typically oversimplify the illumination as a 2D envi-
ronmental map, which assumes infinite lights only. Observ-
ing the superiority of NeRFs in recovering radiance fields,
we propose a novel 5D Neural Plenoptic Function (NeP)
based on NeRFs and ray tracing, such that more accurate
lighting-object interactions can be formulated via the ren-
dering equation. We also design a material-aware cone
sampling strategy to efficiently integrate lights inside the
BRDF lobes with the help of pre-filtered radiance fields.
Our method has two stages: the geometry of the target ob-
ject and the pre-filtered environmental radiance fields are
reconstructed in the first stage, and materials of the target
object are estimated in the second stage with the proposed
NeP and material-aware cone sampling strategy. Exten-
sive experiments on the proposed real-world and synthetic
datasets demonstrate that our method can reconstruct high-
fidelity geometry/materials of challenging glossy objects
with complex lighting interactions from nearby objects.
Project webpage: https://whyy.site/paper/nep1. Introduction
Although Neural Radiance Fields (NeRFs) [1–5, 8, 16–
18, 21, 23, 27, 32] have achieved remarkable progress in
photo-realistic reconstruction, it is still a challenge to in-
tegrate NeRFs into conventional rendering engines since
NeRFs represent the object and illumination in an entan-
gled manner. Disentangling the representation into geome-
try, materials, and environmental lighting, i.e.inverse ren-
dering, is crucial for the applicability in game production
and extended reality.
Recent works have explored geometry reconstruction [9,
11, 20, 26, 28, 30, 31] and further extended to the materi-
als estimation [7, 10, 19, 23, 33], e.g., albedo, roughness,
and metalness. However, they typically represent the illu-
mination as 2D environmental maps [7, 10, 19], which over-
simplifies the complicated real-world lighting distribution
toinfinite lights only. In many practical scenarios where the
target object is surrounded by other objects, a considerable
amount of light actually comes from the radiance of those
nearby objects. Neglecting these common scenarios results
in inferior reconstruction of both geometry and materials,
especially for glossy objects, such as the improper results
of NeRO [10] in Fig. 1.
In this paper, we propose a Neural Plenoptic Func-
tion (NeP) to represent the global illumination as a 5D func-
tion,fp(x,d), which describes the color of each light ob-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
19999
Environmental
Field
Fields
Learning
Posed Images
Material
LearningObject
Geometry
Albedo / Metallic /
Roughness / LightingObject FieldFigure 2. The pipeline of the proposed method. Our method has
two stages: the fields learning stage for object geometry recon-
struction and the neural radiance fields optimization, and the ma-
terial learning stage using ray tracing.
served at position xwith direction d, in line with the defini-
tion of traditional plenoptic function [15]. Observing the su-
periority of NeRFs in recovering radiance fields from multi-
view images, we construct the NeP from neural radiance
fields based on a ray tracing procedure. However, directly
doing so is computationally intensive, because rendering a
ray’s color from NeRF via ray marching is expensive, and
ray tracing requires sampling a large number of rays inside
the BRDF lobe to approximate the integration in the ren-
dering equation. Thus, instead of sampling the lobe with
rays, we propose an efficient material-aware cone-sampling
strategy, where the cone’s angle is derived from the pre-
dicted roughness. The color of lights inside a cone can be
directly rendered from pre-filtered radiance fields, thanks to
the anti-aliasing techniques in Mip-NeRF [1].
Overall, our method is divided into two stages: geometry
reconstruction and material estimation of the target object.
In the first stage, our model consists of an object field with
a decoupled color representation, i.e. albedo and the color
modulated by the lighting, and a pre-filtered environmental
field for capturing scene radiance. To promote high-quality
geometry reconstruction for glossy objects, we design a dy-
namic weighting loss mechanism from the decoupled col-
ors to reduce the impact of highly uncertain reflective re-
gions while amplifying the significance of diffuse areas. In
the second stage, we adopt the Physically-Based Rendering
(PBR) to estimate high-fidelity materials with our proposed
NeP and material-aware cone-sampling strategy, based on
the extracted triangular mesh of the target object and pre-
trained environmental fields in the previous stage. Our two-
stage method can faithfully reconstruct both the geometry
and material properties of the target object, as shown in
Fig. 1, solely from calibrated multi-view images. And im-
portantly, the results can be seamlessly integrated into con-
ventional rendering engines for relighting, as our method
can produce compatible triangle meshes with physically-
based materials.
To evaluate our method, we compiled two challeng-
ing glossy object datasets, from the rendering engine andreal-world captures, respectively. Extensive experiments
both quantitatively and qualitatively demonstrate that our
method is robust, adaptable, and capable of handling di-
verse challenging illuminations. We also demonstrate the
possible applications of our reconstructions, e.g., relight-
ing, confirming the compatibility of our results with con-
ventional rendering engines. Our contributions are summa-
rized as follows:
• We design a simple yet effective dynamic weighting loss
mechanism from the color decomposition for geometry
reconstruction of challenging glossy objects.
• We propose a novel neural plenoptic function (NeP) to
represent the global illumination and a material-aware
cone-sampling method to effectively integrate NeP over
BRDF lobes for high-fidelity material estimation.
• We constructed benchmarks (including both synthetic and
real-world data) for the inverse rendering of challeng-
ing glossy objects with complex lighting interactions, and
conducted extensive experiments to demonstrate the ef-
fectiveness of our method.
2. Related Work
Neural Radiance Fields. The introduction of Neural Ra-
diance Fields (NeRF) [16] has marked a significant mile-
stone in the field of 3D scene reconstruction, offering a
new perspective on synthesizing novel views of complex
scenes with details and realism. NeRF utilizes a fully con-
nected neural network to model the volumetric scene radi-
ance function. Building upon the foundation of NeRF, nu-
merous works have sought to enhance its capabilities, ad-
dressing limitations and expanding its application range.
Some methods [1, 2, 8] utilize enhanced cone sampling
strategies rather than ray sampling, to address the alias-
ing problem and improve the captured details. Some meth-
ods [4, 5, 18] focus on optimizing NeRF for faster training
and inference times. Other methods [13, 17, 25] improve
NeRF for more robust training under degraded imaging con-
ditions. These advancements collectively contribute to the
evolution of NeRF, and solidify its position as a pivotal tool
for 3D scene representation.
While NeRF models have demonstrated remarkable ca-
pabilities in synthesizing novel views, a key limitation lies
in their inability to directly export reconstructed objects to
rendering engines. A few NeRF-based inverse rendering
approaches have been proposed to address this limitation.
NeRF for Inverse Rendering. Leveraging the potential of
NeRF for inverse rendering tasks has garnered substantial
attention, aiming to retrieve the intrinsic properties, espe-
cially physically-based properties of objects or scenes from
2D images with camera poses. Specifically, inverse ren-
dering of a target object in NeRF aims to reconstruct both
the geometry structure and the material information of the
object. In this realm, a number of methods have been pro-
20000
Incident Rays 
Volume
  Rendering  Enviromental Field Cone Sampling
Reflected RaysStage 1: Fields Learning
IPEGeometry
MLPs
Color
MLPsPEGaussian
Samples
Material MLPs
Roughness Eq 10Stage 2: Material Learning
Metallic,
Albedo,
... th Level RayT racing th Level RayT racing
 Marching 
CubesObject Field 
Rendering equationPE
Eq 3
Color
MLPsIDEGeometry
MLPs
Frozen ModulesObject Rays Enviromental Cones
Learnable ModulesNon-Learnable Modules Learnable Modules Non-Learnable ModulesFigure 3. The detailed structure of our proposed method. Fields learning stage consists of an SDF-based object field and a Mip-NeRF
as the environmental field. Based on them, we construct our neural plenoptic function via ray tracing and material-aware cone sampling
method to represent the global illumination.
posed to unravel the complex interplay among geometry,
material properties, and lighting. High-quality inverse ren-
dering results on real-world data are dependent on realistic
rendering process simulation, which can be modeled by the
framework of the rendering equation, to help decompose the
physically based properties and reduce ambiguity.
Previous methods mimic the rendering equation in dif-
ferent manners. [23, 33] represent the early attempts in
this direction, utilizing NeRF to infer the surface normal,
reflectance, and coarse lighting conditions simultaneously.
Subsequently, some methods [7, 19, 29] try to improve ge-
ometry representation and propose a more accurate material
estimating pipeline using Image-Based Lighting, facilitat-
ing a more robust and accurate inverse rendering process.
They further improve the reconstruction quality by approx-
imating the rendering equation with Monte Carlo sampling.
Recently, some methods [10] took a step further by inde-
pendently estimating the geometry and material properties
of the object, and incorporating occlusion-aware constraints
into the NeuS-based inverse rendering framework, ensuring
that the reconstructed geometry has fine details and material
properties adhere to real-world physics.
However, the existing methods either mainly utilize a
2D environmental map for lighting representation in the
rendering equation, implicitly assuming that alllights over
the scene come from infinity, or models direct lighting
only [34]. These assumptions often lead to less realistic
renderings, especially in scenarios where light sources or
other objects are in closer proximity to the target object.
In contrast, our approach utilizes neural plenoptic function
based on neural radiance fields for more realistic shape andmaterial learning, overcoming this fundamental limitation.
3. Method
Our approach targets the inverse rendering of objects from
calibrated multi-view images. As our method is based on
Neural Radiance Fields (NeRFs) and physically-based ren-
dering (PBR), we start by briefly revisiting the relevant con-
cepts in Sec. 3.1. We then introduce our two-stage pipeline,
which involves a fields learning stage (Sec. 3.2) for geome-
try reconstruction and environmental lighting learning, and
a material learning stage (Sec. 3.3) for material estimation,
as illustrated in Fig. 2.
3.1. Preliminaries
Neural Radiance Fields (NeRF) models the scene as a
continuous function that maps a 5D vector (spatial location
xand viewing direction d) to a color c=fc(x,d)and a
volume density σ=fd(x), where fcandfdare MLPs for
predicting color and density, respectively. While training,
NeRF (which is parameterized as ΘF) casts camera rays
for each pixel, and samples points or Gaussian samples [1]
along the ray. The color of the ray is computed as:
LNeRF(ΘF,r) =Ztf
tnT(t)fd(r(t))fc(r(t),d)dt≈nX
i=1wici,
(1)
where r(t) =o+tdis the parametric representation of the
camera ray, T(t)is the accumulated transmittance along the
ray, and wiis the weights for volume rendering. In prac-
tice,T(t)can be defined as Ti(t) =Qi−1
j=1(1−αj). Instead
20001
of the density, NeuS [26] predicts Signed Distance Field
(SDF) via SDF =fSDF(x), where the surface of the object
is modeled by the zero-level set of SDF. We represent the
high-quality target object surfaces and environmental radi-
ance based on both NeuS and NeRF.
Rendering Equation aims to simulate the interaction of
light and surfaces in a way that adheres to physical laws.
Rendering equation, which is an integral equation describ-
ing the equilibrium of light in a scene, is the core of PBR.
It is given by:
L(x,d) =Z
Ωfr(x,di,d)Li(x,di)(n·di)dω, (2)
where L(x,d)is the outgoing radiance from point xin the
view direction d,Li(x,di)is the incoming radiance from
direction di,fris the BRDF (Bidirectional Reflectance Dis-
tribution Function), nis the surface normal at point x, and
dωrepresents an infinitesimal solid angle.
Based on NeRF and PBR techniques, our approach
improves the neural inverse rendering of glossy objects
by innovatively applying neural radiance fields and neural
plenoptic function (NeP).
3.2. Fields Learning
Since simultaneously modeling geometry, lighting, and ma-
terials would lead to ambiguity, we construct a two-stage
pipeline to optimize the geometry and materials separately.
Our primary objective of the first stage is to simultaneously
reconstruct the precise geometry of the target object and the
environmental radiance field. This is the foundation for the
subsequent material learning stage, which requires accurate
light-surface intersection and surface normal.
Recent works [7, 10] have explored geometry recon-
struction for glossy objects by incorporating physical pri-
ors like image-based rendering and split-sum approxima-
tions. However, they oversimplify the illumination as a
2D environmental map, which would cause suboptimal ge-
ometry, as shown in Sec. 4. Although NeRO [10] extends
the environmental map to a directional function defined on
the sphere, it may not capture the depth information of the
scene, which struggles to reconstruct high-fidelity geometry
for high-detailed objects in some cases. To this end, we first
propose to decouple the final color cointo albedo color ca
and color modulated by the lighting cl, as:
co=ca◦cl, (3)
(in line with [23, 25]). Based on the decomposed colors,
we employ a dynamic weighting loss mechanism, inspired
by [6, 14], to strategically reduce the impact of highly un-
certain reflective regions while amplifying the significance
of diffuse areas when computing the photometric loss. This
mechanism ensures high-quality geometry reconstructionof glossy objects. Although it may distort the learned reflec-
tive color, it is inconsequential as we will estimate more ac-
curate decomposed physical materials in the second stage.
Unlike Ref-NeuS [6], which determines the loss weights for
rays by correlating pixels across views, we propose a sim-
plier yet potent strategy, i.e., leveraging the discrepancy be-
tween albedo and final color to derive the weights:
ws= min1
(co−ca)2+ϵ, u
, (4)
where ϵdenotes a small value. uis a hyperparameter (set to
1.5by default) to cap the upper bound of the weights. The
intuition of our strategy lies is that the discrepancy between
albedo and final color is higher in reflective regions.
The field architectures in the first stage are illustrated in
the left part of Fig. 3. We represent the target object in
a NeuS-based field Foand employ a neural radiance field
Feto model the background environment. During the vol-
ume rendering step, we divide samples along a ray into two
sets with the border of the target object: object samples xo
and environmental samples xe. Object samples xoare first
featurized by the position encoding (PE) [16] and then fed
into the Geometry MLPs to predict the SDF value, which
is further mapped to opacity αoas in NeuS [26], albedo
ca, roughness r, and a feature vector f. Next, we encode
the view direction d, normal vector n(derived from the
SDF), and the roughness rby Integrated Directional Encod-
ing (IDE) [23]. The IDE features and the feature vector f
are then fed into the Color MLPs to predict the colors mod-
ulated by the lighting cl. The final colors cofor samples xo
are produced by Eq. 3 from the predicted caandcl. On the
other hand, environmental samples xeare mapped to opac-
ityαeand color ceby a Mip-NeRF [1] model. Finally, the
opacities ( αo,αe) and colors ( co,ce) of object and envi-
ronmental samples are rendered together into pixel color ˆcp
by volume rendering as Eq. 1. The whole fields are trained
jointly by a weighted photometric loss:
Lc=wl· |cp−ˆcp|, (5)
where ˆcpis the GT pixel color, and wlis the pixel-wise
loss weight accumulated from wsalong the rays via volume
rendering. Besides the photometric loss, we also utilize a
loss function to constrain the curvature of normal vectors.
Refer to the Supplemental for more details.
3.3. Material Learning
After obtaining the precise geometry of the target object,
our goal for the second stage is to estimate the physically-
based materials of the object. To achieve this, we adopt a
more comprehensive rendering process, i.e., ray tracing, to
evaluate the rendering equation in Eq. 2. We can employ the
marching cubes algorithm [12] to extract the triangle mesh
20002
No GTInput Sample Nerfacto [22] NeuS [26] NeRO [10] Ours GT
Figure 4. Comparison of the geometry reconstruction among cutting-edge methods and ours. For each method, we utilize marching cubes
to extract the triangle meshes for comparison.
Figure 5. Samples from the proposed dataset. The first two exam-
ples are synthetic and the last two samples are real-world captured.
of the object by efficiently determining the ray-surface in-
tersection xand surface normal n. Thus, the key to es-
timating the materials is to faithfully represent the global
illumination Liin the rendering equation.
Neural Plenoptic Function. Instead of simplifying the
representation of the lighting as a 2D environment map,
we propose a 5D neural plenoptic function (NeP), symbol-
ized by fp(x,d), to represent the color of the light ob-
served at the spatial location xwith the direction d,i.e.
Li=fp(x,di). However, it is challenging to directly learn
NeP due to the high dimensionality. Because of the supe-
riority of NeRF in representing the radiance field, we pro-
pose to construct NeP from the pre-trained environmental
radiance fields Fein the first stage. Specifically, we start
by modeling the intersection between the incoming light
ri(t) = x+tdiand the object mesh Mas a function
I(ri, M)to determine the point of contact xi. If there are
no intersections, which means that the incoming light is a
direct light from the environment, we utilize Eq. 1 with thepretrained Feto define the lighting color. We also utilize a
simple MLP to learn the residual value of LNeRF, permitting
the model to learn the distant lighting that is not captured
by training images. Conversely, if the intersection point xi
exists, we employ a ray-tracing approach based on Eq. 2
to obtain the color of the incoming light as it is an indirect
light in this situation. The plenoptic function is defined as:
fp(x,di) =L(xi,di), if∃xi,
LNeRF(ΘFe,ri(t)), otherwise ,(6a)
(6b)
where Lis a discretized rendering equation discussed next.
Ray Tracing. The discretized rendering equation Lis de-
rived via the ray tracing algorithm as:
L(x,d) =mX
k=1fr(x,dk,d)fp(x,dk)(n·dk),(7)
where mis the number of sampled incoming rays per in-
tersection point. Note that the formulation of fpin Eq. 6a
also incorporates the computation of L. Thus, a recursive
ray tracing process is constructed. The ray tracing algo-
rithm unfolds in Nlevels. At each level, it samples a set
of incoming lights emitted from the current shading point
x, where the color of each light is given using Eq. 6b if no
intersections are found with the object, and the tracing pro-
cess is terminated. Otherwise, the ray tracing delves deeper
20003
into the next level, permitting the light to do an additional
bounce. Upon the N-th level tracing, if there still exists an
intersection point with the object mesh, we employ an MLP
to predict the lighting color: L(N)(x,d) = MLP(x,d),
thus concluding the journey of the light.
Material-Aware Cone Sampling. Representing light via
the proposed 5D neural plenoptic function offers a fidelity
improvement. However, directly applying it is not practi-
cal, as it introduces a significant computational cost in the
ray marching of NeRF, which demands sampling along rays
to get the color information for each individual light. The
complexity is further increased when doing important ray
sampling for ray tracing within the BRDF lobe to satisfy
the rendering equation.
To address this challenge, we introduce a material-aware
cone sampling technique. In the first stage, we adopt a Mip-
NeRF as our environmental field, which samples cones in-
stead of rays like a vanilla NeRF. The introduction of Mip-
NeRF is informed by the innate congruence of its cone sam-
pling with the BRDF lobe-centric importance ray sampling.
During the training of Mip-NeRF in the fields learning
stage, the pre-integrated Gaussian samples are employed
through Integral Positional Encoding (IPE), yielding bet-
ter rendering results than a vanilla NeRF without incurring
obvious extra costs. During the fields learning stage, the
environmental field Feis trained as a cone pre-filterer su-
pervised by the training images.
In the material learning stage, we fix Feand sample
cones from the BRDF lobe. Because of the correlation be-
tween surface roughness and GGX distribution, we derive
the cone angle directly from the predicted roughness. Sub-
sequent ray marching of Feyields the pre-filtered color for
the incoming light of each integral component in the ren-
dering equation. Specifically, considering the roughness pa-
rameter rat a shading point, we adopt the GGX distribution
function D(m)to describe the probability distribution of
the microfacet normals m. From this, we have the proba-
bility density function (PDF) for the azimuth angle ϕand
elevation angle θ[24]:
pm(θ, ϕ) =r4cosθsinθ
π((r4−1) cos2θ+ 1)2. (8)
Our aim is to determine θm, which bounds the range of
sampling the orientation of the microfacet normal m, allow-
ing the BRDF lobe to capture a predefined portion βof the
light energy over the hemisphere. For practical purposes,
we select β= 0.9to encompass 90% of the radiance en-
ergy. This leads us to establish the cumulative distribution
function (CDF) Pmand to resolve the following equation:
Pm(θm) =r4
cos2θm(r4−1)2+ (r4−1)−1
r4−1,(9)wherein the solution of Pm=βis presented as:
θm= arctan 
r2s
β
1−β!
. (10)
The angle θm, indicative of the spatial extent of the
BRDF lobe, is thus directly correlated with the surface
roughness. Consequently, we can sample the cone with
the apex angle θcusing θmvia a simple math transform,
a method without the need for learnable parameters. The
details of the derivation can be found in the Supplemental.
Leveraging the pre-filtered lighting significantly reduces
the number of lighting samples. Typically, employing a
GGX distribution-based important sampling requires about
256 diffuse and 128 specular rays to get satisfactory results.
In contrast, the proposed method achieves competitive re-
sults with merely 8 diffuse and 4 specular cones.
4. Experiments and Analysis
In this section, we detail the experiments conducted to val-
idate the efficacy of our proposed method. We assess our
method’s performance in both geometry and material recon-
struction, and demonstrate its practical applications.
Dataset. Our dataset is constructed to benchmark the in-
verse rendering of challenging glossy objects with diverse
lighting interactions with nearby objects. It comprises 20
scenes in total: 10 real-world scenes captured through pho-
tographing glossy objects under varying lighting scenarios
to produce multi-view images and 10 synthetic scenes. The
synthetic scenes are crafted with glossy objects and manu-
ally designed background environments, and the multi-view
images are rendered using photo-realistic path-tracing ren-
dering engine. A distinctive aspect of our synthetic dataset
is that we did not use environmental maps as the back-
ground directly, which is commonly used in prior works.
Instead, we focus on more realistic settings where the ob-
jects are situated within tangible environments consisting of
other objects. Samples from the proposed dataset are shown
in Fig. 5.
Geometry Evaluations. To evaluate the quality of our
method in geometry reconstruction, we compare our mesh
outputs against those generated by several cutting-edge
approaches, including Nerfacto [22], NeuS [26], and
NeRO [10].
The visual comparison results are displayed in Fig. 4,
which provides a side-by-side comparison of the recon-
structed objects. We also compare the Chamfer Distance
(CD) with other methods on the synthetic scenes with GT
meshes, as shown in Tab. 1. Our method consistently out-
performs the others across all tested scenes, achieving the
lowest average CD and better visual quality with better geo-
metric reconstruction fidelity. The comparison of the geom-
etry reconstruction highlights the strengths of our proposed
20004
Ours NeRO GT Ours NeROAlbedoMetallic &
RoughnessLighting Relighting
No GT
01.0
Figure 6. Comparison on the material estimation results. We display one real-world data (bird) and one synthetic data (Nefertiti) with GT
for visual comparison. In the images of metallic & roughness row, metallic is displayed on the left half and roughness on the right half.
Ours w/o cone Ours w/o Input image Ours
 Ours Input image
(a) (b)Ours w/o 
Figure 7. Ablation study results showcasing the impact of key components on inverse rendering quality for both stages. The absence of
dynamic weighting (Ours w/o ws) leads to less smooth surface reconstruction, while the lack of environmental field (Ours w/o Fe) or
material-aware cone sampling method (Ours w/o cone) diminishes material fidelity. In each image of Figure (b), metallic is displayed on
the left half and roughness on the right half.
dynamic weighting loss mechanism from the decoupled
colors in the first stage. The performance of our method
in reconstructing accurate and detailed geometries lays the
groundwork for the subsequent material property estima-
tion and their applications of seamlessly integrated into the
rendering engine.
Material Evaluations. Moving beyond geometry, we com-
pare the material estimation results produced by our method
with NeRO [10]. Given our synthetic data subset, we pos-sess the ground truth values for albedo, roughness, and
metallic maps, allowing for a direct quantitative assessment
via Mean Squared Error (MSE) as illustrated in Tab. 1. For
visual comparison, we present two samples to validate com-
pared methods and GT in Fig. 6. Both the quantitative and
qualitative results reveal the competitive performances of
our method in reconstructing materials with high fidelity,
where our proposed neural plenoptic function is able to pre-
dict a more precise and smooth roughness, metallic, and
20005
Geometry Comparison (CD ↓) Roughness / Metallic / Albedo Comparison (MSE ↓)
Nerfacto [22] NeuS [26] NeRO [10] Ours NeRO [10] Ours
Bunny 0.05498 0.00852 0.00153 0.00147 0.002 / 0.022 / 0.044 0.002 /0.016 /0.022
Box 0.07028 0.03339 0.00412 0.00135 0.003 / 0.068 / 0.056 0.001 /0.019 / 0.067
Beethoven 0.04038 0.01706 0.00197 0.00146 0.007 / 0.030 / 0.031 0.004 /0.026 /0.027
Suzanne 0.05753 0.00754 0.00264 0.00227 0.004 / 0.030 / 0.024 0.001 /0.022 / 0.029
Nefertiti 0.05299 0.01053 0.00587 0.00167 0.020 / 0.103 / 0.040 0.008 /0.021 /0.035
Avg. 0.05523 0.01541 0.00322 0.00164 0.007 / 0.051 / 0.039 0.003 /0.020 /0.036
Table 1. Quantitative comparison results. We compare the Chamfer Distance (CD) between the meshes from our method and those from
other methods for the synthetic objects with GT on the left, and compare the MSE between the materials from of our methods and those
from other methods on the right.
slightly better albedo compared with the existing methods.
Ablation Study. To assess the contribution of different
components in our model, we perform an ablation study on
the two stages of our method. In the first stage, we study
the effect of the proposed dynamic weighting method. We
compare the result of our full model with that of the variant
where the proposed dynamic weighting scheme is excluded
(i.e., Ours w/o ws). In the second stage, we study the re-
sults of our model in material estimation (1) by replacing
the environmental field with an MLP to predict environ-
mental lighting ( i.e., Ours w/o Fe), and (2) by replacing the
material-aware cone sampling mechanism by a fixed num-
ber of rays ( i.e., Ours w/o cone). The ablation results on
two samples are shown in Fig. 7.
We can see that by omitting the dynamic weighting strat-
egy, which is crucial in balancing the influence of uncer-
tain regions, we observe a decrease in the model’s ability
to reconstruct a smooth surface for the target object. With-
out the environmental field or material-aware cone sampling
method for lighting representation, it leads to a degradation
in material fidelity and introduces more ambiguity. Overall,
we have demonstrated through the ablation study the impor-
tance of different parts of our method on the quality of the
final inverse rendering results.
Limitation. While our method has been shown to make
advancements in physically-based inverse object rendering,
it is important to acknowledge certain limitations inherent
in our current method.
One of the primary constraints of our approach is that
the material learning stage is heavily reliant on the quality
of the reconstructed geometry in the field learning stage.
This dependency implies that the inaccuracies in the recov-
ered geometry may adversely affect the material estimation
process. If the geometry reconstructed in the first stage is
not precise, it could lead to suboptimal material estimations
in the subsequent stage.
Besides, although our method reduces the ambiguitiestypically present in inverse rendering tasks. we observe that
the decomposition between lighting, geometry, and mate-
rial properties is not entirely unambiguous. For example,
there are scenarios where the color affected by the geomet-
ric structure is inaccurately incorporated into the albedo.
This suggests an inherent complexity in perfectly disentan-
gling the contributing factors to the final rendered image
even with an explicit representation of lighting.
In future works, addressing these limitations will be cru-
cial to further enhance the robustness and accuracy of our
neural inverse rendering method. Potential improvements
may include introducing more sophisticated constraints for
geometry-material interdependence and refining the decom-
position process to minimize ambiguities.
5. Conclusion
In this paper, we have presented a novel physically-based
inverse rendering method for glossy objects, with the in-
troduction of Neural Plenoptic Function (NeP) based on
NeRFs. Our method addresses the limitations of the de-
pendency on the simplified lighting representation in pre-
vious NeRF-based inverse rendering approaches. It is for-
mulated as a two-stage model. The fields learning stage
enhances the accuracy of 3D geometry reconstruction, es-
pecially for glossy objects under complex lighting. In the
material learning stage, NeP employs a 5D neural plenoptic
function for lighting representation based on the object field
and environmental field, leading to higher-fidelity material
estimation and inverse rendering. Our proposed material-
aware cone sampling strategy further improves the effi-
ciency of material learning. Experiments on real-world and
synthetic datasets demonstrate the superior performance of
our method.
Acknowledgements
This work is partly supported by a GRF grant from the Re-
search Grants Council of Hong Kong (Ref.: 11205620).
20006
References
[1] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance fields. In ICCV , 2021. 1, 2, 3, 4
[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance fields. In CVPR , 2022. 2
[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-
based neural radiance fields. In ICCV , 2023.
[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance fields. In ECCV , 2022.
2
[5] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In CVPR , 2022. 1,
2
[6] Wenhang Ge, T. Hu, Haoyu Zhao, Shu Liu, and Yingke
Chen. Ref-neus: Ambiguity-reduced neural implicit sur-
face learning for multi-view reconstruction with reflection.
InICCV , 2023. 4
[7] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.
Shape, Light, and Material Decomposition from Images us-
ing Monte Carlo Rendering and Denoising. In NeurIPS ,
2022. 1, 3, 4
[8] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao,
Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip representation
for efficient anti-aliasing neural radiance fields. In ICCV ,
2023. 1, 2
[9] Zhaoshuo Li, Thomas M ¨uller, Alex Evans, Russell H Tay-
lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.
Neuralangelo: High-fidelity neural surface reconstruction. In
CVPR , 2023. 1
[10] Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng
Wang, Lingjie Liu, Taku Komura, and Wenping Wang. Nero:
Neural geometry and brdf reconstruction of reflective objects
from multiview images. In SIGGRAPH , 2023. 1, 3, 4, 5, 6,
7, 8
[11] Xiaoxiao Long, Cheng Lin, Lingjie Liu, Yuan Liu, Peng
Wang, Christian Theobalt, Taku Komura, and Wenping
Wang. Neuraludf: Learning unsigned distance fields for
multi-view reconstruction of surfaces with arbitrary topolo-
gies. In CVPR , 2023. 1
[12] William E. Lorensen and Harvey E. Cline. Marching cubes:
A high resolution 3d surface construction algorithm. Pro-
ceedings of the 14th annual conference on Computer graph-
ics and interactive techniques , 1987. 4
[13] Li Ma, Xiaoyu Li, Jing Liao, Qi Zhang, Xuan Wang, Jue
Wang, and Pedro V . Sander. Deblur-nerf: Neural radiance
fields from blurry images. In CVPR , 2022. 2
[14] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi,
Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. Nerf in the wild: Neural radiance fields for uncon-
strained photo collections. In CVPR , 2021. 4
[15] Leonard McMillan and Gary Bishop. Plenoptic modeling: an
image-based rendering system. Proceedings of the 22nd an-nual conference on Computer graphics and interactive tech-
niques , 1995. 2
[16] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , 2020. 1, 2, 4
[17] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla,
Pratul P. Srinivasan, and Jonathan T. Barron. Nerf in the
dark: High dynamic range view synthesis from noisy raw
images. In CVPR , 2022. 2
[18] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Trans. Graph. , 41(4):1–15,
2022. 1, 2
[19] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,
Wenzheng Chen, Alex Evans, Thomas M ¨uller, and Sanja Fi-
dler. Extracting Triangular 3D Models, Materials, and Light-
ing From Images. In CVPR , 2022. 1, 3
[20] Michael Oechsle, Songyou Peng, and Andreas Geiger.
Unisurf: Unifying neural implicit surfaces and radiance
fields for multi-view reconstruction. In ICCV , 2021. 1
[21] Christian Reiser, Richard Szeliski, Dor Verbin, Pratul P.
Srinivasan, Ben Mildenhall, Andreas Geiger, Jonathan T.
Barron, and Peter Hedman. MERF: memory-efficient radi-
ance fields for real-time view synthesis in unbounded scenes.
ACM Trans. Graph. , 42(4):89:1–89:12, 2023. 1
[22] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,
Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake
Austin, Kamyar Salahi, Abhik Ahuja, David McAllister,
Justin Kerr, and Angjoo Kanazawa. Nerfstudio: A modular
framework for neural radiance field development. In SIG-
GRAPH , 2023. 5, 6, 8
[23] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,
Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF:
Structured view-dependent appearance for neural radiance
fields. In CVPR , 2022. 1, 3, 4
[24] Bruce Walter, Stephen R. Marschner, Hongsong Li, and Ken-
neth E. Torrance. Microfacet models for refraction through
rough surfaces. In Proceedings of the 18th Eurographics
Conference on Rendering Techniques , 2007. 6
[25] Haoyuan Wang, Xiaogang Xu, Ke Xu, and Rynson W.H.
Lau. Lighting up nerf via unsupervised decomposition and
enhancement. In ICCV , 2023. 2, 4
[26] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
InNeurIPS , 2021. 1, 4, 5, 6, 8
[27] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu,
Taku Komura, Christian Theobalt, and Wenping Wang. F2-
nerf: Fast neural radiance field training with free camera tra-
jectories. In CVPR , 2023. 1
[28] Yiming Wang, Qin Han, Marc Habermann, Kostas Dani-
ilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast
learning of neural implicit surfaces for multi-view recon-
struction. In ICCV , 2023. 1
[29] Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian
Fang, David McKinnon, Yanghai Tsin, and Long Quan.
20007
Neilf: Neural incident light field for physically-based ma-
terial estimation. In ECCV , 2022. 3
[30] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. V ol-
ume rendering of neural implicit surfaces. In NeurIPS , 2021.
1
[31] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-
tler, and Andreas Geiger. Monosdf: Exploring monocular
geometric cues for neural implicit surface reconstruction. In
NeurIPS , 2022. 1
[32] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. Nerf++: Analyzing and improving neural radiance
fields. ArXiv , abs/2010.07492, 2020. 1
[33] Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul E.
Debevec, William T. Freeman, and Jonathan T. Barron. Ner-
factor. TOG , 40:1 – 18, 2021. 1, 3
[34] Yiyu Zhuang, Qi Zhang, Xuan Wang, Hao Zhu, Ying Feng,
Xiaoyu Li, Ying Shan, and Xun Cao. Neai: A pre-convoluted
representation for plug-and-play neural ambient illumina-
tion. 2024. 3
20008
