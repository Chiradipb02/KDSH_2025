ES3: Evolving S elf-S upervised Learning
of Robust Audio-Visual S peech Representations
Yuanhang Zhang1,2, Shuang Yang1,2, Shiguang Shan1,2, Xilin Chen1,2
1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),
Institute of Computing Technology, CAS, Beijing 100190, China
2University of Chinese Academy of Sciences, Beijing 100049, China
zhangyuanhang15@mails.ucas.ac.cn, {shuang.yang, sgshan, xlchen }@ict.ac.cn
Abstract
We propose a novel strategy, ES3, for self-supervised learn-
ing of robust audio-visual speech representations from un-
labeled talking face videos. While many recent approaches
for this task primarily rely on guiding the learning pro-
cess using the audio modality alone to capture informa-
tion shared between audio and video, we reframe the prob-
lem as the acquisition of shared ,unique (modality-specific)
andsynergistic speech information to address the inherent
asymmetry between the modalities. Based on this formu-
lation, we propose a novel “evolving” strategy that pro-
gressively builds joint audio-visual speech representations
that are strong for both uni-modal (audio & visual) and bi-
modal (audio-visual) speech. First, we leverage the more
easily learnable audio modality to initialize audio and vi-
sual representations by capturing audio-unique and shared
speech information. Next, we incorporate video-unique
speech information and bootstrap the audio-visual repre-
sentations on top of the previously acquired shared knowl-
edge. Finally, we maximize the total audio-visual speech
information, including synergistic information to obtain ro-
bust and comprehensive representations. We implement ES3
as a simple Siamese framework and experiments on both
English benchmarks and a newly contributed large-scale
Mandarin dataset show its effectiveness. In particular, on
LRS2-BBC, our smallest model is on par with SoTA models
with only 1/2 parameters and 1/8 unlabeled data (223h).
1. Introduction
The task of learning audio-visual speech representations has
gained increasing importance due to its potential to support
a wide range of relevant applications. Building on the suc-
cess in other domains [9, 11, 12, 14, 22, 31], several ef-
fective self-supervised approaches have recently emerged
for this task by pre-training models on paired or mixed au-
dio, visual and text data with pretext tasks like speech unitprediction [30, 59, 80]. However, learning representations
in the audio-visual speech domain faces unique challenges
brought by the inherent asymmetry between the two modal-
ities involved. First, audio and video often convey distinct
and complementary types of speech information. For ex-
ample, /m/and/n/sound similar, but are visually differ-
ent; on the contrary, /p/and/b/are visually similar but
can be distinguished with audio [70]. Even more interest-
ing is the presence of synergistic information – information
that can further enhance speech perception [55] but only
emerges when audio and video are considered together, and
cannot be inferred when either modality is absent. Sec-
ond, they manifest different amounts of noise and infor-
mation. Specifically, in most existing audio-visual speech
datasets, audio is characterized by high sampling rate and
relatively clean speech samples, making audio represen-
tations readily learnable from these datasets. In contrast,
the visual modality often presents several challenging fac-
tors, including missing ( e.g. due to self-occlusion in profile
faces or low image quality), redundant ( e.g. due to appear-
ance variations, makeup and beards etc.) and ambiguous
(e.g. due to the lower sampling rate and the presence of ho-
mophenes) visual information. Their presence complicates
learning meaningful and robust representations solely from
video. The difficulties encountered in the continuous ef-
forts to improve the performance of visual speech recogni-
tion (VSR) [15, 24, 40, 41] models illustrate this persistent
obstacle. Additionally, these differences between the two
modalities lead to an imbalance between their learning dy-
namics, requiring additional manual settings [3, 39, 42, 61].
In this work, we break down the problem of learn-
ing audio-visual speech representations into the learning of
three different types of information: shared (across modal-
ities), unique (modality-specific) and synergistic speech in-
formation. Based on our formulation, we propose a novel
strategy called ES3(short for Evolving Self-Supervised
Learning of Robust Audio-Visual Speech Representations)
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27069
to tackle the asymmetry between the two modalities for
learning robust audio-visual speech representations. Specif-
ically, our ES3employs a three-stage progressive learning
approach. Initially, it focuses on acquiring the easily learn-
able audio- unique andshared speech information. Then, it
addresses the more challenging visual- unique speech infor-
mation while incorporating it into the initial audio-visual
joint representation. Lastly, it optimizes the final audio-
visual representations by leveraging synergy , on top of the
acquired shared and modality-unique information.
Our contributions can be summarized as follows:
• We propose a new formulation for audio-visual speech
representation learning by decomposing the task into
the acquisition of shared (across modalities), unique
(modality-specific) and synergistic speech information.
• We propose a novel strategy, ES3to progressively learn
the three types of information in a self-supervised manner
with a simple Siamese framework. This evolving strategy
significantly alleviates the previous burden of tuning bal-
ancing parameters.
• Extensive experiments on two widely used benchmarks
show that ES3rivals previous methods in auditory, visual
and audio-visual speech recognition performance.
• We also contribute a new large-scale Mandarin audio-
visual speech dataset named CAS-VSR-S101 to the com-
munity for evaluation of similar methods.
2. Related Work
Self-supervised learning from multi-modal data. The ar-
guably most successful paradigm for self-supervised multi-
modal learning in the past few years is contrastive learn-
ing, which exploits the common information encoded in
different modalities [68], such as audio (A) [6, 7], image
(I) [53, 66], video (V) [5, 8, 32, 67] and text (T) [4, 16,
43, 44, 49, 63]. These approaches focus on learning se-
mantically aligned multi-modal representations that are ef-
fective for specific downstream tasks like cross-modal re-
trieval, but discard modality-unique information that is use-
ful for other uni-modal tasks. To solve this problem, Gong
et al. [25] combine contrastive learning with the masked
prediction objective [28] to learn strong audio-visual repre-
sentations suitable for both retrieval and joint classification.
Liang et al. [35] propose a new algorithm, F ACTOR CL to
learn self-supervised multi-modal representations that cap-
ture both shared and unique information and verify its ef-
fectiveness on multiple tasks. In this paper, we explicitly
address capturing unique, shared and synergistic informa-
tion for uni- and multi-modal downstream tasks.
Learning audio-visual speech representations. A main
challenge of learning audio-visual speech representation is
properly handling the aforementioned asymmetry between
audio and video. An established approach is to leverage
the easily learnable audio modality as a teacher to guidevisual or audio-visual speech representation learning. Im-
plementing additional measures to mitigate audio domi-
nance and promote visual features during learning is vi-
tal for this paradigm. For instance, Shi et al. [59] and
Zhang et al. [76] use different masking strengths for the
two modalities, apply modality dropout, and enforce sim-
ple audio features by only using a weak linear projec-
tion layer. Others have explored freezing pre-trained au-
dio representations [2, 38, 78]. However, such an audio-
centric approach is sub-optimal and inefficient. First, it
limits the learned representations to capturing only shared,
i.e. modality-invariant information, or at most, shared and
audio-unique information. Recently, a few attempts have
been made to go beyond merely using audio as the anchor
modality. Haliassos et al. [27] combine multiple audio-
visual prediction tasks in place of purely audio-centric ones,
and treats A-V asymmetry by having the visual stream pre-
dict only auditory but not visual targets. Lian et al. [34] per-
form a series of trials to determine an optimal schedule for
audio and video input combination. Second, the process of
exhaustively testing and identifying the optimal set of bal-
ancing parameters such as masking ratios, modality sched-
ules and modality dropout rate is extremely time-consuming
and computationally demanding. In contrast, our newly
proposed evolving strategy identifies different types of task-
relevant information and learns them progressively, while
avoiding pitfalls and computational burdens (see supp).
3. Our Proposed ES3
3.1. Preliminaries
The goal of ES3is to learn audio-visual speech representa-
tions from unlabeled talking face videos. The inputs to our
ES3are paired audio-visual data Xa,v={(Xa,i, Xv,i)|i=
1,2, . . . , N }. Each of the Nsamples consists of a sequence
of audio frames Xa,i= LogFBank( ai)∈RF×Ta,iand
a temporally synchronized video sequence Xv,i=vi∈
RC×H×W×Tv,ishowing the lips of a speaker, where Fis
the filter bank dimension, Cis the number of video chan-
nels,Ta,iandTv,iare the total number of audio and video
frames in the i-th sample, and HandWare the width and
height of each frame. Audio and video are sampled at 100
and 25 FPS, i.e. for a 1-second clip, Ta= 4Tv= 100 . The
goal is to learn a d-dimensional contextualized representa-
tionγ∈Rd×Tvcapturing vital speech and context informa-
tion within the two modality spaces.
Base architecture. The base architecture used in this pa-
per is a relatively simple one and can be easily extended
if needed. It includes two modality-specific encoders φa
andφv, instantiated as a 2-layer CNN-based audio filter
bank encoder and a lightweight VGG-(2+1)D video en-
coder [57, 58]. Here, φadownsamples Xaby 4x to match
the video frame rate; φvhas a temporal receptive field of
5 frames, or 200ms at 25 FPS, same as the most com-
27070
monly adopted 3D-ResNet18 encoder [41, 59, 64], but has
much fewer parameters. A modality-agnostic Transformer-
based [71] modality fusion encoder θequipped with rotary
positional embeddings (RoPE) [65] follows the modality-
specific encoders φaandφv. To allow θto accept either
audio, video, or audio-visual features, we adopt the simple
sum operation to compute a fused audio-visual representa-
tionFfusedfrom modality features Fm=φm(Xm), where
m∈ {a, v}, to allow easy handling of missing modalities:
Ffused=

Fa+Fv if input is audio-visual ,
Fa+0=Faif input is audio ,
Fv+0=Fvif input is video .(1)
The Transformer encoder θthen yields the final contextual
features E=θ(Ffused).
Based on this base architecture, we build our Siamese
based learning framework as shown in Fig. 1(a). A stu-
dent model ( left) is guided by a teacher model ( right ), which
share the same base architecture. The student and teacher
accept different versions of the same speech inputs ( e.g.
masked vs unmasked). The teacher model updates its pa-
rameters in an exponential moving average (EMA) manner,
and the student is trained to regress the teacher outputs in
general. In the subsequent sections, our objective is to op-
timize the student model with the base architecture to ob-
tain the parameterized audio-visual speech representations
γ(a,v;φa, φv, θ) =E.
3.2. Problem Reformulation
In the audio-visual speech representation learning task, au-
dio and video inputs can be taken as two different views
of the same underlying speech signal, carrying different
types of information. The relationship between the speech
information in the two modalities can be visualized as
the bottom-left Venn diagram in Fig. 1(b). Owing to the
asymmetry between the two modalities, there exists both
modality-unique speech information, i.e. audio- and video-
unique information UaandUv. There is also speech in-
formation Rshared between the two modalities, since they
share the same underlying speech production process. At
the same time, information that only emerges when both
audio and video modality are present is the synergistic in-
formation, which we denote by S. Our overall objective is
to learn a representation that maximizes the total informa-
tionR+Ua+Uv+Sfor bi-modal (audio-visual) speech
representations, as well as R+UaandR+Uvfor uni-modal
(audio & visual) speech representations.
With this in mind, we reframe the task of learning audio-
visual speech representations as a mutual information max-
imization task. Specifically, the two modalities, corre-
sponding to source variables XaandXv, have mutual in-
formation with the target speech content variable Y, con-
taining information useful for downstream tasks that wewish to capture. Denote the joint mutual information be-
tween the sources and the target by I(Xa, Xv;Y). The
problem of learning robust audio-visual speech represen-
tations reduces to finding the set of parameters that maxi-
mizes I(γ(Xa), γ(Xv);Y) =I(Eav;Y)1. Similarly, the
problem of learning robust audio and visual speech rep-
resentations can be viewed as maximizing the mutual in-
formation I(γ(Xa);Y) =I(Ea;Y)andI(γ(Xv);Y) =
I(Ev;Y), respectively. Taking advantage of the Partial In-
formation Decomposition (PID) framework [73], we can
precisely decompose the multivariate mutual information
I(Eav;Y), I(Ea;Y)andI(Ev;Y)into the previous types
of information as:
R+Ua=I(Ea;Y), (2)
R+Uv=I(Ev;Y), (3)
Ua+S=I(Ea;Y|Xv), (4)
Uv+S=I(Ev;Y|Xa), (5)
R−S=I(Ea;Ev;Y), (6)
R+Ua+Uv+S=I(Eav;Y), (7)
where R, U a, UvandSare non-negative.
To optimize the above mutual information terms with a
neural network parameterized by φa,φvandθ, we divide
the optimization process into three progressive and non-
conflicting stages. In stage 1 , we learn audio-unique in-
formation Uaand shared information Rfrom the informa-
tive audio modality to maximize I(Ea;Y). This process
also helps to bootstrap visual representations by learning the
shared information R. Instage 2 , we turn to learning visual-
unique information Uvby maximizing I(Ev;Y) =R+Uv,
which is now easier with the visual information captured
viaRin stage 1. At the same time, we inject the learned vi-
sual information into the audio-visual joint representations
to initialize I(Eav;Y). Finally, in stage 3 we optimize the
joint audio-visual representation on top of the previously
learned information by maximizing I(Eav;Y).
3.3. The Complete Learning Framework
To learn network parameters that maximize the mutual in-
formation terms I(E•;Y), we leverage the popular self-
supervised masked prediction task based on the simple
Siamese framework to support the learning process via self-
distillation. Inspired by the recent work of Baevski et al.
[12], we create multiple masked copies for each training
sample, encode the unmasked regions with the Transformer
encoder, and train the student to recover contextualized tar-
gets of the masked regions with a lightweight 1D convo-
lutional decoder, which are generated by an online EMA
teacher. This task can be used to learn any mutual infor-
1This equality holds since the architecture implemented in this work
only models additive modality synergy stemming from sum fusion.
27071
timetimeTransformer 
Encoder LayerEMA updates
Fusion Module
(EMA Teacher   )
Audio- Video 
Pair
Unmasked
InputsMasked
Input Copies
Transformer 
Encoder Layer
Modality -
Specific
Encoders…
…
Fusion Module
(Student   )Stage 1 loss
Stage 2 loss
Stage 3 loss
Modality -
Specific
Encoders
Shared Parameters1D Convolutional Decoder
(a)
Stage 1 Stage 3 Stage 2
(b)Teacher 
Feature SpaceCodebookcluster
update
(c)CodesStudent
Features
Teacher
Features⊕ ⊕Figure 1. Overview of the proposed method, ES3.(a) We use a Siamese framework with an EMA teacher to implement an iteration-free
training procedure. (b) Venn diagrams showing how the three types of information ( R,U•andS) are acquired in an evolving learning
process. (c) Illustration of self-distillation with online clustering corresponding to Ldistill (Ftea,Fstu).
mation I(E•;Y)when Yis the downstream speech under-
standing task. For example, training on auditory or visual
speech acts as a proxy to maximize I(Ea;Y)orI(Ev;Y).
Self-distillation with online clustering [36]. We perform
distillation from the right-hand side teacher to the left-hand
side student in Fig. 1(a). Specifically, as illustrated in
Fig. 1(c), for each layer kof the top KTransformer lay-
ers in θwith hidden dimension D, we learn a codebook
Qk={qk
1, . . . ,qk
Q}withQcodewords qk
i∈RD, which
is updated as follows: for each entry j, a set ˜Zk
teaof the
teacher output features ˜zk
teaclosest to the current represen-
tation of qin the codebook is found as
˜Zk
tea=
˜zk
teaq= arg min
j∈{1,2,...,Q}||˜zk
tea−qk
j||2
,(8)
where the set indices qare used as discrete pseudo-labels to
train the student model. Codewords are then updated in an
EMA fashion as a weighted sum of the embeddings in ˜Zk
tea
with decay rate τcode. The associated loss function is
Ldistill(Ftea, Fstu) =X
t∈MX
k∈(K−N,K]logpψk(qt|zK
t),
(9)
where FteaandFstuare teacher and student features, Mis
the set of masked time steps, ψkis the softmax activations of
linear projections for each target layer k,qtis the codeword
index for time step tof the teacher ˜θ, and zK
tis the output of
theK-th and final layer of the student model θ. The teacher
and the online clustering process do not require gradients.
EMA teacher parameterization. To reduce GPU memoryusage, we apply EMA updates to the Transformer encoder
θ, but not the earlier modality-specific encoders φaandφv,
inspired by [50]. As shown in Fig. 1, the teacher model
weights ˜θare computed as an exponential moving average
of the student model weights θ:
˜θ←η˜θ+ (1−η)θ, (10)
where η= min {ηmax,(1 + step )/(10 + step )}is a mo-
mentum parameter that slowly increases over time until a
pre-determined value ηmaxis reached.
3.4. The Learning Process
We now elaborate on individual learning stages, based on
the above Siamese framework and distillation loss.
Stage 1: Learning audio-unique information and shared
information. In stage 1, we perform distillation from un-
masked audio teacher model to the masked audio student
model, yielding audio-unique information Uaand shared
information Rfrom audio. The distillation loss is:
La→a′=Ldistill(Ea, E′
a). (11)
Being readily learnable, audio has the potential to provide
most of Rfor learning shared information. However, op-
timizing I(Ea;Y)alone cannot help visual learning. Not-
ing that the InfoNCE loss between two variables has been
widely used to maximize a lower bound on the mutual in-
formation, we adopt the InfoNCE loss to learn shared in-
formation corresponding to I(Ea;Ev)to bootstrap visual
learning with the shared information learned from audio.
27072
Specifically, we apply it to a fine-grained frame-wise
alignment task, where temporally aligned features from the
same video are treated as positives, and unaligned ones neg-
atives [19]. Since phonemes can appear repeatedly within
the same video clip, leading to false negatives, we use a
negative margin following Park et al. [47], which alleviates
the effect of falsely matched pairs by enforcing a looser de-
cision boundary. The objective function involved is:
LNCE(H) =−1
nnX
i=1loge(Hi,i−m)/τ
e(Hi,i−m)/τ+P
i̸=jeHi,j/τ,
(12)
where His a matrix whose (i, j)-th element represents the
cosine similarity between L2-normalized audio and visual
features at time step iandj, respectively, τis a learnable
temperature parameter, and mis a pre-determined margin.
To promote the learning of shared features between the two
modalities, we apply a symmetrized version of the above
NCE loss as:
La↔v=1
2(La→v+Lv→a) =1
2(LNCE(Sav)+LNCE(Sva)).
(13)
Here the similarity matrices Sav=⟨Fa, Fv⟩/(||Fa||·||Fv||)
andSva=S⊤
avare computed with features FaandFvfrom
modality-specific encoders φaandφvinstead of the con-
textual features from the Transformer encoder θto avoid
learning positional shortcuts. The total loss in stage 1 is
Lstage1 =La→a′+La↔v. (14)
Stage 2: Learning video-unique information. In stage
2, we proceed to learn video-unique information, which is
now much easier than learning from scratch since it has
been bootstrapped with shared knowledge from the audio in
the previous stage. The audio encoder φadoes not receive
any gradients in this stage and is essentially frozen, since
it is expected to have already acquired the audio-specific
knowledge well in the previous stage. We adopt a similar
distillation loss as stage 1:
Lv→v′=Ldistill(Ev, E′
v). (15)
On top of the learned visual-unique information, we also
perform distillation from unmasked video features to the
joint features of unmasked audio-visual inputs in this stage
to prepare for the learning of the final robust audio-visual
representations in the next stage. For this purpose, the loss
in this stage is set as:
Lav→v=Ldistill(Ev, Eav), (16)
Lstage2 =Lv→v′+Lav→v. (17)
To avoid repeated learning of only shared information be-
tween A and V again, the cross-modal loss Lav→vis asym-
metric, strictly one-way from A V to V , and a learnable tem-perature τvEq. (16) is introduced to the visual student out-
puts, i.e., we replace ψkin Eq. (9) with softmax activations
oftemperature-scaled linear projections of Ev.
Stage 3: Learning synergistic information. Finally, we
aim to learn synergistic audio-visual information which pre-
vious methods have not explicitly addressed. Our ultimate
goal is to maximize total task-relevant multi-modal infor-
mation I(Eav;Y),i.e. the sum of R,Ua,UvandS, so
that the representations are useful for both uni-modal and
multi-modal tasks. Na ¨ıvely maximizing this objective with-
out prior stages is problematic because the network tends
to be biased towards learning audio-based representations
(i.e. prioritizing RandUa) without considering the visual
modality. By learning most of R,UaandUvthrough the
previous stages, our method is less susceptible to learning
biased representations in the final stage. At this stage, we al-
ways mask identical positions in the audio and video inputs
together so that the masked region never contains any uni-
modal information that can collapse or degrade training. To
better recover the masked inputs, the model must cleverly
leverage both modalities, which requires learning synergis-
tic information that is not well-captured in prior stages. The
loss function in stage 3 can be written as:
Lstage3 =Lav→av′=Ldistill(E′
av, Eav). (18)
By progressively acquiring the three types of useful infor-
mation, we build strong and robust audio-visual speech rep-
resentations for both uni- and bi-modal speech, which we
shall show in subsequent experiments.
3.5. Fine-tuning Stage
After pre-training, we add an extra linear layer after the
fusion encoder θto fine-tune the model end-to-end with
the Lattice-Free Maximum Mutual Information (LF-MMI)
criterion [26, 51], which is much more lightweight than
seq2seq Encoder-Decoder models and allows highly effi-
cient decoding. We adopt a “compact” [33] 1-state hid-
den Markov model (HMM) topology, using k22for differ-
entiable Weighted Finite State Transducer (WFST)-based
training. We use 624bi-char units from text-based cluster-
ing [77] for English and tonal phone-level targets (initials
and finals) for Chinese.
4. Experiments
4.1. Datasets
LRS2-BBC [3]. LRS2-BBC is a 224-hour transcribed
audio-visual English speech dataset collected from pro-
grams of various genres broadcast by the BBC. It is chal-
lenging due to the presence of extreme poses, as well as a
large and diverse vocabulary. We use the official train / val-
idation split (see supplementary materials) for evaluation.
2https://github.com/k2-fsa/k2
27073
LRS3-TED [1]. LRS3-TED is another audio-visual En-
glish speech dataset collected from TED and TEDx talks,
with over 5.5k speakers and 439hours of data. It covers
diverse topics and is widely adopted for evaluating audio-
visual speech representation learning methods for being the
largest publicly available one of its kind. An official train /
validation split is not given, so we use the widely adopted
one prepared by Shi et al. [59], same as [27, 34, 80].
CAS-VSR-S101. We also collect a new large-scale, in-the-
wild Mandarin dataset with 101.1hours of data. The videos
are sourced from broadcast news and conversational pro-
grams in Chinese, covering a highly diverse set of topics,
speakers and filming conditions (see supplementary).
4.2. Experimental Setup
More implementation details can be found in the supple-
mentary materials.
High- and low-resource setups. On LRS3-TED we fol-
low the widely adopted setup of Shi et al. [59], which ex-
cludes the pre-train set during low-resource fine-tuning.
To validate data scaling properties, previous works used
V oxCeleb2-En [59], which is a 1326 -hour subset of V ox-
Celeb2 [18] in English. However, not only are experiments
of this scale computationally expensive, but more impor-
tantly, V oxCeleb2 is no longer available from its official
source as of Feb. 2023. Therefore, we create a similar
scaling protocol, using LRS3-TED (433h) as a pre-training
source and LRS2-BBC (223h) for fine-tuning. For low-
resource fine-tuning, we only use the 28-hour train sub-
set of LRS2-BBC, while for high-resource fine-tuning, we
include the pre-train subset, similar to [17].
Evaluation. Decoding is performed using beam search
with a beam size of 50. The common evaluation metrics
for ASR, VSR and A VSR are Word Error Rate (WER) and
Character Error Rate (CER) for English and Chinese, re-
spectively. As our bi-char LF-MMI models are trained on
verbatim transcriptions for ease of lexicon construction, we
report WERs both without and with using Whisper-style
text normalization [54] (in parentheses) to compensate for
innocuous errors related to text normalization.
4.3. Results on LRS2-BBC and LRS3-TED
Low-resource setup. Low-resource results on LRS2-BBC
and LRS3-TED can be found in Tabs. 1 and 2. Our
smallest B ASE* encoder is modality-agnostic, only half the
size of the one in [27], which uses two separate encoders
to perform A VSR. On LRS3-TED, although we employ
the simple LF-MMI instead of cross-entropy (CE) based
seq2seq criterion for fine-tuning, our B ASE model rivals
or acheives SoTA in all cases. When scaling to a L ARGE
model, we observe significant improvements in both ASR
and A VSR. Meanwhile, VSR performance remains compa-
rable to SoTA, with a moderate 2%gap. This may sug-gest that with our current VGG-based visual encoder φv,
the benefits of scaling up the Transformer primarily mani-
fest in ASR and A VSR.
High-resource setup. High-resource results on LRS2-
BBC and LRS3-TED can be found in Tabs. 1 and 3. On
LRS2-BBC, with a 46M model and relying on its own
223h data for pre-training and fine-tuning, we achieve
30.3%/3.6%/2.9%WER for VSR, ASR and A VSR. No-
tably, these results surpass a highly competitive supervised
method [40] and other self-supervised methods that incor-
porate additional data. This indicates that our progres-
sive learning approach can effectively exploit visual in-
formation. Furthermore, when we introduce additional
433h unlabeled data, we achieve even better performance:
29.8%/3.0%/2.4%WER, which outperforms other meth-
ods that employ the same amount of data. When we switch
to the L ARGE model, performance rivals SoTA methods
that employ 4x data (1759h) for pre-training. Similarly, we
achieve or rival SoTA performance on LRS3-TED.
4.4. Results on CAS-VSR-S101
We also evaluate on our newly collected CAS-VSR-S101
dataset, which is smaller than both of the two previous
datasets and in an even more ambiguous language (Chinese
has more homophonous syllables and words than English).
As shown in Tab. 4, ES3still works out-of-the-box under
this challenging setup, and yields noticeable performance
gains over a strong fully supervised baseline [39]. Inter-
estingly, our fully supervised baseline model beats [39] in
ASR and A VSR but slightly lags behind in VSR, again hint-
ing that our strategy can further benefit from more complex
visual modeling architectures.
Table 4. Results on CAS-VSR-S101. Here we use a B ASE*
model for the fully-supervised baseline and ES3. The self-
supervised models are pre-trained on training data and then fine-
tuned for 50epochs with the labels. The numbers before and after
slashes correspond to CERs on the validation and test sets.
Methods Criterion Encoder Size #Epochs VSR ASR A VSR
Supervised
Baseline LF-MMI 46M 100 60.0 / 57.4 15.4 / 12.6 14.6 / 12.1
E2E Conformer [39] CTC+CE 79M 100 58.6 / 56.5 16.6 / 13.0 16.0 / 12.6
Self-supervised
ES3Stage 1 61.2 / 59.0 14.7 / 11.9 -
ES3Stage 2 LF-MMI 50 59.1 / 56.7 - 13.9 / 11.4
ES3Stage 346M
58.1 /55.6 14.4 / 11.6 13.4 / 11.0
4.5. Analysis and Discussion
Representation quality probing. In Tab. 5, we demon-
strate the effect of each stage on LRS2-BBC. ASR per-
formance between stages 1 and 3 are close, indicating that
stage 1 indeed acquires most of audio-unique information.
Comparing the VSR results between stages 1 and 2 reveals
a significant gap, suggesting that stage 2 also effectively
learns visual-unique information, hence the minimal dif-
ferences between the final VSR result and those of stage
2. Moreover, comparing the results of stage 1-2 and stage
27074
Table 1. Results on LRS2-BBC. We pre-train a B ASE* model with 223h unlabeled data from LRS2-BBC, as well as B ASE and L ARGE
models with 433h unlabeled data (LRS3-TED) to demonstrate scaling properties.∗: external language models.
Methods Unlabeled A V data Labeled Data Encoder Size Criterion VSR ASR A VSR
Supervised
Yu et al. [75] - 1519h - LF-MMI148.9 6.7 5.9
Ma et al. [39]∗- 380h 79M CTC+CE 37.9 3.9 3.7
Ma et al. [41]∗- 818h 186M CTC+CE 27.9 2.6 -
Semi-supervised
Afouras et al. [2]∗777h 223h2- CTC 51.3 - -
Ma et al. [41]∗2630h 818h 186M CTC+CE 14.6 1.5 1.5
Self-supervised ( BASE andBASE* models)
A V-HuBERT [59, 60] 1759h 223h 103M CE 31.23- 3.63
VATLM [80] 1759h4223h 107M CE 30.6 - 2.9
RA VEn [27] 433h 223h 97M CTC+CE 32.1 3.9 -
Pan et al. [45] -5380h 399M CTC+CE 43.2 2.7 2.6
223h 28h 46M 40.2 (39.1) 6.0 (5.0) 5.7 (4.9)
433h 28h 102M 39.3 (38.2) 5.5 (4.8) 5.1 (4.1)
223h 223h 46M 31.4 (30.3) 4.3 (3.6) 3.8 (2.9)ES3(ours)
433h 223h 102MLF-MMI
30.7 (29.8) 3.4 (3.0) 3.2 (2.4)
Self-supervised ( LARGE models)
433h 28h 317M 36.4 (35.4) 5.2 (4.4) 4.7 (4.0)ES3(ours)433h 223h 317MLF-MMI26.7 (25.8) 3.1 (2.5) 3.1 (2.5)
A V-HuBERT [59, 60] 1759h 223h 325M CE 25.53- 2.53
VATLM [80] 1759h4223h 332M CE 24.3 - 2.3
RA VEn [27] 1759h 223h 671M CTC+CE 23.2 2.5 -
1Not end-to-end; requires a GMM-HMM alignment stage.2Uses an additional ASR model trained on LibriSpeech (960h).
3Reproduced by Zhu et al. [80].4Uses additional 3846h audio, 452h audio-text and 600M text data.
5Uses additional 60000h audio data and 1.28M unlabeled images.
Table 2. Low-resource results on LRS3-TED. We pre-train a B ASE and L ARGE model with 433h unlabeled data.
Methods Unlabeled A V data Labeled Data Encoder Size Criterion VSR ASR A VSR
Self-supervised ( BASE models)
A V-HuBERT [59, 60] 433h 30h 103M CE 51.8 4.9 4.71
VATLM [80] 433h230h 107M CE 48.0 - 3.6
RA VEn [27] 433h 30h 97M CTC+CE 47.0 4.7 -
A V2vec [76]3433h 30h 103M CE 45.0 - 5.8
A V-data2vec [34] 433h 30h 103M CE 45.2 4.4 4.2
ES3(ours) 433h 30h 102M LF-MMI 45.5 (44.7) 3.9 (3.3) 3.6 (3.0)
Self-supervised ( LARGE models)
A V-HuBERT 433h 30h 325M CE 44.8 4.5 4.21
A V-data2vec 433h 30h 325M CE 40.5 3.7 3.4
ES3(ours) 433h 30h 317M LF-MMI 43.5 (42.5) 3.8 (2.9) 2.9 (2.3)
1Reproduced by Lian et al. [34].2Uses additional 3846h audio, 452h audio-text and 600M text data.
3Zhang et al. [76] inject noise during pre-training and obtain better fine-tuning results even with its base model A V-HuBERT ( 47.1%).
3 demonstrates that the joint information learning in stage
3, which also incorporates synergistic information, benefits
both uni-modal (ASR & VSR) and bi-modal (A VSR) tasks.
Similar trends can also be observed on the more challenging
Mandarin dataset CAS-VSR-S101 in Tab. 4, further sup-
porting the motivation of our proposed strategy.
Table 5. Representation quality probing on LRS2-BBC. Results
are reported on the validation set using the B ASE* model under the
low-resource setup for each pre-training stage.
Pre-training Stage VSR ASR A VSR
Stage 1 58.3 (57.3) 11.5 (10.1) -
Stage 2 48.7 (47.3) - 12.9 (11.5)
Stage 3 (Full ES3)48.1 (46.9) 11.4 (9.8) 10.1 (8.6)
Effect of automatically balancing shared and unique in-
formation learning. It is known that in a hardness-aware
loss function, the temperature rules the strength of penal-
ties on hard negative samples [72]. We track the automati-
cally learned temperature parameter τused for cross-modallearning objectives La↔vin Eq. (12) and Lav→vin Eq. (16)
when pre-training on LRS2-BBC, as shown in Fig. 2. In
stage 1, τcontinues to fall until it reaches τmin, while in
stage 2 it briefly drops and then rises throughout training.
The trend is in line with our intuition: in stage 1, audio in-
formation is easily learnable, so the model can fully exploit
shared information by lowering τwithout sacrificing audio-
unique information learning; in stage 2, after learning some
residual shared information, an increasing τprogressively
relaxes Lav→vto yield more freedom to Lv→v′,i.e. video-
unique information learning.
0k 25k 50k 75k 100k 125k 150k 175k 200k
Training Step0.0250.0500.0750.1000.125TemperatureStage 1
Stage 2
Figure 2. The temperature parameter paces learning automatically.
27075
Table 3. High-resource results on LRS3-TED. We fine-tune on 433h labeled data of LRS3.†: uses test-time augmentation.∗: uses
external language models.‡: noise injection during pre-training. We list more methods for comparison in the supplementary material.
Methods Year Unlabeled A V data Labeled Data Backbone Encoder Size Criterion VSR ASR A VSR
Supervised
Ma et al. [39]∗2021 - 595h Conformer 79M CTC+CE 43.3 2.3 2.3
Prajwal et al. [52]†∗2022 - 698h Transformer 32M CE 40.6 - -
Ma et al. [40]∗2022 - 818h Conformer 79M CTC+CE 34.7 - -
Semi-Supervised
Shillingford et al. [62]∗2019 - 3886h RNN - CTC 55.1 - -
Makino et al. [42] 2019 - 31kh RNN 43M Transducer 33.6 4.8 4.5
Afouras et al. [2]∗2020 344h 433h1Jasper (CNN) - CTC 59.8 - -
Serdyuk et al. [57] 2021 - 90kh Transformer - Transducer 25.9 - 2.3
Ma et al. [40]∗2022 641h 818h Conformer 79M CTC+CE 31.5 - -
Serdyuk et al. [58] 2022 - 90kh Conformer - Transducer 17.0 1.6 1.6
Ma et al. [41]∗2023 2630h 818h Conformer 186M CTC+CE 19.1 1.0 0.9
Chang et al. [15] 2023 - 100kh Conformer 98M Transducer 12.8 - 0.9
Self-supervised (Base Models)
A V-HuBERT [59, 60] 2022 433h 433h Transformer 103M CE 44.0 3.0 2.82
A V2vec [76]‡2023 433h 433h Transformer 103M CE 39.9 - 2.6
RA VEn [27] 2023 433h 433h Transformer 97M CTC+CE 39.1 2.2 -
ES3(ours) 2023 433h 433h Transformer 102M LF-MMI 40.3 (39.2) 2.9 (2.4) 2.5 (2.0)
A V-data2vec [34] 2023 433h 433h Transformer 103M CE 39.0 2.0 1.8
Self-supervised (Large Models)
A V-HuBERT [59, 60] 2022 433h 433h Transformer 325M CE 41.6 2.7 2.52
A V-data2vec [34] 2023 433h 433h Transformer 325M CE 37.4 1.9 1.7
ES3(ours) 2023 433h 433h Transformer 317M LF-MMI 37.1 (36.7) 2.8 (2.1) 2.1 (1.7)
1Uses an additional ASR model trained on LibriSpeech (960h).2Reproduced by Lian et al. [34].
Visualization of our learned psuedo-label sequences. We
randomly select a sample from the LRS3-TED validation
set and visualize the pseudo-label sequences, i.e. code in-
dices qgenerated by the audio, visual and audio-visual
EMA teacher ˜θin each stage. Ground truth phones for each
audio frame (10ms) are derived by forced alignment [79].
Fig. 3 shows that our learned codebook yields high-quality
pseudo-labels that strongly correlate with phonemic units.
120
75109
250218
183243
196204
149163
21933
317
222104
713S1 A:
178
212100
1256
56111
142143
226225
15637
23987
5772
719
157251S2 V:
164
5985
3624
137223
221241
13358
103191
15845S3 AV:
[SIL]
SOW
WAH
TD
AHZ
DHAE
TM
IYN
[SIL]Phones:
[SIL]
sowhat
doesthat
mean[SIL]Words:
0.0 0.2 0.4 0.6 0.8 1.0
Time (s)
Figure 3. Pseudo-label sequences from each stage. Example of
pseudo-labels generated from the last Transformer layer for the ut-
terance “so what does that mean” by the audio, visual, and audio-
visual teachers in each stage (B ASE model). At the bottom we
show the spectrogram for reference. Please note that our pseudo-
labels are derived at 40ms intervals, limited by video frame rate.
Training efficiency comparison with SoTA methods.
Learning audio-visual speech representations has been ex-
tremely compute-intensive. Here we briefly compare
ES3with prominent SoTA methods. Pre-training a
BASE/LARGE model on LRS3-TED using our ES3for 100
epochs per stage only takes about 6/8.3 days in total with
just 8 NVIDIA A40 GPUs. In contrast, A V-data2vec is
trained for 1000k steps using 64 NVIDIA V100 GPUs
for 4-5/6-7 days using B ASE/LARGE with a batch size
of 1280/2560s, or equivalently, 821/1642 epochs. They
also experiment extensively with hyperparameters such as
modality schedulers, which further increases the cost oftraining. RA VEn performs an equivalent of 6.25/50 days of
training on 8 A100s with B ASE*/LARGE . It uses separate
encoders for audio and video, so the resulting model can-
not perform audio-visual fusion out-of-the-box. This also
requires keeping track of four sets of parameters, which is
very memory-intensive. By focusing on designing a clever
learning strategy, we circumvent such manual tuning alto-
gether and greatly reduce training costs. Finally, we use the
simple LF-MMI criterion, and expect better performance
from tuning beam size, including a larger external language
model (see supplementary), or switching to the common
heavy Attention-based Encoder-Decoder model.
5. Conclusions
We present a novel strategy, ES3, for self-supervised learn-
ing of audio-visual speech representations. Unlike exist-
ing methods that primarily focus on learning information
shared between audio and video, we take an “evolving” ap-
proach that also captures unique and synergistic informa-
tion. Our experimental results demonstrate that ES3indeed
progressively learns unique, shared and synergistic infor-
mation. Moreover, in terms of downstream performance,
it rivals SoTA methods in terms of ASR, VSR and A VSR
accuracy without burdensome manual balancing efforts.
Acknowledgments. This work is partially supported by
the National Natural Science Foundation of China (No.
62276247, 62076250). We are grateful to the ICT com-
puting platform, the Nanjing Institute of InforSuperBahn,
and Capitalonline Data Service Co., Ltd for providing ad-
ditional compute resources. We also thank Chenhao Wang,
Mingmin Yang and Jingyun Xiao for helping to prepare
CAS-VSR-S101, Bingquan Xia and Songtao Luo for help
with Conformer and seq2seq baselines, Bingquan Xia and
Feixiang Wang for help during the rebuttal phase, and Feix-
iang Wang and Zhaoxin Yuan for an early version of Fig. 1.
27076
References
[1] Triantafyllos Afouras, Joon Son Chung, and Andrew Zis-
serman. LRS3-TED: a large-scale dataset for visual speech
recognition. CoRR , abs/1809.00496, 2018. 6
[2] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisser-
man. ASR is all you need: Cross-modal distillation for lip
reading. In ICASSP , pages 2143–2147. IEEE, 2020. 2, 7, 8,
4, 5
[3] Triantafyllos Afouras, Joon Son Chung, Andrew W. Senior,
Oriol Vinyals, and Andrew Zisserman. Deep audio-visual
speech recognition. IEEE Trans. Pattern Anal. Mach. Intell. ,
44(12):8717–8727, 2022. 1, 5, 4
[4] Jean-Baptiste Alayrac, Adri `a Recasens, Rosalia Schneider,
Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lu-
cas Smaira, Sander Dieleman, and Andrew Zisserman. Self-
supervised multimodal versatile networks. In NeurIPS , 2020.
2
[5] Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo
Torresani, Bernard Ghanem, and Du Tran. Self-supervised
learning by cross-modal audio-video clustering. In NeurIPS ,
2020. 2
[6] Relja Arandjelovic and Andrew Zisserman. Look, listen and
learn. In ICCV , pages 609–617. IEEE Computer Society,
2017. 2
[7] Relja Arandjelovic and Andrew Zisserman. Objects that
sound. In ECCV (1) , pages 451–466. Springer, 2018. 2
[8] Yuki Markus Asano, Mandela Patrick, Christian Rupprecht,
and Andrea Vedaldi. Labelling unlabelled videos from
scratch with multi-modal self-supervision. In NeurIPS ,
2020. 2
[9] Alexei Baevski, Michael Auli, and Abdelrahman Mo-
hamed. Effectiveness of self-supervised pre-training for
speech recognition. CoRR , abs/1911.03912, 2019. 1
[10] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and
Michael Auli. wav2vec 2.0: A framework for self-supervised
learning of speech representations. In NeurIPS , 2020. 3
[11] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu,
Jiatao Gu, and Michael Auli. data2vec: A general framework
for self-supervised learning in speech, vision and language.
InICML , pages 1298–1312. PMLR, 2022. 1
[12] Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael
Auli. Efficient self-supervised learning with contextualized
target representations for vision, speech and language. In
ICML , pages 1416–1429. PMLR, 2023. 1, 3
[13] Adrian Bulat and Georgios Tzimiropoulos. How far are we
from solving the 2D & 3D face alignment problem? (and
a dataset of 230,000 3D facial landmarks). In ICCV , pages
1021–1030. IEEE Computer Society, 2017. 2
[14] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision Transformers. In
ICCV , pages 9630–9640. IEEE, 2021. 1
[15] Oscar Chang, Hank Liao, Dmitriy Serdyuk, Ankit Shahy,
and Olivier Siohan. Conformer is all you need for visualspeech recognition. In ICASSP , pages 10136–10140. IEEE,
2024. 1, 8, 5
[16] Brian Chen, Andrew Rouditchenko, Kevin Duarte, Hilde
Kuehne, Samuel Thomas, Angie W. Boggust, Rameswar
Panda, Brian Kingsbury, Rog ´erio Feris, David Harwath,
James R. Glass, Michael Picheny, and Shih-Fu Chang.
Multimodal clustering networks for self-supervised learning
from unlabeled videos. In ICCV , pages 7992–8001. IEEE,
2021. 2
[17] Xize Cheng, Tao Jin, Linjun Li, Wang Lin, Xinyu Duan,
and Zhou Zhao. OpenSR: Open-modality speech recogni-
tion via maintaining multi-modality alignment. In ACL (1) ,
pages 6592–6607. Association for Computational Linguis-
tics, 2023. 6
[18] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.
V oxCeleb2: Deep speaker recognition. In INTERSPEECH ,
pages 1086–1090. ISCA, 2018. 6
[19] Soo-Whan Chung, Joon Son Chung, and Hong-Goo Kang.
Perfect match: Improved cross-modal embeddings for audio-
visual synchronisation. In ICASSP , pages 3965–3969. IEEE,
2019. 5
[20] Tri Dao. FlashAttention-2: Faster attention with better paral-
lelism and work partitioning. CoRR , abs/2307.08691, 2023.
2
[21] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and
Christopher R ´e. FlashAttention: Fast and memory-efficient
exact attention with IO-awareness. In NeurIPS , 2022. 2
[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-
formers for language understanding. In NAACL-HLT (1) ,
pages 4171–4186. Association for Computational Linguis-
tics, 2019. 1, 2
[23] Angela Fan, Edouard Grave, and Armand Joulin. Reducing
Transformer depth on demand with structured dropout. In
ICLR . OpenReview.net, 2020. 2
[24] Dalu Feng, Shuang Yang, and Shiguang Shan. An efficient
software for building lip reading models without pains. In
ICME Workshops , pages 1–2. IEEE, 2021. 1
[25] Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David
Harwath, Leonid Karlinsky, Hilde Kuehne, and James R.
Glass. Contrastive audio-visual masked autoencoder. In
ICLR . OpenReview.net, 2023. 2
[26] Hossein Hadian, Hossein Sameti, Daniel Povey, and Sanjeev
Khudanpur. End-to-end speech recognition using lattice-free
MMI. In INTERSPEECH , pages 12–16. ISCA, 2018. 5
[27] Alexandros Haliassos, Pingchuan Ma, Rodrigo Mira, Stavros
Petridis, and Maja Pantic. Jointly learning visual and audi-
tory speech representations from raw data. In ICLR . Open-
Review.net, 2023. 2, 6, 7, 8, 1, 4, 5
[28] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross B. Girshick. Masked autoencoders are scal-
able vision learners. In CVPR , pages 15979–15988. IEEE,
2022. 2
[29] Franc ¸ois Hernandez, Vincent Nguyen, Sahar Ghannay, Na-
talia A. Tomashenko, and Yannick Est `eve. TED-LIUM
3: Twice as much data and corpus repartition for experi-
ments on speaker adaptation. In SPECOM , pages 198–208.
Springer, 2018. 2
27077
[30] Wei-Ning Hsu and Bowen Shi. u-HuBERT: Unified mixed-
modal speech pretraining and zero-shot transfer to unlabeled
modality. In NeurIPS , 2022. 1
[31] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman
Mohamed. HuBERT: Self-supervised speech representation
learning by masked prediction of hidden units. IEEE ACM
Trans. Audio Speech Lang. Process. , 29:3451–3460, 2021. 1
[32] Bruno Korbar, Du Tran, and Lorenzo Torresani. Coopera-
tive learning of audio and video models from self-supervised
synchronization. In NeurIPS , pages 7774–7785, 2018. 2
[33] Aleksandr Laptev, Somshubra Majumdar, and Boris Gins-
burg. CTC variations through new WFST topologies. In
INTERSPEECH , pages 1041–1045. ISCA, 2022. 5
[34] Jiachen Lian, Alexei Baevski, Wei-Ning Hsu, and Michael
Auli. A V-data2vec: Self-supervised learning of audio-visual
speech representations with contextualized target representa-
tions. In ASRU . IEEE, 2023. 2, 6, 7, 8, 1, 5
[35] Paul Pu Liang, Zihao Deng, Martin Q. Ma, James Y . Zou,
Louis-Philippe Morency, and Ruslan Salakhutdinov. Factor-
ized contrastive learning: Going beyond multi-view redun-
dancy. In NeurIPS , 2023. 2
[36] Alexander H. Liu, Heng-Jui Chang, Michael Auli, Wei-Ning
Hsu, and Jim Glass. DinoSR: Self-distillation and online
clustering for self-supervised speech representation learning.
InNeurIPS , 2023. 4, 2
[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR (Poster) . OpenReview.net, 2019. 2
[38] Pingchuan Ma, Rodrigo Mira, Stavros Petridis, Bj ¨orn W.
Schuller, and Maja Pantic. LiRA: Learning visual speech
representations from audio through self-supervision. In In-
terspeech , pages 3011–3015. ISCA, 2021. 2
[39] Pingchuan Ma, Stavros Petridis, and Maja Pantic. End-to-
end audio-visual speech recognition with Conformers. In
ICASSP , pages 7613–7617. IEEE, 2021. 1, 6, 7, 8, 4, 5
[40] Pingchuan Ma, Stavros Petridis, and Maja Pantic. Visual
speech recognition for multiple languages in the wild. Nat.
Mac. Intell. , 4(11):930–939, 2022. 1, 6, 8, 2, 4, 5
[41] Pingchuan Ma, Alexandros Haliassos, Adriana Fernandez-
Lopez, Honglie Chen, Stavros Petridis, and Maja Pantic.
Auto-A VSR: Audio-Visual speech recognition with auto-
matic labels. In ICASSP , pages 1–5. IEEE, 2023. 1, 3, 7,
8, 4, 5
[42] Takaki Makino, Hank Liao, Yannis M. Assael, Brendan
Shillingford, Basilio Garcia, Otavio Braga, and Olivier Sio-
han. Recurrent neural network transducer for audio-visual
speech recognition. In ASRU , pages 905–912. IEEE, 2019.
1, 8, 5
[43] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
HowTo100M: Learning a text-video embedding by
watching hundred million narrated video clips. In ICCV ,
pages 2630–2640. IEEE, 2019. 2
[44] Arsha Nagrani, Chen Sun, David Ross, Rahul Sukthankar,
Cordelia Schmid, and Andrew Zisserman. Speech2Action:
Cross-modal supervision for action recognition. In CVPR ,
pages 10314–10323. Computer Vision Foundation / IEEE,
2020. 2[45] Xichen Pan, Peiyu Chen, Yichen Gong, Helong Zhou,
Xinbing Wang, and Zhouhan Lin. Leveraging unimodal
self-supervised learning for multimodal audio-visual speech
recognition. In ACL (1) , pages 4491–4503. Association for
Computational Linguistics, 2022. 7, 4
[46] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng
Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V . Le.
SpecAugment: A simple data augmentation method for au-
tomatic speech recognition. In INTERSPEECH , pages 2613–
2617. ISCA, 2019. 2
[47] Sooyoung Park, Arda Senocak, and Joon Son Chung.
MarginNCE: Robust sound localization with a negative mar-
gin. In ICASSP , pages 1–5. IEEE, 2023. 5, 2
[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
ing Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-
son, Andreas K ¨opf, Edward Z. Yang, Zachary DeVito, Mar-
tin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Py-
Torch: An imperative style, high-performance deep learning
library. In NeurIPS , pages 8024–8035, 2019. 2
[49] Mandela Patrick, Yuki Markus Asano, Polina Kuznetsova,
Ruth Fong, Jo ˜ao F. Henriques, Geoffrey Zweig, and Andrea
Vedaldi. On compositions of transformations in contrastive
self-supervised learning. In ICCV , pages 9557–9567. IEEE,
2021. 2
[50] Trung Pham, Chaoning Zhang, Axi Niu, Kang Zhang, and
Chang D. Yoo. On the pros and cons of momentum encoder
in self-supervised visual representation learning. CoRR ,
abs/2208.05744, 2022. 4
[51] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah
Ghahremani, Vimal Manohar, Xingyu Na, Yiming Wang,
and Sanjeev Khudanpur. Purely sequence-trained neural
networks for ASR based on lattice-free MMI. In INTER-
SPEECH , pages 2751–2755. ISCA, 2016. 5
[52] K. R. Prajwal, Triantafyllos Afouras, and Andrew Zisser-
man. Sub-word level lip reading with visual attention. In
CVPR , pages 5152–5162. IEEE, 2022. 8, 4, 5
[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , pages
8748–8763. PMLR, 2021. 2
[54] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,
Christine McLeavey, and Ilya Sutskever. Robust speech
recognition via large-scale weak supervision. In ICML ,
pages 28492–28518. PMLR, 2023. 6
[55] Jordi Robert-Ribes, Jean-Luc Schwartz, Tahar Lallouache,
and Pierre Escudier. Complementarity and synergy in bi-
modal speech: Auditory, visual, and audio-visual identifi-
cation of french oral vowels in noise. The Journal of the
Acoustical Society of America , 103(6):3677–3689, 1998. 1
[56] Steffen Schneider, Alexei Baevski, Ronan Collobert, and
Michael Auli. wav2vec: Unsupervised pre-training for
speech recognition. In INTERSPEECH , pages 3465–3469.
ISCA, 2019. 3
27078
[57] Dmitriy Serdyuk, Otavio Braga, and Olivier Siohan. Audio-
visual speech recognition is worth 32×32×8voxels. In
ASRU , pages 796–802. IEEE, 2021. 2, 8, 5
[58] Dmitriy Serdyuk, Otavio Braga, and Olivier Siohan.
Transformer-based video front-ends for audio-visual speech
recognition for single and multi-person video. In INTER-
SPEECH , pages 2833–2837. ISCA, 2022. 2, 8, 5
[59] Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrah-
man Mohamed. Learning audio-visual speech representation
by masked multimodal cluster prediction. In ICLR . OpenRe-
view.net, 2022. 1, 2, 3, 6, 7, 8, 4, 5
[60] Bowen Shi, Wei-Ning Hsu, and Abdelrahman Mohamed.
Robust self-supervised audio-visual speech recognition. In
INTERSPEECH , pages 2118–2122. ISCA, 2022. 7, 8, 4, 5
[61] Bowen Shi, Abdelrahman Mohamed, and Wei-Ning Hsu.
Learning lip-based audio-visual speaker embeddings with
A V-HuBERT. In INTERSPEECH , pages 4785–4789. ISCA,
2022. 1
[62] Brendan Shillingford, Yannis M. Assael, Matthew W. Hoff-
man, Thomas Paine, C ´ıan Hughes, Utsav Prabhu, Hank Liao,
Hasim Sak, Kanishka Rao, Lorrayne Bennett, Marie Mul-
ville, Misha Denil, Ben Coppin, Ben Laurie, Andrew W. Se-
nior, and Nando de Freitas. Large-scale visual speech recog-
nition. In INTERSPEECH , pages 4135–4139. ISCA, 2019.
8, 5
[63] Nina Shvetsova, Brian Chen, Andrew Rouditchenko, Samuel
Thomas, Brian Kingsbury, Rog ´erio Feris, David Harwath,
James R. Glass, and Hilde Kuehne. Everything at once
- multi-modal fusion Transformer for video retrieval. In
CVPR , pages 19988–19997. IEEE, 2022. 2
[64] Themos Stafylakis and Georgios Tzimiropoulos. Combining
residual networks with LSTMs for lipreading. In INTER-
SPEECH , pages 3652–3656. ISCA, 2017. 3
[65] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng
Liu. RoFormer: Enhanced transformer with rotary position
embedding. CoRR , abs/2104.09864, 2021. 3
[66] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
Wei, and Jifeng Dai. VL-BERT: Pre-training of generic
visual-linguistic representations. In ICLR . OpenReview.net,
2020. 2
[67] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and
Cordelia Schmid. VideoBERT: A joint model for video and
language representation learning. In ICCV , pages 7463–
7472. IEEE, 2019. 2
[68] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
trastive multiview coding. In ECCV (11) , pages 776–794.
Springer, 2020. 2
[69] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,
Gabriel Synnaeve, and Herv ´e J´egou. Going deeper with im-
age Transformers. In ICCV , pages 32–42. IEEE, 2021. 2
[70] Kristin J Van Engen, Avanti Dey, Mitchell S Sommers, and
Jonathan E Peelle. Audiovisual speech perception: Moving
beyond mcgurk. The Journal of the Acoustical Society of
America , 152(6):3216–3225, 2022. 1
[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NIPS , pages 5998–
6008, 2017. 3, 2[72] Feng Wang and Huaping Liu. Understanding the behaviour
of contrastive loss. In CVPR , pages 2495–2504. Computer
Vision Foundation / IEEE, 2021. 7
[73] Paul L. Williams and Randall D. Beer. Nonnegative decom-
position of multivariate information. CoRR , abs/1004.2515,
2010. 3
[74] Bo Xu, Cheng Lu, Yandong Guo, and Jacob Wang. Discrim-
inative multi-modality speech recognition. In CVPR , pages
14421–14430. Computer Vision Foundation / IEEE, 2020. 5
[75] Jianwei Yu, Shi-Xiong Zhang, Jian Wu, Shahram Ghorbani,
Bo Wu, Shiyin Kang, Shansong Liu, Xunying Liu, Helen
Meng, and Dong Yu. Audio-visual recognition of overlapped
speech for the LRS2 dataset. In ICASSP , pages 6984–6988.
IEEE, 2020. 7, 4
[76] Jing-Xuan Zhang, Genshun Wan, Zhen-Hua Ling, Jia Pan,
Jianqing Gao, and Cong Liu. Self-supervised audio-
visual speech representations learning by multimodal self-
distillation. In ICASSP , pages 1–5. IEEE, 2023. 2, 7, 8, 1,
5
[77] Xiaohui Zhang, Vimal Manohar, David Zhang, Frank Zhang,
Yangyang Shi, Nayan Singhal, Julian Chan, Fuchun Peng,
Yatharth Saraf, and Mike Seltzer. On lattice-free boosted
MMI training of HMM and CTC-based full-context ASR
models. In ASRU , pages 1026–1033. IEEE, 2021. 5
[78] Ya Zhao, Rui Xu, Xinchao Wang, Peng Hou, Haihong Tang,
and Mingli Song. Hearing lips: Improving lip reading by
distilling speech recognizers. In AAAI , pages 6917–6924.
AAAI Press, 2020. 2
[79] Jian Zhu, Cong Zhang, and David Jurgens. Phone-to-audio
alignment without text: A semi-supervised approach. In
ICASSP , pages 8167–8171. IEEE, 2022. 8
[80] Qiushi Zhu, Long Zhou, Ziqiang Zhang, Shujie Liu, Binxing
Jiao, Jie Zhang, Lirong Dai, Daxin Jiang, Jinyu Li, and Furu
Wei. V ATLM: Visual-Audio-Text pre-training with unified
masked prediction for speech representation learning. IEEE
Transactions on Multimedia , pages 1–11, 2023. 1, 6, 7, 4, 5
27079
