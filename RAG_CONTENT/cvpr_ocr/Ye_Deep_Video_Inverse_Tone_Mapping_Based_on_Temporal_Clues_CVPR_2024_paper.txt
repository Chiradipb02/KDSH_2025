Deep Video Inverse Tone Mapping Based on Temporal Clues
Yuyao Ye1,Ning Zhang2, Yang Zhao3, Hongbin Cao1,4, Ronggang Wang1
1School of Electronic and Computer Engineering, Peking University2Baidu Netdisk
3School of Computer and Information, Hefei University of Technology4Bytedance Inc.
yeyuyao@pku.edu.cn zhangning34@baidu.com
yzhao@hfut.edu.cn caohongbin.181@bytedance.com rgwang@pkusz.edu.cn
Abstract
Inverse tone mapping (ITM) aims to reconstruct high
dynamic range (HDR) radiance from low dynamic range
(LDR) content. Although many deep image ITM methods
can generate impressive results, the ﬁeld of video ITM is
still to be explored. Processing video sequences by image
ITM methods may cause temporal inconsistency. Besides,
they aren’t able to exploit the potentially useful information
in the temporal domain. In this paper, we analyze the pro-
cess of video ﬁlming, and then propose a Global Sample
and Local Propagate strategy to better ﬁnd and utilize tem-
poral clues. To better realize the proposed strategy, we de-
sign a two-stage pipeline which includes modules named In-
cremental Clue Aggregation Module and Feature and Clue
Propagation Module. They can align and fuse frames effec-
tively under the condition of brightness changes and prop-
agate features and temporal clues to all frames efﬁciently.
Our temporal clues based video ITM method can recover
realistic and temporal consistent results with high ﬁdelity in
over-exposed regions. Qualitative and quantitative experi-
ments on public datasets show that the proposed method
has signiﬁcant advantages over existing methods. The code
is available at https://github.com/ye3why/VITM-TC/ .
1. Introduction
Recently, high dynamic range (HDR) technology has
elicited considerable interest due to its capability to pro-
vide more vivid visual experiences. But because of the lack
of HDR content, it’s highly demanded to convert existing
low dynamic range (LDR) content to HDR. High dynamic
range imaging (HDRI) methods use fusion algorithms to
combine multi-exposure sequences and remove the ghost
artifacts caused by misalignment. However, in most situa-
tions the multiple-exposure sequences are unavailable(e.g.,
single LDR images or videos on the Internet). Therefore,
inverse tone mapping (ITM) methods are designed to esti-
mate HDR radiance directly from single LDR content.
Figure 1. (a) A real-world LDR video on the Internet, the dynamic
range of frame 0 is too wide so that details outside the window are
lost. Fortunately, we can ﬁnd clues along time axis. The textures
from distant frames 81, 130, and 205 can be utilized to recover
over-exposed regions. (b) An LDR video with the corresponding
HDR ground truth from [ 9]. Compared to the SOTA image ITM
method [ 22], the proposed method can use temporal clues to re-
construct more realistic textures for frame t.
With the development of deep learning, a growing num-
ber of image ITM methods have been proposed, where
[7] [22] [26] [32] [38] learn an end-to-end model to re-
cover HDR from LDR straightforwardly and [ 8] [14] [18]
[19] [20] simulate the generation process of HDR images
and estimate the multi-exposure stack. Compared to the
highly sought-after image ITM, the ﬁeld of video ITM has
received little attention. Although The deep image ITM
methods can generate impressive results in restoring lost
textures, directly using it to process LDR videos may en-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25995
counter unexpected issues. On the one hand, these methods
can only estimate HDR radiance from spatial information
of current frame, ignoring any potentially valid information
in the temporal domain. On the other hand, it may also
lead to temporal inconsistencies such as ﬂicker. Xu et al.
[37] propose the ﬁrst deep video ITM method. However,
they simply employ 3D convolution on a sequence of adja-
cent frames and still fail to completely handle ﬂicker. The
SDRTV-to-HDRTV conversion task [ 15] [36] [36] has at-
tracted much attention, which aims to convert the standard
dynamic range video into HDR for displaying on advanced
televisions. However, most of them still focus on a single
frame and assume there is no severe over-exposure problem
in input content. In this paper, by analyzing the principles
of camera imaging, we believe that temporal information
can be investigated to reconstruct HDR videos with higher
quality.Upon that, we propose a novel video ITM pipeline
that can efﬁciently generate temporal consistent and realis-
tic HDR videos.
The dynamic range in the real world is so wide that even
the most advanced camera today can only record a part of
that brightness range. In order to capture the most important
information from a scene, cameras usually choose an appro-
priate exposure value based on the lighting conditions, the
subject and so on. As shown in Fig. 1(a), when shooting
the dim indoor scene, the exposure value is relatively high,
resulting in a severe over-exposure outside the window. As
the scene changes, the exposure value changes accordingly,
and the details outside the window are revealed. Thus, it
is possible that areas that are over-exposed in current frame
are exposed normally in other frames, providing valid infor-
mation for recovering lost textures in the current one. This
phenomenon is named as temporal clues in this paper. The
most straightforward ideas to leverage temporal clues are
using temporal models, such as sliding windows [ 34], 3D
convolution [ 37] or recurrent model [ 31]. However, such a
exposure changing process tends to be designed to be very
slow and smooth in order to ensure ﬁlmed video not ﬂicker,
which makes it difﬁcult to ﬁnd useful temporal clues. A
short sequence of adjacent frames may not cover valid in-
formation while a long one may lead to a signiﬁcant in-
crease in computational complexity and also makes it more
difﬁcult to ﬁnd and utilize temporal clues from long-term
information.
In this paper, we propose a novel video ITM pipeline
which can effectively and efﬁciently ﬁnd and leverage tem-
poral clues from a long sequence to recover natural textures
in over-exposed areas with high ﬁdelity. Speciﬁcally, we
propose a Global Sampling and Local Propagation strat-
egy, which ﬁrstly sample some reference frames from the
whole input sequence with a large stride, and extract use-
ful temporal clues from these reference frames to recover
key frames. And then propagate information in key framesto their neighbors. In order to get meaningful temporal
clues more effectively, we propose an Incremental Clue
Aggregation Module which can fuse temporal clues with
target key frames in a incremental way. In this module,
we design a ﬂow correction convolution to align frames
with different luminance under ITM circumstances and we
also utilize deformable convolution[ 5] and swin transformer
block[ 23] to align local and global features respectively. Af-
ter key frames are fused with temporal features, a BiRNN-
like module, namely Feature and Clue Propagation Mod-
ule is used to propagate features of these key frames to
their adjacent frames while ensuring temporal consistency.
Fig. 1(b) shows results from the state-of-the-art image ITM
method [ 22] and the proposed method. [ 22] can not re-
cover lost details by only using spatial information. On
the contrary, with the temporal clues from the entire se-
quence, the proposed method can utilize more information
and generate impressive results.In addition, due to the lack
of HDR video datasets, we design a novel method to synthe-
size HDR video dataset based on available HDR images and
LDR video datasets. Experimental results demonstrate that
the model trained with this synthetic dataset can achieve
good performance on several publicly available real-world
HDR video testsets.
In summary, this paper has the following main contribu-
tions:
(1) we analyze the temporal clues in LDR videos and
propose a novel video ITM pipeline with the Global Sam-
pling and Local Propagation strategy which can exploit tem-
poral clues effectively and recover over-exposed areas with
high ﬁdelity efﬁciently.
(2) We design a two-stage pipeline, it includes an Incre-
mental Clue Aggregation Module to align and fuse frames
with brightness changes under ITM problem. And it in-
cludes an efﬁcient Feature and Clue Propagation Module
which propagate features of the key frames to their neigh-
bors and generate temporally consistent results.
(3) we propose a novel dataset synthesis method to ob-
tain HDR video training dataset only using available HDR
images and LDR video dataset.
(4) Experiments demonstrate that the proposed method
outperforms the state-of-the-art methods both on quantita-
tive and visual quality.
2. Related Work
Multi-exposure HDR reconstruction HDRI technology
fuses the stack of multi-exposure images into the HDR ra-
diance. Recently, many deep-learning-based methods have
been proposed to generate ghost-free HDR images or videos
from multi-exposure sequences. Chen et al. [ 4] propose
a coarse-to-ﬁne framework for HDR video reconstruction
from alternating exposures based on optical ﬂow and de-
formable alignment. In contrast, we focus on reconstructing
25996
an HDR video from a single LDR video.
Single HDR image reconstruction There are many deep-
learning-based image ITM methods. The direct learning
method aims to generate the HDR radiance from an end-
to-end model. Eilertsen et al. [ 7] focus on restoring the
lost information in the saturated image areas by an end-to-
end network. Liu et al. [ 22] utilize the LDR image for-
mation pipeline to reverse it and reconstruct the HDR im-
age step by step. Zheng et al. [ 38] propose an ultra-high-
deﬁnition HDR reconstruction method via a collaborative
learning manner. The stack-based methods change the ex-
posure of the input image and estimate the multi-exposure
stack. Endo et al. [ 8] estimate LDR images with different
exposures by 3D convolution. Lee et al. [ 19] use the gen-
erative adversarial network (GAN) to recursively generate
the exposure-changed image. Kim et al. [ 14] propose a dif-
ferentiable HDR synthesis layer that forms the end-to-end
stack-based ITM network.
HDR video reconstruction Rempel et al. [ 29] propose
the video ITM method by contrast stretching and saturated
brightness enhancement. However, the lost details of the
over-exposed areas are not recovered. Recently, SDRTV-
HDRTV conversion has attracted much attention. Kim et al.
[15] joint handle the super-resolution and inverse tone map-
ping by a single model. Xu et al. [ 36] propose a frequency-
aware modulation network that can reduce the structural
distortions in the translated low-frequency regions. How-
ever, these methods usually deal with a single frame of
video and assume there are no severely over-exposed re-
gions. Xu et al. [ 37] use a 3D convolutional neural network
to perform the video ITM, which simply processes a batch
of adjacent frames, where the exposure settings are rela-
tively close and little available information can be found to
help the recovery. Different from the above methods, we
generate the linear luminance HDR video based on tempo-
ral clues.
3. Method
3.1. Overview
Given an input LDR video {Lt,t= 1...T}, our goal is to
reconstruct the corresponding HDR video {Ht,t= 1...T}.
As with [ 7][32], we focus on the reconstruction of the over-
exposed regions, which is the most difﬁcult part during
HDR conversion. Speciﬁcally, we fuse the linearized LDR
framef−1(Lt)and the output of the proposed method Ytby
a soft over-exposed mask αtto get the reconstructed result
ˆHt:
ˆHt= (1−αt)f−1(Lt)+αtYt, (1)
wheref−1is the inverse camera curve which transforms the
LDR frame into the linear domain, and Ytis the output of
the proposed method at time step t. The soft over-exposed
maskαtis calculated as in [ 7][32].
Figure 2. Global Sampling and Local Propagation Strategy for
temporal clues. Orange frames such as Ltclue contains useful
temporal clues for recovering over-exposed regions in target key
frameLkeyand usually Ltclueis far away from Lkey. We sample
key frames and reference frames from the whole LDR sequence,
and aligns these references with each Lkey. And then propagate
aligned temporal clues to the neighbors of Lkey.
Because of the video ﬁlming process, it’s difﬁcult to
make full use of temporal information. Instead of recon-
structing all of the frames directly, we use an effective and
efﬁcient recovery strategy called Global Sampling and Lo-
cal Propagation strategy. Firstly, we only focus on the re-
covery of some keyframes with the help of global refer-
ence frames and then propagate the restored keyframes lo-
cally to their neighbor frames. To better ﬁnd useful tem-
poral clues from global-sampled reference frames and re-
construct keyframes, we design the Incremental Clue Ag-
gregation Module with ﬂow correction convolution which
can align and fuse frames with brightness changes under
the circumstances of ITM problem . After that, we utilize
the proposed Feature and Clue Propagation Module which
transfers the information from reconstructed keyframes to
their neighbors and generates temporally consistent results.
We will describe each component in detail in the following.
3.2. Strategy of Temporal Clues
During video ﬁlming, as the exposure value changes, ob-
jects that are over-exposed in current frame may become
normally exposed in following frames. This phenomenon
makes it possible to recover realistic textures for over-
exposed regions by using valid information provided by
other frames, which are the temporal clues. However, such
a exposure changing process tends to be designed to be very
slow and smooth in order to ensure ﬁlmed video not ﬂicker,
which makes it difﬁcult to ﬁnd and utilize temporal clues.
Current ITM strategies can not fully utilize temporal clues.
Image ITM methods like [ 7][14][22] don’t utilize temporal
clues. Because of temporal clues often in distant frames,
the sliding window based strategy in [ 34] which only look
at a short sequence of adjacent frames may not reach valid
temporal clues. Although the RNN-based strategy like in
[2] can cover long-term information in a sequence while it
25997
Figure 3. The proposed pipeline can be divided into two stages (above). In the ﬁrst stage, we sample reference frames and key frames
from input LDR sequence. And then extract and align temporal clues to reconstruct keyframes by ICAM (lower left). In the second stage,
we utilize the restored keyframes with aligned temporal clues to propagate information to their neighbors to generate the full sequence by
FCPM (lower right). The dashed lines of ICAM indicate skip connections and the reference frames are utilized incrementally to perform
clue aggregation.
Figure 4. The details of the Local Feature Alignment Block.
may lead to a signiﬁcant increase in computational com-
plexity and when propagates information, useful temporal
clues may fade gradually and errors also be accumulated.
To address these problems, we propose a Global Sam-
pling and Local Propagation strategy as Fig. 2shows.
Speciﬁcally, we uniformly sample Tkeykeyframes from a
input video, and split the entire input sequence into several
equal-length groups of pictures (GOP). A key frame is the
center frame of a GOP. We reconstruct these key frames
with the help of temporal clues. To get valid temporal clues
we sample Trefreference frames with large stride glob-
ally from the input LDRs. And then we align these refer-
ence frames with each key frame to obtain aligned temporal
clues. After that, we propagate aligned temporal clues of
each key frame locally among its GOP to generate HDR
results.Note that choosing the best matching references fortemporal clues may produce better results, but it will lead
to very cumbersome and inefﬁcient frame-by-frame search-
ing calculation, thus reducing the practical application value
of the algorithm. Due to the smooth changes of exposure
value, a preset stride to sample reference frames is sufﬁcient
for exploiting temporal clues and this makes a good balance
between performance and complexity. There is some over-
lap among GOPs, so each input LDR frame will be taken
care of by several reconstructed key frames, which not only
ensure consistency locally among GOP but also conveys
consistency globally across the whole input sequence.
3.3. Incremental Clue Aggregation Module
Note that the number of reference frames corresponding
to a key frame is not ﬁxed. For example, if there is no
camera motion or light condition change in the video, there
will be no temporal clue. To adapt to different situations,
we propose the ICAM which can borrow useful temporal
clues from global reference frames in an incremental way.
As shown in Fig. 3, ICAM is designed based on 5-level
U-Net[ 30]. However, a simple U-net cannot meet the re-
quirements of extracting and aligning temporal clues. So
we design the local and global feature alignment blocks for
aligning reference frames with current key frame spatially.
3.3.1 Local Feature Alignment Block
Flow Correction Convolution To align temporal clues
with over-exposed regions in current key frame, at ﬁrst, we
calculate the optical ﬂow between them by the pre-trained
25998
optical ﬂow estimation network GMA [ 13]. But typical op-
tical ﬂow methods like GMA estimate optical ﬂows based
on the assumption that the observed brightness of any ob-
ject point is constant over timeline. Unfortunately, for ITM
task, there may be huge differences in the luminance among
frames. What’s worse, it is hard to ﬁnd corresponding pix-
els for over-exposed regions due to the lack of textures in
these regions, which will result in an inaccurate optical ﬂow.
To overcome this limitation, we design a ﬂow-correction
block which takes as input the optical ﬂow estimated by
pre-trained optical ﬂow net Fand the corresponding fea-
tures extracted by encoders and predict a corrected optical
ﬂow. Speciﬁcally, at ﬁrst we use the pre-trained GMA [ 13]
asFto predict the initial optical ﬂow from the current key
frameLcurto reference ones Lref:
fc→r=F(Lcur,Lref). (2)
Then we use the estimated optical ﬂow fc→rto warp the lo-
cal feature Fl
refofLrefextracted by the ﬁrst encoder block
of ICAM to be aligned with Fl
cur. We design a correction
convolutional block Cwhich consists of several 3 ×3 con-
volutional layers to reﬁne the initial optical ﬂow:
˜fc→r=C(Fl
cur,Fl
ref,W(Fl
ref,fc→r))+fc→r,(3)
whereWdenotes the warp operation. To train the cor-
rection convolutional block with supervision, we generate
the corresponding LDR frames from the ground truth HDR
frames by the global tone mapping operator as in [ 35],
which can preserve details in highlight regions and ensure
lighting consistency. Then we use pre-trained GMA [ 13]
to estimate the ground truth optical ﬂow fgtfor them and
calculate the ﬂow correction loss:
Lflow=/vextendsingle/vextendsingle/vextendsingle˜f−fgt/vextendsingle/vextendsingle/vextendsingle. (4)
Flow Guided Deformable Convolution Due to possible
large motions between the key frame and long-distance ref-
erences, the corrected optical ﬂow isn’t enough to handle
all of this complicated situation. So we further incorporate
the deformable convolutional block D[5] to get more accu-
rately aligned features. We also employ ˜fc→rto guide the
deformable convolutional block as in [ 3]:
ˆFl
ref=D(Fl
cur,Fl
ref,W(Fl
ref,˜fc→r),˜fc→r),(5)
whereˆFl
refis the ﬁnal aligned feature.
Feature Fusion Convolution Before fusing ˆFl
refwith
Fl
cur, we calculate a ﬁlter mask Ml
refforˆFl
refby two 3
×3 convolutional layers and adopt Sigmoid to determine
if the feature is valuable or not for the current key frame.
Finally, we fuse the above features by the fusion convolu-
tional block Fuwhich consists of three dense blocks [ 11] to
get the aligned local feature ˜Fl
cur:
˜Fl
cur=Fu(Fl
cur,ˆFl
ref·Ml
ref). (6)3.3.2 Global Feature Aggregation Block
As we know, global spatial information is also important
for ITM task. Therefore, on the basis of aligned local spatial
features, we introduce a global feature aggregation block to
further utilize global spatial information in the smallest res-
olution of the Unet. In terms of implementation, we refer
to the transformer block used in [ 23] [21] which can cap-
ture long-distant dependencies by the self-attention mech-
anism, which is helpful to model high-level semantic fea-
tures and aggregate textures for over-exposed regions. We
extract global features ˆFg
reffrom the aligned ˆFl
refby four 3
×3 convolutional layers with stride 2. Then we concatenate
these features with the global features Fg
curof the current
key frame in the channel dimension, and utilize four swin
transformer blocks Sto get aggregated global features ˜Fg
cur
of the current key frame:
˜Fg
cur=S(Fg
cur,ˆFg
ref), (7)
After alignment locally and globally in space, there are
four decoder blocks with skip connections to obtain tempo-
ral clues and reconstruct the HDR key frame. The details of
each proposed component can be found in the supplemen-
tary material. Overall, the proposed ICAM takes the cur-
rent keyframe Lcurand reference frames Lrefj,j=1...Tclues
as input and obtain temporal clues and predict HDR key
framesXcur. Note that the Local Feature Align block and
the Global Feature Aggregation block are used repeatly if
there is more than one reference frame. We train ICAM
with losses between XcurandHcur, that’s the L1pixel loss
Lpixand the perceptual loss Lperby the VGG-16 [ 33] pre-
trained on ImageNet [ 6]. Therefore, the total training loss
of ICAM is:
LICAM=Lpix+λperLper+λflowLflow, (8)
whereλperandλflow are set to 0.1 and 0.05 separately.
3.4. Feature and Clue Propagation Module
To convert the whole input LDR sequence to HDR se-
quence, the most straightforward idea is to regard all input
frames as key frames and process them one by one using
above proposed ICAM. However, This straightforward way
has two drawbacks. First, this way only considers global
temporal clues but neglects local adjacent frames, which
may ignores valuable information and cause the tempo-
ral inconsistency problem. Second, ICAM involves com-
plicated feature manipulations, processing all frames by
it introduces much computational cost. Considering that
the exposure usually changes smoothly, the reconstruction
of neighbor frames in a GOP should be nearly consistent.
Therefore, we propose the Feature and Clue propagation
module which can broadcast recontructed textures and ag-
gregated temporal clues in HDR keyframes to their neigh-
bor frames. Our method can release the burden of repeat
25999
computation of temporal clues and keep consistency along
time axis. Speciﬁcally, for each frame Ltof input LDR
video, we regard it in ﬁve nearest GOPs. Upon that, ﬁve
according HDR keyframes Xngbj,j= 0...5reconstructed
by ICAM are warped to Ltusing the optical ﬂow estimated
by the proposed ﬂow correction block. And a combination
masksMngbjis predicted by a combination convolution
block, which extracts the local, dilated, and global features
and fuses them into the output features like ExpandNet[ 26].
Then we generate the merged result ˆXt:
ˆXt=4/summationdisplay
j=0Mngbj·W(Xngbj,ft→ngbj). (9)
In this way, the restored textures and temporal clues in
Xngbjcan be propagated into preliminarily reconstructed
resultˆXt. And thus reducing computational cost compared
with directly processing Ltby ICAM. Furthermore, to uti-
lize the local information which may be ignored by ICAM,
we perform the bidirectional features and clues propagation
among the input frames like BasicVSR [ 2]. To further adapt
ITM task, we use the corrected optical-ﬂow and devise the
basic block as the combination block:
hb
t=Fb(ˆXt,ˆXt+1,W(hb
i+1,ft→t+1)), (10)
hf
t=Ff(ˆXt,ˆXt−1,W(hf
i−1,ft→t−1)), (11)
wherehb
tandhf
tdenote the corresponding features of the
backward propagation block Fband forward propagation
blockFfin time step t. Then the forward and backward
features are concatenated and fused by two 3 ×3 convolu-
tional layers to predict the residuals, which are added into
the preliminary image ˆXtto get the ﬁnal reconstructed re-
sults:
Yt=Fu(hf
t,hb
t)+ˆXt. (12)
The training loss of FCPM also contains pixel reconstruc-
tion loss and perceptual loss. Besides, the generative adver-
sarial loss has been proven to improve perceptual quality by
forcing the distribution of generated results closer to that of
ground truth. Therefore, we also incorporate the 3D patch-
GAN [ 12] as the discriminator to distinguish the predicted
sequences by FCPM from the corresponding ground truth
HDR sequences. We adopt the least-square GAN [ 25] as
the adversarial loss LGAN . Therefore, the total training loss
of FCPM is:
LFCPM=Lpix+λperLper+λganLGAN, (13)
whereλperandλganare set to 0.1 and 0.05 separately.3.5. Dataset Synthesis
Since there are few publicly available HDR video
datasets, we propose an HDR video data synthesis method
that can utilize existing HDR image and LDR video
datasets. Firstly, we convert the pixel values of existing
LDR image datasets such as REDS [ 27]Lreds to the lin-
earized domain by inverse camera response curve mapping.
Then the linearized image is multiplied by a randomly sam-
pled value Tto simulate the exposure duration T. Finally,
we clip the pixel to [0,1] and use the camera response curve
to map them back to the pixel domain as the input of the
networkIredsby the following equation:
Oreds=f(clip(f−1(Lreds)·T)), (14)
wherefdenotes the camera response curve and we use
the gamma function with 1/2.2 here. However compared
to the real HDR, the fake HDR data generated using LDR
images still has a large gap in dynamic range and infor-
mation magnitude. Besides datasets from LDR videos, we
use HDR image datasets like SICE dataset [ 1] to simulate
HDR videos. As speciﬁed before, one of the main reasons
that cause the exposure setting to change is camera motions.
Therefore, we perform a random perspective transformation
on the HDR images to simulate camera motions, which we
use as the HDR video clip and follow Eq. 14to generate
the input LDR frames except for the inverse camera curve
mapping.
We utilize the video characteristics of LDR videos and
the HDR characteristics of HDR images to generate HDR
video datasets. Experimental results show that networks
trained by our dataset can achieve good performance on
real-world HDR video testing datasets. Examples of the
synthetic datasets can be found in the supplementary mate-
rial.
4. Experiments
4.1. Implementation details
Dataset The HDR image dataset used to generate syn-
thetic data is SICE [ 1], which contains 589 HDR images.
And we use the “sharp” training dataset of REDS [ 27] as
the LDR video dataset. As for the testing dataset, at ﬁrst,
we evaluate the performance of the synthetic data generated
from the validation dataset of REDS [ 27]. Then we use
three public real-world HDR video datasets: HDM-HDRv
[9], LiU-HDRv [ 17], and MPI-HDRv [ 10]. Because there is
no corresponding LDR version for these datasets, we gen-
erate the LDR videos from them by simulating the camera
imaging pipeline. The details of the datasets can be found
in the supplementary material.
Experiment setup The implementation environment is Py-
Torch 1.9 version and the Adam optimizer is applied to train
26000
Figure 5. Visual comparisons on the frame of “ﬁshing longshot” sequence from HDM-HDRv [ 9] (above) and “sb-tunnel-exr” sequence
from MPI-HDRv [ 10] (below). ( Zoom in for a better view ).
Table 1. Quantitative comparison on HDR videos with existing methods. The scores here are HDR-VDP-3/HDR-VQM, where a higher
score of HDR-VDP-3 and a lower score of HDR-VQM mean better. Red text indicates the best and blue text indicates the second best
result, respectively.
REDS-val [ 27] HDM-HDR [ 9] LiU-HDR [ 17] MPI-HDRv [ 10]
HDRCNN [ 7] 6.774/0.452 5.822/0.467 7.884/ 0.545 6.733/0.053
Diff HDRI [ 14] 6.820/0.502 5.703/0.514 7.751/0.594 6.501/0.061
Single HDRI [ 22] 7.059/0.492 6.346 /0.481 8.143/0.569 7.225 /0.058
FMNet [ 36] 6.895/0.483 5.847/0.496 7.974/0.572 6.847/0.057
Deep VITM [ 37] 7.106/0.458 5.992/0.445 8.036/0.553 7.104/ 0.049
Bascivsr++ [ 3] 7.254 /0.443 6.131/ 0.427 8.265 /0.547 7.119/0.052
Proposed 7.891 /0.398 6.754 /0.356 8.591 /0.522 7.970 /0.037
Figure 6. Visual comparison on the frame of a real-world LDR
video shot by iPhone 13.
the model with a learning rate of 0.0002. We ﬁrst resize the
training pairs in the training dataset to 512 ×512 and aug-
ment them by randomly cropping to 384 ×384. The train-
ing images are randomly ﬂipped and rotated. The frames
of testing datasets are resized to 512 ×512 for evaluation.
For each video sequence, we sample keyframes uniformly
with the stride of six and ﬁnd six temporal clues for each
keyframe in the forward and backward direction respec-
tively with the stride of 15.
Evaluation metrics We evaluated the estimated HDR in
terms of HDR-VDP-3 [ 24] which is a commonly used met-
ric to measure the quality of single HDR image reconstruc-
tion, and HDR-VQM [ 28] which is designed for evaluating
the quality of HDR videos.
4.2. Comparisons on the predicted HDR videos
The proposed method is compared with three image ITM
methods (HDRCNN [ 7], Differentiable HDR [ 19], and Sin-
gle HDR [ 22]), one SDRTV-HDRTV conversion method
FMNet [ 36], one video ITM method Deep VITM [ 37], andone video restoration method BasicVSR++ [ 3]. For fair
comparisons, we re-train these models with the same train-
ing dataset. (For the ﬁrst four methods, only a single frame
is taken as input, and for the last two , we feed them with
frames as the proposed methods and only reconstruct the
over-exposed regions too.)
Visual comparisons. Fig. 5show the results of these ITM
methods on two LDR images with severely over-exposed
regions. The single-frame-based methods (HDRCNN [ 7],
Differentiable HDR [ 19], Single HDR [ 22], and FMNet
[36]) can not restore textures from the rare available tem-
poral information. Deep VITM [ 37] performs 3D convo-
lutions directly and fails to model the correlation between
frames. BasicVSR++ [ 3], due to the lack of explicit exploit,
temporal clues fade gradually when propagated. On the
contrary, based on full use of temporal clues, the proposed
method can generate impressive results with high ﬁdelity.
Fig. 6shows an example of a real-world LDR video shot
by iPhone 13, where compared to the SOTA image ITM
method [ 22], the proposed method can reconstruct realistic
and natural textures with the help of temporal clues in ref-
erence frame. All of the HDR frames are tone mapped by
the [16] for display on LDR devices.
Quantitative comparisons. Table 1shows the average
scores of HDR-VDP-3 and HDR-VQM on the REDS-val
[27], HDM-HDRv [ 9], LiU-HDRv [ 17], and MPI-HDRv
[10] datasets. The proposed method performs favorably
against the state-of-the-art methods on all four datasets.
26001
Figure 7. Visual comparison on reconstructed result with different
temporal clues.
(a) Non-local frames (b) Pre-trained Flow (c) Correct ed Flow (d) Ground Truth Flow
Figure 8. Visual comparison between the pre-trained and corrected
optical ﬂow.
4.3. Ablation studies
Incremental Clue Aggregation. We validate the effective-
ness of the incremental aggregation. As shown in Fig. 7,
with more temporal clues, ICAM can learn the desired tex-
tures from the clues in an incremental way and generates
increasingly realistic and natural details.
Components of the ICAM. We evaluate the effectiveness
of the modules in Incremental Clue Aggregation Module.
The results of HDR-VQM scores on HDM-HDR dataset
are shown in Table 2. The proposed ﬂow correction convo-
lution and according ﬂow-guided deformable convolution
both contribute to generate more accurate results. Mean-
while, the global feature aggregation can also imporve the
performance. Fig. 8shows optical ﬂows of two frames with
large motion and different exposure.
Flow Correction. The pre-trained ﬂow is estimated by pre-
trained GMA [ 13], corrected ﬂow is estimated by the ﬂow
correction block, and the ground truth ﬂow is generate by
pre-trained GMA [ 13] for the tone mapped images with
consistent luminance and no over-exposed regions.
Components of the FCPM. We conduct experiment to ver-
ify the effect of different feature propagation methods on
the results. Speciﬁcally, we ﬁrst obtain preliminary re-
sults by warping and combining neighboring reconstructed
HDR keyframes. Then we perform forward, backward, and
bidirectional feature propagation respectively to ﬁne-tune
the preliminary reconstructed frames using inter-frame in-
formation, and the HDR-VQM scores on the HDM-HDR
dataset are shown in Table 3.
Temporal consistency. Fig. 9shows the comparison be-
tween the results of all generated by ICAM and generated
Figure 9. Visual comparison between the reconstructed adjacent
frames. (a)-(c) all generated only by ICAM. ((d)-(f)) generated by
the proposed pipeline with FCPM and reconstructed key frames.
Table 2. Ablation study of aggregation components. HDR-VQM
scores of (a)pre-trained ﬂow, (b)corrected ﬂow, (c)ﬂow-guided
Deformable Convolution and (d)global aggregation.
w/o align (a) (b) (c) (d)
0.418 0.396 0.380 0.367 0.356
Table 3. Quantitative ablation study of the propagation methods.
w/o FCPM Warp Forward Backward FCPM
0.383 0.376 0.369 0.366 0.356
by the proposed pipeline. only using ICAM generates tem-
poral inconsistent textures while the proposed pipeline can
avoid this problem and reconstruct more realistic result.
Running Time. We also compare the average running time
processing a 512 ×512 frame on Tesla V100 GPU of only
using ICAM for entire sthe equence (1236ms) and the pro-
posed pipeline (521ms).
5. Conclusions
In this paper, we analyze the characteristics of LDR
videos, and propose a novel global sampling and local prop-
agation strategy to fully exploit Temporal Clues. In order
to better server the proposed strategy, we design the In-
cremental Clue Aggregation Module and Feature and Clue
Propagation Module. These modules can make full use of
temporal clues to recovery details with high ﬁdelity mean-
while ensure temporal consistency. In addition, we devise
an HDR video dataset synthesis method to train our method.
Experimental results show that the proposed video ITM
method outperforms the state-of-the-art methods in both
quantitative and qualitative evaluations.
Acknowledgements. This work is ﬁnancially sup-
ported for Outstanding Talents Training Fund in Shen-
zhen, Shenzhen Science and Technology Program-
Shenzhen Cultivation of Excellent Scientiﬁc and
Technological Innovation Talents project(Grant No.
RCJC20200714114435057) , Shenzhen Science and
Technology Program-Shenzhen Hong Kong joint funding
project (Grant No. SGDX20211123144400001), Na-
tional Natural Science Foundation of China U21B2012
and 62272142, R24115SG MIGU-PKU META VISION
TECHNOLOGY INNOV ATION LAB.
26002
References
[1] Jianrui Cai, Shuhang Gu, and Lei Zhang. Learning a deep
single image contrast enhancer from multi-exposure images.
IEEE Transactions on Image Processing , 27(4):2049–2062,
2018. 6
[2] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and
Chen Change Loy. Basicvsr: The search for essential com-
ponents in video super-resolution and beyond. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 4947–4956, 2021. 3,6
[3] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and
Chen Change Loy. Basicvsr++: Improving video super-
resolution with enhanced propagation and alignment. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 5972–5981, 2022. 5,7
[4] Guanying Chen, Chaofeng Chen, Shi Guo, Zhetong Liang,
Kwan-Yee K Wong, and Lei Zhang. Hdr video reconstruc-
tion: A coarse-to-ﬁne network and a real-world benchmark
dataset. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 2502–2511, 2021. 2
[5] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
Zhang, Han Hu, and Yichen Wei. Deformable convolutional
networks. In Proceedings of the IEEE international confer-
ence on computer vision , pages 764–773, 2017. 2,5
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 5
[7] Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafał K
Mantiuk, and Jonas Unger. Hdr image reconstruction from
a single exposure using deep cnns. ACM transactions on
graphics (TOG) , 36(6):1–15, 2017. 1,3,7
[8] Yuki Endo, Yoshihiro Kanamori, and Jun Mitani. Deep re-
verse tone mapping. ACM Trans. Graph. , 36(6):177–1, 2017.
1,3
[9] Jan Froehlich, Stefan Grandinetti, Bernd Eberhardt, Simon
Walter, Andreas Schilling, and Harald Brendel. Creating
cinematic wide gamut hdr-video for the evaluation of tone
mapping operators and hdr-displays. In Digital photography
X, pages 279–288. SPIE, 2014. 1,6,7
[10] Vlastimil Havran, Miloslaw Smyk, Grzegorz Krawczyk,
Karol Myszkowski, and Hans-Peter Seidel. Interactive sys-
tem for dynamic scene lighting using captured video envi-
ronment maps. In Rendering Techniques , pages 31–42, 2005.
6,7
[11] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 4700–4708, 2017. 5
[12] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 6[13] Shihao Jiang, Dylan Campbell, Yao Lu, Hongdong Li, and
Richard Hartley. Learning to estimate hidden motions with
global motion aggregation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 9772–
9781, 2021. 5,8
[14] Jung Hee Kim, Siyeong Lee, and Suk-Ju Kang. End-to-
end differentiable learning to hdr image synthesis for multi-
exposure images. arXiv preprint arXiv:2006.15833 , 2020. 1,
3,7
[15] Soo Ye Kim, Jihyong Oh, and Munchurl Kim. Deep sr-itm:
Joint learning of super-resolution and inverse tone-mapping
for 4k uhd hdr applications. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 3116–
3125, 2019. 2,3
[16] Chris Kiser, Erik Reinhard, Mike Tocci, and Nora Tocci.
Real time automated tone mapping system for hdr video. In
IEEE International Conference on Image Processing , pages
2749–2752. IEEE Orlando, FL, 2012. 7
[17] Joel Kronander, Stefan Gustavson, Gerhard Bonnet, Anders
Ynnerman, and Jonas Unger. A uniﬁed framework for multi-
sensor hdr video reconstruction. Signal Processing: Image
Communication , 29(2):203–215, 2014. 6,7
[18] Siyeong Lee, Gwon Hwan An, and Suk-Ju Kang. Deep chain
hdri: Reconstructing a high dynamic range image from a sin-
gle low dynamic range image. IEEE Access , 6:49913–49924,
2018. 1
[19] Siyeong Lee, Gwon Hwan An, and Suk-Ju Kang. Deep re-
cursive hdri: Inverse tone mapping using generative adver-
sarial networks. In Proceedings of the European Conference
on Computer Vision (ECCV) , pages 596–611, 2018. 1,3,7
[20] Siyeong Lee, So Yeon Jo, Gwon Hwan An, and Suk-Ju
Kang. Learning to generate multi-exposure stacks with cycle
consistency for high dynamic range imaging. IEEE Transac-
tions on Multimedia , 2020. 1
[21] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc
Van Gool, and Radu Timofte. Swinir: Image restoration us-
ing swin transformer. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 1833–1844,
2021. 5
[22] Yu-Lun Liu, Wei-Sheng Lai, Yu-Sheng Chen, Yi-Lung Kao,
Ming-Hsuan Yang, Yung-Yu Chuang, and Jia-Bin Huang.
Single-image hdr reconstruction by learning to reverse the
camera pipeline. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
1651–1660, 2020. 1,2,3,7
[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 10012–10022, 2021. 2,5
[24] Rafał Mantiuk, Kil Joong Kim, Allan G Rempel, and Wolf-
gang Heidrich. Hdr-vdp-2: A calibrated visual metric for
visibility and quality predictions in all luminance conditions.
ACM Transactions on graphics (TOG) , 30(4):1–14, 2011. 7
[25] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares generative
adversarial networks. In Proceedings of the IEEE Interna-
26003
tional Conference on Computer Vision , pages 2794–2802,
2017. 6
[26] Demetris Marnerides, Thomas Bashford-Rogers, Jonathan
Hatchett, and Kurt Debattista. Expandnet: A deep convo-
lutional neural network for high dynamic range expansion
from low dynamic range content. In Computer Graphics Fo-
rum, pages 37–49. Wiley Online Library, 2018. 1,6
[27] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik
Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu
Lee. Ntire 2019 challenge on video deblurring and super-
resolution: Dataset and study. In CVPR Workshops , 2019. 6,
7
[28] Manish Narwaria, Matthieu Perreira Da Silva, and Patrick
Le Callet. Hdr-vqm: An objective quality measure for high
dynamic range video. Signal Processing: Image Communi-
cation , 35:46–60, 2015. 7
[29] Allan G Rempel, Matthew Trentacoste, Helge Seetzen,
H David Young, Wolfgang Heidrich, Lorne Whitehead, and
Greg Ward. Ldr2hdr: on-the-ﬂy reverse tone mapping of
legacy video and photographs. ACM transactions on graph-
ics (TOG) , 26(3):39–es, 2007. 3
[30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention , pages 234–241.
Springer, 2015. 4
[31] Mehdi SM Sajjadi, Raviteja Vemulapalli, and Matthew
Brown. Frame-recurrent video super-resolution. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 6626–6634, 2018. 2
[32] Marcel Santana Santos, Tsang Ing Ren, and Nima Khademi
Kalantari. Single image hdr reconstruction using a cnn with
masked features and perceptual loss. ACM Transactions on
Graphics (TOG) , 39(4):80–1, 2020. 1,3
[33] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 5
[34] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and
Chen Change Loy. Edvr: Video restoration with enhanced
deformable convolutional networks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops , pages 0–0, 2019. 2,3
[35] Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, and Chi-Keung Tang.
Deep high dynamic range imaging with large foreground
motions. In Proceedings of the European Conference on
Computer Vision (ECCV) , pages 117–132, 2018. 5
[36] Gang Xu, Qibin Hou, Le Zhang, and Ming-Ming Cheng.
Fmnet: Frequency-aware modulation network for sdr-to-hdr
translation. In Proceedings of the 30th ACM International
Conference on Multimedia , pages 6425–6435, 2022. 2,3,7
[37] Yucheng Xu, Li Song, Rong Xie, and Wenjun Zhang. Deep
video inverse tone mapping. In 2019 IEEE Fifth Interna-
tional Conference on Multimedia Big Data (BigMM) , pages
142–147. IEEE, 2019. 2,3,7
[38] Zhuoran Zheng, Wenqi Ren, Xiaochun Cao, Tao Wang, and
Xiuyi Jia. Ultra-high-deﬁnition image hdr reconstruction
via collaborative bilateral learning. In Proceedings of theIEEE/CVF International Conference on Computer Vision ,
pages 4449–4458, 2021. 1,3
26004
