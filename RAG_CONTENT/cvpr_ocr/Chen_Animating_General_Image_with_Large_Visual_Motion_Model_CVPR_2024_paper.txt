Animating General Image with Large Visual Motion Model
Dengsheng Chen Xiaoming Wei Xiaolin Wei
Meituan
Beijing, China
{chendengsheng,weixiaoming,weixiaolin02 }@meituan.com
Abstract
We present the pioneering Large Visual Motion
Model (LVMM), meticulously engineered to analyze the
intrinsic dynamics encapsulated within real-world imagery.
Our model, fortified with a wealth of prior knowledge
extracted from billions of image pairs, demonstrates
promising results in predicting a diverse spectrum of scene
dynamics. As a result, it can infuse any generic image with
authentic dynamic effects, enhancing its visual allure.
Project page: https://github.com/densechen/
LVMM .
1. Introduction
Recent progress in generative models [39], specifically
conditional diffusion models [11, 22], and large-scale mod-
els [39], have substantially enhanced our capability to rep-
resent complex and rich distributions. These models have
underscored the transformative potential of harnessing vast
data and intensive training [14], exhibiting unparalleled pro-
ficiency in comprehending and generating human-like text,
and creating visually rich and diverse images from textual
descriptions. This has facilitated a variety of previously
unachievable applications, such as text-conditioned gener-
ation of arbitrary, realistic image content [21]. The advent
of these models [5, 19, 24], propelled by the availability
of large-scale datasets [25] and advancements in training
methodologies [4, 20], has ignited interest in probing other
domains, including audio [38] and multimodal data [15].
In this paper, we present a novel Large Visual Motion
Model (LVMM), specifically designed to proficiently pre-
dict local motion embedded within a given scene, thereby
enhancing the dynamic appeal of a static image. The dy-
namism of the natural world is characterized by subtle
changes even in seemingly static landscapes, influenced by
various factors such as wind, water currents, and inherent
rhythms. When observing a still image, we can envisage
plausible motions that might have been occurring when the
(a)I0
 (b)ˆz1,···,K
 (c)ˆp1,···,K
 (d) X-t slices
(e) Capability to animate faces, generating natural and realistic smiles.
Figure 1. Beginning with a reference image I0as depicted in
Fig. 1a, the Large Visual Motion Model (LVMM) estimates a
latent motion trajectory ˆz1,···,Kas shown by the t-SNE plot in
Fig. 1b, utilizing the motion denoising model ϵθ. This trajectory
is subsequently processed by the Motion Decoder D, generating a
sequence of optical flows ˆδ1,···,K(visualized in the left column of
Fig. 1c) and intention maps ˆω1,···,K(displayed in the right column
of Fig. 1c). Ultimately, the Neural Image Renderer Rtransforms
I0into a series of novel images ˆI1,···,K, guided by optical flows
and intention maps. Fig 1d illustrates the resultant videos, employ-
ing space-time X-t slices across 300frames (corresponding to the
scanline shown in Fig. 1a).
photograph was captured. This predictability is ingrained in
our human perception of real scenes, i.e., we can imagine a
distribution of natural motions conditioned on that image if
there could have been multiple possible motions. Given the
ease with which humans can envision these potential mo-
tions, an intriguing research question is to computationally
model this motion distribution with a large-scale model.
The proposed LVMM excels in associating salient vi-
sual and motion patterns, thereby accurately predicting lo-
cal motion trajectory, as shown in Fig. 1. It comprises two
components: the motion rendering model and the motion
diffusion model. The former extracts a latent motion vector
from the scene and reconstructs the target image. The latter
generates suitable motion trajectories from the given scene
and feeds them to the motion rendering model to produce
realistic dynamic effects.
Our primary contribution is the pioneering proposal and
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7131
design of a large-scale model dedicated to visual motion,
the effectiveness of which has been empirically validated.
2. Overview
𝐼!ℛ(𝐼!|𝑝")𝒫(𝐼!,𝐼")
𝐼"
𝐼)"Eq.1
(a) Train RandP.
𝒟∘ℰ∘𝒫∗(𝐼",𝐼#)Eq.1
𝐼#ℛ∗(𝐼"|𝑝#)
𝐼"
𝐼,#Eq.4 (b) Train EandD.
𝒟∗∘ℰ∗∘𝒫∗(𝐼",𝐼#)Eq.1
𝐼#ℛ(𝐼"|𝑝#)
𝐼"
𝐼,#
(c) Finetune R(Optional).
𝜖!(𝑥",𝑣#,⋯,&,𝑡,𝑦)𝒰(1,𝑇)Eq. 10ℰ∗∘𝒫∗(𝐼",𝐼()𝑧#,⋯,&Image Bind
𝐼(𝑧"
𝐼" (d) Train ϵθ.
Figure 2. Training phase. Modules denoted in gray or marked
with an asterisk ( ∗) in the upper right corner indicate that these
modules do not undergo parameter updates. Conversely, modules
highlighted in orange undergo parameter updates. The color of the
connecting lines directly corresponds to the color of the variables.
Given a single reference image I0, our goal is to synthe-
size a video sequence ˆI1,···,Kof length K. This sequence is
designed to capture local dynamics such as the sway of veg-
etation or the flicker of candle flames under wind influence,
as well as human emotional expressions like joy, anger, sor-
row, and pleasure.
Our proposed framework consists of two components: a
motion rendering model (Sec. 3.1) and a motion diffusion
model (Sec. 3.2). The motion rendering model is a sophisti-
cated system comprising a neural image renderer R, a mo-
tion flow predictor P, and a motion encoder-decoder pair E
andD. The neural image renderer Rtransforms the refer-
ence image I0into the target image Ikby utilizing the mo-
tion flow ppredicted by P. A motion flow pis composed of
an optical flow δand an intention map ω, which respectively
corresponds to low-frequency and high-frequency motion
dynamics. The motion encoder Emaps pinto a latent mo-
tion space, where even the most complex motion flows can
be represented by a motion vector z. The motion decoder D
then converts the motion vector back into the motion flow
space.
Through rigorous training, we discovered that the motion
vector zin the latent motion space demonstrates a higher
degree of regularity compared to motion flow space, as de-
picted in Fig. 1b. Intriguingly, the motion vector can en-
capsulate motion rules in two segments: those associatedwith visual features v(visual segments) and those unre-
lated u(motion segments). By retaining the visual seg-
ments and replacing the motion segments with those from
different scenes, we can achieve cross-scene motion trans-
fer (Para. 3.2.1).
Exploiting this property, the motion diffusion model can
concentrate on learning motion segments uthat are unre-
lated to visual features, thereby significantly simplifying the
task. The motion denoising model ϵθ, can generate a latent
motion trajectory ˆu1,···,Kbased on the provided image I0,
thus producing dynamic image effects (Sec. 3.3).
3. Large Visual Motion Model
3.1. Motion Rendering Model
3.1.1 Neural Image Renderer and Motion Flow Predic-
tor
Our approach commences with the deployment of two im-
ages, namely the reference image I0(Fig. 3a) and the driv-
ing image Ik(Fig. 3b). The concurrent training of the mo-
tion flow predictor Pand the neural image renderer Ris the
initial step. The aim is to steer the motion portrayed by the
motion flow such that the image ˆIkrendered from I0closely
mirrors the actual driving image Ik. The training pipeline
is illustrated in Fig. 2a. This process can be mathematically
expressed as:
arg min
R,PE[∥Ik− R(I0|P(I0, Ik))|{z }
ˆIk∥2
2] (1)
Design of Motion Flow Predictor POur design incorpo-
rates a network architecture akin to that proposed by Siaro-
hin et al. [26]. We introduce two key modifications to en-
hance the estimation of a wide range of motion scenarios
across large-scale datasets.
Initially, the local similarity of optical flow can result in a
loss of generalization capability across different scenarios if
Pis directly instructed to predict a motion flow pin a pixel-
wise manner. To avoid this, we downsample I0andIkby a
factor of 0.25prior to feeding them to P, as follows:
p:=P(I′
0, I′
k) (2)
where I′
0, I′
k∈ RH
4×W
4are the downsampled images. This
approach still produces satisfactory results, while signifi-
cantly reducing the computational overhead of the motion
flow predictor.
Secondly, the motion flow predictor Ptends to assign an
optical flow δwith a non-negligible value to most station-
ary points in the background, as shown in the left column
of Fig. 1c. This tendency severely hinders the model’s abil-
ity to capture subtle local movements, as there will be a
large number of relatively stationary regions in image pair
7132
(a)I0
 (b)Ik
 (c)R(I0|δx, δy)
(d)F(I0|δx, δy, ω)
(e)R(I0|δx, δy, ω)
 (f)R(I0|ˆpk)
 (g)R∗(I0|ˆpk).
Figure 3. Ablation study of the motion accretion model. For a detailed explanation, please refer to Sec. 4.4.
⟨I0, Ik⟩. Specifically, during backpropagation, these rela-
tively stationary regions contribute a large amount of gra-
dient that is not practically meaningful, preventing Pfrom
learning more detailed local motion information, as shown
in Fig. 3c. To address this issue, we predict an intention
mapω∈RH
4×W
4to represent the motion intention of each
region along with optical flow δ. When ωapproaches zero,
it indicates that the current region tends to be stationary,
and its gradient will be reduced by ωduring backpropaga-
tion, thereby avoiding gradient pollution. As shown in the
right column of Fig. 1c, the predicted intention map indi-
cates many high-frequency information, such as the con-
tour of the object, while the optical flow represents more
low-frequency information, such as the motion trend of the
main body.
In summary, the motion flow p∈R3×H
4×W
4can be
viewed as composed of two parts: the optical flow δ∈
R2×H
4×W
4along the x and y dimensions, respectively, and
an intention map ω∈RH
4×W
4that represents the motion
tendency at each location.
Design of Neural Image Renderer RWe denote our
neural image renderer as R. The initial step in the process
involves the construction of a multi-scale feature pyramid,
FI0, for the image I0. Following this, a warp function F
is applied to each feature map within FI0, leading to the
generation of a warped feature pyramid, F′
I0:
F′
I0=F(FI0, δ′)⊙ω′(3)
In the above equation, δ′, ω′represent the interpolated op-
tical flow and the intention map respectively. These com-
ponents are specifically designed to align with the spatial
configuration of the feature in FI0across multiple scales.
⊙denotes an element-wise multiplication operation. In the
final phase, Rgenerates an estimated driving image, ˆIk,
which is based on the warped feature pyramid F′
I0, rather
thanFI0.
The direct application of the motion flow to the feature
pyramid FI0offers several benefits. Primarily, it signifi-
cantly reduces the likelihood of image distortion and degra-
dation that could potentially occur when the motion flow is
applied within the pixel space (Fig. 3d). Furthermore, theneural image renderer can leverage the prior knowledge en-
capsulated within large-scale data to compensate for a vari-
ety of missing features, as depicted in Fig. 3e. Lastly, it can
generate more meaningful gradients across different scales,
thereby augmenting the learning capability of the motion
flow predictor.
3.1.2 Motion Variational Autoencoder (VAE)
In a formalized manner, our objective is to train a Motion
Variational Autoencoder (V AE) that minimizes the error as
defined in the equation below:
arg min
E,DE[∥pk− D ◦ E (pk)∥2
2]| {z }
motion flow regularization+E[∥Ik−ˆIk∥2
2]|{z}
pixel regularization(4)
where, pk=P∗(I0, Ik),ˆIk=R∗(I0|D ◦ E (p)).∗denotes
that the model parameters are fixed at this stage. The train-
ing pipeline is depicted in Fig. 2b.
Eq. 4 comprises two constraints: motion flow regular-
ization and pixel regularization. The loss value of the for-
mer, also referred to as the reconstruction error of the V AE
network, ensures optimal accuracy in the encoding and de-
coding process. However, an exclusive emphasis on motion
flow regularization can lead to model collapse . Specifically,
motion flow does not possess a unique deterministic solu-
tion, implying that the motion flow between I0andIkis
not unique and can accommodate multiple plausible solu-
tions. Highly similar scenes may yield significantly differ-
ent predicted motion flows. Furthermore, as discussed in
Para. 3.1.1, Pmay generate a plethora of irrelevant mo-
tion features for points that are essentially stationary. Mo-
tion flow regularization necessitates the motion decoder D
to accurately reproduce the predicted motion flow, which
deviates from our primary objective. Our goal is to ensure
that the image ˆIkrendered by the neural image renderer
closely resembles Ik. The introduction of pixel regulariza-
tion, which directly imposes constraints on ˆIk, effectively
mitigates this issue, as demonstrated in Fig. 3f.
7133
3.1.3 Fine-tuning Neural Image Renderer (Optional)
To further alleviate the inherent reconstruction error in the
Variational Autoencoder (V AE), we propose an optional
step of fine-tuning the Neural Image Renderer. The train-
ing pipeline is illustrated in Fig. 2c. This is accomplished
by minimizing the following objective function:
arg min
RE[∥Ik− R(I0|D∗◦ E∗◦ P∗(I0, Ik))∥2
2](5)
Based on our empirical observations, this fine-tuning step
might not be essential for scenarios with lower complexity.
Nevertheless, for scenes rich in local motion details, such as
those influenced by facial expressions, this additional fine-
tuning process can significantly improve the quality of the
reconstructed details, as demonstrated in Fig. 3g.
3.2. Motion Diffusion Model
Conceptually, we could train a motion diffusion model to
directly capture the distribution of motion flow p, aspsym-
bolizes the motion within the scene. However, we observe
that when two images exhibit no motion (i.e., I0=Ik),pkis
significantly non-zero and fluctuates with different I0. This
implies that the predicted motion flow pnot only encapsu-
lates motion information but also integrates visual features.
Although the amalgamation of motion information and
visual features might not present a substantial problem
on small-scale data, it considerably impairs the diffusion
model’s capacity to exploit the benefits of large-scale data.
This is attributed to the fact that when training on large-
scale data, the surplus visual features obstruct the model
from abstracting a unified motion law. Conversely, on a
small-scale dataset, the model is entirely capable of memo-
rizing all the information.
3.2.1 Decomposition of the Neural Motion Vector
The challenge of disentangling motion information from vi-
sual features in the motion flow space prompts us to investi-
gate potential solutions within the latent motion space. We
propose a hypothesis, devise a solution based on this hy-
pothesis, and subsequently verify the hypothesis through
rigorous experimentation.
Hypothesis We propose that the neural motion vector
zk=E ◦ P (I0, Ik)can be decomposed into zk=
uk+vk, where ukandvkdenote the motion-related and
visual-related components, respectively. For any given pair
⟨I0, Ik⟩, our objective is to independently compute the com-
ponents ukandvk, which could be advantageous for subse-
quent applications.
Golden leaf
Brown leaf
Avocado(a) t-SNE of z0,···,K.
Golden leaf
Brown leaf
Avocado (b) t-SNE of {v0,···, vK+u′
K}.
(c) Golden leaf (source motion z′
0,···,K).
(d) Brown leaf (driven by z′
0,···,K).
(e) Avocado (driven by z′
0,···,K).
(f) Avocado (driven by {v0, v1+u′
1,···, vK+u′
K}).
Figure 4. Cross-Scene Motion Transfer (For optimal understand-
ing, we recommend viewing the video provided on our homepage).
Solution We observe that for ⟨I0, I0⟩,z0=u0+v0. In
this scenario, the component u0is absent, as there is no mo-
tion information for an image concerning itself. Therefore,
we deduce that v0=z0. Moreover, we find that for a fixed
I0and any Ik, asI0remains constant, we can approximate
vkusing v0(since we are estimating local minor move-
ments, the visual feature disparity between IkandI0is not
significant). Consequently, we can compute uk=zk−v0.
In summary, we derive the following system of equations:


zk=P(I0, Ik)
vk=z0
uk=zk−vk(6)
Validation To validate our proposed hypothesis, we care-
fully constructed three distinct scenarios using computer
simulation. Each scenario involves different objects:
Golden leaf, Brown leaf, and Avocado, all of which follow
identical motion trajectories. Notably, the Golden leaf and
Brown leaf share the same geometric shapes but differ in
7134
their color textures. The hypothesis suggests that the neu-
ral motion vector zcan be effectively decomposed into two
components: the motion-related component u(motion seg-
ment ) and the visual-related component v(visual segment ).
We assign the latent motion vector of the Golden leaf as
the source motion, and all variables related to it are marked
with a prime symbol in the upper right corner. Its actual
motion process is demonstrated in Fig. 4c1.
Initially, we present the latent motion trajectories under
the three scenarios in the form of a t-SNE plot, as depicted
in Fig. 4a. It is evident from the figure that these three tra-
jectories do not overlap. More specifically, we observe that
despite the Golden leaf and Brown leaf differing merely in
their texture maps, there is a slight difference between their
latent motion trajectories. The latent motion trajectory of
the Avocado significantly deviates from the other two. If we
attempt to directly drive the Brown leaf and Avocado using
z′
1,···,K, the Brown leaf can still achieve a relatively good
animation effect (Fig. 4d). However, the Avocado will ex-
hibit noticeable deformation in the central core area, mak-
ing it appear shriveled and not plump (Fig. 4e). The up-
per pulp area also exhibits distortion and deformation. This
strongly suggests that the latent motion vector zencapsu-
lates not only motion information but also visual features.
Then, we calculate the visual component v0,Kfor each
scenario. After that, we use Eq. 6 to replace the visual com-
ponent u′
kinz′
0,···,K, which results in a new latent mo-
tion trajectory, {v0, v1+u′
1,···, vK+u′
K}. In Fig. 4b,
we illustrate the t-SNE graph of these new three trajecto-
ries. When compared with the trajectory of the Golden leaf,
they show notable consistency, particularly for the Avocado.
Afterwards, we try to animate the Avocado scenario using
{v0, v1+u′
1,···, vK+u′
K}(See Fig. 4f). It’s clear that
the core of the avocado remains full throughout the motion
process, and the avocado as a whole doesn’t experience any
illogical deformation. This strongly indicates that our pro-
posed method can effectively separate visual features and
motion information.
3.2.2 Background on Diffusion Models
The foundation of our proposed motion diffusion model
is built upon the denoising diffusion probabilistic models
(DDPM) [11, 28, 31].
The forward process in DDPM generates a Markov
chain x1, . . . ,xTby progressively incorporating Gaussian
noise into x0, a sample drawn from the data distribution.
This process follows a variance schedule β1, . . . , β T, as de-
picted below:
q(xt|xt−1) =N(xt;p
1−βtxt−1, βtI), (7)
1We strongly recommend observing the motion process through the
video provided on our homepageIn this equation, the variances βtare kept constant. When
βtis minimal, the posterior q(st−1|xt)can be precisely
approximated by a diagonal Gaussian [18, 28]. Moreover,
if the length of the chain Tis sufficiently large, xTcan
be closely approximated by a standard Gaussian N(0,I).
These insights suggest that the actual posterior q(xt−1|xt)
can be estimated by pθ(xt−1|xt), defined as follows:
pθ(xt−1|xt) =N(xt−1;µθ(xt), σ2
tI), (8)
where the variances σtare also constants.
The Reverse Procedure of DDPM The sampling pro-
cess, also referred to as the reverse procedure of Denois-
ing Diffusion Probabilistic Models (DDPM), is initiated
by generating samples x0∼pθ(x0)from Gaussian noise
xT∼ N(0,I). This is followed by a progressive reduction
of noise through a Markov chain of xT−1,xT−2, . . . ,x0
utilizing the learned pθ(xt−1|xt).
To train pθ(xt−1|xt), Gaussian noise ϵis added to x0to
generate samples xt∼q(xt|x0). Subsequently, a model ϵθ
is trained to predict ϵusing the mean-squared error loss as
follows:
arg min
ϵθEt∼U(1,T),x0∼q(x0),ϵ∼N (0,I)[||ϵ−ϵθ(xt, t)||2],(9)
Here, the time step tis uniformly sampled from
{1, . . . , T }. The µθ(xt)in Eq. 8 can be derived from
ϵθ(xt, t)to model pθ(xt−1|xt)[11].
The denoising model ϵθis typically implemented via a
time-conditioned U-Net [23] with residual blocks [9] and
self-attention layers [33]. The time step tis conveyed to ϵθ
by the sinusoidal position embedding [33]. For conditional
generation, i.e., sampling x0∼pθ(x0|y), ay-conditioned
model ϵθ(xt, t, y)can be learned [18, 22].
3.2.3 Motion Diffusion Model
We commence by encoding the input video clips I0,···,K
intoz0,···,Kusing a proficiently trained motion render-
ing model. As discussed in Para. 3.2.1, directly setting
x0= cat[ z0,···, zK]as the training target for the motion
diffusion model would necessitate the simultaneous memo-
rization of the visual segment and motion segment, thereby
complicating the training process. We discovered that using
vkas supplementary information and concatenating it with
zkfor input while still directly predicting zkyields superior
results compared to directly predicting the motion compo-
nent by setting x0= cat[ u0,···, uK]. Concurrently, other
additional information is converted into the condition vector
yvia a feature encoder, such as CLIP [19] or ImageBind [8].
In this scenario, we chose to employ ImageBind to encode
I0and the optional text into y.
x0is progressively transformed into a standard Gaussian
noise volume n∼ N (0,I)by integrating Gaussian noise
7135
via the DDPM forward process. Conditioned on y, the de-
noising model ϵθ(xt, t, y)is trained to predict the added
noise ϵinxtbased on a conditional 3D U-Net [7] with the
subsequent loss:
arg min
ϵθEt∼U(1,T),x0∼q(z1,···,K),ϵ∼N (0,I)[||ϵ−ϵθ(xt, t, y)||2],
(10)
where the time step tis uniformly sampled from
{1, . . . , T }.ϵθis further employed in the DDPM reverse
sampling process to output ˆx0= cat[ˆ z0,···,ˆz1]. The train-
ing pipeline is depicted in Fig. 2d.
3.3. LVMM Inference
𝜖!(𝑥",𝑣#,⋯,&,𝑡,𝑦)𝒰(1,𝑇)ℰ∘𝒫(𝐼",𝐼")Image Bind𝒩(0,𝐼)𝑧#,⋯,&ℛ𝐼"𝑝'
𝐼"𝒟(𝑧')𝑧"
𝐼6#
𝐼6(
𝐼6'
𝐼6&
Figure 5. Inference pipeline. Starting with a given reference image
I0, a series of video frames ˆI1,···,Kis ultimately rendered through
neural image renderer R.
As demonstrated in Fig. 5, given an image I0, we ini-
tially compute the corresponding latent motion z0:=E ◦
P(I0, I0)and condition y, which subsequently produces
v0=z0andvk=v0from Eq. 6. Conditioned on y, a
volume of randomly sampled Gaussian noise nis progres-
sively denoised by ϵθthrough the DDPM reverse sampling
process to generate the latent motion sequence ˆz1,···,K.
Following this, a video clip ˆI1,···,Kcan be rendered via
ˆIk=R(I0|D(ˆzk)). It is important to note that the optical
flow predictor Pand motion encoder Eare only employed
once to compute z0, and are not used in subsequent steps.
4. Experiments
4.1. Pretraining of Large Visual Motion Model
We have successfully trained two high-performing pre-
trained models, namely “LVMM-General” and “LVMM-
Facial”, on the WebVid10M [2] and CelebV-HD [40]
datasets respectively. The experiments were deployed on 32
A100-80GB GPUs, and to ensure the stability of the train-
ing, we utilized 32-bit floating-point numbers.
Specifically, the “LVMM-General” model was initially
trained on the WebVid10M dataset. We dedicated approxi-
mately one week each to complete Phase- 2a and Phase- 2b
of the training. Given the presence of watermarks in the
WebVid10M data, we randomly selected a subset of data
from the HDVILA-100M [37] dataset for Phase- 2c train-
ing to eliminate this prior in the Neural Image Renderer R.This process took roughly 12 hours. Before the training of
the notion denoising model, we precomputed the required
training data, i.e., the latent motion trajectories z0,···,K. We
utilized 128 V100-32GB GPUs and spent over a week pre-
processing the data. Ultimately, it took us two weeks to
complete Phase- 2d training.
To enhance the LVMM’s capability of modeling facial
motion features, we retrained the “LVMM-General” param-
eters on the CelebV-HD facial dataset. Specifically, we
spent approximately two weeks completing all computa-
tions and training, including data processing, to obtain the
“LVMM-Facial” model.
For more detailed information on the experimental setup,
model parameters, computational efficiency, etc., please re-
fer to the supplementary materials.
4.2. Quantitative Evaluation
In this section, we validate two aspects through rele-
vant evaluation metrics: a) the superiority of the LVMM
model architecture, and b) the significant performance gain
brought by large-scale data training.
Dataset. We conducted experiments primarily on three
specific task datasets, namely MUG [1], MHAD [6], and
NATOPS [30]. The MUG Facial Expression Dataset com-
prises 1,009 videos featuring 52 subjects, each exhibiting
one of seven distinct expressions: anger, disgust, fear, hap-
piness, neutral, sadness, and surprise. The MHAD Human
Action Dataset includes 861 videos of 27 actions performed
by 8 subjects, covering a wide range of human movements,
such as sports actions, hand gestures, daily activities, and
training exercises. The NATOPS Aircraft Handling Signal
Dataset consists of 9,600 videos of 20 subjects performing
24 body-and-hand gestures used for communication with
U.S. Navy pilots.
Baseline. We benchmark LVMM against four competitive
baseline models, namely the GAN-based I2V model ImaG-
INator [35], video diffusion models VDM [12], a variant
of image latent diffusion models LDM [36], and the latent
flow diffusion models LFDM . We maintain the same ex-
perimental configuration as in LFDM for a fair comparison.
For sampling, we employ a 1000-step DDPM for LDM and
LFDM. Given the slow DDPM sampling in the large latent
space of VDM ( 40×64×64×3), we utilize a 200-step
DDIM [29] to expedite the sampling process. The training
image resolution for LVMM is 512×512, and the gener-
ated latent motion trajectory is 40×16×16×4. We use
a 250-step DDIM sampling strategy for LVMM, and any
potential additional text conditions will be encoded through
ImageBind and concatenated with the features of I0. In the
experiments, we thoroughly trained each model on the cor-
responding dataset to ensure convergence.
7136
I2V-GenXLGen-2LVMM
𝑰𝟎(a) Samples generated through training on the SHM dataset.
I2V-GenXLGen-2LVMM
𝑰𝟎
(b) Samples generated through training on the FMM dataset.
Figure 6. Qualitative comparison between I2V-GenXL and Gen-2. In the lower left corner of Fig. 6a and Fig. 6b, we have plotted the
similarity of CLIP features between each frame in the video sequence generated by each method and the given reference image I0. This
clearly demonstrates that LVMM is capable of stably generating video sequences highly relevant to the given content.
Evaluation Metrics Consistent with preceding research
[10,12,13,27], we utilize the Fr ´echet Video Distance ( FVD )
[32] to evaluate the visual quality ,temporal coherence , and
sample diversity of videos synthesized by various meth-
ods. To quantify the extent to which a synthesized video
aligns with the class condition y(condition accuracy ) and
the provided image I0(subject relevance ), we also adapt
two FVD variants, as proposed in [3]: class conditional
FVD ( cFVD ) and subject conditional FVD ( sFVD ). Both
cFVD and sFVD calculate the distance between the feature
distributions of real and synthesized videos under identical
class conditions yor subject images I0, respectively. We
initially compute the cFVD and sFVD for each condition y
and image I0and subsequently report their mean and vari-
ance as the final results.
Quantitative Results Table 1 presents quantitative com-
parisons between our method and the baselines on our test
set of unseen video clips. As we maintain the same experi-
mental settings as LFDM, we directly use the data from thatpaper in the table. In our experimental setup, to ensure an
accurate estimation of the feature distributions, we generate
10,000 videos for LVMM under consideration to compute
our statistics. The data in the table demonstrates that even
when training LVMM directly from scratch on the rele-
vant dataset, our method can achieve superior results, which
fully illustrates the superiority of the LVMM structure de-
sign. Moreover, the model obtained by fine-tuning based on
”LVMM-Facial” significantly outperforms previous single-
image animation baselines in terms of both image and video
synthesis quality. This suggests that the videos generated
with the assistance of the pre-trained model are more realis-
tic and temporally coherent. Visual comparison results can
be found in the supplementary material.
4.3. Qualitative Visualization
In this section, we aim to further elucidate the profi-
ciency of the Local Visual Motion Model (LVMM) in de-
ducing local motion from visual features. Despite the ex-
tensive pre-training of LVMM on large-scale datasets, the
existing datasets, be it WebVid10M or CelebV-HD, do not
7137
ModelMUG MHAD NATOPS
FVD↓ cFVD↓ sFVD↓ FVD↓ cFVD↓ sFVD↓ FVD↓ cFVD↓ sFVD↓
ImaGINator [35] 170.73 257.46±62.88 319.37±95.23 889.48 1406.56±260.70 1175.74±327.99 721.17 1122.13±150.74 1042.69±416.16
VDM [12] 108.02 182.90±69.56 213.59±97.70 295.55 531.20±104.25 398.09±121.16 169.61 410.71±105.97 350.59±125.03
LDM [22] 123.88 196.49±66.99 236.26±76.08 280.26 515.29±125.70 427.03±112.31 251.72 506.40±125.08 491.37±231.85
LFDM [17] 27.57 77.86±20.27 108.36±39.60 152.48 339.63±52.88 242.61±28.50 160.84 376.14±106.13 324.45±116.21
LVMM†23.47 70.32±19.45 102.78±38.45 143.67 315.72±45.34 235.89±23.40 157.92 365.78±104.25 305.67±113.89
LVMM‡15.34 54.63±16.13 84.23±27.12 124.56 275.89±34.21 204.75±19.76 144.46 305.12±99.53 277.78±105.07
Table 1. Quantitative comparison. “ †” denotes training model from scratch, “ ‡” signifies fine-tuning the “LVMM-Facial” weights.
offer scenes with distinct motion patterns, thereby imped-
ing the training of high-caliber model parameters. As a re-
sult, models trained on these datasets merely serve as pre-
training parameters and lack direct applicability.
To rectify the issue of training data, we strive to develop
new datasets encapsulating precise and distinct motion in-
formation apt for optical flow estimation. We have devised
the Simple Harmonic Motion dataset ( SHM ), targeting the
omnipresent simple harmonic motion in nature, and the Fa-
cial Muscle Movements dataset ( FMM ), concentrating on
muscle motion during facial expression alterations. The
SHM encompasses numerous close-up, high-definition im-
ages of flora swaying in the wind, with objects persistently
visible throughout the motion, thereby facilitating superior
optical flow computation. The dataset ultimately includes
approximately 1500 videos ranging from a few seconds to
several minutes. Conversely, the FMM comprises high-
definition short videos featuring frontal human portraits.
Apart from a rich assortment of facial expressions, all other
parts of the videos, including the background, remain as
static as possible. This dataset eventually includes video
clips of approximately 100 distinct individuals. It is worth
noting that scenes with severe appearance alterations during
motion are omitted from these two datasets. Supplementary
material provides additional information.
Our work is primarily juxtaposed with other large-
scale data-trained video generation models, namely I2V-
GenXL [16, 34] and Gen-2. I2V-GenXL is currently the
sole publicly available video generation model, for which
we utilized the official project code2to generate videos. For
Gen-2, we employed their provided API service3for video
generation. Since both models utilize undisclosed training
data and other implementation details remain unknown, we
solely provide a visual comparison of the videos generated
by different methods herein.
We fine-tuned “LVMM-General” and “LVMM-Facial”
on SHM and FMM for approximately one week, respec-
tively, as depicted in Fig. 6a and Fig. 6b. As can be dis-
cerned, compared to directly predicting video streams, our
video generation approach by predicting latent motion tra-
jectory can more accurately adhere to the given image con-
2https://modelscope.cn/models/damo/Image-to-Video
3https://research.runwayml.com/gen2tent and generate more detailed and controllable motion
processes, thereby fully demonstrating the superiority of
our algorithm.
4.4. Ablation Study
We visually demonstrate the role of each component in
the Motion Rendering Model, primarily corroborating the
viewpoints discussed in Section 3.1. Given the input refer-
ence image I0(Fig. 3a), we aim for the Neural Image Ren-
dererRto render some features not visible in I0, such as the
clear teeth shown in Fig. 3b. If we attempt to predict only
the optical flow δx,y, we find that this results in a notice-
able image blur (Fig. 3c). If we predict the intention map
ωbut directly use the warp function Ffor image rendering,
this can generate some motion effects on the input image
but fails to accomplish complex motion information, such
as opening the subject’s mouth (Fig. 3d). Our final result is
shown in Fig. 3e. With rendering based on the neural image
renderer, we can achieve more complex facial feature trans-
formations, such as opening the mouth or closing the eyes.
Our Motion Encoder and Decoder structure can compress
the optical flow into a latent motion vector with impercepti-
ble errors (Fig. 3f). By further fine-tuning the Neural Image
Renderer R(based on the reconstructed optical flow ˆp), we
can enhance our image rendering capabilities. As shown in
Fig. 3g, the teeth features in the subject’s lip area become
noticeably clearer (note: these teeth are not visible in the
input image I0). This effectively demonstrates the neces-
sity of each component within the LVMM architecture. The
samples are sourced from the CelebV-HQ dataset [41].
5. Discussion and Conclusion
Limitations Although a generalized Motion Diffusion
Model has been trained, it is yet incapable of directly pro-
ducing high-quality motion trajectories encompassing arbi-
trary motion forms in a zero-shot manner.
Conclusion We have introduced the Large Visual Mo-
tion Model (LVMM), empowering it to learn from large-
scale prior data. LVMM exhibits the capability of captur-
ing local motion trends across various real-world scenarios,
generating high-quality motion trajectories from provided
images, and rendering realistic dynamic effects.
7138
References
[1] Niki Aifanti, Christos Papachristou, and Anastasios De-
lopoulos. The mug facial expression database. In 11th In-
ternational Workshop on Image Analysis for Multimedia In-
teractive Services WIAMIS 10 , pages 1–4. IEEE, 2010. 6
[2] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In IEEE International Conference on
Computer Vision , 2021. 6
[3] Yaniv Benny, Tomer Galanti, Sagie Benaim, and Lior Wolf.
Evaluation metrics for conditional image generation. In-
ternational Journal of Computer Vision , 129(5):1712–1731,
2021. 7
[4] Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen
Huang, Yongbin Li, Chuanrui Wang, Fan Cui, and Yang You.
Colossal-ai: A unified deep learning system for large-scale
parallel training. arXiv preprint arXiv:2110.14883 , 2021. 1
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 1
[6] Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. Utd-
mhad: A multimodal dataset for human action recognition
utilizing a depth camera and a wearable inertial sensor. In
2015 IEEE International conference on image processing
(ICIP) , pages 168–172. IEEE, 2015. 6
[7] ¨Ozg¨un C ¸ ic ¸ek, Ahmed Abdulkadir, Soeren S Lienkamp,
Thomas Brox, and Olaf Ronneberger. 3d u-net: learning
dense volumetric segmentation from sparse annotation. In
International conference on medical image computing and
computer-assisted intervention , pages 424–432. Springer,
2016. 6
[8] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan
Misra. Imagebind: One embedding space to bind them all.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 15180–15190, 2023.
5
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 5
[10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 7
[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840–6851, 2020. 1, 5
[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
fusion models. arXiv preprint arXiv:2204.03458 , 2022. 6, 7,
8
[13] Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move:
controllable image-to-video generation with text descrip-tions. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 18219–18228,
2022. 7
[14] Daniel Martin Katz, Michael James Bommarito, Shang Gao,
and Pablo Arredondo. Gpt-4 passes the bar exam. Available
at SSRN 4389233 , 2023. 1
[15] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,
Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foun-
dation models: From specialists to general-purpose assis-
tants. arXiv preprint arXiv:2309.10020 , 1, 2023. 1
[16] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,
Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and
Tieniu Tan. Videofusion: Decomposed diffusion mod-
els for high-quality video generation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2023. 8
[17] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and
Martin Renqiang Min. Conditional image-to-video gener-
ation with latent flow diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18444–18455, 2023. 8
[18] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 5
[19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1, 5
[20] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
Yuxiong He. Deepspeed: System optimizations enable train-
ing deep learning models with over 100 billion parame-
ters. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining , pages
3505–3506, 2020. 1
[21] Mr D Murahari Reddy, Mr Sk Masthan Basha, Mr M Chin-
naiahgari Hari, and Mr N Penchalaiah. Dall-e: Creating im-
ages from text. UGC Care Group I Journal , 8(14):71–75,
2021. 1
[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 1, 5, 8
[23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention , pages 234–241.
Springer, 2015. 5
[24] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487 , 2022. 1
7139
[25] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. Advances in Neural In-
formation Processing Systems , 35:25278–25294, 2022. 1
[26] Aliaksandr Siarohin, Oliver J Woodford, Jian Ren, Menglei
Chai, and Sergey Tulyakov. Motion representations for ar-
ticulated animation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
13653–13662, 2021. 2
[27] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-
seiny. Stylegan-v: A continuous video generator with the
price, image quality and perks of stylegan2. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 3626–3636, 2022. 7
[28] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning , pages 2256–2265. PMLR, 2015.
5
[29] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 6
[30] Yale Song, David Demirdjian, and Randall Davis. Tracking
body and hands for gesture recognition: Natops aircraft han-
dling signals database. In 2011 IEEE International Confer-
ence on Automatic Face & Gesture Recognition (FG) , pages
500–506. IEEE, 2011. 6
[31] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in Neural
Information Processing Systems , 32, 2019. 5
[32] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,
Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To-
wards accurate generative models of video: A new metric &
challenges. arXiv preprint arXiv:1812.01717 , 2018. 7
[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 5
[34] Xiang* Wang, Hangjie* Yuan, Shiwei* Zhang, Dayou*
Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli
Zhao, and Jingren Zhou. Videocomposer: Compositional
video synthesis with motion controllability. arXiv preprint
arXiv:2306.02018 , 2023. 8
[35] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza
Dantcheva. Imaginator: Conditional spatio-temporal gan for
video generation. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision , pages
1160–1169, 2020. 6, 8
[36] Yaohui Wang, Di Yang, Francois Bremond, and Antitza
Dantcheva. Latent image animator: Learning to ani-
mate images via latent space navigation. arXiv preprint
arXiv:2203.09043 , 2022. 6
[37] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,
Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-
vancing high-resolution video-language representation withlarge-scale video transcriptions. In International Conference
on Computer Vision and Pattern Recognition (CVPR) , 2022.
6
[38] Chenshuang Zhang, Chaoning Zhang, Sheng Zheng,
Mengchun Zhang, Maryam Qamar, Sung-Ho Bae, and In So
Kweon. A survey on audio diffusion models: Text to
speech synthesis and enhancement in generative AI. CoRR ,
abs/2303.13336, 2023. 1
[39] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie
Zhang, Zican Dong, et al. A survey of large language mod-
els.arXiv preprint arXiv:2303.18223 , 2023. 1
[40] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei
Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-
hq: A large-scale video facial attributes dataset. In European
conference on computer vision , pages 650–667. Springer,
2022. 6
[41] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei
Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebV-
HQ: A large-scale video facial attributes dataset. In ECCV ,
2022. 8
7140
