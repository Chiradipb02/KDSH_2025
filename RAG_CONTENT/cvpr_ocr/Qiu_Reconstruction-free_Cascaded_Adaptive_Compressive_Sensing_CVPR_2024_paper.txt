Reconstruction-free Cascaded Adaptive Compressive Sensing
Chenxi Qiu, Tao Yue, Xuemei Hu
School of Electronic Science and Engineering, Nanjing University, Nanjing, China
chenxiqiu@smail.nju.edu.cn, yuetao@nju.edu.cn, xuemeihu@nju.edu.cn
Abstract
Scene-aware Adaptive Compressive Sensing (ACS) has
constituted a persistent pursuit, holding substantial promise
for the enhancement of Compressive Sensing (CS) perfor-
mance. Cascaded ACS furnishes a proÔ¨Åcient multi-stage
framework for adaptively allocating the CS sampling based
on previous CS measurements. However, reconstruction is
commonly required for analyzing and steering the succes-
sive CS sampling, which bottlenecks the ACS speed and im-
pedes the practical application in time-sensitive scenarios.
Addressing this challenge, we propose a reconstruction-
free cascaded ACS method, which requires NO reconstruc-
tion during the adaptive sampling process. A lightweight
Score Network (ScoreNet) is proposed to directly deter-
mine the ACS allocation with previous CS measurements
and a differentiable adaptive sampling module is proposed
for end-to-end training. For image reconstruction, we pro-
pose a Multi-Grid Spatial-Attention Network (MGSANet)
that could facilitate efÔ¨Åcient multi-stage training and infer-
encing. By introducing the reconstruction-Ô¨Ådelity supervi-
sion outside the loop of the multi-stage sampling process,
ACS can be efÔ¨Åciently optimized and achieve high imaging
Ô¨Ådelity. The effectiveness of the proposed method is demon-
strated with extensive quantitative and qualitative experi-
ments, compared with the state-of-the-art CS algorithms.
1. Introduction
Compressive sensing provides an efÔ¨Åcient way to sample
the scene information with sub-Nyquist rate [ 13], which
has been applied in a wide range of research Ô¨Åelds, such as
medical imaging [ 29], wireless broadcasting [ 27], ultrafast
photography [ 17] and video snapshot compressive imag-
ing [47,48]. CS methods with uniform sampling [ 8,26,43,
52,56,59] propose to sample each region of the image with
the same sampling rate. Since the complexity and content in
different image regions are distributed non-uniformly, adap-
tively allocating different sampling rates based on scene-
dependent information is highly promising to realize efÔ¨Å-
cient CS with high reconstruction Ô¨Ådelity. Therefore, differ-
ent adaptive sampling methods are proposed [ 6,34,38,49].
(a)
Œ¶i    
x
yi x'n
x yi
Œ¶i    
ùìù Reconstruction
fidelity
supervision(b)ùìùùëñ y1, ..., yn x'nFigure 1. The scheme comparison of n-stage ACS frameworks,
(a) the existing ACS methods with ntimes of reconstruction of
x‚Ä≤
1,x‚Ä≤
2, ...,x‚Ä≤
nin the adaptive sampling loop, and (b) the pro-
posed method which requires no reconstruction during the sam-
pling loop. Only the Ô¨Ånal image reconstruction with all adaptive
measurements, i.e., y1,y2, ..,yn, is required. RA denotes the
sampling rate allocation module and Nidenotes the reconstruc-
tion at the i-th adaptive sampling stage.
Due to the efÔ¨Åciency of ACS in utilizing scene-dependent
information, it has been applied in various Ô¨Åelds, such as
medical imaging [ 33], hyperspectral imaging [ 19], terahertz
(THz) imaging [ 44] and 3D imaging [ 11,35].
Generally, adaptive sampling is performed in multi-
stage, reconstruction based on previous measurements is re-
quired for determining the subsequent adaptive sampling.
Existing works propose to analyze the texture or saliency
distribution on the previously reconstructed coarse image
and allocate higher sampling rates to the regions with richer
textures [ 1,6,34,38,49]. However, the requirement of
image reconstruction in the loop of the adaptive sampling
process prevents ACS from efÔ¨Åcient sampling for time-
sensitive scenarios. As shown in Fig. 1(a), for the existing
multi-stage ACS process with nstages, reconstruction lies
in the loop of the ACS process, which is required ntimes
for the successive reconstruction of the image and bottle-
necks the imaging speed of ACS for practical applications.
In this paper, we propose a reconstruction-free cascaded
ACS framework, which requires NO reconstruction during
the multi-stage adaptive sampling process. As shown in
Fig.1(b), during the sampling process, the adaptive alloca-
tion is determined directly based on the previous CS mea-
surements, and only one reconstruction is required for the
Ô¨Ånal image reconstruction. SpeciÔ¨Åcally, our method pro-
poses a lightweight ScoreNet to score each block based on
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2620
the previous measurements. A linear programming (LP)-
based differentiable adaptive sampling module (ASM) is
proposed to implement CS sampling based on the scores,
enabling end-to-end optimization of the ACS framework.
To realize efÔ¨Åcient reconstruction for training and inferenc-
ing, we propose a Multi-Grid Spatial-Attention-based re-
construction Network, i.e., MGSANet. The out-of-loop su-
pervision is introduced during the training process to pro-
mote the convergence of the overall network. In all, our
contributions are concluded as below.
‚Ä¢ We propose a reconstruction-free cascaded ACS frame-
work that requires NO reconstruction during the adaptive
sampling process and overcomes the bottleneck of imag-
ing speed for multi-stage ACS methods.
‚Ä¢ We design a differentiable adaptive sampling method,
composed of a lightweight ScoreNet and LP-based adap-
tive sampling module, that enables end-to-end optimiza-
tion of the reconstruction-free ACS framework.
‚Ä¢ We develop an efÔ¨Åcient MGSANet for CS reconstruction
to enable efÔ¨Åcient training and inferencing. By introduc-
ing an out-of-loop reconstruction Ô¨Ådelity supervision dur-
ing the training process, the proposed ACS framework is
optimized with high-Ô¨Ådelity imaging performance.
‚Ä¢ The effectiveness of the proposed method is extensively
demonstrated by comparing it with state-of-the-art meth-
ods and ablation studies.
2. Related work
CS methods with adaptive sampling. Due to the poten-
tial of signiÔ¨Åcant improvement in the sampling efÔ¨Åciency
of CS with scene-dependent information, ACS has been
explored in many Ô¨Åelds, such as ghost imaging [ 1], med-
ical imaging [ 33], hyperspectral imaging [ 19], 3D imag-
ing [11,35], and terahertz (THz) imaging [ 44], demonstrat-
ing the superiority of introducing adaptive sampling. Differ-
ent from uniformly sampling each region of the image, how
to design the ACS framework for efÔ¨Åciency and high Ô¨Ådelity
is still an open problem. Existing works propose to sample
the image under a two-stage or multiple-stage framework,
which realizes the adaptive sampling based on the texture
or saliency analysis of the previous reconstruction results.
SpeciÔ¨Åcally, several two-stage ACS models [ 2,6,55] are
proposed, which uniformly sample the original image and
reconstruct the coarse image in the Ô¨Årst stage. Then, adap-
tive sampling based on the analysis of the coarse recon-
struction image is implemented in the second stage. Beyond
two-stage ACS models, multi-stage-based ACS frameworks
are proposed, where adaptive sampling allocation can be
achieved by successively accumulating information from
the scene, promising efÔ¨Åcient utilization of scene-dependent
information. Based upon the reconstructed results of the
previous measurements, texture analysis based on wavelet
transform [ 1,11,19,38,41,49,54], Fourier transform [ 24],DCT [ 28], gradient domain [ 35], edge detection [ 44], and
Ô¨Çuorescence signal domain [ 3] is introduced for steering the
adaptive sampling allocation to regions with abundant tex-
tures. Furthermore, Qiu et al. [36,37] proposes a multi-
stage ACS model to allocate the sampling rate based on
the measurement error. However, within these multi-stage
adaptive sampling processes, image reconstruction based
upon previous measurements is commonly required for de-
termining the adaptive allocation of the next stage, which
bottlenecks the speed of the sampling process and prevents
practical time-sensitive scenarios. In this paper, we pro-
pose a reconstruction-free cascade ACS framework, which
realizes multi-stage ACS without requiring reconstruction
during the adaptive sampling process.
CS Reconstruction Neural Network. With the success
of deep learning in computer vision, a series of CS recon-
struction algorithms based on deep neural networks [ 10,
15,26] have been proposed, which largely improve the ef-
Ô¨Åciency of CS compared to traditional optimization-based
algorithms [ 7,12,31,45]. Recently, transformer [ 46] has
achieved great success in the Ô¨Åeld of natural language pro-
cessing, which has also been introduced into CS recon-
struction algorithms [ 16,39,52] to capture long-range de-
pendencies. Besides, deep unfolding networks (DUN) that
combine traditional optimization algorithms with deep neu-
ral networks [ 8,9,42,53,56,57,59] or transformer [ 43]
are proposed and achieve state-of-the-art performance in
CS reconstruction quality. Owing to the proÔ¨Åciency of
GPUs in parallel processing, a model with extensive par-
allelization can achieve markedly greater acceleration than
a less parallelized model under identical Ô¨Çoating point op-
erations (FLOPs) [ 30]. Therefore, several multi-branch net-
works [ 18,32] are proposed for their high efÔ¨Åciency, but are
underexplored in CS reconstruction. In this paper, we pro-
pose a multi-grid spatial attention network, with high paral-
lelism, to achieve both efÔ¨Åcient training and reconstruction.
3. Reconstruction-free cascaded ACS method
In this section, we detail the proposed reconstruction-free
cascaded ACS method. SpeciÔ¨Åcally, we introduce the
proposed reconstruction-free cascaded ACS framework in
Sec. 3.1, the proposed differentiable adaptive sampling
method in Sec. 3.2, and the MGSANet-based efÔ¨Åcient re-
construction network with the designed supervision and
training strategy of the overall framework in Sec. 3.3.
3.1. Reconstruction¬≠free cascaded ACS framework
As shown in Fig. 2, we propose a multi-stage framework
for realizing reconstruction-free ACS. The overall ACS
framework is composed of three main parts, including the
multi-stage ACS backbone, the forward adaptive sampling
method, and the Ô¨Ånal image reconstruction with the input
2621
Reconstruction
network
Recontruction image
Reconstruction
networkGround truthReconstruction 
fidelity loss
(b)
ùìù1 
ùìùk‚àí1 Reconstruction 
fidelity loss
STM
Œ¶ùëñ √ó 
Padding
with 
zerosMeasu-
rements
Adaptive Sampling ModuleTi
C
C ...Concatenate
(a)ykS1
Sk-1Uniform 
sampling
Adaptive 
Sampling
ModuleOriginal image x
ScoreNet
Adaptive 
Sampling
Module
y1
y2s1
sk-1ScoreNet
1-st stage
2-rd stage
k-th stage...BB
H√óWH/B√óW/B√óT 1
H/B√óW/B√óT 2
H/B√óW/B√óT kH/B√óW/B
H/B√óW/B(c)1
0
xa,byia,b
Mia,bMi
 
yisi
xFigure 2. The overview of the proposed reconstruction-free adaptive CS method, the multi-stage adaptive sampling can be conducted
without reconstruction until the target CS sampling rate is achieved. (a) The multi-stage ACS sampling process, (b) the reconstruction
process and the out-of-loop reconstruction-Ô¨Ådelity supervision during the training process, and (c) the structure of the ASM.
3√ó3 ConvSoftmax
Depthwise
7√ó7 ConvLayerNorm1√ó1 CONVGELU1√ó1 CONV
+3√ó3 Conv+
LeakyReluConvNeXt 
Block
3√ó3 Conv+
LeakyReluConvNeXt Block
Figure 3. The network structure of ScoreNet.
of all the ACS measurements. Furthermore, the forward
adaptive sampling method is composed of ScoreNet and an
adaptive sampling module, which Ô¨Årst scores the previous
measurements of each block and then implements the adap-
tive sampling based on the score. SpeciÔ¨Åcally, for the over-
all ACS process, the original image x‚ààRH√óWis divided
into several non-overlap blocks and Ô¨Çattened to vector form
asxa,b‚ààRB2√ó1, whereHandWare the height and width
of the original image, Bis the block size, a‚àà[1,H
B]and
b‚àà[1,W
B]are the horizontal and vertical indexes of the
block, where a,b‚ààZ. In the Ô¨Årst stage of ACS, we pro-
pose to uniformly sample each block with the same sam-
pling rate to generate uniform measurements y1. Then, the
ScoreNet S1scores the measurements of each block and
outputs the scores s1‚ààRH
B√óW
B. After that, the adaptive
sampling module samples each block at different sampling
rates according to the scores. Before reaching the target
sampling stages, multiple loops of ACS are repeated. In
each loop, image blocks are scored with the accumulated
ACS measurements, i.e. si‚àí1=Si‚àí1(y1,...,yi‚àí1), and
the next adaptive sampling processes are conducted based
upon the score, i.e., yi= ASM( si‚àí1,Œ¶i,x). Finally, the
reconstruction network reconstructs the target image with
all ACS measurements, i.e. x‚Ä≤
k=Nk‚àí1(y1,...,yk).
To realize the optimization of the proposed ACS frame-
work in an end-to-end way, two main challenges are re-
quired to be overcome: 1) how to design the differentiable
adaptive sampling method for end-to-end training, 2) howto design the reconstruction network, supervision, and train-
ing strategy for efÔ¨Åcient training, promoting the optimal
convergence of the proposed reconstruction-free cascaded
ACS framework.
3.2. LP¬≠based differentiable adaptive sampling
To realize adaptive sampling based on previous measure-
ments, we propose two modules, i.e., the ScoreNet module
and the adaptive sampling module, which score each block
with previous measurements and implement adaptive sam-
pling based on the score. As for the ScoreNet shown in
Fig. 3, to avoid introducing too heavy computation, which
may hinder the practical application of the proposed ACS,
we propose a lightweight architecture, which uses two Con-
vNeXt [ 51] blocks to extract the features of the scores. A
Softmax layer is equipped in the Ô¨Ånal layer to ensure the
sum of the output scores equals 1. Then, an adaptive sam-
pling module is required allocate the total number of mea-
surements according to the score. To simplify the prob-
lem, instead of making the capture-or-not decision for the
measurements of a block in the next stage elementwisely,
we propose to decide the number of required measurement
of the block and select the Ô¨Årst corresponding number of
rows of the whole measurement matrix Œ¶i‚ààRTi√óB2to
form the real measurement matrix of the block at stage i,
as shown in Fig. 2(c).Tiis a hyper-parameter constrain-
ing block measurement counts, ensuring the sampling rate
does not exceed 1. SpeciÔ¨Åcally, we introduce a Score To
Mask (STM) module to generate a binary selection mask
Mibased on si. The measurement of each image block
is thusya,b
i=Ma,b
i‚äô(Œ¶ixa,b), where‚äôdenotes the dot
product. Through introducing an auxiliary variable
Œ∑i=C[si‚àí1;si‚àí1‚àí1/mi;...;si‚àí1‚àí(Ti‚àí1)/mi],
(1)
2622
PReLUCONV
CONV
++(e)D UD U
SASASA
++
+
DD
SASASA
UUSA
SA
SA++
+ ++
+++
+ CONVPixel
Shuffle1√ó1
CONV
Reconstruction 
image
H/4√óW/4√ó128H/2√óW/2√ó64H√óW√ó32 Measurements
++
+++
+
D
SA ++
+D
SA SAU U++
+ ++
+
H/8√óW/8√ó256
√ó√óPReLU
5√ó5 CONV
stride=2
DRB(b) (c)
 1√ó1 CONV
PixelShuffleDRB
++ cc Concatante
Broadcasting matrix productsAddU D
Sigmoid7√ó7 CONV
√ó(f) SA
Ave 
PoolMax Pool
c
c
RB1√ó1
CONVRB
cccccc
RB
RB(d) DRB
(a)Figure 4. The overall structure of the proposed MGSANet. (a) The backbone of MGSANet, (b) upsampling module, (c) downsampling
module, (d) Dense Residual Block, (e) Residual Block, and (f) Spatial Attention module.
whereC[¬∑]is the concatenate operation in the third di-
mension and Œ∑i‚ààRH
B√óW
B√óTi.miis the total number
of measurements at stage i. For a given sampling rate ri
ofi-th sampling stage and an image with H√óWpixels,
mi=H√óW√óri. We propose the STM module as
Mi= Binarize( Œ∑i‚àíœÑi), (2)
whereœÑiis themi-th largest value in Œ∑i. However, Eq. ( 2)
is non-differentiable. To address this issue, we propose to
construct an integer linear programming problem with the
solution equal to Eq. ( 2), which can be differentiated with
the perturbed optimizer [ 5]. The constructed LP problem is
arg max
Mi‚ààC‚ü®Mi,Œ∑i‚ü©,
s.t.C={Mi‚àà {0,1}H
B√óW
B√óTi:/summationdisplay
a,b,tMa,b,t
i=mi,
Ma,b,t
i‚â•Ma,b,t+1
i,‚àÄt‚àà {1,...,Ti‚àí1}},
(3)
wheretindexes the third dimension of Mi.Cis the convex
polytope set that meets two conditions. The Ô¨Årst condition
constrains the total number of selected measurements to be
mi, and the second condition denotes that we select the
Ô¨Årst several rows of the sampling matrix Œ¶i. In the train-
ing process, the forward and backward propagation of the
LP-based differentiable STM are deÔ¨Åned below.
Forward propagation:
Mi=EZ/bracketleftbigg
arg max
Mi‚ààC‚ü®Mi,Œ∑i+œÉZ‚ü©/bracketrightbigg
,
=Q/summationdisplay
q=1[STM(Œ∑i+œÉZq)],(4)
whereQdifferent uniform Gaussian noise Zqis added to
perturb the input Œ∑iand the output is the expectation of the
output of the LP module. œÉandQare hyper-parameters.
Backward propagation: The backpropagation can be
achieved with the Jacobian matrix and the Jacobian of theabove forward propagation can be calculated as
JsMi=EZ/bracketleftbigg
arg max
Mi‚ààC‚ü®Mi,Œ∑i+œÉZ‚ü©ZT/œÉ/bracketrightbigg
,
=Q/summationdisplay
q=1/bracketleftBig
STM(Œ∑i+œÉZq)ZT
q/œÉ/bracketrightBig
.(5)
3.3. MGSANet¬≠based reconstruction network
For efÔ¨Åcient training and inferencing, we propose a multi-
grid spatial attention network, as shown in Fig. 4. Due to
GPUs‚Äô parallel computing strength, networks with greater
parallelism achieve faster acceleration than less parallel
models at the same FLOPs [ 30]. Consequently, we propose
the integration of a multi-grid structure as the fundamen-
tal backbone of our architecture. This structure facilitates
the distribution of features across multiple branches, en-
abling parallel processing, as depicted in Fig. 4(a). Specif-
ically, we use the downsampling module and the upsam-
pling module to generate multi-scale features as shown in
Fig. 4(b) and Fig. 4(c). A dense residual block (DRB) is
incorporated for feature processing within each downsam-
pling or upsampling module. The DRB comprises 3 resid-
ual blocks (RB) [ 20] with dense connections [ 21], as shown
in Fig. 4(d) and Fig. 4(e). The downsampling operation is
implemented through a convolution layer with the stride set
to 2, and the upsampling operation is performed using a
pixel shufÔ¨Çe layer [ 40], in conjunction with a convolution
layer with a 1 √ó1 kernel.
Besides, to enhance the model‚Äôs capability of focusing
on the informative regions, we incorporate the spatial at-
tention (SA) module [ 50] within the horizontal branches as
shown in Fig. 4(f). We use 4 scales with 2√óscale factor
between two adjacent horizontal branches, and the sizes
of features in each horizontal branch are H√óW√ó32,
H/2√óW/2√ó64,H/4√óW/4√ó96, andH/8√óW/8√ó128
from top to bottom respectively.
2623
Loss function. For each adaptive stage, we use l1loss as
pixel loss to supervise the reconstructed result x‚Ä≤
i. Besides,
to reconstruct visually pleasing results, we introduce Struc-
ture Similarity Index Measure (SSIM) loss [ 60]. The total
reconstruction Ô¨Ådelity loss function is
Li=Lpixel
i+Œ≤LSSIM
i
=‚à•x‚Ä≤
i‚àíx‚à•1+Œ≤(1‚àíSSIM(x‚Ä≤
i,x)),(6)
whereŒ≤is the loss-balancing hyper-parameter.
Out-of-loop reconstruction-Ô¨Ådelity supervision. As for
the training process, we propose to train the ScoreNet stage-
by-stage. After Ô¨Ånishing the training of the current stage,
we Ô¨Åx the parameters of the current stage and train the
next stage. Since there is no image reconstruction in the
adaptive sampling loop, introducing sufÔ¨Åcient supervision
to promote the convergence of the proposed ACS frame-
work is important. In our paper, we propose reconstruction
Ô¨Ådelity supervision outside the multi-stage ACS loop to op-
timize the ScoreNet in the training process. As shown in
Fig.2, the reconstruction Ô¨Ådelity loss of each ACS stage is
introduced out-of-loop to supervise and promote the con-
vergence of the training process. Note that supervision is
only introduced during the training process, and NO recon-
struction is required during the adaptive sampling process.
Training strategy. Furthermore, the training process of
each adaptive stage is divided into two phases: end-to-end
training and Ô¨Åne-tuning. In the Ô¨Årst phase, and we end-to-
end train all parameters, including the sampling matrix, the
ScoreNet, and the reconstruction network. Qis set to 500.
Miis not binary but an averaged value of perturbed inputs
which is different from the testing process, so in the Ô¨Årst
phase we linearly decay œÉfrom 0.005 to 0 when training
the ScoreNet to keep consistent with the testing process.
Besides, to avoid overÔ¨Åtting, we randomly shufÔ¨Çe the mask
Miin the third dimension. In the second phase, we Ô¨Åx the
parameters of ScoreNet and Ô¨Åne-tune the sampling matrix
and reconstruction network.
4. Experiments
4.1. Implementaion details
For the network training, we use the same training dataset
with [ 10] which contains BSDS500 [ 4] train dataset and the
VOC2012 [ 14] train dataset. We randomly crop 128√ó128
sub-image from the training dataset in the training pro-
cess. We use Adam optimizer [ 25] to train our model with
Œ≤1= 0.9,Œ≤2= 0.999andœµ= 1√ó10‚àí8. The Y channel
of the images in the YUV color space is utilized. The batch
size is set as 32. The block size Bis set to8. We train
a 5-stage model with a 5% sampling rate for each stage,
so adaptive sampling rates of 10%, 15%, 20%, and 25%
can be achieved with only one model. The maximum sam-
pling rate of a block is 100% and the maximum number ofmeasurements is evenly allocated to each adaptive stage, i.e.
T1= 3,T2= 15 ,T3= 15 ,T4= 16 , andT5= 15 . The
loss-balancing hyper-parameter Œ≤is empirically set to 0.1.
For the end-to-end training phase of each adaptive stage, we
train 100 epochs, the initial learning rate is set to 2√ó10‚àí4
and multiplied by 0.8for every 25 epochs. For the Ô¨Ånetun-
ing phase of each adaptive stage, we train 300 epochs, the
initial learning rate is set to 2√ó10‚àí4and multiplied by 0.5
at the 150, 250, 280, and 290 epochs. Two commonly used
test sets Set11 [ 26] and Urban100 [ 22] are adopted for eval-
uating the performance. We use the Peak Signal-to-Noise
Ratio (PSNR) and SSIM to evaluate the quality of the re-
construction results. All the experiments are implemented
on the PyTorch platform with an Intel XEON Gold 6326
CPU and an NVIDIA RTX 4090 GPU.
4.2. Comparisons with state¬≠of¬≠the¬≠art CS methods
We compare our proposed model with state-of-the-art
(SOTA) non-adaptive CS methods and ACS methods pro-
posed in recently years. The non-adaptive CS methods
includes AMP-Net [ 59], OPINE-Net+[57], COAST [ 53],
NL-CS [ 10], MADUN [ 42], TransCS [ 39], FSOINet [ 8],
CSFormer [ 52], TCS-Net [ 16] and OCTUF+[43], while
the ACS methods includes ACCSNet [ 37], CASNet [ 6] and
AMS-Net [ 58]. It is worth mentioning that AMS-Net [ 58]
designs its adaptive sampling scheme with the ground truth
image accessible, which limits its applicability in many
scenarios. The quantitative comparison is summarized in
Tab. 1, we can observe that our proposed model can outper-
form the SOTA CS methods at the sampling rates of 10%,
15%, 20% and 25%. SpeciÔ¨Åcally, on the Set11 test set, our
proposed model can outperform AMP-Net, OPINE-Net+,
COAST, NL-CS, MADUN, CASNet, TransCS, FSOINet,
CSFormer, TCS-Net, OCTUF+, CASNet and AMS-Net
by 1.88 dB/0.026, 1.67 dB/0.0202, 1.73 dB/0.0192, 1.48
dB/0.0121, 1.09 dB/0.01, 1.65 dB/0.0183, 0.79 dB/0.83, 2.2
dB/0.0195, 2.59 dB/0.0239, 0.51 dB/0.007, 0.93 dB/0.0091
and 0.14dB/0.0191 in terms of PSNR/SSIM for average, re-
spectively. Furthermore, we compare our proposed method
with SOTA CS methods on the Urban100 test set which con-
tains 100 more textured architectural images with high res-
olutions. The texture distributions are quite non-uniform in
the high-resolution images, which leads to the non-uniform
sampling rate of different regions required for the high-
quality reconstruction. As shown in Tab. 1, beneÔ¨Åts from
the learned ScoreNet, our proposed methods can achieve
different sampling rates in different regions, thus can out-
perform the SOTA CS methods with a large margin in the
Urban100 test set. Fig. 5shows the visual reconstruction re-
sults, we can observe that the reconstruction results of our
proposed method are closer to the ground truth and have
clearer texture details. In all, through comparison with the
SOTA methods, we demonstrate the superiority of our pro-
posed method both quantitatively and qualitatively.
2624
Table 1. Performance comparison with state-of-the-art CS algorithms on Set11 [ 26] and Urban100 [ 22] test sets.
Datasets Methods10% 15% 20% 25% Average
PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM
Set11Non-adaptive
samplingAMP-Net [ 59] 29.40/0.8779 31.56/0.9119 33.27/0.9338 34.63/0.9481 32.22/0.9179
OPINE-Net+[57]29.81/0.8884 31.73/0.9176 33.32/0.9381 34.86/0.9509 32.43/0.9237
COAST [ 53] 30.01/0.8963 31.99/0.9217 33.50/0.9399 33.98/0.9407 32.37/0.9247
NL-CS [ 10] 30.05/0.8995 31.93/0.9268 33.52/0.9440 34.99/0.9568 32.62/0.9318
MADUN [ 42] 29.96/0.8988 32.38/0.9293 34.04/0.9475 35.66/0.9601 33.01/0.9339
TransCS [ 39] 29.54/0.8877 31.69/0.9189 33.49/0.9411 35.06/0.9548 32.45/0.9256
FSOINet [ 8] 30.46/0.9023 32.60/0.9312 34.39/0.9492 35.80/0.9595 33.31/0.9356
MR-CCSNet [ 15] -/- -/- -/- 34.77/0.9546 -/-
CSFormer [ 52] 29.21/0.8784 31.64/0.9181 33.34/0.9386 34.81/0.9527 31.90/0.9244
TCS-Net [ 16] 29.04/0.8834 30.84/0.9139 32.20/0.9317 33.94/0.9508 31.51/0.9200
OCTUF+[43] 30.73/0.9036 32.92/0.9332 34.61/0.9500 36.10/0.9607 33.59/0.9369
Adaptive
samplingACCSNet [ 37] 29.76/0.8847 31.86/0.9139 33.61/0.9309 -/- -/-
CASNet [ 6] 30.36/0.9014 32.47/0.9301 34.19/0.9485 35.67/0.9591 33.17/0.9348
AMS-Net [ 58] 31.23 /0.8867 33.25 /0.9196 34.99 /0.9406 36.35 /0.9522 33.96 /0.9248
Ours 31.05 /0.9177 33.56 /0.9420 35.16 /0.9543 36.62 /0.9617 34.10 /0.9439
Urban100Non-adaptive
samplingAMP-Net [ 59] 26.04/0.8151 28.02/0.8664 29.60/0.8989 30.89/0.9202 28.64/0.8751
OPINE-Net+[57]26.93/0.8397 28.42/0.8784 30.06/0.9082 31.86/0.9308 29.32/0.8893
COAST [ 53] 26.76/0.8414 28.67/0.8846 30.14/0.9102 31.10/0.9168 29.17/0.8882
NL-CS [ 10] 27.37/0.8492 29.18/0.8909 30.50/0.9166 31.93/0.9332 29.75/0.8975
MADUN [ 42] 27.00/0.8558 29.14/0.8981 30.87/0.9248 32.54/0.9347 29.89/0.9033
TransCS [ 39] 26.72/0.8413 28.33/0.8818 30.07/0.9131 31.72/0.9330 29.21/0.8923
FSOINet [ 8] 27.53/0.8627 29.60/0.9029 31.23/0.9268 32.62/0.9430 30.25/0.9089
CSFormer [ 52] 27.92/0.8458 29.76/0.8896 31.31/0.9166 32.43/0.9332 30.36/0.8963
TCS-Net [ 16] 25.86/0.8284 27.59/0.8744 28.82/0.9000 30.11/0.9236 28.10/0.8816
OCTUF+[43] 27.92/0.8652 30.02/0.9057 31.63/0.9292 33.08/0.9453 30.66/0.9113
Adaptive
samplingACCSNet [ 37] 27.80/0.8422 29.62/0.8793 31.08/0.9009 -/- -/-
CASNet [ 6] 27.46/0.8616 29.42/0.9005 30.91/0.9237 32.20/0.9396 30.00/0.9063
AMS-Net [ 58] 28.04 /0.8399 30.23 /0.8869 31.90 /0.9147 33.23 /0.9328 30.85 /0.8936
Ours 29.09 /0.8979 31.27 /0.9254 32.81 /0.9405 34.27 /0.9504 31.86 /0.9286
Sampling rate = 10%Ground truth CASNet TransCS FSOINet CSFormer TCS-Net OCTUF+Ours
Sampling rate = 25%MADUN
Figure 5. Visual comparison with the state-of-the-art CS algorithms. Top row: Barbara from Set11 [ 26] with sampling rate = 10%, bottom
row: img062from Urban100 [ 22] with sampling rate = 25%.
4.3. Ablation study
Adaptive sampling. To explore the effectiveness of adap-
tive sampling, we conduct an experiment on the model per-
formance with and without adaptive sampling. In the case
of without adaptive sampling, we uniformly sample each
image block and reconstruct the image by the proposed
MGSANet. As shown in Tab. 2, compared to uniform sam-
pling, adaptive sampling can achieve more efÔ¨Åcient sam-
pling at the same sampling rates, resulting in signiÔ¨Åcantimprovements in the reconstruction results. From the vi-
sual results shown in Fig. 6, thanks to our proposed adap-
tive sampling method being able to allocate more samples
to areas that are more difÔ¨Åcult to reconstruct, the recon-
struction results based on adaptively sampled measurements
have clearer texture details.
Differentiable ASM with the perturbed optimizer. In
this paper, we model the STM module in ASM as an LP
problem and introduce the perturbed optimizer to make the
2625
Table 2. Ablation experiments of adaptive sampling on Set11 [ 26]
and Urban100 [ 22] datasets. The best PSNR is marked in bold.
DatasetSampling
moduleRate
10% 15% 20% 25%
Set11Uniform 30.24 32.70 34.18 35.35
Adaptive 31.05 33.56 35.16 36.62
Urban100Uniform 28.10 30.49 31.76 32.92
Adaptive 29.09 31.27 32.81 34.27
2-ndUniform sampling Adaptive sampling Ground truth
PSNR/SSIM 39.97 dB/0.9898 35.30 dB/0.98543-rd 4-th 5-th
Figure 6. Ablation study of adaptive sampling at sampling rate
= 25% on img085from Urban100 [ 22]. The scores for adaptive
sampling of each stage are shown in the top of the middle column.
ASM differentiable. As the differentiable ASM is a key
component to train ScoreNet end-to-end, we explore the ef-
fectiveness of the differentiable ASM by training our model
with and without the perturbed optimizer. As shown in
Tab. 3, the model trained with the perturbed optimizer out-
performs the model trained without the perturbed optimizer
on Set11 [ 26] and Urban100 [ 22] test sets with large mar-
gin. The ASM without the perturbed optimizer is non-
differentiable, which interrupts the entire backpropagation
process, resulting in the inability to optimize the parameters
of the ScoreNet. After introducing the differentiable ASM,
the parameters of the ScoreNet can be optimized through
backpropagation under the supervision of reconstruction Ô¨Å-
delity. Fig. 7shows the visualization of scores, the scores
are normalized to [0, 255]. We can observe that the output
scores of the ScoreNet trained without differentiable ASM
are irregular, leading to limited performance in the recon-
struction results. While introducing the differentiable ASM,
signiÔ¨Åcant improvements in the reconstruction results are
achieved at the same sampling rate.
Table 3. Ablation experiments of differentiable ASM on
Set11 [ 26] and Urban100 [ 22] datasets.
DatasetsPerturbed
optimizerRate=10% Rate=15% Rate=20%
PSNR/SSIM PSNR/SSIM PSNR/SSIM
Set11w/o 30.38/0.9085 32.37/0.9355 33.92/0.9505
w/ 31.05 /0.9177 33.56 /0.9420 35.16 /0.9543
Urban100w/o 28.33/0.8831 30.23/0.9165 31.62/0.9352
w/ 29.09 /0.8979 31.27 /0.9254 32.81 /0.9504
Out-of-loop reconstruction Ô¨Ådelity supervision. In the
training process, we propose to optimize the ScoreNet of
each adaptive stage with the supervision of reconstruction
Ô¨Ådelity. We conduct ablation experiments on the stage-
by-stage reconstruction Ô¨Ådelity supervised training strategy.
2-nd 3-rd 4-th 5-th2-nd 3-rd 4-th 5-thGround truth Ground truth W/ W/o W/o W/Figure 7. The visualization of scores generated by the 5-stage
model trained with (w/) and without (w/o) perturbed optimizer on
cameraman andlena256 from Set11 testset [ 26]. In addition to
the1-st stage, the 2-nd to5-th stages implement adaptive sam-
pling based on the scores.
We train multi-stage models (3 stages for sampling rate =
15% and 5 stages for sampling rate = 25%), and the recon-
struction Ô¨Ådelity loss is only introduced in the last stage.
As shown in Tab. 4, introducing reconstruction Ô¨Ådelity su-
pervision of each stage can effectively improve the quality
of reconstruction results, especially for the sampling rate =
25%. When the number of stages is higher, the reconstruc-
tion quality of introducing reconstruction Ô¨Ådelity supervi-
sion for each stage is more signiÔ¨Åcantly improved.
Table 4. Ablation experiments of the stage-by-stage reconstruction
Ô¨Ådelity-driven training method on Set11 [ 26] and Urban100 [ 22].
DatasetsReconstruction
Ô¨Ådelity-drivenRate=15% Rate=25%
PSNR/SSIM PSNR/SSIM
Set11Last stage 33.02/0.9340 35.65/0.9611
Each stage 33.56 /0.9420 36.62 /0.9617
Urban100Last stage 30.82/0.9142 33.23/0.9493
Each stage 31.27 /0.9254 34.27 /0.9504
4.4. Effectiveness of MGSANet
Attention mechanism. We conduct an ablation experi-
ment on the attention module. We replace the SA mod-
ule with the channel attention (CA) [ 50] module and the
convolutional block attention module (CBAM) [ 50], where
CBAM is a combination of the SA module and the CA
module. As shown in Tab. 5, the results of the SA-based
model are slightly better than the CA-based model on the
Urban100 [ 22] dataset, and on the Set11 [ 26] dataset, the
two results are comparable and both better than the CBAM-
based model. In addition, the computational complexity of
the SA-based and the CA-based models are very close, and
both are smaller than the CBAM-based model. Adding the
attention module may not always lead to performance im-
provement, but may result in a decrease in the performance
of the backbone network [ 23]. In summary, we adopt the
SA module as the attention module.
Number of scales. We also explore the model perfor-
mance with different numbers of scales. Our proposed
MGSANet has 4 horizontal branches as shown in Fig. 4, and
the size of features in each horizontal branch is H√óW√ó32,
H/2√óW/2√ó64,H/4√óW/4√ó96andH/8√óW/8√ó128
from top to bottom respectively. The models with 2 and 3
2626
Table 5. MGSANet with different attention mudule on Set11 [ 26]
and Urban100 [ 22] datasets at sampling rate = 25%.
DatasetAttention
moduleFLOPs (G)Performance
PSNR SSIM
Set11CA 202 35.33 0.9612
CBAM 218 35.18 0.9601
SA 202 35.35 0.9611
Urban100CA 1550 32.90 0.9484
CBAM 1672 32.57 0.9461
SA 1550 32.92 0.9487
scales only contain the top 2 and 3 horizontal branches. Be-
sides, we add a horizontal branch at the bottom to form the
model with 5 scales, and the size of features in the branch is
H/16√óW/16√ó256. As shown in Tab. 6, when the num-
ber of scales increases from 2 to 4, the performance of the
model can be greatly improved. However, when the number
of scales increases to 5, the performance of the model in-
creases very little and even decreases on the Urban100 [ 22]
test set. Besides, the model with 5 scales has more parame-
ters and computational complexity, therefore, we adopt the
model with 4 scales.
Table 6. Model performance of MGSANet with different num-
ber of scales on Set11 [ 26] and Urban100 [ 22] datasets. The best
PSNR is marked in bold.
Datasets RateNumber of scales
2 3 4 5
Set1115% 31.78 32.54 32.70 32.73
25% 34.43 34.48 35.35 35.36
Urban10015% 29.22 30.21 30.48 30.38
25% 31.60 31.75 32.91 32.76
MGSANet
MADUN
FSIO-NetOCTUF+
CSFormer
TCS-NetCASNetOPINE-Net+
AMP-NetCOASTMGSANet OCTUF+
MADUN
CASNetCSFormerFSIO-Net
OPINE-Net+
AMP-NetCOASTTCS-NetAdap-MGSANet Adap-MGSANet
(a) (b)
Figure 8. Comparison on performance and efÔ¨Åciency of various
CS algorithms at sampling rate = 25% on Urban100 dataset [ 22].
Running speed. We compare the efÔ¨Åciency and recon-
struction quality of our model and the SOTA models. As
shown in Fig. 8(a), on the Urban100 [ 22] dataset, the re-
construction quality of MGSANet is slightly lower than
OCTUF+[43], but the reconstruction speed of MGSANet
can reach more than 6 √óthat of OCTUF+. Furthermore,
the training time of MGSANet is short (less than 8 hours)
as shown in Fig. 8(b), which is important for multi-stage
training. Besides, as shown in Tab. 7, although MGSANet
requires more FLOPs, its highly parallelized framework uti-
lizes the potential of the GPU, enabling efÔ¨Åcient and accu-rate reconstruction. In summary, our proposed MGSANet
can achieve good trade-off in reconstruction quality and ef-
Ô¨Åciency. Moreover, we also show the reconstruction efÔ¨Å-
ciency of the MGSANet with adaptive sampling (i.e. Adap-
MGSANet), we can observe that the reconstruction quality
of Adap-MGSANet greatly outperforms the SOTA methods
with comparable training and inferencing speed.
Table 7. Comparison of the FLOPs and running time on Urban100
dataset [ 22] at sampling rate = 25%.
Methods CSFormer FSIONet OCTUF+CASNet MGSANet
FLOPs (T) 0.243 0.202 0.362 0.826 1.550
Time (s) 0.0851 0.0257 0.1923 0.2057 0.0307
4.5. Sensitivity to noise
In practical scenarios, the efÔ¨Åcacy of the model may be af-
fected by noise. In order to evaluate the robustness of our
proposed model to noise, we add Gaussian noise with dif-
ferent standard deviation levels, similar to [ 43]. We com-
pare our proposed method with different SOTA methods
at different noise levels at sampling rates = 10% and 25%.
As shown in Fig. 9, our proposed method outperforms the
SOTA CS methods with standard variances noise from 0 to
8 (the range of pixel values is [0, 255]).
(a) (b)
Figure 9. The results of different noise levels on Urban100 [ 22]
dataset at sampling rate = (a) 10% and (b) 25%.
5. Conclusion
In this paper, we introduce a novel reconstruction-free cas-
caded Adaptive Compressive Sensing (ACS) framework,
which obviates the need for reconstruction at the adap-
tive sampling process. A lightweight ScoreNet is proposed
to allocate sampling rates based on the previous CS mea-
surements and a differentiable adaptive sampling module
is designed for end-to-end training. Furthermore, we pro-
pose a Multi-Grid Spatial-Attention Network (MGSANet)
for efÔ¨Åcient multi-stage training and reconstruction. By in-
corporating reconstruction Ô¨Ådelity supervision outside the
adaptive sampling loop, we optimize ACS for high-quality
imaging. Extensive quantitative and qualitative experiments
demonstrate the effectiveness of our proposed method com-
pared with state-of-the-art CS algorithms.
6. Acknowledgments
This work was supported by the National Key Re-
search and Development Program of China under Grant
2022YFA1207200, and NSFC Projects under Grant
61971465.
2627
References
[1] Marc A Œ≤mann and Manfred Bayer. Compressive adaptive
computational ghost imaging. ScientiÔ¨Åc Reports , 3(1):1‚Äì5,
2013. 1,2
[2] Ali Akbari, Diana Mandache, Maria Trocan, and Bertrand
Granado. Adaptive saliency-based compressive sensing im-
age reconstruction. In IEEE International Conference on
Multimedia & Expo Workshops , pages 1‚Äì6, 2016. 2
[3] Milad Alemohammad, Jaewook Shin, and Mark A Foster.
Adaptively scanned compressive multiphoton microscopy.
InCLEO: Science and Innovations , pages SW4J‚Äì6. Optica
Publishing Group, 2018. 2
[4] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-
tendra Malik. Contour detection and hierarchical image seg-
mentation. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 33(5):898‚Äì916, 2010. 5
[5] Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco
Cuturi, Jean-Philippe Vert, and Francis Bach. Learning with
differentiable pertubed optimizers. Advances in Neural In-
formation Processing Systems , 33:9508‚Äì9519, 2020. 4
[6] Bin Chen and Jian Zhang. Content-aware scalable deep com-
pressed sensing. IEEE Transactions on Image Processing ,
31:5412‚Äì5426, 2022. 1,2,5,6
[7] Scott Shaobing Chen, David L Donoho, and Michael A
Saunders. Atomic decomposition by basis pursuit. SIAM
review , 43(1):129‚Äì159, 2001. 2
[8] Wenjun Chen, Chunling Yang, and Xin Yang. Fsoinet:
feature-space optimization-inspired network for image com-
pressive sensing. In IEEE International Conference on
Acoustics, Speech and Signal Processing , pages 2460‚Äì2464.
IEEE, 2022. 1,2,5,6
[9] Zan Chen, Wenlong Guo, Yuanjing Feng, Yongqiang Li,
Changchen Zhao, Yi Ren, and Ling Shao. Deep-learned
regularization and proximal operator for image compressive
sensing. IEEE Transactions on Image Processing , 30:7112‚Äì
7126, 2021. 2
[10] Wenxue Cui, Shaohui Liu, Feng Jiang, and Debin Zhao.
Image compressed sensing using non-local neural network.
IEEE Transactions on Multimedia , 25:816 ‚Äì 830, 2021. 2,5,
6
[11] Huidong Dai, Guohua Gu, Weiji He, Ling Ye, Tianyi Mao,
and Qian Chen. Adaptive compressed photon counting 3d
imaging based on wavelet trees and depth map sparse rep-
resentation. Optics Express , 24(23):26080‚Äì26096, 2016. 1,
2
[12] Weisheng Dong, Guangming Shi, Xin Li, Yi Ma, and Feng
Huang. Compressive sensing via nonlocal low-rank regu-
larization. IEEE Transactions on Image Processing , 23(8):
3618‚Äì3632, 2014. 2
[13] David L Donoho. Compressed sensing. IEEE Transactions
on Information Theory , 52(4):1289‚Äì1306, 2006. 1
[14] Mark Everingham and John Winn. The pascal visual ob-
ject classes challenge 2012 development kit. Pattern Analy-
sis, Statistical Modelling and Computational Learning, Tech.
Rep, 8:5, 2011. 5
[15] Zi-En Fan, Feng Lian, and Jia-Ni Quan. Global sensing and
measurements reuse for image compressed sensing. In Pro-ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 8954‚Äì8963, 2022. 2,6
[16] Hongping Gan, Minghe Shen, Yi Hua, Chunyan Ma, and Tao
Zhang. From patch to pixel: A transformer-based hierarchi-
cal framework for compressive image sensing. IEEE Trans-
actions on Computational Imaging , 9:133‚Äì146, 2023. 2,5,
6
[17] Liang Gao, Jinyang Liang, Chiye Li, and Lihong V Wang.
Single-shot compressed ultrafast photography at one hun-
dred billion frames per second. Nature , 516(7529):74‚Äì77,
2014. 1
[18] Yuanbiao Gou, Peng Hu, Jiancheng Lv, Joey Tianyi Zhou,
and Xi Peng. Multi-scale adaptive network for single im-
age denoising. Advances in Neural Information Processing
Systems , 35:14099‚Äì14112, 2022. 2
[19] J ¬®urgen Hahn, Christian Debes, Michael Leigsnering, and
Abdelhak M Zoubir. Compressive sensing and adaptive di-
rect sampling in hyperspectral imaging. Digital Signal Pro-
cessing , 26:113‚Äì126, 2014. 1,2
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770‚Äì778, 2016. 4
[21] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 4700‚Äì4708, 2017. 4
[22] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Sin-
gle image super-resolution from transformed self-exemplars.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 5197‚Äì5206, 2015. 5,6,7,8
[23] Zhongzhan Huang, Senwei Liang, Mingfu Liang, Wei He,
Haizhao Yang, and Liang Lin. The lottery ticket hypothe-
sis for self-attention in convolutional neural network. arXiv
preprint arXiv:2207.07858 , 2022. 7
[24] Hongzhi Jiang, Shuguang Zhu, Huijie Zhao, Bingjie Xu, and
Xudong Li. Adaptive regional single-pixel imaging based
on the fourier slice theorem. Optics Express , 25(13):15118‚Äì
15130, 2017. 2
[25] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[26] Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Ker-
viche, and Amit Ashok. Reconnet: Non-iterative reconstruc-
tion of images from compressively sensed measurements. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 449‚Äì458, 2016. 1,2,5,6,
7,8
[27] Chengbo Li, Hong Jiang, Paul Wilford, Yin Zhang, and Mike
Scheutzow. A new compressive video sensing framework for
mobile broadcast. IEEE Transactions on Broadcasting , 59
(1):197‚Äì205, 2013. 1
[28] Jiying Liu and Cong Ling. Adaptive compressed sensing
using intra-scale variable density sampling. IEEE Sensors
Journal , 18(2):547‚Äì558, 2017. 2
[29] Michael Lustig, David Donoho, and John M Pauly. Sparse
mri: The application of compressed sensing for rapid mr
2628
imaging. Magnetic Resonance in Medicine: An OfÔ¨Åcial
Journal of the International Society for Magnetic Resonance
in Medicine , 58(6):1182‚Äì1195, 2007. 1
[30] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
ShufÔ¨Çenet v2: Practical guidelines for efÔ¨Åcient cnn architec-
ture design. In Proceedings of the European Conference on
Computer Vision , pages 116‚Äì131, 2018. 2,4
[31] St ¬¥ephane G Mallat and Zhifeng Zhang. Matching pursuits
with time-frequency dictionaries. IEEE Transactions on Sig-
nal Processing , 41(12):3397‚Äì3415, 1993. 2
[32] Yiqun Mei, Yuchen Fan, Yulun Zhang, Jiahui Yu, Yuqian
Zhou, Ding Liu, Yun Fu, Thomas S Huang, and Humphrey
Shi. Pyramid attention network for image restoration. Inter-
national Journal of Computer Vision , 131(12):3207‚Äì3225,
2023. 2
[33] R Monika and Samiappan Dhanalakshmi. An efÔ¨Åcient
medical image compression technique for telemedicine sys-
tems. Biomedical Signal Processing and Control , 80:
104404, 2023. 1,2
[34] David B Phillips, Ming-Jie Sun, Jonathan M Taylor,
Matthew P Edgar, Stephen M Barnett, Graham M Gibson,
and Miles J Padgett. Adaptive foveated single-pixel imag-
ing with dynamic supersampling. Science Advances , 3(4):
e1601782, 2017. 1
[35] Yan Qian, Ruiqing He, Qian Chen, Guohua Gu, Feng Shi,
and Wenwen Zhang. Adaptive compressed 3d ghost imaging
based on the variation of surface normals. Optics Express , 27
(20):27862‚Äì27872, 2019. 1,2
[36] Chenxi Qiu and Xuemei Hu. Adacs: Adaptive compres-
sive sensing with restricted isometry property-based error-
clamping. IEEE TPAMI , pages 1‚Äì18, 2024. 2
[37] Chenxi Qiu, Tao Yue, and Xuemei Hu. Adaptive and cas-
caded compressive sensing. arXiv:2203.10779 , 2022. 2,5,
6
[38] Florian Rousset, Nicolas Ducros, Andrea Farina, Gianluca
Valentini, Cosimo D‚ÄôAndrea, and Franc ¬∏oise Peyrin. Adap-
tive basis scan by wavelet prediction for single-pixel imag-
ing. IEEE Transactions on Computational Imaging , 3(1):
36‚Äì46, 2016. 1,2
[39] Minghe Shen, Hongping Gan, Chao Ning, Yi Hua, and Tao
Zhang. Transcs: a transformer-based hybrid architecture for
image compressed sensing. IEEE Transactions on Image
Processing , 31:6991‚Äì7005, 2022. 2,5,6
[40] Wenzhe Shi, Jose Caballero, Ferenc Husz ¬¥ar, Johannes Totz,
Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan
Wang. Real-time single image and video super-resolution
using an efÔ¨Åcient sub-pixel convolutional neural network. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 1874‚Äì1883, 2016. 4
[41] Fernando Soldevila, Eva Salvador-Balaguer, P Clemente,
Enrique Tajahuerce, and Jes ¬¥us Lancis. High-resolution adap-
tive imaging with a single photodiode. ScientiÔ¨Åc Reports , 5
(1):1‚Äì9, 2015. 2
[42] Jiechong Song, Bin Chen, and Jian Zhang. Memory-
augmented deep unfolding network for compressive sensing.
InProceedings of the 29th ACM International Conference on
Multimedia , pages 4249‚Äì4258, 2021. 2,5,6[43] Jiechong Song, Chong Mou, Shiqi Wang, Siwei Ma, and Jian
Zhang. Optimization-inspired cross-attention transformer
for compressive sensing. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
6174‚Äì6184, 2023. 1,2,5,6,8
[44] Rayko I Stantchev, David B Phillips, Peter Hobson,
Samuel M Hornett, Miles J Padgett, and Euan Hendry. Com-
pressed sensing with near-Ô¨Åeld thz radiation. Optica , 4(8):
989‚Äì992, 2017. 1,2
[45] Joel A Tropp and Anna C Gilbert. Signal recovery from ran-
dom measurements via orthogonal matching pursuit. IEEE
Transactions on Information Theory , 53(12):4655‚Äì4666,
2007. 2
[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in Neural
Information Processing Systems , 30, 2017. 2
[47] Lishun Wang, Miao Cao, Yong Zhong, and Xin Yuan.
Spatial-temporal transformer for video snapshot compres-
sive imaging. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 2022. 1
[48] Ping Wang, Lishun Wang, and Xin Yuan. Deep optics for
video snapshot compressive imaging. In Proceedings of the
IEEE International Conference on Computer Vision , pages
10646‚Äì10656, 2023. 1
[49] Qiong Wang, Qiurong Yan, Suhui Deng, Hui Wang, Chen-
glong Yuan, and Yuhao Wang. Iterative adaptive photon-
counting compressive imaging based on wavelet entropy au-
tomatic threshold acquisition. IEEE Photonics Journal , 11
(5):1‚Äì13, 2019. 1,2
[50] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So
Kweon. Cbam: Convolutional block attention module. In
Proceedings of the European Conference on Computer Vi-
sion, pages 3‚Äì19, 2018. 4,7
[51] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei
Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext
v2: Co-designing and scaling convnets with masked autoen-
coders. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 16133‚Äì16142, 2023.
3
[52] Dongjie Ye, Zhangkai Ni, Hanli Wang, Jian Zhang, Shiqi
Wang, and Sam Kwong. Csformer: Bridging convolution
and transformer for compressive sensing. IEEE Transactions
on Image Processing , 2023. 1,2,5,6
[53] Di You, Jian Zhang, Jingfen Xie, Bin Chen, and Siwei Ma.
Coast: Controllable arbitrary-sampling network for com-
pressive sensing. IEEE Transactions on Image Processing ,
30:6066‚Äì6080, 2021. 2,5,6
[54] Wen-Kai Yu, Ming-Fei Li, Xu-Ri Yao, Xue-Feng Liu, Ling-
An Wu, and Guang-Jie Zhai. Adaptive compressive ghost
imaging based on wavelet trees and sparse representation.
Optics Express , 22(6):7133‚Äì7144, 2014. 2
[55] Ying Yu, Bin Wang, and Liming Zhang. Saliency-based
compressive sampling for image signals. IEEE Signal Pro-
cessing Letters , 17(11):973‚Äì976, 2010. 2
[56] Jian Zhang and Bernard Ghanem. Ista-net: Interpretable
optimization-inspired deep network for image compressive
2629
sensing. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 1828‚Äì1837,
2018. 1,2
[57] Jian Zhang, Chen Zhao, and Wen Gao. Optimization-
inspired compact deep compressive sensing. IEEE Journal of
Selected Topics in Signal Processing , 14(4):765‚Äì774, 2020.
2,5,6
[58] Kuiyuan Zhang, Zhongyun Hua, Yuanman Li, Yongyong
Chen, and Yicong Zhou. Ams-net: Adaptive multi-scale net-
work for image compressive sensing. IEEE Transactions on
Multimedia , 2022. 5,6
[59] Zhonghao Zhang, Yipeng Liu, Jiani Liu, Fei Wen, and Ce
Zhu. Amp-net: Denoising-based deep unfolding for com-
pressive image sensing. IEEE Transactions on Image Pro-
cessing , 30:1487‚Äì1500, 2020. 1,2,5,6
[60] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss
functions for image restoration with neural networks. IEEE
Transactions on Computational Imaging , 3(1):47‚Äì57, 2016.
5
2630
