Gradient Reweighting: Towards Imbalanced Class-Incremental Learning
Jiangpeng He
he416@purdue.edu
Elmore Family School of Electrical and Computer Engineering, Purdue University, USA
Abstract
Class-Incremental Learning (CIL) trains a model to con-
tinually recognize new classes from non-stationary data
while retaining learned knowledge. A major challenge of
CIL arises when applying to real-world data characterized
by non-uniform distribution, which introduces a dual imbal-
ance problem involving (i) disparities between stored exem-
plars of old tasks and new class data (inter-phase imbal-
ance), and (ii) severe class imbalances within each individ-
ual task (intra-phase imbalance). We show that this dual
imbalance issue causes skewed gradient updates with bi-
ased weights in FC layers, thus inducing over/under-fitting
and catastrophic forgetting in CIL. Our method addresses it
by reweighting the gradients towards balanced optimization
and unbiased classifier learning. Additionally, we observe
imbalanced forgetting where paradoxically the instance-
rich classes suffer higher performance degradation during
CIL due to a larger amount of training data becoming un-
available in subsequent learning phases. To tackle this, we
further introduce a distribution-aware knowledge distilla-
tion loss to mitigate forgetting by aligning output logits pro-
portionally with the distribution of lost training data. We
validate our method on CIFAR-100, ImageNetSubset, and
Food101 across various evaluation protocols and demon-
strate consistent improvements compared to existing works,
showing great potential to apply CIL in real-world scenar-
ios with enhanced robustness and effectiveness.
1. Introduction
The ever-evolving and unpredictable nature of real-world
environments drives the imperative to develop Class-
Incremental Learning (CIL) systems with the capability of
acquiring knowledge continuously from non-stationary data
where new classes appear sequentially over time. The ad-
vantage of CIL resides in both memory and computational
efficiency which eliminates the requirement of storing all
Code is available at: https://github.com/JiangpengHe/
imbalanced_cil
inter-phase imbalanceintra-phase imbalancememory budgetnumber of training datatraining
Learned classesNewclasses
Learned classesNewclassesexemplar setFeature extractorFc layer
Figure 1. The illustration of imbalanced class-incremental with a
dual imbalance issue including the intra-phase imbalance within
each new task Tand inter-phase imbalance between old tasks ex-
emplars and new task training data. Mtrefers to the model after
learning the new task Tt.
previously learned data or retraining the model from scratch
entirely, making CIL applicable to various practical applica-
tions such as on-device learning [17]. However, deep neu-
ral network suffers from catastrophic forgetting [32] when
learning new classes, where the performance on learned
classes decreases significantly due to the unavailability of
old training data. Moreover, conventional CIL methodolo-
gies typically tackle this challenge by assuming a balanced
data distribution, where each class has roughly the same
number of training samples. Nevertheless, this assump-
tion is misaligned with real-world scenarios where data is
usually long-tail distributed with significant disparity be-
tween instance-rich ( head ) classes and instance-rare ( tail)
classes. Such imbalance presents unique challenges that
conventional CIL methods cannot adequately address, thus
reducing their effectiveness and broader applicability.
As shown in Figure 1, CIL with imbalanced data intro-
duces a dual challenge encompassing both inter-phase and
intra-phase imbalances. Conventional CIL approaches pri-
marily address the inter-phase imbalance while the intra-
phase imbalance emerges distinctly in the context of im-
balanced CIL. In general, the inter-phase imbalance arises
from the disparities between new class data and the learned
task data preserved in the exemplar set for knowledge re-
play [29, 37]. Such imbalances can skew the learning pro-
cess of the classifier between new and old classes[11, 16]
(e.g., the learned weights in the FC layer are heavily bi-
ased [46, 53]), leading to biased predictions towards the
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16668
newer classes and thus inducing catastrophic forgetting. In
the case of intra-phase imbalance, either the new classes or
the stored exemplars individually present their own imbal-
anced distributions (the imbalance of the exemplar set arises
when the number of training data is less than the memory
budget for each class). This not only affects the learning of
new knowledge but also exacerbates the forgetting issue by
intensifying the inter-phase biases.
One of the major challenges posed by learning from im-
balanced data is the biased gradient updates in FC layers
that are dominated by instance-rich classes. Specifically,
in gradient-based optimization such as stochastic gradient
descent (SGD), the weight update step in each iteration is
heavily influenced by the magnitude of gradients, which
significantly depends on the data distribution of the sam-
pled mini-batch. As a result, instance-rich classes tend to
receive gradients with larger magnitudes thus skewing the
optimization process to make the classifier over-fit on head
classes while under-fit on tail classes. This problem be-
comes more challenging under the context of CIL as the
training data distribution changes over time, and the learned
classes suffer from catastrophic forgetting. Besides, we ob-
serve that the forgetting could also be imbalanced where
the head classes usually suffer more performance degrada-
tion as a substantial portion of their training data becomes
unavailable in subsequent incremental learning stages. To
this end, our work aims to address the dual imbalance is-
sue by reweighting the gradient updates in the FC layer,
which recalibrates the optimization process and fosters the
learning of unbiased classifiers. Furthermore, we introduce
a distribution-aware knowledge distillation loss to alleviate
the imbalanced catastrophic forgetting problem by consid-
ering the distribution of lost training data to impose stronger
regularization effects on the head classes. The main contri-
butions of this work are summarized in the following,
• We study CIL under realistic class-imbalanced scenar-
ios and introduce a new end-to-end gradient reweighting
framework to tackle both intra-phase and inter-phase im-
balance challenges by rebalancing the optimization pro-
cess in FC layers.
• A new distribution-aware knowledge distillation loss is
proposed to further mitigate imbalance catastrophic for-
getting caused by the uneven number of lost training data
during imbalanced CIL.
• The efficacy of our proposed method is validated through
extensive experiments across a variety of CIL settings,
which achieves notable improvements over existing meth-
ods in both CIL and long-tailed recognition tasks.
2. Related Work
In this section, we review the existing studies that are most
related to our work including class-incremental learning in
Section 2.1 and long-tailed recognition in Section 2.2.2.1. Class-Incremental Learning
Class-incremental learning (CIL) is a subarea of continual
learning where the task index is not provided during the
inference phase, i.e. a single head classifier [30] is used
for all classes seen so far. Existing CIL has been studied
in both online and offline scenarios where each data is ob-
served only once by the model in the former case.
Conventional CIL approaches can be mainly catego-
rized into three groups including regularization, memory
replay, and parameter isolation. The regularization-based
methods aim to restrict drastic alterations to model param-
eters that are important for learned classes. A representa-
tive technique is to apply knowledge distillation [20] based
on output logits [10, 25, 37, 46] or the intermediate lay-
ers [14, 21, 40]. In addition, recent studies [1, 5, 46] re-
veal the biased weights in FC layer towards newer learned
classes and address it by applying post-hoc biased logits
correction [5, 18, 46, 53] or using separated softmax [1] and
cosine normalized classifier [21]. The replay-based meth-
ods store a small set of exemplar data to perform knowledge
rehearsal in subsequent phases. Herding algorithm [45] is
widely employed in CIL [10, 37, 46] for exemplar selection
based on the class mean. Later, a learning-based exemplar
selection method is introduced in [27]. In the online set-
ting, the exemplar selection is performed at the end of each
training iteration [2, 3, 16, 29, 36] without knowing the dis-
tribution of training data. Finally, the parameter isolation
related approaches gradually expand the network size or in-
crease model parameters [28, 44, 47, 49, 50] to provide ded-
icated space for learning new knowledge while ensuring the
previously learned information remains undisturbed.
Imbalanced CIL remains less explored compared to
conventional CIL due to the intricate challenge of address-
ing both catastrophic forgetting and imbalanced learning
simultaneously. The earlier works focused on imbalanced
CIL under multi-label [23], semi-supervised [6], and online
scenarios [12]. The most recent study [26] formulated long-
tailed CIL for both shuffled and ordered cases and proposed
a two-stage technique to decouple representation learning
and classifier learning. Later, a dynamic residual classifier
is introduced in [11] to handle the imbalance between new
and learned classes. In this work, we primarily focus on
CIL following the shuffled long-tailed case [26] to address
the imbalance issue from the perspective of balancing the
gradient updates in FC layer. In addition, contrasting the
findings in [23], we observed imbalanced forgetting where
the head classes instead suffer more performance degrada-
tion attributed to the larger volume of lost training data in
subsequent incremental phases. Therefore, we incorporate
a distribution-aware knowledge distillation loss to impose
a more stringent regularization on head classes to further
mitigate the forgetting issue.
16669
2.2. Long-tailed Learning
The deep long-tailed learning for visual recognition has
been studied comprehensively [48, 52] over the decades
due to the prevalence of class-imbalanced data obtained in
real-world scenarios. The major challenge arises from the
fact that the performance of deep learning models tends
to be dominated by head classes, leaving the learning for
tail classes significantly under-realized. In this work, we
center on class-rebalanced approaches with the goal of re-
balancing the influence resulting from imbalanced train-
ing samples, which mainly consists of re-sampling, and
class-sensitive learning. Specifically, the re-sampling based
methods [8, 35, 43] aim to construct a balanced training
batch by increasing the probability of tail classes to be sam-
pled. The class-sensitive learning seeks to adjust the value
of training loss to rebalance the uneven learning effects by
reweighting class influence [9, 13, 33, 34, 38]. In addi-
tion, recent studies [16, 41, 42] argue that the imbalance
between positive and negative gradients severely inhibits
balanced learning and they address it by adjusting the loss
influence across different classes. Our proposed method
stems from a similar observation but identifying the magni-
tude of gradient vectors diverges significantly between head
and tail classes during the learning process. This discrep-
ancy leads to uneven optimization steps, subsequently caus-
ing the model to over-fit the head classes while under-fitting
the tail classes. To address this, our goal is to equilibrate the
gradient updates, ensuring a more balanced learning pro-
cess. Furthermore, existing long-tailed learning methods
do not consider previously acquired knowledge for learned
classes, making them unsuitable under the context of CIL.
3. Preliminaries
The CIL can be formulated as learning a sequence of N
tasks{T1,T2, ...TN}. Each task t∈[1, N]refers to
one learning phase in CIL, which can be represented as
Tt={Xt,Yt}where Xt={xt
1, ...,xt
|Xt|}denotes the
set of total |Xt|number of training data and Ytdenotes
the associated class labels belonging to |Yt|classes. In
general, we have totalPN
t=1|Xt|training data correspond-
ing toPN
t=1|Yt|classes. After learning each task Tt,
the model is evaluated on test data belonging to all seen
classes Y1:twithout knowing the task identifier t. A strong
constraint in CIL when incrementally learning a new task
Tt(t >1)is the unavailability of previously learned tasks
data{X1, ...,Xt−1}, which causes the catastrophic forget-
ting on learned tasks T1:t−1. Therefore, given a memory
budget nεdata per class, the replay-based methods employ
an exemplar set Et={X1:t−1
e,Y1:t−1
e}to store both data
X1:t−1
e and their class labels Y1:t−1
e to combine with new
data{X1:t−1
e∪Xt}×{Y1:t−1
e∪Yt}during each incremen-
tal learning phase to maintain the learned knowledge.
Imbalanced CIL refers to the case where the number
Number of training data per class100050020
Average magnitude of gradients
Class IndexNew classesLearned classesTask 3Task 2Task 1
Figure 2. The average magnitudes of gradient ||∇Lce(Wj)||for
each class jby incrementally learning 3 tasks T1,T2,T3with
cross-entropy and memory budget nε= 20 exemplars per class.
of training data varies a lot among different classes while
the test data remain balanced. This induces a dual imbal-
ance issue in the training phase including the intra-phase
imbalance and inter-phase imbalance. Specifically, the for-
mer case represents the discrepancy within each task Tt
where given a head class jand a tail class kwithj, k∈ Yt,
we have the corresponding training data nj≫nk. Fur-
thermore, the exemplar set |Et|may also exhibit this class-
imbalance issue especially given a larger memory budget
nεas more training from head classes will be retained
compared to tail classes, thereby amplifying this challenge.
The latter case refers to the discrepancy between new task
data and stored exemplars when t > 1where |Xt| ≫
|X1:t−1
e|. In general, the intra-phase imbalance can re-
strict the model’s capacity to learn new knowledge while
the inter-phase imbalance can lead to catastrophic forget-
ting as the model might exhibit a bias towards new classes.
3.1. Imbalanced Gradients
Imbalanced gradient updates pose a significant challenge
when learning from imbalanced data as studied in [15].
Generally, consider the weights of a classifier W∈Rdf×c
with input feature dimension dfand output classes c. Given
a loss function L, the gradient-based weight update process
is formulated as
Wi+1=Wi−η∇L(Wi) (1)
where idenotes the index of iteration and ηis the learn-
ing rate. The gradient matrix ∇L(Wi)∈Rdf×cdeter-
mines the magnitude of the update η||∇L(Wi)||and the
direction of the update∇L(Wi)
||∇L(Wi)||where || · || denotes the
l2norm. In general, the magnitude of the gradient is pro-
portional ||∇L(Wi)|| ∝ L to the loss value which is usu-
ally computed as the average loss over all data samples.
However, challenges arise when the gradient is estimated
16670
𝜵𝑳𝒅𝒂𝒌𝒅(𝑾)stabilityplasticityAccumulated gradientsΦ
new classesoldclassestask-balance ratioclass-balance ratioloss-balance ratiobalance𝛼𝑟𝑟𝛼𝛽𝜵𝑳𝒄𝒆(𝑾)𝜵𝑳𝒄𝒆(𝑾)𝜵𝑳𝒄𝒆(𝑾)𝜶𝜵𝑳𝒄𝒆(𝑾)Accumulated gradientsΦ𝛼'𝛼(𝛼)𝛼*Imbalanced gradients
Balanced gradientsreweightingback prop𝑾Intra-phase gradient reweightingInter-phase decoupled gradient reweighting
class indexFigure 3. The overview of gradient reweighting under imbalanced CIL. Given the classifier W, the intra-phase gradient weighting is guided
by scaling the gradient matrix ∇Lce(W)with class balance ratios αderived from the cumulative gradients Φover iterations. Concurrently,
the inter-phase Decomposed Gradient Reweighting (DGR) balances the plasticity learning by separately adjusting gradients with class-
balance ratios αand task-balance ratios r. Followed by tuning the stability-plasticity trade-off with a loss balance ratio β.
from a mini-batch containing class-imbalanced data, e.g.,
nj≫nkgiven a head class jand tail class k. In this case,
the gradient magnitude for the head class tends to be sig-
nificantly larger than that for the tail class ||∇L(Wj
i)|| ≫
||∇L(Wk
i)||. In Figure 2, we highlight the gradient im-
balances across different classes by calculating the aver-
age gradient magnitude at the end of each task by incre-
mentally learning 3 new tasks (with cross-entropy loss) on
ImageNetSubset-LT [26] with each task contains 5 classes.
This disparity in gradient magnitudes leads to an imbal-
anced optimization process where larger gradients from
head classes cause larger optimization steps in weight up-
dates, thus potentially leading to over-fitting. Conversely,
smaller updates for tail classes may result in under-fitting.
Furthermore, the larger gradient magnitudes can also induce
biases in the norm of weight vectors as ||Wi+1−Wi|| ∝
||∇L(Wi)||, resulting in biased prediction towards instance-
rich or newly learned classes under CIL. In the following
section, we illustrate our proposed method to address the
aforementioned issues by re-weighting the biased gradients
to facilitate learning of a balanced classifier.
4. Method
Overview. As illustrated in Figure 3, we aim to reweight the
imbalanced gradient matrix by classwisely multiplying it
with a balance vector α= [α1, α2, ...αc]for total cclasses
seen so far. Therefore, the gradient update process can be
modified as
Wj
i+1=Wj
i−ηαj∇L(Wj
i) (2)
where Wj
irefers to the weight vector in FC layer for class
jandαj∇L(Wj
i)denotes the corresponding reweighted
balanced gradient. However, it is non-trivial to obtain
the appropriate balance vector due to the intricate data-
dependent optimization process. In addition, when learning
a new task Tt(t >1)with the present of learned classesbelonging to Y1:t−1, it poses two additional challenges.
First, the training distribution, denoted as D, changed for
learned classes during CIL where D(X1:t−1
e,Y1:t−1
e)̸=
D(X1:t−1,Y1:t−1)as only limited exemplars are stored for
training in subsequent learning phases after they were ini-
tially observed. Therefore, simply balancing all gradients
without accounting for this will result in knowledge for-
getting for learned classes, which could be imbalanced as
well due to the uneven number of lost training data between
head and tail classes. Second, the training objective varies
between new and old tasks where the learned tasks T1:t−1
build upon knowledge accumulated from previous learning
phases, while the new task Ttstarts from scratch without
any prior knowledge. Consequently, balancing the gradi-
ent updates for all classes may not be sufficient due to the
intrinsic imbalance in optimization, which requires a more
adaptive and flexible approach to ensure the model can ef-
fectively learn new tasks without forgetting the knowledge
from previous ones.
In the following sections, we illustrate how to effectively
determine the balance vector αunder CIL for intra-phase
scenario in Section 4.1, and inter-phase case to tackle both
biased gradients and imbalanced forgetting in Section 4.2.
4.1. Intra-Phase Class-Imbalance
Consider the case of learning the first task T1={X1,Y1}
from a scratch model M0(fθ, W)where fθandWrefers to
the feature extractor and classifier, respectively. The cross-
entropy loss is formulated as
Lce=−1
|X1||X1|X
k=1|Y1|X
j=1ˆyjlog(pj) (3)
where pjis the Softmax of the jth output logit zj=
[WTfθ(xk)]jandˆyjdenotes the class label. The goal is to
determine the class-wise balance vector α= [α1, ...α|Y1|].
16671
Recognizing that a static balance vector may not suffi-
ciently capture the biases inherent in the gradients matrix
that evolve as the learning progresses, we propose a simple
yet effective solution in this work to determine the balance
vector adaptively by leveraging historical accumulated gra-
dients. In detail, we calculate the class balance ratio αj
iat
iteration ibased on the accumulated magnitude of gradients
corresponding to each class jas
αj
i=min
m∈Y1Φm
i
Φj
i,Φj
i=iX
n=1||∇Lce(Wj
n)|| (4)
By doing so, we dynamically recalibrate the weight updates,
ensuring that both majority and minority classes contribute
equally to the learning process and address the biased issue.
Regularized softmax cross-entropy. While the gradi-
ent update adjustments help in emphasizing tail classes, it
also inadvertently leads to increased gradients towards head
classes due to the decrease of output logit zjof head classes,
making the cross-entropy loss put more effort into increas-
ing its output logits. Motivated by [33], we alleviate this
side effect by applying regularized cross-entropy using a
modified softmax equation
pj=exp(zj+ log πj)
P|Y1|
m=1exp(zm+ log πm)(5)
where πjdenotes the estimates of class priors and logπjis
the per-label offsets added to the original output logit. In
practice, the calibrated class distribution with the number
of data for each seen class is widely used to determine πj.
Generally, the instance-rich classes have larger πjwith an
increase of softmax output pjto compensate for the side
effect of down-weighting the gradients during the training
process.
4.2. Inter-Phase Class-Imbalance
We extend our intra-phase method to address the inter-phase
imbalance in CIL by adding two modules: (i) a Distribution-
Aware Knowledge Distillation (DAKD) loss to maintain
the information from the original training distribution while
mitigating the imbalanced forgetting (Section 4.2.1), and
(ii) a Decoupled Gradient Reweighting (DGR) framework
to separately manage the gradients between new and already
learned tasks to ensure a balance between the stability of
past knowledge and the plasticity required for learning new
classes (Section 4.2.2).
4.2.1 Distribution-Aware Knowledge Distillation
Similar to existing bias correction methods [1, 46, 53],
we consider adding the knowledge distillation loss [20] to
maintain the learned knowledge due to the changing of
training distribution for learned classes. The original dis-
tillation loss for learning a new task tcan be expressed asLkd(z,ˆz) =−1
|Xt+X1:t−1e||Xt+X1:t−1
e|X
k=1|Y1:t−1|X
j=1ˆpτ
jlog(pτ
j)
(6)
where ˆpτ
j=Softmax (ˆzj/τ)is softened ( τ= 2) output prob-
ability from the teacher model ˆMt−1with frozen parame-
ters obtained in the last learning phase t−1. However, as
mentioned earlier in Section 1, catastrophic forgetting can
be imbalanced as a larger volume of instances from the head
class become unavailable in comparison to the tail class in
the subsequent incremental learning phases due to the fixed
memory budget. Though the most intuitive way to address
this issue is storing more data for head classes and less data
for tail classes, it poses another nontrivial challenge arising
from a class-imbalanced exemplar set that potentially inten-
sifies both over-fitting and under-fitting issues. Therefore,
we introduce the Distribution-Aware Knowledge Distilla-
tion (DAKD) to maintain the knowledge by taking into ac-
count the distribution of the lost training data. Specifically,
we obtain the lost training data distribution sby calculating
sj=|Xj|−|Xj
e|for each class j∈ Y1:t−1where |Xj|and
|Xj
e|denote the number of original training data and stored
exemplars for class j, respectively. Motivated by [54], we
decouple the original distillation loss into a weighted sum
of two parts using a ratio σ∈[0,1](1 indicates balanced
distribution) measured by the entropy of s. The DAKD is
then formulated as
Ldakd(z,ˆz|s) =σLkd(z,ˆz) + (1 −σ)Limb
kd(˜z,ˆz)(7)
where Lkddenotes the balanced part calculated using the
original output logit z, andLimb
kddenotes the imbalanced
part with adjusted output logits ˜zdetermined by
˜zj=sjP|Y1:t−1|
m smzj+ (1−sjP|Y1:t−1|
m sm)ˆzj(8)
where this calibrated distribution of output logits ˜zdemon-
strates larger discrepancy ( |˜zj−ˆzj|) for class jwith a higher
volume of lost training data ( i.e., head classes), thereby
assigning those classes with more efforts for knowledge
distillation compared to those with less data lost ( i.e., tail
classes) which only require a subtler distillation interven-
tion to maintain the learned knowledge. This tailored logit
adjustment ensures that the extent of knowledge distillation
is appropriately aligned with the level of data lost experi-
enced by each class. The overall loss function for learning
a new task Tt, t > 1can be expressed as
L=Lce+λLdakd (9)
We use λ=λbp
|Xold|/|Xnew|to adjust the influence of
knowledge distillation, which increases as more data have
been observed. |Xold|and|Xnew|denote the number of old
and new classes training data and λbis a fixed scalar.
16672
4.2.2 Decoupled Gradient Reweighting
The overview of Decoupled Gradient Reweighting (DGR)
is shown in Figure 3, which addresses the inter-phase imbal-
ance issue by striking a balance between stability (maintain
past knowledge) and plasticity (learn new classes). Gener-
ally, the gradients from cross-entropy loss ( ∇Lce) in CIL
represent the plasticity that enables the model to adapt to
new training data distribution by incorporating both exem-
plars of old classes and data from new classes. In contrast,
the stability is introduced by the gradients from knowledge
distillation loss ( ∇Ldakd) to guide the model towards a so-
lution that aligns with the training distribution from the pre-
vious learning phases. Therefore, the DGR address inter-
phase imbalance in two folds by first reweighting gradient
∇LcefromLceto ensure unbiased plasticity and then mod-
ulating the interaction between ∇Lceand∇Ldakd to attain
a balanced equilibrium between plasticity and stability.
For plasticity , the DGR first separately reweights the gra-
dients∇Lcefor learned classes j∈ Y1:t−1and new classes
forj∈ Ytby calculating class-balance ratios αj
irespec-
tively as specified in Equation 4. Following this, a task-
balance ratio rj
iis introduced for tuning between new and
learned tasks as
rj
i=(min{1,1
rΦi} j∈ Y1:t−1
min{1, rΦi×exp(−γ|X1:t−1|
|X1:t|)} j∈ Yt
(10)
where exp(−γ|X1:t−1|
|X1:t|)is the attenuation factor and γ >
0is a hyper-parameter to adjust its magnitude. rΦi=
Φj∈Y1:t−1
i
Φj∈Yt
iis the ratio of mean accumulated gradients for
learned classes Φj∈Y1:t−1
i to new classes Φj∈Yt
i .
The advantage of DGR with an attenuation factor lies
in promoting a more equitable optimization based on the
fact that the learned classes are built upon the knowledge
learned in prior learning phases, whereas the new classes
are being trained from scratch. Therefore, merely balanc-
ing the gradient contributions between new and old classes
could inadvertently lead to the under-fitting of new classes
as they may not receive adequate training emphasis. Rec-
ognizing this, the attenuation factor adjusts the gradient
reweighting ratio in favor of new classes proportional to
the volume of data that has already been observed so far
|X1:t−1|, thereby providing a calibrated boost in support of
the new classes to mitigate the learning disparities.
For tuning between stability and plasticity , we adjust the
magnitude of gradients from distillation ||∇Ldakd(Wi)||to
make it balance with reweighted cross-entropy by including
a loss balance ratio βiat iteration ias
βi=||αiri∇Lce(Wi)||
||∇Ldakd(Wi)||(11)The overall DGR can be formulated as
Wj
i+1=Wj
i−|{z }
j∈Ytj∈Y1:t−1
z }| {
η(αj
irj
i∇Lce(Wj
i) +βi∇Ldakd(Wj
i))
(12)
Note that in this work, we intentionally avoid classwisely
reweighting the gradients from knowledge distillation loss
as it contains instrumental information obtained from previ-
ous training distribution to help maintain the discrimination
of learned classes during CIL as studied in [53]. Later in
Section 5.3, we will show that reweighting the gradients of
knowledge distillation could harm the overall performance.
5. Experiments
5.1. Experimental Setups
Evaluation protocol. In this work, we adopt two widely
used protocols for CIL including (1) learning from scratch
(LFS) [37] and (2) learning from half ( LFH ) [14, 21]. The
LFS equally split all the classes into Ntasks to incremen-
tally train a model from scratch. The LFH first trains the
model with the initial half of the classes and then equally
divides the remaining half of the classes into Ntasks. For
both cases, the model is evaluated on all classes seen so far
after learning each task without knowing the task identifier.
We apply growing memory withnεexemplars per class se-
lected by Herding [37] for both LFS andLFH . The discus-
sion about fixed memory setup can be found in Appendix .
Datasets. We evaluate our method on three public
datasets including CIFAR100 [24], ImageNetSubset with
100 classes from ImageNet [39] and Food101 [7]. Specif-
ically, we first follow [26] to construct CIFAR100-LT,
ImageNetSubset-LT and Food101-LT, which are the long-
tailed versions of the original balanced datasets by remov-
ing training samples with an imbalance factor [13] ρ=
nmax
nmin= 100 where nmax andnmindenote the maximum
and minimum number of training data per class, respec-
tively. For all three datasets, we use N={10,20}tasks for
LFS andN={5,10}tasks for LFH . The test sets remain
unchanged with the original class-balanced distributions.
Implementation details. We utilize ResNet-32 [19] for
CIFAR100-LT and ResNet-18 [19] for ImageNetSubset-LT
and Food101-LT. Both feature extractor and classifiers are
trained from scratch using Equation 9 in an end-to-end fash-
ion. To ensure a fair comparison, we adopt the same class
order and setups from [26] where we train 160 epochs for
CIFAR100-LT with an initial learning rate of 0.1 and then
reduced by a factor of 10 at the 80th and 120th epochs. For
ImageNetSubset-LT and Food101-LT, we train 90 epochs
with an initial learning rate of 0.1 which is reduced by 10 at
the 30th and 60th epoch. A consistent batch size of 128 with
an SGD optimizer is used across all experiments. For sim-
plicity, we set our hyper-parameters with γ= 1andλb= 1
16673
LFS(𝑵=𝟐𝟎,𝒏𝜺=𝟏𝟎)
class indexclass indexclass indexclass indexLFS(𝑵=𝟐𝟎,𝒏𝜺=𝟓𝟎)LFH(𝑵=𝟏𝟎,𝝆=𝟓𝟎)LFH(𝑵=𝟏𝟎,𝝆=𝟏𝟓𝟎)ImageNetSubset-LTFood101-LTAccuracy(%)Figure 4. The classification accuracy (%) on test data belonging to all classes seen so far at each incremental step by varying the memory
budget nε∈ {10,50}on ImageNetSubset-LT and imbalance factor ρ∈ {50,150}on Food101-LT.
Datasets CIFAR100-LT ImageNetSubset-LT Food101-LT
Evaluation protocol LFS LFH LFS LFH LFS LFH
Total tasks N 10 20 5 10 10 20 5 10 10 20 5 10
iCaRL [37] 21.83 24.28 28.68 28.33 33.75 29.71 41.82 40.21 18.13 12.50 21.83 21.31
IL2M [5] 31.37 29.99 34.90 33.42 31.70 25.20 40.75 39.08 16.11 16.27 23.93 22.48
BiC [46] 28.89 20.10 25.68 25.95 33.31 30.86 33.18 29.23 16.94 16.81 22.80 20.75
WA [53] 27.63 23.48 32.07 26.85 32.58 29.03 32.62 28.10 16.58 15.99 18.45 19.45
SSIL [1] 26.07 26.15 30.72 29.21 30.38 25.99 38.97 35.18 16.86 15.65 21.65 19.03
FOSTER [44] 30.43 29.96 37.25 37.91 34.38 29.75 46.51 43.88 24.27 20.45 32.39 31.46
MAFDRC [11] 32.67 31.95 37.94 38.51 40.01 34.48 48.23 44.12 26.93 19.21 34.22 30.91
EEIL-2stage [10, 26] 33.64 32.25 36.40 34.91 36.84 30.39 43.62 41.49 19.75 20.02 22.65 22.83
LUCIR-2stage [21, 26] 31.09 31.03 38.47 37.86 39.87 34.79 48.97 47.39 27.65 24.68 36.05 35.06
PODNet-2stage [14, 26] 30.41 30.37 38.38 38.45 35.47 31.71 48.02 47.74 23.78 21.13 35.42 35.22
FOSTER-2stage [26, 44] 31.27 30.68 40.26 39.43 36.47 33.95 48.89 46.93 25.82 22.28 35.69 33.48
Ours 35.66 34.35 40.18 39.11 45.12 40.79 50.57 49.13 29.05 26.42 36.84 36.19
Table 1. Results of average accuracy (%) on CIFAR100-LT, ImageNetSubset-LT and Food101-LT with imbalance factor ρ= 100 , memory
budget nε= 20 evaluated under Learning From Scratch ( LFS) and Learning From Half ( LFH ).Best andSecond Best results are marked.
for all experiments. Each experiments are conducted three
times to report average performance. All results are ob-
tained by reproducing the original methods under the same
setting based on the framework in [26, 31].
5.2. Experimental Results
Table 1 summarizes the Average Accuracy (ACC) [29] on
CIFAR100-LT, ImageNetSubset-LT, and Food101-LT with
imbalance factor ρ= 100 , memory budget nε= 20 per
class. We observe the LFS is more challenging for CIL
as the performance under LFH protocol consistently higher
than LFS, underscoring the advantage of pre-training with a
larger portion of classes in the initial incremental step. The
2-stage module [26] shows the effectiveness in addressing
imbalanced data due to its additional learnable layer to mit-
igate bias. Notably, LUCIR-2stage [21], which employs a
cosine classifier with normalized weights, performs well in
handling imbalanced data. Our method achieves promis-
ing results under LFH and significant improvements under
LFS even without necessitating extra training stages and pa-
rameters. This demonstrates the potential of our end-to-end
approach for more efficient training methodologies in CIL,
especially as the task complexity scales up to learn an in-
creasing number of classes from a scratch model.
In Figure 4, we visualize the classification accuracy onall classes seen so far at each incremental step by varying
the imbalance factor ρ∈ {50,150}and the memory bud-
getnε∈ {10,50}. While generally increasing the mem-
ory budget results in better performance, the improvements
could be marginal for some methods such as SSIL [1] and
MAFDRC [11]. This paradox is attributed to the exacer-
bation of exemplar set imbalance by using a larger memory
budget nεwhere the instance-rich classes retain more exem-
plars than instance-rare classes. On the other hand, the in-
crease of imbalance factor ρresults in a noticeable degrada-
tion in performance. Despite these challenges, our method
is able to manage diverse learning environments posed by
both memory budget and imbalance factors to consistently
achieve the best performance at each incremental step.
Additional results for (i) conventional CIL setup ( ρ= 1),
(ii) long-tail recognition, and (iii) computation and memory
evaluations can be found in Appendix .
5.3. Ablation Study
We evaluate each of our components including (i) the ad-
dition of our DAKD loss, (ii) the Decomposed Gradient
Reweighting (DGR) to separately balance the gradients for
new and learned tasks instead of treating all classes to-
gether (GR) for intra-phase imbalance, and (iii) the sig-
16674
ImageNetSubset-LT ( LFS, 𝑵=𝟏𝟎)Food101-LT ( LFH, 𝑵=𝟏𝟎)
class indexclass indexForgetting (%)Figure 5. The Forgetting [29] (%) at each incremental step by
comparing our proposed DAKD with variants of distillation. The
average classification accuracy (ACC) is shown in the legend ( •).
ρ= 100 nε= 20 Food101-LT ImageNetSubset-LT
LFS LFH
DAKD DGR △(DAKD) N= 10 N= 20 N= 5 N= 10
(KD) (GR) 27.77 24.62 47.42 45.17
(KD) ✓ 28.45 25.47 48.64 46.76
✓ (GR) 28.02 26.08 48.72 46.53
✓ (GR) ✓ 28.96 27.14 49.59 47.37
✓ ✓ ✓ 29.05 26.42 50.57 49.13
Table 2. Ablation study on Food101-LT and ImageNetSubset-LT
nificance of maintaining the discrimination of gradients
from knowledge distillation loss ( △(DAKD)) as described
in Section 4.2.2. Specifically, we consider GR and orig-
inal knowledge distillation (KD) as the baseline to evalu-
ate the contributions by adding each component from our
method. The results are summarized in Table 2, where
we observe performance improvements contributed by each
component. Notably, the GR exhibits better performance
compared to DGR due to the potential underfitting of new
classes which received insufficient emphasis during CIL.
We also observe improved performance by maintaining the
discriminativeness of the gradient from DAKD to capture
and leverage information from the original training distri-
bution, thus effectively preserving learned knowledge.
Variants of knowledge distillation loss: Additionally, we
assess the effectiveness of DAKD to mitigate forgetting
under CIL by replacing it with existing variants of logit-
based knowledge distillation loss including (i) the balanced
knowledge distillation (BKD) [51], (ii) multi-level logit
distillation(MKD) [22], (iii) decoupled knowledge distilla-
tion (DCKD) [54], and (iv) decomposed knowledge distil-
lation(DPKD) [4]. The results are visualized in Figure 5
(ρ= 100 , nε= 20 ) where we measure the forgetting
rate [29] at each incremental learning phase. Remarkably,
we notice that the MKD, DCKD, and BKD achieve compa-
rable or even worse results compared to the original KD loss
as they assume the student and teacher model observe the
same training distribution, which does not hold under CIL.
While DPKD marks some improvements, its effectiveness
remains hampered by imbalanced data. Overall, our DAKD
achieves the lowest forgetting and best ACC by factoring
lost training data distribution.
Bias correction effects: Finally, we examine the effective-
class indexclass index𝒍𝟐norm
𝒍𝟐norm𝝆=𝟏𝟎𝟎,𝒏𝜺=𝟐𝟎,𝑵=𝟏𝟎ImageNetSubset-LT𝝆=𝟏𝟎𝟎,𝒏𝜺=𝟐𝟎)(𝑵=𝟏𝟎,Figure 6. The l2norm of learned weight vectors after incremen-
tally learning N= 10 tasks on ImageNetSubset-LT under LFS
protocol. The shaded area shows the variations of each curve.
ness of our method in rectifying the biased weights in the
fully connected layers under CIL. Specifically, we compare
with Baseline (fine-tuning), BiC [46] and WA [53] to show
the variance of l2norm for weight vectors corresponding
to each class during CIL as shown in Figure 6. The Base-
line method shows significant variation both within and be-
tween tasks. While BiC and WA address the variation be-
tween tasks through post-hoc bias correction, they do not
effectively resolve the variation within individual tasks. Our
method, in contrast, achieves more uniform weight vectors
across both intra-task and inter-task learning, contributing
to our best overall performance in imbalanced CIL.
6. Conclusion
In this work, we study class-incremental learning (CIL)
when applied to imbalanced data to address both intra-phase
and inter-phase imbalances by reweighting the gradients in
FC layer to foster a balanced optimization and learn unbi-
ased classifiers. Additionally, we introduce a distribution-
aware knowledge distillation loss that dynamically modu-
lates the loss intensity in proportion to the extent of train-
ing data attrition to further mitigate imbalanced catastrophic
forgetting. Our method shows consistent improvements
under CIL and proves effective in long-tailed recognition.
Overall, our findings underscore the importance of address-
ing data imbalance in CIL and pave the way for more robust
and equitable class-incremental learning models.
Acknowledgement
This project was supported by the National Institutes of
Health (Grant No. U24CA268228). Special thanks are ex-
tended to Professor Fengqing Zhu for invaluable comments
and advice on the manuscript, with a regrettable oversight
of not being included in the author list.
16675
References
[1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang,
Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax
for incremental learning. Proceedings of the IEEE/CVF In-
ternational conference on computer vision , pages 844–853,
2021. 2, 5, 7
[2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Lau-
rent Charlin, Massimo Caccia, Min Lin, and Lucas Page-
Caccia. Online continual learning with maximal interfered
retrieval. Advances in Neural Information Processing Sys-
tems, 32, 2019. 2
[3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. Advances in Neural Information Processing Sys-
tems, 32, 2019. 2
[4] Donghyeon Baek, Youngmin Oh, Sanghoon Lee, Junghyup
Lee, and Bumsub Ham. Decomposed knowledge distilla-
tion for class-incremental semantic segmentation. Advances
in Neural Information Processing Systems , 35:10380–10392,
2022. 8
[5] Eden Belouadah and Adrian Popescu. Il2m: Class incremen-
tal learning with dual memory. Proceedings of the IEEE In-
ternational Conference on Computer Vision , pages 583–592,
2019. 2, 7
[6] Eden Belouadah, Adrian Popescu, Umang Aggarwal, and
L´eo Saci. Active class incremental learning for imbalanced
datasets. European Conference on Computer Vision , pages
146–162, 2020. 2
[7] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
Food-101 – mining discriminative components with random
forests. Proceedings of the European Conference on Com-
puter Vision , 2014. 6
[8] Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A
systematic study of the class imbalance problem in convo-
lutional neural networks. Neural networks , 106:249–259,
2018. 3
[9] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga,
and Tengyu Ma. Learning imbalanced datasets with label-
distribution-aware margin loss. Advances in neural informa-
tion processing systems , 32, 2019. 3
[10] Francisco M. Castro, Manuel J. Marin-Jimenez, Nicolas
Guil, Cordelia Schmid, and Karteek Alahari. End-to-end in-
cremental learning. Proceedings of the European Conference
on Computer Vision , 2018. 2, 7
[11] Xiuwei Chen and Xiaobin Chang. Dynamic residual clas-
sifier for class incremental learning. Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 18743–18752, 2023. 1, 2, 7
[12] Aristotelis Chrysakis and Marie-Francine Moens. Online
continual learning from imbalanced data. International Con-
ference on Machine Learning , pages 1952–1961, 2020. 2
[13] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge
Belongie. Class-balanced loss based on effective number
of samples. Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 9268–9277,
2019. 3, 6[14] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas
Robert, and Eduardo Valle. Podnet: Pooled outputs distilla-
tion for small-tasks incremental learning. Proceedings of the
European Conference on Computer Vision , pages 86–102,
2020. 2, 6, 7
[15] Emanuele Francazi, Marco Baity-Jesi, and Aurelien Lucchi.
A theoretical analysis of the learning dynamics under class
imbalance. International Conference on Machine Learning ,
pages 10285–10322, 2023. 3
[16] Yiduo Guo, Bing Liu, and Dongyan Zhao. Dealing with
cross-task class discrimination in online continual learning.
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 11878–11887, 2023. 1,
2, 3
[17] Tyler L. Hayes and Christopher Kanan. Online continual
learning for embedded devices. Conference on Lifelong
Learning Agents , 2022. 1
[18] Jiangpeng He and Fengqing Zhu. Online continual learning
via candidates voting. Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision (WACV) ,
pages 3154–3163, 2022. 2
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770–778, 2016. 6
[20] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distill-
ing the knowledge in a neural network. Proceedings of the
NIPS Deep Learning and Representation Learning Work-
shop , 2015. 2, 5
[21] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a unified classifier incrementally via
rebalancing. Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 831–839, 2019.
2, 6, 7
[22] Ying Jin, Jiaqi Wang, and Dahua Lin. Multi-level logit dis-
tillation. Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 24276–24285,
2023. 8
[23] Chris Dongjoo Kim, Jinseo Jeong, and Gunhee Kim. Im-
balanced continual learning with partitioning reservoir sam-
pling. European Conference on Computer Vision , pages
411–428, 2020. 2
[24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. Technical Report , 2009.
6
[25] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 40(12):2935–2947, 2017. 2
[26] Xialei Liu, Yu-Song Hu, Xu-Sheng Cao, Andrew D Bag-
danov, Ke Li, and Ming-Ming Cheng. Long-tailed class in-
cremental learning. European Conference on Computer Vi-
sion, pages 495–512, 2022. 2, 4, 6, 7
[27] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, and
Qianru Sun. Mnemonics training: Multi-class incremental
learning without forgetting. Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
12245–12254, 2020. 2
16676
[28] Yaoyao Liu, Bernt Schiele, and Qianru Sun. Adaptive aggre-
gation networks for class-incremental learning. Proceedings
of the IEEE/CVF conference on Computer Vision and Pat-
tern Recognition , pages 2544–2553, 2021. 2
[29] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
episodic memory for continual learning. Advances in neu-
ral information processing systems , pages 6467–6476, 2017.
1, 2, 7, 8
[30] Davide Maltoni and Vincenzo Lomonaco. Continuous learn-
ing in single-incremental-task scenarios. Neural Networks ,
116:56–73, 2019. 2
[31] Marc Masana, Xialei Liu, Bartłomiej Twardowski, Mikel
Menta, Andrew D Bagdanov, and Joost Van De Weijer.
Class-incremental learning: survey and performance evalu-
ation on image classification. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 45(5):5513–5533, 2022.
7
[32] Michael McCloskey and Neal J Cohen. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem. pages 109–165. Elsevier, 1989. 1
[33] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh
Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar.
Long-tail learning via logit adjustment. International Con-
ference on Learning Representations , 2021. 3, 5
[34] Seulki Park, Jongin Lim, Younghan Jeon, and Jin Young
Choi. Influence-balanced loss for imbalanced visual clas-
sification. Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 735–744, 2021. 3
[35] Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun,
and Jin Young Choi. The majority can help the minority:
Context-rich minority oversampling for long-tailed classifi-
cation. Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 6887–6896,
2022. 3
[36] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania.
Gdumb: A simple approach that questions our progress in
continual learning. Proceedings of the European Conference
on Computer Vision , pages 524–540, 2020. 2
[37] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H. Lampert. iCaRL: Incremental clas-
sifier and representation learning. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
2017. 1, 2, 6, 7
[38] Jiawei Ren, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi,
et al. Balanced meta-softmax for long-tailed visual recog-
nition. Advances in neural information processing systems ,
33:4175–4186, 2020. 3
[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge. International Journal of Computer Vision , 115(3):
211–252, 2015. 6
[40] Christian Simon, Piotr Koniusz, and Mehrtash Harandi. On
learning the geodesic path for incremental learning. Pro-
ceedings of the IEEE/CVF conference on Computer Vision
and Pattern Recognition , pages 1591–1600, 2021. 2[41] Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli
Ouyang, Changqing Yin, and Junjie Yan. Equalization
loss for long-tailed object recognition. Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11662–11671, 2020. 3
[42] Jingru Tan, Xin Lu, Gang Zhang, Changqing Yin, and Quan-
quan Li. Equalization loss v2: A new gradient balance ap-
proach for long-tailed object detection. pages 1685–1694,
2021. 3
[43] Jason Van Hulse, Taghi M Khoshgoftaar, and Amri Napoli-
tano. Experimental perspectives on learning from imbal-
anced data. Proceedings of the 24th international conference
on Machine learning , pages 935–942, 2007. 3
[44] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan
Zhan. Foster: Feature boosting and compression for class-
incremental learning. Proceedings of the European Confer-
ence on Computer Vision , pages 398–414, 2022. 2, 7
[45] Max Welling. Herding dynamical weights to learn. Proceed-
ings of the International Conference on Machine Learning ,
pages 1121–1128, 2009. 2
[46] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, and Yun Fu. Large scale in-
cremental learning. Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , 2019. 1, 2, 5, 7, 8
[47] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynam-
ically expandable representation for class incremental learn-
ing.Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 3014–3023, 2021. 2
[48] Lu Yang, He Jiang, Qing Song, and Jun Guo. A survey on
long-tailed visual recognition. International Journal of Com-
puter Vision , 130(7):1837–1872, 2022. 3
[49] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju
Hwang. Lifelong learning with dynamically expandable net-
works. International Conference on Learning Representa-
tions , 2018. 2
[50] Friedemann Zenke, Ben Poole, and Surya Ganguli. Con-
tinual learning through synaptic intelligence. International
Conference on Machine Learning , pages 3987–3995, 2017.
2
[51] Shaoyu Zhang, Chen Chen, Xiyuan Hu, and Silong Peng.
Balanced knowledge distillation for long-tailed learning.
Neurocomputing , 527:36–46, 2023. 8
[52] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and
Jiashi Feng. Deep long-tailed learning: A survey. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
2023. 3
[53] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-
Tao Xia. Maintaining discrimination and fairness in class
incremental learning. Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 13208–
13217, 2020. 1, 2, 5, 6, 7, 8
[54] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun
Liang. Decoupled knowledge distillation. Proceedings of
the IEEE/CVF Conference on computer vision and pattern
recognition , pages 11953–11962, 2022. 5, 8
16677
