DMR: Decomposed Multi-Modality Representations for Frames and Events
Fusion in Visual Reinforcement Learning
Haoran Xu1,2Peixi Peng2,3∗Guang Tan1∗Yuan Li4Xinhai Xu4Yonghong Tian2,3,5
1School of Intelligent Systems Engineering, Shenzhen Campus of Sun Yat-sen University
2Peng Cheng Laboratory
3School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University
4Academy of Military Sciences5School of Computer Science, Peking University
Abstract
We explore visual reinforcement learning (RL) using two
complementary visual modalities: frame-based RGB cam-
era and event-based Dynamic Vision Sensor (DVS). Ex-
isting multi-modality visual RL methods often encounter
challenges in effectively extracting task-relevant informa-
tion from multiple modalities while suppressing the in-
creased noise, only using indirect reward signals instead
of pixel-level supervision. To tackle this, we propose a
Decomposed Multi-Modality Representation (DMR) frame-
work for visual RL. It explicitly decomposes the inputs
into three distinct components: combined task-relevant fea-
tures (co-features), RGB-specific noise, and DVS-specific
noise. The co-features represent the full information from
both modalities that is relevant to the RL task; the two
noise components, each constrained by a data reconstruc-
tion loss to avoid information leak, are contrasted with the
co-features to maximize their difference. Extensive experi-
ments demonstrate that, by explicitly separating the differ-
ent types of information, our approach achieves substan-
tially improved policy performance compared to state-of-
the-art approaches.
1. Introduction
Visual reinforcement learning (RL) is instrumental in train-
ing intelligent agents to make decisions by directly translat-
ing complex visual inputs into actions. It has found appli-
cations in various domains such as autonomous driving [47,
50], robot control [19, 52], and video games [31, 42]. While
most visual RL methods use frame-based RGB cameras as
their primary source of perception [19, 20, 45, 46], these
methods face limitations in certain situations due to the
camera’s low dynamic range (70 dB) [27]. For example, in
traffic scenarios with complex lighting conditions like night
* Corresponding authors.
TD
Loss
TD
Loss
TD
Loss
Left-turnForward
RGB Frame DVS Events
DMR
(ours)Observation RepresentationFeed Forward Back Propagation
Policy
road edgepersonperson laneedge
noisenoise
Encoder Encoder
Policy Policy
DVS
EventsRGB
Frame
Policy
Optimal actionDMRFigure 1. Several typical visual examples of frames and events
based RL. (i) In the first row, insufficient ambient light causes
RGB underexposure, leading to the overlooking of the front pedes-
trian and resulting in a forward policy aligned with the lane direc-
tion that could cause collisions. (ii) In the second row, the lack
of texture in DVS causes the person and the background to blend,
leading to a left-turn policy to avoid the highlighted area on the
right. (iii) In contrast, our method (third row) can fully take ad-
vantage of RGB and DVS to extract task-relevant information and
eliminate task-irrelevant and noisy information through joint TD
and DMR learning, thereby obtaining an optimal evasion policy.
driving or tunnel traversal, the frame-based methods may
experience substantial performance degradation [51].
Recently, there has been a growing interest in using bio-
inspired event cameras to address these challenges [12, 37,
39]. Event-based cameras are well suited to adverse vi-
sual conditions due to their rapid adaptation to changes in
light intensity and a high dynamic range (120 dB) [27].
Their high sampling rate, typically in the order of KHz, al-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26508
lows them to work in high-speed scenarios while exhibit-
ing much less perceptual distortion than conventional cam-
eras or LiDAR [34, 48]. Despite these advantages, event-
based cameras operate by capturing asynchronous per-pixel
brightness changes, called “events”, rather than recording
absolute brightness at a constant rate. As a result, they
may miss crucial visual cues, especially from stationary or
slow-moving objects, which could be captured by frame-
based cameras. Given the distinct mechanisms of frame-
and event-based cameras, it becomes essential to explore
how to effectively leverage the strengths of both modalities.
The integration of frame- and event-based cameras has
been explored for tasks like object detection [26] and depth
estimation [11]. However, in vision-based RL, where entire
observations are mapped to decisions only using temporal-
difference (TD) loss [46], without pixel-level [40, 49]
or instance-level supervision [26, 38], simply aggregating
frames and events can result in increased noise and task-
irrelevant information. This phenomenon results in noise
injection in the latent state space and leads to reduced RL
performance. To address this, we categorize the informa-
tion from frames and events into three distinct types: 1)
Combined task-relevant feature, referred to as co-feature ;
2) RGB-specific noise and task-irrelevant feature, or simply
RGB noise ; and 3) DVS-specific noise and task-irrelevant
feature, or DVS noise . The co-features represent the full
information from both modalities that is essential for the
RL task, while the noise represents unwanted information
that may negatively impact the RL process. As shown in
Fig. 1, combining frames and events helps to extract impor-
tant regions, including the pedestrian and road edges. These
regions are difficult to identify precisely using either modal-
ity alone. It is notable that these three parts are all latent and
only the rewards collected by interacting with environments
are available as external guidance during learning, which is
consistent with the standard RL pipeline.
To learn the three types of information, we propose a
novel three-branch representation learning framework. The
framework comprises parallel branches that independently
encode RGB noise and DVS noise, with a third branch
merging RGB frames and DVS events at the input level to
extract the co-features. In the framework, three types of
constraints are designed. Firstly, the co-features are learned
under the guidance of RL-related loss [13], so that the co-
features are useful for the RL task. Secondly, a contrastive
loss is designed to increase the distance between the noise
and co-features. Thirdly, to ensure information complete-
ness, the co-features and noise features are used to recon-
struct the raw event and frame observations. In summary,
the contributions of this paper are three-fold:
• We present a novel approach to fuse RGB frames and
DVS events in vision-based RL, highlighting the concept
of decomposed representation learning. This approach isa pioneering effort in handling the RL task through the
fusion of frame- and event-based modalities.
• We devise a new three-branch learning framework that ef-
fectively separates task-relevant information from noise.
This filtering process mitigates noise injection in the la-
tent state space, proving to be highly beneficial for down-
stream policy learning.
• We conduct comprehensive experiments using the pro-
posed Carla benchmark. The results verify the efficacy
of our method in various traffic scenarios and adverse
weather conditions.
2. Related work
Vision-based RL. In vision-based reinforcement learn-
ing (RL), the agent typically requires low-dimensional ab-
stract representations of visual observations to expedite
the decision-learning process. This process, termed as
state abstraction , can be accomplished using four main ap-
proaches: (i) Reconstruction-based techniques [39, 44, 45];
(ii) Reward and transition dynamics prediction [13]; (iii)
Contrastive-based representations [1, 8, 23, 24, 32, 45]; (iv)
Bisimulation-based techniques [3, 16, 47]. Conventional
vision-based RL mainly focuses on frame-based RGB cam-
eras, which are vulnerable to lighting anomalies and motion
blur. These issues can be mitigated by integrating event-
based neuromorphic cameras.
Event-based Neuromorphic Sensors. DVS, a prevalent
form of neuromorphic sensor, detects local pixel-level in-
tensity changes without global exposure. Its high dy-
namic range (exceeding 120 dB) enables quick adaptation
to diverse lighting conditions [10]. DVS’s asynchronous
event streams allow it to capture high-speed motion with
a high temporal resolution [4]. Event-based sensors find
applications in object detection [12, 27, 28], image recon-
struction [33], semantic segmentation [37], and odome-
try [17, 53]. DVS’s potential in policy learning is currently
under active exploration [2, 39, 41]. Walters et al. [41] took
full advantage of the high-frequency characters of events
to realize continuous RL. Andersen et al. [2] designed an
event-based autonomous navigation control framework to
detect gates in a racing track. Vemprala et al. [39] proposed
an event variational autoencoder that directly learns rep-
resentations of asynchronous event streams for RL, rather
than pre-processing the events over a time period as an
image-like 3D tensor, such as voxel grid [26, 33]. Hence, it
may require huge GPU memory consumption during train-
ing when the scenario involves a large number of events.
Multi-Modality Learning of RGB and DVS. Since DVS
only captures per-pixel brightness changes, it may miss
crucial visual cues. Recent research on combining frame-
and event-based modalities has gained momentum [11, 36,
38, 51]. Notably, RENet [51] extracted multi-scale tem-
poral cues from events and calibrated them with frames
in a coarse-to-fine manner. EFNet [36] proposed cross-
26509
modality channel-wise attention at multiple levels of the
network to adaptively fuse frames and events. FPNet [38]
combined features from frames and events using a multi-
scale pyramid network to minimize information loss during
fusing. Except for the above multi-modality learning with
RGB and DVS, other traditional modality combinations,
such as RGB&Depth [25] and RGB&Lidar [6], have been
explored. Despite advances in existing multi-modality RL
methods [5, 22, 30], most of them face challenges in effec-
tively removing modality-specific noise and task-irrelevant
information using TD loss, leading to suboptimal decisions
based on a noisy feature map. When considering RGB and
DVS, their distinct advantages in imaging principles and
complementary nature become more apparent, which mo-
tivates us to explore a new decomposition approach to ad-
dress the RL problem. We show that explicitly modelling
task-irrelevant noises and task-relevant information is nec-
essary for RGB and DVS to enhance RL performance.
3. Preliminaries
Soft Actor-Critic (SAC). The RL problem is normally
formulated as a Markov Decision Process (MDP), denoted
as a tuple M=<O,S,A,P,R, γ > , where O,S,Aare
visual observation space, state space, and action space, re-
spectively. As per the convention [19, 31], we define the
agent’s interaction process in an MDP as follows: (i) the
agent perceives the visual observation otand stacks con-
secutive observations {ot−2, ot−1, ot}into the current state
st; (ii) the agent then selects an action at∈ A based on
a stochastic policy π(at|st); (iii) the agent receives reward
feedback rt+1∼ R(st, at), and transitions to the next state
st+1∼ P(st+1|st, at). The goal of this formulation is to
find an optimal policy π∗that maximizes the expected cu-
mulative reward across the entire rollout of MDPs.
SAC [14, 15] is a widely-used RL algorithm, which in-
corporates an α-discounted maximum entropy, denoted as
H(·), to ensure diverse action exploration. Formally, the
objective of SAC is defined as:
J(π) =X
tEst,at∼π[R(st, at) +αH(π(at|st))].(1)
During the interaction with the environment, the action-
value Qis estimated by minimizing the soft Bellman error:
LQ=Est,at∼π[Q(st, at)−(rt+λV(st+1))]. (2)
Additionally, the state-value Vcan be approximated by
sampling an action from the current policy:
V(st+1) =Eat+1∼πh
˜Q(st+1, at+1)−αlogπ(at+1|st+1)i
,
(3)
where the weights in ˜Qare computed as an exponentially
moving average of the weights in Q. Thereby, the policy isoptimized by decreasing the divergence between the expo-
nential of Qfunction and the policy π:
Lπ=Eat∼πh
αlogπ(at|st)−˜Q(st, at)i
. (4)
DeepMDP. DeepMDP [13] initially learns to encode the
high-dimensional observation into a compact and contin-
uous representation, and then acquires a policy under it.
DeepMDP extracts a parameterized latent MDP ¯Mfor the
original MDP M. LetΦ :S → ¯Sbe an state abstract func-
tion. We denote by (¯M,Φ)the latent space model of MDP
M.¯Mcontains transition model ¯Pθpand reward model
¯Rθr, parameterized by θpandθr, respectively. To obtain
the DeepMDP, the reward difference and the 2-Wasserstein
metric W2are minimized through:
LP=Est,at∼πW2(ΦP(·|st, at)−¯Pθp(·|Φ(st), at)),(5)
LR=Est,at∼πR(st, at)−¯Rθr(Φ(st), at). (6)
where the shorthand notation ΦP(·|st, at)denotes the orig-
inal state samples st+1over the distribution P(·|st, at)and
then embedding st+1viaΦ. In this paper, we keep Deep-
MDP as the basic RL method, but our method can be eas-
ily applied to other RL frameworks, as demonstrated in the
supplementary materials.
4. Methodology
4.1. Multi-Modality Visual RL Problem
Our RL pipeline comprises two main components, multi-
modality representation learning and policy learning.
Specifically, the agent obtains a multi-modality perception
{oDi}, where i∈ {1,2,···, d}, from the joint observation
spaceO= ΠODi. Here, Ois defined as the Cartesian
product of dsub observation spaces ODi, with each ODi
representing the observation space of the modality Di. The
joint state stis formed by concatenating several consecu-
tive visual observations from multiple modalities, namely
∪d
i{oDi
t−2, oDi
t−1, oDi
t}. The agent learns to encode the origi-
nal high-dimensional state stinto a compact representation
ztfor the subsequent policy learning.
Since RL directly learns policy from entire observations
without the guidance of pixel-level supervision, the het-
erogeneous modalities pose challenges in extracting task-
relevant features that are crucial for policy. Therefore,
we propose D ecomposed M ulti-modality R epresentations
(DMR) framework for RL, as shown in Fig. 2. This frame-
work mainly integrates two modalities, RGB frames and
DVS events, i.e., Di∈ {rgb,dvs}andd= 2.
4.2. Event Processing in DMR
DVS can capture independent pixel-level changes in light
intensity, resulting in the generation of asynchronous event
26510
Action
Latent Space
Latent Space
Latent Space
Latent Space
Latent Space
Latent SpaceEncoder
DVS Noise
DVS Noiserepel
repelEncoder EncoderRGB Obs.
Decoder
 RGB Noise
RGB Noise
RGB
Noise
RGB
Noise
Co-features
DVS Obs.
Decoder
Predictive
Reward Head
Predictive
State Head
C
Voxelize
RGB Frames
DVS Events
Reinforcement
LearningAgentInteraction
EnvironmentSamples
Replay
Buffer
Multi-Modality
Visual Observations
DMR
Learning
DVS
Noise
DVS
Noise
Co-featuresFigure 2. The proposed multi-modality learning framework DMR. We explicitly decompose the input into co-features and modality-
specific noises. The co-features are extracted under the guidance of two task-relevant predictive heads, while the two noise components
are contrasted with the co-features to maximize their distance. In addition, the completeness of information is ensured by imposing a
reconstruction constraint on the decoder of each modality. The downstream task decisions are learned by using co-features through RL
algorithms. Notice that during testing, DMR retains only the intermediate co-feature branch.
streams. An event eiin the stream is defined as a four-
attribute tuple (xi, yi, ti, pi), which is triggered when the
logarithmic intensity of the pixel (xi, yi)at timestamp ti
exceeds the pre-defined threshold ±Q. This process can be
described as:
L(xi, yi, ti)−L(xi, yi, ti−∆t) =pi·Q, (7)
where ∆tis the sampling rate of DVS, the polarity pi∈
{1,−1}is determined based on the intensity change, with 1
representing an increment and -1 representing a decrement.
When processing asynchronous DVS events, it is a com-
mon practice to convert events within a fixed-length tempo-
ral window into a fixed-size tensor representation, referred
to as a voxel grid [26, 33]. To synchronize events with the
low sampling rate of RGB frames, we partition the incom-
ing events within the fixed time interval of RGB frames.
The events occurring between the pair-wise frames ( orgb
t−1
andorgb
t) are discretized into a spatio-tempral voxel grid Et
withBtemporal bins (as depicted in Fig. 2). Each element
in the voxel grid has three dimensions, two-dimensional lo-
cation ( xl, ym), and temporal dimension ( tn). Formally:
Et(xl, ym, tn) =X
xl,ym=xi,yipimax(1 − |tn−t∗
i|),(8)
where t∗
iis the normalized event timestamp, which is de-
fined as t∗
i=B−1
∆d(ti−t0);∆dis the time interval between
adjacent RGB frames and t0is the first event timestamp
within the interval.
In this paper, we set B= 5, and set RGB sampling rate
to20Hz, that is, ∆d= 0.05s. Therefore, the observationodvs
tof the DVS camera at each time tis preprocessed to
form a voxel grid, which is then fed into DMR.
4.3. Representation Learning in DMR
Letzi
tdenote the representation for the original observa-
tionoi
tof modality i∈ {rgb,dvs}. The representations zi
t
may differ significantly for the two modalities even when
they yield similar policies, because of the different working
principles of RGB and DVS cameras. We decompose zi
t
into co-features zc
tand modality-specific noises hi
tas:
zi
t=zc
t⊕hi
t. (9)
To achieve this, DMR comprises three branches, as de-
picted in Fig. 2. The upper and lower branches take RGB
frames and DVS events as inputs, respectively. The data
then pass through their respective encoders, denoted as
ΦθrgbandΦθdvs, to generate modality-specific noise ( hrgb
t,
hdvs
t). The intermediate branch takes the concatenation of
RGB and DVS as input. Its output, co-features zc
t, are gen-
erated by the intermediate encoder parameterized as Φθc.
To ensure the completeness of information, we employ
reconstruction decoders, denoted as Dθi, to ensure that the
respective original observations oi
tcan be recovered:
LD=X
i∈{rgb,dvs}Dθi(zc
t+hi
t)−oi
t
2, (10)
where t∈ K andKis the set of sample indices in a training
batch that are from different time steps in different MDPs.
While ensuring the completeness of zi
t, we utilize the
task-relevant predictive heads to guide the extraction of the
co-features zc. Here, we incorporate the tractable reward
26511
Algorithm 1 Pseudocode for DMR Learning
1:Initialize the replay buffer Bwith random episodes.
2:while Not converged do
3:// Representation Learning
4: Collect multi-modality visual sequences randomly
{(orgb
t, odvs
t)}t∈K∼ B.
5: Obtain decomposed representations zc
t, hrgb
t, hdvs
t
viaΦθc,Φθrgb,Φθdvs.
6: Perform completeness constraint on zrgb
t, zdvs
tfor
each modality via Eqs. (9) and (10).
7: Extract co-features zc
tvia Eqs. (11) and (12).
8: Distinguish noise from co-features zc
tvia Eq. (13).
9:// Reinforcement Learning
10: Estimate action, state-value via Eqs. (14) and (15).
11: Establish zc
t-driven policies via Eq. (16).
12: // Environment Interaction
13: Execute at∼πϕ(at|zc
t), receive rt∼ R(st, at).
14: Observe orgb
t+1, odvs
t+1, and st+1∼ P(·|st, at).
15: Add experience (st, at, rt, st+1)to the replay buffer.
16:end while
17:return Φθc, πϕ
and state head from DeepMDP [13] into the predictive head.
It is worth noting that DMR can be seamlessly plugged into
various multi-modality visual RL methods, while providing
notable enhancements in performance. Thus, we have:
LP=¯Pθp(zc
t, at)−zc
t+1, (11)
LR=¯Rθr(zc
t, at)−rt+1, (12)
where ¯Pθpand¯Rθrare state and reward predictive heads,
respectively. These auxiliary models share the same struc-
ture except that the output of ¯Rθris a one-dimension scalar.
Finally, the noise should exhibit clear dissimilarity from
the co-features. In other words, there should be minimal
overlap between hi
tandzc
t. To achieve this distinction, we
design the following contrastive constraint:
LC=−logf(zc
t,˜zc
t)
f(zc
t,˜zc
t) +P
i∈{rgb,dvs}P
k∈Kf(zc
k,˜hi
k),(13)
where ˜zc
tand˜hi
tindicate the moving-averaged target val-
ues [7] of zc
tandhi
t, respectively, and the function f(a, b) =
exp(⟨a, b⟩/τ)measures the similarity between aandbus-
ing the dot product ⟨a, b⟩and the temperature parameter τ.
4.4. Reinforcement Learning based on DMR
With the full sensory input decomposed, we can proceed to
develop policies for the downstream task using the extracted
co-features. These co-features are isolated from irrelevant
information, enabling them to more effectively support the
objectives of downstream control.
HB - Hardrain HB - Midnight JW - Hardrain JW - MidnightRGB Frame DVS Events
Figure 3. Illustration of the Carla autopilot benchmark.
We modify the baseline RL algorithm SAC [14, 15] to
align with our co-features-driven policy learning approach.
In this process, we estimate the action-value Qand state-
value Vby utilizing the Bellman equation and the co-
features zc
tgenerated from the encoder Φθc:
LQ=E
t∈K
Q(zc
t, at)−(rt+λV(zc
t+1))
, (14)
V(st+1) =E
t∈Kh
˜Q(zc
t+1, at+1)−αlogπ(at+1|zc
t+1)i
.
(15)
The policy πϕcan be derived from:
Lπ=E
t∈Kh
αlogπϕ(at|zc
t)−˜Q(zc
t, at)i
. (16)
The full training pipeline of DMR is provided in Algo-
rithm 1 and the framework in Fig. 2. Since the policy op-
timization is driven only by the co-features, the auxiliary
encoders of the two noise branches can be omitted during
the testing phase. This means that DMR retains only the
encoder Φθcand the policy learning network πϕduring test,
thereby allowing efficient mapping from high-dimensional
multi-modality observations to visuomotor policies.
5. Experiments
5.1. Experimental Setup
We mainly focus on the autonomous driving environments
in experiments. The environments contain numerous task-
irrelevant objects, and the sensors are sensitive to chang-
ing weather conditions, providing comprehensive and real-
istic scenarios to evaluate our method. Since RL involves
trial-and-error interactions with the environment, most RL
methods test the algorithms in simulators [19–21, 29, 46].
Hence, we adopt the widely-used Carla [6, 18, 43, 50] to
establish our new Carla benchmark. Carla supports a rich
set of scenarios with varying lighting and weather condi-
tions. More importantly, it is one of the few simulators that
allows generation of asynchronous events and RGB frames
simultaneously.
As shown in Fig. 3, our Carla benchmark features two
traffic scenarios: the HighBeam (HB) scenario, where an
26512
Scenario
(Weather)MetricsSingle-modality Policies Multi-modality Policies
RGB DVS-F DVS TransFuser EFNet FPNet RENet DMR
JW
(Midnight)Distance 144±70 163±92 190±101 111±79 84±41 106±96 189±107 230±77
Reward 102±67 130±86 136±93 77±64 62±37 84±82 158±101 194±73
JW
(Hardrain)Distance 113±83 115±69 87±44 123±54 125±66 47±34 50±40 146±58
Reward 83±72 96±65 52±48 84±52 89±68 23±31 6±30 111±57
HB
(Midnight)Distance 80±60 51±63 109±76 97±81 87±67 116±68 106±39 117±68
Reward 58±56 29±59 71±74 46±69 63±63 85±62 68±42 71±72
HB
(Hardrain)Distance 91±61 51±20 70±32 122±61 114±65 106±47 125±62 150±51
Reward 70±63 30±24 49±31 85±57 69±68 64±46 73±59 112±51
Table 1. Testing performance comparison with SOTA methods under the proposed Carla benchmark. (The best single-modality policies
are highlighted in gray background, and the best results in both single- and multi-modality policies are shown in bold .)
ego-vehicle experiences varying lighting conditions while
encountering a cyclist, and the JayWalk (JW) scenario,
where the ego-vehicle encounters both stationary and mov-
ing pedestrian obstacles intermittently. Moreover, the
benchmark includes extreme weather conditions (Midnight
and Hardrain) that can cause RGB camera failure or exces-
sive noise with DVS cameras. For multi-modality obser-
vations, we focus on the fusion of RGB frames (RGB for
short) and DVS voxel grids (DVS). In addition, we intro-
duce the frame-based DVS events, termed DVS-F [26], as a
type of observation to show the effectiveness of DVS vox-
elization. In the benchmark, the ego-vehicle’s objective fol-
lows the common setup as in [47], aiming to drive as far
as possible without collisions within 500 steps. All experi-
ments are trained across 3 random seeds and 20 evaluation
rollouts per seed, yielding mean and standard deviation of
the metrics of episode reward and distance.
Our benchmark and code are available online1.
5.2. Performance Comparison
We compare DMR with both single- and multi-modality
algorithms. For the single-modality baselines, we main-
tain DeepMDP as the baseline RL algorithm, employ-
ing the three types of perception input introduced earlier,
that is RGB, DVS, and DVS-F. Since there is no previ-
ous RL algorithm that combines RGB frames and DVS
events, for the multi-modality baselines, we compare DMR
against state-of-the-art (SOTA) multi-modality fusion meth-
ods, including TransFuser [6], EFNet [36], FPNet [38], and
RENet [51]. To ensure a fair comparison, we adopt the
same 4-layer CNN structure [46] and parameter initializa-
tion for each modality’s encoder in these SOTA methods,
while keeping DeepMDP as the baseline task predictive
head. Further details on comparisons and parameter settings
are provided in the supplementary materials. Evaluation re-
1https://github.com/kyoran/DMR
Number of frames ( ×104 )0 3 6 9Episode return
050100150(a) Single-modality policies.
Number of frames ( ×104 )0 3 6 9
Episode return
050100150
 (b) Multi-modality policies.
Figure 4. Training performance under the JW-Midnight scenario.
sults after 100K training steps on the Carla benchmark are
presented in Tab. 1. The episode reward curves during the
training phase in the JW-Midnight scenario are depicted in
Fig. 4, demonstrating the superiority of our approach.
5.2.1 Single-modality Policies
In the Midnight scenario, abnormal exposure can lead to the
failure of RGB, resulting in the lowest performance result.
DVS is capable of detecting useful events in extremely low-
light conditions because of its high dynamic range, result-
ing in the highest performance in Midnight. However, in
the Hardrain weather conditions, raindrops cause undesir-
able changes in pixel-level illumination, resulting in exces-
sive noise from DVS. Consequently, under HB-Hardrain,
RGB performs the best. Moreover, under JW-Hardrain,
there is only a slight difference between RGB and the best-
performing DVS-F, which is caused by a slight deviation
due to the instability of RL sampling. Remarkably, DMR
outperforms all the single-modality methods by a substan-
tial margin in terms of reward and distance metrics. In
summary, except for the HB-midnight scenario, where our
method offers limited improvement, our method signifi-
cantly surpasses single-modality methods, underscoring the
advantages of multi-modality fusion.
26513
ModelsMetricsDistance Reward
M1 1 Branch 185±55 145±54
M2 2 Branch 194±85 141±78
M3 +Repel 214±74 181±77
M4 +Rec (ours) 230±77 194±73
Table 2. Effect of components in DMR.
5.2.2 Multi-modality Policies
Since DVS outperforms DVS-F in the majority of cases,
we utilize RGB and DVS as the perceptual inputs for the
multi-modality experiments. The learned policies of SOTA
multi-modality methods often fail to match the performance
achieved by single-modality methods. This could be at-
tributed to the common adoption of multi-scale and atten-
tion mechanisms in current state-of-the-art multi-modality
methods. These approaches often mix task-relevant infor-
mation with accumulated noise, complicating the extraction
of information crucial for downstream tasks. In contrast,
our method offers a solution by explicitly eliminating noise
and providing refined co-features for the RL. Compared
to alternative multi-modality RL methods, our approach
obviates the need for constructing intricate and resource-
intensive fusion networks, while still attaining advantages
in sample efficiency and learning performance.
5.3. Ablation Study
5.3.1 Effect of DMR Components
To assess the impact of each component, we incrementally
incorporate individual components, resulting in a series of
models labeled M1 to M4, shown in Tab. 2. Specifically,
M1 utilizes solely the co-feature branch for input fusion. In
M2, the upper and lower branches are employed, and the
features from both branches are concatenated to feed policy
learning. M3 and M4 represent the three-branch variants,
integrating explicit feature decomposition. In M3, the co-
features and modality-specific noises are repelled using the
contrastive constraint as specified by Eq. (13). M4 builds
upon M3 by incorporating the reconstruction decoder de-
picted in Eq. (10). DeepMDP is retained as our task predic-
tive head and baseline RL algorithm.
Tab. 2 presents the performance of each model. It can
be seen that M2 slightly improves on M1 in terms of dis-
tance while having little effect on reward. This is possi-
bly because of the uncertainty in replay buffer sampling
during RL training. In addition, with the introduction of
three branches and contrastive constraints (M3), there is a
significant improvement in both distance and reward. Fi-
nally, with the incorporation of the reconstruction decoder
(M4), reward and distance further improve, indicating the
necessity of the information completeness constraint. Fur-
thermore, we analyze the CAMs across different modality
DMR
(ours)single-branch
multi-modalitysingle-
modality Observations
DVS Events RGB Frame
Figure 5. CAMs under different modality RL configurations in the
JW-Midnight scenario.
DVS noise
co-features co-features
DVS Events RGB Frame
RGB noise
(a) JW-Midnight: an extremely
low-light condition.
DVS noise RGB noise
co-features co-featuresDVS Events RGB Frame(b) HB-Hardrain: a rapidly-
changing illumination condition.
Figure 6. CAMs of DMR under different illumination conditions.
configurations, including single-modality models that take
either RGB frames or DVS events as input (second col-
umn), a basic multi-modality model (M1) that takes both
inputs (third column), and DMR (fourth column). From
Fig. 5, it can be seen that the CAMs for single-modality
models primarily highlight the front road and the adjacent
buildings, activating an unnecessarily broad space. The
simple multi-modality model without using decomposition
and contrastive constraints generates a more focused area,
but still contains task-irrelevant regions. In contrast, our
method DMR effectively captures pertinent areas for RL
while eliminating irrelevant regions. These areas precisely
cover the pedestrians on the road and the left roadside,
which are crucial cues for driving decision making.
5.3.2 Analysis of Modality-Specific Noise
We analyze the encoding capabilities of capturing both task-
irrelevant noise and task-relevant features. Fig. 6 depicts the
original observations and corresponding CAMs of DMR.
In the extremely low-light condition (JW-Midnight), DVS
can capture the front pedestrian while RGB camera suffers
26514
indices of co-featuresindices of RGB noise indices of DVS noise
-1.0              -0.5             0.0             0.5             1.0Figure 7. A similarity matrix example at the 100K’th training step.
from exposure failure. It can be seen that RGB noise high-
lights the high beam region on the road, while DVS noise
is activated across a broader region, with the highest acti-
vation on the building. We can also see that the co-features
attentively grasp the pedestrian and the right roadside si-
multaneously. In the rapidly-changing illumination condi-
tion (HB-Hardrain), DVS generates excessive event noise,
while RGB can capture rich texture information. Notably,
RGB noise mainly highlights brighter regions such as the
front road and nearby vehicles and buildings, while DVS
noise is prominent around puddles and splashing water. We
observe that the co-features distinctly focus on the front cy-
clist, left vehicle, and right building, which are crucial for
driving decision-making. CAMs of SOTA methods are pro-
vided in the supplementary materials.
Fig. 7 demonstrates the encoder’s behavior in decompo-
sition and discrimination through quantitative analysis. We
present the similarity matrix between co-features and the
modality-specific noises from RGB frames and DVS events,
obtained from a training batch of 32 samples at the 100K’th
training step. The color bar at the bottom represents the sim-
ilarity ranging from low to high. The similarity is quantified
by the dot product with a temperature parameter τ= 0.1
as shown in Eq. (13). Notably, 32 samples share the same
property as the set K, where these samples are obtained
from different time steps in different MDPs. Each row in the
similarity matrix depicts similarities between the co-feature
and itself, RGB noise, and DVS noise. We can see that
the co-features exhibit strong coherence among themselves,
while their similarity with the noises is remarkably low, il-
lustrating a clear contrast.
5.3.3 Alternative DVS Backbones
Although we employed the same 4-layer CNN struc-
ture [46] in the lower DVS branch as in other branches,
our architecture exhibits minimal susceptibility to network
structure. Specifically, within the DVS branch, we evalu-
ated a Spiking Neural Network (SNN) structure which is
known for its suitability for temporal information encod-
ing [9], as presented in Tab. 3. It can be seen that DMR with
SNN is better than the other single-modality DVS methods,
suggesting that DMR has the potential of working for alter-Metrics DVS-F +SNN DVS +SNN DMR +SNN
Distance 81±15 101±10 143±30
Reward 35±12 45±2 117±34
Table 3. Performance of DMR with alternative DVS backbones
under the HB-Hardrain scenario.
Metrics RGB Depth LiDARRGB
+Depth +LiDAR
Distance 91±61 103±12 113±9 109±25 145±19
Reward 70±63 69±10 68±13 80±20 112±17
Table 4. Performance of DMR with different modality combina-
tions under the HB-Hardrain scenario.
native backbone structures. The detailed SNN structure is
provided in the supplementary materials.
5.3.4 Different Modality Combinations
While we focus on the RGB frames and DVS events modal-
ities, the DMR framework has broader applicability to var-
ious modalities. We conducted performance evaluations on
additional modality combinations using DMR, as presented
in Tab. 4. The results demonstrate that when RGB is fused
with LiDAR or Depth, the performance of DMR exceeds
that of the individual modalities. Besides, by combining
the results in Tab. 1, we can implicitly observe the efficacy
of the complementary modalities in our DMR. The pro-
nounced complementarity of RGB and DVS modalities ne-
cessitates our joint learning approach. The detailed modal-
ity settings are provided in the supplementary materials.
6. Conclusion
This paper explores a new decomposition perspective to
address the multi-modality visual RL problem. We pro-
pose a novel three-branch multi-modality fusion frame-
work, called DMR, designed for highly-complementary
frame- and event-based visual modalities. DMR can ex-
plicitly extract task-relevant features from both modalities
while mitigating the impact of irrelevant information and
noise from each modality. Experimental results demon-
strate the efficacy and superiority of DMR in policy per-
formance. Our future work includes improving generaliza-
tion and stability in more diverse and realistic scenarios in
a sim2real fashion [35].
Acknowledgement The study was funded by the Na-
tional Natural Science Foundation of China under con-
tracts No. 62372010, No. 62027804, No. 61825101, No.
62088102 and No. 62202010, and the major key project
of the Peng Cheng Laboratory (PCL2021A13). Computing
support was provided by Pengcheng Cloudbrain.
26515
References
[1] Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Cas-
tro, and Marc G. Bellemare. Contrastive behavioral simi-
larity embeddings for generalization in reinforcement learn-
ing. In 9th International Conference on Learning Represen-
tations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 ,
2021. 2
[2] Kristoffer Fogh Andersen, Huy Xuan Pham, Halil Ibrahim
Ugurlu, and Erdal Kayacan. Event-based navigation for au-
tonomous drone racing with sparse gated recurrent network.
In2022 European Control Conference (ECC) , pages 1342–
1348. IEEE, 2022. 2
[3] Pablo Samuel Castro. Scalable methods for computing state
similarity in deterministic markov decision processes. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence , pages 10069–10076, 2020. 2
[4] Guang Chen, Hu Cao, Jorg Conradt, Huajin Tang, Florian
Rohrbein, and Alois Knoll. Event-based neuromorphic vi-
sion for autonomous driving: A paradigm shift for bio-
inspired visual sensing and perception. IEEE Signal Pro-
cessing Magazine , 37(4):34–49, 2020. 2
[5] Kaiqi Chen, Yong Lee, and Harold Soh. Multi-modal mu-
tual information (mummi) training for robust self-supervised
deep reinforcement learning. In 2021 IEEE International
Conference on Robotics and Automation (ICRA) , pages
4274–4280. IEEE, 2021. 3
[6] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu,
Katrin Renz, and Andreas Geiger. Transfuser: Imitation
with transformer-based sensor fusion for autonomous driv-
ing. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 45(11):12878–12895, 2023. 3, 5, 6
[7] Yuanzheng Ci, Chen Lin, Lei Bai, and Wanli Ouyang. Fast-
moco: Boost momentum-based contrastive learning with
combinatorial patches. In European Conference on Com-
puter Vision , pages 290–306. Springer, 2022. 5
[8] Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and
Russ R Salakhutdinov. Contrastive learning as goal-
conditioned reinforcement learning. Advances in Neural In-
formation Processing Systems , 35:35603–35620, 2022. 2
[9] Wei Fang, Yanqi Chen, Jianhao Ding, Zhaofei Yu, Timoth ´ee
Masquelier, Ding Chen, Liwei Huang, Huihui Zhou, Guoqi
Li, and Yonghong Tian. Spikingjelly: An open-source ma-
chine learning infrastructure platform for spike-based intel-
ligence. Science Advances , 9(40):eadi1480, 2023. 8
[10] Guillermo Gallego, Tobi Delbr ¨uck, Garrick Orchard, Chiara
Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger,
Andrew J Davison, J ¨org Conradt, Kostas Daniilidis, et al.
Event-based vision: A survey. IEEE transactions on pattern
analysis and machine intelligence , 44(1):154–180, 2020. 2
[11] Daniel Gehrig, Michelle R ¨uegg, Mathias Gehrig, Javier
Hidalgo-Carri ´o, and Davide Scaramuzza. Combining events
and frames using recurrent asynchronous multimodal net-
works for monocular depth prediction. IEEE Robotics and
Automation Letters , 6(2):2822–2829, 2021. 2
[12] Mathias Gehrig and Davide Scaramuzza. Recurrent vision
transformers for object detection with event cameras. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 13884–13893, 2023. 1, 2
[13] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir
Nachum, and Marc G. Bellemare. Deepmdp: Learning
continuous latent space models for representation learning.
InInternational Conference on Machine Learning , pages
2170–2179. PMLR, 2019. 2, 3, 5
[14] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey
Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. In Interna-
tional conference on machine learning , pages 1861–1870.
PMLR, 2018. 3, 5
[15] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen,
George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,
Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft
actor-critic algorithms and applications. arXiv preprint
arXiv:1812.05905 , 2018. 3, 5
[16] Philippe Hansen-Estruch, Amy Zhang, Ashvin Nair, Patrick
Yin, and Sergey Levine. Bisimulation makes analogies in
goal-conditioned reinforcement learning. In International
Conference on Machine Learning , pages 8407–8426. PMLR,
2022. 2
[17] Javier Hidalgo-Carri ´o, Guillermo Gallego, and Davide
Scaramuzza. Event-aided direct sparse odometry. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-
24, 2022 , pages 5771–5780. IEEE, 2022. 2
[18] Hanjiang Hu, Zuxin Liu, Sharad Chitlangia, Akhil Agni-
hotri, and Ding Zhao. Investigating the impact of multi-lidar
placement on object detection for autonomous driving. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2550–2559, 2022. 5
[19] Yangru Huang, Peixi Peng, Yifan Zhao, Guangyao Chen,
and Yonghong Tian. Spectrum random masking for gener-
alization in image-based reinforcement learning. Advances
in Neural Information Processing Systems , 35:20393–20406,
2022. 1, 3, 5
[20] Yangru Huang, Peixi Peng, Yifan Zhao, Yunpeng Zhai, Hao-
ran Xu, and Yonghong Tian. Simoun: Synergizing inter-
active motion-appearance understanding for vision-based re-
inforcement learning. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV) , pages
176–185, 2023. 1
[21] Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan,
Patrick Langechuan Liu, and Hongyang Li. Driveadapter:
Breaking the coupling barrier of perception and planning
in end-to-end autonomous driving. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 7953–7963, 2023. 5
[22] Yasser H. Khalil and Hussein T. Mouftah. Exploiting multi-
modal fusion for urban autonomous driving using latent deep
reinforcement learning. IEEE Trans. Veh. Technol. , 72(3):
2921–2935, 2023. 3
[23] Michael Laskin, Aravind Srinivas, and Pieter Abbeel.
CURL: contrastive unsupervised representations for rein-
forcement learning. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July
2020, Virtual Event , pages 5639–5650. PMLR, 2020. 2
26516
[24] Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats,
Aravind Rajeswaran, and Pieter Abbeel. Unsupervised
reinforcement learning with contrastive intrinsic control.
Advances in Neural Information Processing Systems , 35:
34478–34491, 2022. 2
[25] Minhyeok Lee, Chaewon Park, Suhwan Cho, and Sangyoun
Lee. Spsn: Superpixel prototype sampling network for rgb-d
salient object detection. In European Conference on Com-
puter Vision , pages 630–647. Springer, 2022. 3
[26] Dianze Li, Jianing Li, and Yonghong Tian. Sodformer:
Streaming object detection with transformer using events and
frames. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2023. 2, 4, 6
[27] Jianing Li, Jia Li, Lin Zhu, Xijie Xiang, Tiejun Huang, and
Yonghong Tian. Asynchronous spatio-temporal memory net-
work for continuous event-based object detection. IEEE
Trans. Image Process. , 31:2975–2987, 2022. 1, 2
[28] Jianing Li, Xiao Wang, Lin Zhu, Jia Li, Tiejun Huang, and
Yonghong Tian. Retinomorphic object detection in asyn-
chronous visual streams. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , pages 1332–1340, 2022. 2
[29] Siao Liu, Zhaoyu Chen, Yang Liu, Yuzheng Wang, Dingkang
Yang, Zhile Zhao, Ziqing Zhou, Xie Yi, Wei Li, Wenqiang
Zhang, et al. Improving generalization in visual reinforce-
ment learning via conflict-aware gradient agreement aug-
mentation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 23436–23446, 2023.
5
[30] Jinming Ma, Feng Wu, Yingfeng Chen, Xianpeng Ji, and
Yu Ding. Effective multimodal reinforcement learning with
modality alignment and importance enhancement. arXiv
preprint arXiv:2302.09318 , 2023. 3
[31] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex
Graves, Ioannis Antonoglou, Daan Wierstra, and Martin
Riedmiller. Playing atari with deep reinforcement learning.
arXiv preprint arXiv:1312.5602 , 2013. 1, 3
[32] Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-
based reinforcement learning by latent imagination without
reconstruction. In 2021 IEEE International Conference on
Robotics and Automation (ICRA) , pages 4209–4215. IEEE,
2021. 2
[33] Henri Rebecq, Ren ´e Ranftl, Vladlen Koltun, and Davide
Scaramuzza. High speed and high dynamic range video with
an event camera. IEEE transactions on pattern analysis and
machine intelligence , 43(6):1964–1980, 2019. 2, 4
[34] Tobias Renzler, Michael Stolz, Markus Schratter, and Daniel
Watzenig. Increased accuracy for fast moving lidars: Cor-
rection of distorted point clouds. In 2020 IEEE interna-
tional instrumentation and measurement technology confer-
ence (I2MTC) , pages 1–6. IEEE, 2020. 2
[35] Sruthi Sudhakar, Jon Hanzelka, Josh Bobillot, Tanmay
Randhavane, Neel Joshi, and Vibhav Vineet. Exploring the
sim2real gap using digital twins. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 20418–20427, 2023. 8
[36] Lei Sun, Christos Sakaridis, Jingyun Liang, Qi Jiang, Kailun
Yang, Peng Sun, Yaozu Ye, Kaiwei Wang, and Luc VanGool. Event-based fusion for motion deblurring with cross-
modal attention. In European Conference on Computer Vi-
sion, pages 412–428. Springer, 2022. 2, 6
[37] Zhaoning Sun, Nico Messikommer, Daniel Gehrig, and Da-
vide Scaramuzza. Ess: Learning event-based semantic seg-
mentation from still images. In Computer Vision–ECCV
2022: 17th European Conference, Tel Aviv, Israel, Octo-
ber 23–27, 2022, Proceedings, Part XXXIV , pages 341–357.
Springer, 2022. 1, 2
[38] Abhishek Tomy, Anshul Paigwar, Khushdeep Singh Mann,
Alessandro Renzaglia, and Christian Laugier. Fusing event-
based and RGB camera for robust object detection in adverse
conditions. In 2022 International Conference on Robotics
and Automation, ICRA 2022, Philadelphia, PA, USA, May
23-27, 2022 , pages 933–939. IEEE, 2022. 2, 3, 6
[39] Sai Vemprala, Sami Mian, and Ashish Kapoor. Representa-
tion learning for event-based visuomotor policies. Advances
in Neural Information Processing Systems , 34:4712–4724,
2021. 1, 2
[40] Yannick Verdi ´e, Jifei Song, Barnab ´e Mas, Benjamin Busam,
Aleˇs Leonardis, and Steven McDonagh. Cromo: Cross-
modal learning for monocular depth estimation. In 2022
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 3927–3937. IEEE, 2022. 2
[41] Celyn Walters and Simon Hadfield. Ceril: Continu-
ous event-based reinforcement learning. arXiv preprint
arXiv:2302.07667 , 2023. 2
[42] Xiangjun Wang, Junxiao Song, Penghui Qi, Peng Peng,
Zhenkun Tang, Wei Zhang, Weimin Li, Xiongjun Pi, Jujie
He, Chao Gao, et al. Scc: An efficient deep reinforcement
learning agent mastering the game of starcraft ii. In Interna-
tional conference on machine learning , pages 10905–10915.
PMLR, 2021. 1
[43] Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang
Li, and Yu Qiao. Trajectory-guided control prediction for
end-to-end autonomous driving: A simple yet strong base-
line. Advances in Neural Information Processing Systems ,
35:6119–6132, 2022. 5
[44] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos,
Joelle Pineau, and Rob Fergus. Improving sample efficiency
in model-free reinforcement learning from images. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
pages 10674–10681, 2021. 2
[45] Tao Yu, Zhizheng Zhang, Cuiling Lan, Yan Lu, and Zhibo
Chen. Mask-based latent reconstruction for reinforcement
learning. Advances in Neural Information Processing Sys-
tems, 35:25117–25131, 2022. 1, 2
[46] Yunpeng Zhai, Peixi Peng, Yifan Zhao, Yangru Huang, and
Yonghong Tian. Stabilizing visual reinforcement learning
via asymmetric interactive cooperation. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 207–216, 2023. 1, 2, 5, 6, 8
[47] Amy Zhang, Rowan Thomas McAllister, Roberto Calandra,
Yarin Gal, and Sergey Levine. Learning invariant repre-
sentations for reinforcement learning without reconstruction.
InInternational Conference on Learning Representations ,
2021. 1, 2, 6
26517
[48] Biao Zhang, Xiaoyuan Zhang, Baochen Wei, and Chenkun
Qi. A point cloud distortion removing and mapping al-
gorithm based on lidar and imu ukf fusion. In 2019
IEEE/ASME International Conference on Advanced Intelli-
gent Mechatronics (AIM) , pages 966–971. IEEE, 2019. 2
[49] Jiaming Zhang, Ruiping Liu, Hao Shi, Kailun Yang, Si-
mon Reiß, Kunyu Peng, Haodong Fu, Kaiwei Wang, and
Rainer Stiefelhagen. Delivering arbitrary-modal semantic
segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1136–
1147, 2023. 2
[50] Yinuo Zhao, Kun Wu, Zhiyuan Xu, Zhengping Che, Qi Lu,
Jian Tang, and Chi Harold Liu. Cadre: A cascade deep rein-
forcement learning framework for vision-based autonomous
urban driving. In Proceedings of the AAAI Conference on
Artificial Intelligence , pages 3481–3489, 2022. 1, 5
[51] Zhuyun Zhou, Zongwei Wu, R ´emi Boutteau, Fan Yang,
C´edric Demonceaux, and Dominique Ginhac. Rgb-event fu-
sion for moving object detection in autonomous driving. In
2023 IEEE International Conference on Robotics and Au-
tomation (ICRA) , pages 7808–7815. IEEE, 2023. 1, 2, 6
[52] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Ab-
hinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven vi-
sual navigation in indoor scenes using deep reinforcement
learning. In 2017 IEEE international conference on robotics
and automation (ICRA) , pages 3357–3364. IEEE, 2017. 1
[53] Yi-Fan Zuo, Jiaqi Yang, Jiaben Chen, Xia Wang, Yifu
Wang, and Laurent Kneip. DEVO: depth-event camera vi-
sual odometry in challenging conditions. In 2022 Interna-
tional Conference on Robotics and Automation, ICRA 2022,
Philadelphia, PA, USA, May 23-27, 2022 , pages 2179–2185.
IEEE, 2022. 2
26518
