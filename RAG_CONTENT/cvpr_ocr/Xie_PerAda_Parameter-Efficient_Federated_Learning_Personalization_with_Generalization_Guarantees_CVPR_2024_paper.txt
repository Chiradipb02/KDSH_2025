PERADA: Parameter-Efﬁcient Federated Learning Personalization with
Generalization Guarantees
Chulin Xie†,‡, De-An Huang, Wenda Chu~, Daguang Xu,
Chaowei Xiao,¶,⇤, Bo Li†,§,⇤, Anima Anandkumar~,⇤
†UIUCNVIDIA~Caltech¶UW-Madison§UChicago
Abstract
Personalized Federated Learning (pFL) has emerged as a
promising solution to tackle data heterogeneity across clients
in FL. However, existing pFL methods either (1) introduce
high computation and communication costs or (2) overﬁt
to local data, which can be limited in scope and vulnerable
to evolved test samples with natural distribution shifts. In
this paper, we propose PERADA, a parameter-efﬁcient pFL
framework that reduces communication and computational
costs and exhibits superior generalization performance, es-
pecially under test-time distribution shifts. PERADAreduces
the costs by leveraging the power of pretrained models and
only updates and communicates a small number of addi-
tional parameters from adapters. PERADAachieves high
generalization by regularizing each client’s personalized
adapter with a global adapter, while the global adapter uses
knowledge distillation to aggregate generalized information
from all clients. Theoretically, we provide generalization
bounds of PERADA, and we prove its convergence to station-
ary points under non-convex settings. Empirically, PERADA
demonstrates higher personalized performance (+4.85% on
CheXpert) and enables better out-of-distribution generaliza-
tion (+5.23% on CIFAR-10-C) on different datasets across
natural and medical domains compared with baselines,
while only updating 12.6% of parameters per model. Our
code is available at https://github.com/NVlabs/PerAda .
1. Introduction
Federated Learning (FL) allows clients to collaboratively
train machine learning models without direct access to their
data, especially for privacy-sensitive tasks [ 45]. FL was ini-
tially designed to train a single global model for all clients.
However, such a one-model-ﬁts-all paradigm is not effective
when there is client heterogeneity , i.e., the local data are non-
IID across clients with heterogeneous features or label dis-
tributions [ 35]. Personalized Federated Learning (pFL) [ 43]
‡work done during an internship at NVIDIA; ⇤equal advising.Figure 1. Accuracy of personalized models on Ofﬁce-Home.
“Full”/“Partial” denotes full/partial model personalization. PER-
ADAachieves the highest personalized performance and general-
ization by updating the smallest number of model parameters.
has emerged as an effective solution to tackle client hetero-
geneity. In pFL, each client trains a personalized model
on its local data to ensure personalized performance, while
leveraging the aggregated knowledge from other clients to
improve its generalization.
Existing works in pFL commonly use full model person-
alization , where each client trains a personalized model as
well as a copy of the global model from the server for regu-
larization [ 33,59]. However, these methods are parameter-
expensive, leading to high computational and communica-
tional costs, which is impractical for clients with limited
computation resources and network bandwidth [ 26]. Later
on,partial model personalization alleviates this issue by
splitting each client’s onemodel into personalized param-
eters and shared parameters, where only the set of shared
parameters would be communicated with the server [ 48].
Nonetheless, these methods tend to overﬁt more to the lo-
cal training samples since the set of shared parameters does
not encode generalized knowledge well compared to a full
global model. This hurts the performance of partially per-
sonalized models in real-world FL deployment, where the
incoming local test samples are evolving with natural shifts
from the local training distribution [ 25], e.g., images taken
under varying weather or lighting conditions.
Our Approach. In this work, we propose PERADA, a pFL
framework thatreduces communication and computation
costs for clients while personalizing the model and maintain-
ing its generalization to test-time distribution shifts , as shown
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23838
Client!LocalDataPersonalizedAdapter!!LocalAdapter"!input
ServerGlobalAdapter#LocalAdapter""input….LocalAdapter"!Averaged logitsPublic DistillationDataKnowledge distillation lossSever sends Global AdapterClient sends Local Adapterregularizeinitialize...logits!!"(#,%#,&)PretrainedParams $❆
PretrainedParams $❆
Figure 2. Illustration of P ERADA.
in Figure 1.PERADAis a parameter-efﬁcient personalized
FL framework based on Adapter [ 50] and Knowledge Distil-
lation (KD) [ 20]. The overview is shown in Figure 2.
Each client has a pretrained model, a personalized adapter,
and a local adapter, where each adapter consists of a small
number of additional parameters planted in the pretrained
model with skip connections. At each training round, to
reduce the computation and communication costs ,PERADA
leverages the power of the pretrained model, and only up-
dates the personalized adapter and the local adapter using
local data, and sends the local adapter to the server. In this
way, it limits the number of trainable parameters and only
communicates the local adapter, instead of the full model.
Then, to improve the generalization , the server aggregates
clients’ local adapters (i.e., teachers) via knowledge distilla-
tion and trains the global adapter (i.e., student). Speciﬁcally,
it uses the averaged logits from teachers on an unlabeled pub-
lic distillation dataset as the pseudo-labels to train the student.
This avoids directly averaging clients’ models trained on het-
erogeneous local data, while enriching the global adapter
with the ensemble knowledge from clients’ models and miti-
gating the potential model aggregation drifts caused by het-
erogeneity. After that, the server sends the distilled global
adapter back to the clients, which is used to initialize the
local adapter and regularize the training of the personalized
adapter to prevent overﬁtting and improve the generalization .
During the testing phase, each client uses the personalized
adapter for inference.
To explain why PERADAis effective in improving gener-
alization, we theoretically derive its generalization bounds
under FL covariate (or feature) shift non-IID setting [ 44]. We
are the ﬁrstto show that the generalization on a target distri-
bution (e.g., potentially with test-time distribution shift) canbe enhanced for both global model and personalized models
by KD when the distillation optimization error is small , and
the distribution of the unlabeled distillation dataset is close
to the target distribution. We also characterize the role of
different components in PERADAon generalization, such
as client heterogeneity, pretrained model, and the prediction
distance between the global and personalized models.
In addition, we establish convergence guarantees for PER-
ADAin general non-convex settings. The analysis of PER-
ADAis challenging due to the bi-level optimization between
server distillation training and local client training. We es-
tablish the convergence rates for the global model and per-
sonalized models to stationary points and demonstrate the
effects of KD and client heterogeneity on the convergence.
As far as we know, these are the ﬁrst-known results for FL
convergence under server distillation .
Empirically, we conduct extensive evaluations on differ-
ent datasets, including natural and medical images (CIFAR-
10, Ofﬁce-Home, and CheXpert) under both FL covariate-
shift and label-shift non-IID settings. We show that PERADA
achieves competitive personalized accuracy over state-of-the-
art pFL methods with only 12.6% of trainable parameters
while obtaining higher generalization, especially when eval-
uated on out-of-distribution data. We further show that the
beneﬁts of PERADAextend to differentially private (DP) FL
settings and improve the DP-utility trade-offs compared to
full model personalization. In summary,
•We propose PERADA, a lightweight pFL framework with
personalized adapters that provides personalization while
reducing computation/communication costs. We improve
the generalization of P ERADAwith server-side KD.
•We theoretically analyze the effectiveness of PERADA,
and prove the generalization bounds and the convergence
rates for both the global model and personalized models
under non-convex settings.
•Through extensive experiments, we show that PERADA
achieves higher personalized performance and better gen-
eralization than state-of-the-art pFL methods with smaller
computation and communication costs. Moreover, PER-
ADAretains its beneﬁts under differential privacy.
2. Related Work
Full Model Personalization. Many pFL approaches require
each client to train a personalized model and a global model,
where the global model is used to prevent the personalized
model from overﬁtting. It includes methods based on meta
learning [ 12], model mixture [ 10,16,43], global reguarl-
ization [ 33], mean regularization [ 16,17,59] and cluster-
ing [15,54]. However, these methods induce high costs by
training two full models in each client and communicating
the full model. Another approach is to locally ﬁnetune an FL
global model (e.g., from FEDAVG[45]). While local ﬁne-
tuning yields promising personalized accuracy [ 8,62,65], it
could be prone to catastrophic forgetting and overﬁtting to its
23839
(limited) local data, sacriﬁcing the generalizability [ 25,49].
Partial Model Personalization trains one model for each
client to reduce the costs, which is partitioned into shared
parameters and personalized parameters, such as personal-
ized feature extractors [ 9], prediction head [ 3,7,38], batch
normalization [ 36], adapters [ 48], and adaptively selected
parameters [ 58]. Nevertheless, the shared parameters do
not learn generalized information well compared to a full
global model, so the partially personalized models can have
inferior generalization ability. To further reduce the costs,
Shysheya et al. [ 56] apply parameter-efﬁcient transfer learn-
ing techniques to train FEDAVGand perform local ﬁnetuning.
However, it does not speciﬁcally address the generalization
issues of personalization, which is the focus of our work.
Knowledge Distillation (KD) in FL. KD is a technique
that transfers the knowledge from one or multiple teacher
models to a student model [ 20].Ensemble distillation has
been used to tackle data heterogeneity in generic FL, by
reﬁning the server model with ensemble knowledge from
clients, rather than directly aggregating their model param-
eters. Speciﬁcally, the ensemble predictions from clients’
models on an unlabeled dataset are used to guide the train-
ing of the server model, where the unlabeled dataset can be
public data [ 6,31,39] or generated data [ 67]. Another line
of work leverages client -side local distillation to transfer
global knowledge to local models in generic FL [ 29,68]
or personalized models in pFL [ 46,66]. To reduce the load
for clients, we focus on parameter-efﬁcient ensemble distil-
lation in the server with public data to train a better global
model, and study its effects on personalized models with
novel convergence guarantees and generalization bounds.
Parameter-efﬁcient ﬁne-tuning techniques applied to
pretrained large models [ 5] have become the prominent prac-
tice in transfer learning to save computation costs [ 14,30,
40]. Motivated by the success of Adapter, a low-cost plug-in
mounted on pre-trained vision models [ 50] or large language
models [ 21,37,41], we investigate Adapter in the context of
parameter-efﬁcient personalization. Instead of training both
the backbone and adapter for pFL as in [ 48], we treat the
adapter parameters as personal and the rest of the model pa-
rameters as frozen, and further leverage sever-side ensemble
distillation to improve pFL performance.
3. Preliminaries and Challenges
We consider a typical setting of FL with Mclients
where each client mhas a training dataset Dm=
{(xm,j,ym,j),j2[nm]}with nmdata samples dawn
from its local distribution µm. Let f(W, x)represents a
model that outputs the logit vector given input x, where
W2Rd, denotes its model parameters. Let the loss
function be `(f(W, x),y), and the empirical loss on lo-
cal data Dmassociated with client mbeLm(W): =
1
nmPnm
j=1`(f(W, x m,j),ym,j).
Generic FL aims to optimize a single global model withall clients’ local data with the FL objective: min WL(W)
where L(W): =1
MPM
m=1Lm(W). A standard way to
solve it is FEDAVG, which iterates between local model
training and global model aggregation for multiple commu-
nication rounds. However, due to the heterogeneous local
data distributions among clients, local model would drift
away from each other, making the aggregated global model
deviate from the optimal solution.
Personalized FL learns a personalized model for each
client to perform well on its local data while preventing
overﬁtting by leveraging the knowledge from other clients.
However, achieving the goal is non-trivial due to the fol-
lowing challenges: (1) High costs : existing full model
personalization studies [ 12,16,33,59], which optimize
min W,{Vm}1
MPM
m=1(Lm(Vm)+ 
2kVm Wk2), require
twice the memory footprint of the full model at each client
by locally updating personalized model Vm2Rdand global
model W2Rdwhere  is the `2regularization weight
controlling the extent of personalization. (2) Limited gener-
alization : partial model personalization [ 7,9,38,48] is more
efﬁcient by training a full model Vm=(u, vm)at each client
and communicating a subset of parameters, where u2Rdu
are shared parameters and vm2Rdvare personal param-
eters: min u,{vm}1
MPM
m=1Lm(u, vm).However, such a
partially personalized model can be dominated by personal
knowledge with vmandpoor at encoding generalized knowl-
edge with the remaining ufrom global distribution, leading
to inferior performance under test-time distribution shifts.
Figure 3depicts such challenges in existing studies.
4. Method
Here we introduce the objectives and algorithm for PERADA.
Personalized and Global Objectives of PERADA.We
address the challenges discussed in Sec. 3by proposing
PERADA, which improves the efﬁciency of learning per-
sonalized adapters and enhances their generalization with
regularization and KD. Speciﬁcally, we (1) train the person-
alized adapter {vm}regularized towards a global adapter
wto optimize a personalized objective ( Personal Obj ), and
(2) train a well-generalized wvia KD to optimize a global
objective ( Global Obj ) under non-IID data, where we use
thealternative optimization between client local training of
local adapter {✓m}and server KD training of w.
Concretely, we improve the efﬁciency of partial model
personalization with a pretrained model and personalized
adapters. Here the personalized adapter consists of a small
number of additional parameters with skip connections (in
Figure 2), which can reduce to the identity function when
its parameters are zero [ 50,66]. Our personalized adapter
is trained with regularization to prevent overﬁtting, yielding
the personal objective of each client m:
min
vmPm(vm,w): = Lm(u, vm)+ 
2kvm wk2,
(Personal Obj)
23840
where u2Rdudenotes the ﬁxed pretrained parameters, and
vm,w2Rdaarepersonalized adapter andglobal adapter ,
respectively, with da⌧du.
Since the global adapter wis trained with all client data,
regularizing vmwith wcould potentially boost vm’s general-
ization power. Thus, enhancing w’s generalization capacity
is crucial for training a personalized model that demonstrates
robust generalization as well. Instead of using FEDAVG[45]
to learn was in regularization-based pFL method [ 33], we
leverage server-side ensemble distillation [ 39] to enrich the
global adapter with ensemble knowledge from clients’ mod-
els and alleviate model aggregation drifts induced by client
heterogeneity, yielding the global objective:
min
wRKD(u,{✓m}M
m=1,w) (Global Obj)
where ✓m= arg min
✓Lm(u, ✓),initialized with w.
Here ✓m2Rdais client m’slocally updated global
adapter , and we call it as local adapter for distinguish-
ment. The KD loss is deﬁned as: RKD(u,{✓m}M
m=1,w): =Pnaux
j=1`KD(PM
m=1f((u,✓m),xj)
M,f((u, w),xj)),which is the
average distillation loss (between the averaged logits of local
models and logits of the global model) on an auxiliary (unla-
beled) dataset Daux={xj}naux
j=1drawn from the distribution
µaux. Here `KD(a, b)=KL( (a), (b))is Kullback-Leibler
divergence loss where  is softmax function [ 20]. Compared
to server-side KD in generic FL [ 6,39,67], we only update
adapters instead of full models, which is more efﬁcient for
training and communication.
Algorithm 1 PERADAwith client and server training
1:Input: Mclients, pretrained model parameters u, initialized adapters
w0,{v0
m}, local datasets {Dm}, an unlabeled dataset Daux
2:Output: Personalized adapters vT
1,...,vT
M
3:forcommunication round t2[T]do
4: St Server samples Cclients from Mclients
5: Server sends global adapter wtto the selected clients
6: forclient m2S tdo
7: Client initializes personalized adapter vt,0
masvt
m
8: forsteps2[S]do
9: // update personalized adapter
10: vt,s+1
m  vt,s
m ⌘p⇣
frLm⇣
u, vt,s
m⌘
+ ⇣
vt,s
m wt⌘⌘
11: Client sets vt+1
m vt,S
m
12: Client initializes local adapter ✓t,0
maswt
13: forstepe2[E]do
14: // update local adapter
15: ✓t,e+1
m  ✓t,e
m ⌘lerLm(u, ✓t,e
m)
16: Client sends local adapter ✓t+1
m ✓t,E
mto server
17: Server initializes the global adapter wt,0by averaging
18: wt,0 P
m2St1
|St|✓t+1
m
19: forstepr2[R]do
20: // update global adapter
21: wt,r+1 wt,r ⌘gerwRKD(u,{✓t+1
m}m2St,wt,r)
22: Server sets wt+1 wt,R
PERADAAlgorithm. Now we introduce the details of
iteratively optimizing the personalized objective and the
global objective. Algorithm 1presents our workﬂow. AtPersonalizedParams!!GlobalParams"regularizePretrainedParams #❆PersonalizedParams!!Global Params #Personalized Model !!regularizeGlobal Model "(a) Full Model pFL(b) Partial Model pFL(e.g., personalized output layer)(c) PerAda with frozen ❆ pretrained model and personalized adapter(Aggregating ! with ensemble distillation)Figure 3. Current full model personalization incurs high computa-
tion costs by training two models, whereas existing partial model
personalization often falls short in terms of generalizability. By up-
dating adapter only, PERADAachieves a favorable balance between
training/communication costs of clients and their pFL performance.
each communication round t2[T], the server selects C
clients Stand broadcasts the current global adapter wt.
To optimize personalized objective , each selected client
m2Stinitializes personalized adapter as vt,0
m vt
m, and
updates it for Ssteps with learning rate ⌘pand mini-batches
{⇠t,s
m}S 1
s=0sampled from Dm(Line 10). The client sets
personalized adapter vt+1
m vt,S
mafter training. To op-
timize global objective , each selected client minitializes
local adapter as the received global adapter ✓t,0
m wt, and
makes local updates for Esteps with learning rate ⌘land
mini-batches {⇠t,e
m}E 1
e=0sampled from Dm(Line 15). Then
client msends the updated local adapter ✓t+1
m ✓t,E
mto
server. After receiving local adapters, the server ﬁrst initial-
izes the global adapter by parameter-averaging wt,0 ¯✓t+1
m
where ¯✓t+1
m:=P
m2St1
|St|✓t+1
m. Then, the server updates
global adapter for Rsteps via knowledge distillation from
local adapters (Line 21) with learning rate ⌘gand batches
{⇠t,r}R
r=1sampled from Daux. The server will send the up-
dated global adapter as wt+1 wt,Rto clients at the next
communication round.
5. Generalization Bounds of P ERADA
In this section, we analyze the generalization bounds for
PERADAby answering the questions: how do the distillation
data distribution and KD optimization impact the generaliza-
tion of the global model? How does the global model impact
the generalization of personalized models?
For notation simplicity, we deﬁne p1,···,pMas the per-
sonalized hypothesis, where each hypothesis pm2P m:
X![0,1]kmaps the input x2Xto a probability vec-
torover the kclasses (i.e., softmax outputs). Similarly,
we deﬁne global hypothesis g2Gand local hypothesis
hm(x)2H m,8m2[M]. We call “hypothesis” as “model”
in this section. The local dataset Dmof each client mis
drawn from the local distribution µm, and the distillation
dataset Dauxof the server is drawn µaux. We study gener-
alization of the global model and personalized models on
atarget distribution µof interest (e.g., with distribution
shifts) , by analyzing the effect of local distributions {µm}
and distillation distribution µauxused in FL training .W e
focus on the generalization bounds under FL covariate shifts
following [ 44] and defer all proofs to Appendix C.
Global Model. Previous KD-based FL generalization
23841
bounds [ 39,68] simply assume a perfect distillation (i.e.,
the global model is the ensemble of local models) which
neglects the actual distillation errors and the choice of
distillation distribution. To take them into account, we
deﬁne the ensemble distillation distance onnauxpoints
{xi}naux
i=1drawn from µauxas: µaux,naux(h1,...,h M;g): =
1
nauxPnaux
i=1kg(xi) 1
MPM
m=1hm(xi)k1which measures
the output difference between the global model and the en-
semble of local models. To show gcan have good generaliza-
tion bounds on µwith KD, our main idea is to bound error
probabilities of gwith the expected distillation distances and
errors of local models, and then bound the errors on µbyµm
based on prior arts from domain adaptation [ 4]. We defer
the preliminaries about learning theory to Appendix C.3.
Theorem 1 (Generalization bound of PERADAglobal
model) .Consider empirical datasets D⇠µ,Daux⇠
µaux,Dm⇠µmwith |D|=|Dm|=n,|Daux|=naux.
Let dmbe the VC dimension of Hm,Rad nauxbe
the empirical Rademacher complexity measured
onnauxsamples. With probability at least 1  ,
for every hm2H m,8m2[M]and g2G, we
have Pr
(x,y)⇠µ
arg max
y0g(x)y06=y 
2E
(x,y)⇠µ[1 
g(x)y]O (k3/2[max j(1
MPM
m=1Rad naux(Hm|j)) +
max jRad naux(G|j)])+6
MMP
m=1(4
3q
2dmlog(2 n)+log(6 M/ )
n+
q
log(6 M/ )
2n+q
log(6 / )
2naux+ O(Rad n(Hm))) +
1
MMP
m=1(2ERR(Dm,hm)| {z }
local empirical risk+ˆdH H(Dm,D)| {z }
client heterogeneity+ m)+
2 µaux,naux(h1,...,h M;g)| {z }
ensemble distillation distance+4TV(µ, µ aux)|{z }
TV divergence, where
ERR(Dm,hm)=1
nPn
j=1⇥
1 hm(xm,j)ym,j⇤
, m=
"µm(h⇤)+"µ(h⇤),h⇤:= arg min h2H"µm(h)+"µ(h).
Remark 1.We discuss key implications of Theorem 1: (1)
Ensemble distillation.  µaux,nauxcaptures the distillation
error measured on the distillation dataset Dauxas minimized
in Line 21. When µaux=µ, e.g., using data from the target
distribution as the distillation dataset, KD improves the
generalization of gduring training by directly minimizing
 µaux,naux. The smaller the distillation distance, the better
the generalization. When µaux6=µ, KD on µauxdecreases
 µaux,nauxwhile causing additional generalization gap
measured by TV divergence TV(µaux,µ). Compared to
without KD, using a distillation dataset from a domain
close to µwith small TV(µaux,µ)and reducing  µaux,naux
during KD can also improve the generalization (e.g., when
 µaux,naux+2TV(µaux,µ) µ,naux). We empirically verify
the effect of different distillation datasets in Sec. 7.1. (2)
Quality of local models. The ERR(Dm,hm)term shows
that reducing the empirical risk of local models w.r.t localdistributions µmimproves the generalization of the global
model. We verify in Sec. 7.1that a more powerful pretrained
model, which results in higher quality local models, leads
to better generalization. (3) Sample complexity. More
empirical samples during training improve the generaliza-
tion. We further discuss the effect of client heterogeneity
ˆdH H(Dm,D)(i.e., the empirical H-divergence between
two datasets) and number of classes kin Appendix C.1.
Personalized Models. We show that personalized model
pmcan generalize well on µif global model ggeneralizes
well on µandpmhas small prediction distance with g.
Theorem 2 (Generalization bound of PERADA per-
sonalized model) .With probability at least 1  ,
for every pm2P m,8m2[M], and for every
g2G, we have Pr(x,y)⇠µ
arg max
y0pm(x)y06=y 

2E(x,y)⇠µ(1 g(x)y)+21
nPn
i=1min{1,kpm(x) g(x)k1}+
6q
log(2 / )
2n+O 
k3/2[max jRad n(P|j) + max jRad n(G|j)] 
.
Remark 2.The ﬁrst term is the population risk of gonµ,
which has been upper bounded by Theorem 1. The second
term is the prediction difference between gand personalized
models. Therefore, the generalization of personalized model
is intrinsically related to the performance of global model. In
Sec. 7.1, we empirically show that moderately increasing the
regularization strength  in (Personal Obj ) could improve the
generalization of pm, by reducing such prediction distance.
6. Convergence Guarantees of P ERADA
In this section, we aim to provide the convergence analysis.
We outline the analysis challenges for PERADA, arising from
the bi-level optimization between server distillation and local
training, as well as the personalization regularized by the
global model. Then, we present the convergence analysis
forPERADAglobal model and personalized model. For
notation simplicity, we will omit the frozen parameters u
and use w/✓m/vmto represent corresponding models.
To convey the salient ideas, we consider full client par-
ticipation (i.e., |St|=M) for convergence analysis follow-
ing [46,52]; thus, the stochasticity comes from mini-batch
samplings during client and server training. Below, we ﬁrst
give several necessary assumptions.
Assumption 1. (Smoothness). Lm(✓)isL-Lipschitz smooth
8m2[M]andR({✓m},w)isLR-Lipschitz smooth.
Assumption 2. (Bounded Variance). The stochastic gra-
dients are unbiased and variance is bounded 8m2[M]:
EkerLm(✓) rL m(✓)k2 2,EkerwR({✓m},w) 
rwR({✓m},w)k2 2
R.
Assumption 3. (Bounded Diversity). The variance
of local gradients to global gradient is bounded
1
MPM
m=1krL m(w) 1
MPM
i=1rLi(w)k2¯ .
23842
Assumption 4. (Bounded Gradients). The func-
tions Lm,R,Pm,8m2[M]have bounded gradi-
ents: krL m(✓)kG,krwR({✓m},w)kGR,
krwPm(vm,w)kGP.
We defer more discussions on the assumptions to Ap-
pendix D.1. Next, we discuss the challenges and present the
main results. All proofs are relegated to Appendix D.
Global Model Convergence with Ensemble Distillation.
Despite the wide applications of knowledge distillation in
FL [29,66,68], its convergence analysis is less explored. To
the best of our knowledge, there is no convergence guaran-
tee under server-side ensemble distillation [ 6,31,39,67].
This lack of research is likely because (1) the complexity of
bi-level optimization between server distillation for wtand
client training for {✓t
m}, which incorporates two objectives
(i.e., minimizing distillation loss and local loss respectively);
(2) at each round, the global model is initialized by averaged
local models before distillation, and local models are initial-
ized by the global model before local training. Such mutual
initializations intervene in the model updating trajectories
ofwtand{✓t
m}w.r.t their training objectives, making the
convergence even harder to analyze. On the other hand, it
has been empirically shown that ensemble distillation can
improve the global model performance by incorporating di-
verse knowledge from clients (e.g., low L(wt)measured on
all clients’ data) [ 6,31,39,67].Therefore, we aim to under-
stand the global model convergence w.r.t L(wt)as a function
of ensemble distillation . To overcome the aforementioned
challenges, we regard {✓t
m}as the intermediate models to
update wt+1, and quantify the effects of local client training
and server distillation on optimizing FL global objective:
Theorem 3 (Convergence of PERADAglobal model) .Let
Assumptions 1to4hold, and ⌘l=1
ELp
T,⌘g=1
LRRT,
denote ¯wt,e=1
MPM
m=1✓t,e
m, then the algorithm satisﬁes
T 1X
t=0E 1X
e=0EkrL(¯wt,e)k2
ETO⇣L L+ 1p
T+¯ 2
T+L2 2
Tp
TL2
RE⌘
,
where  L=L(w0) L(wT), 1= 2
EM+L(G2+ 2)
ELR,
and 2=4 2
R+ 32(3 G2
R+2 2
R
R)/T2+2G2
R. In particular,
¯wt+1,0=wtand ¯wt+1,E 1=¯✓t+1.
Remark 3.(1)Convergence rate isO(1/p
T)as it is the
dominant term, matching the rate of the general FL non-
convex settings of our interest [ 46,59]. (2) Local steps &
distillation steps. With more local updating steps Eand
distillation steps R, the terms  1and 2decrease. It means
that a larger EandRcan reduce the required communica-
tion rounds Tto converge, thus lowering communication
costs. (3) Client heterogeneity is reﬂected in ¯ , whose ef-
fect can be mitigated by larger T. (4) Ensemble distillation
is mainly reﬂected in  2where  2
Rare inherent data sam-
pling noise when using stochastic gradients [ 12,59], and GR
is from the bounded gradient assumption for distillation. Thedistillation gradient can be small when the averaged logits
of local models (teacher) and the logits of the global model
(student) are close (See Equation ( 11) and more discussion
in Appendix D.1). Notably, the convergence bound remains
valid for any distillation data, even if it is out-of-domain .
Personalized Model Convergence. Regarding personaliza-
tion, unlike [ 59], to preserve generalization, the global model
wtofPERADAis not updated based on the personalized ob-
jective P(vt
m,wt). Thus, it remains unclear how the global
model wtlearned from the ensemble distillation impacts
the convergence of personalized models w.r.t P(vt
m,wt).In
Theorem 4(Appendix D.1), we analyze such impacts and
show the convergence rate of personalized models.
7. Experiments
We empirically compare PERADAto existing pFL methods.
We defer the details of experiments and hyperparameter as
well as the additional experimental results to Appendix A.
Data and Model. We use CIFAR-10 [ 28], Ofﬁce-Home [ 61],
and medical image data CheXpert [ 24]. We simulate pFL
setting for (1) label Non-IID using Dirichlet distribution
Dir(↵)[23] with ↵=0.1/0.3on CIFAR-10/CheXpert, cre-
ating different local data size and label distributions for M
clients; and (2) feature Non-IID on Ofﬁce-Home by dis-
tributing the data from 4 domains (Art, Clipart, Product, and
Real Word) to 4 clients respectively [ 58]. We use M= 20
for CIFAR-10/CheXpert, and sample 40% clients at every
round following [ 7,39], and use full client participation for
Ofﬁce-Home following [ 58]. We use ResNet-18 pretrained
on ImageNet-1K [ 53] for all datasets. For PERADA1, we use
out-of-domain distillation dataset CIFAR-100 for CIFAR-10,
and use CIFAR-10 for Ofﬁce-Home/CheXpert.
Baselines. We evaluate full model pFL methods FE-
DAVG+FT [65],DITTO [33],APFL [10],MTL [57],
PFEDME[59], and partial model pFL methods with
decoupled personalized/global parameters , including
FEDBN [36],LG-F EDAVG[38],FEDREP[9],FED-
SIM[48],FEDALT[48]. We also include PERADA W /OKD,
which is PERADAwithout Line 21server-side knowledge
distillation (i.e., using FEDAVGto aggregate global adapter).
Note that we use the same pretrained ResNet as initialization
for all methods for fair comparisons.
Evaluation Metrics. We report the averaged test accuracy
(pFL accuracy ) and standard deviation over all clients’ per-
sonalized models . For CheXpert, we report the AUC score
since it is a multi-label classiﬁcation task. We evaluate
pFL accuracy mainly under two metrics: Local-test (i.e.,
clients’ corresponding local test data) and Global-test (i.e.,
the union of clients’ local test data), to study the personal-
ized performance andgeneralization (against label or co-
variate shifts), respectively. In addition, for CIFAR-10, we
evaluate pFL generalization against distribution shifts on
1We follow [ 48] to implement Adapter, which includes prediction head.
23843
Table 1. Parameter-efﬁciency and averaged test accuracy across all clients’ personalized models. PERADAachieves higher personalized
performance and generalization with a smallest #of trainable parameters. bold/Underline fonts highlight the best/runner-up approach.
AlgorithmPersonalized
Params#Trained
Params#Comm.
ParamsCIFAR-10 Ofﬁce-Home CheXpert
Local-test Global-test CIFAR-10.1 CIFAR-10-C Local-test Global-test Local-test Global-test
STANDALONE Full model 11.18 M 0M 85.94 ±8.8229.77 ± 8.09 25.82 ± 6.27 26.67 ± 7.07 81.64 ±6.0859.15 ± 3.32 65.06 ±1.8865.45 ± 2.3
MTL [ 57] Full model 11.18 M 11.18 M 86.24 ±8.4529.46 ± 8.33 25.64 ± 6.42 26.4 ± 7.29 81.82 ±5.5359.25 ± 2.84 65.15 ±1.9565.48 ± 2.3
FEDAVG+FT [ 65] Full model 11.18 M 11.18 M*88.91 ±5.7143.99 ± 9.57 35.49 ± 8.02 36.51 ± 8.36 79.42 ±5.6277.19 ± 0.56 70.16 ±0.7870.6 ± 0.31
PFEDME[59] Full model 22.36 M 11.18 M 90.73 ±4.6745.06 ± 8.65 36.51 ± 7.2 37.65 ± 7.6 80.21 ±5.3275.69 ± 0.69 65.07 ±1.264.86 ± 1.22
APFL [ 10] Full model 22.36 M 11.18 M 90.74 ±4.7543.92 ± 9.18 35.83 ± 7.5 36.51 ± 7.94 81.24 ±4.5176.98 ± 1.39 68.98 ±1.0468.96 ± 1.1
DITTO [33] Full model 22.36 M 11.18 M 90.21 ±4.6153.82 ± 6.35 42.72 ± 5.68 44.32 ± 5.73 81.77 ±4.3175.66 ± 1.01 68.79 ±1.468.86 ± 1.22
FEDBN [ 36] Batch norm. 11.18 M 11.17 M 90.37 ±5.1943.18 ± 8.67 35.01 ± 7.24 36.29 ± 7.43 81.86 ±5.1374.26 ± 0.52 68.74 ±1.1768.83 ± 1.08
FEDALT[48] Input layer 11.18 M 6.45M 87.07 ±6.5432.23 ± 8.23 27.49 ± 6.41 28.51 ± 7.11 81.07 ±5.5965.85 ± 0.9 67.63 ±1.1867.74 ± 1.1
FEDSIM[48] Input layer 11.18 M 6.45M 87.93 ±6.2533.07 ± 8.16 28.21 ± 6.41 29.15 ± 7.16 82.45 ±5.0367.66 ± 0.82 67.49 ±1.3267.54 ± 1.24
LG-F EDAVG[38] Feat. extractor 11.18 M 0.005 M 86.7 ±8.0129.96 ± 8 25.97 ± 6.21 26.83 ± 6.95 82.04 ±5.9663.57 ± 2.32 65.78 ±1.6266.23 ± 1.75
FEDREP[9] Output layer 11.18 M 11.17 M 87.76 ±6.4635.19 ± 6.97 30.15 ± 5.89 30.68 ± 6.31 79.05 ±5.8874.17 ± 2.02 66.66 ±1.8266.52 ± 1.47
FEDALT[48] Output layer 11.18 M 11.17 M 89.68 ±5.440.68 ± 7.3 33.61 ± 6.12 34.3 ± 6.5 83.24 ±3.9670.62 ± 1.46 68.27 ±1.368.36 ± 1.31
FEDSIM[48] Output layer 11.18 M 11.17 M 89.75 ±5.5141.98 ± 7.66 34.21 ± 6.22 35.31 ± 6.79 82.91 ±4.4672.34 ± 0.51 68.22 ±1.3468.12 ± 1.24
FEDALT[48] Adapter 12.59 M 11.18 M 87.26 ±7.7831.51 ± 8.55 27.38 ± 6.65 27.77 ± 7.19 81.41 ±6.557.88 ± 3.57 72.13 ±1.3474.67 ± 1.57
FEDSIM[48] Adapter 12.59 M 11.18 M 87.76 ±7.5731.97 ± 7.44 27.76 ± 5.78 28.1 ± 6.46 82.14 ±5.4658.62 ± 3.24 71.75 ±1.474.09 ± 1.55
PERADA W /OKD Adapter 2.82 M 1.41M 91.27 ±5.1553.81 ± 6.27 42.5 ± 5.06 44.45 ± 5.48 83.31 ±5.5476.55 ± 2.47 76.77 ±2.2477.59 ± 2.18
PERADA Adapter 2.82M 1.41M 91.82 ±4.4359.05 ± 5.24 47.25 ± 4.48 48.53 ± 4.74 83.58 ±4.74 77.2 ± 1.63 76.98 ±3.8777.88 ± 1.55
*FEDAVG+FT requires full model communciation during F EDAVGtraining and there is no communciation during local ﬁnetuning.
CIFAR-10.1 [ 51] and common image corruptions (e.g. Blur,
Gaussian Noise) on CIFAR-10-C [ 19].
7.1. Evaluation Results
PERADAis parameter-efﬁcient. ResNet-18 model con-
sists of 11.18 million (M) parameters, and the adapter has
1.41M (12.6%) parameters. Tab. 1reports each client’s #
trainable parameters and # communicated parameters to the
server. We see that PERADAis most parameter-efﬁcient
by locally training two adapters and communicating one
adapter. Most full model pFL requires training two full mod-
els ( PFEDME,A P F L ,D ITTO ), and sends one full model to
the server. Partial model pFL requires training one full model
and communicating its shared parameter. Note that adapter-
based partial model pFL in F EDALTand F EDSIMare more
expensive than PERADAbecause they still need to train both
a personalized adapter plus a shared full model (12.59M),
and communicate the full model. Additional comparison
under ResNet-34 shows similar conclusions in Figure 1.
PERADAachieves competitive personalized performance
and better generalization than baselines. Tab. 1shows
that even with the smallest number of trainable parame-
ters, PERADAachieves the comparable personalized per-
formance (+1.08%, 0.34%, 4.85% on CIFAR-10, Ofﬁce-
Home, CheXpert) and better generalization (+5.23%, 4.53%,
4.21%, 0.22%, 3.21% on CIFAR-10, CIFAR-10.1, CIFAR-
10-C, Ofﬁce-Home, CheXpert). Speciﬁcally, (a)PERADA
W/OKDalready achieves favorable performance compared
to the best baseline, which shows that the plug-in module
adapter can adapt the pretrained model to FL data distribu-
tions, and personalized adapter can successfully encode both
local knowledges (with local empirical risk) and generalized
knowledge (with regularization). (b)PERADAoutperforms
PERADA W /OKD, which shows that KD improves the gen-
eralization of personalized models (Theorem 2).We present
the convergence curves in Figure 6(Appendix B) to show
the learning performance from the convergence perspective,
where P ERADAachieves the best convergence speed.Table 2. Generalization comparison of the global model from
different generic FL and pFL methods on CIFAR-10.
Algorithm Algorithm Type Trained Params Global-test CIFAR-10.1 CIFAR-10-C
FEDAVG[45] generic FL Full 69.34 54.95 57.07
FEDPROX [32] generic FL Full 69.64 54.75 56.84
FEDDYN[2] generic FL Full 70.36 56.3 55.91
FEDDF [ 39] (w/ KD) generic FL Full 74.83 60.95 61.23
PFEDME[59] pFL Full 68.25 52.55 56.33
APFL [ 10] pFL Full 69.79 53.6 57.06
DITTO [33] pFL Full 69.95 55.25 57.33
PERADA W /OKD pFL Adapter 74.22 57.6 61.40
PERADA pFL Adapter 76.77 62.5 64.47
To verify that such improvement of pFL is due to an
improved global model (Theorem 1), we compare the perfor-
mance of the global model ofPERADAto the global model of
state-of-the-art methods in pFL ( PFEDME,APFL ,DITTO )
and generic FL ( FEDAVG,FEDPROX [32],FEDDYN[2],
FEDDF[39]).Note that FEDDF[39] also uses ensemble
knowledge distillation for global model aggregation, but up-
dates the full model. Tab. 2shows that the generalization
ofPERADAglobal model with adapter also outperforms
baselines, and KD indeed improves our global model.
Existing partial model pFL can have poor generalization
to out-of-distribution shifts. As shown in Tab. 1, these
methods, while showing promising personalized accuracy
on CIFAR-10 and sometimes outperform full model pFL
on Ofﬁce-Home and CheXpert by personalizing the right
model component, they signiﬁcantly lag in generalizing to
test-time distribution shifts. (a)Compared to full model
pFL, the root causes of this inferior generalization in exist-
ing partial model pFL methods are twofold: (i)a smaller
number of shared parameters prevents them from effectively
learning global information; (ii)personalized parameters
can predominately encode local information for the partially
personalized model. PERADAcircumvents such issues by
regularization, which enforces personalized adapters to learn
both local and global information. (b)Moreover, the fact
thatPERADAeven w/o KD has better generalization than ex-
isting partial pFL methods suggests that updating the shared
parameters globally via FL on heterogeneous data can com-
promise the pretrained feature exactor. Our ﬁndings indicate
23844
Table 3. Averaged test accuracy across personalized models with
data heterogeneity degrees Dir(1) andDir(0 .3)on CheXpert. PER-
ADAachieves best personalized performance and generalization.
Algorithm PersonalizationLocal-test Global-test
Dir(1) Dir(0 .3) Dir(1) Dir(0 .3)
STANDALONE Full 64.69 ± 1.6365.06 ± 1.8865.32 ± 1.765.45 ± 2.3
MTL Full 65.18 ± 1.9565.15 ± 1.9565.67 ± 1.7265.48 ± 2.3
PFEDME Full 64.8 ± 1.465.07 ± 1.264.85 ± 1.2564.86 ± 1.22
APFL Full 69.21 ± 1.2368.98 ± 1.0469.21 ± 1.0568.96 ± 1.1
DITTO Full 68.65 ± 0.8268.79 ± 1.468.72 ± 0.5875.55 ± 0.34
FEDBN BN 69.09 ± 0.7968.74 ± 1.1769.03 ± 0.5768.83 ± 1.08
FEDALT Input 67.74 ± 0.8567.63 ± 1.1867.88 ± 0.667.74 ± 1.1
FEDSIM Input 67.65 ± 0.8867.49 ± 1.3267.82 ± 0.6167.54 ± 1.24
LG-F EDAVG Feat. extractor 65.77 ± 1.4865.78 ± 1.6266.33 ± 1.3866.23 ± 1.75
FEDREP Output 66.42 ± 1.6266.66 ± 1.8266.49 ± 1.5366.52 ± 1.47
FEDALT Output 68.31 ± 0.7968.27 ± 1.368.41 ± 0.4768.36 ± 1.31
FEDSIM Output 68.51 ± 0.8268.22 ± 1.3468.63 ± 0.5768.12 ± 1.24
FEDALT Adapter 72.52 ± 0.9972.13 ± 1.3474.79 ± 1.2174.67 ± 1.57
FEDSIM Adapter 72 ± 1.2671.75 ± 1.474.3 ± 1.5174.09 ± 1.55
PERADA W /OKD Adapter 77.45 ± 1.2176.77 ± 2.2478.02 ± 1.3677.59 ± 2.18
PERADA Adapter 77.47 ± 1.5476.98 ± 1.8178.02 ± 1.5577.88 ± 1.55
that maintaining frozen parameters, as done in PERADA
without KD, is more effective in preserving the capabilities
of the pre-trained model.
Adapter-based personalization methods are generally
effective on CheXpert. Tab. 1shows that adapter-based
personalization, including FEDALT,FEDSIM,PERADA, are
especially effective on the X-ray data CheXpert. This con-
clusion holds under different degrees of data heterogeneity
Dir(0 .3)andDir(1) in Tab. 3. It indicates that when adapt-
ing to FL domains that have a large domain gap for ImageNet
pre-trained models, e.g., medical domains, adapter personal-
ization may be preferable to input/output/batch-norm pFL.
Effects of KD. We use CIFAR-100 as the distillation dataset
on CIFAR-10, and Figure 4shows that more distillation
steps and distillation data samples are better for pFL gen-
eralization. These results echo our theoretical analysis in
Theorem 1that smaller KD optimization error  µaux,nauxand
a larger number of samples can tighten the generalization
bounds. We also evaluate different distillation datasets, and
Figure 4shows that out-of-domain datasets (STL-10, CI-
FAR100) can improve generalization compared to the one
without KD (None) by a margin, and achieve comparable
performance compared to in-domain CIFAR10 validation
data. The ﬂexibility of choosing distillation datasets makes it
practical for the server to leverage public data for KD.
Another potential way to improve generalization is by
moderately increasing regularization strength  for less per-
sonalization. However, Figure 7(Appendix B) show that
an overly large  degrades the personalized performance,
which matches the observation for `2regularization-based
pFL methods in [ 48]. Notably, KD does not have such a
negative impact on personalized performance (in Figure 4).
Effects of pretrained models. Starting personalization
from a pretrained model, such as FEDAVGglobal model,
is commonly considered in pFL [ 44,48]. Therefore, we ﬁrst
train a ResNet-18 global model on FL data from scratch
using FEDAVGand utilize it as initialization for pFL. Re-
sults in Figure 5show that PERADAalso achieves compa-
rable personalized performance and higher generalization
than baselines with FEDAVGpretrained model. Moreover,Figure 4. Effect of KD on PERADAevaluated on CIFAR-10. More
distillation steps and data samples lead to better generalization
and out-of-domain distillation data (STL-10, CIFAR-100) achieve
similar performance as in-domain (validation) data.
Figure 5. Effect of different initializations (Random, FEDAVG
model, and ImageNet pretrained model).
ImageNet-pretraining leads to better generalization than FE-
DAVG-pretraining for PERADA, which echos Theorem 1
that high-quality local models (enabled by good pretrained
model) can further improve generalization.
Utility under differential privacy guarantees. To further
protect local data privacy, we train our method under sample-
level (✏,  )-differential privacy (DP) [ 11] on CIFAR-10
with a ViT-S/16-224 model2. Following [ 42], we consider
full client participation and perform local training with DP-
SGD [ 1] for both personalized models and the global model
(see experimental details in Appendix A); We set  = 10 5
and report averaged ✏across all clients and averaged pFL
accuracy under Local-test . Tab. 4shows that (1) PERADA
W/OKDretains higher utility than full model personaliza-
tion DITTO under reasonable privacy guarantees due to a
smaller number of trainable parameters and the whole model
is less impacted by DP noise. (2) KD with unlabeled pub-
licdata in PERADAcan further improve the utility without
consuming additional privacy budgets.
Table 4. PERADAretains high personalized utility under DP guar-
antee on CIFAR-10 with ViT-S/16-224 model.
Algorithm Personalization ✏=1 ✏=5.99±3.03 ✏=3.7±2.12 ✏=1.81±1.12
Ditto Full 98.59 ±1.63 76.76 ±24.14 76.75 ±24.13 76.67 ±24.12
PERADA W /OKD Adapter 97.69 ±1.79 77.49 ±21.21 77.32 ±21.16 76.68 ±21
PERADA Adapter 98.08 ±1.28 80.33 ±20.76 79.79 ±20.45 77.83 ±19.58
8. Conclusion
We propose a pFL framework PERADA based on
global/personalized adapter and knowledge distillation with
convergence and generalization guarantees, and show that it
reduces computation and communication costs and achieves
higher personalized performance and generalization.
2As batch normalization layer in ResNet creates dependencies between
samples and violates DP, we use ViT model [ 64] for DP experiments.
23845
References
[1]Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMa-
han, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learn-
ing with differential privacy. In Proceedings of the 2016
ACM SIGSAC conference on computer and communications
security , pages 308–318, 2016. 8,13
[2]Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew
Mattina, Paul Whatmough, and Venkatesh Saligrama. Fed-
erated learning based on dynamic regularization. In Inter-
national Conference on Learning Representations , 2020. 7,
14
[3]Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Ku-
mar Singh, and Sunav Choudhary. Federated learning with
personalization layers. arXiv preprint arXiv:1912.00818 ,
2019. 3
[4]Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza,
Fernando Pereira, and Jennifer Wortman Vaughan. A theory
of learning from different domains. Machine learning , 79(1):
151–175, 2010. 5,16,18
[5]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-
man, Simran Arora, Sydney von Arx, Michael S Bernstein,
Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.
On the opportunities and risks of foundation models. arXiv
preprint arXiv:2108.07258 , 2021. 3,15
[6]Hong-You Chen and Wei-Lun Chao. Fedbe: Making bayesian
model ensemble applicable to federated learning. In Interna-
tional Conference on Learning Representations , 2020. 3,4,
6
[7]Hong-You Chen and Wei-Lun Chao. On bridging generic and
personalized federated learning for image classiﬁcation. In
International Conference on Learning Representations , 2022.
3,6
[8]Hong-You Chen and Wei-Lun Chao. On bridging generic and
personalized federated learning for image classiﬁcation. In
International Conference on Learning Representations , 2022.
2
[9]Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay
Shakkottai. Exploiting shared representations for person-
alized federated learning. In International Conference on
Machine Learning , pages 2089–2099. PMLR, 2021. 3,6,7
[10] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad
Mahdavi. Adaptive personalized federated learning. arXiv
preprint arXiv:2003.13461 , 2020. 2,6,7
[11] Cynthia Dwork, Aaron Roth, et al. The algorithmic foun-
dations of differential privacy. Foundations and Trends ®in
Theoretical Computer Science , 9(3–4):211–407, 2014. 8
[12] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Per-
sonalized federated learning: A meta-learning approach.
NeurIPS , 2020. 2,3,6,25
[13] Dylan J Foster and Alexander Rakhlin. `1vector contraction
for rademacher complexity. arXiv preprint arXiv:1911.06468 ,
6, 2019. 17
[14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-
adapter: Better vision-language models with feature adapters.
arXiv preprint arXiv:2110.04544 , 2021. 3[15] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ram-
chandran. An efﬁcient framework for clustered federated
learning. Advances in Neural Information Processing Sys-
tems, 33:19586–19597, 2020. 2
[16] Filip Hanzely and Peter Richtárik. Federated learning
of a mixture of global and local models. arXiv preprint
arXiv:2002.05516 , 2020. 2,3
[17] Filip Hanzely, Slavomír Hanzely, Samuel Horváth, and Peter
Richtárik. Lower bounds and optimal algorithms for person-
alized federated learning. Advances in Neural Information
Processing Systems , 33:2304–2315, 2020. 2
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 13
[19] Dan Hendrycks and Thomas Dietterich. Benchmarking neural
network robustness to common corruptions and perturbations.
Proceedings of the International Conference on Learning
Representations , 2019. 7,12,13
[20] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2(7), 2015. 2,3,4
[21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer
learning for nlp. In International Conference on Machine
Learning , pages 2790–2799. PMLR, 2019. 3
[22] Daniel Hsu, Ziwei Ji, Matus Telgarsky, and Lan Wang. Gener-
alization bounds via distillation. In International Conference
on Learning Representations , 2021. 17,20
[23] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measur-
ing the effects of non-identical data distribution for federated
visual classiﬁcation. arXiv preprint arXiv:1909.06335 , 2019.
6,12
[24] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Sil-
viana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad
Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert:
A large chest radiograph dataset with uncertainty labels and
expert comparison. In Proceedings of the AAAI conference
on artiﬁcial intelligence , pages 590–597, 2019. 6,12
[25] Liangze Jiang and Tao Lin. Test-time robust personalization
for federated learning. International Conference on Learning
Representations , 2023. 1,3
[26] Peter Kairouz, H Brendan McMahan, Brendan Avent, Au-
rélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista
Bonawitz, Zachary Charles, Graham Cormode, Rachel Cum-
mings, et al. Advances and open problems in federated learn-
ing.Foundations and Trends ®in Machine Learning , 14(1–2):
1–210, 2021. 1
[27] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,
Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh.
Scaffold: Stochastic controlled averaging for federated learn-
ing. In International Conference on Machine Learning , pages
5132–5143. PMLR, 2020. 25
[28] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 6,12
[29] Gihun Lee, Yongjin Shin, Minchan Jeong, and Se-Young
Yun. Preservation of the global knowledge by not-true self
23846
knowledge distillation in federated learning. arXiv preprint
arXiv:2106.03097 , 2021. 3,6
[30] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efﬁcient prompt tuning. arXiv preprint
arXiv:2104.08691 , 2021. 3
[31] Daliang Li and Junpu Wang. Fedmd: Heterogenous fed-
erated learning via model distillation. arXiv preprint
arXiv:1910.03581 , 2019. 3,6
[32] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi,
Ameet Talwalkar, and Virginia Smith. Federated optimization
in heterogeneous networks. Proceedings of Machine Learning
and Systems , 2:429–450, 2020. 7,14
[33] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith.
Ditto: Fair and robust federated learning through personal-
ization. In International Conference on Machine Learning ,
pages 6357–6368. PMLR, 2021. 1,2,3,4,6,7,13,25
[34] Xiang Li, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
Communication-efﬁcient local decentralized sgd methods.
arXiv preprint arXiv:1910.09126 , 2019. 30
[35] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and
Zhihua Zhang. On the convergence of fedavg on non-iid data.
InInternational Conference on Learning Representations ,
2020. 1,25,26
[36] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp,
and Qi Dou. Fedbn: Federated learning on non-iid features via
local batch normalization. arXiv preprint arXiv:2102.07623 ,
2021. 3,6,7
[37] Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori
Hashimoto. Large language models can be strong differen-
tially private learners. In International Conference on Learn-
ing Representations , 2022. 3
[38] Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B Allen,
Randy P Auerbach, David Brent, Ruslan Salakhutdinov, and
Louis-Philippe Morency. Think locally, act globally: Fed-
erated learning with local and global representations. arXiv
preprint arXiv:2001.01523 , 2020. 3,6,7
[39] Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi.
Ensemble distillation for robust model fusion in federated
learning. Advances in Neural Information Processing Systems ,
33:2351–2363, 2020. 3,4,5,6,7,14,16
[40] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-
roaki Hayashi, and Graham Neubig. Pre-train, prompt, and
predict: A systematic survey of prompting methods in natural
language processing. arXiv preprint arXiv:2107.13586 , 2021.
3
[41] Yi Liu, Xiaohan Bi, Lei Li, Sishuo Chen, Wenkai Yang, and
Xu Sun. Communication efﬁcient federated learning for mul-
tilingual neural machine translation with adapter. ACL Find-
ings, 2023. 3
[42] Ziyu Liu, Shengyuan Hu, Zhiwei Steven Wu, and Virginia
Smith. On privacy and personalization in cross-silo federated
learning. Advances in Neural Information Processing Systems ,
2022. 8,13
[43] Yishay Mansour, Mehryar Mohri, Jae Ro, and
Ananda Theertha Suresh. Three approaches for per-
sonalization with applications to federated learning. arXiv
preprint arXiv:2002.10619 , 2020. 1,2[44] Othmane Marfoq, Giovanni Neglia, Richard Vidal, and Laeti-
tia Kameni. Personalized federated learning through local
memorization. In International Conference on Machine
Learning , pages 15070–15092. PMLR, 2022. 2,4,8,15,
16
[45] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communication-
Efﬁcient Learning of Deep Networks from Decentralized
Data. In Proceedings of the 20th International Conference
on Artiﬁcial Intelligence and Statistics , pages 1273–1282.
PMLR, 2017. 1,2,4,7
[46] Kaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Dig-
gavi. Quped: Quantized personalization via distillation with
applications to federated learning. Advances in Neural Infor-
mation Processing Systems , 34, 2021. 3,5,6,25
[47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
13
[48] Krishna Pillutla, Kshitiz Malik, Abdelrahman Mohamed,
Michael Rabbat, Maziar Sanjabi, and Lin Xiao. Federated
learning with partial model personalization. ICML , 2022. 1,
3,6,7,8,15
[49] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan
Dyer. Effect of scale on catastrophic forgetting in neural
networks. In International Conference on Learning Repre-
sentations , 2022. 3
[50] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi.
Learning multiple visual domains with residual adapters. Ad-
vances in neural information processing systems , 30, 2017. 2,
3
[51] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do cifar-10 classiﬁers generalize to cifar-
10?arXiv preprint arXiv:1806.00451 , 2018. 7,12
[52] Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary
Garrett, Keith Rush, Jakub Kone ˇcný, Sanjiv Kumar, and
Hugh Brendan McMahan. Adaptive federated optimization.
InInternational Conference on Learning Representations ,
2021. 5,25
[53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115(3):211–252, 2015. 6,13
[54] Felix Sattler, Klaus-Robert Müller, and Wojciech Samek.
Clustered federated learning: Model-agnostic distributed mul-
titask optimization under privacy constraints. IEEE transac-
tions on neural networks and learning systems , 32(8):3710–
3722, 2020. 2
[55] Clayton Scott. Rademacher complexity. 2014. 16
[56] Aliaksandra Shysheya, John F Bronskill, Massimiliano Patac-
chiola, Sebastian Nowozin, and Richard E Turner. Fit: Param-
eter efﬁcient few-shot transfer learning for personalized and
federated image classiﬁcation. In The Eleventh International
Conference on Learning Representations , 2023. 3
23847
[57] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and
Ameet S Talwalkar. Federated multi-task learning. Advances
in neural information processing systems , 30, 2017. 6,7
[58] Benyuan Sun, Hongxing Huo, Yi Yang, and Bo Bai. Par-
tialfed: Cross-domain personalized federated learning via
partial initialization. Advances in Neural Information Pro-
cessing Systems , 34, 2021. 3,6,12
[59] Canh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized
federated learning with moreau envelopes. Advances in Neu-
ral Information Processing Systems , 33:21394–21405, 2020.
1,2,3,6,7,13
[60] Antonio Torralba, Rob Fergus, and William T Freeman. 80
million tiny images: A large data set for nonparametric object
and scene recognition. IEEE transactions on pattern analysis
and machine intelligence , 30(11):1958–1970, 2008. 12
[61] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty,
and Sethuraman Panchanathan. Deep hashing network for
unsupervised domain adaptation. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages
5018–5027, 2017. 6,12
[62] Kangkang Wang, Rajiv Mathews, Chloé Kiddon, Hubert
Eichner, Françoise Beaufays, and Daniel Ramage. Feder-
ated evaluation of on-device personalization. arXiv preprint
arXiv:1910.10252 , 2019. 2
[63] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim
Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam
Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush. Trans-
formers: State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations ,
pages 38–45, Online, 2020. Association for Computational
Linguistics. 13
[64] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao
Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gon-
zalez, Kurt Keutzer, and Peter Vajda. Visual transformers:
Token-based image representation and processing for com-
puter vision, 2020. 8,13
[65] Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Sal-
vaging federated learning by local adaptation. arXiv preprint
arXiv:2002.04758 , 2020. 2,6,7
[66] Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wen-
chao Xu, and Feijie Wu. Parameterized knowledge transfer
for personalized federated learning. Advances in Neural In-
formation Processing Systems , 34:10092–10104, 2021. 3,
6
[67] Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu
Duan. Fine-tuning global model via data-free knowledge
distillation for non-iid federated learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10174–10183, 2022. 3,4,6
[68] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free
knowledge distillation for heterogeneous federated learning.
InInternational Conference on Machine Learning , pages
12878–12889. PMLR, 2021. 3,5,6,16
23848
