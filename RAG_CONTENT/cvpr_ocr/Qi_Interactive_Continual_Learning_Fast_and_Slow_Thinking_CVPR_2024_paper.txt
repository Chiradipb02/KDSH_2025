Interactive Continual Learning: Fast and Slow Thinking
Biqing Qi1,2,4, Xinquan Chen3, Junqi Gao3, Dong Li3, Jianxing Liu1, Ligang Wu1,*, Bowen Zhou1,2,4, *
1Department of Control Science and Engineering, Harbin Institute of Technology,
2Department of Electronic Engineering, Tsinghua University,
3School of Mathematics, Harbin Institute of Technology,
4Frontis.AI, Beijing
{qibiqing7,xinquanchen0117,gjunqi97,arvinlee826}@gmail.com,
{jx.liu,ligangwu}@hit.edu.cn, {zhoubowen}@tsinghua.edu.cn
Abstract
Advanced life forms, sustained by the synergistic inter-
action of neural cognitive mechanisms, continually acquire
and transfer knowledge throughout their lifespan. In con-
trast, contemporary machine learning paradigms exhibit
limitations in emulating the facets of continual learning
(CL). Nonetheless, the emergence of large language mod-
els (LLMs) presents promising avenues for realizing CL via
interactions with these models. Drawing on Complemen-
tary Learning System theory, this paper presents a novel In-
teractive Continual Learning (ICL) framework, enabled by
collaborative interactions among models of various sizes.
Speciﬁcally, we assign the ViT model as System1 and mul-
timodal LLM as System2. To enable the memory mod-
ule to deduce tasks from class information and enhance
Set2Set retrieval, we propose the Class-Knowledge-Task
Multi-Head Attention (CKT-MHA). Additionally, to improve
memory retrieval in System1 through enhanced geomet-
ric representation, we introduce the CL-vMF mechanism,
based on the von Mises-Fisher (vMF) distribution. Mean-
while, we introduce the von Mises-Fisher Outlier Detection
and Interaction (vMF-ODI) strategy to identify hard exam-
ples, thus enhancing collaboration between System1 and
System2 for complex reasoning realization. Comprehensive
evaluation of our proposed ICL demonstrates signiﬁcant re-
sistance to forgetting and superior performance relative to
existing methods. Code is available at github.com/ICL.
1. Introduction
Advanced life forms exhibit continual learning (CL) and
memory formation, facilitated by neural cognitive inter-
actions that enable collaborative knowledge transfer [9,
26, 48]. These underlying mechanisms enhance memory
*Corresponding authorsconsolidation and utilization, as well as reasoning abili-
ties in advanced life forms [16, 39]. However, current
machine learning paradigms, particularly neural network-
based models, face challenges in achieving CL. Speciﬁ-
cally, neural networks learning from evolving data face a
risk known as catastrophic forgetting [5], wherein the in-
tegration of new knowledge frequently disrupts existing
knowledge, resulting in notable performance degradation
[5, 17, 50].
To tackle this challenge, current CL methods strive to
preserve and augment knowledge acquired throughout the
learning process [14, 22, 28, 36]. In CL, rehearsal-based
methods [32, 36, 40, 44–46], are the most direct strategy.
However, these methods often ignore the geometric
structure [30] of memory representations and face chal-
lenges in open-class settings. Another perspective includes
architecture-based methods [21, 22, 31], allocating distinct
parameters for knowledge encoding from various tasks.
Early studies centered on convolution-based architectures
[28]. Recent advancements pivoted towards transformer-
based methods like L2P [46], and Dualprompt [45].
From the perspective of Complementary Learning Sys-
tem (CLS) Theory in neurocognitive science [16], the cur-
rent designs of CL frameworks may not be optimal. In
a brain-like system, multiple memory modules dynam-
ically maintain a balance between stability and plastic-
ity, with each module possessing predictive capabilities
[24, 33, 35]. However, most advanced CL frameworks lean
towards more intuitive systems. Previous CLS-driven meth-
ods [25, 28] involve the separation and expansion of param-
eters to facilitate the learning of both fast and slow knowl-
edge. For instance, DualNet [28] optimizes this process
by emphasizing task-speciﬁc pattern separation. Similarly,
BiMeCo [25] divides model parameters into two distinct
components: a short-term memory module and a long-term
memory module. However, these methods [25, 28] are lim-
ited to a single backbone model. In line with CLS prin-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12882
ciples, this underscores the importance of developing an
interactive CL framework between models to consistently
achieve higher performance levels.
Meanwhile, recent advancements in large language mod-
els (LLMs), as exempliﬁed by ChatGPT [38] and GPT4
[5], have demonstrated remarkable reasoning capabilities.
These models can employ chains of thought [47] to engage
in complex reasoning, much like System2. Consequently,
it raises an interesting question: Can we integrate intuitive
models, such as ViT [8] as System1 alongside LLM-based
as System2 to establish an interactive framework for CL?
To response this question, we reevaluated CL by ex-
ploring the interaction between ViT (System1) and Mul-
timodal Large Language Model (System2). In alignment
with the current CL setting, our focus is on adapting ViT
parameters while keeping System2 parameters stable. Sys-
tem2 is responsible for handling hard examples and facil-
itates collaboration with System1. To enable the contin-
ual updating of ViT, we introduced the Class-Knowledge-
Task Multi-Head Attention (CKT-MHA) module. CKT-
MHA utilizes category features and the knowledge of ViT
to aid System1 in acquiring task-related knowledge, facil-
itating knowledge retrieval through Class-Task collections.
Furthermore, we introduce the CL-vMF mechanism, which
employs von Mises-Fisher distribution modeling to improve
memory geometry and enhance retrieval distinguishability
through an Expectation-Maximization (EM) update strat-
egy. This design enables System1 to retain old memory
parameters, preventing unnecessary updates and addressing
catastrophic forgetting issues. To realize the coordination
between System1 and System2 during their reasoning tran-
sitions, we introduce the von Mises-Fisher Outlier Detec-
tion and Interaction (vMF-ODI) mechanism for assessing
sample difﬁculty. This mechanism is designed to assist the
System1 in adaptive identifying hard examples within each
batch. Once identiﬁed, these hard examples undergo ini-
tial inference by System1, and the resulting prediction out-
comes serve as background knowledge for System2 to fa-
cilitate more intricate reasoning.
We conduct experiments on various benchmarks, includ-
ing the demanding Imagenet-R, to validate proposed inter-
active continual learning (ICL). The results illustrate that
ICL signiﬁcantly mitigates catastrophic forgetting, surpass-
ing state-of-the-art methods. Moreover, it maintains consis-
tently high accuracy across different task. In summary, our
contributions are as follows:
• We propose an ICL framework from a novel perspec-
tive that emphasizes the interaction between fast intu-
itive model (ViT) and slow deliberate model (multimodal
LLM), aligning with CLS principles.
• We propose the CKT-MHA module to acquire task-
related information by leveraging category features and
small model knowledge.• We propose the CL-VMF mechanism, an optimization
strategy guided by VMF distribution modeling with EM
strategy updates to enhance the retrieval of geometric
memory representations.
• We propose vMF-ODI, a batch-wise retrieval interaction
strategy that enables the adaptive identiﬁcation of hard
examples within each batch, fostering collaborative rea-
soning between the two systems.
2. Related Works
We discuss three primary categories of CL methods.
Regularization-based methods incorporate regulariza-
tion terms into the loss function to mitigate catastrophic
forgetting for previously learned tasks. These methods
[1, 14, 17, 20, 27, 50], primarily revolve around the de-
velopment of metrics for assessing task importance, with
additional research efforts dedicated to characterizing the
signiﬁcance of individual features [42]. Nevertheless, these
methods tend to exhibit reduced performance when applied
to more complex datasets.
Rehearsal based methods utilize previous task data to
mitigate catastrophic forgetting within limited memory
buffers. Reservoir sampling techniques [7, 34], randomly
retain a ﬁxed number of old training samples from each
training batch. Further, [12] employs coefﬁcient-based co-
sine similarity to address sample number imbalances among
categories. To better recover past knowledge, GEM [36]
constructs individual constraints based on old training sam-
ples for each task to ensure no increase in their loss. LOGD
[40] decomposes gradients for each task into shared and
task-speciﬁc components, capitalizing on inter-task infor-
mation. CVT [44] explores online CL using external atten-
tion strategy.
Architecture-based methods aim at assigning indepen-
dent parameters for new task data. These methods involve
strategies such as parameter allocation, model division, and
modular network models [21, 22, 31]. Previous studies
concentrated on speciﬁc convolution-based architectures,
with DualNet [28] optimizes memory through separation
representations for speciﬁc tasks. Recent work focus to
transformer-based models. L2P [46] enhances integration
of knowledge by treating prompts as optimization param-
eters. Furthermore, Dualprompt [45] enhances knowledge
memory by constructing dual orthogonal prompt spaces.
3. Methodology
3.1. Problem Setup
A standard paradigm for CL can be deﬁned by a set of
task descriptors t2T, and the corresponding distribution
pt(x;y)for each task. The task-speciﬁc dataset Dt:=
f(xt
i;yt
i)j(xt
i;yt
i)pt;i2[Nt]g, whereNtrepre-
sents the number of samples in the training set of the t-th
12883
Figure 1. Comprehensive Training and Testing Illustration. In the training phase: we propose the CKT-MHA uniﬁed storage module for
System1. And then use memory selection and updates through our CL-VMF mechanism with the EM strategy to optimize CL for the small
model ViT. In the inference phase: 1) The process begins by assessing sample complexity using proposed vMF-ODI in System 1. 2) The
System1 then swiftly generates inferential predictions. 3) If test samples surpass a complexity threshold, we activate collaborative inference.
Speciﬁcally, the predictive results from System1 is used as background knowledge to narrow the scope of inference. 4) Subsequently,
complex reasoning through the multimodal LLM is applied to achieve the ﬁnal prediction.
task. The dataset is drawn i.i.d. from the sample space
XtYt2XY . During the training phase, the train-
ing samples are sequentially fed as inputs, following the
task descriptors from 0tojTj. In formulating the ICL
framework, we provide deﬁnitions for System1 and Sys-
tem2, respectively. System1 is instantiated by the model
f() :X7!Rdparameterized by which updates its pa-
rameters totin thet-th task. Our objective is to deter-
minejTj= arg minjTjEt2TE(xt;yt)
`(fjTj(xt);yt
i)
,
ensuring the memory capacity of the System1. Here `(;yt)
represents loss function, with yt2Yt. The system1 uti-
lizes a memory buffer Mand updatest 1totby using
Dt\M . Furthermore, System2 is instantiated with a Multi-
modal LLM represented as g to model complex reasoning
abilities. To enable collaborative inference, System2 must
handle hard samples ~Xthat System1 struggles with, maxi-
mizing the probability P~x2~X(~y6= arg max if(~x)i). This
requires the ability of System1 to ﬁlter out these samples,
thereby improving the inference of System2 by leveraging
predicted results from System1 Iffor second-stage infer-
ence:^~y=S 1(g (~x;S(If(~x)))); (1)
hereSrepresents label to prompt operation, while
S 1represents its inverse operation. System2 is
expected to produce an output S(~y)that minimizes
 logp (S(~y)j~x;S(If(~x))). Next, we will present de-
tailed designs for each component of the ICL framework
and discuss optimization strategies.
3.2. Query and Value Memory for System1
In general, deep neural networks implicitly encode data
memories within their parameters, and unnecessary param-eter changes can result in memory degradation. The prevail-
ing methods to deploying models in downstream tasks in-
volves utilizing pre-trained feature extractors and introduc-
ing new parameters for adaptation, which has been demon-
strated to be effective in CL setting [45, 46]. Neverthe-
less, these methods face challenge when the precise num-
ber of classes in the downstream task is uncertain. More-
over, updating all parameters of the classiﬁcation head for
each new task worsens the issue of forgetting. To address
these challenges, we propose separating the model’s param-
eters into two distinct groups: value memory parameters,
denoted asZ=Z1;Z2;:::with class-speciﬁc representa-
tionszyt2Zt, and query memory parameters represented
by. This decoupling strategy enhances operational ﬂex-
ibility. In simpler terms, we envision Zas a collection of
class-speciﬁc value memory variables, ensuring that
• Value memory parameters Zcan be augmented as the
number of tasks increases, i.e., there is a memory incre-
ment. When training on task t,Zwill be updated from
fZ1;:::;Zt 1gtofZ1;:::;Ztg.
• The relevant value memory for each class is only updated
when necessary, i.e., the value memory zytwill not be
updated if the input does not contain class yt, thuszyt 1
after trained on task tequals to that trained before training
on taskt.
Then we can ensure that the System1 can be equipped with
persistent value memory of old data and update query pa-
rameters using a portion of the memory buffer as rehearsal
samples, thus can effectively handle old data. At the same
time, this enables System1 to be unconstrained by a prede-
ﬁned number of classes and allows for more ﬂexible mem-
ory allocation for new tasks.
12884
Interactive Query and Value Memory with CKT-MHA
Firstly, we design such a memory module for System1.
Speciﬁcally, we propose a Set2Set memory retrieval mech-
anism to further enhance memory retrieval stability. This
mechanism involves ﬁrst obtaining class (cls) information
via a projector fcparameterized by c:c=fc;'(x)2
RLdc,Lis the obtained token length. Here we use the
pre-trained ViT as image feature extractor f', followed by
an query interactor f, parameterized by , introduced for
memory matching. This results in f;'=ff'. Sub-
sequently, we utilize cand pretrained knowledge k=
f'(x)2RLdcto construct the Class-Knowledge-Task
Multi-Head Attention (CKT-MHA) to capture the task in-
formation corresponding to the class:
h;i=AttentionSA
[:;Ri]
c;[:;Ri]
k;f(k)[:;Ri]
;(2)
=fo(Concat [h;1;:::;h;Nh])2RL; (3)
whereNhis the head number of MHA, Ri= [(i 
1)dc
Nh;idc
Nh]is the corresponding attention head
interval. Then we combine the class and task fea-
tures set to obtain the classiﬁcation feature: =
Concat [1
LPL
l=1l
;1
LPL
l=1l
c], wherel
denotes the l-
th token of. Consequently, the interactor parameters
are denoted as =fc;;SA;og. Finally, we set
a group of task-speciﬁed value memory vector z2Z
and class-speciﬁed zc2 Zccorresponding to the class
to form the ﬁnal value memory variable zfor retrieval:
z=Concat [z;zc]. During the inference stage, the pro-
posed task-class Set2Set retrieval is performed from value
memory as: ypred= arg maxz2Zp;'(zjx).
With decoupled parameters, we also need to decouple
the updates of query memory parameters and value mem-
ory parameters to ensure the ﬂexibility of adjusting these
two parts of parameters, avoiding unnecessary updates to
value memory parameters, which prevents biases in mem-
ory variables for future tasks and mitigates catastrophic for-
getting. Furthermore, prioritizing value memory parameter
optimization guides the optimization of query parameters.
Is it possible to achieve such decoupled optimization while
ensuring a consistent optimization objective? The inherent
nature of EM algorithm provides a framework for meeting
these optimization requirements. Speciﬁcally, considering
the Maximum Likelihood Estimation (MLE) under a clas-
siﬁcation setting, with the probability of task tdenoted as
P(t), the objective can be expressed as follows:
Minimize
jTjX
tP(t)Ext
 logpjT j(ytjxt)
: (4)
For the sake of simplicity in the framework, here we only
consider the parameters that are being updated. Dur-
ing training on task t, the System1 aims to determine
t= arg min tExt[ logpt(ytjxt)]. To achieve this,
we model value memory z2 Z as hidden variables,alongside the hidden distribution q(zjx)which satisﬁesP
z2Zq(zjx) = 1 . This allows us to obtain
logpt(ytjxt) = loghpt(yt;zjxt)
q(zjxt)i
 loghpt(zjyt;xt)
q(zjxt)i
;(5)
taking the expectation with respect to q(zjx)on both sides,
logpt(ytjxt) =X
z2Zq(zjxt) logpt(yt;zjxt)
q(zjxt)
;(6)
+X
z2Zq(zjxt) logq(zjxt)
pt(zjyt;xt)
;(7)
the ﬁrst term of R.H.S is referred to as the Evidence Lower
BoundLEL(pt(yt;zjxt);q(zjxt)), the second term is
KL(q(zjxt)kpt(zjyt;xt)). Since the KL divergence
is non-negative, we can achieve maximum likelihood
by iteratively updating Zandtusing the Generalized
Expectation-Maximization algorithm (GEM) as follows:
Z(i+1)= arg minZExth
KL
q(z(i)jxt)kp(i)
t(z(i)jyt;xt)i
;
(8)
(i+1)
t = arg maxExth
LEL
p(i)
t(yt;z(i+1)jxt);q(z(i+1)jxt)i
:
(9)
In the context of supervised learning, we can deﬁne zas
class-speciﬁczyt2Zt. The prior form of qisq(zjxt) =P
z2Z1(z=zyt). Note that in this case,
pt(zjyt;xt) =pt(zytjxt) =pt(z;ytjxt): (10)
Hence, we can express the two distinct objectives in eq.(8)
and (9) as a uniﬁed objective:

t;Z
t= arg min
;ZExth
 logpt(zytjxt)i
: (11)
When training on a sequence of tasks, if we directly op-
timize as in eq.(4) for the current task’s optimal 
t=
arg minExt[ logpt(ytjxt)], as is typical, it will in-
evitably lead to 
t6=
t 1, resulting in catastrophic forget-
ting. However, our decoupled optimization strategy ensures
that the value memory parameters remain optimal for their
corresponding tasks, and with the integration of rehearsal,
it guarantees that the optimization of query memory param-
eters can be consistently guided by such value memory pa-
rameters of old tasks, thereby mitigating catastrophic for-
getting while continually adapting to new data. Next, we
describe how to model pt(zytjxt)in eq.(11).
3.3. Optimizing Memory via CL-vMF
Modelling Posterior with vMF Distribution. To assure
value memory vectors more discriminative that facilitate
more explicit memory retrieval, necessitating the construc-
tion of more separable geometric relationships for them. In-
troducing improved geometric relationships for value mem-
ory vectors further ensures that the query features of each
12885
class are more separable and closer to the class center when
combined with the EM updating strategy. This also aids
in more effectively ﬁltering out outliers, laying the ground-
work for screening hard samples for System1, as discussed
in Section 3.4. Therefore, we opt for the von Mises-Fisher
(vMF) distribution, which naturally excels in modeling ge-
ometric relationships in high-dimensional spaces [2, 18, 29]
and has demonstrated effectiveness in downstream tasks
[23, 51]. Its probability density function is as follows:
pvMF(x;;) =Cd() exp(h;xi); (12)
where the correlated sample space is deﬁned as fxjx2
Rd;kxk= 1g.2Rdrepresent the mean direc-
tion, while is the concentration parameter. The con-
stantCd()is only related to andd. By setting nor-
malized value memory parameter zyt=kzytkto the mean
direction, we model the distribution of the normalized fea-
turef;'(xt)=kf;'(xt)kfor classytas a vMF distribution
with probability density Cd() exp(D
zyt
kzytk;f;'(xt)
kf;'(xt)kE
).
By utilizing the constructed probability density for
Bayesian discrimination, we can ascertain the form of the
posterior probability pt;'(zytjxt)as:
pt;'(zytjxt) =exp(D
zyt
kzytk;f;'(xt)
kf;'(xt)kE
)
P
z02Zexp(D
z0
kz0k;f;'(xt)
kf;'(xt)kE
):
(13)
When a sample pair of a new class (xt+1;yt+1)2Dt+1is
inputted, a new zyt+1=2Z can be assigned to it for mem-
ory expansion, thereby avoiding the limitation imposed by
the predeﬁned number of classes, i.e., class-free. Combined
with the proposed memory module, we introduce CL-vMF,
a class-free memory retrieval mechanism that accommo-
dates new classes when the total number is unknown, it re-
trieves from memories based on the vMF posterior and en-
sures that class-speciﬁc value memory parameters are only
updated when needed.
Implementation of CL-vMF. We set the value memory
vectorsZas learnable memory embeddings, enabling the
model to use them for retrieval using the vMF posterior
to ascertain a class of sample. The process resembles
constructing a new "hippocampus" within the pre-trained
model by utilizing Zand learning query memory interac-
tions viato adapt and employ the value memory. In the
training phase, upon encountering each new class y, we al-
locate azy, incorporating it into Z. To ensure updating
the value memory parameter zyexclusively when the input
contains a sample of class y, we introduce the subsequent
batchwise vMF loss to maximize likelihood:
LvMF(Bt
i) =1
jBt
ijX
(xt;yt)2Bt
i logh
pBt
i
;'(zytjxt)i
;(14)
whereBt
i2Dtis thei-th input batch of task tandpBt
i
;'(zytjxt) =exp(D
N(zyt);N(t)E
)
P
y2Yt
iexp(hN(zy);N(t)i);(15)
wheret=f;'(xt),Yt
i=Unique (fyg(x;y)2Bt
i)is the
set of classes that appears in Bt
i,N() :=()
kkdenotes the
projection operator. is set as a hyperparameter. Hence,
our training approach involves alternating between the EM
steps based on eq.(8) and eq.(9) throughout the training pro-
cess. Nonetheless, when delving deeper into the optimiza-
tion process, it becomes imperative to consider the gradient
ofN(zyt):
rN(zy)h
 log
pBt
i
;'(zytjxt)i
(16)
=8
<
:h
pBt
i
;'(zytjxt) 1i
N 
t
; y =yt
pBt
i
;'(zyjxt)N 
t
; y6=yt;
thus whenpBt
i
;'(zytjxt)!1, the gradient for N(zy)
(y=yt) will tend to zero, meanwhile, as papproaches 0,
rN(zy)h
 log
pBt
i
;'(zytjxt)i
(y6=yt)will also tend
to zero. Likewise, for N 
t
we have:
rN(t)h
 log
pBt
i
;'(zytjxt)i
(17)
=2
4X
y2Yt
ipBt
i
;'(zyjxt)N(zy) N(zyt)3
5:
Note that when pBt
i
;'(zytjxt)!1,
X
y2Yt
ipBt
i
;'(zyjxt)N(zy)!N(zyt): (18)
This means thatrN(t)h
 log
pBt
i
;'(zytjxt)i
will tend
to zero. To ensure stable gradients during training and
achieve consistent loss reduction, we introduce a gradient
stabilization loss
`
GS(xt;yt) =`
margin
zyt;t
+P
y2Yt
i;y6=yt`
margin 
 zy;t;(19)
where
`
margin 
zy;t
= max(1 
N(zy);N 
t );(20)
which provides a constant gradient as compensation, with
regulating the threshold of gradient compensation. When
the loss becomes small, no further compensation is provided
due to the instability of the zero point for the absolute value
function. Similarly, we incorporate the batchwise gradient
stabilization lossL
GS(Bt
i) =1
jBt
ijP
(xt;yt)2Bt
i`GS(xt;yt)
into the objective, leading to the overall loss:
L 
Bt
i
=LvMF 
Bt
i
+L
GS 
Bt
i
; (21)
here,andare hyperparameters.
12886
The CL-vMF model possesses several advantageous fea-
tures, including incremental value memory and the ability
to handle an arbitrary number of classes. As a result, there
is no need to retrain the classiﬁcation head even when the
number of classes exceeds the pre-deﬁned limit. Since the
value memory parameters Ztare frozen after the comple-
tion of task t, i.e. all memories Z1;:::;Zt 1remain un-
changed both before and after training task t. This guar-
antees stable and persistent value memory. Moreover, the
storage cost of class value memory parameters is calculated
asdzNC+dzcjTj , wheredzc;dzandNCrepre-
sent the dimensions of z;zcand the number of classes,
respectively. Importantly, this implies that the storage cost
of value memory parameters scales linearly with NC.
3.4. Collaborative Inference: System1 and System2
To align with the CLS, we aim for the ICL framework to
activate System2 when System1 fails to perform fast think-
ing, i.e. when encounters hard samples. This activation
leverages the complex reasoning capabilities of System2
to achieve collaborative inference. Speciﬁcally, we use
MLLM to instantiate g . And we propose a hard sample
ﬁltering strategy, vMF-ODI, to screen data that challenges
the System1. Speciﬁcally, we use batchwise normalization
to ﬁlter outliers, thus identifying hard sample set ~X:
~Xi=f(~x;~y)2Bij( Bi)=Bi<g; (22)
where=hf;'(x);z^yi
kf;'(x)kkz^yk,^yis the predicted label, Bi=
1
jBijP
j2Bij,=q
1
jBij 1P
j2Bi(j )2andis a
detection threshold. For the ﬁltered hard samples, we uti-
lize the TopK outputs [f;'(~x)]Kfrom the System1 to con-
struct an inquiry-based language prompt S 
[f;'(~x)]K
.
The operationSconverts labels to language and prompts
the System2 to perform reasoning and rank the results based
on the given context, and ﬁnally gives the prediction ^~ylike
we stated in eq. (1). After completing the inference in stage
2, if the System2 provides a precise answer, we will use that
answer. Otherwise, we will rely on the judgment result from
the System1. This interactive scheme suggests the possi-
bility of using ﬁne-tuning strategies, such as LoRA [13],
to minimize logp  
S(y)jx;S 
[f;'(x)]K
with re-
spect to . This adjustment would help align the System2
with the System1. Consistent with existing CL setups, we
focus on the parameter updates of System1. Detailed al-
gorithmic description can be found in Appendix A. During
the training process, we perform Titerations of EM alter-
nating steps for each task, with updates only applied to in
subsequent updates for that task.
4. Experiments
4.1. Experimental Setups
Following [41, 43], we investigate two common CL se-
tups: Task-Incremental Learning (Task IL) and Class-Incremental Learning (Class IL). In Task IL, task identiﬁers
are provided during both the training and testing phases. In
contrast, the Class IL protocol assigns task identiﬁers only
during the training phase. During the testing phase, the
model faces the challenge of predicting all classes encoun-
tered up to that point, making it a more demanding scenario.
Datasets. Three datasets are used in our experiments: CI-
FAR10 [15], CIFAR100 [15], ImageNet-R [11]. Details of
these datasets are in Appendix.
Baselines. We combine the proposed ICL framework
with the following advanced baselines: Selected rehearsal-
based methods: ER [34], A-GEM [6], iCaRL [32], CVT
[44], SCoMMER [37], BiMeCo [25]. architecture-based
methods: DualNet [28], L2P [46], DualPrompt [45].
Additionally, we perform comparisons with well-known
regularization-based techniques, namely EWC [14], LwF
[19], and SI [50]. Additionally, we assess JOINT, which
entails supervised ﬁne-tuning across all task training sets
and represents an upper performance limit. We also exam-
ine FT-seq, a sequential ﬁne-tuning technique that partially
freezes pre-training weights and generally serves as a per-
formance lower bound. Both JOINT and FT-seq have two
variants, one using ViT [8] and the other using ResNet18
[10] as the backbone. Our main focus on comparing these
methods with rehearsal- and prompt-based methods.
Metrics. We evaluate CL methods in terms of accuracy,
following the accuracy deﬁnition presented in [3, 43, 45]:
AT=1
TPT
t=1aT;t, whereaT;trepresents the testing ac-
curacy for taskTtwhen the model has completed learning
taskTT.
Implementation Details. Following [3, 4, 32, 44, 50], we
use RestNet18 [10], ViT [8] as the backbone in System1.
For System2, we choose MiniGPT4 [53], Inf-MLLM [52],
Pure-MM as the backbone. More details are in Appendix.
4.2. Results
Extensive experiments are conducted on CIFAR10, CI-
FAR100, ImageNet-R. Speciﬁcally, we add our ICL to sev-
eral state of the arts methods to evaluate its effectiveness.
Results of comparisons with State-of-the-Art Methods.
The quantitative comparisons are summarized in Tab. 1. In
the rehearsal-based method, we experiment with different
buffer sizes for comparison. The results consistently show
that our method outperforms others. Notably, to better sim-
ulate the CL scenario, we restrict the number of epochs to
one, allowing the model to encounter the data only once
during incremental task learning. This restriction signiﬁ-
cantly affects the efﬁcacy of regularization techniques like
EWC [14], LwF [19], as well as rehearsal-based methods
such as ER [34], A-GEM [6], iCaRL [32] and CVT [44],
among others. However, transformer-based approaches like
L2P [46] and DualPrompt [45] maintain certain perfor-
mance. Additionally, even in the absence of integrating the
System2, its incorporation leads to further performance en-
12887
20 40 60 80100 120 140 160 180 200
Number of Class010203040506070 Class IL Accuracy (%)
(a) Accuracy over Number of Class (Class IL)
L2P
DualPrompt
DualNet
CVT
SCoMMER
iCarL
ICL w/o System2
ICL
20 40 60 80100 120 140 160 180 200
Number of Class1020304050607080 Task IL Accuracy (%)
(b) Accuracy over Number of Class (Task IL)
L2P
DualPrompt
DualNet
CVT
SCoMMER
iCarL
ICL w/o System2
ICL
103
102
101
100
100101
102
103
74.16 74.8 78.27 76.34
74.16 74.97 78.27 76.34
74.13 74.83 78.27 76.34
73.9(c) Influence of  and  (CIFAR100)
103
102
101
100
100101
102
103
53.52 54.37 53.6 45.84
53.52 54.37 53.6 45.78
53.52 54.37 53.6 45.78
46.44(d) Influence of  and  (ImageNet-R)
CIFAR10 CIFAR100 Imagenet-R
405060708090 Class-IL Accuracy (%)(e) Influence of 
=0.5
=1
=1.5
=2
CIFAR10 CIFAR100 Imagenet-R
K405060708090 Class-IL Accuracy (%)(f) Influence of K
w/o slow
K=2
K=3
K=4
K=5
-15 -10 -5 0 5 10 15-15-10-5051015(g) Trained Memories w/o EM
Task
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
-15 -10 -5 0 5 10 15-15-10-5051015(h) Trained Memories w/ EM
Task
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
7475767778
4648505254Figure 2. Further analysis of the proposed ICL. (a) and (b) are forgetting curves of different methods on the ImageNet-R in the Class IL and
Task IL scenario respectively. (c) and (d) are the impact of regularization parameters andon CIFAR100 and ImageNet-R respectively.
(e) The impact of concentration , is evaluated at values of 0.5, 1, 1.5, and 2, respectively. (f) The impact of the number of category choices
Kin the prompt, is evaluated at values of 2, 3, 4, and 5 respectively. (g) The impact of training memory without EM strategy. (h) The
impact of training memory with EM strategy.
Backbone Method TypeMemoryMethodCIFAR10 CIFAR100 ImageNet-R
Buffer Class-IL Task-IL Class-IL Task-IL Class-IL Task-IL
ResNet18- -JOINT 92.20 98.31 70.62 86.19 7.72 25.48
FT-seq 19.62 61.02 17.58 40.46 0.59 10.82
Non-Rehearsal based 0EWC[14] 17.82 83.52 7.62 55.14 1.08 21.34
LwF[19] 18.52 84.72 8.88 61.32 1.24 45.68
SI[50] 18.41 84.74 6.73 50.44 3.31 22.72
Rehearsal based200ER[34] 44.79 91.19 21.40 61.36 1.01 15.36
A-GEM [6] 18.58 80.19 7.97 55.20 1.23 16.24
iCaRL [32] 23.80 67.82 7.31 33.10 0.81 9.20
CVT [44] 30.74 75.92 12.09 43.14 1.60 9.01
SCoMMER [37] 66.35 92.66 38.89 67.62 1.73 10.65
DualNet [28] 24.50 90.70 25.30 54.60 7.01 20.70
BiMeCo [25] 27.92 92.75 28.71 56.65 10.41 22.75
500/600ER[34] 57.74 93.61 28.02 68.23 1.27 22.84
A-GEM [6] 24.85 84.80 8.89 51.47 1.23 19.35
iCaRL [32] 29.21 67.72 4.40 23.41 1.01 7.60
CVT [44] 40.13 79.61 13.83 46.39 1.24 6.97
SCoMMER [37] 73.95 94.14 49.09 74.50 1.40 10.05
DualNet [28] 35.00 91.90 34.65 62.70 8.70 20.40
BiMeCo [25] 38.40 93.95 38.05 64.75 12.13 22.45
ViT- -JOINT 97.49 99.54 87.23 97.64 74.75 83.39
FT-seq 22.32 86.33 20.48 83.97 32.56 49.62
Non-Rehearsal based 0L2P [46] 92.22 98.99 79.68 96.24 48.68 65.38
DualPrompt [45] 94.43 99.32 79.98 95.92 52.20 69.22
Rehearsal based200L2P [46] 67.13 96.39 65.29 92.16 36.70 55.35
DualPrompt [45] 70.60 97.78 65.97 92.85 38.79 59.32
ICL w/o System2 94.60 99.43 77.34 94.81 49.87 68.62
ICL w MiniGPT4 95.34 99.56 78.28 95.70 52.46 69.87
ICL w Inf-MLLM 95.42 99.56 78.55 95.83 53.20 72.96
ICL w Pure-MM 95.94 99.57 79.12 95.99 53.64 73.59
500/600L2P [46] 71.23 96.78 69.43 93.92 40.17 57.89
DualPrompt [45] 73.56 98.12 69.98 93.76 43.77 61.24
ICL w/o System2 95.54 99.52 80.67 95.24 54.65 76.02
ICL w MiniGPT4 96.49 99.58 81.38 95.62 55.99 79.68
ICL w Inf-MLLM 96.69 99.64 82.29 96.14 57.47 81.82
ICL w Pure-MM 96.83 99.68 82.43 96.35 58.18 82.64
Table 1. Comparison of average incremental accuracy (%) with different continual learning method in two scenarios. Memory Buffer
denotes the size of the memory buffer area ( 0means no rehearsal is used). Note that when training with the ViT in System1, the buffer size
of the Imagenet-R is 600, due to the need to allocate an equal amount of buffer area for each class.
hancement. This is demonstrated by a notable increase of
over 3% in CL accuracy on the ImageNet-R dataset, ob-
served across 10 consecutive splits, with a memory capacity
of 600. The introduction of the System2 enhances System1
ability to effectively recognize and address previously for-
gotten images or information. To ensure fair comparisons,
we incorporate a buffer into the L2P [46] and DualPrompt
[45] methods, using a ﬁxed strategy akin to CVT [44]. Sub-
sequently, we observe that the performance of the L2P [46]
and DualPrompt [45] methods declined after integrating the
buffer. This decline is likely due to interference from the
replayed samples in the buffer, which hampers the train-
ing of task-speciﬁc prompts and leads to greater forgetting.Results of different task settings. To assess CL strate-
gies across varying numbers of data streams, following the
protocol outlined in [44, 49]. This method entails divid-
ing the ImageNet-R dataset, which consists of 200classes,
into subsets containing 5,10, and 20classes each, thereby
creating incremental tasks. Tab. 2 offers a comprehen-
sive analysis of the accuracy achieved by various methods
across different task conﬁgurations. The results unequiv-
ocally establish the signiﬁcant superiority of our method
over both regularization-based and rehearsal-based method
in a wide range of incremental division scenarios. Notably,
even when the memory capacity is held constant at 200,
our method outperforms architecture-based methods such
12888
MemoryMethod5 splits 10 splits 20 splits
Buffer Class-IL Task-IL Class-IL Task-IL Class-IL Task-IL
0EWC[14] 1.56 11.35 1.08 21.34 8.10 12.68
LwF[19] 1.38 14.66 1.24 45.68 0.77 14.34
SI[50] 1.78 11.50 3.31 22.72 3.35 40.29
L2P [46] 29.87 38.58 48.68 65.38 20.08 47.98
DualPrompt [45] 54.43 66.86 52.20 69.22 47.13 71.43
200ER[34] 1.29 9.75 1.01 15.36 1.38 19.81
A-GEM [6] 1.30 4.23 1.23 16.24 1.34 21.78
iCaRL [32] 0.41 3.22 0.81 9.20 0.83 15.23
CVT [44] 1.47 5.65 1.6 9.01 1.01 13.19
SCoMMER [37] 0.80 3.78 1.73 10.65 0.32 10.36
DualNet [28] 9.32 13.14 7.01 20.70 4.92 25.53
BiMeCo [25] 11.18 14.27 10.41 22.75 5.86 26.33
ICL w/o System2 (ours) 49.91 74.36 49.87 68.62 48.75 73.95
ICL (ours) 54.85 75.57 52.46 69.87 49.98 74.63
500/600ER[34] 1.30 13.02 1.27 22.84 1.56 20.42
A-GEM [6] 1.31 9.55 1.23 19.35 1.58 21.89
iCaRL [32] 0.43 3.62 1.01 7.60 1.62 14.23
CVT [44] 1.95 5.68 1.24 6.97 1.45 15.58
SCoMMER [37] 0.61 3.35 1.40 10.05 0.56 12.60
DualNet [28] 10.03 13.44 8.70 20.40 6.40 31.80
BiMeCo [25] 11.89 14.57 12.13 22.45 7.34 32.73
ICL w/o System2 (ours) 54.60 75.57 54.65 76.02 52.46 77.05
ICL (ours) 56.34 78.36 55.99 79.68 53.60 80.47
Table 2. Comparison under different task number settings on
ImageNet-R dataset.
as L2P [46] and DualPrompt [45]. Furthermore, the results
illustrate a consistent enhancement in our method perfor-
mance as the buffer size increases.
Results of forgetting curve comparison. To illustrate the
forgetting process within each compared methods in the CL
data stream, we record the average test accuracy for both the
current and preceding tasks upon completing the training of
each task. Subsequently, we create a line chart to visualize
the change in accuracy with the addition of each task, of-
fering a visual representation of the forgetting process. Fig.
2(a) and Fig. 2(b) provide clear illustrations that, as new
tasks are introduced, most methods exhibit a decline in per-
formance. However, our method consistently outperforms
these methods in terms of accuracy at every stage.
4.3. Ablation Study
Analysis of hyperparameters and margin .
Fig. 2(c) and Fig. 2(d) depict the impact of parameters 
andon our method performance, using the CIFAR100 and
ImageNet-R datasets, with memory sizes set at 500 and 600,
respectively. An analysis of the heat maps reveals that our
method’s sensitivity to varies between datasets. Speciﬁ-
cally, CIFAR100 demonstrates optimal performance when
= 0:1, while ImageNet-R maintains consistent perfor-
mance with values within the range [0:001;0:01;0:1]. In
contrast, variations in do not result in substantial accu-
racy differences between the datasets. This suggests that
the margin regularization term has only a marginal impact
on overall model performance. Importantly, when = 0
and= 1 (as shown in the bottom), the absence of the
regularization term leads to diminished performance, un-
derscoring the value of incorporating margin regularization
to enhance the model’s effectiveness.
Analysis of concentration parameter Selection. We con-
duct an investigation into the inﬂuence of the concentration
parameteron model performance. Fig. 2(e) indicates that
there is no signiﬁcant variation in performance across dif-ferent values of for the CIFAR10 dataset. However, for
datasets such as CIFAR100 and ImageNet-R, it becomes
important to estimate the concentration in advance to en-
sure a more accurate modeling of the concentration. Conse-
quently, these datasets demonstrate a noticeable sensitivity
to the concentration parameter.
Impact of Top-k results in collaboration inference. To in-
vestigate the inﬂuence of the number of prompt categories
Kon the reasoning process of System2, a series of experi-
ments are conducted using different values of Kon multiple
datasets, as depicted in Fig. 2(f). The results clearly indi-
cate that memory vectors possessing well-deﬁned geomet-
ric structures facilitate stable memory retrieval, thereby pre-
venting System1 from deviating signiﬁcantly from the cen-
tral data point during the reasoning phase. Consequently,
when a smaller number of top-k choices are provided, it
can ensure almost guaranteed inclusion of the correct cate-
gories. Conversely, when Kis larger, there is an increased
tendency for erroneous category information to be included
in the prompt, further intensifying the potential for con-
fusion in System2. Fig. 2(f) also presents the accuracy
achieved without System2 across various datasets (blue hor-
izontal line in the ﬁgure), demonstrating the performance
enhancement by the incorporation of System2.
Impact of separate query and value memory optimiza-
tion. To assess the query-value separation strategy impact
on stable value memory modeling in persistent scenarios,
we compare its use (Fig. 2(g)) and absence (Fig. 2(h)).
Visualizing the value memory via tsne reduction reveals
that query-value parameter optimization can create more
focused value memory modeling, enhancing task discrim-
ination. The corresponding results consistently validate the
effectiveness of our proposed CL-vMF mechanism.
5. Conclusion
In our paper, we introduced ICL, a groundbreaking con-
tinual learning (CL) paradigm inspired by Complementary
Learning System theory in neurocognitive science. ICL
combines ViT with an interactive query and value memory
module powered by CKT-MHA, enhancing the efﬁciency of
fast thinking (System1). Additionally, it leverages our CL-
vMF mechanism to improve memory representation dis-
tinction. ICL also integrates multi-modal Large Language
Models (System2) with System1 for advanced reasoning,
dynamically modulated by hard examples detected through
our VMF-ODI strategy. Our experiments conﬁrmed the ef-
fectiveness of our framework in reducing forgetting, sur-
passing contemporary state-of-the-art methods.
Acknowledgement This work was supported in
part by the National Key R&D Program of China (No.
2023YFC3305102). We extend our gratitude to the
anonymous reviewers for their insightful feedback, which
has greatly contributed to the improvement of this paper.
12889
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
pages 139–154, 2018. 2
[2] Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh,
and Suvrit Sra. Clustering on the unit hypersphere using
von mises-ﬁsher distributions. J. Mach. Learn. Res. , 6:
1345–1382, 2005. 5
[3] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide
Abati, and Simone Calderara. Dark experience for gen-
eral continual learning: a strong, simple baseline. Advances
in neural information processing systems , 33:15920–15930,
2020. 6, 1
[4] Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Si-
mone Calderara. Rethinking experience replay: a bag of
tricks for continual learning. In 2020 25th International Con-
ference on Pattern Recognition (ICPR) , pages 2180–2187.
IEEE, 2021. 6, 1
[5] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai,
Philip S Yu, and Lichao Sun. A comprehensive survey of
ai-generated content (aigc): A history of generative ai from
gan to chatgpt. arXiv preprint arXiv:2303.04226 , 2023. 1, 2
[6] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,
and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-
gem. arXiv preprint arXiv:1812.00420 , 2018. 6, 7, 8
[7] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS
Torr, and Marc’Aurelio Ranzato. On tiny episodic memo-
ries in continual learning. arXiv preprint arXiv:1902.10486 ,
2019. 2
[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2, 6
[9] Jonathan St BT Evans. In two minds: dual-process accounts
of reasoning. Trends in cognitive sciences , 7(10):454–459,
2003. 1
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 6
[11] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, et al. The many faces of robust-
ness: A critical analysis of out-of-distribution generalization.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 8340–8349, 2021. 6
[12] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a uniﬁed classiﬁer incrementally via
rebalancing. In Proceedings of the IEEE/CVF conference on
Computer Vision and Pattern Recognition , pages 831–839,
2019. 2[13] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-
rank adaptation of large language models. In International
Conference on Learning Representations , 2021. 6
[14] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proceedings of the National Academy of Sci-
ences , 114(13):3521–3526, 2017. 1, 2, 6, 7, 8
[15] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 6
[16] Dharshan Kumaran, Demis Hassabis, and James L McClel-
land. What learning systems do intelligent agents need?
complementary learning systems theory updated. Trends in
cognitive sciences , 20(7):512–534, 2016. 1
[17] Janghyeon Lee, Hyeong Gwon Hong, Donggyu Joo, and
Junmo Kim. Continual learning with extended kronecker-
factored approximate curvature. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9001–9010, 2020. 1, 2
[18] Omer Levy, Yoav Goldberg, and Ido Dagan. Improving dis-
tributional similarity with lessons learned from word embed-
dings. Transactions of the Association for Computational
Linguistics , 3:211–225, 2015. 5
[19] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE transactions on pattern analysis and machine intelli-
gence , 40(12):2935–2947, 2017. 6, 7, 8
[20] Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer,
Antonio M Lopez, and Andrew D Bagdanov. Rotate your
networks: Better weight consolidation and less catastrophic
forgetting. In 2018 24th International Conference on Pattern
Recognition (ICPR) , pages 2262–2268. IEEE, 2018. 2
[21] Arun Mallya and Svetlana Lazebnik. Packnet: Adding mul-
tiple tasks to a single network by iterative pruning. In Pro-
ceedings of the IEEE conference on Computer Vision and
Pattern Recognition , pages 7765–7773, 2018. 1, 2
[22] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy-
back: Adapting a single network to multiple tasks by learn-
ing to mask weights. In Proceedings of the European con-
ference on computer vision (ECCV) , pages 67–82, 2018. 1,
2
[23] Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.
Weakly-supervised hierarchical text classiﬁcation. In The
Thirty-Third AAAI Conference on Artiﬁcial Intelligence,
AAAI 2019, The Thirty-First Innovative Applications of Arti-
ﬁcial Intelligence Conference, IAAI 2019, The Ninth AAAI
Symposium on Educational Advances in Artiﬁcial Intelli-
gence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -
February 1, 2019 , pages 6826–6833, 2019. 5
[24] Martial Mermillod, Aurélia Bugaiska, and Patrick Bonin.
The stability-plasticity dilemma: Investigating the contin-
uum from catastrophic forgetting to age-limited learning ef-
fects, 2013. 1
[25] Xing Nie, Shixiong Xu, Xiyan Liu, Gaofeng Meng, Chunlei
Huo, and Shiming Xiang. Bilateral memory consolidation
for continual learning. In Proceedings of the IEEE/CVF Con-
12890
ference on Computer Vision and Pattern Recognition , pages
16026–16035, 2023. 1, 6, 7, 8
[26] Randall C O’Reilly and Kenneth A Norman. Hippocampal
and neocortical contributions to memory: Advances in the
complementary learning systems framework. Trends in cog-
nitive sciences , 6(12):505–510, 2002. 1
[27] Dongmin Park, Seokil Hong, Bohyung Han, and Kyoung Mu
Lee. Continual learning by asymmetric loss approxima-
tion with single-side overestimation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 3335–3344, 2019. 2
[28] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Con-
tinual learning, fast and slow. Advances in Neural Informa-
tion Processing Systems , 34:16131–16144, 2021. 1, 2, 6, 7,
8
[29] Biqing Qi, Bowen Zhou, Weinan Zhang, Jianxing Liu, and
Ligang Wu. Improving robustness of intent detection un-
der adversarial attacks: A geometric constraint perspective.
IEEE transactions on neural networks and learning systems ,
PP, 2023. 5
[30] Biqing Qi, Bowen Zhou, Weinan Zhang, Jianxing Liu, and
Ligang Wu. Improving robustness of intent detection un-
der adversarial attacks: A geometric constraint perspective.
IEEE Transactions on Neural Networks and Learning Sys-
tems, 2023. 1
[31] Qi Qin, Wenpeng Hu, Han Peng, Dongyan Zhao, and Bing
Liu. Bns: Building network structures dynamically for con-
tinual learning. Advances in Neural Information Processing
Systems , 34:20608–20620, 2021. 1, 2
[32] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer
and representation learning. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition , pages
2001–2010, 2017. 1, 6, 7, 8
[33] Blake A Richards and Paul W Frankland. The persistence
and transience of memory. Neuron , 94(6):1071–1084, 2017.
1
[34] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,
Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn
without forgetting by maximizing transfer and minimizing
interference. arXiv preprint arXiv:1810.11910 , 2018. 2, 6,
7, 8
[35] Tomás J Ryan and Paul W Frankland. Forgetting as a form
of adaptive engram cell plasticity. Nature Reviews Neuro-
science , 23(3):173–186, 2022. 1
[36] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient
projection memory for continual learning. arXiv preprint
arXiv:2103.09762 , 2021. 1, 2
[37] Fahad Sarfraz, Elahe Arani, and Bahram Zonooz. Sparse
coding in a dual memory system for lifelong learning. In
Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence , pages 9714–9722, 2023. 6, 7, 8
[38] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton,
Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam
Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Opti-
mizing language models for dialogue. OpenAI blog , 2022.
2[39] Advani Sun, Weinan et al. Organizing memories for gener-
alization in complementary learning systems. Nature neuro-
science , 26(8):1438–1448, 2023. 1
[40] Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu, and
Wanli Ouyang. Layerwise optimization by gradient de-
composition for continual learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9634–9643, 2021. 1, 2
[41] Gido M Van de Ven and Andreas S Tolias. Three scenar-
ios for continual learning. arXiv preprint arXiv:1904.07734 ,
2019. 6
[42] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A
comprehensive survey of continual learning: Theory, method
and application. arXiv preprint arXiv:2302.00487 , 2023. 2
[43] Zhen Wang, Liu Liu, Yiqun Duan, Yajing Kong, and
Dacheng Tao. Continual learning with lifelong vision trans-
former. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
171–181, 2022. 6
[44] Zhen Wang, Liu Liu, Yajing Kong, Jiaxian Guo, and
Dacheng Tao. Online continual learning with contrastive vi-
sion transformer. In ECCV , 2022. 1, 2, 6, 7, 8
[45] Zifeng Wang, Zhang, et al. Dualprompt: Complementary
prompting for rehearsal-free continual learning. In ECCV ,
pages 631–648, 2022. 1, 2, 3, 6, 7, 8
[46] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer
Dy, and Tomas Pﬁster. Learning to prompt for continual
learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 139–149,
2022. 1, 2, 3, 6, 7, 8
[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large lan-
guage models. Advances in Neural Information Processing
Systems , 35:24824–24837, 2022. 2
[48] Gordon Winocur, Morris Moscovitch, and Melanie Sekeres.
Memory consolidation or transformation: context manipu-
lation and hippocampal representations of memory. Nature
neuroscience , 10(5):555–557, 2007. 1
[49] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynam-
ically expandable representation for class incremental learn-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 3014–3023,
2021. 7
[50] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
Conference on Machine Learning , pages 3987–3995. PMLR,
2017. 1, 2, 6, 7, 8
[51] Chao Zhang, Liyuan Liu, Dongming Lei, Quan Yuan, Hon-
glei Zhuang, Timothy Hanratty, and Jiawei Han. Triove-
cevent: Embedding-based online local event detection in
geo-tagged tweet streams. pages 595–604, 2017. 5
[52] Qiang Zhou, Zhibin Wang, Wei Chu, Yinghui Xu, Hao Li,
and Yuan Qi. Infmllm: A uniﬁed framework for visual-
language tasks, 2023. 6
[53] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
12891
understanding with advanced large language models. arXiv
preprint arXiv:2304.10592 , 2023. 6, 2
12892
