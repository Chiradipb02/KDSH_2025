All Rivers Run to the Sea: Private Learning with Asymmetric Flows
Yue Niu1Ramy E. Ali2*Saurav Prakash3Salman Avestimehr1
1University of Southern California2Samsung3University of Illinois Urbana-Champaign
yueniu@usc.edu ramy.ali@samsung.com sauravp2@illinois.edu avestime.usc.edu
Abstract
Data privacy is of great concern in cloud machine-
learning service platforms, when sensitive data are exposed
to service providers. While private computing environments
(e.g., secure enclaves), and cryptographic approaches (e.g.,
homomorphic encryption) provide strong privacy protec-
tion, their computing performance still falls short compared
to cloud GPUs. To achieve privacy protection with high
computing performance, we propose Delta , a new private
training and inference framework, with comparable model
performance as non-private centralized training. Delta
features two asymmetric data ﬂows: the main information-
sensitive ﬂow and the residual ﬂow. The main part ﬂows
into a small model while the residuals are ofﬂoaded to a
large model. Speciﬁcally, Delta embeds the information-
sensitive representations into a low-dimensional space while
pushing the information-insensitive part into high-dimension
residuals. To ensure privacy protection, the low-dimensional
information-sensitive part is secured and fed to a small
model in a private environment. On the other hand, the resid-
ual part is sent to fast cloud GPUs, and processed by a large
model. To further enhance privacy and reduce the communi-
cation cost, Delta applies a random binary quantization
technique along with a DP-based technique to the residuals
before sharing them with the public platform. We theoret-
ically show that Delta guarantees differential privacy in
the public environment and greatly reduces the complexity
in the private environment. We conduct empirical analy-
ses on CIFAR-10, CIFAR-100 and ImageNet datasets and
ResNet-18 and ResNet-34, showing that Delta achieves
strong privacy protection, fast training, and inference with-
out signiﬁcantly compromising the model utility.
1. Introduction
In the current machine learning (ML) era, cloud ML services
with high-end GPUs have become indispensable. On the
other hand, ensuring data privacy is one of the most critical
*R.E. Ali was with the University of Southern California.challenges in the ML platforms. During training, privacy
breaches may occur if training data is exposed to ML service
providers, increasing vulnerability to potential attacks. Ad-
ditionally, users’ inference queries can also be susceptible
to attacks when accessing ML services with sensitive data
[36,44]. In particular, an untrusted ML platform can cache,
learn, and leak queries without users’ awareness [ 45].
Related Works Overview . While prior privacy-
preserving machine learning (PPML) frameworks [ 1,7,35,
39,42,51] mitigate privacy concerns in training and infer-
ence, they also come with different tradeoffs. Differential
privacy (DP) based methods perturb the data before outsourc-
ing to an untrusted cloud [ 38,61] to ensure privacy, but they
usually result in degraded model utility even under moderate
privacy constraints [ 8]. On the other hand, crypto-based tech-
niques [ 39,51], that provide data protection with encryption
schemes, have not yet proved efﬁcient and scalable to large
models due to their prohibitive complexities.
PPML with private environments (e.g., trusted execution
environments (TEEs), local environments) presents a promis-
ing solution by physically isolating the running computing
environments. Such private environments are, however, usu-
ally resource-constrained compared to public cloud services
with high-end GPUs, resulting in lower computing perfor-
mance [ 19,47]. However, the prior works based on leverag-
ing these private environments along with the public GPUs
also incur high complexity [ 20,40,41,55]. One reason for
these methods being inefﬁcient is the heavy communication
between the private and the public environments.
Proposed Solution Overview. In this paper, we con-
sider a private training and inference setting where users can
access both private and public environments , as shown in
Figure 1a. We propose a new private training and inference
framework, Delta , that achieves strong privacy protection
with high model utility and low complexity. The core idea of
Delta originates from an observation that the intermediate
representations (IRs) in ML models exhibit an asymmetric
structure. Speciﬁcally, the primary sensitive information
is usually encoded in a low-dimensional space, while the
high-dimensional residuals contain very little information.
Inspired by such an observation, we design a two-way train-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12353
Private
EnvPublic
Env
PrivatePublic
(a) Problem Setup.
Mbb Decompose Mmainlogits
Add SoftMax Pred
IRmain+QuantMreslogits
noise
IRresIRquant
Private (TEEs, local env)Public (cloud GPUs)
(b) An overview of Delta .
Figure 1. Overview of Delta : the backbone Mbbacts as a feature extractor. The features are decomposed into low-dimensional (information-sensitive)
and high-dimensional (residual) parts: IRmainandIRres.IRmainis fed to a small model Mmain, while IRresare outsourced to a large model Mres.Mbband
Mmainrun in a resource-constrained private environment, whereas Mresis ofﬂoaded to a public environment while ensuring privacy through a DP scheme.
While only the forward pass is shown, backpropagation is also private (See Sec 4.4).
ing framework that respectively learns low-dimensional but
information-sensitive features IRmainwith a small model
Mmain, and learns the high-dimensional residuals IRreswith
alarge model Mres, as illustrated in Figure 1b. Given a
model, Delta selects a few front layers as a backbone
(Mbb) for extracting features and generating intermediate
representations. Delta uses singular value decomposi-
tion (SVD) and discrete cosine transformation (DCT) to
extract the low-dimensional information-sensitive represen-
tation IRmainand the residuals IRres. We design a new
low-dimensional model ( Mmain) to learn the information-
sensitive IRmain. While MbbandMmainrun in a private
environment, the rest of the model ( Mres) learns residuals
in a public environment. Delta further applies DP pertur-
bation and binary quantization on the residuals, leading to
further privacy protection and a communication reduction.
Owing to the asymmetric structure in IR, Delta guarantees
differential privacy on IRreswith much smaller noise added
compared to the naive scheme that directly adds noise to IR
(naive-DP), leading to much-improved model utility. In both
training and inference phases, Delta ensures that only the
residual part is public, while the information-sensitive part
is secured in private environments.
Delta is a generic PPML solution that can be ﬂexibly
deployed in several scenarios. For instance, in a cloud ML
platform with private TEEs and public GPUs [ 5,6],Mbb
andMmaincan run inside TEEs to preserve privacy, while
Mresruns in GPUs to speed up the computations. In general
distributed settings, Delta can let clients train Mbband
Mmainlocally, while a cloud server performs side training
with Mres. In summary, our contributions are as follows.
1. We propose a PPML framework for training and in-
ference with a much-improved privacy-utility trade-off com-
pared to the naive-DP methods, a low computing complexity
in the private environments and a low communication cost.
2. We design an asymmetric decomposition layer that
extracts the low-dimensional information-sensitive represen-
tations using SVD and DCT, and design a low-complexity
model for resource-constrained computing environments.3. We provide a formal differential privacy analysis for
the proposed framework. In addition, we show empirically
thatDelta provides strong privacy protection against model
inversion and membership inference attacks.
4. We conduct comprehensive evaluations, showing that
Delta leads to a better privacy-utility trade-off than the
naive DP-based method that directly adds noise to IR. Specif-
ically, under the same privacy budget, Delta improves the
accuracy by up to 31%. Moreover, Delta greatly reduces
the running time compared to other PPML solutions.
2. System Model
We start by describing the problem setting, the threat model,
and our notations.
Problem Setting. We consider a setting where users
have private resource-constrained environments (e.g., cloud
TEEs, local CPUs/GPUs), but they can also access public,
untrusted, and high-end services (e.g., cloud GPUs) to ac-
celerate training and inference. The goal is to protect users’
training and inference data, while maintaining computing
performance and high model utility. Note that this setting
differs from DP-SGD [ 1,30,60], where the goal is to protect
the training data from attacks on gradients or models.
Threat Model. Our threat model is similar to the model
considered in prior works leveraging private environments
[41,47,55]. Speciﬁcally, we assume that the private environ-
ment is protected against all untrusted, unauthorized access
to data and models inside. However, denial-of-service and
side-channel attacks are out of our scope. On the other hand,
the untrusted, public environment is semi-honest (honest-
but-curious). That is, the untrusted public server follows the
training and inference protocol faithfully but may attempt to
learn as much as possible from what it receives.
Remark 1.While we only consider the semi-honest model
in our work, veriﬁable computing techniques [ 3,18,52] can
be incorporated to enhance Delta against malicious parties.
Notations. We denote tensors by capital bold letters as
X, matrices by capital letters as X, and vectors by small
12354
bold letters as x.Xidenotes a tensor slice X(i,:).k.kF
denotes the Frobenius norm or in general the square root of
the sum of squares of elements in a tensor. “ ·” denotes matrix
multiplication or in general batch matrix multiplication. “ ~”
denotes a convolution operation. X⇤indicates a transpose.
3. Asymmetric Structure in IRs
We ﬁrst observe that intermediate representations (IRs) in
neural networks (NNs) exhibit highly asymmetric structures
in multiple dimensions. These asymmetric structures are
essential for an asymmetric decomposition that embeds the
privacy-sensitive information into low-dimensional space.
3.1. Asymmetric Structure in Channel Dimension
In NNs such as convolutional neural networks (CNNs), each
layer’s input and output consists of multiple channels, de-
noted as X2Rc⇥h⇥w, where cis the number of channels
andhandwdenote the height and the width.
We analyze the channel correlation by ﬁrst ﬂattening X
asX2Rc⇥hwand then performing singular value decom-
position (SVD) on the ﬂattened tensor Xas
X=cP
i=1si·ui·v⇤
i,
where siis the i-th singular value, ui2Rcandvi2Rhw
are the i-th left and right singular vectors, respectively. We
reshape vito the original dimensions as a principal channel
Vi2Rh⇥w, and uias a tensor Ui2Rc⇥1⇥1.
We then obtain a l ow-rank representation of Xas follows
Xlr=rP
i=1si·Ui·Vi,
where rdenotes the number of principal channels in Xlr2
Rc⇥h⇥w. Figure 2ashows the normalized errorkX Xlrk
kXkver-
susrafter the ﬁrst convolutional layer in ResNet-18 (based
on ImageNet). We observe that Xlrwith a small ris sufﬁ-
cient to approximate X. That is, most information in Xcan
be embedded into Xlrin a low-dimensional space. We notice
that3LegRace [41] also investigated such a property. How-
ever, unlike 3LegRace , we leverage the low-rank property
to design a new low-complexity model (See Sec. 4.3).
3.2. Asymmetric Structure in Spatial Dimension
The asymmetric structure of the IR also exists over the spatial
dimension due to correlations among pixels in each chan-
nelXi2Rh⇥w. We use discrete cosine transform (DCT)
to analyze the spatial correlation. Speciﬁcally, we obtain
frequency components using t⇥tblock-wise DCT [ 25] as,
Ci=DCT (Xi,T)= 
Ci
k,j=T·Xi
k,j·T⇤ h/t,w/t
k,j=1,
where Ci
k,j2Rt⇥t,Xi
k,j=Xi(kt t:kt,jt  t:jt)
andT2Rt⇥tis the DCT transformation matrix. Ci2Rh⇥wis obtained by simply concatenating Ci
k,jfork2
1, ..., h/t, j 21, ..., w/t . Then, we obtain a low-frequency
representation, Xi
lf, using inverse t0⇥t0block-wise DCT as,
Xi
lf=IDCT (Ci
lf,Tlf)= 
Xi
lf,k,j=T⇤
lf·Ci
lf,k,j·Tlf h/t,w/t
k,j=1,
where t0<t,Xi
lf2Rh
tt0⇥w
tt0,Ci
lf,k,j=Ci
k,j(0 :t0,0:t0),
andTlf2Rt0⇥t0is the DCT matrix. Note that t02represents
the number of low-frequency components in Xi
lf,k,j.
12.5% 25% 37.5% 50%00.1  X Xlr  
kXkFraction of principal channels inXlrError(a) Error of the low-rank approximation vs. r/c ⇥100% .
8% 18% 32% 50%00.1  X Xlf  kXkFraction of low-freq components inXlfError(b) Error of the low-frequency approximation vs.(t0/t)2⇥100%.Figure 2.Asymmetric structures along channel and spatial dimension(based on ResNet-18 on ImageNet). Most information inXcan be embed-ded into low-rank and low-frequency representations.Owing to the spatial correlation,Xlfcan sufﬁciently ap-proximateXusing a few low-frequency components. Fig-ure2bshows the errorkX XlfkkXkversus the number of low-frequency components inXlfafter the ﬁrst convolution layerin ResNet-18. We can also observe that most information inXcan be embedded intoXlfwith low-frequency components.The asymmetric structure also exists in other tasks, suchas language models (See Appendix10.1). Observing theasymmetric structures in different dimensions, we aim to de-sign anasymmetric learningframework that learns privacy-sensitive low-dimensional IR with a low-complexity modelin a private environment, while sending residuals to a largermodel trained on an untrusted platform.4.Delta: Private Asymmetric LearningOverview.At a high level, as shown in Fig.1, given amodel,Deltakeeps the ﬁrst few layers in a private en-vironment as a backbone modelMbband a main modelMmain.Deltathen ofﬂoads all remaining layers to apublic environment as residual modelMres. Speciﬁcally,Deltauses the backbone modelMbbas a feature extrac-tor to compute intermediate representations (IRs).Deltathenasymmetricallydecomposes the IRs, obtaining a low-dimensional information-sensitive part (IRmain) and residualsIRres.Deltadesigns a new low-dimensional modelMmainforIRmainto reduce computation complexity in the privateenvironment. On the other hand, the residualsIRresare
12355
perturbed and quantized before being outsourced to the un-
trusted public environment that hosts Mres. At last, output
logits from MmainandMresare added in the private environ-
ment, leading to the ﬁnal predictions. These ﬁnal predictions
are not disclosed to the public environment. Hence, during
training and inference, Delta allows only minimal residual
information to be leaked to the public environment.
4.1. Asymmetric IR Decomposition
As observed in Section 3, IRs after Mbbexhibit asymmetric
structures in multiple dimensions. Hence, we can decom-
pose IRs such that most information is encoded in the low-
dimensional IR denoted as IR main.
Singular Value Decomposition (SVD). Given an IR X2
Rc⇥h⇥wobtained from the backbone model Mbb, we ﬁrst
apply SVD as explained in Sec. 3.1to obtain the principal
channels 
Vi2Rh⇥w c
i=1and the corresponding coefﬁ-
cients 
Ui2Rc⇥1⇥1 c
i=1. We then select the rmost prin-
cipal channels as a low-rank representation of Xas in Sec.
3.1. The rest of the channels are saved as SVD residuals as
XSVD res =X Xlr=cX
i=r+1si·Ui·Vi. (1)
Discrete Cosine Transform (DCT). After the decompo-
sition along channels, we further decompose Viover the
spatial dimension using DCT.
Speciﬁcally, for each principal channel Vi, we ﬁrst obtain the
frequency-domain coefﬁcients as Ci=DCT (Vi,T). Then,
we only use the low-frequency component to reconstruct a
representation as Vi
lf=IDCT (Ci
lf,Tlf)2Rh
tt0⇥w
tt0as in
Sec3.2.Ci
lfhas a reduced dimension and keeps only top-
left low-frequency coefﬁcients in Ci(as shown in Figure 3).
Tlf2Rt0⇥t0corresponds to DCT transformation matrix with
reduced spatial dimension. The rest of the high-frequency
components are saved as DCT residuals as
Vi
DCT res =IDCT (Ci
res,T), (2)
where Ci
resdenotes Ciwith zeros on the top-left corner.
After SVD and DCT, we obtain the privacy-sensitive low-
dimensional features as
IRmain=rX
i=1si·Ui·Vi
lf. (3)
On the other hand, the residuals to be ofﬂoaded to the un-
trusted public environment are given as
IRres=X IRmain=XSVD res +XDCT res
=cX
i=r+1si·Ui·Vi+rX
i=1si·Ui·Vi
DCT res .(4)IRresis further normalized as
IRres,norm =IRres/max(1 ,kIRresk2/C), (5)
where Cis a scaling parameter for `2normalization. The
normalization is necessary to bound sensitivity for DP. For
simpliﬁcation, we denote the normalized residuals as IR res.
Hence, IRmainhas fewer principal channels and smaller
spatial dimensions but contains most information in IR. The
non-principal channels and high-frequency components, on
the other hand, are saved in IRres.IRmainandIRresare then
respectively fed into a small model Mmainin a private envi-
ronment and a large model Mresin a public environment.
4.2. Residuals Perturbation and Quantization
While the low-dimensional representation IRmainhas the
most important information in the IR, IRresmight still con-
tain some information such as a few high-frequency compo-
nents. Furthermore, as the communication from a private en-
vironment (e.g., TEEs) to a public environment (e.g., GPUs)
is usually slow, sending ﬂoating-point high-dimensional
residuals can signiﬁcantly increase the communication over-
head and prolong the total running time.
In this section, we perturb IRreswith a Gaussian mech-
anism and then apply binary quantization on the perturbed
IRresto reduce the inter-environment communication cost.
Given a DP budget ✏, we consider the Gaussian mechanism,
and compute the corresponding noise parameter  (See Sec-
tion5). For each tensor IRres, we generate a noise tensor
N2Rc⇥h⇥w⇠N(0, 2·I), and add it to IRresin the private
environment. With noisy residuals, IRnoisy=IRres+N, we
apply a binary quantizer as follows
IRquant =B i n Q u a n t ( IRnoisy)=(
0IRnoisy<0,
1IRnoisy 0.(6)
As a result, the tensor to be ofﬂoaded to the public en-
vironment is a binary representation of the residuals. Com-
pared to ﬂoating-point values, such a binary representation
reduces communication by 32⇥. Owing to the asymmet-
ric decomposition, the values in IRresare usually close to
zero. Hence, a small noise is sufﬁcient to achieve strong pri-
vacy protection (See formal analysis in Section 5). Further
ablation studies can be found in Section 9.3.
4.3. Model Design for Low-Dimensional IRmain
Knowing that IRmainhas a low-rank as given in Equation ( 3),
in this section, we show that developing an efﬁcient Mmain
with low computation complexity is attainable.
In linear layers such as convolutional layer, low-rank in-
puts lead to low-rank outputs [ 41]. Assuming inputs and
outputs have rank randq, we split the convolution layer
into two sub-layers as shown in Figure 4(right). The ﬁrst
layer has q(k⇥k) kernels to learn the principal features,
12356
IR
cSVDVi
rDCTCi
rIDCT
rIRmainlow-rank and smaller!Ci
lfXSVD res XDCT res Add IRreslittle information!
c
Figure 3. The asymmetric IR decomposition is shown (See Figure 1for the whole pipeline). We use SVD and DCT to encode channel and spatial information
into a low-dimensional representation, and ofﬂoad the residuals to public environments. The low-dimensional IRmainhas fewer channels and smaller sizes but
still encode most sensitive information. The residuals IR reshave the same dimension as the original IR.
Conv: c, k ⇥kInput: rank r
Output: rank q
Complexity: O(nck2)WConv: q,k ⇥k
Conv: n,1⇥1Input: rank r
Output: rank q
Complexity: O(qck2+nq)W(1)
W(2)
Figure 4. Model design for the low-dimensional IRmain. Knowing the
rank in data, the number of channels in convolution layers can be reduced,
leading to a reduction in computation complexity.
whereas the second layer has c(1⇥1) kernels to combine
the principal channels. We further add a kernel orthogo-
nality regularization [ 59] to the ﬁrst layer to enhance the
orthogonality of the output channels.
Such a design comes with fundamental reasoning. Theo-
rem1shows that, with knowledge of the ranks of inputs and
outputs in a convolutional layer, it is possible to optimize
a low-dimensional layer as in Figure 4(right) such that it
results in the same output as the original layer.
Theorem 1. For a convolution layer with weight W2
Rn⇥c⇥k⇥kwith an input Xwith rank rand output Ywith
rank q, there exists an optimal W(1)2Rq⇥c⇥k⇥k,W(2)2
Rn⇥q⇥1⇥1in the low-dimensional layer such that the output
of this layer denoted as Y0satisﬁes
min
W(1),W(2)   Y Y0   =0. (7)
The proof is deferred to Appendix 8.1.
Remark 2.While the low-dimensional layer in Figure 4
(right) shares similar architecture as low-rank compression
methods [ 23,24,63], low-rank compression inevitably in-
curs information loss in outputs. However, the layer in
Figure 4(right) can theoretically preserve all information
given low-rank inputs and outputs.
4.4. Private Backpropogation
While the asymmetric IR decomposition together with the
randomized quantization mechanism ensures privacy in for-
ward passes, information of IRmaincan still be leaked through
backpropagation on logits. In this section, we further pro-pose a private backpropagation that removes the logits of
Mmainfrom the gradients to Mres.
In detail, the gradients to Mmainare calculated through
a standard backpropagation algorithm that uses logits from
both MmainandMres. While gradients to Mresare calcu-
lated solely using Mreslogits. Hence, the logits from Mmain
will not be revealed to the outside. Speciﬁcally, given the out-
put logits from MmainandMres:zmainandzres, we compute
Softmax fori-th label MmainandMresas
Mmain:otot(i)=ezmain(i)+zres(i)/XL
j=1ezmain(j)+zres(j),
Mres:ores(i)=ezres(i)/XL
j=1ezres(j),
where Ldenotes the number of labels in the current task.
Following the backpropagation in Softmax , we compute
gradients to MmainandMres:gmain,gres, as
gmain=otot y,gres=ores y,
where ydenotes one-hot encoding for labels.
With the separate backpropagation on gradients, Mmain
avoids revealing its logit to Mres, while still using Mres’s
logits for its own backpropagation.
4.5. Training Procedure
The model training using Delta consists of two stages.
Stage 1. We train the backbone, Mbb, and the main model,
Mmain, with IRmainonly. IRresis ignored and not shared
with the public. Therefore, there is no privacy leakage at
this stage, as all data and model parameters are kept in the
private environment. After stage 1, we cache all residual
dataIRresfrom SVD and DCT decomposition, and apply the
randomized quantization mechanism once on IR res.
Stage 2. We freeze the backbone model, Mbb, and continue
to train the main model, Mmain, and the residual model,
Mres. AsMbbis frozen, we directly sample residual inputs
forMresfrom the cached residual data. While for Mbband
Mmain, we fetch data from the raw datasets, apply SVD and
DCT decomposition, and send IR maintoMmain.
The training procedure is provided in algorithm 1.
5. Privacy Analysis
This section provides the differential privacy analysis for
Delta . As the backbone model Mbb, the low-dimensional
12357
Algorithm 1 Delta training procedure
Require: ep1: #epochs at stage 1, ep2: #epochs at stage 2
Require: ✏: privacy constraint
1:Initialize Mbb,Mmain,Mres
2:fort=1,···,ep1do
3: fora batch from the dataset do
4: Train MbbandMmain.
5:Freeze Mbband cache all residual data IR res.
6:Apply the randomized quantization based on Eq ( 6).
7:fort=1,···,ep2do
8: fora batch from the dataset, cached residuals do
9: Train MmainandMres.
model Mmainand ﬁnal predictions remain private, the only
public information are the residuals and the residual model
Mresat stage 2 during training. Therefore, we analyze the
privacy leakage of the perturbed residuals at stage 2.
Given neighboring datasets D= 
X1,·,Xi,·,XN 
and D0= 
X1,·,0,·,XN 
with i-th record re-
moved, the global `2-sensitivity is deﬁned as  2=
supXres,X0
res   Xres X0
res   
FC, where Xres,X0
resdenote
residuals obtained from DandD0.
We provide our DP guarantee in Theorem 2.
Theorem 2. Delta ensures that the perturbed residuals
and operations in the public environment satisfy (✏,  )-DP
given noise N⇠N(0,2C2·log (2 / 0)/✏0·I)given sampling
probability p, and ✏= log (1 + p(e✏0 1)), =p 0.
The proof of Theorem 2relies on the analysis of Gaussian
mechanism and the post-processing rule of DP [ 17,28], and
is provided in Appendix 8.2for completeness.
Remark 3.(Training with Cached Residual Improves DP).
Sampling an image multiple times does not incur additional
privacy leakage. As described in Sec. 4.5, the perturbation is
only performed once before training the residual model Mres.
When training the residual model Mres,Delta directly
samples input for Mresfrom the perturbed cached residuals,
with no need to perform perturbation on the ﬂy [ 48].
6. Empirical Evaluations
In this section, we evaluate Delta in terms of model ac-
curacy, running time, and resilience against attacks. We
conduct further ablation studies on different ways of merg-
ing logits and effects of perturbation in Appendix 9.
Datasets and Models. We use CIFAR10/100 [ 29] and Ima-
geNet [ 15]. For models, we choose ResNet-18 and ResNet-
34 [21]. Hyperparameters are provided in Appendix 11.2
Model Conﬁguration. For ResNet models, Mbbconsists of
the ﬁrst convolution layer, while all ResBlocks [21] and the
fully-connected layer are ofﬂoaded in Mres. On the otherhand, Mmain’s details are deferred to Appendix 11.1.
Asymmetric Decomposition . For the SVD, based on [ 41],
we keep r=8 principal channels in IRmainfor CIFAR-
10/100 and 12for ImageNet to keep >95% information in
the private environment. For the DCT, we set t, t0= 16 ,8on
CIFAR, and 14,7on ImageNet to avoid noticeable informa-
tion leakage in the residuals. Note that while decomposition
with larger randt0leads to less information leakage, it also
incurs more computations in the private environment. There-
fore, we choose the parameters to maintain a reasonable
trade-off between privacy leakage and complexity.
6.1. Model Accuracy
In this section, we evaluate model accuracy. We train ResNet-
18 on CIFAR-10/100, and ResNet-18/34 on ImageNet. We
ﬁrst compare the accuracy of the following schemes.
•Mmain: Train Mbb+Mmainwith no residuals.
•Mmain+Mres: Train Mbb+(Mmain,Mres) with DP.
•Orig: Train the original model without Delta .
Figure 5and6show the ﬁnal accuracy on CIFAR-10/100
and ImageNet. First, owing to the effective asymmetric IR
decomposition and the low-dimensional model, Mmainal-
ready gives an accuracy that is close to the original model
on both CIFAR-10/100 and ImageNet datasets. With adding
Mres,Delta achieves a comparable accuracy as the origi-
nal model. With the residual information further protected
by Gaussian noise, we observe that Delta strikes a much
improved privacy-utility trade-off, with slight performance
degradation under a low privacy budget (small ✏).
929394CIFAR-10Val Acc(%)
MmainMmain,res
(✏=1.4)Mmain,res
(✏=9)Mmain,res
(✏=1 7 )Mmain,res
(✏=1)Orig707274CIFAR-100Val Acc(%)Figure 5. Val acc of ResNet-18 on CIFAR-10, CIFAR-100. Mmaingives
accuracy close to the original model. With adding Mres,Delta achieves
comparable accuracy as the original model. By adding noise to IRres,
Delta achieves strong DP while still preserving the model performance.
To further study the privacy-utility trade-off, we compare
Delta with a scheme that directly adds noise to IR of the
original model (naive-DP) under the same privacy guarantee.
As in Equation ( 5), we perform normalization on IR before
perturbation. As shown in Table 1, training with noise added
to IR incurs signiﬁcant performance degradation given the
same ✏. With the asymmetric IR decomposition, Delta
offers a much better privacy-utility trade-off.
12358
64666870ResNet-18Val Acc(%)
MmainMmain,res
(✏=1.4)Mmain,res
(✏=1 0 )Mmain,res
(✏=2 0 )Mmain,res
(✏=1)Orig707274ResNet-34Val Acc(%)Figure 6. Val acc of ResNet-18 and ResNet-34 on ImageNet.
Table 1. Model accuracy of ResNet-18 by perturbing IR and IRres
under the same privacy budget ( ✏=1.4).
Dataset Delta : perturb IR res naive-DP: perturb IR
CIFAR-10 92.4% 69 .6% (# 22.8)
CIFAR-100 71.4% 48 .3%(# 23.1)
ImageNet 65.9% 34 .4%(# 31.5)
6.2. Running Time Analysis
In this section, we compare running time using Delta with
a private training framework 3LegRace [41] and private
inference framework Slalom [55]. As a reference, we also
include the time of running the original model solely in a
private environment ( Priv-only ).
Theoretical Complexity. Table 2lists the theoreti-
cal computational complexities by MACs (i.e., multiply-
accumulate) of ResNet-18 on CIFAR-10/100, and ResNet-34
on ImageNet during the inference phase with a batch of size
1. For SVD, we use an approximation algorithm [ 41] that
only computes the ﬁrst rprincipal channels. First, we can
see that SVD and DCT only account for a very small fraction
of the total computations, which aligns with the real running
time in Table 4. On the other hand, compared to MACs in
Mres, the computation complexity of Mbb+Mmainis much
smaller, only accounting for ⇠10% of MACs in Mres. This
shows that, with the asymmetric decomposition that embeds
most information into a low-dimensional representation, the
low-dimensional model effectively reduces the computation
cost of the resource-constrained private environments.
Table 2. MACs of ResNet-18 and ResNet-34 during forward passes
with batch size 1. Compared to the residual model, the complexity of the
backbone and the main model is much smaller.
Model Mbb+Mmain SVD DCT Mres
ResNet-18 48.3 M 0.52 M 0.26 M 547M
ResNet-34 437 M 1.6 M 0.7 M 3.5G
Real Running Time. We test ResNet-18 on CIFAR-100
with batch size 32and average per-iteration time across one
epoch. We use Intel SGX [ 14] as a private environment, and
Nvidia RTX 5000 as an untrusted public environment.
Table 3shows the per-iteration training and inference timeTable 3. Running time of Delta and other baselines (ResNet-18/CIFAR-
100, b=3 2 ).Delta achieves signiﬁcant speedup compared to
3LegRace , and Slalom . Each cell denotes (time (ms), speedup)
Task Priv-only 3LegRace Slalom Delta
Train 1372 237 ( 6⇥) - 62 ( 22⇥)
Infer. 510 95 ( 5⇥) 84 ( 6⇥) 20 ( 25⇥)
Table 4. Time breakdown on ResNet-18/CIFAR-100. Delta signiﬁcantly
shrinks the time gap between the private and public environments.
Mbb Decomp. Parallel Mmain/Mres
Forward 1 ms 3 ms 5/16 ms
Backward 2 ms 1 ms 11/39 ms
forDelta and the baselines. In both phases, owing to the ef-
fective asymmetric decomposition, the required computation
in the private environment is greatly reduced. Hence, Delta
achieves signiﬁcant speedups compared to running a model
solely in the private environment ( Priv-only ). More-
over, unlike the existing private inference method Slalom
and training method 3Legrace ,Delta obviates the need
for frequent and costly inter-environment communication.
Hence, Delta offers much faster training and inference.
Table 4further lists the running time breakdown for Delta .
We observe that the time on Mbband IR decomposition is
marginal compared to MmainandMres. While the theoret-
ical computation complexity in Mmainis much lower than
Mres, the real running time of Mmainstill dominates the
forward and backward passes. Nevertheless, compared to
Slalom and3LegRace , the time gap between the private
and public environments is signiﬁcantly shrunk.
6.3. Protection against Attacks
This section evaluates Delta against two privacy attacks: a
model inversion attack called SecretRevealer [62], and
a membership inference attack called ML-Leaks [49].
Model Inversion Attacks. SecretRevealer can
leverage prior knowledge, such as blurred images, when
reconstructing training samples. In our case, we allow the
attack to use the quantized residuals as the prior knowledge
when reconstructing images. Other model inversion attacks
[4,12,27,50] need conﬁdence scores/predicted labels from
the whole model, hence they do not apply to our setting.
Following the attack protocol, we ﬁrst take quantized
residuals as a prior knowledge and train a generative model
G. Then, we optimize the latent inputs zto minimize the
loss on the residual model and the discriminator used to pe-
nalize unrealistic images ( LD):z⇤= arg min zLD(G(z)) +
 LMres(G(z)), where  controls the weights of LMres.
Table 5shows Delta ’s performance against the model
inversion attack on ResNet-18/CIFAR-100. We use the struc-
tural similarity index ( SSIM ) to measure the similarity be-
12359
Table 5. Delta ’s performance against SecretRevealer on ResNet-
18/CIFAR-100. Without DP, the attacker gains some prior information.
With noise added, the quality of reconstruction degrades signiﬁcantly.
No DP DP with ✏=1.4 No residuals
SSIM Acc MSSIM Acc MSSIM Acc M
0.18 6.75% 0.09 2.13% 0.08 1%
tween the reconstructed and the original images [ 58]. We
also measure the target model’s accuracy Acc Mgiven the
reconstructed images as inputs. High Acc Mindicates the tar-
get model regarding the reconstructed images by the attacker
close to the original training samples. First, we observe that
given quantized residuals without noise added, the attack
can generate images that have slightly high accuracy on the
target model. This indicates that the attacker can leverage
the residual information during attacks. The observation is
also visually reﬂected in reconstructed samples in Figure
7. Compared to the original training samples, some recon-
structed images reveal the outline of objects (e.g., row 1, col
3). However, with DP, the attacker fails to use the residual
information to generate images similar to the training sam-
ples. In particular, the accuracy of generated images on the
target model is signiﬁcantly reduced and also SSIM between
the reconstructed and the original samples. As a result, the
attacker behaves like the case that generates images without
prior knowledge of the residuals (col 3 in Table 5).
(a) Original samples
 (b) Recon. (no noise)
 (c) Recon. ( ✏=1.4)
Figure 7. Visualization of reconstructed images compared to the original
training samples. With no DP, the model inversion attack recovers certain
features ((b), row 1, col 3). However, when adding noise to the residual, the
generated images are drastically affected.
Membership Inference Attacks. We choose
ML-Leaks , a strong membership inference attack
that infers membership based on conﬁdence scores [ 49].
In our case, we allow the attack to use conﬁdence scores
from Mres. Other membership inference attacks either
required labels [ 13,33,34] or susceptible to noise [ 10]. As
the predicted labels are secure with Delta and residuals
are perturbed, these methods do not apply to our setting.
Following the procedure in [ 49], we feed private and public
samples to the shadow model, and obtain conﬁdence score
vectors from the residual model Mres. We then use the
vectors from public and private datasets to train an attacker
model, which learns to classify whether the conﬁdence score
vector comes from public or private datasets.
Table 6lists the attack performance on CIFAR-100 with
ResNet-18. To align with ML-Leaks ’s protocol, we use 5k
and10ksamples from the training dataset as a public dataset.
The rest of the samples are used as a private dataset to trainTable 6. Membership inference attack on Delta (ResNet-18, CIFAR-100).
The DP mechanism is essential to provide further protection for the residual.
Attack w. 5ksamples Attack w. 10ksamples
No DP ✏=1.4 ✏=1 ✏=1.4
Acc 0.56 0.52 0.60 0.55
F1 0.68 0.57 0.73 0.68
the target model (using Delta ). We train the attack model
for50epochs, with an initial learning rate of 0.1.
We observe that with perturbed residuals, attacks through
Mres’s outputs result in a poor performance compared to
training without noise added. It indicates the DP mechanism
provides further protection for the residuals, and prevents
attackers from inferring membership. Furthermore, as the
number of public samples reduces, the attack performance
degrades further, implying the attack also heavily depends on
prior knowledge via accessing a subset of the target dataset.
Such an observation reveals one critical limitation of the at-
tacks. That is, they need to get access to a subset of the target
data to obtain a good estimate of the target data distributions.
Otherwise, the attack performance collapses. However, in
real scenarios, private data can be completely out of attack-
ers’ reach, rendering the target distribution’s estimation im-
possible. As a result, the attacks can easily fail.
7. Conclusion
We proposed a generic private training and inference frame-
work, Delta , with strong privacy protection, high model
accuracy, and low complexity. Delta decomposes the inter-
mediate representations into asymmetric ﬂows: information-
sensitive and residual ﬂows. We design a new low-
dimensional model to learn the information-sensitive part in
a private environment, while outsourcing the residual part
to a large model in a public environment. A DP mechanism
and binary quantization scheme further protect residuals and
improve inter-environment communication efﬁciency. Our
evaluations show that Delta achieves strong privacy pro-
tection while maintaining model accuracy and computing
performance. While we evaluate Delta in a TEE-GPU en-
vironment, Delta can be generalized to other setups such as
federated settings, with resource-constrained client side as a
private environment, and the server as a public environment.
Acknowledgments
This material is based upon work supported by ONR
grant N00014-23-1-2191, ARO grant W911NF-22-1-0165,
Defense Advanced Research Projects Agency (DARPA)
under Contract No. FASTNICS HR001120C0088 and
HR001120C0160. The views, opinions, and/or ﬁndings
expressed are those of the author(s) and should not be inter-
preted as representing the ofﬁcial views or policies of the
Department of Defense or the U.S. Government.
12360
References
[1]Martin Abadi, Andy Chu, and et al. Deep learning with
differential privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security , pages
308–318, Vienna Austria, 2016. ACM. 1,2
[2]Ahmad Al Badawi and et al. Demystifying bootstrapping in
fully homomorphic encryption. Cryptology ePrint Archive ,
2023, 2023. 4
[3]Ramy E Ali, Jinhyun So, and A Salman Avestimehr. On poly-
nomial approximations for privacy-preserving and veriﬁable
relu networks. arXiv preprint arXiv:2011.05530 , 2020. 2
[4]Shengwei An, Guanhong Tao, and et al. Mirror: Model
inversion for deep learning network with high ﬁdelity. In Pro-
ceedings of the 29th Network and Distributed System Security
Symposium , 2022. 7
[5]Amazon AWS. Aws aws nitro enclaves. https://aws.
amazon.com/ec2/nitro/nitro-enclaves/ , Ac-
cessed: 2023-11-09. 2
[6]Microsoft Azure. Azure conﬁdential computing enclaves.
https://learn.microsoft.com/en-us/azure/
confidential - computing / confidential -
computing-enclaves , Accessed: 2023-11-09. 2
[7]Sara Babakniya, Souvik Kundu, Saurav Prakash, Yue Niu, and
Salman Avestimehr. Revisiting sparsity hunting in federated
learning: Why does sparsity consensus matter? Transactions
on Machine Learning Research , 2023. 1
[8]Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov.
Differential privacy has disparate impact on model accuracy.
InAdvances in Neural Information Processing Systems , Van-
couver, Canada, 2019. Curran Associates, Inc. 1
[9]Borja Balle and et al. Privacy ampliﬁcation by subsampling:
Tight analyses via couplings and divergences. Advances in
Neural Information Processing Systems , 31, 2018. 1
[10] Nicholas Carlini and et al. Membership inference attacks
from ﬁrst principles. In 2022 IEEE Symposium on Security
and Privacy (SP) , pages 1897–1914. IEEE, 2022. 8
[11] Kumar Chellapilla, Sidd Puri, and et al. High performance
convolutional neural networks for document processing. In
Tenth international workshop on frontiers in handwriting
recognition . Suvisoft, 2006. 1
[12] Si Chen, Mostafa Kahla, and et al. Knowledge-enriched
distributional model inversion attacks. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 16178–16187, Virtual, 2021. IEEE. 7
[13] Christopher A Choquette-Choo and et al. Label-only mem-
bership inference attacks. In International conference on
machine learning , pages 1964–1974, Virtual, 2021. PMLR. 8
[14] Victor Costan and Srinivas Devadas. Intel sgx explained.
Cryptology ePrint Archive , 2016. 7,4
[15] Jia Deng, Wei Dong, and et al. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on
computer vision and pattern recognition , pages 248–255,
Florida, US, 2009. IEEE. 6
[16] Jacob Devlin, Ming-Wei Chang, and et al. Bert: Pre-training
of deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805 , 2018. 3[17] Cynthia Dwork, Aaron Roth, et al. The algorithmic foun-
dations of differential privacy. Foundations and Trends ®in
Theoretical Computer Science , 9(3–4):211–407, 2014. 6,1
[18] Zahra Ghodsi, Tianyu Gu, and Siddharth Garg. Safetynets:
Veriﬁable execution of deep neural networks on an untrusted
cloud. Advances in Neural Information Processing Systems ,
30, 2017. 2
[19] Lucjan Hanzlik, Yang Zhang, and et al. Mlcapsule: Guarded
ofﬂine deployment of machine learning as a service. In Pro-
ceedings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 3300–3309, 2021. 1,4
[20] Hanieh Hashemi, Yongqin Wang, and Murali Annavaram.
Darknight: An accelerated framework for privacy and in-
tegrity preserving deep learning using trusted hardware. In
MICRO-54: 54th Annual IEEE/ACM International Sympo-
sium on Microarchitecture , pages 212–224, 2021. 1
[21] Kaiming He, Xiangyu Zhang, and et al. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
770–778. IEEE, 2016. 6
[22] Ehsan Hesamifard, Hassan Takabi, and et al. Privacy-
preserving machine learning as a service. Proc. Priv. En-
hancing Technol. , 2018(3):123–142, 2018. 4
[23] Yani Ioannou and et al. Training CNNs with low-rank ﬁlters
for efﬁcient image classiﬁcation. International Conference
on Learning Representations (ICLR) , 2016. 5
[24] Max Jaderberg, Andrea Vedaldi, and et al. Speeding up con-
volutional neural networks with low rank expansions. British
Machine Vision Conference (BMVC) , 2014. 5
[25] Anil K Jain. Fundamentals of digital image processing .
Prentice-Hall, Inc., 1989. 3
[26] Patrick Jauernig, Ahmad-Reza Sadeghi, and Emmanuel Stapf.
Trusted execution environments: properties, applications, and
challenges. IEEE Security & Privacy , 18(2):56–60, 2020. 4
[27] Mostafa Kahla, Si Chen, Hoang Anh Just, and Ruoxi Jia.
Label-only model inversion attacks via boundary repulsion.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 15045–15053, 2022. 7
[28] Gautam Kamath. Approximate differential privacy. http://
www.gautamkamath.com/CS860notes/lec5.pdf ,
Accessed: 2024-03-29. 6,1
[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images, 2009. 6
[30] Jaewoo Lee and et al. Scaling up differentially private deep
learning with fast per-example gradient clipping. Proceedings
on Privacy Enhancing Technologies , 2021(1), 2021. 2
[31] Joon-Woo Lee and et al. Privacy-preserving machine learning
with fully homomorphic encryption for deep neural network.
IEEE Access , 10:30039–30054, 2022. 4
[32] Ping Li, Jin Li, Zhengan Huang, and et al. Multi-key privacy-
preserving deep learning in cloud computing. Future Genera-
tion Computer Systems , 74:76–85, 2017. 4
[33] Zheng Li and Yang Zhang. Membership leakage in label-only
exposures. In Proceedings of the 2021 ACM SIGSAC Con-
ference on Computer and Communications Security , pages
880–895, 2021. 8
12361
[34] Zhaobo Lu, Hai Liang, and et al. Label-only membership
inference attacks on machine unlearning without dependence
of posteriors. International Journal of Intelligent Systems ,3 7
(11):9424–9441, 2022. 8
[35] Brendan McMahan, Eider Moore, and et al. Communication-
efﬁcient learning of deep networks from decentralized data. In
Artiﬁcial intelligence and statistics , pages 1273–1282. PMLR,
2017. 1
[36] Midjourney. Midjourney. https://www.midjourney.
com/home , Accessed: 2023-04-21. 1
[37] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
Efﬁcient estimation of word representations in vector space.
arXiv preprint arXiv:1301.3781 , 2013. 3
[38] Fatemehsadat Mireshghallah, Mohammadkazem Taram, and
et al. Shredder: Learning noise distributions to protect infer-
ence privacy. In Proceedings of the Twenty-Fifth International
Conference on Architectural Support for Programming Lan-
guages and Operating Systems , pages 3–18, 2020. 1
[39] Fan Mo, Ali Shahin Shamsabadi, and et al. Darknetz: towards
model privacy at the edge using trusted execution environ-
ments. In Proceedings of the 18th International Conference
on Mobile Systems, Applications, and Services , pages 161–
174, 2020. 1
[40] Krishna Giri Narra, Zhifeng Lin, and et al. Origami inference:
Private inference using hardware enclaves. In 2021 IEEE 14th
International Conference on Cloud Computing (CLOUD) ,
pages 78–84. IEEE, 2021. 1
[41] Yue Niu, Ramy E Ali, and Salman Avestimehr. 3legrace:
Privacy-preserving dnn training over tees and gpus. Proceed-
ings on Privacy Enhancing Technologies , 4:183–203, 2022.
1,2,3,4,6,7
[42] Yue Niu, Saurav Prakash, Souvik Kundu, Sunwoo Lee, and
Salman Avestimehr. Overcoming resource constraints in fed-
erated learning: Large models can be trained with only weak
clients. Transactions on Machine Learning Research , 2023.
1
[43] Nvidia. Nvidia conﬁdential computing. https://www.
nvidia.com/en-us/data-center/solutions/
confidential-computing/ , Accessed: 2023-04-21.
4
[44] OpenAI. Introducing chatgpt. https://openai.com/
blog/chatgpt , Accessed: 2023-04-21. 1
[45] OpenAI. March 20 chatgpt outage: Here’s what hap-
pened. https://openai.com/blog/march-20-
chatgpt-outage , Accessed: 2023-04-21. 1
[46] Saurav Prakash, Hanieh Hashemi, and et al. Secure
and fault tolerant decentralized learning. arXiv preprint
arXiv:2010.07541 , 2020. 4
[47] Do Le Quoc, Franz Gregor, and et al. Securetf: A secure ten-
sorﬂow framework. In Proceedings of the 21st International
Middleware Conference , pages 44–59, 2020. 1,2,4
[48] Sina Sajadmanesh, Ali Shahin Shamsabadi, Aur ´elien Bellet,
and Daniel Gatica-Perez. Gap: Differentially private graph
neural networks with aggregation perturbation. In USENIX
Security 2023-32nd USENIX Security Symposium , 2023. 6
[49] Ahmed Salem, Yang Zhang, and et al. Ml-leaks: Model and
data independent membership inference attacks and defenseson machine learning models. In Network and Distributed
Systems Security (NDSS) Symposium 2019 , 2019. 7,8
[50] Lukas Struppek, Dominik Hintersdorf, and et al. Plug and
play attacks: Towards robust and ﬂexible model inversion
attacks. In International Conference on Machine Learning ,
pages 20522–20545. PMLR, 2022. 7
[51] Hassan Takabi, Ehsan Hesamifard, and Mehdi Ghasemi. Pri-
vacy preserving multi-party machine learning with homo-
morphic encryption. In 29th Annual Conference on Neural
Information Processing Systems (NIPS) , 2016. 1,4
[52] Justin Thaler. Time-optimal interactive proofs for circuit
evaluation. In Annual Cryptology Conference , pages 71–89.
Springer, 2013. 2
[53] Chandra Thapa, Mahawaga Arachchige Pathum Chamikara,
and Seyit A Camtepe. Advancements of federated learn-
ing towards privacy preservation: from federated learning to
split learning. Federated Learning Systems: Towards Next-
Generation AI , pages 79–109, 2021. 4
[54] Chandra Thapa, Pathum Chamikara Mahawaga Arachchige,
and et al. Splitfed: When federated learning meets split
learning. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , pages 8485–8493, 2022. 4
[55] Florian Tramer and Dan Boneh. Slalom: Fast, veriﬁable and
private execution of neural networks in trusted hardware. In
International Conference on Learning Representations , 2019.
1,2,7
[56] Aravind Vasudevan, Andrew Anderson, and David Gregg.
Parallel multi channel convolution using general matrix mul-
tiplication. In 2017 IEEE 28th international conference on
application-speciﬁc systems, architectures and processors
(ASAP) , pages 19–24. IEEE, 2017. 1
[57] Praneeth Vepakomma, Otkrist Gupta, and et al. Split learning
for health: Distributed deep learning without sharing raw
patient data. arXiv preprint arXiv:1812.00564 , 2018. 4
[58] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P
Simoncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing ,
13(4):600–612, 2004. 8
[59] Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond
a good init: Exploring better solution for training extremely
deep convolutional neural networks with orthonormality and
modulation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 6176–6185, 2017.
5
[60] Da Yu, Huishuai Zhang, and et al. Do not let privacy overbill
utility: Gradient embedding perturbation for private learning.
arXiv preprint arXiv:2102.12677 , 2021. 2
[61] Tianwei Zhang, Zecheng He, and Ruby B Lee. Privacy-
preserving machine learning through data obfuscation. arXiv
preprint arXiv:1807.01860 , 2018. 1
[62] Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo
Li, and Dawn Song. The secret revealer: Generative model-
inversion attacks against deep neural networks. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 253–261, 2020. 7
[63] Wei Zhou, Yue Niu, and et al. Sensitivity-oriented layer-
wise acceleration and compression for convolutional neural
network. IEEE Access , 7:38264–38272, 2019. 5
12362
