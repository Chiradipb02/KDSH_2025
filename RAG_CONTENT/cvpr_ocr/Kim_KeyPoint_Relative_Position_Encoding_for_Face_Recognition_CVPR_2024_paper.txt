KeyPoint Relative Position Encoding for Face Recognition
Minchul Kim* Yiyang Su* Feng Liu* Anil Jain* Xiaoming Liu*
{kimminc2; suyiyan1; liufeng6; jain; liuxm}@msu.edu
Abstract
In this paper, we address the challenge of making ViT
models more robust to unseen afﬁne transformations. Such
robustness becomes useful in various recognition tasks such
as face recognition when image alignment failures occur. We
propose a novel method called KP-RPE, which leverages key
points (e.g. facial landmarks) to make ViT more resilient to
scale, translation, and pose variations. We begin with the
observation that Relative Position Encoding (RPE) is a good
way to bring afﬁne transform generalization to ViTs. RPE,
however, can only inject the model with prior knowledge that
nearby pixels are more important than far pixels. Keypoint
RPE (KP-RPE) is an extension of this principle, where the
signiﬁcance of pixels is not solely dictated by their proximity
but also by their relative positions to speciﬁc keypoints within
the image. By anchoring the signiﬁcance of pixels around
keypoints, the model can more effectively retain spatial re-
lationships, even when those relationships are disrupted by
afﬁne transformations. We show the merit of KP-RPE in face
and gait recognition. The experimental results demonstrate
the effectiveness in improving face recognition performance
from low-quality images, particularly where alignment is
prone to failure. Code and pre-trained models are available .
1. Introduction
Geometric alignment has shown to be highly effective for
certain computer vision problems, such as face, body and gait
recognition [ 11,12,25,30,33,34,37,40,43,48–50,59,69,
72,82,83,87,88]. Alignment is the process of transforming
input images, to a consistent and standardized form, often by
scaling, rotating, and translating. This standardization helps
recognition models learn the underlying patterns and features
more effectively. As a result, many state-of-the-art (SoTA)
face recognition models [ 11,30,49,69] rely on well-aligned
datasets [ 10,11,22,91] to achieve high accuracy.
Fig. 1shows a toy example with a training dataset
MNIST [ 14] and test set AffNIST [ 57] which is in unseen
afﬁne transformation of MNIST. Using a shallow ViT [ 15]
*Michigan State University, East Lansing, MI 48824, USATrain Set(MNIST)Test Set(AffNIST)
Toy Example for Measuring Affine Transformation Robustness
Keypoints*Abs-PEAbs-PE*RPEiRPEKP-RPEAffNIST Test Accuracy %Unseen Affine Transform KP-RPE offers robustness to unseen affine transforms by using keypoints.
Figure 1. Toy Example illustrating how different Position Embed-
dings impact the ViT’s robustness to unseen afﬁne transforms. Abs-
PE refers to the learnable Absolute Position Embedding. RPE and
iRPE refers to Relative Position Embedding adopted to ViT [ 28,74].
Keypoints in MNIST is arbitrarily deﬁned to be the four corners of
a box that covers a digit. Abs-PE* is drawing the keypoints onto
the input image. KP-RPE uses the keypoints to adjust the RPE.
model, one can easily achieve 98.1%accuracy in the MNIST
test set. However, in AffNIST, ViT with the original Ab-
solute Position Embedding obtains only 77.27% accuracy.
Such a sharp decrease in performance with unseen afﬁne
transform causes problems in applications that rely on accu-
rate input alignments.
In face recognition, alignment can be imperfect, espe-
cially in low-quality images where accurate landmark detec-
tion is difﬁcult [ 10,39]. Thus, images with low resolution or
taken in poor lighting may result in misalignment during test-
ing. Given the interplay between alignment and recognition,
it becomes crucial to proactively handle potential alignment
failures, which often result from, e.g., low-quality images.
In other words, there is a need for a recognition model that
is robust to scale, rotation, and translation variations.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
244
QueryKeyRelative Position EmbeddingImage Space
a) Eye to Nose RelationshipScale ChangePose Change
b) Eye to Mouth Relationshipc) Skin to Skin RelationshipRPE: relationship based on distance in the image plane.
d) Eye to Mouth RelationshipKeypoint (     ) DependentKP-RPEProblem: query key relationship is invariant to scale, pose change
="(     ,    ,     )
="(     ,    )
="(     ,    )
="(     ,    )!!"	for RPE(learned attention offset)Unique !!" for different   ==Figure 2. Illustration of RPE [ 60] and proposed KP-RPE. The blue arrow represents the learned attention offset Bijbetween a query iand
keyjof attention in RPE. The query-key relationship at the same iandjshould represent different relationships as the scale or pose change.
ButBijdoes not change in RPE. KP-RPE addresses this issue by incorporating the distance to the keypoints when calculating the learned
attention offset in RPE.
We revisit the Relative Position Encoding (RPE) concept
used in ViT [ 15] and ﬁnd that RPE can be useful for introduc-
ing afﬁne transform robustness. RPE [ 60] enables the model
to capture the relative spatial relationships among regions
of an image, learning the positional dependencies without
relying on absolute coordinates. As shown in Fig. 1, adding
RPE to ViT increases the performance in AffNIST. With
RPE [ 60], queries and keys of self-attention [ 68] at closer
distances can be assigned different attention weights com-
pared to those at a greater distance. While RPE allows the
model to exploit relative positions, it has a limitation: even
if an image changes in terms of scaling, shifting, or orienta-
tion, the signiﬁcance of the key-query position in RPE stays
the same. This static behavior is illustrated in Figs. 2a)-c).
Notably, the key-query relationship is the same regardless of
the corresponding pixels’ semantic meaning changes.
We hypothesize that an RPE which dynamically adapts
based on image keypoints, such as facial landmarks, could
improve the model’s comprehension of spatial relationships
in the image. By leveraging the spatial relationships with
respect to these keypoints, the model can adapt to variations
in scale, rotation, and translation, resulting in a more robust
recognition system capable of handling both aligned and mis-
aligned datasets. Fig. 2d) highlights a keypoint-dependent
query-key relationship.
To this end, we introduce KeyPoint RPE (KP-RPE), a
method that dynamically adapts the spatial relationship in
ViT based on the keypoints present in the image. Our experi-
ments demonstrate that incorporating KP-RPE into ViT sig-
niﬁcantly enhances the model’s robustness to misaligned test
datasets while maintaining or even improving performance
on aligned test datasets. We show the usage of KP-RPE
in face recognition and gait recognition as the inputs share
the same topology (face or body) that allows the keypoints
to be deﬁned. Finally, KP-RPE is an order of magnitude
faster than iRPE [ 74], a widely used RPE that depends onthe image content.
In summary, the contributions of this paper include:
•The insight that RPE (or its variants) can improve the
robustness of ViT to unseen afﬁne transformations.
•The development of Keypoint RPE (KP-RPE), a novel
method that dynamically adapts the spatial relationship in
Vision Transformers (ViT) based on the keypoints in the
image, signiﬁcantly enhancing the model’s robustness to
misaligned test datasets while maintaining or improving
performance on aligned test datasets.
•Comprehensive experimental validation demonstrating the
effectiveness of our proposed KP-RPE, showcasing its po-
tential for advancing the ﬁeld of recognition by bringing
model’s robustness to geometric transformation. We im-
prove the recognition performance across unconstrained
face datasets such as TinyFace [ 7] and IJB-S [ 29] and even
non-face datasets such as Gait3D [ 18,87].
2. Related Works
Relative Position Encoding in ViT Relative Position En-
coding (RPE) is ﬁrst introduced by Shaw et al.[60] as a
technique for encoding spatial relationships between differ-
ent elements in a sequence. By adding relative position en-
codings into the queries and keys, the model can effectively
learn positional dependencies without relying on absolute co-
ordinates. Subsequent works, such as those by Dai et al.[9]
and Huang et al.[28], reﬁne and expand upon the concept
of RPE, demonstrating its effectiveness in natural language
processing (NLP) tasks.
The adoption of RPE in Vision Transformers [ 15] has
been explored by several researchers. For instance, Ra-
machandran et al.[53] propose a 2D RPE method that com-
putes the x,ydistance in an image plane separately to in-
clude directional information. A notable RPE method in ViT
is iRPE [ 74], which considers directional relative distance
modeling as well as the interactions between queries and
245
relative position encodings in a self-attention mechanism.
Despite the success of these RPE methods in various vi-
sion tasks, they do not speciﬁcally address the challenges
associated with scale, rotation, and translation variations
in face recognition applications. This shortcoming high-
lights the need for RPE methods that can better handle these
variations, which are common in real-world low-quality
face recognition scenarios. To address this, we propose
KP-RPE, which incorporates keypoint information during
the network’s feature extraction, signiﬁcantly enhancing the
model’s ability to generalize across afﬁne transformations.
Face Recognition with Low-Quality Images Recent FR
models [ 11,27,43,49,69] have achieved high perfor-
mance on datasets with discernable facial attributes, such
as LFW [ 25], CFP-FP [ 59], CPLFW [ 88], AgeDB [ 50],
CALFW [ 89], IJB-B [ 72], and IJB-C [ 48]. However, un-
constrained scenarios like surveillance or low-quality videos
present additional challenges. Datasets in this setting, such
as TinyFace [ 7] and IJB-S [ 29], contain low-quality images,
where detecting facial landmarks and achieving proper align-
ment becomes increasingly difﬁcult. This adversely affects
existing FR models that rely on well-aligned inputs.
Several studies [ 20,21,26,30,31,36,41,54,63,80,
84,85] tackle recognition with low-quality imagery. Par-
ticularly, AdaFace [ 30] introduces an image quality adap-
tive loss function that reduces the inﬂuence of low-quality
or unidentiﬁable samples. A-SKD [ 63] employs teacher-
student distillation to focus on similar areas regardless of
image resolution. But, these models, which are trained on
aligned training sets, do not tackle the challenges associated
with misaligned inputs in real-world situations. In contrast,
KP-RPE adjusts spatial relationships within ViT based on
image keypoints, allowing the model to better generalize
even when alignment is unsuccessful in low-quality imagery.
Keypoints and Spatial Reasoning Keypoint detection,
often associated with landmarks, has been fundamental in
various vision tasks such as human pose estimation [ 5,51],
face landmark detection [ 4,35,65,81], and object localiza-
tion [ 52]. These keypoints serve as representative points that
capture the essential structure or layout of an object, facili-
tating tasks like alignment, recognition, and even animation.
Face landmark detection is commonly carried out along-
side face detection. MTCNN [ 81] is a widely-used method
for combined face detection and facial landmark localiza-
tion, utilizing cascaded CNNs (P-Net, R-Net, and O-Net)
that collaborate to detect faces and landmarks in an image.
RetinaFace [ 10], on the other hand, is a single-stage detec-
tor [38,42] based landmark localization algorithm, demon-
strating strong performance when trained on the annotated
WiderFace [ 78] dataset. TinaFace [ 90] further enhances de-
tection capabilities by incorporating SoTA generic objectdetection algorithms. MTCNN and RetinaFace are often
used for aligning face datasets.
Recent advances in keypoint detection techniques, par-
ticularly using deep neural networks, have led to us-
ing keypoints to improve the performance of recognition
tasks [ 64,77]. For instance, [ 23] proposes a keypoint-based
pooling mechanism and shows promising results in skeleton-
based action recognition and spatio-temporal action localiza-
tion tasks. Albeit its beneﬁt, many models including ViTs do
not have pooling mechanisms. KP-RPE is the ﬁrst attempt
at incorporating keypoints into the RPE which can be easily
inserted into ViT models.
3. Proposed Method
3.1. Background
Self-Attention Self-attention is a crucial component of
transformers [ 68], which is a popular choice for a wide
range of NLP tasks. ViT [ 15] applies the same self-attention
mechanism to images, treating images as sequences of non-
overlapping patches. The self-attention mechanism in Trans-
formers calculates attention weights based on the compati-
bility between a query and a set of keys. Given a set of input
vectors, the Transformer computes query ( Q), key ( K), and
value ( V) matrices through linear transformations:
Qi=xiWQ,Kj=xjWK,Vj=xjWV,(1)
where xiis the i-th input vector, and WQ,WK, and WV
are learnable weight matrices.
The self-attention mechanism computes attention weights
as the dot product between the query and key vectors, fol-
lowed by a softmax normalization:
eij=QiKT
jpdk,a ij=exp(eij)PN
j=1exp(eij), (2)
where dkis the dimension of the key vectors. Finally, the
output matrix Yis computed as the product of the attention
weight matrix and the value matrix: Yi=PN
j=1aijVj.
Absolute Position Encoding Transformers are inherently
order invariant, as their self-attention mechanism does not
consider input token positions. To address this, absolute
position encoding is introduced [ 19,68], which adds ﬁxed,
learnable positional embeddings to input tokens:
x0
i=xi+PE(i), (3)
where x0
iis the updated input token with positional informa-
tion,xiis the original input token, and PE(i)is the positional
encoding for the i-th position. These embeddings, generated
using sinusoidal functions or learned directly, enable the
model to capture the absolute positions of elements.
246
Relative Position Encoding (RPE) RPE, introduced by
Shaw et al.[60] and reﬁned by Dai et al.[9] and Huang
et al.[28], encodes relative position information, essential
for tasks focusing on input element relationships. Unlike
absolute position encoding, RPE considers query-key inter-
actions based on sequence-relative distances. The modiﬁed
self-attention calculation for RPE is:
e0
ij=(Qi+RQ
ij)(Kj+RK
ij)T
pdk,Yi=nX
j=1aij(Vj+RV
ij).
(4)
Here, RQ
ij,RK
ij, and RV
ijare relative position encoding be-
tween the i-th query and j-th key with shape Rdz. Each Ris
a learnable matrix of RK⇥dz, where Ri,jcorresponds to the
relative position encoding for distance d(i, j)=kandKis
the maximum possible value of d(i, j). To obtain relative po-
sition encoding, we index the Rmatrix using the computed
distance R[d(i, j)]. Common choices for dare quantized
Euclidean distance, separate x, ycross distance [ 53]. [74]
uses a quantized x, yproduct distance, which encodes di-
rection information. Note, query location iis a 2D point
(ix,iy). Fig. 3a) and b) illustrate the distance between iand
all possible jwith different distance functions. For KP-RPE,
we modify [ 74] and allow the RPE to be keypoint dependent.
3.2. Keypoint Relative Position Encoding
Building upon the general formulation of [ 74], we begin
with the following RPE formulation:
e0
ij=QiKjT+Bijpdk. (5)
Here, Bijis a scalar that adjusts the attention matrix based
on the query and key indices i, j. Assuming a set of key-
points P2RNL⇥2is available for each x, our goal is to
make Bijdependent on P. For face recognition, Pis the ﬁve
facial landmarks (two eyes, nose, mouth tips). For gait recog-
nition, Pis 17 points from the joint locations of skeleton
predictions. For the MNIST toy example, Pis ﬁve keypoints
from the four corners and the center of the minimum cover
box of a foreground image. As such Pcan be deﬁned for
objects with shared topology.
The novelty of KP-RPE lies in the design of Bij. Since
Bij=W[d(i, j)]2R1, (6)
comprises of a learnable table Wand a distance function
d(i, j), we can make Word(i, j)depend on the keypoints.
At a ﬁrst glance, d(i, j,P), conditioning the distance on P
seems plausible. However, we ﬁnd that it leads to inefﬁcien-
cies, as distance caching, which is precomputing d(i, j)for a
given input size, is only feasible when d(i, j)is independent
of the input. Therefore, we propose an alternative where the
bias matrix itself, W, is a function of P:
Bij=f(P)[d(i, j)]. (7)
a) iRPE: Euclidean Distanceb) iRPE: Product Distance
c) KP-RPE: Product Distance (Two different keypoints)Figure 3. Depiction of key-query combinations in an image, given
a query location i=( 7 ,7)(?). Distinct colors represent varying
attention offset values in RPE based on the distance between i
andj. We are showing Bi=(7,7),jfor all j2(14⇥14). a) The
distance function is a quantized Euclidean distance. b) Product
distance proposed in iRPE accounts for direction. c) We adopt b)
and allow Bi,jto vary based on keypoint locations ( •).
We propose three variants of f(P)building up from the
simplest solution.
Absolute f(P).LetP2RNL⇥2be the normalized key-
points between 0 and 1. First, the simplest way to model
the indexing table is to linearly map Pto the desired shape.
f(P)=P0WLwhere P02R1⇥(2NL)is reshaped keypoints
PandWL2R(2NL)⇥Kis a learnable matrix. Kis the max-
imum distance value in d(i, j). For each distance between i
andj, we learn a keypoint adaptive offset value. However,
thisf(P)only works with the absolute position information
ofPand the relative distance between iandj. It is missing
the relative distance between Pand ( i,j).
Relative f(P).To improve, f(P)can be adjusted to work
with the position of keys and queries relative to the keypoints.
In other words, so that the query-key relationship in Bijde-
pends on the query-landmark relationship. To achieve this,
we generate a mesh grid M2RN⇥2of patch locations con-
taining all possible combinations of ixandiy.Nrepresents
the number of patches. We then compute the element-wise
difference between the normalized grid and keypoints Pto
247
Keypoints !(5×2)!!!"
Mesh Grid &2D ',) Grid(N×5×2)Broadcasted Difference
…(N×K)!!"=0"=1"=2"='
!(,*=#$[i,di,j]"#
b) Model OverviewAdd & NormFeed ForwardAdd & NormMulti-headSelf-AttentionN-layer Transformer BlocksKP-RPEInput Embedding-(.)/0: distance between 2,3
!=0!=1!=+2: attention query location3: attention key location(N×2)!=0!=1
a) KP-RPE detailed diagram!(,* is keypoint dependent.Figure 4. a) Illustration of KP-RPE. First a mesh grid Mand an image-speciﬁc keypoints Pare generated. Then the broadcasted difference
Dis calculated, and we linearly map Dtof(P). Finally for a given i, j, we can ﬁnd the Bij=f(P)[i, d(i, j)]), which is used to adjust
the attention map in self-attention. b) Backbone contains multiple transformer blocks followed by an MLP for classiﬁcation. KP-RPE is
used where multi-head attention modules exist. KP-RPE is efﬁcient as f(P)is computed once.
obtain a grid of i, jrelative to the keypoints:
D=Expand (M,dim=1)  Expand (P,dim=0),(8)
where Dis the broadcasted tensor difference of shape
RN⇥NL⇥2. Finally, we reshape Dand linearly project it
with WL. Speciﬁcally,
D0=Reshape (D)2RN⇥(2NL)(9)
f(P)=D0WL2RN⇥K(10)
Bij=f(P)[i, d(i, j)]2R1. (11)
In other words, the offset value Bijis determined with
respect to the positions of the keypoints and is unique for
each query location. This approach allows for more expres-
sive control of the query-key relationships with the keypoint
locations. An illustration of this is shown in Fig. 4.
Multihead Relative f(P).Lastly, we can further enhance
our method by tailoring the query-keypoint relationship
for each head in the attention mechanism. When there
areHheads, we simply expand the dimension of WLto
WL2R(2NL)⇥HK. By reshaping f(P), we obtain f(P)h
for each head. Furthermore, considering the multiple self-
attentions in ViT which entails multiple RPEs, we can in-
dividualize f(P)for each self-attention by additionally in-
creasing the dimension of WLtoWL2R(2NL)⇥NdHK,
where Ndrepresents the transformer’s depth. Since f(P)
is computed only once per forward pass, this modiﬁcation
introduces negligible computational overhead compared to
other operations. In Sec. 4.2, we evaluate and compare the
various KP-RPE versions (basic, relative keypoint, multiple
relative keypoint), demonstrating the superior performance
of the multiple relative keypoint approaches.4. Face Recognition Experiments
4.1. Datasets and Implementation Details
To validate the efﬁcacy of KP-RPE, we train our model using
aligned face training data and evaluate on three distinct types
of datasets: 1) aligned face data, 2) intentionally unaligned
face data, and 3) low-quality face data containing misaligned
images. For the evaluation, aligned face datasets include
CFPFP [ 59], AgeDB [ 50], and IJB-C [ 48]. For unaligned
face data, we intentionally use the raw CFPFP [ 59] and
IJB-C [ 48] datasets without aligning them. Raw images,
as provided by their respective creators, are equivalent to
images cropped based on face detection bounding boxes.
Lastly, we assess the model’s robustness on low-quality face
datasets, speciﬁcally TinyFace [ 7] and IJB-S [ 29], which
are prone to alignment failures. This comprehensive setup
enables us to examine the effectiveness of our proposed
method across diverse data conditions.
The training datasets MS1MV2 [ 11] MS1MV3 [ 13] and
WebFace4M [ 91] are released as aligned and resized to 112⇥
112by RetinaFace [ 10] whose backbone is ResNet50 model
trained on WiderFace [ 78]. For keypoint detection in KP-
RPE, we also use RetinaFace [ 10] but with lighter backbone
MobileNetV2 for faster inference. Given the sensitivity of
ViTs to hyperparameters, we report the exact settings for
learning rate, weight decay, and other parameters in the
supplementary material. For ablation dataset, we take the
MS1MV2 subset dataset as used in [ 30].
Following the training conventions of [ 30,67], we adopt
RandAug [ 8], repeated augmentation [ 24], random resized
crop, and blurring. We utilize the AdaFace [ 30] loss func-
tion to train all models. For ablation, we employ ViT-small,
while for SoTA comparisons, we use ViT-base models. The
AdamW [ 46] optimizer and Cosine Learning Rate sched-
248
Table 1. Ablation of RPE on ViT-small. Aligned is the standard protocol with raw face images (detector bounding box) aligned by
RetinaFace [ 10] and resized to 112⇥112. Unaligend takes the raw face images and simply resizes it to 112⇥112. Aligned setting always
shows better performances and Unaligned is for simulating alignment failure. Low Quality Aligned dataset may have alignment failures.
MethodLow Quality Aligned Dataset High Quality Aligned Dataset High Quality Unaligned Dataset
TinyFace [ 7] IJB-S [ 29] CFPFP [ 59] IJB-C [ 48] CFPFP [ 59] IJB-C [ 48]
Rank- 1 Rank- 5 Rank- 1 Rank- 5 Veriﬁcation TAR@0.01% Veriﬁcation TAR@0.01%
ViT 68.24 72 .96 59 .60 68 .31 96 .11 92 .22 72 .81 21 .62
ViT + iRPE 69.05 73 .10 62 .49 70 .50 97.01 92.72 77 .91 34 .73
ViT+KP-RPE 69.88 74 .25 63 .44 72 .04 96.60 94.20 93 .56 91 .85
Table 2. Ablation of KP-RPE with three different formulations of keypoint dependent RPE tables f(P). The sharp increase in Unaligned
setting shows the robustness to unseen afﬁne transform manifests with Relative f(P). Multihead f(P)further improves the performance.
MethodLow Quality Aligned Dataset High Quality Aligned Dataset High Quality Unaligned Dataset
TinyFace [ 7] IJB-S [ 29] CFPFP [ 59] IJB-C [ 48] CFPFP [ 59] IJB-C [ 48]
Rank- 1 Rank- 5 Rank- 1 Rank- 5 Veriﬁcation TAR@0.01% Veriﬁcation TAR@0.01%
KP-RPE Absolute f(P)6 8 .11 72 .42 9 .97 69 .13 96 .51 90 .96 68 .09 14 .91
KP-RPE Relative f(P)6 9 .42 73 .71 62 .51 70 .77 96.74 94 .28 89.70 85 .22
KP-RPE MultiHead f(P) 69.88 74 .25 63 .44 72 .04 96.60 94 .20 93.56 91 .85
uler [ 45,73] are used. In WebFace4M trained models, we
adopt PartialFC [ 1,2] to reduce the classiﬁer’s dimension.
4.2. Ablation Analysis
Row 1in Tab. 1shows results on the baseline ViT. Row 2and
3show results on the baseline ViT with iRPE and our pro-
posed KP-RPE. KP-RPE demonstrates a substantial perfor-
mance improvement on unaligned and low-quality datasets,
without compromising performance on aligned datasets. Last
row highlights the difference between ViT and ViT+KP-RPE.
Also, Fig. 5shows the sensitivity to the afﬁne transformation,
i.e., how the performance changes when one interpolates the
afﬁne transformation from the face detection images to the
aligned images in CFPFP dataset.
Tab.2further investigates the effect of modiﬁcations to
KP-RPE. By making KP-RPE dependent on the difference
between the query and keypoints (row 2), we observe a
signiﬁcant improvement in unaligned dataset performance.
Also, by allowing unique mapping for each head and module
in ViT (row 3), we achieve a further improvement. In other
words, more expressive KP-RPE is beneﬁcial for learning
complex RPE that depends on the keypoints of an image.
Overall, the ablation study highlights the necessity of each
component in KP-RPE and the effectiveness of KP-RPE in
enhancing the robustness of face recognition models, partic-
ularly with unaligned and low-quality datasets.
4.3. Computation Analysis
In this section, we analyze the computational efﬁciency of
our proposed KP-RPE in terms of FLOPs, throughput, and
the number of parameters. Tab. 3shows that KP-RPE is
highly efﬁcient, with only a small increase in the computa-
tional cost (FLOPs) compared to the backbone: 0.02GFLOP
increase for ViT Small and 0.07 GFLOP increase for ViTFigure 5. Plot of Veriﬁcation Accuracy in CFPFP [ 59]. On the
X-axis, we interpolate the afﬁne transformation from raw data
(Detection Image) to canonical alignment (Alignment Image). Note
KP-RPE is robust to afﬁne transformations, while all models have
been trained on the aligned image dataset.
Base (ViT vs ViT+KP-RPE). Notably, KP-RPE is consider-
ably more efﬁcient than iRPE, which incurs an increase of
0.71GFLOP for ViT Small and 1.42GFLOP for ViT Base.
Considering training throughput, which factors in com-
putation time during training (with backpropagation), KP-
RPE’s efﬁciency is more pronounced. It only reduces
throughput by 9.15% for ViT Small and 16.44% for ViT
Base, as opposed to iRPE’s larger decrease. Also, we show
the GFLOP and throughput with the landmark detection time
included. Landmark detection time is negligible compared
to the total feature extraction time.
Also, our method introduces a negligible increase in the
number of parameters: just 0.05M for ViT Small and 0.21M
for ViT Base. Hence, incorporating KP-RPE into the model
achieves enhanced performance without a substantial rise in
computational cost or model complexity.
249
Table 3. Computation resource comparison. GFLOP refers to Giga Floating Operating per Second. We measure it as [ 55]. Throughput
refers to the number of images processed per second during the train/eval iteration.
GFLOP  in GFLOPEval
ThroughputTrain
Throughput% in Train
Throughput# Param
IResNet50 12.62 - 1432 .72img/s 337.93img/s - 43.59M
ViT Small 17.42 1 1303 .15imgs/s 333.17img/s 1 95.95M
ViT Small + iRPE 18.13 1+0.71 832 .12imgs/s 186.55img/s 1-44.01% 96.07M
ViT Small + KP-RPE 17.44 1+0.02 1145 .90imgs/s 302.70img/s 1-9.15% 96.00M
ViT Small + KP-RPE (+ Ldmk) 17.58 1+0.16 1085 .22imgs/s 302.70img/s 1-9.15% 96.49M
IResNet101 24.19 - 773.12imgs/s 189.74img/s - 65.15M
ViT Base 24.83 2 644.10imgs/s 162.94img/s 2 114.87M
ViT Base + iRPE 26.25 2+1.42 337 .32imgs/s 79.40img/s 2-51.27% 114.98M
ViT Base + KP-RPE 24.90 2+0.07 502 .57imgs/s 136.15img/s 2-16.44% 115.08M
ViT Base + KP-RPE (+ Ldmk) 25.04 2+0.21 489 .37imgs/s 136.15img/s 2-16.44% 115.56M
4.4. Comparison with SoTA Methods
In this section, we position ViT+KP-RPE, against SoTA
face recognition methodologies with large-scale datasets and
large models. We undertake a comprehensive evaluation,
covering both high-quality and low-quality image datasets.
The results, as shown in Tab. 4, underscore the strengths of
KP-RPE. Notably, the inclusion of KP-RPE does not impair
the performance on high-quality datasets, a testament to its
applicability to both low and high-quality datasets.
This becomes particularly compelling when we observe
the performance on low-quality datasets. Consistent with the
ﬁndings of our ablation study, the introduction of KP-RPE
leads to an appreciable improvement in these challenging
scenarios. This supports our thesis that face recognition
models with robust alignment capabilities can indeed en-
hance performance on low-quality datasets. In summary, our
model with KP-RPE not only maintains competitive perfor-
mance on high-quality datasets but also brings signiﬁcant
improvements on low-quality ones, marking it a valuable
contribution to the ﬁeld of face recognition.
4.5. Note on the Landmark Predictor
KP-RPE in all experiments uses our own MobileNet [ 58]
based RetinaFace [ 10] to predict landmarks for KP-RPE. We
train MobileNet version for computation efﬁciency. How-
ever, the original landmark predictor used for aligning the
test datasets is ResNet50-RetinaFace [ 10]. We also report the
KP-RPE performance with the ofﬁcially released ResNet50-
RetinaFace. We report this to compare KP-RPE on the same
ground with other models by using the same landmark used
to pre-align the testset. The face recognition performance
of KP-RPE+Ofﬁcial is similar to KP-RPE+Ours ( 75.86vs
75.80in TinyFace Rank1). Our MobileNet-RetinaFace is im-
proved to perform similarly to ResNet50 in landmark predic-
tion by applying additional tricks while training. Therefore,
the face recognition performances are also similar. Unlike
vanilla RetinaFace on face alignment, ours is fully differ-
entiable during inference and named Differentiable FaceAligner. Details and analysis can be found in Supp.2 and 3.
4.6. Scalability on Larger Training Datasets
We train the ViT+KP-RPE model on a larger Web-
Face12M [ 91] dataset to demonstrate the potential of KP-
RPE in its scalability and applicability in real-world, data-
rich scenarios. Tab. 4’s last row shows that the performance
continues to increase with WebFace12M dataset.
Discussion. Why are noisy keypoints more useful in KP-
RPE than in simple alignment? The short answer is that not
all predicted points are noisy in an image while alignment as
a result of one or more noisy point impacts all pixels. For our
attempt at a more detailed answer, please refer to Supp.2.4.
5. Gait Recognition Experiments
KP-RPE is a generic method that can generalize beyond face
recognition to any task with keypoints. We apply KP-RPE
to gait recognition using body joints as the keypoints.
Dataset. We train and evaluate on Gait3D [ 87], an in-the-
wild gait video dataset. In our experiments, we use silhou-
ettes and 2D keypoints preprocessed and released by the
authors directly. Following SMPLGait [ 79,87], we use rank-
naccuracy ( n=1,5,10), mean Average Precision (mAP),
and mean Inverse Negative Penalty (mINP) for evaluation.
Implementation Details. We implement SwinGait-2D [ 17]
as the baseline in our experiments. SwinGait-2D is cho-
sen over SwinGait-3D [ 17] because we focus on exploiting
the geometric information in gait recognition. SwinTrans-
former [ 44] uses vanilla relative positional encoding for each
windowed self-attention. To incorporate KP-RPE into the
SwinTransformer, we modify the 2D grid Mto be the size
of the window as opposed to the image size. Following
the default conﬁguration of [ 87], we use an AdamW [ 46]
optimizer with a learning rate 3⇥10 4and weight decay
2⇥10 2, accompanied by an SGDR [ 45] scheduler. We
train our models for 60,000iterations, sampling 32subjects
and4sequences per subject in a batch.
250
Table 4. SoTA comparison on low-quality and high-quality datasets. ViT models are ViT-Base sized.
Method Backbone Train DataLow Quality Dataset High Quality Dataset
TinyFace [ 7] IJB-S [ 29] AgeDB [ 50] CFPFP [ 59] IJB-C [ 48]
Rank- 1 Rank- 5 Rank- 1 Rank- 5 Veriﬁcation Accuracy TAR@FAR=0.01%
PFE [ 61]aaa CNN64 MS1MV2 [ 11]- - 50.16 58 .33 -- -
ArcFace [ 11] ResNet101 MS1MV2 [ 11]- - 57.35 64 .42 98 .28 98 .27 96 .03
URL [ 62] ResNet101 MS1MV2 [ 11] 63.89 68 .67 59 .79 65 .78 - 98.64 96 .60
CurricularFace [ 27] ResNet101 MS1MV2 [ 11] 63.68 67 .65 62 .43 68 .68 98.32 98.37 96 .10
AdaFace [ 11] ResNet101 MS1MV2 [ 11] 68.21 71 .54 65 .26 70 .53 98 .05 98 .49 96 .89
AdaFace [ 11] ResNet101 MS1MV3 [ 13] 67.81 70 .98 67 .12 72 .67 98 .17 99 .03 97 .09
AdaFace [ 30] ViT MS1MV3 [ 13] 72.05 74 .84 65 .95 71 .64 97 .87 99 .06 97 .10
AdaFace [ 30] ViT+KP-RPE MS1MV3 [ 13] 73.50 76 .39 67 .62 73 .25 97.98 99.11 97 .16
ArcFace [ 11] ResNet101 WebFace4M [ 91] 71.11 74 .38 69 .26 74 .31 97.93 99.06 96 .63
AdaFace [ 30] ResNet101 WebFace4M [ 91] 72.02 74 .52 70 .42 75 .29 97 .90 99.17 97 .39
AdaFace [ 30] ViT WebFace4M [ 91] 74.81 77 .58 71 .90 77 .09 97 .48 98 .94 97 .14
AdaFace [ 30] ViT+iRPE WebFace4M [ 91] 74.92 77 .98 71 .93 77 .14 97 .15 99 .01 97 .01
AdaFace [ 30] ViT+KP-RPE WebFace4M [ 91] 75.80 78 .49 72 .78 78 .20 97.67 99 .01 97 .13
AdaFace [ 30] ResNet101 WebFace12M [ 91] 72.42 74 .81 71 .46 77 .04 98 .00 99 .24 97 .66
AdaFace [ 30] ViT+KP-RPE WebFace12M [ 91]76.18 78 .97 72 .94 77 .46 98 .07 99 .30 97 .82
Table 5. KP-RPE performance on Gait3D [ 87] compared with the
baseline. KP-RPE boosts all metrics by a large margin.
Model Rank-1 Rank-5 mAP mINP
GaitSet [ 6] 36.75 8 .33 0 .01 17 .30
MTSGait [ 86] 48.76 7 .13 7 .63 21 .92
DANet [ 47] 48.06 9 .7 ——
GaitGCI [ 16] 50.36 8 .53 9 .52 4 .3
GaitBase [ 18] 64.68 1 .55 5 .31 31 .63
HSTL [ 70] 61.37 6 .35 5 .48 34 .77
DyGait [ 71] 66.38 0 .85 6 .40 37.30
SwinGait-2D [ 17] 67.18 3 .75 8 .76 34 .36
+ KP-RPE 68.28 4 .46 0 .81 36.19
Results and Analyses. In Tab. 5, we compare to SoTA
approaches, including SwinGait-2D [ 17], with and without
KP-RPE. We can see that the KP-RPE shows a signiﬁcant
improvement over SwinGait-2D, with 1.1%and0.7%im-
provement on rank- 1and - 5accuracies, respectively. mAP
has improved by 2.05 % and mINP by 1.23 % of the base-
line) compared to SwinGait-2D. We believe that a great
portion of the improvement comes from KP-RPE exploiting
the gait information contained in 2D skeletons. Gait skele-
tons contain identity-related information, such as body shape
and walking posture. This demonstrates that KP-RPE is both
effective and generalizable to gait recognition.
6. Conclusion
In this work, we introduce Keypoint-based Relative Posi-
tion Encoding (KP-RPE), a method designed to enhance the
robustness of recognition models to alignment errors. Our
method uniquely establishes key-query relationships in self-
attention based on their distance to the keypoints, improving
its performance across a variety of datasets, including those
with low-quality or misaligned images. KP-RPE demon-strates superior efﬁciency in terms of computational cost,
throughput and recognition performance, especially when
afﬁne transform robustness is beneﬁcial. We believe that
KP-RPE opens a new avenue in recognition research, paving
the way for the development of more robust models.
Limitations. While KP-RPE shows impressive face recogni-
tion capabilities, it does require keypoint supervision, which
may not always be readily available and can constrain its
application, particularly when the dataset is not comprised
of images with a consistent topology. Future work should
consider the self-discovery of keypoints to lessen this depen-
dence, thereby boosting the model’s ﬂexibility.
Potential Societal Impacts Within the CV/ML commu-
nity, we must strive to mitigate any negative societal impacts.
This study uses the MS1MV* dataset, derived from the dis-
continued MS-Celeb, to allow a fair comparison with SoTA
methods. However, we encourage a shift towards newer
datasets, showcasing results using the recent WebFace4M
dataset. Data collection ethics are paramount, often requiring
IRB approval for human data collection. Most face recogni-
tion datasets likely lack IRB approval due to their collection
methods. We support the community in gathering large,
consent-based datasets or fully synthetic datasets [ 3,32],
enabling research without societal backlash.
Acknowledgments. This research is based upon work sup-
ported by the Ofﬁce of the Director of National Intelligence
(ODNI), Intelligence Advanced Research Projects Activity
(IARPA), via 2022-21102100004. The views and conclu-
sions contained herein are those of the authors and should
not be interpreted as necessarily representing the ofﬁcial
policies, either expressed or implied, of ODNI, IARPA, or
the U.S. Government. The U.S. Government is authorized to
reproduce and distribute reprints for governmental purposes
notwithstanding any copyright annotation therein.
251
References
[1]Xiang An, Jiankang Deng, Jia Guo, Ziyong Feng, XuHan
Zhu, Jing Yang, and Tongliang Liu. Killing two birds with
one stone: Efﬁcient and robust training of face recognition
cnns by partial fc. In CVPR , pages 4042–4051, 2022. 6
[2]Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao,
Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang,
et al. Partial fc: Training 10 million identities on a single
machine. In ICCV , pages 1445–1449, 2021. 6
[3]Gwangbin Bae, Martin de La Gorce, Tadas Baltrušaitis, Char-
lie Hewitt, Dong Chen, Julien Valentin, Roberto Cipolla, and
Jingjing Shen. Digiface-1m: 1 million digital face images for
face recognition. In WACV . IEEE, 2023. 8
[4]Adrian Bulat and Georgios Tzimiropoulos. How far are we
from solving the 2D & 3D face alignment problem? (and a
dataset of 230,000 3d facial landmarks). In ICCV , 2017. 3
[5]Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In CVPR , pages 7291–7299, 2017. 3
[6]Hanqing Chao, Yiwei He, Junping Zhang, and Jianfeng Feng.
Gaitset: Regarding gait as a set for cross-view gait recog-
nition. In Proceedings of the AAAI conference on artiﬁcial
intelligence , volume 33, pages 8126–8133, 2019. 8
[7]Zhiyi Cheng, Xiatian Zhu, and Shaogang Gong. Low-
resolution face recognition. In ACCV , 2018. 2,3,5,6,
8
[8]Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le.
Randaugment: Practical automated data augmentation with a
reduced search space. In CVPR workshops , pages 702–703,
2020. 5,1
[9]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell,
Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Atten-
tive language models beyond a ﬁxed-length context. In ACL,
pages 2978–2988, 2019. 2,4
[10] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia,
and Stefanos Zafeiriou. Retinaface: Single-shot multi-level
face localisation in the wild. In CVPR , pages 5203–5212,
2020. 1,3,5,6,7,4
[11] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.
ArcFace: Additive angular margin loss for deep face recogni-
tion. In CVPR , 2019. 1,3,5,8,2
[12] Jiankang Deng, Jia Guo, Jing Yang, Alexandros Lattas, and
Stefanos Zafeiriou. Variational prototype learning for deep
face recognition. In CVPR , 2021. 1
[13] Jiankang Deng, Jia Guo, Debing Zhang, Yafeng Deng, Xi-
angju Lu, and Song Shi. Lightweight face recognition chal-
lenge. In ICCVW , 2019. 5,8
[14] Li Deng. The mnist database of handwritten digit images for
machine learning research. IEEE Signal Processing Maga-
zine, 29(6):141–142, 2012. 1
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR , 2021. 1,2,
3[16] Huanzhang Dou, Pengyi Zhang, Wei Su, Yunlong Yu, Yining
Lin, and Xi Li. Gaitgci: Generative counterfactual interven-
tion for gait recognition. In CVPR , 2023. 8
[17] Chao Fan, Saihui Hou, Yongzhen Huang, and Shiqi Yu. Ex-
ploring deep models for practical gait recognition. arXiv
preprint arXiv:2303.03301 , 2023. 7,8
[18] Chao Fan, Junhao Liang, Chuanfu Shen, Saihui Hou,
Yongzhen Huang, and Shiqi Yu. Opengait: Revisiting
gait recognition toward better practicality. arXiv preprint
arXiv:2211.06597 , 2022. 2,8
[19] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,
and Yann N Dauphin. Convolutional sequence to sequence
learning. In ICML , pages 1243–1252. PMLR, 2017. 3
[20] Klemen Grm, Berk Kemal Özata, Vitomir Štruc, and
Hazım Kemal Ekenel. Meet-in-the-middle: Multi-scale up-
sampling and matching for cross-resolution face recognition.
InProceedings of the IEEE/CVF Winter Conference on Ap-
plications of Computer Vision , pages 120–129, 2023. 3
[21] Steven A Grosz and Anil K Jain. Latent ﬁngerprint recogni-
tion: Fusion of local and global embeddings. arXiv preprint
arXiv:2304.13800 , 2023. 3
[22] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and
Jianfeng Gao. MS-Celeb-1M: A dataset and benchmark for
large-scale face recognition. In ECCV , 2016. 1
[23] Ryo Hachiuma, Fumiaki Sato, and Taiki Sekii. Uniﬁed
keypoint-based action recognition framework via structured
keypoint pooling. In CVPR , pages 22962–22971, 2023. 3
[24] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten
Hoeﬂer, and Daniel Soudry. Augment your batch: Improving
generalization through instance repetition. In CVPR , pages
8129–8138, 2020. 5
[25] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric
Learned-Miller. Labeled Faces in the Wild: A database
forstudying face recognition in unconstrained environments.
InWorkshop on Faces in’Real-Life’Images: Detection, Align-
ment, and Recognition , 2008. 1,3
[26] Yuge Huang, Pengcheng Shen, Ying Tai, Shaoxin Li, Xiaom-
ing Liu, Jilin Li, Feiyue Huang, and Rongrong Ji. Improving
face recognition from hard samples via distribution distilla-
tion loss. In ECCV , 2020. 3
[27] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu,
Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang.
CurricularFace: adaptive curriculum learning loss for deep
face recognition. In CVPR , 2020. 3,8
[28] Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang.
Improve transformer models with better relative position em-
beddings. In EMNLP , pages 3327–3335, Online, November
2020. 1,2,4
[29] Nathan D Kalka, Brianna Maze, James A Duncan, Kevin
O’Connor, Stephen Elliott, Kaleb Hebert, Julia Bryan, and
Anil K Jain. IJB–S: IARPA Janus Surveillance Video Bench-
mark. In BTAS , 2018. 2,3,5,6,8
[30] Minchul Kim, Anil K Jain, and Xiaoming Liu. AdaFace:
Quality adaptive margin for face recognition. In CVPR , 2022.
1,3,5,8,2,6
[31] Minchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu. Cluster
and aggregate: Face recognition with large probe set. In
NeurIPS , 2022. 3
252
[32] Minchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu. DC-
Face: Synthetic face generation with dual condition diffusion
model. 2023. 8
[33] Yonghyun Kim, Wonpyo Park, and Jongju Shin. BroadFace:
Looking at tens of thousands of people at once for face recog-
nition. In ECCV , 2020. 1
[34] Brendan F Klare, Ben Klein, Emma Taborsky, Austin Blanton,
Jordan Cheney, Kristen Allen, Patrick Grother, Alan Mah,
and Anil K Jain. Pushing the frontiers of unconstrained face
detection and recognition: IARPA Janus Benchmark-A. In
CVPR , 2015. 1
[35] Abhinav Kumar, Tim K. Marks, Wenxuan Mou, Ye Wang,
Michael Jones, Anoop Cherian, Toshiaki Koike-Akino, Xi-
aoming Liu, and Chen Feng. Luvli face alignment: Estimating
landmarks’ location, uncertainty, and visibility likelihood. In
CVPR , 2020. 3
[36] Chiawei Kuo, Yi-Ting Tsai, Hong-Han Shuai, Yi-ren Yeh,
and Ching-Chun Huang. Towards understanding cross reso-
lution feature matching for surveillance face recognition. In
Proceedings of the 30th ACM International Conference on
Multimedia , pages 6706–6716, 2022. 3
[37] Shen Li, Jianqing Xu, Xiaqing Xu, Pengcheng Shen, Shaoxin
Li, and Bryan Hooi. Spherical conﬁdence learning for face
recognition. In CVPR , 2021. 1
[38] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In CVPR , pages 2117–2125,
2017. 3,4
[39] Feng Liu, Ryan Ashbaugh, Nicholas Chimitt, Najmul Has-
san, Ali Hassani, Ajay Jaiswal, Minchul Kim, Zhiyuan Mao,
Christopher Perry, Zhiyuan Ren, Yiyang Su, Pegah Varghaei,
Kai Wang, Xingguang Zhang, Stanley Chan, Arun Ross,
Humphrey Shi, Zhangyang Wang, Anil Jain, and Xiaoming
Liu. Farsight: A physics-driven whole-body biometric system
at large distance and altitude. In WACV , 2024. 1
[40] Feng Liu, Minchul Kim, ZiAng Gu, Anil Jain, and Xiaoming
Liu. Learning clothing and pose invariant 3d shape represen-
tation for long-term person re-identiﬁcation. In ICCV , 2023.
1
[41] Feng Liu, Minchul Kim, Anil Jain, and Xiaoming Liu. Con-
trollable and guided face synthesis for unconstrained face
recognition. In Computer Vision–ECCV 2022: 17th European
Conference, Tel Aviv, Israel, October 23–27, 2022, Proceed-
ings, Part XII , pages 701–719. Springer, 2022. 3
[42] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg.
Ssd: Single shot multibox detector. In Computer Vision–
ECCV 2016: 14th European Conference, Amsterdam, The
Netherlands, October 11–14, 2016, Proceedings, Part I 14 ,
pages 21–37. Springer, 2016. 3,4
[43] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha
Raj, and Le Song. SphereFace: Deep hypersphere embedding
for face recognition. In CVPR , 2017. 1,3
[44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , 2021. 7[45] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient
descent with warm restarts. arXiv preprint arXiv:1608.03983 ,
2016. 6,7
[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 5,7
[47] Kang Ma, Ying Fu, Dezhi Zheng, Chunshui Cao, Xuecai Hu,
and Yongzhen Huang. Dynamic aggregated network for gait
recognition. In CVPR , 2023. 8
[48] Brianna Maze, Jocelyn Adams, James A Duncan, Nathan
Kalka, Tim Miller, Charles Otto, Anil K Jain, W Tyler Niggel,
Janet Anderson, Jordan Cheney, and Patrick Grother. IARPA
Janus Benchmark-C: Face dataset and protocol. In ICB, 2018.
1,3,5,6,8
[49] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou.
MagFace: A universal representation for face recognition and
quality assessment. In CVPR , 2021. 1,3
[50] Stylianos Moschoglou, Athanasios Papaioannou, Chris-
tos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos
Zafeiriou. AGEDB: the ﬁrst manually collected, in-the-wild
age database. In CVPRW , 2017. 1,3,5,8
[51] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In Computer
Vision–ECCV 2016: 14th European Conference, Amsterdam,
The Netherlands, October 11-14, 2016, Proceedings, Part
VIII 14 , pages 483–499. Springer, 2016. 3
[52] George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander
Toshev, Jonathan Tompson, Chris Bregler, and Kevin Murphy.
Towards accurate multi-person pose estimation in the wild.
InCVPR , pages 4903–4911, 2017. 3
[53] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan
Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-
attention in vision models. NeurIPS , 32, 2019. 2,4
[54] Rajeev Ranjan, Ankan Bansal, Hongyu Xu, Swami Sankara-
narayanan, Jun-Cheng Chen, Carlos D Castillo, and Rama
Chellappa. Crystal loss and quality pooling for uncon-
strained face veriﬁcation and recognition. arXiv preprint
arXiv:1804.01159 , 2018. 3
[55] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yux-
iong He. Deepspeed: System optimizations enable training
deep learning models with over 100 billion parameters. In Pro-
ceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining , pages 3505–3506,
2020. 7
[56] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. Advances in neural information process-
ing systems , 28, 2015. 4
[57] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dy-
namic routing between capsules. Advances in neural infor-
mation processing systems , 30, 2017. 1
[58] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In CVPR , pages 4510–4520,
2018. 7
[59] Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo,
Vishal M Patel, Rama Chellappa, and David W Jacobs.
Frontal to proﬁle face veriﬁcation in the wild. In WACV ,
2016. 1,3,5,6,8
253
[60] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-
attention with relative position representations. arXiv preprint
arXiv:1803.02155 , 2018. 2,4
[61] Yichun Shi and Anil K Jain. Probabilistic face embeddings.
InICCV , 2019. 8
[62] Yichun Shi, Xiang Yu, Kihyuk Sohn, Manmohan Chandraker,
and Anil K Jain. Towards universal representation learning
for deep face recognition. In CVPR , 2020. 8
[63] Sungho Shin, Joosoon Lee, Junseok Lee, Yeonguk Yu, and
Kyoobin Lee. Teaching where to look: Attention similarity
knowledge distillation for low resolution face recognition. In
Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XII ,
pages 631–647. Springer, 2022. 3
[64] Yukun Su, Guosheng Lin, Jinhui Zhu, and Qingyao Wu. Hu-
man interaction learning on 3d skeleton point clouds for video
violence recognition. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part IV 16 , pages 74–90. Springer, 2020. 3
[65] Ying Tai, Yicong Liang, Xiaoming Liu, Lei Duan, Jilin Li,
Chengjie Wang, Feiyue Huang, and Yu Chen. Towards highly
accurate and stable face alignment for high-resolution videos.
InAAAI , 2019. 3
[66] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-
cas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,
Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-
mixer: An all-mlp architecture for vision. In NeurIPS , 2021.
4
[67] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Hervé Jégou. Training
data-efﬁcient image transformers & distillation through atten-
tion. In ICML , 2021. 5
[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 2,3
[69] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong,
Jingchao Zhou, Zhifeng Li, and Wei Liu. CosFace: Large
margin cosine loss for deep face recognition. In CVPR , 2018.
1,3,2
[70] Lei Wang, Bo Liu, Fangfang Liang, and Bincheng Wang.
Hierarchical spatio-temporal representation learning for gait
recognition. In ICCV , 2023. 8
[71] Ming Wang, Xianda Guo, Beibei Lin, Tian Yang, Zheng Zhu,
Lincheng Li, Shunli Zhang, and Xin Yu. Dygait: Exploiting
dynamic representations for high-performance gait recogni-
tion. In ICCV , 2023. 8
[72] Cameron Whitelam, Emma Taborsky, Austin Blanton, Bri-
anna Maze, Jocelyn Adams, Tim Miller, Nathan Kalka,
Anil K Jain, James A Duncan, Kristen Allen, et al. IARPA
Janus Benchmark-B face dataset. In CVPRW , 2017. 1,3,2
[73] Ross Wightman. Pytorch image models. https :
/ / github . com / rwightman / pytorch - image -
models , 2019. 6
[74] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and
Hongyang Chao. Rethinking and improving relative position
encoding for vision transformer. In ICCV , pages 10033–
10041, 2021. 1,2,4[75] Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai,
and Qiang Zhou. Look at boundary: A boundary-aware face
alignment algorithm. In CVPR , pages 2129–2138, 2018. 6
[76] Jiahao Xia, Weiwei Qu, Wenjian Huang, Jianguo Zhang,
Xi Wang, and Min Xu. Sparse local patch transformer for ro-
bust face alignment and landmarks inherent relation learning.
InCVPR , pages 4052–4061, 2022. 6
[77] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal
graph convolutional networks for skeleton-based action recog-
nition. In Proceedings of the AAAI conference on artiﬁcial
intelligence , volume 32, 2018. 3
[78] Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang.
Wider face: A face detection benchmark. In CVPR , pages
5525–5533, 2016. 3,5,4,6
[79] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao,
and Steven CH Hoi. Deep learning for person re-identiﬁcation:
A survey and outlook. IEEE transactions on pattern analysis
and machine intelligence , 44(6):2872–2893, 2021. 7
[80] Xi Yin, Ying Tai, Yuge Huang, and Xiaoming Liu. Fan:
Feature adaptation network for surveillance face recognition
and normalization. In ACCV , 2020. 3
[81] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
Joint face detection and alignment using multitask cascaded
convolutional networks. Signal Processing Letters , 2016. 3
[82] Ziyuan Zhang, Luan Tran, Feng Liu, and Xiaoming Liu. On
learning disentangled representations for gait recognition.
IEEE T-PAMI , 44(1):345–360, 2020. 1
[83] Weisong Zhao, Xiangyu Zhu, Kaiwen Guo, Xiao-Yu Zhang,
and Zhen Lei. Grouped knowledge distillation for deep face
recognition. AAAI , 2023. 1
[84] Jingxiao Zheng, Rajeev Ranjan, Ching-Hui Chen, Jun-Cheng
Chen, Carlos D Castillo, and Rama Chellappa. An automatic
system for unconstrained video-based face recognition. IEEE
Transactions on Biometrics, Behavior, and Identity Science ,
2(3):194–209, 2020. 3
[85] Jingxiao Zheng, Ruichi Yu, Jun-Cheng Chen, Boyu Lu, Car-
los D Castillo, and Rama Chellappa. Uncertainty modeling of
contextual-connections between tracklets for unconstrained
video-based face recognition. In ICCV , pages 703–712, 2019.
3
[86] Jinkai Zheng, Xinchen Liu, Xiaoyan Gu, Yaoqi Sun, Chuang
Gan, Jiyong Zhang, Wu Liu, and Chenggang Yan. Gait recog-
nition in the wild with multi-hop temporal switch. In Pro-
ceedings of the 30th ACM International Conference on Multi-
media , 2022. 8
[87] Jinkai Zheng, Xinchen Liu, Wu Liu, Lingxiao He, Chenggang
Yan, and Tao Mei. Gait recognition in the wild with dense
3d representations and a benchmark. In CVPR , pages 20228–
20237, 2022. 1,2,7,8
[88] Tianyue Zheng and Weihong Deng. Cross-Pose LFW: A
database for studying cross-pose face recognition in un-
constrained environments. Beijing University of Posts and
Telecommunications, Tech. Rep , 5:7, 2018. 1,3
[89] Tianyue Zheng, Weihong Deng, and Jiani Hu. Cross-Age
LFW: A database for studying cross-age face recognition in
unconstrained environments. CoRR , abs/1708.08197, 2017. 3
254
[90] Yanjia Zhu, Hongxiang Cai, Shuhan Zhang, Chenhao Wang,
and Yichao Xiong. Tinaface: Strong but simple baseline for
face detection. arXiv preprint arXiv:2011.13183 , 2020. 3
[91] Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie
Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Da-
long Du, et al. WebFace260M: A benchmark unveiling the
power of million-scale deep face recognition. In CVPR , 2021.
1,5,7,8,2
255
