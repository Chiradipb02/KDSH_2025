1-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness
Bernd Prach,1,*Fabio Brau,2,*Giorgio Buttazzo,2Christoph H. Lampert1
1ISTA, Klosterneuburg, Austria
2Scuola Superiore Sant’Anna, Pisa, Italy
{bprach, chl }@ist.ac.at, {fabio.brau, giorgio.buttazzo }@santannapisa.it
Abstract
The robustness of neural networks against input perturba-
tions with bounded magnitude represents a serious con-
cern in the deployment of deep learning models in safety-
critical systems. Recently, the scientific community has fo-
cused on enhancing certifiable robustness guarantees by
crafting 1-Lipschitz neural networks that leverage Lipschitz
bounded dense and convolutional layers. Different meth-
ods have been proposed in the literature to achieve this
goal, however, comparing the performance of such meth-
ods is not straightforward, since different metrics can be
relevant (e.g., training time, memory usage, accuracy, cer-
tifiable robustness) for different applications. Therefore,
this work provides a thorough comparison between differ-
ent methods, covering theoretical aspects such as compu-
tational complexity and memory requirements, as well as
empirical measurements of time per epoch, required mem-
ory, accuracy and certifiable robust accuracy. The pa-
per also provides some guidelines and recommendations to
support the user in selecting the methods that work best
depending on the available resources. We provide code
atgithub.com/berndprach/1LipschitzLayersCompared.
1. Introduction
Modern artificial neural networks achieve high accuracy
and sometimes superhuman performance in many different
tasks, but it is widely recognized that they are not robust
to tiny and imperceptible input perturbations [4, 39] that, if
properly crafted, can cause a model to produce the wrong
output. Such inputs, known as Adversarial Examples , rep-
resent a serious concern for the deployment of machine
learning models in safety-critical systems [26]. To over-
come this issue, adversarial training has been proposed in
*Joined first authors.
This work was partially supported by project SERICS (PE00000014)
under the MUR National Recovery and Resilience Plan funded by the Eu-
ropean Union - NextGenerationEU.
TT
IT TMIMRA A
1
2
3
4
5AOLTT
IT TMIMRA A
1
2
3
4
5BCOP
TT
IT TMIMRA A
1
2
3
4
5CPLTT
IT TMIMRA A
1
2
3
4
5Cayley
TT
IT TMIMRA A
1
2
3
4
5 LOT TT
IT TMIMRA A
1
2
3
4
5SLL
TT
IT TMIMRA A
1
2
3
4
5SOC Legend
RA
A
TT
IT
TM
IM     Robust Accuracy
Accuracy
Training Time
Inference Time
Train Memory
Inference MemoryFigure 1. Evaluation of 1-Lipschitz methods on different metrics.
Scores are assigned from 1 (worst) to 5 (best) to every method
based on the results reported in Sections 3 and 5.
[14, 30, 39]. It uses adversarial examples during the training
to correct the model prediction. This strategy does improves
the empirical robustness of the model, however, it does not
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
24574
provide any guarantees of robustness.
However, for many application a guarantee of robust-
ness is desired. Roughly speaking, a model fis said to
beε-robust for a given input xif no perturbation of mag-
nitude bounded by εcan change its prediction. Recently,
in the context of image classification, various approaches
have been proposed to achieve certifiable robustness, in-
cluding Verification, Randomized Smoothing , and Lipschitz
Bounded Neural Networks .
Verification strategies aim to establish, for any given
model, whether all samples contained in a l2-ball with ra-
diusεand centered in the tested input xare classified with
the same class as x. In the exact formulation, verification
strategies involve the solution of an NP-hard problem [20].
Nevertheless, even in a relaxed formulation, [44], these
strategies require a huge computational effort [43].
Randomized smoothing strategies, initially presented in
[10], represent an effective way of crafting a certifiable-
robust classifier gbased on a base classifier f. If combined
with an additional denoising step, they can achieve state-of-
the-art levels of robustness, [7]. However, since they require
multiple evaluations of the base model (up to 100k evalua-
tions) for the classification of a single input, they cannot be
used for real-time applications.
Finally, Lipschitz Bounded Neural Networks [6, 9, 24,
27, 29, 34, 40] represent a valid alternative to produce cer-
tifiable classifiers, since they only require a single forward
pass of the model at inference time to deduce guarantees
of robustness. Indeed, for such models, a lower-bound of
the minimal adversarial perturbation capable of fooling the
classifier can be evaluated by considering the difference be-
tween the two largest class scores predicted by the model.
Lipschitz-bounded neural networks can be obtained by
the composition of 1-Lipschitz layers [2]. The process of
parameterizing 1-Lipschitz layers is fairly straightforward
for fully connected layers. However, for convolutions —
with overlapping kernels — deducing an effective parame-
terization is a hard problem. Indeed, the Lipschitz condition
can be essentially thought of as a condition on the Jacobian
of the layer. However, the Jacobian matrix can not be effi-
ciently computed.
In order to avoid the explicit computation of the Jaco-
bian, various methods have been proposed, including pa-
rameterizations that cause the Jacobian to be (very close
to) orthogonal [27, 36, 40, 46] and methods that rely on
an upper bound on the Jacobian instead [34]. Those differ-
ent methods differ drastically in training and validation re-
quirements (in particular time and memory) as well as em-
pirical performance. Furthermore, increasing training time
or model sizes very often also increases the empirical per-
formance. This makes it hard to judge from the existingliterature which methods are the most promising. This be-
comes even worse when working with specific computation
requirements, such as restrictions on the available memory.
In this case, it is important to choose the method that better
suits the characteristics of the system in terms of evaluation
time, memory usage as well and certifiable-robust-accuracy.
This work aims at giving a comprehensive comparison
of different strategies for crafting 1-Lipschitz layers from
both a theoretical and practical perspective. For the sake
of fairness, we consider several metrics such as Time and
Memory requirements for both training and inference,
Accuracy , as well as Certified Robust Accuracy . The main
contributions are the following:
• An empirical comparison of 1-Lipschitz layers based on
six different metrics, and four different datasets on four
architecture sizes with three time constraints.
• A theoretical comparison of the runtime complexity and
the memory usage of existing methods.
• A review of the most recent methods in the literature, in-
cluding implementations with a revised code that we will
release publicly for other researchers to build on.
2. Existing Works and Background
In recent years, various methods have been proposed for
creating artificial neural networks with a bounded Lipschitz
constant. The Lipschitz constant of a function f:Rn→
Rmwith respect to the l2norm is the smallest Lsuch that
for all x, y∈Rn
∥f(x)−f(y)∥2≤L∥x−y∥2. (1)
We also extend this definition to networks and layers, by
considering the l2norms of the flattened input and output
tensors in Equation (1). A layer is called 1-Lipschitz if its
Lipschitz constant is at most 1. For linear layers, the Lip-
schitz constant is equal to the spectral norm of the weight
matrix that is given as
∥M∥2= sup
v̸=0∥Mv∥2
∥v∥2. (2)
A particular class of linear 1-Lipschitz layers are ones with
an orthogonal Jacobian matrix. The Jacobian matrix of a
layer is the matrix of partial derivatives of the flattened out-
puts with respect to the flattened inputs. A matrix Mis
orthogonal if MM⊤=I, where Iis the identity matrix.
For layers with an orthogonal Jacobian, Equation (1) always
holds with equality and, because of this, a lot of methods
aim at constructing such 1-Lipschitz layers.
All the neural networks analyzed in this paper consist of
1-Lipschitz parameterized layers and 1-Lipschitz activation
24575
functions, with no skip connections and no batch normal-
ization. Even though the commonly used ReLU activation
function is 1-Lipschitz, Anil et al. [2] showed that it reduces
the expressive capability of the model. Hence, we adopt the
MaxMin activation proposed by the authors and commonly
used in 1-Lipschitz models. Concatenations of 1-Lipschitz
functions are 1-Lipschitz, so the networks analyzed are 1-
Lipschitz by construction.
2.1. Parameterized 1-Lipschitz Layers
This section provides an overview of the existing methods
for providing 1-Lipschitz layers. We discuss fundamen-
tal methods for estimating the spectral norms of linear and
convolutional layers, i.e. Power Method [32] and Fantis-
tic4[35], and for crafting orthogonal matrices, i.e. Bjorck &
Bowie [5], in Appendix A. The rest of this section describes
7 methods from the literature that construct 1-Lipschitz con-
volutions: BCOP, Cayley, SOC, AOL, LOT, CPL, and SLL.
Further 1-Lipschitz methods, [19, 42, 47], and the reasons
why they were not included in our main comparison can be
found in Appendix B.
BCOP Block Orthogonal Convolution Parameterization
(BCOP) was introduced by Li et al . in [27] to extend a
previous work by Xiao et al. [45] that focused on the im-
portance of orthogonal initialization of the weights. For a
k×kconvolution, BCOP uses a set of (2k−1)parameter
matrices. Each of these matrices is orthogonalized using the
algorithm by Bjorck & Bowie [5] (see also Appendix A).
Then, a k×kkernel is constructed from those matrices in a
way that guarantees that the resulting layer is orthogonal.
Cayley Another family of orthogonal convolutional and
fully connected layers has been proposed by Trockman and
Kolter [40] by leveraging the Cayley Transform [8], which
maps a skew-symmetric matrix Ainto an orthogonal matrix
Qusing the relation
Q= (I−A)(I+A)−1. (3)
The transformation can be used to parameterize orthogonal
weight matrices for linear layers in a straightforward way.
For convolutions, the authors make use of the fact that cir-
cular padded convolutions are vector-matrix products in the
Fourier domain. As long as all those vector-matrix products
have orthogonal matrices, the full convolution will have an
orthogonal Jacobian. For Cayley Convolutions , those matri-
ces are orthogonalized using the Cayley transform.
SOC Skew Orthogonal Convolution is an orthogonal con-
volutional layer presented by Singla et al. [36], obtained by
leveraging the exponential convolution [15]. Analogouslyto the matrix case, given a kernel L∈Rc×c×k×k, the expo-
nential convolution can be defined as
exp(L)(x) :=x+L ⋆ x
1+L ⋆2x
2!+···+L ⋆kx
k!+···,
(4)
where ⋆kdenotes a convolution applied k-times. The au-
thors proved that any exponential convolution has an or-
thogonal Jacobian matrix as long as Lis skew-symmetric,
providing a way of parameterizing 1-Lipschitz layers. In
their work, the sum of the infinite series is approximated
by computing only the first 5terms during training and the
first12terms during the inference, and Lis normalized to
have unitary spectral norm following the method presented
in [35] (see Appendix A).
AOL Prach and Lampert [34] introduced Almost Orthog-
onal Lipschitz (AOL) layers. For any matrix P, they defined
a diagonal rescaling matrix Dwith
Dii=X
jP⊤P
ij−1/2
(5)
and proved that the spectral norm of PD is bounded by 1.
This result was used to show that the linear layer given by
l(x) =PDx +b(where Pis the learnable matrix and Dis
given by Eq. (5)) is 1-Lipschitz. Furthermore, the authors
extended the idea so that it can also be efficiently applied
to convolutions. This is done by calculating the rescaling in
Equation (5) with the Jacobian Jof a convolution instead of
P. In order to evaluate it efficiently the authors express the
elements of J⊤Jexplicitly in terms of the kernel values.
LOT The layer presented by Xu et al. [46] extends the
idea of [19] to use the Inverse Square Root of a matrix in
order to orthogonalize it. Indeed, for any matrix V, the ma-
trixQ=V(VTV)−1
2is orthogonal. Similarly to the Cay-
leymethod, for the layer-wise orthogonal training (LOT)
the convolution is applied in the Fourier frequency domain.
To find the inverse square root, the authors relay on an it-
erative Newton Method . In details, defining Y0=VTV,
Z0=I, and
Yi+1=1
2Yi(3I−ZiYi), Zi+1=1
2(3I−ZiYi)Zi,(6)
it can be shown that Yiconverges to (VTV)−1
2. In their pro-
posed layer, the authors apply 10iterations of the method
for both training and evaluation.
CPL Meunier et al. [31] proposed the Convex Potential
Layer . Given a non-decreasing 1-Lipschitz function σ(usu-
ally ReLU), the layer is constructed as
l(x) =x−2
∥W∥2
2W⊤σ(Wx+b), (7)
24576
which is 1-Lipschitz by design. The spectral norm required
to calculate l(x)is approximated using the power method
(see Appendix A).
SLL TheSDP-based Lipschitz Layers (SLL) proposed by
Araujo et al . [3] combine the CPL layer with the upper
bound on the spectral norm from AOL. The layer can be
written as
l(x) =x−2W⊤Q−2D2σ(Wx+b), (8)
where Qis a learnable diagonal matrix with positive
entries and Dis deduced by applying Equation (5) to
P=W⊤Q−1.
Remark 1. Both CPL and SLL are non-linear by construc-
tion, so they can be used to construct a network without any
further use of activation functions. However, carrying out
some preliminary experiments, we empirically found that
alternating CPL (and SLL) layers with MaxMin activation
layers allows achieving a better performance.
3. Theoretical Comparison
As illustrated in the last section, various ideas and meth-
ods have been proposed to parameterize 1-Lipschitz lay-
ers. This causes the different methods to have very different
properties and requirements. This section aims at highlight-
ing the properties of the different algorithms, focusing on
the algorithmic complexity and the required memory.
Table 1 provides an overview of the computational com-
plexity and memory requirements for the different layers
considered in the previous section. For the sake of clar-
ity, the analysis is performed by considering separately the
transformations applied to the input of the layers and those
applied to the weights to ensure the 1-Lipschitz constraint.
Each of the two sides of the table contains three columns:
i)Operations contains the most costly transformations ap-
plied to the input as well as to the parameters of differ-
ent layers; ii) MACS reports the computational complexity
expressed in multiply-accumulate operations (MACS) in-
volved in the transformations (only leading terms are pre-
sented); iii) Memory reports the memory required by the
transformation during the training phase.
At training time, both input and weight transformations
are required, thus the training complexity of the forward
pass can be computed as the sum of the two corresponding
MACS columns of the table. Similarly, the training memory
requirements can be computed as the sum of the two corre-
sponding Memory columns of the table. For the considered
operations, the cost of the backward pass during training has
the same computational complexity as the forward pass, andtherefore increases the overall complexity by a constant fac-
tor. At inference time, all the parameter transformations can
be computed just once and cached afterward. Therefore, the
inference complexity is equal to the complexity due to the
input transformation (column 3 in the table). At inference
time, the intermediate variables are not stored in memory,
hence, the memory requirements are much lower than dur-
ing training. The values cannot directly be inferred from
Table 1, we reported them separately in Appendix C.1.
Note that all the terms reported in Table 1 depend on the
batch size b, the input size s×s×c, the number of inner
iterations of a method t, and the kernel size k×k. (Often,
tis different at training and inference time.) For the sake of
clarity, the MACS of a naive convolution implementation is
denoted by C(C=bs2c2k2), the number of inputs of a
layer is denoted by M(M=bs2c), and the size of the ker-
nel of a standard convolution is denoted by P(P=c2k2).
Only the leading terms of the computations are reported in
Table 1. In order to simplify some terms, we assume that
c >log2(s)and that rescaling a tensor (by a scalar) as well
as adding two tensors does not require any memory in order
to do backpropagation. We also assume that each additional
activation does require extra memory. All these assump-
tions have been verified to hold within PyTorch , [33]. Also,
when the algorithm described in the paper and the version
provided in the supplied code differed, we considered the
algorithm implemented in the code.
The transformations reported in the table are convolu-
tions (CONV), Fast Fourier Transformations (FFT), matrix-
vector multiplications (MV), matrix-matrix multiplications
(MM), matrix inversions (INV), as well as applications of
an activation function (ACT). The application of algorithms
such as Bjorck & Bowie (BnB), power method, and Fantas-
tic 4 (F4) is also reported (see Appendix A for descriptions).
3.1. Analysis of the computational complexity
It is worth noting that the complexity of the input transfor-
mations (in Table 1) is similar for all methods. This implies
that a similar scaling behaviour is expected at inference time
for the models. Cayley and LOT apply an FFT-based con-
volution and have computational complexity independent
of the kernel size. CPL and SLL require two convolutions,
which make them slightly more expensive at inference time.
Notably, SOC requires multiple convolutions, making this
method more expensive at inference time.
At training time, parameter transformations need to be
applied in addition to the input transformations during ev-
ery forward pass. For SOC and CPL, the input transfor-
mations always dominate the parameter transformations in
terms of computational complexity. This means the com-
plexity scales like c2, just like a regular convolution, with
a further factor of 2and5respectively. All other methods
24577
Table 1. Computational complexity and memory requirements of different methods. We report multiply-accumulate operations
(MACS) as well as memory requirements (per layer) for batch size b, image size s×s×c, kernel size k×kand number of inner iterations
t. We use C=bs2c2k2,M=bs2candP=c2k2. For a detailed explanation on what is reported see Section 3. For some explanation on
how the entries of this table were derived, see Appendix C.
Method Input Transformations Parameter Transformations
Operations MACS O(·)Memory Operations MACS O(·)Memory O(·)
Standard CONV C M - - P
AOL CONV C M CONV c3k45P
BCOP CONV C M BnB & MMs c3kt+c3k3c2kt+c2k3
Cayley FFTs & MVs bs2c2 5
2M FFTs & INVs s2c3 3
2s2c2
CPL CONVs & ACT 2C 3M power method s2c2k2P+s2c
LOT FFTs & MVs bs2c23M FFTs & MMs 4s2c3t 4s2c2t
SLL CONVs & ACT 2C 3M CONVs c3k45P
SOC CONVs Ct1 Mt 1 F4 c2k2t2 P
require parameter transformations that scale like c3, making
them more expensive for larger architectures. In particular,
we do expect Cayley and LOT to require long training times
for larger models, since the complexity of their parameter
transformations further depends on the input size.
3.2. Analysis of the training memory requirements
The memory requirements of the different layers are impor-
tant, since they determine the maximum batch size and the
type of models we can train on a particular infrastructure.
At training time, typically all intermediate results are kept
in memory to perform backpropagation. This includes in-
termediate results for both input and parameter transforma-
tions. The input transformations usually preserve the size,
and therefore the memory required is usually of O(M).
Therefore, for the input transformations, all methods re-
quire memory not more than a constant factor worse than
standard convolutions, with the worst method being SOC,
with a constant t1, typically equal to 5.
In addition to the input transformation, we also need to
store intermediate results of the parameter transformations
in memory in order to evaluate the gradients. Again, most
methods approximately preserve the sizes during the param-
eter transformations, and therefore the memory required is
usually of order O(P). Exceptions to this rule are Cayley
and LOT, with a larger O(s2c2)term, as well as BCOP.
4. Experimental Setup
This section presents an experimental study aimed at com-
paring the performance of the considered layers with re-
spect to different metrics. Before presenting the results,
we first summarize the setup used in our experiments. For
a detailed description see Appendix E. To have a fair and
meaningful comparison among the various models, all theproposed layers have been evaluated using the same archi-
tecture, loss function, and optimizer. Since, according to the
data reported in Table 1, different layers may have different
throughput, to have a fair comparison with respect to the
tested metrics, we limited the total training time instead of
fixing the number of training epochs. Results are reported
for training times of 2h, 10h, and 24h on one A100 GPU.
Our architecture is a standard convolutional network that
doubles the number of channels whenever the resolution is
reduced [6, 40]. For each method, we tested architectures
of different sizes. We denoted them as XS, S, M and L,
depending on the number of parameters, according to the
criteria in Table 7, ranging from 1.5M to 100M parameters.
Since different methods benefit from different learning
rates and weight decay, for each setting (model size, method
and dataset), we used the best values resulting from a ran-
dom search performed on multiple training runs on a val-
idation set composed of 10% of the original training set.
More specifically, 16runs were performed for each con-
figuration of randomly sampled hyperparameters, and we
selected the configuration maximizing the certified robust
accuracy w.r.t. ϵ= 36/255(see Appendix E.4 for details).
The evaluation was carried out using four different
datasets: CIFAR-10, CIFAR-100 [21], Tiny ImageNet
[23], and Imagenette [16] for large images. Augmentation
was used during the training (Random crops and flips on
CIFAR-10 and CIFAR-100, RandAugment [11] on Tiny Im-
ageNet, and random crop as well as RandAugment on Ima-
genette), details in Appendix E.5. We use the loss function
proposed by [34], with same temperature 0.25, and where
we tuned the margin to maximize the robust accuracy for
ϵ=36
255. In detail, we considered a margin of 2√
2ϵwhere
24578
the√
2factor comes from the L2norm [41], and the factor
2has been added to help with generalization.
4.1. Metrics
All the considered models were evaluated based on three
main metrics: the throughput , the required memory, and the
certified robust accuracy.
Throughput and epoch time Thethroughput of a model
is the average number of examples that the model can pro-
cess per second. It determines how many epochs are pro-
cessed in a given time frame. The evaluation of the through-
put was performed on an 80GB-A100-GPU based on the av-
erage time of 100 mini-batches. We measured the inference
throughput with cached parameter transformations.
Memory required Layers that require less memory al-
low for larger batch size, and the memory requirements
also determine the type of hardware we can train a model
on. For each model, we measured and reported the max-
imal GPU memory occupied by tensors using the func-
tiontorch.cuda.max memory allocated() pro-
vided by the PyTorch framework. This is not exactly equal
to the overall GPU memory requirement but gives a fairly
good approximation of it. Note that the model memory
measured in this way also includes additional memory re-
quired by the optimizer (e.g. to store the momentum term)
as well as by the activation layers in the forward pass. How-
ever, this additional memory should be at most of order
O(M+P). As for the throughput, we evaluated and cached
all calculations independent of the input at inference time.
Certified robust accuracy In order to evaluate the per-
formance of a 1-Lipschitz network, the standard metric is
thecertified robust accuracy . An input is classified certifi-
ably robustly with radius ϵby a model, if no perturbations
of the input with norm bounded by ϵcan change the pre-
diction of the model. Certified robust accuracy measures
the proportion of examples that are classified correctly as
well as certifiably robustly. For 1-Lipschitz models, a lower
bound of the certified ϵ-robust accuracy is the portion of cor-
rectly classified inputs such that Mf(xi, li)> ϵ√
2where
themargin Mf(x, l)of a model fat input xwith label l,
given as Mf(x, l) =f(x)l−max j̸=lfj(x), is the differ-
ence between target class score and the highest score of a
different class. For details, see [41].
5. Experimental Results
This section presents the results of the comparison per-
formed by applying the methodology discussed in Sec-
tion 4. The results related to the different metrics are dis-cussed in dedicated subsections and the key takeaways are
summarized in the radar-plot illustrated in Figure 1.
5.1. Training and inference times
Figure 2 plots the training time per epoch of the different
models as a function of their size, and Figure 3 plots the
corresponding inference throughput for the various sizes as
described in Section 4. As described in Table 5, the model
base width, referred to as w, is doubled from one model size
to the next. We expect the training and inference time to
scale with wsimilarly to how individual layers scale with
their number of channels, c(in Table 1). This is because
the width of each of the 5 blocks of our architecture is a
constant multiple of the base width, w.
The training time increases (at most) about linearly with
wfor standard convolutions, whereas the computational
complexity of each single convolution scales like c2. This
suggests that parallelism on the GPU and the overhead from
other operations (activations, parameter updates, etc.) are
important factors determining the training time. This also
explains why CPL (doing two convolutions, with identical
kernel parameters) is only slightly slower than a standard
XS S M L
Model Size7.5 sec15 sec30 sec1 min2 min4 min8 minTime
Training time per epoch
AOL
BCOP
CPL
Cayley
LOT
SLL
SOC
Standard
Figure 2. Training time per epoch (on CIFAR-10) for different
methods and different model sizes.
XS S M L
Model Size29210211212213214215Throughput (examples/s)
Inference Throughput per second
AOL
BCOP
CPL
Cayley
LOT
SLL
SOC
Standard
Figure 3. Inference throughput for different methods as a func-
tion of their size for CIFAR-10 sizes input images. All parameter
transformations have been evaluated and cached beforehand
24579
convolution, and SOC (doing 5 convolutions) is only about
3 times slower than the standard convolution. The AOL and
SLL methods also require times comparable to a standard
convolution for small models, although eventually, the c3
term in the computation of the rescaling makes them slower
for larger models. Finally, Cayley, LOT, and BCOP meth-
ods take much longer training times per epoch. For Cayley
and LOT this behavior was expected, as they have a large
O(s2c3)term in their computational complexity. See Ta-
ble 1 for further details.
At inference time transformations of the weights are
cached, therefore some methods (AOL, BCOP) do not have
any overhead compared to a standard convolution. As ex-
pected, other methods (CPL, SLL, and SOC) that apply ad-
ditional convolutions to the input suffer from a correspond-
ing overhead. Finally, Cayley and LOT have a slightly
different throughput due to their FFT-based convolution.
Among them, Cayley is about twice as fast because it in-
volves a real-valued FFT rather than a complex-valued one.
From Figure 3, it can be noted that cached Cayley and CPL
have the same inference time, even though CPL uses twice
the number of convolutions. We believe this is due to the
fact that the conventional FFT-based convolution is quite
efficient for large kernel sizes, but for smaller ones PyTorch
implements a faster algorithm, i.e.,Winograd , [22], that can
be up to 2.5times faster.
5.2. Training memory requirements
The training and inference memory requirements of the var-
ious models (measured as described in Section 4.1) are re-
ported in Figure 4 as a function of the model size. The
results of the theoretical analysis reported in Table 1 sug-
gest that the training memory requirements always have a
term linear in the number of channels c(usually the activa-
tions from the forward pass), as well as a term quadratic in
c(usually the weights and all transformations applied to the
weights during the forward pass). This behavior can also be
observed from Figure 4. For some of the models, the mem-
ory required approximately doubles from one model size to
the next one, just like the width. This means that the linear
term dominates (for those sizes), which makes those models
relatively cheap to scale up. For the BCOP, LOT, and Cay-
ley methods, the larger coefficients in the c2term (for LOT
and Cayley the coefficient is even dependent on the input
size,s2) cause this term to dominate. This makes it much
harder to scale those methods to more parameters. Method
LOT requires huge amounts of memory, in particular LOT-
L is too large to fit in 80GB GPU memory.
Note that at test time, the memory requirements are
much lower, because the intermediate activation values do
not need to be stored, as there is no backward pass. There-
fore, at inference time, most methods require a very similar
XS S M L
Model Size1 GB2 GB4 GB8 GB16 GB32 GB64 GBMemory (GB)
Memory required training
AOL
BCOP
CPL
Cayley
LOT
SLL
SOC
Standard
XS S M L
Model Size125 MB250 MB500 MB1 GB2 GB4 GBMemory
Memory required inference
AOL
BCOP
CPL
Cayley
LOT
SLL
SOC
StandardFigure 4. Memory required at training and inference time for
input size 32×32.
amount of memory as a standard convolution. The Cayley
and LOT methods require more memory since they perform
the calculation in the Fourier space, creating an intermedi-
ate representation of the weight matrices of size O(s2c2).
5.3. Certified robust accuracy
The results related to the accuracy and the certified ro-
bust accuracy for the different methods, model sizes, and
datasets measured on a 24h training budget are summa-
rized in Table 2. The differences among the various model
sizes are also highlighted in Figure 6 by reporting the sorted
values of the certified robust accuracy. Further tables and
plots relative to different training budgets can be found in
Appendix G. The reader can compare our results with the
state-of-the-art certified robust accuracy summarized in Ap-
pendix D. However, it is worth noting that, to reach state-of-
the-art performance, authors often carry out experiments us-
ing large model sizes and long training times, which makes
it hard to compare the methods themselves. On the other
hand, the evaluation proposed in this paper allows a fairer
comparison among the different methods, since it also con-
siders timing and memory aspects. This restriction based on
time, rather than the number of epochs, ensures that merely
enlarging the model size does not lead to improved perfor-
mance, as bigger models typically process fewer epochs of
data. Indeed, in our results in Figure 6 it is usually the M
(and not the L) model that performs best.
Experiments show that SOC performs best, reaching the
24580
Table 2. Certified robust accuracy for radius ϵ= 36/255on the
evaluated datasets. Training is performed for 24hours.
Accuracy [%] Robust Accuracy [%]
Methods XS S M L XS S M L
CIFAR-10
AOL 71.7 73.6 73.4 73.7 59.1 60.8 61.0 61.5
BCOP 71.7 73.1 74.0 74.6 58.5 59.3 60.5 61.5
CPL 74.9 76.1 76.6 76.8 62.5 64.2 65.1 65.2
Cayley 73.1 74.2 74.4 73.6 59.5 61.1 61.0 60.1
LOT 75.5 76.6 72.0 - 63.4 64.6 58.7 -
SLL 73.7 74.2 75.3 74.3 61.0 62.0 62.8 62.3
SOC 74.1 75.0 76.9 76.9 61.3 62.9 66.3 65.4
CIFAR-100
AOL 40.3 43.4 44.3 41.9 27.9 31.0 31.4 29.7
BCOP 41.4 42.8 43.7 42.2 28.4 30.1 31.2 29.2
CPL 42.3 - 45.2 44.3 30.1 - 33.2 32.1
Cayley 42.3 43.9 43.5 42.9 29.2 30.5 30.5 29.5
LOT 43.5 45.2 42.8 - 30.8 32.5 29.6 -
SLL 41.4 42.8 42.4 42.1 28.9 30.5 29.9 29.6
SOC 43.1 45.2 47.3 46.2 30.6 32.6 34.9 33.5
Tiny ImageNet
AOL 26.6 29.3 30.3 30.0 18.1 19.7 21.0 20.6
BCOP 22.4 26.2 27.6 27.0 13.8 16.9 17.2 16.8
CPL 28.3 29.3 29.8 30.3 18.9 19.7 20.3 20.1
Cayley 27.8 29.6 30.1 27.2 17.9 19.5 19.3 16.7
LOT 30.7 32.5 28.8 - 20.8 21.9 18.1 -
SLL 25.1 27.0 26.5 27.9 16.6 18.4 17.7 18.8
SOC 28.9 28.8 32.1 32.1 18.9 18.8 21.2 21.1
Imagenette
AOL 80.8 83.7 82.8 76.8 79.9 78.5
BCOP 81.2 84.5 9.8 75.6 80.1 9.8
CPL 85.5 86.5 86.4 80.8 82.4 82.3
Cayley 81.2 77.9 - 75.8 71.7 -
SLL 80.8 83.4 79.3 75.4 78.0 72.8
SOC 80.6 83.6 79.0 74.7 78.4 73.5
highest certified robust accuracy on two datasets. CPL mod-
els consistently rank in top-10 position among the three
datasets. LOT performed well, in particular on Tiny Im-
ageNet dataset where it performs the best. AOL did not
reach high accuracy on CIFAR-10, but reached more com-
petitive results on Tiny ImageNet. An opposite effect can
be observed for SLL, which performance seems to strongly
depend on the number of classes. BCOP only reach the top-
10 once, while Cayley is consistently outperformed by the
other methods. The very same analysis can be applied to
the clean accuracy, whose sorted bar-plots are reported in
Appendix G, where the main difference is that Cayley per-
forms slightly better for that metric. Furthermore, it is worthhighlighting that CPL is sensitive to weight initialization.
We faced numerical errors during the 10h and 24h training
of the small model on CIFAR-100. On Imagenette, CPL
clearly performs best, followed by BCOP and AOL. Note
that these methods all construct a kernel so that the convo-
lution is 1-Lipschitz. This seems to be good strategy for
higher resolution datasets. E.g. SOC, that instead applies
multiple convolutions has a drop in performs compared to
other datasets.
5.3.1 Interpretation of the results
We confirm empirically what suspected in [46]: layers that
naturally include a skip connections (CPL, SLL, SOC) gen-
erally perform better than layers that do not have this abil-
ity. Furthermore, we noticed that layers with an identity
initialization (AOL, LOT) perform better than layers that
do neither (BCOP, Cayley). Presumably this is due to the
MaxMin activation reducing the variance in the forward
pass when alternated with non-identity layers.
Our results also allow ruling out some other possible ex-
planation: one might suspect that pure contractive layers
(AOL, CPL, and SLL) would suffer from vanishing gradi-
ents, differently from orthogonal ones, however, our exper-
iments do not show any evidence of this fact. Furthermore,
one might suspect that slower methods perform worse, be-
cause they allow fewer epochs for a given time budget, how-
ever, our experiments do not support this fact; two relative
slow methods (SOC, LOT) are among the best ones.
6. Conclusions and Guidelines
This work presented a comparative study of state-of-the-art
1-Lipschitz layers under the lens of different metrics, such
as time and memory requirements, accuracy, and certified
robust accuracy, all evaluated at training and inference time.
A theoretical comparison of the methods in terms of time
and memory complexity was also presented and validated
by experiments.
Taking all metrics into account (summarized in Fig-
ure 1), the results are in favor of CPL, due to its highest
performance and lower consumption of computational re-
sources. When large computational resources are available
and the application does not impose stringent timing con-
straints during inference and training, the SOC layer could
be used, due to its slightly better performance. Finally,
those applications in which the inference time is crucial may
take advantage of AOL or BCOP, which do not introduce
additional runtime overhead (during inference) compared to
a standard convolution. For higher resolution images, it also
seems that CPL is the most promising method.
24581
References
[1] Thomas Altstidl, David Dobre, Bj ¨orn Eskofier, Gauthier
Gidel, and Leo Schwinn. Raising the bar for certified ad-
versarial robustness with diffusion models. arXiv preprint
arXiv:2305.10388 , 2023. 14, 22
[2] Cem Anil, James Lucas, and Roger Grosse. Sorting out Lip-
schitz function approximation. In International Conference
on Machine Learing (ICML) , 2019. 2, 3, 11
[3] Alexandre Araujo, Aaron J Havens, Blaise Delattre, Alexan-
dre Allauzen, and Bin Hu. A unified algebraic perspective
on Lipschitz neural networks. In International Conference
on Learning Representations (ICLR) , 2023. 4, 14, 22
[4] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nel-
son, Nedim ˇSrndi ´c, Pavel Laskov, Giorgio Giacinto, and
Fabio Roli. Evasion attacks against machine learning at test
time. In Machine Learning and Knowledge Discovery in
Databases , 2013. 1
[5] ˚A. Bj ¨orck and C. Bowie. An iterative algorithm for comput-
ing the best estimate of an orthogonal matrix. SIAM Journal
on Numerical Analysis , 1971. 3, 11
[6] Fabio Brau, Giulio Rossolini, Alessandro Biondi, and Gior-
gio Buttazzo. Robust-by-design classification via unitary-
gradient neural networks. Proceedings of the AAAI Confer-
ence on Artificial Intelligence , 2023. 2, 5
[7] Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvi-
jotham, Leslie Rice, Mingjie Sun, and J Zico Kolter. (Cer-
tified!!) adversarial robustness for free! In International
Conference on Learning Representations (ICLR) , 2023. 2
[8] Arthur Cayley. About the algebraic structure of the orthogo-
nal group and the other classical groups in a field of charac-
teristic zero or a prime characteristic. Journal f ¨ur die reine
und angewandte Mathematik , 1846. 3
[9] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann
Dauphin, and Nicolas Usunier. Parseval networks: Improv-
ing robustness to adversarial examples. In International con-
ference on machine learning , 2017. 2, 11
[10] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified
adversarial robustness via randomized smoothing. In Pro-
ceedings of the 36th International Conference on Machine
Learning , 2019. 2
[11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le. Randaugment: Practical automated data augmen-
tation with a reduced search space. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition workshops , 2020. 5, 16
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2009. 16
[13] Farzan Farnia, Jesse Zhang, and David Tse. Generalizable
adversarial training via spectral normalization. In Interna-
tional Conference on Learning Representations , 2018. 11
[14] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. stat, 2015.
1
[15] Emiel Hoogeboom, Victor Garcia Satorras, Jakub Tomczak,
and Max Welling. The convolution exponential and gener-
alized Sylvester flows. In Advances in Neural Information
Processing Systems , 2020. 3[16] Jeremy Howard. Imagenette. https://github.com/
fastai/imagenette/ . Accessed: 01.02.2024. 5, 16
[17] Kai Hu, Klas Leino, Zifan Wang, and Matt Fredrikson. Ef-
fectively leveraging capacity for improved deterministic ro-
bustness certification. In International Conference on Learn-
ing Representations (ICLR) , 2024. 14, 22
[18] Kai Hu, Andy Zou, Zifan Wang, Klas Leino, and Matt
Fredrikson. Unlocking deterministic robustness certification
on imagenet. Conference on Neural Information Processing
Systems (NeurIPS) , 2024. 14
[19] Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo
Li, and Ling Shao. Controllable orthogonalization in train-
ing DNNs. In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020. 3, 11, 12
[20] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and
Mykel J Kochenderfer. Reluplex: An efficient SMT solver
for verifying deep neural networks. In International confer-
ence on computer aided verification , 2017. 2
[21] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Technical report, 2009. 5, 16
[22] Andrew Lavin and Scott Gray. Fast algorithms for convolu-
tional neural networks. In Conference on Computer Vision
and Pattern Recognition (CVPR) , 2016. 7
[23] Ya Le and Xuan Yang. Tiny imagenet visual recognition
challenge. CS 231N , 2015. 5, 16
[24] Klas Leino, Zifan Wang, and Matt Fredrikson. Globally-
robust neural networks. In International Conference on Ma-
chine Learning , 2021. 2, 11
[25] Mario Lezcano-Casado and David Mart ´ınez-Rubio. Cheap
orthogonal constraints in neural networks: A simple
parametrization of the orthogonal and unitary group. In In-
ternational Conference on Machine Learing (ICML) , 2019.
11
[26] Linyi Li, Tao Xie, and Bo Li. Sok: Certified robustness for
deep neural networks. In 2023 IEEE Symposium on Security
and Privacy (SP) , 2023. 1
[27] Qiyang Li, Saminul Haque, Cem Anil, James Lucas,
Roger B Grosse, and Joern-Henrik Jacobsen. Preventing
gradient attenuation in Lipschitz constrained convolutional
networks. In Conference on Neural Information Processing
Systems (NeurIPS) , 2019. 2, 3, 11, 14
[28] Shuai Li, Kui Jia, Yuxin Wen, Tongliang Liu, and Dacheng
Tao. Orthogonal deep neural networks. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2021. 11
[29] Max Losch, David Stutz, Bernt Schiele, and Mario Fritz.
Certified robust models with slack control and large Lips-
chitz constants. arXiv preprint arXiv:2309.06166 , 2023. 2
[30] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learn-
ing models resistant to adversarial attacks. In International
Conference on Learning Representations (ICLR) , 2018. 1
[31] Laurent Meunier, Blaise J Delattre, Alexandre Araujo, and
Alexandre Allauzen. A dynamical system perspective for
Lipschitz neural networks. In International Conference on
Machine Learing (ICML) , 2022. 3, 11, 14, 22
[32] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and
Yuichi Yoshida. Spectral normalization for generative ad-
versarial networks. In International Conference on Learning
Representations (ICLR) , 2018. 3, 11
24582
[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
ing Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-
son, Andreas Kopf, Edward Yang, Zachary DeVito, Mar-
tin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Py-
torch: An imperative style, high-performance deep learning
library. In Conference on Neural Information Processing
Systems (NeurIPS) . 2019. 4
[34] Bernd Prach and Christoph H Lampert. Almost-orthogonal
layers for efficient general-purpose Lipschitz networks. In
European Conference on Computer Vision (ECCV) , 2022. 2,
3, 5, 14
[35] S Singla and S Feizi. Fantastic four: Differentiable bounds
on singular values of convolution layers. In International
Conference on Learning Representations (ICLR) , 2021. 3,
11
[36] Sahil Singla and Soheil Feizi. Skew orthogonal convolutions.
InInternational Conference on Machine Learing (ICML) ,
2021. 2, 3, 14
[37] Sahil Singla and Soheil Feizi. Improved techniques for de-
terministic l2 robustness. Conference on Neural Information
Processing Systems (NeurIPS) , 2022. 14
[38] Leslie N Smith and Nicholay Topin. Super-convergence:
Very fast training of neural networks using large learning
rates. In Artificial intelligence and machine learning for
multi-domain operations applications , 2019. 15
[39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-
triguing properties of neural networks. In International Con-
ference on Learning Representations (ICLR) , 2014. 1
[40] Asher Trockman and J Zico Kolter. Orthogonalizing convo-
lutional layers with the Cayley transform. In International
Conference on Learning Representations (ICLR) , 2021. 2, 3,
5, 14, 23
[41] Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama.
Lipschitz-margin training: Scalable certification of pertur-
bation invariance for deep neural networks. Conference on
Neural Information Processing Systems (NeurIPS) , 2018. 6
[42] Ruigang Wang and Ian Manchester. Direct parameterization
of Lipschitz-bounded deep networks. In International Con-
ference on Machine Learing (ICML) , 2023. 3, 11, 12
[43] Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-
Jui Hsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon.
Towards fast computation of certified robustness for relu net-
works. In International Conference on Machine Learing
(ICML) , 2018. 2
[44] Eric Wong and Zico Kolter. Provable defenses against adver-
sarial examples via the convex outer adversarial polytope.
InInternational Conference on Machine Learing (ICML) ,
2018. 2
[45] Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein,
Samuel Schoenholz, and Jeffrey Pennington. Dynamical
isometry and a mean field theory of CNNs: How to train
10,000-layer vanilla convolutional neural networks. In In-
ternational Conference on Machine Learing (ICML) , 2018.
3
[46] Xiaojun Xu, Linyi Li, and Bo Li. Lot: Layer-wise orthogonal
training on improving l2 certified robustness. Conference onNeural Information Processing Systems (NeurIPS) , 2022. 2,
3, 8, 14, 23
[47] Tan Yu, Jun Li, Yunfeng Cai, and Ping Li. Constructing or-
thogonal convolutions in an explicit manner. In International
Conference on Learning Representations (ICLR) , 2021. 3,
11, 12
24583
