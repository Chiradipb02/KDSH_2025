TextureDreamer: Image-guided Texture Synthesis through Geometry-aware
Diffusion
Yu-Ying Yeh13Jia-Bin Huang23Changil Kim3Lei Xiao3Thu Nguyen-Phuoc3Numair Khan3
Cheng Zhang3Manmohan Chandraker1Carl S Marshall3Zhao Dong3Zhengqin Li3
1University of California, San Diego2University of Maryland, College Park3Meta Reality Lab
Generated texturesInput Images
Target 3D shape
Generated texturesInput ImagesInput Images
Generated textures
Figure 1. Texture transfer from sparse images. Given a small number of images and a target mesh, our method synthesizes geometry-
aware texture that looks similar to the input appearances for diverse objects.
Abstract
We present TextureDreamer, a novel image-guided tex-
ture synthesis method to transfer relightable textures from a
small number of input images (3 to 5) to target 3D shapes
across arbitrary categories. Texture creation is a pivotal
challenge in vision and graphics. Industrial companies hire
experienced artists to manually craft textures for 3D assets.
Classical methods require densely sampled views and ac-
curately aligned geometry, while learning-based methods
are conﬁned to category-speciﬁc shapes within the dataset.
In contrast, TextureDreamer can transfer highly detailed,
intricate textures from real-world environments to arbi-
trary objects with only a few casually captured images, po-
tentially signiﬁcantly democratizing texture creation. Our
core idea, personalized geometry-aware score distillation
(PGSD), draws inspiration from recent advancements in
diffuse models, including personalized modeling for tex-
ture information extraction, score distillation for detailed
appearance synthesis, and explicit geometry guidance with
ControlNet. Our integration and several essential modiﬁca-
tions substantially improve the texture quality. Experimentson real images spanning different categories show that Tex-
tureDreamer can successfully transfer highly realistic, se-
mantic meaningful texture to arbitrary objects, surpassing
the visual quality of previous state-of-the-art. Project page:
https://texturedreamer.github.io
1. Introduction
High-quality 3D content is indispensable for a wide range
of critical applications, including AR/VR, robotics, ﬁlm,
and gaming. In recent years, remarkable progress has been
made in democratizing 3D content creation pipelines, fa-
cilitated by advancements in 3D reconstruction [ 41,44]
and generative models [ 18,62]. While substantial atten-
tion has been devoted to exploring the geometry compo-
nent [8,12,67] and neural implicit representations [ 46],
such as NeRF [ 41], creation of high-quality textures is rel-
atively under-explored. Textures are pivotal in creating re-
alistic, highly detailed appearances and are integral to vari-
ous graphics pipelines, where industry has traditionally re-
lied on professional, experienced artists to craft textures.
This process usually involves manually authoring procedu-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4304
there is a green chair with a leaf print on it
Input imageImage Captioning via BLIPTarget 3D shapeText-guided texturing [TEXTure] Image-guided texturing [Ours]Figure 2. Limitation of text-guided texturing. Compared to text-
guided texturing method which requires a captioning method to
generate a text prompt which might not express all the details of
the image, image-based guided texturing can be more effective and
more expressive. Image captioning is predicted by BLIP [ 33], text-
guided texturing is generated via TEXTure [ 55], and image-guided
result is from our method.
ral graphs [ 1] and UV maps, making it expensive and in-
efﬁcient. Automatically transferring the diverse visual ap-
pearance of objects around us to the texture of any target
geometry would thus be highly beneﬁcial.
We present TextureDreamer , a novel framework to cre-
ate high-quality relightable textures from sparse images.
Given 3 to 5 randomly sampled views of an object, we
can transfer its texture to an target geometry that may come
from a different category. This is an extremely challenging
problem, as previous texture creation methods usually ei-
ther require densely sampled views with aligned geometry
[3,32,71], or can only work for category-speciﬁc shapes
[4,21,48,61]. Our framework draws inspiration from
recent advancements in diffusion-based generative models
[23,62,63]. Trained on billions of text-image pairs, these
diffusion models enable text-guided image generation with
extraordinary visual quality and diversity [ 54]. Pioneering
works have applied these pre-trained 2D diffusion models
to text-guided 3D content creation [ 35,49,66]. However,
a common limitation among those methods is that text-only
input may not be sufﬁciently expressive to describe com-
plex, detailed patterns, as demonstrated in Figure 2. In con-
trast to text-guided methods, we effectively extract texture
information from a small set of input images by ﬁne-tuning
the pre-trained diffusion model with a unique text token
[16,57]. Our framework, therefore, addresses the challenge
of accurately describing complex textures.
The Score Distillation Sampling (SDS) [ 49,65] is one
core element that bridges pre-trained 2D diffusion models
with 3D content creation. It is widely used to generate and
edit 3D contents by minimizing the discrepancy between
the distribution of rendered images and the distribution de-
ﬁned by the pre-trained diffusion models [ 35,38]. Despite
its popularity, two well-known limitations impede its abil-
ity to generate high-quality textures. First, it tends to create
over-smoothed and saturated appearances due to the unusu-ally high classiﬁer-free guidance necessary for the method
to converge. Second, it lacks the knowledge to generate a
3D-consistent appearance, often resulting in multi-face ar-
tifacts and mismatches between textures and geometry.
We propose two key design choices to tackle these chal-
lenges. Instead of using SDS, we build upon Variational
Score Distillation (VSD) in our optimization approach,
which can generate much more photorealistic and diverse
textures. Initially introduced in ProliﬁcDreamer [ 66], VSD
treats the whole 3D representation as a random variable and
aligns its distribution with the pre-trained diffusion model.
It does not need a large classiﬁer-free guidance weight to
converge, which is essential to create a realistic and diverse
appearance. However, na ¨ıvely applying VSD update does
not sufﬁce for generating high-quality textures in our ap-
plication. We identify a simple modiﬁcation that can im-
prove texture quality while slightly reducing the compu-
tational cost. Additionally, VSD alone cannot fully solve
the 3D consistency issue. Fine-tuning on sparse inputs
makes converging harder, as observed by previous work
[53]. We, therefore, explicitly condition our texture gen-
eration process on geometry information extracted from the
given mesh by injecting rendered normal maps into the ﬁne-
tuned diffusion model through the ControlNet [ 70] archi-
tecture. Our framework, designated as personalized geome-
try aware score distillation (PGSD), can effectively transfer
highly detailed textures to diverse geometry in a semanti-
cally meaningful and visually appealing manner. Exten-
sive qualitative and quantitative experiments demonstrate
that our framework substantially outperforms state-of-the-
art texture-transfer methods.
2. Related Works
Texture synthesis and reconstruction Classical texture
creation methods involve sampling from a distribution de-
rived from the neighborhood [ 13,28], tiling repetitive pat-
terns [ 29,56], or with generative approaches [ 43,72].
These methods fall short in creating semantic meaningful
textures. Texture reconstruction by fusing multi-view im-
ages onto the object surfaces [ 3,32,71] requires highly ac-
curate geometry reconstruction. Numerous learning-based
methods were proposed to learn texture creation from large-
scale 3D datasets [ 4,11,21,48,61] but are conﬁned to
speciﬁc categories within the dataset. Recent works also
use CLIP model [ 52] for text-guided texture generation of
arbitrary objects [ 31,37,40,42], but their texture quali-
ties are usually low. In contrast, TextureDreamer can cre-
ate semantically meaningful, high-quality textures for ar-
bitrary objects using uncorrelated sparse images. Tradi-
tionally, textures are represented as a 2D image and pro-
jected to object surfaces through UV mapping. Leverag-
ing the recent progress in neural implicit representation, our
4305
method, along with recent developments in inverse render-
ing [5,7,17,64] and 3D generation [ 7,17], represents tex-
ture as a neural implicit texture ﬁeld.
Diffusion models Diffusion models [ 62] have emerged
as the state-of-the-art generative models [ 23,63], demon-
strating exceptional visual quality [ 54]. Its training and
inference involve iteratively adding noise with different
variances and denoise the data. Trained on internet-scale
image-text pair datasets [ 54], these pre-trained models ex-
hibit unprecedented capability in text-guided image syn-
thesis and have proven successful in various image editing
tasks. Recent works also manage to ﬁne-tune pre-trained
diffusion models on much smaller datasets or even a few im-
ages to facilitate customized/personalized image synthesis
[57] and image generation conditioned on multi-modal data
[70], such as normal and semantic maps. Building upon this
progress, TextureDreamer can effectively extract texture in-
formation from sparse views and transfer it to a novel target
object in a geometry-aware manner.
3D generation with 2D diffusion priors Diffusion-based
3D content creation has very recently gained substantial in-
terest. Several methods directly train 3D diffusion mod-
els to generate 3D content in various representations, in-
cluding point cloud [ 36], neural radiance ﬁled [ 26], hyper-
network [ 14] and texture [ 69]. Others utilize pre-trained
2D diffusion models by either progressively fusing gener-
ated images from different views [ 2,6,9,55] or optimiz-
ing the 3D representation through score distillation sam-
pling [ 35,38,49] and its improved variations [ 27,66].
While many methods concentrate on text-guided 3D gener-
ation, fewer attempt to leverage diffusion models to gener-
ate 3D content from images. A number of concurrent works
ﬁne-tune 2D diffusion models on large-scale 3D datasets
for sparse view reconstruction [ 50,60], primarily focusing
on whole 3D object reconstruction. In contrast, Texture-
Dreamer targets transferring textures from a small number
of images to a target 3D shape with unmatched geometry.
Dreambooth3D [ 53] and TEXTure [ 55] extract information
from sparse views into a new text token and ﬁne-tuned dif-
fusion model weights, which can be used to generate per-
sonalized 3D object or texture unseen objects. Texture-
Dreamer employs a similar method to extract information
from sparse images. However, it differs from prior works
on utilizing the extracted information for texture generation,
leading to improvements in consistency and photorealism.
3. Method
We propose TextureDreamer, a framework which synthe-
sizes geometry-aware texture for a given mesh with appear-
ance similar to 3-5 input images of an object. In Section 3.1,
we ﬁrst introduce preliminaries on Dreambooth [ 57], Con-
trolNet [ 70] and score distillation sampling [ 49,65,66]. InSection 3.2, we propose personalized geometry-aware score
distillation (PGSD), which is our core technical contribu-
tion that enables high-quality image-guided texture transfer
from sparse images to arbitrary geometries.
3.1. Preliminaries
Dreambooth [57] is a simple yet effective method to ﬁne-
tune pre-trained text-to-image diffusion models on a small
number of input images for personalized text-guided im-
age generation. It stores the subject’s appearance into the
diffusion model weights with a speciﬁc text-token “[V]” .
Dreambooth is ﬁne-tuned with two loss functions. Recon-
struction loss is standard diffusion denoising supervision on
the input images. Class-speciﬁc prior preservation loss is
proposed to avoid language drift and loss of diversity caused
by ﬁne-tuning. It further supervises the pre-trained model
with a large number of its own generated examples. Tex-
tureDreamer uses DreamBooth to distill texture information
from input images. Instead of image synthesis, we apply the
distilled information to a 3D object with different geometry.
ControlNet [70] proposes a novel architecture that adds
spatial conditioning control to pre-trained diffusion models.
The key insight is to reuse the large number of diffusion
model parameters trained on billions of images and insert
small convolution networks into the model with window
size 1 and zero-initialized weights. It enables robust ﬁne-
tuning performance on small datasets with different types of
2D conditions, such as depth, normal, and edge maps. We
utilize ControlNet models to ensure that our created textures
are aligned with the given geometry.
Score Distillation Sampling [49,65] is the core compo-
nent of numerous methods that use pre-trained 2D diffusion
models for 3D content creation [ 10,35,49]. It optimizes
the 3D representation by pushing its rendered images to a
high-dimensional manifold modeled by the pre-trained dif-
fusion model. Let ✓be the 3D representation and ✏ be the
pre-trained diffusion model. The gradient back-propagated
to the parameter ✓is
r✓LSDS(✓),Et,✏
w(t)(✏ (xt,yc,t) ✏)@g(✓, c)
@✓ 
,
where !(t)is the weight coefﬁcient, yis the text input, yc
denotes view-dependent conditioning, tis the time step, c
is the camera pose, g(·)is a differentiable renderer, xtis
the noisy image computed by adding noise to the rendered
image x=g(✓, c)with variance dependent on time t. De-
spite its wide usage, SDS requires a much higher weight
than normal classiﬁer-free guidance [ 22] to converge, over-
smoothed and oversaturated appearance. To overcome this
issue, Wang et al. [ 66] propose an improved version, called
variational score distillation (VSD), which can converge
with standard classiﬁer-free guidance. VSD treats the whole
4306
TextureDreamerOptimizationPhase
TextureExtractionPhase
AlbedoMetallicRoughness
3DMeshOptimizedBRDFField
Rendering
A photo of [V] object
Camera Extrinsic3DMesh
!DifferentiableRenderer
"CameraEncoder+Noise
BRDFField
Generic
Diffusion
Model#
Personalized
Diffusion Model
for [V] Object !!∗ExtractTextureMapsDreamboothfinetuningInput Images
Rendered Surface NormalRendered ImageText Promptℳ"
#$!(⋅)(())
*"x,#-
BackpropagationBackpropagation
./0*=(2,4)
Figure 3. Overview of TextureDreamer , a framework which synthesizes texture for a given mesh with appearance similar to 3-5 input
images of an object. We ﬁrst obtain personalized diffusion model  with Dreambooth [ 57] ﬁnetuning on input images. The spatially-
varying bidirectional reﬂectance distribution (BRDF) ﬁeld f✓for the 3D mesh Mis then optimized through personalized geometric-aware
score distillation (PGSD) (detailed in Section 3.2). After optimization ﬁnished, high-resolution texture maps corresponding to albedo,
metallic, and roughness can be extracted from the optimized BRDF ﬁeld.
3D representation ✓as a random variable and minimizes the
KL divergence between ✓and the distribution deﬁned by
the pre-trained diffusion model. It involves ﬁne-tuning a
LoRA [ 24] network ✏ (and a camera encoder ⇢which em-
beds camera pose cas an condition input to ✏ ) to denoise
the noisy images generated from 3D representation ✓
min
 Et,✏,c⇥
||✏ (xt,y ,t ,c ) ✏||2
2⇤
(1)
The gradient to the 3D representation ✓is then computed as
Et,✏,c
w(t)(✏ (xt,yc,t) ✏ (xt,y ,t ,c ))@g(✓, c)
@✓ 
.
(2)
While VSD signiﬁcantly improves both visual quality and
diversity of generated 3D contents, it cannot address the 3D
consistency issue due to the inherent lack of 3D knowledge,
leading to multi-face errors and mismatches between geom-
etry and textures. We address this challenge by explicitly in-
jecting geometry information to make our diffusion model
geometry aware.
3.2. Personalized Geometry-aware Score Distilla-
tion (PGSD)
Problem setup. We illustrate our method in Figure 3.
The inputs to our framework include a small set of im-
ages (3 to 5) casually captured from different views {I}K
k=1and a target 3D mesh M. The outputs of our frame-
work are relightable textures transferred from image set
{I}K
k=1toMin a semantically meaningful and visually
pleasing manner. Our relightable textures are parameter-
ized as standard microfacet bidirectional reﬂectance distri-
bution (BRDF) model [ 25], which consists of 3 parameters,
diffuse albedo a, roughness r, and metallic m. We delib-
erately do not optimize normal maps as it encourages the
pipeline to fake details that are inconsistent with mesh M.
Following the recent trend of neural implicit representation
[20,44,45], during optimization, we represent our texture
as a neural BRDF ﬁeld f✓(v):v2R3!,a ,r ,m 2R5,
where vis an arbitrary point sampled on the surface of M
andf✓consists of a multi-scale hash encoding and a small
MLP. We ﬁnd such an implicit representation can better reg-
ularize the optimization process, leading to smoother tex-
tures. However, given the UV mapping of M, our rep-
resentation can also be converted to standard 2D texture
maps that are compatible with standard graphics pipelines,
by querying every 3D point corresponding to each texel, as
shown on the right-hand side of Figure 3.
Personalized texture information extraction. We follow
Dreambooth [ 57] to extract texture information from sparse
images. To be speciﬁc, we ﬁne-tune a personalized diffu-
sion model on input images with a text prompt y,“A photo
of [V] object” , where “[V]” is a unique identiﬁer to de-
4307
scribe the input object. Compared to the alternative textual
inversion method [ 16], we observe that Dreambooth con-
verges faster and can better preserve intricate texture pat-
terns, possibly due to its larger capacity. We ﬁrst mask out
the background of the target object with a white color. For
the reconstruction loss, we resize the shorter edge of input
images to 512 and randomly crop 512x512 patches for train-
ing. We do not apply class-speciﬁc prior preservation loss,
as we hope our Dreambooth ﬁnetuning model can gener-
alize to other categories. We also experiment with different
variations, including jointly ﬁne-tuning the text encoder and
replacing the diffusion denoising network with a pre-trained
ControlNet, but do not observe any improvements.
Geometry-aware score distillation Once we ﬁnish ex-
tracting texture information with Dreambooth, we trans-
fer the information to mesh Mby adopting the ﬁne-tuned
Dreambooth model as the denoising network ✏ for score
distillation sampling. Speciﬁcally, we choose VSD instead
of the original SDS because of its superior ability to gen-
erate highly realistic and diverse appearances. To render
images xfor VSD gradient computation, we follow Fanta-
sia3D [ 10] to pre-select a ﬁxed HDR environment map E
as illumination and use Nvdiffrast [ 30] as our differentiable
renderer. We set the object background to be a constant
white color to match the input images for Dreambooth train-
ing. We observe this can help achieve better color ﬁdelity
compared to random color or neutral background.
However, simply replacing SDS with VSD cannot ad-
dress the limitation of lacking 3D knowledge in 2D diffu-
sion models. We thus propose geometry-aware score dis-
tillation, where we inject geometry information extracted
from mesh Minto our personalized diffusion model ✏ 
through a pre-trained ControlNet conditioned on normal
maps krendered from M. This augmentation signiﬁcantly
boosts 3D consistency of generated textures (see Figure 7).
With the ControlNet conditioning, the pillow texture from
the input images can be accurately matched to the target
shape, despite the shape mismatch. We experiment with
different ControlNet conditions and show that normal con-
ditions can best prevent texture-geometry mismatch.
Letx=g(✓, c)be the rendered image under a ﬁxed envi-
ronment map from camera pose cwith the extracted BRDF
maps a✓,r✓,m ✓. The gradient of proposed Personalized
Geometry-aware Score Distillation (PGSD) to optimize the
MLP parameter ✓of BRDF ﬁeld is:
r✓LPGSD(✓)
,Et,✏,c[w(t)(✏ (xt;yc,k ,t) ✏ (xt;y, k, t, c ⇢))@x
@✓],
where xt=↵tx+ t✏is the rendered image xperturbed
by noise ✏⇠N(0,I)at time t,c⇢is the embedding of the
camera extrinsic cencoded by a learnable camera encoder
⇢,✏ and✏ are the ﬁne-tuned personalized diffusion modeland the generic diffusion model pretrained on a large-scale
dataset, respectively. Both models are augmented with Con-
trolNet conditioned on normal map k, as shown in the yel-
low part underneath the diffusion model in Figure 3.
We found that our method does not beneﬁt from
classiﬁer-free guidance (CFG) [ 22], probably because the
personalized model ✏ has been ﬁne-tuned on a small num-
ber of images. Since our goal is to faithfully transfer input
appearance to target shape, it is not necessary to have CFG
to increase the diversity. Similar observation can be found
in recent literature [ 58].
We additionally identify several important design
choices through extensive experiments. First, it is impor-
tant to initialize the ✏ in Eq. 1with original pre-trained
diffusion model weights while the Dreambooth weight will
remove texture details. This is probably because the Dream-
booth ﬁne-tuning process makes the diffusion model over-
ﬁt to a small training set, as pointed out by previous work
[53]. Moreover, we ﬁnd that removing the LoRA weights
can substantially improve texture ﬁdelity. Similar difﬁcul-
ties in training LoRA were also reported in [ 59]. We there-
fore implement our personalized geometry-aware score dis-
tillation loss LPGSD by removing the LoRA structure in ✏ 
and only keeping the camera embedding, achieving the best
quality. We show more comparisons in Figure 7.
4. Experiment
4.1. Experimental setup
Dataset. We conduct our experiments on 4 categories of
objects: sofa, bed, mug/bowl, and plush toy. For each cat-
egory, we select 8 instances of objects and create a small
image set by casually sampling 3 to 5 views surrounding
the object, resulting in 32 image sets in total. For every
image in the 32 image sets, we apply U2-Net [ 51] to obtain
the foreground mask automatically or use a semi-auto back-
ground removal application1to obtain more accurate masks.
We perform texture transfer for each image set to diverse
meshes including but not limited to same category shapes,
different category shapes, or even geometry with different
genus numbers. To test our texture-transferring framework,
we select 3 meshes for each of the 4 categories that are dis-
similar to the captured image sets. We acquire these 3D
meshes from 3D-FUTURE [ 15] and online repositories.23.
We run intra-class texture transfer for all 4 categories of ob-
jects and also run inter-class texture transfer between bed
and chair, to test our method’s generalization ability.
Implementation details. We implement our framework
based on PyTorch [ 47] and Threestudio [ 19]. We use latent
diffusion and ControlNet v1.1 as our pre-trained diffusion
1https://www.remove.bg/upload
2https://www.cgtrader.com/
3https://sketchfab.com/
4308
Input ImagesOur Transfer Results on Diverse ObjectsMugs
Bed
Sofa
Plush Toy
Figure 4. Image-guided transfer results from four categories (beds, sofas, plush toys, and mugs) of image sets to diverse objects. Our
method can be applied to a wide range of object types and transfer the textures to diverse object shapes.
model and ControlNet respectively. In all our experiments,
we set the classiﬁer-free guidance weight of LPGSD as 1.0
(equivalent to setting !=0 in the original CFG formu-
lation). Following DreamFusion [ 49], we also apply view-
dependent conditioning to the input text prompt. The BRDF
ﬁeld is parameterized with an MLP using hash-grid posi-
tional encoding [ 44], following prior works [ 10,66]. Our
camera encoder consists of two linear layers that project the
camera extrinsic to a latent vector of 1,280 dimensions to
be fused with time and text embedding in U-Net. We em-
pirically set the learning rate to 0.01for encoding, 0.001for
MLP, and 0.0001 for camera encoder for all experiments.4.2. Baseline methods
Latent-paint [ 38] and TEXTure [ 55] are two recent text-
guided texturing methods with 2D diffusion prior. They
also demonstrate the capability of texturing meshes from
images. Latent-paint leverages the Texture Inversion [ 16]
to extract image information into text embedding and dis-
tills the texture with SDS. TEXTure ﬁrst ﬁnetunes the pre-
trained diffusion model by combining Texture Inversion and
Dreambooth [ 57] and use this ﬁne-tuned model to synthe-
size texture with an iterative mesh painting algorithm. Fol-
lowing TEXTure, we augment the input images with a ran-
dom color background. We closely follow the original im-
plementation of baseline methods to run the experiments.
4309
Figure 5. Example of cross-category texture synthesis results. Input images (top row) can guide the texture synthesis (bottom row) for
shapes which is not in the same category.
TEXTureOursLatent-PaintInput Images
Figure 6. Comparison between baseline methods. Compared
with Latent-Paint [ 38] and TEXTure [ 55], our method can synthe-
size seamless and geometry-aware textures which are compatible
with the target mesh geometry.
Table 1. User study on image-guided texture transfer.
Ours preferred over
Latent-Paint TEXTure
Image Fidelity 71.82% 69.43%
Texture Photorealism 77.03% 85.52%
Shape-Texture Consistency 78.49% 85.16%
Table 2. Quantitative evaluation on image-guided texturing.
CLIP similarity "
Latent-Paint [ 38] 0.7969
TEXTure [ 55] 0.7988
Ours 0.8296
4.3. Image-guided texture transfer
Qualitative evaluation Our method can synthesize
geometry-aware and seamless textures that has similar pat-
terns and styles as the input for diverse object geometry. In
Figure 4, textures can be synthesized for the same category.
It can be diverse under different seeds as shown in Figure 10
of supplementary material. We also demonstrate that our
method can synthesize textures across different categories .
In Figure 1, we show texture synthesis results from imagesof sofa to bed shapes, and vice versa. Our method is also ca-
pable of performing texture synthesis across a broader range
of different categories. In Figure 5, high-quality and realis-
tic textures can be synthesized across chair, mug, plush toy,
or even non-rigid objects such as bags or clothes. It can also
be used to synthesize texture for shapes captured from 3D
scanner, as shown in Figure 13of supplementary material.
In Figure 6, we qualitatively compare our method with
baseline methods. Two views are shown in each example.
Latent-Paint tends to generate textures with colors and pat-
terns that are different from input images. TEXTure can
preserve the color and texture better than Latent-Paint, but
the texture contains visible seams (possibly due to the itera-
tive painting). Our results method can reason the semantics
of the geometry ( e.g. the positions of eyes) and demonstrate
higher quality, seamless, and geometry-aware texturing re-
sults with higher ﬁdelity from the input images.
Quantitative evaluation It is non-trivial to perform quanti-
tative comparisons for texture transfer due to the shape dif-
ference between geometry and photos. We perform a user
study to evaluate transfer ﬁdelity, texture photorealism, and
texture-geometry compatibility across baselines by asking
users the following questions: 1) Which one has the texture
that looks more similar to input images? 2) Which one has
a texture which looks more like a real object? 3) Which one
has the texture which is more compatible with the meshes?
(Which texture painted more ﬁtted to the geometry?) We
conduct a user study with Amazon Turk with three sepa-
rate tasks. For each task, we ask each user 24 questions.
Each question is a forced single-choice selection with two
options among our and one baseline result with the rendered
images from the same 4sampled views and is evaluated by
20 different users. We only show input photos for the ﬁrst
similarity question, and hide the input photos for the other
two questions to make the user focus on texture quality. We
summarize the results in Table 1. Our results show signiﬁ-
cant preference by the users in terms of image ﬁdelity, tex-
ture photorealism, and shape-texture consistency.
We also evaluate the similarity via image-based CLIP
feature [ 42] between reference and the rendered images of
synthesized textures. The CLIP similarity has been ap-
4310
w/ ControlNet (Depth)w/o ControlNetMeshGeometryw/o LoRAremovedSDSOurs
Input ImagesFigure 7. Ablation study. With ControlNet conditioned on normal maps, the result has the best texture-geometry consistency. Without
ControlNet or with depth-based ControlNet, the results suffer from texture-geometry misalignment. Using SDS loss leads to blurry textures.
Without the LoRA module removed, the results tend to remove the existing texture from the personalized diffusion model. Our full method
can synthesize accurate texture which is similar to input appearances.
plied to material matching [ 68] and stylization [ 39]. A
good transfer should transfer only the texture from images
and should take into account the target shape geometry and
transfer the texture semantically. For example, the transfer
should be painted with respect to each part of the shape. We
use our evaluation set to compute the comparison. For each
image set and target 3D mesh pair, we compute the aver-
age of the metric among each reference image and each of
rendered image from 4sampled views ( i.e. left front, right
front, left back, and right back). We average the CLIP sim-
ilarity across all (image set, mesh) pairs. Table 2shows our
method has the highest CLIP similarity.
Ablation study We qualitatively perform an ablation study
in Figure 7. The results suffer from geometry-texture mis-
alignment without ControlNet or the depth-based Control-
Net. Only normal-based ControlNet can accurately control
the synthesized texture to be consistent with the input mesh
geometry. We validate the importance of score distillation
sampling. Only using SDS in our framework cannot achieve
enough ﬁdelity and the result tends to be blurry. Without
LoRA removed (which is usually optimized with vanilla
VSD), the optimization tends to make the distribution di-
verge from the personalized diffusion model. It makes the
output contain less original texture but more irrelevant pat-
terns from the input. We hypothesize that optimizing LoRA
weights with a text condition containing a rare identiﬁer
tends to drive the distribution of rendered images to have
a rare appearance. Additional ablation study is provided in
the supplementary material in Figure 14and Table 3.
Number of images We evaluate various number of in-
put images. In Figure 8, a single image cannot provide
sufﬁcient information for texturing, while using 11 images
doesn’t show advantages in terms of texture quality.
1 front view1 back view5views (ours)11 views
backfrontFigure 8. Number of images. Input images are from Figure 6.
InputResult
Figure 9. Limitations. Our method may bake-in lighting into tex-
ture, have Janus problem when lacking enough input viewpoints,
and ignore special and non-repeated patterns from the input.
5. Discussions
We proposed a framework to transfer high-quality texture
from input images to an arbitrary shape. There are still some
limitations, as shown in Figure 9. Our method may not be
able to transfer special and non-repeated texture to the tar-
get shapes. Our method tends to bake in lighting to texture
when there are strong specular highlights in the input im-
ages. Janus problem might appear when the viewpoints of
input images do not cover the entire object. Nevertheless,
we believe that our method can be the ﬁrst step to tackling
this challenging problem and will make an impact on the
3D content creation community.
Acknowledgement We thank Google PhD Fellowship sup-
ports the research.
4311
References
[1]Adobe substance 3d. https://docs.substance3d.
com/sat. 2
[2]Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil
Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d
human digitization with shape-guided diffusion. In SIG-
GRAPH Asia , 2023. 3
[3]Sai Bi, Nima Khademi Kalantari, and Ravi Ramamoorthi.
Patch-based optimization for image-based texture mapping.
ACM Trans. Graph. , 36(4):106–1, 2017. 2
[4]Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai.
Mesh2tex: Generating mesh textures from image queries.
arXiv preprint arXiv:2304.05868 , 2023. 2
[5]G. Cai, K. Yan, Z. Dong, I. Gkioulekas, and S. Zhao.
Physics-based inverse rendering using combined implicit
and explicit geometries. Computer Graphics Forum , 41(4):
129–138, 2022. 3
[6]Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp,
and Kangxue Yin. Texfusion: Synthesizing 3d textures with
text-guided image diffusion models. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 4169–4181, 2023. 3
[7]Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efﬁcient
geometry-aware 3d generative adversarial networks. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 16123–16133, 2022. 3
[8]Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012 , 2015. 1
[9]Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey
Tulyakov, and Matthias Nießner. Text2tex: Text-driven
texture synthesis via diffusion models. arXiv preprint
arXiv:2303.11396 , 2023. 3
[10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.
Fantasia3d: Disentangling geometry and appearance for
high-quality text-to-3d content creation. arXiv preprint
arXiv:2303.13873 , 2023. 3,5,6
[11] Zhiqin Chen, Kangxue Yin, and Sanja Fidler. Auv-net:
Learning aligned uv maps for texture transfer and synthesis.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 1465–1474, 2022. 2
[12] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese. 3d-r2n2: A uniﬁed approach
for single and multi-view 3d object reconstruction. In Com-
puter Vision–ECCV 2016: 14th European Conference, Am-
sterdam, The Netherlands, October 11-14, 2016, Proceed-
ings, Part VIII 14 , pages 628–644. Springer, 2016. 1
[13] Alexei A Efros and Thomas K Leung. Texture synthesis
by non-parametric sampling. In Proceedings of the seventh
IEEE international conference on computer vision , pages
1033–1038. IEEE, 1999. 2
[14] Ziya Erkoc ¸, Fangchang Ma, Qi Shan, Matthias Nießner,
and Angela Dai. Hyperdiffusion: Generating implicitneural ﬁelds with weight-space diffusion. arXiv preprint
arXiv:2303.17015 , 2023. 3
[15] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang
Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d fur-
niture shape with texture. International Journal of Computer
Vision , 129:3313–3337, 2021. 5
[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or.
An image is worth one word: Personalizing text-to-image
generation using textual inversion. In The Eleventh Interna-
tional Conference on Learning Representations , 2022. 2,5,
6
[17] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,
Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja
Fidler. Get3d: A generative model of high quality 3d tex-
tured shapes learned from images. Advances In Neural In-
formation Processing Systems , 35:31841–31854, 2022. 3
[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM , 63(11):139–144, 2020. 1
[19] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian
Laforte, Vikram V oleti, Guan Luo, Chia-Hao Chen, Zi-
Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.
threestudio: A uniﬁed framework for 3d content generation.
https://github.com/threestudio-project/
threestudio , 2023. 5
[20] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.
Shape, light, and material decomposition from images using
monte carlo rendering and denoising. Advances in Neural
Information Processing Systems , 35:22856–22869, 2022. 4
[21] Paul Henderson, Vagia Tsiminaki, and Christoph H Lampert.
Leveraging 2d data to learn textured 3d mesh generation. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 7498–7507, 2020. 2
[22] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 3,5
[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840–6851, 2020. 2,3
[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 4
[25] Brian Karis and Epic Games. Real shading in unreal engine
4.Proc. Physically Based Shading Theory Practice , 4(3):1,
2013. 4
[26] Animesh Karnewar, Andrea Vedaldi, David Novotny, and
Niloy J Mitra. Holodiffusion: Training a 3d diffusion model
using 2d images. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
18423–18433, 2023. 3
[27] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani
Lischinski. Noise-free score distillation. arXiv preprint
arXiv:2310.17590 , 2023. 3
[28] Johannes Kopf, Chi-Wing Fu, Daniel Cohen-Or, Oliver
Deussen, Dani Lischinski, and Tien-Tsin Wong. Solid tex-
4312
ture synthesis from 2d exemplars. In ACM SIGGRAPH 2007
papers , pages 2–es. 2007. 2
[29] Vivek Kwatra, Arno Sch ¨odl, Irfan Essa, Greg Turk, and
Aaron Bobick. Graphcut textures: Image and video synthe-
sis using graph cuts. Acm transactions on graphics (tog) , 22
(3):277–286, 2003. 2
[30] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,
Jaakko Lehtinen, and Timo Aila. Modular primitives for
high-performance differentiable rendering. ACM Transac-
tions on Graphics , 39(6), 2020. 5
[31] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven
photorealistic and robust 3d stylization via lighting decom-
position. Advances in Neural Information Processing Sys-
tems, 35:30923–30936, 2022. 2
[32] Marc Levoy, Kari Pulli, Brian Curless, Szymon
Rusinkiewicz, David Koller, Lucas Pereira, Matt Ginz-
ton, Sean Anderson, James Davis, Jeremy Ginsberg, et al.
The digital michelangelo project: 3d scanning of large
statues. In Proceedings of the 27th annual conference
on Computer graphics and interactive techniques , pages
131–144, 2000. 2
[33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uniﬁed
vision-language understanding and generation, 2022. 2
[34] Zhengqin Li, Yu-Ying Yeh, and Manmohan Chandraker.
Through the looking glass: Neural 3d reconstruction of
transparent shapes. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
1262–1271, 2020. 1
[35] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 300–309, 2023. 2,3
[36] Shitong Luo and Wei Hu. Diffusion probabilistic models for
3d point cloud generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 2837–2845, 2021. 3
[37] Yiwei Ma, Xiaoqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei
Wang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji.
X-mesh: Towards fast and accurate text-driven 3d styliza-
tion via dynamic textual guidance. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 2749–2760, 2023. 2
[38] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and
Daniel Cohen-Or. Latent-nerf for shape-guided generation
of 3d shapes and textures. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12663–12673, 2023. 2,3,6,7
[39] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and
Rana Hanocka. Text2mesh: Text-driven neural stylization
for meshes. arXiv preprint arXiv:2112.03221 , 2021. 8
[40] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and
Rana Hanocka. Text2mesh: Text-driven neural stylization
for meshes. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 13492–
13502, 2022. 2[41] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. Communications of the ACM , 65(1):99–106, 2021.
1
[42] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,
and Tiberiu Popa. Clip-mesh: Generating textured meshes
from text using pretrained image-text models. In SIGGRAPH
Asia 2022 conference papers , pages 1–8, 2022. 2,7
[43] Joep Moritz, Stuart James, Tom SF Haines, Tobias Ritschel,
and Tim Weyrich. Texture stationarization: Turning photos
into tileable textures. In Computer graphics forum , pages
177–188. Wiley Online Library, 2017. 2
[44] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Transactions on Graphics
(ToG) , 41(4):1–15, 2022. 1,4,6
[45] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,
Wenzheng Chen, Alex Evans, Thomas M ¨uller, and Sanja Fi-
dler. Extracting triangular 3d models, materials, and lighting
from images. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8280–
8290, 2022. 4
[46] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 165–174, 2019. 1
[47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
5
[48] Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aurelien
Lucchi. Learning generative models of textured 3d meshes
from real-world images. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 13879–
13889, 2021. 2
[49] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv
preprint arXiv:2209.14988 , 2022. 2,3,6
[50] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
rokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:
One image to high-quality 3d object generation using both
2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 ,
2023. 3
[51] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood De-
hghan, Osmar Zaiane, and Martin Jagersand. U2-net: Going
deeper with nested u-structure for salient object detection.
page 107404, 2020. 5
[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2
4313
[53] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,
Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kﬁr Aber-
man, Michael Rubinstein, Jonathan Barron, et al. Dream-
booth3d: Subject-driven text-to-3d generation. arXiv
preprint arXiv:2303.13508 , 2023. 2,3,5
[54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 ,1
(2):3, 2022. 2,3
[55] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,
and Daniel Cohen-Or. Texture: Text-guided texturing of 3d
shapes. In ACM SIGGRAPH 2023 Conference Proceedings ,
New York, NY , USA, 2023. Association for Computing Ma-
chinery. 2,3,6,7
[56] Carlos Rodriguez-Pardo and Elena Garces. Seamless-
gan: Self-supervised synthesis of tileable texture maps.
IEEE Transactions on Visualization and Computer Graph-
ics, 2022. 2
[57] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500–
22510, 2023. 2,3,4,6
[58] Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia,
Dmitry Lagun, Fredo Durand, William T Freeman, and Mark
Matthews. Alchemist: Parametric control of material proper-
ties with diffusion models. arXiv preprint arXiv:2312.02970 ,
2023. 5
[59] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,
Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao
Su. Zero123++: a single image to consistent multi-view dif-
fusion base model, 2023. 5
[60] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,
and Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-
eration. arXiv preprint arXiv:2308.16512 , 2023. 3
[61] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan,
Matthias Nießner, and Angela Dai. Texturify: Generating
textures on 3d shape surfaces. In European Conference on
Computer Vision , pages 72–88. Springer, 2022. 2
[62] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning , pages 2256–2265. PMLR, 2015.
1,2,3
[63] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. arXiv preprint arXiv:2011.13456 , 2020. 2,3
[64] Cheng Sun, Guangyan Cai, Zhengqin Li, Kai Yan, Cheng
Zhang, Carl Marshall, Jia-Bin Huang, Shuang Zhao, and
Zhao Dong. Neural-pbir reconstruction of shape, material,
and illumination. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) , 2023. 3
[65] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,
and Greg Shakhnarovich. Score jacobian chaining: Lifting
pretrained 2d diffusion models for 3d generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12619–12629, 2023. 2,3
[66] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Proliﬁcdreamer: High-ﬁdelity and
diverse text-to-3d generation with variational score distilla-
tion. arXiv preprint arXiv:2305.16213 , 2023. 2,3,6
[67] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and
Josh Tenenbaum. Learning a probabilistic latent space of
object shapes via 3d generative-adversarial modeling. Ad-
vances in neural information processing systems , 29, 2016.
1
[68] K. Yan, F. Luan, M. Ha ˇsan, T. Groueix, V . Deschaintre, and
S. Zhao. Psdr-room: Single photo to scene using differen-
tiable rendering. In ACM SIGGRAPH Asia 2023 Conference
Proceedings , 2023. 8
[69] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and
Xiaojuan Qi. Texture generation on 3d meshes with point-
uv diffusion. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 4206–4216, 2023. 3
[70] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 2,3
[71] Qian-Yi Zhou and Vladlen Koltun. Color map optimization
for 3d reconstruction with consumer depth cameras. ACM
Transactions on Graphics (ToG) , 33(4):1–10, 2014. 2
[72] Xilong Zhou, Milos Hasan, Valentin Deschaintre, Paul Guer-
rero, Kalyan Sunkavalli, and Nima Khademi Kalantari. Ti-
legen: Tileable, controllable material generation and cap-
ture. In SIGGRAPH Asia 2022 conference papers , pages
1–9, 2022. 2
4314
