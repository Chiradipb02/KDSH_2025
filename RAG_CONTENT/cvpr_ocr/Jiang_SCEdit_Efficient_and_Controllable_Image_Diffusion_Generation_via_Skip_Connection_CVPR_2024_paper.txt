SCEdit: Efficient and Controllable Image Diffusion
Generation via Skip Connection Editing
Zeyinzi Jiang Chaojie Mao Yulin Pan Zhen Han Jingfeng Zhang
Alibaba Group
{zeyinzi.jzyz, chaojie.mcj, yanwen.pyl, hanzhen.hz, zhangjingfeng.zjf }@alibaba-inc.com
CannyHEDDepthSegmentation(c) Controllable GenerationInpainting(b) Few-shot(a) Text-to-Image
Pose
Color
COCO Eﬃcient Tuning
Outpainting
A grey and white cat…
SD v1.5
3DAnimeFlat illustrationOil paintingSketchWatercolor
Figure 1. Images generated by SCEdit. With a small number of trainable parameters and low memory usage, SCEdit enables efficient
fine-tuning on specific datasets (left top) and supports transfer learning with a few samples (right top). Additionally, it adopts various
conditions as inputs for efficient controllable generation (middle), while individually learned conditional models combine effortlessly,
providing endless compositional possibilities (bottom).
Abstract
Image diffusion models have been utilized in various
tasks, such as text-to-image generation and controllable im-
age synthesis. Recent research has introduced tuning meth-
ods that make subtle adjustments to the original models,
yielding promising results in specific adaptations of foun-
dational generative diffusion models. Rather than modi-
fying the main backbone of the diffusion model, we delve
into the role of skip connection in U-Net and reveal that hi-
erarchical features aggregating long-distance information
across encoder and decoder make a significant impact on
the content and quality of image generation. Based on
the observation, we propose an efficient generative tuning
framework, dubbed SCEdit , which integrates and editsSkip Connection using a lightweight tuning module named
SC-Tuner. Furthermore, the proposed framework allows
for straightforward extension to controllable image syn-
thesis by injecting different conditions with Controllable
SC-Tuner, simplifying and unifying the network design for
multi-condition inputs. Our SCEdit substantially reduces
training parameters, memory usage, and computational ex-
pense due to its lightweight tuners, with backward propa-
gation only passing to the decoder blocks. Extensive exper-
iments conducted on text-to-image generation and control-
lable image synthesis tasks demonstrate the superiority of
our method in terms of efficiency and performance. Project
page: https://scedit.github.io/ .
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8995
1. Introduction
Building upon large-scale pre-trained image diffusion mod-
els [1–3, 14, 15, 30], researchers have focused on various
downstream tasks and applications, including text-to-image
generation [7–9, 27, 31, 32], controllable image synthesis
[18, 29, 33, 50] and image editing [4, 26, 37, 41, 48]. How-
ever, fully fine-tuning a foundation image diffusion model
often proves inefficient or even impractical in most cus-
tomized scenarios due to the constraints of limited training
data and computational resources.
Recently, efficient tuning methods [20, 21, 46] have
emerged as a practical solution by introducing additional
trainable structures on generative tasks. Nonetheless, many
of these popular efficient tuning methods still suffer from
substantial resource consumption as the network expands.
For instance, LoRA [17] typically adds the trainable low-
rank matrices to multi-head attention layers all across the U-
Net [36], and backward propagation is conducted through-
out the entire backbone, resulting in an accumulation of
gradients and increase in memory usage during training,
even more than that in fully fine-tuning. To address this
issue, we properly design our framework by strategically in-
serting all the trainable modules into the Skip Connections
(SCs) between the encoder and decoder of the U-Net, ef-
fectively decoupling the encoder from the backpropagation
process and significantly reducing computation and mem-
ory requirements, as shown in Fig. 2.
The SCs bridge the gap between distant blocks in the
U-Net architecture, facilitating the integration of informa-
tion over long distances and alleviating vanishing gradients.
Some works [19, 38, 44] have focused on exploring the ef-
ficacy of SCs in enhancing the training stability of the U-
Net, as well as in improving the generation quality [42]. In-
spired, we further investigate the potential of U-Net’s SCs
in adapting to new scenarios. To gain a deeper insight into
each SC within a pre-trained U-Net, we gradually remove
SCs and observe the subsequent changes in the value distri-
butions and feature maps across the blocks of the decoder
in Fig. 3. We first assess the diversity of information in the
final decoder’s output by visualizing the distribution of its
latent feature values. A higher variance signifies a greater
breadth of information, while a variance near zero indicates
a significant loss of detail. As we discard an increasing
number of SC from half to all, the variance of the final de-
coder output gradually decreases. In addition, when half of
the SCs are removed, there is a marked reduction in the level
of detailed structural information within the feature maps of
both the 6th and 11th decoder blocks. Eliminating all SCs
further intensifies the deterioration of information. These
trends further validate the significant impact of SC on the
generation of detailed structural information.
In light of this revelation, we propose a simple yet highly
efficient approach by SkipConnection Editing for image
SCEditSCEditFullLoRAT2I-AdapterControlNetControlNet-XSControlLoRA
Memory (GB)FID (Text-to-Image Generation)
FID (Controllable Image Synthesis)Figure 2. Performance and efficiency comparison on both text-
to-image generation (circular markers) and controllable image
synthesis (pentagonal markers) tasks. The marked area reflects
the relative amount of parameters.
0IN
6Out
6Out
6Out11Out
11Out
11Outorigin/in
drop half SC/outdrop all SC/outorigin/out
-0.50.00.25-0.25
Figure 3. The output distributions (left) and feature maps (right)
of pre-trained U-Net. Removing the skip connections from differ-
ent layers markedly affects the overall network output.
generation, dubbed SCEdit . Specifically, we introduce a
lightweight tuning module named SC-Tuner , which is de-
signed to edit the latent features within each SC of the pre-
trained U-Net for efficient tuning. Furthermore, we extend
the capabilities of SC-Tuner to facilitate controllable im-
age synthesis, which can accommodate various input condi-
tions using the Controllable SC-Tuner ( CSC-Tuner ). Our
SCEdit framework is adaptable to a broad spectrum of im-
age generation tasks using the proposed tuning modules and
by decoupling the encoder blocks in U-Net, it allows for ef-
ficient and flexible training, as it enables backward propa-
gation solely through the decoder block.
8996
We evaluate our SCEdit on efficient tuning of text-
to-image generation tasks as well as controllable image
synthesis tasks. For text-to-image generation tasks, our
approach outperforms existing methods on COCO2017
dataset in terms of FID score and qualitative results, while
also reducing memory consumption by 52% during the
training stage. Additionally, faster transfer and high-quality
results are achieved in the few-shot fine-tuning scenario.
On the controllable generation tasks, our approach can eas-
ily control the results under various conditional inputs and
show impressive results, while having lower computational
costs than existing methods. It utilizes merely 7.9% of the
parameters required by ControlNet and achieves a 30% re-
duction in memory usage.
2. Related work
Image Diffusion Models [15, 43, 44] have achieved the
state-of-the-art performance in sample quality of generative
image. Research on text-to-image diffusion models [8, 35]
incorporates text latent vectors as conditions. Some works
[18, 50] conduct controllable image generation by leverag-
ing various images as conditions, offering personalization,
customization, or task-specific image generation. However,
the scale and computational cost limit the applications of
image diffusion models.
Efficient Tuning [10, 16, 17, 21, 22] has become a popu-
lar solution in recent years, attracting significant attention.
It allows for easy adaptation of image diffusion models by
making light adjustments to the original pre-trained models.
LoRA [17] is to use low-rank matrices to learn weight off-
sets, which has proven effective in customized image gener-
ation scenarios. Other works, such as U-Tuning [20], Res-
Tuning [21], and MAM-Adapter [11], propose a unified
paradigm for efficient tuning methods, offering more op-
tions for tuning pre-trained models. Typically, some tuning
modules are integrated into the U-Net architecture of diffu-
sion models, but few works thoroughly analyze the key fac-
tors for improving the quality of generative images through
tuning modules.
U-Net is originally introduced for diffusion models in
DDPM [15], and it achieves tremendous success in the field
of generative tasks. Recently, there has been a lot of at-
tention focused on the effectiveness of skip connections in
U-Net. To address the issue of doubling signal variance
caused by skip connections and alleviate the oscillations of
U-Net, some methods [38, 44] propose rescaling all skip
connections by1√
2. ScaleLong [19] provides further theo-
retical analysis on why scaling the coefficients of skip con-
nections can help improve the training stability of U-Net
and using constant scaling or learnable scaling for better
stability. FreeU [42] recognizes that the main backbone of
U-Net plays a primary role in denoising, while the skip con-
nections introduce high-frequency features into the decodermodule. By leveraging the strengths of both components,
the denoising capability of U-Net can be enhanced. In-
spired by previous works, we aim to analyze the roles of
skip connections in the image generation process and pro-
pose a novel and effective framework incorporating tuning
modules by editing skip connections.
3. Method
3.1. Preliminaries
Diffusion models [15] are a family of probabilistic genera-
tive models that aim at sampling high-fidelity images from
Gaussian noise. It combines two fundamental processes: a
diffusion process and a denoising process. In the diffusion
process, it gradually decreases the signal-to-noise ratio of
an image via a T-step Markov chain, following prescribed
noise level schedules [β1, β2, ..., β T]. At each step t, the
noisy intermediate variable xtis constructed as:
xt=√¯αtx0+√
1−¯αtϵ, ϵ∼ N(0,1), (1)
where αt= 1−βtand¯αt=Qt
s=1αs. The denoising
process reverses the above diffusion process by estimating
the noise ϵvia a parameterized neural network, which is
trained by minimizing the l2loss between estimated noise
ϵθand real noise ϵ:
ℓt
simple(θ) =Ex0,t,ϵ∥ϵθ(xt, t)−ϵ∥2
2. (2)
U-Net [36] is already a widely adopted architecture in pixel-
corresponded generative tasks, such as image restoration,
medical segmentation, and the aforementioned image gen-
eration. Specifically, it first encodes the input into multi-
scale features through multiple cascaded encoder blocks:
xi+1=Fi(xi),0≤i≤N−1, (3)
where idenotes the index number of the encoder layer, Fi
denotes the i-th encoder block operation, Nis the total
number of encoder layers, xi+1denotes the output of i-th
encoder layer and x0means the original image input.
Subsequently, the decoder gradually decodes the feature
via establishing a skip connection with corresponding en-
coder layer outputs, which complements the high-frequency
information:
gj+1=Gj([xN−j;gj]),0≤j≤N−1, (4)
where [·;·]represents the concatenation operation, jde-
notes the index number of the decoder layer, Gjdenotes
thej-th decoder block operation, Nis the total number of
decoder layers which equals that of encoder layers, gj+1de-
notes the output of j-th decoder layer, g0is the last output
block before decoder input.
8997
❄
×3×3×3×3Denoising U-Net
Skip Connection (SC)
(c) Dense Conv…
"
"
………TunerSCNonLinear
(a) Skip Connection (SC)-Tuner
Tuner Mcondition Mcondition 1Tuner 1SC ………
"
<latexit sha1_base64="gX/EYJoeAEdZjSljLRLt9rW0VTQ=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cK9gPaWCbbTbt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVlDVoLGLVDlAzwSVrGG4EayeKYRQI1gpGN1O/9cSU5rG8N+OE+REOJA85RWOldhdFMsQHr1euuFV3BrJMvJxUIEe9V/7q9mOaRkwaKlDrjucmxs9QGU4Fm5S6qWYJ0hEOWMdSiRHTfja7d0JOrNInYaxsSUNm6u+JDCOtx1FgOyM0Q73oTcX/vE5qwis/4zJJDZN0vihMBTExmT5P+lwxasTYEqSK21sJHaJCamxEJRuCt/jyMmmeVb2L6vndeaV2ncdRhCM4hlPw4BJqcAt1aAAFAc/wCm/Oo/PivDsf89aCk88cwh84nz+2Oo/E</latexit>↵1
<latexit sha1_base64="I8n8O9tdxD44bMcp/u4Ge8DXwDk=">AAAB73icbVDLSgNBEOyNrxhfUY9eBoPgKexKUI9BL16ECOYByRp6J5NkyOzsOjMrhCU/4cWDIl79HW/+jZNkD5pY0FBUddPdFcSCa+O6305uZXVtfSO/Wdja3tndK+4fNHSUKMrqNBKRagWomeCS1Q03grVixTAMBGsGo+up33xiSvNI3ptxzPwQB5L3OUVjpVYHRTzEh9tuseSW3RnIMvEyUoIMtW7xq9OLaBIyaahArdueGxs/RWU4FWxS6CSaxUhHOGBtSyWGTPvp7N4JObFKj/QjZUsaMlN/T6QYaj0OA9sZohnqRW8q/ue1E9O/9FMu48QwSeeL+okgJiLT50mPK0aNGFuCVHF7K6FDVEiNjahgQ/AWX14mjbOyd16u3FVK1assjjwcwTGcggcXUIUbqEEdKAh4hld4cx6dF+fd+Zi35pxs5hD+wPn8AeCqj+A=</latexit>↵M(b) Controllable Skip Connection (CSC)-TunerFigure 4. Illustration of SCEdit framework. Our method achieves efficient tuning by editing the features on skip connections which is
proved to have rich structural information. Leveraging (a) SC-Tuner for text-to-image generation tuning, and controllable image synthesis
can be achieved with the assistance of (b) CSC-Tuner and (c) Cascade Dense Convolution.
3.2. Tuner modules
We present SkipConnection Tuner , termed SC-Tuner , a
method designed to directly edit the latent features within
skip connections. As illustrated in Fig. 4a, the SC-Tuner
is composed of a tuning operation, referred to as Tuner OP,
and a residual connection. The j-th SC-Tuner takes xN−j
as input and produces the sum of xN−jand the output of
Tuner OP with xN−jas input. This process can be mathe-
matically formulated as follows:
OSC
j(xN−j) =Tj(xN−j) +xN−j, (5)
where OSC
j(xN−j)denotes the j-th SC-Tuner module with
xN−jas input, Tjdenotes the Tuner OP of j-th SC-Tuner
module.
The efficient tuning paradigm [21] has demonstrated
comparable effectiveness across various Tuner OPs, includ-
ing LoRA OP, Adapter OP, and Prefix OP, as well as their
versatile combinations. In these independent OPs, we adapt
the form of an Adapter OP, as it has been proven to be the
simplest yet relatively effective method. The Tuner OP Tj
can be defined as follows:
Tj(xN−j) =Wup
jϕ(Wdown
jxN−j), (6)
whereWupandWdownare up and down tunable projec-
tion matrices, respectively, ϕis a GELU [13] activationfunction. Formally, we can employ tuners of various types
and scales to modify the features of skip connections.
Furthermore, The SC-Tuner can be easily adapted to sup-
port controllable image synthesis by incorporating the con-
ditions and xN−jas input, where conditions accommodate
both single and multiple conditions. In the case of multi-
condition controllable generation, we assign weights to dif-
ferent condition branches and combine them with original
skip connections. This modified structure is depicted in
Fig. 4b, named Controllable SC-Tuner orCSC-Tuner .
We extend Eq. (5) by injecting extra conditional informa-
tion which can be formulated as follows:
OCSC
j(xN−j, Cj) =MX
m=1αm(Tm
j(xN−j+cm
j) +cm
j)
+xN−j, (7)
where OCSC
j(xN−j, Cj)denotes the j-th CSC-Tuner mod-
ule with xN−jandMconditions Cj={c0
j, . . . , cM
j}as
inputs, cm
jis the j-th hint block’s output of m-th condition
features, αmis the weight of different independent condi-
tion embeddings, andPM
m=1αm= 1. In practice, we can
engage in joint training with multi-conditions or perform
inference under combined conditions directly after com-
pleting single-condition training. The control conditions
include but are not limited to the canny edge, depth, hed
boundary, semantic segmentation, pose keypoint, color, and
masked image.
8998
3.3. SCEdit framework
We introduce SCEdit , a framework designed for efficient
SkipConnection Editing in image generation that utilizes
the SC-Tuner and CSC-Tuner. All the skip connections and
conditions are fed into the SC-Tuner and CSC-Tuner, with
the outputs subsequently concatenated to the original fea-
ture maps and input into the corresponding decoder blocks.
We depict this process as follows:
gj+1=Gj([Oj(xN−j, Cj);gj]), (8)
where O j(xN−j, Cj)denotes the j-th SC-Tuner or CSC-
Tuner modules. By editing the original xN−j, we can adapt
it to different tasks.
SCEdit can facilitate flexibility and efficiency across
both text-to-image generation and controllable image syn-
thesis tasks. As illustrated in Fig. 4, the SC-Tuner is ap-
plied to the text-to-image generation task and integrated into
all skip connections within the pre-trained U-Net architec-
ture. The CSC-Tuner is employed for controllable image
synthesis, where the input of corresponding condition fea-
tures is encoded via a cascaded convolution network, as
shown in Fig. 4c, consisting of a multi-layer hint block and
several dense modules for feeding into the skip connection
branch, where each level contains zero convolution layers
and SiLU [13] activation functions, ensuring compatibility
with the dimensions of skip connection.
4. Experiments
4.1. Experimental setups
Evaluation. We evaluate the flexibility and efficiency of
our SCEdit, mainly through the text-to-image generation
and controllable image synthesis tasks. We analyze its
quantitative metrics such as trainable parameters, memory
consumption, training speed, and FID [40] evaluation met-
rics to assess its performance and efficiency. Additionally,
we consider qualitative evaluations of the quality and fi-
delity of the generated images.
Datasets. In the text-to-image generation task, we use the
COCO2017 [24] dataset for training and testing, which con-
sists of 118k images and 591k training prompts. Further-
more, we employ customized style datasets [28] with a lim-
ited samples to further validate the effectiveness of our ap-
proach. In controllable image synthesis tasks, for each kind
of condition data, we utilize a filtered version of the LAION
artistic dataset [39] that includes approximately 600k im-
ages that have been removed of duplicates, low-resolution
images, those with a risk for harm, and those that are of low
quality.
Baselines. According to task categories, the evaluation
tasks can be characterized into two groups: (i)the text-to-
image tuning includes fully fine-tuning and LoRA [17] tun-
ing strategies; (ii)the controllable image synthesis focuses
FullLoRASD v1.5SCEdit
[An Amtrak train sitting on the train track][A brown, black and white bird resting on a tree branch]
Figure 5. Qualitative comparison of the original SD v1.5, exist-
ing tuning strategies, and our SCEdit using the same prompts.
Table 1. Comparison of FID and efficiency on text-to-image
generation using COCO2017 dataset. For the FID score, we fol-
low the default settings of SD v1.5 and randomly sample 10k
image-text pair. In terms of efficiency, we compare the aspects
of parameter-, memory-, and training time efficiency. We compare
the performance of LoRA and our method under two different pa-
rameter settings.
Method FID↓ Params Mem. Time
SD v1.5 [1] 15.48 - - -
Full 14.15 859.52M 53.46G ×1.0
LoRA/r=64 [17] 13.96 23.94M 60.57G ×1.24
LoRA/r=6 [17] 15.12 2.24M 59.94G ×1.20
SCEdit 13.82 19.68M 29.02G ×0.78
SCEdit 10 13.99 1.98M 28.06 G×0.77
on methods including additional information as input, such
as ControlNet [50], T2I-Adapter [29], ControlLoRA [12],
and ControlNet-XS [49]. Due to the distinct task character-
istics of these two categories, we apply the SC-Tuner mod-
ule for the former and CSC-Tuner for the latter.
Implementation details. For all experiments, we perform
efficient fine-tuning based on the Stable Diffusion (SD) pre-
trained model, where SD v1.5 [1] is used for the text-to-
image task and SD v2.1 [2] for the conditional task, and
the input image sizes for training are set to 512 ×512. We
utilize the AdamW [25] optimizer with a fixed learning rate
of 5e-5. Unless otherwise specified, models are trained for
100k steps.
Following ControlNet, we use different input condi-
tions, e.g., edge map [5, 47], depth map [34], segmentation
map [23], and body key points [6], in addition to the color
map condition in T2I-Adapter [29] and the mask-generation
strategy presented in LaMa [45].
8999
3DAnimeFlat illustrationMethodSketchOil painting
SCEditWatercolorLoRA
[A boy in a jacket with a camouﬂage scarf]Figure 6. Few-shot transfer learning comparison on various custom-stylized datasets. Compared to LoRA tuning, SCEdit achieves more
precise learning of style characteristics and generates images of superior quality.
4.2. Text-to-image generation
Performance and efficiency comparison. To evaluate the
transferability of our method in downstream tasks, we fine-
tuning COCO2017 with pre-trained U-Net and compare our
method with other training strategies in terms of qualitative
and quantitative results.
The qualitative results can be seen in Fig. 5, the left-
most column is the zero-shot result of the original model,
while the fine-tuned model obtained semantic comprehen-
sion capabilities on downstream tasks. Compared to exist-
ing strategies, our method exhibits fewer artificial artifacts
in the generated images and higher visual quality. For in-
stance, in the second row, the generated bird has more re-
alistic details in the head while maintaining semantic com-
prehension.
In addition, as shown in Tab. 1, compared to the fully
fine-tuning baseline, our SCEdit achieves 0.33 performance
gain in FID score while using only 2.29% of the parameters
and reducing nearly 22% training time. It is worth noted
that at lower parameters compared to LoRA/r=64 strategies
with rank 64, our method is lower in both FID, while the
training memory can be reduced by 52.1% and the training
time of LoRA is 1.6 ×longer than ours. When we further re-
duce the hidden dimensions of the tuner by 10 ×, denoted as
SCEdit 10, which corresponds to a reduction of parameters
by the same factor compared to LoRA/r=6, our FID score
shows a significant decreased by 1.13, while also maintain-
ing a clear advantage in terms of memory usage and training
time.
Few-shot transfer learning. In the image generation com-
munity, few-shot learning is a practical technique that en-
ables users to train a personalized model with just a small
subset of data. Our experiment involved performing few-Table 2. Comparison of FID and efficiency on controllable im-
age synthesis using LAION dataset within the canny edge condi-
tion. “k” denotes the convolution kernel size of conditional model,
and a larger size performs better on FID score, albeit with a mod-
erate increase in parameters.
Method FID↓Params. Mem. Time
ControlNet [50] 74.86 364.23M 49.51G ×1.0
T2I-Adapter [29] 73.37 77.37M 37.01G ×0.92
ControlNet-XS [49] 75.63 55.30M 48.32G ×0.97
ControlLoRA [12] 74.14 21.52 M 68.92G ×1.34
SCEdit/k=1 73.18 28.82M 34.78 G×0.87
SCEdit/k=3 71.78 99.11M 35.28G ×0.87
shot transfer learning on a custom-stylized dataset, which
included classes 3D, anime, flat illustration, oil painting,
sketch, and watercolor, each with only 30 image-text pairs.
Moreover, to prevent style leakage, we perform a cleansing
of style-related words from the original prompts and incor-
porated specific trigger words <sce>during training to en-
sure the reliability of the experiment. In Fig. 6, we provide
a comparison of the quality of samples between LoRA and
our method, employing the same training setup. The results
demonstrate that our method more accurately captures the
style, aligning with the distribution of the original training
data while maintaining text alignment. For instance, the flat
illustration style retained the descriptions of a camouflage
scarf, while the sketch style preserved the line-drawn de-
pictions in monochrome.
4.3. Controllable image synthesis
Performance and efficiency comparison. Extensive ex-
periments are conducted to evaluate the effectiveness of our
9000
DepthHEDSegmentationCanny
ColorInpaintingPose
Figure 7. Controllable image synthesis with various conditions. The odd-numbered rows represent the input conditions, while the
even-numbered rows correspond to the generated results. SCEdit is capable of generating high-quality images precise to input conditions.
ControlLoRAControlNet-XST2I-AdapterControlNetCondition
SCEdit
[The Gorge]
[Concept car, unreal engine, cinematography]
Figure 8. Qualitative comparison to the state-of-the-art controllable image synthesis methods based on the canny edge and semantic
segmentation conditions. The areas in the boxes underneath are enlarged for detailed comparisons.
9001
Mountain(a) Using the same canny edge map and textual prompt in combination with different color maps results in the depiction of a mountain across various seasons.
ColorCannyCannyColor
(b) Interpolations within the canny edge map and color maps.
Figure 9. Composable generation. Combinations of multi-conditions provide more compositional possibilities.
proposed SCEdit using CSC-Tuner on various conditional
generation tasks, e.g., canny edge, depth, hed boundary, se-
mantic segmentation, pose keypoint, color, masked image,
and so on. As illustrated in Fig. 7, our method shows supe-
rior performance and precise control ability under diverse
scenarios and styles control, including real scenes, imag-
ined scenarios, and artistic styles.
We also compare our approach with the state-of-the-
art methods that generate images from different conditions.
From a qualitative perspective, we present the generated re-
sults based on the canny edge map and semantic segmenta-
tion conditions with different methods and zoom-in on the
detailed parts of the image in Fig. 8. It can be observed that
our method achieves better quality in terms of fidelity and
realism, such as the preservation of the reflection on the lake
surface in the first row and the texture information of the car
in the second row. From a quantitative perspective, as seen
in Tab. 2, our method utilizes only 7.9% of the number of
parameters in ControlNet, resulting in a 30% reduction in
memory consumption, and accelerates the training process
while also yielding a lower FID score.
Composable generation. In addition to generating control
based on a single condition, we also support the input of
multi-conditions simultaneously. In Fig. 9, we demonstrate
the joint application of separately trained canny edge and
color map models and present the results on unpaired con-
dition data for training-free scene-level image translation.
By anchoring the controlled subject to a canny edge map
and text prompt, and incorporating various color maps, we
produce a distinctive seasonal transition effect, as illustrated
in Fig. 9a. Additionally, by traversing the embedding space
representations of CSC-Tuner between the two conditions,
we can blend them to achieve variations. In Fig. 9b, SCEdit
SketchOutpainting
Figure 10. Control generalization. Learned edge condition
model is capable of sketch-to-image task and inpainting condition
model can control outpainting generation.
further provides precise control capabilities, enabling dif-
ferent interpolation effects through balancing among multi-
ple elements.
Controllable generalization. We find that additional con-
trollable generation tasks could be attempted on models
that had already been trained under specific conditions. In
Fig. 10, we achieve impressive results using the edge-based
model for the sketch-to-image task and the inpainting-based
model for the outpainting task, although there is no adap-
tation based on non-real conditions such as hand-drawn
sketches for the former, and no specialized adjustment of
mask generation patterns for outpainting for the latter.
5. Conclusion
We propose SCEdit as an efficient and controllable method
for image diffusion generation. We introduce SC-Tuner
to edit skip connections and extends it to CSC-Tuner, en-
abling a diverse range of conditional inputs. Our method
achieves high efficiency by exclusively performing back-
ward propagation in the decoupled U-Net decoder. As a
lightweight and plug-and-play module, SCEdit supports ar-
bitrary single- and multi-condition generation and demon-
strates remarkable superiority in terms of performance.
9002
References
[1] Runway AI. Stable Diffusion v1.5 Model Card,
https://huggingface.co/runwayml/stable-
diffusion-v1-5 , 2022. 2, 5
[2] Stability AI. Stable Diffusion v2-1 Model Card, https:
/ / huggingface . co / stabilityai / stable -
diffusion-2-1 , 2022. 5
[3] Stability AI. Stable Diffusion XL Model Card, https:
/ / huggingface . co / stabilityai / stable -
diffusion-xl-base-1.0 , 2022. 2
[4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-
structPix2Pix: Learning To Follow Image Editing Instruc-
tions. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
18392–18402, 2023. 2
[5] John Canny. A Computational Approach to Edge Detec-
tion. IEEE Trans. Pattern Anal. Mach. Intell. , pages 679–
698, 1986. 5
[6] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and
Yaser Sheikh. OpenPose: Realtime Multi-Person 2D Pose
Estimation Using Part Affinity Fields. IEEE Trans. Pattern
Anal. Mach. Intell. , 43(1):172–186, 2021. 5
[7] Huiwen Chang, Han Zhang, Jarred Barber, A. J. Maschinot,
Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-
phy, William T. Freeman, Michael Rubinstein, Yuanzhen
Li, and Dilip Krishnan. Muse: Text-To-Image Genera-
tion via Masked Generative Transformers. arXiv preprint
arXiv:2301.00704 , 2023. 2
[8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze
Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,
Huchuan Lu, and Zhenguo Li. PixArt- α: Fast Training of
Diffusion Transformer for Photorealistic Text-to-Image Syn-
thesis. arXiv preprint arXiv:2310.00426 , 2023. 3
[9] Alibaba Cloud. Tongyi Wanxiang, https://tongyi.
aliyun.com/wanxiang/ , 2023. 2
[10] David Ha, Andrew M. Dai, and Quoc V . Le. HyperNetworks.
InInt. Conf. Learn. Represent. , 2016. 3
[11] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. Towards a Unified View of
Parameter-Efficient Transfer Learning. In Int. Conf. Learn.
Represent. , 2021. 3
[12] Wu Hecong. ControlLoRA: A Lightweight Neural Network
To Control Stable Diffusion Spatial Information, https:
//github.com/HighCWu/ControlLoRA , 2023. 5, 6
[13] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear
Units (GELUs). arXiv preprint arXiv:1606.08415 , 2023. 4,
5
[14] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion
Guidance. In Adv. Neural Inform. Process. Syst. , 2021. 2
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-
fusion Probabilistic Models. In Adv. Neural Inform. Process.
Syst. Curran Associates, Inc., 2020. 2, 3
[16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for NLP. In Int. Conf. Mach. Learn. , pages 2790–
2799, 2019. 3[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
LoRA: Low-Rank Adaptation of Large Language Models. In
Int. Conf. Learn. Represent. , 2022. 2, 3, 5
[18] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,
and Jingren Zhou. Composer: Creative and Controllable Im-
age Synthesis with Composable Conditions. In Int. Conf.
Mach. Learn. , 2023. 2, 3
[19] Zhongzhan Huang, Pan Zhou, Shuicheng Yan, and Liang
Lin. ScaleLong: Towards More Stable Training of Diffu-
sion Model via Scaling Network Long Skip Connection. In
Adv. Neural Inform. Process. Syst. , 2023. 2, 3
[20] Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Yiliang Lv,
Deli Zhao, and Jingren Zhou. Rethinking Efficient Tun-
ing Methods from a Unified Perspective. arXiv preprint
arXiv:2303.00690 , 2023. 2, 3
[21] Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang
Lv, Yujun Shen, Deli Zhao, and Jingren Zhou. Res-Tuning:
A Flexible and Efficient Tuning Paradigm via Unbinding
Tuner from Backbone. In Adv. Neural Inform. Process. Syst. ,
2023. 2, 3, 4
[22] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power
of Scale for Parameter-Efficient Prompt Tuning. In Conf.
Empirical Methods NLP , 2021. 3
[23] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu,
Hongsheng Li, and Yu Qiao. UniFormer: Unified Trans-
former for Efficient Spatial-Temporal Representation Learn-
ing. In Int. Conf. Learn. Represent. , 2021. 5
[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
Eur. Conf. Comput. Vis. , pages 740–755, 2014. 5
[25] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay
Regularization. In Int. Conf. Learn. Represent. , 2018. 5
[26] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun
Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided Im-
age Synthesis and Editing with Stochastic Differential Equa-
tions. In Int. Conf. Learn. Represent. , 2021. 2
[27] Midjourney. Midjourney, https://www.midjourney.
com/ , 2023. 2
[28] ModelScope. Customized Style Dataset Card, https:
/ / modelscope . cn / datasets / damo / style _
custom_dataset/summary , 2023. 5
[29] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian
Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2I-
Adapter: Learning Adapters to Dig out More Controllable
Ability for Text-to-Image Diffusion Models. arXiv preprint
arXiv:2302.08453 , 2023. 2, 5, 6
[30] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. GLIDE: Towards Photorealistic Image Genera-
tion and Editing with Text-Guided Diffusion Models. arXiv
preprint arXiv:2112.10741 , 2022. 2
[31] OpenAI. DALL·E 2, https://openai.com/dall-e-
2, 2022. 2
[32] OpenAI. DALL·E 3, https://openai.com/dall-e-
3, 2023. 2
9003
[33] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang,
Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming
Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, and Ran
Xu. UniControl: A Unified Diffusion Model for Control-
lable Visual Generation In the Wild. In Adv. Neural Inform.
Process. Syst. , 2023. 2
[34] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards Robust Monocu-
lar Depth Estimation: Mixing Datasets for Zero-Shot Cross-
Dataset Transfer. IEEE Trans. Pattern Anal. Mach. Intell. ,
pages 1623–1637, 2022. 5
[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In IEEE Conf. Comput.
Vis. Pattern Recog. , pages 10684–10695, 2022. 3
[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
Net: Convolutional Networks for Biomedical Image Seg-
mentation. Med. Image Comput. Computer-Assisted Interv. ,
2015. 2, 3
[37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine
Tuning Text-to-Image Diffusion Models for Subject-Driven
Generation. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 22500–22510, 2023. 2
[38] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J. Fleet, and Mohammad Norouzi. Image
Super-Resolution via Iterative Refinement. arXiv preprint
arXiv:2104.07636 , 2021. 2, 3
[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade W. Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa R. Kundurthy, Kather-
ine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and
Jenia Jitsev. LAION-5B: An open large-scale dataset for
training next generation image-text models. In Adv. Neural
Inform. Process. Syst. , 2022. 5
[40] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch,
https://github.com/mseitzer/pytorch-fid ,
2020. 5
[41] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain,
Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman.
Emu Edit: Precise Image Editing via Recognition and Gen-
eration Tasks. arXiv preprint arXiv:2311.10089 , 2023. 2
[42] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu.
FreeU: Free Lunch in Diffusion U-Net. arXiv preprint
arXiv:2309.11497 , 2023. 2, 3
[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing Diffusion Implicit Models. In Int. Conf. Learn. Repre-
sent., 2021. 3
[44] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-Based
Generative Modeling through Stochastic Differential Equa-
tions. In Int. Conf. Learn. Represent. , 2021. 2, 3
[45] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor
Lempitsky. Resolution-Robust Large Mask Inpainting WithFourier Convolutions. In IEEE Winter Conf. Appl. Comput.
Vis., pages 2149–2159, 2022. 5
[46] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou,
Zhaoqiang Liu, Jiawei Li, and Zhenguo Li. DiffFit: Un-
locking Transferability of Large Diffusion Models via Sim-
ple Parameter-efficient Fine-Tuning. In Int. Conf. Comput.
Vis., pages 4230–4239, 2023. 2
[47] Saining Xie and Zhuowen Tu. Holistically-Nested Edge De-
tection. In Int. Conf. Comput. Vis. , pages 1395–1403, 2015.
5
[48] Sihan Xu, Ziqiao Ma, Yidong Huang, Honglak Lee, and
Joyce Chai. CycleNet: Rethinking Cycle Consistency in
Text-Guided Diffusion for Image Manipulation. In Adv. Neu-
ral Inform. Process. Syst. , 2023. 2
[49] Denis Zavadski, Johann-Friedrich Feiden, and Carsten
Rother. ControlNet-XS: Designing an Efficient and Effective
Architecture for Controlling Text-to-Image Diffusion Mod-
els.arXiv preprint arXiv:2312.06573 , 2023. 5, 6
[50] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
Conditional Control to Text-to-Image Diffusion Models. In
Int. Conf. Comput. Vis. , pages 3836–3847, 2023. 2, 3, 5, 6
9004
