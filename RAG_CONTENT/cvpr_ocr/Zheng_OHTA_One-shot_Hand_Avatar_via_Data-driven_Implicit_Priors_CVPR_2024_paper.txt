OHTA: One-shot Hand Avatar via Data-driven Implicit Priors
Xiaozheng Zheng*Chao Wen*Zhuo Su Zeran Xu Zhaohu Li Yang Zhao Zhou Xue‚Ä†
PICO, ByteDance
Single Image Hand Avatar Avatar Animation
Editing
Texture Editing Shape EditingLatent Space Manipulation
Interpolation
Random SamplingText-to-Avatar
‚ÄúLight Skin Tone, Yellow Fingernails‚Äù
‚ÄúDark Skin Tone, Red Fingernails‚Äù
Figure 1. We introduce a novel approach capable of creating implicit animatable hand avatars using just a single image. Our framework
facilitates 1) text-to-avatar conversion, 2) hand texture and geometry editing, and 3) interpolation and sampling within the latent space.
Abstract
In this paper, we delve into the creation of one-shot hand
avatars, attaining high-fidelity and drivable hand represen-
tations swiftly from a single image. With the burgeoning
domains of the digital human, the need for quick and per-
sonalized hand avatar creation has become increasingly
critical. Existing techniques typically require extensive in-
put data and may prove cumbersome or even impractical
in certain scenarios. To enhance accessibility, we present
a novel method OHTA ( One-shot Hand ava TAr) that en-
ables the creation of detailed hand avatars from merely one
image. OHTA tackles the inherent difficulties of this data-
limited problem by learning and utilizing data-driven hand
priors. Specifically, we design a hand prior model initially
employed for 1) learning various hand priors with available
data and subsequently for 2) the inversion and fitting of the
target identity with prior knowledge. OHTA demonstrates
the capability to create high-fidelity hand avatars with con-
sistent animatable quality, solely relying on a single image.
Project page: https://zxz267.github.io/OHTA
‚àóEqual contribution,‚Ä†Corresponding authorFurthermore, we illustrate the versatility of OHTA through
diverse applications, encompassing text-to-avatar conver-
sion, hand editing, and identity latent space manipulation.
1. Introduction
The progress in digital humans is reshaping our everyday
lives by blending the physical and digital realms. In this
emerging reality, human hands have a central role in creat-
ing a close and interactive experience, acting as the primary
way people connect with the digital world. Consequently,
it is vital to convert the hands of users into digital forms,
allowing for the creation of personalized, controllable, and
highly realistic representations in the virtual environment.
Conventional approaches of hand appearance model-
ing have typically relied on texture maps and colored
meshes [10, 13, 14, 37, 57, 59]. However, the construc-
tion of personalized hand meshes and texture maps often
demands the use of costly scanning data and artistic ex-
pertise. Recent efforts focus on creating hand avatars us-
ing data-driven approaches [9, 11, 16, 31, 52, 60]. Many
of these studies have concentrated on creating neural im-
plicit hand avatars, as this form has demonstrated the abil-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
799
ity to effectively model high-fidelity avatars across vari-
ous human body parts [6, 9, 11, 23, 52, 71]. Despite the
notable accomplishments in recent implicit hand represen-
tation, a noteworthy limitation persists, which entails the
necessity of employing extensive sequential or multi-view
images to procure a high-fidelity animatable hand avatar.
This requirement poses a considerable challenge for general
users, as the acquisition of such dense input data is often
labor-intensive and, in certain cases, may not be practical.
To broaden the scope of personalized hand avatar creation
to more diverse situations, we aim to propose a novel ap-
proach capable of one-shot reconstruction of high-fidelity
hand avatars. This endeavor presents a substantial chal-
lenge, given that it places personalized hand avatar creation
at a crossroads, necessitating a data-driven method despite
the scarcity of available data.
In response to this challenge, we introduce OHTA ( One-
shotHand ava TAr) which captures data-driven priors to fa-
cilitate one-shot high-fidelity hand avatars creation. Our
motivation stems from the fact that leveraging priors can
complement unobserved information due to the inherent
similarities shared by diverse human hands. To this end,
we decouple our framework into two stages: 1) hand prior
knowledge learning, and 2) one-shot reconstruction with
the aid of prior knowledge. In the first stage, we employ
the Hand Prior Network (HPNet) to capture prior knowl-
edge from the accessible data with multiple identities. In
the second stage, we utilize HPNet to inverse a similar hand
and subsequently optimize it to the target identity with prior
knowledge for regularization, based on the given image.
HPNet plays a pivotal role since its capacity for both 1)
prior transferring and 2) prior learning significantly im-
pacts the ultimate one-shot performance. Therefore, de-
signing such a network is non-trivial and presents a dual
challenge: the architecture must be well-suited for two pur-
poses simultaneously. To address the challenges, we adopt
corresponding designs in HPNet. For prior transferring ,
we design HPNet in a mesh-guided manner since the aid
of the mesh enables the learned prior knowledge to transfer
to novel identities. Both geometry and texture priors learn-
ing of HPNet are guided by the mesh information, achiev-
ing robustness towards novel poses and identities. For prior
learning , we equip HPNet with 1) the multi-resolution field
based on the mesh scaffold and 2) identity-specific albedo
and identity-shared shadow disentanglement. The multi-
resolution field facilitates learning detailed texture prior,
and the disentanglement models transferable self-occlusion
effect, both of which contribute to high-fidelity one-shot
hand avatars. With all these designs, OHTA is capable of
addressing many downstream tasks (as shown in Fig. 1), in-
cluding 1) hand avatar creation using text prompts, 2) edit-
ing of hand geometry and appearance, and 3) appearance
interpolation and sampling in identity latent space.In summary, our contributions can be listed as follows:
‚Ä¢ We introduce the first framework for one-shot implicit
hand avatar creation. These avatars exhibit high-fidelity
and consistent animatable quality.
‚Ä¢ We present the Hand Prior Network exploiting transfer-
able geometry, albedo, and shadow priors, contributing to
the high fidelity of one-shot hand avatars.
‚Ä¢ We substantiate the efficacy and robustness of our frame-
work through a comprehensive series of experiments and
showcase its utility in diverse applications, including text-
to-avatar, hand editing, and latent space manipulation.
2. Related Work
Animatable Hand Avatar. Research in creating person-
alized, animatable hand avatars has advanced through two
main approaches. Some works utilize explicit geometry
and texture. HTML [59] is prominent for its texture infer-
ence on the MANO model [62]. NIMBLE [37] extends the
hand skeleton model [36], capable of inferring surface with
texture maps. Handy [57] improves realism using a GAN-
based texture model [29], but these methods often suffer
from poor adaptability due to limited training data. HARP
[31] addresses this by adjusting geometry and texture for
individual identities. Another line of research explores neu-
ral implicit representations for more realistic texture mod-
eling. LISA [11] learns an implicit color and surface rep-
resentation. HandAvatar [9] employs disentangled implicit
representations for hand geometry, albedo, and illumina-
tion. HandNeRF [16] and LiveHand [52] extend NeRF [49]
techniques with deformation fields for hands, even achiev-
ing real-time performance. HO-NeRF [60] further explores
hand-object reconstruction using implicit avatars. Notwith-
standing their accomplishments, it is worth noting that these
models are unable to create high-fidelity hand avatars from
a single image, as is achievable with our proposed approach.
One-shot Human Avatar. A similar task to one-shot hand
avatar modeling is reconstructing a human body avatar from
a single image, with methods divided into ‚Äúexplicit‚Äù and
‚Äúimplicit‚Äù categories. Explicit methods [1, 2, 5, 27, 28, 32‚Äì
34, 45, 54, 81] rely on parametric models [44, 53, 54]
to estimate a minimally-clothed body or to regress body
shape offsets, often struggling with loose clothing repre-
sentation. DINAR [67] integrates neural textures with the
explicit model for enhanced photo-realism. Implicit meth-
ods, using topology-agnostic fields, aim for higher real-
ism. Many works [17, 63, 64, 79] utilize large datasets
to estimate body avatars with pixel-aligned features, while
others [18, 21] further propose avatar animation. How-
ever, these approaches are often constrained by training
data, limiting their generalizability. NeRF-based methods
[24, 55, 66, 71, 73, 78] typically require dense inputs but
have evolved to accommodate few-shot or one-shot sce-
narios by incorporating various priors. ActorsNeRF [51]
800
Volume Rendering
Query
PointsStage 1: Hand Prior Learning
HPNet
HPNet
Stage 2: Inversion and Fitting
Occupancy Shadow Shaded Color Albedo
Occupancy
FieldAlbedo
FieldShadow
Field
Identity CodeMesh
 Pose
Hand Prior Networkùê≥ùê≥1..ùëÅùëÅùëßùëß ùê≥ùê≥‚àó
ùíÇùíÇ‚àóùë¢ùë¢ ùëúùëúInverted Avatar Fitted Avatar Target ImageFigure 2. The two-stage framework of OHTA (above) and the Hand Prior Network (below). For stage-1, OHTA optimizes identity code
and HPNet to capture various hand priors. For stage-2, OHTA first optimizes identity code for texture inversion, then optimizes HPNet for
texture fitting to capture the details. HPNet consists of three fields ( i.e. albedo, shadow, and occupancy) for capturing transferable hand
prior knowledge. Combined with the albedo, shadow, and occupancy values, we use volume rendering to obtain final shaded color images.
adapts to new subjects by pre-training category-level hu-
man NeRFs, while ELICIT [20] and SHERF [19] employ
semantic priors or 3D-aware features for one-shot avatar
creation. For faces, CG-NeRF [26] supports one-shot 3D-
aware face synthesis with a conditional NeRF. PhoneScan
[7] learns identity and expression prior with CNNs from
large-scale data, and finetunes the prior model for few-show
head avatar creation. Preface [6] also trains an identity-
conditioned prior model and achieves few-shot high-fidelity
3D head creations with its inversion and fitting. Compared
with PhoneScan and Preface, our method further exploits
geometry, albedo, and shadow priors to create one-shot an-
imatable hand avatars with consistent driving performance.
Mesh-guided Representation. Parametric models gener-
ate meshes from pose and shape parameters [35, 44, 54, 62],
but these meshes have unalterable topology structure. Im-
plicit representations are increasingly researched to ad-
dress these limitations, with studies focusing on human
[3, 8, 12, 47, 48, 65, 68, 69] and hand geometry [9, 22, 30],
often using rigid transformations or skeletons for predic-
tion. COAP [48] and PairOF [9] enhance robustness by
incorporating mesh information and localized encoders or
part-pair-wise decoders, respectively. Mesh information
also aids implicit texture modeling [4, 9, 43, 56, 72, 74, 80].
NeuMesh [74] and DE-NeRF [72] place features on ver-
tices for editable radiance fields. Recent implicit human
body [56, 80] and face [4, 15] modeling also rely on mesh
information for guidance. HandAvatar [9] uses barycen-
tric sampled anchor upon meshes for hand modeling. Our
method differs by employing multi-resolution fields on themesh scaffold for enhanced robustness and fidelity.
Single Image to 3D. Recently, there has been a surge in re-
search focused on learning how to create 3D models from
a single image [46, 75]. Zero-1-to-3 [41] pioneers the
zero-shot single-image to 3D conversion. Recent works
like One-2-3-45 [40], Magic123 [58], and Consistent123
[39] build upon Zero-1-to-3 to obtain more 3D-consistent
results by introducing more priors. Although such methods
have pre-trained networks on large datasets, there is an un-
derutilization of specific domain knowledge. In contrast to
these methods, we are dedicated to considering the distinc-
tive properties of the hand to create animatable avatars.
3. Method
In this section, we provide a detailed description of the pro-
posed hand representation and complete avatar reconstruc-
tion pipeline (Fig. 2). We first introduce a novel hand repre-
sentation that encodes multi-resolution neural implicit fields
at a mesh scaffold (Sec. 3.1). As for the one-shot hand
avatar creation pipeline, we decouple it into two stages.
In the first stage (Sec. 3.2), we devise an effective training
scheme that captures hand prior knowledge through the HP-
Net. In the second stage (Sec. 3.3), we invert and optimize
the HPNet to create high-quality hand avatars according to
a single input image.
3.1. OHTA Hand Representation
Our representation of the hand comprises implicit geometry
and neural texture fields based on a mesh scaffold.
801
Mesh Scaffold. We adopt MANO-HD [9], a super-
resolution variant of MANO [62], as the mesh scaffold.
(1) For training HPNet, we use personalized MANO-HD
by leveraging a multi-layer perceptron (MLP) Mshape to
refine the template mesh. The refined template mesh is de-
fined as M+=¬ØM+Mshape ([P(¬ØM),Œ∏,z]), where ¬ØM,P,
Œ∏,zand[¬∑]denote the template mesh, positional encoding,
pose parameters, identity code and concatenation respec-
tively. The MLP can be optimized via IoU loss Lshape , de-
tailed in [42]. The template mesh can be deformed to posed
mesh with a Linear Blend Skinning transformation accord-
ing to pose parameters. (2) For one-shot hand avatar recon-
struction, where acquiring a precisely refined mesh from a
single view is unfeasible, we use a mesh Mrepresented by
shape Œ≤and pose Œ∏parameters as the input. For simplicity,
the input mesh is denoted as Min the following text.
Geometry Network. To achieve robust geometry model-
ing towards novel identities and poses, we utilize PairOF
[9] to predict occupancy values. Given Nqquery points
q‚ààRNq√ó3, hand mesh and hand pose parameters, PairOF
fOpredicts the occupancy value oq=fO(q,M,Œ∏)to in-
dicate whether the point locates inside or out of the surface.
Hence, the hand surface can be formulated as {q|oq= 0.5}.
For more details, please refer to [9, 48].
Multi-resolution Field. To achieve high-fidelity texture
modeling, we designed a Multi-resolution Field (Fig. 3)
with the mesh guidance. More specifically, we uniformly
sample Np
kpoints Pkon the input mesh and represent them
with barycentric coordinates for each resolution, where k‚àà
{1, ..., K}denotes the resolution level. For each point in
Pk, we attach them with a learnable point encoding of Nc
dimension that is randomly initialized. In this way, we
can obtain point encodings Ekof a resolution. For query
points q, we conduct spatial interpolation to acquire the
queried encoding of this resolution Qk=interp (q,Ek).
Specifically, we first extract the encodings of Nnneigh-
bor points K(Ek)‚ààRNn√óNq√óNcofPk, where Kdenotes
k-nearest neighbors. Then, we perform a weighted aver-
age with the inverse Euclidean distances of those points
to the query point as the weights to acquire the queried
encoding of this resolution Qk‚ààRNq√óNc. After that,
we feed queried encodings with the field-specific informa-
tionœàto resolution-specific MLP Mkto obtain resolution-
specific features: Dk=Mk(Qk,œà), where Dk‚àà
RNq√óNd, and œàis designed to distinguish the albedo-
specific field fAand shadow-specific field fU, which are
combined with the multi-resolution mechanism to better
represent the texture, as described later. Finally, we make
use of another MLP Mfuse to fuse resolution-specific fea-
tures of multi-resolutions to predict the target field value
xq=Mfuse([D1...,DK]). The overall hand representa-
tion comprises the above components, as defined formally:
H: (q,M,Œ∏,z;fA, fU, fO)7‚Üí(o,c), (1)
ùêåùêå
Interp.Interp.
M2
ùê±ùê±ùííùííùùçùùçM1Mùëìùëìùëìùëìùëìùëìùëìùëìùë¨ùë¨ùüèùüè
ùë¨ùë¨ùüêùüêBarycentric
Sampling
ùë∏ùë∏ùüêùüêùë∏ùë∏ùüèùüè
ùë´ùë´ùüêùüêùë´ùë´ùüèùüèFigure 3. Multi-resolution Field. For simplicity, we take two res-
olutions for example. ‚äïdenotes feature concatenation.
where oandcdenote the occupancy and shaded color.
3.2. Hand Prior Learning
To exploit the potential of OHTA representation for one-
shot reconstruction, learning transferrable and abundant
hand prior knowledge is of priority. Inspired by [7], we pro-
pose a novel paradigm for encoding those prior to the HP-
Net through a set of accessible data from multiple identities.
On the dataset including Nzidentities, our goal is to learn
multiple hand avatars within a network, with the respective
identity codes z1...Nzand network parameters optimized.
Geometry Prior. For the hand geometry prior learning, we
first pre-train the PairOF module ( i.e. occupancy field) fO.
We utilize data from different hand shapes to make HPNet
learn the geometric priors of the hand which can transfer to
new identities. With hand meshes derived from input M, we
sample point clouds with Nopoints as training data [48].
Texture prior. Subsequently, we focus on learning the
texture prior knowledge of the hand. We observe that the
albedo features are specific to each identity, while the shad-
ows, which arise due to the hand‚Äôs self-occlusion, are com-
mon across different identities. Hence, we disentangle the
texture prior learning into albedo and shadow prior learn-
ing. Our methodology leverages a multi-resolution field
representation for simultaneous learning of both albedo and
shadow fields by incorporating field-specific information œà.
Identity-specific Albedo Field. Since the albedo of the
hands is dependent on the identity other than pose etc., we
setœà=zfor identity-specific albedo field modeling. For
query points q, we obtain the albedo aq=fA(q,M,z).
Identity-shared Shadow Field. Modeling identity-
specific environmental lighting conditions of hands like
HandAvatar [9] cannot capture shadow prior knowledge
that can generalize to one-shot reconstruction. To overcome
this problem, our approach aims to model a more univer-
sal shadow prior, grounded in the observation that shadows
from self-occlusion are largely consistent across different
hands when performing identical poses. Hence, we build
the shadow field with œà=Œ∏, where Œ∏is pose parameters
without global rotations. For query points q, we obtain the
shadow uq=fU(q,M,Œ∏).
Combining the albedo field and shadow field, we ob-
802
tain a complete Multi-resolution Texture Field (MTField).
For query points q, we derive the shaded color value cq=
uqaq‚ààRNq√ó3. For a casted camera ray r, we uniformly
sample Nqqueries {qi}Nq
i=1, each of which has occupancy o
and texture c. We render each pixel Irof image via volume
rendering [49]:
Ir=NqP
i=1 
i‚àí1Q
j=1(1‚àíoqj)!
oqicqi. (2)
Optimization. The training of HPNet consists of two steps.
In the first step, we pre-train fOwith the MANO pose pa-
rameters annotations and shape parameters randomly sam-
pled from a given range. The objective for optimizing
PairOF is to minimize the error between ground truth o‚àóand
predicted occupancy values, i.e.,Locc=1
NoP
q(oq‚àío‚àó
q)2.
In the second step, we conduct end-to-end training for the
mesh scaffold, occupancy field, and MTField. Specif-
ically, MTField is optimized with shadow regularization
Lshadow =‚à•U‚àí 1‚à•1and the reconstruction loss function
(consisting of LPIPS loss [77] and l1loss):
Lrec=LLPIPS(I,I‚àó) +‚à•I‚àíI‚àó‚à•1, (3)
where U,IandI‚àódenote the shadow values, the rendered
image, and the ground truth image. The shadow field is
regularized to be close to 1 to ensure that it focuses more
on the shadow caused by self-occlusion. The full loss is a
combination of the previous terms: L=Lshape +Locc+
Lrec+Œªshadow Lshadow , where Œªshadow is used to balance
different loss terms.
3.3. One-shot Hand Avatar Reconstruction Pipeline
After hand prior learning, HPNet encodes various prior
knowledge useful for one-shot reconstruction. For each in-
put image, we only optimize part of HPNet to reconstruct
the target hand avatar. The complete pipeline is as follows.
Preprocessing. We utilize the off-the-shelf hand pose esti-
mator [61] to predict the shape parameter Œ≤, pose parameter
Œ∏, and camera pose of the input images.
Geometry. Since it is impractical to learn a refined mesh
based on an image, we adopt meshes derived from hand
shape parameters Œ≤as the input for occupancy prediction.
Texture Inversion. To provide good prior knowledge for
the one-shot reconstruction, we first inverse a similar ap-
pearance from the MTField. More specifically, we solve
an optimization problem to find identity code z‚àóand per-
channel color calibration coefficients {w,b}that can pro-
duce a similar appearance to the target identity of the input
image. The color calibration alleviates the hand skin bias
caused by the data used for prior learning, assisting better
texture inversion for the target identity. It works as follows:
Àúaq=w‚àóaq+b. During the inversion optimization, we
keep the network weights frozen. Given an input image I‚àó
of the target identity, we optimize to render an image Ithat
is similar to the target identity using our MTField with z‚àóand{w,b}. This procedure is optimized with the recon-
struction loss function in Eq. (3): arg min
z‚àó,w,bLrec.
Texture Fitting. The goal of the texture fitting is to adapt
the weights of the MTField to capture the details of the tar-
get identity from the input image. We utilize prior knowl-
edge in this procedure by two means. First, we only fine-
tune the resolution-specific MLPs {Mk}K
k=1and texture
feature fusing MLP Mfuse of albedo field fA, while keep-
ing other components ( e.g., the point encodings and shadow
field) frozen. Hence, we can transfer the prior knowledge
(e.g., shadow prior) well to the target avatar. In this stage,
all learnable parameters are denoted as Œæ. Second, we ap-
ply view regularization to avoid overfitting the target view.
Specifically, we constraint the texture-fitting results of some
reference views with different poses {Ri}Nr
i=1to be close to
the rendering results before texture-fitting {ÀúRi}Nr
i=1, where
Nris the number of the generated reference view. With the
prior knowledge, our reconstructed hand avatar can achieve
stable animation while preserving the details of the target
identity. This fitting is conducted by minimizing a series of
the reconstruction loss function in Eq. (3):
arg min
ŒæLfit=Lrec(I,I‚àó) +ŒªrefNrX
i=1
Lrec(Ri,ÀúRi)
,
(4)
where Œªrefis used to balance different loss terms.
4. Experiment
4.1. Implementation Details and Metrics
Pre-training of PairOF. We adopt all right-hand annota-
tions in InterHand2.6M [50] for pre-training like HandA-
vatar [9]. To make the occupancy field more robust to new
identities, we randomly sample the hand shape parameters
Œ≤from a range ¬±3œÉensuring that the hand shapes are phys-
ically plausible. We set No= 256 for training with Locc.
End-to-end Prior Learning of HPNet. We utilized 21
subjects from the InterHand2.6M [50] training set as the
training data for HPNet. Specifically, each subject used one
capture and left out 7 unseen poses for evaluation. For the
albedo field, we set K= 4 andNp
k={512√ó2k‚àí1}4
k=1.
For the shadow field, we set K= 1 andNp
1= 256 . In
addition, we set Nn= 4,Nq= 64 ,Nc= 128 , and
Nd= 16 . The identity code zis learnable parameters ini-
tialized from a truncated normal distribution with standard
deviation œÉ= 0.02. The rendering resolution is 256√ó256.
One-shot Reconstruction. We use an off-the-shelf estima-
tor [61] to predict the MANO parameters of the in-the-wild
images. We set Nr= 7andŒªref= 0.2. The reconstruction
first takes 50steps for texture inversion and then 100steps
for texture fitting (56 minutes with an A100 GPU). For more
details, please refer to our supplementary materials.
Metrics. Consistent with previous works [9, 11, 52], we
803
GT Ours HandAvatar HARP LiveHand Handy GT Ours HandAvatar HARP LiveHand Handy
test_Capture0_ROM03_RT_No_Occlusion_cam400488_image14772
test_Capture0_ROM03_RT_No_Occlusion_cam400290_image14526
test_Capture0_ROM03_RT_No_Occlusion_cam400266_image14388test_Capture0_ROM03_RT_No_Occlusion_cam400265_image13248test_Capture0_ROM03_RT_No_Occlusion_cam400323_image13098
Figure 4. Qualitative comparison with state-of-the-art methods on InterHand2.6M [50]. The black box indicates the input image.
GT Ours LISA One-2-3-45
Figure 5. Comparison of novel view synthesis on Inter-
Hand2.6M [50]. The green box indicates the input view.
report LPIPS [77], PSNR, and SSIM [70] to reflect image
similarity as the metrics of rendering quality.
4.2. Evaluation of One-shot Avatar Reconstruction
We comprehensively compare OHTA with previous meth-
ods [9, 31, 52, 57] under the one-shot reconstruction sce-
nario with different data sources. Additionally, we bench-
mark against methods [9, 23, 71] that employ a large num-
ber of images, aiming to gauge the potential performance
ceiling in one-shot scenarios. Moreover, we conduct novel-
view comparisons with LISA [11] and One-2-3-45 [40].
InterHand2.6M. We conduct evaluations on the testing set
of InterHand2.6M [50], following HandAvatar [9]. The re-
sults of training on multi-view sequences are reported in [9].
The results of training with a single image ( i.e. one-shot) are
based on their open-source codes [9, 31, 52, 57]. We do not
update the hand pose parameters for HARP since the offi-
cial implementation for a single image may collapse when
updating them. As for Handy [57], we begin by converting
the MANO labels into Handy‚Äôs format. We then optimize
the latent vectors of the texture model in Handy and fit the
single image using differentiable rendering. As shown in
GT HandAvatar
 Ours
 Handy
Figure 6. Qualitative comparison on HanCo [82].
Tab. 1, our one-shot performance is significantly better than
the existing methods [9, 31, 52, 57]. Note that # denotes the
number of images. The qualitative comparisons in Fig. 4
demonstrate our robust performance for novel views and
poses. All the results demonstrate that data-driven priors
can benefit the one-shot performance remarkably. Mean-
while, our performance is comparable to that of methods
using all the images for training, which further validates the
quality of the avatars.
Fig. 5 presents the novel view synthesis comparisons
with LISA [11] and One-2-3-45 [40]. Due to the unavail-
ability of models, we use results reported in the original
LISA paper for comparison. Our one-shot performance can
capture more details than LISA, owing to our adequate prior
knowledge from the prior model. Due to the underutiliza-
tion of specific domain knowledge, the general 3D genera-
tion approach (One-2-3-45) cannot generate plausible novel
views for animatable hand avatar creation.
HanCo. To show OHTA is robust for the appearance dif-
ferent from the training data of the HPNet, we also compare
with existing methods on the HanCo [82] dataset. We take
HandAvatar [9] and Handy [57] for comparisons since they
show robust performance for one-shot reconstruction on In-
804
Method #Train PSNR ‚ÜëLPIPS‚ÜìSSIM‚Üë
SelfRecon [23] 11,757 26.38 14.21 0.879
HumanNeRF [71] 11,757 27.64 11.45 0.884
HandAvatar [9] 11,757 28.23 10.35 0.894
HandAvatar [9] 1 23.79 17.78 0.820
HARP [31] 1 19.82 22.49 0.761
LiveHand [52] 1 23.01 18.64 0.763
Handy [57] 1 25.56 14.98 0.794
Ours 1 26.11 12.93 0.864
Table 1. Evaluation results on InterHand2.6M.
Method PSNR ‚ÜëLPIPS‚ÜìSSIM‚Üë
HandAvatar [9] 19.76 14.41 0.846
Handy [57] 21.11 11.71 0.768
Ours 22.15 11.55 0.886
Table 2. Evaluation results on HanCo.
Fulltest_Capture0_ROM03_RT_No_Occlusion_cam400416_image 13146
test_Capture0_ROM03_RT_No_Occlusion_cam400431_image13230
w/o
Shadoww/o
Tex. Priorw/o
Color Calib .Input
Figure 7. Visual results for ablation study with in-the-wild input.
Figure 8. Robustness towards different input images. The images
above the figure are the corresponding input images.
terHand2.6M (Tab. 1). We take 1 sequence of 4 cameras for
the experiment, with 1 frame for training and others for test-
ing. For more details, please refer to our supplementary ma-
terials. As shown in Tab. 2, our method outperforms other
methods consistently in all metrics. We also show the quali-
tative comparisons in Fig. 6, demonstrating our method can
obtain consistent performance for free-pose animation.
4.3. Ablation Study
Our ablation studies contain two parts. One part is for justi-
fying our components of HPNet. The other part is to show
our designs are effective for one-shot reconstruction.Method PSNR ‚ÜëLPIPS‚ÜìSSIM‚Üë
Full Model 27.64 12.23 0.896
w/o Multi-resolution 27.11 12.96 0.890
w/o Shadow Field 27.09 12.62 0.889
w/o Shape Fitting 27.27 12.63 0.892
Table 3. Ablation of HPNet for unseen poses.
# Method PSNR ‚ÜëLPIPS‚ÜìSSIM‚Üë
1 Full Model 26.11 12.93 0.864
2 w/o Tex. Prior 26.01 14.35 0.863
3 w/o Tex. Fitting 25.87 13.11 0.863
4 w/o Inversion 25.47 13.26 0.860
5 w/o Color Calib. 26.05 13.09 0.862
6 w/o Regularization 25.76 13.56 0.857
Table 4. Ablation of one-shot reconstruction on InterHand2.6M.
Hand Prior Network. The evaluation is conducted in 7 un-
seen poses as described in Sec. 4.1. As shown in Tab. 3, em-
ploying multi-resolution fields contributes greatly to all the
metrics. Meanwhile, modeling pose-aware, identity-shared
shadow is beneficial for overall performance, validating the
existence of a shared self-occlusion effect for hands. At
last, we demonstrate learning identity-specific hand shape
offset is significantly important when the data is abundant
to remove the shape inaccuracy of MANO fitting.
One-shot Reconstruction. We show the effectiveness of
our strategies for one-shot reconstruction in Tab. 4 (on In-
terHand2.6M as Sec. 4.2) and Fig. 7 (in-the-wild). When
there is no texture prior (#2), a significant drop in LPIPS
is observed, indicating that details of the hands are miss-
ing (Fig. 7). When omitting texture fitting (#3), the perfor-
mance slightly drops, indicating that our inversion strategy
can optimize similar hands from the latent space when the
training data is adequate. When not performing inversion
(setting the identity features are zero vectors, #4), the re-
sults become worse. This further shows that inversion can
provide better prior knowledge for the one-shot reconstruc-
tion. When ignoring color calibration (#5), the performance
will degenerate even with the data captured under the same
camera setups. When using in-the-wild data for reconstruc-
tion (Fig. 7), adopting color calibration is of more signifi-
cance. We also demonstrate that overfitting the input image
without view regularization leads to poorer quality (#6). Fi-
nally, we qualitatively show that our self-occlusion model-
ing with a shadow field can contribute to more realistic hand
avatar animation (Fig. 7).
4.4. Robustness towards Diverse Inputs
To show the robustness of OHTA, we present our quantita-
tive and qualitative performance across diverse inputs.
Quantitative. As depicted in Fig. 8, OHTA gets similar
805
Figure 9. In-the-wild results from real-captured images and MSCOCO [38] dataset. The last line shows results on MSCOCO.
‚ÄúThanos ‚Äù ‚ÄúDr. Manhattan‚Äù ‚ÄúFlower Tattoo‚Äù ‚ÄúBlack Tattoo‚Äù Texture Editing Texture Editing
Figure 10. Text-to-avatar and texture editing.
PSNR and LPIPS for different inputs on InterHand2.6M.
Qualitative. In Fig. 9, we illustrate our results on var-
ious real-captured images and whole-body images of the
MSCOCO dataset [25, 38]. OHTA performs consistently
across different skin tones, hand poses, and viewpoints
for real-captured data. Also, OHTA can reconstruct hand
avatars from human images on MSCOCO, which further
justifies the robustness of OHTA. We refer the reader to the
supplementary material for additional qualitative results.
4.5. Application
Based on our one-shot framework, we can achieve 1) hand
avatar creation with text prompts [76], 2) hand avatar edit-ing of the geometry and appearance, and 3) identity-space
manipulation including appearance sampling and interpola-
tion. The results are shown in Fig. 1 and Fig. 10. For more
details, please kindly refer to our supplementary materials.
Text-to-avatar. We use ControlNet [76] to generate a hand
image with the hand mask and text prompts, then utilize
OHTA to reconstruct hand avatars from the hand images.
Editing. The geometry is based on the mesh scaffold. Thus,
we can edit the hand shape parameters Œ≤to edit the geom-
etry. For appearance editing, we can draw arbitrary content
on a target view of the hand avatar and then use OHTA to
update the edited content part with its mask.
Latent space manipulation. Our identity latent space is
continuous. Hence, we can sample the identity code from
a normal distribution to sample hand avatars. Further-
more, we can interpolate two target identity codes to obtain
smooth appearance interpolations between two identities.
5. Conclusion
In this paper, we present the first one-shot implicit hand
avatar creation approach, OHTA. OHTA consists of two
stages. The first stage focuses on learning the hand prior
on datasets with multiple identities. For the creation of one-
shot hand avatars, it is only necessary to optimize the sec-
ond stage with the learned prior, which includes texture in-
version and texture fitting. OHTA is robust for diverse input
images and is capable of solving various downstream tasks
with consistent animations. These tasks include hand avatar
creation with text prompts, hand geometry and appearance
editing, and manipulation of identity latent space.
Limitations and Future Works. OHTA exhibits sub-
optimal performance for input images that feature 1)
notably uneven lighting, and 2) highly inaccurate pose
estimation. Thus, further designing the approach to be more
robust for those situations is worthy of ongoing exploration.
806
References
[1] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar,
Christian Theobalt, and Gerard Pons-Moll. Learning to re-
construct people in clothing from a single rgb camera. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 1175‚Äì1186, 2019. 2
[2] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,
and Marcus Magnor. Tex2shape: Detailed full human
body geometry from a single image. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 2293‚Äì2303, 2019. 2
[3] Thiemo Alldieck, Hongyi Xu, and Cristian Sminchisescu.
imghum: Implicit generative models of 3d human shape and
articulated pose. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 5461‚Äì5470,
2021. 3
[4] Ziqian Bai, Feitong Tan, Zeng Huang, Kripasindhu Sarkar,
Danhang Tang, Di Qiu, Abhimitra Meka, Ruofei Du, Ming-
song Dou, Sergio Orts-Escolano, et al. Learning personal-
ized high quality volumetric head avatars from monocular
rgb videos. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 16890‚Äì
16900, 2023. 3
[5] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
Gehler, Javier Romero, and Michael J Black. Keep it smpl:
Automatic estimation of 3d human pose and shape from a
single image. In The European Conference on Computer
Vision , pages 561‚Äì578, 2016. 2
[6] Marcel C B ¬®uhler, Kripasindhu Sarkar, Tanmay Shah,
Gengyan Li, Daoye Wang, Leonhard Helminger, Ser-
gio Orts-Escolano, Dmitry Lagun, Otmar Hilliges, Thabo
Beeler, et al. Preface: A data-driven volumetric prior for
few-shot ultra high-resolution face synthesis. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 3402‚Äì3413, 2023. 2, 3
[7] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz,
Michael Zollhoefer, Shun-Suke Saito, Stephen Lombardi,
Shih-En Wei, Danielle Belko, Shoou-I Yu, et al. Authen-
tic volumetric avatars from a phone scan. ACM Transactions
on Graphics (TOG) , 41(4):1‚Äì19, 2022. 3, 4
[8] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,
and Andreas Geiger. Snarf: Differentiable forward skinning
for animating non-rigid neural implicit shapes. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 11594‚Äì11604, 2021. 3
[9] Xingyu Chen, Baoyuan Wang, and Heung-Yeung Shum.
Hand avatar: Free-pose hand animation and rendering from
monocular video. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
8683‚Äì8693, 2023. 1, 2, 3, 4, 5, 6, 7
[10] Yujin Chen, Zhigang Tu, Di Kang, Linchao Bao, Ying
Zhang, Xuefei Zhe, Ruizhi Chen, and Junsong Yuan. Model-
based 3d hand reconstruction via self-supervised learning. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 10451‚Äì10460, 2021. 1
[11] Enric Corona, Tomas Hodan, Minh V o, Francesc Moreno-
Noguer, Chris Sweeney, Richard Newcombe, and LingniMa. Lisa: Learning implicit shape and appearance of hands.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 20533‚Äì20543, 2022.
1, 2, 5, 6
[12] Boyang Deng, John P Lewis, Timothy Jeruzalski, Gerard
Pons-Moll, Geoffrey Hinton, Mohammad Norouzi, and An-
drea Tagliasacchi. Nasa neural articulated shape approxi-
mation. In The European Conference on Computer Vision ,
pages 612‚Äì628, 2020. 3
[13] Daiheng Gao, Yuliang Xiu, Kailin Li, Lixin Yang, Feng
Wang, Peng Zhang, Bang Zhang, Cewu Lu, and Ping Tan.
Dart: Articulated hand model with diverse accessories and
rich textures. Advances in Neural Information Processing
Systems , 35:37055‚Äì37067, 2022. 1
[14] Daiheng Gao, Xindi Zhang, Xingyu Chen, Andong Tan,
Bang Zhang, Pan Pan, and Ping Tan. Cyclehand: Increasing
3d pose estimation ability on in-the-wild monocular image
through cyclic flow. In Proceedings of the ACM Interna-
tional Conference on Multimedia , pages 2452‚Äì2463, 2022.
1
[15] Philip-William Grassal, Malte Prinzler, Titus Leistner,
Carsten Rother, Matthias Nie√üner, and Justus Thies. Neural
head avatars from monocular rgb videos. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18653‚Äì18664, 2022. 3
[16] Zhiyang Guo, Wengang Zhou, Min Wang, Li Li, and
Houqiang Li. Handnerf: Neural radiance fields for animat-
able interacting hands. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 21078‚Äì21087, 2023. 1, 2
[17] Tong He, John Collomosse, Hailin Jin, and Stefano Soatto.
Geo-pifu: Geometry and pixel aligned implicit functions for
single-view human reconstruction. Advances in Neural In-
formation Processing Systems , 33:9276‚Äì9287, 2020. 2
[18] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and
Tony Tung. Arch++: Animation-ready clothed human recon-
struction revisited. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 11046‚Äì11056,
2021. 2
[19] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei
Yang, and Ziwei Liu. Sherf: Generalizable human nerf from
a single image. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , 2023. 3
[20] Yangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang,
Boxi Wu, Wenxiao Wang, Binbin Lin, Debing Zhang, and
Deng Cai. One-shot implicit animatable avatars with model-
based priors. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2023. 3
[21] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and
Tony Tung. Arch: Animatable reconstruction of clothed hu-
mans. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 3093‚Äì3102,
2020. 2
[22] Zhisheng Huang, Yujin Chen, Di Kang, Jinlu Zhang, and
Zhigang Tu. Phrit: Parametric hand representation with im-
plicit template. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 14974‚Äì14984,
2023. 3
807
[23] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Sel-
frecon: Self reconstruction your digital avatar from monoc-
ular video. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5605‚Äì
5615, 2022. 2, 6, 7
[24] Yuheng Jiang, Kaixin Yao, Zhuo Su, Zhehao Shen, Haimin
Luo, and Lan Xu. Instant-nvr: Instant neural volumetric ren-
dering for human-object interactions from monocular rgbd
stream. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 595‚Äì605,
2023. 2
[25] Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen
Qian, Wanli Ouyang, and Ping Luo. Whole-body human
pose estimation in the wild. In The European Conference on
Computer Vision , pages 196‚Äì214, 2020. 8
[26] Kyungmin Jo, Gyumin Shim, Sanghun Jung, Soyoung Yang,
and Jaegul Choo. Cg-nerf: Conditional generative neural
radiance fields for 3d-aware image synthesis. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision , pages 724‚Äì733, 2023. 3
[27] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-
emplar fine-tuning for 3d human model fitting towards in-
the-wild 3d human pose estimation. In 2021 International
Conference on 3D Vision (3DV) , pages 42‚Äì52. IEEE, 2021.
2
[28] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Ji-
tendra Malik. End-to-end recovery of human shape and pose.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 7122‚Äì7131, 2018. 2
[29] Tero Karras, Miika Aittala, Samuli Laine, Erik H ¬®ark¬®onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. Advances in Neural Infor-
mation Processing Systems , 34:852‚Äì863, 2021. 2
[30] Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar
Hilliges, and Siyu Tang. A skeleton-driven neural occupancy
representation for articulated hands. In International Confer-
ence on 3D Vision , pages 11‚Äì21, 2021. 3
[31] Korrawe Karunratanakul, Sergey Prokudin, Otmar Hilliges,
and Siyu Tang. Harp: Personalized hand reconstruction from
a monocular rgb video. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12802‚Äì12813, 2023. 1, 2, 6, 7
[32] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges,
and Michael J. Black. Pare: Part attention regressor for 3d
human body estimation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 11127‚Äì
11137, 2021. 2
[33] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and
Kostas Daniilidis. Learning to reconstruct 3d human pose
and shape via model-fitting in the loop. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 2252‚Äì2261, 2019.
[34] Verica Lazova, Eldar Insafutdinov, and Gerard Pons-Moll.
360-degree textures of people in clothing from a single im-
age. In International Conference on 3D Vision , pages 643‚Äì
653, 2019. 2
[35] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier
Romero. Learning a model of facial shape and expressionfrom 4d scans. ACM Transactions on Graphics (ToG) , 36
(6):194‚Äì1, 2017. 3
[36] Yuwei Li, Minye Wu, Yuyao Zhang, Lan Xu, and Jingyi Yu.
Piano: A parametric hand bone model from magnetic reso-
nance imaging. In Proceedings of the Thirtieth International
Joint Conference on Artificial Intelligence, IJCAI-21 , pages
816‚Äì822, 2021. 2
[37] Yuwei Li, Longwen Zhang, Zesong Qiu, Yingwenqi Jiang,
Nianyi Li, Yuexin Ma, Yuyao Zhang, Lan Xu, and Jingyi Yu.
Nimble: a non-rigid hand model with bones and muscles.
ACM Transactions on Graphics (TOG) , 41(4):1‚Äì16, 2022.
1, 2
[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In The
European Conference on Computer Vision , pages 740‚Äì755,
2014. 8
[39] Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu,
Yachao Zhang, and Xiu Li. Consistent123: One image to
highly consistent 3d asset using case-aware diffusion priors.
arXiv preprint arXiv:2309.17261 , 2023. 3
[40] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang
Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh
in 45 seconds without per-shape optimization. Advances in
Neural Information Processing Systems , 2023. 3, 6
[41] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 9298‚Äì9309, 2023. 3
[42] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft ras-
terizer: A differentiable renderer for image-based 3d reason-
ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 7708‚Äì7717, 2019. 4
[43] Stephen Lombardi, Tomas Simon, Gabriel Schwartz,
Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-
ture of volumetric primitives for efficient neural rendering.
ACM Transactions on Graphics (ToG) , 40(4):1‚Äì13, 2021. 3
[44] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. In Seminal Graphics Papers: Pushing
the Boundaries, Volume 2 , pages 851‚Äì866. 2023. 2, 3
[45] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,
Gerard Pons-Moll, Siyu Tang, and Michael J Black. Learn-
ing to dress 3d people in generative clothing. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 6469‚Äì6478, 2020. 2
[46] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 4460‚Äì4470, 2019. 3
[47] Marko Mihajlovic, Yan Zhang, Michael J Black, and Siyu
Tang. Leap: Learning articulated occupancy of people. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 10461‚Äì10471, 2021. 3
[48] Marko Mihajlovic, Shunsuke Saito, Aayush Bansal, Michael
Zollhoefer, and Siyu Tang. Coap: Compositional articulated
808
occupancy of people. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
13201‚Äì13210, 2022. 3, 4
[49] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In The European Conference on Computer Vision ,
pages 405‚Äì421, 2020. 2, 5
[50] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori,
and Kyoung Mu Lee. Interhand2. 6m: A dataset and base-
line for 3d interacting hand pose estimation from a single rgb
image. In The European Conference on Computer Vision ,
pages 548‚Äì564, 2020. 5, 6
[51] Jiteng Mu, Shen Sang, Nuno Vasconcelos, and Xiaolong
Wang. Actorsnerf: Animatable few-shot human rendering
with generalizable nerfs. pages 18391‚Äì18401, 2023. 2
[52] Akshay Mundra, Jiayi Wang, Marc Habermann, Christian
Theobalt, Mohamed Elgharib, et al. Livehand: Real-time
and photorealistic neural hand rendering. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, 2023. 1, 2, 5, 6, 7
[53] Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black.
Star: Sparse trained articulated human body regressor. In The
European Conference on Computer Vision , pages 598‚Äì613,
2020. 2
[54] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and
Michael J Black. Expressive body capture: 3d hands,
face, and body from a single image. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10975‚Äì10985, 2019. 2, 3
[55] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan
Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-
matable neural radiance fields for modeling dynamic human
bodies. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 14314‚Äì14323, 2021. 2
[56] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,
Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Implicit neural representations with structured latent codes
for novel view synthesis of dynamic humans. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 9054‚Äì9063, 2021. 3
[57] Rolandos Alexandros Potamias, Stylianos Ploumpis,
Stylianos Moschoglou, Vasileios Triantafyllou, and Stefanos
Zafeiriou. Handy: Towards a high fidelity 3d hand shape
and appearance model. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 4670‚Äì4680, 2023. 1, 2, 6, 7
[58] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
rokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:
One image to high-quality 3d object generation using both
2d and 3d diffusion priors. In The 12th International Con-
ference on Learning Representations , 2024. 3
[59] Neng Qian, Jiayi Wang, Franziska Mueller, Florian Bernard,
Vladislav Golyanik, and Christian Theobalt. Html: A para-
metric hand texture model for 3d hand reconstruction andpersonalization. In The European Conference on Computer
Vision , pages 54‚Äì71. Springer, 2020. 1, 2
[60] Wentian Qu, Zhaopeng Cui, Yinda Zhang, Chenyu Meng,
Cuixia Ma, Xiaoming Deng, and Hongan Wang. Novel-
view synthesis and pose estimation for hand-object interac-
tion from sparse views. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 15100‚Äì
15111, 2023. 1, 2
[61] Pengfei Ren, Chao Wen, Xiaozheng Zheng, Zhou Xue,
Haifeng Sun, Qi Qi, Jingyu Wang, and Jianxin Liao. De-
coupled iterative refinement framework for interacting hands
reconstruction from a single rgb image. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 8014‚Äì8025, 2023. 5
[62] Javier Romero, Dimitrios Tzionas, and Michael J Black. Em-
bodied hands: modeling and capturing hands and bodies to-
gether. ACM Transactions on Graphics (ToG) , 36(6):245:1‚Äì
245:17, 2017. 2, 3, 4
[63] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 2304‚Äì2314, 2019. 2
[64] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
Joo. Pifuhd: Multi-level pixel-aligned implicit function for
high-resolution 3d human digitization. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 84‚Äì93, 2020. 2
[65] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J
Black. Scanimate: Weakly supervised learning of skinned
clothed avatar networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 2886‚Äì2897, 2021. 3
[66] Shih-Yang Su, Frank Yu, Michael Zollh ¬®ofer, and Helge
Rhodin. A-nerf: Articulated neural radiance fields for learn-
ing human shape, appearance, and pose. Advances in Neural
Information Processing Systems , 34:12278‚Äì12291, 2021. 2
[67] David Svitov, Dmitrii Gudkov, Renat Bashirov, and Victor
Lempitsky. Dinar: Diffusion inpainting of neural textures
for one-shot human avatars. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 7062‚Äì
7072, 2023. 2
[68] Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, and Gerard
Pons-Moll. Neural-gif: Neural generalized implicit func-
tions for animating people in clothing. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 11708‚Äì11718, 2021. 3
[69] Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas
Geiger, and Siyu Tang. Metaavatar: Learning animatable
clothed human models from few depth images. Advances
in Neural Information Processing Systems , 34:2810‚Äì2822,
2021. 3
[70] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE Transactions on Image Process-
ing, 13(4):600‚Äì612, 2004. 6
[71] Chung-Yi Weng, Brian Curless, Pratul P Srinivasan,
Jonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-
809
mannerf: Free-viewpoint rendering of moving people from
monocular video. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
16210‚Äì16220, 2022. 2, 6, 7
[72] Tong Wu, Jia-Mu Sun, Yu-Kun Lai, and Lin Gao. De-nerf:
Decoupled neural radiance fields for view-consistent appear-
ance editing and high-frequency environmental relighting. In
ACM SIGGRAPH 2023 Conference Proceedings , pages 1‚Äì
11, 2023. 3
[73] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu.
H-nerf: Neural radiance fields for rendering and temporal
reconstruction of humans in motion. Advances in Neural In-
formation Processing Systems , 34:14955‚Äì14966, 2021. 2
[74] Bangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, Yinda
Zhang, Zhaopeng Cui, and Guofeng Zhang. Neumesh:
Learning disentangled neural mesh-based implicit field for
geometry and texture editing. In The European Conference
on Computer Vision , pages 597‚Äì614, 2022. 3
[75] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance fields from one or few images.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 4578‚Äì4587, 2021. 3
[76] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836‚Äì3847, 2023. 8
[77] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 586‚Äì595, 2018. 5, 6
[78] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang
Zhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently gen-
erated human radiance field from sparse inputs. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 7743‚Äì7753, 2022. 2
[79] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.
Pamir: Parametric model-conditioned implicit representa-
tion for image-based human reconstruction. IEEE transac-
tions on pattern analysis and machine intelligence , 44(6):
3170‚Äì3184, 2021. 2
[80] Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yan-
dong Guo, and Yebin Liu. Structured local radiance fields for
human avatar modeling. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 15893‚Äì15903, 2022. 3
[81] Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang
Yang. Detailed human shape estimation from a single im-
age by hierarchical mesh deformation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 4491‚Äì4500, 2019. 2
[82] Christian Zimmermann, Max Argus, and Thomas Brox.
Contrastive representation learning for hand shape estima-
tion. In DAGM German Conference on Pattern Recognition ,
pages 250‚Äì264. Springer, 2021. 6
810
