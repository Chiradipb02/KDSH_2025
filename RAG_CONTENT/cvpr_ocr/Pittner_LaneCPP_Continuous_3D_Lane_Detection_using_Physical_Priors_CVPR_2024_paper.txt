LaneCPP: Continuous 3D Lane Detection using Physical Priors
Maximilian Pittner1,2, Joel Janai1, Alexandru P. Condurache1,2
1Bosch Mobility Solutions, Robert Bosch GmbH
2Institute of Signal Processing, University of L ¨ubeck
{Maximilian.Pittner, Joel.Janai, AlexandruPaul.Condurache }@de.bosch.com
Abstract
Monocular 3D lane detection has become a fundamen-
tal problem in the context of autonomous driving, which
comprises the tasks of finding the road surface and locat-
ing lane markings. One major challenge lies in a flexible
but robust line representation capable of modeling complex
lane structures, while still avoiding unpredictable behav-
ior. While previous methods rely on fully data-driven ap-
proaches, we instead introduce a novel approach LaneCPP
that uses a continuous 3D lane detection model leverag-
ing physical prior knowledge about the lane structure and
road geometry. While our sophisticated lane model is ca-
pable of modeling complex road structures, it also shows
robust behavior since physical constraints are incorporated
by means of a regularization scheme that can be analyti-
cally applied to our parametric representation. Moreover,
we incorporate prior knowledge about the road geometry
into the 3D feature space by modeling geometry-aware spa-
tial features, guiding the network to learn an internal road
surface representation. In our experiments, we show the
benefits of our contributions and prove the meaningfulness
of using priors to make 3D lane detection more robust. The
results show that LaneCPP achieves state-of-the-art perfor-
mance in terms of F-Score and geometric errors.
1. Introduction
Robust and precise lane detection systems build one of the
most essential components in the perception stack of au-
tonomous vehicles. While some approaches utilize LiDAR
sensors or multi-sensor setups, the application of monoc-
ular cameras has become more popular due to their lower
cost and the high-resolution visual representation that pro-
vides valuable information to detect lane markings.
In the past, lane detection was mainly treated as a 2D de-
tection task. Deep learning based methods achieved good
results by treating the problem as a segmentation task in
pixel space [7, 11, 16, 26, 29, 33, 53], used to classify and
regress lanes using anchor-based [19, 41] representations,or as key-points on a grid structure [13, 15, 34, 45]. How-
ever, due to the lack of depth information, these 2D repre-
sentations fail to model lane markings and road geometry in
3D space, which forms an important prerequisite for later
functionalities like trajectory planning. Consequently, ap-
proaches for monocular 3D lane detection were introduced,
which adapted lane representations for the 3D domain by
modeling vertical anchors [6, 9] or local segments on a grid
[4] in a Birds-Eye-View (BEV) oriented 3D-frame.
A crucial topic for the application of lane detection al-
gorithms in autonomous systems is safety, which requires
predictable and robust behavior in any traffic situation. One
risk of learning-based methods is the tendency to show un-
predictable behavior in cases of rarely observed scenarios.
Since obtaining large amounts of data with high-quality an-
notations is cumbersome and expensive, publicly available
3D datasets are limited in size and accuracy. Hence, they
do not reflect the variability of real-world scenarios suffi-
ciently. This makes learning-based models prone to overfit-
ting, and eventually, diminishes predictability.
One common way to deal with such problems is the inte-
gration of prior knowledge. Physics provides us a profound
understanding of the 3D world, allowing us to make valid
assumptions about the lane structure and road surface ge-
ometry. Therefore, we introduce physically motivated pri-
ors into the lane detection objective to cope with the limited
data problem and achieve robust and predictable behavior.
There are certain geometric properties that should gen-
erally hold for detected lane lines. For instance, we know
that most lines progress parallel to each other, reside on a
smooth surface and should not exceed certain thresholds in
terms of curvature and slope. However, integrating such
assumptions into prevailing discrete representations is not
straight forward as strong simplifications are necessary. In
contrast, continuous 3D lane representations directly pro-
vide parametric curves using polynomials [1, 23] or more
sophisticated B-Splines [32]. These allow for analytical
computations on the curve function, which enables the inte-
gration of such priors into the lane representation. By mod-
eling these priors explicitly instead of learning them from
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10639
data, the model can focus its full capacity on learning richer
features for the lane detection task.
We can further use physical knowledge about the road
geometry to support the model in learning an internal trans-
formation from image features to 3D space. While methods
based on Inverse Perspective Mapping (IPM) [4, 6, 9, 17,
23, 32] make false flat-ground assumptions, learning based
transformations [1, 2, 46] completely ignore road proper-
ties. In contrast, integrating prior knowledge about the road
surface allows us to model 3D features geometry-aware and
helps the network to focus on the 3D region of interest.
Thus, we propose a novel 3D lane detection approach
named LaneCPP that leverages valuable prior knowledge to
achieve accurate and robust perception behavior. It intro-
duces a new sophisticated continuous curve representation,
which enables us to incorporate physical priors. In addition,
we present a spatial transformation component for learning
a physically inspired mapping from 2D image to 3D space
providing meaningful spatial features.
Our main contributions can be summarized as follows:
• We propose a novel architecture for 3D lane detection
from monocular images using a more sophisticated flexi-
ble parametric spline-based lane representation.
• We present a way to incorporate priors about lane struc-
ture and geometry into our continuous representation.
• We introduce a new way to use prior knowledge about the
road surface geometry for learning spatial features.
• We demonstrate the benefits of our contributions in sev-
eral ablation studies.
• We show state-of-the-art performance of our model.
2. Related work
Different Lane Representations . An important design
choice in deep learning based lane detection is the rep-
resentation that the network uses to model lane line ge-
ometry, which can be categorized as follows: 1) Pixel-
wise representations, which formulate lane detection as a
segmentation problem, were used mainly in 2D methods
[7, 11, 16, 26, 29, 33, 51, 53] and were adopted in 3D
by SALAD [50] combining line segmentation with depth-
prediction. These representations come with high com-
putational load since a large amount of parameters is re-
quired. 2) Grid-based approaches divide the space into
cells and model lanes using local segments [13] or key-
points [15, 34, 45]. 3D-LaneNet+ [4] suggests to use local
line-segments and BEV-LaneDet [46] defines key-points on
a BEV grid representation. Both depend on the grid res-
olution and require costly post-processing to obtain lines.
3) Anchor-based representations [19, 40, 41, 52] model
lines as straight anchors with positional offsets at prede-
fined locations. They are widely used in 3D detection
approaches including 3D-LaneNet [6] and Gen-LaneNet
[9], which use vertical anchors in the top-view, and An-chor3DLane [12], introducing anchor projection with it-
erative regression. Similar to grid-based representations,
it requires subsequent curve-fitting to obtain smooth lines.
4) Continuous curve representations [5, 22, 24, 43, 44] in-
stead directly model smooth curves without requiring costly
post-processing. While CLGO [23] and CurveFormer [1]
use simple polynomials, 3D-SpLineNet [32] proposes B-
Splines [3]. Since B-Splines offer local control over curve
segments, they are compatible to model complex shapes
with low-degree basis functions, while polynomials and
B´ezier curves show global dependence and thus require
higher degrees causing expensive computation. Although
3D-SpLineNet achieves superior detection performance on
synthetic data, it unfortunately lacks flexibility as the curve
formulation is limited to monotonically progressing lanes,
making it hardly applicable to real-world data. To resolve
this issue, we propose a more flexible representation based
on actual 3D B-Splines. In contrast to discrete grids and an-
chors, continuous representation even allow us to integrate
prior knowledge in an analytical manner.
Geometry Priors . Several approaches suggest to in-
corporate prior knowledge into learning-based methods,
e.g. by integrating invariance into the model architecture
[35, 36] or task-specific transformations as for trajectory
planning [10, 47, 49]. In the field of lane detection, line par-
allelism has been formulated as a hard constraint to resolve
depth ambiguity and determine camera parameters [27, 48].
Deep declarative networks [8] offer a general framework
to incorporate arbitrary properties as constraints, by solv-
ing a constrained optimization problem in the forward pass.
While such methods are appropriate when hard constraints
must be enforced, our goal is rather to guide the network
in learning typical geometric lane properties by formulat-
ing soft constraints in a regularization objective. Such a
regularization only affects training and does not require re-
solving an optimization problem in the forward pass, and
thus, comes without additional computational cost during
inference. Following this paradigm, SGNet [24, 40] pro-
poses to penalize the deviation of lateral distance from a
constant lane width in the IPM warped top-view, but ignores
that the property does not hold for lines deviating from the
ground plane. GP [17] presents a parallelism loss that en-
forces constant distance between nearest neighbors locally,
which depends on the number of anchor points. In contrast,
our method presents a way to learn parallelism globally and
independent of resolutions of discrete lane representations.
We propose an elegant way to learn parallelism as well as
other geometry priors using analytical formulations of tan-
gents and normals, which are well-defined on our continu-
ous spline representation.
Leveraging 3D Features . An important model com-
ponent consists in the extraction of 3D features, encoding
valuable information to detect lanes along the road surface.
10640
Figure 1. Our approach: First, front-view image Iis propagated through the backbone extracting multi-scale feature maps. These are
transformed to 3D using our spatial transformation and then fused to obtain a single 3D feature map. Feature pooling is applied to obtain
features for each line proposal that are propagated through fully connected layers to obtain the parameters for our line representation.
Finally, prior knowledge is exploited to regularize the lane representation and to produce surface hypotheses for the spatial transformation.
While some works predict 3D lanes directly from the front-
view, e.g. by utilizing pixel-wise depth estimation [50] or
3D anchor-projection mechanisms [12], prevalent methods
employ an intermediate 3D or BEV feature representation
with an internal transformation from the front-view to the
3D space. 3D-LaneNet [6] proposes to utilize IPM [25] to
project front-view features to a flat road plane due to the
spatial correlation between the warped top-view image and
3D lane geometry and was adopted in several other works
[4, 9, 17, 23, 32]. However, IPM causes visual distortions in
the top-view representation when the flat road assumption
is violated. In related fields like BEV semantic segmen-
tation, BEV transformations are learned via Multi-Layer-
Perceptrons (MLPs) [18, 28], depth prediction [31, 37, 38]
or transformer-based attention mechanisms [20, 30, 39]. In
3D lane detection, PersFormer [2] utilizes attention between
front- and top-view, CurveFormer [1] introduces dynamic
3D anchors that model queries as parametric curves and
BEV-LaneDet [46] uses MLPs for the spatial transforma-
tions. However, these learned transformations do not nec-
essarily provide a 3D feature representation since they are
not guided by valuable priors about the road surface ge-
ometry, which potentially results in unforeseen behavior
for out-of-distribution data. Our approach instead aims for
carefully modeling a geometry-aware feature space using
a depth classification method inspired by [31] that exploits
knowledge about the distribution of the road surface.
3. Methodology
The following section describes our 3D lane detection ap-
proach. An overview of the overall architecture is described
and illustrated in Fig. 1. The main focus lies on our contin-
uous 3D lane line representation, our regularization mecha-
nism using physical priors and our prior-based spatial trans-
Figure 2. Our 3D lane line representation: For each proposal ¯f
(purple lines), line geometry is described by 3D B-Splines with
control points ck(green dots). Each control point is determined
by the offsets αk, βkfrom the control points of the initial proposal
in normal direction (orange vectors). Additionally, visibility v(t)
is modeled by splines with 1D control points γk.
formation module, which we explain in the following.
3.1. Lane line representation
Inspired by prior work in 3D lane detection [32], we lever-
age the benefits of continuous representations and employ a
parametric model based on B-Splines. However, modeling
only lateral ( x-) and vertical ( z-) components with spline-
based functions (as done in previous approaches) is limited
to lanes that merely progress along the longitudinal ( y-) di-
rection. Instead, we propose the first full 3D lane line rep-
resentation modeling each component ( x,y,z) such that we
10641
obtain
f(t) =
x(t)
y(t)
z(t)
=KX
k=1ck·Bk,d(t) (1)
with curve argument t∈[0,1]andKcontrol points ck= 
xk, yk, zkT. Each control point ckweights the respective
basis function Bk,d(t)(recursive polynomials of degree d)
controlling the curve shape.
Due to the ambiguity of curves using 3D B-Splines (the
same spline curve can be described by different configura-
tions of its control points), regressing all three dimensions
per control point results in strong overfitting during training.
We resolve this issue by limiting the degrees of freedom per
control point to two and constraining the control points de-
flection to one direction in the x-y-plane and one direction
in the y-z-plane as illustrated in Fig. 2. More precisely, the
degrees of freedom per control point are specified by the
directions of the normals NxyandNzof an initial curve
proposal ¯fwith control points ¯ck= 
¯xk,¯yk,¯zkT. The
control points are then defined as
ck=
xk
yk
zk
=
¯xk+ Nx·αk
¯yk+ Ny·αk
¯zk+ Nz·βk
, (2)
where Nx,Nydescribe the x- and y-component of the nor-
mal vector Nxyin the x-y-plane. As shown in Eq. (2) and
illustrated in Fig. 2, modeling splines as deflections in nor-
mal direction of its underlying initial line proposal only re-
quires two parameters αk, βkper control point to describe
the 3D shape. We use a wide variety of orientations for the
initial proposals ¯f(see Fig. 2), which allows us to detect
any kind of lines with this formulation. More details about
the initial proposals are provided in the supplementary.
While [32] models the curve range using start- and end-
points that are learned by means of regression, we instead
propose to model visibility1using a continuous representa-
tionv(t)and treat the visibility estimation as a classification
problem. We obtain probability values applying sigmoid
activation and consider σ 
v(t)
>0.5the visible range.
While in theory any kind of function can be utilized, we
found that B-Splines with the same configuration as f(t)
are well-suited and introduce spline control points γkdefin-
ing the shape of v(t).
Eventually, binary cross-entropy is used as a classifica-
tion loss to learn visibility
Lvis=−1
|PGT|X
p∈PGTˆvp·log 
σ 
v(tp)
+ (3)
(1−ˆvp)·log 
1−σ 
v(tp)
, (4)
1For the concept of visibility, we follow the prevailing definition from
the literature [2, 9].
Figure 3. Illustration of different priors expressed by line tangents
and surface normals.
where PGT denotes the ground truth set of points,
ˆvp∈ {0,1}the ground truth visibility for point p.tprepre-
sents the respective curve argument obtained by orthogonal
projection of ponto the underlying line proposal.
3.2. Regularization using physical priors
In this section, we describe our regularization method to in-
tegrate prior knowledge about lane structure and surface ge-
ometry into our parametric line representation (see Fig. 3).
Line parallelism. In order to reinforce parallel lines, the
tangents at point pairs located in opposite normal direction
on neighboring lines must be similar (see Fig. 3 left). We
realize this by penalizing the cosine distance of the unit tan-
gentsT(t)on neighboring lines iandjfor normal point
pairs. More precisely, for each point p∈ P(i)on line iwe
select the normal pair point p∗on neighbor line jthat mini-
mizes the distance to the normal plane, which is defined by
the plane equation T(i)(t)T· 
(x, y, z )T−f(i)(t)
= 0.
In Fig. 3 the normal planes are visualized in a 2D top-view
as lines (orange) for simplicity. Hence the respective curve
argument tp∗for point p∗on line jis given as
tp∗= argmin
p′∈P(j)T(i)(tp)T· 
f(j)(tp′)−f(i)(tp)
,(5)
where P(j)denotes the points on line j. While in theory
Eq. (5) can be solved analytically, the simpler way is to
sample the set of points P(j)instead. (Note that our contin-
uous representation allows us to choose high sampling rates
without losing precision as no interpolation is required.)
With the normal point pairs, we define the parallelism
loss for a neighbor line pair based on the cosine distance of
their tangents as
L(ij)
par=1(ij)
p
|P(i)|·X
p∈P(i)1− 
T(i)(tp)T·T(j)(tp∗).(6)
Since the criterion of line parallelism should not hold for
all normal point pairs of neighboring lines (e.g. merging
or splitting lines), 1(ij)
p∈ {0,1}represents the indicator
10642
function determining whether the parallelism loss is applied
to the point pair. More precisely, the function ensures that
only the overlapping range of neighboring lines is taken into
account. Furthermore, it determines whether the line pair
should be considered as a parallel pair based on the stan-
dard deviation of euclidean distances between normal point
pairs, i.e. high deviations indicate that the line pair might
belong to a merge or split structure. In our experiments, we
achieve state-of-the-art performance on test sets containing
merges and splits, proving that our model is also capable of
learning non-parallel line pairs using this indicator function.
Surface smoothness. Since the lines reside on a smooth
road, the surface normals of neighboring lanes should be
similar. Analogously to Lpar, we express this with the co-
sine distance between surface normals N(ih)andN(ij)as
L(i)
sm=1(hij)
p
|P(i)|·X
p∈P(i)1− 
N(ih)(tp)T·N(ij)(tp),(7)
with indicator function 1(hij)
p. The surface normal between
lineiand left neighbor line hat point pcan be expressed as
the cross product of the tangent on line iand the normalized
connection vector between lines iandh, hence N(ih)(tp) =
T(i)(tp)×f(h)(tp∗)−f(i)(tp)
||f(h)(tp∗)−f(i)(tp)||. For the normal between line
iand right neighbor jthe sign is flipped to obtain upwards
pointing normal vectors.
Curvature. We determine lane curvature by computing
the second order derivatives as the difference of tangents
at consecutive points divided by their euclidean distance
asT′(tp) =T(tp)−T(tp−∆t)
||f(tp)−f(tp−∆t)||. The maximum curvature
inx-y-plane (inverse curve radius) and in z(rate of slope
change) have very different value ranges and are therefore
restricted by different limits. Hence, we define the two
thresholds κxyandκzand formulate the curvature loss on
lineias
L(i)
curv=1
|P(i)|·X
p∈P(i)max 
T′(i)
xy(tp), κxy
(8)
+ max 
T′(i)
z(tp), κz
. (9)
Finally, the prior regularization loss is given as
Lprior =MX
i=1λsmL(i)
sm+λcurvL(i)
curv+NX
j=1λparL(ij)
par,
(10)
with individual weights λpar, λsm, λcurv. Note that all
these properties are expressible by means of tangents and
normals, which can be computed analytically on our para-
metric representation in continuous space. Consequently,
minimization of the herein introduced prior losses does
not depend on numerical approximations as is the case for
anchor-, grid- or key-point representations.
Figure 4. Our proposed spatial transformation module. First, sev-
eral road surface hypotheses are defined (a) to which front-view
features are lifted (b) and weighted according to the predicted
depth distribution. Afterwards, point features are aggregated in
a weighted manner to obtain the 3D feature map (c).
3.3. Spatial transformation
In this section, we describe our spatial transformation
(shown in Fig. 4) that is leveraging valuable physical knowl-
edge about surface geometry. We know that the road sur-
face typically shows small deviations from the ground level
(z= 0) in the near-range and stronger deviations in the far-
range. Based on this knowledge, we sample ground surface
hypotheses that reflect the distribution of the road surface
height profile (Fig. 4a). While in theory different types of
surface functions could be utilized as hypotheses, we decide
to merely rely on planes, since this facilitates the computa-
tion of ray intersections described in the following step.
Next, the multi-scale front-view feature maps extracted
by the backbone are lifted to 3D space (Fig. 4b). Our ap-
proach is inspired by [31], where front-view features are
spreading along rays throughout the space of the road sur-
face. These rays intersect with the surface hypotheses at
different depths spanning a frustum-like point cloud in 3D
space, where each point is affiliated with a C-dimensional
feature vector and additionally attached with its height value
z, hence, each point in the cloud has dimension (C+ 1) .
The front-view feature map is propagated through a depth
branch with a channel-wise softmax applied to obtain a cat-
egorical distribution for each ray, resulting in a tensor of
sizeH×W×S, where H,Wdenote height and width and
channel size Sthe number of surface hypotheses.
In order to aggregate the information in 3D space, a BEV
grid of size X×Yis defined. Features from points map-
ping to the same grid cell are weighted by the categorical
depth distribution for the respective ray and accumulated in
terms of a weighted sum (Fig. 4c). Since the z-component
10643
Priors F1(%) ↑X-near(m) ↓X-far(m) ↓Z-near(m) ↓Z-far(m) ↓
None 65.0 0 .316 0 .384 0 .106 0 .153
Par. 66.2 0 .291 0 .373 0 .103 0 .150
Surf. 65.8 0 .320 0 .356 0 .103 0 .144
Curv. 66.7 0 .322 0 .366 0 .105 0 .146
Comb. 66.7 0.301 0.359 0.103 0.144
Table 1. Effect of different prior losses on OpenLane300.
# Surface Hypotheses 1 3 5 15 27
F1-Score(%) ↑ 65.0 65.9 66.6 66.1 66.0
Table 2. Effect of the surface hypotheses on OpenLane300.
Lane Rep. Prior Reg. Spatial T. F1(%) ↑Gain(%)
62.9 (baseline)
X 65.0 +2 .1
X X 66.7 +3 .8
X X 66.6 +3 .7
X X X 66.9 +4.0
Table 3. Performance gain for different contributions on Open-
Lane300 using our novel Lane Representation ,Prior Regular-
ization andSpatial Transformation instead of IPM.
of the points is also combined by a weighted sum, the value
zuvcan be interpreted as the height value of the surface for
grid cell (u, v). We guide the model in learning the real
surface and prevent it from learning an arbitrary mapping
by introducing a simple grid-based regression loss as
Lsurf=1
X·YX
(u,v)∈X×Y1uv· ∥zuv−ˆzuv∥1,(11)
with 1uvindicating whether surface ground truth ˆzuvis
available for cell (u, v). The height ground truth is obtained
by interpolation of the 3D lane annotations at cell locations.
3.4. Loss functions
The overall loss used during training is given as the
weighted sum of loss components
L=λprLpr+λcatLcat+λregLreg+ (12)
λvisLvis+λpriorLprior +λsurfLsurf. (13)
We use focal loss [21] for lane presence Lprand category
classification Lcat. For the regression loss Lreg, we adapt
the formulation of [32] to three instead of two dimensions.
More details are provided in the supplementary.
4. Experiments
We first describe our experimental setup and then analyze
our approach on two 3D lane datasets.
Figure 5. Qualitative comparison of our model trained with
prior regularization to the same model without regularization both
trained on OpenLane300 with main differences highlighted by ar-
rows. As a reference ground truth lines are visualized dashed.
4.1. Experimental setup
We evaluate our method on two different datasets: Open-
Lane and Apollo 3D Synthetic - both containing 3D lane
ground truth as well as camera parameters per frame.
OpenLane [2] is a real-world dataset containing 150,000
images in the training and 40,000 in the test set from 1000
different sequences. In order to evaluate different driving
scenarios the test set is divided into different situations,
namely Up & Down ,Curve ,Extreme Weather ,Night ,In-
tersection andMerge & Split . For ablation studies we use
the smaller version OpenLane300 including 300 sequences.
Apollo 3D Synthetic [9] is a small synthetic dataset,
consisting of only 10,500 examples from rather simple sce-
narios of highway, urban and rural environments. The data
is split into three subsets, (1) Standard (simple) scenarios,
(2) Rare Scenes and(3) Visual Variations .
Evaluation metrics. For the quantitative evaluation both
datasets utilize the evaluation scheme proposed in [9]. It
evaluates the euclidean distance at uniformly distributed
points in the range of 0-100m along the y-direction. Based
on the mean distance and range, F1-Score is computed, as
well as the mean x- and z-errors innear- (0-40m) and far-
range (40-100m) to evaluate geometric accuracy.
Baseline. Our approach builds up on 3D-SpLineNet.
Since it was applied on synthetic data only, it showed
poor performance on real data. We applied some straight-
forward design adaptations - e.g. larger backbone, multi-
scale features (see supplementary) - and use this modified
3D-SpLineNet as our baseline (first row Table 3).
10644
Method F1-Score(%) ↑X-error X-error Z-error Z-error F1-Score(%) per Scenario ↑
near(m) ↓far(m) ↓near(m) ↓far(m) ↓U&D C EW N I M&S
3D-LaneNet [6] 44.1 0 .479 0 .572 0 .367 0 .443 40.8 46 .5 47 .5 41 .5 32 .1 41 .7
Gen-LaneNet [9] 32.3 0 .591 0 .684 0 .411 0 .521 25.4 33 .5 28 .1 18 .7 21 .4 31 .0
PersFormer [2] 50.5 0 .485 0 .553 0 .364 0 .431 42.4 55 .6 48 .6 46 .6 40 .0 50 .7
PersFormer* [2] 53.1 0 .361 0 .328 0 .124 0 .129 46.8 58 .7 54 .048.4 41 .4 52 .5
CurveFormer [1] 50.5 0 .340 0 .772 0 .207 0 .651 45.2 56 .6 49 .7 49 .1 42 .9 45 .4
BEV-LaneDet [46] 58.4 0.309 0 .659 0 .244 0 .631 48.763.153.4 53 .450.353.7
Anchor3DLane [12] 53.7 0 .276 0 .311 0.107 0 .138 46.7 57 .2 52 .5 47 .8 45 .4 51 .2
Anchor3DLane-T [12] 54.3 0 .275 0.310 0.105 0.135 47.2 58 .0 52 .7 48 .7 45 .8 51 .7
LaneCPP (Ours) 60.3 0.264 0.310 0.077 0.117 53.664.456.754.952.058.7
Table 4. Quantitative comparison on OpenLane [2]. Best performance and second best are highlighted. The scenario categories are Up
and Down (U&D), Curve (C), Extreme Weather (EW), Night (N), Intersection (I), Merge and Split (M&S). PersFormer* denotes the latest
performance reported on the official code base, Anchor3DLane-T represents the temporal multi-frame method of [12].
(a)
 (b)
 (c)
 (d)
 (e)
Figure 6. Qualitative comparison on OpenLane. Our method is compared to PersFormer* with ground truth visualized as dashed lines.
Implementation details. We use input size 360×480
and adopt the same backbone as in [2] based on a modi-
fied EfficientNet [42]. We extract four feature maps of res-
olutions [1
2,1
4,1
8,1
16]. The final 3D feature map has size
26×16with64channels. We use M= 64 initial line pro-
posals and B-Splines of degree d= 3 andK= 10 control
points. We apply Adam optimizer [14] with an initial learn-
ing rate of 2×10−4for OpenLane and 10−4for Apollo
and a dataset specific step-wise scheduler. We train for 30
epochs on OpenLane and 300epochs on Apollo with batch
size16. For more details we refer to the supplementary.
4.2. Ablation studies
Table 1 indicates the effect of our proposed prior-based reg-
ularization. It is evident that each prior improves the F1-
Score as well as geometric errors. While the surface and
curvature priors result in better far-range estimates, line par-
allelism supports X-regression in the near-range. Besides,
using surface smoothness loss results in lowest Z-far errors.
Finally, a combination of priors yields a good balance ofF1-Score and geometric errors. The positive effect of par-
allelism is confirmed by Fig. 5, where reinforcing paral-
lel lane structure leads to better estimates in the near-range
(a) and far-range (b) compared to the unregularized model.
Learning parallel lines also is evidently beneficial in cases
of poor visibility (b) and occlusions (a). In the latter case,
the regularized model even shows better predictions than
the noisy ground truth. This emphasizes the high relevance
of priors for more robust behavior for real-world datasets,
where 3D ground truth often comes with inaccuracies.
For the spatial transformation (see Table 2), too low
numbers of surface hypotheses result in worse score,
presumably as 3D geometry is not captured sufficiently,
whereas larger numbers tend to decreasing performance due
to the higher complexity. The best F1-Score is obtained
with 5 hypotheses, which is chosen for further experiments.
While the improvement over IPM is already considerable,
we think that with the simplifications of plane hypotheses
prevent the component from developing its full potential.
We see ways to enhance the 3D transformation even further
10645
MethodBalanced Scenes Rare Scenes
F1(%) ↑X-error (m) ↓ Z-error (m) ↓F1(%) ↑X-error (m) ↓ Z-error (m) ↓
near far near far near far near far
3D-LaneNet [6] 86.4 0 .068 0 .477 0 .015 0.202 72.0 0 .166 0 .855 0 .039 0.521
GP [17] 91.9 0 .049 0 .387 0.008 0.213 83.7 0 .126 0 .903 0 .023 0.625
PersFormer [2] 92.9 0 .054 0 .356 0 .01 0 .234 87.5 0 .107 0 .782 0 .024 0 .602
3D-SpLineNet [32] 96.3 0 .037 0 .324 0 .009 0.213 92.9 0 .077 0 .699 0.021 0.562
CurveFormer [1] 95.8 0 .078 0 .326 0 .018 0 .219 95.6 0 .182 0 .737 0 .039 0 .561
BEV-LaneDet [46] 96.90.016 0 .242 0.02 0 .216 97.6 0 .031 0 .594 0.040 0 .556
Anchor3DLane [12] 95.4 0 .045 0 .300 0 .016 0 .223 94.4 0 .082 0 .699 0 .030 0 .580
LaneCPP (Ours) 97.4 0.030 0.277 0.011 0.206 96.2 0.073 0.651 0.023 0.543
Table 5. Quantitative comparison of best methods on Apollo 3D Synthetic [9]. Best performance and second best are highlighted.
using more sophisticated spatial representations in future.
The impact of our different contributions is summarized
in Table 3, where the first row shows our baseline (see
Sec. 4.1). More than two percent in F1-Score are gained
with our novel lane representation compared to the simpli-
fied one from [32]. Moreover, it is clear that both, the regu-
larization using combined priors and the spatial transforma-
tion using 5 hypotheses result in significant improvement.
Eventually, combining all components yields the best model
configuration, which we choose for further evaluation.
4.3. Evaluation on OpenLane
On the real-world OpenLane benchmark our model evi-
dently outperforms all other methods with respect to F1-
Score as well as geometric errors as shown in Table 4.
Compared to BEV-Lanedet, which achieves a high detec-
tion score, our model gains +1.9 %, while reaching sig-
nificantly lower geometric errors. In comparison to An-
chor3DLane the improvements with respect to X-errors are
less substantial, however, our approach surpasses the F1-
Score by a large gap of +6.6 %. Analyzing the detection
scores among different scenarios, outstanding performance
gain is observed on the up- and down-hill test set ( +5.9 %)
that highlights the capability of our approach to capture 3D
space proficiently, which is supported by the low Z-errors.
Apart from quantitative results, we show qualitative ex-
amples in Fig. 6. In up-hill scenarios like Fig. 6b our model
manages to estimate both lateral and height profile accu-
rately, since our assumptions about road surface and line
parallelism are satisfied. In contrast, PersFormer lacks spa-
tial features and does not use any kind of physical regular-
ization. Consequently, it fails to estimate the 3D lane ge-
ometry and even collapses in Fig. 6c, whereas our surface
and curvature priors always prevent such a behavior. Note-
worthy is also the top performance on the merges and splits
set. This proves that our soft regularization is even capable
to handle situations containing non-parallel lines, which is
also confirmed by Fig. 6d. However, we rarely observe limi-tations with our formulation for line pairs with a similar ori-
entation but weakly converging course as shown in Fig. 6e.
In such cases the indicator function might erroneously de-
cide for parallelism loss during training. One possible solu-
tion for future work would be to consider ground truth for
the indicator function to identify such situations.
4.4. Evaluation on Apollo 3D Synthetic
The Apollo 3D Synthetic dataset is very limited in size and
only consists of simple situations in contrast to OpenLane.
While we find the results on OpenLane more meaningful,
we would like to still provide and discuss the quantitative
results on the Apollo dataset. Due to the simplicity of the
dataset, our model cannot benefit that significantly from our
priors but still achieves competitive results to state of the art
with the highest F1-Score on the balanced scenes dataset
and comparable error metrics (second best for most errors).
5. Conclusions and future work
In this work, we present LaneCPP, a novel approach for
3D lane detection that leverages physical prior knowledge
about lane structure and road geometry. Our new continu-
ous lane representation overcomes previous deficiencies by
allowing arbitrary lane structures and enables us to regular-
ize lane geometry based on analytically formulated priors.
We further introduce a novel spatial transformation mod-
ule that models 3D features carefully considering knowl-
edge about road surface geometry. In our experiments, we
demonstrate state-of-the-art performance on real and syn-
thetic benchmarks. The full capability of our approach is re-
vealed on real-world OpenLane, for which we prove the rel-
evance of priors quantitatively and qualitatively. In future,
priors could be individualized for different driving scenar-
ios and might support to learn inter-lane relations to achieve
better scene understanding in a global context. We also see
ways to leverage the full potential of the spatial transforma-
tion by using more sophisticated surface representations.
10646
References
[1] Yifeng Bai, Zhirong Chen, Zhangjie Fu, Lang Peng, Peng-
peng Liang, and Erkang Cheng. Curveformer: 3d lane detec-
tion by curve propagation with curve queries and attention.
InProc. IEEE International Conf. on Robotics and Automa-
tion (ICRA) , 2023. 1, 2, 3, 7, 8
[2] Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu,
Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi,
Yu Qiao, et al. Persformer: 3d lane detection via perspective
transformer and the openlane benchmark. In Proc. of the
European Conf. on Computer Vision (ECCV) , 2022. 2, 3, 4,
6, 7, 8
[3] Carl de Boor. On calculating with b-splines. Journal of Ap-
proximation Theory , 1972. 2
[4] Netalee Efrat, Max Bluvstein, Shaul Oron, Dan Levi,
Noa Garnett, and Bat El Shlomo. 3d-lanenet+: An-
chor free lane detection using a semi-local representation.
arXiv/2011.01535 , 2020. 1, 2, 3
[5] Zhengyang Feng, Shaohua Guo, Xin Tan, Ke Xu, Min Wang,
and Lizhuang Ma. Rethinking efficient lane detection via
curve modeling. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2022. 2
[6] Noa Garnett, Rafi Cohen, Tomer Pe’er, Roee Lahav, and Dan
Levi. 3d-lanenet: End-to-end 3d multiple lane detection. In
Proc. of the IEEE International Conf. on Computer Vision
(ICCV) , 2019. 1, 2, 3, 7, 8
[7] Mohsen Ghafoorian, Cedric Nugteren, N ´ora Baka, Olaf
Booij, and Michael Hofmann. EL-GAN: embedding loss
driven generative adversarial networks for lane detection. In
Proc. of the European Conf. on Computer Vision (ECCV) ,
2018. 1, 2
[8] Stephen Gould, Richard Hartley, and Dylan Campbell. Deep
declarative networks. IEEE Trans. on Pattern Analysis and
Machine Intelligence (PAMI) , 2021. 2
[9] Yuliang Guo, Guang Chen, Peitao Zhao, Weide Zhang, Jing-
hao Miao, Jingao Wang, and Tae Eun Choe. Gen-lanenet:
A generalized and scalable approach for 3d lane detection.
InProc. of the European Conf. on Computer Vision (ECCV) ,
2020. 1, 2, 3, 4, 6, 7, 8
[10] Steffen Hagedorn, Marcel Milich, and Alexandru P. Con-
durache. Pioneering se (2)-equivariant trajectory planning
for automated driving. arXiv:2403.11304 , 2024. 2
[11] Yuenan Hou, Zheng Ma, Chunxiao Liu, and Chen Change
Loy. Learning lightweight lane detection cnns by self atten-
tion distillation. In Proc. of the IEEE International Conf. on
Computer Vision (ICCV) , 2019. 1, 2
[12] Shaofei Huang, Zhenwei Shen, Zehao Huang, Zi han Ding,
Jiao Dai, Jizhong Han, Naiyan Wang, and Si Liu. An-
chor3dlane: Learning to regress 3d anchors for monocular
3d lane detection. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2023. 2, 3, 7, 8
[13] Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will
Song, Joel Pazhayampallil, Mykhaylo Andriluka, Pranav Ra-
jpurkar, Toki Migimatsu, Royce Cheng-Yue, Fernando A.
Mujica, Adam Coates, and Andrew Y . Ng. An em-
pirical evaluation of deep learning on highway driving.
arXiv/1504.01716 , 2015. 1, 2[14] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In Proc. of the International Conf.
on Learning Representations (ICLR) , 2015. 7
[15] YeongMin Ko, Jiwon Jun, Donghwuy Ko, and Moongu Jeon.
Key points estimation and point instance segmentation ap-
proach for lane detection. arXiv/2002.06604 , 2020. 1, 2
[16] Seokju Lee, Junsik Kim, Jae Shin Yoon, Seunghak Shin,
Oleksandr Bailo, Namil Kim, Tae-Hee Lee, Hyun Seok
Hong, Seung-Hoon Han, and In So Kweon. Vpgnet: Van-
ishing point guided network for lane and road marking de-
tection and recognition. In Proc. of the IEEE International
Conf. on Computer Vision (ICCV) , 2017. 1, 2
[17] Chenguang Li, Jia Shi, Ya Wang, and Guangliang Cheng.
Reconstruct from top view: A 3d lane detection approach
based on geometry structure prior. In Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR) , 2022. 2,
3, 8
[18] Qi Li, Yue Wang, Yilun Wang, and Hang Zhao. Hdmapnet:
An online HD map construction and evaluation framework.
InProc. IEEE International Conf. on Robotics and Automa-
tion (ICRA) , 2022. 3
[19] Xiang Li, Jun Li, Xiaolin Hu, and Jian Yang. Line-cnn: End-
to-end traffic line detection with line proposal unit. IEEE
Trans. on Intelligent Transportation Systems (T-ITS) , 2020.
1, 2
[20] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:
Learning bird’s-eye-view representation from multi-camera
images via spatiotemporal transformers. In Proc. of the Eu-
ropean Conf. on Computer Vision (ECCV) , 2022. 3
[21] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,
and Piotr Doll ´ar. Focal loss for dense object detection. In
Proc. of the IEEE International Conf. on Computer Vision
(ICCV) , 2017. 6
[22] Ruijin Liu, Zejian Yuan, Tie Liu, and Zhiliang Xiong. End-
to-end lane shape prediction with transformers. In Proc. of
the IEEE Winter Conference on Applications of Computer
Vision (WACV) , 2021. 2
[23] Ruijin Liu, Dapeng Chen, Tie Liu, Zhiliang Xiong, and Ze-
jian Yuan. Learning to predict 3d lane shape and camera pose
from a single image via geometry constraints. In Proc. of the
Conf. on Artificial Intelligence (AAAI) , 2022. 1, 2, 3
[24] Pingping Lu, Chen Cui, Shaobing Xu, Huei Peng, and Fan
Wang. SUPER: A novel lane detection system. IEEE Trans.
on Intelligent Vehicles (T-IV) , 2021. 2
[25] Hanspeter Mallot, Heinrich Blthoff, J.J. Little, and S Bohrer.
Inverse perspective mapping simplifies optical flow compu-
tation and obstacle detection. Biological Cybernetics , 1991.
3
[26] Davy Neven, Bert De Brabandere, Stamatios Georgoulis,
Marc Proesmans, and Luc Van Gool. Towards end-to-end
lane detection: an instance segmentation approach. In Proc.
IEEE Intelligent Vehicles Symposium (IV) , 2018. 1, 2
[27] Marcos Nieto, Luis Salgado, Fernando Jaureguizar, and Jon
Arr´ospide. Robust multiple lane road modeling based on
perspective analysis. In Proc. IEEE International Conf. on
Image Processing (ICIP) , 2008. 2
10647
[28] Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Ando-
nian, and Bolei Zhou. Cross-view semantic segmentation
for sensing surroundings. IEEE Robotics Autom. Lett. , 2020.
3
[29] Xingang Pan, Jianping Shi, Ping Luo, Xiaogang Wang, and
Xiaoou Tang. Spatial as deep: Spatial CNN for traffic scene
understanding. In Proc. of the Conf. on Artificial Intelligence
(AAAI) , 2018. 1, 2
[30] Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang,
and Erkang Cheng. Bevsegformer: Bird’s eye view seman-
tic segmentation from arbitrary camera rigs. In Proc. of the
IEEE Winter Conference on Applications of Computer Vision
(WACV) , 2023. 3
[31] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding
images from arbitrary camera rigs by implicitly unprojecting
to 3d. In Proc. of the European Conf. on Computer Vision
(ECCV) , 2020. 3, 5
[32] Maximilian Pittner, Alexandru Condurache, and Joel Janai.
3d-splinenet: 3d traffic line detection using parametric spline
representations. In Proc. of the IEEE Winter Conference on
Applications of Computer Vision (WACV) , 2023. 1, 2, 3, 4,
6, 8
[33] Fabio Pizzati, Marco Allodi, Alejandro Barrera, and Fer-
nando Garc ´ıa. Lane detection and classification using cas-
caded cnns. In Proc. of the International Conf. on Computer
Aided Systems Theory (EUROCAST) , 2019. 1, 2
[34] Zhan Qu, Huan Jin, Yang Zhou, Zhen Yang, and Wei Zhang.
Focus on local: Detecting lane marker from bottom up via
key point. In Proc. IEEE Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2021. 1, 2
[35] Matthias Rath and Alexandru Paul Condurache. Invariant in-
tegration in deep convolutional feature space. In Proc. of Eu-
ropean Symposium on Artificial Neural Networks, Computa-
tional Intelligence and Machine Learning (ESANN) , 2020.
2
[36] Matthias Rath and Alexandru Paul Condurache. Improving
the sample-complexity of deep classification networks with
invariant integration. In Proc. of International Joint Conf. on
Computer Vision, Imaging and Computer Graphics Theory
and Applications (VISIGRAPP) , 2022. 2
[37] Thomas Roddick and Roberto Cipolla. Predicting semantic
map representations from images using pyramid occupancy
networks. In Proc. IEEE Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2020. 3
[38] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Ortho-
graphic feature transform for monocular 3d object detection.
InProc. of the British Machine Vision Conf. (BMVC) , 2019.
3
[39] Avishkar Saha, Oscar Mendez, Chris Russell, and Richard
Bowden. Translating images into maps. In Proc. IEEE In-
ternational Conf. on Robotics and Automation (ICRA) , 2022.
3
[40] Jinming Su, Chao Chen, Ke Zhang, Junfeng Luo, Xiaom-
ing Wei, and Xiaolin Wei. Structure guided lane detection.
InProc. of the International Joint Conf. on Artificial Intelli-
gence (IJCAI) , 2021. 2
[41] Lucas Tabelini, Rodrigo Berriel, Thiago M Paixao, Claudine
Badue, Alberto F De Souza, and Thiago Oliveira-Santos.Keep your eyes on the lane: Real-time attention-guided lane
detection. In Proc. IEEE Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2021. 1, 2
[42] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In Proc. of the
International Conf. on Machine learning (ICML) , 2019. 7
[43] Lucas Tabelini Torres, Rodrigo Ferreira Berriel, Thiago M.
Paix˜ao, Claudine Badue, Alberto F. De Souza, and Thi-
ago Oliveira-Santos. Polylanenet: Lane estimation via deep
polynomial regression. In Proc. of the International Conf. on
Pattern Recognition (ICPR) , 2020. 2
[44] Bingke Wang, Zilei Wang, and Yixin Zhang. Polynomial
regression network for variable-number lane detection. In
Proc. of the European Conf. on Computer Vision (ECCV) ,
2020. 2
[45] Jinsheng Wang, Yinchao Ma, Shaofei Huang, Tianrui Hui,
Fei Wang, Chen Qian, and Tianzhu Zhang. A keypoint-based
global association network for lane detection. In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR) ,
2022. 1, 2
[46] Ruihao Wang, Jian Qin, Kaiying Li, Yaochen Li, Dong Cao,
and Jintao Xu. Bev-lanedet: An efficient 3d lane detection
based on virtual camera via key-points. In Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR) , 2023.
2, 3, 7, 8
[47] Yuping Wang and Jier Chen. Eqdrive: Efficient equivari-
ant motion forecasting with multi-modality for autonomous
driving. arXiv:2310.17540 , 2023. 2
[48] Lu Xiong, Zhenwen Deng, Peizhi Zhang, and Zhiqiang Fu.
A 3d estimation of structural road surface based on lane-line
information. IFAC Conf. on Engine and Powertrain Control,
Simulation and Modeling (E-COSM) , 2018. 2
[49] Chenxin Xu, Robby T. Tan, Yuhong Tan, Siheng Chen,
Yu Guang Wang, Xinchao Wang, and Yanfeng Wang. Eqmo-
tion: Equivariant multi-agent motion prediction with invari-
ant interaction reasoning. In Proc. IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR) , 2023. 2
[50] Fan Yan, Ming Nie, Xinyue Cai, Jianhua Han, Hang Xu,
Zhen Yang, Chaoqiang Ye, Yanwei Fu, Michael Bi Mi, and
Li Zhang. Once-3dlanes: Building monocular 3d lane detec-
tion. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR) , 2022. 2, 3
[51] Tu Zheng, Hao Fang, Yi Zhang, Wenjian Tang, Zheng Yang,
Haifeng Liu, and Deng Cai. RESA: recurrent feature-shift
aggregator for lane detection. In Proc. of the Conf. on Artifi-
cial Intelligence (AAAI) , 2021. 2
[52] Tu Zheng, Yifei Huang, Yang Liu, Wenjian Tang, Zheng
Yang, Deng Cai, and Xiaofei He. Clrnet: Cross layer re-
finement network for lane detection. In Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR) , 2022. 2
[53] Qin Zou, Hanwen Jiang, Qiyu Dai, Yuanhao Yue, Long
Chen, and Qian Wang. Robust lane detection from continu-
ous driving scenes using deep neural networks. IEEE Trans.
on Vehicular Technology (VTC) , 2020. 1, 2
10648
