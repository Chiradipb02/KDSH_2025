Accelerating Diffusion Sampling with Optimized Time Steps
Shuchen Xue1, Zhaoqiang Liu2*, Fei Chen3, Shifeng Zhang3, Tianyang Hu3, Enze Xie3, Zhenguo Li3
1Academy of Mathematics and Systems Science, Chinese Academy of Sciences
2University of Electronic Science and Technology of China
3Huawei Noah’s Ark Lab
Abstract
Diffusion probabilistic models (DPMs) have shown re-
markable performance in high-resolution image synthesis,
but their sampling efficiency is still to be desired due to the
typically large number of sampling steps. Recent advance-
ments in high-order numerical ODE solvers for DPMs have
enabled the generation of high-quality images with much
fewer sampling steps. While this is a significant devel-
opment, most sampling methods still employ uniform time
steps, which is not optimal when using a small number
of steps. To address this issue, we propose a general
framework for designing an optimization problem that seeks
more appropriate time steps for a specific numerical ODE
solver for DPMs. This optimization problem aims to mini-
mize the distance between the ground-truth solution to the
ODE and an approximate solution corresponding to the
numerical solver. It can be efficiently solved using the
constrained trust region method, taking less than 15sec-
onds. Our extensive experiments on both unconditional and
conditional sampling using pixel- and latent-space DPMs
demonstrate that, when combined with the state-of-the-art
sampling method UniPC, our optimized time steps signifi-
cantly improve image generation performance in terms of
FID scores for datasets such as CIFAR-10 and ImageNet,
compared to using uniform time steps.1
1. Introduction
Diffusion Probabilistic Models (DPMs), as referenced in
various studies [ 15,43,46], have garnered significant at-
tention for their success in a wide range of generative tasks.
These tasks include, but are not limited to, image synthesis
[11,16,40], video generation [ 3,17], text-to-image con-
version [ 36,39,41], and speech synthesis [ 24,45]. Cen-
tral to the operation of DPMs is a dual-process mecha-
*Corresponding author. The work was done when the author worked at
Huawei Noah’s Ark Lab.
1Code is available at https://github.com/scxue/DM-
NonUniformnism: a forward diffusion process incrementally introduces
noise into the data, while a reverse diffusion process is em-
ployed to reconstruct data from this noise. Despite their su-
perior performance in generation tasks when compared to
alternative methodologies like Generative Adversarial Net-
works (GANs) [ 13] or Variational Autoencoders (V AEs)
[21], DPMs require a considerable number of network func-
tion evaluations (NFEs) [ 15], which is a notable computa-
tional limitation for their broader application.
The scholarly pursuit to enhance the sampling efficiency
of DPMs bifurcates into two distinct categories: methods
that require additional training of the DPMs [ 33–35,42,47,
49,50,52] and those that do not [ 2,18,19,31,44,53,55].
This paper focuses on the latter training-free methods, aim-
ing to improve the sampling efficiency of DPMs. Con-
temporary training-free samplers utilize efficient numerical
schemes to solve the diffusion Stochastic Differential Equa-
tions (SDEs) or Ordinary Differential Equations (ODEs).
While introducing stochasticity in diffusion sampling has
been shown to achieve better quality and diversity [ 53],
ODE-based sampling methods are superior when the sam-
pling steps are fewer. For example, Song et al. [ 44] pro-
vide an empirically efficient solver DDIM. Zhang and Chen
[54] and Lu et al. [ 31] point out the semi-linear structure
of diffusion ODEs, and develop higher-order ODE sam-
plers based on it. Zhao et al. [ 55] further improve these
samplers in terms of NFEs by integrating the mechanism
of the predictor-corrector method. Due to its better effi-
ciency, sampling from diffusion ODE has been predomi-
nantly adopted. Quality samples can be generated within
as few as 10 steps. However, state-of-the-art ODE-based
samplers are still far from optimal. How to further improve
efficiency is an active field of research.
In this paper, we focus on another important but less
explored aspect of diffusion sampling, the discretization
scheme for time steps (sampling schedule) . Our main con-
tributions are summarized as follows:
•Motivated by classic assumptions on score approximation
adopted in relevant theoretical works on DPMs, we pro-
pose a general framework for constructing an optimiza-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8292
tion problem with respect to time steps, in order to min-
imize the distance between the ground-truth solution and
an approximate solution to the corresponding ODE. The
parameters for the optimization problem can be easily ob-
tained for any explicit numerical ODE solver. In partic-
ular, we allow the numerical ODE solver to have vary-
ing orders in different steps. The optimization problem
can be efficiently solved using the constrained trust re-
gion method, requiring less than 15seconds.
•We construct the optimization problem with respect to
time steps for DPM-Solver++ [ 32] and UniPC [ 55],
which are recently proposed high-order numerical solvers
for DPMs. Extensive experimental results show that our
method consistently improves the performance for both
unconditional sampling and conditional sampling in both
pixel and latent space diffusion models. For example,
when combined with UniPC and only using 5 neural func-
tion evaluations (NFEs), the time steps found by our op-
timization problem lead to an FID of 11.91 for uncon-
ditional pixel-space CIFAR-10 [ 23], an FID of 10.47 on
conditional pixel-space ImageNet [ 10] 64x64, and an FID
of 8.66 on conditional latent-space ImageNet 256x256.
2. Related Work
Training-based Methods Training-based methods,
e.g., knowledge distillation [ 33–35,42], learning-to-
sample [ 50], integration with GANs [ 49,52], and learning
the consistency of diffusion ODE [ 47], have the potential
to sampling for one or very few steps to enhance the
efficiency. However, they lack a plug-and-play nature
and require substantial extra training, which greatly limits
their applicability across diverse tasks. We mainly focus
on the training-free methods in this paper. Nevertheless,
our investigation for the optimal sampling strategy can be
orthogonally combined with training-based methods to
further boost the performance.
Adaptive Step Size Adaptive step size during diffusion
sampling has also been explored. It’s widely used in nu-
merical solutions of ordinary differential equations the er-
rors of the method. Jolicoeur-Martineau et al. [ 18] designed
an adaptive step size via a lower-order and a higher-order
method. Lu et al. [ 31] also proposes an adaptive step sched-
ule using a similar method. Gao et al. [ 12] propose fast sam-
pling through the restricting backward error schedule based
on dynamically moderating the long-time backward error.
However, the empirical performances of these methods in a
few steps can still be improved.
Learning to Schedule Most related to our work is a line
of research that also aims to find the optimal time sched-
ule. Watson et al. [ 50] use dynamic programming to dis-cover the optimal time schedule with maximum Evidence
Lower Bound (ELBO). Wang et al. [ 48] leverage reinforce-
ment learning method to search a sampling schedule. Liu
et al. [ 30] design a predictor-based search algorithm to opti-
mize both the sampling schedule and decide which model to
sample from at each step given a set of pre-trained diffusion
models. Xia et al. [ 51] propose to train a timestep aligner
to align the sampling schedule. Li et al. [ 27] propose to
utilize the evolution algorithm to search over the space of
sampling schedules in terms of FID score. In comparison,
our method has negligible computational cost compared to
the learning-based method.
3. Preliminary
In this section, we provide introductory discussions about
diffusion models and the commonly-used discretization
schemes for time steps.
3.1. Diffusion Models
In the regime of the continuous SDE, DPMs [ 15,22,43,46]
construct noisy data through the following linear SDE:
dxt=f(t)xtdt+g(t)dwt, (1)
where wt∈RDrepresents the standard Wiener process,
andf(t)xtandg(t)denote the drift and diffusion coef-
ficients respectively. In addition, for any t∈[0, T], the
distribution of xtconditioned on x0is a Gaussian distribu-
tion with mean vector α(t)x0and covariance matrix σ2(t)I,
i.e.,xt|x0∼ N (α(t)x0, σ2(t)I), where positive functions
α(t)andσ(t)are differentiable with bounded derivatives,
and are denoted as αtandσtfor brevity. Let qtdenote the
marginal distribution of xt, and q0tbe the distribution of
xtconditioned on x0. The functions αtandσtare chosen
such that qTclosely approximate a zero-mean Gaussian dis-
tribution with covariance matrix ˜σ2Ifor some ˜σ > 0, and
the signal-to-noise-ratio (SNR) α2
t/σ2
tis strictly decreasing
with respect to t. Moreover, to ensure that the SDE in ( 1)
hase the same transition distribution q0tforxtconditioned
onx0,f(t)andg(t)need to be dependent on αtandσtin
the following form:
f(t) =d logαt
dt, g2(t) =dσ2
t
dt−2d logαt
dtσ2
t.(2)
Anderson [ 1] establishes a pivotal theorem that the forward
process ( 1) has an equivalent reverse-time diffusion pro-
cess (from Tto0) as in the following equation, so that
the generating process is equivalent to solving the diffusion
SDE [ 15,46]:
dxt=
f(t)xt−g2(t)∇xlogqt(xt)
dt+g(t)d¯wt,(3)
where ¯wtrepresents the Wiener process in reverse time, and
∇xlogqt(x)is the score function.
8293
Moreover, Song et al. [ 46] also show that there exists
a corresponding deterministic process whose trajectories
share the same marginal probability densities qt(x)as those
of (3), thus laying the foundation of efficient sampling via
numerical ODE solvers [ 31,55]:
dxt=
f(t)xt−1
2g2(t)∇xlogqt(xt)
dt. (4)
We usually train a score network sθ(x, t)parameterized by
θto approximate the score function ∇xlogqt(x)in (3) by
optimizing the denoising score matching loss [ 46]:
L=Etn
ω(t)Ex0,xtsθ(xt, t)− ∇ xlogq0t(xt|x0)2
2o
,
(5)
where ω(t)is a weighting function. In practice, two meth-
ods are commonly used to reparameterize the score net-
work [ 22]. The first approach utilizes a noise prediction
network ϵθsuch that ϵθ(x, t) =−σtsθ(x, t), and the sec-
ond approach employs a data prediction network xθsuch
thatxθ(x, t) = ( σ2
tsθ(x, t) +x)/αt. In particular, for the
data prediction network xθ, based on score approximation
and ( 2), the reverse ODE in ( 4) can be expressed as2
dxt
dt=d logσt
dtxt+αtd logαt
σt
dtxθ(xt, t), (6)
with the initial vector xTbeing sampled from N(0,˜σ2I).
3.2. Discretization Schemes
As mentioned above, we can numerically solve a reverse
ODE such as ( 6) to perform the task of sampling from dif-
fusion models. Different numerical methods are then em-
ployed to approximately solve the ODE, for which the time
interval [ϵ, T]needs to be divided into Nsub-intervals for
some positive integer Nusing N+ 1time steps T=t0>
t1> . . . > t N=ϵ.3Letλt= log( αt/σt)denote the
one half of the log-SNR. Popular discretization schemes for
the time steps include the i) uniform- tscheme, ii) uniform-
λscheme [ 31], and the discretization strategy used in [ 19],
which we refer to as the EDM scheme.
More specifically, for the uniform- tscheme, we split the
interval [ϵ, T]uniformly and obtain
tn=T+n
N(ϵ−T)forn= 0,1, . . . , N. (7)
Note that since λtis a strictly decreasing function of t, it
has an inverse function tλ(·). For the uniform- λscheme,
2Throughout this work, we focus on the case of using a data prediction
network xθ. For the case of using a noise prediction network ϵθ, we can
utilize the formula xθ(x, t) =x−σtϵθ(x,t)
αtto transform it into the data
prediction network.
3Practically, it is typical to use end time t=ϵ > 0instead of t= 0 to
avoid numerical issues for small t, see [ 31, Appendix D.1]. The parameter
ϵis assumed to be fixed in our work. For the proper selection of ϵ, the
reader may refer to prior works such as [ 20].we split the interval [λϵ, λT]uniformly, and convert the λ
values back to tvalues. That is, we have
tn=tλ
λT+n
N(λϵ−λT)
forn= 0,1, . . . , N. (8)
In addition, Karras et al. [ 19] propose to do a variable sub-
stitution κt=σt
αt, which is the reciprocal of the square root
of SNR. For the EDM scheme, they propose to uniformly
discretize the κ1
ρ
tfor some positive integer ρ, i.e.,
tn=tκ
κ1
ρ
T+n
N(κ1
ρϵ−κ1
ρ
T)ρ
forn= 0,1, . . . , N,
(9)
where tκ(·)denotes the inverse function of κt, which is
guaranteed to exist as κtis strictly increasing with respect
tot. Karras et al. [ 19] performed some ablation studies and
suggested to set the parameter ρ= 7in their experiments.
While the discretization scheme plays an important role
in the numerical solvers, the above-mentioned schemes
have been widely employed for convenience, leaving a sig-
nificant room of improvements.
4. Problem Formulation
Recall that λt= log( αt/σt)is strictly decreasing with re-
spect to t, and it has an inverse function tλ(·). Then, the
termxθ(xt, t)in (6) can be written as xθ(xtλ(λ), tλ(λ)) =
ˆxθ(ˆxλ, λ). Letf(λ) =ˆxθ(ˆxλ, λ). For the fixed starting
time of sampling Tand end time of sampling ϵ, the integral
formula [ 32, Eq. (8)] gives the exact solution to the ODE
in (6) at time ϵ:
xϵ=σϵ
σTxT+σϵZλϵ
λTeλf(λ)dλ.
However, f(λ)is a complicated neural network function
and we cannot take the above integral exactly. Instead, we
need to choose a sequence of time steps T=t0> t 1>
. . . > t N=ϵto split the integral as Nparts and appro-
priately approximate the integral in each part separately. In
particular, for any n∈ {1,2, . . . , N }and a positive inte-
gerkn≤n, akn-th order explicit method for numerical
ODE typically uses a (kn−1)-th degree (local) polyno-
mialPn;kn−1(λ)that involves the function values of f(·)at
the points λn−kn, λn−kn+1, . . . , λ n−1to approximate f(λ)
within the interval [λtn−1, λtn]. That is, the polynomial
Pn;kn−1(λ)can be expressed as
Pn;kn−1(λ) =kn−1X
j=0ℓn;kn,j(λ)f(λn−kn+j), (10)
where ℓn;kn,j(λ)are certain (kn−1)-th degree polynomials
ofλ.
8294
Remark 1. If setting Pn;kn−1(λ)to be the
Lagrange polynomial with interpolation points
λn−kn, λn−kn+1, . . . , λ n−1, we obtain
ℓn;kn,j(λ) :=kn−1Y
i=0,i̸=jλ−λn−kn+i
λn−kn+j−λn−kn+i. (11)
We note that such an approximation has been essentially
considered in the UniPC method proposed in [ 55], if not
using the corrector and a special trick for the case of kn=
2.4In addition, a similar idea was considered in [ 54] with
using local Lagrange polynomials to approximate eλf(λ).
Or alternatively, Pn;kn−1(λ)may be set as the poly-
nomial corresponding to the Taylor expansion of f(λ)at
λn−1, as was done in the works [ 31,32]. That is,
Pn;kn−1(λ) =kn−1X
j=0f(j)(λn−1)(λ−λn−1)j
j!, (12)
where f(j)(λn−1)can be further approximated using the
neural function values at λn−kn, λn−kn+1, . . . , λ n−1[32].
In particular, if letting hn=λn+1−λnfor any n∈[N],
we have for j= 1thatf(j)(λn−1)≈f(λn−1)−f(λn−2)
hn−2and
forj= 2 thatf(j)(λn−1)≈2
hn−2(hn−2+hn−3)f(λn−1)−
2
hn−2hn−3f(λn−2)+2
hn−3(hn−2+hn−3)f(λn−3). Therefore,
we observe that Taylor expansion will also give local poly-
nomials Pn;kn−1(λ)that are of the form in (10), although
they will be slightly different with those for the case of using
Lagrange approximations.
Based on ( 10), we obtain an approximation of xϵas fol-
lows:
xϵ=σϵ
σTxT+σϵNX
n=1Zλtn
λtn−1eλf(λ)dλ (13)
≈σϵ
σTxT+σϵNX
n=1Zλtn
λtn−1eλPn;kn−1(λ)dλ (14)
=σϵ
σTxT+σϵNX
n=1Zλtn
λtn−1eλ kn−1X
j=0ℓn;kn,j(λ)f(λn−kn+j)
dλ
(15)
=σϵ
σTxT+σϵNX
n=1kn−1X
j=0wn;kn,jf(λn−kn+j) (16)
:=˜xϵ, (17)
where
wn;kn,j=Zλtn
λtn−1eλℓn;kn,j(λ)dλ. (18)
4While the iterative formula in [ 55] appears to be different, we have nu-
merically calculated that ( 11) leads to the same coefficients for the neural
function values as those for [ 55].It is worth noting that we would expect that the weights
wn;kn,jsatisfy the following
kn−1X
j=0wn;kn,j=eλn−eλn−1(19)
for all n∈ {1,2, . . . , N }since for the simplest case that
f(λ)is a vector of all ones, its approximation Pn;kn−1(λ)
should also be a vector of all ones, and then ( 19) follows
from ( 14) and ( 16). The condition in ( 19) can also be easily
verified to hold for the case of using Lagrange and Taylor
expansion polynomials discussed in Remark 1.
For the vector ˜xϵdefined in ( 17), since our goal is to find
an estimated vector that is close to the ground-truth solution
x0, a natural question is as follows:
Question :Can we design the time steps t1, t2, . . . , t N−1
(or equivalently, λt1, λt2, . . . , λ tN−1) such that the distance
between the ground-truth solution x0and the approximate
solution ˜xϵis minimized?
Remark 2. The definition of ˜xϵin(17)is fairly general
in the sense that for any explicit numerical ODE solver
with corresponding local polynomials Pn;kn−1(λ)of the
form (10), we can easily calculate all the weights wn;kn,j
via(18). In particular, we allow the (local) orders knto
vary with respect to nto encompass general cases for high-
order numerical solvers. For example, for a third-order
method, we have kn= 1 forn= 1,kn= 2 forn= 2,
andkn= 3forn≥3. The varying kncan also handle the
customized order schedules tested in [ 55].
5. Analysis and Method
To partially answer the question presented in Section 4, we
follow relevant theoretical works [ 4,6–9,25,26,37] to
make the following assumption on the score approximation.
Assumption 1. For any t∈ {t0, t1, . . . , t N}, the error in
the score estimate is bounded in L2(qt):
∥∇xlogqt−sθ(·, t)∥2
L2(qt)
=Eqt
∥∇xlogqt(x)−sθ(x, t)∥2
≤η2ε2
t,
where η >0is an absolute constant.
More specifically, εtcan be set to be 1in the corre-
sponding assumptions in [ 7,8,25], and εtcan be set to
be1
σ2
tin [26, Assumption 1]. Based on Assumption 1and
the transform between score network sθand data prediction
network xθ[22], we obtain the following lemma.
Lemma 1. For any x0∼q0andP0∈(0,1), with prob-
ability at least 1−P0, the following event occurs: For all
t∈ {t0, t1, . . . , t N}andxt∼qt, we have
∥xθ(xt, t)−x0∥ ≤˜η˜εt,
8295
where ˜η:=q
N+1
P0ηand˜εt:=εtσ2
t
αt.
Proof. For any u >0and any t∈ {t0, t1, . . . , t N}, by the
Markov inequality, we obtain
P(∥∇xlogqt(xt)−sθ(xt, t)∥> u)
=P(∥∇xlogqt(xt)−sθ(xt, t)∥2> u2) (20)
≤Eqt
∥∇xlogqt(xt)−sθ(xt, t)∥2
u2(21)
≤η2ε2
t
u2, (22)
where ( 22) follows from Assumption 1. Taking a union
bound over all t∈ {t0, t1, . . . , t N}, we obtain that with
probability at least 1−(N+1)η2ε2
t
u2 , it holds for all t∈
{t0, t1, . . . , t N}that
∥∇xlogqt(xt)−sθ(xt, t)∥ ≤u. (23)
Using the transforms ∇xlogqt(xt) =αtx0−xt
σ2
tand
sθ(xt, t) =αtxθ(xt,t)−xt
σ2
t[22], we derive that with prob-
ability at least 1−(N+1)η2ε2
t
u2 , it holds for all t∈
{t0, t1, . . . , t N}that
∥x0−xθ(xt, t)∥ ≤σ2
t
αtu. (24)
Setting u=q
N+1
P0ηεt, we obtain the desired result.
Note that we have ˜εt=σ2
t
αtfor the case that εt= 1, and
˜εt=1
αtfor the case that εt=1
σ2
t. Additionally, motivated
by the training objective for the data prediction network that
aims to minimizeαt
σt∥x0−xθ(xt, t)∥forx0∼q0andxt∼
qt[42], it is also natural to consider the case that ˜εt=σt
αt.
Therefore, in our experiments, we will set ˜εtasσp
t
αtfor some
non-negative integer p.
Based on Lemma 1, we establish the following theorem,
which provides an upper bound on the distance between the
ground-truth solution x0and the approximate solution ˜xϵ
defined in ( 17).
Theorem 1. Let the approximate solution ˜xϵbe defined as
in(17)withf(λ) =ˆxθ(ˆx(λ), λ)and the weights wn;kn,j
satisfying (19). Suppose that Assumption 1is satisfied for
the score approximation. Then, for any P0∈(0,1), we
have with probability at least 1−P0that
∥˜xϵ−x0∥ ≤σϵ
σTxT+σϵ 
eλϵ−eλT
x0−x0
+σϵ˜ηN−1X
i=0˜εti·X
n−kn+j=iwn;kn,j,(25)
where ˜η:=q
N+1
P0ηand˜εt:=εtσ2
t
αt.Proof. From Lemma 1, we have with probability at least
1−P0, it holds for all n∈ {0,1, . . . , N }thatf(λtn)can
be written as
f(λtn) =ˆxθ(ˆxλtn, λtn) =xθ(xtn, tn) (26)
=x0+ξtn, (27)
where ξtnsatisfies ∥ξtn∥ ≤˜η˜εtn. Then, we obtain
˜xϵ=σϵ
σTxT+σϵNX
n=1kn−1X
j=0wn;kn,jf(λtn−kn+j) (28)
=σϵ
σTxT+σϵNX
n=1kn−1X
j=0wn;kn,j(x0+ξtn−kn+j) (29)
=σϵ
σTxT+σϵNX
n=1kn−1X
j=0wn;kn,jx0
+σNNX
n=1kn−1X
j=0wn;kn,jξtn−kn+j (30)
=σϵ
σTxT+σϵNX
n=1 
eλn−eλn−1
x0
+σNNX
n=1kn−1X
j=0wn;kn,jξtn−kn+j (31)
=σϵ
σTxT+σϵ(eλϵ−eλT)x0+σNN−1X
i=0X
n−kn+j=iwn;kn,jξti,
(32)
where ( 29) follows from ( 27), (31) follows from ( 19),
and ( 32) follows from the condition that j+ 1≤kn≤n.
Therefore, from the triangle inequality, we have
∥˜xϵ−x0∥ ≤σϵ
σTxT+σϵ(eλϵ−eλT)x0−x0
+σϵN−1X
i=0X
n−kn+j=iwn;kn,j∥ξti∥ (33)
≤σϵ
σTxT+σϵ(eλϵ−eλT)x0−x0
+σϵ˜ηN−1X
i=0˜εti·X
n−kn+j=iwn;kn,j, (34)
which gives the desired upper bound.
The starting time of sampling Tand the end time of
sampling ϵare fixed, and we typically have σϵ≈0and
αT≈0. Then, the first term in the upper bound of ( 25)
will be fixed and small. Note that {λtn}N
n=0needs to be
a monotonically increasing sequence. Additionally, since
˜ηis a fixed positive constant, we observe from ( 25) that
8296
Algorithm 1 Finding the time steps via (35)
Input : Number of time steps N, initial time of sampling
T, end time of sampling ϵ, any sampling algorithm that
is characterized by local polynomials Pn;kn−1(λ)of the
form ( 10), the function ˜εt:=σp
t
αtwith a fixed positive
integer pfor score approximation, the initial values of
λt1, . . . , λ tN−1
1: Set λt0=λTandλtN=λϵand calculate ˜εtifor
i= 0,1, . . . , N −1
2: Calculate the weights wn;kn,jfrom ( 18)
3: Solve the optimization problem in ( 35) via the con-
strained trust region method
Output : Optimized λ(or equivalently, time) steps
ˆλt1, . . . , ˆλtN−1
we should choose appropriate λt1, λt2, . . . , λ tN−1(which
lead to the corresponding time steps t1, t2, . . . , t N−1) such
thatPN−1
i=0˜εti·P
n−kn+j=iwn;kn,jis minimized, which
gives the following optimization problem:
min
λt1,...,λ tN−1N−1X
i=0˜εti·X
n−kn+j=iwn;kn,j
s.t.λtn+1> λtn,forn= 0,1, . . . , N −1,
(35)
where λt0=λTandλtN=λϵare fixed. Note that both
˜εtiandwn;kn,jare functions dependent on λt1, . . . , λ tN−1.
More specifically, for any sampling algorithm that uses
the local approximation polynomials Pn;kn−1(λ)of the
form ( 10), the weights wn;kn,jcan be easily calculated
from ( 18). In addition, as mentioned above, we can set ˜εtito
beσp
ti
αtifor some non-negative integer p,5which can be rep-
resented as a functions of λti. Furthermore, the optimiza-
tion problem in ( 35) can be approximately solved highly
efficiently using the constrained trust region method.6Our
method is summarized in Algorithm 1.
6. Experiments
In this section, we demonstrate the effectiveness of our
method combined with the currently most popular ODE
solvers DPM-Solver++ [ 32] and UniPC [ 55]7for the case
5We find in the experiments that setting p= 1 is a good choice for
pixel-space generation, and setting p= 2 is appropriate for latent-space
generation.
6Such an algorithm has been implemented in standard Python
libraries such as scipy. See https : / / docs . scipy . org /
doc/scipy/tutorial/optimize.html#defining-linear-
constraints .
7We observe from the experiments that our optimized time steps work
effectively for UniPC, although the optimization problem does not take the
corrector of UniPC into account, which corresponds to implicit numerical
ODE solvers.of using 5∼15neural function evaluations (NFEs) with
extensive experiments on various datasets. The order is 3
for both DPM-Solver++ and UniPC. We use Fenchel Incep-
tion Distance (FID) [ 14] as the evaluation metric to show
the effectiveness of our method. Unless otherwise speci-
fied, 50K images are sampled for evaluation. The experi-
ments are conducted on a wide range of datasets, with image
sizes ranging from 32x32 to 512x512. We also evaluate the
performance of various previous state-of-the-art pre-trained
diffusion models, including Score-SDE [ 46], ADM [ 11],
EDM [ 19], and DiT [ 38]. which cover pixel-space and
latent-space diffusion models, with unconditional genera-
tion, conditional generation and generation with classifier-
free guidance settings.
6.1. Pixel Diffusion Model Generation
Results on CIFAR-10 32x32 For the CIFAR-10 32x32
experiment, We use the ddpmpp deep continuous Score-
SDE model from [ 46], which is an unconditional VP-
schedule model. The experiment results for UniPC are
shown in Fig. 1. Our proposed optimized sampling schedule
consistently outperforms the other three baseline sampling
schedules and achieves state-of-the-art FID results. For ex-
ample, we improve the FID on CIFAR-10 with only 5NFEs
from 23.22to12.11. We also conducted the experiment for
DPM-Solver++ on CIFAR-10, see Table 1. We found that
although our optimized sampling schedule also consistently
improves DPM-Solver++, UniPC still performs better no
matter which sampling schedule is used. Therefore, in the
remaining part of this section, we only report the results for
UniPC. Due to the page limit, some additional experimental
results have been deferred to the supplementary material.
Results on ImageNet 64x64 For the ImageNet 64x64
experiment, we use the 64x64 diffusion model from
ADM [ 11], which is a conditional VP-schedule model. The
experimental results are shown in Fig. 1. It can be noticed
that among the baseline methods, the uniform- tperforms
the best for NFEs 5∼6, and uniform- λperforms best for
NFEs >7. This indicates that the previous baseline sam-
pling schedules may be far from optimal. Our proposed
optimized sampling schedule consistently outperforms the
other three baseline sampling schedules and achieves state-
of-the-art FID results. For example, we achieved the FID of
10.47on ImageNet 64x64 with only 5NFEs.
Results on FFHQ 64x64 and AFHQv2 64x64 For the
FFHQ 64x64 and AFHQv2 64x64 experiments, we use the
EDM [ 19] unconditional model. The experiment results
are shown in Fig. 1. For the EDM model, the time ranges
from 0.002to80during sampling rather than 0to1, which
makes the result of uniform- tsignificantly worse than the
other two baseline discretization schemes. Thus we do
8297
Methods \NFEs 5 6 7 8 9 10 12 15
DPM-Solver++ with uniform- λ 29.22 13.28 7.18 5.12 4.40 4.03 3.45 3.17
DPM-Solver++ with uniform- t 28.16 19.63 15.29 12.58 11.18 10.15 8.50 7.10
DPM-Solver++ with EDM 40.48 25.10 15.68 10.22 7.42 6.18 4.85 3.49
DPM-Solver++ with optimized step (Ours) 12.91 8.35 5.44 4.74 3.81 3.51 3.24 3.15
UniPC with uniform- λ 23.22 10.33 6.18 4.80 4.19 3.87 3.34 3.17
UniPC with uniform- t 25.11 17.40 13.54 11.33 9.83 8.89 7.38 6.18
UniPC with EDM 38.24 23.79 14.62 8.95 6.60 5.59 4.18 3.16
UniPC with optimized step (Ours) 12.11 7.23 4.96 4.46 3.75 3.50 3.19 3.13
Table 1. Sampling quality measured by FID ( ↓) of different discretization schemes of time steps for DPM-Solver++ [ 32] and UniPC [ 55]
with varying NFEs on CIFAR-10.
5
6
7
8
9
10
12
15
NFE
3
5
10
20
FID
(a) CIFAR10 32x32 (Pixel DPM)
uniform-
uniform-t
EDM
optimized steps (Ours)
5
6
7
8
9
10
12
15
NFE
3
5
10
20
FID
(b) ImageNet 64x64 (Pixel DPM)
uniform-
uniform-t
EDM
optimized steps (Ours)
5
6
7
8
9
10
12
15
NFE
2
3
5
10
20
FID
(c) ImageNet 256x256 (Latent DPM)
uniform-
uniform-t
optimized steps (Ours)
5
6
7
8
9
10
12
15
NFE
3
5
10
20
FID
(d) ImageNet 512x512 (Latent DPM)
uniform-
uniform-t
optimized steps (Ours)
5
6
7
8
9
10
12
15
NFE
3
5
10
20
FID
(e) FFHQ 64x64 (Pixel DPM)
uniform-
EDM
optimized steps (Ours)
5
6
7
8
9
10
12
15
NFE
2
3
5
10
15
FID
(f) AFHQv2 64x64 (Pixel DPM)
uniform-
EDM
optimized steps (Ours)
Figure 1. Sampling quality measured by FID ( ↓) of different discretization schemes of time steps for UniPC [ 55] with varying NFEs on
various DPMs and datasets.
not include the results of the uniform- tscheme. In com-
parison, our proposed optimized sampling schedule consis-
tently outperforms the other two baseline sampling sched-
ules and achieves state-of-the-art FID results. For example,
we achieved an FID of 13.66on FFHQ 64x64 and an FID
of12.11on AFHQv2 64x64 with only 5NFEs.
6.2. Latent Diffusion Model Generation
Results on ImageNet 256x256 and ImageNet 512x512
We evaluate our method on the ImageNet dataset usingthe DiT-XL-2 [ 38] model, which is a Vision-Transformer-
based model in the latent space of KL-8 encoder-decoder.
The corresponding classifier-free guidance scale, namely
s= 1.5, is adopted for evaluation. The experiment re-
sults are shown in Fig. 1. Our proposed optimized sam-
pling schedule consistently outperforms the other two base-
line sampling schedules and achieves state-of-the-art FID
results. For example, we improve the FID from 23.48
to8.66on ImageNet 256x256 and from 20.28to11.40
on ImageNet 512x512 with only 5NFEs. Figure 2visu-
8298
NFE = 5 Optimized Steps (Ours) Uniform- t EDM Uniform- λ
FID = 8.66 FID = 23.48 FID = 45.89 FID = 41.89
Class label = “coral reef” (973)
Class label = “golden retriever” (207)
Class label = “lion” (291)
Class label = “lake shore” (975)
Figure 2. Generated images by UniPC [ 55] with only 5 NFEs for various discretization schemes of time steps from DiT-XL-2 ImageNet
256x256 model [ 38] (with cfg scale s= 1.5and the same random seed).
ally demonstrates the effectiveness of our proposed method.
Since these experiments are a bit time-consuming, and the
EDM discretization scheme for time steps has been shown
to not work well for small NFEs, we mainly use uniform- t
and uniform- λschemes as baselines.
6.3. Running Time Analysis
We test the running time of our Algorithm 1onIntel(R)
Xeon(R) Gold 6278C CPU @ 2.60GHz . We report the
longest running time observed for performing Algorithm 1
for5,6,7,8,9,10,12,15NFEs. The experiment results are
shown in Table 2. Our algorithm can be pre-computed
before sampling and the optimization result can be reuti-
lized. The result shows that our optimization problem can
be solved within 15seconds for NFEs less than or equal
to 15, which is negligible. In comparison, the learning to
schedule methods [ 27,30,48,50,51] usually takes several
GPU hours for optimization and the overall performance is
comparable to ours.NFEs 5 6 7 8 9 10 12 15
Time(s) 1.9 2.3 5.3 5.9 7.8 8.8 11.0 14.1
Table 2. Running time of our optimization algorithm.
7. Conclusion
In this paper, we propose an optimization-based method
to find appropriate time steps to accelerate the sampling
of diffusion models, and thus generating high-quality
images in a small number of sampling steps. We formulate
the problem as a surrogate optimization method with
respect to the time steps, which can be efficiently solved
via the constrained trust region method. Experimental
results on popular image datasets demonstrate that our
method can be employed in a plug-and-play manner and
achieves state-of-the-art sampling performance based on
various pre-trained diffusion models. Our work solves an
approximated optimization object which can be further
improved if a more accurate formulation can be found.
8299
References
[1]Brian D.O. Anderson. Reverse-time diffusion equation mod-
els.Stochastic Processes and their Applications , 12(3):313–
326, 1982. 2
[2]Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-
DPM: An analytic estimate of the optimal reverse variance in
diffusion probabilistic models. In International Conference
on Learning Representations , 2022. 1
[3]Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , 2023. 1
[4]Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved
analysis of score-based generative modeling: User-friendly
bounds under minimal smoothness assumptions. In Interna-
tional Conference on Machine Learning , pages 4735–4763.
PMLR, 2023. 4
[5]Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze
Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,
Huchuan Lu, and Zhenguo Li. Pixart- α: Fast training of dif-
fusion transformer for photorealistic text-to-image synthesis,
2023. 1,2,4,5,6,7
[6]Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi
Wang. Score approximation, estimation and distribution re-
covery of diffusion models on low-dimensional data. arXiv
preprint arXiv:2302.07194 , 2023. 4
[7]Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng
Lu, and Adil Salim. The probability flow ode is provably
fast. arXiv preprint arXiv:2305.11798 , 2023. 4
[8]Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim,
and Anru Zhang. Sampling is as easy as learning the score:
theory for diffusion models with minimal data assumptions.
InInternational Conference on Learning Representations ,
2023. 4
[9]Valentin De Bortoli. Convergence of denoising diffusion
models under the manifold hypothesis. Transactions on Ma-
chine Learning Research , 2022. 4
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. ImageNet: A large-scale hierarchical im-
age database. In 2009 IEEE Conference on Computer Vision
and Pattern Recognition , pages 248–255. IEEE, 2009. 2
[11] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion
models beat GANs on image synthesis. In Advances in
Neural Information Processing Systems , pages 8780–8794,
2021. 1,6
[12] Yansong Gao, Zhihong Pan, Xin Zhou, Le Kang, and Pratik
Chaudhari. Fast diffusion probabilistic model sampling
through the lens of backward error analysis. arXiv preprint
arXiv:2304.11446 , 2023. 2
[13] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,
and Yoshua Bengio. Generative adversarial nets. In Ad-
vances in Neural Information Processing Systems , pages
2672–2680, 2014. 1[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. GANs trained by
a two time-scale update rule converge to a local Nash equi-
librium. In Advances in Neural Information Processing Sys-
tems, pages 6626–6637, 2017. 6
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In Advances in Neural Informa-
tion Processing Systems , pages 6840–6851, 2020. 1,2
[16] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffu-
sion models for high fidelity image generation. Journal of
Machine Learning Research , 23(47):1–33, 2022. 1
[17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J Fleet. Video diffu-
sion models. In Advances in Neural Information Processing
Systems , 2022. 1
[18] Alexia Jolicoeur-Martineau, Ke Li, R ´emi Pich ´e-Taillefer,
Tal Kachman, and Ioannis Mitliagkas. Gotta go fast when
generating data with score-based models. arXiv preprint
arXiv:2105.14080 , 2021. 1,2
[19] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. In Proc. NeurIPS , 2022. 1,3,6
[20] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo
Kang, and Il-Chul Moon. Soft truncation: A universal train-
ing technique of score-based diffusion model for high preci-
sion score estimation. In ICML , pages 11201–11228. PMLR,
2022. 3
[21] Diederik P. Kingma and Max Welling. Auto-encoding varia-
tional bayes. In International Conference on Learning Rep-
resentations , 2014. 1
[22] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan
Ho. Variational diffusion models. In Advances in Neural
Information Processing Systems , 2021. 2,3,4,5
[23] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The
CIFAR-10 Dataset. online: http://www. cs. toronto.
edu/kriz/cifar. html , 55, 2014. 2
[24] Max WY Lam, Jun Wang, Dan Su, and Dong Yu. Bddm:
Bilateral denoising diffusion models for fast and high-quality
speech synthesis. In International Conference on Learning
Representations , 2022. 1
[25] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for
score-based generative modeling with polynomial complex-
ity.Advances in Neural Information Processing Systems , 35:
22870–22882, 2022. 4
[26] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of
score-based generative modeling for general data distribu-
tions. In International Conference on Algorithmic Learning
Theory , pages 946–985. PMLR, 2023. 4
[27] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao,
Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji.
Autodiffusion: Training-free optimization of time steps and
architectures for automated diffusion model acceleration. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7105–7114, 2023. 2,8
[28] Shigui Li, Wei Chen, and Delu Zeng. Scire-solver: Accel-
erating diffusion models sampling by score-integrand solver
with recursive difference. 2023. 2
8300
[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 1
[30] Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, and Yu
Wang. Oms-dpm: Optimizing the model schedule for diffu-
sion probabilistic models. arXiv preprint arXiv:2306.08860 ,
2023. 2,8
[31] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
LI, and Jun Zhu. Dpm-solver: A fast ode solver for dif-
fusion probabilistic model sampling in around 10 steps. In
Advances in Neural Information Processing Systems , pages
5775–5787, 2022. 1,2,3,4
[32] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sam-
pling of diffusion probabilistic models, 2023. 2,3,4,6,7
[33] Eric Luhman and Troy Luhman. Knowledge distillation in
iterative generative models for improved sampling speed.
arXiv preprint arXiv:2101.02388 , 2021. 1,2
[34] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun,
Zhenguo Li, and Zhihua Zhang. Diff-instruct: A universal
approach for transferring knowledge from pre-trained diffu-
sion models. Advances in Neural Information Processing
Systems , 36, 2024.
[35] Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Er-
mon, Jonathan Ho, and Tim Salimans. On distillation of
guided diffusion models. In NeurIPS 2022 Workshop on
Score-Based Methods , 2022. 1,2
[36] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya
Sutskever, and Mark Chen. GLIDE: towards photorealis-
tic image generation and editing with text-guided diffusion
models. In International Conference on Machine Learning
(ICML), 2022 .1
[37] Francesco Pedrotti, Jan Maas, and Marco Mondelli. Im-
proved convergence of score-based diffusion models via
prediction-correction. arXiv preprint arXiv:2305.14164 ,
2023. 4
[38] William Peebles and Saining Xie. Scalable diffusion models
with transformers. arXiv preprint arXiv:2212.09748 , 2022.
6,7,8,2,4
[39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents, 2022. 1
[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 10684–10695, 2022. 1
[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-
mans, Jonathan Ho, David J Fleet, and Mohammad Norouzi.
Photorealistic text-to-image diffusion models with deep lan-
guage understanding. In Advances in Neural Information
Processing Systems , 2022. 1[42] Tim Salimans and Jonathan Ho. Progressive distillation for
fast sampling of diffusion models. In International Confer-
ence on Learning Representations , 2022. 1,2,5
[43] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning , pages 2256–2265. PMLR, 2015.
1,2
[44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations , 2021. 1
[45] Kaitao Song, Yichong Leng, Xu Tan, Yicheng Zou, Tao Qin,
and Dongsheng Li. Transcormer: Transformer for sentence
scoring with sliding language modeling. In Advances in Neu-
ral Information Processing Systems , 2022. 1
[46] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations , 2021. 1,2,3,6
[47] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. Consistency models. arXiv preprint
arXiv:2303.01469 , 2023. 1,2
[48] Yunke Wang, Xiyu Wang, Anh-Dung Dinh, Bo Du, and
Charles Xu. Learning to schedule in diffusion probabilis-
tic models. In Proceedings of the 29th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining , pages
2478–2488, 2023. 2,8
[49] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu
Chen, and Mingyuan Zhou. Diffusion-GAN: Training GANs
with diffusion. In The Eleventh International Conference on
Learning Representations , 2023. 1,2
[50] Daniel Watson, William Chan, Jonathan Ho, and Moham-
mad Norouzi. Learning fast samplers for diffusion models
by differentiating through sample quality. In International
Conference on Learning Representations , 2022. 1,2,8
[51] Mengfei Xia, Yujun Shen, Changsong Lei, Yu Zhou, Ran
Yi, Deli Zhao, Wenping Wang, and Yong-jin Liu. Towards
more accurate diffusion model acceleration with a timestep
aligner. arXiv preprint arXiv:2310.09469 , 2023. 2,8
[52] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling
the generative learning trilemma with denoising diffusion
GANs. In International Conference on Learning Represen-
tations , 2022. 1,2
[53] Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang,
Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma. Sa-solver:
Stochastic adams solver for fast sampling of diffusion mod-
els, 2023. 1
[54] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffu-
sion models with exponential integrator. In The Eleventh In-
ternational Conference on Learning Representations , 2023.
1,4
[55] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and
Jiwen Lu. Unipc: A unified predictor-corrector frame-
work for fast sampling of diffusion models. arXiv preprint
arXiv:2302.04867 , 2023. 1,2,3,4,6,7,8,5
8301
