Scaling Up to Excellence:
Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild
Fanghua Y u1,∗, Jinjin Gu2,∗, Zheyuan Li1, Jinfan Hu1, Xiangtao Kong4,
Xintao Wang5, Jingwen He2,6, Y u Qiao2, Chao Dong1,2,†
1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences2Shanghai AI Laboratory
4The Hong Kong Polytechnic University5ARC Lab, Tencent PCG5The Chinese University of Hong Kong
Project Page: https://supir.xpixel.group
(a) Real-World Image Restoration Results
(b) Controllable Image Restoration with Textual Prompts
Low Quality Input SUPIR output SUPIR output SUPIR output Low Quality Input Low Quality Input
Low Quality Input No Prompt Text: A bicycle  is 
at the end of …
Low Quality Input Text: woman with 
a denim  hat.Text: woman with 
a suede  hat.
Low Quality Input Text: … shows an 
old man …Text: … shows a 
young man  …
Figure 1. Our SUPIR model demonstrates remarkable restoration effects on real-world low-quality images, as illustrated in (a). Addition-
ally, SUPIR features targeted restoration capability driven by textual prompts. For instance, it can specify the restoration of blurry objectsin the distance (case 1), deﬁne the material texture of objects (case 2), and adjust restoration based on high-level semantics (case 3).
Abstract
We introduce SUPIR (Scaling-UP Image Restoration),
a groundbreaking image restoration method that harnessesgenerative prior and the power of model scaling up. Lever-
aging multi-modal techniques and advanced generative
prior , SUPIR marks a signiﬁcant advance in intelligent and
∗Contribute Equally.†Corresponding Author.realistic image restoration. As a pivotal catalyst within
SUPIR, model scaling dramatically enhances its capabil-ities and demonstrates new potential for image restora-tion. We collect a dataset comprising 20 million high-
resolution, high-quality images for model training, each en-
riched with descriptive text annotations. SUPIR provides
the capability to restore images guided by textual prompts,
broadening its application scope and potential. Moreover ,
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25669
we introduce negative-quality prompts to further improve
perceptual quality. We also develop a restoration-guidedsampling method to suppress the ﬁdelity issue encountered
in generative-based restoration. Experiments demonstrate
SUPIR’s exceptional restoration effects and its novel capac-
ity to manipulate restoration through textual prompts.
1. Introduction
The development of image restoration (IR) has greatly el-evated expectations for both the perceptual effects and the
intelligence of IR results. IR methods based on generativepriors [ 42,49,65,79] leverage powerful pre-trained gener-
ative models to introduce high-quality generation and prior
knowledge into IR, bringing signiﬁcant progress in these as-
pects. Continuously improving the capabilities of the gener-
ative prior is key to achieving better IR results, with modelscaling being a crucial and effective approach. There aremany tasks that have obtained astonishing improvements
from scaling, such as SAM [ 44] and large language mod-
els (LLMs) [ 7,71,72]. This further drives our pursuit
of constructing large-scale, intelligent IR models that can
produce ultra-high-quality images. However, due to engi-
neering constraints such as computing resources, model ar-
chitecture, training data, and the cooperation of generative
models and IR, scaling up IR models is challenging.
In this work, we introduce SUPIR (Scaling-UP IR), the
largest-ever IR method, aimed at exploring greater poten-
tial in restoration visual effects and intelligence. Speciﬁ-cally, SUPIR employs StableDiffusion-XL (SDXL) [ 61]a s
a powerful generative prior, which contains 2.6 billion pa-rameters. To effectively deploy this model in IR, we de-sign and train a large-scale adaptor that incorporates a novelcomponent named the ZeroSFT connector. To maximizethe beneﬁts of model scaling, we collect a dataset of over20 million high-quality, high-resolution images, each ac-
companied by detailed descriptive text. We utilize a 13-
billion-parameter multi-modal language model to provide
image content prompts, greatly improving the accuracy and
intelligence of our method. The proposed SUPIR model
demonstrates exceptional performance in a variety of IR
tasks, achieving the best visual quality, especially in com-
plex and challenging real-world scenarios. Additionally, the
model offers ﬂexible control over the restoration processthrough textual prompts, vastly broadening the possibilityof IR. Fig. 1illustrates the effects by our model.
Our work goes far beyond simply scaling. While pur-
suing an increase in model scale, we face a series of com-
plex challenges. First, existing adaptor designs either toosimple to meet the complex requirements of IR [ 57]o ra r e
too large to train together with SDXL [ 92]. To solve this
problem, we trim the ControlNet and designed a new con-
nector called ZeroSFT to work with the pre-trained SDXL,aiming to efﬁciently implement the IR task while reducingcomputing costs. In order to enhance the model’s ability
to accurately interpret the content of low-quality images,we ﬁne-tune the image encoder to improve its robustness tovariations in image degradation. These measures make scal-
ing the model feasible and effective, and greatly improve itsstability. Second, we collect 20 million high-quality, high-resolution images with descriptive text annotations, provid-
ing a solid foundation for the model’s training. We employ acounter-intuitive approach by integrating poor-quality sam-ples into our training process. This allows us to enhance
visual effects by utilizing prompts to guide the model away
from negative qualities. Finally, powerful generative prior
is a double-edged sword. Uncontrolled generation may re-duce restoration ﬁdelity, making IR no longer faithful to the
input image. To address the issue of low ﬁdelity, we intro-
duce the concept of restoration-guided sampling. By inte-
grating these strategies with efﬁcient engineering practices,we not only facilitate the scaling up of SUPIR but also push
the frontiers of advanced IR.
2. Related Work
Image Restoration. The goal of IR is to convert de-
graded images into high-quality degradation-free images
[22,26,86,88,95,96]. In the early stage, researchers
independently explored different types of image degrada-
tion, such as super-resolution (SR) [ 13,19,20], denoising
[11,87,89], and deblurring [ 14,58,70]. However, these
methods are often based on speciﬁc degradation assump-tions [ 25,50,56] and therefore lack generalization ability
to other degradations [ 29,52,94]. Over time, the need for
blind IR methods that are not based on speciﬁc degrada-
tion assumptions has grown [ 5,10,25,34,35,46–48,91].
In this trend, some methods [ 78,90] synthesize real-world
degradation by more complex degradation models, and arewell-known for handling multiple degradation with a single
model. DiffBIR [ 49] uniﬁes different restoration problems
into a single model. In this paper, we adopt a similar set-ting to DiffBIR and use a single model to achieve effective
processing of various severe degradations.
Generative Prior. Generative priors are adept at captur-
ing the inherent structures of the image, enabling the gener-ation of images that follow natural image distribution. The
emergence of GANs [ 23,39,40,62] has underscored the
signiﬁcance of generative priors in IR. V arious approachesemploy generative priors, including GAN inversion [ 2,4,
27,55,
60], GAN encoders [ 9,100], or using GAN as the
core module for IR [ 77,84]. Beyond GANs, other genera-
tive models can also serve as priors [ 10,36,54,73,97–99].
Our work primarily focuses on generative priors derived
from diffusion models [ 31,59,63,65,68,69], which excel
in controllable generation [ 15,18,32,57,92] and model
scaling [ 61,64,66]. Diffusion models have also been effec-
tively used as generative priors in IR [ 42,49,65,75,79].
25670
Degradation-
Robust EncoderzLQLDM 
Image Decoder
Low-Quality 
ImageMulti-Modal Large
Language Model
The image features a close-up of a white 
and gray cat  sitting on a wooden 
surface…... The cat's fur is uffy, and its 
eyes are wide open , … with its front 
paws resting  on the wooden surface.
LDM 
Image Decoder
Text Prompt
Pre-Trained SDXL
Trimmed ControlNet
T
Pre-Trained SDXL
Trimmed ControlNet
T
EDM Sampler with Restoration Guidance×TRestoration 
ResultTrainable
 Fixed
z0zt
Figure 2. This ﬁgure brieﬂy shows the workﬂow of the proposed SUPIR model.
However, these diffusion-based IR methods’ performance
is constrained by the scale of the used generative models,
posing challenges in further enhancing their effectiveness.
Model Scaling is an important means to further im-
prove the capabilities of deep-learning models. The mosttypical examples include the scaling of language models
[7,71,72], text-to-image generation models [ 12,37,61,
65,66,82], and image segmentation models [ 44]. The
scale and complexity of these models have increased dra-matically, now encompassing billions of parameters. Thisincrease in parameters has also resulted in signiﬁcant per-
formance enhancements, showcasing the immense potential
of model scaling [ 38]. However, scaling up is a systematic
problem, involving model design, data collection, comput-
ing resources, and other limitations. Many other tasks have
not yet been able to enjoy the substantial performance im-provements brought by scaling up. IR is one of them.
3. Method
An overview of the proposed SUPIR method is shown
in Fig. 2. We introduce our method from three as-
pects: Sec. 3.1 introduces our network designs and train-
ing method; Sec. 3.2 introduces the collection of training
data and the introduction of textual modality; and Sec. 3.3
introduces the diffusion sampling method for IR.
3.1. Model Scaling Up
Generative Prior. There are not many choices for the
large-scale generative models. The only ones to considerare Imagen [ 66], IF [ 16], and SDXL [ 61]. Our selection
settled on SDXL for the following reasons. Imagen and IFprioritize text-to-image generation and rely on a hierarchi-cal approach. They ﬁrst generate small-resolution images
and then hierarchically upsample them. SDXL aligns with
our objectives by directly generating high-resolution imageswithout a hierarchical design, effectively using its parame-ters to improve image quality rather than focusing on textinterpretation. Additionally, SDXL employs a Base -Reﬁne
strategy. In the Base model, diverse but lower-quality im-
ages are generated. Subsequently, the Reﬁne model, utiliz-
ing training images of signiﬁcantly higher quality but lesser
diversity than those used by the Base model, enhances theimages’ quality. Given our approach of training with a vast
dataset of high-quality images, the dual-phase design of
SDXL becomes redundant for our objectives. We opt for
theBase model, which has a greater number of parameters,
making it an ideal generative prior.
Degradation-Robust Encoder. In SDXL, the diffusion
generation process is performed in the latent space. The im-
age is ﬁrst mapped to the latent space through a pre-trainedencoder. To effectively utilize the pre-trained SDXL, ourLQ image x
LQ should also be mapped to the same latent
space. However, since the original encoder has not been
trained on LQ images, using it for encoding will affect the
model’s judgment of LQ image content, and then misun-
derstand artifacts as image content [ 49]. To this end, we
ﬁne-tune the encoder to make it robust to the degradationby minimizing: L
E=/bardblD(Edr(xLQ))−D(Edr(xGT))/bardbl2
2,
whereEdris the degradation-robust encoder to be ﬁne-
tuned,Dis the ﬁxed decoder, xGTis the ground truth.
Large-Scale Adaptor Design. Considering the SDXL
model as our chosen prior, we need an adaptor that can
steer it to restore images according to the provided LQ in-
puts. The Adaptor is required to identify the content in
the LQ image and to ﬁnely control the generation at the
pixel level. LoRA [ 32], T2I adaptor [ 57], and ControlNet
[92] are existing diffusion model adaptation methods, but
none of them meet our requirements: LoRA limits genera-
tion but struggles with LQ image control; T2I lacks capac-ity for LQ image content identiﬁcation; and ControlNet’sdirect copy is challenging for the SDXL model scale. To
address this issue, we design a new adaptor with two key
features, as shown in Fig. 3(a). First, we keep the high-level
design of ControlNet but employ network trimming [ 33]t o
directly trim some blocks within the trainable copy, achiev-ing an engineering-feasible implementation. Each block
within the encoder module of SDXL is mainly composedof several Vision Transformer (ViT) [ 21] blocks. We iden-
tiﬁed two key factors contributing to the effectiveness ofControlNet: large network capacity and efﬁcient initializa-tion of the trainable copy. Notably, even partial trimming
of blocks in the trainable copy retains these crucial char-
acteristics in the adaptor. Therefore, we simply trim half
of the ViT blocks from each encoder block, as shown in
25671
TrainableEncoder BlockEncoder Block
Encoder Block
Encoder BlockTrimmed CopyTrimmed Copy
Trimmed CopyTrimmed Copy
Trimmed Copy
Mid. Block
Decoder Block
Decoder BlockDecoder Block
Decoder Block
TTTTTTT
zt t,prompt t,promptzLQ
zt−1Trimmed ControlNet
Fixed
Connector(a) Overall Architecture Design (b) Trimmed Encoder Copy
(c) ZeroSFT Connector
ConvXc XsXf
ZeroConv
ZeroConv
ZeroConv×+
Concat
GroupNorm
+
XfoViT Block
ViT Block
ViT Block
ViT BlockN/2
N
BlocksRemained
N/2
Trimmed
V
VZeroConv Conv+Figure 3. This ﬁgure illus-
trates (a) the overall archi-tecture of the used SDXL
and the proposed adap-
tor, (b) a trimmed trainablecopy of the SDXL encoderwith reduced ViT blocks forefﬁciency, and (c) a novelZeroSFT connector for en-hanced control in IR, where
X
fandXsdenote the input
feature maps from the De-coder and Encoder shortcut,
respectively, X
cis the input
from the adaptor, and Xfo
is the output. The model is
designed to effectively usethe large-scale SDXL as agenerative prior.
Fig. 3(b). Second, we redesign the connector that links the
adaptor to SDXL. While SDXL’s generative capacity deliv-
ers excellent visual effects, it also renders pixel-level con-trol challenging. ControlNet employs zero convolution for
generation guidance, but relying solely on residuals is insuf-ﬁcient for the control required by IR. To amplify the inﬂu-ence of LQ guidance, we introduced a ZeroSFT module, asdepicted in Fig. 3(c). Building based on zero convolution,
ZeroSFT encompasses an additional spatial feature transfer
(SFT) [ 76] operation and group normalization [ 81].
3.2. Scaling Up Training Data
Image Collection. The scaling of the model requires a
corresponding scaling of the training data [ 38]. But there
is no large-scale high-quality image dataset available for IRyet. Although DIV2K [ 3] and LSDIR [ 1] offer high im-
age quality, they are limited in quantity. Larger datasets
like ImageNet (IN) [ 17], LAION-5B [ 67], and SA-1B [ 44]
contain more images, but their image quality does not meetour high standards. To this end, we collect a large-scaledataset of high-resolution images, which includes 20 mil-
lion 1024 ×1024 high-quality, texture-rich images. A com-
parison on the scales of the collected dataset and the ex-isting dataset is shown in Fig. 3. We also included an addi-
tional 70K unaligned high-resolution facial images from theFFHQ-raw dataset [ 40] to improve the model’s face restora-
tion performance. In Fig. 5(a), we show the relative size of
our data compared to other well-known datasets.
Multi-Modality Language Guidance. Diffusion models
are renowned for their ability to generate images basedon textual prompts. We believe that textual prompts canalso aid IR: (1) Understanding image content is crucial for
IR. Existing frameworks often overlook or implicitly han-
dle this understanding [ 24,29]. By incorporating textual
Low-Quality Input w/o Negative samplesNo
Negative 
prompt
Use
Negative 
prompt
w/ Negative samplesOurs
Figure 4. CFG introduces artifacts without negative training sam-
ples, hindering visual quality improvement. Adding negative sam-
ples allows further quality enhancement through CFG.
prompts, we explicitly convey the understanding of LQ im-
ages to the IR model, facilitating targeted restoration of
missing information. (2) In cases of severe degradation,even the best IR models struggle to recover completely lost
information. In such cases, textual prompts can serve as a
control mechanism, enabling targeted completion of miss-ing information based on user preferences. (3) We can also
describe the desired image quality through text, further en-
hancing the perceptual quality of the output. See Fig. 1(b)
for some examples. To this end, we make two main mod-iﬁcations. First, we revise the overall framework to incor-
porate the LLaV A multi-modal LLM [ 51] into our pipeline,
as shown in Fig. 2. LLaV A takes the degradation-robust
processed LQ images x
/prime
LQ=D(Edr(xLQ))as input and
explicitly understands the content within the images, out-putting in the form of textual descriptions. These descrip-tions are then used as prompts to guide the restoration. This
process can be automated during testing, eliminating the
need for manual intervention. Secondly, following the ap-proach of PixART [ 12], we also collect textual annotations
for all the training images, to reinforce the role of textual
control during the training of out model. These two changes
endow SUPIR with the ability to understand image contentand to restore images based on textual prompts.
25672
Negative-Quality Samples and Prompt. Classiﬁer-free
guidance (CFG) [ 30] provides another way of control by
using negative prompts to specify undesired content for themodel. We can use this feature to specify the model NOTto produce low-quality images. Speciﬁcally, at each step
of diffusion, we will make two predictions using positive
promptspos and negative prompts neg, and take the fusion
of these two results as the ﬁnal output z
t−1:
zpos
t−1=H(zt,zLQ,σt,pos),zneg
t−1=H(zt,zLQ,σt,neg),
zt−1=zpos
t−1+λcfg×(zpos
t−1−zneg
t−1),
whereH(·)is our diffusion model with adaptor, σtis the
variance of the noise at time-step t, andλcfgis a hyper-
parameter. In our framework, pos can be the image de-
scription with positive words of quality, and neg is the neg-
ative words of quality, e.g., “oil painting, cartoon, blur ,
dirty, messy, low quality, deformation, low resolution, over-
smooth ”. Accuracy in predicting both positive and neg-
ative directions is crucial for the CFG technique. How-ever, the absence of negative-quality samples and promptsin our training data may lead to a failure of the ﬁne-tunedSUPIR in understanding negative prompts. Therefore, us-ing negative-quality prompts during sampling may intro-duce artifacts, see Fig. 4for an example. To address this
problem, we used SDXL to generate 100K images cor-
responding to the negative-quality prompts. We counter-
intuitively add these low-quality images to the training datato ensure that negative-quality concept can be learned by
the proposed SUPIR model.
3.3. Restoration-Guided Sampling
Powerful generative prior is a double-edged sword, as too
much generation capacity will in turn affect the ﬁdelity ofthe recovered image. This highlights the fundamental dif-
ference between IR tasks and generation tasks. We needmeans to limit the generation to ensure that the image recov-ery is faithful to the LQ image. We modiﬁed the EDM sam-
pling method [ 41] and proposed a restoration-guided sam-
pling method to solve this problem. We hope to selectivelyguide the prediction results z
t−1to be close to the LQ image
zLQin each diffusion step. The speciﬁc algorithm is shown
in Algorithm 1, whereTis the total step number, {σt}T
t=1
are the noise variance for Tsteps,cis the additional text
prompt condition. τr,Schurn ,Snoise ,Smin,Smax are ﬁve
hyper-parameters, but only τris related to the restoration
guidance, the others remain unchanged compared to theoriginal EDM method [ 41]. For better understanding, a sim-
ple diagram is shown in Fig. 5(b). We perform weighted
interpolation between the predicted output ˆz
t−1and the LQ
latentzLQas the restoration-guided output zt−1. Since the
low-frequency information of the image is mainly generated
in the early stage of diffusion prediction [ 65] (wheretand
σtare relatively large, and the weight k=(σt/σT)τris
also large), the prediction result is closer to zLQto enhanceAlgorithm 1 Restoration-Guided Sampling.
Input:H,{σt}T
t=1,zLQ,c
Hyper-parameter: τr,Schurn ,Snoise ,Smin ,Smax
1:samplezT∼N(0,σ2
TI)
2:fort∈{T,...,1}do
3: sample /epsilon1t∼N/parenleftbig
0,S2
noiseI/parenrightbig
4:γt←/braceleftBigg
min/parenleftBig
Schurn
N,√
2−1/parenrightBig
ifσt∈[Smin,Smax]
0 otherwise
5:kt←(σt/σT)τr,ˆzt←zt+/radicalBig
ˆσ2
t−σ2
t/epsilon1t,ˆσt←σt+γtσt
6:ˆzt−1←H/parenleftbig
ˆzt,zLQ,ˆσt,c/parenrightbig
7:dt←(ˆzt−(ˆzt−1+kt(zLQ−ˆzt−1)))/ˆσt
8:zt−1←ˆzt+(σt−1−ˆσt)dt
9:end for
1010101110121013
Pixel Numbers ORJVFDOHGOursSA-1BIN21kIN1KLSDIRDIV2KDataset
(a)zLQˆzt−1
zt−1(σt/σT)τr(zLQ−ˆzt−1)
(b)
Figure 5. (a) We show the relative size of our data compared
to other well-known datasets. Compared with SA-1B [ 44], our
dataset has higher quality and more image diversity. (b) Wedemonstrate our restoration-guided sampling mechanism.
ﬁdelity. In the later stages of diffusion prediction, mainly
high-frequency details are generated. There should not be
too many constraints at this time to ensure that detail andtexture can be adequately generated. At this time, tandσ
t
are relatively small, and weight kis also small. Therefore,
the predicted results will not be greatly affected Throughthis method, we can control the generation during the diffu-
sion sampling process to ensure ﬁdelity.
4. Experiments
4.1. Model Training and Sampling Settings
For training, the overall training data includes 20 millionhigh-quality images with text descriptions, 70K face im-
ages and 100K negative-quality samples, together their cor-
responding prompts. To enable a larger batch size, we crop
images into 512 ×512 patches during training. We train
our model using a synthetic degradation model, followingthe setting used by Real-ESRGAN [ 78], the only difference
is that we resize the produced LQ images to 512 ×512 for
training. We use the AdamW optimizer [ 53] with a learning
rate of0.00001 . The training process spans 10 days and is
conducted on 64 Nvidia A6000 GPUs, with a batch size of256. For testing, the hyper-parameters are T=100,λ
cfg=7.5,
andτr=4 . Our method is able to process images with the
size of 1024 ×1024. We resize the short side of the input im-
age to 1024 and crop a 1024 ×1024 sub-image for testing,
and then resize it back to the original size after restoration.
Unless stated otherwise, prompts will not be provided man-ually – the processing will be entirely automatic.
25673
SUPIR (ours)
Low-Quality Input Real-ESRGAN+[ 78] StableSR[ 75]
DiffBIR[ 49] PASD[ 85] SUPIR (ours)
SUPIR (ours).
Low-Quality Input Real-ESRGAN+[ 78] StableSR[ 75]
DiffBIR[ 49] PASD[ 85] SUPIR (ours)
Figure 6. Qualitative comparison with different methods. Our method can accurately restore the texture and details of the corresponding
object under challenging degradation. Other methods fail to recover semantically correct details such as broken beaks and irregular faces.
Low-Quality Input BSRGAN SUPIR (Ours)
SUPIR (Ours)PSNR:
21.28
SSIM:
0.6938
LPIPS:
0.2609PSNR:
19.17
SSIM:0.5578LPIPS:0.3297
PSNR:
21.05
SSIM:0.4339
LPIPS:
0.3492PSNR:
19.28
SSIM:
0.3991LPIPS:
0.3951
StableSRPrefered by metrics Prefered by human
Low-Quality Input
Figure 7. These examples show the misalignment between metric
evaluation and human evaluation. SUPIR generates images with
high-ﬁdelity textures, but obtains lower metrics.
4.2. Comparison with Existing Methods
Our method can handle a wide range of degradations, and
we compare it with the latest methods with the same ca-
pabilities, including BSRGAN [ 90], Real-ESRGAN [ 78],
StableSR [ 75], DiffBIR [ 49] and PASD [ 85]. Some of them
are constrained to generating images of 512 ×512 size. In
our comparison, we crop the test image to meet this require-ment and downsample our results. We conduct comparisons
on both synthetic data and real-world data.Synthetic Data. To synthesize LQ images for testing, we
follow previous works [ 45,94] and demonstrate our effects
on several representative degradations, including both sin-gle degradations and complex mixture degradations. Spe-ciﬁc details can be found in Tab. 1. We selected the fol-
lowing metrics for quantitative comparison: full-reference
metrics PSNR, SSIM, LPIPS [ 93], and the non-reference
metrics ManIQA [ 83], ClipIQA [ 74], MUSIQ [ 43]. It can
be seen that our method achieves the best results on all
non-reference metrics, which reﬂects the excellent image
quality of our results. At the same time, we also note thedisadvantages of our method in full-reference metrics. Wepresent a simple experiment that highlights the limitationsof these full-reference metrics, see Fig. 7. It can be seen
that our results have better visual effects, but they do not
have an advantage in these metrics. This phenomenon has
also been noted in many studies as well [ 6,26,28]. We ar-
gue that with the improving quality of IR, there is a needto reconsider the reference values of existing metrics andsuggest more effective ways to evaluate advanced IR meth-
ods. We also show some qualitative comparison results in
Fig. 6. Even under severe degradation, our method consis-
tently produces highly reasonable and high-quality images
that faithfully represent the content of the LQ images.
25674
Degradation Method PSNR SSIM LPIPS ↓ ManIQA ClipIQA MUSIQ
Single:
SR (×4)BSRGAN 25.06 0.6741 0.2159 0.2214 0.6169 70.38
Real-ESRGAN 24.26 0.6657 0.2116 0.2287 0.5884 69.51
StableSR 22.59 0.6019 0.2130 0.3304 0.7520 72.94
DiffBIR 23.44 0.5841 0.2337 0.2879 0.7147 71.64
PASD 24.90 0.6653 0.1893 0.2607 0.6466 71.39
SUPIR (ours) 22.66 0.5763 0.2662 0.4738 0.8049 73.83
Single:
SR (×8)BSRGAN 22.26 0.5212 0.3523 0.2069 0.5836 67.04
Real-ESRGAN 21.79 0.5280 0.3276 0.2051 0.5349 63.80
StableSR 21.27 0.4857 0.3118 0.3039 0.7333 71.74
DiffBIR 21.86 0.4957 0.3106 0.2845 0.7080 70.26
PASD 21.97 0.5149 0.3034 0.2412 0.6402 70.20
SUPIR (ours) 20.68 0.4488 0.3749 0.4687 0.8009 73.16
Mixture:
Blur (σ=2)+
SR (×4)BSRGAN 24.97 0.6572 0.2261 0.2127 0.5984 69.44
Real-ESRGAN 24.08 0.6496 0.2208 0.2357 0.5853 69.27
StableSR 22.26 0.5721 0.2301 0.3204 0.7488 72.87
DiffBIR 23.28 0.5741 0.2395 0.2829 0.7055 71.22
PASD 24.85 0.6560 0.1952 0.2500 0.6335 71.07
SUPIR (ours) 22.43 0.5626 0.2771 0.4757 0.8110 73.55
Mixture:
SR (×4)+
Noise (σ=40)BSRGAN 17.74 0.3816 0.5659 0.1006 0.4166 51.25
Real-ESRGAN 21.46 0.5220 0.4636 0.1236 0.4536 52.23
StableSR 20.88 0.4174 0.4668 0.2365 0.5833 63.54
DiffBIR 22.08 0.4918 0.3738 0.2403 0.6435 65.97
PASD 21.79 0.4983 0.3842 0.2590 0.5939 69.09
SUPIR (ours) 20.77 0.4571 0.3945 0.4674 0.7840 73.35
Mixture:
Blur (σ=2)+
SR (×4)+
Noise
(σ=20)+
JPEG (q=50)BSRGAN 22.88 0.5397 0.3445 0.1838 0.5402 64.81
Real-ESRGAN 22.01 0.5332 0.3494 0.2115 0.5730 64.76
StableSR 21.39 0.4744 0.3422 0.2974 0.7354 70.94
DiffBIR 21.79 0.4895 0.3465 0.2821 0.7059 69.28
PASD 21.90 0.5118 0.3493 0.2397 0.6326 70.43SUPIR (ours) 20.84 0.4604 0.3806 0.4688 0.8021 73.58
Table 1. Quantitative comparison. Red and blue colors represent
the best and second best performance. ↓represents the smaller the
better, and for the others, the bigger the better.
Metrics BSRGAN Real-ESRGAN StableSR DiffBIR PASD Ours
CLIP-IQA 0.4119 0.5174 0.7654 0.6983 0.7714 0.8232
MUSIQ 55.64 59.42 70.70 69.69 71.87 73.00
MANIQA 0.1585 0.2262 0.3035 0.2619 0.3169 0.4295
(a) Quantitative comparison on 60 real-world LQ images.
Negative PromptsPSNR SSIM LPIPS ↓ ManIQA ClipIQA MUSIQSamples Positive Negative
/check 22.90 0.5519 0.3010 0.3129 0.7049 68.94
/check/check 22.31 0.5250 0.3108 0.4018 0.7937 72.00
/check/check 20.63 0.4747 0.3603 0.4678 0.7933 73.60
/check/check/check 20.66 0.4763 0.3412 0.4740 0.8164 73.66
/check/check 21.79 0.5119 0.3139 0.3180 0.7102 72.68
(b) Ablation study of quality prompts and negative training samples.
Connector PSNR SSIM LPIPS ↓ ManIQA ClipIQA MUSIQ
Zero Convolution [ 92] 19.47 0.4261 0.3969 0.4845 0.8184 74.00
ZeroSFT 20.66 0.4763 0.3412 0.4740 0.8164 73.66
(c) Ablation study of zero convolution and the proposed ZeroSFT.
Table 2. Real-world comparison results and ablation studies.
r=1r=2r=3r=4r=5r=6Nor20.521.021.522.022.5PSNR
PSNR
0.460.480.500.52SSIM
SSIM
r=1r=2r=3r=4r=5r=6Nor0.400.420.440.460.48MANIQA
ManIQA0.770.790.810.83CLIP-IQA
Clip-IQA3.25%StableSR
2.00%DIffBIR
10.58%PASD
84.17%SUPIR
(a) (b)
Figure 8. (a) These plots illustrate the quantitative results as a
function of the variable τr. “Noτr” means not to use the proposed
sampling method. (b) The results of our user study.
Restoration in the Wild. We also test our method on
real-world LQ images. We collect a total of 60 real-world
LQ images from RealSR [ 8], DRealSR [ 80], Real47 [ 49],
and online sources, featuring diverse content including an-imals, plants, faces, buildings, and landscapes. We show
Low-Quality Input Zero Convolution Connector Our ZeroSFT Connector
Figure 9. We compare the proposed ZeroSFT with zero convolu-
tion. Directly using zero convolution results in redundant details.The low-ﬁdelity details can be effectively mitigated by ZeroSFT.
the qualitative results in Fig. 10, and the quantitative results
are shown in Tab. 2a. These results indicate that the im-
ages produced by our method have the best perceptual qual-
ity. We also conduct a user study comparing our method
on real-world LQ images, with 20 participants involved.For each set of comparison images, we instructed partici-
pants to choose the restoration result that was of the highestquality among these test methods. The results are shown
in Fig. 8, revealing that our approach signiﬁcantly outper-
formed state-of-the-art methods in perceptual quality.
4.3. Controlling Restoration with Textual Prompts
After training on a large dataset of image-text pairs and
leveraging the feature of the diffusion model, our methodcan selectively restore images based on human prompts.
Fig. 1(b) illustrates some examples. In the ﬁrst case, the
bike restoration is challenging without prompts, but uponreceiving the prompt, the model reconstructs it accurately.In the second case, the material texture of the hat can beadjusted through prompts. In the third case, even high-levelsemantic prompts allow manipulation over face attributes.In addition to prompting the image content, we can also
prompt the model to generate higher-quality images through
negative-quality prompts. Fig. 11(a) shows two examples.
It can be seen that the negative prompts are very effective inimproving the overall quality of the output image. We alsoobserved that prompts in our method are not always effec-tive. When the provided prompts do not align with the LQ
image, the prompts become ineffective, see Fig. 11(b). We
consider this reasonable for an IR method to stay faithful tothe provided LQ image. This reﬂects a signiﬁcant distinc-tion from text-to-image generation models and underscores
the robustness of our approach.
4.4. Ablation Study
Connector. We compare the proposed ZeroSFT connec-
tor with zero convolution [ 92]. Quantitative results are
shown in Tab. 2c. Compared to ZeroSFT, zero convolu-
tion yields comparable performance on non-reference met-
rics and much lower full-reference performance. In Fig. 9,
we ﬁnd that the drop in non-reference metrics is caused bygenerating low-ﬁdelity content. Therefore, for IR tasks, Ze-roSFT ensures ﬁdelity without losing the perceptual effect.
Training data scaling. We trained our large-scale model
on two smaller datasets for IR, DIV2K [ 3] and LSDIR [ 1].
25675
Low-Quality Input Real-ESRGAN StableSR DiffBIR PASD SUPIR (Ours)
Figure 10. Qualitative comparison on real-world LQ images. SUPIR successfully recovers structured buildings and lifelike rivers. It also
maintains the details existing in LQ, such as the horizontal planks in the beach chairs. Zoom in for better view.
Low-Quality Input No Negative 
PromptUse Negative 
Prompt
Low-Quality Input No Negative 
PromptUse Negative 
PromptLow-Quality Input No Prompt Text: some people  
at the beach
(b) Textual Prompt must align with the LQ image.
Not Working(a) Two Examples Showing the Effect of Negative Prompt 
Figure 11. Inﬂuences of text prompts. (a) Negative prompts lead to detailed and sharp restoration results. (b) Given a positive prompt with
hallucinations, SUPIR avoids generating content absent in the LQ images. Zoom in for better view.
Low-Quality Input Trained on DIV2K Trained on LSDIR Trained on Our Data
Figure 12. Qualitative comparison for SUPIR training on datasetswith different scales. Zoom in for better view.
Low-Quality Input
Fidelity Realistic
r=0.5 r=3 r=6
Figure 13. The effect of the proposed restoration-guided sampling
method. A smaller τrmakes the result more biased toward the
LQ image, which emphasizes the ﬁdelity. A larger τremphasizes
perceived quality, but with lower ﬁdelity. Zoom in for better view.
The qualitative results are shown in Fig. 12, which clearly
demonstrate the importance and necessity of training on
large-scale high-quality data.
Negative-quality samples and prompt. Tab. 2bshows
some quantitative results under different settings. Here,we use positive words describing image quality as “posi-
tive prompt”, and use negative quality words and the CFG
methods described in Sec. 3.2as negative prompt. It can be
seen that adding positive prompts or negative prompts alonecan improve the perceptual quality of the image. Using both
of them simultaneously yields the best perceptual results. If
negative samples are not included for training, these twoprompts will not be able to improve the perceptual quality.Fig. 4and Fig. 11(a) demonstrate the improvement in image
quality brought by using negative prompts.Restoration-guided sampling method. The proposed
restoration-guided sampling method is mainly controlled byτ
r. The larger τris, the fewer corrections are made to the
generation at each step. The smaller τris, the more gen-
erated content will be forced to be closer to the LQ image.
Please refer to Fig. 13for a qualitative comparison. When
τr=0.5, the image is blurry because its output is limited
by the LQ image and cannot generate texture and details.
Whenτr=6 , there is not much guidance during genera-
tion. The model generates a lot of texture that is not present
in the LQ image, especially in ﬂat area. Fig. 8(a) illustrates
the quantitative results of restoration as a function of the
variableτr. As shown in Fig. 8(a), decreasing τrfrom 6 to
4 does not result in a signiﬁcant decline in visual quality,while ﬁdelity performance improves. As restoration guid-ance continues to strengthen, although PSNR continues to
improve, the images gradually become blurry with loss ofdetails, as depicted in Fig. 13. Therefore, we choose τ
r=4
as the default parameter, as it doesn’t compromise image
quality while effectively enhancing ﬁdelity.
5. Conclusion
We propose SUPIR as a pioneering IR method, empowered
by model scaling, dataset enrichment, and advanced design
features, expanding the horizons of IR with enhanced per-
ceptual quality and controlled textual prompts.
Acknowledgment. This work was supported in part
by the National Natural Science Foundation of China
(62276251, 62272450), the Joint Lab of CAS-HK, the Na-
tional Key R&D Program of China (No. 2022ZD0160100),
and in part by the Y outh Innovation Promotion Associationof Chinese Academy of Sciences (No. 2020356).
25676
References
[1] Lsdir dataset: A large scale dataset for image restoration.
http s:// data.vision.ee.ethz.ch/yawli/
index.html , 2023. Accessed: 2023-11-15. 4,7
[2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
age2stylegan: How to embed images into the stylegan la-
tent space? In Proceedings of the IEEE/CVF international
conference on computer vision , pages 4432–4441, 2019. 2
[3] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge
on single image super-resolution: Dataset and study. In The
IEEE Conference on Computer Vision and Pattern Recog-nition (CVPR) Workshops , 2017. 4,7
[4] David Bau, Hendrik Strobelt, William Peebles, Jonas
Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba. Se-
mantic photo manipulation with a generative image prior.
arXiv preprint arXiv:2005.07727 , 2020. 2
[5] Seﬁ Bell-Kligler, Assaf Shocher, and Michal Irani. Blind
super-resolution kernel estimation using an internal-gan.
Advances in Neural Information Processing Systems , 32,
2019. 2
[6] Y ochai Blau and Tomer Michaeli. The perception-distortion
tradeoff. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition , pages 6228–6237,
2018. 6
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. Advances in neu-
ral information processing systems , 33:1877–1901, 2020.
2,3
[8] Jianrui Cai, Hui Zeng, Hongwei Y ong, Zisheng Cao,
and Lei Zhang. Toward real-world single image super-
resolution: A new benchmark and a new model. In Pro-
ceedings of the IEEE/CVF International Conference onComputer Vision , pages 3086–3095, 2019. 7
[9] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu,
and Chen Change Loy. Glean: Generative latent bank
for large-factor image super-resolution. In Proceedings of
the IEEE/CVF conference on computer vision and patternrecognition , pages 14245–14254, 2021. 2
[10] Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xi-
aoguang Han, Tao Yang, and Shihui Guo. Real-world blindsuper-resolution via feature matching with implicit high-resolution priors. In Proceedings of the 30th ACM Interna-
tional Conference on Multimedia , pages 1329–1338, 2022.
2
[11] Haoyu Chen, Jinjin Gu, Yihao Liu, Salma Abdel Magid,
Chao Dong, Qiong Wang, Hanspeter Pﬁster, and Lei Zhu.
Masked image training for generalizable deep image de-
noising. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 1692–
1703, 2023. 2
[12] Junsong Chen, Jincheng Y u, Chongjian Ge, Lewei Yao,
Enze Xie, Y ue Wu, Zhongdao Wang, James Kwok, PingLuo, Huchuan Lu, et al. Pixart- α: Fast training of diffu-
sion transformer for photorealistic text-to-image synthesis.arXiv preprint arXiv:2310.00426 , 2023. 3,4[13] Zheng Chen, Y ulun Zhang, Jinjin Gu, Linghe Kong, Xi-
aokang Yang, and Fisher Y u. Dual aggregation trans-former for image super-resolution. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 12312–12321, 2023. 2
[14] Zheng Chen, Y ulun Zhang, Ding Liu, Bin Xia, Jinjin Gu,
Linghe Kong, and Xin Y uan. Hierarchical integration dif-
fusion model for realistic image deblurring. Advances in
Neural Information Processing Systems , 2023. 2
[15] Jooyoung Choi, Sungwon Kim, Y onghyun Jeong,
Y oungjune Gwon, and Sungroh Y oon. Ilvr: Conditioningmethod for denoising diffusion probabilistic models. arXiv
preprint arXiv:2108.02938 , 2021. 2
[16] DeepFloyd. Deepﬂoyd inference framework. https:
//www.deepfloyd.ai/deepfloyd-if , 2023. Ac-
cessed: 2023-11-14. 3
[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In 2009 IEEE conference on computer vision
and pattern recognition , pages 248–255. Ieee, 2009. 4
[18] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780–8794, 2021. 2
[19] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou
Tang. Image super-resolution using deep convolutional net-works. IEEE transactions on pattern analysis and machine
intelligence , 38(2):295–307, 2015. 2
[20] Chao Dong, Chen Change Loy, and Xiaoou Tang. Accel-
erating the super-resolution convolutional neural network.InComputer Vision–ECCV 2016: 14th European Confer-
ence, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part II 14 , pages 391–407. Springer, 2016. 2
[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,Sylvain Gelly, et al. An image is worth 16x16 words:Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 3
[22] Y uchen Fan, Jiahui Y u, Yiqun Mei, Y ulun Zhang, Y un Fu,
Ding Liu, and Thomas S Huang. Neural sparse representa-
tion for image restoration. Advances in Neural Information
Processing Systems , 33:15394–15404, 2020. 2
[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,and Y oshua Bengio. Generative adversarial nets. Advances
in neural information processing systems , 27, 2014. 2
[24] Jinjin Gu and Chao Dong. Interpreting super-resolution
networks with local attribution maps. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9199–9208, 2021. 4
[25] Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong.
Blind super-resolution with iterative kernel correction. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition , pages 1604–1613, 2019. 2
[26] Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy
Ren, and Chao Dong. Pipal: a large-scale image qual-
ity assessment dataset for perceptual image restoration. In
25677
Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XI
16, pages 633–651, 2020. 2,6
[27] Jinjin Gu, Y ujun Shen, and Bolei Zhou. Image pro-
cessing using multi-code gan prior. In Proceedings of
the IEEE/CVF conference on computer vision and patternrecognition , pages 3012–3021, 2020. 2
[28] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S Ren, Radu
Timofte, Y uan Gong, Shanshan Lao, Shuwei Shi, JiahaoWang, Sidi Yang, et al. Ntire 2022 challenge on perceptual
image quality assessment. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 951–967, 2022. 6
[29] Jinjin Gu, Xianzheng Ma, Xiangtao Kong, Y u Qiao, and
Chao Dong. Networks are slacking off: Understanding gen-eralization problem in image deraining. Advances in Neural
Information Processing Systems , 2023. 2,4
[30] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 5
[31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural informa-
tion processing systems , 33:6840–6851, 2020. 2
[32] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Y uanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 2,3
[33] Hengyuan Hu, Rui Peng, Y u-Wing Tai, and Chi-Keung
Tang. Network trimming: A data-driven neuron pruning ap-
proach towards efﬁcient deep architectures. arXiv preprint
arXiv:1607.03250 , 2016. 3
[34] Yan Huang, Shang Li, Liang Wang, Tieniu Tan, et al. Un-
folding the alternating optimization for blind super resolu-tion. Advances in Neural Information Processing Systems ,
33:5632–5643, 2020. 2
[35] Zheng Hui, Jie Li, Xiumei Wang, and Xinbo Gao. Learn-
ing the non-differentiable optimization for blind super-
resolution. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 2093–2102,
2021. 2
[36] Y ounghyun Jo, Sejong Yang, and Seon Joo Kim. Srﬂow-da:
Super-resolution using normalizing ﬂow with deep convo-
lutional block. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
364–372, 2021. 2
[37] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,
Eli Shechtman, Sylvain Paris, and Taesung Park. Scal-ing up gans for text-to-image synthesis. In Proceedings of
the IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 10124–10134, 2023. 3
[38] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray, AlecRadford, Jeffrey Wu, and Dario Amodei. Scaling laws forneural language models. arXiv preprint arXiv:2001.08361 ,
2020. 3,4
[39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehti-
nen. Progressive growing of gans for improved quality,stability, and variation. arXiv preprint arXiv:1710.10196 ,
2017. 2[40] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 4401–4410, 2019. 2,
4
[41] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generativemodels. Advances in Neural Information Processing Sys-
tems , 35:26565–26577, 2022. 5
[42] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming
Song. Denoising diffusion restoration models. Advances in
Neural Information Processing Systems , 35:23593–23606,
2022. 2
[43] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and
Feng Yang. Musiq: Multi-scale image quality transformer.InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 5148–5157, 2021. 6
[44] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer
Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segmentanything. arXiv preprint arXiv:2304.02643 , 2023. 2,3,4,
5
[45] Xiangtao Kong, Xina Liu, Jinjin Gu, Y u Qiao, and Chao
Dong. Reﬂash dropout in image super-resolution. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6002–6012, 2022. 6
[46] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc
V an Gool, and Radu Timofte. Swinir: Image restoration
using swin transformer. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 1833–
1844, 2021. 2
[47] Jingyun Liang, Kai Zhang, Shuhang Gu, Luc V an Gool, and
Radu Timofte. Flow-based kernel prior with application toblind super-resolution. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 10601–10610, 2021.
[48] Jie Liang, Hui Zeng, and Lei Zhang. Efﬁcient and
degradation-adaptive network for real-world image super-
resolution. In European Conference on Computer Vision ,
pages 574–591. Springer, 2022. 2
[49] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben
Fei, Bo Dai, Wanli Ouyang, Y u Qiao, and Chao Dong. Diff-
bir: Towards blind image restoration with generative diffu-
sion prior. arXiv preprint arXiv:2308.15070 , 2023. 2,3,6,
7
[50] Anran Liu, Yihao Liu, Jinjin Gu, Y u Qiao, and Chao Dong.
Blind image super-resolution: A survey and beyond. IEEE
transactions on pattern analysis and machine intelligence ,
45(5):5461–5480, 2022. 2
[51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Y ong Jae
Lee. Visual instruction tuning. In NeurIPS , 2023. 4
[52] Yihao Liu, Anran Liu, Jinjin Gu, Zhipeng Zhang, Wenhao
Wu, Y u Qiao, and Chao Dong. Discovering distinctive”semantics” in super-resolution networks. arXiv preprint
arXiv:2108.00406 , 2021. 2
[53] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 5
25678
[54] A Lugmayr, M Danelljan, L V an Gool, and R Timofte.
Learning the super-resolution space with normalizing ﬂow.
ECCV , Srﬂow , 2020. 2
[55] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi,
and Cynthia Rudin. Pulse: Self-supervised photo upsam-
pling via latent space exploration of generative models. In
Proceedings of the ieee/cvf conference on computer visionand pattern recognition , pages 2437–2445, 2020. 2
[56] Tomer Michaeli and Michal Irani. Nonparametric blind
super-resolution. In Proceedings of the IEEE International
Conference on Computer Vision , pages 945–952, 2013. 2
[57] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang,
Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter:
Learning adapters to dig out more controllable abil-ity for text-to-image diffusion models. arXiv preprint
arXiv:2302.08453 , 2023. 2,3
[58] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3883–3891,
2017. 2
[59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image genera-tion and editing with text-guided diffusion models. arXiv
preprint arXiv:2112.10741 , 2021. 2
[60] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin,
Chen Change Loy, and Ping Luo. Exploiting deep genera-tive prior for versatile image restoration and manipulation.IEEE Transactions on Pattern Analysis and Machine Intel-ligence , 44(11):7474–7489, 2021. 2
[61] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M ¨uller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion mod-els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952 , 2023. 2,3
[62] Alec Radford, Luke Metz, and Soumith Chintala. Un-
supervised representation learning with deep convolu-
tional generative adversarial networks. arXiv preprint
arXiv:1511.06434 , 2015. 2
[63] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya
Sutskever. Zero-shot text-to-image generation. In Interna-
tional Conference on Machine Learning , pages 8821–8831.
PMLR, 2021. 2
[64] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint
arXiv:2204.06125 , 1(2):3, 2022. 2
[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684–10695, 2022. 2,3,5
[66] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-mans, et al. Photorealistic text-to-image diffusion modelswith deep language understanding. Advances in Neural In-
formation Processing Systems , 35:36479–36494, 2022. 2,
3
[67] Christoph Schuhmann, Romain Beaumont, Richard V encu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for train-
ing next generation image-text models. Advances in Neural
Information Processing Systems , 35:25278–25294, 2022. 4
[68] Yang Song and Stefano Ermon. Generative modeling by
estimating gradients of the data distribution. Advances in
neural information processing systems , 32, 2019. 2
[69] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma,
Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential
equations. arXiv preprint arXiv:2011.13456 , 2020. 2
[70] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji-
aya Jia. Scale-recurrent network for deep image deblurring.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 8174–8182, 2018. 2
[71] InternLM Team. Internlm: A multilingual language model
with progressively enhanced capabilities. https://
github.com/InternLM/InternLM , 2023. 2,3
[72] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Bap-
tiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar,
et al. Llama: Open and efﬁcient foundation language mod-
els. arXiv preprint arXiv:2302.13971 , 2023. 2,3
[73] Aaron V an Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in neural information
processing systems , 30, 2017. 2
[74] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Ex-
ploring clip for assessing the look and feel of images. InProceedings of the AAAI Conference on Artiﬁcial Intelli-gence , pages 2555–2563, 2023. 6
[75] Jianyi Wang, Zongsheng Y ue, Shangchen Zhou, Kelvin CK
Chan, and Chen Change Loy. Exploiting diffusion priorfor real-world image super-resolution. arXiv preprint
arXiv:2305.07015 , 2023. 2,6
[76] Xintao Wang, Ke Y u, Chao Dong, and Chen Change Loy.
Recovering realistic texture in image super-resolution bydeep spatial feature transform. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pages 606–615, 2018. 4
[77] Xintao Wang, Y u Li, Honglun Zhang, and Ying Shan. To-
wards real-world blind face restoration with generative fa-cial prior. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 9168–9178,
2021. 2
[78] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
Real-esrgan: Training real-world blind super-resolution
with pure synthetic data. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 1905–
1914, 2021. 2,5,6
[79] Yinhuai Wang, Jiwen Y u, and Jian Zhang. Zero-shot im-
age restoration using denoising diffusion null-space model.arXiv preprint arXiv:2212.00490 , 2022. 2
25679
[80] Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qix-
iang Ye, Wangmeng Zuo, and Liang Lin. Component
divide-and-conquer for real-world image super-resolution.
InComputer Vision–ECCV 2020: 16th European Confer-
ence, Glasgow, UK, August 23–28, 2020, Proceedings, PartVIII 16 , pages 101–117. Springer, 2020. 7
[81] Y uxin Wu and Kaiming He. Group normalization. In Pro-
ceedings of the European conference on computer vision(ECCV) , pages 3–19, 2018. 4
[82] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu,
Zhuofan Zong, Y u Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. arXiv
preprint arXiv:2305.18295 , 2023. 3
[83] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao,
Y uan Gong, Mingdeng Cao, Jiahao Wang, and Y ujiuYang. Maniqa: Multi-dimension attention network for no-
reference image quality assessment. In Proceedings of the
IEEE/CVF Conference on Computer Vision and PatternRecognition , pages 1191–1200, 2022. 6
[84] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Gan
prior embedded network for blind face restoration in thewild. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 672–681,
2021. 2
[85] Tao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang.
Pixel-aware stable diffusion for realistic image super-resolution and personalized stylization. arXiv preprint
arXiv:2308.14469 , 2023. 6
[86] Jiale Zhang, Y ulun Zhang, Jinjin Gu, Y ongbing Zhang,
Linghe Kong, and Xin Y uan. Accurate image restora-tion with attention retractable transformer. In International
Conference on Learning Representations (ICLR) , 2023. 2
[87] Kai Zhang, Wangmeng Zuo, Y unjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learningof deep cnn for image denoising. IEEE transactions on
image processing , 26(7):3142–3155, 2017. 2
[88] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.
Learning deep cnn denoiser prior for image restoration. In
Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 3929–3938, 2017. 2
[89] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: To-
ward a fast and ﬂexible solution for cnn-based image de-
noising. IEEE Transactions on Image Processing , 27(9):
4608–4622, 2018. 2
[90] Kai Zhang, Jingyun Liang, Luc V an Gool, and Radu Timo-
fte. Designing a practical degradation model for deep blindimage super-resolution. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 4791–
4800, 2021. 2,6
[91] Kai Zhang, Yawei Li, Jingyun Liang, Jiezhang Cao, Y u-
lun Zhang, Hao Tang, Deng-Ping Fan, Radu Timofte, andLuc V an Gool. Practical blind image denoising via swin-
conv-unet and data synthesis. Machine Intelligence Re-
search , pages 1–14, 2023. 2
[92] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. InProceedings of the IEEE/CVF International Conference onComputer Vision , pages 3836–3847, 2023. 2,3,7[93] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion , pages 586–595, 2018. 6
[94] Ruofan Zhang, Jinjin Gu, Haoyu Chen, Chao Dong, Y u-
lun Zhang, and Wenming Yang. Crafting training degra-dation distribution for the accuracy-generalization trade-offin real-world super-resolution. International Conference on
Machine Learning (ICML) , 2023.
2,6
[95] Y ulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and
Y un Fu. Residual non-local attention networks for imagerestoration. In International Conference on Learning Rep-
resentations (ICLR) , 2019. 2
[96] Y ulun Zhang, Yapeng Tian, Y u Kong, Bineng Zhong, and
Y un Fu. Residual dense network for image restoration.IEEE transactions on pattern analysis and machine intel-ligence , 43(7):2480–2495, 2020. 2
[97] Yi Zhang, Xiaoyu Shi, Dasong Li, Xiaogang Wang, Jian
Wang, and Hongsheng Li. A uniﬁed conditional frame-work for diffusion-based image restoration. arXiv preprint
arXiv:2305.20049 , 2023. 2
[98] Yang Zhao, Y u-Chuan Su, Chun-Te Chu, Yandong Li, Mar-
ius Renn, Y ukun Zhu, Changyou Chen, and Xuhui Jia.Rethinking deep face restoration. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7652–7661, 2022.
[99] Shangchen Zhou, Kelvin Chan, Chongyi Li, and
Chen Change Loy. Towards robust blind face restoration
with codebook lookup transformer. Advances in Neural In-
formation Processing Systems , 35:30599–30611, 2022. 2
[100] Jiapeng Zhu, Deli Zhao, Bo Zhang, and Bolei Zhou. Disen-
tangled inference for gans with latently invertible autoen-coder. International Journal of Computer Vision , 130(5):
1259–1276, 2022. 2
25680
