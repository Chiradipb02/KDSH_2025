TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models
Haomiao Ni1* Bernhard Egger2Suhas Lohit3Anoop Cherian3Ye Wang3
Toshiaki Koike-Akino3Sharon X. Huang1Tim K. Marks3
1The Pennsylvania State University, USA2Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg, Germany
3Mitsubishi Electric Research Laboratories (MERL), USA
1{hfn5052, suh972 }@psu.edu2bernhard.egger@fau.de3{slohit,cherian,yewang,koike,tmarks }@merl.com
https://merl.com/demos/TI2V-Zero
Abstract
Text-conditioned image-to-video generation (TI2V) aims
to synthesize a realistic video starting from a given image
(e.g., a woman’s photo) and a text description ( e.g., “a
woman is drinking water. ”). Existing TI2V frameworks of-
ten require costly training on video-text datasets and spe-
cific model designs for text and image conditioning. In
this paper, we propose TI2V-Zero, a zero-shot, tuning-
free method that empowers a pretrained text-to-video (T2V)
diffusion model to be conditioned on a provided image,
enabling TI2V generation without any optimization, fine-
tuning, or introducing external modules. Our approach
leverages a pretrained T2V diffusion foundation model as
the generative prior. To guide video generation with the
additional image input, we propose a “repeat-and-slide”
strategy that modulates the reverse denoising process, al-
lowing the frozen diffusion model to synthesize a video
frame-by-frame starting from the provided image. To ensure
temporal continuity, we employ a DDPM inversion strat-
egy to initialize Gaussian noise for each newly synthesized
frame and a resampling technique to help preserve visual
details. We conduct comprehensive experiments on both
domain-specific and open-domain datasets, where TI2V-
Zero consistently outperforms a recent open-domain TI2V
model. Furthermore, we show that TI2V-Zero can seam-
lessly extend to other tasks such as video infilling and pre-
diction when provided with more images. Its autoregressive
design also supports long video generation.
1. Introduction
Image-to-video (I2V) generation is an appealing topic with
various applications, including artistic creation, entertain-
ment, and data augmentation for machine learning [39].
Given a single image x0and a text prompt y, text-
conditioned image-to-video (TI2V) generation aims to syn-
*Work done during an internship at MERL.
“A man with the expression of slight happiness on his face.”
“A person is drumming.”
“A serene mountain cabin covered in a fresh blanket of snow.”Figure 1. Examples of generated video frames using our proposed
TI2V-Zero. The given first image x0is highlighted with the red
box, and the text condition yis shown under each row of the video.
The remaining columns show the 6th, 11th, and 16th frames of the
generated output videos. Each generated video has 16 frames with
a resolution of 256×256.
thesize Mnew frames to yield a realistic video, ˆx=
⟨x0,ˆx1, . . . , ˆxM⟩, starting from the given frame x0and sat-
isfying the text description y. Current TI2V generation
methods [59, 63, 70] typically rely on computationally-
heavy training on video-text datasets and specific archi-
tecture designs to enable text and image conditioning.
Some [12, 25] are constrained to specific domains due to
the lack of training with large-scale open-domain datasets.
Other approaches, such as [14, 67], utilize pretrained foun-
dation models to reduce training costs, but they still need to
train additional modules using video data.
In this paper, we propose TI2V-Zero, which achieves
zero-shot TI2V generation using only an open-domain pre-
trained text-to-video (T2V) latent diffusion model [60].
Here “zero-shot” means that when using the diffusion
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9015
model (DM) that was trained only for text conditioning,
our framework enables image conditioning without any op-
timization, fine-tuning, or introduction of additional mod-
ules. Specifically, we guide the generation process by incor-
porating the provided image x0into the output latent code
at each reverse denoising step. To ensure that the tempo-
ral attention layers of the pretrained DM focus on informa-
tion from the given image, we propose a “repeat-and-slide”
strategy to synthesize the video in a frame-by-frame man-
ner, rather than directly generating the entire video volume.
Notably, TI2V-Zero is not trained for the specific domain of
the provided image, thus allowing the model to generalize
to any image during inference. Additionally, its autoregres-
sive generation makes the synthesis of long videos possible.
While the standard denoising sampling process start-
ing with randomly initialized Gaussian noise can produce
matching semantics, it often results in temporally inconsis-
tent videos. Therefore, we introduce an inversion strategy
based on the DDPM [20] forward process, to provide a more
suitable initial noise for generating each new frame. We
also apply a resampling technique [33] in the video DM to
help preserve the generated visual details. Our approach en-
sures that the network maintains temporal consistency, gen-
erating visually convincing videos conditioned on the given
starting image (see Fig. 1).
We conduct extensive experiments on MUG [1], UCF-
101 [56], and a new open-domain dataset. In these experi-
ments, TI2V-Zero consistently performs well, outperform-
ing a state-of-the-art model [67] that was based on a video
diffusion foundation model [8] and was specifically trained
to enable open-domain TI2V generation.
2. Related Work
2.1. Conditional Image-to-Video Generation
Conditional video generation aims to synthesize videos
guided by user-provided signals. It can be classified accord-
ing to which type(s) of conditions are given, such as text-
to-video (T2V) generation [5, 16, 21, 23, 31, 65], video-to-
video (V2V) generation [7, 38, 40, 45, 61, 64], and image-
to-video (I2V) generation [4, 10, 25, 34, 39, 69]. Here we
discuss previous text-conditioned image-to-video (TI2V)
generation methods [12, 14, 22, 44, 63, 70]. Hu et al .
[25] introduced MAGE, a TI2V generator that integrates a
motion anchor structure to store appearance-motion-aligned
representations through three-dimensional axial transform-
ers. Yin et al . [70] proposed DragNUWA, a diffusion-
based model capable of generating videos controlled by
text, image, and trajectory information with three modules
including a trajectory sampler, a multi-scale fusion, and
an adaptive training strategy. However, these TI2V frame-
works require computationally expensive training on video-
text datasets and a particular model design to support text-and-image-conditioned training. In contrast, our proposed
TI2V-Zero leverages a pretrained T2V diffusion model to
achieve zero-shot TI2V generation without additional opti-
mization or fine-tuning, making it suitable for a wide range
of applications.
2.2. Adaptation of Diffusion Foundation Models
Due to the recent successful application of diffusion models
(DM) [20, 42, 47, 54, 55] to both image and video genera-
tion, visual diffusion foundation models have gained promi-
nence. These include text-to-image (T2I) models such as
Imagen [50] and Stable Diffusion [47], as well as text-
to-video (T2V) models such as ModelScopeT2V [60] and
VideoCrafter1 [8]. These models are trained with large-
scale open-domain datasets, often including LAION-400M
[52] and WebVid-10M [2]. They have shown immense po-
tential for adapting their acquired knowledge base to ad-
dress a wide range of downstream tasks, thereby reducing or
eliminating the need for extensive labeled data. For exam-
ple, previous works have explored the application of large
T2I models to personalized image generation [13, 49], im-
age editing [17, 33, 35–37], image segmentation [3, 68],
video editing [45, 62], and video generation [14, 27, 53, 66].
In contrast to T2I models, there are fewer works on the
adaptation of large-scale T2V models. Xing et al. [67] pro-
posed DynamicCrafter for open-domain TI2V generation
by adapting a T2V foundation model [8]. To control the
generative process, they first employed a learnable image
encoding network to project the given image into a text-
aligned image embedding space. Subsequently, they uti-
lized dual cross-attention layers to fuse text and image in-
formation and also concatenated the image with the initial
noise to provide the video DM with more precise image de-
tails. In contrast, in this paper we explore how to inject the
provided image to guide the DM sampling process based
solely on the pretrained T2V model itself, with no addi-
tional training for the new TI2V task.
3. Methodology
Given one starting image x0and text y, let x=
⟨x0, x1, . . . , xM⟩represent a real video corresponding
to text y. The objective of text-conditioned image-to-
video (TI2V) generation is to synthesize a video ˆx=
⟨x0,ˆx1, . . . , ˆxM⟩, such that the conditional distribution of
ˆxgiven x0andyis identical to the conditional distribution
ofxgiven x0andy, i.e., p(ˆx|x0, y) = p(x|x0, y). Our
proposed TI2V-Zero can be built on a pretrained T2V diffu-
sion model with a 3D-UNet-based denoising network. Here
we choose ModelScopeT2V [60] as backbone due to its
promising open-domain T2V generation ability. Below, we
first introduce preliminaries about diffusion models, then
introduce the architecture of the pretrained T2V model, and
finally present the details of our TI2V-Zero.
9016
Diffusion ProcessDDPM Inversion…!𝐳!̂𝑧!"̂𝑧!#$%̂𝑧!#𝐬"∪…!𝐳&̂𝑧&"̂𝑧&#$%̂𝑧&#𝐬&Replace…!𝐳"̂𝑧""̂𝑧"#$%̂𝑧"#…𝑠"%𝑠"#$%𝑠""̂𝑧"#Slideadd 𝑡 step noise
𝒟!𝑥'(%………!𝐳&$%̂𝑧&$%"̂𝑧&$%#$%̂𝑧&$%#𝐬&$%add (𝑡−1) step noiseU-Net 𝜖)U-Net 𝜖)U-Net 𝜖)Replace𝑖>0𝑖=0Construct 𝐬"𝑠"%𝑠"#$%𝑠""𝑠"#$%𝐬"…𝑧"ℇ𝑥"Repeat𝐾 times
Reverse Process Using Pretrained Denoising U-NetResampleResample𝑦Resample𝑦𝑦
Figure 2. Illustration of the process of applying TI2V-Zero to generate the new frame ˆxi+1, given the starting image x0and text y. TI2V-
Zero is built upon a frozen pretrained T2V diffusion model, including frame encoder E, frame decoder D, and the denoising U-Net ϵθ. At
the beginning of generation ( i= 0), we encode x0asz0and repeat it Ktimes to form the queue s0. We then apply DDPM-based inversion
tos0to produce the initial Gaussian noise ˆzT. Subsequently, in each reverse denoising step using U-Net ϵθ, we keep replacing the first K
frames of ˆztwith the noisy latent code stderived from s0. Resampling is also applied within each step to improve motion coherence. We
finally decode the final frame of the clean latent code ˆz0as the new synthesized frame ˆxi+1. To compute the new s0for the next iteration
of generation ( i >0), we perform a sliding operation by dequeuing s0
0and enqueuing ˆzK
0within s0.
3.1. Preliminaries: Diffusion Models
Diffusion Models (DM) [20, 54, 55] are probabilistic mod-
els designed to learn a data distribution. Here we introduce
the fundamental concepts of Denoising Diffusion Proba-
bilistic Models (DDPM). Given a sample from the data
distribution z0∼q(z0), the forward diffusion process of
a DM produces a Markov chain z1, . . . ,zTby iteratively
adding Gaussian noise to z0according to a variance sched-
uleβ1, . . . , β T, that is:
q(zt|zt−1) =N(zt;p
1−βtzt−1, βtI), (1)
where variances βtare constant. When the βtare small,
the posterior q(zt−1|zt)can be well approximated by a di-
agonal Gaussian [41, 54]. Furthermore, if the length of the
chain, denoted by T, is sufficiently large, zTcan be well
approximated by a standard Gaussian distribution N(0,I).
These suggest that the true posterior q(zt−1|zt)can be esti-
mated by pθ(zt−1|zt)defined as:
pθ(zt−1|zt) =N(zt−1;µθ(zt), σ2
tI), (2)
where variances σtare also constants. The reverse denois-
ing process in the DM (also termed sampling ) then gener-
ates samples z0∼pθ(z0)by starting with Gaussian noise
zT∼ N(0,I)and gradually reducing noise in a Markov
chain zT−1,zT−2, . . . ,z0using a learned pθ(zt−1|zt). To
learn pθ(zt−1|zt), Gaussian noise ϵis first added to z0to
generate samples zt. Utilizing the independence property
of the noise added at each forward step in Eq. (1), we can
calculate the total noise variance as ¯αt=Qt
i=0(1−βi)and
transform z0toztin a single step:
q(zt|z0) =N(zt;√¯αtz0,(1−¯αt)I). (3)
Then a model ϵθis trained to predict ϵusing the following
mean-squared error loss:
L=Et∼U(1,T),z0∼q(z0),ϵ∼N (0,I)
||ϵ−ϵθ(zt, t)||2
,(4)where diffusion step tis uniformly sampled from
{1, . . . , T }. Then µθ(zt)in Eq. (2) can be derived
from ϵθ(zt, t)to model pθ(zt−1|zt)[20]. The denois-
ing model ϵθis implemented using a time-conditioned U-
Net [48] with residual blocks [15] and self-attention lay-
ers [58]. Diffusion step tis specified to ϵθby the sinu-
soidal position embedding [58]. Conditional generation
that samples z0∼pθ(z0|y)can be achieved by learning
ay-conditioned model ϵθ(zt, t, y)[41, 47] with classifier-
free guidance [19]. During training, the condition yin
ϵθ(zt, t, y)is replaced by a null label ∅with a fixed proba-
bility. When sampling, the output is generated as follows:
ˆϵθ(zt, t, y) =ϵθ(zt, t,∅) +g·(ϵθ(zt, t, y)−ϵθ(zt, t,∅)),(5)
where gis the guidance scale.
3.2. Architecture of Pretrained T2V Model
TI2V-Zero can be built upon a pretrained T2V diffusion
model with a 3D-UNet-based denoising network. Here we
choose ModelScopeT2V [60] as the pretrained model (de-
notedM). We now describe this T2V model in detail.
Structure Overview. Given a text prompt y, the T2V
model Msynthesizes a video ˆx=⟨ˆx0,ˆx1, . . . , ˆxK⟩with a
pre-defined video of length (K+1) using a latent video dif-
fusion model. Similar to Latent Diffusion Models (LDM)
[47],Mincorporates a frame auto-encoder [11, 28] for the
conversion of data between pixel space Xand latent space
Zthrough its encoder Eand decoder D. Given the real
video x=⟨x0, x1, . . . , xK⟩,Mfirst utilizes the frame en-
coderEto encode the video xasz=⟨z0, z1, . . . , zK⟩.
Here the sizes of pixel frame xand latent frame zare
Hx×Wx×3andHz×Wz×Cz, respectively. To be
consistent with the notation used for the DM, we denote the
9017
Algorithm 1 Generation using our TI2V-Zero approach.
Input: The starting frame x0; The text prompt y; The pretrained
T2V Model Mfor generating (K+ 1) -frame videos, includ-
ing frame encoder Eand frame decoder D, and the DM de-
noising networks ϵθ; The iteration number Ufor resampling;
The parameter Mto control the length of the output video.
Output: A synthesized video ˆxwith(M+ 1) frames.
1:z0← E(x0) // Encode x0
2:s0← ⟨z0, z0,···, z0⟩ // Repeat z0forKtimes
3:ˆx← ⟨x0⟩
4:fori= 1,2,···, Mdo
// Generate one new frame ˆxi
5: sT∼ N(√¯αTs0,(1−¯αT)I) // DDPM Inversion
6: ˆzK
T∼ N(√¯αTsK−1
0,(1−¯αT)I)
7: ˆzT←sT∪ˆzK
T // Initialize ˆzT
8: fort=T−1,···,2,1do
9: st∼ N(√¯αts0,(1−¯αt)I)
10: foru= 1,2,···, Udo
11: ⟨ˆz0
t,ˆz1
t,···,ˆzK−1
t⟩ ←st // Replace
12: ˆzt−1∼ N(µθ(ˆzt, y), σ2
tI)
13: ifu < U andt >1then
14: ˆzt∼ N(√1−βtˆzt−1, βtI) // Resample
15: end if
16: end for
17: end for
18: s0← ⟨s1
0, s2
0,···, sK−1
0⟩ ∪ˆzK
0 // Slide
19: ˆxi← D(ˆzK
0) // Decode ˆzK
0
20: ˆx←ˆx∪ˆxi
21:end for
22:return ˆx
clean video latent z=z0=⟨z0
0, z1
0, . . . , zK
0⟩.Mthen
learns a DM on the latent space Zthrough a 3D denois-
ing U-Net ϵθ[9]. Let zt=⟨z0
t, z1
t, . . . , zK
t⟩represent the
latent sequence that results from adding noise over tsteps
to the original latent sequence z0. When training, the for-
ward diffusion process of a DM transforms the initial latent
sequence z0intozTby iteratively adding Gaussian noise ϵ
forTsteps. During inference, denoising U-Net ϵθpredicts
the added noise at each step, enabling the generation of the
clean latent sequence ˆz0=⟨ˆz0
0,ˆz1
0, . . . , ˆzK
0⟩starting from
randomly sampled Gaussian noise zT∼ N(0,I).
Text Conditioning Mechanism. Memploys a cross-
attention mechanism [47] to incorporate text information
into the generative process as guidance. Specifically, M
uses a pretrained CLIP model [46] to encode the prompt y
as the text embedding e. The embedding eis later used as
the key and value in the multi-head attention layer within
the spatial attention blocks, thus enabling the integration of
text features with the intermediate U-Net features in ϵθ.
Denoising U-Net. The denoising U-Net ϵθincludes four
key building blocks: the initial block, the downsampling
block, the spatio-temporal block, and the upsampling block.
The initial block transfers the input into the embedding
Ground TruthVideo TimeTI2V Generation(Replacing) ✘Video Infilling(Replacing) ✘TI2V Generation(TI2V-Zero) ✓Video Infilling(TI2V-Zero) ✓Single-Frame Prediction(Replacing) ✓
………
…
…
…
“A person is riding horse.”Figure 3. Illustration of the motivation behind our framework.
We explore the application of a replacing-based baseline approach
(rows 2–4, labeled “Replacing”) and our TI2V-Zero (rows 5–6, la-
beled “TI2V-Zero”) in various video generation tasks. The given
real frames for each task are highlighted by red boxes and the text
input is shown under the block. The replacing-based approach
is only effective at predicting a single frame when all the other
frames in the video are provided, while TI2V-Zero generates tem-
porally coherent videos for both the TI2V and video infilling tasks.
space, while the downsampling and upsampling blocks are
responsible for spatially downsampling and upsampling the
feature maps. The spatio-temporal block is designed to cap-
ture spatial and temporal dependencies in the latent space,
which comprises 2D spatial convolution, 1D temporal con-
volution, 2D spatial attention, and 1D temporal attention.
3.3. Our Framework
Leveraging the pretrained T2V foundation model M, we
first propose a straightforward replacing -based baseline for
adapting Mto TI2V generation. We then analyze the possi-
ble reasons why it fails and introduce our TI2V-Zero frame-
work, which includes a repeat-and-slide strategy, DDPM-
based inversion, and resampling. Figure 2 and Algorithm 1
demonstrate the inference process of TI2V-Zero.
Replacing-based Baseline. We assume that the pre-
trained model Mis designed to generate the video with a
fixed length of (K+ 1) . So we first consider synthesiz-
ing videos with that same length (K+ 1) , i.e., M=K.
Since the DM process operates within the latent space Z,
we use the encoder Eto map the given starting frame x0
into the latent representation z0. Additionally, we denote
z0=z0
0to specify that the latent is clean and corresponds
9018
to diffusion step 0 of the DM. Note that each reverse de-
noising step in Eq. (2) from ˆzttoˆzt−1depends solely on
ˆzt=⟨ˆz0
t,ˆz1
t, . . . , ˆzK
t⟩. To ensure that the first frame of the
final synthesized clean video latent ˆz0=⟨ˆz0
0,ˆz1
0, . . . , ˆzK
0⟩
at step 0 matches the provided image latent, i.e., ˆz0
0=z0
0,
we can modify the first generated latent ˆz0
tofˆztat each re-
verse step, as long as the signal-to-noise ratio of each frame
latent in ˆztremains consistent. Using Eq. (3), we can add t
steps of noise to the provided image latent z0
0, allowing us to
sample z0
tthrough a single-step calculation. By replacing
the first generated latent ˆz0
twith the noisy image latent z0
tat
each reverse denoising step, we might expect that the video
generation process can be guided by z0
0with the following
expressions defined for each reverse step:
z0
t∼ N(√
¯αtz0
0,(1−¯αt)I), (6a)
ˆz0
t←z0
t, (6b)
ˆzt−1∼ N(µθ(ˆzt, y), σ2
tI). (6c)
Specifically, in each reverse step from ˆzttoˆzt−1, as shown
in Eq. (6a), we first compute the noisy latent z0
tby adding
Gaussian noise to the given image latent z0
0overtsteps.
Then, we replace the first latent ˆz0
tofˆztwithz0
tin Eq. (6b)
to incorporate the provided image into the generation pro-
cess. Finally, in Eq. (6c), we pass ˆztthrough the denoising
network to generate ˆzt−1, where the text yis integrated by
classifier-free guidance (Eq. (5)). After Titerations, the fi-
nal clean latent ˆz0at diffusion step 0 can be mapped back
into the image space Xusing the decoder D.
Using this replacing-based baseline, we might expect
that the temporal attention layers in ϵθcan utilize the con-
text provided by the first frame latent ˆz0
tto generate the sub-
sequent frame latents in a manner that harmonizes with ˆz0
t.
However, as shown in Fig. 3, row 2, this replacing-based ap-
proach fails to produce a video that is temporally consistent
with the first image. The generated frames are consistent
with each other, but not with the provided first frame.
To analyze possible reasons for failure, we apply this
baseline to a simpler video infilling task, where every other
frame is provided and the model needs to predict the inter-
spersed frames. In this case, the baseline replaces the gener-
ated frame latents at positions corresponding to real frames
with noisy provided-frame latents in each reverse step. The
resulting video, in Fig. 3, row 3, looks like a combination
of two independent videos: the generated (even) frames are
consistent with each other but not with the provided (odd)
frames. We speculate that this may result from the intrinsic
dissimilarity between frame latents derived from the given
real images and those sampled from ϵθ. Thus, the tempo-
ral attention values between frame latents sampled in the
same way (both from the given images or both from ϵθ) will
be higher, while the attention values between frame latents
sampled in different ways (one from the given image and the
other from ϵθ) will be lower. Therefore, the temporal atten-
tion layers of Mtend to utilize the information from latents
Ground Truth
w/o InversionDDIM=10Resample=0
w/ InversionDDIM=10Resample=0
w/ InversionDDIM=50Resample=0
w/ InversionDDIM=10Resample=2
w/ InversionDDIM=10Resample=4“A woman with the expression of slight sadness on her face.”Figure 4. Qualitative ablation study comparing different sampling
strategies for our TI2V-Zero on MUG. The first image ˆx0is high-
lighted with the red box and text yis shown under the block. The
1st, 6th, 11th, and 16th frames of the videos are shown in each col-
umn. The terms Inversion ,DDIM , andResample denote the
application of DDPM inversion, the steps using DDIM sampling,
and the iteration number using resampling, respectively.
produced by ϵθto synthesize new frames at each reverse
step, ignoring the provided frames. We further simplify the
task to single-frame prediction, where the model only needs
to predict a single frame when all the other frames in the
video are given. In this setting, all the frame latents except
for the final frame are replaced by noisy provided-frame la-
tents in each reverse step. Thus, temporal attention layers
can only use information from the real frames. In this case,
Fig. 3, row 4, shows that the baseline can now generate a
final frame that is consistent with the previous frames.
Repeat-and-Slide Strategy. Inspired by the observa-
tion in Fig. 3, to guarantee that the temporal attention lay-
ers ofMdepend solely on the given image, we make two
major changes to the proposed replacing-based baseline:
(1) instead of using Mto directly synthesize the entire
(K+ 1) -frame video, we switch to a frame-by-frame gen-
eration approach, i.e., we generate only one new frame la-
tent in each complete DM sampling process; (2) for each
sampling process generating the new frame latent, we en-
sure that only one frame latent is produced from ϵθ, while
the other Kframe latents are derived from the given real
image and previously synthesized frames, thereby forcing
temporal attention layers to only use the information from
these frame latents. Specifically, we construct a queue of
9019
Kframe latents, denoted as s0=⟨s0
0, s1
0,···, sK−1
0⟩. We
also define st=⟨s0
t, s1
t,···, sK−1
t⟩, which is obtained by
adding tsteps of Gaussian noise to the clean s0. Similar to
our replacing-based baseline in the single-frame prediction
task, in each reverse step from ˆzttoˆzt−1, we replace the
firstKframe latents in ˆztbyst. Consequently, the tem-
poral attention layers have to utilize information from s0
to synthesize the new frame’s latent, ˆzK
0. Considering that
only one starting image latent z0is provided, we propose a
“repeat-and-slide” strategy to construct s0. At the begin-
ning of video generation, we repeat z0forKframes to
forms0, and gradually perform a sliding operation within
the queue s0by dequeuing the first frame latent s0
0and en-
queuing the newly generated latent ˆzK
0after each complete
DM sampling process. Note that though the initial s0is cre-
ated by repeating z0, the noise added to get stis different
for each frame’s latent in st, thus ensuring diversity. The
following expressions define one reverse step in the DM
sampling process:
st∼ N(√
¯αts0,(1−¯αt)I), (7a)
⟨ˆz0
t,ˆz1
t,···,ˆzK−1
t⟩ ←st, (7b)
ˆzt−1∼ N(µθ(ˆzt, y), σ2
tI). (7c)
Specifically, in each reverse denoising step from ˆzttoˆzt−1,
we first add tsteps of Gaussian noise to the queue s0to yield
stin Eq. (7a). Subsequently, we replace the previous K
frames of ˆztwithstin Eq. (7b) and input ˆztto the denoising
network to produce the less noisy latent ˆzt−1(Eq. (7c)).
With the repeat-and-slide strategy, model Mis tasked
with predicting only one new frame, while the preceding K
frames are incorporated into the reverse process to ensure
that the temporal attention layers depend solely on informa-
tion derived from the provided image.
DDPM-based Inversion. Though the DM sampling
process starting with randomly sampled Gaussian noise
produces matching semantics, the generated video is often
temporally inconsistent (Fig. 4, row 2). To provide initial
noise that can produce more temporally consistent results,
we introduce an inversion strategy based on the DDPM
[20] forward process when generating the new frame latent.
Specifically, at the beginning of each DM sampling process
to synthesize the new frame latent ˆzK
0, instead of starting
with the ˆzTrandomly sampled from N(0,I), we add Tfull
steps of Gaussian noise to s0to obtain sTusing Eq. (3).
Note that ˆzhasK+ 1frames, while shasKframes. We
then use sTto initialize the first Kframes of ˆzT. We copy
the last frame sK−1
T ofsTto initialize the final frame ˆzK
T,
as the (K−1)th frame is the closest to the Kth frame.
Resampling. Similar to [24, 33], we further apply a re-
sampling technique, which was initially designed for the
image inpainting task, to the video DM to enhance motion
coherence. Particularly, after performing a one-step denois-
ing operation in the reversed process, we add one-step noise
again to revert the latent. This procedure is repeated mul-Inversion DDIM Resample FVD↓ sFVD↓ tFVD↓
✗ 10 0 1656.37 2074.77±411.74 1798.05±235.34
✓ 10 0 339.89 443.97±139.10 405.22±61.58
✓ 50 0 463.55 581.32±234.09 535.06±85.27
✓ 10 2 207.62 299.14±87.24 278.73±47.84
✓ 10 4 180.09 267.17±74.72 252.77±39.02
Table 1. Quantitative ablation study comparing different sam-
pling strategies for proposed TI2V-Zero on the MUG dataset.
Inversion ,DDIM , and Resample denote the application of
DDPM-based inversion, the steps using DDIM sampling, and the
iteration number using resampling, respectively.
Distributions for Comparison FVD↓ tFVD↓
TI2V-Zero-Fake vs.ModelScopeT2V 366.41 921.31±251.85
TI2V-Zero-Real vs.Real Videos 477.19 1306.75±271.82
ModelScopeT2V vs.Real Videos 985.82 2264.08±501.28
TI2V-Zero-Fake vs.Real Videos 937.11 2177.70±436.71
Table 2. Result analysis of TI2V-Zero starting from the real (i.e.,
TI2V-Zero-Real) or synthesized frames (i.e., TI2V-Zero-Fake) on
the UCF101 dataset.
tiple times for each diffusion step, ensuring harmonization
between the predicted and conditioning frame latents (see
Algorithm 1 for details).
4. Experiments
4.1. Datasets and Metrics
We conduct comprehensive experiments on three datasets.
More details about datasets, such as selected subjects and
text prompts, can be found in our Supplementary Materials.
MUG facial expression dataset [1] contains 1,009
videos of 52 subjects performing 7 different expres-
sions. We include this dataset to evaluate the perfor-
mance of models in scenarios with small motion and a
simple, unchanged background. To simplify the exper-
iments, we randomly select 5 male and 5 female sub-
jects, and 4 expressions. We use the text prompt tem-
plates like “ A woman with the expression of
slight {label }on her face. ” to change the ex-
pression class label to be text input. Since the expressions
shown in the videos of MUG are often not obvious, we add
“slight” in the text input to avoid large motion.
UCF101 action recognition dataset [56] contains 13,320
videos from 101 human action classes. We include this
dataset to measure performance under complicated motion
and complex, changing backgrounds. To simplify the exper-
iments, we select 10 action classes and the first 10 subjects
within each class. We use text prompt templates such as
“A person is performing {label }.” to change
the class label to text input.
In addition to the above two datasets, we create an
OPEN dataset to assess the model’s performance in open-
domain TI2V generation. We first utilize ChatGPT [43] to
generate 10 text prompts. Subsequently, we employ Stable
9020
“A woman with the expression of slight anger on her face.” (MUG)Ground TruthTI2V-Zerow/o Resample (Ours)DynamiCrafter
“A person is kayaking.” (UCF101)
“A romantic gondola ride through the canals of Venice at sunset.” (OPEN)
TI2V-Zerow/ Resample (Ours)
Figure 5. Qualitative comparison among different methods on multiple datasets for TI2V generation. Columns in each block display the
1st, 6th, 11th, and 16th frames of the output videos, respectively. There are 16 frames with a resolution of 256×256for each video. The
given image x0is highlighted with the red box and the text prompt yis shown under each block.
ModelMUG UCF101
FVD↓ sFVD↓ tFVD↓ FVD↓ tFVD↓
DynamiCrafter [67] 1094.72 1359.86±257.73 1223.89±105.94 589.59 1540.02±199.59
TI2V-Zero w/o Resample (Ours) 339.89 443.97±139.10 405.22±61.58 493.19 1319.77±283.87
TI2V-Zero w/ Resample (Ours) 180.09 267.17±74.72 252.77±39.02 477.19 1306.75±271.82
Table 3. Quantitative comparison among different methods on multiple datasets for TI2V generation.
Diffusion 1.5 [47] to synthesize 100 images from each text
prompt, generating a total of 1,000 starting images and 10
text prompts for evaluating TI2V models.
Data Preprocessing. We resize all the videos/images to
256×256resolution. For UCF101, since most of the video
frames are not square, we crop the central part of the frames.
To obtain ground truth videos for computing metrics, we
uniformly sample 16 frames from each video in the datasets
to generate the video clips with a fixed length.
Metrics. Following prior work [21, 22, 25], we assess
thevisual quality ,temporal coherence , and sample diversity
of generated videos using Fr ´echet Video Distance ( FVD )
[57]. Similar to Fr ´echet Inception Distance (FID) [18],
which is used for image quality evaluation, FVD utilizes a
video classification network I3D [6] pretrained on Kinetics-
400 dataset [26] to extract feature representation of real and
synthesized videos. Then it calculates the Fr ´echet distance
between the distributions of the real and synthesized video
features. To measure how well a generated video aligns
with the text prompt y(condition accuracy ) and the given
image x0(subject relevance ), following [39], we design two
variants of FVD, namely text-conditioned FVD ( tFVD ) and
subject-conditioned FVD ( sFVD ). tFVD and sFVD com-
pare the distance between real and synthesized video fea-
ture distributions under the same text yor the same subject
image x0, respectively. We first compute tFVD and sFVD
for each condition yand image x0, then report their meanand variance as final results. In our experiments, we gen-
erate 1,000 videos for all the models to estimate the fea-
ture distributions. We compute both tFVD and sFVD on
the MUG dataset, but for UCF101, we only consider tFVD
since it doesn’t contain videos of different actions for the
same subject. For the OPEN dataset, we only present quali-
tative results due to the lack of ground truth videos. Unless
otherwise specified, all the generated videos are 16 frames
(i.e.,M= 15 ) with resolution 256×256.
4.2. Implementation Details
Model Implementation. We take the ModelScopeT2V
1.4.2 [60] as basis and implement our modifications. For
text-conditioned generation, we employ classifier-free guid-
ance with g= 9.0in Eq. (5). Determined by our prelimi-
nary experiments, we choose 10-step DDIM and 4-step re-
sampling as the default setting for MUG and OPEN, and
50-step DDIM and 2-step resampling for UCF101.
Implementation of SOTA Model. We compare our
TI2V-Zero with a state-of-the-art (SOTA) model Dynami-
Crafter , a recent open-domain TI2V framework [67]. Dy-
namiCrafter is based on a large-scale pretrained T2V foun-
dation model VideoCrafter1 [16]. It introduces a learnable
projection network to enable image-conditioned generation
and then fine-tunes the entire framework. We implement
DynamiCrafter using their provided code with their default
settings. For a fair comparison, all the generated videos are
9021
!𝑥)*!𝑥+,!𝑥*+!𝑥-.!𝑥,*!𝑥/,!𝑥))+!𝑥)+.𝑥0“A mesmerizing display of the northern lights in the Arctic.”
!𝑥10Figure 6. Example of long video generation using our TI2V-Zero
on the OPEN dataset. The given image x0is highlighted with a red
box, and the text prompt yis shown under the set of frames. There
are a total of 128 video frames ( M= 127 ), and the synthesized
results for every 14 frames are presented.
centrally-cropped and resized to 256×256.
4.3. Result Analysis
Ablation Study. We conduct ablation study of different
sampling strategies on MUG. As shown in Tab. 1 and Fig. 4,
compared with generating using randomly sampled Gaus-
sian noise, initializing the input noise with DDPM inversion
is important for generating temporally continuous videos,
improving all of the metrics dramatically. For MUG, in-
creasing the DDIM sampling steps from 10 to 50 does not
enhance the video quality but requires more inference time.
Thus, we choose 10-step DDIM as the default setting on
MUG. As shown in Fig. 4 and Tab. 1, adding resampling
helps preserve identity details ( e.g., hairstyle and facial ap-
pearance), resulting in lower FVD scores. Increasing re-
sampling steps from 2 to 4 further improves FVD scores.
Effect of Real/Synthesized Starting Frames. We also
explore the effect of video generation starting with real
or synthesized frames on UCF101. We initially use the
first frame of the real videos to generate videos with our
TI2V-Zero, termed TI2V-Zero-Real. Additionally, we uti-
lize the backbone model ModelScopeT2V [60] to generate
synthetic videos using the text inputs of UCF101. We then
employ TI2V-Zero to create videos from the first frame of
the generated fake videos, denoted as TI2V-Zero-Fake. As
shown in Tab. 2, [TI2V-Zero-Fake vs.ModelScopeT2V]
can achieve better FVD scores than [TI2V-Zero-Real vs.
Real Videos]. The reason may be that frames generated
by ModelScopeT2V can be considered as in-distribution
data since TI2V-Zero is built upon it. We also compare
the output video distribution of TI2V-Zero-Fake and Mod-
elScopeT2V with real videos in Tab. 2. Though starting
from the same synthesized frames, TI2V-Zero-Fake can
generate more realistic videos than the backbone model.
Comparison with SOTA Model. We compare our pro-
posed TI2V-Zero with DynamiCrafter [67] in Tab. 3 and
Fig. 5. From Fig. 5, one can find that DynamiCrafterstruggles to preserve details from the given image, and the
motion of its generated videos is also less diverse. Note
that DynamiCrafter requires additional fine-tuning to enable
TI2V generation. In contrast, without using any fine-tuning
or introducing external modules, our proposed TI2V-Zero
can precisely start with the given image and output more
visually-pleasing results, thus achieving much better FVD
scores on both MUG and UCF101 datasets in Tab. 3. The
comparison between our TI2V-Zero models with and with-
out using resampling in Fig. 5 and Tab. 3 also demonstrates
the effectiveness of using resampling, which can help main-
tain identity and background details.
Extension to Other Applications. TI2V-Zero can also
be extended to other tasks as long as we can construct s0
withKimages at the beginning. These images can be ob-
tained either from ground truth videos or by applying the
repeating operation. Then we can slide s0when generating
the subsequent frames. We have applied TI2V-Zero in video
infilling (see the last row in Fig. 3), video prediction (see
Supplementary Materials), and long video generation (see
Fig. 6). As shown in Fig. 6, when generating a 128-frame
video on the OPEN dataset, our method can preserve the
mountain shape in the background, even at the 71st frame
(frame ˆx70). The generated video examples and additional
experimental results are in our Supplementary Materials.
5. Conclusion
In this paper, we propose a zero-shot text-conditioned
image-to-video framework, TI2V-Zero, to generate videos
by modulating the sampling process of a pretrained video
diffusion model without any optimization or fine-tuning.
Comprehensive experiments show that TI2V-Zero can
achieve promising performance on multiple datasets.
While showing impressive potential, our proposed TI2V-
Zero still has some limitations. First, as TI2V-Zero relies on
a pretrained T2V diffusion model, the generation quality of
TI2V-Zero is constrained by the capabilities and limitations
of the pretrained T2V model. We plan to extend our method
to more powerful video diffusion foundation models in the
future. Second, our method sometimes generates videos
that are blurry or contain flickering artifacts. One possible
solution is to apply post-processing methods such as blind
video deflickering [30] or image/video deblurring [51] to
enhance the quality of final output videos or the newly syn-
thesized frame in each generation. Finally, compared with
GAN and standard video diffusion models, our approach is
considerably slower because it requires running the entire
diffusion process for each frame generation. We will in-
vestigate some faster sampling methods [29, 32] to reduce
generation time.
9022
References
[1] Niki Aifanti, Christos Papachristou, and Anastasios De-
lopoulos. The mug facial expression database. In 11th In-
ternational Workshop on Image Analysis for Multimedia In-
teractive Services WIAMIS 10 , pages 1–4. IEEE, 2010. 2,
6
[2] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In IEEE International Conference on
Computer Vision , 2021. 2
[3] Dmitry Baranchuk, Ivan Rubachev, Andrey V oynov,
Valentin Khrulkov, and Artem Babenko. Label-efficient se-
mantic segmentation with diffusion models. arXiv preprint
arXiv:2112.03126 , 2021. 2
[4] Andreas Blattmann, Timo Milbich, Michael Dorkenwald,
and Bjorn Ommer. Understanding object dynamics for in-
teractive image-to-video synthesis. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5171–5181, 2021. 2
[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
22563–22575, 2023. 2
[6] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 6299–6308, 2017. 7
[7] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A
Efros. Everybody dance now. In Proceedings of the
IEEE/CVF international conference on computer vision ,
pages 5933–5942, 2019. 2
[8] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,
Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,
Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open
diffusion models for high-quality video generation. arXiv
preprint arXiv:2310.19512 , 2023. 2
[9] ¨Ozg¨un C ¸ ic ¸ek, Ahmed Abdulkadir, Soeren S Lienkamp,
Thomas Brox, and Olaf Ronneberger. 3d u-net: learning
dense volumetric segmentation from sparse annotation. In
International conference on medical image computing and
computer-assisted intervention , pages 424–432. Springer,
2016. 4
[10] Michael Dorkenwald, Timo Milbich, Andreas Blattmann,
Robin Rombach, Konstantinos G Derpanis, and Bjorn Om-
mer. Stochastic image-to-video synthesis using cinns. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 3742–3753, 2021. 2
[11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 12873–12883, 2021. 3
[12] Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-
Chyi Su, William Yang Wang, and Sean Bell. Tell me
what happened: Unifying text-guided video completion via
multimodal masked video generation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10681–10692, 2023. 1, 2
[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618 , 2022. 2
[14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725 , 2023. 1, 2
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 3
[16] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
Qifeng Chen. Latent video diffusion models for high-fidelity
video generation with arbitrary lengths. arXiv preprint
arXiv:2211.13221 , 2022. 2, 7
[17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 2
[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 7
[19] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications , 2021. 3
[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840–6851, 2020. 2, 3, 6
[21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 2, 7
[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William
Chan, Mohammad Norouzi, and David J Fleet. Video dif-
fusion models. arXiv preprint arXiv:2204.03458 , 2022. 2,
7
[23] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,
and Jie Tang. Cogvideo: Large-scale pretraining for
text-to-video generation via transformers. arXiv preprint
arXiv:2205.15868 , 2022. 2
[24] Tobias H ¨oppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,
and Andrea Dittadi. Diffusion models for video prediction
and infilling. arXiv preprint arXiv:2206.07696 , 2022. 6
[25] Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move:
controllable image-to-video generation with text descrip-
tions. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 18219–18228,
2022. 1, 2, 7
[26] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
9023
Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-
man action video dataset. arXiv preprint arXiv:1705.06950 ,
2017. 7
[27] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan, Roberto Henschel, Zhangyang Wang, Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-
image diffusion models are zero-shot video generators. arXiv
preprint arXiv:2303.13439 , 2023. 2
[28] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 3
[29] Zhifeng Kong and Wei Ping. On fast sampling of diffu-
sion probabilistic models. arXiv preprint arXiv:2106.00132 ,
2021. 8
[30] Chenyang Lei, Xuanchi Ren, Zhaoxiang Zhang, and Qifeng
Chen. Blind video deflickering by neural filtering with a
flawed atlas. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 10439–
10448, 2023. 8
[31] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and
Lawrence Carin. Video generation from text. In Proceedings
of the AAAI conference on artificial intelligence , 2018. 2
[32] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffu-
sion probabilistic model sampling in around 10 steps. arXiv
preprint arXiv:2206.00927 , 2022. 8
[33] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11461–11471, 2022. 2, 6
[34] Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable
animation of fluid elements in still images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3667–3676, 2022. 2
[35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. arXiv preprint arXiv:2108.01073 , 2021. 2
[36] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,
and Daniel Cohen-Or. Null-text inversion for editing real
images using guided diffusion models. arXiv preprint
arXiv:2211.09794 , 2022.
[37] Nithin Gopalakrishnan Nair, Anoop Cherian, Suhas Lohit,
Ye Wang, Toshiaki Koike-Akino, Vishal M Patel, and Tim K
Marks. Steered diffusion: A generalized framework for plug-
and-play conditional image synthesis. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 20850–20860, 2023. 2
[38] Haomiao Ni, Yihao Liu, Sharon X Huang, and Yuan Xue.
Cross-identity video motion retargeting with joint transfor-
mation and synthesis. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision , pages
412–422, 2023. 2
[39] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and
Martin Renqiang Min. Conditional image-to-video gener-
ation with latent flow diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 18444–18455, 2023. 1, 2, 7[40] Haomiao Ni, Jiachen Liu, Yuan Xue, and Sharon X Huang.
3d-aware talking-head video motion transfer. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision , pages 4954–4964, 2024. 2
[41] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 3
[42] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In International
Conference on Machine Learning , pages 8162–8171. PMLR,
2021. 2
[43] OpenAI. Openai: Introducing chatgpt. URL https://
openai.com/blog/chatgpt , 2022. 6
[44] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad-
wongsa, and Supasorn Suwajanakorn. Diffusion autoen-
coders: Toward a meaningful and decodable representation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 10619–10629, 2022.
2
[45] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,
Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-
ing attentions for zero-shot text-based video editing. arXiv
preprint arXiv:2303.09535 , 2023. 2
[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 4
[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684–10695, 2022. 2, 3, 4, 7
[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention , pages 234–241.
Springer, 2015. 3
[49] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. arXiv preprint arXiv:2208.12242 , 2022. 2
[50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487 , 2022. 2
[51] Siddhant Sahu, Manoj Kumar Lenka, and Pankaj Kumar
Sa. Blind deblurring using deep learning: A survey. arXiv
preprint arXiv:1907.10128 , 2019. 8
[52] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
9024
Open dataset of clip-filtered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114 , 2021. 2
[53] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792 ,
2022. 2
[54] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning , pages 2256–2265. PMLR, 2015.
2, 3
[55] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Advances in Neural
Information Processing Systems , 32, 2019. 2, 3
[56] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
Ucf101: A dataset of 101 human actions classes from videos
in the wild. arXiv preprint arXiv:1212.0402 , 2012. 2, 6
[57] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach,
Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To-
wards accurate generative models of video: A new metric &
challenges. arXiv preprint arXiv:1812.01717 , 2018. 7
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 3
[59] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-
dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi
Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.
Phenaki: Variable length video generation from open domain
textual description. arXiv preprint arXiv:2210.02399 , 2022.
1
[60] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,
Xiang Wang, and Shiwei Zhang. Modelscope text-to-video
technical report. arXiv preprint arXiv:2308.06571 , 2023. 1,
2, 3, 7, 8
[61] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,
Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-
video synthesis. arXiv preprint arXiv:1808.06601 , 2018. 2
[62] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao,
Xinlong Wang, and Chunhua Shen. Zero-shot video editing
using off-the-shelf image diffusion models. arXiv preprint
arXiv:2303.17599 , 2023. 2
[63] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,
Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,
and Jingren Zhou. Videocomposer: Compositional video
synthesis with motion controllability. arXiv preprint
arXiv:2306.02018 , 2023. 1, 2
[64] Yaohui Wang, Di Yang, Francois Bremond, and Antitza
Dantcheva. Latent image animator: Learning to ani-
mate images via latent space navigation. arXiv preprint
arXiv:2203.09043 , 2022. 2
[65] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,
Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-
erating open-domain videos from natural descriptions. arXiv
preprint arXiv:2104.14806 , 2021. 2
[66] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, XiaohuQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning
of image diffusion models for text-to-video generation. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7623–7633, 2023. 2
[67] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xin-
tao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter:
Animating open-domain images with video diffusion priors.
arXiv preprint arXiv:2310.12190 , 2023. 1, 2, 7, 8
[68] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-
long Wang, and Shalini De Mello. Open-vocabulary panop-
tic segmentation with text-to-image diffusion models. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2955–2966, 2023. 2
[69] Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping
Shi, and Dahua Lin. Pose guided human video generation.
InProceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 201–216, 2018. 2
[70] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang
Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained
control in video generation by integrating text, image, and
trajectory. arXiv preprint arXiv:2308.08089 , 2023. 1, 2
9025
