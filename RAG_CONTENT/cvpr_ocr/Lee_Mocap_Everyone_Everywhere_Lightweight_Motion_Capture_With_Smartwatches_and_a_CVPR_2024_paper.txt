Mocap Everyone Everywhere:
Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera
Jiye Lee
Seoul National University
kay2353@snu.ac.krHanbyul Joo
Seoul National University
hbjoo@snu.ac.kr
Figure 1. We present a lightweight and affordable motion capture method based on two smartwatches and a head-mounted camera.
Abstract
We present a lightweight and affordable motion capture
method based on two smartwatches and a head-mounted
camera. In contrast to the existing approaches that use six
or more expert-level IMU devices, our approach is much
more cost-effective and convenient. Our method can make
wearable motion capture accessible to everyone every-
where , enabling 3D full-body motion capture in diverse en-
vironments. As a key idea to overcome the extreme spar-
sity and ambiguities of sensor inputs with different modali-
ties, we integrate 6D head poses obtained from the head-
mounted cameras for motion estimation. To enable cap-
ture in expansive indoor and outdoor scenes, we propose
an algorithm to track and update floor level changes to de-
fine head poses, coupled with a multi-stage Transformer-
based regression module. We also introduce novel strate-
gies leveraging visual cues of egocentric images to further
enhance the motion capture quality while reducing ambi-
guities. We demonstrate the performance of our method on
various challenging scenarios, including complex outdoor
environments and everyday motions including object inter-
actions and social interactions among multiple individuals.
1. Introduction
Human motion encapsulates the diverse and rich stories
of human life in real-world situations. Thus, it is essential
Project Page: https://jiyewise.github.io/projects/MocapEveryfor machines not only to decipher the subtle nuances within
these movements but also to authentically mimic them for
seamless interaction and assistance with humans. However,
the advancement in the research to make machines fully
understand human motions lags considerably behind other
AI domains focused on images or languages. The primary
obstacle lies in the fact that 3D human motion datasets
that reflect real-world scenarios remain exceedingly scarce,
compared to the abundance of massive image or language
datasets that capture diverse aspects of human life. This
scarcity stems from the inherent challenge of acquiring mo-
tion capture data, necessitating specialized equipment and
tools that are not readily available to the public. Despite
notable efforts in collecting and refining motion capture
datasets that are actively used in learning-based approaches
(e.g., [33, 45]), the scale of these datasets remains in orders
of magnitude smaller than the vast repositories of Internet
images and language datasets.
In our endeavor to democratize motion capture for the
general public free from the constraints of expert-level
equipment and intricate settings, we present a lightweight
and affordable motion capture method that utilizes Iner-
tial Measurement Unit (IMU) sensors of smartwatches on
wrists and a head-mounted camera. Our method can make
wearable motion capture accessible to everyone, given the
widespread availability and popularity of smartwatches, as
well as the existence of action cameras and camera glasses
(e.g., GoPro or Smart Glasses [2, 3] equipped with cam-
eras). Our system can be applied for large-scale and long-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1091
term motion captures without location constraints (indoors
or outdoors). To this end, our method can enable the explo-
ration of new research areas, such as social interaction sce-
narios involving multiple participants, each equipped with
our lightweight capture setup.
Going beyond the traditional optical motion capture
methods [5, 24], primarily feasible in well-set lab environ-
ments with limited scope and accessibility, there have been
explorations to capture human motions with wearable sen-
sors. Existing commercial solutions [4, 6] require attaching
IMU sensors to all major body segments (17 to 19) using
specialized suits or straps. While recent start-of-the-art ap-
proaches [23,55,56] have reduced the necessity to 6 sensors,
it is still challenging for the public to equip themselves with
6 expert-level IMU sensors. Moreover, attaching sensors to
the pelvis and legs may compromise usability. Due to the
rise of VR/AR technologies and hardware, another research
direction [9, 12, 22, 28, 51] utilize head-mounted displays
(HMDs) [1] and hand-held controllers, which provide 6DoF
(3D rotation and translation) information of the head and
wrist movements, to reconstruct full-body motions. Using
only three upper body sensors may improve usability, but
this reliance on specialized devices restricts motion capture
to indoor settings optimized for VR/AR equipment, thus
limiting its broader applicability.
In this paper, we present a motion capture method that
utilizes IMU sensors of smartwatches on wrists and a head-
mounted monocular camera. We overcome the sparsity of
using only 2 sensors and the intrinsic ambiguities of IMU
sensors (orientation, acceleration) by integrating head 6DoF
poses obtained from the head-mounted camera into the mo-
tion estimation pipeline. In contrast to VR settings (e.g.,
HMDs) where head poses are given in a fixed world coordi-
nate in a small indoor environment [9,12,22,28,51], it is not
trivial to define head poses in expansive outdoor settings.
Through an algorithm that automatically tracks and adjusts
floor levels, our approach can robustly capture motion in
“in-the-wild” scenarios in challenging locations with non-
flat grounds such as hilly areas or stairs. Furthermore, we
also present a motion optimization approach for enhancing
capture quality in complex everyday motions, such as object
interactions, by leveraging observed body part cues through
the head-mounted camera. As our system can be conve-
niently equipped by multiple users, we also explore scenar-
ios where the signals among the individuals are shared and
the egocentric observations from one person can be used as
sparse third-person views for the others, which can be addi-
tionally used for our motion optimization module.
Our contribution can be summarized as follows: (1) the
first method to capture high-quality 3D full-body motion
from a head-mounted camera and two smartwatches on
wrists. Notably, we demonstrate our method on commonly
available smartwatches rather than expert-level IMU sen-sors; (2) a novel algorithm to track and update floor levels
coupled with a multi-stage Transformer-based motion esti-
mation module, which enables capture in expansive indoor
and outdoor settings; (3) a novel motion optimization mod-
ule that utilizes visual information captured by monocular
egocentric cameras.
2. Related Work
Motion Capture with IMU Sensors. Traditional motion
capture mainly relied on optical methodologies such as
optical markers [5] or markerless capture in a multi-view
camera setup [24, 30, 59]. Although such methods demon-
strate high-quality capture, they suffer from occlusions and
are only applicable in specific settings with cameras. To
mitigate such limitations there have been methods to fuse
inertial sensor data to optical motion capture [14, 17, 25,
26, 34, 35, 39, 44, 46, 47]. Wearable motion capture with
IMU sensors offers the advantage of freedom from loca-
tion constraints and occlusions. Commercial methods [4, 6]
leverage 17 to 19 IMU sensors, necessitating tight suits
or straps with densely packed sensors. More recent ap-
proaches introduce more lightweight solutions by reduc-
ing the number of sensors up to 6, posed on the pelvis,
wrists, legs, and head [19,23,48,54–56]. SIP [48] introduces
an optimization-based approach and DIP [19] presents a
deep learning-based method from 6 sensors using bidirec-
tional RNNs. TransPose [56] extends this work by including
global root translation estimation based on foot contact de-
tection. More recently, PIP [55] and TIP [23] demonstrate
state-of-the-art performance by combining physics-based
optimization with an RNN-based kinematic approach [55]
or combining body contact estimation and terrain genera-
tion with Transformer decoders [23]. To address the inher-
ent root drift issue in IMU-based methods, there have been
approaches that additionally utilize camera positions from
head-mounted cameras to enhance the accuracy of root
translations [16, 54]. In these methods, the camera poses
are often used as auxiliary cues to post-process root trans-
lation, while human pose estimation is still based on full-
body IMU sensor setup (17 for [16] or 6 for [54]). Typically,
previous methods are demonstrated using expert-level de-
vices [6]. Different from previous approaches, our method
uses two smartwatches and integrates a head-mounted cam-
era into the body pose estimation pipeline.
Motion Capture with Egocentric Videos. Pose estimation
from egocentric videos is receiving increased attention in
recent studies. Several methods exploit fish-eye cameras
by leveraging their advantages in visibility for egocentric-
pose estimation [21, 42, 43, 49, 50, 52]. Cha et al. [10] ex-
tend these methods by fusing full-body IMU sensors with
fish-eye camera input. A few approaches pursue more chal-
lenging scenarios by estimating human pose with body-
mounted cameras without other sensors. Shiratori et al. [40]
1092
use structure-from-motion (SfM) to determine body poses
from body-mounted cameras, and Jiang et al. [20] exploit
motion graphs for estimation from a single chest-mounted
camera. Other methods [57, 58] combine kinematics and
physics-based approaches to estimate physically plausible
poses, which is extended by [32, 36] to simple scene inter-
actions. You2Me [38] estimates pose under social interac-
tion scenarios where the interaction target’s pose is visible
in the egocentric video. Apart from these methods that di-
rectly learn from image inputs, EgoEgo [29] estimates mo-
tions from an egocentric video alone by introducing head
poses as intermediate representations.
Motion Capture with VR-Based Upper Body Sensors.
Recent advances in VR/AR technologies have led to the
emergence of new devices that provide 6DoF information
on head and wrist movements. Consequently, research has
been done on controlling virtual avatars through such de-
vices [7–9, 11, 12, 22, 28, 51, 53]. As determining full body
motion from upper body sensors is challenging, earlier
methods [11, 53] use root information explicitly by adding
sensors on the pelvis [53] or implicitly by root-relative
data representations [11]. CoolMoves [7] suggests kNN-
based search which is limited to specific action types. More
recent approaches demonstrate increased performance by
exploiting various neural network structures such as flow
models [8], Transformers [22], and diffusion models [12].
QuestSim [51] utilizes physics-based character control us-
ing deep reinforcement learning (DRL) to generate physi-
cally plausible full-body motions, which is extended by [28]
to environment interactions. It is important to note that mo-
tion estimation with VR devices fundamentally differs from
ours, as the 6DoF cues from the head and wrists are directly
provided by the system and thus rely on a specified setting.
3. Method
3.1. Overview
Our method reconstructs 3D full-body human motion
M={mt}T
t=0by taking, as inputs, an egocentric video
Iand IMU sensor signals SIMU ={R,A}(orientation,
acceleration) from two smartwatches on wrists. We denote
our system as a function F,
M=F(I,SIMU) (1)
where Fis composed of two modules FestandFopt, as
shown in Fig. 2. The egocentric video from a head-mounted
camera is a sequence of images, I={It}T
t=0where Tis
the sequence length, and It∈Rh×w×3indicates an im-
age at time t.SIMU ={R,A}is a set of IMU signal se-
quences for both the left and right wrist. R={Rt}T
t=0,
where Rt= (rleft
t,rright
t), indicates the orientation sig-
nal for each sensor at time tin angle axis, rt∈so(3).
Similarly, A={At}T
t=0, where At= (aleft
t,aright
t),represents the acceleration signals. The human motion out-
putM={mt}T
t=0is a sequence of pose mt, andmt=
(p0
t,q0
t,q1
t, ...,qJ
t)∈R3J+3is a concatenated vector of
root translation p0∈R3defined in a world coordinate and
local joint rotations of Jjoints where the rotation of j-th
joint is represented as qj∈so(3).
We first apply an off-the-shelf monocular SLAM [41]
with the egocentric video I. Given the original world co-
ordinate arbitrarily defined from the SLAM model, we re-
define the world coordinate via an alignment procedure de-
scribed in Sec. 3.2, such that the scale of the world coordi-
nate reflects the metric scale (in meter) and the negative z-
axis is aligned to the gravity direction. From the SLAM, we
obtain the 3D pointcloud Wof the environment and cam-
era pose at each time C={Ct}T
t=0, defined in the world
coordinate. Then, given the camera poses C, we compute
the head pose H={Ht}T
t=0, where Ht∈SE(3)is the lo-
cation and orientation of the head joint. The head poses are
directly leveraged into the estimation pipeline with sensor
inputs, playing a key role in disambiguating motions dur-
ing estimation. Notably, in non-flat spaces as in Fig. 3, the
z-directional component of Htdefined in the world coor-
dinate does not necessarily reflect the metric height of the
person because the height should be defined from the actual
floor the person is currently standing. Thus, we compute
the height of the head Htby tracking and updating the floor
levelft∈Rat time taccordingly.
We build a transformer-based regression model coupled
with a floor-level update module to estimate the full-body
human motion from the cues of estimated head pose and
wrist IMU signals: Mreg=Fest(H,W,SIMU). While
our initial full-body estimation Mregis already compelling,
there exist fundamental ambiguities due to the extreme spar-
sity of our input signals. To address this issue, we leverage
the visual cues captured by the head-mounted view I, where
the hand position and the interactions with the environments
are observed. While available only occasionally, we demon-
strate the use of such visual cues can enhance the motion
capture quality. Going one step further, we also consider
multi-people capture scenarios where each individual wears
our lightweight system. Assuming the visual signals among
people can be shared, we can additionally leverage the occa-
sional “third-person” views from other people for the target
person’s motion capture. To fuse all of these available sig-
nals for more accurate motion estimation, we build a mo-
tion optimizer module Foptwhere M=Fopt(Mreg,Φ).
Optimizing the human motion is done in aspatiotemporal
manifold space [18], where Mregis the initial output from
our motion regressor module and Φis the visual cues ex-
tracted from head-mounted camera or third-person views
from other users’ cameras. For Φ, we mainly use the 2D
keypoint cues estimated from images in egocentric scenar-
ios and 3D pose estimation results in multi-person scenar-
1093
Figure 2. System Overview
ios.
3.2. Pre-processing Sensor Inputs
Head Trajectory from Monocular SLAM. From egocen-
tric video I, we first apply DROID-SLAM [41], noted for its
accuracy and robustness compared to classical SLAM sys-
tems, to estimate camera trajectory Cand reconstruct the
3D pointcloud W. In some cases, however, outliers may
exist in camera pose estimation due to insufficient textures
in the scenes or blurs, which may negatively affect our body
pose estimation quality. As a way to filter out outliers, we
compute the temporal acceleration between camera move-
ments and detect erroneous camera pose estimation when
the acceleration values are over a certain threshold. We fill
in the missing camera poses via linear interpolation.
Aligning the Coordinate for Camera and IMU Sensors.
For preprocessing, we align the arbitrarily defined original
coordinate from SLAM into our desired real-world coordi-
nate with metric scale and the negative z-axis to be aligned
to the gravity direction. Additionally, we also calibrate the
orientation of IMUs to have the common orientation axes
aligned to the gravity direction. This is done via two steps:
(1) aligning IMU sensors to real-world coordinates, and (2)
aligning the coordinate from SLAM to IMU coordinates.
Because the camera center Cmay not be necessarily the
same as the head joint location, we compute the fixed trans-
formation Tcam
head to transform the camera pose into the head
pose, resulting in Hdefined w.r.t world coordinate. Tcam
head
is computed by approximating the camera location in a sur-
face point of SMPL mesh. Refer to the supp. mat. for the
details of the alignment protocol and time synchronization.
3.3. Motion Estimation
Pre-Processing Input Signals. The motion estimator mod-
uleFestis a transformer-based network to estimate motion
and foot contacts from head trajectory H={Ht}T
t=0and
IMU sensor signals SIMU ={R,A}. We input the data
into the network in a sliding temporal window manner with
length N. For every window, we normalize the input to be
local to the head coordinate Hτatτ= 0 which is the
first frame of the current window by applying a transfor-
mation matrix Tw
Hτthat transforms the world coordinate to
the coordinate of Hτ=0. The normalized cues are denoted
with the hat symbol: ˆH={ˆHτ}N
τ=0,ˆR={ˆRτ}N
τ=0,ˆA={ˆAτ}N
τ=0, and ˆ mτ={ˆ p0
τ,ˆ q0
τ,q1
τ, ...qJ
τ}1. The nor-
malized output motion is recovered by applying THτw, or
(Tw
Hτ)−1.
Normalizing every cue to the first frame head coordi-
nates may miss out on important absolute cues critical to
defining human motions. For example, a person standing
still and sitting still would have identical input when nor-
malized. Thus, we furthermore include absolute head height
(z-axis value) ht∈R, and the head up-vector θup
t∈R3de-
fined in world coordinate. To accurately define head height
h, we introduce an algorithm to update floor level ftat time
tbased on network output (motion and foot contact) ob-
tained until time t−1with 3D pointcloud W. Once the
ftis estimated, as described in Fig. 3, the head height ht
is computed based on the floor level, or ht=Hz−ft.
The input components are concatenated into {xτ}N
τ=0=
{ˆH,ˆR,ˆA, hτ, θup
τ} ∈RN×31. Before concatenating, rota-
tions are converted into 6D representations [60].
Network Architecture. We formulate the estimation as a
sequence-to-sequence (seq2seq) problem to effectively in-
corporate temporal information to address the ambiguities
arising from the sparsity of sensor inputs. Following the
previous work [22], we utilize Transformer encoder mod-
els and their self-attention mechanism to efficiently capture
intricate relationships in the time-series data.
Different from the previous work [22] that directly maps
input sequence {xτ}N
τ=0to the output motion ˆ mτ, we
adopt a multi-stage method as in [56] by introducing end-
effector positions as intermediate representations. Specif-
ically, we separate the system Festinto two submodules
FendandFbody(we drop the subscript ‘ est’ on these
submodule names for brevity). The first submodule Fend
takes{xτ}N
τ=0as input and generates end-effector positions
(hands and feet) {xmid
τ}N
τ=0∈RN×12. The input {xτ}and
{xmid
τ}are fed into submodule Fbody. TheFbodyregresses
the output motion {ˆ mτ}N
τ=0and also foot contact {cτ}N
τ=0
where cτ={clf
τ, crf
τ}.clf
τandcrf
τindicate the contact
probability of the left and right foot at time τ.
Both submodules are based on Transformer encoder
models with linear embedding layers to project the input
vector into continuous embeddings in the first part of the
1In the normalized output pose, the local joint rotations q1
t, ...,qJ
tis
identical to the original pose mtas local joint rotations are coordinate
invariant, while the root orientation q0
tis changed.
1094
Figure 3. (a) Visualization of input signals. (b) Visualization of the
updated floor levels ft.
networks. For Fbodywhere {xi
τ}and{xmid
τ}are concate-
nated as input, there are two separate linear layers and the
linear embeddings are concatenated and fed into the Trans-
former encoder. In submodule Fend, the features gener-
ated by the Transformer encoder are converted into mid-
representations {xmid
τ}by a 2-layer MLP. In submodule
Fbody, the output of the Transformer encoder is first con-
verted into foot contact probabilities {cτ}. The contact
probability and Transformer encoder output are concate-
nated and converted to output motion {ˆ mτ}. The network
architecture for both submodules are in Fig. 4.
Updating Floor Levels. Among the input {xi
τ}, the height
cuehrepresents the metric height of the person’s head joint,
which should be computed from the actual floor the char-
acter stands on. Assuming large-scale indoor and outdoor
environments (e.g., multiple floors with stairs, and uphills),
the actual floor the character stands on is not necessarily the
same as the z= 0 plane in the world coordinate defined
in the alignment procedure (Sec. 3.2). Therefore, the floor
level should be updated based on the current character status
and the environment.
The key idea of updating floor levels is to track the foot
contact {ct}t−1
t=0. As foot contact is a form of interaction
between the human and the floor, the foot position during
contact can be considered as the floor level. From the con-
tact estimation {ct}t−1
t=0obtained by Fbody, we find the lat-
est time frame tmwhere the foot contact occurs in either
foot with a confidence value above a certain fixed thresh-
old,cf
tm> λ. Given the corresponding foot joint location
pf
tmin contact, the floor point can be obtained by project-
ingpf
tmto the 3D scene pointcloud W, where we simply
find nearby points and take the mean of their zvalues as ft.
Visual examples of updated floors are shown in Fig. 3 (b)2.
Training. The submodules FendandFbodyare trained end-
to-end, with the following loss terms Lpos,Lrot,Lroot,
Lmid,Lcontact ,Lfootvel , andLcons. The loss term Lroot
2While we only keep the floor height ft, the 3D floor planes in Fig. 3
are shown for visualization purposes using root’s xy positions at time t.
Figure 4. Network architecture of submodules Fend(left) and
Fbody(right) of Fest.
andLrotare:
Lroot=∥ˆ p0−ˆ p0
GT∥+∥ˆ q0−ˆ q0
GT∥
Lrot=X
j∈J∥qj−qj
GT∥.(2)
The terms LrootandLrotenforces the output motion ˆ mt=
{ˆ p0
t,ˆ q0
t, ...,qJ
t}to follow the given ground truth data.
The loss term Lpospenalizes the error accumulating
along the kinematic chain by considering position values
of each joint, and is measured by the weighted difference of
joint positions.
Lpos=X
j∈Jwj∥ˆ pj−ˆ pj
GT∥ (3)
where ˆ pjindicates position of joint j, which is obtained
by forward kinematics operation. In case of foot joints,
Lfootvel =∥ˆ vfoot−ˆ vfoot
GT∥is additionally computed to
penalize foot slip artifacts and unnatural foot movements.
ˆ vjindicates velocity of joint j.
LmidandLcons are terms to jointly train the submodule
FendandFbody.
Lmid=X
j∈end∥ˆ pj
mid−ˆ pj
GT∥
Lcons=X
j∈end∥ˆ pj−ˆ pj
mid∥(4)
where ˆ pj
midindicates the output of Fend(end-effector po-
sitions). Lmid directly trains the Fendmodule to gener-
ate accurate intermediate representations xmid
τ. The con-
sistency loss Lcons enforces the consistency between mid-
representations and the output motion. Intuitively, the end-
effector joint positions derived from the output motion ˆ mt
should be consistent with the intermediate output xmid
τ
which is also the positions of end-effector joints. The two
loss terms enforce Fendto provide reliable additional in-
formation to submodule Fbody, and enforce Fbodyto reflect
the provided mid-representation to the final output.
Lcontact =BCELoss (c,cGT)is to train the model to
estimate contact probabilities c. Here the subscript GT indi-
cates ground truth while time τis omitted for convenience.
1095
3.4. Motion Optimization With Visual Cues
Generating Visual Cues From Images. Since we only use
upper body sensors, fundamental ambiguities exist on the
lower body joints. For example, sitting and crossing one’s
legs cannot be easily distinguishable from other sitting pos-
tures. Also, different from VR-based systems, the absence
of explicit positional data in raw IMU sensors can lead to
uncertainties in determining hand positions. Although such
ambiguities can be resolved in dynamic body movements
(e.g., locomotion) by taking the dynamics into account via
temporal information, our daily activities (e.g., manipula-
tion) may often contain subtle and sophisticated motions
with minimum movements. As a solution to enhance the
motion capture accuracy, we present a method to leverage
the visual cues from the head-mounted camera, based on the
observation that it is common to look at one’s hands and the
object during typical hand-involved interactions. The idea
can be extended into multi-people scenarios, where we can
assume multiple people wear our motion capture systems
and their visual cues are shared with each other. As demon-
strated in our experiments, a person’s egocentric view can
be treated as occasionally available sparse 3rd person views
which can be used as informative additional visual cues.
Motivated by this idea, we leverage the RGB information of
the egocentric video I. The visual cues Φ= [ϕE, ϕT]rep-
resent additional information of the human pose. In single-
person egocentric setting, ϕE={µj
t}where µj
t∈R2, in-
dicates the 2D location of joint jin 2D image coordinates.
Joint jis either left or right wrist, or both in practice. In
the case of multi-person capture scenarios, we can leverage
a monocular 3D human pose estimation approach from 3rd
person views of other individuals, which can be used during
our motion optimization. The estimated 3D pose is denoted
asϕT={τ− − →BA
t}, where τ− − →BA
t= (q1
t...qj
t)Aindicates local
joint rotations (excluding the root) of person Aestimated
viaIB
t, or image taken by person Bat time t. We use off-
the-shelf models [15, 31] to obtain visual cues ϕEandϕT.
Manifold-Based Motion Optimization. The optimized
motion Mshould not only fulfill the occasionally detected
cuesΦ, but it should maintain the semantics and natural-
ness of Mreg, preserving spatiotemporal coherency. Instead
of directly optimizing the motion output from MregtoΦ,
we utilize the motion manifold-based method demonstrated
in [18, 27]. As the motion manifold is learned to preserve
spatio-temporal correlation of human movements, optimiz-
ing within this manifold space can enforce the optimized
motion Mto maintain its naturalness while fulfilling the
target cues Φ. Especially in the case of egocentric visual
cueϕEwhere only 2D joint cues are available, optimizing
within this manifold space helps to maintain naturalness in
the optimized motion despite the ambiguity of the cue.
To build motion manifolds we use convolutional au-toencoders [18], compressing motion sequences into corre-
sponding latent vectors. For training, the motion sequence
X∈RT×137is represented as a concatenated vector of
foot contact, root translation, and joint rotations. The root
translation and orientation are normalized based on the first
frame of the motion sequence. The rotation values are con-
verted into 6D representations [60] for network learning.
The encoder Eand decoder module E−1are trained based
on reconstruction loss, or Lrecon =||X−E−1(E(X))||2.
Motion optimization is done by searching an optimal
latent vector among the manifold. From the initial latent
vector z=E(X), where Xis computed from Mreg,
the optimal latent vector z∗is found by minimizing loss
L=Lvis+Lreg+Lcontact .3Lvisenforces to meet the
provided visual cues:
Lvis=(
||µj
t−π(Ct,pj
t)||µj
t∈ϕEP
jwj||τj
t−qj
t||τj
t∈ϕT.(5)
The function π(Ct,pj
t)indicates the projection of 3D joint
position pj
twith camera Ct.Lregis for regularization and
Lcontact =ct· ∥˙pfoot
t∥penalizes foot slip when the foot is
in contact. Finally, the optimized motion Mis derived from
the optimized latent vector z∗byE−1(z∗).
4. Experiments
4.1. Experimental Setup
Synthesizing IMU Data from Mocap. To build training
data from motion datasets, we synthesize IMU signals using
wrist poses, following the protocol in [23, 55, 56].
Processing IMU Signals from Smartwatches. For demon-
strations with real-world IMU data using smartwatches, we
apply filtering to reduce noise as in [23] beforehand.
Evaluation Metrics. We adopt the following metrics for
quantitative experiments.
•Mean Per Joint Position Error (MPJPE): MPJPE
represents the average position error per joint. ( cm)
•Mean Per Joint Velocity Error (MPJVE): MPJVE
measures the average velocity error across joints. ( cm/s )
•Jitter: Jitter indicates how smooth the motion is, and
is measured by acceleration changes over time averaged by
body joints. Here we present the relative ratio between the
jitter of the predicted motion and the ground truth.
Baselines. Since we are the first to estimate full body mo-
tion from a head-mounted camera and two IMU sensors,
no direct baselines exist. Thus, we compare our estimation
module Festwith previous SOTAs based on (1) 6 IMUs
(full body) [23, 55], and (2) VR devices (upper body, 6
DoF) [12], which pursue similar goals with more infor-
mation than our setup through additional sensors or 6DoF
3Here the superscript− →BA andAindicating humans in τis omitted for
convenience, and is replaced with joint indicators. τj
t=τ− − →BA
tof joint j.
1096
hand measurements. In the context of wearable and out-
door motion capture, we consider IMU-based methods as
our main competitors. However, for completeness, we addi-
tionally compare with the VR device-based methods due to
their technical similarity to our approach of estimating mo-
tion from upper body sensors. Note that in this comparison
we assume fixed flat floor planes since previous approaches
cannot handle varying floor levels.
•IMU Baselines (Full Body): We compare ours with
PIP [55] and TIP [23]. Both estimate motion from 6 IMU
sensors on the full body; head, wrists, pelvis, and legs. In
the supp. mat. we also compare with EgoLocate [54] which
leverages an additional head-mounted camera on top of the
full-body IMU sensors for global translation correction.
•VR Baselines (Upper Body): We consider
AGRoL [12], the state-of-the-art method for tracking
full body motion from 3 6DoF (position and orientation)
sensor signals from the head and both wrists, as a baseline.
We compare with two versions of AGRoL, the original
version where the 6DoF values are given for all 3 sensors
and a modified version where the position values of the
wrist are replaced with acceleration as in IMU sensors.
•Ablative Baselines: We perform ablation studies on the
floor level update in Festto present the contribution of the
algorithm on scenarios with large areas with drastic floor
changes such as walking down the stairs from the third to
the second floor of a building. We furthermore compare the
output of Festonly and with Foptto demonstrate the con-
tribution of egocentric video to enhance motion estimation.
4.2. Comparison with IMU-Based Baselines
Dataset. We follow the previous approaches [23, 55] by
training the models on AMASS [33] dataset and evaluating
on the TotalCapture [44] dataset with real IMU data.4
Results. As shown in Table 1, despite only utilizing sensors
on the upper body, our method shows comparable or bet-
ter performances compared to 6 IMU-based setups. In the
root-relative position errors (r.MPJPE) which compare pose
estimation quality ignoring root drifts, our approach outper-
forms the baselines. Notably, although PIP [55] explicitly
correct pose with physics-based optimization, the perfor-
mance of ours is comparable without such explicit correc-
tions. In other metrics where root drift errors are also con-
sidered (MPJPE, Root PE), our method shows much better
performance with significant decreases in root drift com-
pared to baselines (MPJPE decreases by 87.7% for PIP and
89.8% for TIP; Root PE by 90.5% and 92.1%, respectively).
For comparison with EgoLocate, results in supp. Table 2
shows our superior performance, especially in root-related
4While previous approaches use DIP dataset [19] as well, we exclude
this in our test, because DIP does not include global translations and we
cannot generate head positions from the dataset. DIP is much smaller than
AMASS and consists of less than 5% of the whole training dataset.Method r.MPJPE MPJPE MPJVE Root PE Jitter
PIP [55] 4.40 34.69 19.51 34.15 0.95
TIP [23] 4.88 41.79 36.26 41.18 16.52
Ours (Fest) 3.77 4.27 19.56 3.25 5.23
Table 1. Comparison with full body IMU-based methods on the
TotalCapture dataset.
Dataset Method MPJPE (r.) MPJVE Root PE Jitter
AMASS AGRoL [12] 4.58 (4.89) 19.31 4.15 2.60
AGRoL* 6.53 (6.18) 24.99 5.33 3.49
Ours (Fest) 5.20 (4.95) 17.00 4.36 1.64
HPS AGRoL [12] 31.47 (27.64) 258.92 25.76 40.83
AGRoL* 33.46 (27.69) 389.83 28.22 62.06
Ours (Fest) 8.65 (8.24) 21.42 6.97 1.84
Table 2. Comparison with upper body VR-based methods on
AMASS and HPS datasets. AGRoL with an asterisk is the modi-
fied version where wrist position are replaced with accelerations.
(r.) indicates root-relative position errors.
position error terms (MPJPE, Root PE), despite the reduced
number of sensors. These results demonstrate that incorpo-
rating head pose directly in estimation can improve perfor-
mance and mitigate root drift issues without any additional
localization or correction as done in previous approaches.
4.3. Comparison with VR-Based Baselines
Dataset. For quantitative comparison, we generate
train/test/validation split from a subset of AMASS dataset
(CMU [45], HDM05 [37], BMLMovi [13]). We first
evaluate our motion estimation module and AGRoL on the
testing split of the AMASS dataset. Additionally, we also
demonstrate the generalization ability of the modules by
testing on the HPS [16] dataset. Regarding HPS dataset, we
use the motion optimization result [16] as ground truth. We
randomly select 10 sequences in 6 scenes for comparison.
Results. As demonstrated in Table 2, our method shows
on-par performance with the original AGRoL, even though
in AGRoL the positions of both hands are provided. Com-
pared with the modified AGRoL where wrist positions are
replaced with accelerations, our module Festoutperforms it
in all of the metrics. Furthermore, our module Festshows a
notable performance gap compared to AGRoL on the HPS
dataset which includes motions in large-scale scenes. This
is expected as AGRoL focuses on full body motion estima-
tion in a VR-optimized setting, not in large-scale scenes.
4.4. Ablation Studies
Setup. For quantitative evaluation, we capture with our
setup and with XSens MVN Link [6] together, where we
use XSens as ground truth. More details are in supp. mat.
Ablation on Motion Estimation. We demonstrate the con-
tribution of the floor-level updating algorithm by capturing
a sequence of walking downstairs. As seen in Table 3 and
Fig. 5, without floor update (left) the head height is mis-
1097
Method Upper PE Lower PE Full PE
Festw/o Floor Update 9.19 16.58 12.35
Festw/ Floor Update (Ours) 6.04 7.19 6.53
Table 3. Quantitative comparison with and Festwithout floor level
update.
Figure 5. Comparison with Festwithout floor update (left) and
with floor update (right). The floor update algorithm corrects head
height for estimating accurate poses.
Method Right Hand PE Right Arm PE Upper PE
Fest 11.91 9.72 8.04
Fest+Fopt(Ours) 5.55 5.97 6.65
Table 4. Quantitative comparison with FestandFest+Foptop-
timized with egocentric-view visual cues.
leading and consequently, the module generates inaccurate
poses. However, by accordingly updating the floor levels
(right) the head height is adjusted so the network accurately
estimates the motion of walking down the stairs.
Ablation on Motion Optimization. We first demonstrate
the contribution of Foptin hand-based everyday interaction
scenarios. For quantitative comparison, we compare the ini-
tial motion output Mregand optimized final output Mto
ground-truth. As shown in Table 4, right arm PE and both
arm PE both decreased in the final motion output Mcom-
pared to the initial regression-based output Mreg. Example
results are shown in Fig. 6. We also demonstrate motion
optimization results from visual cues in multi-person sce-
narios. As seen in Fig. 7, the egocentric image of person
B serves as a “third-person view” of person A. Using this
information can resolve the fundamental ambiguities of the
legs that stem from upper-body sensor setups. Although the
legs of Fest(red) in Fig. 7 (a) are plausible, Fopt(blue) fur-
ther captures the slight nuances of leg movements by lever-
aging visual cues τ− − →BA
t.
5. Discussion
We present a novel, easy-to-use, and affordable motion
capture system with smartwatches on wrists and a head-
mounted camera. We overcome the sparsity and ambigu-
ity of sensor inputs with different modalities by integrating
head poses in the motion estimation pipeline. By tracking
and updating floor levels to define head poses and incorpo-
rating into multi-stage Transformer-based estimation mod-
Figure 6. Comparison with Fest(left) and Fest+Fopt(right). The
optimization module Foptcorrects the arm and hand pose based
on the 2D hand keypoint detected in the egocentric image.
Figure 7. Results of FestandFest+Foptin multi-person sce-
narios. (a) Comparison with Fest(red) and Fest+Fopt(blue) of
person A with visual cue τ− − →BA
t(Sec. 3.4). (b) Image IB
twith per-
son A. (c) 3rd person view of person A, B, and C.
ule our method can robustly capture motion in challenging
locations with non-flat grounds. We further present a mo-
tion optimization approach by using visual cues through the
camera. As multi-users can conveniently equip our setup,
we also explore the scenarios when user signals are shared.
Although the off-the-shelf models (e.g., DROID-SLAM)
in our method are robust and show reliable results in most
cases, our method does not work in rare catastrophic failures
of the off-the-shelf models. Moreover, the networks in our
module are trained with mean body shape, and head poses in
real-world demonstrations are adjusted by scaling. Explic-
itly handling body shape variations in the module would be
more promising which we leave as future work.
Acknowledgements. We thank Inhee Lee for support-
ing the system setup and thank Jiwon Song, Sumin Lee,
and Hakjean Kim for assisting multi-person capture. This
work was supported by SNU Creative-Pioneering Re-
searchers Program, NRF grant funded by the Korea govern-
ment (MSIT) (No.2022R1A2C2092724 and No. RS-2023-
00218601), and IITP grant funded by the Korean govern-
ment (MSIT) (No.2021-0-01343). H. Joo is the correspond-
ing author.
1098
References
[1] Meta quest vr headsets. https://www.meta.com/
quest/ . 2
[2] Project aria. https://www.projectaria.com/ . 1
[3] Ray-ban meta smart glasses. https://www.meta.com/
smart-glasses . 1
[4] Rokoko smartsuit pro. https://www.rokoko.com/
products/smartsuit-pro . 2
[5] Vicon motion capture system. https://www.vicon.
com/motion-capture . 2
[6] Xsens mvn link. https://www.movella.com/
products/motion-capture/xsens-mvn-link .
2, 7
[7] Karan Ahuja, Eyal Ofek, Mar Gonzalez-Franco, Christian
Holz, and Andrew D Wilson. Coolmoves: User motion ac-
centuation in virtual reality. ACM IMWUT , 2021. 3
[8] Sadegh Aliakbarian, Pashmina Cameron, Federica Bogo,
Andrew Fitzgibbon, and Thomas J Cashman. Flag: Flow-
based 3d avatar generation from sparse observations. In
CVPR , 2022. 3
[9] Sadegh Aliakbarian, Fatemeh Saleh, David Collier, Pash-
mina Cameron, and Darren Cosker. Hmd-nemo: Online 3d
avatar motion generation from sparse observations. In ICCV ,
2023. 2, 3
[10] Young-Woon Cha, Husam Shaik, Qian Zhang, Fan Feng, An-
drei State, Adrian Ilie, and Henry Fuchs. Mobile. egocentric
human body motion reconstruction using only eyeglasses-
mounted cameras and a few body-worn inertial sensors. In
IEEE VR , 2021. 2
[11] Andrea Dittadi, Sebastian Dziadzio, Darren Cosker, Ben
Lundell, Thomas J Cashman, and Jamie Shotton. Full-body
motion from a single head-mounted device: Generating smpl
poses from partial observations. In ICCV , 2021. 3
[12] Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke,
Ali Thabet, and Artsiom Sanakoyeu. Avatars grow legs: Gen-
erating smooth human motion from sparse tracking inputs
with diffusion model. In CVPR , 2023. 2, 3, 6, 7
[13] Saeed Ghorbani, Kimia Mahdaviani, Anne Thaler, Konrad
Kording, Douglas James Cook, Gunnar Blohm, and Niko-
laus F. Troje. MoVi: A Large Multipurpose Motion and
Video Dataset, 2020. 7
[14] Andrew Gilbert, Matthew Trumble, Charles Malleson,
Adrian Hilton, and John Collomosse. Fusing visual and in-
ertial sensors with semantics for 3d human pose estimation.
IJCV , 127, 2019. 2
[15] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran,
Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Re-
constructing and tracking humans with transformers. In
ICCV , 2023. 6
[16] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard
Pons-Moll. Human poseitioning system (hps): 3d human
pose estimation and self-localization in large scenes from
body-mounted sensors. In CVPR , 2021. 2, 7
[17] Thomas Helten, Meinard Muller, Hans-Peter Seidel, and
Christian Theobalt. Real-time body tracking with one depth
camera and inertial sensors. In ICCV , 2013. 2[18] Daniel Holden, Jun Saito, and Taku Komura. A deep learning
framework for character motion synthesis and editing. ACM
TOG , 35(4), 2016. 3, 6
[19] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J.
Black, Otmar Hilliges, and Gerard Pons-Moll. Deep inertial
poser learning to reconstruct human pose from sparse inertial
measurements in real time. ACM TOG , 37(6), 2018. 2, 7
[20] Hao Jiang and Kristen Grauman. Seeing invisible poses:
Estimating 3d body pose from egocentric video. In CVPR ,
2017. 3
[21] Hao Jiang and Vamsi Krishna Ithapu. Egocentric pose esti-
mation from human vision span. In ICCV , 2021. 2
[22] Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa
Laich, Patrick Snape, and Christian Holz. Avatarposer: Ar-
ticulated full-body pose tracking from sparse motion sens-
ing. In ECCV , 2022. 2, 3, 4
[23] Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won,
Alexander W Winkler, and C Karen Liu. Transformer inertial
poser: Real-time human motion reconstruction from sparse
imus with simultaneous terrain generation. In SIGGRAPH
Asia, 2022. 2, 6, 7
[24] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,
Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser
Sheikh. Panoptic studio: A massively multiview system for
social motion capture. In ICCV , 2015. 2
[25] Tomoya Kaichi, Tsubasa Maruyama, Mitsunori Tada, and
Hideo Saito. Resolving position ambiguity of imu-based hu-
man pose with a single rgb camera. Sensors , 20(19), 2020.
2
[26] Christoph Kalkbrenner, Steffen Hacker, Maria-Elena Algo-
rri, and Ronald Blechschmidt-Trapp. Motion capturing with
inertial measurement units and kinect. In BIOSTEC , 2014. 2
[27] Jiye Lee and Hanbyul Joo. Locomotion-action-
manipulation: Synthesizing human-scene interactions
in complex 3d environments. In ICCV , 2023. 6
[28] Sunmin Lee, Sebastian Starke, Yuting Ye, Jungdam Won,
and Alexander Winkler. Questenvsim: Environment-aware
simulated motion tracking from sparse sensors. In SIG-
GRAPH , 2023. 2, 3
[29] Jiaman Li, Karen Liu, and Jiajun Wu. Ego-body pose esti-
mation via ego-head pose estimation. In CVPR , 2023. 3
[30] Yebin Liu, Juergen Gall, Carsten Stoll, Qionghai Dai, Hans-
Peter Seidel, and Christian Theobalt. Markerless motion cap-
ture of multiple characters using multiview image segmenta-
tion. IEEE TPAMI , 35(11), 2013. 2
[31] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-
Clanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-
Ling Chang, Ming Guang Yong, Juhyun Lee, et al. Medi-
apipe: A framework for building perception pipelines. arXiv
preprint arXiv:1906.08172 , 2019. 6
[32] Zhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani.
Dynamics-regulated kinematic policy for egocentric pose es-
timation. 2021. 3
[33] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-
ard Pons-Moll, and Michael J Black. Amass: Archive of mo-
tion capture as surface shapes. In ICCV , 2019. 1, 7
1099
[34] Charles Malleson, John Collomosse, and Adrian Hilton.
Real-time multi-person motion capture from multi-view
video and imus. IJCV , 128, 2020. 2
[35] Charles Malleson, Andrew Gilbert, Matthew Trumble, John
Collomosse, Adrian Hilton, and Marco V olino. Real-time
full-body motion capture from video and imus. In 3DV,
2017. 2
[36] Josh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval
Tassa, Leonard Hasenclever, Vu Pham, Tom Erez, Greg
Wayne, and Nicolas Heess. Catch & carry: reusable neural
controllers for vision-guided whole-body tasks. ACM TOG ,
39(4), 2020. 3
[37] Meinard M ¨uller, Tido R ¨oder, Michael Clausen, Bernhard
Eberhardt, Bj ¨orn Kr ¨uger, and Andreas Weber. Documenta-
tion mocap database hdm05. Computer Graphics Technical
Report CG-2007-2, Universit ¨at Bonn , 2007. 7
[38] Evonne Ng, Donglai Xiang, Hanbyul Joo, and Kristen Grau-
man. You2me: Inferring body pose in egocentric video via
first and second person interactions. In CVPR , 2020. 3
[39] Gerard Pons-Moll, Andreas Baak, Thomas Helten,
Meinard M ¨uller, Hans-Peter Seidel, and Bodo Rosen-
hahn. Multisensor-fusion for 3d full-body human motion
capture. In CVPR , 2010. 2
[40] Takaaki Shiratori, Hyun Soo Park, Leonid Sigal, Yaser
Sheikh, and Jessica K. Hodgins. Motion capture from body-
mounted cameras. In SIGGRAPH , 2011. 2
[41] Zachary Teed and Jia Deng. DROID-SLAM: Deep Visual
SLAM for Monocular, Stereo, and RGB-D Cameras. In
NeurIPS , 2021. 3, 4
[42] Denis Tome, Thiemo Alldieck, Patrick Peluse, Gerard Pons-
Moll, Lourdes Agapito, Hernan Badino, and Fernando De la
Torre. Selfpose: 3d egocentric pose estimation from a head-
set mounted camera. IEEE TPAMI , 2020. 2
[43] Denis Tome, Patrick Peluse, Lourdes Agapito, and Hernan
Badino. xr-egopose: Egocentric 3d human pose from an hmd
camera. In ICCV , 2019. 2
[44] Matthew Trumble, Andrew Gilbert, Charles Malleson,
Adrian Hilton, and John Collomosse. Total capture: 3d hu-
man pose estimation fusing video and inertial sensors. In
BMVC , 2017. 2, 7
[45] Carnegie Mellon University. Cmu mocap dataset. 1, 7
[46] Timo V on Marcard, Roberto Henschel, Michael J Black,
Bodo Rosenhahn, and Gerard Pons-Moll. Recovering ac-
curate 3d human pose in the wild using imus and a moving
camera. In ECCV , 2018. 2
[47] Timo V on Marcard, Gerard Pons-Moll, and Bodo Rosen-
hahn. Human pose estimation from video and imus. IEEE
TPAMI , 38(8), 2016. 2
[48] Timo V on Marcard, Bodo Rosenhahn, Michael J Black, and
Gerard Pons-Moll. Sparse inertial poser: Automatic 3d hu-
man pose estimation from sparse imus. In Comput. Graph.
Forum , 2017. 2
[49] Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar,
and Christian Theobalt. Estimating egocentric 3d human
pose in global space. In ICCV , 2021. 2
[50] Jian Wang, Diogo Luvizon, Weipeng Xu, Lingjie Liu, Kri-
pasindhu Sarkar, and Christian Theobalt. Scene-aware ego-
centric 3d human pose estimation. In CVPR , 2023. 2[51] Alexander Winkler, Jungdam Won, and Yuting Ye. Quest-
sim: Human motion tracking from sparse sensors with simu-
lated avatars. In SIGGRAPH Asia , 2022. 2, 3
[52] Weipeng Xu, Avishek Chatterjee, Michael Zollhoefer, Helge
Rhodin, Pascal Fua, Hans-Peter Seidel, and Christian
Theobalt. Mo 2 cap 2: Real-time mobile 3d motion cap-
ture with a cap-mounted fisheye camera. IEEE TVCG , 25(5),
2019. 2
[53] Dongseok Yang, Doyeon Kim, and Sung-Hee Lee. Lob-
str: Real-time lower-body pose prediction from sparse upper-
body tracking signals. In Comput. Graph. Forum , 2021. 3
[54] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Vladislav
Golyanik, Shaohua Pan, Christian Theobalt, and Feng Xu.
Egolocate: Real-time motion capture, localization, and map-
ping with sparse body-mounted sensors. ACM TOG , 42(4),
2023. 2, 7
[55] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada,
Vladislav Golyanik, Christian Theobalt, and Feng Xu. Phys-
ical inertial poser (pip): Physics-aware real-time human mo-
tion tracking from sparse inertial sensors. In CVPR , 2022. 2,
6, 7
[56] Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time
3d human translation and pose estimation with six inertial
sensors. ACM TOG , 40(4), 2021. 2, 4, 6
[57] Ye Yuan and Kris Kitani. 3d ego-pose estimation via imita-
tion learning. In ECCV , 2018. 3
[58] Ye Yuan and Kris Kitani. Ego-pose estimation and forecast-
ing as real-time pd control. In ICCV , 2019. 3
[59] Yuxiang Zhang, Zhe Li, Liang An, Mengcheng Li, Tao Yu,
and Yebin Liu. Lightweight multi-person total motion cap-
ture using sparse multi-view cameras. In ICCV , 2021. 2
[60] Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li
Hao. On the continuity of rotation representations in neural
networks. In CVPR , 2019. 4, 6
1100
