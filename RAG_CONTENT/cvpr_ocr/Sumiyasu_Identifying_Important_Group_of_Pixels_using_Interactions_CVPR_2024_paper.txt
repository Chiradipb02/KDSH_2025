Identifying Important Group of Pixels using Interactions
Kosuke Sumiyasu1Kazuhiko Kawamoto2Hiroshi Kera3*
Chiba University, Japan
1kosuke.sumiyasu@gmail.com ,2kawa@faculty.chiba-u.jp ,3kera@chiba-u.jp ,
Abstract
To better understand the behavior of image classifiers,
it is useful to visualize the contribution of individual pix-
els to the model prediction. In this study, we propose a
method, MoXI ( Model e Xplanation by Interactions), that
efficiently and accurately identifies a group of pixels with
high prediction confidence. The proposed method employs
game-theoretic concepts, Shapley values and interactions,
taking into account the effects of individual pixels and
the cooperative influence of pixels on model confidence.
Theoretical analysis and experiments demonstrate that our
method better identifies the pixels that are highly contribut-
ing to the model outputs than widely-used by Grad-CAM,
Attention rollout, and Shapley value. While prior stud-
ies have suffered from the exponential computational cost
in the computation of Shapley value and interactions, we
show that this can be reduced to quadratic cost for our
task. The code is available at https://github.com/
KosukeSumiyasu/MoXI .
1. Introduction
Visualization of important image pixels has been widely
used to understand machine learning models in computer
vision tasks such as image classification [1, 3, 18, 20, 23,
31]. To this end, visualization methods compute the contri-
bution of each pixel to model decisions. For example, Grad-
CAM [23] measures the contribution using a weighted sum
of the feature maps of convolutional layers, where weights
are determined by the gradient of confidence score for any
target class with respect to the feature map entries. Atten-
tion rollout [1] measures it based on the attention weight of
encoders of a Vision Transformer.
Several recent studies revealed that a game-theoretic
concept, Shapley values [24], is a powerful indicator of
pixel contribution [8, 16, 18]. In multi-player games, Shap-
ley values measures the contribution of each player from the
average change in the total game reward with his/her pres-
*Corresponding Author.ence versus absence. When applied to an image classifier,
the pixels of an image are the players, which work cooper-
atively for the model output (e.g., confidence score). Un-
like Grad-CAM and Attention rollout, Shapley values com-
pute the contribution of pixels to the model output more
directly. The former methods use feature maps or attention
weights, the magnitude of whose entries are not necessarily
well-aligned with their contributions to the model output,
whereas the latter uses logits or confidence scores. Indeed,
Fig. 1 shows that the pixels with high Shapely values have a
significantly larger impact on confidence scores than those
determined by Grad-CAM or Attention rollout in both (a)
insertion case and (b) deletion case.
A crucial caveat of the aforementioned methods is that
they identify a group of important pixels by the individual
contribution of each pixel and overlook the collective con-
tribution of multiple pixels. For example, Fig. 1(a) shows
that the three methods only highlight the class object (i.e.,
duck) and do not indicate the background (i.e., sea) as an in-
formative factor. However, the set of pixels with the highest
contributions (e.g., highest Shapley values) does not imply
the most informative pixel set as a whole because the in-
formation overlap among pixels is not considered. Indeed,
the bottom row of Fig. 1(a) shows that the class object and
background greatly impact in synergy the confidence score.
In this paper, we propose an efficient game-theoretic
visualization method of image pixels with a high impact
on the prediction of an image classifier. Besides Shapley
values, we exploit interactions , a game-theoretical concept
that reflects the average effect of the cooperation of pix-
els. Namely, unlike prior methods, including Grad-CAM,
Attention rollout, and Shapley values, the proposed method
takes into account the cooperative contribution of pixels and
identifies the image pixels as a whole. In Fig. 1(a), the pro-
posed method identifies a pixel set on which the classifier
puts high classification confidence. Similarly, in Fig. 1(b),
it identifies a minimal pixel set without which the classi-
fication fails. Notably, we define self-context variants of
Shapley values and interactions, and reduce the number of
forward passes from exponential to quadratic times, which
resolves the fundamental challenge of game-theoretic ap-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6017
0.760.940.99(a) Insertion
0.060.020.89
0.910.700.490.670.110.04
0.000.000.000.990.990.99
MoXI(Ours)Shapley valueGrad-CAMAttention Rollout
0.000.020.03
0.990.980.93
(b) DeletionFigure 1. Examples of image patches with high contributions to the output of ViT-T. (a) Starting from an empty image, image patches
are inserted according to their contribution measured by each method. (b) Starting from an original image, image patches are removed
according to their contribution measured by each method. The heatmaps highlight the image patches inserted (deleted) to obtain the correct
(incorrect) classification. The selected patches are colored according to the timing of insertion/deletion. For insertion, only the proposed
method selects patches from the background. For deletion, the proposed method highlights the class object only. For both cases, the
proposed method highlights the least number of patches while achieving the highest/lowest confidence score.
proaches to be handy tools for model explanation.
In the experiments, we consider the insertion curve and
deletion curve on a subset of ImageNet images that are cor-
rectly classified by a pretrained classifier. Starting from
fully masked images, an insertion curve plots the increase
of classification accuracy as unmasking image patches from
highly contributing ones determined by each method. Sim-
ilarly, a deletion curve plots the accuracy decrease from the
clean images to fully masked ones. The results show that
the proposed method gives sharp insertion/deletion curves.
For example, the classification accuracy reached 90% with
images with 4% unmasked patches if selected by the pro-
posed method, significantly outperforming the results of
Grad-CAM (accuracy of 2%), Attention rollout (accuracy
of4%), and Shapley values (accuracy of 25%). Similar re-
sults are observed for the deletion curves and also when we
use common corruptions [15] instead of masking. Qualita-
tively, the heatmaps using the patches selected in the early
stage of the insertion curve show that our method highlights
both a class object and background, while the other meth-
ods mostly highlight the class object only. Meanwhile, in
the heatmaps from the deletion curves, our method particu-
larly highlights the class-discriminative region of the object,
while the others do not.
Our contributions are summarized as follows:
• We propose an efficient game-theoretic visualiza-
tion method, named MoXI ( Model e Xplanation by
Interactions), for a group of pixels that significantly in-
fluences the classification.• Our analysis supports a simple greedy strategy from a
game-theoretic perspective, leading us to use self-context
variants of Shapley values and interactions, which can be
computed exponentially faster than computing the origi-
nal ones.
• Extensive experiments show that our method more accu-
rately identifies the pixels that are highly contributing to
the model outputs than standard visualization methods.
2. Related Work
Visual explanation of model decision. Various methods
have been proposed to understand deep learning models for
vision tasks by quantifying and visualizing the contribution
of image pixels to the model output [1, 3, 5, 6, 18, 20, 23,
27, 31]. The contribution of pixels has been typically mea-
sured using feature maps in models. For example, Grad-
CAM [23] determines the contribution by applying weights
to the feature maps of the convolutional layers of a CNN
using gradients. Attention rollout [1], commonly used for
Vision Transformers, calculates the contributions using at-
tention maps. Several methods instead calculate the con-
tribution of each pixel by analyzing the sensitivity of the
confidence score with respect to each pixel [8, 16, 18, 20].
For example, RISE (Randomized Input Sampling for Ex-
planation; [20]) calculates the contributions empirically by
probing the model with randomly masked images of the
input image and obtaining the corresponding confidence
scores. SHAP (SHapley Additive exPlanations; [18]) dis-
6018
tributes confidence scores fairly to contributions by lever-
aging Shapley values from game theory. Importantly, the
aforementioned methods all measure the contribution of
each pixel independently; the collection of important pixels
consists of the pixels with high contributions. In contrast,
this study identifies the important pixels by further taking
into account the collective contributions of pixels.
Game-theoretic approach of model explanation. Sev-
eral recent studies have utilized a game-theoretic concept,
interactions, to analyze various phenomena of deep learn-
ing models and quantify an effect of pixel cooperation on
the model inference [7, 9, 21, 25, 28, 29]. Wang et al. [28]
showed that the transferability of adversarial images has a
negative correlation to the interactions. Zhang et al. [29]
showed the similarity between the computation of interac-
tions and dropout regularization. Deng et al. [9] discussed
the difference in information obtained between humans and
machine learning models using interactions. Sumiyasu et al.
[25] investigated misclassification by models using inter-
actions and discovered that the distribution of interactions
varies with the type of misclassified images. Thus, interac-
tions are helpful for understanding the model from the per-
spective of cooperative relationships between pixels. A crit-
ical issue of interaction-based analysis is its computational
cost; the computation of interaction requires an exponen-
tial number of forward passes with respect to the number
of pixels. In this paper, we propose an efficient approach
to explain a model using variants of interactions (and also
Shapley values), achieving the identification of important
pixels with only a quadratic number of forward passes.
3. Preliminaries
Shapley values. Shapley values was proposed in game
theory to measure the contribution of each player to the to-
tal reward that is obtained from multiple players working
cooperatively [24]. Let N={1, . . . , n }be the index set of
players, and let 2Ndef={S|S⊆N}be its power set. Given
a reward function f: 2N→R, the Shapley value ϕ(i|N)
of player iwith a context Nis defined as follows.
ϕ(i|N)def=X
S⊆N\{i}P(S|N\ {i}) [f(S∪ {i})−f(S)],
(1)
where P(A|B) =(|B|−|A|)!|A|!
(|B|+1)!. Here, |·|denotes the car-
dinality of set. Namely, the Shapley value ϕ(i|N)averages
over all S⊆N\{i}the reward increase on the participation
of player ito player set S.
Interactions. Interactions measure the contribution of
the cooperation between the two players to the total re-ward [13]. Interactions I(i, j)by players iandjare defined
as follows.
I(i, j|N)def=ϕ(Sij|N′)−ϕ(i|N\ {j})−ϕ(j|N\ {i}),
(2)
where two players i, j∈Nare regarded as a single player
Sij={i, j}andN′=N\{i, j}∪{Sij}(i.e.,|N′|=n−1).
In Eq. (2), the first term corresponds to the joint contribu-
tion from players (i, j), and the second and the third terms
correspond to the individual contribution of players iand
j, respectively. Namely, interactions quantify the average
cooperation on the reward of two players joining simultane-
ously . Importantly, we have I(i, i|N) =−ϕ(i|N).
Application to image classifiers. In the application of
Shapley values and interactions to image classifiers, an im-
agexwith npixels is regarded as the index set N=
{1, . . . , m }of players. Typically, the reward function fis
defined by f(x) = logP(y|x)
1−P(y|x)[9], where yrepresents the
class of x, and P(y|x)denotes the classifier’s confidence
score on class ywith input x. The reward f(S)of a sub-
set of pixels S⊂2Nof image xis similarly computed by
feeding a partially masked xto the classifier (i.e., the pixels
inN\Sare masked).
If the classifier is a convolutional neural network (CNN),
the masked region is conventionally filled with some base
value, such as 0 or the average pixel value [2, 30]. Such a
replacement may drop the original information of an image
but also inject a new feature. Thus, the choice of base value
affects the Shapley values and interactions. In contrast,
when a Vision Transformer is used, one can realize masking
in a rigid manner by applying a mask to the attention. To
our knowledge, most prior studies exploited Shapley values
and interactions on CNNs with the base value replacement,
which might not unleash the full potential of these quanti-
ties. To our knowledge, the only exception is [8], which
demonstrated that Shapley values can be calculated more
accurately using attention masking. We follow this strategy
in the computation of Shapley values and interactions for
Vision Transformers.
4. Method
We address the problem of identifying in a given image a set
of pixels that significantly influence the confidence score of
a classifier. While prior studies solve this by explicitly or
implicitly measuring the independent contribution of each
pixel to the confidence score, the proposed method takes
into account the collective contribution of pixels using inter-
actions. We refer to the proposed method as MoXI ( Model
eXplanation by Interactions).
We consider two approaches to measuring the contribu-
tion of pixels to the confidence score: (i) pixel insertion and
6019
(ii) pixel deletion. The former measures the contribution of
a pixel by the confidence gain when it is unmasked as in
Eqs. (1) and (2), while the latter measures it by the confi-
dence drop when it is masked.
4.1. Pixel Insertion
Problem 1 LetNbe the index set of all pixels of image x.
Letf: 2N→[0,1]be a function that gives the confidence
score on the class of index set, with the convention that pix-
els not included in the index set are masked. Find a subset
Sk⊂Nsuch that
Sk= arg max
S⊆N,|S|=kf(S), (3)
fork= 1,2, . . . ,|N|.
By its formulation, this problem is an NP-hard problem in
general. Particularly, fis here a CNN or Vision Trans-
former,1a highly nonlinear function. Thus, we resort to
a greedy strategy to solve it approximately.
Fork= 1, the index b1∈Nof the pixel with the highest
Shapley value of ϕ(b1|{b1})gives the optimal set S1=
{b1}by the its definition. For k= 2, we select the next pixel
b2with the one maximizing f({b1, b2}). Importantly, this
is equivalent to maximizing the sum of the Shapley value
and interaction, not the Shapley value alone.
b2= arg max
b∈N\{b1}f({b1, b})−f(∅)
= arg max
b∈N\{b1}ϕ({b1, b}|{{b1, b}})
= arg max
b∈N\{b1}ϕ(b|{b}) +I(b1, b|{b1, b})
= arg max
b∈N\{b1}ϕ(0)(b) +I(0)(b1, b), (4)
where
ϕ(0)(a)def=ϕ(a|{a}) =f(a)−f(∅) (5)
I(0)(a1, a2)def=I(a1, a2|{a1, a2})
=f(a1∪a2)−f(a1)−f(a2) +f(∅).(6)
We refer to such a particular form of Shapley values and in-
teractions to be self-context in the pixel insertion approach,
and they play an essential role in our framework. For k≥3,
we can similarly show that maximizing f(Sk−1∪{bk})with
respect to bkis equivalent to
bk= argmax
b∈N\Sk−1ϕ(0)(b) +I(0)(Sk−1, b). (7)
1With this assumption, we use a slight abuse of notation and assume,
e.g.,f({a,{b, c}}) =f({a, b, c })because in either case of {a,{b, c}}
or{a, b, c }, we input the image with pixels a, b, c to the model.Algorithm 1 Identification of a group of pixels in the pixel
insertion approach
Input: reward function f, index set Nof image pixels.
Output: Sequence of subsets S1, . . . , S |N|⊂N
1:Sk← {} for all k= 0, . . . ,|N|
2:fork= 1, . . . ,|N|do
3:bk←argmax
b∈N\Sk−1f(Sk−1∪ {b})
4:Sk←Sk−1∪ {bk}
5:end for
6:return S1, . . . , S |N|
Equation (7) shows that for identifying of index bkforSk, it
is crucial to consider the interaction between Sk−1andbk.
Even when a pixel indexed bhas a large Shapley value (the
first term), it may have a large negative interaction (the sec-
ond term) if its pixel information overlaps with that of Sk−1.
Namely, collecting pixels with large Shapley values does
not necessarily give the most informative pixel set.
To summarize, our analysis justifies a very simple greedy
algorithm Algorithm 1 from a game-theoretic perspective.
The algorithm seems trivial in hindsight, but prior studies
visualize highly contributing pixels only using Shapley val-
ues [8, 16, 18].
Computational cost. The identification of important pix-
els (or patches, in practice) using Shapley values requires
O(|N|2|N|)times of forward passes because of the average
over all S∈N\ {i}for all i∈N(cf. Eq. (1)). In con-
trast, our approach only requires O(|N|2)times of forward
passes in the worst case (see Appendix C for details of the
algorithm complexity and runtime).
SET-SUMtask. We now give an intuitive example for
showing the necessity of interactions using S ET-SUMtask.
SET-SUMtask is a variant of Problem 1 with a collection of
integers N⊂Zand reward function f(S) =sforS⊆N,
where sdenotes the sum of all types of integers in S. For
example, s= 3forS={2,2,1}. Note that for any i∈N,
we have f(Sk−1∪ {i}) =f(Sk−1) +iifi /∈Sk−1and
otherwise f(Sk−1∪ {i}) =f(Sk−1). In this way, when
the features already possessed are equal to the newly added
features, the model does not gain new information. This
shows the role of interaction in considering information re-
dundancy.
Visual S ET-SUMtask. We empirically confirm the ad-
vantage of using interactions in the visual S ET-SUMtask
on the synthetic MNIST dataset. This task is to accurately
6020
Shapley valueMoXI(-)MoXI
Top 1 AccuracyInsertion (%)
(a) ExampleLabel: 17
(b) InsertionFigure 2. (a) Example of a synthetic MNIST image in the visual
SET-SUMtask, labeled 17 by the sum of all types of numbers in
the image. (b) Insertion curves. The curves illustrate the change
of accuracy when adding image patches gradually with high con-
tributions identified by different methods at various unmasked im-
age rates, ranging from 0to100% . These curves use a mask-
ing method that fills in zeros for game-theoretic calculations and
model input during classification accuracy measurement. MoXI(-
) only employs self-context Shapley values, whereas MoXI addi-
tionally uses interactions across highly contributing patches.
predict the sum of all types of numbers in an image using
a model. We constructed composite images, each of which
consists of four randomly selected MNIST images arranged
in a 2x2 grid (cf. Fig. 2(a)). The label of a composite im-
age is the sum of all types of numbers in the image as in
the S ET-SUMproblem. The evaluation metric utilizes the
insertion curve, as detailed in Sec. 5. For the model and
dataset details, refer to Appendix A. The insertion curves in
Fig. 2(b) show that the MoXI achieves higher accuracy than
the methods using MoXI(-), which uses self-context Shap-
ley values, and the Shapley value methods when 50% and
75% of the image area are unmasked, i.e., the second and
the third number is appended. This demonstrates that MoXI
acquires non-redundant information more effectively.
4.2. Pixel Deletion
To address Problem 1, we considered the problem of identi-
fying groups of pixels with high confidence scores through
pixel insertion. Here, we aim at decreasing the confidence
scores via pixel deletion.
Problem 2 With the same conditions as outlined in Prob-
lem 1, find a subset Sk⊂Nsuch that
Sk= arg min
S⊆N,|S|=kf(N\S), (8)
fork= 1,2, . . . ,|N|.
We again resort to a greedy approach. The key difference
is that now we define and utilize a variant of Shapley valuethat measures the contribution of a player by its absence.
ϕd(i|N)def=X
S⊆N,i∈SPd(S\ {i}|N)[f(S)−f(S\ {i})],
(9)
where Pd(A|B) =(|B|−|A|−1)!|A|!
|B|!. This Shapley value
quantifies the average impact attributable to the removal of
player i. In Problem 1, we addressed the issue by defining
self-context Shapley values and interactions, as it involves
the case of incrementally adding pixels from the entire im-
age. In contrast, Problem 2 involves the sequential deletion
of pixels from an image, necessitating the formulation of
full-context Shapley values and interactions as follows:
ϕ(|N|)
d(a)def=Pd(S\ {a}|N)[f(N)−f(N\ {a})]
=1
|N|[f(N)−f(N\ {a})] (10)
I(|N|−1)
d(a1, a2)
def=ϕ(|N|−1)
d({a1, a2}|N\ {a1, a2} ∪ {{ a1, a2}})
−ϕ(|N|−1)
d(a1|N\ {a2})−ϕ(|N|−1)
d(a2|N\ {a1})
=1
|N| −1[f(N)−f(N\ {a1})
−f(N\ {a2}) +f(N\ {a1, a2})]. (11)
With these quantities, the greedy algorithm for pixel dele-
tion is as follows. For k= 1 , the index b1∈Nof
the pixel with the highest (deletion-based) Shapley value
ϕ(|N|)
d(b1) =−1
|N|[f(N\ {a})−f(N)]gives the optimal
setS1={b1}by its definition. For k= 2, we select the
next pixel b2that minimizes f(N\{b1, b2}). This choice is
again explained as a sum of Shapley value and interaction,
b2= arg min
b∈N\{b1}f(N\ {b1, b})−f(N)
= arg max
b∈N\{b1}ϕ(|N|−1)
d({b1, b}|N\ {b1, b} ∪ {{ b1, b}})
= arg max
b∈N\{b1}ϕ(|N|)
d(b) + (|N| −1)I(|N|−1)
d(b1, b).
(12)
Fork≥3, we can similarly show that minimizing f(N\
Sk−1∪ {b})with respect to bkis equivalent to
bk= argmax
b∈N\Sk−1[ϕ(|N|)
d(b)
+ (|N| − |Sk−1|)I(|N|−|Sk−1|)
d(Sk−1, b)].(13)
Again, the greedy algorithm is described from a game-
theoretic viewpoint. The only difference from the insertion
6021
Algorithm 2 Identification of a group of pixels in the pixel
deletion approach
Input: reward function f, index set of all images N.
Output: Sequence of subsets S1, . . . , S |N|⊂N
1:Sk← {} for all k= 0, . . . ,|N|
2:fork= 1, . . . ,|N|do
3:bk←argmin
b∈N\Sk−1f(N\(Sk−1∪ {b}))
4:Sk←Sk−1∪ {bk}
5:end for
6:return S1, . . . , S |N|
case is that the interaction term is now weighted. Algo-
rithm 2 summarises the procedure. The computational cost
of the pixel deletion approach is the same as the pixel inser-
tion approach, which only requires O(|N|2)times of for-
ward passes in the worst case.
5. Experiments
In this section, we evaluate the characteristics of identi-
fied patches through comparative experiments with existing
methods and demonstrate the effectiveness of our method.
Setup. Our experiments utilize the ImageNet dataset [10]
and focus on analyzing Vision Transformer [11] pre-trained
for the classification task. For baseline methods, we use
Grad-CAM [23]2, Grad-CAM++ [5], Attention rollout [1],
Shapley values, and MoXI(-), which do not utilize the inter-
actions present in MoXI. For insertion curve experiments,
we use the Pixel Insertion approach, while for deletion
curves, we utilize the Pixel Deletion approach. Following
the previous studies [21, 29], we consider image patches in-
stead of pixels to reduce computational costs. All methods
calculate the contributions for 14×14patches with a patch
size of 16×16, which is equal to the patch size and the
number of tokens in standard ViT models. We used a pre-
trained ViT-T3[11], DeiT-T4[26] and ResNet-185[14]. We
selected 1000 images, one corresponding to each label, all
of which were successfully classified in the test set. To re-
duce the computational burden, we computed Shapley val-
ues approximately by random sampling of Sin Eq. (1) as
in other studies [4, 22, 25, 29]. The sampling size is set to
2The target layer of Grad-CAM is set to the one before the layer nor-
malization in the final attention block of network. This choice is common,
seehttps://github.com/jacobgil/pytorch-grad-cam .
3https : / / huggingface . co / WinKawaks / vit - tiny -
patch16-224
4https : / / huggingface . co / facebook / deit - tiny -
patch16-224
5https://huggingface.co/microsoft/resnet-18
Top 1 AccuracyInsertion (%)Deletion (%)(a) Insertion(b) DeletionShapley valueMoXI(-)MoXIGrad-CAMAttention RolloutGrad-CAM++Figure 3. (a) Insertion curves. (b) Deletion curves. The curves il-
lustrate the change of accuracy when appending (removing) image
patches gradually with high contributions identified by different
methods at various unmasked (masked) image rates, ranging from
0to100% .
200. Moreover, we have adopted feature patch deletion as
the masking method for Shapley values and interactions. In
the following, we focus on ViT-T. See Appendix B for more
results.
5.1. Evaluating the importance of identified patches
We evaluate the importance of the image patches as de-
termined by the above methods, using insertion/deletion
curve metrics. The insertion curve identifies information-
rich patches, while the deletion curve helps identify patches
important for the model’s decision-making process. In our
insertion/deletion curve experiments, we utilized the mask-
ing method for patch deletion. For Grad-CAM, Attention
rollout, and Shapley value, image patches are inserted and
deleted in the same order.
The insertion curves in Fig. 3(a) show that MoXI ex-
hibits a sharper increase in classification accuracy compared
to the other methods. In particular, even with images where
only 4%is visible, MoXI achieves an accuracy of 90%,
whereas Grad-CAM, Attention rollout, and Shapley value
achieve 2%,4%, and 25%, respectively. This result indi-
cates that MoXI can efficiently identify important patches
for classification. Then, both the self-context and origi-
nal Shapley values, which are based on confidence scores,
achieve a sharper increase in classification accuracy. How-
ever, these two methods calculate the importance of individ-
ual patches and often select patches with similar informa-
tion. Consequently, MoXI can identify features contribut-
ing to a higher classification accuracy than these methods.
The deletion curves in Fig. 3(b) show that MoXI exhibits
a sharp decrease in classification accuracy compared to the
other methods. When concealing just 10% of an image,
MoXI significantly decreases the model’s accuracy to 16%.
In contrast, Grad-CAM and Attention rollout only decrease
the accuracy to approximately 79% under the same condi-
tions. This result indicates that MoXI, which accounts for
6022
(b) Deletionbitterntenchwire-haired fox terrier(a) InsertionMoXIMoXI(-)Grad-CAMAttention R.Shapley V .Imagesulphurbutterflywoodrabbitspoonbill
MoXIMoXI(-)Grad-CAMAttention R.Shapley V .ImageFigure 4. Visualization of important image patches by each method. The highlighted image patches are selected based on their contributions
calculated by each method. (a) Highlighting the patches incrementally added to an entire image until classification success. (b) Highlighting
the patches sequentially removed from a full image until classification failure.
interactions between patches, effectively identifies the im-
age patches important for classification. We observed anal-
ogous results for DeiT-T [26] and ResNet-18 [14] models,
as detailed in Appendix B. Additionally, we discuss the ap-
plication of masks using our method in Appendix D.
5.2. Confidence score-based visualization
We introduce two heatmap-based visualization methods tai-
lored for analyzing insertion and deletion patches. The first
method visualizes insertion patches, highlighting those im-
portant for accurate classification. The second focuses on
deletion patches, specifically identifying those whose dele-
tion significantly impacts the classification. The heatmap
shows higher values, indicated by shades closer to red, for
patches that were inserted or deleted earlier. The insertion
or deletion stops when the model reaches a successful clas-
sification or misclassification.
Heatmap visualization. Figure 4(a) displays a heatmap
for patch insertion. Compared to the existing methods,
MoXI’s heatmap highlights fewer regions and identifies the
class object. Interestingly, MoXI selects the patches on the
background as well as the class object. This visualization
explains the object and background is required for classifi-
cation and demonstrates the usefulness of the interaction.
Figure 4(b) displays a heatmap for patch deletion. The
heatmaps generated by MoXI(-) and Grad-CAM display ex-
tensive highlights across the image, while MoXI, Atten-
tion rollout, and Shapley value show more concentrated
highlights on the class object. This finding indicates that
these latter methods accurately capture important informa-
tion from the object. Notably, MoXI places less emphasis
on the background than Attention rollout and Shapley value.
This result suggests that MoXI effectively narrows down
information by selectively deleting the class object, which
could be advantageous for precise object localization.
(a) original(b) bull mastiff(c) tiger cat
Figure 5. Visualization of important region for a targeted class us-
ing the proposed method. (a) Original image. (b) Targeting the
bull mastiff class, which is predicted by the model. The high-
lighted patches are those sequentially removed from a full image
until predict the bull mastiff class. (c) Targeting tiger cat class.
We first removed the patches that has a positive contribution to
bull mastiff class and also negative contribution to tiger cat. Once
the tiger cat becomes the predicted class of the model, the patches
highly contributing to tiger cat is removed sequentially until the
prediction change, which are the highlighted patches.
Class-dicriminative localization. To enhance under-
standing of the model’s prediction process, localization for
specific classes improve interpretability. We have extended
MoXI to analyze a target class that differs from the model’s
prediction. For the detailed visualization, see Appendix F.
Figures 5(b) and 5(c) visualizes important regions for two
classes: the bull mastiff, as predicted by the model, and the
tiger cat, the target class. The heatmaps reveal that MoXI
highlights the bull mastiff’s facial area and the tiger cat’s
face and body. These observations demonstrate that MoXI
can identify important groups of image patches relevant to
the predicted class and class-specific features important for
decision-making.
6023
Deletion (%)Deletion (%)(a) gaussian noise(b) fog
Top 1 Accuracy
Shapley valueMoXI(-)MoXIGrad-CAMAttention RolloutFigure 6. Deletion curves by image corruptions instead of mask-
ing: (a) Gaussian noise and (b) fog. The curves illustrate the
change in accuracy along with the increase in the number of cor-
rupted image patches. The patches are corrupted from the highly
contributing ones determined by each method.
5.3. Common corruption effect on patch deletion
We investigate the risk of model misclassification when im-
age patches important for model accuracy are disrupted by
adding noise. In the deletion curve experiment of Sec. 5.1,
we used patch masking to simulate feature absence. Instead
of patch masking, we consider common corruption [15]:
fog and Gaussian noise at level 5 (for the other corruptions
such as brightness and motion blur, see Appendix G.1). We
apply these corruptions to image patches in the order se-
lected for patch deletion in Sec. 5.1.
Figure 6(a) shows the effect of Gaussian noise on the
deletion curve results. MoXI exhibits a significant decrease
in accuracy compared to the others, indicating MoXI is vul-
nerable to Gaussian noise. This result implies that MoXI
efficiently identifies important patches. Figure 6(b) shows
the fog corruption results, which are similar to those ob-
served for Gaussian noise. Furthermore, as detailed in Ap-
pendix G.1, MoXI similarly affects accuracy with the other
common corruptions. Additionally, we evaluate the effect
of adversarial perturbations. Interestingly, adversarial per-
turbations yield distinct results due to their deceptive effect
on the model’s internal features (see Appendix G.2).
5.4. Consistant explainability
We examine the consistent explainability of visualization
methods, regardless of the internal feature representation,
which is a key aspect of explainable artificial intelligence.
Specifically, we examine whether the models, trained with
varying numbers of classification classes, consistently se-
lect important image patches. We evaluate the consistency
using insertion and deletion curves for the models trained
with datasets containing 10, 20, 100, and 1000 classes. For
training the 10-class model, we select images from Ima-
geNet that share labels with CIFAR10. For the models with
20, 100, and 1000 classes, we extend the 10-class dataset
by adding images with randomly selected classes from Im-
class 10class 20class 100class 1000Top 1 AccuracyInsertion (%)(a) Attention rolloutInsertion (%)(b) MoXI
Figure 7. Insertion curves. (a) Attention rollout, (b) MoXI. The
curves illustrate the change in accuracy along with the increase in
the number of unmasked image patches. Each curve represents
the results from the pretrained models with 10,20,100, and 1000
classes, respectively. As the number of classes the model learns
increases, the accuracy of Attention rollout significantly decreases,
whereas MoXI experiences only a minor decrease in accuracy.
ageNet. We draw the insertion and deletion curves using the
10-class test images that are correctly classified.
Figures 7(a) and 7(b) show the insertion curve results for
Attention rollout and MoXI, respectively. Attention roll-
out decreases accuracy as the number of classes increases.
In contrast, MoXI does not decrease in accuracy. There-
fore, MoXI consistently selects important image patches for
accurate classification. In addition, the results from other
methods and deletion experiments are shown in the Ap-
pendix H. We confirmed that MoXI provides consistent ex-
plainability in the deletion curve experiments.
6. Conclusion
This study addressed the problem of identifying a group
of pixels that largely and collectively impact confidence
scores in image classification models. We justify simple
greedy algorithms from a game-theoretic view using Shap-
ley values and interactions. This analysis naturally suggests
the use of self-context and full-context variants of Shap-
ley values and interactions. Their computation only re-
quires a quadratic number of forward passes, whereas prior
studies compute Shapley values and/or interactions with an
exponential number of forward passes or heavy sampling-
based approximation. The experimental results show that
our method is more accurate in identifying the important
image patches for models than popular methods.
Acknowledgments
This work was supported by JSPS KAKENHI Grant Num-
ber JP22H03658 and JP22K17962.
6024
References
[1] Samira Abnar and Willem Zuidema. Quantifying attention
flow in transformers. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics ,
pages 4190–4197, Online, 2020. Association for Computa-
tional Linguistics. 1, 2, 6
[2] Marco Ancona, Cengiz Oztireli, and Markus Gross. Explain-
ing deep neural networks with a polynomial time algorithm
for shapley value approximation. In Proceedings of the 36th
International Conference on Machine Learning , pages 272–
281, Long Beach, California, USA, 2019. PMLR. 3
[3] Alexander Binder, Gr ´egoire Montavon, Sebastian La-
puschkin, Klaus-Robert M ¨uller, and Wojciech Samek.
Layer-Wise Relevance Propagation for Neural Networks
with Local Renormalization Layers , pages 63–71. Springer
International Publishing, Cham, 2016. 1, 2
[4] Javier Castro, Daniel G ´omez, and Juan Tejada. Polynomial
calculation of the shapley value based on sampling. Com-
puters & Operations Research , 36(5):1726–1730, 2009. Se-
lected papers presented at the Tenth International Sympo-
sium on Locational Decisions (ISOLDE X). 6
[5] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader,
and Vineeth N Balasubramanian. Grad-CAM ++: Gener-
alized gradient-based visual explanations for deep convolu-
tional networks. In Proceedings of the IEEE Winter Confer-
ence on Applications of Computer Vision . IEEE, 2018. 2,
6
[6] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter-
pretability beyond attention visualization. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 782–791, 2021. 2
[7] Xu Cheng, Chuntung Chu, Yi Zheng, Jie Ren, and Quanshi
Zhang. A game-theoretic taxonomy of visual concepts in
DNNs. arXiv preprint arXiv:2106.10938 , 2021. 3
[8] Ian Connick Covert, Chanwoo Kim, and Su-In Lee. Learn-
ing to estimate shapley values with vision transformers. In
The Eleventh International Conference on Learning Repre-
sentations , 2023. 1, 2, 3, 4
[9] Huiqi Deng, Qihan Ren, Hao Zhang, and Quanshi Zhang.
Discovering and explaining the representation bottleneck of
DNNs. In Proceedings of the International Conference on
Learning Representations , 2022. 3
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. ImageNet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 248–255, 2009.
6
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image
is worth 16x16 words: Transformers for image recognition
at scale. In Proceedings of the International Conference on
Learning Representations , 2021. 6, 1
[12] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. In Proceed-ings of the International Conference on Learning Represen-
tations , 2015. 4
[13] Michel Grabisch and Marc Roubens. An axiomatic approach
to the concept of interaction among players in cooperative
games. International Journal of Game Theory , 28:547–565,
1999. 3
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770–778, 2016. 6, 7, 1
[15] Dan Hendrycks and Thomas Dietterich. Benchmarking neu-
ral network robustness to common corruptions and pertur-
bations. In Proceedings of the International Conference on
Learning Representations , 2019. 2, 8
[16] Neil Jethani, Mukund Sudarshan, Ian Connick Covert, Su-In
Lee, and Rajesh Ranganath. FastSHAP: Real-time Shapley
value estimation. In Proceedings of the International Con-
ference on Learning Representations , 2022. 1, 2, 4
[17] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adver-
sarial examples in the physical world. Proceedings of the In-
ternational Conference on Learning Representations Work-
shop , 2017. 4
[18] Scott M. Lundberg and Su-In Lee. A unified approach to
interpreting model predictions. In Proceedings of the 31st
International Conference on Neural Information Processing
Systems , page 4768–4777, Red Hook, NY , USA, 2017. Cur-
ran Associates Inc. 1, 2, 4
[19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learning
models resistant to adversarial attacks. In Proceedings of
the International Conference on Learning Representations ,
2018. 4
[20] Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Random-
ized input sampling for explanation of black-box models. In
Proceedings of the British Machine Vision Conference , 2018.
1, 2
[21] Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou,
Yiting Chen, Xu Cheng, Xin Wang, Meng Zhou, Jie Shi,
and Quanshi Zhang. Towards a unified game-theoretic view
of adversarial perturbations and robustness. In Proceedings
of the Advances in Neural Information Processing Systems ,
pages 3797–3810, 2021. 3, 6
[22] Jie Ren, Zhanpeng Zhou, Qirui Chen, and Quanshi Zhang.
Towards a game-theoretic view of baseline values in the
shapley value, 2022. 6
[23] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek
Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-
tra. Grad-CAM: Visual explanations from deep networks via
gradient-based localization. In Proceedings of IEEE/CVF In-
ternational Conference on Computer Vision , pages 618–626,
2017. 1, 2, 6
[24] Lloyd S. Shapley. A value for n-person games. In Contri-
butions to the Theory of Games , pages 307–317, 1953. 1,
3
[25] Kosuke Sumiyasu, Kazuhiko Kawamoto, and Hiroshi Kera.
Game-theoretic understanding of misclassification, 2022. 3,
6
6025
[26] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herve Jegou. Training
data-efficient image transformers & distillation through at-
tention. In Proceedings of the 38th International Conference
on Machine Learning , pages 10347–10357, 2021. 6, 7, 1
[27] Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian
Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. Score-cam:
Score-weighted visual explanations for convolutional neural
networks, 2020. 2
[28] Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen
Wang, and Quanshi Zhang. A unified approach to inter-
preting and boosting adversarial transferability. In Proceed-
ings of the International Conference on Learning Represen-
tations , 2021. 3
[29] Hao Zhang, Sen Li, YinChao Ma, Mingjie Li, Yichen Xie,
and Quanshi Zhang. Interpreting and boosting dropout from
a game-theoretic view. In Proceedings of the International
Conference on Learning Representations , 2021. 3, 6
[30] Hao Zhang, Yichen Xie, Longjie Zheng, Die Zhang, and
Quanshi Zhang. Interpreting multivariate shapley interac-
tions in dnns. In The AAAI Conference on Artificial Intelli-
gence , 2021. 3
[31] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,
and Antonio Torralba. Learning deep features for discrimi-
native localization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2921–
2929, 2016. 1, 2
6026
